{
  "name" : "1104.0235.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Gaussian Robust Classification",
    "authors" : [ "Ido Ginodi", "Amir Globerson" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The generalization error of a classifier may be thought of as correlated with its robustness to perturbations of the data. Namely, if a classifier is capable of coping with distrubance, it is expected to generalize well. Indeed, it was established that the ordinary SVM formulation is equivalent to a robust formulation, in which an adversary may displace the training and testing points within a ball of pre-determined radius (Xu et al. [2009]).\nIn this work we explore a different kind of robustness. We suggest changing each data point with a Gaussian cloud centered at the original point. The loss is evaluated as the expectation of an underlying loss function on the cloud. This setup fits the fact that in many applications, the data is sampled along with noise. We develop a robust optimization (RO) framework, in which the adversary chooses the covariance of the noise. In our algorithm named GURU, the tuning parameter is the variance of the noise that contaminates the data, and so it can be estimated using physical or applicative considerations. Our experiments show that this framework generates classifiers that perform as well as SVM and even slightly better in some cases. Generalizations for Mercer kernels and for the multiclass case are presented as well. We also show that our framework may be further generalized, using the technique of convex perspective functions.\nContents"
    }, {
      "heading" : "1 Introduction 5",
      "text" : "1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2 The supervised learning framework . . . . . . . . . . . . . . . . . . . . . 5 3 Support Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 4 Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 5 Between robustness and regularization . . . . . . . . . . . . . . . . . . . . 9 6 Our contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 7 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10"
    }, {
      "heading" : "2 Gaussian Robust Classification 12",
      "text" : "1 Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2 The adversarial Choice . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.1 The structure of the loss function . . . . . . . . . . . . . . . . . . . 13 2.2 The optimal covariance matrix subject to a trace constraint . . . . . 16\n3 A smooth loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4 GURU: a primal algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 22 5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23"
    }, {
      "heading" : "3 Dual formulation 27",
      "text" : "1 Mathematical Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2 A general framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33"
    }, {
      "heading" : "4 Introducing Kernels 37",
      "text" : "1 A representer result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 2 KEN-GURU: A primal kernelized version of GURU . . . . . . . . . . . . 40 3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43"
    }, {
      "heading" : "5 The Multiclass Case 45",
      "text" : "1 Problem formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 2 The adversarial choice . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n2.1 Applying a spectral norm constraint . . . . . . . . . . . . . . . . . 46 2.2 The connection to the trace constraint . . . . . . . . . . . . . . . . 48\n3 M-GURU: a primal algorithm for the multiclass case . . . . . . . . . . . . 49\n4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50"
    }, {
      "heading" : "6 Discussion 52",
      "text" : "1 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 2 Generalizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53"
    }, {
      "heading" : "A Single-Point Algorithms 57",
      "text" : "1 Problem presentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 2 Computing the optimal displacement . . . . . . . . . . . . . . . . . . . . . 58 3 ASVC: Adversarial Support Vector Classification . . . . . . . . . . . . . . 58 4 The Multiclass Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 5 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60"
    }, {
      "heading" : "B Diagonal Covariance 61",
      "text" : "C Using the Multi-Hinge Loss 63\nChapter 1\nIntroduction"
    }, {
      "heading" : "1. Motivation",
      "text" : "The ability to understand new unseen data, based on knowledge that was gained using a training sample, is probably the main goal of machine learning. In the supervised learning setup, one is given a training set, consists of data samples along with labels indicating their ’type’ or ’class’. The learning task in this case is to develop a decision rule, which will allow predicting the correct label of unfamiliar data.\nAs the main goal is to be able to generalize, it makes sense to design the learning process so it reflects the conditions under which the classifier is going to be tested and used. In many real world applications, the data we are given is corrupted by noise. The noise may be either inherent to the process that generates the data or adversarial. Examples to an inherent noise include a noisy sensor and natural variability of the data. Adversarial noise is present for example in spam emails. In either way, it is vital to learn how to classify when it is present. We suggest to do it by preparing for the worst case. Amongst all noise distribution that have a bounded power (i.e. bounded covariance), the Gaussian noise is believed to be the most problematic, since it has the maximal entropy.\nBy designing a classifier that is robust to Gaussian noise, we are able to learn and generalize well, without the need to introduce an explicit regularization term. In that respect, our work aims at shading more light on the connection between robustness and generalization."
    }, {
      "heading" : "2. The supervised learning framework",
      "text" : "Formally speaking, the supervised learning setup consists of three major components:\n1. Data. We denote X the sample space, in which the data samples live (i.e. the objects one tries to classify. e.g., vector representation of handwritten digits). Alongside the sample space, we are given the label set, denoted Y . This set contains the various classes to which the data points may be assigned (e.g., 0, 1, . . . , 9 in the handwritten digits example). A distribution D is defined over X ×Y , and dictates the probability to sample a data point x ∈ X along with a label y ∈ Y . In our discussion we will\nrestrict ourselves to the Euclidean case, namely X = Rd. Unless stated otherwise, we assume a binary setting, in which Y = {+1,−1}.\n2. Hypothesis class. In the learning process, one considers candidate hypotheses taken out of the class H. This class consists of functions from X to Y . Its contents reflect some kind of prior data about the problem at hand. A well known example is the class of half-spaces, defined as\nHhalf−space = { φw(x) = sgn(wTx) ∣∣φw : Rd → {+1,−1}, w ∈ Rd} (1.1) 3. Loss measure. The means to measure the performance of a specific instance h ∈ H\nis the loss function, ` : X × Y × H → R+ The most intuitive loss function in the binary case is the zero-one loss, defined by\n`0−1(x, y;h) = 1[h(x)6=y] (1.2)\nThe learning task is to find the classifier h∗ ∈ H which is optimal, in the sense that it minimizes the actual risk, defined as\nerr(h) = E(x,y)∼D`(x, y;h) (1.3)\nMost of the times, however, it is the case that D is unknown. Even in the rare cases in which it is known, it is not always possible to optimize the expectation over it. The learner is thus given a training set S ⊆ X × Y of i.i.d. samples. The learning task in that case is to minimize the empirical risk, defined as\nêrr(h) = 1\nM M∑ m=1 `(xm, ym;h) (1.4)\nwhere S = {(xm, ym)}Mm=1. This technique is called empirical risk minimization (ERM). It is important to keep in mind that although the technical tool is ERM, the objective is always to have the actual risk as low as possible.\nSometimes, however, this is not the case. That is, in spite of the fact that the learned decision rule is capable of classifying the training data, it fails to do so on fresh test data. In this case we say that the generalization error is high, although the training error is low. The reason for such a failure is most often overfitting. In this situation, the learned classifier fits the training data very well, but misses the general rule behind the data. In the PAC model, overfitting is explained by a too rich hypothesis class. If the learner can choose a model that fits perfectly the training data - it will do so, ignoring the fact that the chosen model will possibly not be able to explain new data. Say for example that the hypothesis class consists of all the functions from the sample space to the labels space. A naı̈ve learner might choose a classifier that handles all the training points well, whereas any unknown sample is classified as +1. This selection might obviously have erroneous results. In the\nspirit of this idea, the PAC theory bounds the difference between the empirical and the actual risk using a combinatorial measure of the hypothesis class complexity, named VC dimension. For a detailed review see Vapnik [1995].\nA common solution for this problem is to add a regularization term to the objective of the minimization problem. Usually, a norm of the classifier is taken as a regularization term. From the statistical learning theory’s point of view, the regularization restricts the complexity of the model, and by that controls the difference between the training and testing error (Smola et al. [1998]; Evgeniou et al. [2000]; Bartlett et al. [2002]). The idea of minimizing the complexity of the model is not unique to the statistical theory, and may be traced back to the Ocaam’s razor principle: the simplest hypothesis that explains the phenomenon is likely to be the correct one. Another way to understand the regularization term, is as a means to introduce prior knowledge."
    }, {
      "heading" : "3. Support Vector Machines",
      "text" : "In support vector machine (SVM), the loss measure at hand is the hinge-loss\n`hinge(x m, ym;w) = [1− ymwTxm]+\n`hinge is a surrogate loss function, in the sense that it upper-bounds the zero-one loss. Furthermore, `hinge is convex, which makes it a far more convenient objective for numerical optimization than the zero-one loss. Note that the hinge loss intorduces penalty when the classifier correctly predicts the label of a sample, but does so with too little margin, i.e. wTxm < 1. The penalty on a wrong classification is linear in the distance of the sample from the hyperplane.\nAs discussed, optimizing the sum of the losses solely may result in poor generalization performace. The SVM solution is to add an L2 regularization term. The geometrical intuition behind this term is the following: The distance between the point xm and the hyperplane wTx = b is given by\n|wTxm − b| ‖w‖\n(1.5)\nOne may scale w and b in such a way that the point with the smallest margin (that is, the one closest to the hyperplane) will have ‖wTxm−b‖ = 1. In that case, the bilateral margin is 2‖w‖ (see Figure 1.2). This geometrical intuition, along with the fact that the hinge loss punishes too little margin, motivates the name Maximum Margin Classification that was granted to SVM. Hence, the SVM optimization task is\nmin w,b\nλ 2 ‖w‖2 + M∑ m=1 [1− ym(wTxm − b)]+ (1.6)\nThe parameter λ controls the tradeoff between the training error and the margin of the classifier (cf. Section 5)."
    }, {
      "heading" : "4. Robustness",
      "text" : "The objective of the learning is to be able to classify new data. Thus, being robust to perturbations of the data is usually a desirable property for a classifier. In some cases, the training data and the testing data are sampled from different processes, which are similar to some extent but are not identical (Bi and Zhang [2004]). This situation can happen also due to application specific issues, when new samples are sampled with reduced accuracy (for example, the training data may be collected with an expensive sensor, whereas cheaper sensors are deployed for actual use).\nEven harder scenario is the one of learning in the presence of an adversary that may corrupt the training data, the testing data or both. The key step in order to formulate the robust learning task, is to model the action of the adversary, i.e., to define what is the family\nof perturbations that he may apply on the data points. In the Robust-SVM model, the adversary may apply a bounded additive distrubance, by displacing a sample point within a ball around it (Shivaswamy et al. [2006]). This case is referred to as box-uncertainty. Globerson and Roweis [2006] assumed a different type of adversary. In their model, named FDROP, the adversary is allowed to delete a bounded number of features. This model results in more balanced classifiers, which are less likely to base their prediction only on a small subset of informative features.\nTwo issues usually repeat in robust learning formulations. The first one is the problem of the adversarial choice. Most of the times, the first step in the analysis of the model is characterizing the exact action of the adversary on a specific data sample, given specific model parameters. The Robust-SVM adversary will choose to displace the point perpendicularly to the separating hyperplane. FDROP’s adversary will delete the most informative features, i.e. those that have the maximal contribution to the dot product between the weights vector and the data point. The second issue is the restriction on the adversary’s action. Regardless of the actual type of perturbation that the adversary uses, one needs to bound the extent to which it is applied. If no constraint is specified, the adversary will choose his action in such a way that the signal to noise ratio (SNR) will vanish, and the data will no longer carry any information. In the Robust-SVM formulation, the adversary is constrained to perform a displacement within a bounded ball. In the case of FDROP, no more than a pre-defined number of features may be deleted.\nNote that robust formulations are closely related to the notion of consistency. A classifier is said to be consistent, if close enough data points are predicted to have the same label. Different adversarial models befit different notions of distance. For example, the boxuncertainty model is related to the Euclidean metric and feature deletion suits the hamming distance.\nIt should be mentioned that robustness has quite a few meanings in the literature of statistics and machine learning. In this work, we use robustness in the sense of robust optimization (RO), i.e. minimizing the worst-case loss under given circumstances."
    }, {
      "heading" : "5. Between robustness and regularization",
      "text" : "The fact the robustness is related to regularization and generalization is not too surprising. Indeed, first equivalence results have been established for learning problems other than classification more than a decade ago (Ghaoui and Lebret [1997]; Xu et al. [2008]; Bishop [1994]). Recently, Xu et al. [2009] have proven the fact that the regularization employed by SVM is equivalent to a robust formulation. Specificly, they have shown that the following two formulations are equivalent\nmin w,b λ‖w‖+ M∑ m=1 [1− ym(wTxm − b)]+\nmin w,b max∑ m ‖δm‖∗≤λ M∑ m=1 [1− ym(wT (xm − δm)− b)]+\nwhere ‖ · ‖∗ is the dual-norm. This equivalence has a strong geometric interpertation, and sheds a new light on the function of the tuning parameter of SVM. Using the notion of robustness, a consistency result for SVM was given, without the use of VC or stability arguments. The novelty of that work stems from the fact that most previous works on robust classification were not aimed at relating robustness to regularization. Rather, the models were based on an already regularized SVM formulation, in which the loss measure was effectively modified."
    }, {
      "heading" : "6. Our contribution",
      "text" : "In this work we adopt the idea of using robustness as a means to achieve generalization. We present a new robust-learning setup in which each data point is altered by a stochastic cloud centered on it. The loss is then evaluated as the expectation of an underlying loss on the cloud. The parameters of this cloud’s distribution are chosen in an adversarial fashion. We analyze the case in which the adversary is restricted to choose a Gaussin cloud with a tracebounded covariance matrix. Then we show that this formulation culminates in a smooth upper-approximation of the hinge loss, which gets tighter as the cloud around each data sample shrinks. This loss function can be shown to have a convex perspective structure. By deriving the dual problem, we are able to demonstrate a method of generating new smooth loss functions. Our algorithmic approach is to directly solve the primal problem. We show that this yields a learning algorithm which generalizes as well as SVM on synthetic as well as real data. Generalizations to the non-linear and multiclass cases are given."
    }, {
      "heading" : "7. Related work",
      "text" : "Other works have incorporated a noise model into the learning setup. For example, baptiste Pothin and Richard [2008] have warped the data points with ellipsoids. Pozdnoukhov et al. [2005] have shown how to train classifiers for distributions. Similar to what we do in this work, they use tails of distributions in their derivation. Their work, however, treated each data class as a distribution, whereas in this work we attach a noise distribution for each data point separately. Bhattacharyya et al. [2004b]; Shivaswamy et al. [2006] have employed second order cone programming (SOCP) methods in order to handle the uncertainty in the data. Bhattacharyya et al. [2004a] have assumed stochastic clouds instead of discrete points, as we do, but they did not try to minimize the expectation of the loss function over the cloud. Instead, their idea was to incorporate the idea with the soft margin framework. Bi and Zhang [2004] have tried to learn a better classifier by presenting the learning algorithm ’more reasonable’ samples. We elaborate on this model in Appendix A.\nSmooth loss function were studied by Zhang et al. [2003]; Chapelle [2007]. Analysis of methods for Solving SVM and SVM-like problems using the primal formulation was done by Shalev-Shwartz et al. [2007a]; Chapelle [2007].\nThe rest of this document is organized as follows: in Chapter 2 we present our framework\nformally, derive the explicit form of the smooth loss function and devise an algorithm that finds the optimal classifier. In Chapter 3 we derive a dual formulation for the problem, and point out that our model may be generalized for other loss functions. In Chapter 4 we apply the kernel trick and devise a method for training non-linear classifiers in the same cost as for the linear kernel. Chapter 5 contatins a generalization of the binary algorithm for the multiclass case. At last, in Chapter 6 we discuss the contributions presented in this work and mention possible directions for future work. In Appendix A we discuss a far more basic version of resistance to noise. The results of the first section therein are not original and presented here only for the sake of logical order. The next section contains a simple generalization for the multiclass case. Appendix B gives the solution to the adversarial choice problem for an adversary that is restricted to spread the noise along the primary axes. At last, in Appendix C we explain why we find the usual multiclass hinge loss inapplicable in our framework.\nChapter 2\nGaussian Robust Classification\nIn this work we take the approach of robust optimization (RO). Accordingly, we present a min-max learning framework, in which the learner strives to minimize the loss, whereas the adversary tries to maximize it. The model that we introduce in this chapter has two layers of ’robustness’. Firstly, we use the min-max robustness, which lays in the foundations of RO. Secondly, we effectively enhance the training dataset by taking into consideration all the possible outputs of the adversarial perturbation. More concretely, we alter each training sample with a stochastic cloud. The shape of this cloud is chosen by the adversary from a pre-determined family of distributions. The spreading of the samples should be understood as adding noise, where different disturbances take place with different probability. The loss on each sample is finally computed as the expectation of an underlying loss on the respective cloud."
    }, {
      "heading" : "1. Problem Formulation",
      "text" : "In this section we formally describe the model we investigate in the work. We take the hinge-loss as the underlying loss function, and build the learning framework on top of it. We then show that the new framewok we introduce is equivalent to an unconstrained minimization of an effective loss function.\nRecall that the hinge loss is defined\n`hinge(x m, ym;w) = [1− ymwTxm]+ (2.1)\nWe introduce the expected hinge loss\n`Ehinge(x m, ym;w,D) = En∼D\n[ 1− ymwT (xm + n) ] +\n(2.2)\nwhereD is a predefined noise distribution over the sample space. The optimization problem for learning a classifier w.r.t. the expected hinge loss is thus\nmin w M∑ m=1 `Ehinge(x m, ym;w,D) (2.3)\nGranting an adversary the ability to choose the noise distribution, we end up with the following formulation\nmin w max D1×D2×...×DM∈C1×C2×...×CM M∑ m=1 `Ehinge(x m, ym;w,Dm) (2.4)\nwhere Cm is the set of allowed noise distributions for the mth sample. In order for the adversarial optimization to be meaningful, all Cm’s should have a ’bounded’ nature. We now alter the order of maximization and summation, and write\nmin w M∑ m=1 max Dm∈Cm `Ehinge(x m, ym;w,Dm) (2.5)\nAt last, we observe that the optimization task at hand is nothing else than optimizing the effective loss function\n`robhinge(x m, ym;w,D) = max D∈C En∼D\n[ 1− ymwT (xm + n) ] +\n(2.6)\nWe refer to `robhinge(x m, ym;w,D) as the expected robust hinge loss."
    }, {
      "heading" : "2. The adversarial Choice",
      "text" : "Equation 2.5 presents the general noise-robust formulation. In the following, we will derive an explicit loss function for a specific collection of noise distributions. We focus on the case in which the adversary is constrained to spread a Gaussian noise, having a trace bounded covariance-matrix. The motivation behind this constraint is physical. When a noise is modeled with a distribution, its covariance is considered as its power. Thus, by constraining the sum of the eigenvalues of the covariance matrix we bound the power that the adversary can spread. The Gaussian noise is the worst case noise, in the sense that amongst all distributions with a certain poer bound it has the maximal entropy.\nUsing the notations of the previous section, we specify the restriction on the adversary as\nC = Cβ = {D ∼ N (0,Σ)|Σ ∈ Λβ}\nwhere Λβ = {Σ ∈ PSD|Tr(Σ) ≤ β}, i.e. Gaussian distributions having the zero vector as mean and a covariance matrix with a bounded sum of eigenvalues.\nIn the next couple of sections we will characterize the adversarial choice of the covariance matrix and derive an explicit loss function."
    }, {
      "heading" : "2.1 The structure of the loss function",
      "text" : "The following paragraphs are rather technical. For later use, we explicitly perform the integration of the robust hinge loss function. We then prove a monotony property of the integrated loss. This property will help us analyze the nature of the adversarial choice in\nour case. The key observation throughout the derivation is that the multivarite expectation can be transformed to a univariate problem.\nWe plug the notations that were introduced above into Equation 2.6 and get:\n`robhinge(x m, ym;w,Σ) = max Σ∈Λβ c|Σ|− 1 2\n∫ e− 1 2 nTΣ−1n [ 1− ymwT (xm + n) ] + dn (2.7)\nwhere c = (2π)−d/2 is the normalization constant. This is equivalent to:\n`robhinge(x m, ym;w,Σ) = max Σ∈Λβ c|Σ|− 1 2\n∫ e− 1 2 nTΣ−1n [ 1− ymwTxm − ymwTn ] + dn\n(2.8) As a first step in the analysis of the expected robust hinge loss, we shall handle the\nquantity\nQ def = c|Σ|− 1 2 ∫ e− 1 2 nTΣ−1n [ 1− ymwTxm − ymwTn ] + n (2.9)\nNote that the above only depends on n via products of the form wTn. Therefore, we define a new scalar variable u = ymwTn. Equation 2.9 can now be viewed as the expected value of g(u) = [1− ymwTxm − u]+. The moments of u are\nEu = EymwTn = = ymwTEn = 0\nand\nV ar[u] = V ar[ymwTn] =\n= ymwTV ar[n]ymw = = (ym)2wTΣw =\n= wTΣw\nThus we get\nQ = 1√\n2πwTΣw\n∫ e− u2 2wTΣw [ 1− ymwTxm − u ] + du (2.10)\nDefine erf(t) = 1√ 2π ∫ t −∞ exp(− 1 2 z2)dz. In addition, denote σ2(w,Σ) = wTΣw. The following proposition holds. Proposition 1 Q = ( 1− ymwTxm ) erf (\n1−ymwTxm√ σ2(w,Σ)\n) + √\nσ2(w,Σ) 2π exp\n( −(1−y mwTxm) 2\n2σ2(w,Σ)\n)\nProof: We conduct a direct computation:\nQ = 1√\n2πσ2(w,Σ)\n∫ e − u 2 2σ2(w,Σ) [ 1− ymwTxm − u ] + du =\n= 1√\n2πσ2(w,Σ) ∫ 1−ymwTxm −∞ e − u 2 2σ2(w,Σ) ( 1− ymwTxm − u ) du =\n= 1√\n2πσ2(w,Σ)\n(( 1− ymwTxm ) ∫ 1−ymwTxm −∞ e − u 2 2σ2(w,Σ)du\n− ∫ 1−ymwTxm −∞ ue − u 2 2σ2(w,Σ)du ) =\n= ( 1− ymwTxm ) erf ( 1− ymwTxm√\nσ2(w,Σ)\n)\n− 1√ 2πσ2(w,Σ) ∫ 1−ymwTxm −∞ ue − u 2 2σ2(w,Σ)du\nBy using the variable substitution theorem and observing that the remaining integrand is an odd function (thus the identity ∫ t −∞ odd = ∫ −t −∞ odd holds), we conclude that\nQ = ( 1− ymwTxm ) erf ( 1− ymwTxm√\nσ2(w,Σ)\n) + √ σ2(w,Σ)\n2π exp\n( − ( 1− ymwTxm )2 2σ2(w,Σ) ) (2.11)\nLet us establish the following simple property of Q.\nLemma 2 Q is monotone-increasing in σ2.\nProof: The fundamental theorem of calculus yields that\nd\ndt erf(t) = 1√ 2π exp\n( −t 2\n2\n) (2.12)\nUsing the chain rule we compute\ndQ dσ2 =\n1\n2 √ 2π √ σ2 exp\n( − ( 1− ymwTxm )2 2σ2(w,Σ) ) (2.13)\nIt is evident that for all σ2 ≥ 0 dQ\ndσ2 ≥ 0 (2.14)\ni.e. Q is monotone-increasing in σ2."
    }, {
      "heading" : "2.2 The optimal covariance matrix subject to a trace constraint",
      "text" : "We will now focus on finding the optimal adversary, i.e., performing the maximization of Equation 2.8 over the range of allowed covariance matrices. The next theorem specifies which covariance matrix attains the worst-case loss. In out terminology, refer to this result as the adversarial choice.\nTheorem 2.1: The optimal Σ in Equation 2.8 is given by Σ∗ = βwwT‖w‖2 where the optimization is done over Σ ∈ Λβ .\nBefore actually proving the theorem, we will give some geometric intuition. The idea behind the expected loss is to replace the original sample point with a Gaussian cloud centered at the original point (Figure 2.1a). Consider an arbitrary displacement x̂m = xm + n. For fixed w, n can be written as n = n‖ + n⊥. The relevant quantity is wT x̂m = wTxm + wTn‖, that is, the orthogonal component does not have any effect. Accordingly, it makes sense that the optimal noise direction is orthogonal to the separating hyperplane, i.e. parallel to the vector w (see Figure 2.1b).\nThe proof of Theorem 2.1 applies simple algebraic results to establish this result rigorously.\nProof: Plugging Proposition 1 into Equation 2.8 we get\n`robhinge(x m, ym;w,Σ) = max\nΣ∈Λβ\n[( 1− ymwTxm ) erf ( 1− ymwTxm√\nσ2(w,Σ)\n)\n+\n√ σ2(w,Σ)\n2π exp\n( − ( 1− ymwTxm )2 2σ2(w,Σ) )]\nThe above depends on Σ only via σ2(w,Σ). According to Lemma 2, the objective is monotone increasing in σ2. Therefore, the adversary would like to choose Σ so that\nσ2(w,Σ) is maximized. By applying the Cauchy-Schwartz inequality, we conclude that the maximum value of σ2(w,Σ) is λmax(Σ)‖w‖2. For all Σ ∈ Λβ it holds that Tr(Σ) ≤ β. Since all of the eigenvalues are positive, it holds that λmax ≤ β as well. Consider the candidate solution Σ0 = βww T ‖w‖2 . Since σ 2(w,Σ0) = β‖w‖2, this selection attains the maximum. Note that this covariance matrix reflects the fact that the adversarial choice would be to spread the noise parallel to the separator."
    }, {
      "heading" : "3. A smooth loss function",
      "text" : "In the previous sections we have done the technical computations needed in order to derive the robust hinge loss explicitly, and found the optimal covariance matrix. In the following we will put these results together, and present an explicit formulation of the loss function resulting from our model. In addition, it is shown that our robust loss can be represented as a perspective of a scalar smooth approximation of the hinge loss. By analyzing this function we are able to gain a better understanding of `robhinge. We conclude this section by showing that our loss function is a smooth convex upper-approximation of the hinge-loss. When the ’diameter’ of the noise cloud is shrunk, `robhinge coincides with the hinge-loss.\nWe devote a notation for the result of Proposition 1\nL(xm, ym;w, σ2) = ( 1− ymwTxm ) erf ( 1− ymwTxm\nσ ) +\nσ√ 2π exp\n( − ( 1− ymwTxm )2 2σ2 )\nBy combining the above equation with the result of Theorem 2.1 we conclude\n`robhinge(x m, ym;w, β) =\n( 1− ymwTxm ) erf (\n1− ymwTxm√ β‖w‖ ) + √ β‖w‖√\n2π exp\n( − ( 1− ymwTxm )2 2β‖w‖2 )\nβ has the meaning of statistical variance, and therefore in the following we will replace it with σ2 (not to be confused with σ2(w,Σ)). In order to understand the nature of the loss function we have defined, it is suggestive to define\nf(z) = zerf(z) + 1√ 2π e− z2 2 (2.15)\nUsing f , the robust expected hinge loss can be written as\n`robhinge(x m, ym;w, σ2) = σ‖w‖f\n( 1− ymwTxm\nσ‖w‖\n) (2.16)\nA direct computation shows that\ndf dz = erf(z) + z√ 2π e− z2 2 − z√ 2π e− z2 2 = erf(z) (2.17)\nWe are now ready to prove a simple yet fundamental property of f .\nTheorem 3.1: f is a smooth strictly-convex upper-approximation of the hinge loss.\nProof: Denote the hinge loss h(z) = [z]+. We must show that\n1. f is strictly-convex\n2. f(z) ≥ h(z)\n3. limz→−∞ f(z)− h(z) = 0\n4. limz→∞ f(z)− h(z) = 0\nDifferentiating Equation 2.17 once again, we get\nd2f dz2 = 1√ 2π e− z2 2 (2.18)\nwhich is clearly positive for all z. Thus, f is strictly-convex. For the upper bound property, notice that f(z) ≥ 0 for all z ∈ R. Hence, for z < 0 we\nsimply have f(z) > 0 = h(z). For the complementary case, denote the difference function\nover the positives δ(z) = f(z)− h(z) = zerf(z) + 1√ 2π e−\nz2\n2 − z. Using Equation 2.17 we obtain\ndh dz = erf(z)− 1 (2.19)\nIt can be easily seen that dh dz < 0, i.e. h is monotone decreasing. Observe that δ(0) = 1√\n2π\nand limz→∞ δ(z) = 0. Since all the functions involved are continous, we conclude that for z ≥ 0 it holds that h(z) ≤ f(z) ≤ h(z) + 1√\n2π . Altogether, we have established the upper\nbound property. For the asymptote at z → −∞, observe that from l’Hopital\nlim z→−∞ zerf(z) = lim z→−∞\n√ 2πe− z2 2 = 0\nSince the exponent in the right summand of f decays as well, we have that at z → −∞ both f(z) and h(z) coincide on z = 0. For the asymptote at z → ∞, we must show that f asymptotically coincide with the linear function z. To this end, let us write f(z) − z = z (erf(z)− 1) + 1√\n2π e−\nz2\n2 . Applying l’Hopital rule along with the asymptotic behavior of the exponent, we deduce that limz→∞ f(z)− z = 0, as desired.\nNext, we will analyze the relation between f and `robhinge.\nDefinition 3 Perspective of a function (from Boyd and Vandenberghe [2004] 3.2.6). If f : Rn → R, then the perspective of f is the function g : Rn+1 → R defined by\ng(t, x) = tf (x t ) with domain\ndom(g) = { (x, t) ∣∣∣x t ∈ dom(f), t > 0 }\nThe following lemma if useful. For a proof see Boyd and Vandenberghe [2004] 3.2.6, e.g.\nLemma 4 If f is convex (concave), then its perspective is convex (concave) as well.\nDefine the function\ng(a, b) = af\n( b\na\n) (2.20)\nLemma 4 implies that g ( σ‖w‖, 1− ymwTxm ) is jointly convex in both its arguments. In order to establish the strict-convexity of `robhinge in w, we need a more powerful tool. Consider the following lemma (Boyd and Vandenberghe [2004] 3.2.4)\nLemma 5 Let h : Rk → R, gi : Rn → R. Consider the function f(x) = h (g(x)) = h (g1(x), g2(x), ..., gk(x)). Then, f is convex if h is convex, h is nondecreasing in each argument, and gi are convex.\nThis lemma can be easily generalized to the case of strictly-convex functions. The proof is identical to that of the original version, thus will be skipped. We are now ready to prove the following theorem\nTheorem 3.2: `robhinge is strictly-convex in w.\nProof: From Lemma 4, g is convex. In additon, g is nondecreasing in each of its arguments. To see that, observe that\ndg da = f\n( b\na ) − b a f ′ ( b a ) = 1√ 2π exp ( −1 s b2 a2 ) dg db = f ′ ( b a ) = erf ( b a\n) which are both strictly positive. σ‖w‖ and ( 1− ymwTxm ) are both convex inw, thus we conclude by applying Lemma 5.\nThe next theorem explores some of the other properties of the loss function we have defined:\nTheorem 3.3: `robhinge is an upper-approximation to the hinge loss. Furthermore, when σ2 → 0, the loss function `robhinge coincides with the hinge loss.\nProof: For the upper bound property, we apply Theorem 3.1:\nσ‖w‖f ( 1− yiwTxi\nσ‖w‖\n) ≥ σ‖w‖h ( 1− yiwTxi\nσ‖w‖ ) = σ‖w‖ [ (1− yiwTxi\nσ‖w‖\n] +\n= [ 1− yiwTxi ] +\nFor the second part of the theorem, let us first observe that\nσ√ 2π exp\n( − ( 1− ymwTxm )2 2σ2 ) → 0 (2.21)\nas a multiplication of two vanishing factors at σ → 0. We consider two cases:\n1. 1− ymwTxm ≥ 0. Observe that erf ( 1− ymwTxm\nσ‖w‖\n) → erf(∞) = 1\nThus, `robhinge(x m, ym;w, σ2)→ 1− ymwTxm.\n2. 1− ymwTxm < 0. In this case erf ( 1− ymwTxm\nσ‖w‖\n) → erf(−∞) = 0\nThus, `robhinge(x m, ym;w, σ2)→ 0 Altogether, we have shown that when σ → 0, `robhinge(xm, ym;w, σ2) → [ 1− ymwTxm ] + .\nObserve that at w = 0 the loss function is not continuous. The discontinuity is removable, however, so this issue does not pose any problem.\nThe norm of the classifier ‖w‖ always appears in a multiplication with σ. Thus, we observe that it has a similar function. Namely, it controls the tightness of the approximation of the smooth loss function to the hinge. Since σ is pre-determined, the optimal norm should reflect some kind of compensation. We thus conjecture that there exist a inverse ratio between σ and the optimal norm (cf. Chapter 4).\nAt last, it should be noted that this loss smooth function can be viewed as a multiplicative regularized loss."
    }, {
      "heading" : "4. GURU: a primal algorithm",
      "text" : "We are now ready to devise an algorithm that solves our learning problem. In this section we describe a stochastic gradient descent (SGD) method that minimizes the strictly-convex loss function at hand. A convergence result for the algorithm stems from general properties of SGD that were studied extensively (see Shalev-Shwartz et al. [2007b]; Kivinen et al. [2003]; Zhang et al. [2003]; Nedic and Bertsekas [2000]; Bottou and Bousquet [2008], e.g.).\nPlugging the robust hinge loss function we have derived (Equation 2.15) into the original optimization task (Equation 2.5), we get\nmin w M∑ m=1 ( 1− ymwTxm ) erf ( 1− ymwTxm σ‖w‖ ) + σ‖w‖√ 2π exp ( − ( 1− ymwTxm )2 2σ2‖w‖2 ) (2.22) This formulation is a convex unconstrained minimization task. One very natural approach for solving this kind of task is using the family gradient descent methods. Denote the objective of the optimization as\nG(w) = ∑ gi(w) (2.23)\nIn batch gradient descent, in each step the algorithm updates\nw ← w − η∇G(w) (2.24)\nIn stochastic gradient methods the gradient is approximated as the gradient of one of the summands. Thus, the algorithm first randomizes an index i, then updates\nw ← w − η∇gi(w) (2.25)\nwhere η is the learning rate. The stochastic version suits settings of online learning, in which the learner is presented one training sample at a time. It has been suggested that using the stochastic version yields better generalization performance in learning tasks (Amari [1998]; Bottou and LeCun [2003]).\nOur algorithm, named GURU (GaUssian RobUst), optimizes Equation 2.22 using an SGD procedure. (For a full treatment see, e.g. Boyd (ref).) In order to derive the update formula, one should first calculate the gradient of the loss function. A straight forward computation yields\n∇w`robhinge(xi, yi;w, σ2) = −yixierf ( 1− yiwTxi\nσ‖w‖\n) +\nσw√ 2π‖w‖ exp\n( −(1− y iwTxi)2\n2σ2‖w‖2 ) (2.26)\nName #Training samples #Crossvalidation samples #Test samples\n#features\nToy(a) 200 200 200 2 Toy(b) 200 200 200 2 Ionosphere 100 100 152 34 diabetes 200 100 468 8 splice 1 vs. 2 500 400 635 60 USPS 3 vs. 5 800 700 700 256 USPS 5 vs. 8 800 700 700 256 USPS 7 vs. 9 800 700 700 256\nFor convergence results see Nedic and Bertsekas [2000]. For a full treatment, see Bertsekas et al. [2003], chapter 8."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section we present experimental results that demonstrate the fact that GURU generalizes as well as SVM. Experiments were carried out on two toy problems (see Figure 2.4 for a visualization), USPS handwritten digits classification (3 vs. 5, 5 vs. 8 and 7 vs. 9) and a couple of UCI databases (Frank and Asuncion [2010]). The sizes of the data sets are detailed in Table 2.1.\nWe have trianed GURU for σ values varying from 2−20 to 220, with exponential jumps. The learning rate was tuned empirically (values between 4−10 to 410 were tested). SVM was trained and tested using the SVM-light package. λ values between 4−15 and 415 were used. Note that in the SVM-light formulation, λ multiples the loss and not the regularization term. Thus, the qualitative relation between λ and σ is roughly σ ∼ 1\nλ . Parameter selection\nwas done based on the cross-validation set, and performance was evaluated for the optimal parameter on a testing set. The results are summarized in Table 2.2.\nOn the toy databases (a)-(b), the performance of GURU is identical to that of SVM. We have tested the learned classifiers’ resistance to Noise, by adding uniformly distributed random noise to both cross-validation and test sets. The results are presented in Figure 2.5. Observe that the resistance of GURU slightly outperforms that of SVM. Nontheless, this result gives an experimental support to the theoretical result in Xu et al. [2009], where it was shown that the ordinary SVM formulation is equivalent to a robust formulation, in which the adversary is capable of displacing the data samples.\nOn the Ionosphere database, GURU significantly outperforms SVM. The samples of this database consist of radar reading. Thus, GURU’s performance may be understood by the noisy nature of the samples. This finding supports the intuition that GURU perfoms well in noisy setups.\nOn USPS, the performance of GURU is pretty similar to that of SVM. Since the samples can be easily visualized as images, it is convenient to examine the adversarial action in this case. Consider Figure 2.6. The GURU adversary is symmetric, in the sense that it may move the samples either closer or further from the separating hyperplane. Hence, some digits look even more clear that the original ones, whereas others look as the opponent digit.\nIn addition, on the USPS dataset, GURU has demonstrated a significantly better resistance to noise than SVM (see Figure 2.7).\nChapter 3\nDual formulation\nIn this chapter we derive a dual probelm for the learning task at hand. We do not use the dual as a means to solve the primal problem, since the primal optimization works well. Rather, we use it to gain a better understanding of the problem. In the course of the derivation we use the notion of conjugate functions. We will show that the dual problem itself specifies the classifier up to a scailing factor. Thus, we devise a method to extract the norm using the available information. It is interesting to observe that throughout the derivation of the dual, the smooth function f plays a specific and distinguished role. Thus, the entire procedure may be applied as is for other smooth convex function, by only calculating their conjugate dual. We demonstrate this principle in Section 2, where we also discuss the relation between the primal loss and the dual formulation."
    }, {
      "heading" : "1. Mathematical Derivation",
      "text" : "This section is rather technical, and goes through the derivation of the dual. We start with the perspective representation of `robhinge, and introduce copule of auxilliary variables. Using these variables, the Lagrangian takes a form that we are able to analyze. Theorem 1.1 encapsulates the effect of f , in such a manner that other loss functions can be plugged into the derivation rather easily.\nThe main result of this section is that the dual form of Equation 2.22 is\nmax ∑\nm αm s.t. ‖ ∑\nm αmy mxm‖ ≤ σ ∑ m 1√ 2π exp ( − erfinv 2(αm) 2 ) α ≥ 0\n(3.1)\nIn the following paragraphs we will go through the details. The optimization task Equation 2.22 may be written as\nmin w σ‖w‖ ∑ m f ( 1− ymwTxm σ‖w‖ ) (3.2)\nWe introduce the auxilliary varibles zm, and constrain them with 1 − ymwTx ≤ zm. Note that f(z) is monotone increasing in z. Thus, at optimality zm = 1 − ymwTx. In addition, we introduce the variable r and constrain it with σ‖w‖ ≤ r. At the optimum r = σ‖w‖, since rf( z\nr ) is monotone increasing in r. Altogether we get the following\noptimization task min r ∑ m f ( zm r ) s.t. σ‖w‖ ≤ r\n1− ymwTxm ≤ zm r ≥ 0\n(3.3)\nwhere the optimization variables are w, z1, . . . , zn, r. The objective is convex according to Lemma 4, and the constraint on w is a second order cone. To find the dual, write the Lagrangian:\nL(w, r, z,α, λ) = r ∑ m f (zm r ) + λ [σ‖w‖ − r] + ∑ m αm [ 1− ymwTxm − zm ] − µr\nwhere λαm, µ ≥ 0 are the Lagrange multipliers. For later convenience we add a set of variables rm and force them all to equal r. So the new Lagrangian is:\nL(w, r, z,α, λ, δ) = ∑ m rmf ( zm rm ) + λ [σ‖w‖ − r]\n+ ∑ m αm [ 1− ymwTxm − zm ] − µr − ∑ m δm [rm − r]\nwhere δm ≥ 0. Recall that we have defined g(a, b) = af ( b a ) . Using this notion we get the following task\nmin w,r,z L(w, r, z,α, λ, δ) = min w,r ∑ m min zm,rm [g(rm, zm)− αmzm − δmrm]\n+λ [σ‖w‖ − r] + ∑ m αm [ 1− ymwTxm ] − µr + r ∑ m δm\n= min w,r ∑ m g∗(αm; δm) + λ [σ‖w‖ − r]\n+ ∑ m αm [ 1− ymwTxm ] − µr + r ∑ m δm\nwhere g∗ is by definition the conjugate function of g (for details see Boyd and Vandenberghe [2004] 3.3, e.g). Deriving the Lagrangian w.r.t. w gives:\nσλ w ‖w‖ = ∑ m αmy mxm (3.4)\nTaking the norm of both sides of the equation yields σλ(α) = ‖ ∑ m αmy mxm‖ (3.5)\nSubstituting this back into the objective, the terms with w cancel out and we have:\nmin r ∑ m g∗(αm; δm)− rλ(α)− µr + r ∑ m δm (3.6)\nThis is linear in r, thus deriving w.r.t. r yields a constraint ∑\nm δm = λ(α) + µ. Since µ ≥ 0, the equality constraint might be relaxed to ∑ m δm ≥ λ(α), and we end up with the following formulation max ∑ m αm + ∑ m g ∗(αm; δm)\ns.t. ∑\nm δm ≥ λ(α) α ≥ 0\n(3.7)\nOr: max ∑ m αm + ∑ m g ∗(αm; δm)\ns.t. ‖ ∑\nm αmy mxm‖ ≤ σ ∑ m δm\nα ≥ 0 (3.8)\nThe overall problem has a concave objective (since it’s a conjugate dual of a convex function) and second order cone constraints. In what follows we work out the form of the conjugate dual g∗.\nDenote by f ∗(α) the conjugate function of f (it is concave). The next theorem specifies the conjugate g∗ in terms of f ∗:\nTheorem 1.1: The conjugate dual of g(a, b) is\ng∗(α, δ) = { 0 f ∗(α) ≥ δ −∞ otherwise (3.9)\nProof: We must calculate\ng∗(α; δ) = min x,t\n( tf( x\nt )− αx− δt\n) (3.10)\nTo prove, we change from variables x, t to a variables z = x/t, t:\nmin t≥0,z tf(z)− αzt− δt = min t≥0,z t(f(z)− αz − δ) (3.11)\nFor the first case, assume that f ∗(α) ≥ δ, which implies that for all z:\nf(z)− αz ≥ δ (3.12)\nThen in Equation 3.11 the minimization is always of the product of t ≥ 0 and some nonnegative number. Hence it is always greater than zero, and zero can be attained at the limit t→ 0.\nOn the other hand if f ∗(α) < δ, we will show that there exists a pair t, z that achieves a value −∞: Since f ∗(α) < δ there exists a z for which\nf(z)− αz − δ < 0 (3.13)\nIf we take t→∞ and this z we get a value of −∞.\nIn order the complete the derivation of the dual formulation, we should compute the conjugate dual f ∗. The following lemma gives the desired result\nLemma 6 The conjugate dual of f is\nf ∗(α) = 1√ 2π exp\n( −erfinv 2(α)\n2 ) Proof: Recall that:\nf(z) = zerf(z) + 1√ 2π e− z2 2 (3.14)\nand that its first derivative is df\ndz = erf(z)\n(see Equation 2.17). By Theorem 3.1, f is convex, thus we compute f ’s conjugate dual:\nf ∗(α) = min z f(z)− αz (3.15)\nThe minimum satisfies:\nf ′(z) = α\nerf(z) = α z = erfinv(α)\nwhere erfinv is the inverse function of erf. We plug this equality into the objective and conclude\nf ∗(α) = f(erfinv(α))− αerfinv(α)\n= erfinv(α)α + 1√ 2π exp\n( −erfinv(α) 2\n2\n) − αerfinv(α)\n= 1√ 2π exp\n( −erfinv 2(α)\n2 ) It can be easily verified that f ∗ is concave, as expected from the theory. Note that from the derivation above it follows that αM ≤ 1.\nTaking the dual problem in Equation 3.8 and plugging in the conjugate duals derived above, we get: max ∑\nm αm s.t. ‖ ∑ m αmy mxm‖ ≤ σ ∑\nm δm 1√ 2π exp ( − erfinv 2(αm) 2 ) ≥ δm α ≥ 0\n(3.16)\nConsider the following problem: max ∑\nm αm s.t. ‖ ∑\nm αmy mxm‖ ≤ σ ∑ m 1√ 2π exp ( − erfinv 2(αm) 2 ) α ≥ 0\n(3.17)\nThe following proposition asserts that both of the formulations above are equivalent.\nProposition 7 Equation 3.16 and Equation 3.17 are equivalent.\nProof: Denote C1 the feasible region of Equation 3.16, and C2 the feasible region of Equation 3.17. Let α ∈ C1. Then trivially we have α ∈ C2. On the other hand, let α ∈ C2. Denote δm = 1√2π exp(− 1 2 erfinv(αm)2). It is easy to verify that this selection corresponds to a feasible point for Equation 3.16 (i.e. ∈ C1) with the same objective value.\nAs we have seen, the optimization problem we analyze in this work is a relative of the SVM problem. It is interesting to examine what happens when considering the duals. Consider the SVM formulation\nminw λ 2 ‖w‖2 + ∑M m=1 ξm s.t. ∀m ∈ {1, 2, . . . ,M} : ξm ≥ 1− ymwTxm ξm ≥ 0\n(3.18)\nIts dual is minα ∑M m=1 αm − 1 2 ∑M m,n=1 αmαny myn(xm)Txn\ns.t. ∀m ∈ {1, 2, . . . ,M} : 0 ≤ αm ≤ 1λ (3.19)\nThis dual form shares some properties with the dual form of GURU. For example, notice that in both cases one tries to maximize the sum of the dual variables αm. Another issue is that of the norm minimization. The SVM dual explicitly minimizes the norm of the classifier. In our dual, however, the situation is rather implicit: there exist a bound on the norm of the classifier. Without going into the details, we mention that moving a constraint into the objective or vice versa is possible in the context of Lagrangian duality. At last, notice that in spite of the fact that σ and λ play similar roles, increasing λ results in srinking the feasible region of the SVM duak, whereas in our problem, increasing σ expands the feasible region.\nThe last issue we discuss in this section is the norm of the optimal classifier. Note that by solving the dual formulation, one can only get the optimal classifier up to a scailing factor. Of course, it is essential to know the norm exactly in order to be able to use the classifier. This goal can be achieved using the following theorem:\nTheorem 1.2: The norm of the optimal classifier is\n‖w∗‖ = 1 erfinv(αm∗) + ym(ŵ∗) Txm (3.20)\nfor every m, where ŵ∗ is the normalized optimal classifier.\nProof: Equation 3.4 may be written as\nmin w,r,z L(w, r, z,α, λ, δ) = min w,r ∑ m min zm,rm [ rmf ( zm rm ) − αmzm − δmrm ] +λ [σ‖w‖ − r] +\n∑ m αm [ 1− ymwTxm ] − µr + r ∑ m δm\n= min w,r ∑ m min rm [ rm min zm [ f ( zm rm ) − αm zm rm ] − δmrm ] +λ [σ‖w‖ − r] +\n∑ m αm [ 1− ymwTxm ] − µr + r ∑ m δm\nsince rm ≥ 0. We define qm = zmrm . Since the equation above depends on zm only via qm, we get\nmin w,r,z L(w, r, z,α, λ, δ) = min w,r ∑ m min rm [ rm min qm [f (qm)− αmqm]− δmrm ] +λ [σ‖w‖ − r] +\n∑ m αm [ 1− ymwTxm ] − µr + r ∑ m δm\nIf when substituting the dual optimum in the Lagrangian, there exists a unique primal feasible solution, then it must be primal optimal (see Boyd and Vandenberghe [2004], 5.5.5 for details). Thus, at the optimum q∗m = minqm [f (qm)− αmqm]. According to the proof of Lemma 6 it holds that q∗m = erfinv(αm). By exploiting the monotony properties of the problem (that were presented in the beginning of the section), we conclude that\n1− ymwTxm\nσ‖w‖ = erfinv(αm) (3.21)\nThe desired result follows from basic algebraic operations.\nNote that the values of the optimal α’s are known, as well as the normalized vector ŵ∗ = w ∗\n‖w∗‖ . Thus, we can compute the optimal norm. It is possible that the norm of the optimal classifier is bounded (as a function of σ). Although we couldn’t prove this result, we conjecture that such a result might stem from a strong duality argument:\n‖α∗‖1 = ∑ m `robhinge(x m, ym;w∗, σ2) (3.22)\nBy plugging Equation 3.21 into the previous equality, we obtain\n‖w‖ = ‖α ∗‖1∑\nm f(erfinv (αm)) (3.23)\nA better understanding of the constraints onαmay help bounding the RHS of the equation. We have plotted the norm of the optimal classifiers for the toy problems of Chapter 2 (refer to Section 5 for more details). The results are shown in Figure 3.1 and clearly support this conjecture."
    }, {
      "heading" : "2. A general framework",
      "text" : "The dual form we have derived sheds some light on the structure of the problem. In this section we discuss the relation between the loss function f and the norm constraint that appears in the dual. We claim that there is a correspondence between approximations of f and relaxations of the dual problem. More specifically, approximations of the loss function culminates in approximations of the feasible region of the dual problem.\nThe norm constraint in the dual is a core component of the optimization. We denote by\ns(α) = exp ( −erfinv(α) 2\n2\n) (3.24)\nthe function under summation. It is complicated to handle and understand s(α), thus it is appealing to approximate it using elementary functions. Two such approximations are\ns̃1(α) = H2(α) = −α log2(α)− (1− α) log2(1− α)\ns̃2(α) = 4α(1− α) (3.25)\n(see Figure 3.2).\nNote that in the previous section we only used f as a means to express g∗ (Equation 3.9). Thus, if one replaces f with some alternative convex loss function f̃ , the derivation of the dual will remain correct. Of course, the dual norm constraint will be affected by this change. In order to understand the nature of the approximations in Equation 3.25, it is necessary to explore the respective dual conjugates.\nLemma 8 Let f̃2(z) = log2(1 + 2z). Then its conjugate dual is\nf̃ ∗2 (α) = −α log2(α)− (1− α) log2(1− α) (3.26)\nProof: We compute f̃2’s conjugate dual:\nf̃ ∗2 (α) = min z f̃2(z)− αz (3.27)\nThe minimum satisfies:\nf̃ ′2(z) = α 2z 2z + 1 = α\n2z = α\n1− α\nz = log2\n( α\n1− α\n)\nWe plug this equality into the objective and conclude\nf̃ ∗2 (α) = log2\n( 1 + α\n1− α\n) − α log2 α\n1− α = −α log2(α)− (1− α) log2(1− α)\nas claimed. As in the case of our Gaussian robust loss, we have α ≤ 1.\nLemma 9 Let\nf̃3(z) =  0 if z < −4 (z+4)2 16 if − 4 ≤ z ≤ 4\nz if z > 4 (3.28)\nThen its conjugate dual is\nf̃ ∗3 (α) = { 4α(1− α) if 0 ≤ α ≤ 1 −∞ if α > 1\n(3.29)\nProof: It is easy to verify that f̃3 is smooth. We thus compute f̃3’s conjugate dual in the following way:\nf̃ ∗3 (α) = min z f̃3(z)− αz (3.30)\nExtremum points satisfy:\nf̃ ′3(z)− α = 0 −α if z < −4 (z+4) 8 − α if − 4 ≤ z ≤ 4\n1− α if z > 4 = 0\nThe above equation vanishes at z = 8α− 4. For 0 ≤ α ≤ 1 we have −4 ≤ z ≤ 4, thus we conclude\nf̃ ∗3 (α) ∣∣∣ 0≤α≤1 = 4α(1− α)\nFor α > 1 we take z → ∞, and f̃3(z) = ∣∣∣ z>4\n(1 − α)z → −∞. Altogether we hace established the desired result.\nThese lemmas shade some light on `robhinge and on the structure of our problem. It turns out that the well-known log-loss as well as a quadratic loss that has the same flavour as the Huber loss appear naturally in our framework (see Figure 3.3 for a visualization). What we have demonstrated is that there exist a close connection between approximations of the primal loss and relaxations of the dual problem. Specifically, we have that the dual of\nmin w M∑ m=1 ‖w‖f̃ ( 1− ymwTxm ‖w‖ ) (3.31)\nis max ∑ m αm\ns.t. ‖ ∑\nm αmy mxm‖ ≤ ∑ m s̃(αm)\nα ≥ 0 (3.32)\nNote, however, that this connection should be further investigated. It should be observed that not every smooth convex primal loss f̃ yields a perspective that is convex in w. For that to happen, f̃ should satisfy some mathematical properties that are yet to be understood. One example for such a condition is f(z) ≥ z df̃\ndz (z). Under this condition we\ncan use the same reasoning as in the proof of Theorem 3.2 and conclude that the primal probem is convex. In this case, we can automaticaly apply the derivation presented in the previous section and deduce the respective dual problem. Another issue that should be better understood is the connection between approximations of f and the robust setup we have begun with. In particular, it is interesting to understand if the logarithmic loss may be interperted as resulting from RO.\nChapter 4\nIntroducing Kernels\nOne of the greatest stengths of the theory of support vector machines, is the simple generalization to nonlinear cases. This generalization is carried out via the elegant notion of kernels. An examination of our derivation suggests that one may apply the kernel trick and introduce a means to learn nonlinear classifiers in Gaussian Robust framework.\nIn this chapter we will develop a kernelized version of the GURU algorithm. Most of the derivation is straight forward: we begin by giving a representer result. Plugging the new parametrization of the classifer into the framework, we show that our update formulas are perfectly suitable for maintaining this kind of representation. The tricky part stems from the fact that our updates depend directly on the norm of the weights vector. Naive computation of the norm costs O(M2) operations, which significantly slows down the algorithm. We thus derive a procedure to update the norm in O(1), based on previous computations."
    }, {
      "heading" : "1. A representer result",
      "text" : "The first step towards kernelization of GURU, is to change our represention of the classifier from a weights vector (w) to a linear combination of the training samples. The theoretical justification of such an operations is known as a representer result. The fact that an optimal classifier may be represented as a linear combination of the training sample, stems from the mathematical theory of Hilbert spaces. In our case, as well as in SVM, however, the same result can be derived using far more simple and explicit argumentation. In this section we will show three ways to establish the representer result for the case of GURU. In spite of the fact that we could prove the theroem using abstract argumentation, it is necsssary to develop the technical proof, as it lays the foundations for the derivation of the kerenelized algorithm.\nWe start by stating a version of the representer theorem:\nTheorem 1.1: LetH be a reproducing kernel Hilbert space with a kernel κ : X ×X → R, a symmetric positive semi-definite function on the compact domain. For any function L :\nRn → R, and any nondecreasing function Ω : R→ R. If\nJ∗ = min f∈H J(f) = min f∈H\n{ Ω ( ‖f‖2H ) + L (f(x1), f(x2), . . . , f(xn)) } is well-defined, then there are some α1, α2, . . . αn ∈ R, such that\nf(·) = n∑ i=1 αiκ(xi, ·) (4.1)\nacheives J(f) = J∗. Furthermore, if Ω is increasing, then each minimizer of J(f) can be expressed in the form of Equation 4.1.\nFor a proof and more details, see for example Schölkopf and Smola [2002]. As mentioned, we will discuss three techniques to establis the required result. First, using the structure of the updates that GURU perform. Second, by the derivation of the dual problem presented in Chapter 3, and third, using the general representer theorem.\nTheorem 1.2: There exists a solution of Equation 2.22 that takes the form\nw = M∑ m=1 αmy mxm (4.2)\nProof: Via the structure of GURU Recall that the updates in the GURU algorithm are of the form\nw ← w − η√ t\n( −yixierf ( 1− yiwTxi\nσ‖w‖\n) +\nσw√ 2π‖w‖ exp\n( −(1− y iwTxi)2\n2σ2‖w‖2 )) It is suggestive to observe that the update formula can be split and written as two successive steps. The first of which is\nw ← w − η√ t σw√ 2π‖w‖ exp\n( −(1− y iwTxi)2\n2σ2‖w‖2 ) followed by\nw ← w + η√ t yixierf\n( 1− yiwTxi\nσ‖w‖\n) (4.3)\nThe first step is nothing else then a rescailing of the weights vector\nw = γw, γ = 1− η√ t σ√ 2π‖w‖ exp\n( −(1− y iwTxi)2\n2σ2‖w‖2\n) (4.4)\nRecall that GURU initializes the weight vector as w = 0, which clearly can be represented as\n0 = M∑ m=1 0ymxm (4.5)\nWe thus assume that the desired representation exists, and proceed by induction. By plugging the representation into the previous equations, we get\nM∑ m=1 αnewm y mxm = M∑ m=1 αmy mxm + γ M∑ m=1 αmy mxm\ni.e. for all m αnewm = (1 + γ)αm (4.6)\nwhere αnewm is the result of thee respective update. The second step in the update formula (Equation 4.3), may be written as\nM∑ m=1 αnewm y mxm = M∑ m=1 αmy mxm + µiy ixi, µi = η√ t yixierf ( 1− yiwTxi σ‖w‖ ) i.e.\nαnewm = { αm if m 6= i αi + µi if m = i\n(4.7)\nCombining both steps, we end up with the following update rule:\nαt+1m = { γαtm if m 6= i γαti + µi if m = i\n(4.8)\nSince GURU is guranteed to converge to the optimum, by taking t → ∞ we establish the desired result.\nProof: Via the dual formulation We have already seen (Equation 3.4) that\nσλ w ‖w‖ = ∑ m αmy mxm\nBy defining α̃m = ‖w‖ σλ αm and plugging it into the previous equality, we conclude that w = ∑ m α̃my mxm\nas required.\nProof: Via the general representer theorem Set Ω ≡ 0, L((f(x1), f(x2), . . . , f(xn))) = ∑n i=1 f(xi), f = ` rob hinge. and let κ be the linear kernel κ(x1, x2) = xT1 x2. The desired result stems immidiately from Theorem 1.1."
    }, {
      "heading" : "2. KEN-GURU: A primal kernelized version of GURU",
      "text" : "In the pevious section we have established a representer result for GURU. The next step in the derivation is to work the components of the algorithm, so the only dpendence on the data samples and on the classifier would be via dot products. That being the case, we can apply the kernel trick, namely to replace each dot product (xm)Txn with the kernel entry κ(xm,xn) (for details see, for example, Aizerman et al. [1964]; Schölkopf and Smola [2002]). We start by expanding the quantities that appear in the update formula in terms of αm’s. Then, we introduce a method to update the value of the norm variable in a computationally cheap way. We conclude the section by putting the results together, and prsenting the KEN-GURU (KErNelized GaUssian RobUst) algorithm.\nIn order to compute γ and µi of Equation 4.4 and Equation 4.7, one must know the values of wTxi and ‖w‖. Let us expand the first quantity\nwTxi = ( M∑ m=1 αmy mxm )T xi\n= M∑ m=1 αmy m(xm)Txi\n= M∑ m=1 αmy mKmi\nThe norm might be computed as\n‖w‖2 = wTw = ( M∑ m=1 αmy mxm )T M∑ n=1 αny nxn\n= M∑ m=1 M∑ n=1 αmαny myn(xm)Txn\n= M∑ m=1 M∑ n=1 αmαny mynKmn\nNote that the Gram matrix K may be precomputed and cached (total cost of O(M2)). Thus,wTxi can be computed inO(M), and ‖w‖ inO(M2). As both of these values should be computed for each update, the cost of the norm computation is extremely expensive. Instead of computing the norm each time from scratch, it is possible to use its previous value. The updated norm may be computed as\n‖w‖2t+1 = M∑ m=1 M∑ n=1 αt+1m α t+1 n y mynKmn\n= M∑ m=1 [∑ n 6=i αt+1m α t+1 n y mynKmn + α t+1 m α t+1 i y myiKmi ]\n= M∑ m=1 ∑ n6=i αt+1m α t+1 n y mynKmn + M∑ m=1 αt+1m α t+1 i y myiKmi\n= ∑ n6=i [∑ m6=i αt+1m α t+1 n y mynKmn + α t+1 i α t+1 n y iynKin ]\n+ M∑ m=1 αt+1m α t+1 i y myiKmi\n= ∑ n6=i ∑ m 6=i αt+1m α t+1 n y mynKmn + ∑ n6=i αt+1i α t+1 n y iynKin\n+ ∑ m 6=i αt+1m α t+1 i y myiKmi + α t+1 i α t+1 i y iyiKii\n= ∑ n6=i ∑ m 6=i αt+1m α t+1 n y mynKmn + 2 ∑ m 6=i αt+1m α t+1 i y myiKmi\n+αt+1i α t+1 i y iyiKii\nBy plugging Equation 4.8 we get\n‖w‖2t+1 = γ2 ∑ n6=i ∑ m 6=i αtmα t ny mynKmn + 2γ ∑ m 6=i αtm(γα t i + µi)y myiKin\n+(γαti + µi) 2Kii\n= γ2‖w‖2t + 2γµiyi M∑ m=1 αtmy mKmi + µ 2 iKii = γ2‖w‖2t + 2γµiyiwTxi + µ2iKii\nwhere wTxi is computed regardless of ‖w‖2. Thus, the value of the norm can be maintained in O(1).\nIn may be easily observed that the data samples xm participate in the computations of the update only via the Gram matrix K. Thus, we can apply the kernel trick, and use\nKij = κ(x i,xj) (4.9)\nfor any Mercer Kernel κ. Based on the results established in the previous sections, we may translate GURU into a kerenlized version, named KEN-GURU. We intoduce an auxilliary variable ζ , that holds the value of the product κ(w,xi) and is evaluated by\nζt+1 = M∑ m=1 αtmy mK(xm,xi) (4.10)\nAccording to Equation 4.4, Equation 4.7 and Equation 4.9 we introduce the following update formulas\nγt+1 = 1− η√ t σ√ 2πνt exp\n( −(1− y iζt+1) 2\n2σ2ν2t\n) (4.11)\nµt+1 = η√ t erf ( 1− yiζt+1 σνt ) (4.12)\nνt+1 = √ γ2t+1ν 2 t + 2γt+1µt+1y iζt+1 + µ2t+1Kii (4.13)\nAlgorithm 2: KEN-GURU(κ,S,η0, ) Data: Kernel function κ, training set S , learning rate η0, accuracy Result: α //initializations forall m,n = 1..m do\nKmn = κ(x m,xn)\nend α0 ← 0; ν0 ← 0; t← 0; while ∆L ≥ do\n//randomize a sample i← rand(M); //evaluate coefficients Compute ζt+1 (Equation 4.10); Compute γt+1 (Equation 4.11); Compute µt+1 (Equation 4.12);\n//update alphas αt+1 ← γt+1αt; αit+1 ← αit+1 + µt+1; t← t+ 1;\nend return α;\nThe correctness of the algorithm stems directly from that of GURU."
    }, {
      "heading" : "3. Experiments",
      "text" : "In this section we present experimental results regarding the performance of KEN-GURU. We show how σ affects the learned classifier and then compare KEN-GURU to SVM on USPS pairs and on the Ionosphere database (see Table 2.1 for details). For the USPS tasks, a polynomial kernel of degree 2 was used and for Ionosphere, RBF with γ = 1. The results are summarized in Table 4.1.\nConsider Figure 4.1, in which KEN-GURU classifiers trained for various values of the parameter σ with a polynomial kernel of degree 2 are presented. The toy probelm was synthesized by first generating uniformly points on [−7.5, 7.5]× [−7.5, 7.5]. Points which fall within the ball of radius 2 around the origin were assigned a positive label. Points which are more distant from the origin than 3.5 units were taken as negative examples. Points which fell in between were dropped. Observe that increasing σ puts extra emphasis on the number of samples in each class. Specifically, in the problem at hand, there are much more points outside the circle than inside. When σ is rather small, the training is ’local’ in the sense that each sample governs what happens in its immediate environment. On the contrary, when σ is relatively big, the emphasis is on global tendencies.\nOn the Ionosphere databse, KEN-GURU performs significantly better than SVM. Recall that the outperformance of GURU on SVM in this case is consistent with the performance in the case of a linear kernel. This behavior is explained by the noisy nature of the Ionosphere database. For the USPS couples, KEN-GURU’s performance is pretty similar to that of SVM.\nChapter 5\nThe Multiclass Case\nIn the previous chapters we have developed the binary algorhtm GURU, and its kernelized version KEN-GURU. In this chapter we will analyze anotther extension of the algorithm, for the case of multiclass cases.\nThe ideas that were presented in Chapter 2 may be generalized for the multi-class case. To that end, we first should generalize the loss function we are working with. This goal is acheived by solving the generalized problem of the adversarial choice. After establishing this reuslt we devise the effective robust loss function, and devise an optimization algorithm for it.\nWe relax the problem twice in order to solve it. First, we work with the sum-of-hinges loss function (Weston and Watkins [1999]). In addition, we use a superset of noise distribution, that contains all covariance matrix with a bounded maximal eigenvalue. By the end of the chapter we will prove that for the binary case the maximal eigenvalue and trace constraint give the same result.\nThe setting we address in the followings is of data drawn from X = Rd, accompanied by labels drawn from Y = {1, 2, . . . , C}. The learning task is to train the weight vectors w1,w2, . . . ,wC . The target classifier is φ : X → Y , defined by\nφ(x;w1,w2, . . . ,wC) = max y∈Y\n[ wTy x ] (5.1)"
    }, {
      "heading" : "1. Problem formulation",
      "text" : "In this section we formally describe the generalization of the learning task from the binary to the multiclass case. We show that the generalization culminates in a loss function which is the sum of several appropriate binary losses.\nIn Chapter 2 we have started our derivation from the hinge loss\n`hinge(x m, ym;w) = [1− ymwTx]+ (5.2)\nThe most common generalization of the hinge loss to the multi-class case is\n`mult(x m, ym;w1, . . . ,wC) = max\ny\n[ wTy x m −wTymxm + δy,ym ]\n(5.3)\nHowever, this loss function is not applicable in our framework (see Appendix C). Instead, we suggest to minimize the following surrogate loss function (Weston & Watkins, e.g. ref):\n`sum(x m, ym;w1,w2, . . . ,wC) = ∑ i 6=y [ 1− (wym −wi)Txm ] +\n(5.4)\nwhich is a surrogate to the zero-one loss. Let us write down the formulation of the problem in this case:\nmin w1,w2,...,wC ∑ m max Σ∈Γβ En∼N (0,Σ) ∑ y′ 6=ym [ 1− (wym −wy′)T (xm + n) ] +\n(5.5)\nwhere Γβ = {Σ ∈ PSD ∣∣∣ρ(Σ) ≤ β} (5.6) and ρ is the spectral norm of a matrix, defined by\nρ(A) = √ λmax(A∗A)\nUsing this set we constrain the maximal power of noise that the adversary may spread in each primary direction."
    }, {
      "heading" : "2. The adversarial choice",
      "text" : "In the followings we will focus on deriving the adversarial choise for the problem at hand. It appears that in the current setup, the solution is simpler than the one we had in Chapter 2."
    }, {
      "heading" : "2.1 Applying a spectral norm constraint",
      "text" : "Let us investigate what is the adversary’s optimal way for spsreading the noise. The ideas of the development are similar to that of Theorem 2.1.\nDenote ∆W y,y′ = wy −w′y (5.7)\nUsing the same procedure we have employed in the binary case (see Section 2.1 and Equation 2.15 thereby) we can write Equation 5.5 as:\nmin w1,w2,...,wC ∑ m max Σ∈Γβ ∑ y′ 6=ym L ( xm,+1; ∆W ym,y′ ,∆W T ym,y′Σ∆W ym,y′ ) (5.8)\ni.e. the task at hand is to optimize the effective loss function\n`robsum(x m, ym;w1,w2, . . .wC , β) = max\nΣ∈Γβ ∑ y′ 6=ym L ( xm,+1; ∆W ym,y′ ,∆W T ym,y′Σ∆W ym,y′ ) (5.9)\nObserve that in every appearance of `robhinge, the label y m was replaced with +1. The reason for this change is that we are classifying using the weight vector wym −wy′ . That is, our prediction is\n(wym −wy′)Txm = wTymxm −wTy′xm\nOur objective is, of course, to have wTymx m > wTy′x m, which corresponds to the label +1. The next theorem specifies the adversarial choice of the covariance matrix Σ, and is the multi-class analog of Theorem 2.1:\nTheorem 2.1: The optimal Σ in Equation 5.9 is given by Σ∗ = βI .\nProof: In Lemma 2 we have shown that L is monotone increasing in its 4th argument. By the Cauchy-Schwartz inequality we have that\n∆W Tym,y′Σ∆W ym,y′ ≤ β‖∆W ym,y′‖2 (5.10)\nOn the other hand, it holds that for all y′\n∆W Tym,y′βI∆W ym,y′ = β‖∆W ym,y′‖2 (5.11)\nhence this upper bound is attained for all C − 1 summands cuncurrenlty with Σ = βI . The geometric interpertation of this result is that under the spectral norm constraint, the adversary will choose to spread the noise in an isothropic fashion around the sample point.\nWe thus get the following optimization problem:\nmin w1,w2,...,wC ∑ m ∑ y′ 6=ym L(xm,+1; ∆W ym,y′ , β‖∆W ym,y′‖2) (5.12)\nApplying the same terminology used in the binary case, we have:\nmin w1,w2,...,wC ∑ m ∑ y′ 6=ym `robhinge(x m,+1; ∆W ym,y′ , β) (5.13)\nand Equation 5.9 equals\n`robsum(x m, ym;w1, w2, ..., wC , β) = ∑ y′ 6=ym `robhinge(x m,+1; ∆W ym,y′), β (5.14)"
    }, {
      "heading" : "2.2 The connection to the trace constraint",
      "text" : "It is interesting to examine the reduction of the multiclass loss we have derived, to the binary case. Note that since we have used a substantially larger matrix collection, there is no apriori reason to expect that the results will coincide.\nTaking C = 2 brings us back to the binary case. We use w+1, w−1 for the weight vectors of the classes. By expanding Equation 5.9, we get\n`robsum(x m, ym;w+1,w−1, β) = ` rob hinge(x m,+1;wym −w−ym , β) (5.15)\nIf we take w = w+1 −w−1, we end up with\n`robsum(x m, ym;w+1, w−1, β) = ` rob hinge(x m, ym;w, β) (5.16)\nIt is interesting to observe that the resulting loss functions are identical, even though the constraints we put on the convariance matrices are different. In order to explain this phenomenon, let us go back the geometric intuition that we have given prior to the proof of Theorem 2.1.\nConsider Figure B.1, which presents a visualization of Λβ and Γβ in the 2-dimensional case. What we have shown in Theorem 2.1, is that the multiclass adversary will choose the point (1, 1). Under the trace constraint, however, the adversary will have to choose either (1, 0), (0, 1), or any other point lying on the line connecting them. Our geometric intuition says that all the power that was not spread perpendicularly to the separating hyperplane is irrelevant. Thus, when the adversary has to choose a directional noise, he would take the perpendicular direction. On the other hand, if we limit his action axis-wise (and not overall), he will surely choose to spread the noise equally over all of the axes."
    }, {
      "heading" : "3. M-GURU: a primal algorithm for the multiclass case",
      "text" : "In the following we generalize GURU (that was presented in Section 4) for the multiclass case. As a direct corrolary of the results presnted in previous chapters, we have that our loss function in this case is strictly-convex. Thus, we turn to devise an SGD procedure.\nWe shall begin by computing the gradient of `robsum(x m, ym;w1,w2, . . . ,wC , β). For\nconvenience, we write it in terms of the binary loss function `robhinge:\n∇wr`robsum(xm, ym ; w1,w2, . . . ,wC , β) = ∑ y′ 6=r∇w`robhinge(xn,+1;w, β) ∣∣∣ w=wym−wy′ if r = ym\n−∇w`robhinge(xn,+1;w, β) ∣∣∣ w=wym−wr\notherwise\nFollowing the considerations that we have introduced in Section 4, we devise an SGD procedure for the minimization task:\nAlgorithm 3: M-GURU(S,η0, ) Data: Training set S , learning rate η0, accuracy Result: w w ← 0; while ∆L ≥ do\nm← rand(M); for y′ ∈ {1, 2, . . . , C} do wy′ ← wy′ − η0√t∇wy′ ` rob sum(x\nm, ym;w1,w2, ...,wC , β); end\nend return w;\nIn Algorithm 3, the notion of stochastic gradient was applied once, to the extent that our updates depend on a single sample in each iteration. It may be applied again, however. Instead of updating all the weight vectors concurrently, one might randomize which vector to update, as well. The resulting algrithm is\nAlgorithm 4: M-GURU-S2(S,η0, ) Data: Training set S , learning rate η0, accuracy Result: w w ← 0; while ∆L ≥ do\nm← rand(M); y′ ← rand(C) wy′ ← wy′ − η0√t∇wy′ ` rob sum(x\nm, ym;w1,w2, ...,wC , β); end return w;"
    }, {
      "heading" : "4. Experiments",
      "text" : "M-GURU and M-GURU-S2 were tested on toy problems, USPS and a couple of UCI databases (Frank and Asuncion [2010]). The datasets are detailed in Table 5.1. In Toy-3 and Toy-4 each class is a Gaussian distribution. These problems are visualized in Figure 5.3. The rsults are summarized in Table 5.2.\nObserve that the performance of M-GURU is similar to that of SVM. Nontheless, it should be noted that SVM slightly outperforms M-GURU. This difference is explained by the fact that M-GURU is based on the sum-of-hinges loss function, which is a looser surrogate of the zero-one loss than the SVM multi-hinge loss function. We have tested the relative performance of M-GURU and M-GURU-S2 on the toy-3 dataset.\nWe observe that M-GURU outperforms the S2 variant. Our experiments show that the empirical behavior of the classifiers stabilizes a significant time before the optimization process converges. Thus, M-GURU-S2 may be used to learn classifiers more quickly.\nChapter 6\nDiscussion"
    }, {
      "heading" : "1. Contribution",
      "text" : "In this work we presented a new robust learning framework. In our framework we minimize the expected loss over a spreading of the sample points. Each displacement is assumed to take place with a probability that depends on its distance from the original point. Thus, we effectively replace each point with a fading cloud.\nWe have analyzed the case of Gaussian noise distribution, where the underlying loss measure is the hinge-loss. In this case, we have shown that the resulting effective loss function is a smooth strictly-convex upper-approximation of the hinge-loss, denoted `robhinge. One of the main advantages of this loss function, is its parameter σ that has a clear meaning: the variance of the noise that contaminates the data. Similarly to SVM, our algorithm, named GURU, depends on a single parameter. A significant difference is the ability to assign a value to this parameter. In the case of SVM, for a long time all that was known on this parameter is that it controls the tradeoff between the training error and the margin of the classifier. Xu et al. [2009] have shown that SVM is equivalent to a robust formulation in which the parameter corrsponds to the radius of a rigid ball in which the sample point may be displaced. This result, however, relates the parameter with the entire data set. Thus, it is still difficult to tune it. In our method, σ is the magnitude of noise that possibly corrupts each sample point, hence it might be evaluated from physical consideration, such as the process that generates the data, etc. Without putting extra effort, we are able to point out an alternative explanation for non-regularized SVMs lack of ability to generalize. We have shown that as σ tends to 0, `robhinge coincides asymptotically with the hinge loss. Thus, non-regularized SVM may be understood as not trying to acheive robustness to perturbations, hence it tends to overfit the data. We have shown that `robhinge may be written as a perspective of a smooth loss function (denoted f ), where the scaling factor is σ‖w‖. This representation suggests that the robust framework we have developed introduces a multiplicative regularization. Using both this representation we have derived a dual problem. The dual formulation depends on the actual loss function f only via its conjugate dual. Thus, it is possible to plug into the same formulation some other losses that follow certain conditions. In particular, as we have demonstrated in Chapter 3, there is\na tight connection between approximations of the loss function and relaxations of the dual problem. We believe that applying the same technique we have apllied here to other loss functions will result in new robust learning algorithms. The connection between the primal loss and the resulting dual shold be investigated more throughly. The algorithmic approach we have taken in this work is rather simplistic. Due to the fact that our objective is strictlyconvex, many off-the-shelf convex optimization algorithms may be used. Our method of choice was stochastic gradient descent. Furthrmore, if there is a bound on the norm of the optimal classifier (as in SVM. see Shalev-Shwartz et al. [2007a] for details), it is probably possible to use it in order to achieve even faster algorithms. Specifically, subject to such a bound, we may restrict the optimization problem to a ball around the origin. In this ball, it is possible that our loss function is strongly-convex,hence it can be optimized using more aggressive procedure (Shalev-Shwartz and Kakade [2008]). Our generalization to Mercer kernels, is done based on the primal formulation. In order to compute the updates fast (O(M)), we have shown how to maintain the value of the norm of the classifier in O(1) based on pre-computed values. This technique may be employed in Pegasos, e.g, in order to perform the projection step efficiently."
    }, {
      "heading" : "2. Generalizations",
      "text" : "The framework we have introduced may be generalized in couple of interesting directions. Obviously, various families of noise distributions may be plugged into the model. One particularly interesting is the class of all probabilty distributions having a specific first and second moment. Vandenberghe et al. [2007] have shown that the probability of a set defined by quadratic inequalities may be computed using semidefinite programming. In addition, they have shown that the optimum is acheived over a discrete probability distribution. We conjecture that a similar technique may be employed in our case, in order to show that the optimum of the loss expectation is attained over a discrete distribution. In addition, the same framework can be used in order to explore more convex perturbations. For example, in the field of computer vision it is possible to assume that the adversary rotates or translates the sample, and that the distribution of these perturbations is chosen adversely. In order to make this practical, it is crucial to understand in which cases the integration and integration of the loss are possible.\nRegarding the theoretical aspects of this work, it still remains to show how to derive performance bounds for the introduced framework. In particular, it is interesting to understand what kind of gurantees can be derived for the general perspective-optimization framework we have discussed."
    }, {
      "heading" : "Appendix A",
      "text" : "Single-Point Algorithms\nThe object of this work is to learn classifiers that are robust to noise. As discussed, a possible way to achieve this goal is by applying an adversarial framework. The most important issue in this case is designing an effective adversary. While in the previous chapters of the work we explored more sophisticated adversaries, it is nice to end the journey with a rather simple mathematical formulation. The binary version of the algorithms was extensively studied. We review the result here for the sake of a complete presentation. A simple generalization for the multiclass case is presented subsequently."
    }, {
      "heading" : "1. Problem presentation",
      "text" : "Maybe the simplest action that the adversary can take at test-time is displacing a test point, in such a way that will cause this point to be missclassified. If we limit the freedom given to the adversary, it might not be able to corrupt the classification of the point, but rather only reduce the associated confiedence. The model that we will explore in the followings grants the adversary the ability to displace a sample point within a ball centered at the original point. In order for the learned classifier to be robust to such displacements, we should modify the objective of the learning task. In the following we present and anlyze one way to do it, by optimizaing the worst-case scenario:\nmin w max ‖∆xm‖≤δ: m=1..M\nλ 2 ‖w‖2 + M∑ m=1 [ 1− ymwT (xm + ∆xm) ] +\n(A.1)\nThis formulation has an additive structure, in which each term ∆xm appears exactly once. We use these properties in order to decouple the optimization problem. The learning task at hand in this case is thus\nmin w\nλ 2 ‖w‖2 + M∑ m=1 max ‖∆xm‖≤δ [ 1− ymwT (xm + ∆xm) ] +\n(A.2)\nRecall that in the general SVM setting, one tries to minimize the hinge loss:\n`hinge(x, y;w) = [1− ywTx]+ (A.3)\nEquation A.2 can be interpreted as optimizing the effective loss function\n`robhinge(x, y;w) = max‖∆x‖≤δ [1− ywT (x+ ∆x)]+ (A.4)\nWe say that this loss function is robust, in the sense that it represents the worst-case loss subject to the potential action of the adversary."
    }, {
      "heading" : "2. Computing the optimal displacement",
      "text" : "In order to derive a closed form for the loss function `robhinge, we should explore the nature of the adversarial choice in our model. Intuitively, the adversary will try to relocate the point to the wrong side of the seperating hyperplane. For this end, it is pointless to move the point along any axes not orthogonal to the seperating hyperplane. This idea is visualized in Figure A.1. We will now prove this simple theorem:\nTheorem 2.1: The optimum of the maximization in Equation A.4 is acheived at xopt = x− δ w‖w‖\nProof: First we observe that the function f(z) = [1 − z]+ is a monotone non-increasing function of its argument z. Thus, maximizing f(z) is equivalent to minimizing z. By the Cauchy-Schwartz inequality, we have that |ywT∆x| ≤ ‖w‖ · ‖∆x‖, with equality iff ∆x is proportional to w. Therefore, the minimal value possible is attained at ∆xopt = −δ w‖w‖ . We conclude that xopt = x− δ w‖w‖ as claimed.\nPlugging the result of the theorem above into Equation A.4 we end up with\n`robhinge(x, y;w) = [1− ywTx+ δ‖w‖]+ (A.5)"
    }, {
      "heading" : "3. ASVC: Adversarial Support Vector Classification",
      "text" : "The fact that Equation A.4 has a simple closed-form solution allows us to employ the algorithmic scheme of alternating optimization for Equation A.1. The structure of the algorithm is quite simple:\n1. Alternately:\n(a) Optimize for w\n(b) Optimize for ∆x1, ∆x2,..., ∆xM\nUntil convergence.\nNotice that 1a is nothing more than an SVM taking the displaced points as input. Furthermore, 1b has a closed-form solution as we have proved in Theorem 2.1. Thus, to solve for the optimal classifier, any off-the-shelf SVM solver can be used. We end up with Algorithm 5.\nAlgorithm 5: ASVC(S, δ, λ,T ,k) Data: Training set S , radius δ, tradeoff λ Result: The weight vector w w ← 0; repeat\n∆xm ← −δ w‖w‖ ; S̃ ← {xm + ∆xm}xm∈S ; w ← solveSVM(S̃, λ)\nuntil convergence ; return w;"
    }, {
      "heading" : "4. The Multiclass Case",
      "text" : "Pretty similar ideas can be adopted in order to generalize ASVC for the multiclass case.\nThe multi-hinge loss is defined as\n`mult(x m, ym;w1,w2, ...,wC) = max\ny=1,2,...,C\n[ δy,ym − (wym −wy)Txm ] (A.6)\nUsing the notions of the previous section, we define\n`singlemult (x m, ym;w1,w2, ...,wC) = max\n‖∆x‖≤δ max y=1,2,...,C\n[ δy,ym − (wym −wy)T (xm + ∆x) ] (A.7)\nNote the order of maximization can be changes, i.e.\n`singlemult (x m, ym;w1,w2, ...,wC) = max\ny=1,2,...,C max ‖∆x‖≤δ\n[ δy,ym − (wym −wy)T (xm + ∆x) ] (A.8)\nApplying a slight variation of Theorem 2.1, we conclude with\n`singlemult (x m, ym ; w1,w2, ...,wC)\n= max y=1,2,...,C\n[ δy,ym − (wym −wy)T ( xm − δ wy\nm −wy ‖wym −wy‖ )] = max\ny=1,2,...,C\n[ δy,ym − (wym −wy)Txm + δ‖wym −wy‖ ]"
    }, {
      "heading" : "5. Related work",
      "text" : "Our ASVC algorithm is a mirror reflection of TSVC presented in (Bi & Zhang, NIPS04). TSVC performs alternating optimization, each time replacing the set of training samples with {xi + yiδi w‖w‖}, which are more distant from the separator (thus, easier to classify). The idea there is to address the case in which noisy data distracts the classifier, by using the shifted training sets."
    }, {
      "heading" : "Appendix B",
      "text" : "Diagonal Covariance\nIn this appendix we discuss the case in which the adversary is constrained to choose a diagonal covariance matrix. This setting corresponds to the case when the noise is alligned to the primary axes. In this case we are able to give a closed form analytical result, subject to a bounded trace constraint on the covariance matrix.\nThe adversarial choice problem can can be written\nmax Σ=diag(a1,a2,...,ad) tr(Σ)≤β\nL(xm, ym;w,wTΣw) (B.1)\nLet us expand\nwTΣw = wTdiag(a1, a2, . . . , ad)w\n= d∑ i=1 aiwi 2 = aTw·2\nwhere w·2 represents the coordinate-wise product of w with itself. Let i∗ be the index of the maximal entry in w·2. It hold that\nwTΣw ≤ ∑ i βiw 2 i∗ (B.2)\nUsing the same argumentation as in Chapter 2, we conclude that the adversary will choose the covariance matrix\nΣ∗ = βei∗i∗ (B.3)\nwhere eij is the matrix having zeros in all of its entries beside (i, j), where it takes the value 1. The geometric meaning of this result is that the adversary will choose to spread the noise in a single direction, along the primary axis that creates the biggest angle with the separating hyperplane."
    }, {
      "heading" : "Appendix C",
      "text" : "Using the Multi-Hinge Loss\nThe most common generalization of the hinge loss for the multiclass case is the following loss function\n`mult(x m, ym;w1, . . . ,wC) = max\ny\n[ wTy x m −wTymxm + δy,ym ]\n(C.1)\n(see Crammer and Singer [2002]). In this appendix we point out some of the issues that made us choose to work with the sum-of-hinges loss function and not with the one above.\nIf we plug the multi-hinge loss into our framework, we get the following learning problem:\nmin w ∑ m max Σ∈S ∫ p(x̂|xm; Σ) max y [ wTy x̂−wTymx̂+ δy,ym ] dx̂ (C.2)\nDefine ∆wy,ym = wy −wym and write:\nmin w ∑ m max Σ∈S ∫ p(x̂|xm; Σ) max y [∆wy,ymx̂+ δy,ym ] dx̂ (C.3)\nAnd for Gaussian noise this is:\nmin w ∑ m max Σ∈S c|Σ|−0.5 ∫ e− 1 2 nTΣ−1n max y [∆wy,ymx m + ∆wy,ymn+ δy,ym ] dn (C.4)\nThe ability to understand the solution of the adversarial choice problem in this case, is connected to the ability to understand the expectation of the maximum of a set of normal random variables. This problem probably does not have an analytical solution (see Ross [2003]).\nUNIDIRECTIONAL NOISE\nIn another approach we have studied, we assumed an adversary that spreads the noise in a single direction. The motivation for this kind of adversary is the solution to the adversarial choice problem in the binary case.\nWe formulate the problem by letting the adversary to choose a unit length vector. Thus, in the case of unidirectonal noise, the task that the adversary faces is:\nmax v:‖v‖≤1 ∫ R Nz(0, σ2) max y [ ∆wTy,ymx m + ∆wTy,ymn+ δy,ym ] dz (C.5)\nThe integrand (excluding the pdf) is a piecewise linear function. The knees of this function as well as the slopes of the linear sections are strongly dependent on v. Nontheless, it is impossible to find a closed form solution for the position of the knees. Therefore, we find this direction inapplicable in our case, as well."
    } ],
    "references" : [ {
      "title" : "Theoretical foundations of the potential function method in pattern recognition learning",
      "author" : [ "M.A. Aizerman", "E.A. Braverman", "L. Rozonoer" ],
      "venue" : "In Automation and Remote Control,,",
      "citeRegEx" : "Aizerman et al\\.,? \\Q1964\\E",
      "shortCiteRegEx" : "Aizerman et al\\.",
      "year" : 1964
    }, {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "Shun-Ichi Amari" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Amari.,? \\Q1998\\E",
      "shortCiteRegEx" : "Amari.",
      "year" : 1998
    }, {
      "title" : "Incorporating prior information into support vector machines in the form of ellipsoidal knowledge",
      "author" : [ "Jean baptiste Pothin", "Cdric Richard" ],
      "venue" : null,
      "citeRegEx" : "Pothin and Richard.,? \\Q2008\\E",
      "shortCiteRegEx" : "Pothin and Richard.",
      "year" : 2008
    }, {
      "title" : "Local rademacher complexities",
      "author" : [ "Peter L. Bartlett", "Olivier Bousquet", "Shahar Mendelson" ],
      "venue" : "In Annals of Statistics,",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2002
    }, {
      "title" : "Convex analysis and optimization",
      "author" : [ "Dimitri P. Bertsekas", "Angelia Nedic", "Asuman E. Ozdaglar" ],
      "venue" : "Athena Scientific, Nashua,",
      "citeRegEx" : "Bertsekas et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bertsekas et al\\.",
      "year" : 2003
    }, {
      "title" : "Robust sparse hyperplane classifiers: Application to uncertain molecular profiling data",
      "author" : [ "Chiranjib Bhattacharyya", "L.R. Grate", "Michael I. Jordan", "Laurent El Ghaoui", "I. Saira Mian" ],
      "venue" : "Journal of Computational Biology,",
      "citeRegEx" : "Bhattacharyya et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bhattacharyya et al\\.",
      "year" : 2004
    }, {
      "title" : "A second order cone programming formulation for classifying missing data",
      "author" : [ "Chiranjib Bhattacharyya", "Pannagadatta K. Shivaswamy", "Alex J. Smola" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Bhattacharyya et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bhattacharyya et al\\.",
      "year" : 2004
    }, {
      "title" : "Support vector classification with input data uncertainty",
      "author" : [ "Jinbo Bi", "Tong Zhang" ],
      "venue" : "nips,",
      "citeRegEx" : "Bi and Zhang.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bi and Zhang.",
      "year" : 2004
    }, {
      "title" : "Training with noise is equivalent to tikhonov regularization",
      "author" : [ "Chris M. Bishop" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Bishop.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1994
    }, {
      "title" : "The Tradeoffs of Large Scale Learning",
      "author" : [ "Léon Bottou", "Olivier Bousquet" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bottou and Bousquet.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bottou and Bousquet.",
      "year" : 2008
    }, {
      "title" : "Large scale online learning",
      "author" : [ "Léon Bottou", "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "Bottou and LeCun.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bottou and LeCun.",
      "year" : 2003
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen Boyd", "Lieven Vandenberghe" ],
      "venue" : "URL http://www.stanford.edu/ ̃boyd/ cvxbook/",
      "citeRegEx" : "Boyd and Vandenberghe.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2004
    }, {
      "title" : "Training a support vector machine in the primal",
      "author" : [ "Olivier Chapelle" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Chapelle.,? \\Q2007\\E",
      "shortCiteRegEx" : "Chapelle.",
      "year" : 2007
    }, {
      "title" : "On the algorithmic implementation of multiclass kernelbased vector machines",
      "author" : [ "Koby Crammer", "Yoram Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Crammer and Singer.,? \\Q2002\\E",
      "shortCiteRegEx" : "Crammer and Singer.",
      "year" : 2002
    }, {
      "title" : "Regularization networks and support vector machines",
      "author" : [ "Theodoros Evgeniou", "Massimiliano Pontil", "Tomaso Poggio" ],
      "venue" : "In Advances in Computational Mathematics,",
      "citeRegEx" : "Evgeniou et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Evgeniou et al\\.",
      "year" : 2000
    }, {
      "title" : "Robust solutions to least-squares problems with uncertain",
      "author" : [ "Laurent El Ghaoui", "Herve Lebret" ],
      "venue" : null,
      "citeRegEx" : "Ghaoui and Lebret.,? \\Q1997\\E",
      "shortCiteRegEx" : "Ghaoui and Lebret.",
      "year" : 1997
    }, {
      "title" : "Nightmare at test time: robust learning by feature deletion",
      "author" : [ "Amir Globerson", "Sam T. Roweis" ],
      "venue" : "ICML, volume 148 of ACM International Conference Proceeding Series,",
      "citeRegEx" : "Globerson and Roweis.,? \\Q2006\\E",
      "shortCiteRegEx" : "Globerson and Roweis.",
      "year" : 2006
    }, {
      "title" : "Advances in Neural Information Processing Systems",
      "author" : [ "Daphne Koller", "Dale Schuurmans", "Yoshua Bengio", "Léon Bottou", "editors" ],
      "venue" : "Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Koller et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Koller et al\\.",
      "year" : 2008
    }, {
      "title" : "Convergence rate of incremental subgradient algorithms. In Stochastic Optimization: Algorithms and Applications, pages 263–304",
      "author" : [ "Angelia Nedic", "Dimitri Bertsekas" ],
      "venue" : null,
      "citeRegEx" : "Nedic and Bertsekas.,? \\Q2000\\E",
      "shortCiteRegEx" : "Nedic and Bertsekas.",
      "year" : 2000
    }, {
      "title" : "A kernel classifier for distributions",
      "author" : [ "Alexei Pozdnoukhov", "Samy Bengio" ],
      "venue" : null,
      "citeRegEx" : "Pozdnoukhov et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Pozdnoukhov et al\\.",
      "year" : 2005
    }, {
      "title" : "Useful bounds on the expected maximum of correlated normal variables",
      "author" : [ "Andrew M. Ross" ],
      "venue" : null,
      "citeRegEx" : "Ross.,? \\Q2003\\E",
      "shortCiteRegEx" : "Ross.",
      "year" : 2003
    }, {
      "title" : "Learning with kernels : support vector machines, regularization, optimization, and beyond. Adaptive computation and machine learning",
      "author" : [ "Bernhard Schölkopf", "Alexander J. Smola" ],
      "venue" : null,
      "citeRegEx" : "Schölkopf and Smola.,? \\Q2002\\E",
      "shortCiteRegEx" : "Schölkopf and Smola.",
      "year" : 2002
    }, {
      "title" : "Mind the duality gap: Logarithmic regret algorithms for online optimization",
      "author" : [ "Shai Shalev-Shwartz", "Sham M. Kakade" ],
      "venue" : "In Koller et al",
      "citeRegEx" : "Shalev.Shwartz and Kakade.,? \\Q2009\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Kakade.",
      "year" : 2009
    }, {
      "title" : "Pegasos: Primal estimated subgradient solver for svm",
      "author" : [ "Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro" ],
      "venue" : "In Zoubin Ghahramani, editor, ICML, volume 227 of ACM International Conference Proceeding Series,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2007
    }, {
      "title" : "Pegasos: Primal estimated subgradient solver for svm. 2007b. URL http://ttic.uchicago.edu/ ̃shai/ papers/ShalevSiSr07.pdf. A fast online algorithm for solving the linear svm in primal using sub-gradients",
      "author" : [ "Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2007
    }, {
      "title" : "Second order cone programming approaches for handling missing and uncertain data",
      "author" : [ "Pannagadatta K. Shivaswamy", "Chiranjib Bhattacharyya", "Alexander J. Smola" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Shivaswamy et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Shivaswamy et al\\.",
      "year" : 2006
    }, {
      "title" : "From regularization operators to support vector kernels",
      "author" : [ "Alexander Smola", "Bernhard Schlkopf", "Rudower Chaussee", "Bernhard Sch Olkopf" ],
      "venue" : "Advances in Neural information processings systems",
      "citeRegEx" : "Smola et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Smola et al\\.",
      "year" : 1998
    }, {
      "title" : "Generalized chebyshev bounds via semidefinite programming",
      "author" : [ "Lieven Vandenberghe", "Stephen Boyd", "Katherine Comanor" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Vandenberghe et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Vandenberghe et al\\.",
      "year" : 2007
    }, {
      "title" : "The Nature of Statistical Learning",
      "author" : [ "Vladimir Vapnik" ],
      "venue" : null,
      "citeRegEx" : "Vapnik.,? \\Q1995\\E",
      "shortCiteRegEx" : "Vapnik.",
      "year" : 1995
    }, {
      "title" : "Support vector machines for multi-class pattern recognition",
      "author" : [ "J. Weston", "C. Watkins" ],
      "venue" : "In Proceedings of the Seventh European Symposium On Artificial Neural Networks,",
      "citeRegEx" : "Weston and Watkins.,? \\Q1999\\E",
      "shortCiteRegEx" : "Weston and Watkins.",
      "year" : 1999
    }, {
      "title" : "Robust regression and lasso",
      "author" : [ "Huan Xu", "Constantine Caramanis", "Shie Mannor" ],
      "venue" : "In Koller et al",
      "citeRegEx" : "Xu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2009
    }, {
      "title" : "Robustness and regularization of support vector machines",
      "author" : [ "Huan Xu", "Constantine Caramanis", "Shie Mannor" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Xu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2009
    }, {
      "title" : "Modified logistic regression: An approximation to svm and its applications in large-scale text categorization",
      "author" : [ "Jian Zhang", "Rong Jin", "Yiming Yang", "Alexander G. Hauptmann" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2003
    }, {
      "title" : "In this appendix we point out some of the issues that made us choose to work with the sum-of-hinges loss function and not with the one above. If we plug the multi-hinge loss",
      "author" : [ "Crammer", "Singer" ],
      "venue" : null,
      "citeRegEx" : "Crammer and Singer,? \\Q2002\\E",
      "shortCiteRegEx" : "Crammer and Singer",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Indeed, it was established that the ordinary SVM formulation is equivalent to a robust formulation, in which an adversary may displace the training and testing points within a ball of pre-determined radius (Xu et al. [2009]).",
      "startOffset" : 207,
      "endOffset" : 224
    }, {
      "referenceID" : 25,
      "context" : "For a detailed review see Vapnik [1995]. A common solution for this problem is to add a regularization term to the objective of the minimization problem.",
      "startOffset" : 26,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : "From the statistical learning theory’s point of view, the regularization restricts the complexity of the model, and by that controls the difference between the training and testing error (Smola et al. [1998]; Evgeniou et al.",
      "startOffset" : 188,
      "endOffset" : 208
    }, {
      "referenceID" : 13,
      "context" : "[1998]; Evgeniou et al. [2000]; Bartlett et al.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "[2000]; Bartlett et al. [2002]).",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "In some cases, the training data and the testing data are sampled from different processes, which are similar to some extent but are not identical (Bi and Zhang [2004]).",
      "startOffset" : 148,
      "endOffset" : 168
    }, {
      "referenceID" : 24,
      "context" : "In the Robust-SVM model, the adversary may apply a bounded additive distrubance, by displacing a sample point within a ball around it (Shivaswamy et al. [2006]).",
      "startOffset" : 135,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "Globerson and Roweis [2006] assumed a different type of adversary.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "Indeed, first equivalence results have been established for learning problems other than classification more than a decade ago (Ghaoui and Lebret [1997]; Xu et al.",
      "startOffset" : 128,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "Indeed, first equivalence results have been established for learning problems other than classification more than a decade ago (Ghaoui and Lebret [1997]; Xu et al. [2008]; Bishop [1994]).",
      "startOffset" : 128,
      "endOffset" : 171
    }, {
      "referenceID" : 8,
      "context" : "[2008]; Bishop [1994]).",
      "startOffset" : 8,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "[2008]; Bishop [1994]). Recently, Xu et al. [2009] have proven the fact that the regularization employed by SVM is equivalent to a robust formulation.",
      "startOffset" : 8,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "For example, baptiste Pothin and Richard [2008] have warped the data points with ellipsoids.",
      "startOffset" : 22,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "For example, baptiste Pothin and Richard [2008] have warped the data points with ellipsoids. Pozdnoukhov et al. [2005] have shown how to train classifiers for distributions.",
      "startOffset" : 22,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "For example, baptiste Pothin and Richard [2008] have warped the data points with ellipsoids. Pozdnoukhov et al. [2005] have shown how to train classifiers for distributions. Similar to what we do in this work, they use tails of distributions in their derivation. Their work, however, treated each data class as a distribution, whereas in this work we attach a noise distribution for each data point separately. Bhattacharyya et al. [2004b]; Shivaswamy et al.",
      "startOffset" : 22,
      "endOffset" : 440
    }, {
      "referenceID" : 2,
      "context" : "For example, baptiste Pothin and Richard [2008] have warped the data points with ellipsoids. Pozdnoukhov et al. [2005] have shown how to train classifiers for distributions. Similar to what we do in this work, they use tails of distributions in their derivation. Their work, however, treated each data class as a distribution, whereas in this work we attach a noise distribution for each data point separately. Bhattacharyya et al. [2004b]; Shivaswamy et al. [2006] have employed second order cone programming (SOCP) methods in order to handle the uncertainty in the data.",
      "startOffset" : 22,
      "endOffset" : 466
    }, {
      "referenceID" : 2,
      "context" : "For example, baptiste Pothin and Richard [2008] have warped the data points with ellipsoids. Pozdnoukhov et al. [2005] have shown how to train classifiers for distributions. Similar to what we do in this work, they use tails of distributions in their derivation. Their work, however, treated each data class as a distribution, whereas in this work we attach a noise distribution for each data point separately. Bhattacharyya et al. [2004b]; Shivaswamy et al. [2006] have employed second order cone programming (SOCP) methods in order to handle the uncertainty in the data. Bhattacharyya et al. [2004a] have assumed stochastic clouds instead of discrete points, as we do, but they did not try to minimize the expectation of the loss function over the cloud.",
      "startOffset" : 22,
      "endOffset" : 602
    }, {
      "referenceID" : 2,
      "context" : "For example, baptiste Pothin and Richard [2008] have warped the data points with ellipsoids. Pozdnoukhov et al. [2005] have shown how to train classifiers for distributions. Similar to what we do in this work, they use tails of distributions in their derivation. Their work, however, treated each data class as a distribution, whereas in this work we attach a noise distribution for each data point separately. Bhattacharyya et al. [2004b]; Shivaswamy et al. [2006] have employed second order cone programming (SOCP) methods in order to handle the uncertainty in the data. Bhattacharyya et al. [2004a] have assumed stochastic clouds instead of discrete points, as we do, but they did not try to minimize the expectation of the loss function over the cloud. Instead, their idea was to incorporate the idea with the soft margin framework. Bi and Zhang [2004] have tried to learn a better classifier by presenting the learning algorithm ’more reasonable’ samples.",
      "startOffset" : 22,
      "endOffset" : 857
    }, {
      "referenceID" : 2,
      "context" : "For example, baptiste Pothin and Richard [2008] have warped the data points with ellipsoids. Pozdnoukhov et al. [2005] have shown how to train classifiers for distributions. Similar to what we do in this work, they use tails of distributions in their derivation. Their work, however, treated each data class as a distribution, whereas in this work we attach a noise distribution for each data point separately. Bhattacharyya et al. [2004b]; Shivaswamy et al. [2006] have employed second order cone programming (SOCP) methods in order to handle the uncertainty in the data. Bhattacharyya et al. [2004a] have assumed stochastic clouds instead of discrete points, as we do, but they did not try to minimize the expectation of the loss function over the cloud. Instead, their idea was to incorporate the idea with the soft margin framework. Bi and Zhang [2004] have tried to learn a better classifier by presenting the learning algorithm ’more reasonable’ samples. We elaborate on this model in Appendix A. Smooth loss function were studied by Zhang et al. [2003]; Chapelle [2007].",
      "startOffset" : 22,
      "endOffset" : 1060
    }, {
      "referenceID" : 2,
      "context" : "For example, baptiste Pothin and Richard [2008] have warped the data points with ellipsoids. Pozdnoukhov et al. [2005] have shown how to train classifiers for distributions. Similar to what we do in this work, they use tails of distributions in their derivation. Their work, however, treated each data class as a distribution, whereas in this work we attach a noise distribution for each data point separately. Bhattacharyya et al. [2004b]; Shivaswamy et al. [2006] have employed second order cone programming (SOCP) methods in order to handle the uncertainty in the data. Bhattacharyya et al. [2004a] have assumed stochastic clouds instead of discrete points, as we do, but they did not try to minimize the expectation of the loss function over the cloud. Instead, their idea was to incorporate the idea with the soft margin framework. Bi and Zhang [2004] have tried to learn a better classifier by presenting the learning algorithm ’more reasonable’ samples. We elaborate on this model in Appendix A. Smooth loss function were studied by Zhang et al. [2003]; Chapelle [2007]. Analysis of methods for Solving SVM and SVM-like problems using the primal formulation was done by Shalev-Shwartz et al.",
      "startOffset" : 22,
      "endOffset" : 1077
    }, {
      "referenceID" : 2,
      "context" : "For example, baptiste Pothin and Richard [2008] have warped the data points with ellipsoids. Pozdnoukhov et al. [2005] have shown how to train classifiers for distributions. Similar to what we do in this work, they use tails of distributions in their derivation. Their work, however, treated each data class as a distribution, whereas in this work we attach a noise distribution for each data point separately. Bhattacharyya et al. [2004b]; Shivaswamy et al. [2006] have employed second order cone programming (SOCP) methods in order to handle the uncertainty in the data. Bhattacharyya et al. [2004a] have assumed stochastic clouds instead of discrete points, as we do, but they did not try to minimize the expectation of the loss function over the cloud. Instead, their idea was to incorporate the idea with the soft margin framework. Bi and Zhang [2004] have tried to learn a better classifier by presenting the learning algorithm ’more reasonable’ samples. We elaborate on this model in Appendix A. Smooth loss function were studied by Zhang et al. [2003]; Chapelle [2007]. Analysis of methods for Solving SVM and SVM-like problems using the primal formulation was done by Shalev-Shwartz et al. [2007a]; Chapelle [2007].",
      "startOffset" : 22,
      "endOffset" : 1207
    }, {
      "referenceID" : 2,
      "context" : "For example, baptiste Pothin and Richard [2008] have warped the data points with ellipsoids. Pozdnoukhov et al. [2005] have shown how to train classifiers for distributions. Similar to what we do in this work, they use tails of distributions in their derivation. Their work, however, treated each data class as a distribution, whereas in this work we attach a noise distribution for each data point separately. Bhattacharyya et al. [2004b]; Shivaswamy et al. [2006] have employed second order cone programming (SOCP) methods in order to handle the uncertainty in the data. Bhattacharyya et al. [2004a] have assumed stochastic clouds instead of discrete points, as we do, but they did not try to minimize the expectation of the loss function over the cloud. Instead, their idea was to incorporate the idea with the soft margin framework. Bi and Zhang [2004] have tried to learn a better classifier by presenting the learning algorithm ’more reasonable’ samples. We elaborate on this model in Appendix A. Smooth loss function were studied by Zhang et al. [2003]; Chapelle [2007]. Analysis of methods for Solving SVM and SVM-like problems using the primal formulation was done by Shalev-Shwartz et al. [2007a]; Chapelle [2007].",
      "startOffset" : 22,
      "endOffset" : 1224
    }, {
      "referenceID" : 11,
      "context" : "Definition 3 Perspective of a function (from Boyd and Vandenberghe [2004] 3.",
      "startOffset" : 45,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "For a proof see Boyd and Vandenberghe [2004] 3.",
      "startOffset" : 16,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "Consider the following lemma (Boyd and Vandenberghe [2004] 3.",
      "startOffset" : 30,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "A convergence result for the algorithm stems from general properties of SGD that were studied extensively (see Shalev-Shwartz et al. [2007b]; Kivinen et al.",
      "startOffset" : 111,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "A convergence result for the algorithm stems from general properties of SGD that were studied extensively (see Shalev-Shwartz et al. [2007b]; Kivinen et al. [2003]; Zhang et al.",
      "startOffset" : 111,
      "endOffset" : 164
    }, {
      "referenceID" : 21,
      "context" : "A convergence result for the algorithm stems from general properties of SGD that were studied extensively (see Shalev-Shwartz et al. [2007b]; Kivinen et al. [2003]; Zhang et al. [2003]; Nedic and Bertsekas [2000]; Bottou and Bousquet [2008], e.",
      "startOffset" : 111,
      "endOffset" : 185
    }, {
      "referenceID" : 17,
      "context" : "[2003]; Nedic and Bertsekas [2000]; Bottou and Bousquet [2008], e.",
      "startOffset" : 8,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "[2003]; Nedic and Bertsekas [2000]; Bottou and Bousquet [2008], e.",
      "startOffset" : 36,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "It has been suggested that using the stochastic version yields better generalization performance in learning tasks (Amari [1998]; Bottou and LeCun [2003]).",
      "startOffset" : 116,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "It has been suggested that using the stochastic version yields better generalization performance in learning tasks (Amari [1998]; Bottou and LeCun [2003]).",
      "startOffset" : 116,
      "endOffset" : 154
    }, {
      "referenceID" : 17,
      "context" : "We therefore suggest the following SGD procedure Algorithm 1: GURU(S,η0, ) Data: Training set S , learning rate η0, accuracy Result: w w ← 0; while ∆L ≥ do m← rand(M); w ← w − η0 √ t ∇w` hinge(x, y;w, σ); end return w; For convergence results see Nedic and Bertsekas [2000]. For a full treatment, see Bertsekas et al.",
      "startOffset" : 247,
      "endOffset" : 274
    }, {
      "referenceID" : 4,
      "context" : "For a full treatment, see Bertsekas et al. [2003], chapter 8.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 30,
      "context" : "Nontheless, this result gives an experimental support to the theoretical result in Xu et al. [2009], where it was shown that the ordinary SVM formulation is equivalent to a robust formulation, in which the adversary is capable of displacing the data samples.",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "where g∗ is by definition the conjugate function of g (for details see Boyd and Vandenberghe [2004] 3.",
      "startOffset" : 71,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "If when substituting the dual optimum in the Lagrangian, there exists a unique primal feasible solution, then it must be primal optimal (see Boyd and Vandenberghe [2004], 5.",
      "startOffset" : 141,
      "endOffset" : 170
    }, {
      "referenceID" : 21,
      "context" : "For a proof and more details, see for example Schölkopf and Smola [2002]. As mentioned, we will discuss three techniques to establis the required result.",
      "startOffset" : 46,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "That being the case, we can apply the kernel trick, namely to replace each dot product (xm)xn with the kernel entry κ(x,x) (for details see, for example, Aizerman et al. [1964]; Schölkopf and Smola [2002]).",
      "startOffset" : 154,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "That being the case, we can apply the kernel trick, namely to replace each dot product (xm)xn with the kernel entry κ(x,x) (for details see, for example, Aizerman et al. [1964]; Schölkopf and Smola [2002]).",
      "startOffset" : 154,
      "endOffset" : 205
    }, {
      "referenceID" : 29,
      "context" : "First, we work with the sum-of-hinges loss function (Weston and Watkins [1999]).",
      "startOffset" : 53,
      "endOffset" : 79
    }, {
      "referenceID" : 30,
      "context" : "Xu et al. [2009] have shown that SVM is equivalent to a robust formulation in which the parameter corrsponds to the radius of a rigid ball in which the sample point may be displaced.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 22,
      "context" : "see Shalev-Shwartz et al. [2007a] for details), it is probably possible to use it in order to achieve even faster algorithms.",
      "startOffset" : 4,
      "endOffset" : 34
    }, {
      "referenceID" : 22,
      "context" : "In this ball, it is possible that our loss function is strongly-convex,hence it can be optimized using more aggressive procedure (Shalev-Shwartz and Kakade [2008]).",
      "startOffset" : 130,
      "endOffset" : 163
    }, {
      "referenceID" : 27,
      "context" : "Vandenberghe et al. [2007] have shown that the probability of a set defined by quadratic inequalities may be computed using semidefinite programming.",
      "startOffset" : 0,
      "endOffset" : 27
    } ],
    "year" : 2011,
    "abstractText" : "Supervised learning is all about the ability to generalize knowledge. Specifically, the goal of the learning is to train a classifier using training data, in such a way that it will be capable of classifying new unseen data correctly. In order to acheive this goal, it is important to carefully design the learner, so it will not overfit the training data. The later can be done in a couple of ways, where adding a regularization term is probably the most common one. The statistical learning theory explains the success of the regularization method by claiming that it restricts the complexity of the learned model. This explanation, however, is rather abstract and does not have a geometric intuition. The generalization error of a classifier may be thought of as correlated with its robustness to perturbations of the data. Namely, if a classifier is capable of coping with distrubance, it is expected to generalize well. Indeed, it was established that the ordinary SVM formulation is equivalent to a robust formulation, in which an adversary may displace the training and testing points within a ball of pre-determined radius (Xu et al. [2009]). In this work we explore a different kind of robustness. We suggest changing each data point with a Gaussian cloud centered at the original point. The loss is evaluated as the expectation of an underlying loss function on the cloud. This setup fits the fact that in many applications, the data is sampled along with noise. We develop a robust optimization (RO) framework, in which the adversary chooses the covariance of the noise. In our algorithm named GURU, the tuning parameter is the variance of the noise that contaminates the data, and so it can be estimated using physical or applicative considerations. Our experiments show that this framework generates classifiers that perform as well as SVM and even slightly better in some cases. Generalizations for Mercer kernels and for the multiclass case are presented as well. We also show that our framework may be further generalized, using the technique of convex perspective functions.",
    "creator" : "LaTeX with hyperref package"
  }
}