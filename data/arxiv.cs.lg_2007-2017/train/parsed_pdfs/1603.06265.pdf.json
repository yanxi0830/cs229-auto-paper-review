{
  "name" : "1603.06265.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Robust Collaborative Online Learning",
    "authors" : [ "Paul Christiano" ],
    "emails" : [ "paulfchristiano@eecs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 3.\n06 26\n5v 1\n[ cs\n.L G\nWe present a robust collaborative algorithm for prediction with expert advice, which guarantees that every subset of users H perform nearly as well as if they had shared all of their data, and ignored all data from users outside of H. This algorithm limits the damage done\nby the dishonest users to O (√ T ) , compared to the O (T ) we would\nobtain by naively aggregating data. We also extend our results to general online convex optimization. The resulting algorithm achieves low regret, but is computationally intractable. This demonstrates that there is no statistical obstruction to generalizing robust collaborative online learning, but leaves the design of efficient algorithms as an open problem."
    }, {
      "heading" : "1 Introduction",
      "text" : "Modern machine learning systems often aggregate data from many users. This facilitates rapid learning, but leaves these systems vulnerable to manipulation by malicious users. Traditional results in learning theory make very weak guarantees about robustness to manipulation.\nThis is not an abstract concern. Machine learning systems are used to make a range of economically significant decisions, from aggregated ratings that shape what we buy to search rankings that shape what we read, and the incentives to manipulate these systems can be very strong.\nWe consider a collaborative version of prediction with expert advice, a fundamental problem in online learning. In our collaborative formulation, each round involves a particular user ut, who can make a decision based on all of the information reported by users in previous rounds.\nWe would like to guarantee that every set of users performs relatively well. That is, if we choose an arbitrary subset H of users, and restrict our attention to the set of rounds involving a user ut ∈ H , we would still like to compete with the best fixed expert. This ensures that adding additional malicious or varied users can’t significantly reduce the performance of existing users.\nThe chief difficulty is achieving this bound for every set H simultaneously. We are not only ignorant of what expert will perform best, we are ignorant of what payoffs we actually care about.\nWe show how to transform any traditional algorithm for prediction with expert advice into a robust collaborative version. This transformation ensures that the original algorithm’s regret bound applies for every subset of users H . It does this at the expense of some additional regret—if there are\nN users in total, the additional regret is at most O (√ TN ) . The additional\nregret improves significantly when most users are honest, or when few users are; we present precise statements in Section 3.\nThe resulting algorithms automatically compete with the best strategy which divides the honest users into m groups and makes a different recommendation for each group. These bounds may be of interest even in domains where there is no adversarial behavior.\nWe extend our results to general online convex optimization, at the expense of computational tractability. This result indicates that it is statistically possible to implement very efficient robust collaborative learning in a very broad range of settings."
    }, {
      "heading" : "1.1 Our model",
      "text" : "In this section we present our model for online convex optimization over a convex set S. Online convex optimization is an extremely general learning problem that captures many traditional learning problems as special cases ([11]). Throughout, we assume that the losses are in [−1, 1].\nOnline convex optimization includes prediction with expert advice as a special case. We only consider the decision-theoretic setting for prediction with expert advice ([7]). This is the special case of online convex optimization where S is the set of probability distributions over a finite set of “experts” X , and the loss functions ℓt have the form ℓt (p) = ∑ x∈X ℓ x t p\nx. We imagine ourselves in the position of a central recommendation service, which must provide advice to some fixed set of users U , with |U| = N . We make our decisions in a sequence of rounds, t = 1, 2, . . . , T . At the beginning of each round, we are given the identity of a user ut ∈ U . We then choose an element pt ∈ S, and nature reveals a convex loss function ℓt : S → [−1, 1]. The user ut and loss ℓt may be adversarial, depending on our choices in previous rounds. The loss ℓt may also depend on pt.\nOur loss in round t is simply ℓt (pt). The loss of a set of users H ⊂ U is the total loss in all rounds involving a user ut ∈ H :\nℓH≤T = ∑\nt≤T :ut∈H\nℓt (pt)\nTo measure our performance, we compare to the performance of the best single point in S:\nOPTH≤T = min p∈S\n∑\nt≤T :ut∈H\nℓt (p) .\nWe are interested in minimizing the regret ℓH≤T − OPTH≤T . We will prove bounds that hold simultaneously for every set H .\nRather than producing algorithms for this problem from scratch, we will implement transformations that start with an algorithm for the single-user case, and produce a robust collaborative version.\nAlgorithms for online convex optimization typically make some assumption about the loss functions ℓt, for example that they are Lipschitz in an appropriate norm. Our transformation will apply for any convex optimization problem, under arbitrary assumptions about the loss functions ℓt. The transformed algorithm will make exactly the same assumptions as the original algorithm.\nMore precisely, we take as given an algorithm OCO for online convex optimization over the set S, with some set of admissible loss functions ℓt. Write regret RT (OCO) for the worst-case regret of that algorithm over the first T rounds. We produce an algorithm OCO for collaborative online convex optimization over S, with the same set of admissible loss functions. OCO satisfies a bound of the form\nℓH≤T ≤ OPTH≤T +RT (OCO) +R′T (H)\nfor every set H . We call R′T (H) the additional regret of the transformation. Our goal is to prove bounds on the additional regret.\nWe will also consider a stronger benchmark, in which we divide the honest users into m groups and choose the optimal p for each group:\nOPTH,m≤T = min p1,...,pm∈S\n∑\nu∈H\nmin p∈{p1,...,pm}\n∑\nt≤T :ut=u\nℓt (p) .\nRegret bounds against this class of strategies will follow automatically from our other results.\nMany of our bounds include an additive term in O (√ T log log T ) . We write Õ (·) to hide these additive terms. Typically they will not affect the asymptotics unless T is ω (exp (N)), which is not a regime in which we expect our results to be meaningful. We expect that these terms can be removed with some additional work."
    }, {
      "heading" : "1.2 Our contributions",
      "text" : "Prediction with expert advice. For prediction with expert advice, we exhibit a transformation that introduces additional regret of Õ (√ TN ) .\nThe key technical idea is reversing the relationship between the experts and the users: rather than having the experts always offer advice, we have each expert learn which users to offer advice to. The goal of the expert is to offer advice if and only if their advice will be helpful. The resulting experts are “specialists,” who sometimes abstain from offering advice, and we can apply a standard technique to compete with the best specialist ([8]).\nBy improving the learning algorithm used by the experts, we can significantly improve this bound when there are either few or many malicious users.\nFor example, when only an ǫ < 1/2 fraction of users are malicious (respectively honest), and only an ǫ fraction of rounds involve a malicious (respec-\ntively honest) user, we prove that the additional regret is Õ ( ǫ √ TN log (ǫ) ) .\nThe parameter ǫ is not known to the algorithm; the strengthened guarantee still holds simultaneously for every H .\nAs an immediate consequence, the same algorithm is also competitive with the best strategy that divides the honest users into m > 1 groups and chooses a different expert for each group. In this case, we obtain re-\ngret mRT (PEA) + Õ (√ TN log (m) ) , where RT (PEA) is the regret of an\nalgorithm for single-user prediction with expert advice over T steps. Online convex optimization. For online convex optimization with losses in [−1, 1], we exhibit a transformation that introduces additional regret Õ (√ TN ) . However, this transformation requires exponential time.\nWe demonstrate that there is no statistical obstruction to robust collaborative online optimization, but leave open the problem of designing efficient algorithms.\nThis result can also be improved whenever H is small or large, and can also be improved if we have side information about the set H .\nThe key idea behind the transformation is to introduce an expert for every subset of the users. Each expert uses the underlying online convex optimization algorithm to make recommendations to the corresponding subset of users. We then aggregate these recommendations using an algorithm for learning from specialists ([8]).\nThis result can also be applied to the case where we want to divide the honest users into m subgroups. We obtain regret mRT/m (OCO) + Õ (√ TN log (m) ) , where RT/m (OCO) is the regret of an algorithm for\nsingle-user online convex optimization over T/m steps."
    }, {
      "heading" : "1.3 Related work",
      "text" : "Collaborative filtering: In the collaborative filtering problem, a set of users interact with a set of resources, and exploit their common tastes to more efficiently predict which resources each of them will rate highly. This problem has been studied at length; see ([12]) for an overview. A wide range of theoretical models for this problem have been studied ([1], [9], [2], [6]).\nOur work differs from this literature in two respects. First, we focus on\nachieving very strong robustness and non-manipulability guarantees without sacrificing statistical efficiency. Second, we consider the general problem of prediction with expert advice rather than predicting which resources a user will rate highly. As a result, we require completely different techniques.\nRobust collaborative filtering. The most closely related work is ([5]). Their results fit in our model of robust collaborative learning, but they study a different problem (a particular model of the collaborative filtering problem) and use different techniques.\nAlong similar lines, ([3]) provides a robust collaborative algorithm for the multi-armed bandit problem, under an additional stochastic assumption. The multi-armed bandit problem is more similar to, and motivated by, applications to collaborative filtering than to our setting. Moreover, it is not clear how we would apply similar techniques to our setting.\nAdversarial learning. Another literature deals with learning problems in which an adversary has some influence over the training or testing data ([4]). Robust collaborative learning can be viewed within this framework, as an attack model in which an adversary controls the data associated with some users. The unique characteristic of our model is that we only care about the payoff associated with the uncorrupted users; in our view this is a very natural model of an important class of attacks. This allows us to obtain extremely strong regret bounds, and also leads us to use a novel set of techniques."
    }, {
      "heading" : "2 Preliminaries: learning algorithms as sub-",
      "text" : "routines\nOur results are transformations, from single-user learning algorithms to collaborative versions.\nThe transformed algorithm invokes several learning algorithms as subroutines, (one of which is the algorithm PEA or OCO that we are transforming to a collaborative version). We will typically use the Roman letters A,B,C, . . . to represent these subroutines. Each of them is solving an independent learning problem.\nThe “outer” learning problem that we are trying to solve proceeds in a series of rounds t = 1, 2, . . . , T, . . .. In general, the “inner” learning problems will proceed more slowly; some but not all of the subroutines will make\na prediction and receive a loss function in each round of the outer learning problem. We say that a subroutines is “active” in a given round if it produces an output and receives a loss. Write τA (T ) = {t ≤ T : A is active}.\nFor t ∈ τA (T ), we write At for the output of A, at the beginning of the tth round of the outer learning problem. In the case of prediction with expert advice, At is a distribution over experts, and we write At (x) for the probability assigned to expert x.\nSimilarly, for t ∈ τA (T ), we write ℓAt for the loss function observed by A during round t of the outer learning problem. In the case of prediction with expert advice, we abuse notation and write ℓAt (x) for the loss of expert x, i.e. the loss of the distribution that assigns probability 1 to x.\nMany of our subroutines will be algorithms for prediction with expert advice and non-uniform priors. The chief subtlety of such algorithms, compared to classical algorithms for prediction with expert advice, is adapting the learning rate appropriately. We will use the algorithm Squint from ([10]). We write A ← Squint (D) to indicate that A is a new instance of Squint with experts D. We may also specify some initial weights B0 (d) for d ∈ D; if we don’t specify initial weights, then B0 (d) = 1/ |D|.\nRecalling that τB (T ) is the set of rounds t ≤ T where B makes a prediction and receives a loss, we have:\nLemma 1. ([10, Theorem 4]) If B is an instance of Squint with experts D, then for any d∗ ∈ D we have\n∑\nt∈τB(T )\nℓBt (Bt) ≤ ∑\nt∈τB(T )\nℓBt (d ∗) + Õ (√ |τB (T )| log (B0 (d∗)) ) .\nMoreover, if V = ∑\nt∈τB(T ) ( ℓBt (B t)− ℓBt (d∗) )2 , then we have\n∑\nt∈τB(T )\nℓBt (Bt) ≤ ∑\nt∈τB(T )\nℓBt (d ∗) + Õ (√ V log (B0 (d∗)) ) .\nFor most of our purposes Squint is overkill, and we could obtain the same results using any algorithm that satisfies a similar non-uniform regret bound. In Section 4, we will make use of the improved bound in terms of the regret variance V , and so we use Squint throughout for simplicity.\nSquint is one source of our O (√ T log log (T ) ) regret term, which we\nhide in Õ (·); it is not the only source."
    }, {
      "heading" : "3 Collaborative prediction with expert ad-",
      "text" : "vice\nRecall the model discussed in section 1.1. N = |U| is the number of users. We will take as given an algorithm PEA for the single-user case, with regret RT (PEA), and exhibit a protocol PEA with the following guarantee:\nTheorem 1. Let H be any set of users. Then PEA satisfies:\nℓH≤T ≤ OPTH≤T +RT (PEA) + Õ (√ TN ) .\nIf we write D = U\\H and TS = |{t ≤ T : ut ∈ S}|, then PEA satisfies the stronger bounds:\nℓH≤T ≤ OPTH≤T +RT (PEA) + Õ (√ TH |H| log ( T\nTH\nN\n|H|\n) + TH |H|\n)\nand\nℓH≤T ≤ OPTH≤T +RT (PEA) + Õ (√ TD |D| log ( T\nTD\nN\n|D|\n) + TD |D|\n)\nIn all of these bounds, Õ (·) hides additive terms that are Õ (√ T log log T ) .\nFor example, if |H| = Θ (N), then the total additional regret per honest user is O (√ k ) , where k = T/N is the average number of rounds per user.\nIf the fraction of honest users is ǫ, and an ǫ fraction of rounds involve an\nhonest user, then the total additional regret per honest user isO (√ k log ( 1 ǫ )) . Similarly, if the fraction of dishonest users is ǫ, and an ǫ fraction of rounds involve a dishonest user, then total additional regret per dishonest user is\nO (√ k log ( 1 ǫ )) (this additional regret is amortized over all of the honest users). For any set of users H , recall the stronger benchmark OPTH,m≤T which can choose up to m different experts, and then pick a different expert from this set for each user in H .\nWe can compete with this benchmark without modifying our algorithm. In Appendix C we prove\nCorollary 1. For any set of users H and any m > 1, PEA satisfies:\nℓH≤T ≤ OPTH,m≤T +mRT (PEA) + Õ (√ TN log (m) ) ."
    }, {
      "heading" : "It also satisfies the stronger bound:",
      "text" : "ℓH≤T ≤ OPTH,m≤T +mRT (PEA) + Õ (√ TH |H| log ( m T\nTH\nN\n|H|\n) + T\n)\nwhere as before TH = |{t ≤ T : ut ∈ H}|.\nIn our result for online convex optimization, we obtain a similar regret bound that depends on mRT/m (PEA) rather than mRT (PEA). However, the algorithm that achieves this bound is not efficient. We leave closing this gap as an open problem. The most likely approach is replacing the term RT (PEA) in Theorem 1 with RTH (PEA)."
    }, {
      "heading" : "3.1 Basic algorithm",
      "text" : "Our basic algorithm is given in Figure 1. We also informally explain the behavior of the algorithm in this section. The full algorithm, for which we prove Theorem 1, is discussed in the next section and analyzed in Appendix B.\nFor each x ∈ X and each round t, we compute a quantity, zxt reflecting the probability that expert x should offer advice in round t. We imagine these quantities as being computed “by the experts” but this has no substantive effect on the algorithm. We will describe how the experts compute the quantities zxt later.\nOnce the experts have decided whether to participate in a round, we use a standard algorithm for combining “specialists,” experts who are only active in some rounds ([8]). In order to make a prediction, we renormalize the weights of the active experts. We compute the losses of the active experts as usual, and for each inactive expert we provide a loss equal to the average loss of the active experts (weighted by their current weights). We then update the weights using the algorithm PEA.\nThe experts choose zxt in order to minimize their losses and maximize the weight given to them by PEA. Each expert instantiates one Squint instance per user, with one expert that always recommends “active” and one expert that always recommends “inactive.” In round t, each expert consults the\ninstance corresponding to ut to decide whether to be active. The loss for the “active” expert is ℓt (x), while the loss for the “inactive” expert is the average loss of the other experts (weighted by their current weights).\nA←PEA (X ); for x ∈ X , u ∈ U do\nBxu←Squint ({0, 1}); end for t = 1, 2, . . . do Observe ut ∈ U ; for x ∈ X do\nzxt ←Bxutt (1); wxt ←zxt At (x);\nend Wt← ∑ x w x t ; Play pt (x) = w x t /Wt ∈ ∆(X ); Observe ℓt : X → [−1, 1]; for x ∈ X do\nℓAt (x)←zxt ℓt (x) + (1− zxt )ℓt (pt); ℓB xut\nt (1)←ℓt (x); ℓB xut\nt (0)←ℓt (pt); end\nend\nAlgorithm 1: Collaborative prediction with expert advice\nTo analyze this algorithm, we consider the excess performance of expert x in rounds where they are active: ∑ t z x t (ℓt (x)− ℓt (pt)). We show that this quantity is nearly as negative as if we had taken zxt = 1 precisely when ut ∈ H . On the other hand, we show that this quantity can’t be more than RT (PEA). This places a bound on the excess performance of expert x in the rounds where ut ∈ H , which is precisely what we want to establish.\nLemma 2. For every x ∈ X we prove: ∑\nt≤T\nzxt (ℓt (x)− ℓt (pt)) ≤ ∑\nt≤T :ut∈H\n(ℓt (x)− ℓt (pt)) + Õ (√ TN )\nProof. We apply Lemma 1 to each Squint instance Bxu, and sum the resulting\ninequalities. Write hx = 1 if x ∈ H , and 0 otherwise.∑\nt≤T\nzxt (ℓt (x)− ℓt (pt)) = ∑\nu∈U\n∑\nt≤T :ut=u\nzxt (ℓt (x)− ℓt (pt))\n≤ ∑\nu∈U\n( ∑\nt≤T :ut=u\nhx (ℓt (x)− ℓt (pt)) + Õ (√ |{t ≤ T : ut = u}| ))\n≤ ∑\nt≤T :ut∈H\n(ℓt (x)− ℓt (pt)) + Õ (√ TN )\nWhere the last inequality follows by Jensen’s.\nLemma 3. For any x ∈ X , ∑\nt≤T\nzxt (ℓt (pt)− ℓt (x)) ≤ RT (PEA)\nProof. First, observe that the loss of A is exactly equal to the loss of pt: We compute the loss of A:\nℓAt (A) = ∑\nx\nAt (x) (z x t ℓt (x) + (1− zxt )ℓt (pt))\n= ∑\nx\nAt (x) z x t ℓt (x) + ℓt (pt)\n∑\nx\nAt (x)− ℓt (pt) ∑\nx\nAt (x) z x t\n= ∑\nx\nwxt ℓt (x) + ℓt (pt)− ℓt (pt) ∑\nx\nwxt\n= Wtℓt (pt) + ℓt (pt)−Wtℓt (pt) = ℓt (pt) .\nSo we can apply the regret bound for A, and obtain: ∑\nt≤T\nℓt (pt) = ∑\nt≤T\nℓAt (A)\n≤ ∑\nt≤T\nℓAt (x) +RT (PEA)\n= ∑\nt≤T\n(zxt ℓt (x) + (1− zxt )ℓt (pt)) +RT (PEA)\n= ∑\nt≤T\nℓt (pt) + ∑\nt≤T\nzxt (ℓt (x)− ℓt (pt)) +RT (PEA)\n0 ≤ ∑\nt≤T\nzxt (ℓt (x)− ℓt (pt)) +RT (PEA) .\nCombining these lemmas, we have that for any expert x ∈ X : ∑\nt≤T :ut∈H\n(ℓt (x)− ℓt (pt)) ≤ ∑\nt≤T\nzxt (ℓt (x)− ℓt (pt)) + Õ (√ TN )\n≤ RT (PEA) + Õ (√ TN )\nas desired."
    }, {
      "heading" : "3.2 Improving the algorithm",
      "text" : "In the previous algorithm, the experts treat each user as a separate learning problem. We can improve the algorithm by having the experts learn what fraction of the users are honest, rather than implicitly expecting half of all users to be honest.\nWe introduce a new learning algorithm Aθ for solving a simultaneous prediction with expert advice problem for each user u ∈ U . We instantiate Aθ as A ← Aθ (U). For each round t, Autt is a probability distribution over {0, 1}. Write ℓAt for the loss function given to A in round t. We imagine a separate learning problem for each user u ∈ U , with ut indicating which learning problem is being addressed in round t. However, rather than simply using an independent instance of Squint for each user, Aθ learns a parameter θ reflecting a prior distribution over {0, 1}.\nIn Appendix B, we define Aθ and prove the following result: Theorem 2. Let A be an instance of Aθ. For any H ⊂ U , we have: ∑\nt≤T\nℓAt (A ut t ) ≤\n∑\nt≤T :ut∈H\nℓAt (1)+ ∑\nt≤T :ut 6∈H\nℓAt (0)+Õ (√ TH |H| log ( TN\nTH |H|\n) + TH |H|\n)\nThis bound also holds if we replace each occurrence of H in the regret term with U\\H.\nWith Aθ in hand, it is straightforward to improve Algorithm 1. Rather than having each expert instantiate a separate instance Bxu of Squint for each user u, we instantiate a single instance Bx of Aθ. The analysis of the improved algorithm is then identical to the analysis of Algorithm 1, except that the conclusion of Lemma 2 is strengthened appropriately. The result is precisely the strengthened conclusion in Theorem 1. The full algorithm and analysis are given in Appendix B."
    }, {
      "heading" : "4 Online convex optimization",
      "text" : "In this section, we present a general method for transforming an online convex optimization algorithm OCO into a collaborative version OCO. Both algorithms are optimizing over some convex set S.\nOCO maintains a separate instance of OCO for every subset of the users. Each of these instances aggregates data over all users in the corresponding subset. We then use a standard algorithm for sleeping experts to aggregate across these different OCO instances; the experts awake at step t are precisely those corresponding to sets that contain ut.\nA←Squint ( 2U ) , where 2U is the power set of U , with A0 (S) = p (S); for S ⊆ U do BS←OCO();\nend for t = 1, 2, . . . do Observe ut ∈ U ; Wt← ∑ S⊆U :ut∈S At (S);\nPlay pt = ∑\nS⊆U :ut∈S At (S)B S t /Wt;\nObserve ℓt : S → [−1, 1]; for S ⊆ U do\nif ut ∈ S then ℓAt (S)←ℓt ( BSt ) ;\nℓB S t ←ℓt; else\nℓAt (S)←ℓt (pt); end\nend\nend\nAlgorithm 2: OCO\nTheorem 3. For any distribution p over subsets of U , there is an algorithm OCOp such that, for every set H:\nℓH≤T ≤ OPTH≤T +RTH (OCO) + Õ (√ THp (H) )\nProof. We have\nℓAt (At) = ∑\nS⊂U\nAt (S) ℓ A t (S)\n= ∑\nS⊂U :ut∈S\nAt (S) ℓt ( BSt ) + ∑\nS⊂U :ut 6∈S\nAt (S) ℓt (pt)\n= Wt ∑\nS⊂U :ut∈S\n( At (S) ℓt ( BSt ) /Wt ) + (1−Wt)ℓt (pt)\n≥ Wtℓt (pt) + (1−Wt)ℓt (pt) = ℓt (pt)\nwhere the inequality follows from the convexity of ℓt. Moreover, by Lemma 1, we have:\n∑\nt\nℓAt (At) ≤ ∑\nt\nℓAt (H) + Õ (√ THp (H) )\n∑\nt:ut∈H\n( ℓAt (A)− ℓAt (H) ) = ∑\nt\n( ℓAt (A)− ℓAt (H) )\n≤ Õ (√ THp (H) )\n∑\nt:ut∈H\nℓAt (At) ≤ ∑\nt:ut∈H\nℓAt (H) + Õ (√ Tp (H) )\n∑\nt:ut∈H\nℓt (pt) ≤ ∑\nt:ut∈H\nℓAt (H) + Õ (√ Tp (H) )\nWe obtain TH in the bound on the first line, rather than T , because ℓ A t (H) = ℓt (pt) = ℓ A t (A) for all t such that ut 6∈ H . Thus V = ∑ t≤T ( ℓAt (H)− ℓAt (A)\n)2 is upper-bounded by TH .\nThe second line is valid because ℓAt (A) = ℓ A t (H) for t such that ut 6∈ H . Finally, for any x ∈ S: ∑\nt:ut∈H\nℓAt (H) = ∑\nt:ut∈H\nℓt ( BHt )\n≤ ∑\nt:ut∈H\nℓt (x) +RTH (OCO)\nWhere the inequality is valid because ℓt = ℓ BH\nt . Combining these inequalities yields\n∑\nt:ut∈H\nℓt (pt) ≤ ∑\nt:ut∈H\nℓt (x) +RTH (OCO) + Õ (√ THp (H) ) ,\nas desired.\nWe define OCO by plugging in an appropriate p: first draw θ from the distribution µ defined in Appendix B (roughly log uniform), and then sample a random set with density θ. We obtain:\nCorollary 2. For every set H, OCO satisfies:\nℓH≤T ≤ OPTH≤T +RTH (OCO) + Õ (√ TH |H| log ( N\n|H|\n) + TH |H|\n)\nProof. All we need to show is that\nlog (p (H)) = Õ ( |H| log ( N\n|H|\n) + |H|+ log log T ) .\nBut this is easily verified by direct calculation, as in the proof of Theorem 2.\nFinally, in Appendix C we show that OCO competes with OPTH,m≤T . We assume that RT (OCO) is a convex function of T—this is true for essentially all learning algorithms used in practice.\nCorollary 3. For every set H, and any m > 1, OCO satisfies:\nℓH≤T ≤ OPTH≤T +RT/m (OCO) + Õ (√ TH |H| log ( m N\n|H|\n)) ,\nprovided that RT (OCO) is a convex function of T ."
    }, {
      "heading" : "5 Open questions",
      "text" : "The robust collaborative learning framework provides a general transformation from single-user learning problem to robust collaborative learning problems. We have answered a few fundamental questions, but we leave many more open.\n• Efficient online convex optimization. Our algorithm for online convex optimization is intractable. It may be possible to produce a general transformation from a tractable online convex optimization algorithm to a tractable collaborative online convex optimization algorithm; this would be a major improvement over our results. Even if such a general transformation is impossible, we can seek efficient algorithms for a broader range of online convex optimization problems, for example online learning over combinatorial structures.\n• Exploiting information about users. Our additional regret bounds depend on a quantity like |H|, reflecting the prior probability of the set H under an appropriate distribution. If we can find a better probability distribution p over subsets, potentially exploiting other information available about the users, then the statistically optimal additional regret is √ p(H)TH (as in Theorem 3). For example, if we know that\nmalicious users tend to have few honest friends, then we can use this information to reduce our additional regret bounds, potentially down to negligible levels per user. Our algorithm for online convex optimization is able to make use of this kind of information, but we dont’ propose any efficient algorithms that can nor do we consider any promising sources of information.\n• Parallel expert problems. Suppose the same set of users participate in many online services 1, 2, . . . , k, with each user participating in some arbitrary subset. We would like to be able to amortize the additional regret over all of these services, rather than running a separate collaborative learning algorithm for each of them. This corresponds to an experts problem with a simple combinatorial structure: an “expert” corresponds to a choice of expert in each of the k underlying problems. ([5]) develops robust collaborative algorithms for this problem when the\nnumber of experts in each problem is Õ (1). But their regret bounds are suboptimal, and moreover the general problem remains open.\n• Translating results from the single-user setting. The robust collaborative learning framework provides a transformation from singleuser learning problems to collaborative learning problems. It is natural to try to extend desirable properties to the collaborative case, such as performing well on “easy” instances, achieving quantile bounds, or competing with enlarged spaces of of single-user algorithms to the collaborative case. Natural contenders are tighter bounds for “easy” instances, quantile bounds, or competing with enlarged spaces of strategies (such as switching experts).\n• Better regret against OPTH,m≤T . For prediction with expert advice, our regret bounds against OPTH,m≤T depend on mRT (PEA), while our intractable algorithm for online convex optimization has regret that depends on mRT/m (PEA). For practical learning algorithms this corresponds to a significant √ m slowdown.\n• Contextual bandits. Our algorithms all require full feedback. It seems likely that they can be extended to the contextual bandits setting, which would be important for many practical applications. Without some additional stochastic assumptions, we expect that the addi-\ntional regret will have to likely scale like Õ (√ TAN ) , where A is the\nnumber of available actions; but even this result would be a significant generalization of our algorithm for predicting with expert advice.\n• Memory requirements. Our algorithm for prediction with expert advice requires maintaining one weight for each (expert, user) pair. When the number of users and experts is large, this may be infeasible. A more efficient algorithm might only require O (|U|+ |X |) storage rather than O (|U| ∗ |X |) storage."
    }, {
      "heading" : "A Defining Aθ",
      "text" : "We will assume that T is known in advance. This assumption can be removed with a standard doubling trick.\nLet Θ be the smallest set such that:\n• 0 ∈ Θ\n• 2−i ∈ Θ for each i ≤ log (NT )\n• 1− x ∈ Θ, for each x ∈ Θ\nand let µ be a uniform distribution over Θ.\nA←Squint instance with experts Θ and initial weights A0 (θ) = µ (θ).; for θ ∈ ΘN , u ∈ U do\nBθu←Squint instance with experts 0 and 1, with initial weight Bθu0 (1) = θ;\nend for t = 1, 2, . . . do Observe ut ∈ U ; Play pt = ∑ θ∈Θ A (θ)B\nθut (1); Observe ℓt : {0, 1} → [−1, 1]; for θ ∈ Θ do\nℓAt (θ)←ℓt ( Bθut ) ; ℓB θut\nt ←ℓt; end\nend\nAlgorithm 3: Aθ\nAθ is defined in Figure 3. Recall that our goal is to bound the regret of Aθ, compared to the best\nstrategy corresponding to a fixed set H , by\nÕ (√ TH |H| log ( TN\nTH |H|\n) + TH |H| ) ,\nand a similar term with H replaced by U\\H .\nTheorem 2. Aθ is symmetric under swapping 0 and 1 (since µ is symmetric), and so is symmetric under swapping H and U\\H . So it suffices to prove the regret bound with H .\nNote that Aθ has regret of at most Õ (√ T log log (NT ) ) with respect to\neach set of experts Bθu. Since Õ (√ T log log T ) = Õ (1), and Õ (√ T log logN ) is dominated by the desired regret bound for every set H , it suffices to prove that at least one of the families Bθu has the claimed regret.\nWrite ǫ = TH T |H| N . If ǫ > 1/2, then TN = O (TH |H|), and so the family of predictors B1/2u have the desired regret bound. So assume ǫ < 1/2. Note that there is a unique θ ∈ (ǫ/2, ǫ] ∩ Θ. We will show that Bθu satisfies the desired regret bound. If θ = 0, then this claim is trivial since the corresponding predictors are constants. So assume θ > 0.\nFor each user u ∈ H who has been involved in Tu rounds, the regret is upper-bounded by\nÕ (√ Tu log (θ) ) = Õ (√ Tu log (ǫ) ) .\nFor each user u 6∈ H , it is bounded by\nÕ (√ Tu log (1− θ) ) = Õ (√ Tu log (1− ǫ) ) = Õ (√ Tuǫ )\nBy Jensen’s inequality, the total regret is maximized when Tu is equal to TH/ |H| for all u ∈ H and (T − TH)/(N − |H|) for all u 6∈ H .\nThe total regret of the instances Bθ,u is thus at most\nÕ ( |H| √ TH |H| log (ǫ) +N √ T/Nǫ ) ≤ Õ (√ TH |H| log (ǫ) + √ TU\\H |U\\H| ǫ )\n≤ Õ (√ TH |H| log (ǫ) + √ TNǫ ) = Õ (√ TH |H| log (ǫ) + √ TH |H| ) ,\nas desired."
    }, {
      "heading" : "B Proof of Theorem 1",
      "text" : "The algorithm PEA is defined in Figure 4. The algorithm is precisely analogous to the algorithm in Section 3.1, and its analyis is precisely parallel to the analysis in that section.\nA←PEA (X ); for x ∈ X do\nBx←Aθ (U); end for t = 1, 2, . . . do Observe ut ∈ U ; for x ∈ X do\nzxt ←Bxutt (1); wxt ←zxt At (x);\nend Wt← ∑ x w x t ; Play pt (x) = w x t /Wt ∈ ∆(X ); Observe ℓt : X → [−1, 1]; for x ∈ X do\nℓAt (x)←zxt ℓt (x) + (1− zxt )ℓt (pt); ℓB x\nt (1)←ℓt (x); ℓB x\nt (0)←ℓt (pt); end\nend\nAlgorithm 4: PEA\nLemma 4.\n∑\nt≤T\nzxt (ℓt (x)− ℓt (pt)) ≤ ∑\nt≤T :ut∈H\n(ℓt (x)− ℓt (pt))+Õ (√ TH |H| log ( TN\nTH |H|\n) + TH |H| ) ,\nand similarly if we replace each occurence of H in the final term with U\\H. Proof. If we subtract ∑\nt≤T ℓt (pt) to both sides, then the left hand side is\nprecisely ∑\nt ℓ Bx t (B x t ). We can therefore apply Theorem 2, from which the\ndesired result follows immediately.\nLemma 5. For any x ∈ X , ∑\nt≤T\nzxt (ℓt (pt)− ℓt (x)) ≤ RT (PEA)\nProof. This proof is identical to the proof of Lemma 3.\nCombining these lemmas, we have that for any expert x ∈ X :\n∑\nt≤T :ut∈H\n(ℓt (x)− ℓt (pt)) ≤ ∑\nt≤T\nzxt (ℓt (x)− ℓt (pt)) + Õ (√ TH |H| log ( TN\nTH |H|\n) + TH |H|\n)\n≤ RT (PEA) + Õ (√ TH |H| log ( TN\nTH |H|\n) + TH |H|\n)\nas desired.\nC Competing with OPT H,m ≤T\nWe first show that OCO competes with OPTH,m≤T , which is slightly easier than analyzing PEA.\nCorollary 3. Fix an optimizing set {p1, . . . , pm}. Let Hi be the set of users for which the minimum in the definition of OPTH,m≤T is obtained at pi. We assume that the Hi are a partition, we can break ties lexicographically if needed. We apply Corollary 2 to each of the Hi, and add up the resulting inequalities.\nOn the LHS we obtain ℓH≤T .\nOn the RHS, for the benchmark terms we obtain ∑\ni OPT Hi ≤T = OPT H,m ≤T .\nFor the RT (OCO) terms, we obtain ∑\ni RTHi (OCO). Under our convexity assumption, this is at most mRT/m (OCO).\nFinally, we have the sum of regret terms Õ (√ THi |Hi| log ( N |Hi| )) (the\nsecond summand in the regret terms is dominated on average). Optimizing\nwith respect to THi , we can see that this is maximized if THi ∝ |Hi| log ( N |Hi| ) . Substituting in these values, the resulting expression is a convex function of |Hi| (essentially it is a constant times the entropy of the numbers |Hi| / |H|), and so is maximized when |Hi| = |H| /m. Finally, plugging in these values we obtain a total regret of\nmÕ (√\nTH m |H| m log (m) log\n( N\n|H|\n))\n=Õ (√ TH |H| log (m) log ( N\n|H|\n)) .\nAdding up these three terms on the RHS, we obtain precisely the desired inequality.\nThe analysis of PEA is essentially identical. There are two differences:\n• The regret term in Theorem 1 depends on RT (PEA) rather than RTH (PEA), and so we obtain mRT (PEA) rather than mRT/m (PEA). • The regret term in Theorem 1 depends on log (\nN |H| T TH\n) rather than on\nlog (\nN |H|\n) . However, we can split up this log into two summands, and\nbound each summand in exactly the same way that we bound the sum of regret terms in the proof of Corollary 3."
    } ],
    "references" : [ {
      "title" : "Tell me who I am: An interactive recommendation system",
      "author" : [ "Alon", "Awerbuch", "Azar", "Patt-Shamir" ],
      "venue" : "In SPAA: Annual ACM Symposium on Parallel Algorithms and Architectures",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Collaborate with strangers to find own preferences",
      "author" : [ "Awerbuch", "Azar", "Lotker", "Patt-Shamir", "Tuttle" ],
      "venue" : "MST: Mathematical Systems Theory",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Competitive collaborative learning",
      "author" : [ "Awerbuch", "Kleinberg" ],
      "venue" : "In COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "Can machine learning be secure",
      "author" : [ "M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar" ],
      "venue" : "In Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security (New York, NY, USA,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Provably manipulation-resistant reputation systems",
      "author" : [ "P. Christiano" ],
      "venue" : "CoRR abs/1411.1127",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Competitive recommendation systems",
      "author" : [ "Drineas", "Kerenidis", "Raghavan" ],
      "venue" : "In STOC: ACM Symposium on Theory of Computing (STOC)",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Freund", "Schapire" ],
      "venue" : "JCSS: Journal of Computer and System Sciences",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1997
    }, {
      "title" : "Using and combining predictors that specialize",
      "author" : [ "Freund", "Schapire", "Singer", "Warmuth" ],
      "venue" : "In STOC: ACM Symposium on Theory of Computing (STOC)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Near-optimal algorithms for online matrix prediction",
      "author" : [ "E. Hazan", "S. Kale", "S. Shalev-Shwartz" ],
      "venue" : "CoRR abs/1204.0136",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Second-order quantile methods for experts and combinatorial games",
      "author" : [ "W.M. Koolen", "T. van Erven" ],
      "venue" : "CoRR abs/1502.08009",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "S. Shalev-Shwartz" ],
      "venue" : "Foundations and Trends in Machine Learning 4,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Online convex optimization is an extremely general learning problem that captures many traditional learning problems as special cases ([11]).",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "We only consider the decision-theoretic setting for prediction with expert advice ([7]).",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "The resulting experts are “specialists,” who sometimes abstain from offering advice, and we can apply a standard technique to compete with the best specialist ([8]).",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 7,
      "context" : "We then aggregate these recommendations using an algorithm for learning from specialists ([8]).",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "A wide range of theoretical models for this problem have been studied ([1], [9], [2], [6]).",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "A wide range of theoretical models for this problem have been studied ([1], [9], [2], [6]).",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "A wide range of theoretical models for this problem have been studied ([1], [9], [2], [6]).",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "A wide range of theoretical models for this problem have been studied ([1], [9], [2], [6]).",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "The most closely related work is ([5]).",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "Along similar lines, ([3]) provides a robust collaborative algorithm for the multi-armed bandit problem, under an additional stochastic assumption.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "Another literature deals with learning problems in which an adversary has some influence over the training or testing data ([4]).",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "We will use the algorithm Squint from ([10]).",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "Once the experts have decided whether to participate in a round, we use a standard algorithm for combining “specialists,” experts who are only active in some rounds ([8]).",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "([5]) develops robust collaborative algorithms for this problem when the number of experts in each problem is Õ (1).",
      "startOffset" : 1,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "We consider a collaborative variant of prediction with expert advice. In each round, one user wishes to make a prediction, and must choose which expert to follow. The users would like to share their experiences in order to learn faster—ideally they could amortize the same total regret cross the whole community of users. However, some of the users may behave maliciously, distorting their reported payoffs in order to manipulate the honest users of the system. And even if all users behave honestly, different experts may perform better for different users, such that sharing data can be counterproductive. We present a robust collaborative algorithm for prediction with expert advice, which guarantees that every subset of users H perform nearly as well as if they had shared all of their data, and ignored all data from users outside of H. This algorithm limits the damage done by the dishonest users to O (√ T ) , compared to the O (T ) we would obtain by naively aggregating data. We also extend our results to general online convex optimization. The resulting algorithm achieves low regret, but is computationally intractable. This demonstrates that there is no statistical obstruction to generalizing robust collaborative online learning, but leaves the design of efficient algorithms as an open problem.",
    "creator" : "LaTeX with hyperref package"
  }
}