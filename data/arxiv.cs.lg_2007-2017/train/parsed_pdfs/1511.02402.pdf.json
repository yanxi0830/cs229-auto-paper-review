{
  "name" : "1511.02402.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Mehrdad Ghadiri", "M. Ghadiri" ],
    "emails" : [ "ghadiri}@ce.sharif.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n02 40\n2v 1\n[ cs\n.L G\n] 7\nN ov\n2 01\n5"
    }, {
      "heading" : "Introduction",
      "text" : "In many search applications, the search engine should guess the correct results from a given query; therefore, it is important to deliver a diversified and representative set of documents to a user. Diversification can be viewed as a trade-off between having more relevant results and having more diverse results among the top results for a given query [3]. “Jaguar” is a cliche example in the diversification literature [2, 4, 9], but it illustrates the point perfectly as it has different meanings including car, animal, and a football team. A set of good “quality” result should cover all these diversified items. The paper by Borodin et al [1] determines the good quality results with a monotone submodular function and defines diversity as the sum of distances between selected objects. Since they consider the distances to be metric, they ask in the conclusion section:\nFor a relaxed version of the triangle inequality can we relate the approximation ratio to the parameter of a relaxed triangle inequality?\nIn this study we answer to this question. We call this relaxed triangle inequality distance as semi-metric. A semi-metric distance on a set of items is just like a metric distance, but the triangle inequality is relaxed with a parameter α ≥ 1 (i.e., d(u, v) ≤ α(d(v, w)+d(w, u))). Answering to this question will make this method applicable to algorithms that are defined on semi-metric spaces, e.g., [5, 7, 8]. The IBM’s Query by Image Content system is one of the other best-known examples of the semi-metric usage in practice; although, it does not\nsatisfy the triangle inequality [6]. By modifying the analysis of the previous proposed algorithms in [1], we will show that these algorithms can still achieve a 2α-approximation for this question in the case that there is not any matroid constraint and a 2α2-approximation for an arbitrary matroid constraint. In other words, these new modified analysis are a generalization of the previous analysis as they are consistent with the previous approximation ratios for α = 1 (i.e., the metric distance).\nProblem 1. Max-Sum Diversification\nLet U be the underlying ground set, and let d(., .) be a semi-metric distance function on U . The goal of the problem is to find a subset S ⊆ U that:\nmaximizes f(S) + λ ∑\n{u,v}:u,v∈S d(u, v)\nsubject to |S| = p,\nwhere p is a given constant number and λ is a parameter specifying a trade-off between the distance and submodular function. We give a 2α−approximation for this problem.\nFirstly we introduce our notations following [1]. For any S ⊆ U , we let d(S) = ∑ {u,v}:u,v∈S d(u, v). We can also define d(S, T ), for any two disjoint sets S and T as: d(S ∪ T )− d(S)− d(T ).\nLet φ(S) and u be the value of the objective function and an element in U − S respectively. We can define the marginal gain of the distance function as\ndu(S) = ∑ v∈S d(u, v)\nand similarly marginal gain of the wight function as:\nfu(S) = f(S + u)− f(S).\nThe total marginal gain can also be defined using du(S) and fu(S) as\nφu(S) = fu(S) + λdu(S).\nLet\nf ′u(S) = 1\n2 fu(S),\nφ′u(S) = f ′ u(S) + λdu(S).\nStarting with an empty set S, the greedy algorithm (Algorithm 1) adds an element u from U − S in each iteration, in such a way that maximize φ′u(S).\nLemma 1. Given an α-relaxed triangle inequality semi-metric distance function d(., .), and two disjoint sets X and Y , we have the following inequality:\nα(|X | − 1)d(X,Y ) ≥ |Y |d(X)\nAlgorithm 1 Greedy algorithm\n1: Input 2: U : set of ground elements 3: p: size of final set 4: Output 5: S: set of selected elements with size p 6: S = ∅ 7: while |S| < p do 8: find u ∈ U \\ S maximizing φ′\nu (S)\n9: S = S ∪ {u} 10: end while 11: return S\nProof. Consider u, v ∈ X and an arbitrary w ∈ Y . We know that:\nα(d(v, w) + d(w, u)) ≥ d(u, v)\nBy changing w we get:\nα(d({v}, Y ) + d({u}, Y )) ≥ |Y |d(u, v)\nand then all combinations of u and v:\nα(|X | − 1)d(X,Y ) ≥ |Y |d(X)\nTheorem 1. Algorithm 1 achieves a 2α-approximation for solving Problem 1 with α-relaxed distance d(., .) and monotone submodular function f .\nProof. Let Gi be the greedy solution at the end of step i, i < p and G be the greedy solution at the end of the algorithm. Suppose that O is the optimal solution and let A = O ∩ Gi, B = Gi \\ A and C = O \\ A. Obviously the algorithm achieves the optimal solution when p = 1; thus we assume p > 1. Now we consider two different cases: |C| = 1 and |C| > 1. If |C| = 1 then i = p− 1. Let C = {v} and u be the element that algorithm will take for the next (last) step. Then for all v ∈ U \\ S we have:\nφ′u(Gi) ≥ φ ′ v(Gi)\nf ′u(Gi) + λdu(Gi) ≥ f ′ v(Gi) + λdv(Gi)\nthus:\nφu(Gi) = fu(Gi) + λdu(Gi)\n≥ f ′u(Gi) + λdu(Gi) ≥ f ′v(Gi) + λdv(Gi) ≥ 1\n2 φv(Gi)\nas a result φ(G) ≥ 12φ(O) ≥ 1 2αφ(O). Now consider |C| > 1. By using Lemma 1 we have the following inequalities:\nα(|C| − 1)d(B,C) ≥ |B|d(C) (1)\nα(|C| − 1)d(A,C) ≥ |A|d(C) (2)\nα(|A| − 1)d(A,C) ≥ |C|d(A) (3)\nA and C are two disjoint sets and we know that A ∪ C = O; thus:\nd(A,C) + d(A) + d(C) = d(O) (4)\nWe can assume that p > 1 and |C| > 1 (The greedy algorithm obviously finds the optimal solution when p = 1). Then following multipliers are applied to equations 1, 2, 3, 4 respectively:\n1 (|C|−1) , |C|−|B| p(|C|−1) , i p(p−1) , i|C| αp(p−1) .\nIf we add them, we have:\nd(B,C) + d(A,C)− d(A,C) i|C|(1 − 1 α )\np(p− 1) − d(C)\ni|C|(p− |C|)\nαp(p− 1)(|C| − 1) ≥ d(O)\ni|C|\nαp(p − 1)\nSince p > |C| and α ≥ 1,\nd(A,C) + d(B,C) ≥ d(O) i|C|\nαp(p − 1) .\nthus (we substituted 1 α with x, thus 0 < x ≤ 1),\nd(C,Gi) ≥ d(O) xi|C|\np(p − 1)\nFrom the submodularity of f ′(.) we can get\n∑\nv∈C\nf ′v(Gi) ≥ f ′(C ∪Gi)− f ′(Gi)\nalso the monotonity of f ′(.) suggests that\nf ′(C ∪Gi)− f ′(Gi) ≥ f ′(O) − f ′(G).\nSubsequently we have:\n∑\nv∈C\nf ′v(Gi) ≥ f ′(O)− f ′(G).\nTherefore ∑\nv∈C\nφ′v(Gi) = ∑\nv∈C\n[f ′v(Gi) + λd({v}, Gi)]\n= ∑\nv∈C\nf ′v(Gi) + λd(C,Gi)\n≥ [f ′(O) − f ′(G)] + d(O) λxi|C|\np(p − 1) .\nLet ui+1 be the element taken at step (i+ 1), then we have\nφ′ui+1(Gi) ≥ 1\np [f ′(O)− f ′(G)] + d(O)\nλxi\np(p− 1) .\nIf we sum over all i from 0 to p− 1, we have\nφ′(G) =\np−1∑\ni=0\nφ′ui+1(Gi) ≥ [f ′(O)− f ′(G)] + d(O)\nλx\n2\nHence,\nf ′(G) + λd(G) ≥ f ′(O) − f ′(G) + d(O) λx\n2\nand\nφ(G) = f(G) + λd(G) ≥ 1\n2 [f(O) + xλd(O)]\n≥ x\n2 [f(O) + λd(O)]\n= 1\n2α φ(O).\n⊓⊔\nProblem 2. Max-Sum Diversification for Matroids\nLet U be the underlying ground set, and F be the set of independent subsets of U such that M =< U,F > is a matroid. Let d(., .) be a semi-metric distance function on U and f(.) be a non-negative monotone submodular set function measuring the weight of the subsets of U . This problem aims to find a subset S ⊆ F that:\nmaximizes f(S) + λ ∑\n{u,v}:u,v∈S d(u, v)\nwhere λ is a parameter specifying a trade-off between the two objectives. Again, φ(S) is the value of the objective function. Because of the monotonicity of the φ(.), S should be a basis of the matroid M. We give a 2α−approximation for this problem.\nWithout loss of generality, we assume that the rank of the matroid is greater than one. Let\n{x, y} = argmax x,y∈F [f({x, y}) + λd(x, y)].\nWe now consider the following local search algorithm:\nAlgorithm 2 Local Search algorithm\n1: Input 2: U : set of ground elements 3: M =< U ,F >: a matroid on U 4: S: a basis of M containing both x and y 5: Output 6: S 7: while ∃{u ∈ (U − S) ∧ v ∈ S} such that S + u− v ∈ F ∧ φ(S + u− v) > φ(S) do 8: S = S + u− v 9: end while 10: return S\nTheorem 2. Algorithm 2 achieves an approximation ratio of 2α2 for max-sum diversification with a matroid constraint.\nAs the algorithm is optimal for the case that the rank of the matroid is two, we assume that the rank of the matroid is greater than two. The notation is like before and O and S are the optimal solution and the solution at the end of the local search algorithm, respectively. Let A = O ∩S, B = S−A and C = O−A. We utilize the following two lemmas from the [1].\nLemma 2. For any two sets X,Y ∈ F with |X | = |Y |, there is a bijective mapping g : X → Y such that X − x+ g(x) ∈ F for any x ∈ X.\nSince both S and O are bases of the matroid, they have the same cardinality; subsequently, B and C have the same cardinality, too. Let g : B → C be the bijective mapping results from Lemma 2 such that S − b + g(b) ∈ F for any b ∈ B. Let B = {b1, b2, ..., bt}, and let ci = g(bi) for all i. As claimed before, since the algorithm is optimal for t = 1, we assume t ≥ 2. Lemma 3. ∑t\ni=1 f(S − bi + ci) ≥ (t− 2)f(S) + f(O).\nNow we are going to prove two lemmas regarding to our semi-metric distance function. Lemma 4. If t > 2, α(d(B,C) − ∑t\ni=1 d(bi, ci)) ≥ d(C).\nProof. For any bi, cj , ck, we have\nα(d(bi, cj) + d(bi, ck)) ≥ d(cj , ck).\nSumming up these inequalities over all i, j, k with i 6= j, i 6= k, j 6= k, we have each d(bi, cj) with i 6= j is counted (t− 2) times; and each d(ci, cj) with i 6= j is counted (t− 2) times. Therefore\nα(t− 2)[d(B,C) − t∑\ni=1\nd(bi, ci)] ≥ (t− 2)d(C),\nand the lemma follows.\nLemma 5. ∑t\ni=1 d(S − bi + ci) ≥ (t− 2)d(S) + 1 α d(O).\nProof.\nt∑\ni=1\nd(S − bi + ci)\n=\nt∑\ni=1\n[d(S) + d(ci, S − bi)− d(bi, S − bi)]\n= td(S) + t∑\ni=1\nd(ci, S − bi)− t∑\ni=1\nd(bi, S − bi)\n= td(S) +\nt∑\ni=1\nd(ci, S)− t∑\ni=1\nd(ci, bi)− t∑\ni=1\nd(bi, S − bi)\n= td(S) + d(C, S) − t∑\ni=1\nd(ci, bi)− d(A,B)− 2d(B).\nThere are two cases. If t > 2 then by Lemma 4 we have\nd(C, S)− t∑\ni=1\nd(ci, bi)\n= d(A,C) + d(B,C) − t∑\ni=1\nd(ci, bi)\n≥ d(A,C) + 1\nα d(C).\nWe know that\nd(S) = d(A) + d(B) + d(A,B)\nthus we have\n2d(S)− d(A,B) − 2d(B) ≥ d(A).\nTherefore\nt∑\ni=1\nd(S − bi + ci)\n= td(S) + d(C, S)− t∑\ni=1\nd(ci, bi)− d(A,B)− 2d(B)\n≥ (t− 2)d(S) + d(A,C) + 1\nα d(C) + d(A)\n≥ (t− 2)d(S) + 1\nα d(O)\nif t = 2, then since the rank of the matroid is greater than two, A 6= ∅. Let z be an element in A, then we have\n2d(S) + d(C, S)− t∑\ni=1\nd(ci, bi)− d(A,B) − 2d(B)\n= d(A,C) + d(B,C)− t∑\ni=1\nd(ci, bi) + 2d(A) + d(A,B)\n≥ d(A,C) + d(c1, b2) + d(c2, b1) + d(A) + d(z, b1) + d(z, b2)\n≥ d(A,C) + d(A) + 1\nα d(c1, z) +\n1 α d(c2, z)\n≥ d(A,C) + d(A) + 1\nα2 d(c1, c2)\n≥ 1\nα2 (d(A,C) + d(A) + d(C))\n≥ 1\nα2 d(O)\nTherefore\nt∑\ni=1\nd(S − bi + ci)\n= td(S) + d(C, S) − t∑\ni=1\nd(ci, bi)− d(A,B) − 2d(B)\n≥ (t− 2)d(S) + 1\nα2 d(O).\nThis completes the proof.\nNow we can complete the proof of Theorem 2.\nProof. Since S is a locally optimal solution, we have φ(S) ≥ φ(S − bi + ci) for all i. Therefore for all i we have\nf(S) + λd(S) ≥ f(S − bi + ci) + λd(S − bi + ci)\nSumming up over all i, we have\ntf(S) + λtd(S) ≥ t∑\ni=1\nf(S − bi + ci) + λ t∑\ni=1\nd(S − bi + ci)\nBy Lemma 3 we know\ntf(S) + λtd(S) ≥ (t− 2)f(S) + f(O) + λ t∑\ni=1\nd(S − bi + ci)\nThen by Lemma 5 we have\ntf(S) + λtd(S) ≥ (t− 2)f(S) + f(O) + λ(t− 2)d(S) + λ\nα2 d(O)\nTherefore,\n2f(S) + 2λd(S) ≥ f(O) + λ\nα2 d(O)\nSince α ≥ 1,\n2f(S) + 2λd(S) ≥ f(O) + λ\nα2 d(O) ≥\n1\nα2 φ(O)\nφ(S) ≥ 1\n2α2 φ(O).\n⊓⊔"
    }, {
      "heading" : "Conclusion",
      "text" : "In this study we answer a proposed question in [1] about the existence of a bound on max-sum diversification problem with semi-metric distances and give a 2α-approximation for this question in the case that there is not any matroid constraint and a 2α2-approximation for an arbitrary matroid constraint. One interesting question that may be posed is whether it is possible to prove similar results for a non-monotone submodular function?"
    } ],
    "references" : [ {
      "title" : "Max-sum diversification, monotone submodular functions and dynamic updates",
      "author" : [ "A. Borodin", "H.C. Lee", "Y. Ye" ],
      "venue" : "In Proceedings of the 31st symposium on Principles of Database Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Web search using automatic classification",
      "author" : [ "C. Chekuri", "M.H. Goldwasser", "P. Raghavan", "E. Upfal" ],
      "venue" : "In Proceedings of the Sixth International Conference on the World Wide Web,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1997
    }, {
      "title" : "Less is more: probabilistic models for retrieving fewer relevant documents",
      "author" : [ "H. Chen", "D.R. Karger" ],
      "venue" : "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Novelty and diversity in information retrieval evaluation",
      "author" : [ "C.L. Clarke", "M. Kolla", "G.V. Cormack", "O. Vechtomova", "A. Ashkan", "S. Büttcher", "I. MacKinnon" ],
      "venue" : "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Comparing top k lists",
      "author" : [ "R. Fagin", "R. Kumar", "D. Sivakumar" ],
      "venue" : "SIAM Journal on Discrete Mathematics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Relaxing the triangle inequality in pattern matching",
      "author" : [ "R. Fagin", "L. Stockmeyer" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Streaming-data algorithms for high-quality clustering",
      "author" : [ "L. O’callaghan", "A. Meyerson", "R. Motwani", "N. Mishra", "S. Guha" ],
      "venue" : "In icde,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2002
    }, {
      "title" : "Shape matching: Similarity measures and algorithms",
      "author" : [ "R.C. Veltkamp" ],
      "venue" : "In Shape Modeling and Applications, SMI 2001 International Conference on.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2001
    }, {
      "title" : "Learn from web search logs to organize search results",
      "author" : [ "X. Wang", "C. Zhai" ],
      "venue" : "In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This problem was first studied by Borodin et al [1], but a crucial property used throughout their proof is the triangle inequality.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "Diversification can be viewed as a trade-off between having more relevant results and having more diverse results among the top results for a given query [3].",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 1,
      "context" : "“Jaguar” is a cliche example in the diversification literature [2, 4, 9], but it illustrates the point perfectly as it has different meanings including car, animal, and a football team.",
      "startOffset" : 63,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "“Jaguar” is a cliche example in the diversification literature [2, 4, 9], but it illustrates the point perfectly as it has different meanings including car, animal, and a football team.",
      "startOffset" : 63,
      "endOffset" : 72
    }, {
      "referenceID" : 8,
      "context" : "“Jaguar” is a cliche example in the diversification literature [2, 4, 9], but it illustrates the point perfectly as it has different meanings including car, animal, and a football team.",
      "startOffset" : 63,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "The paper by Borodin et al [1] determines the good quality results with a monotone submodular function and defines diversity as the sum of distances between selected objects.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : ", [5, 7, 8].",
      "startOffset" : 2,
      "endOffset" : 11
    }, {
      "referenceID" : 6,
      "context" : ", [5, 7, 8].",
      "startOffset" : 2,
      "endOffset" : 11
    }, {
      "referenceID" : 7,
      "context" : ", [5, 7, 8].",
      "startOffset" : 2,
      "endOffset" : 11
    }, {
      "referenceID" : 5,
      "context" : "satisfy the triangle inequality [6].",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "By modifying the analysis of the previous proposed algorithms in [1], we will show that these algorithms can still achieve a 2α-approximation for this question in the case that there is not any matroid constraint and a 2α-approximation for an arbitrary matroid constraint.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "Firstly we introduce our notations following [1].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "We utilize the following two lemmas from the [1].",
      "startOffset" : 45,
      "endOffset" : 48
    } ],
    "year" : 2015,
    "abstractText" : "In many applications such as web-based search, document summarization, facility location and other applications, the results are preferable to be both representative and diversified subsets of documents. The goal of this study is to select a good “quality”, bounded-size subset of a given set of items, while maintaining their diversity relative to a semimetric distance function. This problem was first studied by Borodin et al [1], but a crucial property used throughout their proof is the triangle inequality. In this modified proof we want to relax the triangle inequality and relate the approximation ratio of max-sum diversification problem to the parameter of the relaxed triangle inequality in the normal form of the problem (i.e., a uniform matroid) and also in an arbitrary matroid. Introduction In many search applications, the search engine should guess the correct results from a given query; therefore, it is important to deliver a diversified and representative set of documents to a user. Diversification can be viewed as a trade-off between having more relevant results and having more diverse results among the top results for a given query [3]. “Jaguar” is a cliche example in the diversification literature [2, 4, 9], but it illustrates the point perfectly as it has different meanings including car, animal, and a football team. A set of good “quality” result should cover all these diversified items. The paper by Borodin et al [1] determines the good quality results with a monotone submodular function and defines diversity as the sum of distances between selected objects. Since they consider the distances to be metric, they ask in the conclusion section: For a relaxed version of the triangle inequality can we relate the approximation ratio to the parameter of a relaxed triangle inequality? In this study we answer to this question. We call this relaxed triangle inequality distance as semi-metric. A semi-metric distance on a set of items is just like a metric distance, but the triangle inequality is relaxed with a parameter α ≥ 1 (i.e., d(u, v) ≤ α(d(v, w)+d(w, u))). Answering to this question will make this method applicable to algorithms that are defined on semi-metric spaces, e.g., [5, 7, 8]. The IBM’s Query by Image Content system is one of the other best-known examples of the semi-metric usage in practice; although, it does not 2 S. Abbasi Zadeh and M. Ghadiri satisfy the triangle inequality [6]. By modifying the analysis of the previous proposed algorithms in [1], we will show that these algorithms can still achieve a 2α-approximation for this question in the case that there is not any matroid constraint and a 2α-approximation for an arbitrary matroid constraint. In other words, these new modified analysis are a generalization of the previous analysis as they are consistent with the previous approximation ratios for α = 1 (i.e., the metric distance). Problem 1. Max-Sum Diversification Let U be the underlying ground set, and let d(., .) be a semi-metric distance function on U . The goal of the problem is to find a subset S ⊆ U that: maximizes f(S) + λ ∑ {u,v}:u,v∈S d(u, v) subject to |S| = p, where p is a given constant number and λ is a parameter specifying a trade-off between the distance and submodular function. We give a 2α−approximation for this problem. Firstly we introduce our notations following [1]. For any S ⊆ U , we let d(S) = ∑ {u,v}:u,v∈S d(u, v). We can also define d(S, T ), for any two disjoint sets S and T as: d(S ∪ T )− d(S)− d(T ). Let φ(S) and u be the value of the objective function and an element in U − S respectively. We can define the marginal gain of the distance function as du(S) = ∑ v∈S d(u, v) and similarly marginal gain of the wight function as: fu(S) = f(S + u)− f(S). The total marginal gain can also be defined using du(S) and fu(S) as φu(S) = fu(S) + λdu(S). Let f ′ u(S) = 1 2 fu(S), φu(S) = f ′ u(S) + λdu(S). Starting with an empty set S, the greedy algorithm (Algorithm 1) adds an element u from U − S in each iteration, in such a way that maximize φu(S). Lemma 1. Given an α-relaxed triangle inequality semi-metric distance function d(., .), and two disjoint sets X and Y , we have the following inequality: α(|X | − 1)d(X,Y ) ≥ |Y |d(X) Max-Sum Diversification and Semi-metric Spaces 3 Algorithm 1 Greedy algorithm 1: Input 2: U : set of ground elements 3: p: size of final set 4: Output 5: S: set of selected elements with size p 6: S = ∅ 7: while |S| < p do 8: find u ∈ U \\ S maximizing φ′ u (S) 9: S = S ∪ {u} 10: end while 11: return S Proof. Consider u, v ∈ X and an arbitrary w ∈ Y . We know that: α(d(v, w) + d(w, u)) ≥ d(u, v) By changing w we get: α(d({v}, Y ) + d({u}, Y )) ≥ |Y |d(u, v) and then all combinations of u and v: α(|X | − 1)d(X,Y ) ≥ |Y |d(X) Theorem 1. Algorithm 1 achieves a 2α-approximation for solving Problem 1 with α-relaxed distance d(., .) and monotone submodular function f . Proof. Let Gi be the greedy solution at the end of step i, i < p and G be the greedy solution at the end of the algorithm. Suppose that O is the optimal solution and let A = O ∩ Gi, B = Gi \\ A and C = O \\ A. Obviously the algorithm achieves the optimal solution when p = 1; thus we assume p > 1. Now we consider two different cases: |C| = 1 and |C| > 1. If |C| = 1 then i = p− 1. Let C = {v} and u be the element that algorithm will take for the next (last) step. Then for all v ∈ U \\ S we have: φu(Gi) ≥ φ ′ v(Gi) f ′ u(Gi) + λdu(Gi) ≥ f ′ v(Gi) + λdv(Gi) thus: φu(Gi) = fu(Gi) + λdu(Gi) ≥ f ′ u(Gi) + λdu(Gi) ≥ f ′ v(Gi) + λdv(Gi) ≥ 1 2 φv(Gi) 4 S. Abbasi Zadeh and M. Ghadiri as a result φ(G) ≥ 12φ(O) ≥ 1 2αφ(O). Now consider |C| > 1. By using Lemma 1 we have the following inequalities: α(|C| − 1)d(B,C) ≥ |B|d(C) (1) α(|C| − 1)d(A,C) ≥ |A|d(C) (2) α(|A| − 1)d(A,C) ≥ |C|d(A) (3) A and C are two disjoint sets and we know that A ∪ C = O; thus: d(A,C) + d(A) + d(C) = d(O) (4) We can assume that p > 1 and |C| > 1 (The greedy algorithm obviously finds the optimal solution when p = 1). Then following multipliers are applied to equations 1, 2, 3, 4 respectively: 1 (|C|−1) , |C|−|B| p(|C|−1) , i p(p−1) , i|C| αp(p−1) . If we add them, we have: d(B,C) + d(A,C)− d(A,C) i|C|(1 − 1 α ) p(p− 1) − d(C) i|C|(p− |C|) αp(p− 1)(|C| − 1) ≥ d(O) i|C| αp(p − 1) Since p > |C| and α ≥ 1, d(A,C) + d(B,C) ≥ d(O) i|C| αp(p − 1) . thus (we substituted 1 α with x, thus 0 < x ≤ 1), d(C,Gi) ≥ d(O) xi|C| p(p − 1) From the submodularity of f (.) we can get ∑ v∈C f ′ v(Gi) ≥ f (C ∪Gi)− f (Gi) also the monotonity of f (.) suggests that f (C ∪Gi)− f (Gi) ≥ f (O) − f (G). Subsequently we have: ∑ v∈C f ′ v(Gi) ≥ f (O)− f (G). Max-Sum Diversification and Semi-metric Spaces 5",
    "creator" : "LaTeX with hyperref package"
  }
}