{
  "name" : "1509.01240.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stability of stochastic gradient descent",
    "authors" : [ "Moritz Hardt", "Benjamin Recht", "Yoram Singer" ],
    "emails" : [ "mrtz@google.com", "brecht@berkeley.edu,", "singer@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 9.\n01 24\n0v 1\n[ cs\n.L G\n] 3\nS ep\nApplying our results to the convex case, we provide new explanations for why multiple epochs of stochastic gradient descent generalize well in practice. In the nonconvex case, we provide a new interpretation of common practices in neural networks, and provide a formal rationale for stability-promoting mechanisms in training large, deep models. Conceptually, our findings underscore the importance of reducing training time beyond its obvious benefit."
    }, {
      "heading" : "1 Introduction",
      "text" : "The most widely used optimization method in machine learning practice is stochastic gradient method (SGM). Stochastic gradient methods aim to minimize the empirical risk of a model by repeatedly computing the gradient of a loss function on a single training example, or a batch of few examples, and updating the model parameters accordingly. SGM is scalable, robust, and performs well across many different domains ranging from smooth and strongly convex problems to complex non-convex objectives.\nIn a nutshell, our results establish that:\nAny model trained with stochastic gradient method in a reasonable amount of time attains small generalization error.\nAs training time is inevitably limited in practice, our results help to explain the strong generalization performance of stochastic gradient methods observed in practice. More concretely, we bound the generalization error of a model in terms of the number of iterations that stochastic gradient method took in order to train the model. Our main analysis tool is to employ the notion of algorithmic stability due to Bousquet and Elisseeff [4]. We demonstrate that the stochastic gradient method is\n∗Email: mrtz@google.com †Email: brecht@berkeley.edu, work performed at Google. ‡Email: singer@google.com\nstable provided that the objective is relatively smooth and the number of steps taken is sufficiently small.\nIt is common in practice to perform a linear number of steps in the size of the sample and to access each data point multiple times. Our results show in a broad range of settings that, provided the number of iterations is linear in the number of data points, the generalization error is bounded by a vanishing function of the sample size. The results hold true even for complex models with large number of parameters and no explicit regularization term in the objective. Namely, fast training time by itself is sufficient to prevent overfitting.\nOur bounds are algorithm specific: Since the number of iterations we allow can be larger than the sample size, an arbitrary algorithm could easily achieve small training error by memorizing all training data with no generalization ability whatsoever. In contrast, if the stochastic gradient method manages to overfit the training data in a reasonable number of iterations, it is still guaranteed to generalize.\nConceptually, we show that minimizing training time is not only beneficial for the obvious computational advantages, but in addition has the important by-product of decreasing generalization error. Consequently, it may make sense for practitioners to focus on minimizing training time, for instance, by designing model architectures for which stochastic gradient method converges fastest to a desired error level."
    }, {
      "heading" : "1.1 Our contributions",
      "text" : "Our focus is on generating generalization bounds for models learned with stochastic gradient descent. Recall that the generalization bound is the expected difference between the error a model incurs on a training set versus the error incurred on a new data point, sampled from the same distribution that generated the training data. Throughout, we assume we are training models using n sampled data points.\nOur results build on a fundamental connection between the generalization error of an algorithm and its stability properties. Roughly speaking, an algorithm is stable if the training error it achieves varies only slightly if we change any single training data point. The precise notion of stability we use is known as uniform stability due to [4]. It states that a randomized algorithm A is uniformly stable if for all data sets differing in only one element, the learned models produce nearly the same predictions. We review this method in Section 2, and provide a new adaptation of this theory to iterative algorithms.\nIn Section 3, we show that stochastic gradient is uniformly stable, and our techniques mimic its convergence proofs. For convex loss functions, we prove that the stability measure decreases as a function of the sum of the step sizes. For strongly convex loss functions, we show that stochastic gradient is stable, even if we train for an arbitrarily long time.\nMore surprisingly, our results carry over to the case where the loss-function is non-convex. In this case we show that the method generalizes provided the steps are sufficiently small and the number of iterations is not too large. More specifically, we show the number of steps of stochastic gradient can grow as nc for a small c > 1. This provides some explanation as to why neural networks can be trained for multiple epochs of stochastic gradient and still exhibit excellent generalization. In Section 4, we furthermore show that various heuristics used in practice, especially in the deep learning community, help to increase the stability of stochastic gradient method. For example, the popular dropout scheme [19, 39] improves all of our bounds. Similarly, ℓ2-regularization improves\nthe exponent of n in our non-convex result. In fact, we can drive the exponent arbitrarily close to 1/2 while preserving the non-convexity of the problem.\nWe can combine our bounds on the generalization error of stochastic gradient method with optimization bounds quantifying the convergence of the empirical loss achieved by SGM. In Section 5, we show that models trained for multiple epochs match classic bounds for stochastic gradient [27,28]."
    }, {
      "heading" : "1.2 Related work",
      "text" : "There is a venerable line of work on stability and generalization dating back more than thirty years [4, 8, 18, 25, 38]. The landmark work by Bousquet and Elisseeff [4] introduced the notion of uniform stability that we rely on. They showed that several important classification techniques are uniformly stable. In particular, under certain regularity assumptions, it was shown that the optimizer of a regularized empirical loss minimization problem is uniformly stable. Previous work generally applies only to the exact minimizer of specific optimization problems. It is not immediately evident on how to compute a generalization bound for an approximate minimizer such as one found by using stochastic gradient. Subsequent work studied stability bounds for randomized algorithms but focused on random perturbations of the cost function, such as those induced by bootstrapping or bagging [9]. This manuscript differs from this foundational work in that it derives stability bounds about the learning procedure, analyzing algorithmic properties that induce stability.\nStochastic gradient descent, of course, is closely related to our inquiry. Classic results by Nemirovski and Yudin show that the stochastic gradient method produces are nearly optimal for empirical risk minimization of convex loss functions [11,26–28]. These results have been extended by many machine learning researchers, yielding tighter bounds and probabilistic guarantees [13,14,34]. However, there is an important limitation of all of this prior art. The derived generalization bounds only hold for single passes over the data. That is, in order for the bounds to be valid, each training example must be used no more than once in a stochastic gradient update. In practice, of course, one tends to run multiple epochs of the stochastic gradient method. Our results resolve this issue by combining stability with optimization error. We use the foundational results to estimate the error on the empirical risk and then use stability to derive a deviation from the true risk. This enables us to study the risk incurred by multiple epochs and provide simple analyses of regularization methods for convex stochastic gradient. We compare our results to this related work in Section 5. We note that Rosasco and Villa obtain risk bounds for least squares minimization with an incremental gradient method in terms of the number of epochs [36]. These bounds are akin to our study in Section 5, although our results are incomparable due to various different assumptions.\nFinally, we note that in the non-convex case, the stochastic gradient method is remarkably successful for training large neural networks [2, 19]. However, our theoretical understanding of this method is limited. Several authors have shown that the stochastic gradient method finds a stationary point of nonconvex cost functions [12, 21]. Beyond asymptotic convergence to stationary points, little is known about finding models with low training or generalization error in the nonconvex case. There have recently been several important studies investigating optimal training of neural nets. For example Livni et al. show that networks with polynomial activations can be learned in a greedy fashion [24]. Janzamin et al. [16] show that two layer neural networks can be learned using tensor methods. Arora et al. [1] show that two-layer sparse coding dictionaries can be learned via stochastic gradient. Our work complements these developments: rather than providing new insights into mechanisms that yield low training error, we provide insights into mechanisms\nthat yield low generalization error. If one can achieve low training error quickly on a nonconvex problem with stochastic gradient, our results guarantee that the resulting model generalizes well."
    }, {
      "heading" : "2 Stability of randomized iterative algorithms",
      "text" : "Consider the following general setting of supervised learning. There is an unknown joint distribution D over pairs of labeled examples from a product space X × Y. We receive a sample S = (z1, . . . , zn) of n examples drawn i.i.d. from D. Our goal is to find a model w with small population risk, defined as:\nR[w] def = Ez∼D f(w; z) .\nHere, where f is a loss function and f(w; z) designates the loss of the model described by w encountered on example z.\nSince we cannot measure the objective R[w] directly, we instead use a sample-averaged proxy, the empirical risk, defined as\nRS [w] def =\n1\nn\nn ∑\ni=1\nf(w; zi) ,\nThe generalization error of a model w is the difference RS [w]−R[w]. When w = A(S) is chosen as a function of the data by a potentially randomized algorithm A it makes sense to consider the expected generalization error\nǫgen def = E[RS [A(S)] −R[A(S)]] , (2.1)\nwhere the expectation is over the randomness of A and the sample S. In order to bound the generalization error of an algorithm, we employ with the following notion of uniform stability in which we allow randomized algorithms as well.\nDefinition 2.1. A randomized algorithm A is ǫ-uniformly stable if for all data sets S, S′ ∈ Zn such that S and S′ differ in at most one example, we have\nsup z EA\n[ f(A(S); z) − f(A(S′); z) ] ≤ ǫ . (2.2)\nHere, the expectation is taken only over the internal randomness of A. We will denote by ǫstab(A,n) the infimum over all ǫ for which (2.2) holds. We will omit the tuple (A,n) when it is clear from the context.\nWe recall the important theorem that uniform stability implies generalization in expectation. Since our notion of stability differs slightly from existing ones with respect to the randomness of the algorithm, we include a proof for the sake of completeness. The proof is very similar to the argument in Lemma 7 of [4].\nTheorem 2.2 (Generalization in expectation). Let A be ǫ-uniformly stable. Then,\n|ES,A [RS[A(S)] −R[A(S)]]| ≤ ǫ .\nProof. Denote by S = (z1, . . . , zn) and S ′ = (z′1, . . . , z ′ n) two independent random samples and let S(i) = (z1, . . . , zi−1, z ′ i, zi+1, . . . , zn) be the sample that is identical to S except in the i’th example where we replace zi with z ′ i. With this notation, we get that\nES EA [RS [A(S)]] = ES EA\n[\n1\nn\nn ∑\ni=1\nf(A(S); zi)\n]\n= ES ES′ EA\n[\n1\nn\nn ∑\ni=1\nf(A(S(i)); z′i)\n]\n= ES ES′ EA\n[\n1\nn\nn ∑\ni=1\nf(A(S); z′i)\n]\n+ δ\n= ES EA [R[A(S)]] + δ,\nwhere we can express δ as\nδ = ES ES′ EA\n[\n1\nn\nn ∑\ni=1\nf(A(S(i)); z′i)− 1\nn\nn ∑\ni=1\nf(A(S); z′i)\n]\n.\nFurthermore, taking the supremum over any two data sets S, S′ differing in only one sample, we can bound the difference as\n|δ| ≤ sup S,S′,z EA\n[ f(A(S); z) − EA f(A(S′); z) ] ≤ ǫ,\nby our assumption on the uniform stability of A. The claim follows.\nTheorem 2.2 proves that if an algorithm is uniformly stable, then its generalization is small. We now turn to some properties of iterative algorithms that control their uniform stability."
    }, {
      "heading" : "2.1 Properties of update rules",
      "text" : "We consider general update rules of the form G : Ω → Ω which map a point w ∈ Ω in the parameter space to another point G(w). The most common update is the gradient update rule\nG(w) = w − α∇f(w) ,\nwhere α ≥ 0 is a step size and f : Ω → R is a function that we want to optimize. The canonical update rule we will consider in this manuscript is an incremental gradient update, where G(w) = w − α∇f(w) for some convex function f . We will return to a detailed discussion of this specific update in the sequel, but the reader should keep this particular example in mind throughout the remainder of this section.\nThe following two definitions provide the foundation of our analysis of how two different sequences of update rules diverge when iterated from the same starting point. These definitions will ultimately be useful when analyzing the stability of stochastic gradient descent.\nDefinition 2.3. An update rule is η-expansive if\nsup v,w∈Ω ‖G(v) −G(w)‖ ‖v −w‖ ≤ η . (2.3)\nDefinition 2.4. An update rule is σ-bounded if\nsup w∈Ω\n‖w −G(w)‖ ≤ σ . (2.4)\nWith these two properties, we can establish the following lemma of how a sequence of updates to a model diverge when the training set is perturbed.\nLemma 2.5 (Growth recursion). Fix an arbitrary sequence of updates G1, . . . , GT and another sequence G′1, . . . , G ′ T . Let w0 = w ′ 0 be a starting point in Ω and define δt = ‖w′t − wt‖ where wt, w′t are defined recursively through\nwt+1 = Gt(wt) w ′ t+1 = Gt(w ′ t) . (t > 0)\nThen, we have the recurrence relation\nδ0 = 0\nδt+1 ≤ { ηδt Gt = G ′ t is η-expansive\nmin(η, 1)δt + 2σt Gt and G ′ t are σ-bounded, Gt is η expansive\n(t > 0)\nProof. The first bound on δt follow directly from the assumption that Gt = G ′ t and the definition of expansiveness. For the second bound, recall from Definition 2.4 that if Gt and G ′ t are σ-bounded, then by the triangle inequality,\nδt+1 = ‖G(wt)−G′(w′t)‖ ≤ ‖G(wt)− wt + w′t −G′(w′t)‖+ ‖wt − w′t‖ ≤ δt + ‖G(wt)− wt‖+ ‖G(w′t)− w′t‖ ≤ δt + 2σ ,\nwhich gives half of the second bound. We can alternatively bound δt+1 as\nδt+1 = ‖Gt(wt)−G′t(w′t)‖ = ‖Gt(wt)−Gt(w′t) +Gt(w′t)−G′t(w′t)‖ ≤ ‖Gt(wt)−Gt(w′t)‖+ ‖Gt(w′t)−G′t(w′t)‖ ≤ ‖Gt(wt)−Gt(w′t)‖+ ‖w′t −Gt(w′t)‖+ ‖w′t −G′t(w′t)‖ ≤ ηδt + 2σ .\nWe now turn to using these lemmas to analyze one of the core algorithms in statistical learning: the stochastic gradient method."
    }, {
      "heading" : "3 Stability of Stochastic Gradient Descent",
      "text" : "Given n labeled examples S = (z1, . . . , zn) where zi = (xi, yi) ∈ X × Y, consider a decomposable objective function\nf(w) = 1\nn\nn ∑\ni=1\nf(w; (xi, yi)),\nwhere f(w; (xi, yi)) denotes the loss of w on the example (xi, yi). The stochastic gradient update for this problem with learning rate αt > 0 is given by\nwt+1 = wt − αt∇wf(wt; zit) .\nStochastic gradient method (SGM) is the algorithm resulting from performing stochastic gradient updates T times where the indices it are randomly chosen. There are two popular schemes for choosing the examples’ indices. One is to pick it uniformly at random in {1, . . . , n} at each step. The other is to choose a random permutation over {1, . . . , n} and cycle through the examples repeatedly in the order determined by the permutation. Our results hold for both variants.\nIn parallel with the previous section the stochastic gradient method is akin to applying the gradient update rule defined as follows.\nDefinition 3.1. For a nonnegative step size α ≥ 0 and a function f : Ω → R, we define the gradient update rule Gf,α as\nGf,α(w) = w − α∇f(w) ."
    }, {
      "heading" : "3.1 Proof idea: Stability of stochastic gradient method",
      "text" : "In order to prove that the stochastic gradient method is stable, we will analyze the output of the algorithm on two data sets that differ in precisely one location. Note that if the loss function is L-Lipschitz, we have E |f(w; ) − f(w′; z)| ≤ LE ‖w − w′‖ for any w and w′. Hence, it suffices to analyze how wt and w ′ t diverge in the domain as a function of time t. Recalling that wt is obtained from wt−1 via a gradient update, our goal is to bound δt = ‖wt−w′t‖ recursively and in expectation as a function of δt−1.\nThere are two cases to consider. In the first case, SGM selects the index of an example at step t on which is identical in S and S′. In this case, the gradients added to wt and w ′ t are identical. Unfortunately, it could still be the case that δt grows. Below, we will show how to control δt in terms of the convexity and smoothness properties of the stochastic gradients.\nThe second case to consider is when SGM selects the one example to update in which S and S′ differ. Note that this happens only with probability 1/n if examples are selected randomly. In this case, we simply bound the increase in δt by the norm of the two gradient ∇f(wt−1, z) and ∇f(w′t−1; z′). The sum of the norms is bounded by 2αtL and we obtain δt ≤ δt +2αtL. Combining the two cases, we can then solve a simple recurrence relation to obtain a bound on δT .\nThis simple approach suffices to obtain the desired result in the convex case, but there are additional difficulties in the non-convex case. Here, we need to use an intriguing stability property of stochastic gradient method. Specifically, the first time step t0 at which SGM even encounters the example in which S and S′ differ is a random variable in {1, . . . , n} which tends to be relatively large. Specifically, for any m ∈ {1, . . . , n}, the probability that t0 ≤ m is upper bounded by m/n. This allows us to argue that SGM has a long “burn-in period” where δt does not grow at all. Once δt begins to grow, the step size has already decayed allowing us to obtain a non-trivial bound.\nWe now turn to making this argument precise."
    }, {
      "heading" : "3.2 Expansion properties of stochastic gradients",
      "text" : "Let us now record some of the core properties of the stochastic gradient update. The gradient update rule is bounded provided that the function f satisfies the following common Lipschitz condition.\nDefinition 3.2. We say that f is L-Lipschitz if for all points u in the domain of f we have ‖∇f(x)‖ ≤ L. This implies that\n|f(u)− f(v)| ≤ L‖u− v‖ . (3.1)\nLemma 3.3. Assume that f is L-Lipschitz. Then, the gradient update Gf,α is (αL)-bounded.\nProof. By our Lipschitz assumption, ‖w −Gf,α(w)‖ = ‖α∇f(w)‖ ≤ αL .\nWe now turn to expansiveness. As we will see shortly, different expansion properties are achieved for non-convex, convex, and strongly convex functions.\nDefinition 3.4. A function f : Ω → R is convex if for all u, v ∈ Ω we have\nf(u) ≥ f(v) + 〈∇f(v), u− v〉 .\nDefinition 3.5. A function f : Ω → R is γ-strongly convex if for all u, v ∈ Ω we have\nf(u) ≥ f(v) + 〈∇f(v), u− v〉+ γ 2 ‖u− v‖2 .\nThe following standard notion of smoothness leads to a bound on how expansive the gradient update is.\nDefinition 3.6. A function f : Ω → R is β-smooth if for all for all u, v ∈ Ω we have\n‖∇f(u)−∇f(v)‖ ≤ β‖u− v‖ . (3.2)\nIn general, smoothness will imply that the gradient updates cannot be overly expansive. When the function is also convex and the step size is sufficiently small the gradient update becomes non-expansive. When the function is additionally strongly convex, the gradient update becomes contractive in the sense that η will be less than one and u and v will actually shrink closer to one another. The majority of the following results can be found in several textbooks and monographs. Notable references are Polyak [33] and Nesterov [29]. We include proofs in the appendix for completeness.\nLemma 3.7. Assume that f is β-smooth. Then, the following properties hold.\n1. Gf,α is (1 + αβ)-expansive.\n2. Assume in addition that f is convex. Then, for any α ≤ 2/β, the gradient update Gf,α is 1-expansive.\n3. Assume in addition that f is γ-strongly convex. Then, for α ≤ 2β+γ , Gf,α is ( 1− 2αβγβ+γ ) -\nexpansive.\nWe will also make use of the following convenient lemma for analyzing the stability of the stochastic gradient method.\nLemma 3.8. Assume that the loss function f(· ; z) is L-Lipschitz for all z. Let S and S′ be two samples of size n differing in only a single example. Denote by wT and w ′ T the output of T steps of SGM on S and S′, respectively. Then, for every z ∈ X × Y and every t0 ∈ {0, 1, . . . , n}, under both the random update rule and the random permutation rule, we have\nE ∣ ∣f(wT ; z) − f(w′T ; z) ∣ ∣ ≤ t0 n + LE [δT | δt0 = 0] .\nProof. Let S and S′ be two samples of size n differing in only a single example, and let z ∈ X × Y be an arbitrary example. Consider running SGM on sample S and S′, respectively. As stated, wT and w ′ T denote the corresponding outputs of SGM. Let E = 1[δt0 = 0] denote the event that δt0 = 0. We have,\nE\n∣ ∣f(wT ; z)− f(w′T ; z) ∣ ∣ = P {E}E [ ∣ ∣f(wT ; z)− f(w′T ; z) ∣ ∣ | E ]\n+ P {Ec}E [ ∣ ∣f(wT ; z) − f(w′T ; z) ∣ ∣ | Ec ]\n≤ E [ ∣ ∣f(wT ; z)− f(w′T ; z) ∣ ∣ | E ] + P {Ec} ≤ LE [∥\n∥wT − w′T ∥ ∥ | E ] + P {Ec} .\nThe first inequality uses the fact that |f(wT ; z)− f(w′T ; z)| ≤ 1, from our assumption on the range of the function f. The second inequality follows from the Lipschitz assumption.\nIt remains to bound P {Ec} . Toward that end, let i∗ ∈ {1, . . . , n} denote the position in which S and S′ differ and consider the random variable I assuming the index of the first time step in which SGM uses the example zi∗ . Note that when I > t0, then we must have that δt0 = 0, since the execution on S and S′ is identical until step t0. Hence,\nP {Ec} = P {δt0 6= 0} ≤ P {I ≤ t0} .\nUnder the random permutation rule, I is a uniformly random number in {1, . . . , n} and therefore\nP {I ≤ t0} = t0 n .\nThis proves the claim we stated for the random permutation rule. For the random selection rule, we have by the union bound P {I ≤ t0} ≤ ∑t0 t=1 P {I = t} = t0n . This completes the proof.\nHenceforth we will no longer mention which random selection rule we use as the proofs are almost identical for both rules."
    }, {
      "heading" : "3.3 Convex optimization",
      "text" : "We begin with a simple stability bound for convex loss minimization via stochastic gradient method.\nTheorem 3.9. Assume that the loss function f(· ; z) ∈ [0, 1] is β-smooth, convex and L-Lipschitz for every z. Suppose that we run SGM with step sizes αt ≤ 2/β for T steps. Then, SGM satisfies uniform stability with\nǫstab ≤ 2L2\nn\nT ∑\nt=1\nαt .\nProof. Let S and S′ be two samples of size n differing in only a single example. Consider the gradient updates G1, . . . , GT and G ′ 1, . . . , G ′ T induced by running SGM on sample S and S\n′, respectively. Let wT and w ′ T denote the corresponding outputs of SGM.\nWe now fix an example z ∈ X × Y and apply Lemma 3.8 to f(· ; z) with t0 = 0, to get\nE ∣ ∣f(wT ; z) − f(w′T ; z) ∣ ∣ ≤ LE [δT | δ0 = 0] = LE [δT ] , (3.3)\nwhere δt = ‖wT − w′T ‖. Observe that at step t, with probability 1 − 1/n, the example selected by SGM is the same in both S and S′. In this case we have that Gt = G ′ t and we can use the 1- expansivity of the update rule Gt which follows from Lemma 3.7.2 using the fact that the objective function is convex and that αt ≤ 2/β. With probability 1/n the selected example is different in which case we use that both Gt and G ′ t are αtL-bounded as a consequence of Lemma 3.3. Hence, we can apply Lemma 2.5 and linearity of expectation to conclude that for every t,\nE [δt+1] ≤ ( 1− 1 n ) E [δt] + 1 n E [δt] + 2αtL n = E [δt] + 2Lαt n . (3.4)\nUnraveling the recursion gives\nE [δT ] ≤ 2L\nn\nT ∑\nt=1\nαt .\nPlugging this back into equation (3.3), we obtain\nE\n∣ ∣f(wT ; z) − f(w′T ; z) ∣\n∣ ≤ 2L 2\nn\nT ∑\nt=1\nαt .\nSince this bounds holds for all S, S′ and z, we obtain the desired bound on the uniform stability."
    }, {
      "heading" : "3.4 Strongly convex optimization",
      "text" : "In the strongly convex case our bound has no dependence on the number of steps at all.\nTheorem 3.10. Assume that the loss function f(· ; z) ∈ [0, 1] is γ-strongly convex, L-Lipschitz and β-smooth for all z. Suppose we run SGM with constant step size α ≤ 1/β for T steps. Then, SGM satisfies uniform stability with\nǫstab ≤ 2L2\nγn .\nProof. The proof is analogous to that of Theorem 3.9 with a slightly different recurrence relation. We repeat the the argument of completeness. Let S and S′ be two samples of size n differing in only a single example. Consider the gradient updates G1, . . . , GT and G ′ 1, . . . , G ′ T induced by running SGM on sample S and S′, respectively. Let wT and w ′ T denote the corresponding outputs of SGM.\nApplying Lemma 3.8 with t0 = 0, we obtain\nE ∣ ∣f(wT ; z) − f(w′T ; z) ∣ ∣ ≤ LE [δT | δ0 = 0] = LE [δT ] , (3.5)\nwhere δt = ‖wT − w′T ‖. Observe that at step t, with probability 1 − 1/n, the example selected by SGM is the same in both S and S′. In this case we have that Gt = G ′ t. We can now apply the following useful simplification of Lemma 3.7.3 if α ≤ 1/β. Since we also require that α ≤ 2β+γ we\nget that 2αβγβ+γ ≥ α2βγ ≥ αγ, thus Gf,α is (1 − αγ)-expansive. With probability 1/n the selected example is different in which case we use that both Gt and G ′ t are αL-bounded as a consequence of Lemma 3.3 which appears in the next section. Hence, we can apply Lemma 2.5 and linearity of expectation to conclude that for every t,\nE δt+1 ≤ ( 1− 1 n ) (1− αγ)E δt + 1 n (1− αγ)E δt + 2αL n (3.6)\n= (1− αγ)E δt + 2αL\nn\nUnraveling the recursion gives\nE δT ≤ 2Lα\nn\nT ∑\nt=0\n(1− αγ)t ≤ 2L γn .\nPlugging the above inequality into equation (3.3), we obtain\nE ∣ ∣f(wT ; z)− f(w′T ; z) ∣\n∣ ≤ 2L 2\nγn .\nSince this bounds holds for all S, S′ and z, the lemma follows.\nWe would like to note that a nearly identical result holds for a “staircase” decaying step-size that is also popular in machine learning and stochastic optimization.\nTheorem 3.11. Assume that the loss function f(· ; z) ∈ [0, 1] is γ-strongly convex, L-Lipschitz, and β-smooth function for all z. Suppose we run SGM with step sizes αt = 1 γt . Then, SGM has uniform stability of\nǫstab ≤ 2L2 + β\nγn .\nProof. Note that once t > βγ , the iterates are contractive with contractivity 1−αtγ ≤ 1− 1t . Thus, for t ≥ t0 := βγ , we have\nE[δt+1] ≤ (1− 1n)(1− αtγ)E[δt] + 1n((1 − αtγ)E[δt] + 2αtL)\n= (1− αtγ)E[δt] + 2αtL\nn\n=\n(\n1− 1 t\n)\nE[δt] + 2L\nγtn .\nAssuming that δt0 = 0 and expanding this recursion, we find.\nE[δT ] ≤ T ∑\nt=t0\n{\nT ∏\ns=t+1\n(\n1− 1 s\n)\n}\n2L γtn =\nT ∑\nt=t0\nt\nT\n2L γtn = T − t0 T 2L γn .\nNow, the result follows from Lemma 3.8 while the fact that t0 = β γ ."
    }, {
      "heading" : "3.5 Non-convex optimization",
      "text" : "In this section we prove stability results for stochastic gradient methods that do not require convexity. We will still assume that the objective function is smooth and Lipschitz as defined previously.\nTheorem 3.12. Assume that f(·; z) ∈ [0, 1] is an L-Lipschitz and β-smooth loss function for every z. Suppose that we run SGM for T steps with monotonically non-increasing step sizes αt ≤ c/t. Then, SGM has uniform stability with\nǫstab ≤ 1 + 1/βc\nn− 1 (2cL 2)\n1 βc+1T βc βc+1\nIn particular, omitting constant factors that depend on β, c, and L, we get\nǫstab / T 1−1/(βc+1)\nn .\nProof. Let S and S′ be two samples of size n differing in only a single example. Consider the gradient updates G1, . . . , GT and G ′ 1, . . . , G ′ T induced by running SGM on sample S and S\n′, respectively. Let wT and w ′ T denote the corresponding outputs of SGM.\nBy Lemma 3.8, we have for every t0 ∈ {1, . . . , n},\nE ∣ ∣f(wT ; z) − f(w′T ; z) ∣ ∣ ≤ t0 n + LE [δT | δt0 = 0] , (3.7)\nwhere δt = ‖wT −w′T ‖. To simplify notation, let ∆t = E [δT | δt0 = 0] . We will bound ∆t a function of t0 and then minimize for t0.\nToward this goal, observe that at step t, with probability 1−1/n, the example selected by SGM is the same in both S and S′. In this case we have that Gt = G ′ t and we can use the (1 + αtβ)expansivity of the update rule Gt which follows from our smoothness assumption via Lemma 3.7.1. With probability 1/n the selected example is different in which case we use that both Gt and G ′ t are αtL-bounded as a consequence of Lemma 3.3. Hence, we can apply Lemma 2.5 and linearity of expectation to conclude that for every t ≥ t0,\n∆t+1 ≤ ( 1− 1 n ) (1 + αtβ)∆t + 1 n ∆t + 2αtL n\n≤ ( 1\nn + (1− 1/n)(1 + cβ/t)\n)\n∆t + 2cL\ntn\n=\n(\n1 + (1− 1/n)cβ t\n)\n∆t + 2cL\ntn\n≤ exp ( (1− 1/n)cβ t ) ∆t + 2cL tn .\nHere we used that 1 + x ≤ exp(x) for all x.\nUsing the fact that ∆t0 = 0, we can unwind this recurrence relation from T down to t0 + 1. This gives\n∆T ≤ T ∑\nt=t0+1\n{\nT ∏\nk=t+1\nexp (\n(1− 1n) βc k\n)\n}\n2cL\ntn\n= T ∑\nt=t0+1\nexp\n(\n(1− 1n)βc T ∑\nk=t+1\n1 k\n)\n2cL\ntn\n≤ T ∑\nt=t0+1\nexp ( (1− 1n)βc log(Tt ) ) 2cL\ntn\n= 2cL\nn T βc(1−1/n)\nT ∑\nt=t0+1\nt−βc(1−1/n)−1\n≤ 1 (1− 1/n)βc 2cL n\n(\nT\nt0\n)βc(1−1/n)\n≤ 2L β(n− 1)\n(\nT\nt0\n)βc\n,\nPlugging this bound into (3.7), we get\nE\n∣ ∣f(wT ; z)− f(w′T ; ) ∣ ∣ ≤ t0 n + 2L2 β(n − 1)\n(\nT\nt0\n)βc\n.\nLetting q = βc, the right hand side is approximately minimized when\nt0 = ( 2cL2 )\n1 q+1 T q q+1 .\nThis setting gives us\nE\n∣ ∣f(wT ; z)− f(w′T ; z) ∣ ∣ ≤ 1 + 1/q n− 1 ( 2cL2 ) 1 q+1 T q q+1 = 1 + 1/βc n− 1 (2cL 2) 1 βc+1T βc βc+1 .\nSince the bound we just derived holds for all S, S′ and z, we immediately get the claimed upper bound on the uniform stability."
    }, {
      "heading" : "4 Stability-inducing operations",
      "text" : "In light of our results, it makes sense to analyse for operations that increase the stability of the stochastic gradient method. We show in this section that pleasingly several popular heuristics and methods indeed improve the stability of SGM. Our rather straightforward analyses both to strengthen the bounds we previously obtained and provides an explanation of empirical success of the methods we overview.\nWeight Decay and Regularization. Weight decay is a simple and effective that often improves generalization [20].\nDefinition 4.1. Let f : Ω → Ω, be a differentiable function. We define the gradient update with weight decay at rate µ as Gf,µ,α(w) = (1− αµ)w − α∇f(w).\nIt is easy to verify that the above update rule is equivalent to performing a gradient update on the ℓ2-regularized objective g(w) = f(w) + µ 2 ‖w‖2.\nLemma 4.2. Assume that f is β-smooth. Then, Gf,µ,α is (1 + α(β − µ))-expansive.\nProof. Let G = Gf,µ,α. By triangle inequality and our smoothness assumption,\n‖G(v) −G(w)‖ ≤ (1− αµ)‖v − w‖+ α‖∇f(w)−∇f(v)‖ ≤ (1− αµ)‖v − w‖+ αβ‖w − v‖ = (1− αµ + αβ)‖v − w‖ .\nThe above lemma shows as that a regularization parameter µ counters a smoothness parameter β. Once r > β, the gradient update with decay becomes contractive. Any theorem we proved in previous sections that has a dependence on β leads to a corresponding theorem for stochastic gradient with weight decay in which β is replaced with β − µ.\nGradient Clipping. It is common when training deep neural networks to enforce bounds on the norm of the gradients encountered by SGD. This is often done by either truncation, scaling, or dropping of examples that cause an exceptionally large value of the gradient norm. Any such heuristic directly leads to a bound on the Lipschitz parameter L that appears in our bounds. It is also easy to introduce a varying Lipschitz parameter Lt to account for possibly different values.\nDropout. Dropout [39] is a popular and effective heuristic for preventing large neural networks from overfitting. Here we prove that, indeed, dropout improves all of our stability bounds generically. From the point of view of stochastic gradient descent, dropout is equivalent to setting a fraction of the gradient weights to zero. That is, instead of updating with a stochastic gradient ∇f(w; z) we instead update with a perturbed gradient D∇f(w; z) which is is typically identical to ∇f(w; z) in some of the coordinates and equal to 0 on the remaining coordinates, although our definition is a fair bit more general.\nDefinition 4.3. We say that a randomized map D : Ω → Ω is a dropout operator with dropout rate s if for every v ∈ D we have E ‖Dv‖ = s‖v‖. For a differentiable function f : Ω → Ω, we let DGf,α denote the dropout gradient update defined as DGf,α(v) = v − αP∇f(v)\nAs expected, the dropout update improves the effective Lipschitz constant of the objective function.\nLemma 4.4. Assume that f is L-Lipschitz. Then, the dropout update DGf,α with dropout rate s is (sαL)-bounded.\nProof. By our Lipschitz assumption and linearity of expectation,\nE ‖Gf,α(v)− v‖ = αE ‖D∇f(v)‖ = αsE ‖∇f(v)‖ ≤ αsL, .\nFrom this lemma we can obtain various corollaries by replacing L with sL in our theorems.\nProjections and Proximal Steps. Related to regularization, there are many popular updates which follow a stochastic gradient update with a projection onto a set or some statistical shrinkage operation. The vast majority of these operations can be understood as applying a proximal-point operation associated with a convex function. Similar to the gradient operation, we can define the proximal update rule.\nDefinition 4.5. For a nonnegative step size α ≥ 0 and a function f : Ω → R, we define the proximal update rule Pf,α as\nPf,α(w) = argmin v\n1 2 ‖w − v‖2 + αf(v) . (4.1)\nFor example, Euclidean projection is the proximal point operation associated with the indicator of the associated set. Soft-thresholding is the proximal point operator associated with the ℓ1-norm. For more information, see the surveys by Combettes and Wajs [6] or Parikh and Boyd [32].\nAn elementary proof of the following Lemma, due to Rockafellar [35], can be found in the appendix.\nLemma 4.6. If f is convex, the proximal update (4.1) is 1-expansive.\nIn particular, this Lemma implies that the Euclidean projection onto a convex set is 1-expansive. Note that in many important cases, proximal operators are actually contractive. That is, they are η-expansive with η < 1. An notable example is when f(·) is the Euclidean norm for which the update rule is η-expansive with η = (1 + α)−1. So stability can be induced by the choice of an appropriate prox-operation, which can always be interpreted as some form of regularization.\nModel Averaging. Model averaging refers to the idea of averaging out the iterates wt obtained by a run of SGD. In convex optimization, model averaging is sometimes observed to lead to better empirical performance of SGM and closely replated updates such as the Perceptron [10]. Here we show that model averaging improves our bound for the convex optimization by a constant factor.\nTheorem 4.7. Assume that f : Ω → [0, 1] is a decomposable convex L-Lipschitz β-smooth function and that we run SGD with step sizes αt ≤ α ≤ 2/β for T steps. Then, the average of the first T iterates of SGD has uniform stability of ǫstab ≤ αTL 2\nn .\nProof. Let w̄T = 1 T\nT ∑\nt=1\nwt denoet the average of the stochastic gradient iterates. Since\nwt = t ∑\nk=1\nα∇f(wk; (xk, yk)) ,\nwe have\nw̄T = α\nT ∑\nt=1\nT − t+ 1 T ∇f(wk; (xk, yk))\nUsing Lemma 3.8, the deviation between w̄t and w̄ ′ t obeys\nδt ≤ (1− 1/n)δt−1 + 1\nn\n(\nδt−1 + 2αL T − t+ 1\nT\n)\n.\nwhich implies\nδT ≤ 2αL\nn\nT ∑\nt=1\nT − t+ 1 T = αL(T + 1) n .\nSince f is L-Lipschitz, we have\nE |f(w̄T )− f(w̄′T )| ≤ L‖w̄T − w̄′T ‖ ≤ α(T + 1)L2\nn .\nHere the expectation is taken over the algorithm and hence the claim follows by our definition of uniform stability."
    }, {
      "heading" : "5 Convex risk minimization",
      "text" : "We now outline how our generalization bounds lead to bounds on the population risk achieved by SGM in the convex setting. We restrict our attention to the convex case where we can contrast against known results. The main feature of our results is that we show that one can achieve bounds comparable or perhaps better than known results on stochastic gradient for risk minimization by running for multiple passes over the data set.\nThe key to the analysis in this section is to decompose the risk estimates into an optimization error term and a stability term. The optimization error designates how closely we optimize the empirical risk or a proxy of the empirical risk. By optimizing with stochastic gradient, we will be able to balance this optimization accuracy against how well we generalize. These results are inspired by the work of Bousquet and Bottou who provided similar analyses for SGM based on uniform convergence [3]. However, our stability results will yield sharper bounds.\nThroughout this section, our risk decomposition works as follows. We define the optimization error to be the gap between the empirical risk and minimum empirical risk in expectation:\nǫopt(w) def = E [ RS [w]−RS [wS⋆ ] ] where wS⋆ = argminw RS [w] .\nBy Theorem 2.2, the expected risk of a w output by SGM is bounded as\nE[R[w]] ≤ E[RS [w]] + ǫstab ≤ E[RS [wS⋆ ]] + ǫopt(w) + ǫstab\nIn general, the optimization error decreases with the number of SGM iterations while the stability increases. Balancing these two terms will thus provide a reasonable excess risk against the empirical risk minimizer. Note that our analysis involves the expected minimum empirical risk which could be considerably smaller than the minimum risk. However, as we now show, it can never be larger.\nLemma 5.1. Let w⋆ denote the minimizer of the population risk and w S ⋆ denote the minimizer of the empirical risk given a sampled data set S. Then E[RS [w S ⋆ ]] ≤ R[w⋆].\nProof.\nR[w⋆] = inf w R[w] = inf w Ez[f(w; z)]\n= inf w ES\n[\n1 n\nn ∑\ni=1\nf(w; zi))\n]\n≥ inf w ES\n[\n1 n\nn ∑\ni=1\nf(wS⋆ ; zi))\n]\n= ES\n[\n1 n\nn ∑\ni=1\nf(wS⋆ ; zi)\n]\n= E[RS [w S ⋆ ]] .\nTo analyze the optimization error, we will make use of a classical result due to Nemirovski and Yudin [28].\nTheorem 5.2. Assume we run stochastic gradient descent with constant stepsize α on a convex function\nR[w] = Ez[f(w; z)] .\nAssume further that ‖∇f(w; z)‖ ≤ L and ‖w0 − w⋆‖ ≤ D for some minimizer w⋆ of J . Let w̄T denote the average of the T iterates of the algorithm. Then we have\nR[w̄T ] ≤ R[w⋆] + 12 D2\nTα + 12L 2α .\nThe upper bound stated in the previous theorem is known to be tight even if the function is β-smooth [28]\nIf we plug in the population risk for J in the previous theorem, we directly obtain a generalization bound for SGM that holds when we make a single pass over the data. The theorem requires fresh samples from the distribution in each update step of SGM. Hence, given n data points, we cannot make more than n steps, and each sample must not be used more than once.\nCorollary 5.3. Let f be a convex loss function satisfying ‖∇f(w, z)‖ ≤ L and let w⋆ be a minimizer of the population risk R[w] = Ez f(w; z). Suppose we make a single pass of SGM over the sample S = (z1, . . . , zn) with a suitably chosen fixed step size starting from a point w0 that satisfies ‖w0 − w⋆‖ ≤ D. Then, the average w̄n of the iterates satisfies\nE[R[w̄n]] ≤ R[w⋆] + DL√ n . (5.1)\nWe now contrast this bound with what follows from our results.\nProposition 5.4. Let S = (z1, . . . , zn) be a sample of size n. Let f be a β-smooth convex loss function satisfying ‖∇f(w, z)‖ ≤ L and let wS⋆ be a minimizer of the empirical risk RS [w] =\n1 n ∑n i=1 f(w; zi). Suppose we run T steps of SGM with suitably chosen step size from a starting point w0 that satisfies ‖w0 −wS⋆ ‖ ≤ D. Then, the average w̄T over the iterates satisfies\nE[R[w̄T ]] ≤ E[RS [wS⋆ ]] + DL√ n\n√\nn+ 2T\nT .\nProof. On the one hand, applying Theorem 5.2 to the empirical risk RS , we get\nǫopt(w̄T ) ≤ 12 D2\nTα + 12L 2α .\nHere, wS⋆ is an empirical risk minimizer. On the other hand, by our stability bound from Theorem 4.7,\nǫstab ≤ TL2α\nn\nCombining these two inequalities we have,\nE[R[w̄T ]] ≤ E[Remp[wS⋆ ]] + 12 D2\nTα + 12L 2\n(\n1 + 2T\nn\n)\nα\nChoosing α to be\nα = D √ n\nL √ T (n+ 2T ) ,\nyields the bound provided in the proposition.\nNote that the bound from our stability analysis is not directly comparable to Corollary 5.3 as we are comparing against the expected minimum empirical risk rather than the minimum risk. Lemma 5.1 implies that the excess risk in our bound is at most worse by a factor of √ 3 compared with Corollary 5.3 when T = n. Moreover, the excess risk in our bound tends to a factor merely√ 2 larger than the Nemirovski-Yudin bound as T goes to infinity. In contrast, the classical bound does not apply when T > n."
    }, {
      "heading" : "6 Future Work and Open Problems",
      "text" : "Our analysis parts from much previous work in that we directly analyze the generalization performance of an algorithm rather than the solution of an optimization problem. In doing so we build on the toolkit usually used to prove that algorithms converge in objective value.\nThis approach could be more powerful than analyzing optimality conditions, as it may be easier to understand how each data point affects a procedure rather than an optimal solution. It also has the advantage that the generalization bound holds even if the algorithm fails to find a unique optimal solution as is common in non-convex problems.\nIn addition to this broader perspective on algorithms for learning, there are many exciting theoretical and empirical directions that we intend to pursue in future work. We close this manuscript by detailing some of the more concrete open problems and avenues for direct empirical validation.\nHigh Probability Bounds The results in this paper are all in expectation. Similar to the wellknown proofs of the stochastic gradient method, deriving bounds on the expected risk is relatively straightforward, but high probability bounds need more attention and care [26, 34]. In the case of stability, the standard techniques from Bousquet and Elisseeff require uniform stability on the order of O(1/n) to apply exponential concentration inequalities like McDiaramid’s [4]. For larger values of the stability parameter ǫstab, it is more difficult to construct such high probability bounds. In our setting, things are further complicated by the fact that our algorithm is itself randomized, and thus a concentration inequality must be devised to account for both the randomness in the data and in the training algorithm. Since differential privacy and stability are closely related, one possibility is to derive concentration via an algorithmic method, similar to the one developed by Nissim and Stemmer [31].\nStability of the gradient method Since gradient descent can be considered a “limiting case” of the stochastic gradient descent method, one can use an argument like our Growth Recursion Lemma (Lemma 2.5) to analyze its stability. Such an argument provides an estimate of ǫstab ≤ αTn where α is the step size and T is the number of iterations. Generic bounds for convex functions suggest that gradient descent achieves an optimization error of O(1/T ). Thus, a generalization bound of O(1/ √ n) is achievable, but at a computational complexity of O(n1.5). SGM, on the other hand, achieves a generalization of O(1/ √ n) in time O(n).\nIn the non-convex case, we are unable to prove any reasonable form of stability at all. We are unable to apply our “burn-in” argument to the gradient method, and thus trajectories may diverge exponentially quickly. Poor generalization behavior of gradient descent has been observed in practice, but lower bounds for this approach are necessary to rule out a stable implementation for non-convex machine learning.\nAcceleration and momentum We have described how many of the best practices in neural net training can be understood as stability inducing operations. One very important technique that we did not discuss is momentum. In momentum methods, the update is a linear combination of the current iterate and the previous direction. For convex problems, momentum is known to decrease the number of iterations required by stochastic gradient descent [22]. For general nonlinear problems, is believed to decrease the number of iterations required to achieve low-training error [33,37]. However, it is not clear that momentum adds stability. Indeed, in the case of convex optimization, momentum methods are less robust to noise than gradient methods [7, 23]. Thus, it is possible that momentum speeds up training but adversely impacts generalization.\nEmpirical Validation Most notably, we hope to investigate whether these theoretical developments parallel empirical behavior. For such an investigation, it is necessary to devise efficient methods to measure the stability exhibited by stochastic gradient. We can then determine if the models that generalize in practice are indeed the most stable ones. Moreover, we can verify whether minor algorithmic modifications that induce stability also result in the models which generalize best. We expect to perform such experiments in the near future.\nModel Selection Another related avenue that bridges theory and practice is using stability as a method for model selection. In particular, our results imply that the models that train the fastest also generalize the best. This suggests that a heuristic for model selection would be to run many\ndifferent parameter settings and choose the model which results in the lowest training error most quickly. This idea is relatively simple to try in practice, and ideas from bandit optimization can be applied to efficiently search with this heuristic cost [15, 17]. From the theoretical perspective, understanding the sensibility of this heuristic would require understanding lower bounds for generalizability. Are there necessary conditions which state that models which take a long training time by SGM generalize less well than those with short training times?\nHigh capacity models that train quickly If the models can be trained quickly via stochastic gradient, our results prove that these models will generalize. However, this manuscript provides no guidance as to how to build a model where training is stable and training error is low. Designing a family of models which both has high capacity and can be trained quickly would be of significant theoretical and practical interest.\nIndeed, the capacity of models trained in current practice steadily increases as growing computational power makes it possible to effectively train larger models. It is not uncommon for some models, such as large neural networks, to have more free parameters than the size of the sample yet have rather small generalization error [19,40]. In fact, sometimes increasing the model capacity even seems to decrease the generalization error [30]. Is it possible to understand this phenomena via stability? How can we find models which provably both have high capacity and train quickly?\nAlgorithm Design Finally, we note that stability may also provide new ideas for designing learning rules. There are a variety of successful methods in machine learning and signal processing that do not compute an exact stochastic gradient, yet are known to find quality stationary points in theory and practice [5]. Do the ideas developed in this paper provide new insights into how to design learning rules that accelerate the convergence and improve the generalization of SGM?"
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank John Duchi, Vineet Gupta, Kevin Jamieson, Eric Price, Nati Srebro, and Oriol Vinyals for their insightful feedback and helpful suggestions."
    }, {
      "heading" : "A Elementary properties of convex functions",
      "text" : "Proof of Lemma 3.7.1. Let G = Gf,α. By triangle inequality and our smoothness assumption,\n‖G(v) −G(w)‖ ≤ ‖v − w‖+ α‖∇f(w)−∇f(v)‖ ≤ ‖v − w‖+ αβ‖w − v‖ = (1 + αβ)‖v − w‖ .\nProof of Lemma 3.7.2. Convexity and β-smoothness implies that the gradients are co-coercive, namely\n〈∇f(v)−∇f(w), v − w〉 ≥ 1 β ‖∇f(v)−∇f(w)‖2 . (A.1)\nWe conclude that\n‖Gf,α(v)−Gf,α(w)‖2 = ‖v − w‖2 − 2α〈∇f(v) −∇f(w), v − w〉+ α2‖∇f(v)−∇f(w)‖2\n≤ ‖v − w‖2 − (\n2α β − α\n2 )\n‖∇f(v)−∇f(w)‖2\n≤ ‖v − w‖2 .\nProof of Lemma 3.7.3. First, note that if f is γ strongly convex, then ϕ(w) = f(w) − γ2‖w‖2 is convex with (β − γ)-smooth. Hence, applying (A.1) to ϕ yields the inequality\n〈∇f(v)−∇f(w), v − w, 〉 ≥ βγ β + γ ‖v − w‖2 + 1 β + γ ‖∇f(v)−∇f(w)‖2\nUsing this inequality gives\n‖Gf,α(v)−Gf,α(w)‖ = ‖v −w‖2 − 2α〈∇f(v) −∇f(w), v − w, 〉+ α2‖∇f(v)−∇f(w)‖2\n≤ ( 1− 2 αβγ β + γ ) ‖v − w‖2 − α ( 2 β + γ − α ) ‖∇f(v)−∇f(w)‖2 .\nThe lemma follows.\nProof of Lemma 4.6. This proof is due to Rockafellar [35]. Define\nPν(w) = argmin v\n1\n2ν ‖w − v‖2 + f(v) . (A.2)\nThis is the proximal mapping associated with f . Define the map Qν(w) := w − Pν(w). Then, by the optimality conditions associated with (A.2), we have\nν−1Qν(w) ∈ ∂f(Pν(w)) .\nBy convexity of f , we then have\n〈Pν(v)− Pν(w), Qν(v)−Qν(w)〉 ≥ 0 .\nUsing this inequality, we have\n‖v − w‖2 = ‖[Pν(v)− Pν(w)] + [Qν(v) −Qν(w)]‖2\n= ‖Pν(v)− Pν(w)‖2 + 2〈Pν(v)− Pν(w), Qν(v) −Qν(w)〉+ ‖Qν(v)−Qν(w)‖2 ≥ ‖Pν(v)− Pν(w)‖2 ,\nthus completing the proof."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "<lb>We show that any model trained by a stochastic gradient method with few iterations has<lb>vanishing generalization error. We prove this by showing the method is algorithmically stable<lb>in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex<lb>and continuous optimization. Our results apply to both convex and non-convex optimization<lb>under standard Lipschitz and smoothness assumptions.<lb>Applying our results to the convex case, we provide new explanations for why multiple<lb>epochs of stochastic gradient descent generalize well in practice. In the nonconvex case, we<lb>provide a new interpretation of common practices in neural networks, and provide a formal<lb>rationale for stability-promoting mechanisms in training large, deep models. Conceptually, our<lb>findings underscore the importance of reducing training time beyond its obvious benefit.",
    "creator" : "LaTeX with hyperref package"
  }
}