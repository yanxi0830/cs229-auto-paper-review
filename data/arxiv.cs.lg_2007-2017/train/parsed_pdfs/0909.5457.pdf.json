{
  "name" : "0909.5457.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Guaranteed Rank Minimization via Singular Value Projection",
    "authors" : [ "Raghu Meka", "Prateek Jain", "Inderjit S. Dhillon" ],
    "emails" : [ "inderjit}@cs.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :0\n90 9.\n54 57\nv1 [\ncs .L\nG ]\n3 0\nSe p"
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper we study the general affine rank minimization problem (ARMP),\nmin rank(X) s.t A(X) = b, X ∈ Rm×n, b ∈ Rd, (ARMP)\nwhere A is an affine transformation from Rm×n to Rd. The general affine rank manimization problem is of considerable practical interest and many important machine learning problems such as matrix completion, low-dimensional metric embedding, low-rank kernel learning can be viewed as instances of the above problem. Unfortunately, ARMP is NP-hard in general and is also NP-hard to approximate ([MJCD08]).\nUntil recently, most known methods for ARMP were heuristic in nature with few known rigorous guarantees. The most commonly used heuristic for the problem is to assume a factorization of X and optimize the resulting non-convex problem by alternating minimization [Bra03, Kor08, MB07], alternative projections [GB00] or alternating LMIs [SIG97]. Another common approach is to relax the rank constraint to a convex function such as the trace-norm or the log determinant [FHB01], [FHB03]. However, most of these methods do not have any optimality guarantees. Recently, Meka et al. [MJCD08] proposed online learning based methods for ARMP. However, their methods can only guarantee at best a logarithmic approximation for the minimum rank.\nIn a recent breakthrough, Recht, Fazel and Parillo [RFP07] obtained the first non-trivial exactrecovery results for ARMP obtaining guaranteed rank minimization for affine transformations A that satisfy a restricted isometry property (RIP). Define the isometry constant of A, δk to be the smallest number such that for all X ∈ Rm×n of rank at most k,\n(1− δk)‖X‖2F ≤ ‖A(X)‖22 ≤ (1 + δk)‖X‖2F . (1)\nRecht et al. show that for affine constraints with bounded isometry constants (specifically, δ5k < 1/10), finding the minimum trace-norm solution recovers the minimum rank solution. However, their results only address the case of exact measurements and are hard to analyse. Moreover, even the best existing optimization algorithms for their convex relaxation problem are relatively inefficient in practice.\nIn this paper we propose a simple and fast algorithm SVP (Singular Valur Projection) based on the classical projected gradient algorithm. We present a simple analysis showing that SVP recovers the minimum rank solution for affine constraints that satisfy RIP even in the presence of noise and prove the following guarantees.\nTheorem 1.1. Suppose the isometry constant of A satisfies δ2k ≤ 1/3 and let b = A(X∗) for a rank-k matrix X∗. Then, SVP (Algorithm 1) with step-size ηt = 1/(1 + δ2k) converges to X\n∗. Furthermore, SVP outputs a matrix X of rank at most k such that ‖A(X) − b‖22 ≤ ǫ in at most ⌈\n1 log((1−δ2k)/2δ2k) log ‖b‖ 2 2ǫ\n⌉\niterations.\nTheorem 1.2 (Main). Suppose the isometry constant of A satisfies δ2k ≤ 1/3 and let b = A(X∗)+e for a rank k matrix X∗ and an error vector e ∈ Rd. Then, SVP with step-size ηt = 1/(1 + δ2k) outputs a matrix X of rank at most k such that ‖A(X) − b‖22 ≤ (C2 + ǫ)‖e‖2, ǫ ≥ 0, in at most ⌈\n1 log(1/D) log\n‖b‖2\n(C2+ǫ)‖e‖2\n⌉\niterations for universal constants C,D.\nOur analysis of SVP is motivated by the recent work in the field of conpressed sensing by Blumensath and Davies [BD09], Garg and Khandekar [GK09]. Our results improve the results of Recht et al. as follows.\n1. SVP is considerably simpler to analyze than minimizing the trace-norm. Further, we need weaker isometry assumptions on A than those of Recht et al. [RFP07]: we only require δ2k < 1/3 as opposed to δ5k < 1/10 required by Recht et al.\n2. We show robustness of SVP to noise, whereas the results of Recht et al. [RFP07] only address the case of exact measurements.\n3. SVP has a strong geometric convergence rate and is faster than using the best trace-norm optimization algorithms by an order of magnitude.\nWe also remark that to the best of our knowledge ours is the only work with provable guarantees for rank minimization for ARMP in the presence of noise.\nThough restricted isometry property is natural in settings where the affine constraints contain information about all the entries of the unknown matrix, in several cases of considerable practical interest the affine constraints only contain local information and may not satisfy RIP directly.\nOne such important problem where RIP does not hold directly is the low-rank matrix completion problem. In the matrix completion problem we are given the entries of an unknown low-rank matrix X∗ for ordered pairs (i, j) ∈ Ω ⊆ [m]× [n] and the goal is to complete the missing entries of X∗. A highly popular application of the matrix completion problem is in the field of collaborative filtering, where typically the task is to predict user ratings given past ratings of the users. Recently, a lot of attention has been given to the problem due to the Netflix Challenge [Net]. Other applications of matrix completion include triangulation from incomplete data, link prediction in social networks etc.\nSimilar to ARMP, the low-rank matrix completion is also NP-hard in general and most methods are heuristic in nature with no theoretical guarantees. The alternating least squares minimization heuristic and its variants [Kor08, MB07] perform the best in practice but are notoriously hard to analyze.\nRecently, Candes and Recht [CR08], Candes and Tao [CT09] and Keshavan et al. [KOM09] obtained the first non-trivial results for low-rank matrix completion under a few additional assumptions. Broadly, these works give exact-recovery guarantees when the optimal solution X∗ is µ-incoherent (see Definition 4.1), and the entries Ω are chosen uniformly at random with |Ω| ≥ C(µ, k)n poly log n. However, the algorithms of the above works, even when using methods taylored specifically for matrix-completion such as those of Cai et al. [CCS08], are quite expensive in practice and not very tolerant to noise.\nAs low-rank matrix completion is a special case of ARMP, we can naturally adapt our algorithm SVP for matrix completion. We demonstrate empirically that for a suitable step-size, SVP significantly outperforms the methods of [CR08], [CT09], [CCS08], [KOM09] in accuracy, computational time and tolerance to noise. Furthermore, our experiments strongly suggest (see Figure 1) that guarantees similar to those of [CT09], [KOM09] hold for SVP, achieving exact recovery for incoherent matrices from an almost optimal number of entries1.\nAlthough we do not provide a rigorous proof of exact-recovery for SVP applied to matrix completion, we make partial progress in this direction and give strong intuition for the performance of SVP. We prove that though the affine constraints defining the matrix-completion problems do not obey the restricted isometry property, they obey the restricted isometry property over incoherent matrices. This weaker RIP condition along with a hypothesis bounding the incoherence of the iterates of SVP imply exact-recovery of a low-rank incoherent matrix from an almost optimal number of entries. We also provide strong empirical evidence supporting our hypothesis bounding the incoherence of the iterates of SVP (see Figure 2).\n1It follows from a coupon collector argument that exact-recovery from random samples requires nk log n samples.\nWe first present our algorithm SVP in Section 2 and present its analysis for affine constraints satisfying RIP in Section 3. In Section 4, we specialize our algorithm SVP to the task of lowrank matrix completion and prove a more restricted isometry property for the matrix completion problem. In Section 6, we give empirical results for SVP applied to ARMP and matrix-completion on real-world and synthetic problems."
    }, {
      "heading" : "2 Singular Value Projection (SVP)",
      "text" : "Consider the following robust formulation of ARMP (RARMP),\nmin X\nψ(X) = 1\n2 ‖A(X)− b‖22 s.t X ∈ C(k) = {X : rank(X) ≤ k}. (1)\nThe hardness of the above problem mainly comes from the non-convexity of the set of low-rank matrices C(k). However, in spite of the hardness of the rank constraint, the Euclidean projection onto the non-convex set C(k) can be computed efficiently using singular value decomposition. Our algorithm uses this observation along with the projected gradient method for efficiently minimizing the objective function specified in problem (1).\nLet Pk : Rm×n → Rm×n denote the orthogonal projection on to the set C(k). That is, Pk(X) = argminY {‖Y − X‖F : Y ∈ C(k)}. It is well known that Pk(X) can be computed efficiently by computing the top k singular values and vectors of X.\nIn SVP, a candidate solution to ARMP is computed iteratively by starting from the all-zero matrix and adapting the classical projected gradient descent update as follows (Observe that ∇ψ(X) = AT (A(X) − b)) :\nXt+1 ← Pk ( Xt − ηt∇ψ(Xt) ) = Pk ( Xt − ηtAT (A(Xt)− b) ) . (2)\nAlgorithm 1 presents our SVP algorithm. Note that the iterates Xt are always low-rank, facilitating faster computation of the SVD. See Section 5 for a more detailed discussion of the computational issues.\n3 Analysis for Affine Constraints Satisfying RIP\nWe now show that SVP solves exact rank minimization for affine constraints that satisfy RIP and prove our main results, Theorems 1.1 and 1.2. We first present a lemma that bounds the error at\nAlgorithm 1 Singular Value Projection (SVP) Algorithm Require: A, b, tolerance ε, ηt, t = 0, 1, 2, . . . 1: Initialize: X0 = 0 and t = 0 2: repeat 3: Y t+1 ← Xt − ηtAT (A(Xt)− b) 4: Compute top k singular vectors of Y t+1: Uk, Σk, Vk 5: Xt+1 ← UkΣkV Tk 6: until ‖A(Xt+1)− b‖22 ≤ ε\n(t + 1)-th iteration (ψ(Xt+1)) w.r.t. the error incurred by the optimal solution (ψ(X∗)) and the t-th iteration.\nLemma 3.1. Let Xt be the iterate obtained by SVP algorithm at t-th iteration. Then,\nψ(Xt+1) ≤ ψ(X∗) + δ2k (1− δ2k) ‖A(X∗ −Xt)‖22,\nwhere δ2k is the rank 2k isometry constant of A.\nProof. Recall that ψ(X) = 12‖A(X)− b‖22. Since ψ(·) is a quadratic function, we have\nψ(Xt+1)− ψ(Xt) = 〈∇ψ(Xt),Xt+1 −Xt〉+ 1 2 ‖A(Xt+1 −Xt)‖22\n≤ 〈AT (A(Xt)− b),Xt+1 −Xt〉+ 1 2 · (1 + δ2k) · ‖Xt+1 −Xt‖2F , (1)\nwhere inequality (1) follows from RIP applied to the matrix Xt+1 − Xt of rank at most 2k. Let Y t+1 = Xt − 11+δ2kA T (A(Xt)− b) and\nft(X) = 〈AT (A(Xt)− b),X −Xt〉+ 1\n2 · (1 + δ2k) · ‖X −Xt‖2F .\nThen,\nft(X) = 1\n2 (1 + δ2k)\n[ ‖X −Xt‖2F + 2 〈AT (A(Xt)− b)\n1 + δ2k ,X −Xt\n〉]\n= 1\n2 (1 + δ2k)‖X − Y t+1‖2F −\n1\n2(1 + δ2k) · ‖AT (A(Xt)− b)‖2F .\nNow, by definition, Pk(Y t+1) = Xt+1 is the minimizer of ft(X) over all matrices X ∈ C(k) of rank at most k. In particular, ft(X t+1) ≤ ft(X∗). Thus,\nψ(Xt+1)− ψ(Xt) ≤ ft(Xt+1) ≤ ft(X∗) = 〈AT (A(Xt)− b),X∗ −Xt〉+ 1\n2 (1 + δ2k)‖X∗ −Xt‖2F\n≤ 〈AT (A(Xt)− b),X∗ −Xt〉+ 1 2 · 1 + δ2k 1− δ2k ‖A(X∗ −Xt)‖22 (2) = ψ(X∗)− ψ(Xt) + δ2k (1− δ2k) ‖A(X∗ −Xt)‖22,\nwhere inequality (2) follows from RIP applied to X∗ −Xt.\nWe now prove that SVP obtains the optimal solution for ARMP with restricted isometry property.\nProof of Theorem 1.1. Using Lemma 3.1 and the fact that ψ(X∗) = 0 for the noise-less case,\nψ(Xt+1) ≤ δ2k (1− δ2k) ‖A(X∗ −Xt)‖22 = 2δ2k (1− δ2k) ψ(Xt).\nAlso, note that for δ2k < 1/3, 2δ2k (1−δ2k) < 1. Hence, ψ(Xτ ) ≤ ǫ where τ =\n⌈\n1 log((1−δ2k)/2δ2k) log ψ(X 0) ǫ\n⌉\n.\nNow, the SVP algorithm is initialized usingX0 = 0, i.e., ψ(X0) = ‖b‖ 2 2 . Hence, τ = ⌈ 1 log((1−δ2k)/2δ2k) log ‖b‖ 2 2ǫ ⌉ .\nNext, we prove the noisy version of Theorem 1.1.\nProof of Theorem 1.2. Let the current solution Xt satisfy ψ(Xt) ≥ C2‖e‖2/2, where C ≥ 0 is a universal constant. Using Lemma 3.1 and the fact that b−A(X∗) = e,\nψ(Xt+1) ≤ ‖e‖ 2 2\n2 + δ2k (1− δ2k) ‖b−A(Xt)− e‖22,\n≤ ‖e‖ 2 2\n2 + 2δ2k (1− δ2k)\n(\nψ(Xt)− eT (b−A(Xt)) + ‖e‖ 2\n2\n)\n,\n≤ ψ(X t)\nC2 + 2δ2k (1− δ2k)\n(\nψ(Xt) + 2\nC ψ(Xt) +\n1\nC2 ψ(Xt)\n)\n,\n≤ ( 1\nC2 + 2δ2k (1− δ2k)\n(\n1 + 1\nC\n)2 )\nψ(Xt)\n= Dψ(Xt),\nwhere D = (\n1 C2 + 2δ2k(1−δ2k) ( 1 + 1C )2 )\n. Recall that δ2k < 1/3. Hence, selecting C > (1 + δ2k)/(1 − 3δ2k), we get D < 1. Also, ψ(X\n0) = ψ(0) = ‖b‖2/2. Hence, ψ(Xτ ) ≤ (C2 + ǫ)‖e‖2/2 where τ = ⌈\n1 log(1/D) log\n‖b‖2\n(C2+ǫ)‖e‖2\n⌉\n."
    }, {
      "heading" : "4 Matrix Completion",
      "text" : "We first describe the low-rank matrix completion problem formally. Let PΩ : Rm×n → Rm×n denote the projection onto the index set Ω. That is, (PΩ(X))ij = Xij for (i, j) ∈ Ω and (PΩ(X))ij = 0 otherwise. Then, the low-rank matrix completion problem (MCP) can be formulated as follows,\nmin X\nrank(X) s.t PΩ(X) = PΩ(X∗), X ∈ Rm×n. (MCP)\nObserve that the matrix completion problem is a special case of ARMP. However, the affine constraints that define MCP, PΩ, do not satisfy RIP in general. Thus Theorems 1.1, 1.2 above and the results of Recht et al. [RFP07] do not directly apply to MCP. The first non-trivial results for MCP were obtained recently by Candes and Recht [CR08], Keshavan et al. [KOM09] and Candes and Tao [CT09]. These works show exact recovery of the unknown matrix X∗ when the observed entries are sampled uniformly and X∗ is incoherent in the sense defined below.\nDefinition 4.1 (Incoherence). A rank-k matrix X ∈ Rm×n with singular value decomposition X = UΣV T is µ-incoherent if\nmax i,j |Uij| ≤ √ µ√ m , max i,j |Vij | ≤ √ µ√ n .\nIntuitively, high incoherence implies that the non-zero entries of X are not concentrated in a small number of entries. Hence, a random sampling of the matrix should provide enough information to reconstruct the entire matrix.\nAs matrix completion is a special case of ARMP, we can apply SVP for matrix completion. We apply SVP to matrix-completion with step-size ηt = 1/(1 + δ)p, where p is the density of sampled entries, leading to the update\nXt+1 ← Pk ( Xt − 1 (1 + δ)p (PΩ(Xt)− PΩ(X∗)) ) . (1)\nWe now provide some intuition for our choice of step-size ηt and make partial progress towards proving that SVP achieves exact recovery for low-rank incoherent matrices. We show that though the affine constraints defining MCP, PΩ, do not satisfy RIP for all low-rank matrices, they satisfy RIP for all low-rank incoherent matrices. Thus, if the iterates appearing in SVP remain incoherent throughout the execution of the algorithm, then Theorem 1.1 would imply recovery of the unknown entries of the matrix. Empirical evidence strongly supports our hypothesis that the incoherence of the iterates arising in SVP remains bounded.\nFigure 1 plots the threshold sampling density beyond which matrix completion for randomly generated matrices is solved exactly by SVP for fixed k and varying matrix sizes n. Note that the density threshold matches the optimal bound of O(k log n/n) with the constant being C = 1.28. Figure 2 plots the maximum incoherence maxt µ(X t) = √ n maxt,i,j |U tij |, where U t are the left singular vectors of the intermediate iterates Xt computed by SVP. The figure clearly shows that the incoherence µ(Xt) of the iterates is bounded by a constant independent of the matrix size n and density p throughout the execution of SVP.\nFix an incoherent matrix X ∈ Rm×n of rank at most k and let Ω be sampled according to the Bernoulli model with each (i, j) ∈ Ω independently with probability p. Then, E[‖PΩ(X)‖2F ] = p‖X‖2F . Further, by Chernoff bounds, for δ > 0, p ≥ Ck2 log n/m for a universal constant C, with high probability\n(1− δ)p ‖X‖2F ≤ ‖PΩ(X)‖2F ≤ (1 + δ)p ‖X‖2F . (2) Combining the above Chernoff bound estimate with a union bound over low-rank incoherent matrices, we obtain the following restricted isometry property for the projection operator PΩ restricted to low-rank incoherent matrices.\nTheorem 4.2. There exists a constant C ≥ 0 such that the following holds for all 0 < δ < 1, µ ≥ 1, n ≥ m ≥ 3: For Ω ⊆ [m] × [n] chosen according to the Bernoulli model with density p ≥ Cµ2k2 log n/δ2m, with probability at least 1 − exp(−n log n), the restricted isometry property in (2) holds for all µ-incoherent matrices X of rank at most k.\nMotivated by the above theorem and supported by empirical evidence (Figures 1, 2) we hypothesize that SVP achieves exact recovery from an almost optimal number of samples.\nConjecture 4.3. Fix µ, k and δ ≤ 1/3. Then, there exists a constant C such that for a µincoherent matrix X∗ of rank at most k and Ω sampled from the Bernoulli model with density p ≥ Cµ2k2 log n/m, SVP with step-size ηt = 1/(1 + δ)p converges to X∗ with high probability. Moreover, SVP outputs a matrix X of rank at most k such that ‖PΩ(X) − PΩ(X∗)‖2F ≤ ǫ after Oµ,k (⌈ log ( 1 ǫ )⌉) iterations.\n4.1 RIP for Matrix Completion on Incoherent Matrices\nWe now prove the RIP property of Theorem 4.2 for the projection operator PΩ. To prove Theorem 4.2 we first show the theorem for a discrete collection of matrices using Chernoff type large-deviation bounds and use standard quantization arguments to generalize to the continuous case. We first introduce some notation.\nDefinition 4.4. For a matrix X ∈ Rm×n, let ‖X‖mx = maxi,j |Xij | and call X α-regular if\n‖X‖mx ≤ α√ mn · ‖X‖F .\nWe need Bernstein’s inequality [Wik09] stated below.\nLemma 4.5 (Bernstein’s inequality). Let X1,X2, . . . ,Xn be independent random variables with E[Xi] = 0,∀i. Furthermore, let |Xi| ≤ M . Then,\nP [ ∑\ni\nXi > t] ≤ exp ( − t 2/2 ∑\nV ar(Xi) +Mt/3\n)\n.\nLemma 4.6. Fix an α-regular X ∈ Rm×n and 0 < δ < 1. Then, for Ω ⊆ [m]× [n] chosen according to the Bernoulli model, with each pair (i, j) ∈ Ω chosen independently with probability p,\nPr[ ∣ ∣‖PΩ(X)‖2F − p‖X‖2F ∣ ∣ ≥ δp‖X‖2F ] ≤ 2 exp ( −δ 2pmn\n3α2\n)\n.\nProof. For (i, j) ∈ [m] × [n], let ωij be the indicator variables with ωij = 1 if (i, j) ∈ Ω and 0 otherwise. Then, ωij are independent random variables with Pr[ωij = 1] = p. Let random variable Zij = ωijX 2 ij . Note that,\nE[Zij ] = pX 2 ij, V ar(Zij) = p(1− p)X4ij .\nObserve that |Zij − E[Zij ]| < |Xij |2 < (α2/mn) · ‖X‖2F . Thus,\nM = max i,j\n|Zij − E[Zij ]| ≤ α2\nmn ‖X‖2F . (3)\nNow, define random variable S = ∑ i,j Zij = ∑ i,j ωijX 2 ij = ‖PΩ(X)‖2F . Note that, E[S] = p‖X‖2F . Since, Zij are independent random variables,\nV ar(S) = ∑\ni,j\np(1− p)X4ij ≤ p (max i,j X2ij) · ∑\ni,j\nX2ij ≤ pα2\nmn ‖X‖4F . (4)\nUsing Bernstein’s inequality (Lemma 4.5) for S with t = δp‖X‖2F and Equations (3) and (4) we get,\nPr[|S − E[S]| > t] ≤ 2 exp ( −t2/2 V ar(Z) +Mt/3 )\n≤ 2 exp ( −δ 2pmn\n3α2\n)\n.\nWe now discretize the space of low-rank incoherent matrices so as to be able to use the above lemma with a union bound. We need the following simple lemmas. Lemma 4.7. Let X ∈ Rm×n be a µ-incoherent matrix of rank at most k. Then X is µ √ k-regular.\nProof. Let X = UΣV T be the singular value decomposition of X. Then, Xij = UiΣV T j , where Ui, Vj are the i’th and j’th rows of U, V respectively. Now,\n|Xij | = |eTi UΣV T ej | = | k ∑\nl=1\nUilΣllVjl| ≤ k ∑\nl=1\nΣll|Uil||Vjl|.\nSince X is µ-incoherent,\n|Xij | ≤ k ∑\nl=1\nΣll|Uil||Vjl| ≤ µ√ mn\n· ( k ∑\nl=1\nΣll) ≤ µ√ mn\n· √ k · ( k ∑\nl=1\nΣ2ll) 1/2 =\nµ √ k√\nmn · ‖X‖F .\nLemma 4.8. Let a, b, c, x, y, z ∈ [−1, 1]. Then,\n|abc− xyz| ≤ |a− x|+ |b− y|+ |c− z|.\nThe following lemma shows that the space of low-rank µ-incoherent matrices can be discretized into a reasonably small set of regular matrices such that every low-rank µ-incoherent matrix is close to a matrix from the set.\nLemma 4.9. For all 0 < ǫ < 1/2, µ ≥ 1, m,n ≥ 3 and k ≥ 1, there exists a set S(µ, ǫ) ⊆ Rm×n with |S(µ, ǫ)| ≤ (mnk/ǫ)3 (m+n)k such that the following holds. For any µ-incoherent X ∈ Rm×n of rank k with ‖X‖2 = 1, there exists Y ∈ S(µ, ǫ) such that ‖Y −X‖F < ǫ and Y is (4µ √ k)-regular.\nProof. We construct S(µ, ǫ) by discretizing the space of low-rank incoherent matrices. Let ρ = ǫ/ √ 9k2mn and D(ρ) = {ρ i : i ∈ Z, |i| ≤ ⌊1/ρ⌋}. Let\nU(ρ) = {U ∈ Rm×k : Uij ∈ ( √ µ/m) ·D(ρ) },\nV (ρ) = {V ∈ Rn×k : Vij ∈ ( √ µ/n) ·D(ρ) }, Σ(ρ) = {Σ ∈ Rk×k : Σij = 0, i 6= j, Σii ∈ D(ρ)},\nS(µ, ǫ) = {UΣV T : U ∈ U(ρ),Σ ∈ Σ(ρ), V ∈ V (ρ) }. We will show that S(µ, ǫ) satisfies the conditions of the Lemma. Observe that |D(ρ)| < 1/ρ. Thus,\n|U(ρ)| < (1/ρ)mk, |V (ρ)| < (1/ρ)nk, |Σ(ρ)| < (1/ρ)k .\nHence, |S(µ, ǫ)| < (1/ρ)mk+nk+k < (mnk/ǫ)3(m+n)k . Fix a µ-incoherent X ∈ Rm×n of rank at most k with ‖X‖2 = 1. Let the singular value decomposition of X be X = UΣV T . Let U1 be the matrix obtained by rounding entries of U to integer multiples of √ µ ρ/ √ m as follows: for (i, l) ∈ [m]× [k], let\n(U1)il = √ µρ√ m · ⌊ Uil √ m√ µρ ⌋ .\nNow, since Uil ≤ √ µ/ √ m, it follows that U1 ∈ U(ρ). Further, for all i ∈ [m], l ∈ [k],\n|(U1)il − Uil| < √ µ√ m ρ ≤ ρ.\nSimilarly, define V1,Σ1 by rounding entries of V,Σ to integer multiples of √ µ ρ/ √ n and ρ respectively. Then, V1 ∈ V (ρ), Σ1 ∈ Σ(ρ) and for (j, l) ∈ [n]× [k],\n|(V1)jl − Vjl| < √ µρ√ n ≤ ρ, |(Σ1)ll − Σll| < ρ.\nLet X(ρ) = U1Σ1V T 1 . Then, by the above equations and Lemma 4.8, for i ∈ [m], l ∈ [k], j ∈ [n],\n|(U1)il(Σ1)ll(V1)jl − UilΣllVjl| < 3ρ.\nThus, for i, j ∈ [m]× [n],\n|X(ρ)ij −Xij | = | k ∑\nl=1\n(U1)il(Σ1)ll(V1)jl − UilΣllVjl|\n≤ k ∑\nl=1\n|(U1)il(Σ1)ll(V1)jl − UilΣllVjl|\n< 3kρ. (5)\nUsing Lemma 4.7 and Equation (5)\n‖X(ρ)‖mx < ‖X‖mx + 3kρ ≤ µ √ k√\nmn · ‖X‖F + ǫ√ mn .\nAlso, using (5),\n‖X(ρ) −X‖2F = ∑\ni,j\n|X(ρ)ij −Xij |2 < 9k2mnρ2 = ǫ2.\nFurthermore, using triangular inequality, ‖X(ρ)‖F > ‖X‖F − ǫ > ‖X‖F /2. Since, ǫ < 1 and µ √ k‖X‖F ≥ 1,\n‖X(ρ)‖mx < 2µ √ k√\nmn · ‖X‖F <\n4µ √ k√\nmn · ‖X(ρ)‖F .\nThus, X(ρ) is 4µ √ k-regular. The lemma now follows by taking Y = X(ρ).\nWe now prove Theorem 4.2 by combining Lemmas 4.6 and 4.9.\nProof of Theorem 4.2. Let m ≤ n, ǫ = δ/9mnk and\nS′(µ, ǫ) = {Y : Y ∈ S(µ, ǫ), Y is 4µ √ k-regular},\nwhere S(µ, ǫ) is as in Lemma 4.9. Then, by Lemma 4.2 and union bound,\nPr [ ∣ ∣‖PΩ(Y )‖2F − p‖Y ‖2F ∣ ∣ ≥ δp‖Y ‖2F for some Y ∈ S′(µ, ǫ) ]\n≤ 2 ( mnk\nǫ\n)3(m+n)k\nexp (−δ2pmn 16µ2k )\n≤ exp(C1nk log n) · exp (−δ2pmn\n16µ2k\n)\n,\nwhere C1 ≥ 0 is a constant independent of m,n, k. Thus, if p > Cµ2k2 log n/δ2m, where C = 16(C1+1), with probability at least 1−exp(−n log n), the following holds ∀Y ∈ S′(µ, ǫ), |‖PΩ(Y )‖2F − p‖Y ‖2F | ≤ δp‖Y ‖2F . (6)\nAs the statement of the theorem is invariant under scaling, it is enough to show the statement for all µ-incoherent matrices X of rank at most k and ‖X‖2 = 1. Fix such a X and suppose that Equation (6) holds. Now, by Lemma 4.9 there exists Y ∈ S′(µ, ǫ) such that ‖Y −X‖F ≤ ǫ. Moreover,\n‖Y ‖2F ≤ (‖X‖F + ǫ)2 ≤ ‖X‖2F + 2ǫ‖X‖F + ǫ2 ≤ ‖X‖2F + 3ǫk.\nProceeding similarly, we can show that\n|‖X‖2F − ‖Y ‖2F | ≤ 3ǫk. (7)\nFurther, starting with ‖PΩ(Y −X)‖F ≤ ‖Y −X‖F ≤ ǫ and arguing as above we get that\n|‖PΩ(Y )‖2F − ‖PΩ(X)‖2F | ≤ 3ǫk. (8)\nCombining inequalities (7), (8) above, we have\n|‖PΩ(X)‖2F − p‖X‖2F | ≤ |‖PΩ(X)‖2F − ‖PΩ(Y )‖2F |+ p |‖X‖2F − ‖Y ‖2F |+ |‖PΩ(Y )‖2F − p‖Y ‖2F | ≤ 6ǫk + δp‖Y ‖2F Equations (6), (7), (8) ≤ 6ǫk + δp(‖X‖2F + 3ǫk) Equation (7) ≤ 9ǫk + δp‖X‖2F ≤ 2δp‖X‖2F . Since ‖X‖2F ≥ 1\nThe theorem now follows."
    }, {
      "heading" : "5 Computational Issues and Related Work",
      "text" : "Minimizing the trace-norm of a matrix subject to affine constraints can be cast as a semi-definite programming problem. However, algorithms for semi-definite programming, as used by most methods for minimizing trace-norm, are prohibitively expensive even for moderately large datasets. Recently, a variety of methods mostly based on iterative soft-thresholding have been proposed to solve the trace-norm minimization problem efficiently. For instance, Cai et al. [CCS08] proposed a Singular Value Thresholding (SVT) algorithm which is based on Uzawa’s algorithm[AHU58]. A related approach based on linearized Bregman iteration was proposed by Ma et al. [MGC09].\nToh and Yun [TY09], Ji and Ye [JY09] proposed Nesterov’s projected gradient based methods for optimizing the trace-norm.\nWhile the soft-thresholding based methods for trace-norm minimization are significantly faster than semi-definite programming approaches, they suffer from an important bottleneck: though the final solution to the trace-norm minimization is a low-rank matrix, the rank of the iterate in intermediate iterations can be large. In contrast, the rank of the iterates in our method is always equal to the rank of the optimal solution.\nAlso, though minimizing the trace-norm does most likely approximate the low-rank solution even in the presence of noise (see [CP09] for instance), noise poses considerable computational challenges for trace-norm optimization. Cai et al. propose a variant of SVT for handling noise that performs moderately well for uniformly bounded noise. However, the performance of SVT worsens considerably in the presence of outlier noise. SVP on the other hand is robust to both outlier and uniformly bounded noise as it minimizes the cumulative loss function ‖A(X) − b‖22.\nFor the case of low-rank matrix completion, Candes and Recht [CR08] obtained the first nontrivial results for the problem obtaining guaranteed completion for incoherent matrices X∗ and randomly sampled entries Ω. Candes and Recht show that for X∗ µ-incoherent and Ω chosen at random with |Ω| ≥ C(µ) k2n1.2, trace-norm relaxation recovers the optimal solution. Building on the work of Candes and Recht, Candes and Tao [CT09] obtained the near-optimal bound of |Ω| ≥ min(Cµ4k2n log2 n,Cµ2kn log6 n) for exact-recovery via trace-norm minimization. However, the analysis of Candes and Recht, Candes and Tao is considerably complicated and minimizing trace-norm, even when using methods taylored for matrix-completion such as those of [CCS08], is relatively expensive in practice.\nFor the case of matrix completion, SVT has the important property that the intermediate iterations of the algorithm only require computing the singular value decomposition of a sparse matrix. This facilitates the use of fast SVD computing package such as PROPACK [Lar] that only require subroutines that compute matrix-vector products.\nOur SVP algorithm has a similar property facilitating fast computation of the update in equation (1); each iteration of SVP involves computing the SVD of the matrix Y = Xt+PΩ(Xt−X∗), where Xt is a matrix of rank at most k whose SVD we know and PΩ(Xt −X∗) is a sparse matrix. Thus, we can compute matrix-vector products of the form Y x in time O((m+ n)k + |Ω|).\nIn a different line of work, Keshavan et al. [KOM09] obtained exact-recovery from uniformly sampled Ω with |Ω| ≥ C(µ, k)n log n using different technqiues. The first iteration of SVP is similar to the first step of Keshavan et al. However, after the first iteration, Keshavan et al. use a sophisticated alternating minimization algorithm based on gradient descent on the Grassmannian manifold of low-rank matrices. However, convergence of their alternating minimization algorithm is slow. The simplicity of the updates in SVP makes it both easier to implement and significantly less computationally intensive than the alternating minimization algorithm of Keshavan et al.\nA related problem to the matrix completion problem is the problem of low-rank plus sparse decomposition of a matrix addressed by Chandrasekaran et al. [CSPW09] andWright et al. [WGRM09]. Interestingly, Wright et al. [WGRM09] show that the low-rank matrix completion problem can be reduced to the low-rank plus sparse decomposition problem. Here again, their method relies on the trace-norm relaxation and is significantly more computationally intensive than our algorithm."
    }, {
      "heading" : "6 Experimental Results",
      "text" : "In this section, we empirically evaluate our SVP method for the affine rank minimization and lowrank matrix completion problems. For both problems we present empirical results on synthetic as well as real-world datasets. For ARMP we compare our method against the trace-norm based\nsingular value thresholding (SVT) method [CCS08]. Note that although [CCS08] presents SVT method in the context of matrix completion problem, however it can be easily adapted for ARMP. For matrix completion we compare against SVT, the spectral matrix completion (SMC) method of [KOM09], and regularized alternating least squares minimization (ALS). We use our own implentation of SVT for ARMP and ALS, while for matrix completion we use the code provided by the respective authors for SVT and SMC. We report results averaged over 20 runs. All the methods are implemented in Matlab and uses mex files."
    }, {
      "heading" : "6.1 Affine Rank Minimization",
      "text" : "We first compare our method against SVT on random instances of ARMP. We generate random matrices X ∈ Rn×n of different sizes n and fixed rank k = 5. We then generate d = 6kn random affine constraint matrices Ai, 1 ≤ i ≤ d and compute b = A(X). Figure 3 (a) compares the computational time required by SVP and SVT for achieving a relative error (‖A(X) − b‖2/‖b‖2) of 10−3. Our method requires many fewer iterations and is signficantly faster than SVT.\nNext we evaluate our method for the problem of matrix reconstruction from random measurements. As in Recht et al. [RFP07], we use the MIT logo as the test image for reconstruction. the MIT logo we use is a 38 × 73 image and has rank four. For reconstruction, we generate random measurement matrices Ai and measure bi = Tr(AiX). Figure 3 (a) shows that our method incurs significantly smaller reconstruction error than SVT with lower number of iterations."
    }, {
      "heading" : "6.2 Matrix Completion",
      "text" : "Next, we evaluate our method against various matrix completion methods for random low-rank matrices and uniform samples. We generate a random rank k matrix X ∈ Rn×n and generate random Bernoulli samples with probability p. Figure 4 compares the time required by various methods to obtain a root mean square error (RMSE) of 10−3. Clearly, our method is substantially faster than both SVT and SMC, and is competitive with ALS.\nNext, we study the behavior of our method when there are outliers in the sampled entries. For this experiment, we generate random matrices of different size and corrupt around 10% of the sampled entries by adding large amplitude Gaussian noise. Figure 5 plots error incurred and time required by various methods as n increases from 500 to 5000. Note that SVT is particularly sensitive to outlier noise and incurs high RMSE. Also, for noisy samples the computational cost of\nboth SVT and SMC increases considerably.\nMatrix Completion: Movie-Lens Dataset Finally, we evaluate our method on the Movie-Lens dataset [Mov], which contains 1 million ratings for 3900 movies by 6040 users. For SVP and ALS, we fix the rank of the matrix to be k = 15. For SVP, we set the step size ηt to be 5/ √ t. SVP incurs RMSE of 1.01 in 64.85 seconds, while SVT incurs RMSE of 1.21 in 1214.78 seconds. In contrast, ALS achieves RMSE of 0.90 in 195.34 seconds. We attribute the relatively poor performance of SVP and SVT as compared with ALS to the fact that the ratings matrix is not sampled uniformly, thus violating a crucial assumption of both our method and SVT. Similar to Figure 5 (b), SVT converges much slower than SVP on the Movie-Lens data."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "There has been a significant amount of work recently in the area of low-rank approximations. Examples include minimizing rank subject to affine constraints, low-rank matrix completion, low-rank plus sparse decomposition. Most of these works, with the exception of Keshavan et al. [KOM09], rely on relaxing the rank constraint with trace-norm and give guarantees for recovering the optimal\nsolution under certain additional assumptions. However, trace-norm relaxation based methods are typically hard to analyse and are relatively expensive in practice.\nIn this paper, we proposed a simple and natural algorithm based on iterative hard-thresholding. We give a simple anlaysis of our algorithm for the affine rank minimization problem satisfying the restricted isometry property and give geometric convergence guarantees even in the presence of noise. The intermediate steps in our algorithm are less computationally demanding than those of current state-of-the-art methods. We empirically demonstrate that our method is significantly faster and more robust to both uniformly bounded and outlier noise than most existing methods.\nAn immediate question arising out of our work is to prove our hypothesis bounding the incoherence of the iterates of SVP for low-rank matrix completion, or otherwise directly prove Conjecture 4.3. Another interesting direction is to study parallels between our work and the iterative hardthresholding and matching pursuit lines of work in compressed sensing. Such a connection can be contransted with the natural relation between trace-norm minimization methods for rank minimization and l1-norm minimization methods in compressed sensing. Other directions include application of our methods to other problems of similar flavour such as the low-rank plus sparse matrix decomposition [CSPW09], or other matrix completion type problems like minimum dimensionality embedding using partial distance observations [FHB03] and low-rank kernel learning [MJCD08]."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported by NSF grant CCF-0431257, NSF-ITR award IIS-0325116 and NSF grant CCF-0728879."
    } ],
    "references" : [ {
      "title" : "Studies in Linear and Nonlinear Programming",
      "author" : [ "K. Arrow", "L. Hurwicz", "H. Uzawa" ],
      "venue" : "Stanford University Press, Stanford",
      "citeRegEx" : "AHU58",
      "shortCiteRegEx" : null,
      "year" : 1958
    }, {
      "title" : "Applied and Computational Harmonic Analysis",
      "author" : [ "Thomas Blumensath", "Mike E. Davies. Iterative hard thresholding for compressed sensing" ],
      "venue" : "27(3):265 – 274,",
      "citeRegEx" : "BD09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Fast online svd revisions for lightweight recommender systems",
      "author" : [ "Matthew Brand" ],
      "venue" : "In SIAM International Conference on Data Mining.",
      "citeRegEx" : "Bra03",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "and Zuowei Shen",
      "author" : [ "Jian-Feng Cai", "Emmanuel J. Candes" ],
      "venue" : "A singular value thresholding algorithm for matrix completion,",
      "citeRegEx" : "CCS08",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Candès and Yaniv Plan",
      "author" : [ "J Emmanuel" ],
      "venue" : "Matrix completion with noise,",
      "citeRegEx" : "CP09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Candès and Benjamin Recht",
      "author" : [ "J Emmanuel" ],
      "venue" : "Exact matrix completion via convex optimization,",
      "citeRegEx" : "CR08",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Sparse and low-rank matrix decompositions",
      "author" : [ "V. Chandrasekaran", "S. Sanghavi", "P. Parrilo", "A. Willsky" ],
      "venue" : "IFAC Symposium on System Identification.",
      "citeRegEx" : "CSPW09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Candès and Terence Tao",
      "author" : [ "J Emmanuel" ],
      "venue" : "The power of convex relaxation: Near-optimal matrix completion,",
      "citeRegEx" : "CT09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Arlington",
      "author" : [ "M. Fazel", "H. Hindi", "S. Boyd. A rank minimization heuristic with application to minimum order system approximation. In American Control Conference" ],
      "venue" : "Virginia.",
      "citeRegEx" : "FHB01",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Advances in linear matrix inequality methods in control: advances in design and control",
      "author" : [ "Karolos M. Grigoriadis", "Eric B. Beran. Alternating projection algorithms for linear matrix inequalities problems with rank constraints" ],
      "venue" : "pages 251–267,",
      "citeRegEx" : "GB00",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property",
      "author" : [ "Rahul Garg", "Rohit Khandekar" ],
      "venue" : "ICML.",
      "citeRegEx" : "GK09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "An accelerated gradient method for trace norm minimization",
      "author" : [ "Shuiwang Ji", "Jieping Ye" ],
      "venue" : "ICML.",
      "citeRegEx" : "JY09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and Andrea Montanari",
      "author" : [ "Raghunandan H. Keshavan", "Sewoong Oh" ],
      "venue" : "Matrix completion from a few entries,",
      "citeRegEx" : "KOM09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Factorization meets the neighborhood: a multifaceted collaborative filtering model",
      "author" : [ "Yehuda Koren" ],
      "venue" : "KDD, pages 426–434.",
      "citeRegEx" : "Kor08",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "In ICDM",
      "author" : [ "Yehuda Koren M. Bell. Scalable collaborative filtering with jointly derived neighborhood interpolation weights" ],
      "venue" : "pages 43–52.",
      "citeRegEx" : "MB07",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "and L",
      "author" : [ "S. Ma", "D. Goldfarb" ],
      "venue" : "Chen. Fixed point and bregman iterative methods for matrix rank minimization,",
      "citeRegEx" : "MGC09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "In ICML",
      "author" : [ "Raghu Meka", "Prateek Jain", "Constantine Caramanis", "Inderjit S. Dhillon. Rank minimization via online learning" ],
      "venue" : "pages 656–663.",
      "citeRegEx" : "MJCD08",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Parrilo",
      "author" : [ "Benjamin Recht", "Maryam Fazel", "Pablo A" ],
      "venue" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization,",
      "citeRegEx" : "RFP07",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Taylor & Francis",
      "author" : [ "Robert E. Skelton", "T. Iwasaki", "K.M. Grigoriadis. A Unified Algebric Approach to Control Design" ],
      "venue" : "Inc., Bristol, PA, USA,",
      "citeRegEx" : "SIG97",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "and Y",
      "author" : [ "J. Wright", "A. Ganesh", "S. Rao" ],
      "venue" : "Ma. Robust principal component analysis: Exact recovery of corrupted low-rank matrices by convex optimization,",
      "citeRegEx" : "WGRM09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Bernstein inequalities (probability theory) — wikipedia",
      "author" : [ "Wikipedia" ],
      "venue" : "the free encyclopedia,",
      "citeRegEx" : "Wik09",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Our results improve upon a recent breakthrough by Recht, Fazel and Parillo [RFP07] in three significant ways: 1) our method (SVP) is significantly simpler to analyse and easier to implement, 2) we give geometric convergence guarantees for SVP and, as demonstrated empiricially, SVP is significantly faster on real-world and synthetic problems, 3) we give optimality and geometric convergence guarantees even for the noisy version of ARMP.",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 16,
      "context" : "Unfortunately, ARMP is NP-hard in general and is also NP-hard to approximate ([MJCD08]).",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "The most commonly used heuristic for the problem is to assume a factorization of X and optimize the resulting non-convex problem by alternating minimization [Bra03, Kor08, MB07], alternative projections [GB00] or alternating LMIs [SIG97].",
      "startOffset" : 203,
      "endOffset" : 209
    }, {
      "referenceID" : 18,
      "context" : "The most commonly used heuristic for the problem is to assume a factorization of X and optimize the resulting non-convex problem by alternating minimization [Bra03, Kor08, MB07], alternative projections [GB00] or alternating LMIs [SIG97].",
      "startOffset" : 230,
      "endOffset" : 237
    }, {
      "referenceID" : 8,
      "context" : "Another common approach is to relax the rank constraint to a convex function such as the trace-norm or the log determinant [FHB01], [FHB03].",
      "startOffset" : 123,
      "endOffset" : 130
    }, {
      "referenceID" : 16,
      "context" : "[MJCD08] proposed online learning based methods for ARMP.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "In a recent breakthrough, Recht, Fazel and Parillo [RFP07] obtained the first non-trivial exactrecovery results for ARMP obtaining guaranteed rank minimization for affine transformations A that satisfy a restricted isometry property (RIP).",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "Our analysis of SVP is motivated by the recent work in the field of conpressed sensing by Blumensath and Davies [BD09], Garg and Khandekar [GK09].",
      "startOffset" : 112,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "Our analysis of SVP is motivated by the recent work in the field of conpressed sensing by Blumensath and Davies [BD09], Garg and Khandekar [GK09].",
      "startOffset" : 139,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "[RFP07]: we only require δ2k < 1/3 as opposed to δ5k < 1/10 required by Recht et al.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "[RFP07] only address the case of exact measurements.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 5,
      "context" : "Recently, Candes and Recht [CR08], Candes and Tao [CT09] and Keshavan et al.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "Recently, Candes and Recht [CR08], Candes and Tao [CT09] and Keshavan et al.",
      "startOffset" : 50,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "[KOM09] obtained the first non-trivial results for low-rank matrix completion under a few additional assumptions.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "[CCS08], are quite expensive in practice and not very tolerant to noise.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 5,
      "context" : "We demonstrate empirically that for a suitable step-size, SVP significantly outperforms the methods of [CR08], [CT09], [CCS08], [KOM09] in accuracy, computational time and tolerance to noise.",
      "startOffset" : 103,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "We demonstrate empirically that for a suitable step-size, SVP significantly outperforms the methods of [CR08], [CT09], [CCS08], [KOM09] in accuracy, computational time and tolerance to noise.",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "We demonstrate empirically that for a suitable step-size, SVP significantly outperforms the methods of [CR08], [CT09], [CCS08], [KOM09] in accuracy, computational time and tolerance to noise.",
      "startOffset" : 119,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "We demonstrate empirically that for a suitable step-size, SVP significantly outperforms the methods of [CR08], [CT09], [CCS08], [KOM09] in accuracy, computational time and tolerance to noise.",
      "startOffset" : 128,
      "endOffset" : 135
    }, {
      "referenceID" : 7,
      "context" : "Furthermore, our experiments strongly suggest (see Figure 1) that guarantees similar to those of [CT09], [KOM09] hold for SVP, achieving exact recovery for incoherent matrices from an almost optimal number of entries1.",
      "startOffset" : 97,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, our experiments strongly suggest (see Figure 1) that guarantees similar to those of [CT09], [KOM09] hold for SVP, achieving exact recovery for incoherent matrices from an almost optimal number of entries1.",
      "startOffset" : 105,
      "endOffset" : 112
    }, {
      "referenceID" : 17,
      "context" : "[RFP07] do not directly apply to MCP.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 5,
      "context" : "The first non-trivial results for MCP were obtained recently by Candes and Recht [CR08], Keshavan et al.",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "[KOM09] and Candes and Tao [CT09].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "[KOM09] and Candes and Tao [CT09].",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 20,
      "context" : "We need Bernstein’s inequality [Wik09] stated below.",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "[CCS08] proposed a Singular Value Thresholding (SVT) algorithm which is based on Uzawa’s algorithm[AHU58].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "[CCS08] proposed a Singular Value Thresholding (SVT) algorithm which is based on Uzawa’s algorithm[AHU58].",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "[MGC09].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "Toh and Yun [TY09], Ji and Ye [JY09] proposed Nesterov’s projected gradient based methods for optimizing the trace-norm.",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "Also, though minimizing the trace-norm does most likely approximate the low-rank solution even in the presence of noise (see [CP09] for instance), noise poses considerable computational challenges for trace-norm optimization.",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "For the case of low-rank matrix completion, Candes and Recht [CR08] obtained the first nontrivial results for the problem obtaining guaranteed completion for incoherent matrices X∗ and randomly sampled entries Ω.",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "Building on the work of Candes and Recht, Candes and Tao [CT09] obtained the near-optimal bound of |Ω| ≥ min(Cμ4k2n log n,Cμ2kn log n) for exact-recovery via trace-norm minimization.",
      "startOffset" : 57,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "However, the analysis of Candes and Recht, Candes and Tao is considerably complicated and minimizing trace-norm, even when using methods taylored for matrix-completion such as those of [CCS08], is relatively expensive in practice.",
      "startOffset" : 185,
      "endOffset" : 192
    }, {
      "referenceID" : 12,
      "context" : "[KOM09] obtained exact-recovery from uniformly sampled Ω with |Ω| ≥ C(μ, k)n log n using different technqiues.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "[CSPW09] andWright et al.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 19,
      "context" : "[WGRM09].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 19,
      "context" : "[WGRM09] show that the low-rank matrix completion problem can be reduced to the low-rank plus sparse decomposition problem.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 3,
      "context" : "singular value thresholding (SVT) method [CCS08].",
      "startOffset" : 41,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "Note that although [CCS08] presents SVT method in the context of matrix completion problem, however it can be easily adapted for ARMP.",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 12,
      "context" : "For matrix completion we compare against SVT, the spectral matrix completion (SMC) method of [KOM09], and regularized alternating least squares minimization (ALS).",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "[RFP07], we use the MIT logo as the test image for reconstruction.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 12,
      "context" : "[KOM09], rely on relaxing the rank constraint with trace-norm and give guarantees for recovering the optimal",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "Other directions include application of our methods to other problems of similar flavour such as the low-rank plus sparse matrix decomposition [CSPW09], or other matrix completion type problems like minimum dimensionality embedding using partial distance observations [FHB03] and low-rank kernel learning [MJCD08].",
      "startOffset" : 143,
      "endOffset" : 151
    }, {
      "referenceID" : 16,
      "context" : "Other directions include application of our methods to other problems of similar flavour such as the low-rank plus sparse matrix decomposition [CSPW09], or other matrix completion type problems like minimum dimensionality embedding using partial distance observations [FHB03] and low-rank kernel learning [MJCD08].",
      "startOffset" : 305,
      "endOffset" : 313
    } ],
    "year" : 2017,
    "abstractText" : "Minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization with affine constraints (ARMP) and show that SVP recovers the minimum rank solution for affine constraints that satisfy the restricted isometry property. We show robustness of our method to noise with a strong geometric convergence rate even for noisy measurements. Our results improve upon a recent breakthrough by Recht, Fazel and Parillo [RFP07] in three significant ways: 1) our method (SVP) is significantly simpler to analyse and easier to implement, 2) we give geometric convergence guarantees for SVP and, as demonstrated empiricially, SVP is significantly faster on real-world and synthetic problems, 3) we give optimality and geometric convergence guarantees even for the noisy version of ARMP. In addition, we address the practically important problem of low-rank matrix completion, which can be seen as a special case of ARMP. However, the affine constraints defining the matrix-completion problem do not obey the restricted isometry property in general. We empirically demonstrate that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sampled entries. We make partial progress towards proving exact recovery and provide some intuition for the performance of SVP applied to matrix completion by showing a more restricted isometry property. Our algorithm outperforms existing methods, such as those of [RFP07, CR08, CT09, CCS08, KOM09], for ARMP and the matrix-completion problem by an order of magnitude and is also significantly more robust to noise.",
    "creator" : "LaTeX with hyperref package"
  }
}