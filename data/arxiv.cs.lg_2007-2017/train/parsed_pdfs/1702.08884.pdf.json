{
  "name" : "1702.08884.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Low-rank Label Propagation for Semi-supervised Learning with 100 Millions Samples",
    "authors" : [ "Raphael Petegrosso", "Wei Zhang", "Zhuliu Li", "Yousef Saad", "Rui Kuang" ],
    "emails" : [ "kuang@cs.umn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n08 88\n4v 1\n[ cs\n.L G\n] 2\n8 Fe\nb 20\n17"
    }, {
      "heading" : "1 Introduction",
      "text" : "Semi-supervise learning is particularly helpful when only a few labeled data points and a large amount of unlabelled data are available for training a classifier. The unlabelled data are utilized to capture the underlying manifold structure and clusters by smoothness assumption such that the information from the labelled data points can be propagated through the clusters along the manifold structure. Graph-based semisupervised learning algorithms perform label propagation in a positively-weighted similarity graph between the data points [18, 2]. With the initialization of the vertices of the labeled data, the labels are iteratively propagated between the neighboring vertices and the propagation process will finally converge to the unique global optimum minimizing a quadratic criterion [17]. To construct the similarity graph for label propagation, the commonly used and well accepted measure is Gaussian kernel similarity. The Gaussian kernel applies a non-linear mapping of the data points from the orig-\n∗Department of Computer Science and Engineering, University\nof Minnesota Twin Cities, USA. †co-first authors ‡kuang@cs.umn.edu\ninal feature space to a new infinite-dimensional space and computes a positive kernel value for each pair of data points as the similarity in the graph. Since computing the pairwise similarity between the training data is prohibitively expensive under the presence of a huge amount of unlabelled data, no general label propagation method/tool is available for learning with tens of millions or more data points.\nIn this paper, we propose to improve the scalability of label propagation algorithms with a method based on both low-rank approximation of the kernel matrix, and parallelization of the approximation algorithms and label propagation, named BigLP (Big-data label propagation). We first adopted two low-rank label propagation algorithms, GLNP (Global Linear Neighborhood Propagation) [13] and Kernel Nyström Approximation [14], and implemented the parallelized algorithms. Specifically, GLNP was accelerated with Nesterov’s accelerated projected gradient descent and implemented with OpenMP for shared memory, and Kernel Nyström Approximation was implemented with Message Passing Interface (MPI) for distributed memory. The low-rank approximation and the parallelization of the algorithms allowed the scalability of label propagation up to 100 million samples in our experiments. The low-rank approximation of the kernel graph preserved the useful information in the original uncomputable similarity graph such that the classification results are similar or often better than the original label propagation or supervised learning algorithms that only use labeled data points. Overall, our results suggest that BigLP is effective and ready-to use implementation that will be greatly helpful for big data analysis with semi-supervise learning."
    }, {
      "heading" : "2 Graph-based Semi-Supervised Learning",
      "text" : "In this section, we first review the graph-based semisupervised learning for label propagation and then introduce the two methods for low-rank approximation of the similarity graph matrix for scalable label propagation.\n2.1 Label Propagation In a given dataset X = {x1, . . . , xl, . . . , xn} and a given label set L =\n{+1,−1}, {x1, . . . , xl} are data points in Rm labeled by {y1, . . . , yl|yi ∈ L, i = 1, . . . , l} and {xl+1, . . . , xn} are unlabeled data points in Rm. In graph-based semisupervised learning, a similarity graph G = (V,E) is first constructed from the dataset X , where the vertex set V = X and the edges E are weighted by adjacency matrix W computed by Gaussian kernel as Wij = exp(− ‖xi−xj‖ 2\n2σ2 ), where σ is the width param-\neter of the Gaussian function. Let S = D−1/2WD−1/2, where D is a diagonal matrix with Dii equal to the sum of the ith row of W. By relaxing the class label variables as real numbers, label propagation algorithm iteratively updates the predicted label f by\n(2.1) f t+1 = αSf t + (1− α)f0,\nwhere t is the step, and α ∈ (0, 1). f0 is a vector encoding the labeling of data points from set L and 0 is assigned to the unlabeled data. After running label propagation, the labels of the data points {xl+1, . . . , xn} are assigned based on f∗.\n2.2 Low-rank Label Propagation In large-scale semi-supervised learning, the number of samples can be in the order of tens of millions or more, leading to the difficulty in storing and operating the adjacency matrix W . A general solution is to generate a low-rank approximation of W . Specifically, the n×n symmetric positive semi-definite kernel matrix W can be approximated by W ≈ FFT , where F ∈ Rn×k and k≪n. Let F̄ denotes the normalized F with\n(2.2) F̄ij = Fij √\nFi,: ∑ F\nwhere Fi,: represents row i of F , ∑\nF is a vector composed by the sum of each column of F , and S ≈ F̄ F̄T . With the approximation, Eqn. (2.1) can be rewritten as\n(2.3) f t+1 = αF̄ F̄T f t + (1− α)f0.\nIn this new formula, the computational and memory requirements associated with handing the matrix F̄ is O(kn), which is much lower than O(n2). Nyström Method [14] and Global Linear Neighborhood Propagation (GLNP) [13] were previously proposed to learn the low rank approximation for label propagation.\nAs shown in [17], the closed-form solution of Eqn. (2.3) can be directly derived\n(2.4) f∗ = lim t→∞ f t = (1− α)(In − αF̄ F̄T )−1f0,\nwhere In denotes the n×n identity matrix. Taking advantage of the low-rank structure of In−αF̄ F̄T , applying Matrix-Inversion Lemma [15] generates a simplified\nsolution as\n(2.5) (In−αF̄ F̄T )−1 = In− F̄ (F̄T F̄ − (1/α)Ik)−1F̄T .\nIn this solution, the k× k matrix F̄T F̄ − (1/α)Ik needs to be inverted instead of the n× n matrix In − αF̄ F̄T . Overall, the time complexity of computing the closedform solution f∗ is O(k3 +nk), which is a better choice for small k, compared with the time complexity of iterative Eqn. (2.3) which is O(knT ) where T is the total number of iterations for convergence.\n2.3 Nyström Method Let Wij = w(xi, xj) for a kernel function w(a, b) = 〈Φ(a),Φ(b)〉, where a, b ∈ X and Φ is a mapping function. The Nyström method generates low-rank approximations of W using a subset of the samples in X [14]. Suppose k ≪ n data points { x̄1, x̄2, ..., x̄k } are sampled from X without replacement and let G be the k×k kernel matrix of the random samples, where Gi,j = w(x̄i, x̄j). Let C be the n by k kernel matrix between X and the random samples, where Ci,g = w(xi, x̄g). The kernel matrices W and C can be written in blocks as\nW =\n[\nG WT21 W21 W22\n]\nand C =\n[\nG W21\n]\n.\nG and C can be applied to construct a rank-k approximation to W :\n(2.6) W ≈ CG+k CT = FFT ,\nwhere G+k is the pseudo-inverse of G and the low rank matrix F = C √\nG+k , where √ G+k denotes element-wise\nsquare root of G+k , can be computed to approximate W for low-rank label propagation in Eqn. (2.1).\nInstead of selecting k random data points, k-means clustering could be applied to construct Nyström lowrank approximation. The k centroids obtained from the k-means were used as the landmark points to improve the approximation over random sampling [16]."
    }, {
      "heading" : "2.4 Global Linear Neighborhood Propagation",
      "text" : "Another strategy to learn the low rank representation is through global linear neighborhood [13]. Global linear neighborhood propagation (GLNP) was proposed to preserve the global cluster structures by exploring both the direct neighbors and the indirect neighbors in [13]. It is shown that global linear neighborhoods can be approximated by a low-rank factorization of an unknown similarity graph. Let X be the n × m data matrix from X where Xij is the value of the data point xi at the jth dimension. Instead of selecting k neighbors to construct the similarity graph, GLNP\nlearns a non-negative symmetric similarity graph by solving the following optimization problem:\n(2.7) minQ(F ) = ∥ ∥X − FFTX ∥ ∥\n2\nsubject to Fij ≥ 0 where F is a n × k matrix. To solve Eqn. (2.7), a multiplicative updating algorithm for nonnegative matrix factorization was proposed in [13]. Assume that X contains only nonnegative values, a nonnegative F can be learned by the following multiplicative update rule:\n(2.8) Fij ← Fij × √\n(2XXTF )ij (FFTXXTF +XXTFFTF )ij ,\nwhere × represents element-wise multiplication. After F is learned, it can be used for label propagation.\n2.5 Accelerated Projected Gradient Descent The objective function Q(F ) in Eqn. (2.7) is a fourth order non-convex function of F similar to the symmetric NMF problem in [7]. For large-scale data, a firstorder optimization method is preferred to find a stationary point [3]. Applying the gradient descent method ar+1 = ar − 1L∇f(ar) to a convex Lipschitz continuous function f(a) with ||∇f(a) − ∇f(b)|| ≤ L||a − b||, the rate of convergence after r steps is O(1/r) satisfying f(ar − a∗) ≤ 2L||a 0−a∗||2\nr+3 . In [10], an optimal first or-\nder Nesterov’s method was proposed to achieve O(1/r2) convergence rate with f(ar − a∗) ≤ 2L||a 0−a∗||2\nr2 . Since Nesterov’s method is often used to accelerate the projected gradient descent to solve constraint optimization problems [1, 11]. Here we adopt Nesterov’s accelerated projected gradient descent method to minimize the objective function Q(F ) in Eqn. (2.7) in Algorithm 1.\nAlgorithm 1 Accelerated Projected Gradient Descent\n1: initialize Y 1 = F 0, γ1 = 1 2: for t = 1 → maxIter do 3: F t = P [Y t − αt∇Q(Y t)/||∇Q(Y t)||] 4: γt+1 = 1+ √ 1+4γ2t 2 5: Y t+1 = F t + (γt−1γt+1 )(F t − F t−1) 6: if ||∇PQ(F t)|| ≤ ǫ||∇Q(F 0)|| then 7: break 8: end if 9: end for\n10: return F\nThe operation P [C] denotes projecting C into the nonnegative orthant such that:\nP [C] =\n{\n0, if C < 0\nC, otherwise\n∇PQ(F ) is the projected gradient of variable F defined as:\n(∇PQ(F ))ij = { (∇Q(F ))ij , if Fij ≥ 0 min(0, (∇Q(F ))ij), otherwise\nThe stopping condition ||∇PQ(F t)|| ≤ ǫ||∇Q(F 0)|| checks if a point F t is close to a stationary point in a bound-constrained optimization problem [8]. The step size αt in the projected gradient descent is chosen by Backtracking line search [3, 8] as: Given 0 < β < 1 and 0 < σ < 1, starting with α1 = 1 and shrinking α as αt+1 := βαt until the condition Q(Y t+1)−Q(Y t) ≤ σ〈∇Q(Y t), (Y t+1−Y t)〉 is satisfied."
    }, {
      "heading" : "3 Parallel Implementation",
      "text" : "The architecture of the parallel implementation of the low-rank label propagation algorithms is shown in Figure 1. In this section, we first give a brief overview of the distributed memory and shared memory architecture, and linear algebra libraries used in the implementation, and then describe the parallel implementation of each algorithm.\n3.1 Memory Architecture The parallel computing approach reduces memory requirements on Label Propagation and Nyström low-rankmatrix computation with distributed memory architecture. Shared-memory architecture was applied to run GLNP in a single computer with multi-threading.\n3.1.1 Distributed Memory: The distributed memory architecture follows the SPMD (single program, multiple data) paradigm for parallelism. The same program simultaneously runs on multiple CPUs according to the data decomposition. The processes communicate with each other to exchange data, as needed by the programs. The distributed memory architecture allows allocation of dedicated memory to each process possibly running on different machines for better scalability in memory requirement on each machine. The disadvantage is the overhead incurred through the data communication through the network among the machines.\nMessage Passing Interface (MPI) [6] was used to implement the distributed memory architecture. MPI provides a rich set of interfaces for point-to-point operations and collective communications operations (group operations). In addition, MPI-2 [5] introduces one-sided communications operations for remote memory access. We used MPI to implement the parallel Low-rank Label Propagation and the Nyström approximation. In particular, the implementation of Nyström approximation only requires communication of size O(n+ k2).\n3.1.2 Shared Memory: The computation of GLNP involves a large number of matrix multiplication operations which, to be performed in parallel with distributed memory, requires too much data communication. Even if distributed memory still considerably reduces the memory requirements, the overall running time could be worse. Therefore, we adopted shared memory architecture in the implementation.\nIn the shared memory architecture, the program runs in multi-threading with all the threads accessing the same shared memory. There is no incurred overhead in data communication. However, the architecture can only utilize the memory available in one machine. Moreover, the shared memory architecture incurs an overhead of cache coherence, in which threads compete to access the same cache with different data, resulting in high cache misses. We implemented the shared memory architecture using the OpenMP API.\n3.2 Linear Algebra Libraries In all the implementations, OpenBLAS was used to perform basic linear algebra operations. OpenBLAS is an optimized version of the BLAS library, and allows multi-threading implementation. For more advanced linear algebra operations, in the eigen-decomposition for Nyström Approximation, we used the LAPACK library.\n3.3 Parallel Nyström Approximation The parallel Nyström approximation algorithm implements both random and k-means sampling of k samples to calculate the low-rank representation. Algorithm S.3 in the Supplementary document describes sampling k random samples without replacement. Algorithm S.4 selects k samples as the centroids learned by k-means. For improved efficiency, we typically only run k-means with\na small number of iterations, which usually generates reasonably good selection.\nAlgorithm 2 Parallel Nyström\n1: functionPar Nyström(Xp, Xpk ,m, n, k,maxIter) 2: for i = 0 → k − 1 do 3: MPI Broadcast(Xpk i, sample, kIdxsi) 4: Wi = RBF (X p k , sample) 5: Ci = RBF (X p, sample) 6: end for 7: MPI Gather(W, 0) 8: if rank = 0 then 9: [eigvals, eigvecs] = EIG(W )\n10: end if 11: MPI Broadcast(eigvals, 0) 12: MPI Broadcast(eigvecs, 0) 13: G = C ∗ eigvecs 14: for i = 0 → k − 1 do 15: Gi = Gi/ √ eigvalsi 16: end for 17: return G 18: end function\nBased on the selected k samples, Nyström approximation algorithm is implemented in Algorithm 2. In Algorithm 2, the process assigned with sample i broadcasts sample i to the other processes (line 3). After receiving sample i, each process calculates W and C entries between sample i and all the samples at the node, with RBF kernel (lines 4-5). Matrix W is then gathered by process 0 to perform the eigen-decomposition of W (lines 7-10). Note that since W is only k× k, the eigendecomposition is not expensive for small k. Process 0 then broadcasts the eigenvectors and eigenvalues to the other processes at lines 11-12. Each process finally cal-\nculates the G based on the received eigenvectors and eigenvalues (lines 13-16).\n3.4 Parallel GLNP We implemented parallel GLNP following the two optimization frameworks presented previously: multiplicative update rule and accelerated projected gradient descent with line search. In the multiplicative update rule, given the input data matrix X , the function PAR SHIFT() in Algorithm S.1 checks the minimum value of X and then adds the minimum value to X to obtain the non-negative matrix X̄ since GLNP is based on non-negative multiplicative updating. The implementation of GLNP using multiplicative update rule is described in Algorithm 3.\nAlgorithm 3 Parallel GLNP - Multiplicative update rule 1: function Par GLNP MUL(X,m, n, k,maxIter, tol) 2: F ← Um×k[0, 1] 3: for t = 0 → maxIter do 4: Fold = F 5: B = X(XTF ) 6: D = F (FTB) 7: G = B(FTF ) 8: for i = 0 → m− 1 do 9: for j = 0 → k − 1 do 10: Fij = Fijsqrt(2Bij/(Dij +Gij)) 11: end for 12: end for 13: if max(abs(Fold − F )) < tol then 14: break 15: end if 16: end for 17: return F 18: end function\nIn Algorithm 3, F is first randomly initialized with uniform distribution between 0 and 1 in parallel by OpenMP. Then, the multiplicative update rule in Eqn. (2.7) is decomposed into several steps of matrix multiplication for parallelization according to the data dependency (lines 5-7). These operations are performed in multi-threading by the OpenBLAS library. Note that all these multiplications are computed in O(kn). Lines 8-12 update F with the multiplicative rule using the intermediate results in B, C and D with openMP. Lines 13-15 check for convergence by the threshold tol. Instead of checking the convergence of the objective function, which increase the memory requirements, the algorithm checks the maximum change among the elements in F . In our observation, the convergence is always achieved with this criteria.\nThe GLNP implementation with projected gradient\nAlgorithm 4 Parallel GLNP - Projected Gradient Descent with Line Search 1: function Par GLNP APGD(X ,m,n,k,maxIter,\n,maxInnerIter,beta,tol,roll) 2: F ← Um×k[0, 1] 3: Y = F 4: for t = 0 → maxIter do 5: B = X(XTY ) 6: D = Y (Y TB) 7: G = B(Y TY ) 8: for i = 0 → m− 1 do 9: for j = 0 → k − 1 do\n10: Gradij = 2Dij + 2Gij − 4Bij 11: end for 12: end for 13: Grad0 = Grad 14: Grad = Grad/sqrt(sum(Grad)) 15: objold = obj 16: obj = ||X − Y TY X ||2 17: alpha = 1 18: for inner = 0 → maxInnerIter do 19: Y1 = max(Y − alpha.Grad, 0) 20: obj1 = ||X − Y T1 Y1X ||2 21: sum = ∑\n(Grad0 ∗ (Y1 − Y )) 22: if obj1 − obj < roll.sum then 23: break 24: end if 25: alpha = betainner+1 26: end for 27: Fold = F 28: F = Y1 29: told = t 30: t = (1 + sqrt(1 + 4t2))/2 31: Y = F + (F − Fold)(told − 1)/t 32: if abs((obj1 − objold)/obj1) < tol then 33: break 34: end if 35: end for 36: return F 37: end function\nDataset HEPMASS SUSY mnist8m Protein Gisette Sample 10.5 × 106 5 × 106 1,648,890 13,077 7,000 Feature 27 128 784 357 5,000\nTable 1: Summary of datasets\ndescent and line search is presented in Algorithm 4. In Algorithm 4, we first calculate the normalized and unnormalized gradient of the objective function (lines 9-15). Line 16 calculates the objective function used by the line search. Lines 18-26 will perform the inner iterations of the projected gradient descent. Finally, the convergence is checked on line 33.\n3.5 Parallel Low-rank Label Propagation After normalizing low rank matrix F by the function PAR NORMALIZ() in Algorithm S.2 according to Eqn. (2.2), parallel low-rank label propagation is performed on the normalized low-rank data F̄ and the initial labeling vector f0 ∈ Rn×1 with Algorithm S.5. Note that f0 is also divided among the processes such that each process contains only a vector f0\np ∈ Rnp×1. Algorithm S.5 first initializes fp by sampling an uniform distribution between -1 and 1 (line 2). Each process is only responsible for calculating the allocated part of f . Lines 5-7 perform label propagation, and lines 8-12 check for convergence. Each process will return the local fp."
    }, {
      "heading" : "4 Results",
      "text" : "The parallel algorithms are tested on five real datasets and a simulation dataset. The runtime and memory requirement are measured. The prediction accuracy for semi-supervised learning was also reported.\n4.1 Datasets Five datasets with various sample sizes and feature sizes described in Table 1 were downloaded. The two largest datasets, HEPMASS and SUSY, were downloaded from UCI. Each of them contains millions of samples but a small number of features. mnist8m is the handwritten digit data from [9] which contains digits 7 and 9 for classification. The Protein dataset is for protein secondary structure prediction. In the experiments we only selected two out of the three classes for classification. The Gisette dataset is also a handwritten digit dataset used for feature selection challenge in NIPS 2003. Finally, we also created a random simulation dataset, with 100 million samples and 100 features to test the scalability of the implementation.\n4.2 Runtime and Memory Requirements We measured the runtime and memory requirements of our parallel implementation of Nyström (both random sampling and k-means sampling) and GLNP in all the datasets, shown in Figures 2 and 3.\nFigure 2 shows that GLNP is more scalable up to 4 threads and becomes worst at 8 threads due to the overhead by cache coherence with different threads competing to access the same cache which results in many cache misses. In the SUSY dataset, parallel GLNP with\nk = 20 runs 1.89x faster than the serial implementation. In the HEPMASS with 10.5 millions samples, parallel GLNP is 1.71x faster than the serial implementation. The multithreading by 4 threads clearly reduces the runtime considerably. GLNP was implemented in the shared-memory architecture, which always requires a constant amount of memory independent of the number of threads in Figure 3.\nFigure 2 also confirms that Nyström is a very scalable algorithm. Using 8 processes, the parallel implementation of the random sample selection with k = 20 performs 7.67x faster than the serial implementation on the mnist8m dataset, and 7.48x faster with sample selection by k-means. In the HEPMASS dataset, the algorithm was 7.08x faster using random sampling, and 7.42x using k-means. In Figure 3, the Nyström implementation reduces the memory requirements on each machine with the distributed memory architecture without introducing much overhead consumption. Note that among the large datasets, mnist dataset has relative more features. The memory consumption for different k is very similar since the original dataset is larger than the low-rank approximation data by a big magnitude.\nIn Figure 4, the plots show a comparison of the optimization by GLNP with acceleration plus line-search and multiplicative updating on three datasets Gisette, Protein and HEPMASS. In all the three cases, accelerated projected gradient descent achieved a better local optimal. Multiplicative updating has a very fast drop in the objective function in the first iteration and then gets into very slow steps for convergence. In practice, we observed that accelerated projected gradient descent achieves better local optimal and convergence in less iterations in all the experiments.\nFinally, we evaluated the performance on the simulation dataset with 100 millions of samples and 100 features. We were able to run this dataset using at least 8 processes by the Nyström implementation. With k=20 under random sample selection, the implementation completes in 140 seconds with 8 processes. The implementation under k-means sample selection runs in 543 seconds with 16 processes. It is also important to note that the memory requirements by each process is only 6.5 GB when 16 processes are used, which allows the implementation to run even on most personal computers available nowadays.\n4.3 Classification on Five Datasets To test the performance of semi-supervised learning with low-rank matrix approximation, we compared label propagation on the low-rank matrices approximated by GLNP and Nyström approximation (both random sampling and kmeans sampling) with the k-nearest neighbor (KNN)\nclassification algorithm on the original data by considering the five nearest training samples. To evaluate the classification results, we tested different k for low-rank approximation. In the experiments, we held out 20% of samples as the test set, and randomly selected different percentages of samples as the training set in each trail. On each dataset, for each k and each percentage of training samples, we ran 10 trails with different randomly selected training data and report the average classification accuracy on the test set. The same setup\nwas applied to test KNN as a base line. In label propagation, α was set to 0.01.\nThe classification results are reported in Figure 5. In Figure 5(A), k was fixed to 100 for each experiment and the plots show the results of training with different percentages of training samples. In general, semisupervised learning by label propagation with low-rank matrix approximation performs better than KNN when only a small size of training data is available. As the size of training data increases, KNN based on all the original features can perform similarly or better on the large datasets. The observation is consistent with the assumption of semi-supervised learning that the underlying manifold structure among labeled and unlabeled data can be explored to improve classification of unlabeled samples when only a small amount of training data is available. As more and more samples become available for training, the structural information becomes less important. Furthermore, low-rank matrix approximation can potentially lose information in the original dataset when k is small. Thus, it is possible that the classification results with low-rank label propagation could be slightly worse than KNN when the size of training data is large. Another observation is that the performance of GLNP is better than Nyström on the small datasets but worse on the large ones. It is possibly because GLNP often requires more iterations to learn the low-rank matrix and convergence is more difficult to achieve on the large datasets. Finally, consistent with previous observations, Nyström with k-means sampling consistently is better than random sampling. In Figure 5(B), the number of training samples were fixed to around 100 for each dataset and results show the effect of choosing different rank k. In general, as the size of k increase, the classification performances of low-rank approximation algorithms are closer to the baseline method. In addition, as k increases, the classification performances of Nyström, both kmeans and random sampling, become better. It is also noticeable that the performance of GLNP is less\nsensitive to the parameter k since it relies on the global information. Overall, the classification performances of low-rank label propagation are very competitive or better than supervised learning algorithm KNN using the original feature space when k is sufficient. Furthermore, for the largest three datasets, KNN is only scalable to use up to 1% of samples as training data while the low-rank label propagation are scalable to use all of the training data."
    }, {
      "heading" : "5 Discussion",
      "text" : "In this paper, we applied low-rank matrix approximation and Nesterov’s accelerated projected gradient descent with parallel implementation for Big-data Label Propagation (BigLP). BigLP was implemented and tested on the datasets of huge sample sizes for semisupervised learning. Compared with sparsity induced measures [4] to construct similarity graphs, BigLP is more applicable to the datasets of huge sample size with a relatively small number of features that need to be kernelized for better classification in semi-supervised learning. Sparsity induced measures rely on knowing all the pairwise similarities and would not scale to the datasets with more than hundreds of thousands of samples due to the low scalability in sample size and optimization for sparsity. In addition, compared with the sparsity induced measures and local linear embedding method [12], in which the neighbors are selected “locally”, GLNP preserves the global structures among the data points, and construct more robust and reliable similarity graphs for graph-based semi-supervised learning. In terms of scalability of the two low-rank approximation methods, Nyström approximation is potentially better than GLNP depending on the iterations of k-means for sample selection. In practice, the quality of the similarity matrix constructed by Nyström method could also depend on the samples learned by k-means which could introduce uncertainty."
    }, {
      "heading" : "6 Funding",
      "text" : "The research work is supported by grant from the National Science Foundation (IIS 1149697). RP is also supported by CAPES Foundation, Ministry of Education of Brazil (BEX 13250/13-2)."
    }, {
      "heading" : "1 Algorithms",
      "text" : ""
    }, {
      "heading" : "1.1 Data Processing",
      "text" : ""
    }, {
      "heading" : "In the first step in the parallel low-rank label propagation, each feature is individually shifted to contain only non-negative numbers. Given a low-rank matrix X ∈ Rn×k, each process p will contain",
      "text" : "a chunk of X, Xp ∈ R n p ×k . Each process performs shift of its data. Algorithm S.1 shows that each process first calculates the minimum value for each feature in line 3, followed by a MPI All Reduce operation (line 4) to give each process the minimum value among all the processes, for each feature. If the number obtained is negative, that feature is then shifted to avoid the negative number (lines 5-7). The algorithm is outlined below: Algorithm 1 Parallel data shift for label propagation 1: procedure Par Shift(Xp, n, k)\n2: for i = 0→ k − 1 do 3: colMin = minj x p ji ⊲ minimum of column i 4: MPI Allreduce(colMin, 1,MPI MIN)\n5: if colMin < 0 then 6: xpi = x p i − colMin 7: end if\n8: end for 9: return Xp\n10: end procedure"
    }, {
      "heading" : "1.2 Data Normalization",
      "text" : "The normalization was implemented in MPI, also following the assumption that each process p contains only a chunk Xp of X. Algorithm S.2 outlines the implementation. In algorithm S.2, we\nfirst build a vector containing the sum for each column (lines 2-4) followed by a MPI All Reduce\noperation (line 5), which updates the vector with the sum of the vectors in all the processes. Lines\n6-9 apply the normalization equation in the data in each process. The data normalization algorithm\nis outlined bellow:\nPage 1 of 5\nLow-rank Label Propagation for Big Data, Petegrosso et. al., 2016\nAlgorithm 2 Parallel data normalization for label propagation\n1: procedure Par Normalization(Xp, n, k) 2: for i = 0→ k − 1 do 3: colSumi = ∑ j x p ji ⊲ sum of column i 4: end for\n5: MPI Allreduce(colSum, k,MPI SUM) 6: tmp = Xp ∗ colSum 7: for i = 0→ n\np − 1 do\n8: xpi = x p i / √ tmpi 9: end for\n10: return Xp\n11: end procedure"
    }, {
      "heading" : "1.3 Random sampling",
      "text" : ""
    }, {
      "heading" : "In lines 2-6, process 0 first select k indices. This list is then sent to all processes (line 7). After, each process will look up and return the subset of the k indices which refers to data present in that process (lines 8-13). The algorithm is outlined below:",
      "text" : "Algorithm 3 Parallel Random Sampling\n1: function Par Random Sampl(Xp,m, n, k)\n2: if rank = 0 then ⊲ if process 0 3: tmp = [0 . . . (m− 1)] 4: tmp = shuffle(tmp) 5: kIndices = tmp[0 . . . (k − 1)] 6: end if\n7: MPI Broadcast(kIndices, 0) 8: for i = 0→ k − 1 do 9: if kIndicesi/(m/p) = rank then\n10: Xp k .insert(Xp[kIndicesi%(m/p)]) 11: end if\n12: end for 13: return [Xpk , kIndices] 14: end function\n1.4 k-means sampling\nThe algorithm is divided into two sections. First, each process assigns the closest centroid to\nits local data points (lines 6-17). For each centroid i, the process containing it broadcasts the\ncentroid to other processes (line 12). Each process then calculated the distance of its data points\nto the centroid received (line 13-15). In the next part, the algorithm finds new centroids based on\nthe datapoints assignment (lines 20-41). For each centroid i, each process finds how many local\ndatapoints belongs to it. The total number of datapoints in the centroid is then obtained by a\nMPI All reduce operation (line 29). Then, the mean of all the datapoints is obtained (lines 30-26),\nresulting in the new centroid i.\nPage 2 of 5\nLow-rank Label Propagation for Big Data, Petegrosso et. al., 2016\nAlgorithm 4 Parallel k-means Sampling\n1: function Par Kmeans Sampl(Xp,m, n, k,maxIter) 2: [centrs, kIdxs] = Par Kmeans Sampling(Xp,m, n, k) ⊲ find random centroids 3: for t = 0→ maxIter do ⊲ assign centroid to datapoints 4: for i = 0→ k − 1 do 5: if kIdxsi/(m/p) = rank then\n6: centr = centrs.next()\n7: end if\n8: MPI Broadcast(centr, kIdxsi/(m/p))\n9: dist = ||centr −Xp||2 10: end for\n11: minIdxs = min(dist) 12: for i = 0→ k − 1 do ⊲ find new k centroids 13: for j = 0→ (m/p)− 1 do 14: if minIdxsj = i then\n15: samplesIdx.insert(j)\n16: end if\n17: end for\n18: nSamples = samplesIdx.length\n19: MPI Allreduce(nSamples,\n20: nTotalSamples,MPI SUM) 21: for j = 0→ nSamples− 1 do 22: for l = 0→ n− 1 do 23: s = samplesIdxj 24: newCtrl = X p sl/nTotalSamples 25: end for\n26: end for\n27: MPI Reduce(newCtr, kIdxsi/(m/p),\n28: MPI SUM)\n29: if kIdxsi/(m/p) then\n30: centrs.replace(newCtr)\n31: end if\n32: end for\n33: end for\n34: return centrs\n35: end function\nPage 3 of 5\nLow-rank Label Propagation for Big Data, Petegrosso et. al., 2016"
    }, {
      "heading" : "1.5 Parallel low-rank label propagation",
      "text" : "The parallel low-rank label propagation algorithm is shown below. In the algorithm, X and f0 are divided among the processes such that each process contains only a matrix Xp ∈ R n p ×n and a vector f0 p ∈ R n p ×1\n. The algorithm first initializes fp of size n p with an uniform distribution between\n-1 and 1 (line 2). Each process is only responsible for calculating the allocated part of f . Lines 5-7\nperform label propagation, and lines 8-12 check for convergence. Each process will return the local vector fp.\nAlgorithm 5 Parallel low-rank label propagation\n1: function Par LRLP(Xp, f0 p , n, k, α,maxIter, tol) 2: fp ← Un p\n[−1, 1] 3: for t = 0→ maxIter − 1 do 4: fpold = f p 5: fptmp = (X p)T ∗ fp 6: MPI Allreduce(fptmp, k,MPI SUM) 7: f = α ∗Xp ∗ fptmp + (1− α) ∗ f0 p 8: tmp = max(abs(fpold − fp)) 9: MPI Allreduce(tmp, 1,MPI MAX)\n10: if tmp < tol then\n11: break\n12: end if\n13: end for 14: return fp\n15: end function\nPage 4 of 5\nLow-rank Label Propagation for Big Data, Petegrosso et. al., 2016\n2 Classification Results\nThe classification results tested under different choice of k and percentage of training samples are\nshown in Tables 1-5 for the five datasets, respectively.\nPage 5 of 5"
    } ],
    "references" : [ {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM journal on imaging sciences,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Using manifold stucture for partially labeled classification, in Advances in Neu-  ral Information",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "Processing Systems",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Sparsity induced similarity measure for label propagation, in 2009",
      "author" : [ "H. Cheng", "Z. Liu", "J. Yang" ],
      "venue" : "IEEE 12th international conference on computer vision,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "A high-performance, portable implementation of the mpi message passing interface standard",
      "author" : [ "W. Gropp", "E. Lusk", "N. Doss", "A. Skjellum" ],
      "venue" : "Parallel computing,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1996
    }, {
      "title" : "Symmetric nonnegative matrix factorization for graph clustering",
      "author" : [ "D. Kuang", "H. Park", "C.H.Q. Ding" ],
      "venue" : "in SDM, SIAM / Omnipress,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Projected gradient methods for nonnegative matrix factorization",
      "author" : [ "C.-J. Lin" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Training invariant support vector machines using selective sampling, Large scale kernel machines",
      "author" : [ "G. Loosli", "S. Canu", "L. Bottou" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate o (1/k2)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Soviet Mathematics Doklady,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1983
    }, {
      "title" : "Nonlinear dimensionality reduction by locally linear embedding",
      "author" : [ "S.T. Roweis", "L.K. Saul" ],
      "venue" : "Science,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2000
    }, {
      "title" : "Global linear neighborhoods for efficient label propagation",
      "author" : [ "Z. Tian", "R. Kuang" ],
      "venue" : "in SDM,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Using the Nyström method to speed up kernel machines, in Proceedings of the 14th annual conference on neural information processing",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "systems, no. EPFL-CONF-161322,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2001
    }, {
      "title" : "Inverting modified matrices",
      "author" : [ "M.A. Woodbury" ],
      "venue" : "Memorandum report,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1950
    }, {
      "title" : "Improved nyström low-rank approximation and error analysis",
      "author" : [ "K. Zhang", "I.W. Tsang", "J.T. Kwok" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Graph-based semisupervised learning algorithms perform label propagation in a positively-weighted similarity graph between the data points [18, 2].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "We first adopted two low-rank label propagation algorithms, GLNP (Global Linear Neighborhood Propagation) [13] and Kernel Nyström Approximation [14], and implemented the parallelized algorithms.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "We first adopted two low-rank label propagation algorithms, GLNP (Global Linear Neighborhood Propagation) [13] and Kernel Nyström Approximation [14], and implemented the parallelized algorithms.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 10,
      "context" : "Nyström Method [14] and Global Linear Neighborhood Propagation (GLNP) [13] were previously proposed to learn the low rank approximation for label propagation.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "Nyström Method [14] and Global Linear Neighborhood Propagation (GLNP) [13] were previously proposed to learn the low rank approximation for label propagation.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "Taking advantage of the low-rank structure of In−αF̄ F̄ , applying Matrix-Inversion Lemma [15] generates a simplified solution as",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "The Nyström method generates low-rank approximations of W using a subset of the samples in X [14].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 12,
      "context" : "The k centroids obtained from the k-means were used as the landmark points to improve the approximation over random sampling [16].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "4 Global Linear Neighborhood Propagation Another strategy to learn the low rank representation is through global linear neighborhood [13].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "Global linear neighborhood propagation (GLNP) was proposed to preserve the global cluster structures by exploring both the direct neighbors and the indirect neighbors in [13].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 9,
      "context" : "7), a multiplicative updating algorithm for nonnegative matrix factorization was proposed in [13].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "7) is a fourth order non-convex function of F similar to the symmetric NMF problem in [7].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "In [10], an optimal first order Nesterov’s method was proposed to achieve O(1/r) convergence rate with f(a − a) ≤ 2L||a −a|| r2 .",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "Since Nesterov’s method is often used to accelerate the projected gradient descent to solve constraint optimization problems [1, 11].",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 5,
      "context" : "(∇Q(F ))ij , if Fij ≥ 0 min(0, (∇Q(F ))ij), otherwise The stopping condition ||∇PQ(F t)|| ≤ ǫ||∇Q(F 0)|| checks if a point F t is close to a stationary point in a bound-constrained optimization problem [8].",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 5,
      "context" : "The step size αt in the projected gradient descent is chosen by Backtracking line search [3, 8] as: Given 0 < β < 1 and 0 < σ < 1, starting with α1 = 1 and shrinking α as αt+1 := βαt until the condition Q(Y t+1)−Q(Y ) ≤ σ〈∇Q(Y ), (Y t+1−Y t)〉 is satisfied.",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "Message Passing Interface (MPI) [6] was used to implement the distributed memory architecture.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "Algorithm 3 Parallel GLNP - Multiplicative update rule 1: function Par GLNP MUL(X,m, n, k,maxIter, tol) 2: F ← Um×k[0, 1] 3: for t = 0 → maxIter do 4: Fold = F 5: B = X(XF ) 6: D = F (FB) 7: G = B(FF ) 8: for i = 0 → m− 1 do 9: for j = 0 → k − 1 do 10: Fij = Fijsqrt(2Bij/(Dij +Gij)) 11: end for 12: end for 13: if max(abs(Fold − F )) < tol then 14: break 15: end if 16: end for 17: return F 18: end function",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "The GLNP implementation with projected gradient Algorithm 4 Parallel GLNP - Projected Gradient Descent with Line Search 1: function Par GLNP APGD(X ,m,n,k,maxIter, ,maxInnerIter,beta,tol,roll) 2: F ← Um×k[0, 1] 3: Y = F 4: for t = 0 → maxIter do 5: B = X(XY ) 6: D = Y (Y B) 7: G = B(Y Y ) 8: for i = 0 → m− 1 do 9: for j = 0 → k − 1 do 10: Gradij = 2Dij + 2Gij − 4Bij 11: end for 12: end for 13: Grad0 = Grad 14: Grad = Grad/sqrt(sum(Grad)) 15: objold = obj 16: obj = ||X − Y Y X ||2 17: alpha = 1 18: for inner = 0 → maxInnerIter do 19: Y1 = max(Y − alpha.",
      "startOffset" : 204,
      "endOffset" : 210
    }, {
      "referenceID" : 6,
      "context" : "mnist8m is the handwritten digit data from [9] which contains digits 7 and 9 for classification.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "Compared with sparsity induced measures [4] to construct similarity graphs, BigLP is more applicable to the datasets of huge sample size with a relatively small number of features that need to be kernelized for better classification in semi-supervised learning.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "In addition, compared with the sparsity induced measures and local linear embedding method [12], in which the neighbors are selected “locally”, GLNP preserves the global structures among the data points, and construct more robust and reliable similarity graphs for graph-based semi-supervised learning.",
      "startOffset" : 91,
      "endOffset" : 95
    } ],
    "year" : 2017,
    "abstractText" : "The success of semi-supervised learning crucially relies on the scalability to a huge amount of unlabelled data that are needed to capture the underlying manifold structure for better classification. Since computing the pairwise similarity between the training data is prohibitively expensive in most kinds of input data, currently, there is no general readyto-use semi-supervised learning method/tool available for learning with tens of millions or more data points. In this paper, we adopted the idea of two low-rank label propagation algorithms, GLNP (Global Linear Neighborhood Propagation) and Kernel Nyström Approximation, and implemented the parallelized version of the two algorithms accelerated with Nesterov’s accelerated projected gradient descent for Big-data Label Propagation (BigLP). The parallel algorithms are tested on five real datasets ranging from 7000 to 10,000,000 in size and a simulation dataset of 100,000,000 samples. In the experiments, the implementation can scale up to datasets with 100,000,000 samples and hundreds of features and the algorithms also significantly improved the prediction accuracy when only a very small percentage of the data is labeled. The results demonstrate that the BigLP implementation is highly scalable to big data and effective in utilizing the unlabeled data for semi-supervised learning.",
    "creator" : "LaTeX with hyperref package"
  }
}