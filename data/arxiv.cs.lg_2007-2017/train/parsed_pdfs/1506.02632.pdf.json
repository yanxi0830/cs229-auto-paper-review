{
  "name" : "1506.02632.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cumulative Prospect Theory Meets Reinforcement Learning: Estimation and Control",
    "authors" : [ "Prashanth L.A", "Jie Cheng", "Michael Fu", "Steve Marcus" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 6.\n02 63\n2v 1\n[ cs\n.L G"
    }, {
      "heading" : "1 Introduction",
      "text" : "For a random variable X, let pi, i = 1, . . . ,K denote the probability of incurring a gain/loss xi, i = 1, . . . ,K . Given a utility function u and weighting function w, Prospect theory (PT) value is defined as V (X) = ∑K i=1 u(xi)w(pi). The idea is to take an utility function that is S-shaped, so that it satisfies the diminishing sensitivity property. If we take the weighting function w to be the identity, then one recovers the classic expected utility. A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for a justification, in particular via empirical tests using human subjects). However, PT is lacking in some theoretical aspects as it violates first-order stochastic dominance1.\nCumulative prospect theory (CPT) (Tversky and Kahneman 1992) uses a similar measure as PT, except that the weights are a function of cumulative probabilities. First, separate the gains and losses as\n1Consider the following example from Fennema and Wakker (1997): Suppose there are 20 prospects (outcomes) ranging from −10 to 180, each with probability 0.05. If the weight function is such that w(0.05) > 0.05, then it uniformly overweights all low-probability prospects and the resulting PT value is higher than the expected value 85. This violates stochastic dominance, since a shift in the probability mass from bad outcomes did not result in a better prospect.\nx1 ≤ . . . ≤ xl ≤ 0 ≤ xl+1 ≤ . . . ≤ xK . Then, the CPT-value is defined as\nV (X) =\nl∑\ni=1\nu−(xi) ( w−( i∑\nj=1\npj)− w−( i−1∑\nj=1\npj) ) + K−1∑\ni=l+1\nu+(xi) ( w+( K∑\nj=i\npj)− w−( K∑\nj=i+1\npj) ) ,\nwhere u+, u− are utility functions and w+, w− are weight functions corresponding to gains and losses, respectively. The utility functions u+ and u− are non-decreasing, while the weight functions are continuous, non-decreasing and have the range [0, 1] with w+(0) = w−(0) = 0 and w+(1) = w−(1) = 1 . Unlike PT, the CPT-value does not violate stochastic dominance2."
    }, {
      "heading" : "CPT-value estimation.",
      "text" : "With this background, the first aim of this paper is develop a scheme for estimating the CPT-value, given only samples of the random variable X. In particular, we want to estimate the following equivalent form of CPT-value:\nV (X) =\n∫ +∞\n0 w+(P (u+(X)) > x)dx−\n∫ +∞\n0 w−(P (u−(X)) > x)dx. (1)\nWe derive an estimate of the first integral above as follows: first compute the empirical distribution function for u+(X), then compose it with the weight function w+ and finally, integrate the resulting composition to obtain the final estimate. The second integral in (1) is estimated in a similar fashion and the CPT-value estimate is the difference in the estimates of the two integrals in (1). Assuming that the weight functions are Lipschitz, we establish convergence (asymptotic) of our CPT-value estimate to the true CPT-value. We also provide a sample complexity result that establishes that O ( 1 ǫ2 ) samples are required to be ǫ-close to the CPT-value with high probability."
    }, {
      "heading" : "Optimizing CPT-value in MDPs.",
      "text" : "The second aim of this paper is use a performance measure inspired by the CPT-value in a risk-sensitive reinforcement learning setting. In particular, we consider a stochastic shortest path (SSP) problem and instead of the classic expected utility criterion, we employ a weight function to distort the probabilities and then, aim to minimize the CPT-value. For this purpose, we first parameterize the policies such that they are continuously differentiable and then, design policy optimization algorithms in order to find a good-enough policy that optimizes the CPT-value. There are three challenges involved in the design of a policy-optimizing algorithm:\nBiased policy evaluation: For a fixed policy, a finite sample run results in a biased estimate of its CPTvalue, although the bias is bounded.\nSimulation optimization: Given only sample values of the CPT-value for any policy, it is necessary to devise an adaptive search scheme that improves the policy iteratively.\n2In the aforementioned example, increasing w−(0.05) and w+(0.05) does not impact outcomes other than those on the extreme, i.e., −10 and 180, respectively. For instance, the weight for outcome 100 would be w+(0.45) −w+(0.40). Thus, CPT formalizes the intuitive notion that humans are sensitive to extreme outcomes and relatively insensitive to intermediate ones.\nNon-dynamic programming: The CPT-value is a non-coherent and non-convex risk measure, unlike traditional objectives such as expected utility and coherent risk measures (e.g. conditional value-at-risk (CVaR)). The implication is that it is non-trivial to develop dynamic programming based algorithms here.\nFor the first problem, we increase the number of samples as the policy update progresses and this gradually takes the bias in CPT-value estimates to zero. Using two well-known ideas from the simulation optimization literature Fu (2015), we propose three policy optimization algorithms that overcome the second and third problems. Our proposed algorithms are summarized as follows:\nGradient-based methods: We propose two algorithms in this class. The first is a policy gradient algorithm that employs simultaneous perturbation stochastic approximation (SPSA)-based estimates for the gradient of the CPT-value, while the second is a policy Newton algorithm that also uses SPSAbased estimates of the gradient and also the Hessian. We remark here that, unlike traditional settings for SPSA, our estimates for CPT-value have a non-zero (albeit bounded) bias. We establish that our algorithms converge to a locally CPT-value optimal policy.\nGradient-free method: We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to devise a globally optimizing policy update scheme. The idea is to use a reference model that eventually concentrates on the global minimum and then empirically approximate this reference distribution well-enough. The latter is achieved via natural exponential families in conjunction with KullbackLeibler (KL) divergence to measure the “distance” from the reference distribution. Unlike the setting of Chang et al. (2013), we neither observe the objective function (CPT-value) perfectly nor with zero-mean noise. We establish that our algorithm converges to a globally CPT-value optimal policy (assuming it exists)."
    }, {
      "heading" : "Related work",
      "text" : "Risk-sensitive reinforcement learning (RL) has received a lot of attention recently (cf. Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000). On the other hand, the CPT-value in (1) does not lend itself to stochastic approximation-based estimation schemes since the underlying probabilities are distorted via a weighting function. Unlike previous applications of SPSA and the algorithm from Chang et al. (2013), our he policy optimization algorithms suffer from biased estimates from the policy evaluation procedure, where the bias is non-zero and bounded. We overcome the bias asymptotically by slowly increasing the number of samples and show that the resulting policy optimization algorithms converge.\nThe closest related work is Lin (2013), where the authors propose a CPT-measure for an abstract MDP setting (see Bertsekas (2013)). While the CPT-value (1) that we aim to optimize is based on that in Lin (2013), we extend the latter work in several ways:\n(i) Unlike Lin (2013), we do not assume model information and develop an estimation scheme for the CPT-value function;\n(ii) Further, we also propose control algorithms using SPSA and model-based policy search in order to find a policy that optimizes the CPT-value.\nThe rest of the paper is organized as follows: In Section 2, we describe the empirical distribution based scheme for estimating the CPT-value of any random variable. In Section 4, we present the gradient-based algorithms for optimizing the CPT-value of an MDP. Next, in Section 5, we present a gradient-free modelbased algorithm for CPT-value optimization in an MDP. We provide the proofs of convergence for all the proposed algorithms in Section 6. We present the results from numerical experiments for the CPT-value estimation scheme in Section 7 and finally, provide the concluding remarks in Section 8."
    }, {
      "heading" : "2 CPT-value estimation",
      "text" : "In traditional settings, one is trying to estimate an expected value by obtaining samples from the distribution w.r.t. which the expectation is taken. However, in our setting, one obtains samples of the underlying random variable X using its distribution, but the CPT-value integral in (1) distorts this distribution using a non-linear weight function w. Thus, one cannot employ classic stochastic approximation schemes Robbins and Monro (1951) in our setting. Earlier works on risk-sensitive RL (cf. Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013)) involved estimating the value function using some form of temporal difference learning, which is a stochastic approximation version of a fixed point algorithm.\nSince the integral in (1) requires the CDF estimate (over the entire domain), our approach is to use the empirical distribution function (EDF) to approximate the CDF and then perform an integration of the weight-distorted EDF. We establish later in Propositions 1 and 2 that the resulting estimate converges and also with the canonical Monte Carlo convergence rate."
    }, {
      "heading" : "2.1 Basic algorithm",
      "text" : "Let Xi, i = 1, . . . , n denote n samples of the random variable X. Using conventional notation, we define the empirical distribution function (EDF) for u+(X) and u−(X), for any given real-valued functions u+ and u−, as follows:\nF̂+n (x) = 1\nn\nn∑\ni=1\n1(u+(Xi)≤x), and F̂ − n (x) =\n1\nn\nn∑\ni=1\n1(u−(Xi)≤x).\nUsing EDFs, the CPT-value (1) is estimated as follows:\nV̂n(X) =\n∫ +∞\n0 w+(1− F̂+n (x))dx−\n∫ +∞\n0 w−(1− F̂−n (x))dx. (2)\nNotice that we have substituted 1− F̂+n (x) (resp.1− F̂−n (x)) for P (u+(X) > x) (resp. P (u−(X) > x)) in (1) and then performed an integration of the complementary EDF composed with the weight function."
    }, {
      "heading" : "2.2 Main results",
      "text" : "Without any additional assumption, the integral in (2) may not even be finite, even though the weight functions w+, w− are bounded in [0, 1]. In order to overcome this difficulty, we make the following assumption:\nAssumption (A1). The weight functions w+, w− are Lipschitz with common constant L.\nThe above assumption covers a broad class of functions that are encountered in practice. For the asymptotic rate and sample complexity results below, we require the following assumption:\nAssumption (A2). The utility functions u+(X) and u−(X) are bounded above by M < ∞. The following result shows that the estimate (2) converges to the true CPT value almost surely and at the (nearly) canonical Monte Carlo asymptotic rate.\nProposition 1. (Asymptotic convergence and rate.) Under (A1), we have\nV̂n(X) → V (X) a.s. as n → ∞. (3)\nIn addition, if we assume (A2), then we have\nlim sup n→∞\n√ n\n2 ln lnn ||V̂n(X)− V (X)||∞ ≤ LM a.s.\nProof. Section 6.1.\nWhile the result in Proposition 1 establishes that (2) is unbiased estimate in the asymptotic sense, it is important to know the rate at which the estimate in (2) converges to the CPT-value. The following sample complexity result shows that O ( 1 ǫ2 ) number of samples are required to be ǫ-close to the CPT-value in high probability.\nProposition 2. (Sample Complexity) Under (A1) and (A2), for any ǫ, δ > 0, we have\nP (|V̂n(X)− V (X)| ≤ ǫ) ≥ 1− δ, for all n ≥ 2L2M2\nǫ2 ln\n4 δ .\nProof. Section 6.1."
    }, {
      "heading" : "3 CPT-value objective for MDPs",
      "text" : ""
    }, {
      "heading" : "3.1 Setting",
      "text" : "We consider a stochastic shortest path (SSP) problem with state space X = {0, 1, . . . , l}, with 0 denoting the terminating state that is absorbing. For any x ∈ S , let A(x) denote the set of actions in state x. Let g(x, a) denote the single-stage cost incurred by choosing action a in state s. A stationary randomized policy π maps states to probability distributions over the actions. We parameterize the polices and assume that each policy (identified by its parameter θ) in this set Θ ∈ Rd is continuously differentiable3 . Moreover, we make the standard assumption that all policies in the parameterized class that we consider are proper, i.e., there is a positive probability that the terminal state 0 is reached from any state x ∈ X . In other words, a proper policy makes the state 0 recurrent and the rest of the states transient for the underlying Markov chain."
    }, {
      "heading" : "3.2 CPT-value objective",
      "text" : "An episode is a simulated sample path of the SSP that starts in state x0 and ends in the recurrent state 0. Let Dπ(x0) be a random variable (r.v) that denotes the total cost from an episode simulated using policy π, i.e.,\nDπ(x0) = τ∑\nm=0\ng(sm, am),\n3In this paper, we use π and θ interchangeably to denote a policy.\nwhere am ∼ π(·, sm),∀m, i.e., the actions are chosen using policy π and τ is the first passage time to state 0.\nThe traditional objective is to minimize the expected value of the total cost defined above. In this paper, we adopt the CPT approach and aim to minimize the CPT-value function, defined as\nV π(x0) =\n∫ +∞\n0 w+(P (u+(Dπ(x0))) > z)dz −\n∫ +∞\n0 w−(P (u−(Dπ(x0))) > z)dz. (4)\nThe first component in (4) relates to the gains, while the second component handles the losses. If we assume that the single-stage rewards are positive (and bounded), then we have only the first component above. If both the weight and utility functions are identity maps, then it is easy to see that (4) is just the traditional value function (i.e., expectation of the return).\nHaving defined the CPT-value, a natural policy search objective is\nmin θ∈Θ\nV θ(x0).\nGiven only biased estimates of the CPT-value function, one requires a scheme for obtaining gradients of the CPT-value in order to descend in the policy parameter θ. We next introduce SPSA-based schemes for this purpose."
    }, {
      "heading" : "4 Gradient-based algorithms for optimizing CPT-value",
      "text" : ""
    }, {
      "heading" : "4.1 Policy gradient algorithm (PG-CPT-SPSA)",
      "text" : "Update rule. We update the policy parameter in the descent direction as follows:\nθn+1 = θn − an∇̂V θn (x0), (5)\nwhere ∇̂V θn (x0) is an estimate of the gradient of the CPT-value function (4) and an is a step-size chosen to satisfy (6) below.\nFig. 1 illustrates the overall flow of the policy gradient algorithm based on SPSA, while Algorithm 1 presents the pseudocode."
    }, {
      "heading" : "Gradient estimation",
      "text" : "Given that we operate in a learning setting and only have biased estimates of the CPT-value from (2), we require a simulation optimization scheme that outputs ∇̂V θn (x0). Simultaneous perturbation methods are a general class of stochastic gradient schemes that optimize a function given only noisy sample values - see\nAlgorithm 1 Structure of PG-CPT-SPSA algorithm. Input: initial parameter θ0, perturbation constants δn > 0, trajectory lengths {mn}, step-sizes {an}. for n = 0, 1, 2, . . . do\nGenerate {∆in, i = 1, . . . , d} using Rademacher distribution, independent of {∆m,m = 0, 1, . . . , n− 1}\nPolicy Evaluation (Trajectory 1) Simulate mn episodes of the SSP using policy (θn + δn∆n)\nObtain CPT-value estimate V̂ (θn+δn∆n)n (x0) using (2)\nPolicy Evaluation (Trajectory 2) Simulate mn episodes of the SSP using policy θn − δn∆n Obtain CPT-value estimate V̂ θn+δn∆nn (x 0)) using (2) Policy Improvement (Gradient descent)\nSPSA-based gradient estimate ∇̂V θ(x0) = V̂ θn+δn∆n(x0)−V̂ θn−δn∆n (x0)2δn ∆ −1 n\nUpdate θn+1 = θn − an∇̂V θ(x0) end for Return θn\nBhatnagar et al. (2013) for textbook introduction. SPSA is a well-known scheme that estimates the gradient using two sample values as follows:\n∇V θ(x0) ≈ V̂ θn+δn∆n n (x 0)− V̂ θn−δn∆nn (x0) 2δn ∆−1n ,\nwhere δn is a positive scalar that satisfies (6) below and ∆−1n = ( 1 ∆1n , . . . , 1 ∆dn )T, where {∆in, i = 1, . . . , d, n = 1, 2, . . .} are i.i.d. Rademacher random variables. This idea of using two-point feedback for estimating the gradient has been employed in various settings. Machine learning applications include bandit/stochastic convex optimization - see Hazan (2015), Flaxman et al. (2005), Duchi et al. (2013). However, the idea applies to non-convex functions as well - see Spall (2005), Bhatnagar et al. (2013). The correctness of the gradient estimate is briefly sketched below (see Lemma 7 for a formal proof): By using suitable Taylor’s expansions, we obtain\nV θn+δn∆n(x0)− V θn−δn∆n(x0) 2δn∆in −∇iV θn(x0) = N∑\nj=1,j 6=i\n∆jn(n) ∆in(n) ∇jV θn(x0)\n︸ ︷︷ ︸ (I)\n+O(δ2n).\nThe conditional expectation of term (I) above is zero since ∆n are Rademacher. Hence, in expectation, SPSA-based gradient estimate is only an O(δ2n) term away from the true gradient.\nJustification for increasing number mn of episodes. Recall that the CPT-value estimation scheme is biased, i.e., providing samples with policy θn at instant n, we obtain its CPT-value estimate as V θ(x0)+ ǫθn. Here ǫθn denotes the bias. Now, rewrite the update rule (5) as follows:\nθn+1 = θn − an ( (V θn+δn∆n(x0)− V θn−δn∆n(x0))\n2δn ∆−1n + ηn\n) ,\nwhere ηn = (ǫθn+δn∆nn − ǫθn−δn∆nn )\n2δn ∆−1n . Let ζn =\n∑n l=0 alηl. Then, a critical requirement that allows us\nto ignore the bias term ζn is the following condition (cf. Lemma 1 in Chapter 2 of Borkar (2008)):\nsup l≥0\n(ζn+l − ζn) → 0 as n → ∞.\nWhile Theorems 1–2 show that the bias ǫθ is bounded above, it is insufficient to establish convergence of the policy gradient recursion (5) and hence, we increase the number of samples mn so that the bias vanishes asymptotically - see assumption (A3) below for the precise condition on the rate at which mn has to increase.\nAssumption (A3). The step-sizes an and the perturbation constants δn are positive ∀n and satisfy\nan, δn → 0, 1√\nmnδn → 0 as n → ∞,\n∑\nn\nan = ∞ and ∑\nn\na2n δ2n < ∞. (6)\nWhile the conditions on an and δn are standard for SPSA-based algorithms, the condition on mn is motivated by the earlier discussion. A simple choice that satisfies the above conditions is an = a0/n, mn = m0nν and δn = δ0/nγ , for some ν, γ > 0 with γ < ν/2."
    }, {
      "heading" : "Convergence result",
      "text" : "Theorem 1. (Strong convergence) Assume (A1)-(A3). Let θn be bounded almost surely 4 and let θ∗ be an asymptotically stable equilibrium of the ordinary differential equation (ODE): θ̇t = −∇V θt(x0), with domain of attraction D(θ∗). Then, if there exists a compact subset D of D(θ∗) such that θn visits D infinitely often, we have θn → θ∗ a.s. as n → ∞.\nProof. See Section 6.2."
    }, {
      "heading" : "4.2 Policy Newton algorithm (PN-CPT-SPSA)",
      "text" : "Need for second order methods. While stochastic gradient descent methods are useful in minimizing the CPT-value given biased estimates, they are sensitive to the choice of the step-size sequence {an}. In particular, for a step-size choice an = a0/n, if a0 is not chosen to be greather than 1/3λmin(∇2V θ ∗ (x0)), then the optimum rate of convergence is not achieved. Here λmin denotes the minimum eigenvalue, while θ∗ corresponds to the optimal policy (see Theorem 1). A standard approach to overcome this step-size dependency is to use iterate averaging, suggested independently by Polyak Polyak and Juditsky (1992) and Ruppert Ruppert (1991). The idea is to use larger step-sizes an = 1/nα, where α ∈ (1/2, 1), and then combine it with averaging of the iterates. However, it is well-known that iterate averaging is optimal only in an asymptotic sense, while finite time bounds show that the initial condition is not forgotten sub- exponentially fast (see Theorem 2.2 in Fathi and Frikha (2013)). Thus, it is optimal to average iterates only after a sufficient number of iterations have passed and all the iterates are very close to the optimum. However, the latter situation serves as a stopping condition in practice. An alternative approach is to employ step-sizes of the form an = (a0/n)Mn, where Mn converges to( ∇2V θ∗(x0) )−1 , i.e., the inverse of the Hessian of the CPT-value at the optimum θ∗. Such a scheme gets rid of the step-size dependency (one can set a0 = 1) and still obtains optimal convergence rates. This is the motivation behind having a second-order policy optmization scheme and we next adapt the scheme from Spall (2000) for estimating the Hessian inverse.\n4This is not a restrictive assumption, as one can project to a compact set to ensure boundedness. See the discussion pp. 40-41 of Kushner and Clark (1978) and also remark E.1 of Bhatnagar et al. (2013).\nUpdate rule. A second-order method would update the policy parameter as follows:\nθn+1 =θn − anΥ(Hn)−1∇̂V θn (x0), (7)\nHn = n\nn+ 1 Hn−1 +\n1\nn+ 1 Ĥn, (8)\nwhere ∇̂V θn (x0) is an estimate of the gradient of the CPT-value function and Ĥn and Hn denote the Hessian estimate and its smooth counterpart, respectively. Notice that we invert Hn in each iteration, and to ensure that this inversion is feasible (so that the θ-recursion descends), we project Hn onto the set of positive definite matrices using the operator Υ. The operator has to be such that asymptotically Υ(Hn) should be the same as Hn (since the latter would converge to the true Hessian), while ensuring inversion is feasible in the initial iterations. A simple way is to have Υ(Hn) as a diagonal matrix and then add a positive scalar δn to the diagonal elements so as to ensure invertibility - see Gill et al. (1981), Spall (2000) for a similar operator.\nAlgorithm 2 presents the pseudocode. Fig. 1 illustrates the overall structure of PN-CPT-SPSA, except that three system trajectories with a different perturbation sequence are used.\nAlgorithm 2 Structure of PN-CPT-SPSA algorithm. Input: initial parameter θ0, perturbation constants δn > 0, trajectory lengths {mn}, step-sizes {an}. for n = 0, 1, 2, . . . do\nGenerate {∆in, ∆̂in, i = 1, . . . , d} using Rademacher distribution, independent of {∆m, ∆̂m,m = 0, 1, . . . , n− 1}\nPolicy Evaluation (Trajectory 1) Simulate mn episodes of the SSP using policy (θn + δn(∆n + ∆̂n))\nObtain CPT-value estimate V̂ (θn+δn(∆n+∆̂n))n (x0) using (2)\nPolicy Evaluation (Trajectory 2) Simulate mn episodes of the SSP using policy (θn − δn(∆n + ∆̂n)) Obtain CPT-value estimate V̂ θn+δn(∆n+∆̂n)n (x0)) using (2)\nPolicy Evaluation (Trajectory 3) Simulate mn episodes of the SSP using policy θn Obtain CPT-value estimate V̂ θnn (x 0)) using (2) Policy Improvement (Newton decrement)\nSPSA-based gradient estimate ∇̂iV θn (x0) = V̂\nθn+δn(∆n+∆̂n) n (x0)− V̂ θn−δn(∆n+∆̂n)n (x0)\n2δn∆in\nSPSA-based Hessian estimate Ĥn = V̂\nθn+δn(∆n+∆̂n) n (x0) + V̂ θn−δn(∆n+∆̂n) n (x0)− 2V̂ θnn (x0) δ2n∆ i n∆̂ j n\nPolicy update: θn+1 = θn − anΥ(Hn)−1∇̂V θn (x0) Hessian update: Hn = nn+1Hn−1 + 1 n+1Ĥn\nend for Return θn"
    }, {
      "heading" : "Gradient and Hessian estimation",
      "text" : "We estimate the Hessian of the CPT-value function using the scheme suggested by Bhatnagar and Prashanth (2015). As in the case of the first-order method, we use Rademacher random variables to simultaneously\nperturb all the coordinates. However, in this case, we require three system trajectories with corresponding policy parameters θn + δn(∆n + ∆̂n), θn − δn(∆n + ∆̂n) and θn, where {∆in, ∆̂in, i = 1, . . . , d, n = 1, 2, . . .} are i.i.d. Rademacher random variables. Using the CPT-value estimates for the aforementioned policy parameters, we estimate the Hessian and the gradient of the CPT-value function as follows: For i, j = 1, . . . , d, set\nGradient: ∇̂iV θnn (x0) = V̂\nθn+δn(∆n+∆̂n) n (x0)− V̂ θn−δn(∆n+∆̂n)n (x0)\n2δn∆in ,\nHessian: Ĥ i,jn = V̂\nθn+δn(∆n+∆̂n) n (x0) + V̂ θn−δn(∆n+∆̂n) n (x0)− 2V̂ θnn (x0) δ2n∆ i n∆̂ j n .\nNotice that the above estimates require three samples, while the the second order SPSA algorithm proposed first in Spall (2000) required four. Both the gradient estimate ∇̂V θnn (x0) and the Hessian estimate Ĥn can be shown to be an O(δ2n) term away from the true gradient ∇V θn (x0) and Hessian ∇2V θn (x0), respectively (see Lemmas 8–9 in Appendix 6.3)."
    }, {
      "heading" : "Convergence result",
      "text" : "Theorem 2. (Strong convergence) Assume (A1)-(A3). Let θn be bounded almost surely, an, δn satisfy (6) and also ∑ n 1 (n+1)2δ2n\n< ∞. Further, assume the following: (i) ∀n, θ, ∃ ρ > 0 independent of n and θ, such that (θ − θ∗)TΥ(Hn)−1∇V θ(x0) ≥ ρ ‖θn − θ∗‖. (ii) Υ(Hn)−1 exists a.s. ∀n, δ2nΥ(Hn)−1 → 0 and E ∥∥Υ(H̄n)−1 ∥∥2+η ≤ α0, for some η, α0 > 0.\nThen, we have θn → θ∗ and Hn → ∇2V θ ∗ (x0) a.s. as n → ∞.\nProof. See Section 6.3."
    }, {
      "heading" : "5 Gradient-free algorithm for optimizing CPT-value",
      "text" : "We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to our setting of optimizing CPT-value in MDPs. We require that there exists a unique global optimum θ∗ for the problem minθ∈Θ V θ(x0).\nTo illustrate the main idea in the algorithm, assume we know the form of V θ(x0). Then, the idea is to generate a sequence of reference distributions gk(θ) on the policy space Θ, such that it eventually concentrates on the global optimum θ∗. One simple way, suggested in Chapter 4 of Chang et al. (2013) is\ngk(θ) = H(V θ(x0))gk−1(θ)∫\nΘH(V θ ′(x0))gk−1(θ′)ν(dθ′)\n, ∀ θ ∈ Θ, (9)\nwhere ν is the Lebesgue/counting measure on Θ and H is a strictly decreasing function. The above construction for gk’s assigns more weight to policies having lower CPT-values and it is easy to show that gk converges to a point-mass concentrated at θ∗.\nNext, consider a setting where one can obtain the CPT-value V θ(x0) (without any noise) for any policy θ. In this case, we consider a family of parameterized distributions, say {f(·, η), η ∈ C} and incrementally update the distribution parameter η such that it minimizes the following KL divergence:\nD(gk, f(·, η)) := Egk [ ln gk(R(Θ)) f(R(Θ), η) ] = ∫ Θ ln gk(θ) f(θ, η) gk(θ)ν(dθ),\nwhere R(Θ) is a random vector taking values in the policy space Θ. An algorithm to optimize CPT-value in this noise-less setting would perform the following update for the parameter ηn:\nηn+1 ∈ argmin η∈C Eηn [ [H(V R(Θ)(x0))]n f(R(Θ), ηn) ln f(R(Θ), η) ] , (10)\nwhere Eηn [V R(Θ)(x0)] = ∫ Θ V\nθ(x0)f(θ, ηn)ν(dθ). Finally, we get to our setting where we only obtain biased estimate of the CPT-value V θ(x0) for any policy θ. Recall that the bias is due to a finite sample run followed by estimation scheme (2). As in the case of SPSA-based algorithms, it is easy to see that the number of samples mn (in iteration n) should asymptotically increase to infinity. Assuming this setup, the gradient-free model-based policy search algorithm would involve the following steps (see Algorithm 3 for the pseudocode):\nStep 1 (Candidate policies): Generate Nn policy parameters {θ1n, . . . , θNn} using the distribution f(·, ηn).\nStep 2 (CPT-value estimation): Run mn SSP episodes for each of the policies in θin, i = 1, . . . , Nn and return CPT-value estimates V̂ θ i n(x0).\nStep 3 (Parameter update):\nηn+1 ∈ argmin η∈C\n1\nNn\nNn∑\ni=1\n[H(V̂ θin(x0))]n f(θin, ηn) ln f(θin, η). (11)\nA few remarks are in order.\nRemark 1. (Choice of sampling distribution) A natural question is how to compute the KL-distance (10) in order to update the policy. A related question is how to choose the family of distributions f(·, θ), so that the update (10) can be done efficiently. One choice is to employ the natural exponential family (NEF) since it ensures that the KL distance in (10) can be computed analytically.\nRemark 2. (Elite sampling) In practice, it is efficient to use only an elite portion of the candidate policies that have been sampled in order to update the sampling distribution f(·, η). This can be achieved by using a quantile estimate of the CPT-value function corresponding to candidate policies that were estimated in a particular iteration. The intuition here is that using policies that have performed well guides the policy search procedure towards better regions more efficiently in comparison to an alternative that uses all the candidate policies for updating η."
    }, {
      "heading" : "Convergence result",
      "text" : "Theorem 3. Assume (A1)-(A2). Suppose that multivariate normal densities are used for the sampling distribution, i.e., ηn = (µn,Σn), where µn and Σn denote the mean and covariance of the normal densities. Then,\nlim n→∞\nµn = θ ∗ and lim\nn→∞ Σn = 0d×d a.s. (13)\nProof. See Section 6.4.\n1Here V̂ θ (i) n n (x 0) denotes the ith order statistic.\n2Here Ĩ(z, χ) :=\n \n 0 if z ≤ χ− ε, (z − χ+ ε)/ε if χ− ε < z < χ, 1 if z ≥ χ.\nAlgorithm 3 Structure of gradient free model-based policy optimization algorithm. Input: family of distributions {f(·, η)}, initial parameter vector η0 s.t. f(θ, η0) > 0 ∀ θ ∈ Θ, trajectory lengths {mn}, ρ0 ∈ (0, 1], N0 > 1, ε > 0, α > 1, λ ∈ (0, 1), strictly decreasing function H. for n = 0, 1, 2, . . . do\nCandidate Policies Generate Nn policy parameters using the mixed distribution f̃(·, ηn) = (1−λ)f(·, η̃n)+λf(·, η0). Denote these candidate policies by Λn = {θ1n, . . . , θNn}. CPT-value Estimation for i = 1, 2, . . . , Nn do\nSimulate mn episodes of the SSP using policy θin Obtain CPT-value estimate V̂ θ i n\nn (x0) using (2) end for\nElite Sampling\nOrder the CPT-value estimates1 {V̂ θ(1)nn (x0), . . . , V̂ θ (Nn) n n (x 0)}. Compute the (1− ρn)-quantile from the above samples as follows:\nχ̃n(ρn, Nn) = V̂ θ ⌈(1−ρk)Nk⌉ n n (x 0). (12)\nThresholding if n = 0 or χ̃n(ρn, Nn) ≥ χ̄n−1 + ε then\nSet χ̄k = χ̃k(ρn, Nn), ρk+1 = ρn, Nk+1 = Nk and Set θ∗n = θ1−ρn , where θ1−ρn is the policy that corresponds to the (1− ρn)-quantile in (12).\nelse find the largest ρ̄ ∈ (0, ρn) such that χ̃n(ρ̄, Nn) ≥ χ̄n−1 + ε; if ρ̄ exists then\nSet χ̄n = χ̃n(ρ̄, Nn), ρk+1 = ρ̄, Nn+1 = Nn and θ∗n = θ1−ρ̄ else\nSet χ̄n = V̂ θ∗n−1 n (x0),ρn+1 = ρn,Nn+1 = ⌈αNn⌉, and θ∗n = θ∗n−1.\nend if end if\nSampling Distribution Update Parameter update2:\nηn+1 ∈ argmin η∈C\n1\nNn\nNn∑\ni=1\n[H(V̂ θin(x0))]n f̃(θ, ηn) Ĩ ( V̂ θ i n(x0), χ̄n ) ln f(θ, η).\nend for Return θn"
    }, {
      "heading" : "6 Convergence Proofs",
      "text" : ""
    }, {
      "heading" : "6.1 Proofs for CPT-value estimator",
      "text" : "In order to prove Proposition 1, we require the dominated convergence theorem in its generalized form, which is provided below.\nTheorem 4. (Generalized Dominated Convergence theorem) Let {fn}∞n=1 be a sequence of measurable functions on E that converge pointwise a.e. on a measurable space E to f . Suppose there is a sequence {gn} of integrable functions on E that converge pointwise a.e. on E to g such that |fn| ≤ gn for all n ∈ N. If lim\nn→∞\n∫ E gn = ∫ E g, then limn→∞ ∫ E fn = ∫ E f .\nProof. This is a standard result that can be found in any textbook on measure theory. For instance, see Theorem 2.3.11 in Athreya and Lahiri (2006)."
    }, {
      "heading" : "Proof of Proposition 1: Asymptotic convergence",
      "text" : "For notational convenience, we shall henceforth denote u+(X) and u−(X) by U+ and U−, respectively.\nProof. Recall that the CPT-value for any r.v. X is defined as\nV (X) =\n∫ +∞\n0 w+(P (U+ > x)dx−\n∫ +∞\n0 w−(P (U− > x)dx.\nAlso, recall that we estimate V (X) using the empirical distribution as follows:\nV̂n(X) =\n∫ +∞\n0 w+(1− F̂+n (x))dx−\n∫ +∞\n0 w−(1− F̂−n (x))dx, (14)\nwhere\nF̂+n (x) = 1\nn\nn∑\ni=1\n1(U+≤x), and F̂ − n (x) =\n1\nn\nn∑\ni=1\n1(U−≤x).\nWe first prove the claim for the first integral in (14), i.e., we show\n∫ +∞\n0 w+(1− F̂+n (x))dx →\n∫ +∞\n0 w+(P (U+ > x)dx. (15)\nSince w+ is Lipschitz continuous with constant L, we have almost surely that w+(1− F̂n(x)) ≤ L(1− F̂n(x)), for all n and w+((P (U+ > x)) ≤ L · (P (U+ > x), since w+(0) = 0.\nNotice that the empirical distribution function F̂+n (x) generates a Stieltjes measure which takes mass 1/n on each of the sample points U+i .\nWe have ∫ +∞\n0 (P (U+ > x))dx = E(U+)\nand\n∫ +∞\n0 (1− F̂+n (x))dx =\n∫ +∞\n0\n∫ ∞\nx dF̂n(t)dx. (16)\nSince F̂+n (x) has bounded support on R ∀n, the integral in (16) is finite. Applying Fubini’s theorem to the RHS of (16), we obtain\n∫ +∞\n0\n∫ ∞\nx dF̂n(t)dx =\n∫ +∞\n0\n∫ t\n0 dxdF̂n(t) =\n∫ +∞\n0 tdF̂n(t) =\n1\nn\nn∑\ni=1\nU+ [i] , (17)\nwhere U+[i], i = 1, . . . , n denote the order statistics, i.e., U + [1] ≤ . . . ≤ U + [n].\nNow, notice that\n1\nn\nn∑\ni=1\nU+[i] = 1\nn\nn∑\ni=1\nU+i a.s−→ E(U+),\nFrom the foregoing,\nlim n→∞\n∫ +∞\n0 L · (1− F̂n(x))dx a.s−→\n∫ +∞\n0 L · (P (U+ > x))dx.\nHence, we have ∫ ∞\n0 w(+)(1− F̂n(x))dx a.s.−−→\n∫ ∞\n0 w(+)(P (U+) > x)dx.\nThe claim in (15) now follows by invoking the generalized dominated convergence theorem by setting fn = w +(1 − F̂+n (x)) and gn = L · (1 − F̂n(x)), and noticing that L · (1 − F̂n(x)) a.s.−−→ L(P (U+ > x)) uniformly ∀x. The latter fact is implied by the Glivenko-Cantelli theorem (cf. Chapter 2 of Wasserman (2015)).\nFollowing similar arguments, it is easy to show that\nw−(1− F̂−n (x))dx → ∫ +∞\n0 w−(P (U−) > x)dx.\nThe final claim regarding convergence of V̂n(X) to V (X) now follows.\nProof of Proposition 1: Asymptotic convergence rate\nIn order to prove the convergence rate of the policy optimization algorithms, we need a uniform bound on the distance between Vn(X) and the CPT-value V (X), i.e., ||V̂n(X) − V (X)||∞. For this purpose, Proposition 1 had a law of the iterated logarithm type result, which states that ||V̂n(X)− V (X)||∞ is of the order O(n−1/2) (ignoring log-factors) for sufficiently large n. Before proving this result, we recall the law of the iterated logarithm when empirical distribution function is in one dimension (see Van der Vaart (2000) for a detailed description):\nTheorem 5. (Law of the iterated logarithm.) Let F̂n denote the empirical distribution and F the true distribution. Then, for sufficiently large n, we have\nlim sup n→∞\n√ n\n2loglogn ||F̂n − F ||∞ ≤\n1 2 , a.s.\nProof. (Proof of Proposition 1: Asymptotic convergence rate)\nLet’s focus on the difference ∣∣∣ ∫ +∞ 0 w +(P (u+((X))) > x)dx− ∫ +∞ 0 w +(1− F̂+n (x))dx ∣∣∣. We have\n∣∣∣∣ ∫ +∞\n0 w+(P (U+) > x)dx−\n∫ +∞\n0 w+(1− F̂+n (x))dx\n∣∣∣∣\n= ∣∣∣∣ ∫ M\n0 w+(P (U+) > x)dx−\n∫ M\n0 w+(1− F̂+n (x))dx\n∣∣∣∣\n≤ ∣∣∣∣ ∫ M\n0 L · |P (U+ < x)− F̂+n (x)|dx\n∣∣∣∣\n≤ LM sup x∈R\n∣∣∣P (U+ < x)− F̂+n (x) ∣∣∣ .\nUsing the law of iterated logarithm, we obtain\nlim sup n→∞\n√ n\n2loglogn\n∥∥∥∥ ∫ +∞\n0 w+(1− F̂+n (x))−\n∫ +∞\n0 P (U+ > x)dx ∥∥∥∥ ∞ ≤ 1 2 LM, a.s.\nAlong similar lines, we obtain\nlim sup n→∞\n√ n\n2loglogn\n∥∥∥∥ ∫ +∞\n0 w−(1− F̂−n (x))−\n∫ +∞\n0 P (U− > x)dx ∥∥∥∥ ∞ ≤ 1 2 LM, a.s.\nThe main claim follows by summing the above two inequalities."
    }, {
      "heading" : "Proof of Proposition 2",
      "text" : "For proving Proposition 2, we require the following well-known inequality that provide a finite-time bound on the distance between empirical distribution and the true distribution:\nLemma 6. (Dvoretzky-Kiefer-Wolfowitz (DKW) inequality) Let F̂n(x) = 1n ∑n i=1 1((Xi)≤x) denote the empirical distribution of a r.v. X, with X1, . . . ,Xn being sampled from the true distribution F (X). The, for any n and ǫ > 0, we have\nP (sup x∈R\n|F̂n(x)− F (x)| > ǫ) ≤ 2e−2nǫ 2 .\nThe reader is referred to Chapter 2 of Wasserman (2015) for more on empirical distributions in general and DKW inequality in particular.\nProof. (Theorem 2) Since U+ is bounded above by M and w+ is Lipschitz with constant L, we have\n∣∣∣∣ ∫ +∞\n0 w+(P (U+) > x)dx−\n∫ +∞\n0 w+(1− F̂+n (x))dx\n∣∣∣∣\n= ∣∣∣∣ ∫ M\n0 w+(P (U+) > x)dx−\n∫ M\n0 w+(1− F̂+n (x))dx\n∣∣∣∣\n≤ ∣∣∣∣ ∫ M\n0 L · |P (U+ < x)− F̂+n (x)|dx\n∣∣∣∣\n≤LM sup x∈R\n∣∣∣P (U+ < x)− F̂+n (x) ∣∣∣ .\nNow, plugging in the DKW inequality, we obtain\nP (∣∣∣∣ ∫ +∞\n0 w+(P (U+) > x)dx−\n∫ +∞\n0 w+(1− F̂+n (x))dx\n∣∣∣∣ > ǫ/2 )\n≤ P ( LM sup\nx∈R\n∣∣∣(P (U+ < x)− F̂+n (x) ∣∣∣ > ǫ/2 ) ≤ 2e−n ǫ2 2L2M2 . (18)\nAlong similar lines, we obtain\nP (∣∣∣∣ ∫ +∞\n0 w−(P (U−) > x)dx−\n∫ +∞\n0 w−(1− F̂−n (x))dx\n∣∣∣∣ > ǫ/2 ) ≤ 2e−n ǫ2 2L2M2 . (19)\nCombining (18) and (19), we obtain P (|V̂n(X) − V (X)| > ǫ) ≤ P (∣∣∣∣ ∫ +∞\n0 w+(P (U+) > x)dx−\n∫ +∞\n0 w+(1− F̂+n (x))dx\n∣∣∣∣ > ǫ/2 )\n+ P (∣∣∣∣ ∫ +∞\n0 w−(P (U−) > x)dx−\n∫ +∞\n0 w−(1− F̂−n (x))dx\n∣∣∣∣ > ǫ/2 )\n≤ 4e−n ǫ2 2L2M2 .\nAnd the claim follows."
    }, {
      "heading" : "6.2 Proofs for PG-CPT-SPSA",
      "text" : "To prove the main result in Theorem 1, we first show that the gradient estimate using SPSA is only an order O(δ2n) term away from the true gradient. The following lemma establishes this claim and its proof can be inferred from Spall (1992), though we give it here for the sake of completeness. Following this lemma, we complete the proof of Theorem 1 by invoking the well-known Kushner-Clark lemma Kushner and Clark (1978) and this involves verfying conditions A2.2.1-A2.2.4 there.\nLemma 7. Let Fn = σ(θm,m ≤ n,∆m,m < n), n ≥ 1. Then, for any i = 1, . . . , d, we have almost surely,\n∣∣∣∣∣E [ V̂ θn+δn∆nn (x 0)− V̂ θn−δn∆nn (x0) 2δn∆in ∣∣∣∣∣Fn ] −∇iV θn(x0) ∣∣∣∣∣ → 0 as n → ∞. (20)\nProof. Recall that the CPT-value estimation scheme is biased, i.e., providing samples with policy θ, we obtain its CPT-value estimate as V θ(x0) + ǫθ. Here ǫθ denotes the bias.\nWe claim\nE\n[ V̂ θn+δn∆nn (x\n0)− V̂ θn−δn∆nn (x0) 2δn∆in\n| Fn ] = E [ V θn+δn∆nn (x\n0)− V θn−δn∆nn (x0) 2δn∆in\n| Fn ] + E [ηn | Fn] ,\n(21)\nwhere ηn =\n( ǫθn+δn∆ − ǫθn−δn∆\n2δn\n) ∆−1n is the bias arising out of the empirical distribution based CPT-\nvalue estimation scheme. From Proposition 1, we see that, ǫθ = LM √\n2 log logmn mn , provided mn is suffi-\nciently large. Thus, ηn = O (√\n2 log logmn mn 1 δn ) and since 1√mnδn → 0 by assumption (see (6) in the main\npaper), we have that ηn goes to zero asymptotically. In other words,\nE\n[ V̂ θn+δn∆nn (x\n0)− V̂ θn−δn∆nn (x0) 2δn∆in\n| Fn ] n→∞−−−→ E [ V θn+δn∆n(x0)− V θn−δn∆n(x0)\n2δn∆in | Fn\n] . (22)\nWe now analyse the RHS of the above. By using suitable Taylor’s expansions,\nV θn+δn∆n(x0) = V θn(x0) + δn∆ T n∇V θn(x0) + δ2\n2 ∆Tn∇2V θn(x0)∆n +O(δ3n),\nV θn−δn∆n(x0) = V θn(x0)− δn∆Tn∇V θn(x0) + δ2\n2 ∆Tn∇2V θn(x0)∆n +O(δ3n).\nFrom the above, it is easy to see that\nV θn+δn∆n(x0)− V θn−δn∆n(x0) 2δn∆in −∇iV θn(x0) = N∑\nj=1,j 6=i\n∆jn ∆in\n∇jV θn(x0) ︸ ︷︷ ︸\n(I)\n+O(δ2n).\nTaking conditional expectation on both sides, we obtain\nE\n[ V θn+δn∆n(x0)− V θn−δn∆n(x0)\n2δn∆in | Fn\n] =∇iV θn(x0) + E   N∑\nj=1,j 6=i\n∆jn ∆in\n ∇jV θn(x0) +O(δ2n)\n=∇iV θn(x0) +O(δ2n). (23)\nThe first equality above follows from the fact that ∆n is distributed according to a d-dimensional vector of Rademacher random variables and is independent of Fn. The second inequality follows by observing that ∆in is independent of ∆ j n, for any i, j = 1, . . . , d.\nThe claim follows by using the fact that δn → 0 as n → ∞."
    }, {
      "heading" : "Proof of Theorem 1",
      "text" : "Proof. We first rewrite the update rule (5) as follows:\nθn+1 = θn − an(∇V θn(x0) + βn + ηn), (24)\nwhere\nβn = E\n( (V̂ θn+δn∆n(x0)− V̂ θn−δn∆n(x0))\n2δn ∆−1n | Fn\n) −∇V θ(x0), and\nηn =\n( V̂ θn+δn∆n(x0)− V̂ θn−δn∆n(x0)\n2δn\n) ∆−1n − E ( (V̂ θn+δn∆n(x0)− V̂ θn−δn∆n(x0))\n2δn ∆−1n | Fn\n) .\nIn the above, βn is the bias in the gradient estimate due to SPSA and ηn is a martingale difference sequence.. Convergence of (24) can be inferred from Theorem 2.3.1 on pp. 29 of Kushner and Clark (1978), provided we verify that the assumptions A2.2.1 to A2.2.3 and A2.2.4” of Kushner and Clark (1978) are satisfied for θn governed by (5). We verify them below:\n• A2.2.1 requires that ∇V θ(x0) is a continuous Rd-valued function and this holds by assumption in our setting.\n• A2.2.2 requires that the sequences βn and ηn, n ≥ 0 are bounded and converge to zero asymptotically. Lemma 7 above establishes that the bias βn is O(δ2n) and since δn → 0 as n → ∞, it is easy to see that A2.2.2 is satisfied for βn. As noted in the proof of Lemma 7, ηn → 0 as n → ∞ and hence, A2.2.2 is satisfied for ηn as well. • A2.2.3 requires that the step-sizes an, n ≥ 0 satisfy a(n) → 0 as n → ∞ and ∑\nn an = ∞. These step-size conditions hold by assumption in our setting - see (6) in the main paper.\n• Finally, we verify A2.2.4” using arguments similar to those used in Spall (1992) for the classic SPSA algorithm: We first recall Doob’s martingale inequality (see (2.1.7) on pp. 27 of Kushner and Clark (1978)):\nP ( sup m≥0 ‖Wm‖ ≥ ǫ ) ≤ 1 ǫ2 lim m E ‖Wm‖2 . (25)\nApplying the above inequality to the martingale sequence {Wn}, where Wn := ∑n−1\ni=0 aiηi, n ≥ 1, we obtain\nP ( sup m≥n ∥∥∥∥∥ m∑\ni=n\naiηi ∥∥∥∥∥ ≥ ǫ ) ≤ 1 ǫ2 E ∥∥∥∥∥ ∞∑\ni=n\naiηi ∥∥∥∥∥ 2 = 1 ǫ2 ∞∑\ni=n\na2iE ‖ηi‖2 . (26)\nWe now bound E ‖ηi‖2 as follows:\nE\n( V̂ θn+δn∆n(x0)− V̂ θn−δn∆n(x0)\n2δn∆in\n)2\n≤ 1 4δ2n\n[ E ( 1\n(∆in) 2+2α1\n)] 1 1+α1 [ E [ (V̂ θn+δn∆n(x0)− V̂ θn−δn∆n(x0)) ]2+2α2] 11+α2 (27)\n≤ 1 4δ2n\n([ E [ (V̂ θn+δn∆n(x0)) ]2+2α2] 11+α2 + [ E [ (V̂ θn−δn∆n(x0)) ]2+2α2] 11+α2 )\n(28)\n≤C δ2n , for some C < ∞. (29)\nThe inequality in (27) uses Holder’s inequality, with α1, α2 > 0 satisfying 11+α1 + 1 1+α2 = 1. The\nequality in (28) above follows owing to the fact that E (\n1 (∆in) 2+2α1\n) = 1 as ∆in is Rademacher. The\ninequality in (29) follows by using the fact that, for any θ, the CPT-value estimate V̂ θ(x0) = V θ(x0)+ ǫθ. We assume a finite state-action spaced SSP (which implies that the costs maxs,a g(s, a) < ∞) and consider only proper policies (which implies that the total cost Dθ(x0) is bounded for any policy θ) and finally, by (A1), the weight functions are Lipschitz - these together imply that V θ(x0) is bounded for any policy θ. The bias ǫθ is bounded by Proposition 1 in the main paper.\nThus, E ‖ηi‖2 ≤ Cδ2n for some C < ∞. Plugging this in (26), we obtain\nlim n→∞ P ( sup m≥n ∥∥∥∥∥ m∑\ni=n\naiηi ∥∥∥∥∥ ≥ ǫ ) ≤ dC ǫ2 lim n→∞ ∞∑\ni=n\na2i δ2i = 0.\nThe equality above follows from (A3) in the main paper.\nThe claim follows."
    }, {
      "heading" : "6.3 Proofs for PN-CPT-SPSA",
      "text" : "Before proving Theorem 2, we bound the bias in the SPSA based estimate of the Hessian in the following lemma.\nLemma 8. Let Fn = σ(θm,m ≤ n,∆m, ∆̂m,m < n), n ≥ 1. Then, for any i, j = 1, . . . , d, we have almost surely, ∣∣∣∣∣∣ E   V̂ θn+δn(∆n+∆̂n) n (x0) + V̂ θn−δn(∆n+∆̂n) n (x0)− 2V̂ θnn (x0) δ2n∆ i n∆̂ j n ∣∣∣∣∣∣ Fn  −∇2i,jV θn(x0) ∣∣∣∣∣∣ → 0 as n → ∞.\n(30)\nProof. As in the proof of Lemma 7, we can ignore the bias from the CPT-value estimation scheme and conclude that,\nE\n  V̂ θn+δn(∆n+∆̂n) n (x0) + V̂\nθn−δn(∆n+∆̂n) n (x0)− 2V̂ θnn (x0) δ2n∆ i n∆̂ j n | Fn\n \nn→∞−−−→ E [ V θn+δn(∆n+∆̂n)(x0) + V θn−δn(∆n+∆̂n)(x0)− 2V θn(x0)\nδ2n∆ i n∆̂ j n\n| Fn ] . (31)\nNow, the RHS of the above approximates the true gradient with only an O(δ2n) error and this can be inferred using arguments similar to that used in the proof of Proposition 4.2 of Bhatnagar and Prashanth (2015). We provide the proof here for the sake of completeness. Using Taylor’s expansion as in Lemma 7, we obtain\nV θn+δn(∆n+∆̂n)(x0) + V θn−δn(∆n+∆̂n)(x0)− 2V θn(x0) δ2n∆ i n∆̂ j n\n= (∆n + ∆̂n) T∇2V θn(x0)(∆n + ∆̂n) △i(n)△̂j(n) +O(δ2n)\n=\nN∑\nl=1\nN∑\nm=1\n∆ln∇2l,mV θn(x0)∆mn ∆in∆̂ j n + 2 N∑\nl=1\nN∑\nm=1\n∆ln∇2l,mV θn(x0)∆̂mn ∆in∆̂ j n + N∑\nl=1\nN∑\nm=1\n∆̂ln∇2l,mV θn(x0)∆̂mn ∆in∆̂ j n +O(δ2n).\nTaking conditional expectation, we observe that the first and last term above become zero, while the second term becomes ∇2ijV θn(x0). The claim follows by using the fact that δn → 0 as n → ∞.\nLemma 9. Let Fn = σ(θm,m ≤ n,∆m,m < n), n ≥ 1. Then, for any i = 1, . . . , d, we have almost surely,\n∣∣∣∣∣E [ V̂ θn+δn(∆n+∆̂n) n (x0)− V̂ θn−δn(∆n+∆̂n)n (x0)\n2δn∆in\n∣∣∣∣∣Fn ] −∇iV θn(x0) ∣∣∣∣∣ → 0 as n → ∞. (32)\nProof. As in the proof of Lemma 7, we can ignore the bias from the CPT-value estimation scheme and conclude that,\nE\n  V̂ θn+δn(∆n+∆̂n) n (x0)− V̂ θn−δn(∆n+∆̂n)n (x0)\n2δn∆in | Fn\n  n→∞−−−→ E [ V θn+δn∆n(x0)− V θn−δn∆n(x0)\n2δn∆in | Fn\n] .\nThe rest of the proof amounts to showing that the RHS of the above approximates the true gradient with an O(δ2n) correcting term and this can be done in a similar manner as the proof of Lemma 7."
    }, {
      "heading" : "Proof of Theorem 2",
      "text" : "Proof. The claim related to convergence of θn can be proven in a manner similar to Theorem 1a of Spall (2000) after observing that Lemma 9 implies that the SPSA based gradient estimate in (7) is only an order O(δ2n) term away from the true gradient ∇V θn(x0).\nFor proving the claim related to convergence of the Hessian recursion (8), we use the proof technique from Spall (2000). The proof proceeds as follows:\nLet Wm = Ĥm − E [ Ĥm ∣∣∣ θm ] . Then, EWm = 0 and ∑ m E‖Wm‖2 m2 < ∞, since E [ δ2m ∥∥∥Ĥm ∥∥∥ 2 ] < ∞,∀m and ∑n 1(n+1)2δ2n < ∞ by assumption. Now, applying a martingale convergence result from p. 397 of Laha and Rohatgi (1979) to Wm, we obtain\n1\nn+ 1\nn∑\nm=0\nĤm − E [ Ĥm ∣∣∣ θm ] → 0 a.s. (33)\nFrom (31) and Lemma 8, we know that E [ Ĥn ∣∣∣ θn ] = ∇2V θn(x0) +O(δ2n). Hence,\n1\nn+ 1\nn∑\nm=0\nE [ Ĥm ∣∣∣ θm ] = 1\nn+ 1\nn∑\nm=0\n∇2V θm(x0) +O(δ2m) → ∇2V θ ∗ (x0) a.s.\nThe final step above follows from the fact that the Hessian is continuous near θn and the fact that θn converges almost surely to θ∗. Thus, we obtain\n1\nn+ 1\nn∑\nm=0\nĤm → ∇2V θ ∗ (x0) a.s.\nand the claim follows by observing that Hn = 1\nn+ 1\n∑n m=0 Ĥm."
    }, {
      "heading" : "6.4 Proofs for gradient-free policy optimization algorithm",
      "text" : "We begin by remarking that there is one crucial difference between our algorithm and MRAS2 from Chang et al. (2013): MRAS2 has an expected function value objective, i.e., it aims to minimize a function by using sample observations that have zero-mean noise. On the other hand, the objective in our setting is the CPT-value, which distorts the underlying transition probabilities. The implication here is that MRAS2 can estimate the expected value using sample averages, while we have to resort to integrating the empirical distribution.\nSince we obtain samples of the objective (CPT) in a manner that differs from MRAS2, we need to establish that the thresholding step in Algorithm 3 achieves the same effect as it did in MRAS2. This is achieved by the following lemma, which is a variant of Lemma 4.13 from Chang et al. (2013), adapted to our setting.\nLemma 10. The sequence of random variables {θ∗n, n = 0, 1, . . .} in Algorithm 3 converges w.p.1 as n → ∞.\nProof. Let An be the event that either the first if statement (see 16) is true or the second if statement in the else clause (see 21) is true within the Thresholding step of Algorithm 3. Let Bn := {V θ ∗ n(x0)−V θ∗n−1(x0) ≤ ε 2}. Whenever An holds, we have V̂ θ∗n n (x0)− V̂ θ∗n−1 n (x0) ≥ ε and hence, we obtain\nP (An ∩ Bn) ≤P ({ V̂ θ ∗ n n (x 0)− V̂ θ ∗ n−1 n−1 (x 0) ≥ ε } ∩ { V θ ∗ n(x0)− V θ∗n−1(x0) ≤ ε\n2\n})\n≤P ( ⋃\nθ∈Λn,θ′∈Λn−1\n{{ V̂ θn (x 0)− V̂ θ′n−1(x0) ≥ ε } ∩ { V θ(x0)− V θ′(x0) ≤ ε\n2\n}})\n≤ ∑ θ∈Λn,θ′∈Λk−1 P ({ V̂ θn (x 0)− V̂ θ′n−1(x0) ≥ ε } ∩ { V θ(x0)− V θ′(x0) ≤ ε 2 })\n≤|Λn||Λn−1| sup θ,θ′∈Θ\nP ({\nV̂ θn (x 0)− V̂ θ′n−1(x0) ≥ ε\n} ∩ { V θ(x0)− V θ′(x0) ≤ ε\n2\n})\n≤|Λn||Λn−1| sup θ,θ′∈Θ\nP ( V̂ θn (x 0)− V̂ θ′n−1(x0)− V θ(x0) + V θ ′ (x0) ≥ ε\n2\n)\n≤|Λn||Λn−1| sup θ,θ′∈Θ\n( P ( V̂ θn (x\n0)− V θ(x0) ≥ ε 4\n) + P ( V̂ θ ′ n−1(x 0)− V θ′(x0) ≥ ε\n4\n))\n≤4|Λn||Λk−1|e−mn ǫ2 8L2M2 .\nFrom the foregoing, we have ∑∞\nn=1 P (An ∩ Bn) < ∞ since mn → ∞ as n → ∞. Applying the BorelCantelli lemma, we obtain\nP (An ∩ Bn i.o.) = 0.\nFrom the above, it is implied that if An happens infinitely often, then Bcn will also happen infinitely often. Hence,\n∞∑\nn=1\n[ V θ ∗ n(x0)− V θ∗n−1(x0) ] =\n∑\nn: Anoccurs\n[ V θ ∗ n(x0)− V θ∗n−1(x0) ] +\n∑\nn: Acnoccurs\n[ V θ ∗ n(x0)− V θ∗n−1(x0) ]\n= ∑\nn: Anoccurs\n[ V θ ∗ n(x0)− V θ∗n−1(x0) ]\n= ∑\nn: An∩Bnoccurs\n[ V θ ∗ n(x0)− V θ∗n−1(x0) ] +\n∑\nn: An∩Bcnoccurs\n[ V θ ∗ n(x0)− V θ∗n−1(x0) ]\n=∞ w.p.1, since ε > 0.\nIn the above, the first equality follows from the fact that if the else clause in the second if statement (see 23) in Algorithm 3 is hit, then θ∗n = θ ∗ n−1. From the last equality above, we conclude that it is a contradiction because, V θ(x0) > V θ ∗ (x0) for any θ (since θ∗ is the global minimum). The main claim now follows since An can happen only a finite number of times."
    }, {
      "heading" : "Proof of Theorem 3",
      "text" : "Proof. Once we have established Lemma 10, the rest of the proof follows in an identical fashion as the proof of Corollary 4.18 of Chang et al. (2013). This is because our algorithm operates in a similar manner as MRAS2 w.r.t. generating the candidate solution using a parameterized family f(·, η) and updating the distribution parameter η. The difference, as mentioned earlier, is the manner in which the samples are generated and the objective (CPT-value) function is estimated. The aforementioned lemma established that the elite sampling and thresholding achieve the same effect as that in MRAS2 and hence the rest of the proof follows from Chang et al. (2013)."
    }, {
      "heading" : "7 Numerical Experiments for CPT-value estimation scheme",
      "text" : "Setup. We consider a random variable X that is uniformly distributed in [0, 5]. We set the utility function to be identity and define the weight function w as follows:\nw(x) = { 2 3 (2x− x2) 0 ≤ x < 12 1 3 + 2 3x 2 1 2 ≤ x ≤ 1\nThe graph of w(x) can be seen in Fig. 2. Thus, the CPT-value of X can be seen to be\nV (X) =\n∫ ∞\n0 w(P (X > x))dx, (34)\nSince we consider gains only, the second integral component in V (X) from (1) is zero.\nBackground. Let X1, . . . ,Xn be i.i.d. random variables with underlying distribution U [0, 5]. Then, the empirical distribution function is defined as\nF̂n(x) = 1\nn\nn∑\ni=1\n1(Xi≤x). (35)\nThus, 1 − F̂n(x) is an unbiased estimator of P (X > x). From (35), it is clear that F̂n(x) generates a Lebesgue Stieljes measure which takes mass 1n at each of the points Xi, i = 1, . . . , n. So does w(1−F̂n(x)). Based on this observation, one can see the weight function equivalently as follows:\nw(1− F̂n(x)) = ∫ ∞\nx dw(1 − F̂n(y))\nHence, the estimate V̂n(X) of V (X) is arrived at as follows:\nV̂n(X) =\n∫ ∞\n0 w(1 − F̂n(x))dx =−\n∫ +∞\n0\n∫ ∞\nx dw(1 − F̂n(y)) (36)\n=− ∫ +∞\n0 xdw(1 − F̂n(x))\n=\nn∑\ni=1\nX[i]\n( w ( i+ 1\nn\n) − w ( i\nn\n)) , (37)\nwhere X[i] denotes the ith order statistic of the sample set {X1, . . . ,Xn}. Note that, we let w+ ( n+1 n ) = 1, ∀n. We use (37) to estimate the CPT-value V (X). Analytically, V (X) = 2.5 for a U [0, 5] distributed random variable X. In the following, we report the accuracy of the estimator V̂n(X) using simulation experiments.\nResults. Fig. 3 shows the estimation error obtained for a random variable X with distribution U [0.5]. Here, the estimation error denotes the absolute difference between the estimated CPT-value (37) and true CPT-value, which is 2.5. From Fig. 3, it is evident that the CPT-value estimation scheme (37) converges rapidly to the true CPT-value."
    }, {
      "heading" : "8 Conclusions and Future Work",
      "text" : "We considered the problem of optimizing the CPT-value of an MDP. For this purpose, we first designed an estimation scheme that evaluates the CPT-value of any given policy. Next, using this estimator as the inner loop, we proposed three policy optimization algorithms. The first two algorithms use SPSA to estimate the gradient and/or Hessian of the CPT-value function and then perform a descent in the policy parameter, whereas the third algorithm is gradient-free and is based on a reference distribution that concentrates on the global optimum. Using an empirical distribution over the policy space in conjunction with KL-divergence to the reference distribution, we get a global policy optimization scheme. We provided theoretical convergence guarantees for all the proposed algorithms. In particular, we first showed that the CPT-value estimator\nconverges asymptotically and at the optimal rate as well. Next, the SPSA based policy optimization algorithms were shown to converge to a locally CPT-value optimal policy, while the gradient-free algorithm was shown to converge to a globally CPT-value optimal policy. As future work, it would be interesting to test our algorithms on simple as well as sophisticated empirical domains."
    } ],
    "references" : [ {
      "title" : "Measure theory and probability theory",
      "author" : [ "K.B. Athreya", "S.N. Lahiri" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Athreya and Lahiri.,? \\Q2006\\E",
      "shortCiteRegEx" : "Athreya and Lahiri.",
      "year" : 2006
    }, {
      "title" : "Abstract Dynamic Programming",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 2013
    }, {
      "title" : "Simultaneous perturbation Newton algorithms for simulation optimization",
      "author" : [ "S. Bhatnagar", "L.A. Prashanth" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "Bhatnagar and Prashanth.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bhatnagar and Prashanth.",
      "year" : 2015
    }, {
      "title" : "Stochastic Recursive Algorithms for Optimization, volume 434",
      "author" : [ "S. Bhatnagar", "H.L. Prasad", "L.A. Prashanth" ],
      "venue" : null,
      "citeRegEx" : "Bhatnagar et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bhatnagar et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic Approximation: A Dynamical Systems Viewpoint",
      "author" : [ "V. Borkar" ],
      "venue" : null,
      "citeRegEx" : "Borkar.,? \\Q2008\\E",
      "shortCiteRegEx" : "Borkar.",
      "year" : 2008
    }, {
      "title" : "Learning algorithms for risk-sensitive control",
      "author" : [ "V. Borkar" ],
      "venue" : "In Proceedings of the 19th International Symposium on Mathematical Theory of Networks and Systems–MTNS,",
      "citeRegEx" : "Borkar.,? \\Q2010\\E",
      "shortCiteRegEx" : "Borkar.",
      "year" : 2010
    }, {
      "title" : "Risk-constrained Markov decision processes",
      "author" : [ "V. Borkar", "R. Jain" ],
      "venue" : "In IEEE Conference on Decision and Control (CDC),",
      "citeRegEx" : "Borkar and Jain.,? \\Q2010\\E",
      "shortCiteRegEx" : "Borkar and Jain.",
      "year" : 2010
    }, {
      "title" : "Simulation-based Algorithms for Markov Decision Processes",
      "author" : [ "H.S. Chang", "J. Hu", "M.C. Fu", "S.I. Marcus" ],
      "venue" : null,
      "citeRegEx" : "Chang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2013
    }, {
      "title" : "Optimal rates for zero-order convex optimization: the power of two function evaluations",
      "author" : [ "J.C. Duchi", "M.I. Jordan", "M.J. Wainwright", "A. Wibisono" ],
      "venue" : "arXiv preprint arXiv:1312.2139,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2013
    }, {
      "title" : "Transport-entropy inequalities and deviation estimates for stochastic approximation schemes",
      "author" : [ "M. Fathi", "N. Frikha" ],
      "venue" : "Electron. J. Probab,",
      "citeRegEx" : "Fathi and Frikha.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fathi and Frikha.",
      "year" : 2013
    }, {
      "title" : "Original and cumulative prospect theory: A discussion of empirical differences",
      "author" : [ "H. Fennema", "P. Wakker" ],
      "venue" : "Journal of Behavioral Decision Making,",
      "citeRegEx" : "Fennema and Wakker.,? \\Q1997\\E",
      "shortCiteRegEx" : "Fennema and Wakker.",
      "year" : 1997
    }, {
      "title" : "Online convex optimization in the bandit setting: gradient descent without a gradient",
      "author" : [ "A.D. Flaxman", "A.T. Kalai", "H.B. McMahan" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Flaxman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Flaxman et al\\.",
      "year" : 2005
    }, {
      "title" : "Practical Optimization",
      "author" : [ "P.E. Gill", "W. Murray", "M.H. Wright" ],
      "venue" : null,
      "citeRegEx" : "Gill et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Gill et al\\.",
      "year" : 1981
    }, {
      "title" : "Online Convex Optimization",
      "author" : [ "E. Hazan" ],
      "venue" : null,
      "citeRegEx" : "Hazan.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hazan.",
      "year" : 2015
    }, {
      "title" : "Prospect theory: An analysis of decision under risk",
      "author" : [ "D. Kahneman", "A. Tversky" ],
      "venue" : "Econometrica: Journal of the Econometric Society,",
      "citeRegEx" : "Kahneman and Tversky.,? \\Q1979\\E",
      "shortCiteRegEx" : "Kahneman and Tversky.",
      "year" : 1979
    }, {
      "title" : "Stochastic Approximation Methods for Constrained and Unconstrained Systems",
      "author" : [ "H. Kushner", "D. Clark" ],
      "venue" : null,
      "citeRegEx" : "Kushner and Clark.,? \\Q1978\\E",
      "shortCiteRegEx" : "Kushner and Clark.",
      "year" : 1978
    }, {
      "title" : "Stochastic Systems with Cumulative Prospect Theory",
      "author" : [ "K. Lin" ],
      "venue" : "Ph.D. Thesis,",
      "citeRegEx" : "Lin.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 2013
    }, {
      "title" : "Acceleration of stochastic approximation by averaging",
      "author" : [ "B.T. Polyak", "A.B. Juditsky" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "Polyak and Juditsky.,? \\Q1992\\E",
      "shortCiteRegEx" : "Polyak and Juditsky.",
      "year" : 1992
    }, {
      "title" : "Policy Gradients for CVaR-Constrained MDPs. In Algorithmic Learning Theory, pages 155–169",
      "author" : [ "L.A. Prashanth" ],
      "venue" : null,
      "citeRegEx" : "Prashanth.,? \\Q2014\\E",
      "shortCiteRegEx" : "Prashanth.",
      "year" : 2014
    }, {
      "title" : "Actor-critic algorithms for risk-sensitive MDPs",
      "author" : [ "L.A. Prashanth", "M. Ghavamzadeh" ],
      "venue" : "In Proceedings of Advances in Neural Information Processing Systems",
      "citeRegEx" : "Prashanth and Ghavamzadeh.,? \\Q2013\\E",
      "shortCiteRegEx" : "Prashanth and Ghavamzadeh.",
      "year" : 2013
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Robbins and Monro.,? \\Q1951\\E",
      "shortCiteRegEx" : "Robbins and Monro.",
      "year" : 1951
    }, {
      "title" : "Optimization of conditional value-at-risk",
      "author" : [ "R.T. Rockafellar", "S. Uryasev" ],
      "venue" : "Journal of risk,",
      "citeRegEx" : "Rockafellar and Uryasev.,? \\Q2000\\E",
      "shortCiteRegEx" : "Rockafellar and Uryasev.",
      "year" : 2000
    }, {
      "title" : "Stochastic approximation. Handbook of Sequential Analysis, pages 503–529",
      "author" : [ "D. Ruppert" ],
      "venue" : null,
      "citeRegEx" : "Ruppert.,? \\Q1991\\E",
      "shortCiteRegEx" : "Ruppert.",
      "year" : 1991
    }, {
      "title" : "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation",
      "author" : [ "J.C. Spall" ],
      "venue" : "IEEE Trans. Auto. Cont.,",
      "citeRegEx" : "Spall.,? \\Q1992\\E",
      "shortCiteRegEx" : "Spall.",
      "year" : 1992
    }, {
      "title" : "Adaptive stochastic approximation by the simultaneous perturbation method",
      "author" : [ "J.C. Spall" ],
      "venue" : "IEEE Trans. Autom. Contr.,",
      "citeRegEx" : "Spall.,? \\Q2000\\E",
      "shortCiteRegEx" : "Spall.",
      "year" : 2000
    }, {
      "title" : "Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control, volume 65",
      "author" : [ "J.C. Spall" ],
      "venue" : null,
      "citeRegEx" : "Spall.,? \\Q2005\\E",
      "shortCiteRegEx" : "Spall.",
      "year" : 2005
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Sutton.,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1988
    }, {
      "title" : "Variance adjusted actor-critic algorithms",
      "author" : [ "A. Tamar", "S. Mannor" ],
      "venue" : "arXiv preprint arXiv:1310.3697,",
      "citeRegEx" : "Tamar and Mannor.,? \\Q2013\\E",
      "shortCiteRegEx" : "Tamar and Mannor.",
      "year" : 2013
    }, {
      "title" : "Policy gradients with variance related risk criteria",
      "author" : [ "A. Tamar", "D.D. Castro", "S. Mannor" ],
      "venue" : "In Proceedings of the Twenty-Ninth International Conference on Machine Learning,",
      "citeRegEx" : "Tamar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tamar et al\\.",
      "year" : 2012
    }, {
      "title" : "Advances in prospect theory: Cumulative representation of uncertainty",
      "author" : [ "A. Tversky", "D. Kahneman" ],
      "venue" : "Journal of Risk and Uncertainty,",
      "citeRegEx" : "Tversky and Kahneman.,? \\Q1992\\E",
      "shortCiteRegEx" : "Tversky and Kahneman.",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for a justification, in particular via empirical tests using human subjects).",
      "startOffset" : 159,
      "endOffset" : 187
    }, {
      "referenceID" : 10,
      "context" : "A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for a justification, in particular via empirical tests using human subjects).",
      "startOffset" : 188,
      "endOffset" : 214
    }, {
      "referenceID" : 10,
      "context" : "A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see Kahneman and Tversky (1979), Fennema and Wakker (1997) for a justification, in particular via empirical tests using human subjects). However, PT is lacking in some theoretical aspects as it violates first-order stochastic dominance1. Cumulative prospect theory (CPT) (Tversky and Kahneman 1992) uses a similar measure as PT, except that the weights are a function of cumulative probabilities. First, separate the gains and losses as Consider the following example from Fennema and Wakker (1997): Suppose there are 20 prospects (outcomes) ranging from −10 to 180, each with probability 0.",
      "startOffset" : 188,
      "endOffset" : 654
    }, {
      "referenceID" : 7,
      "context" : "Gradient-free method: We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to devise a globally optimizing policy update scheme.",
      "startOffset" : 80,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "Gradient-free method: We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to devise a globally optimizing policy update scheme. The idea is to use a reference model that eventually concentrates on the global minimum and then empirically approximate this reference distribution well-enough. The latter is achieved via natural exponential families in conjunction with KullbackLeibler (KL) divergence to measure the “distance” from the reference distribution. Unlike the setting of Chang et al. (2013), we neither observe the objective function (CPT-value) perfectly nor with zero-mean noise.",
      "startOffset" : 80,
      "endOffset" : 525
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al.",
      "startOffset" : 0,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).",
      "startOffset" : 0,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).",
      "startOffset" : 0,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf.",
      "startOffset" : 0,
      "endOffset" : 189
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).",
      "startOffset" : 0,
      "endOffset" : 342
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)).",
      "startOffset" : 0,
      "endOffset" : 376
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)).",
      "startOffset" : 0,
      "endOffset" : 501
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)).",
      "startOffset" : 0,
      "endOffset" : 519
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000).",
      "startOffset" : 0,
      "endOffset" : 828
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000). On the other hand, the CPT-value in (1) does not lend itself to stochastic approximation-based estimation schemes since the underlying probabilities are distorted via a weighting function.",
      "startOffset" : 0,
      "endOffset" : 1012
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000). On the other hand, the CPT-value in (1) does not lend itself to stochastic approximation-based estimation schemes since the underlying probabilities are distorted via a weighting function. Unlike previous applications of SPSA and the algorithm from Chang et al. (2013), our he policy optimization algorithms suffer from biased estimates from the policy evaluation procedure, where the bias is non-zero and bounded.",
      "startOffset" : 0,
      "endOffset" : 1282
    }, {
      "referenceID" : 3,
      "context" : "Borkar (2010); Borkar and Jain (2010); Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). However, previous works consider either an exponential utility formulation (cf. Borkar (2010)) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. Tamar et al. (2012); Prashanth and Ghavamzadeh (2013)). Another constraint alternative is to bound the CVaR, while minimizing the usual cost objective (cf. Borkar and Jain (2010); Prashanth (2014)). However, the risk measure we adopt is inspired by CPT, and this measure is both non-coherent and non-convex and hence, departs from the approach used in aforementioned references. For instance, expected utility and variance constraint approaches used some form of temporal difference learning Sutton (1988) for policy evaluation, while CVaR-based formulation could perform gradient descent since there was a well-known convex optimization formulation for CVaR Rockafellar and Uryasev (2000). On the other hand, the CPT-value in (1) does not lend itself to stochastic approximation-based estimation schemes since the underlying probabilities are distorted via a weighting function. Unlike previous applications of SPSA and the algorithm from Chang et al. (2013), our he policy optimization algorithms suffer from biased estimates from the policy evaluation procedure, where the bias is non-zero and bounded. We overcome the bias asymptotically by slowly increasing the number of samples and show that the resulting policy optimization algorithms converge. The closest related work is Lin (2013), where the authors propose a CPT-measure for an abstract MDP setting (see Bertsekas (2013)).",
      "startOffset" : 0,
      "endOffset" : 1615
    }, {
      "referenceID" : 1,
      "context" : "The closest related work is Lin (2013), where the authors propose a CPT-measure for an abstract MDP setting (see Bertsekas (2013)).",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "The closest related work is Lin (2013), where the authors propose a CPT-measure for an abstract MDP setting (see Bertsekas (2013)). While the CPT-value (1) that we aim to optimize is based on that in Lin (2013), we extend the latter work in several ways:",
      "startOffset" : 113,
      "endOffset" : 211
    }, {
      "referenceID" : 16,
      "context" : "(i) Unlike Lin (2013), we do not assume model information and develop an estimation scheme for the CPT-value function; (ii) Further, we also propose control algorithms using SPSA and model-based policy search in order to find a policy that optimizes the CPT-value.",
      "startOffset" : 11,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "Thus, one cannot employ classic stochastic approximation schemes Robbins and Monro (1951) in our setting.",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013)) involved estimating the value function using some form of temporal difference learning, which is a stochastic approximation version of a fixed point algorithm.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013)) involved estimating the value function using some form of temporal difference learning, which is a stochastic approximation version of a fixed point algorithm.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "Borkar (2010), Tamar and Mannor (2013), Prashanth and Ghavamzadeh (2013)) involved estimating the value function using some form of temporal difference learning, which is a stochastic approximation version of a fixed point algorithm.",
      "startOffset" : 0,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "Lemma 1 in Chapter 2 of Borkar (2008)): sup l≥0 (ζn+l − ζn) → 0 as n → ∞.",
      "startOffset" : 24,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "A standard approach to overcome this step-size dependency is to use iterate averaging, suggested independently by Polyak Polyak and Juditsky (1992) and Ruppert Ruppert (1991).",
      "startOffset" : 121,
      "endOffset" : 148
    }, {
      "referenceID" : 14,
      "context" : "A standard approach to overcome this step-size dependency is to use iterate averaging, suggested independently by Polyak Polyak and Juditsky (1992) and Ruppert Ruppert (1991). The idea is to use larger step-sizes an = 1/nα, where α ∈ (1/2, 1), and then combine it with averaging of the iterates.",
      "startOffset" : 121,
      "endOffset" : 175
    }, {
      "referenceID" : 8,
      "context" : "2 in Fathi and Frikha (2013)).",
      "startOffset" : 5,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "2 in Fathi and Frikha (2013)). Thus, it is optimal to average iterates only after a sufficient number of iterations have passed and all the iterates are very close to the optimum. However, the latter situation serves as a stopping condition in practice. An alternative approach is to employ step-sizes of the form an = (a0/n)Mn, where Mn converges to ( ∇2V θ∗(x0) )−1 , i.e., the inverse of the Hessian of the CPT-value at the optimum θ∗. Such a scheme gets rid of the step-size dependency (one can set a0 = 1) and still obtains optimal convergence rates. This is the motivation behind having a second-order policy optmization scheme and we next adapt the scheme from Spall (2000) for estimating the Hessian inverse.",
      "startOffset" : 5,
      "endOffset" : 681
    }, {
      "referenceID" : 8,
      "context" : "2 in Fathi and Frikha (2013)). Thus, it is optimal to average iterates only after a sufficient number of iterations have passed and all the iterates are very close to the optimum. However, the latter situation serves as a stopping condition in practice. An alternative approach is to employ step-sizes of the form an = (a0/n)Mn, where Mn converges to ( ∇2V θ∗(x0) )−1 , i.e., the inverse of the Hessian of the CPT-value at the optimum θ∗. Such a scheme gets rid of the step-size dependency (one can set a0 = 1) and still obtains optimal convergence rates. This is the motivation behind having a second-order policy optmization scheme and we next adapt the scheme from Spall (2000) for estimating the Hessian inverse. This is not a restrictive assumption, as one can project to a compact set to ensure boundedness. See the discussion pp. 40-41 of Kushner and Clark (1978) and also remark E.",
      "startOffset" : 5,
      "endOffset" : 871
    }, {
      "referenceID" : 3,
      "context" : "1 of Bhatnagar et al. (2013).",
      "startOffset" : 5,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "A simple way is to have Υ(Hn) as a diagonal matrix and then add a positive scalar δn to the diagonal elements so as to ensure invertibility - see Gill et al. (1981), Spall (2000) for a similar operator.",
      "startOffset" : 146,
      "endOffset" : 165
    }, {
      "referenceID" : 12,
      "context" : "A simple way is to have Υ(Hn) as a diagonal matrix and then add a positive scalar δn to the diagonal elements so as to ensure invertibility - see Gill et al. (1981), Spall (2000) for a similar operator.",
      "startOffset" : 146,
      "endOffset" : 179
    }, {
      "referenceID" : 2,
      "context" : "Gradient and Hessian estimation We estimate the Hessian of the CPT-value function using the scheme suggested by Bhatnagar and Prashanth (2015). As in the case of the first-order method, we use Rademacher random variables to simultaneously",
      "startOffset" : 112,
      "endOffset" : 143
    }, {
      "referenceID" : 23,
      "context" : "Notice that the above estimates require three samples, while the the second order SPSA algorithm proposed first in Spall (2000) required four.",
      "startOffset" : 115,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "5 Gradient-free algorithm for optimizing CPT-value We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to our setting of optimizing CPT-value in MDPs.",
      "startOffset" : 109,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : "5 Gradient-free algorithm for optimizing CPT-value We perform a non-trivial adaptation of the algorithm from Chang et al. (2013) to our setting of optimizing CPT-value in MDPs. We require that there exists a unique global optimum θ∗ for the problem minθ∈Θ V θ(x0). To illustrate the main idea in the algorithm, assume we know the form of V θ(x0). Then, the idea is to generate a sequence of reference distributions gk(θ) on the policy space Θ, such that it eventually concentrates on the global optimum θ∗. One simple way, suggested in Chapter 4 of Chang et al. (2013) is gk(θ) = H(V (x))gk−1(θ) ∫ ΘH(V θ (x0))gk−1(θ′)ν(dθ′) , ∀ θ ∈ Θ, (9) where ν is the Lebesgue/counting measure on Θ and H is a strictly decreasing function.",
      "startOffset" : 109,
      "endOffset" : 569
    }, {
      "referenceID" : 0,
      "context" : "11 in Athreya and Lahiri (2006).",
      "startOffset" : 6,
      "endOffset" : 32
    }, {
      "referenceID" : 22,
      "context" : "The following lemma establishes this claim and its proof can be inferred from Spall (1992), though we give it here for the sake of completeness.",
      "startOffset" : 78,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "Following this lemma, we complete the proof of Theorem 1 by invoking the well-known Kushner-Clark lemma Kushner and Clark (1978) and this involves verfying conditions A2.",
      "startOffset" : 104,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "29 of Kushner and Clark (1978), provided we verify that the assumptions A2.",
      "startOffset" : 6,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "29 of Kushner and Clark (1978), provided we verify that the assumptions A2.2.1 to A2.2.3 and A2.2.4” of Kushner and Clark (1978) are satisfied for θn governed by (5).",
      "startOffset" : 6,
      "endOffset" : 129
    }, {
      "referenceID" : 22,
      "context" : "4” using arguments similar to those used in Spall (1992) for the classic SPSA algorithm: We first recall Doob’s martingale inequality (see (2.",
      "startOffset" : 44,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "27 of Kushner and Clark (1978)):",
      "startOffset" : 6,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "2 of Bhatnagar and Prashanth (2015). We provide the proof here for the sake of completeness.",
      "startOffset" : 5,
      "endOffset" : 36
    }, {
      "referenceID" : 23,
      "context" : "The claim related to convergence of θn can be proven in a manner similar to Theorem 1a of Spall (2000) after observing that Lemma 9 implies that the SPSA based gradient estimate in (7) is only an order O(δ2 n) term away from the true gradient ∇V θn(x0).",
      "startOffset" : 90,
      "endOffset" : 103
    }, {
      "referenceID" : 23,
      "context" : "The claim related to convergence of θn can be proven in a manner similar to Theorem 1a of Spall (2000) after observing that Lemma 9 implies that the SPSA based gradient estimate in (7) is only an order O(δ2 n) term away from the true gradient ∇V θn(x0). For proving the claim related to convergence of the Hessian recursion (8), we use the proof technique from Spall (2000). The proof proceeds as follows: Let Wm = Ĥm − E [ Ĥm ∣∣ θm ] .",
      "startOffset" : 90,
      "endOffset" : 374
    }, {
      "referenceID" : 23,
      "context" : "The claim related to convergence of θn can be proven in a manner similar to Theorem 1a of Spall (2000) after observing that Lemma 9 implies that the SPSA based gradient estimate in (7) is only an order O(δ2 n) term away from the true gradient ∇V θn(x0). For proving the claim related to convergence of the Hessian recursion (8), we use the proof technique from Spall (2000). The proof proceeds as follows: Let Wm = Ĥm − E [ Ĥm ∣∣ θm ] . Then, EWm = 0 and ∑ m E‖Wm‖ m2 < ∞, since E [ δ2 m ∥∥Ĥm ∥∥ 2 ] < ∞,∀m and n 1 (n+1)2δ2 n < ∞ by assumption. Now, applying a martingale convergence result from p. 397 of Laha and Rohatgi (1979) to Wm, we obtain 1 n+ 1 n ∑",
      "startOffset" : 90,
      "endOffset" : 630
    }, {
      "referenceID" : 7,
      "context" : "4 Proofs for gradient-free policy optimization algorithm We begin by remarking that there is one crucial difference between our algorithm and MRAS2 from Chang et al. (2013): MRAS2 has an expected function value objective, i.",
      "startOffset" : 153,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "4 Proofs for gradient-free policy optimization algorithm We begin by remarking that there is one crucial difference between our algorithm and MRAS2 from Chang et al. (2013): MRAS2 has an expected function value objective, i.e., it aims to minimize a function by using sample observations that have zero-mean noise. On the other hand, the objective in our setting is the CPT-value, which distorts the underlying transition probabilities. The implication here is that MRAS2 can estimate the expected value using sample averages, while we have to resort to integrating the empirical distribution. Since we obtain samples of the objective (CPT) in a manner that differs from MRAS2, we need to establish that the thresholding step in Algorithm 3 achieves the same effect as it did in MRAS2. This is achieved by the following lemma, which is a variant of Lemma 4.13 from Chang et al. (2013), adapted to our setting.",
      "startOffset" : 153,
      "endOffset" : 885
    }, {
      "referenceID" : 7,
      "context" : "18 of Chang et al. (2013). This is because our algorithm operates in a similar manner as MRAS2 w.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "18 of Chang et al. (2013). This is because our algorithm operates in a similar manner as MRAS2 w.r.t. generating the candidate solution using a parameterized family f(·, η) and updating the distribution parameter η. The difference, as mentioned earlier, is the manner in which the samples are generated and the objective (CPT-value) function is estimated. The aforementioned lemma established that the elite sampling and thresholding achieve the same effect as that in MRAS2 and hence the rest of the proof follows from Chang et al. (2013).",
      "startOffset" : 6,
      "endOffset" : 540
    } ],
    "year" : 2017,
    "abstractText" : "Cumulative prospect theory (CPT) is known to model human decisions well, with substantial empirical evidence supporting this claim. CPT works by distorting probabilities and is more general than the classic expected utility and coherent risk measures. We bring this idea to a risk-sensitive reinforcement learning (RL) setting and design algorithms for both estimation and control. The estimation scheme that we propose uses the empirical distribution in order to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of policy optimization procedures for a Markov decision process (MDP). We propose both gradient-based as well as gradient-free policy optimization algorithms. The former includes both first-order and second-order methods that are based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation (SPSA), while the latter is based on a reference distribution that concentrates on the global optima. Using an empirical distribution over the policy space in conjunction with Kullback-Leibler (KL) divergence to the reference distribution, we get a global policy optimization scheme. We provide theoretical convergence guarantees for all the proposed algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}