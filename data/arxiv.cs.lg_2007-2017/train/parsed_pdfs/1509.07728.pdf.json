{
  "name" : "1509.07728.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Stochastic Linear Optimization under One-bit Feedback",
    "authors" : [ "Lijun Zhang", "Tianbao Yang", "Rong Jin" ],
    "emails" : [ "zhanglj@lamda.nju.edu.cn", "tianbao-yang@uiowa.edu", "rongjin@cse.msu.edu", "zhouzh@lamda.nju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 9.\n07 72\n8v 1\n[ cs\n√ T ), which\nmatches the optimal result of stochastic linear bandits.\nKeywords: bandit, online, regret bound, stochastic linear optimization, logit model"
    }, {
      "heading" : "1. Introduction",
      "text" : "Online learning with bandit feedback plays an important role in several industrial domains, such as ad placement, website optimization, and packet routing (Bubeck and Cesa-Bianchi, 2012). A canonical framework for studying this problem is the multi-armed bandits (MAB), which models the situation that a gambler must choose which of K slot machines to play (Robbins, 1952). In the basic stochastic MAB, each arm is assumed to deliver rewards that are drawn from a fixed but unknown distribution. The goal of the gambler is to minimize the regret, namely the difference between his expected cumulative reward and that of the best single arm in hindsight (Auer et al., 2002). Although MAB is a powerful framework for modeling online decision problems, it becomes intractable when the number of arms is very large or even infinite. To address this challenge, various algorithms have been designed to exploit different structure properties of the reward function, such as\nLipschitz (Kleinberg et al., 2008) and convex (Flaxman et al., 2005; Agarwal et al., 2013). Among them, stochastic linear bandits (SLB) has received considerable attentions during the past decade (Auer, 2002; Dani et al., 2008a; Abbasi-yadkori et al., 2011). In each round of SLB, the learner is asked to choose an action xt from a decision set D ∈ Rd, then he observes yt such that E[yt|xt] = x⊤t w∗, (1) where w∗ ∈ Rd is a vector of unknown parameters. The goal of learner is to minimize the (pseudo) regret\nT max x∈D\nx⊤w∗ − T∑\nt=1\nx⊤t w∗. (2)\nIn this paper, we consider a special bandit setting of online linear optimization where the feedback yt only contains one-bit of information. In particular, yt ∈ {±1}. Our setting is motivated from the fact that in many real-world applications, such as online advertising and recommender systems, user feedback (e.g., click or not) is usually binary. Since the feedback is binary-valued, we assume it is generated according to the logit model (Hastie et al., 2009), i.e.,\nPr[yt = ±1|xt] = 1\n1 + exp(−ytx⊤t w∗) =\nexp(ytx ⊤ t w∗)\n1 + exp(ytx⊤t w∗) . (3)\nWithout loss of generality, suppose 1 is the preferred outcome. Then, it is natural to define the regret in terms of the expected times that 1 is observed, i.e.,\nT max x∈D exp(x⊤w∗) 1 + exp(x⊤w∗) − T∑\nt=1\nexp(x⊤t w∗)\n1 + exp(x⊤t w∗) . (4)\nThe observation model in (3) and the nonlinear regret in (4) can be treated as a special case of the Generalized Linear Bandit (GLB) (Filippi et al., 2010). However, the existing algorithm for GLB is inefficient in the sense that: i) it is not a truly online algorithm since the whole learning history is stored in memory and used to estimate w∗; and ii) it is limited to the case that the number of arms is finite because an upper bound for each arm needs to be calculated explicitly in each round.\nThe main contribution of this paper is an efficient online learning algorithm that effectively exploits particular structures of the logit model. Based on the analytical properties of the logistic function, we first show that the linear regret defined in (2) and the nonlinear regret in (4) only differs by a constant factor, and then focus on minimizing the former one due to its simplicity. Similar to previous studies (Bubeck and Cesa-Bianchi, 2012), we follow the principle of “optimism in face of uncertainty” to deal with the exploration-exploitation dilemma. The basic idea is to maintain a confidence region for w∗, and choose an estimate from the confidence region and an action so that the linear reward is maximized. Thus, the problem reduces to the construction of the confidence region from one-bit feedback that satisfies (3). Based on the exponential concavity of the logistic loss, we propose to use a variant of the online Newton step (Hazan et al., 2007) to find the center of the confidence region and derive its width by a rather technical analysis of the updating rule. Theoretical analysis shows that our algorithm achieves a regret bound of Ø(d √ T ),1 which matches the\n1. We use the Ø notation to hide constant factors as well as polylogarithmic factors in d and T .\nresult for SLB (Dani et al., 2008a). Furthermore, we provide several strategies to reduce the computational cost of the proposed algorithm."
    }, {
      "heading" : "2. Related Work",
      "text" : "The stochastic multi-armed bandits (MAB) (Robbins, 1952), has become the canonical formalism for studying the problem of decision-making under uncertainty. A long line of successive problems have been extensively studied in statistics (Berry and Fristedt, 1985) and computer science (Bubeck and Cesa-Bianchi, 2012)."
    }, {
      "heading" : "2.1 Stochastic Multi-armed Bandits (MAB)",
      "text" : "In their seminal paper, Lai and Robbins (1985) establish an asymptotic lower bound of O(K log T ) for the expected cumulative regret over T periods, under the assumption that the expected rewards of the best and second best arms are well-separated. By making use of upper confidence bounds (UCB), they further construct policies which achieve the lower bound asymptotically. However, this initial algorithm is quite involved, because the computation of UCB relies on the entire sequence of rewards obtained so far. To address this limitation, Agrawal (1995) introduces a family of simpler policies that only needs to calculate the sample mean of rewards, and the regret retains the optimal logarithmic behavior. A finite time analysis of stochastic MAB is conducted by Auer et al. (2002). In particular, they propose a UCB-type algorithm based on the Chernoff-Hoeffding bound, and demonstrate it achieves the optimal logarithmic regret uniformly over time."
    }, {
      "heading" : "2.2 Stochastic Linear Bandits (SLB)",
      "text" : "SLB is first studied by Auer (2002), who considers the case D is finite. Although an elegant UCB-type algorithm named LinRel is developed, he fails to bound its regret due to independence issues. Instead, he designs a complicated master algorithm which uses LinRel as a subroutine, and achieves a regret bound of Ø((log |D|)3/2 √ Td), where |D| is the number of feasible decisions. In a subsequent work, Dani et al. (2008a) generalize LinRel slightly so that it can be applied in settings where D may be infinite. They refer to the new algorithm as ConfidenceBall2, and show it enjoys a bound of Ø(d √ T ), which does not depend on the cardinality of D. Later, Abbasi-yadkori et al. (2011) improve the theoretical analysis of ConfidenceBall2 by employing tools from the self-normalized processes. Specifically, the worst case bound is improved by a logarithmic factor and the constant is improved.\n2.3 ConfidenceBall2\nTo facilitate comparisons, we give a brief description of the ConfidenceBall2 algorithm (Dani et al., 2008a). In each round, the algorithm maintains a confidence region Ct such that with a high probability w∗ ∈ Ct. Then, the algorithm finds the greedy optimistic decision\nxt = argmax x∈D max w∈Ct\nx⊤w.\nAfter submitting xt to the oracle, the algorithm receives yt that satisfies (1). Given the past action-feedback pairs (x1, y1), . . . (xt, yt), the confidence region Ct+1 is constructed as\nfollows. The center of Ct+1 is found by minimizing the ℓ2-regularized square loss, i.e.,\nwt+1 = argmax w\nt∑\ni=1\n(x⊤i w − yi)2 + λ‖w‖22.\nNotice that wt+1 can be computed efficiently in an online fashion. Let At+1 = λI +∑t i=1 xix ⊤ i . Based on the self-normalized bound for vector-valued martingales (Abbasi-yadkori et al., 2011), the width of Ct+1 can be characterized by\n(w −wt+1)⊤At+1(w −wt+1) ≤ δt+1\nfor some constant δt+1 > 0. As can be seen, the above procedure for constructing the confidence region is specially designed for the observation model in (1), and thus cannot be applied to the model in (3)."
    }, {
      "heading" : "2.4 Generalized Linear Bandit (GLB)",
      "text" : "Filippi et al. (2010) extend SLB to the nonlinear case based on the Generalized Linear Model framework of statistics. In the so-called GLB model, yt is assumed to satisfy E[yt|xt] = µ(x⊤t w∗) where µ : R 7→ R is certain link function. The regret is also defined in terms of µ(·) and given by\nT max x∈D\nµ(x⊤w∗)− T∑\nt=1\nµ(x⊤t w∗). (5)\nNote that by setting µ(x) = exp(x)/[1 + exp(x)], the problem considered in this paper becomes a special case of GLB. A UCB-type algorithm has been proposed for GLB and also achieves a regret bound of Ø(d √ T ). Different from ConfidenceBall2 which constructs a confidence region in the parameter space, the algorithm of Filippi et al. (2010) operates only in the reward space. However, the space and time complexities of that algorithm in the t-th iteration are O(t) and O(t+ |D|), respectively. The O(t) factor comes from the fact it needs to store the past action-feedback pairs (x1, y1), . . . (xt−1, yt−1) and use all of them to estimate w∗. The O(|D|) factor is due to the fact it needs to calculate an upper bound for each arm in order to decide the next action xt."
    }, {
      "heading" : "2.5 Adversarial Setting",
      "text" : "All the results mentioned above are under the stochastic setting, where the reward of each arm is generated from a unknown but fixed distribution. A more general setting is the adversarial case, in which the reward from each arm may change arbitrary (Bubeck and Cesa-Bianchi, 2012). The most well-known method for the adversarial multi-armed bandits is the Exp3 algorithm, which achieves a regret bound of Ø( √ KT ) (Auer et al., 2003). The problem of adversarial linear bandits has been extensively studied, and the start-of-the-art regret bound is Ø(poly(d) √ T ) (Dani et al., 2008b; Abernethy et al., 2008; Bubeck et al., 2012). For more results, please refer to Bubeck and Cesa-Bianchi (2012), Shamir (2013) and references therein."
    }, {
      "heading" : "2.6 Bandit Learning with One-bit Feedback",
      "text" : "There are several new variants of bandit learning that also rely on one one-bit feedback, such as multi-class bandits (Kakade et al., 2008; Chen et al., 2014) and K-armed dueling bandits (Yue et al., 2009; Ailon et al., 2014). For example, in multi-class bandits, the feedback is whether the predicted label is correct or not, and in K-armed dueling bandits, the feedback is the comparison between the rewards from two arms. However, none of them are designed for online linear optimization."
    }, {
      "heading" : "2.7 One-bit Compressive Sensing (CS)",
      "text" : "Finally, we would like to discuss one closely related work in signal processing—one-bit Compressive Sensing (CS) (Boufounos and Baraniuk, 2008; Plan and Vershynin, 2013). One-bit CS aims to recover a sparse vectors w∗ from a set of one-bit measurements {yi} where yi is generated from x⊤i w∗ according to certain observation model such as (3). The main difference is that one-bit CS is studied in batch setting with the goal to minimize the recovery error, while our problem is studied in online setting with the goal to minimize the regret.\n3. Online Learning for Logit Model (OL2M)\nWe first describe the proposed algorithm for online stochastic linear optimization given onebit feedback, next compare it with existing methods, then state its theoretical guarantees, and finally discuss implementation issues."
    }, {
      "heading" : "3.1 The Algorithm",
      "text" : "For a positive definite matrix A ∈ Rd×d, the weighted ℓ2-norm is defined by ‖x‖2A = x⊤Ax. Without loss of generality, we assume the decision space D is contained in the unit ball, that is,\n‖x‖2 ≤ 1, ∀x ∈ D. (6)\nWe further assume the ℓ2-norm of w∗ is upper bounded by some constant R, which is known to the learner. Our first observation is that the linear regret in (2) and the nonlinear regret in (4) only differs by a constant factor as indicated below.\nLemma 1 We have 1\n2(1 + exp(R)) (2) ≤ (4) ≤ 1 4 (2) (7)\nIn the following, we will develop an efficient algorithm that minimizes the linear regret, which in turn minimizes the nonlinear regret as well.\nThe algorithm is motivated as follows. Suppose actions x1, . . . ,xt have been submitted to the oracle, and let y1, . . . , yt be the one-bit feedback from the oracle. To approximate w∗, the most straightforward way is to find the maximum likelihood estimator by solving the following logistic regression problem\nmin ‖w‖2≤R\n1\nt\nt∑\ni=1\nlog ( 1 + exp(−yix⊤i w) ) .\nAlgorithm 1 Online Learning for Logit Model (OL2M)\n1: Input: Step Size η, Regularization Parameter λ 2: Z1 = λI, w1 = 0 3: for t = 1, 2, . . . do 4:\n(xt, ŵt) = argmax x∈D,w∈Ct\nx⊤w\n5: Submit xt and observe yt ∈ {±1} 6: Solve the optimization problem in (8) to find wt+1 7: end for\nHowever, this approach does not scale well since it requires the leaner to store the entire learning history. Instead, we propose an online algorithm to find an approximate solution. The key observation is that the logistic loss\nft(w) = log ( 1 + exp(−ytx⊤t w) )\nis exponentially concave over bounded domain (Hazan et al., 2014), which motives us to apply a variant of the online Newton step (Hazan et al., 2007). Specifically, we propose to find an approximate solution wt+1 by solving the following problem\nmin ‖w‖2≤R ‖w −wt‖2Zt+1 2 + η(w −wt)⊤∇ft(wt) (8)\nwhere η > 0 is the step size,\nZt+1 = Zt + ηβ\n2 xtx\n⊤ t , (9)\nand β is defined in (14). Although our updating rule is similar to the method in (Hazan et al., 2007), there also exist some differences. As indicated by (9), in our case xtx ⊤ t is used to approximate the Hessian matrix, while in Hazan et al. (2007) ∇ft(wt)[∇ft(wt)⊤] is used. After a theoretical analysis, we are able to show that with a high probability\nw∗ ∈ Ct+1 = { w : ‖w −wt+1‖Zt+1 ≤ √ γt+1 } (10)\nwhere the value of γt+1 is given in (12). Given the confidence region, we adopt the principle of “optimism in face of uncertainty”, and the next action xt+1 is given by\n(xt+1, ŵt+1) = argmax x∈D,w∈Ct+1\nx⊤w. (11)\nAt the beginning, we set Z1 = λI, and w1 = 0.\nThe above procedure is summarized in Algorithm 1, and is refer to as Online Learning for Logit Model (OL2M).\nSince both ConfidenceBall2 (Dani et al., 2008a) and our OL 2M are UCB-type algorithms, their overall frameworks are similar. The main difference lies in the construction\nof the confidence region and the related analysis. While ConfidenceBall2 uses online least square to update the center of the confidence region, OL2M resorts to online Newton step. Due to the difference in the updating rule and the observation model, the self-normalized bound for vector-valued martingales (Abbasi-yadkori et al., 2011) can not be applied here.\nAlthough our observation model in (3) can be handled by the Generalized Linear Bandit (GLB) (Filippi et al., 2010), this paper differs from GLB in the following aspects.\n• To estimate w∗, GLB needs to store the learning history and perform batch updating in each round. In contrast, the proposed OL2M performs online updating. • While GLB only considers a finite number of arms, we allow the number of arms to be infinite. • Our algorithm follows the learning framework of SLB. Thus, existing techniques for speeding up SLB can also be used to accelerate our algorithm, which is discussed in Section 3.3."
    }, {
      "heading" : "3.2 Theoretical Guarantees",
      "text" : "The main theoretical contribution of this paper is the following theorem regarding the confidence region of w∗ at each round.\nTheorem 1 With a probability at least 1− δ, we have\n‖wt+1 −w∗‖Zt+1 ≤ √ γt+1, ∀t > 0\nwhere\nγt+1 = 2η\n[ 4R+ ( 4\nβ +\n8 3 R\n) τt + 1\nβ log\ndet(Zt+1)\ndet(Z1)\n] +max ( λ, ηβ\n2\n) R2, (12)\nτt = log\n( 2⌈2 log2 t⌉t2\nδ\n) , (13)\nβ = 1\n2(1 + exp(R)) . (14)\nThe main idea is to analyze the growth of ‖wt+1−w∗‖2Zt+1 by exploring the properties of the logistic loss (Lemmas 2 and 4) and concentration inequalities for martingales (Lemma 5). By a simple upper bound of log det(Zt+1)/det(Z1), we can show that the width of the confidence region is O( √ d log t).\nCorollary 2 We have\nlog det(Zt+1)\ndet(Z1) ≤ d log\n( 1 + ηβt\n2λd\n)\nand thus\nγt+1 ≤ O(d log t), ∀t > 0.\nBased on Theorem 1, we have the following regret bound for OL2M.\nTheorem 3 With a probability at least 1− δ, we have\nT max x∈D\nx⊤w∗ − T∑\nt=1\nx⊤t w∗ ≤ 4 √ γTT\nηβ log\ndet(ZT+1)\ndet(Z1)\nholds for all T > 0.\nCombining with the upper bound in Corollary 2, the above theorem implies our algorithm achieves a regret bound of Ø(d √ T ) which matches the bound for Stochastic Linear Bandits (Dani et al., 2008a)."
    }, {
      "heading" : "3.3 Implementation Issues",
      "text" : "The main computational cost of OL2M comes from (11) which is NP-hard in general (Dani et al., 2008a). In the following, we discuss several strategies for reducing the computational cost.\nOptimization Over Ball As mentioned by Dani et al. (2008a), in the special case that D is the unit ball, (11) could be solved in time O(poly(d)). Here, we provide an explanation using techniques from convex optimization. To this end, we rewrite the optimization problem in (11) as follows\nmax ‖x‖2≤1,‖w−wt+1‖Zt+1≤ √ γt+1 x⊤w = max ‖w−wt+1‖Zt+1≤ √ γt+1 ‖w‖2\nwhich is equivalent to min\n‖w−wt+1‖2Zt+1≤γt+1 −‖w‖22.\nThe above problem is an optimization problem with a quadratic objective and one quadratic inequality constraint, it is well-known that strong duality holds provided there exists a strictly feasible point (Boyd and Vandenberghe, 2004). Thus, we can solve its dual problem which is convex and given by\nmax γ s. t. λ ≥ 0[ −I + λZt+1 −λZt+1wt+1\n−λw⊤t+1Zt+1 λ(‖wt+1‖2Zt+1 − γt+1)− γ\n] 0\nAfter obtaining the dual solution, we can get the primal solution based on KKT conditions.\nEnlarging the Confidence region For a positive definite matrix A ∈ Rd×d, we define\n‖x‖1,A = ‖A1/2x‖1.\nWhen studying SLB, Dani et al. (2008a) propose to enlarge the confidence region from Ct+1 = { w : ‖w −wt+1‖Zt+1 ≤ √ γt+1 } to C̃t+1 = { w : ‖w −wt+1‖1,Zt+1 ≤ √ dγt+1 } such that the computational cost could be reduced. This idea can be direct incorporated to our OL2M. Let Et+1 be the set of extremal points of C̃t+1. With this modification, (11) becomes\n(xt+1, ŵt+1) = argmax x∈D,w∈C̃t+1 x⊤w = argmax x∈D,w∈Et+1 x⊤w\nAlgorithm 2 OL2M with Lazy Updating\n1: Input: Step Size η, Regularization Parameter λ, Constant c 2: Z1 = λI, w1 = 0, τ = 1 3: for t = 1, 2, . . . do 4: if det(Zt) > (1 + c) det(Vτ ) then 5:\n(xt, ŵt) = argmax x∈D,w∈Ct\nx⊤w\n6: τ = t 7: end if 8: xt = xτ 9: Submit xt and observe yt ∈ {±1}\n10: Solve the optimization problem in (8) to find wt+1 11: end for\nwhich means we just need to enumerate over the 2d vertices in Et+1. Following the arguments in Dani et al. (2008a), it is straightforward to show that the regret is only increased by a factor of √ d.\nLazy Updating Abbasi-yadkori et al. (2011) propose a lazy updating strategy which only needs to solve (11) O(log T ) times. The key idea is to recompute xt whenever det(Zt) increases by a constant factor (1+c). While the computation cost is saved dramatically, the regret is only increased by a constant factor √ 1 + c. We provide the lazy updating version of OL2M in Algorithm 2."
    }, {
      "heading" : "4. Analysis",
      "text" : "We here present the proofs of main theorems. The omitted proofs are provided in the appendix."
    }, {
      "heading" : "4.1 Proof of Theorem 1",
      "text" : "We begin with several lemmas that are central to our analysis. Although the application of online Newton step (Hazan et al., 2007) in Algorithm 1 is motivated from the fact that ft(w) is exponentially concave over bounded domain, our analysis is built upon a related but different property that the logistic loss log(1 + exp(x)) is strongly convex over bounded domain, from which we obtain the following lemma.\nLemma 2 Denote the ball of radius R by BR, i.e., BR = {w : ‖w‖2 ≤ R}. The following holds for β ≤ 12(1+exp(R)) :\nft(w2) ≥ ft(w1) + [∇ft(w1)]⊤(w2 −w1) + β\n2\n( (w2 −w1)⊤xt )2 , ∀w1,w2 ∈ BR.\nComparing Lemma 2 with Lemma 3 in (Hazan et al., 2007), we can see that the quadratic term in our inequality does not depends on yt. This independence allows us to simplify the subsequent analysis involving martingales.\nOur second lemma is devoted to analyzing the property of the updating rule in (8).\nLemma 3\n〈wt −w∗,∇ft(wt)〉 ≤ ‖wt −w∗‖2Zt+1\n2η − ‖wt+1 −w∗‖2Zt+1 2η + η 2 ‖∇ft(wt)‖2Z−1 t+1 . (15)\nFor each function ft(·), we denote its conditional expectation over yt by f̄t(w), i.e.,\nf̄t(w) = Eyt\n[ log ( 1 + exp ( −ytx⊤t w ))] . (16)\nBased the property of Kullback–Leibler divergence (Cover and Thomas, 2006), we obtain the following lemma.\nLemma 4 We have f̄t(w) ≥ f̄t(w∗), ∀w ∈ Rd.\nNext, we introduce one inequality for bounding the weighted ℓ2-norm of the gradient\n‖∇ft(w)‖2A = (\nexp(−ytx⊤t w) 1 + exp(−ytx⊤t w)\n)2 x⊤t Axt ≤ ‖xt‖2A, ∀A 0, w ∈ Rd. (17)\nWe continue the proof of Theorem 1 in the following. Our updating rule in (8) ensures ‖wt‖2 ≤ R, ∀t > 0. Combining with the assumption ‖w∗‖2 ≤ R, Lemma 2 implies\nft(wt) ≤ ft(w∗) + [∇ft(wt)]⊤(wt −w∗)− β\n2\n( (w∗ −wt)⊤xt )2 . (18)\nBy taking expectation over yt, (18) becomes\nf̄t(wt) ≤ f̄t(w∗) + [∇f̄t(wt)]⊤(wt −w∗)− β\n2\n[( (w∗ −wt)⊤xt )2] .\nCombining with Lemma 4, we have\n0 ≤[∇f̄t(wt)]⊤(wt −w∗)− β\n2\n( (w∗ −wt)⊤xt )2 ︸ ︷︷ ︸ :=at\n=[∇ft(wt)]⊤(wt −w∗)− β 2 at + [∇f̄t(wt)−∇ft(wt)]⊤(wt −w∗)︸ ︷︷ ︸\n:=bt\n=[∇ft(wt)]⊤(wt −w∗)− ‖wt −w∗‖2Zt+1\n2η + ‖wt −w∗‖2Zt+1 2η − β 2 at + bt\n(15) ≤ − ‖wt+1 −w∗‖2Zt+1\n2η +\nη 2 ‖∇ft(wt)‖2Z−1 t+1\n+ ‖wt −w∗‖2Zt+1\n2η − β 2 at + bt\n(17) ≤ − ‖wt+1 −w∗‖2Zt+1\n2η +\nη 2 ‖xt‖2Z−1\nt+1︸ ︷︷ ︸ :=ct\n+ ‖wt −w∗‖2Zt+1\n2η − β 2 at + bt\n(9) = − ‖wt+1 −w∗‖2Zt+1 2η − β 2 at + bt + η 2 ct + ‖wt −w∗‖2Zt 2η + β 4 ( x⊤t (wt −w∗) )2\n=− ‖wt+1 −w∗‖2Zt+1\n2η − β 4 at + bt + η 2 ct + ‖wt −w∗‖2Zt 2η .\nWe thus have\n‖wt+1 −w∗‖2Zt+1 ≤ ‖wt −w∗‖ 2 Zt −\nηβ\n2 at + 2ηbt + η\n2ct\nSumming the above inequality over iterations 1 to t, we obtain\n‖wt+1 −w∗‖2Zt+1 + ηβ\n2\nt∑\ni=1\nai ≤ λR2 + 2η t∑\ni=1\nbi + η 2\nt∑\ni=1\nci. (19)\nNext, we discuss how to bound the summation of martingale difference sequence ∑t\ni=1 bi. To this end, we prove the following lemma, which is built up the Bernstein’s inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique (Bartlett et al., 2005).\nLemma 5 With a probability at least 1− δ, we have\nt∑\ni=1\nbi ≤ 4R+ 2 √√√√τt t∑\ni=1\nai + 8\n3 Rτt, ∀t > 0\nwhere τt is defined in (13).\nFrom Lemma 5 and the basic inequality\n2 √√√√τt t∑\ni=1\nai ≤ β\n4\nt∑\ni=1\nai + 4\nβ τt,\nwith a probability at least 1− δ, we have\nt∑\ni=1\nbi ≤ 4R+ β\n4\nt∑\ni=1\nai +\n( 4\nβ +\n8 3 R\n) τt (20)\nholds for all t > 0. Substituting (20) into (19), we obtain\n‖wt+1 −w∗‖2Zt+1 ≤ λR 2 + 2η\n[ 4R + ( 4\nβ +\n8 3 R\n) τt ] + η2 t∑\ni=1\nci. (21)\nFinally, we show an upper bound for ∑t\ni=1 ci, which is a direct consequence of Lemma 12 in Hazan et al. (2007).\nLemma 6 We have t∑\ni=1\n‖xi‖2Z−1 i+1 ≤ 2 ηβ log det(Zt+1) det(Z1) .\nWe complete the proof by combining (21) with the above lemma."
    }, {
      "heading" : "4.2 Proof of Lemma 2",
      "text" : "We first show that the one-dimensional logistic loss ℓ(x) = log(1 + exp(−x)) is 12(1+exp(R)) - strongly convex over domain [−R,R]. It is easy to verify that ∀x ∈ [−R,R],\nℓ′′(x) = exp(x) (1 + exp(x))2 ≥ 1 2(1 + exp(R))\nimplying the strongly convexity of ℓ(·). From the property of strongly convex, for any a, b ∈ [−R,R] we have\nℓ(b) ≥ ℓ(a) + ℓ′(a)(b− a) + β 2 (b− a)2. (22)\nNotice that for any w1,w2 ∈ BR, we have\nytx ⊤ t w1, ytx ⊤ t w2 ∈ [−R,R],\nsince yt ∈ {±1} and ‖xt‖2 ≤ 1. Substituting a = ytx⊤t w1 and b = ytx⊤t w2 into (22), we have\nℓ(ytx ⊤ t w2) ≥ ℓ(ytx⊤t w1) +\nβ 2 (ytx ⊤ t w2 − ytx⊤t w1)2 + ℓ′(ytx⊤t w1)(ytx⊤t w2 − ytx⊤t w1).\nWe complete the proof by noticing\nft(w1) = ℓ(ytx ⊤ t w1), ft(w2) = ℓ(ytx ⊤ t w2), and ∇ft(w1) = ℓ′(ytx⊤t w1)ytxt."
    }, {
      "heading" : "4.3 Proof of Lemma 3",
      "text" : "Lemma 3 follows from a more general result stated below.\nLemma 7 Let M be a positive definite matrix, and\ny = argmin w∈W η〈w,g〉 + 1 2 ‖w − x‖2M ,\nwhere W is a convex set. Then for all w ∈ W, we have\n〈x−w,g〉 ≤ ‖x−w‖ 2 M − ‖y −w‖2M 2η + η 2 ‖g‖2M−1 .\nProof Since y is the optimal solution to the optimization problem, from the first-order optimality condition (Boyd and Vandenberghe, 2004), we have\n〈ηg +M(y − x),w − y〉 ≥ 0, ∀w ∈ W. (23)\nBased on the above inequality, we have\n‖x−w‖2M − ‖y −w‖2M =x⊤Mx− y⊤My + 2〈M(y − x),w〉\n(23)\n≥ x⊤Mx− y⊤My + 2〈M(y − x),y〉 − 2〈ηg,w − y〉 =‖y − x‖2M + 2〈ηg,y − x+ x−w〉 =2〈ηg,x −w〉+ ‖y − x‖2M + 2〈ηg,y − x〉\nCombining with the following inequality\n‖y − x‖2M + 2〈ηg,y − x〉 ≥ min w ‖w‖2M + 2〈ηg,w〉 = −η2‖g‖2M−1 ,\nwe have ‖x−w‖2M − ‖y −w‖2M ≥ 2〈ηg,x −w〉 − η2‖g‖2M−1 ."
    }, {
      "heading" : "4.4 Proof of Lemma 4",
      "text" : "For each w ∈ Rd, we introduce a discrete probability distribution pw over {±1} such that\npw(i) = 1\n1 + exp(−ix⊤t w) , i ∈ {±1}.\nThen, it is easy to verify that\nf̄t(w) = − ∑\ni∈{±1} pw∗(i) log pw(i).\nAs a result\nf̄t(w)− f̄t(w∗) = ∑\ni∈{±1} pw∗(i) log pw∗(i)−\n∑\ni∈{±1} pw∗(i) log pw(i)\n= ∑\ni∈{±1} pw∗(i) log\npw∗(i)\npw(i) = DKL(pw∗‖pw) ≥ 0\nwhereDKL(·‖·) is the Kullback–Leibler divergence between two distributions (Cover and Thomas, 2006)."
    }, {
      "heading" : "4.5 Proof of Lemma 5",
      "text" : "We need the Bernstein’s inequality for martingales (Cesa-Bianchi and Lugosi, 2006), which is provided in Appendix D. Form our definition of f̄i(·) in (16), it is clear bi = [∇f̄i(wi)−∇fi(wi)]⊤(wi −w∗) is a martingale difference sequence. Furthermore, |bi| ≤ ∣∣∣[∇f̄i(wi)]⊤(wi −w∗) ∣∣∣+ ∣∣∣[∇fi(wi)]⊤(wi −w∗) ∣∣∣ ≤ 2|x⊤i (wi−w∗)| ≤ 2‖wi−w∗‖2 ≤ 4R. Define the martingale Bt = ∑t i=1 bi. Define the conditional variance Σ 2 t as\nΣ2t =\nt∑\ni=1\nEyi [( [∇f̄i(wi)−∇fi(wi)]⊤(wi −w∗) )2]\n≤ t∑\ni=1\nEyi [( ∇fi(wi)⊤(wi −w∗) )2] ≤ t∑\ni=1\n( x⊤i (wi −w∗) )2 ︸ ︷︷ ︸ :=At ,\nwhere the first inequality is due to the fact that E[(ξ − E[ξ])2] ≤ E[ξ2] for any random variable ξ.\nIn the following, we consider two different scenarios, i.e., At ≤ 4R 2 t and At > 4R2 t .\nAt ≤ 4R 2\nt In this case, we have\nBt ≤ t∑\ni=1\n|bi| ≤ 2 t∑\ni=1\n|x⊤i (wi −w∗)| ≤ 2\n√√√√t t∑\ni=1\n( x⊤i (wi −w∗) )2 ≤ 4R. (24)\nAt > 4R2 t Since At in the upper bound for Σ 2 t is a random variable, we cannot apply Bernstein’s inequality directly. To address this issue, we make use of the peeling process (Bartlett et al., 2005). Note that we have both a lower bound and an upper bound for At, i.e., 4R2/t < At ≤ 4R2t. Then,\nPr [ Bt ≥ 2 √ Atτt + 8\n3 Rτt\n]\n=Pr [ Bt ≥ 2 √ Atτt + 8\n3 Rτt,\n4R2\nt < At ≤ 4R2t\n]\n=Pr [ Bt ≥ 2 √ Atτt + 8\n3 Rτt,Σ\n2 t ≤ At,\n4R2\nt < At ≤ 4R2t\n]\n≤ m∑\ni=1\nPr [ Bt ≥ 2 √ Atτt + 8\n3 Rτt,Σ\n2 t ≤ At,\n4R22i−1\nt < At ≤\n4R22i\nt\n]\n≤ m∑\ni=1\nPr [ Bt ≥ √ 2 4R22i\nt τt +\n8 3 Rτt,Σ 2 t ≤\n4R22i\nt\n] ≤ me−τt ,\nwhere m = ⌈2 log2 t⌉, and the last step follows the Bernstein’s inequality for martingales. By setting τt = log 2mt2 δ , with a probability at least 1− δ/[2t2], we have\nBt ≤ 2 √ Atτt + 8\n3 Rτt. (25)\nCombining (24) and (25), with a probability at least 1− δ/[2t2], we have\nBt ≤ 4R+ 2 √ Atτt + 8\n3 Rτt.\nWe complete the proof by taking the union bound over t > 0, and using the well-known result ∞∑\nt=1\n1 t2 =\nπ2\n6 ≤ 2."
    }, {
      "heading" : "4.6 Proof of Theorem 3",
      "text" : "The proof is standard and can be found from Dani et al. (2008a) and Abbasi-yadkori et al. (2011). We include it for the sake of completeness.\nLet x∗ = argmaxx∈D x ⊤w∗. Recall that in each round, we have\n(xt, ŵt) = argmax x∈D,w∈Ct\nx⊤w.\nWe decompose the instantaneous regret at round t as follows\nx⊤∗ w∗ − x⊤t w∗ ≤x⊤t ŵt − x⊤t w∗ = x⊤t (ŵt −wt) + x⊤t (wt −w∗) ≤ (‖ŵt −wt‖Zt + ‖wt −w∗‖Zt) ‖xt‖Z−1\nt ≤ 2√γt‖xt‖Z−1 t .\nOn the other hand, we always have\nx⊤∗ w∗ − x⊤t w∗ ≤ ‖x∗ − xt‖2‖w∗‖2 ≤ 2R.\nFrom the definition in (12), we have √\n2 ηβ γT ≥ R. Thus, the total regret can be upper\nbounded by\nT max x∈D\nx⊤w∗ − T∑\nt=1\nx⊤t w∗\n≤2 T∑\nt=1\nmin (√\nγt‖xt‖Z−1 t\n, R )\n≤2 √ 2\nηβ γT\nT∑\nt=1\nmin\n(√ ηβ\n2 ‖xt‖Z−1 t , 1\n)\n≤2 √ 2T\nηβ γT\n√√√√ T∑\nt=1\nmin\n( ηβ\n2 ‖xt‖2Z−1 t , 1\n) .\nTo proceed, we need the following results from Lemma 11 in Abbasi-yadkori et al. (2011),\nT∑\nt=1\nmin\n( ηβ\n2 ‖xt‖2Z−1 t , 1\n) ≤ 2 T∑\nt=1\nlog ( 1 + ηβ\n2 ‖xt‖2Z−1 t\n)\nand\ndet(ZT+1) = det ( ZT + ηβ\n2 xTx\n⊤ T\n)\n=det(ZT ) det\n( I + ηβ\n2 Z\n−1/2 T xTx ⊤ T Z −1/2 T\n)\n=det(ZT )\n( 1 + ηβ\n2 ‖xT ‖2Z−1 T\n) = det(Z1) T∏\nt=1\n( 1 + ηβ\n2 ‖xt‖2Z−1 t\n) .\nCombining the above inequations, we have\nT max x∈D\nx⊤w∗ − T∑\nt=1\nx⊤t w∗ ≤ 4 √ γTT\nηβ log\ndet(ZT+1)\ndet(Z1) ."
    }, {
      "heading" : "5. Conclusions",
      "text" : "In this paper, we consider the problem of online linear optimization under one-bit feedback. Under the assumption that the binary feedback is generated from the logit model, we develop a variant of the online Newton step to approximate the unknown vector, and discuss how to construct the confidence region theoretically. Given the confidence region, we choose the action that produces maximal reward in each round. Theoretical analysis reveals that our algorithm achieves a regret bound of Ø(d √ T ).\nThe current algorithm assumes that the one-bit feedback is generated from a logit model. In contrast, a much broader class of observation models are allowed in one-bit compressive sensing (Plan and Vershynin, 2013), as long as there is a positive correlation between the one-bit output and the real-valued measurement. In the future, we will investigate how to extend our algorithm to other observation models. Another direction is to consider the adversary setting where the unknown vector w∗ may change from time to time."
    }, {
      "heading" : "Appendix A. Proof of Lemma 1",
      "text" : "Let µ(x) = exp(x)1+exp(x) . It is easy to verify that ∀x ∈ [−R,R], 1\n2(1 + exp(R)) ≤ µ′(x) = exp(x) (1 + exp(x))2 ≤ 1 4 (26)\nNote that for any −R ≤ a ≤ b ≤ R, we have\nµ(b) = µ(a) +\n∫ b\na µ′(x)dx (27)\nCombining (26) with (27), we have\n1 2(1 + exp(R)) (b− a) ≤ µ(b)− µ(a) ≤ 1 4 (b− a)\nLet\nx∗ = argmax x∈D x⊤w∗ = argmax x∈D exp(x⊤w∗) 1 + exp(x⊤w∗)\nSince −R ≤ x⊤t w∗ ≤ x⊤∗ w∗ ≤ R, we have 1\n2(1 + exp(R))\n( x⊤∗ w∗ − x⊤t w∗ ) ≤ exp(x ⊤ ∗ w∗)\n1 + exp(x⊤∗ w∗) − exp(x\n⊤ t w∗)\n1 + exp(x⊤t w∗) ≤ 1 4\n( x⊤∗ w∗ − x⊤t w∗ )\nwhich implies (7)."
    }, {
      "heading" : "Appendix B. Proof of Lemma 6",
      "text" : "We have\n‖xi‖2Z−1 i+1\n= 2 ηβ 〈Z−1i+1, Zi+1 − Zi〉 ≤ 2 ηβ log det(Zi+1) det(Zi) ,\nwhere the inequality follows from Lemma 12 in Hazan et al. (2007). Thus, we have\nt∑\ni=1\n‖xi‖2Z−1 i+1 ≤ 2 ηβ\nt∑\ni=1\nlog det(Zi+1)\ndet(Zi) =\n2\nηβ log\ndet(Zt+1)\ndet(Z1) ."
    }, {
      "heading" : "Appendix C. Proof of Corollary 2",
      "text" : "Recall that\nZt+1 = Z1 + ηβ\n2\nt∑\ni=1\nxtx ⊤ t\nand ‖xt‖2 ≤ 1 for all t > 0. From Lemma 10 of Abbasi-yadkori et al. (2011), we have\ndet(Zt+1) ≤ ( λ+ ηβt\n2d\n)d .\nSince det(Z1) = λ d, we have\nlog det(Zt+1)\ndet(Z1) ≤ d log\n( 1 + ηβt\n2λd\n) ."
    }, {
      "heading" : "Appendix D. Bernstein’s Inequality for Martingales",
      "text" : "Theorem 4 Let X1, . . . ,Xn be a bounded martingale difference sequence with respect to the filtration F = (Fi)1≤i≤n and with |Xi| ≤ K. Let\nSi =\ni∑\nj=1\nXj\nbe the associated martingale. Denote the sum of the conditional variances by\nΣ2n = n∑\nt=1\nE [ X2t |Ft−1 ] .\nThen for all constants t, ν > 0,\nPr [ max\ni=1,...,n Si > t and Σ\n2 n ≤ ν\n] ≤ exp ( − t 2\n2(ν +Kt/3)\n) ,\nand therefore,\nPr [ max\ni=1,...,n Si >\n√ 2νt+ 2\n3 Kt and Σ2n ≤ ν\n] ≤ e−t."
    } ],
    "references" : [ {
      "title" : "Improved algorithms for linear stochastic bandits",
      "author" : [ "Yasin Abbasi-yadkori", "Dávid Pál", "Csaba Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Abbasi.yadkori et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Abbasi.yadkori et al\\.",
      "year" : 2011
    }, {
      "title" : "Competing in the dark: An efficient algorithm for bandit linear optimization",
      "author" : [ "Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin" ],
      "venue" : "In Proceedings of the 21st Annual Conference on Learning,",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2008
    }, {
      "title" : "Stochastic convex optimization with bandit feedback",
      "author" : [ "Alekh Agarwal", "Dean P. Foster", "Daniel Hsu", "Sham M. Kakade", "Alexander Rakhlin" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2013
    }, {
      "title" : "Sample mean based index policies with O(log n) regret for the multi-armed bandit problem",
      "author" : [ "Rajeev Agrawal" ],
      "venue" : "Advances in Applied Probability,",
      "citeRegEx" : "Agrawal.,? \\Q1995\\E",
      "shortCiteRegEx" : "Agrawal.",
      "year" : 1995
    }, {
      "title" : "Reducing dueling bandits to cardinal bandits",
      "author" : [ "Nir Ailon", "Zohar Karnin", "Thorsten Joachims" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "Ailon et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ailon et al\\.",
      "year" : 2014
    }, {
      "title" : "Using confidence bounds for exploitation-exploration trade-offs",
      "author" : [ "Peter Auer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Auer.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer.",
      "year" : 2002
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2003
    }, {
      "title" : "Local rademacher complexities",
      "author" : [ "Peter L. Bartlett", "Olivier Bousquet", "Shahar Mendelson" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2005
    }, {
      "title" : "Bandit problems: Sequential Allocation of Experiments",
      "author" : [ "Donald A. Berry", "Bert Fristedt" ],
      "venue" : "Monographs on Statistics and Applied Probability. Springer Netherlands,",
      "citeRegEx" : "Berry and Fristedt.,? \\Q1985\\E",
      "shortCiteRegEx" : "Berry and Fristedt.",
      "year" : 1985
    }, {
      "title" : "1-bit compressive sensing",
      "author" : [ "Petros T. Boufounos", "Richard G. Baraniuk" ],
      "venue" : "In Proceedings of the 42nd Annual Conference on Information Sciences and Systems,",
      "citeRegEx" : "Boufounos and Baraniuk.,? \\Q2008\\E",
      "shortCiteRegEx" : "Boufounos and Baraniuk.",
      "year" : 2008
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen Boyd", "Lieven Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2004
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Towards minimax policies for online linear optimization with bandit feedback",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi", "Sham M. Kakade" ],
      "venue" : "In Proceedings of the 25th Annual Conference on Learning Theory,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2012
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolo Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "Boosting with online binary learners for the multiclass bandit problem",
      "author" : [ "Shang-Tse Chen", "Hsuan-Tien Lin", "Chi-Jen Lu" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade" ],
      "venue" : "In Proceedings of the 21st Annual Conference on Learning,",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "The price of bandit information for online optimization",
      "author" : [ "Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "Parametric bandits: The generalized linear case",
      "author" : [ "Sarah Filippi", "Olivier Cappe", "Aurélien Garivier", "Csaba Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Filippi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Filippi et al\\.",
      "year" : 2010
    }, {
      "title" : "Online convex optimization in the bandit setting: Gradient descent without a gradient",
      "author" : [ "Abraham D. Flaxman", "Adam Tauman Kalai", "H. Brendan McMahan" ],
      "venue" : "In Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Flaxman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Flaxman et al\\.",
      "year" : 2005
    }, {
      "title" : "The Elements of Statistical Learning",
      "author" : [ "Trevor Hastie", "Robert Tibshirani", "Jerome Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2009
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "Elad Hazan", "Amit Agarwal", "Satyen Kale" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2007
    }, {
      "title" : "Logistic regression: Tight bounds for stochastic and online optimization",
      "author" : [ "Elad Hazan", "Tomer Koren", "Kfir Y. Levy" ],
      "venue" : "In Proceedings of The 27th Conference on Learning Theory,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient bandit algorithms for online multiclass prediction",
      "author" : [ "Sham M. Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari" ],
      "venue" : "In Proceedings of the 25th International Conference on Machine Learning,",
      "citeRegEx" : "Kakade et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2008
    }, {
      "title" : "Multi-armed bandits in metric spaces",
      "author" : [ "Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal" ],
      "venue" : "In Proceedings of the 40th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2008
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "Herbert Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Lai and Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach",
      "author" : [ "Yaniv Plan", "Roman Vershynin" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Plan and Vershynin.,? \\Q2013\\E",
      "shortCiteRegEx" : "Plan and Vershynin.",
      "year" : 2013
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "Herbert Robbins" ],
      "venue" : "Bulletin of the American Mathematical Society,",
      "citeRegEx" : "Robbins.,? \\Q1952\\E",
      "shortCiteRegEx" : "Robbins.",
      "year" : 1952
    }, {
      "title" : "On the complexity of bandit and derivative-free stochastic convex optimization",
      "author" : [ "Ohad Shamir" ],
      "venue" : "In Proceedings of the 26th Conference on Learning Theory,",
      "citeRegEx" : "Shamir.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shamir.",
      "year" : 2013
    }, {
      "title" : "The K-armed dueling bandits problem",
      "author" : [ "Yisong Yue", "Josef Broder", "Robert Kleinberg", "Thorsten Joachims" ],
      "venue" : "In Proceedings of the 22nd Annual Conference on Learning Theory,",
      "citeRegEx" : "Yue et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Introduction Online learning with bandit feedback plays an important role in several industrial domains, such as ad placement, website optimization, and packet routing (Bubeck and Cesa-Bianchi, 2012).",
      "startOffset" : 168,
      "endOffset" : 199
    }, {
      "referenceID" : 27,
      "context" : "A canonical framework for studying this problem is the multi-armed bandits (MAB), which models the situation that a gambler must choose which of K slot machines to play (Robbins, 1952).",
      "startOffset" : 169,
      "endOffset" : 184
    }, {
      "referenceID" : 6,
      "context" : "The goal of the gambler is to minimize the regret, namely the difference between his expected cumulative reward and that of the best single arm in hindsight (Auer et al., 2002).",
      "startOffset" : 157,
      "endOffset" : 176
    }, {
      "referenceID" : 24,
      "context" : "Lipschitz (Kleinberg et al., 2008) and convex (Flaxman et al.",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : ", 2008) and convex (Flaxman et al., 2005; Agarwal et al., 2013).",
      "startOffset" : 19,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : ", 2008) and convex (Flaxman et al., 2005; Agarwal et al., 2013).",
      "startOffset" : 19,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "Among them, stochastic linear bandits (SLB) has received considerable attentions during the past decade (Auer, 2002; Dani et al., 2008a; Abbasi-yadkori et al., 2011).",
      "startOffset" : 104,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "Among them, stochastic linear bandits (SLB) has received considerable attentions during the past decade (Auer, 2002; Dani et al., 2008a; Abbasi-yadkori et al., 2011).",
      "startOffset" : 104,
      "endOffset" : 165
    }, {
      "referenceID" : 20,
      "context" : "Since the feedback is binary-valued, we assume it is generated according to the logit model (Hastie et al., 2009), i.",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 18,
      "context" : "The observation model in (3) and the nonlinear regret in (4) can be treated as a special case of the Generalized Linear Bandit (GLB) (Filippi et al., 2010).",
      "startOffset" : 133,
      "endOffset" : 155
    }, {
      "referenceID" : 12,
      "context" : "Similar to previous studies (Bubeck and Cesa-Bianchi, 2012), we follow the principle of “optimism in face of uncertainty” to deal with the exploration-exploitation dilemma.",
      "startOffset" : 28,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "Based on the exponential concavity of the logistic loss, we propose to use a variant of the online Newton step (Hazan et al., 2007) to find the center of the confidence region and derive its width by a rather technical analysis of the updating rule.",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 27,
      "context" : "Related Work The stochastic multi-armed bandits (MAB) (Robbins, 1952), has become the canonical formalism for studying the problem of decision-making under uncertainty.",
      "startOffset" : 54,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "A long line of successive problems have been extensively studied in statistics (Berry and Fristedt, 1985) and computer science (Bubeck and Cesa-Bianchi, 2012).",
      "startOffset" : 79,
      "endOffset" : 105
    }, {
      "referenceID" : 12,
      "context" : "A long line of successive problems have been extensively studied in statistics (Berry and Fristedt, 1985) and computer science (Bubeck and Cesa-Bianchi, 2012).",
      "startOffset" : 127,
      "endOffset" : 158
    }, {
      "referenceID" : 21,
      "context" : "1 Stochastic Multi-armed Bandits (MAB) In their seminal paper, Lai and Robbins (1985) establish an asymptotic lower bound of O(K log T ) for the expected cumulative regret over T periods, under the assumption that the expected rewards of the best and second best arms are well-separated.",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "To address this limitation, Agrawal (1995) introduces a family of simpler policies that only needs to calculate the sample mean of rewards, and the regret retains the optimal logarithmic behavior.",
      "startOffset" : 28,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "To address this limitation, Agrawal (1995) introduces a family of simpler policies that only needs to calculate the sample mean of rewards, and the regret retains the optimal logarithmic behavior. A finite time analysis of stochastic MAB is conducted by Auer et al. (2002). In particular, they propose a UCB-type algorithm based on the Chernoff-Hoeffding bound, and demonstrate it achieves the optimal logarithmic regret uniformly over time.",
      "startOffset" : 28,
      "endOffset" : 273
    }, {
      "referenceID" : 4,
      "context" : "2 Stochastic Linear Bandits (SLB) SLB is first studied by Auer (2002), who considers the case D is finite.",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "2 Stochastic Linear Bandits (SLB) SLB is first studied by Auer (2002), who considers the case D is finite. Although an elegant UCB-type algorithm named LinRel is developed, he fails to bound its regret due to independence issues. Instead, he designs a complicated master algorithm which uses LinRel as a subroutine, and achieves a regret bound of Ø((log |D|)3/2 √ Td), where |D| is the number of feasible decisions. In a subsequent work, Dani et al. (2008a) generalize LinRel slightly so that it can be applied in settings where D may be infinite.",
      "startOffset" : 58,
      "endOffset" : 458
    }, {
      "referenceID" : 0,
      "context" : "Later, Abbasi-yadkori et al. (2011) improve the theoretical analysis of ConfidenceBall2 by employing tools from the self-normalized processes.",
      "startOffset" : 7,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Based on the self-normalized bound for vector-valued martingales (Abbasi-yadkori et al., 2011), the width of Ct+1 can be characterized by",
      "startOffset" : 65,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "4 Generalized Linear Bandit (GLB) Filippi et al. (2010) extend SLB to the nonlinear case based on the Generalized Linear Model framework of statistics.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 18,
      "context" : "Different from ConfidenceBall2 which constructs a confidence region in the parameter space, the algorithm of Filippi et al. (2010) operates only in the reward space.",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "A more general setting is the adversarial case, in which the reward from each arm may change arbitrary (Bubeck and Cesa-Bianchi, 2012).",
      "startOffset" : 103,
      "endOffset" : 134
    }, {
      "referenceID" : 7,
      "context" : "The most well-known method for the adversarial multi-armed bandits is the Exp3 algorithm, which achieves a regret bound of Ø( √ KT ) (Auer et al., 2003).",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "The problem of adversarial linear bandits has been extensively studied, and the start-of-the-art regret bound is Ø(poly(d) √ T ) (Dani et al., 2008b; Abernethy et al., 2008; Bubeck et al., 2012).",
      "startOffset" : 129,
      "endOffset" : 194
    }, {
      "referenceID" : 13,
      "context" : "The problem of adversarial linear bandits has been extensively studied, and the start-of-the-art regret bound is Ø(poly(d) √ T ) (Dani et al., 2008b; Abernethy et al., 2008; Bubeck et al., 2012).",
      "startOffset" : 129,
      "endOffset" : 194
    }, {
      "referenceID" : 1,
      "context" : ", 2008b; Abernethy et al., 2008; Bubeck et al., 2012). For more results, please refer to Bubeck and Cesa-Bianchi (2012), Shamir (2013) and references therein.",
      "startOffset" : 9,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : ", 2008b; Abernethy et al., 2008; Bubeck et al., 2012). For more results, please refer to Bubeck and Cesa-Bianchi (2012), Shamir (2013) and references therein.",
      "startOffset" : 9,
      "endOffset" : 135
    }, {
      "referenceID" : 23,
      "context" : "6 Bandit Learning with One-bit Feedback There are several new variants of bandit learning that also rely on one one-bit feedback, such as multi-class bandits (Kakade et al., 2008; Chen et al., 2014) and K-armed dueling bandits (Yue et al.",
      "startOffset" : 158,
      "endOffset" : 198
    }, {
      "referenceID" : 15,
      "context" : "6 Bandit Learning with One-bit Feedback There are several new variants of bandit learning that also rely on one one-bit feedback, such as multi-class bandits (Kakade et al., 2008; Chen et al., 2014) and K-armed dueling bandits (Yue et al.",
      "startOffset" : 158,
      "endOffset" : 198
    }, {
      "referenceID" : 29,
      "context" : ", 2014) and K-armed dueling bandits (Yue et al., 2009; Ailon et al., 2014).",
      "startOffset" : 36,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : ", 2014) and K-armed dueling bandits (Yue et al., 2009; Ailon et al., 2014).",
      "startOffset" : 36,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "7 One-bit Compressive Sensing (CS) Finally, we would like to discuss one closely related work in signal processing—one-bit Compressive Sensing (CS) (Boufounos and Baraniuk, 2008; Plan and Vershynin, 2013).",
      "startOffset" : 148,
      "endOffset" : 204
    }, {
      "referenceID" : 26,
      "context" : "7 One-bit Compressive Sensing (CS) Finally, we would like to discuss one closely related work in signal processing—one-bit Compressive Sensing (CS) (Boufounos and Baraniuk, 2008; Plan and Vershynin, 2013).",
      "startOffset" : 148,
      "endOffset" : 204
    }, {
      "referenceID" : 22,
      "context" : "is exponentially concave over bounded domain (Hazan et al., 2014), which motives us to apply a variant of the online Newton step (Hazan et al.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : ", 2014), which motives us to apply a variant of the online Newton step (Hazan et al., 2007).",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "Although our updating rule is similar to the method in (Hazan et al., 2007), there also exist some differences.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "Although our updating rule is similar to the method in (Hazan et al., 2007), there also exist some differences. As indicated by (9), in our case xtx ⊤ t is used to approximate the Hessian matrix, while in Hazan et al. (2007) ∇ft(wt)[∇ft(wt)⊤] is used.",
      "startOffset" : 56,
      "endOffset" : 225
    }, {
      "referenceID" : 0,
      "context" : "Due to the difference in the updating rule and the observation model, the self-normalized bound for vector-valued martingales (Abbasi-yadkori et al., 2011) can not be applied here.",
      "startOffset" : 126,
      "endOffset" : 155
    }, {
      "referenceID" : 18,
      "context" : "Although our observation model in (3) can be handled by the Generalized Linear Bandit (GLB) (Filippi et al., 2010), this paper differs from GLB in the following aspects.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 16,
      "context" : "3 Implementation Issues The main computational cost of OL2M comes from (11) which is NP-hard in general (Dani et al., 2008a). In the following, we discuss several strategies for reducing the computational cost. Optimization Over Ball As mentioned by Dani et al. (2008a), in the special case that D is the unit ball, (11) could be solved in time O(poly(d)).",
      "startOffset" : 105,
      "endOffset" : 270
    }, {
      "referenceID" : 11,
      "context" : "The above problem is an optimization problem with a quadratic objective and one quadratic inequality constraint, it is well-known that strong duality holds provided there exists a strictly feasible point (Boyd and Vandenberghe, 2004).",
      "startOffset" : 204,
      "endOffset" : 233
    }, {
      "referenceID" : 16,
      "context" : "When studying SLB, Dani et al. (2008a) propose to enlarge the confidence region from Ct+1 = { w : ‖w −wt+1‖Zt+1 ≤ √ γt+1 } to C̃t+1 = { w : ‖w −wt+1‖1,Zt+1 ≤ √ dγt+1 } such that the computational cost could be reduced.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 15,
      "context" : "Following the arguments in Dani et al. (2008a), it is straightforward to show that the regret is only increased by a factor of √ d.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Lazy Updating Abbasi-yadkori et al. (2011) propose a lazy updating strategy which only needs to solve (11) O(log T ) times.",
      "startOffset" : 14,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : "Although the application of online Newton step (Hazan et al., 2007) in Algorithm 1 is motivated from the fact that ft(w) is exponentially concave over bounded domain, our analysis is built upon a related but different property that the logistic loss log(1 + exp(x)) is strongly convex over bounded domain, from which we obtain the following lemma.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "Comparing Lemma 2 with Lemma 3 in (Hazan et al., 2007), we can see that the quadratic term in our inequality does not depends on yt.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "To this end, we prove the following lemma, which is built up the Bernstein’s inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique (Bartlett et al.",
      "startOffset" : 104,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "To this end, we prove the following lemma, which is built up the Bernstein’s inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique (Bartlett et al., 2005).",
      "startOffset" : 162,
      "endOffset" : 185
    }, {
      "referenceID" : 21,
      "context" : "Finally, we show an upper bound for ∑t i=1 ci, which is a direct consequence of Lemma 12 in Hazan et al. (2007). Lemma 6 We have t ∑",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 11,
      "context" : "Proof Since y is the optimal solution to the optimization problem, from the first-order optimality condition (Boyd and Vandenberghe, 2004), we have 〈ηg +M(y − x),w − y〉 ≥ 0, ∀w ∈ W.",
      "startOffset" : 109,
      "endOffset" : 138
    }, {
      "referenceID" : 14,
      "context" : "5 Proof of Lemma 5 We need the Bernstein’s inequality for martingales (Cesa-Bianchi and Lugosi, 2006), which is provided in Appendix D.",
      "startOffset" : 70,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "To address this issue, we make use of the peeling process (Bartlett et al., 2005).",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "6 Proof of Theorem 3 The proof is standard and can be found from Dani et al. (2008a) and Abbasi-yadkori et al.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "(2008a) and Abbasi-yadkori et al. (2011). We include it for the sake of completeness.",
      "startOffset" : 12,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "To proceed, we need the following results from Lemma 11 in Abbasi-yadkori et al. (2011),",
      "startOffset" : 59,
      "endOffset" : 88
    }, {
      "referenceID" : 26,
      "context" : "In contrast, a much broader class of observation models are allowed in one-bit compressive sensing (Plan and Vershynin, 2013), as long as there is a positive correlation between the one-bit output and the real-valued measurement.",
      "startOffset" : 99,
      "endOffset" : 125
    }, {
      "referenceID" : 21,
      "context" : "Proof of Lemma 6 We have ‖xi‖2Z−1 i+1 = 2 ηβ 〈Z−1 i+1, Zi+1 − Zi〉 ≤ 2 ηβ log det(Zi+1) det(Zi) , where the inequality follows from Lemma 12 in Hazan et al. (2007). Thus, we have t ∑",
      "startOffset" : 143,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "From Lemma 10 of Abbasi-yadkori et al. (2011), we have det(Zt+1) ≤ ( λ+ ηβt 2d )d .",
      "startOffset" : 17,
      "endOffset" : 46
    } ],
    "year" : 2015,
    "abstractText" : "In this paper, we study a special bandit setting of online stochastic linear optimization, where only one-bit of information is revealed to the learner at each round. This problem has found many applications including online advertisement and online recommendation. We assume the binary feedback is a random variable generated from the logit model, and aim to minimize the regret defined by the unknown linear function. Although the existing method for generalized linear bandit can be applied to our problem, the high computational cost makes it impractical for real-world problems. To address this challenge, we develop an efficient online learning algorithm by exploiting particular structures of the observation model. Specifically, we adopt online Newton step to estimate the unknown parameter and derive a tight confidence region based on the exponential concavity of the logistic loss. Our analysis shows that the proposed algorithm achieves a regret bound of Ø(d √ T ), which matches the optimal result of stochastic linear bandits.",
    "creator" : "LaTeX with hyperref package"
  }
}