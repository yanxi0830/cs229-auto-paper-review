{
  "name" : "1608.06007.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Theodoros Tsiligkaridis" ],
    "emails" : [ "atsili@bu.edu", "ttsili@ll.mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 8.\n06 00\n7v 1\n[ cs\n.S I]\n2 1\nA ug\n2 01\nIndex Terms— Probabilistic bisection, consensus, decentralized estimation, convergence rate, belief sharing"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "We consider the problem of distributed bisection search using a network of agents. This problem has applications to stochastic root finding algorithms, distributed group testing, object tracking using cameras, and localization in sensor networks. The agents are connected through a topology, and sequentially search for the unknown target X∗. Each agent forms a query Zi,t = I(X∗ ∈ Si,t) for some subset Si,t ⊂ X based on its local information about the target location and obtains a noisy response, Yi,t+1, which it uses to update its belief. After this stage, the agents average their log-beliefs with their neighbors. As this process is repeated after several iterations, the agents converge to the correct consensus.\nPrior work on distributed signal processing includes the consensus literature [1, 2]. Extensions to consensus-plusinnovations algorithms with applications to detection and estimation problems includes nodes making noisy observations and implementing a consensus method to spread information around the network. Several such works on distributed estimation include that of [3, 4] in which they show distributed estimators are consistent and asymptotically normal. In similar spirit, [5, 6] studied the problem of distributed detection and proved convergence rates on the rate of learning the correct hypothesis.\nThe adaptive querying and Bayesian updating are known in the literature as the Probabilistic Bisection Algorithm (PBA). The PBA was first introduced by Horstein [7]. The PBA was shown to be optimal in an information-theoretic sense in the work by Jedynak, et al. [8], and the convergence rate of the single-agent PBA was shown to be exponential in Waeber, et al, [9]. The PBA was generalized in [10] to multiple agents using a centralized controller strategy, and in [11, 12] using a decentralized algorithm based on belief consensus. In [10, 11, 12], the convergence analysis showed that all agents reach consensus to the true target.\nThe contribution of this paper is to propose and analyze a distributed bisection algorithm using a social learning approach. First, we first derive a social learning algorithm (inspired by the work of Lalitha, et al, [5]). Second, we derive an asymptotic performance bound that characterizes the rate of learning in terms of the social influence of nodes on the network and the error probability of each agent. Finally, we show using simulations, that our proposed algorithm outperforms the case of no collaboration and improves the distributed bisection search algorithm presented in Tsiligkaridis, et al, [11]."
    }, {
      "heading" : "2. PROBLEM FORMULATION",
      "text" : "For concreteness, we focus on the one-dimensional case, i.e, X = [0, 1]. In this case, the query sets Si,t take the form of an interval [0, X̂i,t], where X̂i,t are the query points. The response to the query Zi,t = I(X∗ ∈ Si,t) is modeled as a binary symmetric channel [8, 10, 11] and is given by:\nYi,t+1 =\n{\nZi,t w.p. 1− ǫi 1− Zi,t w.p. ǫi\nWe define pi,t(x) as the posterior distribution on the target space X = [0, 1] for agent i at time t. We also denote the corresponding CDF as Fi,t(x) = ∫ x\n0 pi,t(u)du. The poste-\nrior distributions at time t are measurable with respect to the noisy responses {Yi,τ+1}τ≤t up to time t. The proposed social learning algorithm consists of two stages (see Algorithm 1). In the first stage, agents perform a probabilistic bisection, and using the noisy response, and they update their beliefs\nusing Bayes rule. In the second stage, the log-beliefs are averaged among local neighbors to spread information around the network.\nThe observation probability mass function used in the first stage is given by:\nli(x, yi,t+1) = f (i) 1 (y)I(x ≤ X̂i,t) + f (i) 0 (y)I(x > X̂i,t)\n(1) where f (i)1 (y) = (1−ǫi) I(y=1)ǫ I(y=0) i , f (i) 0 (y) = 1−f (i) 1 (y). 1\nIn the second stage, we assume that the social interaction matrix A is a stochastic matrix corresponding to a strongly connected, aperiodic graph G = (N , E). Here, N represents the nodes of the network and E is the set of edges. An edge (i, j) ∈ E exists iff Ai,j > 0.\nAlgorithm 1 Distributed Probabilistic Bisection Algorithm 1: Input: G = (N , E),A = {Ai,j}, {ǫi} 2: Output: {X̂i,t : i ∈ N} 3: Initialize pi,0(·) to be positive everywhere. 4: repeat 5: Stage 1: For each agent i ∈ N :\n6: Bisect posterior density to obtain query point X̂i,t = F−1i,t ( 1 2 ), and obtain noisy response yi,t+1 ∈ {0, 1}. 7: Belief update:\np̃i,t(x) = pi,t(x) · 2li(x, yi,t+1) (2)\n8: Stage 2: For each agent i ∈ N : 9: Average log-beliefs:\npi,t+1(x) =\n∏N j=1 p̃j,t(x) Ai,j\n∫ 1\n0 ∏N j=1 p̃j,t(x)\nAi,jdx (3)\n10: until convergence"
    }, {
      "heading" : "3. PERFORMANCE ANALYSIS",
      "text" : "In this section, we define a lower bound on the posterior distribution pi,t(x) at the target X∗. Our result shows that for large t, we have pi,t(X∗) ≥ 2tK for all agents i ∈ N .\nTheorem 1. Assume that A corresponds to an irreducible, aperiodic Markov chain, and pi,0(X∗) > 0 for all i. Then, the following asymptotic result holds:\nlim inf t→∞\nlog2 pi,t(X ∗)\nt ≥\nN∑\ni=1\nviC(ǫi) = K (4)\nwhere v is the normalized left eigenvector of A which corresponds to the unit eigenvalue 2 and C(ǫi) is the capacity of\n1We remark that the normalizing factor here ∫ 1\n0 pi,t(x)li(x, yi,t+1)dx\nis equal to 1/2 [11]. 2This is also known as the stationary distribution of the Markov chain.\nthe binary symmetric channel [13].\nProof. Consider the case for a fixed node i. From Algorithm 1, we may rewrite (3) as:\nlog2 pi,t+1(x) = ∑\nj\nAi,j log2 p̃j,t(x)\n− log2\n(∫ 1\n0\n2 ∑ N j=1 Ai,j log2 p̃j,t(x)dx\n)\n︸ ︷︷ ︸\nDt\nUsing Jensen’s inequality, we have:\nDt ≤ log2\n\n\n∫ 1\n0\nN∑\nj=1\nAi,j p̃j,t(x)dx\n\n = log2\nN∑\nj=1\nAi,j = 0\nSince Dt ≤ 0, it follows that:\nlog2 pi,t+1(x) ≥ N∑\nj=1\nAi,j log2 p̃j,t(x) (5)\nEvaluating the above inequality at x = X∗ and using Equation (2) along with lj(X∗, yj,t+1) = P (yj,t+1|zj,t):\nlog2 pi,t+1(X ∗) ≥\nN∑\nj=1\nAi,j log2 pj,t(X ∗) +\nN∑\nj=1\nAi,j log2(2P (yj,t+1|zj,t))\n(6)\nBy using new variables: qi,t def = log2(pi,t(X ∗)) and\nwj,t def = log2(2P (yi,t+1|zi,t)), Equation (6) becomes:\nqi,t+1 ≥ N∑\nj=1\nAi,jqj,t +\nN∑\nj=1\nAi,jwj,t (7)\nUsing induction, it follows that:\nqi,t ≥ t∑\nτ=1\nN∑\nj=1\nAτi,jwj,t−τ +\nN∑\nj=1\nAti,jqj,0 (8)\nDividing both sides by t and taking the limit, it follows that:\nlim inf t→∞\nqi,t\nt ≥ lim t→∞\n1\nt\nt∑\nτ=1\nN∑\nj=1\nAτi,jwj,t−τ + lim t→∞\n1\nt\nN∑\nj=1\nAti,jqj,0\n(9)\nUsing 0 < pj,0(X∗) < ∞ and Ati,j ≤ 1, the second term in Equation (9) vanishes and we have:\nlim inf t→∞\nqi,t\nt ≥ lim t→∞\n1\nt\nt∑\nτ=1\nN∑\nj=1\nAτi,jwj,t−τ =: Ki (10)\nNext, we evaluate the rate exponent Ki using the convergence to a stationary distribution v for aperiodic Markov chains. Specifically, using Theorem 7 in Sec. 2.7 from [14]:\nlim t→∞\nAti,j = vj\nfor any two nodes (i, j). Thus, for any arbitrarily small δ > 0, there exists T = T (δ) such that |Ati,j − vj | ≤ δ for all t ≥ T .\nSplitting the sum in (10):\nKi = lim t→∞\n[\n1\nt\nN∑\nj=1\nT−1∑\nτ=1\nAτi,jwj,t−τ + 1\nt\nN∑\nj=1\nt∑\nτ=T\nAτi,jwj,t−τ\n]\n(11)\nThe first term in Equation (11) is negligible since it can be upper bounded by limt→∞ 1t ∑N j=1 ∑T−1 τ=1 |wj,t−τ |. Since |wj,t−τ | ≤ max{| log2(2(1 − ǫj))|, | log2(2ǫj)|} = Bj , limt→∞ 1 t ∑N j=1 ∑T−1 τ=1 |wj,t−τ | ≤ limt→∞ 1 t (T −1)B = 0, where ∑N\nj=1 Bj = B. The second term in Equation (11) dominates and is given by:\nKi = lim t→∞\n1\nt\nN∑\nj=1\nt∑\nτ=T\n[Aτi,j − vj ]wj,t−τ + lim t→∞\n1\nt\nN∑\nj=1\nt∑\nτ=T\nvjwj,t−τ\n(12)\nThe first term in Equation (12) is negligible since:\nlim t→∞\n1\nt\nN∑\nj=1\nt∑\nτ=T\n|Aτi,j − vj ||wj,t−τ |\n≤ lim t→∞\nδ\nt\nN∑\nj=1\nt∑\nτ=T\n|wj,t−τ | ≤ lim t→∞\nδ\nt\nt∑\nτ=T\nN∑\nj=1\nBj = δB\n(13)\nThe second term in Equation (12) dominates and using the LLN:\nKi =\nN∑\nj=1\nvj\n[\nlim t→∞\n1\nt\nt∑\nτ=T\nwj,t−τ\n]\n= N∑\nj=1\nvjE[wj,t−τ ] = N∑\nj=1\nvjC(ǫj) =: K (14)\nThe proof is complete.\nIn Theorem 1, we derived a lower bound on the rate of learning of each agent for the distributed bisection algorithm (Algorithm 1). Since the rate exponent, Ki, is independent of the agent index, i, the lower bound is the same for all agents and it depends the channel capacities and the eigenvector centrality, v. The values of v represent a centrality measure and the magnitude of social influence of an agent i [5]. The higher\nthe value of vi, the higher larger the contribution that node i has on the network learning rate, K .\nUsing an analogous method, we can analyze Equation (2) for the case of no inter-agent collaboration; the result is an exact convergence rate. For this case, A = IN , and using Equations (2) and (3), pi,t+1(X∗) = p̃i,t(X∗) = pi,t(X\n∗) + 2P (yi,t+1|zi,t). Unrolling the preceding equation over t steps, we see that pi,t(x) = ∏t−1 j=0 2P (yi,j+1|zi,j). By taking the logarithm of both sides, dividing by t, and applying the LLN, the convergence rate of the agent beliefs becomes: limt→∞ log 2 pi,t(x) t\n= C(ǫi). For the case of a homogeneous network, i.e. ǫi = ǫ, ∀i, our distributed algorithm asymptotically learns faster than when agents do not collaborate. In general, the rate exponent of Theorem 1 is a linear combination of agents’ capacities weighed by the eigenvector centrality. This bound is expected to be maximized by placing nodes with low error probability in central locations in networks where they can have a strong social influence on other nodes."
    }, {
      "heading" : "4. SIMULATIONS",
      "text" : "To validate and strengthen the preceding performance analysis, simulations are performed to show that the proposed method achieves better performance than the belief consensus approach of [11] and the case of no inter-agent collaboration.\nWe randomly generate an irreducible N × N adjacency matrix, modeling a random geometric graph [15] (see Figure 1). In this setup, N = 20 and 18 agents have high error probabilities (ǫi = 0.40)while the remaining 2 have low error probabilities (ǫi = 0.05). The low error agents are the ones with the highest two vi’s, so they can positively affect the high error nodes around them. Figure 1 displays the 2 low error nodes and their connections in blue along with the other nodes and their connections in black.\nFigure 2 shows the average network Mean Squared Error (MSE), MSEavg(t) = 1N ∑N i=1(X̂i,t−X\n∗)2, and the worstcase network MSE, MSEmax(t) = maxi(X̂i,t − X∗)2, averaged over 150 Monte Carlo trials. We observe that our proposed method outperforms the belief consensus approach of [11] and the case of no collaboration. Regarding the average MSE, all three methods converge to the consensus but our proposed method does so much quicker than the other methods. In terms of worst case MSE, our proposed method remains robust while that of [11] converges slower and the method with no collaboration, has not converged after 75 time steps. From this, we see that the proposed method performs very well even in the worst case and that inter-agent collaboration is advantageous in our target localization task.\nIn addition to the MSE, we also plot log2 p4,t(X ∗) for both the proposed method and that of no collaboration. The log2 p4,t(X\n∗) values represent a base-2 logarithm of agent 4’s belief, an agent with a high error probability. We analytically\nshowed that for the case of no collaboration, a graph of the log2 pi,t(x\n∗) vs. t would be a line with slope K = C(ǫi) that would serve as an exact bound; with our proposed method, the same graph should be a line with the slope at least as large as the lower bound K =\n∑N j=1 vjC(ǫi) (14). The anal-\nysis results are affirmed by the simulation results in Figure 3. Here, the dotted lines represent the two preceding bounds and the solid lines represent the calculations of log2 p4,t(X\n∗) for each iteration. The solid red line represents the no collaboration method and it lines up with the exact bound displayed in the dotted red line. The solid blue line represents our proposed method and its slope exceeds that displayed by the lower bound with the dotted blue line. The simulations verify our analysis and show that the beliefs generated by our proposed method concentrate fast on the true target location.\nWith our distributed algorithm, it is important to note that we improve the network-wide performance (average and worst case) with a small penalty in performance in the case of no collaboration for the lowest error agent."
    }, {
      "heading" : "5. CONCLUSION",
      "text" : "In this paper, we proposed a new distributed probabilistic bisection algorithm for target localization and derived a lower bound on the rate of convergence. Through analysis and simulation, we show that our proposed method attains superior performance to other state of the art methods in terms of rate of convergence and MSE. For future work, a refined and rigorous analysis of the rate of convergence for the periodic case can be pursued."
    }, {
      "heading" : "6. REFERENCES",
      "text" : "[1] S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah, “Randomized Gossip Algorithms,” IEEE Transactions on Information Theory, vol. 52, no. 6, pp. 2508–2530, June 2006.\n[2] A. Dimakis, S. Kar, J. M. F. Moura, M. G. Rabbat, and A. Scaglione, “Gossip Algorithms for Distributed Signal Processing,” Proceedings of the IEEE, vol. 98, no. 11, pp. 1847–1864, November 2010.\n[3] S. Kar and J. M. F. Moura, “Convergence Rate Analysis of Distributed Gossip (Linear Parameter) Estimation: Fundamental Limits and Tradeoffs,” IEEE Journal of Selected Topics in Signal Processing, vol. 5, no. 4, pp. 674–690, August 2011.\n[4] S. Kar, J. M. F. Moura, and K. Ramanan, “Distributed Parameter Estimation in Sensor Networks: Nonlinear Observation Models and Imperfect Communication,” IEEE Transactions on Information Theory, vol. 58, no. 6, pp. 3575–3605, June 2012.\n[5] A. Lalitha, T. Javidi, and A. Sarwate, “Social Learning and Distributed Hypothesis Testing,” ArXiv e-prints, Oct. 2014.\n[6] A. Nedic, A. Olshevsky, and C. A. Uribe, “Nonasymptotic Convergence Rates for Cooperative Learning over Time-Varying Directed Graphs,” in Proceedings of the American Control Conference (ACC), Chicago, IL, July 2015.\n[7] M. Horstein, “Sequential Transmission Using Noiseless Feedback,” IEEE Transactions on Information Theory, vol. 9, no. 3, pp. 136–143, July 1963.\n[8] B. Jedynak, P. I. Frazier, and R. Sznitman, “Twenty Questions with noise: Bayes optimal policies for entropy loss,” Journal of Applied Probability, vol. 49, pp. 114–136, 2012.\n[9] R. Waeber, P. I. Frazier, and S. G. Henderson, “Bisection Search with Noisy Responses,” Journal of Control Optimization, vol. 53, no. 3, pp. 2261–2279, 2013.\n[10] T. Tsiligkaridis, B. M. Sadler, and A. O. Hero III, “Collaborative 20 Questions for Target Localization,” IEEE Transactions on Information Theory, vol. 60, no. 4, pp. 2233–2352, April 2014.\n[11] T. Tsiligkaridis, B. M. Sadler, and A. O. Hero III, “On Decentralized Estimation with Active Queries,” IEEE Transactions on Signal Processing, vol. 63, no. 10, pp. 2610–2622, May 2015.\n[12] T. Tsiligkaridis, “Asynchronous Decentralized Algorithms for the Noisy 20 Questions Problem,” in Proceedings of the IEEE International Symposium on Information Theory (ISIT), Barcelona, Spain, July 2016.\n[13] T. D. Cover and J. A. Thomas, Elements of Information Theory, Wiley, New York, NY, USA, 2006.\n[14] P. G. Hoel, S. C. Port, and C. J. Stone, Introduction to Stochastic Processes, Houghton Mifflin Company, Boston, MA, USA, 1972.\n[15] P. Gupta and P. R. Kumar, “The Capacity of Wireless Networks,” IEEE Transactions on Information Theory, vol. 46, no. 2, pp. 388–404, March 2000."
    } ],
    "references" : [ {
      "title" : "Randomized Gossip Algorithms",
      "author" : [ "S. Boyd", "A. Ghosh", "B. Prabhakar", "D. Shah" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 52, no. 6, pp. 2508–2530, June 2006.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Gossip Algorithms for Distributed Signal Processing",
      "author" : [ "A. Dimakis", "S. Kar", "J.M.F. Moura", "M.G. Rabbat", "A. Scaglione" ],
      "venue" : "Proceedings of the IEEE, vol. 98, no. 11, pp. 1847–1864, November 2010.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1847
    }, {
      "title" : "Convergence Rate Analysis of Distributed Gossip (Linear Parameter) Estimation: Fundamental Limits and Tradeoffs",
      "author" : [ "S. Kar", "J.M.F. Moura" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing, vol. 5, no. 4, pp. 674–690, August 2011.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Distributed Parameter Estimation in Sensor Networks: Nonlinear Observation Models and Imperfect Communication",
      "author" : [ "S. Kar", "J.M.F. Moura", "K. Ramanan" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 58, no. 6, pp. 3575–3605, June 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Social Learning and Distributed Hypothesis Testing",
      "author" : [ "A. Lalitha", "T. Javidi", "A. Sarwate" ],
      "venue" : "ArXiv e-prints, Oct. 2014.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Nonasymptotic Convergence Rates for Cooperative Learning over Time-Varying Directed Graphs",
      "author" : [ "A. Nedic", "A. Olshevsky", "C.A. Uribe" ],
      "venue" : "Proceedings of the American Control Conference (ACC), Chicago, IL, July 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sequential Transmission Using Noiseless Feedback",
      "author" : [ "M. Horstein" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 9, no. 3, pp. 136–143, July 1963.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1963
    }, {
      "title" : "Twenty Questions with noise: Bayes optimal policies for entropy loss",
      "author" : [ "B. Jedynak", "P.I. Frazier", "R. Sznitman" ],
      "venue" : "Journal of Applied Probability, vol. 49, pp. 114–136, 2012.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Bisection Search with Noisy Responses",
      "author" : [ "R. Waeber", "P.I. Frazier", "S.G. Henderson" ],
      "venue" : "Journal of Control Optimization, vol. 53, no. 3, pp. 2261–2279, 2013.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Collaborative 20 Questions for Target Localization",
      "author" : [ "T. Tsiligkaridis", "B.M. Sadler", "A.O. Hero III" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 60, no. 4, pp. 2233–2352, April 2014.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On Decentralized Estimation with Active Queries",
      "author" : [ "T. Tsiligkaridis", "B.M. Sadler", "A.O. Hero III" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 63, no. 10, pp. 2610–2622, May 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Asynchronous Decentralized Algorithms for the Noisy 20 Questions Problem",
      "author" : [ "T. Tsiligkaridis" ],
      "venue" : "Proceedings of the IEEE International Symposium on Information Theory (ISIT), Barcelona, Spain, July 2016.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Introduction to Stochastic Processes",
      "author" : [ "P.G. Hoel", "S.C. Port", "C.J. Stone" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1972
    }, {
      "title" : "The Capacity of Wireless Networks",
      "author" : [ "P. Gupta", "P.R. Kumar" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 46, no. 2, pp. 388–404, March 2000.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Prior work on distributed signal processing includes the consensus literature [1, 2].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "Prior work on distributed signal processing includes the consensus literature [1, 2].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "Several such works on distributed estimation include that of [3, 4] in which they show distributed estimators are consistent and asymptotically normal.",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "Several such works on distributed estimation include that of [3, 4] in which they show distributed estimators are consistent and asymptotically normal.",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "In similar spirit, [5, 6] studied the problem of distributed detection and proved convergence rates on the rate of learning the correct hypothesis.",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "In similar spirit, [5, 6] studied the problem of distributed detection and proved convergence rates on the rate of learning the correct hypothesis.",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "The PBA was first introduced by Horstein [7].",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "[8], and the convergence rate of the single-agent PBA was shown to be exponential in Waeber, et al, [9].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[8], and the convergence rate of the single-agent PBA was shown to be exponential in Waeber, et al, [9].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "The PBA was generalized in [10] to multiple agents using a centralized controller strategy, and in [11, 12] using a decentralized algorithm based on belief consensus.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "The PBA was generalized in [10] to multiple agents using a centralized controller strategy, and in [11, 12] using a decentralized algorithm based on belief consensus.",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "The PBA was generalized in [10] to multiple agents using a centralized controller strategy, and in [11, 12] using a decentralized algorithm based on belief consensus.",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "In [10, 11, 12], the convergence analysis showed that all agents reach consensus to the true target.",
      "startOffset" : 3,
      "endOffset" : 15
    }, {
      "referenceID" : 10,
      "context" : "In [10, 11, 12], the convergence analysis showed that all agents reach consensus to the true target.",
      "startOffset" : 3,
      "endOffset" : 15
    }, {
      "referenceID" : 11,
      "context" : "In [10, 11, 12], the convergence analysis showed that all agents reach consensus to the true target.",
      "startOffset" : 3,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "First, we first derive a social learning algorithm (inspired by the work of Lalitha, et al, [5]).",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "Finally, we show using simulations, that our proposed algorithm outperforms the case of no collaboration and improves the distributed bisection search algorithm presented in Tsiligkaridis, et al, [11].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 0,
      "context" : "e, X = [0, 1].",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 7,
      "context" : "The response to the query Zi,t = I(X ∈ Si,t) is modeled as a binary symmetric channel [8, 10, 11] and is given by:",
      "startOffset" : 86,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "The response to the query Zi,t = I(X ∈ Si,t) is modeled as a binary symmetric channel [8, 10, 11] and is given by:",
      "startOffset" : 86,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : "The response to the query Zi,t = I(X ∈ Si,t) is modeled as a binary symmetric channel [8, 10, 11] and is given by:",
      "startOffset" : 86,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "ǫi We define pi,t(x) as the posterior distribution on the target space X = [0, 1] for agent i at time t.",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "where v is the normalized left eigenvector of A which corresponds to the unit eigenvalue 2 and C(ǫi) is the capacity of 1We remark that the normalizing factor here ∫ 1 0 pi,t(x)li(x, yi,t+1)dx is equal to 1/2 [11].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 12,
      "context" : "7 from [14]:",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 4,
      "context" : "The values of v represent a centrality measure and the magnitude of social influence of an agent i [5].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "To validate and strengthen the preceding performance analysis, simulations are performed to show that the proposed method achieves better performance than the belief consensus approach of [11] and the case of no inter-agent collaboration.",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 13,
      "context" : "We randomly generate an irreducible N × N adjacency matrix, modeling a random geometric graph [15] (see Figure 1).",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "We observe that our proposed method outperforms the belief consensus approach of [11] and the case of no collaboration.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "In terms of worst case MSE, our proposed method remains robust while that of [11] converges slower and the method with no collaboration, has not converged after 75 time steps.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "Average (top) and worst-case (bottom) MSE performance for our proposed social learning algorithm, the belief consensus approach of [11], and the case of no collaboration.",
      "startOffset" : 131,
      "endOffset" : 135
    } ],
    "year" : 2017,
    "abstractText" : "We present a novel distributed probabilistic bisection algorithm using social learning with application to target localization. Each agent in the network first constructs a query about the target based on its local information and obtains a noisy response. Agents then perform a Bayesian update of their beliefs followed by a local averaging of the log beliefs. This two stage algorithm consisting of repeated querying and averaging runs until convergence. We derive bounds on the rate of convergence of the beliefs at the correct target location. Numerical simulations show that our method outperforms current state of the art methods.",
    "creator" : "LaTeX with hyperref package"
  }
}