{
  "name" : "1210.5196.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Matrix reconstruction with the local max norm",
    "authors" : [ "Rina Foygel", "Nathan Srebro" ],
    "emails" : [ "rinafb@stanford.edu", "nati@ttic.edu", "rsalakhu@utstat.toronto.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In the matrix reconstruction problem, we are given a matrix Y ∈ Rn×m whose entries are only partly observed, and would like to reconstruct the unobserved entries as accurately as possible. Matrix reconstruction arises in many modern applications, including the areas of collaborative filtering (e.g. the Netflix prize), image and video data, and others. This problem has often been approached using regularization with matrix norms that promote low-rank or approximately-low-rank solutions, including the trace norm (also known as the nuclear norm) and the max norm, as well as several adaptations of the trace norm described below.\nIn this paper, we introduce a unifying family of norms that generalizes these existing matrix norms, and that can be used to interpolate between the trace and max norms. We show that this family includes new norms, lying strictly between the trace and max norms, that give empirical and theoretical improvements over the existing norms. We give results allowing for large-scale optimization with norms from the new family. Some proofs are deferred to the Supplementary Materials.\nNotation Without loss of generality we take n ≥ m. We let R+ denote the nonnegative real numbers. For any n ∈ N, let [n] = {1, . . . , n}, and define the simplex on [n] as ∆[n] ={ r ∈ Rn+ : ∑ i ri = 1 } . We analyze situations where the locations of observed entries are sampled\ni.i.d. according to some distribution p on [n]× [m]. We write pi• = ∑ j pij to denote the marginal probability of row i, and prow = (p1•, . . . ,pn•) ∈ ∆[n] to denote the marginal row distribution. We define p•j and pcol similarly for the columns."
    }, {
      "heading" : "1.1 Trace norm and max norm",
      "text" : "A common regularizer used in matrix reconstruction, and other matrix problems, is the trace norm ‖X‖tr, equal to the sum of the singular values ofX . This norm can also be defined via a factorization\nar X\niv :1\n21 0.\n51 96\nv1 [\nst at\n.M L\n] 1\n8 O\nof X [1]:\n1√ nm ‖X‖tr = 1 2 min AB>=X  1 n ∑ i ∥∥A(i)∥∥2 + 1 m ∑ j ∥∥B(j)∥∥2  , (1)\nwhere M(i) denotes the ith row of a matrix M , and where the minimum is taken over factorizations ofX of arbitrary dimension—that is, the number of columns inA andB is unbounded. Note that we choose to scale the trace norm by 1/ √ nm in order to emphasize that we are averaging the squared row norms of A and B.\nRegularization with the trace norm gives good theoretical and empirical results, as long as the locations of observed entries are sampled uniformly (i.e. when p is the uniform distribution on [n]×[m]), and, under this assumption, can also be used to guarantee approximate recovery of an underlying low-rank matrix [1, 2, 3, 4].\nThe factorized definition of the trace norm (1) allows for an intuitive comparison with the max norm, defined as [1]:\n‖X‖max = 1\n2 min AB>=X ( sup i ∥∥A(i)∥∥22 + sup j ∥∥B(j)∥∥22) . (2) We see that the max norm measures the largest row norms in the factorization, while the rescaled trace norm instead considers the average row norms. The max norm is therefore an upper bound on the rescaled trace norm, and can be viewed as a more conservative regularizer. For the more general setting where p may not be uniform, Foygel and Srebro [4] show that the max norm is still an effective regularizer (in particular, bounds on error for the max norm are not affected by p). On the other hand, Salakhutdinov and Srebro [5] show that the trace norm is not robust to non-uniform sampling—regularizing with the trace norm may yield large error due to over-fitting on the rows and columns with high marginals. They obtain improved empirical results by placing more penalization on these over-represented rows and columns, described next."
    }, {
      "heading" : "1.2 The weighted trace norm",
      "text" : "To reduce overfitting on the rows and columns with high marginal probabilities under the distribution p, Salakhutdinov and Srebro propose regularizing with the p-weighted trace norm,\n‖X‖tr(p) := ∥∥∥diag(prow)1/2 ·X · diag(pcol)1/2∥∥∥\ntr .\nIf the row and the column of entries to be observed are sampled independently (i.e. p = prow · pcol is a product distribution), then the p-weighted trace norm can be used to obtain good learning guarantees even when prow and pcol are non-uniform [3, 6]. However, for non-uniform non-product sampling distributions, even the p-weighted trace norm can yield poor generalization performance. To correct for this, Foygel et al. [6] suggest adding in some “smoothing” to avoid under-penalizing the rows and columns with low marginal probabilities, and obtain improved empirical and theoretical results for matrix reconstruction using the smoothed weighted trace norm:\n‖X‖tr(p̃) := ∥∥∥diag(p̃row)1/2 ·X · diag(p̃col)1/2∥∥∥\ntr ,\nwhere p̃row and p̃col denote smoothed row and column marginals, given by\np̃row = (1− ζ) · prow + ζ · 1/n and p̃col = (1− ζ) · pcol + ζ · 1/m , (3) for some choice of smoothing parameter ζ which may be selected with cross-validation1. The smoothed empirically-weighted trace norm is also studied in [6], where pi• is replaced with p̂i• = # observations in row i total # observations , the empirical marginal probability of row i (and same for p̂•j). Using empirical rather than “true” weights yielded lower error in experiments in [6], even when the true sampling distribution was uniform.\nMore generally, for any weight vectors r ∈ ∆[n] and c ∈ ∆[m] and a matrix X ∈ Rn×m, the (r, c)-weighted trace norm is given by\n‖X‖tr(r,c) = ∥∥∥diag(r)1/2 ·X · diag(c)1/2∥∥∥\ntr .\n1Our ζ parameter here is equivalent to 1− α in [6].\nOf course, we can easily obtain the existing methods of the uniform trace norm, (empirically) weighted trace norm, and smoothed (empirically) weighted trace norm as special cases of this formulation. Furthermore, the max norm is equal to a supremum over all possible weightings [7]:\n‖X‖max = sup r∈∆[n],c∈∆[m] ‖X‖tr(r,c) ."
    }, {
      "heading" : "2 The local max norm",
      "text" : "We consider a generalization of these norms, which lies “in between” the trace norm and max norm. For anyR ⊆ ∆[n] and C ⊆ ∆[m], we define the (R, C)-norm of X:\n‖X‖(R,C) = sup r∈R,c∈C ‖X‖tr(r,c) .\nThis gives a norm on matrices, except in the trivial case where, for some i or some j, ri = 0 for all r ∈ R or cj = 0 for all c ∈ C. We now show some existing and novel norms that can be obtained using local max norms."
    }, {
      "heading" : "2.1 Trace norm and max norm",
      "text" : "We can obtain the max norm by taking the largest possibleR and C, i.e. ‖X‖max = ‖X‖(∆[n],∆[m]), and similarly we can obtain the (r, c)-weighted trace norm by taking the singleton sets R = {r} and C = {c}. As discussed above, this includes the standard trace norm (when r and c are uniform), as well as the weighted, empirically weighted, and smoothed weighted trace norm."
    }, {
      "heading" : "2.2 Arbitrary smoothing",
      "text" : "When using the smoothed weighted max norm, we need to choose the amount of smoothing to apply to the marginals, that is, we need to choose ζ in our definition of the smoothed row and column weights, as given in (3). Alternately, we could regularize simultaneously over all possible amounts of smoothing by considering the local max norm with\nR = {(1− ζ) · prow + ζ · 1/n : any ζ ∈ [0, 1]} , and same for C. That is, R and C are line segments in the simplex—they are larger than any single point as for the uniform or weighted trace norm (or smoothed weighted trace norm for a fixed amount of smoothing), but smaller than the entire simplex as for the max norm."
    }, {
      "heading" : "2.3 Connection to (β, τ)-decomposability",
      "text" : "Hazan et al. [8] introduce a class of matrices defined by a property of (β, τ)-decomposability: a matrix X satisfies this property if there exists a factorization X = AB> (where A and B may have an arbitrary number of columns) such that\nmax { max i ∥∥A(i)∥∥22 ,maxj ∥∥B(j)∥∥22 } ≤ 2β, ∑ i ∥∥A(i)∥∥22 +∑ j ∥∥B(j)∥∥22 ≤ τ , where A(i) and B(j) are the ith row of A and the jth row of B, respectively2.\nComparing with (1) and (2), we see that the β and τ parameters essentially correspond to the max norm and trace norm, with the max norm being the minimal 2β∗ such that the matrix is (β∗, τ)decomposable for some τ , and the trace norm being the minimal τ∗/2 such that the matrix is (β, τ∗)-decomposable for some β. However, Hazan et al. go beyond these two extremes, and rely on balancing both β and τ : they establish learning guarantees (in an adversarial online model, and thus also under an arbitrary sampling distribution p) which scale with √ β · τ . It may therefore be useful to consider a penalty function of the form:\nPenalty(β,τ)(X) = min X=AB>  √ max i ∥∥A(i)∥∥22 + maxj ∥∥B(j)∥∥22 · √∑\ni ∥∥A(i)∥∥22 +∑ j ∥∥B(j)∥∥22  . (4)\n2Hazan et al. state the property differently, but equivalently, in terms of a semidefinite matrix decomposition.\n(Note that max { maxi ∥∥A(i)∥∥22 ,maxj ∥∥B(j)∥∥22} is replaced with maxi ∥∥A(i)∥∥22 + maxj ∥∥B(j)∥∥22, for later convenience. This affects the value of the penalty function by at most a factor of √ 2.)\nThis penalty function does not appear to be convex inX . However, the proposition below (proved in the Supplementary Materials) shows that we can use a (convex) local max norm penalty to compute a solution to any objective function with a penalty function of the form (4):\nProposition 1. Let X̂ be the minimizer of a penalized loss function with this modified penalty,\nX̂ := arg min X\n{ Loss(X) + λ · Penalty(β,τ)(X) } ,\nwhere λ ≥ 0 is some penalty parameter and Loss(·) is any convex function. Then, for some penalty parameter µ ≥ 0 and some t ∈ [0, 1],\nX̂ = arg min X\n{ Loss(X) + µ · ‖X‖(R,C) } , where\nR = { r ∈ ∆[n] : ri ≥\nt 1 + (n− 1)t ∀i } and C = { c ∈ ∆[m] : cj ≥\nt 1 + (m− 1)t ∀j } .\nWe note that µ and t cannot be determined based on λ alone—they will depend on the properties of the unknown solution X̂ .\nHere the sets R and C impose a lower bound on each of the weights, and this lower bound can be used to interpolate between the max and trace norms: when t = 1, each ri is lower bounded by 1/n (and similarly for cj), i.e. R and C are singletons containing only the uniform weights and we obtain the trace norm. On the other hand, when t = 0, the weights are lower-bounded by zero, and so any weight vector is allowed, i.e. R and C are each the entire simplex and we obtain the max norm. Intermediate values of t interpolate between the trace norm and max norm and correspond to different balances between β and τ ."
    }, {
      "heading" : "2.4 Interpolating between trace norm and max norm",
      "text" : "We next turn to an interpolation which relies on an upper bound, rather than a lower bound, on the weights. Consider\nR = { r ∈ ∆[n] : ri ≤ ∀i } and Cδ = { c ∈ ∆[n] : cj ≤ δ ∀j } , (5)\nfor some ∈ [1/n, 1] and δ ∈ [1/m, 1]. The (R , Cδ)-norm is then equal to the (rescaled) trace norm when we choose = 1/n and δ = 1/m, and is equal to the max norm when we choose = δ = 1. Allowing and δ to take intermediate values gives a smooth interpolation between these two familiar norms, and may be useful in situations where we want more flexibility in the type of regularization.\nWe can generalize this to an interpolation between the max norm and a smoothed weighted trace norm, which we will use in our experimental results. We consider two generalizations—for each one, we state a definition ofR, with C defined analogously. The first is multiplicative:\nR×ζ,γ := { r ∈ ∆[n] : ri ≤ γ · ((1− ζ) · pi• + ζ · 1/n) ∀i } , (6)\nwhere γ = 1 corresponds to choosing the singleton set R×ζ,γ = {(1− ζ) · prow + ζ · 1/n} (i.e. the smoothed weighted trace norm), while γ = ∞ corresponds to the max norm (for any choice of ζ) since we would getR×ζ,γ = ∆[n].\nThe second option for an interpolation is instead defined with an exponent: Rζ,τ := { r ∈ ∆[n] : ri ≤ ((1− ζ) · pi• + ζ · 1/n) 1−τ ∀i } . (7)\nHere τ = 0 will yield the singleton set corresponding to the smoothed weighted trace norm, while τ = 1 will yieldRζ,τ = ∆[n], i.e. the max norm, for any choice of ζ. We find the second (exponent) option to be more natural, because each of the row marginal bounds will reach 1 simultaneously when τ = 1, and hence we use this version in our experiments. On the other hand, the multiplicative version is easier to work with theoretically, and we use this in our learning guarantee in Section 4.2. If all of the row and column marginals satisfy some loose upper bound, then the two options will not be highly different."
    }, {
      "heading" : "3 Optimization with the local max norm",
      "text" : "One appeal of both the trace norm and the max norm is that they are both SDP representable [9, 10], and thus easily optimizable, at least in small scale problems. Indeed, in the Supplementary Materials we show that the local max norm is also SDP representable, as long as the setsR and C can be written in terms of linear or semi-definite constraints—this includes all the examples we mention, where in all of them the setsR and C are specified in terms of simple linear constraints. However, for large scale problems, it is not practical to directly use SDP optimization approaches. Instead, and especially for very large scale problems, an effective optimization approach for both the trace norm and the max norm is to use the factorized versions of the norms, given in (1) and (2), and to optimize the factorization directly (typically, only factorizations of some truncated dimensionality are used) [11, 12, 7]. As we show in Theorem 1 below, a similar factorization-optimization approach is also possible for any local max norm with convexR and C. We further give a simplified representation which is applicable when R and C are specified through element-wise upper bounds R ∈ Rn+ and C ∈ Rm+ , respectively: R = {r ∈ ∆[n] : ri ≤ Ri ∀i} and C = {c ∈ ∆[m] : cj ≤ Cj ∀j} , (8) with 0 ≤ Ri ≤ 1, ∑ iRi ≥ 1, 0 ≤ Cj ≤ 1, ∑ j Cj ≥ 1 to avoid triviality. This includes the interpolation norms of Section 2.4. Theorem 1. IfR and C are convex, then the (R, C)-norm can be calculated with the factorization\n‖X‖(R,C) = 1\n2 inf AB>=X ( sup r∈R ∑ i ri ∥∥A(i)∥∥22 + sup c∈C ∑ j cj ∥∥B(j)∥∥22 ) . (9)\nIn the special case whenR and C are defined by (8), writing (x)+ := max{0, x}, this simplifies to\n‖X‖(R,C) = 1\n2 inf AB>=X;a,b∈R\n{ a+ ∑ i Ri (∥∥A(i)∥∥22 − a)+ + b+∑ j Cj (∥∥B(j)∥∥22 − b)+ } . Proof sketch for Theorem 1. For convenience we will write r1/2 to mean diag(r)\n1/2, and same for c. Using the trace norm factorization identity (1), we have\n2 ‖X‖(R,C) = 2 sup r∈R,c∈C ∥∥∥r1/2 ·X · c1/2∥∥∥ tr = sup r∈R,c∈C inf CD>=r1/2·X·c1/2 ( ‖C‖2F + ‖D‖ 2 F ) = sup\nr∈R,c∈C inf AB>=X (∥∥∥r1/2 ·A∥∥∥2 F + ∥∥∥c1/2 ·B∥∥∥2 F ) ≤ inf AB>=X ( sup r∈R ∥∥∥r1/2A∥∥∥2 F + sup c∈C ∥∥∥c1/2B∥∥∥2 F ) ,\nwhere for the next-to-last step we set C = r1/2A and D = c1/2B, and the last step follows because sup inf ≤ inf sup always (weak duality). The reverse inequality holds as well (strong duality), and is proved in the Supplementary Materials, where we also prove the special-case result."
    }, {
      "heading" : "4 An approximate convex hull and a learning guarantee",
      "text" : "In this section, we look for theoretical bounds on error for the problem of estimating unobserved entries in a matrix Y that is approximately low-rank. Our results apply for either uniform or nonuniform sampling of entries from the matrix. We begin with a result comparing the (R, C)-norm unit ball to a convex hull of rank-1 matrices, which will be useful for proving our learning guarantee."
    }, {
      "heading" : "4.1 Convex hull",
      "text" : "To gain a better theoretical understanding of the (R, C) norm, we first need to define corresponding vector norms on Rn and Rm. For any u ∈ Rn, let\n‖u‖R := √\nsup r∈R ∑ i riu2i = sup r∈R ∥∥∥diag(r)1/2 · u∥∥∥ 2 .\nWe can think of this norm as a way to interpolate between the `2 and `∞ vector norms. For example, if we choose R = R as defined in (5), then ‖u‖R is equal to the root-mean-square of the −1 largest entries of u whenever −1 is an integer. Defining ‖v‖C analogously for v ∈ Rm, we can now relate these vector norms to the (R, C)-norm on matrices.\nTheorem 2. For any convex R ⊆ ∆[n] and C ⊆ ∆[m], the (R, C)-norm unit ball is bounded above and below by a convex hull as:\nConv { uv>:‖u‖R = ‖v‖C = 1 } ⊆ { X :‖X‖(R,C) ≤ 1 } ⊆ KG·Conv { uv>:‖u‖R = ‖v‖C = 1 } ,\nwhere KG ≤ 1.79 is Grothendieck’s constant, and implicitly u ∈ Rn, v ∈ Rm.\nThis result is a nontrivial extension of Srebro and Shraibman [1]’s analysis for the max norm and the trace norm. They show that the statement holds for the max norm, i.e. when R = ∆[n] and C = ∆[m], and that the trace norm unit ball is exactly equal to the corresponding convex hull (see Corollary 2 and Section 3.2 in their paper, respectively).\nProof sketch for Theorem 2. To prove the first inclusion, given anyX = uv> with ‖u‖R = ‖v‖C = 1, we apply the factorization result Theorem 1 to see that ‖X‖(R,C) ≤ 1. Since the (R, C)-norm unit ball is convex, this is sufficient. For the second inclusion, we state a weighted version of Grothendieck’s Inequality (proof in the Supplementary Materials):\nsup { 〈Y, UV >〉 : U ∈ Rn×k, V ∈ Rm×k, ∥∥U(i)∥∥2 ≤ ai ∀i, ∥∥V(j)∥∥2 ≤ bj ∀j} = KG · sup { 〈Y, uv>〉 : u ∈ Rn, v ∈ Rm, |ui| ≤ ai ∀i, |vj | ≤ bj ∀j } .\nWe then apply this weighted inequality to the dual norm to the (R, C)-norm to prove the desired inclusion, as in Srebro and Shraibman [1]’s work for the max norm case (see Corollary 2 in their paper). Details are given in the Supplementary Materials."
    }, {
      "heading" : "4.2 Learning guarantee",
      "text" : "We now give our main matrix reconstruction result, which provides error bounds for a family of norms interpolating between the max norm and the smoothed weighted trace norm.\nTheorem 3. Let p be any distribution on [n] × [m]. Suppose that, for some γ ≥ 1, R ⊇ R×1/2,γ and C ⊇ C × 1/2,γ , where these two sets are defined in (6). Let S = {(it, jt) : t = 1, . . . , s} be a random sample of locations in the matrix drawn i.i.d. from p, where s ≥ n. Then, in expectation over the sample S,∑\nij\npij ∣∣∣Yij − X̂ij∣∣∣ ≤ inf ‖X‖(R,C)≤ √ k ∑ ij\npij |Yij −Xij |︸ ︷︷ ︸ Approximation error +O\n(√ kn\ns\n) · (\n1 + log(n) √ γ ) ︸ ︷︷ ︸\nExcess error\n,\nwhere X̂ = arg min‖X‖(R,C)≤ √ k ∑s t=1 |Yitjt −Xitjt |. Additionally, if we assume that s ≥\nn log(n), then in the excess risk bound, we can reduce the term log(n) to √ log(n).\nProof sketch for Theorem 3. The main idea is to use the convex hull formulation from Theorem 2 to show that, for any X with ‖X‖(R,C) ≤ √ k, there exists a decomposition X = X ′ + X ′′ with ‖X ′‖max ≤ O( √ k) and ‖X ′′‖tr(p̃) ≤ O( √ k/γ), where p̃ represents the smoothed marginals with smoothing parameter ζ = 1/2 as in (3). We then apply known bounds on the Rademacher complexity of the max norm unit ball [1] and the smoothed weighted trace norm unit ball [6], to bound the Rademacher complexity of { X : ‖X‖(R,C) ≤ √ k }\n. This then yields a learning guarantee by Theorem 8 of Bartlett and Mendelson [13]. Details are given in the Supplementary Materials.\nAs special cases of this theorem, we can re-derive the existing results for the max norm and smoothed weighted trace norm. Specifically, choosing γ = ∞ gives us an excess error term of order √ kn/s for the max norm, previously shown by [1], while setting γ = 1 yields an excess error term of order√ kn log(n)/s for the smoothed weighted trace norm as long as s ≥ n log(n), as shown in [6].\nWhat advantage does this new result offer over the existing results for the max norm and for the smoothed weighted trace norm? To simplify the comparison, suppose we choose γ = log2(n), and defineR = R×1/2,γ and C = C × 1/2,γ . Then, comparing to the max norm result (when γ =∞), we see\nthat the excess error term is the same in both cases (up to a constant), but the approximation error term may in general be much lower for the local max norm than for the max norm. Comparing next to the weighted trace norm (when γ = 1), we see that the excess error term is lower by a factor of log(n) for the local max norm. This may come at a cost of increasing the approximation error, but in general this increase will be very small. In particular, the local max norm result allows us to give a meaningful guarantee for a sample size s = Θ (kn), rather than requiring s ≥ Θ (kn log(n)) as for any trace norm result, but with a hypothesis class significantly richer than the max norm constrained class (though not as rich as the trace norm constrained class)."
    }, {
      "heading" : "5 Experiments",
      "text" : "We test the local max norm on simulated and real matrix reconstruction tasks, and compare its performance to the max norm, the uniform and empirically-weighted trace norms, and the smoothed empirically-weighted trace norm."
    }, {
      "heading" : "5.1 Simulations",
      "text" : "We simulate n × n noisy matrices for n = 30, 60, 120, 240, where the underlying signal has rank k = 2 or k = 4, and we observe s = 3kn entries (chosen uniformly without replacement). We performed 50 trials for each of the 8 combinations of (n, k).\nData For each trial, we randomly draw a matrix U ∈ Rn×k by drawing each row uniformly at random from the unit sphere in Rn. We generate V ∈ Rm×k similarly. We set Y = UV > + σ · Z, where the noise matrix Z has i.i.d. standard normal entries and σ = 0.3 is a moderate noise level. We also divide the n2 entries of the matrix into sets S0 tS1 tS2 which consist of s = 3kn training entries, s validation entries, and n2 − 2s test entries, respectively, chosen uniformly at random. Methods We use the two-parameter family of norms defined in (7), but replacing the true marginals pi• and p•j with the empirical marginals p̂i• and p̂•j . We consider ζ, τ ∈ {0, 0.1, . . . , 0.9, 1}. For each (ζ, τ) combination and each penalty parameter value λ ∈ {21, 22, . . . , 210}, we compute the fitted matrix\nX̂ = arg min {∑\n(i,j)∈S0 (Yij −Xij) 2 + λ · ‖X‖(Rζ,τ ,Cζ,τ )\n} . (10)\n(In fact, we use a rank-8 approximation to this optimization problem, as described in Section 3.) For each of the considered matrix norm methods, we use the validation set S1 to select the best combination of ζ, τ , and λ, with restrictions on ζ and/or τ as specified by the definition of the method (see Table 1). We then report the error of the resulting fitted matrix on the test set S2.\nResults The results for these simulations are displayed in Figure 1. We see that the local max norm results in lower error than any of the tested existing norms, across all the settings used."
    }, {
      "heading" : "5.2 Movie ratings data",
      "text" : "We next compare several different matrix norms on two collaborative filtering movie ratings datasets, the Netflix [14] and MovieLens [15] datasets. The sizes of the data sets, and the split of the ratings into training, validation and test sets3, are:\nDataset # users # movies Training set Validation set Test set Netflix 480,189 17,770 100,380,507 100,000 1,408,395 MovieLens 71,567 10,681 8,900,054 100,000 1,000,000\n3 For Netflix, the test set we use is their “qualification set”, designed for a more uniform distribution of ratings across users relative to the training set. For MovieLens, we choose our test set at random from the available data.\nWe test the local max norm given in (7) with ζ ∈ {0, 0.05, 0.1, 0.15, 0.2} and τ ∈ {0, 0.05, 0.1}. We also test τ = 1 (the max norm—here ζ is arbitrary) and ζ = 1, τ = 0 (the uniform trace norm). We follow the test protocol of [6], with a rank-30 approximation to the optimization problem (10).\nTable 2 shows root mean squared error (RMSE) for the experiments. For both the MovieLens and Netflix data, the local max norm with τ = 0.05 and ζ = 0.05 gives strictly better accuracy than any previously-known norm studied in this setting. (In practice, we can use a validation set to reliably select good values for the τ and ζ parameters4.) For the MovieLens data, the local max norm achieves RMSE of 0.7822, compared to 0.7831 achieved by the smoothed empirically-weighted trace norm with ζ = 0.10, which gives the best result among the previously-known norms. For the Netflix dataset the local max norm achieves RMSE of 0.9090, improving upon the previous best result of 0.9096 achieved by the smoothed empirically-weighted trace norm [6]."
    }, {
      "heading" : "6 Summary",
      "text" : "In this paper, we introduce a unifying family of matrix norms, called the “local max” norms, that generalizes existing methods for matrix reconstruction, such as the max norm and trace norm. We examine some interesting sub-families of local max norms, and consider several different options for interpolating between the trace (or smoothed weighted trace) and max norms. We find norms lying strictly between the trace norm and the max norm that give improved accuracy in matrix reconstruction for both simulated data and real movie ratings data. We show that regularizing with any local max norm is fairly simple to optimize, and give a theoretical result suggesting improved matrix reconstruction using new norms in this family.\n4 To check this, we subsample half of the test data at random, and use it as a validation set to choose (ζ, τ) for each method (as specified in Table 1). We then evaluate error on the remaining half of the test data. For MovieLens, the local max norm gives an RMSE of 0.7820 with selected parameter values ζ = τ = 0.05, as compared to an RMSE of 0.7829 with selected smoothing parameter ζ = 0.10 for the smoothed weighted trace norm. For Netflix, the local max norm gives an RMSE of 0.9093 with ζ = τ = 0.05, while the smoothed weighted trace norm gives an RMSE of 0.9098 with ζ = 0.05. The other tested methods give higher error on both datasets."
    }, {
      "heading" : "A Proof of Theorem 1",
      "text" : "Special case: element-wise upper bounds First, we assume that the general result is true, i.e.\n2 ‖X‖(R,C) = inf AB>=X sup r∈R ∑ i ri ∥∥A(i)∥∥22 + sup c∈C ∑ j cj ∥∥B(j)∥∥22  , (11) and prove the result in the special case, where"
    }, {
      "heading" : "R = {r ∈ ∆[n] : ri ≤ Ri ∀i} and C = {c ∈ ∆[m] : cj ≤ Cj ∀j} .",
      "text" : "Using strong duality for linear programs, we have\nsup r∈R ∑ i ri ∥∥A(i)∥∥22 = sup r∈Rn+ {∑ i ri ∥∥A(i)∥∥22 : ri ≤ Ri, ∑ i ri = 1 } = inf a∈R,a1∈Rn+ { a+R>a1 : a+ a1i ≥ ∥∥A(i)∥∥22 ∀i} .\nIn this last line, if we fix a and want to minimize over a1 ∈ Rn+, it is clear that the infimum is obtained by setting a1i = ( ∥∥A(i)∥∥22 − a)+ for each i. This proves that sup r∈R ∑ i ri ∥∥A(i)∥∥22 = infa∈R { a+ ∑ i Ri (∥∥A(i)∥∥22 − a)+ } .\nApplying the same reasoning to the columns and plugging everything in to (11), we get\n2 ‖X‖(R,C) = inf AB>=X, a,b∈R\n{ a+ ∑ i Ri (∥∥A(i)∥∥22 − a)+ + b+∑ j Cj (∥∥B(j)∥∥22 − b)+ } .\nGeneral factorization result In the proof sketch given in the main paper, we showed that\n2 ‖X‖(R,C) ≤ inf AB>=X ( sup r∈R ∥∥∥r1/2A∥∥∥2 F + sup c∈C ∥∥∥c1/2B∥∥∥2 F ) .\nWe now want to prove the reverse inequality. Since ‖X‖(R,C) = ‖X‖(R,C) by definition (where S denotes the closure of a set S), we can assume without loss of generality that R and C are both closed (and compact) sets.\nFirst, we restrict our attention to a special case (the “positive case”), where we assume that for all r ∈ R and all c ∈ C, ri > 0 and cj > 0 for all i and j. (We will treat the general case below.) Therefore, since ‖X‖tr(r,c) is continuous as a function of (r, c) for any fixed X and since R and C are closed, we must have some r? ∈ R and c? ∈ C such that ‖X‖(R,C) = ‖X‖tr(r?,c?), with r?i > 0 for all i and c?j > 0 for all j.\nNext, let UDV > = r?1/2 ·X · c?1/2 be a singular value decomposition, and let A? = r?−1/2UD1/2 and B? = c?−1/2V D1/2. Then A?B?> = X , and∥∥∥r?1/2A?∥∥∥2 F = ∥∥∥UD1/2∥∥∥2 F = trace(UDU>) = trace(D) = ‖X‖tr(r?,c?) = ‖X‖(R,C) .\nBelow, we will show that\nr? = arg max r∈R ∥∥∥r1/2A?∥∥∥2 F . (12)\nThis will imply that ‖X‖(R,C) = supr∈R ∥∥r1/2A?∥∥2 F , and following the same reasoning for B?, we will have proved\n2 ‖X‖(R,C) = (\nsup r∈R ∥∥∥r1/2A?∥∥∥2 F + sup c∈C ∥∥∥c1/2B?∥∥∥2 F ) ≥ inf AB>=X ( sup r∈R ∥∥∥r1/2A∥∥∥2 F + sup c∈C ∥∥∥c1/2B∥∥∥2 F ) ,\nwhich is sufficient. It remains only to prove (12). Take any r ∈ R with r 6= r? and let w = r− r?. We have ∥∥∥r1/2A∥∥∥2 F − ∥∥∥r?1/2A∥∥∥2 F = ∑ i wi ∥∥A(i)∥∥22 = ∑ i wi r?i · (UDU>)ii ,\nand it will be sufficient to prove that this quantity is≤ 0. To do this, we first define, for any t ∈ [0, 1],\nf(t) := ∑ i √ 1 + t · wi r?i · (UDU>)ii = trace (( r? + tw r? )1/2 UDU> ) .\nUsing the fact that trace(·) ≤ ‖·‖tr for all matrices, we have\nf(t) ≤ ∥∥∥∥∥ ( r? + tw r? )1/2 UDU> ∥∥∥∥∥ tr = ∥∥∥(r? + tw)1/2Xc?1/2 · V U>∥∥∥ tr\n= ∥∥∥(r? + tw)1/2Xc?1/2∥∥∥\ntr = ‖X‖tr(r?+tw,c?) ≤ ‖X‖(R,C) = ∑ i (UDU>)ii = f(0) ,\nwhere the last inequality comes from the fact that r? + tw ∈ R by convexity ofR. Therefore,\n0 ≥ d dt f(t) ∣∣∣∣ t=0 = d dt (∑ i √ 1 + t · wi r?i · (UDU>)ii ) ∣∣∣∣ t=0 = 1 2 · ∑ i wi r?i · (UDU>)ii ,\nas desired. (Here we take the right-sided derivative, i.e. taking a limit as t approaches zero from the right, since f(t) is only defined for t ∈ [0, 1].) This concludes the proof for the positive case. Next, we prove that the general factorization (11) hold in the general case, where we might have R 6⊂ Rn++ and/or C 6⊂ Rm++. If for any i ∈ [n] we have ri = 0 for all r ∈ R, we can discard this row of X , and same for any j ∈ [m]. Therefore, without loss of generality, for all i ∈ [n] there is some r(i) ∈ R with r(i)i > 0. Taking a convex combination, r+ = 1n ∑ i r\n(i) ∈ R, we have r+ ∈ R ∩ Rn++. Similarly, we can construct c+ ∈ C ∩ Rm++.\nFix any > 0, and let δ = min{mini r+i ,minj c + j } · 2(1+ ) > 0, and define closed subsets R0 = { r ∈ R : min\ni ri ≥ δ\n} ⊆ R and C0 = { c ∈ C : min\ni ci ≥ δ\n} ⊆ C .\nSince we know that the factorization result holds for the “positive case”, we have\ninf AB>=X ( sup r∈R0 ∥∥∥r1/2A∥∥∥2 F + sup c∈C0 ∥∥∥c1/2B∥∥∥2 F ) = 2 ‖X‖(R0,C0)\n= 2 sup r∈R0,c∈C0 ∥∥∥r1/2Xc1/2∥∥∥ tr ≤ 2 sup r∈R,c∈C ∥∥∥r1/2Xc1/2∥∥∥ tr = 2 ‖X‖(R,C) .\nNow choose any factorization ÃB̃> = X such that( sup r∈R0 ∥∥∥r1/2Ã∥∥∥2 F + sup c∈C0 ∥∥∥c1/2B̃∥∥∥2 F ) ≤ 2 sup r∈R,c∈C ∥∥∥r1/2Xc1/2∥∥∥ tr (1 + /2) . (13)\nNext, we need to show that supr∈R ∥∥∥r1/2Ã∥∥∥2\nF is not much larger than supr∈R0 ∥∥∥r1/2Ã∥∥∥2 F (and same\nfor B̃). Choose any r′ ∈ R, and let r′′ = (\n1− δ mini r + i\n) r′ + ( δ\nmini r + i\n) r+ ∈ R. Then\nmin i\nr′′i ≥ (\nδ\nmini r + i\n) min i r+i = δ ,\nand so r′′ ∈ R0. We also have r′i ≤ (\n1− δ mini r + i )−1 r′′i for all i. Therefore,∥∥∥r′1/2Ã∥∥∥ F ≤ ( 1− δ mini r + i )−1/2 ∥∥∥r′′1/2Ã∥∥∥ F ≤ ( 1− δ mini r + i )−1/2 sup r∈R0 ∥∥∥r1/2Ã∥∥∥ F .\nSince this is true for any r′ ∈ R, applying the definition of δ, we have\nsup r∈R ∥∥∥r1/2Ã∥∥∥ F ≤ ( 1− δ mini r + i )−1/2 sup r∈R0 ∥∥∥r1/2Ã∥∥∥ F ≤ ( 1 + /2 1 + )−1/2 sup r∈R0 ∥∥∥r1/2Ã∥∥∥ F .\nApplying the same reasoning for B̃ and then plugging in the bound (13), we have\ninf AB>=X ( sup r∈R ∥∥∥r1/2A∥∥∥2 F + sup c∈C ∥∥∥c1/2B∥∥∥2 F ) ≤ ( sup r∈R ∥∥∥r1/2Ã∥∥∥ F + sup c∈C ∥∥∥c1/2B̃∥∥∥2 F ) ≤ ( 1 + /2\n1 +\n)−1 · (\nsup r∈R0 ∥∥∥r1/2Ã∥∥∥2 F + sup c∈C0 ∥∥∥c1/2B̃∥∥∥2 F ) ≤ ( 1 + /2\n1 +\n)−1 (1 + /2) · 2 ‖X‖(R,C) = (1 + ) · 2 ‖X‖(R,C) .\nSince this analysis holds for arbitrary > 0, this proves the desired result, that\ninf AB>=X ( sup r∈R ∥∥∥r1/2A∥∥∥2 F + sup c∈C ∥∥∥c1/2B∥∥∥2 F ) ≤ 2 ‖X‖(R,C) ."
    }, {
      "heading" : "B Proof of Theorem 2",
      "text" : "We follow similar techniques as used by Srebro and Shraibman [1] in their proof of the analogous result for the max norm. We need to show that\nConv { uv> : u ∈ Rn, v ∈ Rm, ‖u‖R = ‖v‖C = 1 } ⊆ { X : ‖X‖(R,C) ≤ 1 } ⊆\nKG · Conv { uv> : u ∈ Rn, v ∈ Rm, ‖u‖R = ‖v‖C = 1 } .\nFor the left-hand inclusion, since ‖·‖(R,C) is a norm and therefore the constraint ‖X‖(R,C) ≤ 1 is convex, it is sufficient to show that ∥∥uv>∥∥ (R,C) ≤ 1 for any u ∈ R\nn, v ∈ Rm with ‖u‖R = ‖v‖C = 1. This is a trivial consequence of the factorization result in Theorem 1.\nNow we prove the right-hand inclusion. Grothendieck’s Inequality states that, for any Y ∈ Rn×m and for any dimension k,\nsup { 〈Y,UV >〉 : U ∈ Rn×k, V ∈ Rm×k, ∥∥U(i)∥∥2 ≤ 1 ∀i, ∥∥V(j)∥∥2 ≤ 1 ∀j} ≤ KG · sup { 〈Y, uv>〉 : u ∈ Rn, v ∈ Rm, |ui| ≤ 1 ∀i, |vj | ≤ 1 ∀j } ,\nwhere KG ∈ (1.67, 1.79) is Grothendieck’s constant. We now extend this to a slightly more general form. Take any a ∈ Rn+ and b ∈ Rm+ . Then, setting Ũ = diag(a)+U and Ṽ = diag(b)+V (where M+ is the pseudoinverse of M ), and same for ũ and ṽ, we see that\nsup { 〈Y, UV >〉 : U ∈ Rn×k, V ∈ Rm×k, ∥∥U(i)∥∥2 ≤ ai ∀i, ∥∥V(j)∥∥2 ≤ bj ∀j} = sup { 〈diag(a) · Y · diag(b), Ũ Ṽ >〉 : Ũ ∈ Rn×k, Ṽ ∈ Rm×k, ∥∥∥Ũ(i)∥∥∥ 2 ≤ 1 ∀i, ∥∥∥Ṽ(j)∥∥∥ 2 ≤ 1 ∀j\n} ≤ KG · sup { 〈diag(a) · Y · diag(b), ũṽ>〉 : ũ ∈ Rn, ṽ ∈ Rm, |ũi| ≤ 1 ∀i, |ṽj | ≤ 1 ∀j\n} = KG · sup { 〈Y, uv>〉 : u ∈ Rn, v ∈ Rm, |ui| ≤ ai ∀i, |vj | ≤ bj ∀j } . (14)\nNow take any Y ∈ Rn×m. Let ‖·‖∗(R,C) be the dual norm to the (R, C)-norm. To bound this dual norm of Y , we apply the factorization result of Theorem 1:\n‖Y ‖∗(R,C) = sup ‖X‖(R,C)≤1 〈Y,X〉\n= sup U,V 〈Y, UV >〉 : 12 sup r∈R ∑ i ri ∥∥U(i)∥∥22 + sup c∈C ∑ j cj ∥∥V(j)∥∥22  ≤ 1 \n(∗) = sup\nU,V 〈Y, UV >〉 : supr∈R∑i ri ∥∥U(i)∥∥22 = sup c∈C ∑ j cj ∥∥V(j)∥∥22 ≤ 1  = sup a∈Rn+:‖a‖R≤1 b∈Rm+ :‖b‖C≤1 sup U,V { 〈Y, UV >〉 :\n∥∥U(i)∥∥2 ≤ ai ∀i, ∥∥V(j)∥∥2 ≤ bj ∀j} ≤ KG · sup\na∈Rn+:‖a‖R≤1 b∈Rm+ :‖b‖C≤1\nsup U,V\n{ 〈Y, uv>〉 : |ui| ≤ ai ∀i, |vj | ≤ bj ∀j } = KG · sup\nu,v\n{ 〈Y, uv>〉 : ‖u‖R ≤ 1, ‖v‖C ≤ 1 } = KG · sup\nX\n{ 〈Y,X〉 : X ∈ Conv { uv> : u ∈ Rn, v ∈ Rm, ‖u‖R = ‖v‖C = 1 }} = sup\nX\n{ 〈Y,X〉 : X ∈ KG · Conv { uv> : u ∈ Rn, v ∈ Rm, ‖u‖R = ‖v‖C = 1 }} .\nAs in [1], this is sufficient to prove the result. Above, the step marked (*) is true because, given any U and V with\n1\n2 sup r∈R ∑ i ri ∥∥U(i)∥∥22 + sup c∈C ∑ j cj ∥∥V(j)∥∥22  ≤ 1 ,\nwe can replace U and V with U ′ := U · ω and V ′ := V · ω−1, where ω := 4 √ supc∈C ∑ j cj‖V(j)‖22\nsupr∈R ∑ i ri‖U(i)‖22 .\nThis will give U ′V ′> = UV >, and\nsup r∈R ∑ i ri ∥∥∥U ′(i)∥∥∥2 2 = sup c∈C ∑ j cj ∥∥∥V ′(j)∥∥∥2 2 = √ sup r∈R ∑ i ri ∥∥U(i)∥∥22 · sup c∈C ∑ j cj ∥∥V(j)∥∥22\n≤ 1 2 sup r∈R ∑ i ri ∥∥U(i)∥∥22 + sup c∈C ∑ j cj ∥∥V(j)∥∥22  ≤ 1 ."
    }, {
      "heading" : "C Proof of Theorem 3",
      "text" : "Following the strategy of Srebro & Shraibman (2005), we will use the Rademacher complexity to bound this excess risk. By Theorem 8 of Bartlett & Mendelson (2002)5, we know that\nES ∑ ij pij ∣∣∣Yij − X̂ij∣∣∣− inf ‖X‖(R,C)≤ √ k ∑ ij pij |Yij −Xij |  = O ( ES [ R̂S ({ X ∈ Rn×m : ‖X‖(R,C) ≤ √ k })]) , (15)\nwhere the expected Rademacher complexity is defined as\nES [ R̂S ({ X ∈ Rn×m : ‖X‖(R,C) ≤ √ k })] := 1\ns ES,ν  sup ‖X‖(R,C)≤ √ k ∑ t νt ·Xitjt  , where ν ∈ {±1}s is a random vector of independent unbiased signs, generated independently from S.\nNow we bound the Rademacher complexity. By scaling, it is sufficient to consider the case k = 1. The main idea for this proof is to first show that, for any X with ‖X‖(R,C) ≤ 1, we can decompose X into a sum X ′ + X ′′ where ‖X ′‖max ≤ KG and ‖X ′′‖tr(p̃) ≤ 2KGγ−\n1/2, where p̃ represents the smoothed row and column marginals with smoothing parameter ζ = 1/2, and where KG ≤ 1.79 is Grothendieck’s constant. We will then use known Rademacher complexity bounds for the classes of matrices that have bounded max norm and bounded smoothed weighted trace norm.\nTo construct the decomposition of X , we start with a vector decomposition lemma, proved below. Lemma 1. SupposeR ⊇ R×1/2,γ . Then for any u ∈ R\nn with ‖u‖R = 1, we can decompose u into a sum u = u′ + u′′ such that ‖u′‖∞ ≤ 1 and ‖u′′‖p̃row := ∑ i p̃i•u ′′ i 2 ≤ γ−1/2.\nNext, by Theorem 2, we can write\nX = KG · ∞∑ l=1 tl · ulv>l ,\nwhere tl ≥ 0, ∑∞ l=1 tl = 1, and ‖ul‖R = ‖vl‖C = 1 for all l. Applying Lemma 1 to ul and to vl for each l, we can write ul = u′l + u ′′ l and vl = v ′ l + v ′′ l , where\n‖u′l‖∞ ≤ 1, ‖u ′′ l ‖p̃row ≤ γ −1/2, ‖v′l‖∞ ≤ 1, ‖v ′′ l ‖p̃col ≤ γ −1/2 .\nThen\nX = KG · ( ∞∑ l=1 tl · u′lv′l> + ∞∑ l=1 tl · u′lv′′l > + ∞∑ l=1 tl · u′′l vl> ) =: KG (X1 +X2 +X3) .\n5 The statement of their theorem gives a result that holds with high probability, but in the proof of this result they derive a bound in expectation, which we use here.\nFurthermore, ‖u′l‖p̃row ≤ ‖u ′ l‖∞ ≤ 1, and ‖vl‖p̃row ≤ ‖vl‖C ≤ 1. Applying Srebro and Shraibman [1]’s convex hull bounds for the trace norm and max norm (stated in Section 4 of the main paper), we see that ‖X1‖max ≤ 1, and that that ‖Xi‖tr(p̃) ≤ γ−\n1/2 for i = 2, 3. Defining X ′ = X1 and X ′′ = X2 +X3, we have the desired decomposition.\nApplying this result to every X in the class { X ∈ Rn×m : ‖X‖(R,C) ≤ 1 } , we see that\nES [ R̂S ({ X ∈ Rn×m : ‖X‖(R,C) ≤ 1 })] ≤ ES [ R̂S ({X ′ : ‖X ′‖max ≤ KG}) ] + ES [ R̂S ({ X ′′ : ‖X ′′‖tr(p̃) ≤ KG · 2γ −1/2 })]\n≤ KG · O (√ n\ns\n) +KG · 2γ− 1/2 · O (√ n log(n)\ns + n log(n) s\n) ,\nwhere the last step uses bounds on the Rademacher complexity of the max norm and weighted trace norm unit balls, shown in Theorem 5 of [1] and Theorem 3 of [6], respectively. Finally, we want to deal with the last term, n log(n)s , that is outside the square root. Since s ≥ n by assumption, we\nhave n log(n)s ≤ √ n log2(n) s , and if s ≥ n log(n), then we can improve this to n log(n) s ≤ √ n log(n)\ns . Returning to (15) and plugging in our bound on the Rademacher complexity, this proves the desired bound on the excess risk.\nC.1 Proof of Lemma 1\nFor u ∈ Rn with ‖u‖R = 1, we need to find a decomposition u = u′ + u′′ such that ‖u′‖∞ ≤ 1 and ‖u′′‖p̃row = √∑ i p̃i•u ′′ i\n2 ≤ γ−1/2. Without loss of generality, assume |u1| ≥ · · · ≥ |un|. Find N ∈ {1, . . . , n} and t ∈ (0, 1] so that ∑N−1 i=1 p̃i• + t · p̃N• = γ−1, and let\nr = γ · (p̃1•, . . . , p̃(N−1)•, t · p̃N•, 0, . . . , 0) ∈ ∆[n] .\nClearly, ri ≤ γ · p̃i• for all i, and so r ∈ R×1/2,γ ⊆ R. Now let u′′ = (u1, . . . , uN−1, √ t · uN , 0, . . . , 0), and set u′ = u− u′′. We then calculate\n‖u′′‖2p̃row = N−1∑ i=1 p̃i•u 2 i + t · p̃N•u2N = γ−1 n∑ i=1 riu 2 i ≤ γ−1 ‖u‖ 2 R ≤ γ −1 .\nFinally, we want to show that ‖u′‖∞ ≤ 1. Since u′i = 0 for i < N , we only need to bound |u′i| for each i ≥ N . We have\n1 = ‖u‖2R ≥ n∑\ni′=1\nri′u 2 i′ ≥ N∑ i′=1 ri′u 2 i′ (∗) ≥ u2i · N∑ i′=1 ri′ (#) = u2i ≥ u′i2 ,\nwhere the step marked (*) uses the fact that |ui′ | ≥ |ui| for all i′ ≤ N , and the step marked (#) comes from the fact that r is supported on {1, . . . , N}. This is sufficient."
    }, {
      "heading" : "D Proof of Proposition 1",
      "text" : "Let L0 = Loss(X̂). Then, by definition, X̂ = arg min { Penalty(β,τ)(X) : Loss(X) ≤ L0 } .\nThen to prove the lemma, it is sufficient to show that for some t ∈ [0, 1], X̂ = arg min { ‖X‖(R(t),C(t)) : Loss(X) ≤ L0 } ,\nwhere we set R(t) = { r ∈ ∆[n] : ri ≥\nt 1 + (n− 1)t ∀i } , C(t) = { c ∈ ∆[m] : cj ≥\nt 1 + (m− 1)t ∀j } .\nTrivially, we can rephrase these definitions as R(t) = { t\n1 + (n− 1) · t · (1, . . . , 1) + 1− t 1 + (n− 1) · t · r : r ∈ ∆[n]\n} and\nC(t) = {\nt 1 + (m− 1) · t · (1, . . . , 1) + 1− t 1 + (m− 1) · t · c : c ∈ ∆[m]\n} . (16)\nNote that for any vectors u ∈ Rn+ and v ∈ Rm+ ,\nsup r∈∆[n] ∑ i riui = max i ui and sup c∈∆[m] ∑ j cjvj = max j vj . (17)\nApplying the SDP formulation of the local max norm (proved in Lemma 2 below), we have\n‖X‖(R(t),C(t)) = 1\n2 inf  supr∈R(t) ∑ i riUii + sup c∈C(t) ∑ j cjVjj : ( U X X> V ) 0  By (16) and (17)\n= 1\n2 inf\n{ t 1 + (n− 1) · t · ∑ i Uii + 1− t 1 + (n− 1) · t max i Uii\n+ t 1 + (m− 1) · t · ∑ j Vjj + 1− t 1 + (m− 1) · t max j Vjj : ( U X X> V ) 0 }\n= ωt 2 inf { t ∑ i Aii + (1− t) max i Aii + t ∑ j Bjj + (1− t) max j Bjj : ( A X X> B ) 0 }\n= ωt 2 inf\n{ (1− t) ·M(A,B) + t · T(A,B) : X ∈ XA,B } , (18)\nwhere for the next-to-last step, we define\nA = U · √ 1 + (m− 1) · t 1 + (n− 1) · t , B = V · √ 1 + (n− 1) · t 1 + (m− 1) · t , ωt = 1√ (1 + (n− 1) · t)(1 + (m− 1) · t) ,\nand for the last step, we define\nT(A,B) = trace(A) + trace(B), M(A,B) = max i Aii + max j Bjj ,\nand\nXA,B = { X : ( A X X> B ) 0 } .\nNext, we compare this to the (β, τ) penalty formulated in our main paper. Recall\nPenalty(β,τ)(X) = inf X=AB>  √ max i ∥∥A(i)∥∥22 + maxj ∥∥B(j)∥∥22 · √∑\ni ∥∥A(i)∥∥22 +∑ j ∥∥B(j)∥∥22  .\nApplying Lemma 3 below, we can obtain an equivalent SDP formulation of the penalty\nPenalty(β,τ)(X) = inf A,B\n{√ M(A,B) · √ T(A,B) : X ∈ XA,B } . (19)\nSince M(A,B) ≤ T(A,B) ≤ max{n,m}M(A,B), and since for any x, y > 0 we know √xy ≤ 1 2 ( α · x+ α−1 · y ) for any α > 0 with equality attained when α = √ y/x, we see that\nPenalty(β,τ)(X̂) = 1\n2 inf A,B\n{ inf\nα∈[1, √ max{n,m}]\n{ α ·M(A,B) + α−1 · T(A,B) } : X̂ ∈ XA,B\n}\n= inf α∈[1, √ max{n,m}]\n[ 1\n2 inf A,B\n{ α ·M(A,B) + α−1 · T(A,B) : X̂ ∈ XA,B }] .\nSince the quantity inside the square brackets is nonnegative and is continuous in α, and we are minimizing over α in a compact set, the infimum is attained at some α̂, so we can write\nPenalty(β,τ)(X̂) = 1\n2 inf A,B\n{ α̂ ·M(A,B) + α̂−1 · T(A,B) : X̂ ∈ XA,B } .\nRecall that X̂ minimizes Penalty(β,τ)(X) subject to the constraint Loss(X) ≤ L0. Setting t := α̂−1 α̂+α̂−1 , we get\nX̂ ∈ arg min X { inf A,B { α̂ ·M(A,B) + α̂−1 · T(A,B) : X ∈ XA,B } : Loss(X) ≤ L0 } = arg min\nX { inf A,B { α̂ α̂+ α̂−1 ·M(A,B) + α̂ −1 α̂+ α̂−1 · T(A,B) : X ∈ XA,B } : Loss(X) ≤ L0 } = arg min\nX { inf A,B {(1− t) ·M(A,B) + t · T(A,B) : X ∈ XA,B} : Loss(X) ≤ L0 } = arg min\nX\n{ ‖X‖(R(t),C(t)) : Loss(X) ≤ L0 } ,\nas desired."
    }, {
      "heading" : "E Computing the local max norm with an SDP",
      "text" : "Lemma 2. Suppose R and C are convex, and are defined by SDP-representable constraints. Then the (R, C)-norm can be calculated with the semidefinite program\n‖X‖(R,C) = 1\n2 inf supr∈R∑i riAii + supc∈C ∑ j cjBjj : ( A X X> B ) 0  . In the special case whereR and C are defined as in (8) in the main paper, then the norm is given by\n‖X‖(R,C) = 1\n2 inf\n{ a+R>a1 + b+ C >b1 : a1i ≥ 0 and a+ a1i ≥ Aii ∀i,\nb1j ≥ 0 and b+ b1j ≥ Bjj ∀j, (\nA X X> B\n) 0 } .\nProof. For the general case, based on Theorem 1 in the main paper, we only need to show that\ninf supr∈R∑i riAii + supc∈C ∑ j cjBjj : ( A X X> B ) 0  = inf sup r∈R ∑ i ri ∥∥A(i)∥∥22 + sup c∈C ∑ j cj ∥∥B(j)∥∥22 : AB> = X\n . This is proved in Lemma 3 below.\nFor the special case where R and C are defined by element-wise bounds, we return to the proof of Theorem 1 given in Section A, where we see that"
    }, {
      "heading" : "2 ‖X‖(R,C) = inf",
      "text" : "AB>=X,a,b∈R a1∈Rn+,b1∈R m +\n{ a+R>a1+b+C >b1 : a+a1i ≥ ∥∥A(i)∥∥22 ∀i, b+b1j ≥ ∥∥B(j)∥∥22 ∀j} .\nNoting that ∥∥A(i)∥∥22 = (AA>)ii and ∥∥B(j)∥∥22 = (BB>)jj , we again use Lemma 3 to see that this is equivalent to the SDP\ninf { a+R>a1 + b+ C >b1 : a1i ≥ 0 and a+ a1i ≥ Aii ∀i,\nb1j ≥ 0 and b+ b1j ≥ Bjj ∀j, (\nA X X> B\n) 0 } .\nLemma 3. Let f : Rn ×Rm → R be any function that is nondecreasing in each coordinate and let X ∈ Rn×m be any matrix. Then\ninf { f (∥∥A(1)∥∥22 , . . . ,∥∥A(n)∥∥22 ,∥∥B(1)∥∥22 , . . . ,∥∥B(m)∥∥22) : AB> = X}\n= inf { f (Φ11, . . . ,Φnn,Ψ11, . . . ,Ψmm) : ( Φ X X> Ψ ) 0 } ,\nwhere the factorization AB> = X is assumed to be of arbitrary dimension, that is, A ∈ Rn×k and B ∈ Rm×k for arbitrary k ∈ N.\nProof. We follow similar arguments as in Lemma 14 in [16], where this equality is shown for the special case of calculating a trace norm.\nFor convenience, we write g(A,B) = f (∥∥A(1)∥∥22 , . . . ,∥∥A(n)∥∥22 ,∥∥B(1)∥∥22 , . . . ,∥∥B(m)∥∥22)\nand h(Φ,Ψ) = f (Φ11, . . . ,Φnn,Ψ11, . . . ,Ψmm) .\nThen we would like to show that\ninf { g(A,B) : AB> = X } = inf { h(Φ,Ψ) : ( Φ X X> Ψ ) 0 } .\nFirst, take any factorization AB> = X . Let Φ = AA> and Ψ = BB>. Then (\nΦ X X> Ψ\n) 0,\nand we have g(A,B) = h(Φ,Ψ) by definition. Therefore,\ninf { g(A,B) : AB> = X } ≥ inf { h(Φ,Ψ) : ( Φ X X> Ψ ) 0 } .\nNext, take any Φ and Ψ such that (\nΦ X X> Ψ\n) 0. Take a Cholesky decomposition\n( Φ X X> Ψ ) = ( A 0 B C ) · ( A 0 B C )> = ( AA> AB> BA> BB> + CC> ) .\nFrom this, we see that AB> = X , that Φii = ∥∥A(i)∥∥22 for all i, and that Ψjj ≥ ∥∥B(j)∥∥22 for all j. Since f is nondecreasing in each coordinate, we have h(Φ,Ψ) ≥ g(A,B). Therefore, we see that\ninf { g(A,B) : AB> = X } ≤ inf { h(Φ,Ψ) : ( Φ X X> Ψ ) 0 } ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "We introduce a new family of matrix norms, the “local max” norms, generalizing<lb>existing methods such as the max norm, the trace norm (nuclear norm), and the<lb>weighted or smoothed weighted trace norms, which have been extensively used in<lb>the literature as regularizers for matrix reconstruction problems. We show that this<lb>new family can be used to interpolate between the (weighted or unweighted) trace<lb>norm and the more conservative max norm. We test this interpolation on simulated<lb>data and on the large-scale Netflix and MovieLens ratings data, and find improved<lb>accuracy relative to the existing matrix norms. We also provide theoretical results<lb>showing learning guarantees for some of the new norms.",
    "creator" : "LaTeX with hyperref package"
  }
}