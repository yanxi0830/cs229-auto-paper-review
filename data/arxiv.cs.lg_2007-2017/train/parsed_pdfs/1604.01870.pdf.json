{
  "name" : "1604.01870.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Globally Convergent Stochastic Optimization for Canonical Correlation Analysis",
    "authors" : [ "Weiran Wang", "Jialei Wang" ],
    "emails" : [ "weiranwang@ttic.edu", "jialei@uchicago.edu", "nati@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 4.\n01 87\n0v 1\n[ cs\n.L G\n] 7\nA pr"
    }, {
      "heading" : "1 Introduction",
      "text" : "Canonical correlation analysis (CCA, [1]) and its extensions are ubiquitous techniques in scientific research areas for revealing the common sources of variability in multiple views of the same phenomenon. In CCA, the trainng set consists of paired observations from two views, denoted (x1,y1), . . . , (xN ,yN ), where N is the training set size, xi ∈ Rdx and yi ∈ Rdy for i = 1, . . . , N . We also denote the data matrices for each view1 by X = [x1, . . . ,xN ] ∈ Rdx×N and Y = [y1, . . . ,yN ] ∈ Rdy×N . The objective of (regularized) linear CCA is to find linear projections of each view such that the correlation between the projections is maximized:\nmax u,v\nu⊤Σxyv (1)\ns.t. u⊤Σxxu = v⊤Σyyv = 1\nwhere Σxy = 1NXY ⊤ is the cross-covariance matrix, Σxx = 1NXX ⊤+γxI and Σyy = 1NYY ⊤+ γyI are the auto-covariance matrices, and (γx, γy) ≥ 0 are regularization parameters [2, 3]. We denote by (u∗,v∗) the global optimum of (1), which is unique up to a change of sign.\nThe CCA objective has a closed form solution as follows. Define\nT := Σ − 1\n2 xx ΣxyΣ\n− 1 2\nyy ∈ Rdx×dy , (2) and let (φ,ψ) be the (unit-length) left and right singular vector pair associated with T’s largest singular value ρ1. Then the optimal objective value, i.e., the canonical correlation between the views, is ρ1, achieved by (u∗, v∗) = (Σ − 1 2 xx φ, Σ − 1 2 yy ψ). Note that\nρ1 = ‖T‖ ≤ ∥ ∥ ∥ Σ − 1 2 xx X ∥ ∥ ∥ ∥ ∥ ∥ Σ − 1 2 yy Y ∥ ∥ ∥ ≤ 1.\n∗The first two authors contributed equally. 1We assume that X and Y are centered at the origin for notational simplicity; if they are not, we can center\nthem as a pre-processing operation.\nAlgorithm 1 Alternating least squares for CCA.\nInput: Data matrices X ∈ Rdx×N , Y ∈ Rdy×N , regularization parameters (γx, γy). Initialize ũ0 ∈ Rdx , ṽ0 ∈ Rdy . { φ̃0, ψ̃0 }\nu0 ← ũ0/ √ ũ⊤0 Σxxũ0\n{ φ0 ← φ̃0/ ∥ ∥ ∥ φ̃0 ∥ ∥ ∥ }\nv0 ← ṽ0/ √ ṽ⊤0 Σyyṽ0\n{ ψ0 ← ψ̃0/ ∥ ∥ ∥ ψ̃0 ∥ ∥ ∥ }\nfor t = 1, 2, . . . , T do\nũt ← Σ−1xxΣxyvt−1 { φ̃t ← Σ − 1 2 xx ΣxyΣ − 1 2 yy ψt−1 } ṽt ← Σ−1yy Σ⊤xyut−1 { ψ̃t ← Σ − 1 2 yy Σ ⊤ xyΣ − 1 2 xx φt−1 }\nut ← ũt/ √ ũ⊤t Σxxũt\n{ φt ← φ̃t/ ∥ ∥ ∥ φ̃t ∥ ∥ ∥ }\nvt ← ṽt/ √ ṽ⊤t Σyyṽt\n{ ψt ← ψ̃t/ ∥ ∥ ∥ ψ̃t ∥ ∥ ∥ }\nend for Output: (uT ,vT ) → (u∗,v∗) as T → ∞. {(φT ,ψT ) → (φ,ψ)}\nFurthermore, we are guaranteed to have ρ1 < 1 if (γx, γy) > 0.\nFor large high dimensional datasets, it is time and memory consuming to first explicitly form the matrix T (which requires eigenvalue decomposition of the covariance matrices) and then compute its singular value decomposition (SVD). For such problems, it is desirable to develop stochastic algorithms that have efficient updates, converges fast, and can take advantage of the input sparsity. There have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [4, 5, 6], but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.\nFurther assume T has a positive gap between its first and second singular values ρ1 and ρ2. The main result of this paper is the proof of the following theorem.\nTheorem 1.1 (Main result). Fix η > 0, and assume the initialization (u0,v0) satisfies u⊤0 Σxxu ∗ 6= 0 and v⊤0 Σyyv ∗ 6= 0. Then there exists an algorithm that starts with (u0,v0) and finds an approximate solution (u,v) to (1) such that u⊤Σxxu = v⊤Σyyv = 1, and\nmin ( (u⊤Σxxu ∗)2, (v⊤Σyyv ∗)2 ) ≥ 1− η, u⊤Σxyv ≥ ρ1(1− 2η) in total time\nÕ ( d (N + κ) ρ21\nρ21 − ρ22\n)\n,\nwhere d = dx+dy, κ = max ( maxi‖xi‖ 2\nσmin(Σxx) , maxi‖yi‖\n2\nσmin(Σyy)\n)\n, and the notation Õ(·) hides poly-logarithmic dependencies on d and 1/η."
    }, {
      "heading" : "2 Alternating least squares",
      "text" : "Our solution to (1) is inspired by the alternating least squares formulation of CCA [7, Algorithm 5.2], as shown in Algorithm 1. A similar algorithm is recently used by [8] for large scale linear CCA with high dimensional sparse inputs, and by [4, 5] for motivating stochastic CCA algorithms.\nLet the nonzero singular values of T be 1 ≥ ρ1 ≥ ρ2 ≥ · · · ≥ ρr > 0, where r = rank(T) ≤ min(dx, dy), and the corresponding (unit-length) left and right singular vector pairs be (a1,b1), . . . , (ar,br), with a1=φ and b = ψ. Now define\nC =\n[\n0 T T⊤ 0\n]\n∈ R(dx+dy)×(dx+dy).\nIt is straightforward to check that the nonzero eigenvalues of C are:\nρ1 ≥ ρ2 ≥ · · · ≥ ρr ≥ −ρr ≥ · · · ≥ −ρ1,\nwith corresponding eigenvectors [\na1 b1\n]\n, . . . ,\n[\nar br\n]\n,\n[\nar −br\n]\n, . . . ,\n[\na1 −b1\n]\n.\nThe key observation is that Algorithm 1 effectively runs a variant of power iterations on C to extract\nits top eigenvector\n[\nφ ψ\n]\n. To see this, make the change of variables φt = Σ 1 2 xxut, ψt = Σ 1 2 yyvt,\nand similarly for their unnormalized version φ̃t = Σ 1 2 xxũt, ψ̃t = Σ 1 2 yyṽt. Then we can equivalently rewrite the steps of Algorithm 1 in terms of the new variables as in {} of each line. Observe that the iterates are updated as follows from step t− 1 to step t:\n[\nφ̃t ψ̃t\n] ← [ 0 T\nT⊤ 0\n] [\nφt−1 ψt−1\n]\n,\n[\nφt ψt\n] ← [ φ̃t/||φ̃t|| ψ̃t/||ψ̃t|| ] . (3)\nTherefore, except for the special normalization steps (which rescale the two sets of variables separately), Algorithm 1 is very similar to the power iterations [9].\nWe now prove the convergence of this “power iterations”. The first natural measure of progress is the objective of (1), i.e., u⊤t Σxyvt, with the maximum value being ρ1. Another measure is the alignment of φt to φ, and the alignment of ψt to ψ, i.e., (φ ⊤ t φ) 2 = (u⊤t Σxxu ∗)2 and (ψ⊤t ψ)\n2 = (v⊤t Σyyv\n∗)2. The maximum value for such alignments is 1, achieved when the iterates completely align with the optimal solution.\nTheorem 2.1 (Convergence of Algorithm 1). Let µ := min ( (u⊤0 Σxxu ∗)2, (v⊤0 Σyyv ∗)2 ) > 0. Then for t ≥ ⌈ ρ 2 1\nρ2 1 −ρ2 2\nlog (\n1 µη\n)\n⌉, we have\nmin ( (u⊤t Σxxu ∗)2, (v⊤t Σyyv ∗)2 ) ≥ 1− η,\nand u⊤t Σxyvt ≥ ρ1(1 − 2η).\nRemarks We have assumed a nonzero singular value gap in Theorem 2.1 to obtain linear convergence in both the alignments and the objective. When there exists no singular value gap, the top singular vector pair is not unique and it is no longer meaningful to measure the alignments. Nonetheless, it is possible to extend our proof to obtain sublinear convergence for the objective in this case.\nObserve that, besides the steps of normalization to unit length, the basic operation in each iteration of Algorithm 1 is of the form ũt ← Σ−1xxΣxyvt−1 = ( 1NXX⊤ + γxI)−1 1NXY⊤vt−1, which is equivalent to solving the following regularized least squares (ridge regression) problem\nmin u\n1\n2N\n∥ ∥u⊤X− v⊤t−1Y ∥ ∥ 2 + γx 2 ‖u‖2 = 1 N\nN ∑\ni=1\n1\n2\n∣ ∣u⊤xi − v⊤t−1yi ∣ ∣ 2 + γx 2 ‖u‖2 , (4)\nwhich has the finite sum structure. In the next section, we show that, to maintain the convergence rate of power iterations, it is unnecessary to solve the least squares problem exactly. This enables us to use state-of-art stochastic gradient descent methods for solving the subproblem (4) to sufficient accuracy, and to obtain a globally convergent stochastic algorithm for CCA."
    }, {
      "heading" : "3 Our algorithm",
      "text" : "Our algorithm consists of two nested loops. The outer loop runs inexact power iterations while the inner loop uses advanced stochastic optimization methods, e.g., stochastic variance reduced gradient (SVRG, [10]) to obtain approximate matrix/vector product for power iterations. A sketch of our algorithm is provided in Algorithm 2.\nWe make the following observations from Algorithm 2.\nAlgorithm 2 Stochastic optimization of CCA.\nInput: Data matrices X ∈ Rdx×N , Y ∈ Rdy×N , regularization parameters (γx, γy). Initialize ũ0 ∈ Rdx , ṽ0 ∈ Rdy . u0 ← ũ0/ √ ũ⊤0 Σxxũ0, v0 ← ṽ0/ √ ṽ⊤0 Σyyṽ0\nfor t = 1, 2, . . . , T do Solve the least squares problem\nmin u\nft(u) := 1\n2N\n∥ ∥u⊤X− v⊤t−1Y ∥ ∥ 2 + γx 2 ‖u‖2\nwith SVRG and output approximate solution ũt satisfying ft(ũt) ≤ minu ft(u) + ǫ. Solve the least squares problem\nmin v\ngt(v) := 1\n2N\n∥ ∥v⊤Y − u⊤t−1X ∥ ∥ 2 + γy 2 ‖v‖2\nwith SVRG and output approximate solution ṽt satisfying gt(ṽt) ≤ minv gt(v) + ǫ. ut ← ũt/ √ ũ⊤t Σxxũt vt ← ṽt/ √ ṽ⊤t Σyyṽt\nend for Output: (uT ,vT ) is the approximate solution to CCA.\n• Connection to previous work At step t, if we optimize ft(u) and gt(v) crudely by a single batch gradient descent step from the initialization (ũt−1, ṽt−1), we obtain the following update rule (assuming γx = γy = 0):\nũt ← ũt−1 − 2sX(X⊤ũt−1 −Y⊤vt−1)/N, ut ← ũt/ √ ũ⊤t Σxxũt\nṽt ← ṽt−1 − 2sY(Y⊤ṽt−1 −X⊤ut−1)/N, vt ← ṽt/ √ ṽ⊤t Σyyṽt\nwhere s > 0 is the stepsize. This coincides with the AppGrad algorithm of [4, Algorithm 3], for which only local convergence is shown. Since the objectives ft(u) and gt(v) decouple over training samples, it is very efficient to apply stochastic gradient methods to them. This observation motivated the stochastic CCA algorithms of [4, 5]. We note however, no convergence guarantee was shown for these stochastic CCA algorithms.\n• Normalization Notice that at the end of each outer loop, Algorithm 2 implements exact normalization of the form ut ← ũt/ √\nũ⊤t Σxxũt to ensure the constraints, where the computation of ũ⊤t Σxxũt = 1 N (ũ⊤t X)(ũ ⊤ t X)\n⊤ + γx ‖ũt‖2 requires projecting the entire training set. However, this does not introduce extra computation because we already compute the projection of entire training set anyway to obtain the batch gradient for SVRG (at the beginning of time step t + 1). In contrast, the stochastic algorithms of [4, 5] use (possibly adaptive) estimate of covariance from a minibatch for normalization, in order to reduce the cost of (frequent) normalization after each stochastic gradient descent step, which further introduces noise to the updates.\n• Input sparsity For high dimensional sparse data (e.g., such as those used in natural language processing [11, 12]), an advantage of gradient based methods over the closedform solution is that it can take into account the input sparsity.\n• Warm start Observe that for different t, the least squares problems ft(u) only differ in their targets as vt changes over time. Since vt−1 is close to vt (especially near convergence), we may use ũt as initialization for minimizing ft+1(u) with an iterative algorithm.\n• Canonical ridge When (γx, γy) > 0, ft(u) and gt(v) are guaranteed to be strongly convex due to the ℓ2 regularizations, in which case SVRG converges linearly. It is therefore beneficial to use small nonzero regularization for improved computational efficiency, especially for high dimensional datasets where the inputs X and Y are approximately low-rank."
    }, {
      "heading" : "3.1 Analysis of inexact power iterations",
      "text" : "The key to showing the convergence of Algorithm 2 is the convergence of inexact power iterations, where the the least squares problems are solved only to necessary accuracy.\nFrom now on, we distinguish the iterates of our stochastic algorithm (Algorithm 2) from the iterates of the exact power iterations (Algorithm 1) and denote the latter with asterisks, i.e., ũ∗t and ṽ ∗ t for the unnormalized iterates and u∗t and v ∗ t for the normalized iterates. We denote the exact optimum of ft(u) and gt(v) by ūt and v̄t respectively.\nThe follow lemma bounds the distance between the iterates of inexact and exact power iterations. Lemma 3.1. Assume that Algorithm 1 and Algorithm 2 start with the same initialization, i.e., ũ0 = ũ∗0 and ṽ0 = ṽ ∗ 0.\nThen, for t ≥ 0, the unnormalized iterates of Algorithm 2 satisfy max ( ∥ ∥ ∥ Σ 1 2 xxũt −Σ 1 2 xxũ ∗ t ∥ ∥ ∥ , ∥ ∥ ∥ Σ 1 2 yyṽt −Σ 1 2 yyṽ ∗ t ∥ ∥ ∥ )\n≤ St, where\nSt := √ 2ǫ\n{\nt if ρ1 = 1 1−ρt1 1−ρ1 if ρ1 < 1 .\nFurthermore, for t ≥ 1, the normalized iterates of Algorithm 2 satisfy max (∥ ∥ ∥ Σ 1 2 xxut −Σ 1 2 xxu ∗ t ∥ ∥ ∥ , ∥ ∥ ∥ Σ 1 2 yyvt −Σ 1 2 yyv ∗ t ∥ ∥ ∥ )\n≤ 2St νt ,\nwhere νt := max ( ρr, ρ1 min ( u⊤t−1Σxxu ∗, v⊤t−1Σyyv ∗ )) .\nBased on Lemma 3.1, we show the following convergence property of Algorithm 2.\nTheorem 3.2 (Convergence of Algorithm 2). Let T = ⌈ ρ 2 1\nρ2 1 −ρ2 2\nlog (\n2 µη\n)\n⌉, and set\nǫ =\n{\nη2ν2T 128T 2 if ρ1 = 1 η2(1−ρ1)\n2ν2T 128(1−ρT\n1 )2\nif ρ1 < 1 .\nin Algorithm 2 where ν2T ≥ max ( ρ2r, ρ 2 1(1− η/2) ) . Then, for t > T , we have\nmin ( (u⊤t Σxxu ∗)2, (v⊤t Σyyv ∗)2 ) ≥ 1− η, and u⊤t Σxyvt ≥ ρ1(1 − 2η).\nProof. We prove the theorem by relating the iterates of inexact power iterations to those of exact power iterations.\nAssume the same initialization as in Lemma 3.1. First observe that\n(u⊤t Σxxu ∗)2 =\n(\n(u∗t ) ⊤ Σxxu ∗ + (ut − u∗t )⊤ Σxxu∗ )2\n≥ (\n(u∗t ) ⊤ Σxxu\n∗ )2 + 2 (\n(u∗t ) ⊤ Σxxu\n∗ )(\n(ut − u∗t ) ⊤ Σxxu\n∗ )\n≥ (\n(u∗t ) ⊤ Σxxu\n∗ )2 − 2 ∣ ∣ ∣\n∣\n(\nΣ 1 2 xx (ut − u∗t )\n)⊤ (\nΣ 1 2 xxu ∗ )\n∣ ∣ ∣ ∣\n≥ (\n(u∗t ) ⊤ Σxxu\n∗ )2 − 2 ∥ ∥ ∥ Σ 1 2 xxut −Σ 1 2 xxu ∗ t ∥ ∥ ∥ (5)\nwhere we have used the fact that ∥ ∥ ∥ Σ 1 2 xxut ∥ ∥ ∥ = ∥ ∥ ∥ Σ 1 2 xxu ∗ t ∥ ∥ ∥ = ∥ ∥ ∥ Σ 1 2 xxu ∗ ∥ ∥ ∥ = 1 and the Cauchy-Schwartz\ninequality in the last two steps.\nApplying Theorem 2.1 with T = ⌈ ρ 2 1\nρ2 1 −ρ2 2\nlog (\n2 µη\n) ⌉, we have that (\n(u∗t ) ⊤ Σxxu\n∗ )2\n≥ 1− η/2 for t ≥ T . On the other hand, in view of Lemma 3.1, we have for the specified ǫ value in Algorithm 2 that ∥ ∥ ∥ Σ 1 2 xxut −Σ 1 2 xxu ∗ t ∥ ∥ ∥ ≤ η/4. Plugging these two bounds into (5) gives the desired bound.\nThe proof for vt is completely analogous.\nAlgorithm 3 SVRG for minu f(u) := 1N ∑N i=1\n(\n1 2\n∣ ∣u⊤xi − v⊤yi ∣ ∣ 2 + γx2 ‖u‖\n2 )\n.\nInput: Stepsize s. Initialize u(0) ∈ Rdx . for j = 1, 2, . . . ,M do w0 ← u(j−1) Evaluate the batch gradient ∇f(w0) = X(X⊤w0 −Y⊤v)/N + γxw0 for t = 1, 2, . . . ,m do\nRandomly pick it from {1, . . . , N} wt ← wt−1 − s ( (xitx ⊤ it + γxI)(wt−1 −w0) +∇f(w0) )\nend for u(j) ← wt for randomly chosen t ∈ {1, . . . ,m}.\nend for Output: u(M) is the approximate solution."
    }, {
      "heading" : "3.2 Stochastic optimization of regularized least squares",
      "text" : "We now discuss the inner loop of our algorithm, which approximately solves subproblems of the form (4). As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates. All of these algorithms can be readily applied to the regularized least squares problem (4), and we choose SVRG because it is memory efficient and easy to implement.\nWe give the sketch of SVRG for (4) in Algorithm 3. Note that f(u) = ∑N i=1 f i(u) where each component f i(u) = 12 ∣ ∣u⊤xi − v⊤yi ∣ ∣ 2 + γx2 ‖u‖\n2 is ‖xi‖2-smooth, and f(u) is σmin(Σxx)-strongly convex where σmin(Σxx) ≥ γx is the minimum eigenvalue of Σxx. We quote the convergence rate of SVRG below from [10].\nLemma 3.3. Fix ǫ > 0. The SVRG algorithm detailed in Algorithm 3 finds a vector ũ satisfying E[f(ũ)]−minu f(u) ≤ ǫ2 in total time\nO (\ndx (N + κx) log\n(\n1\nǫ\n))\nwhere κx = maxi‖xi‖\n2\nσmin(Σxx) .\nRemarks We can also apply the recently developed accelerations techniques for first order optimization methods [16, 17]. These techniques can help replace the κx term in the time complexity with √ Nκx, which is advantageous when κx ≫ N .\nWe obtain the total time complexity of our algorithm, Õ ( d (N + κ) ρ21\nρ2 1 −ρ2 2\n)\nas stated in Theo-\nrem 1.1, by combining the time complexity of inexact power iterations (Theorem 3.2) and the time complexity of SVRG (Lemma 3.3). For comparison, the local convergence of batch AppGrad ([4, Theorem 2.1]) can be translated into a total time complexity of Õ ( dNκ ρ21\nρ2 1 −ρ2 2\n)\n, where N and the\ncondition number3 are multiplied together as AppGrad essentially applies batch gradient descent to the least squares problems. Within our framework, we can use accelerated gradient descent instead and reduce the dependence on condition number to √ κ [18].\n2The expectation is taken over random sampling of component functions. 3The condition number of least squares for batch gradient descent is defined differently as κ =\nmax\n(\nσmax(Σxx) σmin(Σxx) , σmax(Σyy) σmin(Σyy)\n)\n."
    }, {
      "heading" : "3.3 Extension to multi-dimensional projections",
      "text" : "In practice, we may be interested in projections of more than one dimensions. The CCA objective for extracting L-dimensional projections is\nmax U∈Rdx×L,V∈Rdy×L\ntr ( U⊤ΣxyV )\ns.t. U⊤ΣxxU = V⊤ΣyyV = I.\nTo adapt Algorithm 2 to this problem, one option is to extract the dimensions sequentially and remove the explained correlation from Σxy each time we extract a new dimension [19]. A simpler approach is to extract the L dimensions simultaneously using (approximate) orthogonal iterations (an extension of power iterations to multiple dimensions, [9]). In this case, our subproblems become multi-dimensional regressions and our normalization steps are of the form Ut ← Ũt(Ũ⊤t ΣxxŨt)− 1 2 ; the same normalization is used by [4, 5]. Such normalization involves the eigenvalue decomposition of a L × L matrix and can be solved exactly as we typically look for low dimensional projections. Our analysis for L = 1 can be extended to this scenario and the convergence rate of the algorithm now depends on the singular value gap between ρL and ρL+1."
    }, {
      "heading" : "4 Related work",
      "text" : "Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30]. But as pointed out by [23], the CCA objective is more challenging due to the whitening constraints. Our algorithm is inspired by the stochastic PCA algorithm of [28] which transforms the nonconvex PCA objective into a small number of well-conditioned regularized least squares problems (solved by SVRG) through shifting and inverting the covariance matrix and running power iterations on the transformed matrix. For CCA, the alternating least squares formulation (Algorithm 1) already reduces to solving convex regularized least squares problems.\n[31] propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints. However, the goal of their algorithm is anomaly detection for streaming data with a varying distribution, rather than to optimize the CCA objective on a given dataset. Similar to our algorithm, the stochastic CCA algorithms of [4, 5] are motivated by the alternating least squares formulation. [6] proposed a stochastic algorithm based on the Lagrangian formulation of the CCA objective. It is important to note that none of these online/stochastic algorithms have rigorous global convergence guarantee."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section we demonstrate our algorithm, denoted by ALS-VR (Alternating Least Squares with Variance Reduction), on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34]. These datasets have also been used by [4, 5] for demonstrating their stochastic CCA algorithms. We give the description and size of these datasets in Table 1. We extract L = 5 dimensional projections, and for the approximate solution obtained after each pass over the data, we evaluate the canonical correlation between the projections and compare it with that of the exact solution by SVD.\nWe compare ALS-VR with batch AppGrad, and its stochastic version s-AppGrad [4] with both gradient and normalization steps estimated with minibatchs of 200 samples.4 We do not compare the algorithm of [5] because the main difference between it and AppGrad is that it uses adaptive\n4The authors of [4] suggest that the minibatch size shall be at least the same magnitude as L.\nγx = γy = 10 −5 γx = γy = 10 −4 γx = γy = 10 −3 γx = γy = 10 −2\nρ2 k −ρ2 k+1\n.\nestimates of the covariance matrices for projections, which become unnecessary with large enough minibatches. For ALS-VR, at every outer loop we apply SVRG to solve the least squares problems ft(u) and gt(v) with M = 4 epochs, and each epoch includes a batch gradient evaluation and m = 2N stochastic gradient steps. For each dataset, we also vary the regularization parameters rx = ry over {10−5, 10−4, 10−3, 10−2} and larger regularization leads to better conditioned least squares problems. We set the SGD step size according to the smoothness for each method, i.e., 1/σmax(Σxx) for AppGrad/s-AppGrad, and 1/maxi ‖xi‖2 for SVRG in ALS-VR. The results for each dataset and different regularizations are shown in Figure 1. ALS-VR achieves accurate solution more quickly than AppGrad and s-AppGrad in all settings."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We have proposed a globally convergent stochastic algorithm for CCA, whose objective is nonconvex and does not decouple over training samples. Our algorithm makes use of the alternating least squares/power iterations formulation of CCA, and solves the least squares problems approximately with state-of-the-art stochastic gradient descent methods. The overall time complexity of our algorithm significantly improve upon previous work both theoretically and empirically.\nOne straightforward application of our algorithm is low-rank approximate kernel CCA [35] using random Fourier features [36, 37]. The algorithm of [35] first map original inputs into high dimensional random feature space and then perform linear CCA on top. We can solve the resulting high dimensional CCA problem with our stochastic algorithm, as done by [6, 38]. Another future direction is to extend the general inexact power iterations approach to other spectral dimension reduction algorithms (e.g., linear discriminant analysis is equivalent to CCA with proper representation for the class labels [39])."
    }, {
      "heading" : "A Proof of Theorem 2.1",
      "text" : "Proof. It is easy to see that by the end of the first iteration of Algorithm 1, ψ̃1 andψ1 lie in the span of {bi}ri=1, while φ̃1 and φ1 lie in the span of {ai}ri=1. And therefore they remain in these spaces for all t ≥ 1. Let us first focus on φt. For t ≥ 2, we observe that\nφt = Tψt−1 /\n∥ ∥ ∥ φ̃t ∥ ∥ ∥ = TT⊤φt−2 / (∥ ∥ ∥ φ̃t ∥ ∥ ∥ · ∥ ∥ ∥ ψ̃t−1 ∥ ∥ ∥ ) . (6)\nSince ∥ ∥φt−2 ∥ ∥ = ‖φt‖ = 1, it is equivalent to using the following updates:\nφt ← TT⊤φt−2, φt ← φt/ ‖φt‖ . (7)\nThis indicates that, Algorithm 1 runs the standard power iterations on TT⊤ to generate the {φt}t≥1 sequence for every two steps.\n(i) For t = 2, 4, . . . , we have φt = (TT⊤)\nt 2φ0\n∥ ∥ ∥ (TT⊤) t 2φ0 ∥ ∥ ∥\n. Let M = TT⊤, whose nonzero eigenvalues are\nρ21 ≥ ρ22 ≥ · · · ≥ ρ2r > 0, with corresponding eigenvectors a1, . . . , ar. Then, for i = 1, . . . , r,\n(a⊤i φt) 2 =\n(\na⊤i M t 2φ0\n)2\n∥ ∥ ∥ M t 2φ0 ∥ ∥ ∥\n2 =\n(\na⊤i M t 2φ0\n)2\nφ⊤0 M tφ0\n=\n(\nρtia ⊤ i φ0\n)2\n∑r j=1 ρ 2t j (a ⊤ j φ0)\n2 =\n( a⊤i φ0 )2\n∑r j=1\n(\nρ2 j ρ2 i\n)t\n(a⊤j φ0) 2\n≤ ( a⊤i φ0 )2\n(\nρ2 1 ρ2 i\n)t\n(a⊤1 φ0) 2\n=\n( a⊤i φ0 )2 (a⊤1 φ0) 2 ( ρ2i ρ21 )t = ( a⊤i φ0 )2 (a⊤1 φ0) 2 ( 1− ρ 2 1 − ρ2i ρ21 )t\n≤ ( a⊤i φ0 )2\n(a⊤1 φ0) 2 exp\n(\n−ρ 2 1 − ρ2i ρ21 t\n)\n. (8)\n(ii) For t = 1, 3, . . . , we have φt = (TT⊤)\nt−1 2 Tψ0\n∥ ∥ ∥ ∥ (TT⊤) t−1 2 Tψ0 ∥ ∥ ∥ ∥\n. Let N = T⊤T, whose nonzero eigenvalues\nare ρ21 ≥ ρ22 ≥ · · · ≥ ρ2r > 0, with corresponding eigenvectors b1, . . . ,br. Then, for i = 1, . . . , r,\n(a⊤i φt) 2 =\n(\na⊤i (TT ⊤) t−1 2 Tψ0\n)2\n∥ ∥ ∥ (TT⊤) t−1 2 Tψ0 ∥ ∥ ∥\n2 =\n(\n(T⊤ai) ⊤N t−1 2 ψ0\n)2\nψ⊤0 N tψ0\n=\n(\nρtib ⊤ i ψ0\n)2\n∑r j=1 ρ 2t j (b ⊤ j ψ0) 2\n≤ ( b⊤i ψ0 )2\n(b⊤1 ψ0) 2 exp\n(\n−ρ 2 1 − ρ2i ρ21 t\n)\n.\nGiven δ ∈ (0, 1), define S(δ) = {i : ρ2i > (1− δ)ρ21}. For δ1, δ2 ∈ (0, 1), define\nT (δ1, δ2) := ⌈ 1\nδ1 log\n(\n1\nµδ2\n)\n⌉. (9)\nFor all i 6∈ S(δ1), when t > T (δ1, δ2), it holds that (a⊤i φt)2 ≤ δ2(a⊤i φ0)2 if t is even, and (a⊤i φt) 2 ≤ δ2(b⊤i ψ0)2 if t is odd. In both cases, we have ∑ i∈S(δ1) (a⊤i φt) 2 ≥ 1− δ2.\nWhen there exists a postive singular value gap, i.e., ρ1 − ρ2 > 0, set δ1 = (ρ21 − ρ22)/ρ21 and thus S(δ1) = 1. Futhermore, set δ2 = η and we obtain (a⊤1 φt) 2 ≥ 1− η.\nThe proof for ψt is completely analogous. To obtain the bound on the objective, we have\nu⊤t Σxyvt = φ ⊤ t Tψt = ρ1(φ ⊤ t a1)(ψ ⊤ t b1) +\nr ∑\ni=2\nρi(φ ⊤ t ai)(ψ ⊤ t bi)\n≥ ρ1(φ⊤t a1)(ψ⊤t b1)− ρ1 r ∑\ni=2\n∣ ∣ ∣ φ⊤t ai ∣ ∣ ∣ ∣ ∣ ∣ ψ⊤t bi ∣ ∣ ∣\n≥ ρ1(1− η)− ρ1\n√ √ √ √ r ∑\ni=2\n(\nφ⊤t ai\n)2\n√ √ √ √ r ∑\ni=2\n(\nψ⊤t bi\n)2\n≥ ρ1(1− η)− ρ1η = ρ1(1− 2η),\nwhere we have used the Cauchy-Schwartz inequality in the second inequality."
    }, {
      "heading" : "B Proof of Lemma 3.1",
      "text" : "Proof. We focus on the {ũt}t≥0 and {ut}t≥0 sequences below; the proof for {ṽt}t≥0 and {vt}t≥0 is completely analogous.\n(i) We prove the bound for unnormalized iterates recursively. First, the case for t = 0 holds trivially. For t ≥ 1, we can bound the error of the unnormalized iterates using the exact solution to ft(u):\n∥ ∥ ∥ Σ 1 2 xxũt −Σ 1 2 xxũ ∗ t ∥ ∥ ∥ ≤ ∥ ∥ ∥ Σ 1 2 xxũt −Σ 1 2 xxūt ∥ ∥ ∥ + ∥ ∥ ∥ Σ 1 2 xxūt −Σ 1 2 xxũ ∗ t ∥ ∥ ∥ . (10)\nFor the first term of (10), notice ft(u) is a quadratic function with minimum achieved at ūt = Σ−1xxΣxyvt−1. For the approximate solution ũt, we have\nft(ũt)− ft(ūt) = 1\n2 (ũt − ūt)⊤Σxx(ũt − ūt) =\n1\n2\n∥ ∥ ∥ Σ 1 2 xxũt −Σ 1 2 xxūt ∥ ∥ ∥\n2\n≤ ǫ.\nIt then follows that ∥ ∥ ∥ Σ 1 2 xxũt −Σ 1 2 xxūt ∥ ∥ ∥ ≤ √ 2ǫ.\nThe second term of (10) is concerned with the error due to inexact target in the least squares problem ft(u) as vt−1 is different from v∗t−1. We can bound it as\n∥ ∥ ∥ Σ 1 2 xxūt −Σ 1 2 xxũ ∗ t ∥ ∥ ∥ = ∥ ∥ ∥ Σ 1 2 xxΣ −1 xxΣxyvt−1 −Σ 1 2 xxΣ −1 xxΣxyv ∗ t−1 ∥ ∥ ∥\n= ∥ ∥\n∥\n(\nΣ − 1\n2 xx ΣxyΣ\n− 1 2 yy\n)(\nΣ 1 2 yy(vt−1 − v∗t−1)\n)∥\n∥ ∥\n≤ ‖T‖ ∥ ∥ ∥ Σ 1 2 yy(vt−1 − v∗t−1) ∥ ∥ ∥\n≤ ρ1St−1.\nNow combine the two terms depending on the value of ρ1. If ρ1 = 1, we have\n∥ ∥ ∥ Σ 1 2 xxũt −Σ 1 2 xxũ ∗ t ∥ ∥ ∥ ≤ √ 2ǫ+ √ 2ǫ(t− 1) = √ 2ǫt.\nOtherwise, we have ρ1 < 1 and\n∥ ∥ ∥ Σ 1 2 xxũt −Σ 1 2 xxũ ∗ t ∥ ∥ ∥ ≤ √ 2ǫ+ √ 2ǫρ1 1− ρt−11 1− ρ1 = √ 2ǫ 1− ρt1 1− ρ1 .\n(ii) We now prove the bound for normalized iterates. In view of the update rule of our algorithm and the triangle inequality, we have\n∥ ∥ ∥ Σ 1 2 xxut −Σ 1 2 xxu ∗ t ∥ ∥ ∥ ≤\n∥ ∥ ∥ ∥ ∥ ∥ Σ 1 2 xxũt ∥ ∥\n∥ Σ 1 2 xxũt\n∥ ∥ ∥\n− Σ 1 2 xxũt ∥\n∥ ∥ Σ 1 2 xxũ ∗ t\n∥ ∥ ∥\n∥ ∥ ∥ ∥ ∥ ∥ + ∥ ∥ ∥ ∥ ∥ ∥ Σ 1 2 xxũt ∥ ∥\n∥ Σ 1 2 xxũ ∗ t\n∥ ∥ ∥\n− Σ 1 2 xxũ ∗ t ∥\n∥ ∥ Σ 1 2 xxũ ∗ t\n∥ ∥ ∥\n∥ ∥ ∥ ∥ ∥ ∥\n= ∥ ∥ ∥ Σ 1 2 xxũt ∥ ∥ ∥\n∣ ∣ ∣ ∣ ∣ ∣ 1 ∥ ∥\n∥ Σ 1 2 xxũt\n∥ ∥ ∥\n− 1∥ ∥\n∥ Σ 1 2 xxũ ∗ t\n∥ ∥ ∥\n∣ ∣ ∣ ∣ ∣ ∣ + 1 ∥ ∥\n∥ Σ 1 2 xxũ ∗ t\n∥ ∥ ∥\n∥ ∥ ∥ Σ 1 2 xxũt −Σ 1 2 xxũ ∗ t ∥ ∥ ∥\n= 1 ∥\n∥ ∥ Σ 1 2 xxũ ∗ t\n∥ ∥ ∥\n∣ ∣ ∣ ∥ ∥ ∥ Σ 1 2 xxũ ∗ t ∥ ∥ ∥ − ∥ ∥ ∥ Σ 1 2 xxũt ∥ ∥ ∥ ∣ ∣ ∣ + 1 ∥\n∥ ∥ Σ 1 2 xxũ ∗ t\n∥ ∥ ∥\n∥ ∥ ∥ Σ 1 2 xxũt −Σ 1 2 xxũ ∗ t ∥ ∥ ∥\n≤ 2∥ ∥\n∥ Σ 1 2 xxũ ∗ t\n∥ ∥ ∥\n∥ ∥ ∥ Σ 1 2 xxũt −Σ 1 2 xxũ ∗ t ∥ ∥ ∥ = 2St ∥\n∥ ∥ Σ 1 2 xxũ ∗ t\n∥ ∥ ∥\n.\nIt remains to bound ∥ ∥ ∥ Σ 1 2 xxũ ∗ t ∥ ∥ ∥ from below. Note that\nΣ 1 2 xxũ ∗ t = Σ 1 2 xxΣ −1 xxΣxyũ ∗ t =\n(\nΣ − 1\n2 xx ΣxyΣ\n− 1 2 yy\n)(\nΣ 1 2 yyv ∗ t−1\n) = T ( Σ 1 2 yyv ∗ t−1 ) .\nAlso, Σ 1 2 yyv ∗ t−1 corresponds to ψt−1 of Algorithm 1 which has unit length and lies in the span of {b1, . . . ,br}, i.e., ψt−1 = (ψ⊤t−1b1) · b1 + · · ·+ (ψ⊤t−1br) · br. As a result,\nTψt−1 = ρ1(ψ ⊤ t−1b1) · a1 + · · ·+ ρr(ψ⊤t−1br) · ar.\nSince {a1, . . . , ar} are orthonormal, we have ∥\n∥ ∥ Σ 1 2 xxũ ∗ t\n∥ ∥ ∥ = ∥ ∥Tψt−1 ∥ ∥ ≥ ρr.\nNote this bound can be pessimistic for large t. For large enough t, we expect the ψt−1 to be close to ψ, and have\n∥ ∥ ∥ Σ 1 2 xxũ ∗ t ∥ ∥ ∥ = ∥ ∥Tψt−1 ∥ ∥ ≥ ρ1 ∣ ∣ ∣ ψ⊤t−1b1 ∣ ∣ ∣ ≈ ρ1.\nThen the lemma follows."
    } ],
    "references" : [ {
      "title" : "Relations between two sets of variates",
      "author" : [ "Harold Hotelling" ],
      "venue" : "Biometrika, 28(3/4):321–377,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1936
    }, {
      "title" : "Canonical ridge and econometrics of joint production",
      "author" : [ "H.D. Vinod" ],
      "venue" : "Journal of Econometrics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1976
    }, {
      "title" : "On the regularization of canonical correlation analysis",
      "author" : [ "Tijl De Bie", "Bart De Moor" ],
      "venue" : "www.esat.kuleuven.ac.be/sista-cosic-docarch/,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Finding linear structure in large datasets with scalable canonical correlation analysis",
      "author" : [ "Zhuang Ma", "Yichao Lu", "Dean Foster" ],
      "venue" : "In Proc. of the 32st Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Stochastic optimization for deep CCA via nonlinear orthogonal iterations",
      "author" : [ "Weiran Wang", "Raman Arora", "Nati Srebro", "Karen Livescu" ],
      "venue" : "In 53nd Annual Allerton Conference on Communication, Control and Computing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Scale up nonlinear component analysis with doubly stochastic gradients",
      "author" : [ "Bo Xie", "Yingyu Liang", "Le Song" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Linear Algebra for Signal Processing, volume 69 of The IMA Volumes in Mathematics and its Applications, chapter The Canonical Correlations of Matrix Pairs and their Numerical Computation, pages 27–49",
      "author" : [ "Gene H. Golub", "Hongyuan Zha" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1995
    }, {
      "title" : "Large scale canonical correlation analysis with iterative least squares",
      "author" : [ "Yichao Lu", "Dean P. Foster" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Multi-view learning of word embeddings via CCA",
      "author" : [ "Paramveer Dhillon", "Dean Foster", "Lyle Ungar" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Using CCA to improve CCA: A new spectral method for estimating vector models of words",
      "author" : [ "Paramveer Dhillon", "Jordan Rodu", "Dean Foster", "Lyle Ungar" ],
      "venue" : "In Proc. of the 29th Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis Bach" ],
      "venue" : "Technical Report HAL 00860051,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Un-regularizing: Approximate proximal point and faster stochastic algorithms for empirical risk minimization",
      "author" : [ "Roy Frostig", "Rong Ge", "Sham Kakade", "Aaron Sidford" ],
      "venue" : "In Proc. of the 32st Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "A universal catalyst for first-order optimization",
      "author" : [ "Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Introductory Lectures on Convex Optimization",
      "author" : [ "Y. Nesterov" ],
      "venue" : "A Basic Course. Number 87 in Applied Optimization. Springer-Verlag,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis",
      "author" : [ "Daniela M. Witten", "Robert Tibshirani", "Trevor Hastie" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Krasulina. A method of stochastic approximation for the determination of the least eigenvalue of a symmetric matrix",
      "author" : [ "P. T" ],
      "venue" : "USSR Computational Mathematics and Mathematical Physics,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1969
    }, {
      "title" : "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix",
      "author" : [ "Erkki Oja", "Juha Karhunen" ],
      "venue" : "J. Math. Anal. Appl.,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1985
    }, {
      "title" : "Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension",
      "author" : [ "Manfred K. Warmuth", "Dima Kuzmin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "Stochastic optimization for PCA and PLS",
      "author" : [ "Raman Arora", "Andy Cotter", "Karen Livescu", "Nati Srebro" ],
      "venue" : "In 50th Annual Allerton Conference on Communication,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Stochastic optimization of PCA with capped MSG",
      "author" : [ "Raman Arora", "Andy Cotter", "Nati Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Memory limited, streaming PCA",
      "author" : [ "Ioannis Mitliagkas", "Constantine Caramanis", "Prateek Jain" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "The fast convergence of incremental PCA",
      "author" : [ "Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    }, {
      "title" : "A stochastic PCA and SVD algorithm with an exponential convergence rate",
      "author" : [ "Ohad Shamir" ],
      "venue" : "In Proc. of the 32st Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Fast and simple pca via convex optimization",
      "author" : [ "Dan Garber", "Elad Hazan" ],
      "venue" : "[math.OC],",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation",
      "author" : [ "Chi Jin", "Sham M. Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford" ],
      "venue" : "[cs.DS],",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Principal component projection without principal component analysis",
      "author" : [ "Roy Frostig", "Cameron Musco", "Christopher Musco", "Aaron Sidford" ],
      "venue" : "[cs.DS],",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Adaptive canonical correlation analysis based on matrix manifolds",
      "author" : [ "Florian Yger", "Maxime Berar", "Gilles Gasso", "Alain Rakotomamonjy" ],
      "venue" : "In Proc. of the 29th Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2012
    }, {
      "title" : "The challenge problem for automated detection of 101 semantic concepts in multimedia",
      "author" : [ "Cees G.M. Snoek", "Marcel Worring", "Jan C. van Gemert", "Jan-Mark Geusebroek", "Arnold W.M. Smeulders" ],
      "venue" : "In Proceedings of the 14th ACM international conference on Multimedia,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2006
    }, {
      "title" : "X-Ray Microbeam Speech Production",
      "author" : [ "John R. Westbury" ],
      "venue" : "Database User’s Handbook Version",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1994
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proc. IEEE,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1998
    }, {
      "title" : "Randomized nonlinear component analysis",
      "author" : [ "David Lopez-Paz", "Suvrit Sra", "Alex Smola", "Zoubin Ghahramani", "Bernhard Schoelkopf" ],
      "venue" : "In Proc. of the 31st Int. Conf. Machine Learning (ICML",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2014
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "Ali Rahimi", "Ben Recht" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2008
    }, {
      "title" : "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning",
      "author" : [ "Ali Rahimi", "Benjamin Recht" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2009
    }, {
      "title" : "Large-scale approximate kernel canonical correlation analysis",
      "author" : [ "Weiran Wang", "Karen Livescu" ],
      "venue" : "In Proc. of the 4nd Int. Conf. Learning Representations (ICLR 2016),",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction Canonical correlation analysis (CCA, [1]) and its extensions are ubiquitous techniques in scientific research areas for revealing the common sources of variability in multiple views of the same phenomenon.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "uΣxxu = vΣyyv = 1 where Σxy = 1 NXY ⊤ is the cross-covariance matrix, Σxx = 1 NXX +γxI and Σyy = 1 NYY + γyI are the auto-covariance matrices, and (γx, γy) ≥ 0 are regularization parameters [2, 3].",
      "startOffset" : 190,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "uΣxxu = vΣyyv = 1 where Σxy = 1 NXY ⊤ is the cross-covariance matrix, Σxx = 1 NXX +γxI and Σyy = 1 NYY + γyI are the auto-covariance matrices, and (γx, γy) ≥ 0 are regularization parameters [2, 3].",
      "startOffset" : 190,
      "endOffset" : 196
    }, {
      "referenceID" : 3,
      "context" : "There have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [4, 5, 6], but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.",
      "startOffset" : 96,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "There have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [4, 5, 6], but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.",
      "startOffset" : 96,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "There have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [4, 5, 6], but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.",
      "startOffset" : 96,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "A similar algorithm is recently used by [8] for large scale linear CCA with high dimensional sparse inputs, and by [4, 5] for motivating stochastic CCA algorithms.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "A similar algorithm is recently used by [8] for large scale linear CCA with high dimensional sparse inputs, and by [4, 5] for motivating stochastic CCA algorithms.",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "A similar algorithm is recently used by [8] for large scale linear CCA with high dimensional sparse inputs, and by [4, 5] for motivating stochastic CCA algorithms.",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : ", stochastic variance reduced gradient (SVRG, [10]) to obtain approximate matrix/vector product for power iterations.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "This observation motivated the stochastic CCA algorithms of [4, 5].",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "This observation motivated the stochastic CCA algorithms of [4, 5].",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "In contrast, the stochastic algorithms of [4, 5] use (possibly adaptive) estimate of covariance from a minibatch for normalization, in order to reduce the cost of (frequent) normalization after each stochastic gradient descent step, which further introduces noise to the updates.",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : "In contrast, the stochastic algorithms of [4, 5] use (possibly adaptive) estimate of covariance from a minibatch for normalization, in order to reduce the cost of (frequent) normalization after each stochastic gradient descent step, which further introduces noise to the updates.",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : ", such as those used in natural language processing [11, 12]), an advantage of gradient based methods over the closedform solution is that it can take into account the input sparsity.",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 10,
      "context" : ", such as those used in natural language processing [11, 12]), an advantage of gradient based methods over the closedform solution is that it can take into account the input sparsity.",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 12,
      "context" : "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "As mentioned earlier, the subproblems have the finite sum structure for which several stochastic optimization methods, such as SAG/SAGA [13, 14], SDCA [15], SVRG [10], provide linear convergence rates.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 8,
      "context" : "We quote the convergence rate of SVRG below from [10].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "Remarks We can also apply the recently developed accelerations techniques for first order optimization methods [16, 17].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 15,
      "context" : "Remarks We can also apply the recently developed accelerations techniques for first order optimization methods [16, 17].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "Within our framework, we can use accelerated gradient descent instead and reduce the dependence on condition number to √ κ [18].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "To adapt Algorithm 2 to this problem, one option is to extract the dimensions sequentially and remove the explained correlation from Σxy each time we extract a new dimension [19].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : "In this case, our subproblems become multi-dimensional regressions and our normalization steps are of the form Ut ← Ũt(Ũt ΣxxŨt) 1 2 ; the same normalization is used by [4, 5].",
      "startOffset" : 169,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "In this case, our subproblems become multi-dimensional regressions and our normalization steps are of the form Ut ← Ũt(Ũt ΣxxŨt) 1 2 ; the same normalization is used by [4, 5].",
      "startOffset" : 169,
      "endOffset" : 175
    }, {
      "referenceID" : 18,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 19,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 20,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 21,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 22,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 23,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 24,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 25,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 5,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 26,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 27,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 28,
      "context" : "4 Related work Recent years have witnessed continuous efforts to scale up fundamental methods such as principal component analysis (PCA) and partial least squares with stochastic/online updates [20, 21, 22, 23, 24, 25, 26, 27, 6, 28, 29, 30].",
      "startOffset" : 194,
      "endOffset" : 241
    }, {
      "referenceID" : 21,
      "context" : "But as pointed out by [23], the CCA objective is more challenging due to the whitening constraints.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 26,
      "context" : "Our algorithm is inspired by the stochastic PCA algorithm of [28] which transforms the nonconvex PCA objective into a small number of well-conditioned regularized least squares problems (solved by SVRG) through shifting and inverting the covariance matrix and running power iterations on the transformed matrix.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : "[31] propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "Similar to our algorithm, the stochastic CCA algorithms of [4, 5] are motivated by the alternating least squares formulation.",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "Similar to our algorithm, the stochastic CCA algorithms of [4, 5] are motivated by the alternating least squares formulation.",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "[6] proposed a stochastic algorithm based on the Lagrangian formulation of the CCA objective.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 30,
      "context" : "5 Experiments In this section we demonstrate our algorithm, denoted by ALS-VR (Alternating Least Squares with Variance Reduction), on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 31,
      "context" : "5 Experiments In this section we demonstrate our algorithm, denoted by ALS-VR (Alternating Least Squares with Variance Reduction), on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 32,
      "context" : "5 Experiments In this section we demonstrate our algorithm, denoted by ALS-VR (Alternating Least Squares with Variance Reduction), on three real-world datasets: Mediamill [32], JW11 (a subset of the XRMB corpus [33]), and MNIST [34].",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 3,
      "context" : "These datasets have also been used by [4, 5] for demonstrating their stochastic CCA algorithms.",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "These datasets have also been used by [4, 5] for demonstrating their stochastic CCA algorithms.",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "We compare ALS-VR with batch AppGrad, and its stochastic version s-AppGrad [4] with both gradient and normalization steps estimated with minibatchs of 200 samples.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "4 We do not compare the algorithm of [5] because the main difference between it and AppGrad is that it uses adaptive The authors of [4] suggest that the minibatch size shall be at least the same magnitude as L.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "4 We do not compare the algorithm of [5] because the main difference between it and AppGrad is that it uses adaptive The authors of [4] suggest that the minibatch size shall be at least the same magnitude as L.",
      "startOffset" : 132,
      "endOffset" : 135
    } ],
    "year" : 2017,
    "abstractText" : "We study the stochastic optimization of canonical correlation analysis (CCA), whose objective is nonconvex and does not decouple over training samples. Although several stochastic optimization algorithms have been recently proposed to solve this problem, no global convergence guarantee was provided by any of them. Based on the alternating least squares formulation of CCA, we propose a globally convergent stochastic algorithm, which solves the resulting least squares problems approximately to sufficient accuracy with state-of-the-art stochastic gradient methods for convex optimization. We provide the overall time complexity of our algorithm which significantly improves upon that of previous work. Experimental results demonstrate the superior performance of our algorithm.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}