{
  "name" : "1603.06160.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stochastic Variance Reduction for Nonconvex Optimization",
    "authors" : [ "Sashank J. Reddi", "Ahmed Hefny" ],
    "emails" : [ "sjakkamr@cs.cmu.edu", "ahefny@cs.cmu.edu", "suvrit@mit.edu", "bapoczos@cs.cmu.edu", "alex@smola.org" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We study nonconvex finite-sum problems of the form\nmin x∈Rd\nf(x) := 1\nn n∑ i=1 fi(x), (1)\nwhere neither f nor the individual fi (i ∈ [n]) are necessarily convex; just Lipschitz smooth (i.e., Lipschitz continuous gradients). We use Fn to denote all functions of the form (1). We optimize such functions in the Incremental First-order Oracle (IFO) framework (Agarwal & Bottou, 2014) defined below.\nDefinition 1. For f ∈ Fn, an IFO takes an index i ∈ [n] and a point x ∈ Rd, and returns the pair (fi(x),∇fi(x)).\nar X\niv :1\n60 3.\n06 16\n0v 1\n[ m\nIFO based complexity analysis was introduced to study lower bounds for finite-sum problems. Algorithms that use IFOs are favored in large-scale applications as they require only a small amount first-order information at each iteration. Two fundamental models in machine learning that profit from IFO algorithms are (i) empirical risk minimization, which typically uses convex finite-sum models; and (ii) deep learning, which uses nonconvex ones.\nThe prototypical IFO algorithm, stochastic gradient descent (Sgd)1 has witnessed tremendous progress in the recent years. By now a variety of accelerated, parallel, and faster converging versions are known. Among these, of particular importance are variance reduced (VR) stochastic methods (Schmidt et al., 2013; Johnson & Zhang, 2013; Defazio et al., 2014a), which have delivered exciting progress such as linear convergence rates (for strongly convex functions) as opposed to sublinear rates of ordinary Sgd (Robbins & Monro, 1951; Nemirovski et al., 2009). Similar (but not same) benefits of VR methods can also be seen in smooth convex functions. The Svrg algorithm of (Johnson & Zhang, 2013) is particularly attractive here because of its low storage requirement in comparison to the algorithms in (Schmidt et al., 2013; Defazio et al., 2014a).\nDespite the meteoric rise of VR methods, their analysis for general nonconvex problems is largely missing. Johnson & Zhang (2013) remark on convergence of Svrg when f ∈ Fn is locally strongly convex and provide compelling experimental results (Fig. 4 in (Johnson & Zhang, 2013)). However, problems encountered in practice are typically not even locally convex, let alone strongly convex. The current analysis of Svrg does not extend to nonconvex functions as it relies heavily on convexity for controlling the variance. Given the dominance of stochastic gradient methods in optimizing deep neural nets and other large nonconvex models, theoretical investigation of faster nonconvex stochastic methods is much needed.\nConvex VR methods are known to enjoy the faster convergence rate of GradientDescent but with a much weaker dependence on n, without compromising the rate like Sgd. However, it is not clear if these benefits carry beyond convex problems, prompting the central question of this paper:\nFor nonconvex functions in Fn, can one achieve convergence rates faster than both Sgd and GradientDescent using an IFO? If so, then how does the rate depend on n and on the number of iterations performed by the algorithm?\nPerhaps surprisingly, we provide an affirmative answer to this question by showing that a careful selection of parameters in Svrg leads to faster convergence than both Sgd and GradientDescent.\n1We use ‘incremental gradient’ and ‘stochastic gradient’ interchangeably, though we are only interested in finite-sum problems.\nTo our knowledge, ours is the first work to improve convergence rates of Sgd and GradientDescent for IFO-based nonconvex optimization.\nMain Contributions. We summarize our main contributions below and also list the key results in Table 1.\n• We analyze nonconvex stochastic variance reduced gradient (Svrg), and prove that it has faster rates of convergence than GradientDescent and ordinary Sgd. We show that Svrg is faster than GradientDescent by a factor of n1/3 (see Table 1). • We provide new theoretical insights into the interplay between step-size, iteration complexity and convergence of nonconvex Svrg (see Corollary 2). • For an interesting nonconvex subclass of Fn called gradient dominated functions (Polyak, 1963; Nesterov & Polyak, 2006), we propose a variant of Svrg that attains a global linear rate of convergence. We improve upon many prior results for this subclass of functions (see Section 3.1). To the best of our knowledge, ours is the first work that shows a stochastic method with linear convergence for gradient dominated functions. • We analyze mini-batch nonconvex Svrg and show that it provably benefits from mini-batching. Specifically, we show theoretical linear speedups in parallel settings for large mini-batch sizes. By using a mini-batch of size b (< n2/3), we show that mini-batch nonconvex Svrg is faster by a factor of b (Theorem 7). We are not aware of any prior work on mini-batch first-order stochastic methods that shows linear speedup in parallel settings for nonconvex optimization. • Our analysis yields as a byproduct a direct convergence analysis for Svrg for smooth convex functions (Section 4). • We examine a variant of Svrg (called Msvrg) that has faster rates than both GradientDescent and Sgd."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Convex. Bertsekas (2011) surveys several incremental gradient methods for convex problems. A key reference for stochastic convex optimization (for minEz[F (x, z)]) is (Nemirovski et al., 2009). Faster rates of convergence are attained for problems in Fn by VR methods, see e.g., (Defazio et al., 2014a; Johnson & Zhang, 2013; Schmidt et al., 2013; Konečný et al., 2015; Shalev-Shwartz & Zhang, 2013; Defazio et al., 2014b). Asynchronous VR frameworks are developed in (Reddi et al., 2015; Mania et al., 2015). Agarwal & Bottou (2014) study lower-bounds for convex finite-sum problems. ShalevShwartz (2015) prove linear convergence of stochastic dual coordinate ascent when the individual fi (i ∈ [n]) are nonconvex but f is strongly convex. They do not study the general nonconvex case. Moreover, even in their special setting our results improve upon theirs for the high condition number regime.\nNonconvex. Sgd dates at least to the seminal work (Robbins & Monro, 1951); and since then it has been developed in several directions (Poljak & Tsypkin, 1973; Ljung, 1977; Bottou, 1991; Kushner & Clark, 2012). In the (nonsmooth) finite-sum setting, Sra (2012) considers proximal splitting methods, and analyzes asymptotic convergence with nonvanishing gradient errors. Hong (2014) studies a distributed nonconvex incremental ADMM algorithm.\nThese works, however, only prove expected convergence to stationary points and often lack analysis of rates. The first nonasymptotic convergence rate analysis for Sgd is in (Ghadimi & Lan, 2013), who show that Sgd ensures ‖∇f‖2 ≤ in O(1/ 2) iterations. A similar rate for parallel and distributed Sgd was shown recently in (Lian et al., 2015). GradientDescent is known to ensure ‖∇f‖2 ≤ in O(1/ ) iterations (Nesterov, 2003, Chap. 1.2.3).\nThe first analysis of nonconvex Svrg seems to be due to Shamir (2014), who considers the special problem of computing a few leading eigenvectors (e.g., for PCA); see also the follow up work (Shamir, 2015). Finally, we note another interesting example, stochastic optimization of locally quasi-convex functions (Hazan et al., 2015), wherein actually a O(1/ 2) convergence in function value is shown."
    }, {
      "heading" : "2 Background & Problem Setup",
      "text" : "We say f is L-smooth if there is a constant L such that\n‖∇f(x)−∇f(y)‖ ≤ L‖x− y‖, ∀ x, y ∈ Rd.\nThroughout, we assume that the functions fi in (1) are L-smooth, so that ‖∇fi(x) − ∇fi(y)‖ ≤ L‖x− y‖ for all i ∈ [n]. Such an assumption is very common in the analysis of first-order methods. Here the Lipschitz constant L is assumed to be independent of n. A function f is called λ-strongly convex if there is λ ≥ 0 such that\nf(x) ≥ f(y) + 〈∇f(y), x− y〉+ λ2 ‖x− y‖ 2 ∀x, y ∈ Rd.\nThe quantity κ := L/λ is called the condition number of f , whenever f is L-smooth and λ-strongly convex. We say f is non-strongly convex when f is 0-strongly convex.\nWe also recall the class of gradient dominated functions (Polyak, 1963; Nesterov & Polyak, 2006), where a function f is called τ -gradient dominated if for any x ∈ Rd\nf(x)− f(x∗) ≤ τ‖∇f(x)‖2, (2)\nwhere x∗ is a global minimizer of f . Note that such a function f need not be convex; it is also easy to show that a λ-strongly convex function is 1/2λ-gradient dominated.\nWe analyze convergence rates for the above classes of functions. Following Nesterov (2003); Ghadimi & Lan (2013) we use ‖∇f(x)‖2 ≤ to judge when is iterate x approximately stationary. Contrast this with Sgd for convex f , where one uses [f(x) − f(x∗)] or ‖x − x∗‖2 as a convergence criterion. Unfortunately, such criteria cannot be used for nonconvex functions due to the hardness of the problem. While the quantities ‖∇f(x)‖2 and f(x)− f(x∗) or ‖x− x∗‖2 are not comparable in general (see (Ghadimi & Lan, 2013)), they are typically assumed to be of similar magnitude. Throughout our analysis, we do not assume n to be constant, and report dependence on it in our results. For our analysis, we need the following definition.\nDefinition 2. A point x is called -accurate if ‖∇f(x)‖2 ≤ . A stochastic iterative algorithm is said to achieve -accuracy in t iterations if E[‖∇f(xt)‖2] ≤ , where the expectation is over the stochasticity of the algorithm.\nWe introduce one more definition useful in the analysis of Sgd methods for bounding the variance.\nDefinition 3. We say f ∈ Fn has a σ-bounded gradient if ‖∇fi(x)‖ ≤ σ for all i ∈ [n] and x ∈ Rd."
    }, {
      "heading" : "2.1 Nonconvex SGD: Convergence Rate",
      "text" : "Stochastic gradient descent (Sgd) is one of the simplest algorithms for solving (1); Algorithm 1 lists its pseudocode. By using a uniformly randomly chosen (with replacement) index it from [n], Sgd\nAlgorithm 1 SGD\nInput: x0 ∈ Rd, Step-size sequence: {ηt > 0}T−1t=0 for t = 0 to T − 1 do\nUniformly randomly pick it from {1, . . . , n} xt+1 = xt − ηt∇fit(x)\nend for\nuses an unbiased estimate of the gradient at each iteration. Under appropriate conditions, Ghadimi & Lan (2013) establish convergence rate of Sgd to a stationary point of f . Their results include the following theorem.\nTheorem 1. Suppose f has σ-bounded gradient; let ηt = η = c/ √ T where c =\n√ 2(f(x0)−f(x∗))\nLσ2 , and\nx∗ is an optimal solution to (1). Then, the iterates of Algorithm 1 satisfy\nmin 0≤t≤T−1\nE[‖∇f(xt)‖2] ≤ √\n2(f(x0)− f(x∗))L T σ.\nFor completeness we present a proof in the appendix. Note that our choice of step size η requires knowing the total number of iterations T in advance. A more practical approach is to use a ηt ∝ 1/ √ t or 1/t. A bound on IFO calls made by Algorithm 1 follows as a corollary of Theorem 1.\nCorollary 1. Suppose function f has σ-bounded gradient, then the IFO complexity of Algorithm 1 to obtain an -accurate solution is O(1/ 2).\nAs seen in Theorem 1, Sgd has a convergence rate of O(1/ √ T). This rate is not improvable in general even when the function is (non-strongly) convex (Nemirovski & Yudin, 1983). This barrier is due to the variance introduced by the stochasticity of the gradients, and it is not clear if better rates can be obtained Sgd even for convex f ∈ Fn."
    }, {
      "heading" : "3 Nonconvex SVRG",
      "text" : "We now turn our focus to variance reduced methods. We use Svrg (Johnson & Zhang, 2013), an algorithm recently shown to be very effective for reducing variance in convex problems. As a result, it has gained considerable interest in both machine learning and optimization communities. We seek to understand its benefits for nonconvex optimization. For reference, Algorithm 2 presents Svrg’s pseudocode.\nObserve that Algorithm 2 operates in epochs. At the end of epoch s, a full gradient is calculated at the point x̃s, requiring n calls to the IFO. Within its inner loop Svrg performs m stochastic updates. The total number of IFO calls for each epoch is thus Θ(m+ n). For m = 1, the algorithm reduces to the classic GradientDescent algorithm. Suppose m is chosen to be O(n) (typically used in practice), then the total IFO calls per epoch is Θ(n). To enable a fair comparison with Sgd, we assume that the total number of inner iterations across all epochs in Algorithm 2 is T . Also note a simple but important implementation detail: as written, Algorithm 2 requires storing all the iterates xs+1t (0 ≤ t ≤ m). This storage can be avoided by keeping a running average with respect to the probability distribution {pi}mi=0.\nAlgorithm 2 attains linear convergence for strongly convex f (Johnson & Zhang, 2013); for nonstrongly convex functions, rates faster than Sgd can be shown by using an indirect perturbation argument—see e.g., (Konečný & Richtárik, 2013; Xiao & Zhang, 2014).\nWe first state an intermediate result for the iterates of nonconvex Svrg. To ease exposition, we define\nΓt = ( ηt −\nct+1ηt βt\n− η2tL− 2ct+1η2t ) , (3)\nfor some parameters ct+1 and βt (to be defined shortly). Our first main result is the following theorem that provides convergence rate of Algorithm 2.\nTheorem 2. Let f ∈ Fn. Let cm = 0, ηt = η > 0, βt = β > 0, and ct = ct+1(1+ηβ+2η2L2)+η2L3 such that Γt > 0 for 0 ≤ t ≤ m − 1. Define the quantity γn := mint Γt. Further, let pi = 0 for 0 ≤ i < m and pm = 1, and let T be a multiple of m. Then for the output xa of Algorithm 2 we have\nE[‖∇f(xa)‖2] ≤ f(x0)− f(x∗)\nTγn ,\nwhere x∗ is an optimal solution to (1).\nAlgorithm 2 SVRG ( x0, T,m, {pi}mi=0, {ηi} m−1 i=0 ) 1: Input: x̃0 = x0m = x\n0 ∈ Rd, epoch length m, step sizes {ηi > 0}m−1i=0 , S = dT/me, discrete probability distribution {pi}mi=0\n2: for s = 0 to S − 1 do 3: xs+10 = x s m 4: gs+1 = 1 n ∑n i=1∇fi(x̃\ns) 5: for t = 0 to m− 1 do 6: Uniformly randomly pick it from {1, . . . , n} 7: vs+1t = ∇fit(xs+1t )−∇fit(x̃s) + gs+1 8: xs+1t+1 = x s+1 t − ηtvs+1t\n9: end for 10: x̃s+1 = ∑m i=0 pix s+1 i 11: end for 12: Output: Iterate xa chosen uniformly random from {{xs+1t }m−1t=0 } S−1 s=0 .\nFurthermore, we can also show that nonconvex Svrg exhibits expected descent (in objective) after every epoch. The condition that T is a multiple of m is solely for convenience and can be removed by slight modification of the theorem statement. Note that the value γn above can depend on n. To obtain an explicit dependence, we simplify it using specific choices for η and β, as formalized below.\nTheorem 3. Suppose f ∈ Fn. Let η = µ0/(Lnα) (0 < µ0 < 1 and 0 < α ≤ 1), β = L/nα/2, m = bn3α/2/(3µ0)c and T is some multiple of m. Then there exists universal constants µ0, ν > 0 such that we have the following: γn ≥ νLnα in Theorem 2 and\nE[‖∇f(xa)‖2] ≤ Lnα[f(x0)− f(x∗)]\nTν ,\nwhere x∗ is an optimal solution to the problem in (1) and xa is the output of Algorithm 2.\nBy rewriting the above result in terms IFO calls, we get the following general corollary for nonconvex Svrg.\nCorollary 2. Suppose f ∈ Fn. Then the IFO complexity of Algorithm 2 (with parameters from Theorem 3) for achieving an -accurate solution is:\nIFO calls =\n{ O ( n+ (n1− α 2 / ) ) , if α < 2/3,\nO (n+ (nα/ )) , if α ≥ 2/3.\nCorollary 2 shows the interplay between step size and the IFO complexity. We observe that the number of IFO calls is minimized in Corollary 2 when α = 2/3. This gives rise to the following key results of the paper.\nCorollary 3. Suppose f ∈ Fn. Let η = µ1/(Ln2/3) (0 < µ1 < 1), β = L/n1/3, m = bn/(3µ1)c and T is some multiple of m. Then there exists universal constants µ1, ν1 > 0 such that we have the following: γn ≥ ν1Ln2/3 in Theorem 2 and\nE[‖∇f(xa)‖2] ≤ Ln2/3[f(x0)− f(x∗)]\nTν1 ,\nwhere x∗ is an optimal solution to the problem in (1) and xa is the output of Algorithm 2.\nCorollary 4. If f ∈ Fn, then the IFO complexity of Algorithm 2 (with parameters in Corollary 3) to obtain an -accurate solution is O(n+ (n2/3/ )).\nNote the rate of O(1/T ) in the above results, as opposed to slower O(1/ √ T ) rate of Sgd (The-\norem 1). For a more comprehensive comparison of the rates, refer to Section 6.\nAlgorithm 3 GD-SVRG ( x0,K, T,m, {pi}mi=0, {ηi} m−1 i=0 ) Input: x0 ∈ Rd, K, epoch length m, step sizes {ηi > 0}m−1i=0 , discrete probability distribution {pi} m i=0\nfor k = 0 to K do xk = SVRG(xk−1, T,m, {pi}mi=0, {ηi}m−1i=0 ) end for Output: xK"
    }, {
      "heading" : "3.1 Gradient Dominated Functions",
      "text" : "Before ending our discussion on convergence of nonconvex Svrg, we prove a linear convergence rate for the class of τ -gradient dominated functions (2). For ease of exposition, assume that τ > n1/3, a property analogous to the “high condition number regime” for strongly convex functions typical in machine learning. Note that gradient dominated functions can be nonconvex.\nTheorem 4. Suppose f is τ -gradient dominated where τ > n1/3. Then, the iterates of Algorithm 3 with T = d2Lτn2/3/ν1e, m = bn/(3µ1)c, ηt = µ1/(Ln2/3) for all 0 ≤ t ≤ m − 1 and pm = 1 and pi = 0 for all 0 ≤ i < m satisfy\nE[‖∇f(xk)‖2] ≤ 2−k[‖∇f(x0)‖2].\nHere µ1 and ν1 are the constants used in Corollary 3.\nIn fact, for τ -gradient dominated functions we can prove a stronger result of global linear convergence.\nTheorem 5. If f is τ -gradient dominated (τ > n1/3), then with T = d2Lτn2/3/ν1e, m = bn/(3µ1)c, ηt = µ1/(Ln\n2/3) for 0 ≤ t ≤ m − 1 and pm = 1 and pi = 0 for all 0 ≤ i < m, the iterates of Algorithm 3 satisfy\nE[f(xk)− f(x∗)] ≤ 2−k[f(x0)− f(x∗)].\nHere µ1, ν1 are as in Corollary 3; x ∗ is an optimal solution.\nAn immediate consequence is the following.\nCorollary 5. If f is τ -gradient dominated, the IFO complexity of Algorithm 3 (with parameters from Theorem 4) to compute an -accurate solution is O((n+ τn2/3) log(1/ )).\nNote that GradientDescent can also achieve linear convergence rate for gradient dominated functions (Polyak, 1963). However, GradientDescent requires O(n + nτ log(1/ )) IFO calls to obtain an -accurate solution as opposed to O(n + n2/3τ log(1/ )) for Svrg. Similar (but not the same) gains can be seen for Svrg for strongly convex functions (Johnson & Zhang, 2013). Also notice that we did not assume anything except smoothness on the individual functions fi in the above results. In particular, the following corollary is also an immediate consequence.\nCorollary 6. If f is λ-strongly convex and the functions {fi}ni=1 are possibly nonconvex, then the number of IFO calls made by Algorithm 3 (with parameters from Theorem 4) to compute an -accurate solution is O((n+ n2/3κ) log(1/ )).\nRecall that here κ denotes the condition number L/λ for a λ-strongly convex function. Corollary 6 follows from Corollary 5 upon noting that λ-strongly convex function is 1/2λ-gradient dominated. Theorem 5 generalizes the linear convergence result in (Johnson & Zhang, 2013) since it allows nonconvex fi. Observe that Corollary 6 also applies when fi is strongly convex for all i ∈ [n], though in this case a more refined result can be proved (Johnson & Zhang, 2013).\nFinally, we note that our result also improves on a recent result on Sdca in the setting of Corollary 6 when the condition number κ is reasonably large – a case that typically arises in machine\nlearning. More precisely, for l2-regularized empirical loss minimization, Shalev-Shwartz (2015) show that Sdca requires O((n + κ2) log(1/ ) iterations when the fi’s are possibly nonconvex but their sum f is strongly convex. In comparison, we show that Algorithm 3 requires O((n+n2/3κ) log(1/ )) iterations, which is an improvement over Sdca when κ > n2/3."
    }, {
      "heading" : "4 Convex Case",
      "text" : "In the previous section, we showed nonconvex Svrg converges to a stationary point at the rate O(n2/3/T ). A natural question is whether this rate can be improved if we assume convexity? We provide an affirmative answer. For non-strongly convex functions, this yields a direct analysis (i.e., not based on strongly convex perturbations) for Svrg. While we state our results in terms of stationarity gap ‖∇f(x)‖2 for the ease of comparison, our analysis also provides rates with respect to the optimality gap [f(x)− f(x∗)] (see the proof of Theorem 6 in the appendix). Theorem 6. If fi is convex for all i ∈ [n], pi = 1/m for 0 ≤ i ≤ m − 1, and pm = 0, then for Algorithm 2, we have\nE[‖∇f(xa)‖2] ≤ L‖x0 − x∗‖2 + 4mL2η2[f(x0)− f(x∗)]\nTη(1− 4Lη) ,\nwhere x∗ is optimal for (1) and xa is the output of Algorithm 2.\nWe now state corollaries of this theorem that explicitly show the dependence on n in the convergence rates. Corollary 7. If m = n and η = 1/(8L √ n) in Theorem 6, then we have the following bound:\nE[‖∇f(xa)‖2] ≤ L √ n(16L‖x0 − x∗‖2 + [f(x0)− f(x∗)])\nT ,\nwhere x∗ is optimal for (1) and xa is the output of Algorithm 2.\nThe above result uses a step size that depends on n. For the convex case, we can also use step sizes independent of n. The following corollary states the associated result.\nCorollary 8. If m = n and η = 1/(8L) in Theorem 6, then we have the following bound:\nE[‖∇f(xa)‖2] ≤ L(16L‖x0 − x∗‖2 + n[f(x0)− f(x∗)])\nT ,\nwhere x∗ is optimal for (1) and xa is the output of Algorithm 2.\nWe can rewrite these corollaries in terms of IFO complexity to get the following corollaries.\nCorollary 9. If fi is convex for all i ∈ [n], then the IFO complexity of Algorithm 2 (with parameters from Corollary 7) to compute an -accurate solution is O(n+ ( √ n/ )).\nCorollary 10. If fi is convex for all i ∈ [n], then the IFO complexity of Algorithm 2 (with parameters from Corollary 8) to compute -accurate solution is O(n/ ).\nThese results follow from Corollary 7 and Corollary 8 and noting that for m = O(n) the total IFO calls made by Algorithm 2 is O(n). It is instructive to quantitatively compare Corollary 9 and Corollary10. With a step size independent of n, the convergence rate of Svrg has a dependence that is in the order of n (Corollary 8). But this dependence can be reduced to √ n by either carefully selecting a step size that diminishes with n (Corollary 7) or by using a good initial point x0 obtained by, say, running O(n) iterations of Sgd.\nWe emphasize that the convergence rate for convex case can be improved significantly by slightly modifying the algorithm (either by adding an appropriate strongly convex perturbation (Xiao & Zhang, 2014) or by using a choice of m that changes with epoch (Zhu & Yuan, 2015)). However, it is not clear if these strategies provide any theoretical gains for the general nonconvex case."
    }, {
      "heading" : "5 Mini-batch Nonconvex SVRG",
      "text" : "In this section, we study the mini-batch version of Algorithm 2. Mini-batching is a popular strategy, especially in multicore and distributed settings as it greatly helps one exploit parallelism and reduce the communication costs. The pseudocode for mini-batch nonconvex Svrg (Algorithm 4) is provided in the supplement due to lack of space. The key difference between the mini-batch Svrg and Algorithm 2 lies in lines 6 to 8. To use mini-batches we replace line 6 with sampling (with replacement) a mini-batch It ⊂ [n] of size b; lines 7 to 8 are replaced with the following updates:\nus+1t = 1 |It| ∑ it∈It ( ∇fit(xs+1t )−∇fit(x̃s) ) + gs+1, xs+1t+1 = x s+1 t − ηtus+1t\nWhen b = 1, this reduces to Algorithm 2. Mini-batch is typically used to reduce the variance of the stochastic gradient and increase the parallelism. Lemma 4 (in Section G of the appendix) shows the reduction in the variance of stochastic gradients with mini-batch size b. Using this lemma, one can derive the mini-batch equivalents of Lemma 1, Theorem 2 and Theorem 3. However, for the sake of brevity, we directly state the following main result for mini-batch Svrg.\nTheorem 7. Let γn denote the following quantity:\nγn := min 0≤t≤m−1\n( η − ct+1ηβ − η 2L− 2ct+1η2 ) .\nwhere cm = 0, ct = ct+1(1 + ηβ + 2η 2L2/b) + η 2 tL 3 /b for 0 ≤ t ≤ m − 1. Suppose η = µ2b/(Ln2/3) (0 < µ2 < 1), β = L/n 1/3, m = bn/(3bµ2)c and T is some multiple of m. Then for the mini-batch version of Algorithm 2 with mini-batch size b < n2/3, there exists universal constants µ2, ν2 > 0 such that we have the following: γn ≥ ν2bLn2/3 and\nE[‖∇f(xa)‖2] ≤ Ln2/3[f(x0)− f(x∗)]\nbTν2 ,\nwhere x∗ is optimal for (1).\nIt is important to compare this result with mini-batched Sgd. For a batch size of b, Sgd obtains a rate of O(1/ √ bT ) (Dekel et al., 2012) (obtainable by a simple modification of Theorem 1). Specifically, Sgd has a 1/ √ b dependence on the batch size. In contrast, Theorem 7 shows that Svrg has a much better dependence of 1/b on the batch size. Hence, compared to Sgd, Svrg allows more efficient mini-batching. More formally, in terms of IFO queries we have the following result.\nCorollary 11. If f ∈ Fn, then the IFO complexity of the mini-batch version of Algorithm 2 (with parameters from Theorem 7 and mini-batch size b < n2/3) to obtain an -accurate solution is O(n+ (n2/3/ )).\nCorollary 11 shows an interesting property of mini-batch Svrg. First, note that b IFO calls are required for calculating the gradient on a mini-batch of size b. Hence, Svrg does not gain on IFO complexity by using mini-batches. However, if the b gradients are calculated in parallel, then this leads to a theoretical linear speedup in multicore and distributed settings. In contrast, Sgd does not yield an efficient mini-batch strategy as it requires O(b1/2/ 2) IFO calls for achieving an -accurate solution (Li et al., 2014). Thus, the performance of Sgd degrades with mini-batching."
    }, {
      "heading" : "6 Comparison of the convergence rates",
      "text" : "In this section, we give a comprehensive comparison of results obtained in this paper. In particular, we compare key aspects of the convergence rates for Sgd, GradientDescent, and Svrg. The comparison is based on IFO complexity to achieve an -accurate solution.\nDependence on n: The number of IFO calls of Svrg and GradientDescent depend explicitly on n. In contrast, the number of oracle calls of Sgd is independent of n (Theorem 1). However, this comes at the expense of worse dependence on . The number of IFO calls in GradientDescent is proportional to n. But for Svrg this dependence reduces to n1/2 for convex (Corollary 7) and n2/3 for nonconvex (Corollary 3) problems. Whether this difference in dependence on n is due to nonconvexity or just an artifact of our analysis is an interesting open problem.\nDependence on : The dependence on (or alternatively T ) follows from the convergence rates of the algorithms. Sgd is seen to depend as O(1/ 2) on , regardless of convexity or nonconvexity. In contrast, for both convex and nonconvex settings, Svrg and GradientDescent converge as O(1/ ). Furthermore, for gradient dominated functions, Svrg and GradientDescent have global linear convergence. This speedup in convergence over Sgd is especially significant when medium to high accuracy solutions are required (i.e., is small).\nAssumptions used in analysis: It is important to understand the assumptions used in deriving the convergence rates. All algorithms assume Lipschitz continuous gradients. However, Sgd requires two additional subtle but important assumptions: σ-bounded gradients and advance knowledge of T (since its step sizes depend on T ). On the other hand, both Svrg and GradientDescent do not require these assumptions, and thus, are more flexible.\nStep size / learning rates: It is valuable to compare the step sizes used by the algorithms. The step sizes of Sgd shrink as the number of iterations T increases—an undesirable property. On the other hand, the step sizes of Svrg and GradientDescent are independent of T . Hence, both these algorithms can be executed with a fixed step size. However, Svrg uses step sizes that depend on n (see Corollary 3 and Corollary 7). A step size independent of n can be used for Svrg for convex f , albeit at cost of worse dependence on n (Corollary 8). GradientDescent does not have this issue as its step size is independent of both n and T .\nDependence on initial point and mini-batch: Svrg is more sensitive to the initial point in comparison to Sgd. This can be seen by comparing Corollary 3 (of Svrg) to Theorem 1 (of Sgd). Hence, it is important to use a good initial point for Svrg. Similarly, a good mini-batch can be beneficial to Svrg. Moreover, mini-batches not only provides parallelism but also good theoretical guarantees (see Theorem 7). In contrast, the performance gain in Sgd with mini-batches is not very pronounced (see Section 5)."
    }, {
      "heading" : "7 Best of two worlds",
      "text" : "We have seen in the previous section that Svrg combines the benefits of both GradientDescent and Sgd. We now show that these benefits of Svrg can be made more pronounced by an appropriate step size under additional assumptions. In this case, the IFO complexity of Svrg is lower than those of Sgd and GradientDescent. This variant of Svrg (Msvrg) chooses a step size based on the total number of iterations T (or alternatively ). For our discussion below, we assume that T > n. Theorem 8. Let f ∈ Fn have σ-bounded gradients. Let ηt = η = max{c/ √ T , µ1/(Ln2/3)} (µ1 is the\nuniversal constant from Corollary 3), m = bn/(3µ1)c, and c = √ f(x0)−f(x∗) 2Lσ2 . Further, let T be a multiple of m, pm = 1, and pi = 0 for 0 ≤ i < m. Then, the output xa of Algorithm 2 satisfies\nE[‖∇f(xa)‖2]\n≤ ν̄min { 2 √ 2(f(x0)− f(x∗))L\nT σ, Ln2/3[f(x0)− f(x∗)]\nTν1\n} ,\nwhere ν̄ is a universal constant, ν1 is the universal constant from Corollary 3 and x ∗ is an optimal solution to (1).\nCorollary 12. If f ∈ Fn has σ-bounded gradients, the IFO complexity of Algorithm 2 (with parameters from Theorem 8) to achieve an -accurate solution is O(min{1/ 2, n2/3/ }).\nAn almost identical reasoning can be applied when f is convex to get the bounds specified in Table 1. Hence, we omit the details and directly state the following result.\nCorollary 13. Suppose fi is convex for i ∈ [n] and f has σ-bounded gradients, then the IFO complexity of Algorithm 2 (with step size η = max{1/(L √ T ), 1/(8L √ n)}, m = n and pi = 1/m for 0 ≤ i ≤ m− 1 and pm = 0) to achieve an -accurate solution is O(min{1/ 2, √ n/ }).\nMsvrg has a convergence rate faster than those of both Sgd and Svrg, though this benefit is not without cost. Msvrg, in contrast to Svrg, uses the additional assumption of σ-bounded gradients. Furthermore, its step size is not fixed since it depends on the number of iterations T . While it is often difficult in practice to compute the step size of Msvrg (Theorem 8), it is typical to try multiple step sizes and choose the one with the best results."
    }, {
      "heading" : "8 Experiments",
      "text" : "We present our empirical results in this section. For our experiments, we study the problem of multiclass classification using neural networks. This is a typical nonconvex problem encountered in machine learning.\nExperimental Setup. We train neural networks with one fully-connected hidden layer of 100 nodes and 10 softmax output nodes. We use `2-regularization for training. We use CIFAR-10\n2, MNIST3, and STL-104 datasets for our experiments. These datasets are standard in the neural networks literature. The `2 regularization is 1e-3 for CIFAR-10 and MNIST, and 1e-2 for STL-10. The features in the datasets are normalized to the interval [0, 1]. All the datasets come with a predefined split into training and test datasets.\nWe compare Sgd (the de-facto algorithm for training neural networks) against nonconvex Svrg. The step size (or learning rate) is critical for Sgd. We set the learning rate of Sgd using the popular t−inverse schedule ηt = η0(1 + η′bt/nc)−1, where η0 and η′ are chosen so that Sgd gives the best\n2www.cs.toronto.edu/~kriz/cifar.html 3http://yann.lecun.com/exdb/mnist/ 4https://cs.stanford.edu/~acoates/stl10/\nperformance on the training loss. In our experiments, we also use η′ = 0; this results in a fixed step size for Sgd. For Svrg, we use a fixed step size as suggested by our analysis. Again, the step size is chosen so that Svrg gives the best performance on the training loss.\nInitialization & mini-batching. Initialization is critical to training of neural networks. We use the normalized initialization in (Glorot & Bengio, 2010) where parameters are chosen uniformly from [− √ 6/(ni + no), √ 6/(ni + no)] where ni and no are the number of input and output layers of the neural network, respectively. For Svrg, we use n iterations of Sgd for CIFAR-10 and MINST and 2n iterations of Sgd for STL-10 before running Algorithm 2. Such initialization is standard for variance reduced schemes even for convex problems (Johnson & Zhang, 2013; Schmidt et al., 2013). As noted earlier in Section 6, Svrg is more sensitive than Sgd to the initial point, so such an initialization is typically helpful. We use mini-batches of size 10 in our experiments. Sgd with mini-batches is common in training neural networks. Note that mini-batch training is especially beneficial for Svrg, as shown by our analysis in Section 5. Along the lines of theoretical analysis provided by Theorem 7, we use an epoch size m = n/10 in our experiments.\nResults. We report objective function (training loss), test error (classification error on the test set), and ‖∇f(xt)‖2 (convergence criterion throughout our analysis) for the datasets. For all the algorithms, we compare these criteria against the number of effective passes through the data, i.e., IFO calls divided by n. This includes the cost of calculating the full gradient at the end of each epoch of Svrg. Due to the Sgd initialization in Svrg and mini-batching, the Svrg plots start from x-axis value of 10 for CIFAR-10 and MNIST and 20 for STL-10. Figure 1 shows the results for our experiment. It can be seen that the ‖∇f(xt)‖2 for Svrg is lower compared to Sgd, suggesting faster convergence to a stationary point. Furthermore, the training loss is also lower compared to Sgd in all the datasets. Notably, the test error for CIFAR-10 is lower for Svrg, indicating better generalization; we did not notice substantial difference in test error for MNIST and STL-10 (see Section H in the appendix). Overall, these results on a network with one hidden layer are promising; it will be interesting to study Svrg for deep neural networks in the future."
    }, {
      "heading" : "9 Discussion",
      "text" : "In this paper, we examined a VR scheme for nonconvex optimization. We showed that by employing VR in stochastic methods, one can perform better than both Sgd and GradientDescent in the context of nonconvex optimization. When the function f in (1) is gradient dominated, we proposed a variant of Svrg that has linear convergence to the global minimum. Our analysis shows that Svrg has a number of interesting properties that include convergence with fixed step size, descent property after every epoch; a property that need not hold for Sgd. We also showed that Svrg, in contrast to Sgd, enjoys efficient mini-batching, attaining speedups linear in the size of the minibatches in parallel settings. Our analysis also reveals that the initial point and use of mini-batches are important to Svrg.\nBefore concluding the paper, we would like to discuss the implications of our work and few caveats. One should exercise some caution while interpreting the results in the paper. All our theoretical results are based on the stationarity gap. In general, this does not necessarily translate to optimality gap or low training loss and test error. One criticism against VR schemes in nonconvex optimization is the general wisdom that variance in the stochastic gradients of Sgd can actually help it escape local minimum and saddle points. In fact, Ge et al. (2015) add additional noise to the stochastic gradient in order to escape saddle points. However, one can reap the benefit of VR schemes even in such scenarios. For example, one can envision an algorithm which uses Sgd as an exploration tool to obtain a good initial point and then uses a VR algorithm as an exploitation tool to quickly converge to a good local minimum. In either case, we believe variance reduction can be used as an important tool alongside other tools like momentum, adaptive learning rates for faster and better nonconvex optimization."
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "A Nonconvex SGD: Convergence Rate",
      "text" : ""
    }, {
      "heading" : "Proof of Theorem 1",
      "text" : "Theorem. Suppose f has σ-bounded gradient; let ηt = η = c/ √ T where c =\n√ 2(f(x0)−f(x∗))\nLσ2 , and\nx∗ is an optimal solution to (1). Then, the iterates of Algorithm 1 satisfy\nmin 0≤t≤T−1\nE[‖∇f(xt)‖2] ≤ √\n2(f(x0)−f(x∗))L T σ.\nProof. We include the proof here for completeness. Please refer to (Ghadimi & Lan, 2013) for a more general result.\nThe iterates of Algorithm 1 satisfy the following bound: E[f(xt+1)] ≤ E[f(xt) + 〈 ∇f(xt), xt+1 − xt 〉 + L2 ‖x t+1 − xt‖2] (4)\n≤ E[f(xt)]− ηtE[‖∇f(xt)‖2] + Lη 2 t 2 E[‖∇fit(x t)‖2] ≤ E[f(xt)]− ηtE[‖∇f(xt)‖2] + Lη 2 t 2 σ 2. (5)\nThe first inequality follows from Lipschitz continuity of ∇f . The second inequality follows from the update in Algorithm 1 and since Eit [∇fit(xt)] = ∇f(xt) (unbiasedness of the stochastic gradient). The last step uses our assumption on gradient boundedness. Rearranging Equation (5) we obtain\nE[‖∇f(xt)‖2] ≤ 1ηtE[f(x t)− f(xt+1)] + Lηt2 σ 2. (6)\nSumming Equation (6) from t = 0 to T − 1 and using that ηt is constant η we obtain\nmin t\nE[‖∇f(xt)‖2] ≤ 1T ∑T−1 t=0 E[‖f(xt)‖2]\n≤ 1TηE[f(x 0)− f(xT )] + Lη2 σ 2 ≤ 1Tη (f(x 0)− f(x∗)) + Lη2 σ 2\n≤ 1√ T ( 1 c ( f(x0)− f(x∗) ) + Lc2 σ 2 ) .\nThe first step holds because the minimum is less than the average. The second and third steps are obtained from Equation (6) and the fact that f(x∗) ≤ f(xT ), respectively. The final inequality follows upon using η = c/ √ T . By setting\nc =\n√ 2(f(x0)− f(x∗))\nLσ2\nin the above inequality, we get the desired result."
    }, {
      "heading" : "B Nonconvex SVRG",
      "text" : "In this section, we provide the proofs of the results for nonconvex Svrg. We first start with few useful lemmas and then proceed towards the main results.\nLemma 1. For ct, ct+1, βt > 0, suppose we have\nct = ct+1(1 + ηtβt + 2η 2 tL 2) + η2tL 3.\nLet ηt, βt and ct+1 be chosen such that Γt > 0 (in Equation (3)). The iterate x s+1 t in Algorithm 2 satisfy the bound:\nE[‖∇f(xs+1t )‖2] ≤ Rs+1t −Rs+1t+1\nΓt ,\nwhere Rs+1t := E[f(x s+1 t ) + ct‖xs+1t − x̃s‖2] for 0 ≤ s ≤ S − 1.\nProof. Since f is L-smooth we have\nE[f(xs+1t+1 )] ≤ E[f(x s+1 t ) + 〈∇f(xs+1t ), xs+1t+1 − x s+1 t 〉\n+ L2 ‖x s+1 t+1 − x s+1 t ‖2].\nUsing the Svrg update in Algorithm 2 and its unbiasedness, the right hand side above is further upper bounded by\nE[f(xs+1t )− ηt‖∇f(xs+1t )‖2 + Lη2t 2 ‖v s+1 t ‖2]. (7)\nConsider now the Lyapunov function\nRs+1t := E[f(x s+1 t ) + ct‖xs+1t − x̃s‖2].\nFor bounding it we will require the following:\nE[‖xs+1t+1 − x̃s‖2] = E[‖x s+1 t+1 − x s+1 t + x s+1 t − x̃s‖2]\n= E[‖xs+1t+1 − x s+1 t ‖2 + ‖xs+1t − x̃s‖2\n+ 2〈xs+1t+1 − x s+1 t , x s+1 t − x̃s〉]\n= E[η2t ‖vs+1t ‖2 + ‖xs+1t − x̃s‖2] − 2ηtE[〈∇f(xs+1t ), xs+1t − x̃s〉] ≤ E[η2t ‖vs+1t ‖2 + ‖xs+1t − x̃s‖2]\n+ 2ηtE [\n1 2βt ‖∇f(xs+1t )‖2 + 12βt‖x s+1 t − x̃s‖2\n] . (8)\nThe second equality follows from the unbiasedness of the update of Svrg. The last inequality follows from a simple application of Cauchy-Schwarz and Young’s inequality. Plugging Equation (7) and Equation (8) into Rs+1t+1 , we obtain the following bound:\nRs+1t+1 ≤ E[f(x s+1 t )− ηt‖∇f(xs+1t )‖2 + Lη2t 2 ‖v s+1 t ‖2]\n+ E[ct+1η2t ‖vs+1t ‖2 + ct+1‖xs+1t − x̃s‖2] + 2ct+1ηtE [\n1 2βt ‖∇f(xs+1t )‖2 + 12βt‖x s+1 t − x̃s‖2 ] ≤ E[f(xs+1t )− ( ηt − ct+1ηtβt ) ‖∇f(xs+1t )‖2\n+ ( Lη2t 2 + ct+1η 2 t ) E[‖vs+1t ‖2]\n+ (ct+1 + ct+1ηtβt)E [ ‖xs+1t − x̃s‖2 ] . (9)\nTo further bound this quantity, we use Lemma 3 to bound E[‖vs+1t ‖2], so that upon substituting it in Equation (9), we see that\nRs+1t+1 ≤ E[f(x s+1 t )] − ( ηt − ct+1ηtβt − η 2 tL− 2ct+1η2t ) E[‖∇f(xs+1t )‖2]\n+ [ ct+1 ( 1 + ηtβt + 2η 2 tL 2 ) + η2tL 3 ] E [ ‖xs+1t − x̃s‖2 ] ≤ Rs+1t − ( ηt − ct+1ηtβt − η 2 tL− 2ct+1η2t ) E[‖∇f(xs+1t )‖2].\nThe second inequality follows from the definition of ct and R s+1 t , thus concluding the proof."
    }, {
      "heading" : "Proof of Theorem 2",
      "text" : "Theorem. Let f ∈ Fn. Let cm = 0, ηt = η > 0, βt = β > 0, and ct = ct+1(1 + ηβ + 2η2L2) + η2L3 such that Γt > 0 for 0 ≤ t ≤ m − 1. Define the quantity γn := mint Γt. Further, let pi = 0 for\n0 ≤ i < m and pm = 1, and let T be a multiple of m. Then for the output xa of Algorithm 2 we have\nE[‖∇f(xa)‖2] ≤ f(x0)− f(x∗)\nTγn ,\nwhere x∗ is an optimal solution to (1).\nProof. Since ηt = η for t ∈ {0, . . . ,m− 1}, using Lemma 1 and telescoping the sum, we obtain∑m−1 t=0 E[‖∇f(xs+1t )‖2] ≤ Rs+10 −Rs+1m γn .\nThis inequality in turn implies that∑m−1 t=0 E[‖∇f(xs+1t )‖2] ≤ E[f(x̃s)− f(x̃s+1)] γn , (10)\nwhere we used that Rs+1m = E[f(xs+1m )] = E[f(x̃s+1)] (since cm = 0, pm = 1, and pi = 0 for i < m), and that Rs+10 = E[f(x̃s)] (since x s+1 0 = x̃\ns, as pm = 1 and pi = 0 for i < m). Now sum over all epochs to obtain\n1\nT S−1∑ s=0 m−1∑ t=0 E[‖∇f(xs+1t )‖2] ≤ f(x0)− f(x∗) Tγn . (11)\nThe above inequality used the fact that x̃0 = x0. Using the above inequality and the definition of xa in Algorithm 2, we obtain the desired result."
    }, {
      "heading" : "Proof of Theorem 3",
      "text" : "Theorem. Suppose f ∈ Fn. Let η = µ0/(Lnα) (0 < µ0 < 1 and 0 < α ≤ 1), β = L/nα/2, m = bn3α/2/(3µ0)c and T is some multiple of m. Then there exists universal constants µ0, ν > 0 such that we have the following: γn ≥ νLnα in Theorem 2 and\nE[‖∇f(xa)‖2] ≤ Lnα[f(x0)− f(x∗)]\nTν ,\nwhere x∗ is an optimal solution to the problem in (1) and xa is the output of Algorithm 2.\nProof. For our analysis, we will require an upper bound on c0. We observe that c0 = µ20L n2α (1+θ)m−1 θ where θ = 2η2L2 + ηβ. This is obtained using the relation ct = ct+1(1 + ηβ + 2η 2L2) + η2L3 and the fact that cm = 0. Using the specified values of β and η we have\nθ = 2η2L2 + ηβ = 2µ20 n2α + µ0 n3α/2 ≤ 3µ0 n3α/2 .\nThe above inequality follows since µ0 ≤ 1 and n ≥ 1. Using the above bound on θ, we get\nc0 = µ20L n2α (1 + θ)m − 1 θ = µ0L((1 + θ) m − 1) 2µ0 + nα/2\n≤ µ0L((1 +\n3µ0 n3α/2 )bn 3α/2/3µ0c − 1) 2µ0 + nα/2\n≤ n−α/2(µ0L(e− 1)), (12)\nwherein the second inequality follows upon noting that (1+ 1l ) l is increasing for l > 0 and liml→∞(1+ 1 l ) l = e (here e is the Euler’s number). Now we can lower bound γn, as\nγn = min t\n( η − ct+1ηβ − η 2L− 2ct+1η2 )\n≥ ( η − c0ηβ − η 2L− 2c0η2 ) ≥ ν Lnα ,\nwhere ν is a constant independent of n. The first inequality holds since ct decreases with t. The second inequality holds since (a) c0/β is upper bounded by a constant independent of n as c0/β ≤ µ0(e − 1) (follows from Equation (12)), (b) η2L ≤ µ0η and (c) 2c0η2 ≤ 2µ20(e − 1)η (follows from Equation (12)). By choosing µ0 (independent of n) appropriately, one can ensure that γn ≥ ν/(Lnα) for some universal constant ν. For example, choosing µ0 = 1/4, we have γn ≥ ν/(Lnα) with ν = 1/40. Substituting the above lower bound in Equation (11), we obtain the desired result."
    }, {
      "heading" : "Proof of Corollary 2",
      "text" : "Corollary. Suppose f ∈ Fn. Then the IFO complexity of Algorithm 2 (with parameters from Theorem 3) for achieving an -accurate solution is:\nIFO calls =\n{ O ( n+ (n1− α 2 / ) ) , if α < 2/3,\nO (n+ (nα/ )) , if α ≥ 2/3.\nProof. This result follows from Theorem 3 and the fact that m = bn3α/2/(3µ0)c. Suppose α < 2/3, then m = o(n). However, n IFO calls are invested in calculating the average gradient at the end of each epoch. That is, n IFO calls are required for m iterations of the algorithm. Using this\nrelationship, we get O ( n+ (n1− α 2 / ) ) in this case.\nOn the other hand, when α ≥ 2/3, the total number of IFO calls made by Algorithm 2 in each epoch is Ω(n) since m = bn3α/2/(3µ0)c. Hence, the oracle calls required for calculating the average gradient (per epoch) is of lower order, leading to O ( n+ (nα/ ) ) IFO calls."
    }, {
      "heading" : "C GD-SVRG",
      "text" : ""
    }, {
      "heading" : "Proof of Theorem 4",
      "text" : "Theorem. Suppose f is τ -gradient dominated where τ > n1/3. Then, the iterates of Algorithm 3 with T = d2Lτn2/3/ν1e, m = bn/(3µ1)c, ηt = µ1/(Ln2/3) for all 0 ≤ t ≤ m − 1 and pm = 1 and pi = 0 for all 0 ≤ i < m satisfy\nE[‖∇f(xk)‖2] ≤ 2−k[‖∇f(x0)‖2].\nHere µ1 and ν1 are the constants used in Corollary 3.\nProof. Corollary 3 shows that the iterates of Algorithm 3 satisfy\nE[‖∇f(xk)‖2] ≤ Ln 2/3E[f(xk−1)− f(x∗)]\nTν1 .\nSubstituting the specified value of T in the above inequality, we have\nE[‖∇f(xk)‖2] ≤ 1 2τ\n( E[f(xk−1)− f(x∗)] ) ≤ 12E[‖∇f(x k−1)‖2].\nThe second inequality follows from τ -gradient dominance of the function f ."
    }, {
      "heading" : "Proof of Theorem 5",
      "text" : "Theorem. If f is τ -gradient dominated (τ > n1/3), then with T = d2Lτn2/3/ν1e, m = bn/(3µ1)c, ηt = µ1/(Ln\n2/3) for 0 ≤ t ≤ m − 1 and pm = 1 and pi = 0 for all 0 ≤ i < m, the iterates of Algorithm 3 satisfy\nE[f(xk)− f(x∗)] ≤ 2−k[f(x0)− f(x∗)].\nHere µ1, ν1 are as in Corollary 3; x ∗ is an optimal solution.\nProof. The proof mimics that of Theorem 4; now we have the following condition on the iterates of Algorithm 3:\nE[‖∇f(xk)‖2] ≤ E[f(x k−1)− f(x∗)]\n2τ . (13)\nHowever, f is τ -gradient dominated, so E[‖∇f(xk)‖2] ≥ E[f(xk) − f(x∗)]/τ , which combined with Equation (13) concludes the proof."
    }, {
      "heading" : "D Convex SVRG: Convergence Rate",
      "text" : ""
    }, {
      "heading" : "Proof of Theorem 6",
      "text" : "Theorem. If fi is convex for all i ∈ [n], pi = 1/m for 0 ≤ i ≤ m − 1, and pm = 0, then for Algorithm 2, we have\nE[‖∇f(xa)‖2] ≤ L‖x0 − x∗‖2 + 4mL2η2[f(x0)− f(x∗)]\nTη(1− 4Lη) ,\nwhere x∗ is optimal for (1) and xa is the output of Algorithm 2.\nProof. Consider the following sequence of inequalities:\nE[‖xs+1t+1 − x∗‖2] = E[‖x s+1 t − ηvs+1t − x∗‖2]\n≤ E[‖xs+1t − x∗‖2] + η2E[‖vs+1t ‖2] − 2ηE[〈vs+1t , xs+1t − x∗〉] ≤ E[‖xs+1t − x∗‖2] + η2E[‖vs+1t ‖2] − 2ηE[f(xs+1t )− f(x∗)] ≤ E[‖xs+1t − x∗‖2]− 2η(1− 2Lη)E[f(xs+1t )− f(x∗)] + 4Lη2E[f(x̃s)− f(x∗)] = E[‖xs+1t − x∗‖2]− 2η(1− 4Lη)E[f(xs+1t )− f(x∗)] + 4Lη2E[f(x̃s)− f(x∗)]− 4Lη2E[f(xs+1t )− f(x∗)].\nThe second inequality uses unbiasedness of the Svrg update and convexity of f . The third inequality follows from Lemma 6. Defining the Lyapunov function\nP s := E[‖xsm − x∗‖2] + 4mLη2E[f(x̃s)− f(x∗)],\nand summing the above inequality over t, we get\n2η(1− 4Lη) m−1∑ t=0 E[f(xs+1t )− f(x∗)] ≤ P s − P s+1.\nAlgorithm 4 Mini-batch SVRG\n1: Input: x̃0 = x0m = x 0 ∈ Rd, epoch length m, step sizes {ηi > 0}m−1i=0 , S = dT/me, discrete probability\ndistribution {pi}mi=0, mini-batch size b 2: for s = 0 to S − 1 do 3: xs+10 = x s m 4: gs+1 = 1 n ∑n i=1∇fi(x̃\ns) 5: for t = 0 to m− 1 do 6: Choose a mini-batch (uniformly random with replacement) It ⊂ [n] of size b 7: us+1t = 1 b ∑ it∈It(∇fit(x s+1 t )−∇fit(x̃s)) + gs+1 8: xs+1t+1 = x s+1 t − ηtus+1t\n9: end for 10: x̃s+1 = ∑m i=0 pix s+1 i 11: end for 12: Output: Iterate xa chosen uniformly random from {{xs+1t }m−1t=0 } S−1 s=0 .\nThis due is to the fact that\nP s+1 = E[‖xs+1m − x∗‖2] + 4mLη2E[f(x̃s+1)− f(x∗)]\n= E[‖xs+1m − x∗‖2] + 4Lη2 m−1∑ t=0 E[f(xs+1t )− f(x∗)].\nThe above equality uses the fact that pm = 0 and pi = 1/m for 0 ≤ i < m. Summing over all epochs and telescoping we then obtain\nE[f(xa)− f(x∗)] ≤ P 0 ( 2Tη(1− 4Lη) )−1 .\nThe inequality also uses the definition of xa given in Alg 2. On this inequality we use Lemma 5, which yields\nE[‖∇f(xa)‖2] ≤ 2LE[f(xa)− f(x∗)]\n≤ L‖x 0 − x∗‖2 + 4mL2η2[f(x0)− f(x∗)]\nTη(1− 4Lη) .\nIt is easy to see that we can obtain convergence rates for E[f(xa) − f(x∗)] from the above reasoning. This leads to a direct analysis of Svrg for convex functions."
    }, {
      "heading" : "E Minibatch Nonconvex SVRG",
      "text" : ""
    }, {
      "heading" : "Proof of Theorem 7",
      "text" : "The proofs essentially follow along the lines of Lemma 1, Theorem 2 and Theorem 3 with the added complexity of mini-batch. We first prove few intermediate results before proceeding to the proof of Theorem 7.\nLemma 2. Suppose we have\nR s+1\nt := E[f(x s+1 t ) + ct‖xs+1t − x̃s‖2],\nct = ct+1(1 + ηtβt + 2η2tL 2\nb ) +\nη2tL 3\nb ,\nfor 0 ≤ s ≤ S − 1 and 0 ≤ t ≤ m− 1 and the parameters ηt, βt and ct+1 are chosen such that( ηt −\nct+1ηt βt\n− η2tL− 2ct+1η2t ) ≥ 0.\nThen the iterates xs+1t in the mini-batch version of Algorithm 2 i.e., Algorithm 4 with mini-batch size b satisfy the bound:\nE[‖∇f(xs+1t )‖2] ≤ R s+1 t −R s+1 t+1( ηt − ct+1ηtβt − η 2 tL− 2ct+1η2t ) , Proof. Using essentially the same argument as the proof of Lemma. 1 until Equation (9), we have\nR s+1 t+1 ≤ E[f(xs+1t )− ( ηt − ct+1ηtβt ) ‖∇f(xs+1t )‖2\n+ ( Lη2t 2 + ct+1η 2 t ) E[‖us+1t ‖2]\n+ (ct+1 + ct+1ηtβt)E [ ‖xs+1t − x̃s‖2 ] . (14)\nWe use Lemma 4 in order to bound E[‖us+1t ‖2] in the above inequality. Substituting it in Equation (14), we see that\nR s+1\nt+1 ≤ E[f(xs+1t )] − ( ηt − ct+1ηtβt − η 2 tL− 2ct+1η2t ) E[‖∇f(xs+1t )‖2]\n+ [ ct+1 ( 1 + ηtβt + 2η2tL 2\nb\n) + η2tL 3\nb\n] E [ ‖xs+1t − x̃s‖2 ] ≤ Rs+1t − ( ηt − ct+1ηtβt − η 2 tL− 2ct+1η2t ) E[‖∇f(xs+1t )‖2].\nThe second inequality follows from the definition of ct and R s+1 t , thus concluding the proof.\nOur intermediate key result is the following theorem that provides convergence rate of mini-batch Svrg.\nTheorem 9. Let γn denote the following quantity:\nγn := min 0≤t≤m−1\n( η − ct+1ηβ − η 2L− 2ct+1η2 ) .\nSuppose ηt = η and βt = β for all t ∈ {0, . . . ,m− 1}, cm = 0, ct = ct+1(1 + ηtβt + 2η 2 tL 2 b ) + η2tL 3\nb for t ∈ {0, . . . ,m− 1} and γn > 0. Further, let pm = 1 and pi = 0 for 0 ≤ i < m. Then for the output xa of mini-batch version of Algorithm 2 with mini-batch size b, we have\nE[‖∇f(xa)‖2] ≤ f(x0)− f(x∗)\nTγn ,\nwhere x∗ is an optimal solution to (1).\nProof. Since ηt = η for t ∈ {0, . . . ,m− 1}, using Lemma 2 and telescoping the sum, we obtain\n∑m−1 t=0 E[‖∇f(xs+1t )‖2] ≤ R s+1 0 −R s+1 m\nγn .\nThis inequality in turn implies that∑m−1 t=0 E[‖∇f(xs+1t )‖2] ≤ E[f(x̃s)− f(x̃s+1)]\nγn ,\nwhere we used that R s+1\nm = E[f(xs+1m )] = E[f(x̃s+1)] (since cm = 0, pm = 1, and pi = 0 for i < m), and that R s+1\n0 = E[f(x̃s)] (since x s+1 0 = x̃ s, as pm = 1 and pi = 0 for i < m). Now sum over all epochs and using the fact that x̃0 = x0, we get the desired result.\nWe now present the proof of Theorem 7 using the above results.\nTheorem. Let γn denote the following quantity:\nγn := min 0≤t≤m−1\n( η − ct+1ηβ − η 2L− 2ct+1η2 ) .\nwhere cm = 0, ct = ct+1(1 + ηβ + 2η 2L2/b) + η 2 tL 3 /b for 0 ≤ t ≤ m − 1. Suppose η = µ2b/(Ln2/3) (0 < µ2 < 1), β = L/n 1/3, m = bn/(3bµ2)c and T is some multiple of m. Then for the mini-batch version of Algorithm 2 with mini-batch size b < n2/3, there exists universal constants µ2, ν2 > 0 such that we have the following: γn ≥ ν2bLn2/3 and\nE[‖∇f(xa)‖2] ≤ Ln2/3[f(x0)− f(x∗)]\nbTν2 ,\nwhere x∗ is optimal for (1).\nProof of Theorem 7. We first observe that using the specified values of β and η we obtain\nθ := 2η2L2\nb + ηβ =\n2µ22b\nn4/3 + µ2b n ≤ 3µ2b n .\nThe above inequality follows since µ2 ≤ 1 and n ≥ 1. For our analysis, we will require the following bound on c0:\nc0 = µ22b 2L bn4/3 (1 + θ)m − 1 θ = µ2bL((1 + θ) m − 1) 2bµ2 + bn1/3\n≤ n−1/3(µ2L(e− 1)), (15)\nwherein the first equality holds due to the relation ct = ct+1(1 + ηtβt + 2η2tL 2 b ) + η2tL 3\nb , and the inequality follows upon again noting that (1 + 1/l)l is increasing for l > 0 and liml→∞(1 + 1 l ) l = e. Now we can lower bound γn, as\nγn = min t\n( η − ct+1ηβ − η 2L− 2ct+1η2 )\n≥ ( η − c0ηβ − η 2L− 2c0η2 ) ≥ bν2 Ln2/3 ,\nwhere ν2 is a constant independent of n. The first inequality holds since ct decreases with t. The second one holds since (a) c0/β is upper bounded by a constant independent of n as c0/β ≤ µ2(e−1) (due to Equation (15)), (b) η2L ≤ µ2η (as b < n2/3) and (c) 2c0η2 ≤ 2µ22(e − 1)η (again due to Equation (15) and the fact b < n2/3). By choosing an appropriately small constant µ2 (independent of n), one can ensure that γn ≥ bν2/(Ln2/3) for some universal constant ν2. For example, choosing µ2 = 1/4, we have γn ≥ bν2/(Ln2/3) with ν2 = 1/40. Substituting the above lower bound in Theorem 9, we get the desired result."
    }, {
      "heading" : "F MSVRG: Convergence Rate",
      "text" : ""
    }, {
      "heading" : "Proof of Theorem 8",
      "text" : "Theorem. Let f ∈ Fn have σ-bounded gradients. Let ηt = η = max{c/ √ T , µ1/(Ln2/3)} (µ1 is the universal constant from Corollary 3), m = bn/(3µ1)c, and c = √ f(x0)−f(x∗) 2Lσ2 . Further, let T be a multiple of m, pm = 1, and pi = 0 for 0 ≤ i < m. Then, the output xa of Algorithm 2 satisfies\nE[‖∇f(xa)‖2]\n≤ ν̄min { 2 √ 2(f(x0)− f(x∗))L\nT σ, Ln2/3[f(x0)− f(x∗)]\nTν1\n} ,\nwhere ν̄ is a universal constant, ν1 is the universal constant from Corollary 3 and x ∗ is an optimal solution to (1). Proof. First, we observe that the step size η is chosen to be max{c/ √ T , µ1/(Ln 2/3)} where\nc =\n√ f(x0)− f(x∗)\n2Lσ2 .\nSuppose η = µ1/(Ln 2/3), we obtain the convergence rate in Corollary 3. Now, lets consider the case\nwhere η = c/ √ T . In this case, we have the following bound:\nE[‖vs+1t ‖2] = E[‖∇fit(xs+1t )−∇fit(x̃s) +∇f(x̃s)‖2] ≤ 2 ( E[‖∇fit(xs+1t )‖2 + ‖∇fit(x̃s)−∇f(x̃s)‖2]\n) ≤ 2 ( E[‖∇fit(xs+1t )‖2 + ‖∇fit(x̃s)‖2]\n) ≤ 4σ2.\nThe first inequality follows from Lemma 7 with r = 2. The second inequality follows from (a) σ-bounded gradient property of f and (b) the fact that for a random variable ζ, E[‖ζ − E[ζ]‖2] ≤ E[‖ζ‖2]. The rest of the proof is along exactly the lines as in Theorem 1. This provides a convergence rate similar to Theorem 1. More specifically, using step size c/ √ T , we get\nE[‖f(xa)‖2] ≤ 2 √\n2(f(x0)− f(x∗))L T σ. (16)\nThe only thing that remains to be proved is that with the step size choice of max{c/ √ T , µ1/(Ln\n2/3)}, the minimum of two bounds hold. Consider the case c/ √ T > µ1/(Ln\n2/3). In this case, we have the following:\n2 √\n2(f(x0)−f(x∗))L T σ\nLn2/3[f(x0)−f(x∗)] Tν1\n= 2ν1σ\n√ 2LT\nLn2/3 √ f(x0)− f(x∗)\n≤ 2ν1/µ1 ≤ ν̄ := max {\n2ν1 µ1 , µ1 2ν1\n} ,\nwhere ν1 is the constant in Corollary 3. This inequality holds since c/ √ T > µ1/(Ln\n2/3). Rearranging the above inequality, we have\n2\n√ 2(f(x0)− f(x∗))L\nT σ ≤ ν̄Ln 2/3[f(x0)− f(x∗)] T\nin this case. Note that the left hand side of the above inequality is precisely the bound obtained by using step size c/ √ T (see Equation (16)). Similarly, when c/ √ T ≤ µ1/(Ln2/3), the inequality holds in the other direction. Using these two observations, we have the desired result."
    }, {
      "heading" : "G Key Lemmatta",
      "text" : "Lemma 3. For the intermediate iterates vs+1t computed by Algorithm 2, we have the following:\nE[‖vs+1t ‖2] ≤ 2E[‖∇f(xs+1t )‖2] + 2L2E[‖xs+1t − x̃s‖2].\nProof. The proof simply follows from the proof of Lemma 4 with It = {it}.\nWe now present a result to bound the variance of mini-batch Svrg.\nLemma 4. Let us+1t be computed by the mini-batch version of Algorithm 2 i.e., Algorithm 4 with mini-batch size b. Then,\nE[‖us+1t ‖2] ≤ 2E[‖∇f(xs+1t )‖2] + 2L 2 b E[‖x s+1 t − x̃s‖2].\nProof. For the ease of exposition, we use the following notation:\nζs+1t = 1 |It| ∑ it∈It ( ∇fit(xs+1t )−∇fit(x̃s) ) .\nUse convexity of ‖·‖2 and definition of us+1t to get\nE[‖us+1t ‖2] = E[‖ζs+1t +∇f(x̃s)‖2] = E[‖ζs+1t +∇f(x̃s)−∇f(xs+1t ) +∇f(xs+1t )‖2] ≤ 2E[‖∇f(xs+1t )‖2] + 2E[‖ζs+1t − E[ζs+1t ]‖2] ≤ 2E[‖∇f(xs+1t )‖2] + 2E[‖ζs+1t ‖2].\nThe first inequality follows from Lemma 7 (with r = 2) and the fact that E[ζs+1t ] = ∇f(xs+1t ) − ∇f(x̃s). The second inequality is obtained by noting that for a random variable ζ, E[‖ζ −E[ζ]‖2] ≤ E[‖ζ‖2]. Using Jensen’s inequality in the inequality above, we get\nE[‖us+1t ‖2]\n≤ 2E[‖∇f(xs+1t )‖2] + 2\nb E[‖∇fit(xs+1t )−∇fit(x̃s)‖2]\n≤ 2E[‖∇f(xs+1t )‖2] + 2L2\nb E[‖xs+1t − x̃s‖2].\nThe last inequality follows from L-smoothness of fit ."
    }, {
      "heading" : "H Experiments",
      "text" : "Figure 2 shows the remaining plots for MNIST and STL-10 datasets. As seen in the plots, there is no significant difference in the test error of Svrg and Sgd for these datasets."
    }, {
      "heading" : "I Other Lemmas",
      "text" : "We need Lemma 5 for our results in the convex case.\nLemma 5 (Johnson & Zhang (2013)). Let g : Rd → R be convex with L-Lipschitz continuous gradient. Then,\n‖∇g(x)−∇g(y)‖2 ≤ 2L[g(x)− g(y)− 〈∇g(y), x− y〉],\nfor all x, y ∈ Rd.\nProof. Consider h(x) := g(x)− g(y)− 〈∇g(y), x− y〉 for arbitrary y ∈ Rd. Observe that ∇h is also L-Lipschitz continuous. Note that h(x) ≥ 0 (since h(y) = 0 and ∇h(y) = 0, or alternatively since h defines a Bregman divergence), from which it follows that\n0 ≤ min ρ [h(x− ρ∇h(x))]\n≤ min ρ\n[h(x)− ρ‖∇h(x)‖2 + Lρ 2\n2 ‖∇h(x)‖ 2]\n= h(x)− 12L‖∇h(x)‖ 2.\nRewriting in terms of g we obtain the required result.\nLemma 6 bounds the variance of Svrg for the convex case. Please refer to (Johnson & Zhang, 2013) for more details.\nLemma 6 ((Johnson & Zhang, 2013)). Suppose fi is convex for all i ∈ [n]. For the updates in Algorithm 2 we have the following inequality:\nE[‖vs+1t ‖2] ≤ 4L[f(xs+1t )− f(x∗) + f(x̃s − f(x∗)].\nProof. The proof follows upon observing the following:\nE[‖vs+1t ‖2 = E[‖∇fit(xs+1t )−∇fit(xs+10 ) +∇f(x̃s)‖2] ≤ 2E[‖∇fit(xs+1t )−∇fit(x∗)‖2]\n+ 2E[‖∇fit(x̃s)−∇fit(x∗)− (∇f(x̃s)−∇f(x∗))‖2] ≤ 2E[‖∇fit(xs+1t )−∇fit(x∗)‖2] + 2E[‖∇fit(x̃s)−∇fit(x∗)‖2] ≤ 4L[f(xs+1t − f(x∗) + f(x̃s)− f(x∗)].\nThe first inequality follows from Cauchy-Schwarz and Young inequality; the second one from E[‖ξ− E[ξ]‖2] ≤ E[‖ξ‖2], and the third one from Lemma 5.\nLemma 7. For random variables z1, . . . , zr, we have E [ ‖z1 + ...+ zr‖2 ] ≤ rE [ ‖z1‖2 + ...+ ‖zr‖2 ] ."
    } ],
    "references" : [ {
      "title" : "A lower bound for the optimization of finite sums",
      "author" : [ "Agarwal", "Alekh", "Bottou", "Leon" ],
      "venue" : null,
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey",
      "author" : [ "Bertsekas", "Dimitri P" ],
      "venue" : null,
      "citeRegEx" : "Bertsekas and P.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bertsekas and P.",
      "year" : 2011
    }, {
      "title" : "Stochastic gradient learning in neural networks",
      "author" : [ "Bottou", "Léon" ],
      "venue" : "Proceedings of Neuro-Nımes,",
      "citeRegEx" : "Bottou and Léon.,? \\Q1991\\E",
      "shortCiteRegEx" : "Bottou and Léon.",
      "year" : 1991
    }, {
      "title" : "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Defazio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Defazio et al\\.",
      "year" : 2014
    }, {
      "title" : "Finito: A faster, permutable incremental gradient method for big data problems",
      "author" : [ "Defazio", "Aaron J", "Caetano", "Tibério S", "Domke", "Justin" ],
      "venue" : null,
      "citeRegEx" : "Defazio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Defazio et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimal distributed online prediction using mini-batches",
      "author" : [ "Dekel", "Ofer", "Gilad-Bachrach", "Ran", "Shamir", "Ohad", "Xiao", "Lin" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Dekel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2012
    }, {
      "title" : "Escaping from saddle points - online stochastic gradient for tensor decomposition",
      "author" : [ "Ge", "Rong", "Huang", "Furong", "Jin", "Chi", "Yuan", "Yang" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory,",
      "citeRegEx" : "Ge et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2015
    }, {
      "title" : "Stochastic first- and zeroth-order methods for nonconvex stochastic programming",
      "author" : [ "Ghadimi", "Saeed", "Lan", "Guanghui" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Ghadimi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ghadimi et al\\.",
      "year" : 2013
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Glorot", "Xavier", "Bengio", "Yoshua" ],
      "venue" : "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10),",
      "citeRegEx" : "Glorot et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2010
    }, {
      "title" : "Beyond convexity: Stochastic quasi-convex optimization",
      "author" : [ "Hazan", "Elad", "Levy", "Kfir", "Shalev-Shwartz", "Shai" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2015
    }, {
      "title" : "A distributed, asynchronous and incremental algorithm for nonconvex optimization: An admm based approach",
      "author" : [ "Hong", "Mingyi" ],
      "venue" : "arXiv preprint arXiv:1412.6058,",
      "citeRegEx" : "Hong and Mingyi.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hong and Mingyi.",
      "year" : 2014
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Johnson", "Rie", "Zhang", "Tong" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Johnson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2013
    }, {
      "title" : "Semi-Stochastic Gradient Descent Methods",
      "author" : [ "Konečný", "Jakub", "Richtárik", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Konečný et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Konečný et al\\.",
      "year" : 2013
    }, {
      "title" : "Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting",
      "author" : [ "Konečný", "Jakub", "Liu", "Jie", "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : null,
      "citeRegEx" : "Konečný et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Konečný et al\\.",
      "year" : 2015
    }, {
      "title" : "Stochastic approximation methods for constrained and unconstrained systems, volume 26",
      "author" : [ "Kushner", "Harold Joseph", "Clark", "Dean S" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Kushner et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kushner et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient mini-batch training for stochastic optimization",
      "author" : [ "Li", "Mu", "Zhang", "Tong", "Chen", "Yuqiang", "Smola", "Alexander J" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization",
      "author" : [ "Lian", "Xiangru", "Huang", "Yijun", "Li", "Yuncheng", "Liu", "Ji" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lian et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lian et al\\.",
      "year" : 2015
    }, {
      "title" : "Analysis of recursive stochastic algorithms",
      "author" : [ "Ljung", "Lennart" ],
      "venue" : "Automatic Control, IEEE Transactions on,",
      "citeRegEx" : "Ljung and Lennart.,? \\Q1977\\E",
      "shortCiteRegEx" : "Ljung and Lennart.",
      "year" : 1977
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Nemirovski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nemirovski et al\\.",
      "year" : 2009
    }, {
      "title" : "Problem Complexity and Method Efficiency in Optimization",
      "author" : [ "Nemirovski", "Arkadi", "D. Yudin" ],
      "venue" : null,
      "citeRegEx" : "Nemirovski et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Nemirovski et al\\.",
      "year" : 1983
    }, {
      "title" : "Introductory Lectures On Convex Optimization: A Basic Course",
      "author" : [ "Nesterov", "Yurii" ],
      "venue" : null,
      "citeRegEx" : "Nesterov and Yurii.,? \\Q2003\\E",
      "shortCiteRegEx" : "Nesterov and Yurii.",
      "year" : 2003
    }, {
      "title" : "Cubic regularization of newton method and its global performance",
      "author" : [ "Nesterov", "Yurii", "Polyak", "Boris T" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Nesterov et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nesterov et al\\.",
      "year" : 2006
    }, {
      "title" : "Pseudogradient adaptation and training algorithms",
      "author" : [ "BT Poljak", "Tsypkin", "Ya Z" ],
      "venue" : "Automation and Remote Control,",
      "citeRegEx" : "Poljak et al\\.,? \\Q1973\\E",
      "shortCiteRegEx" : "Poljak et al\\.",
      "year" : 1973
    }, {
      "title" : "Gradient methods for the minimisation of functionals",
      "author" : [ "B.T. Polyak" ],
      "venue" : "USSR Computational Mathematics and Mathematical Physics,",
      "citeRegEx" : "Polyak,? \\Q1963\\E",
      "shortCiteRegEx" : "Polyak",
      "year" : 1963
    }, {
      "title" : "On variance reduction in stochastic gradient descent and its asynchronous variants",
      "author" : [ "Reddi", "Sashank", "Hefny", "Ahmed", "Sra", "Suvrit", "Poczos", "Barnabas", "Smola", "Alex J" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Reddi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Reddi et al\\.",
      "year" : 2015
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "Robbins and Monro,? \\Q1951\\E",
      "shortCiteRegEx" : "Robbins and Monro",
      "year" : 1951
    }, {
      "title" : "Minimizing Finite Sums with the Stochastic Average Gradient",
      "author" : [ "Schmidt", "Mark W", "Roux", "Nicolas Le", "Bach", "Francis R" ],
      "venue" : null,
      "citeRegEx" : "Schmidt et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2013
    }, {
      "title" : "A stochastic PCA and SVD algorithm with an exponential convergence rate",
      "author" : [ "Shamir", "Ohad" ],
      "venue" : null,
      "citeRegEx" : "Shamir and Ohad.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shamir and Ohad.",
      "year" : 2014
    }, {
      "title" : "Fast stochastic algorithms for SVD and PCA: Convergence properties and convexity",
      "author" : [ "Shamir", "Ohad" ],
      "venue" : null,
      "citeRegEx" : "Shamir and Ohad.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shamir and Ohad.",
      "year" : 2015
    }, {
      "title" : "Scalable nonconvex inexact proximal splitting",
      "author" : [ "Sra", "Suvrit" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Sra and Suvrit.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sra and Suvrit.",
      "year" : 2012
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "Xiao", "Lin", "Zhang", "Tong" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Xiao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2014
    }, {
      "title" : "Univr: A universal variance reduction framework for proximal stochastic gradient method",
      "author" : [ "Zhu", "Zeyuan Allen", "Yuan", "Yang" ],
      "venue" : "CoRR, abs/1506.01972,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Among these, of particular importance are variance reduced (VR) stochastic methods (Schmidt et al., 2013; Johnson & Zhang, 2013; Defazio et al., 2014a), which have delivered exciting progress such as linear convergence rates (for strongly convex functions) as opposed to sublinear rates of ordinary Sgd (Robbins & Monro, 1951; Nemirovski et al.",
      "startOffset" : 83,
      "endOffset" : 151
    }, {
      "referenceID" : 18,
      "context" : ", 2014a), which have delivered exciting progress such as linear convergence rates (for strongly convex functions) as opposed to sublinear rates of ordinary Sgd (Robbins & Monro, 1951; Nemirovski et al., 2009).",
      "startOffset" : 160,
      "endOffset" : 208
    }, {
      "referenceID" : 26,
      "context" : "The Svrg algorithm of (Johnson & Zhang, 2013) is particularly attractive here because of its low storage requirement in comparison to the algorithms in (Schmidt et al., 2013; Defazio et al., 2014a).",
      "startOffset" : 152,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : ", 2013; Johnson & Zhang, 2013; Defazio et al., 2014a), which have delivered exciting progress such as linear convergence rates (for strongly convex functions) as opposed to sublinear rates of ordinary Sgd (Robbins & Monro, 1951; Nemirovski et al., 2009). Similar (but not same) benefits of VR methods can also be seen in smooth convex functions. The Svrg algorithm of (Johnson & Zhang, 2013) is particularly attractive here because of its low storage requirement in comparison to the algorithms in (Schmidt et al., 2013; Defazio et al., 2014a). Despite the meteoric rise of VR methods, their analysis for general nonconvex problems is largely missing. Johnson & Zhang (2013) remark on convergence of Svrg when f ∈ Fn is locally strongly convex and provide compelling experimental results (Fig.",
      "startOffset" : 31,
      "endOffset" : 675
    }, {
      "referenceID" : 23,
      "context" : "• For an interesting nonconvex subclass of Fn called gradient dominated functions (Polyak, 1963; Nesterov & Polyak, 2006), we propose a variant of Svrg that attains a global linear rate of convergence.",
      "startOffset" : 82,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "A key reference for stochastic convex optimization (for minEz[F (x, z)]) is (Nemirovski et al., 2009).",
      "startOffset" : 76,
      "endOffset" : 101
    }, {
      "referenceID" : 26,
      "context" : ", (Defazio et al., 2014a; Johnson & Zhang, 2013; Schmidt et al., 2013; Konečný et al., 2015; Shalev-Shwartz & Zhang, 2013; Defazio et al., 2014b).",
      "startOffset" : 2,
      "endOffset" : 145
    }, {
      "referenceID" : 13,
      "context" : ", (Defazio et al., 2014a; Johnson & Zhang, 2013; Schmidt et al., 2013; Konečný et al., 2015; Shalev-Shwartz & Zhang, 2013; Defazio et al., 2014b).",
      "startOffset" : 2,
      "endOffset" : 145
    }, {
      "referenceID" : 24,
      "context" : "Asynchronous VR frameworks are developed in (Reddi et al., 2015; Mania et al., 2015).",
      "startOffset" : 44,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "A similar rate for parallel and distributed Sgd was shown recently in (Lian et al., 2015).",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "Finally, we note another interesting example, stochastic optimization of locally quasi-convex functions (Hazan et al., 2015), wherein actually a O(1/ ) convergence in function value is shown.",
      "startOffset" : 104,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : ", (Defazio et al., 2014a; Johnson & Zhang, 2013; Schmidt et al., 2013; Konečný et al., 2015; Shalev-Shwartz & Zhang, 2013; Defazio et al., 2014b). Asynchronous VR frameworks are developed in (Reddi et al., 2015; Mania et al., 2015). Agarwal & Bottou (2014) study lower-bounds for convex finite-sum problems.",
      "startOffset" : 3,
      "endOffset" : 257
    }, {
      "referenceID" : 3,
      "context" : ", (Defazio et al., 2014a; Johnson & Zhang, 2013; Schmidt et al., 2013; Konečný et al., 2015; Shalev-Shwartz & Zhang, 2013; Defazio et al., 2014b). Asynchronous VR frameworks are developed in (Reddi et al., 2015; Mania et al., 2015). Agarwal & Bottou (2014) study lower-bounds for convex finite-sum problems. ShalevShwartz (2015) prove linear convergence of stochastic dual coordinate ascent when the individual fi (i ∈ [n]) are nonconvex but f is strongly convex.",
      "startOffset" : 3,
      "endOffset" : 329
    }, {
      "referenceID" : 3,
      "context" : ", (Defazio et al., 2014a; Johnson & Zhang, 2013; Schmidt et al., 2013; Konečný et al., 2015; Shalev-Shwartz & Zhang, 2013; Defazio et al., 2014b). Asynchronous VR frameworks are developed in (Reddi et al., 2015; Mania et al., 2015). Agarwal & Bottou (2014) study lower-bounds for convex finite-sum problems. ShalevShwartz (2015) prove linear convergence of stochastic dual coordinate ascent when the individual fi (i ∈ [n]) are nonconvex but f is strongly convex. They do not study the general nonconvex case. Moreover, even in their special setting our results improve upon theirs for the high condition number regime. Nonconvex. Sgd dates at least to the seminal work (Robbins & Monro, 1951); and since then it has been developed in several directions (Poljak & Tsypkin, 1973; Ljung, 1977; Bottou, 1991; Kushner & Clark, 2012). In the (nonsmooth) finite-sum setting, Sra (2012) considers proximal splitting methods, and analyzes asymptotic convergence with nonvanishing gradient errors.",
      "startOffset" : 3,
      "endOffset" : 880
    }, {
      "referenceID" : 3,
      "context" : ", (Defazio et al., 2014a; Johnson & Zhang, 2013; Schmidt et al., 2013; Konečný et al., 2015; Shalev-Shwartz & Zhang, 2013; Defazio et al., 2014b). Asynchronous VR frameworks are developed in (Reddi et al., 2015; Mania et al., 2015). Agarwal & Bottou (2014) study lower-bounds for convex finite-sum problems. ShalevShwartz (2015) prove linear convergence of stochastic dual coordinate ascent when the individual fi (i ∈ [n]) are nonconvex but f is strongly convex. They do not study the general nonconvex case. Moreover, even in their special setting our results improve upon theirs for the high condition number regime. Nonconvex. Sgd dates at least to the seminal work (Robbins & Monro, 1951); and since then it has been developed in several directions (Poljak & Tsypkin, 1973; Ljung, 1977; Bottou, 1991; Kushner & Clark, 2012). In the (nonsmooth) finite-sum setting, Sra (2012) considers proximal splitting methods, and analyzes asymptotic convergence with nonvanishing gradient errors. Hong (2014) studies a distributed nonconvex incremental ADMM algorithm.",
      "startOffset" : 3,
      "endOffset" : 1001
    }, {
      "referenceID" : 3,
      "context" : ", (Defazio et al., 2014a; Johnson & Zhang, 2013; Schmidt et al., 2013; Konečný et al., 2015; Shalev-Shwartz & Zhang, 2013; Defazio et al., 2014b). Asynchronous VR frameworks are developed in (Reddi et al., 2015; Mania et al., 2015). Agarwal & Bottou (2014) study lower-bounds for convex finite-sum problems. ShalevShwartz (2015) prove linear convergence of stochastic dual coordinate ascent when the individual fi (i ∈ [n]) are nonconvex but f is strongly convex. They do not study the general nonconvex case. Moreover, even in their special setting our results improve upon theirs for the high condition number regime. Nonconvex. Sgd dates at least to the seminal work (Robbins & Monro, 1951); and since then it has been developed in several directions (Poljak & Tsypkin, 1973; Ljung, 1977; Bottou, 1991; Kushner & Clark, 2012). In the (nonsmooth) finite-sum setting, Sra (2012) considers proximal splitting methods, and analyzes asymptotic convergence with nonvanishing gradient errors. Hong (2014) studies a distributed nonconvex incremental ADMM algorithm. These works, however, only prove expected convergence to stationary points and often lack analysis of rates. The first nonasymptotic convergence rate analysis for Sgd is in (Ghadimi & Lan, 2013), who show that Sgd ensures ‖∇f‖ ≤ in O(1/ ) iterations. A similar rate for parallel and distributed Sgd was shown recently in (Lian et al., 2015). GradientDescent is known to ensure ‖∇f‖ ≤ in O(1/ ) iterations (Nesterov, 2003, Chap. 1.2.3). The first analysis of nonconvex Svrg seems to be due to Shamir (2014), who considers the special problem of computing a few leading eigenvectors (e.",
      "startOffset" : 3,
      "endOffset" : 1567
    }, {
      "referenceID" : 23,
      "context" : "We also recall the class of gradient dominated functions (Polyak, 1963; Nesterov & Polyak, 2006), where a function f is called τ -gradient dominated if for any x ∈ R f(x)− f(x∗) ≤ τ‖∇f(x)‖, (2)",
      "startOffset" : 57,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : "Note that GradientDescent can also achieve linear convergence rate for gradient dominated functions (Polyak, 1963).",
      "startOffset" : 100,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "For a batch size of b, Sgd obtains a rate of O(1/ √ bT ) (Dekel et al., 2012) (obtainable by a simple modification of Theorem 1).",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "In contrast, Sgd does not yield an efficient mini-batch strategy as it requires O(b/ ) IFO calls for achieving an -accurate solution (Li et al., 2014).",
      "startOffset" : 133,
      "endOffset" : 150
    }, {
      "referenceID" : 26,
      "context" : "Such initialization is standard for variance reduced schemes even for convex problems (Johnson & Zhang, 2013; Schmidt et al., 2013).",
      "startOffset" : 86,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "In fact, Ge et al. (2015) add additional noise to the stochastic gradient in order to escape saddle points.",
      "startOffset" : 9,
      "endOffset" : 26
    } ],
    "year" : 2017,
    "abstractText" : "We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (Svrg) methods for them. Svrg and related methods have recently surged into prominence for convex optimization given their edge over stochastic gradient descent (Sgd); but their theoretical analysis almost exclusively assumes convexity. In contrast, we prove non-asymptotic rates of convergence (to stationary points) of Svrg for nonconvex optimization, and show that it is provably faster than Sgd and gradient descent. We also analyze a subclass of nonconvex problems on which Svrg attains linear convergence to the global optimum. We extend our analysis to mini-batch variants of Svrg, showing (theoretical) linear speedup due to mini-batching in parallel settings.",
    "creator" : "TeX"
  }
}