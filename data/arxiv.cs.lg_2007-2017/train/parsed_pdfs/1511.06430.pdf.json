{
  "name" : "1511.06430.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Mohammad Pezeshki", "Linxi Fan", "Aaron Courville", "Yoshua Bengio" ],
    "emails" : [ "mohammad.pezeshki@umontreal.ca", "linxi.fan@columbia.edu", "pbpop3@gmail.com,", "<findme>}@iro.umontreal.ca" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Labeling data sets is typically a costly task and in many settings there are far more unlabeled examples than labeled ones. Semi-supervised learning aims to improve the performance on some supervised learning problem by using information obtained from both labeled and unlabeled examples. Since the recent success of deep learning methods has mainly relied on supervised learning based on very large labeled datasets, it is interesting to explore semi-supervised deep learning approaches to extend the reach of deep learning to these settings.\nSince unsupervised methods for pre-training layers or neural networks were an essential part of the first wave of deep learning methods (Hinton et al., 2006; Vincent et al., 2008; Bengio, 2009),\n∗Yoshua Bengio is a CIFAR Senior Fellow\nar X\niv :1\n51 1.\n06 43\n0v 1\n[ cs\n.L G\n] 1\n9 N\na natural step is to investigate how ideas inspired by Restricted Boltzmann Machine training and regularized autoencoders can be used for semi-supervised learning. Examples of approaches based on such ideas are the discriminative RBM (Larochelle & Bengio, 2008) and a deep architecture based on semi-supervised autoencoders that was used for document classification (Ranzato & Szummer, 2008). More recent examples of approaches for semi-supervised deep learning are the semisupervised Variational Autoencoder (Kingma et al., 2014) and the Ladder Network (Rasmus et al., 2015) which obtained very impressive, state of the art results (1.13% error) on the MNIST handwritten digits classification benchmark using just 100 labeled training examples.\nThe Ladder Network adds an unsupervised component to the supervised learning objective of a deep feedforward network by treating this network as part of a deep stack of denoising autoencoders or DAEs (Vincent et al., 2010) that learns to reconstruct each layer (including the input) based on a corrupted version of it, using feedback from upper levels. The term ’ladder’ refers to how this architecture extends the stacked DAE in the way the feedback paths are formed.\nThis paper is focusing on the design choices that lead to the Ladder Network’s superior performance and tries to disentangle them empirically. We identify some general properties of the model that make it different from standard feedforward networks and compare various architectures to identify those properties and design choices that are the most essential for obtaining good performance with Ladder Networks. While the original authors of the Ladder Network paper explored some variants of their model already, we provide a thorough comparison of a large number of architectures controlling for both hyper parameter settings and data set selection.\nWe also introduce a variant of the Ladder Network that yields new state-of-the-art results for the Permutation-Invariant MNIST classification task in both semi- and fully- supervised settings."
    }, {
      "heading" : "2 THE LADDER NETWORK ARCHITECTURE",
      "text" : "Consider a dataset with N labeled examples (x(1), y∗(1)), (x(2), y∗(2)), ..., (x(N), y∗(N)) and M unlabeled examples x(N+1), x(N+2), ..., x(N+M) where M N . The objective is to learn a function f(x) to model P (y|x) for a new unseen x. The function f(x) needs to be defined in a way that incorporates information about the large amount of unlabeled examples to help the supervised task. Consider a deep denoising Auto Encoder (DAE) in which noise is injected to all hidden layers (See figure 1). However, the objective function is a weighted sum of the supervised Cross Entropy cost and the unsupervised denoising Square Error costs at each layer.\nTherefore, in the above-mentioned DAE, the forward path encodes the input x to high level features and in the backward path, all hidden layers and the input layer are reconstructed using a decoder. As such, hidden representations must contain a set of features suitable for both the supervised task and the unsupervised task.\nThrough lateral skip connections, each layer of the encoder is connected to its corresponding layer in decoder. This enables the higher layer features to focus on more abstract and task-specific features. Hence, at each layer of the decoder, two signals, one from the layer above and the other from the corresponding layer in the encoder are combined.\nFormally, the Ladder Network is defined as follows:\nx̃, z̃(1), ..., z̃(L), ỹ = Encodernoisy(x), (1)\nx, z(1), ..., z(L), y = Encoderclean(x), (2)\nx̂, ẑ(1), ..., ẑ(L) = Decoder(z̃(1), ..., z̃(L)), (3)\nwhere Encoder and Decoder can be replaced by any multi-layer architecture such as a multi-layer perceptron in this case. The variables x, y, and ỹ are the input, the noiseless output, and the noisy output respectively. The variables zl, z̃l, and ẑl are the hidden representation, its noisy version, and its reconstructed version at layer l. The objective function is a weighted sum of supervised and unsupervised costs:\nCost = ΣLl=1λl SquaredError(z l, ẑl) + CrossEntropy(y∗, ỹ) (4)\nin which\nSquaredError(zl, ẑl) = ||zl − ẑl||2and (5) CrossEntropy(y∗, ỹ) = −ΣNn=1 logP (ỹ(n) = y∗(n)|x(n)). (6)\nIn the forward path, individual layers of the encoder are formalized as a linear transformation followed by batch normalization and then application of the activation function:\nz̃lpre = W l · h̃l−1, (7)\nµ̃l = mean(z̃lpre), (8)\nσ̃l = stdv(z̃lpre), (9)\nz̃l = z̃lpre − µ̃l\nσ̃l +N (0, σ2), (10)\nh̃l = φ(γl(z̃l + βl)), (11)\nwhere h̃l−1 is the post-activation at layer l − 1, W l is the weight matrix from layer l − 1 to l, z̃lpre is the pre-normalization, and z̃l is the pre-activation which is normalized using the batch statistics µl and σl. The parameters βl and γl are responsible for shifting and scaling before feeding the activation function φ(.). Note that the above equations are for the noisy path. If we remove noise (N (0, σ2)) and replace h̃ and z̃ with h and z respectively, we get the noiseless encoder.\nIn the backward path, at each layer of the decoder, the activation from the next layer ĥl+1 and the noisy representation z̃l are mapped into the reconstruction ẑl using the following equations:\nul+1pre = V l · ĥl+1, (12)\nµl+1 = mean(ul+1pre ), (13)\nσl+1 = stdv(ul+1pre ), (14)\nul+1 = ul+1pre − µl+1\nσl+1 , (15)\nẑl = g(z̃l, ul+1), (16)\nĥl = φ(ẑl), (17)\nin which V l is a weight matrix from layer l + 1 to layer l. We call the function g(·, ·) the combinator function which combines the vertical u(l+1) and the lateral z̃l connections in an element-wise fashion. The original Ladder Network proposes the following design for g(·, ·), which we call the vanilla combinator:\ng(z̃l, u(l+1)) = b0 + w0z z̃(l) + w0u u(l+1) + w0zu z̃(l)u(l+1)\n+ wσ Sigmoid(b1 + w1z z̃(l) + w1u u(l+1) + w1zu z̃(l) u(l+1)), (18)\nwhere is an element-wise multiplication operator and each per-element weight is initialized as: w{0,1}z ← 1 w{0,1}u ← 0 w{0,1}zu, b{0,1} ← 0 wσ ← 1 (19)\nIn later sections, we will explore alternative initialization schemes on the vanilla combinator.\nIt is also worth mentioning that in batch normalization, the statistics (mean and variance) are computed over a single batch and not the whole training set. Hence, the statistics are not accurate or in other words are noisy. Since Batch Normalization is applied to both the noisy and noiseless encoders, both zl and z̃l share the noise that is due to the Batch Normalization. This makes z̃l and zl highly correlated and biases the network towards not using the vertical connections at all. To cancel the effect of this unwanted noise, the vertical connections are also normalized using the encoder\nstatistics. Therefore, instead of using Cl = λl ||ẑ(l) − z(l)||2 in the final cost function, the function Cl = λ′l || ẑ(l)−µl σl\n− z(l)||2 is used. µl and σl are the encoder’s sample mean and standard deviation statistics, respectively. For a more detailed explanation of the Ladder architecture, see (Rasmus et al., 2015; Valpola, 2014)."
    }, {
      "heading" : "3 COMPONENTS OF THE LADDER NETWORK",
      "text" : "Now that the precise architecture of the Ladder Network has been described in details, we can identify a couple of important additions to the standard feed-forward neural network architecture that may have pronounced impact on the performance. A distinction can also be made between those design choices that follow naturally from the motivation of the ladder network as a deep autoencoder and those that are more ad-hoc and task specific.\nThe most obvious addition is the extra reconstruction cost for every hidden layer and the input layer. While it is obvious that reconstruction provides an unsupervised objective to harness the unlabeled examples, it is not clear how important the penalization is for each layer and what role it plays for fully-supervised tasks.\nA second important change is the addition of Gaussian noise to the input and the hidden representations. While adding noise to the first layer is a part of denoising autoencoder training, it is again\nnot clear whether it is necessary to add this noise at every layer or not. We would also like to know if the noise helps by making the reconstruction task nontrivial and useful or just by regularizing the feed-forward network in a similar way that noise-based regularizers like dropout (Srivastava et al., 2014) and adaptive weight noise (Graves, 2011) do.\nFinally, the lateral skip connections are the most notable deviation from the standard denoising autoencoder architecture. To some degree, it is unorthodox in the way the vanilla Ladder Network combines the lateral stream of information z̃l and the downward stream of information u(l+1). We have conducted extensive experiments on how important the precise choice for this function (which we refer to as the combinator function) actually is."
    }, {
      "heading" : "4 EXPERIMENTAL SETUP",
      "text" : "In this section, we introduce different variants of the vanilla Ladder Architecture and describe our experiment methodology. Each variant differs from either the vanilla model or a previous variant in only one component. This enables us to isolate each component and observe its effects while other components remain unchanged. Table 1 depicts the different variants we have investigated and their abbreviations."
    }, {
      "heading" : "4.1 NOISE VARIANTS",
      "text" : "Different configurations of noise injection, penalizing reconstruction errors, and the lateral connection removal suggest four different variants:\n• Add noise only to the first layer (FirstNoise). • Only penalize the reconstruction at the first layer (FirstRecons), i.e. λ(l≥1) are set to\n0.\n• Apply both of the above changes: add noise and penalize the reconstruction only at the first layer (FirstN&R).\n• Remove all lateral connections from FirstN&R. Therefore, equivalent to to a denoising autoencoder with an additional supervised cost at the top, the encoder and the decoder are connected only through the topmost connection. We call this variant NoLateral."
    }, {
      "heading" : "4.2 VANILLA COMBINATOR VARIANTS",
      "text" : "We try different variants of the vanilla combinator function that combines the two streams of information from the lateral and the vertical connections in an unusual way. As defined in equation 18, the output of the vanilla combinator depends on u, z̃, and u z̃ 1, which are connected to the output via two paths, one linear and the other through a sigmoid non-linearity unit. (See figure 2(a))\nNote that the vanilla combinator is initialized in a very specific way (equation 19), which sets the initial weights for lateral connection z̃ to 1 and vertical connection u to 0. This particular scheme encourages the Ladder decoder path to learn more from the lateral information stream z̃ than the vertical u at the beginning of training.\nWe explore two variants of the initialization scheme:\n• Random initialization (RandInit): all per-element weights w∗ and biases b∗ are randomly initialized to N (0, 0.2).\n• Reverse initialization (RevInit)  w{0,1}z ← 0 w{0,1}u ← 1 w{0,1}zu, b{0,1} ← 0 wσ ← 1\nWe also investigate three other vanilla combinator variants:\n• Remove sigmoid non-linearity (NoSig). The corresponding per-element weights are initialized in the same way as the vanilla combinator.\n• Remove the multiplicative term u z̃ (NoMult). • Simple linear combination (Linear)\ng(z̃, u) = b+ wu u+ wz z̃ (20) where the initialization scheme resembles the vanilla one: { wz ← 1 wu, b ← 0"
    }, {
      "heading" : "4.3 GAUSSIAN COMBINATOR VARIANTS",
      "text" : "Another choice for the combinator function is the Gaussian combinator proposed in the original papers about the Ladder Architecture (Rasmus et al., 2015; Valpola, 2014). It is defined as:\ng(z̃, u) = (z̃ − µ(u)) σ(u) + µ(u), (21) µ(u) = w1 Sigmoid(w2 u+ w3) + w4 u+ w5, (22) σ(u) = w6 Sigmoid(w7 u+ w8) + w9 u+ w10. (23)\nThis combinator assumes that z is Gaussian given u. u’s mean µ(u) and standard deviation σ(u) are modeled as non-linearity applied on u in equation 22 and 23. Strictly speaking, σ(u) is not a proper standard deviation, because it is not guaranteed to be positive all the time.\nTo make the Gaussian interpretation rigorous, we explore a variant that we call GatedGauss, where equations 21 and 22 stay the same but 23 is replaced by:\n1For simplicity, subscript i and superscript l are implicit from now on.\nσ(u) = Sigmoid(w6 u+ w7). (24)\nGatedGauss guarantees that 0 < σ(u) < 1. Now we can re-arrange 21 into a convex combination of z̃ and µ(u):\ng(z̃, u) = σ(u) z̃ + (1− σ(u)) µ(u) (25)\nWe expect that σ(u)i will be close to 1 if the information from the lateral connection for unit i is more helpful to reconstruction, and close to 0 if the vertical connection becomes more useful."
    }, {
      "heading" : "4.4 MLP COMBINATOR VARIANTS",
      "text" : "We also propose another type of element-wise combinator functions based on fully-connected MLPs. The design is motivated by the fact that neural networks are universal function approximators, which should theoretically be able to approximate any combinator function.\nWe have explored two classes in this family. The first one, denoted simply as MLP, maps two scalars [u, z̃] to a single output g(z̃, u) (figure 2(b)). We empirically determine the choice of activation function for the hidden layers. Preliminary experiments show that the Leaky Rectifier Linear Unit (LReLU) (Maas et al., 2013) performs better than either the conventional ReLU or the sigmoid unit. Our LReLU function is formulated as\nLReLU(x) = { x, if x ≥ 0, 0.1x, otherwise\n(26)\nWe experiment with different numbers of layers and hidden units per layer in the MLP. We present results for three specific configurations: [4] for a single hidden layer of 4 units, [2, 2] for 2 hidden layers each with 2 units, and [2, 2, 2] for 3 hidden layers. For example, in the [2, 2, 2] configuration, the MLP combinator function is defined as:\ng(z̃, u) = W3 · LReLU ( W2 · LReLU(W1 · [u, z̃] + b1) + b2 ) + b3 (27)\nwhere W1, W2, and W3 are 2× 2 weight matrices; b1, b2, and b3 are 2× 1 bias vectors. The second class, which we denote as AMLP (Augmented MLP), has a multiplicative term as an augmented input unit (figure 2(c)). AMLP maps three scalars [u, z̃, u z̃] to a single output. We use the same LReLU unit for AMLP. We do similar experiments as in the MLP case and include results for [4], [2, 2] and [2, 2, 2] hidden layer configurations.\nBoth MLP and AMLP weight parameters are randomly initialized toN (0, σ). σ is considered to be a hyperparameter and tuned on the validation set. Precise values for the best σs are listed in Appendix A."
    }, {
      "heading" : "4.5 METHODOLOGY",
      "text" : "Experiment settings include two semi-supervised classification tasks with 100 and 1000 labeled examples and a fully-supervised classification task with 60000 labeled examples for PermutationInvariant MNIST handwritten digit classification.\nIn all of our experiments, labeled examples are chosen randomly but the number of examples for different classes is balanced. The test set is not used during all the hyperparameter search and tuning.\nEach experiment is repeated 10 times with 10 different but fixed random seeds to measure the standard error of the results for different parameter initializations and different selections of labeled examples. All standard errors are directly comparable with each other because the array of 10 seeds is the same.\nAll variants and the vanilla Ladder Network itself are trained using the ADAM optimization algorithm (Kingma & Ba, 2014) with a learning rate of 0.002 for 100 iterations followed by 50 iterations with a learning rate decaying linearly to 0. Hyperparameters including the standard deviation of the noise injection and the denoising weights at each layer are tuned separately for each variant and each experiment setting (100-, 1000-, and fully-labeled). Hyperparmeters are optimized by random grid search (Bergstra & Bengio, 2012) and combinatorial grid search with some manual deletion over the search space (see Appendix A for precise configurations)."
    }, {
      "heading" : "5 RESULTS & DISCUSSION",
      "text" : "In the previous section, we introduced the definitions of different Ladder Network variants. Table 2 collects all results for the variants and the baselines. The results are organized into three main categorizes in table 2. Boxplots of the results are also shown in figure 3.\nThe Baseline model is a simple feed-forward neural network with no reconstruction penalty and the Baseline+noise is the same network but with additive noise at each layer. The best results in terms of average error rate on the test set are achieved by the proposed AMLP combinator function: in the fully-supervised setting, the best average error rate is 0.569 ± 0.010, while in the semi-supervised settings with 100 and 1000 labeled examples, the averages are 1.002 ± 0.037 and 0.974± 0.021 respectively."
    }, {
      "heading" : "5.1 NOISE VARIANTS",
      "text" : "The results in the first part of the table indicate that in the fully-supervised setting, adding noise either to the first layer only or to all layers leads to a lower error rate with respect to the baselines. Our intuition is that the effect of additive noise to layers is very similar to the weight noise regularization method (Graves, 2011) or dropout (Hinton et al., 2012).\nIn addition, the error rates in the first part of the table tell us that removing the lateral connections hurts much more than the absence of noise injection or reconstruction penalty in the intermediate layers. It is also worth mentioning that hyperparameter tuning yields zero weights for penalizing the reconstruction errors in all layers except the input layer in the fully-supervised task for the vanilla model. Something similar occurs to NoLateral as well, where hyperparameter tuning yields zero reconstruction weights for all layers including the input layer. In other words, NoLateral and Baseline+noise become the same models for the fully-supervised task. Moreover, the weights for the reconstruction penalty of the hidden layers are relatively small in the semi-supervised task. This is in line with similar observations (relatively small weights for the unsupervised part of the objective) in hybrid discriminant RBM (Larochelle & Bengio, 2008)."
    }, {
      "heading" : "5.2 COMBINATOR FUNCTION VARIANTS",
      "text" : "The second and the third parts of table 2 show the relative performance of different combinator functions.\nUnsurprisingly, the performance deteriorates considerably if we remove the sigmoid non-linearity (NoSig) or the multiplicative term (NoMult) or both (Linear) from the vanilla combinator. Judging from the size of increase in average error rates, the multiplicative term is more important than the sigmoid unit.\nAs described in 4.2 and equation 19, the per-element weights of the lateral connections are initialized to ones while those of the vertical are initialized to zeros. Interestingly, the results are slightly worse for the RandInit variant, in which these weights are initialized randomly. The RevInit variant is even worse than random initialization scheme.\nWe suspect the reason is that the optimization algorithm finds it easier to reconstruct a representation z starting from its noisy version z̃, rather than starting from an initially arbitrary reconstruction from the untrained upper layers. Another justification is that this scheme 19 corresponds to optimizing the ladder network as if it behaves like a stack of decoupled DAEs initially.\nThe Gaussian combinator performs better than the vanilla combinator. GatedGauss, the other variant with strict 0 < σ(u) < 1, does not perform as well as the one with unconstrained σ(u). In the Gaussian formulation, z̃ is regulated by two functions of u: µ(u) and σ(u). This combinator interpolates between the noisy activation and the predicted reconstruction (equation 25), and the scaling parameter can be interpreted as a measure of the certainty of the network.\nFinally, the AMLP model yields state-of-the-art results in all of 100-, 1000- and 60000-labeled experiments for PI MNIST. It outperforms both the MLP and the vanilla model. Even though MLP is theoretically able to model any combinator function, the additional multiplicative input unit u z̃ helps the learning process significantly."
    }, {
      "heading" : "5.3 PROBABILISTIC INTERPRETATIONS OF THE LADDER NETWORK",
      "text" : "Since many of the motivations behind regularized autoencoder architectures are based on observations about generative models, we briefly discuss how the Ladder Network can be related to some other models with varying degrees of probabilistic interpretability. Considering that the components that are most defining of the Ladder Network seem to be the most important ones for semi-supervised learning in particular, comparisons with generative models are at least intuitively appealing to get more insight about how the model learns about unlabeled examples.\nBy training the individual denoising autoencoders that make up the Ladder Network with a single objective function, this coupling goes as far as encouraging the lower levels to produce representations that are going to be easy to reconstruct by the upper levels. We find a similar term (-log of the top-level prior evaluated at the output of the encoder) in hierarchical extensions of the variational\nautoencoder (Rezende et al., 2014; Bengio, 2014). While the Ladder Network differs too much from an actual variational autoencoder to be treated as such, the similarities can still give one intuitions about the role of the noise and the interactions between the layers. The other way round, one also may wonder how a variational autoencoder might benefit from some of the components of Ladder Networks like Batch Normalization and multiplicative connections.\nWhen one simply views the Ladder Network as a peculiar type of denoising autoencoder, one could extend the recent work on the generative interpretation of denoising autoencoders (Alain & Bengio, 2013; Bengio et al., 2013) to interpret the Ladder Network as a generative model as well. It would be interesting to see if the Ladder Network architecture can be used to generate samples and if the architecture’s success at semi-supervised learning translates to this profoundly different use of the model."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "The paper systematically compares different variants of the recent Ladder Network architecture (Rasmus et al., 2015; Valpola, 2014) with two feedforward neural networks as the baselines and the standard architecture (proposed in the original paper). Comparisons are done in a deconstructive way, starting from the standard architecture. Based on the comparisons of different variants we conclude that:\n• Unsurprisingly, the reconstruction cost is crucial to obtain the desired regularization from unlabeled data.\n• Applying additive noise to each layer and especially the first layer has a regularization effect which helps generalization. This seems to be one of the most important contributors to the performance on the fully supervised task.\n• The lateral connection is a vital component in the Ladder architecture to the extent that removing it considerably deteriorates the performance for all of the semi-supervised tasks.\n• The precise choice of the combinator function has less dramatic impact, although the vanilla combinator can be replaced by the Augmented MLP to yield better performance, in fact allowing us to improve the record error rates on Permutation-Invariant MNIST for semiand fully-supervised settings.\nWe hope that these comparisons between different architectural choices will help deep learning researchers improve their understanding of what makes semi-supervised learning successful, at least for architectures related to the Ladder Network."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The experiments were conducted using Theano (Bergstra et al., 2010),(Bastien et al., 2012), Blocks and Fuel (van Merriënboer et al., 2015) libraries. The authors would like to acknowledge the funding support from: NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR."
    } ],
    "references" : [ {
      "title" : "What regularized auto-encoders learn from the data generating distribution",
      "author" : [ "Alain", "Guillaume", "Bengio", "Yoshua" ],
      "venue" : "In ICLR’2013. also arXiv report 1211.4246,",
      "citeRegEx" : "Alain et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Alain et al\\.",
      "year" : 2013
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Bastien", "Frédéric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Warde-Farley", "David", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1211.5590,",
      "citeRegEx" : "Bastien et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "Generalized denoising autoencoders as generative models",
      "author" : [ "Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal" ],
      "venue" : "In NIPS’2013,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Random search for hyper-parameter optimization",
      "author" : [ "Bergstra", "James", "Bengio", "Yoshua" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2012
    }, {
      "title" : "Theano: a cpu and gpu math expression compiler",
      "author" : [ "Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Frédéric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua" ],
      "venue" : "In Proceedings of the Python for scientific computing conference (SciPy),",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Practical variational inference for neural networks",
      "author" : [ "Graves", "Alex" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Graves and Alex.,? \\Q2011\\E",
      "shortCiteRegEx" : "Graves and Alex.",
      "year" : 2011
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee-Whye" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik", "Ba", "Jimmy" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Semisupervised learning with deep generative models",
      "author" : [ "Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Classification using discriminative restricted boltzmann machines",
      "author" : [ "Larochelle", "Hugo", "Bengio", "Yoshua" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Larochelle et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Larochelle et al\\.",
      "year" : 2008
    }, {
      "title" : "Rectifier nonlinearities improve neural network acoustic models",
      "author" : [ "Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y" ],
      "venue" : "In Proc. ICML,",
      "citeRegEx" : "Maas et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2013
    }, {
      "title" : "Semi-supervised learning of compact document representations with deep networks",
      "author" : [ "Ranzato", "Marc’Aurelio", "Szummer", "Martin" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2008
    }, {
      "title" : "Semisupervised learning with ladder network",
      "author" : [ "Rasmus", "Antti", "Valpola", "Harri", "Honkala", "Mikko", "Berglund", "Mathias", "Raiko", "Tapani" ],
      "venue" : "arXiv preprint arXiv:1507.02672,",
      "citeRegEx" : "Rasmus et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rasmus et al\\.",
      "year" : 2015
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan" ],
      "venue" : "In ICML’2014,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "From neural pca to deep unsupervised learning",
      "author" : [ "Valpola", "Harri" ],
      "venue" : "arXiv preprint arXiv:1411.7783,",
      "citeRegEx" : "Valpola and Harri.,? \\Q2014\\E",
      "shortCiteRegEx" : "Valpola and Harri.",
      "year" : 2014
    }, {
      "title" : "Blocks and fuel: Frameworks for deep learning",
      "author" : [ "van Merriënboer", "Bart", "Bahdanau", "Dzmitry", "Dumoulin", "Vincent", "Serdyuk", "Dmitriy", "Warde-Farley", "David", "Chorowski", "Jan", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1506.00619,",
      "citeRegEx" : "Merriënboer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Merriënboer et al\\.",
      "year" : 2015
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine" ],
      "venue" : "J. Machine Learning Res.,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Since unsupervised methods for pre-training layers or neural networks were an essential part of the first wave of deep learning methods (Hinton et al., 2006; Vincent et al., 2008; Bengio, 2009), ∗Yoshua Bengio is a CIFAR Senior Fellow",
      "startOffset" : 136,
      "endOffset" : 193
    }, {
      "referenceID" : 18,
      "context" : "Since unsupervised methods for pre-training layers or neural networks were an essential part of the first wave of deep learning methods (Hinton et al., 2006; Vincent et al., 2008; Bengio, 2009), ∗Yoshua Bengio is a CIFAR Senior Fellow",
      "startOffset" : 136,
      "endOffset" : 193
    }, {
      "referenceID" : 8,
      "context" : "More recent examples of approaches for semi-supervised deep learning are the semisupervised Variational Autoencoder (Kingma et al., 2014) and the Ladder Network (Rasmus et al.",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : ", 2014) and the Ladder Network (Rasmus et al., 2015) which obtained very impressive, state of the art results (1.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 19,
      "context" : "The Ladder Network adds an unsupervised component to the supervised learning objective of a deep feedforward network by treating this network as part of a deep stack of denoising autoencoders or DAEs (Vincent et al., 2010) that learns to reconstruct each layer (including the input) based on a corrupted version of it, using feedback from upper levels.",
      "startOffset" : 200,
      "endOffset" : 222
    }, {
      "referenceID" : 13,
      "context" : "For a more detailed explanation of the Ladder architecture, see (Rasmus et al., 2015; Valpola, 2014).",
      "startOffset" : 64,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : "Another choice for the combinator function is the Gaussian combinator proposed in the original papers about the Ladder Architecture (Rasmus et al., 2015; Valpola, 2014).",
      "startOffset" : 132,
      "endOffset" : 168
    }, {
      "referenceID" : 11,
      "context" : "Preliminary experiments show that the Leaky Rectifier Linear Unit (LReLU) (Maas et al., 2013) performs better than either the conventional ReLU or the sigmoid unit.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : "Our intuition is that the effect of additive noise to layers is very similar to the weight noise regularization method (Graves, 2011) or dropout (Hinton et al., 2012).",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : "autoencoder (Rezende et al., 2014; Bengio, 2014).",
      "startOffset" : 12,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "When one simply views the Ladder Network as a peculiar type of denoising autoencoder, one could extend the recent work on the generative interpretation of denoising autoencoders (Alain & Bengio, 2013; Bengio et al., 2013) to interpret the Ladder Network as a generative model as well.",
      "startOffset" : 178,
      "endOffset" : 221
    }, {
      "referenceID" : 13,
      "context" : "The paper systematically compares different variants of the recent Ladder Network architecture (Rasmus et al., 2015; Valpola, 2014) with two feedforward neural networks as the baselines and the standard architecture (proposed in the original paper).",
      "startOffset" : 95,
      "endOffset" : 131
    } ],
    "year" : 2017,
    "abstractText" : "Manual labeling of data is and will remain a costly endeavor. For this reason, semi-supervised learning remains a topic of practical importance. The recently proposed Ladder Network is one such approach that has proven to be very successful. In addition to the supervised objective, the Ladder Network also adds an unsupervised objective corresponding to the reconstruction costs of a stack of denoising autoencoders. Although the results are impressive, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. In order to help elucidate and disentangle the different ingredients in the Ladder Network recipe, this paper presents an extensive experimental investigation of variants of the Ladder Network in which we replace or remove individual components to gain more insight into their relative importance. We find that all of the components are necessary for achieving optimal performance, but they do not contribute equally. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connection, followed by the application of noise, and finally the choice of combinator function in the decoder path. We also find that as the number of labeled training examples increases, the lateral connections and reconstruction criterion become less important, with most of the improvement in generalization due to the injection of noise in each layer. Furthermore, we present a new type of combinator functions that outperforms the original design in both fullyand semi-supervised tasks, reducing record test error rates on Permutation-Invariant MNIST to 0.57% for supervised setting, and to 0.97% and 1.0% for semi-supervised settings with 1000 and 100 labeled examples respectively.",
    "creator" : "LaTeX with hyperref package"
  }
}