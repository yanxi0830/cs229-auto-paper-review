{
  "name" : "1702.07028.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the ability of neural nets to express distributions",
    "authors" : [ "Holden Lee", "Rong Ge", "Tengyu Ma", "Andrej Risteski", "Sanjeev Arora" ],
    "emails" : [ "holdenl@princeton.edu", "rongge@cs.duke.edu", "tengyu@cs.princeton.edu", "risteski@princeton.edu", "arora@cs.princeton.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n07 02\n8v 2\n[ cs\n.L G\n] 2\nJ un\n2 01\nWe take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with n hidden layers. A key ingredient is Barron’s Theorem [Bar93], which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of n functions which satisfy certain Fourier conditions (“Barron functions”) can be approximated by a n+ 1- layer neural network.\nFor probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance—a natural metric on probability distributions—by a neural network applied to a fixed base distribution (e.g., multivariate gaussian).\nBuilding up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone."
    }, {
      "heading" : "1 Introduction",
      "text" : "Deep neural networks have led to state-of-the-art performance on classification tasks in many domains such as computer vision, speech recognition, and reinforcement learning ([BCV13; Sch15]). One can view a neural network as a way to learn a function mapping inputs x to outputs y. For image classification, the input is a vector representing an image and the output can be probabilities of being in various classes.\nBut another recent (and less understood) use of neural networks is as generative models for complicated probability distributions, such as distributions over images on ImageNet, handwritten characters from various alphabets, or speech. Here the network may map a stochastic input— such as a uniform normal gaussian—to a realistic image. Such networks are trained using various methods such as variational autoencoders ([KW13], [RMW14]) or generative adversarial networks (GANs) ([Goo+14]). A GAN consists of a repeated zero-sum game between two networks: the\n∗Princeton University, Mathematics Department holdenl@princeton.edu †Duke University, Computer Science Department, rongge@cs.duke.edu ‡Princeton Univerisity, Computer Science Department, tengyu@cs.princeton.edu §Princeton Univerisity, Computer Science Department, risteski@princeton.edu ¶Princeton Univerisity, Computer Science Department, arora@cs.princeton.edu. Supported by NSF grants CCF-\n1302518, CCF-1527371, Simons Investigator Award, Simons Collaboration Grant, and ONR- N00014-16-1-2329\ngenerator attempts to imitate a given probability distribution; it obtains its samples by passing a base distribution (e.g. a gaussian) through its neural network. The discriminator attempts to distinguish between samples from the generator and the true distribution, and thus forces the generator to improve over many repetitions.\nThe current paper is concerned with the following natural question that appears not to have been studied before: Why are deep neural networks so well-suited to efficiently generate many distributions that occur in nature?"
    }, {
      "heading" : "1.1 Our work",
      "text" : "We give a sufficient criterion for a function to be approximable by a neural network with n hidden layers (Theorem 3.1). This criterion holds with respect to any distribution of inputs supported on a compact set. As a consequence of our main result, we obtain a criterion for a distribution to be approximately generated by a neural network with n hidden layers in the Wasserstein metric W2, a natural metric on the space of distributions (Corollary 3.3).\nOur criterion relies on Fourier properties of the function. We build on Barron’s Theorem [Bar93], which says that if a certain quantity involving the Fourier transform is small, then the function can be approximated by a neural network with one hidden layer and a small number of nodes. Calling such a function a Barron function, our criterion roughly says that if a distribution is generated by a composition of n Barron functions, then the distribution can be approximately generated by a neural network with n hidden layers.\nMany nice functions, such as polynomials and ridge functions, are Barron; this property is also preserved under natural operations such as linear combinations. Thus, our result says that if nature creates a distribution by starting from a base distribution (such as a gaussian) and applying a sequence of functions in this class, then we can also generate that distribution with a neural network.\nThis “correspondence” between compositions of Barron functions and multi-layer neural networks raises questions analogous to those raised about neural nets: for example, are compositions of k Barron functions more expressive than Barron functions? Using a technique to lower-bound the Barron constant (Theorem 4.2), we show a separation theorem between Barron functions and composition of Barron functions (Theorem 4.1). This parallels —and is inspired by—the separation between 2-layer and 3-layer neural networks in [ES15]."
    }, {
      "heading" : "1.2 Related work",
      "text" : "Despite the practical success of neural networks, we lack a good theoretical understanding of their effectiveness. An initial attempt to understand the effectiveness of neural networks was by their function approximation properties. A series of works showed that any continuous function in a bounded domain can be approximated by a sufficiently large 2-layer neural network ([Cyb89], [Fun89], [HSW89]). However, the network size can be exponential in the dimension. Barron ([Bar93]) gave a upper bound for the size of the network required in terms of a Fourier criterion. He showed that a function f can be approximated in L2 up to error ε by a 2-layer neural network\nwith O\nÅ C2\nf\nε\nã units, where Cf depends on Fourier properties of f . One remarkable consequence is\nthat representationally speaking, neural nets can evade the curse of dimensionality: the number of parameters required to obtain a fixed error increases linearly, rather than superlinearly, in the\nnumber of dimensions. (Fixing the number of nodes in the hidden layer, the number of parameters scales linearly in the number of dimensions.)\nHowever, such approximability results only explain a small part of the success of neural networks. Firstly, they only deal with 2-layer neural networks. Empirically speaking, deep neural networks— networks with many layers—appear to be much more effective than shallow neural networks. There have been several attempts to explain the effectiveness of deep neural networks. Following the paradigm in circuit complexity, one produces a function f that can be computed by a deep neural network but requires exponentially many nodes to be computed by a shallow neural network. Eldan and Shamir ([ES15]) show a certain radial function can be approximated by a 3-layer neural net but not by a 2-layer neural net with a subexponential number of nodes. [Dan17] shows such a separation but with respect to the uniform distribution on the sphere. Telgarsky ([Tel16]) shows such a separation between k2-layer and k-layer neural networks. Cohen, Sharir, and Shashua ([CSS15]) show a separation for a different model, a certain type of convolutional neural net architecture. Kane and Williams ([KW16]) show super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits, which can be thought of as a boolean analogue to neural networks.\nSecondly, these works—as well as our paper—do not address how to learn neural networks, or why the established method, gradient descent, has been so successful. [Bar93] and [Bar94] address the generalization theory, and show that the nodes can be chosen “greedily”; however the optimization problem is nonconvex. Under the assumption that certain properties of the input distribution (related to the score function) are known and that the function is exactly representable by a 2-layer neural network, Janzamin, Sedghi, and Anandkumar ([JSA15]) give an algorithm inspired by Barron’s Fourier criterion and utilizing tensor decomposition, to learn 2-layer neural networks.\nFinally, we note that the learnability for distributions has been studied for discrete distributions ([Kea+94]).\nOrganization of the paper We explain Barron’s original theorem in Section 2, our criterion for representation by multi-layer neural networks in Section 3, and give our separation result in Section 4. Most proofs and background on Fourier analysis are left in Appendix."
    }, {
      "heading" : "1.3 Notation and Definitions",
      "text" : "First, we formally define the model of a feedforward neural network that we will use.\nDefinition 1.1. A neural network with n hidden layers (also referred to as a n + 1-layer neural network) is defined as follows. A neural network has an associated input space Rm0 , output space Rmn+1 , and n hidden layers of sizes m1, . . . ,mn ∈ N.The neural network has parameters A(l) ∈ Rml−1×ml and b(l) ∈ Rml for 1 ≤ l ≤ n + 1. The neural network has a fixed activation function σ, which is applied component-wise on a vector. On input x ∈ Rm0 , the network computes\nx(0) : = x (1)\nx(l) : = σ(A(l−1)x(l−1) + b(l)) 1 ≤ l ≤ n (2) x(n+1) : = A(n+1)x(n) + b(n+1). (3)\nand outputs x(n+1). This can also be written out in terms of the components:\nx (l) j := σ\n( ml∑\nk=1\nA (l−1) jk x (l−1) k + b (l−1) k\n) .\nCommon choices of activation functions σ include the logistic function 11+e−x , tanh(x), and the ReLU function max{0, x}.\nDefinition 1.2. For a function f : Rm → Rn, define Lip(f) = Lip2(f), the Lipschitz constant of f with respect to the L2 norm, by\ninf {C : ∀x, y, ‖f(x)− f(y)‖2 ≤ C ‖x− y‖2} .\nLet Bn be the unit ball in n dimensions{x ∈ Rn : ‖x‖ ≤ 1}. For sets A,B and a scalar r, let\nA+B := {x+ y : x ∈ A, y ∈ B} , rA := {rx : x ∈ A} . (4)\nFor example, rBn denotes the ball of radius r in n dimensions, and A + rBn is the neighborhood of radius r around A.\nLet ‖·‖ = ‖·‖2 denote the usual Euclidean norm on vectors in Rn. For a function f , let f∨(x) := f(−x). (This notation is often used in Fourier analysis.) Let f (n)(x) = dndxn f(x) denote the nth derivative, and ∆f = ∑n i=1 ∂2\n∂xi2 f denote the Laplacian."
    }, {
      "heading" : "2 Barron’s Theorem",
      "text" : "For f ∈ L1(R) we define the Fourier transform of f : Rn → R with the following normalization.\nf̂(ω) := 1\n(2π)n\n∫\nRn f(x)e−i〈ω,x〉 dx. (5)\nFor vector-valued functions f : Rn → Rm, define the Fourier transform componentwise. The inverse Fourier transform is\n(F−1g)(x) := ∫\nRn g(ω)ei〈ω,x〉 dx = (2π)nĝ∨\nThe Fourier inversion formula, which holds for all sufficiently “nice” functions, is\nf(x) =\n∫\nRn f̂(x)ei〈ω,x〉 dx. = (2π)n ˆ̂f∨\nFor background on Fourier analysis with rigorous statements, see Appendix A. [Bar93] defines a norm on functions defined on a set B, and shows that a small norm implies that the function is amenable to approximation by a neural network with one hidden layer.\nDefinition 2.1. For a bounded set B ⊆ Rp let ‖ω‖B = supx∈B | 〈ω, x〉 |. For a function f : Rn → R, define the norm ‖f‖∗B := ∫ Rn ‖ω‖B |f̂(ω)| dω.\nWhen B = Bn is the unit ball, ‖ω‖B = ‖ω‖2. In this case, using Theorem A.3,\n‖f‖∗B = ∫\nRn ‖ω‖ |f̂(ω)| dω =\n∥∥∥ ∥∥∥ωf̂ ∥∥∥ 2 ∥∥∥ 1 = ∥∥∥ ∥∥∥∇̂f ∥∥∥ 2 ∥∥∥ 1\nwhere for a function g : Rn → Rn, ‖g‖2 is thought of as a function Rn → R, and ‖‖g‖2‖1 is the L1 norm of this function.\nWe would like to define this norm for functions f : B → R. However, the Fourier transform is defined for functions f : Rn → R. Because we only care about the value of f on B, we allow arbitrary extension outside of B.\nDefinition 2.2. Let B ⊆ Rn. Let FB be the set of functions for which the Fourier inversion formula holds on B after subtracting out g(0):1\nFB = ß g : Rn → R : ∀x ∈ B, g(x) = g(0) + ∫ (ei〈ω,x〉 − 1)ĝ(ω) dω ™ .\nDefine ΓB = {f : B → R : ∃g, g|B = f, g ∈ FB}, let ΓB(C) be the subset with norm ≤ C ΓB(C) = {f : B → R : ∃g, g|B = f, ‖g‖∗B ≤ C, g ∈ FB}. We say that a function f ∈ ΓB(C) is C-Barron on B. For a function f : B → R, let Cf,B be the minimal constant for which f ∈ ΓB,C :\nCf,B := inf g|B=f,g∈FB\n∫\nRn ‖ω‖B |ĝ(ω)| dω. (6)\nWhen the set B is clear, we just write Cf .\nThis definition is non-algorithmic. How to compute or approximate the Barron constant in general is an open problem. The difficulty stems from the fact that we have to take an infimum over all possible extensions. The Barron constant can be upper-bounded by choosing any extension f , but is more difficult to lower-bound. We will give a technique to lower-bound the Barron constant in Theorem 4.2.\nWe give some intuition on the Barron constant. First, in order for the Barron constant to be finite, f must be continuously differentiable. Indeed, the inverse Fourier transform of ωf̂(ω) is −i∇f(x), and integrability of a function implies continuity of its (inverse) Fourier transform, so ∇f is continuous.\nSecond, the Barron constant will be larger when f̂ is more “spread out.” One can think of ‖g‖B as a kind of L1 norm. This makes sense in the context of neural networks, because if f(x) = ∑k i=1 ciσ(〈ai, x〉 + bi) then f has Fourier transform completely supported on the lines in the direction of the ai. 2 One can think of the Barron constant as a L1 relaxation of this “sparsity” condition. Barron’s Theorem gives an upper bound on how well a function can be approximated by a neural network with 1 hidden layer of k nodes, in terms of the Barron constant. For a list of functions with small Barron constant, as well as the effect of various operations on the Barron constant, see [Bar93, §IX]. Examples of Barron functions include polynomials of low degree, ridge functions, and linear combinations of Barron functions.\nDefinition 2.3. A sigmoidal function is a bounded measurable function f : R → R such that limx→−∞ f(x) = 0 and limx→∞ f(x) = 1.\n1This is a strictly larger set than functions for which the Fourier inversion formula holds. 2Here f does not approach 0 as ‖x‖ → ∞, so the Fourier transform must be understood in the sense of distributions.\nTheorem 2.4 (Barron, [Bar93]). Let B ⊆ Rn be a bounded set, and µ any probability measure on B. Let f ∈ ΓB(C) and σ be sigmoidal. There exist ai ∈ Rn, bi ∈ R, ci ∈ R with ∑k i=1 |ci| ≤ 2C\nsuch that letting fk(x) = ∑k i=1 ciσ(〈ai, x〉+ bi), we have\n‖f − fk‖2µ := ∫\nB (f(x)− fk(x))2 µ(dx) ≤\n(2C)2\nk .\nBarron’s Theorem works for the logistic function (which is sigmoidal), hyperbolic tangent (which is sigmoidal if rescaled to [0, 1]), and ReLU up to a factor of 2 in the number of nodes. Even though the ReLU function ReLU(x) = max{0, x} is not sigmoidal, the linear combination ReLU(x) = ReLU(x)− ReLU(x− 1) is.\nNote that Barron’s Theorem doesn’t give approximability tailored to a specific measure µ; it simultaneously gives approximability for all µ defined on B, and up to any degree of accuracy. This is why some degree of smoothness is necessary for f : otherwise, µ could be concentrated on the regions where B is not smooth. Note that approximability for all µ will be crucial to the proof of the main theorem (Theorem 3.1). 3"
    }, {
      "heading" : "3 Multilayer Barron’s Theorem",
      "text" : ""
    }, {
      "heading" : "3.1 Main theorem",
      "text" : "Barron’s Theorem says that a Barron function can be approximated by a neural net with 1 hidden layer. From this, it is reasonable to suspect that a composition of l Barron functions can be approximated by a neural network with l hidden layers. Our main theorem says that this is the case; we give a sufficient criterion for a function to be approximated by a neural network with l hidden layers, on any distribution supported in a fixed set K0.\nWe note two caveats: first, fi need to be Lipschitz to prevent the error from blowing up. Second, we will need our functions fi to be Barron on a slightly expanded set (assumption 3), because an approximation gi to fi could take points outside Ki, and we need to control the error for those points.\nGiven a sequence of functions fi and j ≥ i, let fj:i := fj ◦ fj−1 ◦ · · · ◦ fi.\nTheorem 3.1 (Main theorem). Let ε, s > 0 be parameters, and l ≥ 1. For 0 ≤ i ≤ l let mi ∈ N. Let fi : R\nmi−1 → Rmi be functions, µ0 be any probability distribution on Rm0 , and Ki ⊂ Rmi be sets.\nSuppose the following hold.\n1. (Support of initial distribution) Supp(µ0) ⊂ K0.\n2. (fi is Lipschitz) Lip(fi) ≤ 1. 3Although Barron’s Theorem seems to require a strong smoothness assumption, we can approximate any continuous function arbitrarily well with a smooth function and then apply Barron’s Theorem. A converse to Barron’s Theorem cannot hold in the form stated, because if ‖ai‖ is not restricted, then σ(〈ai, x〉+ bi) could have large gradient; the Barron constant of φ(〈ai, x〉+ bi) would scale as ‖ai‖. It is natural to ask whether we can choose the ai to have bounded norm. Barron [Bar93, Theorem 3] shows a version of the theorem that produces a representation with ‖ai‖ ≤ τ , but that incurs an additive error Cτ in the approximation. Note that the following weak converse holds: the Barron constant of f = c0 + ∑r i=1 ciσ(〈ai, x〉 + bi) is bounded by\nO(diam(K) ∑r\ni=1 |ci| ‖ai‖).\n3. (fi is Barron) f1 ∈ ΓK0(C0) and for 1 ≤ i ≤ l, fi ∈ ΓKi−1+sBmi−1 (Ci).\n4. (fi takes each set to the next) fi(Ki−1) ⊆ Ki Suppose that the diameter of Kl is D. Then there exists a neural network g with l hidden layers\nwith ⌈ 4C2i mi\nε2\n⌉ nodes on the ith layer, so that\nÇ∫\nK0\n‖fl:1 − g‖2 dµ0 å 1 2 ≤ lε   (2Cl √ ml +D)2 l\n3s2 + 1. (7)\nWe prove this in Section 3.3. It is crucial to the proof that Barron’s Theorem simultaneously gives approximability for all probability distributions on a given set.\nNote that if Kl−1 is a ball of radius r, by the way we defined the norm ‖·‖Kl−1 in the Barron constant, Cl will at least scale as s + r. If we set s to be on the same order as r, then the RHS of (7) is on the order of l 3 2m 1 2\nl ε."
    }, {
      "heading" : "3.2 Approximating probability distributions",
      "text" : "Theorem 3.1 can be interpreted in a very natural way when the aim is to approximate the probability distribution fl:1(x), x ∼ µ0. The Wasserstein distance is a natural distance defined on distributions.\nDefinition 3.2. Let µ, ν be two probability distributions on Rn. Let Γ(µ, ν) denote the set of probability distributions on Rn × Rn whose marginals on the first and second factors are µ and ν respectively. (A distribution γ ∼ Γ(µ, ν) is called a coupling of µ, ν.) For 1 ≤ p < ∞, define the pth Wasserstein distance by\nWp(µ, ν) =\nÇ inf\nγ∈Γ(µ,ν)\n∫\nRn×Rn ‖x− y‖p2 dγ(x, y)\nå 1 p\nWhen p = 1, this is also known as the “earth mover’s distance.” One can think of it as the minimum “effort” required to change the distribution of µ to that of ν by shifting probability mass (where “effort” is an integral of mass times distance).\nCorollary 3.3. Keep the notation in Theorem 3.1 and suppose the diameter of the set fl:1(K0) is D. Then the Wasserstein distance between the distribution fl:1(X)(X ∼ µ0) and g(X), (X ∼ µ0) is at most lε » 1 + (2Cl √ ml +D)2 l 3s2 .\nThe proof of this is simple: observe that (fl:1(X), g(X)), X ∼ µ0 defines a coupling between the distributions. Thus by Theorem 3.1 the W2 Wasserstein distance is at most\nñ E\nX∼µ0 ‖fl:1(X)− g(X)‖2\nô 1 2 ≤ lε   (2Cl √ ml +D)2 l\n3s2 + 1.\nThe Wasserstein distance is a suitable metric in the context of GANs ([AB17], [ACB17]). One way to model a discriminator is as a function f in a certain class F that maximizes the difference between Ef on the real distribution µ and the generated distribution ν,\nsup f∈F ∣∣∣∣ Ex∼µ f(x)− Ey∼ν f(y) ∣∣∣∣ . (8)\nThis is called the maximal mean discrepancy ([KBG04], [DRG15]). The Wasserstein distance captures the idea that if two distributions are close, then it is hard for such a Lipschitz discriminator to tell the difference, as the following lemma shows.\nLemma 3.4 (Properties of Wasserstein metric). For any two distributions µ, ν over Rn, W1(µ, ν) ≤ W2(µ, ν). Moreover, for any Lipschitz function f : R\nn → R, ∣∣∣∣ Ex∼µ f(x)− Ey∼ν f(y) ∣∣∣∣ ≤ Lip(f)W1(µ, ν). (9)\nProof is deferred to Appendix C. In the context of Corollary 3.3, Lemma 3.4 says that the distribution generated by fl:1 and by the neural network cannot be distinguished by a Lipschitz function. [ACB17] discuss why the class of Lipschitz functions is a good choice in comparison to other classes. For instance, if we maximize over the class of indicator functions (of measurable sets) instead, (8) becomes the total variation (TV) distance, which is unstable under perturbations to the function generating the distribution. In particular, the TV distance is discontinuous under perturbations of distributions supported on lower-dimensional subsets of the ambient space Rn."
    }, {
      "heading" : "3.3 Proof of main theorem",
      "text" : "To prove Theorem 3.1 we first prove the following theorem.\nTheorem 3.5. Keep conditions 1–4 and the notation of Theorem 3.1. Then there exists a neural network g with l hidden layers and S ⊂ Rm0 satisfying µ0(S) ≥ 1− Ä∑l−1 i=1 i 2 ä ε2 s2 so that\nÅ∫ 1S ‖fl:1 − g‖2 dµ0 ã 1 2 ≤ lε (10)\nProof. Let ri = ⌈ 4C2i mi\nε2\n⌉ . We will show that we can take g = gl:1, where g1, . . . , gl are functions\ndefined by\ngi : R mi−1 → Rmi (11)\n(gi(x))j = cij0 + ri∑\nk=1\ncijkσ(〈aijk, x〉+ bijk), (12)\nfor some parameters cijk, bijk ∈ R, aijk ∈ Rmi−1 . Note that each gi is a neural net with one hidden layer and a linear output layer. When the next layer gi+1 is applied to the output y of gi, first linear functions 〈ai+1,j,k, y〉 + bi+1,j,k are applied; these linear functions can be collapsed with the linear output layer of gi. Thus only one hidden layer is added each time.\nWe prove the statement by induction on l. For l = 1, the theorem follows directly from Barron’s Theorem 2.4, using assumptions 1 and 3.\nFor the induction step, assume we have functions g1, . . . , gl−1 satisfying the conclusion for f1, . . . , fl−1. Let Sl−1 be the set in the conclusion. Apply Barron’s Theorem 2.4 to fl to get that that for each 1 ≤ j ≤ ml, for any µ supported on a set K ′l−1 ⊆ Rml−1 and any rl ∈ N, there exists a neural net gl,j with 1 hidden layer with rl nodes such that\nÅ∫\nR ml−1\n[(fl)j − (gl)j]2 dµ ã 1 2 ≤ 2Cfl,K ′l−1√\nrl .\nNote it is vital here that Barron’s Theorem applies to any distribution µ supported on K ′l−1. Let\nSl = Sl−1 ∩ ¶ x : gl−1:1(x) ∈ Kl−1 + sBml−1 © . Apply Barron’s Theorem with K ′l = Kl + sBml , rl =°\n4C2l ml ε2 § . µ = gl−1:1∗(1Slµ0).\n4 We have that µ is supported on gl−1:1(Sl) ⊆ Kl−1+sBml−1 = K ′l−1, as required, and fl is Cl-Barron on this set by assumption 3. (Note that µ is not a probability measure because it was restricted to the set gl−1:1(Sl), but it is a nonnegative measure with total L1 mass at most 1. Because Barron’s Theorem holds for any probability measure, it also holds for these measures.) The conclusion of Barron’s Theorem gives (gl)j such that\nÅ∫\nR ml−1\n[(fl)j − (gl)j]2 d(gl−1:1∗(1Slµ0)) ã 1\n2 ≤ 2Cl√ rl ≤ ε√ ml\n(13)\n=⇒ Å∫\nR ml−1\n‖fl − gl‖2 d(gl−1:1∗(1Slµ0)) ã 1 2 ≤ ε (14)\nWe bound by the triangle inequality\nÅ∫\nRm 1Sl ‖fl:1 − gl:1‖2 dµ0\nã 1 2\n≤ Å∫\nRm 1Sl ‖fl ◦ fl−1:1 − fl ◦ gl−1:1‖2 dµ0\nã 1 2\n+\nÅ∫\nRm 1Sl ‖fl ◦ gl−1:1 − gl ◦ gl−1:1‖2 dµ0\nã 1 2\n≤ Å∫\nRm 1Sl ‖fl ◦ fl−1:1 − fl ◦ gl−1:1‖2 dµ0\nã 1 2\n+\nÅ∫\nR ml−1\n‖fl − gl‖2 dgl−1:1∗(1Slµ0) ã 1 2\n≤ Lip(fl) Å∫\nRm 1Sl ‖(fl−1:1 − gl−1:1)‖2 dµ0\nã 1 2\n+ ε\n≤ Lip(fl) Å∫\nRm 1Sl−1 ‖(fl−1:1 − gl−1:1)‖2 dµ0\nã 1 2\n+ ε\n≤ 1 · (l − 1)ε + ε = lε\nThe last inequality holds by assumption 2 and the induction hypothesis. To finish, we have to check that µ0(Sl) ≥ 1− Ä∑l−1 i=1 i 2 ä ε2 s2 . As above, we have that\n∫ 1Sl−1 ‖fl−1:1 − gl−1:1‖2 dµ0 ≤ (l − 1)2ε2\nby the induction hypothesis. Also, fl−1:1(x) ∈ Kl−1 for all x ∈ Supp(µ0) by assumption 4. Thus by Markov’s inequality and the induction hypothesis on Sl−1,\nµ0(Sl−1 ∩ ¶ x : x 6∈ Kl−1 + sgl−1:1(Bml−1) © )\n≤ µ0(Sl−1 ∩ {x : ‖fl−1:1(x)− gl−1:1(x)‖ ≥ s}) ≤ (l − 1)2ε2\ns2\nTherefore µ0(Sl) ≤ µ0(Sl−1)− (l−1) 2ε2 s2 ≤ 1− Ä∑l−1 i=1 i 2 ä ε2 s2 .\nIt is inelegant to have to exclude the sets Sl. The main theorem is a statement that doesn’t involve the sets Sl. We achieve this by using the trivial bound on S c l .\n4The pushforward of a measure µ by a function f is denoted by f∗µ and defined by f∗µ(S) = µ(f −1(S)). Here,\ngl−1:1∗(1Slµ0)(S) = µ0(g −1 l−1:1(S) ∩ Sl).\nProof of Theorem 3.1. The functions g1, . . . , gl in Theorem 3.5 satisfy ∫ Sl ‖fl:1 − gl:1‖2 dµ0 ≤ l2ε2.\nThe range of gl = ((gl)1, . . . , (gl)ml) is contained in a set of diameter 2Cl √ ml because the function σ has range contained in [0, 1] and Barron’s Theorem gives functions (gl)j , 1 ≤ j ≤ ml, with∑r k=1 |cljk| ≤ 2Cl. Choose a constant vector k to minimize\n∫ Sl ‖fl:1(x)− gl:1(x)− k‖2 dµ0 and replace gl with\ngl + k. Note that now, the range of gl and fl necessarily overlap; otherwise a further translation will decrease this error. We still have ∫ Sl ‖fl:1 − gl:1‖2 dµ0 ≤ l2ε2. Moreover, ‖gl(x)− fl(x)‖ ≤\n2Cl √ ml +D for any x ∈ K0.\nNow we have (using µ0(S c l ) ≤ Ä∑l−1 i=1 i 2 ä ε2 s2 ≤ l3ε2 3s2 )\n∫\nK0\n‖fl:1 − gl:1‖2 dµ0 ≤ ∫\nSl\n‖fl:1 − gl:1‖2 dµ0 + ∫\nSc l\n‖fl:1 − gl:1‖2 dµ0 (15)\n≤ l2ε2 + (2Cl √ ml +D) 2 l 3ε2\n3s2 . (16)\nTaking square roots gives the theorem."
    }, {
      "heading" : "4 Separation between Barron functions and composition of Bar-",
      "text" : "ron functions\nIn this section we produce an explicit function f : Rn → R that is a composition of two poly(n)Barron functions, but is not O(cn)-Barron for some c > 1.\nTheorem 4.1. For any n ≡ 3 (mod 4) and c > 1, there exists a function f and C2 > 0 such that\n1. (f is not Barron) Cf,C2nBn ≥ cn.\n2. (f is the composition of 2 Barron functions) f = j ◦ k where for all r, s > 0, k : Rn → R is O(nr3)-Barron on rBn, and j : R → R is O(sn2)-Barron on sB1.\nThe condition n ≡ 3 (mod 4) is not necessary; we include it only to avoid case analysis. Note that this theorem gives a separation between Barron functions and compositions of Barron functions, and does not give a separation between distributions expressible by Barron functions and compositions of Barron functions. The analogous question for distributions is an open problem.\nWe will choose f to be a certain radial function f = f1(‖x‖) defined in Section 4.1.5 In order for f to have large Barron constant, it is necessary for ∫ Rn\n‖ω‖2 |f̂(ω)| dω to be large, i.e. for f̂ to have significant mass far away from the origin. We ensure this holds by choosing f to change sharply in the radial direction. This means f̂ has mass far away from the origin. Moreover, f̂ is radial because f is radial, so f̂ has significant mass in a large shell.\nHowever, lower-bounding ∫ Rn\n‖ω‖2 |f̂(ω)| dω is not sufficient because the definition of the Barron constant requires us to bound this quantity over all extensions of f .\nTo solve this problem, we give a technique to lower bound the Barron constant in Section 4.2 (Theorem 4.2). Although we cannot certify f is Barron by showing ∫ Rn ∥∥∥∇̂f(ω) ∥∥∥ dω = ∫ Rn\n‖ω‖2 |f̂(ω)| dω is large, it suffices to show ∫ Rn ∥∥∥÷(∇f)g(ω) ∥∥∥ dω is large for a judiciously chosen g. We use this to show that f is not Barron in Section E.1 (Theorem E.4).\n5For any radial function a : Rn → R, we write a1 : R → R for the function such that a(x) = a1(‖x‖).\nWe will see in Section 4.3 (Theorem 4.4) that f is a composition of two Barron functions x 7→ ‖x‖2 and y 7→ f1( √ y). The function x 7→ ‖x‖2 is Barron because it is a polynomial. The function y 7→ f1( √ y) is a function in 1 variable, and it is much easier for a 1-dimensional function h to be Barron as bounds on h, h′, and h′′ suffice (Lemma A.6). Our result is similar to the construction in [ES15] of an explicit function that can be approximated by a 3-layer neural net but cannot be approximated (to better than constant error) by any 2-layer neural net with subexponential number of units. [ES15] use a different Fourier criterion in order to prove a certain function is not computable by a two-layer neural network.\nRoughly speaking, Eldan and Shamir implicitly show that for a specific probability measure that they chose (ϕ2, where ϕ̂ = 1RnBn , where Rn is chosen so that Vol(RnBn) = 1), a necessary criterion for f to be approximated by a 2-layer neural network with k nodes is that most of its mass is concentrated in k “tubes” ⋃k i=1(span{vi}+RnBn). (See [ES15, Proposition 13, Claim 15, Lemma 16].) The idea can be adapted to other measures. The main difference from Barron’s Theorem is that their criterion is a necessary condition for approximability (so useful to show lower bounds), is measure-specific (rather than agnostic to the measure), and is more similar to a “sparsity” condition than a “L1 measure” as in Barron’s Theorem."
    }, {
      "heading" : "4.1 Definition of f",
      "text" : "Let f1 : R → R be a function such that f1 is nonnegative, Supp(f1) ⊆ [K1,K1+ε], ∫∞ 0 f1(x) dx = 1,\nand |f (i)1 | = O Ä 1 εi+1 ä for all i = 0, 1, 2. This function exists by Lemma D.1(1). We will choose K1, ε depending on n. By Theorem A.5,\nf̂(ω) = 1\n2π\nÇ 1\n2π ‖ω‖\nån 2 −1 ∫ ∞\n0 r\nn 2 −1f1(r)Jn\n2 −1(‖ω‖ r) dr. (17)\nWe will choose [K1,K1 + ε] to be an interval on which Jn 2 (‖ω‖ r) is large and positive for some large ‖ω‖. We use the notation of Lemma B.1. For x ≥ n,\n(fn,xx) ′ = x√ x2 − Ä n2−1 4\nä − √ n2 − 1 2 · 1√ 1− n2−1\n4x2\n· − √ n2 − 1 2x2 =   1− n 2 − 1 4x2 ∈ [  3 4 , 1 ] .\nLet K3 = C3 √ n for some C3 to be chosen. In every interval of length ≥ 4π\nK3 √ 3/4 there is an interval\nof length ≥ πK3 on which\ncos Ç −(n+ 1)π\n4 + fd,K3rK3r\nå ≥ 1√\n2 . (18)\nLet [K1,K1 + ε] be the first such interval with K1 ≥ C1 √ n, where C1 is a constant to be chosen.\nNote we have K1 ∼ C1 √ n and ε = Θ Ä 1 K3 ä ."
    }, {
      "heading" : "4.2 A technique to lower bound the Barron constant",
      "text" : "The main difficulty in showing a function is not Barron is to lower bound the integral ∫\nRn ‖ω‖ |“F (ω)| dω =\n∫\nRn\n∥∥∥‘∇F (ω) ∥∥∥ dω\nover all extensions F of f . In general, it is not known how to calculate the infimum over all extensions.\nTheorem 4.2 gives us a way to lower-bound the Barron constant for f over a ball rBn. The idea is the following. Instead of bounding ∫ Rn ∥∥∥‘∇F (ω) ∥∥∥ dω for every extension F , we choose g with support in B and compute ∫ Rn ∥∥∥◊ (∇F )g(ω) ∥∥∥ dω. This does not depend on the extension F because (∇F )g = (∇f)g. It turns out that we can bound ∫ Rn ∥∥∥‘∇F (ω) ∥∥∥ dω in terms of ∫ Rn ∥∥∥◊ (∇F )g(ω) ∥∥∥ dω.\nTheorem 4.2. If f is differentiable, then for any g such that Supp(g) ⊆ rBn and g, ĝ ∈ L1(Rn),\nCf,rBn ≥ r ∫ Rn |÷(∇f)g(ω)| dω∫ Rn |ĝ(ω)| dω\nNote that g is a function that we are free to choose. To use the theorem we will choose g with Supp(g) ⊆ C2nBn and ∫ Rn\n|ĝ(ω)| dω small. This theorem is similar to [Bar93, §IX.11], which bounds the Barron constant of a product of two functions. We defer the proof to Appendix E.\nTo use this bound for a function f , we need to judiciously choose the function g. Let b be the “bump” function given by Lemma D.1(3) for m = n+12 . This function has the properties that b(x) = 1 for x ∈ [−1, 1], b(x) = 0 for |x| ≥ 2, and for k ≤ m, b(k)(x) ≤ (n + 1)k. Let g1(x) = b(K2)(x) = b Ä x K2 ä and g(x) = g1(‖x‖) for K2 = C2n, where C2 is a constant to be chosen.\nIn Appendix E, we show the following lemma that bounds the Barron constant for f .\nLemma 4.3. For n ≡ 3 (mod 4) and constants C1, C2, C3 such that C1C3 ≥ 32 , C2 > C1 ≥ 1, C3 ≥ 1, the functions f, g we choose satisfy\n∫\nRn |ĝ(ω)| dω = O((5eC2)\nn 2 ), (19)\n∫\nRn\n∥∥∥÷(∇f)g(ω) ∥∥∥ dω = Ω(C n 2 −3 1 C n 2 3 n − 1 2 e n 2 ). (20)\nAs a result the Barron constant Cf,2K2Bn ≥ Ω Å 2−nC n 2 −3 1 C n 2 3 C −(n2−1) 2 n 1 2 ã .\nTherefore, as long as we choose C3 to be large enough this constant is exponentially large. The constraint that n ≡ 3 (mod 4) is only there to avoid case analysis. We give the proof in Section E."
    }, {
      "heading" : "4.3 h is a composition of Barron functions",
      "text" : "We can write f as the composition of a function that computes the square norm, and a one dimensional function. The Barron constant for both functions can be bounded by polynomials.\nLemma 4.4. Suppose that C1 < C3. f is the composition of the two functions\nx 7→ ‖x‖2 Rn → R (21) y 7→ f1( √ y) R → R. (22)\nThe function x 7→ ‖x‖2 satisfies C‖x‖2,rBn ≤ O(nr 3) and the function y 7→ f1(\n√ y) satisfies\nCf1( √ y),[−s,s] = O(sC\n1 2 1 C 3 2 3 n 2) for any s.\nIntuitively, the proof uses the fact that polynomials are Barron, and all “nice” one dimensional functions are Barron. We leave the detailed proofs in Section E. Now it is easy to see the separation:\nof Theorem 4.1. By Lemma 4.3, we know we can choose C3 large enough so that the Barron constant for f is exponential. On the other hand, by Lemma 4.4 we know f is a composition of two Barron functions."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper we show if a generative model can be expressed as the composition of n Barron functions, then it can be approximated by a n+ 1-layer neural network. Along the way we proved a multi-layer version of the Barron’s Theorem [Bar93], and a key observation is to use Wasserstein distance W 2 as the distance measure between distributions. This partly explains the expressive power of neural networks as generative models. However, there are still many open problems: what natural transformations can be represented by a composition of Barron functions? Is there a separation between composition of n Barron functions and composition of n+1 Barron functions? How can we learn such a representation efficiently? We hope this paper serves as a first step towards understanding the power of deep generative models."
    }, {
      "heading" : "A Background from Fourier Analysis",
      "text" : "The Fourier transform is defined in (5).\nTheorem A.1 (Fourier inversion). For continuous f such that f ∈ L1(Rn) and f̂ ∈ L1(Rn),\nf(x) = ∫ f̂(x)ei〈ω,x〉 dx. = (2π)n ˆ̂f∨\nTheorem A.2 (Plancherel’s Theorem). For f, g : Rn → C such that f, g ∈ L1(Rn) ∩ L2(Rn), ∫\nRn f(x)g(x) dx =\n∫\nRn (2π)nf̂(ω)ĝ(ω) dω.\nTheorem A.3 (Fourier transform of derivative). For differentiable f : Rn → R, f ∈ L1(Rn),\n∇̂f(x) = ixf̂(x).\nFor f : Rn → R such that f, ‖x‖ f ∈ L1(Rn),\n(xf)∧ = i∇f̂(x).\nTheorem A.4 (Fourier transform of convolution). For f, g ∈ L1(Rn)\n’f ∗ g(x) = f̂(ω)ĝ(ω) (23)\nFor f, g ∈ L1(Rn) with fg, f̂ , ĝ ∈ L1(Rn),\nf̂ g(x) = (f̂ ∗ ĝ)(ω). (24)\nTheorem A.5 (Fourier transform of radial function). Suppose f(x) = f1(‖x‖) where f ∈ L1(Rn), f : R≥0 → R. Then\nf̂(ω) = 1\n2π\nÇ 1\n2π ‖ω‖\nån 2 −1 ∫ ∞\n0 r\nn 2 −1f1(r)Jn\n2 −1(‖ω‖ r) dr.\nwhere Jα is the Bessel function of order α.\nLemma A.6 (L1 bound on Fourier transform).\n1. Let k ≥ n+12 and k be even. Then for g : Rn → R that is k times differentiable,\n∫\nRn ‖ĝ(ω)‖ dω ≤\nÑ Γ Ä 1 2 ä\n2nπ n 2 Γ Ä n+1 2\nä é 1 2 Å∫\nRn [(I −∆)k2 g(x)]2 dx\nã 1 2\n. (25)\n2. Let h : R → R be once or twice differentiable, respectively. Then ∫ ∞\n−∞ |ĥ(ω)| dω ≤ 2− 12 Å∫ ∞ −∞ |h|2 + |h′|2 dx ã 1 2\n(26)\n∫ ∞\n−∞ |ωĥ(ω)| dω ≤ 2− 12 Å∫ ∞ −∞ |h′|2 + |h′′|2 dx ã 1 2 . (27)\nProof. By Cauchy-Schwarz and the fact that ∫ Rn\n1\n(1+‖ω‖2) n+1 2\ndω = π\nn 2 Γ( 12)\nΓ(n+12 ) (this is used e.g. to\ndefine the Cauchy probability distribution)\n∫\nRn ‖ĝ(ω)‖ dω ≤\nÑ∫\nRn 1 Ä 1 + ‖ω‖2\näk dω é 1 2 Å∫\nRn (1 + ‖ω‖2)k|ĝ(ω)|2 dω\nã 1 2\n(28)\n≤\nÖ ∫\nRn 1 Ä 1 + ‖ω‖2 än+1 2 dω\nè 1 2 Å∫\nRn (1 + ‖ω‖2)k|ĝ(ω)|2 dω\nã 1 2\n(29)\n≤ Ñ π n 2 Γ Ä 1 2 ä\nΓ Ä n+1 2\nä é 1 2 Å∫\nRn\n∣∣∣(1 + ‖ω‖2)k2 ĝ(ω) ∣∣∣ 2 dω\nã 1 2\n(30)\n≤ Ñ π n 2 Γ Ä 1 2 ä\nΓ Ä n+1 2\nä é 1 2 (2π)− n 2 Å∫\nRn [(I −∆)k2 g(x)]2 dx\nã 1 2\n(31)\nwhere in the last step we used Theorem A.2 and the calculation\n∆̂g =\n( n∑\ni=1\n∂2\n∂xi2 g\n)∧ = − n∑\ni=1\nω2i ĝ(ω) = −‖ω‖2 ĝ(ω).\nFor the second part, again by Cauchy-Schwarz and “h′(ω) = iωh(ω), ∫ ∞\n−∞ |ĥ(ω)| dω ≤ Ç∫ ∞ −∞\n1 1 + |ω|2 dω ∫ ∞ −∞ |ĥ(ω)|2(1 + |ω|2) dω å1 2\n(32)\n≤ √ π Å∫ ∞ −∞ |ĥ|2 + |“h′|2 dω ã 1 2\n(33)\n≤ √ π(2π)− 1 2 Å∫ ∞ −∞ |h|2 + |h′|2 dx ã 1 2 . (34)\nThis gives the first equation. To get the second, replace h with h′."
    }, {
      "heading" : "B Bessel functions",
      "text" : "We will need some facts about Bessel functions Jα(x), α ∈ R. Jα(x) has an oscillating shape like a damped sinusoid.\nLemma B.1 ([Kra14, Theorem 5], [ES15, Lemma 21]). If d ≥ 2 and x ≥ d, then ∣∣∣∣∣Jd/2(x)− √ 2\nπcd,xx cos\nÇ −(d+ 1)π\n4 + fd,xx å∣∣∣∣∣ ≤ x −3/2,\nwhere\ncd,x =\n  1− d\n2 − 1 4x2 , fd,x = cd,x + √ d2 − 1 2x arcsin Ç√ d2 − 1 2x å .\nMoreover, assuming x ≥ d, 1 ≥ cd,x ≥ 1− 0.15 d\nx ≥ 0.85\nand\n1.3 ≥ 1 + 0.3 d x ≥ fd,x ≥ 1− 0.15 d x ≥ 0.85.\nLemma B.2 ([ES15, Lemma 20]). For any α ≥ 1 and x ≥ 3α, Jα(x) is 1-Lipschitz in x."
    }, {
      "heading" : "C Properties of Wasserstein Distance",
      "text" : "Lemma C.1 (Lemma 3.4 restated). For any two distributions µ, ν over Rn,\nW1(µ, ν) ≤ W2(µ, ν). (35)\nMoreover, for any Lipschitz function f : Rn → R, ∣∣∣∣ Ex∼µ f(x)− Ey∼ν f(y) ∣∣∣∣ ≤ Lip(f)W1(µ, ν). (36)\nProof. Let γ ∈ Γ(µ, ν) be a coupling of µ, ν. Then by the Cauchy-Schwarz inequality,\nW1(µ, ν) ≤ ∫\nRn×Rn ‖x− y‖2 dγ(x, y) (37)\n≤ Å∫\nRn×Rn ‖x− y‖22 dγ(x, y)\nã 1 2 Å∫\nRn×Rn dγ\nã2\n︸ ︷︷ ︸ 1\n. (38)\nThe infimum of (38) over all couplings γ ∼ Γ(µ, ν) is exactly W2(µ, ν). This shows (35). Now for any γ ∈ Γ(µ, ν), because its marginals are µ and ν,\n∣∣∣∣ Ex∼µ f(x)− Ey∼ν f(y) ∣∣∣∣ = ∣∣∣∣ ∫ Rn×Rn f(x)− f(y) dγ(x, y) ∣∣∣∣ (39)\n≤ Lip(f) ∫\nRn×Rn ‖f(x)− f(y)‖2 dγ(x, y). (40)\nThe Lipschitz constant is with respect to the L2 norm because we use the L2 norm to measure the distance between f(x) and f(y). Taking the infimum of (40) gives (36).\nIn fact, (36) is sharp when µ, ν have bounded support. The duality theorem of Kantorovich and Rubinstein ([KR58]) says that\nW1(µ, ν) = sup ß E\nx∼µ f(x)− E y∼ν f(y) : f : Rn → R,Lip(f) ≤ 1\n™ ."
    }, {
      "heading" : "D Test functions",
      "text" : "For a function f , let f(K)(x) := f ( x K ) . Lemma D.1. Let m ≥ 2 be a given positive integer. 1. There exists a function g : R → R with the following properties.\n(a) g ≥ 0 everywhere. (b) Supp(g) ⊆ [0, 1]. (c) ∫ 1 0 g(x) dx = 1. (d) g is m times continuously differentiable and for all k ≤ m, |g(k)(x)| = O((2m)k+1).\nThe function 1K g(K)(x) satisfies Supp(g(K)) ⊆ [0,K], ∫K 0 g(K) dx = 1, and for k ≤ m, g (k) (K)(x) = O (Ä 2m K äk+1) .\n2. There exists a function G : R → R with the following properties.\n(a) G is nondecreasing. (b) G(x) = 0 for x ≤ 0. (c) G(x) = 1 for x ≥ 1. (d) G is m+ 1 times continuously differentiable and for all k ≤ m, G(k)(x) = O((2m)k).\n3. There exists a function b : R → R with the following properties:\n(a) Supp(b) ⊆ [−2, 2]. (b) b(x) = 1 for x ∈ [−1, 1]. (c) b is is m+ 1 times continuously differentiable and for all k ≤ m, b(k)(x) = O((2m)k).\nThe function b(K) satisfies Supp(b(K)) ⊆ [−2K, 2K], b(K)(x) = 1 for x ∈ [−K,K], and b (m) (K)(x) = O (Ä 2m K äk) .\nProof. Take\ng(x) =\n{ Cm4\nm+1xm+1(1− x)m+1, x ∈ [0, 1] 0, else.\nwhere Cm is chosen so that ∫ 1 0 g(x) dx = 1. Note that x(1− x) ≤ 14 so g(x) ≤ Cm and\n1 =\n∫ 1\n0 g(x) dx ≤ Cm (41)\n1 =\n∫ 1\n0 g(x) dx ≥\n∫ 1 2 + 1 2 √ m\n1 2 − 1 2 √ m\nCm4 m+1xm+1(1− x)m+1 dx (42)\n≥ 1√ m Cm4 m+1\nÇ 1\n2 +\n1\n2 √ m\nåm+1 Ç 1\n2 − 1 2 √ m\nåm+1 (43)\n≥ 1√ m Cm\nÅ 1− 1\nm\nãm+1 (44)\n≥ Cm 2e √ m\n(45)\nso 1 ≤ Cm ≤ 2e √ m.\nNow, note that for functions u, v,\n(uv)(k) = k∑\nj=0\nÇ k\nj\nå u(j)v(k−j). (46)\nApplying this to xm+1 and (1− x)m+1 and gives that for 0 ≤ x ≤ 1, k ≤ m,\n|g(k)(x)| ≤ Cm √ m k∑\nj=0\nÇ k\nj\nå (m+ 1)j(m+ 1)k−j (47)\n≤ O(m(2(m+ 1))k) (48) = O((2m)k+1). (49)\nFor the second part, take F (x) = ∫ x −∞ f(t) dt. The normalization ∫ 1 0 f(x) dx = 1 ensures\nF (x) = 1 for x ≥ 1, and for k ≤ m, F (k+1)(x) = f (k)(x) = O((2m)k). For the third part, define\nb(x) =    0, |x| > 2 F (2− |x|), 1 ≤ |x| ≤ 2 1, |x| < 1.\nFor the rescaled functions, just note that for any function f , f (k) (K)(x) = 1 Kk f (k) ( x K ) ."
    }, {
      "heading" : "E Omitted Proofs in Section 4",
      "text" : "Theorem E.1 (Theorem 4.2 restated). If f is differentiable, then for any g such that Supp(g) ⊆ rBn and g, ĝ ∈ L1(Rn),\nCf,rBn ≥ r ∫ Rn |÷(∇f)g(ω)| dω∫ Rn\n|ĝ(ω)| dω Proof. Let B = rBn. We have\nCf,B = inf F |B=f\n∫\nRn ‖ω‖B |“F (ω)| dω (50)\n= r inf F |B=f\n∫\nRn ‖ω‖2 |“F (ω)| dω (51)\n= r inf F |B=f\n∫\nRn\n∥∥∥‘∇F (ω) ∥∥∥ 2 dω. (52)\nYoung’s inequality and Theorem A.4 give ∫\nRn\n∥∥∥‘∇F (ω) ∥∥∥ 2 dω\n∫\nRn |ĝ(ω)| dω ≥\n∫\nRn\n∥∥∥(‘∇F ∗ ĝ)(ω) ∥∥∥ 2 dω (53)\n=\n∫\nRn\n∥∥∥◊ (∇F )g(ω) ∥∥∥ 2 dω (54)\n=\n∫\nRn\n∥∥∥÷(∇f)g(ω) ∥∥∥ 2 dω. (55)\nwhere the last step uses the fact that Supp(g) ⊆ rBn, so (∇F )g = (∇f)g. Then\n∫\nRn\n∥∥∥‘∇F (ω) ∥∥∥ 2 dω ≥\n∫ Rn ∥∥∥÷(∇f)g(ω) ∥∥∥ 2 dω\n∫ Rn |ĝ(ω)| dω . (56)\nE.1 f is not Barron\nIn this section we prove Lemma 4.3. We first prove the function g we choose gives a small denominator in the lowerbound equation.\nLemma E.2. For n ≡ 3 (mod 4), ∫\nRn ‖ĝ(ω)‖ dω ≤ O((5eC2)\nn 2 ).\nTo prove this we will need bound certain combinations of derivatives of a radial function.\nLemma E.3. Let f : Rn → Rn be a radial function with f(x) = f1(‖x‖). Then for k ∈ N, 1 ≤ k ≤ n4 + 1,\n((I −∆)kf)(x) = ∑\n0 ≤ i ≤ 2k, 0 ≤ j ≤ max{0, 2k − 1} i+ j ≤ 2k\nci,jn jf (i) 1 (r)\nrj , r = ‖x‖ (57)\nfor some ci,j with ∑\ni,j |ci,j | ≤ 5k. Here, (I −∆)f denotes f −∆f .\nProof. We proceed by induction. The case k = 0 is just f(x) = f1(r). Suppose the statement is true for a given k ≤ n4 ; we show it for k + 1. Let (I −∆)kf be given by (57). We use the formula for the Laplacian of a radial function,\n∆f(x) = n− 1 r f ′1(r) + f ′′ 1 (r). (58)\nFor ease of notation, in the below the arguments of f and f1, which are x and r, are omitted. Then using (58) and the product rule,\n(I −∆)k+1f = ∑\n0 ≤ i ≤ 2k, 0 ≤ j ≤ max{0, 2k − 1} i+ j ≤ 2k\nci,jn j\nÇ 1\nrj f (i) 1 + n− 1 r\nÅ j\nrj+1 f (i) 1 −\n1\nrj f (i+1) 1\nã (59)\n+ Ç −j(j + 1)\nrj+2 f (i) 1 +\n2j\nrj+1 f (i+1) 1 −\n1\nrj f (i+2) 1\nåå (60)\nThe largest derivative of f1 increases by 2 and the power of r increases by 2, except when k = 0, when the power increases by 1 (from (58)). Write this as\n∑\n0 ≤ i ≤ 2(k + 1), 0 ≤ j ≤ 2k + 1 i+ j ≤ 2(k + 1)\nc′i,jn jf (i) 1\nrj .\nA term is identified by the order f (i) that appears and the power 1 rj that appears. For example, the term ci,jn j n−1 r j rj+1 f (i) 1 = ci,jn j+2 (n−1)j n2 1 rj+2 f (i) 1 in (59) will contribute ci,j (n−1)j n2 to c ′ i,j+2. Noting k ≤ n4 implies 2k ≤ n2 , we have\n∑\ni,j\n|c′i,j | ≤ ∑\n0 ≤ i ≤ 2k, 0 ≤ j ≤ max{0, 2k − 1} i+ j ≤ 2k\n|ci,j | Ç 1 +\n(n − 1)j n2 + n− 1 n + j(j + 1) n2 + 2j n + 1\nå (61)\n≤ ∑\ni,j\n|ci,j | Å 1 + 1\n2 + 1 +\n1 4 + 1 + 1\nã (62)\n≤ 5 ∑\ni,j\n|ci,j |. (63)\nThis completes the induction step and proves the theorem.\nProof of Lemma E.2. By Lemma A.6 with k = n+12 ,\n∫\nRn ‖ĝ(ω)‖ dω ≤\nÑ Γ Ä 1 2 ä\n2nπ n 2 Γ Ä n+1 2\nä é 1 2 Å∫\nRn [(I −∆)n+14 g(x)]2 dx\nã 1 2\n. (64)\nNote [(I − ∆)n+14 g(x)]2 is nonzero only on 2K2Bn. Then letting ci,j be as in Lemma E.3 with k = n+14 , we have\n(I −∆)n+14 g(x) = ∑\n0 ≤ i ≤ n+1 2 , 0 ≤ j ≤ n−1 2\ni+ j ≤ n+1 2\nci,jn jg (i) 1 (r)\nrj , r = ‖x‖ (65)\nWe separate out the one term g1(r), and bound the derivatives noting that g1 was defined using the bump function b(K2) in Lemma D.1. Note that g (i) 1 = 0 for r < K2, so we can take r ≥ K2 in the sum.\n|(I −∆)n+14 g(x)| ≤ g1(r) + ∑\n1 ≤ i ≤ n+1 2 , 0 ≤ j ≤ n−1 2\ni+ j ≤ n+1 2\n|ci,j | nj|g(i)1 (r)|\nrj (66)\n≤ g1(r) + ∑\n1 ≤ i ≤ n+1 2 , 0 ≤ j ≤ n−1 2\ni+ j ≤ n+1 2\n|ci,j | njO\n( (n+1)i\n(C2n)i\n)\n(C2n)j (67)\n= O(4 n+1 4 ). (68)\n(69)\nNoting that the volume of 2K2Bn is π\nn 2\nΓ(n2+1) (2K2)\nn,\nÅ∫\nRn [(I −∆)n+14 g(x)]2 dx\nã 1 2\n= O\nÑÇ π n 2\nΓ (n 2 + 1\n)(2K2)n ( 5 n+1 4\n)2å 1 2 é (70)\n= O\nÑÇ π\nn 2 2nCn2 n n\nΓ(n2 + 1)\nå 1 2\n5 n+1 4 é . (71)\nCombining (64) and (71) and using Stirling’s approximation Γ(n+ 1) ∼ √ 2πn (n e )n gives\n∫\nRn ‖ĝ(ω)‖ dω ≤ O\nÖ C n 2\n2 n n 2 5 n+1 4\nΓ Ä n+1 2 ä 1 2 Γ (n 2 + 1 ) 1 2\nè\n(72)\n= O Ä (5eC2) n 2 ä . (73)\nNow we are ready to bound the numerator and finish the proof.\nLemma E.4. For f defined as in Section 4.1, n ≡ 3 (mod 4), and constants C1, C2, C3 such that C1C3 ≥ 32 , C2 > C1 ≥ 1, C3 ≥ 1,\nCf,2K3Bn = Ω\nÅ 2−nC n 2 −3 1 C n 2 3 C −(n2 −1) 2 n 1 2 ã .\nIn particular, this is exponentially large if we choose C3 large enough (i.e. if we make f vary sharply enough).\nProof. For ‖ω‖ = K3, by (17), (18), and Lemma B.1,\nf̂(ω) = 1\n2π\nÅ 1\n2πK3\nãn 2 −1 ∫ K1+ε\nK1\nr n 2 −1f1(r)Jn\n2 −1(K3r) dr (74)\n≥ 1 2π\nÅ 1\n2πK3\nãn 2 −1 ∫ K1+ε\nK1\nr n 2 −1f1(r)\nÇ  2\nπK3r 1√ 2 − (K3r)− 3 2\nå dr (75)\n≥ 1 2π\nÅ K1\n2πK3\nãn−3 2   1\nπ (1− o(1)) (76)\nwhere in the last step we used ∫K1+ε K1\nf1(r) = 1. Now we show that f̂ is also large for ‖ω‖ ≈ K3. Let ω, ω0 be such that ‖ω0‖ = K3 and ω ≥ ω0. Then using the fact that Jn\n2 −1 is 1-Lipschitz for\nx ≥ 3 (n2 − 1 ) (Lemma B.2) and K3K1 ≥ C3C1n ≥ 3n2 ,\n|f̂(ω)− f̂(ω0)| ≤ 1\n2π\nÅ 1\n2πK3\nãn 2 −1 ∫ K1+ε\nK1\nr n 2 −1f1(r)|Jn\n2 −1(‖ω‖ r)− Jn 2 (K3r)| dr (77)\n≤ 1 2π\nÅ 1\n2πK3\nãn 2 −1 ∫ K1+ε\nK1\nr n 2 −1f1(r)r(‖ω‖ −K3) dr (78)\n≤ 1 2π\nÅ 1\n2πK3\nãn 2 −1\n(K1 + ε) n 2 (‖ω‖ −K3) (79)\n= O\nÇÅ K1\n2πK3\nãn 2 −1\nK 3 2 1 K 1 2 3 (‖ω‖ −K3) å\n(80)\nBy (76) and (80), for n ≥ 3, there exists δ such that for all ‖ω‖ ∈ ï K3,K3 +\nδ\nK 3/2 1 K 1/2 3\nò ,\n|f̂(ω)| = Ω (Å K1\n2πK3\nãn−3 2 ) (81)\nThen using the fact that the surface area of a sphere in Rn is 2π n 2\nΓ(n2 ) ,\n∫\nRn ‖ω‖ |f̂(ω)| dω =\n∫\nK3≤‖ω‖≤K3+ δ K\n3/2 1\nΩ\n(Å K1\n2πK3\nãn−3 2 ) dω (82)\n= Ω\n( π n 2\nΓ (n 2\n)Kn−13 δ\nK 3/2 1 K 1/2 3\nÅ K1\n2πK3\nãn−3 2 ) (83)\n= Ω\nÇ 1\nΓ (n 2\n)K n 2 3 K n 2 −3 1 2 −n 2 å (84)\n= Ω\nÇÅ 2e\nn− 2\nãn 2 −1\n(C3n 1 2 ) n 2 (C1n 1 2 ) n 2 −32− n 2 å (85)\n= Ω(C n 2 −3 1 C n 2 3 n − 1 2 e n 2 ). (86)\nNote K2 = C2n > C1 √ n+ ε = K1 + ε. Then g = 1 on the support of f , so (∇f)g = ∇f and\n∫\nRn\n∥∥∥÷(∇f)g(ω) ∥∥∥ dω =\n∫\nRn\n∥∥∥∇̂f(ω) ∥∥∥ dω (87)\n= Ω(C n 2 −3 1 C n 2 3 n − 1 2 e n 2 ). (88)\nThen by Lemma E.2,\nCf,2K2Bn ≥ 2K2 ∫ Rn ∥∥∥÷(∇f)g(ω) ∥∥∥ dω\n∫ Rn |ĝ(ω)| dω (89)\n= 2K2 Ω(C\nn 2 −3 1 C n 2 3 n − 1 2 e n 2 )\nO((5eC2) n 2 )\n= Ω Å 5− n 2 C n 2 −3 1 C n 2 3 C −(n2−1) 2 n 1 2 ã . (90)\nE.2 h is a composition of Barron functions\nIn this section we proof Lemma 4.4. In order to do that, let us first define the following set of functions:\nDefinition E.5. Define\nΓ(A,C) := ß f : Rn → R : ∫\nRn |f̂(ω)| dω ≤ A,\n∫\nRn ‖ω‖ |f̂(ω)| dω ≤ C\n™\nBarron functions have many nice properties:\nProposition E.6 (Properties of Barron constant). 1. (Subadditivity, [Bar93, §IV.3]) For any set B,\nC∑ i βifi,B ≤\n∑\ni\n|βi|Cfi,B .\n2. (Ridge functions, [Bar93, §IV.7]) Suppose f = h(〈a, x〉), where h : R → R is a 1-dimensional function and ‖a‖2 = 1. Then\nCf,rBn ≤ Ch,[−r,r].\n3. (Powers, [Bar93, §IV.12]) If g : R → R, g ∈ Γ(a, c), then g(x)k ∈ Γ(ak, kak−1c).\n4. The function f(x) = x has an extension h agreeing with x on [−r, r], which satisfies h(x) ∈ Γ(O(r 3 2 ), O(r 1 2 )).\nProof. We show (4). Choose a bump function b as in Lemma D.1 for m = 2. Consider the extension h(x) = xb(r)(x) = xb (x r ) which is supported on [−2r, 2r]. Because b, b′, b′′ are all bounded by a constant, on [−2r, 2r],\n|h(x)| ≤ x (91)\n|h′(x)| = |b(r)(x) + xb′(r)(x)| ≤ 1 +O Å x\nr\nã (92)\n|h′′(x)| = |2b′(r)(x) + b′′(r)(x)| ≤ O Å x\nr\nã +O Å 1\nr2\nã . (93)\nThen by Lemma A.6(2),\n∫ ∞\n−∞ |ĥ(ω)| dω ≤ 2− 12 Å∫ r −r |h(x)|2 + |h′(x)|2 dx ã 1\n2 ≤ O(r 32 ) (94) ∫ ∞\n−∞ |ωĥ(ω)| dω ≤ 2− 12 Å∫ r −r |h′(x)|2 + |h′′(x)|2 dx ã 1 2 ≤ O(r 12 ). (95)\nProof of Theorem 4.4. By Proposition E.6(4) and (3), the 1-dimensional function y 7→ y2 has an extension k(y) with k(y) ∈ Γ(O(r3), O(r2)). Thus, Cy2,[−r,r] ≤ r ∫∞ −∞ ‖ω‖ |k̂(ω)| dω = O(r3).\nBecause x2i : R n → R is the composition of the projection x 7→ 〈ei, x〉 and the 1-dimensional\nfunction y 7→ y2 and , by (2), Cx2i ,rBn ≤ Cy2,[−r,r] ≤ O(r3) By (1), because ‖x‖2 = ∑ni=1 x2i , C‖x‖2,rBn ≤ O(nr 3).\nNow consider the function h(y) := f1( √ y). We have, noting this is nonzero only for x ∈\n[K21 , (K1 + ε) 2], and f (i) 1 ( √ y) = O(Ki+13 ),\nh′(y) = 1\n2y 1 2\nf1( √ y) + f ′1( √ y) = O\nÅÅ K3\nK1\nã +K23 ã (96)\nh′′(y) = 1\n4y 3 2\nf1( √ y) + 1\n4y f ′1(\n√ y) + 1\n2y 1 2\nf ′′1 ( √ y) = O\nÇ K3\nK31 + K23 K21 + K33 K1\nå . (97)\nUsing C3 < C1 we have |h′|2 + |h′′|2 = O(K43 ). Thus by Lemma A.6,\n∫ ∞\n0 |ωĥ(ω)| dω =\nÇ∫ (K1+ε)2\nK2 1\nO Ä K43 äå 12 = O (Å K1\nK3 O(K43 )\nã 1 2 ) = O Å K 1 2 1 K 3 2 3 ã .\nThus f1( √ x) is O(sC 1 2 1 C 3 2 3 n 2)-Barron on [−s, s]."
    } ],
    "references" : [ {
      "title" : "Towards principled methods for training generative adversarial networks",
      "author" : [ "M. Arjovsky", "L. Bottou" ],
      "venue" : "NIPS 2016 Workshop on Adversarial Training. In review for ICLR",
      "citeRegEx" : "Arjovsky and Bottou.,? \\Q2017\\E",
      "shortCiteRegEx" : "Arjovsky and Bottou.",
      "year" : 2017
    }, {
      "title" : "Wasserstein GAN",
      "author" : [ "M. Arjovsky", "S. Chintala", "L. Bottou" ],
      "venue" : "arXiv preprint arXiv:1701.07875",
      "citeRegEx" : "Arjovsky et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Arjovsky et al\\.",
      "year" : 2017
    }, {
      "title" : "Universal approximation bounds for superpositions of a sigmoidal function",
      "author" : [ "A.R. Barron" ],
      "venue" : "IEEE Transactions on Information Theory",
      "citeRegEx" : "Barron.,? \\Q1993\\E",
      "shortCiteRegEx" : "Barron.",
      "year" : 1993
    }, {
      "title" : "Approximation and estimation bounds for artificial neural networks",
      "author" : [ "A.R. Barron" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Barron.,? \\Q1994\\E",
      "shortCiteRegEx" : "Barron.",
      "year" : 1994
    }, {
      "title" : "Representation learning: A review and new perspectives”. In: IEEE transactions on pattern analysis and machine intelligence",
      "author" : [ "Y. Bengio", "A. Courville", "P. Vincent" ],
      "venue" : null,
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "On the expressive power of deep learning: A tensor analysis",
      "author" : [ "N. Cohen", "O. Sharir", "A. Shashua" ],
      "venue" : "arXiv preprint arXiv:1509.05009",
      "citeRegEx" : "Cohen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2015
    }, {
      "title" : "Approximation by superpositions of a sigmoidal function",
      "author" : [ "G. Cybenko" ],
      "venue" : "Mathematics of Control, Signals, and Systems (MCSS)",
      "citeRegEx" : "Cybenko.,? \\Q1989\\E",
      "shortCiteRegEx" : "Cybenko.",
      "year" : 1989
    }, {
      "title" : "Depth Separation for Neural Networks",
      "author" : [ "A. Daniely" ],
      "venue" : "arXiv preprint arXiv:1702.08489",
      "citeRegEx" : "Daniely.,? \\Q2017\\E",
      "shortCiteRegEx" : "Daniely.",
      "year" : 2017
    }, {
      "title" : "Training generative neural networks via maximum mean discrepancy optimization",
      "author" : [ "G.K. Dziugaite", "D.M. Roy", "Z. Ghahramani" ],
      "venue" : null,
      "citeRegEx" : "Dziugaite et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dziugaite et al\\.",
      "year" : 2015
    }, {
      "title" : "The Power of Depth for Feedforward Neural Networks",
      "author" : [ "R. Eldan", "O. Shamir" ],
      "venue" : null,
      "citeRegEx" : "Eldan and Shamir.,? \\Q2015\\E",
      "shortCiteRegEx" : "Eldan and Shamir.",
      "year" : 2015
    }, {
      "title" : "On the approximate realization of continuous mappings by neural networks",
      "author" : [ "K.-I. Funahashi" ],
      "venue" : "Neural networks",
      "citeRegEx" : "Funahashi.,? \\Q1989\\E",
      "shortCiteRegEx" : "Funahashi.",
      "year" : 1989
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "I. Goodfellow" ],
      "venue" : "Advances in neural information processing systems",
      "citeRegEx" : "Goodfellow,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow",
      "year" : 2014
    }, {
      "title" : "Multilayer feedforward networks are universal approximators",
      "author" : [ "K. Hornik", "M. Stinchcombe", "H. White" ],
      "venue" : "Neural networks",
      "citeRegEx" : "Hornik et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Hornik et al\\.",
      "year" : 1989
    }, {
      "title" : "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods",
      "author" : [ "M. Janzamin", "H. Sedghi", "A. Anandkumar" ],
      "venue" : null,
      "citeRegEx" : "Janzamin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Janzamin et al\\.",
      "year" : 2015
    }, {
      "title" : "Detecting change in data streams",
      "author" : [ "D. Kifer", "S. Ben-David", "J. Gehrke" ],
      "venue" : "Proceedings of the Thirtieth international conference on Very large data bases-Volume 30. VLDB Endowment",
      "citeRegEx" : "Kifer et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Kifer et al\\.",
      "year" : 2004
    }, {
      "title" : "On the learnability of discrete distributions",
      "author" : [ "M. Kearns" ],
      "venue" : "Proceedings of the twenty-sixth annual ACM symposium on Theory of computing",
      "citeRegEx" : "Kearns,? \\Q1994\\E",
      "shortCiteRegEx" : "Kearns",
      "year" : 1994
    }, {
      "title" : "On a space of completely additive functions",
      "author" : [ "L.V. Kantorovich", "G.S. Rubinstein" ],
      "venue" : "Vestnik Leningrad. Univ",
      "citeRegEx" : "Kantorovich and Rubinstein.,? \\Q1958\\E",
      "shortCiteRegEx" : "Kantorovich and Rubinstein.",
      "year" : 1958
    }, {
      "title" : "Approximations for the Bessel and Airy functions with an explicit error term",
      "author" : [ "I. Krasikov" ],
      "venue" : "LMS Journal of Computation and Mathematics",
      "citeRegEx" : "Krasikov.,? \\Q2014\\E",
      "shortCiteRegEx" : "Krasikov.",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "arXiv preprint arXiv:1312.6114",
      "citeRegEx" : "Kingma and Welling.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits",
      "author" : [ "D.M. Kane", "R. Williams" ],
      "venue" : "Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing",
      "citeRegEx" : "Kane and Williams.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kane and Williams.",
      "year" : 2016
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "D.J. Rezende", "S. Mohamed", "D. Wierstra" ],
      "venue" : null,
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep Learning in Neural Networks: An Overview",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Neural Networks",
      "citeRegEx" : "Schmidhuber.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 2015
    }, {
      "title" : "Benefits of depth in neural networks",
      "author" : [ "M. Telgarsky" ],
      "venue" : "arXiv preprint arXiv:1602.04485",
      "citeRegEx" : "Telgarsky.,? \\Q2016\\E",
      "shortCiteRegEx" : "Telgarsky.",
      "year" : 2016
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution—also theoretically not understood—concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks. We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with n hidden layers. A key ingredient is Barron’s Theorem [Bar93], which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of n functions which satisfy certain Fourier conditions (“Barron functions”) can be approximated by a n+ 1layer neural network. For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance—a natural metric on probability distributions—by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.",
    "creator" : "LaTeX with hyperref package"
  }
}