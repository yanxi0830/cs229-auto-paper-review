{
  "name" : "1307.8187.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning with Unknown Time Horizon",
    "authors" : [ "Haipeng Luo" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 7.\n81 87\nv1 [\ncs .L"
    }, {
      "heading" : "1 Introduction",
      "text" : "We study online learning problems with unknown time horizon with the aim of developing algorithms and approaches for the realistic case that the number of time steps is initially unknown.\nWe first adopt the standard Hedge setting [1] where the learner chooses a distribution over N actions on each round, and the losses for each action are then selected by an adversary. The learner incurs loss equal to the expected loss of the actions in terms of the distribution it chose for this round, and its goal is to minimize the regret, the difference between its cumulative loss and that of the best action after T rounds.\nVarious algorithms are known to achieve the optimal (up to a constant) upper bound O( √ T lnN) on the regret. Most of them assume that the horizon T is known ahead\nof time, especially those which are minimax optimal. When the horizon is unknown, the so-called doubling trick [2] is a general technique to make a learning algorithm adaptive and still achieve O( √ T lnN) regret uniformly for any T . The idea is to first guess a horizon, and once the actual horizon exceeds this guess, double it and restart the algorithm. Although, in theory, it is widely applicable, the doubling trick is aesthetically inelegant, and intuitively wasteful, since it repeatedly restarts itself, entirely forgetting all the preceding information. Other approaches have also been proposed, as we discuss shortly.\nIn this paper, we study the problem of learning with unknown horizon in a gametheoretic framework. We consider a number of variants of the problem, and make progress toward a minimax solution. Based on this approach, we give a new general technique which can also make other algorithms adaptive and achieve low regret. The resulting algorithm is still not exactly optimal, but it makes use of all the previous information on each round and achieves much lower regret in experiments.\nWe view this online learning problem as a repeated game between the learner and the adversary. In [3] and [4], an exact minimax optimal solution was proposed for a slightly different game with binary losses which assumes that the loss of the best action is at most some fixed constant. They derived the solution under a very simple type of loss space; that is, on each round only one action suffers one unit loss. We call this the basis vector loss space. As a preliminary of this paper, we also derive a similar minimax solution under this simple loss space for our setting where the horizon T is fixed and known to the learner ahead of time.\nWe then move on to the primary interest of this paper, that is, the case when the horizon is unknown to the learner. We study this unknown horizon setting in the minimax framework, with the aim of ultimately deriving game-theoretically optimal algorithms. Two types of models are studied. The first one assumes the horizon is chosen according to some known distribution, and the learner’s goal is to minimize the expected regret. We show the exact minimax solution for the basis vector loss space in this case. It turns out that the distribution the learner should choose on each round is simply the conditional expectation of the distributions the learner would have chosen for the fixed horizon case.\nThe second model we study gives the adversary the power to decide the horizon on the fly, which is possibly the most adversarial case. In this case, we no longer use the regret as the performance measure. Otherwise the adversary would obviously choose an infinite horizon. Instead, we use a scaled regret to measure the performance. Specifically, we scale the regret at time t by the optimal regret under fixed horizon t. The exact optimal solution in this case is unfortunately not found and remains an open problem, even for the extremely simple case. However, we give a lower bound for this setting to show that the optimal regret is strictly greater than the one in the fixed horizon game. That is, the adversary does obtain strictly more power if allowed to pick the horizon.\nWe then propose our new adaptive algorithm based on the minimax solution in the random horizon setting. One might doubt how realistic a random horizon is in practice. Even if the true horizon is indeed drawn from a fixed distribution, how can we know this distribution? We address these problems at the same time. Specifically, we prove that no matter how the horizon is chosen, if we assume it is drawn from a distribution\nfrom a special family, and let the learner play in a way similar to the one in the random horizon setting, then the worst-case regret at any time T (not the expected regret) can still be of the optimal order. In other words, although the learner is behaving as if the horizon is random, its regret will be small even if the horizon is actually controlled by an adversary.\nOur idea can be combined with not only the minimax algorithm, but also the exponential weights algorithm, and even other online optimization settings [5, 6]. Moreover, our technique can not only deal with unknown horizon, but also other unknown information such as the loss of the best action, thus leading to a first order regret bound that depends on the loss of the best action [7]. Like the doubling trick, this seems to be a quite general way to make an algorithm adaptive. Furthermore, we conduct experiments showing that our algorithm outperforms many other existing algorithms, including the doubling trick, in an online linear optimization setting within an ℓ2 ball where our algorithm has an explicit closed form.\nThe rest of the paper is organized as follows. We define the Hedge setting formally in Section 2, and derive the minimax solution for the fixed horizon setting as the preliminary of this paper in Section 3. In Section 4, we study two unknown horizon settings in the minimax framework. Our new adaptive algorithm is presented in Section 5, and several generalizations and an experiment are shown in Section 6. We omit most of the proofs in the paper due to space limitations, but all details can be found in the supplementary material.\nRelated work Besides the doubling trick, other adaptive algorithms have been studied [8, 9, 10, 11]. Auer et al. [8] showed that for algorithms such as the exponential weights algorithm [12, 1, 13], where a learning rate η should be set as a function of the horizon, typically in the form √\n(b lnN)/T for some constant b, one can simply set the learning rate adaptively as √\n(b lnN)/t, where t is the current number of rounds. In other words, this algorithm always pretends the current round is the last round. Although this idea works with the exponential weights algorithm, we remark that assuming the current round is the last round does not always work. Specifically, one can show that it will fail if applied to the minimax algorithm (see Section 6.1). In another approach to online learning with unknown horizon, Chaudhuri et al. [11] proposed an adaptive algorithm based on a novel potential function reminiscent of the half-normal distribution.\nOther performance measures different from the usual regret were studied before. Foster and Vohra [14] introduced internal regret comparing the loss of an online algorithm to the loss of a modified algorithm which consistently replaces one action by another. In [15] and [16], the learner’s loss is compared with the best k-shifting expert instead of the best one. Hazan and Seshadhri [17] studied the usual regret within any time interval, which they called adaptive regret. To the best of our knowledge, the form of scaled regret that we study is new. Lower bounds on anytime regret in terms of the quadratic variations for any loss sequence (instead of the worst case sequence this paper considers) were studied in [18]."
    }, {
      "heading" : "2 Repeated Games",
      "text" : "We first consider the following repeated game between a learner and an adversary. The learner has access to N actions. On each round t = 1, . . . , T , (1) the learner chooses a distribution Pt over the actions; (2) the adversary reveals the loss vector Zt = (Zt,1, . . . , Zt,N) ∈ LS, where Zt,i is the loss for action i for this round, and the loss space LS is a subset of [0, 1]N ; (3) the learner suffers loss ℓt = Pt · Zt for this round.\nNotice that the adversary can choose the losses on round t with full knowledge of the history P1:t and Z1:t−1, that is, all the previous choices of the learner and the adversary. We also denote the cumulative loss up to round t for the learner and the actions by Lt = ∑t t′=1 ℓt′ and Mt = ∑t t′=1 Zt′ respectively. The goal for the learner is to minimize the difference between its total loss and that of the best action at the end of the game. In other words, the goal of the learner is to minimize Reg(LT ,MT ), where we define the regret function Reg(L,M) , L − mini Mi, for L ∈ R and M ∈ RN . The number of rounds T is called the horizon.\nRegarding the loss space LS, perhaps the simplest one is {e1, . . . , eN}, the N standard basis vectors in N dimensions. Playing with this loss space means that on each round, the adversary chooses one single action to incur one unit loss. In Sections 3, 4 and 5, we mainly focus on this basis vector loss space, but we return to the most general case [0, 1]N in Section 6."
    }, {
      "heading" : "3 Minimax Solution for Fixed Horizon",
      "text" : "Although our primary interest in this paper is the case when the horizon is unknown to the learner, we first present some preliminary results on the setting where the horizon is known to both the learner and the adversary ahead of time. These will later be useful for the unknown horizon case.\nIf we treat the learner as an algorithm Alg that takes the information of previous rounds as inputs, and outputs a distribution Pt = Alg(P1:t−1,Z1:t−1) that the learner is going to play with, then finding the optimal solution in this fixed horizon setting can be viewed as solving the minimax expression\ninf Alg sup Z1:T Reg(LT ,MT ). (1)\nAlternatively, we can recursively define a function V (M, r) as:\nV (M, 0) , −min i Mi ; V (M, r) , min P∈∆(N) max Z∈LS (P · Z+ V (M+ Z, r − 1)) ,\nwhere M ∈ RN is a loss vector, r is a nonnegative integer, and ∆(N) is the N dimensional simplex. By a simple argument, one can show that the value of V (M, r) is the regret of a game with r rounds starting from the situation that each action has initial loss Mi, and assuming both the learner and the adversary will play optimally. In fact, the value of Eq. (1) is exactly V (0, T ), and the optimal learner algorithm is the one that chooses the P∗ which realizes the minimum in the definition of V (M, r) when the\nactions’ cumulative loss vector is M and there are r rounds left. We call V (0, T ) the value of the game.\nWe now consider the basis vector loss space 1, that is, LS = {e1, . . . , eN}. It turns out that under this loss space, the value function V has a nice closed form. Similar to the results from [2] and [3], V can also be expressed in terms of a random walk. Suppose R(M, r) is the expectation of the loss of the best action if the adversary chooses each ei uniformly randomly for the remaining r rounds, starting from loss vector M. Formally, R(M, r) can be defined in a recursive way: R(M, 0) , mini Mi ;R(M, r) , 1 N ∑N i=1 R(M+ ei, r − 1). The connection between V and R, and the optimal algorithm are then shown by the following theorem.\nTheorem 1. If LS = {e1, . . . , eN}, then for any vector M and integer r ≥ 0,\nV (M, r) = r\nN −R(M, r).\nLet cN = 1N √ 2(N − 1) lnN . Then the value of the game satisfies\nV (0, T ) ≤ cN √ T . (2)\nMoreover, on round t, the optimal learner algorithm is the one that chooses weight Pt,i = V (Mt−1, r)−V (Mt−1+ei, r−1) for each action i, where Mt−1 is the current cumulative loss vector and r is the number of remaining rounds, that is, r = T − t+1.\nTheorem 1 tells us that under the basis vector loss space, the best way to play is to assume that the adversary is playing uniformly randomly, because r/N and R(M, r) are exactly the expected loss for the learner and the best action respectively. In practice, computing R(M, r) needs exponential time. However, we can estimate it by sampling (see similar work in [3]). Note that cN is decreasing when N ≥ 4 (with maximum value about 0.72). So contrary to the O( √ T lnN) regret bound for the general loss space [0, 1]N which is increasing in N , here V (0, T ) is of order O( √ T ).\nBefore we move on, we point out a few properties of the function V , which we will use later.\nProposition 1. If LS = {e1, . . . , eN}, then for any vector M and integer r, Property 1. V (M, r) = V ((M1 − a, . . . ,MN − a), r)− a for any real number a and r ≥ 0. Property 2. V (M, r) is non-increasing in Mi for each i = 1, . . . , N .\nProperty 3. V (M, r) is non-decreasing in r."
    }, {
      "heading" : "4 Playing without Knowing the Horizon",
      "text" : "We turn now to the case in which the horizon T is unknown to the learner, which is often more realistic in practice. There are several ways of modeling this setting. For example, the horizon can be chosen ahead of time according to some fixed distribution, or it can even be chosen by the adversary. We will discuss these two variants separately.\n1For other loss spaces, finding the minimax solutions seems difficult. Nevertheless, we show the relation of the values of the game under different loss spaces in the supplementary file, see Theorem 9."
    }, {
      "heading" : "4.1 Random Horizon",
      "text" : "Suppose the horizon T is chosen according to some fixed distribution Q which is known to both the learner and the adversary. Before the game starts, a random T is drawn, and neither the learner nor the adversary knows the actual value of T . The game stops after T rounds, and the learner aims to minimize the expectation of the regret. Using our earlier notation, the problem can be formally defined as\ninf Alg sup Z1:∞ ET∼Q[Reg(LT ,MT )],\nwhere we assume the expectation is always finite. We sometimes omit the subscript T ∼ Q for simplicity. For the basis vector loss space, we again show the exact minimax solution, which has a strong connection with the one for the fixed horizon setting.\nTheorem 2. If LS = {e1, . . . , eN}, then\ninf Alg sup Z1:∞ ET∼Q[Reg(LT ,MT )] = ET∼Q[inf Alg sup Z1:T Reg(LT ,MT )]. (3)\nMoreover, on round t, the optimal learner plays with the distributionPt = ET∼Q[PTt |T ≥ t], where PTt is the optimal distribution the learner would play if the horizon is T , that is, PTt,i = V (Mt−1, T − t+ 1)− V (Mt−1 + ei, T − t).\nEq. (3) tells us that if the horizon is drawn from some distribution, then even though the learner does not know the actual horizon before playing the game, as long as the adversary does not know this information either, it can still do as well as the case when they are both aware of the horizon.\nHowever, so far this model does not seem to be quite useful in practice for several reasons. First of all, the horizon might not be chosen according to a distribution. Even if it is, this distribution is probably unknown. Secondly, what we really care about is the performance which holds uniformly for any horizon, instead of the expected regret. Fortunately, we address all these problems and develop new adaptive algorithms based on the result in this section. We discuss these in Section 5 after first introducing the fully adversarial model."
    }, {
      "heading" : "4.2 Adversarial Horizon",
      "text" : "The most adversarial setting is the one where the horizon is completely controlled by the adversary. That is, we let the adversary decide whether to continue or stop the game on each round according to the current situation. However, notice that the value of the game is increasing in the horizon. So if the adversary can determine the horizon and its goal is still to maximize the regret, then the problem would not make sense because the adversary would clearly choose to play the game forever and never stop leading to infinite regret. One reasonable way to address this issue is to scale the regret by the value of the fixed horizon game V (0, T ), so that the scaled regret Reg(LT ,MT )/V (0, T ) indicates how many times worse is the regret compared to the\none that is optimal given the horizon. Under this setting, the corresponding minimax expression is\nṼ = inf Alg sup T sup Z1:T\nReg(LT ,MT )\nV (0, T ) . (4)\nUnfortunately, finding the minimax solution to this setting seems to be quite challenging, even for the simplest case N = 2. It is clear, however, that Ṽ is at most some constant due to the existence of adaptive algorithms such as the doubling trick, which can achieve the optimal regret bound up to a constant without knowing T . Another clear fact is Ṽ ≥ 1, since it is impossible for the learner to do better than the case when it is aware of the horizon. Below, we derive a nontrivial lower bound that is greater than 1, thus proving that the adversary does gain strictly more power when it can stop the game whenever it wants. Theorem 3. If N = 2 and LS = [0, 1]2, then Ṽ ≥ √ 2. That is, for every algorithm, there exists an adversary and a horizon T such that the regret of the learner after T rounds is at least √ 2V (0, T )."
    }, {
      "heading" : "5 A New Adaptive Algorithm",
      "text" : "We study next how the random-horizon algorithm of Section 4.1 can be used when the horizon is entirely unknown. In Theorem 2, we proposed an algorithm that simply takes the conditional expectation of the distributions we would have played if the horizon were given. Notice that even though it is derived from the random horizon setting, it can still be used in any setting as an adaptive algorithm in the sense that it does not require the horizon as a parameter. However, to use this algorithm, we should ask two questions: What distribution should we use? What can we say about the algorithm’s performance for an arbitrary horizon instead of in expectation?\nAs a first attempt, suppose we use a uniform distribution over 1, . . . , T0, where T0 is a huge integer. From what we observe in some numerical calculations, Pt = E[PTt |T ≥ t] tends to be a uniform distribution in this case. Clearly it cannot be a good algorithm if for each round, it just places equal weights for each action regardless of the actions’ behaviors. In fact, one can verify that the exponential distribution (that is, Pr[T = t] ∝ αt for some constant 0 < α < 1) also does not work. These examples show that even though this algorithm gives us the optimal expected regret, it can still suffer a big regret for a particular trial of the game, which we definitely want to avoid.\nNevertheless, it turns out that there does exist a family of distributions that can guarantee the regret to be of order O( √ T ) for any T . Below, we first give a general upper bound on the regret that holds for any distribution Q and has no dependence on the choices of the adversary. After that we will show what the appropriate distributions are to make this upper bound be O( √ T ).\nTheorem 4. Let LS = {e1, . . . , eN}, V̄t(M) = ET∼Q[V (M, T − t + 1)|T ≥ t], and qt = PrT∼Q[T = t|T ≥ t]. Suppose on round t, the learner chooses Pt = ET∼Q[PTt |T ≥ t], where PTt,i = V (Mt−1, T − t+ 1)− V (Mt−1 + ei, T − t). Then\nfor any Ts, the regret after Ts rounds is at most\nV̄1(0) +\nTs∑\nt=1\nqtV̄t+1(0).\nProof. By the definition of PTt and the fact that Pr[T = t ′|T ≥ t] = (1 − qt) Pr[T = t′|T ≥ t+ 1] for any t′ > t, we have Pt,i = E[V (Mt−1, T − t+ 1)− V (Mt−1 + ei, T − t)|T ≥ t]\n= V̄t(Mt−1)− qtV (Mt−1 + ei, 0)− (1− qt)V̄t+1(Mt−1 + ei) ≤ V̄t(Mt−1)− V̄t+1(Mt−1 + ei) + qtV̄t+1(0). (5)\nHere the inequality holds because for anyM, if we let M′ = (M1−R(M, 0), . . . ,MN− R(M, 0)) so that M ′i ≥ 0 for each i, then by Property 1 and 2 in Proposition 1, V̄t+1(M) − V (M, 0) = E[V (M′, T − t)|T ≥ t + 1] ≤ V̄t+1(0). Replacing M with Mt−1 + ei shows Eq. (5). Now let the choice of the adversary on round t be it, so that Mt = Mt−1 + eit . The regret at the end of round Ts is\nV (MTs , 0) +\nTs∑\nt=1\nPt,it ≤ V (MTs , 0) + Ts∑\nt=1\n( V̄t(Mt−1)− V̄t+1(Mt) + qtV̄t+1(0) )\n= V (MTs , 0) + V̄1(0)− V̄Ts+1(MTs) + Ts∑\nt=1\nqtV̄t+1(0),\nwhich is at most V̄1(0) + ∑Ts\nt=1 qtV̄t+1(0) because for any M and t, by Property 3 in Proposition 1, V̄t(M)− V (M, 0) = E[V (M, T − t+ 1)− V (M, 0)|T ≥ t] ≥ 0.\nAs a direct corollary, we now show an appropriate choice of Q. We emphasize that even though we let T be a random variable drawn from Q, the actual horizon Ts is not necessarily chosen according to this distribution. It can even be chosen adversarially.\nTheorem 5. Under the same conditions of Theorem 4, if Pr[T = t] ∝ 1/td where d > 32 is a constant, then for any Ts, the regret after Ts rounds is at most\nΓ(d− 32 ) Γ(d) (d− 1)2cN √ πTs + o( √ Ts),\nwhere cN is defined in Eq. (2) and Γ is the gamma function. Choosing d ≈ 2.35 approximately minimizes the main term in the bound, leading to regret 3cN √ Ts +\no( √ Ts).\nTheorem 5 tells us that pretending that the horizon is drawn from the distribution Pr[T = t] ∝ 1/td (d > 3/2) can always achieve low regret. Although we do not have a simple closed form for E[PTt |T ≥ t] (this can be addressed under some specific settings though, see Section 6.3), computing the sum of the first sufficient number of terms in the series can be a good estimate since the weight for each term decreases rapidly. Also notice that the constant 3 in the bound for the term cN √ Ts is less than\nthe one for the doubling trick with the fixed horizon optimal algorithm, which is 2+ √ 2 [7]. We will see in Section 6.3 an experiment on a different but related online learning setting, which shows that our algorithm performs much better than the doubling trick."
    }, {
      "heading" : "6 Generalizations",
      "text" : "It turns out that the idea of using a “pretend prior distribution” is very applicable in online learning. Below we will discuss a couple of generalizations, including Hedge with general loss space, handling other unknown information besides the horizon, and an application to online linear optimization within an ℓ2 ball."
    }, {
      "heading" : "6.1 Generalizing the Exponential Weights Algorithm",
      "text" : "Up until now, we mainly focused on the basis vector loss space, but in fact, we can also deal with the most general loss space [0, 1]N . The optimal solution for the loss space [0, 1]N is unknown even for the fixed horizon setting. However, generalizing the weighted majority algorithm of [12], Freund and Schapire [1, 13] presented an algorithm using exponential weights that can deal with this general loss space and achieve the O( √ T lnN) bound on the regret. The algorithm takes the horizon T as a parameter, and on round t, it simply chooses Pt,i ∝ exp(−ηMt−1,i), where η =√ (8 lnN)/T is the learning rate. It is shown that the regret of this algorithm is at most √\n(T lnN)/2. Auer et al. [8] proposed a way to make this algorithm adaptive by simply setting a time-varying learning rate η = √\n(8 lnN)/t, where t is the current round, leading to a regret bound of √ T lnN for any T (see Chapter 2.5 of [19]). In other words, the algorithm always treats the current round as the last round. Below, we show that our “pretend distribution” idea can also be used to make this exponential weights algorithm adaptive, and is in fact a generalization of the adaptive learning rate algorithm in [8].\nTheorem 6. Let LS = [0, 1]N , Pr[T = t] ∝ 1/td (d > 3/2) and ηT = √\n(b lnN)/T , where b is a constant. If on round t, the learner assigns weight ET∼Q[PTt,i|T ≥ t] to each action i, where PTt,i ∝ exp(−ηTMt−1,i), then for any Ts, the regret after Ts rounds is at most\n(√ b(d− 1)\n4(d− 1/2) + d− 1 (d− 3/2) √ b\n) √\nTs lnN + o( √ Ts lnN).\nSetting b = 4d−2d−3/2 minimizes the main term, which approaches 1 as d → ∞.\nNote that different from Theorem 5 where the regret is minimized when d ≈ 2.35, Theorem 6 implies that the bigger we choose d, the smaller will be the leading term in this bound. Also note that if d → ∞, our algorithm simply becomes the one of [8], because Pr[T = τ |T ≥ t] is 1 if τ = t and 0 otherwise. Therefore, our algorithm can be viewed as a generalization of the idea of treating the current round as the last round. However, we emphasize that the way we deal with unknown horizon is more applicable in the sense that if we try to make the minimax algorithm in Theorem 1 adaptive by treating each round as the last round, one can construct an adversary that leads to linear regret (just consider a game of two actions and the adversary choosing e1 and e2 alternatively)."
    }, {
      "heading" : "6.2 First Order Regret Bound",
      "text" : "So far all the regret bounds we have discussed are in terms of the horizon, which are also called the zeroth order bound. More refined bounds have been studied in the literature [7]. For example, the first order bound, that depends on the loss of the best action m∗ at the end of the game, usually is of order O( √ m∗ lnN). Again, using the exponential weights algorithm with a slightly different learning rate η = ln(1 + √ (2 lnN)/m∗), one can show that the regret is at most √ 2m∗ lnN + lnN . Here, m∗ is prior information on the loss sequence similar to the horizon. To avoid exploiting this information that is not available in practice, one can again use techniques like the doubling trick or the time-varying learning rate. Alternatively, we show that the “pretend distribution” technique can also be used here. Instead of using a discrete distribution on the horizon, it now makes more sense to assign a continuous distribution on the loss of the best action.\nTheorem 7. Let LS = [0, 1]N , mt = miniMt,i + 1, ηm = √\n(lnN)/m, and m ≥ 1 be a continuous random variable with probability density f(m) ∝ 1/md (d > 3/2). If on round t, the learner assigns weight E[Pmt,i |m ≥ mt−1] to each action i, where Pmt,i ∝ exp(−ηmMt−1,i), then for any Ts, the regret after Ts rounds is at most\n3(d− 7/6)(d− 1) (d− 3/2)(d− 1/2) √ m∗ lnN + (1 + (d− 1) ln(m∗ + 1)) lnN + o( √ m∗ lnN),\nwhere m∗ = miniMTs,i is the loss of the best action after Ts rounds. Setting d = 5/2 + √ 2 minimizes the main term, which becomes (3/2 + √ 2) √ m∗ lnN ."
    }, {
      "heading" : "6.3 Online Linear Optimization",
      "text" : "All the examples we have shown previously do not have an explicit form to compute the learner’s strategy. In this section, however, we will show an explicit algorithm for online linear optimization within an ℓ2 ball using the technique of pretended distribution, which also illustrates the generality of our technique. Specifically, we consider the following repeated game adopted from [6] (all the norms are ℓ2 norms). Let B = {x ∈ RN : ‖x‖ ≤ 1}. On each round t = 1, . . . , T , the learner first chooses a vector xt ∈ B, then the adversary chooses another vector wt ∈ B. The learner suffers loss xt · wt for this round, and the regret of the learner after T rounds is\n∑T t=1 xt · wt − inf\nx∈B\n∑T t=1 x · wt = ∑T t=1 xt · wt + ‖WT‖, where we define\nWt = ∑t\nt′=1 wt′ . In [6], a simple but exact minimax optimal algorithm for the fixed horizon setting was shown: on each round t, choose\nxTt = −Wt−1 /√ ‖Wt−1‖2 + (T − t+ 1) . (6)\nThis strategy guarantees the regret to be at most √ T . To make this algorithm adaptive, we again assign a distribution over the horizon. However, in order to get an explicit form for E[xTt |T ≥ t], a continuous distribution on T is necessary. It does not seem to make sense at first glance since the horizon is always an integer, but keep in mind that\nthe random variable T is merely an artifact of our algorithm. As long as the output of the learner is in the set B, our algorithm is valid. Specifically, we show the following:\nTheorem 8. Let T ≥ 1 be a continuous random variable with probability density f(T ) ∝ 1/T 2. If the learner chooses xt = E[xTt |T ≥ t] on round t, where xTt is defined by Eq. (6), then the regret after Ts rounds is at most π √ Ts + o( √ Ts) for any Ts. Moreover, xt has the following explicit form\nxt =\n \n\n( t·tanh−1 (√ 1−t/c )\n(c−t)3/2 − √ c c−t\n)\nWt−1, if c 6= t\n− 2t 3c3/2\nWt−1, else (7)\nwhere c = 1 + ‖Wt−1‖2.\nThe algorithm we are proposing in Eq. (7) looks quite inexplicable if one does not realize that it comes from the expression E[xTt |T ≥ t] with an appropriate distribution. Yet the algorithm not only enjoys a low theoretic regret bound as shown in Theorem 8, but also achieves very good performance in simulated experiments. To show this, we conduct an experiment that compares the regrets of five algorithms at any time step within 1000 rounds against an adversary that chooses points in B uniformly at random (N = 10). The results are shown in Figure 1, where each data point is the maximum regret over a thousand randomly generated adversaries for the corresponding algorithm and horizon. The\nfive algorithms are: the minimax algorithm in Eq. (6) (OPT); the one we proposed in Theorem 8 (DIST); online gradient descent, a general algorithm for online optimization (see [5]) (OGD); the algorithm in Eq. (6) with T replaced by t (OPT2); the doubling trick with the minimax algorithm (DOUBLE). Note that OPT is not really an adaptive algorithm: it “cheats” by knowing the horizon T = 1000 in advance, and thus performs best at the end of the game. We include this algorithm merely as a baseline. Figure 1 shows that our algorithm DIST achieves consistently much lower regret than any other adaptive algorithm. OPT2 adapts the idea of treating the current round as the last round, which can actually be forced to achieve linear regret under the worst case scenario (similar to what we discussed at the end of Section 6.1). It is surprising though that its performance is comparable to OGD under a uniformly random adversary. Finally, we remark that although the doubling trick is widely applicable in theory, in experiments it is beaten by most of the other algorithms."
    }, {
      "heading" : "A Proof of Theorem 1 and Proposition 1",
      "text" : "We first state a few properties of the function R:\nProposition 2. For any vector M of N dimensions and integer r,\nProperty 4. R(M, r) = a+R((M1 − a, . . . ,MN − a), r) for any real number a and r ≥ 0.\nProperty 5. R(M, r) is non-decreasing in Mi for each i = 1, . . . , N .\nProperty 6. If r > 0, R(M, r)−R(M, r − 1) ≤ 1/N .\nProperty 7. If r > 0, andPi = 1N+R(M+ei, r−1)−R(M, r) for each i = 1, . . . , N , then P = (P1, . . . , PN ) is a distribution in the simplex ∆(N).\nProof of Proposition 2. We omit the proof for Property 4 and 5, since it is straightforward. We prove Property 6 by induction. For the base case r = 1, let S = {j : Mj = mini Mi}. If |S| = 1, then R(M + ei, 0) is R(M, 0) for i /∈ S and R(M, 0) + 1 otherwise. If |S| > 1, then R(M + ei, 0) is simply R(M, 0) for all i. In either case, we have\nR(M, 1) = 1\nN\nN∑\ni=1\nR(M+ ei, 0) ≤ 1\nN (1 +\nN∑\ni=1\nR(M, 0)) = 1\nN +R(M, 0),\nproving the base case. Now for r > 1, by definition of R and induction,\nR(M, r)−R(M, r−1) = 1 N\nN∑\nj=1\n(R(M+ ei, r − 1)−R(M+ ei, r − 2)) ≤ 1\nN\nN∑\nj=1\n1\nN =\n1\nN ,\ncompleting the induction. For Property 7, it suffices to prove Pi ≥ 0 for each i and∑N i=1 Pi = 1. The first part can be shown using Property 5 and 6:\nPi = 1\nN +R(M+ei, r−1)−R(M, r) ≥\n1 N +R(M, r−1)−( 1 N +R(M, r−1)) = 0.\nThe second part is also easy to show by definition of R:\nN∑\ni=1\nPi = 1+\nN∑\ni=1\nR(M+ei, r−1)−NR(M, r) = 1+NR(M, r)−NR(M, r) = 1.\nProof of Theorem 1. First proveV (M, r) = r/N−R(M, r) for any r ≥ 0 inductively. The base case r = 0 is trivial by definition. For r > 0,\nV (M, r) = min P∈∆(N) max Z∈LS\n(P · Z+ V (M+ Z, r − 1))\n= min P∈∆(N) max i∈[N ]\n(Pi + V (M + ei, r − 1)) (LS = {e1, . . . , eN})\n= min P∈∆(N) max i∈[N ]\n(\nPi + r − 1 N −R(M + ei, r − 1) )\n(by induction)\nDenote Pi + (r − 1)/N − R(M + ei, r − 1) by g(P, i). Notice that the average of g(P, i) over all i is irrelevant to P : 1N ∑N i=1 g(P, i) = r/N − R(M, r). Therefore, maxi g(P, i) ≥ r/N −R(M, r) for any P, and\nV (M, r) = min P max i\ng(P, i) ≥ r/N −R(M, r). (8)\nOn the other hand, from Proposition 2, we know that P ∗i = 1/N+R(M+ei, r−1)− R(M, r) (i ∈ [N ]) is a valid distribution. Also,\nV (M, r) = min P max i g(P, i) ≤ max i g(P∗, i) = max i\n( r\nN −R(M, r)\n)\n= r\nN −R(M, r).\n(9) So from Eq. (8) and (9) we have V (M, r) = r/N − R(M, r), and also P ∗i = 1/N + R(M + ei, r − 1) − R(M, r) = V (M, r) − V (M + ei, r − 1) realizes the minimum, and thus is the optimal strategy.\nIt remains to prove V (0, T ) ≤ cN √ T . Let Z1, . . . ,ZT be independent uniform\nrandom variables taking values in {e1, . . . , eN}. By what we proved above,\nV (0, T ) = T\nN − E[min i∈[N ]\nT∑\nt=1\nZt,i] = E[max i∈[N ]\nT∑\nt=1\n(1/N − Zt,i)].\nLet yt,i = 1/N − Zt,i. Then each yt,i is a random variable that takes value 1/N with probability 1 − 1/N and 1/N − 1 with probability 1/N . Also, for a fixed i, y1,i, . . . , yT,i are independent (note that this is not true for yt,1, . . . , yt,N for a fixed t). It is not hard to verify that each yt,i satisfies\nE[exp(λyt,i)] ≤ exp( λ2σ2\n2 ), ∀λ > 0,\nwhere σ2 = (N − 1)/N2 is the variance of yt,i. So if we let Yi = ∑T\nt=1 yt,i, by the independence of each term, we have\nE[exp(λYi)] = E[\nT∏\nt=1\nexp(λyt,i)] =\nT∏\nt=1\nE[exp(λyt,i)] ≤ exp( λ2σ2T\n2 ), ∀λ > 0.\nNow using Lemma A.13 in [7], we arrive at\nE[max i∈[N ]\nYi] ≤ σ √ 2T lnN = cN √ T .\nWe conclude the proof by pointing out\nV (0, T ) = E[max i∈[N ]\nT∑\nt=1\n(1/N − Zt,i)] = E[max i∈[N ]\nYi] ≤ cN √ T .\nProof of Proposition 1. Based on the equality V (M, r) = r/N −R(M, r) from Theorem 1, the three properties in Proposition 1 are direct corollaries of Property 4, 5 and 6 in Proposition 2."
    }, {
      "heading" : "B Proof of Theorem 2",
      "text" : "Proof. Define V̄t(M) = E[V (M, T − t+ 1)|T ≥ t] and q(t′, t) = Pr[T = t′|T ≥ t]. We will prove an important property of the V̄ function:\nV̄t(M) = min P∈∆(N) max i∈[N ]\n( Pi + q(t, t)V (M+ ei, 0) + (1− q(t, t)) V̄t+1(M + ei) ) .\n(10) This equation shows that V̄t(M) is the conditional expectation of the regret given T ≥ t, starting from cumulative loss vector M and assuming both the learner and the adversary are optimal. This is similar to the function V in the fixed horizon case, and again the value of the game infAlg supZ1:∞ ET∼Q[Reg(LT ,MT )] is simply V̄1(0).\nTo prove Eq. (10), we plug the definition of V̄t+1(M+ ei) into the right hand side and get minP maxi g(P, i) where g(P, i) is\nPi + q(t, t)V (M+ ei, 0) + (1− q(t, t))E[V (M+ ei, T − t)|T ≥ t+ 1].\nUsing the fact that for any t′ ≥ t+ 1,\n(1−q(t, t))q(t′, t+1) = Pr[T > t|T ≥ t] Pr[T = t′|T ≥ t+1] = Pr[T = t′|T ≥ t] = q(t′, t),\ng(P, i) can be simplified in the following way:\ng(P, i) = Pi + q(t, t)V (M+ ei, 0) + (1− q(t, t)) ∞∑\nT=t+1\n(q(T, t+ 1)V (M+ ei, T − t))\n= Pi + q(t, t)V (M+ ei, 0) +\n∞∑\nT=t+1\n(q(T, t)V (M + ei, T − t))\n= Pi + E[V (M+ ei, T − t)|T ≥ t]. (11)\nAlso, the average of g(P, i) over all i is independent of P:\n1\nN\nN∑\ni=1\ng(P, i) = 1\nN +\n1\nN\nN∑\ni\nE[V (M+ ei, T − t)|T ≥ t]\n= E\n[\n1\nN +\n1\nN\nN∑\ni\nV (M + ei, T − t)|T ≥ t ]\n= E\n[\n1\nN +\n1\nN\nN∑\ni\n( T − t N −R(M + ei, T − t) ) |T ≥ t ]\n= E[ T − t+ 1\nN −R(M, T − t+ 1)|T ≥ t] (by definition of R)\n= E[V (M, T − t+ 1)|T ≥ t]\nwhich implies\nmin P∈∆(N) max i∈[N ]\ng(P, i) ≥ E[V (M, T − t+ 1)|T ≥ t]. (12)\nOn the other hand, let P∗ = E[PT |T ≥ t], where PTi = V (M, T − t+ 1)− V (M+ ei, T − t). P∗ is a valid distribution since PT is a distribution for any T . Also, by plugging into Eq. (11),\ng(P∗, i) = E[V (M, T − t+ 1)− V (M+ ei, T − t)|T ≥ t] + E[V (M+ ei, T − t)|T ≥ t] = E[V (M, T − t+ 1)|T ≥ t].\nTherefore,\nmin P∈∆(N) max i∈[N ] g(P, i) ≤ max i∈[N ] g(P∗, i) = E[V (M, T − t+ 1)|T ≥ t]. (13)\nEq. (12) and (13) show that minP maxi g(P, i) = E[V (M, T − t+ 1)|T ≥ t], which agrees with the left hand side of Eq. (10). We thus prove\ninf Alg sup Z1:∞\nET∼Q[Reg(LT ,MT )] = V̄1(0) = E[V (0, T )|T ≥ 1]\n= ET∼Q[inf Alg sup Z1:T Reg(LT ,MT )],\nand P∗ is the optimal strategy."
    }, {
      "heading" : "C Proof of Theorem 3",
      "text" : "To prove Theorem 3, we need to find out what V (0, T ) is under the general loss space [0, 1]2. Note that Theorem 1 only tells us the case of the basis vector loss space. Fortunately, it turns out that they are the same if N = 2. To be more specific, we will show\nlater in Theorem 9 that if N = 2 and LS = [0, 1]2, then V (0, T ) = T/2 − R(0, T ), which can be further simplified as\nV (0, T ) = T 2 − 1 2T\nT∑\nm=0\n( T\nm\n)\nmin{m,T −m} = T 2T ( T − 1 ⌊T2 ⌋ ) .\nWe can now prove Theorem 3 using this explicit scaling factor, denoted by S(T ) for simplicity.\nProof of Theorem 3. Again, solving Eq. (4) is equivalent to finding the value function Ṽ defined on each state of the game, similar to the functions V and V̄ we had before. The difference is that Ṽ should be a function of not only the index of the current round t and the cumulative loss vector M, but also the cumulative loss L for the learner. Moreover, to obtain a base case for the recursive definition, it is convenient to first assume that T is at most T0, where T0 is some fixed integer. Under these conditions, we define Ṽ T0t (L,M) recursively as:\nṼ T0T0 (L,M) , minP∈∆(N) max Z∈LS Reg(L+P · Z,M+ Z) V (0, T0) ,\nṼ T0t (L,M) , min P∈∆(N) max Z∈LS max\n{ Reg(L+P · Z,M + Z)\nV (0, t) , Ṽ T0t+1(L+P · Z,M+ Z)\n}\n,\nwhich is the scaled regret starting from round t with cumulative loss L for the learner and M for the actions, assuming both the learner and the adversary will play optimally from this round on. The value of the game Ṽ is now lim\nT0→+∞ Ṽ T01 (0,0).\nTo simplify this value function, we will need three facts. First, the base case can be related to V (M, 1):\nṼ T0T0 (L,M) = minP∈∆(N) max Z∈LS Reg(L+P · Z,M+ Z) V (0, T0)\n=\n(\nL+ min P∈∆(N) max Z∈LS\nReg(P · Z,M+ Z) )/\nV (0, T0)\n=\n(\nL+ min P∈∆(N) max Z∈LS\nP · Z+ V (M+ Z, 0) )/\nV (0, T0)\n= L+ V (M, 1)\nV (0, T0) .\nSecond, for any L and M, one can inductively show that\nṼ T0t (L,M) = Ṽ T0 t (L −R(M, 0),M′), (14)\nwhere M ′i = Mi −R(M, 0). (We omit the details since it is straightforward.)\nThird, when M = 0, by symmetry, one has\nṼ T0t (L,0) = max Z∈LS max\n{ Reg(L+Pu · Z,Z)\nV (0, t) , Ṽ T0t+1(L+Pu · Z,Z)\n}\n(Pu = ( 1N , . . . , 1 N ))\n≥ max { L+ 1N V (0, t) , Ṽ T0t+1(L+ 1 N , e1) } . (15)\nNow we can make use of the condition N = 2 to lower bound Ṽ . The key point is to consider a restricted adversary who can only place one unit more loss on one of the action than the other, if not stopping the game. Clearly the value of this restricted game serves as a lower bound of Ṽ . Specifically, consider the value of Ṽ T0t (L, e1) for t ≤ T0 − 2:\nṼ T0t (L, e1) ≥ min p∈[0,1] max\n{ Reg(L+ p, 2e1)\nS(t) , Ṽ T0t+1(L+ 1− p, e1 + e2)\n}\n(restricted adversary)\n= min p∈[0,1] max\n{ L+ p\nS(t) , Ṽ T0t+1(L− p,0)\n}\n(by Eq. (14))\n≥ min p∈[0,1] max\n{ L+ p\nS(t) , L+ 1/2− p S(t+ 1) , Ṽ T0t+2(L+ 1 2 − p, e1)\n}\n(by Eq. (15))\n≥ min p∈R max\n{ L+ p\nS(t) , Ṽ T0t+2(L+\n1 2 − p, e1)\n}\nTherefore, if we assume T0 is even without loss of generality and define function GT0t (L) recursively as:\nGT0T0(L) , Ṽ T0 T0\n(L, e1) = L+ V (e1, 1)\nS(T0) =\nL\nS(T0)\nGT0t (L) , min p∈R max\n{ L+ p\nS(t) , GT0t+2(L +\n1 2 − p)\n}\n,\nthen it is clear that Ṽ T0t (L, e1) ≥ GT0t (L), and thus by 15,\nṼ T01 (0,0) ≥ max{1, Ṽ T02 ( 1\n2 , e1)} ≥ max{1, GT02 (\n1 2 )}.\nIt remains to compute GT02 ( 1 2 ). By some elementary computations and the fact that\nfor two linear functionsh1(p) and h2(p) of different signs of slopes, minp max{h1(p), h2(p)} = h1(p\n∗) where p∗ is such that h1(p∗) = h2(p∗), one can inductively prove that for t = 2, 4, . . . , T0,\nGT0t (L) = 2\nT0−t\n2 (L+ 12 )− 12\nS(T0) + (T0−t)/2∑\nk=1\n(2k−1S(T0 − 2k)) .\nPlugging S(t) = t2t ( t−1 ⌊t/2⌋ ) and letting T0 → ∞, we arrive at\nlim T0→∞ GT02 (1/2) = lim T0→∞\n\n\nT0/2−1∑\nk=1\n( 2k−T0/2S(T0 − 2k) )\n\n\n−1\n= lim T0→∞\n\n\nT0/2−1∑\nk=1\n( S(2k)\n2k\n) \n\n−1\n=\n\n\n∞∑\nj=0\nj\n8j\n( 2j\nj\n) \n\n−1\n.\nDefine G(x) = ∑∞\nj=0 ( 2j j ) xj and F (x) = x · G′(x). Note that what we want to\ncompute above is exactly 1/F (18 ). In [20], it was shown that G(x) = (1 − 4x)−1/2. Therefore, F (x) = 2x · (1− 4x)−3/2 and\nlim T0→∞\nGT02 (1/2) = 1/F (1/8) = √ 2.\nWe conclude the proof by pointing out\nṼ = lim T0→∞ Ṽ T01 (0,0) ≥ max{1, lim T0→∞\nGT02 (1/2)} = √ 2.\nAs we mentioned at the beginning of this section, the last thing we need to show is that the value V (0, T ) is the same under the two loss spaces. In fact, we will prove stronger results in the following theorem claiming that this is true only if N = 2.\nTheorem 9. Let LS1,LS2,LS3 be the three loss spaces {e1, . . . , eN}, {0, 1}N and [0, 1]N respectively, and VLS(0, T ) be the value of the game V (0, T ) under the loss space LS. If N > 2, we have for any T ,\nVLS1(0, T ) < VLS2(0, T ) = VLS3(0, T ).\nHowever, the three values above are the same if N = 2.\nProof. We first inductively show that for anyM and r, VLS2(M, r) = VLS3(M, r) and VLS3(M, r) is convex in M. For the base case r = 0, by definition, VLS2(M, 0) = VLS3(M, 0) = −mini Mi. Also, for any two loss vectors M and M′, and λ ∈ [0, 1],\nVLS3(λM + (1− λ)M′, 0) = −min i (λMi + (1 − λ)M ′i)\n≤ −min i (λMi)−min i ((1− λ)M ′i) = λVLS3(M, 0) + (1− λ)VLS3(M′, 0),\nshowing VLS3(M, 0) is convex in M. For r > 0,\nVLS3(M, r) = min P∈∆(N) max Z∈LS3\n(P · Z+ VLS3(M+ Z, r − 1)) .\nNotice that P ·Z+ VLS3(M+Z, r − 1) is equal to P ·Z+ VLS2(M+ Z, r − 1) and is convex in Z by induction. Therefore the maximum is always achieved at one of the corner points of LS3, which is in LS2. In other words,\nVLS3(M, r) = min P∈∆(N) max Z∈LS2\n(P · Z+ VLS2(M+ Z, r − 1)) = VLS2(M, r).\nOn the other hand, by introducing a distribution Q over all the elements in LS2, we have\nVLS3(M, r) = min P∈∆(N) max Z∈LS2\n(P · Z+ VLS3(M+ Z, r − 1))\n= min P∈∆(N) max Q\nEZ∼Q [P · Z+ VLS3(M+ Z, r − 1)]\n= max Q min P∈∆(N)\nEZ∼Q [P · Z+ VLS3(M+ Z, r − 1)]\n= max Q\n(\nEZ∼QVLS3(M + Z, r − 1) + min P∈∆(N)\nP · EZ∼Q[Z] ) ,\nwhere we switch the min and max by Corollary 37.3.2 of [21]. Note that the last expression is the maximum over a family of linear combinations of convex functions in M, which is still a convex function in M, completing the induction step. To conclude, VLS2(0, T ) = VLS3(0, T ) for any N and T .\nWe next prove if N = 2, VLS1(0, T ) = VLS2(0, T ). Again, we inductively prove VLS1(M, r) = VLS2(M, r) for any M and r. The base case is clear. For r > 0, let P ∗i = VLS1(M, r)− VLS1(M+ ei, r − 1) (i = 1, 2). By induction,\nVLS2(M, r) = min P∈∆(2) max Z∈LS2\n(P · Z+ VLS1(M + Z, r − 1))\n≤ max Z1,Z2∈{0,1} (P ∗1Z1 + P ∗ 2Z2 + VLS1(M+ (Z1, Z2), r − 1)) = max {VLS1(M, r − 1), 1 + VLS1(M+ (1, 1), r − 1), VLS1(M, r)} = max {VLS1(M, r − 1), VLS1(M, r)}\n(by Property 1 in Proposition 1)\n= VLS1(M, r). (by Property 3 in Proposition 1)\nHowever, it is clear that VLS2(M, r) ≥ VLS1(M, r). Therefore, VLS1(M, r) = VLS2(M, r).\nFinally, to prove VLS1(0, T ) < VLS2(0, T ) for N > 2, we inductively prove VLS1((T − r)e1, r) < VLS2((T − r)e1, r) for r = 1, . . . , T . For the base case r = 1, VLS1((T − 1)e1, 1) = 1/N −R((T − 1)e1, 1) = 1/N , while\nVLS2((T − 1)e1, 1) = min P∈∆(N) max Z∈LS2 (P · Z+ VLS2((T − 1)e1 + Z, 0))\n≥ min P∈∆(N) max i∈[N ] (1− Pi + VLS2((T − 1)e1 + 1− ei, 0))\n= min P∈∆(N)\nmax {−P1, 1− P2, . . . , 1− PN} .\nWe claim that the value of the last minimax expression above, denoted by v, is (N − 2)/(N − 1), which is strictly greater than 1/N if N > 2 and thus proves the base case. To show that, notice that for any P ∈ ∆(N), there must exist i ∈ {2, . . . , N} such that Pi ≤ 1/(N − 1) and\nmax {−P1, 1− P2, . . . , 1− PN} ≥ 1− Pi ≥ N − 2 N − 1 ,\nshowing v ≥ (N − 2)/(N − 1). On the other hand, the equality is realized by the distribution P∗ = (0, 1N−1 , . . . , 1 N−1).\nFor r > 1, we have\nVLS2((T − r)e1, r) = min P∈∆(N) max Z∈LS2 (P · Z+ VLS2((T − r)e1 + Z, r − 1))\n≥ min P∈∆(N) max i∈[N ] (Pi + VLS2((T − r)e1 + ei, r − 1))\n≥ min P∈∆(N)\n1\nN\nN∑\ni=1\n(Pi + VLS2((T − r)e1 + ei, r − 1))\n= 1\nN +\n1\nN\nN∑\ni=1\nVLS2((T − r)e1 + ei, r − 1)\n> 1\nN +\n1\nN\nN∑\ni=1\nVLS1((T − r)e1 + ei, r − 1)\n= VLS1((T − r)e1, r).\nHere, the last strict inequality holds because for i = 1, VLS2((T − r + 1)e1, r − 1) > VLS1((T − r+1)e1, r− 1) by induction; for i 6= 1, it is trivial that VLS2((T − r)e1 + ei, r − 1) ≥ VLS1((T − r)e1 + ei, r − 1). Therefore, we complete the induction step and thus prove VLS1(0, T ) < VLS2(0, T )."
    }, {
      "heading" : "D Proof of Theorem 5",
      "text" : "The proof (and the one of Theorem 6) relies heavily on a common technique to approximate a sum using an integral, which we state without proof as the following claim.\nClaim 1. Let f(x) be a non-increasing nonnegative function defined on R+. Then the following inequalities hold for any integer 0 < j ≤ k.\n∫ k+1\nj\nf(x) dx ≤ k∑\ni=j\nf(i) ≤ ∫ k\nj−1 f(x) dx\nProof of Theorem 5. By Theorem 4, it suffices to upper bound V̄1(0) and ∑Ts\nt=1 qtV̄t+1(0). Let St = ∑∞ t′=t 1/t ′d. By applying Claim 1 multiple times, we have\n1\nSt ≤\n(∫ ∞\nt\ndx\nxd\n)−1 = td−1(d− 1); (16)\nqt = 1 St · td ≤ d− 1 t ;\nV̄1(0) = E[V (0, T )|T ≥ 1] ≤ cN S1\n∞∑\nT=1\n1\nT d− 1 2\n(by Theorem 1)\n≤ cN S1\n(\n1 +\n∫ ∞\n1\ndx\nxd− 1 2\n)\n(by Claim 1)\n= cN (d− 12 ) S1(d− 32 ) ≤ cN (d− 1)(d− 1 2 )\nd− 32 = O(1); (by Eq. (16))\nV̄t+1(0) = E[V (0, T − t)|T ≥ t+ 1]\n≤ cN St+1\n∫ ∞\n0\n√ xdx\n(t+ x)d\n= cN St+1\n· Γ(d− 3 2 ) 2Γ(d) · √ π td− 3 2\n≤ (d− 1)cN √ π · Γ(d− 3 2 )\n2Γ(d) · (t+ 1)\nd−1\ntd− 3 2\n;\nTs∑\nt=1\nqtV̄t+1(0) ≤ (d− 1)2cN √ π · Γ(d− 3 2 )\n2Γ(d)\nTs∑\nt=1\ntd−1 + o(td−1)\ntd− 1 2\n≤ Γ(d− 3 2 )\nΓ(d) (d− 1)2cN\n√ πTs + o( √ Ts),\nwhich proves the theorem."
    }, {
      "heading" : "E Proof of Theorem 6",
      "text" : "Proof. We will first show that\nReg(LTs ,MTs) ≤ (lnN) · E [ 1\nηT |T ≥ TS + 1\n]\n︸ ︷︷ ︸\nA\n+ 1\n8\nTs∑\nt=1 E[ηT |T ≥ t] ︸ ︷︷ ︸\nB\n. (17)\nLet ΦTt = 1 ηT\nln ( ∑N i=1 exp(−ηTMt−1,i) ) . The key point of the proof for the non-\nadaptive version of the exponential weights algorithm is to use ΦTt as a “potential” function, and bound the change in potential before and after a single round [7]. Specifically, they showed that\nPTt · Zt ≤ ηT 8 + ΦTt − ΦTt+1.\nWe also base our proof on this inequality. The total loss of the learner after Ts rounds is\nLTs =\nTs∑\nt=1\nE[PTt |T ≥ t] · Zt = Ts∑\nt=1\nE[PTt · Zt|T ≥ t]\n≤ B + Ts∑\nt=1\nE[ΦTt − ΦTt+1|T ≥ t].\nDefine Ut = E[ΦTt |T ≥ t]. We do the following transformation:\nE[ΦTt − ΦTt+1|T ≥ t] = Ut − ET [ΦTt+1|T ≥ t] = Ut − qtΦtt+1 − (1− qt)Ut+1 = Ut − Ut+1 + qt(Ut+1 − Φtt+1) = Ut − Ut+1 + qt · E[ΦTt+1 − Φtt+1|T ≥ t+ 1] = Ut − Ut+1 + qt · E[FT,t(Mt)|T ≥ t+ 1],\nwhere we define\nFT,t(M) = ln (\n∑\ni exp(−ηTMi)) ηT − ln ( ∑ i exp(−ηtMi)) ηt .\nA key observation is\nmax M∈RN\n+ ηT <ηt\nFT,t(M) = lnN ηT − lnN ηt , (18)\nwhich can be verified by a standard derivative analysis that we omit. (An alternative approach using KL-divergence can be found in Chapter 2.5 of [19].)\nWe further define another potential function Φ̄Tt = (lnN)/ηT and also Ūt = E[Φ̄Tt |T ≥ t]. Note that the new potential Φ̄Tt has no dependence on t and thus\nΦ̄Tt = Φ̄ T t′ for any t, t ′. We now have\nTs∑\nt=1\nE[ΦTt − ΦTt+1|T ≥ t]\n=\nTs∑\nt=1\n( Ut − Ut+1 + qt · E[ΦTt+1 − Φtt+1|T ≥ t+ 1] )\n= U1 − UTs+1 + Ts∑\nt=1\n( qt · E[ΦTt+1 − Φtt+1|T ≥ t+ 1] )\n︸ ︷︷ ︸\nC\n(19)\n≤ U1 − UTs+1 + Ts∑\nt=1\n(\nqt · E[ lnN ηT − lnN ηt |T ≥ t+ 1]\n)\n(by Eq. (18))\n= Ū1 − ŪTs+1 + Ts∑\nt=1\n( qt · E[Φ̄Tt+1 − Φ̄tt+1|T ≥ t+ 1] )\n︸ ︷︷ ︸\nD\n+ŪTs+1 − UTs+1.\n(∵ U1 = Ū1)\nNotice that D has the exact same form as C except for a different definition of the potential, and also Eq. (19) is an equality. Therefore, by a reverse transformation, we have\nTs∑\nt=1\nE[ΦTt − ΦTt+1|T ≥ t] = Ts∑\nt=1\nE[Φ̄Tt − Φ̄Tt+1|T ≥ t] + ŪTs+1 − UTs+1\n= ŪTs+1 − UTs+1 (∵ Φ̄Tt = Φ̄Tt+1)\nŪTs+1 is exactly A in Eq. (17), and UTs+1 can be related to the loss of the best action:\nUTs+1 = E\n[\n1\nηT ln\nN∑\ni=1\nexp(−ηTMTs,i) | T ≥ Ts + 1 ]\n≥ E [ 1\nηT ln exp(−ηTR(MTs , 0)) | T ≥ Ts + 1\n]\n= −R(MTs , 0).\nThe regret is therefore\nReg(LTs ,MTs) = LTS −R(MTs , 0) ≤ A+B − UTs+1 −R(MTs , 0) ≤ A+B,\nproving Eq. (17). The rest of the proof is merely to plug in the distribution and ηT = √\n(b lnN)/T , and upper bound Eq. (17) using Claim 1. Adopting the notation St = ∑∞ t′=t 1/t ′d and\nthe result of Eq. (16) in the proof of Theorem 5, we have\nA =\n√ lnN\nSTs+1 √ b\n∞∑\nT=Ts+1\n1\nT d−1/2\n≤ (d− 1) √ lnN√\nb (Ts + 1)\nd−1 (∫ ∞\nTs+1\ndx\nxd−1/2 +\n1\n(Ts + 1)d−1/2\n)\n= d− 1\n(d− 3/2) √ b\n√ Ts lnN + o( √ Ts lnN);\nB =\n√ b lnN\n8\nTs∑\nt=1\n1\nSt\n∞∑\nT=t\n1\nT d+1/2\n≤ (d− 1) √ b lnN\n8\nTs∑\nt=1\ntd−1 (∫ ∞\nt\ndx\nxd+1/2 +\n1\ntd+1/2\n)\n≤ (d− 1) √ b lnN\n8\nTs∑\nt=1\n( 1\n(d− 1/2) √ t +\n1\ntd+3/2\n)\n≤ √ b(d− 1)\n4(d− 1/2) √ Ts lnN + o( √ Ts lnN).\nCombining the bounds above for A and B proves the theorem."
    }, {
      "heading" : "F Proof of Theorem 7",
      "text" : "Proof. The main idea resembles the one of Theorem 6, but the details are much more technical. Let us first define several notations:\nSt ,\n∫ ∞\nmt\ndm md =\n1\n(d− 1)md−1t ,\nqt , Pr[m < mt|m ≥ mt−1] = 1\nSt−1\n∫ mt\nmt−1\ndm md = 1− ( mt−1 mt )d−1 ,\nY mt ,\nN∑\ni=1\nexp(−ηmMt−1,i),\nΦmt ,\n(\n1 + 1\nηm\n)\nlnY mt , Ut , E[Φ m t |m ≥ mt−1].\nThe proof starts from the following property of the exponential weights algorithm [7]:\nPmt · Zt ≤ 1 1− e−ηm ( lnY mt − lnY mt+1 )\n≤ Φmt − Φmt+1. (∵ ηm ≥ ln(1 + ηm))\nBy the fact that fm≥mt−1(m ′) = (1 − qt)fm≥mt(m′) for any m′ ≥ mt, where fm≥mt−1 and fm≥mt are conditional density functions, the loss of the learner after Ts rounds is\nLTs =\nTs∑\nt=1\nE[Pmt · Zt|m ≥ mt−1]\n≤ Ts∑\nt=1\nE[Φmt − Φmt+1|m ≥ mt−1]\n=\nTs∑\nt=1\n(\nUt − ∫ mt\nmt−1\nΦmt+1fm≥mt−1(m)dm+ (1 − qt)Ut+1 )\n≤ Ts∑\nt=1\n(\nUt − Φmt−1t+1 ∫ mt\nmt−1\nfm≥mt−1(m)dm+ (1− qt)Ut+1 )\n= U1 − UTs+1 + Ts∑\nt=1\nqt(Ut+1 − Φmt−1t+1 ),\nHere the last inequality holds becauseΦmt is increasing in m. To show this, we consider the following\n(\n1 + 1\nη\n)\nln\nN∑\ni=1\nexp(−ηai) = ( 1 + 1\nη\n)(\n−ηa1 + ln N∑\ni=1\nexp(−η(ai − a1)) )\n= −(η + 1)a1 + ( 1 + 1\nη\n)\nln N∑\ni=1\nexp(−η(ai − a1)),\nwhere η, a1, . . . , aN are positive. Since ln ∑\ni exp(−η(ai − a1)) ≥ 0, the expression above is decreasing in η, which along with the fact that ηm decreases in m shows that Φmt increases in m.\nWe now compute U1 and UTs+1:\nU1 = E[(1 + √ m/ lnN) lnN | m ≥ 1] = lnN + d− 1 d− 3/2 √ lnN\nUTs+1 = E\n[\n(1 + 1/ηm) ln ∑\ni\nexp(−ηmMTs,i) | m ≥ mTs\n]\n≥ E[(1 + 1/ηm)(−ηmm∗) | m ≥ mTs ] = −m∗ (1 + E[ηm | m ≥ mTs ]) = −m∗ ( 1 + d− 1\nd− 1/2\n√\nlnN\nmTs\n)\n≥ −m∗ − d− 1 d− 1/2 √ m∗ lnN (∵ mTs = m ∗ + 1)\nFor Ut+1 − Φmt−1t+1 = E[Φmt+1 − Φ mt−1 t+1 | m ≥ mt], we first upper bound the part\ninside the expectation:\nΦmt+1 − Φ mt−1 t+1\n= ( lnY mt+1 ηm − lnY mt−1 t+1 ηmt−1 ) + (ηmt−1 − ηm)min i Mt,i + ln ∑ e−ηm(Mt,i−mini Mt,i) ∑ e−ηmt−1(Mt,i−mini Mt,i) .\nThe first term above is at most (\n1 ηm − 1ηmt−1 ) lnN = √ lnN( √ m − √mt−1) by\nEq. (18). The second term is at most √ lnN( 1√mt−1 − 1√ m )mt−1 since miniMt,i = mt − 1 ≤ mt−1, and the last term is at most lnN since the numerator is at most N while the denominator is at least 1. Therefore, we have\nUt+1 − Φmt−1t+1 ≤ lnN + √ lnN · E[ √ m− mt−1√\nm | m ≥ mt]\n= lnN + √ lnN ( d− 1 d− 3/2 √ mt − d− 1 d− 1/2 mt−1√ mt ) ≤ lnN + √ lnN\n( d− 1 d− 3/2 √ mt − d− 1 d− 1/2 mt − 1√ mt )\n= lnN + (d− 1)\n√ mt lnN\n(d− 3/2)(d− 1/2) + d− 1 d− 1/2\n√\nlnN\nmt .\nIt remains to compute ∑Ts t=1 qt(Ut+1−Φ mt−1 t+1 ), which, using the above, can be done by computing A = ∑Ts\nt=1 qt, B = ∑Ts t=1 qt √ mt and C = ∑Ts t=1 qt/ √ mt. By inequality\n1− x ≤ − lnx for any x > 0, we have\nA =\nTs∑\nt=1\n(\n1− ( mt−1 mt\n)d−1)\n≤ −(d−1) Ts∑\nt=1\n(lnmt−1 − lnmt) = (d−1) ln(m∗+1).\nFor B, we first show qt √ mt ≤ 2(d− 1)( √ mt −√mt−1), which is equivalent to\nqt √ mt√\nmt −√mt−1 =\n( mt\nmt−1\n)d−1 − 1\n( mt\nmt−1\n)d−1 − (\nmt mt−1\n)d−3/2 ≤ 2(d− 1)\nif mt 6= mt−1 (it is trivial otherwise). Define h(x) = (xd−1− 1)/(xd−1−xd−3/2) for x ∈ [1, 2] (note that mt/mt−1 is within this interval). One can verify that h′(x) < 0 and thus h(x) ≤ limx→1 h(x) = 2(d − 1). So we prove qt √ mt ≤ 2(d − 1)(\n√ mt −√\nmt−1) and\nB ≤ 2(d− 1) Ts∑\nt=1\n( √ mt − √ mt−1) = 2(d− 1)( √ mTs − 1) ≤ 2(d− 1) √ m∗.\nA simple comparison of B and C shows C = o( √ m∗). We finally conclude the proof by combining all we have\nReg(LTs ,MTs)\n≤ U1 − UTs+1 + Ts∑\nt=1\nqt(Ut+1 − Φmt−1t+1 )−m∗\n= (1 + (d− 1) ln(m∗ + 1)) lnN + ( d− 1 d− 1/2 + 2(d− 1)2 (d− 3/2)(d− 1/2) )√ m∗ lnN + o( √ m∗ lnN) = 3(d− 7/6)(d− 1) (d− 3/2)(d− 1/2) √ m∗ lnN + (1 + (d− 1) ln(m∗ + 1)) lnN + o( √ m∗ lnN)."
    }, {
      "heading" : "G Proof of Theorem 8",
      "text" : "Proof. Let ΦTt = √\n‖Wt−1‖2 + (T − t+ 1) be the potential function for this setting. The key property of the minimax algorithm Eq. (6) shown in [6] is the following:\nxTt ·wt ≤ ΦTt − ΦTt+1.\nBased on this property, the loss of our algorithm after Ts rounds is\nTs∑\nt=1\nE[xTt |T ≥ t] ·wt = Ts∑\nt=1\nE[xTt ·wt|T ≥ t] ≤ Ts∑\nt=1\nE[ΦTt − ΦTt+1|T ≥ t].\nNow define Ut = E[ΦTt |T ≥ t] and qt = Pr[T < t + 1|T ≥ t]. By the fact that fT≥t(t′) = (1− qt)fT≥t+1(t′) for any t′ ≥ t+ 1, where fT≥t and fT≥t+1 are condi-\ntional density functions, we have\nTs∑\nt=1\nE[xTt |T ≥ t] ·wt\n≤ Ts∑\nt=1\n( Ut − E[ΦTt+1|T ≥ t] )\n=\nTs∑\nt=1\n( Ut − ∫ t+1\nt\nΦTt+1fT≥t(T )dT − (1− qt)Ut+1 )\n≤ Ts∑\nt=1\n( Ut − Φtt+1 ∫ t+1\nt\nfT≥t(T )dT − (1 − qt)Ut+1 ) (∵ ΦTt+1 increases in T )\n=\nTs∑\nt=1\n(Ut − Ut+1 + qt(Ut+1 − ‖Wt−1‖)) (∵ Φtt+1 = ‖Wt−1‖)\n= U1 − UTs+1 + Ts∑\nt=1\nqtE [√ ‖Wt−1‖2 + (T − t)− ‖Wt−1‖ | T ≥ t+ 1 ]\n≤ U1 − UTs+1 + Ts∑\nt=1\nqtE [√ T − t | T ≥ t+ 1 ] . (∵ √ a+ b−√a ≤ √ b)\nNote that UTs+1 ≥ ‖WT ‖, and thus it remains to plug in the distribution and compute U1 and ∑Ts t=1 qtE[ √ T − t | T ≥ t+ 1], which is almost the same process as what we did in the proof of Theorem 5 if one realizes qt ≤ (d− 1)/t also holds here. In a word, the regret can be bounded by\nΓ(d− 32 ) Γ(d) (d− 1)2 √ πTs + o( √ Ts),\nwhich is π √ Ts + o( √ Ts) if d = 2. The explicit form in Eq. (7) comes from a direct calculation."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>We consider online learning when the time horizon is unknown. We apply a<lb>minimax analysis, beginning with the fixed horizon case, and then moving on to<lb>two unknown-horizon settings, one that assumes the horizon is chosen randomly<lb>according to some known distribution, and the other which allows the adversary<lb>full control over the horizon. For the random horizon setting with restricted losses,<lb>we derive a fully optimal minimax algorithm. And for the adversarial horizon set-<lb>ting, we prove a nontrivial lower bound which shows that the adversary obtains<lb>strictly more power than when the horizon is fixed and known. Based on the mini-<lb>max solution of the random horizon setting, we then propose a new adaptive algo-<lb>rithm which “pretends” that the horizon is drawn from a distribution from a special<lb>family, but no matter how the actual horizon is chosen, the worst-case regret is of<lb>the optimal rate. Furthermore, our algorithm can be generalized in many ways,<lb>including handling other unknown information and other online learning settings.<lb>Experiments show that our algorithm outperforms many other existing algorithms<lb>in an online linear optimization setting.",
    "creator" : "LaTeX with hyperref package"
  }
}