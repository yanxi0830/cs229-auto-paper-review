{
  "name" : "1706.03607.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Clustering over Multi-Objective Samples: The one2all Sample",
    "authors" : [ "Edith Cohen", "Shiri Chechik", "Haim Kaplan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "metric space (where the triangle inequality can be relaxed by a constant factor). Each set of points Q (centers) defines a clustering of X according to the closest center with cost V (Q) = ∑ x∈X dxQ. This formulation generalizes classic k-means clustering, which uses squared distances. Two basic tasks, parametrized by k ≥ 1, are cost estimation, which returns (approximate) V (Q) for queries Q such that |Q| = k and clustering, which returns an (approximate) minimizer of V (Q) of size |Q| = k. With very large data sets X, we seek efficient constructions of small summaries that allow us to efficiently approximate clustering costs over the full data.\nWe present a novel data reduction tool based on multi-objective probability-proportional-to-size (pps) sampling: Our one2all construction inputs any set of centers M and efficiently computes a sample of size O(|M |) from which we can tightly estimate the clustering cost V (Q) for any Q that has at least a fraction of the clustering cost of M . For cost queries, we apply one2all to a bicriteria approximation to obtain a sample of size O(k −2) for all |Q| = k. For clustering, we propose a wrapper that applies a black-box algorithm to a sample and tests clustering quality over X, adaptively increasing the sample size. Our approach exploits the structure of the data to provide quality guarantees through small samples, without the use of typically much larger worst-case-size summaries."
    }, {
      "heading" : "1 Introduction",
      "text" : "Clustering is a fundamental and prevalent tool in data analysis. We have a set X of data points that lie in a (relaxed) metric spaceM, where distances satisfy a relaxed triangle inequality: For some constant ρ ≥ 1, for any three points x, y, z, dxy ≤ ρ(dxz + dzy). Note that any metric space with distances replaced by their pth power satisfies this relaxation: For p ≤ 1 it remains a metric and otherwise we have ρ = 2p−1. In particular, for squared distances, commonly used for clustering, we have ρ = 2.\nEach set Q ⊂M of points (centers) defines a clustering, which is a partition of X into |Q| clusters, which we denote by Xq for q ∈ Q, so that a point x ∈ X is in Xq if and only if it is in the Voronoi region of q, that is q = arg miny∈Q dxy. We allow points x ∈ X to have optional weights wx > 0, and define the cost of clustering X by Q to be\nV (Q | X,w) = ∑ x∈X wxdxQ , (1)\nwhere dxQ = miny∈Q dxy is the distance from point x to the set Q. Two fundamental computational tasks are cost queries and clustering (cost minimization). The clustering cost (1) of query Q can be computed using n|Q| pairwise distance computations, where n = |X| is the number of points in X. With multiple queries, it is useful to pre-process X and return fast approximate answers. Clustering amounts to finding Q of size |Q| ≤ k with minimum cost:\narg min Q||Q|≤k\nV (Q | X,w) . (2)\nar X\niv :1\n70 6.\n03 60\n7v 1\n[ cs\n.L G\n] 1\n2 Ju\nn 20\nClustering is computationally hard [2] even on Euclidean spaces and even to tightly approximate [4]. There is a local search polynomial algorithm with 9 + approximation ratio [20]. In practice, clustering is solved using heuristics, most notably Lloyd’s algorithm [22], and scalable approximation algorithms such as kmeans++ [3], which obtains a log k factor approximation using k centers. A recent result [27] established that kmeans++ provides constant factors bi-criteria guarantees: When applied to obtain βk centers for some constant β > 1, the clustering cost is within a constant factor of the optimum k-means cost.\nWhen the set of points X is very large, we seek an efficient procedure that computes a small summary structure from which we can approximate clustering costs. These structures are commonly in the form of subsets S ⊂ X with weights w′ so that V (Q | S,w′) approximates V (Q | X,w) for each Q of size k. The term coresets for such structures was introduced in the computational geometry literature [1, 18], building on the theory of -nets. Some notable constructions include [23, 9, 15, 16]. Initial coresets constructions had bounds with high (exponential or high polynomial) dependence on some parameters (dimension, , k) and poly logarithmic dependence on n, with the best current asymptotic bound of O(k −2 log k log n) claimed in [5].\nThe bulk of these randomized summary structures are aimed to provide strong “ForAll” statistical guarantees, which bound the distribution of the maximum approximation error over all Q of size k. The constructions are worst-case and based on general (VC) dimension bounds. Moreover, they do not exploit typical structure present in the data and must use size that meets the worst-case bounds in order to obtain guarantees on quality of the results. The ForAll requirement, however, is a costly overkill for the two tasks we have at hand: For clustering cost queries, weaker per-query “ForEach” typically suffice, which for each Q, with very high probability over the structure distribution, bound the error of the estimate of V (Q). For clustering, it suffices to guarantee that the (approximate) minimizers of V (Q | S,w′) are approximate minimizers of V (Q | X,w). In particular, sets Q with high clustering costs require only very coarse approximation. Moreover, our data typically has structure that includes a natural clustering (which is what we seek) and lower dimensionality than the ambient space. While a notion of “weak coresets” that aim to only support optimization was considered in the coreset literature [15], the constructions are also worst-case.\nContribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].\nConsider a particular set Q of centers. The theory of weighted sampling [25, 26] tells us that to estimate the sum V (Q | X,w) it suffices to sample −2 points with probabilities px ∝ wxdQx proportional to their contribution to the sum [17]. The inverse-probability [19] estimate obtained from the sample S,\nV̂ (Q | X,w) ≡ V (Q | S, {wx/px}) ,\nis an unbiased estimate of V (Q | X,w) with well-concentrated normalized squared error of . The challenge here for us is that we are interested in quality guarantees for all subsets Q of size k whereas the estimate V (Q′ | S, {wx/px}) when S is taken for a particular Q 6= Q′ will not provide quality guarantees for the set Q′. To obtain these quality guarantees for all Q by a single sample, we apply the machinery of multi-objective sampling: We use multi-objective pps sampling probabilities, where each point x ∈ X is sampled with the maximum pps probability over all Q of size k.\nApriori, however, it seems that the size of such a multi-objective sample can be very large. Surprisingly, we show that on any (relaxed) metric space, the multi-objective pps sample size is O(k −2). Note that the size does not depend on the dimensionality of the space or on the size of the data. Our result generalizes previous work [8] that only applied to the case where k = 1, where clustering cost reduces to inverse classic closeness centrality (sum of distances from a single point Q).\nFor our applications, we also need to efficiently perform the sampling. Clearly, a straightforward computation of these multi-objective pps probabilities is not feasible.\nOur main technical contribution, which is the basis of both the existential and algorithmic results, is an efficient general construction which we refer to as one2all: For any given set M of centers, and any α ≤ 1,\nwe compute using |M |n distance computation, multi-objective sampling probabilities for all subsets Q with clustering cost V (Q) ≥ αV (M). The size of the sample is O(α−1|M | −2).\nBy applying our one2all construction to an optimal clustering M of size k and α = 1, we establish existentially that a multi-objective pps sample for all sets Q of size k has size O(k −2). To obtain an efficient construction, we can apply kmeans++ [3] or another efficient bi-criteria approximation algorithm to compute M of size 2k that has cost that is at most the optimum divided by some constant α [27]. We then apply our one2all construction to M with α to obtain a multi-objective sample for all k-subsets of size O(α−1|M | −2) = O(k −2).\nFor the task of approximate cost queries, we pre-process the data as above to obtain a multi-objective sample of size O(k −2). We then process cost queries Q by computing and returning the clustering cost of S by Q: V (Q | S, {wx/px}). Our sample provide statistical guarantees that apply to each Q of size k, or more generally, for each Q with V (Q | X,w) ≥ αV (M | X,w) over the distribution of the sample S.\nFor approximate clustering tasks, we propose a wrapper algorithm which uses as a black-box (approximate, bicriteria) clustering algorithm A. The algorithm A is applied to the smaller sample to obtain a respective approximate minimizer of the clustering cost over the sample. The ForEach guarantee, however, in-and-off itself, does not guarantee us that the solution over the sample has the respective quality over the full data set. A larger sample size may be required and can be achieved by controlling the size parameter of the multi-objective sample. We use an optimization framework over multi-objective samples [11]. This framework relies on a critical property of ForEach samples: While not guaranteeing optimization, they do facilitate testing of the quality of the sample approximate optimizer Q returned by A: If the clustering cost of V (Q | X,w) agrees with the estimate V (Q | S,w′) then we can certify that Q is an approximate optimizer (in the sense of A) over the full data X. Otherwise, V (Q | S,w′) serves as an approximate lower bound for what we could do over X. Conveniently, this test can be performed with high confidence using another independent validation ForEach multi-objective pps sample. Our wrapper algorithm iteratively doubles the multi-objective pps sample size S and applies A to cluster S until the sample (approximate) minimizer satisfies the test. This approach allows us to work with the smallest sample that suffices to meet the specified quality.\nThe paper is organized as follows. The relevant components of the machinery of multi-objective pps sampling and its application to clustering are reviewed in Section 2. Section 3 presents a statement of our one2all theorem and some implications. Section 4 provides a full proof of our one2all Theorem."
    }, {
      "heading" : "2 Multi-objective pps samples for clustering",
      "text" : "We review the framework of weighted and multi-objective weighted sampling in our context of clustering costs. Consider approximating the clustering cost V (Q | X,w) from a sample S of X. For probabilities px > 0 for x ∈ X and a sample S drawn according to these probabilities, we have the unbiased inverse probability estimator [19] of V (Q | X,w):\nV̂ (Q | X,w) = ∑ x∈S wx dxQ px = V (Q | S, {wx/px}) . (3)\nNote that the estimate is equal to the clustering cost of S with weights wx/px by Q."
    }, {
      "heading" : "2.1 Probability proportional to size (pps) sampling",
      "text" : "[17] To obtain guarantees on the estimate quality of the clustering cost by Q, we need to use weighted sampling. The pps base probabilities for x ∈ X are\nψx(Q | X,w) = wxdxQ∑ y∈X wydyQ . (4)\nThe pps probabilities for a sample with size parameter r > 1 are\nψ(r)x (Q | X,w) = min{1, rψx(Q | X,w)} .\nWith pps sampling we obtain the following guarantees:\nTheorem 2.1 Consider a sample S where each x ∈ X is included (independently or using VarOpt dependent sampling [7, 12]) with probability px ≥ ψ(r)x (Q | X,w). Then the estimate (3) has the following statistical guarantees:\n• The coefficient of variation (CV), defined as the ratio of the standard deviation to the mean, (measure of the “relative error”) is at most = 1/ √ r.\n• The estimate is well concentrated in the Chernoff-Bernstein sense: The probability of relative error larger than c decreases exponentially with c.\nNote that the (expected) sample size is ∑ x px. When px = ψ (r) x (Q | X,w), the size is at most r."
    }, {
      "heading" : "2.2 Multi-objective pps sampling",
      "text" : "When we seek estimates with statistical guarantees for a set Q of queries (for example, all sets of k points in the metric spaceM), we use multi-objective samples [13, 11]. The multi-objective (MO) pps base sampling probabilities are defined as the maximum of the pps base probabilities over Q ∈ Q:\nψx(Q | X,w) = max Q∈Q ψx(Q | X,w) . (5)\nFor a size parameter r, the multi-objective pps probabilities are\nψ(r)x (Q | X,w) = min{1, rψx(Q | X,w)} = max Q∈Q ψ(r)x (Q | X,w) .\nAs a corollary of Theorem 2.1, noting that\n∀Q ∈ Q, ψx(Q | X,w) ≥ ψx(Q | X,w) ,\nwe obtain the following.\nCorollary 2.1 Consider a sample of X where each x ∈ X is included (independently or using VarOpt dependent sampling) with probability px ≥ min{1, rψx(Q | X)}. Then for each Q ∈ Q the inverse probability estimator (3) has the statistical guarantees stated in Theorem 2.1\nWith some notation abuse, we define the overhead of multi-objective sampling Q to be:\nh(Q | X,w) ≡ h(ψ(Q | X,w)) = ∑ x∈X ψx(Q | X,w) .\nNote that with size parameter r, the multi-objective pps sample size is at most rh(Q | X,w), so h(Q | X,w) bounds the factor-increase in sample size due to the “multi-objectiveness” of the sample.\nSometimes we can not compute ψ exactly but can instead efficiently obtain upper bounds π ≥ ψ(Q | X,w). Accordingly, we use sampling probabilities π(r)x = min{1, rπx}. The use of upper bounds increases the sample size, and we refer to h(π) = ∑ x πx as the overhead of π. We therefore seek upper-bounds π with a overhead not much larger than h(Q | X,w). Note that when sampling with the upper-bounds π, we retain the statistical guarantees on estimation quality as stated in Theorem 5."
    }, {
      "heading" : "2.3 Optimization over multi-objective pps samples",
      "text" : "We now consider clustering, with an approximate or bicriteria-approximate objective. We would like to perform this optimization by running a clustering algorithm A on the sample S with weights wx/px.\nA multi-objective pps sample for Q with size parameter r = −2 provides ForEach guarantees. This guarantee is not sufficient to ensure that Q returned by A satisfies the quality requirements of A, even approximately, over X, but it does allow us to test that using:\nV (Q | X,w) ≤ (1 + )V (Q | S, {wx/px}) . (6)\nTo perform the test, we can compute the exact cost V (Q | X,w). Alternatively, we can simply draw a different, independent, ForEach sample S′, using the same distribution, and compute the estimate V (Q | S′, {wx/px}).\nIf the test is satisfied, we know (within the ForEach guarantees and the approximation quality guarantee provided by A) that Q is an approximate solution over X. If the condition (6) does not hold, it still provides us with an approximate lower bound of the solution over X. We can increase the sample size and apply A to the larger sample. A pseudocode for this wrapper is provided as Algorithm 1.\nAlgorithm 1 Wrapper for clustering over samples Input: points X, weights w > 0, (upper bounds on) base pps probabilities π for Q, > 0, an approximate clustering\nalgorithm A that inputs a weighted set of points and returns Q ∈ Q that satisfies some (bi-criteria) quality guarantees.\n// Initialization foreach x ∈ X do // for sampling\nux ∼ U [0, 1] r ← −2 // Start with ForEach guarantee // Main Loop repeat\nS ←⊥ // Initialize sample foreach x ∈ X such that ux ≤ rπx do // sample points\nS ← S ∪ {x} // Cluster the sample S foreach x ∈ S do // weights for sampled points\nw′x ← wx/min{1, rπx} Q← A(S,w) // Apply approximate clustering algorithm A to sample r ← 2r // Double the sample size parameter until V (Q | X,w)) ≤ (1 + )V (Q | S,w′) // Exact or approx using a validation sample\nreturn Q"
    }, {
      "heading" : "3 The one2all Theorem statement and implications",
      "text" : "Consider a relaxed metric spaceM where distances satisfy all properties of a metric space except that the triangle inequality is relaxed using a parameter ρ ≥ 1:\n∀x, y, z ∈M, dxy ≤ ρ(dxz + dzy) . (7)\nFor v > 0, we use the notation Q(v) = {Q | V (Q | X,w) ≥ v}\nfor the set of all subsets Q ⊂M that cluster X with cost at least v. For a set M , we denote by\nXq = {x ∈ X | dxq = dxM}\nthe points in X that are closest to q ∈ M . In case of ties we apply arbitrary tie breaking to ensure that Xq q ∈M is a partition of X. We will assume that Xq is not empty for all q ∈M , since otherwise, we can remove the point q from M without affecting the clustering cost of X by M .\nFor the set M , we define the probabilities π(M) ≡ π(M |X,w) as follows:\n∀m ∈M, ∀x ∈ Xm, π(M |X,w)x = min { 1,max { 2ρ\ndxM V (M | X,w) , 8ρ2\nw(Xm)\n}} . (8)\nWe are now ready to state our one2all Theorem: We show that 1απ (M |X,w) upper bound the multi-objective pps base probabilities for Q(v) for v = αV (M | X,w). The full proof of the Theorem is provided in the next section.\nTheorem 3.1 LetM be a relaxed metric space with parameter ρ, X ⊂M be finite set of points with weights w, and M ⊂M be a finite set of points. Let v = V (M | X,w) Then for all α ≤ 1,\nmin{1, 1 α π(M |X,w)} ≥ ψ(Q(αv) | X,w) .\nCorollary 3.1 The multi-objective overhead of the set of all sets of centers Q with clustering cost at least v = αV (M | X,w) is at most h(Q(v) ≤ 1α (8ρ 2|M |+ 2ρ). Proof Note that the sum ∑ x π (M |X,w) x ≤ 8ρ2|M |+ 2ρ\nCorollary 3.2 For k ≥ 1, let Q be the set of all k-subsets of points in a relaxed metric space M with parameter ρ. The multi-objective pps overhead of Q satisfies\nh(Q) ≤ 8ρ2k + 2ρ .\nProof We apply Theorem 3.1 with M being the k-means optimum and α = 1."
    }, {
      "heading" : "4 Proof of the one2all Theorem",
      "text" : "To prove Theorem 3.1, we need to show that ∀Q such that V (Q | X,w) ≥ αV (M | X,w) and ∀x ∈ X,\ndxQ V (Q | X,w) ≤ 1 α π(M |X,w)x . (9)\nWe will do a case analysis, as illustrated in Figure 1. We first consider Q and points x such that the distance of x to Q is not much larger than the distance of x to M . Property (9) follows using the first term of the maximum in (8) by applying the following Lemma with c = 2ρ:\nLemma 4.1 Let Q and x be such that dxQ ≤ cdxM . Then\ndxQ V (Q | X,w) ≤ c α dxM V (M | X,w) .\nProof Since Q ∈ Q(V (M)), V (Q | X,w) ≥ αV (M | X,w). Therefore\ndxQ V (Q | X,w) ≤ 1 α dxQ V (M | X,w) ≤ c α dxM V (M | X,w) .\nIt remains to consider the complementary case where point x is much closer to M than to Q:\ndxQ ≥ 2ρdxM . (10)\nWe first introduce a useful definition: For a point q ∈ M , we denote by ∆q the weighted median of the distances dqy for y ∈ Xq, weighted by wy. The median ∆q is a value that satisfies the following two conditions:∑\nx∈Xq|dxq≤∆q\nwx ≥ 1\n2 w(Xq) (11)\n∑ x∈Xq|dxq≥∆q wx ≥ 1 2 w(Xm) . (12)\nIt follows from (12) that for all q ∈M ,\nV (M | Xq,w) = ∑ x∈Xq wxdqx ≥ ∑ x∈Xq|dxq≥∆q wxdxq ≥ ∆q ∑ x∈Xq|dxq≥∆q wx ≥ 1 2 w(Xm)∆q .\nTherefore,\nV (M | X,w) = ∑ q∈M V (M | Xq,w) ≥ 1 2 ∑ q∈M w(Xm)∆q . (13)\nWe now return to our proof for x that satisfies (10). We will show that property (9) holds using the second term in the max operation in the definition (8). Specifically, let m be the closest M point to x. We will show that\ndxQ V (Q | X,w) ≤ 8ρ 2 α\n1\nw(Xm) . (14)\nWe divide the proof to two subcases, in the two following Lemmas, each covering the complement of the other: When dmQ ≥ 2ρ∆m and when dmQ ≤ 2ρ∆m.\nLemma 4.2 Let Q and point x be such that\n∃m ∈M, dmx < 1\n2ρ dxQ and dmQ ≥ 2ρ∆m .\nThen dxQ\nV (Q | X,w) ≤ 8ρ\n2\nw(Xm) .\nProof Let q = arg minz∈Q dmz be the closest Q point to m. From (relaxed) triangle inequality (7) and our assumptions:\ndxQ ≤ dxq ≤ ρ(dmq + dmx) = ρ(dmQ + dmx) ≤ ρdmQ + 1\n2 dxQ .\nRearranging, we get dxQ ≤ 2ρdmQ . (15)\nConsider a point y such that dmy ≤ ∆m. Let q′ = arg minz∈Q dyz be the closest Q point to y. From relaxed triangle inequality we have dmq′ ≤ ρ(dyq′ + dym) and therefore\ndyQ = dyq′ ≥ 1\nρ dmq′ − dym ≥\n1 ρ dmQ −∆m ≥ 1 ρ dmQ − 1 2ρ dmQ ≥ 1 2ρ dmQ .\nThus, using the definition of ∆m (11):\nV (Q | X,w) ≥ ∑\ny|dyQ≤∆m\nwydyQ ≥ 1\n2ρ ∑ y|dyQ≤∆m wydmQ\n≥ 1 2ρ dmQ ∑ y∈Xm|dyQ≤∆m wy ≥ 1 2ρ dmQ w(Xm) 2 = 1 4ρ dmQw(Xm) . (16)\nCombining (15) and (16) we obtain:\ndxQ V (Q | X,w) ≤ 2ρdmQ1 4ρw(Xm)dmQ = 8ρ2 1 w(Xm) .\nLemma 4.3 Let Q and point x be such that\n∃m ∈M, dxm < 1\n2ρ dxQ and dmQ ≤ 2ρ∆m .\nThen dxQ\nV (Q | X,w) ≤ 8ρ\n2\nα\n1\nw(Xm) .\nProof Let q = arg minz∈Q dzm be the closest Q point to m. We have\ndxQ ≤ dxq ≤ ρ(dxm + dmq) ≤ 1\n2 dxQ + ρdmQ ≤\n1 2 dxQ + 2ρ 2∆m\nTherefore, dxQ ≤ 4ρ2∆m . (17)\nUsing (13) we obtain\nV (Q | X,w) ≥ αV (M | X,w) ≥ α1 2 ∑ y∈M w(Xy)∆y ≥ α 1 2 w(Xm)∆m . (18)\nCombining (17) and (18) we obtain\ndxQ V (Q | X,w) ≤ 4ρ 2∆m\n1 2αw(Xm)∆m\n≤ 8ρ 2\nα\n1\nw(Xm) ."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We presented the one2all construction for clustering costs: From any set of centers M with cost V (M) we obtain a summary structure of size O(|M |), in the form of a multi-objective sample, with which we can estimate the clustering cost by any set Q with cost that is at least a fraction of V (M).\nLooking forward, we point on some potential applications beyond estimation and optimization of clustering cost. First, we note that the set of distances of Q to the one2all sample S is essentially a sketch of the full (weighted) distance vector of Q to X [13]. Sketches of different sets Q allow us to estimate relations between the respective full vectors, such as distance norms, weighted Jaccard similarity, quantile aggregates, and more, which can be useful building blocks in other applications. Second, recent work casted Euclidean k-means clustering as a constrained rank-k approximation problem [14]. This connection facilitated interesting feedback between techniques designed for low-rank approximation and for clustering. We thus hope that our technique and insights might lead to further progress on other low-rank approximation problems."
    } ],
    "references" : [ {
      "title" : "Geometric approximation via coresets",
      "author" : [ "P.K. Agarwal", "S. Har-Peled", "K.R. Varadarajan" ],
      "venue" : "Combinatorial and computational geometry, MSRI. University Press",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "NP-hardness of Euclidean sum-of-squares clustering",
      "author" : [ "D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat" ],
      "venue" : "Mach. Learn., 75(2)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "K-means++: The advantages of careful seeding",
      "author" : [ "D. Arthur", "S. Vassilvitskii" ],
      "venue" : "SODA",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The hardness of approximation of Euclidean k-means",
      "author" : [ "P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A.K. Sinop" ],
      "venue" : "SoCG",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "New frameworks for offline and streaming coreset constructions",
      "author" : [ "V. Braverman", "D. Feldman", "H. Lang" ],
      "venue" : "CoRR, abs/1612.00889",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Selecting several samples from a single population",
      "author" : [ "K.R.W. Brewer", "L.J. Early", "S.F. Joyce" ],
      "venue" : "Australian Journal of Statistics, 14(3):231–239",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "A general purpose unequal probability sampling plan",
      "author" : [ "M.T. Chao" ],
      "venue" : "Biometrika, 69(3):653–656",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Average distance queries through weighted samples in graphs and metric spaces: High scalability with tight statistical guarantees",
      "author" : [ "S. Chechik", "E. Cohen", "H. Kaplan" ],
      "venue" : "RANDOM. ACM",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On coresets for k-median and k-means clustering in metric and Euclidean spaces and their applications",
      "author" : [ "K. Chen" ],
      "venue" : "SIAM J. Comput., 39(3)",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Size-estimation framework with applications to transitive closure and reachability",
      "author" : [ "E. Cohen" ],
      "venue" : "J. Comput. System Sci., 55:441–453",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Multi-objective weighted sampling",
      "author" : [ "E. Cohen" ],
      "venue" : "HotWeb. IEEE",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Efficient stream sampling for variance-optimal estimation of subset sums",
      "author" : [ "E. Cohen", "N. Duffield", "C. Lund", "M. Thorup", "H. Kaplan" ],
      "venue" : "SIAM J. Comput., 40(5)",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Coordinated weighted sampling for estimating aggregates over multiple weight assignments",
      "author" : [ "E. Cohen", "H. Kaplan", "S. Sen" ],
      "venue" : "VLDB, 2(1–2)",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Dimensionality reduction for k-means clustering and low rank approximation",
      "author" : [ "M.B. Cohen", "S. Elder", "C. Musco", "C. Musco", "M. Persu" ],
      "venue" : "STOC. ACM",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A unified framework for approximating and clustering data",
      "author" : [ "D. Feldman", "M. Langberg" ],
      "venue" : "STOC. ACM",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Turning big data into tiny data: Constant-size coresets for k-means",
      "author" : [ "D. Feldman", "M. Schmidt", "C. Sohler" ],
      "venue" : "PCA and projective clustering. In SODA. ACM-SIAM",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On the theory of sampling from finite populations",
      "author" : [ "M.H. Hansen", "W.N. Hurwitz" ],
      "venue" : "Ann. Math. Statist., 14(4)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1943
    }, {
      "title" : "On coresets for k-means and k-median clustering",
      "author" : [ "S. Har-Peled", "S. Mazumdar" ],
      "venue" : "STOC. ACM",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A generalization of sampling without replacement from a finite universe",
      "author" : [ "D.G. Horvitz", "D.J. Thompson" ],
      "venue" : "Journal of the American Statistical Association, 47(260):663–685",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1952
    }, {
      "title" : "A local search approximation algorithm for k-means clustering",
      "author" : [ "T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu" ],
      "venue" : "Computational Geometry, 28(2)",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Retaining units after changing strata and probabilities",
      "author" : [ "L. Kish", "A. Scott" ],
      "venue" : "Journal of the American Statistical Association, 66(335):pp. 461–470",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Least squares quantization in PCM",
      "author" : [ "S. Lloyd" ],
      "venue" : "IEEE Trans. Inf. Theor.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Optimal time bounds for approximate clustering",
      "author" : [ "R.R. Mettu", "C.G. Plaxton" ],
      "venue" : "Mach. Learn., 56(1-3)",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Fixed sample size pps approximations with a permanent random number",
      "author" : [ "P.J. Saavedra" ],
      "venue" : "Proc. of the Section on Survey Research Methods, pages 697–700, Alexandria, VA",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Model Assisted Survey Sampling",
      "author" : [ "C-E. Särndal", "B. Swensson", "J. Wretman" ],
      "venue" : "Springer",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Sampling Algorithms",
      "author" : [ "Y. Tillé" ],
      "venue" : "Springer-Verlag, New York",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A constant-factor bi-criteria approximation guarantee for k-means++",
      "author" : [ "D. Wei" ],
      "venue" : "NIPS",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Clustering is computationally hard [2] even on Euclidean spaces and even to tightly approximate [4].",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "Clustering is computationally hard [2] even on Euclidean spaces and even to tightly approximate [4].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "There is a local search polynomial algorithm with 9 + approximation ratio [20].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "In practice, clustering is solved using heuristics, most notably Lloyd’s algorithm [22], and scalable approximation algorithms such as kmeans++ [3], which obtains a log k factor approximation using k centers.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "In practice, clustering is solved using heuristics, most notably Lloyd’s algorithm [22], and scalable approximation algorithms such as kmeans++ [3], which obtains a log k factor approximation using k centers.",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 26,
      "context" : "A recent result [27] established that kmeans++ provides constant factors bi-criteria guarantees: When applied to obtain βk centers for some constant β > 1, the clustering cost is within a constant factor of the optimum k-means cost.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "The term coresets for such structures was introduced in the computational geometry literature [1, 18], building on the theory of -nets.",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "The term coresets for such structures was introduced in the computational geometry literature [1, 18], building on the theory of -nets.",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 22,
      "context" : "Some notable constructions include [23, 9, 15, 16].",
      "startOffset" : 35,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "Some notable constructions include [23, 9, 15, 16].",
      "startOffset" : 35,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "Some notable constructions include [23, 9, 15, 16].",
      "startOffset" : 35,
      "endOffset" : 50
    }, {
      "referenceID" : 15,
      "context" : "Some notable constructions include [23, 9, 15, 16].",
      "startOffset" : 35,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "Initial coresets constructions had bounds with high (exponential or high polynomial) dependence on some parameters (dimension, , k) and poly logarithmic dependence on n, with the best current asymptotic bound of O(k −2 log k log n) claimed in [5].",
      "startOffset" : 243,
      "endOffset" : 246
    }, {
      "referenceID" : 14,
      "context" : "While a notion of “weak coresets” that aim to only support optimization was considered in the coreset literature [15], the constructions are also worst-case.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 10,
      "context" : "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].",
      "startOffset" : 212,
      "endOffset" : 227
    }, {
      "referenceID" : 5,
      "context" : "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].",
      "startOffset" : 212,
      "endOffset" : 227
    }, {
      "referenceID" : 23,
      "context" : "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].",
      "startOffset" : 212,
      "endOffset" : 227
    }, {
      "referenceID" : 9,
      "context" : "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].",
      "startOffset" : 212,
      "endOffset" : 227
    }, {
      "referenceID" : 24,
      "context" : "The theory of weighted sampling [25, 26] tells us that to estimate the sum V (Q | X,w) it suffices to sample −2 points with probabilities px ∝ wxdQx proportional to their contribution to the sum [17].",
      "startOffset" : 32,
      "endOffset" : 40
    }, {
      "referenceID" : 25,
      "context" : "The theory of weighted sampling [25, 26] tells us that to estimate the sum V (Q | X,w) it suffices to sample −2 points with probabilities px ∝ wxdQx proportional to their contribution to the sum [17].",
      "startOffset" : 32,
      "endOffset" : 40
    }, {
      "referenceID" : 16,
      "context" : "The theory of weighted sampling [25, 26] tells us that to estimate the sum V (Q | X,w) it suffices to sample −2 points with probabilities px ∝ wxdQx proportional to their contribution to the sum [17].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 18,
      "context" : "The inverse-probability [19] estimate obtained from the sample S, V̂ (Q | X,w) ≡ V (Q | S, {wx/px}) , is an unbiased estimate of V (Q | X,w) with well-concentrated normalized squared error of .",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "Our result generalizes previous work [8] that only applied to the case where k = 1, where clustering cost reduces to inverse classic closeness centrality (sum of distances from a single point Q).",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "To obtain an efficient construction, we can apply kmeans++ [3] or another efficient bi-criteria approximation algorithm to compute M of size 2k that has cost that is at most the optimum divided by some constant α [27].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 26,
      "context" : "To obtain an efficient construction, we can apply kmeans++ [3] or another efficient bi-criteria approximation algorithm to compute M of size 2k that has cost that is at most the optimum divided by some constant α [27].",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 10,
      "context" : "We use an optimization framework over multi-objective samples [11].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 18,
      "context" : "For probabilities px > 0 for x ∈ X and a sample S drawn according to these probabilities, we have the unbiased inverse probability estimator [19] of V (Q | X,w):",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 16,
      "context" : "1 Probability proportional to size (pps) sampling [17] To obtain guarantees on the estimate quality of the clustering cost by Q, we need to use weighted sampling.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "1 Consider a sample S where each x ∈ X is included (independently or using VarOpt dependent sampling [7, 12]) with probability px ≥ ψ x (Q | X,w).",
      "startOffset" : 101,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "1 Consider a sample S where each x ∈ X is included (independently or using VarOpt dependent sampling [7, 12]) with probability px ≥ ψ x (Q | X,w).",
      "startOffset" : 101,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "2 Multi-objective pps sampling When we seek estimates with statistical guarantees for a set Q of queries (for example, all sets of k points in the metric spaceM), we use multi-objective samples [13, 11].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : "2 Multi-objective pps sampling When we seek estimates with statistical guarantees for a set Q of queries (for example, all sets of k points in the metric spaceM), we use multi-objective samples [13, 11].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 0,
      "context" : "// Initialization foreach x ∈ X do // for sampling ux ∼ U [0, 1] r ← −2 // Start with ForEach guarantee // Main Loop repeat S ←⊥ // Initialize sample foreach x ∈ X such that ux ≤ rπx do // sample points S ← S ∪ {x} // Cluster the sample S foreach x ∈ S do // weights for sampled points w′ x ← wx/min{1, rπx} Q← A(S,w) // Apply approximate clustering algorithm A to sample r ← 2r // Double the sample size parameter until V (Q | X,w)) ≤ (1 + )V (Q | S,w′) // Exact or approx using a validation sample",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "First, we note that the set of distances of Q to the one2all sample S is essentially a sketch of the full (weighted) distance vector of Q to X [13].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "Second, recent work casted Euclidean k-means clustering as a constrained rank-k approximation problem [14].",
      "startOffset" : 102,
      "endOffset" : 106
    } ],
    "year" : 2017,
    "abstractText" : "Clustering is a fundamental technique in data analysis. Consider data points X that lie in a (relaxed) metric space (where the triangle inequality can be relaxed by a constant factor). Each set of points Q (centers) defines a clustering of X according to the closest center with cost V (Q) = ∑ x∈X dxQ. This formulation generalizes classic k-means clustering, which uses squared distances. Two basic tasks, parametrized by k ≥ 1, are cost estimation, which returns (approximate) V (Q) for queries Q such that |Q| = k and clustering, which returns an (approximate) minimizer of V (Q) of size |Q| = k. With very large data sets X, we seek efficient constructions of small summaries that allow us to efficiently approximate clustering costs over the full data. We present a novel data reduction tool based on multi-objective probability-proportional-to-size (pps) sampling: Our one2all construction inputs any set of centers M and efficiently computes a sample of size O(|M |) from which we can tightly estimate the clustering cost V (Q) for any Q that has at least a fraction of the clustering cost of M . For cost queries, we apply one2all to a bicriteria approximation to obtain a sample of size O(k −2) for all |Q| = k. For clustering, we propose a wrapper that applies a black-box algorithm to a sample and tests clustering quality over X, adaptively increasing the sample size. Our approach exploits the structure of the data to provide quality guarantees through small samples, without the use of typically much larger worst-case-size summaries.",
    "creator" : "LaTeX with hyperref package"
  }
}