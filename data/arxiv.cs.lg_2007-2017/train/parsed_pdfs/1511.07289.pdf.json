{
  "name" : "1511.07289.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "EXPONENTIAL LINEAR UNITS (ELUS)",
    "authors" : [ "Djork-Arné Clevert", "Thomas Unterthiner" ],
    "emails" : [ "okko@bioinf.jku.at", "unterthiner@bioinf.jku.at", "hochreit@bioinf.jku.at" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We introduce the “exponential linear unit” (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs also avoid a vanishing gradient via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero. Zero means speed up learning because they bring the gradient closer to the unit natural gradient. We show that the unit natural gradient differs from the normal gradient by a bias shift term, which is proportional to the mean activation of incoming units. Like batch normalization, ELUs push the mean towards zero, but with a significantly smaller computational footprint. While other activation functions like LReLUs and PReLUs also have negative values, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. Consequently, dependencies between ELUs are much easier to model and distinct concepts are less likely to interfere. We found that ELUs lead not only to faster learning, but also to better generalization performance once networks have many layers (≥ 5). ELU networks were among top 10 reported CIFAR-10 results and yielded the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Currently the most popular activation function for neural networks is the rectified linear unit (ReLU), which was first proposed for restricted Boltzmann machines Nair & Hinton (2010) and then successfully used for neural networks Glorot et al. (2011); Zeiler et al. (2013); Dahl et al. (2013). The ReLU activation function is linear with slope 1 for positive arguments and zero otherwise, that is, for positive values it is the identity and for negative values the zero function. Besides their sparseness, the main advantage of ReLUs is that they avoid the vanishing gradient Glorot et al. (2011); Maas et al. (2013); Hochreiter & Schmidhuber (1997); Hochreiter (1991; 1998); Hochreiter et al. (2001a) since their derivative is 1 for positive values. However ReLUs are non-negative and, therefore, have a mean activation larger than zero.\nUnits that have a non-zero mean activation act as bias for the next layer. If such units do not cancel each other out, learning causes a bias shift for units in next layer. The more units are correlated, the higher their bias shift. We will see that Fisher optimal learning, i.e., the natural gradient Amari (1997); Yang & Amari (1998); Amari (1998), would correct for the bias shift by adjusting the bias weight update. Thus, less bias shift brings the standard gradient closer to the natural gradient and\nar X\niv :1\n51 1.\n07 28\n9v 1\n[ cs\n.L G\n] 2\n3 N\nov 2\n01 5\nspeeds up learning. Therefore we aim at activations that have a mean close to zero across the training examples, in order to decrease the bias shift effect.\nCentering the activation functions at zero has been proposed in order to keep the off-diagonal entries of the Fisher information matrix small Raiko et al. (2012) but was previously found to speed up learning in neural networks LeCun et al. (1991; 1998); Schraudolph (1998). Centering also improved learning in restricted Boltzmann machines Cho et al. (2011); Montavon & Müller (2012). To counter the internal covariate shift of the network activations, “batch normalization” has been suggested which also centers the hidden activations Ioffe & Szegedy (2015). The Projected Natural Gradient Descent algorithm (PRONG) goes beyond batch normalization by implicitly whitening the activations Desjardins et al. (2015), therefore also centers them at zero. All these methods tweak the network parameters to obtain desired statistics of the activation functions. However distorting the network parameters may take back the last learning update and therefore hamper learning. Instead, natural gradient descent uses efficient update directions without restricting the network dynamics.\nAn alternative way to push the mean activation towards zero is using an appropriate activation function. Therefore tanh was previously preferred over logistic functions LeCun et al. (1991; 1998). Recently “Leaky ReLUs” (LReLUs) have been proposed, which replace the negative part of the ReLU with a linear function Maas et al. (2013). Leaky ReLUs have been shown to be superior to ReLUs in terms of learning speed and performance Maas et al. (2013); Xu et al. (2015). They have later been generalized to Parametric Rectified Linear Units (PReLUs) He et al. (2015), which adjust the slope of the negative part during learning. PReLUs performed very good on large image benchmark data sets, where they showed better learning behavior than ReLUs He et al. (2015). Another version of leaky ReLUs are Randomized Leaky Rectified Linear Units (RReLUs), where the slope of the negative part is randomly sampled Xu et al. (2015). Evaluations on image benchmark datasets and convolutional networks demonstrated that non-zero slopes for the negative part in ReLUs improve results Xu et al. (2015).\nIn contrast to ReLUs, LReLUs, PReLUs, and RReLUs do no longer ensure a noise-robust deactivation state of the units. We propose an activation function that has negative values to allow for mean activations close to zero, but at the same time saturate to a negative value with decreasing input. The saturation effect decreases the variation of the units if deactivated, so the precise deactivation value is irrelevant. Such an activation function can code the degree of presence of particular phenomena in the input, but does not quantitatively model the degree of their absence. Therefore, our new activation function is more robust to noise. Consequently, dependencies between coding units are much easier to model and much easier to interpret. Furthermore, distinct concepts are much less likely to interfere with such activation functions since the deactivation state is non-informative, i.e. variance decreasing."
    }, {
      "heading" : "2 ZERO MEAN ACTIVATIONS SPEED UP LEARNING",
      "text" : "To derive and analyze the bias shift effect mentioned in Section1 we use the natural gradient. The natural gradient is an update direction which corrects the gradient direction with the Fisher information matrix. The natural gradient method is founded on information geometry Amari (1985); Amari & Murata (1993) and became very popular for independent component analysis Amari et al. (1996); Choi et al. (1998); Zhang et al. (1999). Thereafter, the natural gradient has also been used for stochastic models Park et al. (2000) and for neural networks Amari (1997); Yang & Amari (1998); Amari (1998). The recently introduced Hessian-Free Optimization techniques Martens (2010); Martens & Sutskever (2011); Chapelle & Erhan (2011); Kiros (2013) and the Krylov Subspace Descent methods Mizutani & Demmel (2003); Vinyals & Povey (2012) use an extended Gauss-Newton approximation of the Hessian, therefore they can be interpreted as versions of natural gradient descent Pascanu & Bengio (2014). The natural gradient was also applied to deep Boltzmann machines Desjardins et al. (2014), reinforcement learning Kakade (2002); Peters et al. (2005); Bhatnagar et al. (2008), and natural evolution strategies Sun et al. (2009); Wierstra et al. (2014).\nSince for neural networks and deep learning the computation of the Fisher information matrix is too expensive, different approximations of the natural gradient have been proposed. For example the Fisher matrix can be approximated in a block-diagonal form, where unit-wise or quasi-diagonal natural gradients are used Olivier (2013). Unit-wise natural gradients have been previously used Kurita (1993) and go back to natural gradients for perceptrons Amari (1998); Yang & Amari (1998). Top-\nmoumoute Online natural Gradient Algorithm (TONGA) LeRoux et al. (2008) uses a low-rank approximation of natural gradient descent. FActorized Natural Gradient (FANG) Grosse & Salakhudinov (2015) estimates the natural gradient via an approximation of the Fisher matrix by a Gaussian graphical model. The natural gradient has been combined with the Newton method for an efficient second order method LeRoux & Fitzgibbon (2010).\nWe assume a parametrized probabilistic model p(x;w) with parameter vector w and data x. The training data are X = (x1, . . . ,xN ) ∈ R(d+1)×N with xn = (zTn , yn)T ∈ Rd+1, where zn is the input for example n and yn is its label. The loss of one example x = (zT , y)T using model p(.;w) is L(y, p(z;w)) and the average loss on the training data X is the empirical riskRemp(p(.;w),X). Gradient descent updates the weight vector w by wnew = wold−η∇wRemp where η is the learning rate. The natural gradient is the inverse Fisher information matrix F−1 multiplied by the gradient of the empirical risk: ∇natw Remp = F−1∇wRemp. For a multi-layer perceptron (MLP) a is the vector of the unit’s activity and a0 = 1 is the bias unit activity. We consider the ingoing weights to unit i, therefore we drop the index i: wj = wij for the weight from unit j to unit i, net = neti for the net input to unit i, a = ai for the activation of unit i, and δ = δi for the delta error at unit i. w0 is the bias weight of unit i. The net input is net = ∑ j wjaj . The activation function f maps the net input to the activation a = f(net). For example x = (zT , y)T , the delta error at unit i is δ = ∂∂netL (y, p (z;w)), which gives for the derivative ∂∂wjL (y, p (z;w)) = δaj .\nFor the Fisher information matrix, we need the gradient of the log output function ∂∂wj ln p (x;w). Since p (x;w) = p (y | z;w) p(z), only p (y | z;w) depends on the weights. Therefore we define the δ̂ at unit i as δ̂ = ∂∂net ln p (y | z;w), which can be computed via the backpropagation algorithm, but starting from the log-output probability instead of the loss function. For the derivative we obtain ∂ ∂wj ln p (y | z;w) = δ̂aj .\nWe restrict the Fisher information matrix to weights leading to unit i, which is the unit Fisher information matrix F . The unit Fisher information matrix F captures only the interactions of weights to unit i. Consequently, the unit natural gradient only corrects the interactions of weights to unit i. In particular the affect of the bias weight update can be adjusted by the unit natural gradient. Unit natural gradients have been previously used Kurita (1993); Olivier (2013) and go back to natural gradients for perceptrons Amari (1998); Yang & Amari (1998). The unit Fisher information matrix is\n[F (w)]kj = Ep(x;w)\n( ∂ ln p(y | z;w)\n∂wk\n∂ ln p(y | z;w) ∂wj\n) = Ep(x;w) ( δ̂2 ak aj ) (1)\n= Ep(z) ( Ep(y|z;w) ( δ̂2 ) ak aj ) = Ep(z) ( δ̄2 ak aj ) , δ̄2 = Ep(y|z;w) ( δ̂2 ) .\nWeighting the activations by δ̄2 is equivalent to adjusting the probability of drawing inputs z. z with large δ̄2 a drawn with higher probability and and inputs z with small δ̄2 with smaller probability. Since 0 ≤ δ̄2 = δ̄2(z), we can define a distribution q(z):\nq(z) = δ̄2(z) p(z) (∫ δ̄2(z) p(z) dz )−1 = δ̄2(z) p(z) E−1p(z) ( δ̄2 ) . (2)\nUsing the distribution q(z), the entries of the Fisher information matrix can be expressed as a covariance:\n[F (w)]kj = Ep(z) ( δ̄2 ak aj ) = ∫ δ̄2 ak aj p(z) dz = Ep(z) ( δ̄2 ) Eq(z) (ak aj) . (3)\nIf the bias unit is unit j = 0 with a0 = 1 and weight w0 then we can divide the complete weight vector w+ into a bias part w0 and the rest w: w+ = (wT , w0)T . For the row b of the Fisher information matrix that is associated with the bias, we have for b = [F (w)]0:\nb = Ep(z) ( δ̄2a ) = Ep(z) ( δ̄2 ) Eq(z) (a) = Covp(z) ( δ̄2,a ) + Ep(z) (a) Ep(z) ( δ̄2 ) . (4)\nThe next Theorem 1 gives the correction of the standard gradient by the unit natural gradient where the bias weight is treated separately. The resulting formulas are similar to Definition 6 of Olivier (2013). Theorem 1. The unit natural gradient corrects the weight update to a unit (∆wT ,∆w0)T by following affine transformation of the gradient∇(w,w0)Remp = (gT , g0)T :(\n∆w ∆w0\n) = ( A−1 (g − ∆w0 b) s ( g0 − bT A−1g )) , (5) where A is the unit Fisher information matrix without the bias weight. The vector b = [F (w)]0 is the unit Fisher matrix column corresponding to the bias weight, and the scalar s is\ns = E−1p(z) ( δ̄2 ) ( 1 + ETq(z) (a) Var −1 q(z) (a) Eq(z) (a) ) , (6)\nwhere a is the vector of activations of units with weights to unit i and q(z) = δ̄2(z) p(z) E−1p(z) ( δ̄2 ) , δ̄2(z) = Ep(y|z;w) ( δ̂2 ) . (7)\nProof. We separate the local Fisher matrix F of unit i with W incoming weights w+ = (wT , w0)T into a bias part given by w0 and the rest w. Multiplying the inverse Fisher matrix with the separated gradient∇w+Remp(w+,X) = g+ = (gT , g0)T gives the weight update ∆w+ = (∆wT ,∆w0)T :(\n∆w ∆w0\n) = ( A b bT c )−1 ( g g0 ) = ( A−1 g + u s−1uTg + go u uTg + s g0 ) . (8)\nwhere\nb = [F (w)]0 , c = [F (w)]00 , u = − sA −1 b , s =\n( c − bTA−1b )−1 . (9)\nThis formula is derived in Lemma 1. When inserting ∆w0 in the update of ∆w, we obtain( ∆w ∆w0 ) = ( A−1 g + s−1u ∆w0 uTg + s g0 ) , ( ∆w ∆w0 ) = ( A−1 (g − ∆w0 b) s ( g0 − bT A−1g )) . (10) The right hand side is obtained by inserting u = −sA−1b in the left hand side update. Since c = F00 = Ep(z) ( δ̄2 ) , b = Ep(z) ( δ̄2 ) Eq(z) (a), and A = Ep(z) ( δ̄2 ) Eq(z) ( aaT ) , we obtain\ns = ( c − bTA−1b )−1 = E−1p(z) ( δ̄2 ) ( 1 − ETq(z) (a) E −1 q(z) ( aaT ) Eq(z) (a) )−1 . (11)\nApplying Lemma 2 gives the formula for s.\nCorollary 1. The weight update correction of the unit natural gradient for the bias weight increases monotonically with the length of the vector Eq(z) (a).\nProof. The weight update correction is given in Theorem 1. The larger the norm of Eq(z) (a) is, the larger is the multiplicative correction s of the bias weight update. The value s is a positive constant plus a quadratic form of the inverse of the positive definite covariance matrix Var−1q(z)(a) in Theorem 1. The additive bias correction of the bias weight update is the inner product between w’s natural gradient update A−1g and b. Since b is a multiple of Eq(z) (a), the inner product also increases with the length of Eq(z) (a). The correction for ∆w is directly proportional to both b, therefore also to Eq(z) (a), and ∆w0.\nWe compute the bias shift of unit i caused by the weight update, i.e. the change of the mean of unit i. Towards this end we compute the average change of the net input of unit i. Since only the activations depend on the input, the bias shift is given by multiplying the weight update by the mean of the activation vector a, where we use ∆w0 = − s bTA−1g + s g0:(\nEp(z)(a) 1 )T ( ∆w ∆w0 ) = ( Ep(z)(a) 1 )T ( A−1 g − A−1b ∆w0 ∆w0 ) (12)\n= Ep(z)(a) T A−1 g + ( 1 − Ep(z)(a)T A−1b ) ∆w0\n= ( Ep(z)(a) T − ( 1 − Ep(z)(a)T A−1b ) s bT ) A−1 g + s ( 1 − Ep(z)(a)T A−1b ) g0 .\nIn Eq. (12) the term ( Ep(z)(a) T − ( 1− Ep(z)(a)TA−1b ) sbT ) A−1g is the bias shift due weight\nupdates of connections from the previous layer while s ( 1− Ep(z)(a)TA−1b ) g0 is the shift of the\nmean value of unit i due to the regular bias weight update. The term ( 1− Ep(z)(a)TA−1b ) sb is the bias shift correction due to the unit natural gradient.\nNext, Theorem 2 states that the bias shift of unit i is mitigated or even prevented by the bias shift correction. Furthermore, the theorem shows that the bias shift correction is goverened by the size of Covp(z) ( δ̄2,a ) . Theorem 2. The bias shift correction of unit i by the unit natural gradient update is equivalent to following correction of the incoming mean Ep(z)(a):\n− (1 + k) Eq(z)(a) , where k = s Ep(z) ( δ̄2 ) CovTp(z) ( δ̄2,a ) A−1Eq(z)(a) . (13)\nFor Covp(z) ( δ̄2,a ) = 0, the incoming mean Ep(z)(a) is corrected to zero.\nProof. We analyze the bias shift correction term defined in Eq. (12), starting with its first factor: 1 − Ep(z)(a)T A−1b = 1 − Ep(z) ( δ̄2 ) Ep(z)(a) T A−1Eq(z)(a) (14)\n= 1 − ( Ep(z) ( δ̄2 ) Eq(z)(a) T − CovTp(z) ( δ̄2,a )) A−1Eq(z)(a)\n= 1 − Eq(z)(a)T E−1q(z)(aa T )Eq(z)(a) + Cov T p(z)\n( δ̄2,a ) A−1Eq(z)(a)\n= E−1p(z) ( δ̄2 ) s−1 + CovTp(z) ( δ̄2,a ) A−1Eq(z)(a) ,\nwhere we used Eq. (4). Next, we reformulate the full bias shift correction term using previous equation:\ns ( 1 − Ep(z)(a)T A−1b ) b = ( E−1p(z) ( δ̄2 ) + s CovTp(z) ( δ̄2,a ) A−1Eq(z)(a) ) b (15)\n= ( 1 + s Ep(z) ( δ̄2 ) CovTp(z) ( δ̄2,a ) A−1Eq(z)(a) ) Eq(z)(a) = (1 + k) Eq(z)(a) .\nFor Covp(z) ( δ̄2,a ) = 0 we have k = 0 and Eq(z)(a) = Ep(z)(a) (see Eq. (4)), therefore the mean incoming activation vector Ep(z)(a) is corrected to a zero mean vector by the unit natural gradient.\nIf the incoming units have non-zero means of the same sign, then the mean of unit i is likely to be shifted via the weight updates depending on the unit natural gradient A−1g. The unit natural gradient corrects this bias shift of unit i by the bias weight to ensure optimal learning. This correction can be viewed as shifting the mean activation of the incoming units towards zero. Without the correction, bias shift of unit i lead to oscillations and impede learning progress. Therefore small absolute means of the activations are desired for efficient learning. Small means may be achieved by (i) unit zero mean normalization LeCun et al. (1991; 1998); Schraudolph (1998); Cho et al. (2011); Raiko et al. (2012); Montavon & Müller (2012); Ioffe & Szegedy (2015); Desjardins et al. (2015) or (ii) activation functions with a negative part Maas et al. (2013); Xu et al. (2015); He et al. (2015). We introduce new activation functions with a negative part."
    }, {
      "heading" : "3 EXPONENTIAL LINEAR UNITS (ELUS)",
      "text" : "We introduce the exponential linear unit (ELU) with 0 < α:\nf(x) = { x if x ≥ 0 α (exp(x)− 1) if x < 0 , (16)\nf ′(x) = { 1 if x ≥ 0 f(x) + α if x < 0 . (17)\nThe ELU hyperparameter α controls the value to which an ELU saturate for negative net inputs (see Fig. 1). ELUs avoid the vanishing gradient problem as rectified linear units (ReLUs) and leaky ReLUs (LReLUs) do. The gradient does not vanish because the positive part of these functions is the identity, therefore their derivative is one. Thus, these activation functions are well suited for deep neural networks with many layers where a vanishing gradient impedes learning Unterthiner et al. (2014; 2015).\nIn contrast to ReLUs, ELUs have negative values which pushes the mean of the activations closer to zero. Mean activations that are closer to zero enable faster learning as they bring the gradient closer to the natural gradient (see Corollary 1 and Theorem 2). For the same reasons tanh was previously found to speed up learning compared to the logistic function LeCun et al. (1991; 1998). While LReLUs and PReLUs have also negative values, they do not ensure a noise-robust deactivation state, that is, a noise-robust negative value. ELUs saturate to a negative value for negative net inputs. Saturation means a small derivative of the activation function which decreases the variation and the information that is propagated to the next layer Hochreiter et al. (2001b); Hochreiter (1997; 1998). ELUs code the degree of presence of\ninput concepts, while they do not quantify the degree of their absence Clevert et al. (2015). Dependencies between ELUs are easier to model than between LReLUs or PReLUs because the causes for deactivations are not distinguished. Furthermore ELUs that code distinct concepts are less likely to interfere because uninformative negative values avoid distributed codes. Positively activated ELUs interact by activating units of the next layer."
    }, {
      "heading" : "4 EXPERIMENTS USING ELUS",
      "text" : "In this section, we assess the performance of exponential linear units (ELUs) if used for unsupervised and supervised learning of deep autoencoders and deep convolutional networks, by comparing them with (i) Rectified Linear Units (ReLUs) and (ii) Leaky Linear Units (LReLUs). For comparisons we use the following benchmark datasets to compare ELUs with other units: (i) MNIST (gray images in 10 classes, 60k training and 10k testing images), (ii) CIFAR-10 (color images in 10 categories, 50k training and 10k testing images), (iii) CIFAR-100 (color images in 100 categories, 50k training and 10k testing images), and (iv) ImageNet (color images in 1000 categories, 1.3M training and 100k testing images)."
    }, {
      "heading" : "4.1 MNIST",
      "text" : ""
    }, {
      "heading" : "4.1.1 LEARNING BEHAVIOR",
      "text" : "We first established that ELUs keep the mean activation closer to zero by training fully connected deep neural networks on the MNIST digit classification dataset and tracking each hidden unit’s activation during training. We did this for ELUs, ReLUs as well as Leaky ReLU units with a negative slope of 0.1. Each network had five hidden layers of 256 units each, and was trained by stochastic gradient descent using a learning rate of 0.01 with mini-batches of size 64 was trained for 50 epochs. The ELU hyperparameter α was set to 1.0 for all experiments. The weights have been initialized according to He et al. (2015), which has been specifically designed to speed up learning with ReLU units. After each epoch we calculated the units’ average activations on a fixed subset of the training data. We plotted the median over all units in Fig. 2, which shows that ELUs stay closer to zero throughout the training process, while their training error decreases much more rapidly than it does for the other activation functions.\n4.1.2 AUTOENCODER LEARNING\nTo evaluate how well ELUs work in an unsupervised setting. Therefore, we replicated the experiments from Martens (2010) and Desjardins et al. (2015), where a deep autoencoder was trained on the MNIST dataset, with the objective to minimize the reconstruction error. The encoder part of the network consisted of four fully connected hidden layers with sizes 1000, 500, 250 and 30, respectively. The decoder part was symmetrical to the encoder. We performed the experiments using four different learning rates (10−2, 10−3, 10−4, 10−5). Learning was based on stochastic gradient descent with minibatches of 64 samples for 500 epochs. Fig. 3 shows, that ELUs outperform the competing activation functions in terms of training set reconstruction error for all learning rates. We also note that similar to Desjardins et al. (2015), higher learning rates always seem to perform better."
    }, {
      "heading" : "4.2 COMPARISON OF ACTIVATION FUNCTIONS",
      "text" : "In this subsection, we conducted a benchmark experiment using a relatively simple deep convolutional network architecture in order to compare ELUs to ReLUs and LReLUs based on their learning behavior.\nThe model used for our baseline CIFAR-100 experiment consists of 11 convolutional layers arranged in stacks of {1 × 192 × 5, 2 × 240 × 3, 2 × 260 × 2, 2 × 280 × 2, 2 × 300 × 2, 1 × 300 × 1, 1 × 100 × 1} layers × units × receptive fields. 2×2 max-pooling with a stride of 2 was applied after each stack. For network regularization we used the following drop-out rate for the last layer of each stack {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.0}. The L2-weight decay regularization term was set to 0.0005. The following learning rate schedule was applied {0−35k(0.03), 35k−85k(0.015), 85k− 135k(0.0015), 135k − 165k(0.00015)} (iterations (learning rate)). The momentum term was fixed at 0.9. The dataset was preprocessed as described in Goodfellow et al. (2013) with global contrast normalization and ZCA whitening. Additionally, the images were padded with four zero pixels at all borders. The model was trained on 32× 32 random crops with random horizontal flipping. Besides that, we no further augmented the dataset during training.\nFig. 4 shows, that ELUs achieve better training loss and test error than ReLUs or LReLUs. Notice that for the sake of comparison, we used the same step-wise learning rate schedule for all activation functions, which was optimized for ReLU networks during previous experiments. Fig. 4 suggests that ELUs converge faster than ReLU and LReLU networks and therefore would benefit from a better designed schedule adjusted to ELUs. The current schedule was optimized for ReLUs and\nLReLUs. ELUs yield a test error of 28.16%, while ReLUs and LReLUs yield 29.06% and 28.78%, respectively. In summary, ELUs outperform ReLUs and LReLUs in terms of training loss and test error."
    }, {
      "heading" : "4.3 CLASSIFICATION PERFORMANCE ON CIFAR-100 AND CIFAR-10",
      "text" : "After showing that ELUs indeed posses the superior learning behavior as postulated in Section 3, the following experiments were meant to highlight their generalization capabilities. Therefore, the networks in this section have a more sophisticated architecture than the networks in the previous subsection. The architecture of the deep convolution network consists of 18 convolutional layers arranged in stacks of {1 × 384 × 3, 3 × 640 × 3, 4 × 768 × 3, 3 × 896 × 3, 3 × 1024 × 3, 3 × 1152 × 3, 1 × 100 × 1} layers × units × receptive fields. 2×2 max-pooling with a stride of 2 was applied after each stack. For network regularization we used the following drop-out rate for all layers in a stack {0.0, 0.15, 0.3, 0.45, 0.6, 0.8, 0.0}. The L2-weight decay regularization term was set to 0.0005. The initial learning rate was set to 0.01 and decreased by a factor of 10 after 35k iterations. The momentum term was fixed as 0.9 and the mini-batch size is fixed as 50. For both datasets we applied the same preprocessing, padding and cropping as described in the previous section.\nA comparison of ELU networks and other convolutional networks on CIFAR-10 and CIFAR-100 is given in Tab.1. On both datasets, ELU-networks performed best and reached a test error of 6.55% and 24.28% for CIFAR-10 and CIFAR-100. ELU networks were among top 10 reported CIFAR10 results and yielded the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging."
    }, {
      "heading" : "4.3.1 IMAGENET CHALLENGE DATASET",
      "text" : "In the final experiment, we evaluated ELU-networks on the 1000-class ImageNet dataset, which contains about 1.3M training color images as well as additional 50k images and 100k images for validation and testing, respectively. For this task, we designed a 15 layer deep convolutional network, which was arranged in stacks of {1×96×6, 3×512×3, 5×768×3, 3×1024×3, 2×4096× FC, 1 × 1000 × FC} layers × units × receptive fields or fully-connected (FC). 2×2 max-pooling with a stride of 2 was applied after each stack and spatial pyramid pooling (SPP) with 3 levels before the first FC layer He et al. (2015). For network regularization we set the L2-weight decay term to 0.0005 and used 50% drop-out in the two penultimate FC layers. Before training we re-sized the images to 256×256 pixels, subtracted the per-pixel mean. Training was done on 224 × 224 random crops with random horizontal flipping. Besides that, we didn’t augment the dataset during training. The single-model performance was evaluated on the single center crop with no further\naugmentation and yielded a top-5 validation error below 10%. Fig. 5 shows the learning behavior of our model for ELUs and ReLUs. Panel (b) suggests that ELUs start reducing the error earlier. The ELU-network already reaches the 50% top-5 error after 26k iterations, while the ReLU network needs 38k iterations to reach the same error rate."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper we introduced the novel exponential linear units, units with a nonlinear activation function for deep learning. These units have negative values, which allows the network to push the mean activation of its units closer to zero. This helps to bridge the gap between the normal gradient and the unit natural gradient, thereby speeding up learning. We believe that this property is also the reason for the success of activation functions like LReLUs and PReLUs. In contrast to these methods, ELUs have a clear saturation plateau in its negative regime, allowing them to learn a more robust and stable representation. Experimental results show that ELUs outperform other activation functions on many vision datasets, achieving one of the top 10 best reported results on CIFAR-10 and setting a new state of the art in CIFAR-100 without the need for multi-view test evaluation or model averaging. Furthermore, ELU networks produced competitive results on the ImageNet in much fewer epochs than a corresponding ReLU network.\nIn our experience ELUs are most effective once the number of layers in a network is larger than 4. For such networks, ELUs consistently outperform ReLUs and its variants with negative slopes. On ImageNet we observed that ELUs are able to converge to a state of the art solution in much less time it takes comparable ReLU networks. Given their outstanding performance, we expect ELU networks to become a real time saver in convolutional networks, which are notably time-intensive to train from scratch otherwise.\nAcknowledgment. We thank the NVIDIA Corporation for supporting this research with several Titan X GPUs and Roland Vollgraf and Martin Heusel for helpful discussions and comments on this work."
    }, {
      "heading" : "B QUADRATIC FORM OF MEAN AND INVERSE SECOND MOMENT",
      "text" : "Lemma 2. For a random variable a holds\nE(a)TE−1(a aT ) E(a) ≤ 1 (38)\nand ( 1 − E(a)TE−1(a aT ) E(a) )−1 = 1 + E(a)TVar−1(a) E(a) . (39)\nProof. The Sherman-Morrison Theorem states( A + b cT )−1 = A−1 − A −1 b cT A−1\n1 + cTA−1b . (40)\nTherefore we have\ncT ( A + b cT )−1 b = cTA−1b − c TA−1 b cT A−1b\n1 + cTA−1b (41)\n= cTA−1b\n( 1 + cTA−1b ) − ( cTA−1 b )2 1 + cTA−1b\n= cTA−1b\n1 + cTA−1b .\nUsing the identity\nE(a aT ) = Var(a) + E(a) E(a)T (42)\nfor the second moment and previous formula, we get E(a)TE−1(a aT ) E(a) = E(a)T ( Var(a) + E(a) E(a)T )−1 E(a) (43)\n= E(a)TVar−1(a) E(a)\n1 + E(a)TVar−1(a) E(a) ≤ 1 .\nThe last inequality follows from the fact that Var(a) is positive definite.\nWe obtain further ( 1 − E(a)TE−1(a aT ) E(a) )−1 = 1 + E(a)TVar−1(a) E(a) . (44)"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "We introduce the “exponential linear unit” (ELU) which speeds up learning in<lb>deep neural networks and leads to higher classification accuracies. Like rectified<lb>linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PRe-<lb>LUs), ELUs also avoid a vanishing gradient via the identity for positive values.<lb>However ELUs have improved learning characteristics compared to the units with<lb>other activation functions. In contrast to ReLUs, ELUs have negative values which<lb>allows them to push mean unit activations closer to zero. Zero means speed up<lb>learning because they bring the gradient closer to the unit natural gradient. We<lb>show that the unit natural gradient differs from the normal gradient by a bias shift<lb>term, which is proportional to the mean activation of incoming units. Like batch<lb>normalization, ELUs push the mean towards zero, but with a significantly smaller<lb>computational footprint. While other activation functions like LReLUs and PRe-<lb>LUs also have negative values, they do not ensure a noise-robust deactivation state.<lb>ELUs saturate to a negative value with smaller inputs and thereby decrease the<lb>propagated variation and information. Therefore ELUs code the degree of pres-<lb>ence of particular phenomena in the input, while they do not quantitatively model<lb>the degree of their absence. Consequently, dependencies between ELUs are much<lb>easier to model and distinct concepts are less likely to interfere.<lb>We found that ELUs lead not only to faster learning, but also to better general-<lb>ization performance once networks have many layers (≥ 5). ELU networks were<lb>among top 10 reported CIFAR-10 results and yielded the best published result on<lb>CIFAR-100, without resorting to multi-view evaluation or model averaging. On<lb>ImageNet, ELU networks considerably speed up learning compared to a ReLU<lb>network with the same architecture, obtaining less than 10% classification error<lb>for a single crop, single model network.",
    "creator" : "LaTeX with hyperref package"
  }
}