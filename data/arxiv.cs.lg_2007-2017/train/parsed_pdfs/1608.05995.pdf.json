{
  "name" : "1608.05995.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing",
    "authors" : [ "Ming Lin", "Jieping Ye" ],
    "emails" : [ "linmin@umich.edu", "jpye@umich.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 8.\n05 99\n5v 5\n[ st"
    }, {
      "heading" : "1 Introduction",
      "text" : "Linear models are one of the foundations of modern machine learning due to their strong learning guarantees and efficient solvers [Koltchinskii, 2011]. Conventionally linear models only consider the first order information of the input feature which limits their capacity in non-linear problems. Among various efforts extending linear models to the non-linear domain, the Factorization Machine [Rendle, 2010] (FM) captures the second order information by modeling the pairwise feature interaction in regression under low-rank constraints. FMs have\n∗linmin@umich.edu †jpye@umich.edu\nbeen found successful in many applications, such as recommendation systems [Rendle et al., 2011] and text retrieval [Hong et al., 2013]. In this paper, we consider a generalized version of FM called gFM which removes several redundant constraints in the original FM such as positive semi-definite and zero-diagonal, leading to a more general model without sacrificing its learning ability. From theoretical side, the gFM includes rank-one matrix sensing [Zhong et al., 2015, Chen et al., 2015, Cai and Zhang, 2015, Kueng et al., 2014] as a special case, where the latter one has been studied widely in context such as inductive matrix completion [Jain and Dhillon, 2013] and phase retrieval [Candes et al., 2011].\nDespite of the popularity of FMs in industry, there is rare theoretical study of learning guarantees for FMs. One of the main challenges in developing a provable FM algorithm is to handle its symmetric rank-one matrix sensing operator. For conventional matrix sensing problems where the matrix sensing operator is RIP, there are several alternating methods with provable guarantees [Hardt, 2013, Jain et al., 2013, Hardt and Wootters, 2014, Zhao et al., 2015a,b]. However, for a symmetric rank-one matrix sensing operator, the RIP condition doesn’t hold trivially which turns out to be the main difficulty in designing efficient provable FM solvers.\nIn rank-one matrix sensing, when the sensing operator is asymmetric, the problem is also known as inductive matrix completion which can be solved via alternating minimization with a global linear convergence rate [Jain and Dhillon, 2013, Zhong et al., 2015]. For symmetric rank-one matrix sensing operators, we are not aware of any efficient solver by the time of writing this paper. In a special case when the target matrix is of rank one, the problem is called “phase retrieval” whose convex solver is first proposed by Candes et al. [2011] then alternating methods are provided in [Lee et al., 2013, Netrapalli et al., 2013]. While the target matrix is of rank k > 1 , only convex methods minimizing the trace norm have been proposed recently, which are computationally expensive [Kueng et al., 2014, Cai and Zhang, 2015, Chen et al., 2015, Davenport and Romberg, 2016].\nDespite of the above fundamental challenges, extending rank-one matrix sensing algorithm to gFM itself is difficult. Please refer to Section 2.1 for an indepth discussion. The main difficulty is due to the first order term in the gFM formulation, which cannot be trivially converted to a standard matrix sensing problem.\nIn this paper, we develop a unified theoretical framework and an efficient solver for generalized Factorization Machine and its special cases such as rankone matrix sensing, either symmetric or asymmetric. The key ingredient is to show that the sensing operator in gFM satisfies a so-called Conditionally Independent RIP condition (CI-RIP, see Definition 2) . Then we can construct an estimation sequence via noisy power iteration [Hardt and Price, 2013]. Unlike previous approaches, our method does not require alternating minimization or choosing the step-size as in alternating gradient descent. The proposed method works on steaming data, converges linearly and has O(kd) space complexity for a d-dimension rank-k gFM model. The solver achieves O(ǫ) recovery error after retrieving O(k3d log(1/ǫ)) training instances.\nThe remainder of this paper is organized as following. In Section 2, we introduce necessary notation and background of gFM. Subsection 2.1 investigates several fundamental challenges in depth. Section 3 presents our algorithm, called One-Pass gFM, followed by its theoretical guarantees. Our analysis framework is presented in Section 4. Section 5 concludes this paper."
    }, {
      "heading" : "2 Generalized Factorization Machine (gFM)",
      "text" : "In this section, we first introduce necessary notation and background of FM and its generalized version gFM. Then in Subsection 2.1, we reveal the connection between gFM and rank-one matrix sensing followed by several fundamental challenges encountered when applying frameworks of rank-one matrix sensing to gFM.\nThe FM predicts the labels of instances by not only their features but also high order interactions between features. In the following, we focus on the second order FM due to its popularity. Suppose we are given N training instances xi ∈ Rd independently and identically (I.I.D.) sampled from the standard Gaussian distribution and so are their associated labels yi ∈ R. Denote the feature matrix X = [x1,x2, · · · ,xn] ∈ Rd×n and the label vector y = [y1, y2, · · · , yn]⊤ ∈ Rn . In second order FM, yi is assumed to be generated from a target vector w∗ ∈ Rd and a target rank k matrix M∗ ∈ Rd×d satisfying\nyi =xi ⊤w∗ + xi ⊤M∗xi + ξi (1)\nwhere ξi is a random subgaussian noise with proxy variance ξ 2 . It is often more convenient to write Eq. (1) in matrix form. Denote the linear operator A : Rd×d → Rn as A(M) , [〈A1,M〉 , 〈A2,M〉 , · · · , 〈An,M〉]⊤ where Ai = xixi ⊤ . Then Eq. (1) has a compact form:\ny = X⊤w∗+A(M∗) + ξ . (2)\nThe FM model given by Eq. (2) consists of two components: the first order component X⊤w∗ and the second order component A(M∗). The component A(M∗) is a symmetric rank-one Gaussian measurement since Ai(M) = xi⊤Mxi where the left/right design vectors (xi and xi\n⊤) are identical. The original FM requires that M∗ should be positive semi-definite and the diagonal elements of M∗ should be zero. However our analysis shows that both constraints are redundant for learning Eq. 2. Therefore in this paper we consider a generalized version of FM which we call gFM where M∗ is only required to be symmetric and low rank. To make the recovery of M∗ well defined, it is necessary to assume M∗ to be symmetric. Indeed for any asymmetric matrix M∗, there is always a symmetric matrix M∗sym = (M\n∗+M∗⊤)/2 such that A(M∗) = A(M∗sym) thus the symmetric constraint does not affect the model. Another standard assumption in rank-one matrix sensing is that the rank of M∗ should be no more than k for k ≪ d. When w∗ = 0, gFM is equal to the symmetric rank-one matrix sensing problem. Recent researches have proposed several convex programming methods based on the trace norm minimization to recover M∗ with\na sampling complexity on order of O(k3d) [Candes et al., 2011, Cai and Zhang, 2015, Kueng et al., 2014, Chen et al., 2015, Zhong et al., 2015]. Some authors also call gFM as second order polynomial network [Blondel et al., 2016].\nWhen d is much larger than k, the convex programming on the trace norm or nuclear norm of M∗ becomes difficult since M∗ can be a d× d dense matrix. Although modern convex solvers can scale to large d with reasonable computational cost, a more popular strategy to efficiently estimate w∗ and M∗ is to decompose M∗ as UV ⊤ for some U, V ∈ Rd×k, then alternatively update w, U, V to minimize the empirical loss function\nmin w,U,V\n1\n2N ‖y −X⊤w −A(UV ⊤)‖22 . (3)\nThe loss function in Eq. (3) is non-convex. It is even unclear whether an estimator of the optimal solution {w∗,M∗} of Eq. (3) with a polynomial time complexity exists or not.\nIn our analysis, we denote M +O(ǫ) as a matrix M plus a perturbation matrix whose spectral norm is bounded by ǫ. We use ‖ · ‖2 , ‖ · ‖F , ‖ · ‖∗ to denote the matrix spectral norm, Frobenius norm and nuclear norm respectively. To abbreviate the high probability bound, we denote C = polylog(d, n,T, 1/η) to be a constant polynomial logarithmic in {d, n, T, 1/η}. The eigenvalue decomposition of M∗ is M∗ = U∗Λ∗U∗⊤ where U∗ ∈ Rd×k is the top-k eigenvectors of M∗ and Λ∗ = diag(λ∗1, λ ∗ 2, · · · , λ∗k) are the corresponding eigenvalues sorted by |λi| ≥ |λi+1|. Let σ∗i = |λ∗i | denote the singular value of M∗ and σi{M} be the i-th largest singular value of M . U∗⊥ denotes an matrix whose columns are the orthogonal basis of the complementary subspace of U∗."
    }, {
      "heading" : "2.1 gFM and Rank-One Matrix Sensing",
      "text" : "Whenw∗ = 0 in Eq. (1), the gFM becomes the symmetric rank-one matrix sensing problem. While the recovery ability of rank-one matrix sensing is somehow provable recently despite of the computational issue, it is not the case for gFM. It is therefore important to discuss the differences between gFM and rank-one matrix sensing to give us a better understanding of the fundamental barriers in developing provable gFM algorithm.\nIn the rank-one matrix sensing problem, a relaxed setting is to assume that the sensing operator is asymmetric, which is defined by Aasyi (M) = ui⊤Mvi where ui and vi are independent random vectors. Under this setting, the recovery ability of alternating methods is provable [Jain and Dhillon, 2013]. However, existing analyses cannot be generalized to their symmetric counterpart, since ui and vi are not allowed to be dependent in these frameworks. For example, the sensing operator Aasy(·) is unbiased ( EAasy(·) = 0) but the symmetric sensing operator is clearly not [Cai and Zhang, 2015]. Therefore, the asymmetric setting oversimplifies the problem and loses important structure information which is critical to gFM.\nAs for the symmetric rank-one matrix sensing operator, the state-of-theart estimator is based on the trace norm convex optimization [Tropp, 2014,\nChen et al., 2015, Cai and Zhang, 2015], which is computationally expensive. When w∗ 6= 0, the gFM has an extra perturbation term X⊤w∗ . This first order perturbation term turns out to be a fundamental challenge in theoretical analysis. One might attempt to merge w∗ into M∗ in order to convert gFM as a rank (k + 1) matrix sensing problem. For example, one may extend the feature x̂i , [xi, 1]\n⊤ and the matrix M̂∗ = [M∗;w∗⊤] ∈ R(d+1)×d. However, after this simple extension, the sensing operator becomes Â(M∗) = x̂i⊤M̂∗xi. It is no longer symmetric. The left/right design vector is neither independent nor identical. Especially, not all dimensions of x̂i are random variables. According to the above discussion, the conditions to guarantee the success of rank-one matrix sensing do not hold after feature extension and all the mentioned analyses cannot be directly applied."
    }, {
      "heading" : "3 One-Pass gFM",
      "text" : "In this section, we present the proposed algorithm, called One-Pass gFM followed by its theoretical guarantees. We will focus on the intuition of our algorithm. A rigorous theoretical analysis is presented in the next section.\nThe One-Pass gFM is a mini-batch algorithm. In each mini-batch, it processes n training instances and then alternatively updates parameters. The iteration will continue until T mini-batch updates. Since gFM deals with a nonconvex learning problem, the conventional gradient descent framework hardly works to show the global convergence. Instead, our method is based on a construction of an estimation sequence. Intuitively, when w∗ = 0, we will show in the next section that 1nA′A(M) ≈ 2M + tr(M)I and tr(M) ≈ 1n1⊤A(M). Since y ≈ A(M∗), we can estimate M∗ via 12nA′(y)− 1n1⊤yI. But this simple construction cannot generate a convergent estimation sequence since the perturbation terms in the above approximate equalities cannot be reduced along iterations. To overcome this problem, we replace A(M∗) with A(M∗−M (t)) in our construction. Then the perturbation terms will be on order of O(‖M∗−M (t)‖2). When w∗ 6= 0, we can apply a similar trick to construct its estimation sequence via the second and the third order moments of X . Algorithm 1 gives a step-bystep description of our algorithm1.\nIn Algorithm 1, we only need to store w(t) ∈ Rd, U (t), V (t) ∈ Rd×k. Therefore the space complexity isO(d+kd). The auxiliary variablesM (t), H\n(t) 1 , h (t) 2 ,h (t) 3\ncan be implicitly presented by w(t), U (t), V (t). In each mini-batch updating, we only need matrix-vector product operations which can be efficiently implemented on many computation architectures. We use truncated SVD to initialize gFM, a standard initialization step in matrix sensing. We do not require this step to be computed exactly but up to an accuracy of O(δ) where δ is the RIP constant. The QR step on line 6 requires O(k2d) operations. Compared with SVD which requires O(kd2) operations, the QR step is much more efficient when d ≫ k. Algorithm 1 retrieves instances streamingly, a favorable behavior\n1Implementation is available from https://minglin-home.github.io/\nAlgorithm 1 One-Pass gFM\nRequire: The mini-batch size n, number of total mini-batch update T , training instances X = [x1,x2, · · ·xnT }, y = [y1, y2, · · · , ynT ]⊤, desired rank k ≥ 1. Ensure: w(T ), U (T ), V (T ).\n1: Define M (t) , (U (t)V (t)⊤ + V (t)U (t)⊤)/2 , H(t)1 , 1 2nA′(y − A(M (t)) −\nX(t)⊤w(t)) , h(t)2 , 1 n1 ⊤(y − A(M (t)) − X(t)⊤w(t)) , h(t)3 , 1nX(t)(y − A(M (t))−X(t)⊤w(t)) .\n2: Initialize: w(0) = 0, V (0) = 0. U (0) = SVD(H (0) 1 − 12h (0) 2 I, k), that is, the\ntop-k left singular vectors. 3: for t = 1, 2, · · · , T do 4: Retrieve n training instances X(t) = [x(t−1)n+1, · · · ,x(t−1)n+n] . Define\nA(M) , [X(t)i ⊤MX (t) i ] n i=1.\n5: Û (t) = (H (t−1) 1 − 12h (t−1) 2 I +M (t−1)⊤)U (t−1) .\n6: Orthogonalize Û (t) via QR decomposition: U (t) = QR ( Û (t) ) .\n7: w(t) = h (t−1) 3 +w (t−1) . 8: V (t) = (H (t−1) 1 − 12h (t−1) 2 I +M (t−1))U (t) 9: end for\n10: Output: w(T ), U (T ), V (T ) .\non systems with high speed cache. Finally, we export w(T ), U (T ), V (T ) as our estimation of w∗ ≈ w(T ) and M∗ ≈ U (T )V (T )⊤.\nOur main theoretical result is presented in the following theorem, which gives the convergence rate of recovery and sampling complexity of gFM when M∗ is low rank and the noise ξ = 0.\nTheorem 1. Suppose xi’s are independently sampled from the standard Gaussian distribution. M∗ is a rank k matrix. The noise ξ = 0. Then with a probability at least 1 − η, there exists a constant C and a constant δ < 1 such that\n‖w∗ −w(t)‖2 + ‖M∗ −M (t)‖2 ≤δt(‖w∗‖2 + ‖M∗‖2)\nprovided n ≥ C(4 √ 5σ∗1/σ ∗ k + 3) 2k3d/δ2, δ ≤ (4 √ 5σ∗1/σ ∗ k +3)σ∗ k 4 √ 5σ∗\n1 +3σ∗ k +4\n√ 5‖w∗‖2\n2\n.\nTheorem 1 shows that {w(t),M (t)} will converge to {w∗,M∗} linearly. The convergence rate is controlled by δ, whose value is on order of O(1/ √ n). A small δ will result in a fast convergence rate but a large sampling complexity. To reduce the sampling complexity, a large δ is preferred. The largest allowed δ is bounded by O(1/(‖M∗‖2 + ‖w∗‖2)). The sampling complexity is O((σ∗1/σ ∗ k) 2k3d). If M∗ is not well conditioned, it is possible to remove (σ∗1/σ ∗ k) 2 in the sampling complexity by a procedure called “soft-deflation” [Jain et al., 2013, Hardt and Wootters, 2014]. By theorem 1, gFM achieves ǫ recovery error after retrieving nT = O(k3d log ((‖w∗‖2 + ‖M∗‖2)/ǫ)) instances.\nThe noisy case where M∗ is not exactly low rank and ξ > 0 is more intricate therefore we postpone it to Subsection 4.1. The main conclusion is similar to the noise-free case Theorem 1 under a small noise assumption."
    }, {
      "heading" : "4 Theoretical Analysis",
      "text" : "In this section, we give the sketch of our proof of Theorem 1. Omitted details are postponed to appendix.\nFrom high level, our proof constructs an estimation sequence {w̃(t), M̃ (t), ǫt} such that ǫt → 0 and ‖w∗ − w̃(t)‖2 + ‖M∗ − M̃ (t)‖2 ≤ ǫt . In conventional matrix sensing, this construction is possible when the sensing matrix satisfies the Restricted Isometric Property (RIP) [Candès and Recht, 2009]:\nDefinition 2 (ℓ2-norm RIP). A sensing operator A is ℓ2-norm δk-RIP if for any rank k matrix M ,\n(1 − δk)‖M‖F ≤ 1\nn ‖A(M)‖22 ≤ (1 + δk)‖M‖F .\nWhen A is ℓ2-norm δk-RIP for any rank k matrix M , A′A is nearly isometric [Jain et al., 2012], which implies ‖M−A′A(M)/n‖2 ≤ δ. Then we can construct our estimation sequence as following:\nM̃ (t) = 1 n A′A(M∗ − M̃ (t−1)) + M̃ (t−1) , w̃(t) = (I − 1 n XX⊤)(w∗ − w̃(t−1)) + w̃(t−1) .\nHowever, in gFM and symmetric rank-one matrix sensing, the ℓ2-norm RIP condition cannot be satisfied with high probability [Cai and Zhang, 2015]. To establish an RIP-like condition for rank-one matrix sensing, several variants have been proposed, such as the ℓ2/ℓ1-RIP condition [Cai and Zhang, 2015, Chen et al., 2015]. The essential idea of these variants is to replace the ℓ2norm ‖A(M)‖2 with ℓ1-norm ‖A(M)‖1 then a similar norm inequality can be established for all low rank matrix again. However, even using these ℓ1-norm RIP variants, we are still unable to design an efficient alternating algorithm. All these ℓ1-norm RIP variants have to deal with trace norm programming problems. In fact, it is impossible to construct an estimation sequence based on ℓ1-norm RIP because we require ℓ2-norm bound on A′A during the construction.\nA key ingredient of our framework is to propose a novel ℓ2-norm RIP condition to overcome the above difficulty. The main technique reason for the failure of conventional ℓ2-norm RIP is that it tries to bound A′A(M) over all rank k matrices. This is too aggressive to be successful in rank-one matrix sensing. Regarding to our estimation sequence, what we really need is to make the RIP hold for current low rank matrix M (t). Once we update our estimation M (t+1), we can regenerate a new sensing operator independent of M (t) to avoid bounding A′A over all rank k matrices. To this end, we propose the Conditionally Independent RIP (CI-RIP) condition.\nDefinition 3 (CI-RIP). A matrix sensing operator A is Conditionally Independent RIP with constant δk, if for a fixed rank k matrix M , A is sampled independently regarding to M and satisfies\n‖(I − 1 n A′A)M‖22 ≤ δk . (4)\nAn ℓ2-norm or ℓ1-norm RIP sensing operator is naturally CI-RIP but the reverse is not true. In CI-RIP,A is no longer a fixed but random sensing operator independent of M . In one-pass algorithm, this is achievable if we always retrieve new instances to construct A in one mini-batch updating. Usually Eq. (4) doesn’t hold in a batch method since M (t+1) depends on A(M (t)).\nAn asymmetric rank-one matrix sensing operator is clearly CI-RIP due to the independency between left/right design vectors. But a symmetric rank-one matrix sensing operator is not CI-RIP. In fact it is a biased estimator since E(x⊤Mx) = tr(M) . To this end, we propose a shifted version of CI-RIP for symmetric rank-one matrix sensing operator in the following theorem. This theorem is the key tool in our analysis.\nTheorem 4 (Shifted CI-RIP). Suppose xi are independent standard random Gaussian vectors, M is a fixed symmetric rank k matrix independent of xi and w is a fixed vector. Then with a probability at least 1−η, provided n ≥ Ck3d/δ2 ,\n‖ 1 2n A′A(M)− 1 2 tr(M)I −M‖2 ≤ δ‖M‖2 .\nTheorem 4 shows that 12nA′A(M) is nearly isometric after shifting by its expectation 12 tr(M)I. The RIP constant δ = O( √ k3d/n) . In gFM, we choose M = M∗ −M (t) therefore M is of rank 3k . Under the same settings of Theorem 4, suppose that d ≥ C then the following lemmas hold true with a probability at least 1− η for fixed w and M . Lemma 5. | 1n1⊤A(M))− tr(M)| ≤ δ‖M‖2 provided n ≥ Ck/δ2 . Lemma 6. | 1n1⊤X⊤w| ≤ ‖w‖2δ provided n ≥ C/δ2 . Lemma 7. ‖ 1nA′(X⊤w)‖2 ≤ ‖w‖2δ provided n ≥ Cd/δ2 . Lemma 8. ‖ 1nX⊤A(M)‖2 ≤ ‖M‖2δ provided n ≥ Ck2d/δ2 . Lemma 9. ‖I − 1nXX⊤‖2 ≤ δ provided n ≥ Cd/δ2 .\nEquipping with the above lemmas, we construct our estimation sequence as following.\nLemma 10. Let M (t), H (t) 1 , h (t) 2 ,h (t) 3 be defined as in Algorithm 1. Define ǫt = ‖w∗ −w(t)‖2 + ‖M∗ −M (t)‖2 . Then with a probability at least 1− η, provided n ≥ Ck3d/δ2 ,\nH (t) 1 =M ∗ −M (t) + tr(M∗ −M (t))I +O(δǫt) , h(t)2 = tr(M∗ −M (t)) +O(δǫt) h (t) 3 =w ∗ −w(t) +O(δǫt) .\nSuppose by construction, ǫt → 0 when t → ∞. Then H(t)1 − h (t) 2 I +M (t) → M∗ and h(t)3 +w\n(t) → w∗ and then the proof of Theorem 1 is completed. In the following we only need to show that Lemma 10 constructs an estimation sequence with ǫt = O(δ\nt) → 0. To this end, we need a few things from matrix perturbation theory.\nBy Theorem 1, U (t) will converge to U∗ up to column order perturbation. We use the largest canonical angle to measure the subspace distance spanned by U (t) and U∗, which is denoted as θt = θ(U (t), U∗). For any matrix U , it is well known [Zhu and Knyazev, 2013] that\nsin θ(U,U∗) = ‖U∗⊥⊤U‖2, cos θ(U,U∗) = σk{U∗⊤U}, tan θ(U,U∗) = ‖U∗⊥⊤U(U∗⊤U)−1‖2 .\nThe last tangent equality allows us to bound the canonical angle after QR decomposition. Suppose U (t)R = Û (t) in the QR step of Algorithm 1, we have\ntan θ(Û (t), U∗) = ‖U∗⊥⊤Û (t)(U∗⊤Û (t))−1‖2 = ‖U∗⊥⊤U (t)R(U∗⊤U (t)R)−1‖2 = ‖U∗⊥⊤U (t)(U∗⊤U (t))−1‖2 = tan θ(U (t), U∗) .\nTherefore, it is more convenient to measure the subspace distance by tangent function.\nTo show ǫt → 0, we recursively define the following variables:\nαt , tan θt, βt , ‖w∗ −w(t)‖2, γt , ‖M∗ −M (t)‖2, ǫt , βt + γt .\nThe following lemma derives the recursive inequalities regarding to {αt, βt, γt} .\nLemma 11. Under the same settings of Theorem 1, suppose αt ≤ 2, δǫt ≤ 4 √ 5σ∗k, then αt+1 ≤ 4 √ 5δσ∗−1k (βt + γt), βt+1 ≤ δ(βt + γt), γt+1 ≤ αt+1‖M∗‖2 + 2δ(βt + γt) .\nIn Lemma 11, when we choose n such that δ = O(1/ √ n) is small enough,\n{αt, βt, γt} will converge to zero. The only question is the initial value {α0, β0, γ0}. According to the initialization step of gFM, β0 ≤ ‖w∗‖2 and γ0 ≤ ‖M∗‖2 . To bound α0 , we need the following lemma which directly follows Wely’s and Wedin’s theorems [Stewart and Sun, 1990].\nLemma 12. Denote U and Ũ as the top-k left singular vectors of M and M̃ = M + O(ǫ) respectively. The i-th singular value of M is σi. Suppose that ǫ ≤ σk−σk+14 . Then the largest canonical angle between U and Ũ , denoted as θ(U, Ũ), is bounded by sin θ(U, Ũ) ≤ 2ǫ/(σk − σk+1) .\nAccording to Lemma 12, when 2δ(‖w∗‖2+‖M∗‖2) ≤ σ∗k/4, we have sin θ0 ≤ 4δ(‖w∗‖2+‖M∗‖2)/σ∗k. Therefore, α0 ≤ 2 provided δ ≤ σ∗k/[8(‖w∗‖2+‖M∗‖2)] .\nProof of Theorem 1. Suppose that at step t, αt ≤ 2, δǫt ≤ 4 √ 5σ∗k, from Lemma 11, βt+1 + γt+1 ≤βt+1 + αt+1‖M∗‖2 + 2δ(βt + γt) ≤ δǫt + 4 √ 5δσ∗−1k ǫt‖M∗‖2 + 2δǫt\n=(4 √ 5σ∗1/σ ∗ k + 3)δǫt .\nTherefore,\nǫt = βt + γt ≤ [(4 √ 5σ∗1/σ ∗ k + 3)δ] t(β0 + γ0)\nαt+1 ≤ 4 √ 5δσ∗−1k (βt + γt) ≤ 4 √ 5δσ∗−1k [(4 √ 5σ∗1/σ ∗ k + 3)δ] t(β0 + γ0) .\nClearly we need (4 √ 5σ∗1/σ ∗ k+3)δ < 1 to ensure convergence, which is guaranteed by δ < σ∗ k\n4 √ 5σ∗\n1 +3σ∗ k\n. To ensure the recursive inequality holds for any t, we require\nαt+1 ≤ 2, which is guaranteed by\n4 √ 5(β0 + γ0)δ/σ ∗ k ≤ 2 ⇔ δ ≤ σ∗k 2 √ 5(σ∗1 + β0) .\nTo ensure the condition δǫt ≤ 4 √ 5σ∗k,\nδ ≤ 4 √ 5σ∗k/ǫ0 = 4 √ 5σ∗k/(σ ∗ 1 + β0) ⇒ δ ≤ 4 √ 5σ∗k/ǫt .\nIn summary, when\nδ ≤ min {\nσ∗k 4 √ 5(σ∗1 + β0) , σ∗k 4 √ 5σ∗1 + 3σ ∗ k , σ∗k 2 √ 5(σ∗1 + β0) , σ∗k 8(σ∗1 + β0)\n}\n⇐δ ≤ σ ∗ k\n4 √ 5σ∗1 + 3σ ∗ k + 4 √ 5β0 .\nwe have ǫt = [(4 √ 5σ∗1/σ ∗ k + 3)δ] t(σ∗1 + γ0) . To simplify the result, replace δ with δ1 = (4 √ 5σ∗1/σ ∗ k + 3)δ. The proof is completed."
    }, {
      "heading" : "4.1 Noisy Case",
      "text" : "In this subsection, we analyze the performance of gFM under noisy setting. Suppose that M∗ is no longer low rank, M∗ = U∗Λ∗U∗⊤ + U∗⊥Λ ∗ ⊥U ∗ ⊥ ⊤ where Λ∗⊥ = diag(λk+1, · · · , λd) is the residual spectrum. Denote M∗k = U∗Λ∗U∗⊤ to be the best rank k approximation of M∗ and M∗⊥ = M\n∗ − M∗k . The additive noise ξi’s are independently sampled from subgaussian with proxy variance ξ.\nFirst we generalize the above theorems and lemmas to noisy case.\nLemma 13. Suppose that in Eq. (1) xi’s are independent standard random Gaussian vectors. M is a fixed rank k matrix. M∗⊥ 6= 0 and ξ > 0. Then provided n ≥ Ck3d/δ2, with a probability at least 1− η,\n‖ 1 2n A′A(M∗ −M)− 1 2 tr(M∗k −M)I − (M∗k −M)‖2 ≤ δ‖M∗k −M‖2 + Cσ∗k+1d2/\n√ n\n(5)\n| 1 n 1⊤A(M∗ −M)− tr(M∗k −M)| ≤ δ‖M∗k −M‖2 + Cσ∗k+1d2/\n√ n (6)\n‖ 1 n X⊤A(M∗ −M)‖2 ≤ δ‖M∗k −M‖2 + Cσ∗k+1d2/\n√ n (7)\n‖ 1 n A′(X⊤w)‖2 ≤ δ‖w‖2, ‖ 1 n 1⊤X⊤w‖2 ≤ δ‖w‖2 . (8)\nDefine γt = ‖M∗k−M (t)‖2 similar to the noise-free case. According to Lemma 13, when ξ = 0, for n ≥ Ck3d/δ2,\nH (t) 1 =M ∗ k −M (t) +\n1 2 tr(M∗k −M (t))I +O(δǫt + Cσ∗k+1d2/\n√ n)\nh (t) 2 =tr(M ∗ −M (t)) +O(δǫt + Cσ∗k+1d2/ √ n) h (t) 3 =w ∗ −w(t) +O(δǫt + Cσ∗k+1d2/ √ n) .\nDefine r = Cσ∗k+1d 2/ √ n. If ξ > 0, it is easy to check that the perturbation\nbecomes r̂ = r + O(ξ/ √ n) . Therefore we uniformly use r to present the perturbation term. The recursive inequalities regarding to the recovery error is constructed in Lemma 14.\nLemma 14. Under the same settings of Lemma 13, define ρ , 2σ∗k+1/(σ ∗ k + σ∗k+1). Suppose that at any step i, 0 ≤ i ≤ t , αi ≤ 2 . When provided 4 √ 5(δǫt + r) ≤ σ∗k − σ∗k+1, αt+1 ≤ραt + 4 √ 5\nσ∗k + σ ∗ k+1\nδǫt + 4 √ 5\nσ∗k + σ ∗ k+1\nr , βt+1 ≤ δǫt + r , γt+1 ≤ αt+1‖M∗‖2 + 2δǫt + 2r .\nThe solution to the recursive inequalities in Lemma 14 is non-trivial. Comparing to the inequalities in Lemma 11, αt+1 is bounded by αt in noisy case. Therefore, if we simply follow Lemma 11 to construct recursive inequality about ǫt , we will quickly be overloaded by recursive expansion terms. The key construction of our solution is to bound the term αt + 8 √ 5/(σ∗k + σ ∗ k+1)δǫt . The solution is given in the following theorem.\nTheorem 15. Define constants\nc =4 √ 5/(σ∗k + σ ∗ k+1) , b = 3+ 4 √ 5σ∗1/(σ ∗ k + σ ∗ k+1) , q = (1 + ρ)/2 ."
    }, {
      "heading" : "Then for any t ≥ 0,",
      "text" : "αt + 2cδǫt ≤qt ( 2− (1 + ρ)cr\n1− q\n) + (1 + ρ)cr\n1− q . (9)\nprovided\nδ ≤ min{ 1− ρ 4ρσ∗1c , ρ 2b } , (2 + c(σ∗k − σ∗k+1))δǫ0 + r ≤ (σ∗k − σ∗k+1) (10)\n4 √ 5 ( 4 + 2c(σ∗k − σ∗k+1) ) δǫ0 + 4 √ 5 ( 4 + (σ∗k − σ∗k+1) ) r ≤ (σ∗k − σ∗k+1)2 .\nTheorem 15 gives the convergence rate of gFM under noisy settings. We bound αt + 2cδǫt as the index of recovery error, whose convergence rate is linear. The convergence rate is controlled by q, a constant depends on the eigen gap σ∗k+1/σ ∗ k . The final recovery error is bounded by O(r/(1 − q)) . Eq. (10) is the small noise condition to ensure the noisy recovery is possible. Generally speaking, learning a d×d matrix with O(d) samples is an ill-conditioned problem when the target matrix is full rank. The small noise condition given by Eq. (10) essentially says that M∗ can be slightly deviated from low rank manifold and the noise shouldn’t be too large to blur the spectrum ofM∗. When the noise is large, Eq. (10) will be satisfied with n = O(d2) which is the information-theoretical lower bound for recovering a full rank matrix."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose a provable efficient algorithm to solve generalized Factorization Machine (gFM) and rank-one matrix sensing. Our method is based on an one-pass alternating updating framework. The proposed algorithm is able to learn gFM within O(kd) memory on steaming data, has linear convergence rate and only requires matrix-vector product implementation. The algorithm takes no more than O(k3d log (1/ǫ)) instances to achieve O(ǫ) recovery error."
    }, {
      "heading" : "A Preliminary",
      "text" : "In this section, we present several important theorems and lemmas in our analysis. The following concentration inequalities are well known.\nLemma 16. Let xi be zero-mean sub-Gaussian distribution with variance proxy σ 2. Denote Sn = ∑n i=1 aixi for a fixed sequence {ai}. Then\nPr(|Sn| > t) ≤ 2 exp(− t 2\n2σ2( ∑n i=1 a 2 i ) ) .\nThat is, with a probability at least 1− η,\n|Sn| ≤ σ\n√ √ √ √ n ∑\ni=1\na2i √ 2 log(2/η) .\nCorollary 17. Let xi ∼ N (0, 1) be a standard Gaussian distribution. Then with a probability at least 1− η,\nn ∑\ni=1\nai(x 2 i − 1) ≤2\n√ √ √ √ n ∑\ni=1\nai √ 2 log(2/η) .\nFor random matrix, we have matrix concentration inequalities [?].\nTheorem 18 (Matrix Bernstein’s Inequality [?]). Suppose {Si}ni=1 are set of independent random matrices of dimension d1 × d2,\n‖Si − ESi‖ ≤ L ."
    }, {
      "heading" : "Define",
      "text" : "Z = n ∑\ni=1\nSi, σ 2 =\n1 n max(E‖(Z − EZ)(Z − EZ)⊤‖2, E‖(Z − EZ)⊤(Z − EZ)‖2) .\nThe with a probability at least 1− δ, for any 0 < ǫ < 1, 1\nn ‖Z − EZ‖2 ≤ 9ǫ\n√\nlog((d1 + d2)/δ)\nprovided n ≥ max(σ2, L)/ǫ2 . And for any n,\n1 n ‖Z −EZ‖2 ≤4 3 L n log((d1 + d2)/δ) + 3\n√\n2 σ2\nn log((d1 + d2)/δ) .\nUsing matrix Bernstein’s inequality, we can bound the covariance estimator.\nCorollary 19 (Matrix Bernstein’s Inequality for Covariance Estimator [?]). Suppose xi ∈ Rd, i = 1, 2, · · · , n are independent random variables with zero mean. ‖xi‖2 ≤ B, A = E(xixi⊤) Then with a probability at least 1− δ,\n‖ 1 n\nn ∑\ni=1\nxix ⊤ i − A‖2 ≤ 9ǫ √ log(2d/δ)/n\nprovided\nn ≥ max(B‖A‖, B)/ǫ2 ."
    }, {
      "heading" : "B Proof of Lemmas",
      "text" : ""
    }, {
      "heading" : "B.1 Proof of Lemma 5",
      "text" : "Proof. Denote the eigenvalue decomposition of M as\nM = UΛU⊤ = Udiag(λ1, λ2, · · · , λk)U⊤\nSince Gaussian distribution is rotation invariant, x̂i = U ⊤ xi also follows standard Gaussian distribution.\nxi ⊤Mxi =xi ⊤UΛU⊤xi = |x̂i⊤Λx̂i| = k ∑\nj=1\nλjx̂ 2 i,j .\nIt is easy to see that E(xi ⊤Mxi) = ∑\nj λj = tr(M). Define\nai ,xi ⊤Mxi − tr(M) =\nd ∑\nj=1\nλj(x̂ 2 i,j − 1)\nAccording to Corollary 17, for a fixed i, with a probability at least 1− η,\n|ai| ≤ 2‖M‖F √ 2 log(2/η) .\nThen for any i, with a probability at least 1− η,\n|ai| ≤ 2‖M‖F √ 2 log(2n/η) .\nApply Corollary 17 again, with a probability at least 1− 2η,\n| 1 n\nn ∑\ni=1\nai − tr(M)| ≤2‖M‖F √ 2 log(2n/η) √ 2 log(2/η)/n\n≤2 √ k‖M‖2 √ 2 log(2n/η) √ 2 log(2/η)/n .\nDenote δ = 2 √ k √ 2 log(2n/η) √ 2 log(2/η)/n. Then when n ≥ Ck/δ2,\n| 1 n\nn ∑\ni=1\nai − tr(M)| ≤ ‖M‖2δ ."
    }, {
      "heading" : "B.2 Proof of Lemma 6",
      "text" : "Proof. Define random variable\nai =xi ⊤ w, Eai = 0\nEa2i ≤‖w‖22 Then according to Lemma 16, with a probability at least 1− η,\n| 1 n\nn ∑\ni=1\nai| ≤ ‖w‖2 √ 2 log(2/η)/n ."
    }, {
      "heading" : "B.3 Proof of Lemma 8",
      "text" : "Proof. Define random vector\nai = xixi ⊤Mxi, Eai = 0 .\nWith a probability at least (1− η1)(1− η2),\n‖ai‖2 =‖xixi⊤Mxi‖2 ≤ ‖xi⊤Mxi‖2‖xi‖2 ≤(|tr(M)|+ 2‖M‖F √ 2 log(2n/η1)) √ 2d log(2n/η2)\n,c1 √ 2d log(2n/η2) .\n‖Eai⊤ai‖2 =‖xi⊤Mxixi⊤xixi⊤Mxi‖2 ≤(xi⊤Mxi)2‖xi‖22 ≤2c21d log(2n/η2) .\nBy matrix Bernstein’s inequality, the concentration holds when\nn ≥ 1 ǫ2 max{c1 √ 2d log(2n/η2), 2c 2 1d log(2n/η2)}\n= 1\nǫ2 O(k2d‖M‖22) .\nTherefore, after taking the union bound, there exists some constant C2 = O(log(2n/η)),\n‖ 1 n\nn ∑\ni=1\nai‖2 ≤ ǫ\nn ≥ C2k2d‖M‖22 log(2(d+ 1)/η)/ǫ2 . Denote δ = ‖M‖2/ǫ. Then when n ≥ Ck2d/δ,\n‖ 1 n\nn ∑\ni=1\nai‖2 ≤ ‖M‖2δ .\nB.4 Proof of Lemma 7\n‖ 1 n A′(X⊤w)‖2 =‖ 1 n\nn ∑\ni=1\nxixi ⊤ wxi ⊤‖2 .\nE{xixi⊤wxi⊤} = 0\n‖xixi⊤wxi⊤‖2 ≤‖xi⊤w‖2‖xi‖22 ≤2‖w‖2 √ 2 log(2/η)(d+ 2 √ 2d log(2n/η))\n≤4‖w‖2 √ 2 log(2/η)d\nprovided d ≥ 8 log(2n/η). Now considering\n{Exixi⊤wxi⊤xiw⊤xixi⊤}pq =E{( ∑ xpxqwi1xi1wi2xi2x 2 i3)}\nWhen p 6= q,\nE{( ∑ xpxqwi1xi1wi2xi2x 2 i3)} = E{(2 ∑\ni3\nxpxqwpxpwqxqx 2 i3)}\n= E{(2 ∑\ni3\nx2px 2 qwpwqx 2 i3)}\n= 2wpwqE{( ∑\ni3\nx2px 2 qx 2 i3)}\n= 2wpwqd\nWhen p = q,\nE{( ∑ xpxqwi1xi1wi2xi2x 2 i3)} =E{( ∑ x2pwi1xi1wi2xi2x 2 i3)}\n=E{( ∑ x2pwpxpwpxpx 2 i3 + ∑ x2pwjxjwjxjx 2 i3 + ∑ x2pwi3xi3wi3xi3x 2 i3)}\n=E{( ∑ i36=p x4pw 2 px 2 i3 + ∑ j 6=i36=p x2pw 2 jx 2 jx 2 i3 + ∑ i36=p x2pw 2 i3x 4 i3)} =w2p(d− 1) + ∑\nj 6=p w2j (d− 1) +\n∑ i36=p w2i3\n=w2p(d− 1) + ∑ j 6=p w2jd = w 2 p(d− 1) +\nd ∑\nj=1\nw2jd− w2pd\n= d ∑\nj=1\nw2jd− w2p\nTherefore,\nExixi ⊤ wxi ⊤ xiw ⊤ xixi ⊤ =ddiag{‖w‖22} − diag{w ◦w}+ 2dww⊤\n‖Exixi⊤wxi⊤xiw⊤xixi⊤‖2 ≤ 4d‖w‖22 Using matrix Bernstein’s inequality,\n‖ 1 n\nn ∑\ni=1\nxixi ⊤ wxi ⊤‖2 ≤4 3\n4‖w‖2 √ 2 log(2/η)d\nn log(2d/η)\n+ 3\n√\n2 4d‖w‖22\nn log(2d/η)\n≤C‖w‖2 √ d\nn\nDenote δ = C √ d/n, when n ≥ Cd/δ2, d ≥ 8 log(2n/η),\n‖ 1 n\nn ∑\ni=1\nxixi ⊤ wxi ⊤‖2 ≤ ‖w‖2δ"
    }, {
      "heading" : "B.5 Proof of Lemma 9",
      "text" : "According to Corollary 19, when d ≥ 8 log(2n/η),\n‖xi‖2 ≤ 2d\nTherefore, with a probability at least 1− η,\n‖I − 1 n XX⊤‖2 ≤9ǫ √ log(2d/η)/n\nfor n ≥ 2d/ǫ2. Denote δ = 9ǫ √ log(2d/η)/n, then when n ≥ Cd/δ2,\n‖I − 1 n XX⊤‖2 ≤δ ."
    }, {
      "heading" : "B.6 Proof of Lemma 11",
      "text" : "To derive αt+1 ,\n‖U∗⊥⊤[M∗ +O(2δǫt)]U (t)‖2 ≤‖U∗⊥⊤M∗U (t)‖2 + 2δǫt ≤2δǫt\nσk{U∗⊤[M∗ +O(2δǫt)]U (t)} ≥U∗⊤M∗U (t) − 2δǫt ≥σ∗kσk{U∗⊤U (t)} − 2δǫt =σ∗k cos θt − 2δǫt\nαt+1 = tan θt+1 = ‖U∗⊥⊤[M∗ +O(2δǫt)]U (t)‖2 σk{U∗⊤[M∗ +O(2δǫt)]U (t)}\n≤ 2δǫt σ∗k cos θt − 2δǫt .\nAccording to the assumption, cos θt ≥ 1√5 , 2δǫt ≤ 1 2 √ 5 σ∗k, therefore\nαt+1 ≤2 √ 5ǫt/σ ∗ k = 4 √ 5δ(βt + γt)/σ ∗ k .\nTo derive γt+1,\nγt+1 =‖M∗ −M (t+1)‖2 =‖M∗ − (U (t+1)U (t+1)⊤(H(t)1 −H(t)2 +M (t))⊤)‖2 =‖M∗ − U (t+1)U (t+1)⊤(M∗ +O(2δ(γt + βt)))⊤‖2 =‖(I − U (t+1)U (t+1)⊤)M∗ + U (t+1)U (t+1)⊤O(2δ(γt + βt)))⊤‖2 ≤‖(I − U (t+1)U (t+1)⊤)M∗‖2 +O(2δ(γt + βt)) ≤ tan θt+1‖M∗‖2 + 2δ(γt + βt) =αt+1‖M∗‖2 + 2δ(γt + βt) .\nThe recursive inequality of βt is trivial."
    }, {
      "heading" : "C Proof of Theorem 4",
      "text" : "Proof. Denote σ1 = ‖M‖2. Define random matrix\nBi =xixi ⊤Mxixi ⊤ .\nIt is easy to check that\nEBi =2M + tr(M)I .\n‖Bi − EBi‖2 =‖xixi⊤Mxixi⊤ − 2M − tr(M)I‖2 ≤‖xixi⊤Mxixi⊤‖2 + ‖2M − tr(M)I‖2 ≤‖xixi⊤Mxixi⊤‖2 + 2‖M‖2 + |tr(M)| .\nAccording to Lemma 5, with a probability at least 1− η2, for any i ∈ {1, · · · , n},\n|xi⊤Mxi| ≤|tr(M)|+ 2‖M‖F √ 2 log(2n/η2) , c1 .\nTherefore we have, with a probability at least (1− η1)(1− η2),\n‖Bi − EBi‖2 ≤‖xi‖22 |xi⊤Mxi|+ 2‖M‖2 + |tr(M)| ≤2d log(2n/η1)|tr(M)|+ 2‖M‖F √\n2 log(2n/η2) + 2‖M‖2 + |tr(M)| ≤Cdkσ1 .\nNext we need to bound\n‖E(Bi − EBi)(Bi −EBi)⊤‖2 = ‖E(B2i )− (EBi)2‖2 ≤ ‖E(B2i )‖2 + ‖EBi‖22 ≤ ‖E(B2i )‖2 + 2|tr(M)|2 + 2‖M‖22\nTo bound ‖E(B2i )‖2, denote the eigenvalue decomposition of M as\nM = UΛU⊤ = Udiag(λ1, λ2, · · · , λk)U⊤\nLet U⊥ be the complementary basis matrix of U . Define random variables ui , U ⊤ xi, vi , U⊥ ⊤ xi. Since xi are standard random Gaussian, u and v should also be jointly random Gaussian thus independent.\n‖E(B2i )‖2 =‖E(xixi⊤Mxixi⊤xixi⊤Mxixi⊤)‖2\n=‖E( [ ui\nvi\n]\nu ⊤ i Λui(‖ui‖22 + ‖vi‖22)ui⊤Λui\n[\nui\nvi\n]\n⊤)‖2\n≤‖E(uiu⊤i Λui(‖ui‖22 + ‖vi‖22)ui⊤Λuiui⊤‖2 + 2‖E(uiu⊤i Λui(‖ui‖22 + ‖vi‖22)ui⊤Λuivi⊤‖2 + ‖E(viu⊤i Λui(‖ui‖22 + ‖vi‖22)ui⊤Λuivi⊤‖2 ≤‖E(uiu⊤i Λui‖ui‖22ui⊤Λuiui⊤)‖2 + ‖E(uiu⊤i Λui‖vi‖22ui⊤Λuiui⊤)‖2 + 2‖E(uiu⊤i Λui‖ui‖22ui⊤Λuivi⊤)‖2 + 2‖E(uiu⊤i Λui‖vi‖22ui⊤Λuivi⊤)‖2 + ‖E(viu⊤i Λui‖ui‖22ui⊤Λuivi⊤)‖2 + ‖E(viu⊤i Λui‖vi‖22ui⊤Λuivi⊤)‖2 .\nLet us bound the above 6 terms respectively. Recall that with a probability at least 1− η2,\n|u⊤i Λui| =|xi⊤Mxi| ≤ c1 .\nWith a probability at least 1−η3, for any i ∈ {1, · · · , n}, ‖ui‖2 ≤ 2 √ k log(4n/η3),‖vi‖2 ≤ 2 √ d log(4n/η3). Then:\n‖E(uiu⊤i Λui‖ui‖22ui⊤Λuiui⊤)‖2 =‖E{ ( (u⊤i Λui) 2‖ui‖22 ) uiui ⊤}‖2\n≤(u⊤i Λui)2‖ui‖42 ≤32c21k2 log2(2n/η3) .\n‖E(uiu⊤i Λui‖vi‖22ui⊤Λuiui⊤)‖2 =‖E(‖vi‖22)E(uiu⊤i Λuiui⊤Λuiui⊤)‖2 ≤4d log(4n/η3)‖E(uiu⊤i Λuiui⊤Λuiui⊤)‖2 ≤4d log(4n/η3)‖ui‖22(u⊤i Λui)2 ≤4d log(4n/η3)c21(4k log(4n/η3)) .\n2‖E(uiu⊤i Λui‖ui‖22ui⊤Λuivi⊤)‖2 =2‖E(uiu⊤i Λui‖ui‖22ui⊤Λui)E(vi⊤)‖2 = 0\n2‖E(uiu⊤i Λui‖vi‖22ui⊤Λuivi⊤)‖2 =2‖E(ui(u⊤i Λui)2)E(‖vi‖22vi⊤)‖2 =2‖E(ui(u⊤i Λui)2)E(vi⊤vivi⊤)‖2 = 0\n‖E(viu⊤i Λui‖ui‖22ui⊤Λuivi⊤)‖2 =‖E(u⊤i Λui‖ui‖22ui⊤Λui)E(vivi⊤)‖2 =‖E(u⊤i Λui‖ui‖22ui⊤Λui)‖2 ≤(u⊤i Λui)2‖ui‖22 ≤4c21k log(4n/η3) .\n‖E(viu⊤i Λui‖vi‖22ui⊤Λuivi⊤)‖2 =‖E{(u⊤i Λui)2}E(vi‖vi‖22vi⊤)‖2 =‖E{(u⊤i Λui)2}(d+ 2)I‖2 ≤(d+ 2)(u⊤i Λui)2 ≤c21(d+ 2)\nAdd all above together, we have\n‖E(B2i )‖2 ≤32c21k2 log2(2n/η3) + 4d log(4n/η3)c21(4k log(4n/η3)) + 4c21k log(4n/η3) + c 2 1(d+ 2)\n≤Ck3dσ1 .\nApply matrix Bernsterin’s inequality, the proof is completed."
    }, {
      "heading" : "D Proof of Lemma 15",
      "text" : "We assume that n ≥ Ck3d/δ2 . To prove Eq. (5)\n‖ 1 2n A′A(M∗ −M)− 1 2 tr(M∗k −M)I − (M∗k −M)‖2\n≤‖ 1 2n A′A(M∗k −M)− 1 2 tr(M∗k −M)I − (M∗k −M)‖2 + ‖ 1 2n A′A(M∗⊥)‖2 ≤‖ 1 2n A′A(M∗⊥)‖2 + δ‖M∗k −M‖2 .\nThe last inequality is because of Theorem 4. To bound the first term in the last inequality, define random matrix\nBi = xixi ⊤M∗⊥xixi ⊤\nAs proved in Theorem 4, EBi = 2M ∗ ⊥ + tr(M ∗ ⊥)I .\n‖(Bi − EBi)‖2 =‖xixi⊤M∗⊥xixi⊤ − 2M∗⊥ + tr(M∗⊥)I‖2 ≤‖xixi⊤M∗⊥xixi⊤‖2 + 2‖M∗⊥‖2 + ‖tr(M∗⊥)I‖2 =‖xixi⊤M∗⊥xixi⊤‖2 + 2σ∗k+1 + |tr(M∗⊥)|\nWhile\n‖xixi⊤M∗⊥xixi⊤‖2 ≤‖M∗⊥‖2‖xi‖42 ≤σ∗k+1(d+ 2 √ 2d log(2n/η))2\n≤Cd2σ∗k+1\nApplying matrix Bernstein’s inequality, with a probability at least 1− η, we have\n‖ 1 n\nn ∑\ni=1\n(Bi − EBi)‖2 ≤Cσ∗k+1d2/ √ n .\nTherefore\n‖ 1 2n A′A(M∗ −M)− 1 2 tr(M∗k −M)I − (M∗k −M)‖2 ≤δ‖M∗k −M‖2 + Cσ∗2k+1d4/\n√ n .\nThe other inequalities can be similarly proved."
    }, {
      "heading" : "E Proof of Lemma 14",
      "text" : "First we bound αt+1. According to assumption, when\n2(δǫt + r) ≤ σ∗k − σ∗k+1\n2σ∗k σ∗k/\n√ 5\nwe have\nαt+1 ≤ σ∗k+1 sin θt + 2(δǫt + r)\nσ∗k cos θt − 2(δǫt + r)\n≤ 2σ ∗ k\nσ∗k + σ ∗ k+1\nσ∗k+1 sin θt + 2(δǫt + r)\nσ∗k cos θt\n≤ 2σ ∗ k+1\nσ∗k + σ ∗ k+1\ntan θt + 2\nσ∗k + σ ∗ k+1\n2(δǫt + r)\ncos θt\n≤ 2σ ∗ k+1\nσ∗k + σ ∗ k+1\ntan θt + 4 √ 5\nσ∗k + σ ∗ k+1\n(δǫt + r)\n≤ραt + 4 √ 5\nσ∗k + σ ∗ k+1\nδǫt + 4 √ 5\nσ∗k + σ ∗ k+1\nr .\nTo bound βt+1. Clearly βt+1 ≤ δǫt + r. To bound γt+1, following the noise-free case,\nγt+1 ≤αt+1‖M∗‖2 + 2δǫt + 2r ."
    }, {
      "heading" : "F Proof of Lemma 15",
      "text" : "Abbreviate\nc = 4 √ 5\nσ∗k + σ ∗ k+1\nThen\nαt+1 ≤ραt + cδǫt + cr .\nAccording to Lemma 14,\nβt+1 + γt+1 ≤δǫt + r + αt+1‖M∗‖2 + 2δǫt + 2r =σ∗1αt+1 + 3δǫt + 3r\n≤σ∗1(ραt + cδǫt + cr) + 3δǫt + 3r =ρσ∗1αt + (σ ∗ 1c+ 3)δǫt + (σ ∗ 1c+ 3)r\nTherefore, abbreviate b , (σ∗1c+ 3), {\nαt+1 ≤ ραt + cδǫt + cr ǫt+1 ≤ ρσ∗1αt + bδǫt + br\ndefine\nft =αt + 2cδǫt\nft+1 =at+1 + 2cδǫt+1\n≤ραt + cδǫt + cr + 2cδ(ρσ∗1αt + bδǫt + br) =ραt + cδǫt + cr + 2cδρσ ∗ 1αt + 2cδbδǫt + 2cδbr =(ρ+ 2cδρσ∗1)αt + (c+ 2cδb)δǫt + (1 + 2δb)cr\nWhen\nδ ≤ 1− ρ 4ρσ∗1c\n⇒ρ+ 2cδρσ∗1 ≤ 1 + ρ\n2\nAnd when\n⇒δ ≤ ρ 2b ⇒2δb ≤ ρ ⇒2cδb ≤ ρc ⇒c+ 2cδb ≤ (1 + ρ)c\n⇒c+ 2cδb ≤ 1 + ρ 2 2c\nThen abbreviate R , (c+ 2cδb)δǫt + (1 + 2δb)cr we have\nft+1 ≤ 1 + ρ 2 ft + (1 + 2δb)cr ≤ 1 + ρ 2 ft + (1 + ρ)cr\nAbbreviate q = (1 + ρ)/2,\nft ≤ (1 + ρ)cr 1− q + q t(f0 − (1 + ρ)cr 1− q )\nTo ensure αt+1 ≤ 2, we require\nf0 ≤ 2 ⇐α0 + 2cδǫ0 ≤ 2\nAccording to Lemma 12,\nα0 ≤ 2 σ∗k − σ∗k+1 2(δǫ0 + r) = 4 σ∗k − σ∗k+1 (δǫ0 + r)\nα0 + 2cδǫ0 ≤ 2\n⇐ 4 σ∗k − σ∗k+1 (δǫ0 + r) + 2cδǫ0 ≤ 2 ⇐(4 + 2c(σ∗k − σ∗k+1))δǫ0 + 4r ≤ 2(σ∗k − σ∗k+1) ⇐(2 + c(σ∗k − σ∗k+1))δǫ0 + r ≤ (σ∗k − σ∗k+1)\nIn summary,\nαt + 2cδǫt ≤qt(f0 − (1 + ρ)cr 1− q ) + (1 + ρ)cr\n1− q provided\nδ ≤min{ 1− ρ 4ρσ∗1c , ρ 2b }\nand\n(2 + c(σ∗k − σ∗k+1))δǫ0 + r ≤ (σ∗k − σ∗k+1) 4 √ 5(δmax\nt ǫt + r) ≤ σ∗k − σ∗k+1\nTo ensure the last inequality,\nδmax t ǫt ≤f0 ≤ α0 + 2cδǫ0 ≤ 4 σ∗k − σ∗k+1 (δǫ0 + r) + 2cδǫ0\n=( 4\nσ∗k − σ∗k+1 + 2c)δǫ0 +\n4\nσ∗k − σ∗k+1 r\nTherefore we need the condition\n4 √ 5 (\n4\nσ∗k − σ∗k+1 + 2c\n) δǫ0 + 4 √ 5 (\n4\nσ∗k − σ∗k+1 + 1\n)\nr ≤ σ∗k − σ∗k+1\n⇐4 √ 5 (4 + 2c(σ∗k − σ∗k+1)) δǫ0 + 4 √ 5 (4 + (σ∗k − σ∗k+1)) r ≤ (σ∗k − σ∗k+1)2"
    } ],
    "references" : [ {
      "title" : "Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms",
      "author" : [ "Mathieu Blondel", "Masakazu Ishihata", "Akinori Fujino", "Naonori Ueda" ],
      "venue" : null,
      "citeRegEx" : "Blondel et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Blondel et al\\.",
      "year" : 2016
    }, {
      "title" : "ROP: Matrix recovery via rank-one projections",
      "author" : [ "T. Tony Cai", "Anru Zhang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Cai and Zhang.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cai and Zhang.",
      "year" : 2015
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computational mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "Phase Retrieval via Matrix Completion",
      "author" : [ "Emmanuel J. Candes", "Yonina Eldar", "Thomas Strohmer", "Vlad Voroninski" ],
      "venue" : null,
      "citeRegEx" : "Candes et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Candes et al\\.",
      "year" : 2011
    }, {
      "title" : "Exact and stable covariance estimation from quadratic sampling via convex programming",
      "author" : [ "Yuxin Chen", "Yuejie Chi", "Andrea J. Goldsmith" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "An overview of low-rank matrix recovery from incomplete observations",
      "author" : [ "Mark A. Davenport", "Justin Romberg" ],
      "venue" : null,
      "citeRegEx" : "Davenport and Romberg.,? \\Q2016\\E",
      "shortCiteRegEx" : "Davenport and Romberg.",
      "year" : 2016
    }, {
      "title" : "The Noisy Power Method: A Meta Algorithm",
      "author" : [ "Moritz Hardt", "Eric Price" ],
      "venue" : null,
      "citeRegEx" : "Hardt and Price.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hardt and Price.",
      "year" : 2013
    }, {
      "title" : "Low-rank Matrix Completion",
      "author" : [ "Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi" ],
      "venue" : null,
      "citeRegEx" : "Jain et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2013
    }, {
      "title" : "Low rank matrix recovery",
      "author" : [ "Richard Kueng", "Holger Rauhut", "Ulrich Terstiege" ],
      "venue" : null,
      "citeRegEx" : "Kueng et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kueng et al\\.",
      "year" : 2011
    }, {
      "title" : "Matrix Perturbation Theory",
      "author" : [ "G.W. Stewart", "Ji-guang Sun" ],
      "venue" : null,
      "citeRegEx" : "Stewart and Sun.,? \\Q1990\\E",
      "shortCiteRegEx" : "Stewart and Sun.",
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : ", 2014] as a special case, where the latter one has been studied widely in context such as inductive matrix completion [Jain and Dhillon, 2013] and phase retrieval [Candes et al., 2011].",
      "startOffset" : 164,
      "endOffset" : 185
    }, {
      "referenceID" : 6,
      "context" : "Then we can construct an estimation sequence via noisy power iteration [Hardt and Price, 2013].",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : ", 2015, Cai and Zhang, 2015, Kueng et al., 2014] as a special case, where the latter one has been studied widely in context such as inductive matrix completion [Jain and Dhillon, 2013] and phase retrieval [Candes et al., 2011]. Despite of the popularity of FMs in industry, there is rare theoretical study of learning guarantees for FMs. One of the main challenges in developing a provable FM algorithm is to handle its symmetric rank-one matrix sensing operator. For conventional matrix sensing problems where the matrix sensing operator is RIP, there are several alternating methods with provable guarantees [Hardt, 2013, Jain et al., 2013, Hardt and Wootters, 2014, Zhao et al., 2015a,b]. However, for a symmetric rank-one matrix sensing operator, the RIP condition doesn’t hold trivially which turns out to be the main difficulty in designing efficient provable FM solvers. In rank-one matrix sensing, when the sensing operator is asymmetric, the problem is also known as inductive matrix completion which can be solved via alternating minimization with a global linear convergence rate [Jain and Dhillon, 2013, Zhong et al., 2015]. For symmetric rank-one matrix sensing operators, we are not aware of any efficient solver by the time of writing this paper. In a special case when the target matrix is of rank one, the problem is called “phase retrieval” whose convex solver is first proposed by Candes et al. [2011] then alternating methods are provided in [Lee et al.",
      "startOffset" : 8,
      "endOffset" : 1421
    }, {
      "referenceID" : 0,
      "context" : "Some authors also call gFM as second order polynomial network [Blondel et al., 2016].",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "For example, the sensing operator Aasy(·) is unbiased ( EAasy(·) = 0) but the symmetric sensing operator is clearly not [Cai and Zhang, 2015].",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "In conventional matrix sensing, this construction is possible when the sensing matrix satisfies the Restricted Isometric Property (RIP) [Candès and Recht, 2009]:",
      "startOffset" : 136,
      "endOffset" : 160
    }, {
      "referenceID" : 1,
      "context" : "However, in gFM and symmetric rank-one matrix sensing, the l2-norm RIP condition cannot be satisfied with high probability [Cai and Zhang, 2015].",
      "startOffset" : 123,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "To bound α0 , we need the following lemma which directly follows Wely’s and Wedin’s theorems [Stewart and Sun, 1990].",
      "startOffset" : 93,
      "endOffset" : 116
    } ],
    "year" : 2016,
    "abstractText" : "We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from d dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank k, our algorithm converges linearly, achieves O(ǫ) recovery error after retrieving O(kd log(1/ǫ)) training instances, consumes O(kd) memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval.",
    "creator" : "LaTeX with hyperref package"
  }
}