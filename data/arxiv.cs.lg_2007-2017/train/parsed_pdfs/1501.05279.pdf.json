{
  "name" : "1501.05279.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Extreme Entropy Machines: Robust information theoretic classification",
    "authors" : [ "Wojciech Marian Czarnecki", "Jacek Tabor" ],
    "emails" : [ "jacek.tabor}@uj.edu.pl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The main contribution of this paper is proposing a model based on the information theoretic concepts which on the one hand shows new, entropic perspective on known linear classifiers and on the other leads to a construction of very robust method competetitive with the state of the art non-information theoretic ones (including Support Vector Machines and Extreme Learning Machines).\nEvaluation on numerous problems spanning from small, simple ones from UCI repository to the large (hundreads of thousands of samples) extremely unbalanced (up to 100:1 classes’ ratios) datasets shows wide applicability of the EEM in real life problems and that it scales well."
    }, {
      "heading" : "1 Introduction",
      "text" : "There is no one, universal, perfect optimization criterion that can be used to train machine learning model. Even for linear classifiers one can find multiple objective functions, error measures to minimize, regularization methods to include [14]. Most of the existing methods are aimed at minimization of empirical risk (through some simple point-based error measured with loss function) with added regularization. We propose to approach this problem in more information theoretic way by investigating applicability of entropy measures as a classification model objective function. We focus on quadratic Renyi’s entropy and connected Cauchy-Schwarz Divergence.\nOne of the information theoretic concepts which has been found very effective in machine learning is the entropy measure. In particular the rule of maximum entropy modeling led to the construction of MaxEnt model and its structural generalization – Conditional Random Fields which are considered state of the art in many applications. In this paper we propose to use Renyi’s quadratic cross entropy as the measure of two density estimations divergence\nar X\niv :1\n50 1.\n05 27\n9v 1\n[ cs\n.L G\n] 2\n1 Ja\nn 20\nin order to find best linear classifier. It is a conceptually different approach than typical entropy models as it works in the input space instead of decisions distribution. As a result we obtain a model closely related to the Fischer’s Discriminant (or more generally Linear Discriminant Analysis) which deepens the understanding of this classical approach. Together with a powerful extreme data transformation we obtain a robust, nonlinear model competetive with the state of the art models not based on information theory like Support Vector Machines (SVM [4]), Extreme Learning Machines (ELM [10]) or Least Squares Support Vector Machines (LS-SVM [21]). We also show that under some simplifing assumptions ELM and LS-SVM can be seen through a perspective of information theory as their solutions are (up to some constants) identical to the ones obtained by proposed method.\nPaper is structured as follows: first we recall some preliminary information regarding ELMs and Support Vector Machines, including Least Squares Support Vector Machines. Next we introduce our Extreme Entropy Machine (EEM) together with its kernelized extreme counterpart – Extreme Entropy Kernel Machine (EEKM). We show some connections with existing models and some different perspectives for looking at proposed model. In particular, we show how learning capabilities of EEMs (and EEKM) reasamble those of ELM (and LS-SVM respectively). During evaluation on over 20 binary datasets (of various sizes and characteristics) we analyze generalization capabilities and learning speed. We show that it can be a valuable, robust alternative for existing methods. In particular, we show that it achieves analogous of ELM stability in terms of the hidden layer size. We conclude with future development plans and open problems."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Let us begin with recalling some basic information regarding Extreme Learning Machines [11] and Support Vector Machines [4] which are further used as a competiting models for proposed solution. We focus here on the optimization problems being solved to underline some basic differences between these methods and EEMs."
    }, {
      "heading" : "2.1 Extreme Learning Machines",
      "text" : "Extreme Learning Machines are relatively young models introduced by Huang et al. [10] which are based on the idea that single layer feed forward neural networks (SLFN) can be trained without iterative process by performing linear regression on the data mapped through random, nonlinear projection (random hidden neurons). More precisely speaking, basic ELM architecture consists of d input neurons connected with each input space dimension which are fully connected with h hidden neurons by the set of weights wj (selected randomly from some arbitrary distribution) and set of biases bj (also randomly selected). Given some generalized nonlinear activation function G one can express the hidden neurons activation matrix H for the whole training set X,T = {(xi, ti)} N i=1 such that xi ∈ Rd and ti ∈ {−1. + 1} as\nHij =G(xi,wj , bj).\nwe can formulate following optimization problem\nOptimization problem: Extreme Learning Machine\nminimize β\n∥Hβ −T∥ 2\nwhere Hij =G(xi,wj , bj)., i = 1, . . . ,N, j = 1, . . . , h\nIf we denote the weights between hidden layer and output neurons by β it is easy to show [11] that putting\nβ =H†T,\ngives the best solution in terms of mean squared error of the regression:\n∥Hβ −T∥ 2 = ∥H(H†T) −T∥ 2 = min α∈Rh ∥Hα −T∥ 2\nwhere H† denotes Moore-Penrose pseudoinverse of matrix H. Final classification of the new point x can be now performed analogously by classifying according to\ncl(x) = sign([G(x,w1, b1) . . . G(x,wd, bd)]β).\nAs it is based on the oridinary least squares optimization, it is possible to balance it in terms of unbalanced datasets by performing weighted ordinary least squares. In such a scenario, given a vector B such that Bi is a square root of the inverse of the xi’s class size and B ⋅X denotes element wise multiplication between B and X:\nβ = (B ⋅H)†B ⋅T"
    }, {
      "heading" : "2.2 Support Vector Machines and Least Squares Support Vector Machines",
      "text" : "One of the most well known classifiers of the last decade is Vapnik’s Support Vector Machine (SVM [4]), based on the principle of creating linear classifier that maximizes the separating margin between elements of two classes.\nOptimization problem: Support Vector Machine\nminimize β,b.ξ\n1 2 ∥β∥ 2 +C\nN ∑ i=1 ξi\nsubject to ti(⟨β,xi⟩ + b) = 1 − ξi, i = 1, . . . ,N\nwhich can be further kernelized (delinearized) using any kernel K (valid in the Mercer’s sense):\nOptimization problem: Kernel Support Vector Machine\nmaximize β\nN ∑ i=1 βi − 1 2 N ∑ i,j=1 βiβjtitjK(xi,xj)\nsubject to N\n∑ i=1 βiti = 0\n0 ≤ βi ≤ C, i = 1, . . . ,N\nThe problem is a quadratic optimization with linear constraints, which can be efficiently solved using quadratic programming techniques. Due to the use of hinge loss function on ξi SVM attains very sparse solutions in terms of nonzero βi. As a result, classifier does not have to remember the whole training set, but instead, the set of so called support vectors (SV = {xi ∶ βi > 0}), and classify new point according to\ncl(x) = sign ⎛\n⎝ ∑\nxi∈SV\nβiK(xi,x) + b ⎞\n⎠ .\nIt appears that if we change the loss function to the quadratic one we can greatly reduce the complexity of the resulting optimization problem, leading to the so called Least Squares Support Vector Machines (LS-SVM).\nOptimization problem: Least Squares Support Vector Machine\nminimize β,b.ξ\n1 2 ∥β∥ 2 +C\nN ∑ i=1 ξ2i\nsubject to ti(⟨β,xi⟩ + b) = 1 − ξi, i = 1, . . . ,N\nand decision is made according to\ncl(x) = sign(⟨β,x⟩ + b)\nAs Suykens et al. showed [21] this can be further generalized for abitrary kernel induced spaces, where we classify according to:\ncl(x) = sign( N\n∑ i=1 βiK(xi,x) + b)\nwhere βi are Lagrange multipliers associated with particular training examples xi and b is a threshold, found by solving the linear system\n[ 0 1T 1 K(X,X) + I/C ] [ b β ] = [ 0 T ]\nwhere 1 is a vector of ones and I is an identity matrix of appropriate dimensions. Thus a training procedure becomes\n[ b β ] = [\n0 1T 1 K(X,X) + I/C ]\n−1\n[ 0 T ] .\nSimilarly to the classical SVM, this formulation is highly unbalanced (it’s results are skewed towards bigger classes). To overcome this issue one can introduce a weighted version [20], where given diagonal matrix of weights Q, such that Qii is invertibly proportional to the size of xi’s class and .\n[ b β ] = [\n0 1T 1 K(X,X) +Q/C ]\n−1\n[ 0 T ] .\nUnfortunately, due to the introduction of the square loss, the Support Vector Machines sparseness of the solution is completely lost. Resulting training has a closed form solution, but requires the computation of the whole Gram matrix\nand the resulting machine has to remember1 whole training set in order to perform new point’s classification."
    }, {
      "heading" : "3 Extreme Entropy Machines",
      "text" : "Let us first recall the formulation of the linear classification problem in the highly dimensional feature spaces, ie. when number of samples N is equal (or less) than dimension of the feature space h. In particular we formulate the problem in the limiting case2 when h =∞:\nProblem 1. We are given finite number of (often linearly independent) points h±i in an infinite dimensional Hilbert space H. Points h\n+ ∈ H+ constitute the positive class, while h− ∈H− the negative class.\nWe search for β ∈ H such that the sets βTH+ and βTH− are optimally separated.\nObserve that in itself (without additional regularization) the problem is not well-posed as, by applying the linear independence of the data, for arbitrary m+ ≠m− in R we can easily construct β ∈H such that\nβTH+ = {m+} and β TH− = {m−}. (1)\nHowever, this leads to a model case of overfitting, which typically yields suboptimal results on the testing set (different from the orginal training samples).\nTo make the problem well-posed, we typically need to:\n1. add/allow some error in the data,\n2. specify some objective function including term penalising model’s complexity.\nPopular choices of the objective function include per-point classification loss (like square loss in neural networks or hinge loss in SVM) with a regularization term added, often expressed as the square of the norm of our operator β (like in SVM or in weight decay regularization of neural networks). In general one can divide objective functions derivations into following categories:\n• regression based (like in neural networks or ELM),\n• probabilistic (like in the case of Naive Bayes),\n• geometric (like in SVM),\n• information theoretic (entropy models).\nWe focus on the last group of approaches, and investigate the applicability of the Cauchy-Schwarz divergence [12], which for two densities f and g is given by\nDcs(f, g) = ln(∫ f 2 ) + ln(∫ g 2 ) − 2 ln(∫ fg)\n= −2 ln(∫ f\n∥f∥2 g ∥g∥2 ) .\n1there are some pruning techniques for LS-SVM but we are not investigating them here 2which is often obtained by the kernel approach\nCauchy-Schwarz divergence is connected to Renyi’s quadratic cross entropy (H×2 [18]) and Renyi’s quadratic entropy (H2), defined for densities f, g as\nH×2 (f, g) = − ln(∫ fg)\nH2(f) =H × 2 (f, f) = − ln(∫ f 2 ) ,\nconsequently Dcs(f, g) = 2H × 2 (f, g) −H2(f) −H2(g).\nand as we showed in [6], it is well-suited as a discrimination measure which allows the construction of mulit-threshold linear classifiers. In general increase of the value of Cauchy-Schwarz Divergence results in better sets’ (densities’) discrimination. Unfortunately, there are a few problems with such an approach:\n• true datasets are discrete, so we do not have densities f and g,\n• statistical density estimators require rather large sample sizes and are very computationally expensive.\nThere are basically two approaches which help us recover underlying densities from the samples. First one is performing some kind of density esimation, like the well known Kernel Density Estimation (KDE) technique, which is based on the observation that any arbitrary continuous distribution can be sufficiently approximated by the convex combination of Gaussians. The other approach is to assume some density model (distribution’s family) and fit its parameters in order to maximize the data generation probability. In statistics it is known as maximum likelihood esetimation (MLE) approach. MLE has an advantage that in general it produces much simplier densities descriptions than KDE as later’s description is linearly big in terms of sample size.\nA common choice of density models are Gaussian distributions due to their nice theoretical and practical (computational) capabilities. As mentioned eariler, the conxex combination of Gaussians can approximate the given continuous distribution f with arbitrary precision. In order to fit a Gaussian Mixture Model (GMM) to given dataset one needs algorithm like Expectation Maximization [8] or conceptually similar Cross-Entropy Clustering [22]. However, for simplicity and strong regularization we propose to model f as one big Gaussian N (m,Σ). One of the biggest advantages of such an approach is closed form MLE parameter estimation, as we simply put m equal to the empirical mean of the data, and Σ as some data covariance estimator. Secondly, this way we introduce an error to the data which has an important regularizing role and leads to better posed optimization problem.\nLet us now recall that the Shannon’s differential entropy (expressed in nits) of the continuous distribution f is\nH(f) = −∫ f ln f,\nwe will now show that choice of Normal distributions is not arbitrary but supported by the assumptions of the entropy maximization. Following result is known, but we include the whole reasoning for completeness.\nRemark 1. Normal distribution N (m,Σ) has a maximum Shannon’s differential entropy among all real-valued distributions with mean m ∈ Rh and covariance Σ ∈ Rh×h.\nProof. Let f and g be arbitrary distributions with covariance Σ and means m. For simplicity we assume that m = 0 but the analogous proof holds for arbitrary mean, then\n∫ fhihjdhidhj = ∫ ghihjdhidhj = Σij ,\nso for quadratic form A\n∫ Af = ∫ Ag.\nNotice that\nlnN (0,Σ)[h] = ln ⎛\n⎝\n1 √\n(2π)h det(Σ) exp(− 1 2 hTΣ−1h)\n⎞\n⎠\n= − 1\n2 ln((2π)h det(Σ)) − 1 2 hTΣ−1h\nis a quadratic form plus constant thus\n∫ f lnN (0,Σ) = ∫ N (0,Σ) lnN (0,Σ),\nwhich together with non-negativity of Kullback-Leibler Divergence gives\n0 ≤Dkl(f ∣∣ N (0,Σ))\n= ∫ f ln( f N (0,Σ) )\n= ∫ f ln f − ∫ f lnN (0,Σ)\n= −H(f) − ∫ f lnN (0,Σ)\n= −H(f) − ∫ N (0,Σ) lnN (0,Σ)\n= −H(f) +H(N (0,Σ)),\nthus H(N (0,Σ)) ≥H(f).\nThere appears nontrivial question how to find/estimate the desired Gaussian as the covariance can be singular. In this case to regularize the covariance we apply the well-known Ledoit-Wolf approach [15].\nΣ± = cov†(H ± ) = (1 − ε±)cov(H±) + ε±tr(cov(H±))h−1I,\nwhere cov(⋅) is an empirical covariance and ε± is a shrinkage coefficient given by Ledoit and Wolf [15].\nThus, our optimization problem can be stated as follows:\nProblem 2 (Optimization problem). Suppose that we are given two datasets H± in a Hilbert space H which come from the Gaussian distributions N (m±,Σ±). Find β ∈H such that the datasets\nβTH+ and βTH−\nare optimally discriminated in terms of Cauchy-Schwarz Divergence.\nBecause H± has densityN (m±,Σ±), βTX± has the densityN (βTm±,βTΣ±β). We put\nm± = β Tm±, S± = β TΣ±β. (2)\nSince, as one can easily compute [5],\n∫ N (m+, S+) ∥N (m+, S+)∥2 ⋅ N (m−, S−) ∥N (m−, S−)∥2\n= N (m+ −m−, S+ + S−)[0]\n(N (0,2S+)[0]N (0,2S−)[0])1/2\n= (2πS+S−)\n1/4\n(S+ + S−)1/2 exp(−\n(m+ −m−) 2\n2(S+ + S−) ) ,\nwe obtain that\nDcs(N (m+, S+),N (m−, S−))\n= − ln(∫ N (m+, S+) ∥N (m+, S+)∥2 ⋅ N (m−, S−) ∥N (m−, S−)∥2 )\n= − 1 2 ln π 2 − ln 1 2 (S+ + S−) √ S+S− + (m+ −m−) 2 S+ + S− .\n(3)\nObserve that in the above equation the first term is constant, the second is the logarithm of the quotient of arithmetical and geometrical means (and therefore in the typical cases is bounded and close to zero). Consequently, crucial information is given by the last term. In order to confirm this claim we perform experiments on over 20 datasets used in further evaluation (more details are located in the Evaluation section). We compute the Spearman’s rank correlation\ncoefficient between the Dcs(N (m+, S+),N (m−, S−)) and (m+−m−)\n2\nS++S− for hundread\nrandom projections to H and hundread random linear operators β.\nAs one can see in Table 1, in small datasets (first part of the table) the correlation is generally high, with some exceptions (like sonar, splice, liverdisorders and diabetes). However, for bigger datasets (consisting of thousands examples) this correlation is nearly perfect (up to the randomization process it is nearly 1.0 for all cases) which is a very strong empirical confirmation of our claim that maximization of the (m+−m−) 2\nS++S− is generally equivalent to the\nmaximization of Dcs(N (m+, S+),N (m−, S−)). This means that, after the above reductions, and application of (2) our final problem can be stated as follows:\nOptimization problem: Extreme Entropy Machine\nminimize β\nβTΣ+β +βTΣ−β\nsubject to βT (m+ −m−) = 2\nwhere Σ± = cov†(H ± )\nm± = 1\n∣H±∣ ∑ h±∈H± h±\nH± = ϕ(X±)\nBefore we continue to the closed-form solution we outline two methods of actually transforming our data X± ⊂ X to the highly dimensional H± ⊂H, given by the ϕ ∶ X →H.\nWe investigate two approaches which lead to the Extreme Entropy Machine and Extreme Entropy Kernel Machine respectively.\n• for Extreme Entropy Machine (EEM) we use the random projection technique, exactly the same as the one used in the ELM. In other words, given some generalized activation function G(x,w, b) ∶ X ×X ×R→ R and a constant h denoting number of hidden neurons:\nϕ ∶ X ∋ x→ [G(x,w1, b1), . . . ,G(x,wh, bh)] T ∈ Rh\nwhere wi are random vectors and bi are random biases.\n• for Extreme Entropy Kernel Machine (EEKM) we use the randomized kernel approximation technique [9], which spans our Hilbert space on randomly selecteed subset of training vectors. In other words, given valid kernel K(⋅, ⋅) ∶ X ×X → R+ and size of the kernel space base h:\nϕK ∶ X ∋ x→ (K(x,X [h] )K(X[h],X[h])−1/2)T ∈ Rh\nwhere X[h] is a h element random subset of X. It is easy to verify that such low rank approxmation truly behaves as a kernel, in the sense that for ϕK(xi), ϕK(xj) ∈ Rh\nϕK(xi) TϕK(xj) =\n=((K(xi,X [h] )K(X[h],X[h])−1/2)T )T\n(K(y,X[h])K(X[h],X[h])−1/2)T\n=K(xi,X [h] )K(X[h],X[h])−1/2\n(K(y,X[h])K(X[h],X[h])−1/2)T\n=K(xi,X [h] )K(X[h],X[h])−1/2\nK(X[h],X[h])−1/2KT (xj ,X [h] )\n=K(xi,X [h] )K(X[h],X[h])−1K(X[h],xj),\ngiven true kernel projection φK such that\nK(xi,xj) = φK(xi) TφK(xj)\nwe have K(xi,X [h] )K(X[h],X[h])−1K(X[h],xj) =\n=φK(xi) TφK(X [h] )\n(φK(X [h] ) TφK(X [h] )) −1\nφK(X [h] ) TφK(xj)\n=φK(xi) TφK(X [h] )φK(X [h] ) −1\n(φK(X [h] ) T ) −1φK(X [h] ) TφK(xj)\n=φK(xi) TφK(xj)\n=K(xi,xj).\nThus for the whole samples’ set we have\nϕK(X) TϕK(X) =K(X,X),\nwhich is a complete Gram matrix.\nSo the only difference between Extreme Entropy Machine and Extreme Entropy Kernel Machine is that in later we use H± = ϕK(X\n±) where K is a selected kernel instead of H± = ϕ(X±). Fig. 1 visualizes these two approaches as neural networks, in particular EEM is a simple SLFN, while EEKM leads to the network with two hidden layers.\nRemark 2. Extreme Entropy Machine optimization problem is closely related to the SVM optimization, but instead of maximizing the margin between closest points we are maximizing the mean margin.\nProof. Let us recall that in SVM we try to maximize the margin 2 ∥β∥ under constraints that negative samples are projected at values at most -1 (βTh−+b ≤ −1) and positive samples on at least 1 (βTh+ + b ≥ 1) In other words, we are minimizing the β operator norm\n∥β∥\nwhich is equivalent to minimizing the square of this norm ∥β∥2, under constraint that\nmin h+∈H+ {βTh+} − max h−∈H− {βTh−} = 1 − (−1) = 2.\nOn the other hand, EEM tries to minimize\nβTΣ+β +βTΣ−β = βT (Σ+ +Σ−)β\n= ∥β∥2Σ++Σ−\nunder the constraint that\n1 ∣H+∣ ∑ h+∈H+ βTh+ − 1 ∣H−∣ ∑ h−∈H− βTh− = 2.\nSo what is happening here is that we are trying to maximize the mean margin between classes in the Mahalanobis norm generated by the sum of classes’ covariances. It was previously shown in Two ellipsoid Support Vector Machines model [7] that such norm is an approximation of the margin coming from two ellpisoids instead of the single ball used by traditional SVM.\nSimilar observation regarding connection between large margin classification and entropy optimization has been done in case of the Multithreshold Linear Entropy Classifier [6].\nWe are going to show by applying the standard method of Lagrange multipliers that the above problem has a closed form solution (similar to the Fischer’s Discriminant). Let\nΣ = Σ+ +Σ− and m =m+ −m−.\nWe put L(β, λ) ∶= 2βTΣβ − λ(βTm − 2).\nThen\n∇vL = 2Σβ − λm and ∂\n∂λ L = βTm − 2,\nwhich means that we need to solve, with respect to β, the system\n⎧⎪⎪ ⎨ ⎪⎪⎩ 2Σβ − λm = 0, βTm = 2.\nTherefore β = λ 2 Σ−1m, which yields\nλ 2 mTΣ−1m = 2,\nand consequently3, if m ≠ 0, then λ = 4/∥m∥2Σ and\nβ = 2 ∥m∥2\nΣ\nΣ−1m\n= 2(Σ+ +Σ−)−1(m+ −m−)\n∥m+ −m−∥2Σ++Σ− .\n(4)\nThe final decision of the class of the point h is therefore given by the comparison of the values"
    }, {
      "heading" : "N (βTm+,βTΣ+β)[βTh] and N (βTm−,βTΣ−β)[βTh].",
      "text" : "We distinguish two cases based on number of resulting classifier’s thresholds (points t such that N (βTm+,βTΣ+β)[t] = N (βTm−,βTΣ−β)[t]):\n3where ∥m∥2Σ =m T Σ−1m denotes the squared Mahalanobis norm of m.\n1. S− = S+, then there is one threshold\nt0 =m− + 1,\nwhich results in a traditional (one-threshold) linear classifier,\n2. S− ≠ S+, then there are two thresholds\nt± =m− + 2S−±\n√ S−S+(ln(S−/S+)(S−−S+)+4)\nS−−S+ ,\nwhich makes the resulting classifier a member of two-thresholds linear classifiers family [1].\nObviously, in the degenerated case, when m = 0 ⇐⇒ m− = m+ there is no solution, as the constraint βT (m− −m+) = 2 is not fulfilled for any β. In such a case EEM returna a trivial classifier constantly equal to any class (we put β = 0).\nFrom the neural network perspetive we simply construct a custom activation function F(⋅) in the output neuron depending on one of the two described cases:\n1. F(x) = { +1, if x ≥ t0 −1, if x < t0 = sign(x − t0),\n2. F(x) = { +1, if x ∈ [t−, t+] −1, if x ∉ [t−, t+] = −sign(x − t−)sign(x − t+),\nif t− < t+ and F(x) = { −1, if x ∈ [t+, t−] +1, if x ∉ [t+, t−] = sign(x − t−)sign(x − t+), otherwise.\nThe whole classification process is visualized in Fig. 2, we begin with data in the input space X , transform it into Hilbert space H where we model them as Gaussians, then perform optimization leading to the projection on R through β and perform densitiy based classification leading to non-linear decision boundary in X ."
    }, {
      "heading" : "4 Theory: density estimation in the kernel case",
      "text" : "To illustrate our reasoning, we consider a typical basic problem concerning the density estimation.\nProblem 3. Assume that we are given a finite data set H in a Hilbert space H generated by the unknown density f , and we want to obtain estimate of f .\nSince the problem in itself is infinite dimensional typically the data would be linearly independent. Moreover, one usually can not obtain reliable density estimation - the most we can hope is that after transformation by a linear functional into R, the resulting density will be well-estimated.\nTo simplify the problem assume therefore that we want to find the desired density in the class of normal densities – or equivalently that we are interested only in the estimation of the mean and covariance of f .\nThe generalization of the above problem is given by the following problem:\nProblem 4. Assume that we are given a finite data sets H± in a Hilbert space H generated by the unknown densities f±, and we want to obtain estimate of the unknown densities.\nIn general dim(H) = h ≫ N which means that we have very sparse data in terms of Hilbert space. As a result, classical kernel density estimation (KDE) is not reliable source of information [16]. In the absence of different tools we can however use KDE with very big kernel width in order to cover at least some general shape of the whole density.\nRemark 3. Assume that we are given a finite data sets H± with means m± and covariances Σ± in a Hilbert space H. If we conduct kernel density estimation using Gaussian kernel then, in a limiting case, each class becomes a Normal distribution.\nlim σ→∞\n∥⟦H±⟧σ −N (m ±, σ2Σ±)∥2 = 0,\nwhere ⟦A⟧σ = 1 ∣A∣ ∑\na∈A\nN (a, σ2 ⋅ cov(A))\nProof of this remark is given by Czarnecki and Tabor [6] and means that if we perform a Gaussian kernel density estimation of our data with big kernel width (which is reasonable for small amount of data in highly dimensional space) then for big enough σ̂ EEM is nearly optimal linear classifier in terms of estimated densities\nf̂± = N (m±, σ̂2Σ±) ≈ ⟦H±⟧σ̂.\nLet us now investigate the probabilistic interpretation of EEM. Under the assumption that H± ∼ N (m±,Σ±) we have the conditional probabilities\np(h∣±) = N (m±,Σ±)[h],\nso from Bayes rule we conclude that\np(±∣h) = p(h∣±)p(±)\np(h)\n∝ N (m±,Σ±)[h]p(±),\nwhere p(±) is a prior classes’ distribution. In our case, due to the balanced nature (meaning that despite classes imbalance we maximize the balanced quality measure such as Averaged Accuracy) we have p(±) = 1/2. But\np(h) = ∑ t∈{+,−} p(h∣t),\nso\np(±∣h) = N (m±,Σ±)[h]\n∑t∈{+,−}N (m t,Σt)[h]\n.\nFurthermore it is easy to show that under the normality assumption, the resulting classifier is optimal in the Bayesian sense.\nRemark 4. If data in feature space comes from Normal distributions N (m±,Σ±) then β given by EEM minimizes probability of missclassification. More strictly speaking, if we draw h+ with probability 1/2 from N (m+,Σ+) and h− with 1/2 from N (m−,Σ−) then for any α ∈ Rh\np(∓∣βTh±) ≤ p(∓∣αTh±)\nT ab\nle 2:\nco m\np ar\nis on\nof co\nn si\nd er\ned cl\nas si\nif er\ns. ∣S V ∣\nd en\no te\ns n u\nm b\ner o f\nsu p\np o rt\nve ct\no rs\n. A\nst er\nix d\nen o te\ns fe\na tu\nre s\nw h\nic h\nca n\nb e\na d\nd e\nto a\np ar\nti cu\nla r\nm o d\nel b y\nso m\ne m\nin or\nm o d\nifi ca\nti on\ns, b\nu t\nw e\nco m\np a re\nh er\ne th\ne b\na se\nve rs\nio n\nd o f\nea ch\nm o d\nel .\nE L\nM S V\nM L\nS -S\nV M\nE E\n(K )M\nop ti\nm iz\nat io\nn m\net h\no d\nL in\nea r\nre gr\nes si\no n\nQ u\na d\nra ti\nc P\nro g ra\nm m\nin g\nL in\nea r\nS y st\nem F\nis ch\ner ’s\nD is\ncr im\nin a n t\nn on\nli n\nea ri\nty ra\nn d\nom p\nro je\nct io\nn ke\nrn el\nke rn\nel ra\nn d\no m\n(k er\nn el\n) p\nro je\nct io n cl os ed fo rm y es n o ye s ye s b al an ce d n o* n o * n o * ye s re gr es si on ye s n o * ye s n o cr it er io n M ea n S q u ar ed E rr o r H in g e lo ss M ea n S q u a re d E rr o r E n tr o p y o p ti m iz a ti o n\nn o.\nof th\nre sh\nol d\ns 1\n1 1\n1 o r 2 p ro b le m ty p e re gr es si on cl a ss ifi ca ti o n re g re ss io n cl a ss ifi ca\nti o n\nm o d\nel le\nar n\nin g\nd is\ncr im\nin at\niv e\nd is\ncr im\nin a ti\nve d\nis cr\nim in\na ti\nve g en\ner a ti ve d ir ec t p ro b ab il it y es ti m at es n o n o n o ye s tr ai n in g co m p le x it y O ( N h 2 ) O ( N 3 ) O ( N 2 .3 4 ) O ( N h 2 ) re su lt in g m o d el co m p le x it y h d ∣S V ∣d , ∣S V ∣ ≪ N N d + 1 h d + 4 m em or y re q u ir em en ts O ( N d ) O ( N d ) O ( N 2 ) O ( N d ) so u rc e of re gu la ri za ti on M o or eP en ro se p se u d o in ve rs e m a rg in m a x im iz a ti o n q u a d ra ti c lo ss p en a lt y te rm L ed o it -W o lf es ti\nm a to\nr"
    }, {
      "heading" : "5 Theory: learning capabilities",
      "text" : "First we show that under some simplifing assumptions, proposed method behaves as Extreme Learning Machine (or Weighted Extreme Learning Machine [25]).\nBefore proceeding further we would like to remark that there are two popular notations for projecting data onto hyperplanes. One, used in ELM model, assumes that H is a row matrix and β is a column vector, which results in the projection’s equation Hβ. Second one, used in SVM and in our paper, assumes that both H and β are column oriented, which results in the βTH projection. In the following theorem we will show some duality between β found by ELM and by EEM. In order to do so, we will need to change the notation during the proof, which will be indicated.\nTheorem 1. Let us assume that we are given an arbitrary, balanced4 dataset {(xi, ti)} N i=1, xi ∈ R d, ti ∈ {−1,+1}, ∣X −∣ = ∣X+∣ which can be perfectly learned by ELM with N hidden neurons. If this dataset’s points’ image through random neurons H = ϕ(X) is centered (points’ images have 0 mean) and classes have homogenous covariances (we can assume that ∃a±∈R+cov(H) = a+cov(H +) = a−cov(H −) then EEM with the same hidden layer will also learn this dataset perfectly (with 0 error).\nProof. In the first part of the proof we use the ELM notation. Projected data is centered, so cov(H) = HTH. ELM is able to learn this dataset perfectly, consequently H is invertible, thus also HTH is invertible, as a result cov†(H) = cov(H) = H TH. We will now show that ∃a∈R+βELM = a ⋅ βEEM. First, let us recall that βELM = H †T = H−1T and βEEM = 2(Σ++Σ−)−1(m+−m−) ∥m+−m−∥2\nΣ−+Σ+\nwhere Σ± = cov†(H ±). Due to the assumption of geometric homogenity βEEM =\n2 ∥m+−m−∥2\nΣ\n( a++a− a+a− Σ)−1(m+ −m−) , where Σ = cov†(H). Therefore\nβELM =H −1T\n= (HTH)−1HTT\n= cov† −1 (H)HTT\nFrom now we change the notation back to the one used in this paper.\nβELM = Σ −1\n( ∑ h+∈H+ (+1 ⋅ h+) + ∑ h−∈H− (−1 ⋅ h−))\n= Σ−1 ( ∑ h+∈H+ h+ − ∑ h−∈H− h−) = Σ−1 N\n2 (m+ −m−)\n= N\n2 ∥m+ −m−∥2Σ 2 a+ + a− a+a− βEEM\n= a ⋅βEEM,\nfor a = N 2 ∥m+−m−∥2Σ 2 a++a− a+a− ∈ R+. Again from homogenity we obtain just one equilibrium point, located in the βTEEM(m + −m−)/2 which results in the exact same classifier as the one given by ELM. This completes the proof.\n4analogous result can be shown for unbalanced dataset and Weighted ELM with particular weighting scheme.\nSimilar result holds for EEKM and Least Squares Support Vector Machine.\nTheorem 2. Let us assume that we are given arbitrary, balanced5 dataset {(xi, ti)} N i=1, xi ∈ R d, ti ∈ {−1,+1}, ∣X −∣ = ∣X+∣ which can be perfectly learned by LS-SVM. If dataset’s points’ images through Kernel induced projection ϕK have homogenous classes’ covariances (we can assume that ∃a±∈R+cov(ϕK(X)) = a+cov(ϕK(X +)) = a−cov(ϕK(X −)) then EEKM with the same kernel and N hidden neurons will also learn this dataset perfectly (with 0 error).\nProof. It is a direct consequence of the fact that with N hidden neurons and honogenous classes projections covariances, EEKM degenerates to the kernelized Fischer Discriminant which, as Gestel et al. showed [24], is equivalent to the solution of the Least Squares SVM."
    }, {
      "heading" : "6 Practical considerations",
      "text" : "We can formulate the whole EEM training as a very simple algorithm (see Alg. 1).\nAlgorithm 1 Extreme Entropy (Kernel) Machine\ntrain(X+,X−) build ϕ using Algorithm 2 H± ← ϕ(X±) m± ← 1/∣H±∣∑h±∈H± h ±\nΣ± ← cov†(H ±) β ← 2 (Σ+ +Σ−) −1 (m+ −m−)/∥m+ −m−∥Σ++Σ− F(x) = arg maxt∈{+,−}N (β Tmt,βTΣtβ)[x] return β, ϕ,F\npredict(X) return F(βTϕ(X))\nAlgorithm 2 ϕ building\nExtreme Entropy Machine(G, h) select randomly wi, bi for i ∈ {1, ..., h} ϕ(x) = [G(x,w1, b1), ...,G(x,wh, bh)] T\nreturn ϕ\nExtreme Entropy Kernel Machine(K, h,X) select randomly X[h] ⊂X, ∣X[h]∣ = h K[h] ←K(X[h],X[h])−1/2\nϕK(x) =K [h]K(X[h],x) return ϕK\n5analogous result can be shown for unbalanced dataset and Balanced LS-SVM with particular weighting scheme.\nResulting model consists of three elements:\n• feature projection function ϕ,\n• linear operator β,\n• classification rule F.\nAs described before, F can be further compressed to just one or two thresholds t± using equations from previous sections. Either way, complexity of the resulting model is linear in terms of hidden units and classification of the new point takes O(dh) time.\nDuring EEM training, the most expensive part of the algorithm is the computation of the covariance estimators and inversion of the sum of covariances. Even computation of the empirical covariance takes O(Nh2) time so the total complexity of training, equal to O(h3 + Nh2) = O(Nh2), is acceptable. It is worth noting that training of the ELM also takes exactly O(Nh2) time as it requires computation of HTH for H ∈ RN×h. Training of EEMK requires additional computation of the square root of the sampled kernel matrix inverse K(X[h],X[h])−1/2 but as K(X[h],X[h]) ∈ Rh×h can be computed in O(dh2) and both inverting and square rooting can be done in O(h3) we obtain exact same asymptotical computational complexity as the one of EEM. Procedure of square rooting and inverting are both always possible as assuming that K is a valid kernel in the Mercer’s sense yields that K(X[h],X[h]) is strictly positive definite and thus invertible. Further comparision of EEM, ELM and SVM is summarized in Table 2.\nNext aspect we would like to discuss is the cost sensitive learning. EEMs are balanced models in the sense that they are trying to maximize the balanced quality measures (like Averaged Accuracy or GMean). However, in practical applications it might be the case that we are actually more interested in the positive class then the negative one (like in the medical applications). Proposed model gives a direct probability estimates of p(βTh∣t), which we can easily convert to the cost sensitive classifier by introducing the prior probabilities of each class. Directly from Bayes Theorem, given p(+) and p(−), we can label our new sample h according to\np(t∣βTh)∝ p(t)p(βTh∣t),\nso if we are given costs C+,C− ∈ R+ we can use them as weighting of priors\ncl(x) = arg max t∈{−,+}\nCy C−+C+ p(βTh∣t).\nLet us now investigate the possible efficiency bottleneck. In EEKM, the classification of the new point h is based on\ncl(x) = F(βTϕK(x))\n= F(βT (K(x,X[h])K[h])T )\n= F(βT (K[h])TK(x,X[h])T )\n= F((K[h]β)TK(X[h],x)).\nOne can convert EEKM to the SLFN by putting:\nϕ̂K(x) =K(X [h],x)\nβ̂ =K[h]β,\nso the classification rule becomes\ncl(x) = F(β̂T ϕ̂K(x)).\nThis way complexity of the new point’s classification is exactly the same as in the case of EEM and ELM (or any other SLFN)."
    }, {
      "heading" : "7 Evaluation",
      "text" : "For the evaluation purposes we implemented five methods, namely: Weighted Extreme Learning Machine (WELM [25]), Extreme Entropy Machine (EEM), Extreme Entropy Kernel Machine (EEKM), Least Squares Support Vector Machines (LS-SVM [21]) and Support Vector Machines (SVM [4]).\nAll methods but SVM were implemented using Python with use of the bleeding-edge versions of numpy [23] and scipy [13] libraries included in anaconda6 for fair comparision. For SVM we used highly efficient libSVM [3] library with bindings avaliable in scikit-learn [17]. Random projection based methods (WELM, EEM) were tested using three following generalized activation functions G(x,w, b)\n• sigmoid (sig): 1 1+exp(−⟨w,x⟩+b) ,\n• normalized sigmoid (nsig): 1 1+exp(−⟨w,x⟩/d+b) ,\n• radial basis function (rbf): exp(−b∥w − x∥2).\nRandom parameters (weights and biases) were selected from uniform distributions on [0,1]. Training of WELM was performed using Moore-Penrose pseudoinverse and of EEM using Ledoit-Wolf covariance estimator, as both are parameter less, closed form estimators of required objects. For kernel methods (EEKM, LS-SVM, SVM) we used the Gaussian kernel (rbf) Kγ(xi,xj) = exp(−γ∥xi − xj∥\n2). In all methods requiring class balancing schemes (WELM, LS-SVM, SVM) we used balance weights wi equal to the ratio of bigger class and current class (so ∑ N i=1witi = 0).\nMetaparameters of each model were fitted, performed grid search included: hidden layer size h = 50,100,250,500,1000 (WELM, EEM, EEKM), Gaussian Kernel width γ = 10−10, . . . ,100 (EEKM, LS-SVM, SVM), SVM regularization parameter C = 10−1, . . . ,1010 (LS-SVM, SVM).\nDatasets’ features were linearly scaled in order to have each feature in the interval [0,1]. No other data whitening/filtering was performed. All experiments were performed in repeated 10-fold stratified cross-validation.\nWe use GMean7 (geometric mean of accuracy over positive and negative samples) as an evaluation metric. due to its balanced nature and usage in previous works regarding Weighted Extreme Learning Machines [25].\n6https://store.continuum.io/cshop/anaconda/ 7GMean(TP,FP,TN,FN) = √\nTP TP+FN ⋅ TN TN+FP ."
    }, {
      "heading" : "7.1 Basic UCI datasets",
      "text" : "We start our experiments with nine datasets coming from UCI repository [2], namely australian, breast-cancer, diabetes, german.numer, heart, ionosphere, liver-disorders, sonar and splice, summarized in the Table 3. This datasets include rather balanced, low dimensional problems.\nOn such data, EEM seems to perform noticably better than ELM when using RBF activation function (see Table 4), and rather similar when using sigmoid one – in such a scenario, for some datasets ELM achieves better results while for other EEM wins. Results obtained for EEKM are comparable with those obtained by LS-SVM and SVM, in both cases proposed method achieves better results on about third of problems, on the third it draws and on a third it loses. This experiments can be seen as a proof of concept of the whole methodology, showing that it can be truly a reasonable alternative for existing models in some problems. It appears that contrary to ELM, proposed methods (EEM and EEKM) achieve best scores across all considered models in some of the datasets regardless of the used activation function/kernel (only Support Vector Machines and their least squares counterpart are competetitive in this sense).\nT a b\nle 4 : G M e a n\no n U C I\nd a ta\nse ts\nW E\nL M\nsi g\nE E\nM si g\nW E\nL M\nn si g\nE E\nM n si g\nW E\nL M\nr b f\nE E\nM r b f\nL S\n-S V\nM r b f\nE E\nK M\nr b f\nS V\nM r b f\na u st\nr a l ia n\n8 6 .3 ± 4 .5\n8 7 .0\n± 4 .0\n8 5 .9 ± 4 .4\n8 6 .5 ± 3 .2\n8 5 .8 ± 4 .9\n8 6 .9 ± 4 .4\n8 6 .9 ± 4 .1\n8 6 .8 ± 3 .8\n8 6 .8 ± 3 .7\nb r e a st\n-c a n c e r\n9 6 .9 ± 1 .7\n9 7 .3 ± 1 .2\n9 7 .6 ± 1 .5\n9 7 .4 ± 1 .2\n9 6 .6 ± 1 .8\n9 7 .3 ± 1 .1\n9 7 .6 ± 1 .3\n9 7 .8\n± 1 .1\n9 6 .8 ± 1 .7\nd ia b e t e s\n7 4 .2 ± 4 .6\n7 4 .5 ± 4 .6\n7 4 .1 ± 5 .5\n7 4 .9 ± 5 .0\n7 3 .2 ± 5 .6\n7 4 .9 ± 5 .9\n7 5 .5 ± 5 .6\n7 5 .7\n± 5 .6\n7 4 .8 ± 3 .5\ng e r m a n\n6 8 .8 ± 6 .9\n7 1 .3 ± 4 .1\n7 0 .7 ± 6 .1\n7 2 .4 ± 5 .4\n7 1 .1 ± 6 .1\n7 2 .2 ± 5 .7\n7 3 .2 ± 4 .5\n7 2 .9 ± 5 .3\n7 3 .4\n± 5 .4\nh e a r t\n7 8 .8 ± 6 .3\n8 2 .5 ± 7 .4\n7 8 .1 ± 7 .0\n8 3 .7 ± 7 .2\n8 0 .2 ± 8 .9\n8 1 .9 ± 6 .9\n8 3 .7 ± 8 .5\n8 3 .6 ± 7 .5\n8 4 .6\n± 7 .0\nio n o sp\nh e r e\n7 1 .5 ± 9 .5\n7 7 .0 ± 1 2 .8\n8 2 .7 ± 7 .8\n8 4 .6 ± 9 .1\n8 5 .6 ± 8 .4\n9 0 .8 ± 5 .2\n9 1 .2 ± 5 .5\n9 3 .4 ± 4 .3\n9 4 .7\n± 3 .9\nl iv e r -d\nis o r d e r s\n6 8 .1 ± 8 .0\n6 8 .6 ± 8 .9\n6 6 .3 ± 8 .2\n6 2 .1 ± 8 .1\n6 7 .2 ± 5 .9\n7 1 .4 ± 7 .0\n7 1 .1 ± 8 .3\n7 0 .2 ± 6 .9\n7 2 .3\n± 6 .2\nso n a r\n6 6 .7 ± 1 0 .1\n7 0 .1 ± 1 1 .5\n8 0 .2 ± 7 .4\n7 8 .3 ± 1 1 .2\n8 3 .2 ± 6 .9\n8 2 .8 ± 5 .2\n8 6 .5 ± 5 .4\n8 7 .0\n± 7 .5\n8 3 .0 ± 7 .1\nsp l ic e\n6 4 .7 ± 2 .8\n4 9 .4 ± 5 .5\n8 1 .8 ± 3 .2\n8 0 .9 ± 2 .7\n7 5 .5 ± 3 .9\n8 2 .2 ± 3 .5\n8 9 .9\n± 3 .0\n8 8 .0 ± 4 .0\n8 8 .0 ± 2 .2"
    }, {
      "heading" : "7.2 Highly unbalanced datasets",
      "text" : "In the second part we proceeded to the nine highly unbalanced datasets, summarized in the second part of the Table 3. Ratio between bigger and smaller class varies from 10 ∶ 1 to even 20 ∶ 1 which makes them really hard for unbalanced models. Obtained results (see Table 5) resembles these obtained on UCI repository. We can see better results in about half of experiments if we fix a particular activation function/kernel (so we compare ELMx with EEMx and LS-SVMx with EEKMx). Table 6 shows that training time of Extreme Entropy Machines are comparable with the ones obtained by Extreme Learning Machines (differences on the level of 0.1−0.2 are not significant on such datasets’ sizes). We have a robust method which learns in below two seconds a model for hundreads/thousands of examples. For larger datasets (like abalone7 or sick euthyroid) proposed methods not only outperform SVM and LS-SVM in terms of robustness but there is also noticable difference between their training times and ELMs. This suggests that even though ELM and EEM are quite similar and on small datasets are equally fast, EEM can better scale up to truly big datasets. Obviously obtained training times do not resemble the full training time as it strongly depends on the technique used for metaparameters selection and resolution of grid search (or other parameters tuning technique). In such full scenario, training times of SVM related models is significantly bigger due to the requirment of exact tuning of both C and γ in real domains."
    }, {
      "heading" : "7.3 Extremely unbalanced datasets",
      "text" : "Third part of experiments consists of extremely unbalanced datasets (with class imbalance up to 100:1) containing tens and hundreads thousands of examples. Five analyzed datasets span from NLP tasks (webpages) through medical applications (mammography) to bioinformatics (protein homology). This type of datasets often occur in the true data mining which makes these results much more practical than the one obtained on small/balanced data.\n0.0 scores on Isolet dataset (see Table 7) for sigmoid based random projections is a result of very high values (∼ 200) of ⟨x,w⟩ for all x, which results in G(x,w, b) = 1, so the whole dataset is reduced to the singleton {[1, . . . ,1]T } ⊂ Rh ⊂H which obviously is not separable by any classifier, netither ELM nor EEM.\nFor other activation functions we see that EEM achieves sllightly worse results than ELM. On the other hand, scores of EEKM generally outperform the ones obtained by ELM and are very close to the ones obtained by well tuned SVM and LS-SVM. In the same time, EEM and EEKM were trained significantly faster, as Table 8 shows, it was order of magnitude faster than SVM related models and even 1.5 − 2× faster than ELM. It seems that the LedoitWolf covariance estimation computation with this matrices inversion is simply a faster operation (scales better) than computation of the Moore-Penrose pseudoinverse of the HTH. Obviously one can alternate ELM training routine to the regularized one where instead of (HTH)† one computes (HTH + I/C)−1, but we are analyzing here parameter less approaches, while the analogous could be used for EEM in the form of (cov(X−)+cov(X+)+I/C)−1 instead of computing Ledoit-Wolf estimator. In other words, in the parameter less training scenario, as described in this paper EEMs seems to scale better than ELMs while still\nT a b\nle 5 :\nH ig\nh ly\nu n b\na la\nn ce\nd d\na ta\nse ts\nW E\nL M\nsi g\nE E\nM si g\nW E\nL M\nn si g\nE E\nM n si g\nW E\nL M\nr b f\nE E\nM r b f\nL S\n-S V\nM r b f\nE E\nK M\nr b f\nS V\nM r b f\na b a l o n e 7\n7 9 .7 ± 2 .3\n7 9 .8 ± 3 .5\n8 0 .0 ± 2 .8\n7 6 .1 ± 3 .7\n8 0 .1 ± 3 .2\n7 9 .7 ± 3 .6\n8 0 .2\n± 3 .4\n7 9 .9 ± 3 .4\n7 9 .7 ± 2 .7\na r y t h m ia\n2 8 .3 ± 3 5 .4\n4 0 .3 ± 2 0 .9\n6 4 .2 ± 2 4 .6\n8 5 .6\n± 1 0 .3\n6 6 .9 ± 2 5 .8\n7 9 .4 ± 1 2 .5\n8 4 .4 ± 1 0 .0\n8 5 .2 ± 1 0 .6\n8 0 .9 ± 1 1 .8\nc a r e v a l u a t io n\n9 9 .1 ± 0 .3\n9 8 .9 ± 0 .4\n9 9 .0 ± 0 .3\n9 7 .9 ± 0 .6\n9 9 .0 ± 0 .3\n9 8 .5 ± 0 .3\n9 9 .5 ± 0 .2\n9 9 .2 ± 0 .3\n1 0 0 .0\n± 0 .0\ne c o l i\n8 6 .9 ± 6 .5\n8 8 .3 ± 7 .1\n8 6 .9 ± 6 .8\n8 8 .6 ± 6 .9\n8 6 .4 ± 7 .0\n8 8 .8 ± 7 .2\n8 9 .2 ± 6 .3\n8 9 .4\n± 6 .9\n8 8 .5 ± 6 .2\nl ib r a s m o v e\n6 5 .5 ± 1 0 .7\n1 9 .3 ± 8 .1\n8 2 .5 ± 1 2 .0\n9 3 .0 ± 1 1 .8\n8 9 .6 ± 1 1 .9\n9 3 .9 ± 1 1 .9\n9 6 .5 ± 8 .6\n9 6 .6\n± 8 .7\n9 1 .6 ± 1 1 .9\no il\nsp il l\n8 6 .0 ± 6 .9\n8 8 .8\n± 6 .5\n8 3 .8 ± 7 .6\n8 4 .7 ± 8 .7\n8 5 .8 ± 9 .3\n8 8 .1 ± 6 .1\n8 6 .7 ± 8 .4\n8 7 .2 ± 4 .9\n8 5 .7 ± 1 1 .4\nsi c k\ne u t h y r o id\n8 8 .1 ± 1 .7\n8 7 .9 ± 2 .4\n8 8 .5 ± 2 .1\n8 1 .7 ± 2 .7\n8 9 .1 ± 1 .9\n8 8 .2 ± 2 .4\n8 9 .5 ± 1 .7\n8 9 .3 ± 1 .9\n9 0 .9\n± 2 .0\nso l a r f l a r e\n6 0 .4 ± 1 6 .8\n6 3 .7 ± 1 2 .9\n6 1 .3 ± 1 0 .8\n6 7 .4 ± 9 .0\n6 0 .3 ± 1 4 .8\n6 8 .9 ± 9 .3\n6 7 .3 ± 8 .8\n6 7 .3 ± 9 .0\n7 0 .9\n± 8 .5\nsp e c t r o m e t e r\n8 2 .9 ± 1 3 .0\n8 7 .3 ± 7 .8\n8 8 .0 ± 1 0 .8\n9 0 .2 ± 8 .6\n8 6 .6 ± 8 .2\n9 3 .0 ± 1 4 .6\n9 4 .6 ± 8 .4\n9 3 .5 ± 1 4 .7\n9 5 .4\n± 5 .1\nT ab\nle 6 :\nh ig\nh ly\nu n b\na la\nn ce\nd d\na ta\nse ts\nti m\nes\nW E\nL M\nsi g\nE E\nM si g\nW E\nL M\nn si g\nE E\nM n si g\nW E\nL M\nr b f\nE E\nM r b f\nL S\n-S V\nM r b f\nE E\nK M\nr b f\nS V\nM r b f\na b a l o n e 7\n1 .9\ns 1 .2\ns 2 .5\ns 1 .6\ns 1 .8\ns 1 .2\ns 2 0 .8\ns 1 .9\ns 4 .7\ns\na r y t h m ia\n0 .2\ns 0 .7\ns 0 .3\ns 0 .9\ns 0 .3\ns 0 .7\ns 0 .1\ns 0 .3\ns 0 .1\ns\nc a r e v a l u a t io n\n1 .3\ns 0 .9\ns 1 .5\ns 1 .0\ns 1 .1\ns 0 .9\ns 2 .0\ns 1 .4\ns 0 .1\ns\ne c o l i\n0 .2\ns 0 .8\ns 0 .2\ns 0 .8\ns 0 .1\ns 0 .7\ns 0 .0\ns 0 .1\ns 0 .2\ns\nl ib r a s m o v e\n0 .2\ns 0 .9\ns 0 .2\ns 0 .8\ns 0 .1\ns 0 .7\ns 0 .0\ns 0 .1\ns 0 .0\ns\no il\nsp il l\n0 .7\ns 0 .8\ns 0 .6\ns 0 .8\ns 0 .6\ns 0 .8\ns 0 .4\ns 0 .9\ns 0 .1\ns\nsi c k\ne u t h y r o id\n1 .5\ns 1 .1\ns 1 .4\ns 1 .1\ns 1 .5\ns 1 .1\ns 9 .6\ns 1 .7\ns 2 1 .0\ns\nso l a r f l a r e\n0 .7\ns 0 .8\ns 0 .7\ns 0 .8\ns 0 .8\ns 0 .8\ns 1 .1\ns 1 .3\ns 1 6 .1\ns\nsp e c t r o m e t e r\n0 .2\ns 0 .7\ns 0 .3\ns 0 .7\ns 0 .2\ns 0 .7\ns 0 .1\ns 0 .3\ns 0 .0\ns\nobtaining similar classification results. In the same time EEKM obtains SVMlevel results with orders of magnitude smaller training times. Both ELM and EEM could be transformed into regularization parameter based learning, but this is beyond the scope of this work."
    }, {
      "heading" : "7.4 Entropy based hyperparameter optimization",
      "text" : "Now we proceed to entropy based evaluation. Given particular set of linear hypotheses M in H we want to select optimal set of hyperparameters θ (such as number of hidden neurons or regularization parameter) which identify a particular model βθ ∈M ⊂ H. Instead of using expensive internal cross-validation (or other generalization error estimation technique like Err0.632) we select such θ which maximizes our entropic measure. In particular we consider a simpified Cauchy-Schwarz Divergence based strategy where we select θ maximizing\nDcs(N (β T θm +,var(βTθH + )),N (βTθm −,var(βTθH − ))),\nand kernel density based entropic strategy [6] selecting θ maximizing\nDcs(⟦β T θH + ⟧, ⟦βTθH − ⟧), (5)\nwhere ⟦A⟧ = ⟦A⟧σ(A) is a Gaussian KDE using Silverman’s rule of the window width [19]\nσ(A) = ( 4 3∣A∣\n) 1/5\nstd(A) ≈ 1.065√∣A∣ std(A).\nThis way we can use whole given set for training and do not need to repeat the process, as Dcs is computed on the training set instead of the hold-out set.\nFirst, one can notice on Table 9 and Table 10 that such entropic criterion works well for EEM, EEKM and Support Vector Machines. On the other hand, it is not very well suited for ELM models. This confirms conclusions from our previous work on classification using Dcs [6] where we claimed that SVMs are conceptually similar in terms of optimization objective, as well as widens it to the new class of models (EEMs). Second, Table 9 shows that EEM and EEKM can truly select their hyperparameters using very simple technique requiring no model retrainings. Computation of (5) is linear in terms of training set and constant time if performed using precomputed projections of required objects (which are either way computed during EEM training). This make this very fast model even more robust."
    }, {
      "heading" : "7.5 EEM stability",
      "text" : "It was previously reported [11] that ELMs have very stable results in the wide range of the number of hidden neurons. We performed analogous experiments with EEM on UCI datasets. We trained models for 100 increasing hidden layers sizes (h = 5,10, . . . ,500) and plotted resulting GMean scores on Fig. 3.\nT a b\nle 7 :\nB ig\nh ig\nh ly\nu n b\na la\nn ce\nd d\na ta\nse ts\nW E\nL M\nsi g\nE E\nM si g\nW E\nL M\nn si g\nE E\nM n si g\nW E\nL M\nr b f\nE E\nM r b f\nL S\n-S V\nM r b f\nE E\nK M\nr b f\nS V\nM r b f\nf o r e st\nc o v e r\n9 0 .8 ± 0 .3\n9 0 .5 ± 0 .3\n9 0 .7 ± 0 .3\n8 5 .1 ± 0 .4\n9 0 .9 ± 0 .3\n8 7 .1 ± 0 .0\n- 9 1 .8\n± 0 .3\n-\nis o l e t\n0 .0 ± 0 .0\n0 .0 ± 0 .0\n9 6 .3 ± 0 .7\n9 5 .6 ± 1 .1\n9 3 .0 ± 0 .9\n9 1 .4 ± 1 .0\n9 8 .0\n± 0 .7\n9 7 .4 ± 0 .6\n9 7 .6 ± 0 .6\nm a m m o g r a p h y\n9 0 .4 ± 2 .8\n8 9 .0 ± 3 .2\n9 0 .7 ± 3 .3\n8 7 .2 ± 3 .0\n8 9 .9 ± 3 .8\n8 9 .5 ± 3 .1\n9 1 .0\n± 3 .1\n8 9 .5 ± 3 .1\n8 9 .8 ± 3 .8\np r o t e in\nh o m o l o g y\n9 5 .3 ± 0 .8\n9 4 .9 ± 0 .8\n9 5 .1 ± 0 .9\n9 4 .2 ± 1 .3\n9 5 .0 ± 1 .0\n9 5 .1 ± 1 .1\n- 9 5 .7\n± 0 .9\n-\nw e b pa\ng e s\n7 2 .0 ± 0 .0\n7 3 .1 ± 2 .0\n9 3 .0 ± 1 .8\n9 3 .1\n± 1 .7\n8 6 .7 ± 0 .0\n8 4 .4 ± 1 .6\n- 9 3 .1\n± 1 .7\n9 3 .1\n± 1 .7\nT ab\nle 8 :\nB ig\nh ig\nh ly\nu n b\na la\nn ce\nd d\na ta\nse ts\nti m\nes\nW E\nL M\nsi g\nE E\nM si g\nW E\nL M\nn si g\nE E\nM n si g\nW E\nL M\nr b f\nE E\nM r b f\nL S\n-S V\nM r b f\nE E\nK M\nr b f\nS V\nM r b f\nf o r e st\nc o v e r\n1 1 0 .7\ns 1 0 4 .6\ns 1 4 4 .9\ns 4 5 .6\ns 1 1 1 .3\ns 3 8 .2\ns > 6 0 0\ns 1 0 7 .4\ns > 6 0 0\ns\nis o l e t\n9 .7\ns 4 .5\ns 4 .9\ns 3 .0\ns 3 .4\ns 2 .1\ns 1 2 6 .9\ns 3 .2\ns 5 3 .5\ns\nm a m m o g r a p h y\n4 .0\ns 2 .2\ns 6 .1\ns 3 .0\ns 4 .0\ns 2 .2\ns 3 2 7 .3\ns 3 .3\ns 9 .5\ns\np r o t e in\nh o m o l o g y\n2 7 .6\ns 2 1 .6\ns 8 6 .3\ns 2 7 .9\ns 6 2 .5\ns 2 2 .0\ns > 6 0 0\ns 3 0 .7\ns > 6 0 0\ns\nw e b pa\ng e s\n1 6 .0\ns 6 .2\ns 1 4 .5\ns 8 .5\ns 7 .1\ns 6 .4\ns > 6 0 0\ns 9 .0\ns 2 1 7 .0\ns\nT ab\nle 9:\nU C I\nd at\nas et\ns G M e a n\nw it\nh p\na ra\nm et\ner s\ntu n in\ng b\na se\nd on\nse le\nct in\ng a\nm o d\nel a cc\no rd\nin g to D c s ( N ( β T m + ,β T Σ + β ) ,N ( β T m − ,β T Σ − β ) ) w h er e β is a li n ea r o p er a to r fo u n d b y a p a rt ic u la r o p ti m iz a ti o n p ro ce d u re in st ea d o f in te rn al cr os s va li d at io n W E L M si g E E M si g W E L M n si g E E M n si g W E L M r b f E E M r b f L S -S V M r b f E E K M r b f S V M r b f a u st r a l ia n 5 1 .2 ± 7 .5 8 6 .3 ± 4 .8 5 0 .3 ± 6 .4 8 6 .5 ± 3 .2 5 0 .3 ± 8 .5 8 6 .2 ± 5 .3 5 8 .5 ± 7 .9 8 5 .2 ± 5 .6 8 5 .7 ± 4 .7 b r e a st -c a n c e r 8 3 .0 ± 4 .3 9 7 .0 ± 1 .6 7 2 .0 ± 6 .6 9 7 .1 ± 1 .9 7 7 .3 ± 5 .3 9 7 .3 ± 1 .1 7 9 .2 ± 7 .7 9 6 .9 ± 1 .4 9 7 .5 ± 1 .2 d ia b e t e s 5 2 .3 ± 4 .7 7 4 .4 ± 4 .0 5 1 .7 ± 4 .0 7 4 .7 ± 5 .2 5 2 .1 ± 3 .7 7 3 .5 ± 5 .9 6 0 .1 ± 4 .2 7 2 .2 ± 5 .4 7 3 .2 ± 5 .9 g e r m a n 5 7 .1 ± 4 .0 6 9 .3 ± 5 .0 5 1 .7 ± 3 .0 7 2 .4 ± 5 .4 5 2 .8 ± 6 .3 7 0 .9 ± 6 .9 5 5 .0 ± 4 .3 6 7 .8 ± 5 .7 6 0 .5 ± 4 .5 h e a r t 6 8 .6 ± 5 .8 7 9 .4 ± 6 .9 6 5 .6 ± 5 .9 8 2 .9 ± 7 .4 6 0 .3 ± 9 .4 7 7 .4 ± 7 .2 6 6 .2 ± 4 .2 7 7 .7 ± 7 .0 7 6 .5 ± 6 .6 io n o sp h e r e 6 2 .7 ± 1 0 .6 7 7 .0 ± 1 2 .8 6 8 .5 ± 5 .1 8 4 .6 ± 9 .1 6 9 .5 ± 9 .6 9 0 .8 ± 5 .2 7 2 .8 ± 6 .1 9 3 .4 ± 4 .2 9 4 .7 ± 3 .9 l iv e r -d is o r d e r s 5 3 .2 ± 7 .0 6 8 .5 ± 6 .7 5 2 .2 ± 1 1 .8 6 2 .1 ± 8 .1 5 3 .9 ± 8 .0 7 1 .4 ± 7 .0 6 2 .9 ± 7 .8 6 9 .6 ± 8 .2 6 6 .9 ± 8 .0 so n a r 6 6 .3 ± 6 .1 6 6 .1 ± 1 5 .0 8 0 .2 ± 7 .4 7 6 .9 ± 5 .2 8 3 .2 ± 6 .9 8 2 .8 ± 5 .2 8 5 .9 ± 4 .9 8 7 .7 ± 6 .1 8 6 .6 ± 3 .3 sp l ic e 5 1 .8 ± 4 .3 4 9 .4 ± 5 .5 6 4 .9 ± 3 .1 8 0 .2 ± 2 .6 6 0 .8 ± 3 .5 8 2 .2 ± 3 .5 8 9 .7 ± 3 .3 8 8 .0 ± 4 .0 8 9 .5 ± 2 .9\nT ab\nle 10\n: U C I\nd at\nas et\ns G M e a n\nw it\nh p\nar am\net er\ns tu\nn in\ng b\na se\nd o n\nse le\nct in\ng a\nm o d\nel a cc\no rd\nin g\nto D\nc s ( ⟦ β T H + ⟧ ,⟦ β T H − ⟧ )\nw h\ner e β\nis a\nli n\nea r\nop er\nat or\nfo u\nn d\nb y\na p\nar ti\ncu la\nr op\nti m\niz at\nio n\np ro\nce d\nu re\nin st\nea d\no f\nin te\nrn a l\ncr o ss\nva li\nd a ti\no n\nW E\nL M\nsi g\nE E\nM si g\nW E\nL M\nn si g\nE E\nM n si g\nW E\nL M\nr b f\nE E\nM r b f\nL S\n-S V\nM r b f\nE E\nK M\nr b f\nS V\nM r b f\na u st\nr a l ia n\n5 1 .2 ± 7 .5\n8 6 .3 ± 4 .8\n5 0 .3 ± 6 .4\n8 6 .5\n± 3 .2\n5 0 .3 ± 8 .5\n8 6 .2 ± 5 .3\n5 8 .5 ± 7 .9\n8 5 .2 ± 5 .6\n8 4 .2 ± 4 .1\nb r e a st\n-c a n c e r\n8 3 .0 ± 4 .3\n9 7 .0 ± 1 .6\n7 2 .0 ± 6 .6\n9 7 .4\n± 1 .2\n7 7 .3 ± 5 .3\n9 7 .3 ± 1 .1\n7 9 .3 ± 7 .1\n9 6 .9 ± 1 .4\n9 6 .3 ± 2 .4\nd ia b e t e s\n5 2 .3 ± 4 .7\n7 4 .4 ± 4 .0\n5 1 .7 ± 4 .0\n7 4 .7\n± 5 .2\n5 2 .1 ± 3 .7\n7 3 .5 ± 5 .9\n6 0 .1 ± 4 .2\n7 2 .2 ± 5 .4\n7 1 .9 ± 5 .4\ng e r m a n\n5 7 .1 ± 4 .0\n6 9 .3 ± 5 .0\n5 1 .7 ± 3 .0\n7 1 .7\n± 5 .9\n5 2 .8 ± 6 .3\n7 0 .9 ± 6 .9\n5 4 .4 ± 5 .7\n6 7 .8 ± 5 .7\n5 9 .5 ± 4 .2\nh e a r t\n6 0 .0 ± 9 .2\n7 9 .4 ± 6 .9\n6 5 .6 ± 5 .9\n8 2 .9\n± 7 .4\n5 2 .6 ± 9 .0\n7 7 .4 ± 7 .2\n6 1 .9 ± 5 .8\n7 7 .7 ± 7 .0\n7 6 .3 ± 7 .7\nio n o sp\nh e r e\n6 2 .4 ± 8 .1\n7 7 .0 ± 1 2 .8\n6 8 .5 ± 5 .1\n8 4 .6 ± 9 .1\n6 7 .6 ± 9 .8\n9 0 .8 ± 5 .2\n6 7 .0 ± 1 0 .7\n9 3 .4\n± 4 .2\n9 2 .3 ± 4 .6\nl iv e r -d\nis o r d e r s\n5 0 .9 ± 1 1 .5\n6 8 .5 ± 6 .7\n5 0 .4 ± 9 .2\n6 2 .1 ± 8 .1\n5 3 .9 ± 8 .0\n7 1 .4\n± 7 .0\n6 2 .9 ± 7 .8\n6 9 .6 ± 8 .2\n6 6 .9 ± 8 .0\nso n a r\n6 6 .3 ± 6 .1\n6 6 .1 ± 1 5 .0\n8 0 .2 ± 7 .4\n7 6 .9 ± 5 .2\n6 2 .9 ± 9 .4\n8 2 .8 ± 5 .2\n8 3 .6 ± 4 .5\n8 7 .7\n± 6 .1\n8 6 .6 ± 3 .3\nsp l ic e\n5 1 .8 ± 4 .3\n3 3 .1 ± 6 .5\n6 4 .9 ± 3 .1\n8 0 .2 ± 2 .6\n6 0 .8 ± 3 .5\n8 2 .2 ± 3 .5\n8 5 .4 ± 4 .1\n8 8 .0 ± 4 .0\n8 9 .5\n± 2 .9\nOne can notice that similarly to ELM proposed methods are very stable. Once machine gets enough neurons (around 100 in case of tested datasets) further increasing of the feature space dimension has minor effect on the generalization capabilities of the model. It is also important that some of these datasets (like sonar) do not even have 500 points, so there are more dimensions in the Hilbert space than we have points to build our covariance estimates, and even though we still do not observe any rapid overfitting."
    }, {
      "heading" : "8 Conclusions",
      "text" : "In this paper we have presented Extreme Entropy Machines, models derived from the information theoretic measures and applied to the classification problems. Proposed methods are strongly related to the concepts of Extreme Learning Machines (in terms of general workflow, rapid training and randomization) as well as Support Vector Machines (in terms of margin maximization interpretation as well as LS-SVM duality).\nMain characteristics of EEMs are:\n• information theoretic background based on differential and Renyi’s quadratic entropies,\n• closed form solution of the optimization problem,\n• generative training, leading to direct probability estimates,\n• small number of metaparameters,\n• good classification results,\n• rapid training that scales well to hundreads of thousands of examples and beyond,\n• theoretical and practical similarities to the large margin classifiers and Fischer Discriminant.\nPerformed evaluation showed that, similarly to ELM, proposed EEM is a very stable model in terms of the size of the hidden layer and achieves comparable classification results to the ones obtained by SVMs and ELMs. Furthermore we showed that our method scales better to truly big datasets (consisting of hundreads of thousands of examples) without sacrificing results quality.\nDuring our considerations we pointed out some open problems and issues, which are worth investigation:\n• Can one construct a closed-form entropy based classifier with different distribution families than Gaussians?\n• Is there a theoretical justification of the stability of the extreme learning techniques?\n• Is it possible to further increase achieved results by performing unsupervised entropy based optimization in the hidden layer?"
    }, {
      "heading" : "Acknowledgment",
      "text" : "We would like to thank Daniel Wilczak from Institute of Computer Science and Computational Mathematics of Jagiellonian Univeristy for the access to the Fermi supercomputer, which made numerous experiments possible."
    } ],
    "references" : [ {
      "title" : "Learning multivalued multithreshold functions",
      "author" : [ "Martin Anthony" ],
      "venue" : "Citeseer,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Libsvm: a library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Cluster based RBF Kernel for Support Vector Machines",
      "author" : [ "W.M. Czarnecki", "J. Tabor" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Multithreshold Entropy Linear Classifier",
      "author" : [ "W.M. Czarnecki", "J. Tabor" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Two ellipsoid Support Vector Machines",
      "author" : [ "Wojciech Marian Czarnecki", "Jacek Tabor" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm",
      "author" : [ "Arthur P Dempster", "Nan M Laird", "Donald B Rubin" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1977
    }, {
      "title" : "On the Nyström method for approximating a Gram matrix for improved kernel-based learning",
      "author" : [ "Petros Drineas", "Michael W Mahoney" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "Extreme learning machine: a new learning scheme of feedforward neural networks",
      "author" : [ "Guang-Bin Huang", "Qin-Yu Zhu", "Chee-Kheong Siew" ],
      "venue" : "In Neural Networks,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Extreme learning machine: theory and applications",
      "author" : [ "Guang-Bin Huang", "Qin-Yu Zhu", "Chee-Kheong Siew" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "The cauchy–schwarz divergence and parzen windowing: Connections to graph theory and mercer kernels",
      "author" : [ "Robert Jenssen", "Jose C Principe", "Deniz Erdogmus", "Torbjørn Eltoft" ],
      "venue" : "Journal of the Franklin Institute,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Scipy: Open source scientific tools for python",
      "author" : [ "Eric Jones", "Travis Oliphant", "Pearu Peterson" ],
      "venue" : "http://www. scipy. org/,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2001
    }, {
      "title" : "Learning pattern classification-a survey",
      "author" : [ "Sanjeev R Kulkarni", "Gábor Lugosi", "Santosh S. Venkatesh" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "A well-conditioned estimator for large-dimensional covariance matrices",
      "author" : [ "Olivier Ledoit", "Michael Wolf" ],
      "venue" : "Journal of multivariate analysis,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "On estimation of a probability density function and mode. The annals of mathematical statistics, pages",
      "author" : [ "Emanuel Parzen" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1962
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Information theoretic learning",
      "author" : [ "Jose C Principe" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2000
    }, {
      "title" : "Density estimation for statistics and data analysis, volume 26",
      "author" : [ "Bernard W Silverman" ],
      "venue" : "CRC press,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1986
    }, {
      "title" : "Weighted least squares support vector machines: robustness and sparse approximation",
      "author" : [ "Johan AK Suykens", "Jos De Brabanter", "Lukas Lukas", "Joos Vandewalle" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "Least squares support vector machine classifiers",
      "author" : [ "Johan AK Suykens", "Joos Vandewalle" ],
      "venue" : "Neural processing letters,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1999
    }, {
      "title" : "The numpy array: a structure for efficient numerical computation",
      "author" : [ "Stefan Van Der Walt", "S Chris Colbert", "Gael Varoquaux" ],
      "venue" : "Computing in Science & Engineering,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Benchmarking least squares support vector machine classifiers",
      "author" : [ "Tony Van Gestel", "Johan AK Suykens", "Bart Baesens", "Stijn Viaene", "Jan Vanthienen", "Guido Dedene", "Bart De Moor", "Joos Vandewalle" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2004
    }, {
      "title" : "Weighted extreme learning machine for imbalance",
      "author" : [ "Weiwei Zong", "Guang-Bin Huang", "Yiqiang Chen" ],
      "venue" : "learning. Neurocomputing,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Even for linear classifiers one can find multiple objective functions, error measures to minimize, regularization methods to include [14].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "Together with a powerful extreme data transformation we obtain a robust, nonlinear model competetive with the state of the art models not based on information theory like Support Vector Machines (SVM [4]), Extreme Learning Machines (ELM [10]) or Least Squares Support Vector Machines (LS-SVM [21]).",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 18,
      "context" : "Together with a powerful extreme data transformation we obtain a robust, nonlinear model competetive with the state of the art models not based on information theory like Support Vector Machines (SVM [4]), Extreme Learning Machines (ELM [10]) or Least Squares Support Vector Machines (LS-SVM [21]).",
      "startOffset" : 292,
      "endOffset" : 296
    }, {
      "referenceID" : 8,
      "context" : "Let us begin with recalling some basic information regarding Extreme Learning Machines [11] and Support Vector Machines [4] which are further used as a competiting models for proposed solution.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "[10] which are based on the idea that single layer feed forward neural networks (SLFN) can be trained without iterative process by performing linear regression on the data mapped through random, nonlinear projection (random hidden neurons).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "If we denote the weights between hidden layer and output neurons by β it is easy to show [11] that putting β =H†T, gives the best solution in terms of mean squared error of the regression:",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 18,
      "context" : "showed [21] this can be further generalized for abitrary kernel induced spaces, where we classify according to:",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 17,
      "context" : "To overcome this issue one can introduce a weighted version [20], where given diagonal matrix of weights Q, such that Qii is invertibly proportional to the size of xi’s class and .",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "We focus on the last group of approaches, and investigate the applicability of the Cauchy-Schwarz divergence [12], which for two densities f and g is given by",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "Cauchy-Schwarz divergence is connected to Renyi’s quadratic cross entropy (H 2 [18]) and Renyi’s quadratic entropy (H2), defined for densities f, g as H 2 (f, g) = − ln(∫ fg) H2(f) =H × 2 (f, f) = − ln(∫ f 2 ) ,",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "and as we showed in [6], it is well-suited as a discrimination measure which allows the construction of mulit-threshold linear classifiers.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "In order to fit a Gaussian Mixture Model (GMM) to given dataset one needs algorithm like Expectation Maximization [8] or conceptually similar Cross-Entropy Clustering [22].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "In this case to regularize the covariance we apply the well-known Ledoit-Wolf approach [15].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "Σ = cov†(H ± ) = (1 − ε)cov(H) + εtr(cov(H))hI, where cov(⋅) is an empirical covariance and ε is a shrinkage coefficient given by Ledoit and Wolf [15].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "(2) Since, as one can easily compute [5],",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "• for Extreme Entropy Kernel Machine (EEKM) we use the randomized kernel approximation technique [9], which spans our Hilbert space on randomly selecteed subset of training vectors.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "It was previously shown in Two ellipsoid Support Vector Machines model [7] that such norm is an approximation of the margin coming from two ellpisoids instead of the single ball used by traditional SVM.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "Similar observation regarding connection between large margin classification and entropy optimization has been done in case of the Multithreshold Linear Entropy Classifier [6].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "which makes the resulting classifier a member of two-thresholds linear classifiers family [1].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "As a result, classical kernel density estimation (KDE) is not reliable source of information [16].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "Proof of this remark is given by Czarnecki and Tabor [6] and means that if we perform a Gaussian kernel density estimation of our data with big kernel width (which is reasonable for small amount of data in highly dimensional space) then for big enough σ̂ EEM is nearly optimal linear classifier in terms of estimated densities f̂ = N (m, σ̂Σ) ≈ ⟦H⟧σ̂.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 21,
      "context" : "First we show that under some simplifing assumptions, proposed method behaves as Extreme Learning Machine (or Weighted Extreme Learning Machine [25]).",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 20,
      "context" : "showed [24], is equivalent to the solution of the Least Squares SVM.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 21,
      "context" : "For the evaluation purposes we implemented five methods, namely: Weighted Extreme Learning Machine (WELM [25]), Extreme Entropy Machine (EEM), Extreme Entropy Kernel Machine (EEKM), Least Squares Support Vector Machines (LS-SVM [21]) and Support Vector Machines (SVM [4]).",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : "For the evaluation purposes we implemented five methods, namely: Weighted Extreme Learning Machine (WELM [25]), Extreme Entropy Machine (EEM), Extreme Entropy Kernel Machine (EEKM), Least Squares Support Vector Machines (LS-SVM [21]) and Support Vector Machines (SVM [4]).",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 19,
      "context" : "All methods but SVM were implemented using Python with use of the bleeding-edge versions of numpy [23] and scipy [13] libraries included in anaconda for fair comparision.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "All methods but SVM were implemented using Python with use of the bleeding-edge versions of numpy [23] and scipy [13] libraries included in anaconda for fair comparision.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "For SVM we used highly efficient libSVM [3] library with bindings avaliable in scikit-learn [17].",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : "For SVM we used highly efficient libSVM [3] library with bindings avaliable in scikit-learn [17].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "Random parameters (weights and biases) were selected from uniform distributions on [0,1].",
      "startOffset" : 83,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "Datasets’ features were linearly scaled in order to have each feature in the interval [0,1].",
      "startOffset" : 86,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "due to its balanced nature and usage in previous works regarding Weighted Extreme Learning Machines [25].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "and kernel density based entropic strategy [6] selecting θ maximizing Dcs(⟦β T θH + ⟧, ⟦βTθH − ⟧), (5)",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 16,
      "context" : "where ⟦A⟧ = ⟦A⟧σ(A) is a Gaussian KDE using Silverman’s rule of the window width [19] σ(A) = ( 4 3∣A∣ ) 1/5 std(A) ≈ 1.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "This confirms conclusions from our previous work on classification using Dcs [6] where we claimed that SVMs are conceptually similar in terms of optimization objective, as well as widens it to the new class of models (EEMs).",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "5 EEM stability It was previously reported [11] that ELMs have very stable results in the wide range of the number of hidden neurons.",
      "startOffset" : 43,
      "endOffset" : 47
    } ],
    "year" : 2015,
    "abstractText" : "Most of the existing classification methods are aimed at minimization of empirical risk (through some simple point-based error measured with loss function) with added regularization. We propose to approach this problem in a more information theoretic way by investigating applicability of entropy measures as a classification model objective function. We focus on quadratic Renyi’s entropy and connected Cauchy-Schwarz Divergence which leads to the construction of Extreme Entropy Machines (EEM). The main contribution of this paper is proposing a model based on the information theoretic concepts which on the one hand shows new, entropic perspective on known linear classifiers and on the other leads to a construction of very robust method competetitive with the state of the art non-information theoretic ones (including Support Vector Machines and Extreme Learning Machines). Evaluation on numerous problems spanning from small, simple ones from UCI repository to the large (hundreads of thousands of samples) extremely unbalanced (up to 100:1 classes’ ratios) datasets shows wide applicability of the EEM in real life problems and that it scales well.",
    "creator" : "LaTeX with hyperref package"
  }
}