{
  "name" : "1406.6474.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Convergence Rate of Decomposable Submodular Function Minimization",
    "authors" : [ "Robert Nishihara", "Stefanie Jegelka", "Michael I. Jordan" ],
    "emails" : [ "rkn@eecs.berkeley.edu", "stefje@eecs.berkeley.edu", "jordan@eecs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 6.\n64 74\nv3 [\nm at\nh. O\nC ]\n5 N"
    }, {
      "heading" : "1 Introduction",
      "text" : "A large body of recent work demonstrates that many discrete problems in machine learning can be phrased as the optimization of a submodular set function [2]. A set function F : 2V → R over a ground set V of N elements is submodular if the inequality F (A)+F (B) ≥ F (A∪B)+F (A∩B) holds for all subsets A,B ⊆ V . Problems like clustering [33], structured sparse variable selection [1], MAP inference with higher-order potentials [28], and corpus extraction problems [31] can be reduced to the problem of submodular function minimization (SFM), that is\nmin A⊆V F (A). (P1)\nAlthough SFM is solvable in polynomial time, existing algorithms can be inefficient on large-scale problems. For this reason, the development of scalable, parallelizable algorithms has been an active area of research [24, 25, 29, 35]. Approaches to solving Problem (P1) are either based on combinatorial optimization or on convex optimization via the Lovász extension.\nFunctions that occur in practice are usually not arbitrary and frequently possess additional exploitable structure. For example, a number of submodular functions admit specialized algorithms that solve Problem (P1) very quickly. Examples include cut functions on certain kinds of graphs, concave functions of the cardinality |A|, and functions counting joint ancestors in trees. We will use the term simple to refer to functions F for which we have a fast subroutine for minimizing F + s, where s ∈ RN is any modular function. We treat these subroutines as black boxes. Many commonly occuring submodular functions (for example, graph cuts, hypergraph cuts, MAP inference with higher-order potentials [16, 28, 37], co-segmentation [22], certain structured-sparsity inducing functions [26], covering functions [35], and combinations thereof) can be expressed as a sum\nF (A) = ∑R\nr=1 Fr(A) (1)\nof simple submodular functions. Recent work demonstrates that this structure offers important practical benefits [25, 29, 35]. For instance, it admits iterative algorithms that minimize each Fr separately and combine the results in a straightforward manner (for example, dual decomposition).\nIn particular, it has been shown that the minimization of decomposable functions can be rephrased as a best-approximation problem, the problem of finding the closest points in two convex sets [25]. This formulation brings together SFM and classical projection methods and yields empirically fast, parallel, and easy-to-implement algorithms. In these cases, the performance of projection methods depends heavily on the specific geometry of the problem at hand and is not well understood in general. Indeed, while Jegelka et al. [25] show good empirical results, the analysis of this alternative approach to SFM was left as an open problem.\nContributions. In this work, we study the geometry of the submodular best-approximation problem and ground the prior empirical results in theoretical guarantees. We show that SFM via alternating projections, or block coordinate descent, converges at a linear rate. We show that this rate holds for the best-approximation problem, relaxations of SFM, and the original discrete problem. More importantly, we prove upper and lower bounds on the worst-case rate of convergence. Our proof relies on analyzing angles between the polyhedra associated with submodular functions and draws on results from spectral graph theory. It offers insight into the geometry of submodular polyhedra that may be beneficial beyond the analysis of projection algorithms.\nSubmodular minimization. The first polynomial-time algorithm for minimizing arbitrary submodular functions was a consequence of the ellipsoid method [19]. Strongly and weakly polynomialtime combinatorial algorithms followed [32]. The current fastest running times are O(N5τ1 +N6) [34] in general and O((N4τ1 + N5) logFmax) for integer-valued functions [23], where Fmax = maxA |F (A)| and τ1 is the time required to evaluate F . Some work has addressed decomposable functions [25, 29, 35]. The running times in [29] apply to integer-valued functions and range from O((N +R)2 logFmax) for cuts to O((N +Q2R)(N +Q2R+QRτ2) logFmax), where Q ≤ N is the maximal cardinality of the support of any Fr , and τ2 is the time required to minimize a simple function. Stobbe and Krause [35] use a convex optimization approach based on Nesterov’s smoothing technique. They achieve a (sublinear) convergence rate of O(1/k) for the discrete SFM problem. Their results and our results do not rely on the function being integral.\nProjection methods. Algorithms based on alternating projections between convex sets (and related methods such as the Douglas–Rachford algorithm) have been studied extensively for solving convex feasibility and best-approximation problems [4, 5, 7, 11, 12, 20, 21, 36, 38]. See Deutsch [10] for a survey of applications. In the simple case of subspaces, the convergence of alternating projections has been characterized in terms of the Friedrichs angle cF between the subspaces [5, 6]. There are often good ways to compute cF (see Lemma 6), which allow us to obtain concrete linear rates of convergence for subspaces. The general case of alternating projections between arbitrary convex sets is less well understood. Bauschke and Borwein [3] give a general condition for the linear convergence of alternating projections in terms of the value κ∗ (defined in Section 3.1). However, except in very limited cases, it is unclear how to compute or even bound κ∗. While it is known that κ∗ < ∞ for polyhedra [5, Corollary 5.26], the rate may be arbitrarily slow, and the challenge is to bound the linear rate away from one. We are able to give a specific uniform linear rate for the submodular polyhedra that arise in SFM.\nAlthough both κ∗ and cF are useful quantities for understanding the convergence of projection methods, they largely have been studied independently of one another. In this work, we relate these two quantities for polyhedra, thereby obtaining some of the generality of κ∗ along with the computability of cF . To our knowledge, we are the first to relate κ∗ and cF outside the case of subspaces. We feel that this connection may be useful beyond the context of submodular polyhedra."
    }, {
      "heading" : "1.1 Background",
      "text" : "Throughout this paper, we assume that F is a sum of simple submodular functions F1, . . . , FR and that F (∅) = 0. Points s ∈ RN can be identified with (modular) set functions via s(A) = ∑n∈A sn. The base polytope of F is defined as the set of all modular functions that are dominated by F and that sum to F (V ),\nB(F ) = {s ∈ RN | s(A) ≤ F (A) for all A ⊆ V and s(V ) = F (V )}. The Lovász extension f : RN → R of F can be written as the support function of the base polytope, that is f(x) = maxs∈B(F ) s⊤x. Even though B(F ) may have exponentially many faces, the extension f can be evaluated in O(N logN) time [15]. The discrete SFM problem (P1) can be relaxed to\nthe non-smooth convex optimization problem\nmin x∈[0,1]N f(x) ≡ min x∈[0,1]N\nR∑\nr=1\nfr(x), (P2)\nwhere fr is the Lovász extension of Fr. This relaxation is exact – rounding an optimal continuous solution yields the indicator vector of an optimal discrete solution. The formulation in Problem (P2) is amenable to dual decomposition [30] and smoothing techniques [35], but suffers from the nonsmoothness of f [25]. Alternatively, we can formulate a proximal version of the problem\nmin x∈RN f(x) + 12‖x‖2 ≡ min x∈RN\nR∑\nr=1\n(fr(x) + 1 2R‖x‖2). (P3)\nBy thresholding the optimal solution of Problem (P3) at zero, we recover the indicator vector of an optimal discrete solution [17], [2, Proposition 8.4].\nLemma 1. [25] The dual of the right-hand side of Problem (P3) is the best-approximation problem\nmin ‖a− b‖2 a ∈ A, b ∈ B, (P4)\nwhere A = {(a1, . . . , aR) ∈ RNR | ∑R r=1 ar = 0} and B = B(F1)× · · · ×B(FR).\nLemma 1 implies that we can minimize a decomposable submodular function by solving Problem (P4), which means finding the closest points between the subspace A and the product B of base polytopes. Projecting onto A is straightforward because A is a subspace. Projecting onto B amounts to projecting onto each B(Fr) separately. The projection ΠB(Fr)z of a point z onto B(Fr) may be solved by minimizing Fr − z [25]. We can compute these projections easily because each Fr is simple.\nThroughout this paper, we use A and B to refer to the specific polyhedra defined in Lemma 1 (which live in RNR) and P and Q to refer to general polyhedra (sometimes arbitrary convex sets) in RD. Note that the polyhedron B depends on the submodular functions F1, . . . , FR, but we omit the dependence to simplify our notation. Our bound will be uniform over all submodular functions."
    }, {
      "heading" : "2 Algorithm and Idea of Analysis",
      "text" : "A popular class of algorithms for solving best-approximation problems are projection methods [5]. The most straightforward approach uses alternating projections (AP) or block coordinate descent. Start with any point a0 ∈ A, and inductively generate two sequences via bk = ΠBak and ak+1 = ΠAbk. Given the nature of A and B, this algorithm is easy to implement and use in our setting, and it solves Problem (P4) [25]. This is the algorithm that we will analyze.\nThe sequence (ak, bk) will eventually converge to an optimal pair (a∗, b∗). We say that AP converges linearly with rate α < 1 if ‖ak−a∗‖ ≤ C1αk and ‖bk−b∗‖ ≤ C2αk for all k and for some constants C1 and C2. Smaller values of α are better.\nAnalysis: Intuition. We will provide a detailed analysis of the convergence of AP for the polyhedra A and B. To motivate our approach, we first provide some intuition with the following muchsimplified setup. Let U and V be one-dimensional subspaces spanned by the unit vectors u and v respectively. In this case, it is known that AP converges linearly with rate cos2 θ, where θ ∈ [0, π2 ] is the angle such that cos θ = u⊤v. The smaller the angle, the slower the rate of convergence. For subspaces U and V of higher dimension, the relevant generalization of the “angle” between the subspaces is the Friedrichs angle [11, Definition 9.4], whose cosine is given by\ncF (U, V ) = sup { u⊤v |u ∈ U ∩ (U ∩ V )⊥, v ∈ V ∩ (U ∩ V )⊥, ‖u‖ ≤ 1, ‖v‖ ≤ 1 } . (2)\nIn finite dimensions, cF (U, V ) < 1. In general, when U and V are subspaces of arbitrary dimension, AP will converge linearly with rate cF (U, V )2 [11, Theorem 9.8]. If U and V are affine spaces, AP still converges linearly with rate cF (U − u, V − v)2, where u ∈ U and v ∈ V . We are interested in rates for polyhedra P and Q, which we define as the intersection of finitely many halfspaces. We generalize the preceding results by considering all pairs (Px, Qy) of\nfaces of P and Q and showing that the convergence rate of AP between P and Q is at worst maxx,y cF (aff0(Px), aff0(Qy))\n2, where aff(C) is the affine hull of C and aff0(C) = aff(C) − c for some c ∈ C. The faces {Px}x∈RD of P are defined as the nonempty maximizers of linear functions over P , that is\nPx = argmax p∈P\nx⊤p. (3)\nWhile we look at angles between pairs of faces, we remark that Deutsch and Hundal [13] consider a different generalization of the “angle” between arbitrary convex sets.\nRoadmap of the Analysis. Our analysis has two main parts. First, we relate the convergence rate of AP between polyhedra P and Q to the angles between the faces of P and Q. To do so, we give a general condition under which AP converges linearly (Theorem 2), which we show depends on the angles between the faces of P and Q (Corollary 5) in the polyhedral case. Second, we specialize to the polyhedra A and B, and we equate the angles with eigenvalues of certain matrices and use tools from spectral graph theory to bound the relevant eigenvalues in terms of the conductance of a specific graph. This yields a worst-case bound of 1− 1N2R2 on the rate, stated in Theorem 12.\nIn Theorem 14, we show a lower bound of 1− 2π2N2R on the worst-case convergence rate."
    }, {
      "heading" : "3 The Upper Bound",
      "text" : "We first derive an upper bound on the rate of convergence of AP between the polyhedra A and B. The results in this section are proved in Appendix A."
    }, {
      "heading" : "3.1 A Condition for Linear Convergence",
      "text" : "We begin with a condition under which AP between two closed convex sets P and Q converges linearly. This result is similar to that of Bauschke and Borwein [3, Corollary 3.14], but the rate we achieve is twice as fast and relies on slightly weaker assumptions.\nWe will need a few definitions from Bauschke and Borwein [3]. Let d(K1,K2) = inf{‖k1 − k2‖ : k1 ∈ K1, k2 ∈ K2} be the distance between sets K1 and K2. Define the sets of “closest points” as\nE = {p ∈ P | d(p,Q) = d(P,Q)} H = {q ∈ Q | d(q, P ) = d(Q,P )}, (4) and let v = ΠQ−P 0 (see Figure 1). Note that H = E + v, and when P ∩ Q 6= ∅ we have v = 0 and E = H = P ∩ Q. Therefore, we can think of the pair (E,H) as a generalization of the intersection P ∩Q to the setting where P and Q do not intersect. Pairs of points (e, e+ v) ∈ E×H are solutions to the best-approximation problem between P and Q. In our analysis, we will mostly study the translated version Q′ = Q− v of Q that intersects P at E. For x ∈ RD\\E, the function κ relates the distance to E with the distances to P and Q′,\nκ(x) = d(x,E)\nmax{d(x, P ), d(x,Q′)} .\nIf κ is bounded, then whenever x is close to both P and Q′, it must also be close to their intersection. If, for example, D ≥ 2 and P and Q are balls of radius one whose centers are separated by distance\nexactly two, then κ is unbounded. The maximum κ∗ = supx∈(P∪Q′)\\E κ(x) is useful for bounding the convergence rate.\nTheorem 2. Let P and Q be convex sets, and suppose that κ∗ < ∞. Then AP between P and Q converges linearly with rate 1− 1κ2\n∗\n. Specifically,\n‖pk − p∗‖ ≤ 2‖p0 − p∗‖(1− 1κ2 ∗ )k and ‖qk − q∗‖ ≤ 2‖q0 − q∗‖(1− 1κ2 ∗ )k."
    }, {
      "heading" : "3.2 Relating κ∗ to the Angles Between Faces of the Polyhedra",
      "text" : "In this section, we consider the case of polyhedra P and Q, and we bound κ∗ in terms of the angles between pairs of their faces. In Lemma 3, we show that κ is nondecreasing along the sequence of points generated by AP between P and Q′. We treat points p for which κ(p) = 1 separately because those are the points from which AP between P and Q′ converges in one step. This lemma enables us to bound κ(p) by initializing AP at p and bounding κ at some later point in the resulting sequence.\nLemma 3. For any p ∈ P\\E, either κ(p) = 1 or 1 < κ(p) ≤ κ(ΠQ′p). Similarly, for any q ∈ Q′\\E, either κ(q) = 1 or 1 < κ(q) ≤ κ(ΠP q).\nWe can now bound κ by angles between faces of P and Q.\nProposition 4. If P and Q are polyhedra and p ∈ P\\E, then there exist faces Px and Qy such that\n1− 1 κ(p)2 ≤ cF (aff0(Px), aff0(Qy))2.\nThe analogous statement holds when we replace p ∈ P\\E with q ∈ Q′\\E.\nNote that aff0(Qy) = aff0(Q′y). Proposition 4 immediately gives us the following corollary.\nCorollary 5. If P and Q are polyhedra, then\n1− 1 κ2∗ ≤ max x,y∈RD cF (aff0(Px), aff0(Qy)) 2."
    }, {
      "heading" : "3.3 Angles Between Subspaces and Singular Values",
      "text" : "Corollary 5 leaves us with the task of bounding the Friedrichs angle. To do so, we first relate the Friedrichs angle to the singular values of certain matrices in Lemma 6. We then specialize this to base polyhedra of submodular functions. For convenience, we prove Lemma 6 in Appendix A.5, though this result is implicit in the characterization of principal angles between subspaces given in [27, Section 1]. Ideas connecting angles between subspaces and eigenvalues are also used by Diaconis et al. [14].\nLemma 6. Let S and T be matrices with orthonormal rows and with equal numbers of columns. If all of the singular values of ST⊤ equal one, then cF (null(S), null(T )) = 0. Otherwise, cF (null(S), null(T )) is equal to the largest singular value of ST⊤ that is less than one.\nFaces of relevant polyhedra. Let Ax and By be faces of the polyhedra A and B from Lemma 1. Since A is a vector space, its only nonempty face is Ax = A. Hence, Ax = null(S), where S is an N ×NR matrix of N ×N identity matrices IN :\nS = 1√ R\n(\nIN · · · IN ︸ ︷︷ ︸\nrepeated R times\n)\n. (5)\nThe matrix for aff0(By) requires a bit more elaboration. Since B is a Cartesian product, we have By = B(F1)y1 × · · · × B(FR)yR , where y = (y1, . . . , yR) and B(Fr)yr is a face of B(Fr). To proceed, we use the following characterization of faces of base polytopes [2, Proposition 4.7].\nProposition 7. Let F be a submodular function, and let B(F )x be a face of B(F ). Then there exists a partition of V into disjoint sets A1, . . . , AM such that\naff(B(F )x) =\nM⋂\nm=1\n{s ∈ RN | s(A1 ∪ · · · ∪ Am) = F (A1 ∪ · · · ∪Am)}.\nThe following corollary is immediate.\nCorollary 8. Define F , B(F )x, and A1, . . . , AM as in Proposition 7. Then\naff0(B(F )x) =\nM⋂\nm=1\n{s ∈ RN | s(A1 ∪ · · · ∪ Am) = 0}.\nBy Corollary 8, for each Fr, there exists a partition of V into disjoint sets Ar1, . . . , ArMr such that\naff0(By) = R⋂\nr=1\nMr⋂\nm=1\n{(s1, . . . , sR) ∈ RNR | sr(Ar1 ∪ · · · ∪ Arm) = 0}. (6)\nIn other words, we can write aff0(By) as the nullspace of either of the matrices\nT ′ =\n\n            \n1⊤A11 ...\n1⊤A11∪···∪A1M1 . . .\n1⊤AR1 ...\n1⊤AR1∪···∪ARMR\n\n             or T =\n\n                \n1⊤A11√ |A11| ...\n1⊤A1M1√ |A1M1 | . . .\n1⊤AR1√ |AR1| ...\n1⊤ARMR√ |ARMR |\n\n                 ,\nwhere 1A is the indicator vector of A ⊆ V . For T ′, this follows directly from Equation (6). T can be obtained from T ′ via left multiplication by an invertible matrix, so T and T ′ have the same nullspace. Lemma 6 then implies that cF (aff0(Ax), aff0(By)) equals the largest singular value of\nST⊤ = 1√ R ( 1A11√ |A11| · · · 1A1M1√|A1M1 | · · · 1AR1√ |AR1| · · · 1ARMR√|ARMR | )\nthat is less than one. We rephrase this conclusion in the following remark.\nRemark 9. The largest eigenvalue of (ST⊤)⊤(ST⊤) less than one equals cF (aff0(Ax), aff0(By))2.\nLet Mall = M1 + · · ·+MR. Then (ST⊤)⊤(ST⊤) is the Mall ×Mall square matrix whose rows and columns are indexed by (r,m) with 1 ≤ r ≤ R and 1 ≤ m ≤ Mr and whose entry corresponding to row (r1,m1) and column (r2,m2) equals\n1\nR\n1⊤Ar1m1 1Ar2m2 √\n|Ar1m1 ||Ar2m2 | =\n1\nR\n|Ar1m1 ∩ Ar2m2 | √\n|Ar1m1 ||Ar2m2 | ."
    }, {
      "heading" : "3.4 Bounding the Relevant Eigenvalues",
      "text" : "It remains to bound the largest eigenvalue of (ST⊤)⊤(ST⊤) that is less than one. To do so, we view the matrix in terms of the symmetric normalized Laplacian of a weighted graph. Let G be the graph whose vertices are indexed by (r,m) with 1 ≤ r ≤ R and 1 ≤ m ≤ Mr. Let the edge between vertices (r1,m1) and (r2,m2) have weight |Ar1m1 ∩ Ar2m2 |. We may assume that G is connected (the analysis in this case subsumes the analysis in the general case). The symmetric normalized Laplacian L of this graph is closely related to our matrix of interest,\n(ST⊤)⊤(ST⊤) = I − R−1R L. (7)\nHence, the largest eigenvalue of (ST⊤)⊤(ST⊤) that is less than one can be determined from the smallest nonzero eigenvalue λ2(L) of L. We bound λ2(L) via Cheeger’s inequality (stated in Appendix A.6) by bounding the Cheeger constant hG of G. Lemma 10. For R ≥ 2, we have hG ≥ 2NR and hence λ2(L) ≥ 2N2R2 .\nWe prove Lemma 10 in Appendix A.7. Combining Remark 9, Equation (7), and Lemma 10, we obtain the following bound on the Friedrichs angle.\nProposition 11. Assuming that R ≥ 2, we have cF (aff0(Ax), aff0(By))2 ≤ 1− R−1R 2N2R2 ≤ 1− 1N2R2 .\nTogether with Theorem 2 and Corollary 5, Proposition 11 implies the final bound on the rate.\nTheorem 12. The AP algorithm for Problem (P4) converges linearly with rate 1− 1N2R2 , i.e.,\n‖ak − a∗‖ ≤ 2‖a0 − a∗‖(1− 1N2R2 )k and ‖bk − b∗‖ ≤ 2‖b0 − b∗‖(1− 1N2R2 )k."
    }, {
      "heading" : "4 A Lower Bound",
      "text" : "To probe the tightness of Theorem 12, we construct a “bad” submodular function and decomposition that lead to a slow rate. Appendix B gives the formal details. Our example is an augmented cut function on a cycle: for each x, y ∈ V , define Gxy to be the cut function of a single edge (x, y),\nGxy = { 1 if |A ∩ {x, y}| = 1 0 otherwise .\nTake N to be even and R ≥ 2 and define the submodular function F lb = F lb1 + · · ·+ F lbR , where F lb1 = G12 +G34 + · · ·+G(N−1)N F lb2 = G23 +G45 + · · ·+GN1\nand F lbr = 0 for all r ≥ 3. The optimal solution to the best-approximation problem is the all zeros vector.\nLemma 13. The cosine of the Friedrichs angle between A and aff(Blb) is cF (A, aff(Blb))2 = 1− 1R ( 1− cos ( 2π N )) .\nAround the optimal solution 0, the polyhedra A and Blb behave like subspaces, and it is possible to pick initializations a0 ∈ A and b0 ∈ Blb such that the Friedrichs angle exactly determines the rate of convergence. That means 1− 1/κ2∗ = cF (A, aff(Blb))2, and\n‖ak‖ = (1− 1R (1− cos(2πN )))k‖a0‖ and ‖bk‖ = (1− 1R (1− cos(2πN )))k‖b0‖.\nBounding 1− cos(x) ≤ 12x2 leads to the following lower bound on the rate. Theorem 14. There exists a decomposed function F lb and initializations for which the convergence rate of AP is at least 1− 2π2N2R .\nThis theoretical bound can also be observed empirically (Figure 3 in Appendix B)."
    }, {
      "heading" : "5 Convergence of the Primal Objective",
      "text" : "We have shown that AP generates a sequence of points {ak}k≥0 and {bk}k≥0 in RNR such that (ak, bk) → (a∗, b∗) linearly, where (a∗, b∗) minimizes the objective in Problem (P4). In this section, we show that this result also implies the linear convergence of the objective in Problem (P3) and of the original discrete objective in Problem (P1). The proofs may be found in Appendix C.\nDefine the matrix Γ = −R1/2S, where S is the matrix defined in Equation (5). Multiplication by Γ maps a vector (w1, . . . , wR) to − ∑\nr wr, where wr ∈ RN for each r. Set xk = Γbk and x∗ = Γb∗. As shown in Jegelka et al. [25], Problem (P3) is minimized by x∗. Proposition 15. We have f(xk) + 12‖xk‖2 → f(x∗) + 12‖x∗‖2 linearly with rate 1− 1N2R2 .\nThis linear rate of convergence translates into a linear rate for the original discrete problem.\nTheorem 16. Choose A∗ ∈ argminA⊆V F (A). Let Ak be the suplevel set of xk with smallest value of F . Then F (Ak) → F (A∗) linearly with rate 1− 12N2R2 ."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this work, we analyze projection methods for parallel SFM and give upper and lower bounds on the linear rate of convergence. This means that the number of iterations required for an accuracy of ǫ is logarithmic in 1/ǫ, not linear as in previous work [35]. Our rate is uniform over all submodular functions. Moreover, our proof highlights how the number R of components and the facial structure of B affect the convergence rate. These insights may serve as guidelines when working with projection algorithms and aid in the analysis of special cases. For example, reducing R is often possible. Any collection of Fr that have disjoint support, such as the cut functions corresponding to the rows or columns of a grid graph, can be grouped together without making the projection harder.\nOur analysis also shows the effects of additional properties of F . For example, suppose that F is separable, that is, F (V ) = F (S) + F (V \\S) for some nonempty S ( V . Then the subsets Arm ⊆ V defining the relevant faces of B satisfy either Arm ⊆ S or Arm ⊆ Sc [2]. This makes G in Section 3.4 disconnected, and as a result, the N in Theorem 12 gets replaced by max{|S|, |Sc|} for an improved rate. This applies without the user needing to know S when running the algorithm.\nA number of future directions suggest themselves. For example, Jegelka et al. [25] also considered the related Douglas–Rachford (DR) algorithm. DR between subspaces converges linearly with rate cF [6], as opposed to c2F for AP. We suspect that our approach may be modified to analyze DR between polyhedra. Further questions include the extension to cyclic updates (instead of parallel ones), multiple polyhedra, and stochastic algorithms.\nAcknowledgments. We would like to thank Mădălina Persu for suggesting the use of Cheeger’s inequality. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Apple, C3Energy, Cisco, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel, Microsoft, NetApp, Pivotal, Splunk, Virdata, VMware, WANdisco, and Yahoo!. This work is supported in part by the Office of Naval Research under grant number N00014-11-1-0688, the US ARL and the US ARO under grant number W911NF-11-1-0391, and the NSF under grant number DGE-1106400."
    }, {
      "heading" : "A Upper Bound Results",
      "text" : ""
    }, {
      "heading" : "A.1 Proof of Theorem 2",
      "text" : "For the proof of this theorem, we will need the fact that projection maps are firmly nonexpansive, that is, for a closed convex nonempty subset C ⊆ RD, we have\n‖ΠCx−ΠCy‖2 + ‖(x−ΠCx)− (y −ΠCy)‖2 ≤ ‖x− y‖2 for all x, y ∈ RD. Now, suppose that κ∗ < ∞. Let e = ΠEpk and note that v = ΠQe − e and that ΠQe ∈ H . We have\nκ−2∗ d(pk, E) 2 ≤ d(pk, Q′)2\n≤ ‖pk −ΠQpk + v‖2\n≤ ‖(pk −ΠQpk)− (e −ΠQe)‖2\n≤ ‖pk − e‖2 − ‖ΠQpk −ΠQe‖2\n≤ d(pk, E)2 − d(qk, H)2. It follows that d(qk, H) ≤ (1 − κ−2∗ )1/2d(pk, E). Similarly, we have d(pk+1, E) ≤ (1 − κ−2∗ ) 1/2d(qk, H). When combining these, induction shows that\nd(pk, E) ≤ (1− κ−2∗ )kd(p0, E) d(qk, H) ≤ (1− κ−2∗ )kd(q0, H).\nAs shown in [3, Theorem 3.3], the above implies that pk → p∗ ∈ E and qk → q∗ ∈ H and that ‖pk − p∗‖ ≤ 2‖p0 − p∗‖(1− κ−2∗ )k\n‖qk − q∗‖ ≤ 2‖q0 − q∗‖(1− κ−2∗ )k."
    }, {
      "heading" : "A.2 Connection Between κ and cF in the Subspace Case",
      "text" : "In this section, we introduce a simple lemma connecting κ and cF in the case of subspaces U and V . We will use this lemma in several subsequent proofs. Lemma 17. Let U and V be subspaces and suppose u ∈ U ∩ (U ∩ V )⊥ and that u 6= 0. Then\n(a) ‖ΠV u‖ ≤ cF (U, V )‖u‖ (b) κ(u) ≤ (1− cF (U, V )2)−1/2 (c) κ(u) = (1− cF (U, V )2)−1/2 if and only if ‖ΠV u‖ = cF (U, V )‖u‖.\nProof. Part (a) follows from the definition of cF . Indeed,\ncF (U, V ) ≥ u⊤(ΠV u)\n‖u‖‖ΠV u‖ = ‖ΠV u‖2 ‖u‖‖ΠV u‖ = ‖ΠV u‖ ‖u‖ .\nPart (b) follows from Part (a) and the observation that κ(u) = (1 − ‖ΠV u‖2/‖u‖2)−1/2. Part (c) follows from the same observation."
    }, {
      "heading" : "A.3 Proof of Lemma 3",
      "text" : "It suffices to prove the statement for p ∈ P\\E. For p ∈ P\\E, define q = ΠQ′p, e = ΠEq, and p′′ = Π[p,e]q, where [p, e] denotes the line segment between p and e (which is contained in P by convexity). See Figure 2 for a graphical depiction. If q ∈ E, then κ(p) = 1. So we may assume that q /∈ E which also implies that d(p′′, E) > 0 and d(ΠP q, E) > 0. We have\nκ(p) = d(p,E) d(p,Q′) ≤ ‖p− e‖‖p− q‖ ≤ ‖q − e‖ ‖q − p′′‖ ≤ d(q, E) d(q, P ) = κ(q). (8)\nThe first inequality holds because d(p,E) ≤ ‖p−e‖ and d(p,Q′) = ‖p−q‖. The middle inequality holds because the area of the triangle with vertices p, q, and e can be expressed as both 12‖p−e‖‖q− p′′‖ and 12‖p− q‖‖q − e‖ sin θ, where θ is the angle between vectors p− q and e− q, so ‖p− e‖‖q − p′′‖ = ‖p− q‖‖q − e‖ sin θ ≤ ‖p− q‖‖q − e‖. The third inequality holds because ‖q − e‖ = d(q, E) and ‖q − p′′‖ ≥ d(q, P ). The chain of inequalities in Equation (8) prove the lemma."
    }, {
      "heading" : "A.4 Proof of Proposition 4",
      "text" : "Suppose that p ∈ P\\E (the case q ∈ Q′\\E is the same), and let e = ΠEp. If κ(p) = 1, the statement is evident, so we may assume that κ(p) > 1. We will construct sequences of polyhedra\nP ⊇ P1 ⊇ · · · ⊇ PJ Q′ ⊇ Q′1 ⊇ · · · ⊇ Q′J .\nwhere Pj+1 is a face of Pj and Q′j+1 is a face of Q ′ j for 1 ≤ j ≤ J − 1. Either dim(aff(Pj+1)) < dim(aff(Pj)) or dim(aff(Q′j+1)) < dim(aff(Q ′ j)) will hold. We will further define Ej = Pj ∩Q′j , which will contain e, so that we can define\nκj(x) = d(x,Ej)\nmax{d(x, Pj), d(x,Q′j)} for x ∈ RD\\Ej (this is just the function κ defined for the polyhedra Pj and Q′j). Our construction will yield points pj ∈ Pj , and qj ∈ Q′j such that pj ∈ relint(Pj)\\Ej , qj ∈ relint(Q′j)\\Ej , and qj = ΠQ′jpj for each j. Furthermore, we will have\nκ(p) ≤ κ1(p1) ≤ · · · ≤ κJ(pJ ). (9)\nNow we describe the construction. For any t ∈ [0, 1], define pt = (1 − t)p + te to be the point obtained by moving p by the appropriate amount toward e. Note that t 7→ κ(pt) is a nondecreasing function on the interval [0, 1). Choose ǫ > 0 sufficiently small so that every face of either P or Q′ that intersects Bǫ(e), the ball of radius ǫ centered on e, necessarily contains e. Now choose 0 ≤ t0 < 1 sufficiently close to 1 so that ‖pt0 − e‖ < ǫ. It follows that e is contained in the face of P whose relative interior contains pt0 . It further follows that e is contained in the face of Q′ whose relative interior contains ΠQ′pt0 because\n‖ΠQ′pt0 − e‖ = ‖ΠQ′pt0 −ΠQ′e‖ ≤ ‖pt0 − e‖ < ǫ.\nTo initialize the construction, set\np1 = p t0\nq1 = ΠQ′p t0 ,\nand let P1 and Q′1 be the unique faces of P and Q ′ respectively such that p1 ∈ relint(P1) and q1 ∈ relint(Q′1) (the relative interiors of the faces of a polyhedron partition that polyhedron [8, Theorem 2.2]). Note that q1 /∈ E because κ(p1) ≥ κ(p) > 1. Note that e ∈ E1 = P1 ∩Q′1 so that\nκ(p) ≤ κ(p1) = d(p1, E)\nd(p1, Q′) = ‖p1 − e‖ ‖p1 − q1‖ = d(p1, E1) d(p1, Q′1) = κ1(p1).\nNow, inductively assume that we have defined Pj , Q′j , pj , and qj satisfying the stated properties. Generate the sequences {xk}k≥0 and {yk}k≥0 with xk ∈ Pj and yk ∈ Q′j by running AP between the polyhedraPj andQ′j initialized with x0 = pj . There are two possibilities, either xk ∈ relint(Pj) and yk ∈ relint(Q′j) for every k, or there is some k for which either xk /∈ relint(Pj) or yk /∈ relint(Q′j). Note that Pj and Q ′ j intersect and that AP between them will not terminate after a finite number of steps.\nSuppose that xk ∈ relint(Pj) and yk ∈ relint(Q′j) for every k. Then set J = j and terminate the procedure. Otherwise, choose k′ such that either xk′ /∈ relint(Pj) or yk′ /∈ relint(Q′j). Now set pj+1 = xk′ and qj+1 = yk′ . Let Pj+1 and Q′j+1 be the unique faces of Pj and Q ′ j respectively such that pj+1 ∈ relint(Pj+1) and qj+1 ∈ relint(Q′j+1). Note that pj+1, qj+1 /∈ Ej+1 = Pj+1 ∩ Q′j+1 and e ∈ Ej+1. We have\nκj(pj) < κj(pj+1) = d(pj+1, Ej)\nd(pj+1, Q′j) =\nd(pj+1, Ej) ‖pj+1 − qj+1‖ ≤ d(pj+1, Ej+1) d(pj+1, Q′j+1) = κj+1(pj+1).\nThe preceding work shows the inductive step. Note that if Pj+1 6= Pj then dim(aff(Pj+1)) < dim(aff(Pj)) and if Q′j+1 6= Q′j then dim(aff(Q′j+1)) < dim(aff(Q′j)). One of these will hold, so the induction will terminate after a finite number of steps.\nWe have produced the sequence in Equation (9) and we have created pJ , PJ , and Q′J such that AP between PJ and Q′J , when initialized at pJ , generates the same sequence of points as AP between aff(PJ ) and aff(Q′J). Using this fact, along with [12, Theorem 9.3], we see that Πaff(PJ )∩aff(Q′J )pJ ∈ EJ . Using this, along with Lemma 17(b), we see that κJ(pJ ) ≤ (1− cF (aff0(PJ ), aff0(Q′J))2)−1/2. (10) Equations (10) and (9) prove the result. Note that PJ and Q′J are faces of P and Q\n′ respectively. We can switch between faces of Q′ and faces of Q because doing so amounts to translating by v which does not affect the angles."
    }, {
      "heading" : "A.5 Proof of Lemma 6",
      "text" : "We have\ncF (null(S), null(T )) = cF (range(S ⊤)⊥, range(T⊤)⊥)\n= cF (range(S ⊤), range(T⊤)),\nwhere the first equality uses the fact that null(W ) = range(W⊤)⊥ for matrices W , and the second equality uses the fact that cF (U⊥, V ⊥) = cF (U, V ) for subspaces U and V [6, Fact 2.3].\nLet S⊤ and T⊤ have dimensions D× J and D×K respectively, and let X and Y be the subspaces spanned by the columns of S⊤ and T⊤ respectively. Without loss of generality, assume that J ≤ K . Let σ1 ≥ · · · ≥ σJ be the singular values of ST⊤ with corresponding left singular vectors u1, . . . , uJ and right singular vectors v1, . . . , vJ . Let xj = S⊤uj and let yj = T⊤vj for 1 ≤ j ≤ J . By definition, we can write\nσj = max u,v\n{u⊤ST⊤v |u ⊥ span(u1, . . . , uj−1), v ⊥ span(v1, . . . , vj−1), ‖u‖ = 1, ‖v‖ = 1}.\nSince the {uj}j are orthonormal, so are the {xj}j . Similarly, since the {vj}j are orthonormal, so are the {yj}j . Suppose that all of the singular values of ST⊤ equal one. Then we must have xj = yj for each j, which implies that X ⊆ Y , and so cF (X,Y ) = 0. Now suppose that σ1 = · · · = σℓ = 1, and σℓ+1 6= 1. It follows that\nX ∩ Y = span(x1, . . . , xℓ) = span(y1, . . . , yℓ), and so\nσℓ+1 = sup u,v\n{u⊤ST⊤v |u ∈ span(u1, . . . , uℓ)⊥, v ∈ span(v1, . . . , vℓ)⊥, ‖u‖ = 1, ‖v‖ = 1}\n= sup x,y\n{x⊤y |x ∈ X ∩ (X ∩ Y )⊥, y ∈ Y ∩ (X ∩ Y )⊥, ‖x‖ = 1, ‖y‖ = 1}\n= cF (X,Y )."
    }, {
      "heading" : "A.6 Cheeger’s Inequality",
      "text" : "For an overview of spectral graph theory, see Chung [9]. We state Cheeger’s inequality below.\nLet G be a weighted, connected graph with vertex set VG and edge weights (wij)i,j∈VG . Define the weighted degree of a vertex i to be δi = ∑\nj 6=i wij , define the volume of a subset of vertices to be the sum of their weighted degrees, vol(U) = ∑\ni∈U δi, and define the size of the cut between U and its complement U c to be the sum of the weights of the edges between U and U c,\n|E(U,U c)| = ∑\ni∈U,j∈Uc wij .\nThe Cheeger constant is defined as\nhG = min ∅6=U(VG |E(U,U c)| min(vol(U), vol(U c)) .\nLet L be the unnormalized Laplacian of G, i.e. the |VG| × |VG| matrix whose entries are defined by\nLij = { −wij i 6= j δi otherwise .\nLet D be the |VG| × |VG| diagonal matrix defined by Dii = δi. Then L = D−1/2LD−1/2 is the normalized Laplacian. Let λ2(L) denote the second smallest eigenvalue of L (since G is connected, there will be exactly one eigenvalue equal to zero).\nTheorem 18 (Cheeger’s inequality). We have λ2(L) ≥ h 2 G\n2 ."
    }, {
      "heading" : "A.7 Proof of Lemma 10",
      "text" : "Proof. We have\nmin(vol(U), vol(U c)) ≤ 1 2 vol(VG)\n= 1\n2\n∑\n(r,m)\n\n ∑\n(r′,m′) 6=(r,m) |Arm ∩ Ar′m′ |\n\n\n= 1\n2\n∑\n(r,m)\n(R− 1)|Arm|\n= 1\n2 NR(R− 1).\nSince G is connected, for any nonempty set U ( VG, there must be some element v ∈ V (here V is the ground set of our submodular functionF , not the set of vertices VG) such that v ∈ Ar1m1∩Ar2m2 for some (r1,m1) ∈ U and (r2,m2) ∈ U c. Suppose that v appears in k of the subsets of V indexed by elements of U and in R− k of the subsets of V indexed by elements of U c. Then\n|E(U,U c)| ≥ k(R − k) ≥ R − 1. It follows that\nhG ≥ R− 1\n1 2NR(R− 1)\n= 2\nNR .\nIt follows from Theorem 18 that λ2(L) ≥ 2N2R2 ."
    }, {
      "heading" : "B Results for the Lower Bound",
      "text" : ""
    }, {
      "heading" : "B.1 Some Helpful Results",
      "text" : "In Lemma 19, we show how AP between subspaces U and V can be initialized to exactly achieve the worst-case rate of convergence. Then in Corollary 20, we show that if subsets U ′ and V ′ look like subspaces U and V near the origin, we can initialize AP between U ′ and V ′ to achieve the same worst-case rate of convergence.\nLemma 19. Let U and V be subspaces with U 6⊆ V and V 6⊆ U . Then there exists some nonzero point u0 ∈ U ∩ (U ∩ V )⊥ such that when we initialize AP at u0, the resulting sequences {uk}k≥0 and {vk}k≥0 satisfy\n‖uk‖ = cF (U, V )2k‖u0‖ ‖vk‖ = cF (U, V )2k‖v0‖.\nProof. Find u∗ ∈ U ∩ (U ∩ V )⊥ and v∗ ∈ V ∩ (U ∩ V )⊥ with ‖u∗‖ = 1 and ‖v∗‖ = 1 such that u⊤∗ v∗ = cF (U, V ), which we can do by compactness. By Lemma 17(a),\ncF (U, V ) = v ⊤ ∗ u∗ = v ⊤ ∗ ΠV u∗ ≤ ‖ΠV u∗‖ ≤ cF (U, V ).\nSet u0 = u∗ and generate the sequences {uk}k≥0 and {vk}k≥0 via AP. Since ‖ΠV u0‖ = cF (U, V ), Lemma 17(c) implies that κ(u0) = (1 − cF (U, V )2)−1/2. Since κ attains its maximum at u0, Lemma 3 implies that κ attains the same value at every element of the sequences {uk}k≥0 and {vk}k≥0. Therefore, Lemma 17(c) implies that ‖ΠV uk‖ = cF (U, V )‖uk‖ and ‖ΠUvk‖ = cF (U, V )‖vk‖ for all k. This proves the lemma. Corollary 20. Let U and V be subspaces with U 6⊆ V and V 6⊆ U . Let U ′ ⊆ U and V ′ ⊆ V be subsets such that U ′∩Bǫ(0) = U ∩Bǫ(0) and V ′∩Bǫ(0) = V ∩Bǫ(0) for some ǫ > 0. Then there is a point u′0 ∈ U ′ such that the sequences {u′k}k≥0 and {v′k}k≥0 generated by AP between U ′ and V ′ initialized at u′0 satisfy\n‖u′k‖ = cF (U, V )2k‖u′0‖ ‖v′k‖ = cF (U, V )2k‖v′0‖.\nProof. Use Lemma 19 to choose some nonzero u0 ∈ U ∩ (U ∩ V )⊥ satisfying this property. Then set u′0 = ǫ ‖u0‖u0."
    }, {
      "heading" : "B.2 Proof of Lemma 13",
      "text" : "Observe that we can write\naff(Blb) = {(s1,−s1, . . . , sN 2 ,−sN 2 ,−tN 2 , t1,−t1, . . . , tN 2 , 0, . . . , 0, . . . , 0, . . . , 0) | si, tj ∈ R}.\nWe can write aff(Blb) as the nullspace of the matrix\nTlb =\n\n    \nTlb,1 Tlb,2\nIN . . .\nIN\n\n     ,\nwhere the N×N identity matrix IN is repeated R−2 times and where Tlb,1 and Tlb,2 are the N2 ×N matrices\nTlb,1 = 1√ 2\n\n  \n1 1 1 1\n. . . 1 1\n\n  \nTlb,2 = 1√ 2\n\n  \n1 1 1 1\n. . . 1 1\n\n   .\nRecall that we can write A as the nullspace of the matrix S defined in Equation (5). It follows from Lemma 6 that cF (A, aff(Blb)) equals the largest singular value of ST⊤lb that is less than one. We have\nST⊤lb = 1√ R ( T⊤lb,1 T ⊤ lb,2 IN · · · IN ) .\nWe can permute the columns of ST⊤lb without changing the singular values, so cF (A, aff(Blb)) equals the largest singular value of\n1√ R ( T⊤lb,0 IN · · · IN ) ,\nthat is less than one, where Tlb,0 is the N ×N circulant matrix\nTlb,0 = 1√ 2\n\n    \n1 1 1 1\n. . . 1 1\n1 1\n\n     .\nTherefore, cF (A, aff(Blb))2 equals the largest eigenvalue of 1 R ( T⊤lb,0 IN · · · IN ) ( T⊤lb,0 IN · · · IN )⊤ = 1R ( T⊤lb,0Tlb,0 + (R− 2)IN ) that is less than one. Therefore, it suffices to examine the N ×N circulant matrix\nT⊤lb,0Tlb,0 = 1\n2\n\n    \n2 1 1 1 2\n. . . 2 1\n1 1 2\n\n     .\nThe eigenvalues of T⊤lb,0Tlb,0 are given by λj = 1 + cos ( 2πj N ) for 0 ≤ j ≤ N − 1 (see Gray [18, Section 3.1] for a derivation). Therefore,\ncF (A, aff(Blb))2 = 1− 1R (1− cos(2πN ))."
    }, {
      "heading" : "B.3 Lower Bound Illustration",
      "text" : "The proof of Theorem 14 shows that there is some a0 ∈ A such that when we initialize AP between A and Blb at a0, we generate a sequence {ak}k≥0 satisfying\nd(ak, E) = (1− 1R (1 − cos(2πN ))kd(a0, E), where E = A ∩ Blb is the optimal set. In Figure 3, we plot the theoretical bound in red, and in blue the successive ratios d(ak+1, E)/d(ak, E) for five runs of AP between A and Blb with random initializations. Had we initialized AP at a0, the successive ratios would exactly equal 1 − 1R (1 − cos(2πN )). The plot of these ratios would coincide with the red line in Figure 3.\nFigure 3 illustrates that the empirical behavior of AP between A and Blb is often similar to the worst-case behavior, even when the initialization is random. When we initialize AP randomly, the successive ratios appear to increase to the lower bound and then remain constant. Figure 3 shows the case N = 10 and R = 10, but the plot looks similar for other N and R.\nWe also note that the graph corresponding to our lower bound example actually achieves a Cheeger constant similar to the one used in Lemma 10."
    }, {
      "heading" : "C Results for Convergence of the Primal and Discrete Problems",
      "text" : ""
    }, {
      "heading" : "C.1 Proof of Proposition 15",
      "text" : "First, suppose that s ∈ B(F ). Let A = {n ∈ V | sn ≥ 0} be the set of indices on which s is nonnegative. Then we have\n‖s‖ ≤ ‖s‖1 = 2s(A)− s(V ) ≤ 3Fmax. (11)\nRecall that we defined Fmax = maxA |F (A)|. Now, we show that f(xk) + 12‖xk‖2 converges to f(x∗) + 12‖x∗‖2 linearly with rate 1 − 1N2R2 . We will use Equation (11) to bound the norms of xk and x∗, both of which lie in −B(F ). We will also use the fact that ‖xk − x∗‖ ≤ ‖Γ‖‖bk − b∗‖ ≤√ R‖bk − b∗‖. Finally, we will use the proof of Theorem 12 to bound ‖bk − b∗‖. First, we bound the difference between the squared norms using convexity. We have\n1 2‖xk‖2 − 12‖x∗‖2 ≤ x⊤k (xk − x∗)\n≤ ‖xk‖‖xk − x∗‖ ≤ 3Fmax √ R‖bk − b∗‖ ≤ 6Fmax √ R‖b0 − b∗‖(1− 1N2R2 )k. (12)\nNext, we bound the difference in Lovász extensions. Choose s ∈ argmaxs∈B(F ) s⊤xk . Then\nf(xk)− f(x∗) ≤ s⊤(xk − x∗) ≤ ‖s‖‖xk − x∗‖ ≤ 3Fmax √ R‖bk − b∗‖\n≤ 6Fmax √ R‖b0 − b∗‖(1− 1N2R2 )k. (13)\nCombining the bounds (12) and (13), we find that\n(f(xk) + 1 2‖xk‖2)− (f(x∗) + 12‖x∗‖2) ≤ 12Fmax √ R‖b0 − b∗‖(1− 1N2R2 )k. (14)"
    }, {
      "heading" : "C.2 Proof of Theorem 16",
      "text" : "We will make use of the following result, shown in [2, Proposition 10.5] and stated below for convenience.\nProposition 21. Let (w, s) ∈ RN ×B(F ) be a pair of primal-dual candidates for the minimization of 12‖w‖2 + f(w), with duality gap ǫ = 12‖w‖2 + f(w) + 12‖s‖2. Then if A is the suplevel set of w with smallest value of F , then\nF (A)− s−(V ) ≤ √ Nǫ/2.\nUsing this result in our setting, recall that by definition Ak is the set of the form {n ∈ V | (xk)n ≥ c} for some constant c with smallest value of F ({n ∈ V | (xk)n ≥ c}). Let (w∗, s∗) ∈ RN ×B(F ) be a primal-dual optimal pair for the left-hand version of Problem (P3). The dual of this minimization problem is the projection problemmins∈B(F ) 1 2‖s‖2. From [2, Propo-\nsition 10.5], we see that\nF (Ak)− F (A∗) ≤ F (Ak)− (s∗)−(V )\n≤ √\nN 2 ((f(xk) + 1 2‖xk‖2)− (f(x∗) + 12‖x∗‖2))\n≤ √\n6FmaxNR1/2‖b0 − b∗‖ (1 − 1N2R2 )k/2\n≤ √\n6FmaxNR1/2‖b0 − b∗‖(1− 12N2R2 )k,\nwhere the third inequality uses the proof of Proposition 15. The second inequality relies on Bach [2, Proposition 10.5], which states that a duality gap of ǫ for the left-hand version of Problem (P3) turns into a duality gap of √\nNǫ/2 for the original discrete problem. If our algorithm converged with rate 1k , this would translate to a rate of 1√ k\nfor the discrete problem. But fortunately, our algorithm converges linearly, and taking a square root preserves linear convergence."
    }, {
      "heading" : "C.3 Running times",
      "text" : "Theorem 16 implies that the number of iterations required for an accuracy of ǫ is at most\n2N2R2 log\n(√\n6FmaxNR1/2‖b0 − b∗‖ ǫ\n)\n. (15)\nEach iteration involves minimizing each of the Fr separately. For comparison, the number of iterations required in Stobbe and Krause [35] is\n24 √ NR\nFmax ǫ .\nThe dependence of this algorithm on N and R is better, but its dependence on Fmax/ǫ is worse. For example, to obtain the exact discrete solution, we need ǫ < minS,T |F (S)− F (T )|. This is one for integer-valued functions (in which case the lower rate may be desirable), but can otherwise become very small. The constant Fmax can be of order O(N) in general (or even larger if the function becomes very negative). For empirical comparisons, we refer the reader to [25].\nThe running times of the combinatorial algorithm by Kolmogorov [29] apply to integer-valued functions (as opposed to the generic ones above) and range from O((N + R)2 logFmax) for cuts to O((N + Q2R)(N + Q2R + QRτ2) logFmax), where Q ≤ N is the maximal cardinality of the support of any Fr , and τ2 is the time required to minimize a simple function. This is better than (15) if Q is a small constant, and worse as Q gets closer to N .\nFor comparison, if not exploiting decomposition, one may use combinatorial algorithms, the FrankWolfe algorithm (conditional gradient descent), or a subgradient method. The combinatorial algorithm by Orlin [34] has a running time of O(N5τ1 + N6), and the algorithm by Iwata [23] (for integer-valued functions) has a running time of O((N4τ1 +N5) logFmax), where τ1 is the time required to evaluate F . For an accuracy of ǫ in the discrete objective, Frank-Wolfe will take 64N Fmaxǫ2 iterations, each taking time O(N logN). The subgradient method behaves similarly."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "Submodular functions describe a variety of discrete problems in machine learn-<lb>ing, signal processing, and computer vision. However, minimizing submodular<lb>functions poses a number of algorithmic challenges. Recent work introduced an<lb>easy-to-use, parallelizable algorithm for minimizing submodular functions that<lb>decompose as the sum of “simple” submodular functions. Empirically, this al-<lb>gorithm performs extremely well, but no theoretical analysis was given. In this<lb>paper, we show that the algorithm converges linearly, and we provide upper and<lb>lower bounds on the rate of convergence. Our proof relies on the geometry of<lb>submodular polyhedra and draws on results from spectral graph theory.",
    "creator" : "LaTeX with hyperref package"
  }
}