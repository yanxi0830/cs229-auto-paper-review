{
  "name" : "1406.2083.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Kernel MMD, the Median Heuristic and Distance Correlation in High Dimensions",
    "authors" : [ "Sashank J. Reddi", "Aaditya Ramdas", "Barnabás Póczos", "Larry Wasserman" ],
    "emails" : [ "sjakkamr@cs.cmu.edu", "aramdas@cs.cmu.edu", "bapoczos@cs.cmu.edu", "aarti@cs.cmu.edu", "larry@stat.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Nonparametric two-sample testing and independence testing are two related problems of paramount importance in statistics. In the former, we have two sets of samples and we would like to determine if these were drawn from the same or different distributions. In the latter, we have one set of samples from a multivariate distribution, and we would like to determine if the joint is the product of marginals or not. The two problems are related because an algorithm for testing the former can be used to test the latter.\nKernel maximum mean discrepancy is a quantity introduced in Gretton et al. [2012a] to tackle the first problem using Reproducing Kernel Hilbert Spaces (RKHSs). In brief, one can embed the two probability distributions as functions in the RKHS, and calculate the squared RKHS-norm of their difference (called the MMD). For characteristic kernels like the Gaussian and Laplace kernels we will later consider, the MMD is zero iff the distributions are equal (see Gretton et al. [2012a]). The corresponding test uses empirical distributions for plug-in estimators (described later) and is consistent (for fixed dimension, power tends to one as number of samples becomes infinite) against any single fixed alternative.\nDistance correlation is a quantity introduced in Székely et al. [2007] to tackle the second problem using distances between pairs of points. The population quantity is a weighted norm of difference between characteristic functions of the joint and product-of-marginal distributions, which is zero if and only if the random variables are independent. Empirically, one can calculate the matrix dot-product between the two pairwise centered distance matrices (one for each random variable) giving a consistent test against any dependent alternative.\nWe will explore the behavior of these related methods when the number of dimensions could be as large as, or larger than the number of samples. We will challenge existing folklore that the “performance” of both tests is unaffected by the underlying\n∗Both student authors had equal contribution.\nar X\niv :1\n40 6.\n20 83\nv1 [\nst at\ndimensionality by explaining the source of both misconceptions. We demonstrate theoretically and experimentally that the power of both these methods actually goes down as d increases relative to n. We will also see explicit examples where the median heuristic for bandwidth selection leads to good power and when it is suboptimal."
    }, {
      "heading" : "2 Summary of Contributions",
      "text" : "Kernel Maximum Mean Discrepancy (MMD) Gretton et al. [2012a] showed that the estimated MMD converges to the true MMD at rate O(n−1/2) independently of dimension d. This gives the impression that the two sample test works well for large d. The result is correct but possibly misleading. We will see that the true value of the population MMD can be polynomially or even exponentially small in d (we were notified that a special case of Corollary 1 was earlier independently noted by Balakrishnan [2013], but do not know other examples). Also, while it is known that MMD2 is smaller than the KL-divergence, for the first time we give several examples where it can be polynomially or exponentially smaller in d than KL. This does indicate (not imply) that the test might have low power, and we indeed experimentally demonstrate that the power against fair alternatives (discussed later) degrades polynomially in d.\nMedian Heuristic A crucial issue when using the Laplace kernel (exp(−‖x−x′‖1/γ)) or the Gaussian kernel (exp(−‖x− x′‖2/2γ2) for MMD is the choice of the associated bandwidth γ. One of the most common heuristic choices for γ in the literature, is to choose it as the median distance between all pairs of points. This is called the median heuristic Gretton et al. [2012a]. We will show an example (separated Gaussian distributions with Gaussian kernel) where if the median heuristic is used, the population MMD will (theoretically) drop to 0 polynomially in d and if the bandwidth is of smaller order than the median distance, then it could drop exponentially. A similar conclusion holds for a second example of same mean Gaussians with different variances. In a third example, separated Laplace distributions with Laplace kernel, if the median heuristic is used, the population MMD will (theoretically) drop to 0 exponentially in d; with a larger bandwidth choice, however, the MMD drops to zero only polynomially in d. All our theoretical predictions are also validated by simulations. In the above examples, choosing the bandwidth optimally to maximize the MMD did also experimentally maximize the power against fair alternatives (it has been noted in Gretton et al. [2012b] that choosing the bandwidth to maximize MMD is sometimes better than the median heuristic). However, in all simulations, power goes to zero as d→∞ for all settings of the bandwidth.\nDistance Correlation (dCor) Székely and Rizzo [2013] studied distance correlation in high dimensions where they considered the following example. (X,Y ) are drawn from a standard normal, and even though they are independent, as dimension is increased (keeping number of samples fixed), the sample test statistic approaches one even though the true dCor is zero. So even though the test statistic is consistent with n increasing and d fixed, in high dimensions its value approaches 1. They thus motivate an unbiased dCor statistic (“udCor”), and show that for the above example, it is well behaved (centered at zero) as d increases. However, this only tells half the story - several other facts also matter for the complete picture. Specifically, the quantity that matters is the power, and the behavior of null and alternate distributions of biased and unbiased test statistics determine their power. We will empirically show that there is no difference in the polynomial decay of power of dCor and udCor against fair alternatives. We also experimentally demonstrate the different reasons that they have low power. To the best of our knowledge, there has been no prior attempt to study the power of distance correlation in the literature, in either low or high dimensions.\nDue to limited space, we only provide a brief introduction to MMD and dCor, and we refer the reader to the aforementioned papers for detailed treatment. We will first get into the details of our results about MMD and the median heuristic (Sec. 3.5), returning to dCor in Sec. 4."
    }, {
      "heading" : "3 The Power of MMD in High Dimensions",
      "text" : "Let P be a class of continuous distributions on topological space X . Our goal is to test"
    }, {
      "heading" : "H0 : p = q against H1 : p 6= q",
      "text" : "where p, q ∈ P We construct a test for the hypothesis from samples (x1, . . . , xn) and (y1, . . . , ym) from distributions p and q, respectively. To do so, one defines a divergence measure ρ(p, q) such that: (a) ρ(p, q) ≥ 0 for all p, q ∈ F and (b) ρ(p, q) = 0 if and only if p = q. We are interested in the high-dimensional regime i.e X ⊆ Rd for large d, possibly larger than n. Most existing non-parametric methods for this problem, like KL divergence, suffer from the curse of dimensionality - if the smoothness of densities p and q doesn’t grow with d, then the estimators generally require exponentially many samples in dimension to obtain a good estimate of the measure (see Birge and Massart [1995], Laurent [1996], Kerkyacharian and Picard\n[1996], Bickel and Ritov [1988], Hero and Michel [1999]). However, there is some folklore that MMD does not suffer from such a curse.\nLet us first introduce some known results before delving into our detailed analysis that will show that this folklore is false and that MMD’s power decays with d against fair alternatives.\nLet F be a class of functions f : X → R. The MMD is defined as:\nMMD(F , p, q) := sup f∈F Ex∼p[f(x)]− Ey∼q[f(y)].\nWe restrict our attention to the case where function class F is a unit ball in a RKHS (H, k) where we assume that k is a bounded, continuous and positive definite kernel function. In this case, it can be shown that MMD(p, q) := ‖µp − µq‖H where µp = Ex∼p[k(x, .)] for any distribution p ∈ P . Furthermore, it is also well-known that MMD(p, q) = 0 iff p = q when we use characteristic kernels Gretton et al. [2012a]. The following is a biased estimator for MMD2 from Gretton et al. [2012a]:\nMMD2b(p, q) = 1\nn2 n∑ i=1 n∑ j=1 k(xi, xj) + 1 m2 m∑ i=1 m∑ j=1 k(yi, yj)− 2 n∑ i=1 m∑ j=1 k(xi, yj).\nA similar unbiased estimator exists without the k(xi, xi), k(yj , yj) terms Gretton et al. [2012a]. The convergence of the above estimator to their respective measures is shown by the following result.\nTheorem 1. Gretton et al. [2012a] Suppose 0 ≤ k(x, x) ≤ K, then with probability at least 1− δ, we have\n|MMD2b(p, q)−MMD 2(p, q)| ≤ 2\n(( K\nn\n)1/2 + ( K\nm\n)1/2)( 1 + log ( 2\nδ\n)) .\nThe unbiased estimator has a very similar convergence rate. Since this rate is independent of dimension, it is sometimes claimed not to suffer from any curse of dimensionality. We will show that this claim is misleading, i.e. hypothesis testing using such quantities can still suffer from the curse."
    }, {
      "heading" : "3.1 The Difficulty of Analytically Characterizing Power",
      "text" : "The power of a test depends on the distribution of the statistic under H0 and H1. If the distributions are nearly Gaussian, the mean statistic and its standard deviation (s.d.) under both H0 (µ0, σ0) and H1 (µ1, σ1) play a role in determining power (the probability mass of the alternate distribution beyond a predetermined α in the right tail of the null distribution). Characterizing the asymptotic (as d is fixed and n goes to infinity) behavior of the test statistic under the null and alternative is usually hard. For example, the above MMD2b estimator has a null distribution which is an infinite weighted sum of chi-squared variables Gretton et al. [2012a]. A different linear-time statistic called MMD2l is unbiased and has an asymptotic normal distribution Gretton et al. [2012a]. However, the associated quantities µ0, σ0, µ1, σ1 actually vary with d and n. Further, in the highdimensional setting, classical large sample theory does not apply as d can be comparable to or larger than n, and calculations assuming the “asymptotically” normal distribution can be misleading.\nSpecifically, consider Q := √ n(MMD2l−MMD\n2) σ1\nN(0, 1) d\n=: Z as shown in Gretton et al. [2012a]. Thus, an asymptotic level α test rejects when MMD2l > σ0zα/ √ n. Under the alternate, the power is\nP (\nMMD2l > σ0zα√ n\n) = P ( Q >\nσ0zα σ1 − √ nMMD2 σ1\n) ≈ P ( Z >\nσ0zα σ1 − νn\n) ,\nwhere νn,d = √ nMMD2/σ1 is the non-centrality parameter. The power will tend to one and the test will be consistent only if νn,d → ∞. Hence, one might be tempted to use νn,d to measure the effectiveness of the test, and indeed choosing the kernel (or bandwidth) to maximize νn,d was studied by Gretton et al. [2012b]. However, in the high dimensional setting the normal approximation used in the last step can be extremely poor, as we have experimentally verified. Development of high dimensional theory, like Barry-Esseen bounds to explicitly characterize the closeness of Q and Z, is needed.\nHence on the issue of power, we will only demonstrate carefully designed experiments showing that MMD does suffer from the curse of dimensionality against reasonable alternatives. We will give two examples where explicit calculations for the population value of MMD2 are possible (not necessarily implying anything about power) and demonstrate that MMD2 can be much smaller than the KL-divergence. These examples will also yield insights into the crucial bandwidth choice.\n3.2 Relating MMD2, TV, KL To simplify our analysis, let us restrict ourselves to translation invariant kernels i.e. for all δ, we have k(x + δ, x′ + δ) = k(x, x′). For these kernels, it is relatively easy to characterize MMD2.\nLemma 1. For translation invariant kernels, there exists a pdf s such that\nMMD2(p, q) = ∫ s(w)|Φp(w)− Φq(w)|2dw,\nwhere Φp,Φq denote the characteristic functions of p, q respectively.\nThe above lemma can be proved using Bochner’s theorem (Appendix). Note that |Φp(w)− Φq(w)| = ∣∣∣∣∫ x exp(iw>x)(p(x)− q(x))dx ∣∣∣∣ ≤ ∫ x |p(x)− q(x)|dx = TV(p, q).\nFrom the fact that |Φp(w)− Φq(w)| ≤ TV(p, q) and Pinsker’s inequality, we can conclude\nLemma 2. For translation invariant kernels, MMD2(p, q) ≤ TV2(p, q) ≤ 2KL(p, q).\nA more general version of the above lemma for all kernels (with a different constant than 1) is presented in Sriperumbudur et al. [2012] (Proposition 5.1). The aforementioned result gives an intuitive justification that, in general, MMD2 is smaller than the other well known non-parametric divergence measures, whose estimators suffer from the curse of dimensionality. We will see that MMD2 can be polynomially, and sometimes exponentially smaller than KL, and while that does not immediately imply lower power, it is an important determining factor. The proofs of the following examples are in the Appendix."
    }, {
      "heading" : "3.3 Example: Gaussian Kernel for Different Mean, Same Covariance Normal Distributions",
      "text" : "Theorem 2. Let µ1, µ2 ∈ Rd. Suppose p : N (µ1,Σ) and q : N (µ2,Σ). Then MMD2 between p and q using a Gaussian kernel with bandwidth γ is,\nMMD2(p, q) = 2\n( γ2\n2\n)d/2 1− exp(−(µ1 − µ2)>(Σ + γ2I/2)−1(µ1 − µ2)/4)\n|Σ + γ2I/2|1/2 .\nSuppose Σ = σ2I . Using Taylor’s theorem for 1− e−x ≈ x and ignoring−x 2\n2 and other smaller remainder terms for clarity, Then the above expression simplifies to\nMMD2(p, q) ≈ ‖µ1 − µ2‖ 2\nγ2(1 + 2σ2/γ2)d/2+1 .\nKeep in mind that the KL-divergence in the case of Σ = σ2I is given by\nKL(p, q) = 1\n2 (µ1 − µ2)TΣ−1(µ1 − µ2) =\n‖µ1 − µ2‖2\n2σ2 ."
    }, {
      "heading" : "3.4 Example: Gaussian Kernel for Same Mean, Different Covariance Normal Distribution",
      "text" : "The next example is for the Gaussian kernel with product Gaussian distributions having the same mean and different variances. Example 3 in Sec. 4.2 of Sriperumbudur et al. [2012] has related calculations, with a different aim that we discuss in Section 3.7.\nTheorem 3. Suppose p : ⊗d−1i=1N (0, σ2)⊗N (0, τ2) and q : ⊗di=1N (0, σ2). Then MMD 2 between q and p using a Gaussian kernel with bandwidth γ is\nMMD2(p, q) ≈ (τ 2 − σ2)2\nγ4(1 + 4σ2/γ2)d/2−1/2 .\nBy Taylor’s theorem for log x, the KL divergence in this case is approximately given by\nKL(p, q) = 1\n2 (tr(Σ−11 Σ0)− d− log(det Σ0/ det Σ1))\n= 1 2 (τ2/σ2 − 1− log(τ2/σ2)) ≈ (τ\n2 − σ2)2\n4σ4 ."
    }, {
      "heading" : "3.5 Bandwidth Choice and the Median Heuristic",
      "text" : "We investigate how bandwidth choice affects the population MMD2 for the example in Theorem 2 (corollaries for Theorem 3 are similar). In what follows, scaling bandwidth choices by a constant does not change the qualitative behavior, so we leave out constants for simplicity. For clarity in the following corollaries, we also ignore the Taylor residuals, and use (1+1/d)d ≈ e for large d.\nUnderestimated bandwidth\nCorollary 1. Suppose Σ = σ2I . If we choose γ = σd1/2− for 0 < ≤ 1/2, then\nMMD2(p, q) ≈ ‖µ1 − µ2‖ 2\nσ2(d1−2 + 2) exp(d2 /2) .\nHence, the population MMD2 goes to zero exponentially fast in d (verified by experiments that follow). The special case of a constant bandwidth with = 1/2 has already been noted by Balakrishnan [2013].\nThe median heuristic. We approximate the choice of the median heuristic by γ2 = E‖xi−xj‖2. Note that when Σ = σ2I , we have E‖xi − xj‖2 ≈ 2σ2d + ‖µ1 − µ2‖2 (the first term dominates, see Sec.3.7 for explanation). Also, the experimental median (Sec.3.7) is exactly of this order. Corollary 2. Suppose Σ = σ2I . If we choose γ = σ √ d, then\nMMD2(p, q) ≈ ‖µ1 − µ2‖ 2\nσ2(d+ 2)e .\nNote the population MMD2 goes to zero polynomially as 1/d. This is the largest MMD value one can hope for, but it is still smaller than the KL divergence by a factor of 1/d.\nOverestimating the bandwidth\nCorollary 3. Suppose Σ = σ2I . If γ = σd1/2+ for > 0, then\nMMD2(p, q) ≈ ‖µ1 − µ2‖ 2\nσ2(d1+2 + 2) exp(1/2d2 ) .\nHence, the population MMD2 goes to zero polynomially as 1/d1+2 , since exp(1/2d2 ) ≈ 1 for large d. So one pays very little for overestimating the bandwidth, compared to underestimating it."
    }, {
      "heading" : "3.6 Example : Laplace Kernel for Different Mean, Same Variance Laplace Distributions",
      "text" : "Theorem 4 (MMD2 Approximation). Let µ1, µ2 ∈ Rd. Suppose p : ⊗iLaplace(µ1,i, σ) and q : ⊗iLaplace(µ2,i, σ). Using a Laplace kernel with bandwidth γ, we have\nMMD2(p, q) ≈ ‖µ1 − µ2‖ 2\n2σγ (1 + σ/γ) d .\nNote that by applying Taylor’s theorem for e−x ≈ 1− x+ x2/2, we have\nKL(p, q) = e− ‖µ1−µ2‖ σ − 1 + ‖µ1−µ2‖σ ≈ ‖µ1 − µ2‖2\n2σ2 .\nOnce again, E‖xi − xj‖2 ≈ 2σ2d and indeed experimentally the median heuristic chooses γ ≈ σ √ d. This time, the median heuristic is suboptimal and MMD2 drops exponentially in d. A larger bandwidth of γ = σd is optimal, making the denominator ≈ σ2de. An overestimated bandwidth again leads to only a slow polynomial drop in MMD. In summary: Corollary 4 (Underestimated bandwidth, median heuristic). If we choose γ = σd1− for 0 < < 1,\nMMD2(p, q) ≈ ‖µ1 − µ2‖ 2\n2σ2d1− exp(d ) .\nCorollary 5 (Correct or Overestimated bandwidth). If we choose γ = σd1+ , for ≥ 0\nMMD2(p, q) ≈ ‖µ1 − µ2‖ 2\n2σ2d1+ exp(1/d ) .\n3.7 MMD2 Power for the mean-separated Gaussian and Laplace examples The null hypothesis is either chosen as p = q = N (0, σ2I) or p = q = ⊗di=1Laplace(0, σ). For the alternative, p is the same and we choose q = N(µ, σ2I) or q = ⊗di=1Laplace(µi, σ). The choice of µ is subtle - any effect on power should not arise from the unfair choice of alternative. We choose to keep ‖µ‖2/σ2 constant, for example by setting µ = (1, 0, ..., ) for all d. This can be justified by :\n• The KL divergence between the two distributions equals (or scales like) ‖µ‖2/2σ2 in both cases, and hence by keeping the KL constant with d, we are not making it information theoretically harder or easier to distinguish the hypotheses as d grows.\n• This quantity represents the Mahalanobis distance µTΣ−1µ which is considered as signal-to-noise-ratio, and stays constant with d.\nFig. 1 does confirm that power drops with dimension in both settings. The experiments in the Appendix of Gretton et al. [2012b] do use (alas, with no justification) our fair choice of alternatives, and they also observe decaying power with d (represented in terms of type 2 error). We contrast this with the choice of Fig. 3 in Sriperumbudur et al. [2012], which was not a power study but an empirical study of convergence rates for estimation of MMD2, where the authors choose to let the mean separation be (1, 1, 1, ..., 1) which makes the problem easier with dimension. Also, they use it to argue that the mean squared error (as summarised by Thm 1) with increasing n is indeed independent of d. Another relevant comparison is with Fig. 5A in Gretton et al. [2012a] where they show an extremely slow decrease in power with dimension for the same example of mean-shifted Gaussians with Gaussian kernel that we consider. Since the details are not in the paper, it was verified by personal communication that the bandwidth was chosen to maximize MMD, but that the means we chosen such that ‖µ1−µ2‖ equaled d.\nWe also ran a simulation to verify that our derived expressions and approximations for MMD2 are accurate. Fig 3 in the Appendix shows the results."
    }, {
      "heading" : "4 The Power of Distance Correlation (dCor) in high dimensions",
      "text" : "Now we discuss nonparametric independence testing. Given n samples (xi, yi) ∈ Rdx+dy (dx 6= dy is allowed) drawn from a joint distribution PXY with marginals PX , PY , we would like to test\nH0 : PXY = PXPY against H1 : PXY 6= PXPY .\nThe authors of Székely et al. [2007] introduce a test statistic called (squared) distance covariance which is defined as\ndCov2n(X,Y ) = 1\nn2 tr(ÃB̃) =\n1\nn2 n∑ i,j=1 ÃijB̃ij . (1)\nHere, Ã = HAH, B̃ = HBH where H = I − 11T /n is a centering matrix, and A,B are distance matrices for X,Y respectively, i.e. Aij = ‖xi − xj‖, Bij = ‖yi − yj‖. One can use other negative definite metrics instead of Euclidean norms to generalize the definition to metric spaces Lyons [2013]. The above expression is different from the presentation in the original papers (but mathematically equivalent). They then define (squared) distance correlation as the normalized version of dCov2:\ndCor2n(X,Y ) = dCov2n(X,Y )√\ndCov2n(X,X)dCov 2 n(Y, Y )\n.\ndCor2n is always between [0, 1], and unlike correlation, the population dCor 2 = 0 iff X,Y are independent, and Székely et al. [2007] proves it is consistent against any fixed alternatives (with finite second moments). There is an interesting connection with MMD2 which justifies its appearance in this paper. The MMD2 between µPXY and µPX×PY is called HSIC (see Gretton et al. [2005]), which has the sample expression:\nHSICn = 1\nn2 tr(K̃L̃) =\n1\nn2 n∑ i,j=1 K̃ijL̃ij , (2)\nwhere K̃ = HKH, L̃ = HLH , H is defined as before and K,L are kernel matrices i.e. Kij = k(xi, xj), Lij = l(yi, yj). The striking similarity between Eqs.(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov2. Hence, dCor2 and MMD are very strongly related quantities, for very related problems.\nIn Székely and Rizzo [2013], the authors advocate the use of a modified unbiased dCor (called udCor), claiming that it works well for large d. We will describe experiments that demonstrate that dCor2 and udCor2 as test statistics both suffer in high d, but for a slightly different reason than for MMD2.\nWhen (X,Y ) are drawn from a standard Gaussian, the authors of Székely and Rizzo [2013] show that the biased dCorn → 1, if n is kept fixed and dx, dy are increased. Then, they show that their unbiased udCorn hovers around 0 in the same situation (even when dx, dy n), and conclude that it performs well in high-dimensions. The bias is indeed zero, but we argue that the variance of udCorn remains the same order as dCorn. Székely and Rizzo [2013] show that when the null is true, udCorn is well behaved. However, the right followup question to ask is - when the alternate hypothesis is true, how does the statistic behave in high dimensions? Below we demonstrate that in this case it fails to detect such dependence in high dimensions, i.e. its power goes to zero."
    }, {
      "heading" : "4.1 Experimental Verification",
      "text" : "Here we carefully design a simple simulation experiment to demonstrate this decrease in power with dimension, with some subtleties in choice of the alternative hypothesis that are quite crucial. For the null hypothesis of independent variables, we let (x, y) be sampled from a d-dimensional standard normal, like in Székely and Rizzo [2013]. For the alternative hypothesis, we need to make a choice about how to change the covariance matrix, such that as d increases, the problem neither gets easier nor harder. We choose to make a constant number of off-diagonal elements non-zero, i.e. we don’t change the number or value of non-zero off-diagonal elements as d increases. The marginals are still standard Gaussians; the non-zero elements are only in the cross-diagonal blocks indicating dependence between X ,Y .\nOne can argue that a constant number of non-zeros (not growing with d) is the fairest choice, which does not increase/decrease (with d) the amount of information provided to the statistician:\n1. All the information to decide between null and alternate is captured in the covariance matrix. From classical information theory, the Gaussian entropy log det Σ is the amount of information encoded in Σ, which (with our choice) remains constant as d increases.\n2. Another information theoretic quantity of relevance is the mutual information (MI) between X,Y . Since the MI between Gaussians is given by log det Σdet ΣX det ΣY , one can easily check that the mutual information between X,Y stays constant as dimension increases.\n3. All one is trying to do is differentiate Σ from I , so ‖Σ − I‖2F is also a measure of difficulty of the problem. Even if the statistician detects dependence caused by a single off-diagonal element, he will reject the null. With our choice, ‖Σ− I‖2F does stay constant with d.\nThis is similar in spirit to the MMD2 case, where we justified our alternative by verifying that the signal to noise ratio and the KL-divergence weren’t changing with d. Figure 2 provides a detailed analysis of the power of dCor, udCor. P0, P1 represent the distributions of the corresponding test statistic (possibly not normally distributed) under H0 and H1 and µ0, σ0, µ1, σ1 are their mean and standard deviation. The explanations in the figure subtext bring out the complicated scenario of µ0, µ1, σ0, σ1 all changing with d - i.e. P0, P1 change with d."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In summary, we believe that we have made a strong case for the first time that the power of the closely related kernel and distance based tests both suffer from the curse of dimensionality against fair alternatives. In the process, we also undertook a detailed study of bandwidth choices and explicitly demonstrated cases when and why the median heuristic works and fails, and made a case for overestimating the bandwidth. The reasons for the observed power decay can be complicated, and a better theory is necessary to understand the null and alternate distributions in high dimensions."
    }, {
      "heading" : "A Proof of Lemma 1",
      "text" : "Proof. From definition of MMD2, we have\nMMD2(p, q) = ∫ x,x′ k(x, x′)p(x)p(x′)dxdx′ + ∫ x,x′ k(x, x′)q(x)q(x′)dxdx′ − 2 ∫ x,x′ k(x, x′)p(x)q(x′)dxdx′.\nFrom Bochner’s theorem (see [Rudin, 1962]) for translation invariant kernels, we know k(x, x′) = ∫ w s(w)eiw >xe−iw >x′dw where s is the fourier transform of the kernel. Substituting the above equality in the definition of MMD2, we have the required result."
    }, {
      "heading" : "B Proof of Theorem 2",
      "text" : "Proof. Since Gaussian kernel is a translation invariant kernel, we can use Lemma 1 to derive the MMD2 in this case. It is well-known that the Fourier transform s(w) of Gaussian kernel is Gaussian distribution. Substituting the characteristic function of normal distribution in Lemma 1, we have\nMMD2(p, q) = ∫ w ( γ2/2π )d/2 exp ( −γ2‖w‖2/2 ) ∣∣exp(iµ>1 w − w>Σw/2)− exp(iµ>1 w − w>Σw/2)∣∣2 dw = ( γ2/2π\n)d/2 ∫ w exp ( −w>Σw ) exp ( −γ2‖w‖2/2 ) ∣∣exp(iµ>1 w)− exp(iµ>2 w)∣∣2 dw = ( γ2/2π\n)d/2 ∫ w exp ( −w>(Σ + γ2I/2)w ) ( 2− exp ( −i(µ1 − µ2)>w ) − exp ( −i(µ2 − µ1)>w )) dw\n= 2 ( γ2/2π )d/2 ∫ w exp ( −w>(Σ + γ2I/2)w ) ( 1− exp ( −i(µ1 − µ2)>w )) dw (3)\nThe third step follows from definition of complex conjugate. In what follows, we do the following change of variable u = (Σ + γ2I/2)1/2w. Consider the following term:∫\nw\nexp ( −w>(Σ + γ2I/2)w ) exp ( −i(µ1 − µ2)>w ) dw\n= ∫ u exp− ( u>u+ i(µ1 − µ2)>(Σ + γ2I/2)−1/2u ) |Σ + γ2I/2|−1/2du = |Σ + γ2I/2|−1/2 exp(−(µ1 − µ2)>(Σ + γ2I/2)−1(µ1 − µ2)/4)×∫ u exp− ( ‖u− i(Σ + γ2I/2)−1/2(µ1 − µ2)/2‖2 ) du = πd/2|Σ + γ2I/2|−1/2 exp(−(µ1 − µ2)>(Σ + γ2I/2)−1(µ1 − µ2)/4)\nThe second step follows from well-known theory of change of variables (see Theorem 263D of Fremlin [2000]). By substituting the above equality in Equation 3, we get the required result."
    }, {
      "heading" : "C Proof of Proposition 1",
      "text" : "Proposition 1. Suppose λ 6= σ, then we have,\n∞∫ −∞ exp ( −|x− λ| γ ) exp ( −|x| σ ) dx =\ne−|λ|/σ\n1/γ + 1/σ +\ne−|λ|/γ\n1/σ − 1/γ − e\n−|λ|/σ\n1/σ − 1/γ +\ne−|λ|/γ\n1/γ + 1/σ\nand when λ = σ, we have,\n∞∫ −∞ exp ( −|x− λ| σ ) exp ( −|x| σ ) dx =\ne−|λ|/σ\n1/γ + 1/σ + |λ|e−|λ|/σ + e\n−|λ|/γ\n1/γ + 1/σ\nProof. We show this when λ ≤ 0 as an example proof:\n∞∫ −∞ exp ( −|x− λ| γ ) exp ( −|x| σ ) dx = λ∫ −∞ exp ( x− λ γ ) exp (x σ ) dx+ 0∫ λ exp ( λ− x γ ) exp (x σ ) dx\n+ ∞∫ 0 exp ( λ− x γ ) exp ( −x σ ) dx\n= e−λ/γeλ/σ+λ/γ 1/γ + 1/σ + e−λ/γ(1− e−λ/γ+λ/σ) 1/σ − 1/γ +\neλ/γ\n1/γ + 1/σ\nAlso, when γ = σ, we obtain the same expression for the first and last terms. However, the middle term has the following constant integrand, thereby, leading to the required expression.\n0∫ λ exp ( λ− x γ ) exp (x σ ) dx = |λ|e−|λ|/σ."
    }, {
      "heading" : "D Proof of Proposition 2",
      "text" : "Proposition 2. Let ψ = σ/γ. Then we have,\n∞∫ −∞ ∞∫ −∞ exp ( −|x− x ′| γ ) 1 4σ2 exp ( −|x− µ| σ ) exp ( −|x ′| σ ) dxdx′\n= −1 2 e−|µ|/σ\n( ψ + |µ|/γ\n1− ψ2\n) +\n1\n1− ψ2\n( −ψe −|µ|/σ\n1− ψ2 + e−|µ|/γ 1− ψ2 ) = − µ 2\n4σγ(1 + ψ)2 +\n2 + ψ\n2(1 + ψ)2 +O\n( |µ|3\nσ2γ(1− ψ2)2\n) −O ( |µ|3\nγ3(1− ψ2)2 ) Proof. We first integrate with respect to x′ using the Proposition 1 to get\n1\n4σ2 ∞∫ −∞ ( e−|x|/σ 1/γ + 1/σ + e−|x|/γ 1/σ − 1/γ − e −|x|/σ 1/σ − 1/γ + e−|x|/γ 1/γ + 1/σ ) exp ( −|x− µ| σ ) dx\nWe then integrate these terms once again using both parts of Proposition 1 to get the first equality. We simplify the second\nequation in the following manner:\n−1 2 e−|µ|/σ\n( ψ + |µ|/γ\n1− ψ2\n) +\n1\n1− ψ2\n( −ψe −|µ|/σ\n1− ψ2 + e−|µ|/γ 1− ψ2 ) = −1\n2\n( 1− |µ|\nσ + |µ|2 2σ2\n)( ψ + |µ|/γ\n1− ψ2\n) +\n1\n1− ψ2\n( − (σ/γ − |µ|/γ + µ 2/2σγ)\n1− ψ2 +\n1− |µ|/γ + µ2/2γ2\n1− ψ2 ) +O ( |µ|3\nσ2γ(1− ψ2)2\n) −O ( |µ|3\nγ3(1− ψ2)2 ) = − 1\n2(1− ψ2)\n( ψ − µ 2\n2σγ + |µ|3 2σ2γ\n) +\n1\n(1− ψ2)2\n( 1− ψ − µ 2\n2σγ +\nµ2\n2γ2 ) +O ( |µ|3\nσ2γ(1− ψ2)2\n) −O ( |µ|3\nγ3(1− ψ2)2 ) = − 1\n2(1− ψ2)\n( ψ − µ 2\n2σγ\n) +\n(1− µ2/2σγ)(1− ψ) (1− ψ2)2 +O\n( |µ|3\nσ2γ(1− ψ2)2\n) −O ( |µ|3\nγ3(1− ψ2)2 ) = 1\n1− ψ2\n( −ψ\n2 +\n1\n2\nµ2\n2σγ\n) +\n1\n1− ψ2\n( 1\n1 + ψ − µ\n2\n(1 + ψ)2σγ\n) +O ( |µ|3\nσ2γ(1− ψ2)2\n) −O ( |µ|3\nγ3(1− ψ2)2 ) = − µ 2\n4σγ(1 + ψ)2 +\n2 + ψ\n2(1 + ψ)2 +O\n( |µ|3\nσ2γ(1− ψ2)2\n) −O ( |µ|3\nγ3(1− ψ2)2\n)"
    }, {
      "heading" : "E Proof of Theorem 4",
      "text" : "Proof. Recall that we use Laplace kernel, i.e., k(x, x′) = exp(−‖x− x′‖1/γ). By using the definition of MMD2, we have\nMMD2 = ∫ x,x′ (p(x)p(x′) + q(x)q(x′)− 2p(x)q(x′))k(x, x′)dxdx′. (4)\nConsider the term ∫ x,x′\np(x)q(x′)k(x, x′)dxdx′. The other terms can be calculated in a similar manner. Let ψ = σ/γ and β = (1 + ψ/2)/(1 + ψ)2. We have,∫\nx,x′ p(x)q(x′)k(x, x′)dxdx′ = d∏ i=1 ∫ xi,x′i exp ( −|x− x ′| γ ) 1 4σ2 exp ( −|x− µ| σ ) exp ( −|x ′| σ ) dxidx)i ′\n= d∏ i=1 β ( 1− µ 2 i 4βσγ(1 + ψ)2 +O ( |µi|3 βσ2γ(1− ψ2)2 ) −O ( |µi|3 βγ3(1− ψ2)2 )) = βd ( 1− ‖µ‖ 2\n4βσγ(1 + ψ) +O\n( |µi|3\nβσ2γ(1− ψ2)2\n) −O ( |µi|3\nβγ3(1− ψ2)2 )) The first step follows from the fact that both Laplace kernel and Laplace distribution decompose over the coordinates. The second step follows from Proposition 2. Substituting the above expression in Equation 4, we get,\nMMD2 = βd−1‖µ‖2\n2σγ(1 + ψ) −O\n( βd−1‖µ‖33\nσ2γ(1− ψ2)2\n) +O ( βd−1‖µ‖33 γ3(1− ψ2)2 ) .\nF Verifying MMD approximations"
    }, {
      "heading" : "G Biased MMD for Gaussian Distribution",
      "text" : "H Verifying Power Plots decay polynomially\nI Standard deviation of udCor, dCor"
    }, {
      "heading" : "J MMD between Gaussians with same mean, different variances",
      "text" : "Suppose P = ⊗di=1N(0, σ2) ⊗ N(0, a2) and Q = ⊗di=1N(0, σ2) ⊗ N(0, b2). If a, b are of the same order as σ then the median heuristic will still pick γ ≈ σ √ d for bandwidth γ of the Gaussian kernel. First we note that for distributions with the same mean, by Taylor’s theorem,\nKL(P,Q) = 1\n2 (tr(Σ−11 Σ0 − d− log(det Σ0)/det Σ1)) =\n1 2 (a2/b2 − 1− log(a2/b2))\n≈ (a 2/b2 − 1)2\n4\nThe MMD2 can be derived (approximated using (1 + x)n ≈ 1 + nx for small x) as\n1\n(1 + 4σ2/γ2)d/2−1/2\n( 1√\n1 + 4a2/γ2 + 1√ 1 + 4b2/γ2 − 2√ 1 + 2(a2 + b2)/γ2\n)\n≈ 1 (1 + 4σ2/γ2)d/2−1/2\n( 1\n1 + 2a2/γ2 +\n1 1 + 2b2/γ2 − 2 1 + (a2 + b2)/γ2\n)\n≈ 1 (1 + 4σ2/γ2)d/2−1/2\n( 1√\n1 + 2a2/γ2 − 1√ 1 + 2b2/γ2 )2 ≈ 1\n(1 + 4σ2/γ2)d/2−1/2 ( (1− a2/γ2)− (1− b2/γ2) )2 = b4/γ4\n(1 + 4σ2/γ2)d/2−1/2 (a2/b2 − 1)2\nIf γ is chosen by the median heuristic (optimal in this case), we see that this is smaller than KL by σ4d2e/b4. If it is chosen as constant, it can be exponentially smaller than KL."
    } ],
    "references" : [ {
      "title" : "Finding and Leveraging Structure in Learning Problems",
      "author" : [ "S. Balakrishnan" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Balakrishnan.,? \\Q2013\\E",
      "shortCiteRegEx" : "Balakrishnan.",
      "year" : 2013
    }, {
      "title" : "Estimating integrated squared density derivatives: sharp best order of convergence estimates",
      "author" : [ "P. Bickel", "Y. Ritov" ],
      "venue" : "Sankhyā: The Indian Journal of Statistics,",
      "citeRegEx" : "Bickel and Ritov.,? \\Q1988\\E",
      "shortCiteRegEx" : "Bickel and Ritov.",
      "year" : 1988
    }, {
      "title" : "Estimation of integral functionals of a density",
      "author" : [ "L. Birge", "P. Massart" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Birge and Massart.,? \\Q1995\\E",
      "shortCiteRegEx" : "Birge and Massart.",
      "year" : 1995
    }, {
      "title" : "Measuring statistical dependence with Hilbert-Schmidt norms",
      "author" : [ "A. Gretton", "O. Bousquet", "A. Smola", "B. Schölkopf" ],
      "venue" : "In Proceedings of Algorithmic Learning Theory,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2005
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "A. Gretton", "K. Borgwardt", "M. Rasch", "B. Schoelkopf", "A. Smola" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimal kernel choice for large-scale two-sample tests",
      "author" : [ "A. Gretton", "B. Sriperumbudur", "D. Sejdinovic", "H. Strathmann", "S. Balakrishnan", "M. Pontil", "K. Fukumizu" ],
      "venue" : "Neural Information Processing Systems,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2012
    }, {
      "title" : "Estimation of Rényi information divergence via pruned minimal spanning trees",
      "author" : [ "A. Hero", "O. Michel" ],
      "venue" : "In Higher-Order Statistics,",
      "citeRegEx" : "Hero and Michel.,? \\Q1999\\E",
      "shortCiteRegEx" : "Hero and Michel.",
      "year" : 1999
    }, {
      "title" : "Estimating nonquadratic functionals of a density using haar wavelets",
      "author" : [ "G. Kerkyacharian", "D. Picard" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Kerkyacharian and Picard.,? \\Q1996\\E",
      "shortCiteRegEx" : "Kerkyacharian and Picard.",
      "year" : 1996
    }, {
      "title" : "Efficient estimation of integral functionals of a density",
      "author" : [ "B. Laurent" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Laurent.,? \\Q1996\\E",
      "shortCiteRegEx" : "Laurent.",
      "year" : 1996
    }, {
      "title" : "Distance covariance in metric spaces",
      "author" : [ "R. Lyons" ],
      "venue" : "Annals of Probability,",
      "citeRegEx" : "Lyons.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lyons.",
      "year" : 2013
    }, {
      "title" : "Fourier analysis on groups",
      "author" : [ "W. Rudin" ],
      "venue" : null,
      "citeRegEx" : "Rudin.,? \\Q1962\\E",
      "shortCiteRegEx" : "Rudin.",
      "year" : 1962
    }, {
      "title" : "Equivalence of distance-based and RKHS-based statistics in hypothesis testing",
      "author" : [ "D. Sejdinovic", "B. Sriperumbudur", "A. Gretton", "K. Fukumizu" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Sejdinovic et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sejdinovic et al\\.",
      "year" : 2013
    }, {
      "title" : "On the empirical estimation of integral probability metrics",
      "author" : [ "B. Sriperumbudur", "K. Fukumizu", "A. Gretton", "B. Schoelkopf", "G. Lanckriet" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "Sriperumbudur et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sriperumbudur et al\\.",
      "year" : 2012
    }, {
      "title" : "The distance correlation t-test of independence in high dimension",
      "author" : [ "G.J. Székely", "M.L. Rizzo" ],
      "venue" : "J. Multivariate Analysis,",
      "citeRegEx" : "Székely and Rizzo.,? \\Q2013\\E",
      "shortCiteRegEx" : "Székely and Rizzo.",
      "year" : 2013
    }, {
      "title" : "Measuring and testing dependence by correlation of distances",
      "author" : [ "G.J. Székely", "M.L. Rizzo", "Bakirov N.K" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Székely et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Székely et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Kernel maximum mean discrepancy is a quantity introduced in Gretton et al. [2012a] to tackle the first problem using Reproducing Kernel Hilbert Spaces (RKHSs).",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "Kernel maximum mean discrepancy is a quantity introduced in Gretton et al. [2012a] to tackle the first problem using Reproducing Kernel Hilbert Spaces (RKHSs). In brief, one can embed the two probability distributions as functions in the RKHS, and calculate the squared RKHS-norm of their difference (called the MMD). For characteristic kernels like the Gaussian and Laplace kernels we will later consider, the MMD is zero iff the distributions are equal (see Gretton et al. [2012a]).",
      "startOffset" : 60,
      "endOffset" : 483
    }, {
      "referenceID" : 3,
      "context" : "Kernel maximum mean discrepancy is a quantity introduced in Gretton et al. [2012a] to tackle the first problem using Reproducing Kernel Hilbert Spaces (RKHSs). In brief, one can embed the two probability distributions as functions in the RKHS, and calculate the squared RKHS-norm of their difference (called the MMD). For characteristic kernels like the Gaussian and Laplace kernels we will later consider, the MMD is zero iff the distributions are equal (see Gretton et al. [2012a]). The corresponding test uses empirical distributions for plug-in estimators (described later) and is consistent (for fixed dimension, power tends to one as number of samples becomes infinite) against any single fixed alternative. Distance correlation is a quantity introduced in Székely et al. [2007] to tackle the second problem using distances between pairs of points.",
      "startOffset" : 60,
      "endOffset" : 785
    }, {
      "referenceID" : 2,
      "context" : "2 Summary of Contributions Kernel Maximum Mean Discrepancy (MMD) Gretton et al. [2012a] showed that the estimated MMD converges to the true MMD at rate O(n−1/2) independently of dimension d.",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "We will see that the true value of the population MMD can be polynomially or even exponentially small in d (we were notified that a special case of Corollary 1 was earlier independently noted by Balakrishnan [2013], but do not know other examples).",
      "startOffset" : 195,
      "endOffset" : 215
    }, {
      "referenceID" : 3,
      "context" : "This is called the median heuristic Gretton et al. [2012a]. We will show an example (separated Gaussian distributions with Gaussian kernel) where if the median heuristic is used, the population MMD will (theoretically) drop to 0 polynomially in d and if the bandwidth is of smaller order than the median distance, then it could drop exponentially.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "This is called the median heuristic Gretton et al. [2012a]. We will show an example (separated Gaussian distributions with Gaussian kernel) where if the median heuristic is used, the population MMD will (theoretically) drop to 0 polynomially in d and if the bandwidth is of smaller order than the median distance, then it could drop exponentially. A similar conclusion holds for a second example of same mean Gaussians with different variances. In a third example, separated Laplace distributions with Laplace kernel, if the median heuristic is used, the population MMD will (theoretically) drop to 0 exponentially in d; with a larger bandwidth choice, however, the MMD drops to zero only polynomially in d. All our theoretical predictions are also validated by simulations. In the above examples, choosing the bandwidth optimally to maximize the MMD did also experimentally maximize the power against fair alternatives (it has been noted in Gretton et al. [2012b] that choosing the bandwidth to maximize MMD is sometimes better than the median heuristic).",
      "startOffset" : 36,
      "endOffset" : 965
    }, {
      "referenceID" : 3,
      "context" : "This is called the median heuristic Gretton et al. [2012a]. We will show an example (separated Gaussian distributions with Gaussian kernel) where if the median heuristic is used, the population MMD will (theoretically) drop to 0 polynomially in d and if the bandwidth is of smaller order than the median distance, then it could drop exponentially. A similar conclusion holds for a second example of same mean Gaussians with different variances. In a third example, separated Laplace distributions with Laplace kernel, if the median heuristic is used, the population MMD will (theoretically) drop to 0 exponentially in d; with a larger bandwidth choice, however, the MMD drops to zero only polynomially in d. All our theoretical predictions are also validated by simulations. In the above examples, choosing the bandwidth optimally to maximize the MMD did also experimentally maximize the power against fair alternatives (it has been noted in Gretton et al. [2012b] that choosing the bandwidth to maximize MMD is sometimes better than the median heuristic). However, in all simulations, power goes to zero as d→∞ for all settings of the bandwidth. Distance Correlation (dCor) Székely and Rizzo [2013] studied distance correlation in high dimensions where they considered the following example.",
      "startOffset" : 36,
      "endOffset" : 1200
    }, {
      "referenceID" : 2,
      "context" : "Most existing non-parametric methods for this problem, like KL divergence, suffer from the curse of dimensionality - if the smoothness of densities p and q doesn’t grow with d, then the estimators generally require exponentially many samples in dimension to obtain a good estimate of the measure (see Birge and Massart [1995], Laurent [1996], Kerkyacharian and Picard",
      "startOffset" : 301,
      "endOffset" : 326
    }, {
      "referenceID" : 2,
      "context" : "Most existing non-parametric methods for this problem, like KL divergence, suffer from the curse of dimensionality - if the smoothness of densities p and q doesn’t grow with d, then the estimators generally require exponentially many samples in dimension to obtain a good estimate of the measure (see Birge and Massart [1995], Laurent [1996], Kerkyacharian and Picard",
      "startOffset" : 301,
      "endOffset" : 342
    }, {
      "referenceID" : 1,
      "context" : "[1996], Bickel and Ritov [1988], Hero and Michel [1999]).",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "[1996], Bickel and Ritov [1988], Hero and Michel [1999]).",
      "startOffset" : 8,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "[1996], Bickel and Ritov [1988], Hero and Michel [1999]). However, there is some folklore that MMD does not suffer from such a curse. Let us first introduce some known results before delving into our detailed analysis that will show that this folklore is false and that MMD’s power decays with d against fair alternatives. Let F be a class of functions f : X → R. The MMD is defined as: MMD(F , p, q) := sup f∈F Ex∼p[f(x)]− Ey∼q[f(y)]. We restrict our attention to the case where function class F is a unit ball in a RKHS (H, k) where we assume that k is a bounded, continuous and positive definite kernel function. In this case, it can be shown that MMD(p, q) := ‖μp − μq‖H where μp = Ex∼p[k(x, .)] for any distribution p ∈ P . Furthermore, it is also well-known that MMD(p, q) = 0 iff p = q when we use characteristic kernels Gretton et al. [2012a]. The following is a biased estimator for MMD from Gretton et al.",
      "startOffset" : 8,
      "endOffset" : 851
    }, {
      "referenceID" : 1,
      "context" : "[1996], Bickel and Ritov [1988], Hero and Michel [1999]). However, there is some folklore that MMD does not suffer from such a curse. Let us first introduce some known results before delving into our detailed analysis that will show that this folklore is false and that MMD’s power decays with d against fair alternatives. Let F be a class of functions f : X → R. The MMD is defined as: MMD(F , p, q) := sup f∈F Ex∼p[f(x)]− Ey∼q[f(y)]. We restrict our attention to the case where function class F is a unit ball in a RKHS (H, k) where we assume that k is a bounded, continuous and positive definite kernel function. In this case, it can be shown that MMD(p, q) := ‖μp − μq‖H where μp = Ex∼p[k(x, .)] for any distribution p ∈ P . Furthermore, it is also well-known that MMD(p, q) = 0 iff p = q when we use characteristic kernels Gretton et al. [2012a]. The following is a biased estimator for MMD from Gretton et al. [2012a]:",
      "startOffset" : 8,
      "endOffset" : 924
    }, {
      "referenceID" : 3,
      "context" : "A similar unbiased estimator exists without the k(xi, xi), k(yj , yj) terms Gretton et al. [2012a]. The convergence of the above estimator to their respective measures is shown by the following result.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "A similar unbiased estimator exists without the k(xi, xi), k(yj , yj) terms Gretton et al. [2012a]. The convergence of the above estimator to their respective measures is shown by the following result. Theorem 1. Gretton et al. [2012a] Suppose 0 ≤ k(x, x) ≤ K, then with probability at least 1− δ, we have",
      "startOffset" : 76,
      "endOffset" : 236
    }, {
      "referenceID" : 3,
      "context" : "For example, the above MMDb estimator has a null distribution which is an infinite weighted sum of chi-squared variables Gretton et al. [2012a]. A different linear-time statistic called MMDl is unbiased and has an asymptotic normal distribution Gretton et al.",
      "startOffset" : 121,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "For example, the above MMDb estimator has a null distribution which is an infinite weighted sum of chi-squared variables Gretton et al. [2012a]. A different linear-time statistic called MMDl is unbiased and has an asymptotic normal distribution Gretton et al. [2012a]. However, the associated quantities μ0, σ0, μ1, σ1 actually vary with d and n.",
      "startOffset" : 121,
      "endOffset" : 268
    }, {
      "referenceID" : 3,
      "context" : "For example, the above MMDb estimator has a null distribution which is an infinite weighted sum of chi-squared variables Gretton et al. [2012a]. A different linear-time statistic called MMDl is unbiased and has an asymptotic normal distribution Gretton et al. [2012a]. However, the associated quantities μ0, σ0, μ1, σ1 actually vary with d and n. Further, in the highdimensional setting, classical large sample theory does not apply as d can be comparable to or larger than n, and calculations assuming the “asymptotically” normal distribution can be misleading. Specifically, consider Q := √ n(MMD2l−MMD ) σ1 N(0, 1) d =: Z as shown in Gretton et al. [2012a]. Thus, an asymptotic level α test rejects when MMDl > σ0zα/ √ n.",
      "startOffset" : 121,
      "endOffset" : 660
    }, {
      "referenceID" : 3,
      "context" : "Hence, one might be tempted to use νn,d to measure the effectiveness of the test, and indeed choosing the kernel (or bandwidth) to maximize νn,d was studied by Gretton et al. [2012b]. However, in the high dimensional setting the normal approximation used in the last step can be extremely poor, as we have experimentally verified.",
      "startOffset" : 160,
      "endOffset" : 183
    }, {
      "referenceID" : 12,
      "context" : "A more general version of the above lemma for all kernels (with a different constant than 1) is presented in Sriperumbudur et al. [2012] (Proposition 5.",
      "startOffset" : 109,
      "endOffset" : 137
    }, {
      "referenceID" : 12,
      "context" : "2 of Sriperumbudur et al. [2012] has related calculations, with a different aim that we discuss in Section 3.",
      "startOffset" : 5,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "The special case of a constant bandwidth with = 1/2 has already been noted by Balakrishnan [2013].",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "The experiments in the Appendix of Gretton et al. [2012b] do use (alas, with no justification) our fair choice of alternatives, and they also observe decaying power with d (represented in terms of type 2 error).",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "The experiments in the Appendix of Gretton et al. [2012b] do use (alas, with no justification) our fair choice of alternatives, and they also observe decaying power with d (represented in terms of type 2 error). We contrast this with the choice of Fig. 3 in Sriperumbudur et al. [2012], which was not a power study but an empirical study of convergence rates for estimation of MMD, where the authors choose to let the mean separation be (1, 1, 1, .",
      "startOffset" : 35,
      "endOffset" : 286
    }, {
      "referenceID" : 3,
      "context" : "The experiments in the Appendix of Gretton et al. [2012b] do use (alas, with no justification) our fair choice of alternatives, and they also observe decaying power with d (represented in terms of type 2 error). We contrast this with the choice of Fig. 3 in Sriperumbudur et al. [2012], which was not a power study but an empirical study of convergence rates for estimation of MMD, where the authors choose to let the mean separation be (1, 1, 1, ..., 1) which makes the problem easier with dimension. Also, they use it to argue that the mean squared error (as summarised by Thm 1) with increasing n is indeed independent of d. Another relevant comparison is with Fig. 5A in Gretton et al. [2012a] where they show an extremely slow decrease in power with dimension for the same example of mean-shifted Gaussians with Gaussian kernel that we consider.",
      "startOffset" : 35,
      "endOffset" : 698
    }, {
      "referenceID" : 14,
      "context" : "The authors of Székely et al. [2007] introduce a test statistic called (squared) distance covariance which is defined as",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "One can use other negative definite metrics instead of Euclidean norms to generalize the definition to metric spaces Lyons [2013]. The above expression is different from the presentation in the original papers (but mathematically equivalent).",
      "startOffset" : 117,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "dCor n is always between [0, 1], and unlike correlation, the population dCor 2 = 0 iff X,Y are independent, and Székely et al. [2007] proves it is consistent against any fixed alternatives (with finite second moments).",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "The MMD between μPXY and μPX×PY is called HSIC (see Gretton et al. [2005]), which has the sample expression: HSICn = 1 n2 tr(K̃L̃) = 1 n2 n ∑",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov.",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov. Hence, dCor and MMD are very strongly related quantities, for very related problems. In Székely and Rizzo [2013], the authors advocate the use of a modified unbiased dCor (called udCor), claiming that it works well for large d.",
      "startOffset" : 34,
      "endOffset" : 374
    }, {
      "referenceID" : 11,
      "context" : "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov. Hence, dCor and MMD are very strongly related quantities, for very related problems. In Székely and Rizzo [2013], the authors advocate the use of a modified unbiased dCor (called udCor), claiming that it works well for large d. We will describe experiments that demonstrate that dCor and udCor as test statistics both suffer in high d, but for a slightly different reason than for MMD. When (X,Y ) are drawn from a standard Gaussian, the authors of Székely and Rizzo [2013] show that the biased dCorn → 1, if n is kept fixed and dx, dy are increased.",
      "startOffset" : 34,
      "endOffset" : 735
    }, {
      "referenceID" : 11,
      "context" : "(1) and (2) is not coincidental - Sejdinovic et al. [2013] recently showed for every negative definite metric, there exists a positive definite kernel, and for every positive definite kernel, there exists a negative definite metric, such that HSIC equals dCov. Hence, dCor and MMD are very strongly related quantities, for very related problems. In Székely and Rizzo [2013], the authors advocate the use of a modified unbiased dCor (called udCor), claiming that it works well for large d. We will describe experiments that demonstrate that dCor and udCor as test statistics both suffer in high d, but for a slightly different reason than for MMD. When (X,Y ) are drawn from a standard Gaussian, the authors of Székely and Rizzo [2013] show that the biased dCorn → 1, if n is kept fixed and dx, dy are increased. Then, they show that their unbiased udCorn hovers around 0 in the same situation (even when dx, dy n), and conclude that it performs well in high-dimensions. The bias is indeed zero, but we argue that the variance of udCorn remains the same order as dCorn. Székely and Rizzo [2013] show that when the null is true, udCorn is well behaved.",
      "startOffset" : 34,
      "endOffset" : 1094
    }, {
      "referenceID" : 13,
      "context" : "For the null hypothesis of independent variables, we let (x, y) be sampled from a d-dimensional standard normal, like in Székely and Rizzo [2013]. For the alternative hypothesis, we need to make a choice about how to change the covariance matrix, such that as d increases, the problem neither gets easier nor harder.",
      "startOffset" : 121,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "The left panel shows that dCorn → 1, udCorn ≈ 0, as predicted by Székely and Rizzo [2013]. The middle panel shows that dCorn → 1, udCorn → 0, similar to the null.",
      "startOffset" : 65,
      "endOffset" : 90
    } ],
    "year" : 2017,
    "abstractText" : "This paper is about two related methods for two sample testing and independence testing which have emerged over the last decade: Maximum Mean Discrepancy (MMD) for the former problem and Distance Correlation (dCor) for the latter. Both these methods have been suggested for high-dimensional problems, and sometimes claimed to be unaffected by increasing dimensionality of the samples. We will show theoretically and practically that the power of both methods (for different reasons) does actually decrease polynomially with dimension. We also analyze the median heuristic, which is a method for choosing tuning parameters of translation invariant kernels. We show that different bandwidth choices could result in the MMD decaying polynomially or even exponentially in dimension.",
    "creator" : "LaTeX with hyperref package"
  }
}