{
  "name" : "1611.02189.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CoCoA: A General Framework for Communication-Efficient Distributed Optimization",
    "authors" : [ "Virginia Smith", "Simone Forte", "Chenxin Ma" ],
    "emails" : [ "vsmith@berkeley.edu", "simone.forte@gess.ethz.ch", "chm514@lehigh.edu", "Takac.MT@gmail.com", "jordan@cs.berkeley.edu", "martin.jaggi@epfl.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for the distributed environment, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets.\nKeywords: Convex optimization, distributed systems, large-scale machine learning, parallel and distributed algorithms\nar X\niv :1\n61 1."
    }, {
      "heading" : "1. Introduction",
      "text" : "Distributed computing architectures have come to the fore in modern machine learning, in response to the challenges arising from a wide range of large-scale learning applications. Distributed architectures offer the promise of scalability by increasing both computational and storage capacities. A critical challenge in realizing this promise of scalability is to develop efficient methods for communicating and coordinating information between distributed machines, taking into account the specific needs of machine-learning algorithms. On most distributed systems, the communication of data between machines is vastly more expensive than reading data from main memory and performing local computation. Moreover, the optimal trade-off between communication and computation can vary widely depending on the dataset being processed, the system being used, and the objective being optimized. It is therefore essential for distributed methods to accommodate flexible communicationcomputation profiles while still providing convergence guarantees.\nAlthough numerous distributed optimization methods have been proposed, the minibatch optimization approach has emerged as one of the most popular paradigms for tackling this communication-computation tradeoff (see, e.g., Takáč et al., 2013; Shalev-Shwartz and Zhang, 2013b; Qu et al., 2015; Richtárik and Takáč, 2016). Mini-batch methods are often developed by generalizing classical stochastic methods to process multiple data points at a time, which helps to alleviate the communication bottleneck by enabling more distributed computation per round of communication. However, while the need to reduce communication would suggest large mini-batch sizes, the theoretical convergence rates of these methods degrade with increased mini-batch size, reverting to the rates of classical (batch) gradient methods. Empirical results corroborate these theoretical rates, and in practice, mini-batch methods have limited flexibility to adapt to the communication-computation tradeoffs that would maximally leverage parallel execution. Moreover, because mini-batch methods are typically derived from a specific single-machine solver, these methods and their associated analyses are often tailored to specific problem instances and can suffer both theoretically and practically when applied outside of their restricted problem setting.\nIn this work, we propose a framework, CoCoA1, that addresses these two fundamental limitations. First, we allow arbitrary local solvers to be used on each machine in parallel. This allows our framework to directly incorporate state-of-the-art, application-specific singlemachine solvers in the distributed setting. Second, we share information between machines in our framework with a highly flexible communication scheme. This allows the amount of communication to be easily tailored to the problem and system at hand, in particular allowing for the case of significantly reduced communication in the distributed environment.\nA key step in providing these features in our framework is to first define meaningful subproblems for each machine to solve in parallel, and to then combine updates from the subproblems in an efficient manner. Our method and convergence results rely on noting that, depending on the distribution of the data (e.g., by feature or by training point), and whether we solve the problem in the primal or the dual, certain machine learning objectives can be\n1. CoCoA-v1 (Jaggi et al., 2014) and CoCoA+ (Ma et al., 2015a,b) are predecessors of this work. We continue to use the name CoCoA for the more general framework proposed here, and show how earlier work can be derived as a special case (Section 4). Portions of this newer work appear in SF’s Master’s Thesis (Forte, 2015) and Smith et al. (2015).\nmore easily decomposed into subproblems in the distributed environment. In particular, we categorize common machine learning objectives into several cases, and use duality to help decompose these objectives. Using primal-dual information in this manner not only allows for highly efficient methods (achieving, e.g., up to 50x speedups compared to state-of-theart distributed methods), but also allows for strong primal-dual convergence guarantees and practical benefits such as computation of the duality gap for use as an accuracy certificate and stopping criterion."
    }, {
      "heading" : "1.1 Contributions",
      "text" : "General framework. We develop a communication-efficient primal-dual framework that is applicable to a broad class of convex optimization problems. Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al. (2015b), our generalized, cohesive framework: (1) specifically incorporates difficult cases of L1 regularization and other non-strongly convex regularizers; (2) allows for the flexibility of distributing the data by either feature or training point; and (3) can be run on either a primal or dual formulation, which we show to have significant theoretical and practical implications.\nFlexible communication and local solvers. Two key advantages of the proposed framework are its communication efficiency and ability to employ off-the-shelf single-machine solvers internally. On real-world systems, the cost of communication versus computation can vary widely, and it is thus advantageous to permit a flexible amount of communication depending on the setting at hand. Our framework provides exactly such control. Moreover, we allow arbitrary solvers to be used on each machine, which permits the reuse of existing code and the benefits from multi-core or other optimizations therein.\nPrimal-dual rates. We derive convergence rates for our framework, leveraging a novel approach in the analysis of primal-dual rates for non-strongly convex regularizers. The proposed technique is a significant improvement over simple smoothing techniques used in, e.g., Nesterov (2005); Shalev-Shwartz and Zhang (2014); and Zhang and Lin (2015) that enforce strong convexity by adding a small L2 term to the objective. Our results include primal-dual rates and certificates for the general class of linear regularized loss minimization, and we show how earlier work can be derived as a special case of our more general approach.\nExperimental comparison. The proposed framework yields order-of-magnitude speedups (as much as 50× faster) compared to state-of-the-art methods for large-scale machine learning. We demonstrate these performance gains with an extensive experimental comparison on real-world distributed datasets. We additionally explore properties of the framework itself, including the effect of running the framework in the primal or the dual. All algorithms for comparison are implemented in Apache Spark and run on Amazon EC2 clusters. Our code is open-source and publicly available at: github.com/gingsmith/proxcocoa."
    }, {
      "heading" : "2. Background and Setup",
      "text" : "In this paper we develop a general framework for minimizing problems of the following form:\n`(u) + r(u) , (I)\nfor convex functions ` and r. Frequently the first term ` is an empirical loss over the data, taking the form ∑ i `i(u), and the second term r is a regularizer, e.g., r(u) = λ‖u‖p. This formulation includes many popular methods in machine learning and signal processing, such as support vector machines, linear and logistic regression, lasso and sparse logistic regression, and many others."
    }, {
      "heading" : "2.1 Definitions",
      "text" : "The following standard definitions will be used throughout the paper.\nDefinition 1 (L-Lipschitz Continuity). A function h : Rm → R is L-Lipschitz continuous if ∀u,v ∈ Rm, we have\n|h(u)− h(v)| ≤ L‖u− v‖ . (1)\nDefinition 2 (L-Bounded Support). A function h : Rm → R ∪ {+∞} has L-bounded support if its effective domain is bounded by L, i.e.,\nh(u) < +∞ ⇒ ‖u‖ ≤ L . (2)\nDefinition 3 ((1/µ)-Smoothness). A function h : Rm → R is (1/µ)-smooth if it is differentiable and its derivative is (1/µ)-Lipschitz continuous, or equivalently\nh(u) ≤ h(v) + 〈∇h(v),u− v〉+ 1 2µ ‖u− v‖2 ∀u,w ∈ Rm . (3)\nDefinition 4 (µ-Strong Convexity). A function h : Rm → R is µ-strongly convex for µ ≥ 0 if\nh(u) ≥ h(v) + 〈s,u− v〉+ µ 2 ‖u− v‖2 ∀u,v ∈ Rm , (4)\nfor any s ∈ ∂h(v), where ∂h(v) denotes the subdifferential of h at v."
    }, {
      "heading" : "2.2 Primal-Dual Setting",
      "text" : "Numerous methods have been proposed to solve (I), and these methods generally fall into two categories: primal methods, which run directly on the primal objective, and dual methods, which instead run on the dual formulation of the primal objective. In developing our framework, we present an abstraction that allows for either a primal or a dual variant of our framework to be run. In particular, to solve the input problem (I), we consider mapping the problem to one of the following two general problems:\nmin α∈Rn\n[ OA(α) := f(Aα) + g(α)\n] (A)\nmin w∈Rd\n[ OB(w) := f∗(w) + g∗(−A>w)\n] (B)\nHere α ∈ Rn and w ∈ Rd are parameter vectors, A := [x1; . . . ;xn] ∈ Rd×n is a data matrix with column vectors xi ∈ Rd, i ∈ {1, . . . , n}, and the functions f∗ and g∗i are the convex conjugates of f and gi, respectively.\nThe dual relationship in problems (A) and (B) is known as Fenchel-Rockafellar duality (Borwein and Zhu, 2005, Theorem 4.4.2). We provide a self-contained derivation of the duality in Appendix B. Note that while dual problems are typically presented as a pair of (min, max) problems, we have equivalently reformulated (A) and (B) to both be minimization problems in accordance with their roles in our framework.\nGiven α ∈ Rn in the context of (A), a corresponding vector w ∈ Rd for problem (B) is obtained by:\nw = w(α) := ∇f(Aα) . (5)\nThis mapping arises from first-order optimality conditions on the f -part of the objective. The duality gap, given by:\nG(α) := OA(α)− [−OB(w(α))] (6)\nis always non-negative, and under strong duality, the gap will reach zero only for an optimal pair (α?,w?). The duality gap at any point provides a practically computable upper bound on the unknown primal as well as dual optimization error (suboptimality), since\nOA(α) ≥ OA(α?) ≥ −OB(w?) ≥ −OB(w(α)) .\nIn developing the proposed framework, noting the duality between (A) and (B) has many benefits, including the ability to compute the duality gap, which acts as a certificate of the approximation quality. It is also useful as an analysis tool, helping us to present a cohesive framework and relate this work to the prior work of Yang (2013); Jaggi et al. (2014); and Ma et al. (2015b,a). As a word of caution, note that we avoid prescribing the name “primal” or “dual” directly to either of the problems (A) or (B), as we demonstrate below that their role as primal or dual can change depending on the application problem of interest."
    }, {
      "heading" : "2.3 Assumptions and Problem Cases",
      "text" : "Our main assumptions on problem (A) are that f is (1/τ)-smooth, and the function g is separable, i.e., g(α) = ∑ i gi(αi), with each gi having L-bounded support. Given the duality between the problems (A) and (B), this can be equivalently stated as assuming that in problem (B), f∗ is τ -strongly convex, and the function g∗(−A>w) = ∑ i g ∗ i (−x>i w) is separable with each g∗i being L-Lipschitz. For clarity, in Table 1 we relate our assumptions on objectives (A) and (B) to the general input problem (I). Suppose, as in equation (I), we would like to find a minimizer of the general objective `(u)+r(u). Depending on the smoothness of the function ` and the strong convexity of the function r, we will be able to map the input function (I) to one (or both) of the objectives (A) and (B) based on our assumptions.\nIn particular, we outline three separate cases: Case I, in which the function ` is smooth and the function r is strongly convex; case II, in which ` is smooth, and r is non-strongly convex and separable; and case III, in which ` is non-smooth and separable, and r is strongly convex. The union of these cases will capture most commonly-used applications of linear regularized loss minimization problems. In Section 3, we will see that different variants of our framework may be realized depending on which of these three cases we consider when solving the input problem (I)."
    }, {
      "heading" : "2.4 Running Examples",
      "text" : "To illustrate the three cases in Table 1, we consider several examples below. These applications will serve as running examples throughout the paper, and we will revisit them in our experiments (Section 6). Further applications and details are provided in Section 5.\n1. Elastic Net Regression (Case I: map to either (A) or (B)). We can map elastic-net regularized least squares regression,\nmin u∈Rp\n1 2‖Au− b‖ 2 2 + ηλ‖u‖1 + (1− η)\nλ 2 ‖u‖22 , (7)\nto either objective (A) or (B). To map to objective (A), we let: f(Aα) = 12‖Aα−b‖ 2 2 and g(α) = ∑ i gi(αi) = ∑ i ηλ|αi| + (1 − η) λ 2α 2 i , setting n to be the number of\nfeatures and d the number of training points. To map to (B), we let: g(−A>w) =∑ i g ∗ i (−x>i w) = ∑ i 1 2(x > i w−bi)2 and f∗(w) = ηλ‖w‖1 + (1− η)λ2‖w‖ 2 2, setting d to be the number of features and n the number of training points. We discuss in Section 3 how the choice of mapping elastic net regression to either (A) or to (B) will result in one of two variants of our framework, and can have implications on the distribution scheme and overall performance of the method.\n2. Lasso (Case II: map to (A)). We can represent L1-regularized least squares regression by mapping the model:\nmin u∈Rp\n1 2‖Au− b‖ 2 2 + λ‖u‖1 (8)\nto objective (A), letting f(Aα) = 12‖Aα − b‖ 2 2 and g(α) = ∑ i gi(αi) = ∑ i λ|αi|. In this mapping, n represents the number of features, and d the number of training points. Note that we cannot map the lasso objective to (B) directly, as f∗ must be τ -strongly convex and the L1-norm is non-strongly convex.\n3. Support Vector Machine (Case III: map to (B)). We can represent a hinge loss support vector machine (SVM) by mapping the model:\nmin u∈Rp\n1\nm m∑ i=1 max { 0, 1− yi(x>i u) } + λ2‖u‖ 2 2 , (9)\nto objective (B), letting g∗(−A>w) = ∑\ni g ∗ i (−x>i w) = ∑ i 1 n max{0, 1− yix > i w} and\nf∗(w) = λ2‖w‖ 2 2. In this mapping, d represents the number of features, and n the number of training points. Note that we cannot map the hinge loss SVM primal to objective (A) directly, as f must be (1/τ)-smooth and the hinge loss is non-smooth."
    }, {
      "heading" : "2.5 Data Partitioning",
      "text" : "To view our setup in the distributed environment, we suppose that the dataset A is distributed over K machines according to a partition {Pk}Kk=1 of the columns of A ∈ Rd×n. We denote the size of the partition on machine k by nk = |Pk|. For machine k ∈ {1, . . . ,K} and weight vector α ∈ Rn, we define α[k] ∈ Rn as the n-vector with elements (α[k])i := αi if i ∈ Pk and (α[k])i := 0 otherwise. Analogously, we write A[k] for the corresponding group of columns of A, and zeros elsewhere (note that columns can correspond to either training examples or features, depending on the application). We discuss these distribution schemes in greater detail in Section 3."
    }, {
      "heading" : "3. CoCoA",
      "text" : "We first describe the proposed framework, CoCoA, at a high level, and then discuss two approaches for using the framework in practice: CoCoA in the primal, where we consider (A) to be the primal objective and run the framework on this problem directly, and CoCoA in the dual, where we instead consider (B) to be the primal objective, and then run the framework on the dual (A). Note that in both approaches, the aim will be to compute a minimizer of the problem (A) in a distributed fashion; the main difference will be whether we view (A) as the primal objective or as the dual objective."
    }, {
      "heading" : "3.1 The Generalized Framework",
      "text" : "The goal of our framework is to find a global minimizer of the objective (A), while distributing computation based on the partitioning of the dataset A across machines (Section 2.5). As a first step, note that distributing the update to the function g in objective (A) is straightforward, as we have required that this term is separable according to the partitioning of our data, i.e., g(α) = ∑n i=1 gi(αi). However, the same does not hold for the term f(Aα). To minimize this part of the objective in a distributed fashion, we propose minimizing a quadratic approximation of the function, which allows the minimization to separate across machines. We make this approximation precise in the following subsection.\nData-local quadratic subproblems. In the general CoCoA framework (Algorithm 1), we distribute computation by defining a data-local subproblem of the optimization problem (A) for each machine. This simpler problem can be solved on machine k and only requires accessing data which is already available locally, i.e., the columns A[k]. More formally, each machine k is assigned the following local subproblem, which depends only on the previous shared vector v := Aα ∈ Rd, and the local data A[k]:\nmin ∆α[k]∈Rn\nGσ′k (∆α[k];v,α[k]) , (10)\nwhere\nGσ′k (∆α[k];v,α[k]) := 1 K f(v) + w>A[k]∆α[k] +\nσ′\n2τ ∥∥∥A[k]∆α[k]∥∥∥2 + ∑ i∈Pk gi(αi + ∆α[k]i),\nandw := ∇f(v). Here we let ∆α[k] denote the change of local variables αi for indices i ∈ Pk, and we set (∆α[k])i := 0 for all i /∈ Pk. It is important to note that the subproblem (10)\nAlgorithm 1 Generalized CoCoA Distributed Framework\n1: Input: Data matrix A distributed column-wise according to partition {Pk}Kk=1, aggregation parameter γ∈(0, 1], and parameter σ′ for the local subproblems Gσ′k (∆α[k];v,α[k]). Starting point α(0) := 0 ∈ Rn, v(0) := 0 ∈ Rd. 2: for t = 0, 1, 2, . . . do 3: for k ∈ {1, 2, . . . ,K} in parallel over computers do 4: call local solver, returning a Θ-approximate solution ∆α[k] of the local subproblem (10) 5: update local variables α(t+1)[k] := α (t) [k] + γ∆α[k] 6: return updates to shared state ∆vk := A[k]∆α[k] 7: end for 8: reduce v(t+1) := v(t) + γ ∑K k=1 ∆vk 9: end for\nis simple in the sense that it is always a quadratic objective (apart from the gi term). The subproblem does not depend on the function f itself, but only its linearization at the fixed shared vector v. This property additionally simplifies the task of the local solver, especially for cases of complex functions f .\nFramework parameters γ and σ′. There are two parameters that must be set in our framework: γ, the aggregation parameter, which controls how the updates from each machine are combined, and σ′, the subproblem parameter, which is a data-dependent term measuring the difficulty of the data partitioning {Pk}Kk=1. These terms play a crucial role in the convergence of the method, as we demonstrate in Section 4. In practice, we provide a simple and robust way to set these parameters: For a given aggregation parameter γ ∈ (0, 1], the subproblem parameter σ′ will be set as σ′ := γK, but can also be improved in a datadependent way as we discuss below. In general, as we show in Section 4, setting γ := 1 and σ′ := K will guarantee convergence while delivering our fastest convergence rates.\nDefinition 5 (Data-dependent aggregation parameter). In Algorithm 1, the aggregation parameter γ controls the level of adding (γ := 1) versus averaging (γ := 1K ) of the partial solutions from all machines. For our convergence results (Section 4) to hold, the subproblem parameter σ′ must be chosen not smaller than\nσ′ ≥ σ′min := γ max α∈Rn ‖Aα‖2∑K k=1 ‖A[k]α[k]‖2 . (11)\nThe simple choice of σ′ := γK is valid for (11), i.e.,\nγK ≥ σ′min .\nIn some cases, it will be possible to give a better (data-dependent) choice for σ′, closer to the actual bound given in σ′min.\nSubproblem Interpretation. Here we provide further intuition behind the data-local subproblems (10). The local objective functions Gσ′k are defined to closely approximate the global objective in (A) as the “local” variable ∆α[k] varies, which we will see in the analysis (Appendix D, Lemma 1). In fact, if the subproblem were solved exactly, this could be interpreted as a data-dependent, block-separable proximal step, applied to the f part of the objective (A) as follows: K∑ k=1 Gσ′k (∆α[k];v,α[k]) = R+ f(v) +∇f(v)>A∆α + σ′ 2τ ∆α> A > [1]A[1] 0 . . . 0 A>[K]A[K]\n∆α , where R = ∑ i∈[n] gi(−αi −∆αi) .\nHowever, note that in contrast to traditional proximal methods, our algorithm does not assume that this subproblem is solved to high accuracy, as we instead allow the use of local solvers of any approximation quality Θ.\nReusability of existing single-machine solvers. Our local subproblems (10) have the appealing property of being very similar in structure to the global problem (A), with the main difference being that they are defined on a smaller (local) subset of the data, and are simpler because they are not dependent on the shape of f . For a user of CoCoA, this presents a major advantage in that existing single machine-solvers can be directly re-used in our distributed framework (Algorithm 1) by employing them on the subproblems Gσ′k .\nTherefore, problem-specific tuned solvers which have already been developed, along with associated speed improvements (such as multi-core implementations), can be easily leveraged in the distributed setting. We quantify the dependence on local solver performance with the following assumption and remark, and relate this performance to our global convergence rates in Section 4.\nAssumption 1 (Θ-approximate solution). We assume that there exists Θ ∈ [0, 1) such that ∀k ∈ [K], the local solver at any outer iteration t produces a (possibly) randomized approximate solution ∆α[k], which satisfies\nE [ Gσ′k (∆α[k];v,α[k])− Gσ ′ k (∆α ? [k];v,α[k]) ] ≤ Θ ( Gσ′k (0;v,α[k])− Gσ ′ k (∆α ? [k];v,α[k]) ) ,\n(12)\nwhere ∆α?[k] ∈ arg min\n∆α∈Rn Gσ′k (∆α[k];v,α[k]), ∀k ∈ [K] . (13)\nRemark 1. In practice, the time spent solving the local subproblems in parallel should be chosen comparable to the required time of a communication round, for best overall efficiency on a given system. We study this trade-off both in theory (Section 4) and experiments (Section 6).\nRemark 2. Note that the accuracy parameter Θ does not have to be chosen a priori: Our convergence results (Section 4) are valid if Θ is an upper bound on the actual empirical values Θ in the rounds of Algorithm 1. This allows for some of the K machines to at times deliver better or worse accuracy (e.g., if a slow local machine is stopped early during a specific round, to avoid the others needing to wait).\nWith this general framework in place, we next discuss two variants of our framework, CoCoA-Primal and CoCoA-Dual. In running either the primal or dual variant of our framework, the goal will always be to solve objective (A) in a distributed fashion. The main difference will be whether this objective is viewed as the primal or dual of the input problem (I). If we map the input (I) to objective (A), then (A) will be viewed as the primal. If we map (I) to (B), the objective (A) will be viewed as the dual. We make this mapping technique precise and discuss its implications in the following subsections (Sections 3.2–3.4)."
    }, {
      "heading" : "3.2 Primal Distributed Optimization",
      "text" : "In the primal distributed version of the framework (Algorithm 2), the framework is run by mapping the initial problem (I) directly to objective (A) and then applying the generalized CoCoA framework described in Algorithm 1. In other words, we view problem (A) as the primal objective, and solve this problem directly.\nFrom a theoretical perspective, viewing (A) as the primal will allow us to consider nonstrongly convex regularizers, since we allow the terms gi to be non-strongly convex. This setting was not covered in earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015b); and Ma et al. (2015a), and we discuss it in detail in Section 4, as additional machinery must be introduced to develop primal-dual rates for this setting.\nRunning the primal version of the framework has important practical implications in the distributed setting, as it typically implies that the data is distributed by feature rather than by training point. In this setting, the amount of communication at every outer iteration will be O(# of training points). When the number of features is high (as is common when using sparsity-inducing regularizers) this can help to reduce communication and improve overall performance, as we demonstrate in Section 6.\nAlgorithm 2 CoCoA-Primal (Mapping Problem (I) to (A))\n1: Map: Input problem (I) to objective (A) 2: Distribute: Dataset A by columns (here typically features) according to par-\ntition {Pk}Kk=1 3: Run: Algorithm 1 with aggregation parameter γ and subproblem parameter σ′"
    }, {
      "heading" : "3.3 Dual Distributed Optimization",
      "text" : "In the dual distributed version of the framework (Algorithm 3), we run the framework by mapping the original problem (I) to objective (B), and then solve the problem by running Algorithm 1 on the dual (A). In other words, we view problem (B) as the primal, and solve this problem via the dual (A).\nThis version of the framework will allow us to consider non-smooth losses, such as the hinge loss or absolute deviation loss, since the terms g∗i can be non-smooth. From a practical perspective, this version of the framework will typically imply that the data is distributed by training point, and for a vector O(# of features) to be communicated at every outer iteration. This variant may therefore be preferable when the number of training points exceeds the number of features.\nAlgorithm 3 CoCoA-Dual (Mapping Problem (I) to (B))\n1: Map: Input problem (I) to objective (B) 2: Distribute: Dataset A by columns (here typically training points) according\nto partition {Pk}Kk=1 3: Run: Algorithm 1 with aggregation parameter γ and subproblem parameter σ′"
    }, {
      "heading" : "3.4 Primal vs. Dual",
      "text" : "In Table 2, we revisit the three cases from Section 2, showing how the primal and dual variants of CoCoA can be applied to various input problems `(u) + r(u), depending on properties of the functions ` and r. In particular, in the setting where ` is smooth and r is strongly convex, the user may choose whether to run the framework in the primal (Algorithm 2), or in the dual (Algorithm 3). Intuitively, Algorithm 2 will be preferable as r loses strong convexity, and Algorithm 3 will be preferable as ` loses smoothness. However, there are also systems-related aspects to consider. In Algorithm 2, we typically distribute the data by feature, and in Algorithm 3, by training point (this distribution depends on how the terms n and d are defined in our mapping, see Section 5). Depending on whether the number of features or number of training points is the dominating term, we may chose to run Algorithm 2 or Algorithm 3, respectively, in order to reduce communication costs. We validate these ideas empirically in Section 6 by comparing the performance of each variant (primal vs. dual) on real distributed datasets.\nIn the following two subsections, we provide greater insight into the form of the generalized CoCoA framework and its relation to prior work. An extended discussion on related work is available in Section 7.\n3.5 Interpretation of CoCoA in the Context of Classical Parallelization Schemes\nThere are numerous methods that have been developed to solve (A) and (B) in parallel and distributed environments. We describe related work in detail in Section 7, and here briefly highlight a major algorithmic difference between CoCoA and other widely-used parallelized methods. In particular, we contrast CoCoA with mini-batch and batch methods commonly used in distributed computing environments, such as mini-batch stochastic gradient descent or coordinate descent, gradient descent, and quasi-Newton methods.\nCoCoA is similar to these methods in that they are all iterative, i.e., they make progress towards the optimal solution by updating the parameter vector α according to some function\nh : Rn → Rn at each iteration t:\nα(t+1) = h(α(t)) t = 0, 1, . . . ,\nuntil convergence is reached. From a coordinate-wise perspective, two approaches for updating the parameter vector α in an iterative fashion include the Jacobi method, in which updates made to coordinates of α do not take into account the most recent updates to the other coordinates, and Gauss-Seidel, in which the most recent information is used (Bersekas and Tsitsiklis, 1989). In particular, these two paradigms make the following updates to a coordinate i at iteration t+ 1:\nJacobi: α(t+1)i = hi(α (t) 1 , . . . ,α (t) n ), i = 1, . . . , n,\nGauss-Seidel: α(t+1)i = hi(α (t+1) 1 , . . . ,α (t+1) i−1 ,α (t) i , . . . ,α (t) n ), i = 1, . . . , n.\nThe Jacobi method does not require information from the other coordinates to update coordinate i, which makes this style of method well-suited for parallelization. However, the Gauss-Seidel style method tends to converge faster in terms of iterations, since it is able to incorporate information from the other coordinates more quickly. This difference is wellknown and evident in single machine solvers, where stochastic methods (benefiting from fresh updates) tend to outperform their batch counterparts.\nTypical mini-batch methods, e.g., mini-batch coordinate descent, perform a Jacobi-style update on a subset of the coordinates at each iteration. This makes these methods amenable to high levels of parallelization. However, they are unable to incorporate information as quickly as their serial counterparts in terms of number of data points accessed, because they must wait for a synchronization step to update the coordinates. As the size of the mini-batch grows, this can slow them down in terms of overall runtime, and can even lead to divergence in practice (Takáč et al., 2013; Takáč et al., 2015; Richtárik and Takáč, 2016; Marecek et al., 2015).\nCoCoA instead attempts to combine attractive properties of both of these update paradigms. It performs Jacobi-style parallel updates to blocks of the coordinates of α to parallelize the method, while allowing for (though not necessarily requiring) faster GaussSeidel style updates on each machine. This change in parallelization scheme is one of the major reasons for improved performance over simpler mini-batch or batch style methods.\nCoCoA incorporates an additional level of flexibility by allowing an arbitrary number of Gauss-Seidel iterations (or any other local solver for that matter) to be performed on each machine, which lets the framework scale from very low-communication environments, where more iterations will be made before communicating, to higher communication environments, where fewer internal iterations are necessary. We will see in Section 6 that this communication flexibility also greatly improves the overall runtime in practice."
    }, {
      "heading" : "3.6 Comparison to ADMM",
      "text" : "Finally, in this subsection we provide a direct comparison between CoCoA and ADMM (Boyd et al., 2010). Alternating direction method of multipliers (ADMM) is a well-established framework for distributed optimization. Similar to CoCoA, ADMM differs from the methods discussed in the previous section in that it defines a subproblem for each problem to solve\nin parallel, rather than parallelizing a global batch or mini-batch update. It also leverages duality structure, similar to that presented in Section 2.\nFor consensus ADMM, the objective (B) is decomposed with a re-parameterization:\nmax w1,...wK ,w K∑ k=1 ∑ i∈Pk g∗(−x>i wk) + f∗(w)\ns.t. wk = w, k = 1, . . . ,K.\nThis problem is then solved by constructing the augmented Lagrangian, which yields the following decomposable updates:\nw (t) k = arg min\nwk\n∑ i∈Pk g∗(−x>i wk) + ρ 2 ‖wk − ( w(t−1) − u(t−1)k ) ‖2, (14)\nw(t) = arg min w f∗(w) + ρ K∑ k=1 u>k (wk −w) + ρ 2 K∑ k=1 ‖wk −w‖2,\nu (t) k = u (t−1) k + w (t) k −w (t),\nwhere ρ is a penalty parameter that must be tuned for best performance. When running CoCoA in the dual (Algorithm 3) and setting f(·) = 12‖ · ‖ 2 2, we can derive a similar subproblem for updating wk in the CoCoA framework. In particular, the following subproblem can be found by unrolling the CoCoA update and viewing the dual subproblem in its primal formulation:\nmin wk ∑ i∈Pk g∗i (−x>i wk) + τ 2σ′ ∥∥∥wk − (w(t−1) + γ∆v(t−1))∥∥∥2. (15) Comparing (14) and (15) we can see that in the specific case where f(·) = 12‖ · ‖ 2 2 and we solve the problem in the dual (according to Algorithm 3), ADMM and CoCoA consider a similar subproblem on each machine, but where the parameter ρ is explicitly set in CoCoA as τσ′ . However, there are major differences between the methods even in this setting. First, CoCoA has a more direct and simplified scheme for updating the global weight vector w. Second, and most importantly, in the CoCoA method and theory, we allow for the subproblem to be solved approximately, rather than requiring a full batch update as in ADMM. We will see in our experiments that these differences have a large impact in practice (Section 6). We provide a full derivation of the comparison to ADMM for reference in Appendix C."
    }, {
      "heading" : "4. Convergence Analysis",
      "text" : "In this section, we provide convergence rates for the proposed framework and introduce an important theoretical technique in analyzing non-strongly convex terms in the primal-dual setting. For simplicity of presentation, we assume in the analysis that the data partitioning is balanced; i.e., nk = n/K for all k. Furthermore, we assume that the columns of A satisfy ‖xi‖ ≤ 1 for all i ∈ [n]. We present rates for the case where γ := 1 in Algorithm 1, and where the subproblems (10) are defined using the corresponding safe bound σ′ := K. This\ncase will guarantee convergence while delivering our fastest rates in the distributed setting, which in particular don’t degrade as the number of machines K grows and n remains fixed."
    }, {
      "heading" : "4.1 Proof Strategy: Relating Subproblem Approximation to Global Progress",
      "text" : "To guarantee convergence, it is critical to show how progress made on the local subproblems (10) relates to the global objective OA. Our first lemma provides exactly this information. In particular, we see that if the aggregation and subproblem parameters are selected according to Definition 5, the sum of the subproblem objectives, ∑K k=1 Gσ ′ k , will form a block-separable upper bound on the global objective OA.\nLemma 1. For any weight vector α,∆α ∈ Rn, v = v(α) := Aα, and real values γ, σ′ satisfying (11), it holds that\nOA ( α + γ K∑ k=1 ∆α[k] ) ≤ (1− γ)OA(α) + γ K∑ k=1 Gσ′k (∆α[k];v,α[k]) . (16)\nA proof of Lemma 1 is provided in Appendix D. We use this main lemma, in combination with our assumption on the quality of the subproblem approximations (Assumption 1), to deliver our global convergence rates.\n4.2 Rates for General Convex gi, L-Lipschitz g∗i Our first main theorem provides convergence guarantees for objectives with general convex gi (or, equivalently, L-Lipschitz g∗i ), including models with non-strongly convex regularizers such as lasso and sparse logistic regression, or models with non-smooth losses, such as the hinge loss support vector machine.\nProviding primal-dual rates and globally defined primal-dual accuracy certificates for these objectives may require an important theoretical technique that we introduce below, in which we show how to satisfy the notion of L-bounded support for gi, as stated in Definition 2.\nTheorem 2. Consider Algorithm 1 with γ := 1, and let Θ be the quality of the local solver as in Assumption 1. Let gi have L-bounded support, and let f be (1/τ)-smooth. Then after T iterations, where\nT ≥ T0 + max{ ⌈ 1\n1−Θ\n⌉ ,\n4L2n2\nτ G(1−Θ) } , (17)\nT0 ≥ t0 + [ 2\n1−Θ\n( 8L2n2 τ G − 1 )] + ,\nt0 ≥ max(0, ⌈ 1 (1−Θ) log ( τ(OA(α(0))−OA(α?)) 2L2Kn )⌉ ) ,\nwe have that the expected duality gap satisfies\nE [ OA(α)− (−OB(w(α))) ] ≤ G ,\nwhere α is the averaged iterate: 1T−T0 ∑T−1 t=T0+1 α(t)."
    }, {
      "heading" : "4.2.1 Bounded support modification",
      "text" : "As mentioned earlier, additional work is necessary if Theorem 2 is to be applied to nonstrongly convex regularizers such as the L1 norm, which do not have L-bounded support for each gi, and thus violate the assumptions of the theorem. Note for example that the conjugate function of gi = | · |, which is the indicator function of an interval, is not defined globally over R, and thus (without further modification) the duality gap G(α) := OA(α)− (−OB(w(α))) is not even defined at all points α.\nSmoothing. To address this problem, existing approaches typically use a simple smoothing technique (as in Nesterov, 2005; Shalev-Shwartz and Zhang, 2014): by adding a small amount of L2 to the objective gi, the functions gi become strongly convex. Followed by this change, the algorithms are then run on the dual of instead of the original primal problem at hand. While this modification satisfies the necessary assumptions for convergence of our framework, this Nesterov smoothing technique is often undesirable in practice, as it changes the iterates, the algorithms at hand, the convergence rate, and the tightness of the resulting duality gap compared to the original objective. Further, the amount of smoothing can be difficult to tune and can have a large influence on the performance of the method at hand. We show practical examples of these difficulties in Section 6.\nBounded support modification. In contrast to smoothing, our approach preserves all solutions of the original objective, leaves the iterate sequence unchanged, and allows for direct reusability of existing solvers for the original gi objectives (such as L1 solvers). It also removes the need for tuning a smoothing parameter. To achieve this, we modify the function gi by imposing an additional weak constraint that is inactive in our region of interest. Formally, we replace gi(αi) by the following modified function:\nḡi(αi) := { gi(αi) : αi ∈ [−B,B] +∞ : otherwise.\n(18)\nFor large enough B, this problem yields the same solution as the original objective. Note also that this only affects convergence theory, in that it allows us to present a strong primaldual rate (Theorem 2 for L=B). The modification of gi does not affect the algorithms for the original problems. Whenever a monotone optimizer is used, we will never leave the level set defined by the objective at the starting point.\nUsing the resulting modified function will allow us to apply the results of Theorem 2 for general convex functions gi. This technique can also be thought of as “Lipschitzing” the dual g∗i , because of the general result that g ∗ i is L-Lipschitz if and only if gi has Lbounded support (Rockafellar, 1997, Corollary 13.3.3). We derive the conjugate function ḡ∗i for completeness in Appendix B (Lemma 6). In Section 5, we show how to leverage this technique for a variety of application input problems. See also Dünner et al. (2016) for a follow-up discussion of this technique in the non-distributed case.\n4.3 Rates for Strongly Convex gi, Smooth g∗i For the case of objectives with strongly convex gi (or, equivalently, smooth g∗i ), e.g., elastic net regression or logistic regression, we obtain the following faster linear convergence rate.\nTheorem 3. Consider Algorithm 1 with γ := 1, and let Θ be the quality of the local solver as in Assumption 1. Let gi be µ-strongly convex ∀i ∈ [n], and let f be (1/τ)-smooth. Then after T iterations where\nT ≥ 1(1−Θ) µτ+n µτ log n OA , (19)\nit holds that E [ OA(α(T ))−OA(α?) ] ≤ OA .\nFurthermore, after T iterations with\nT ≥ 1(1−Θ) µτ+n µτ log\n( 1\n(1−Θ) µτ+n µτ n G\n) ,\nwe have the expected duality gap E [ OA(α(T ))− (−OB(w(α(T ))) ] ≤ G .\nWe provide proofs of both Theorem 2 and Theorem 3 in Appendix D."
    }, {
      "heading" : "4.4 Convergence Cases",
      "text" : "Revisiting Table 1 from Section 2, we summarize our convergence guarantees for the three cases of input problems (I) in the following table. In particular, we see that for cases II and III, we obtain a sublinear convergence rate, whereas for case I we can obtain a faster linear rate, as provided in Theorem 3."
    }, {
      "heading" : "4.5 Recovering Earlier Work as a Special Case",
      "text" : "As a special case, the proposed framework and rates directly apply to L2-regularized lossminimization problems, including those presented in the earlier work of Jaggi et al. (2014) and Ma et al. (2015b).\nRemark 3. If we run Algorithm 3 (mapping (I) to (B)), restrict f∗(·) := λ2‖ · ‖ 2 (so that τ = λ), and let g∗i := 1 n` ∗ i , Theorem 2 recovers as a special case the CoCoA\n+ rates for general L-Lipschitz `∗i losses (see Ma et al., 2015b, Corollary 9). The earlier work of CoCoA-v1 (Jaggi et al., 2014) did not provide rates for L-Lipschitz `∗i losses.\nThese cases follow since g∗i is L-Lipschitz if and only if gi has L-bounded support (Rockafellar, 1997, Corollary 13.3.3).\nRemark 4. If we run Algorithm 3 (mapping (I) to (B)), restrict f∗(·) := λ2‖ · ‖ 2 (so that τ = λ), and scale g∗i := 1 n` ∗ i , Theorem 3 recovers as a special case the CoCoA\n+ rates for (1/`∗i )-smooth losses (see Ma et al., 2015b, Corollary 11). The earlier rates of CoCoA-v1 can be obtained by setting γ:= 1K and σ ′=1 in Algorithm 1 (Jaggi et al., 2014, Theorem 2).\nThese cases follow since g∗i is µ-strongly convex if and only if gi is (1/µ)-smooth (HiriartUrruty and Lemaréchal, 2001, Theorem 4.2.2)."
    }, {
      "heading" : "5. Applications",
      "text" : "In this section we provide a detailed treatment of example applications that can be cast within the general CoCoA framework. For each example, we describe the primal-dual setup and algorithmic details, discuss the convergence properties our framework for the application, and include practical concerns such as information on state-of-the-art local solvers. We discuss examples according to the three cases defined in Table 1 of Section 2 for finding a minimizer of the general objective `(u) + r(u), and provide a summary of these common examples in Table 4."
    }, {
      "heading" : "5.1 Case I: Smooth `, Strongly convex r",
      "text" : "For input problems (I) with smooth ` and strongly convex r, Theorem 3 from Section 4 gives a global linear (geometric) convergence rate. Smooth loss functions can be mapped either to the function f in objective (A), or g∗ in (B). Similarly, strongly convex regularizers can be mapped either to function g in objective (A), or f∗ in (B). To illustrate the role of f as a smooth loss function and g as a strongly convex regularizer in objective (A), contrasting with their traditional roles in prior work (Yang, 2013; Jaggi et al., 2014; Ma et al., 2015b,a), we consider the following examples. Note that mapping to objective (B) instead will follow trivially assuming that the loss is separable across training points (see Table 4).\nFor the examples in this subsection, we use nonstandard definitions of the number of training points as d and the number of features as n. These definitions are intentionally used so that we can present both the primal and dual variations of our framework (Algorithms 2 and 3) with a single abstracted method (Algorithm 1).\nSmooth `: least squares loss. Let b ∈ Rd be labels or response values, and consider the least squares objective, f(v) := 12‖v − b‖ 2 2, which is 1-smooth. We obtain the familiar least-squares regression objective in our optimization problem (A), using\nf(Aα) := 12‖Aα− b‖ 2 2 . (20)\nObserving that the gradient of f is ∇f(v) = v − b, the primal-dual mapping is given by: w(α) := ∇f(v(α)) = Aα − b, which is well known as the residual vector in least-squares regression.\nSmooth `: logistic regression loss. For classification problems, we consider a logistic regression model with d training examples, yj ∈ Rn for j ∈ [d], collected as the rows of the data matrix A. For each training example, we are given a binary label, which we collect in the vector b ∈ {−1, 1}d. Formally, the objective is defined as f(v) := ∑d j=1 log (1 + exp (−bjvj)), which is again a separable function. The classifier loss is given by\nf(Aα) := d∑ j=1 log (1 + exp (−bjy>j α)) , (21)\nwhere α ∈ Rn is the parameter vector. It is not hard to show that f is 1-smooth if the labels satisfy bj ∈ [−1, 1]. The primal-dual mapping w(α) := ∇f(v(α)) = ∇f(Aα) is given by wj(α) :=\n−bj 1+exp (bjy>j α) .\nStrongly convex r: elastic net regularizer. An application we can consider for a strongly convex regularizer, g in (A) or f∗ in (B), is elastic net regularization, ηλ‖u‖1 + (1− η)λ2‖u‖ 2 2, for fixed parameter η ∈ (0, 1]. This can be obtained in (A) by setting\ng(α) = n∑ i=1 gi(αi) := n∑ i=1 ηλ|αi|+ (1− η)λ2α 2 i . (22)\nFor the special case η = 1, we obtain the L1-norm, and for η = 0, we obtain the L2-norm. The conjugate of gi is given by: g∗i (x) := 1 2(1−η) ([ |x| − η ] + )2, where [.]+ is the positive part operator, [s]+ = s for s > 0, and zero otherwise."
    }, {
      "heading" : "5.2 Case II: Smooth `, Non-Strongly Convex Separable r",
      "text" : "In case II, we consider mapping the input problem (I) to objective (A), where ` is assumed to be smooth, and r non-strongly convex and separable. For smooth losses in (A), we can consider as examples those provided in Subsection 5.1, e.g., the least squares loss or logistic loss. For an example of a non-strongly convex regularizer, we consider the important case of L1 regularization below. Again, we note that this application cannot be realized by objective (B), where it is assumed that the regularization term f∗ is strongly convex.\nNon-strongly convex r: L1 regularizer. L1 regularization is obtained in objective (A) by letting gi(·) := λ| · |. However, an additional modification is necessary to obtain primaldual convergence and certificates for this setting. In particular, we employ the modification introduced in Section 4, which will guarantee L-bounded support. Formally, we replace gi(·) = | · | by\nḡ(α) := { |α| : α ∈ [−B,B], +∞ : otherwise.\nFor large enough B, this problem yields the same solution as the original L1-objective. Note that this only affects convergence theory, in that it allows us to present a strong primal-dual\nrate (Theorem 2 for L=B). With this modified L1-regularizer, the optimization problem (A) with regularization parameter λ becomes\nmin α∈Rn f(Aα) + λ n∑ i=1 ḡ(αi) . (23)\nFor large enough choice of the value B, this problems yields the same solution as the original objective:\nmin α∈Rn\n{ OA(α) := f(Aα) + λ n∑ i=1 |αi| } . (24)\nThe modified ḡ is simply a constrained version of the absolute value to the interval [−B,B]. Therefore by setting B to a large enough value that the values of αi will never reach it, ḡ∗ will be continuous and at the same time make (23) equivalent to (24). Formally, a simple way to obtain a large enough value of B, so that all solutions of (24) are unaffected, is the following: If we start the algorithm at α = 0, for every solution encountered during execution, the objective values will never become worse than OA(0). Formally, under the assumption that f is non-negative, we will have that (for each i):\nλ|αi| ≤ f(0) = OA(0) =⇒ |αi| ≤ f(0)\nλ .\nWe can therefore safely set the value of B as f(0)λ . For the modified ḡi, the conjugate ḡ ∗ i is given by:\nḡ∗i (x) := { 0 : x ∈ [−1, 1], B(|x| − 1) : otherwise.\nWe provide a proof of this in Appendix B (Lemma 6).\nNon-strongly convex r: group lasso. The group lasso penalty can be mapped to objective (A), with:\ng(α) := λ P∑ p=1 ‖αIp‖2 with P⋃ p=1 Ip = {1, . . . , n} , (25)\nwhere the disjoint sets Ip ⊆ {1, . . . , n} represent a partitioning of the total set of variables. This penalty can be viewed as an intermediate between a pure L1 or L2 penalty, performing variable selection only at the group level. The term αIp ∈ R|Ip| denotes part of the vector α with indices Ip. The conjugate is given by:\ng∗(w) = I{w|maxIp∈[n] ‖αIp‖2≤λ}(w).\nFor details, see, e.g., Dünner et al. (2016) or Boyd and Vandenberghe (2004, Example 3.26)."
    }, {
      "heading" : "5.3 Case III: Non-Smooth Separable `, Strongly Convex r",
      "text" : "Finally, in case III, we consider mapping the input problem (I) to objective (B), where ` is assumed to be non-smooth and separable, and r strongly convex. We discuss two common cases of general non-smooth losses `, including the the hinge loss for classification and absolute deviation loss for regression. When paired with a strongly convex regularizer, the regularizer via f gives rise to the primal-dual mapping, and Theorem 2 provides a sublinear convergence rate for objectives of this form. We note that these losses cannot be realized directly by objective (A), where it is assumed that the data fit term f is smooth.\nNon-smooth `: hinge loss. For classification problems, we can consider a hinge loss support vector machine model, on n training points in Rd, given with the loss:\ng∗(−A>w) = n∑ i=1 g∗i (−x>i w) := 1 n n∑ i=1 max{0, 1− yix>i w}. (26)\nThe conjugate function of the hinge loss φ(a) = max{0, 1 − b} is given by φ∗(b) = {b if b ∈ [−1, 0], else ∞ .}. When using the L2 norm for regularization in this problem: f∗(w) := λ‖w‖22, a primal-dual mapping is given by: w(α) := 1λnAα.\nNon-smooth `: absolute deviation loss. The absolute deviation loss, used, e.g., in quantile regression or least absolute deviation regression, can be realized in objective (B) by setting:\ng∗(−A>w) = n∑ i=1 g∗i (−x>i w) := 1 n n∑ i=1 ∣∣∣x>i w − yi∣∣∣ . (27) The conjugate function of the absolute deviation loss φ(a) = |a − yi| is given by φ∗(−b) = −byi, with b ∈ [−1, 1]."
    }, {
      "heading" : "5.4 Local Solvers",
      "text" : "As discussed in Section 3, the subproblems solved on each machine in the CoCoA framework are appealing in that they are very similar in structure to the global problem (A), with the main difference being that they are defined on a smaller (local) subset of the data, and have a simpler dependence on the term f . Therefore, solvers which have already proven their value in the single machine or multicore setting can be easily leveraged within the framework. We discuss some specific examples of local solvers below, and point the reader to Ma et al. (2015a) for an empirical exploration of these choices.\nIn the primal setting (Algorithm 2), the local subproblem (10) becomes a simple quadratic problem on the local data, with regularization applied only to local variables α[k]. For the L1 examples discussed, existing fast L1-solvers for the single-machine case, such as glmnet variants (Friedman et al., 2010) or blitz (Johnson and Guestrin, 2015) can be directly applied to each local subproblem Gσ′k ( · ;v,α[k]) within Algorithm 1. The sparsity induced on the subproblem solutions of each machine naturally translates into the sparsity of the global solution, since the local variables α[k] will be concatenated.\nIn terms of the approximation quality parameter Θ for the local problems (Assumption 1), we can apply existing recent convergence results from the single machine case. For\nexample, for randomized coordinate descent (as part of glmnet), Lu and Xiao (2013, Theorem 1) gives a O(1/t) approximation quality for any separable regularizer, including L1 and elastic net; see also Tappenden et al. (2015) and Shalev-Shwartz and Tewari (2011).\nIn the dual setting (Algorithm 3) for the discussed examples, the losses are applied only to local variables α[k], and the regularizer is approximated via a quadratic term. Current state of the art for the problems of the form in (B) are variants of randomized coordinate ascent—Stochastic Dual Coordinate Ascent (SDCA) (Shalev-Shwartz and Zhang, 2013a). This algorithm and its variants are increasingly used in practice (Wright, 2015), and extensions such as accelerated and parallel versions can directly be applied (Shalev-Shwartz and Zhang, 2014; Fan et al., 2008) in our framework. For non-smooth losses such as SVMs, the analysis of Shalev-Shwartz and Zhang (2013a) provides a O(1/t) rate, and for smooth losses, a faster linear rate. There have also been recent efforts to derive a linear convergence rate for problems like the hinge-loss support vector machine that could be applied, e.g., by using error bound conditions (Necoara and Nedelcu, 2014; Wang and Lin, 2014), weak strong convexity conditions (Ma et al., 2015c; Necoara, 2015) or by considering Polyak-Łojasiewicz conditions (Karimi et al., 2016)."
    }, {
      "heading" : "6. Experiments",
      "text" : "In this section we demonstrate the empirical performance of CoCoA in the distributed setting. We first compare CoCoA to competing methods for two common machine learning applications: lasso regression (Section 6.1) and support vector machine (SVM) classification (Section 6.2). We then explore the performance of CoCoA in the primal versus the dual directly by solving an elastic net regression model with both variants (Section 6.3). Finally, we illustrate general properties of the CoCoA method empirically in Section 6.4.\nExperimental setup. We compare CoCoA to numerous state-of-the-art general-purpose methods for large-scale optimization, including:\n• Mb-SGD: Mini-batch stochastic gradient. For our experiments with lasso, we compare against Mb-SGD with an L1-prox.\n• GD: Full gradient descent. For lasso we use the proximal version, Prox-GD. • L-BFGS: Limited-memory quasi-Newton method. For lasso, we use OWL-QN (orthant-\nwise limited quasi-Newton).\n• ADMM: Alternating direction method of multipliers. We use conjugate gradient internally for the lasso experiments, and SDCA for SVM experiments.\n• Mb-CD: Mini-batch parallel coordinate descent. For SVM experiments, we implement Mb-SDCA (mini-batch stochastic dual coordinate ascent).\nThe first three methods are optimized and implemented in Apache Spark’s MLlib (v1.5.0) (Meng et al., 2016). We test the performance of each method in large-scale experiments fitting lasso, elastic net regression, and SVM models to the datasets shown in Table 5. In comparing to other methods, we plot the distance to the optimal primal solution. This optimal value is calculated by running all methods for a large number of iterations (until progress has stalled), and then selecting the smallest primal value amongst the re-\nsults. All code is written in Apache Spark and experiments are run on public-cloud Amazon EC2 m3.xlarge machines with one core per machine. Our code is publicly available at github.com/gingsmith/proxcocoa.\nWe carefully tune each competing method in our experiments for best performance. ADMM requires the most tuning, both in selecting the penalty parameter ρ and in solving the subproblems. Solving the subproblems to completion for ADMM is prohibitively slow, and we thus use an iterative method internally and improve performance by allowing early stopping. We also use a varying penalty parameter ρ — practices described in Boyd et al. (2010, Sections 4.3, 8.2.3, 3.4.1). For Mb-SGD, we tune the step size and mini-batch size parameters. For Mb-CD and Mb-SDCA, we scale the updates at each round by βb for mini-batch size b and β ∈ [1, b], and tune both parameters b and β. Further implementation details for all methods are given in Section 6.5.\nFor simplicity of presentation and comparison, in all of the following experiments, we restrict CoCoA to only use simple coordinate descent as the local solver. We note that even stronger empirical results for CoCoA could be obtained by plugging in state of the art local solvers for each application at hand.\n6.1 CoCoA in the Primal\nWe first demonstrate the performance of CoCoA in the primal (Algorithm 2) by applying CoCoA to a lasso regression model (8) fit to the distributed datasets in Table 5. We use stochastic coordinate descent as a local solver for CoCoA, and select the number of local iterations H (a proxy for subproblem approximation quality, Θ) from several options with best performance.\nWe compare CoCoA to the general methods listed above, including Mb-SGD with an L1-prox, Prox-GD, OWL-QN, ADMM and Mb-CD. A comparison with Shotgun (Bradley et al., 2011), a popular method for solving L1-regularized problems in the multicore environment, is provided as an extreme case to highlight the detrimental effects of frequent communication in the distributed environment. For Mb-CD, Shotgun, and CoCoA in the primal, datasets are distributed by feature, whereas for Mb-SGD, Prox-GD, OWL-QN and ADMM they are distributed by training point.\nIn analyzing the performance of each algorithm (Figure 1), we measure the improvement to the primal objective given in (A) (OA(α)) in terms of wall-clock time in seconds. We see that both Mb-SGD and Mb-CD are slow to converge, and come with the additional burden of having to tune extra parameters (though Mb-CD makes clear improvements over Mb-SGD). As expected, naively distributing Shotgun (single coordinate updates per\nmachine) does not perform well, as it is tailored to shared-memory systems and requires communicating too frequently. OWL-QN performs the best of all compared methods, but is still much slower to converge than CoCoA, and converges, e.g., 50× more slowly for the webspam dataset. The optimal performance of CoCoA is particularly evident in datasets with large numbers of features (e.g., url, kddb, webspam), which are exactly the datasets of interest for L1 regularization.\nResults are shown for regularization parameters λ such that the resulting weight vector α is sparse. However, our results are robust to varying values of λ as well as to various problem settings, as we illustrate in Figure 2.\nA case against smoothing. We additionally motivate the use of CoCoA in the primal by showing how it improves upon CoCoA in the dual (Yang, 2013; Jaggi et al., 2014; Ma et al., 2015b,a) for non-strongly convex regularizers. First, CoCoA in the dual cannot be included in the set of experiments in Figure 1 because it cannot be directly applied to the lasso objective (recall that Algorithm 3 only allows for strongly convex regularizers).\nSeconds 0 100 200 300 400 500 600 700 800\nP rim\nal S\nub op\ntim al\nity : O\nA (,\n) -\nO A (,\n*)\n10-3\n10-2\n10-1\n100 Epsilon - Lasso: Convergence Across 6\nCoCoA-Primal 6=1e-4 OWL-QN 6=1e-4 CoCoA-Primal 6=1e-5 OWL-QN 6=1e-5 CoCoA-Primal 6=1e-6 OWL-QN 6=1e-6\nSeconds 0 100 200 300 400 500\nP rim\nal S\nub op\ntim al\nity : O\nA (,\n) -\nO A (,\n*)\n10-3\n10-2\n10-1\n100 Url - Elastic Net: Convergence Across 2\nCoCoA-Primal 2=.25 OWL-QN 2=.25 CoCoA-Primal 2=.5 OWL-QN 2=.5 CoCoA-Primal 2=.75 OWL-QN 2=.75\nFigure 3 & Table 6: For pure L1 regularization, Nesterov smoothing is not an effective option for CoCoA in the dual. It either modifies the solution (Figure 3) or slows convergence (Table 6). This motivates running CoCoA instead on the primal for these problems.\nTo get around this requirement, previous work has suggested implementing the Nesterov smoothing technique used in, e.g., Shalev-Shwartz and Zhang (2014); Zhang and Lin (2015) — adding a small amount of strong convexity δ‖α‖22 to the objective for lasso regression. In Figure 3 we demonstrate the issues with this approach, comparing CoCoA in the primal on a pure L1-regularized regression problem to CoCoA in the dual for decreasing levels of δ. The smaller we set δ, the less smooth the problem becomes. As δ decreases, the final sparsity of running CoCoA in the dual starts to match that of running pure L1 (Table 6), but the performance also degrades (Figure 3). We note that by using CoCoA in the primal with the modification presented in Section 4, we can deliver strong rates without having to make these fundamental alterations to the problem of interest.\n6.2 CoCoA in the Dual\nNext we present results on CoCoA in the dual against competing methods, for an SVM model (9) on the datasets in Table 5. We use stochastic dual coordinate ascent (SDCA) as a local solver for CoCoA in this setting, again selecting the number of local iterations H from several options with best performance. We compare CoCoA to the general methods listed above, including Mb-SGD, GD, L-BFGS, ADMM, and Mb-SDCA. All datasets are distributed by training point for these methods.\nIn analyzing the performance the methods in this setting (Figure 3), we measure the improvement to the primal objective given in (B) (OB(w)) in terms of wall-clock time in seconds. We see again that Mb-SGD and Mb-CD are slow to converge, and come with the additional burden of having to tune extra parameters. ADMM performs the best of the methods other than CoCoA, followed by L-BFGS. However, both are still much slower to converge than CoCoA in the dual. ADMM was in particular affected by the fact that many internal iterations of SDCA were necessary in order to guarantee convergence. In contrast,\nCoCoA is able to incorporate arbitrary amounts of work locally and still converge. We note that although CoCoA, ADMM and Mb-SDCA run in the dual, the plots in Figure 3 mark progress towards the primal objective, OB(w)."
    }, {
      "heading" : "6.3 Primal vs. Dual",
      "text" : "To understand the effect of primal versus dual optimization for CoCoA, we compare the performance of both variants by fitting an elastic net regression model (7) to two datasets. For comparability of the methods, we use coordinate descent (with closed-form updates) as the local solver in both variants. From the results in Figure 4, we see that CoCoA in the dual tends to perform better on datasets with a large number of training points (relative to the number of features), and that as expected, the performance deteriorates as the strong convexity in the problem disappears. In contrast, CoCoA in the primal performs well on datasets with a large number of features relative to training points, and is robust to changes in strong convexity. These changes in performance are to be expected, as we have already discussed that CoCoA in the primal is more suited for non-strongly convex regularizers (Section 6.1), and that the feature size dominates communication for CoCoA in the dual, as compared to the training point size for CoCoA in the primal (Section 3.4)."
    }, {
      "heading" : "6.4 General Properties: Effect of Communication",
      "text" : "Finally, we note that in contrast to the compared methods from Sections 6.1 and 6.2, CoCoA comes with the benefit of having only a single parameter to tune: the subproblem approximation quality, Θ, which we control in our experiments via the number of local subproblem iterations, H, for the example of local coordinate descent. We further explore the effect of this parameter in Figure 5, and provide a general guideline for choosing it in prac-\ntice (see Remark 1). In particular, we see that while increasing H always results in better performance in terms of the number of communication rounds, smaller or larger values of H may result in better performance in terms of wall-clock time, depending on the cost of communication and computation. The flexibility to fine-tune H is one of the reasons for CoCoA’s significant performance gains."
    }, {
      "heading" : "6.5 Experiment Details",
      "text" : "In this subsection we provide thorough details on the experimental setup and methods used in our comparison. All experiments are run on Amazon EC2 clusters of m3.xlarge machines, with one core per machine. The code for each method is written in Apache Spark, v1.5.0. Our code is open source and publicly available at github.com/gingsmith/proxcocoa.\nADMM. Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2010) is a popular method that lends itself naturally to the distributed environment. For lasso regression, implementing ADMM for the problems of interest requires solving a large linear system Cx = d on each machine, where C ∈ Rn×n with n scaling beyond 107 for the datasets in Table 5, and with C being possibly dense. It is prohibitively slow to solve this directly on each machine, and we therefore employ the iterative method of conjugate gradient with early stopping (see, e.g., Boyd et al., 2010, Section 4.3). For SVM classification, we use stochastic dual coordinate ascent as an internal optimizer, which is shown in Zhang et al. (2012) to have superior performance. We further improve performance by using a varying rather than constant penalty parameter, as suggested in Boyd et al. (2010, Section 3.4.1).\nMini-batch SGD and proximal GD. Mini-batch SGD is a standard and widely used method for parallel and distributed optimization. We use the optimized code provided in Spark’s machine learning library, MLlib, v1.5.0 (Meng et al., 2016). We tune both the size of the mini-batch and the SGD step size using grid search. For lasso, we use the proximal version of the method. Full gradient descent can be seen as a specific setting of mini-batch\nSGD, where the mini-batch size is equal to the total number of training points. We thus also use the implementation in MLlib for full GD, and tune the step size parameter using grid search.\nMini-batch CD and SDCA. Mini-batch CD (for lasso) and SDCA (for SVM) aim to improve mini-batch SGD by employing coordinate descent, which has theoretical and practical justifications (Shalev-Shwartz and Tewari, 2011; Takáč et al., 2015; Fercoq and Richtárik, 2015; Tappenden et al., 2015; Takáč et al., 2013). We implement mini-batch CD and SDCA in Spark and scale the updates made at each round by βb for mini-batch size b and β ∈ [1, b], tuning both parameters b and β via grid search. For the case of lasso regression, we implement Shotgun (Bradley et al., 2011), which is a popular method for parallel optimization. Shotgun can be seen an extreme case of mini-batch CD where the mini-batch is set to K, i.e., there is a single update made by each machine per round. We see in the experiments that communicating this frequently becomes prohibitively slow in the distributed environment.\nOWL-QN. OWN-QN (Yu et al., 2010) is a quasi-Newton method optimized in Spark’s spark.ml package (Meng et al., 2016). Outer iterations of OWL-QN make significant progress towards convergence, but the iterations themselves can be slow because they require processing the entire dataset. CoCoA, the mini-batch methods, and ADMM with early stopping all improve on this by allowing the flexibility of only a subset of the dataset to be processed at each iteration. CoCoA and ADMM have even greater flexibility by allowing internal methods to process the dataset more than once. CoCoA makes this approximation quality explicit, both in theoretical convergence rates and by providing general guidelines for setting the parameter.\nCoCoA. We implement CoCoA with coordinate descent as the local solver. We note that since the framework and theory allow any internal solver to be used, CoCoA could benefit even beyond the results shown, e.g., by using existing fast L1-solvers for the singlemachine case, such as glmnet variants (Friedman et al., 2010) or blitz (Johnson and Guestrin, 2015) or SVM solvers like liblinear (Fan et al., 2008). The only parameter necessary to tune for CoCoA is the level of approximation quality, which we parameterize in the experiments through H, the number of local iterations of the iterative method run locally. Our theory relates local approximation quality to global convergence (Section 4), and we provide a guideline for how to choose this value in practice that links the parameter to the systems environment at hand (Remark 1)."
    }, {
      "heading" : "7. Related Work",
      "text" : "Single-machine coordinate solvers. For strongly convex regularizers, the current stateof-the-art for empirical loss minimization is randomized coordinate ascent on the dual (SDCA) (Shalev-Shwartz and Zhang, 2013a) and its accelerated variants (e.g., ShalevShwartz and Zhang, 2014). In contrast to primal stochastic gradient descent (SGD) methods, the SDCA family is often preferred as it is free of learning-rate parameters and has faster (geometric) convergence guarantees. Interestingly, a similar trend in coordinate solvers has been observed in the recent literature on the lasso, but with the roles of primal and dual reversed. For those problems, coordinate descent methods on the primal have become state-\nof-the-art, as in glmnet (Friedman et al., 2010) and extensions (Yuan et al., 2012); see, e.g., the overview in Yuan et al. (2010). However, primal-dual convergence rates for unmodified coordinate algorithms have to our knowledge only been obtained for strongly convex regularizers to date (Shalev-Shwartz and Zhang, 2014; Zhang and Lin, 2015).\nCoordinate descent on L1-regularized problems (i.e., (A) with g(·) = λ‖·‖1) can be interpreted as the iterative minimization of a quadratic approximation of the smooth part of the objective (as in a one-dimensional Newton step), followed by a shrinkage step resulting from the L1 part. In the single-coordinate update case, this is at the core of glmnet (Friedman et al., 2010; Yuan et al., 2010), and widely used in, e.g., solvers based on the primal formulation of L1-regularized objectives (Shalev-Shwartz and Tewari, 2011; Yuan et al., 2012; Bian et al., 2013; Fercoq and Richtárik, 2015; Tappenden et al., 2015). When changing more than one coordinate at a time, again employing a quadratic upper bound on the smooth part, this results in a two-loop method as in glmnet for the special case of logistic regression. This idea is crucial for the distributed setting. When the set of active coordinates coincides with the ones on the local machine, these single-machine approaches closely resemble the distributed framework proposed here.\nParallel methods. For the general regularized loss minimization problems of interest, methods based on stochastic subgradient descent (SGD) are well-established. Several variants of SGD have been proposed for parallel computing, many of which build on the idea of asynchronous communication (Niu et al., 2011; Duchi et al., 2013). Despite their simplicity and competitive performance on shared-memory systems, the downside of this approach in the distributed environment is that the amount of required communication is equal to the amount of data read locally, since one data point is accessed per machine per round (e.g., mini-batch SGD with a batch size of one per worker). These variants are in practice not competitive with the more communication-efficient methods considered in this work, which allow more local updates per communication round.\nFor the specific case of L1-regularized objectives, parallel coordinate descent (with and without using mini-batches) was proposed in Bradley et al. (2011) (Shotgun) and generalized in Bian et al. (2013), and is among the best performing solvers in the parallel setting. Our framework reduces to Shotgun as a special case when the internal solver is a singlecoordinate update on the subproblem (10), γ = 1, and for a suitable σ′. However, Shotgun is not covered by our convergence theory, since it uses a potentially unsafe upper bound of β instead of σ′, which isn’t guaranteed to satisfy our condition for convergence (11). We compare empirically with Shotgun in Section 6 to highlight the detrimental effects of running this high-communication method in the distributed environment.\nOne-shot communication schemes. At the other extreme, there are distributed methods that use only a single round of communication, such as Mann et al. (2009); Zinkevich et al. (2010); Zhang et al. (2013); McWilliams et al. (2014); and Heinze et al. (2016). These methods require additional assumptions on the partitioning of the data, which are usually not satisfied in practice if the data are distributed “as is”, i.e., if we do not have the opportunity to distribute the data in a specific way beforehand. Furthermore, some cannot guarantee convergence rates beyond what could be achieved if we ignored data residing on all but a single computer, as shown in Shamir et al. (2014). Additional relevant lower bounds on\nthe minimum number of communication rounds necessary for a given approximation quality are presented in Balcan et al. (2012) and Arjevani and Shamir (2015).\nMini-batch methods. Mini-batch methods (which use updates from several training points or features per round) are more flexible and lie within the two extremes of parallel and one-shot communication schemes. However, mini-batch versions of both SGD and coordinate descent (CD) (e.g., Takáč et al., 2013; Shalev-Shwartz and Zhang, 2013b; Qu et al., 2015; Richtárik and Takáč, 2016) suffer from their convergence rate degrading towards the rate of batch gradient descent as the size of the mini-batch is increased. This follows because mini-batch updates are made based on the outdated previous parameter vector w, in contrast to methods that allow immediate local updates like CoCoA.\nAnother disadvantage of mini-batch methods is that the aggregation parameter is more difficult to tune, as it can lie anywhere in the order of mini-batch size. The optimal choice is often either unknown or too challenging to compute in practice. In the CoCoA framework there is no need to tune parameters, as the aggregation parameter and subproblem parameters can be set directly using the safe bound discussed in Section 3 (Definition 5).\nBatch solvers. ADMM (Boyd et al., 2010), gradient descent, and quasi-Newton methods such as L-BFGS and are also often used in distributed environments because of their relatively low communication requirements. However, they require at least a full (distributed) batch gradient computation at each round, and therefore do not allow the gradual trade-off between communication and computation provided by CoCoA. In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007).\nFinally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness Θ for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods.\nDistributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al. (2011); Yang (2013); Yang et al. (2013) and Lee and Roth (2015), the CoCoA-v1 and CoCoA+ frameworks (which are special cases of the presented framework, CoCoA) are the first to allow the use of any local solver—of weak local approximation quality—in each round in the distributed setting. The practical variant of the DisDCA (Yang, 2013), called DisDCA-p, allows for additive updates in a similar manner to CoCoA, but is restricted to coordinate decent (CD) being the local solver, and was initially proposed without convergence guarantees. DisDCA-p, CoCoA-v1, and CoCoA+ are all limited to strongly convex regularizers, and therefore are not as general as the CoCoA framework discussed in this work.\nIn the L1-regularized setting, an approach related to our framework includes distributed variants of glmnet as in Mahajan et al. (2014). Inspired by glmnet and Yuan et al.\n(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression.\nIf hypothetically each of our quadratic subproblems Gσ′k (∆α[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality Θ as in our framework, the works of Trofimov and Genkin (2014); Yuan et al. (2012); and Yen et al. (2015) assume that the quadratic subproblems are solved exactly. Therefore, these methods are not able to freely trade off communication and computation. Also, they do not allow the re-use of arbitrary local solvers. On the theoretical side, the convergence rate results provided by Mahajan et al. (2014); Trofimov and Genkin (2014); and Yuan et al. (2012) are not explicit convergence rates but only asymptotic, as the quadratic upper bounds are not explicitly controlled for safety as with our σ′."
    }, {
      "heading" : "8. Discussion",
      "text" : "To enable large-scale machine learning, we have developed, analyzed, and evaluated a general-purpose framework for communication-efficient primal-dual optimization in the distributed environment. Our framework, CoCoA, takes a unique approach by using duality to derive subproblems for each machine to solve in parallel. These subproblems closely match the global problem of interest, which allows for state-of-the-art single-machine solvers to easily be re-used in the distributed setting. Further, by allowing the local solvers to find solutions of arbitrary approximation quality to the subproblems on each machine, our framework permits a highly flexible communication scheme. In particular, as the local solvers make updates directly to their local parameters, the need to communicate reduces and can be adapted to the system at hand, which helps to manage the communication bottleneck in the distributed setting.\nWe analyzed the impact of the local solver approximation quality and derived global primal-dual convergence rates for our framework that are agnostic to the specifics of the local solvers. We have taken particular care in extending our framework to the case of nonstrongly convex regularizers, where we introduced a bounded-support modification technique to provide robust convergence guarantees. Finally, we demonstrated the efficiency of our framework in an extensive experimental comparison with state-of-the-art distributed solvers. Our framework achieves up to a 50× speedup over other widely-used methods on real-world distributed datasets."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Michael P. Friedlander, Jakub Konečný, and Peter Richtárik for their help and for fruitful discussions."
    }, {
      "heading" : "Appendix A. Convex Conjugates",
      "text" : "The convex conjugate of a function f : Rd → R is defined as\nf∗(v) := max u∈Rd v>u− f(u) . (28)\nBelow we list several useful properties of conjugates (see, e.g., Boyd and Vandenberghe, 2004, Section 3.3.2):\n• Double conjugate: (f∗)∗ = f if f is closed and convex.\n• Value Scaling: (for α > 0) f(v) = αg(v) ⇒ f∗(w) = αg∗(w/α) .\n• Argument Scaling: (for α 6= 0) f(v) = g(αv) ⇒ f∗(w) = g∗(w/α) .\n• Conjugate of a separable sum: f(v) = ∑ i φi(vi) ⇒ f∗(w) = ∑ i φ ∗ i (wi) .\nLemma 4 (Duality between Lipschitzness and L-Bounded Support, (Rockafellar, 1997, Corollary 13.3.3)). Given a proper convex function f , it holds that f is L-Lipschitz if and only if f∗ has L-bounded support.\nLemma 5 (Duality between Smoothness and Strong Convexity, (Hiriart-Urruty and Lemaréchal, 2001, Theorem 4.2.2)). Given a closed convex function f , it holds that f is µ strongly convex w.r.t. the norm ‖ · ‖ if and only if f∗ is (1/µ)-smooth w.r.t. the dual norm ‖ · ‖∗."
    }, {
      "heading" : "Appendix B. Proofs of Primal-Dual Relationships",
      "text" : "In the following subsections we provide derivations of the primal-dual relationship of the general objectives (A) and (B), and then show how to derive the conjugate of the modified L1-norm, as an example of the bounded-support modification introduced in Section 4.\nB.1 Primal-Dual Relationship\nThe relation of our original formulation (A) to its dual formulation (B) is standard in convex analysis, and is a special case of the concept of Fenchel Duality. Using the combination with the linear map A as in our case, the relationship is called Fenchel-Rockafellar Duality, see e.g. Borwein and Zhu (2005, Theorem 4.4.2) or Bauschke and Combettes (2011, Proposition 15.18). For completeness, we illustrate this correspondence with a self-contained derivation of the duality.\nStarting with the original formulation (A), we introduce an auxiliary vector v ∈ Rd representing v = Aα. Then optimization problem (A) becomes:\nmin α∈Rn\nf(v) + g(α) such that v = Aα . (29)\nIntroducing Lagrange multipliers w ∈ Rd, the Lagrangian is given by:\nL(α,v;w) := f(v) + g(α) + w> (Aα− v) .\nThe dual problem of (A) follows by taking the infimum with respect to both α and v:\ninf α,v L(w,α,v) = inf v\n{ f(v)−w>v } + inf\nα\n{ g(α) + w>Aα } = − sup\nv\n{ w>v − f(v) } − sup\nα\n{ (−w>A)α− g(α) } = −f∗(w)− g∗(−A>w) . (30)\nWe change signs and turn the maximization of the dual problem (30) into a minimization, thereby arriving at the dual formulation (B) as claimed:\nmin w∈Rd\n[ OB(w) := g∗(−A>w) + f∗(w) ] .\nB.2 Continuous Conjugate Modification for Indicator Functions\nLemma 6 (Conjugate of the modified L1-norm). The convex conjugate of the bounded support modification of the L1-norm, as defined in (18), is:\nḡ∗i (x) := { 0 : x ∈ [−1, 1], B(|x| − 1) : otherwise,\nand is B-Lipschitz.\nProof. We start by applying the definition of convex conjugate:\nḡi(α) = sup x∈R\n[αx− ḡ∗i (x)] .\nWe begin by looking at the case in which α ≥ B; in this case it’s easy to see that when x→ +∞, we have:\nαx−B(|x| − 1) = (α−B)x−B → +∞ ,\nas α − B ≥ 0. The case α ≤ −B holds analogously. We’ll now look at the case α ∈ [0, B]; in this case it is clear we must have x? ≥ 0. It also must hold that x? ≤ 1, since\nαx−B(x− 1) < αx ,\nfor every x > 1. Therefore the maximization becomes\nḡi(α) = sup x∈[0,1] αx ,\nwhich has maximum α at x = 1. The remaining α ∈ [−B, 0] case follows in similar fashion. Lipschitz continuity of ḡ∗i follows directly, or alternatively also from the general result that g∗i is L-Lipschitz if and only if gi has L-bounded support (Rockafellar, 1997, Corollary 13.3.3) or (Dünner et al., 2016, Lemma 5)."
    }, {
      "heading" : "Appendix C. Comparison to ADMM",
      "text" : "Here we derive the comparison of ADMM and CoCoA discussed in Section 3.6, following the line of reasoning in Yang (2013). For consensus ADMM, the objective (B) is decomposed using the following re-parameterization:\nmax w1,...wK ,w K∑ k=1 ∑ i∈Pk g∗(−x>i wk) + f∗(w)\ns.t. wk = w, k = 1, . . . ,K.\nTo solve this problem, we construct the augmented Lagrangian:\nLρ(w1, . . . ,wk,u1, . . . ,uk,w) := K∑ k=1 ∑ i∈Pk g∗(−x>i wk)\n+ f∗(w) + ρ K∑ k=1 u>k (wk −w) + ρ 2 K∑ k=1 ‖wk −w‖2 ,\nwhich yields the following decomposable updates:\nw (t) k = arg min\nwk\n∑ i∈Pk g∗(−x>i wk) + ρ 2 ‖wk −w(t−1) + u (t−1) k ‖ 2,\nw(t) = arg min w f∗(w) + ρ K∑ k=1 u>k (wk −w) + ρ 2 K∑ k=1 ‖wk −w‖2,\nu (t) k = u (t−1) k + w (t) k −w (t).\nTo compare this to the proposed framework, recall that the subproblem (10) (excluding the extraneous term f(v)) can be written as:\nmin α[k]∈Rn ∑ i∈Pk gi(α[k]i) + w >Aα[k] + σ′ 2τ ∥∥∥A[k]α[k]∥∥∥2. We can further reformulate by completing the square:\nmin α[k]∈Rn ∑ i∈Pk gi((α[k])i) + τ 2σ′ ∥∥∥ w + σ′ τ A[k]α[k] ∥∥∥2. Assuming for the time being that f(·) = 12‖ · ‖ 2 2 such that w = ∇f(v) = v, we can unroll\nthe update as follows, using γ∆v(t−1) = γ ∑K\ni=1 ∆v (t−1) k :\nmin α[k]∈Rn ∑ i∈Pk gi((α[k])i) + τ 2σ′ ∥∥∥ w(t−1) + γ∆v(t−1) + σ′ τ A[k]α[k] ∥∥∥2. We will show that the above objective has the following primal form for each machine k:\nmin w ∑ i∈Pk g∗i (−x>i w) + τ 2σ′ ∥∥∥w − (w(t−1) + γ∆v(t−1))∥∥∥2. (31)\nIndeed, suppressing the subscript k for simplicity, we have:\nmin w ∑ i g∗i (−x>i w) + τ 2σ′ ∥∥∥w − (w(t−1) + γ∆v(t−1))∥∥∥2 = min\nw ∑ i max αi −x>i wαi − gi(αi) + τ 2σ′ ∥∥∥w − (w(t−1) + γ∆v(t−1))∥∥∥2 = max\nα min w ∑ i −x>i wαi − gi(αi) + τ 2σ′ ∥∥∥w − (w(t−1) + γ∆v(t−1))∥∥∥2. Solving the minimization yields: w = wt−1 +γ∆v(t−1) + σ ′\nτ Aα. Plugging this back in yields:\n= max α ∑ i −gi(αi)− (Aα)>w(t−1) − (Aα)>γ∆v(t−1) − σ′ τ ‖Aα‖2 + τ 2σ′ ∥∥∥σ′ τ Aα ∥∥∥2\n= max α ∑ i −gi(αi)− (Aα)>w(t−1) − (Aα)>γ∆v(t−1) − σ′ 2τ ‖Aα‖2\n= min α ∑ i gi(αi) + (Aα) >w(t−1) + (Aα)>γ∆v(t−1) + σ′ 2τ ‖Aα‖2\n= min α ∑ i gi(αi) + τ 2σ′ ∥∥∥ w(t−1) + γ∆v(t−1) + σ′ τ Aα ∥∥∥2."
    }, {
      "heading" : "Appendix D. Convergence Proofs",
      "text" : "In this section we provide proofs of our main convergence results. The arguments follow the reasoning in Ma et al. (2015b,a), but where we have generalized them to be applicable directly to (A). We provide full details of Lemma 1 as a proof of concept, but omit details in later proofs that can be derived using the arguments in Ma et al. (2015b) or earlier work of Shalev-Shwartz and Zhang (2013a), and instead outline the proof strategy and highlight sections where the theory deviates.\nD.1 Approximation of OA(·) by the Local Subproblems Gσ ′ k (·)\nOur first lemma in the overall proof of convergence helps to relate progress on the local subproblems to the global objective OA(·).\nLemma’ 1. For any dual variables α,∆α ∈ Rn, v = v(α) := Aα, and real values γ, σ′ satisfying (11), it holds that\nOA ( α + γ K∑ k=1 ∆α[k] ) ≤ (1− γ)OA(α) + γ K∑ k=1 Gσ′k (∆α[k];v,α[k]) . (32)\nProof. In this proof we follow the line of reasoning in Ma et al. (2015b, Lemma 4) with a more general (1/τ) smoothness assumption on f(·). An outer iteration of CoCoA performs\nthe following update:\nOA(α + γ K∑ k=1 ∆α[k]) = f(v(α + γ K∑ k=1\n∆α[k]))︸ ︷︷ ︸ A\n+ n∑ i=1 gi(αi + γ( K∑ k=1\n∆α[k])i)︸ ︷︷ ︸ B\n. (33)\nWe bound A and B separately. First we bound A using (1/τ)-smoothness of f :\nA = f ( v(α + γ K∑ k=1 ∆α[k]) ) = f ( v(α) + γ K∑ k=1 v(∆α[k]) )\nsmoothness of f as in (3) ≤ f(v(α)) + K∑ k=1 γ∇f(v(α))>v(∆α[k]) + γ2 2τ ‖ K∑ k=1 v(α[k])‖2\ndefinition of w as in (5) ≤ f(v(α)) + K∑ k=1 γv(∆α[k]) >w(α) + γ2 2τ ‖ K∑ k=1 v(α[k])‖2\nsafe choice of σ′ as in (11) ≤ f(v(α)) + K∑ k=1 γv(∆α[k]) >w(α) + 1 2τ γσ′ K∑ k=1 ‖v(α[k])‖2 .\nNext we use Jensen’s inequality to bound B:\nB = K∑ k=1 ∑ i∈Pk gi(αi + γ(∆α[k])i)  = K∑ k=1 ∑ i∈Pk gi((1− γ)αi + γ(α + ∆α[k])i)  ≤\nK∑ k=1 ∑ i∈Pk (1− γ)gi(αi) + γgi(αi + ∆α[k]i)  . Plugging A and B back into (33) yields:\nOA ( α + γ K∑ k=1 ∆α[k] ) ≤ f(v(α))± γf(v(α)) + K∑ k=1 γv(∆α[k]) >w(α) + 1 2τ γσ′ K∑ k=1 ‖v(α[k])‖2\n+ K∑ k=1 ∑ i∈Pk (1− γ)gi(αi) + γgi(αi + ∆α[k]i)\n= (1− γ)f(v(α)) + K∑ k=1 ∑ i∈Pk (1− γ)gi(αi)  ︸ ︷︷ ︸\n(1−γ)OA(α)\n+ γ K∑ k=1  1 K f(v(α)) + v(∆α[k]) >w(α) + σ′ 2τ ‖v(α[k])‖2 + ∑ i∈Pk gi(αi + ∆α[k]i)  (10) = (1− γ)OA(α) + γ\nK∑ k=1 Gσ′k (∆α[k];v) ,\nwhere the last equality is by the definition of the subproblem objective Gσ′k (.) as in (10).\nD.2 Proof of Main Convergence Result (Theorem 2)\nBefore proving the main convergence results, we introduce several useful quantities, and establish the following lemma, which characterizes the effect of iterations of Algorithm 1 on the duality gap for any chosen local solver of approximation quality Θ.\nLemma 7. Let gi be strongly convex 2 with convexity parameter µ ≥ 0 with respect to the norm ‖ · ‖, ∀i ∈ [n]. Then at each iteration of Algorithm 1 under Assumption 1, and any s ∈ [0, 1], it holds that\nE[OA(α(t))−OA(α(t+1))] ≥ γ(1−Θ) ( sG(α(t))− σ ′s2\n2τ R(t)\n) , (34)\nwhere\nR(t) := − τµ(1−s)σ′s ‖u (t) −α(t)‖2 + ∑K k=1‖A[k](u(t) −α(t))[k]‖2 , (35)\nfor u(t) ∈ Rn with\nu (t) i ∈ ∂g ∗ i (−x>i w(α(t))) . (36)\nProof. This proof is motivated by Shalev-Shwartz and Zhang (2013a, Lemma 19) and follows Ma et al. (2015b, Lemma 5), with a difference being the extension to our generalized subproblems Gσ′k (·;v,α[k]) along with the mappings w(α) := ∇f(v(α)) with v(α) := Aα.\nFor simplicity, we write α instead of α(t), v instead of v(α(t)), w instead of w(α(t)) and u instead of u(t). We can estimate the expected change of the objective OA(α) as follows. Starting from the definition of the update α(t+1) := α(t) + γ ∑ k ∆α[k] from Algorithm 1, we apply Lemma 1, which relates the local approximation Gσ′k (α;v,α[k]) to the global objective OA(α), and then bound this using the notion of quality of the local solver (Θ), as in Assumption 1. This gives us:\nE [ OA(α(t))−OA(α(t+1)) ] = E [ OA(α)−OA ( α + γ K∑ k=1 ∆α[k] )]\n≥ γ(1−Θ) OA(α)− K∑ k=1\nGσ′k (∆α?[k];v,α[k])︸ ︷︷ ︸ C\n . (37)\nWe next upper bound the C term, denoting ∆α? = ∑K\nk=1 ∆α ? [k]. We first plug in the\ndefinition of the objective OA in (A) and the local subproblems (10), and then substitute\n2. Note that the case of weakly convex gi(.) is explicitly allowed here as well, as the Lemma holds for the case µ = 0.\ns(ui − αi) for ∆α?i and apply the µ-strong convexity of the gi terms. This gives us:\nC = n∑ i=1 (gi(αi)− gi(αi + ∆α?i ))− (A∆α?)>w(α)− K∑ k=1 σ′ 2τ ∥∥∥A[k]∆α?[k]∥∥∥2 ≥\nn∑ i=1 ( sgi(αi)− sgi(ui) + µ 2 (1− s)s(ui − αi)2 ) −A(s(u−α))>w(α)−\nK∑ k=1 σ′ 2τ ∥∥∥A[k](s(u−α)[k])∥∥∥2 . (38) From the definition of the optimization problems (A) and (B), and definition of convex\nconjugates, we can write the duality gap as:\nG(α) := OA(α)− (−OB(w(α)) (A),(B) = n∑ i=1 ( g∗i (−x>i w(α)) + gi(αi) ) + f∗(w(α)) + f(Aα))\n= n∑ i=1 ( g∗i (−x>i w(α)) + gi(αi) ) + f∗(∇f(Aα)) + f(Aα)\n= n∑ i=1 ( g∗i (−x>i w(α)) + gi(αi) ) + (Aα)>w(α)\n= n∑ i=1 ( g∗i (−x>i w(α)) + gi(αi) + αix>i w(α) ) . (39)\nThe convex conjugate maximal property from (36) implies that\ngi(ui) = ui(−x>i w(α))− g∗i (−x>i w(α)) . (40)\nUsing (40) and (39), we therefore have:\nC (40) ≥ n∑ i=1 ( sgi(αi)− sui(−x>i w(α)) + sg∗i (−x>i w(α)) + µ 2 (1− s)s(ui − αi)2 ) −A(s(u−α))>w(α)−\nK∑ k=1 σ′ 2τ ∥∥∥A[k](s(u−α)[k])∥∥∥2 =\nn∑ i=1 [ sgi(αi) + sg ∗ i (−x>i w(α)) + sx>i w(α)αi ] − n∑ i=1 [ sx>i w(α)(αi − ui)− µ 2 (1− s)s(ui − αi)2 ] −A(s(u−α))>w(α)−\nK∑ k=1 σ′ 2τ ∥∥∥A[k](s(u−α)[k])∥∥∥2 (39) = sG(α) + µ\n2 (1− s)s‖u−α‖2 − σ\n′s2\n2τ K∑ k=1 ‖A[k](u−α)[k]‖2 . (41)\nThe claimed improvement bound (34) then follows by plugging (41) into (37).\nThe following Lemma provides a uniform bound on R(t):\nLemma 8. If g∗i are L-Lipschitz continuous for all i ∈ [n], then\n∀t : R(t) ≤ 4L2 K∑ k=1\nσknk︸ ︷︷ ︸ =:σ\n, (42)\nwhere\nσk := max α[k]∈Rn\n‖A[k]α[k]‖2\n‖α[k]‖2 . (43)\nProof. (Ma et al., 2015b, Lemma 6). For general convex functions, the strong convexity parameter is µ = 0, and hence the definition (35) of the complexity constant R(t) becomes\nR(t) = K∑ k=1 ‖A[k](u(t) −α(t))[k]‖2 (43) ≤ K∑ k=1 σk‖(u(t) −α(t))[k]‖2 ≤ K∑ k=1 σk|Pk|4L2 .\nHere the last inequality follows from (Shalev-Shwartz and Zhang, 2013a, Lemma 21), which shows that for g∗i : R→ R being L-Lipschitz, it holds that for any real value a with |a| > L one has that gi(a) = +∞.\nRemark 5. (Ma et al., 2015b, Remark 7) If the data points xi are normalized such that ‖xi‖ ≤ 1, ∀i ∈ [n], then σk ≤ |Pk| = nk. Furthermore, if we assume that the data partition is balanced, i.e., that nk = n/K for all k, then σ ≤ n2/K. This can be used to bound the constants R(t), above, as R(t) ≤ 4L2n2K .\nTheorem 9. Consider Algorithm 1, using a local solver of quality Θ (See Assumption 1). Let g∗i (·) be L-Lipschitz continuous, and G > 0 be the desired duality gap (and hence an upper-bound on suboptimality OA). Then after T iterations, where\nT ≥ T0 + max{ ⌈ 1 γ(1−Θ) ⌉ ,\n4L2σσ′\nτ Gγ(1−Θ) } , (44)\nT0 ≥ t0 + [ 2 γ(1−Θ) ( 8L2σσ′ τ G − 1 )] + , t0 ≥ max(0, ⌈ 1 γ(1−Θ) log ( τ(OA(α(0))−OA(α?)) 2L2σσ′ )⌉ ) ,\nwe have that the expected duality gap satisfies\nE[OA(α)− (−OB(w(α)))] ≤ G\nat the averaged iterate α := 1T−T0 ∑T−1 t=T0+1 α(t) . (45)\nProof. We begin by estimating the expected change of feasibility for OA. We can bound this above by using Lemma 7 and the fact that the OB(·) is always a lower bound for −OA(·), and then applying (42) to find:\nE[OA(α(t+1))−OA(α?)] ≤ (1− γ(1−Θ)s) (OA(α(t))−OA(α?)) + γ(1−Θ)σ ′s2\n2τ 4L 2σ . (46)\nUsing (46) recursively we have\nE[OA(α(t))−OA(α?)] ≤ (1− γ(1−Θ)s)t (OA(α(0))−OA(α?)) + s 4L2σσ′\n2τ . (47)\nChoosing s = 1 and t = t0 := max{0, d 1γ(1−Θ) log(2(OA(α (0))−OA(α?))/(4L2σσ′))e} leads to\nE[OA(α(t))−OA(α?)] ≤ (1− γ(1−Θ))t0 (OA(α(0))−OA(α?)) + 4L2σσ′\n2τ ≤ 4L\n2σσ′ τ .\n(48)\nNext, we show inductively that\n∀t ≥ t0 : E[OA(α(t))−OA(α?)] ≤ 4L2σσ′\nτ(1 + 12γ(1−Θ)(t− t0)) . (49)\nClearly, (48) implies that (49) holds for t = t0. Assuming that it holds for any t ≥ t0, we show that it must also hold for t+ 1. Indeed, using\ns = 1\n1 + 12γ(1−Θ)(t− t0) ∈ [0, 1] , (50)\nwe obtain\nE[OA(α(t+1))−OA(α?)] ≤ 4L2σσ′\nτ\n( 1 + 12γ(1−Θ)(t− t0)− 1 2γ(1−Θ)\n(1 + 12γ(1−Θ)(t− t0))2 ) ︸ ︷︷ ︸\nD\nby applying the bounds (46) and (49), plugging in the definition of s (50), and simplifying. We upper bound the term D using the fact that geometric mean is less or equal to arithmetic mean:\nD = 1 1 + 12γ(1−Θ)(t+ 1− t0) (1 + 12γ(1−Θ)(t+ 1− t0))(1 + 1 2γ(1−Θ)(t− 1− t0))\n(1 + 12γ(1−Θ)(t− t0))2︸ ︷︷ ︸ ≤1\n≤ 1 1 + 12γ(1−Θ)(t+ 1− t0) .\nIf α is defined as (45), we apply the results of Lemma 7 and Lemma 8 to obtain\nE[G(α)] = E G T−1∑ t=T0 1 T−T0α (t)  ≤ 1T−T0E T−1∑ t=T0 G ( α(t) ) ≤ 1 γ(1−Θ)s 1 T − T0 E [ OA(α(T0))−OA(α?) ] + 4L 2σσ′s 2τ . (51)\nIf T ≥ d 1γ(1−Θ)e+ T0 such that T0 ≥ t0 we have\nE[G(α)] (51),(49) ≤ 1 γ(1−Θ)s 1 T − T0\n( 4L2σσ′\nτ(1 + 12γ(1−Θ)(T0 − t0))\n) + 4L2σσ′s\n2τ\n= 4L2σσ′\nτ\n( 1\nγ(1−Θ)s 1 T − T0 1 1 + 12γ(1−Θ)(T0 − t0) + s 2\n) . (52)\nChoosing\ns = 1\n(T − T0)γ(1−Θ) ∈ [0, 1] (53)\ngives us\nE[G(α)] (52),(53) ≤ 4L 2σσ′\nτ\n( 1\n1 + 12γ(1−Θ)(T0 − t0) +\n1 (T − T0)γ(1−Θ) 1 2\n) . (54)\nTo have right hand side of (54) smaller then G it is sufficient to choose T0 and T such that\n4L2σσ′\nτ\n( 1\n1 + 12γ(1−Θ)(T0 − t0)\n) ≤ 1\n2 G , (55)\n4L2σσ′\nτ\n( 1\n(T − T0)γ(1−Θ) 1 2\n) ≤ 1\n2 G . (56)\nHence if T0 ≥ t0 + 2γ(1−Θ) ( 8L2σσ′ τ G − 1 ) and T ≥ T0 + 4L 2σσ′\nτ Gγ(1−Θ) then (55) and (56) are satisfied.\nThe following main theorem simplifies the results of Theorem 9 and is a generalization of Ma et al. (2015b, Corollary 9) for general f∗(·) functions: Theorem’ 2. Consider Algorithm 1 with γ := 1, using a local solver of quality Θ (see Assumption 1). Let g∗i (·) be L-Lipschitz continuous, and assume that the columns of A satisfy ‖xi‖ ≤ 1, ∀i ∈ [n]. Let G > 0 be the desired duality gap (and hence an upper-bound on primal sub-optimality). Then after T iterations, where\nT ≥ T0 + max{ ⌈ 1\n1−Θ\n⌉ ,\n4L2n2\nτ G(1−Θ) } , (57)\nT0 ≥ t0 + [ 2\n1−Θ\n( 8L2n2 τ G − 1 )] + ,\nt0 ≥ max(0, ⌈ 1 (1−Θ) log ( τ(OA(α(0))−OA(α?)) 2L2Kn )⌉ ) ,\nwe have that the expected duality gap satisfies\nE[OA(α)− (−OB(w(α)))] ≤ G ,\nwhere α is the averaged iterate returned by Algorithm 1.\nProof. Plug in parameters γ := 1, σ′ := γK = K to the results of Theorem 9, and note that for balanced datasets we have σ ≤ n2K (see Remark 5). We can further simplify the rate by noting that τ = 1 for the 1-smooth losses (least squares and logistic) given as examples in this work.\nD.3 Proof of Convergence Result for Strongly Convex gi\nOur second main theorem follows reasoning in Shalev-Shwartz and Zhang (2013a) and is a generalization of Ma et al. (2015b, Corollary 11). We first introduce a lemma to simplify the proof.\nLemma 10. Assume that gi(0) ∈ [0, 1] for all i ∈ [n], then for the zero vector α(0) := 0 ∈ Rn, we have\nOA(α(0))−OA(α?) = OA(0)−OA(α?) ≤ n . (58)\nProof. For α := 0 ∈ Rn, we have w(α) = Aα = 0 ∈ Rd. Therefore, since the dual −OA(·) is always a lower bound on the primal OB(·), and by definition of the objective OA given in (A),\n0 ≤ OA(α)−OA(α?) ≤ OA(α)− (−OB(w(α))) (A) ≤ n .\nTheorem 11. Assume that gi are µ-strongly convex ∀i ∈ [n]. We define σmax = maxk∈[K] σk. Then after T iterations of Algorithm 1, with\nT ≥ 1γ(1−Θ) µτ+σmaxσ′\nµτ log n OA ,\nit holds that E[OA(α(T ))−OA(α?)] ≤ OA .\nFurthermore, after T iterations with\nT ≥ 1γ(1−Θ) µτ+σmaxσ′ µτ log ( 1 γ(1−Θ) µτ+σmaxσ′ µτ n G ) ,\nwe have the expected duality gap\nE[OA(α(T ))− (−OB(w(α(T ))))] ≤ G .\nProof. Given that gi(.) is µ-strongly convex with respect to the ‖ · ‖ norm, we can apply (35) and the definition of σk to find:\nR(t) ≤ − τµ(1−s)σ′s ‖u (t) −α(t)‖2 + ∑K k=1 σk‖u(t) −α (t) [k]‖ 2\n≤ ( − τµ(1−s)σ′s + σmax ) ‖u(t) −α(t)‖2 , (59)\nwhere σmax = maxk∈[K] σk. If we plug the following value of s\ns = τµ\nτµ+ σmaxσ′ ∈ [0, 1] (60)\ninto (59) we obtain that ∀t : R(t) ≤ 0. Putting the same s into (34) will give us\nE[OA(α(t))−OA(α(t+1))] (34),(60) ≥ γ(1−Θ) τµ\nτµ+ σmaxσ′ G(α(t))\n≥ γ(1−Θ) τµ τµ+ σmaxσ′ (OA(α(t))−OA(α?)) . (61)\nUsing the fact that E[OA(α(t))−OA(α(t+1))] = E[OA(α?)−OA(α(t+1))]+OA(α(t))−OA(α?) we have\nE[OA(α?)−OA(α(t+1))] +OA(α(t))−OA(α?) (61) ≥ γ(1−Θ) τµ\nτµ+ σmaxσ′ (OA(α(t))−OA(α?)) ,\nwhich is equivalent to E[OA(α(t+1))−OA(α?)] ≤ (\n1− γ(1−Θ) τµ τµ+ σmaxσ′\n) (OA(α(t))−OA(α?)) . (62)\nTherefore if we denote (t)OA = OA(α (t))−OA(α?) we have recursively that\nE[ (t)OA ] (62) ≤ ( 1− γ(1−Θ) τµ τµ+ σmaxσ′ )t (0) OA (58) ≤ ( 1− γ(1−Θ) τµ τµ+ σmaxσ′ )t n\n≤ exp ( −tγ(1−Θ) τµ\nτµ+ σmaxσ′\n) n .\nThe right hand side will be smaller than some OA if\nt ≥ 1 γ(1−Θ)\nτµ+ σmaxσ ′\nτµ log\nn OA .\nMoreover, to bound the duality gap, we have\nγ(1−Θ) τµ τµ+ σmaxσ′ G(α(t)) (61) ≤ E[OA(α(t))−OA(α(t+1))] ≤ E[OA(α(t))−OA(α?)] .\nThus, G(α(t)) ≤ 1γ(1−Θ) τµ+σmaxσ′ τµ (t) OA . Hence if OA ≤ γ(1−Θ) τµ τµ+σmaxσ′\nG then G(α(t)) ≤ G. Therefore after\nt ≥ 1 γ(1−Θ)\nτµ+ σmaxσ ′\nτµ log\n( 1\nγ(1−Θ) τµ+ σmaxσ\n′\nτµ\nn\nG ) iterations we have obtained a duality gap less than G.\nTheorem’ 3. Consider Algorithm 1 with γ := 1, using a local solver of quality Θ (see Assumption 1). Let gi(·) be µ-strongly convex, ∀i ∈ [n], and assume that the columns of A satisfy ‖xi‖ ≤ 1 ∀i ∈ [n]. Then we have that T iterations are sufficient for suboptimality OA, with\nT ≥ 1γ(1−Θ) τµ+n τµ log n OA .\nFurthermore, after T iterations with\nT ≥ 1γ(1−Θ) τµ+n τµ log\n( 1\nγ(1−Θ) τµ+n τµ n G\n) ,\nwe have the expected duality gap\nE[OA(α(T ))− (−OB(w(α(T ))))] ≤ G .\nProof. Plug in parameters γ := 1, σ′ := γK = K to the results of Theorem 11 and note that for balanced datasets we have σmax ≤ nK (see Remark 5). We can further simplify the rate by noting that τ = 1 for the 1-smooth losses (least squares and logistic) given as examples in this work."
    } ],
    "references" : [ {
      "title" : "Scalable training of L1-regularized log-linear models",
      "author" : [ "G. Andrew", "J. Gao" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Andrew and Gao.,? \\Q2007\\E",
      "shortCiteRegEx" : "Andrew and Gao.",
      "year" : 2007
    }, {
      "title" : "Communication complexity of distributed convex learning and optimization",
      "author" : [ "Y. Arjevani", "O. Shamir" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Arjevani and Shamir.,? \\Q2015\\E",
      "shortCiteRegEx" : "Arjevani and Shamir.",
      "year" : 2015
    }, {
      "title" : "Distributed learning, communication complexity and privacy",
      "author" : [ "M.-F. Balcan", "A. Blum", "S. Fine", "Y. Mansour" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2012
    }, {
      "title" : "Convex Analysis and Monotone Operator Theory in Hilbert Spaces",
      "author" : [ "H.H. Bauschke", "P.L. Combettes" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Bauschke and Combettes.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bauschke and Combettes.",
      "year" : 2011
    }, {
      "title" : "Parallel and Distributed Computation: Numerical Methods",
      "author" : [ "D.P. Bersekas", "J.N. Tsitsiklis" ],
      "venue" : null,
      "citeRegEx" : "Bersekas and Tsitsiklis.,? \\Q1989\\E",
      "shortCiteRegEx" : "Bersekas and Tsitsiklis.",
      "year" : 1989
    }, {
      "title" : "Parallel coordinate descent Newton method for efficient `1-regularized minimization",
      "author" : [ "Y. Bian", "X. Li", "Y. Liu", "M.-H. Yang" ],
      "venue" : "arXiv.org,",
      "citeRegEx" : "Bian et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bian et al\\.",
      "year" : 2013
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2004
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Boyd et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Boyd et al\\.",
      "year" : 2010
    }, {
      "title" : "Parallel coordinate descent for l1-regularized loss minimization",
      "author" : [ "J.K. Bradley", "A. Kyrola", "D. Bickson", "C. Guestrin" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Bradley et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bradley et al\\.",
      "year" : 2011
    }, {
      "title" : "Estimation, optimization, and parallelism when data is sparse",
      "author" : [ "J. Duchi", "M.I. Jordan", "B. McMahan" ],
      "venue" : "Neural Information Processing Systems,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2013
    }, {
      "title" : "Primal-dual rates and certificates",
      "author" : [ "C. Dünner", "S. Forte", "M. Takáč", "M. Jaggi" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Dünner et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dünner et al\\.",
      "year" : 2016
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Fan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2008
    }, {
      "title" : "Accelerated, parallel, and proximal coordinate descent",
      "author" : [ "O. Fercoq", "P. Richtárik" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Fercoq and Richtárik.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fercoq and Richtárik.",
      "year" : 2015
    }, {
      "title" : "Distributed Optimization for Non-Strongly Convex Regularizers",
      "author" : [ "S. Forte" ],
      "venue" : "Master’s thesis, ETH Zürich,",
      "citeRegEx" : "Forte.,? \\Q2015\\E",
      "shortCiteRegEx" : "Forte.",
      "year" : 2015
    }, {
      "title" : "Regularization paths for generalized linear models via coordinate descent",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Journal of Statistical Software,",
      "citeRegEx" : "Friedman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2010
    }, {
      "title" : "DUAL-LOCO: Distributing statistical estimation using random projections",
      "author" : [ "C. Heinze", "B. McWilliams", "N. Meinshausen" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Heinze et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Heinze et al\\.",
      "year" : 2016
    }, {
      "title" : "Fundamentals of convex analysis",
      "author" : [ "J.-B. Hiriart-Urruty", "C. Lemaréchal" ],
      "venue" : "Springer–Verlag, Berlin,",
      "citeRegEx" : "Hiriart.Urruty and Lemaréchal.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hiriart.Urruty and Lemaréchal.",
      "year" : 2001
    }, {
      "title" : "Communication-efficient distributed dual coordinate ascent",
      "author" : [ "M. Jaggi", "V. Smith", "M. Takáč", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Jaggi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jaggi et al\\.",
      "year" : 2014
    }, {
      "title" : "Blitz: A principled meta-algorithm for scaling sparse optimization",
      "author" : [ "T. Johnson", "C. Guestrin" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Johnson and Guestrin.,? \\Q2015\\E",
      "shortCiteRegEx" : "Johnson and Guestrin.",
      "year" : 2015
    }, {
      "title" : "Linear convergence of gradient and proximal-gradient methods under the Polyak-łojasiewicz condition",
      "author" : [ "H. Karimi", "J. Nutini", "M. Schmidt" ],
      "venue" : "In European Conference on Machine Learning,",
      "citeRegEx" : "Karimi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Karimi et al\\.",
      "year" : 2016
    }, {
      "title" : "Distributed box-constrained quadratic optimization for dual linear SVM",
      "author" : [ "C.-P. Lee", "D. Roth" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Lee and Roth.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lee and Roth.",
      "year" : 2015
    }, {
      "title" : "On the complexity analysis of randomized block-coordinate descent methods",
      "author" : [ "Z. Lu", "L. Xiao" ],
      "venue" : null,
      "citeRegEx" : "Lu and Xiao.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lu and Xiao.",
      "year" : 2013
    }, {
      "title" : "Distributed optimization with arbitrary local solvers. arXiv.org, 2015a",
      "author" : [ "C. Ma", "J. Konečný", "M. Jaggi", "V. Smith", "M. Jordan", "P. Richtárik", "M. Takáč" ],
      "venue" : null,
      "citeRegEx" : "Ma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2015
    }, {
      "title" : "Adding vs. averaging in distributed primal-dual optimization",
      "author" : [ "C. Ma", "V. Smith", "M. Jaggi", "M.I. Jordan", "P. Richtárik", "M. Takáč" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Ma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2015
    }, {
      "title" : "Linear convergence of the randomized feasible descent method under the weak strong convexity assumption. arXiv.org, 2015c",
      "author" : [ "C. Ma", "R. Tappenden", "M. Takáč" ],
      "venue" : null,
      "citeRegEx" : "Ma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2015
    }, {
      "title" : "A distributed block coordinate descent method for training l1 regularized linear classifiers",
      "author" : [ "D. Mahajan", "S.S. Keerthi", "S. Sundararajan" ],
      "venue" : "arXiv.org,",
      "citeRegEx" : "Mahajan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mahajan et al\\.",
      "year" : 2014
    }, {
      "title" : "Distributed Block Coordinate Descent for Minimizing Partially Separable Functions, volume",
      "author" : [ "J. Marecek", "P. Richtárik", "M. Takáč" ],
      "venue" : "Proceedings in Mathematics & Statistics. Springer International Publishing,",
      "citeRegEx" : "Marecek et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Marecek et al\\.",
      "year" : 2015
    }, {
      "title" : "LOCO: Distributing ridge regression with random projections",
      "author" : [ "B. McWilliams", "C. Heinze", "N. Meinshausen", "G. Krummenacher", "H.P. Vanchinathan" ],
      "venue" : null,
      "citeRegEx" : "McWilliams et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "McWilliams et al\\.",
      "year" : 2014
    }, {
      "title" : "MLlib: Machine learning in apache spark",
      "author" : [ "X. Meng", "J. Bradley", "B. Yavuz", "E. Sparks", "S. Venkataraman", "D. Liu", "J. Freeman", "D. Tsai", "M. Amde", "S. Owen", "D. Xin", "R. Xin", "M.J. Franklin", "R. Zadeh", "M. Zaharia", "A. Talwalkar" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Meng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2016
    }, {
      "title" : "Linear convergence of first order methods under weak nondegeneracy assumptions for convex programming",
      "author" : [ "I. Necoara" ],
      "venue" : null,
      "citeRegEx" : "Necoara.,? \\Q2015\\E",
      "shortCiteRegEx" : "Necoara.",
      "year" : 2015
    }, {
      "title" : "Distributed dual gradient methods and error bound conditions",
      "author" : [ "I. Necoara", "V. Nedelcu" ],
      "venue" : "arXiv.org,",
      "citeRegEx" : "Necoara and Nedelcu.,? \\Q2014\\E",
      "shortCiteRegEx" : "Necoara and Nedelcu.",
      "year" : 2014
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Nesterov.,? \\Q2005\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2005
    }, {
      "title" : "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "F. Niu", "B. Recht", "C. Ré", "S.J. Wright" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Niu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2011
    }, {
      "title" : "Solving large scale linear SVM with distributed block minimization",
      "author" : [ "D. Pechyony", "L. Shen", "R. Jones" ],
      "venue" : "In International Conference on Information and Knowledge Management,",
      "citeRegEx" : "Pechyony et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pechyony et al\\.",
      "year" : 2011
    }, {
      "title" : "Quartz: Randomized dual coordinate ascent with arbitrary sampling",
      "author" : [ "Z. Qu", "P. Richtárik", "T. Zhang" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Qu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2015
    }, {
      "title" : "SDNA: Stochastic dual Newton ascent for empirical risk minimization",
      "author" : [ "Z. Qu", "P. Richtárik", "M. Takáč", "O. Fercoq" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Qu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2016
    }, {
      "title" : "Distributed coordinate descent method for learning with big data",
      "author" : [ "P. Richtárik", "M. Takáč" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Richtárik and Takáč.,? \\Q2016\\E",
      "shortCiteRegEx" : "Richtárik and Takáč.",
      "year" : 2016
    }, {
      "title" : "Convex Analysis",
      "author" : [ "R.T. Rockafellar" ],
      "venue" : null,
      "citeRegEx" : "Rockafellar.,? \\Q1997\\E",
      "shortCiteRegEx" : "Rockafellar.",
      "year" : 1997
    }, {
      "title" : "Stochastic methods for l1-regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Shalev.Shwartz and Tewari.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Tewari.",
      "year" : 2011
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Shalev.Shwartz and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang.",
      "year" : 2013
    }, {
      "title" : "Accelerated mini-batch stochastic dual coordinate ascent",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Shalev.Shwartz and Zhang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang.",
      "year" : 2013
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "Mathematical Programming, Series",
      "citeRegEx" : "Shalev.Shwartz and Zhang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang.",
      "year" : 2014
    }, {
      "title" : "Communication-efficient distributed optimization using an approximate newton-type method",
      "author" : [ "O. Shamir", "N. Srebro", "T. Zhang" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Shamir et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shamir et al\\.",
      "year" : 2014
    }, {
      "title" : "L1-Regularized Distributed Optimization: A Communication-Efficient Primal-Dual Framework",
      "author" : [ "V. Smith", "S. Forte", "M.I. Jordan", "M. Jaggi" ],
      "venue" : null,
      "citeRegEx" : "Smith et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2015
    }, {
      "title" : "Mini-batch primal and dual methods for SVMs",
      "author" : [ "M. Takáč", "A. Bijral", "P. Richtárik", "N. Srebro" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Takáč et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Takáč et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed mini-batch SDCA",
      "author" : [ "M. Takáč", "P. Richtárik", "N. Srebro" ],
      "venue" : "arXiv.org,",
      "citeRegEx" : "Takáč et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Takáč et al\\.",
      "year" : 2015
    }, {
      "title" : "On the complexity of parallel coordinate descent",
      "author" : [ "R. Tappenden", "M. Takáč", "P. Richtárik" ],
      "venue" : null,
      "citeRegEx" : "Tappenden et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tappenden et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed coordinate descent for l1-regularized logistic regression",
      "author" : [ "I. Trofimov", "A. Genkin" ],
      "venue" : "arXiv.org,",
      "citeRegEx" : "Trofimov and Genkin.,? \\Q2014\\E",
      "shortCiteRegEx" : "Trofimov and Genkin.",
      "year" : 2014
    }, {
      "title" : "Iteration complexity of feasible descent methods for convex optimization",
      "author" : [ "P.-W. Wang", "C.-J. Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Wang and Lin.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang and Lin.",
      "year" : 2014
    }, {
      "title" : "Coordinate descent algorithms",
      "author" : [ "S.J. Wright" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Wright.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wright.",
      "year" : 2015
    }, {
      "title" : "Trading computation for communication: Distributed stochastic dual coordinate ascent",
      "author" : [ "T. Yang" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Yang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yang.",
      "year" : 2013
    }, {
      "title" : "On theoretical analysis of distributed stochastic dual coordinate ascent",
      "author" : [ "T. Yang", "S. Zhu", "R. Jin", "Y. Lin" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2013
    }, {
      "title" : "A dual augmented block minimization framework for learning with limited memory",
      "author" : [ "I.E.-H. Yen", "S.-W. Lin", "S.-D. Lin" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Yen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yen et al\\.",
      "year" : 2015
    }, {
      "title" : "Large linear classification when data cannot fit in memory",
      "author" : [ "H.-F. Yu", "C.-J. Hsieh", "K.-W. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Knowledge Discovery from Data,",
      "citeRegEx" : "Yu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2012
    }, {
      "title" : "A quasi-Newton approach to nonsmooth convex optimization problems in machine learning",
      "author" : [ "J. Yu", "S. Vishwanathan", "S. Günter", "N.N. Schraudolph" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Yu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2010
    }, {
      "title" : "A comparison of optimization methods and software for large-scale l1-regularized linear classification",
      "author" : [ "G.-X. Yuan", "K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Yuan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2010
    }, {
      "title" : "An improved GLMNET for L1-regularized logistic regression",
      "author" : [ "G.-X. Yuan", "C.-H. Ho", "C.-J. Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Yuan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient distributed linear classification algorithms via the alternating direction method of multipliers",
      "author" : [ "C. Zhang", "H. Lee", "K.G. Shin" ],
      "venue" : "In Artificial Intelligence and Statistics Conference,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2012
    }, {
      "title" : "Stochastic primal-dual coordinate method for regularized empirical risk minimization",
      "author" : [ "Y. Zhang", "X. Lin" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Zhang and Lin.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang and Lin.",
      "year" : 2015
    }, {
      "title" : "Communication-efficient algorithms for statistical optimization",
      "author" : [ "Y. Zhang", "J.C. Duchi", "M.J. Wainwright" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    }, {
      "title" : "Parallelized stochastic gradient descent",
      "author" : [ "M.A. Zinkevich", "M. Weimer", "A.J. Smola", "L. Li" ],
      "venue" : "Neural Information Processing Systems,",
      "citeRegEx" : "Zinkevich et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zinkevich et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "Although numerous distributed optimization methods have been proposed, the minibatch optimization approach has emerged as one of the most popular paradigms for tackling this communication-computation tradeoff (see, e.g., Takáč et al., 2013; Shalev-Shwartz and Zhang, 2013b; Qu et al., 2015; Richtárik and Takáč, 2016).",
      "startOffset" : 209,
      "endOffset" : 317
    }, {
      "referenceID" : 36,
      "context" : "Although numerous distributed optimization methods have been proposed, the minibatch optimization approach has emerged as one of the most popular paradigms for tackling this communication-computation tradeoff (see, e.g., Takáč et al., 2013; Shalev-Shwartz and Zhang, 2013b; Qu et al., 2015; Richtárik and Takáč, 2016).",
      "startOffset" : 209,
      "endOffset" : 317
    }, {
      "referenceID" : 17,
      "context" : "CoCoA-v1 (Jaggi et al., 2014) and CoCoA+ (Ma et al.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : "Portions of this newer work appear in SF’s Master’s Thesis (Forte, 2015) and Smith et al.",
      "startOffset" : 59,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "Portions of this newer work appear in SF’s Master’s Thesis (Forte, 2015) and Smith et al. (2015).",
      "startOffset" : 60,
      "endOffset" : 97
    }, {
      "referenceID" : 42,
      "context" : "Notably, in contrast to earlier work of Yang (2013); Jaggi et al.",
      "startOffset" : 40,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al.",
      "startOffset" : 53,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al. (2015b), our generalized, cohesive framework: (1) specifically incorporates difficult cases of L1 regularization and other non-strongly convex regularizers; (2) allows for the flexibility of distributing the data by either feature or training point; and (3) can be run on either a primal or dual formulation, which we show to have significant theoretical and practical implications.",
      "startOffset" : 53,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al. (2015b), our generalized, cohesive framework: (1) specifically incorporates difficult cases of L1 regularization and other non-strongly convex regularizers; (2) allows for the flexibility of distributing the data by either feature or training point; and (3) can be run on either a primal or dual formulation, which we show to have significant theoretical and practical implications. Flexible communication and local solvers. Two key advantages of the proposed framework are its communication efficiency and ability to employ off-the-shelf single-machine solvers internally. On real-world systems, the cost of communication versus computation can vary widely, and it is thus advantageous to permit a flexible amount of communication depending on the setting at hand. Our framework provides exactly such control. Moreover, we allow arbitrary solvers to be used on each machine, which permits the reuse of existing code and the benefits from multi-core or other optimizations therein. Primal-dual rates. We derive convergence rates for our framework, leveraging a novel approach in the analysis of primal-dual rates for non-strongly convex regularizers. The proposed technique is a significant improvement over simple smoothing techniques used in, e.g., Nesterov (2005); Shalev-Shwartz and Zhang (2014); and Zhang and Lin (2015) that enforce strong convexity by adding a small L2 term to the objective.",
      "startOffset" : 53,
      "endOffset" : 1374
    }, {
      "referenceID" : 17,
      "context" : "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al. (2015b), our generalized, cohesive framework: (1) specifically incorporates difficult cases of L1 regularization and other non-strongly convex regularizers; (2) allows for the flexibility of distributing the data by either feature or training point; and (3) can be run on either a primal or dual formulation, which we show to have significant theoretical and practical implications. Flexible communication and local solvers. Two key advantages of the proposed framework are its communication efficiency and ability to employ off-the-shelf single-machine solvers internally. On real-world systems, the cost of communication versus computation can vary widely, and it is thus advantageous to permit a flexible amount of communication depending on the setting at hand. Our framework provides exactly such control. Moreover, we allow arbitrary solvers to be used on each machine, which permits the reuse of existing code and the benefits from multi-core or other optimizations therein. Primal-dual rates. We derive convergence rates for our framework, leveraging a novel approach in the analysis of primal-dual rates for non-strongly convex regularizers. The proposed technique is a significant improvement over simple smoothing techniques used in, e.g., Nesterov (2005); Shalev-Shwartz and Zhang (2014); and Zhang and Lin (2015) that enforce strong convexity by adding a small L2 term to the objective.",
      "startOffset" : 53,
      "endOffset" : 1407
    }, {
      "referenceID" : 17,
      "context" : "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al. (2015b), our generalized, cohesive framework: (1) specifically incorporates difficult cases of L1 regularization and other non-strongly convex regularizers; (2) allows for the flexibility of distributing the data by either feature or training point; and (3) can be run on either a primal or dual formulation, which we show to have significant theoretical and practical implications. Flexible communication and local solvers. Two key advantages of the proposed framework are its communication efficiency and ability to employ off-the-shelf single-machine solvers internally. On real-world systems, the cost of communication versus computation can vary widely, and it is thus advantageous to permit a flexible amount of communication depending on the setting at hand. Our framework provides exactly such control. Moreover, we allow arbitrary solvers to be used on each machine, which permits the reuse of existing code and the benefits from multi-core or other optimizations therein. Primal-dual rates. We derive convergence rates for our framework, leveraging a novel approach in the analysis of primal-dual rates for non-strongly convex regularizers. The proposed technique is a significant improvement over simple smoothing techniques used in, e.g., Nesterov (2005); Shalev-Shwartz and Zhang (2014); and Zhang and Lin (2015) that enforce strong convexity by adding a small L2 term to the objective.",
      "startOffset" : 53,
      "endOffset" : 1433
    }, {
      "referenceID" : 46,
      "context" : "It is also useful as an analysis tool, helping us to present a cohesive framework and relate this work to the prior work of Yang (2013); Jaggi et al.",
      "startOffset" : 124,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : "It is also useful as an analysis tool, helping us to present a cohesive framework and relate this work to the prior work of Yang (2013); Jaggi et al. (2014); and Ma et al.",
      "startOffset" : 137,
      "endOffset" : 157
    }, {
      "referenceID" : 46,
      "context" : "This setting was not covered in earlier work of Yang (2013); Jaggi et al.",
      "startOffset" : 48,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "This setting was not covered in earlier work of Yang (2013); Jaggi et al. (2014); Ma et al.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "This setting was not covered in earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015b); and Ma et al.",
      "startOffset" : 61,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "This setting was not covered in earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015b); and Ma et al. (2015a), and we discuss it in detail in Section 4, as additional machinery must be introduced to develop primal-dual rates for this setting.",
      "startOffset" : 61,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "From a coordinate-wise perspective, two approaches for updating the parameter vector α in an iterative fashion include the Jacobi method, in which updates made to coordinates of α do not take into account the most recent updates to the other coordinates, and Gauss-Seidel, in which the most recent information is used (Bersekas and Tsitsiklis, 1989).",
      "startOffset" : 318,
      "endOffset" : 349
    }, {
      "referenceID" : 44,
      "context" : "As the size of the mini-batch grows, this can slow them down in terms of overall runtime, and can even lead to divergence in practice (Takáč et al., 2013; Takáč et al., 2015; Richtárik and Takáč, 2016; Marecek et al., 2015).",
      "startOffset" : 134,
      "endOffset" : 223
    }, {
      "referenceID" : 45,
      "context" : "As the size of the mini-batch grows, this can slow them down in terms of overall runtime, and can even lead to divergence in practice (Takáč et al., 2013; Takáč et al., 2015; Richtárik and Takáč, 2016; Marecek et al., 2015).",
      "startOffset" : 134,
      "endOffset" : 223
    }, {
      "referenceID" : 36,
      "context" : "As the size of the mini-batch grows, this can slow them down in terms of overall runtime, and can even lead to divergence in practice (Takáč et al., 2013; Takáč et al., 2015; Richtárik and Takáč, 2016; Marecek et al., 2015).",
      "startOffset" : 134,
      "endOffset" : 223
    }, {
      "referenceID" : 26,
      "context" : "As the size of the mini-batch grows, this can slow them down in terms of overall runtime, and can even lead to divergence in practice (Takáč et al., 2013; Takáč et al., 2015; Richtárik and Takáč, 2016; Marecek et al., 2015).",
      "startOffset" : 134,
      "endOffset" : 223
    }, {
      "referenceID" : 7,
      "context" : "6 Comparison to ADMM Finally, in this subsection we provide a direct comparison between CoCoA and ADMM (Boyd et al., 2010).",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 41,
      "context" : "To address this problem, existing approaches typically use a simple smoothing technique (as in Nesterov, 2005; Shalev-Shwartz and Zhang, 2014): by adding a small amount of L2 to the objective gi, the functions gi become strongly convex.",
      "startOffset" : 88,
      "endOffset" : 142
    }, {
      "referenceID" : 10,
      "context" : "See also Dünner et al. (2016) for a follow-up discussion of this technique in the non-distributed case.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 17,
      "context" : "The earlier work of CoCoA-v1 (Jaggi et al., 2014) did not provide rates for L-Lipschitz `i losses.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "5 Recovering Earlier Work as a Special Case As a special case, the proposed framework and rates directly apply to L2-regularized lossminimization problems, including those presented in the earlier work of Jaggi et al. (2014) and Ma et al.",
      "startOffset" : 205,
      "endOffset" : 225
    }, {
      "referenceID" : 17,
      "context" : "5 Recovering Earlier Work as a Special Case As a special case, the proposed framework and rates directly apply to L2-regularized lossminimization problems, including those presented in the earlier work of Jaggi et al. (2014) and Ma et al. (2015b). Remark 3.",
      "startOffset" : 205,
      "endOffset" : 247
    }, {
      "referenceID" : 9,
      "context" : ", Dünner et al. (2016) or Boyd and Vandenberghe (2004, Example 3.",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "For the L1 examples discussed, existing fast L1-solvers for the single-machine case, such as glmnet variants (Friedman et al., 2010) or blitz (Johnson and Guestrin, 2015) can be directly applied to each local subproblem Gσ k ( · ;v,α[k]) within Algorithm 1.",
      "startOffset" : 109,
      "endOffset" : 132
    }, {
      "referenceID" : 18,
      "context" : ", 2010) or blitz (Johnson and Guestrin, 2015) can be directly applied to each local subproblem Gσ k ( · ;v,α[k]) within Algorithm 1.",
      "startOffset" : 17,
      "endOffset" : 45
    }, {
      "referenceID" : 20,
      "context" : "We discuss some specific examples of local solvers below, and point the reader to Ma et al. (2015a) for an empirical exploration of these choices.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 49,
      "context" : "This algorithm and its variants are increasingly used in practice (Wright, 2015), and extensions such as accelerated and parallel versions can directly be applied (Shalev-Shwartz and Zhang, 2014; Fan et al.",
      "startOffset" : 66,
      "endOffset" : 80
    }, {
      "referenceID" : 41,
      "context" : "This algorithm and its variants are increasingly used in practice (Wright, 2015), and extensions such as accelerated and parallel versions can directly be applied (Shalev-Shwartz and Zhang, 2014; Fan et al., 2008) in our framework.",
      "startOffset" : 163,
      "endOffset" : 213
    }, {
      "referenceID" : 11,
      "context" : "This algorithm and its variants are increasingly used in practice (Wright, 2015), and extensions such as accelerated and parallel versions can directly be applied (Shalev-Shwartz and Zhang, 2014; Fan et al., 2008) in our framework.",
      "startOffset" : 163,
      "endOffset" : 213
    }, {
      "referenceID" : 30,
      "context" : ", by using error bound conditions (Necoara and Nedelcu, 2014; Wang and Lin, 2014), weak strong convexity conditions (Ma et al.",
      "startOffset" : 34,
      "endOffset" : 81
    }, {
      "referenceID" : 48,
      "context" : ", by using error bound conditions (Necoara and Nedelcu, 2014; Wang and Lin, 2014), weak strong convexity conditions (Ma et al.",
      "startOffset" : 34,
      "endOffset" : 81
    }, {
      "referenceID" : 29,
      "context" : ", by using error bound conditions (Necoara and Nedelcu, 2014; Wang and Lin, 2014), weak strong convexity conditions (Ma et al., 2015c; Necoara, 2015) or by considering Polyak-Łojasiewicz conditions (Karimi et al.",
      "startOffset" : 116,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : ", 2015c; Necoara, 2015) or by considering Polyak-Łojasiewicz conditions (Karimi et al., 2016).",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 19,
      "context" : "example, for randomized coordinate descent (as part of glmnet), Lu and Xiao (2013, Theorem 1) gives a O(1/t) approximation quality for any separable regularizer, including L1 and elastic net; see also Tappenden et al. (2015) and Shalev-Shwartz and Tewari (2011).",
      "startOffset" : 64,
      "endOffset" : 225
    }, {
      "referenceID" : 19,
      "context" : "example, for randomized coordinate descent (as part of glmnet), Lu and Xiao (2013, Theorem 1) gives a O(1/t) approximation quality for any separable regularizer, including L1 and elastic net; see also Tappenden et al. (2015) and Shalev-Shwartz and Tewari (2011). In the dual setting (Algorithm 3) for the discussed examples, the losses are applied only to local variables α[k], and the regularizer is approximated via a quadratic term.",
      "startOffset" : 64,
      "endOffset" : 262
    }, {
      "referenceID" : 11,
      "context" : "This algorithm and its variants are increasingly used in practice (Wright, 2015), and extensions such as accelerated and parallel versions can directly be applied (Shalev-Shwartz and Zhang, 2014; Fan et al., 2008) in our framework. For non-smooth losses such as SVMs, the analysis of Shalev-Shwartz and Zhang (2013a) provides a O(1/t) rate, and for smooth losses, a faster linear rate.",
      "startOffset" : 196,
      "endOffset" : 317
    }, {
      "referenceID" : 28,
      "context" : "0) (Meng et al., 2016).",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "A comparison with Shotgun (Bradley et al., 2011), a popular method for solving L1-regularized problems in the multicore environment, is provided as an extreme case to highlight the detrimental effects of frequent communication in the distributed environment.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 31,
      "context" : "To get around this requirement, previous work has suggested implementing the Nesterov smoothing technique used in, e.g., Shalev-Shwartz and Zhang (2014); Zhang and Lin (2015) — adding a small amount of strong convexity δ‖α‖2 to the objective for lasso regression.",
      "startOffset" : 77,
      "endOffset" : 153
    }, {
      "referenceID" : 31,
      "context" : "To get around this requirement, previous work has suggested implementing the Nesterov smoothing technique used in, e.g., Shalev-Shwartz and Zhang (2014); Zhang and Lin (2015) — adding a small amount of strong convexity δ‖α‖2 to the objective for lasso regression.",
      "startOffset" : 77,
      "endOffset" : 175
    }, {
      "referenceID" : 7,
      "context" : "Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2010) is a popular method that lends itself naturally to the distributed environment.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 28,
      "context" : "0 (Meng et al., 2016).",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2010) is a popular method that lends itself naturally to the distributed environment. For lasso regression, implementing ADMM for the problems of interest requires solving a large linear system Cx = d on each machine, where C ∈ Rn×n with n scaling beyond 107 for the datasets in Table 5, and with C being possibly dense. It is prohibitively slow to solve this directly on each machine, and we therefore employ the iterative method of conjugate gradient with early stopping (see, e.g., Boyd et al., 2010, Section 4.3). For SVM classification, we use stochastic dual coordinate ascent as an internal optimizer, which is shown in Zhang et al. (2012) to have superior performance.",
      "startOffset" : 52,
      "endOffset" : 712
    }, {
      "referenceID" : 38,
      "context" : "Mini-batch CD (for lasso) and SDCA (for SVM) aim to improve mini-batch SGD by employing coordinate descent, which has theoretical and practical justifications (Shalev-Shwartz and Tewari, 2011; Takáč et al., 2015; Fercoq and Richtárik, 2015; Tappenden et al., 2015; Takáč et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 284
    }, {
      "referenceID" : 45,
      "context" : "Mini-batch CD (for lasso) and SDCA (for SVM) aim to improve mini-batch SGD by employing coordinate descent, which has theoretical and practical justifications (Shalev-Shwartz and Tewari, 2011; Takáč et al., 2015; Fercoq and Richtárik, 2015; Tappenden et al., 2015; Takáč et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 284
    }, {
      "referenceID" : 12,
      "context" : "Mini-batch CD (for lasso) and SDCA (for SVM) aim to improve mini-batch SGD by employing coordinate descent, which has theoretical and practical justifications (Shalev-Shwartz and Tewari, 2011; Takáč et al., 2015; Fercoq and Richtárik, 2015; Tappenden et al., 2015; Takáč et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 284
    }, {
      "referenceID" : 46,
      "context" : "Mini-batch CD (for lasso) and SDCA (for SVM) aim to improve mini-batch SGD by employing coordinate descent, which has theoretical and practical justifications (Shalev-Shwartz and Tewari, 2011; Takáč et al., 2015; Fercoq and Richtárik, 2015; Tappenden et al., 2015; Takáč et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 284
    }, {
      "referenceID" : 44,
      "context" : "Mini-batch CD (for lasso) and SDCA (for SVM) aim to improve mini-batch SGD by employing coordinate descent, which has theoretical and practical justifications (Shalev-Shwartz and Tewari, 2011; Takáč et al., 2015; Fercoq and Richtárik, 2015; Tappenden et al., 2015; Takáč et al., 2013).",
      "startOffset" : 159,
      "endOffset" : 284
    }, {
      "referenceID" : 8,
      "context" : "For the case of lasso regression, we implement Shotgun (Bradley et al., 2011), which is a popular method for parallel optimization.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 54,
      "context" : "OWN-QN (Yu et al., 2010) is a quasi-Newton method optimized in Spark’s spark.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 28,
      "context" : "ml package (Meng et al., 2016).",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 14,
      "context" : ", by using existing fast L1-solvers for the singlemachine case, such as glmnet variants (Friedman et al., 2010) or blitz (Johnson and Guestrin, 2015) or SVM solvers like liblinear (Fan et al.",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 18,
      "context" : ", 2010) or blitz (Johnson and Guestrin, 2015) or SVM solvers like liblinear (Fan et al.",
      "startOffset" : 17,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : ", 2010) or blitz (Johnson and Guestrin, 2015) or SVM solvers like liblinear (Fan et al., 2008).",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "of-the-art, as in glmnet (Friedman et al., 2010) and extensions (Yuan et al.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 56,
      "context" : ", 2010) and extensions (Yuan et al., 2012); see, e.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 41,
      "context" : "However, primal-dual convergence rates for unmodified coordinate algorithms have to our knowledge only been obtained for strongly convex regularizers to date (Shalev-Shwartz and Zhang, 2014; Zhang and Lin, 2015).",
      "startOffset" : 158,
      "endOffset" : 211
    }, {
      "referenceID" : 58,
      "context" : "However, primal-dual convergence rates for unmodified coordinate algorithms have to our knowledge only been obtained for strongly convex regularizers to date (Shalev-Shwartz and Zhang, 2014; Zhang and Lin, 2015).",
      "startOffset" : 158,
      "endOffset" : 211
    }, {
      "referenceID" : 14,
      "context" : "In the single-coordinate update case, this is at the core of glmnet (Friedman et al., 2010; Yuan et al., 2010), and widely used in, e.",
      "startOffset" : 68,
      "endOffset" : 110
    }, {
      "referenceID" : 55,
      "context" : "In the single-coordinate update case, this is at the core of glmnet (Friedman et al., 2010; Yuan et al., 2010), and widely used in, e.",
      "startOffset" : 68,
      "endOffset" : 110
    }, {
      "referenceID" : 38,
      "context" : ", solvers based on the primal formulation of L1-regularized objectives (Shalev-Shwartz and Tewari, 2011; Yuan et al., 2012; Bian et al., 2013; Fercoq and Richtárik, 2015; Tappenden et al., 2015).",
      "startOffset" : 71,
      "endOffset" : 194
    }, {
      "referenceID" : 56,
      "context" : ", solvers based on the primal formulation of L1-regularized objectives (Shalev-Shwartz and Tewari, 2011; Yuan et al., 2012; Bian et al., 2013; Fercoq and Richtárik, 2015; Tappenden et al., 2015).",
      "startOffset" : 71,
      "endOffset" : 194
    }, {
      "referenceID" : 5,
      "context" : ", solvers based on the primal formulation of L1-regularized objectives (Shalev-Shwartz and Tewari, 2011; Yuan et al., 2012; Bian et al., 2013; Fercoq and Richtárik, 2015; Tappenden et al., 2015).",
      "startOffset" : 71,
      "endOffset" : 194
    }, {
      "referenceID" : 12,
      "context" : ", solvers based on the primal formulation of L1-regularized objectives (Shalev-Shwartz and Tewari, 2011; Yuan et al., 2012; Bian et al., 2013; Fercoq and Richtárik, 2015; Tappenden et al., 2015).",
      "startOffset" : 71,
      "endOffset" : 194
    }, {
      "referenceID" : 46,
      "context" : ", solvers based on the primal formulation of L1-regularized objectives (Shalev-Shwartz and Tewari, 2011; Yuan et al., 2012; Bian et al., 2013; Fercoq and Richtárik, 2015; Tappenden et al., 2015).",
      "startOffset" : 71,
      "endOffset" : 194
    }, {
      "referenceID" : 12,
      "context" : "of-the-art, as in glmnet (Friedman et al., 2010) and extensions (Yuan et al., 2012); see, e.g., the overview in Yuan et al. (2010). However, primal-dual convergence rates for unmodified coordinate algorithms have to our knowledge only been obtained for strongly convex regularizers to date (Shalev-Shwartz and Zhang, 2014; Zhang and Lin, 2015).",
      "startOffset" : 26,
      "endOffset" : 131
    }, {
      "referenceID" : 32,
      "context" : "Several variants of SGD have been proposed for parallel computing, many of which build on the idea of asynchronous communication (Niu et al., 2011; Duchi et al., 2013).",
      "startOffset" : 129,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "Several variants of SGD have been proposed for parallel computing, many of which build on the idea of asynchronous communication (Niu et al., 2011; Duchi et al., 2013).",
      "startOffset" : 129,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "For the specific case of L1-regularized objectives, parallel coordinate descent (with and without using mini-batches) was proposed in Bradley et al. (2011) (Shotgun) and generalized in Bian et al.",
      "startOffset" : 134,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "(2011) (Shotgun) and generalized in Bian et al. (2013), and is among the best performing solvers in the parallel setting.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 55,
      "context" : "(2009); Zinkevich et al. (2010); Zhang et al.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 54,
      "context" : "(2010); Zhang et al. (2013); McWilliams et al.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 26,
      "context" : "(2013); McWilliams et al. (2014); and Heinze et al.",
      "startOffset" : 8,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "(2014); and Heinze et al. (2016). These methods require additional assumptions on the partitioning of the data, which are usually not satisfied in practice if the data are distributed “as is”, i.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "(2014); and Heinze et al. (2016). These methods require additional assumptions on the partitioning of the data, which are usually not satisfied in practice if the data are distributed “as is”, i.e., if we do not have the opportunity to distribute the data in a specific way beforehand. Furthermore, some cannot guarantee convergence rates beyond what could be achieved if we ignored data residing on all but a single computer, as shown in Shamir et al. (2014). Additional relevant lower bounds on",
      "startOffset" : 12,
      "endOffset" : 460
    }, {
      "referenceID" : 34,
      "context" : "However, mini-batch versions of both SGD and coordinate descent (CD) (e.g., Takáč et al., 2013; Shalev-Shwartz and Zhang, 2013b; Qu et al., 2015; Richtárik and Takáč, 2016) suffer from their convergence rate degrading towards the rate of batch gradient descent as the size of the mini-batch is increased.",
      "startOffset" : 69,
      "endOffset" : 172
    }, {
      "referenceID" : 36,
      "context" : "However, mini-batch versions of both SGD and coordinate descent (CD) (e.g., Takáč et al., 2013; Shalev-Shwartz and Zhang, 2013b; Qu et al., 2015; Richtárik and Takáč, 2016) suffer from their convergence rate degrading towards the rate of batch gradient descent as the size of the mini-batch is increased.",
      "startOffset" : 69,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : "ADMM (Boyd et al., 2010), gradient descent, and quasi-Newton methods such as L-BFGS and are also often used in distributed environments because of their relatively low communication requirements.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007).",
      "startOffset" : 179,
      "endOffset" : 201
    }, {
      "referenceID" : 50,
      "context" : "The practical variant of the DisDCA (Yang, 2013), called DisDCA-p, allows for additive updates in a similar manner to CoCoA, but is restricted to coordinate decent (CD) being the local solver, and was initially proposed without convergence guarantees.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "the minimum number of communication rounds necessary for a given approximation quality are presented in Balcan et al. (2012) and Arjevani and Shamir (2015).",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "(2012) and Arjevani and Shamir (2015). Mini-batch methods.",
      "startOffset" : 11,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness Θ for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al.",
      "startOffset" : 180,
      "endOffset" : 1113
    }, {
      "referenceID" : 0,
      "context" : "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness Θ for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al. (2011); Yang (2013); Yang et al.",
      "startOffset" : 180,
      "endOffset" : 1137
    }, {
      "referenceID" : 0,
      "context" : "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness Θ for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al. (2011); Yang (2013); Yang et al.",
      "startOffset" : 180,
      "endOffset" : 1150
    }, {
      "referenceID" : 0,
      "context" : "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness Θ for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al. (2011); Yang (2013); Yang et al. (2013) and Lee and Roth (2015), the CoCoA-v1 and CoCoA+ frameworks (which are special cases of the presented framework, CoCoA) are the first to allow the use of any local solver—of weak local approximation quality—in each round in the distributed setting.",
      "startOffset" : 180,
      "endOffset" : 1170
    }, {
      "referenceID" : 0,
      "context" : "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness Θ for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al. (2011); Yang (2013); Yang et al. (2013) and Lee and Roth (2015), the CoCoA-v1 and CoCoA+ frameworks (which are special cases of the presented framework, CoCoA) are the first to allow the use of any local solver—of weak local approximation quality—in each round in the distributed setting.",
      "startOffset" : 180,
      "endOffset" : 1194
    }, {
      "referenceID" : 0,
      "context" : "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness Θ for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al. (2011); Yang (2013); Yang et al. (2013) and Lee and Roth (2015), the CoCoA-v1 and CoCoA+ frameworks (which are special cases of the presented framework, CoCoA) are the first to allow the use of any local solver—of weak local approximation quality—in each round in the distributed setting. The practical variant of the DisDCA (Yang, 2013), called DisDCA-p, allows for additive updates in a similar manner to CoCoA, but is restricted to coordinate decent (CD) being the local solver, and was initially proposed without convergence guarantees. DisDCA-p, CoCoA-v1, and CoCoA+ are all limited to strongly convex regularizers, and therefore are not as general as the CoCoA framework discussed in this work. In the L1-regularized setting, an approach related to our framework includes distributed variants of glmnet as in Mahajan et al. (2014). Inspired by glmnet and Yuan et al.",
      "startOffset" : 180,
      "endOffset" : 1967
    }, {
      "referenceID" : 25,
      "context" : "If hypothetically each of our quadratic subproblems Gσ k (∆α[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016).",
      "startOffset" : 290,
      "endOffset" : 348
    }, {
      "referenceID" : 56,
      "context" : "If hypothetically each of our quadratic subproblems Gσ k (∆α[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016).",
      "startOffset" : 290,
      "endOffset" : 348
    }, {
      "referenceID" : 35,
      "context" : "If hypothetically each of our quadratic subproblems Gσ k (∆α[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016).",
      "startOffset" : 290,
      "endOffset" : 348
    }, {
      "referenceID" : 5,
      "context" : "(2012), the works of Bian et al. (2013) and Mahajan et al.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context.",
      "startOffset" : 21,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression.",
      "startOffset" : 21,
      "endOffset" : 210
    }, {
      "referenceID" : 5,
      "context" : "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems Gσ k (∆α[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality Θ as in our framework, the works of Trofimov and Genkin (2014); Yuan et al.",
      "startOffset" : 21,
      "endOffset" : 645
    }, {
      "referenceID" : 5,
      "context" : "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems Gσ k (∆α[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality Θ as in our framework, the works of Trofimov and Genkin (2014); Yuan et al.",
      "startOffset" : 21,
      "endOffset" : 795
    }, {
      "referenceID" : 5,
      "context" : "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems Gσ k (∆α[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality Θ as in our framework, the works of Trofimov and Genkin (2014); Yuan et al. (2012); and Yen et al.",
      "startOffset" : 21,
      "endOffset" : 815
    }, {
      "referenceID" : 5,
      "context" : "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems Gσ k (∆α[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality Θ as in our framework, the works of Trofimov and Genkin (2014); Yuan et al. (2012); and Yen et al. (2015) assume that the quadratic subproblems are solved exactly.",
      "startOffset" : 21,
      "endOffset" : 838
    }, {
      "referenceID" : 5,
      "context" : "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems Gσ k (∆α[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality Θ as in our framework, the works of Trofimov and Genkin (2014); Yuan et al. (2012); and Yen et al. (2015) assume that the quadratic subproblems are solved exactly. Therefore, these methods are not able to freely trade off communication and computation. Also, they do not allow the re-use of arbitrary local solvers. On the theoretical side, the convergence rate results provided by Mahajan et al. (2014); Trofimov and Genkin (2014); and Yuan et al.",
      "startOffset" : 21,
      "endOffset" : 1136
    }, {
      "referenceID" : 5,
      "context" : "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems Gσ k (∆α[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality Θ as in our framework, the works of Trofimov and Genkin (2014); Yuan et al. (2012); and Yen et al. (2015) assume that the quadratic subproblems are solved exactly. Therefore, these methods are not able to freely trade off communication and computation. Also, they do not allow the re-use of arbitrary local solvers. On the theoretical side, the convergence rate results provided by Mahajan et al. (2014); Trofimov and Genkin (2014); and Yuan et al.",
      "startOffset" : 21,
      "endOffset" : 1164
    }, {
      "referenceID" : 5,
      "context" : "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems Gσ k (∆α[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality Θ as in our framework, the works of Trofimov and Genkin (2014); Yuan et al. (2012); and Yen et al. (2015) assume that the quadratic subproblems are solved exactly. Therefore, these methods are not able to freely trade off communication and computation. Also, they do not allow the re-use of arbitrary local solvers. On the theoretical side, the convergence rate results provided by Mahajan et al. (2014); Trofimov and Genkin (2014); and Yuan et al. (2012) are not explicit convergence rates but only asymptotic, as the quadratic upper bounds are not explicitly controlled for safety as with our σ′.",
      "startOffset" : 21,
      "endOffset" : 1188
    }, {
      "referenceID" : 50,
      "context" : "6, following the line of reasoning in Yang (2013). For consensus ADMM, the objective (B) is decomposed using the following re-parameterization:",
      "startOffset" : 38,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : "The arguments follow the reasoning in Ma et al. (2015b,a), but where we have generalized them to be applicable directly to (A). We provide full details of Lemma 1 as a proof of concept, but omit details in later proofs that can be derived using the arguments in Ma et al. (2015b) or earlier work of Shalev-Shwartz and Zhang (2013a), and instead outline the proof strategy and highlight sections where the theory deviates.",
      "startOffset" : 38,
      "endOffset" : 280
    }, {
      "referenceID" : 22,
      "context" : "The arguments follow the reasoning in Ma et al. (2015b,a), but where we have generalized them to be applicable directly to (A). We provide full details of Lemma 1 as a proof of concept, but omit details in later proofs that can be derived using the arguments in Ma et al. (2015b) or earlier work of Shalev-Shwartz and Zhang (2013a), and instead outline the proof strategy and highlight sections where the theory deviates.",
      "startOffset" : 38,
      "endOffset" : 332
    }, {
      "referenceID" : 36,
      "context" : "3 Proof of Convergence Result for Strongly Convex gi Our second main theorem follows reasoning in Shalev-Shwartz and Zhang (2013a) and is a generalization of Ma et al.",
      "startOffset" : 98,
      "endOffset" : 131
    } ],
    "year" : 2016,
    "abstractText" : "The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for the distributed environment, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}