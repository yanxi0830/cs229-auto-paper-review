{
  "name" : "1609.03426.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Large-Scale Label Prediction for Sparse Data with Probable Guarantees",
    "authors" : [ "Sayantan Dasgupta" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1. Introduction\nMulti-label prediction is one of the most difficult problems in Large-Scale Machine Learning. Unlike the multiclass classification where a text is assigned only one label from a set of labels, here a text can have a variable number of labels. It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4]. A basic approach to the problem is to use 1-vs-all classification technique by training a single binary classifier for every label. However, if there are L labels and the data has D dimension, then these 1-vs-all models require O(DL) parameters. Also, it requires O(DL) steps to predict labels for a new data point. Most of the applications for this task use data with moderate to high dimension, such as text or image datasets, and 1-vs-all models for label prediction is feasible as long as L D. However, as the number of labels increases to a point when L ∼ D, it is no longer possible to use 1-vs-all classifier, since the number of parameters required increases to O(D2), and the model can no longer be stored in the memory.\nRecently there has been attempts to reduce the complexity of such models, by using a low rank mapping Φ : RD → RL in between the data and the labels. If the rank of such mappings is limited to K D, then the model requires Θ ((L+D)K) parameters. Both WSABIE [1] and LEML [5] utilizes such mappings. WSABIE\ndefines weighted approximate pair-wise rank (WARP) loss on such mappings and optimizes the loss on the training dataset. LEML uses similar mapping but generalizes the loss function to squared-loss, sigmoid loss or hinge loss, which are typical to the cases of Linear Regression, Logistic Regression, and Linear SVM respectively.\nBoth of WSABIE and LEML uses low-rank discriminative models, where the low-rank mapping usually has the form Z = HW>, where W ∈ RD×K and H ∈ RL×K . Here we propose a generative solution for the same problem using latent variable based probabilistic modeling. Unlike the usual cases where such latent variable models are trained using EM, we use Method of Moments [6] to extract the parameters from the latent variable model. We show that our method can be globally convergent when the sample size is larger than a specific lower bound, and establish theoretical bounds for the extracted parameters. We also show the competitive performance of our method regarding classification measures as well as computation time.\n2. Latent Variable Model for Method of Moments\nWe use a generative model as shown in Figure 1. The underlying generative process of the model is described as follows.\n2.1. Generative Model\nLet us assume that there are N documents, the vocabulary size is D, and total number of labels are L. For any document d ∈ {d1, d2 . . . dN} we first choose a latent state of h ∈ {1, 2 . . .K} from the discrete distribution P [ h ] , then we choose an word v ∈ {v1, v2 . . . vD} from the discrete distribution P [ v|h ] , and a label l ∈ {l1, l2 . . . lL} from the\ndiscrete distribution P [ l|h ] . The generative process is as follows,\nh ∼ Discrete(P [ h ] )\nv ∼ Discrete(P [ v|h ] )\nl ∼ Discrete(P [ l|h ] ) (1)\nLet us denote the probability of the latent variable h assuming the state k ∈ 1 . . .K as,\nπk = P [ h = k ] (2)\nar X\niv :1\n60 9.\n03 42\n6v 1\n[ cs\n.L G\n] 1\n2 Se\np 20\n16\nLet us define µk ∈ RD as the probability vector of all the words conditional to the latent state k ∈ 1 . . .K, i.e.\nµk = P [ v|h = k ] (3)\nand γk ∈ RL as the probability vector of all the labels conditional to the latent state k ∈ 1 . . .K, i.e.\nγk = P [ l|h = k ] (4)\nLet the matrix O ∈ RD×K denote the conditional probabilities for the words, i.e. Oi,k = P [ vi|h = k ] . Then O = [µ1|µ2| . . . |µK ]. Similarly, let Q ∈ RL×K denote the conditional prob-\nabilities for the words, i.e. Qj,k = P [ lj |h = k ] . Then, Q = [γ1|γ2| . . . |γK ]. We assume that the matrix O and Q are of full rank, and their columns are fully identifiable. The aim of our algorithm is to estimate the matrices O, Q and the vector π.\nFollowing the generative model in equation 1, we can define the probability of individual word as,\nP [vj ] = K∑ k=1 P [vj |h]P [h = k] = K∑ k=1 πk[µk]j\n∀j = 1, 2, . . . D\nwhere [µk]j is the jth element of the vector µk, for j ∈ [D].\nTherefore, the average probability of the words across the data can be defined as,\nM1 = P [v1, v2, . . . vD] > = K∑ k=1 πk [[µk]1, [µk]1 . . . [µk]D] >\n= K∑ k=1 πkµk (5)\nFrom [6], if we define M2 as the pairwise probability matrix, with [M2]i,j = P [ vi, vj ] , we can express it as,\nM2 = K∑ k=1 πkµkµk > = K∑ k=1 πkµk ⊗ µk (6)\nSimilarly, the tensor M3 defined as the third order probability moment, with [M3]i,j,τ = P [vi, vj , vτ ] ∀i, j, τ ∈ {1, 2 . . . D}, can be represented as,\nM3 = K∑ k=1 πkµk ⊗ µk ⊗ µk (7)\nFurther, if we define the cross moment between the labels and the words as M2L, with [M2L]τ,i,j = P [lτ , vi, vj ], where i, j ∈ {1, 2 . . . D} and τ ∈ {1, 2 . . . L}, then\nM2L = K∑ k=1 πkγk ⊗ µk ⊗ µk (8)\n2.2. Parameter Extraction\nIn this section, we revisit the method to extract the matrices O and Q as well as the latent state probabilities π. The first step is to whiten the matrix M2, where we try to find a matrix low rank W such that W>M2W = I . This is a method similar to the whitening in ICA, with the covariance matrix being replaced with the co-occurrence probability matrix in our case.\nThe whitening is usually done through eigenvalue decomposition of M2. If the K maximum eigenvalues of M2 are {νk}Kk=1, and the corresponding eigenvectors are {ωk}Kk=1, then the whitening matrix of rank K is computed as W = ΩΣ−1/2, where Ω = [ ω1|ω2| . . . |ωK ] , and Σ = diag(ν1, ν2, . . . , νK). Upon whitening M2 takes the form\nW>M2W = W >( K∑\nk=1\nπkµkµ > k\n) W\n= K∑ k=1 (√ πkW >µk )(√ πkW >µk )> =\nK∑ k=1 µ̃kµ̃ > k = I (9)\nHence µ̃k = √ πkW\n>µk are orthonormal vectors. Multiplying M3 along all three dimensions by W , we get\nM̃3 = M3(W,W,W )\n= K∑ k=1 πk(W >µk)⊗ (W>µk)⊗ (W>µk)\n= K∑ k=1 1 √ πk µ̃k ⊗ µ̃k ⊗ µ̃k (10)\nUpon canonical decomposition of M̃3, if the eigenvalues and eigenvectors are {λk}Kk=1 and {uk}Kk=1 respectively, then λk = 1/√πk. i.e., πk = λ−2k , and,\nuk = µ̃k = √ πkW >µk = 1\nλk W>µk (11)\nThe µks can be recovered as µk = λkW †uk, where W † is the pseudo-inverse of W>, i.e., W † = W ( W>W )−1 .\nThe matrix O can be constructed as O = [ µ1|µ2| . . . |µK ] . Since we normalize the columns of O as Ovk = Ovk∑ v Ovk\n. it is sufficient to compute µk = W †uk, since λk will be cancelled during normalization.\nIt is possible to compute the γk for k = 1 . . .K through the factorization of second and third order moments of the labels. However, it is not possible to match the topics between µ1:K and γ1:K . Therefore, we use the cross moment M2L between the words and the labels. If we multiply the tensor M2L twice by W , we get\nM̃2L = M2L(W,W )\n= K∑ k=1 πkγk ⊗ (W>µk)⊗ (W>µk)\n= K∑ k=1 γk ⊗ ( √ πkW >µk)⊗ ( √ πkW >µk)\n= K∑ k=1 γk ⊗ µ̃k ⊗ µ̃k (12)\nIf the kth eigenvalue of M̃3 is uk, then\nu>kM2L(W,W )uk\n= µ̃>kM2L(W,W )µ̃k\n= µ̃>k ( K∑ k=1 γk ⊗ µ̃k ⊗ µ̃k ) µ̃k\n= γk\ni.e., γk can be retrieved as u>kM2L(W,W )uk, since µ̃k is orthonormal. Thus, we can make sure that µk and γk will correspond to the same topic k for k = 1, 2 . . .K.\nTherefore, Q = [ γ1|γ2| . . . |γK ] = [ u>1 M2L(W,W )u1|u>2 M2L(W,W )u2| . . .\n. . . |u>KM2L(W,W )uK ]\n= [ u>1 |u2|> . . . |u>K ] M2L(W,W ) [u1|u2| . . . |uK ] = U>M2L(W,W )U\n= M2L(WU,WU) (13)\nwhere, U = [u1|u2| . . . |uK ] are all the K eigenvectors of the tensor M̃3.\n2.3. Label Prediction\nOnce we have O and π, the probability of a document d given h can be expressed as,\nP [ d|h = k ] = ∏ v∈Wd P [ v|h = k ] (14)\nwhere Wd is the set of distinct words in the document d. Then the document personalization probabilities P [ h =\nk|d ] can be estimated using Bayes Rule.\nP [ h = k|d ] =\nP [ h = k ]∏ v∈Wd P [ v|h = k ]∑K k=1 P [ h = k ]∏ v∈Wd P [ v|h = k\n] = πk ∏ v∈Wd Ovk∑K\nk=1 πk ∏ v∈Wd Ovk\n(15)\nThen the probability of a label l for the document can be computed as,\nP [ l|d ] = K∑ k=1 P [ l|h = k ] P [ h = k ∣∣d] =\nK∑ k=1 QlkP [ h = k ∣∣d] (16) The labels are ranked by the probabilities P [ l|d ] , and the labels with highest ranks are assigned to the document. If the number of unique words in a test document is nd = |Wd|, then the prediction step has a complexity of Θ ((nd + L)K) to compute the probability for all L labels.\n3. Implementation Detail\nWe create an estimation of the sparse moments M2 by counting the pairwise occurrence of the items across the selections made by all the users in the dataset, and normalizing by the total number of occurrence in each case. For large datasets, this can be achieved in one pass through the dataset using frameworks like Hadoop. Alternately, if X ∈ RN×D is the sparse matrix representing the data, then the sum of all the pairwise word count is ∑N i=1 nnz(xi)\n2, where xi is the word vector of the ith document, and nnz(xi) is the number of distinct words or non-zero entries in that document.\nTherefore, M2 can be estimated as,\nM̂2 = 1∑N\ni=1 nnz(xi) 2 X>X (17)\nAlso, M3 can be estimated as\nM̂3 = 1∑N\ni=1 nnz(xi) 3 X ⊗X ⊗X (18)\nThe dimensions of M2 and M3 are D2 and D3 respectively, but in practice, these quantities are extremely sparse. M2 has a total number of elements O (∑N i=1 nnz(xi) 2 )\n, with the worst case occurring when no two documents has any word in common, and all the pairwise counts are 1. The whitening of M2 is carried out through extracting the K maximum eigenvalues and corresponding eigenvectors. This step is the bottleneck of the algorithm. We use the eigs function in Matlab for computing the eigenvalues, which uses Arnoldi’s iterations, and has a complexity\nAlgorithm 1 Method of Moments for Parameter Extraction\nInput: Sparse Data X ∈ RN×D, Label Y ∈ RN×L and K ∈ Z+ Output: P [ v|h ] , P [ l|h ]\nand π 1) Estimate\nM̂2 = 1∑N\ni=1 nnz(xi) 2 X>X (pass #1)\n2) Compute maximum eigenvalues K of M̂2 as {νk}Kk=1, and corresponding eigenvectors as {ωk}Kk=1. Define Ω =[ ω1|ω2| . . . |ωK ] , and Σ = diag (ν1, ν2, . . . , νK) 3) Estimate the whitening matrix Ŵ = ΩΣ−1/2 so that Ŵ>M̂2Ŵ = IK×K 4) Estimate\nˆ̃M3 = 1∑N\ni=1 nnz(xi) 3 XŴ ⊗XŴ ⊗XŴ (pass #2)\n5) Compute eigenvalues {λk}Kk=1 and eigenvectors {uk}Kk=1 of ˆ̃M3. Assign Û = [u1|u2 . . . |uK ]. 6) Estimate the columns of O as µ̂k = Ŵ †uk and π̂k = λ−2k , ∀k ∈ 1, 2 . . .K 7) Assign Ô = [ˆ̄µ1| ˆ̄µ2| . . . | ˆ̄µK ] & π̂ = [π̂1, π̂2 . . . π̂K ]> 8) Estimate\nQ̂ = 1∑N\ni=1 nnz(xi) 2nnz(yi)\nY ⊗XŴÛ ⊗XŴÛ (pass #3)\n9) Estimate P [ v|h = k ] = Ôvk∑\nv Ôvk ,∀k ∈ 1 . . .K, v ∈ v1 . . . vD P [ l|h = k ] = Q̂lk∑\nl Q̂lk ,∀k ∈ 1 . . .K, l ∈ l1 . . . lL\nO ( ( ∑N\ni=1 nnz(xi) 2)K\n) , since the total number of non-\nzero entries in M2 is O (∑N i=1 nnz(xi) 2 )\n[7]. As for M3, we do not need to explicitly compute it. Since M̃3 = M3(W,W,W ), we can estimate M̃3 right away as,\nˆ̃M3 = 1∑N\ni=1 nnz(xi) 3 XW ⊗XW ⊗XW (19)\nComputing ˆ̃M3 takes a second pass through the entire dataset, and has a complexity of O(NK3).\nSimilarly, if Y ∈ RN×L represents the labels for N documents, M2L can be estimated as,\nM̂2L = 1∑N\ni=1 nnz(xi) 2nnz(yi)\nY ⊗X ⊗X (20)\nwhere yi is the label vector of ith document, and nnz(yi) is the number of distinct labels in that document. We do not need to compute M2L either. Once we obtain the eigenvectors U of M̃3, since Q = M2L(WU,WU) from Equation 13, we can estimate Q right away as,\nQ̂ = 1∑N\ni=1 nnz(xi) 2nnz(yi)\nY ⊗XWU ⊗XWU (21)\nThis step has a complexity of O(K2 ∑D\ni=1 nnz(yi)). The entire algorithm is outlined as Algorithm 1. The overall complexity is O ( ( ∑N\ni=1 nnz(xi) 2)K +K2 ∑D i=1 nnz(yi) +NK 3 )\n. We used the Tensor Toolbox [8] for tensor decomposition.\nOnce the matrix O and π are extracted, it requires one more pass through the entire dataset to compute P [l|h], resulting in a total of three passes to extract all parameters. The label prediction step has a complexity of Θ ((nd + L)K) for a document with distinct number of words nd.\nTheorem 1. Let us assume that we draw N i.i.d samples x1, x2 . . . xN with labels y1, y2 . . . yN using the generative process in Equation 1 with bounded support such that\n||x|| ≤ 1. Let us define ε1 = ( 1 + √ log(1/δ) 2 ) , and\nε2 =\n( 1 + √ log(2/δ)\n2\n) for some δ ∈ (0, 1). Then, if the\nnumber of samples N ≥ max(n1, n2, n3), where\n• n1 = c2\n( logK + log log ( K c1 · √ πmax πmin )) • n2 = Ω (( ε1\nd̃2sσK(M2) )2) • n3 = Ω ( K2 (\n10 d̃2sσK(M2)5/2\n+ 2 √\n2 d̃3sσK(M2)3/2\n)2 ε21 ) for some constants c1 and c2, and we run Algorithm 1 on these N samples, then the following bounds on the estimated parameters hold with probability at least 1− δ,\n||µk − µ̂k|| ≤ ( 160 √ σ1(M2)\nd̃2sσK(M2)5/2 +\n32 √\n2σ1(M2) d̃3sσK(M2)3/2\n+ 4 √ σ1(M2)\nd̃2sσK(M2)\n) ε1√ N ,\n||γk − γ̂k|| ≤ 1024σK(M2) ( 5 σK(M2)5/2 + √ 2 σK(M2)3/2 )2 ε21 (d̃3s)2N\n+ 16σK(M2)3 ε21 (d̃2s)2N + 4σK(M2) ε2 d̃ls √ N ,\nand |πk − π̂k| ≤ (\n200 σK(M2)5/2\n+ 40 √\n2 σK(M2)3/2\n) ε1\nd̃3s √ N , where σ1(M2) . . . σK(M2) are the K largest eigenvalues of the pairwise probability matrix M2, d̃2s = ∑N i=1 nnz(xi)\n2, d̃3s = ∑N i=1 nnz(xi) 3 and d̃ls = ∑N i=1 nnz(xi) 2nnz(yi) The proof is included in the appendix.\n4. Experimental Results\nWe used six datasets for our methods, as described in table 1. The datasets range from small datasets like Bibtex with 4880 training instances with 159 labels to large datasets like WikiLSHTC with around 1.7M training instances with 325K labels. Since LEML is shown to outperform WSABIE and other benchmark algorithms on various small and large-scale datasets in [5], we benchmark the performance of our method against LEML. Also both LEML and MoM has similar model complexity due to similar number (Θ ((L+D)K)) of parameters for the same latent dimensionality K. For LEML, we ran ten iterations for the smaller datasets (Bibtex and Delicious) and five iterations for the larger datasets, since the authors of LEML chose a similar number of iterations for their experiments in [5]. We measured AUC (of Receiver Operating Characteristics (ROC)) against K. AUC is a versatile measure, and is used to evaluate the performance of classification as well as prediction algorithms [9]. Also, it is shown that there exists a one-to-one relation between AUC and PrecisionRecall curve in [10], i.e., a classifier with higher AUC will also achieve better Precision and Recall. We carried out our experiments on Unix Platform on a single machine with Intel\ni5 Processor (2.4GHz) and 16GB memory, and no multithreading or any other performance enhancement method is used in the code. For AmazonCat and WikiLSHTC datasets, we ran LEML on an i2.4xlarge instance of Amazon EC2 with 122 GB of memory, since LEML needs significantly larger memory for these two datasets (Figure 2).\nWe computed AUC for every test documents and perform a macro-averaging across the documents, and repeat the experiments for K = {50, 75, 100, 125, 150} (Figure 2). Both LEML and Method of Moments perform very similarly, but the memory footprint (Figure 2) of MoM is significantly less than LEML. MoM takes longer to finish for the smaller datasets like Bibtex or Delicious since tensor factorization takes a lot more time compared to the LEML iterations on smaller datasets. However, for the larger datasets, each iteration of LEML becomes extremely costly, and MoM takes a fraction of the time taken by LEML. For WikiLSHTC dataset, LEML takes more than two days to finish, while MoM finished within a few hours. The runtime as well as speed-up is shown in Table 3 for K = 100. Due to the large discrepancy between the runtime of LEML and MoM for the larger datasets, we do not give a detailed plot of runtime vs. K.\n5. Conclusion\nHere we propose a method for multi-label prediction for large-scale datasets based on moment factorization. Our method (MoM) gives similar performance in comparison with state-of-art algorithms like LEML while taking a fraction of time and memory for the larger datasets. MoM takes\nonly three passes through the training dataset to extract all the parameters. Since MoM consists of only linear algebraic operations, it is embarrassingly parallel, and can easily be scaled up in any parallel eco-system using linear algebra libraries. In our implementation, we used Matlab’s linear algebra library based on LAPACK/ARPACK, although we did not incorporate any parallelization.\nBoth LEML and MoM have error bound of O(1/ √ N) on training performance w.r.t. the number of training samples N . However, when we compute the AUC on test dataset, the AUC of LEML decreases with latent dimensionality(K) for some datasets, including the larger dataset of AmazonCat containing more than 1M training instance. This shows the possibility of over-fitting in LEML. MoM, on the other hand, is not an optimization algorithm, and the parameters are extracted from Moment Factorization\nrather than optimizing any target function. It is not susceptible to over-fitting, which is evident from its performance. On the other hand, MoM has the requirement N ≥ Ω(K2) on the number of documents in the training set, and it will not work if N < Θ(K2). However, for smaller text corpora where N < Θ(K2) hold, 1-vs-all classifiers are usually sufficient to predict the labels. We need dimensionality reduction techniques for large text corpora where 1-vs-all classifiers fail, and MoM provides a very competitive choice for such cases.\nReferences\n[1] J. Weston, S. Bengio, and N. Usunier, “Wsabie: Scaling up to large vocabulary image annotation,” in IJCAI, vol. 11, 2011, pp. 2764– 2770.\n[2] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, “Large-scale video classification with convolutional neural networks,” in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2014, pp. 1725–1732.\n[3] T. N. Rubin, A. Chambers, P. Smyth, and M. Steyvers, “Statistical topic models for multi-label document classification,” Machine learning, vol. 88, no. 1-2, pp. 157–208, 2012.\n[4] R. Agrawal, A. Gupta, Y. Prabhu, and M. Varma, “Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages,” in Proceedings of the 22nd international conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2013, pp. 13–24.\n[5] H.-F. Yu, P. Jain, P. Kar, and I. S. Dhillon, “Large-scale multi-label learning with missing labels,” in ICML, vol. 31, 2014.\n[6] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky, “Tensor decompositions for learning latent variable models,” Journal of Machine Learning Research, vol. 15, pp. 2773–2832, 2014. [Online]. Available: http://jmlr.org/papers/v15/anandkumar14b.html\n[7] J. Lee, V. Balakrishnan, C.-K. Koh, and D. Jiao, “From o (k 2 n) to o (n): a fast complex-valued eigenvalue solver for large-scale onchip interconnect analysis,” in Microwave Symposium Digest, 2009. MTT’09. IEEE MTT-S International. IEEE, 2009, pp. 181–184.\n[8] B. W. Bader, T. G. Kolda et al., “Matlab tensor toolbox version 2.6,” Available online, February 2015. [Online]. Available: http://www.sandia.gov/∼tgkolda/TensorToolbox/\n[9] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme, “Bpr: Bayesian personalized ranking from implicit feedback,” in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2009, pp. 452–461.\n[10] J. Davis and M. Goadrich, “The relationship between precision-recall and roc curves,” in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 233–240.\n[11] Y. Wang and J. Zhu, “Spectral methods for supervised topic models,” in Advances in Neural Information Processing Systems, 2014, pp. 1511–1519.\n[12] T. G. Kolda and J. R. Mayo, “Shifted power method for computing tensor eigenpairs,” SIAM Journal on Matrix Analysis and Applications, vol. 32, no. 4, pp. 1095–1124, October 2011.\n[13] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain, “Sparse local embeddings for extreme multi-label classification,” in Advances in Neural Information Processing Systems, 2015, pp. 730–738.\n[14] Y. Prabhu and M. Varma, “Fastxml: A fast, accurate and stable treeclassifier for extreme multi-label learning,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 263–272.\n[15] A. T. Chaganty and P. Liang, “Spectral experts for estimating mixtures of linear regressions,” arXiv preprint arXiv:1306.3729, 2013.\nAppendix A. Vector Norms\nLet the true pairwise probability matrix and the third order probability moment be M2 and M3. Let us assume that we select N i.i.d. samples x1, . . . xN from the population, and the estimates of pairwise matrix and third order moment are M̂2 and M̂3. Let εM2 = ||M2−M̂2||2. We use the second order operator norm of the matrices here. Let us assume εM2 ≤ σK(M2)/2, where σK is the Kth largest eigenvalue of M2. We will derive the conditions which satisfies this later.\nIf Σ = diag(σ1, σ2 . . . σK) are the top-K eigenvalues of M2, and U are the corresponding eigenvectors, then the whitening matrix W = UΣ−1/2. Also, W>M2KW = IK×K , where M2K is the K rank approximation of M2. Then,\n||W ||2 = √ max eig(W>W ) = √ max eig(Σ−1)\n= 1√\nσK(M2)\nSimilarly, if W † = W (W>W )−1, then W † = WΣ = UΣ1/2. Therefore,\n||W †||2 = √ max eig(Σ) = √ σ1(M2) (22)\nLet Ŵ be the whitening matrix for M̂2, i.e., Ŵ>M̂2Ŵ = IK×K . Then by Weyl’s inequality, σk(M2)− σk(M̂2) ≤ ||M2 − M̂2||,∀k = 1, 2 . . .K.\nTherefore,\n||Ŵ ||22 = 1\nσK(M̂2)\n≤ 1 σK (M2)− ||M2 − M̂2|| ≤ 2 σK (M2)\n(23)\nAlso, by Weyl’s Theorem,\n||Ŵ †||22 = σ1(M̂2) ≤ σ1(M2) + εM2 ≤ 1.5σ1(M2) =⇒ ||Ŵ †||2 ≤ √ 1.5σ1(M2) ≤ 1.5 √ σ1(M2) (24)\nLet D be the eigenvectors of ŴM2Ŵ , and A be the corresponding eigenvalues. Then we can write, ŴM2Ŵ=ADA>. Then W = ŴAD−1/2A> whitens M2, i.e., W>M2W = I . Therefore,\n||I −D||2 = ||I −ADA>||2 = ||I − ŴM2Ŵ ||2 = ||ŴM̂2Ŵ − ŴM2Ŵ ||2 ≤ ||Ŵ ||22||M2 − M̂2||\n≤ 2 σK (M2) εM2 (25)\nεW = ||W − Ŵ ||2 = ||W −WAD1/2A>||2 = ||W ||2||I −AD1/2A>||2 = ||W ||2||I −D1/2||2 ≤ ||W ||2||I −D1/2||2||I +D1/2||2 = ||W ||2||I −D||2\n≤ 2 σK(M2)3/2 εM2\nεW † = ||W † − Ŵ †||2 = ||Ŵ †AD1/2A> − Ŵ †||2 = ||Ŵ †||2||I −AD1/2A>||2 ≤ ||Ŵ †||2||I −D||2\n≤ 2 √ σ1(M2)\nσK (M2) εM2 (26)\nAppendix B. Tensor Norm\nLet us define the second order operator norm of a tensor T ∈ RD×D×D as,\n||T ||2 = sup u {|T (u, u, u)| : u ∈ RD&||u|| = 1} (27)\nLemma 1. For a tensor T ∈ RD×D×D, ||T ||2 ≤ ||T ||F , where ||T ||F is the Frobenius norm defined as,\n||T ||F = √∑ i,j,k (Ti,j,k)2 (28)\nProof. For any real matrix A, ||A||2 ≤ ||A||F . Let us unfold the tensor T as the collection of D matrices, as, T = {T1, T2 . . . TD}. Then,\nT (u, u, u) = u>[T1u|T2u| . . . |TKu]u = 〈[u>T1u, u>T2u, . . . u>TKu], u〉 (29)\nTherefore,\n||T ||2 = sup u {|T (u, u, u)| : u ∈ RD&||u|| = 1} = sup u { ∣∣〈[u>T1u, u>T2u, . . . , u>TKu], u〉∣∣ : u ∈ RD\n&||u|| = 1} (30)\nUsing Cauchy-Schwarz inequality,\n||T ||2 ≤ sup u { ∣∣∣∣[u>T1u, u>T2u, . . . u>TKu]∣∣∣∣ ||u||\n: u ∈ RD&||u|| = 1} = sup u { ∣∣∣∣[u>T1u, u>T2u, . . . u>TKu]∣∣∣∣\n: u ∈ RD&||u|| = 1} = ∣∣∣∣[ ||T1||2 , ||T2|| , . . . ||TD|| ]∣∣∣∣\n≤ ∣∣∣∣[ ||T1||F , ||T2||F , . . . ||TD||F ]∣∣∣∣\n= √( ||T1||2F + ||T2|| 2 F + · · ·+ ||TD||F ) = ||T ||F (31)\nLet us define εM3 = ||M3−M̂3||2. Then from Appendix B in [15],\nεtw = ||M3(W,W,W )− M̂3(Ŵ , Ŵ , Ŵ )||2 ≤ ||M3||2 ( ||Ŵ ||22 + ||Ŵ ||2||W ||2 + ||W ||22 ) εW\n+ ||Ŵ ||3εM3\n≤ ||M3||2 (2 +\n√ 2 + 1)\nσK(M2) εW +\n2 √ 2\nσK(M2)3/2 εM3\n≤ ||M3||2 (3 +\n√ 2)\nσK(M2) · 2 σK(M2)3/2\nεM2 + 2 √ 2\nσK(M2)3/2 εM3\n≤ 10||M3||2 σK(M2)5/2\n· εM2 + 2 √ 2\nσK(M2)3/2 εM3 ≤ ( 10\nd̃2sσK(M2)5/2 +\n2 √ 2\nd̃3sσK(M2)3/2 ) 2ε√ N\n(32)\nPlease note that ||M3||2 ≤ ||M3||F ≤ 1, because M3 is a tensor with individual elements as probabilities.\nLemma 2. (Robust Power Method from [6]) If T̂ = T + E ∈ RK×K×K , where T is an symmetric tensor with orthogonal decomposition T = ∑K k=1 λkuk ⊗ uk ⊗ uk with each λk > 0, and E has operator norm ||E||2 ≤ . Let λmin = min K k=1{λk} and λmax = maxKk=1{λk}. Let there exist constants c1, c2 such that ≤ c1 · (λmin/K), and N ≥ c2(logK + log log (λmax/ )). Then if Algorithm 1 in [6] is called for K times, with L = poly(K) log(1/η) restarts each time for some η ∈ (0, 1), then with probability at least 1 − η, there exists a permutation Π on [K], such that,\n||uΠ(k) − ûk|| ≤ 8 λΠ(k) , |λk − λΠ(k)| ≤ 5 ∀k ∈ [K]\n(33)\nSince λk = 1 √ πk , ∀k ∈ [K] (34)\nTherefore, we need, N ≥ c2 ( logK + log log ( Kλmax c1λmin )) = c2 ( logK + log log ( K c1 · √ πmax πmin )) (35)\nThis contributes in the first lower bound (n1) of N in Theorem 1.\nAppendix C. Tail Inequality\nLemma 3. If we draw N i.i.d. documents x1, x2 . . . xN through the generative process in Equation 1, with the labels as y1, y2 . . . yN , and the vectors probability mass function of the words v estimated from these N samples are p̂(v) whereas the true p.m.f is p(v) with v ∈ {v1, v2 . . . vD} , then with probability at least 1− δ with δ ∈ (0, 1),\n||p̂(v)− p(v)||F ≤ 2\nd̃1s √ N\n( 1 + √ log(1/δ)\n2 ) (36)\n||p̂(v, v)− p(v, v)||F ≤ 2\nd̃2s √ N\n( 1 + √ log(1/δ)\n2 ) (37)\n||p̂(v, v, v)− p(v, v, v)||F ≤ 2\nd̃3s √ N\n( 1 + √ log(1/δ)\n2 ) (38)\nwhere, d̃1s = 1N ∑N\ni=1 nnz(xi), d̃2s = 1 N ∑N i=1 nnz(xi) 2, d̃3s = 1N ∑N i=1 nnz(xi) 3, and nnz(xi) is the non-zero entries in row xi of the data X as described in section 3.\nProof. The generative process in Equation 1 results in N sample documents x1:N that are vectors of count data, with∑\nv[x]v = nd, where x is the sample corresponding to the document d, and nd is the sum of the counts of all distinct words in that vector. The operation ∑ v denotes the sum\nacross the dimensions. From here, we can show that ||x|| =√∑ v[x] 2 v ≤ ∑ v[x]v = nd, since [x]v ≥ 0,∀v ∈ 1, 2 . . . D. Therefore, the samples have bounded norm. Without loss of generality, if we assume ||x|| ≤ 1 ∀x ∈ X , then from Lemma 7 of supplementary material of [11], with probability at least 1− δ with δ ∈ (0, 1),\n∣∣∣∣∣∣Ê[x]− E[x]∣∣∣∣∣∣ F ≤ 2√\nN\n( 1 + √ log(1/δ)\n2 ) (39)∣∣∣∣∣∣Ê[x⊗ x]− E[x⊗ x]∣∣∣∣∣∣\nF ≤ 2√\nN\n( 1 + √ log(1/δ)\n2 ) (40)∣∣∣∣∣∣Ê[x⊗ x⊗ x]− E[x⊗ x⊗ x]∣∣∣∣∣∣\nF ≤ 2√\nN\n( 1 + √ log(1/δ)\n2 ) (41)\nwhere E stands for true expectation, and Ê stands for the expectation estimated from the N samples, i.e.,\nÊ[x] = 1\nN N∑ i=1 xi = 1 N X>1\nÊ[x⊗ x] = 1 N N∑ i=1 xi ⊗ xi = 1 N X>X\nÊ[x⊗ x⊗ x] = 1 N N∑ i=1 xi ⊗ xi ⊗ xi = 1 N X ⊗X ⊗X\nNow, since each of our samples x1:N contains binary data, probability of the items can be estimated from the training data as p̂(v) = Ê[x]∑\nv Ê[x] , where\n∑ v Ê[x] is the sum\nof Ê[x] across the dimensions, i.e., all the items. Also, it can be shown that ∑ v Ê[x] = d̃1s. Therefore p̂(v) = Ê[x] d̃1s .\nPlease note that ∑ v E[x] ≈ ∑\nv Ê[x] = d̃1s, and therefore, p̂(v)− p(v) = 1\nd̃1s (Ê[x]−E[x]), and using this in Equation\n39, we get the first inequality of the Lemma (Equation 36). Since d̃2s = ∑ v ∑ v Ê[x ⊗ x] and d̃3s =∑\nv ∑ v ∑ v Ê[x⊗x⊗x], the pairwise and triple-wise prob-\nability matrices can be estimated as,\np̂(v, v) = Ê[x⊗ x]∑\nv ∑ v Ê[x⊗ x] = Ê[x⊗ x] d̃2s\np̂(v, v, v) = Ê[x⊗ x⊗ x]∑\nv ∑ v ∑ v Ê[x⊗ x⊗ x] = Ê[x⊗ x⊗ x] d̃3s\nSince ∑\nv ∑ v E[x ⊗ x] ≈ ∑ v ∑ v Ê[x ⊗ x] = d̃2s, and∑\nv ∑ v ∑ v E[x⊗x⊗x] ≈ ∑ v ∑ v ∑ v Ê[x⊗x⊗x] = d̃3s,\nwe can establish the following equations,\np̂(v, v)− p(v, v) = 1 d̃2s\n( Ê[x⊗ x]− E[x⊗ x] ) p̂(v, v, v)− p(v, v, v) = 1\nd̃3s\n( Ê[x⊗ x⊗ x]− E[x⊗ x⊗ x] ) Substituting these equations in Equation 40 and 41, we\ncomplete the proof.\nAlso, if yi represents the label vector associated with ith document, whereas xi represent the word vector,\nÊ[y ⊗ x⊗ x]− E[y ⊗ x⊗ x]\n= 1\nN N∑ i=1 yi ⊗ xi ⊗ xi − E[y ⊗ x⊗ x]\n= 1\nN N∑ i=1 yi ⊗ xi ⊗ xi − 1 N N∑ i=1 yi ⊗ E[x⊗ x]\n+ 1\nN N∑ i=1 yi ⊗ E[x⊗ x]− E[y ⊗ x⊗ x]\n(42)\nTherefore,∣∣∣∣∣∣Ê[y ⊗ x⊗ x]− E[y ⊗ x⊗ x]∣∣∣∣∣∣ F\n≤ ∣∣∣∣∣∣Ê[y]∣∣∣∣∣∣\nF ∣∣∣∣∣∣Ê[x⊗ x]− E[x⊗ x]∣∣∣∣∣∣ F\n+ ||E[x⊗ x]||F ∣∣∣∣∣∣Ê[y]− E[y]∣∣∣∣∣∣\nF\nWithout loss of generality, we can assume ||y|| ≤ 1. Then, ||Ê[y]|| ≤ 1.\nFrom Equation 39 and 40,\nP [∣∣∣∣∣∣Ê[y]− E[y]∣∣∣∣∣∣ F ≥ 2√\nN\n( 1 + √ log(1/δ)\n2\n)] ≤ δ\n(43)\nP [∣∣∣∣∣∣Ê[x⊗ x]− E[x⊗ x]∣∣∣∣∣∣ F ≥ 2√\nN\n( 1 + √ log(1/δ)\n2 )] ≤ δ\nTherefore, using union-bound principle on the above two probability,\nP [ ∣∣∣∣∣∣Ê[x]∣∣∣∣∣∣\nF ∣∣∣∣∣∣Ê[x⊗ x]− E[x⊗ x]∣∣∣∣∣∣ F\n+ ||E[x⊗ x]||F ∣∣∣∣∣∣Ê[y]− E[y]∣∣∣∣∣∣\nF\n≥ 4√ N\n( 1 + √ log(1/δ)\n2\n)] ≤ 2δ\nUsing Equation 42, and replacing δ by δ/2,\nP [ ∣∣∣∣∣∣Ê[y ⊗ x⊗ x]− E[y ⊗ x⊗ x]∣∣∣∣∣∣\nF\n≤ 4√ N\n( 1 + √ log(2/δ)\n2\n)] ≥ 1− δ\np̂(l, v, v) = Ê[y ⊗ x⊗ x]∑\nv ∑ v ∑ v Ê[y ⊗ x⊗ x] = Ê[y ⊗ x⊗ x]\nd̃ls (44) Also, since ∑\nl ∑ v ∑ v E[y⊗x⊗x] ≈ ∑ l ∑ v ∑ v Ê[y⊗\nx ⊗ x] = d̃ls, where d̃ls = 1N ∑N i=1 nnz(yi)nnz(xi) 2, and nnz(yi) is the number of labels associated with the ith document. In a similar way to the proof of Lemma 3, we can prove that with probability at least 1− δ,\n||p̂(l, v, v)− p(l, v, v)||F ≤ 4\nd̃ls √ N\n( 1 + √ log(2/δ)\n2\n) (45)\nAssigning ε1 = ( 1 + √ log(1/δ) 2 ) , and ε2 =(\n1 + √\nlog(2/δ) 2\n) , we get\nεM2 ≤ ||p(v, v)− p̂(v, v)||F ≤ 2ε1\nd̃2s √ N\nεM3 ≤ ||p(v, v, v)− p̂(v, v, v)||F ≤ 2ε1\nd̃3s √ N\nεM2L ≤ ||p(l, v, v)− p̂(l, v, v)||F ≤ 4ε2\nd̃ls √ N\nsince operator norm is smaller than Frobenius norm. Also, to satisfy εM2 ≤ σK(M2)/2, we need,\nN ≥ Ω ( 1 d̃2sσK(M2) ( 1 + √ log(1/δ) 2 ))2 (46) Or, N ≥ Ω (( ε1\nd̃2sσK(M2)\n)2) . This contributes in the\nsecond lower bound (n2) of N in Theorem 1. Also, from Equation 32,\nεtw ≤ (\n10\nd̃2sσK(M2)5/2 +\n2 √ 2\nd̃3sσK(M2)3/2 ) 2ε1√ N (47)\nFrom Lemma 2, ≤ c1 · (λmin/K), and we can assign as the upper bound of εtw. To satisfy this, we need,(\n10\nd̃2sσK(M2)5/2 +\n2 √ 2\nd̃3sσK(M2)3/2 ) 2ε1√ N ≤ c1 λmin K\n, or,( 10\nd̃2sσK(M2)5/2 +\n2 √ 2\nd̃3sσK(M2)3/2 ) 2ε1√ N ≤ c1\n1\nK √ πmax\nSince πmax ≤ 1, we need\nN ≥ Ω ( K2 (\n10\nd̃2sσK(M2)5/2 +\n2 √ 2\nd̃3sσK(M2)3/2\n)2 ε21 ) This contributes to n3 in Theorem 1.\nAppendix D. Completing the Proof\nHere, we will derive the final bounds for the reconstruction error for the parameters. Since µk = W †uk (Algorithm 1), with probability at least 1− δ,\n||µk − µ̂k|| = ||W †uk − Ŵ †ûk|| = ||W †uk −W †ûk +W †ûk − Ŵ †ûk|| ≤ ||W †||2||uk − ûk||+ ||W † − Ŵ †||2||ûk||\n≤ ||W †||2 8\nλk + εW †\n≤ 8 √ σ1(M2) +\n2 √ σ1(M2)\nσK (M2) εM2\n(48)\nSince 1λk = √ πk ≤ 1. Assigning as the upper bound\nof εtw in equation 32, with probability at least 1− δ,\n||µk − µ̂k|| ≤ 8 √ σ1(M2) ( 10\nd̃2sσK(M2)5/2 +\n2 √ 2\nd̃3sσK(M2)3/2 ) 2ε1√ N\n+ 2 √ σ1(M2)\nσK (M2)\n2ε1\nd̃2s √ N\n≤\n( 160 √ σ1(M2)\nd̃2sσK(M2)5/2 +\n32 √\n2σ1(M2)\nd̃3sσK(M2)3/2 +\n4 √ σ1(M2)\nd̃2sσK (M2)\n) ε1√ N\n(49)\nSimilarly, since πk ≤ 1, with probability at least 1− δ,\n|πk − π̂k| = ∣∣∣∣∣ 1λ2k − 1λ̂2k ∣∣∣∣∣ = ∣∣∣∣∣ (λk + λ̂k)(λk − λ̂k)λ2kλ̂2k ∣∣∣∣∣\n= ∣∣∣√πkπ̂k (√πk +√π̂k) (λk − λ̂k)∣∣∣\n≤ 2|λk − λ̂k| ≤ 10\nsince |λk − λ̂k| ≤ 5 from Lemma 2. Therefore, with probability at least 1− δ, we get\n|πk − π̂k| ≤ (\n200\nσK(M2)5/2 +\n40 √ 2\nσK(M2)3/2\n) ε1\nd̃3s √ N\nwhere ε1 = ( 1 + √ log(1/δ) 2 ) .\nAlso, since γk = u>kM2L(W,W )uk, with probability at least 1− δ,\n||γk − γ̂k|| = ∣∣∣∣∣∣u>kM2L(W,W )uk − û>k M̂2L(Ŵ , Ŵ )ûk∣∣∣∣∣∣\n≤ ∣∣∣∣u>kM2L(W,W )uk − û>kM2L(W,W )ûk∣∣∣∣\n+ ∣∣∣∣∣∣û>kM2L(W,W )û− û>k M̂2L(Ŵ , Ŵ )ûk∣∣∣∣∣∣\n≤ ||uk − ûk||2 ||M2L(W,W )||2 + ||ûk||2 ∣∣∣∣∣∣M2L(W,W )− M̂2L(Ŵ , Ŵ )∣∣∣∣∣∣ 2 ≤ ||uk − ûk||2||W ||2 ||M2L||2 + ||M2L||2||W − Ŵ || 2\n+ ||Ŵ ||2||M2L − M̂2L||2\n≤ 64||M2L||2 λ2kσK(M2) 2 + 4||M2L||2 σK(M2)3 ε2M2 + 2 σK(M2) εM2L ≤ 64||M2L||2 λ2kσK(M2) 2 + 4||M2L||2 σK(M2)3 ε2M2 + 2 σK(M2) εM2L ≤ 64 σK(M2) 2 + 4 σK(M2)3 ε2M2 + 2 σK(M2) εM2L\nSince the elements of M2L are also a probability, ||M2L||2 ≤ ||M2L||F ≤ 1, and 1/λ2k = π2k ≤ 1.\nAssigning as the upper bound of εtw in equation 32, with probability at least 1− δ,\n||γk − γ̂k||\n≤ 1024 σK(M2)\n( 5\nσK(M2)5/2 +\n√ 2\nσK(M2)3/2\n)2 ε21\n(d̃3s)2N\n+ 16 σK(M2)3 ε21 (d̃2s)2N +\n4\nσK(M2)\nε2\nd̃ls √ N\n(50) where ε1 = ( 1 + √ log(1/δ) 2 ) and ε2 =(\n1 + √\nlog(2/δ) 2\n) . This completes the proof of Theorem 1."
    } ],
    "references" : [ {
      "title" : "Wsabie: Scaling up to large vocabulary image annotation",
      "author" : [ "J. Weston", "S. Bengio", "N. Usunier" ],
      "venue" : "IJCAI, vol. 11, 2011, pp. 2764– 2770.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Large-scale video classification with convolutional neural networks",
      "author" : [ "A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei" ],
      "venue" : "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2014, pp. 1725–1732.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Statistical topic models for multi-label document classification",
      "author" : [ "T.N. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers" ],
      "venue" : "Machine learning, vol. 88, no. 1-2, pp. 157–208, 2012.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages",
      "author" : [ "R. Agrawal", "A. Gupta", "Y. Prabhu", "M. Varma" ],
      "venue" : "Proceedings of the 22nd international conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2013, pp. 13–24.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Large-scale multi-label learning with missing labels",
      "author" : [ "H.-F. Yu", "P. Jain", "P. Kar", "I.S. Dhillon" ],
      "venue" : "ICML, vol. 31, 2014.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky" ],
      "venue" : "Journal of Machine Learning Research, vol. 15, pp. 2773–2832, 2014. [Online]. Available: http://jmlr.org/papers/v15/anandkumar14b.html",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "From o (k 2 n) to o (n): a fast complex-valued eigenvalue solver for large-scale onchip interconnect analysis",
      "author" : [ "J. Lee", "V. Balakrishnan", "C.-K. Koh", "D. Jiao" ],
      "venue" : "Microwave Symposium Digest, 2009. MTT’09. IEEE MTT-S International. IEEE, 2009, pp. 181–184.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Matlab tensor toolbox version 2.6",
      "author" : [ "B.W. Bader", "T.G. Kolda" ],
      "venue" : "Available online, February 2015. [Online]. Available: http://www.sandia.gov/∼tgkolda/TensorToolbox/",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Bpr: Bayesian personalized ranking from implicit feedback",
      "author" : [ "S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme" ],
      "venue" : "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2009, pp. 452–461.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The relationship between precision-recall and roc curves",
      "author" : [ "J. Davis", "M. Goadrich" ],
      "venue" : "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 233–240.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Spectral methods for supervised topic models",
      "author" : [ "Y. Wang", "J. Zhu" ],
      "venue" : "Advances in Neural Information Processing Systems, 2014, pp. 1511–1519.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Shifted power method for computing tensor eigenpairs",
      "author" : [ "T.G. Kolda", "J.R. Mayo" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications, vol. 32, no. 4, pp. 1095–1124, October 2011.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sparse local embeddings for extreme multi-label classification",
      "author" : [ "K. Bhatia", "H. Jain", "P. Kar", "M. Varma", "P. Jain" ],
      "venue" : "Advances in Neural Information Processing Systems, 2015, pp. 730–738.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fastxml: A fast, accurate and stable treeclassifier for extreme multi-label learning",
      "author" : [ "Y. Prabhu", "M. Varma" ],
      "venue" : "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 263–272.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : "Both WSABIE [1] and LEML [5] utilizes such mappings.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "Both WSABIE [1] and LEML [5] utilizes such mappings.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "Unlike the usual cases where such latent variable models are trained using EM, we use Method of Moments [6] to extract the parameters from the latent variable model.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "From [6], if we define M2 as the pairwise probability matrix, with [M2]i,j = P [ vi, vj ] , we can express it as,",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "O ( ( ∑N i=1 nnz(xi) )K ) , since the total number of nonzero entries in M2 is O (∑N i=1 nnz(xi) 2 ) [7].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "We used the Tensor Toolbox [8] for tensor decomposition.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "The scores of LEML and MoM are not directly comparable, since the score of LEML can be negative, whereas the score of MoM lies within [0, 1]",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 4,
      "context" : "Since LEML is shown to outperform WSABIE and other benchmark algorithms on various small and large-scale datasets in [5], we benchmark the performance of our method against LEML.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 4,
      "context" : "For LEML, we ran ten iterations for the smaller datasets (Bibtex and Delicious) and five iterations for the larger datasets, since the authors of LEML chose a similar number of iterations for their experiments in [5].",
      "startOffset" : 213,
      "endOffset" : 216
    }, {
      "referenceID" : 8,
      "context" : "AUC is a versatile measure, and is used to evaluate the performance of classification as well as prediction algorithms [9].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "Also, it is shown that there exists a one-to-one relation between AUC and PrecisionRecall curve in [10], i.",
      "startOffset" : 99,
      "endOffset" : 103
    } ],
    "year" : 2017,
    "abstractText" : "Here we study the problem of predicting labels for large text corpora where each text can be assigned multiple labels. The problem might seem trivial when the number of labels is small, and can be easily solved using a series of one-vsall classifiers. However, as the number of labels increases to several thousand, the parameter space becomes extremely large, and it is no longer possible to use the one-vs-all technique. Here we propose a model based on the factorization of higher order word vector moments, as well as the cross moments between the labels and the words for multi-label prediction. Our model provides guaranteed converge bounds on the extracted parameters. Further, our model takes only three passes through the training dataset to extract the parameters, resulting in a highly scalable algorithm that can train on GB’s of data consisting of millions of documents with hundreds of thousands of labels using a nominal resource of a single processor with 16GB RAM. Our model achieves 10x-15x order of speed-up on large-scale datasets while producing competitive performance in comparison with existing benchmark algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}