{
  "name" : "1604.00783.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Topic Model Based Multi-Label Classification from the Crowd",
    "authors" : [ "Divya Padmanabhan", "Satyanath Bhat", "Shirish Shevade" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Multi label classification is a variant of a classification problem wherein an instance d is associated with multiple classes or labels. There are several areas where multi-label classification finds applications, for example, text classification, image retrieval, etc. Consider the task of classification of documents into several classes such as crime, politics, arts, sports etc. The classes are not mutually exhaustive since a document belonging to the ‘politics’ category may also belong to ‘crime’. In the classification of images, an image belonging to ‘forest’ category may also belong to ‘scenery’ category, and so on.\nOne of the solution approaches for multi-label classification is to generate a new label set that is a power set of the original label set, and then use traditional single label classification techniques. The immediate limitation here is an exponential blow-up of the label set and availability of only a small sized training data for each of the generated labels. Another approach is to build one-vs-all binary classifiers, where, for each label, a binary classifier is built. This method, however, does not take into account the correlation between the labels.\nIn the past, topic models [1] have proved to be successful in modeling the process behind generating text documents. The idea is to model latent topics responsible for generating words. Originally, topic models were used in an unsupervised manner and were gradually adapted to the supervised learning setting. In the supervised setting, the topics and hence words are assumed to\nar X\niv :1\n60 4.\n00 78\n3v 1\n[ cs\n.L G\n] 4\nA pr\n2 01\nbe generated depending only on the classes that are present. The topic models discussed in the literature do not make use of the information provided by ‘absence’ of classes. The absence of a class often provides critical information about the words present. For example, a document labeled ‘sports’ is less likely to have words related to ‘arts’. Similarly in the images domain, an image labeled ‘city’ is less likely to have the characteristics of ‘forest’. Needless to say, such correlations are dataset dependent. However a principled analysis must account for such correlations. Motivated by this subtle observation, we introduce a novel topic model for multi-label classification.\nFurther the problem renders itself more interesting when the labels are procured from multiple heterogenous noisy crowd-workers whose qualities are unknown. We also refer to crowd-workers as annotators in the paper. In the current era of big data where large amounts of unlabeled data are readily available, obtaining a noiseless source for labels is almost impossible. However it is possible to get instances labeled by several human annotators. The problem becomes harder as now the true labels are unknown and the qualities of the annotators must be learnt in order to train a model. We non-trivially extend our topic model to this scenario.\nContributions\n1. We introduce a novel topic model for multi-label classification; our model has the distinctive feature of exploiting any additional information provided by the absence of classes. We refer to our topic model as ML-PA-LDA (Multilabel Presence-Absence LDA). 2. If the labels are provided by multiple noisy annotators (from a crowd), we enhance our model to account for heterogenous annotators with unknown qualities. We refer to this enhanced model as ML-PA-LDA-C (ML-PA-LDA with Crowd). A feature of ML-PA-LDA-C is it does not require an annotator to label all classes for a document. Even partial labeling by the annotators upto the granularity of labels within a document is adequate. 3. Our experiments on real world datasets establish the superior performance of both of our models. The qualities of the annotators learned by our algorithm approximate closely the true qualities of the annotators.\nWe now describe relevant approaches available in the literature."
    }, {
      "heading" : "2 Related Work",
      "text" : "Several approaches have been devised for multilabel classification with labels provided by a single source. The most natural approach is the Label Powerset (LP) method [5] which generates a new class for every combination of labels and then solves the problem using multiclass classification approaches. The main drawback of this approach is the exponential growth in the number of classes, leading to several generated classes having very few labeled instances leading to\noverfitting. To overcome this drawback, RAndom k-labELsets method (RAkEL) [20] was introduced, which constructs an ensemble of LP classifiers where each classifier is trained with a random subset of k labels. However, the large number of labels still poses challenges. The approach of pairwise comparisons (PW) improves upon the above methods, by constructing C(C-1)/2 classifiers for every pair of classes, where C is the number of classes. Finally a ranking of the predictions from each classifier yields the labels for a test instance. Rank-SVM [8] uses PW approach to construct SVM classifiers for every pair of classes and then performs a ranking.\nThe previously described approaches are discriminative approaches. Generative models for multilabel classification model the correlation between the classes using mixing weights for the classes [13]. Other probabilistic mixture models include Parameteric Mixture Models PMM1 and PMM2 [22]. After the advent of the topic models like Latent Dirichlet Allocation (LDA) [1], extensions have been proposed for multi-label classification such as Wang et al [23]. However in [23], due to the non-conjugacy of the distributions involved, closed form updates cannot be obtained for several parameters and iterative optimization algorithms such as conjugate gradient and Newton Raphson are required to be used in the variational E step as well as M step, introducing additional implementation issues. Adapting this model to the case of crowds would result in enormous complexity. The topic models proposed for multi-label classification in [19] involve far too many parameters which can be learnt effectively only in the presence of large amounts of labeled data. For small and medium sized datasets, the approach suffers from overfitting. Moreover it is not clear how this model can be adapted when labels are procured from crowdworkers with unknown qualities. SLDA [12] is a single label classification technique which works well on multilabel classification when used with the one-vs-all approach. SLDA inherently captures the correlation between classes through the latent topics.\nWith crowdsourcing gaining popularity due to the availability of large amounts of unlabeled data and difficulty in procuring noiseless labels for these datasets, aggregating labels from the crowd has become an important problem. Raykar et al [15] look at training binary classification models with labels from a crowd with unknown annotator qualities. Being a model for multiclass classification, this model does not capture the correlation between classes and thereby cannot be used for multi-label classification from the crowd. Mausam et al [4] look at multi-label classification for taxonomy creation from the crowd. They construct C classifiers by modeling the dependence between the classes explicitly. The graphical model representation involves too many edges especially when the number of classes is large and hence the model suffers from overfitting. Deng et al [6] look at selecting the instance to be given to a set of crowd-workers. However they do not look at aggregating these labels and developing a model for classification given these labels. In the report [7], Duan et al. look at methods to aggregate a multi-label set provided by crowd-workers. However, they do not look at building a model for classification for new test instances for which the labels are not provided by the crowd. Recently the topic model, SLDA, has been\nadapted to learning from the labels provided by crowd annotators [18]. However, like its predecessor SLDA, it is only applicable to the single label setting and not to multi-label classification.\nThe existing topic models in the literature assume that the presence of a class generates words pertaining to those classes and do not take into account the fact that the absence of a class may also play a role in generating words. In practice, the absence of a class may yield information about occurrence of words. We propose a model for multi-label classification based on latent topics where the presence as well as absence of a class generates topics. The labels could be procured from a set of crowd workers whose qualities are unknown."
    }, {
      "heading" : "3 Our Approach for Multi-label Classification: ML-PA-LDA",
      "text" : "We now explain our model for multi-label classification. For ease of exposition, we use notations from the text domain. However the model itself is general and can be applied to several domains by suitable transformation of features into words. In our experiments we have applied the model to domains other than text. We will explain the transformation of features to words when we describe our experiments.\nLet D be the number of documents in the training set, C the total number of classes, T the number of topics and K the number of annotators. In multi-label classification, a document may belong to any ‘subset’ of the C classes as opposed to the standard classification setting where a document belongs to exactly one class. We denote by V the size of the vocabulary ν={ν1, . . . , νV }, where νj refers to the jth word in ν. Consider a document d comprising N words w= {w1, w2, . . . , wN} from the vocabulary ν. Let λ= [λ1, . . . , λC ]∈{0, 1}C denote the true class membership of the document. In our notations, we denote by wnj the value 1[wn=νj ], that is the indicator that the word wn is the j\nth word of the vocabulary. Similarly, we denote by λij , the indicator that λi= j, where j= 0 or 1. The objective is to obtain λ for every test document.\nTopic Model for the Documents\nWe introduce a model to capture the correlation between the various classes generating a given document. The presence as well as absence of a class provides additional information about the topics present in a document. We now describe the generative process for each document assuming labels are provided by a perfect source.\n1. Draw λi∼Bern(ξi) for every class i= 1, . . . , C. 2. Draw θi,j,.∼Dir(αi,j,.) for i= 1, . . . , C, for j∈{0, 1}, where αi,j,. is a Dirichlet\ndistribution with T parameters. 3. For every word w in the document\n(a) Sample u∼Unif{1, . . . , C} from one of the C classes.\n(b) Generate a topic z∼Mult(θu,λu,.), where θi,j,. is a multinomial distribution in T dimensions. (c) Generate the word w∼Mult(βz.) where βt. is a multinomial distribution in V dimensions.\nWe refer to this model where the true class vector λ is observed for the training documents as ML-PA-LDA (Multi-label Presence-Absence LDA).\nSingle Coin Model for the Annotators\nWhen the true labels of the documents are not observed, λ is unknown. Instead noisy versions y1, . . . , yK of λ provided by a set of K independent annotators with heterogenous unknown qualities {ρ1, . . . , ρK} are observed.yji can be either 0, 1 or −1. yji= 1 indicates that, according to annotator j, the class i is present while yji= 0 indicates that the class i is absent as per annotator j. yji=−1 indicates that the annotator j has not made a judgement on the presence of class i in the document. This allows for partial labeling upto the granularity of labels even within a document. This flexibility in the modeling is essential, especially when the number of classes is large. ρj is the probability with which an annotator reports the ground truth corresponding to each of the classes. ρj is not known to the learning algorithm. For simplicity we have assumed the single coin model for annotators and also that the qualities of the annotators are independent of the class under consideration. That is, P (yj1 = 1|λ1 = 1) = P (yj1 = 0|λ1 = 0) = . . .=P (yjC = 1|λC = 1) =P (yjC = 0|λC = 0) =ρj .\nThe generative process for the documents is depicted pictorially in Figure 1a. The parameters of our model consist of π={α , ξ , ρ, β}. The observed variables for each document are d={w, yij} for i= 1, . . . , C, j= 1, . . . ,K. The hidden random variables are Θ={θ, λ, u, z}. We refer to our topic model trained with labels from annotators as ML-PA-LDA-C (multi-label presence-absence LDA from crowds)."
    }, {
      "heading" : "4 Variational EM for ML-PA-LDA-C",
      "text" : "We now detail the steps for estimating the parameters of our proposed model. Since ML-PA-LDA-C is a generalization of ML-PA-LDA to adapt to the crowd, we provide the details of the steps for ML-PA-LDA-C and give pointers to highlight the differences with ML-PA-LDA whenever appropriate.\nGiven the observed words w and the labels y1, . . . , yk for a document d, the objective of the model described above is to obtain p(Θ|d). Here, the challenge lies in the intractable computation of p(Θ|d) which arises due to the intractability in the computation of p(d|π). We use variational inference with mean field assumptions to overcome this challenge.\nSuppose q(Θ) is any distribution over Θ for any arbitrary Θ={θ, λ, u, z} which approximates p(Θ|d). The underlying variational model is provided in Figure 1b.\nln p(d|π) = ln p(d, Θ) p(Θ|d) = ln p(d, Θ)q(Θ) q(Θ)p(Θ|d) =Eq(Θ) [ ln p(d, Θ)q(Θ) q(Θ)p(Θ|d) ] =Eq(Θ) [ln p(d, Θ)− ln q(Θ)] + Eq(Θ) [ln q(Θ)− p(d|Θ)] =L(Θ) + KL(q(Θ)||p(Θ|d) (1)\nThe idea is to maximise L(Θ) over the variational parameters {δ, γ, φ, δ} so that KL(q(Θ)||p(d|Θ) also gets minimized.\nlnp(d, Θ|π) = ln p(w, y, θ, λ, u, z|π) = ln p(λ|ξ) + ln p(θ|α) + ln p(u) + ln(z|λ, u, θ) + ln p(w|z, β) + ln p(y|λ, ρ)\nln p(λ|ξ) = C∑ i=1 λi ln ξi + (1− λi) ln(1− ξi) (2)\nln p(θij |α) = lnΓ ( T∑ t=1 αijt ) − T∑ t=1 lnΓαijt + T∑ t=1 (αijt − 1) log θijt (3)\nlog p(u) = N∑ n=1 C∑ i=1 uni log 1/C (4)\nlog p(z|u, λ, θ) = N∑ n=1 T∑ t=1 C∑ i=1 1∑ j=0 uniλijznt log θijt (5)\nlog p(w|z, β) = N∑ n=1 T∑ t=1 V∑ j=1 wnjznt log βtj (6)\nlog p(y|λ, ρ) = K∑ j=1 C∑ i=1 [λiyij + (1− λi)(1− yij)] log ρj\n+ [(1− λi)yij + λi(1− yij)] log 1− ρj (7)\nAssume the following variational distributions over Θ for a document d. ud∼Mult(δd) , λdi ∼Bern(∆di ) for i= 1, . . . , C ,zd∼ Mult(φd) θdij∼Dir(γdij) for i= 1, . . . , C, j= 0 and 1 Therefore, for a document d,\nq(Θd) = C∏ i=1 q(λd) C∏ i=1 1∏ j=0 q(θdij) N∏ n=1 C∏ i=1 q(udni)q(z d ni)\nThe E-step involves computing the document-specific variational parameters {δd, ∆d, γd, φd} assuming a fixed value for the parameters Π={α, ξ, ρ, β}. As a consequence of the mean field assumptions on the variational distributions, we get the following update rules for the distributions. From now on, when clear from context we omit the superscript d.\nlog q(z) =EΘ\\z [p(d, Θ)]∝Eu,λ,θ [log p(z|u, λ, θ)] + log p(w|z, β)\n∝ N∑ n=1 T∑ t=1 znt [ C∑ i=1 1∑ j=0 E[uni]E[λij ]E[log θijt] + V∑ j=1 wnj log βtj ] (8)\nIn the computation of the expectation of EΘ\\z [p(d, Θ)] in Eqn 8, the terms in p(d, Θ) that are a function of z need to be considered as the rest of the terms contribute to the normalizing constant for the density function q(z). Hence expectations of log p(z|u, λ, θ) (Eqn 5) and log p(w|z, β) (Eqn 6) must be taken with respect to u, λ, θ. Therefore ,\nlog φnt∝ C∑ i=1 1∑ j=0 E[uni]E[λij ]E[log θijt] + V∑ j=1 wnj log βtj\n= C∑ i=1 1∑ j=0 δni∆ j i (1−∆i) 1−jE[log θijt] + V∑ j=1 wnj log βtj (9)\nSimilarly, the updates for the other variational parameters follows.\nlog q(u) =EΘ\\u [p(d, Θ)] =EΘ\\u [log p(u) + p(z|u, λ, θ)]\n∝ N∑ n=1 C∑ i=1 uni log 1/C + N∑ n=1 T∑ t=1 C∑ i=1 1∑ j=0 uniE[λij ]E[znt]E[log θijt] (10)\nTherefore,\nlog δni∝ log 1\nC + T∑ t=1 φnt∆iE [log θi1t] + φnt(1−∆i)E [log θi0t] (11)\nlog q(θ) =EΘ\\θ [p(d, Θ)]∝E [p(θ|α) + p(z|u, λ, θ)] (12)\n= C∑ i=1 1∑ j=0 T∑ t=1 (αijt − 1) log θijt + N∑ n=1 C∑ i=1 1∑ j=0 T∑ t=1 E[uni]E[znt]E[λij ] log θijt (13)\n= C∑ i=1 1∑ j=0 T∑ t=1 (αijt − 1) log θijt + N∑ n=1 C∑ i=1 1∑ j=0 T∑ t=1 δniφnt∆ j i (1−∆i) 1−j log θijt\n= C∑ i=1 1∑ j=0 T∑ t=1 (γijt − 1) log θijt (14)\nwhere,\nγijt=αijt + (∆i) j(1−∆i)1−j N∑ n=1 δniφnt (15)\nlog q(λ) =EΘ\\λ [p(d, Θ)]∝E [log p(λ|ξ) + log p(z|u, λ, θ) + log p(y|λ, ρ)]\n= C∑ i=1 λi log ξi + (1− λi) log(1− ξi)\n+ N∑ n=1 T∑ t=1 C∑ i=1 1∑ j=0 λji (1− λi) 1−jE[uni]E[znt]E[log θijt]\n+ K∑ j=1 C∑ i=1 [λiyij + (1− λi)(1− yij)] log ρj\n+ [(1− λi)yij + λi(1− yij)] log 1− ρj (16)"
    }, {
      "heading" : "4.1 E-step Updates for ML-PA-LDA-C",
      "text" : "In the E-step, the following steps need to be performed iteratively for each document d.\nlog∆di ∝ log ξi + K∑ j=1 ydij log ρj + (1− ydij) log 1− ρj + Nd∑ n=1 T∑ t=1 δdniφ d ntE [ log θdi1t ] (17)\nlog(1−∆di )∝ log 1− ξi + K∑ j=1 (1− ydij) log ρj + ydij log(1− ρj) + Nd∑ n=1 T∑ t=1 δdniφ d ntE [ log θdi0t ] (18)\nlog φdnt∝ C∑ i=1 δdni [ ∆diE [ log θdi1t ] + (1−∆di )E [ log θdi0t ]] + V∑ j=1 wdnj log βtj (19)\nlog δdni∝ log 1\nC + T∑ t=1 φnt∆ d iE [ log θdi1t ] + φdnt(1−∆di )E [ log θdi0t ] (20)\nγdijt=αijt + (∆ d i ) j(1−∆di )1−j N∑ n=1 δdniφ d nt (21)\nIn all the above update rules, E[log θdijt] =ψ(γijt)−ψ( ∑T t′=1 γ d ijt′), where ψ(.) is the digamma function. For the non-crowd model ML-PA-LDA, the variational parameter ∆i is absent as λi is observed. Therefore the E-step boils down to computing the updates for φ, δ and γ from Eqns 19, 20 and 21 with ∆i replaced by λi."
    }, {
      "heading" : "4.2 M-step Updates for ML-PA-LDA-C",
      "text" : "In the M-step, the parameters ξ, ρ, β and α are estimated using the values of ∆d, φdnt, δ d ni, γ d ijt estimated from the E-step. The function L(Θ) in Eqn 1 is maximized with respect to the parameters π yielding the following update equations. Updates for ξ:\nξi=\n∑D d=1∆ d i\nD for i= 1, . . . , C. (22)\nIntuitively, Eqn 22 makes sense as ξi is the probability that any document in the corpus belongs to class i. ∆di is the probability that document d belongs to class i and is computed in the E-step. Therefore ξi is an average of ∆ d i over all documents. Updates for ρ: for j= 1, . . . ,K:\nρj =\nD∑ d=1 C∑ i=1 1 [ ydij 6=−1 ] [ ydij∆ d i + (1− ydij)(1−∆di ) ] D∑ d=1 C∑ i=1 1 [ ydij 6=−1 ] [ ydij∆ d i + (1− ydij)(1−∆di ) + ydij(1−∆di ) + (1− ydij)∆di\n] (23)\nFrom Eqn 23, we observe that ρj is the fraction of times that crowd-worker j has provided a label that is consistent with the probability estimate ∆di over all classes i. The implicit assumption is that every crowd-worker has provided at least one label, otherwise such a crowd-worker need not be considered in the model. Updates for β: for t= 1, . . . , T ; for j= 1, . . . , V :\nβtj =\n∑D d=1 ∑Nd n=1 w d njφ\nd nt∑D\nd=1Nd (24)\nIntuitively, the variational parameter φdnt is the probability that the word w d n is associated with topic t. Having updated this parameter in the E-step, βtj computes the fraction of times the word j is associated with topic t by giving a weight φdnt to its occurrence in document d.\nUpdates for α: There do not exist closed form updates for α parameters. Hence we use Newton Raphson method to iteratively obtain the solution as follows.\nαt+1ijr =α t ijr − gr − c hr\n(25)\nwhere, c= ∑T τ=1 gτ/hτ\nz−1+ ∑T τ=1 1/hτ\n, z=Dψ′ (∑T\nt′=1 α t ijt′ ) , hτ =−Dψ′(αtijr),\ngr =D [ ψ (∑T τ=1 α t ijτ ) − ψ ( αtijr )] + ∑D d=1 [ ψ ( γdijr ) − ψ (∑T tau=1 γ d ijτ )] The M-step updates for β and α involved in ML-PA-LDA (non-crowd version) are same as the updates in the crowd version, ML-PA-LDA-C. The parameter ρ is absent in ML-PA-LDA. Eqn 22, with ∆di replaced by λ d i (as in the E-step) is used to update ξi."
    }, {
      "heading" : "5 Smoothing",
      "text" : "In the model described in Section 3, we modeled β to be a parameter that governs the multinomial distributions for generating the words from each topic. In general, a new document can include words that have not been encountered in any of the training documents. The unsmoothed model described earlier does not handle this issue. In order to handle this, we must “smoothen” the multinomial parameters involved. One way to perform smoothing is to model β as a multinomial random variable with parameters η. The corresponding graphical model is provided in Figure 2. Again due to the intractable nature of the computations, we model the variational distribution for β as β∼Mult(χ). We estimate variational parameter χ in the E-step of variational EM using Eqn 26 assuming η is known.\nχtj =ηtj + D∑ d=1 Nd∑ n=1 φdntw d nj (26)\nThe model parameter η is estimated in the M-step using Newton Raphson method as follows.\nηt+1ir =η t ir − gr − c hr\n(27)\nwhere, c= ∑V τ=1 gτ/hτ\nz−1+ ∑T τ=1 1/hτ\n, z=ψ′ (∑V\nj′=1 η t ij′ ) , hr =−ψ′(ηtir),\ngr = [ ψ (∑V j′=1 η t ij′ ) − ψ (ηtir) ] + [ ψ (χir)− ψ (∑V j′=1 χij′ )] . The steps for the derivation are similar to the steps for non-smooth version."
    }, {
      "heading" : "6 Experiments",
      "text" : "In order to test the efficacy of the proposed techniques, we evaluate our model on datasets from several domains."
    }, {
      "heading" : "6.1 Dataset Descriptions",
      "text" : "We have carried out our experiments on several datasets from the text domain as well as non-text domain. We now describe the datasets and the preprocessing steps below.\nText Datasets: In the text domain, we have performed studies on the Reuters21578, Bibtex and Enron datasets. Reuters-21578: The Reuters-21578 dataset [11] is a collection of documents with news articles. The original corpus had 10,369 documents and a vocabulary of 29930 words. We performed stemming using the Porter Stemmer algorithm [14] and also removed the stop words. From this set the words which occurred more than 50 times across the corpus were retained and only documents which contained more than 20 words were retained. Finally the most commonly occurring top 10 labels were retained namely acq, crude, earn, fx, grain , interest, money, ship, trade, wheat. This led to a total of 6547 documents and a vocabulary of size 1996. Of these, a random 80% was used as training set and the remaining 20% as test. Bibtex: The Bibtex dataset [10] was released as part of the ECML-PKDD 2008 Discovery Challenge. The task is to assign tags such as physics, graph, electrochemistry etc to bibtex entries. There are a total of 4880 and 2515 entries in the training set and test respectively. The size of the vocabulary is 1836 and the number of tags is 159. Enron: The Enron dataset [21] is a collection of emails for which a set of pre-defined categories are to be assigned. There are a total of 1123 and 573 training and test instances respectively with a vocabulary of 1001 words. The total number of email tags are 53.\nNon-text Datasets: We also evaluate our model on datasets from domains other than text, where the notion of words is not explicit. Converting real valued features to words: Since we assume a bag-of-words model, we must replace every real-valued feature with a ‘word’ from a ‘vocabulary’. We begin by choosing an appropriate size for the vocabulary. Thereafter, we collect every real number which occurs across features and instances in the corpus into a set. We then cluster this set into V clusters, using the k-means algorithm, where V is the size of the vocabulary previously chosen. Therefore, each real valued feature has a new representative word given by the nearest cluster center to the feature under consideration. The corpus is then generated according to this new feature representation scheme. Yeast: The Yeast dataset [8] contains a set of genes which may be associated with several functional classes. There are 1500 training examples and 917 examples in the test set with a total of 14 classes and 103 real valued features. Scene: The Scene dataset [3] is a dataset of images. The task is to classify images into the following 6 categories- beach, sunset, fall, field, mountain, urban. The dataset contains 1211 instances in the training set and 1196 instances in the test set with a total of 294 real valued features.\nIn our experiments, we use the measures, accuracy across classes, micro-f1 score and average class log likelihood on the test sets to evaluate our model.\nThe average class log-likelihood on the test instances is computed as follows:\nlog-l =\n∑Dtest d=1 ∑C i=1 λ d i log∆ d i + (1− λdi ) log(1−∆di )\nDtest × C\nwhere Dtest is the number of instances in the test set. The details of computation of the other measures can be found in the survey [9]."
    }, {
      "heading" : "6.2 Results: ML-PA-LDA (Non-Crowd Version)",
      "text" : "We run our model first assuming labels from a perfect source. In Table 1 we compare the performance of our non-annotator model vs other methods such as RAKel, Monte Carlo Classifier Chains (MCC) [16], Binary Relevance Method - Random Subspace (BRq) [17], Bayesian Chain Classifiers (BCC) [24] and SLDA. BCC [24] is a probabilistic method which constructs a chain of classifiers by modeling the dependencies between the classes using a bayesian network. MCC instead uses a monte-carlo strategy to learn the dependencies. BRq improves upon binary relevance methods of combining classifiers by constructing an ensemble. As mentioned earlier RAKel draws subsets of the classes, each of size k and constructs ensemble classifiers. The implementations of RAKel, MCC, BRq and BCC provided by Meka (http://meka.sourceforge.net/) were used. For SLDA the code provided by the authors was used. On the reuters dataset, MLPA-LDA (without the annotators) performs significantly better than SLDA. In fact, ML-PA-LDA-C trained with noisy labels provided by simulated annotators also performs much better than SLDA. On the bibtex and enron datasets, MLPA-LDA does better than SLDA while ML-PA-LDA-C is not far. On scene and\nyeast datasets, ML-PA-LDA, ML-PA-LDA-C and SLDA give the same performance. It is to be noted that these datasets, known to be hard, are from the images and biology domains respectively. As can be seen from the table, our model gives a better overall performance than SLDA and also does not require training C binary classifiers. This advantage is a significant one, especially in datasets such as bibtex where the number of classes is 159.\nWe compared the performance of our algorithm with the size of the datasets used for training as well as the number of topics used. The results of our model are shown in Figure 3. An increase in the size of the dataset improves the performance of our model with respect to all the measures in use. Similarly an increase in the number of topics generally improves the measures under consideration. A striking observation is the low accuracy, log likelihood and micro-f1 scores associated with the model when the number of topics = 80 (eight times the number of classes) and the size of the dataset is low (S=25%). This is expected as the number of parameters to be estimated is too large to be learned using very few training examples. However as more training data is available, the model achieves enhanced performance. This observation is consistent with Occam’s razor [2]."
    }, {
      "heading" : "6.3 Results: ML-PA-LDA-C (Crowd Version)",
      "text" : "To verify the performance of the annotator model where the labels are provided by multiple noisy annotators, we simulated 50 annotators with varying qualities. The ρ values of the annotators were sampled from a uniform distribution. For 10 of these annotators, ρ was sampled from U [0.51, 0.65]. For another 20 of them, ρ was sampled from U [0.66, 0.85] and for the remaining 20 of them ρ was sampled from U [0.86, 0.9999]. This captures the heterogenity in the annotator qualities. For each document in the training set, a random 10% (= 5) annotators were picked for generating the noisy labels.\nIn Table 2 we compare the performance of the annotator model against our non-annotator model version. We find that the performance of ML-PA-LDA-C is close to that of ML-PA-LDA and most often better than SLDA, inspite of having\naccess to only noisy labels. We also report ‘Ann RMSE’ which is the L2 norm of the difference in predicted qualities of the annotators vs the true qualities.\nAnn RMSE = √√√√ K∑ j=1 |ρ̂j − ρj |2/K (28)\nwhere ρ̂j is the quality of annotator j as predicted by our variational EM algorithm and ρj is the true, hidden annnotator quality. We find that ‘Ann RMSE’ decreases as more training data is available showing the efficacy of our model for learning the qualities of the annotators.\nSimilar to the experiment carried out on the non-annotator model, we vary the number of topics as well as data-set sizes and compute all the measures used. The plots are shown in Figure 4. As in the non-annotator model, an increase in the topics as well as dataset size improves the performance of the algorithm in general. The Occam’s razor observation holds here too as similar performane is achieved for a small as well as large number of topics when the size of the training dataset is small. But as more training data becomes available, having more number of topics helps.\nAdversarial Annotators: We also tested the robustness of our model against labels from adversarial or malicious annotators. An adversarial annotator is characterised by a quality parameter ρ<0.5. As in the previous case, we simulated 50 annotators. The ρ values of 10 of them was sampled from U [0.0001, 0.1]. For another 15 annotators, ρ was sampled from U [0.51, 0.65]. For another 20 of them ρ was sampled from U [0.66, 0.85] and for the remaining 5 of them ρ was sampled from U [0.86, 0.9999]. On the Reuters dataset, we obtained an average accuracy of 0.955, average class log likelihood of -0.193, average micro-f1 of 0.793 and an average ann-rmse of 0.002 over five runs, with 40 topics. This shows that even in the presence of malicious annotators, our model remains unaffected and performs well."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have introduced a new approach for multi-label classification using a novel topic model, which uses information about the presence as well as absence of classes. In the scenario when the true labels are not available and instead a noisy version of the labels is provided by the annotators, we have adapted our topic model to learn the parameters including the qualities of the annotators. Our experiments on real world datasets demonstrate the superior performance of our approach."
    } ],
    "references" : [ {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "JMLR,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Occam’s razor",
      "author" : [ "A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth" ],
      "venue" : "Inf. Process. Lett.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Learning multi-label scene classification",
      "author" : [ "M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown" ],
      "venue" : "Pattern recognition,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Mausam, and D",
      "author" : [ "J. Bragg" ],
      "venue" : "S. Weld. Crowdsourcing multi-label classification for taxonomy creation. In HCOMP,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Scalable multi-label annotation",
      "author" : [ "J. Deng", "O. Russakovsky", "J. Krause", "M.S. Bernstein", "A. Berg", "L. Fei-Fei" ],
      "venue" : "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A kernel method for multi-labelled classification",
      "author" : [ "A. Elisseeff", "J. Weston" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Multilabel text classification for automated tag suggestion",
      "author" : [ "I. Katakis", "G. Tsoumakas", "I. Vlahavas" ],
      "venue" : "In Proceedings of the ECML/PKDD-08 Workshop on Discovery Challenge,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Supervised topic models",
      "author" : [ "J.D. Mcauliffe", "D.M. Blei" ],
      "venue" : "In NIPS.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Multi-label text classification with a mixture model trained by em",
      "author" : [ "A.K. McCallum" ],
      "venue" : "In AAAI 99 Workshop on Text Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Learning from crowds",
      "author" : [ "V.C. Raykar", "S. Yu", "L.H. Zhao", "G.H. Valadez", "C. Florin", "L. Bogoni", "L. Moy" ],
      "venue" : "JMLR,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Efficient monte carlo optimization for lultilabel classifier chains",
      "author" : [ "J. Read", "L. Martino", "D. Luengo" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Classifier chains for multi-label classification",
      "author" : [ "J. Read", "B. Pfahringer", "G. Holmes", "E. Frank" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning supervised topic models from crowds",
      "author" : [ "F. Rodrigues", "B. Ribeiro", "M. Lourenço", "F. Pereira" ],
      "venue" : "In HCOMP,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Statistical topic models for multi-label document classification",
      "author" : [ "T.N. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Random k-labelsets for multilabel classification",
      "author" : [ "G. Tsoumakas", "I. Katakis", "I. Vlahavas" ],
      "venue" : "TKDE,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Mulan: A java library for multi-label learning",
      "author" : [ "G. Tsoumakas", "E. Spyromitros-Xioufis", "J. Vilcek", "I. Vlahavas" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Parametric mixture models for multi-labeled text",
      "author" : [ "N. Ueda", "K. Saito" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A generative probabilistic model for multi-label classification",
      "author" : [ "H. Wang", "M. Huang", "X. Zhu" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Bayesian chain classifiers for multidimensional classification",
      "author" : [ "J.H. Zaragoza", "L.E. Sucar", "E.F. Morales", "C. Bielza", "P. Larrañaga" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In the past, topic models [1] have proved to be successful in modeling the process behind generating text documents.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "To overcome this drawback, RAndom k-labELsets method (RAkEL) [20] was introduced, which constructs an ensemble of LP classifiers where each classifier is trained with a random subset of k labels.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "Rank-SVM [8] uses PW approach to construct SVM classifiers for every pair of classes and then performs a ranking.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "Generative models for multilabel classification model the correlation between the classes using mixing weights for the classes [13].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "Other probabilistic mixture models include Parameteric Mixture Models PMM1 and PMM2 [22].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "After the advent of the topic models like Latent Dirichlet Allocation (LDA) [1], extensions have been proposed for multi-label classification such as Wang et al [23].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 17,
      "context" : "After the advent of the topic models like Latent Dirichlet Allocation (LDA) [1], extensions have been proposed for multi-label classification such as Wang et al [23].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 17,
      "context" : "However in [23], due to the non-conjugacy of the distributions involved, closed form updates cannot be obtained for several parameters and iterative optimization algorithms such as conjugate gradient and Newton Raphson are required to be used in the variational E step as well as M step, introducing additional implementation issues.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 13,
      "context" : "The topic models proposed for multi-label classification in [19] involve far too many parameters which can be learnt effectively only in the presence of large amounts of labeled data.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "SLDA [12] is a single label classification technique which works well on multilabel classification when used with the one-vs-all approach.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 9,
      "context" : "Raykar et al [15] look at training binary classification models with labels from a crowd with unknown annotator qualities.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "Mausam et al [4] look at multi-label classification for taxonomy creation from the crowd.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 4,
      "context" : "Deng et al [6] look at selecting the instance to be given to a set of crowd-workers.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 12,
      "context" : "adapted to learning from the labels provided by crowd annotators [18].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "Bibtex: The Bibtex dataset [10] was released as part of the ECML-PKDD 2008 Discovery Challenge.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "Enron: The Enron dataset [21] is a collection of emails for which a set of pre-defined categories are to be assigned.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "Yeast: The Yeast dataset [8] contains a set of genes which may be associated with several functional classes.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "Scene: The Scene dataset [3] is a dataset of images.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "In Table 1 we compare the performance of our non-annotator model vs other methods such as RAKel, Monte Carlo Classifier Chains (MCC) [16], Binary Relevance Method - Random Subspace (BRq) [17], Bayesian Chain Classifiers (BCC) [24] and SLDA.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "In Table 1 we compare the performance of our non-annotator model vs other methods such as RAKel, Monte Carlo Classifier Chains (MCC) [16], Binary Relevance Method - Random Subspace (BRq) [17], Bayesian Chain Classifiers (BCC) [24] and SLDA.",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 18,
      "context" : "In Table 1 we compare the performance of our non-annotator model vs other methods such as RAKel, Monte Carlo Classifier Chains (MCC) [16], Binary Relevance Method - Random Subspace (BRq) [17], Bayesian Chain Classifiers (BCC) [24] and SLDA.",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 18,
      "context" : "BCC [24] is a probabilistic method which constructs a chain of classifiers by modeling the dependencies between the classes using a bayesian network.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "This observation is consistent with Occam’s razor [2].",
      "startOffset" : 50,
      "endOffset" : 53
    } ],
    "year" : 2016,
    "abstractText" : "Multi-label classification is a common supervised machine learning problem where each instance is associated with multiple classes. The key challenge in this problem is learning the correlations between the classes. An additional challenge arises when the labels of the training instances are provided by noisy, heterogeneous crowdworkers with unknown qualities. We first assume labels from a perfect source and propose a novel topic model where the present as well as the absent classes generate the latent topics and hence the words. We non-trivially extend our topic model to the scenario where the labels are provided by noisy crowdworkers. Extensive experimentation on real world datasets reveals the superior performance of the proposed model. The proposed model learns the qualities of the annotators as well, even with minimal training data.",
    "creator" : "TeX"
  }
}