{
  "name" : "1708.07227.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Proportionate gradient updates with PercentDelta",
    "authors" : [ "Sami Abu-El-Haija" ],
    "emails" : [ "haija@google.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Weights of Deep Neural Networks (DNNs) are commonly randomly initialized. The j-th layer’s weight matrix Wj is initialized from a distribution that is generally conditioned on the shape of Wj .\nFeed-forward DNNs generally consist of a series of matrix multiplications and element-wise activation functions. For example, input vector x0 is transformed to the output vector xL by an L-layer neural network hθ(x0) = xL, depicted as:\nx0\nW0\n× σ\nW1\n× σ . . .\nWL−1\n× σ xL\nWhere × is a matrix-multply operator, σ is an element-wise activation (e.g. logistic or ReLu). The DNN parameters θ = {W0,W1, . . . ,WL−1} are optimized by a training algorithm, such as Stochastic Gradient Descent (SGD), which iteratively applies gradient updates:\nW (t+1) j := W (t) j − η γ(t)\n∂J\n∂Wj , (1)\nwhere W (t)j is the value of the j-th layer weight matrix Wj at timestep t, η ∈ R is the learning rate, the decay function γ : R→ [0, 1], generally decreases with time t, and ∂J∂Wj is the partial gradient of training objective J w.r.t. Wj , evaluated at time t for a batch of training examples. For notational convenience, we define the delta SGD as: ∆SGDW = η γ(t) ∂J∂W\nar X\niv :1\n70 8.\n07 22\n7v 1\n[ cs\n.L G\n] 2\n4 A\nug 2\n01 7\nUnder usual circumstances, the magnitudes of gradients can widely vary across layers of a DNN, causing some trainable tensors to change much slower than others, slowing down overall training. Several proposed methods metigate this problem, including utilizing: per-parameter adaptive learning rate such as AdaGrad (Duchi et al., 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al., 2016); and intelligent initialization schemes such as xavier’s (Glorot and Bengio, 2010). These methods heuristically attack the disproportionate training problem, which we justify in section 2. In this paper, we propose to directly enforce proportionate training of layers. Specifically, We propose a gradient update rule that moves every weight tensor in the direction of the gradient, but with a magnitude that changes the tensor value by a relative amount. We use the same relative amount across all layers, to train them all at the same speed.\nThe remainder of the paper is organized as follows. In Section 2 we illustrate the disproportionate training phenomena on a (toy) hypothetical 4-layer neural network, by expanding gradient terms using BackProp Rumelhart et al. (1986). In Section 3, we summarize related work. Then, we introduce our algorithm in Section 4. We show experimental results on MNIST in Section 5. In Section 6, we discuss where PercentDelta could be useful and potential future direction. Finally, we conclude our findings in Section 7."
    }, {
      "heading" : "2 Disproprtionate Training",
      "text" : "We use a toy example to illustrate disproportionate training across layers. Assume a 4-layer network with trainable weight matrices {W0,W1,W2,W3} and output vector x4. The gradient of the objective w.r.t. output vector x4 can be directly calculated from the data e.g. using cross-entropy loss. We write-down the gradient of the objective J w.r.t. the last layer’s weight matrix W3 as:\n∂J\n∂W3 =\n[ σ′(W3 × x3) ◦ ∂J\n∂x4\n] × xT3 , (2)\nwhere σ′() is the derivative of σ() w.r.t. its input, and ◦ is the Hadamard product. We also write-down the gradient w.r.t. W1 and W2, then expand the expressions using the Back Propagation Algorithm (Rumelhart et al., 1986):\n∂J\n∂W2 = σ′(W2 × x2) ◦ ∂J∂x3︸︷︷︸ = × xT2\n= σ′(W2 × x2) ◦ ︷ ︸︸ ︷[ WT3 × [ σ′(W3 × x3) ◦ ∂J\n∂x4 ]]× xT2 ∂J\n∂W1 = σ′(W1 × x1) ◦ ∂J∂x2︸︷︷︸ = × xT1\n= σ ′(W1 × x1) ◦ ︷ ︸︸ ︷WT2 × σ′(W2 × x2) ◦ [ WT3 × [ σ′(W3 × x3) ◦ ∂J ∂x4 ]] ︸ ︷︷ ︸\n=∂J/∂x3\n  × x T 1\nNote the following in the above equations: First, the derivatives ∂J∂W3 , ∂J ∂W2 , ∂J∂W1 look similar. In fact, they are almost sub-expressions of one another, with the exception of the right-most row-vector xTj . Second, all quantities in brackets are column-vectors. The gradient matrix ∂J∂Wj is determined by an outer product of a column-vector (in brackets) times a row-vector xTj .\nWhat can we conclude about the magnitudes (e.g. the L1 norm) of ∂J∂W3 , ∂J ∂W2 , ∂J∂W1 ? Each ∂J ∂Wj is calculated using three types of multiplicands: σ′(.), WT. , and x T j . Therefore, the magnitudes of ∂J ∂Wj are affected by:\n1. The value of the derivative of σ′(), which is evaluated element-wise. If σ′() is generally less than 1, then we can expect the gradient to be smaller for earlier layers than later ones, since they are multiplied by σ′() more times. If it is generally greater than 1, then we can expect the gradient to be larger for earlier layers. For very deep networks (recurrent or otherwise), the former situation can cause the gradients to vanish, while the latter can cause the gradients to explode. ReLu mitigates this problem, as its derivative = 1, in locations where input is positive.\n2. The magnitude of W ’s. In practice, all W ’s are initialized from the same distribution. If the L1-norms of rows in WTj are less than 1, then we should expect ||WTj × σ′(z)|| < ||σ′(z)||, yielding smaller gradients for earlier layers. Otherwise, if the row L1 norms are greater than 1, then earleir layers should receive larger gradient magnitudes. Weight normalization (Salimans and Kingma, 2016) and intelligent initialization schemes (e.g. Glorot and Bengio, 2010) can mitigate this problem.\n3. The norm of the row-vector xj . If in the forward pass, the activations consistently grow (e.g. unbounded activation) with subsequent layers, then later layers will receive larger gradients. BatchNorm (Ioffe and Szegedy, 2015) and LayerNorm (Ba et al., 2016) mitigate this problem."
    }, {
      "heading" : "3 Related Work",
      "text" : ""
    }, {
      "heading" : "3.1 Adaptive Gradient",
      "text" : "Duchi et al. (2011) proposed AdaGrad. A training algorithm that keeps a cumulative sum-of-squared gradients (i.e. second moment of gradients):\nS (t) j = S (t−1) j +\n( ∂J\n∂Wj ◦ ∂J ∂Wj\n) , (3)\nThen divides (element-wise) the gradient by the square-root of the sum:\nW (t+1) j := W (t) j − η γ(t)\n( ∂J ∂Wj ◦ ( S (t) j )−1/2) , (4)\nor equivalently ∆AdaGradWj = η γ(t) ∂J∂Wj ◦ ( S (t) j )−1/2 , where the (.)−1/2 power operator is applied element-wise. In essense, if some layer receives large gradients, then they will be normalized to smaller gradients through the division. However, a weakness in AdaGrad is that at some point, the S will grow too large, effectively making (S)−1/2 ≈ 0 and therefore slowing or halting training. Ba and Kingma (2015) propose Adam, which keeps exponential decaying sums, of gradients and of square gradients, respectively known as first and second moments. Adam offers two benefits over AdaGrad. First, its decaying sums should not grow to infinity and therefore training should not halt. Second, the exponential-decay averaging was shown to speed up training (Momentum, Sutskever et al., 2013). For details, we point readers to the Adam paper (Ba and Kingma, 2015)."
    }, {
      "heading" : "3.2 LARS",
      "text" : "You et al. (2017) propose to normalize every layer’s gradients by the ratio of L2 norms of the parameter and the gradient. Namely, they propose the gradient update rule:\n∆LARSWj = η γ(t)\n ∣∣∣∣∣∣W (t)j ∣∣∣∣∣∣ 2\n||(∂J/∂Wj)||2  ∂J ∂Wj , (5)\nwhich effectively normalizes the gradient ∂J∂Wj to be unit-norm. This setup is very similar to ours, with two differences: First, our norm operator is L1 rather than L2. Second, our norm operator is\napplied outside the division (i.e. our division is element-wise). Our proposed algorithm, PercentDelta, was used to train our work (Abu-El-Haija et al., 2017), before we where aware of the work of (You et al., 2017). In (Abu-El-Haija et al., 2017), PercentDelta gave us 1% improvement over Adam, over all datasets. Nonetheless, we only discuss MNIST experiments in this paper, and leave graph embedding experiments for follow-up work."
    }, {
      "heading" : "4 PercentDelta",
      "text" : "For every weight matrix Wj , and similarily bias vectors, we propose the gradient update rule:\n∆PercentDeltaWj = η γ(t)  size(Wj)∣∣∣∣∣∣∣∣ (∂J/∂Wj)W (t)j ∣∣∣∣∣∣∣∣ 1  ∂J∂Wj , (6) where scalar size(W ) ∈ Z+ is the number of entries1 in W , and the scalar:\nsize(Wj)∣∣∣∣∣∣∣∣ (∂J/∂Wj)W (t)j ∣∣∣∣∣∣∣∣ 1\n(7)\nnormalizes the gradient ∂J∂Wj of the j-th layer, so that it is more proportional to its current parameter value W (t)j , and ||.||1 is the L1-norm. We avoid division-by-zero errors by adding an epsilon to the denominator2. The divide operator within the L1 norm is applied element-wise and the outer divide operator is scalar. Note that the “gradient multiplier” fraction (Equation 7) is ∈ R+. Therefore, we only changes the gardient’s magnitude, but not its direction.\nNow, we justify Equation 6. Assume that Ws is scalar. Therefore, size(Ws) = 1 and the L1-norm becomes an absolute value. Equation 6 simplifies to:\nW (t+1)s := W (t) s − η γ(t) ∣∣∣∣∣ W (t)s(∂J/∂Ws) ∣∣∣∣∣ ∂J∂Ws\n:= W (t)s − η γ(t) ∣∣∣W (t)s ∣∣∣ sign( ∂J∂Ws ) ,\n(8)\nwhere sign(z) = 1 if z > 0 and = −1 if z < 0. Therefore, Ws will change with a quantity proportional to its current value. In particular, the scalar η γ(t) determines the percentage at which W changes at timestep t, giving rise to the name: PercentDelta. For example, if we setup a decay schedule on γ(t), such that ηγ(t) changes during training from 10% to 0.01%, then the network parameters will change at a rate of 10% in early mini-batches, and gradually decrease their PercentDelta updates to 0.01% towards end of training, consistently across all layers, regardless of the current parameter value or the gradient value, which are influenced by choice of activation function, initialization, and network architecture."
    }, {
      "heading" : "5 Experiments",
      "text" : "We run experiments on MNIST. We use the same model for all experiments and fix the batch size to 500. Our model code is copied from the TensorFlow tutorial3, and contains 4 trainable layers: 2D Convolution, Max-pooling, 2D Convolution, Max-pooling, Fully-connected, Fully-connected. The convolutional layers contain trainable tensors with dimensions: (5, 5, 1, 32), (5, 5, 32, 64), and bias vectors. The Fully-connected layers contain trainable tensors with dimensions: (3136, 1024), (1024, 10), and bias vectors. The model is trained with Softmax loss, uses ReLu for hidden activations, and does not use BatchNorm.\n1size(W ) is the product of W ’s dimensions. For a 2-D matrix, it is equal to # rows × # columns, for a vector, it is equal to its length, etc. We also define size(scalar) = 1.\n2In practice, rather than converting a b to a b+ , we use a b+ ·sign(b) , as our goal is to push b slightly away from\nzero while keeping its sign. 3https://www.tensorflow.org/get_started/mnist/pros"
    }, {
      "heading" : "5.1 MNIST Network Gradient Magnitudes",
      "text" : ""
    }, {
      "heading" : "5.2 MNIST Test Accuracy Curves",
      "text" : "We want to measure how fast can PercentDelta train MNIST. We compare training speed with other algorithms, including per-parameter adaptive learning rate algorithms, AdaGrad (Duchi et al., 2011) and Adam (Ba and Kingma, 2015), as well as a recent algorithm with similar spirit, LARS (You et al., 2017), which also normalizes the gradient w.r.t. a weight tensor, by the current value of the weight tensor."
    }, {
      "heading" : "6 Discussion",
      "text" : ""
    }, {
      "heading" : "6.1 Situations where PercentDelta is useful",
      "text" : "While PercentDelta has outperforms other training algorithms on 4-layer MNIST, the space of modelsdatasets is enormous we leave it as future work to try PercentDelta under various models and datasets. Nonetheless, we speculate that PercentDelta (and similarily, LARS, You et al. (2017)) would be very useful in the following scenarios:\n1. Learning Embeddings. Consider the common setup of feeding word embeddings (or graph embeddings, Abu-El-Haija et al., 2017) into a shared Neural Network and jointly learning the embeddings and Neural Network for an upstream objective. In this setup, a certain\nembedding vector is only affected by a fraction of training examples, while the shared network parameters are affected by all training examples. The sum of gradients w.r.t. the shared network parameters over all training examples can be disproportionately larger than embedding gradients. PercentDelta ensures that the shared network is not being updated much faster than the emebddings.\n2. Soft-Attention Models on Bag-of-Words. It is common to convert from variable-length bag-of-words (xj [1],xj [2], . . . ) into fixed-length representation by a convex combination: xj+1 := ∑ i αixj [i], which can then be used for an upstream objective (e.g. event detection\nin videos, Ramanathan et al., 2016). Here, αi can be the i-th position of the softmax over all Words. The parameters of the softmax model would receive gradients from all words. PercentDelta ensures that, the otherwise disproportionately large, gradient updates of the softmax model are proportional to the remainder of the network.\n3. Matrix Factorization Models. For example, Koren et al. (2009) propose to factorize a user-movie rating matrix R ∈ Ru×m into:\nR ≈WU ×WM + bU × ~1T + ~1× bM + b,\nwhere WU ∈ Ru×d and WM ∈ Rd×m are the user and movie embedding matrix; d is the size of the latent-space; bU ∈ Ru and bM ∈ Rm are the user and movie bias vectors, and b ∈ R is the global bias scalar. In this setup, b would receive very large sum-of-gradients, and PercentDelta can ensure that all parameters are training at the same speed."
    }, {
      "heading" : "6.2 Hyperparamters and Decay Function",
      "text" : "It seems that PercentDelta has many knobs to tune. However, we can fix η to some value and only change γ(t) as their product determines the effective rate of change across all layers’ trainable tensors. We can set γ(t) to constant decay: γ(t) = 1− t×m, (9) where 0 < m << 1 determines the decay slope. In addition, we can ensure that γ(t) > 0 to allow training continue indefinitely, by modifying Equation 9 to:\nγ(t) = max(β, 1− t×m), (10)\nwhere β can be set to a small positive value, such as 0.01. In this case, if we fix η = 0.03, then we are effectively changing each trainable tensor by 3% for every training batch initially, then gradually annealing this change-rate to 0.03% after 1m steps.\nMore importantly, we feel that η and γ(t) are a function of the dataset, and not the model. Experimentally, we observe the algorithm is insensitive to the choices ofm and η as long as they are “reasonable” (i.e. removing diverging setups that can be quickly detected). However, we do not yet have a formula to automatically set them. Nonetheless, with a wide range of η and m, we experimentally show on MNIST that PercentDelta beats all training algorithms, given the same budget of training steps."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We propose an algorithm that trains layers of a neural network, all at the same speed. Our algorithm, PercentDelta, is a simple modification over standard Gradient Descent. It divides the gradient w.r.t. a trainable tensor over the mean of ||gradient / tensor||1. The division over mean L1-norm is scalar, and only changes the gradient’s magnitude but not its direction. Effectively, this updates the L1 norm of trainable layers, all at the same rate. We recommend a linear decaying change-rate schedule. Our modified gradients can be passed through a standard momentum accumulator (Sutskever et al., 2013). Overall, we show experimentally that our algorithm puts an upper envelop on all training algorithms, reaching higher test accuracy with fewer steps."
    } ],
    "references" : [ {
      "title" : "Learning edge representations via lowrank asymmetric projections",
      "author" : [ "S. Abu-El-Haija", "B. Perozzi", "R. Al-Rfou" ],
      "venue" : "ACM International Conference on Information and Knowledge Management (CIKM).",
      "citeRegEx" : "Abu.El.Haija et al\\.,? 2017",
      "shortCiteRegEx" : "Abu.El.Haija et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "J. Ba", "D. Kingma" ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Ba and Kingma,? 2015",
      "shortCiteRegEx" : "Ba and Kingma",
      "year" : 2015
    }, {
      "title" : "Layer normalization",
      "author" : [ "J.L. Ba", "J.R. Kiros", "G.E. Hinton" ],
      "venue" : "arxiv.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS’10).",
      "citeRegEx" : "Glorot and Bengio,? 2010",
      "shortCiteRegEx" : "Glorot and Bengio",
      "year" : 2010
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "Journal of Machine Learning Research (JMLR).",
      "citeRegEx" : "Ioffe and Szegedy,? 2015",
      "shortCiteRegEx" : "Ioffe and Szegedy",
      "year" : 2015
    }, {
      "title" : "Matrix factorization techniques for recommender systems",
      "author" : [ "Y. Koren", "R.M. Bell", "C. Volinsky" ],
      "venue" : "IEEE Computer.",
      "citeRegEx" : "Koren et al\\.,? 2009",
      "shortCiteRegEx" : "Koren et al\\.",
      "year" : 2009
    }, {
      "title" : "Detecting events and key actors in multi-person videos",
      "author" : [ "V. Ramanathan", "J. Huang", "S. Abu-El-Haija", "A. Gorban", "K. Murphy", "L. Fei-Fei" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Ramanathan et al\\.,? 2016",
      "shortCiteRegEx" : "Ramanathan et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning representations by backpropagating errors",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : "Nature.",
      "citeRegEx" : "Rumelhart et al\\.,? 1986",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1986
    }, {
      "title" : "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
      "author" : [ "T. Salimans", "D.P. Kingma" ],
      "venue" : "Neural Information Processing Systems.",
      "citeRegEx" : "Salimans and Kingma,? 2016",
      "shortCiteRegEx" : "Salimans and Kingma",
      "year" : 2016
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton" ],
      "venue" : "International Conference on Machine Learning.",
      "citeRegEx" : "Sutskever et al\\.,? 2013",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2013
    }, {
      "title" : "Scaling sgd batch size to 32k for imagenet training",
      "author" : [ "Y. You", "I. Gitman", "B. Ginsburg" ],
      "venue" : "UC Berkeley Technical Report.",
      "citeRegEx" : "You et al\\.,? 2017",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Several proposed methods metigate this problem, including utilizing: per-parameter adaptive learning rate such as AdaGrad (Duchi et al., 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al.",
      "startOffset" : 119,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al., 2016); and intelligent initialization schemes such as xavier’s (Glorot and Bengio, 2010).",
      "startOffset" : 161,
      "endOffset" : 178
    }, {
      "referenceID" : 4,
      "context" : ", 2016); and intelligent initialization schemes such as xavier’s (Glorot and Bengio, 2010).",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : ", 2011) or Adam (Ba and Kingma, 2015); normalization operators such as BatchNorm (Ioffe and Szegedy, 2015), WeightNorm (Salimans and Kingma, 2016), or LayerNorm (Ba et al., 2016); and intelligent initialization schemes such as xavier’s (Glorot and Bengio, 2010). These methods heuristically attack the disproportionate training problem, which we justify in section 2. In this paper, we propose to directly enforce proportionate training of layers. Specifically, We propose a gradient update rule that moves every weight tensor in the direction of the gradient, but with a magnitude that changes the tensor value by a relative amount. We use the same relative amount across all layers, to train them all at the same speed. The remainder of the paper is organized as follows. In Section 2 we illustrate the disproportionate training phenomena on a (toy) hypothetical 4-layer neural network, by expanding gradient terms using BackProp Rumelhart et al. (1986). In Section 3, we summarize related work.",
      "startOffset" : 17,
      "endOffset" : 956
    }, {
      "referenceID" : 8,
      "context" : "W1 and W2, then expand the expressions using the Back Propagation Algorithm (Rumelhart et al., 1986):",
      "startOffset" : 76,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "Weight normalization (Salimans and Kingma, 2016) and intelligent initialization schemes (e.",
      "startOffset" : 21,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "BatchNorm (Ioffe and Szegedy, 2015) and LayerNorm (Ba et al.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "BatchNorm (Ioffe and Szegedy, 2015) and LayerNorm (Ba et al., 2016) mitigate this problem.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "1 Adaptive Gradient Duchi et al. (2011) proposed AdaGrad.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "For details, we point readers to the Adam paper (Ba and Kingma, 2015).",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "Ba and Kingma (2015) propose Adam, which keeps exponential decaying sums, of gradients and of square gradients, respectively known as first and second moments.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "2 LARS You et al. (2017) propose to normalize every layer’s gradients by the ratio of L2 norms of the parameter and the gradient.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "Our proposed algorithm, PercentDelta, was used to train our work (Abu-El-Haija et al., 2017), before we where aware of the work of (You et al.",
      "startOffset" : 65,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : ", 2017), before we where aware of the work of (You et al., 2017).",
      "startOffset" : 46,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "In (Abu-El-Haija et al., 2017), PercentDelta gave us 1% improvement over Adam, over all datasets.",
      "startOffset" : 3,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "We compare training speed with other algorithms, including per-parameter adaptive learning rate algorithms, AdaGrad (Duchi et al., 2011) and Adam (Ba and Kingma, 2015), as well as a recent algorithm with similar spirit, LARS (You et al.",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : ", 2011) and Adam (Ba and Kingma, 2015), as well as a recent algorithm with similar spirit, LARS (You et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : ", 2011) and Adam (Ba and Kingma, 2015), as well as a recent algorithm with similar spirit, LARS (You et al., 2017), which also normalizes the gradient w.",
      "startOffset" : 96,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "Nonetheless, we speculate that PercentDelta (and similarily, LARS, You et al. (2017)) would be very useful in the following scenarios:",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "Top-to-bottom: PercentDelta (our algorithm), AdaGrad (Duchi et al., 2011), Adam (Ba and Kingma, 2015), LARS (You et al.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : ", 2011), Adam (Ba and Kingma, 2015), LARS (You et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : ", 2011), Adam (Ba and Kingma, 2015), LARS (You et al., 2017), and the last row shows the best performer from all algorithms.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "For example, Koren et al. (2009) propose to factorize a user-movie rating matrix R ∈ Ru×m into:",
      "startOffset" : 13,
      "endOffset" : 33
    } ],
    "year" : 2017,
    "abstractText" : "Deep Neural Networks are generally trained using iterative gradient updates. Magnitudes of gradients are affected by many factors, including choice of activation functions and initialization. More importantly, gradient magnitudes can greatly differ across layers, with some layers receiving much smaller gradients than others. causing some layers to train slower than others and therefore slowing down the overall convergence. We analytically explain this disproportionality. Then we propose to explicitly train all layers at the same speed, by scaling the gradient w.r.t. every trainable tensor to be proportional to its current value. In particular, at every batch, we want to update all trainable tensors, such that the relative change of the L1-norm of the tensors is the same, across all layers of the network, throughout training time. Experiments on MNIST show that our method appropriately scales gradients, such that the relative change in trainable tensors is approximately equal across layers. In addition, measuring the test accuracy with training time, shows that our method trains faster than other methods, giving higher test accuracy given same budget of training steps.",
    "creator" : "LaTeX with hyperref package"
  }
}