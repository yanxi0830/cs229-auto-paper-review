{
  "name" : "1706.04638.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "thomas.frerix@tum.de", "moellenh@in.tum.de", "michael.moeller@uni-siegen.de", "cremers@tum.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 6.\n04 63\n8v 1\n[ cs\n.L G\n] 1\n4 Ju"
    }, {
      "heading" : "1 Introduction",
      "text" : "In recent years neural networks have gained considerable attention in solving difficult correlation tasks such as classification in computer vision [8] or sequence learning [20] and as building blocks of larger learning systems [17]. Training neural networks is accomplished by optimizing a nonconvex, possibly nonsmooth, nested function of the network parameters. Since the introduction of stochastic gradient descent (SGD) [15, 1], several more sophisticated optimization methods have been studied. One such class is that of quasi-Newton methods, as for example the comparison of LBFGS with SGD in [9], Hessian-free approaches [10], and the Sum of Functions Optimizer in [18]. The latter uses BFGS to approximate the Hessian of every subfunction of a sum of differentiable functions, combining the computational efficiency of SGD with tractable second-order information. Several works consider specific properties of energy landscapes of certain deep learning models such as frequent saddle points [4] and well-generalizable local optima [3]. Among the most popular optimization methods in currently used deep learning frameworks are momentum based improvements of classical SGD, notably Nesterov’s Accelerated Gradient [13, 19], and the Adam optimizer [7], which uses estimates of first and second order moments of the gradients for parameter updates.\nNevertheless, the optimization of these models remains challenging, as learning with SGD and its variants requires careful weight initialization and a sufficiently small learning rate in order to yield\n∗contributed equally.\na stable and convergent algorithm. Moreover, SGD often has difficulties in propagating a learning signal deeply into a network, commonly referred to as the vanishing gradient problem [6].\nRecently, the authors of [2] have tackled the problem of optimizing the nested objective function by explicitly introducing the network activations as variables of the optimization, also known as the method of auxiliary coordinates (MAC). They use layer-wise constraints to couple the activation variables with the network parameters and propose a quadratic penalty method to solve the constrained problem. Closely related to the previous approach, Taylor et al. [21] introduce additional auxiliary variables to further split linear and nonlinear transfer between layers.\nIn this work we draw a connection between the penalty formulations [2, 21] and the classical backpropagation algorithm [16]. In particular, we show that the backpropagation algorithm can be interpreted as a method alternating between two steps. First, a forward pass that resets the auxiliary activation variables in accordance with the previously computed weights. Secondly, an ordered sequence of gradient descent steps on a quadratic penalty energy.\nInterestingly, for many common network architectures the updates arising in the second step still have a closed form solution if the explicit gradient descent step is replaced by an implicit gradient step (also known as proximal step, for the definition see (8), Section 4.1). We investigate different combinations of explicit and implicit optimization subproblems and demonstrate that implicit gradient steps can significantly improve optimization progress."
    }, {
      "heading" : "2 Notation and model",
      "text" : "We denote the Euclidean norm for vectors and the Frobenius norm for matrices by || · ||, induced by an inner product 〈·, ·〉. We use the gradient symbol ∇ to denote the transpose of the Jacobian matrix, such that the chain rule applies in the form “inner derivative times outer derivative”. For all computations involving matrix-valued functions and their gradient/Jacobian, we uniquely identify all involved quantities with their vectorized form by flattening matrices in a column-first order.\nThe method we propose in this work is suitable for optimizing a parametrized function composition:\nf(θ) = fK (θK , fK−1 (θK−1, · · · (θ2, f1 (θ1)) · · · )) . (1) Note that almost any common neural network architecture can be phrased in the form of (1).\n. . .\n. . .\n. . .\nLy\nn0\nX\nn1\nz1 φ\nn1\na1 σ\nn2\nz2 φ\nnL−2\nzL−2\nnL−2\naL−2 σ φ\nR\nnl . The targets for the batch are denoted as y ∈ RnL−1×N and we use the convention a0 := X . Our nonlinearities are either rectified linear units (ReLU), σ(x) = max(0, x), or sigmoid units, σ(x) = 1/(1 + exp(−x)). For convolutional neural networks we also combine the ReLU with a max pooling nonlinearity and we make our choice explicit where needed. When writing σ(zl) for some matrix zl ∈ Rnl×N it is to be understood elementwise. The final objective J is a nested function consisting of the prediction loss Ly : RnL−1×N → R evaluated at layer L. More precisely, let us define the function φ(W, b, a) = Wa+b1, for matricesW and a, a column vector b, and a row vector 1 ∈ R1×N . Note that a convolution is also a linear operator which is why it can be written as\nAlgorithm 1 - Penalty formulation of backprop.\nInput: Current parameters (W , b)k . // Forward pass. for l = 1 to L− 2 do z (k) l = φ(W (k) l , b (k) l , a (k) l−1), // a0 = X .\na (k) l = σ(z (k) l ).\nend for // Backward pass by τ -gradient steps on (4). a© grad. step on E wrt. (WL−1, bL−1, aL−2). for l = L− 2 to 1 do\nb© grad. step on E wrt. zl, c© grad. step on E wrt. (Wl, bl, al−1).\nend for Output: New parameters (W , b)k+1.\nAlgorithm 2 - Generalization of backprop.\nInput: Current parameters (W , b)k . // Forward pass. for l = 1 to L− 2 do z (k) l = φ(W (k) l , b (k) l , a (k) l−1), // a0 = X .\na (k) l = σ(z (k) l ).\nend for // Perform minimization steps on (4). a©min. step on E wrt. (WL−1, bL−1, aL−2). for l = L− 2 to 1 do\nb© min. step on E wrt. zl, c© min. step on E wrt. (Wl, bl, al−1).\nend for Output: New parameters (W , b)k+1.\na matrix-vector product. Therefore, except for the matrix W being constrained to be of a particular structure, the form of the problem remains the same for convolutional layers.\nUsing the above notation we can express the final objective of the fully connected feed-forward network analogous to (1) as\nJ(W , b;X, y) = Ly(φ(WL−1, bL−1, σ(φ(· · · , σ(φ(W1, b1, X)) · · · )). (2)\nFormally, the functions σ and φ map between spaces of different dimensions depending on the layer. However, to keep the presentation clean we do not state this dependence explicitly. Figure 1 illustrates our notation for the fully-connected network architecture."
    }, {
      "heading" : "3 Penalty formulation of backpropagation",
      "text" : "The gradient descent iteration on a nested function J(W , b;X, y) like (2),\n(W , b)k+1 = (W , b)k − τ∇J(W k, bk;X, y), (3)\nis commonly implemented using the backpropagation algorithm [16]. As the basis for the proposed proximal optimization method, we will now derive a connection between the classical backpropagation algorithm and quadratic penalty functions of the form\nE(W , b,a, z) = Ly(φ(WL−1, bL−1, aL−2)) + L−2∑\nl=1\nγ 2 ‖σ(zl)− al‖2 + ρ 2 ‖φ(Wl, bl, al−1)− zl‖2.\n(4)\nUnder mild conditions the limit ρ, γ → ∞ leads to the convergence of the sequence of minimizers of E to the minimizer of J , see [14, Theorem 17.1], also [2, Theorem B.1-B.4].\nOne of the main insights of this paper, which motivates a novel class of optimization algorithms, is that the iteration shown in Algorithm 1 of forward passes followed by a sequential gradient descent on the penalty function E is equivalent to the classical gradient descent iteration.\nProposition 1. For ρ = γ = 1/τ and (W , b)k as the input to Algorithm 1, its output (W , b)k+1 satisfies (3), i.e., Algorithm 1 computes one gradient descent iteration on J .\nProof. See appendix.\nObserve that although the above proposition refers to our specific setting no assumptions on σ and φ (except their differentiability) are made in the proof of Proposition 1."
    }, {
      "heading" : "4 Generalizing backpropagation",
      "text" : "The interpretation of Proposition 1 leads to the natural idea of replacing the explicit gradient steps a©, b© and c© in Algorithm 1 with other – possibly more powerful – minimization steps. For example, for l = 1 step c© of Algorithm 1 is\n(W1, b1) k+1 = (W1, b1) k − τρ∇f((W1, b1)k) (5) for f(W, b) = 12‖φ(W, b,X)− z1‖2. As we will discuss in the next subsection, replacing Equation (5) by an implicit gradient step\n(W1, b1) k+1 = (W1, b1) k − τρ∇f((W1, b1)k+1) (6) has several advantageous properties and therefore motivates the general concept presented in Algorithm 2, i.e., alternating between forward passes of the network and sequential minimization steps on the penalty function (4). In the following section we explore novel minimization strategies for a©, b© and c© in Algorithm 2 by replacing the explicit gradient descent steps with implicit steps known as proximal mappings."
    }, {
      "heading" : "4.1 Implicit gradient steps",
      "text" : "The proximal mapping of a function f : Rn → R is defined as the following [12]:\nproxτf (y) := argmin x∈Rn\nf(x) + 1\n2τ ||x− y||2. (7)\nBy rearranging the optimality conditions to (7) and taking y = xk, it can be interpreted as an implicit gradient step evaluated at the new point xk+1:\nxk+1 := argmin x∈Rn\nf(x) + 1\n2τ ||x− xk||2 = xk − τ∇f(xk+1). (8)\nThe advantage of such proximal steps is that in contrast to explicit gradient descent, the update scheme (8), also known as the proximal point algorithm [11], is unconditionally stable. Unconditional stability means that (8) monotonically decreases the energy f for any τ > 0, as by definition of xk+1 it holds that f(xk+1) + 12τ ||xk+1 − xk||2 ≤ f(xk). Within our model, we can recover the proximal point algorithm (8) as proximal optimization of the prediction loss (4) for a 2-layer network.\nNote that explicit gradient steps pose severe restrictions on the allowed step size τ : Even for a convex, twice continuously differentiable, L -smooth function f : Rn → R, the convergence of the gradient descent algorithm can only be guaranteed for step sizes 0 < τ < 2/L . The Lipschitz constant L of the gradient∇f is in this case equal to the largest eigenvalue of the Hessian H . For the example of the first layer shown in (5), the Hessian is H = [X 1] [X 1] ⊤ , which for the CIFAR-10 dataset has a largest eigenvalue of 6.7 · 106. Similar problems also arise in other layers where poorly conditioned matrices al pose severe restrictions for guaranteeing the energy of the subproblem to decrease.\nNote that the instability of the substeps does not need to imply the instability of the gradient descent algorithm. Nevertheless, proximal mappings yield unconditionally stable subproblems, which motivates us to use proximal steps in Algorithm 2.\nIn many cases, the full proximity operator in steps a© and c© is difficult to compute exactly due to the nonconvex product of W and a in the function φ. Therefore, we use one iteration of coordinate descent in the two blocks (W, b) and a as an approximate solution. For the two block coordinate descent steps, we have the option to take proximal steps, or make the updates explicit. We discuss the proximal steps one can possibly take on each of the variables (W, b), a, and z in the next subsection.\nOf particular importance for our proposed scheme is the proximity operator of the squared ℓ2 norm\nprox ζ 2 ‖T ·−c‖2 2 (d) = argmin x\n1 2 ‖x− d‖2 + ζ 2 ‖T · −c‖22 = (I + ζT ∗T )−1(d+ ζT ∗c), (9)\nfor different linear operators T . In the above T ∗ denotes the adjoint operator (which would be the transpose in the case of matrices). For any linear operator T , equation (9) merely requires the solution of a linear system of equations."
    }, {
      "heading" : "4.2 Computing the proximal steps",
      "text" : "In this subsection we discuss the proximal operators for taking implicit gradient steps with respect to our variablesW , b, a, and z. For the sake of simplicity, we limit the discussion to the fully connected case. Please note that convolutional layers can be treated in a similar fashion, which we detail in the appendix.\n4.2.1 Proximal steps in (W, b)\nLast layer In the last layer one needs to solve\n(W (k+1) L−1 ,b (k+1) L−1 ) = argmin W,b Ly(Wa(k)L−2 + b1) +\n1\n2τ ||W −W (k)L−1||2 +\n1\n2τ ||b− b(k)L−1||2.\n(10) Depending on the choice of the loss function Ly , this leads to different subproblems. For the square loss Ly(z) = 12 ||z − y||2, the proximity operator has a closed form solution given by (9). We detail the update formulas in the appendix. For Ly(z) = − ∑N\ni=1 ∑nL−1 j=1 yj,izj,i − log ∑nL−1 j=1 exp(zj,i), the proximity operator does not have\na simple, closed form solution. While in principle it can be solved using algorithms from convex optimization, we found that it is more efficient to simply take an explicit gradient step in this case.\nFirst and hidden layers In all other layers, one needs to determine\n(W (k+1) l ,b (k+1) l ) = argmin\nW,b\nρ 2 ||Wa(k)l−1 + b1− z (k+1) l ||2 + 1 2τ ||W −W (k)l ||2 + 1 2τ ||b− b(k)l ||2\n(11) with the convention a0 = X , which again resembles a quadratic proximity operator (9)."
    }, {
      "heading" : "4.2.2 Proximal steps in a",
      "text" : "Last layer Similar to the above case of (W, b) one needs to solve\na (k+1) L−2 = argmin a Ly(W (k+1)L−1 a+ b (k+1) L−1 1) +\nγ 2 ||a− a(k)L−2||2 + 1 2τ ||a− a(k)L−2||2, (12)\nwhere we substituted σ(z(k)L−2) = a (k) L−2 in the γ-penalty term. This is a valid substitution due to the forward pass. Again, for a square loss the above problem becomes an instance of (9), and we propose to take explicit steps for a softmax loss.\nFirst and hidden layers All layers except the last result in the problem\na (k+1) l−1 = argmin\na\n1 2 ‖W (k+1)l a+ b (k+1) l 1‖22 + γ 2 ||a− a(k)l−1||2 + 1 2τ ||a− a(k)l−1||2, (13)\nand again amount to an instance of (9)."
    }, {
      "heading" : "4.2.3 Proximal steps in z",
      "text" : "Taking a proximal step on E with respect to zl, i.e. taking implicit steps in b©, amounts to the nonconvex minimization problem\nz (k+1) l =argmin\nz\nγ 2 ||σ(z)− a(k+1)l ||2 + ρ 2 ||z(k)l − z||2 + 1 2τ ||z − z(k)l ||2, (14)\nwhere we substituted the variable z(k)l from the forward pass. Problem (14) decomposes into independent univariate problems which can be solved to global optimality for σ(x) = max(0, x) despite its nonconvexity. We state the explicit solution of (14) in the appendix."
    }, {
      "heading" : "4.3 Explicit vs. implicit steps",
      "text" : "Proposition 1 shows that Algorithm 2 with explicit gradient descent steps and γ = ρ = 1/τ yields the gradient descent iteration. Motivated by the more general scheme of Algorithm 2, we consider the following variants: 1. Explicit steps on all variables, 2. implicit steps on zl, explicit steps on (Wl, bl, al−1), 3. explicit steps on (zl, al−1), explicit steps on (Wl, bl), 4. explicit steps on zl, implicit steps on (Wl, bl, al−1), and 5. implicit steps on all variables.\nTo investigate their stability, we train a fully connected 256− 300− 100− 20 − 100− 300− 256 autoencoder with ReLU nonlinearities and square loss using the above update equations. We train on the USPS dataset with 2000 grayscale images of size 16×16 and set γ = ρ = 1/τ for all five variants.\nEach method is run with stepsizes τ ∈ {10, 1, 10−1, 10−2, 10−3, 10−4, 10−5, 10−6} and for four different initializations of the weights: A Gaussian distribution with variance 0.1, a uniform distribution of values in [−0.1, 0.1] as well as the adaptive initialization scheme suggested in [5], denoted as adaptive uniform and adaptive Gaussian respectively. Figure 2 illustrates the final energies of each combination after training for 20 seconds. For the case of adaptive uniform weight initialization, Figure 3 furthermore shows the decay of the training energy over the iterations using the optimal step size for each method.\nWhile the more implicit methods yield decaying energies for a much wider range of different step sizes, implicit steps do not necessarily yield the fastest decay of the energy: Taking implicit steps in (W, b, a) and explicit ones in z usually yields lower energies than the fully implicit method.\nMoreover, we can see that for adaptive initializations the method using implicit steps in (W, b) and explicit steps in (a, z) experienced the fastest convergence. Altogether, we therefore focus on this method in our further evaluation in Section 6."
    }, {
      "heading" : "5 Proximal backpropagation as a first-order oracle",
      "text" : "Algorithm 2 can be used as a gradient oracle for first-order methods (such as Adam [7]) or quasiNewton methods (such as SFO [18]), by using it to compute a direction dk+1 = (W , b)k+1 − (W , b)k instead of the usual gradient. Indeed, as seen in Proposition 1, for ρ = γ = 1τ and explicit\ngradient steps, one obtains the direction of steepest descent\nd k+1 = −τ∇(W ,b)J(W k, bk;X, y). (15)\nNote that in general it is not clear if d is a descent direction for the energy J , i.e., whether 〈∇J,d 〉 < 0. This property is often assumed in convergence proofs for gradient-based methods. Nevertheless, in practice we observed convergence over a wide range of parameters. For the experiments in this paper, we stick to SGD with Nesterov momentum µ, which is given as:\nv k+1 = µvk + dk+1, (W , b)k+1 = (W , b)k + dk+1 + µvk+1. (16)\nIn the following section, we will refer to SGD with Nesterov momentum as SGD. Ours will be (16), where dk+1 is computed using ProxProp (Algorithm 2 with explicit updates in the network activations a, z and proximal steps in the network parametersW, b)."
    }, {
      "heading" : "6 Numerical evaluation",
      "text" : "We evaluate our method in two representative experiments, a supervised visual learning problem on the CIFAR-10 dataset and an autoencoder on the USPS handwritten digit dataset."
    }, {
      "heading" : "6.1 CIFAR-10 supervised learning",
      "text" : "Fully connected network We trained a fully connected network with architecture 3072− 4000− 1000− 4000− 10, ReLU nonlinearities and cross-entropy + softmax loss. The weights are initialized according to the uniform distribution scheme described in [5] and a weight-decay of 10−6 was chosen. In Fig. 4, we compare SGD to the proposed method (ours). The momentum was set to µ = 0.95 and the batch size chosen as 250 for both methods. For the proposed method, we picked τ = 5 · 10−3 and the penalty parameters were chosen as ρ = 10/τ , γ = 2.5/τ . Since the proximal step for the softmax loss does not have a closed form solution (see Section 4.2.1), we performed a single explicit τ -gradient step to approximately solve the subproblem in (13). We further compare the proposed method to SFO [18] and used the publicly available MATLAB implementation2 with 20 subfunctions (minibatch size of 2250). As the memory requirement of SFO is linear in the minibatches, a larger number of subfunctions exceeded the available memory. All methods were implemented in MATLAB to have a comparable performance. From Fig. 4 we infer that the proposed method leads to lower energies in less time and iterations, and also to the overall lowest energy. The test accuracies reached by the different methods were 54% for SFO, 54% - 57% for SGD and 56% for ours.\nConvolutional neural network Our ProxProp algorithm can be used with any architecture with a linear transfer function to which we apply the proximal updates. To demonstrate our algorithm on a\n2https://github.com/Sohl-Dickstein/Sum-of-Functions-Optimizer\ngeneric architecture with layers commonly used in practice, we trained on the convolutional neural network of the form:\nConv[16× 32× 32] → ReLU → Pool[16× 16× 16] → Conv[20× 16× 16] → ReLU\n→ Pool[20 × 8× 8] → Conv[20× 8× 8] → ReLU → Pool[20 × 4× 4] → FC+ Softmax[10 × 1× 1]\nHere, the first dimension denotes the respective number of filters with kernel size 5 × 5 and max pooling downsamples its input by a factor of two. For this experiment, we chose τ = 5 · 10−3, ρ = 10/τ, γ = 2.5/τ, µ = 0.9 and a batch size of 250. We compared with a Nesterov SGD optimizer with parameters τ = 1 · 10−4/τ = 1 · 10−5, µ = 0.95. To this end, we have implemented our algorithm as an optimizer in Lasagne/Theano [22] and compared to Lasagne’s Nesterov SGD optimizer. Training was executed on the CPU of a machine with 8 Intel Xeon CPU E5-2637 with 3.50GHz. The results are shown in Fig. 5. The ProxProp optimizer achieves a lower energy on this nonconvex full batch training objective, while arriving at a comparable generalization accuracy on the test set, namely 65% for ProxProp, 66% for SGD with step size τ = 1 · 10−4 and 68% for SGD with step size τ = 1 · 10−5. Note that we have focused on the conceptual difference in parameter updates and have not performed extensive hyperparameter tuning for either of the methods."
    }, {
      "heading" : "6.2 USPS autoencoder",
      "text" : "To compare with MAC [2], we trained a fully connected autoencoder with the same architecture as for our experiments in Section 4.3, used sigmoid activation functions, and trained on the USPS dataset with 5000 grayscale images of size 16 × 16 using minibatches of size 250. We used the same initialization as [2] with the initial weights in layer l being uniformly sampled from [−1/√nl−1, 1/√nl−1]. The authors of [2] kindly provided us with their MATLAB code to reproduce the results of their parallel MAC method, as well as the SGD and CG methods they compared to. We are therefore able to reproduce Figure 2 in [2] without the minibatch MAC on our computer with an Intel Core i7-6700HQ CPU with 2.60GHz,and MATLABs parpool parallelizing MAC on 4 cores.\nFigure 6 shows the proposed semi-implicit method along with our implementation of a minibatchSGD method with momentum, as well as the SFO algorithm. We used τ = 5 ·10−5 and τ = 5 ·10−4 as a stepsize for the SGD and our semi-implicit algorithm respectively, used a momentum of µ = 0.99 for both methods, and set ρ = γ = 1/τ . As we can see in Fig. 6 the resulting advantage of our approach to explicit SGD steps and competing approaches like SFO and MAC becomes significant, particularly in the earlier phase of training."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have proposed ProxProp as an efficient novel method for training neural networks. To this end, we first showed the equivalence of the classical backprop algorithm with an algorithm that alternates between sequential gradient steps on a quadratic penalty function and forward passes through the network. Subsequently, we developed a generalization of backprop, which replaces explicit gradient steps with implicit (proximal) steps. Numerical experiments demonstrate that ProxProp yields a stable decrease of the objective function on a much wider range of step sizes than classical backprop. A performance comparison to related minimization methods shows that our algorithm exhibits faster convergence and often yields overall lower nonconvexobjective values. We believe that the proposed framework creates an important bridge between ideas from convex optimization and deep learning.\nAppendix"
    }, {
      "heading" : "A Theoretical results",
      "text" : "Proof of Proposition 1. We first take a gradient step on\nE(W , b,a, z) = Ly(φ(WL−1, bL−1, aL−2))\n+ γ\n2\nL−2∑\nl=1\n‖σ(zl)− al‖2 + ρ\n2\nL−2∑\nl=1\n‖φ(Wl, bl, al−1)− zl‖2, (17)\nwith respect to (WL−1, bL−1, aL−2). The gradient step with respect to (WL−1, bL−1) is the same as in the gradient descent update,\n(W , b)k+1 = (W , b)k − τ∇J(W k, bk;X, y), (18) since J depends on (WL−1, bL−1) only via Ly ◦ φ. The gradient descent step on aL−2 in a© yields\na (k+1/2) L−2 = a (k) L−2 − τ∇aφ(W (k) L−1, b (k) L−1, a (k) L−2) · ∇φLy(φ(W (k) L−1, b (k) L−1, a (k) L−2)), (19)\nwhere we use a(k+ 1/2)\nL−2 to denote the updated variable aL−2 before the forward pass of the next iteration.\nFor all layers l ≤ L− 2 note that due to the forward pass in Algorithm 1 we have\nσ(z (k) l ) = a (k) l , φ(W (k) l , b (k) l , a (k) l−1) = z (k) l\nand we therefore get the following update equations in the gradient step b©\nz (k+1/2) l = z (k) l − τγ∇σ(z (k) l )\n( σ(z\n(k) l )− a (k+1/2) l\n) = z\n(k) l −∇σ(z (k) l ) ( a (k) l − a (k+1/2) l ) , (20)\nand in the gradient step c© w.r.t. al−1,\na (k+1/2) l−1 = a (k) l − τρ∇aφ(W (k) l , b (k) l , a (k) l−1) ·\n( φ(W\n(k) l , b (k) l , a (k) l−1)− z (k+1/2) l\n)\n= a (k) l −∇aφ(W (k) l , b (k) l , a (k) l−1) · ( z (k) l − z (k+1/2) l ) .\n(21)\nEquations (20) and (21) can be combined to obtain:\nz (k) l − z (k+1/2) l =∇σ(z (k) l )∇aφ(W (k) l+1, b (k) l+1, a (k) l ) · ( z (k) l+1 − z (k+1/2) l+1 ) . (22)\nThe above formula allows us to backtrack the differences of the old z(k)l and the updated z (k+1/2) l up to layer L − 2, where we can use equations (20) and (19) to relate the difference to the loss. Altogether, we obtain\nz (k) l − z (k+1/2) l = τ\n  L−2∏\nq=l\n∇σ(z(k)q )∇aφ(W (k) q+1, b (k) q+1, a (k) q )   · ∇φLy(φ(W (k)L−1, b (k) L−1, a (k) L−2)).\n(23) By inserting (23) into the gradient descent update equation with respect to (Wl, bl) in c© ,\n(Wl, bl) k+1 = (Wl, bl) k −∇(W,b)φ(W (k)l , b (k) l , a (k) l−1) · ( z (k) l − z (k+1/2) l ) , (24)\nwe obtain the chain rule for update (18)."
    }, {
      "heading" : "B Solution of the proximal subproblems",
      "text" : "B.1 Proximal operator for the square loss\nThe solution of the problem\n(W (k+1) L−1 ,b (k+1) L−1 ) = argmin W,b Ly(Wa(k)L−2 + b1) +\n1\n2τ ||W −W (k)L−1||2 +\n1\n2τ ||b− b(k)L−1||2.\n(25) for the choice Ly(z) = 12 ||z − y||2 is given by\n(WL−1,bL−1) k+1 = [ ya⊤ + 1τW y1 ⊤ + 1τ b ] ([aa⊤ a1⊤\n1a⊤ 11⊤\n] + 1\nτ I\n)−1 , (26)\nwhere a ← a(k)L−2, W ← W (k) L−1, b ← b (k) L−1 and y is the ground-truth of the training data. Interestingly, since we have a ∈ RnL−2×N and 1 ∈ R1×N , the size of the linear system is independent of the batch size N .\nB.2 Proximal operator for ReLU penalty term\nIt can be quickly verified by checking the individual cases, that the global solution of the onedimensional nonconvex optimization problem\nx̂ = argmin x∈R\n1 2 (max(0, x)− a)2 + 1 2α (x− z)2 , (27)\nis given by the following:\nx̂ =    αa+z α+1 , if (z ≥ −αa) ∧ [ (z ≥ 0) ∨ ( (z < 0) ∧ ( (a−z) 2 2(α+1) < a2 2 ) )] , z, if (z < 0) ∧ [ (z < −αa) ∨ ( (z ≥ −αa) ∧ ( (a−z) 2 2(α+1) > a2 2 ) )] ,\n0, otherwise.\n(28)\nB.3 Proximal operator for convolutional layers\nFor some input activation a of shape (nb, nc, ny, nx) with batch size nb, number of input channels nc, sample dimensions ny × nx, respectively, and a filterbank K with nf filters and quadratic kernel size nk of dimension (nf , nc, nk, nk), we denote the operation of the convolutional layer by y = K ∗ a (with bias absorbed in K). For a proximal update of the network parametersK we then have to solve the following problem for some parameter τ > 0:\nK(k+1) = argmin K\n1 2 ||K ∗ a− y||2 + 1 2τ ||K −K(k)||2. (29)\nTo this end, we explicitly construct a dense matrix representation of the convolution operation. This matrix contains a flattened version of every input image shifted to all positions of the convolution\nkernel, i.e. for a kernel of size 5× 5, every image is represented by 25 shifted entries in that matrix. This representation is written to the first dimension and indexed by the kernel position in the second dimension. One can then perform the convolution by matrix vector multiplication with the flattened filterbank. To be concrete, denote this matrix by A, which, for the above case of dimensions, is of shape (nb · nx · ny, nc · nk · nk). Problem (29) can then be equivalently formulated as\nK̂(k+1) = argmin K̂\n1 2 ||K̂A⊤ − y||2 + 1 2τ ||K̂ − K̂(k)||2, (30)\nwhere K̂A⊤ denotes the standard matrix-matrix multiplication for a matrix K̂ of shape (nf , nc · nk · nk). Note that in order to solve (30), one has to invert a matrix of the form I + A⊤A ∈ R ncnknk×ncnknk , which is very small. Hence this proximal subproblem can be solved efficiently."
    } ],
    "references" : [ {
      "title" : "Stochastic gradient learning in neural networks",
      "author" : [ "Léon Bottou" ],
      "venue" : "Proceedings of Neuro-Nımes,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1991
    }, {
      "title" : "Distributed optimization of deeply nested systems",
      "author" : [ "Miguel Á. Carreira-Perpiñán", "Weiran Wang" ],
      "venue" : "In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics, AISTATS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Entropy-SGD: Biasing gradient descent into wide valleys",
      "author" : [ "Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Yann N. Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 27th International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, ICCV,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
      "author" : [ "Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi" ],
      "venue" : "In Field Guide to Dynamical Recurrent Networks",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "International Conference on Learning Representations,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 25th International Conference of Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "On optimization methods for deep learning",
      "author" : [ "Quoc V Le", "Adam Coates", "Bobby Prochnow", "Andrew Y Ng" ],
      "venue" : "Proceedings of The 28th International Conference on Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Deep learning via Hessian-free optimization",
      "author" : [ "James Martens" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Régularisation d’inéquations variationnelles par approximations successives",
      "author" : [ "B. Martinet" ],
      "venue" : "Rev. Francaise Inf. Rech. Oper., pages 154–159,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Proximité et dualité dans un espace hilbertien",
      "author" : [ "Jean-Jacques Moreau" ],
      "venue" : "Bulletin de la Société mathématique de France,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1965
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate O(1/k)",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Soviet Mathematics Doklady,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1983
    }, {
      "title" : "Numerical Optimization",
      "author" : [ "J. Nocedal", "S. Wright" ],
      "venue" : "Springer Series in Operations Research and Financial Engineering.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "Herbert Robbins", "Sutton Monro" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1951
    }, {
      "title" : "Learning representations by backpropagating",
      "author" : [ "David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams" ],
      "venue" : "errors. Nature,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1986
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods",
      "author" : [ "Jascha Sohl-Dickstein", "Ben Poole", "Surya Ganguli" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton" ],
      "venue" : "In Proceedings of the 30th International Conference on International Conference on Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : "In Proceedings of the 27th International Conference of Neural Information Processing Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Training neural networks without gradients: A scalable ADMM approach",
      "author" : [ "Gavin Taylor", "Ryan Burmeister", "Zheng Xu", "Bharat Singh", "Ankit Patel", "Tom Goldstein" ],
      "venue" : "In Proceedings of the 33rd International Conference on Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "In recent years neural networks have gained considerable attention in solving difficult correlation tasks such as classification in computer vision [8] or sequence learning [20] and as building blocks of larger learning systems [17].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 19,
      "context" : "In recent years neural networks have gained considerable attention in solving difficult correlation tasks such as classification in computer vision [8] or sequence learning [20] and as building blocks of larger learning systems [17].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 16,
      "context" : "In recent years neural networks have gained considerable attention in solving difficult correlation tasks such as classification in computer vision [8] or sequence learning [20] and as building blocks of larger learning systems [17].",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 14,
      "context" : "Since the introduction of stochastic gradient descent (SGD) [15, 1], several more sophisticated optimization methods have been studied.",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "Since the introduction of stochastic gradient descent (SGD) [15, 1], several more sophisticated optimization methods have been studied.",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "One such class is that of quasi-Newton methods, as for example the comparison of LBFGS with SGD in [9], Hessian-free approaches [10], and the Sum of Functions Optimizer in [18].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 9,
      "context" : "One such class is that of quasi-Newton methods, as for example the comparison of LBFGS with SGD in [9], Hessian-free approaches [10], and the Sum of Functions Optimizer in [18].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 17,
      "context" : "One such class is that of quasi-Newton methods, as for example the comparison of LBFGS with SGD in [9], Hessian-free approaches [10], and the Sum of Functions Optimizer in [18].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "Several works consider specific properties of energy landscapes of certain deep learning models such as frequent saddle points [4] and well-generalizable local optima [3].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "Several works consider specific properties of energy landscapes of certain deep learning models such as frequent saddle points [4] and well-generalizable local optima [3].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 12,
      "context" : "Among the most popular optimization methods in currently used deep learning frameworks are momentum based improvements of classical SGD, notably Nesterov’s Accelerated Gradient [13, 19], and the Adam optimizer [7], which uses estimates of first and second order moments of the gradients for parameter updates.",
      "startOffset" : 177,
      "endOffset" : 185
    }, {
      "referenceID" : 18,
      "context" : "Among the most popular optimization methods in currently used deep learning frameworks are momentum based improvements of classical SGD, notably Nesterov’s Accelerated Gradient [13, 19], and the Adam optimizer [7], which uses estimates of first and second order moments of the gradients for parameter updates.",
      "startOffset" : 177,
      "endOffset" : 185
    }, {
      "referenceID" : 6,
      "context" : "Among the most popular optimization methods in currently used deep learning frameworks are momentum based improvements of classical SGD, notably Nesterov’s Accelerated Gradient [13, 19], and the Adam optimizer [7], which uses estimates of first and second order moments of the gradients for parameter updates.",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 5,
      "context" : "Moreover, SGD often has difficulties in propagating a learning signal deeply into a network, commonly referred to as the vanishing gradient problem [6].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "Recently, the authors of [2] have tackled the problem of optimizing the nested objective function by explicitly introducing the network activations as variables of the optimization, also known as the method of auxiliary coordinates (MAC).",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 20,
      "context" : "[21] introduce additional auxiliary variables to further split linear and nonlinear transfer between layers.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "In this work we draw a connection between the penalty formulations [2, 21] and the classical backpropagation algorithm [16].",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 20,
      "context" : "In this work we draw a connection between the penalty formulations [2, 21] and the classical backpropagation algorithm [16].",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "In this work we draw a connection between the penalty formulations [2, 21] and the classical backpropagation algorithm [16].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "The gradient descent iteration on a nested function J(W , b;X, y) like (2), (W , b) = (W , b) − τ∇J(W , b;X, y), (3) is commonly implemented using the backpropagation algorithm [16].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "1 Implicit gradient steps The proximal mapping of a function f : R → R is defined as the following [12]: proxτf (y) := argmin x∈Rn f(x) + 1 2τ ||x− y||.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "The advantage of such proximal steps is that in contrast to explicit gradient descent, the update scheme (8), also known as the proximal point algorithm [11], is unconditionally stable.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "1] as well as the adaptive initialization scheme suggested in [5], denoted as adaptive uniform and adaptive Gaussian respectively.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "Algorithm 2 can be used as a gradient oracle for first-order methods (such as Adam [7]) or quasiNewton methods (such as SFO [18]), by using it to compute a direction d = (W , b) − (W , b) instead of the usual gradient.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : "Algorithm 2 can be used as a gradient oracle for first-order methods (such as Adam [7]) or quasiNewton methods (such as SFO [18]), by using it to compute a direction d = (W , b) − (W , b) instead of the usual gradient.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : "Our method clearly outperforms SGD, and performs comparably to the recent work SFO [18] while being more memory efficient.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "The weights are initialized according to the uniform distribution scheme described in [5] and a weight-decay of 10 was chosen.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "We further compare the proposed method to SFO [18] and used the publicly available MATLAB implementation2 with 20 subfunctions (minibatch size of 2250).",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "Our method outperformsMAC [2] by several orders of magnitude (w.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "The methods denoted with an asterisk are based on code kindly provided by [2].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "To compare with MAC [2], we trained a fully connected autoencoder with the same architecture as for our experiments in Section 4.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "We used the same initialization as [2] with the initial weights in layer l being uniformly sampled from [−1/√nl−1, 1/√nl−1].",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "The authors of [2] kindly provided us with their MATLAB code to reproduce the results of their parallel MAC method, as well as the SGD and CG methods they compared to.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "We are therefore able to reproduce Figure 2 in [2] without the minibatch MAC on our computer with an Intel Core i7-6700HQ CPU with 2.",
      "startOffset" : 47,
      "endOffset" : 50
    } ],
    "year" : 2017,
    "abstractText" : "We offer a generalized point of view on the backpropagation algorithm, currently the most common technique to train neural networks via stochastic gradient descent and variants thereof. Specifically, we show that backpropagation of a prediction error is equivalent to sequential gradient descent steps on a quadratic penalty energy. This energy comprises the network activations as variables of the optimization and couples them to the network parameters. Based on this viewpoint, we illustrate the limitations on step sizes when optimizing a nested function with gradient descent. Rather than taking explicit gradient steps, where step size restrictions are an impediment for optimization, we propose proximal backpropagation (ProxProp) as a novel algorithm that takes implicit gradient steps to update the network parameters. We experimentally demonstrate that our algorithm is robust in the sense that it decreases the objective function for a wide range of parameter values. In a systematic quantitative analysis, we compare to related approaches on a supervised visual learning task (CIFAR-10) for fully connected as well as convolutional neural networks and for an unsupervised autoencoder (USPS dataset). We demonstrate that ProxProp leads to a significant speed up in training performance.",
    "creator" : "LaTeX with hyperref package"
  }
}