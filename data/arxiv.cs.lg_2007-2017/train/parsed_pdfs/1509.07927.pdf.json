{
  "name" : "1509.07927.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Algorithms for Linear Bandits on Polyhedral Sets",
    "authors" : [ "Manjesh K Hanawal", "Amir Leshem" ],
    "emails" : [ "mhanawal@bu.edu", "leshema@eng.biu.ac.il", "srv@bu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We study stochastic linear optimization problem with bandit feedback. The set of arms take values in an N -dimensional space and belong to a bounded polyhedron described by finitely many linear inequalities. We provide a lower bound for the expected regret that scales as Ω(N log T ). We then provide a nearly optimal algorithm that alternates between exploration and exploitation intervals and show that its expected regret scales as O(N log1+ (T )) for an arbitrary small > 0. We also present an algorithm than achieves the optimal regret when sub-Gaussian parameter of the noise is known. Our key insight is that for a polyhedron the optimal arm is robust to small perturbations in the reward function. Consequently, a greedily selected arm is guaranteed to be optimal when the estimation error falls below some suitable threshold. Our solution resolves a question posed by [1] that left open the possibility of efficient algorithms with asymptotic logarithmic regret bounds. We also show that the regret upper bounds hold with probability 1. Our numerical investigations show that while theoretical results are asymptotic the performance of our algorithms compares favorably to state-of-the-art algorithms in finite time as well."
    }, {
      "heading" : "1 Introduction",
      "text" : "Stochastic bandits are sequential decision making problems where a learner plays an action in each round and observes the corresponding reward. The goal of the learner is to collect as much reward as possible or, alternatively minimize regret over a period of T rounds. Stochastic linear bandits are a class of structured bandit problems where the rewards from different actions are correlated. In particular, the expected reward of each action or arm is expressed as an inner product of a feature vector associated with the action and an unknown parameter which is identical for all the arms. With this structure, one can infer reward of arms that are not yet played from the observed rewards of other arms. This allows for considering cases where number of arms can be unbounded and playing each arm is infeasible.\nStochastic linear bandits have found rich applications in many fields including web advertisements [2], recommendation systems [3], packet routing, revenue management, etc. In many applications the set of actions are often defined by a finite set of constraints. For example, in packet routing, the amount of traffic to be routed on a link is constrained by its capacity. In web-advertisements\nar X\niv :1\n50 9.\n07 92\n7v 1\n[ cs\n.L G\n] 2\nproblems, the budget constraints determine the set of available advertisements. It follows that the each arm in these applications belongs to a polyhedron.\nBandit algorithms are evaluated by comparing their cumulative reward against the optimal achievable cumulative reward and the difference is referred to as regret. The focus of this paper is on characterizing asymptotic bounds for regret for fixed but unknown reward distributions, which are commonly referred to as problem dependent bounds [4].\nWe consider linear bandits where the arms take values in an N -dimensional space and belong to a bounded polyhedron described by finitely many linear inequalities. We derive an asymptotic lower bound of Ω(N log T ) for this problem and present an algorithm that is (almost) asymptotically optimal. Our solution resolves a question posed by [1] that left open the possibility of efficient algorithms with asymptotic logarithmic regret bounds. Our algorithm alternates between exploration and exploitation phases, where a set of arms on the boundary of the polyhedron is played in exploration phases and a greedily selected arm is played super-exponentially many times in the exploitation phase. Due to the simple nature of the strategy we are able to provide upper bounds which hold almost surely. We show that our regret concentrates around its expected value with probability one for all T . In contrast regret for upper confidence bound based algorithms concentrates only at a polynomial rate [5]. Thus, our algorithms are more suitable for risk-averse decision making. A summary of our results and comparison of regrets bounds is given in Table 1. Numerical experiments show that its regret performance compares well against state-of-the-art linear bandit algorithms even for reasonably small rounds while being significantly better asymptotically.\nRelated Work: Our regret bounds are related to those described in [4], who describe an algorithm (ConfidenceBall2) with regret bounds that scale as O((N2/∆) log3 T ), where ∆ is the reward gap defined over extremal points. These algorithms belong to the class of so called OFU algorithms (optimism in the face of uncertainty). Since OFU algorithms play only extremal points (arms), one may think that log T regret bounds can be attained for linear bandits by treating them as K-armed bandits, were K denotes the number of extremal points of the set of actions. This possibility arises from the classical results on the K-armed bandit problem due to Lai and Robbins [8] who provided a complete characterization of expected regret by establishing a problem dependent lower bound of Ω(K log T ) and then providing an asymptotically (optimal) algorithm with a matching upper bound. But, as noted in [1][Sec 4.1, Example 4.5], the number of extremal points can be exponential in N , and this renders such adaptation of multi-armed bandits algorithm inefficient. In the same paper, the authors pose it as an open problem to develop efficient algorithms for linear bandits over polyhedral set of arms that have logarithmic regret. They also remark that since convex hull of a polyhedron is not strongly convex, regret guarantees of their PEGE (Phased Exploration Greedy Exploitation) algorithm does not hold.\nOur work is close to FEL (Forced Exploration for Linear bandits) algorithm developed in [17]. FEL separates the exploration and exploitation phases by comparing the current round number against a predetermined sequence. FEL plays randomly selected arms in the exploration intervals and greedily selected arms in the exploitation intervals. However, our policy differs from FEL as follows– 1) we always play fixed set of arms (deterministic) in the exploration phases. 2) noise is assumed to be bounded in [17], whereas we consider more general sub-Gaussian noise model 3) unlike FEL, our policy does not require computationally costly matrix inversions. FEL provides expected regret guarantee of onlyO(c log2 T ) whereas our policy PolyLin has optimalO(N log T ) regret guarantee. Moreover, the authors in [17] remark that the leading constant c in their regret bound can be set proportional to √ N (see discussion following Th 2.4 in [17]), but this seems incorrect in light of the lower bound of Ω(N log T ) we establish in this paper.\nIn contrast to the asymptotic setting considered here, much of the machine learning literature deals with problem independent bounds. These bounds on regret apply in finite time and for the minimax case, namely, for the worst-case over all reward (probability) distributions. [9] established a problem independent lower bound of Ω( √ KT ) for multi-armed bandits, and was shown to be achievable in [7]. For linear bandits, problem dependent bounds and well studied and stated in terms of dimension of the set of arms rather than its size. In [10], for the case of finite number of arms, a lower bound of Ω( √ NT ) with matching upperbounds is established, where N denotes the dimension of the set of arms. For the case when the number of arms is infinite or form a bounded subset of aN -dimensional space, a lower bound of Ω(N √ T ) is established in [4, 1] with matching achievable bounds.\nSeveral variants and special cases of stochastic linear bandits are available depending on what forms the set of arms. The classical stochastic multi-armed bandits introduced by Robbins [11] and later studied by Lai and Robbins [8] is a special case of linear bandits where the set of actions available in each round is the standard orthonormal basis. Auer [12] first studied stochastic linear bandits as an extension of “associated reinforcement learning” introduced in [13]. Since then several variants of the problems have been studied motivated by various applications. In [2, 14], the linear bandit setting is adopted to study content-based recommendation systems where the set of actions can change at each round (contextual), but their number is fixed. Another variant of linear bandits with finite action set are spectral bandits [15, 16], where the graph structure defines the set of actions and its size. Several authors [4, 1, 17] have considered linear bandits with arms constituting a (bounded) subset of a finite-dimensional vector space and remains fixed over the learning period. [18] considers cases where the set of arms can change between the rounds but must belong to a bounded subset of a fixed finite-dimensional vector space.\nThe paper is organized as follows: In Section 2, we describe the problem and setup notations. In Section 3, we derive a lower bound on expected regret and describe our main algorithm SEE and its variant SEE2. In Section 5, we analyze the performance of SEE, and its adaptation for general polyhedron is discussed in Section6. In Section 7 we provide probability 1 bounds on the regret of SEE. Finally, we numerically compare performance of our algorithm against sate-of-the-art in 8."
    }, {
      "heading" : "2 Problem formulation",
      "text" : "We consider a stochastic linear optimization problem with bandit feedback over a set of arms defined by a polyhedron. Let C ⊂ RN denote a bounded polyhedral set of arms given by\nC = { x ∈ RN : Ax ≤ b } (1)\nwhere A ∈ RM×N ,b ∈ RM . At each round t, selecting an arm xt ∈ C results in reward rt(xt). We investigate the case where the expected reward for each arm is a linear function regardless of the history. I.e., for any historyHt, there is a parameter θ ∈ [−1, 1]N , fixed but unknown, such that E[rt(x)|Ht] = θ′x for all t and x ∈ C. Under these setting the noise sequence {νt}∞t=1, where νt = rt(x)− x′θ forms a martingale difference sequence. Let Ft = σ{ν1, ν2, · · · , νt,x1, · · · ,xt+1} denote the σ-algebra generated by noise events and arms selections till time t. Then νt is Ft-measurable and we assume that it satisfies for all b ∈ R1 E[ebνt |Ft−1] ≤ exp{b2R2/2}, (2) i.e., noise is conditionally R- sub-Gaussian which automatically implies E[νt|Ft] = 0 and Var(νt) ≤ R2. We can think of R2 as the conditional variance of noise. An example of Rsub-Gaussian noise isN (0, R2), or any bounded distribution over an interval of length 2R and zero mean. In our work, R is fixed but unknown.\nA policy φ := (φ1, φ2, · · · ) is a sequence of functions φt : Ht−1 → C such that an arm is selected in round t based on the historyHt−1. Define expected (pseudo) regret of policy φ over T -rounds as:\nRT (φ) = Tθ ′x∗ − E [ T∑ t=1 θ′φ(t) ] (3)\nwhere x∗ = arg maxx∈C θ′x denotes the optimal arm in C, which exists and is an extremal point1 of the polyhedron C [19]. The expectation is over the random realization of the arm selections induced\n1Extremal point of a set is a point that is not a proper convex combination of points in the set.\nby the noise process. The goal is to learn a policy that keeps the regret as small as possible. We will be also interested in regret of the policy defined as\nRT (φ) = Tθ ′x∗ − T∑ t=1 θ′φ(t). (4)\nFor the above setting, we can useConfidenceBall2 [4] or UncertainityEllipsoid [1] and achieve optimal regret of order N √ T . For linear bandits over a set with finite number of extremal points, one can also achieve regret that scales more gracefully, growing logarithmically in time T , using algorithms for the standard multi-armed bandits. Indeed, from fundamentals of linear programming\narg max x∈C θ′x = arg max x∈E(C) θ′x,\nwhere E := E(C) denotes the set of extremal points of C. Since the set of extremal points is finite for a polyhedron, we can use the standard Lai and Robbin’s algorithm [8] or UCB1 in [6] treating each extremal point as an arm and obtain regret bound (problem dependent) of order |E|∆ log T , where ∆ := θ′x∗ − maxE\\x∗ θ′x denotes the gap between the best and the next best extremal point. However, the leading term in these bounds can be exponential in N , rendering these algorithm ineffective. For example, the number of extremal points of C can be of the order ( M+N M ) = O((2N)M ). Nevertheless, in analogy with the problem independent regret bounds in linear bandits, one wishes to derive problem dependent logarithmic regret where the dependence on set of arms is only linear in its dimension. Hence we seek an algorithm with regret of order N log T .\nIn the following, we first derive a lower bound on the expected regret and develop an algorithm that is (almost) asymptotically optimal."
    }, {
      "heading" : "3 Main results",
      "text" : "In this section we provide a lower bound on the expected regret and present our proposed policy and prove the main results regarding its complexity."
    }, {
      "heading" : "3.1 Lower Bound",
      "text" : "We establish through a simple example that regret of any asymptotically optimal linear bandit algorithm is lower bounded as Ω(N log T ). Recall the fundamental property of the linear optimization that an optimal point is always an extremal point. Then any linear bandit algorithm on a polyhedral set of arms always play the extremal points. We exploit this fact, and mapping the problem to a standard multi-armed bandits we obtain the lower bound.\nWe need the following notations to prove the result. Let {η(β)}β∈[0,1] denote a set of distributions parametrized by β ∈ [0, 1] and such that each η(β) is absolutely continuous with respect to a positive measure m on R. Let p(x;β) denote the probability density function associated with distribution η(β), and let KL(β1, β2) denote the Kullback-Leibler (KL) divergence between distributions η(β1) and η(β2) defined as KL(β1, β2) = ∫ x p(x;β1) log p(x;β1) p(x;β2)\nm(dx). Consider a set of K arms. We say that arm k is parametrized by βk if its reward is distributed according to η(βk).\nWe are now ready to state asymptotic lower bound for the linear bandit problem over any bounded polyhedron with positive measure . Without loss of generality, we restrict our attention to uniformly good policies as defined in [8]. We say that a policy φ is uniformly optimal if for all θ ∈ Θ, R(T, φ) = o(Tα) for all α > 0.\nTheorem 1 Let φ any uniformly good policy on a bounded polyhedron with positive measure. For any θ ∈ [0, 1]N , let E[η(θk)] = θk for all k = 1, 2, · · · , N . Then,\nlim inf T→∞\nRT (φ) log T ≥ (N − 1)∆\nmax k:θk<θ∗\nKL(θ∗, θk) where θ∗ = arg max n θn (5)\nProof sketch: First, note that number of extremal points of any bounded polyhedron with positive measure is atleast (N + 1). We can then restrict to a bounded polyhedron with N + 1 extremal\npoints. Let C̃ = {x ∈ RN : 0 ≤ xi ≤ 1 ∀ i = 1, 2 · · · , N}. The (N + 1) extremal points of C̃ are {en : n = 1, 2, · · · , N} ∪ {0}. In the linear bandit problem with unknown parameter θ, playing the extremal point en gives mean reward θn. Also, by the property of linear optimization, any OFU policy will only play extremal points in every round. Then, the linear bandit over polyhedron C̃ is the same as N + 1-armed bandit where reward of kth arm k = 1, 2 · · · , N is distributed as η(θk) with mean θk, and the reward of N + 1th arm is distributed as η(0) with mean 0.\nThe result follows from Lai-Robbin’s lower bound for stochastic multi-armed bandits proved in [8] after verifying that the mean values of the parametrized distribution satisfy the required conditions."
    }, {
      "heading" : "3.2 Algorithms",
      "text" : "The basic idea underlying our proposed technique is based on the following observations for linear optimization over a polyhedron. 1) The set of extremal points of polyhedron is finite and hence ∆ > 0. 2) When θ̂ is sufficiently close to θ, then over the set C both arg maxθ′x and arg max θ̂ ′ x give the same value. We exploit these observations and propose a two stage technique, where we first estimate θ based on a block of samples and then exploit it for much longer block. This is repeated with increasing block lengths so that at each point the regret is logarithmic. For ease of exposition, we first consider the polyhedron that contains origin and postpone the general case to Section 6.\nAssume that the polyhedron C = { x ∈ RN : Ax ≤ b } contains origin as an interior point. Let en denote nth standard unit vector of dimension N . For all 1 ≤ n ≤ N , let zn = max {z ≥ 0, zen ∈ C}. The subset of arms B := {znen : n = 1, 2 · · · , N} are the vertices of the largest simplex bounded in C. Since θn = θ′en we can estimate θn by repeatedly playing the arm znen. One can also estimate θn by playing an interior point zen ∈ C for some z > 0. But as will see later selecting the maximum possible z improves the probability of estimation error."
    }, {
      "heading" : "Algorithm-SEE",
      "text" : "In our policy- which we refer as SequentialEstimation-Exploitation (SEE)- we split the time horizon into cycles and each cycle consists of an exploration interval followed by an exploitation interval. We index the cycles by c and denote the exploration and exploitation intervals in cycle c as Ec and Rc, respectively. In the exploration intervalEc, we play each arm in B repeatedly for (2c + 1) times. At the end of Ec, using the rewards observed for each arm in B in the past c- cycles we compute ordinary least square (OLS) to estimate each component θn, n = 1, 2, · · · , N separately and obtain the estimate θ̂(c). Using θ̂(c) as a proxy for θ, we compute a greedy arm x(c) by solving a linear program and play it repeatedly for 2c 2/(1+ ) times in the exploitation interval Rc, where > 0 in an input parameter. We repeat the process for each cycle. A formal description of SEE is given in the adjacent figure. The estimation in line 13 is computed for all n = 1, 2, · · · , N as follows:\nθ̂n(c) = 1\nc2 c∑ i=0 2i+1∑ j=1 rti,n,j/zn, (6)\nAlgorithm 1 SEE 1: Input: 2: C: The polyhedron 3: : Algorithm parameter 4: Initialization: 5: Compute the set B 6: for c = 0, 1, 2, · · · do 7: Exploration: 8: for n = 1→ N do 9: for j = 1→ 2c+ 1 do 10: Play arm znen ∈ B, observe reward rtc,n,j 11: end for 12: Compute θ̂n(c) 13: end for 14: θ̂(c)← (θ̂1(c), θ̂2(c) · · · , θ̂N (c)) 15: x(c)← arg max\nx∈C x′θ̂(c)\n16: Exploitation: 17: for j = 1→ b2c2/1+ c do 18: Play arm x(c), observe reward 19: end for 20: end for\nNote that in the exploration intervals, SEE plays a fixed set of arms and no adaption happens, adding positive regret in each cycle. The regret incurred in the exploitation intervals starts reducing as the estimation error gets small, and when it falls below ∆/2 the step (line-16) selects the optimal arm and no regret is incurred in the exploitation intervals (Lemma 2). As we will show later, the probability of estimation error decays super-exponentially across the cycles, and hence the probability of playing a sub-optimal arm in the exploitation interval also decays super-exponentially.\nTheorem 2 Let the noise be R-sub-Gaussian and without loss of generality2 assume θ ∈ [−1, 1]N . Then, the expected regret of SEE, with parameter > 0 is bounded as follows:\nRT (SEE) ≤ 2RmN log1+ T + 4RmNγ1, (7)\nwhere Rm denotes the maximum reward. γ1 is a constant that depends on noise parameter R and the sub-optimality gap ∆.\nThe parameter determines the length of the exploitation intervals, and larger implies that SEE spends less time in exploitation and more time in exploration. Increasing will make SEE spend more time in explorations resulting in improved estimations and reduces the probability of playing sub-optimal arm in the exploitation intervals. Hence parameter determines how fast the regret concentrates, and larger its value more ’risk-averse’ is the algorithm. This motivates us to consider a variant of SEE that is more risk averse but at the cost of increased expected regret."
    }, {
      "heading" : "3.3 Risk Averse Variant",
      "text" : "Our second algorithm-which we refer to as SEE2- is essentially same as SEE, except for the length of the exploration intervals which is exponential instead of super-exponential and does not depend on . Specifically, we play the greedy arm 2c times in cycle c. Compared to SEE, SEE2 spends significantly more time in the exploration intervals, and hence the probability that it makes error in the exploitation intervals is also significantly smaller and thus its regret concentrates around the expected regret faster.\nTheorem 3 Let the noise be R-sub-Gaussian and θ ∈ [−1, 1]N . Then, the expected regret of SEE2 is bounded as follows:\nRT (SEE2) ≤ 2RmN log2 T + 4NRmγ2 (8)\nwhere γ2 is a constant that depends on noise parameter R and the sub-optimality gap ∆."
    }, {
      "heading" : "4 Optimal Algorithm.",
      "text" : "We next obtain an optimal algorithm that achieves the lower bound in (5) within a constant factor when the sub-Gaussian parameter R is known.\n2For general θ, we replace it by θ ‖θ‖∞ and the same method works. Only Rm is scaled by a constant factor."
    }, {
      "heading" : "Algorithm-PolyLin:",
      "text" : "In our next policy- which we refer as PolyhedralLinear-bandits we again split the time horizon into cycles consisting of an exploration interval followed by an exploitation interval as in SEE. As earlier, we index the cycles by i and denote the exploration and exploitation intervals in cycle i as Ei and Ri, respectively. In the exploration interval Ei, we play each arm in B once. After c-cycles, using the rewards observed for each arm in B in the past {Ei, i = 12, · · · , c} exploration intervals we compute ordinary least square (OLS) to estimate each component θn, n = 1, 2, · · · , N separately, and obtain the estimate θ̂(c) as follows.\nθ̂n(c) = 1\nc c∑ i=1 rti,n/zn, (9)\nUsing θ̂(c) as a proxy for θ we compute a greedy arm x(c) and the sub-optimality gap ∆̂(c) as follows.\n∆̂(c) = x′(c)θ(c)− max x∈C\\x(c) x′θ(c).\nIn the exploitation interval Rc, we play x(c) repeatedly for 2κ(c)c times where κ(c) is set to a∆̂(c)/2, where a = minn zn/R2. We repeat the process for each cycle. A formal description of PolyLin is given in the adjacent figure.\nAlgorithm 2 PolyLin 1: Input: 2: C: The polyhedron 3: R: Noise parameter 4: Initialization 5: Compute the set B 6: a := minn z2n/R 2\n7: for i = 1, 2, · · · do 8: Exploration: 9: for n = 1→ N do\n10: Play arm znen ∈ B observe reward rti,n\n11: c = i, Compute θ̂n(c) as in (9) 12: end for 13: θ̂(c)← (θ̂1(c), θ̂2(c) · · · , θ̂N (c)) 14: x(c)← arg max\nx∈C x′θ̂(c)\n15: κ(c)← a∆̂(c)/2 16: Exploitation: 17: for j = 1→ b2κ(c)cc do 18: Play arm x(c), observe reward 19: end for 20: end for\nNote that the exploration intervals of PolyLin are fixed length, whereas in SEE they are increasing as the the time progresses. Also, exploitation intervals in PolyLin are adaptive, whereas it is nonadaptive in SEE.\nTheorem 4 Let the noise be R-sub-Gaussian and without loss of generality assume θ ∈ [−1, 1]N . Then, the expected regret of PloyLin is bounded as follows:\nRT (PolyLin) ≤ 2RmN log T\nκ + 4RmNγ3, (10)\nwhere Rm denotes the maximum reward. γ3 and κ are constants that depends on noise parameter R and the sub-optimality gap ∆."
    }, {
      "heading" : "5 Regret Analysis",
      "text" : "In this section we prove Theorem 2, the proof of Theorem 3 follows similarly and omitted. We first derive the probability of error in estimating each component of θ in each cycle. Note that in the exploration stage of each cycle cwe sample each arm znen ∈ B, i = 1, 2, · · · , N , 2 times more than that in the exploration stage of the previous cycle. Thus, we have c2 plays of each arm znen ∈ B at the end of cycle c. The estimation error of component θn after c-cycles is given as follows:\nLemma 1 Let the noise be R-sub-Gaussian and δ > 0. In any cycle c of both SEE and SEE2, for all n = 1, 2, · · · , N we have\nP (∣∣∣θ̂n(c)− θn∣∣∣ > δ) ≤ 2 exp{−c2δ2z2n/2R2}. (11)\nNote that larger the value of zn, the smaller the probability of estimation error is. The next lemma gives the probability that we play a suboptimal arm in the exploitation intervals of a cycle.\nLemma 2 For every cycle c, we have\na. Let a := minn z2n/R 2. The estimation error is bounded as\nPr{‖θ̂(c)− θ‖∞ > η} ≤ 2N exp{−c2η2a}., (12)\nb. Let h = supx∈C ‖x‖1. The error in reward estimation is bounded as\nPr ( ∃ x ∈ C such that ∣∣∣θ̂′(c)x− θ′x∣∣∣ > η) ≤ 2Ne− c2η2ah2 . (13) c. Probability that we play a sub-optimal arm is bounded as\nPr ( arg max\nx∈C θ̂(c)′x 6= arg max x∈C θ′x.\n) ≤ 2Ne− ac2∆2/4 h2 . (14)\nThe proofs of Lemmas 1 and 2 are given in appendix. Recall that the number of extremal points is finite for the polyhedron C and ∆ > 0. We use this fact to argue that whenever ‖θ̂(c)−θ‖∞ < ∆/2, the greedy stage of the algorithm selects the optimal arm. This in an importation observation and follows from continuity property of optimal point in linear optimization theory [19]. Further, the probability of this event decays super-exponentially fast in our policy implying that the probability that we incur a positive regret in the exploitation intervals is gets negligibly small over the cycles. We compute the expected regret incurred in the exploration and exploitation intervals separately."
    }, {
      "heading" : "5.1 Regret of SEE.",
      "text" : "We analyze the regret in the Exploration and Exploitation phases separately as follows. Exploration regret: At the end of cycle c, each arm in B is played ∑c i=1(2i+ 1) = c\n2 times. The total expected regret from the exploration intervals after c cycles is at most Nc2Rm. Exploitation regret: Total expected regret from the exploration intervals after c cycle is\n4NRm c∑ i=1 2i 2/(1+ ) 2−i 2a∆2 = 4NRm c∑ i=1 2i 2/(1+ )−i2a∆2 ≤ 4NRmγ2 (15)\nwhere γ2 := ∑∞ i=1 2 i(i(1− )/(1+ )−c1i∆2/4) is a convergent series. After c cycles, the total number of\nplays is T = ∑c i=1 e i 2 1+ +Nc2 ≥ ec 2 1+ and we get c2 ≤ log1+ T . Finally, expected regret form T -rounds is bounded as\nRT (SEE) ≤ 2RmN log1+ T + 4NRmγ2 = O(N log1+ T )."
    }, {
      "heading" : "5.2 Regret of PolyLin.",
      "text" : "We analyze the regret in the Exploration and Exploitation phases separately as follows. Exploration regret: After c cycles, each arm in B is played c times. The total expected regret from the exploration intervals after c cycles is at most NcRm. Exploitation regret: Total expected regret from the explorations interval after c cycles is\n4NRm c∑ i=1 2κ(i)i2−ia∆ 2 = 4NRm c∑ i=1 2iκ(i)−ia∆ 2 ≤ 4NRm ∞∑ i=1 2ia{∆̂ 2(i)/2−∆2}. (16)\nNow consider the series γ3 := ∑∞ i=1 2 ia{∆̂2(i)/2−∆2}.\n• From Lemma 2(a), θ(c) → θ as c → ∞almost surely, we get x̂(c) → x∗ almost surely and which in turn implies ∆̂(c)→ ∆ almost surely.\n• Then, for 0 < < ∆2/4, the difference ∆̂(c)2/2 − ∆2 ≤ −∆2/2 + < 0 for all but finitely many c. Hence, γ3 is finite.\nAfter c cycles the total number of plays is T = ∑c i=1 2\niκ(i) + Nc ≥ 2cκ(c), and we get c ≤ log Tκ(c) . Finally, expected regret form T -rounds, as T →∞, is bounded as\nRT (PolyLin) ≤ 2RmN log T\nκ(c) + 4NRmγ3.\nNote that ∆̂(c)2/2−∆2 ≥ −∆2/2− for all but finitely many c. Then for sufficiently large c we get k(c)/a ≥ ∆2/2− ≥ ∆2/4. Substituting in the last inequality we get\nRT (PolyLin) ≤ 8RmN log T\na∆ + 4NRmγ3 = O(N log T )."
    }, {
      "heading" : "6 General Polyhedron",
      "text" : "In this section we extend the analysis of the previous section to the case where origin is not an interior point of C. Analogous to set B, we first define a set of arms that lie on the boundary of the polyhedron and these points are computed with respected to an interior point x of C that we use as a proxy for origin. We use OPT-1 to find an interior point, whose smallest distance to boundaries along all the directions {e1, e2, · · · eN} is the largest. The motivation to maximize the minimal distances to the boundaries comes from lemma 2, where larger value of a imply smaller probability of estimation error."
    }, {
      "heading" : "OPT-1:",
      "text" : "(x,y) = arg max\nx min i yi\nsubjected to: Ax ≤ b yi ≥ 0 ∀i = 1, 2, · · · , N A(x + yiei) ≤ b ∀i = 1, 2, · · · , N A(x− yiei) ≤ b ∀i = 1, 2, · · · , N\nOPT-2: (x,y) = arg max\nx,y,α α\nsubjected to: α > 0; Ax ≤ b yi − α > 0 ∀i = 1, 2, · · · , N A(x + yiei) ≤ b ∀i = 1, 2, · · · , N A(x− yiei) ≤ b ∀i = 1, 2, · · · , N\nOPT-1 can be translated into an equivalent linear progamme given in OPT-2 and hence the point x can be efficiently computed. We note that the set of points {x + ynen : n = 1, 2, · · · , N} need not all necessarily lie on the boundary. To see this, let the point x returned by OPT-1 is such that it is closer to the boundary along ith direction. Then the vector with all its component equal to yi is a solution of OPT-1. To overcome this, we further stretch each point x + ynen along the direction en such that it hits the boundary. Let zn = arg maxz{|z| : zen ∈ C}. Finally, we fix the set of arms we use for explorations as B = {znen + x : n = 1, 2, · · · , N}. We are now ready to present an algorithm for linear bandits over for any polyhedra. For the general polyhedron, we use the SEE with the exploration strategy modified as follows. In cycle c, we first play the arm x for 2c + 1 and then play each arm in B 2c + 1 times as earlier. To estimate the component θn, we average the difference in rewards observed from arms x + znen and x so far. From a straightforward modification of regret analysis of SEE, we can show that the expected regret of modified algorithm is upper bounded as O(N log1+ T ) for all > 0.\nThe new algorithm required that we play the arm x along with the arms in B in the exploration intervals to obtain estimate of θ, and it increases the length of exploration intervals. However, it is possible that one can obtain estimates only by playing arms in B provided we suitably modify the estimation method. More details are given in the appendix."
    }, {
      "heading" : "7 Probability 1 Regret Bounds",
      "text" : "Recall the definiton of expected regret and regret in (3) and (4). In this section we show that with probability 1, the regret of our algorithms are within a constant factor from the their expected regret.\nTheorem 5 With probability 1, RT (SEE) is O(N log1+ T ) and RT (SEE2) is O(N log2 T ).\nProof: Let Cn denote an event that we select sub-optimal arm in the nth cycle. From Lemma 2, this event is bounded as Pr{Cn} ≤ N exp{−O(n2)}. Hence ∑∞ n=1 Pr{Cn} < ∞. Now, from application of Borel-Cantelli lemma, we get Pr{lim supn→∞Cn} = 0, which implies that almost surely SEE and SEE2 play optimal arm in all but finitely many cycles. Hence the exploitation intervals contribute only a bounded regret. Since the regret due to exploration intervals is deterministic, the regret of SEE and SEE2 are within a constant factor from their expected regret with probability 1, i.e., Pr{∃ C1 such that RT (SEE) ≤ RT (SEE) + C1} and Pr{∃ C2 such that RT (SEE2) ≤ RT (SEE2) + C2}. This completes the claim. We note that the regret bounds proved in [4] hold with high confidence, where as ours hold with probability 1 and hence provides a stronger performance guarantee."
    }, {
      "heading" : "8 Experiments",
      "text" : "In this section we investigate numerical performance of our algorithms against the known algorithms. We run the algorithms on a hypercube with dimension N = 10. We generated θ ∈ [0, 1]N randomly and noise is zero mean Gaussian random variable with variance 1 in each round. The experiments are averaged over 10 runs. In Fig. 1 we compare SEE ( = 0.3) and SEE2 against UCB-Normal [20], where we treated each extremal point as an arm of an 2N -armed bandit problem. As expected, our algorithms perform much better. UCB-Normal need to sample each of the 2N atleast once before it could start learning the right arm. Whereas, our algorithm starts playing the right arm after a few cycles of exploration intervals. In Fig. 2, we compare our algorithms against the linear bandits algorithm LinUCB and self-normalization based algorithm in [18], which is labeled SelfNormalized in the figure. For these we set confidence parameter to 0.001. We see that SEE beats LinUCB by a huge margin, but its performance comes close to that of SelfNormalized algorithm. Note that SelfNormalzed algorithm requires knowledge of sub-Gaussianity parameter R of noises super. Whereas, our algorithms are agnostic to this parameter. Though, SEE2 seems to play the right arm in exploitation intervals, its regret performance is poor. This is due to increased number of exploration intervals, where no adaptation happens and a positive regret is always incurred.\nThe numerical performance of SEE2 can be improved by adaptively playing the arms in the exploration plays as follows, but at the increase cost of computations complexity. In each cycle c+ 1, we find a new set B computed by setting x to x(c), the greedy arm selected in the previous cycle, and play the new set arms as in the explorations intervals of the algorithm given for the general polyhedron. However, since x(c) is an extremal points some of the zn’s are zero. To overcome this, we slightly shift the point x(c) into the interior of the polyhedron along the direction x(c)− x and find a new set B with respect to the new interior point. The regret of the algortihm based on this adaptive exploitation strategy is shown is Fig. 2 with label ’Improved-SEE2’. As shown, the modification improves performance of SEE2 significantly. In all the numerical plots, we initialized the algorithm to run from cycle number 5."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We studied stochastic linear optimization over polyhedral set of arms with bandit feedback. We provided asymptotic lower bound for any policy and developed algorithms that are near asymptotically optimal. The regret of the algorithms grow (near) logarithmically in T and its growth rate is linear\nin the dimension of the polyhedron. We showed that the regret upper bounds hold almost surely. The regret growth rate of our algorithms is log1+ T for some > 0. It is interesting to develop strategies that work for = 0, while still maintain linear growth rate in N ."
    }, {
      "heading" : "Proof of Lemma 1",
      "text" : "Let ti,n,j denote the noise in reward from playing znen in phase i for the jth time. We bound the estimation error as follows:\nP (∣∣∣θ̂n(c)− θn∣∣∣ > δ) (17) = P\n∣∣∣∣∣∣ c2∑ i=1 ti,n,j / c2zn| > δ  (18) = P\ns ∣∣∣∣∣∣ c2∑ i=1 ti,n,j ∣∣∣∣∣∣ > sc2znδ  (19)\n= P exp s ∣∣∣∣∣∣ c2∑ i=1 ti,n,j ∣∣∣∣∣∣  > exp{sc2znδ}  (20) ≤ 2P exp s c2∑ i=1 ti,n,j  > exp{sc2znδ}  (21)\n≤ 2E exp s c2∑ i=1 ti,n,j   exp{−sc2znδ}} (22) ≤ 2 c2∏ i=1 E [ exp { s ti,n,j } |Ft−1 ] exp{−sc2znδ}} (23) ≤ 2 c2∏ i= exp{s2β2/2} exp{−sc2znδ}} (24)\n= 2 exp{c2(s2β2/2− sznδ)}}, (25)\nwhere (18) follows from estimation step given in (6). In (19) and (20) we exponentiated both sides within the probability functions after multiplying them by s > 0. (21) follows by applying union bound and using the symmetric property of the noise terms. In (22) we applied the Markov inequality. In (23) we aplied conditional independence property of the noise. (24) follows by applying the definition of sub-Gaussian property.\nNote that upper bound in (25) holds for all s > 0 and is minimized at s∗ = δznβ2 > 0. Finally, the lemma by substituting s∗ in (25)."
    }, {
      "heading" : "Proof of Lemma 2",
      "text" : ""
    }, {
      "heading" : "Part a:",
      "text" : "We bound the estimation error as follows: Pr (∥∥∥θ̂(c)− θ∥∥∥\n∞ > η\n) (26)\n≤ Pr ( ∃n : ∣∣∣θ̂n(c)− θn∣∣∣ > η) (27) ≤\nN∑ n=1 Pr (∣∣∣θ̂n(c)− θn∣∣∣ > η) (28)\n≤ 2Nc1e−ac 2η2 . (29)\nIn (28) we applied the union bound result and in (29) we applied (11)."
    }, {
      "heading" : "Part b:",
      "text" : "For all x ∈ C, we have |x′θ(c)− x′θ| ≤ ‖θ(c)− θ‖∞‖x‖1. (30) Define events A = {∃ x such that|x′θ(c) − x′θ| > η} and B = {‖θ(c) − θ‖∞h > η}. The last inequality implies Pr{A} ≤ Pr{B}. The claim follows from part-a of the lemma."
    }, {
      "heading" : "Part c:",
      "text" : "Suppose y 6= x∗, where x∗ is the optimal arm, such that θ′(c)y > θ′(c)x∗. Then, since θ′x∗ − θ′y ≥ ∆ we must have that either |θ′x∗ − θ′(c)x∗| ≥ ∆/2 or |θ′(c)y − θ′y| ≥ ∆/2, otherwise we cannot close the gap. Hence, if the greedy selection in cycle c is not x∗, it implies that there exists a x ∈ C such that |θ′(c)x − θx| > ∆/2. From part-b this probability is bounded as 2N exp{−ac2η2/h}, where η = ∆/2. This completes the proof.\nEstimation in the case general polyhedron Let xi = x + αiei. Let r̂i(c) := 1c2 ∑c i=1 ∑2c+1 j=1 rti,n,j denote the average of the reward obtained from arm xi till end of phase m. At the end of phase m, we estimate θ as follows:\nθ̂(c) = (1x′ +D(α)) −1 r̂(c),\nwhere α denote the diagonal matrix with diagonal elements as α and r̂(c) is the vector with ith component as r̂i(m). By applying matrix inversion lemma we get\nθ̂(c) = ( D−1(α)− D −1(α)1x′D−1(α)\nx′D−1(α)1 ) After simplification, for each i = 1, 2, , · · · , N we have\nθ̂i(c) = 1\nαi\n( r̂i(c)− ∑N j=1(xj/αj)r̂j(c)∑N\nl=1 xl/αl ) Substituting the reward from arm xi, i.e.,\nrxi = x ′θ + αiθi +\nand further simplifying we get\nθ̂i(c) = 1\nαi αiθi − x′θ + N∑ j=1 βj ̂j(c)  where βj =\nxj αj and ̂j(m) is the noise average from playing arm xi."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "We study stochastic linear optimization problem with bandit feedback. The set of<lb>arms take values in an N -dimensional space and belong to a bounded polyhedron<lb>described by finitely many linear inequalities. We provide a lower bound for the<lb>expected regret that scales as Ω(N log T ). We then provide a nearly optimal al-<lb>gorithm that alternates between exploration and exploitation intervals and show<lb>that its expected regret scales as O(N log (T )) for an arbitrary small > 0.<lb>We also present an algorithm than achieves the optimal regret when sub-Gaussian<lb>parameter of the noise is known. Our key insight is that for a polyhedron the op-<lb>timal arm is robust to small perturbations in the reward function. Consequently, a<lb>greedily selected arm is guaranteed to be optimal when the estimation error falls<lb>below some suitable threshold. Our solution resolves a question posed by [1] that<lb>left open the possibility of efficient algorithms with asymptotic logarithmic re-<lb>gret bounds. We also show that the regret upper bounds hold with probability 1.<lb>Our numerical investigations show that while theoretical results are asymptotic the<lb>performance of our algorithms compares favorably to state-of-the-art algorithms<lb>in finite time as well.",
    "creator" : "LaTeX with hyperref package"
  }
}