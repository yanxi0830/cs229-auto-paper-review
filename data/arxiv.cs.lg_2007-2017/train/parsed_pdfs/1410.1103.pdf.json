{
  "name" : "1410.1103.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Ranking with Top-1 Feedback",
    "authors" : [ "Sougata Chaudhuri", "Ambuj Tewari" ],
    "emails" : [ "sougata@umich.edu", "tewaria@umich.edu", "Precision@k." ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Consider a system that is learning to rank a fixed set of objects for presentation to users, when different users have varied preference for the objects. Learning occurs in an online setting: at each round, the system outputs a ranked list of the objects and the quality of ranking is measured by one of several popular ranking measures (like DCG [12] or MAP [3]), taking into account the users preferences encoded as relevance vectors. We work in a game-theoretic setting and do not make any stochastic assumptions on how the relevance vectors are generated. Thus, it is assumed that the relevance vectors are generated by an oblivious adversary. The objective of the learner is to have a sub-linear (in the number of rounds) regret against the best ranking in hindsight. The idea of ranking for diverse preferences has been motivated from a branch of work, sometimes called “ranking with diversity” [18, 17, 1] . In “ranking with diversity” work in literature, the focus has been on learning an optimal ranking of a fixed set of objects where the loss in a round is simply 0− 1, depending on whether the user finds any relevant object in top-k (of m) ranked items.\nIn addition to considering more practical losses associated with ranking like DCG and MAP, we consider a novel and challenging feedback model in this paper: the learner only gets to see the relevance of the object placed at the top (rank 1), whereas the ranking performance measure, and hence the regret, depends on the full relevance vector. We highlight two practical scenarios motivating the feedback model.\nEconomic Reason: A company wants to produce a ranked order of a fixed set of products related to a query. Different products are likely to have varying relevance to different users, depending on user characteristics such as age, gender etc. In principle, a user can browse through the\nar X\niv :1\n41 0.\n11 03\nv1 [\ncs .L\nG ]\n5 O\nct 2\n01 4\nentire list giving carefully considered ratings, say on a 5 point scale, to each product. In practice, however, it is quite likely that user will scan through all the products and have a rough idea about how relevant each product is to her. But she will likely be reluctant to give a thorough feedback on each product, unless the company provides some economic incentives to do so. Though the company needs high quality feedback on each product, to keep refining the ranking strategy, it cannot afford to give incentives due to budget constraints. Hence, they require the user to give feedback only on top placed product. This allows the user to look at all products but does not burden her with task of providing feedback beyond the top-ranked product. In this scenario, a full relevance vector is implicit in the user’s mind but the system (company) gets to see the relevance of only the top placed product.\nPrivacy Reason: A medical company wants to advertise multiple new drugs for a particular medical condition. Not all drugs are suitable for everyone since the effects of the drugs vary depending on the user attributes like age, gender etc. To satisfy most users, the company wants to produce a useful ordering of the drugs, but users are unlikely to give feedback on the usefulness (relevance) of every drug due to privacy concerns. So the company can ask for feedback about just the top ranked drug while, as above, every drug has an implicit relevance score for each user. However, the system will only get to see the relevance of the top ranked drug.\nTheoretically, the top-1 feedback model is neither full-feedback nor bandit-feedback since not even the loss (quantified by some ranking measure) at each round is revealed to the learner. The appropriate framework to study the problem is that of partial monitoring [7]. A very recent paper shows another practical application of the idea where feedback is neither full information nor bandit [14]. Recent advances in the classification of partial monitoring games tell us that the minimax regret, in an adversarial setting, is governed by a property of the loss and feedback functions called observability [4, 10]. Observability is of two kinds: local and global. We instantiate these general observability notions for the top-1 feedback case and prove that, for some ranking measures, namely PairwiseLoss [9], DCG and Precision@k [15], global observability holds. This immediately shows that the upper bound on regret scales as O(T 2/3) (ignoring log T factors). Specifically for PairwiseLoss and DCG, we further prove that local observability fails, which shows that their minimax regret scales as Θ(T 2/3). However, the generic algorithm that enjoys O(T 2/3(log T )1/3) regret for globally observable games maintains an explicit distribution over learner actions. For us, the action set is the exponentially large set of m! rankings over m objects. We therefore provide an efficient algorithm that exploits the structure of rankings. It runs in time O(m logm) per step and achieves a slightly improved (over generic algorithm) O(T 2/3) regret bound for PairwiseLoss, DCG and Precision@k.\nFor several measures, their normalized versions are considered. For example, the normalized versions of PairwiseLoss, DCG and Precision@k are called AUC [8], NDCG [11] and MAP respectively. We show an unexpected result for the normalized versions: they do not admit sub-linear regret algorithms under top-1 feedback. This is despite the fact that the opposite is true for their unnormalized counterparts! Intuitively, the normalization makes it hard to construct an unbiased estimator of the (unobserved) relevance vector. Surprisingly, we are able to translate this intuitive hurdle into a provable impossibility. Finally, we present some preliminary experiments to explore the performance of our efficient algorithm and compare its regret to its full information counterpart."
    }, {
      "heading" : "2 Notation and Preliminaries",
      "text" : "We have a fixed set of m objects numbered {1, 2, . . . ,m}. A permutation σ gives a mapping from objects to their ranks and its inverse σ−1 gives a mapping from ranks back to objects. Thus, σ(i) = j means object i is placed at position j while σ−1(i) = j means object j is placed at position i. For a binary relevance vector r ∈ {0, 1}m, r(i) indicates relevance level of object i. We denote {1, . . . , n} by [n]. The learner can choose from m! actions (permutations) whereas nature/adversary can choose from 2m outcomes (relevance vectors, when relevance levels are restricted to binary). The learner’s (resp. adversary’s) actions are denoted by {σi : i ∈ [m!]} (resp. {ri : i ∈ [2n]}). We sometimes interchangeably denote action σi as i (resp. ri as i). With this convention, σ(i) is a number but σi is a permutation. Also, a vector can be row or column vector depending on context. At round t, the learner outputs a permutation (ranking) σt of the objects (possibly using some internal randomization, based on feedback history so far). The user looks at the list and generates a relevance vector rt which indicates relevance level of each object. The quality of σt is judged against rt by a ranking loss RL. Crucially, only the relevance of the top ranked object (i.e., rt(σ −1 t (1))) is revealed to the learner at end of round t. Thus, the learner gets to know neither rt (full information problem) nor RL(σt, rt) (bandit problem). The objective of the learner is to minimize the expected regret with respect to best permutation in hindsight (when RL is a gain, not loss, we need to negate the quantity below):\nEσ1,...,σT [ T∑ t=1 RL(σt, rt) ] −min σ T∑ t=1 RL(σ, rt). (1)\nWe consider binary relevance but many of our techniques should easily extend to multi-graded relevance provided the performance measure has the right form.\nPrecisely define minimax regret here: What is the precise definition? What is the precise technical formula?"
    }, {
      "heading" : "3 Ranking Measures",
      "text" : "We consider ranking measures which can be expressed in the form f(σ) · r, where the function f : Rm → Rm is composed of m copies of a univariate, monotonic, scalar valued function. Thus, f(σ) = [fs(σ(1)), fs(σ(2)), . . . , f s(σ(m))], where fs : R → R. Monotonic (increasing) means fs(σ(i)) ≥ fs(σ(j)), whenever σ(i) > σ(j), ∀ i, j. Monotonic (decreasing) is defined similarly. The following popular ranking measures can be expressed in the form f(σ) · r.\nPairwiseLoss & SumLoss: PairwiseLoss is defined as: PL(σ, r) = ∑m\ni=1 ∑m j=1 1(σ(i) <\nσ(j))1(r(i) < r(j)). PairwiseLoss cannot be directly expressed in the form of f(σ) · r. Instead, we consider SumLoss, defined as: SumLoss(σ, r) = ∑m i=1 σ(i) r(i). SumLoss has the form f(σ) · r, where f(σ) = σ. It has been shown in [2] that SumLoss differs from PairwiseLoss only by an r-dependent constant and hence the regret under the two measures are equal:\nT∑ t=1 PL(σt, rt)− T∑ t=1 PL(σ, rt) =\nT∑ t=1 SumLoss(σt, rt)− T∑ t=1 SumLoss(σ, rt).\n(2)\nDiscounted Cumulative Gain: DCG, which admits non-binary relevance vectors, is defined as: DCG(σ, r) = ∑m\ni=1 2r(i)−1\nlog2(1+σ(i)) and becomes\n∑m i=1 r(i) log2(1+σ(i))\nfor r(i) ∈ {0, 1}. Thus, for binary relevance, DCG(σ, r) has the form f(σ) · r, where f(σ) = [ 1log2(1+σ(1)) , 1 log2(1+σ(2)) , . . . , 1log2(1+σ(m)) ].\nPrecision@k Gain: Precision@k is defined as Prec@k(σ, r) = ∑m\ni=1 1(σ(i) ≤ k) r(i). Precision@k can be written as f(σ) · r where f(σ) = [1(σ(1) < k), . . . ,1(σ(m) < k)]. Our focus is on k ≥ 2, since for k = 1, top-1 feedback is actually the same as full information feedback, for which efficient algorithms exist.\nNormalized measures are not of the form f(σ) · r: PairwiseLoss, DCG and Precion@k are unnormalized versions of popular ranking measures, namely, Area Under Curve (AUC), Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP) respectively. None of the latter can be expressed in the form f(σ) · r.\nNDCG: NDCG(σ, r) = 1Z(r) ∑m i=1 2r(i)−1 log2(1+σ(i)) and becomes 1Z(r) ∑m i=1 r(i) log2(1+σ(i))\nfor r(i) ∈ {0, 1}. Here Z(r) = max\nσ\n∑m i=1 2r(i)−1 log2(1+σ(i)) is the normalizing factor. It can be clearly seen that\nNDCG(σ, r) = f(σ) · g(r), where f(σ) is same as DCG but g(r) = rZ(r) is non-linear in r. MAP: MAP is a gain function and is defined as: MAP (σ, r) = 1‖r‖1 ∑m i=1 ∑m j=1 1(r(i) < r(j))1(σ(i) < σ(j)). It can be clearly seen that MAP cannot be expressed in the form f(σ) · r. MAP: AUC is a loss function and is defined as: AUC(σ, r) = 1N(r) ∑m i=1 ∑m j=1 1(σ(i) <\nσ(j))1(r(i) < r(j)), where N(r) = ( ∑m i=1 1(r(i) = 1)) ∗ (m − ∑m\ni=1 1(r(i) = 1)). It can be clearly seen that AUC cannot be expressed in the form f(σ) · r.\nAll subsequent results will be for binary valued relevance vectors, unless stated otherwise."
    }, {
      "heading" : "4 Summary of Results",
      "text" : "With notations fixed, we now summarize our main results here before delving into technical details. The regret bounds are over time horizon T , with learner playing against an oblivious adversary. Unless otherwise stated, all proofs and extensions are given in the appendix.\nResult 1: The minimax regret under DCG and PairwiseLoss (and hence SumLoss) is Θ(T 2/3). Result 2: An efficient algorithm, with running time O(m logm) per step, achieves the minimax regret under DCG and PairwiseLoss and also has a regret of O(T 2/3) for Precision@k. The precise minimax regret under Precision@k, k ≥ 2, remains an open issue.\nResult 3: The minimax regret for any of the normalized versions – NDCG, MAP and AUC – is Θ(T ). Thus, there is no algorithm that achieves sublinear regret for the normalized measures.\nResult 4: The minimax regret, both for DCG and NDCG, does not change (i.e., remains Θ(T 2/3) and Θ(T ) respectively) when we consider non-binary, multi-graded relevance vectors.\nWe re-emphasize that the relevance vectors are generated adversarially, without any stochastic assumption. Had the relevance vectors been stochastic in nature, the results would have held; however, efficient algorithm already exists in stochastic case [5] ."
    }, {
      "heading" : "5 Relevant Definitions from Partial Monitoring",
      "text" : "We develop all results in context of SumLoss. We then extend the results to other ranking measures. Our main results on regret bounds build on some of the theory for abstract partial monitoring games\ndeveloped in [4, 10]. For ease of understanding, we reproduce the relevant notations and definitions in context of SumLoss.\nLoss and Feedback Matrices: The online learning game with the SumLoss measure and feedback being relevance of top ranked object, can be expressed in form of a pair of loss matrix and feedback matrix. The loss matrix L is an m!×2m dimensional matrix, with rows indicating the learner’s actions (permutations) and columns representing adversary’s actions (relevance vectors). The entry in cell (i, j) of L indicates loss suffered when learner plays action i (i.e., σi) and adversary plays action j (i.e., rj), that is, Li,j = σi · rj = ∑m k=1 σi(k)rj(k). The feedback matrix H has same dimension as loss matrix, with entry in cell {i, j} being the relevance of top ranked object, i.e., Hi,j = rj(σ −1 i (1)). When the learner plays action σi and adversary plays action rj , the true loss is Li,j , while the feedback received is Hi,j . Table 1 and 2 illustrate the matrices, with number of objects m = 3. In both the tables, the permutations indicate rank of each object and relevance vector indicates relevance of each object. For example, σ5 = 312 means object 1 is ranked 3, object 2 is ranked 1 and object 3 is ranked 2. r5 = 100 means object 1 has relevance level 1 and other two objects have relevance level 0. Also, L3,4 = σ3 · r4 = ∑3 i=1 σ(i)r(i) = 2 ·0 + 1 ·1 + 3 ·1 = 4; H3,4 = r4(σ −1 3 (1)) = r4(2) = 1. Other entries are computed similarly. Let `i ∈ R2 m denote row i of L. Let ∆ be the probability simplex in R2 m , i.e., ∆ = {p ∈ R2m :\n∀ 1 ≤ i ≤ 2m, pi ≥ 0, ∑ pi = 1}. The following definitions, given for abstract problems in [4], has been refined to fit our problem context.\nDefinition 1: Pareto-optimality Learner action i is called optimal under distribution p ∈ ∆, if `i ·p ≤ `j ·p, for all other learner actions 1 ≤ j ≤ m!, j 6= i. For every action i ∈ [m!], probability cell of i is defined as Ci = {p ∈ ∆ : action i is optimal under p}. If a non-empty cell Ci is 2m − 1 dimensional, then associated action i is called Pareto-optimal.\nNote that since entries in H are relevance levels of objects, there can be maximum of 2 distinct elements in each row of H, i.e., 0 or 1 (Assuming binary relevance).\nDefinition 2: Signal-matrices Signal matrix Si, associated with learner’s action σi, is a matrix with 2 rows and 2m columns, with each entry 0 or 1, i.e., Si ∈ {0, 1}2×2 m . The entries of ` th column of row 1 and 2 of Si are respectively: (Si)1,` = I(Hi,` = 0) and (Si)2,` = I(Hi,` = 1), where I is the indicator function and H is the feedback matrix.\nNote that by definitions of signal and feedback matrices, the 2nd row of Si (or 2nd column of S>i )) is precisely the ith row of H. The 1st row of Si (or 1st column of S > i )) is the (boolean) complement of ith row of H."
    }, {
      "heading" : "6 Minimax Regret for SumLoss",
      "text" : "The minimax regret of SumLoss will be established by showing that: a) SumLoss satisfies global observability, and b) it does not satisfy local observability."
    }, {
      "heading" : "6.1 Global Observability",
      "text" : "Definition 3: The condition of global observability holds, w.r.t. loss matrix L and feedback matrix H, if for every pair of learner’s actions {σi, σj}, it is true that `i− `j ∈ ⊕k∈[m!]Col(S>k ) (where Col refers to column space).\nThe global observability condition basically states that the (vector) loss difference between any pair of learner’s actions has to belong to the vector space spanned by columns of (transposed) signal matrices corresponding to all possible learner’s actions. We derive the following theorem on global observability for SumLoss (Proof in appendix).\nTheorem 1. The global observability condition, as per Definition 3., holds w.r.t. loss matrix L and feedback matrix H defined for SumLoss, for any m ≥ 1.\nProof. For some σa (learner’s action) and rb (adversary’s action), we have\nLa,b = σa · rb = m∑ i=1 σa(i)rb(i) 1 = m∑ j=1 j rb(σ −1 a (j)) 2 =\nm∑ j=1 j rb(σ̃ −1 j(a)(1)) 3 = m∑ j=1 j (S>σ̃j(a))rb,2.\nThus, we have `a =\n[La,1, La,2, . . . , La,2m ] = [Lσa,r1 , Lσa,r2 , . . . , Lσa,r2m ] = [ m∑ j=1 j (S>σ̃j(a))r1,2, m∑ j=1 j (S>σ̃j(a))r2,2, .., m∑ j=1 j (S>σ̃j(a))r2m ,2]\n4 = m∑ j=1 j (S>σ̃j(a)):,2\nEquality 4 shows that `a is in the column span of m of the m! possible (transposed) signal matrices, specifically in the span of the 2nd columns of those (transposed) m matrices. Hence, for all actions σa, it is true that `a ∈ ⊕k∈[m!]Col(S>k ), =⇒ la− lb ∈ ⊕k∈[m!]Col(S>k ), ∀ actions σa, σb.\n1: Equality 1 holds by the relation σa(i) = j ⇒ i = σ−1a (j). 2. Equality 2 holds because permutations are bijections. Thus, for a permutation σa and for every j ∈ [m], ∃ a permutation σ̃j(a), s.t. the object which is assigned rank j by σa is the same object assigned rank 1 by σ̃j(a), i.e, σ −1 a (j) = σ̃ −1 j(a)(1).\n3. In equality 3, (S>σ̃j(a))rb,2 indicates the rb th row and 2nd column of (transposed) signal matrix Sσ̃j(a) , corresponding to learner action σ̃j(a). Equality 3 holds because rb(σ̃ −1 j(a)(1)) is the entry in the row corresponding to action σ̃j(a) and column corresponding to action rb of H and from Definition 2.\n4. Equality 4 holds from the observation that for a particular j, [(S>σ̃j(a))r1,2, (S > σ̃j(a) )r2,2, . . . , (S > σ̃j(a) )r2m ,2]\nforms the 2nd column of (S>σ̃j(a)), i.e, (S > σ̃j(a) ):,2."
    }, {
      "heading" : "6.2 Local Observability",
      "text" : "Definition 4: Two Pareto-optimal (learner’s) actions i and j are called neighboring actions if Ci∩ Cj is a (2\nm−2) dimensional polytope (where Ci is probability cell of action σi). The neighborhood action set of two neighboring (learner’s) actions i and j is defined asN+i,j = {k ∈ [m!] : Ci∩Cj ⊆ Ck}.\nDefinition 5: A pair of neighboring (learner’s) actions i and j is said to be locally observable if `i − `j ∈ ⊕k∈N+i,jCol(S > k ). The condition of local observability holds if every pair of neighboring (learner’s) actions is locally observable. We now show that local observability condition fails for L,H under SumLoss. First, we propose the following lemmas characterizing Pareto-optimal actions and neighboring actions for SumLoss.\nLemma 2. For SumLoss, each learner’s action i is Pareto-optimal, where Pareto-optimality has been defined in Definition 1.\nLemma 3. A pair of learner’s actions {σi, σj} is a neighboring pair, if there is exactly one pair of objects, numbered {a, b}, whose positions differ in σi and σj. Moreover, a needs to be placed just before b in σi and b needs to placed just before a in σj.\nLemma 2 and 3 lead to following result.\nTheorem 4. The local observability condition, as per Definition 5, fails w.r.t. loss matrix L and feedback matrix H defined for SumLoss, already at m = 3."
    }, {
      "heading" : "6.3 Minimax Regret Bound",
      "text" : "We establish the minimax regret for SumLoss by combining results on global and local observability. First, we get a lower bound by combining our Theorem 4 with Theorem 4 in [4].\nCorollary 5. There exists an online game for SumLoss with top-1 feedback, such that for every online algorithm, there is an adversary strategy generating relevance vectors, that guarantees the following\nE [ T∑ t=1 SumLoss(σt, rt) ] −min σ T∑ t=1 SumLoss(σ, rt)\n= Ω(T 2/3)\n(3)\nwhere the expectation is taken w.r.t. randomized learner’s actions.\nAn immediate corollary of Theorem 1 and Theorem 3.1 in [7] gives an in-efficient algorithm (inspired by the algorithm originally given in [16]) obtaining O(T 2/3(log T )1/3) regret.\nCorollary 6. The algorithm in Figure 1 of [7] achieves O(T 2/3(log T )1/3) regret bound for SumLoss.\nThe results above establish that the minimax regret for SumLoss, under top-1 feedback model, is Θ̃(T 2/3) where Θ̃ hides log(T ) factors. However, the algorithm in [7] is intractable in our setting since the number of learner’s actions is exponential in number of objects m. The next section tackles the efficiency issue."
    }, {
      "heading" : "7 Efficient Algorithm for Obtaining Minimax Regret under Sum-",
      "text" : "Loss\nWe provide an efficient algorithm for getting an O(poly(m)T 2/3) regret bound for SumLoss. The per round running time of the algorithm is O(m logm).\nThe key idea that we use in our algorithm is to divide time into phases. Within each phase, we allot a small number of rounds for pure exploration (this lets us estimate the average relevance vector for that phase). Using the average vector as a full information vector for next phase, rest of the rounds in next phase follow the actions according to the distribution suggested by Follow the Perturbed Leader (FTPL) [13] (this is exploitation of previous experience). One of the key reasons for exploiting FTPL in our algorithm, instead of exponential weighing schemes, is that the structure of our problem allows distribution to be maintained implicitly over m objects, instead of explicitly maintaining distribution over m! arms, a prohibitively expensive step.\nOur algorithm is motivated by the reduction from bandit-feedback to full feedback given in [6]. However, the algorithm in [6] cannot be directly applied to our problem, because we are not in the bandit setting and hence do not know loss of any action. Further, the algorithm spends N rounds per phase to try out each of the N available actions – this is infeasible in our setting since N = m!.\nOur main theorem on regret of Algorithm 1 is as follows.\nAlgorithm 1 RankingwithTop-1Feedback (RTop-1F)\nT = Time horizon, K = No. of (equal sized) blocks, Time horizon divided into blocks {B1, . . . , BK}, randomization parameter . Here, Bi = {(i− 1)(T/K) + 1, (i− 1)(T/K) + 2, . . . , i(T/K)}. Initialize ŝ0 = 0, r̂0 = 0. For i = 1, . . . ,K\nUpdate ŝi = ŝi−1 + r̂i−1. Select m time points {i1, . . . , im} from block Bi, uniformly at random, without replacement. For t ∈ Bi\nIf t = ij ∈ {i1, . . . , im} Output any permutation σt which places jth object on top. Receive feedback on the jth object as rij (j). Else Sample pt ∈ [0, 1/ ]m. (Product of uniform distribution in each dimension). Output permutation σt = M(ŝi + pt) where M(y) = argmin\nσ σ · y.\nend for Set r̂i = [ri1(1), . . . , rim(m)].\nend for\nTheorem 7. The expected regret of SumLoss, obtained by applying Algorithm 1, with K = m−1/3T 2/3 and = √\n1 mK , and the expectation being taken over random learner’s actions σt, is\nE [ T∑ t=1 SumLoss(σt, rt) ] ≤min σ T∑ t=1 SumLoss(σt, rt)+\nO(m8/3T 2/3).\n(4)\nThe following simple but useful lemma is required to prove Theorem 7.\nLemma 8. Let the average of full relevance vectors over the time period {1, 2, . . . , t} be denoted as ravg1:t , that is, r avg 1:t = ∑t k=1\nrk t\n. Let {i1, i2, . . . , im} be m arbitrary time points, chosen uniformly at random, without replacement, from {1, . . . , t}. At time point ij, only the jth component of vector rij , i.e rij (j), becomes known, ∀j ∈ {1, . . . ,m}. Then the relevance vector r̂t = [ri1(1), . . . , rim(m)] is an unbiased estimator of ravg1:t ."
    }, {
      "heading" : "8 Regret Bounds for PairwiseLoss, DCG and Prec@k",
      "text" : "As shown in Eq. 2, the regret of SumLoss is same as regret of PairwiseLoss. Thus, SumLoss in Cor. 5 and Thm. 7 can be replaced by PairwiseLoss to get exactly same results on regret.\nAll the results of SumLoss can be extended to DCG. Moreover, the results can be extended even for discrete, non-binary relevance vectors. Thus, the minimax regret of DCG, when the\nadversary can take any discrete valued, non-negative relevance vector is Θ(T 2/3), which can be achieved by the efficient algorithm of Sec. 7 . The main differences between SumLoss and DCG are the following. The former is a loss function, the latter is a gain function. Also, f(σ) 6= σ in DCG (Def. in Sec.2 ) and when r ∈ {0, 1, . . . , n}m, DCG cannot be expressed as f(σ) · r, as is clear from definition in Sec. 3 . Nevertheless, DCG can be expressed as f(σ) · g(r), , where g(r) = [gs(r(1)), gs(r(2)), . . . , gs(r(m))], gs(i) = 2i − 1 is constructed from univariate, monotonic, scalar valued functions. Thus, there are minor differences in definitions and proofs of theorems for SumLoss and DCG. The structural properties of f(·), g(·) are key in extending results. For binary valued relevance vectors, Algorithm1 can be applied to DCG as is. For multi-graded relevance vector, the only thing that changes is that the relevance feedback is transformed via component functions of g(·).\nWe provide the extension of Theorem 7 for DCG. Let relevance vectors chosen by adversary be of level n + 1, i.e., r ∈ {0, 1, . . . , n}m. In practice, n is always less than 5; hence exp(n) is also small.\nTheorem 9. The expected regret of DCG, obtained by applying Algorithm 1 , with K = m−1/3T 2/3 and = √\n1 (2n−1)2mK , and the expectation being taken over random learner’s actions σt, is\nE [ T∑ t=1 DCG(σt, rt) ] ≥max σ T∑ t=1 DCG(σ, rt)−\nO((2n − 1)m5/3T 2/3)\n(5)\nIn case of binary relevance vector, the regret term is O(m5/3T 2/3). Moreover, since local observability fails, there is a matching Ω(T 2/3) lower bound.\nThe regret upper bounds we proved for SumLoss also easily extend to Precision@k. We have the following extension of Theorem 7.\nTheorem 10. The expected regret of Prec@k, obtained by applying algorithm 1, with K = m−1/3T 2/3 and = √\n1 mK , and the expectation being taken over random learner’s actions σt, is\nE [ T∑ t=1 Prec@k(σt, rt) ] ≥max σ T∑ t=1 Prec@k(σ, rt)−\nO(km2/3T 2/3)\n(6)\nHowever, Prec@k is independent of the order of objects in top k positions of ranked list. This changes the neighboring action claims. Therefore, the minimax regret of Prec@k remains an open question, since the local observability results do not directly apply to Prec@k."
    }, {
      "heading" : "9 Non-Existence of Sublinear Regret Bounds for NDCG, MAP",
      "text" : "and AUC\nAs stated in Sec. 3 , NDCG, MAP and AUC are normalized versions of measures DCG, Precision@k and PairwiseLoss. We have the following lemma for all these normalized ranking measures.\nLemma 11. The global observability condition, as per Definition 1, fails for NDCG, MAP and AUC.\nCombining the above lemma with Theorem 2 of [4], we conclude that there cannot exist any algorithm which has sublinear regret for any of the following measures: NDCG, MAP or AUC, with top-1 feedback.\nTheorem 12. There exists an online game for NDCG with top-1 feedback, such that for every online algorithm, there is an adversary strategy that guarantees the following\nmax σ T∑ t=1 NDCG(σ, rt)− E [ T∑ t=1 NDCG(σt, rt) ] = Ω(T ). (7)\nFurthermore, the same lower bound holds if NDCG is replaced by MAP or AUC.\nNote: In the NDCG case, allowing the adversary to play multigraded, and not just binary, relevance vectors only makes the adversary more powerful. So the lower bound above continues to apply."
    }, {
      "heading" : "10 Empirical Results",
      "text" : "We conducted simulation studies to compare regret rates of SumLoss and DCG, when feedback is received only for top ranked object (by applying Algorithm1 ) and full relevance vector is revealed at end of each round (by applying FPL of [13]). Relevance vectors were restricted to take binary values.\nWe simulated relevance vectors for a fixed set of 10 objects (m = 10). We initially fixed half of the objects to be relevant and other half irrelevant, as the true relevance vector. Then, binary valued relevance vectors for adversary were simulated by adding small Gaussian noise to the true relevance vector. Thus, there was mostly small variances among the relevance vectors, simulating the fact that, in real world, majority of users will agree on the relevance of most objects, with small differences. A total of T = 10000 relevance vectors were generated (simulating number of rounds).\nIn Algorithm 1 , since the average of the relevance vectors per block was estimated by uniform sampling according to Lemma 8, the code for the algorithm was run 10 times, with the same set of relevance vectors, for averaging under the algorithm’s randomization. Fig.1 shows (averaged over time) regret with top-1 feedback for SumLoss. Averaged over time means the cumulative regret upto time t was divided by t, for 1 ≤ t ≤ T . The figure clearly indicates that after the learning phase of the initial few iterations, the learner outputs mostly correct rankings, with the average regret going down to 0 at rate O(T−1/3). Fig.3 shows the same graph for DCG. It corroborates the fact that Algorithm1 can be applied to DCG to get same order of regret.\nFig.4 compares (averaged over time) regret, with top-1 and full information feedback respectively, for SumLoss. The comparison was done from 1000 iterations onwards, i.e., roughly after the learning phase of the learner. It can be clearly seen that average regret with full information goes down at rate faster (Θ(T−1/2)) than average regret with top-1 feedback (Θ(T−1/3)). Fig.4 shows the same comparison for DCG."
    }, {
      "heading" : "11 Conclusion",
      "text" : "We introduced a novel, interesting feedback model for online ranking of a fixed set of objects with diverse preferences. Our results are quite comprehensive as far as the T dependence is concerned. The only exception is Precision@k where the possibility of an O(T 1/2) regret algorithm remains open. Note that Precision@k is really peculiar since top-1 feedback is actually full feedback for k = 1.\nThe most interesting future extension of this work is to move beyond ranking fixed set of objects and considering different document lists associated with queries. This falls under the category of partial monitoring with side information. Very little relevant work has been done in the general setting and our current work can lay the foundations for interesting application in this field. Another extension is investigating whether an algorithm with sublinear regret can be defined for NDCG, MAP or AUC, when the regret is defined relative to some constant factor (larger than 1) times the best performance in hindsight."
    }, {
      "heading" : "12 Regret for SumLoss",
      "text" : ""
    }, {
      "heading" : "12.1 Proof of Lemma 2",
      "text" : "Proof. For any p ∈ ∆, we have `i · p = ∑2m j=1 pj (σi · rj) = σi · ( ∑2m\nj=1 pjrj) = σi · Er[r], where the expectation is taken w.r.t p (pj is the j th component of p). By dot product rule between 2 vectors, li ·p is minimized when ranking of objects according to σi and expected relevance of objects are in opposite order. That is, the object with highest expected relevance is ranked 1 and so on. Formally, li · p is minimized when Er[r(σ−1i (1)] ≥ Er[r(σ −1 i (2)] ≥ . . . ≥ Er[r(σ −1 i (m)].\nThus, for action i, probability cell is defined as Ci = {p ∈ ∆ : ∑2m j=1 pj = 1, Er[r(σ −1 i (1)] ≥\nEr[r(σ −1 i (2)] ≥ . . . ≥ Er[r(σ −1 i (m)]}. That is, for any p ∈ Ci, action i is optimal w.r.t p. Since Ci is obviously non-empty and it has only 1 equality constraint (hence 2m − 1 dimensional), action i is Pareto optimal.\nThe above holds true for all learner’s actions i."
    }, {
      "heading" : "12.2 Proof of Lemma 3",
      "text" : "Proof. From Lemma 2, we know that every one of learner’s actions are pareto-optimal and Ci, associated with action σi, has structure Ci = {p ∈ ∆ : ∑2m j=1 pj = 1, Er[r(σ −1 i (1)] ≥ Er[r(σ −1 i (2)] ≥ . . . > Er[r(σ −1 i (m)]}.\nLet σ−1i (k) = a, σ −1 i (k + 1) = b. Let it also be true that σ −1 j (k) = b, σ −1 j (k + 1) = a and σ−1i (n) = σ −1 j (n), ∀n 6= {k, k + 1}. This indicates the sufficient condition stated in Lemma. 3 for {σi, σj} to be neighbors. Then, Ci∩Cj = {p ∈ ∆ : ∑2m j=1 pj = 1, Er[r(σ −1 i (1)] ≥ . . . ≥ Er[r(σ −1 i (k)] = Er[r(σ −1 i (k+1)] ≥ . . . ≥ Er[r(σ−1i (m)]}. Hence, there are two equalities in the non-empty set Ci ∩ Cj and it is an (2m − 2) dimensional polytope. Hence condition of Definition 4. holds true and {σi, σj} are neighbors."
    }, {
      "heading" : "12.3 Proof of Theorem 4",
      "text" : "Proof. We will explicitly show that local observability condition fails by considering the case when number of objects is m = 3. Specifically, action pair {σ1, σ2}, in Table 1 and 2, are neighboring actions, using Lemma 3 . Moreover, from Lemma 2 , since every action σk is Pareto-optimal, every Ck is 2\nm − 1 dimensional. Since C1 ∩ C2 is 2m − 2 dimensional (from Def. 4), the neighborhood action set of actions {σ1, σ2} is precisely σ1 and σ2 and contains no other actions. By definition of signal matrices Sσ1 , Sσ2 and entries `1, `2 in table 1 and 2 , we have\nSσ1 = Sσ2 = [ 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ] `1 − `2 = [ 0 1 −1 0 0 1 −1 0\n] (8) It is clear that `1 − `2 /∈ Col(S>σ1). Hence, Definition 5 fails to hold."
    }, {
      "heading" : "13 Efficient Algorithm for Obtaining Regret",
      "text" : ""
    }, {
      "heading" : "13.1 Proof of Lemma 8",
      "text" : "Proof. We can write r̂t = ∑m j=1 rij (j)ej , where ej is the standard basis vector along coordinate j.\nThen Ei1,...,im(r̂t) = ∑m j=1Eij (rij (j)ej) = ∑m j=1 ∑t k=1\nrk(j)ej t = ravg1:t ."
    }, {
      "heading" : "13.2 Proof of Theorem 7",
      "text" : "Proof. Instead of top-1 feedback, assume that at each round, after learner reveals his action, the full relevance vector is revealed to the learner. Then an O( √ T ) expected regret for SumLoss can be obtained by applying FTPL (Follow the perturbed leader), in the following manner:\nAt end of every round t, the full relevance vector generated by the adversary is revealed. The relevance vectors are accumulated as r1:t = r1:t−1 + rt, where r1:s = ∑s i=1 ri. A learner’s action (permutation) for round t + 1 is generated by solving M(r1:t + pt), where pt ∈ [0, η]m and η is algorithmic (randomization) parameter. It should be noted that M(y) = argmin\nσ σ · y is simply\nsorting of y since f(σ) = σ is a monotone function as defined in Sec.3 . The key idea is that FTPL implicitly maintains a distribution over m! actions (permutations) at beginning of each round, by randomly perturbing the scores of only m objects: score of each object is sum of (deterministic) accumulated relevance so far and (random) uniform value from [0, η]. Thus, it bypasses having to maintain explicit weight on each of m! arms, which is computationally prohibitive. This key property which introduces efficiency in our algorithm is in contrast to the general algorithms based on exponential weights, which have to maintain explicit weights, based on accumulated history, on each action and randomly select an action based on weights.\nNow let us look at a variant of the full information problem. The (known) time horizon T is divided into K blocks, i.e, {B1, . . . , BK}, of equal size T/K. Here, Bi = {(i − 1)(T/K) + 1, (i − 1)(T/K)+2, (i−1)T/K+3, . . . , i(T/K)}. While operating in a block, the relevance vectors within the block are accumulated, but not used to generate learner’s actions like in the full information version. Assume at the start of block Bi, there was some relevance vector r\ni. Then at each time point in the block, a fresh p ∈ [0, η]m is sampled and M(ri+p) is solved to generate permutation for next time point. At the end of a block, the average of the accumulated relevance vectors (ravg) for the block is used for updation, as ri + ravg, to get ri+1 for the next block. The process is repeated for each block. At the beginning of the first block, r1 = {0}m.\nFormally, let the FTPL have an implicit distribution ρi (over the permutations) at the beginning of block Bi. That is ρi ∈ ∆, where ∆ is the probability simplex over m! actions. Sampling a permutation using ρi at each time point of the block Bi means sampling a fresh p ∈ [0, η]m at every time point t and solving M(s1:(i−1) +p), where s1:(i−1) = ∑i−1 j=1 sj and sj is the average of relevance vectors of block Bj . Since action σ is generated according to distribution ρ, and in block k, distribution ρk is fixed,\nwe have Eρk ∑ t∈[Bk] SumLoss(σt, rt) = ∑ t∈[Bk] ρk · [SumLoss(σ1, rt), . . . , SumLoss(σm!, rt)] Thus,\nthe total expected loss of this variant of the full information problem is:\nEρ1,...,ρK T∑ t=1 SumLoss(σt, rt) =\nEρ1,...,ρK K∑ k=1 ∑ t∈[Bk] SumLoss(σt, rt) (9)\n= K∑ k=1 ∑ t∈[Bk] ρk · [SumLoss(σ1, rt), . . . , SumLoss(σm!, rt)]\n= K∑ k=1 ∑ t∈[Bk] ρk · [σ1 · rt, . . . , σm! · rt)]\n= T\nK K∑ k=1 ρk · [σ1 · sk, . . . , σm! · sk]\n= T\nK K∑ k=1 EρkSumLoss(σk, sk)\n= T\nK Eρ1,...,ρk K∑ k=1 SumLoss(σk, sk) (10)\nwhere sk = ∑ t∈[Bk] rt\nT/K . This clearly implies that the variant of the full information problem over\nT rounds reduces to full information problem in K rounds, where at end of every round k ∈ [K], ρk is being updated to ρk+1 by updating s1:(k−1) to s1:k.\nBy the regret bound of FTPL, for K rounds of full information problem, with = √ D/RAK,\nwe have:\nEρ1,...,ρk K∑ k=1 SumLoss(σk, sk)\n≤ min σ K∑ k=1 SumLoss(σ, sk) + 2 √ DRAK\n= min σ T∑ t=1 σ · sk + 2 √ DRAK\n= min σ T∑ t=1 σ · rt T/K + 2 √ DRAK\n(11)\nwhere D,R,A are parameters implicit to the loss under consideration(To be detailed later).\nNow, since min σ\n∑T t=1 σ ·\nrt T/K = min σ 1 T/K\n∑T t=1 SumLoss(σ, rt), combining Eq. 10 and Eq.\n11, we get:\nEρ1,...,ρK T∑ t=1 SumLoss(σt, rt)\n≤ min σ T∑ t=1 SumLoss(σ, rt) + 2 T K √ DRAK.\n(12)\nHowever, in our top-1 feedback model, the learner does not get to see the full relevance vector at each round. Thus, we form the unbiased estimator ŝi of si, using Lemma. 8 . That is, at start of each block, we choose m time points uniformly at random, and at those time points, we output a random permutation which places each object, in turn, at top. At the end of the block, we form the relevance vector ŝ which is the unbiased estimator of s. Note that using ŝi instead of true si makes the distributions ρi themselves random. But significantly, ρk is dependent only on information received upto the beginning of block k and is independent of the information collected in the block. Thus, for block k, we have:\nEρk/ŝ1,ŝ2,..,ŝk−1 ∑ t∈[Bk] SumLoss(σt, rt)\n= T\nK Eρk/ŝ1,ŝ2,..,ŝk−1SumLoss(σk, sk) (From Eq.10)\n= T\nK Eρk/ŝ1,ŝ2,..,ŝk−1EŝkSumLoss(σk, ŝk) (Linearity property)\n(13)\nCrucially, since random distribution ρk is independent of ŝk, the expectations are exchangeable and hence we have the following equation\nEŝEρ/ŝ[ T∑ t=1 SumLoss(σt, rt)]\n= T\nK EŝEρ/ŝ[ K∑ k=1 SumLoss(σk, ŝk)] (From Eq.10)\n≤ T K {Eŝ[min σ K∑ k=1 σi · ŝk] + 2 √ DRAK} (From Eq.11)\n≤ T K {min σ K∑ k=1 σi · sk + 2 √ DRAK} (Jensen’s Inequality)\n≤ min σ T∑ t=1 σi · rt + 2 T K √ DRAK\n= min σ T∑ t=1 SumLoss(σ, rt) + 2 T K √ DRAK\n(14)\nHowever, since in each block, m rounds are reserved for exploration, which do not follow from distribution ρ, the total expected loss is the sum of expected loss from exploitation and exploration.\nExploration leads to an extra regret of RmK, where R, as has been stated before, is an implicit parameter depending on the loss under consideration. The extra regret is because loss in each of the exploration rounds is ≤ R (explained later), total of m rounds in each block and total of K blocks. Thus, overall regret is:\nE[ T∑ t=1 SumLoss(σt, rt)]\n≤ EŝEρ/ŝ[ T∑ t=1 SumLoss(σt, rt)] +RmK\n⇒E[ T∑ t=1 SumLoss(σt, rt)]−min σ T∑ t=1 SumLoss(σ, rt)\n≤ RmK + 2 T K\n√ DRAK\n(15)\nNow, optimizing over K, we get K = (DA/R)1/3(T/m)2/3, and\nE[ T∑ t=1 SumLoss(σt, rt)] ≤ min σ T∑ t=1 SumLoss(σ, rt)\n+O(m1/3R2/3(DA)1/3T 2/3)\n(16)\nNow, by definition of D, R and A from [13], D is upper bound on `1 norm on vectors in learner’s action space, R is upper bound on dot product of vectors in learner’s and adversary’s action space and A is upper bound on `1 norm on vectors in adversary’s action space. Thus, for SumLoss, we have\nD = ∑m\ni=1 σ(i) = O(m 2), R = ∑m i=1 σ(i)r(i) = O(m 2), A = ∑m\ni=1 r(i) = O(m). Plugging in these values gives us result of Theorem. 7 . R, as defined in [13], can be seen to be upper bound on loss σ · r, for any σ and r."
    }, {
      "heading" : "14 Regret Bounds for DCG and Prec@k",
      "text" : ""
    }, {
      "heading" : "14.1 Extension of Results of SumLoss to DCG",
      "text" : "We give pointers in the direction of proving the following results: a) Local observability condition fails to hold for DCG which, in combination with Theorem 4 in [4], proves that the lower bound on regret of DCG is Ω(T 2/3). b)The efficient algorithm of Sec.7 applies to DCG, with regret of O(T 2/3). Thus, the minimax regret of DCG is Θ(T 2/3). All results are applicable to non-binary relevance vectors. This allows us to skip the proof of global observability, which is complicated for non-binary relevance vectors.\nLet adversary be able to choose r ∈ {0, 1, . . . , n}m. Then, from definition of DCG in Sec.3 , it is clear DCG=f(σ) · g(r). f(σ) and g(r) has already been defined for DCG. Both are composed of m copies of univariate, monotonic, scalar valued function, where for f(·), it is monotonically decreasing whereas for g(·), it is increasing.\nWith slight abuse of notations, the Loss matrix L implicitly means Gain matrix, where entry in cell {i, j} of L is f(σi) · g(rj). The feedback matrix H remains the same. In Definition 1., learner action i is optimal if `i · p ≥ `j · p, ∀j 6= i.\nIn Definition 2., the maximum number of distinct elements that can be in a row of H is n+ 1. The signal matrix now becomes Si ∈ {0, 1}(n+1)×2 m , where (Si)k,` = 1(Hi,` = k − 1).\nLocal Observability: In Lemma.2 , proposed for SumLoss, `i · p equates to f(σ) · Er[g(r)]. From definition of DCG, and from the structure and properties of f(·), g(·) , it is clear that `i · p is maximized under the same condition, i.e, Er[r(σ −1 i (1)] ≥ Er[r(σ −1 i (2)] ≥ . . . ≥ Er[r(σ −1 i (m)]. Thus, all actions are pareto-optimal Careful observation of Lemma.3 shows that it is directly applicable to DCG, in light of extension of Lemma.2 to DCG. Finally, just like in SumLoss, simple calculations with m = 3 and n = 1, in light of Lemma.2 and 3, show that local observability condition fails to hold, Implementation of the Efficient Algorithm: The only change in Algorithm.1 which allows extension to DCG with non-binary relevance is that relevance values will enter into the algorithm via the transformation gs(·). That is, every component of relevance vector r, i.e. r(i), will become 2r(i) − 1. Every operation of Algorithm.1 will happen on the transformed relevance vectors. It is very easy to see that every step in analysis of the algorithm will be valid by just considering the transformed relevance vectors to be some new relevance vectors with magnified relevance values. The fact that r was binary valued in SumLoss played no role in the analysis of the algorithm or Lemma.8 . The properties which allowed the extension was that g(·) is composed of univariate, monotonic, scalar valued functions andDCG(σ, r) is a linear function of f(σ) and g(r).\nIt is also interesting to note that M(y) = argmax σ f(σ) · y = argmin σ σ · y. Thus, no changes in the algorithm is required, other than simple transformation of relevance values.\nProof of Theorem 9: Following the proof of Theorem 7 , modified for DCG, Eq.16 gives (for DCG):\nE[ T∑ t=1 DCG(σt, rt)] ≥max σ T∑ t=1 DCG(σ, rt)\n−O(m1/3R2/3(DA)1/3T 2/3)\nFor DCG , D = ∑m\ni=1 f s(σ(i)) = O(m), R = ∑m i=1 f\ns(σ(i))gs(r(i)) = O(m(2n − 1)), A =∑m i=1 g s(r(i)) = O(m(2n − 1)) and hence the regret is O((2n − 1)m5/3T 2/3)."
    }, {
      "heading" : "14.2 Extension of Results of SumLoss to Prec@k",
      "text" : "Since Prec@k= f(σ) · r, with f(·) having properties enlisted in Sec. 3 , all results of SumLoss trivially extend to Prec@k, except results on local observability. The reason is that while f(·) of SumLoss is strictly monotonic, f(·) of Prec@k is monotonic but not strict. The gain function depends only on the objects in the top-k position of the ranked list, irrespective of the order. A careful analysis shows that Lemma 3 fails to extend to the case of Prec@k. Thus, we cannot define the neighboring action set of the Pareto optimal action pairs, and hence cannot prove or disprove local observability. The structure of neighbors in Prec@k remains an open question.\nHowever, the non-strict monotonicity of Prec@k is required for solving M(y) = argmax σ f(σ) · y efficiently.\nProof of Corollary.10: Following the proof of Theorem.7 , modified for Prec@k, Eq.16 gives (for Prec@k):\nE[ T∑ t=1 Prec@k(σt, rt)] ≥max σ T∑ t=1 Prec@k(σ, rt)\n−O(m1/3R2/3(DA)1/3T 2/3) for Prec@k, D = ∑k\ni=1 f s(σ(i)) = O(k), R = ∑m i=1 f s(σ(i))gs(r(i)) = O(k), A = ∑m\ni=1 r(i) = O(m) and hence the regret is O(km2/3T 2/3)."
    }, {
      "heading" : "15 Non-Existence of sublinear regret bounds for NDCG, MAP",
      "text" : "and AUC- Extensions\nWe show via simple calculations that for the case m = 3, global observability condition fails to hold for NDCG, when relevance vectors are restricted to take binary values. The intuition behind failure to satisfy global observability condition is that the NDCG(σ, r) = f(σ) · g(r), where where g(r) = r/Z(r) (See Sec.3 ). Thus, g(·) cannot be by univariate, scalar valued functions. This makes it impossible to write the difference between two rows as linear combination of columns of (transposed) signal matrices.\nProof. The following calculations can be easily done: The first and last row of Table 1 , when calculated for NDCG, is:\n`σ1 =[1, 1/2, log22/log23, (1 + log23/2)/(1 + log23),\n1, 3/(2(1 + 1/log23)), 1, 1]\n`σ6 =[1, 1, log22/log23, 1, 1/2, 3/(2(1 + 1/log23)),\n(1 + log23/2)/(1 + log23), 1]\nWe remind once again that NDCG is a gain function, as opposed to SumLoss. The difference between the two vectors is:\n`σ1 − `σ6 =[0, 1/2, 0, log23/(2(1 + log23)), − 1/2, 0,−log23/(2(1 + log23)), 0].\nThe signal matrices are same as SumLoss :\nSσ1 = Sσ2 = [ 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ]\nSσ3 = Sσ5 = [ 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 ]\nSσ4 = Sσ6 = [ 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 ]\nIt can now be easily checked that `σ1 − `σ6 does not lie in the (combined) column span of the (transposed) signal matrices.\nWe show similar calculations for MAP and AUC: MAP: We once again take m = 3. The first and last row of Table 1 , when calculated for MAP, is:\n`σ1 = [1, 1/3, 1/2, 7/12, 1, 5/6, 1, 1] `σ6 = [1, 1, 1/2, 1, 1/3, 5/6, 7/12, 1]\nLike NDCG, MAP is also a gain function. The difference between the two vectors is:\n`σ1 − `σ6 = [0,−2/3, 0,−5/12, 2/3, 0, 5/12, 0].\nThe signal matrices are same SumLoss :\nSσ1 = Sσ2 = [ 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ]\nSσ3 = Sσ5 = [ 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 ]\nSσ4 = Sσ6 = [ 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 ] It can now be easily checked that `σ1 − `σ6 does not lie in the (combined) column span of the\n(transposed) signal matrices.\nAUC: For AUC, we will show the calculations for m = 4. This is because global observability does hold with m = 3, as the normalizing factor for all relevance vectors with mixture of 0 and 1 is same. The normalizing factor changes from m = 4 onwards; hence global observability fails.\nTable.1 will be extended since m = 4. Instead of illustrating the full table, we point out the important facts about the loss matrix table with m = 4 for AUC.\nThe 24 relevance vectors heading the columns are: r1 = 0000, r2 = 0001, r3 = 0010, r4 = 0100, r5 = 10000, r6 = 0011, r7 = 0101, r8 = 1001, r9 = 0110, r10 = 1010, r11 = 1100, r12 = 0111, r13 = 1011, r14 = 1101, r15 = 1110, r16 = 1111.\nWe will calculate the losses of 1st and last (24th) action, where σ1 = 1234 and σ24 = 4321.\n`σ1 = [0, 1, 2/3, 1/3, 0, 1, 3/4, 1/2, 1/2, 1/4, 0, 1, 2/3, 1/3, 0, 0] `σ24 = [0, 0, 1/3, 2/3, 1, 0, 1/4, 1/2, 1/2, 3/4, 1, 0, 1/3, 2/3, 1, 0]\nAUC, like SumLoss, is a loss function. The difference between the two vectors is:\n`σ1 − `σ24 = [0, 1, 1/3,−1/3,−1, 1, 1/2, 0, 0,−1/2,−1, 1, 1/3,−1/3,−1, 0].\nThe signal matrices for AUC with m = 4 will be slightly different. This is becuase there are 24 signal matrices, corresponding to 24 actions. However, every 6 action will have same signal matrix. That is, all 6 permutations which places object 1 first will have same signal matrix, 6 permutations which places object 2 first will have same signal matrix and so on. For simplicity, we denote the signal matrices as {S1, S2, S3, S4}, where Si corresponds to signal matrix where object i is placed at top :\nS1 = [ 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 ]\nS2 = [ 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 1 ]\nS3 = [ 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 ]\nS4 = [ 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 ] It can now be easily checked that `σ1 − `σ24 does not lie in the (combined) column span of\nS1, S2, S3, S4 ."
    } ],
    "references" : [ {
      "title" : "Diversifying search results",
      "author" : [ "Rakesh Agrawal", "Sreenivas Gollapudi", "Alan Halverson", "Samuel Ieong" ],
      "venue" : "In Proceedings of the Second ACM International Conference on Web Search and Data Mining,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Improved bounds for online learning over the permutahedron and other ranking polytopes",
      "author" : [ "Nir Ailon" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Modern information retrieval, volume 463",
      "author" : [ "R. Baeza-Yates", "B. Ribeiro-Neto" ],
      "venue" : "ACM press New York.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1999
    }, {
      "title" : "Partial monitoring–classification, regret bounds, and algorithms, 2013",
      "author" : [ "Gábor Bartók", "Dean Foster", "Dávid Pál", "Alexander Rakhlin", "Csaba Szepesvári" ],
      "venue" : "Best viewed in color. Code available on request",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Minimax regret of finite partial-monitoring games in stochastic environments",
      "author" : [ "Gábor Bartók", "Dávid Pál", "Csaba Szepesvári" ],
      "venue" : "Journal of Machine Learning Research-Proceedings Track,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "From external to internal regret",
      "author" : [ "Avrim Blum", "Yishay Mansour" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Regret minimization under partial monitoring",
      "author" : [ "Nicolo Cesa-Bianchi", "Gábor Lugosi", "Gilles Stoltz" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "AUC optimization vs. error rate minimization",
      "author" : [ "Corinna Cortes", "Mehryar Mohri" ],
      "venue" : "In NIPS, page 10,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "On the consistency of ranking algorithms",
      "author" : [ "John C Duchi", "Lester W Mackey", "Michael I Jordan" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning (ICML-",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "No internal regret via neighborhood watch",
      "author" : [ "Dean P Foster", "Alexander Rakhlin" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Cumulated gain-based evaluation of IR techniques",
      "author" : [ "K. Järvelin", "J. Kekäläinen" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "IR evaluation methods for retrieving highly relevant documents",
      "author" : [ "Kalervo Järvelin", "Jaana Kekäläinen" ],
      "venue" : "In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2000
    }, {
      "title" : "Efficient algorithms for online decision problems",
      "author" : [ "Adam Kalai", "Santosh Vempala" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "Combinatorial partial monitoring game with linear feedback and its applications",
      "author" : [ "Tian Lin", "Bruno Abrahao", "Robert Kleinberg", "John Lui" ],
      "venue" : "In Proceedings of ICML 2014 Cycle",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Letor: Benchmark dataset for research on learning to rank for information retrieval",
      "author" : [ "Tie-Yan Liu", "Jun Xu", "Tao Qin", "Wenying Xiong", "Hang Li" ],
      "venue" : "In Proceedings of SIGIR 2007 workshop on learning to rank for information retrieval,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Discrete prediction games with arbitrary feedback and loss",
      "author" : [ "Antonio Piccolboni", "Christian Schindelhauer" ],
      "venue" : "In Computational Learning Theory,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    }, {
      "title" : "Redundancy, diversity and interdependent document relevance",
      "author" : [ "Filip Radlinski", "Paul N Bennett", "Ben Carterette", "Thorsten Joachims" ],
      "venue" : "In ACM SIGIR Forum,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Learning occurs in an online setting: at each round, the system outputs a ranked list of the objects and the quality of ranking is measured by one of several popular ranking measures (like DCG [12] or MAP [3]), taking into account the users preferences encoded as relevance vectors.",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 2,
      "context" : "Learning occurs in an online setting: at each round, the system outputs a ranked list of the objects and the quality of ranking is measured by one of several popular ranking measures (like DCG [12] or MAP [3]), taking into account the users preferences encoded as relevance vectors.",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 16,
      "context" : "The idea of ranking for diverse preferences has been motivated from a branch of work, sometimes called “ranking with diversity” [18, 17, 1] .",
      "startOffset" : 128,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "The idea of ranking for diverse preferences has been motivated from a branch of work, sometimes called “ranking with diversity” [18, 17, 1] .",
      "startOffset" : 128,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "The appropriate framework to study the problem is that of partial monitoring [7].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 13,
      "context" : "A very recent paper shows another practical application of the idea where feedback is neither full information nor bandit [14].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "Recent advances in the classification of partial monitoring games tell us that the minimax regret, in an adversarial setting, is governed by a property of the loss and feedback functions called observability [4, 10].",
      "startOffset" : 208,
      "endOffset" : 215
    }, {
      "referenceID" : 9,
      "context" : "Recent advances in the classification of partial monitoring games tell us that the minimax regret, in an adversarial setting, is governed by a property of the loss and feedback functions called observability [4, 10].",
      "startOffset" : 208,
      "endOffset" : 215
    }, {
      "referenceID" : 8,
      "context" : "We instantiate these general observability notions for the top-1 feedback case and prove that, for some ranking measures, namely PairwiseLoss [9], DCG and Precision@k [15], global observability holds.",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 14,
      "context" : "We instantiate these general observability notions for the top-1 feedback case and prove that, for some ranking measures, namely PairwiseLoss [9], DCG and Precision@k [15], global observability holds.",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "For example, the normalized versions of PairwiseLoss, DCG and Precision@k are called AUC [8], NDCG [11] and MAP respectively.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : "For example, the normalized versions of PairwiseLoss, DCG and Precision@k are called AUC [8], NDCG [11] and MAP respectively.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "It has been shown in [2] that SumLoss differs from PairwiseLoss only by an r-dependent constant and hence the regret under the two measures are equal:",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "Had the relevance vectors been stochastic in nature, the results would have held; however, efficient algorithm already exists in stochastic case [5] .",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "developed in [4, 10].",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "developed in [4, 10].",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "The following definitions, given for abstract problems in [4], has been refined to fit our problem context.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "First, we get a lower bound by combining our Theorem 4 with Theorem 4 in [4].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "1 in [7] gives an in-efficient algorithm (inspired by the algorithm originally given in [16]) obtaining O(T 2/3(log T )1/3) regret.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 15,
      "context" : "1 in [7] gives an in-efficient algorithm (inspired by the algorithm originally given in [16]) obtaining O(T 2/3(log T )1/3) regret.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "The algorithm in Figure 1 of [7] achieves O(T 2/3(log T )1/3) regret bound for SumLoss.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 6,
      "context" : "However, the algorithm in [7] is intractable in our setting since the number of learner’s actions is exponential in number of objects m.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "Using the average vector as a full information vector for next phase, rest of the rounds in next phase follow the actions according to the distribution suggested by Follow the Perturbed Leader (FTPL) [13] (this is exploitation of previous experience).",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 5,
      "context" : "Our algorithm is motivated by the reduction from bandit-feedback to full feedback given in [6].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "However, the algorithm in [6] cannot be directly applied to our problem, because we are not in the bandit setting and hence do not know loss of any action.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "Combining the above lemma with Theorem 2 of [4], we conclude that there cannot exist any algorithm which has sublinear regret for any of the following measures: NDCG, MAP or AUC, with top-1 feedback.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 12,
      "context" : "We conducted simulation studies to compare regret rates of SumLoss and DCG, when feedback is received only for top ranked object (by applying Algorithm1 ) and full relevance vector is revealed at end of each round (by applying FPL of [13]).",
      "startOffset" : 234,
      "endOffset" : 238
    } ],
    "year" : 2017,
    "abstractText" : "We consider a setting where a system learns to rank a fixed set of m items. The goal is produce a good ranking for users with diverse interests who interact with the system for T rounds in an online fashion. We consider a novel top-1 feedback model for this problem: at the end of each round, the relevance score for only the top ranked object is revealed to the system. However, the performance of the system is judged on the entire ranked list. We provide a comprehensive set of results regarding learnability under this challenging setting. For popular ranking measures such as PairwiseLoss and DCG, we prove that the minimax regret is Θ(T ). Moreover, the minimax regret is achievable using an efficient algorithmic strategy that only spends O(m logm) time per round. The same algorithmic strategy achieves O(T ) regret for Precision@k. Surprisingly, we show that for normalized versions of these ranking measures, namely AUC, NDCG and MAP, no online ranking algorithm can have sub-linear regret.",
    "creator" : "LaTeX with hyperref package"
  }
}