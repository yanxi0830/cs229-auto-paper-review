{
  "name" : "1702.03465.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Enabling Robots to Communicate their Objectives",
    "authors" : [ "Sandy H. Huang", "David Held", "Pieter Abbeel", "Anca D. Dragan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then selecting those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives.\nWe consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be exact in their IRL inference. We thus introduce two factors to define candidate approximateinference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what the robot will do in test situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.\nI. INTRODUCTION\nImagine riding in a self-driving car that needs to quickly change lanes to make a right turn. The car suddenly brakes in order to merge safely behind a car, because it deems it unsafe to speed up and merge in front. A passenger who knows the car is defensive and that it values safety much more than efficiency would be able to anticipate this behavior. But passengers less familiar with the car would not anticipate this sudden braking, so they may be surprised and possibly frightened.\nThere are many reasons why it is beneficial for humans to be able to anticipate a robot’s movements, from subjective comfort [6] to ease of coordination when working with and around the robot [7, 21]. However, people have difficulty anticipating robot actions [6, 8, 14]. Our goal is to enable end-users to accurately anticipate how a robot will act, even in novel situations that they have not seen the robot act in before—like a new traffic scenario, or a new placement of objects on a table that the robot needs to clear.\nA robot’s behavior in any situation is a direct consequence of the objective (or reward) function the robot is optimizing: (most) robots are rational agents, acting to maximize expected cumulative reward [19]. Whether the robot’s objective function is hard-coded or learned, it captures the trade-offs the robot makes between different features relevant to the task. For instance, a car might trade off between features related to collision avoidance and efficiency [13], with more “aggressive”\ncars prioritizing efficiency at the detriment of, say, distance to obstacles [20].\nThe insight underlying our approach is that the key to end-users being able to anticipate what a robot will do in novel situations is having a good mental model or understanding of the robot’s objective function.\nNote that understanding the objective function does not mean users must be able to explicate it—to write down the equation, or even to assign the correct reward to a behavior or a state-action pair. Instead, users should have an implicit representation of what drives the robot’s behavior. In the autonomous driving domain, a user should qualitatively understand the trade-offs that the car is making, perhaps that it tends to drive more “aggressively,” thereby enabling them to better anticipate what the car will do in different traffic scenarios.\nFortunately, users will naturally develop and improve their mental model of the robot’s objective function, given examples of the robot behaving optimally with respect to its true objective. People naturally make inferences about an agent’s objective function when observing behavior [10], and get better over time at anticipating how robots will act [6].\nHowever, in many environments a robot’s optimal behavior does not fully define how the robot will behave in all environments; parts of the robot’s objective will be under-determined. For example, an autonomous car driving down a highway with no other cars around will drive at the speed limit and stay in its lane, regardless of whether its objective places a higher value on efficiency or staying far away from other cars. Another example, shown on the right of Fig. 1, is when a car can change lanes without interacting with any other vehicles. An end-user mainly exposed to these types of behavior will take\nar X\niv :1\n70 2.\n03 46\n5v 1\n[ cs\n.R O\n] 1\n1 Fe\nb 20\n17\na long time to form an accurate mental model of the robot’s objective function and anticipate how the robot will behave in more complex scenarios.\nOn the other hand, suppose an autonomous car chooses to speed up and merge in front of another car, cutting it off (Fig. 1, left). This scenario illustrates this car values efficiency more.\nWe focus on enabling robots to purposefully choose such informative behaviors that actively communicate the robot’s objective function. We envision a training phase for interaction, where the robot showcases informative behavior in order to quickly teach the end-user what it is optimizing for.\nIn order to choose the most informative example behaviors for communicating a robot’s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot’s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function. The opposite problem, machines inferring objective functions from observed human behavior, can be solved using Inverse Reinforcement Learning (IRL) [16]. Prior work has investigated how to teach an objective function through example behavior to machine learners running IRL [5].\nThe challenge in teaching people instead of machines is that while machines can perform exact inference, people are likely to be approximate in their inference.1 People do not have direct access to configuration-space trajectory and the exact environment state, whereas robots do, at least in kinesthetic teaching (and in [5]). People also cannot necessarily distinguish between a perfectly optimal trajectory for one objective and an ever-so-slightly suboptimal one [24].\nOur main contribution is to introduce a systematic collection of approximate-inference models and, in a user study, compare their performance relative to the exact inference model. We focus on the autonomous driving domain, where a car chooses example behaviors that are informative about the trade-offs it makes in its objective function. We measure teaching performance—how useful the generated examples are in enabling users to anticipate the car’s behavior in test situations—and find that one particular approximate-inference model significantly outperforms exact inference (the others perform on par). This supports our central hypothesis that accounting for approximations in user inference is indeed helpful, but suggests that we need to be careful about how we model this approximate inference.\nFurther analysis shows teaching performance correlates with covering the full space of strategies that the robot is capable of adopting. For instance, the teaching algorithm cannot just show the car cutting people off, it also needs to show an example where it is optimal to brake and merge behind. We show the best results are obtained by a coverage-augmented algorithm that leverages an approximate-inference user model while encouraging full coverage of all possible driving strategies.\nOur work takes a first stab at an important yet under-\n1Prior work has applied algorithmic teaching to teach humans, primarily for binary classification of images [3, 4, 11, 22]. In line with our work, Patil et al. show accounting for human limitations (in their case limiting the number of recalled examples) improves teaching performance [17].\nexplored problem of making robot objectives more transparent to people. This is important in the short term for human-robot interaction, as well as in the long term for building AI systems that are trustworthy and beneficial to people. Our results are encouraging, but also leave room for better models of how people extrapolate from observed robot behavior."
    }, {
      "heading" : "II. ALGORITHMIC TEACHING OF OBJECTIVE FUNCTIONS",
      "text" : "We model how people infer a robot’s objective function from its behavior, so that we can leverage this model to generative informative examples of behavior."
    }, {
      "heading" : "A. Preliminaries",
      "text" : "Let S be the (continuous) set of states and A be the (continuous) set of actions available to the robot. We assume the robot’s objective (or reward) function is represented as a linear combination of features2 weighted by some θ∗ [1]:\nRθ∗(st, at, st+1;E) = θ ∗>φ(st, at, st+1;E), (1)\nwhere st is the state at time t, at is the action taken at time t, and E is the environment (or world) description. In the case of driving, E contains information about the lanes, the trajectories of other cars, and the starting state of the robot.\nGiven an environment E, the parameters θ of the objective function determine the robot’s (optimal) trajectory ξθE :\nξθE = arg max ξE∈ΞE θTφ(ξE), (2)\nwhere φ(ξE) = ∑T t=0 γ\ntφ(st, at, st+1;E) and γ is a discount factor between 0 and 1 that favors obtaining rewards earlier. ΞE refers to all possible trajectories in environment E."
    }, {
      "heading" : "B. Algorithmic Teaching Framework",
      "text" : "We model the human observer as starting with a prior P (θ) over what θ∗ might be, and updating their belief as they observe the robot act. The robot behaves optimally with respect to the objective induced by θ∗, but as Fig. 1 shows, the details of the environment (e.g., the locations of nearby cars and the robot’s goal) influence the behavior, and therefore influence what effect the behavior has on the person’s belief.\nTo best leverage this effect, we search for a sequence of environments E1:n such that when the person observes the optimal trajectories in those environments, their updated belief places maximum probability on the correct θ, i.e., θ∗:\narg max E1:n\nP (θ∗|ξθ ∗\nE1:n) (3)\nTo solve this optimization problem, the robot needs to model how examples update the person’s belief, P (θ∗|ξθ∗E1:n). We propose to model P (θ∗|ξθ∗E1:n) via Bayesian inference:\nP (θ|ξθ ∗ E1:n) ∝ P (ξ θ∗ E1:n |θ)P (θ) = P (θ) n∏ i=1 P (ξθ ∗ Ei |θ). 3 (4)\nWith this assumption4, modeling how people infer the 2We can make this assumption without loss of generality, as there are no restrictions on how complex these features can be. 3Conditional independence can be assumed, since θ contains all the information needed to calculate the probability of a trajectory. 4and further assuming that the person has access to the features that are relevant to the task; in future work, we plan to study interactions for achieving common ground on what features are important.\nobjective function parameters reduces to modeling P (ξ|θ): how probable they would find trajectory ξ if they assumed the robot optimizes the objective function induced by θ. We explore different models of this, starting with exact-inference IRL as a special case. We then introduce models that account for the inexactness that is inevitable when real people make this inference."
    }, {
      "heading" : "C. Exact-Inference IRL as a Special Case",
      "text" : "Inverse Reinforcement Learning (IRL) [16] extracts an objective function from observed behavior by assuming that the observed behavior is optimizing some objective from a set of candidates. When that assumption is correct, IRL finds an objective function that assigns maximum reward (or minimum cost) to the observed behavior.\nAlgorithmic teaching has been used with exact-inference IRL learners [5]: the learner eliminates all objective functions which would not assign maximum reward to the observed behavior. This can be expressed by the model in (4) via a particular distribution for P (ξθ ∗\nE |θ):\nP (ξθ ∗\nE |θ) =\n{ 1, if ∀ξE , θ>φ(ξθ ∗\nE )− θ>φ(ξE) ≥ 0. 0, otherwise. (5)\nThis assumes people assign probability 0 to trajectories that are not perfectly optimal with respect to θ, so those candidate θs receive a probability of zero. Thus, each trajectory that the person observes completely eliminates from their belief any objective function that would not have produced exactly this trajectory when optimized. Assuming learners start with a uniform prior over objective functions, the resulting belief is a uniform distribution across the remaining candidate objective functions—across θs for which all observed trajectories are perfectly optimal.\nWhile this is a natural starting point, it relies on people being able to perfectly evaluate whether a trajectory is the (or one of the) global optima of any candidate objective function. We relax this requirement in our approximate-inference models."
    }, {
      "heading" : "D. Approximate-Inference Models",
      "text" : "We introduce a space of approximate-inference models, obtained by manipulating two factors in a 2–by–3 factorial design. Deterministic vs. Probabilistic Effect. In the exact-inference model, a candidate θ is either out or still in: the trajectories observed so far have either shown that θ is impossible (because they were not global optima for the objective induced by that θ), or have left it in the mix, assigning it equal probability as the other remaining θs.\nWe envision two ways to relax the assumption that a person can perfectly identify whether a trajectory is a global optimum given a θ:\nOne way is for observed trajectories to still either eliminate the θ or keep it in the running, but to be more conservative about which θs get eliminated. That is, even if the observed trajectory is not a perfect global optimum for a θ, the person will not eliminate that θ if the trajectory is close enough (under some distance metric) to the global optimum. We call this the deterministic effect.\nA second way is for observed trajectories to have a probabilistic effect on θs: rather than eliminating them completely. I.e., trajectories can make a θ less likely, depending on how far away its optimal trajectory is from the observed trajectory.\nIn both cases, P (ξθ ∗\nE |θ) no longer depends on the example trajectory being optimal with respect to θ. Instead, it depends on the distance d(ξθE , ξ θ∗\nE ) between the optimal trajectory for θ and the observed trajectory (optimal with respect to θ∗).\nGiven some distance metric d and hyperparameters τ, λ > 0, • For deterministic effect, P (ξθ ∗\nE |θ) ∝ 0 if d(ξθE , ξθ ∗ E ) > τ , or 1 otherwise.\n• For probabilistic effect, P (ξθ ∗\nE |θ) ∝ e−λ·d(ξ θ E ,ξ\nθ∗ E ).5\nThe deterministic effect results in conservative hypothesis elimination: it models a user who will either completely eliminate a θ or not, but who will not eliminate θs with optimal trajectories close to the example trajectory. In contrast, the probabilistic effect decreases the probability of θs with far away optimal trajectories, never fully eliminating any.\nThe exact-inference IRL model (Sec. II-C) is a special case with deterministic effect and a reward-based distance metric with τ = 0; it assumes there is no approximate inference. Distance Metrics. Both deterministic and probabilistic effects rely on the person’s notion of how close the optimal trajectory with respect to a θ is from the observed trajectory. We envision that closeness can be measured either in terms of the reward of the trajectories with respect to θ, or in terms of the trajectories themselves.\nWe explore three options for d. The first depends on the reward. This distance metric models people with difficulty comparing the cumulative discounted rewards of two trajectories, with respect to a given setting of the reward parameters. So, if in environment E the example trajectory ξθ ∗\nE has almost the same reward as the optimal trajectory with respect to θ, then P (ξθ ∗\nE |θ) will be high. • reward-based6: d(ξθE , ξ θ∗ E ) = θ >φ(ξθE)− θ>φ(ξθ ∗ E ).\nThe second option depends not on reward, but on the physical trajectories. It assumes it is not high reward that can confuse people about whether the observed trajectory is optimal with respect to θ, but rather physical proximity to the true optimal trajectory: this models people who cannot perfectly distinguish between perceptually-similar trajectories. • Euclidean-based: d(ξθE , ξ θ∗ E ) = 1 T ∑T t=1 ||sθE,t − sθ ∗\nE,t||2 where sθE,t is the state at time t for the trajectory ξ θ E . Finally, a more conservative version of the Euclidean distance metric is the strategy-based metric. The idea here is that for any environment E, trajectories generated by candidate θs can be clustered into types, or strategies. The strategy-based metric assumes people do not distinguish among trajectories that follow the same strategy. For instance, people will consider all trajectories in which the robot speeds up and merges in front of the car to be equivalent, and all trajectories in\n5We noticed that normalizing this distribution produced very similar results to leaving it unnormalized, so that is what we do in our experiments, analogous to other algorithmic teaching work not based on reward functions [22].\n6Note that this is always positive because ξθE has maximal reward w.r.t. θ.\nwhich the robot merges behind the car to be equivalent. So, if in environment E the example trajectory and the optimal trajectory with respect to θ have the same strategy, then P (ξθ ∗\nE |θ) ∝ 1. • strategy-based: d(ξθE , ξ θ∗ E ) = 0 if ξ θ E and ξ θ∗\nE are in the same trajectory strategy cluster, ∞ otherwise.\nRelation to MaxEnt IRL. MaxEnt IRL [26] is an IRL algorithm that assumes demonstrations are noisy (i.e., not necessarily optimal). In our setting, we instead assume the learner is approximate. These two sources of noise result in the same model: the MaxEnt distribution is equivalent to our probabilistic reward-based model:\nP (ξθ ∗ E |θ) ∝ eλθ Tφ(ξθ ∗ E ) (6)\n∝ eλ(θ Tφ(ξθ\n∗ E )−θ\nTφ(ξθE)) = e−λ·dr(ξ θ E ,ξ θ∗ E ). (7)"
    }, {
      "heading" : "E. Example Selection",
      "text" : "Given a learner model M that predicts PM(θ∗|ξθ ∗\nE1:n ),\nour approach greedily selects environment Et to maximize PM(θ\n∗|ξθ∗E1:t), which is estimated by uniformly sampling candidate θs. We allow the model to select up to ten examples; it stops early if no additional example improves this probability.\nThis greedy approach is near-optimal for deterministic effect with a uniform prior, since in this case maximizing PM(θ ∗|ξθ∗E1:t) is equivalent to maximizing − ∑ θ PM(θ|ξθ ∗ E1:t ), which is a non-decreasing monotonic submodular function [15]. This function is non-decreasing because adding example trajectories ξθ ∗\nE can only eliminate candidate θ’s, not add them. Additionally, a particular example trajectory ξθ ∗\nE\neliminates the same set of θs no matter when it is added to the sequence. Thus, showing that example later on in the sequence cannot eliminate more θs than adding it earlier, which makes this function submodular."
    }, {
      "heading" : "F. Hyperparameter Selection",
      "text" : "We would like to select values for hyperparameters τ and λ (for deterministic and probabilistic effect, respectively) that accurately model human learning in this domain. τ and λ affect the informativeness of examples. If τ is too large, then most environments will be uninformative, since the example trajectory will be within τ distance away from optimal trajectories of many θs, so those θs will not be eliminated. Thus, PM(θ\n∗|ξθ∗E1:n) will be low. On the other hand, if τ is too small, then some environments will be extremely informative, so only one or a few examples will be selected before no further improvement in PM(θ∗|ξθ ∗\nE1:n ) can be achieved. Analogous\nreasoning holds for λ. We expect humans to be teachable (i.e., τ cannot be too large) and to have approximate rather than exact inference (i.e., τ cannot be too small), so they would benefit from observing several examples rather than just one or two. Based on this, we select τ and λ for each approximate-inference model by choosing the value in {10−5, 10−4, . . . , 104, 105} that results in an increase from PM(θ∗|ξθ ∗ E1 ) to PM(θ∗|ξθ ∗ E1:n ) of at least 0.1, and selects the most unique examples to show."
    }, {
      "heading" : "III. EXAMPLE DOMAIN",
      "text" : "We evaluate how our proposed approximate-inference models perform for teaching the driving style of a simulated autonomous car. In this domain, participants witness examples (in simulation) of how the car drives, with the goal of being able to anticipate how it will drive when they actually ride in it. Driving Simulator. We model the dynamics of the car with the bicycle vehicle model [23]. Let the state of the a car be x = [x y θ v α]>, where (x, y) are the coordinates of the center of the car’s rear axle, θ is the heading of the car, v is\nits velocity, and α is the steering angle. Let u = [u1 u2]> represent the control input, where u1 is the change in steering angle and u2 is the acceleration. Additionally, let L be the distance between the front and rear axles of the car. Then the dynamics model of the vehicle is\n[ẋ ẏ θ̇ v̇ α̇] = [v ·cos(θ) v ·sin(θ) v L tan(α) v ·u1 u2] (8) Environments. We consider a total of 21,216 environments of highway driving configurations (Table I). Each environment has three lanes and a single non-autonomous car. The autonomous car always starts in the middle lane with the same initial velocity, whereas the initial location and velocity of the single non-autonomous car varies.\nThese driving environments naturally fall into four classes, with two trajectory strategies per class (Fig. 2): • Merging: when the non-autonomous car starts in the right\nlane, and the goal in this environment is to merge into the right lane. The two trajectory strategies are to either speed up and merge ahead of the non-autonomous car, or slow down and merge behind the non-autonomous car.\n• Braking: when the non-autonomous car starts in the center lane in front of the autonomous car, and the goal is to drive forward. The two trajectory strategies are to either keep driving in the center lane behind the nonautonomous car, or merge into another lane to pass it.\n• Tailgating: when the non-autonomous car starts in the center lane behind the autonomous car, and the goal is to drive forward. The two trajectory strategies are to either change lanes to avoid the tailgater, or speed up to maintain a safe distance from the tailgater.\n• Other: all environments not included in one of the first three. The autonomous car is able to reach its goal without any interaction with the non-autonomous car.\nReward Features. We use the following reward features φ(ξ): • distance to other car: ∑T t=1 γ te− 1 2 (pt−p ′ t) >Σ−1t (pt−p ′ t);\neach term corresponds to a multivariate Gaussian kernel, where pt = [xt, yt]>, p′t is the non-autonomous car’s position, and Σt is chosen so that the major axis is along the non-autonomous car’s heading.\n• acceleration, squared: ∑T−1 t=1 γ t(vt+1 − vt)2\n• deviation from initial speed, squared: ∑T t=1 γ t(vt − v1)2\n• turning:\n∑T\nt=1 γ t|θt − θ1|\n• distance from goal: ∑T t=1 γ\nt max(0, (x1 + w) − xt)2 if the goal is to merge into the right lane, and yT if the goal is to drive forward. w is the width of one lane.\nThe last four features do not depend on the environment, so we normalize them such that the maximum value of that feature across all trajectories is 1 and the minimum is 0. We use γ = 1. Optimal θ. We select θ∗ = [−64 −0.1 −1 −0.1 −0.5]>, a reward function that is not overly cautious about staying away from other cars."
    }, {
      "heading" : "IV. ANALYSIS OF APPROXIMATE INFERENCE MODELS WITH IDEAL USERS",
      "text" : "In Sec. II-D, we introduced six possible approximateinference user models M. They all model people as judging candidate θs based on the distance between the trajectory they observed and the optimal trajectory with respect to θ, but they differ in what the distance metric is, and whether they completely eliminate candidate θs (deterministic effect) or smoothly re-weight them (probabilistic effect).\nHere, we investigate how well algorithmic teaching with these models performs for teaching θ∗ to ideal users. First, we generate a sequence of examples for each of our approximateinference models M, by greedily maximizing PM(θ∗|ξθ ∗\nE1:n ).\nWe also generate the sequence for the exact-inference model and include a random sequence, for a total of eight sequences. Types of Examples Selected. Fig. 3 summarizes the types of examples that each algorithm selected for its optimal sequence. The exact-inference model selects a single example, because that is enough to completely eliminate all other θs. This works well with an ideal user running exact inference, but our hypothesis is that it does not work as well with real users. Relative Evaluation. We evaluate algorithmic teaching on seven ideal users, whose learning exactly matches one of our six approximate-inference models or exact-inference IRL. We measure, for each “user” M , the probability they assign to the correct objective function parameters, PM(θ∗|ξθ ∗\nE1:n ), given\nξθ ∗\nE1:n from each of the eight generated sequences.\nFig. 4 shows the results. First, we see for any ideal user M , the sequence generated by assuming a learner model M performs best at teaching that user. This is by design—that sequence of examples is optimized to teach M .\nLooking across the columns of Fig. 4, we see all eight sequences perform equally well for teaching an exact IRL learner (column 1)—even random, because it provides enough examples to perfectly eliminate all incorrect θs. This suggests exact IRL does not accurately model real users, whose performance likely varies based on which examples they see. Looking across the rows, we notice assuming a Euclidean distance approximation when generating examples (rows 3 and 6) leads to robust performance across different user models. Sec. V evaluates these generated sequences on real users.\nFinally, the random sequence is very uninformative for all ideal users except exact IRL, showing the utility of algorithmic teaching. We explore this utility with real users in Sec. VI."
    }, {
      "heading" : "V. USER STUDY",
      "text" : "We now evaluate whether approximate-inference models are useful with real, as opposed to ideal, users."
    }, {
      "heading" : "A. Experiment Design",
      "text" : "Manipulated Variables. We manipulate whether algorithmic teaching assumes exact-inference or approximate-inference. For the approximate-inference case, we manipulated two variables: the effect of approximate inference (either deterministic or probabilistic) and the distance metric (reward-based, Euclidean-based, or strategy-based), in a 2–by–3 factorial design, for a total of six approximate inference models. For the strategy-based distance metric, the type of effect does not matter since distances are either 0 or ∞, so there are five unique approximate-inference models.\nWe show the participant one training environment at a time, in the order that the examples were selected by each algorithm. Dependent Measures. In the end, we are interested in how well human participants learn a specific setting of reward parameters θ∗ from the training examples. Since we cannot ask them to write down a θ, or to drive according to how they think the car will drive (people can drive like themselves, but not so easily as others), we evaluate this by testing the participant’s ability to identify the trajectory produced by θ∗ in a few test environments. For each test environment, we show\nthe participant four trajectories and ask them to select the one that most closely matches the autonomous car’s driving style.\nWe have two dependent variables that measure this: the fraction of participants who are able to identify ξθ ∗\nEtest for\neach test environment Etest, and their confidence in selecting that trajectory. We combine them in a confidence score: the confidence if they are correct, negative of the confidence if they are not—this score captures that if one is incorrect, it is better to be not confident about it.\nWe use rejection sampling to select test environments in which there are a wider variety of possible robot trajectories. We ensure the rewards of alternate trajectories under θ∗ are below a reward-based distance threshold, to make sure the four trajectories do not look too similar. In order to not bias the measure, we select one test environment for each of the two trajectory strategy clusters in each of the three informative environment classes, for a total of six test environments. For each test environment, we show two trajectory options in each strategy cluster. Hypothesis. Accounting for approximate inference significantly improves performance (the confidence score). We leave open which approximate-inference models work well and which do not, since the goal is to identify which captures users’ inferences the best. Subject Allocation. We used a between-subjects design, since examples of the same reward functions interfere with each other. We ran this experiment on a total of 191 participants across the six conditions, recruited via Amazon Mechanical Turk. At the end of the experiment, we ask participants what the two possible goals were, to filter out participants who were not paying attention. 30 out of 191 participants (15.7%) did not answer this control question correctly. The average age of the 161 non-filtered participants was 37.0 (SD = 11.0). The gender ratio was 0.46 female."
    }, {
      "heading" : "B. Analysis",
      "text" : "Number of Examples. Different algorithms produce different numbers of examples. Exact-inference IRL might produce as few as one example (and does in our case). Approximateinference models produce more, and random can produce an almost unlimited amount if allowed. Thus, a possible confound in our experiment is the number of examples.\nWe checked whether this is indeed a confound: do more examples help? Surprisingly, we found no correlation between the number of examples and performance: the sample Pearson correlation coefficient is r = 0.03 (Fig. 6, left). This suggests that example quantity matters less than example quality. Approximate-Inference Models. We start our analysis by comparing the different approximate-inference models. We ran a factorial ANOVA on confidence score with distance measure and determinism as factors (Fig. 5, left). We found a marginal effect for distance (F (2, 163) = 2.69, p = .07), with the Euclidean-based distance performing the best (as already suggested by our simulation experiment in Sec. IV, where Euclidean was the most robust across different ideal users), and the reward-based distance performing the worst.\nEuclidean-based might be better than reward-based because people decide to keep imperfect θs not when the trajectory\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n0_5 0_6 0_4_2\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n0_5 0_6 0_4_2\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n0_5 0_6 0_4_2\nCo nfi\nde nc\ne Sc\nor e\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n*\napprox*\nexact inference IRL reward-based\nrandom coverage-random approx* coverage-approx*\n*\nEuclidean-based strategy-based probabilistic de te rm in ist ic\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nNo coverage, determeuc Coverage, determeuc\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nNo coverage, determeuc Coverage, determeuc\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nNo coverage, determeuc Coverage, determeuc\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n0_2 0_3 0_4\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n0\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nNo coverage, random Coverage, random\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n0_2 0_3 0_4\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nrando\nrando Fig. 5: Performance of human participants on ide tifying the autonomous car’s trajectory in test environments, after seeing the example trajectories selected by the approximate-inference models (left) and after adding coverage (right) to the sequence of environments selected by the best-performing approximate-inference model, approx*. Participants in the coverage-approx* condition performed significantly better than those in the random condition. In contrast, enforcing coverage but selecting random sequences (coverage-random) does not lead to statistically significantly better performance.\n-7\n-5\n-3\n-1\n1\n3\n5\n7\n0 2 4 6 8\nCo nfi\nde nc\ne Sc\nor e\n-7\n-5\n-3\n-1\n1\n3\n5\n7\n-3 -1 1 3\nNumber of Helpful Environments Shown Total Number of Environments Shown\nFig. 6: Left: Lack of correlation between total number of environments shown in a condition, and average participant performance on each test example in that condition. Right: Correlation exists between the number of helpful training examples shown for an environment class, and average participant performance on the test example corresponding to that environment class.\nthey see obtains high reward under that θ, but when it is visually similar to what optimizing for θ would have produced. Euclidean-based might be better than strategy-based because people differentiate between trajectories even when they follow the same strategy. For example, a trajectory that gets very close to another car would be in the same strategy class as a trajectory that stays farther away, as long as they both merge behind the other car, but these two trajectories may give people very different impressions of the car’s driving style.\nThere was no effect for determinism. On average, prob-\nabilistic models performed ever-so-slightly worse than deterministic ones, and the difference was largest for Euclidean distance. This might be because keeping track of what is possible is easier than maintaining an entire probability distribution.\nThe best approximate-inference model used the Euclideanbased distance with deterministic effects. We refer to this as the approx* model (Fig. 5, left).\nResults for accuracy were analogous. Central Hypothesis: Utility of Approximate Inference. Despite showing more examples, most approximate-inference models did not perform much better than exact-inference IRL. This shows that not just any approximate-inference model is useful. However, it does not imply that no approximateinference model is useful. To test the utility of accounting for approximate inference, we compared the best model, approx*, with exact-inference IRL, and found a significant improvement (Welch’s t-test p = 0.025).\nThis supports our central hypothesis of this paper, that accounting for approximate inference in our model of human inferences about objective functions helps, with the caveat that not just any type of approximation will work."
    }, {
      "heading" : "VI. UTILITY OF ALGORITHMIC TEACHING",
      "text" : "So far, we have tested our central hypothesis for this work, that accounting for approximate IRL inferences can indeed improve the performance of algorithmic teaching of humans. While this is very promising, we also wanted to test the utility of algorithmic teaching itself: whether this new algorithm is preferable not just to algorithmic teaching with exact inference, but to the robot not actively teaching. Instead, the person must learn from the robot’s behavior in environments that it happens to encounter."
    }, {
      "heading" : "A. Baselining Performance",
      "text" : "Baseline Condition. We ran a follow-up study comparing algorithmic teaching with a sequence of optimal trajectories in random environments—simulating that the robot does not choose these, but instead happens to encounter them. We recruited 33 users for this condition. The average age of the 28 non-filtered participants was 33.3 (SD = 9.6). The gender ratio was 0.46 female. Controlling for Confounds. There are several variables that could confound this study. First, when generating the random sequence, we might get very lucky or unlucky and generate a particularly informative or uninformative one. To avoid this, we randomly sample 1000 random sequences with the desired number of examples, and sort them based on PM(θ∗|ξθ ∗\nE1:n )\nwhere M is the exact-inference IRL model. Then we choose the median sequence in that ranking as our random sequence, which will have median informativeness.\nSecond, different algorithms produce different numbers of examples. For instance, exact-inference IRL only selects one example, which eliminates all θs other than θ∗—because in that environment the optimal trajectories for all θs are at least slightly different than that for θ∗. To give the random baseline the best chance, we choose to select eight environments for it, which is the maximum number of examples shown by any of the other conditions. Since the majority of environments are\nuninformative (i.e., not in the merging, braking, or tailgating classes), providing the random condition with eight environments is needed to not put it at a serious disadvantage. Analysis. Algorithmic teaching with our approximate inference model did outperform the random baseline, albeit not significantly (Welch’s t-test p = 0.23 when comparing participants’ confidence scores). Algorithmic teaching without accounting for approximate inference actually seems to perform poorly compared to random (Fig. 5, right). Coverage. Digging deeper, we realized users tended to perform well on test cases for strategies in which they had seen a training example. In addition, for each pair of environment strategies A and B (e.g., merge-in-front and merge-behind), if users did not see an example from strategy A but saw one from strategy B, their performance was worse than if they did not see any examples from either A or B! In other words, if users see one trajectory strategy in the training examples and not the opposite strategy, they tend to think the autonomous car will always take the first strategy in that environment type.\nBased on this observation, given that x training examples are shown in environment class A and y from B, we define the number of helpful environments shown in A as equal to x if x > 0, and equal to −y otherwise. We found a strong correlation between the number of “helpful” environments shown and users’ confidence scores, with a Pearson correlation coefficient of r = 0.83 (p = 1.4× 10−11) (Fig. 6, right).\nWe leverage this result to introduce augmented algorithms that ensure coverage of strategies."
    }, {
      "heading" : "B. Coverage-Augmented Algorithmic Teaching",
      "text" : "Since coverage correlates with better user performance, we add a coverage term to our optimization over trajectories ξθ∗E1:n :\narg max ξθ∗E1:n\nPM(θ ∗|ξθ∗E1:n) + λ ∑ c 1[∃i, h(ξθ∗Ei) = c], (9)\nwhere the sum is over trajectory strategy clusters c and the function h maps a trajectory to the strategy it belongs to.\nWe set the trade-off parameter λ at 0 until [PM(θ∗|ξθ∗E1:t)− PM(θ\n∗|ξθ∗E1:t−1)] < , so that first the examples that maximize the probability of θ∗ will be selected, and only after no further examples will significantly improve the probability of θ∗, extra examples are selected to provide coverage across the strategies. We select these extra examples by choosing the best with respect to the approximate-inference model M, to make sure they are informative. Using this approach, we augment our best approximate-inference model, approx*, to achieve coverage."
    }, {
      "heading" : "C. User Study on Coverage",
      "text" : "We next run a study to test the benefit of coverage. Manipulated Variables. We manipulate two variables: whether we augmented the training examples with coverage, and whether we used a user model to generate the examples or sampled uniformly. We select our best model for the former, approx*. From the previous experiment, we have already obtained user performance data along the no-coverage dimension—for random and approx*—so we run this experiment on only the two new conditions that incorporate coverage, which we will refer to as coverage-random and coverageapprox*. We generate random sequences with coverage by\nrandomly selecting exactly one random environment from each of the eight environment and trajectory strategy classes. Dependent Measures. We keep the same dependent measures as in our previous Mechanical Turk experiment (Sec. V-A). Hypothesis. We hypothesize coverage augmentation improves user performance in both conditions, random and approx*, compared to the respective conditions without coverage. Subject Allocation. We ran this experiment between-subjects, on a total of 63 participants across the two conditions. The average age of the 53 non-filtered participants was 34.43 (SD = 9.0). The gender ratio was 0.53 female. Analysis. We ran a factorial ANOVA on confidence score with coverage and model as factors. We found a marginal effect for coverage (F (1, 107) = 1.82, p = .07), suggesting that coverage improves performance. There was no interaction effect, suggesting that coverage helps regardless of using a user model for teaching or not.\nCoverage-approx* performed best out of the four conditions. The coverage augmentation enabled it to significantly outperform the random baseline (with a Welch’s ttest p = 0.049), which suggests coverage is useful. Coverage augmentation did not enable the coverage-random condition to outperform the random baseline (p = 0.159), which suggests the approximate-inference model is useful (Fig. 5, right).\nOverall, coverage alone helped, but was not sufficient to outperform the baseline. From the previous experiment, we know that the approximate-inference user model helped, but was also not sufficient to outperform the baseline. Overall, the improvement is largest (and significant, modulo compensating for multiple hypotheses) when we have a coverage-augmented approximate-inference IRL model.\nWhen leveraged together, coverage with the right approximate-inference model have a significant teaching advantage over random teaching, as well as over IRL models that assume exact-inference users."
    }, {
      "heading" : "VII. DISCUSSION",
      "text" : "Summary. We take a step towards communicating robot objectives to people. We found that an approximate-inference model using a deterministic Euclidean-based update on the space of candidate reward function parameters performed best at teaching real users, and outperforms algorithmic teaching that assumes exact inference. We additionally found after augmenting such model with a coverage objective, it significantly outperformed letting the user passively familiarize to the robot. Limitations and Future Work. Our results reveal the promise of algorithmic teaching of robot objective functions. Our model of human learning is far from perfect, and more work is needed to explore the process people use to extrapolate from observed robot behavior.\nFurthermore, in this work we focused on the robot’s physical behavior as a communication channel because people naturally infer utility functions from it. Future work could augment this with other channels, for instance visualizations of the objective function or language-based explanations."
    }, {
      "heading" : "VIII. ACKNOWLEDGMENTS",
      "text" : "This research was funded in part by Intel Labs, an NSF CAREER award (#1351028), and the Berkeley Deep Drive\nconsortium. Sandy Huang was supported by an NSF Fellowship."
    } ],
    "references" : [ {
      "title" : "Apprenticeship learning via inverse reinforcement learning",
      "author" : [ "Pieter Abbeel", "Andrew Y. Ng" ],
      "venue" : "In Proceedings of the Twenty-First International Conference on Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "Recent developments in algorithmic teaching",
      "author" : [ "Frank J. Balbach", "Thomas Zeugmann" ],
      "venue" : "In Proceedings of the Third International Conference on Language and Automata Theory and Applications,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Teaching classification boundaries to humans",
      "author" : [ "Sumit Basu", "Janara Christensen" ],
      "venue" : "In Proceedings of the Twenty- Seventh AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston" ],
      "venue" : "In Proceedings of the Twenty-Sixth Annual International Conference on Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Algorithmic and human teaching of sequential decision tasks",
      "author" : [ "Maya Cakmak", "Manuel Lopes" ],
      "venue" : "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Familiarization to robot motion",
      "author" : [ "Anca Dragan", "Siddhartha Srinivasa" ],
      "venue" : "In Proceedings of the Ninth International Conference on Human-Robot Interaction,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Effects of robot motion on humanrobot collaboration",
      "author" : [ "Anca Dragan", "Shira Bauman", "Jodi Forlizzi", "Siddhartha Srinivasa" ],
      "venue" : "In Proceedings of the Tenth International Conference on Human-Robot Interaction,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Generating human-like motion for robots",
      "author" : [ "Michael J. Gielniak", "C. Karen Liu", "Andrea L. Thomaz" ],
      "venue" : "International Journal of Robotics Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "On the complexity of teaching",
      "author" : [ "Sally A. Goldman", "Michael J. Kearns" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1995
    }, {
      "title" : "The naı̈ve utility calculus: Computational principles underlying commonsense psychology",
      "author" : [ "Julian Jara-Ettinger", "Hyowon Gwen", "Laura E. Schulz", "Joshua B. Tenenbaum" ],
      "venue" : "Trends in Cognitive Sciences,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "How do humans teach: On curriculum learning and teaching dimension",
      "author" : [ "Faisal Khan", "Bilge Mutlu", "Xiaojin Zhu" ],
      "venue" : "In Proceedings of the Twenty-Fourth Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "New potentials for datadriven intelligent tutoring system development and optimization",
      "author" : [ "Kenneth R. Koedinger", "Emma Brunskill", "Ryan Shaun Joazeiro de Baker", "Elizabeth A. McLaughlin", "John C. Stamper" ],
      "venue" : "AI Magazine,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Continuous inverse optimal control with locally optimal examples",
      "author" : [ "Sergey Levine", "Vladlen Koltun" ],
      "venue" : "In Proceedings of the Twenty-Ninth International Conference on Machine Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Generating natural motion in an  android by mapping human motion",
      "author" : [ "Daisuke Matsui", "Takashi Minato", "Karl F. MacDorman", "Hiroshi Ishiguro" ],
      "venue" : "In IEEE/RSJ International Conference on Intelligent Robots and Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "An analysis of approximations for maximizing submodular set functions",
      "author" : [ "G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1978
    }, {
      "title" : "Algorithms for inverse reinforcement learning",
      "author" : [ "Andrew Y. Ng", "Stuart Russell" ],
      "venue" : "In Proceedings of the Seventeenth International Conference on Machine Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "Optimal Teaching for Limited-Capacity Human Learners",
      "author" : [ "Kaustubh R Patil", "Xiaojin Zhu", "Ł ukasz Kopeć", "Bradley C Love" ],
      "venue" : "In Proceedings of the Twenty-Seventh Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Faster Teaching by POMDP Planning",
      "author" : [ "Anna N. Rafferty", "Emma Brunskill", "Thomas L. Griffiths", "Patrick Shafto" ],
      "venue" : "In Proceedings of the Fifteenth International Conference on Artificial Intelligence in Education,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Artificial Intelligence: A Modern Approach",
      "author" : [ "Stuart J. Russell", "Peter Norvig" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Information gathering actions over human internal state",
      "author" : [ "Dorsa Sadigh", "S. Shankar Sastry", "Sanjit A. Seshia", "Anca Dragan" ],
      "venue" : "In IEEE/RSJ International Conference on Intelligent Robots and Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Joint action: Bodies and minds moving together",
      "author" : [ "Natalie Sebanz", "Harold Bekkering", "Gnther Knoblich" ],
      "venue" : "Trends in Cognitive Sciences,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Near-optimally teaching the crowd to classify",
      "author" : [ "Adish Singla", "Ilija Bogunovic", "Gabor Bartok", "Amin Karbasi", "Andreas Krause" ],
      "venue" : "In Proceedings of the Thirty-First International Conference on Machine Learning,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Investigation of a combined slip control braking and closed loop four wheel steering system for an automobile during combined hard braking and severe steering",
      "author" : [ "Saied Taheri", "E. Harry Law" ],
      "venue" : "In Proceedings of the 1990 American Control Conference,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1990
    }, {
      "title" : "One and done? Optimal decisions from very few samples",
      "author" : [ "Edward Vul", "Noah D. Goodman", "Thomas L. Griffiths", "Joshua B. Tenenbaum" ],
      "venue" : "Cognitive Science,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Machine teaching: An inverse problem to machine learning and an approach toward optimal education",
      "author" : [ "Xiaojin Zhu" ],
      "venue" : "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Maximum entropy inverse reinforcement learning",
      "author" : [ "Brian D. Ziebart", "Andrew Maas", "J. Andrew Bagnell", "Anind Dey" ],
      "venue" : "In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2008
    }, {
      "title" : "Models of cooperative teaching and learning",
      "author" : [ "Sandra Zilles", "Steffen Lange", "Robert Holte", "Martin Zinkevich" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "There are many reasons why it is beneficial for humans to be able to anticipate a robot’s movements, from subjective comfort [6] to ease of coordination when working with and around the robot [7, 21].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "There are many reasons why it is beneficial for humans to be able to anticipate a robot’s movements, from subjective comfort [6] to ease of coordination when working with and around the robot [7, 21].",
      "startOffset" : 192,
      "endOffset" : 199
    }, {
      "referenceID" : 20,
      "context" : "There are many reasons why it is beneficial for humans to be able to anticipate a robot’s movements, from subjective comfort [6] to ease of coordination when working with and around the robot [7, 21].",
      "startOffset" : 192,
      "endOffset" : 199
    }, {
      "referenceID" : 5,
      "context" : "However, people have difficulty anticipating robot actions [6, 8, 14].",
      "startOffset" : 59,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "However, people have difficulty anticipating robot actions [6, 8, 14].",
      "startOffset" : 59,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "However, people have difficulty anticipating robot actions [6, 8, 14].",
      "startOffset" : 59,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "A robot’s behavior in any situation is a direct consequence of the objective (or reward) function the robot is optimizing: (most) robots are rational agents, acting to maximize expected cumulative reward [19].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 12,
      "context" : "For instance, a car might trade off between features related to collision avoidance and efficiency [13], with more “aggressive” P( θ | ξ 1 )",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "cars prioritizing efficiency at the detriment of, say, distance to obstacles [20].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "People naturally make inferences about an agent’s objective function when observing behavior [10], and get better over time at anticipating how robots will act [6].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "People naturally make inferences about an agent’s objective function when observing behavior [10], and get better over time at anticipating how robots will act [6].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 1,
      "context" : "In order to choose the most informative example behaviors for communicating a robot’s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot’s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.",
      "startOffset" : 157,
      "endOffset" : 179
    }, {
      "referenceID" : 8,
      "context" : "In order to choose the most informative example behaviors for communicating a robot’s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot’s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.",
      "startOffset" : 157,
      "endOffset" : 179
    }, {
      "referenceID" : 11,
      "context" : "In order to choose the most informative example behaviors for communicating a robot’s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot’s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.",
      "startOffset" : 157,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : "In order to choose the most informative example behaviors for communicating a robot’s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot’s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.",
      "startOffset" : 157,
      "endOffset" : 179
    }, {
      "referenceID" : 24,
      "context" : "In order to choose the most informative example behaviors for communicating a robot’s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot’s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.",
      "startOffset" : 157,
      "endOffset" : 179
    }, {
      "referenceID" : 26,
      "context" : "In order to choose the most informative example behaviors for communicating a robot’s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot’s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.",
      "startOffset" : 157,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : "The opposite problem, machines inferring objective functions from observed human behavior, can be solved using Inverse Reinforcement Learning (IRL) [16].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : "Prior work has investigated how to teach an objective function through example behavior to machine learners running IRL [5].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "1 People do not have direct access to configuration-space trajectory and the exact environment state, whereas robots do, at least in kinesthetic teaching (and in [5]).",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 23,
      "context" : "People also cannot necessarily distinguish between a perfectly optimal trajectory for one objective and an ever-so-slightly suboptimal one [24].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "1Prior work has applied algorithmic teaching to teach humans, primarily for binary classification of images [3, 4, 11, 22].",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "1Prior work has applied algorithmic teaching to teach humans, primarily for binary classification of images [3, 4, 11, 22].",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "1Prior work has applied algorithmic teaching to teach humans, primarily for binary classification of images [3, 4, 11, 22].",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 21,
      "context" : "1Prior work has applied algorithmic teaching to teach humans, primarily for binary classification of images [3, 4, 11, 22].",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 16,
      "context" : "show accounting for human limitations (in their case limiting the number of recalled examples) improves teaching performance [17].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "We assume the robot’s objective (or reward) function is represented as a linear combination of features2 weighted by some θ∗ [1]:",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "Inverse Reinforcement Learning (IRL) [16] extracts an objective function from observed behavior by assuming that the observed behavior is optimizing some objective from a set of candidates.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "Algorithmic teaching has been used with exact-inference IRL learners [5]: the learner eliminates all objective functions which would not assign maximum reward to the observed behavior.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "5We noticed that normalizing this distribution produced very similar results to leaving it unnormalized, so that is what we do in our experiments, analogous to other algorithmic teaching work not based on reward functions [22].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 25,
      "context" : "MaxEnt IRL [26] is an IRL algorithm that assumes demonstrations are noisy (i.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 14,
      "context" : "This greedy approach is near-optimal for deterministic effect with a uniform prior, since in this case maximizing PM(θ ∗|ξθ E1:t) is equivalent to maximizing − ∑ θ PM(θ|ξ ∗ E1:t ), which is a non-decreasing monotonic submodular function [15].",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 22,
      "context" : "We model the dynamics of the car with the bicycle vehicle model [23].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 19,
      "context" : "Final velocity, non-autonomous car (if acceleration time 6= 0) [20, 30, 70, 80]",
      "startOffset" : 63,
      "endOffset" : 79
    } ],
    "year" : 2017,
    "abstractText" : "Our ultimate goal is to efficiently enable end-users to correctly anticipate a robot’s behavior in novel situations. This behavior is often a direct result of the robot’s underlying objective function. Our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then selecting those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be exact in their IRL inference. We thus introduce two factors to define candidate approximateinference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what the robot will do in test situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.",
    "creator" : "LaTeX with hyperref package"
  }
}