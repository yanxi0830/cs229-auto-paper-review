{
  "name" : "1405.0133.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Geodesic Distance Function Learning via heat flow on Vector Fields",
    "authors" : [ "Binbin Lin", "Ji Yang", "Xiaofei He", "Jieping Ye" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Learning a distance function or metric on a given data manifold is of great importance in machine learning and pattern recognition. The goal of distance metric learning on the manifold is to find a desired distance function d(x, y) such that it provides a natural measure of the similarity between two data points x and y on the manifold. It has been applied widely in many problems, such as information retrieval [18], classification and clustering [27]. Depending on whether there is label information available, metric learning methods can be classified into two categories: supervised and unsupervised. In supervised learning, one often assumes that data points with the same label should have small distance, while data points with different labels should have large distance [27, 25, 10]. In this paper, we consider the problem of unsupervised distance metric learning.\nUnsupervised manifold learning can be viewed as an alternative way of distance metric learning. It aims to find a map F from the original high dimensional space to a lower dimensional Euclidean space such that the mapped Euclidean distance d(F (x), F (y)) preserves the original distance d(x, y). The classical Principal Component Analysis (PCA, [11]) can be considered as linear manifold learning method in which the map F is linear. The learned Euclidean distance after linear mapping is also referred to as Mahalanobis distance. Note that when the manifold is nonlinear, the Mahalanobis distance may fail to faithfully preserve the original distance.\nThe typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5]. Both Isomap and HLLE try to preserve the original\nar X\niv :1\n40 5.\n01 33\nv2 [\ncs .L\nG ]\n8 M\nay 2\ngeodesic distance on the data manifold. Diffusion maps try to preserve diffusion distance on the manifold which reflects the connectivity of data. Coifman and Lafon [5] also showed that both LLE and LE belong to the diffusion map framework which preserves the local structure of the manifold. MVU is proposed to learn a kernel eigenmap that preserves pairwise distances on the manifold. One problem of the existing manifold learning approaches is that there may not exist a distance preserving map F such that d(F (x), F (y)) = d(x, y) holds since the geometry and topology of the original manifold may be quite different from the Euclidean space. For example, there does not exist a distance preserving map between a sphere S2 and a 2-dimensional plane.\nIn this paper, we assume the data lies approximately on a low-dimensional manifold embedded in Euclidean space. Our aim is to approximate the geodesic distance function on this manifold. The geodesic distance is a fundamental intrinsic distance on the manifold and many useful distances (e.g., the diffusion distance) are variations of the geodesic distance. There are several ways to characterize the geodesic distance due to its various definitions and properties. The most intuitive and direct characterization of the geodesic distance is by definition that it is the shortest path distance between two points (e.g., [24]). However, it is well known that computing pairwise shortest path distance is time consuming and it cannot handle the case when the manifold is not geodesically convex [8]. A more convincing and efficient way to characterize the geodesic distance function is using partial differential equations (PDE). Mémoli et al. [19] proposes an iterated algorithm for solving the Hamilton-Jacobi equation ‖∇r‖ = 1 [17], where ∇r represents the gradient field of the distance function. However, the fast marching part requires a grid of the same dimension as the ambient space which is impractical when the ambient dimension is very high.\nNote that the tangent space dimension is equal to the manifold dimension [13] which is usually much smaller than the ambient dimension. One possible way to reduce the complexity of representing the gradient field ∇r is to use the local tangent space coordinates rather than the ambient space coordinates. Inspired by recent work on vector fields [23, 15, 14] and heat flow on scalar fields [6], we propose to learn the geodesic distance function via the characterization of its gradient field and heat flow on vector fields. Specifically, we study the geodesic distance function d(p, x) for a given base point p. Our theoretical analysis shows that if a function rp(x) is a local distance function around p, and its gradient field∇rp has unit norm or∇rp is parallel along geodesics passing through p, then rp(x) is the unique geodesic distance function d(p, x). Based on our theoretical analysis, we propose a novel algorithm to first learn the gradient field of the distance function and then learn the distance function itself. Specifically, we set the gradient field of a local distance function around a given point as an initial vector field. Then we transport the initial local vector field to the whole manifold via heat flow on vector fields. By asymptotic analysis of the heat kernel, we show that the learned vector field is approximately parallel to the gradient field of the distance function at each point. Thus, the geodesic distance function can be obtained by requiring its gradient field to be close to the normalized vector field. The corresponding optimization problem involves sparse linear systems which can be solved efficiently. Moreover, the sparse linear systems can be easily extended to matrix form to learn the complete distance function d(·, ·). Both synthetic and real data experiments demonstrate the effectiveness of our proposed algorithm."
    }, {
      "heading" : "2 Characterization of Distance Functions using Gradient Fields",
      "text" : "Let (M, g) be a d-dimensional Riemannian manifold, where g is a Riemannian metric tensor onM. The goal of distance metric learning on the manifold is to find a desired distance function d(x, y) such that it provides a natural measure for the similarity between two data points x and y on the manifold. In this paper, we study a fundamental intrinsic distance function1 - the geodesic distance function. Similar to many geometry textbooks (e.g., [12, 20]), we call it the distance function. In the following, we will briefly introduce the most relevant concepts. A detailed and much more technical introduction can be found in the appendix.\nWe first show how to assign a metric structure on the manifold. For each point p on the manifold, a Riemannian metric tensor g at p is an inner product gp on each of the tangent space TpM ofM. We define the norm of a tangent vector v ∈ TpM as ‖v‖ = √ gp(v, v). Let [a, b] be a closed interval in R, and γ : [a, b] → M be a smooth curve. The length of γ can then be defined as\n1A distance function d(·, ·) defined by its Riemannian metric g is often called an intrinsic distance function.\nl(γ) :=\n∫ b\na ‖dγdt (t)‖dt. The distance between two points p, q on the manifoldM can be defined as:\nd(p, q) := inf{l(γ) : γ : [a, b]→M piecewise smooth, γ(a) = p and γ(b) = q}.\nWe call d(·, ·) the distance function and it satisfies the usual axioms of a metric, i.e., positivity, symmetry and triangle inequality [12]. We study the distance function d(p, ·) when p is given. Definition 2.1 (Distance function based at a point). LetM be a Riemannian manifold, and let p be a point on the manifoldM. A distance function onM based at p is defined as rp(x) = d(p, x). For simplicity, we might write r(·) instead of rp(·).\nDefinition 2.2 (Geodesic, [20]). Let γ : [a, b] → M, t 7→ γ(t) be a smooth curve. γ is called a geodesic if γ′(t) is parallel along γ, i.e.,∇γ′(t)γ′(t) = 0 for all t ∈ [a, b].\nHere ∇ is the covariant derivative on the manifold which measures the change of vector fields. A geodesic can be viewed as a curved straight line on the curved manifold. The geodesics and the distance function is related as follows: Theorem 2.1 ([20]). If γ is a local minimum for inf l(γ) with fixed end points, then γ is a geodesic.\nIn the following, we characterize the distance function rp(x) by using its gradient field∇r. A vector field X on the manifold is called a gradient field if there exists a function f on the manifold such that X = ∇f holds. Therefore, gradient fields are one kind of vector fields. Interestingly, we can precisely characterize the distance function based at a point by its gradient field. For simplicity, let ∂r denote the gradient field∇r. Then we have the following result. Theorem 2.2. Let M be a complete manifold. A continuous function r : M → R is a distance function onM based at p if and only if (a) r(x) = ‖ exp−1p (x)‖ holds for a neighborhood of p; (b) ∇∂r∂r = 0 holds on the manifoldM except for p ∪ Cut(p).\nHere expp is the exponential map at p and Cut(p) is the cut locus of p. The detailed definitions of the exponential map and the cut locus can be found in the appendix. Condition (a) states that locally r(x) is a Euclidean distance function in the exponential coordinates. Combining condition (b) which states that the integral curves of ∂r are all geodesics, we assert that r is a global distance function. As can be seen from Fig. 1(c), the gradient field of the distance function is parallel along the geodesics passing through p. It might be worth noting that condition (a) cannot be replaced by a weaker condition r(p) = 0 which is often used in PDE. A simple counter-example would be the function rp(x) = x defined onM = R with p = 0. rp(x) satisfies rp(0) = 0 and ∇∂r∂r = 0 holds for all x. However, it is not a distance function since it does not satisfy the positivity condition.\nThe second order condition∇∂r∂r = 0 can be replaced by a first order condition ‖∂r‖ = 1. Theorem 2.3. Let M be a complete manifold. A continuous function r : M → R is a distance function onM based at p if and only if (a) r(x) = ‖ exp−1p (x)‖ holds for a neighborhood of p; (b) ‖∂r‖ = 1 holds on the manifoldM except for p ∪ Cut(p).\nA detailed proof of Theorems 2.2 and 2.3 can be found in the appendix. We visualize the relationship among the distance function, the gradient field of the distance function and geodesics in Fig. 1. It can be seen from the figure that: (1) the gradient field of the distance function is parallel along geodesics passing through p; (2) the gradient field of the distance function has unit norm almost everywhere except for p and its cut locus which is the antipodal point of p."
    }, {
      "heading" : "3 Geodesic Distance Function Learning",
      "text" : "We show in the last section that the distance function can be characterized by its gradient field. Based on our theoretical analysis, we propose to first learn the gradient field of the distance function and then learn the distance function itself."
    }, {
      "heading" : "3.1 Geodesic Distance Learning",
      "text" : "Let (M, g) be a d-dimensional Riemannian manifold embedded in a much higher dimensional Euclidean space Rm, where g is a Riemannian metric tensor onM. Given a point p on the manifold, we aim to learn the distance function fp(x) = d(p, x). Let U := {x : d(p, x) ≤ } ⊂ M be a geodesic ball around p and let f0 denote a local distance function on U . That is, f0(x) = d(p, x) if p ∈ U and 0 otherwise. Let V 0 denote the gradient field of f0, i.e., V 0 = ∇f0. Now we are ready to summarize our Geodesic Distance Learning (GDL) algorithm as follows:\n• Learn a vector field V by transporting V 0 to the whole manifold using heat flow:\nmin V E(V ) := ∫ M ‖V − V 0‖2dx+ t ∫ M ‖∇V ‖2HSdx, (1)\nwhere ‖ · ‖HS denotes the Hilbert-Schmidt tensor norm [7] and t > 0 is a parameter. • Learn a normalized vector field V̂ via normalizing V at each point x: set V̂x = Vx/‖Vx‖\nwhen x 6= p and set V̂x = 0 when x = p. Here Vx denotes the tangent vector at x of V . • Learn the distance function f via solving the following equation:\nmin f Φ(f) := ∫ M ‖∇f − V̂ ‖2dx, s.t.f(p) = 0. (2)\nThe above algorithmic steps are illustrated in Fig. 2.\nThe theoretical justification of the above algorithm is given in the appendix. Our analysis indicates that solving Eq. (1) is equivalent to transporting the initial vector field to the whole manifold via heat flow on vector fields. By asymptotic analysis of the heat kernel, the learned vector field is approximately parallel to the gradient field of the distance function at each point. Thus, the gradient field of the distance function can be obtained via normalization. Finally, the geodesic distance function can be obtained by requiring its gradient field to be close to the normalized vector field. Our analysis also indicate the factors of controlling the quality of the approximation. It mainly relies on two factors: the distance to the query and the cut locus of the query. If the data point is not in the cut locus of the query, the smaller the distance between the data point and the query is, the better the\napproximation would be. If the data point is in the cut locus, the approximation might fail since the vector field around the cut locus varies dramatically. Note that the measure of the cut locus is zero, thus the approximation would fail only in a zero measure set."
    }, {
      "heading" : "3.2 Implementation",
      "text" : "Given n data points xi, i = 1, . . . , n, on the d-dimensional manifold M where M is embedded in the high dimensional Euclidean space Rm. Let xq denote the base point. We aim to learn the distance function f :M→ R based at xq , i.e., f(xi) = d(xq, xi), i = 1, . . . , n. We first construct an undirected nearest neighbour graph by either -neighbourhood or k nearest neighbours. It might be worth noting for a k-nn graph that the degree of a vertex will typically be larger than k since k nearest neighbour relationships are not symmetrical. Let xi ∼ xj denote that xi and xj are neighbors. Let wij denote the weight which can be approximated by the heat kernel weight or the simple 0-1 weight. For each point xi, we estimate its tangent space TxiM by performing PCA on its neighborhood. Before performing PCA, we mean-shift the neighbor vectors using their true mean. Let Ti ∈ Rm×d denote the matrix whose columns are constituted by the d principal components. Let V be a vector field on the manifold. For each point xi, let Vxi denote the tangent vector at xi. Recall from Definition 7.1 in the appendix that each tangent vector Vxi should be in the corresponding tangent space TxiM, we can represent Vxi as Vxi = Tivi, where vi ∈ Rd. We will abuse the notation f to denote the vector f = (f(x1), . . . , f(xn))T ∈ Rn and use V to denote the vector V = ( v1 T , . . . , vn T )T ∈ Rdn. We propose to first learn V and then learn f .\nSet an initial vector field V 0 as follows:\nv0j =  TTj (xj − xq) ‖TjTTj (xj − xq)‖ , if j ∼ q\n0, otherwise\n(3)\nNote that the vector TTj (xj − xq)/‖TjTTj (xj − xq)‖ is a unit vector at xj pointing outward from the base point xq (please see Fig. 2(a)). Following [15], the discrete form of our objective functions can be given as follows:\nE(V ) = V TV − 2V 0TV + V 0TV 0 + tV TBV, Φ(f) = 2fTLf + V̂ TGV̂ − 2V̂ TCf,\n(4)\nwhere L is the graph Laplacian matrix [4], B is a dn × dn block matrix, G is a dn × dn block diagonal matrix and C is a dn × n block matrix. Let Bij (i 6= j) denote the ij-th d × d block, Gii denote the i-th d × d diagonal block of G, and Ci denote the i-th d × n block of C. We have: Bii = ∑ j∼i wij(QijQ T ij + I), Bij = −2wijQij , Gii = ∑ j∼i wijT T i (xj − xi)(xj − xi)TTi,\nand Ci = ∑ j∼i wijT T i (xj − xi)sTij , where Qij = TTi Tj and sij ∈ Rn is a selection vector of all zero elements except for the i-th element being −1 and the j-th element being 1. The matrix Qij transports from the tangent space TxjM to TxiM which approximates the parallel transport from xj to xi. It might be worth noting that one can also approximate the parallel transport by solving a singular value decomposition problem [23]. The block matrix B provides a discrete approximation of the connection Laplacian operator, which is a symmetric and positive semi-definite matrix.\nNow we give our algorithm in the discrete setting. By taking derivatives of E(V ) with respect to V , V can be obtained via the following sparse linear system:\n(I + tB)V = V 0. (5)\nThen we learn a normalized vector field V̂ via normalizing V at each point: v̂i = vi/‖vi‖ if i 6= q and v̂i = 0 if i = q. The final distance function can be obtained via taking derivatives of Φ(f) with respect to f : 2Lf = CT V̂ , (6) where we restrict fq = 0 when solving Eq. (6). A direct way is to plug the constraint fq = 0 into Eq. (6). It is equivalent to removing the q-th column of L and the q-th element of f on the left hand side of Eq. (6). For each point xq , we have corresponding vectors V , V 0,V̂ andf . If xq varies, we can write V , V 0, V̂ and f in matrix form where each column is a vector field or a distance function. Then the complete distance function d(·, ·) can be obtained via solving the corresponding matrix form linear systems of Eq. (5) and Eq. (6). We summarize our algorithm in Algorithm 1.\nAlgorithm 1 GDL (Geodesic Distance Learning) Require: Data sample X = (x1, . . . , xn) ∈ Rm×n and a base point xq , 1 ≤ q ≤ n. Ensure: f = (f1, . . . , fn) ∈ Rn\nfor i = 1 to n do Compute the tangent space coordinates Ti ∈ Rm×d by using PCA end for Set an initial vector field V 0 via Eq. (3) and construct sparse block matrices B and C Solve (I + tB)V = V 0 to obtain V Normalize each vector in V to obtain V̂ Solve 2Lf = CT V̂ to obtain f return f"
    }, {
      "heading" : "3.3 Computation Complexity Analysis",
      "text" : "The computational complexity of our proposed Geodesic Distance Learning (GDL) algorithm is dominated by three parts: searching for k-nearest neighbors, computing local tangent spaces, computing Qij and solving the sparse linear system Eq. (5). For the k nearest neighbor search, the complexity is O((m+ k)n2), where O(mn2) is the complexity of computing the distance between any two data points, and O(kn2) is the complexity of finding the k nearest neighbors for all the data points. The complexity for local PCA is O(mk2). Therefore, the complexity for computing the local tangent space for all the data points is O(mnk2). Note that the matrix B is not a dense matrix but a very sparse block matrix with at most kn non-zero d × d blocks. Therefore the computation complexity of computing all Qij’s is O(knmd2). We use LSQR package2 to solve Eq. (5). It has a complexity of O(Iknd2), where I is the number of iterations. In summary, the overall computational cost for one base point is O((m+ k)n2 +mndk + kmnd2 + Iknd2). For p base points, the extra cost is to solve Eq. (5) by adding p− 1 columns which has a complexity of O(pIknd2). Empirically, the manifold dimension d and the number of nearest neighbors k are usually much smaller than the ambient dimension m and the number of data points n. So the total computational cost for p base points could be O(mn2 + pIn). There are several ways to further reduce the computational complexity. One way is to select anchor points and construct the graph using these anchor points. Another possible way is to learn the distance functions of nearby points simultaneously."
    }, {
      "heading" : "3.4 Related Work and Discussion",
      "text" : "Our approach is based on the idea of vector field regularization which is similar to Vector Diffusion Maps (VDM, [23]). Both methods employ vector fields to discover the geometry of the manifold. However, VDM and our approach differ in several key aspects: Firstly, they solve different problems. VDM tries to preserve the vector diffusion distance by dimensionality reduction while we try to learn the geodesic distance function directly on the manifold. It is worth noting that the vector diffusion distance is a variation of the geodesic distance. Secondly, they use different approximation methods. VDM approximates the parallel transport by learning an orthogonal transformation and we simply use projection adopted from [15]. GDL can also be regarded as a generalization of the heat method [6] on scalar fields. Both methods employ heat flow to obtain the gradient field of distance function. The algorithm proposed in [6] first learns a scalar field by heat flow on scalar fields and then learns the desired vector field by evaluating the gradient field of the obtained scalar field. Our method tries to learn the desired vector field directly by heat flow on vector fields. Note that the scalar field is zero order and the vector field is first order. It is expected that the first order approximation of the vector field might be more effective for high dimensional data.\nThere are several interesting future directions suggested in this work. One is to generalize the theory and algorithm in this paper to the multi-manifold case. The main challenge is that how to transport an initial vector field from one manifold to other manifolds. One feasible idea is to transport the vector from one manifold to another using the parallel transport in the ambient space since each tangent vector on the manifold is also a vector in the ambient space. Another direction is to estimate the true underlying manifold structure of the data despite the noise, e.g., the manifold dimension.\n2http://www.stanford.edu/group/SOL/software/lsqr.html\nWe employ the tangent space structure to model the manifold and each tangent space is estimated by performing PCA. Note that the dimension of the manifold equals to the dimension of the tangent space. Therefore, if we can combine the work of PCA with noisy data and our framework, it might provide new methods and perspectives to the manifold dimension estimation problem. The third direction is to develop the machine learning theory and design efficient algorithms using heat flow on vector fields as well as other general partial differential equations."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we empirically evaluate the effectiveness of our proposed Geodesic Distance Learning (GDL) algorithm in comparison with three representative distance metric learning algorithms: Laplacian Eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian Eigenmaps (HLLE, [8]) as well as two state-of-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]). As LE, MVU and HLLE cannot directly obtain the distance function, we compute the embedding first and then compute the Euclidean distance between data points in the embedded Euclidean space.\nWe empirically set t = 1 for GDL in all experiments as GDL performs very stable when t varies. The dimension of the manifold d is set to 2 in the synthetic example. For real data, we perform cross-validation to choose d. Specifically, d = 9 for the CMU PIE data set and d = 2 for the Corel data set. We use the same nearest neighbor graph for all six algorithms. The number of nearest neighbors is set to 16 on both synthetic and real data sets and the weight is the simple 0− 1 weight."
    }, {
      "heading" : "4.1 Geodesic Distance Learning",
      "text" : "A simple synthetic example is given in Fig. 3. We randomly sample 2000 data points from a torus. It is a 2-dimensional manifold in the 3-dimensional Euclidean space. The base point is marked by the black dot on the right side of the torus. Figs. 3(a) shows the ground truth geodesic distance function which is computed by shortest path distance. Fig. 3(b)-(d) and (f)-(h) visualize the distances functions learned by different algorithms respectively. To better evaluate the results, we compute the error by using the equation 1n ∑n i=1 |f(xi)−d(xq, xi)|, where f(xi) represents the learned distance and {d(xq, xi)} represents the ground truth distance. To remove the effect of scale, {f(xi)} and {d(xq, xi)} are rescaled to the range [0, 1]. As can be seen from Fig. 3, GDL better preserves the distance metric on the torus. Although MVU is comparable to GDL in this example, GDL is approximately thirty times faster than MVU. It might be worth noting that both MR and PFRank achieve poor performance since they are deigned to preserve the ranking order but not the distance."
    }, {
      "heading" : "4.2 Image Retrieval",
      "text" : "In this section, we apply our GDL algorithm to the image retrieval problem in real world image databases. Two real world data sets are used in our experiments. The first one is from the CMU PIE face database [22], which contains 32 × 32 cropped face images of 68 persons. We choose the frontal pose (C27) with varying lighting conditions, which leaves us 42 images per person. The second data set contains 5,000 images of 50 semantic categories, from the Corel database. Each image is extracted to be a 297-dimensional feature vector. Both of the two image data sets we use have category labels. For each data set, we randomly choose 10 images from each category as queries, and average the retrieval performance over all the queries.\nWe use precision, recall, and Mean Average Precision (MAP, [16]) to evaluate the retrieval results of different algorithms. Precision is defined as the number of relevant presented images divided by the number of presented images. Recall is defined as the number of relevant presented images divided by the total number of relevant images in our database. Given a query, let ri be the relevance score of the image ranked at position i, where ri = 1 if the image is relevant to the query and ri = 0 otherwise. Then we can compute the Average Precision (AP):\nAP = ∑ i ri × Precision@i\n# of relevant images . (7)\nMAP is the average of AP over all the queries.\nFig. 4(a) and Fig. 4(b) show the average precision-scope curves of various methods on the two data sets, respectively. The scope means the number of top-ranked images returned to the user. The precision-scope curves describe the precision with various scopes, and therefore provide an overall performance evaluation of the algorithms. As can be seen from Fig. 4(a) and Fig. 4(b), our proposed GDL algorithm outperforms all the other algorithms. We also present the recall and MAP scores of different algorithms on the two data sets in Table 1 and Table 2, respectively. MAP provides a single figure measure of quality across all the recall levels. Our GDL achieves the highest MAP, indicating reliable performance over the entire ranking list. We also performed comprehensive t-test with 99% confidence level. The improvements of GDL compared to PFRank and other algorithms are significant with most of the p-values less than 10−3, including those in Fig. 4(a), Fig. 4(b), Table 1 and Table 2. These results indicate that learning the distance function directly on the manifold might be better than learning the distance function after embedding."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we study the geodesic distance from the vector field perspective. We provide theoretical analysis to precisely characterize the geodesic distance function and propose a novel heat flow on vector fields approach to learn it. Our experimental results on synthetic and real data demonstrate the effectiveness of the proposed method. The future work includes developing the machine learning theory and designing efficient algorithms using heat flow on vector fields as well as other general partial differential equations."
    }, {
      "heading" : "6 Appendix. Justification",
      "text" : "We first show that solving Eq. (1) is equivalent to solving the heat equation on vector fields. According to the Bochner technique [20], with appropriate boundary conditions we have ∫ M ‖∇V ‖ 2 HSdx =∫\nM g(V,∇ ∗∇V )dx, where ∇∗∇ is the connection Laplacian operator. Define the inner product (·, ·) on the space of vector fields as (X,Y ) = ∫ M g(X,Y )dx. Then we can rewrite E(V ) as E(V ) = (V −V 0, V −V 0) + t(V,∇∗∇V ). The necessary condition of E(V ) to have an extremum at V is that the functional derivative δE(V )/δV = 0 [1]. Using the calculus rules of the functional derivative and the fact∇∗∇ is a self-adjoint operator, we have δE(V )/δV = 2V −2V 0+2t∇∗∇V . A detailed derivation can be found in the appendix. Since ∇∗∇ is also a positive semi-definite operator, the optimal V is then given by:\nV = (I + t∇∗∇)−1V 0, (8)\nwhere I is the identity operator on vector fields. Let X(t) be a vector field valued function. That is, for each t, X(t) is a vector field on the manifold. Given an initial vector field X(t)|t=0 = X0, the heat equation on vector fields [3] is given by ∂X(t)∂t +∇\n∗∇X(t) = 0. When t is small, we can discrete it as follows: X(t)−X0t +∇ ∗∇X(t) = 0. Then X(t) can be solved as\nX(t) = (I + t∇∗∇)−1X0. (9)\nIf we set X0 = V 0, then Eq. (9) is exactly the same as Eq. (8). Therefore when t is small, solving Eq (1) is essentially solving the heat equation on vector fields.\nNext we analyze the asymptotic behavior of X(t) and show that the heat equation transfers the initial vector field primarily along geodesics. Let x, y ∈ M, then X(t) can be obtained via the heat kernel as X(t)(x) = ∫ M k(t, x, y)X0(y)dy, where k(t, x, y) is the heat kernel for the connection Laplacian. It is well known for small t, we have the asymptotic expansion of the heat kernel [3]:\nk(t, x, y) ≈ ( 1 4πt ) d 2 e−d(x,y) 2/4tτ(x, y), (10)\nwhere d(·, ·) is the distance function, τ : TyM→ TxM is the parallel transport along the geodesic connecting x and y.\nNow we consider X0 = V 0. By construction X0(y) = 0 if y /∈ U . Then the vector X(t)(x) = ∫ U e−d(x,y) 2/4tτ(x, y)X0(y)dy up to a scale. To analyze what X(t)(x) is, we first\nmap the manifold M to the tangent space TpM by using exp−1p . Then U becomes a ball in TpM; please see Fig. 5. In the following we will still use x and U to represent exp−1p (x) and exp−1p (U ) for simplicity of notation. Given any point x ∈ TpM, we can decompose the ball U as U = ∪ ′,sU ′,s where U ′,s := {y|d(p, y) = ′, d(x, y) = s}, ′ ≤ and 0 ≤ s ≤ ∞. Then each section U ′,s is a sphere centered at some point lying on the line segment connecting p and x. Therefore U ′,s is symmetric with respect to the vector x − p. For any y ∈ U ′,s, there is a unique reflection point ȳ such that τ(x, y)X0(y) + τ(x, ȳ)X0(ȳ) is parallel to τ(x, y′)X0(y′) where y′ = arg miny∈U d(x, y). Note that the weight e\n−d(x,y)2/4t is the same on the section U ′,s. We conclude that ∫ U ′,s\ne−d(x,y) 2/4tτ(x, y)X0(y)dy is parallel to τ(x, y′)X0(y′). Since∫ U = ∫ ′ ∫ s ∫ U ′,s , X0(x) ≈ ∫ U e−d(x,y)\n2/4tτ(x, y)X0(y)dy is parallel to τ(x, y′)X0(y′). In other words, the vector field flows primarily along geodesics. Therefore given an initial distance vector field around the base point, solving the heat equation will get a vector field which is approximately parallel to the gradient field of the distance function at each point. We can further normalize the vector field at each point to obtain the gradient field of the distance function. From this heat equation point of view, it also provides guidance on the algorithm setting. Specifically, we should set the initial vector field uniformly around the base point and set a small t."
    }, {
      "heading" : "7 Appendix. Backgrounds on Riemannian Geometry",
      "text" : "Let (M, g) be a d-dimensional Riemannian manifold, where g is a Riemannian metric tensor onM. The goal of distance metric learning on the manifold is to find a desired distance function d(x, y) such that it provides a natural measure for the similarity between two data points x and y on the manifold. A distance function d(·, ·) defined by its Riemannian metric g is often called an intrinsic distance function. In this paper, we study a fundamental intrinsic distance function - the geodesic distance function. Similar to many differential geometry textbooks (e.g., [12, 20]), we simply call it the distance function."
    }, {
      "heading" : "7.1 Tangent Spaces and Vector Fields",
      "text" : "As the manifold is locally a Euclidean space, the key tool for studying the manifold will be the idea of linear approximation. The fundamental linear structure of the manifold is the tangent space.\nDefinition 7.1 (Tangent space, [13]). LetM be a smooth manifold and let p be a point onM. A linear map X : C∞(M)→ R is called a derivation at p if it satisfies\nX(fg) = f(p)Xg + g(p)Xf\nfor all smooth functions f, g ∈ C∞(M). The set of all derivations of C∞(M) at p is a vector space called the tangent space toM at p, and is denoted by TpM. An element of TpM is called a tangent vector at p.\nThe definition of the tangent space is totally abstract. We first take an example in Euclidean space to show that why a tangent vector is a derivation. Let v denote a geometric tangent vector in Rm. Define a map Dv|a : C∞(Rm)→ R, which takes the directional derivative in the direction v at a:\nDv|af = Dvf(a) := d\ndt |t=0f(a+ tv).\nClearly this operation is linear and it satisfies the derivation rule. Therefore we might write the directional derivative of f in the direction of Y as Y f = Y (f) = DY f = ∇Y f , where ∇ denotes the covariant derivative on the manifold. Next we show what a tangent space is on the manifold by using local coordinates. Let {xi|i = 1, . . . , d} denote a local coordinate chart around p. Then it can be easily verified by definition that ∂i|p := ∂∂xi |p is a tangent vector at p. Moreover, these coordinate vectors ∂1|p, . . . , ∂d|p form a basis for TpM [13]. Therefore, the dimension of the tangent space is exactly the same as the dimension of the manifold. For example, consider a two dimensional sphere embedded in R3; given any point of the sphere, the tangent space of the sphere is just a two dimensional plane.\nFor any smooth manifoldM, we define the tangent bundle ofM, denoted by TM, to be the disjoint union of the tangent spaces at all points ofM: TM = ∪p∈MTpM. Now we define the vector field. Definition 7.2 (Vector field, [13]). A vector field is a continuous map X : M → TM, usually written as p 7→ Xp, with the property that for each p ∈M, Xp is an element of TpM.\nIntuitively, a vector field is nothing but a collection of tangent vectors with the continuous constraint. Since at each point, a tangent vector is a derivation. A vector field can be viewed as a directional derivative on the whole manifold. It might be worth noting that each vector Xp of a vector field X must be in the corresponding tangent space TpM. Let X be a vector field on the manifold. We can represent the vector field locally using the coordinate basis as X = ∑d i=1 a\ni∂i, where each ai is a function which is often called the coefficient of X . For the sake of convenience, we will use the Einstein summation convention: when an index variable appears twice in a single term, it implies summation of that term over all the values of the index, i.e., we might simply write ai∂i instead of∑d i=1 a i∂i."
    }, {
      "heading" : "7.2 Distance Functions",
      "text" : "Next, we show how to assign a metric structure on the manifold. For each point p on the manifold, a Riemannian metric tensor g at p is a Euclidean inner product gp on each of the tangent space TpM of M. In addition we assume that gp varies smoothly [20]. This means that for any two smooth vector fields X,Y the inner product gp(Xp, Yp) should be a smooth function of p, where Xp and Yp denote the tangent vectors of X and Y at p. The subscript p will be omitted when it is clear from the context. Thus we might write g(X,Y ) or gp(X,Y ) with the understanding that this is to be evaluated at each p where X and Y are defined. We define the norm of a tangent vector v ∈ TpM as ‖v‖ = √ gp(v, v). Once we have defined a metric tensor on the manifold, we can define distance on the manifold. Let [a, b] be a closed interval in R, and γ : [a, b] → M be a smooth curve. The length of γ can then be defined as l(γ) = ∫ b a ‖dγdt (t)‖dt. The distance between two points p, q on the manifoldM can be defined as:\nd(p, q) := inf{l(γ) : γ : [a, b]→M piecewise smooth curve with γ(a) = p, γ(b) = q}.\nWe call d(·, ·) the distance function and it satisfies the usual axioms of a metric, i.e., positivity, symmetry and triangle inequality [12].\nComputing the distance function d(·, ·) for a given manifold is very difficult. In this paper, we study the distance function d(p, ·) when p is given. Definition 7.3 (Distance function based at a point). LetM be a Riemannian manifold, and let p be a point inM. A distance function onM based at p is defined as rp(x) = d(p, x).\nFor simplicity, we might write r(·) instead of rp(·). Recall from the definition of the distance function that rp(x) measures the minimum path distance between p and x."
    }, {
      "heading" : "7.3 Covariant Derivative",
      "text" : "A vector field can measure the change of functions on the manifold. Now we consider the question of measuring the change of vector fields. Let X = ai∂i be a vector field in Rd where ∂i denotes the standard Cartesian coordinate. Then it is natural to define the covariant derivative of X in the direction Y as ∇YX = (∇Y ai)∂i = Y (ai)∂i. Therefore we measure the change in X by measuring how the coefficients of X change. However, this definition relies on the fact that the coordinate vector field ∂i is constant vector field. In other words, ∇Y ∂i = 0 for any vector field Y . For general coordinate vector fields, they are not always constant. Therefore, we should give a coordinate free definition of the covariant derivative. Theorem 7.1 (The fundamental theorem of Riemannian geometry, [20]). The assignmentX → ∇X on (M, g) is uniquely defined by the following properties:\n1. Y → ∇YX is a (1, 1)-tensor:\n∇αv+βwX = α∇vX + β∇wX.\n2. X → ∇YX is a derivation: ∇Y (X1 +X2) = ∇YX1 +∇YX2,\n∇Y (fX) = (Y f)X + f∇YX\nfor functions f :M→ R.\n3. Covariant differentiation is torsion free:\n∇XY −∇YX = [X,Y ].\n4. Covariant differentiation is metric:\nZg(X,Y ) = g(∇ZX,Y ) + g(X,∇ZY ),\nwhere Z is a vector field.\nHere [·, ·] denotes the Lie derivative on vector fields defined as [X,Y ] = XY − Y X . Any assignment on a manifold that satisfies rules 1-4 is called an Riemannian connection. This connection is uniquely determined by these four rules.\nLet us see what a connection is in local coordinates. Let X and Y be two vector fields on the manifold, we can represent them by local coordinates as X = ai∂i and Y = bj∂j . Now we can compute∇YX in local coordinates using the four rules as follows:\n∇YX = ∇bi∂ia j∂j = b i∇∂iaj∂j = bi∂i(aj)∂j + biaj∇∂i∂j . (11)\nThe second equality holds due to the first rule of the connection and the third equality holds due to the second rule of the connection. Since∇∂i∂j is still a vector field on the manifold, we can further represent it as ∇∂i∂j = Γkij∂k, where γkij are called Christoffel symbols [20]. The Christoffel symbols can be represented in terms of the metric."
    }, {
      "heading" : "7.4 Geodesics",
      "text" : "Let γ : [a, b] → M be a curve inM. Let {xi|i = 1, . . . , d} denote a local coordinate chart of the manifold, then ∂i := ∂∂xi form a basis of the tangent spaces. We can represent γ by the coordinates {xi} as γ(t) = (γ1(t), . . . , γd(t)). The velocity in the t direction is then given by γ′(t) = dγ i\ndt ∂i. A vector field V along γ is by definition a function V : [a, b] → TM with V (t) ∈ Tγ(t)M for all t ∈ [a, b]. Next we show how to define the derivative of the vector field V along the curve γ(t): ∇γ′V = ddtV (t). Since V (t) is a vector field along the curve γ, we can represent it by the basis of\ntangent spaces ∂i as V (t) = ϕi(t)∂i, where ϕi(t) are the coefficients of the vector field V (t). We define\nd dt V (t) :=\ndϕi\ndt ∂i + ϕ\ni∇γ′∂i, (12)\nwhere ∇·· is the covariant derivative on the manifold. One can show that this definition is independent of the choice of ∂i.\nA vector field V along γ is said to be parallel along γ if ∇γ′V = 0. If V and W are parallel fields along γ, then we have that g(V,W ) is constant along γ. It can be seen from the following: d dtg(V,W ) = g( d dtV,W ) + g(V, d dtW ) = g(∇γ′V,W ) + g(V,∇γ′W ) = 0. Therefore, parallel fields along a curve do not change their lengths or their angles relative to each other, just as parallel fields in Euclidean space are of constant length and make constant angles.\nTheorem 7.2 (Existence and Uniqueness of Parallel Fields). If t0 ∈ [a, b] and v ∈ Tγ(t0)M, then there is a unique parallel field V (t) defined on all of [a, b] with V (t0) = v.\nTherefore, given a curve and an initial vector on it, there exists a parallel field along the curve and such a parallel field is unique. Suppose we are given a vector v ∈ Tγ(t0)M. We define the parallel transport of v along γ as the extension of v to a parallel vector field V along γ. Therefore a parallel transport map τ(γ(t1), γ(t2)) : Tγ(t2)M→ Tγ(t1)M can be defined as follows:\nτ(γ(t1), γ(t2))Vγ(t2) = V (t1).\nWe show an example of the parallel transport in Fig. 6. Three parallel fields along three different curves are presented.\nIf we consider V (t) to be γ′(t) but not a general vector field, then ∇γ′γ′ = 0 means a curve γ is parallel along itself. Such a curve is called a geodesic. A geodesic can be viewed as a curved straight line on the curved manifold.\nDefinition 7.4 (Geodesic, [20]). Let γ : [a, b] → M, t 7→ γ(t) be a smooth curve. γ is called a geodesic if γ′(t) is parallel along γ, i.e.,∇γ′(t)γ′(t) = 0 for all t ∈ [a, b].\nLet us see what it is in local coordinates. Recall Eq. (12) and note that γ′(t) = dγ i\ndt ∂i, we have\n∇γ′(t)γ′(t) = d2γi\ndt2 ∂i +\ndγi\ndt ∇γ′∂i\n= d2γi\ndt2 ∂i +\ndγi\ndt ∇ dγj dt ∂j ∂i\n= d2γi\ndt2 ∂i +\ndγi\ndt\ndγj\ndt ∇∂j∂i\n= d2γk\ndt2 ∂k +\ndγj\ndt\ndγi\ndt ∇∂i∂j\n= d2γk\ndt2 ∂k +\ndγi\ndt\ndγj\ndt Γkij∂k.\nThe first equality holds due to Eq. (12) and the third equality holds due to the first rule of the covariant derivative. We can also derive what ∇γ′(t)γ′(t) is using Eq. (11) and we will get the same result. As can be seen from the derivation, a geodesic can also be characterized by following equations: d 2γk\ndt2 + dγi dt dγj dt Γ k ij = 0 for k = 1, . . . , d. Each equation is a second order nonlinear partial differential equation. If γ is a geodesic, the speed of the geodesic ‖γ′(t)‖ = √ g(γ′(t), γ′(t)) is constant since parallel fields along a curve have constant speed.\nThe geodesic and the distance function is related as follows.\nTheorem 7.3 ([20]). If γ is a local minimum for inf l(γ) with fixed end points, then γ is a geodesic.\nThis theorem tells us that the shortest path on the manifold must be a geodesic. However, the converse is not always true. We call a geodesic a minimal geodesic if it has shortest length among all curves with the same end points. Therefore, given any two points x and y, the distance function measures the distance of the minimal geodesic connecting x and y.\nAnother important concept for studying geodesics is the exponential map.\nDefinition 7.5 ([20]). Let v ∈ TpM be a tangent vector. Then there is a unique geodesic γv satisfying γv(0) = p with the initial tangent vector γ′v(0) = v. The corresponding exponential map is defined as\nexpp(tv) = γv(t).\nThe exponential map expp : TpM→M maps the tangent space TpM to the manifoldM. Given a point p, the exponential map is well-defined at p if expp(tv) exists for any t ≥ 0 and v ∈ TpM. A manifold is called complete or geodesically complete if the exponential map is well-defined at every point of the manifold. When t is small, the exponential map expp(·v) is the minimal geodesic\nconnecting p and γv(t). When t grows up, this property may not hold. Please see Fig. 7 as an example. When t is small, the exponential map expp(tv) is the minimal geodesic connecting p and γv(t). When t grows up, the exponential map expp(tv) will pass through the opposite point of p. Then expp(tv) is no longer a minimal geodesic connecting p and γv(t). Let t0 = sup{t > 0| γ is the unique minimal geodesic connecting p and γv(t)}. If t0 < +∞, we call γv(t0) the cut point of p. All cut points of p constitute the cut locus of p and we denote it as Cut(p). Given a point p, there is at most one cut point for each unit vector v ∈ TpM. Since the space of unit vector in TpM is a (d − 1)-dimensional unit sphere, the dimension of the cut locus is at most d − 1. Therefore, the measure of the cut locus for a given point is zero. On the standard round sphere, the cut locus of a point consists of the single point opposite of it. The distance function rp(x) is differentiable everywhere except at the point p and its cut locus Cut(p)."
    }, {
      "heading" : "8 Appendix. Connection Laplacian and Functional Derivative on Vector Fields",
      "text" : "In this section, we introduce the connection Laplacian operator and the functional derivative on vector fields."
    }, {
      "heading" : "8.1 Connection Laplacian",
      "text" : "Let Γ(TM) denote the space of smooth vector fields. Let X,Y ∈ Γ(TM) be two smooth vector fields. Then it can be easily verified that X + Y and fX are smooth vector fields for any smooth function f . Therefore Γ(TM) is a vector space. Let T be a (1, 1)-tensor. Then T is a linear operator T : Γ(TM)→ Γ(TM). The adjoint T ∗ can be defined as a linear operator satisfying\ng(TX, Y ) = g(X,T ∗Y ),\nfor all vector fields X,Y ∈ Γ(TM). We can further define the inner product of two (1, 1) tensors T1, T2 as 〈T1, T2〉 := tr(T ∗1 T2), where tr denotes the trace. The trace of a linear operator L : Γ(TM) → Γ(TM) can be given as tr(L) = ∑ i g(L(∂i), ∂i)/g(∂i, ∂i) where {∂i} is a basis of Γ(TM).\nNote that the connection∇ is a linear operator: ∇ : Γ(TM)→ L(Γ(TM),Γ(TM)),\nwhere L(Γ(TM),Γ(TM)) denotes the space of linear operators on Γ(TM). We can get an adjoint operator ∇∗ : L(Γ(TM),Γ(TM))→ Γ(TM) defined implicitly by∫\nM g(∇∗T, Y ) = ∫ M 〈T,∇Y 〉,\nfor any X,Y ∈ Γ(TM). If we set T = ∇X , then we get a linear operator ∇∗∇ : X 7→ ∇∗∇X . ∇∗∇ is called the connection Laplacian on vector fields. By the construction of the connection Laplacian, it is not difficult to see that it is a positive semidefinite self-adjoint operator. By definition, we also have the following equation:∫\nM g(∇∗∇X,Y ) = ∫ M 〈∇X,∇Y 〉.\nThere is another way to define the connection Laplacian using local coordinates. Consider the second covariant derivative and take the trace ∑d i=1∇2∂i,∂iX with respect to some orthonormal frame ∂i. This can be seen to be invariantly defined. We shall use the notation:\ntr(∇2X) = d∑ i=1 ∇2∂i,∂iX,\ntr(∇2) = d∑ i=1 ∇2∂i,∂i .\n(13)\nThe connection Laplaican∇∗∇ equals to −tr(∇2) [20]."
    }, {
      "heading" : "8.2 Functional Derivative",
      "text" : "Our objective function for learning a vector field V is defined as follows:\nmin V E(V ) := ∫ M ‖V − V 0‖2 + t ∫ M ‖∇V ‖2HS, (14)\nwhere ‖ · ‖HS denotes the Hilbert-Schmidt tensor norm [7]. We already showed in the last section that Eq. (14) is equivalent to the following equation:\nmin V E(V ) := ∫ M ‖V − V 0‖2 + t ∫ M g(V,∇∗∇V ).\nThe necessary condition of E(V ) to have an extremum at V is that the functional derivative δE(V )/δV = 0 [1]. Next we show how to compute the functional derivative δE(V )/δV .\nDefinition 8.1 ([1]). Let M,N be normed vector spaces, U an open subset of M and let f : U ⊂ M → N be a given mapping. Let u0 ∈ U . We say that f is differentiable at the point u0 provided there is a bounded linear mapping Df(u0) : M → N such that for every > 0, there is a δ > 0 such that whenever 0 < ‖u− u0‖ < δ, we have\n‖f(u)− f(u0)−Df(u0) · (u− u0)‖ ‖u− u0‖ < ,\nwhere ‖ · ‖ represents the norm on the appropriate space and where the evaluation of Df(u0) on e ∈M is denoted Df(u0) · e.\nThe derivative Df(u0) if it exist, is unique [1]. f : U ⊂M → N is differentiable at u0 ∈ U if and only if there exists a linear mapping Df(u0) ∈ L(M,N) such that\nf(u0 + e) = f(u0) +Df(u0) · e+ o(e),\nwhere o(e) denotes a continuous function of e defined in a neighborhood of the origin of a normed vector space M , satisfying lime→0(o(e)/‖e‖) = 0. For any two vector fields X and Y , define the inner product (·, ·) on the space of vector fields as\n(X,Y ) = ∫ M g(X,Y )dx.\nThe norm of X can be defined as ‖X‖2 = ∫ M g(X,X)dx. Therefore (Γ(TM), ‖ · ‖) is a normed vector space. We first consider the functional (V − V 0, V − V 0). We have\n(V − V 0 + e, V − V 0 + e) = (V − V 0, V − V 0) + 2(V − V 0, e) + (e, e).\nTherefore 2(V − V 0) is a derivative of (V − V 0, V − V 0) with respect to V . Since if the derivative exists, it is unique. Therefore 2(V − V 0) is exactly the unique derivative of (V − V 0, V − V 0). Similarly, we expand the functional (V,∇∗∇V ) as\n(V + e,∇∗∇(V + e)) = (V,∇∗∇V ) + (V,∇∗∇e) + (e,∇∗∇V ) + (e,∇∗∇e) = (V,∇∗∇V ) + 2(e,∇∗∇V ) + (e,∇∗∇e) = (V,∇∗∇V ) + 2(e,∇∗∇V ) + o(e)\nwhere the second equality holds due to that ∇∗∇ is a self-adjoint operator and the third equality holds due to that ∇∗∇ is a bounded operator. Therefore, the derivative of (V,∇∗∇V ) with respect to V is 2∇∗∇V . In summary, we have\nδE(V )\nδV = 2V − 2V 0 + 2t∇∗∇V."
    }, {
      "heading" : "9 Appendix. Proof of Main Theorems",
      "text" : ""
    }, {
      "heading" : "9.1 Proof of Theorem 2.2",
      "text" : "Proof. (⇒) (a) SinceM is a complete manifold, according to Hopf-Rinow Theorem [20], it is also geodesic complete. That is, for every p in M, the exponential map expp is defined on the entire tangent space TpM. Therefore, for any x ∈ M \\ p ∪ Cut(p), there exist some v ∈ TpM such that x = expp(v). Actually expp(tv), 0 ≤ t ≤ 1, is the unique minimal geodesic connecting p and x. Thus r(x) = d(p, x) = l(expp(v)) = ‖v‖ = ‖ exp−1p (x)‖.\n(b) Since the distance function d(p, x) has constant speed 1, we have ‖∂r‖ = 1. For any q ∈M, let γ(s) : [0, t] → M be a curve connecting p and q, i.e., γ(0) = p and γ(t) = q. Then the length of the curve satisfies the following inequality:\nl(γ) = ∫ t 0 ‖γ′(s)‖ds\n= ∫ t 0 ‖∂r‖‖γ′(s)‖ds\n≥ ∫ t 0 |g(∂r, γ′(s))|ds\n≥ | ∫ t 0 dr ◦ γ ds ds| = r(q)− r(p) = r(q).\nHere the second equality holds due to ‖∂r‖ = 1 and first inequality holds due to the Cauchy-Schwarz inequality. Since γ is arbitrary, we have d(p, q) ≥ r(q). Let γ be an integral curve [13] of ∂r, i.e., γ′(s) = ∂r ◦ γ(s). Then the equality holds in the Cauchy-Schwarz inequality and dr◦γds > 0. Thus l(γ) = |r(q)− r(p)| = r(q) = d(p, q). This shows that the integral curve of ∂r must be a minimal geodesic. According to the definition of geodesic, we have∇∂r∂r = 0 hold whenever r is smooth. (⇐) We first show that each integral curve of ∂r inM\\ p ∪ Cut(p) must be a geodesic. Let γ(s) be an integral curve of ∂r. According to the definition of the integral curve, we have γ′(s) = ∂r. Therefore ∇γ′(s)γ′(s) = ∇∂r∂r = 0 which implies γ(s) is a geodesic. Since geodesics have constant nonzero speed, we have ‖γ′(s)‖ = ‖∂r‖ > 0 for each integral curve γ(s). Recall that if a vector field is smooth at some point, then there exists a unique integral curve passing through it [13]. We have ‖γ′(s)‖ = ‖∂r‖ > 0 holds whenever r is smooth. Next we show each integral curve γ(s) must pass through p. We first prove this claim in the neighborhood U of p. For any point x ∈ U , there exists some t0 ≥ 0 and a unit vector v ∈ TpM such that x = expp(t0v). Since r(x) = ‖ exp−1p (x)‖, according to the Gauss lemma [20], we have ∂r = D expp(∂r). Thus the integral curve of ∂r passing through x is the exponential map expp(tv), and we have ‖∂r‖ = ‖Dt expp(tv)‖ = ‖v‖ = 1. So far, for any point x ∈ U , we have proved that the integral curve passing through x must be a geodesic passing through p and vice versa. Next we show for any point x ∈ M \\ p ∪ Cut(p), the integral curve passing through x must pass through p. Since the manifold is complete, according to Hopf-Rinow Theorem [20], there always exists a minimal geodesic connecting any two points on the manifold. Let γ(s) be the unique minimal geodesic connecting p and x satisfies γ(0) = p and γ(t1) = x. Then γ(s)∩U is an integral curve of ∂r in U . Note that each integral curve of ∂r must be a geodesic, due to the global uniqueness of geodesic [20, please see lemma. 7 of chap. 5], so γ(s) is an integral curve of ∂r onM\\ p ∪ Cut(p). Now we are ready to prove r is a distance function, i.e., r(x) = d(p, x). For each point x ∈ M \\ p ∪ Cut(p), we show that r(x) = d(p, x). Let γ(s) be the integral curve connecting p and x, without loss of generality, we assume that γ(0) = p, γ(t) = x for some t > 0. According to the condition (a), we have ‖γ′(s)‖ = ‖∂r‖ = 1 for small s. Note that γ is also a geodesic, thus γ has constant speed which implies ‖γ′(s)‖ = ‖∂r‖ = 1 for s ∈ [0, t]. Thus\nr(x) = r(x)− r(p) = ∫ x p ‖∂r‖dx = ∫ t 0 ‖γ′(s)‖ds = t.\nSince l is a geodesic with speed 1, we have d(p, x) = ∫ t 0 ‖γ′(s)‖ds = t which implies r(x) = d(p, x).\nFinally, we show that r(x) = d(p, x) holds even if x ∈ Cut(p). Choose any geodesic γ connecting p and x. We first show that there is no any other cut point on γ. If not, assume there is a point z on γ between p and x. Then we have d(p, z) < d(p, x) which is contradict to the definition of the cut point. Since x is the unique cut point on γ, we can choose a sequence of points xi on γ such that its limit being x. Then we have r(x) = limxi→x r(xi) = limxi→x d(p, xi) = d(p, x)."
    }, {
      "heading" : "9.2 Proof of Theorem 2.3",
      "text" : "Proof. (⇒) is true throughout the proof of Theorem 2.2. Next we show (⇐). Compared to Theorem 2.2, we only have to show ∇∂r∂r = 0 onM\\ p ∪ Cut(p). Let S(·) denote the (1,1) version of the Hessian of r, S(·) = ∇·∂r, i.e., Hessr(X,Y ) = g(S(X), Y ). It is evident that Hessr is a symmetric tensor, thus Hessr(X,Y ) = Hessr(Y,X).\nFor any vector field Y onM\\ p ∪ Cut(p), then\ng(∇∂r∂r, Y ) = Hessr(∂r, Y ) = Hessr(Y, ∂r)\n= g(∇Y ∂r, ∂r)\n= 1\n2 Y g(∂r, ∂r)\n= 1\n2 Y (1) = 0.\nThe second equality holds due to the symmetry of Hessr, the third equality holds due to the definition of Hessr and the fourth equality holds due to the property of the covariant derivative. Thus∇∂r∂r = 0 holds onM\\ p ∪ Cut(p). Since r satisfies conditions (a) and (b) in Theorem 2.2, r is a distance function based at p."
    } ],
    "references" : [ {
      "title" : "Manifolds",
      "author" : [ "R. Abraham", "J.E. Marsden", "T. Ratiu" ],
      "venue" : "tensor analysis, and applications, volume 75 of Applied Mathematical Sciences. Springer-Verlag, New York, second edition",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Laplacian eigenmaps and spectral techniques for embedding and clustering",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Heat kernels and Dirac operators",
      "author" : [ "N. Berline", "E. Getzler", "M. Vergne" ],
      "venue" : "Springer-Verlag",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Spectral Graph Theory",
      "author" : [ "F.R.K. Chung" ],
      "venue" : "volume 92 of Regional Conference Series in Mathematics. AMS",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Diffusion maps",
      "author" : [ "R.R. Coifman", "S. Lafon" ],
      "venue" : "Applied and Computational Harmonic Analysis, 21(1):5 – 30",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Geodesics in heat: A new approach to computing distance based on heat flow",
      "author" : [ "K. Crane", "C. Weischedel", "M. Wardetzky" ],
      "venue" : "ACM Trans. Graph., 32(5):152:1–152:11",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Tensor Norms and Operator Ideals",
      "author" : [ "A. Defant", "K. Floret" ],
      "venue" : "North-Holland Mathematics Studies, North-Holland, Amsterdam",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data",
      "author" : [ "D.L. Donoho", "C.E. Grimes" ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America, 100(10):5591–5596",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Parallel field ranking",
      "author" : [ "M. Ji", "B. Lin", "X. He", "D. Cai", "J. Han" ],
      "venue" : "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’12, pages 723–731",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Regularized distance metric learning:theory and algorithm",
      "author" : [ "R. Jin", "S. Wang", "Y. Zhou" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Principal Component Analysis",
      "author" : [ "I.T. Jolliffe" ],
      "venue" : "Springer-Verlag, New York",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Riemannian Geometry and Geometric Analysis (5",
      "author" : [ "J. Jost" ],
      "venue" : "ed.). Springer",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Introduction to Smooth Manifolds",
      "author" : [ "J.M. Lee" ],
      "venue" : "Springer Verlag, New York, 2nd edition",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Parallel vector field embedding",
      "author" : [ "B. Lin", "X. He", "C. Zhang", "M. Ji" ],
      "venue" : "Journal of Machine Learning Research, 14:2945–2977",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Semi-supervised regression via parallel field regularization",
      "author" : [ "B. Lin", "C. Zhang", "X. He" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Introduction to Information Retrieval",
      "author" : [ "C.D. Manning", "P. Raghavan", "H. Schtze" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Hamilton-jacobi equations and distance functions on riemannian manifolds",
      "author" : [ "C. Mantegazza", "A.C. Mennucci" ],
      "venue" : "Applied Mathematics and Optimization, 47(1):1–26",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Metric learning to rank",
      "author" : [ "B. McFee", "G. Lanckriet" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 775–782",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fast computation of weighted distance functions and geodesics on implicit hyper-surfaces",
      "author" : [ "F. Mémoli", "G. Sapiro" ],
      "venue" : "Journal of Computational Physics, 173(2):730 – 764",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Riemannian Geometry",
      "author" : [ "P. Petersen" ],
      "venue" : "Springer, New York",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Nonlinear dimensionality reduction by locally linear embedding",
      "author" : [ "S. Roweis", "L. Saul" ],
      "venue" : "Science, 290(5500):2323–2326",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "The CMU pose",
      "author" : [ "T. Sim", "S. Baker", "M. Bsat" ],
      "venue" : "illuminlation, and expression database. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(12):1615–1618",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Vector diffusion maps and the connection Laplacian",
      "author" : [ "A. Singer", "H.-T. Wu" ],
      "venue" : "Communications on Pure and Applied Mathematics, 65(8):1067–1144",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "V",
      "author" : [ "J. Tenenbaum" ],
      "venue" : "de Silva, and J. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "K. Weinberger", "J. Blitzer", "L. Saul" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    }, {
      "title" : "Learning a kernel matrix for nonlinear dimensionality reduction",
      "author" : [ "K.Q. Weinberger", "F. Sha", "L.K. Saul" ],
      "venue" : "Proceedings of the twenty-first international conference on Machine learning (ICML-04), ICML ’04, pages 839–846",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Distance metric learning with application to clustering with side-information",
      "author" : [ "E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell" ],
      "venue" : "Advances in Neural Information Processing Systems 15, pages 505–512",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Ranking on data manifolds",
      "author" : [ "D. Zhou", "J. Weston", "A. Gretton", "O. Bousquet", "B. Schölkopf" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "It has been applied widely in many problems, such as information retrieval [18], classification and clustering [27].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "It has been applied widely in many problems, such as information retrieval [18], classification and clustering [27].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 26,
      "context" : "In supervised learning, one often assumes that data points with the same label should have small distance, while data points with different labels should have large distance [27, 25, 10].",
      "startOffset" : 174,
      "endOffset" : 186
    }, {
      "referenceID" : 24,
      "context" : "In supervised learning, one often assumes that data points with the same label should have small distance, while data points with different labels should have large distance [27, 25, 10].",
      "startOffset" : 174,
      "endOffset" : 186
    }, {
      "referenceID" : 9,
      "context" : "In supervised learning, one often assumes that data points with the same label should have small distance, while data points with different labels should have large distance [27, 25, 10].",
      "startOffset" : 174,
      "endOffset" : 186
    }, {
      "referenceID" : 10,
      "context" : "The classical Principal Component Analysis (PCA, [11]) can be considered as linear manifold learning method in which the map F is linear.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 23,
      "context" : "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 7,
      "context" : "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 25,
      "context" : "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 4,
      "context" : "The typical nonlinear manifold learning approaches include Isomap [24], Locally Linear Embedding (LLE, [21]), Laplacian Eigenmaps (LE, [2]), Hessian Eigenmaps (HLLE, [8]), Maximum Variance Unfolding (MVU, [26]) and Diffusion Maps [5].",
      "startOffset" : 230,
      "endOffset" : 233
    }, {
      "referenceID" : 4,
      "context" : "Coifman and Lafon [5] also showed that both LLE and LE belong to the diffusion map framework which preserves the local structure of the manifold.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 23,
      "context" : ", [24]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "However, it is well known that computing pairwise shortest path distance is time consuming and it cannot handle the case when the manifold is not geodesically convex [8].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 18,
      "context" : "[19] proposes an iterated algorithm for solving the Hamilton-Jacobi equation ‖∇r‖ = 1 [17], where ∇r represents the gradient field of the distance function.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[19] proposes an iterated algorithm for solving the Hamilton-Jacobi equation ‖∇r‖ = 1 [17], where ∇r represents the gradient field of the distance function.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "Note that the tangent space dimension is equal to the manifold dimension [13] which is usually much smaller than the ambient dimension.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : "Inspired by recent work on vector fields [23, 15, 14] and heat flow on scalar fields [6], we propose to learn the geodesic distance function via the characterization of its gradient field and heat flow on vector fields.",
      "startOffset" : 41,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "Inspired by recent work on vector fields [23, 15, 14] and heat flow on scalar fields [6], we propose to learn the geodesic distance function via the characterization of its gradient field and heat flow on vector fields.",
      "startOffset" : 41,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "Inspired by recent work on vector fields [23, 15, 14] and heat flow on scalar fields [6], we propose to learn the geodesic distance function via the characterization of its gradient field and heat flow on vector fields.",
      "startOffset" : 41,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "Inspired by recent work on vector fields [23, 15, 14] and heat flow on scalar fields [6], we propose to learn the geodesic distance function via the characterization of its gradient field and heat flow on vector fields.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : ", [12, 20]), we call it the distance function.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 19,
      "context" : ", [12, 20]), we call it the distance function.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 11,
      "context" : ", positivity, symmetry and triangle inequality [12].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 19,
      "context" : "2 (Geodesic, [20]).",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 19,
      "context" : "1 ([20]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "where ‖ · ‖HS denotes the Hilbert-Schmidt tensor norm [7] and t > 0 is a parameter.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "Following [15], the discrete form of our objective functions can be given as follows: E(V ) = V V − 2V 0V + V 0V 0 + tV BV, Φ(f) = 2fLf + V̂ GV̂ − 2V̂ Cf, (4)",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "where L is the graph Laplacian matrix [4], B is a dn × dn block matrix, G is a dn × dn block diagonal matrix and C is a dn × n block matrix.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : "It might be worth noting that one can also approximate the parallel transport by solving a singular value decomposition problem [23].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : "Our approach is based on the idea of vector field regularization which is similar to Vector Diffusion Maps (VDM, [23]).",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : "VDM approximates the parallel transport by learning an orthogonal transformation and we simply use projection adopted from [15].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "GDL can also be regarded as a generalization of the heat method [6] on scalar fields.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "The algorithm proposed in [6] first learns a scalar field by heat flow on scalar fields and then learns the desired vector field by evaluating the gradient field of the obtained scalar field.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "In this section, we empirically evaluate the effectiveness of our proposed Geodesic Distance Learning (GDL) algorithm in comparison with three representative distance metric learning algorithms: Laplacian Eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian Eigenmaps (HLLE, [8]) as well as two state-of-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]).",
      "startOffset" : 220,
      "endOffset" : 223
    }, {
      "referenceID" : 25,
      "context" : "In this section, we empirically evaluate the effectiveness of our proposed Geodesic Distance Learning (GDL) algorithm in comparison with three representative distance metric learning algorithms: Laplacian Eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian Eigenmaps (HLLE, [8]) as well as two state-of-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]).",
      "startOffset" : 259,
      "endOffset" : 263
    }, {
      "referenceID" : 7,
      "context" : "In this section, we empirically evaluate the effectiveness of our proposed Geodesic Distance Learning (GDL) algorithm in comparison with three representative distance metric learning algorithms: Laplacian Eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian Eigenmaps (HLLE, [8]) as well as two state-of-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]).",
      "startOffset" : 294,
      "endOffset" : 297
    }, {
      "referenceID" : 27,
      "context" : "In this section, we empirically evaluate the effectiveness of our proposed Geodesic Distance Learning (GDL) algorithm in comparison with three representative distance metric learning algorithms: Laplacian Eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian Eigenmaps (HLLE, [8]) as well as two state-of-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]).",
      "startOffset" : 369,
      "endOffset" : 373
    }, {
      "referenceID" : 8,
      "context" : "In this section, we empirically evaluate the effectiveness of our proposed Geodesic Distance Learning (GDL) algorithm in comparison with three representative distance metric learning algorithms: Laplacian Eigenmaps (LE, [2]), Maximum Variance Unfolding (MVU, [26]) and Hessian Eigenmaps (HLLE, [8]) as well as two state-of-art ranking algorithms: Manifold Ranking (MR, [28]) and Parallel Field Rank (PFRank, [9]).",
      "startOffset" : 408,
      "endOffset" : 411
    }, {
      "referenceID" : 0,
      "context" : "To remove the effect of scale, {f(xi)} and {d(xq, xi)} are rescaled to the range [0, 1].",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 21,
      "context" : "The first one is from the CMU PIE face database [22], which contains 32 × 32 cropped face images of 68 persons.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 15,
      "context" : "We use precision, recall, and Mean Average Precision (MAP, [16]) to evaluate the retrieval results of different algorithms.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 19,
      "context" : "According to the Bochner technique [20], with appropriate boundary conditions we have ∫ M ‖∇V ‖ 2 HSdx = ∫ M g(V,∇ ∗∇V )dx, where ∇∗∇ is the connection Laplacian operator.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "The necessary condition of E(V ) to have an extremum at V is that the functional derivative δE(V )/δV = 0 [1].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "Given an initial vector field X(t)|t=0 = X0, the heat equation on vector fields [3] is given by ∂X(t) ∂t +∇ ∗∇X(t) = 0.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "It is well known for small t, we have the asymptotic expansion of the heat kernel [3]:",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : ", [12, 20]), we simply call it the distance function.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 19,
      "context" : ", [12, 20]), we simply call it the distance function.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 12,
      "context" : "1 (Tangent space, [13]).",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : ", ∂d|p form a basis for TpM [13].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "2 (Vector field, [13]).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 19,
      "context" : "In addition we assume that gp varies smoothly [20].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 11,
      "context" : ", positivity, symmetry and triangle inequality [12].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 19,
      "context" : "1 (The fundamental theorem of Riemannian geometry, [20]).",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 19,
      "context" : "Since∇∂i∂j is still a vector field on the manifold, we can further represent it as ∇∂i∂j = Γij∂k, where γ ij are called Christoffel symbols [20].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 19,
      "context" : "4 (Geodesic, [20]).",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 19,
      "context" : "3 ([20]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "5 ([20]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "The connection Laplaican∇∗∇ equals to −tr(∇) [20].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "where ‖ · ‖HS denotes the Hilbert-Schmidt tensor norm [7].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "The necessary condition of E(V ) to have an extremum at V is that the functional derivative δE(V )/δV = 0 [1].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "1 ([1]).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "The derivative Df(u0) if it exist, is unique [1].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "(⇒) (a) SinceM is a complete manifold, according to Hopf-Rinow Theorem [20], it is also geodesic complete.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "Let γ be an integral curve [13] of ∂r, i.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "Recall that if a vector field is smooth at some point, then there exists a unique integral curve passing through it [13].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 19,
      "context" : "Since r(x) = ‖ exp−1 p (x)‖, according to the Gauss lemma [20], we have ∂r = D expp(∂r).",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 19,
      "context" : "Since the manifold is complete, according to Hopf-Rinow Theorem [20], there always exists a minimal geodesic connecting any two points on the manifold.",
      "startOffset" : 64,
      "endOffset" : 68
    } ],
    "year" : 2014,
    "abstractText" : "Learning a distance function or metric on a given data manifold is of great importance in machine learning and pattern recognition. Many of the previous works first embed the manifold to Euclidean space and then learn the distance function. However, such a scheme might not faithfully preserve the distance function if the original manifold is not Euclidean. Note that the distance function on a manifold can always be well-defined. In this paper, we propose to learn the distance function directly on the manifold without embedding. We first provide a theoretical characterization of the distance function by its gradient field. Based on our theoretical analysis, we propose to first learn the gradient field of the distance function and then learn the distance function itself. Specifically, we set the gradient field of a local distance function as an initial vector field. Then we transport it to the whole manifold via heat flow on vector fields. Finally, the geodesic distance function can be obtained by requiring its gradient field to be close to the normalized vector field. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}