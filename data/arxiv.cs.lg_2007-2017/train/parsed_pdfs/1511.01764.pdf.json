{
  "name" : "1511.01764.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Discrete Rényi Classifiers",
    "authors" : [ "Meisam Razaviyayn", "Farzan Farnia", "David Tse" ],
    "emails" : [ "meisamr@stanford.edu", "farnia@stanford.edu", "dntse@stanford.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n01 76\n4v 1\n[ cs\n.L G\n] 5\nN ov\n2 01"
    }, {
      "heading" : "1 Introduction",
      "text" : "Statistical classification, a core task in many modern data processing and prediction problems, is the problem of predicting labels for a given feature vector based on a set of training data instances containing feature vectors and their corresponding labels. From a probabilistic point of view, this problem can be formulated as follows: given data samples (X1, Y 1), . . . , (Xn, Y n) from a probability distribution P(X, Y ), predict the target label ytest for a given test point X = xtest.\nMany modern classification problems are on high dimensional categorical features. For example, in the genome-wide association studies (GWAS), the classification task is to predict a trait of interest based on observations of the SNPs in the genome. In this problem, the feature vector X = (X1, . . . , Xd) is categorical with Xi ∈ {0, 1, 2}.\nWhat is the optimal classifier leading to the minimum misclassification rate for such a classification problem with high dimensional categorical feature vectors? When the joint probability distribution of the random vector (X, Y ) is known, the MAP decision rule defined by δMAP , argmaxy P(Y =\n∗Department of Electrical Engineering, Stanford University, Stanford, CA 94305.\ny|X = x) achieves the minimum misclassification rate. However, in practice the joint probability distribution P(X, Y ) is not known. Moreover, estimating the complete joint probability distribution is not possible due to the curse of dimensionality. For example, in the above GWAS problem, the dimension of the feature vector X is d ≈ 3, 000, 000 which leads to the alphabet size of 33,000,000 for the feature vector X. Hence, a practical approach is to first estimate some low order marginals of P(X, Y ), and then use these low order marginals to build a classifier with low misclassification rate. This approach, which is the sprit of various machine learning and statistical methods [2–6], is also useful when the complete data instances are not available due to privacy concerns in applications such as medical informatics.\nIn this work, we consider the above problem of building a classifier for a given set of low order marginals. First, we formally state the problem of finding the robust classifier with the minimum worst case misclassification rate. Our goal is to find a (possibly randomized) decision rule which has the minimum worst case misclassification rate over all probability distributions satisfying the given low order marginals. Then a surrogate objective function, which is obtained by the minimum HGR correlation principle [1], is used to propose a randomized classification rule. The proposed classification method has the worst case misclassification rate no more than twice the misclassification rate of the optimal classifier. When only pairwise marginals are estimated, it is shown that this classifier is indeed a randomized linear regression classifier on indicator variables under a separability condition. Then, we formulate a feature selection problem based on the knowledge of pairwise marginals which leads to the minimum misclassification rate. Our analysis provides a theoretical justification for using group lasso objective function for feature selection over the discrete set of features. Finally, we conclude by presenting numerical experiments comparing the proposed classifier with discrete Chebyshev classifier [2], Tree Augmented Naive Bayes [3], and Minimax Probabilistic Machine [4]. In short, the contributions of this work is as follows.\n• Providing a rigorous theoretical justification for using the minimum HGR correlation principle for binary classification problem.\n• Proposing a randomized classifier with misclassification rate no larger than twice the misclassification rate of the optimal classifier.\n• Introducing a computationally efficient method for calculating the proposed randomized classifier when pairwise marginals are estimated and a separability condition is satisfied.\n• Providing a mathematical justification based on maximal correlation for using group lasso problem for feature selection in categorical data.\nRelated Work: The idea of learning structures in data through low order marginals/moments is popular in machine learning and statistics. For example, the maximum entropy principle [7], which is the spirit of the variational method in graphical models [5] and tree augmented naive Bayes [3], is based on the idea of fixing the marginal distributions and fitting a probabilistic model which maximizes the Shannon entropy. Although these methods fit a probabilistic model satisfying the low order marginals, they do not directly optimize the misclassification rate of the resulting classifier.\nAnother related information theoretic approach is the minimum mutual information principle [8] which finds the probability distribution with the minimum mutual information between the feature vector and the target variable. This approach is closely related to the framework of this paper; however, unlike the minimum HGR principle, there is no known computationally efficient approach for calculating the probability distribution with the minimum mutual information.\nIn the continuous setting, the idea of minimizing the worst case misclassification rate leads to the minimax probability machine [4]. This algorithm and its analysis is not easily extendible to the discrete scenario.\nThe most related algorithm to this work is the recent Discrete Chebyshev Classifier (DCC) algorithm [2]. The DCC is based on the minimization of the worst case misclassification rate over the class of probability distributions with the given marginals of the form (Xi, Xj , Y ). Similar to our framework, the DCC method achieves the misclassification rate no larger than twice the misclassification rate of the optimum classifier. However, computation of the DCC classifier requires solving a non-separable non-smooth optimization problem which is computationally demanding, while the proposed algorithm results in a least squares optimization problem with a closed form solution. Furthermore, in contrast to [2] which only considers deterministic decision rules, in this work we\nconsider the class of randomized decision rules. Finally, it is worth noting that the algorithm in [2] requires tree structure to be tight, while our proposed algorithm works on non-tree structures as long as the separability condition is satisfied."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "Consider the binary classification problem with d discrete features X1, X2, . . . , Xd ∈ X and a target variable Y ∈ Y , {0, 1}. Without loss of generality, let us assume that X , {1, 2, . . . ,m} and the data points (X, Y ) are coming from an underlying probability distribution P̄X,Y (x, y). If the joint probability distribution P̄(x, y) is known, the optimal classifier is given by the maximum a posteriori probability (MAP) estimator, i.e., ŷMAP(x) , argmaxy∈{0,1} P̄(Y = y | X = x). However, the joint probability distribution P̄(x, y) is often not known in practice. Therefore, in order to utilize the MAP rule, one should first estimate P̄(x, y) using the training data instances. Unfortunately, estimating the joint probability distribution requires estimating the value of P̄(X = x, Y = y) for all (x, y) ∈ X d × Y which is intractable for large values of d. Therefore, as mentioned earlier, our approach is to first estimate some low order marginals of the joint probability distribution P̄(·); and then utilize the minimax criterion for classification.\nLet C be the class of probability distributions satisfying the estimated marginals. For example, when only pairwise marginals of the ground-truth distribution P̄ is estimated, the set C is the class of distributions satisfying the given pairwise marginals, i.e.,\nCpairwise , { PX,Y (·, ·) ∣∣ PXi,Xj (xi, xj) = P̄Xi,Xj (xi, xj), PXi,Y (xi, y) = P̄Xi,Y (xi, y),\n∀xi, xj ∈ X , ∀y ∈ Y, ∀i, j\n} .\n(1)\nIn general, C could be any class of probability distributions satisfying a set of estimated low order marginals.\nLet us also define δ to be a randomized classification rule with\nδ(x) =\n{ 0 with probability qxδ\n1 with probability 1− qxδ ,\nfor some qxδ ∈ [0, 1], ∀x ∈ X d. Given a randomized decision rule δ and a joint probability distribution PX,Y (x, y), we can extend P(·) to include our randomized decision rule. Then the misclassification rate of the decision rule δ, under the probability distribution P(·), is given by P(δ(X) 6= Y ). Hence, under minimax criterion, we are looking for a decision rule δ∗ which minimizes the worst case misclassification rate. In other words, the robust decision rule is given by\nδ∗ ∈ argmin δ∈D max P∈C P (δ(X) 6= Y ) , (2)\nwhere D is the set of all randomized decision rules. Notice that the optimal decision rule δ∗ may not be unique in general."
    }, {
      "heading" : "3 Worst Case Error Minimization",
      "text" : "In this section, we propose a surrogate objective for (2) which leads to a decision rule with misclassification rate no larger than twice of the optimal decision rule δ∗. Later we show that the proposed surrogate objective is connected to the minimum HGR principle [1].\nLet us start by rewriting (2) as an optimization problem over real valued variables. Notice that each probability distribution PX,Y (·, ·) can be represented by a probability vector p = [px,y | (x, y) ∈ X d × Y] ∈ R2m d with px,y = P(X = x, Y = y) and ∑ x,y px,y = 1. Similarly, every randomized rule δ can be represented by a vector qδ = [qxδ | x ∈ X d] ∈ Rm d\n. Adopting these notations, the set C can be rewritten in terms of the probability vector p as\nC , { p ∣∣ Ap = b, 1Tp = 1, p ≥ 0 } ,\nwhere the system of linear equations Ap = b represents all the low order marginal constraints in B; and the notation 1 denotes the vector of all ones. Therefore, problem (2) can be reformulated as\nq∗δ ∈ argmin 0≤qδ≤1 max p∈C\n∑\nx\n(qxδ px,1 + (1− q x δ )px,0) , (3)\nwhere px,0 and px,1 denote the elements of the vector p corresponding to the probability values P(X = x, Y = 0) and P(X = x, Y = 1), respectively. The simple application of the minimax theorem [9] implies that the saddle point of the above optimization problem exists and moreover, the optimal decision rule is a MAP rule for a certain probability distribution P∗ ∈ C. In other words, there exists a pair (δ∗,P∗) for which\nP(δ∗(X) 6= Y ) ≤ P∗(δ∗(X) 6= Y ), ∀P ∈ C and P∗(δ(X) 6= Y ) ≥ P∗(δ∗(X) 6= Y ), ∀δ ∈ D.\nAlthough the above observation characterizes the optimal decision rule to some extent, it does not provide a computationally efficient approach for finding the optimal decision rule. Notice that it is NP-hard to verify the existence of a probability distribution satisfying a given set of low order marginals [10]. Based on this observation and the result in [11], we conjecture that in general, solving (2) is NP-hard in the number variables and the alphabet size even when the set C is nonempty. Hence, here we focus on developing a framework to find an approximate solution of (2).\nLet us continue by utilizing the minimax theorem [9] and obtain the worst case probability distribution in (3) by p∗ ∈ argmaxp∈C min0≤qδ≤1 ∑ x (q x δpx,1 + (1 − q x δ )px,0) , or equivalently,\np∗ ∈ argmax p∈C\n∑\nx\nmin {px,0 , px,1} . (4)\nDespite convexity of the above problem, there are two sources of hardness which make the problem intractable for moderate and large values of d. Firstly, the objective function is non-smooth. Secondly, the number of optimization variables is 2md and grows exponentially with the alphabet size. To deal with the first issue, notice that the function inside the summation is the max-min fairness objective between the two quantities px,1 and px,0. Replacing this objective with the harmonic average leads to the following smooth convex optimization problem:\np̃ ∈ argmax p∈C\n∑\nx\npx,1px,0 px,1 + px,0 . (5)\nIt is worth noting that the harmonic mean of the two quantities is intuitively a reasonable surrogate for the original objective function since\npx,1px,0 px,1 + px,0 ≤ min {px,0 , px,1} ≤ 2px,1px,0 px,1 + px,0 . (6)\nAlthough this inequality suggests that the objective functions in (5) and (4) are close to each other, it is not clear whether the distribution p̃ leads to any classification rule having low misclassification rate for all distributions in C. In order to obtain a classification rule from p̃, the first naive approach is to use MAP decision rule based on p̃. However, the following result shows that this decision rule does not achieve the factor two misclassification rate obtained in [2].\nTheorem 1 Let us define δ̃map(x) , argmaxy∈Y p̃x,y with the worst case error probability\nẽmap , maxP∈C P ( δ̃map(X) 6= Y ) . Then, e∗ ≤ ẽmap ≤ 4e∗, where e∗ is the worst case\nmisclassification rate of the optimal decision rule δ∗, that is, e∗ , maxP∈C P (δ∗(X) 6= Y ) .\nProof The proof is similar to the proof of next theorem and hence omitted here.\nNext we show that, surprisingly, one can obtain a randomized decision rule based on the solution of (5) which has a misclassification rate no larger than twice of the optimal decision rule δ∗.\nGiven p̃ as the optimal solution of (5), define the random decision rule δ̃ as\nδ̃(x) =    0 with probability p̃ 2 x,0 p̃ 2 x,0+p̃ 2 x,1\n1 with probability p̃ 2 x,1\np̃ 2 x,0+p̃ 2 x,1\n(7)\nLet ẽ be the worst case classification error of the decision rule δ̃, i.e.,\nẽ , max P∈C P\n( δ̃(X) 6= Y ) .\nClearly, e∗ ≤ ẽ according to the definition of the optimal decision rule e∗. The following theorem shows that ẽ is also upper-bounded by twice of the optimal misclassification rate e∗.\nTheorem 2 Define θ , max\np∈C\n∑\nx\npx,1px,0 px,1 + px,0\n(8)\nThen, θ ≤ ẽ ≤ 2θ ≤ 2e∗. In other words, the worst case misclassification rate of the decision rule δ̃ is at most twice the optimal decision rule δ∗.\nProof The proof is relegated to the supplementary materials.\nSo far, we have resolved the non-smoothness issue in solving (4) by using a surrogate objective function. In the next section, we resolve the second issue by establishing the connection between problem (5) and the minimum HGR correlation principle [1]. Then, we use the existing result in [1] to develop a computationally efficient approach for calculating the decision rule δ̃(·) for Cpairwise."
    }, {
      "heading" : "4 Connection to Hirschfeld-Gebelein-Rényi Correlation",
      "text" : "A commonplace approach to infer models from data is to employ the maximum entropy principle [7]. This principle states that, given a set of constraints on the ground-truth distribution, the distribution with the maximum (Shannon) entropy under those constraints is a proper representer of the class. To extend this rule to the classification problem, the authors in [8] suggest to pick the distribution maximizing the target entropy conditioned to features, or equivalently minimizing mutual information between target and features. Unfortunately, this approach does not lead to a computationally efficient approach for model fitting and there is no guarantee on the misclassification rate of the resulting classifier. Here we study an alternative approach of minimum HGR correlation principle [1]. This principle suggests to pick the distribution in C minimizing HGR correlation between the target variable and features. The HGR correlation coefficient between the two random objects X and Y , which was first introduced by Hirschfeld and Gebelein [12, 13] and then studied by Rényi [14], is defined as ρ(X, Y ) , supf,g E [f(X)g(Y )] , where the maximization is taken over the class of all measurable functions f(·) and g(·) with E[f(X)] = E[g(Y )] = 0 and E[f2(X)] = E[g2(Y )] = 1. The HGR correlation coefficient has many desirable properties. For example, it is normalized to be between 0 and 1. Furthermore, this coefficient is zero if and only if the two random variables are independent; and it is one if there is a strict dependence between X and Y . For other properties of the HGR correlation coefficient see [14, 15] and the references therein.\nLemma 1 Assume the random variable Y is binary and define q , P(Y = 0). Then,\nρ(X, Y ) = √ 1−\n1\nq(1− q)\n∑\nx\n[ PX,Y (x, 0)PX,Y (x, 1)\nPX,Y (x, 0) + PX,Y (x, 1)\n] ,\nProof The proof is relegated to the supplementary material.\nThis lemma leads to the following observation.\nObservation: Assume the marginal distribution P(Y = 0) and P(Y = 1) is fixed for any distribution P ∈ C. Then, the distribution in C with the minimum HGR correlation between X and Y is the distribution P̃ obtained by solving (5). In other words, ρ(X, Y ; P̃) ≤ ρ(X, Y ;P), ∀ P ∈ C, where ρ(X, Y ;P) denotes the HGR correlation coefficient under the probability distribution P.\nBased on the above observation, from now on, we call the classifier δ̃(·) in (7) as the “Rényi classifier”. In the next section, we use the result of the recent work [1] to compute the Rényi classifier δ̃(·) for a special class of marginals C = Cpairwise."
    }, {
      "heading" : "5 Computing Rényi Classifier Based on Pairwise Marginals",
      "text" : "In many practical problems, the number of features d is large and therefore, it is only computationally tractable to estimate marginals of order at most two. Hence, hereafter, we restrict ourselves to the case where only the first and second order marginals of the distribution P̄ is estimated, i.e., C = Cpairwise. In this scenario, in order to predict the output of the Rényi classifier for a given data point x, one needs to find the value of p̃x,0 and p̃x,1. Next, we state a result from [1] which sheds light on the computation of p̃x,0 and p̃x,1. To state the theorem, we need the following definitions:\nLet the matrix Q ∈ Rdm×dm and the vector d ∈ Rdm×1 be defined through their entries as\nQmi+k,mj+ℓ = P̄(Xi+1 = k,Xj+1 = ℓ), dmi+k = P̄(Xi+1 = k, Y = 1)− P̄(Xi+1 = k, Y = 0),\nfor every i, j = 0, . . . , d − 1 and k, ℓ = 1, . . . ,m. Also define the function h(z) : Rmd×1 7→ R as h(z) , ∑d i=1 max{zmi−m+1, zmi−m+2, . . . , zmi}. Then, we have the following theorem.\nTheorem 3 (Rephrased from [1]) Assume Cpairwise 6= ∅. Let\nγ , min z∈Rmd×1\nzTQz− dT z+ 1\n4 . (9)\nThen, √ 1− γ\nq(1−q) ≤ minP∈Cpairwise ρ(X, Y ;P), where the inequality holds with equality if and\nonly if there exists a solution z∗ to (9) such that h(z∗) ≤ 12 and h(−z ∗) ≤ 12 ; or equivalently, if and only if the following separability condition is satisfied for some P ∈ Cpairwise.\nEP[Y |X = x] = d∑\ni=1\nζi(xi), ∀x ∈ X d, (10)\nfor some functions ζ1, . . . , ζd. Moreover, if the separability condition holds with equality, then\nP̃(Y = y ∣∣X = (x1, . . . , xd)) = 1\n2 − (−1)y\nd∑\ni=1\nz∗(i−1)m+xi . (11)\nCombining the above theorem with the equality\nP̃ 2(Y = 0,X = x)\nP̃2(Y = 0,X = x) + P̃2(Y = 1,X = x) =\nP̃ 2(Y = 0 | X = x)\nP̃2(Y = 0 | X = x) + P̃2(Y = 1 | X = x)\nimplies that the decision rule δ̃ and δ̃map can be computed in a computationally efficient manner under the separability condition. Notice that when the separability condition is not satisfied, the approach proposed in this section would provide a classification rule whose error rate is still bounded by 2γ. However, this error rate does no longer provide a 2-factor approximation gap. It is also worth mentioning that the separability condition is a property of the class of distribution Cpairwise and is independent of the classifier at hand. Moreover, this condition is satisfied with a positive measure over the simplex of the all probability distributions, as discussed in [1]. Two remarks are in order:\nInexact knowledge of marginal distribution: The optimization problem (9) is equivalent to solving the stochastic optimization problem\nz∗ = argmin z E\n[( WT z− C )2] ,\nwhere W ∈ {0, 1}md×1 is a random vector with Wm(i−1)+k = 1 if Xi = k in the and Wm(i−1)+k = 0, otherwise. Also define the random variable C ∈ {− 1 2 , 1 2} with C = 1 2 if the random variable Y = 1 and C = − 12 , otherwise. Here the expectation could be calculated with respect to any distribution in C. Hence, in practice, the above optimization problem can be estimated using Sample Average Approximation (SAA) method [16, 17] through the optimization problem\nẑ = argmin z\n1\nn\nn∑\ni=1\n( (wi)T z− ci )2 ,\nwhere (wi, ci) corresponds to the i-th training data point (xi, yi). Clearly, this is a least square problem with a closed form solution. Notice that in order to bound the SAA error and avoid overfitting, one could restrict the search space for ẑ [18]. This could also be done using regularizers such as ridge regression by solving\nẑ ridge = argmin z\n1\nn\nn∑\ni=1\n( (wi)T z− ci )2 + λridge‖z‖22.\nBeyond pairwise marginals: When d is small, one might be interested in estimating higher order marginals for predicting Y . In this scenario, a simple modification for the algorithm is to define the new set of feature random variables { X̃ij = (Xi, Xj) | i 6= j } ; and apply the algorithm to the\nnew set of feature variables. It is not hard to see that this approach utilizes the marginal information P(Xi, Xj , Xk, Xℓ) and P(Xi, Xj , Y )."
    }, {
      "heading" : "6 Robust Rényi Feature Selection",
      "text" : "The task of feature selection for classification purposes is to preselect a subset of features for use in model fitting in prediction. Shannon mutual information, which is a measure of dependence between two random variables, is used in many recent works as an objective for feature selection [19, 20]. In these works, the idea is to select a small subset of features with maximum dependence with the target variable Y . In other words, the task is to find a subset of variables S ⊆ {1, . . . , d} with |S| ≤ k based on the following optimization problem\nSMI , argmax S⊆{1,...,d} I(XS ;Y ), (12)\nwhere XS , (Xi)i∈S and I (XS ;Y ) denotes the mutual information between the random variable XS and Y . Almost all of the existing approaches for solving (12) are based on heuristic approaches and of greedy nature which aim to find a sub-optimal solution of (12). Here, we suggest to replace mutual information with the maximal correlation. Furthermore, since estimating the joint distribution of X and Y is computationally and statistically impossible for large number of features d, we suggest to estimate some low order marginals of the groundtruth distribution P̄(X, Y ) and then solve the following robust Rényi feature selection problem:\nSRFS , argmax S⊆{1,...,d} min P∈C ρ(XS , Y ;P). (13)\nWhen only pairwise marginals are estimated from the training data, i.e., C = Cpairwise, maximizing\nthe lower-bound √ 1− γ\nq(1−q) instead of (13) leads to the following optimization problem\nŜ RFS , argmax |S|≤k\n√ 1−\n1\nq(1− q) min z∈ZS\nzTQz− dT z+ 1\n4 ,\nor equivalently, Ŝ RFS , argmin\n|S|≤k\nmin z∈ZS\nzTQz− dT z,\nwhere ZS , { z ∈ Rmd ∣∣ ∑m k=1 |zmi−m+k| = 0, ∀i /∈ S } . This problem is of combinatorial nature. Howevre, using the standard group Lasso regularizer leads to the feature selection procedure in Algorithm 1.\nAlgorithm 1 Robust Rényi Feature Selection\nChoose a regularization parameter λ > 0 and define h(z) , ∑d\ni=1 max{zmi−m+1, . . . , zmi}. Let ẑRFS ∈ argminz z TQz− dT z+ λh(|z|). Set S = {i | ∑m\nk=1 |z RFS mi−m+k| > 0}.\nNotice that, when the pairwise marginals are estimated from a set of training data points, the above feature selection procedure is equivalent to applying the group Lasso regularizer to the standard linear regression problem over the domain of indicator variables. Our framework provides a justification for this approach based on the robust maximal correlation feature selection problem (13).\nRemark 1 Another natural approach to define the feature selection procedure is to select a subset of features S by minimizing the worst case classification error, i.e., solving the following optimization problem\nmin |S|≤k min δ∈DS max P∈C P(δ(X) 6= Y ), (14)\nwhere DS is the set of randomized decision rules which only uses the feature variables in S. Define F(S) , minδ∈DS maxP∈C P(δ(X) 6= Y ). It can be shown that F(S) ≤ min|S|≤k minz∈ZS z\nTQz − dT z + 14 . Therefore, another justification for Algorithm 1 is to minimize an upper-bound of F(S) instead of itself.\nRemark 2 Alternating Direction Method of Multipliers (ADMM) algorithm [21] can be used for solving the optimization problem in Algorithm 1; see the supplementary material for more details."
    }, {
      "heading" : "7 Numerical Results",
      "text" : "We evaluated the performance of the Rényi classifiers δ̃ and δ̃map on five different binary classification datasets from the UCI machine learning data repository. The results are compared with five different benchmarks used in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM). In addition to the classifiers δ̃ and δ̃map which only use pairwise marginals, we also use higher order marginals in δ̃2 and δ̃ map 2 . These classifiers are obtained by defining the new feature variables {X̃ij = (Xi, Xj)} as discussed in section 5. Since in this scenario, the number of features is large, we combine our Rényi classifier with the proposed group lasso feature selection. In other words, we first select a subset of {X̃ij} and then find the maximum correlation classifier for the selected features. The value of λridge and λ is determined through cross validation. The results are averaged over 100 Monte Carlo runs each using 70% of the data for training and the rest for testing. The results are summarized in the table below where each number shows the percentage of the error of each method. The boldface numbers denote the best performance on each dataset.\nAs can be seen in this table, in four of the tested datasets, at least one of the proposed methods outperforms the other benchmarks. Furthermore, it can be seen that the classifier δ̃map on average performs better than δ̃. This fact could be due to the specific properties of the underlying probability distribution in each dataset.\nDatasets δ̃map δ̃ δ̃map2 δ̃2 δ̃ map FS,2 δ̃FS,2 DCC gDCC MPM TAN SVM\nadult 17 21 16 20 16 20 18 18 22 18 22 credit 13 16 16 17 16 17 14 13 13 17 16\nkr-vs-kp 5 10 5 14 5 14 10 10 5 7 3 promoters 6 16 3 4 3 4 5 3 6 44 9\nvotes 3 4 3 4 2 4 3 3 4 8 5\nIn order to evaluate the computational efficiency of the Rényi classifier, we compare its running time with SVM over the synthetic data set with d = 10, 000 features and n = 200 data points. Each feature Xi is generated by i.i.d. Bernoulli distribution with P(Xi = 1) = 0.7. The target variable y is generated by y = sign(αTX + n) with n ∼ N (0, 1); and α ∈ Rd is generated with 30% nonzero elements each drawn from standard Gaussian distribution N (0, 1). The results are averaged over 1000 Monte-Carlo runs of generating the data set and use 85% of the data points for training and 15% for test. The Rényi classifier is obtained by gradient descent method with regularizer λridge = 104. The numerical experiment shows 19.7% average misclassification rate for SVM and 19.9% for Rényi classifier. However, the average training time of the Rényi classifier is 0.2 seconds while the training time of SVM (with Matlab SVM command) is 1.25 seconds.\nAcknowledgments: The authors are grateful to Stanford University supporting a Stanford Graduate Fellowship, and the Center for Science of Information (CSoI), an NSF Science and Technology Center under grant agreement CCF-0939370 , for the support during this research."
    } ],
    "references" : [ {
      "title" : "Minimum HGR correlation principle: From marginals to joint distribution",
      "author" : [ "F. Farnia", "M. Razaviyayn", "S. Kannan", "D. Tse" ],
      "venue" : "arXiv preprint arXiv:1504.06010,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Discrete chebyshev classifiers",
      "author" : [ "E. Eban", "E. Mezuman", "A. Globerson" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Bayesian network classifiers",
      "author" : [ "N. Friedman", "D. Geiger", "M. Goldszmidt" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1997
    }, {
      "title" : "A robust minimax approach to classification",
      "author" : [ "C. Bhattacharyya", "M.I. Jordan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : "Marginals-to-models reducibility",
      "author" : [ "T. Roughgarden", "M. Kearns" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Information theory and statistical mechanics",
      "author" : [ "E.T. Jaynes" ],
      "venue" : "Physical review,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1957
    }, {
      "title" : "The minimum information principle for discriminative learning",
      "author" : [ "A. Globerson", "N. Tishby" ],
      "venue" : "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "On general minimax theorems",
      "author" : [ "M. Sion" ],
      "venue" : "Pacific J. Math,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1958
    }, {
      "title" : "The complexity of three-way statistical tables",
      "author" : [ "J. De Loera", "S. Onn" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Moment problems and semidefinite optimization",
      "author" : [ "D. Bertsimas", "J. Sethuraman" ],
      "venue" : "In Handbook of semidefinite programming,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2000
    }, {
      "title" : "A connection between correlation and contingency",
      "author" : [ "H.O. Hirschfeld" ],
      "venue" : "In Mathematical Proceedings of the Cambridge Philosophical Society,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1935
    }, {
      "title" : "Das statistische problem der korrelation als variations-und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung",
      "author" : [ "H. Gebelein" ],
      "venue" : "ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1941
    }, {
      "title" : "On measures of dependence",
      "author" : [ "A. Rényi" ],
      "venue" : "Acta mathematica hungarica,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1959
    }, {
      "title" : "On maximal correlation, hypercontractivity, and the data processing inequality studied by Erkip and Cover",
      "author" : [ "V. Anantharam", "A. Gohari", "S. Kamath", "C. Nair" ],
      "venue" : "arXiv preprint arXiv:1304.6133,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Lectures on stochastic programming: modeling and theory, volume",
      "author" : [ "A. Shapiro", "D. Dentcheva", "A. Ruszczyński" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Monte carlo sampling methods",
      "author" : [ "A. Shapiro" ],
      "venue" : "Handbooks in operations research and management science,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2003
    }, {
      "title" : "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization",
      "author" : [ "S.M. Kakade", "K. Sridharan", "A. Tewari" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Feature selection based on mutual information criteria of maxdependency, max-relevance, and min-redundancy",
      "author" : [ "H. Peng", "F. Long", "C. Ding" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Using mutual information for selecting features in supervised neural net learning",
      "author" : [ "R. Battiti" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1994
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends R  © in Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We prove that for a given set of marginals, the minimum Hirschfeld-Gebelein-Rényi (HGR) correlation principle introduced in [1] leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "Our theoretical upper-bound is similar to the recent Discrete Chebyshev Classifier (DCC) approach [2], while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem.",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "This approach, which is the sprit of various machine learning and statistical methods [2–6], is also useful when the complete data instances are not available due to privacy concerns in applications such as medical informatics.",
      "startOffset" : 86,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "This approach, which is the sprit of various machine learning and statistical methods [2–6], is also useful when the complete data instances are not available due to privacy concerns in applications such as medical informatics.",
      "startOffset" : 86,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "This approach, which is the sprit of various machine learning and statistical methods [2–6], is also useful when the complete data instances are not available due to privacy concerns in applications such as medical informatics.",
      "startOffset" : 86,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "This approach, which is the sprit of various machine learning and statistical methods [2–6], is also useful when the complete data instances are not available due to privacy concerns in applications such as medical informatics.",
      "startOffset" : 86,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "This approach, which is the sprit of various machine learning and statistical methods [2–6], is also useful when the complete data instances are not available due to privacy concerns in applications such as medical informatics.",
      "startOffset" : 86,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "Then a surrogate objective function, which is obtained by the minimum HGR correlation principle [1], is used to propose a randomized classification rule.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 1,
      "context" : "Finally, we conclude by presenting numerical experiments comparing the proposed classifier with discrete Chebyshev classifier [2], Tree Augmented Naive Bayes [3], and Minimax Probabilistic Machine [4].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "Finally, we conclude by presenting numerical experiments comparing the proposed classifier with discrete Chebyshev classifier [2], Tree Augmented Naive Bayes [3], and Minimax Probabilistic Machine [4].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "Finally, we conclude by presenting numerical experiments comparing the proposed classifier with discrete Chebyshev classifier [2], Tree Augmented Naive Bayes [3], and Minimax Probabilistic Machine [4].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 6,
      "context" : "For example, the maximum entropy principle [7], which is the spirit of the variational method in graphical models [5] and tree augmented naive Bayes [3], is based on the idea of fixing the marginal distributions and fitting a probabilistic model which maximizes the Shannon entropy.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "For example, the maximum entropy principle [7], which is the spirit of the variational method in graphical models [5] and tree augmented naive Bayes [3], is based on the idea of fixing the marginal distributions and fitting a probabilistic model which maximizes the Shannon entropy.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "For example, the maximum entropy principle [7], which is the spirit of the variational method in graphical models [5] and tree augmented naive Bayes [3], is based on the idea of fixing the marginal distributions and fitting a probabilistic model which maximizes the Shannon entropy.",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : "Another related information theoretic approach is the minimum mutual information principle [8] which finds the probability distribution with the minimum mutual information between the feature vector and the target variable.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "In the continuous setting, the idea of minimizing the worst case misclassification rate leads to the minimax probability machine [4].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "The most related algorithm to this work is the recent Discrete Chebyshev Classifier (DCC) algorithm [2].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, in contrast to [2] which only considers deterministic decision rules, in this work we",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "Finally, it is worth noting that the algorithm in [2] requires tree structure to be tight, while our proposed algorithm works on non-tree structures as long as the separability condition is satisfied.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "1 with probability 1− q δ , for some q δ ∈ [0, 1], ∀x ∈ X .",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "Later we show that the proposed surrogate objective is connected to the minimum HGR principle [1].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "The simple application of the minimax theorem [9] implies that the saddle point of the above optimization problem exists and moreover, the optimal decision rule is a MAP rule for a certain probability distribution P ∈ C.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "Notice that it is NP-hard to verify the existence of a probability distribution satisfying a given set of low order marginals [10].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "Based on this observation and the result in [11], we conjecture that in general, solving (2) is NP-hard in the number variables and the alphabet size even when the set C is nonempty.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "Let us continue by utilizing the minimax theorem [9] and obtain the worst case probability distribution in (3) by p ∈ argmaxp∈C min0≤qδ≤1 ∑ x (q x δpx,1 + (1 − q x δ )px,0) , or equivalently, p ∈ argmax p∈C ∑",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "However, the following result shows that this decision rule does not achieve the factor two misclassification rate obtained in [2].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 0,
      "context" : "In the next section, we resolve the second issue by establishing the connection between problem (5) and the minimum HGR correlation principle [1].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : "Then, we use the existing result in [1] to develop a computationally efficient approach for calculating the decision rule δ̃(·) for Cpairwise.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "A commonplace approach to infer models from data is to employ the maximum entropy principle [7].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "To extend this rule to the classification problem, the authors in [8] suggest to pick the distribution maximizing the target entropy conditioned to features, or equivalently minimizing mutual information between target and features.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "Here we study an alternative approach of minimum HGR correlation principle [1].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "The HGR correlation coefficient between the two random objects X and Y , which was first introduced by Hirschfeld and Gebelein [12, 13] and then studied by Rényi [14], is defined as ρ(X, Y ) , supf,g E [f(X)g(Y )] , where the maximization is taken over the class of all measurable functions f(·) and g(·) with E[f(X)] = E[g(Y )] = 0 and E[f(X)] = E[g(Y )] = 1.",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 12,
      "context" : "The HGR correlation coefficient between the two random objects X and Y , which was first introduced by Hirschfeld and Gebelein [12, 13] and then studied by Rényi [14], is defined as ρ(X, Y ) , supf,g E [f(X)g(Y )] , where the maximization is taken over the class of all measurable functions f(·) and g(·) with E[f(X)] = E[g(Y )] = 0 and E[f(X)] = E[g(Y )] = 1.",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 13,
      "context" : "The HGR correlation coefficient between the two random objects X and Y , which was first introduced by Hirschfeld and Gebelein [12, 13] and then studied by Rényi [14], is defined as ρ(X, Y ) , supf,g E [f(X)g(Y )] , where the maximization is taken over the class of all measurable functions f(·) and g(·) with E[f(X)] = E[g(Y )] = 0 and E[f(X)] = E[g(Y )] = 1.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 13,
      "context" : "For other properties of the HGR correlation coefficient see [14, 15] and the references therein.",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "For other properties of the HGR correlation coefficient see [14, 15] and the references therein.",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "In the next section, we use the result of the recent work [1] to compute the Rényi classifier δ̃(·) for a special class of marginals C = Cpairwise.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "Next, we state a result from [1] which sheds light on the computation of p̃x,0 and p̃x,1.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Theorem 3 (Rephrased from [1]) Assume Cpairwise 6= ∅.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "Moreover, this condition is satisfied with a positive measure over the simplex of the all probability distributions, as discussed in [1].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : "Hence, in practice, the above optimization problem can be estimated using Sample Average Approximation (SAA) method [16, 17] through the optimization problem",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "Hence, in practice, the above optimization problem can be estimated using Sample Average Approximation (SAA) method [16, 17] through the optimization problem",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : "Notice that in order to bound the SAA error and avoid overfitting, one could restrict the search space for ẑ [18].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 18,
      "context" : "Shannon mutual information, which is a measure of dependence between two random variables, is used in many recent works as an objective for feature selection [19, 20].",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 19,
      "context" : "Shannon mutual information, which is a measure of dependence between two random variables, is used in many recent works as an objective for feature selection [19, 20].",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 20,
      "context" : "Remark 2 Alternating Direction Method of Multipliers (ADMM) algorithm [21] can be used for solving the optimization problem in Algorithm 1; see the supplementary material for more details.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "The results are compared with five different benchmarks used in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM).",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "The results are compared with five different benchmarks used in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM).",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "The results are compared with five different benchmarks used in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM).",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "The results are compared with five different benchmarks used in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM).",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "The results are compared with five different benchmarks used in [2]: Discrete Chebyshev Classifier [2], greedy DCC [2], Tree Augmented Naive Bayes [3], Minimax Probabilistic Machine [4], and support vector machines (SVM).",
      "startOffset" : 182,
      "endOffset" : 185
    } ],
    "year" : 2015,
    "abstractText" : "Consider the binary classification problem of predicting a target variable Y from a discrete feature vector X = (X1, . . . , Xd). When the probability distribution P(X, Y ) is known, the optimal classifier, leading to the minimum misclassification rate, is given by the Maximum A-posteriori Probability (MAP) decision rule. However, in practice, estimating the complete joint distribution P(X, Y ) is computationally and statistically impossible for large values of d. Therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution P(X, Y ) and then design the classifier based on the estimated low order marginals. This approach is also helpful when the complete training data instances are not available due to privacy concerns. In this work, we consider the problem of finding the optimum classifier based on some estimated low order marginals of (X, Y ). We prove that for a given set of marginals, the minimum Hirschfeld-Gebelein-Rényi (HGR) correlation principle introduced in [1] leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier. Then, under a separability condition, it is shown that the proposed algorithm is equivalent to a randomized linear regression approach. In addition, this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case HGR correlation with the target variable. Our theoretical upper-bound is similar to the recent Discrete Chebyshev Classifier (DCC) approach [2], while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem. Finally, we numerically compare our proposed algorithm with the DCC classifier and show that the proposed algorithm results in better misclassification rate over various UCI data repository datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}