{
  "name" : "1512.00442.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing",
    "authors" : [ "Ke Li", "Jitendra Malik" ],
    "emails" : [ "KE.LI@EECS.BERKELEY.EDU", "MALIK@EECS.BERKELEY.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The k-nearest neighbour method is commonly used both as a classifier and as subroutines in more complex algorithms in a wide range domains, including machine learning, computer vision, graphics and robotics. Consequently, finding a fast algorithm for retrieving nearest neighbours has been a subject of sustained interest among the artificial intelligence and the theoretical computer science communities alike. Work over the past several decades has produced a rich collection of algorithms; however, they suffer from one key shortcoming: as the dimensionality increases, the running time and/or space complexity grows rapidly; this phenomenon is often known as the curse of dimensionality. Finding a solution to this problem has proven to be elusive and has been conjectured to be fundamentally impossible (Minsky & Seymour, 1969). In this era of rapid growth in both the volume and dimensionality of data, it has become increasingly important to devise a fast algorithm that scales to high dimensional space.\nWe argue that the difficulty in overcoming the curse of dimensionality stems in part from inherent deficiencies in space partitioning, the strategy that underlies almost all existing algorithms. Space partitioning is a divide-andconquer strategy that partitions the vector space into a finite number of cells and keeps track of the data points that each cell contains. At query time, exhaustive search is performed over all data points that lie in cells containing the query point. This strategy forms the basis of most existing algorithms, including k-d trees (Bentley, 1975) and locality-sensitive hashing (LSH) (Indyk & Motwani, 1998).\nWhile this strategy seems natural and sensible, it suffers from critical deficiencies as the dimensionality increases. Because the volume of any region in the vector space grows exponentially in the dimensionality, either the number or the size of cells must increase exponentially in the number of dimensions, which tends to lead to exponential time or space complexity in the dimensionality. In addition, space partitioning limits the algorithm’s “field of view” to the cell containing the query point; points that lie in adjacent cells have no hope of being retrieved. Consequently, if a query point falls near cell boundaries, the algorithm will fail to retrieve nearby points that lie in adjacent cells. Since the number of such cells is exponential in the dimensionality, it is intractable to search these cells when dimensionality is high. One popular approach used by LSH and spill trees (Liu et al., 2004) to mitigate this effect is to partition the space using overlapping cells and search over all points that lie in any of the cells that contain the query point. Because the ratio of surface area to volume grows in dimensionality, the number of overlapping cells that must be used increases in dimensionality; as a result, the running time or space usage becomes prohibitively expensive as dimensionality increases. Further complications arise from variations in dataset density across different regions of the space. If the partitioning is too fine, most cells in sparse regions of the space will be empty and so for a query point that lies in a sparse region, no points will be retrieved. If the partitioning is too coarse, each cell in dense regions of the space will contain many points and so for a query point that lies in a dense region, many points that are not the true nearest neighbours must be searched. This phenomenon is notably ar X iv :1 51 2. 00 44 2v 1 [ cs\n.D S]\n1 D\nec 2\n01 5\nexhibited by LSH, whose performance is highly sensitive to the choice of the hash function, which defines an implicit partitioning. A good partitioning scheme must therefore depend on the data; however, such data-dependent partitioning schemes would require possibly expensive preprocessing and prohibit online updates to the dataset. These fundamental limitations of space partitioning beg an interesting question: is it possible to devise a strategy that does not partition the space, yet still enables fast retrieval of nearest neighbours?\nIn this paper, we present a new strategy for retrieving k-nearest neighbours that avoids discretizing the vector space, which we call dynamic continuous indexing (DCI). Instead of partitioning the space into discrete cells, we construct continuous indices, each of which imposes an ordering on data points such that closeness in position serves as an approximate indicator of proximity in the vector space. The resulting algorithm runs in time linear in dimensionality and sub-linear in the size of the dataset, while only requiring space constant in dimensionality and linear in the size of the dataset. Unlike existing methods, the algorithm allows fine-grained control over accuracy and speed at query time and adapts to varying dataset density on-thefly while permitting dynamic updates to the dataset. Furthermore, the algorithm is easy-to-implement and does not rely on any complex or specialized data structure."
    }, {
      "heading" : "2. Related Work",
      "text" : "Extensive work over the past several decades has produced a rich collection of algorithms for fast retrieval of k-nearest neighbours. Space partitioning forms the basis of almost all these algorithms. Early approaches store points in deterministic tree-based data structures, such as k-d trees (Bentley, 1975), R-trees (Guttman, 1984) and X-trees (Berchtold et al., 1996; 1998), which effectively partition the vector space into a hierarchy of half-spaces, hyper-rectangles or Voronoi polygons. These methods achieve query times that are logarithmic in the number of data points and work very well for low-dimensional data. Unfortunately, their query times grow exponentially in the dimensionality of data because the number of leaves in the tree that need to be searched increases exponentially in dimensionality; as a result, for high-dimensional data, these algorithms become slower than exhaustive search. More recent methods like spill trees (Liu et al., 2004) and RP trees (Dasgupta & Freund, 2008) extend these approaches by randomizing the dividing hyperplane at each node. Multiple leaves need to be searched to ensure low failure probability; unfortunately, the number of such leaves increases exponentially in dimensionality.\nIn an effort at taming the curse of dimensionality, researchers have considered relaxing the problem to allow -\napproximate solutions, which can contain any point whose distance to the query point differs from that of the true nearest neighbours by at most a factor of (1 + ). Treebased methods (Arya et al., 1998) have been proposed for this setting; unfortunately, the running time still exhibits exponential dependence on dimensionality. Another popular method is locality-sensitive hashing (LSH) (Indyk & Motwani, 1998), which relies on a hash function that implicitly defines a partitioning of the space. Unfortunately, LSH struggles on datasets with varying density, as cells in sparse regions of the space tend to be empty and cells in dense regions tend to contain a large number of points. As a result, it fails to return any point on some queries and requires a long time on some other queries. This motivated the development of data-dependent hashing schemes based on k-means (Paulevé et al., 2010) and spectral partitioning (Weiss et al., 2009). Unfortunately, these methods do not support dynamic updates to the dataset or provide correctness guarantees. Furthermore, they incur a significant pre-processing cost, which can be expensive on large datasets. Other data-dependent algorithms outside the LSH framework have also been proposed, such as (Fukunaga & Narendra, 1975; Brin, 1995; Nister & Stewenius, 2006; Wang, 2011), which work by constructing a hierarchy of clusters using k-means and can be viewed as performing a highly data-dependent form of space partitioning. For these algorithms, no guarantees on approximation quality or running time are known.\nOne algorithm that does not rely on space partitioning is proposed in (Karger & Ruhl, 2002). Instead, the algorithm uses a local search strategy and iteratively finds a new point by sampling that is closer to the query point than the previous point. While this works well for datasets whose density is near-uniform everywhere, it is difficult to extend this approach to datasets with arbitrary density variations since the algorithm may get stuck in a dense region of the space, even if it is very far from the query point."
    }, {
      "heading" : "3. Algorithm",
      "text" : "The proposed algorithm relies on the construction of continuous indices of data points that support both fast searching and online updates. To this end, we use one-dimensional random projections as the basic building blocks and initially construct mL simple indices, each of which orders data points by their projections along a random direction. Such an index has both desired properties: data points can be efficiently retrieved using binary search and the index can be maintained as a self-balancing binary search tree. This ordered arrangement of data points exploits an important property of the k-nearest neighbour search problem that has often been overlooked: it suffices to construct an index that approximately preserves the rel-\native order between the true k-nearest neighbours and the other data points in terms of their distances to the query point without necessarily preserving all pairwise distances. This observation enables projection to a much lower dimensional space than the Johnson-Lindenstrauss transform (Johnson & Lindenstrauss, 1984). We show in the following section that with high probability, one-dimensional random projection preserves the relative order of two points whose distances to the query point differ significantly regardless of the dimensionality of the points.\nAlgorithm 1 Proposed algorithm\nRequire: A dataset D of n points, a query point q in Rd, the number of simple indices m that constitute a composite index and the number of composite indices L function CONSTRUCT(D) {vjl}j∈[m],l∈[L] ← mL random unit vectors in Rd {Tjl}j∈[m],l∈[L] ← mL empty binary search trees for j = 1 to m do\nfor l = 1 to L do for pi ∈ D do\nrijl ← 〈pi, vjl〉 Insert (rijl, i) into Tjl with r i jl being the key and\ni being the value end for\nend for end for\nend function function QUERY(q)\nCl ← array of size n with entries initialized to 0 ∀l ∈ [L] wjl ← 〈q, vjl〉 ∀j ∈ [m], l ∈ [L] for i = 1 to n do\nfor l = 1 to L do S̃l ← ∅ for j = 1 to m do\n(r (i) jl , h (i) jl )← the node in Tj whose key is the ith closest to wj Cl[h (i) jl ]← Cl[h (i) jl ] + 1\nend for for j = 1 to m do\nif Cl[h(i)jl ] = m then S̃l ← S̃l ∪ {h(i)jl }\nend if end for\nend for // Stopping condition depends on version of the algo-\nrithm used if stopping condition is met then\nbreak end if\nend for return k instances in ⋃ l∈[L] S̃l that are the closest in\nEuclidean distance in Rd to q end function\nWe combinem simple indices to form a composite index in which points are ordered by the maximum difference over all simple indices between the positions of the point and the query in the simple index. The composite index en-\nables fast retrieval of a small number of data points, which will be referred to as candidate points, that are close to the query point along several random directions and therefore are likely to be truly close to the query. Exhaustive search is then performed over candidate points retrieved from all L composite indices to identify the subset of k points closest to the query. Please refer to Algorithm 1 for a precise statement of the proposed algorithm.\nBecause data points are retrieved according to their positions in the composite index rather than the regions of space they lie in, the algorithm is able to automatically adapt to changes in dataset density as dynamic updates are made to the dataset without requiring any pre-processing to estimate dataset density at construction time. Also, unlike existing methods, the number of retrieved candidate points can be controlled on a per-query basis, enabling the user to easily trade off accuracy against speed. We develop two versions of the algorithm, a data-independent and a datadependent version. In the former, the number of candidate points is preset indirectly according to the global dataset density and the maximum tolerable failure probability; in the latter, the number of candidate points is chosen adaptively at query time based on the local dataset density in the neighbourhood of the query. We analyze the algorithm below and show that its failure probability is independent of dimensionality, its running time is linear in dimensionality and sub-linear in the size of the dataset and its space complexity is constant in dimensionality and linear in the size of the dataset."
    }, {
      "heading" : "3.1. Properties of Random 1D Projection",
      "text" : "First, we examine the effect of projecting d-dimensional vectors to one dimension, which motivates its use in the proposed algorithm. We are interested in the probability that a distant point appears closer than a nearby point under projection; if this probability is low, then each simple index approximately preserves the order of points by distance to the query point. We consider displacement vectors between the query point and data points and this probability is then is equivalent to the probability of the lengths of these vectors inverting under projection.\nLemma 1. Let l, s ∈ Rd such that ‖l‖2 > ‖s‖2, and p ∈ Rd be a unit vector drawn uniformly at random. Then the probability of s being at least as long as l under projection p is at most 1− 2π cos −1 ( ‖s‖2 ‖l‖2 ) .\nProof. Assuming that l and s are not collinear, consider the two-dimensional subspace spanned by l and s, which we will denote as P . (If l and s are collinear, we define P to be the subspace spanned by l and an arbitrary vector that’s linearly independent of l.) For any vector v, we denote v‖ and v⊥ as the components of v in P and P⊥ such that v =\nv‖ + v⊥. For v ∈ {s, l}, because v⊥ = 0, 〈v, p〉 = 〈v, p‖〉. So, we can limit our attention to P for this analysis. We parameterize p‖ in terms of its angle relative to l, which we denote as θ. Also, we denote the angle of p‖ relative to s as ψ. Then,\nPr (|〈l, p〉| ≤ |〈s, p〉|) = Pr (∣∣∣〈l, p‖〉∣∣∣ ≤ ∣∣∣〈s, p‖〉∣∣∣)\n= Pr ( ‖l‖2 ∥∥∥p‖∥∥∥ 2 |cos θ| ≤ ‖s‖2 ∥∥∥p‖∥∥∥ 2 |cosψ| ) ≤ Pr ( |cos θ| ≤\n‖s‖2 ‖l‖2 ) = 2Pr ( θ ∈ [ cos−1\n( ‖s‖2 ‖l‖2 ) , π − cos−1 ( ‖s‖2 ‖l‖2 )]) = 1− 2\nπ cos−1 ( ‖s‖2 ‖l‖2 )\nObserve that if |〈l, p〉| ≤ |〈s, p〉|, the relative order of l and s by length would be inverted when projected along p. This occurs when p‖ is close to orthogonal to l, which is illustrated in Figure 1a. Also note that the probability of inverting the relative order of l and s is small when l is much longer than s. On the other hand, this probability is high when l and s are similar in length, which corresponds to the case when two data points are almost equidistant to the query point. So, if we consider a sequence of vectors ordered by length, applying random one-dimensional projection will likely perturb the ordering locally, but will preserve the ordering globally.\nNext, we build on this result to analyze the order-inversion probability when there are more than two vectors. Con-\nsider the sample space B = { p ∈ Rd ∣∣ ‖p‖2 = 1} and the set V sl = { p ∈ B\n∣∣∣|cos θ| ≤ ‖s‖2‖l‖2 }, which is illustrated in Figure 1b, where θ is the angle between p‖ and l. If we let area(S) to denote the area of the region formed by the endpoints of all vectors in the set S, then we can write the above bound on the order-inversion probability as:\nPr (|〈l, p〉| ≤ |〈s, p〉|) ≤ Pr (p ∈ V sl )\n= area(V sl ) area(B)\n= 1− 2 π cos−1 ( ‖s‖2 ‖l‖2 )\nLemma 2. Let {li}Ni=1 be a set of vectors such that ‖li‖2 > ‖s‖2 ∀i ∈ [N ]. Then the probability that there is a subset of k′ vectors from {li}Ni=1 that are all not longer than s under projection p is at most 1k′ ∑N i=1 ( 1− 2π cos −1 ( ‖s‖2 ‖li‖2 )) . Furthermore, if k′ = N , this probability is at most mini∈[N ] { 1− 2π cos −1 ( ‖s‖2 ‖li‖2 )} .\nProof. For a given subset S ⊆ [N ] of size k′, the probability that all vectors indexed by elements in S are not longer than s under projection p is at most Pr ( p ∈ ⋂ i∈S V s li ) = area (⋂ i∈S V s li ) /area (B). So, the probability that this occurs on some subset S is at most Pr ( p ∈ ⋃ S⊆[N ]:|S|=k′ ⋂ i∈S V s li ) =\narea (⋃ S⊆[N ]:|S|=k′ ⋂ i∈S V s li ) /area (B).\nObserve that each point in ⋃ S⊆[N ]:|S|=k′ ⋂ i∈S V s li must be\ncovered by at least k′ V sli ’s, so:\nk′ · area  ⋃ S⊆[N ]:|S|=k′ ⋂ i∈S V sli  ≤ N∑ i=1 area ( V sli )\nIt follows that the probability this event occurs on some subset S is bounded above by 1k′ ∑N i=1 area(V sli) area(B) =\n1 k′ ∑N i=1 ( 1− 2π cos −1 ( ‖s‖2 ‖li‖2 )) .\nIf k′ = N , we use the fact that area (⋂\ni∈[N ] V s li\n) ≤\nmini∈[N ] { area ( V sli )} to obtain the desired result.\nIntuitively, if this event occurs, then there are at least k′ vectors that rank above s when sorted in nondecreasing order by their lengths under projection. This can only occur when the endpoint of p falls in a region on the unit sphere corresponding to ⋃ S⊆[N ]:|S|=k′ ⋂ i∈S V s li\n. We illustrate this region in Figure 1c for the case of d = 3.\nTheorem 3. Let {li}Ni=1 and {sj} M j=1 be sets of vectors such that ‖li‖2 > ‖sj‖2 ∀i ∈ [N ], j ∈ [M ]. Then the probability that there is a subset of k′ vectors from {li}Ni=1 that are all not longer than some sj under projection p is at most 1k′ ∑N i=1 ( 1− 2π cos −1 ( ‖smax‖2 ‖li‖2 )) , where ‖smax‖2 ≥ ‖sj‖2 ∀j ∈ [M ].\nProof. The probability that this event occurs is at most Pr ( p ∈ ⋃ j∈[M ] ⋃ S⊆[N ]:|S|=k′ ⋂ i∈S V sj li ) . We\nobserve that for all i, j, { θ ∣∣∣|cos θ| ≤ ‖sj‖2‖li‖2 } ⊆{\nθ ∣∣∣|cos θ| ≤ ‖smax‖2‖li‖2 }, which implies that V sjli ⊆ V smaxli .\nIf we take intersection followed by union on both sides, we obtain ⋃ S⊆[N ]:|S|=k′ ⋂ i∈S V sj li\n⊆⋃ S⊆[N ]:|S|=k′ ⋂ i∈S V smax li\n. Because this is true for all j, ⋃ j∈[M ] ⋃ S⊆[N ]:|S|=k′ ⋂ i∈S V sj li\n⊆⋃ S⊆[N ]:|S|=k′ ⋂ i∈S V smax li . Therefore, this probability is bounded above by Pr ( p ∈ ⋃ S⊆[N ]:|S|=k′ ⋂ i∈S V smax li ) . By Lemma 2,\nthis is at most 1k′ ∑N i=1 ( 1− 2π cos −1 ( ‖smax‖2 ‖li‖2 )) ."
    }, {
      "heading" : "3.2. Dataset Density",
      "text" : "We now formally characterize dataset density by defining the following notion of local relative sparsity:\nDefinition 4. Given a dataset D ⊆ Rd, let Bp(r) be the set of points in D that are within a ball of radius r around a point p. We say D has local relative sparsity of (τ, γ)\nat a point p ∈ Rd if there exists r such that |Bp(r)| ≥ τ , |Bp(γr)| ≤ 2 |Bp(r)|, where γ ≥ 1.\nIntuitively, γ represents a lower bound on the increase in radius when the number of points within the ball is doubled. When γ is close to 1, the dataset is dense in the neighbourhood of p, since there could be many points inD that are almost equidistant from p. Retrieving the nearest neighbours of such a p is considered “hard”, since it would be difficult to tell which of these points are the true nearest neighbours without computing the distances to all these points exactly. If we consider a dataset consisting of points on a manifold, the local relative sparsity at points close to the manifold is high and so the region of the space near these points is considered “easy”, while the opposite is true for points far from the manifold.\nWe also define a related notion of global relative sparsity, which we will use to derive a value of k̃ and a bound on the running time that’s independent of the query:\nDefinition 5. A dataset D has global relative sparsity of (τ, γ) if for all r and p ∈ Rd such that |Bp(r)| ≥ τ , |Bp(γr)| ≤ 2 |Bp(r)|, where γ ≥ 1.\nNote that a dataset with global relative sparsity of (τ, γ) has local relative sparsity of (τ, γ) at every point."
    }, {
      "heading" : "3.3. Data-Independent Version",
      "text" : "In the data-independent version of the algorithm, the outer loop in the querying function executes for a preset number of iterations k̃. The values of L, m and k̃ are fixed for all queries and will be chosen later.\nWe apply the results obtained above to analyze the algorithm. Consider the event that the algorithm fails to return the correct set of k-nearest neighbours – this can only occur if a true k-nearest neighbour is not contained in any of the S̃l’s, which entails that for each l ∈ [L], there is a set of k̃ − k + 1 points that are not the true k-nearest neighbours but are closer to the query than the true k-nearest neighbours under all projections v1l, . . . , vml. We analyze the probability that this occurs below and derive the parameter settings that ensure the algorithm succeeds with high probability. Please refer to the supplementary material for proofs of the following results.\nLemma 6. For a dataset with global relative sparsity (k, γ), there is some k̃ ∈ Ω(max(k log(n/k), k(n/k)1−log2 γ)) such that the probability that the candidate points retrieved from a given composite index do not include some of the true k-nearest neighbours is at most some constant c < 1.\nTheorem 7. For a dataset with global relative sparsity (k, γ), for any > 0, there is some L and k̃ ∈ Ω(max(k log(n/k), k(n/k)1−log2 γ)) such that the algo-\nrithm returns the correct set of k-nearest neighbours with probability of at least 1− .\nThe above result suggests that we should choose k̃ ∈ Ω(max(k log(n/k), k(n/k)1−log2 γ)) to ensure the algorithm succeeds with high probability. Next, we analyze the time and space complexity of the algorithm. Proofs of Theorems 10 and 11 are found in the supplementary material.\nTheorem 8. The algorithm takes O(max(dk log(n/k), dk(n/k)1−log2 γ)) time to retrieve the k-nearest neighbours at query time.\nProof. Computing projections of the query point along all vjl’s takesO(d) time, sincem and L are constants. Searching in the binary search trees Tjl’s takes O(log n) time. The total number of candidate points retrieved is at most Θ(max(k log(n/k), k(n/k)1−log2 γ)). Computing the distance between each candidate point and the query point takes at most O(max(dk log(n/k), dk(n/k)1−log2 γ)) time. We can find the k closest points to q in the set of candidate points using a selection algorithm like quickselect, which takes O(max(k log(n/k), k(n/k)1−log2 γ)) time on average. Since the time taken to compute distances to the query point dominates, the entire algorithm takes O(max(dk log(n/k), dk(n/k)1−log2 γ)) time.\nTheorem 9. The algorithm takes O(dn + n log n) time to preprocess the data points in S at construction time.\nProof. Computing projections of all n points along all vjl’s takes O(dn) time, since m and L are constants. Inserting all n points into mL self-balancing binary search trees takes O(n log n) time.\nTheorem 10. The algorithm requires O(d+ log n) time to insert a new data point and O(log n) time to delete a data point.\nTheorem 11. The algorithm requires O(n) space in addition to the space used to store the data."
    }, {
      "heading" : "3.4. Data-Dependent Version",
      "text" : "Conceptually, performance of the proposed algorithm depends on two factors: how likely the index returns the true nearest neighbours before other points and when the algorithm stops retrieving points from the index. The preceding sections primarily focused on the former; in this section, we will take a closer look at the latter in this section.\nOne strategy, which is used by the data-independent version of the algorithm, is to stop after a preset number of iterations of the outer loop. Although simple, such a strategy leaves much to be desired. First of all, in order to set the number of iterations, it requires knowledge of the global\nrelative sparsity of the dataset, which is rarely known a priori. Computing this is either very expensive in the case of large datasets or infeasible in the case of streaming data, as global relative sparsity may change as new data points arrive. More importantly, it is unable to take advantage of the local relative sparsity in the neighbourhood of the query. A method that is capable of adapting to local relative sparsity could potentially be much faster because query points tend to be close to the manifold on which points in the dataset lie, resulting in the dataset being sparse in the neighbourhood of the query point.\nIdeally, the algorithm should stop as soon as it has retrieved the true nearest neighbours. Determining if this is the case amounts to asking if there exists a point that we have not seen lying closer to the query than the points we have seen. At first sight, because nothing is known about unseen points, it seems not possible to do better than exhaustive search, as we can only rule out the existence of such a point after computing distances to all unseen points. Somewhat surprisingly, by exploiting the fact that the projections associated with the index are random, it is possible to make inferences about points that we have never seen. We do so by leveraging ideas from statistical hypothesis testing.\nAfter each iteration of the outer loop, we perform a hypothesis test, with the null hypothesis being that the complete set of the k-nearest neighbours has not yet been retrieved. Rejecting the null hypothesis implies accepting the alternative hypothesis that all the true k-nearest neighbours have been retrieved. At this point, the algorithm can safely terminate while guaranteeing that the probability that the algorithm fails to return the correct results is bounded above by the significance level. The test statistic is an upper bound on the probability of missing a true k-nearest neighbour. The resulting algorithm does not require any prior knowledge about the dataset and can adapt to the local relative sparsity of the dataset in the neighbourhood of the query point on-the-fly; for this reason, we will refer to this version of the algorithm as the data-dependent version.\nMore concretely, as the algorithm retrieves candidate points, it computes their true distances to the query point and maintains a list of k points that are the closest to the query among the points retrieved from all composite indices so far. Let p(i) denote the ith closest point, d(i) denote the the distance from p(i) to the query point and dmaxl denote the distance to the query point from the farthest candidate point retrieved from the lth composite index. When the number of candidate points exceeds k, the algorithm checks if ∏L l=1 ( 1− ( 2 π cos\n−1 (d(k)/dmaxl ))m) ≤ , where is the maximum tolerable failure probability, after each iteration of the outer loop. If the condition is satisfied, the algorithm terminates and returns { p(i) }k i=1 .\nBecause distances to the query point are only computed for candidate points that have been retrieved so far, the time complexity of this algorithm is the same as that of the data-independent version when k̃ is set such that the outer loop executes for the same number of iterations as the data-dependent version. We show the correctness of this algorithm below: Theorem 12. For any > 0, m and L, the algorithm returns the correct set of k-nearest neighbours of the query point q with probability of at least 1− .\nProof. We analyze the probability that the algorithm fails to return the correct set of k-nearest neighbours. Let p∗ denote a true k-nearest neighbour that was missed and d∗ denote the distance from p∗ to the query. If the algorithm fails, then for any given composite index, p∗ is not among the candidate points retrieved from the said index. In other words, the composite index must return all these points before p∗, implying that at least one constituent simple index returns all these points before p∗. This means that all these points must appear closer to the query point than p∗ under the projection associated with the simple index. By Lemma 2, if we take {li}Ni=1 to be displacement vectors from the query point to the candidate points that are farther from the query point than p∗ and s to be the displacement vector from the query point to p∗, the probability of this occurring for a given constituent simple index of the lth composite index is at most 1 − 2π cos\n−1 (d∗/dmaxl ). The probability that this occurs for some constituent simple index is at most 1 − ( 2 π cos −1 (d∗/dmaxl ) )m\n. For the algorithm to fail, this must occur for all composite indices; so the failure probability is at most ∏L l=1 ( 1− ( 2 π cos −1 (d∗/dmaxl ) )m) .\nWe observe that d∗ ≤ d(k) since there can be at most k− 1 points in the dataset that are closer to the query point than p∗. So, the failure probability can be bounded above by ∏L l=1 ( 1− 2π cos\n−1 (d(k)/dmaxl )). When the algorithm terminates, we know this quantity is at most . Therefore, the algorithm returns the correct set of k-nearest neighbours with probability of at least 1− ."
    }, {
      "heading" : "4. Experiments",
      "text" : "In many applications, we are more interested in how close the points returned by the algorithm are to the query rather than whether the returned points are the true k-nearest neighbours. We empirically evaluate the performance of the proposed algorithm in this approximate setting below.\nWe compare the performance of the proposed algorithm and LSH on the CIFAR-100 and MNIST datasets, which consist of 32× 32 colour images of various real-world objects and 28 × 28 grayscale images of handwritten digits respectively. We reshape the images into vectors, with each dimension representing the pixel intensity at a particular lo-\ncation and colour channel of the image. The resulting vectors have a dimensionality of 32×32×3 = 3072 in the case of CIFAR-100 and 28×28 = 784 in the case of MNIST, so the dimensionality under consideration is higher than what traditional tree-based algorithms can handle. We combine the training set and the test set of each dataset, so each dataset contains 60, 000 instances in total.\nWe randomize the instances that serve as queries using cross-validation. Specifically, we randomly select 100 instances to serve as query points and the designate the remaining instances as data points. Each algorithm is then used to retrieve approximate k-nearest neighbours of each query point among the set of all data points. This procedure is repeated for ten folds, each with a different split of query vs. data points.\nWe compare the number of candidate points that each algorithm requires to achieve a desired level of approximation quality. We quantify approximation quality using the approximation ratio, which is defined as the ratio of the distance between the query and the ith approximate nearest neighbour to the distance between the query and the ith true nearest neighbour averaged over all i ∈ [k]. So, the smaller the approximation ratio, the better the approximation quality. Because dimensionality is high and exhaustive search must be performed over all candidate points, the time taken for compute distances between the candidate points and the query dominates the overall running time of the querying operation. Therefore, the number of candidate points can be viewed as an implementation-independent proxy for the running time.\nBecause the hash table constructed by LSH depends on the desired level of approximation quality, we construct a different hash table for each level of approximation quality. On the other hand, the indices constructed by the proposed method are not specific to a particular level of approximation quality; instead, approximation quality can be controlled at query time by varying the number of iterations of the outer loop. So, the same indices are used for all levels of approximation quality. Therefore, our evaluation scheme is biased towards LSH and against the proposed method.\nFor comparability, we set m and L and their analogous counterparts in LSH (k and L) to the same values. For our experiments, we used m = 3 and L = 3, which we found to work well in practice. In Figures 2a and 2b, we plot the performance of the proposed method and LSH on the CIFAR-100 and MNIST datasets for retrieving 10-nearest neighbours. We found performance to be similar for other values of k.\nFor retrieving nearest neighbours, MNIST is a more challenging dataset than CIFAR-100. This is because the instances in MNIST form dense clusters, whereas the in-\nstances in CIFAR-100 are more visually diverse and so are more dispersed in space. Intuitively, if the query falls inside a dense cluster of points, there are many points that are very close to the query and so it is difficult to distinguish true nearest neighbours from points that are only slightly farther away. Viewed differently, because true nearest neighbours tend to be extremely close to the query on MNIST, the denominator for computing the approximation ratio is usually very small. Consequently, returning points that are only slightly farther away than the true nearest neighbours would result in a large approximation ratio. As a result, both the proposed method and LSH require far more candidate points on MNIST than on CIFAR-100 to achieve comparable approximation ratios.\nWe find the proposed method achieves better performance than LSH at all levels of approximation quality. As the required approximation quality increases, the improvement in performance over LSH becomes greater. Additionally, while the performance of LSH degrades drastically on MNIST, the performance of the proposed method drops less significantly, resulting in a larger gain of the proposed method relative to LSH. Quantitatively, compared to LSH, the proposed method requires 32.5% − 42.1% fewer candidate points on CIFAR-100, and 66.6% − 82.1% fewer candidate points on MNIST.\nThis indicates that the candidate points retrieved by the proposed method are more likely to lie near the query than those retrieved by LSH. This is because LSH introduces discretization errors when it partitions the space and effectively loses information about the precise locations of points. So, points located in the same bin are returned in arbitrary order. Additionally, if the query lies close to the\npartition boundary, points close to the query lying in adjacent cells will not be returned. These two effects lead to lower precision and recall, and so LSH requires more candidate points to achieve the same approximation ratio compared to the proposed method.\nWe also find the proposed method to be more spaceefficient than LSH. Empirically, the proposed method uses 76.0% less memory on CIFAR-100 and 75.8% less memory on MNIST."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we delineated the inherent deficiencies of space partitioning and presented a new strategy for fast retrieval of k-nearest neighbours which we dub dynamic continuous indexing (DCI). Instead of discretizing the space, the proposed algorithm constructs continuous indices, each of which imposes an ordering on data points in which closeby positions approximately reflect proximity in the vector space. Unlike existing methods, the proposed algorithm allows granular control over accuracy and speed on a per-query basis, adapts to variations to dataset density on-the-fly and supports online updates to the dataset. We analyzed the proposed algorithm and showed it runs in time linear in dimensionality and sub-linear in the size of the dataset and takes space constant in dimensionality and linear in the size of the dataset. Furthermore, we demonstrated empirically that the proposed algorithm compares favourably to LSH in terms of approximation quality and speed."
    }, {
      "heading" : "6. Supplementary Material",
      "text" : "Below, we present proofs of the results shown in the main paper:\nLemma 6. For a dataset with global relative sparsity (k, γ), there is some k̃ ∈ Ω(max(k log(n/k), k(n/k)1−log2 γ)) such that the probability that the candidate points retrieved from a given composite index do not include some of the true k-nearest neighbours is at most some constant c < 1.\nProof of Lemma 6. Consider a ranking of the points {pi}ni=1 by their distance to the query point under a given projection vjl in nondecreasing order. We will refer to points in the top k̃ positions that are the true k-nearest neighbours as true positives and those that are not as false positives. In addition, we will refer to points not in the top k̃ positions that are the true k-nearest neighbours as false negatives.\nWhen not all the true k-nearest neighbours are in the top k̃ positions, then there must be at least one false negative. Since there are at most k − 1 true positives, there must be at least k̃ − (k − 1) false positives.\nNow we categorize the false positives as either reasonable or silly. A false positive is reasonable if it is one of the 2k-nearest neighbours, and is silly otherwise. There are at most k reasonable false positives, and so there must be at least k̃ − (k − 1) − k = k̃ − 2k + 1 silly false positives. Therefore, the event that not all the true k-nearest neighbours are in the top k̃ positions must be contained by the event that there are at least k̃ − 2k + 1 silly false positives.\nWe find the probability that such a set of silly false positives exists under projection vjl. For each pi, define vector di = pi − q, so that ‖di‖2 corresponds to the distance between pi and q. Let Tp and Tf be the sets consisting of indices of points that are the k-nearest neighbours and points are not the 2k-nearest neighbours respectively. By applying Theorem 3, where we take {si}Mi=1 to be {dj}j∈Tp , {li}Ni=1 to be {dj}j∈Tf , k′ to be k̃ − 2k + 1 and p to be vjl, we conclude that the probability that there are at least k̃−2k+1 silly false positives under projection vjl is at most\n1 k̃−2k+1 ∑ i∈Tf ( 1− 2π cos −1 ( maxj∈Tp‖dj‖2 ‖di‖2 )) . This implies that the probability that the top k̃ positions do not contain some of the true k-nearest neighbours is bounded above by the same quantity, and so the probability that the top k̃ positions contain all true k-nearest neighbours is at least 1− 1\nk̃−2k+1 ∑ i∈Tf ( 1− 2π cos −1 ( maxj∈Tp‖dj‖2 ‖di‖2 )) .\nNow consider the lth composite index, which consists of m simple indices associated with the projections v1l, . . . , vml. The probability that candidate points re-\ntrieved from the composite index after k̃ iterations of the outer loop contain all true k-nearest neighbours is at least[ 1− 1\nk̃−2k+1 ∑ i∈Tf ( 1− 2π cos −1 ( maxj∈Tp‖dj‖2 ‖di‖2 ))]m ,\nsince a point is in the candidate set if and only if it is in the top k̃ positions for all projections v1l, . . . , vml. The failure probability is therefore at most 1 −[ 1− 1\nk̃−2k+1 ∑ i∈Tf ( 1− 2π cos −1 ( maxj∈Tp‖dj‖2 ‖di‖2 ))]m .\nIf we let {p(i)}ni=1 denote a re-ordering of the points {pi}ni=1 so that p(i) is the ith closest point to q and use the fact that 1− (2/π) cos−1 (x) ≤ x ∀x ∈ [0, 1], this quantity\nis at most 1− [ 1− 1\nk̃−2k+1\n∑n i=2k+1 ‖p(k)−q‖ 2\n‖p(i)−q‖ 2\n]m .\nBy definition of global relative sparsity, for all i ≥ 2k + 1,∥∥p(i) − q∥∥ 2 > γ ∥∥p(k) − q∥∥ 2 . By applying this recur-\nsively, we see that for all i ≥ 2jk + 1, ∥∥p(i) − q∥∥\n2 > γj ∥∥p(k) − q∥∥\n2 . It follows that ∑n i=2k+1 ‖p(k)−q‖ 2\n‖p(i)−q‖ 2\nis less than ∑dlog2(n/k)e−1 j=1 2\njkγ−j . If γ ≥ 2, this quantity is at most k log2 ( n k ) . If 1 ≤ γ < 2, this quantity can be expressed as:\nk\n( 2\nγ\n)(( 2\nγ\n)dlog2(n/k)e−1 − 1 ) / ( 2 γ − 1 )\n= O ( k ( 2\nγ\n)dlog2(n/k)e−1)\n= O ( k (n k )1−log2 γ)\nSo, there is some k̃ ∈ Ω(max(k log(n/k), k(n/k)1−log2 γ)) that makes\n1 k̃−2k+1 ∑n i=2k+1 ‖p(k)−q‖ 2\n‖p(i)−q‖ 2\nstrictly less than 1.\nHence, the upper bound on the failure probability, 1 − [ 1− 1\nk̃−2k+1\n∑n i=2k+1 ‖p(k)−q‖ 2\n‖p(i)−q‖ 2\n]m , is strictly less\nthan 1.\nTheorem 7. For a dataset with global relative sparsity (k, γ), for any > 0, there is some L and k̃ ∈ Ω(max(k log(n/k), k(n/k)1−log2 γ)) such that the algorithm returns the correct set of k-nearest neighbours with probability of at least 1− .\nProof of Theorem 7. By Lemma 6, the first k̃ points retrieved from a given composite index do not include some of the true k-nearest neighbours with probability of at most c. For the algorithm to fail, this must occur for all composite indices. Since each composite index is constructed independently, the algorithm fails with probability of at most\ncL, and so must succeed with probability of at least 1− cL. Since c < 1, there is some L that makes 1−cL ≥ 1− .\nTheorem 10. The algorithm requires O(d+ log n) time to insert a new data point and O(log n) time to delete a data point.\nProof of Theorem 10. In order to insert a data point, we need to compute its projection along all vjl’s and insert it into each binary search tree. Computing the projection takesO(d) time and inserting an entry into a self-balancing binary search tree takes O(log n) time. In order to delete a data point, we simply remove it from each of the binary search trees, which takes O(log n) time.\nTheorem 11. The algorithm requires O(n) space in addition to the space used to store the data.\nProof of Theorem 11. The only additional information that needs to be stored are the mL binary search trees. Since n entries are stored in each binary search tree, the additional space required is O(n)."
    } ],
    "references" : [ {
      "title" : "An optimal algorithm for approximate nearest neighbor searching fixed dimensions",
      "author" : [ "Arya", "Sunil", "Mount", "David M", "Netanyahu", "Nathan S", "Silverman", "Ruth", "Wu", "Angela Y" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Arya et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Arya et al\\.",
      "year" : 1998
    }, {
      "title" : "Multidimensional binary search trees used for associative searching",
      "author" : [ "Bentley", "Jon Louis" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Bentley and Louis.,? \\Q1975\\E",
      "shortCiteRegEx" : "Bentley and Louis.",
      "year" : 1975
    }, {
      "title" : "The X-tree : An Index Structure for HighDimensional Data",
      "author" : [ "Berchtold", "Stefan", "Keim", "Daniel A", "peter Kriegel", "Hans" ],
      "venue" : "In Very Large Data Bases, pp",
      "citeRegEx" : "Berchtold et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Berchtold et al\\.",
      "year" : 1996
    }, {
      "title" : "Fast nearest neighbor search in high-dimensional space",
      "author" : [ "Berchtold", "Stefan", "Ertl", "Bernhard", "Keim", "Daniel A", "Kriegel", "H-P", "Seidl", "Thomas" ],
      "venue" : "In Data Engineering,",
      "citeRegEx" : "Berchtold et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Berchtold et al\\.",
      "year" : 1998
    }, {
      "title" : "Near neighbor search in large metric spaces",
      "author" : [ "Brin", "Sergey" ],
      "venue" : "VLDB, pp",
      "citeRegEx" : "Brin and Sergey.,? \\Q1995\\E",
      "shortCiteRegEx" : "Brin and Sergey.",
      "year" : 1995
    }, {
      "title" : "Random projection trees and low dimensional manifolds",
      "author" : [ "Dasgupta", "Sanjoy", "Freund", "Yoav" ],
      "venue" : "In Proceedings of the fortieth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Dasgupta et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2008
    }, {
      "title" : "A branch and bound algorithm for computing k-nearest neighbors",
      "author" : [ "Fukunaga", "Keinosuke", "Narendra", "Patrenahalli M" ],
      "venue" : "Computers, IEEE Transactions on,",
      "citeRegEx" : "Fukunaga et al\\.,? \\Q1975\\E",
      "shortCiteRegEx" : "Fukunaga et al\\.",
      "year" : 1975
    }, {
      "title" : "R-trees: a dynamic index structure for spatial searching, volume",
      "author" : [ "Guttman", "Antonin" ],
      "venue" : null,
      "citeRegEx" : "Guttman and Antonin.,? \\Q1984\\E",
      "shortCiteRegEx" : "Guttman and Antonin.",
      "year" : 1984
    }, {
      "title" : "Approximate nearest neighbors: towards removing the curse of dimensionality",
      "author" : [ "Indyk", "Piotr", "Motwani", "Rajeev" ],
      "venue" : "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Indyk et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Indyk et al\\.",
      "year" : 1998
    }, {
      "title" : "Extensions of lipschitz mappings into a hilbert space",
      "author" : [ "Johnson", "William B", "Lindenstrauss", "Joram" ],
      "venue" : "Contemporary mathematics,",
      "citeRegEx" : "Johnson et al\\.,? \\Q1984\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 1984
    }, {
      "title" : "Finding nearest neighbors in growth-restricted metrics",
      "author" : [ "Karger", "David R", "Ruhl", "Matthias" ],
      "venue" : "In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Karger et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Karger et al\\.",
      "year" : 2002
    }, {
      "title" : "An investigation of practical approximate nearest neighbor algorithms. In Advances in neural information processing",
      "author" : [ "Liu", "Ting", "Moore", "Andrew W", "Yang", "Ke", "Gray", "Alexander G" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2004
    }, {
      "title" : "Scalable recognition with a vocabulary tree",
      "author" : [ "Nister", "David", "Stewenius", "Henrik" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Nister et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nister et al\\.",
      "year" : 2006
    }, {
      "title" : "Locality sensitive hashing: A comparison of hash function types and querying mechanisms",
      "author" : [ "Paulevé", "Loı̈c", "Jégou", "Hervé", "Amsaleg", "Laurent" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "Paulevé et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Paulevé et al\\.",
      "year" : 2010
    }, {
      "title" : "A fast exact k-nearest neighbors algorithm for high dimensional search using k-means clustering and triangle inequality",
      "author" : [ "Wang", "Xueyi" ],
      "venue" : "In Neural Networks (IJCNN), The 2011 International Joint Conference on,",
      "citeRegEx" : "Wang and Xueyi.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wang and Xueyi.",
      "year" : 2011
    }, {
      "title" : "Spectral hashing",
      "author" : [ "Weiss", "Yair", "Torralba", "Antonio", "Fergus", "Rob" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Weiss et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "One popular approach used by LSH and spill trees (Liu et al., 2004) to mitigate this effect is to partition the space using overlapping cells and search over all points that lie in any of the cells that contain the query point.",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "Early approaches store points in deterministic tree-based data structures, such as k-d trees (Bentley, 1975), R-trees (Guttman, 1984) and X-trees (Berchtold et al., 1996; 1998), which effectively partition the vector space into a hierarchy of half-spaces, hyper-rectangles or Voronoi polygons.",
      "startOffset" : 146,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : "More recent methods like spill trees (Liu et al., 2004) and RP trees (Dasgupta & Freund, 2008) extend these approaches by randomizing the dividing hyperplane at each node.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Treebased methods (Arya et al., 1998) have been proposed for this setting; unfortunately, the running time still exhibits exponential dependence on dimensionality.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 13,
      "context" : "This motivated the development of data-dependent hashing schemes based on k-means (Paulevé et al., 2010) and spectral partitioning (Weiss et al.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : ", 2010) and spectral partitioning (Weiss et al., 2009).",
      "startOffset" : 34,
      "endOffset" : 54
    } ],
    "year" : 2017,
    "abstractText" : "Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by almost all existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality and sub-linear in the size of the dataset and takes space constant in dimensionality and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in dataset density, supports dynamic updates to the dataset and is easy-to-implement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms localitysensitivity hashing (LSH) in terms of approximation quality and speed.",
    "creator" : "LaTeX with hyperref package"
  }
}