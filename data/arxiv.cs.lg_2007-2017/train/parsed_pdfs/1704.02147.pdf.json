{
  "name" : "1704.02147.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hierarchical Clustering: Objective Functions and Algorithms",
    "authors" : [ "Vincent Cohen-Addad", "Varun Kanade", "Frederik Mallmann-Trenn" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n02 14\n7v 1\n[ cs\n.D S]\nWe take an axiomatic approach to defining ‘good’ objective functions for both similarity and dissimilarity-based hierarchical clustering. We characterize a set of admissible objective functions (that includes the one introduced by Dasgupta) that have the property that when the input admits a ‘natural’ ground-truth hierarchical clustering, the ground-truth clustering has an optimal value.\nEquipped with a suitable objective function, we analyze the performance of practical algorithms, as well as develop better and faster algorithms for hierarchical clustering. For similaritybased hierarchical clustering, Dasgupta (2016) showed that a simple recursive sparsest-cut based approach achieves an Oplog3{2 nq-approximation on worst-case inputs. We give a more refined analysis of the algorithm and show that it in fact achieves an Op ? lognq-approximation1. This improves upon the LP-based Oplog nq-approximation of Roy and Pokutta (2016). For dissimilarity-based hierarchical clustering, we show that the classic average-linkage algorithm gives a factor 2 approximation, and provide a simple and better algorithm that gives a factor 3{2 approximation. This aims at explaining the success of this heuristics in practice. Finally, we consider ‘beyond-worst-case’ scenario through a generalisation of the stochastic block model for hierarchical clustering. We show that Dasgupta’s cost function also has desirable properties for these inputs and we provide a simple algorithm that for graphs generated according to this model yields a 1 + o(1) factor approximation.\n1Charikar and Chatziafratis (2017) independently proved that the sparsest-cut based approach achieves a Op ? log nq approximation.\nContents"
    }, {
      "heading" : "1 Introduction 2",
      "text" : "1.1 Summary of Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"
    }, {
      "heading" : "2 Preliminaries 6",
      "text" : "2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2 Ultrametrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"
    }, {
      "heading" : "3 Quantifying Output Value: An Axiomatic Approach 9",
      "text" : "3.1 Admissible Cost Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.2 Characterizing Admissible Cost Functions . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.2.1 Characterizing g that satisfy conditions of Theorem 3.4 . . . . . . . . . . . . 13 3.2.2 Characterizing Objective Functions for Dissimilarity Graphs . . . . . . . . . . 14"
    }, {
      "heading" : "4 Similarity-Based Inputs: Approximation Algorithms 15",
      "text" : ""
    }, {
      "heading" : "5 Admissible Objective Functions and Algorithms for Random Inputs 17",
      "text" : "5.1 A Random Graph Model For Hierarchical Clustering . . . . . . . . . . . . . . . . . . 17 5.2 Objective Functions and Ground-Truth Tree . . . . . . . . . . . . . . . . . . . . . . . 19 5.3 Algorithm for Clustering in the HSBM . . . . . . . . . . . . . . . . . . . . . . . . . . 21"
    }, {
      "heading" : "6 Dissimilarity-Based Inputs: Approximation Algorithms 25",
      "text" : "6.1 Average-Linkage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 6.2 A Simple and Better Approximation Algorithm for Worst-Case Inputs . . . . . . . . 27"
    }, {
      "heading" : "7 Perfect Ground-Truth Inputs and Beyond 29",
      "text" : "7.1 Perfect Ground-Truth Inputs are Easy . . . . . . . . . . . . . . . . . . . . . . . . . . 29 7.2 A Near-Linear Time Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 7.3 Beyond Structured Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n8 Worst-Case Analysis of Common Heuristics 36"
    }, {
      "heading" : "1 Introduction",
      "text" : "A hierarchical clustering is a recursive partitioning of a dataset into successively smaller clusters. The input is a weighted graph whose edge weights represent pairwise similarities or dissimilarities between datapoints. A hierarchical clustering is represented by a rooted tree where each leaf represents a datapoint and each internal node represents a cluster containing its descendant leaves. Computing a hierarchical clustering is a fundamental problem in data analysis; it is routinely used to analyze, classify, and pre-process large datasets. A hierarchical clustering provides useful information about data that can be used, e.g., to divide a digital image into distinct regions of different granularities, to identify communities in social networks at various societal levels, or to determine the ancestral tree of life. Developing robust and efficient algorithms for computing hierarchical clusterings is of importance in several research areas, such as machine learning, big-data analysis, and bioinformatics.\nCompared to flat partition-based clustering (the problem of dividing the dataset into k parts), hierarchical clustering has received significantly less attention from a theory perspective. Partitionbased clustering is typically framed as minimizing a well-defined objective such as k-means, kmedians, etc. and (approximation) algorithms to optimize these objectives have been a focus of study for at least two decades. On the other hand, hierarchical clustering has rather been studied at a more procedural level in terms of algorithms used in practice. Such algorithms can be broadly classified into two categories, agglomerative heuristics which build the candidate cluster tree bottom up, e.g., average-linkage, single-linkage, and complete-linkage, and divisive heuristics which build the tree top-down, e.g., bisection k-means, recursive sparsest-cut etc. Dasgupta (2016) identified the lack of a well-defined objective function as one of the reasons why the theoretical study of hierarchical clustering has lagged behind that of partition-based clustering.\nDefining a Good Objective Function.\nWhat is a ‘good’ output tree for hierarchical clustering? Let us suppose that the edge weights represent similarities (similar datapoints are connected by edges of high weight)2. Dasgupta (2016) frames hierarchical clustering as a combinatorial optimization problem, where a good output tree is a tree that minimizes some cost function; but which function should that be? Each (binary) tree node is naturally associated to a cut that splits the cluster of its descendant leaves into the cluster of its left subtree on one side and the cluster of its right subtree on the other, and Dasgupta defines the objective to be the sum, over all tree nodes, of the total weight of edges crossing the cut multiplied by the cardinality of the node’s cluster. In what sense is this good? Dasgupta argues that it has several attractive properties: (1) if the graph is disconnected, i.e., data items in different connected components have nothing to do with one another, then the hierarchical clustering that minimizes the objective function begins by first pulling apart the connected components from one another; (2) when the input is a (unit-weight) clique then no particular structure is favored and all binary trees have the same cost; and (3) the cost function also behaves in a desirable manner for data containing a planted partition. Finally, an attempt to generalize the cost function leads to functions that violate property (2).\nIn this paper, we take an axiomatic approach to defining a ‘good’ cost function. We remark that in many application, for example in phylogenetics, there exists an unknown ‘ground truth’ hierarchical clustering— the actual ancestral tree of life—from which the similarities are generated (possibly with noise), and the goal is to infer the underlying ground truth tree from the available data. In this sense, a cluster tree is good insofar as it is isomorphic to the (unknown) ground-truth\n2This entire discussion can equivalently be phrased in terms of dissimilarities without changing the essence.\ncluster tree, and thus a natural condition for a ‘good’ objective function is one such that for inputs that admit a ‘natural’ ground-truth cluster tree, the value of the ground-truth tree is optimal. We provide a formal definition of inputs that admit a ground-truth cluster tree in Section 2.2.\nWe consider, as potential objective functions, the class of all functions that sum, over all the nodes of the tree, the total weight of edges crossing the associated cut times some function of the cardinalities of the left and right clusters (this includes the class of functions considered by Dasgupta (2016)). In Section 3 we characterize the ‘good’ objective functions in this class and call them admissible objective functions. We prove that for any objective function, for any ground-truth input, the ground-truth tree has optimal cost (w.r.t to the objective function) if and only if the objective function (1) is symmetric (independent of the left-right order of children), (2) is increasing in the cardinalities of the child clusters, and (3) for (unit-weight) cliques, has the same cost for all binary trees (Theorem 3.4). Dasgupta’s objective function is admissible in terms of the criteria described above.\nIn Section 5, we consider random graphs that induce a natural clustering. This model can be seen as a noisy version of our notion of ground-truth inputs and a hierarchical stochastic block model. We show that the ground-truth tree has optimal expected cost for any admissible objective function. Furthermore, we show that the ground-truth tree has cost at most p1 ` op1qqOPT with high probability for the objective function introduced by Dasgupta (2016).\nAlgorithmic Results\nThe objective functions identified in Section 3 allow us to (1) quantitatively compare the performances of algorithms used in practice and (2) design better and faster approximation algorithms.3\nAlgorithms for Similarity Graphs: Dasgupta (2016) shows that the recursive φ-approximate sparsest cut algorithm, that recursively splits the input graph using a φ-approximation to the sparsest cut problem, outputs a tree whose cost is at most Opφ log n ¨ OPTq. Roy and Pokutta (2016) recently gave an Oplog nq-approximation by providing a linear programming relaxation for the problem and providing a clever rounding technique. Charikar and Chatziafratis (2017) showed that the recursive φ-sparsest cut algorithm of Dasgupta gives an Opφq-approximation. In Section 4, we obtain an independent proof showing that the φ-approximate sparsest cut algorithm is an Opφq-approximation (Theorem 4.1)4. Our proof is quite different from the proof of Charikar and Chatziafratis (2017) and relies on a charging argument. Combined with the celebrated result of Arora et al. (2009), this yields an Op ? log nq-approximation. The results stated here apply to Dasgupta’s objective function; the approximation algorithms extend to other objective functions, though the ratio depends on the specific function being used. We conclude our analysis of the worst-case setting by showing that all the linkage-based algorithms commonly used in practice can perform rather poorly on worst-case inputs (see Sec. 8).\nAlgorithms for Dissimilarity Graphs: Many of the algorithms commonly used in practice, e.g., linkage-based methods, assume that the input is provided in terms of pairwise dissimilarity (e.g., points that lie in a metric space). As a result, it is of interest to understand how they fare when compared using admissible objective functions for the dissimilarity setting. When the edge\n3For the objective function proposed in his work, Dasgupta (2016) shows that finding a cluster tree that minimizes the cost function is NP-hard. This directly applies to the admissible objective functions for the dissimilarity setting as well. Thus, the focus turns to developing approximation algorithms.\n4Our analysis shows that the algorithm achieves a 6.75φ-approximation and the analysis of Charikar and Chatziafratis (2017) yields a 8φ-approximation guarantee. This minor difference is of limited impact since the best approximation guarantee for sparsest-cut is Op ? log nq.\nweights of the input graph represent dissimilarities, the picture is considerably different from an approximation perspective. For the analogue of Dasgupta’s objective function in the dissimilarity setting, we show that the average-linkage algorithm (see Algorithm 3) achieves a 2-approximation (Theorem 6.2). This stands in contrast to other practical heuristic-based algorithms, which may have an approximation guarantee as bad as Ωpn1{4q (Theorem 8.6). Thus, using this objectivefunction based approach, one can conclude that the average-linkage algorithm is the more robust of the practical algorithms, perhaps explaining its success in practice. We also provide a new, simple, and better algorithm, the locally densest-cut algorithm,5 which we show gives a 3{2-approximation (Theorem 6.5). Our results extend to any admissible objective function, though the exact approximation factor depends on the specific choice.\nStructured Inputs and Beyond-Worst-Case Analysis: The recent work of Roy and Pokutta (2016) and Charikar and Chatziafratis (2017) have shown that obtaining constant approximation guarantees for worst-case inputs is beyond current techniques (see Section 1.2). Thus, we consider inputs that admit a ‘natural’ ground-truth cluster tree. For such inputs, we show that essentially all the practical algorithms do the right thing, in that they recover the ground-truth cluster tree. Since real-world inputs might exhibit a noisy structure, we consider more general scenarios:\n• We consider a natural generalization of the classic stochastic block model that generates random graphs with a hidden ground-truth hierarchical clustering. We provide a simple algorithm based on singular value decomposition (SVD) and agglomerative methods that achieves a p1 ` op1qq-approximation for Dasgupta’s objective function (in fact, it recovers the ground-truth tree) with high probability. Interestingly, this algorithm is very similar to approaches used in practice for hierarchical clustering.\n• We introduce the notion of a δ-adversarially perturbed ground-truth input, which can be viewed as being obtained from a small perturbation to an input that admits a natural ground truth cluster tree. This approach bears similarity to the stability-based conditions used by Balcan et al. (2008) and Bilu and Linial (2012). We provide an algorithm that achieves a δ-approximation in both the similarity and dissimilarity settings, independent of the objective function used as long as it is admissible according to the criteria used in Section 3."
    }, {
      "heading" : "1.1 Summary of Our Contributions",
      "text" : "Our work makes significant progress towards providing a more complete picture of objectivefunction based hierarchical clustering and understanding the success of the classic heuristics for hierarchical clustering.\n• Characterization of ‘good’ objective functions. We prove that for any ground-truth input, the ground-truth tree has strictly optimal cost for an objective function if and only if, the objective function (1) is symmetric (independent of the left-right order of children), (2) is monotone in the cardinalities of the child clusters, and (3) for unit-weight cliques, gives the same weight to all binary trees (Theorem 3.4). We refer to such objective functions as admissible; according to these criteria Dasgupta’s objective function is admissible.\n• Worst-case approximation. First, for similarity-based inputs, we provide a new proof that the recursive φ-approximate sparsest cut algorithm is an Opφq-approximation (hence an Op ? log nq-approximation) (Theorem 4.1) for Dasgupta’s objective function. Second, for\n5We say that a cut pA,Bq is locally dense if moving a vertex from A to B or from to B to A does not increase the density of the cut. One could similarly define locally-sparsest-cut.\ndissimilarity-based inputs, we show that the classic average-linkage algorithm is a 2-approximation (Theorem 6.2), and provide a new algorithm which we prove is a 3{2-approximation (Theorem 6.5). All those results extend to other cost functions but the approximation ratio is function-dependent.\n• Beyond worst-case. First, stochastic models. We consider the hierarchical stochastic block model (Definition 5.1). We give a simple algorithm based on SVD and classic agglomerative methods that, with high probability, recovers the ground-truth tree and show that this tree has cost that is p1` op1qqOPT with respect to Dasgupta’s objective function (Theorem 5.8). Second, adversarial models. We introduce the notion of δ-perturbed inputs, obtained by a small adversarial perturbation to ground-truth inputs, and give a simple δ-approximation algorithm (Theorem 7.8).\n• Perfect inputs, perfect reconstruction. For ground-truth inputs, we note that the algorithms used in practice (the linkage algorithms, the bisection 2-centers, etc.) correctly reconstruct a ground truth tree (Theorems 7.1, 7.3, 7.5). We introduce a simple, faster algorithm that is also optimal on ground-truth inputs (Theorem 7.7)."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "The recent paper of Dasgupta (2016) served as the starting point of this work. Dasgupta (2016) defined an objective function for hierarchical clustering and thus formulated the question of constructing a cluster tree as a combinatorial optimization problem. Dasgupta also showed that the resulting problem is NP-hard and that the recursive φ-sparsest-cut algorithm achieves an Opφ log nqapproximation. Dasgupta’s results have been improved in two subsequent papers. Roy and Pokutta (2016) wrote an integer program for the hierarchical clustering problem using a combinatorial characterization of the ultrametrics induced by Dasgupta’s cost function. They also provide a spreading metric LP and a rounding algorithm based on sphere/region-growing that yields an Oplog nq-approximation. Finally, they show that no polynomial size SDP can achieve a constant factor approximation for the problem and that under the Small Set Expansion (SSE) hypothesis, no polynomial-time algorithm can achieve a constant factor approximation.\nCharikar and Chatziafratis (2017) also gave a proof that the problem is hard to approximate within any constant factor under the Small Set Expansion hypothesis. They also proved that the recursive φ-sparsest cut algorithm produces a hierarchical clustering with cost at most OpφOPTq; their techniques appear to be significantly different from ours. Additionally, Charikar and Chatziafratis (2017) introduce a spreading metric SDP relaxation for the hierarchical clustering problem introduced by Dasgupta that has integrality gap Op ? log nq and a spreading metric LP relaxation that yields an Oplog nq-approximation to the problem.\nOn hierarchical clustering more broadly. There is an extensive literature on hierarchical clustering and its applications. It will be impossible to discuss most of it here; for some applications the reader may refer to e.g., (Jardine and Sibson, 1972; Sneath and Sokal, 1962; Felsenstein and Felenstein, 2004; Castro et al., 2004). Algorithms for hierarchical clustering have received a lot of attention from a practical perspective. For a definition and overview of agglomerative algorithms (such as average-linkage, complete-linkage, and single-linkage) see e.g., (Friedman et al., 2001) and for divisive algorithms see e.g., Steinbach et al. (2000).\nMost previous theoretical work on hierarchical clustering aimed at evaluating the cluster tree output by the linkage algorithms using the traditional objective functions for partition-based clustering, e.g., considering k-median or k-means cost of the clusters induced by the top levels of the\ntree (see e.g., (Plaxton, 2003; Dasgupta and Long, 2005; Lin et al., 2006)). Previous work also proved that average-linkage can be useful to recover an underlying partition-based clustering when it exists under certain stability conditions (see (Balcan et al., 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions.\nIn Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously. This approach was applied to hierarchical clustering algorithms by Carlsson and Mémoli (2010) who showed that in the case of hierarchical clustering one gets a positive result, unlike the impossibility result of Kleinberg. Their focus was on finding an ultrametric (on the datapoints) that is the closest to the metric (in which the data lies) in terms of the Gromov-Hausdorf distance. Our approach is completely different as we focus on defining objective functions and use these for quantitative analyses of algorithms.\nOur condition for inputs to have a ground-truth cluster tree, and especially their δ-adversarially perturbed versions, can be to be in the same spirit as that of the stability condition of Bilu and Linial (2012) or Bilu et al. (2013): the input induces a natural clustering to be recovered whose cost is optimal. It bears some similarities with the “strict separation” condition of Balcan et al. (2008), while we do not require the separation to be strict, we do require some additional hierarchical constraints. There are a variety of stability conditions that aim at capturing some of the structure that realworld inputs may exhibit (see e.g., (Awasthi et al., 2012; Balcan et al., 2013, 2008; Ostrovsky et al., 2012)). Some of them induce a condition under which an underlying clustering can be mostly recovered (see e.g., (Bilu and Linial, 2012; Balcan et al., 2009a, 2013) for deterministic conditions and e.g., Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al. (2009b) for probabilistic conditions). Imposing other conditions allows one to bypass hardness-of-approximation results for classical clustering objectives (such as k-means), and design efficient approximation algorithms (see, e.g., (Awasthi et al., 2010; Awasthi and Sheffet, 2012; Kumar and Kannan, 2010)). Eldridge et al. (2016) also investigate the question of understanding hierarchical cluster trees for random graphs generated from graphons. Their goal is quite different from ours—they consider the “single-linkage tree” obtained using the graphon as the ground-truth tree and investigate how a cluster tree that has low merge distortion with respect to this be obtained.6 This is quite different from the approach taken in our work which is primarily focused on understanding performance with respect to admssible cost functions."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Notation",
      "text" : "An undirected weighted graph G “ pV,E,wq is defined by a finite set of vertices V , a set of edges E Ď ttu, vu | u, v P V u and a weight function w : E Ñ R`, where R` denotes non-negative real numbers. We will only consider graphs with positive weights in this paper. To simplify notation (and since the graphs are undirected) we let wpu, vq “ wpv, uq “ wptu, vuq. When the weights on the edges are not pertinent, we simply denote graphs as G “ pV,Eq. When G is clear from the context, we denote |V | by n and |E| by m. We define GrU s to be the subgraph induced by the nodes of U .\n6This is a simplistic characterization of their work. However, a more precise characterization would require introducing a lot of terminology from their paper, which is not required in this paper.\nA cluster tree or hierarchical clustering T for graph G is a rooted binary tree with exactly |V | leaves, each of which is labeled by a distinct vertex v P V .7 Given a graph G “ pV,Eq and a cluster tree T for G, for nodes u, v P V we denote by LCAT pu, vq the lowest common ancestor (furthest from the root) of u and v in T .\nFor any internal node N of T , we denote the subtree of T rooted at N by TN . 8 Moreover, for any node N of T , define V pNq to be the set of leaves of the subtree rooted at N . Additionally, for any two trees T1, T2, define the union of T1, T2 to be the tree whose root has two children C1, C2 such that the subtree rooted at C1 is T1 and the subtree rooted at C2 is T2. Finally, given a weighted graph G “ pV,E,wq, for any set of vertices A Ď V , let wpAq “ř a,bPAwpa, bq and for any set of edges E0, let wpE0q “ ř ePE0\nwpeq. Finally, for any sets of vertices A,B Ď V , let wpA,Bq “ řaPA,bPB wpa, bq."
    }, {
      "heading" : "2.2 Ultrametrics",
      "text" : "Definition 2.1 (Ultrametric). A metric space pX, dq is an ultrametric if for every x, y, z P X, dpx, yq ď maxtdpx, zq, dpy, zqu.\nSimilarity Graphs Generated from Ultrametrics\nWe say that a weighted graph G “ pV,E,wq is a similarity graph generated from an ultrametric, if there exists an ultrametric pX, dq, such that V Ď X, and for every x, y P V, x ‰ y, e “ tx, yu exists, and wpeq “ fpdpx, yqq, where f : R` Ñ R` is a non-increasing function.9\nDissimilarity Graphs Generated from Ultrametrics\nWe say that a weighted graph G “ pV,E,wq is a dissimilarity graph generated from an ultrametric, if there exists an ultrametric pX, dq, such that V Ď X, and for every x, y P V, x ‰ y, e “ tx, yu exists, and wpeq “ fpdpx, yqq, where f : R` Ñ R` is a non-decreasing function.\nMinimal Generating Ultrametric\nFor a weighted undirected graph G “ pV,E,wq generated from an ultrametric (either similarity or dissimilarity), in general there may be several ultrametrics and the corresponding function f mapping distances in the ultrametric to weights on the edges, that generate the same graph. It is useful to introduce the notion of a minimal ultrametric that generates G.\nWe focus on similarity graphs here; the notion of minimal generating ultrametric for dissimilarity graphs is easily obtained by suitable modifications. Let pX, dq be an ultrametric that generates G “ pV,E,wq and f the corresponding function mapping distances to similarities. Then we consider the ultrametric pV, rdq defined as follows: (i) rdpu, uq “ 0 and (ii) for u ‰ v,\nrdpu, vq “ rdpv, uq “ max u1,v1 tdpu1, v1q | fpdpu1, v1qq “ fpdpu, vqqu (1)\nIt remains to be seen that pV, rdq is indeed an ultrametric. First, notice that by definition, rdpu, vq ě dpu, vq and hence clearly rdpu, vq “ 0 if and only if u “ v as d is the distance in an ultrametric.\n7In general, one can look at trees that are not binary. However, it is common practice to use binary trees in the context of hierarchical trees. Also, for results presented in this paper nothing is gained by considering trees that are not binary.\n8For any tree T , when we refer to a subtree T 1 (of T ) rooted at a node N , we mean the connected subgraph containing all the leaves of T that are descendant of N .\n9In some cases, we will say that e “ tx, yu R E, if wpeq “ 0. This is fine as long as fpdpx, yqq “ 0.\nThe fact that rd is symmetric is immediate from the definition. The only part remaining to check is the so called isosceles triangles with longer equal sides conditions—the ultrametric requirement that for any u, v, w, dpu, vq ď maxtdpu,wq, dpv,wqu implies that all triangles are isosceles and the two sides that are equal are at least as large as the third side. Let u, v, w P V , and assume without loss of generality that according to the distance d of pV, dq, dpu,wq “ dpv,wq ě dpu, vq. From (1) it is clear that rdpu,wq “ rdpv,wq ě dpu,wq. Also, from (1) and the non-increasing nature of f it is clear that if dpu, vq ď dpu1, v1q, then rdpu, vq ď rdpu1, v1q. Thence, pV, rdq is an ultrametric. The advantage of considering the minimal ultrametric is the following: if D “ trdpu, vq | u, v P V, u ‰ vu and W “ twpu, vq | u, v P V, u ‰ vu, then the restriction of f from D Ñ W is actually a bijection. This allows the notion of a generating tree to be defined in terms of distances in the ultrametric or weights, without any ambiguity. Applying an analogous definition and reasoning yields a similar notion for the dissimilarity case.\nDefinition 2.2 (Generating Tree). Let G “ pV,E,wq be a graph generated by a minimal ultrametric pV, dq (either a similarity or dissimilarity graph). Let T be a rooted binary tree with |V | leaves and |V | ´ 1 internal nodes; let N denote the internal nodes and L the set of leaves of T and let σ : L Ñ V denote a bijection between the leaves of T and nodes of V . We say that T is a generating tree for G, if there exists a weight function W : N Ñ R`, such that for N1, N2 P N , if N1 appears on the path from N2 to the root, W pN1q ď W pN2q. Moreover for every x, y P V , wptx, yuq “ W pLCAT pσ´1pxq, σ´1pyqqq.\nThe notion of a generating tree defined above more or less corresponds to what is referred to as a dendogram in the machine learning literature (see e.g., (Carlsson and Mémoli, 2010)). More formally, a dendogram is a rooted tree (not necessarily binary), where the leaves represent the datapoints. Every internal node in the tree has associated with it a height function h which is the distance between any pairs of datapoints for which it is the least common ancestor. It is a well-known fact that a set of points in an ultrametric can be represented using a dendogram (see e.g., (Carlsson and Mémoli, 2010)). A dendogram can easily be modified to obtain a generating tree in the sense of Definition 2.2: an internal node with k children is replace by an arbitrary binary tree with k leaves and the children of the nodes in the dendogram are attached to these k leaves. The height h of this node is used to give the weight W “ fphq to all the k´ 1 internal nodes added when replacing this node. Figure 1 shows this transformation.\nGround-Truth Inputs\nDefinition 2.3 (Ground-Truth Input.). We say that a graph G is a ground-truth input if it is a similarity or dissimilarity graph generated from an ultrametric. Equivalently, there exists a tree T\nthat is generating for G.\nMotivation. We briefly describe the motivation for defining graphs generated from an ultrametric as ground-truth inputs. We’ll focus the discussion on similarity graphs, though essentially the same logic holds for dissimilarity graphs. As described earlier, there is a natural notion of a generating tree associated with graphs generated from ultrametrics. This tree itself can be viewed as a cluster tree. The clusters obtained using the generating tree have the property that any two nodes in the same cluster are at least as similar to each other as they are to points outside this cluster; and this holds at every level of granularity. Furthermore, as observed by Carlsson and Mémoli (2010), many practical hierarchical clustering algorithms such as the linkage based algorithms, actually output a dendogram equipped with a height function, that corresponds to an ultrametric embedding of the data. While their work focuses on algorithms that find embeddings in ultrametrics, our work focuses on finding cluster trees. We remark that these problems are related but also quite different.\nFurthermore, our results show that the linkage algorithms (and some other practical algorithms), recover a generating tree when given as input graphs that are generated from an ultrametric. Finally, we remark that relaxing the notion further leads to instances where it is hard to define a ‘natural’ ground-truth tree. Consider a similarity graph generated by a tree-metric rather than an ultrametric, where the tree is the caterpillar graph on 5 nodes (see Fig. 2(a)). Then, it is hard to argue that the tree shown in Fig. 2(b) is not a more suitable cluster tree. For instance, D and E are more similar to each other than D is to B or A. In fact, it is not hard to show that by choosing a suitable function f mapping distances from this tree metric to similarities, Dasgupta’s objective function is minimized by the tree shown in Fig. 2(b), rather than the ‘generating’ tree in Fig. 2(a)."
    }, {
      "heading" : "3 Quantifying Output Value: An Axiomatic Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Admissible Cost Functions",
      "text" : "Let us focus on the similarity case; in this case we use cost and objective interchangeably. Let G “ pV,E,wq be an undirected weighted graph and let T be a cluster tree for graph G. We want to consider cost functions for cluster trees that capture the quality of the hierarchical clustering produced by T . Following the recent work of Dasgupta (2016), we adopt an approach in which a cost is assigned to each internal node of the tree T that corresponds to the quality of the split at that node.\nThe Axiom. A natural property we would like the cost function to satisfy is that a cluster tree T has minimum cost if and only if T is a generating tree for G. Indeed, the objective function can then be used to indicate whether a given tree is generating and so, whether it is an underlying ground-truth hierarchical clustering. Hence, the objective function acts as a “guide” for finding the correct hierarchical classification. Note that there may be multiple trees that are generating for the same graph. For example, if G “ pV,E,wq is a clique with every edge having the same weight then every tree is a generating tree. In these cases, all the generating tree are valid ground-truth hierarchical clusterings.\nFollowing Dasgupta (2016), we restrict the search space for such cost functions. For an internal node N in a clustering tree T , let A,B Ď V be the leaves of the subtrees rooted at the left and right child of N respectively. We define the cost Γ of the tree T as the sum of the cost at every internal node N in the tree, and at an individual node N we consider cost functions γ of the form\nΓpT q “ ÿ\nN\nγpNq, (2)\nγpNq “ ˜ ÿ\nxPA,yPB\nwpx, yq ¸ ¨ gp|A|, |B|q (3)\nWe remark that Dasgupta (2016) defined gpa, bq “ a` b.\nDefinition 3.1 (Admissible Cost Function). We say that a cost function γ of the form (2,3) is admissible if it satisfies the condition that for all similarity graphs G “ pV,E,wq generated from a minimal ultrametric pV, dq, a cluster tree T for G achieves the minimum cost if and only if it is a generating tree for G.\nRemark 3.2. Analogously, for the dissimilarity setting we define admissible value functions to be the functions of the form (2,3) that satisfy: for all dissimilarity graph G generated from a minimal ultrametric pV, dq, a cluster tree T for G achieves the maximum value if and only if it is a generating tree for G.\nRemark 3.3. The RHS of (3) has linear dependence on the weight of the cut pA,Bq in the subgraph of G induced by the vertex set AYB as well as on an arbitrary function of the number of leaves in the subtrees of the left and right child of the internal node creating the cut pA,Bq. For the purpose of hierarchical clustering this form is fairly natural and indeed includes the specific cost function introduced by Dasgupta (2016). We could define the notion of admissibility for other forms of the cost function similarly and it would be of interest to understand whether they have properties that are desirable from the point of view of hierarchical clustering."
    }, {
      "heading" : "3.2 Characterizing Admissible Cost Functions",
      "text" : "In this section, we give an almost complete characterization of admissible cost functions of the form (3). The following theorem shows that cost functions of this form are admissible if and only if they satisfy three conditions: that all cliques must have the same cost, symmetry and monotonicity.\nTheorem 3.4. Let γ be a cost function of the form (3) and let g be the corresponding function used to define γ. Then γ is admissible if and only if it satisfies the following three conditions.\n1. Let G “ pV,E,wq be a clique, i.e., for every x, y P V , e “ tx, yu P E and wpeq “ 1 for every e P E. Then the cost ΓpT q for every cluster tree T of G is identical.\n2. For every n1, n2 P N, gpn1, n2q “ gpn2, n1q.\n3. For every n1, n2 P N, gpn1 ` 1, n2q ą gpn1, n2q.\nProof. We first prove the only if part and then the if part.\nOnly If Part: Suppose that γ is indeed an admissible cost function. We prove that all three conditions must be satisfied by γ. 1. All cliques have same cost. We observe that a clique G “ pV,E,wq can be generated from an ultrametric. Indeed, let X “ V and let dpu, vq “ dpv, uq “ 1 for every u, v P X such that u ‰ v and dpu, uq “ 0. Clearly, for f : R` Ñ R` that is non-increasing and satisfying fp1q “ 1, pV, dq is a minimal ultrametric generating G.\nLet T be any binary rooted tree with leaves labeled by V , i.e., a cluster tree for graph G. For any internal node N of T define W pNq “ 1 as the weight function. This satisfies the definition of generating tree (Defn. 2.2). Thus, every cluster tree T for G is generating and hence, by the definition of admissibility all of them must be optimal, i.e., they all must have exactly the same cost. 2. gpn1, n2q “ gpn2, n1q. This part follows more or less directly from the previous part. Let G be a clique on n1 ` n2 nodes. Let T be any cluster tree for G, with subtrees T1 and T2 rooted at the left and right child of the root respectively, such that T1 contains n1 leaves and T2 contains n2 leaves. The number of edges, and hence the total weight of the edges, crossing the cut induced by the root node of T is n1 ¨ n2. Let rT be a tree obtained by making T2 be rooted at the left child of the root and T1 at the right child. Clearly rT is also a cluster tree for G and induces the same cut at the root node, hence using the property that all cliques have the same cost, ΓpT q “ Γp rT q. But ΓpT q “ n1 ¨ n2 ¨ gpn1, n2q ` ΓpT1q ` ΓpT2q and Γp rT q “ n1 ¨ n2 ¨ gpn2, n1q ` ΓpT1q ` ΓpT2q. Thence, gpn1, n2q “ gpn2, n1q. 3. gpn1`1, n2q ą gpn1, n2q. Consider a graph on n1`n2`1 nodes generated from an ultrametric as follows. Let V1 “ tv1, . . . , vn1u, V2 “ tv11, . . . , v1n2u and consider the ultrametric pV1 Y V2 Y tv˚u, dq defined by dpx, yq “ 1 if x ‰ y and x, y P V1 or x, y P V2, dpx, yq “ 2 if x ‰ y and x P V1, y P V2 or x P V2, y P V1, dpv˚, xq “ dpx, v˚q “ 3 for x P V1 YV2, and dpu, uq “ 0 for u P V1 YV2 Ytv˚u. It can be checked easily by enumeration that this is indeed an ultrametric. Furthermore, if f : R` Ñ R` is non-increasing and satisfies fp1q “ 2, fp2q “ 1 and fp3q “ 0, i.e., wptu, vuq “ 2 if u and v are both either in V1 or V2, wptu, vuq “ 1 if u P V1 and v P V2 or the other way around, and wptv˚, uuq “ 0 for u P V1 Y V2, then pV1 Y V2, tv˚u, dq is a minimal ultrametric generating G.\nNow consider two possible cluster trees defined as follows: Let T1 be an arbitrary tree on nodes V1, T2 and arbitrary tree on nodes V2. T is obtained by first joining T1 and T2 using internal node N and making this the left subtree of the root node ρ and the right subtree of the root node is just the singleton node v˚. T 1 is obtained by first creating a tree by joining T1 and the singleton node v˚ using internal node N 1, this is the left subtree of the root node ρ1 and T2 is the right subtree of the root node. (See Figures 3a and 3b.)\nNow it can be checked that T is generating by defining the following weight function. For every internal node M of T1, let W pMq “ 1, similarly for every internal node M of T2, let W pMq “ 1, define W pNq “ 2 and W pρq “ 3. Now, we claim that T 1 cannot be a generating tree. This follows from the fact that for a node u P V1, v P V2, the root node ρ1 “ LCAT 1pu, vq, but it is also the case that ρ1 “ LCAT 1pv˚, vq. Thus, it cannot possibly be the case that W pρq “ wptu, vuq and W pρq “ wptv˚, vuq as wptu, vuq ‰ wptv˚, vuq. By definition of admissibility, it follows that ΓpT q ă ΓpT 1q, but ΓpT q “ ΓpT1q ` ΓpT2q ` n1 ¨ n2 ¨ gpn1, n2q. The last term arises from the cut at node N ; the root makes no contribution as the cut at the root node ρ has weight 0. On the other hand ΓpT 1q “ ΓpT1q ` ΓpT2q ` n1 ¨ n2 ¨ gpn1 ` 1, n2q. There is no cost at the node N 1, since the cut\nhas size 0; however, at the root node the cost is now n1 ¨ n2 ¨ gpn1 ` 1, n2q as the left subtree at the root contains n1 ` 1 nodes. It follows that gpn1 ` 1, n2q ą gpn1, n2q. If Part: For the other direction, we first use the following observation. By condition 2 in the statement of the theorem, every clique on n nodes has the same cost irrespective of the tree used for hierarchical clustering; let κpnq denote said cost. Let n1, n2 ě 1, then we have,\nn1 ¨ n2 ¨ gpn1, n2q “ κpn1 ` n2q ´ κpn1q ´ κpn2q (4)\nWe will complete the proof by induction on |V |. The minimum number of nodes required to have a cluster tree with at least one internal node is 2. Suppose |V | “ 2, then there is a unique (up to interchanging left and right children) cluster tree; this tree is also generating and hence by definition any cost function is admissible. Thus, the base case is covered rather easily.\nNow, consider a graph G “ pV,E,wq with |V | “ n ą 2. Let T ˚ be a tree that is generating. Suppose that T is any other tree. Let ρ˚ and ρ be the root nodes of the trees respectively. Let V ˚L and V ˚R be the nodes on the left subtree and right subtree of ρ\n˚; similarly VL and VR in the case of ρ. Let A “ V ˚L X VL, B “ V ˚L X VR, C “ V ˚R X VL, D “ V ˚R X VR. Let a, b, c and d denote the sizes of A, B, C and D respectively.\nWe will consider the case when all of a, b, c, d ą 0; the proof is similar and simpler in case some of them are 0. Let rT be a tree with root rρ that has the following structure: Both children of the root are internal nodes, all of A appears as leaves in the left subtree of the left child of the root, B as leaves in the right subtree of the left child of the root, C as leaves in the left subtree of the right child of the root and D as leaves in the right subtree of the right child of the root. We assume that all four subtrees for the sets A, B, C, D are generating and hence by induction optimal. We claim that the cost of rT is at least as much as the cost of T ˚. To see this note that V ˚L “ AYB. Thus, the left subtree of ρ˚ is optimal for the set V ˚L (by induction), whereas that of rρ may or may not be. Similarly for all the nodes in V ˚R . The only other thing left to account for is the cost at the root. But since ρ˚ and rρ induce exactly the same cut on V , the cost at the root is the same. Thus, ΓprT q ě ΓpT ˚q. Furthermore, equality holds if and only if rT is also generating for G.\nLet W ˚ denote the weight function for the generating tree T ˚ such that for all u, v P V , W ˚pLCAT˚pu, vqq “ wptu, vuq. Let ρ˚L and ρ˚R denote the left and right children of the root ρ˚ of T ˚. For all ua P A, ub P B, wptua, ubuq ě W ˚pρ˚Lq. Let\nx “ 1 ab\nÿ\nuaPA,ubPB\nwptua, ubuq\ndenote the average weight of the edges going between A and B; it follows that x ě W ˚pρ˚Lq. Similarly for all uc P C, ud P D, wptuc, uduq ě W ˚pρ˚Rq. Let\ny “ 1 cd\nÿ\nucPC,udPD\nwptuc, uduq\ndenote the average weight of the edges going between C and D; it follows that y ě W ˚pρ˚Rq. Finally for every u P A Y B,u1 P C Y D, wptu, u1uq “ W ˚pρ˚q; denote this common value by z. By the definition of generating tree, we know that x ě z and y ě z.\nNow consider the tree T . Let TL and TR denote the left and right subtrees of ρ. By induction, it must be that TL splits A and C as the first cut (or at least that’s one possible tree, if multiple cuts exist), similarly TR first cuts B and D. Both, T and rT have subtrees containing only nodes from A, B, C and D. The costs for these subtrees are identical in both cases (by induction). Thus, we have\nΓpT q ´ ΓprT q “ zac ¨ gpa, cq ` zbd ¨ gpb, dq ` pxab` ycd` zpad` bcqq ¨ gpa` c, b` dq ´ xab ¨ gpa, bq ` y ¨ cdgpc, dq ´ zpa` bqpc` dq ¨ gpa` b, c` dq\n“ px´ zqabpgpa ` c, b` dq ´ gpa, bqq ` py ´ zqcdpgpa ` c, b` dq ´ gpc, dqq ` zppa` cqpb ` dq ¨ gpa` c, b` dq ` ac ¨ gpa, cq ` bd ¨ gpb, dqq ´ zppa` bqpc ` dq ¨ gpa` b, c` dq ` ab ¨ gpa, bq ` cd ¨ gpc, dqq\nUsing (4), we get that the last two expressions above both evaluate to zpκpa` b` c` dq ´ κpaq ´ κpbq ´ κpcq ´ κpdqq, but have opposite signs. Thus, we get\nΓpT q ´ ΓprT q “ px´ zqabpgpa ` c, b` dq ´ gpa, bqq ` py ´ zqcdpgpa ` c, b` dq ´ gpc, dqq\nIt is clear that the above expression is always non-negative and is 0 if and only if x “ z and y “ z. If it is the latter case and it is also the case that Γp rT q “ ΓpT ˚q, then it must actually be the case that T is a generating tree."
    }, {
      "heading" : "3.2.1 Characterizing g that satisfy conditions of Theorem 3.4",
      "text" : "Theorem 3.4 give necessary and sufficient conditions on g for cost functions of the form (3) be admissible. However, it leaves open the question of the existence of functions satisfying the criteria and also characterizing the functions g themselves. The fact that such functions exist already follows from the work of Dasgupta (2016), who showed that if gpn1, n2q “ n1 ` n2, then all cliques have the same cost. Clearly, g is monotone and symmetric and thus satisfies the condition of Theorem 3.4.\nIn order to give a more complete characterization, we define g as follows: Suppose gp¨, ¨q is symmetric, we define gpn, 1q for all n ě 1 so that gpn, 1q{pn ` 1q is non-decreasing.10 We consider a particular cluster tree for a clique that is defined using a caterpillar graph, i.e., a cluster tree where the right child of any internal node is a leaf labeled by one of the nodes of G and the left child is another internal node, except at the very bottom. Figure 4 shows a caterpillar cluster tree for a clique on 4 nodes. The cost of the clique on n nodes, say κpnq, using this cluster tree is given by\nκpnq “ n´1ÿ\ni“0\ni ¨ gpi, 1q\n10The function proposed by Dasgupta (2016) is gpn, 1q “ n` 1, so this ratio is always 1.\nNow, we enforce the condition that all cliques have the same cost by defining gpn1, n2q for n1, n2 ą 1 suitably, in particular,\ngpn1, n2q “ κpn1 ` n2q ´ κpn1q ´ κpn2q\nn1 ¨ n2 (5)\nThus it only remains to be shown that g is strictly increasing. We show that for n2 ď n1, gpn1 ` 1, n2q ą gpn1, n2q. In order to show this it suffices to show that,\nn1pκpn1 ` n2 ` 1q ´ κpn1 ` 1q ´ κpn2qq ´ pn1 ` 1qpκpn1 ` n2q ´ κpn1q ´ κpn2qq ą 0\nThus, consider\nn1pκpn1 ` n2 ` 1q ´ κpn1 ` 1q ´ κpn2qq ´ pn1 ` 1qpκpn1 ` n2q ´ κpn1q ´ κpn2qq “ n1pκpn1 ` n2 ` 1q ´ κpn1 ` n2q ´ κp1q ´ κpn1 ` 1q ` κpn1q ` κp1qq ´ pκpn1 ` n2q ´ κpn1q ´ κpn2qq “ n1pn1 ` n2qgpn1 ` n2, 1q ´ n21gpn1, 1q ´ pκpn1 ` n2q ´ κpn1q ´ κpn2qq\ně n1pn1 ` n2qgpn1 ` n2, 1q ´ n21gpn1, 1q ´ n1`n2´1ÿ\ni“n1\ni ¨ gpi, 1q\ně gpn1 ` n2, 1q n1 ` n2 ` 1\n¨ ˜ n1pn1 ` n2qpn1 ` n2 ` 1q ´ n21pn1 ` 1q ´ n1`n2´1ÿ\ni“n1\nipi ` 1q ¸ ą 0\nAbove we used the fact that gpn, 1q{pn`1q is non-decreasing in n and some elementary calculations. This shows that the objective function proposed by Dasgupta (2016) is by no means unique. Only in the last step, do we get an inequality where we use the condition that gpn, 1q{pn ` 1q is increasing. Whether this requirement can be relaxed further is also an interesting direction."
    }, {
      "heading" : "3.2.2 Characterizing Objective Functions for Dissimilarity Graphs",
      "text" : "When the weights of the edges represent dissimilarities instead of similarities, one can consider objective functions of the same form as (3). As mentioned in Remark 3.2, the difference in this case is that the goal is to maximize the objective function and hence the definition of admissibility now requires that generating trees have a value of the objective that is strictly larger than any tree that is not generating.\nThe characterization of admissible objective functions as given in Theorem 3.4 for the similarity case continues to hold in the case of dissimilarities. The proof follows in the same manner by appropriately switching the direction of the inequalities when required."
    }, {
      "heading" : "4 Similarity-Based Inputs: Approximation Algorithms",
      "text" : "In this section, we analyze the recursive φ-sparsest-cut algorithm (see Algorithm 1) that was described previously in Dasgupta (2016). For clarity, we work with the cost function introduced by Dasgupta (2016): The goal is to find a tree T minimizing costpT q “ řNPT costpNq where for each node N of T with children N1, N2, costpNq “ wpV pN1q, V pN2qq ¨ V pNq. We show that the φ-sparsest-cut algorithm achieves a 6.75φ-approximation. (Charikar and Chatziafratis (2017) also proved an Opφq approximation for Dasgupta’s function.) Our proof also yields an approximation guarantee not just for Dasgupta’s cost function but more generally for any admissible cost function, but the approximation ratio depends on the cost function.\nThe φ-sparsest-cut algorithm (Algorithm 1) constructs a binary tree top-down by recursively finding cuts using a φ-approximate sparsest cut algorithm, where the sparsest-cut problem asks for a set A minimizing the sparsity wpA,V zAq{p|A||V zA|q of the cut pA,V zAq.\nAlgorithm 1 Recursive φ-Sparsest-Cut Algorithm for Hierarchical Clustering\n1: Input: An edge weighted graph G “ pV,E,wq. 2: tA,V zAu Ð cut with sparsity ď φ ¨ min\nSĂV wpS, V zSq{p|S||V zS|q\n3: Recurse on GrAs and on GrV zAs to obtain trees TA and TV zA 4: return the tree whose root has two children, TA and TV zA.\nTheorem 4.1. 11 For any graph G “ pV,Eq, and weight function w : E Ñ R`, the φ-sparsest-cut algorithm (Algorithm 1) outputs a solution of cost at most 27\n4 φOPT.\nProof. Let G “ pV,Eq be the input graph and n denote the total number of vertices of G. Let T denote the tree output by the algorithm and T ˚ be any arbitrary tree. We will prove that costpT q ď 27\n4 φcostpT ˚q. 12\nRecall that for an arbitrary tree T0 and node N of T0, the vertices corresponding to the leaves of the subtree rooted at N is denoted by V pNq. Consider the node N0 of T ˚ that is the first node reached by the walk from the root that always goes to the child tree with the higher number of leaves, stopping when the subtree of T ˚ rooted at N0 contains fewer than 2n{3 leaves. The balanced cut (BC) of T ˚ is the cut pV pN0q, V ´ V pN0qq. For a given node N with children N1, N2, we say that the cut induced by N is the sum of the weights of the edges between that have one extremity in V pN1q and the other in V pN2q.\nLet pAYC,BYDq be the cut induced by the root node u of T , where A,B,C,D are such that pAYB,C YDq is the balanced cut of T ˚. Since pAY C,B YDq is a φ-approximate sparsest cut:\nwpA Y C,B YDq |A YC| ¨ |B YD| ď φ wpA YB,C YDq |AYB| ¨ |C YD| .\nBy definition of N0, A Y B and C YD both have size in rn{3, 2n{3s, so the product of their sizes is at least pn{3qp2n{3q “ 2n2{9; developing wpAYB,C YDq into four terms, we obtain\nwpAY C,B YDq ď φ 9 2n2 |AY C||B YD|pwpA,Cq ` wpA,Dq ` wpB,Cq `wpB,Dqq\nď φ9 2 r |B YD| n wpA,Cq ` wpA,Dq `wpB,Cq ` |AY C| n wpB,Dqs,\n11For Dasgupta’s function, this was already proved in Charikar and Chatziafratis (2017) with a different constant. The present, independent proof, uses a different method.\n12The following paragraph bears similarities with the first part of the analysis of (Dasgupta, 2016, Lemma 11) but we obtain a more fine-grained analysis by introducing a charging scheme.\nand so the cost induced by node u of T ˚ satisfies\nn ¨ wpAY C,B YDq ď 9 2 φ|B YD|wpA,Cq ` 9 2 φ|AYC|wpB,Dq ` 9 2 φnpwpA,Dq ` wpB,Cqq.\nTo account for the cost induced by u, we thus assign a charge of p9{2qφ|B YD|wpeq to each edge e of pA,Cq, a charge of p9{2qφ|A Y C|wpeq to each edge e of pB,Dq, and a charge of p9{2qφnwpeq to each edge e of pA,Dq or pB,Cq.\nWhen we do this for every node u of T , how much does each edge get charged?\nLemma 4.2. Let G “ pV,Eq be a graph on n nodes. We consider the above charging scheme for T and T ˚. Then, an edge pv1, v2q P E gets charged at most p9{2qφminpp3{2q|V pLCAT˚pv1, v2qq|, nqwpeq overall, where LCAT˚pv1, v2q denotes the lowest common ancestor of v1 and v2 in T ˚.\nWe temporarily defer the proof and first see how Lemma 4.2 implies the theorem. Observe (as in Dasgupta (2016)) that costpT ˚q “ řtu,vuPE |V pLCAT˚pu, vqq|wpu, vq. Thanks to Lemma 4.2, when we sum charges assigned because of every node N of T , overall we obtain\ncostpT q ď 9 2 φ\nÿ\ntv1,v2uPE\n3 2 |V pLCAT˚pv1, v2qq|wpv1, v2q “ 27 4 φcostpT ˚q.\nProof of Lemma 4.2. The lemma is proved by induction on the number of nodes of the graph. (The base case is obvious.) For the inductive step, consider the cut pAYC,B YDq induced by the root node u of T .\n• Consider the edges that cross the cut. First, observe that edges of pA,Bq or of pC,Dq never get charged at all. Second, an edge e “ tv1, v2u of pA,Dq or of pB,Cq gets charged p9{2qφnwpeq when considering the cost induced by node u, and does not get charged when considering any other node of T . In T ˚, edge e is separated by the cut pAYB,C YDq induced by N0, so the least common ancestor of v1 and v2 is the parent node of N0 (or above), and by definition of N0 we have |V pLCAT˚pv1, v2qq| ě 2n{3, hence the lemma holds for e.\n• An edge e “ tv1, v2u of GrAsYGrCs does not get charged when considering the cut induced by node u. Apply Lemma 4.2 to GrAYCs for the tree T ˚AYC defined as the subtree of T ˚ induced by the vertices of A Y C13. By induction, the overall charge to e due to the recursive calls for GrAYCs is at most p9{2qφminpp3{2q|V pLCAT˚ AYC pv1, v2qq|, |AYC|qwpeq. By definition of\nT ˚AYC , we have |V pLCAT˚AYC pv1, v2qq| ď |V pLCAT˚pv1, v2qq|, and |A Y C| ď n, so the lemma holds for e.\n• An edge tv1, v2u of pA,Cq gets a charge of p9{2qφ|BYD|wpeq plus the total charge to e coming from the recursive calls for GrAY Cs and the tree T ˚AYC . By induction the latter is at most\np9{2qφminpp3{2q|V pLCAT˚ AYC pv1, v2qq|, |A Y C|qwpeq ď p9{2qφ|A Y C|wpeq.\nOverall the charge to e is at most p9{2qφnwpeq. Since the cut induced by node u0 of T ˚ separates v1 from v2, we have |V pLCAT˚pv1, v2qq| ě 2n{3, hence the lemma holds for e. For edges of pB,Dq or of GrBs YGrDs, a symmetrical argument applies.\n13note that T˚AYC is not necessarily the optimal tree for GrAYCs, which is why the lemma was stated in terms of every tree T˚, not just on the optimal tree.\nRemark 4.3. The recursive φ-sparsest-cut algorithm achieves an Opfnφq-approximation for any admissible cost function f , where fn “ maxn fpnq{fprn{3sq. Indeed, adapting the definition of the balanced cut as in Dasgupta (2016) and rescaling the charge by a factor of fn imply the result.\nWe complete our study of classical algorithms for hierarchical clustering by showing that the standard agglomerative heuristics can perform poorly (Theorems 8.1, 8.3). Thus, the sparsest-cutbased approach seems to be more reliable in the worst-case. To understand better the success of the agglomerative heuristics, we restrict our attention to ground-truth inputs (Section 7), and random graphs (Section 5), and show that in these contexts these algorithms are efficient."
    }, {
      "heading" : "5 Admissible Objective Functions and Algorithms for Random",
      "text" : "Inputs\nIn this section, we initiate a beyond-worst-case analysis of the hierarchical clustering problem (see also Section 7.3). We study admissible objective functions in the context of random graphs that have a natural hierarchical structure; for this purpose, we consider a suitable generalization of the stochastic block model to hierarchical clustering.\nWe show that, for admissible cost functions, an underlying ground-truth cluster tree has optimal expected cost. Additionally, for a subfamily of admissible cost functions (called smooth, see Defn. 5.4) which includes the cost function introduced by Dasgupta, we show the following: The cost of the ground-truth cluster tree is with high probability sharply concentrated (up to a factor of p1`op1qq around its expectation), and so of cost at most p1`op1qqOPT. This is further evidence that optimising admissible cost functions is an appropriate strategy for hierarchical clustering.\nWe also provide a simple algorithm based on the SVD based approach of McSherry (2001) followed by a standard agglomerative heuristic yields a hierarchical clustering which is, up to a factor p1` op1qq, optimal with respect to smooth admissible cost functions."
    }, {
      "heading" : "5.1 A Random Graph Model For Hierarchical Clustering",
      "text" : "We describe the random graph model for hierarchical clustering, called the hierarchical block model. This model has already been studied earlier, e.g., Lyzinski et al. (2017). However, prior work has mostly focused on statistical hypothesis testing and exact recovery in some regimes. We will focus on understanding the behaviour of admissible objective functions and algorithms to output cluster trees that have almost optimal cost in terms of the objective function.\nWe assume that there are k “bottom”-level clusters that are then arranged in a hierarchical fashion. In order to model this we will use a similarity graph on k nodes generated from an ultrametric (see Sec. 2.2). There are n1, . . . , nk nodes in each of the k clusters. Each edge is present in the graph with a probability that is a function of the clusters in which their endpoints lie and the underlying graph on k nodes generated from the ultrametric. The formal definition follows.\nDefinition 5.1 (Hierarchical Stochastic Block Model (HSBM)). A hierarchical stochastic block model with k bottom-level clusters is defined as follows:\n• Let rGk “ prVk, rEk, wq be a graph generated from an ultrametric (see Sec. 2.2), where |rVk| “ k\nfor each e P rEk, wpeq P p0, 1q.14 Let rTk be a tree on k leaves, let rN denote the internal nodes of rT and rL denote the leaves; let rσ : rL Ñ rks be a bijection. Let rT be generating for rGk with weight function ĂW : rN Ñ r0, 1q (see Defn. 2.2).\n• For each i P rks, let pi P p0, 1s be such that pi ą ĂW pNq, if N denotes the parent of rσ´1piq in rT . • For each i P rks, there is a fixed constant fi P p0, 1q; furthermore řk i“1 fi “ 1.\nThen a random graph G “ pV,Eq on n nodes with sparsity parameter αn P p0, 1s is defined as follows: pn1, . . . , nkq is drawn from the multinomial distribution with parameters pn, pf1, . . . , fkqq. Each vertex i P rns is assigned a label ψpiq P rks, so that exactly nj nodes are assigned the label j for j P rks. An edge pi, jq is added to the graph with probability αnpψpiq if ψpiq “ ψpjq and with probability αnĂW pNq if ψpiq ‰ ψpjq and N is the least common ancestor of rσ´1piq and rσ´1pjq in rT . The graph G “ pV,Eq is returned without any labels.\nAs the definition is rather long and technical, a few remarks are in order.\n• Rather than focusing on an arbitrary hierarchy on n nodes, we assume that there are k clusters (which exhibit no further hierarchy) and there is a hierarchy on these k clusters. The model assumes that k is fixed, but in future work, it may be interesting to study models where k itself may be a (modestly growing) function of n. The condition pi ą ĂW pNq (where N is the parent of rσ´1piq ) ensures that nodes in cluster i are strictly more likely to connect to each other than to node from any other cluster.\n• The graphs generated can be of various sparsity, depending on the parameter αn. If αn P p0, 1q is a fixed constant, we will get dense graphs (with Ωpn2q edges), however if αn Ñ 0 as n Ñ 8, sparser graphs may be achieved. This is similar to the approach taken by Wolfe and Olhede (2013) when considering random graph models generated according to graphons.\nWe define the expected graph, Ḡ, which is a complete graph where an edge pi, jq has weight pi,j where pi,j is the probability with which it appears in the random graph G. In order to avoid ambiguity, we denote by ΓpT ;Gq and ΓpT ; Ḡq the costs of the cluster tree T for the unweighted (random) graph G and weighted graph Ḡ respectively. Observe that due to linearity (see Eqns. (3) and (2)), for any tree T and any admissible cost function, ΓpT ; Ḡq “ E rΓpT ;Gq s, where the expectation is with respect to the random choices of edges in G (in particular this holds even when conditioning on n1, . . . , nk).\nFurthermore, note that Ḡ itself is generated from an ultrametric and the generating trees for Ḡ are obtained as follows: Let rTk be any generating tree for rGk, let T̂1, T̂2, . . . , T̂k be any binary trees with n1, . . . , nk leaves respectively. Let the weight of every internal node of T̂i be pi and replace each leaf l in rTk by T̂rσplq. In particular, this last point allows us to derive Proposition 5.3. We refer to any tree that is generating for the expected graph Ḡ as a ground-truth tree for G.\nRemark 5.2. Although it is technically possible to have ni “ 0 for some i under the model, we will assume in the rest of the section that ni ą 0 for each i. This avoids getting into the issue of degenerate ground-truth trees; those cases can be handled easily, but add no expository value.\n14In addition to rGk being generated from an ultrametric, we make the further assumption that the function f : R` Ñ R`, that maps ultrametric distances to edge weights, has range p0, 1q, so that the weight of an edge can be interpreted as a probability of an edge being present. We rule out wpeq “ 0 as in that case the graph is disconnected and each component can be treated separately."
    }, {
      "heading" : "5.2 Objective Functions and Ground-Truth Tree",
      "text" : "In this section, we assume that the graphs represent similarities. This is clearly more natural in the case of unweighted graphs; however, all our results hold in the dissimilarity setting and the proofs are essentially identical.\nProposition 5.3. Let Γ be an admissible cost function. Let G be a graph generated according to an HSBM (See Defn. 5.1). Let ψ be the (hidden) function mapping the nodes of G to rks (the bottom-level clusters). Let T be a ground-truth tree for G Then,\nE rΓpT q | ψ s ď min T 1\nE “ ΓpT 1q | ψ ‰ .\nMoreover, for any tree T 1, E rΓpT q | ψ s “ E rΓpT 1q | ψ s if and only if T 1 is a ground-truth tree.\nProof. As per Remark 5.2, we’ll assume that each ni ą 0 to avoid degenerate cases. Let Ḡ be a the expected graph, i.e., Ḡ is complete and an edge pi, jq has weight pij, the probability that the edge pi, jq is present in the random graph G generated according to the hierarchical model. Thus, by definition of admissibility ΓpT ; Ḡq “ minT 1 ΓpT 1; Ḡq if an only if T is generating (see Defn. 3.1). As ground-truth trees for G are precisely the generating trees for Ḡ; the result follows by observing that for any tree T (not necessarily ground-truth) E rΓpT ;Gq | ψ s “ ΓpT ; Ḡq, where the expectation is taken only over the random choice of the edges, by linearity of expectation and the definition of the cost function (Eqns. 3 and 2).\nDefinition 5.4. Let γ be a cost function defined using the function gp¨, ¨q (see Defn. 3.1). We say that the cost function Γ (as defined in Eqn. 2) satisfies the smoothness property if\ngmax :“ maxtgpn1, n2q | n1 ` n2 “ nu “ O ˆ κpnq n2 ˙ ,\nwhere κpnq is the cost of a unit-weight clique of size n under the cost function Γ.\nFact 5.5. The cost function introduced by Dasgupta (2016) satisfies the smoothness property. Theorem 5.6. Let αn “ ωp a\nlog n{nq. Let Γ be an admissible cost function satisfying the smoothness property (Defn. 5.4). Let k be a fixed constant and G be a graph generated from an HSBM (as per Defn. 5.1) where the underlying graph rGk has k nodes and the sparsity factor is αn. Let ψ be the (hidden) function mapping the nodes of G to rks (the bottom-level clusters). For any binary tree T with n leaves labelled by the vertices of G, the following holds with high probability:\n|ΓpT q ´ E rΓpT q | ψ s| ď opE rΓpT q | ψ sq.\nThe expectation is taken only over the random choice of edges. In particular if T ˚ is a ground-truth tree for G, then, with high probability,\nΓpT ˚q ď p1` op1qqmin T 1 ΓpT 1q “ p1` op1qqOPT.\nProof. Our goal is to show that for any fixed cluster tree T 1 the cost is sharply concentrated around its expectation with an extremely high probability. We then apply the union bound over all possible cluster trees and obtain that in particular the cost of OPT is sharply concentrated around its expectation. Note that there are at most 2c¨n logn possible cluster trees (including labellings of\nthe leaves to vertices of G), where c is a suitably large constant. Thus, it suffices to show that for any cluster tree T 1 we have\nP “ ˇ̌ ΓpT 1q ´ E “ ΓpT 1q | ψ ‰ˇ̌ ě opE “ ΓpT 1q | ψ ‰ q ‰ ď exp p´c˚n log nq ,\nwhere c˚ ą c. Recall that for a given node N of T 1 with children N1, N2, we have γpNq “ wpV pN1q, V pN2qq ¨\ngp|V pN1q|, |V pN2q|q and ΓpT 1q “ ř\nNPT 1 γpNq (see Eqns. (3) and (2)). Let Yi,j “ 1pi,jqPE for all 1 ď i, j ď n and observe that tYi,j|i ă ju are independent and Yi,j “ Yj,i. Furthermore, let Zi,j “ gp|V pchild1pN i,jqq|, |V pchild2pN i,jqq|q ¨ Yi,j, where N i,j is the node in T 1 separating nodes i and j and child1pN i,jq and child2pN i,jq are the two children of N i,j. We can thus write\nΓpT 1q “ ÿ\nNPT 1\ngp|V pchild1pNqq|, |V pchild2pNqq|q ÿ\niPV pchild1pNqq jPV pchild2pNqq\nYi,j (6)\n“ ÿ\nNPT 1\nÿ\niPV pchild1pNqq jPV pchild2pNqq\nZi,j (7)\n“ ÿ\niăj\nZi,j, (8)\nwhere we used that every potential edge i, j, i ‰ j appears in exactly one cut and that Zi,j “ Zj,i. Observe that ř iăj Zi,j is a sum of independent random variables. Assume that the following claim holds. Claim 5.7. Let wmin “ Ωp1q be the minimum weight in rTk, the tree generating tree for rGk (see Defn. 5.1), i.e., wmin “ minNP rTk\nĂW pNq) and recall that gmax “ maxtgpn1, n2q | n1 ` n2 “ nu. We have\n1. E rΓpT 1q | ψ s ě κpnq ¨ αn ¨ wmin 2. ř\niăj gp|V pchild1pN i,jqq|, |V pchild2pN i,jqq|q2 ď gmax ¨ κpnq\nWe defer the proof to later and first finish the proof of Theorem 5.6. We will make use of the slightly generalized version of Hoeffding bounds (see Hoeffding (1963)). For X1,X2, . . . ,Xm independent random variables satisfying ai ď Xi ď bi for i P rns. Let X “ řm i“1 Xi, then for any t ą 0\nP r |X ´ E rX s | ě t s ď exp ˆ ´ 2t 2\nřm i“1pbi ´ aiq2\n˙ . (9)\nBy assumption, there exists a function yn : N Ñ R` such that αn “ ω ˆ yn ¨ b logn n ˙ with yn “ ωp1q\nfor all n. We apply (9) with t “ E rΓpT 1q | ψ s ¨ yn¨ b log n n\nαn “ opE rΓpT 1q | ψ sq and derive\nP » – ˇ̌ΓpT 1q ´ E “ ΓpT 1q | ψ ‰ˇ̌ ď E “ ΓpT 1q | ψ ‰ ¨ yn ¨ b logn n\nαn\nfi fl ě\ně 1´ exp ¨ ˚̊ ˚̊ ˚̋´ 2 ˜ E rΓpT 1q | ψ s ¨ yn¨ b log n n αn ¸2\nř iăj gp|V pN i,j 1 q|, |V pN i,j 2 q|q2 ˛ ‹‹‹‹‹‚\ně 1´ exp ˆ ´2 ¨ κpnq ¨ w 2 min ¨ y2n ¨ log n\ngmax ¨ n\n˙\ně 1´ exp p´c˚ ¨ n log nq ,\nwhere the last inequality follows by assumption of the lemma and since yn “ ωp1q.\nWe now turn to the proof of Claim 5.7.\nProof of Claim 5.7. Note that for any two vertices i, j of G, the edge pi, jq exists in G with probability at least αn ¨ wmin. Thus, we have\nE “ ΓpT 1q | ψ ‰ “ ÿ\nNPT 1\ngp|V pchild1pNqq|, |V pchild2pNqq|q ÿ\niPV pchild1pNqq jPV pchild2pNqq\nαn ¨ wpi, jq\ně wmin ¨ αn ¨ ÿ\nNPT 1\ngp|V pchild1pNqq|, |V pchild2pNqq|q|V pchild1pNqq| ¨ |V pchild2pNqq|\n“ wmin ¨ αn ¨ κpnq,\nwhere we made use of Eqn. (5). Furthermore, we have ÿ\niăj\ngp|V pchild1pN i,jqq|,|V pchild2pN i,jqq|q2 “\n“ ÿ\nNPT 1\ngp|V pchild1pNqq|, |V pchild2pNqq|q2 ¨ |V pchild1pNqq| ¨ |V pchild2pNqqq|\nď gmax ÿ\nNPT 1\ngp|V pchild1pNqq|, |V pchild1pNqq|q ¨ |V pchild1pNqq| ¨ |V pchild2pNqqq|\n“ gmax ¨ κpnq,\nwhere we made use of Eqn. (5).\n5.3 Algorithm for Clustering in the HSBM\nIn this section, we provide an algorithm for obtaining a hierarchical clustering of a graph generated from an HSBM. The algorithm is quite simple and combines approaches that are used in practice for hierarchical clustering: SVD projections and agglomerative heuristics. See Algorithm 2 for a complete description.\nTheorem 5.8. Let αn “ ωp a\nlog n{nq. Let Γ be an admissible cost function (Defn. 3.1) satisfying the smoothness property (Defn 5.4). Let k be a fixed constant and G be a graph generated from an HSBM (as per Defn. 5.1) where the underlying graph rGk has k nodes and the sparsity factor is αn. Let T be a ground-truth tree for G. With high probability, Algorithm 2 with parameter k on graph G outputs a tree T 1 that satisfies ΓpT q ď p1` op1qqOPT.\nAlgorithm 2 Agglomerative Algorithm for Recovering Ground-Truth Tree of an HSBM Graph\n1: Input: Graph G “ pV,Eq generated from an HSBM. 2: Parameter: A constant k. 3: Apply (SVD) projection algorithm of (McSherry, 2001, Thm. 12) with parameters G, k, δ “\n|V |´2, to get ζp1q, . . . , ζp|V |q P R|V | for vertices in V , where dimpspanpζp1q, . . . , ζp|V |qqq “ k. 4: Run the single-linkage algorithm on the points tζp1q, . . . , ζp|V |qu until there are exactly k clus-\nters. Let C “ tCζ1 , . . . , C ζ ku be the clusters (of points ζpiq) obtained. Let Ci Ď V denote the set\nof vertices corresponding to the cluster Cζi . 5: while there are at least two clusters in C do 6: Take the pair of clusters Ci, Cj of C that maximizes\ncutpCi,Cjq |Ci|¨|Cj|\n7: C Ð C z tCiu z tCju Y tCi Y Cju 8: end while 9: The sequence of merges in the while-loop (Steps 5 to 8) induces a hierarchical clustering tree on tC1, . . . , Cku, say T 1k with k leaves (represented by C1, . . . , Ck). Replace each leaf of T 1k by an arbitrary binary tree on |Ck| leaves labelled according to the vertices Ck to obtain T . 10: Repeat the algorithm k1 “ 2k log n times. Let T 1, . . . T k1 be the corresponding hierarchical clustering trees. 11: Output: Tree T i (out of the k1 candidates) that minimises ΓpTiq.\nRemark 5.9. In an HSBM, k is a fixed constant. Thus, even if k is not known in advance, one can simply run the Algorithm 2 with all possible different values (constantly many) and return the solution with the minimal cost ΓpT q.\nLet G “ pV,Eq be the input graph generated according to an HSBM. Let T be the tree output by Algorithm 2. We divide the proof into two claims that correspond to the outcome of Step 3 and the while-loop (Steps 5 to 8) of Algorithm 2.\nWe use a result of McSherry (2001) who considers a random graph model with k clusters that is (slightly) more general than the HSBM considered here. The difference is that there is no hierarchical structure on top of the k clusters in his setting; on the other hand, his goal is also simply to identify the k clusters and not any hierarchy upon them. The following theorem is derived from McSherry (2001) (Observation 11 and a simplification of Theorem 12).\nTheorem 5.10 (McSherry (2001)). Let s be the size of the smallest cluster (of the k clusters) and δ be the confidence parameter. Assume that for all u, v belonging to different clusters with with adjacency vectors u,v (i.e., ui is 1 if the edge pu, iq exists in G and 0 otherwise) satisfy\n}E ru s ´ E rv s }22 ě c ¨ k ¨ pn{s` logpn{δqq for a large enough constant c, where E ru s is the entry-wise expectation. Then, the algorithm of McSherry (2001, Thm. 12) with parameters G, k, δ projects the columns of the adjacency matrix of G to points tζp1q, . . . , ζp|V |qu in a k-dimensional subspace of R|V | such that the following holds w.p. at least 1 ´ δ over the random graph G and with probability 1{k over the random bits of the algorithm. There exists η ą 0 such that for any u in the ith cluster and v in the jth cluster:\n1. if i “ j then }ζpuq ´ ζpvq}22 ď η;\n2. if i ‰ j then }ζpuq ´ ζpvq}22 ą 2η,\nRecall that ψ : V Ñ rks is the (hidden) labelling assigning each vertex of G to one of the k bottom-level clusters. Let C˚i “ tv P V | ψpvq “ iu. Recall that ni “ |V pC˚i q|. Note that the algorithm of (McSherry, 2001, Thm. 12) might fail for two reasons. The first reason is that the random choices by the algorithm yield an incorrect clustering. This happens w.p. at most 1{k and we can simply repeat the algorithm sufficiently many times to be sure that at least once we get the desired result, i.e., the projections satisfy the conclusion of Thm. 5.10. Claims 5.11 and 5.12 show that in this case, Steps 5 to 8 of Alg. 2 produce a tree that has cost close to optimal. Ultimately, the algorithm simply outputs a tree that has the least cost among all the ones produced (and one of them is guaranteed to have cost p1` op1qqOPT) with high probability.\nThe second reason why the McSherry’s algorithm may fail is that the generated random graph G might “deviate” too much from its expectation. This is controlled by the parameter δ (which we set to 1{|V |2). Deviations from expected behaviour will cause our algorithm to fail as well. We bound this failure probability in terms of two events. Let Ē1 be the event that there exists i, such that ni ă fin{2, i.e., at least one of the bottom-level clusters has size that is not representative. This event occurs with a very low probability which is seen by a simple application of the ChernoffHoeffding bound, as E rni s “ fin. Note that Ē1 depends only on the random choices that assign labels to the vertices according to ψ (and not on random choice of the edges). Let E1 be the complement of Ē1. If E1 holds the term n{s that appears in Thm. 5.10 is a constant. The second bad event is that McSherry’s algorithm fails due to the random choice of edges. This happens with probability at most δ which we set at δ “ 1\n|V |2 . We denote the complement of this event E2. Thus,\nfrom now on we assume that both “good” events E1 and E2 occur, allowing Alg. 2 to fail if either of them don’t occur.\nIn order to prove Theorem 5.8 we establish the following claims.\nClaim 5.11. Let αn “ ωp a\nlog n{nq. Let G be generated by an HSBM. Let C˚1 , . . . , C˚k be the hidden bottom-level clusters, i.e., C˚i “ tv | ψpvq “ iu. Assume that events E1 and E2 occur. With probability at least 1{k, the clusters obtained after Step 4 correspond to the assignment ψ, i.e., there exists a permutation π : rks Ñ rks, such that Cj “ C˚πpjq.\nProof. The proof relies on Theorem 5.10. As we know that the event E1 occurs, we may conclude that s “ mini ni ě n2 mini fi. Thus, n{s ď 2fmin , where fmin “ mini fi and hence n{s is bounded by some fixed constant.\nLet u, v be two nodes such that i “ ψpuq ‰ ψpvq “ j. Let u and v denote the random variables corresponding to the columns of u and v in the adjacency matrix of G. Let q “ ĂW pNq where N is the LCA rTkprσ\n´1piq, rσ´1pjqq in rTk, the generating tree for rGk used in defining the HSBM. Assuming E1 and taking expectations only with respect to the random choice of edges, we have:\n}E ru | ψ, E1 s ´ E rv | ψ, E1 s }22 ě niα2nppi ´ qq2 ` njα2nppj ´ qq2 “ Ωpα2nnq “ ωplog nq\nAbove we used that pi ´ q ą 0 and ni “ Ωpnq for each i. Note that for δ “ 1\nn2 , this satisfies the condition of Theorem 5.10. Since, we are already\nassuming that E2 holds, the only failure arises from the random coins of the algorithm. Thus, with probability at least 1{k the conclusions of Theorem 5.10 hold. In the rest of the proof we assume that the following holds: There exists η ą 0 such that for any pair of nodes u, v we have\n1. if ψpuq “ ψpvq then }ζpuq ´ ζpvq}22 ď η;\n2. if ψpuq ‰ ψpvq then }ζpuq ´ ζpvq}22 ą 2η.\nTherefore, any linkage algorithm, e.g., single linkage (See Alg. 6), performing merges starting from the set tζp1q, . . . , ζpnqu until there are k clusters will merge clusters at a distance of at most η and hence, the clusters obtained after Step 4 correspond to the assignment ψ. This yields the claim. Claim 5.12. Let αn “ ωp a\nlog n{nq. Let G be generated according to an HSBM and let T ˚ be a ground-truth tree for G. Assume that events E1 and E2 occur, and that furthermore, the clusters obtained after Step 4 correspond to the assignment ψ, i.e., there exists a permutation π : rks Ñ rks such that for each v P Ci, ψpvq “ πpiq. Then, the sequence of merges in the while-loop (Steps 5 to 8) followed by Step 9 produces w.h.p. a tree T such that ΓpT q ď p1` op1qqOPT .\nProof. For simplicity of notation, we will assume that π is the identity permutation, i.e., the algorithm has not only identified the true clusters correctly but also guessed the correct label. This only makes the notation less messy, though the proof is essentially unchanged.\nLet N be some internal node of any generating tree rTk and let S1 “ V pchild1pNqq and S2 “ V pchild2pNqq. Note that both S1 and S2 are a disjoint union of some of the clusters tC1, . . . , Cku. Then notice that for any u P S1 and v P S2, the probability that the edge pu, vq exists in G is αnĂW pNq. Thus, conditioned on E1 and ψ, we have\nE „ cutpSi, Sjq |Si||Sj |  “ αnĂW pNq\nFor now, let us assume that Alg. 2 makes merges in Steps 5 to 8 based on the true expectations instead of the empirical estimates. Then essentially, the algorithm is performing any linkage algorithm (i.e., average, single, or complete-linkage) on a ground-truth input and hence is guaranteed to recover the generating tree (see Theorem. 7.1).\nTo complete the proof, we will show the following: For any partition C1, . . . , Ck of V satisfying mini |Ci| ě n2 mini fi, and for any S1, S2, S11, S12, where S1 and S2 (and S11, S12) are disjoint and are both unions of some cluster tC1, . . . , Cku. and i1 ‰ j1, with probability at least 1 ´ 1{n3, the following holds:\nˇ̌ ˇ̌E „ cutpS1, S2q |S1| ¨ |S2|  ´ wpS1, S2q|S1| ¨ |S2| ˇ̌ ˇ̌ ď 1 log n (10)\nNote that as the probability of any edge existing is Ωpαnq, it must be the case that E ”\nwpS1,S2q |S1||S2|\nı “\nΩpαnq. Furthermore, since |S1|, |S2| “ Ωpnq by the Chernoff-Hoeffding bound for a single pair pS1, S2q the above holds with probability expp´cn2αn{ log nq. Thus, even after taking a union bound over Opknq possible partitions and 2Opkq possible pairs of sets pS1, S2q that can be derived from said partition, Eqn. 10 holds with high probability.\nTo complete the proof, we will show the following: We will assume that the algorithm of McSherry has identified the correct clusters at the bottom level, i.e., E2 holds, and that Eqn. 10 holds.\nWe restrict our attention to sets S1, S2 that are of the form that S1 “ V pchild1pNqq and S2 “ V pchild2pNqq for some internal node N of some generating tree rTk of the graph rGk used to generate G. Then for pairs pS1, S2q and pS11, S12q both of this form, the following holds: there exists 3\nlogn ą η ą 0 such that\n1. If E ”\nwpS1,S2q |S1|¨|S2|\nı “ E ” wpS11,S 1 2q\n|S11|¨|S 1 2|\nı , then ˇ̌ ˇwpS1,S2q|S1|¨|S2| ´ wpS11,S 1 2q\n|S11|¨|S 1 2|\nˇ̌ ˇ ď η\n2. If E ”\nwpS1,S2q |S1|¨|S2|\nı ‰ E ” wpS11,S 1 2q\n|S11|¨|S 1 2|\nı , then ˇ̌ ˇwpS1,S2q|S1|¨|S2| ´ wpS11,S 1 2q\n|S11|¨|S 1 2|\nˇ̌ ˇ ą 2η\nNote that the above conditions are enough to ensure that the algorithm performs the same steps as with perfect inputs, up to an arbitrary choice of tie-breaking rule. Since Theorem 7.1 is true no matter the tie breaking rule chosen, the proof follows since the two above conditions hold with probability at least expp´cn2αn{ log nq.\nWe are ready to prove Theorem 5.8.\nProof of Theorem 5.8. Conditioning on E1 and E2 which occur w.h.p. we get from Claims 5.11 and 5.12 that w.p. at least 1{k the tree Ti obtain in step 9 fulfills ΓpTiq ď p1` op1qqOPT . It is possible to boost this probability by running Algorithm 2 multiple times. Running it Ωpk log nq times and taking the tree with the smallest ΓpTiq yields the result.\nRemark 5.13. It is worth mentioning that one of the trees Ti computed by the algorithm is w.h.p. the ground-truth tree T ˚. If one desires to recover that tree, then this is possible by verifying for each candidate tree with minimal Ti whether is is indeed generating."
    }, {
      "heading" : "6 Dissimilarity-Based Inputs: Approximation Algorithms",
      "text" : "In this section, we consider general dissimilarity inputs and admissible objective for these inputs. For ease of exposition, we focus on an particular admissible objective function for dissimilarity inputs. Find T maximizing the value function corresponding to Dasgupta’s cost function of Section 4: valpT q “ řNPT valpNq where for each node N of T with children N1, N2, valpNq “ wpV pN1q, V pN2qq ¨V pNq. This optimization problem is NP-Hard Dasgupta (2016), hence we focus on approximation algorithms.\nWe show (Theorem 6.2) that average-linkage achieves a 2 approximation for the problem. We then introduce a simple algorithm based on locally-densest cuts and show (Theorem 6.5) that it achieves a 3{2` ε approximation for the problem.\nWe remark that our proofs show that for any admissible objective function, those algorithms have approximation guarantees, but the approximation guarantee depends on the objective function.\nWe start with the following elementary upper bound on OPT.\nFact 6.1. For any graph G “ pV,Eq, and weight function w : E Ñ R`, we have OPT ď n ¨ř ePE wpeq."
    }, {
      "heading" : "6.1 Average-Linkage",
      "text" : "We show that average-linkage is a 2-approximation in the dissimilarity setting.\nTheorem 6.2. For any graph G “ pV,Eq, and weight function w : E Ñ R`, the average-linkage algorithm (Algorithm 3) outputs a solution of value at least n ř ePE wpeq{2 ě OPT{2.\nWhen two trees are chosen at Step 4 of Algorithm 3, we say that they are merged. We say that all the trees considered at the beginning of an iteration of the while loop are the trees that are candidate for the merge or simply the candidate trees.\nWe first show the following lemma and then prove the theorem.\nAlgorithm 3 Average-Linkage Algorithm for Hierarchical Clustering (dissimilarity setting)\n1: Input: Graph G “ pV,Eq with edge weights w : E ÞÑ R` 2: Create n singleton trees. 3: while there are at least two trees do 4: Take trees roots N1 and N2 minimizing\nř xPV pN1q,yPV pN2q wpx, yq{p|V pN1q||V pN2q|q\n5: Create a new tree with root N and children N1 and N2 6: end while 7: return the resulting binary tree T\nLemma 6.3. Let T be the output tree and A,B be the children of the root. We have,\nwpV pAq, V pBqq |V pAq| ¨ |V pBq| ě wpV pAqq |V pAq| ¨ p|V pAq| ´ 1q ` wpV pBqq |V pBq| ¨ p|V pBq| ´ 1q .\nProof. Let a “ |V pAq|p|V pAq| ´ 1q{2 and b “ |V pBq|p|V pBq| ´ 1q{2. For any node N0 of T , let child1pN0q and child2pN0q be the two children of N0. We first consider the subtree TA of T rooted at A. We have #\nwpV pAqq “ řA0PTA wpV pchild1pA0qq, V pchild2pA0qqq, a “ řA0PTA |V pchild1pA0qq| ¨ |V pchild2pA0qq|.\nBy an averaging argument, there exists A1 P TA with children A1, A2 such that\nwpV pA1q, V pA2qq |V pA1q| ¨ |V pA2q| ě wpV pAqq a . (11)\nWe now consider the iteration of the while loop at which the algorithm merged the trees A1 and A2. Let A1, A2, . . . , Ak and B1, B2, . . . , Bℓ be the trees that were candidate for the merge at that iteration, and such that V pAiq X V pBq “ H and V pBiq X V pAq “ H. Observe that the leaves sets of those trees form a partition of the sets V pAq and V pBq, so\n# wpA,Bq “ ři,j wpV pAiq, V pBjqq, |V pAq| ¨ |V pBq| “ ři,j |V pAiq| ¨ |V pBjq|.\nBy an averaging argument again, there exists Ai, Bj such that\nwpV pAiq, V pBjqq |V pAiq| ¨ |V pBjq| ď wpV pAq, V pBqq|V pAq| ¨ |V pBq| . (12)\nNow, since the algorithm merged A1, A2 rather than Ai, Bj , by combining Eq. 11 and 12, we have\nwpV pAqq a ď wpV pA1q, V pA2qq|V pA1q| ¨ |V pA2q| ď wpV pAiq, V pBjqq|V pAiq| ¨ |V pBjq| ď wpV pAq, V pBqq|V pAq| ¨ |V pBq| .\nApplying the same reasoning to B and taking the sum yields the lemma.\nProof of Theorem 6.2. We proceed by induction on the number of the nodes n in the graph. Let A,B be the children of the root of the output tree T . By induction,\nvalpT q ě p|V pAq| ` |V pBq|q ¨ wpV pAq, V pBqq ` |V pAq| 2 wpV pAqq ` |V pBq| 2 wpV pBqq. (13)\nLemma 6.3 implies p|V pAq|`|V pBq|q¨wpV pAq, V pBqq ě |V pBq|wpV pAqq`|V pAq|wpV pBqq. Dividing both sides by 2 and plugging it into (13) yields\nvalpT q ě |V pAq| ` |V pBq| 2 wpV pAq, V pBqq ` |V pAq| ` |V pBq| 2 pwpV pAqq ` wpV pBqqq.\nObserving that n “ |V pAq| ` |V pBq| and combining řePE wpeq “ wpV pAq, V pBqq ` wpV pAqq ` wpV pBqq with Fact 6.1 completes the proof."
    }, {
      "heading" : "6.2 A Simple and Better Approximation Algorithm for Worst-Case Inputs",
      "text" : "In this section, we introduce a very simple algorithm (Algorithm 5) that achieves a better approximation guarantee. The algorithm follows a divisive approach by recursively computing locallydensest cuts using a local search heuristic (see Algorithm 4). This approach is similar to the recursive-sparsest-cut algorithm of Section 4. Here, instead of trying to solve the densest cut problem (and so being forced to use approximation algorithms), we solve the simpler problem of computing a locally-densest cut. This yields both a very simple local-search-based algorithm and a good approximation guarantee.\nWe use the notation A‘x to mean the set obtained by adding x to A if x R A, and by removing x from A if x P A. We say that a cut pA,Bq is a ε{n-locally-densest cut if for any x,\nwpA‘ x,B ‘ xq |A‘ x| ¨ |B ‘ x| ď\n´ 1` ε\nn ¯ wpA,Bq |A||B| .\nThe following local search algorithm computes an ε{n-locally-densest cut.\nAlgorithm 4 Local Search for Densest Cut\n1: Input: Graph G “ pV,Eq with edge weights w : E ÞÑ R` 2: Let pu, vq be an edge of maximum weight 3: A Ð tvu, B Ð V ztvu 4: while Dx: wpA‘x,B‘xq|A‘x|¨|B‘x| ą p1` ε{nq wpA,Bq |A||B| do 5: A Ð A‘ x, B Ð B ‘ x 6: end while 7: return pA,Bq\nTheorem 6.4. Algorithm 4 computes an ε{n-locally-densest cut in time rOpnpn`mq{εq.\nProof. The proof is straightforward and given for completeness. By definition, the algorithm computes an ε{n-locally densest cut so we only need to argue about the running time. The weight of the cut is initially at least wmax, the weight of the maximum edge weight, and in the end at most nwmax. Since the weight of the cut increases by a factor of p1 ` ε{nq at each iteration, the total number of iterations of the while loop is at most log1`ε{npnwmax{wmaxq “ rOpn{εq. Each iteration takes time Opm` nq, so the running time of the algorithm is rOpnpm` nq{εq.\nTheorem 6.5. Algorithm 5 returns a tree of value at least\n2n\n3 p1´ εq\nÿ\ne\nwpeq ě 2 3 p1´ εqOPT,\nin time rOpn2pn `mq{εq.\nAlgorithm 5 Recursive Locally-Densest-Cut for Hierarchical Clustering\n1: Input: Graph G “ pV,Eq, with edge weights w : E ÞÑ R`, ε ą 0 2: Compute an ε{n-locally-densest cut pA,Bq using Algorithm 4 3: Recurse on GrAs and GrBs to obtain rooted trees TA and TB . 4: Return the tree T whose root node has two children, TA and TB .\nThe proof relies on the following lemma.\nLemma 6.6. Let pA,Bq be an ε{n-locally-densest cut. Then,\np|A| ` |B|qwpA,Bq ě 2p1´ εqp|B|wpAq ` |A|wpBqq.\nProof. Let v P A. By definition of the algorithm,\np1` ε{nqwpA,Bq|A||B| ě wpAztvu, B Y tvuq p|A| ´ 1qp|B| ` 1q .\nRearranging,\np|A| ´ 1qp|B| ` 1q |A||B| p1` ε{nqwpA,Bq ě wpAztvu, B Y tvuq “ wpA,Bq ` wpv,Aq ´ wpv,Bq\nSumming over all vertices of A, we obtain\n|A| p|A| ´ 1qp|B| ` 1q|A||B| p1` ε{nqwpA,Bq ě |A|wpA,Bq ` 2wpAq ´ wpA,Bq.\nRearranging and simplifying,\np|A| ´ 1qp|B| ` 1q ε n wpA,Bq ` p|A| ´ 1qp1 ` ε{nqwpA,Bq ě 2|B|wpAq.\nSince |B| ` 1 ď n, this gives |A|wpA,Bq ě 2p1´ εq|B|wpAq.\nProceeding similarly with B and summing the two inequalities yields the lemma.\nProof of Theorem 6.5. We first show the approximation guarantee. We proceed by induction on the number of vertices. The base case is trivial. By inductive hypothesis,\nvalpT q ě nwpA,Bq ` 2 3 ¨ p1´ εqp|A|wpAq ` |B|wpBqq,\nwhere n “ |A| ` |B|. Lemma 6.6 implies\nnwpA,Bq “ p|A| ` |B|qwpA,Bq ě 2p1 ´ εqp|B|wpAq ` |A|W pBqq.\nHence, |A| ` |B|\n3 wpA,Bq ě 2 3 p1´ εqp|B|wpAq ` |A|wpBqq.\nTherefore,\nvalpT q ě 2n 3 p1´ εqpwpA,Bq ` wpAq ` wpBqq “ p1´ εq2n 3\nÿ\ne\nwpeq.\nTo analyze the running time, observe that by Theorem 6.4, a recursive call on a graph G1 “ pV 1, E1q takes time rOp|V 1|p|V 1| ` |E1|q{εq and that the depth of the recursion is Opnq.\nRemark 6.7. The average-linkage and the recursive locally-densest-cut algorithms achieve an Opgnq- and Ophnq-approximation respectively, for any admissible cost function f , where gn “ maxn fpnq{fprn{2sq. hn “ maxn fpnq{fpr2n{3sq. An almost identical proof yields the result.\nRemark 6.8. In Section 8, we show that other commonly used algorithms, such as completelinkage, single-linkage, or bisection 2-Center, can perform arbitrarily badly (see Theorem 8.6). Hence average-linkage is more robust in that sense."
    }, {
      "heading" : "7 Perfect Ground-Truth Inputs and Beyond",
      "text" : "In this section, we focus on ground-truth inputs. We state that when the input is a perfect groundtruth input, commonly used algorithms (single linkage, average linkage, and complete linkage; as well as some divisive algorithms – the bisection k-Center and sparsest-cut algorithms) yield a tree of optimal cost, hence (by Definition 3.1) a ground-truth tree. Some of those results are folklore (and straightforward when there are no ties), but we have been unable to pin down a reference, so we include them for completeness (Section 7.1). We also introduce a faster optimal algorithm for “strict” ground-truth inputs (Section 7.2). The proofs present no difficulty. The meat of this section is Subsection 7.3, where we go beyond ground-truth inputs; we introduce δ-adversariallyperturbed ground-truth inputs and design a simple, more robust algorithm that, for any admissible objective function, is a δ-approximation."
    }, {
      "heading" : "7.1 Perfect Ground-Truth Inputs are Easy",
      "text" : "Algorithm 6 Linkage Algorithm for Hierarchical Clustering (similarity setting)\n1: Input: A graph G “ pV,Eq with edge weights w : E ÞÑ R` 2: Create n singleton trees. Root labels: C “ ttv1u, . . . , tvnuu\n3: Define dist : C ˆ C ÞÑ R`: distpC1, C2q “ $ ’’& ’’% 1 |C1||C2| ř xPC1,yPC2 wppx, yqq Average Linkage\nminxPC1,yPC2 wppx, yqq Single Linkage maxxPC1,yPC2 wppx, yqq Complete Linkage\n.\n4: while there are at least two trees do 5: Take the two trees with root labels C1, C2 such that distpC1, C2q is maximum 6: Create a new tree by making those two tree children of a new root node labeled C1 Y C2 7: Remove C1, C2 from C, add C1 Y C2 to C, and update dist 8: end while 9: return the resulting binary tree T\nIn the following, we refer to the tie breaking rule of Algorithm 6 as the rule followed by the algorithm for deciding which of Ci, Cj or Ck, Cℓ to merge, when maxC1,C2PC distpC1, C2q “ distpCi, Cjq “ distpCk, Cℓq.\nTheorem 7.1. 15 Assume that the input is a (dissimilarity or similarity) ground-truth input. Then, for any admissible objective function, the agglomerative heuristics average-linkage, single-linkage, and complete-linkage (see Algorithm 6) return an optimal solution. This holds no matter the tie breaking rule of Algorithm 6.\n15This Theorem may be folklore, at least when there are no ties, but we have been unable to find a reference.\nProof. We focus on the similarity setting; the proof for the dissimilarity setting is almost identical. We define the candidate trees after t iterations of the while loop to be sets of trees in C at that time. The theorem follows from the following statement, which we will prove by induction on t: If Ct “ tC1, . . . , Cku denotes the set of clusters after t iterations, then there exists a generating tree T t for G, such that the candidate trees are subtrees of T t.\nFor the base case, initially each candidate tree contains exactly one vertex and the statement holds. For the general case, let C1, C2 be the two trees that constitute the t\nth iteration. By induction, there exists a generating tree T t´1 for G, and associated weights W t´1 (according to Definition 2.2) such that C1 and C2 are subtrees of T t´1, rooted at nodes N1 and N2 of T t´1 respectively. To define T t, we start from T t´1. Consider the path P “ tN1, N1, N2, . . . , Nk, N2u joining N1 to N2 in T t´1 and let Nr “ LCAT t´1pN1, N2qq. If Nr is the parent of N1 and N2, then T t “ tt´1, else do the following transformation: remove the subtrees rooted at N1 and at N2; create a new node N˚ as second child of Nk, and let N1 and N2 be its children. This defines T\nt. To define W t, extend W t´1 by setting W tpN˚q “ W pNrq. Claim 7.2. For any Ni, Nj P P , W t´1pNiq “ W t´1pNjq.\nThanks to the inductive hypothesis, with Claim 7.2 it is easy to verify that W t certifies that T t\nis generating for G.\nProof of Claim 7.2. Fix a node Ni on the path from Nr to N 1 (the argument for nodes on the path from Nr to N 2 is similar). By induction W t´1pNiq ě W t´1pNrq. We show that since the linkage algorithms merge the trees C1 and C2, we also have W t´1pNiq ď W t´1pNrq and so W t´1pNiq “ W t´1pNrq, hence the claim. Let w0 “ W t´1pNrq. By induction, for all u P C1, v P C2, wpu, vq “ w0, and thus distpC1, C2q “ w0 in the execution of all the algorithms. Fix a candidate tree C 1 P Ct, C 1 ‰ C1, C2 and C 1 Ď V pNiq. Since C is a partition of the vertices of the graph and since candidate trees are subtrees of T t´1, such a cluster exists. Thus, for u P C1, v P C 1 wpu, vq “ W t´1pLCAT t´1pu, vqq “ W t´1pNiq ě w0 since Ni is a descendant of Nr.\nIt is easy to check that by their definitions, for any of the linkage algorithms, we thus have that distpC1, C 1q ě w0 “ distpC1, C2q. But since the algorithms merge the clusters at maximum distance, it follows that distpC1, C 1q ď distpC1, C2q “ w0 and therefore, W t´1pNiq ď W t´1pNrq and so, W t´1pNiq “ W t´1pNrq and the claim follows. This is true no matter the tie breaking chosen for the linkage algorithms.\nDivisive Heuristics. In this section, we focus on two well-known divisive heuristics: (1) the bisection 2-Center which uses a partition-based clustering objective (the k-Center objective) to divide the input into two (non necessarily equal-size) parts (see Algorithm 7), and (2) the recursive sparsest-cut algorithm, which can be implemented efficiently for ground-truth inputs (Lemma 7.6).\nAlgorithm 7 Bisection 2-Center (similarity setting)\n1: Input: A graph G “ pV,Eq and a weight function w : E ÞÑ R` 2: Find tu, vu Ď V that maximizes minxmaxyPtu,vu wpx, yq 3: A Ð tx | wpx, uq ě maxyPtu,vu wpx, yqu 4: B Ð V zA. 5: Apply Bisection 2-Center on GrAs and GrBs to obtain trees TA,TB respectively 6: return The union tree of TA, TB .\nLoosely speaking, we show that this algorithm computes an optimal solution if the optimal solution is unique. More precisely, for any similarity graph G, we say that a tree T is strictly generating for G if there exists a weight function W such that for any nodes N1, N2, if N1 appears on the path from N2 to the root, then W pN1q ă W pN2q and for every x, y P V , wpx, yq “ W pLCAT px, yqq. In this case we say that the input is a strict ground-truth input. In the context of dissimilarity, an analogous notion can be defined and we obtain a similar result.\nTheorem 7.3. 16 For any admissible objective function, the bisection 2-Center algorithm returns an optimal solution for any similarity or dissimilarity graph G that is a strict ground-truth input.\nProof. We proceed by induction on the number of nodes in the graph. Consider a strictly generating tree T and the corresponding weight function W . Consider the root node Nr of T and let N1, N2 be the children of the root. Let pα, βq be the cut induced by the root node of T (i.e., α “ V pN1q, β “ V pN2q). Define w0 to be the weight of an edge between u P α and v P β for any u, v (recall that since T is strictly generating all the edges between α and β are of same weight). We show that the bisection 2-Centers algorithm divides the graph into α and β. Applying the inductive hypothesis on both subgraphs yields the result.\nSuppose that the algorithm locates the two centers in β. Then, minxmaxyPtu,vu wpx, yq “ w0 since the vertices of α are connected by an edge of weight w0 to the centers. Thus, the value of the clustering is w0. Now, consider a clustering consisting of a center c0 in α and a center c1 in β. Then, for each vertex u, we have maxcPtc0,c1uwpu, cq ě minpW pN1q,W pN2qq ą W pNrq “ w0 since T and W are strictly generating; Hence a strictly better clustering value. Therefore, the algorithm locates x P α and y P β. Finally, it is easy to see that the partitioning induced by the centers yields parts A “ α and B “ β.\nRemark 7.4. To extend our result to (non-strict) ground-truth inputs, one could consider the following variant of the algorithm (which bears similarities with the popular elbow method for partition-based clustering): Compute a k-Center clustering for all k P t1, . . . , nu and partition the graph according to the k-Center clustering of the smallest k ą 1 for which the value of the clustering increases. Mimicking the proof of Theorem 7.3, one can show that the tree output by the algorithm is generating.\nWe now turn to the recursive sparsest-cut algorithm (i.e., the recursive φ-sparsest-cut algorithm of Section 4, for φ “ 1). The recursive sparsest-cut consists in recursively partitioning the graph according to a sparsest cut of the graph. We show (1) that this algorithm yields a tree of optimal cost and (2) that computing a sparsest cut of a similarity graph generated from an ultrametric can be done in linear time. Finally, we observe that the analogous algorithm for the dissimilarity setting consists in recursively partitioning the graph according to the densest cut of the graph and achieves similar guarantees (and similarly the densest cut of a dissimilarity graph generated from an ultrametric can be computed in linear time).\nTheorem 7.5. 17 For any admissible objective function, the recursive sparsest-cut (respectively densest-cut) algorithm computes a tree of optimal cost if the input is a similarity (respectively dissimilarity) ground-truth input.\nProof. The proof, by induction, has no difficulty and it may be easier to recreate it than to read it. Let T be a generating tree and W be the associated weight function. Let Nr be the root of T , N1, N2 the children of Nr, and pα “ V pN1q, β “ V pN2qq the induced root cut. Since T is strictly 16This Theorem may be folklore, but we have been unable to find a reference. 17This Theorem may be folklore, at least when there are no ties, but we have been unable to find a reference.\ngenerating, all the edges between α and β are of same weight w, which is therefore also the sparsity of pα, βq. For every edge pu, vq of the graph, wpu, vq “ W pLCAT pu, vqq ě w, so every cut has sparsity at least w, so pα, βq has minimum sparsity.\nNow, consider the tree T ˚ computed by the algorithm, and let pγ, δq denote the sparsest-cut used by the algorithm at the root (in case of ties it might not different from pα, βq). By induction the algorithm on Grγs and Grδs gives two generating trees Tγ and Tδ with associated weight functions Wγ and Wδ. To argue that T\n˚ is generating, we define W ˚ as follows, where N˚r denotes the root of T ˚.\nW ˚pNq “ $ ’& ’%\nWγpNq if N P Tγ WδpNq if N P Tδ w if N “ N˚r\nBy induction wpu, vq “ W pLCAT pu, vqq if either both u, v P γ, or both u, v P δ. For any u P γ, v P δ, we have wpu, vq “ w “ W pN˚r q “ W pLCAT pu, vqq. Finally, since w ď wpu, vq for any u, v, we have W pN˚r q “ w ď W pNq, for any N P T ˚, and therefore T ˚ is generating.\nWe then show how to compute a sparsest-cut of a graph that is a ground-truth input.\nLemma 7.6. If the input graph is a ground-truth input then the sparsest cut is computed in Opnq time by the following algorithm: pick an arbitrary vertex u, let wmin be the minimum weight of edges adjacent to u, and partition V into A “ tx | wpu, xq ą wminu and B “ V zA.\nProof. Let wmin “ wpu, vq. We show that wpA,Bq{p|A||B|q “ wmin and since wmin is the minimum edge weight of the graph, that the cut pA,Bq only contains edges of weight wmin. Fix a generating tree T . Consider the path from u to the root of T and let N0 be the first node on the (bottom-up) path such that W pN0q “ wmin. For any vertex x P A, we have that wpu, xq ą wmin. Hence by definition, we have that N0 is an ancestor of LCAT pu, xq. Therefore, for any other node y such that wpu, yq “ wmin, we have LCAT pu, yq “ LCAT px, yq and so, wpx, yq “ W pLCAT px, yqq “ W pLCAT pu, yqq “ wmin. It follows that all the edges in the cut pA,Bq are of weight wmin and so, the cut is a sparsest cut."
    }, {
      "heading" : "7.2 A Near-Linear Time Algorithm",
      "text" : "In this section, we propose a simple, optimal, algorithm for computing a generating tree of a groundtruth input. For any graph G, the running time of this algorithm is Opn2q, and rOpnq if there exists a tree T that is strictly generating for the input. For completeness we recall that for any graph G, we say that a tree T is strictly generating for G if there exists a weight function W such that for any nodes N1, N2, if N1 appears on the path from N2 to the root, then W pN1q ă W pN2q and for every x, y P V , wpx, yq “ W pLCAT px, yqq. In this case we say that the inputs is a strict ground-truth input.\nThe algorithm is described for the similarity setting but could be adapted to the dissimilarity case to achieve the same performances.\nTheorem 7.7. For any admissible objective function, Algorithm 8 computes a tree of optimal cost in time Opn log2 nq with high probability if the input is a strict ground-truth input or in time Opn2q if the input is a (non-necessarily strict) ground-truth input.\nProof. We proceed by induction on the number of vertices in the graph. Let p be the first pivot chosen by the algorithm and let B1, . . . , Bk be the sets defined by p at Step 4 of the algorithm, with wpp, uq ą wpv, pq, for any u P Bi, v P Bi`1.\nAlgorithm 8 Fast and Simple Algorithm for Hierarchical Clustering on Perfect Data (similarity setting)\n1: Input: A graph G “ pV,Eq and a weight function w : E ÞÑ R` 2: p Ð random vertex of V 3: Let w1 ą . . . ą wk be the edge weights of the edges that have p as an endpoint 4: Let Bi “ tv | wpp, vq “ wiu, for 1 ď i ď k. 5: Apply the algorithm recursively on each GrBis and obtain a collection of trees T1, . . . , Tk 6: Define T ˚0 as a tree with p as a single vertex 7: For any 1 ď i ď k, define T ˚i to be the union of T ˚i´1 and Ti 8: Return T ˚k\nWe show that for any u P Bi, v P Bj, j ą i, we have wpu, vq “ wpp, vq. Consider a generating tree T and define N1 “ LCAT pp, uq and N2 “ LCAT pp, vq. Since T, h, σ is generating and wpp, uq ą wpp, vq, we have that N2 is an ancestor of N1, by Definition 2.2. Therefore, LCAT pu, vq “ N2, and so wpu, vq “ W pN2q “ wpp, vq. Therefore, combining the inductive hypothesis on any GrBis and by Definition 2.2 the tree output by the algorithm is generating.\nA bound of Opn2q for the running time follows directly from the definition of the algorithm. We now argue that the running time is Opn log2 nq with high probability if the input is strongly generated from a tree T . First, it is easy to see that a given recursive call on a subgraph with n0 vertices takes Opn0q time. Now, observe that if at each recursive call the pivot partitions the n0 vertices of its subgraph into buckets of size at most 2n0{3, then applying the master theorem implies a total running time of Opn log nq. Unfortunately, there are trees where picking an arbitrary vertex as a pivot yields a single bucket of size n´ 1.\nThus, consider the node N of T that is the first node reached by the walk from the root that always goes to the child tree with the higher number of leaves, stopping when the subtree of T rooted at N contains fewer than 2n{3 but at least n{3 leaves. Since T is strongly generating we have that the partition into B1, . . . , Bk induced by any vertex v P V pNq is such that any Bi contains less than 2n{3 vertices. Indeed, for any u such that LCAT pu, vq is an ancestor of N and x P V pNq, we have that wpu, vq ă wpx, vq, and so u and x belong to different parts of the partition B1, . . . , Bk.\nSince the number of vertices in V pNq is at least n{3, the probability of picking one of them is at least 1{3. Therefore, since the pivots are chosen independently, after c log n recursive calls, the probability of not picking a vertex of V pNq as a pivot is Op1{ncq. Taking the union bound yields the theorem."
    }, {
      "heading" : "7.3 Beyond Structured Inputs",
      "text" : "Since real-world inputs might sometimes differ from our definition of ground-truth inputs introduced in the Section 2, we introduce the notion of δ-adversarially-perturbed ground-truth inputs. This notion aims at accounting for noise in the data. We then design a simple and arguably more reliable algorithm (a robust variant of Algorithm 8) that achieves a δ-approximation for δ-adversariallyperturbed ground-truth inputs in Opnpn `mqq time. An interesting property of this algorithm is that its approximation guarantee is the same for any admissible objective function.\nWe first introduce the definition of δ-adversarially-perturbed ground-truth inputs. For any real δ ě 1, we say that a weighted graph G “ pV,E,wq is a δ-adversarially-perturbed ground-truth input if there exists an ultrametric pX, dq, such that V Ď X, and for every x, y P V, x ‰ y, e “ tx, yu exists, and fpdpx, yqq ď wpeq ď δfpdpx, yqq, where f : R` Ñ R` is a non-increasing function. This defines δ-adversarially-perturbed ground-truth inputs for similarity graphs and an analogous\ndefinition applies for dissimilarity graphs. We now introduce a robust, simple version of Algorithm 8 that returns a δ-approximation if the input is a δ-adversarially-perturbed ground-truth inputs. Algorithm 8 was partitioning the input graph based on a single, random vertex. In this slightly more robust version, the partition is built iteratively: Vertices are added to the current part if there exists at least one vertex in the current part or in the parts that were built before with which they share an edge of high enough weight (see Algorithm 9 for a complete description).\nAlgorithm 9 Robust and Simple Algorithm for Hierarchical Clustering on δ-adversariallyperturbed ground-truth inputs (similarity setting)\n1: Input: A graph G “ pV,Eq and a weight function w : E ÞÑ R`, a parameter δ 2: p Ð arbitrary vertex of V 3: i Ð 0 4: rVi Ð tpu 5: while rVi ‰ V do 6: Let p1 P rVi, p2 P V zrVi s.t. pp1, p2q is an edge of maximum weight in the cut prVi, V zrViq 7: wi Ð wpp1, p2q 8: Bi Ð tu | wpp1, uq “ wiu 9: while Du P V zp rVi YBiq s.t. Dv P Bi Y rVi, wpu, vq ě wi do\n10: Bi Ð Bi Y tuu. 11: end while 12: rVi`1 Ð rVi YBi 13: i Ð i` 1 14: end while 15: Let B1, . . . , Bk be the sets obtained 16: Apply the algorithm recursively on each GrBis and obtain a collection of trees T1, . . . , Tk 17: Define T ˚0 as a tree with p as a single vertex 18: For any 1 ď i ď k, define T ˚i to be the union of T ˚i´1 and Ti 19: Return T ˚k\nTheorem 7.8. For any admissible objective function, Algorithm 9 returns a δ-approximation if the input is a δ-adversarially-perturbed ground-truth input.\nTo prove the theorem we introduce the following lemma whose proof is temporarily differed. The lemma states that the tree built by the algorithm is almost generating (up to a factor of δ in the edge weights).\nLemma 7.9. Let T be a tree output by Algorithm 9, let N be the set of internal nodes of T . For any node N with children N1, N2 there exists a function ω : N ÞÑ R`, such that for any u P V pN1q, v P V pN2q, ωpNq ď wpu, vq ď δωpNq. Moreover, for any nodes N,N 1, if N 1 is an ancestor of N , we have that ωpNq ě ωpN 1q.\nAssuming Lemma 7.9, the proof of Theorem 7.8 is as follows.\nProof of Theorem 7.8. Let G “ pV,Eq, w : E ÞÑ R` be the input graph and T ˚ be a tree of optimal cost. By Lemma 7.9, the tree T output by the algorithm is such that for any node N with children N1, N2 there exists a real ωpNq, such that for any u P V pN1q, v P V pN2q, ω ď wpu, vq ď δω. Thus, consider the slightly different input graph G1 “ pV,E,w1q, where w1 : E ÞÑ R` is defined as follows.\nFor any edge pu, vq, define w1pu, vq “ ωpLCAT pu, vqq. Since by Lemma 7.9, for any nodes N,N 1 of T , if N 1 is an ancestor of N , we have that ωpNq ě ωpN 1q and by definition 2.2, T is generating for G1. Thus, for any admissible cost function, we have that for G1, costG1pT q ď costG1pT ˚q.\nFinally, observe that for any edge e, we have w1peq ď wpeq ď δw1peq. It follows that costGpT q ď δcostG1pT q for any admissible cost function and costG1pT ˚q ď costGpT ˚q. Therefore, costGpT q ď δcostGpT ˚q “ δOPT.\nProof of Lemma 7.9. We proceed by induction on the number of vertices in the graph (the base case is trivial). Consider the first recursive call of the algorithm. We show the following claim. Claim 7.10. For any 1 ď i ď k, for any y P rVi, x P Bi, wi ě wpx, yq ě wi{δ. Additionally, for any x, y P Bi, wpx, yq ě wi{δ.\nWe first argue that Claim 7.10 implies the lemma. Let T be the tree output by the algorithm. Consider the nodes on the path from p to the root of T ; Let Ni denote the node whose subtree is the union of T ˚i´1 and Ti. By definition, V pT ˚i´1q “ rVi and V pTiq “ Bi. Applying Claim 7.10 and observing that wi ą wi`1 implies that the lemma holds for all the nodes on the path. Finally, since for any edge tu, vu, for u, v P Bi, we also have wpu, vq ě wi{δ, combining with the inductive hypothesis on Bi implies the lemma for all the nodes of the subtree Ti.\nProof of Claim 7.10. Let pX, dq and f be a pair of ultrametric and function that is generating for G. Fix i P t1, . . . , ku. For any vertex x P Bi, let σpxq denote a vertex y that is in rVi or inserted to Bi before x and such that wpy, xq “ wi. For any vertex v, let σipxq denotes the vertex obtained by applying σ i times to x (i.e., σ2pxq “ σpσpxqq). By definition of the algorithm that for any x P Bi, Ds ě 1, such that σspxq P rVi.\nFix x P Bi. For any y P rVi, we have that wpy, xq ď wi since otherwise, the algorithm would have added x before.\nNow, let y P rVi or y inserted to Bi prior to x. We aim at showing that wpy, xq ě wi{δ. Observe that since X, d is an ultrametric, dpx, yq ď maxpdpx, σpxqq, dpσpxq, yqq.\nWe now “follow” σ by applying the function σ to σpxq and repeating until we reach σpxqℓ “ z P rVi. Combining with the definition of an ultrametric, it follows that\ndpx, yq ď maxpdpx, σpxqq, dpσpxq, σ2pxqq, . . . , dpσpxqℓ´1, zq, dpz, yqq.\nIf y was in rVi, we define ŷ “ y. Otherwise y is also in Bi (and so was added to Bi before x). We then proceed similarly than for x and “follow” σ. In this case, let ŷ “ σkpyq P rVi. Applying the definition of an ultrametric again, we obtain\ndpx, yq ď maxpdpx, σpxqq, dpσpxq, σ2pxqq, . . . , dpσℓ´1, zq, dpz, ŷq, dpy, σpyqq, . . . , dpσk´1pyq, ŷqq.\nAssume for now that dpz, ŷq is not greater than the others. Applying the definition of a δadversarially-perturbed input, we have that\nδwpx, yq ě minp. . . , wpσapxq, σa`1pxqq, . . . , wpσbpyq, σb`1pyqq, . . .q.\nFollowing the definition of σ, we have wpv, σpvqq ě wi, @v. Therefore, we conclude δwpx, yq ě wi. We thus turn to the case where dpz, ŷq is greater than the others. Since both z, ŷ P rVi, we have that they belong to some Bj0 , Bj1 , where j0, j1 ă i. We consider the minimum j such that a pair at distance at least dpz, ŷq was added to rVj . Consider such a pair u, v P rVj satisfying dpu, vq ě dpz, ŷq and suppose w.l.o.g that v P Bj´1 (we could have either u P Bj´1 or u P rVj´1). Again, we follow the\npath σpvq, σpσpvqq, . . ., until we reach σr1pvq P rVj´1 and similarly for u: σr2puq P rVj´1. Applying the definition of an ultrametric this yields that\ndpu, vq ď maxp. . . , dpσapuq, σa`1puqq, . . . , dpσbpvq, σb`1pvqq, . . . , dpσr1pvq, σr2puqqq. (14)\nNow the difference is that rVj´1 does not contain any pair at distance at least dpz, ŷq. Therefore, we have dpσr1pvq, σr2puqq ă dpz, ŷq. Moreover, recall that by definition of u, v, dpz, ŷq ď dpu, vq. Thus, dpσr1pvq, σr2puqq is not the maximum in Equation 14 since it is smaller than the left-hand side. Simplifying Equation 14 yields\ndpx, yq ă dpz, ŷq ď dpu, vq ď maxp. . . , dpσapuq, σa`1puqq, . . . , dpσbpvq, σb`1pvqq, . . .q.\nBy definition of a δ-adversarially-perturbed input, we obtain δwpx, yq ě minℓ wpσℓpbq, σℓ`1pbqq ě wj . Now, it is easy to see that for j ă i, wi ă wj and therefore δwpx, yq ě wi.\nWe conclude that for any y P rVi, x P Bi, wi ě wpx, yq ě wi{δ and for x, y P Bi, we have that wpx, yq ě wi{δ, as claimed."
    }, {
      "heading" : "8 Worst-Case Analysis of Common Heuristics",
      "text" : "The results presented in this section shows that for both the similarity and dissimilarity settings, some of the widely-used heuristics may perform badly. The proofs are not difficult nor particularly interesting, but the results stand in sharp contrast to structured inputs and help motivate our study of inputs beyond worst case.\nSimilarity Graphs. We show that for very simple input graphs (i.e., unweighted trees), the linkage algorithms (adapted to the similarity setting, see Algorithm 6) may perform badly.\nTheorem 8.1. There exists an infinite family of inputs on which the single-linkage and completelinkage algorithms output a solution of cost ΩpnOPT{ log nq.\nProof. The family of inputs consists of the graphs that represent paths of length n ą 2. More formally, Let Gn be a graph on n vertices such that V “ tv1, . . . , vnu and that has the following edge weights. Let wpvi´1, viq “ wpvi, vi`1q “ 1, for all 1 ă i ă n and for any i, j, j R ti´1, i, i`1u, define wpvi, vjq “ 0. Claim 8.2. OPTpGnq “ Opn log nq.\nProof. Consider the tree T ˚ that recursively divides the path into two subpaths of equal length. We aim at showing that costpT ˚q “ Opn log nq. The cost induced by the root node is n (since there is only one edge joining the two subpaths). The cost induced by each child of the root is n{2 since there is again only one edge joining the two sub-subpaths and now only n{2 leaves in the two subtrees. A direct induction shows that for a descendant at distance i from the root, the cost induced by this node is n{2i. Since the number of children at distance i is 2i, we obtain that the total cost induced by all the children at distance i is n. Since the tree divides the graph into two subgraph of equal size, there are at most Oplog nq levels and therefore, the total cost of T (and so OPTpGnq) is Opn log nq.\nComplete-Linkage. We show that the complete-linkage algorithm could perform a sequence of merges that would induce a tree of cost Ωpn2q. At start, each cluster contains a single vertex and so, the algorithm could merge any two clusters tviu, tvi`1u with 1 ď i ă n since their distance\nare all 1 (and it is the maximum edge weight in the graph). Suppose w.l.o.g that the algorithm merges v1, v2. This yields a cluster C1 such that the maximum distance between vertices of C1 and v3 is 1. Thus, assume w.l.o.g that the second merge of the algorithm is C1, v3. Again, this yields a cluster C2 whose maximum distance to v4 is also 1. A direct induction shows that the algorithm output a tree whose root induces the cut pV ztvnu, vnq and one of the child induces the cut pV ztvn´1, vnu, vn´1q and so on. We now argue that this tree T̂ has cost Ωpn2q. Indeed, for any 1 ă i ď n, we have V pLCA\nT̂ pvi´1, viqq “ i. Thus the cost is at least řn i“2 i “ Ωpn2q.\nSingle-Linkage. We now turn to the case of the single-linkage algorithm. Recall that the algorithm merges the two candidate clusters Ci, Cj that minimize wpu, vq for u P Ci, v P Cj.\nAt start, each cluster contains a single vertex and so, the algorithm could merge any two clusters tviu, tvju for j R ti ´ 1, i, i ` 1u since the edge weight is 0 (and it is the minimum edge weight). Suppose w.l.o.g that the algorithm merges v1, v3. This yields a cluster C1 such that the distance between vertices of C1 and any vi for i “ 1 mod 2 is 0. Thus, assume w.l.o.g that the second merge of the algorithm is C1, v5. A direct induction shows that w.l.o.g the output tree contains a node rN such that V p rN q contains all the vertices of odd indices. Now observe that the cost of the tree is at least |V p rNq| ¨ wpV p rN q, V zV p rNqq “ Ωpn2q.\nThus, by Claim 8.2 the single-linkage and complete-linkage algorithms can output a solution of cost ΩpnOPT{ log nq.\nTheorem 8.3. There exists an infinite family of inputs on which the average-linkage algorithm output a solution of cost Ωpn1{3OPTq.\nProof. For any n “ 2i for some integer i, we define a tree Tn “ pV,Eq as follows. Let k “ n1{3. Let P “ pu1, . . . , ukq be a path of length k (i.e., for each 1 ď i ă k, we have an edge between ui and ui`1). For each ui, we define a collection Pi “ tP i1 “ pV i1 , Ei1q, . . . , P ik “ pV ik , Eikqu of k paths of length k and for each P ij we connect one of its extremities to ui. Define Vi “ tuiu Ť j V i j .\nClaim 8.4. OPTpTnq ď 3n4{3\nProof. Consider the following non-binary solution tree T ˚: Let the root have children N1, . . . , Nk such that V pNiq “ Vi and for each child Ni let it have children N ji such that V pN j i q “ V j i . Finally, for each N ji let the subtree rooted at N j i be any tree.\nWe now analyze the cost of T ˚. Observe that for each edge e in the path P , we have |V pLCAT˚peqq| “ n. Moreover, for each edge e connecting a path P ij to ui, we have |V pLCAT˚peqq| “ k2 “ n2{3. Finally, for each edge e whose both endpoints are in a path P ij , we have that |V pLCAT˚peqq| ď k “ n1{3.\nWe now sum up over all edges to obtain the overall cost of Tn. There are k “ n1{3 edges in P ; They incur a cost of nk “ n4{3. There are k2 edges joining a vertex ui to a path P ij ; They incur a cost of k2 ¨ n2{3 “ n4{3. Finally, there are k3 edges whose both endpoints are in a path P ij ; They incur a cost of k3 ¨ n1{3 ď n4{3. Thus, the total cost of this tree is at most 3n4{3 ě OPTpTnq.\nWe now argue that there exists a sequence of merges done by the average-linkage algorithm that yield a solution of cost at least n5{3.\nClaim 8.5. There exists a sequence of merges and an integer t such that the candidate trees at time t have leaves sets ttu1, . . . , ukuu Ť i,jtV ij u.\nEquipped with this claim, we can finish the proof of the proposition. Since there is no edge between V ij and V i1 j1 for i 1 ‰ i or j1 ‰ j the distance between those trees in the algorithm will always be 0. However, the distance between the tree rT that has leaves set tu1, . . . , uku and any other tree\nis positive (since there is one edge joining those two sets of vertices in Tn). Thus, the algorithm will merge rT with some tree whose vertex set is exactly V ij for some i, j. For the same reasons, the resulting cluster will be merged to a cluster whose vertex set is exactly V i 1\nj1 , and so on. Hence, after\nn{2k “ k2{2 such merges, the tree rT has a leaves set of size k ¨ k2{2 “ n{2. However, the number of edges from this cluster to the other candidate clusters is k2{2 (since the other remaining clusters corresponds to vertex sets V ij for some i, j). For each such edge e we have |V pLCAT peqq| ě n{2. Since there are k2{2 of them, the resulting tree has cost Ωpn5{3q. Combining with Claim 8.4 yields the theorem.\nWe thus turn to the proof of Claim 8.5.\nProof of Claim 8.5. Given a graph G a set of candidate trees C, defineG{C to be the graph resulting from the contraction of all the edges whose both endpoints belong to the same cluster. We show a slightly stronger claim. We show that for any graph G and candidate trees V such that\n1. All the candidate clusters in V have the same size; and\n2. There exists a bijection φ between vertices v P Tn and vertices in G{C;\nThere exists a sequence of merges and an integer t such that the candidate trees at time t have leaves sets ttφpu1q, . . . , φpukquu Ť i,jtφpV ij qu where φpV ij q “ tφpvq | v P V ij u.\nThis slightly stronger statement yields the claim by observing that Tn and the candidate trees at the start of the algorithm satisfies the conditions of the statement.\nWe proceed by induction on the number of vertices of the graph. Let V ij “ tvijp1q, . . . , vijpkqu such that pvijpℓq, vijpℓ` 1qq P Eij for any 1 ď ℓ ă k, and pvijpkq, uiq P E.\nWe argue that the algorithm could perform a sequence of merges that results in the following set C of candidate trees. C contains candidate trees U i “ φpu2i´1qY φpu2iq for 1 ď i ă k{2, and for each i, j, candidate trees vi,j,ℓ “ φpvijp2ℓ´ 1q Y φpvijp2ℓqq, for 1 ď ℓ ă k{2. Let s0 be the number of vertices in each candidate tree.\nAt first, all the trees contain a single vertex and so, for each adjacent vertices of the graph the distance between their corresponding trees in the algorithm is 1{s0. For any non-adjacent pair of vertices, the corresponding trees are at distance 0. Thus, w.l.o.g assume the algorithm first merges u1, u2. Then, the distance between the newly created tree U\n1 and any other candidate tree C is 0 if there is no edge between u1 and u2 and C or 1{p2s0q if there is one (since U1 contains now two vertices). For the other candidate trees the distance is unchanged. Thus, the algorithm could merge vertices u3, u4. Now, observe that the distance between U\n2 and U1 is at most 1{p4s0q. Thus, it is possible to repeat the argument and assume that the algorithm merges the candidate trees corresponding to u5, u6. Repeating this argument k{2 times yields that after k{2 merges, the algorithm has generated the candidates trees U1, . . . , Uk{2´1. The other candidate trees still contain a single vertex. Thus, the algorithm is now forced to merge candidate trees that contains single vertices that are adjacent (since their distance is 1{s0 and any other distance is ă 1{s0). Assume, w.l.o.g, that the algorithm merges v11p1q, v11p2q. Again, applying a similar reasoning to each v11p2ℓ ´ 1q, v11p2ℓq yields the set of candidate clusters v1,1,1, . . . , v1,1,k{2´1. Applying this argument to all sets V ij yields that the algorithm could perform a sequence of merges that results in the set C of candidate clusters described above.\nNow, all the clusters have size 2s0 and there exists a bijection between vertices of G{C and Tn{2. Therefore, combining with the induction hypothesis yields the claim.\nDissimilarity Graphs. We now show that single-linkage, complete-linkage, and bisection 2- Center might return a solution that is arbitrarily bad compared to OPT in some cases. Hence, since average-linkage achieves a 2-approximation in the worst-case it seems that it is more robust than the other algorithms used in practice.\nTheorem 8.6. For each of the single-linkage, complete-linkage, and bisection 2-Center algorithms, there exists a family of inputs for which the algorithm outputs a solution of value OpOPT{nq. Proof. We define the family of inputs as follow. For any n ą 2, the graph Gn consists of n vertices V “ tv1, . . . , vn´1, uu and the edge weights are the following: For any i, j P t1, . . . , n´ 1u, wpvi, vjq “ 1, for any 1 ă i ď n ´ 1, wpvi, uq “ 1, and wpv1, uq “ W for some fixed W ě n3. Consider the tree T ˚ whose root induces a cut pV ztuu, tuuq. Then, the value of this tree (and so OPT) is at least nW , since |V pLCAT˚pv1, uqq| “ n.\nSingle-Linkage. At start, all the clusters are at distance 1 from each other except v1 and u that are at distance W . Thus, suppose that the first merge generates a candidate tree C1 whose leaves set is tv1, v2u. Now, since wpv2, uq “ 1, we have that all the clusters are at distance 1 from each other. Therefore, the next merge could possibly generate the cluster C2 with leaves sets tu, v1, v2u. Assume w.l.o.g that this is the case and let T be the tree output by the algorithm. We obtain |V pLCAT pu, v1qq| “ 3 and so, since for any vi, vj , |V pvi, vjq| ď n, valpT q ď n2 ` 3W ď 4W , since W ą W 3. Hence, valpT q “ OpvalpT ˚q{nq.\nComplete-Linkage. Again, at first all the clusters are at distance 1 from each other except v1 and u that are at distance W . Since the algorithm merges the two clusters that are at maximum distance, it merges u and v1. Again, let T be the tree output by the algorithm. We have valpT q ď n2 ` 2W ď 3W , since W ą W 3. Hence, valpT q “ OpvalpT ˚q{nq.\nBisection 2-Center. It is easy to see that for any location of the two centers, the cost of the clustering is 1. Thus, suppose that the algorithm locates centers in v2, v3 and that the induced partitioning is tv1, v2, uu, V ztv1, v2, uu. It follows that |V pLCAT pu, v1qq| ď 3 and so, valpT q ď n2 ` 3W ď 4W , since W ą W 3. Again, valpT q “ OpvalpT ˚q{nq.\nProposition 8.7. For any input I lying in a metric space, for any solution tree T for I, we have valpT q “ OpOPTq. Proof. Consider a solution tree T and the node u0 of T ˚ S that is the first node reached by the walk from the root that always goes to the child tree with the higher number of leaves, stopping when the subtree of T ˚S rooted at u0 contains fewer than 2n{3 leaves. Let A “ V pu0q, B “ V zV pu0q. Note that the number of edges in GrAs is at most a “ ` |A| 2 ˘ , the number of edges in GrBs is at\nmost b “ ` |B| 2 ˘ , whereas the number of edges in the cut pA,Bq is |A| ¨ |B|. Recall that n{3 ď |A|, |B| ď 2n{3 and so a, b “ Θp|A| ¨ |B|q. Finally observe that for each edge pu, vq P GrAs, we have wpu, vq ď wpu, xq ` wpx, vq for any x P B. Thus, since a ` b “ Θp|A| ¨ |B|q, by a simple counting argument, we deduce valpT q “ Ωpn ř ewpeqq and by Fact 6.1, ΩpOPTq."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors are grateful to Sanjoy Dasgupta for sharing thoughtful comments at various stages of this project."
    } ],
    "references" : [ {
      "title" : "Learning mixtures of arbitrary gaussians",
      "author" : [ "Sanjeev Arora", "Ravi Kannan" ],
      "venue" : "In Proceedings on 33rd Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Arora and Kannan.,? \\Q2001\\E",
      "shortCiteRegEx" : "Arora and Kannan.",
      "year" : 2001
    }, {
      "title" : "Expander flows, geometric embeddings and graph partitioning",
      "author" : [ "Sanjeev Arora", "Satish Rao", "Umesh V. Vazirani" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Arora et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2009
    }, {
      "title" : "Improved spectral-norm bounds for clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th International Workshop",
      "author" : [ "Pranjal Awasthi", "Or Sheffet" ],
      "venue" : "APPROX 2012, and 16th International Workshop,",
      "citeRegEx" : "Awasthi and Sheffet.,? \\Q2012\\E",
      "shortCiteRegEx" : "Awasthi and Sheffet.",
      "year" : 2012
    }, {
      "title" : "Stability yields a PTAS for k-median and k-means clustering",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet" ],
      "venue" : "In 51th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Awasthi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2010
    }, {
      "title" : "Center-based clustering under perturbation stability",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet" ],
      "venue" : "Inf. Process. Lett.,",
      "citeRegEx" : "Awasthi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2012
    }, {
      "title" : "Clustering under perturbation resilience",
      "author" : [ "Maria-Florina Balcan", "Yingyu Liang" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Balcan and Liang.,? \\Q2016\\E",
      "shortCiteRegEx" : "Balcan and Liang.",
      "year" : 2016
    }, {
      "title" : "A discriminative framework for clustering via similarity functions",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala" ],
      "venue" : "In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2008
    }, {
      "title" : "Approximate clustering without the approximation",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta" ],
      "venue" : "In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2009
    }, {
      "title" : "Agnostic clustering",
      "author" : [ "Maria-Florina Balcan", "Heiko Röglin", "Shang-Hua Teng" ],
      "venue" : "In Algorithmic Learning Theory, 20th International Conference,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2009
    }, {
      "title" : "Clustering under approximation stability",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2013
    }, {
      "title" : "Are stable instances easy? Combinatorics",
      "author" : [ "Yonatan Bilu", "Nathan Linial" ],
      "venue" : "Probability & Computing,",
      "citeRegEx" : "Bilu and Linial.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bilu and Linial.",
      "year" : 2012
    }, {
      "title" : "On the practically interesting instances of MAXCUT",
      "author" : [ "Yonatan Bilu", "Amit Daniely", "Nati Linial", "Michael E. Saks" ],
      "venue" : "In 30th International Symposium on Theoretical Aspects of Computer Science, STACS 2013,",
      "citeRegEx" : "Bilu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bilu et al\\.",
      "year" : 2013
    }, {
      "title" : "Isotropic PCA and affine-invariant clustering",
      "author" : [ "S. Charles Brubaker", "Santosh Vempala" ],
      "venue" : "Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008, October",
      "citeRegEx" : "Brubaker and Vempala.,? \\Q2008\\E",
      "shortCiteRegEx" : "Brubaker and Vempala.",
      "year" : 2008
    }, {
      "title" : "Characterization, stability and convergence of hierarchical clustering methods",
      "author" : [ "Gunnar Carlsson", "Facundo Mémoli" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Carlsson and Mémoli.,? \\Q2010\\E",
      "shortCiteRegEx" : "Carlsson and Mémoli.",
      "year" : 2010
    }, {
      "title" : "Likelihood based hierarchical clustering",
      "author" : [ "Rui M Castro", "Mark J Coates", "Robert D Nowak" ],
      "venue" : "IEEE Transactions on signal processing,",
      "citeRegEx" : "Castro et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Castro et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning mixtures of gaussians",
      "author" : [ "Sanjoy Dasgupta" ],
      "venue" : "Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Dasgupta.,? \\Q1999\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 1999
    }, {
      "title" : "A cost function for similarity-based hierarchical clustering",
      "author" : [ "Sanjoy Dasgupta" ],
      "venue" : "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,",
      "citeRegEx" : "Dasgupta.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 2016
    }, {
      "title" : "Performance guarantees for hierarchical clustering",
      "author" : [ "Sanjoy Dasgupta", "Philip M Long" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Dasgupta and Long.,? \\Q2005\\E",
      "shortCiteRegEx" : "Dasgupta and Long.",
      "year" : 2005
    }, {
      "title" : "A probabilistic analysis of EM for mixtures of separated, spherical gaussians",
      "author" : [ "Sanjoy Dasgupta", "Leonard J. Schulman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Dasgupta and Schulman.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dasgupta and Schulman.",
      "year" : 2007
    }, {
      "title" : "Graphons, mergeons, and so on",
      "author" : [ "Justin Eldridge", "Mikhail Belkin", "Yusu Wang" ],
      "venue" : "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Eldridge et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Eldridge et al\\.",
      "year" : 2016
    }, {
      "title" : "Inferring phylogenies, volume 2",
      "author" : [ "Joseph Felsenstein", "Joseph Felenstein" ],
      "venue" : "Sinauer Associates Sunderland,",
      "citeRegEx" : "Felsenstein and Felenstein.,? \\Q2004\\E",
      "shortCiteRegEx" : "Felsenstein and Felenstein.",
      "year" : 2004
    }, {
      "title" : "The elements of statistical learning, volume 1. Springer series in statistics",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Friedman et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2001
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "Wassily Hoeffding" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Hoeffding.,? \\Q1963\\E",
      "shortCiteRegEx" : "Hoeffding.",
      "year" : 1963
    }, {
      "title" : "Mathematical Taxonomy. Wiley series in probability and mathematical statistiscs",
      "author" : [ "N. Jardine", "R. Sibson" ],
      "venue" : null,
      "citeRegEx" : "Jardine and Sibson.,? \\Q1972\\E",
      "shortCiteRegEx" : "Jardine and Sibson.",
      "year" : 1972
    }, {
      "title" : "An impossibility theorem for clustering",
      "author" : [ "Jon Kleinberg" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kleinberg.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kleinberg.",
      "year" : 2002
    }, {
      "title" : "Clustering with spectral norm and the k-means algorithm",
      "author" : [ "Amit Kumar", "Ravindran Kannan" ],
      "venue" : "In 51th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Kumar and Kannan.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kumar and Kannan.",
      "year" : 2010
    }, {
      "title" : "A general approach for incremental approximation and hierarchical clustering",
      "author" : [ "Guolong Lin", "Chandrashekhar Nagarajan", "Rajmohan Rajaraman", "David P Williamson" ],
      "venue" : "In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm,",
      "citeRegEx" : "Lin et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2006
    }, {
      "title" : "Community detection and classification in hierarchical stochastic blockmodels",
      "author" : [ "Vince Lyzinski", "Minh Tang", "Avanti Athreya", "Youngser Park", "Carey E Priebe" ],
      "venue" : "IEEE Transactions on Network Science and Engineering,",
      "citeRegEx" : "Lyzinski et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lyzinski et al\\.",
      "year" : 2017
    }, {
      "title" : "Spectral partitioning of random graphs",
      "author" : [ "Frank McSherry" ],
      "venue" : "Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "McSherry.,? \\Q2001\\E",
      "shortCiteRegEx" : "McSherry.",
      "year" : 2001
    }, {
      "title" : "The effectiveness of Lloyd-type methods for the k-means problem",
      "author" : [ "Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Ostrovsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ostrovsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Approximation algorithms for hierarchical location problems",
      "author" : [ "C Greg Plaxton" ],
      "venue" : "In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Plaxton.,? \\Q2003\\E",
      "shortCiteRegEx" : "Plaxton.",
      "year" : 2003
    }, {
      "title" : "Hierarchical clustering via spreading metrics",
      "author" : [ "Aurko Roy", "Sebastian Pokutta" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "Roy and Pokutta.,? \\Q2016\\E",
      "shortCiteRegEx" : "Roy and Pokutta.",
      "year" : 2016
    }, {
      "title" : "A comparison of document clustering techniques",
      "author" : [ "Michael Steinbach", "George Karypis", "Vipin Kumar" ],
      "venue" : "KDD Workshop on Text Mining,",
      "citeRegEx" : "Steinbach et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Steinbach et al\\.",
      "year" : 2000
    }, {
      "title" : "Nonparametric graphon estimation",
      "author" : [ "Patrick J Wolfe", "Sofia C Olhede" ],
      "venue" : "arXiv preprint arXiv:1309.5936,",
      "citeRegEx" : "Wolfe and Olhede.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wolfe and Olhede.",
      "year" : 2013
    }, {
      "title" : "A uniqueness theorem for clustering",
      "author" : [ "Reza Bosagh Zadeh", "Shai Ben-David" ],
      "venue" : "In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence,",
      "citeRegEx" : "Zadeh and Ben.David.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zadeh and Ben.David.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Motivated by the fact that most work on hierarchical clustering was based on providing algorithms, rather than optimizing a specific objective, Dasgupta (2016) framed similarity-based hierarchical clustering as a combinatorial optimization problem, where a ‘good’ hierarchical clustering is one that minimizes some cost function.",
      "startOffset" : 144,
      "endOffset" : 160
    }, {
      "referenceID" : 15,
      "context" : "Motivated by the fact that most work on hierarchical clustering was based on providing algorithms, rather than optimizing a specific objective, Dasgupta (2016) framed similarity-based hierarchical clustering as a combinatorial optimization problem, where a ‘good’ hierarchical clustering is one that minimizes some cost function. He showed that this cost function has certain desirable properties, such as in order to achieve optimal cost disconnected components must be separated first and that in ‘structureless’ graphs, i.e., cliques, all clusterings achieve the same cost. We take an axiomatic approach to defining ‘good’ objective functions for both similarity and dissimilarity-based hierarchical clustering. We characterize a set of admissible objective functions (that includes the one introduced by Dasgupta) that have the property that when the input admits a ‘natural’ ground-truth hierarchical clustering, the ground-truth clustering has an optimal value. Equipped with a suitable objective function, we analyze the performance of practical algorithms, as well as develop better and faster algorithms for hierarchical clustering. For similaritybased hierarchical clustering, Dasgupta (2016) showed that a simple recursive sparsest-cut based approach achieves an Oplog nq-approximation on worst-case inputs.",
      "startOffset" : 144,
      "endOffset" : 1203
    }, {
      "referenceID" : 15,
      "context" : "Motivated by the fact that most work on hierarchical clustering was based on providing algorithms, rather than optimizing a specific objective, Dasgupta (2016) framed similarity-based hierarchical clustering as a combinatorial optimization problem, where a ‘good’ hierarchical clustering is one that minimizes some cost function. He showed that this cost function has certain desirable properties, such as in order to achieve optimal cost disconnected components must be separated first and that in ‘structureless’ graphs, i.e., cliques, all clusterings achieve the same cost. We take an axiomatic approach to defining ‘good’ objective functions for both similarity and dissimilarity-based hierarchical clustering. We characterize a set of admissible objective functions (that includes the one introduced by Dasgupta) that have the property that when the input admits a ‘natural’ ground-truth hierarchical clustering, the ground-truth clustering has an optimal value. Equipped with a suitable objective function, we analyze the performance of practical algorithms, as well as develop better and faster algorithms for hierarchical clustering. For similaritybased hierarchical clustering, Dasgupta (2016) showed that a simple recursive sparsest-cut based approach achieves an Oplog nq-approximation on worst-case inputs. We give a more refined analysis of the algorithm and show that it in fact achieves an Op ? lognq-approximation1. This improves upon the LP-based Oplog nq-approximation of Roy and Pokutta (2016). For dissimilarity-based hierarchical clustering, we show that the classic average-linkage algorithm gives a factor 2 approximation, and provide a simple and better algorithm that gives a factor 3{2 approximation.",
      "startOffset" : 144,
      "endOffset" : 1513
    }, {
      "referenceID" : 15,
      "context" : "Dasgupta (2016) identified the lack of a well-defined objective function as one of the reasons why the theoretical study of hierarchical clustering has lagged behind that of partition-based clustering.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "Dasgupta (2016) frames hierarchical clustering as a combinatorial optimization problem, where a good output tree is a tree that minimizes some cost function; but which function should that be? Each (binary) tree node is naturally associated to a cut that splits the cluster of its descendant leaves into the cluster of its left subtree on one side and the cluster of its right subtree on the other, and Dasgupta defines the objective to be the sum, over all tree nodes, of the total weight of edges crossing the cut multiplied by the cardinality of the node’s cluster.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "We consider, as potential objective functions, the class of all functions that sum, over all the nodes of the tree, the total weight of edges crossing the associated cut times some function of the cardinalities of the left and right clusters (this includes the class of functions considered by Dasgupta (2016)).",
      "startOffset" : 294,
      "endOffset" : 310
    }, {
      "referenceID" : 15,
      "context" : "We consider, as potential objective functions, the class of all functions that sum, over all the nodes of the tree, the total weight of edges crossing the associated cut times some function of the cardinalities of the left and right clusters (this includes the class of functions considered by Dasgupta (2016)). In Section 3 we characterize the ‘good’ objective functions in this class and call them admissible objective functions. We prove that for any objective function, for any ground-truth input, the ground-truth tree has optimal cost (w.r.t to the objective function) if and only if the objective function (1) is symmetric (independent of the left-right order of children), (2) is increasing in the cardinalities of the child clusters, and (3) for (unit-weight) cliques, has the same cost for all binary trees (Theorem 3.4). Dasgupta’s objective function is admissible in terms of the criteria described above. In Section 5, we consider random graphs that induce a natural clustering. This model can be seen as a noisy version of our notion of ground-truth inputs and a hierarchical stochastic block model. We show that the ground-truth tree has optimal expected cost for any admissible objective function. Furthermore, we show that the ground-truth tree has cost at most p1 ` op1qqOPT with high probability for the objective function introduced by Dasgupta (2016).",
      "startOffset" : 294,
      "endOffset" : 1372
    }, {
      "referenceID" : 14,
      "context" : "Algorithms for Similarity Graphs: Dasgupta (2016) shows that the recursive φ-approximate sparsest cut algorithm, that recursively splits the input graph using a φ-approximation to the sparsest cut problem, outputs a tree whose cost is at most Opφ log n  ̈ OPTq.",
      "startOffset" : 34,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "Algorithms for Similarity Graphs: Dasgupta (2016) shows that the recursive φ-approximate sparsest cut algorithm, that recursively splits the input graph using a φ-approximation to the sparsest cut problem, outputs a tree whose cost is at most Opφ log n  ̈ OPTq. Roy and Pokutta (2016) recently gave an Oplog nq-approximation by providing a linear programming relaxation for the problem and providing a clever rounding technique.",
      "startOffset" : 34,
      "endOffset" : 285
    }, {
      "referenceID" : 14,
      "context" : "Algorithms for Similarity Graphs: Dasgupta (2016) shows that the recursive φ-approximate sparsest cut algorithm, that recursively splits the input graph using a φ-approximation to the sparsest cut problem, outputs a tree whose cost is at most Opφ log n  ̈ OPTq. Roy and Pokutta (2016) recently gave an Oplog nq-approximation by providing a linear programming relaxation for the problem and providing a clever rounding technique. Charikar and Chatziafratis (2017) showed that the recursive φ-sparsest cut algorithm of Dasgupta gives an Opφq-approximation.",
      "startOffset" : 34,
      "endOffset" : 463
    }, {
      "referenceID" : 14,
      "context" : "Algorithms for Similarity Graphs: Dasgupta (2016) shows that the recursive φ-approximate sparsest cut algorithm, that recursively splits the input graph using a φ-approximation to the sparsest cut problem, outputs a tree whose cost is at most Opφ log n  ̈ OPTq. Roy and Pokutta (2016) recently gave an Oplog nq-approximation by providing a linear programming relaxation for the problem and providing a clever rounding technique. Charikar and Chatziafratis (2017) showed that the recursive φ-sparsest cut algorithm of Dasgupta gives an Opφq-approximation. In Section 4, we obtain an independent proof showing that the φ-approximate sparsest cut algorithm is an Opφq-approximation (Theorem 4.1). Our proof is quite different from the proof of Charikar and Chatziafratis (2017) and relies on a charging argument.",
      "startOffset" : 34,
      "endOffset" : 775
    }, {
      "referenceID" : 1,
      "context" : "Combined with the celebrated result of Arora et al. (2009), this yields an Op ? log nq-approximation.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 15,
      "context" : "For the objective function proposed in his work, Dasgupta (2016) shows that finding a cluster tree that minimizes the cost function is NP-hard.",
      "startOffset" : 49,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "For the objective function proposed in his work, Dasgupta (2016) shows that finding a cluster tree that minimizes the cost function is NP-hard. This directly applies to the admissible objective functions for the dissimilarity setting as well. Thus, the focus turns to developing approximation algorithms. Our analysis shows that the algorithm achieves a 6.75φ-approximation and the analysis of Charikar and Chatziafratis (2017) yields a 8φ-approximation guarantee.",
      "startOffset" : 49,
      "endOffset" : 428
    }, {
      "referenceID" : 31,
      "context" : "Structured Inputs and Beyond-Worst-Case Analysis: The recent work of Roy and Pokutta (2016) and Charikar and Chatziafratis (2017) have shown that obtaining constant approximation guarantees for worst-case inputs is beyond current techniques (see Section 1.",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 31,
      "context" : "Structured Inputs and Beyond-Worst-Case Analysis: The recent work of Roy and Pokutta (2016) and Charikar and Chatziafratis (2017) have shown that obtaining constant approximation guarantees for worst-case inputs is beyond current techniques (see Section 1.",
      "startOffset" : 69,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "This approach bears similarity to the stability-based conditions used by Balcan et al. (2008) and Bilu and Linial (2012).",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "This approach bears similarity to the stability-based conditions used by Balcan et al. (2008) and Bilu and Linial (2012). We provide an algorithm that achieves a δ-approximation in both the similarity and dissimilarity settings, independent of the objective function used as long as it is admissible according to the criteria used in Section 3.",
      "startOffset" : 73,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "The recent paper of Dasgupta (2016) served as the starting point of this work.",
      "startOffset" : 20,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "The recent paper of Dasgupta (2016) served as the starting point of this work. Dasgupta (2016) defined an objective function for hierarchical clustering and thus formulated the question of constructing a cluster tree as a combinatorial optimization problem.",
      "startOffset" : 20,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "The recent paper of Dasgupta (2016) served as the starting point of this work. Dasgupta (2016) defined an objective function for hierarchical clustering and thus formulated the question of constructing a cluster tree as a combinatorial optimization problem. Dasgupta also showed that the resulting problem is NP-hard and that the recursive φ-sparsest-cut algorithm achieves an Opφ log nqapproximation. Dasgupta’s results have been improved in two subsequent papers. Roy and Pokutta (2016) wrote an integer program for the hierarchical clustering problem using a combinatorial characterization of the ultrametrics induced by Dasgupta’s cost function.",
      "startOffset" : 20,
      "endOffset" : 489
    }, {
      "referenceID" : 15,
      "context" : "The recent paper of Dasgupta (2016) served as the starting point of this work. Dasgupta (2016) defined an objective function for hierarchical clustering and thus formulated the question of constructing a cluster tree as a combinatorial optimization problem. Dasgupta also showed that the resulting problem is NP-hard and that the recursive φ-sparsest-cut algorithm achieves an Opφ log nqapproximation. Dasgupta’s results have been improved in two subsequent papers. Roy and Pokutta (2016) wrote an integer program for the hierarchical clustering problem using a combinatorial characterization of the ultrametrics induced by Dasgupta’s cost function. They also provide a spreading metric LP and a rounding algorithm based on sphere/region-growing that yields an Oplog nq-approximation. Finally, they show that no polynomial size SDP can achieve a constant factor approximation for the problem and that under the Small Set Expansion (SSE) hypothesis, no polynomial-time algorithm can achieve a constant factor approximation. Charikar and Chatziafratis (2017) also gave a proof that the problem is hard to approximate within any constant factor under the Small Set Expansion hypothesis.",
      "startOffset" : 20,
      "endOffset" : 1057
    }, {
      "referenceID" : 15,
      "context" : "The recent paper of Dasgupta (2016) served as the starting point of this work. Dasgupta (2016) defined an objective function for hierarchical clustering and thus formulated the question of constructing a cluster tree as a combinatorial optimization problem. Dasgupta also showed that the resulting problem is NP-hard and that the recursive φ-sparsest-cut algorithm achieves an Opφ log nqapproximation. Dasgupta’s results have been improved in two subsequent papers. Roy and Pokutta (2016) wrote an integer program for the hierarchical clustering problem using a combinatorial characterization of the ultrametrics induced by Dasgupta’s cost function. They also provide a spreading metric LP and a rounding algorithm based on sphere/region-growing that yields an Oplog nq-approximation. Finally, they show that no polynomial size SDP can achieve a constant factor approximation for the problem and that under the Small Set Expansion (SSE) hypothesis, no polynomial-time algorithm can achieve a constant factor approximation. Charikar and Chatziafratis (2017) also gave a proof that the problem is hard to approximate within any constant factor under the Small Set Expansion hypothesis. They also proved that the recursive φ-sparsest cut algorithm produces a hierarchical clustering with cost at most OpφOPTq; their techniques appear to be significantly different from ours. Additionally, Charikar and Chatziafratis (2017) introduce a spreading metric SDP relaxation for the hierarchical clustering problem introduced by Dasgupta that has integrality gap Op ? log nq and a spreading metric LP relaxation that yields an Oplog nq-approximation to the problem.",
      "startOffset" : 20,
      "endOffset" : 1420
    }, {
      "referenceID" : 23,
      "context" : ", (Jardine and Sibson, 1972; Sneath and Sokal, 1962; Felsenstein and Felenstein, 2004; Castro et al., 2004).",
      "startOffset" : 2,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : ", (Jardine and Sibson, 1972; Sneath and Sokal, 1962; Felsenstein and Felenstein, 2004; Castro et al., 2004).",
      "startOffset" : 2,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : ", (Jardine and Sibson, 1972; Sneath and Sokal, 1962; Felsenstein and Felenstein, 2004; Castro et al., 2004).",
      "startOffset" : 2,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : ", (Friedman et al., 2001) and for divisive algorithms see e.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 14,
      "context" : ", (Jardine and Sibson, 1972; Sneath and Sokal, 1962; Felsenstein and Felenstein, 2004; Castro et al., 2004). Algorithms for hierarchical clustering have received a lot of attention from a practical perspective. For a definition and overview of agglomerative algorithms (such as average-linkage, complete-linkage, and single-linkage) see e.g., (Friedman et al., 2001) and for divisive algorithms see e.g., Steinbach et al. (2000). Most previous theoretical work on hierarchical clustering aimed at evaluating the cluster tree output by the linkage algorithms using the traditional objective functions for partition-based clustering, e.",
      "startOffset" : 87,
      "endOffset" : 429
    }, {
      "referenceID" : 30,
      "context" : ", (Plaxton, 2003; Dasgupta and Long, 2005; Lin et al., 2006)).",
      "startOffset" : 2,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : ", (Plaxton, 2003; Dasgupta and Long, 2005; Lin et al., 2006)).",
      "startOffset" : 2,
      "endOffset" : 60
    }, {
      "referenceID" : 26,
      "context" : ", (Plaxton, 2003; Dasgupta and Long, 2005; Lin et al., 2006)).",
      "startOffset" : 2,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "Previous work also proved that average-linkage can be useful to recover an underlying partition-based clustering when it exists under certain stability conditions (see (Balcan et al., 2008; Balcan and Liang, 2016)).",
      "startOffset" : 168,
      "endOffset" : 213
    }, {
      "referenceID" : 5,
      "context" : "Previous work also proved that average-linkage can be useful to recover an underlying partition-based clustering when it exists under certain stability conditions (see (Balcan et al., 2008; Balcan and Liang, 2016)).",
      "startOffset" : 168,
      "endOffset" : 213
    }, {
      "referenceID" : 4,
      "context" : ", (Awasthi et al., 2012; Balcan et al., 2013, 2008; Ostrovsky et al., 2012)).",
      "startOffset" : 2,
      "endOffset" : 75
    }, {
      "referenceID" : 29,
      "context" : ", (Awasthi et al., 2012; Balcan et al., 2013, 2008; Ostrovsky et al., 2012)).",
      "startOffset" : 2,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : ", (Bilu and Linial, 2012; Balcan et al., 2009a, 2013) for deterministic conditions and e.",
      "startOffset" : 2,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : ", (Awasthi et al., 2010; Awasthi and Sheffet, 2012; Kumar and Kannan, 2010)).",
      "startOffset" : 2,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : ", (Awasthi et al., 2010; Awasthi and Sheffet, 2012; Kumar and Kannan, 2010)).",
      "startOffset" : 2,
      "endOffset" : 75
    }, {
      "referenceID" : 25,
      "context" : ", (Awasthi et al., 2010; Awasthi and Sheffet, 2012; Kumar and Kannan, 2010)).",
      "startOffset" : 2,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously.",
      "startOffset" : 8,
      "endOffset" : 460
    }, {
      "referenceID" : 1,
      "context" : ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously.",
      "startOffset" : 8,
      "endOffset" : 497
    }, {
      "referenceID" : 1,
      "context" : ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously. This approach was applied to hierarchical clustering algorithms by Carlsson and Mémoli (2010) who showed that in the case of hierarchical clustering one gets a positive result, unlike the impossibility result of Kleinberg.",
      "startOffset" : 8,
      "endOffset" : 674
    }, {
      "referenceID" : 1,
      "context" : ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously. This approach was applied to hierarchical clustering algorithms by Carlsson and Mémoli (2010) who showed that in the case of hierarchical clustering one gets a positive result, unlike the impossibility result of Kleinberg. Their focus was on finding an ultrametric (on the datapoints) that is the closest to the metric (in which the data lies) in terms of the Gromov-Hausdorf distance. Our approach is completely different as we focus on defining objective functions and use these for quantitative analyses of algorithms. Our condition for inputs to have a ground-truth cluster tree, and especially their δ-adversarially perturbed versions, can be to be in the same spirit as that of the stability condition of Bilu and Linial (2012) or Bilu et al.",
      "startOffset" : 8,
      "endOffset" : 1314
    }, {
      "referenceID" : 1,
      "context" : ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously. This approach was applied to hierarchical clustering algorithms by Carlsson and Mémoli (2010) who showed that in the case of hierarchical clustering one gets a positive result, unlike the impossibility result of Kleinberg. Their focus was on finding an ultrametric (on the datapoints) that is the closest to the metric (in which the data lies) in terms of the Gromov-Hausdorf distance. Our approach is completely different as we focus on defining objective functions and use these for quantitative analyses of algorithms. Our condition for inputs to have a ground-truth cluster tree, and especially their δ-adversarially perturbed versions, can be to be in the same spirit as that of the stability condition of Bilu and Linial (2012) or Bilu et al. (2013): the input induces a natural clustering to be recovered whose cost is optimal.",
      "startOffset" : 8,
      "endOffset" : 1336
    }, {
      "referenceID" : 1,
      "context" : ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously. This approach was applied to hierarchical clustering algorithms by Carlsson and Mémoli (2010) who showed that in the case of hierarchical clustering one gets a positive result, unlike the impossibility result of Kleinberg. Their focus was on finding an ultrametric (on the datapoints) that is the closest to the metric (in which the data lies) in terms of the Gromov-Hausdorf distance. Our approach is completely different as we focus on defining objective functions and use these for quantitative analyses of algorithms. Our condition for inputs to have a ground-truth cluster tree, and especially their δ-adversarially perturbed versions, can be to be in the same spirit as that of the stability condition of Bilu and Linial (2012) or Bilu et al. (2013): the input induces a natural clustering to be recovered whose cost is optimal. It bears some similarities with the “strict separation” condition of Balcan et al. (2008), while we do not require the separation to be strict, we do require some additional hierarchical constraints.",
      "startOffset" : 8,
      "endOffset" : 1505
    }, {
      "referenceID" : 0,
      "context" : ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al.",
      "startOffset" : 2,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al.",
      "startOffset" : 2,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al.",
      "startOffset" : 2,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al. (2009b) for probabilistic conditions).",
      "startOffset" : 2,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al. (2009b) for probabilistic conditions). Imposing other conditions allows one to bypass hardness-of-approximation results for classical clustering objectives (such as k-means), and design efficient approximation algorithms (see, e.g., (Awasthi et al., 2010; Awasthi and Sheffet, 2012; Kumar and Kannan, 2010)). Eldridge et al. (2016) also investigate the question of understanding hierarchical cluster trees for random graphs generated from graphons.",
      "startOffset" : 2,
      "endOffset" : 449
    }, {
      "referenceID" : 13,
      "context" : ", (Carlsson and Mémoli, 2010)).",
      "startOffset" : 2,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : ", (Carlsson and Mémoli, 2010)).",
      "startOffset" : 2,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, as observed by Carlsson and Mémoli (2010), many practical hierarchical clustering algorithms such as the linkage based algorithms, actually output a dendogram equipped with a height function, that corresponds to an ultrametric embedding of the data.",
      "startOffset" : 28,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "Following the recent work of Dasgupta (2016), we adopt an approach in which a cost is assigned to each internal node of the tree T that corresponds to the quality of the split at that node.",
      "startOffset" : 29,
      "endOffset" : 45
    }, {
      "referenceID" : 15,
      "context" : "Following Dasgupta (2016), we restrict the search space for such cost functions.",
      "startOffset" : 10,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "We remark that Dasgupta (2016) defined gpa, bq “ a` b.",
      "startOffset" : 15,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "For the purpose of hierarchical clustering this form is fairly natural and indeed includes the specific cost function introduced by Dasgupta (2016). We could define the notion of admissibility for other forms of the cost function similarly and it would be of interest to understand whether they have properties that are desirable from the point of view of hierarchical clustering.",
      "startOffset" : 132,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "The fact that such functions exist already follows from the work of Dasgupta (2016), who showed that if gpn1, n2q “ n1 ` n2, then all cliques have the same cost.",
      "startOffset" : 68,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "The function proposed by Dasgupta (2016) is gpn, 1q “ n` 1, so this ratio is always 1.",
      "startOffset" : 25,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "This shows that the objective function proposed by Dasgupta (2016) is by no means unique.",
      "startOffset" : 51,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "In this section, we analyze the recursive φ-sparsest-cut algorithm (see Algorithm 1) that was described previously in Dasgupta (2016). For clarity, we work with the cost function introduced by Dasgupta (2016): The goal is to find a tree T minimizing costpT q “ NPT costpNq where for each node N of T with children N1, N2, costpNq “ wpV pN1q, V pN2qq  ̈ V pNq.",
      "startOffset" : 118,
      "endOffset" : 134
    }, {
      "referenceID" : 15,
      "context" : "In this section, we analyze the recursive φ-sparsest-cut algorithm (see Algorithm 1) that was described previously in Dasgupta (2016). For clarity, we work with the cost function introduced by Dasgupta (2016): The goal is to find a tree T minimizing costpT q “ NPT costpNq where for each node N of T with children N1, N2, costpNq “ wpV pN1q, V pN2qq  ̈ V pNq.",
      "startOffset" : 118,
      "endOffset" : 209
    }, {
      "referenceID" : 15,
      "context" : "In this section, we analyze the recursive φ-sparsest-cut algorithm (see Algorithm 1) that was described previously in Dasgupta (2016). For clarity, we work with the cost function introduced by Dasgupta (2016): The goal is to find a tree T minimizing costpT q “ NPT costpNq where for each node N of T with children N1, N2, costpNq “ wpV pN1q, V pN2qq  ̈ V pNq. We show that the φ-sparsest-cut algorithm achieves a 6.75φ-approximation. (Charikar and Chatziafratis (2017) also proved an Opφq approximation for Dasgupta’s function.",
      "startOffset" : 118,
      "endOffset" : 469
    }, {
      "referenceID" : 15,
      "context" : "For Dasgupta’s function, this was already proved in Charikar and Chatziafratis (2017) with a different constant.",
      "startOffset" : 4,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : "Observe (as in Dasgupta (2016)) that costpT  ̊q “ tu,vuPE |V pLCAT ̊pu, vqq|wpu, vq.",
      "startOffset" : 15,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "Indeed, adapting the definition of the balanced cut as in Dasgupta (2016) and rescaling the charge by a factor of fn imply the result.",
      "startOffset" : 58,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "4) which includes the cost function introduced by Dasgupta, we show the following: The cost of the ground-truth cluster tree is with high probability sharply concentrated (up to a factor of p1`op1qq around its expectation), and so of cost at most p1`op1qqOPT. This is further evidence that optimising admissible cost functions is an appropriate strategy for hierarchical clustering. We also provide a simple algorithm based on the SVD based approach of McSherry (2001) followed by a standard agglomerative heuristic yields a hierarchical clustering which is, up to a factor p1` op1qq, optimal with respect to smooth admissible cost functions.",
      "startOffset" : 50,
      "endOffset" : 469
    }, {
      "referenceID" : 27,
      "context" : ", Lyzinski et al. (2017). However, prior work has mostly focused on statistical hypothesis testing and exact recovery in some regimes.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 33,
      "context" : "This is similar to the approach taken by Wolfe and Olhede (2013) when considering random graph models generated according to graphons.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "The cost function introduced by Dasgupta (2016) satisfies the smoothness property.",
      "startOffset" : 32,
      "endOffset" : 48
    }, {
      "referenceID" : 22,
      "context" : "We will make use of the slightly generalized version of Hoeffding bounds (see Hoeffding (1963)).",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 28,
      "context" : "We use a result of McSherry (2001) who considers a random graph model with k clusters that is (slightly) more general than the HSBM considered here.",
      "startOffset" : 19,
      "endOffset" : 35
    }, {
      "referenceID" : 28,
      "context" : "We use a result of McSherry (2001) who considers a random graph model with k clusters that is (slightly) more general than the HSBM considered here. The difference is that there is no hierarchical structure on top of the k clusters in his setting; on the other hand, his goal is also simply to identify the k clusters and not any hierarchy upon them. The following theorem is derived from McSherry (2001) (Observation 11 and a simplification of Theorem 12).",
      "startOffset" : 19,
      "endOffset" : 405
    }, {
      "referenceID" : 28,
      "context" : "10 (McSherry (2001)).",
      "startOffset" : 4,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "Find T maximizing the value function corresponding to Dasgupta’s cost function of Section 4: valpT q “ NPT valpNq where for each node N of T with children N1, N2, valpNq “ wpV pN1q, V pN2qq  ̈V pNq. This optimization problem is NP-Hard Dasgupta (2016), hence we focus on approximation algorithms.",
      "startOffset" : 54,
      "endOffset" : 252
    } ],
    "year" : 2017,
    "abstractText" : "Hierarchical clustering is a recursive partitioning of a dataset into clusters at an increasingly finer granularity. Motivated by the fact that most work on hierarchical clustering was based on providing algorithms, rather than optimizing a specific objective, Dasgupta (2016) framed similarity-based hierarchical clustering as a combinatorial optimization problem, where a ‘good’ hierarchical clustering is one that minimizes some cost function. He showed that this cost function has certain desirable properties, such as in order to achieve optimal cost disconnected components must be separated first and that in ‘structureless’ graphs, i.e., cliques, all clusterings achieve the same cost. We take an axiomatic approach to defining ‘good’ objective functions for both similarity and dissimilarity-based hierarchical clustering. We characterize a set of admissible objective functions (that includes the one introduced by Dasgupta) that have the property that when the input admits a ‘natural’ ground-truth hierarchical clustering, the ground-truth clustering has an optimal value. Equipped with a suitable objective function, we analyze the performance of practical algorithms, as well as develop better and faster algorithms for hierarchical clustering. For similaritybased hierarchical clustering, Dasgupta (2016) showed that a simple recursive sparsest-cut based approach achieves an Oplog nq-approximation on worst-case inputs. We give a more refined analysis of the algorithm and show that it in fact achieves an Op ? lognq-approximation1. This improves upon the LP-based Oplog nq-approximation of Roy and Pokutta (2016). For dissimilarity-based hierarchical clustering, we show that the classic average-linkage algorithm gives a factor 2 approximation, and provide a simple and better algorithm that gives a factor 3{2 approximation. This aims at explaining the success of this heuristics in practice. Finally, we consider ‘beyond-worst-case’ scenario through a generalisation of the stochastic block model for hierarchical clustering. We show that Dasgupta’s cost function also has desirable properties for these inputs and we provide a simple algorithm that for graphs generated according to this model yields a 1 + o(1) factor approximation. Charikar and Chatziafratis (2017) independently proved that the sparsest-cut based approach achieves a Op ? log nq approximation.",
    "creator" : "LaTeX with hyperref package"
  }
}