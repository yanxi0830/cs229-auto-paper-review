{
  "name" : "1301.2310.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Policy Improvement for POMDPs using Normalized Importance Sampling",
    "authors" : [ "Christian R. Shelton" ],
    "emails" : [ ],
    "sections" : null,
    "references" : [ {
      "title" : "Approximate planning in large POMDPs via reusable trajectories",
      "author" : [ "Kearns et al", "M. 1999] Kearns", "Y. Mansour", "A. Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "al. et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1999
    }, {
      "title" : "Bayesian estimates of equation system param­ eters: An application of integration by monte carlo",
      "author" : [ "Kloek", "van Dijk", "T. 1978] Kloek", "H.K. van Dijk" ],
      "venue" : null,
      "citeRegEx" : "Kloek et al\\.,? \\Q1978\\E",
      "shortCiteRegEx" : "Kloek et al\\.",
      "year" : 1978
    }, {
      "title" : "Exploration in gradient-based reinforce­ ment learning",
      "author" : [ "Meuleau et al", "N. 2001] Meuleau", "L. Peshkin", "Kim", "K.-E" ],
      "venue" : "Technical Report AI-MEMO 2001-003,",
      "citeRegEx" : "al. et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2001
    }, {
      "title" : "Learning policies with external mem­ ory",
      "author" : [ "P. L" ],
      "venue" : "In Proceedings of the Sixteenth International Con­ ference on Machine Learning",
      "citeRegEx" : "L.,? \\Q1999\\E",
      "shortCiteRegEx" : "L.",
      "year" : 1999
    }, {
      "title" : "Bounds on sample size for policy eval­ uation in markov environments",
      "author" : [ "Peshkin", "Mukherjee", "L. 2001] Peshkin", "S. Mukher­ jee" ],
      "venue" : "In Fourteenth Annual Conference on Computational Learning Theory",
      "citeRegEx" : "Peshkin et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Peshkin et al\\.",
      "year" : 2001
    }, {
      "title" : "Off-policy temporal-difference learning with function approximation",
      "author" : [ "Precup et al", "D. 2001] Precup", "R.S. Sutton", "S. Das­ gupta" ],
      "venue" : "In Proceedings of the Eigh­ teenth International Conference on Machine Learning",
      "citeRegEx" : "al. et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2001
    }, {
      "title" : "Eligibility traces for off-polcy policy evalua­ tion",
      "author" : [ "D. Precup", "R.S. Sutton", "S. Singh" ],
      "venue" : "[Precup et a!.,",
      "citeRegEx" : "Precup et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Precup et al\\.",
      "year" : 2000
    }, {
      "title" : "Numerical Recipes in C",
      "author" : [ "Press et al", "W.H. 1992] Press", "S.A. Teukolsky", "Vet­ terling", "W. T", "B.P. Flannery" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1992
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "We present a new method for estimating the expected return of a POMDP from experi­ ence. The estimator does not assume any knowledge of the POMDP, can estimate the returns for finite state controllers, allows ex­ perience to be gathered from arbitrary se­ quences of policies, and estimates the return for any new policy. We motivate the estima­ tor from function-approximation and impor­ tance sampling points-of-view and derive its bias and variance. Although the estimator is biased, it has low variance and the bias is of­ ten irrelevant when the estimator is used for pair-wise comparisons. We conclude by ex­ tending the estimator to policies with mem­ ory and compare its performance in a greedy search algorithm to the REINFORCE algo­ rithm showing an order of magnitude reduc­ tion in the number of trials required.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}