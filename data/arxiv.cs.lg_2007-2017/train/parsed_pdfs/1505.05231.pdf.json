{
  "name" : "1505.05231.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bounds on the Minimax Rate for Estimating a Prior over a VC Class from Independent Learning Tasks",
    "authors" : [ "Liu Yang", "Steve Hanneke", "Jaime Carbonell" ],
    "emails" : [ "yangli@us.ibm.com", "steve.hanneke@gmail.com", "jgc@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 5.\n05 23\n1v 1\n[ cs\n.L G"
    }, {
      "heading" : "1 Introduction",
      "text" : "In the transfer learning setting, we are presented with a sequence of learning problems, each with some respective target concept we are tasked with learning. The key question in transfer learning is how to leverage our access to past learning problems in order to improve performance on learning problems we will be presented with in the future.\nAmong the several proposed models for transfer learning, one particularly appealing model supposes the learning problems are independent and identically distributed, with unknown distribution, and the advantage of transfer learning then comes from the ability to estimate this shared distribution based on the data from past learning problems [2,12]. For instance, when customizing a speech recognition system to a particular speaker’s voice, we might expect the first few people would need to speak many words or phrases in order for the system to accurately identify the nuances. However, after performing this for many different people, if the software has access to those past training sessions when customizing itself to a new user, it should have identified important properties of the speech patterns, such as the common patterns within each of the major dialects or accents, and other such information about the distribution of speech patterns\nwithin the user population. It should then be able to leverage this information to reduce the number of words or phrases the next user needs to speak in order to train the system, for instance by first trying to identify the individual’s dialect, then presenting phrases that differentiate common subpatterns within that dialect, and so forth.\nIn analyzing the benefits of transfer learning in such a setting, one important question to ask is how quickly we can estimate the distribution from which the learning problems are sampled. In recent work, [12] have shown that under mild conditions on the family of possible distributions, if the target concepts reside in a known VC class, then it is possible to estimate this distribtion using only a bounded number of training samples per task: specifically, a number of samples equal the VC dimension. However, that work left open the question of quantifying the rate of convergence. This rate of convergence can have a direct impact on how much benefit we gain from transfer learning when we are faced with only a finite sequence of learning problems. As such, it is certainly desirable to derive tight characterizations of this rate of convergence.\nThe present work continues that of [12], bounding the rate of convergence for estimating this distribution, under a smoothness condition on the distribution. We derive a generic upper bound, which holds regardless of the VC class the target concepts reside in. The proof of this result builds on that earlier work, but requires several interesting innovations to make the rate of convergence explicit, and to dramatically improve the upper bound implicit in the proofs of those earlier results. We further derive a nontrivial lower bound that holds for certain constructed scenarios, which illustrates a lower limit on how good of a general upper bound we might hope for in results expressed only in terms of the number of tasks, the smoothness conditions, and the VC dimension.\nWe additionally include an extension of the results of [12] to the setting of real-valued functions, establishing consistency (at a uniform rate) for an estimator of a prior over any VC subgraph class. In addition to the application to transfer learning, analogous to the original work of [12], we also discuss an application of this result to a preference elicitation problem in algorithmic economics, in which we are tasked with allocating items to a sequence of customers to approximately maximize the customers’ satisfaction, while permitted access to the customer valuation functions only via value queries."
    }, {
      "heading" : "2 The Setting",
      "text" : "Let (X ,BX ) be a measurable space [8] (where X is called the instance space), and let D be a distribution on X (called the data distribution). Let C be a VC class of measurable classifiers h : X → {−1,+1} (called the concept space), and denote by d the VC dimension of C [10]. We suppose C is equipped with its Borel σ-algebra B induced by the pseudo-metric ρ(h, g) = D({x ∈ X : h(x) 6= g(x)}). Though our results can be formulated for general D (with somewhat more complicated theorem statements), to simplify the statement of results we\nsuppose ρ is actually a metric, which would follow from appropriate topological conditions on C relative to D.\nFor any two probability measures µ1, µ2 on a measurable space (Ω,F), define the total variation distance\n‖µ1 − µ2‖ = sup A∈F µ1(A)− µ2(A).\nFor a set function µ on a finite measurable space (Ω,F), we abbreviate µ(ω) = µ({ω}), ∀ω ∈ Ω. Let ΠΘ = {πθ : θ ∈ Θ} be a family of probability measures on C (called priors), where Θ is an arbitrary index set (called the parameter space). We suppose there exists a probability measure π0 on C (the reference measure) such that every πθ is absolutely continuous with respect to π0, and therefore has a density function fθ given by the Radon-Nikodym derivative\ndπθ dπ0\n[8]. We consider the following type of estimation problem. There is a collection of C-valued random variables {h∗tθ : t ∈ N, θ ∈ Θ}, where for any fixed θ ∈ Θ the {h∗tθ} ∞ t=1 variables are i.i.d. with distribution πθ. For each θ ∈ Θ, there is a sequence Zt(θ) = {(Xt1, Yt1(θ)), (Xt2, Yt2(θ)), . . .}, where {Xti}t,i∈N are i.i.d. D, and for each t, i ∈ N, Yti(θ) = h∗tθ(Xti). We additionally denote by Z t k(θ) = {(Xt1, Yt1(θ)), . . . , (Xtk, Ytk(θ))} the first k elements of Zt(θ), for any k ∈ N, and similarly Xtk = {Xt1, . . . , Xtk} and Ytk(θ) = {Yt1(θ), . . . , Ytk(θ)}. Following the terminology used in the transfer learning literature, we refer to the collection of variables associated with each t collectively as the tth task. We will be concerned with sequences of estimators θ̂Tθ = θ̂T (Z1k(θ), . . . ,Z T k (θ)), for T ∈ N, which are based on only a bounded number k of samples per task, among the first T tasks. Our main results specifically study the case of d samples per task. For any such estimator, we measure the risk as E [\n‖πθ̂Tθ⋆ − πθ⋆‖\n]\n, and will be particularly\ninterested in upper-bounding the worst-case risk supθ⋆∈Θ E [ ‖πθ̂Tθ⋆ − πθ⋆‖ ] as a function of T , and lower-bounding the minimum possible value of this worst-case risk over all possible θ̂T estimators (called the minimax risk).\nIn previous work, [12] showed that, if ΠΘ is a totally bounded family, then even with only d number of samples per task, the minimax risk (as a function of the number of tasks T ) converges to zero. In fact, that work also proved this is not necessarily the case in general for any number of samples less than d. However, the actual rates of convergence were not explicitly derived in that work, and indeed the upper bounds on the rates of convergence implicit in that analysis may often have fairly complicated dependences on C, ΠΘ, and D, and furthermore often provide only very slow rates of convergence.\nTo derive explicit bounds on the rates of convergence, in the present work we specifically focus on families of smooth densities. The motivation for involving a notion of smoothness in characterizing rates of convergence is clear if we consider the extreme case in which ΠΘ contains two priors π1 and π2, with π1({h}) = π2({g}) = 1, where ρ(h, g) is a very small but nonzero value; in this case, if we have only a small number of samples per task, we would require many tasks (on the order of 1/ρ(h, g)) to observe any data points carrying any information that would distinguish between these two priors (namely, points x with h(x) 6= g(x));\nyet ‖π1−π2‖ = 1, so that we have a slow rate of convergence (at least initially). A total boundedness condition on ΠΘ would limit the number of such pairs present in ΠΘ, so that for instance we cannot have arbitrarily close h and g, but less extreme variants of this can lead to slow asymptotic rates of convergence as well. Specifically, in the present work we consider the following notion of smoothness. For L ∈ (0,∞) and α ∈ (0, 1], a function f : C → R is (L, α)-Hölder smooth if\n∀h, g ∈ C, |f(h)− f(g)| ≤ Lρ(h, g)α."
    }, {
      "heading" : "3 An Upper Bound",
      "text" : "We now have the following theorem, holding for an arbitrary VC class C and data distribution D; it is the main result of this work.\nTheorem 1. For ΠΘ any class of priors on C having (L, α)-Hölder smooth densities {fθ : θ ∈ Θ}, for any T ∈ N, there exists an estimator θ̂Tθ = θ̂T (Z1d (θ), . . . ,Z T d (θ)) such that\nsup θ⋆∈Θ E‖πθ̂T − πθ⋆‖ = Õ\n(\nLT− α2 2(d+2α)(α+2(d+1))\n)\n.\nProof. By the standard PAC analysis [9,3], for any γ > 0, with probability greater than 1 − γ, a sample of k = O((d/γ) log(1/γ)) random points will partition C into regions of width less than γ (under L1(D)). For brevity, we omit the t subscripts and superscripts on quantities such as Ztk(θ) throughout the following analysis, since the claims hold for any arbitrary value of t.\nFor any θ ∈ Θ, let π′θ denote a (conditional on X1, . . . , Xk) distribution defined as follows. Let f ′θ denote the (conditional onX1, . . . , Xk) density function of π′θ with respect to π0, and for any g ∈ C, let f ′ θ(g) = πθ({h∈C:∀i≤k,h(Xi)=g(Xi)}) π0({h∈C:∀i≤k,h(Xi)=g(Xi)}) (or 0 if π0({h ∈ C : ∀i ≤ k, h(Xi) = g(Xi)}) = 0). In other words, π′θ has the same probability mass as πθ for each of the equivalence classes induced by X1, . . . , Xk, but conditioned on the equivalence class, simply has a constantdensity distribution over that equivalence class. Note that every h ∈ C has f ′θ(h) between the smallest and largest values of fθ(g) among g ∈ C with ∀i ≤ k, g(Xi) = h(Xi); therefore, by the smoothness condition, on the event (of probability greater than 1 − γ) that each of these regions has diameter less than γ, we have ∀h ∈ C, |fθ(h)− f ′θ(h)| < Lγ α. On this event, for any θ, θ′ ∈ Θ,\n‖πθ − πθ′‖ = (1/2)\n∫\n|fθ − fθ′ |dπ0 < Lγ α + (1/2)\n∫\n|f ′θ − f ′ θ′ |dπ0.\nFurthermore, since the regions that define f ′θ and f ′ θ′ are the same (namely, the partition induced by X1, . . . , Xk), we have\n(1/2)\n∫\n|f ′θ − f ′ θ′ |dπ0 = (1/2)\n∑\ny1,...,yk∈{−1,+1}\n|πθ({h ∈ C : ∀i ≤ k, h(Xi) = yi})\n− πθ′({h ∈ C : ∀i ≤ k, h(Xi) = yi})|\n= ‖PYk(θ)|Xk − PYk(θ′)|Xk‖.\nThus, we have that with probability at least 1− γ,\n‖πθ − πθ′‖ < Lγ α + ‖PYk(θ)|Xk − PYk(θ′)|Xk‖.\nFollowing analogous to the inductive argument of [12], suppose I ⊆ {1, . . . , k}, fix x̄I ∈ X |I| and ȳI ∈ {−1,+1}|I|. Then the ỹI ∈ {−1,+1}|I| for which ‖ȳI − ỹI‖1 is minimal, subject to the constraint that no h ∈ C has h(x̄I) = ỹI , has (1/2)‖ȳI − ỹI‖1 ≤ d+ 1; also, for any i ∈ I with ȳi 6= ỹi, letting ȳ′j = ȳj for j ∈ I \\ {i} and ȳ′i = ỹi, we have\nPYI(θ)|XI (ȳI |x̄I) = PYI\\{i}(θ)|XI\\{i}(ȳI\\{i}|x̄I\\{i})− PYI(θ)|XI (ȳ ′ I |x̄I),\nand similarly for θ′, so that\n|PYI(θ)|XI (ȳI |x̄I)− PYI(θ′)|XI (ȳI |x̄I)|\n≤ |PYI\\{i}(θ)|XI\\{i}(ȳI\\{i}|x̄I\\{i})− PYI\\{i}(θ′)|XI\\{i}(ȳI\\{i}|x̄I\\{i})|\n+ |PYI (θ)|XI (ȳ ′ I |x̄I)− PYI(θ′)|XI (ȳ ′ I |x̄I)|.\nNow consider that these two terms inductively define a binary tree. Every time the tree branches left once, it arrives at a difference of probabilities for a set I of one less element than that of its parent. Every time the tree branches right once, it arrives at a difference of probabilities for a ȳI one closer to an unrealized ỹI than that of its parent. Say we stop branching the tree upon reaching a set I and a ȳI such that either ȳI is an unrealized labeling, or |I| = d. Thus, we can bound the original (root node) difference of probabilities by the sum of the differences of probabilities for the leaf nodes with |I| = d. Any path in the tree can branch left at most k − d times (total) before reaching a set I with only d elements, and can branch right at most d+1 times in a row before reaching a ȳI such that both probabilities are zero, so that the difference is zero. So the depth of any leaf node with |I| = d is at most (k−d)d. Furthermore, at any level of the tree, from left to right the nodes have strictly decreasing |I| values, so that the maximum width of the tree is at most k − d. So the total number of leaf nodes with |I| = d is at most (k − d)2d. Thus, for any ȳ ∈ {−1,+1}k and x̄ ∈ X k,\n|PYk(θ)|Xk(ȳ|x̄)− PYk(θ′)|Xk(ȳ|x̄)|\n≤ (k − d)2d · max ȳd∈{−1,+1}d max D∈{1,...,k}d |PYd(θ)|Xd(ȳ d|x̄D)− PYd(θ′)|Xd(ȳ d|x̄D)|.\nSince\n‖PYk(θ)|Xk − PYk(θ′)|Xk‖ = (1/2) ∑\nȳk∈{−1,+1}k\n|PYk(θ)|Xk(ȳ k)− PYk(θ′)|Xk(ȳ k)|,\nand by Sauer’s Lemma this is at most\n(ek)d max ȳk∈{−1,+1}k |PYk(θ)|Xk(ȳ k)− PYk(θ′)|Xk(ȳ k)|,\nwe have that\n‖PYk(θ)|Xk − PYk(θ′)|Xk‖\n≤ (ek)dk2d max ȳd∈{−1,+1}d max D∈{1,...,k}d |PYd(θ)|XD(ȳ d)− PYd(θ′)|XD(ȳ d)|.\nThus, we have that\n‖πθ − πθ′‖ = E‖πθ − πθ′‖\n< γ+Lγα+(ek)dk2dE\n[\nmax ȳd∈{−1,+1}d max D∈{1,...,k}d\nPYd(θ)|XD(ȳ d)− PYd(θ′)|XD (ȳ d)|\n]\n.\nNote that\nE\n[\nmax ȳd∈{−1,+1}d max D∈{1,...,k}d\n|PYd(θ)|XD(ȳ d)− PYd(θ′)|XD(ȳ d)|\n]\n≤ ∑\nȳd∈{−1,+1}d\n∑\nD∈{1,...,k}d\nE [ |PYd(θ)|XD(ȳ d)− PYd(θ′)|XD (ȳ d)| ]\n≤ (2k)d max ȳd∈{−1,+1}d max D∈{1,...,k}d E [ |PYd(θ)|XD(ȳ d)− PYd(θ′)|XD(ȳ d)| ] ,\nand by exchangeability, this last line equals\n(2k)d max ȳd∈{−1,+1}d E [ |PYd(θ)|Xd(ȳ d)− PYd(θ′)|Xd(ȳ d)| ] .\n[12] showed that E [ |PYd(θ)|Xd(ȳ d)− PYd(θ′)|Xd(ȳ d)| ] ≤ 4 √ ‖PZd(θ) − PZd(θ′)‖, so that in total we have ‖πθ − πθ′‖ < (L+1)γα+4(2ek)2d+2 √\n‖PZd(θ)−PZd(θ′)‖. Plugging in the value of k = c(d/γ) log(1/γ), this is\n(L+1)γα + 4\n(\n2ec d\nγ log\n(\n1\nγ\n))2d+2√\n‖PZd(θ)−PZd(θ′)‖.\nThus, it suffices to bound the rate of convergence (in total variation distance) of some estimator of PZd(θ⋆). IfN(ε) is the ε-covering number of {PZd(θ) : θ ∈ Θ}, then taking θ̂Tθ⋆ as the minimum distance skeleton estimate of [13,5] achieves expected total variation distance ε from PZd(θ⋆), for some T = O((1/ε 2) logN(ε/4)). We can partition C into O((L/ε)d/α) cells of diameter O((ε/L)1/α), and set a constant density value within each cell, on an O(ε)-grid of density values, and every prior with (L, α)-Hölder smooth density will have density within ε of some density so-constructed; there are then at most (1/ε)O((L/ε) d/α) such densities, so this bounds the covering numbers of ΠΘ. Furthermore, the covering number of ΠΘ upper bounds N(ε) [12], so that N(ε) ≤ (1/ε)O((L/ε) d/α).\nSolving T =O(ε−2(L/ε)d/α log(1/ε)) for ε, we have ε=O\n(\nL (\nlog(TL) T\n) α\nd+2α\n)\n.\nSo this bounds the rate of convergence for E‖PZd(θ̂T ) − PZd(θ⋆)‖, for θ̂T the\nminimum distance skeleton estimate. Plugging this rate into the bound on the priors, combined with Jensen’s inequality, we have\nE‖πθ̂T − πθ⋆‖ < (L+ 1)γ α + 4\n(\n2ec d\nγ log\n(\n1\nγ\n))2d+2\n×O\n(\nL\n(\nlog(TL)\nT\n) α\n2d+4α\n)\n.\nThis holds for any γ > 0, so minimizing this expression over γ > 0 yields a bound on the rate. For instance, with γ = Õ ( T− α 2(d+2α)(α+2(d+1)) ) , we have\nE‖πθ̂T − πθ⋆‖ = Õ\n(\nLT− α2 2(d+2α)(α+2(d+1))\n)\n.\n⊓⊔"
    }, {
      "heading" : "4 A Minimax Lower Bound",
      "text" : "One natural quesiton is whether Theorem 1 can generally be improved. While we expect this to be true for some fixed VC classes (e.g., those of finite size), and in any case we expect that some of the constant factors in the exponent may be improvable, it is not at this time clear whether the general form of T−Θ(α\n2/(d+α)2) is sometimes optimal. One way to investigate this question is to construct specific spaces C and distributions D for which a lower bound can be obtained. In particular, we are generally interested in exhibiting lower bounds that are worse than those that apply to the usual problem of density estimation based on direct access to the h∗tθ⋆ values (see Theorem 3 below).\nHere we present a lower bound that is interesting for this reason. However, although larger than the optimal rate for methods with direct access to the target concepts, it is still far from matching the upper bound above, so that the question of tightness remains open. Specifically, we have the following result.\nTheorem 2. For any integer d ≥ 1, any L > 0, α ∈ (0, 1], there is a value C(d, L, α) ∈ (0,∞) such that, for any T ∈ N, there exists an instance space X , a concept space C of VC dimension d, a distribution D over X , and a distribution π0 over C such that, for ΠΘ a set of distributions over C with (L, α)-Hölder smooth density functions with respect to π0, any estimator θ̂T = θ̂T (Z1d (θ⋆), . . . ,Z T d (θ⋆)) has\nsup θ⋆∈Θ E\n[ ‖πθ̂T − πθ⋆‖ ] ≥ C(d, L, α)T− α 2(d+α) .\nProof. (Sketch) We proceed by a reduction from the task of determining the bias of a coin from among two given possibilities. Specifically, fix any γ ∈ (0, 1/2), n ∈ N, and let B1(p), . . . , Bn(p) be i.i.d Bernoulli(p) random variables, for each p ∈ [0, 1]; then it is known that, for any (possibly nondeterministic) decision rule\np̂n : {0, 1}n → {(1 + γ)/2, (1− γ)/2},\n1\n2\n∑\np∈{(1+γ)/2,(1−γ)/2}\nP(p̂n(B1(p), . . . , Bn(p)) 6= p)\n≥ (1/32) · exp { −128γ2n/3 } . (1)\nThis easily follows from the results of [1], combined with a result of [7] bounding the KL divergence (see also [11])\nTo use this result, we construct a learning problem as follows. Fix some m ∈ N with m ≥ d, let X = {1, . . . ,m}, and let C be the space of all classifiers h : X → {−1,+1} such that |{x ∈ X : h(x) = +1}| ≤ d. Clearly the VC dimension of C is d. Define the distribution D as uniform over X . Finally, we specify a family of (L, α)-Hölder smooth priors, parameterized by Θ = {−1,+1}( m d), as follows. Let γm = (L/2)(1/m) α. First, enumerate the ( m d )\ndistinct d-sized subsets of {1, . . . ,m} as X1,X2, . . . ,X(md ) . Define the reference distribution π0 by the property that, for any h ∈ C, letting q = |{x : h(x) = +1}|, π0({h}) = (12 ) d ( m−q d−q ) / ( m d ) . For any b = (b1, . . . , b(md ) ) ∈ {−1, 1}( m d ), define the prior πb as the distribution of a random variable hb specified by the following generative model. Let i∗ ∼ Uniform({1, . . . , (\nm d\n)\n}), let Cb(i∗) ∼ Bernoulli((1 + γmbi∗)/2); finally, hb ∼ Uniform({h ∈ C : {x : h(x) = +1} ⊆ Xi∗ ,Parity(|{x : h(x) = +1}|) = Cb(i∗)}), where Parity(n) is 1 if n is odd, or 0 if n is even. We will refer to the variables in this generative model below. For any h ∈ C, letting H = {x : h(x) = +1} and q = |H |, we can equivalently express\nπb({h}) = ( 1 2 )\nd ( m d )−1∑(md) i=1 1[H ⊆ Xi](1 + γmbi) Parity(q)(1 − γmbi)1−Parity(q).\nFrom this explicit representation, it is clear that, letting fb = dπb dπ0\n, we have fb(h) ∈ [1− γm, 1+ γm] for all h ∈ C. The fact that fb is Hölder smooth follows from this, since every distinct h, g ∈ C have D({x : h(x) 6= g(x)}) ≥ 1/m = (2γm/L)\n1/α. Next we set up the reduction as follows. For any estimator π̂T = π̂T (Z1d(θ⋆),\n. . . ,ZTd (θ⋆)), and each i ∈ {1, . . . , ( m d ) }, let hi be the classifier with {x : hi(x) = +1} = Xi; also, if π̂T ({hi}) > ( 1 2 ) d/ ( m d ) , let b̂i = 2Parity(d) − 1, and otherwise b̂i = 1 − 2Parity(d). We use these b̂i values to estimate the original bi values. Specifically, let p̂i = (1 + γmb̂i)/2 and pi = (1 + γmbi)/2, where b = θ⋆. Then\n‖π̂T − πθ⋆‖ ≥ (1/2)\n(md ) ∑\ni=1\n|π̂T ({hi})− πθ⋆({hi})|\n≥ (1/2)\n(md ) ∑\ni=1\nγm\n2d ( m d\n) |b̂i − bi|/2 = (1/2)\n(md ) ∑\ni=1\n1 2d (\nm d\n) |p̂i − pi|.\nThus, we have reduced from the problem of deciding the biases of these ( m d ) independent Bernoulli random variables. To complete the proof, it suffices to lower bound the expectation of the right side for an arbitrary estimator.\nToward this end, we in fact study an even easier problem. Specifically, consider an estimator q̂i = q̂i(Z1d(θ⋆), . . . ,Z T d (θ⋆), i ∗ 1, . . . , i ∗ T ), where i ∗ t is the i\n∗ random variable in the generative model that defines h∗tθ⋆ ; that is, i ∗ t ∼ Uniform({1, . . . , (\nm d\n)\n}), Ct ∼ Bernoulli((1 + γmbi∗t )/2), and h ∗ tθ⋆ ∼ Uniform({h ∈ C : {x : h(x) = +1} ⊆ Xi∗t ,Parity(|{x : h(x) = +1}|) = Ct}), where the i ∗ t are independent across t, as are the Ct and h ∗ tθ⋆\n. Clearly the p̂i from above can be viewed as an estimator of this type, which simply ignores the knowledge of i∗t . The knowledge of these i∗t variables simplifies the analysis, since given {i ∗ t : t ≤ T }, the data can be partitioned into (\nm d\n)\ndisjoint sets, {{Ztd(θ⋆) : i ∗ t = i} : i = 1, . . . ,\n(\nm d\n)\n}, and we can use only the set {Ztd(θ⋆) : i ∗ t = i} to estimate pi. Furthermore, we can use only the subset of these for which Xtd = Xi, since otherwise we have zero information about the value of Parity(|{x : h∗tθ⋆(x) = +1}|). That is, given i∗t = i, any Z t d(θ⋆) is conditionally independent from every bj for j 6= i, and is even conditionally independent from bi when Xtd is not completely contained in Xi; specifically, in this case, regardless of bi, the conditional distribution of Ytd(θ⋆) given i ∗ t = i and given Xtd is a product distribution, which deterministically assigns label −1 to those Ytk(θ⋆) with Xtk /∈ Xi, and gives uniform random values to the subset of Ytd(θ⋆) with their respective Xtk ∈ Xi. Finally, letting rt = Parity(|{k ≤ d : Ytk(θ⋆) = +1}|), we note that given i∗t = i, Xtd = Xi, and the value rt, bi is conditionally independent from Ztd(θ⋆). Thus, the set of values CiT (θ⋆) = {rt : i∗t = i,Xtd = Xi} is a sufficient statistic for bi (hence for pi). Recall that, when i ∗ t = i and Xtd = Xi, the value of rt is equal to Ct, a Bernoulli(pi) random variable. Thus, we neither lose nor gain anything (in terms of risk) by restricting ourselves to estimators q̂i of the type q̂i = q̂i(Z1d(θ⋆), . . . ,Z T d (θ⋆), i ∗ 1, . . . , i ∗ T ) = q̂ ′ i(CiT (θ⋆)), for some q̂ ′ i [8]: that is, estimators that are a function of the NiT (θ⋆) = |CiT (θ⋆)| Bernoulli(pi) random variables, which we should note are conditionally i.i.d. given NiT (θ⋆).\nThus, by (1), for any n ≤ T ,\n1\n2\n∑\nbi∈{−1,+1}\nE\n[ |q̂i − pi| ∣ ∣ ∣ NiT (θ⋆) = n ] = 1\n2\n∑\nbi∈{−1,+1}\nγmP ( q̂i 6= pi\n∣ ∣ ∣ NiT (θ⋆) = n )\n≥ (γm/32) · exp { −128γ2mNi/3 } .\nAlso note that, for each i, E[Ni] = d!(1/m)d\n(md ) T ≤ (d/m)2dT = d2d(2γm/L)2d/αT .\nThus, Jensen’s inequality, linearity of expectation, and the law of total expectation imply\n1\n2\n∑\nbi∈{−1,+1}\nE [|q̂i − pi|] ≥ (γm/32) · exp { −43(2/L)2d/αd2dγ2+2d/αm T } .\nThus, by linearity of the expectation,\n(\n1\n2\n)(md ) ∑\nb∈{−1,+1}( m d )\nE\n\n \n(md ) ∑\ni=1\n1 2d (\nm d\n) |q̂i − pi|\n\n  =\n(md ) ∑\ni=1\n1 2d (\nm d\n)\n1\n2\n∑\nbi∈{−1,+1}\nE [|q̂i − pi|]\n≥ (γm/(32 · 2 d)) · exp\n{ −43(2/L)2d/αd2dγ2+2d/αm T } .\nIn particular, taking m = ⌈ (L/2)1/α ( 43(2/L)2d/αd2dT ) 1 2(d+α) ⌉\n, we have γm =\nΘ ( ( 43(2/L)2d/αd2dT )− α 2(d+α) ) , so that\n(\n1\n2\n)(md ) ∑\nb∈{−1,+1}( m d )\nE\n\n \n(md ) ∑\ni=1\n1 2d (\nm d\n) |q̂i − pi|\n\n \n= Ω\n(\n2−d ( 43(2/L)2d/αd2dT )− α 2(d+α)\n)\n.\nIn particular, this implies there exists some b for which\nE\n\n \n(md) ∑\ni=1\n1 2d (\nm d\n) |q̂i − pi|\n\n  = Ω\n(\n2−d ( 43(2/L)2d/αd2dT )− α 2(d+α)\n)\n.\nApplying this lower bound to the estimator p̂i above yields the result. ⊓⊔\nIt is natural to wonder how these rates might potentially improve if we allow θ̂T to depend on more than d samples per data set. To establish limits on such improvements, we note that in the extreme case of allowing the estimator to depend on the full Zt(θ⋆) data sets, we may recover the known results lower bounding the risk of density estimation from i.i.d. samples from a smooth density, as indicated by the following result.\nTheorem 3. For any integer d ≥ 1, there exists an instance space X , a concept space C of VC dimension d, a distribution D over X , and a distribution π0 over C such that, for ΠΘ the set of distributions over C with (L, α)-Hölder smooth density functions with respect to π0, any sequence of estimators, θ̂T = θ̂T (Z1(θ⋆), . . . ,ZT (θ⋆)) (T = 1, 2, . . .), has\nsup θ⋆∈Θ E\n[ ‖πθ̂T − πθ⋆‖ ] = Ω ( T− α d+2α ) .\nThe proof is a simple reduction from the problem of estimating πθ⋆ based on direct access to h∗1θ⋆ , . . . , h ∗ Tθ⋆\n, which is essentially equivalent to the standard model of density estimation, and indeed the lower bound in Theorem 3 is a wellknown result for density estimation from T i.i.d. samples from a Hölder smooth density in a d-dimensional space [5]."
    }, {
      "heading" : "5 Real-Valued Functions and an Application in Algorithmic Economics",
      "text" : "In this section, we present results generalizing the analysis of [12] to classes of real-valued functions. We also present an application of this generalization to a preference elicitation problem."
    }, {
      "heading" : "5.1 Consistent Estimation of Priors over Real-Valued Functions at a Bounded Rate",
      "text" : "In this section, we let B denote a σ-algebra on X × R, and again let BX denote the corresponding σ-algebra on X . Also, for measurable functions h, g : X → R, let ρ(h, g) = ∫\n|h − g|dPX , where PX is a distribution over X . Let F be a class of functions X → R with Borel σ-algebra BF induced by ρ. Let Θ be a set, and for each θ ∈ Θ, let πθ denote a probability measure on (F ,BF ). We suppose {πθ : θ ∈ Θ} is totally bounded in total variation distance, and that F is a uniformly bounded VC subgraph class with pseudodimension d. We also suppose ρ is a metric when restricted to F .\nAs above, let {Xti}t,i∈N be i.i.d. PX random variables. For each θ ∈ Θ, let {h∗tθ}t∈N be i.i.d. πθ random variables, independent from {Xti}t,i∈N. For each t ∈ N and θ ∈ Θ, let Yti(θ) = h∗tθ(Xti) for i ∈ N, and let Z\nt(θ) = {(Xt1, Yt1(θ)), (Xt2, Yt2(θ)), . . .}; for each k ∈ N, define Ztk(θ) = {(Xt1, Yt1(θ)), . . . , (Xtk, Ytk(θ))}, Xtk = {Xt1, . . . , Xtk}, and Ytk(θ) = {Yt1(θ), . . . , Ytk(θ)}.\nWe have the following result. The proof parallels that of [12] (who studied the special case of binary functions), with a few important twists (in particular, a significantly different approach in the analogue of their Lemma 3). The details are included in Appendix A.\nTheorem 4. There exists an estimator θ̂Tθ⋆ = θ̂T (Z 1 d(θ⋆), . . . ,Z T d (θ⋆)), and functions R : N0 × (0, 1] → [0,∞) and δ : N0 × (0, 1] → [0, 1] such that, for any α > 0, lim\nT→∞ R(T, α) = lim T→∞ δ(T, α) = 0 and for any T ∈ N0 and θ⋆ ∈ Θ,\nP\n(\n‖πθ̂Tθ⋆ − πθ⋆‖ > R(T, α)\n)\n≤ δ(T, α) ≤ α."
    }, {
      "heading" : "5.2 Maximizing Customer Satisfaction in Combinatorial Auctions",
      "text" : "Theorem 4 has a clear application in the context of transfer learning, following analogous arguments to those given in the special case of binary classification by [12]. In addition to that application, we can also use Theorem 4 in the context of the following problem in algorithmic economics, where the objective is to serve a sequence of customers so as to maximize their satisfaction.\nConsider an online travel agency, where customers go to the site with some idea of what type of travel they are interested in; the site then poses a series of questions to each customer, and identifies a travel package that best suits their desires, budget, and dates. There are many options of travel packages, with\noptions on location, site-seeing tours, hotel and room quality, etc. Because of this, serving the needs of an arbitrary customer might be a lengthy process, requiring many detailed questions. Fortunately, the stream of customers is typically not a worst-case sequence, and in particular obeys many statistical regularities: in particular, it is not too far from reality to think of the customers as being independent and identically distributed samples. With this assumption in mind, it becomes desirable to identify some of these statistical regularities so that we can pose the questions that are typically most relevant, and thereby more quickly identify the travel package that best suits the needs of the typical customer. One straightforward way to do this is to directly estimate the distribution of customer value functions, and optimize the questioning system to minimize the expected number of questions needed to find a suitable travel package.\nOne can model this problem in the style of Bayesian combinatorial auctions, in which each customer has a value function for each possible bundle of items. However, it is slightly different, in that we do not assume the distribution of customers is known, but rather are interested in estimating this distribution; the obtained estimate can then be used in combination with methods based on Bayesian decision theory. In contrast to the literature on Bayesian auctions (and subjectivist Bayesian decision theory in general), this technique is able to maintain general guarantees on performance that hold under an objective interpretation of the problem, rather than merely guarantees holding under an arbitrary assumed prior belief. This general idea is sometimes referred to as Empirical Bayesian decision theory in the machine learning and statistics literatures. The ideal result for an Empirical Bayesian algorithm is to be competitive with the corresponding Bayesian methods based on the actual distribution of the data (assuming the data are random, with an unknown distribution); that is, although the Empirical Bayesian methods only operate with a data-based estimate of the distribution, the aim is to perform nearly as well as methods based on the true (unobservable) distribution. In this work, we present results of this type, in the context of an abstraction of the aforementioned online travel agency problem, where the measure of performance is the expected number of questions to find a suitable package.\nThe specific application we are interested in here may be expressed abstractly as a kind of combinatorial auction with preference elicitation. Specifically, we suppose there is a collection of items on a menu, and each possible bundle of items has an associated fixed price. There is a stream of customers, each with a valuation function that provides a value for each possible bundle of items. The objective is to serve each customer a bundle of items that nearly-maximizes his or her surplus value (value minus price). However, we are not permitted direct observation of the customer valuation functions; rather, we may query for the value of any given bundle of items; this is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]). The objective is to achieve this near-maximal surplus guarantee, while making only a small number of queries per customer. We suppose the customer valuation function are sampled i.i.d. according to an unknown distri-\nbution over a known (but arbitrary) class of real-valued functions having finite pseudo-dimension. Reasoning that knowledge of this distribution should allow one to make a smaller number of value queries per customer, we are interested in estimating this unknown distribution, so that as we serve more and more customers, the number of queries per customer required to identify a near-optimal bundle should decrease. In this context, we in fact prove that in the limit, the expected number of queries per customer converges to the number required of a method having direct knowledge of the true distribution of valuation functions.\nFormally, suppose there is a menu of n items [n] = {1, . . . , n}, and each bundle B ⊆ [n] has an associated price p(B) ≥ 0. Suppose also there is a sequence of customers, each with a valuation function vt : 2\n[n] → R. We suppose these vt functions are i.i.d. samples. We can then calculate the satisfaction function for each customer as st(x), where x ∈ {0, 1}\nn, and st(x) = vt(Bx) − p(Bx), where Bx ⊆ [n] contains element i ∈ [n] iff xi = 1.\nNow suppose we are able to ask each customer a number of questions before serving up a bundle Bx̂t to that customer. More specifically, we are able to ask for the value st(x) for any x ∈ {0, 1}n. This is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]). We are interested in asking as few questions as possible, while satisfying the guarantee that E[st(x̂t)−maxx st(x)] ≤ ε.\nNow suppose, for every π and ε, we have a method A(π, ε) such that, given that π is the actual distribution of the st functions, A(π, ε) guarantees that the x̂t value it selects has E[maxx st(x) − st(x̂t)] ≤ ε; also let N̂t(π, ε) denote the actual (random) number of queries the method A(π, ε) would ask for the st function, and let Q(π, ε) = E[N̂t(π, ε)]. We suppose the method never queries any st(x) value twice for a given t, so that its number of queries for any given t is bounded.\nAlso suppose F is a VC subgraph class of functions mapping X = {0, 1}n into [−1, 1] with pseudodimension d, and that {πθ : θ ∈ Θ} is a known totally bounded family of distributions over F such that the st functions have distribution πθ⋆ for some unknown θ⋆ ∈ Θ. For any θ ∈ Θ and γ > 0, let B(θ, γ) = {θ′ ∈ Θ : ‖πθ − πθ′‖ ≤ γ}.\nSuppose, in addition to A, we have another method A′(ε) that is not πdependent, but still provides the ε-correctness guarantee, and makes a bounded number of queries (e.g., in the worst case, we could consider querying all 2n points, but in most cases there are more clever π-independent methods that use far fewer queries, such as O(1/ε2)). Consider the following method; the quantities\nθ̂Tθ⋆ , R(T, α), and δ(T, α) from Theorem 4 are here considered with respect PX taken as the uniform distribution on {0, 1}n.\nThe following theorem indicates that this method is correct, and furthermore that the long-run average number of queries is not much worse than that of a method that has direct knowledge of πθ⋆ . The proof of this result parallels that of [12] for the transfer learning setting, but is included here for completeness.\nAlgorithm 1 An algorithm for sequentially maximizing expected customer satisfaction.\nfor t = 1, 2, . . . , T do Pick points Xt1, Xt2, . . . , Xtd uniformly at random from {0, 1} n\nif R(t− 1, ε/2) > ε/8 then Run A′(ε) Take x̂t as the returned value else\nLet θ̌tθ⋆ ∈ B ( θ̂(t−1)θ⋆ , R(t− 1, ε/2) ) be such that\nQ(πθ̌tθ⋆ , ε/4) ≤ min θ∈B(θ̂(t−1)θ⋆ ,R(t−1,ε/2)) Q(πθ, ε/4) +\n1 t\nRun A(πθ̌tθ⋆ , ε/4) and let x̂t be its return value\nend if\nend for\nTheorem 5. For the above method, ∀t ≤ T,E[maxx st(x) − st(x̂t)] ≤ ε. Furthermore, if ST (ε) is the total number of queries made by the method, then\nlim sup T→∞\nE[ST (ε)]\nT ≤ Q(πθ⋆ , ε/4) + d.\nProof. By Theorem 4, for any t ≤ T , if R(t−1, ε/2) ≤ ε/8, then with probability at least 1 − ε/2, ‖πθ⋆ − πθ̂(t−1)θ⋆ ‖ ≤ R(t − 1, ε/2), so that a triangle inequality implies ‖πθ⋆ − πθ̌tθ⋆‖ ≤ 2R(t− 1, ε/2) ≤ ε/4. Thus,\nE\n[\nmax x\nst(x)− st(x̂t) ]\n≤ ε/2 + E [ E [\nmax x\nst(x) − st(x̂t) ∣ ∣ ∣θ̌tθ⋆ ] 1 [ ‖πθ̌tθ⋆ − πθ⋆‖ ≤ ε/2 ]] .\nFor θ ∈ Θ, let x̂tθ denote the point x that would be returned by A(πθ̌tθ⋆ , ε/4) when queries are answered by some stθ ∼ πθ instead of st (and supposing st = stθ⋆). If ‖πθ̌tθ⋆ − πθ⋆‖ ≤ ε/4, then\nE\n[\nmax x\nst(x)− st(x̂t) ∣ ∣ ∣θ̌tθ⋆ ] = E [\nmax x\nstθ⋆(x)− stθ⋆(x̂t) ∣ ∣ ∣θ̌tθ⋆ ]\n≤ E [\nmax x stθ̌tθ⋆ (x) − stθ̌tθ⋆ (x̂tθ̌tθ⋆ ) ∣ ∣ ∣θ̌tθ⋆ ] + ‖πθ̌tθ⋆ − πθ⋆‖ ≤ ε/4 + ε/4 = ε/2.\nPlugging into the above bound, we have E [maxx st(x)− st(x̂t)] ≤ ε. For the result on ST (ε), first note that R(t−1, ε/2) > ε/8 only finitely many times (due to R(t, α) = o(1)), so that we can ignore those values of t in the asymptotic calculation (as the number of queries is always bounded), and rely on the correctness guarantee of A′. For the remaining values t, let Nt denote the number of queries made by A(πθ̌tθ⋆ , ε/4). Then\nlim sup T→∞\nE[ST (ε)]\nT ≤ d+ lim sup\nT→∞\nT ∑\nt=1\nE [Nt]\nT .\nSince\nlim T→∞\n1\nT\nT ∑\nt=1\nE\n[\nNt1[‖πθ̂(t−1)θ⋆ − πθ⋆‖ > R(t− 1, ε/2)]\n]\n≤ lim T→∞\n1\nT\nT ∑\nt=1\n2nP (\n‖πθ̂(t−1)θ⋆ − πθ⋆‖ > R(t− 1, ε/2)\n)\n≤ 2n lim T→∞\n1\nT\nT ∑\nt=1\nδ(t− 1, ε/2) = 0,\nwe have\nlim sup T→∞\nT ∑\nt=1\nE [Nt]\nT = lim sup\nT→∞\n1\nT\nT ∑\nt=1\nE\n[\nNt1[‖πθ̂(t−1)θ⋆ − πθ⋆‖ ≤ R(t− 1, ε/2)]\n]\n.\nFor t ≤ T , let Nt(θ̌tθ⋆) denote the number of queries A(πθ̌tθ⋆ , ε/4) would make if queries were answered with stθ̌tθ⋆ instead of st. On the event ‖πθ̂(t−1)θ⋆ − πθ⋆‖ ≤ R(t− 1, ε/2), we have\nE\n[\nNt\n∣ ∣ ∣ θ̌tθ⋆ ] ≤ E [ Nt(θ̌tθ⋆) ∣ ∣ ∣ θ̌tθ⋆ ] + 2R(t− 1, ε/2)\n= Q(πθ̌tθ⋆, ε/4) + 2R(t−1, ε/2) ≤ Q(πθ⋆ , ε/4) + 2R(t−1, ε/2)+ 1/t.\nTherefore,\nlim sup T→∞\n1\nT\nT ∑\nt=1\nE\n[\nNt1[‖πθ̂(t−1)θ⋆ − πθ⋆‖ ≤ R(t− 1, ε/2)]\n]\n≤ Q(πθ⋆ , ε/4) + lim sup T→∞\n1\nT\nT ∑\nt=1\n2R(t− 1, ε/2) + 1/t = Q(πθ⋆ , ε/4).\n⊓⊔\nIn many cases, this result will even continue to hold with an infinite number of goods (n = ∞), since Theorem 4 has no dependence on the cardinality of the space X ."
    }, {
      "heading" : "6 Open Problems",
      "text" : "There are several interesting questions that remain open at this time. Can either the lower bound or upper bound be improved in general? If, instead of d samples per task, we instead use m ≥ d samples, how does the minimax risk vary with m? Related to this, what is the optimal value of m to optimize the rate of convergence as a function of mT , the total number of samples? More generally, if an estimator is permitted to use N total samples, taken from however many tasks it wishes, what is the optimal rate of convergence as a function of N?"
    }, {
      "heading" : "A Proofs for Section 5",
      "text" : "The proof of Theorem 4 is based on the following sequence of lemmas, which parallel those used by [12] for establishing the analogous result for consistent estimation of priors over binary functions. The last of these lemmas (namely, Lemma 3) requires substantial modifications to the original argument of [12]; the others use arguments more-directly based on those of [12].\nLemma 1. For any θ, θ′ ∈ Θ and t ∈ N,\n‖πθ − πθ′‖ = ‖PZt(θ) − PZt(θ′)‖.\nProof. Fix θ, θ′ ∈ Θ, t ∈ N. Let X = {Xt1, Xt2, . . .}, Y(θ) = {Yt1(θ), Yt2(θ), . . .}, and for k ∈ N let Xk = {Xt1, . . . , Xtk}. and Yk(θ) = {Yt1(θ), . . . , Ytk(θ)}. For h ∈ F , let cX(h) = {(Xt1, h(Xt1)), (Xt2, h(Xt2)), . . .}.\nFor h, g ∈ F , define ρX(h, g) = lim m→∞\n1 m ∑m i=1 |h(Xti) − g(Xti)| (if the limit\nexists), and ρXk(h, g) = 1 k ∑k i=1 |h(Xti)−g(Xti)|. Note that since F is a uniformly bounded VC subgraph class, so is the collection of functions {|h− g| : h, g ∈ F}, so that the uniform strong law of large numbers implies that with probability one, ∀h, g ∈ F , ρX(h, g) exists and has ρX(h, g) = ρ(h, g) [10].\nConsider any θ, θ′ ∈ Θ, and any A ∈ BF . Then any h /∈ A has ∀g ∈ A, ρ(h, g) > 0 (by the metric assumption). Thus, if ρX(h, g) = ρ(h, g) for all h, g ∈ F , then ∀h /∈ A,\n∀g ∈ A, ρX(h, g) = ρ(h, g) > 0 =⇒\n∀g ∈ A, cX(h) 6= cX(g) =⇒ cX(h) /∈ cX(A).\nThis implies c−1 X (cX(A)) = A. Under these conditions,\nPZt(θ)|X(cX(A)) = πθ(c −1 X (cX(A))) = πθ(A),\nand similarly for θ′. Any measurable set C for the range of Zt(θ) can be expressed as C = {cx̄(h) : (h, x̄) ∈ C′} for some appropriate C′ ∈ BF⊗B∞X . Letting C ′ x̄ = {h : (h, x̄) ∈ C\n′}, we have\nPZt(θ)(C) =\n∫\nπθ(c −1 x̄ (cx̄(C ′ x̄)))PX(dx̄) =\n∫\nπθ(C ′ x̄)PX(dx̄) = P(h∗tθ,X)(C ′).\nLikewise, this reasoning holds for θ′. Then\n‖PZt(θ) − PZt(θ′)‖ = ‖P(h∗tθ,X) − P(h∗tθ′ ,X)‖\n= sup C′∈BF⊗B∞X\n∣ ∣ ∣ ∣ ∫ (πθ(C ′ x̄)− πθ′(C ′ x̄))PX(dx̄) ∣ ∣ ∣ ∣\n≤\n∫\nsup A∈BF |πθ(A)− πθ′(A)|PX(dx̄) = ‖πθ − πθ′‖.\nSince h∗tθ and X are independent, ∀A ∈ BF , πθ(A) = Ph∗tθ (A) = Ph∗tθ (A)PX(X ∞) = P(h∗tθ,X)(A×X ∞). Analogous reasoning holds for h∗tθ′ . Thus, we have\n‖πθ − πθ′‖ = ‖P(h∗tθ,X)(· × X ∞)− P(h∗ tθ′ ,X)(· × X ∞)‖\n≤ ‖P(h∗tθ,X) − P(h∗tθ′ ,X)‖ = ‖PZ t(θ) − PZt(θ′)‖.\nAltogether, we have ‖PZt(θ) − PZt(θ′)‖ = ‖πθ − πθ′‖. ⊓⊔\nLemma 2. There exists a sequence rk = o(1) such that, ∀t, k ∈ N, ∀θ, θ′ ∈ Θ,\n‖PZtk(θ) − PZtk(θ′)‖ ≤ ‖πθ − πθ′‖ ≤ ‖PZtk(θ) − PZtk(θ′)‖+ rk.\nProof. This proof follows identically to a proof of [12], but is included here for completeness. Since PZtk(θ)(A) = PZt(θ)(A × (X × R) ∞) for all measurable A ⊆ (X × R)k, and similarly for θ′, we have\n‖PZt k (θ) − PZt k (θ′)‖ = sup A∈Bk PZt k (θ)(A)− PZt k (θ′)(A)\n= sup A∈Bk\nPZt(θ)(A× (X × R) ∞)− PZt(θ′)(A× (X × R) ∞)\n≤ sup A∈B∞ PZt(θ)(A)− PZt(θ′)(A) = ‖PZt(θ) − PZt(θ′)‖,\nwhich implies the left inequality when combined with Lemma 1. Next, we focus on the right inequality. Fix θ, θ′ ∈ Θ and γ > 0, and let B ∈ B∞ be such that\n‖πθ − πθ′‖ = ‖PZt(θ) − PZt(θ′)‖ < PZt(θ)(B)− PZt(θ′)(B) + γ.\nLet A = {A × (X × R)∞ : A ∈ Bk, k ∈ N}. Note that A is an algebra that generates B∞. Thus, Carathéodory’s extension theorem (specifically, the version presented by [8]) implies that there exist disjoint sets {Ai}i∈N in A such that B ⊆ ⋃\ni∈N Ai and\nPZt(θ)(B) − PZt(θ′)(B) < ∑\ni∈N\nPZt(θ)(Ai)− ∑\ni∈N\nPZt(θ′)(Ai) + γ.\nSince these Ai sets are disjoint, each of these sums is bounded by a probability value, which implies that there exists some n ∈ N such that\n∑\ni∈N\nPZt(θ)(Ai) < γ +\nn ∑\ni=1\nPZt(θ)(Ai),\nwhich implies\n∑\ni∈N\nPZt(θ)(Ai)− ∑\ni∈N\nPZt(θ′)(Ai) < γ + n ∑\ni=1\nPZt(θ)(Ai)− n ∑\ni=1\nPZt(θ′)(Ai)\n= γ + PZt(θ)\n(\nn ⋃\ni=1\nAi\n)\n− PZt(θ′)\n(\nn ⋃\ni=1\nAi\n)\n.\nAs ⋃n i=1 Ai ∈ A, there exists m ∈ N and measurable Bm ∈ B m such that ⋃n i=1 Ai = Bm × (X × R) ∞, and therefore\nPZt(θ)\n(\nn ⋃\ni=1\nAi\n)\n− PZt(θ′)\n(\nn ⋃\ni=1\nAi\n)\n= PZtm(θ)(Bm)− PZtm(θ′)(Bm)\n≤ ‖PZtm(θ) − PZtm(θ′)‖ ≤ limk→∞ ‖PZtk(θ) − PZtk(θ′)‖.\nCombining the above, we have ‖πθ − πθ′‖ ≤ limk→∞ ‖PZtk(θ)−PZtk(θ′)‖+3γ. By letting γ approach 0, we have\n‖πθ − πθ′‖ ≤ lim k→∞ ‖PZtk(θ) − PZtk(θ′)‖.\nSo there exists a sequence rk(θ, θ ′) = o(1) such that\n∀k ∈ N, ‖πθ − πθ′‖ ≤ ‖PZtk(θ) − PZtk(θ′)‖+ rk(θ, θ ′).\nNow let γ > 0 and let Θγ be a minimal γ-cover of Θ. Define the quantity rk(γ) = maxθ,θ′∈Θγ rk(θ, θ\n′). Then for any θ, θ′ ∈ Θ, let θγ = argminθ′′∈Θγ ‖πθ−πθ′′‖ and θ′γ = argminθ′′∈Θγ ‖πθ′ − πθ′′‖. Then a triangle inequality implies that ∀k ∈ N,\n‖πθ − πθ′‖ ≤ ‖πθ − πθγ‖+ ‖πθγ − πθ′γ‖+ ‖πθ′γ − πθ′‖\n< 2γ + rk(θγ , θ ′ γ) + ‖PZtk(θγ) − PZtk(θ′γ)‖ ≤ 2γ + rk(γ) + ‖PZtk(θγ) − PZtk(θ′γ)‖.\nTriangle inequalities and the left inequality from the lemma statement (already established) imply\n‖PZtk(θγ)−PZtk(θ′γ)‖ ≤ ‖PZtk(θγ)−PZtk(θ)‖+ ‖PZtk(θ)−PZtk(θ′)‖+ ‖PZtk(θ′γ)−PZtk(θ′)‖\n≤ ‖πθγ − πθ‖+ ‖PZtk(θ) − PZtk(θ′)‖+ ‖πθ′γ − πθ′‖ < 2γ + ‖PZtk(θ) − PZtk(θ′)‖.\nSo in total we have\n‖πθ − πθ′‖ ≤ 4γ + rk(γ) + ‖PZtk(θ) − PZtk(θ′)‖.\nSince this holds for all γ > 0, defining rk = infγ>0(4γ+rk(γ)), we have the right inequality of the lemma statement. Furthermore, since each rk(θ, θ\n′) = o(1), and |Θγ | < ∞, we have rk(γ) = o(1) for each γ > 0, and thus we also have rk = o(1).\n⊓⊔\nLemma 3. ∀t, k ∈ N, there exists a monotone function Mk(x) = o(1) such that, ∀θ, θ′ ∈ Θ,\n‖PZtk(θ) − PZtk(θ′)‖ ≤ Mk ( ‖PZtd(θ) − PZtd(θ′)‖ ) .\nProof. Fix any t ∈ N, and let X = {Xt1, Xt2, . . .} andY(θ) = {Yt1(θ), Yt2(θ), . . .}, and for k ∈ N let Xk = {Xt1, . . . , Xtk} and Yk(θ) = {Yt1(θ), . . . , Ytk(θ)}.\nIf k ≤ d, then PZtk(θ)(·) = PZtd(θ)(· × (X × {−1,+1}) d−k), so that\n‖PZtk(θ) − PZtk(θ′)‖ ≤ ‖PZtd(θ) − PZtd(θ′)‖,\nand therefore the result trivially holds. Now suppose k > d. Fix any γ > 0, and let Bθ,θ′ ⊆ (X ×R)k be a measurable set such that\nPZtk(θ) (Bθ,θ′)− PZtk(θ′)(Bθ,θ′) ≤ ‖PZtk(θ) − PZtk(θ′)‖\n≤ PZtk(θ)(Bθ,θ′)− PZtk(θ′)(Bθ,θ′) + γ.\nBy Carathéodory’s extension theorem (specifically, the version presented by [8]), there exists a disjoint sequence of sets {Bi(θ, θ′)}∞i=1 such that\nPZt k (θ)(Bθ,θ′)− PZt k (θ′)(Bθ,θ′) < γ +\n∞ ∑\ni=1\nPZt k (θ)(Bi(θ, θ\n′))− ∞ ∑\ni=1\nPZt k (θ′)(Bi(θ, θ\n′)),\nand such that each Bi(θ, θ ′) is representable as follows; for some ℓi(θ, θ ′) ∈ N, and sets Cij = (Aij1 × (−∞, tij1])× · · · × (Aijk × (−∞, tijk]), for j ≤ ℓi(θ, θ′), where each Aijp ∈ BX , the set Bi(θ, θ′) is representable as ⋃\ns∈Si\n⋂ℓi(θ,θ ′)\nj=1 Dijs, where\nSi ⊆ {0, . . . , 2ℓi(θ,θ ′) − 1}, each Dijs ∈ {Cij , Ccij}, and s 6= s ′ ⇒ ⋂ℓi(θ,θ ′) j=1 Dijs ∩ ⋂ℓi(θ,θ ′)\nj=1 Dijs′ = ∅. Since the Bi(θ, θ ′) are disjoint, the above sums are bounded,\nso that there exists mk(θ, θ ′, γ) ∈ N such that every m ≥ mk(θ, θ′, γ) has\nPZtk(θ) (Bθ,θ′)− PZtk(θ′)(Bθ,θ′) < 2γ+\nm ∑\ni=1\nPZtk(θ) (Bi(θ, θ\n′))− m ∑\ni=1\nPZtk(θ ′)(Bi(θ, θ\n′)),\nNow define M̃k(γ) = maxθ,θ′∈Θγ mk(θ, θ ′, γ). Then for any θ, θ′ ∈ Θ, let θγ , θ′γ ∈ Θγ be such that ‖πθ − πθγ‖ < γ and ‖πθ′ − πθ′γ‖ < γ, which implies ‖PZtk(θ) − PZtk(θγ) ‖ < γ and ‖PZtk(θ′) − PZtk(θ′γ)‖ < γ by Lemma 2. Then\n‖PZtk(θ) − PZtk(θ′)‖ < ‖PZtk(θγ) − PZtk(θ′γ)‖+ 2γ\n≤ PZtk(θγ)(Bθγ ,θ′γ )− PZtk(θ′γ)(Bθγ ,θ′γ ) + 3γ\n≤\nM̃k(γ) ∑\ni=1\nPZtk(θγ) (Bi(θγ , θ ′ γ))− PZtk(θ′γ)(Bi(θγ , θ ′ γ)) + 5γ.\nAgain, since the Bi(θγ , θ ′ γ) are disjoint, this equals\n5γ + PZtk(θγ)\n\n\nM̃k(γ) ⋃\ni=1\nBi(θγ , θ ′ γ)\n\n − PZtk(θ′γ)\n\n\nM̃k(γ) ⋃\ni=1\nBi(θγ , θ ′ γ)\n\n\n≤ 7γ + PZtk(θ)\n\n\nM̃k(γ) ⋃\ni=1\nBi(θγ , θ ′ γ)\n\n− PZtk(θ′)\n\n\nM̃k(γ) ⋃\ni=1\nBi(θγ , θ ′ γ)\n\n\n= 7γ +\nM̃k(γ) ∑\ni=1\nPZtk(θ) (Bi(θγ , θ ′ γ))− PZtk(θ′)(Bi(θγ , θ ′ γ))\n≤ 7γ + M̃k(γ) max i≤M̃k(γ)\n∣ ∣ ∣PZtk(θ) (Bi(θγ , θ ′ γ))− PZtk(θ′)(Bi(θγ , θ ′ γ)) ∣ ∣ ∣ .\nThus, if we can show that each term ∣ ∣\n∣PZtk(θ) (Bi(θγ , θ ′ γ))− PZtk(θ′)(Bi(θγ , θ ′ γ))\n∣ ∣ ∣\nis bounded by a o(1) function of ‖PZtd(θ) − PZtd(θ′)‖, then the result will follow by substituting this relaxation into the above expression and defining Mk by minimizing the resulting expression over γ > 0.\nToward this end, let Cij be as above from the definition of Bi(θγ , θ ′ γ), and note that IBi(θγ ,θ′γ) is representable as a function of the ICij indicators, so that\n∣ ∣ ∣ PZt\nk (θ)(Bi(θγ , θ ′ γ))−PZtk(θ′)(Bi(θγ , θ ′ γ))\n∣ ∣ ∣ = ‖PIBi(θγ,θ′γ )(Z t k (θ))−PIBi(θγ,θ′γ )(Z t k (θ′))‖\n≤ ‖P(ICi1 (Ztk(θ)),...,ICiℓi(θγ,θ′γ ) (Ztk(θ))) − P(ICi1 (Ztk(θ′)),...,ICiℓi(θγ,θ′γ ) (Ztk(θ ′)))‖\n≤ 2ℓi(θγ ,θ ′ γ) max\nJ⊆{1,...,ℓi(θγ ,θ′γ)} E\n[(\n∏\nj∈J\nICij (Z t k(θ))\n)\n∏\nj /∈J\n(\n1− ICij (Z t k(θ))\n)\n−\n(\n∏\nj∈J\nICij (Z t k(θ ′))\n)\n∏\nj /∈J\n(\n1− ICij (Z t k(θ ′))\n)]\n≤ 2ℓi(θγ ,θ ′ γ)\n∑\nJ⊆ { 1,...,2 ℓi(θγ ,θ ′ γ ) }\n∣ ∣ ∣ ∣ ∣ ∣ E   ∏\nj∈J\nICij (Z t k(θ)) −\n∏\nj∈J\nICij (Z t k(θ ′))\n\n\n∣ ∣ ∣ ∣ ∣ ∣\n≤ 4ℓi(θγ ,θ ′ γ) max\nJ⊆ { 1,...,2 ℓi(θγ ,θ ′ γ ) }\n∣ ∣ ∣ ∣ ∣ ∣ E   ∏\nj∈J\nICij (Z t k(θ)) −\n∏\nj∈J\nICij (Z t k(θ ′))\n\n\n∣ ∣ ∣ ∣ ∣ ∣\n= 4ℓi(θγ ,θ ′ γ) max\nJ⊆ { 1,...,2 ℓi(θγ ,θ ′ γ ) }\n∣ ∣ ∣ ∣ ∣ ∣ PZtk(θ)   ⋂\nj∈J\nCij\n\n− PZtk(θ′)\n\n\n⋂\nj∈J\nCij\n\n\n∣ ∣ ∣ ∣ ∣ ∣ .\nNote that ⋂\nj∈J Cij can be expressed as some (A1×(−∞, t1])×· · ·×(Ak×(−∞, tk]),\nwhere each Ap ∈ BX and tp ∈ R, so that, for ℓ̂ = maxθ,θ′∈Θγ maxi≤M̃k(γ) ℓi(θ, θ ′) and Ck = {(A1 × (−∞, t1])× · · · × (Ak × (−∞, tk]) : ∀j ≤ k,Aj ∈ BX , tk ∈ R}, this last expression is at most\n4ℓ̂ sup C∈Ck\n∣ ∣ ∣PZtk(θ) (C)− PZtk(θ′)(C) ∣ ∣ ∣ .\nNext note that for any C = (A1× (−∞, t1])×· · ·× (Ak × (−∞, tk]) ∈ Ck, letting C1 = A1 × · · · ×Ak and C2 = (−∞, t1]× · · · × (−∞, tk],\nPZtk(θ) (C)− PZtk(θ′)(C) = E\n[( PYtk(θ)|Xtk(C2)− PYtk(θ′)|Xtk(C2) ) IC1(Xtk) ]\n≤ E [∣ ∣PYtk(θ)|Xtk(C2)− PYtk(θ′)|Xtk(C2) ∣ ∣ ] .\nFor p ∈ {1, . . . , k}, let C2p = (−∞, tp]. Then note that, by definition of d, for any given x = (x1, . . . , xk), the class Hx = {xp 7→ IC2p(h(xp)) : h ∈ F} is a VC\nclass over {x1, . . . , xk} with VC dimension at most d. Furthremore, we have\n∣ ∣PYtk(θ)|Xtk(C2)− PYtk(θ′)|Xtk(C2) ∣ ∣\n= ∣ ∣\n∣ P(IC21 (h ∗ tθ(Xt1)),...,IC2k (h ∗ tθ(Xtk)))|Xtk\n({(1, . . . , 1)})\n− P(IC21 (h∗tθ′ (Xt1)),...,IC2k (h ∗ tθ′\n(Xtk)))|Xtk({(1, . . . , 1)}) ∣ ∣ ∣.\nTherefore, the results of [12] (in the proof of their Lemma 3) imply that\n∣ ∣PYtk(θ)|Xtk(C2)− PYtk(θ′)|Xtk(C2) ∣ ∣\n≤ 2k max y∈{0,1}d max D∈{1,...,k}d\n∣ ∣ ∣P{IC2j (h ∗ tθ(Xtj))}j∈D |{Xtj}j∈D ({y})\n− P{IC2j (h∗tθ′ (Xtj))}j∈D |{Xtj}j∈D ({y}) ∣ ∣ ∣ .\nThus, we have\nE [∣ ∣PYtk(θ)|Xtk(C2)− PYtk(θ′)|Xtk(C2) ∣ ∣ ]\n≤ 2kE\n[\nmax y∈{0,1}d max D∈{1,...,k}d\n∣ ∣ ∣P{IC2j (h ∗ tθ(Xtj))}j∈D |{Xtj}j∈D ({y})\n− P{IC2j (h∗tθ′ (Xtj))}j∈D |{Xtj}j∈D ({y}) ∣ ∣ ∣\n]\n≤ 2k ∑\ny∈{0,1}d\n∑\nD∈{1,...,k}d\nE\n[\n∣ ∣ ∣P{IC2j (h ∗ tθ(Xtj))}j∈D |{Xtj}j∈D ({y})\n− P{IC2j (h∗tθ′ (Xtj))}j∈D |{Xtj}j∈D ({y}) ∣ ∣ ∣\n]\n≤ 2d+kkd max y∈{0,1}d max D∈{1,...,k}d E\n[\n∣ ∣ ∣P{IC2j (h ∗ tθ(Xtj))}j∈D |{Xtj}j∈D ({y})\n− P{IC2j (h∗tθ′ (Xtj))}j∈D |{Xtj}j∈D ({y}) ∣ ∣ ∣\n]\n.\nExchangeability implies this is at most\n2d+kkd max y∈{0,1}d sup t1,...,td∈R E\n[\n∣ ∣ ∣P{I(−∞,tj ](h ∗ tθ(Xtj))} d j=1|Xtd ({y})\n− P{I(−∞,tj ](h ∗ tθ′ (Xtj))}dj=1|Xtd ({y})\n∣ ∣ ∣\n]\n= 2d+kkd max y∈{0,1}d sup t1,...,td∈R E\n[\n∣ ∣ ∣P{I(−∞,tj ](Ytj(θ))} d j=1|Xtd ({y})\n− P{I(−∞,tj ](Ytj(θ′))} d j=1|Xtd\n({y}) ∣ ∣\n∣\n]\n.\n[12] argue that for all y ∈ {0, 1}d and t1, . . . , td ∈ R,\nE\n[∣\n∣ ∣P{I(−∞,tj ](Ytj(θ))} d j=1|Xtd ({y})− P{I(−∞,tj ](Ytj(θ′))} d j=1|Xtd\n({y}) ∣ ∣\n∣\n]\n≤ 4 √\n‖P{I(−∞,tj ](Ytj(θ))} d j=1,Xtd − P{I(−∞,tj ](Ytj(θ′))} d j=1,Xtd ‖.\nNoting that\n‖P{I(−∞,tj ](Ytj(θ))} d j=1,Xtd − P{I(−∞,tj ](Ytj(θ′))} d j=1,Xtd ‖ ≤ ‖PZtd(θ) − PZtd(θ′)‖\ncompletes the proof. ⊓⊔\nWe are now ready for the proof of Theorem 4.\nProof (Proof of Theorem 4). The estimator θ̂Tθ⋆ we will use is precisely the minimum-distance skeleton estimate of PZtd(θ⋆) [13,5]. [13] proved that if N(ε) is the ε-covering number of {PZtd(θ⋆) : θ ∈ Θ}, then taking this θ̂Tθ⋆ estimator, then for some Tε = O((1/ε 2) logN(ε/4)), any T ≥ Tε has\nE\n[\n‖PZtd(θ̂Tθ⋆ ) − PZtd(θ⋆)‖\n]\n< ε.\nThus, taking GT = inf{ε > 0 : T ≥ Tε}, we have\nE\n[\n‖PZtd(θ̂Tθ⋆ ) − PZtd(θ⋆)‖\n]\n≤ GT = o(1).\nLetting R′(T, α) be any positive sequence withGT ≪ R′(T, α) ≪ 1 and R′(T, α) ≥ GT /α, and letting δ(T, α) = GT /R ′(T, α) = o(1), Markov’s inequality implies\nP\n(\n‖PZtd(θ̂Tθ⋆ ) − PZtd(θ⋆)‖ > R\n′(T, α) ) ≤ δ(T, α) ≤ α. (2)\nLetting R(T, α) = mink (Mk (R ′(T, α)) + rk), since R ′(T, α) = o(1) and rk = o(1), we have R(T, α) = o(1). Furthermore, composing (2) with Lemmas 1, 2, and 3, we have\nP\n(\n‖πθ̂Tθ⋆ − πθ⋆‖ > R(T, α)\n)\n≤ δ(T, α) ≤ α.\n⊓⊔\nRemark: Although the above proof makes use of the minimum-distance skeleton estimator, which is typically not computationally efficient, it is often possible to achieve this same result (for certain families of distributions) using a simpler estimator, such as the maximum likelihood estimator. All we require is that the risk of the estimator converges to 0 at a known rate that is independent of θ⋆. For instance, see [6] for conditions on the family of distributions sufficient for this to be true of the maximum likelihood estimator."
    } ],
    "references" : [ {
      "title" : "Sampling lower bounds via information theory",
      "author" : [ "Z. Bar-Yossef" ],
      "venue" : "Proceedings of the 35th Annual ACM Symposium on the Theory of Computing. pp. 335–344",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A Bayesian/information theoretic model of learning to learn via multiple task sampling",
      "author" : [ "J. Baxter" ],
      "venue" : "Machine Learning 28, 7–39",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learnability and the Vapnik-Chervonenkis dimension",
      "author" : [ "A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M. Warmuth" ],
      "venue" : "Journal of the Association for Computing Machinery 36(4), 929–965",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Combinatorial Auctions",
      "author" : [ "P. Cramton", "Y. Shoham", "R. Steinberg" ],
      "venue" : "The MIT Press",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Combinatorial Methods in Density Estimation",
      "author" : [ "L. Devroye", "G. Lugosi" ],
      "venue" : "Springer, New York, NY, USA",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Empirical Processes in M-Estimation",
      "author" : [ "S. van de Geer" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "MDL convergence speed for Bernoulli sequences",
      "author" : [ "J. Poland", "M. Hutter" ],
      "venue" : "Statistics and Computing 16, 161–175",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Theory of Statistics",
      "author" : [ "M.J. Schervish" ],
      "venue" : "Springer, New York, NY, USA",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Estimation of Dependencies Based on Empirical Data",
      "author" : [ "V. Vapnik" ],
      "venue" : "Springer-Verlag, New York",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "On the uniform convergence of relative frequencies of events to their probabilities",
      "author" : [ "V. Vapnik", "A. Chervonenkis" ],
      "venue" : "Theory of Probability and its Applications 16, 264–280",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Sequential tests of statistical hypotheses",
      "author" : [ "A. Wald" ],
      "venue" : "The Annals of Mathematical Statistics 16(2), 117–186",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1945
    }, {
      "title" : "A theory of transfer learning with applications to active learning",
      "author" : [ "L. Yang", "S. Hanneke", "J. Carbonell" ],
      "venue" : "Machine Learning 90(2), 161–189",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Rates of convergence of minimum distance estimators and Kolmogorov’s entropy",
      "author" : [ "Y.G. Yatracos" ],
      "venue" : "The Annals of Statistics 13, 768–774",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "On polynomial-time preference elicitation with value queries",
      "author" : [ "M. Zinkevich", "A. Blum", "T. Sandholm" ],
      "venue" : "Proceedings of the 4 ACM Conference on Electronic Commerce. pp. 175–185",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Among the several proposed models for transfer learning, one particularly appealing model supposes the learning problems are independent and identically distributed, with unknown distribution, and the advantage of transfer learning then comes from the ability to estimate this shared distribution based on the data from past learning problems [2,12].",
      "startOffset" : 343,
      "endOffset" : 349
    }, {
      "referenceID" : 11,
      "context" : "Among the several proposed models for transfer learning, one particularly appealing model supposes the learning problems are independent and identically distributed, with unknown distribution, and the advantage of transfer learning then comes from the ability to estimate this shared distribution based on the data from past learning problems [2,12].",
      "startOffset" : 343,
      "endOffset" : 349
    }, {
      "referenceID" : 11,
      "context" : "In recent work, [12] have shown that under mild conditions on the family of possible distributions, if the target concepts reside in a known VC class, then it is possible to estimate this distribtion using only a bounded number of training samples per task: specifically, a number of samples equal the VC dimension.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "The present work continues that of [12], bounding the rate of convergence for estimating this distribution, under a smoothness condition on the distribution.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "We additionally include an extension of the results of [12] to the setting of real-valued functions, establishing consistency (at a uniform rate) for an estimator of a prior over any VC subgraph class.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "In addition to the application to transfer learning, analogous to the original work of [12], we also discuss an application of this result to a preference elicitation problem in algorithmic economics, in which we are tasked with allocating items to a sequence of customers to approximately maximize the customers’ satisfaction, while permitted access to the customer valuation functions only via value queries.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "2 The Setting Let (X ,BX ) be a measurable space [8] (where X is called the instance space), and let D be a distribution on X (called the data distribution).",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "Let C be a VC class of measurable classifiers h : X → {−1,+1} (called the concept space), and denote by d the VC dimension of C [10].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "We suppose there exists a probability measure π0 on C (the reference measure) such that every πθ is absolutely continuous with respect to π0, and therefore has a density function fθ given by the Radon-Nikodym derivative dπθ dπ0 [8].",
      "startOffset" : 228,
      "endOffset" : 231
    }, {
      "referenceID" : 11,
      "context" : "In previous work, [12] showed that, if ΠΘ is a totally bounded family, then even with only d number of samples per task, the minimax risk (as a function of the number of tasks T ) converges to zero.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "By the standard PAC analysis [9,3], for any γ > 0, with probability greater than 1 − γ, a sample of k = O((d/γ) log(1/γ)) random points will partition C into regions of width less than γ (under L1(D)).",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "By the standard PAC analysis [9,3], for any γ > 0, with probability greater than 1 − γ, a sample of k = O((d/γ) log(1/γ)) random points will partition C into regions of width less than γ (under L1(D)).",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "Following analogous to the inductive argument of [12], suppose I ⊆ {1, .",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : "[12] showed that E [ |PYd(θ)|Xd(ȳ )− PYd(θ′)|Xd(ȳ )| ] ≤ 4 √ ‖PZd(θ) − PZd(θ′)‖, so that in total we have ‖πθ − πθ′‖ < (L+1)γ+4(2ek) √ ‖PZd(θ)−PZd(θ′)‖.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "IfN(ε) is the ε-covering number of {PZd(θ) : θ ∈ Θ}, then taking θ̂Tθ⋆ as the minimum distance skeleton estimate of [13,5] achieves expected total variation distance ε from PZd(θ⋆), for some T = O((1/ε ) logN(ε/4)).",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 4,
      "context" : "IfN(ε) is the ε-covering number of {PZd(θ) : θ ∈ Θ}, then taking θ̂Tθ⋆ as the minimum distance skeleton estimate of [13,5] achieves expected total variation distance ε from PZd(θ⋆), for some T = O((1/ε ) logN(ε/4)).",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "Furthermore, the covering number of ΠΘ upper bounds N(ε) [12], so that N(ε) ≤ (1/ε) d/α).",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "d Bernoulli(p) random variables, for each p ∈ [0, 1]; then it is known that, for any (possibly nondeterministic) decision rule",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "(1) This easily follows from the results of [1], combined with a result of [7] bounding the KL divergence (see also [11]) To use this result, we construct a learning problem as follows.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "(1) This easily follows from the results of [1], combined with a result of [7] bounding the KL divergence (see also [11]) To use this result, we construct a learning problem as follows.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "(1) This easily follows from the results of [1], combined with a result of [7] bounding the KL divergence (see also [11]) To use this result, we construct a learning problem as follows.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : ", i ∗ T ) = q̂ ′ i(CiT (θ⋆)), for some q̂ ′ i [8]: that is, estimators that are a function of the NiT (θ⋆) = |CiT (θ⋆)| Bernoulli(pi) random variables, which we should note are conditionally i.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "samples from a Hölder smooth density in a d-dimensional space [5].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "Prior Estimation 11 5 Real-Valued Functions and an Application in Algorithmic Economics In this section, we present results generalizing the analysis of [12] to classes of real-valued functions.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 11,
      "context" : "The proof parallels that of [12] (who studied the special case of binary functions), with a few important twists (in particular, a significantly different approach in the analogue of their Lemma 3).",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : ",Z T d (θ⋆)), and functions R : N0 × (0, 1] → [0,∞) and δ : N0 × (0, 1] → [0, 1] such that, for any α > 0, lim T→∞ R(T, α) = lim T→∞ δ(T, α) = 0 and for any T ∈ N0 and θ⋆ ∈ Θ, P (",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "2 Maximizing Customer Satisfaction in Combinatorial Auctions Theorem 4 has a clear application in the context of transfer learning, following analogous arguments to those given in the special case of binary classification by [12].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 3,
      "context" : "However, we are not permitted direct observation of the customer valuation functions; rather, we may query for the value of any given bundle of items; this is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).",
      "startOffset" : 277,
      "endOffset" : 280
    }, {
      "referenceID" : 13,
      "context" : "However, we are not permitted direct observation of the customer valuation functions; rather, we may query for the value of any given bundle of items; this is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).",
      "startOffset" : 282,
      "endOffset" : 286
    }, {
      "referenceID" : 3,
      "context" : "This is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 13,
      "context" : "This is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "The proof of this result parallels that of [12] for the transfer learning setting, but is included here for completeness.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : "16 Liu Yang, Steve Hanneke, and Jaime Carbonell A Proofs for Section 5 The proof of Theorem 4 is based on the following sequence of lemmas, which parallel those used by [12] for establishing the analogous result for consistent estimation of priors over binary functions.",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 11,
      "context" : "The last of these lemmas (namely, Lemma 3) requires substantial modifications to the original argument of [12]; the others use arguments more-directly based on those of [12].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "The last of these lemmas (namely, Lemma 3) requires substantial modifications to the original argument of [12]; the others use arguments more-directly based on those of [12].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "Note that since F is a uniformly bounded VC subgraph class, so is the collection of functions {|h− g| : h, g ∈ F}, so that the uniform strong law of large numbers implies that with probability one, ∀h, g ∈ F , ρX(h, g) exists and has ρX(h, g) = ρ(h, g) [10].",
      "startOffset" : 253,
      "endOffset" : 257
    }, {
      "referenceID" : 11,
      "context" : "This proof follows identically to a proof of [12], but is included here for completeness.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "Thus, Carathéodory’s extension theorem (specifically, the version presented by [8]) implies that there exist disjoint sets {Ai}i∈N in A such that B ⊆ ⋃ i∈N Ai and PZt(θ)(B) − PZt(θ′)(B) < ∑",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "By Carathéodory’s extension theorem (specifically, the version presented by [8]), there exists a disjoint sequence of sets {Bi(θ, θ)}i=1 such that PZt k (θ)(Bθ,θ′)− PZt k (θ′)(Bθ,θ′) < γ + ∞ ∑",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "Therefore, the results of [12] (in the proof of their Lemma 3) imply that",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "[12] argue that for all y ∈ {0, 1} and t1, .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "The estimator θ̂Tθ⋆ we will use is precisely the minimum-distance skeleton estimate of PZt d(θ⋆) [13,5].",
      "startOffset" : 97,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "The estimator θ̂Tθ⋆ we will use is precisely the minimum-distance skeleton estimate of PZt d(θ⋆) [13,5].",
      "startOffset" : 97,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "[13] proved that if N(ε) is the ε-covering number of {PZt d(θ⋆) : θ ∈ Θ}, then taking this θ̂Tθ⋆ estimator, then for some Tε = O((1/ε ) logN(ε/4)), any T ≥ Tε has E [",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2015,
    "abstractText" : "Abstract. We study the optimal rates of convergence for estimating a prior distribution over a VC class from a sequence of independent data sets respectively labeled by independent target functions sampled from the prior. We specifically derive upper and lower bounds on the optimal rates under a smoothness condition on the correct prior, with the number of samples per data set equal the VC dimension. These results have implications for the improvements achievable via transfer learning. We additionally extend this setting to real-valued function, where we establish consistency of an estimator for the prior, and discuss an additional application to a preference elicitation problem in algorithmic economics.",
    "creator" : "LaTeX with hyperref package"
  }
}