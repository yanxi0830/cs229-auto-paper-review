{
  "name" : "0912.0086.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Mixtures of Gaussians Using the k-Means Algorithm",
    "authors" : [ "Kamalika Chaudhuri" ],
    "emails" : [ "kamalika@soe.ucsd.edu", "dasgupta@cs.ucsd.edu", "avattani@cs.ucsd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :0\n91 2.\n00 86\nv1 [\ncs .L\nG ]\nWe analyze three aspects of the k-means algorithm under this assumption. First, we show that when the input comes from a mixture of two spherical Gaussians, a variant of the 2-means algorithm successfully isolates the subspace containing the means of the mixture components. Second, we show an exact expression for the convergence of our variant of the 2-means algorithm, when the input is a very large number of samples from a mixture of spherical Gaussians. Our analysis does not require any lower bound on the separation between the mixture components.\nFinally, we study the sample requirement of k-means; for a mixture of 2 spherical Gaussians, we show an upper bound on the number of samples required by a variant of 2-means to get close to the true solution. The sample requirement grows with increasing dimensionality of the data, and decreasing separation between the means of the Gaussians. To match our upper bound, we show an information-theoretic lower bound on any algorithm that learns mixtures of two spherical Gaussians; our lower bound indicates that in the case when the overlap between the probability masses of the two distributions is small, the sample requirement of k-means is near-optimal."
    }, {
      "heading" : "1 Introduction",
      "text" : "One of the most popular algorithms for clustering in Euclidean space is the k-means algorithm [Llo82, For65, Mac67]; this is a simple, local-search algorithm that iteratively refines a partition of the input points until convergence. Like many local-search algorithms, k-means is notoriously difficult to analyze, and few theoretical guarantees are known about it.\nThere has been three lines of work on the k-means algorithm. A first line of questioning addresses the quality of the solution produced by k-means, in comparison to the globally optimal solution. While it has been well-known that for general inputs the quality of this solution can be arbitrarily bad, the conditions under which k-means yields a globally optimal solution on wellclustered data are not well-understood. A second line of work [AV06, Vat09] examines the number of iterations required by k-means to converge. [Vat09] shows that there exists a set of n points on the plane, such that k-means takes as many as Ω(2n) iterations to converge on these points. A smoothed analysis upper bound of poly(n) iterations has been established by [AMR09], but this bound is still much higher than what is observed in practice, where the number of iterations are frequently sublinear in n. Moreover, the smoothed analysis bound applies to small perturbations of arbitrary inputs, and the question of whether one can get faster convergence on well-clustered inputs, is still unresolved. A third question, considered in the statistics literature, is the statistical efficiency of k-means. Suppose the input is drawn from some simple distribution, for which k-means is statistically consistent; then, how many samples is required for k-means to converge? Are there other consistent procedures with a better sample requirement?\nIn this paper, we study all three aspects of k-means, by studying the behavior of k-means on Gaussian clusters. Such data is frequently modelled as a mixture of Gaussians; a mixture is a collection of Gaussians D = {D1, . . . ,Dk} and weights w1, . . . , wk, such that ∑\ni wi = 1. To sample from the mixture, we first pick i with probability wi and then draw a random sample from Di. Clustering such data then reduces to the problem of learning a mixture; here, we are given only the ability to sample from a mixture, and our goal is to learn the parameters of each Gaussian Di, as well as determine which Gaussian each sample came from.\nOur results are as follows. First, we show that when the input comes from a mixture of two spherical Gaussians, a variant of the 2-means algorithm successfully isolates the subspace containing the means of the Gaussians. Second, we show an exact expression for the convergence of a variant of the 2-means algorithm, when the input is a large number of samples from a mixture of two spherical Gaussians. Our analysis shows that the convergence-rate is logarithmic in the dimension, and decreases with increasing separation between the mixture components. Finally, we address the sample requirement of k-means; for a mixture of 2 spherical Gaussians, we show an upper bound on the number of samples required by a variant of 2-means to get close to the true solution. The sample requirement grows with increasing dimensionality of the data, and decreasing separation between the means of the distributions. To match our upper bound, we show an information-theoretic lower bound on any algorithm that learns mixtures of two spherical Gaussians; our lower bound indicates that in the case when the overlap between the probability masses of the two distributions is small, the sample requirement of 2-means is near-optimal.\nAdditionally, we make some partial progress towards analyzing k-means in the more general case – we show that if our variant of 2-means is run on a mixture of k spherical Gaussians, then, it converges to a vector in the subspace containing the means of Di.\nThe key insight in our analysis is a novel potential function θt, which is the minimum angle between the subspace of the means of Di, and the normal to the hyperplane separator in 2-means. We show that this angle decreases with iterations of our variant of 2-means, and we can characterize convergence rates and sample requirements, by characterizing the rate of decrease of the potential.\nOur Results. More specifically, our results are as follows. We perform a probabilistic analysis of a variant of 2-means; our variant is essentially a symmetrized version of 2-means, and it reduces to 2-means when we have a very large number of samples from a mixture of two identical spherical Gaussians with equal weights. In the 2-means algorithm, the separator between the two clusters is always a hyperplane, and we use the angle θt between the normal to this hyperplane and the mean of a mixture component in round t, as a measure of the potential in each round. Note that when θt = 0, we have arrived at the correct solution.\nFirst, in Section 3, we consider the case when we have at our disposal a very large number of samples from a mixture of N(µ1, (σ1)2Id) and N(µ 2, (σ2)2Id) with mixing weights ρ 1, ρ2 respectively. We show an exact relationship between θt and θt+1, for any value of µ j, σj, ρj and t. Using this relationship, we can approximate the rate of convergence of 2-means, for different values of the separation, as well as different initialization procedures. Our guarantees illustrate that the progress of k-means is very fast – namely, the square of the cosine of θt grows by at least a constant factor (for high separation) each round, when one is far from the actual solution, and slow when the actual solution is very close.\nNext, in Section 4, we characterize the sample requirement for our variant of 2-means to succeed, when the input is a mixture of two spherical Gaussians. For the case of two identical spherical Gaussians with equal mixing weight, our results imply that when the separation µ < 1, and when Ω̃( d\nµ4 ) samples are used in each round, the 2-means algorithm makes progress at roughly the same rate as in Section 3. This agrees with the Ω( 1 µ4 ) sample complexity lower bound [Lin96] for learning a mixture of Gaussians on the line, as well as with experimental results of [SSR06]. When µ > 1, our variant of 2-means makes progress in each round, when the number of samples is at least Ω̃( d\nµ2 ).\nThen, in Section 5, we provide an information-theoretic lower bound on the sample requirement of any algorithm for learning a mixture of two spherical Gaussians with standard deviation 1 and equal weight. We show that when the separation µ > 1, any algorithm requires Ω( dµ2 ) samples to converge to a vector within angle θ = cos−1(c) of the true solution, where c is a constant. This indicates that k-means has near-optimal sample requirement when µ > 1.\nFinally, in Section 6, we examine the performance of 2-means when the input comes from a mixture of k spherical Gaussians. We show that, in this case, the normal to the hyperplane separating the two clusters converges to a vector in the subspace containing the means of the mixture components. Again, we characterize exactly the rate of convergence, which looks very similar to the bounds in Section 3.\nRelated Work. The convergence-time of the k-means algorithm has been analyzed in the worstcase [AV06, Vat09], and the smoothed analysis settings [MR09, AMR09]; [Vat09] shows that the convergence-time of k-means may be Ω(2n) even in the plane. [AMR09] establishes a O(n30) smoothed complexity bound. [ORSS06] analyzes the performance of k-means when the data obeys a clusterability condition; however, their clusterability condition is very different, and moreover, they examine conditions under which constant-factor approximations can be found. In statistics literature, the k-means algorithm has been shown to be consistent [Mac67]. [Pol81] shows that minimizing the k-means objective function (namely, the sum of the squares of the distances between each point and the center it is assigned to), is consistent, given sufficiently many samples. As optimizing the k-means objective is NP-Hard, one cannot hope to always get an exact solution. None of these two works quantify either the convergence rate or the exact sample requirement of k-means.\nThere has been two lines of previous work on theoretical analysis of the EM algorithm [DLR77], which is closely related to k-means. Essentially, for learning mixtures of identical Gaussians, the only difference between EM and k-means is that EM uses partial assignments or soft clusterings,\nwhereas k-means does not. First, [RW84, XJ96] views learning mixtures as an optimization problem, and EM as an optimization procedure over the likelihood surface. They analyze the structure of the likelihood surface around the optimum to conclude that EM has first-order convergence. An optimization procedure on a parameter m is said to have first-order convergence, if,\n||mt+1 −m∗|| ≤ R · ||mt −m∗||\nwhere mt is the estimate of m at time step t using n samples, m ∗ is the maximum likelihood estimator for m using n samples, and R is some fixed constant between 0 and 1. In contrast, our analysis also applies when one is far from the optimum.\nThe second line of work is a probabilistic analysis of EM due to [DS00]; they show a tworound variant of EM which converges to the correct partitioning of the samples, when the input is generated by a mixture of k well-separated, spherical Gaussians. For their analysis to work, they require the mixture components to be separated such that two samples from the same Gaussian are a little closer in space than two samples from different Gaussians. In contrast, our analysis applies when the separation is much smaller.\nThe sample requirement of learning mixtures has been previously studied in the literature, but not in the context of k-means. [CHRZ07, Cha07] provides an algorithm that learns a mixture of two binary product distributions with uniform weights, when the separation µ between the mixture components is at least a constant, so long as Ω̃( d\nµ4 ) samples are available. (Notice that for such\ndistributions, the directional standard deviation is at most 1.) Their algorithm is similar to kmeans in some respects, but different in that they use different sets of coordinates in each round, and this is very crucial in their analysis. Additionally, [BCOFZ07] show a spectral algorithm which learns a mixture of k binary product distributions, when the distributions have small overlap in probability mass, and the sample size is at least Ω̃(d/µ2). [Lin96] shows that at least Ω̃( 1\nµ4 ) samples\nare required to learn a mixture of two Gaussians in one dimension. We note that although our lower bound of Ω(d/µ2) for µ > 1 seems to contradict the upper bound of [CHRZ07, Cha07], this is not actually the case. Our lower bound characterizes the number of samples required to find a vector at an angle θ = cos−1(1/10) with the vector joining the means. However, in order to classify a constant fraction of the points correctly, we only need to find a vector at an angle θ′ = cos−1(1/µ) with the vector joining the means. Since the goal of [CHRZ07] is to simply classify a constant fraction of the samples, their upper bound is less than O(d/µ2).\nIn addition to theoretical analysis, there has been very interesting experimental work due to [SSR06], which studies the sample requirement for EM on a mixture of k spherical Gaussians. They conjecture that the problem of learning mixtures has three phases, depending on the number of samples : with less than about d\nµ4 samples, learning mixtures is information-theoretically hard;\nwith more than about d µ2\nsamples, it is computationally easy, and in between, computationally hard, but easy in an information-theoretic sense. Finally, there has been a line of work which provides algorithms (different from EM or k-means) that are guaranteed to learn mixtures of Gaussians under certain separation conditions – see, for example, [Das99, VW02, AK05, AM05, KSV05, CR08, BV08]. For mixtures of two Gaussians, our result is comparable to the best results for spherical Gaussians [VW02] in terms of separation requirement, and we have a smaller sample requirement."
    }, {
      "heading" : "2 The Setting",
      "text" : "The k-means algorithm iteratively refines a partitioning of the input data. At each iteration, k points are maintained as centers; each input is assigned to its closest center. The center of each\ncluster is then recomputed as the empirical mean of the points assigned to the cluster. This procedure is continued until convergence.\nOur variant of k-means is described below. There are two main differences between the actual 2-means algorithm, and our variant. First, we use a separate set of samples in each iteration. Secondly, we always fix the cluster boundary to be a hyperplane through the origin. When the input is a very large number of samples from a mixture of two identical Gaussians with equal mixing weights, and with center of mass at the origin, this is exactly 2-means initialized with symmetric centers (with respect to the origin). We analyze this symmetrized version of 2-means even when the mixing weights and the variances of the Gaussians in the mixture are not equal.\nThe input to our algorithm is a set of samples S, a number of iterations N , and a starting vector ŭ0, and the output is a vector uN obtained after N iterations of the 2-means algorithm. 2-means-iterate(S, N , u0) 1. Partition S randomly into sets of equal size S1, . . . ,SN .\n2. For iteration t = 0, . . . , N − 1, compute: Ct+1 = {x ∈ St+1|〈x, ut〉 > 0} C̄t+1 = {x ∈ St+1|〈x, ut〉 < 0}\nCompute: ut+1 as the empirical average of Ct+1.\nNotation. In Sections 3 and 4, we analyze Algorithm 2-means-iterate, when the input is generated by a mixture D = {D1,D2} of two Gaussians. We let D1 = N(µ1, (σ1)2Id), D2 = N(µ2, (σ2)2Id), with mixing weights ρ1 and ρ2. We also assume without loss of generality that for all j, σj ≥ 1. As the center of mass of the mixture lies at the origin, ρ1µ1 + ρ2µ2 = 0. In Section 6, we study a somewhat more general case.\nWe define b as the unit vector along µ1, i.e. b = µ 1\n||µ1|| .Henceforth, for any vector v, we use the notation v̆ to denote the unit vector along v, i.e. v̆ = v||v|| . Therefore, ŭt is the unit vector along ut. We assume without loss of generality that µ 1 lies in the cluster Ct+1. In addition, for each t, we define θt as the angle between µ 1 and ut.We use the cosine of θt as a measure of progress of the algorithm at round t, and our goal is to show that this quantity increases as t increases. Observe that 0 ≤ cos(θt) ≤ 1, and cos(θt) = 1 when ut and µ1 are aligned along the same direction. For each t, we define τ jt = 〈µj, ŭt〉 = 〈µj , b〉 cos(θt). Moreover, from our notation, cos(θt) = τ1t ||µ1|| . In addition, we define ρmin = minj ρ j, µmin = minj ||µj ||, and σmax = maxj σj . For the special case of two identical spherical Gaussians with equal weights, we use µ = ||µ1|| = ||µ2||. Finally, for a ≤ b, we use the notation Φ(a, b) to denote the probability that a standard normal variable takes values between a and b."
    }, {
      "heading" : "3 Exact Estimation",
      "text" : "In this section, we examine the performance of Algorithm 2-means-iterate when one can estimate the vectors ut exactly – that is, when a very large number of samples from the mixture is available. Our main result of this section is Lemma 1, which exactly characterizes the behavior of 2-meansiterate at a specific iteration t.\nFor any t, we define the quantities ξt and mt as follows:\nξt = ∑\nj\nρjσj e−(τ j t ) 2/2(σj )2\n√ 2π\n, mt = ∑ j ρ j〈µj , b〉 · Φ(− τ\nj t\nσj ,∞)\nunit vector along µ1 −〈µ1, ŭt〉ŭt. Therefore, we have τ1t = ||µ1|| cos(θt) and √ ||µ1||2 − (τ1 t )2 = ||µ1|| sin(θt).\nNow, our main lemma can be stated as follows.\nLemma 1.\ncos2(θt+1) = cos 2(θt)\n(\n1 + tan2(θt) 2 cos(θt)ξtmt +m\n2 t\nξ2t + 2cos(θt)ξtmt +m 2 t\n)\nThe proof is in the Appendix. Using Lemma 1, we can characterize the convergence rates and times of 2-means-iterate for different values of µj, ρj and σj , as well as different initializations of u0.\nThe convergence rates can be characterized in terms of two natural parameters of the problem,\nM = ∑ j ρj ||µj ||2 σj , which measures how much the distributions are separated, and V = ∑ j ρ jσj , which measures the average standard deviations of the distributions. We observe that as σj ≥ 1, for all j, V ≥ 1 always. To characterize these rates, it is also convenient to look at two different cases, according to the value of µj , the separation between the mixture components. Small µj. First, we consider the case when each ||µj ||/σj is less than a fixed constant √\nln 92π ,\nincluding the case when ||µj|| can be much less than 1. In this case, the Gaussians are not even separated in terms of probability mass; in fact, as ||µj ||/σj decreases, the overlap in probability mass between the Gaussians tends to 1. However, we show that 2-means-iterate can still do something interesting, in terms of recovering the subspace containing the means of the distributions. Theorem 2 summarizes the convergence rate in this case.\nTheorem 2 (Small µj). Let ||µj ||/σj < √\nln 92π , for j = 1, 2. Then, there exist fixed constants\na1 and a2, such that:\ncos2(θt)(1 + a1(M/V ) sin 2(θt)) ≤ cos2(θt+1) ≤ cos2(θt)(1 + a2(M/V ) sin2(θt))\nFor a mixture of two identical Gaussians with equal mixing weights, we can conclude:\nCorollary 3. For a mixture of two identical spherical Gaussians with equal mixing weights, standard deviation 1, if µ = ||µ1|| = ||µ2|| < √\nln 92π , then,\ncos2(θt)(1 + a ′ 1µ 2 sin2(θt)) ≤ cos2(θt+1) ≤ cos2(θt)(1 + a′2µ2 sin2(θt))\nThe proof follows by a combination of Lemma 1, and Lemma 25. From Corollary 3, we observe that cos2(θt) grows by a factor of (1 + Θ(µ 2)) in each iteration, except when θt is very close to 0.\nThis means that when 2-means-iterate is far from the actual solution, it approaches the solution at a consistently high rate. The convergence rate only grows slower, once k-means is very close to the actual solution. Large µj. In this case, there exists a j such that ||µj ||/σj ≥ √\nln 92π . In this regime, the Gaussians\nhave small overlap in probability mass, yet, the distance between two samples from the same distribution is much greater than the separation between the distributions. Our guarantees for this case are summarized by Theorem 4.\nWe see from Theorem 4 that there are two regimes of behavior of the convergence rate, depending on the value of maxj |τ jt |/σj . These regimes have a natural interpretation. The first regime corresponds to the case when θt is large enough, such that when projected onto ut, at most a constant fraction of samples from the two distributions can be classified with high confidence. The second regime corresponds to the case when θt is close enough to 0 such that when projected along ut, most of the samples from the distributions can be classified with high confidence. As expected, in the second regime, the convergence rate is much slower than in the first regime.\nTheorem 4 (Large µj). Suppose there exists j such that ||µj ||/σj ≥ √\nln 92π . If |τ j t |/σj <\n√\nln 92π , for all j, then, there exist fixed constants a3, a4, a5 and a6 such that:\ncos2(θt)\n(\n1 + a3(M/V )\n2 sin2(θt)\na4 + (M/V )2 cos2(θt)\n) ≤ cos2(θt+1) ≤ cos2(θt) ( 1 + a5((M/V ) + (M/V ) 2) sin2(θt)\na6 + (M/V )2 cos2(θt)\n)\nOn the other hand, if there exists j such that |τ jt |/σj ≥ √\nln 92π , then, there exist fixed constants a7 and a8 such that:\ncos2(θt)(1 + a7ρ\n2 minµ 2 min\na8V 2 + ρ 2 minµ 2 min\ntan2(θt)) ≤ cos2(θt+1) ≤ cos2(θt)(1 + tan2(θt))\nFor two identical Gaussians with standard deviation 1, we can conclude:\nCorollary 5. For a mixture of two identical Gaussians with equal mixing weights, and standard deviation 1, if µ = ||µ1|| = ||µ2|| > √\nln 92π , and if |τ1t | = |τ2t | ≤ √ ln 92π , then, there exist fixed\nconstants a′3, a ′ 4, a ′ 5, a ′ 6 such that:\ncos2(θt)\n( 1 + a′3µ 4 sin2(θt)\na′4 + µ 4 cos2(θt)\n) ≤ cos2(θt+1) ≤ cos2(θt) ( 1 + a′5µ 4 sin2(θt)\na6 + µ4 cos2(θt)\n)\nOn the other hand, if |τ1t | = |τ2t | ≥ √ ln 92π , then, there exists a fixed constant a ′ 7 such that:\ncos2(θt)(1 + a ′ 7 tan 2(θt)) ≤ cos2(θt+1) ≤ cos2(θt)(1 + tan2(θt))\nIn this case as well, we observe the same phenomenon: the convergence rate is high when we are far away from the solution, and slow when we are close. Using Theorems 2 and 4, we can characterize the convergence times of 2-means-iterate; for the sake of simplicity, we present the convergence time bounds for a mixture of two spherical Gaussians with equal mixing weights and standard deviation 1. We recall that in this case 2-means-iterate is exactly 2-means.\nCorollary 6 (Convergence Time). If θ0 is the initial angle between µ 1 and u0, then, cos 2(θN ) ≥ 1− ǫ after N = C0 · ( ln( 1 cos2(θ0) ) ln(1+µ2) + 1 ln(1+ǫ) ) iterations, where C0 is a fixed constant.\nEffect of Initialization. As apparent from Corollary 6, the effect of initialization is only to ensure a lower bound on the value of cos(θ0). We illustrate below, two natural ways by which one can select u0, and their effect on the convergence rate. For the sake of simplicity, we state these bounds for the case in which we have two identical Gaussians with equal mixing weights and standard deviation 1.\n• First, one can choose u0 uniformly at random from the surface of a unit sphere in Rd; in this case, cos2(θ0) = Θ( 1 d ), with constant probability, and as a result, the convergence time\nto reach cos−1(1/ √ 2) is O( ln d\nln(1+µ2) ).\n• A second way to choose u0 is to set it to be a random sample from the mixture; in this case, cos2(θ0) = Θ( (1+µ)2 d ) with constant probability, and the time to reach cos −1(1/ √ 2) is\nO( lnd ln(1+µ2) )."
    }, {
      "heading" : "4 Finite Samples",
      "text" : "In this section, we analyze Algorithm 2-means-iterate, when we are required to estimate the statistics at each round with a finite number of samples. We characterize the number of samples needed to ensure that 2-means-iterate makes progress in each round, and we also characterize the rate of progress when the required number of samples are available.\nThe main result of this section is the following lemma, which characterizes θt+1, the angle between µ1 and the hyperplane separator in 2-means-iterate, given θt. Notice that now θt is a random variable, which depends on the samples drawn in rounds 1, . . . , t− 1, and given θt, θt+1 is a random variable, whose value depends on samples in round t. Also we use ut+1 as the center of partition Ct in iteration t+ 1, and E[ut+1] is the expected center. Note that all the expectations in round t are conditioned on θt. In addition, we use St+1 to denote the quantity E[X · 1X∈Ct+1 ], where 1X∈Ct+1 is the indicator function for the event X ∈ Ct+1, and the expectation is taken over the entire mixture. Note that, St+1 = E[ut+1] Pr[X ∈ Ct+1] = Zt+1E[ut+1]. We use Ŝt+1 to denote the empirical value of St+1.\nLemma 7. If we use n samples in iteration t, then, given θt, with probability 1− 2δ,\ncos2(θt+1) ≥ cos2(θt) ( 1 + tan2(θt) 2 cos(θt)ξtmt+m2t\nξ2t+2 cos(θt)ξtmt+m 2 t+∆2\n) − ( ∆2 cos2(θt)+2∆1(mt+ξt cos(θt)) m2t+ξ 2 t+2ξtmt cos(θt)+∆2 )\nwhere,\n∆1 = 8 log(4n/δ)(σmax +maxj ||µj ||)√\nn\n∆2 = 128 log2(8n/δ)(σ2maxd+\n∑\nj ||µj ||2) n + 8 log(n/δ)√ n (σmax||St+1||+max j |〈St+1, µj〉|)\nThe main idea behind the proof of Lemma 7 is that we can write cos2(θt+1) = 〈Ŝt+1,µ1〉2\n||µ1||2||Ŝt+1||2 .\nNext, we can use Lemma 1, and the definition of St+1 to get an expression for 〈St+1,µ1〉2\n||St+1||2||µ1||2 , and\nLemmas 8 and 9 to bound 〈Ŝt+1 − St+1, µ1〉, and ||Ŝt+1||2 − ||St+1||2. Plugging in all these values gives us a proof of Lemma 7. We also assume for the rest of the section that the number of samples n is at most some polynomial in d, such that log(n) = Θ(log(d)).\nThe two main lemmas used in the proof of Lemma 7 are Lemmas 8 and 9. To state them, we need to define some notation. At time t, we use the notation\nLemma 8. For any t, and for any vector v with norm ||v||, with probability at least 1− δ,\n|〈Ŝt+1 − St+1, v〉| ≤ 8 log(4n/δ)(σmax||v|| +maxj |〈µj , v〉|)√\nn\nLemma 9. For any t, with probability at least 1− δ,\n||Ŝt+1||2 ≤ ||St+1||2+ 128 log2(8n/δ)(σ2maxd+\n∑\nj(µ j)2)\nn + 16 log(8n/δ)√ n (σmax||St+1||+max j |〈St+1, µj〉|)\nThe proofs of Lemmas 8 and 9 are in the Appendix. Applying Lemma 7, we can characterize the number of samples required such that 2-means-iterate makes progress in each round for different values of ||µj||. Again, it is convenient to look at two separate cases, based on ||µj ||.\nTheorem 10 (Small µj). Let ||µj||/σj < √\nln 92π , for all j. If the number of samples drawn in\nround t is at least a9σ 2 max log\n2(d/δ) (\nd MV sin4(θt) + 1 M2 sin4(θt) cos2(θt)\n)\n, for some fixed constant a9,\nthen, with probability at least 1 − δ, cos2(θt+1) ≥ cos2(θt)(1 + a10(M/V ) sin2(θt)), where a10 is some fixed constant.\nIn particular, for the case of two identical Gaussians with equal mixing weights and standard deviation 1, our results implies the following.\nCorollary 11. Let µ = ||µ1|| = ||µ2|| < √\nln 92π . If the number of samples drawn in round t is at\nleast a9 log 2(d/δ)\n(\nd µ2 sin4(θt) + 1 µ4 cos2(θt) sin4(θt)\n)\n, for some fixed constant a9, then, with probability\nat least 1− δ, cos2(θt+1) ≥ cos2(θt)(1 + a10µ2 sin2(θt)), where a10 is some fixed constant.\nIn particular, when we initialize u0 with a vector picked uniformly at random from a ddimensional sphere, cos2(θ0) ≥ 1d , with constant probability, and thus the number of samples required for success in the first round is Θ̃( d\nµ4 ). This bound matches with the lower bounds for\nlearning mixtures of Gaussians in one dimension [Lin96], as well as with conjectured lower bounds in experimental work [SSR06]. The following corollary summarizes the total number of samples required to learn the mixture with some fixed precision, for two identical spherical Gaussians with variance 1 and equal mixing weights.\nCorollary 12. Let µ = ||µ1|| = ||µ2|| ≤ √\nln 92π . Suppose u0 is chosen uniformly at random, and\nthe number of rounds is N ≥ C0 · ( ln dln(1+µ2) + 1ln(1+ǫ)), where C0 is the fixed constant in Corollary 6. If the number of samples |S| is at least: N ·a9d log\n2(d) µ4ǫ2 , then, with constant probability, after N\nrounds, cos2(θN ) ≥ 1− ǫ.\nOne can show a very similar corollary when u0 is initialized as a random sample from the mixture. We note that the total number of samples is a factor of N ≅ ln d\nµ2 times greater than the\nbound in Theorem 10. This is due to the fact that we use a fresh set of samples in every round, in order to simplify our analysis. In practice, successive iterations of k-means or EM is run on the same data-set. Theorem 13 (Large µj). Suppose that there exists some j such that ||µj ||/σj ≥ √\nln 92π , and\nsuppose that the number of samples drawn in round t is at least\na11 log 2(d/δ)\n(\ndσ2max ρ2minµ 2 min sin 4(θt) + σ2max +maxj ||µj ||2 M2 cos2(θt) sin 4(θt) + σ2max maxj ||µj||2 +maxj ||µj ||4 ρ4minµ 4 min sin 4(θt) )\nfor some constant a11. If |τ jt | ≤ √\nln 92π , for all j, then, with probability at least 1− δ, cos2(θt+1) ≥ cos2(θt)(1 + a12 min(1,M\n2 +MV ) sin2(θt)); otherwise, with probability at least 1− δ, cos2(θt+1) ≥ cos2(θt)(1 + a13 ρ2minµ 2 min tan 2(θt)\nV 2+ρ2minµ 2 min\n), where a12 and a13 are fixed constants.\nFor a mixture of two identical Gaussians with equal mixing weights and standard deviation 1, our result implies:\nCorollary 14. Suppose that µ = ||µ1|| = ||µ2|| ≥ √\nln 92π , and suppose that the number of samples\nin round t is at least: a11 log 2(d/δ)\n(\nd µ2 sin4(θt) + 1 µ2 cos2(θt) sin4(θt)\n)\n, for some constant a11. If |τ jt | ≤ √\nln 92π , then, with probability at least 1− δ, cos2(θt+1) ≥ cos2(θt)(1 + a12 sin2(θt)); otherwise, with probability 1− δ, cos2(θt+1) ≥ cos2(θt)(1 + a13 tan2(θt)), where a12 and a13 are fixed constants.\nAgain, if we pick u0 uniformly at random, we require about Ω̃( d\nµ2 ) samples for the first round\nto succeed. When µ > 1, this bound is worse than d µ4 , but matches with the upper bounds of [BCOFZ07]. The following corollary shows the number of samples required in total for 2-meansiterate to converge.\nCorollary 15. Let µ ≥ √\nln 92π . Suppose u0 is chosen uniformly at random and the number of\nrounds is N ≥ C0 · (ln d + 1ln(1+ǫ)), where C0 is the constant in Corollary 6. If |S| is at least 2NC0d log\n2(d) µ2ǫ2 , then, with constant probability, after N rounds, cos2(θN ) ≥ 1− ǫ."
    }, {
      "heading" : "5 Lower Bounds",
      "text" : "In this section, we prove a lower bound on the sample complexity of learning mixtures of Gaussians, using Fano’s Inequality [Yu97, CT05], stated in Theorem 19. Our main theorem in this section can be summarized as follows.\nTheorem 16. Suppose we are given samples from the mixture D(µ) = 12N (µ, Id) + 12N (−µ, Id), for some µ, and let µ̂ be the estimate of µ computed from n samples. If n < Cd||µ||2 for some constant C, and ||µ|| > 1, then, there exists µ such that ED(µ)||µ− µ̂|| ≥ C ′||µ||, where C ′ is a constant.\nThe main tools in the proof of Theorem 16 are the following lemmas, and a generalized version of Fano’s Inequality [CT05, Yu97].\nLemma 17. Let µ1, µ2 ∈ Rd, and let D1 and D2 be the following mixture distributions: D1 = 1 2N (µ1, Id) + 12N (−µ1, Id), and D2 = 12N (µ2, Id) + 12N (−µ2, Id). Then,\nKL(D1,D2) ≤ 1√ 2π\n· ( ||µ2||2 − ||µ1||2 + 3 √ 2π\n2 ln 2 + 2||µ1||(e−||µ1||\n2/2 + √ 2π||µ1||Φ(0, ||µ1||))\n)\nLemma 18. There exists a set of vectors V = {v1, . . . , vK} in Rd with the following properties: (1) For each i and j, d(vi, vj) ≥ 15 , d(vi,−vj) ≥ 15 . (2) K = ed/10. (3) For all i, ||vi|| ≤ √ 7 5 .\nTheorem 19 (Fano’s Inequality). Consider a class of densities F , which contains r densities f1, . . . , fr, corresponding to parameter values θ1, . . . , θr. Let d(·) be any metric on θ, and let θ̂ be an estimate of θ from n samples from a density f in F . If, for all i and j, d(θi, θj) ≥ α, and KL(fi, fj) ≤ β, then, maxj Ejd(θ̂, θj) ≥ α2 (1 − nβ+log 2 log(r−1) ), where Ej denotes the expectation with respect to distribution j.\nProof. (Of Theorem 16) We apply Fano’s Inequality. Our class of densities F is the class of all mixtures of the form 12N (µ′, Id)+ 12N (−µ′, Id). We set the parameter θ = µ′, and d(µ1, µ2) = ||µ1− µ2||. We construct a subclass F = {f1, . . . , fr} of F as follows. We set each fi = 12N (||µ||vi, Id) + 1 2N (−||µ||vi, Id), for each vector vi in V in Lemma 18. Notice that now r = ed/10. Moreover, for each pair i and j, from Lemma 17 and Lemma 18, KL(fi, fj) ≤ C1||µ||2 + C2, for constants C1 and C2. Finally, from Lemma 18, for each pair i and j, d(µi, µj) ≥ ||µ||5 . The Theorem now follows by an application of Fano’s Inequality 19.\n6 More General k-means\nIn this section, we show that when we apply 2-means on an input generated by a mixture of k spherical Gaussians, the normal to the hyperplane which partitions the two clusters in the 2-means algorithm, converges to a vector in the subspace M containing the means of mixture components. We assume that our input is generated by a mixture of k spherical Gaussians, with means µj, variances (σj)2, j = 1, . . . , k, and mixing weights ρ1, . . . , ρk. The mixture is centered at the origin such that ∑\nρjµj = 0. We use M to denote the subspace containing the means µ1, . . . , µk. We use Algorithm 2-means-iterate on this input, and our goal is to show that it still converges to a vector in M. Notation. In the sequel, given a vector x and a subspace W , we define the angle between x and W as the angle between x and the projection of x onto W . We examine the angle θt, between ut and M, and our goal is to show that the cosine of this angle grows as t increases. Our main result of this section is Lemma 20, which exactly defines the behavior of 2-means-iterate on a mixture of k spherical Gaussians. Recall that at time t, we use ŭt to partition the input data, and the projection of ŭt along M is cos(θt) by definition. Let b1t be a unit vector lying in the subspace M such that: ŭt = cos(θt)b 1 t + sin(θt)vt, where vt lies in the orthogonal complement of M, and has norm 1. We define a second vector ŭ⊥t as follows: ŭ ⊥ t = sin(θt)b 1 t − cos(θt)vt. We observe that 〈ŭt, ŭ⊥t 〉 = 0, ||ŭ⊥t || = 1, and the projection of ŭ⊥t on M is sin(θt)b1t .We now extend the set {b1t } to complete an orthonormal basis B = {b1t , . . . , bk−1t } of M. We also observe that {b2t , . . . , bk−1t , ŭt, ŭ⊥t } is an orthonormal basis of the subspace spanned by any basis of M, along with vt, and can be extended to a basis of Rd.\nFor j = 1, . . . , k, we define τ jt as follows: τ t j = 〈µj , ŭt〉 = cos(θt)〈µj , b1t 〉. Finally we (re)-define\nthe quantity ξt, and define m l t, for l = 1, . . . , k − 1 as\nξt = ∑\nj\nρjσj e−(τ j t ) 2/2(σj )2\n√ 2π\n, mlt = ∑\nj\nρjΦ(− τ j t\nσj ,∞)〈µj , blt〉\nOur main lemma is stated below. The proof is in the Appendix.\nLemma 20. At any iteration t of Algorithm 2-means-iterate,\ncos2(θt+1) = cos 2(θt)\n(\n1 + tan2(θt) 2 cos(θt)ξtm\n1 t +\n∑\nl(m l t) 2\nξ2t + 2cos(θt)ξtm 1 t +\n∑\nl(m l t) 2\n)"
    }, {
      "heading" : "6.1 Proof of Lemma 1",
      "text" : "In this section, we prove Lemma 1. First, we need some additional notation.\nNotation. We define, for j = 1, 2:\nwjt+1 = Pr[x ∼ Dj|x ∈ Ct+1] ujt+1 = E[x|x ∼ Dj , x ∈ Ct+1]\nWe observe that ut+1 now can be written as:\nut+1 = w 1 t+1u 1 t+1 + w 2 t+1u 2 t+1\nMoreover, we define Zt+1 = Pr[x ∈ Ct+1]. Proof of Lemma 1. We start by providing exact expressions for w1t+1 and w 2 t+1 with respect to the partition computed in the previous round t. These are used to compute the projections of ut+1 along the vectors ŭt and µ1 − 〈µ1, ŭt〉ŭt, which finally leads to a proof of Lemma 1.\nLemma 21. In round t, for j = 1, 2, wjt+1 = ρjΦ(− τ\nj t\nσj ,∞)\nZt+1 .\nProof. We can write:\nwjt+1 = Pr[x ∈ Ct+1|x ∼ Dj] Pr[x ∼ Dj ]\nPr[x ∈ Ct+1] We note that Pr[x ∼ Dj ] = ρj , and Pr[x ∈ Ct+1] = Zt+1.\nAs Dj is a spherical Gaussian, for any x generated from Dj , and for any vector y orthogonal to ut, 〈y, x〉 is distributed independently from 〈ŭt, x〉. Moreover, we observe that 〈ŭt, x〉 is distributed as a Gaussian with mean 〈µj, ŭt〉 = τ jt and standard deviation σj. Therefore,\nPr[x ∈ Ct+1|x ∼ Dj ] = Pr x∼Dj [〈ŭt, x〉 > 0] = Pr[N(τ jt , σj) ≥ 0] = Φ(− τ jt σj ,∞)\nfrom which the lemma follows.\nLemma 22. For any t, 〈ut+1, ŭt〉 = ξt+mt cos(θt)Zt+1 .\nProof. Consider a sample x drawn from Dj . Then, 〈x, ŭt〉 is distributed as a Gaussian with mean 〈µj , ŭt〉 = τ jt and standard deviation σj . We recall that Pr[x ∈ Ct+1] = Zt+1. Therefore, 〈ujt+1, ŭt〉 is equal to:\nE[x, x ∈ Ct+1|x ∼ Dj] Pr[x ∈ Ct+1|x ∼ Dj ] = 1\nPr[N(τ jt , σ j) > 0]\n· ∫ ∞\ny=0\nye−(y−τ j t ) 2/2(σj )2\nσj √ 2π\ndy\nwhich is, again, equal to:\n1\nΦ(− τ j t\nσj ,∞)\n(\nτ jt\n∫ ∞\ny=0\ne−(y−τ j t ) 2/2(σj )2\nσj √ 2π\ndy +\n∫ ∞\ny=0\n(y − τ jt )e−(y−τ j t ) 2/2(σj )2\nσj √ 2π\ndy\n)\n= 1\nΦ(− τ j t\nσj ,∞)\n(\nτ jt Φ(− τ jt σj ,∞) + ∫ ∞\ny=0\n(y − τ jt )e−(y−τ j t ) 2/2(σj )2\nσj √ 2π\ndy\n)\nWe can compute the integral in the equation above as follows.\n∫ ∞\ny=0 (y − τ jt )e−(y−τ j t )\n2/2(σj )2dy = (σj)2 ∫ ∞\nz=(τ jt ) 2/2(σj )2\ne−zdz = (σj)2e−(τ j t ) 2/2(σj )2\nWe can now compute 〈ut+1, ŭt〉 as follows.\n〈ut+1, ŭt〉 = w1t+1〈u1t+1, ŭt〉+ w2t+1〈u2t+1, ŭt〉 = 1 Zt+1 · ∑\nj\n(\nρjτ jt Φ(− τ jt σj ,∞) + ρj(σj)2 e −(τ jt )2/2(σj )2 σj √ 2π\n)\nThe lemma follows by recalling τ jt = 〈µj, b〉 cos(θt) and plugging in the values of mt and ξt.\nLemma 23. Let v̆t be a unit vector along µ1−〈µ1, ŭt〉ŭt. Then, 〈ut+1, v̆t〉 = mt sin(θt)Zt+1 . In addition, for any vector z orthogonal to ŭt and v̆t, 〈ut+1, z〉 = 0.\nProof. We observe that for a sample x drawn from distribution D1 (respectively, D2) and any unit vector v1, orthogonal to ŭt, 〈x, v1〉 is distributed as a Gaussian with mean 〈µ1, v1〉 (〈µ2, v1〉, respectively) and standard deviation σ1 (resp. σ2). Therefore, the projection of ut+1 on v̆t can be written as:\n〈ut+1, v̆t〉 = ∑\nj\nwjt+1〈µj , v̆t〉 = 1\nZt+1\n∑\nj\nρjΦ(− τ j t\nσj ,∞)〈µj , v̆t〉\nfrom which the first part of the lemma follows. The second part of the lemma follows from the observation that for any vector z orthogonal to\nŭt and v̆t, 〈µj , z〉 = 0, for j = 1, 2.\nLemma 24. For any t,\n〈ut+1, µ1〉 = ||µ1||(ξt cos(θt) +mt)\nZt+1\n||ut+1||2 = ξ2t +m 2 t + 2ξtmt cos(θt)\n(Zt+1)2\nProof. As we have an infinite number of samples, θt+1 lies on the same plane as θt. Therefore, we can write 〈ut+1, µ1〉 = 〈ut+1, ŭt〉〈µ1, ŭt〉 + 〈ut+1, v̆t〉〈µ1, v̆t〉. Moreover, we can write ||ut+1||2 = 〈ut+1, ŭt〉2+〈ut+1, v̆t〉2. Thus, the first two equation follow by using Lemma 22 and 23, and recalling that 〈µ1, ŭt〉 = τ1t = ||µ1|| cos(θt) and 〈µ1, v̆t〉 = ||µ1|| sin(θt).\nWe are now ready to complete the proof of Lemma 1.\nProof. (Of Lemma 1) By definition of θt+1, cos 2(θt+1) = 〈ut+1,µ1〉2 ||ut+1||2||µ1||2 . Therefore,\n||µ1||2 cos2(θt+1) = 〈ut+1, µ1〉2 ||ut+1||2\n= (τ1t ) 2\n( 1 + 〈ut+1, µ1〉2 − ||µ1||2 cos2(θt)||ut+1||2 ||µ1||2 cos2(θt)||ut+1||2 )\n= (τ1t ) 2\n( 1 + ||µ1||2 sin2(θt)(m2t + 2ξtmt cos(θt)) ||µ1||2 cos2(θt)||ut+1||2 )\n= ||µ1||2 cos2(θt) ( 1 + tan2(θt) m2t + 2ξtmt cos(θt) ||ut+1||2 )\nwhere we used Lemma 24 and the observation that cos(θt) = τ1t\n||µ1|| . The Lemma follows by replacing\n||ut+1||2 using the expression in Lemma 24.\nThe next Lemma helps us to derive Theorem 2 from Lemma 1. It shows how to approximate Φ(−τ, τ) when τ is small.\nLemma 25. Let τ ≤ √\nln 92π . Then, 5 3 √ 2π τ ≤ Φ(−τ, τ) ≤ 2√ 2π τ . In addition, 2e −τ2/2√ 2π ≥ 23 ."
    }, {
      "heading" : "6.2 Proofs of Sample Requirement Bounds",
      "text" : "For the rest of the section, we prove Lemmas 8 and 9, which lead to a proof of Lemma 7. First, we need to define some notation.\nNotation. At time t, we use the notation St+1 to denote the quantity E[X · 1X∈Ct+1 ], where 1X∈Ct+1 is the indicator function for the event X ∈ Ct+1, and the expectation is taken over the entire mixture.\nIn the sequel, we also use the notation Ŝt+1 to denote the empirical value of St+1. Our goal is to bound the concentration of certain functions of Ŝt+1 around their expected values, when we are given only n samples from the mixture. Recall that we define θt+1 as the angle between µ\n1 and the hyperplane separator in 2-means-iterate, given θt. Notice that now θt is a random variable, which depends on the samples drawn in rounds 1, . . . , t−1, and given θt, θt+1 is a random variable, whose value depends on samples in round t. Also we use ut+1 as the center of partition Ct in iteration t+1, and E[ut+1] is the expected center. Note that all the expectations in round t are conditioned on θt.\nProofs. We are now ready to prove Lemmas 8 and 9.\nProof. (Of Lemma 8) Let X1, . . . ,Xn be the n iid samples from the mixture; for each i, we can write the projection of Xi along v as follows:\n〈Xi, v〉 = Yi + Zi\nwhere Zi ∼ N(0, σj), if Xi is generated from distribution Dj, and Yi = 〈µj, v〉, if Xi is generated by Dj. Therefore, we can write:\n〈Ŝt+1, v〉 = 1\nn\n(\n∑\ni\nYi · 1Xi∈Ct+1 + ∑\ni\nZi · 1Xi∈Ct+1\n)\nTo determine the concentration of 〈Ŝt+1, v〉 around its expected value, we address the two terms separately.\nThe first term is a sum of n independently distributed random variables, such that changing\none variable changes the sum by at most maxj 2|〈µj ,v〉|\nn ; therefore, to calculate its concentration,\none can apply Hoeffding’s Inequality. It follows that with probability at most δ2 ,\n| 1 n ∑\ni\nYi · 1Xi∈Ct+1 −E[ 1\nn\n∑\ni\nYi · 1Xi∈Ct+1 ]| > max j\n4|〈µj , v〉| √\nlog(4n/δ)√ n\nWe note that, in the second term, each Zi is a Gaussian with mean 0 and variance σ j , scaled\nby ||v||. For some 0 ≤ δ′ ≤ 1, let Ei(δ′) denote the event\n−σmax||v|| √ 2 log(1/δ′) ≤ Zi · 1Xi∈Ct+1 ≤ σmax||v|| √ 2 log(1/δ′)\nAs Zi ∼ N(0, σj), if Xi is generated from distribution Dj , and 1Xi∈Ct+1 takes values 0 and 1, for any i, for δ′ small enough,Pr[Ei(δ′)] ≥ 1− δ′.\nWe use δ′ = δ4n , and condition on the fact that all the events {Ei(δ′), i = 1, . . . , n} happen; using an Union bound over the events ¯Ei(δ′), the probability that this holds is at least 1− δ4 . We also observe that, as the Gaussians Zi are independently distributed, conditioned on the union of the events Ei, the Gaussians Zi are still independent. Therefore, conditioned on the event ∪iEi(δ′),\n1 n\n∑\ni Zi · 1Xi∈Ct+1 is the sum of n independent random variables, such that changing one variable changes the sum by at most 2σmax||v|| √ 2 log(1/δ′)\nn . We can now apply Hoeffding’s bound to conclude\nthat with probability at least 1− δ2 ,\n| 1 n ∑\ni\nZi·1Xi∈Ct+1−E[ 1\nn\n∑\ni\nZi·1Xi∈Ct+1]| ≤ 4σmax||v||\n√ 2 log(1/δ′) √\n2 log(1/δ)√ n ≤ 8σmax||v|| log(4n/δ)√ n\nThe lemma now follows by applying an union bound.\nProof. (Of Lemma 9) We can write:\n||Ŝt+1||2 ≤ ||St+1||2 + ||Ŝt+1 − St+1||2 + 2|〈Ŝt+1 − St+1, St+1〉|\nIf v1, . . . , vd is any orthonormal basis of R d, then, we can bound the second term as follows.\nWith probability at least 1− δ2 ,\n||Ŝt+1 − St+1||2 = d ∑\ni=1\n(〈Ŝt+1 − St+1, vi〉)2 ≤ 128 log2(8n/δ)\nn ( ∑\ni\nσ2max||vi||2 + ∑\ni,j\n〈µj , vi〉2)\n≤ 128 log 2(8n/δ)\nn (σ2maxd+\n∑\nj\n(µj)2)\nThe second step follows by the application of Lemma 8, and the fact that for any a and b, (a+ b)2 ≤ 2(a2 + b2).\nUsing Lemma 8, with probability at least 1− δ2 ,\n〈Ŝt+1 − St+1, St+1〉 ≤ 8 log(8n/δ)√\nn (σmax||St+1||+max j |〈St+1, µj〉|)\nThe lemma follows by a union bound over these two above events."
    }, {
      "heading" : "6.3 Proofs of Lower Bounds",
      "text" : "Proof. (Of Lemma 17) Let P be the plane containing the origin O and the vectors µ1 and µ2. If v is a vector orthogonal to P , then, the projection of D1 along v is a Gaussian N (0, 1), which is distributed independently of the projection of D1 along P (and same is the case for D2).Therefore, to compute the KL-Divergence of D1 and D2, it is sufficient to compute the KL-Divergence of the projections of D1 and D2 along the plane P .\nLet x be a vector in P . Then,\nKL(D1,D2) = 1√ 2π\n∫\nx∈P ( 1 2 e−||x−µ1|| 2/2 + 1 2 e−||x+µ1|| 2/2) ln\n(\n1 2e −||x−µ1||2/2 + 12e −||x+µ1||2/2 1 2e −||x−µ2||2/2 + 12e −||x+µ2||2/2\n)\ndx\n= 1√ 2π\n∫\nx∈P ( 1 2 e−||x−µ1|| 2/2 + 1 2 e−||x+µ1|| 2/2) ln\n(\ne−||x+µ1|| 2/2 · (1 + e2〈x,µ1〉) e−||x+µ2||2/2 · (1 + e2〈x,µ2〉)\n)\ndx\n= 1√ 2π\n∫\nx∈P ( 1 2 e−||x−µ1|| 2/2 + 1 2 e−||x+µ1|| 2/2)\n(\n(||x+ µ2||2 − ||x+ µ1||2) + ln 1 + e2〈x,µ1〉\n1 + e2〈x,µ2〉\n)\ndx\nWe observe that for any x, ||x + µ2||2 − ||x + µ1||2 = ||µ2||2 − ||µ1||2 + 2〈x, µ2 − µ1〉. As the expected value of D1 is 0, we can write that:\n∫\nx∈P ( 1 2 e−||x−µ1|| 2/2 + 1 2 e−||x+µ1|| 2/2)〈x, µ2 − µ1〉 = Ex∼D1〈x, µ1 − µ2〉 = 0 (1)\nWe now focus on the case where ||µ1|| >> 1. We observe that for any µ2 and any x, 1+e2〈x,µ2〉 > 1. Therefore, combining the previous two equations,\nKL(D1,D2) ≤ 1√ 2π\n(\n||µ2||2 − ||µ1||2 + ∫ x∈P ( 1 2 e−||x−µ1|| 2/2 + 1 2 e−||x+µ1|| 2/2) ln(1 + e2〈x,µ1〉)dx\n)\nAgain, since the projection of D1 perpendicular to µ1 is distributed independently of the projection of D1 along µ1, the above integral can be taken over a one-dimensional x which varies along the vector µ1. For the rest of the proof, we abuse notation, and use µ1 to denote both the vector µ1 and the scalar ||µ1||. We can write:\n∫ ∞\nx=−∞ ( 1 2 e−(x−µ1) 2/2 + 1 2 e−(x+µ1) 2/2) ln(1 + e2µ1x)dx\n≤ √ 2π ln 2 + ∫ ∞\nx=0 ( 1 2 e−(x−µ1) 2/2 + 1 2 e−(x+µ1) 2/2) ln(1 + e2µ1x)dx\n≤ √ 2π ln 2 + ∫ ∞\nx=0 ( 1 2 e−(x−µ1) 2/2 + 1 2 e−(x+µ1) 2/2)(ln 2 + 2xµ1)dx\n≤ 3 √ 2π\n2 ln 2 + 2µ1\n∫ ∞\nx=0 ( 1 2 e−(x−µ1) 2/2 + 1 2 e−(x+µ1) 2/2)xdx\nThe first part follows because for x < 0, ln(1 + e2xµ1) ≤ ln 2. The second part follows because for x > 0, ln(1 + e2xµ1) ≤ ln(2e2xµ1). The third part follows from the symmetry of D1 around the origin.\nNow, for any a, we can write:\n1√ 2π\n∫ ∞\nx=0 xe−(x+a) 2/2dx = 1√ 2π · e−a2/2 − aΦ(a,∞)\nPlugging this in, we can show that,\nKL(D1,D2) ≤ 1√ 2π\n( ||µ2||2 − ||µ1||2 + 3 √ 2π\n2 ln 2 + 2||µ1||(e−||µ1||\n2/2 + √ 2π||µ1||Φ(0, ||µ1||))\n)\nfrom which the lemma follows.\nProof. (Of Lemma 18) For each i, let each vi be drawn independently from the distribution 1√ d N (0, Id). For each i, j, let Pij = d2 · d(vi, vj) and Nij = d2 · d(vi,−vj). Then, for each i and j, Pij and Nij are distributed according to the Chi-squared distribution with parameter d. From Lemma 26, it follows that: Pr[Pij < d 10 ] ≤ e−3d/10. A similar lemma can also be shown to hold for the random variables Nij . Applying the Union Bound, the probability that this holds for Pij and Nij for all pairs (i, j), i ∈ V, j ∈ V is at most 2K2e−3d/10. This probability is at most 12 when K = ed/10.\nIn addition, we observe that for each vector vi, d · ||vi||2 is also distributed as a Chi-squared distribution with parameter d. From Lemma 26, for each i, Pr[||vi||2 > 7/5] ≤ e−2d/15. The second part of the lemma now follows by an Union Bound over all K vectors in the set V .\nLemma 26. Let X be a random variable, drawn from the Chi-squared distribution with parameter d. Then,\nPr[X < d 10 ] ≤ e−3d/10\nMoreover,\nPr[X > 7d 5 ] ≤ e−2d/15\nProof. Let Y be the random variable defined as follows: Y = d−X. Then,\nPr[X < d\n10 ] = Pr[Y >\n9d 10 ] = Pr[etY > e9dt/10] ≤ E[e\ntY ] e9dt/10\nwhere the last step uses a Markov’s Inequality. We observe that E[etY ] = etdE[e−tX ] = etd(1 − 2t)d/2, for t < 12 . The first part of the lemma follows from the observation that (1− 2t)d/2 ≤ e−td, and by plugging in t = 13 .\nFor the second part, we again observe that\nPr[X > 7d 5 ] ≤ (1− 2t)−d/2e−7dt/5 ≤ e−2dt/5\nThe lemma now follows by plugging in t = 13 .\n6.4 More General k-means : Results and Proofs\nIn this section, we show that when we apply 2-means on an input generated by a mixture of k spherical Gaussians, the normal to the hyperplane which partitions the two clusters in the 2-means algorithm, converges to a vector in the subspace M containing the means of mixture components. This subspace is interesting because, in this subspace, the distance between the means is as high as in the original space; however, if the number of clusters is small compared to the dimension, the distance between two samples from the same cluster is much smaller. In fact, several algorithms for learning mixture models [VW02, AM05, CR08] attempt to isolate this subspace first, and then use some simple clustering methods in this subspace."
    }, {
      "heading" : "6.4.1 The Setting",
      "text" : "We assume that our input is generated by a mixture of k spherical Gaussians, with means µj, variances (σj)2, j = 1, . . . , k, and mixing weights ρ1, . . . , ρk. The mixture is centered at the origin such that ∑\nρjµj = 0. We use M to denote the subspace containing the means µ1, . . . , µk. We use Algorithm 2-means-iterate on this input, and our goal is to show that it still converges to a vector in M. In the sequel, given a vector x and a subspace W , we define the angle between x and W as the angle between x and the projection of x onto W . As in Sections 2 and 3, we examine the angle θt, between ut and M, and our goal is to show that the cosine of this angle grows as t increases. Our main result of this section is Lemma 20, which, analogous to Lemma 1 in Section 3, exactly defines the behavior of 2-means on a mixture of k spherical Gaussians.\nBefore we can prove the lemma, we need some additional notation."
    }, {
      "heading" : "6.4.2 Notation",
      "text" : "Recall that at time t, we use ŭt to partition the input data, and the projection of ŭt along M is cos(θt) by definition. Let b 1 t be a unit vector lying in the subspace M such that:\nŭt = cos(θt)b 1 t + sin(θt)vt\nwhere vt lies in the orthogonal complement of M, and has norm 1. We define a second vector ŭ⊥t as follows:\nŭ⊥t = sin(θt)b 1 t − cos(θt)vt\nWe observe that 〈ŭt, ŭ⊥t 〉 = 0, ||ŭ⊥t || = 1, and the projection of ŭ⊥t on M is sin(θt)b1t . We now extend the set {b1t } to complete an orthonormal basis B = {b1t , . . . , bk−1t } of M. We also observe that {b2t , . . . , bk−1t , ŭt, ŭ⊥t } is an orthonormal basis of the subspace spanned by any basis of M, along with vt, and can be extended to a basis of Rd.\nFor j = 1, . . . , k, we define τ jt as follows:\nτ tj = 〈µj, ŭt〉 = cos(θt)〈µj , b1t 〉\nFinally we (re)-define the quantity ξt as\nξt = ∑\nj\nρjσj e−(τ j t ) 2/2(σj )2\n√ 2π\nand, for any l = 1, . . . , k − 1, we define:\nmlt = ∑\nj\nρjΦ(− τ j t\nσj ,∞)〈µj , blt〉"
    }, {
      "heading" : "6.4.3 Proof of Lemma 20",
      "text" : "The main idea behind the proof of Lemma 20 is to estimate the norm and the projection of ut+1; we do this in three steps. First, we estimate the projection of ut+1 along ŭt; next, we estimate this projection on ŭ⊥t , and finally, we estimate its projection along b 2 t , . . . , b l t. Combining these projections, and observing that the projection of ut+1 on any direction perpendicular to these is 0, we can prove the lemma.\nAs before, we define Zt+1 = Pr[x ∈ Ct+1]\nNow we make the following claim.\nLemma 27. For any t and any j,\nPr[x ∼ Dj |x ∈ Ct+1] = ρj Zt+1 Φ(− τ j t σj ,∞)\nProof. Same proof of Lemma 21\nNext, we estimate the projection of ut+1 along ŭt.\nLemma 28.\n〈ut+1, ŭt〉 = ξt + cos(θt)m\n1 t\nZt+1\nProof. Consider a sample x drawn from distribution Dj. The projection of x on ŭt is distributed as a Gaussian with mean τ jt and standard deviation σ j . The probability that x lies in Ct+1 is Pr[N(τ jt , σ j) > 0] = Φ(− τ j t σj ,∞). Given that x lies in Ct+1, the projection of x on ŭt is distributed as a truncated Gaussian, with mean τ jt and standard deviation σ j, which is truncated at 0. Therefore,\nE[〈x, ŭt〉|x ∈ Ct+1, x ∼ Dj ] = 1\nΦ(− τ j t\nσj ,∞)\n(\n∫ ∞\ny=0\nye−(y−τ j t ) 2/2\nσj √ 2π dy\n)\nwhich is again equal to\n1\nΦ(− τ j t\nσj ,∞)\n(\nτ jt\n∫ ∞\ny=0\ne−(y−τ j t ) 2/2(σj )2\nσj √ 2π\ndy +\n∫ ∞\ny=0\n(y − τ jt )e−(y−τ j t ) 2/2(σj )2\nσj √ 2π\ndy\n)\n= 1\nΦ(− τ j t\nσj ,∞)\n(\nτ jt Φ(− τ jt σj ,∞) + ∫ ∞\ny=0\n(y − τ jt )e−(y−τ j t ) 2/2(σj )2\nσj √ 2π\ndy\n)\nWe can evaluate the integral in the equation above as follows. ∫ ∞\ny=0 (y − τ jt )e−(y−τ j t )\n2/2(σj )2dy = (σj)2 ∫ ∞\nz=(τ jt ) 2/2(σj )2\ne−zdz = (σj)2e−(τ j t ) 2/2(σj )2\nTherefore we can conclude that\nE[〈x, ŭt〉|x ∈ Ct+1, x ∼ Dj ] = τ jt + 1\nΦ(− τ j t\nσj ,∞)\n· σj e −(τ jt )2/2(σj )2\n√ 2π\nNow we can write\n〈ut+1, ŭt〉 = ∑\nj\nE[〈x, ŭt〉|x ∼ Dj , x ∈ Ct+1] Pr[x ∼ Dj |x ∈ Ct+1]\n= 1\nZt+1\n∑\nj\nρjΦ(− τ j t\nσj ,∞)E[〈x, ŭt〉|x ∼ Dj , x ∈ Ct+1]\nwhere we used lemma 27. The lemma follows by recalling that τ jt = cos(θt)〈µj , b1t 〉.\nLemma 29. For any t,\n〈ut+1, ŭ⊥t 〉 = sin(θt)m\n1 t\nZt+1\nProof. Let x be a sample drawn from distribution Dj. Since ŭ ⊥ t is perpendicular to ŭt, and Dj is a spherical Gaussian, given that x ∈ Ct+1, that is, the projection of x on ŭt is greater than 0, the projection of x on ŭ⊥t is still distributed as a Gaussian with mean 〈µj , ŭ⊥t 〉 and standard deviation σj . That is, E[〈x, ŭ⊥t 〉|x ∼ Dj , x ∈ Ct+1] = 〈µj , ŭ⊥t 〉 Also recall that, by definition of ŭ⊥t , 〈µj, ŭ⊥t 〉 = sin(θt)〈µj , b1t 〉. To prove the lemma, we observe that 〈ut+1, ŭ⊥t 〉 is equal to\n∑\nj\nE[〈x, ŭ⊥t 〉|x ∼ Dj , x ∈ Ct+1] Pr[x ∼ Dj |x ∈ Ct+1]\nThe lemma follows by using lemma 27.\nLemma 30. For l ≥ 2, 〈ut+1, blt〉 =\nmlt Zt+1\nProof. Let x be a sample drawn from distribution Dj . Since b l t is perpendicular to ŭt, and Dj is a spherical Gaussian, given that x ∈ Ct+1, that is, the projection of x on ŭt is greater than 0, the projection of x on blt is still distributed as a Gaussian with mean 〈µj , blt〉 and standard deviation σj . That is, E[〈x, blt〉|x ∼ Dj , x ∈ Ct+1] = 〈µj , blt〉 To prove the lemma, we observe that 〈blt, ut+1〉 is equal to\n∑\nj\nE[〈x, blt〉|x ∼ Dj , x ∈ Ct+1] Pr[x ∼ Dj |x ∈ Ct+1]\nThe lemma follows by using lemma 27.\nFinally, we show a lemma which estimates the norm of the vector ut+1.\nLemma 31.\n||ut+1||2 = 1\nZ2t+1 (ξ2t + 2ξt cos(θt)m 1 t +\nk ∑\nl=1\n(mlt) 2)\nProof. Combining Lemmas 28, 29 and 30, we can write:\n||ut+1||2 = 〈ŭt, ut+1〉2 + 〈ŭ⊥t , ut+1〉2 + ∑ l≥2 〈blt, ut+1〉2\n= 1\nZ2t+1\n(\nξ2t + 2ξt cos(θt)m 1 t + cos 2(θt)(m 1 t ) 2 + sin2(θt)(m 1 t ) 2 +\nk ∑\nl=2\n(mlt) 2\n)\nThe lemma follows by plugging in the fact that cos2(θt) + sin 2(θt) = 1.\nNow we are ready to prove Lemma 20.\nProof. (Of Lemma 20) Since b1t , . . . , b k t form a basis of M, we can write:\ncos2(θt+1) = ∑k l=1〈ut+1, blt〉2 ||ut+1||2\n(2)\n||ut+1||2 is estimated in Lemma 31, and 〈ut+1, blt〉 is estimated by Lemma 29. Using these lemmas, as b1t lies in the subspace spanned by the orthogonal vectors ŭt and ŭ ⊥ t , we can write:\n〈ut+1, b1t 〉 = 〈ŭt, ut+1〉〈ŭt, b1t 〉+ 〈ŭ⊥t , ut+1〉〈ŭ⊥t , b1t 〉\n= cos(θt)ξt +m\n1 t\nZt+1\nPlugging this in to Equation 2, we get:\ncos2(θt+1) = ξ2t cos 2(θt) + 2ξt cos(θt)m 1 t +\n∑\nl(m l t) 2\nξ2t + 2ξt cos(θt)m 1 t +\n∑\nl(m l t) 2\nThe lemma follows by rearranging the above equation, similar to the proof of Lemma 1."
    } ],
    "references" : [ {
      "title" : "Learning mixtures of separated nonspherical Gaussians",
      "author" : [ "S. Arora", "R. Kannan" ],
      "venue" : "Ann. Applied Prob.,",
      "citeRegEx" : "Arora and Kannan.,? \\Q2005\\E",
      "shortCiteRegEx" : "Arora and Kannan.",
      "year" : 2005
    }, {
      "title" : "On spectral learning of mixtures of distributions",
      "author" : [ "D. Achlioptas", "F. McSherry" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Achlioptas and McSherry.,? \\Q2005\\E",
      "shortCiteRegEx" : "Achlioptas and McSherry.",
      "year" : 2005
    }, {
      "title" : "k-means has polynomial smoothed complexity",
      "author" : [ "D. Arthur", "B. Manthey", "H. Röglin" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Arthur et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Arthur et al\\.",
      "year" : 2009
    }, {
      "title" : "How slow is the k-means method",
      "author" : [ "D. Arthur", "S. Vassilvitskii" ],
      "venue" : "SoCG,",
      "citeRegEx" : "Arthur and Vassilvitskii.,? \\Q2006\\E",
      "shortCiteRegEx" : "Arthur and Vassilvitskii.",
      "year" : 2006
    }, {
      "title" : "Separating populations with wide data: A spectral analysis",
      "author" : [ "A. Blum", "A. Coja-Oghlan", "A.M. Frieze", "S. Zhou" ],
      "venue" : "In ISAAC,",
      "citeRegEx" : "Blum et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 2007
    }, {
      "title" : "Isotropic PCA and affine-invariant clustering",
      "author" : [ "S.C. Brubaker", "S. Vempala" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Brubaker and Vempala.,? \\Q2008\\E",
      "shortCiteRegEx" : "Brubaker and Vempala.",
      "year" : 2008
    }, {
      "title" : "Learning Mixtures of Distributions",
      "author" : [ "K. Chaudhuri" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Chaudhuri.,? \\Q2007\\E",
      "shortCiteRegEx" : "Chaudhuri.",
      "year" : 2007
    }, {
      "title" : "A rigorous analysis of population stratification with limited data",
      "author" : [ "K. Chaudhuri", "E. Halperin", "S. Rao", "S. Zhou" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning mixtures of distributions using correlations and independence",
      "author" : [ "K. Chaudhuri", "S. Rao" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Chaudhuri and Rao.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chaudhuri and Rao.",
      "year" : 2008
    }, {
      "title" : "Learning mixtures of gaussians",
      "author" : [ "S. Dasgupta" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Dasgupta.,? \\Q1999\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 1999
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm (with discussion)",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society B,",
      "citeRegEx" : "Dempster et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "A two-round variant of EM for Gaussian mixtures",
      "author" : [ "S. Dasgupta", "L. Schulman" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Dasgupta and Schulman.,? \\Q2000\\E",
      "shortCiteRegEx" : "Dasgupta and Schulman.",
      "year" : 2000
    }, {
      "title" : "Cluster analysis of multivariate data: Efficiency vs. interpretability of classification",
      "author" : [ "E. Forgey" ],
      "venue" : null,
      "citeRegEx" : "Forgey.,? \\Q1965\\E",
      "shortCiteRegEx" : "Forgey.",
      "year" : 1965
    }, {
      "title" : "The spectral method for general mixture models",
      "author" : [ "R. Kannan", "H. Salmasian", "S. Vempala" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Kannan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kannan et al\\.",
      "year" : 2005
    }, {
      "title" : "Mixture Models:Theory",
      "author" : [ "B.G. Lindsey" ],
      "venue" : "Geometry and Applications. IMS,",
      "citeRegEx" : "Lindsey.,? \\Q1996\\E",
      "shortCiteRegEx" : "Lindsey.",
      "year" : 1996
    }, {
      "title" : "Least squares quantization in PCM",
      "author" : [ "S.P. Lloyd" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "Lloyd.,? \\Q1982\\E",
      "shortCiteRegEx" : "Lloyd.",
      "year" : 1982
    }, {
      "title" : "Some methods for classification and analysis of multivariate observations",
      "author" : [ "J.B. MacQueen" ],
      "venue" : "In Berkeley Symposium on Mathematical Statistics and Probability,",
      "citeRegEx" : "MacQueen.,? \\Q1967\\E",
      "shortCiteRegEx" : "MacQueen.",
      "year" : 1967
    }, {
      "title" : "Improved smoothed analysis of the k-means method",
      "author" : [ "B. Manthey", "H. Röglin" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Manthey and Röglin.,? \\Q2009\\E",
      "shortCiteRegEx" : "Manthey and Röglin.",
      "year" : 2009
    }, {
      "title" : "The effectiveness of lloyd-type methods for the k-means problem",
      "author" : [ "Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Ostrovsky et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ostrovsky et al\\.",
      "year" : 2006
    }, {
      "title" : "Strong consistency of k-means clustering",
      "author" : [ "D. Pollard" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Pollard.,? \\Q1981\\E",
      "shortCiteRegEx" : "Pollard.",
      "year" : 1981
    }, {
      "title" : "Mixture densities, maximum likelihood and the em algorithm",
      "author" : [ "R. Redner", "H. Walker" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Redner and Walker.,? \\Q1984\\E",
      "shortCiteRegEx" : "Redner and Walker.",
      "year" : 1984
    }, {
      "title" : "An investigation of computational and informational limits in gaussian mixture clustering",
      "author" : [ "N. Srebro", "G. Shakhnarovich", "S.T. Roweis" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Srebro et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2006
    }, {
      "title" : "k-means takes exponentially many iterations even in the plane",
      "author" : [ "A. Vattani" ],
      "venue" : "SoCG,",
      "citeRegEx" : "Vattani.,? \\Q2009\\E",
      "shortCiteRegEx" : "Vattani.",
      "year" : 2009
    }, {
      "title" : "A spectral algorithm for learning mixtures of distributions",
      "author" : [ "V. Vempala", "G. Wang" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Vempala and Wang.,? \\Q2002\\E",
      "shortCiteRegEx" : "Vempala and Wang.",
      "year" : 2002
    }, {
      "title" : "On convergence properties of the em algorithm for gaussian mixtures",
      "author" : [ "L. Xu", "M.I. Jordan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Xu and Jordan.,? \\Q1996\\E",
      "shortCiteRegEx" : "Xu and Jordan.",
      "year" : 1996
    }, {
      "title" : "Assaoud, fano and le cam",
      "author" : [ "B. Yu" ],
      "venue" : null,
      "citeRegEx" : "Yu.,? \\Q1997\\E",
      "shortCiteRegEx" : "Yu.",
      "year" : 1997
    } ],
    "referenceMentions" : [ ],
    "year" : 2009,
    "abstractText" : "One of the most popular algorithms for clustering in Euclidean space is the k-means algorithm; k-means is difficult to analyze mathematically, and few theoretical guarantees are known about it, particularly when the data is well-clustered. In this paper, we attempt to fill this gap in the literature by analyzing the behavior of k-means on well-clustered data. In particular, we study the case when each cluster is distributed as a different Gaussian – or, in other words, when the input comes from a mixture of Gaussians. We analyze three aspects of the k-means algorithm under this assumption. First, we show that when the input comes from a mixture of two spherical Gaussians, a variant of the 2-means algorithm successfully isolates the subspace containing the means of the mixture components. Second, we show an exact expression for the convergence of our variant of the 2-means algorithm, when the input is a very large number of samples from a mixture of spherical Gaussians. Our analysis does not require any lower bound on the separation between the mixture components. Finally, we study the sample requirement of k-means; for a mixture of 2 spherical Gaussians, we show an upper bound on the number of samples required by a variant of 2-means to get close to the true solution. The sample requirement grows with increasing dimensionality of the data, and decreasing separation between the means of the Gaussians. To match our upper bound, we show an information-theoretic lower bound on any algorithm that learns mixtures of two spherical Gaussians; our lower bound indicates that in the case when the overlap between the probability masses of the two distributions is small, the sample requirement of k-means is near-optimal.",
    "creator" : "LaTeX with hyperref package"
  }
}