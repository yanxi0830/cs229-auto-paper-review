{
  "name" : "1610.09072.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Orthogonal Random Features",
    "authors" : [ "Felix Xinnan Yu", "Ananda Theertha Suresh", "Krzysztof Choromanski", "Daniel Holtmann-Rice", "Sanjiv Kumar" ],
    "emails" : [ "sanjivk}@google.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Kernel methods are widely used in nonlinear learning [9], but they are computationally expensive for large datasets. Kernel approximation is a powerful technique to make kernel methods scalable, by mapping input features into a new space where dot products approximate the kernel well [20]. With accurate kernel approximation, efficient linear classifiers can be trained in the transformed space while retaining the expressive power of nonlinear methods [11, 22].\nFormally, given a kernel K(·, ·) : Rd × Rd → R, kernel approximation methods seek to find a nonlinear transformation φ(·) : Rd → Rd′ such that, for any x,y ∈ Rd\nK(x,y) ≈ K̂(x,y) = φ(x)Tφ(y).\nRandom Fourier Features [20] are used widely in approximating smooth, shift-invariant kernels. This technique requires the kernel to exhibit two properties: 1) shift-invariance, i.e. K(x,y) = K(∆) where ∆ = x−y; and 2) positive semi-definiteness of K(∆) on Rd. The second property guarantees that the Fourier transform of K(∆) is a nonnegative function [3]. Let p(w) be the Fourier transform of K(z). Then,\nK(x− y) = ∫ Rd p(w)ejw T (x−y)dw.\nThis means that one can treat p(w) as a density function and use Monte-Carlo sampling to derive the following nonlinear map for a real-valued kernel:\nφ(x) = √ 1/D [ sin(wT1 x), · · · , sin(wTDx), cos(wT1 x), · · · , cos(wTDx) ]T ,\nwhere wi is sampled i.i.d. from a probability distribution with density p(w). Let W =[ w1, · · · ,wD ]T . The linear transformation Wx is central to the above computation since,\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n09 07\n2v 1\n[ cs\n.L G\n] 2\n8 O\nct 2\n• The choice of matrix W determines how well the estimated kernel converges to the actual kernel; • The computation of Wx has space and time costs of O(Dd). This is expensive for high-\ndimensional data, especially since D is often required to be larger than d to achieve low approximation error.\nIn this work, we address both of the above issues. We first show an intriguing discovery (Figure 1): by enforcing orthogonality on the rows of W, the kernel approximation error can be significantly reduced. We call this method Orthogonal Random Features (ORF). Section 3 describes the method and provides theoretical explanation for the improved performance.\nSince both generating a d× d orthogonal matrix (O(d3) time and O(d2) space) and computing the transformation (O(d2) time and space) are prohibitively expensive for high-dimensional data, we further propose Structured Orthogonal Random Features (SORF) in Section 4. The idea is to replace random orthogonal matrices by a class of special structured matrices consisting of products of binary diagonal matrices and Walsh-Hadamard matrices. SORF has fast computation time, O(D log d), and almost no extra memory cost (with efficient in-place implementation). We show extensive experiments in Section 5. We also provide theoretical discussions in Section 6 of applying the structured matrices in a broader range of applications where random Gaussian matrix is used."
    }, {
      "heading" : "2 Related Works",
      "text" : "Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19]. In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].\nKey to the RFF technique is Monte-Carlo sampling. It is well known that the convergence of MonteCarlo can be largely improved by carefully choosing a deterministic sequence instead of random samples [18]. Following this line of reasoning, Yang et al. [26] proposed to use low-displacement rank sequences in RFF. Yu et al. [29] studied optimizing the sequences in a data-dependent fashion to achieve more compact maps. In contrast to the above works, this paper is motivated by an intriguing new discovery that using orthogonal random samples provides much faster convergence. Compared to [26], the proposed SORF method achieves both lower kernel approximation error and greatly reduced computation and memory costs. Furthermore, unlike [29], the results in this paper are data independent.\nStructured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8]. For the kernel approximation works, in particular, the “structured randomness” leads to a minor loss of accuracy, but allows faster computation since the structured matrices enable the use of FFT-like algorithms. Furthermore, these matrices provide substantial model compression since they require subquadratic (usually only linear)\nspace. In comparison with the above works, our proposed methods SORF and ORF are more effective than RFF. In particular SORF demonstrates both lower approximation error and better efficiency than RFF. Table 1 compares the space and time costs of different techniques."
    }, {
      "heading" : "3 Orthogonal Random Features",
      "text" : "Our goal is to approximate a Gaussian kernel of the form\nK(x,y) = e−||x−y|| 2/2σ2 .\nIn the paragraph below, we assume a square linear transformation matrix W ∈ RD×d, D = d. When D < d, we simply use the first D dimensions of the result. When D > d, we use multiple independently generated random features and concatenate the results. We comment on this setting at the end of this section.\nRecall that the linear transformation matrix of RFF can be written as\nWRFF = 1\nσ G, (1)\nwhere G ∈ Rd×d is a random Gaussian matrix, with every entry sampled independently from the standard normal distribution. Denote the approximate kernel based on the above WRFF asKRFF(x,y). For completeness, we first show the expectation and variance of KRFF(x,y). Lemma 1. (Appendix A.2) KRFF(x,y) is an unbiased estimator of the Gaussian kernel, i.e., E(KRFF(x,y)) = e−||x−y||\n2/2σ2 . Let z = ||x − y||/σ. The variance of KRFF(x,y) is Var (KRFF(x,y)) = 12D ( 1− e−z2 )2 .\nThe idea of Orthogonal Random Features (ORF) is to impose orthogonality on the matrix on the linear transformation matrix G. Note that one cannot achieve unbiased kernel estimation by simply replacing G by an orthogonal matrix, since the norms of the rows of G follow the χ-distribution, while rows of an orthogonal matrix have the unit norm. The linear transformation matrix of ORF has the following form\nWORF = 1\nσ SQ, (2)\nwhere Q is a uniformly distributed random orthogonal matrix1. The set of rows of Q forms a bases in Rd. S is a diagonal matrix, with diagonal entries sampled i.i.d. from the χ-distribution with d degrees of freedom. S makes the norms of the rows of SQ and G identically distributed.\nDenote the approximate kernel based on the above WORF as KORF(x,y). The following shows that KORF(x,y) is an unbiased estimator of the kernel, and it has lower variance in comparison to RFF. Theorem 1. KORF(x,y) is an unbiased estimator of the Gaussian kernel, i.e.,\nE(KORF(x,y)) = e−||x−y|| 2/2σ2 .\n1We first generate the random Gaussian matrix G in (1). Q is the orthogonal matrix obtained from the QR decomposition of G. Q is distributed uniformly on the Stiefel manifold (the space of all orthogonal matrices) based on the Bartlett decomposition theorem [17].\nLet D ≤ d, and z = ||x − y||/σ. There exists a function f such that for all z, the variance of KORF(x,y) is bounded by\nVar (KORF(x,y)) ≤ 1\n2D\n(( 1− e−z 2 )2 − D − 1\nd e−z\n2 z4 ) + f(z)\nd2 .\nProof. We first show the proof of the unbiasedness. Let z = x−yσ , and z = ||z||, then E(KORF (x,y)) = E ( 1 D ∑D i=1 cos(w T i z) ) = 1D ∑D i=1 E ( cos(wTi z) ) . Based on the definition of ORF, w1,w2, . . . ,wD are D random vectors given by wi = siui, with u1,u2, . . . ,ud a uniformly chosen random orthonormal basis for Rd, and si’s are independent χ-distributed random variables with d degrees of freedom. It is easy to show that for each i, wi is distributed according to N(0, Id), and hence by Bochner’s theorem,\nE[cos(wT z)] = e−z 2/2.\nWe now show a proof sketch of the variance. Suppose, ai = cos(wTi z).\nVar\n( 1\nD D∑ i=1 ai\n) = E [(∑D i=1 ai\nD\n)2] − E [(∑D i=1 ai\nD )]2 = 1\nD2 ∑ i ( E[a2i ]− E[ai]2 ) + 1 D2 ∑ i ∑ j 6=i (E[aiaj ]− E[ai]E[aj ])\n=\n( 1− e−z 2 )2\n2D + D(D − 1) D2\n( E[a1a2]− e−z 2 ) ,\nwhere the last equality follows from symmetry. The first term in the resulting expression is exactly the variance of RFF. In order to have lower variance, E[a1a2]− e−z 2\nmust be negative. We use the following lemma to quantify this term.\nLemma 2. (Appendix A.3) There is a function f such that for any z,\nE[aiaj ] ≤ e−z 2 − e−z 2 z4 2d + f(z) d2 .\nTherefore, for a large d, and D ≤ d, the ratio of the variance of ORF and RFF is\nVar(KORF(x,y)) Var(KRFF(x,y)) ≈ 1− (D − 1)e −z2z4 d(1− e−z2)2 . (3)\nFigure 2(a) shows the ratio of the variance of ORF to that of RFF when D = d and d is large. First notice that this ratio is always smaller than 1, and hence ORF always provides improvement over\nthe conventional RFF. Interestingly, we gain significantly for small values of z. In fact, when z → 0 and d → ∞, the ratio is roughly z2 (note ex ≈ 1 + x when x → 0), and ORF exhibits infinitely lower error relative to RFF. Figure 2(b) shows empirical simulations of this ratio. We can see that the variance ratio is close to that of d =∞ (3), even when d = 32, a fairly low-dimensional setting in real-world cases.\nRecall that z = ||x− y||/σ. This means that ORF preserves the kernel value especially well for data points that are close, thereby retaining the local structure of the dataset. Furthermore, empirically σ is typically not set too small in order to prevent overfitting—a common rule of thumb is to set σ to be the average distance of 50th-nearest neighbors in a dataset. In Figure 2(c), we plot the distribution of z for several datasets with this choice of σ. These distributions are all concentrated in the regime where ORF yields substantial variance reduction.\nThe above analysis is under the assumption that D ≤ d. Empirically, for RFF, D needs to be larger than d in order to achieve low approximation error. In that case, we independently generate and apply the transformation (2) multiple times. The next lemma bounds the variance for this case.\nCorollary 1. Let D = m · d, for an integer m and z = ||x− y||/σ. There exists a function f such that for all z, the variance of KORF(x,y) is bounded by\nVar (KORF(x,y)) ≤ 1\n2D\n(( 1− e−z 2 )2 − d− 1\nd e−z\n2 z4 ) + f(z)\ndD ."
    }, {
      "heading" : "4 Structured Orthogonal Random Features",
      "text" : "In the previous section, we presented Orthogonal Random Features (ORF) and provided a theoretical explanation for their effectiveness. Since generating orthogonal matrices in high dimensions can be expensive, here we propose a fast version of ORF by imposing structure on the orthogonal matrices. This method can provide drastic memory and time savings with minimal compromise on kernel approximation quality. Note that the previous works on fast kernel approximation using structured matrices do not use structured orthogonal matrices [14, 29, 8]. Let us first introduce a simplified version of ORF: replace S in (2) by a scalar √ d. Let us call this method ORF′. The transformation matrix thus has the following form:\nWORF′ =\n√ d\nσ Q. (4)\nTheorem 2. (Appendix B) Let KORF′(x,y) be the approximate kernel computed with linear transformation matrix (4). Let D ≤ d and z = ||x− y||/σ. There exists a function f such that the bias of KORF′(x,y) satisfies ∣∣∣E(KORF′(x,y))− e−z2/2∣∣∣ ≤ e−z2/2 z4\n4d + f(z) d2 ,\nand the variance satisfies\nVar (KORF′(x,y)) ≤ 1\n2D\n( (1− e−z 2\n)2 − D − 1 d e−z 2\nz4 ) + f(z)\nd2 .\nThe above implies that when d is large KORF′(x,y) is a good estimation of the kernel with low variance. Figure 3(a) shows that even for relatively small d, the estimation is almost unbiased. Figure 3(c) shows that when d ≥ 32, the variance ratio is very close to that of d =∞. We find empirically that ORF′also provides very similar MSE in comparison with ORF in real-world datasets.\nWe now introduce Structured Orthogonal Random Features (SORF). It replaces the random orthogonal matrix Q of ORF′in (4) by a special type of structured matrix HD1HD2HD3:\nWSORF =\n√ d\nσ HD1HD2HD3, (5)\nwhere Di ∈ Rd×d, i = 1, 2, 3 are diagonal “sign-flipping” matrices, with each diagonal entry sampled from the Rademacher distribution. H is the normalized Walsh-Hadamard matrix.\nComputing WSORFx has the time cost O(d log d), since multiplication with D takes O(d) time and multiplication with H takes O(d log d) time using fast Hadamard transformation. The computation of SORF can also be carried out with almost no extra memory due to the fact that both sign flipping and the Walsh-Hadamard transformation can be efficiently implemented as in-place operations [10].\nFigures 3(b)(d) show the bias and variance of SORF. Note that although the curves for small d are different from those of ORF, when d is large (d > 32 in practice), the kernel estimation is almost unbiased, and the variance ratio converges to that of ORF. In other words, it is clear that SORF can provide almost identical kernel approximation quality as that of ORF. This is also confirmed by the experiments in Section 5. In Section 6, we provide theoretical discussions to show that the structure of (5) can also be generally applied to many scenarios where random Gaussian matrices are used."
    }, {
      "heading" : "5 Experiments",
      "text" : "Kernel Approximation. We first show kernel approximation performance on six datasets. The input feature dimension d is set to be power of 2 by padding zeros or subsampling. Figure 4 compares the mean squared error (MSE) of all methods. For fixed D, the kernel approximation MSE exhibits the following ordering:\nSORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29]. By imposing orthogonality on the linear transformation matrix, Orthogonal Random Features (ORF) achieves significantly lower approximation error than Random Fourier Features (RFF). The Structured Orthogonal Random Features (SORF) have almost identical MSE to that of ORF. All other fast kernel approximation methods, such as circulant [29] and FastFood [14] have higher MSE. We also include DigitalNet, the best performing method among Quasi-Monte Carlo techniques [26]. Its MSE is lower than that of RFF, but still higher than that of ORF and SORF. The order of time cost for a fixed D is\nSORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20]. Remarkably, SORF has both better computational efficiency and higher kernel approximation quality compared to other methods.\nWe also apply ORF and SORF on classification tasks. Table 2 shows classification accuracy for different kernel approximation techniques with a (linear) SVM classifier. SORF is competitive with or better than RFF, and has greatly reduced time and space costs.\nThe Role of σ. Note that a very small σ will lead to overfitting, and a very large σ provides no discriminative power for classification. Throughout the experiments, σ for each dataset is chosen to be the mean distance of the 50th `2 nearest neighbor, which empirically yields good classification results [29]. As shown in Section 3, the relative improvement over RFF is positively correlated with σ. Figure 5(a)(b) verify this on the mnist dataset. Notice that the proposed methods (ORF and SORF) consistently improve over RFF.\nSimplifying SORF. The SORF transformation consists of three Hadamard-Diagonal blocks. A natural question is whether using fewer computations and randomness can achieve similar empirical performance. Figure 5(c) shows that reducing the number of blocks to two (HDHD) provides similar performance, while reducing to one block (HD) leads to large error."
    }, {
      "heading" : "6 Analysis and General Applicability of the Hadamard-Diagonal Structure",
      "text" : "We provide theoretical discussions of SORF in this section. We first show that for large d, SORF is an unbiased estimator of the Gaussian kernel.\nTheorem 3. (Appendix C) Let KSORF(x,y) be the approximate kernel computed with linear transformation matrix\n√ dHD1HD2HD3. Let z = ||x− y||/σ. Then∣∣∣E(KSORF(x,y))− e−z2/2∣∣∣ ≤ 6z√\nd .\nEven though SORF is nearly-unbiased, proving tight variance and concentration guarantees similar to ORF remains an open question. The following discussion provides a sketch in that direction. We first show a lemma of RFF. Lemma 3. Let W be a random Gaussian matrix as in RFF, for a given z, the distribution of Wz is N(0, ||z||2Id).\nNote that Wz in RFF can be written as Rg, where R is a scaled orthogonal matrix such that each row has norm ||z||2 and g is distributed according to N(0, Id). Hence the distribution of Rg is N(0, ||z||2Id), identical to Wz. The concentration results of RFF use the fact that the projections of a Gaussian vector g onto orthogonal directions R are independent. We show that √ dHD1HD2HD3z has similar properties. In particular, we show that it can be written as R̃g̃, where rows of R̃ are “near-orthogonal” (with high probability) and have norm ||z||2, and the vector g̃ is close to Gaussian (g̃ has independent sub-Gaussian elements), and hence the projections behave “near-independently”. Specifically, g̃ = vec(D1) (vector of diagonal entries of D1), and R̃ is a function of D2, D3 and z.\nTheorem 4. (Appendix D) For a given z, there exists a R̃ (function of D2,D3, z), such that√ dHD1HD2HD3z = R̃vec(D1). Each row of R̃ has norm ||z||2 and for any t ≥ 1/d, with probability 1− de−c·t2/3d1/3 , the inner product between any two rows of R̃ is at most t||z||2, where c is a constant.\nThe above result can also be applied to settings not limited to kernel approximation. In the appendix, we show empirically that the same scheme can be successfully applied to angle estimation where the nonlinear map f is a non-smooth sign(·) function [4]. We note that the HD1HD2HD3 structure has also been recently used in fast cross-polytope LSH [2, 13, 7]."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have demonstrated that imposing orthogonality on the transformation matrix can greatly reduce the kernel approximation MSE of Random Fourier Features when approximating Gaussian kernels. We further proposed a type of structured orthogonal matrices with substantially lower computation and memory cost. We provided theoretical insights indicating that the Hadamard-Diagonal block structure can be generally used to replace random Gaussian matrices in a broader range of applications. Our method can also be generalized to other types of kernels such as general shift-invariant kernels and polynomial kernels based on Schoenberg’s characterization as in [19]."
    }, {
      "heading" : "Appendix A Variance Reduction via Orthogonal Random Features",
      "text" : "A.1 Notation\nLet z = x−yσ , and z = ||z||. For a vector y, let y(i) denote its i th coordinate. Let n!! be the double factorial of n, i.e., the product of every number from n to 1 that has the same parity as n.\nA.2 Proof of Lemma 1\nLet z = (x− y)/σ. Recall that in RFF, we compute the Kernel approximation as\nD∑ i=1 1 D cos(wTi z),\nwhere each wi is a d dimensional vector distributed N(0, Id). Let w be a d dimensional vector distributed N(0, Id). By Bochner’s theorem,\nE[cos(wT z)] = e−z 2/2,\nand hence RFF yields an unbiased estimate.\nWe now compute the variance of RFF approximation. Observe that\ncos2(wT z) = 1 + cos(2wT z)\n2 =\n1 + cos(wT (2z))\n2 .\nHence by Bochner’s theorem\nE[cos2(wT z)] = 1 + e−2z\n2\n2 .\nTherefore,\nVar(cos(wT z)) = E[cos2(wT z)]− (E[cos(wT z)])2\n= 1 + e−2z\n2\n2 − e−z\n2 = (1− e−z2)2\n2 .\nIf we take D such independent random variables w1,w2, . . .wD, since variance of the sum is sum of variances,\nVar\n( 1\nD D∑ i=1 cos(wTi z)\n) = (1− e−z2)2\n2D .\nA.3 Proof of Lemma 2\nThe proof uses the following lemma.\nLemma 4. For a set of non-negative values α1, α2, . . . αk and β1, β2, . . . βk such that for all i, βi ≤ αi, ∣∣∣∣∣ 1∏k\ni (1 + αi) −\n( 1−\nk∑ i=1 αi )∣∣∣∣∣ ≤ ( k∑ i=1 αi )2 ,\nand ∣∣∣∣∣ k∏ i 1 + βi 1 + αi − ( 1 + k∑ i=1 βi − k∑ i=1 αi )∣∣∣∣∣ ≤ ( k∑ i=1 (αi − βi) )2 + k∑ i=1 (αi − βi)βi.\nProof. Since αis are non-negative,\n1∏k i (1 + αi)\n− ( 1− k∑ i=1 αi ) ≤ 1 1 + ∑k i αi − ( 1− k∑ i=1 αi )\n= 1− (1 +\n∑k i=1 αi)(1− ∑k i=1 αi)\n1 + ∑k i αi\n= ( ∑k i=1 αi) 2\n1 + ∑k i αi ≤ ( k∑ i=1 αi )2 .\nFurthermore, by convexity\n1∏k i (1 + αi) ≥ 1 (1 + ∑k i=1 αi/k) k ≥ e−\n∑k i=1 αi ≥ 1− k∑ i=1 αi.\nCombining the above two equations results in the first part of the lemma. For the second part observe that\nk∏ i 1 + βi 1 + αi = 1∏k\ni ( 1 + αi−βi1+βi ) . Hence, by the first part∣∣∣∣∣ k∏ i 1 + βi 1 + αi − ( 1− k∑ i=1 αi − βi 1 + βi )∣∣∣∣∣ ≤ ( k∑ i=1 αi − βi 1 + βi )2 ≤ ( k∑ i=1 (αi − βi) )2 .\nFurthermore, for every i ∣∣∣∣ 11 + βi − 1 ∣∣∣∣ ≤ βi.\nCombining the above two equations yields the second part of the lemma.\nProof of Lemma 2. Observe that\ncos(wT1 z) cos(w T 2 z) =\ncos(wT1 z + w T 2 z) + cos(w T 1 z−wT2 z)\n2 .\nSince the problem is rotation invariant, instead of projecting a vector z onto a randomly chosen two orthogonal vectors u1 and u2, we can choose a vector y that is uniformly distributed on a sphere of radius z and project it on to the first two dimensions. Thus,\nE[cos(wT1 z + wT2 z)] = E[cos((s1y(1) + s2y(2))z)].\nSimilarly,\nE[cos(wT1 z−wT2 z)] = E[cos((s1y(1)− s2y(2))z)].\nThe kth term in the Taylor’s series expansion of sum of above two terms is\n(−1)k\n(2k)! ((s1y(1) + s2y(2))z)\n2k +\n(−1)k\n(2k)! ((s1y(1)− s2y(2))z)2k\n= (−z2)k\n(2k)! k∑ i=0 ( 2k 2i ) s2i1 y 2i(1)s2k−2i2 y 2k−2i(2).\nA way to compute a uniformly distributed random variable on a sphere with radius z is to generate d independent random variables x = (x(1), x(2), . . . , x(d)) each distributed N(0, 1) and setting\ny(i) = zx(i)/||x||. Hence,\nE [ k∑ i=0 ( 2k 2i ) s2i1 y 2i(1)s2k−2i2 y 2k−2i(2) ] (a) = E\n[ 2k∑ i=0 ( 2k 2i ) s2i1 x 2i(1)s2k−2i2 x 2k−2i(2) ||x||2k ] (b) =\nk∑ i=0 ( 2k 2i ) E[s2i1 ]E[s 2k−2i 2 ]E [ x2i(1)x2k−2i(2) ||x||2k ] (c) =\nk∑ i=0 ( 2k 2i ) E[s2i1 ]E[s 2k−2i 2 ] E[x2i(1)]E[x2k−2i(2)] E[||x||2k]\n(d) = k∑ i=0 ( 2k 2i ) (d+ 2i− 2)!!(d+ 2k − 2i− 2)!! · (2i− 1)!!(2k − 2i− 1)!! (d+ 2k − 2)!!(d− 2)!!\n(e) =\n(2k)!\n2kk! k∑ i=0 ( k i ) (d+ 2i− 2)!!(d+ 2k − 2i− 2)!! (d+ 2k − 2)!!(d− 2)!! .\n(a) follows from linearity of expectation and the observation above. (b) follows from the independence of s1, s2, and x. (d) follows from substituting the moments of chi and Gaussian distributions. (e) follows from numerical simplification. We now describe the reasoning behind (c). Let z = x||y||||x|| , where y and x are independent N(0, Id) random variables. By the properties of the Gaussian random variables z is also a N(0, Id) random variable. Thus,\nE[z2i(1)]E[z2k−2i(2)] = E [ x2i(1)x2k−2i(2)\n||x||2k\n] E[||y||2k].\nRearranging terms, we get E [ x2i(1)x2k−2i(2)\n||x||2k\n] =\nE[z2i(1)]E[z2k−2i(2)] E[||y||2k = E[x2i(1)]E[x2k−2i(2)] E[||x||2k] ,\nand hence (c). Substituting the above equation in the cosine expansion, we get that the expectation is\nE[cos(s1y(1) + s2y(2)]] = ∞∑ k=0 (−z2)k k! k∑ i=0 ( k i ) 1 2k (d+ 2i− 2)!!(d+ 2k − 2i− 2)!! (d+ 2k − 2)!!(d− 2)!! .\nObserve that\n(d+ 2i− 2)!!(d+ 2k − 2i− 2)!! (d+ 2k − 2)!!(d− 2)!! = ∏k−i−1 j=0 (1 + 2j/d)∏k−i−1\nj=0 (1 + 2(j + i)/d) ,\nHence by Lemma 4,∣∣∣∣∣∣ ∏k−i−1 j=0 (1 + 2j/d)∏k−i−1 j=0 (1 + 2(j + i)/d) − 1 + k−i−1∑ j=0 2j d − k−i−1∑ j=0 2(j + i) d ∣∣∣∣∣∣ ≤\nk−i−1∑ j=0 2i d 2 + k−i−1∑ j=0 2i d ( 2j d ) .\nSimplifying we get,∣∣∣∣∣ ∏k−i−1 j=0 (1 + 2j/d)∏k−i−1 j=0 (1 + 2(j + i)/d) − ( 1 + 2i2 − 2ik d )∣∣∣∣∣ ≤ 4i2(k − i)2d2 + 2i(k − i)(k − i− 1)d3 .\nHence summing over i,∣∣∣∣∣ k∑ i=0 ( k i ) 1 2k ∏k−i−1 j=0 (1 + 2j/d)∏k−i−1 j=0 (1 + 2(j + i)/d) − ( 1 + k − k2 2d )∣∣∣∣∣ ≤ k44d2 + k2(k − 1)2d3 . Substituting,\nE[cos((s1y(1) + s2y(2))z]] + E[cos((s1y(1)− s2y(2))z]] 2 = ∞∑ k=0 (−z2)k k! ( 1 + k − k2 2d + ck,d ) ,\nwhere |ck,d| ≤ k 4 4d2 + k2(k−1) 2d3 . Thus,\nE[cos((s1y(1) + s2y(2))z]] + E[cos((s1y(1)− s2y(2))z]] 2\n= ∞∑ k=0 (−z2)k k! ( 1 + k − k2 2d + ck,d )\n≤ ∞∑ k=0 (−z2)k k! ( 1 + k − k2 2d ) + ∞∑ k=0 (z2)k k! ( k4 4d2 + k2(k − 1) 2d3 )\n≤ e−z 2 − e−z 2 z4 2d + ez\n2\n(z8 + 6z6 + 7z4 + z2) 4d2 + ez\n2\nz4(z6 + 2z4)\n2d3 ."
    }, {
      "heading" : "Appendix B Proof of Theorem 2",
      "text" : "The proof of the theorem is similar to that of Lemma 2 and we outline some key steps. We first bound the bias in Lemma 5 and then the variance in Lemma 6. Lemma 5. If w =\n√ dy, where y is distributed uniformly on a unit sphere, then∣∣∣∣E[coswT z]− (e−z2/2 − e−z2/2 z44d )∣∣∣∣ ≤ ez2/2z4(z4 + 8z2 + 8)16d2 . Proof. Without loss of generality, we can assume z is along the first coordinate and hence wT z =√ dzy(1). A way to compute a uniformly distributed random variable on a sphere with radius z is to generate d independent random variables x = (x(1), x(2), . . . , x(d)) each distributed N(0, 1) and setting y(i) = zx(i)/||x||. hence,\nE[coswT z] = E [ cos ( z √ dx(1)\n||x||\n)] .\nThe kth term in the Taylor’s series expansion of cosine in the above equation is\n(−1)k\n(2k)!\n(√ dx(1)z\n||x|| )2k Similar to the proof of Lemma 2, it can be shown that the expectation of this term is\nE  (−1)k (2k)! ( z √ dx(1) ||x|| )2k = (−z2)k 2kk!\ndk\n(d, 2k − 2)!! .\nApplying Lemma 4 and simplifying,\nE[cos(dy(1))] = ∞∑ k=0 (−z2)k 2kk! ( 1− k(k − 1) d + c′k,d ) ,\nwhere |c′k,d| ≤ ( k(k−1) d )2 . Hence,∣∣∣∣∣E[cos(dy(1))]− ∞∑ k=0 (−z2)k 2kk! ( 1− k(k − 1) d )∣∣∣∣∣ ≤ ∞∑ k=0 (z2)k 2kk! ( k(k − 1) d )2 , and thus ∣∣∣∣E[cos(dy(1))]− e−z2/2 + e−z2/2 z44d ∣∣∣∣ ≤ ez2/2z4(z4 + 8z2 + 8)16d2 .\nLemma 6. Let D ≤ d. If W = √ dQ, where Q is a uniformly chosen random rotation, then\nVar\n( 1\nD D∑ i=1 cos(wTi z)\n) ≤ 1\n2D\n( (1− e−z 2\n)2 − D − 1 d e−z 2\nz4 )\n+ O(e3z2) d2 .\nProof. Let ai = cos(wTi z). Expanding the variance we have,\nVar\n( 1\nD D∑ i=1 ai\n) = 1\nD2 ∑ i ( E[a2i ]− (E[ai])2 ) + 1 D2 ∑ i ∑ j 6=i (E[aiaj ]− E[ai]E[aj ])\n= 1\nD\n( E[a21]− (E[a1]2) ) + D − 1 D (E[a1a2]− E[a1]E[a2]) .\nFor the first term, rewriting cos2(wT z) = 1+cos(2w T z)\n2 , similar to the proof of Lemma 5 it can be shown that\n(E[a21]− (E[a1]2) ≤ (1− e−z2)2 2 + O(e3z2) d .\nSecond term can be bounded similar to Lemma 2 and here we just sketch an outline. Similar to the proof of Lemma 2, the variance boils down to computing the expectation of cos(wT1 z + w T 2 z). Using Lemma 4 and summing Taylor’s series we get∣∣∣∣E[cos(wT1 z + wT2 z)]− e−z2 + e−z2 z4d ∣∣∣∣ ≤ ez2z4(z4 + 4z2 + 2)d2 .\nSubstituting the above bound and the expectation from Lemma 5, we get\nE[a1a2]− E[a1]E[a2] ≤ −ez 2 z4 2d + O(e3z2) d2 ,\nand hence the lemma."
    }, {
      "heading" : "Appendix C Proof of Theorem 3",
      "text" : "The proof follows from the following two technical lemmas. Lemma 7. Let z′ be distributed according to N(0, ||x||22) and y′ = ∑d i=1 x(i)di, where dis are independent Rademacher random variables. For any function g such that |g′| ≤ 1 and |g| ≤ 1,\n|E[g(z′)]− E[g(y′)]| ≤ 3 2 d∑ i=1 x3(i) ||x||22 .\nProof. Let z = z′/||x||2, y = y′/||x||2, and h(x) = g(||x||2x), for all x. Hence h(z) = g(z′) and h(y) = g(y′). By a lemma due to Stein [5],\n|E[g(z′)]− E[g(y′)]| = |E[h(z)]− E[h(y)]| ≤ supf{|E[f ′(y)− yf(y)]| : |f |∞ ≤ ||x||2, |f ′|∞ ≤ √ 2/π||x||2, |f ′′|∞ ≤ 2||x||2}.\nWe now bound the term on the right hand side by classic Stein-type arguments.\nE[yf(y)] = d∑ i=1 x(i)di ||x||2 E[f(y)].\nLet yi = y − x(i)di||x||2 . Observe that\nE[dif(y)] = E[di(f(y)− f(yi))] = E[di(f(y)− f(yi))− di(y − yi)f ′(yi)] + E[di(y − yi)f ′(yi)],\nwhere the first equality follows from the fact that yi and di are independent and di has zero mean. By Taylor series approximation, the first term is bounded by\n|E[dif(y)− f(yi)− di(y − yi)f ′(yi)]| ≤ 1\n2 (y − yi)2|f ′′|∞ =\n1\n2\nx2(i) ||x||22 |f ′′|∞.\nSimilarly,\nE[di(y − yi)f ′(yi)] = x(i)\n||x||2 f ′(yi).\nCombining the above four equations, we get∣∣∣∣∣E [ yf(y)− d∑ i=1 x2(i) ||x||22 f ′(yi) ]∣∣∣∣∣ ≤ d∑ i=1 |x3(i)| ||x||32 |f ′′|∞. Similarly, note that∣∣∣∣∣E [ f ′(y)− d∑ i=1 x2(i) ||x||22 f ′(yi) ]∣∣∣∣∣ ≤ d∑ i=1 |f ′′|∞ x2(i) ||x||22 E[|y − yi|] = d∑ i=1 |f ′′|∞ |x3(i)| ||x||32 .\nCombining the above two equations, we get\n||E[yf(y)− f ′(y))]| ≤ 3|f ′′|∞ 2 d∑ i=1 |x3(i)| ||x||32 .\nSubstituting the bound on the second moment of f yields the result.\nLet G be a random matrix with i.i.d. N(0, 1) entries as before. Using the above lemma we show that√ dHD1HD2 behaves like G while computing the bias. Lemma 8. For a given x, let z = Gx and y = √ dHD1HD2x. For any function g such that |g′| ≤ 1 and |g| ≤ 1, ∣∣∣∣∣1d d∑ i=1 E [g(z(i))]− 1 d d∑ i=1 E [g(y(i))]\n∣∣∣∣∣ ≤ 6‖x‖2√d . Proof. By triangle inequality,∣∣∣∣∣1d d∑ i=1 E [g(z(i))]− 1 d d∑ i=1 E [g(y(i))] ∣∣∣∣∣ ≤ 1d d∑ i=1 |E[g(z(i))]− E[g(y(i))]|.\nLet u = HD2x. Then for every i, y(i) = ∑ j H(i, j)D2(j)u(j). Hence by Lemma 7, we can relate expectation under y to the expectation under Gaussian distribution:\n|E[g(z(i))]− E[g(y(i))]| (a)= |E[E[g(z(i))]− E[g(y(i))|u]]|\n≤ 3 2 d∑ i=1 E [ |u3(i)| ||u||22 ]\n= 3\n2 d∑ i=1 E [ |u3(i)| ||x||22 ] ,\nwhere the last equality follows from the fact that HD2 does not change rotation and (a) follows from the law of total expectation. By Cauchy-Schwartz inequality, for each i\nE[|u(i)|3] ≤ √ E[u6(i)],\nIt can be shown that\nE[u6(i)] ≤ 15||x|| 6 2\nd3 ,\nSumming over all the indices yields the lemma.\nTheorem 3 follows from the Bochner’s theorem and the fact that cos(·) satisfies requirements for the above lemma. We note that Theorem 3 holds for the matrix √ dHD1HD2 itself and the third component HD3 is not necessary to bound the bias."
    }, {
      "heading" : "Appendix D Proof of Theorem 4",
      "text" : "To prove Theorem 4, we use the Hanson-Wright Inequality.\nLemma 9 (Hanson-Wright Inequality). Let X = (X1, ..., Xn) ∈ Rn be a random vector with independent subgaussian components Xi which satisfy: E[Xi] = 0 and ‖Xi‖sg ≤ K for some constant K > 0. Let A ∈ Rn×n. Then for any t > 0 the following holds:\nP[|XTAX− E[XTAX]| > t] ≤ 2e −cmin( t2 K4‖A‖2 F , t K2‖A‖2 ) ,\nfor some universal positive constant c > 0.\nProof of Theorem 4. For a vector u, let diag(u) denote the diagonal matrix whose entries correspond to the entries of u. For a diagonal matrix D, let vec(D) denote the vector corresponding to the diagonal entries of D. Let v = HD3z and u = HD2v = Hdiag(v)vec(D2). Observe that\n√ dHD1HD2HD3z = √ dHdiag(HD2HD3z)vec(D1).\nHence R̃ = √ dHdiag(HD2HD3z). Note that all the entries of √ dH have magnitude 1 and HD2HD3 do not change norm of the vector. Hence, each row of R̃ has norm ||z||2. To prove the orthogonality of rows of R̃, we need to show that for any i and j 6= i,\n√ d d∑ k=1 H(i, k)H(j, k)u2(k)\nis small. We first show that the expectation of the above quantity is 0 and then use the Hanson-Wright inequality to prove concentration. Let A be a diagonal matrix with kth entry being √ dH(i, k)H(j, k). The above equation can be rewriten as\nd∑ k=1 H(i, k)H(j, k)u2(k) = vec(D2)T diag(v)HTAHdiag(v)vec(D2).\nObserve that the (l, l) entry of the HTAH is\nd∑ k=1 HT (l, k)A(k, k)H(k, l) = d∑ k=1 H(k, l)A(k, k)H(k, l)\n= 1\nd d∑ k=1 A(k, k)\n= d∑ k=1 H(i, k)H(j, k) = 0,\nwhere the last equality follows from observing that the rows of H are orthogonal to each other. Together with the fact that elements of D2 are independent of each other, we get\nE[uTAu] = E[vec(D2)T diag(v)HTAHdiag(v)vec(D2)] = 0,\nTo prove the concentration result, observe that the entries of vec(D2) are independent and subGaussian, and hence we can use the Hanson-Wright inequality. To this end, we bound the Frobenius and the spectral norm of the underlying matrix. For the Frobenius norm, observe that\n||diag(v)HTAHdiag(v)||F (a)\n≤ (||v||∞)4 ||HTAH||F (b) = (||v||∞)4 ||A||F (c) = d (||v||∞)4 ,\nwhere (a) follows by observing that each diag(v) changes the Frobenius norm by at most ||v||2∞, (b) follows from the fact that H does not change the Frobenius norm, and (c) follows by substituting A.\nTo bound the spectral norm, observe that\n||diag(v)HTAHdiag(v)||2 (a)\n≤ (||v||∞)2 ||HTAH||2 (b) = (||v||∞)2 ||A||2 (c) = (||v||∞)2 ,\nwhere (a) follows by observing that each diag(v) changes the spectral norm by at most ||v||∞, (b) follows from the fact that rotation does not change the spectral norm, and (c) follows by substituting A. Since v = HD3z, by McDiarmid’s inequality, it can be shown that with probability≥ 1−2de−d\n2/2, ||v||∞ ≤ ||z||2. Hence, by the Hanson-Wright inequality, we get\nPr ( √ d\nd∑ k=1 H(i, k)H(j, k)u2(k) > t||z||2\n) ≤ 2de−d 2/2 + 2e−cmin(t 2/(d 4),t/ 2),\nwhere c is a constant. Choosing = (t/d)1/3 results in the theorem."
    }, {
      "heading" : "Appendix E Discrete Hadamard-Diagonal Structure in Binary Embedding",
      "text" : "Motivated by the recent advances in using structured matrices in binary embedding, we show empirically that the same type of structured discrete orthogonal matrices (three blocks of HadamardDiagonal matrices) can also be applied to approximate angular distances for high-dimensional data. Let W ∈ RD×d be a random matrix with i.i.d. normally distributed entries. The classic Locality\nSensitive Hashing (LSH) result shows that the sign nonlinear map φ : φ(x) = 1√ D sign(Wx) can be used to approximate the angle, i.e., for any x,y ∈ Rd\nφ(x)Tφ(y) ≈ θ(x,y)/π.\nWe compare random projection based Locality Sensitive Hashing (LSH) [4], Circulant Binary Embedding (CBE) [28] and Kronecker Binary Embedding (KBE) [30]. We closely follow the experimental settings of [30]. We choose to compare with [30] because it proposed to use another type of structured random orthogonal matrix (Kronecker product of orthogonal matrices). As shown in Figure 6, our result (HDHDHD) provides higher recall and lower angular MSE in comparison with other methods."
    } ],
    "references" : [ {
      "title" : "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform",
      "author" : [ "N. Ailon", "B. Chazelle" ],
      "venue" : "STOC,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Practical and optimal lsh for angular distance",
      "author" : [ "A. Andoni", "P. Indyk", "T. Laarhoven", "I. Razenshteyn", "L. Schmidt" ],
      "venue" : "NIPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Harmonic analysis and the theory of probability",
      "author" : [ "S. Bochner" ],
      "venue" : "Dover Publications,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1955
    }, {
      "title" : "Similarity estimation techniques from rounding algorithms",
      "author" : [ "M.S. Charikar" ],
      "venue" : "STOC,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Lecture notes on Stein’s method and applications",
      "author" : [ "S. Chatterjee" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "An exploration of parameter redundancy in deep networks with circulant projections",
      "author" : [ "Y. Cheng", "F.X. Yu", "R.S. Feris", "S. Kumar", "A. Choudhary", "S.-F. Chang" ],
      "venue" : "ICCV,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Triplespin-a generic compact paradigm for fast machine learning computations",
      "author" : [ "K. Choromanski", "F. Fagan", "C. Gouy-Pailler", "A. Morvan", "T. Sarlos", "J. Atif" ],
      "venue" : "arXiv,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Recycling randomness with structure for sublinear time kernel expansions",
      "author" : [ "K. Choromanski", "V. Sindhwani" ],
      "venue" : "ICML,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Support-vector networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Machine Learning, 20(3):273–297,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Unified matrix treatment of the fast walsh-hadamard transform",
      "author" : [ "B.J. Fino", "V.R. Algazi" ],
      "venue" : "IEEE Transactions on Computers, (11):1142–1146,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Training linear SVMs in linear time",
      "author" : [ "T. Joachims" ],
      "venue" : "KDD,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Random feature maps for dot product kernels",
      "author" : [ "P. Kar", "H. Karnick" ],
      "venue" : "AISTATS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Fast cross-polytope locality-sensitive hashing",
      "author" : [ "C. Kennedy", "R. Ward" ],
      "venue" : "arXiv,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Fastfood – approximating kernel expansions in loglinear time",
      "author" : [ "Q. Le", "T. Sarlós", "A. Smola" ],
      "venue" : "ICML,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Random fourier approximations for skewed multiplicative histogram kernels",
      "author" : [ "F. Li", "C. Ionescu", "C. Sminchisescu" ],
      "venue" : "Pattern Recognition, pages 262–271,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Max-margin additive classifiers for detection",
      "author" : [ "S. Maji", "A.C. Berg" ],
      "venue" : "ICCV,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Aspects of multivariate statistical theory, volume 197",
      "author" : [ "R.J. Muirhead" ],
      "venue" : "John Wiley & Sons,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Quasi-Monte Carlo Methods",
      "author" : [ "H. Niederreiter" ],
      "venue" : "Wiley Online Library,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Spherical random features for polynomial kernels",
      "author" : [ "J. Pennington", "F.X. Yu", "S. Kumar" ],
      "venue" : "NIPS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "NIPS,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Generalization properties of learning with random features",
      "author" : [ "A. Rudi", "R. Camoriano", "L. Rosasco" ],
      "venue" : "arXiv:1602.04474,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for SVM, volume = 127, year",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Generalized RBF feature maps for efficient detection",
      "author" : [ "V. Sreekanth", "A. Vedaldi", "A. Zisserman", "C. Jawahar" ],
      "venue" : "BMVC,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Optimal rates for random fourier features",
      "author" : [ "B. Sriperumbudur", "Z. Szabó" ],
      "venue" : "NIPS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Efficient additive kernels via explicit feature maps",
      "author" : [ "A. Vedaldi", "A. Zisserman" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(3):480–492,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Quasi-monte carlo feature maps for shift-invariant kernels",
      "author" : [ "J. Yang", "V. Sindhwani", "H. Avron", "M. Mahoney" ],
      "venue" : "ICML,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Nyström method vs random fourier features: A theoretical and empirical comparison",
      "author" : [ "T. Yang", "Y.-F. Li", "M. Mahdavi", "R. Jin", "Z.-H. Zhou" ],
      "venue" : "NIPS,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Circulant binary embedding",
      "author" : [ "F.X. Yu", "S. Kumar", "Y. Gong", "S.-F. Chang" ],
      "venue" : "ICML,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Compact nonlinear maps and circulant extensions",
      "author" : [ "F.X. Yu", "S. Kumar", "H. Rowley", "S.-F. Chang" ],
      "venue" : "arXiv:1503.03893,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast orthogonal projection based on kronecker product",
      "author" : [ "X. Zhang", "F.X. Yu", "R. Guo", "S. Kumar", "S. Wang", "S.-F. Chang" ],
      "venue" : "ICCV,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "1 Introduction Kernel methods are widely used in nonlinear learning [9], but they are computationally expensive for large datasets.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : "Kernel approximation is a powerful technique to make kernel methods scalable, by mapping input features into a new space where dot products approximate the kernel well [20].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 10,
      "context" : "With accurate kernel approximation, efficient linear classifiers can be trained in the transformed space while retaining the expressive power of nonlinear methods [11, 22].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 21,
      "context" : "With accurate kernel approximation, efficient linear classifiers can be trained in the transformed space while retaining the expressive power of nonlinear methods [11, 22].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 19,
      "context" : "Random Fourier Features [20] are used widely in approximating smooth, shift-invariant kernels.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "The second property guarantees that the Fourier transform of K(∆) is a nonnegative function [3].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 22,
      "context" : "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 14,
      "context" : "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 24,
      "context" : "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 11,
      "context" : "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].",
      "startOffset" : 261,
      "endOffset" : 269
    }, {
      "referenceID" : 18,
      "context" : "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].",
      "startOffset" : 261,
      "endOffset" : 269
    }, {
      "referenceID" : 19,
      "context" : "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 26,
      "context" : "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].",
      "startOffset" : 196,
      "endOffset" : 208
    }, {
      "referenceID" : 20,
      "context" : "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].",
      "startOffset" : 196,
      "endOffset" : 208
    }, {
      "referenceID" : 23,
      "context" : "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].",
      "startOffset" : 196,
      "endOffset" : 208
    }, {
      "referenceID" : 17,
      "context" : "It is well known that the convergence of MonteCarlo can be largely improved by carefully choosing a deterministic sequence instead of random samples [18].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 25,
      "context" : "[26] proposed to use low-displacement rank sequences in RFF.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[29] studied optimizing the sequences in a data-dependent fashion to achieve more compact maps.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "Compared to [26], the proposed SORF method achieves both lower kernel approximation error and greatly reduced computation and memory costs.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 28,
      "context" : "Furthermore, unlike [29], the results in this paper are data independent.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 27,
      "context" : "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].",
      "startOffset" : 154,
      "endOffset" : 165
    }, {
      "referenceID" : 28,
      "context" : "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].",
      "startOffset" : 154,
      "endOffset" : 165
    }, {
      "referenceID" : 7,
      "context" : "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].",
      "startOffset" : 154,
      "endOffset" : 165
    }, {
      "referenceID" : 19,
      "context" : "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 28,
      "context" : "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 25,
      "context" : "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 28,
      "context" : "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].",
      "startOffset" : 236,
      "endOffset" : 244
    }, {
      "referenceID" : 13,
      "context" : "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].",
      "startOffset" : 236,
      "endOffset" : 244
    }, {
      "referenceID" : 19,
      "context" : "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].",
      "startOffset" : 471,
      "endOffset" : 475
    }, {
      "referenceID" : 16,
      "context" : "Q is distributed uniformly on the Stiefel manifold (the space of all orthogonal matrices) based on the Bartlett decomposition theorem [17].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "Note that the previous works on fast kernel approximation using structured matrices do not use structured orthogonal matrices [14, 29, 8].",
      "startOffset" : 126,
      "endOffset" : 137
    }, {
      "referenceID" : 28,
      "context" : "Note that the previous works on fast kernel approximation using structured matrices do not use structured orthogonal matrices [14, 29, 8].",
      "startOffset" : 126,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "Note that the previous works on fast kernel approximation using structured matrices do not use structured orthogonal matrices [14, 29, 8].",
      "startOffset" : 126,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "The computation of SORF can also be carried out with almost no extra memory due to the fact that both sign flipping and the Walsh-Hadamard transformation can be efficiently implemented as in-place operations [10].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 25,
      "context" : "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 19,
      "context" : "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].",
      "startOffset" : 143,
      "endOffset" : 151
    }, {
      "referenceID" : 28,
      "context" : "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].",
      "startOffset" : 143,
      "endOffset" : 151
    }, {
      "referenceID" : 28,
      "context" : "All other fast kernel approximation methods, such as circulant [29] and FastFood [14] have higher MSE.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "All other fast kernel approximation methods, such as circulant [29] and FastFood [14] have higher MSE.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 25,
      "context" : "We also include DigitalNet, the best performing method among Quasi-Monte Carlo techniques [26].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 28,
      "context" : "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 25,
      "context" : "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 28,
      "context" : "Throughout the experiments, σ for each dataset is chosen to be the mean distance of the 50th `2 nearest neighbor, which empirically yields good classification results [29].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 3,
      "context" : "In the appendix, we show empirically that the same scheme can be successfully applied to angle estimation where the nonlinear map f is a non-smooth sign(·) function [4].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 1,
      "context" : "We note that the HD1HD2HD3 structure has also been recently used in fast cross-polytope LSH [2, 13, 7].",
      "startOffset" : 92,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "We note that the HD1HD2HD3 structure has also been recently used in fast cross-polytope LSH [2, 13, 7].",
      "startOffset" : 92,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "We note that the HD1HD2HD3 structure has also been recently used in fast cross-polytope LSH [2, 13, 7].",
      "startOffset" : 92,
      "endOffset" : 102
    }, {
      "referenceID" : 18,
      "context" : "Our method can also be generalized to other types of kernels such as general shift-invariant kernels and polynomial kernels based on Schoenberg’s characterization as in [19].",
      "startOffset" : 169,
      "endOffset" : 173
    } ],
    "year" : 2016,
    "abstractText" : "We present an intriguing discovery related to Random Fourier Features: in Gaussian kernel approximation, replacing the random Gaussian matrix by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost fromO(d) toO(d log d), where d is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORF. Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of applications.",
    "creator" : "LaTeX with hyperref package"
  }
}