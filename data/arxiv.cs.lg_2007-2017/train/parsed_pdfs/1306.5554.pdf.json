{
  "name" : "1306.5554.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Correlated random features for fast semi-supervised learning",
    "authors" : [ "Brian McWilliams" ],
    "emails" : [ "brian.mcwilliams@inf.ethz.ch", "david.balduzzi@inf.ethz.ch", "jbuhmann@inf.ethz.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 6.\n55 54\nv2 [\nst at\n.M L"
    }, {
      "heading" : "1 Introduction",
      "text" : "As the volume of data collected in the social and natural sciences increases, the computational cost of learning from large datasets has become an important consideration. For learning non-linear relationships, kernel methods achieve excellent performance but naı̈vely require operations cubic in the number of training points.\nRandomization has recently been considered as an alternative to optimization that, surprisingly, can yield comparable generalization performance at a fraction of the computational cost [1, 2]. Random features have been introduced to approximate kernel machines when the number of training examples is very large, rendering exact kernel computation intractable. Among several different approaches, the Nyström method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3–5].\nA second problem arising with large datasets concerns obtaining labels, which often requires a domain expert to manually assign a label to each instance which can be very expensive – requiring significant investments of both time and money – as the size of the dataset increases. Semi-supervised learning aims to improve prediction by extracting useful structure from the unlabeled data points and using this in conjunction with a function learned on a small number of labeled points.\nContribution. This paper proposes a new semi-supervised algorithm for regression and classification, Correlated Nyström Views (XNV), that addresses both problems simultaneously. The method\nconsists in essentially two steps. First, we construct two “views” using random features. We investigate two ways of doing so: one based on the Nyström method and another based on random Fourier features (so-called kitchen sinks) [2, 6]. It turns out that the Nyström method almost always outperforms Fourier features by a quite large margin, so we only report these results in the main text.\nThe second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views. Intuitively, if both views contain accurate estimators, then penalizing uncorrelated features reduces variance without increasing the bias by much. Recent theoretical work by Bach [5] shows that Nyström views can be expected to contain accurate estimators.\nWe perform an extensive evaluation of XNV on 18 real-world datasets, comparing against a modified version of the SSSL (simple semi-supervised learning) algorithm introduced in [10]. We find that XNV outperforms SSSL by around 10-15% on average, depending on the number of labeled points available, see §3. We also find that the performance of XNV exhibits dramatically less variability than SSSL, with a typical reduction of 30%.\nWe chose SSSL since it was shown in [10] to outperform a state of the art algorithm, Laplacian Regularized Least Squares [11]. However, since SSSL does not scale up to large sets of unlabeled data, we modify SSSL by introducing a Nyström approximation to improve runtime performance. This reduces runtime by a factor of ×1000 on N = 10, 000 points, with further improvements as N increases. Our approximate version of SSSL outperforms kernel ridge regression (KRR) by > 50% on the 18 datasets on average, in line with the results reported in [10], suggesting that we lose little by replacing the exact SSSL with our approximate implementation.\nRelated work. Multiple view learning was first introduced in the co-training method of [12] and has also recently been extended to unsupervised settings [13,14]. Our algorithm builds on an elegant proposal for multi-view regression introduced in [7]. Surprisingly, despite guaranteeing improved prediction performance under a relatively weak assumption on the views, CCA regression has not been widely used since its proposal – to the best of our knowledge this is first empirical evaluation of multi-view regression’s performance. A possible reason for this is the difficulty in obtaining naturally occurring data equipped with multiple views that can be shown to satisfy the multi-view assumption. We overcome this problem by constructing random views that satisfy the assumption by design."
    }, {
      "heading" : "2 Method",
      "text" : "This section introduces XNV, our semi-supervised learning method. The method builds on two main ideas. First, given two equally useful but sufficiently different views on a dataset, penalizing regression using the canonical norm (computed via CCA), can substantially improve performance [7]. The second is the Nyström method for constructing random features [1], which we use to construct the views."
    }, {
      "heading" : "2.1 Multi-view regression",
      "text" : "Suppose we have data T = ( (x1, y1), . . . , (xn, yn) ) for xi ∈ RD and yi ∈ R, sampled according to joint distribution P (x, y). Further suppose we have two views on the data\nz(ν) : RD −→ H(ν) = RM : x 7→ z(ν)(x) =: z(ν) for ν ∈ {1, 2}.\nWe make the following assumption about linear regressors which can be learned on these views. Assumption 1 (Multi-view assumption [7]). Define mean-squared error loss function ℓ(g,x, y) = (g(x) − y)2 and let loss(g) := EP ℓ(g(x), y). Further let L(Z) denote the space of linear maps from a linear space Z to the reals, and define:\nf (ν) := argmin g∈L(H(ν)) loss(g) for ν ∈ {1, 2} and f := argmin g∈L(H(1)⊕H(2)) loss(g).\nThe multi-view assumption is that\nloss ( f (ν) ) − loss(f) ≤ ǫ for ν ∈ {1, 2}. (1)\nIn short, the best predictor in each view is within ǫ of the best overall predictor.\nCanonical correlation analysis. Canonical correlation analysis [8, 9] extends principal component analysis (PCA) from one to two sets of variables. CCA finds bases for the two sets of variables such that the correlation between projections onto the bases are maximized. The first pair of canonical basis vectors, ( b (1) 1 ,b (2) 1 ) is found by solving:\nargmax b(1),b(2)∈RM\ncorr ( b(1)⊤z(1),b(2)⊤z(2) ) . (2)\nSubsequent pairs are found by maximizing correlations subject to being orthogonal to previously found pairs. The result of performing CCA is two sets of bases, B(ν) = [ b (ν) 1 , . . . ,b (ν) M ] for\nν ∈ {1, 2}, such that the projection of z(ν) onto B(ν) which we denote z̄(ν) satisfies\n1. Orthogonality: ET [ z̄ (ν)⊤ j z̄ (ν) k ] = δjk , where δjk is the Kronecker delta, and 2. Correlation: ET [ z̄ (1)⊤ j z̄ (2) k ] = λj · δjk where w.l.o.g. we assume 1 ≥ λ1 ≥ λ2 ≥ · · · ≥ 0.\nλj is referred to as the jth canonical correlation coefficient.\nDefinition 1 (canonical norm). Given vector z̄(ν) in the canonical basis, define its canonical norm as\n‖z̄(ν)‖CCA :=\n√√√√ D∑\nj=1\n1− λj λj\n( z̄ (ν) j )2 .\nCanonical ridge regression. Assume we observe n pairs of views coupled with real valued labels{ z (1) i , z (2) i , yi }n i=1 , canonical ridge regression finds coefficients β̂ (ν) = [ β̂ (ν) 1 , . . . , β̂ (ν) M ]⊤ such that\nβ̂ (ν)\n:= argmin β\n1\nn\nn∑\ni=1\n( yi − β (ν) ⊤z̄ (ν) i )2 + ‖β(ν)‖2CCA. (3)\nThe resulting estimator, referred to as the canonical shrinkage estimator, is\nβ̂ (ν) j =\nλj\nn\nn∑\ni=1\nz̄ (ν) i,j yi. (4)\nPenalizing with the canonical norm biases the optimization towards features that are highly correlated across the views. Good regressors exist in both views by Assumption 1. Thus, intuitively, penalizing uncorrelated features significantly reduces variance, without increasing the bias by much. More formally:\nTheorem 1 (canonical ridge regression, [7]). Assume E[y2|x] ≤ 1 and that Assumption 1 holds. Let f (ν)\nβ̂ denote the estimator constructed with the canonical shrinkage estimator, Eq. (4), on training\nset T , and let f denote the best linear predictor across both views. For ν ∈ {1, 2} we have\nET [loss(f (ν)\nβ̂ )]− loss(f) ≤ 5ǫ+\n∑M j=1 λ 2 j\nn\nwhere the expectation is with respect to training sets T sampled from P (x, y).\nThe first term, 5ǫ, bounds the bias of the canonical estimator, whereas the second, 1n ∑\nλ2j bounds the variance. The ∑ λ2j can be thought of as a measure of the “intrinsic dimensionality” of the unlabeled data, which controls the rate of convergence. If the canonical correlation coefficients decay sufficiently rapidly, then the increase in bias is more than made up for by the decrease in variance."
    }, {
      "heading" : "2.2 Constructing random views",
      "text" : "We construct two views satisfying Assumption 1 in expectation, see Theorem 3 below. To ensure our method scales to large sets of unlabeled data, we use random features generated using the Nyström method [1].\nSuppose we have data {xi}Ni=1. When N is very large, constructing and manipulating the N × N Gram matrix [K]ii′ = 〈φ(xi), φ(xi′ )〉 = κ(xi,xi′) is computationally expensive. Where here, φ(x) defines a mapping from RD to a high dimensional feature space and κ(·, ·) is a positive semi-definite kernel function.\nThe idea behind random features is to instead define a lower-dimensional mapping, z(xi) : RD → R\nM through a random sampling scheme such that [K]ii′ ≈ z(xi) ⊤z(xi′ ) [6, 15]. Thus, using random features, non-linear functions in x can be learned as linear functions in z(x) leading to significant computational speed-ups. Here we give a brief overview of the Nyström method, which uses random subsampling to approximate the Gram matrix.\nThe Nyström method. Fix an M ≪ N and randomly (uniformly) sample a subset M = {x̂i}Mi=1 of M points from the data {xi}Ni=1. Let K̂ denote the Gram matrix [K̂]ii′ where i, i\n′ ∈ M. The Nyström method [1, 3] constructs a low-rank approximation to the Gram matrix as\nK ≈ K̃ := N∑\ni=1\nN∑\ni′=1\n[κ(xi, x̂1), . . . , κ(xi, x̂M )] K̂ † [κ(xi′ , x̂1), . . . , κ(xi′ , x̂M )] ⊤ , (5)\nwhere K̂† ∈ RM×M is the pseudo-inverse of K̂. Vectors of random features can be constructed as\nz(xi) = D̂ −1/2V̂⊤ [κ(xi, x̂1), . . . , κ(xi, x̂M )] ⊤ ,\nwhere the columns of V̂ are the eigenvectors of K̂ with D̂ the diagonal matrix whose entries are the corresponding eigenvalues. Constructing features in this way reduces the time complexity of learning a non-linear prediction function from O(N3) to O(N) [15].\nAn alternative perspective on the Nyström approximation, that will be useful below, is as follows. Consider integral operators\nLN [f ](·) := 1\nN\nN∑\ni=1\nκ(xi, ·)f(xi) and LM [f ](·) := 1\nM\nM∑\ni=1\nκ(xi, ·)f(xi), (6)\nand introduce Hilbert space Ĥ = span {ϕ̂1, . . . , ϕ̂r} where r is the rank of K̂ and the ϕ̂i are the first r eigenfunctions of LM . Then the following proposition shows that using the Nyström approximation is equivalent to performing linear regression in the feature space (“view”) z : X → Ĥ spanned by the eigenfunctions of linear operator LM in Eq. (6):\nProposition 2 (random Nyström view, [3]). Solving\nmin w∈Rr\n1\nN\nN∑\ni=1\nℓ(w⊤z(xi), yi) + γ\n2 ‖w‖22 (7)\nis equivalent to solving\nmin f∈Ĥ\n1\nN\nN∑\ni=1\nℓ(f(xi), yi) + γ\n2 ‖f‖2Hκ. (8)\n2.3 The proposed algorithm: Correlated Nyström Views (XNV)\nAlgorithm 1 details our approach to semi-supervised learning based on generating two views consisting of Nyström random features and penalizing features which are weakly correlated across views. The setting is that we have labeled data {xi, yi}ni=1 and a large amount of unlabeled data {xi} N i=n+1.\nStep 1 generates a set of random features. The next two steps implement multi-view regression using the randomly generated views z(1)(x) and z(2)(x). Eq. (9) yields a solution for which unimportant\nAlgorithm 1 Correlated Nyström Views (XNV).\nInput: Labeled data: {xi, yi}ni=1 and unlabeled data: {xi} N i=n+1\n1: Generate features. Sample x̂1, . . . , x̂2M uniformly from the dataset, compute the eigendecompositions of the sub-sampled kernel matrices K̂(1) and K̂(2) which are constructed from the samples 1, . . . ,M and M + 1, . . . , 2M respectively, and featurize the input:\nz(ν)(xi) ← D̂ (ν),−1/2V̂(ν)⊤ [κ(xi, x̂1), . . . , κ(xi, x̂M )] ⊤ for ν ∈ {1, 2}.\n2: Unlabeled data. Compute CCA bases B(1), B(2) and canonical correlations λ1, . . . , λM for the two views and set z̄i ← B(1)z(1)(xi). 3: Labeled data. Solve\nβ̂ = argmin β\n1\nn\nn∑\ni=1\nℓ ( β⊤z̄i, yi ) + ‖β‖2CCA + γ‖β‖ 2 2 . (9)"
    }, {
      "heading" : "Output: β̂",
      "text" : "features are heavily downweighted in the CCA basis without introducing an additional tuning parameter. The further penalty on the ℓ2 norm (in the CCA basis) is introduced as a practical measure to control the variance of the estimator β̂ which can become large if there are many highly correlated features (i.e. the ratio 1−λjλj ≈ 0 for large j). In practice most of the shrinkage is due to the CCA norm: cross-validation obtains optimal values of γ in the range [0.00001, 0.1].\nComputational complexity. XNV is extremely fast. Nyström sampling, step 1, reduces the O(N3) operations required for kernel learning to O(N). Computing the CCA basis, step 2, using standard algorithms is in O(NM2). However, we reduce the runtime to O(NM) by applying a recently proposed randomized CCA algorithm of [16]. Finally, step 3 is a computationally cheap linear program on n samples and M features.\nPerformance guarantees. The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3–5, 15]. Recent work of Bach [5] provides theoretical guarantees on the quality of Nyström estimates in the fixed design setting that are relevant to our approach.1\nTheorem 3 (Nyström generalization bound, [5]). Let ξ ∈ RN be a random vector with finite variance and zero mean, y = [y1, . . . , yN ]\n⊤, and define smoothed estimate ŷkernel := (K + NγI)−1K(y + ξ) and smoothed Nyström estimate ŷNyström := (K̃ + NγI)−1K̃(y + ξ), both computed by minimizing the MSE with ridge penalty γ. Let η ∈ (0, 1). For sufficiently large M (depending on η, see [5]), we have\nEMEξ [ ‖y − ŷNyström‖ 2 2 ] ≤ (1 + 4η) · Eξ [ ‖y − ŷkernel‖ 2 2 ]\nwhere EM refers to the expectation over subsampled columns used to construct K̃.\nIn short, the best smoothed estimators in the Nyström views are close to the optimal smoothed estimator. Since the kernel estimate is consistent, loss(f) → 0 as n → ∞. Thus, Assumption 1 holds in expectation and the generalization performance of XNV is controlled by Theorem 1.\nRandom Fourier Features. An alternative approach to constructing random views is to use Fourier features instead of Nyström features in Step 1. We refer to this approach as Correlated Kitchen Sinks (XKS) after [2]. It turns out that the performance of XKS is consistently worse than XNV, in line with the detailed comparison presented in [3]. We therefore do not discuss Fourier features in the main text, see §SI.3 for details on implementation and experimental results.\n1Extending to a random design requires techniques from [17].\n2.4 A fast approximation to SSSL\nThe SSSL (simple semi-supervised learning) algorithm proposed in [10] finds the first s eigenfunctions φi of the integral operator LN in Eq. (6) and then solves\nargmin w∈Rs\nn∑\ni=1\n  s∑\nj=1\nwjφk(xi)− yi\n  2\n, (10)\nwhere s is set by the user. SSSL outperforms Laplacian Regularized Least Squares [11], a state of the art semi-supervised learning method, see [10]. It also has good generalization guarantees under reasonable assumptions on the distribution of eigenvalues of LN . However, since SSSL requires computing the full N × N Gram matrix, it is extremely computationally intensive for large N . Moreover, tuning s is difficult since it is discrete.\nWe therefore propose SSSLM , an approximation to SSSL. First, instead of constructing the full Gram matrix, we construct a Nyström approximation by sampling M points from the labeled and unlabeled training set. Second, instead of thresholding eigenfunctions, we use the easier to tune ridge penalty which penalizes directions proportional to the inverse square of their eigenvalues [18].\nAs justification, note that Proposition 2 states that the Nyström approximation to kernel regression actually solves a ridge regression problem in the span of the eigenfunctions of L̂M . As M increases, the span of L̂M tends towards that of LN [15]. We will also refer to the Nyström approximation to SSSL using 2M features as SSSL2M . See experiments below for further discussion of the quality of the approximation."
    }, {
      "heading" : "3 Experiments",
      "text" : "Setup. We evaluate the performance of XNV on 18 real-world datasets, see Table 1. The datasets cover a variety of regression (denoted by R) and two-class classification (C) problems. The sarcos dataset involves predicting the joint position of a robot arm; following convention we report results on the 1st, 5th and 7th joint positions.\nThe SSSL algorithm was shown to exhibit state-of-the-art performance over fully and semisupervised methods in scenarios where few labeled training examples are available [10]. However, as discussed in §2.2, due to its computational cost we compare the performance of XNV to the Nyström approximations SSSLM and SSSL2M .\nWe used a Gaussian kernel for all datasets. We set the kernel width, σ and the ℓ2 regularisation strength, γ, for each method using 5-fold cross validation with 1000 labeled training examples. We trained all methods using a squared error loss function, ℓ(f(xi), yi) = (f(xi)−yi)2, with M = 200 random features, and n = 100, 150, 200, . . . , 1000 randomly selected training examples.\n2Taken from the UCI repository http://archive.ics.uci.edu/ml/datasets.html 3Taken from http://www.causality.inf.ethz.ch/activelearning.php 4Taken from http://www.dcc.fc.up.pt/˜ltorgo/Regression/DataSets.html 5Taken from http://www.gaussianprocess.org/gpml/data/\nRuntime performance. The SSSL algorithm of [10] is not computationally feasible on large datasets, since it has time complexity O(N3). For illustrative purposes, we report run times6 in seconds of the SSSL algorithm against SSSLM and XNV on three datasets of different sizes.\nruntimes bank8 cal housing sylva SSSL 72s 2300s - SSSL2M 0.3s 0.6s 24s XNV 0.9s 1.3s 26s\nFor the cal housing dataset, XNV exhibits an almost 1800× speed up over SSSL. For the largest dataset, sylva, exact SSSL is computationally intractable. Importantly, the computational overhead of XNV over SSSL2M is small.\nGeneralization performance. We report on the prediction performance averaged over 100 experiments. For regression tasks we report on the mean squared error (MSE) on the testing set normalized by the variance of the test output. For classification tasks we report the percentage of the test set that was misclassified.\nThe table below shows the improvement in performance of XNV over SSSLM and SSSL2M (taking whichever performs better out of M or 2M on each dataset), averaged over all 18 datasets. Observe that XNV is considerably more accurate and more robust than SSSLM .\nXNV vs SSSLM/2M n = 100 n = 200 n = 300 n = 400 n = 500 Avg reduction in error 11% 16% 15% 12% 9% Avg reduction in std err 15% 30% 31% 33% 30%\nThe reduced variability is to be expected from Theorem 1.\nTable 2 presents more detailed comparison of performance for individual datasets when n = 200, 400. The plots in Figure 1 shows a representative comparison of mean prediction errors for several datasets when n = 100, . . . , 1000. Error bars represent one standard deviation. Observe that XNV almost always improves prediction accuracy and reduces variance compared with SSSLM and SSSL2M when the labeled training set contains between 100 and 500 labeled points. A complete set of results is provided in §SI.1.\nDiscussion of SSSLM . Our experiments show that going from M to 2M does not improve generalization performance in practice. This suggests that when there are few labeled points, obtaining a\n6Computed in Matlab 7.14 on a Core i5 with 4GB memory.\nmore accurate estimate of the eigenfunctions of the kernel does not necessarily improve predictive performance. Indeed, when more random features are added, stronger regularization is required to reduce the influence of uninformative features, this also has the effect of downweighting informative features. This suggests that the low rank approximation SSSLM to SSSL suffices.\nFinally, §SI.2 compares the performance of SSSLM and XNV to fully supervised kernel ridge regression (KRR). We observe dramatic improvements, between 48% and 63%, consistent with the results observed in [10] for the exact SSSL algorithm.\nRandom Fourier features. Nyström features significantly outperform Fourier features, in line with observations in [3]. The table below shows the relative improvement of XNV over XKS:\nXNV vs XKS n = 100 n = 200 n = 300 n = 400 n = 500 Avg reduction in error 30% 28% 26% 25% 24% Avg reduction in std err 36% 44% 34% 37% 36%\nFurther results and discussion for XKS are included in the supplementary material."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We have introduced the XNV algorithm for semi-supervised learning. By combining two randomly generated views of Nyström features via an efficient implementation of CCA, XNV outperforms the prior state-of-the-art, SSSL, by 10-15% (depending on the number of labeled points) on average over 18 datasets. Furthermore, XNV is over 3 orders of magnitude faster than SSSL on medium sized datasets (N = 10, 000) with further gains as N increases. An interesting research direction is to investigate using the recently developed deep CCA algorithm, which extracts higher order correlations between views [19], as a preprocessing step.\nIn this work we use a uniform sampling scheme for the Nyström method for computational reasons since it has been shown to perform well empirically relative to more expensive schemes [20]. Since CCA gives us a criterion by which to measure the important of random features, in the future we aim to investigate active sampling schemes based on canonical correlations which may yield better performance by selecting the most informative indices to sample.\nAcknowledgements. We thank Haim Avron for help with implementing randomized CCA and Patrick Pletscher for drawing our attention to the Nyström method."
    }, {
      "heading" : "SI.2 Comparison with Kernel Ridge Regression",
      "text" : "We compare SSSLM and XNV to kernel ridge regression (KRR). The table below reports the percentage improvement in mean error of both of these methods against KRR, averaged over the 18 datasets according to the experimental procedure detailed in §3. Parameters σ (kernel width) and γ (ridge penalty) for KRR were chosen by 5-fold cross validation. We observe that both SSSLM and XNV far outperform KRR, by 50 − 60%. Importantly, this shows our approximation to SSSL far outperforms the fully supervised baseline.\nSSSLM and XNV vs KRR n = 100 n = 200 n = 300 n = 400 n = 500 Avg reduction in error for SSSLM 48% 52% 56% 58% 60% Avg reduction in error for XNV 56% 62% 63% 63% 63%"
    }, {
      "heading" : "SI.3 Random Fourier features",
      "text" : "Random Fourier features are a method for approximating shift invariant kernels [6], i.e. where κ(xi,xi′ ) = κ(xi − xi′ ). Such a kernel function can be represented in terms of its inverse Fourier transform as κ(xi − xi′ ) = ∫ RD P (ω)ejω ⊤(xi−xi′). P (ω) is the Fourier transform of κ which is guaranteed to be a proper probability distribution and so for real-valued features κ(xi,xi′) can be equivalently interpreted as Eω [ z(xi) ⊤z(xi′ ) ]\nwhere z(xi) = 1√2 cos(ω ⊤xi + b) . Replacing\nthe expectation by the sample average leads to a scheme for constructing random features. In particular, a Gaussian kernel of width σ has a Fourier transform which is also Gaussian. Sampling ωm ∼ N (0, 2σID) and bm ∼ Unif [−π, π], we can then construct features whose inner product approximates this kernel as zi = 1√M [ cos(ω⊤1 xi + b1), . . . , cos(ω ⊤ Mxi + bM ) ] .\nIt was recently shown how both random Fourier features the Nyström approximation could be cast in the same framework [3]. A major difference between the methods lies in the sampling scheme employed. Random Fourier features are constructed in a data independent fashion which makes them extremely cheap to compute. Nyström features are constructed in a data dependent way which leads to improved performance but, in the case of semi-supervised learning, more expensive since we need to evaluate the approximate kernel for all unlabeled points we wish to use.\nAlgorithm 2 details Correlated Kitchen Sinks (XKS). This algorithm generates random views using the random Fourier features procedure in step 1. Steps 2 and 3 proceed exactly as in Algorithm 1.\nAlgorithm 2 Correlated Kitchen Sinks (XKS).\nInput: Labeled data: {xi, yi}ni=1 and unlabeled data: {xi} N i=n+1\n1: Generate features. Draw ω1, . . . ω2K i.i.d. from P and featurize the input:\nz (1) i ← [φ(xi;ω1), . . . , φ(xi;ωM )] ,\nz (2) i ← [φ(xi;ωM+1), . . . , φ(xi;ω2M )] .\n2: Unlabeled data. Compute CCA bases B(1), B(2) and canonical correlations λ1, . . . , λM for the two views and set z̄i ← B(1)z (1) i . 3: Labeled data. Solve\nβ̂ = min β\n1\nn\nn∑\ni=1\nℓ ( β⊤z̄i, yi ) + ‖β‖2CCA + γ‖β‖ 2 2 . (11)"
    }, {
      "heading" : "Output: β̂",
      "text" : "It can be shown that, with sufficiently many features, views constructed via random Fourier features contain good approximations to a large class of functions with high probability, see main theorem of [2]. We do not provide details, since XKS is consistently outperformed by XNV in practice.\nSI.4 Complete XKS results\nFor completeness we report on the performance of the XKS algorithm. We use the same experimental setup as in Section 3. We compare the performance of XKS against a linear machine learned using M and 2M random Fourier features respectively.\nTable 4 shows the performance improvement of XKS over RFFM/2M , averaged across the 18 datasets. Table 6 compares the prediction error and standard deviation for each of the datasets individually. Figure 3 shows the performance across the full range of values of n for all datasets. The relative performance of XKS against RFFM and RFF2M follows the same trend seen in Section 3, suggesting that CCA-based regression consistently improves on regression across single and joint views.\nFinally, Table 5 compares the performance of correlated Nyström features against correlated kitchen sinks. XNV typically outperforms XKS on 16 out of 18 datasets; with XKS only ever outperforming XNV on bank8, house and orange. Since XNV almost always outperformsXKS, we only discuss Nyström features in the main text."
    } ],
    "references" : [ {
      "title" : "Using the Nyström method to speed up kernel machines",
      "author" : [ "C Williams", "M Seeger" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning",
      "author" : [ "A Rahimi", "B Recht" ],
      "venue" : "In Adv in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "ZH: Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison",
      "author" : [ "T Yang", "YF Li", "M Mahdavi", "R Jin", "Zhou" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Revisiting the Nyström method for improved large-scale machine learning",
      "author" : [ "A Gittens", "MW Mahoney" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Sharp analysis of low-rank kernel approximations",
      "author" : [ "F Bach" ],
      "venue" : "COLT",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Random Features for Large-Scale Kernel Machines",
      "author" : [ "A Rahimi", "B Recht" ],
      "venue" : "In Adv in Neural Information Processing Systems",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Multi-view Regression Via Canonical Correlation Analysis",
      "author" : [ "S Kakade", "DP Foster" ],
      "venue" : "In Computational Learning Theory (COLT)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Relations between two sets of variates",
      "author" : [ "H Hotelling" ],
      "venue" : "Biometrika",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1936
    }, {
      "title" : "Canonical Correlation Analysis: An Overview with Application to Learning Methods",
      "author" : [ "DR Hardoon", "S Szedmak", "J Shawe-Taylor" ],
      "venue" : "Neural Comp",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    }, {
      "title" : "A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound",
      "author" : [ "M Ji", "T Yang", "B Lin", "R Jin", "J Han" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples",
      "author" : [ "M Belkin", "P Niyogi", "V Sindhwani" ],
      "venue" : "JMLR",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "A Blum", "T Mitchell" ],
      "venue" : "COLT",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1998
    }, {
      "title" : "Multiview clustering via Canonical Correlation Analysis",
      "author" : [ "K Chaudhuri", "SM Kakade", "K Livescu", "K Sridharan" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Multi-view predictive partitioning in high dimensions",
      "author" : [ "B McWilliams", "G Montana" ],
      "venue" : "Statistical Analysis and Data Mining",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning",
      "author" : [ "P Drineas", "MW Mahoney" ],
      "venue" : "JMLR",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2005
    }, {
      "title" : "Efficient Dimensionality Reduction for Canonical Correlation Analysis",
      "author" : [ "H Avron", "C Boutsidis", "S Toledo", "A Zouzias" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "An Analysis of Random Design Linear Regression",
      "author" : [ "D Hsu", "S Kakade", "T Zhang" ],
      "venue" : "COLT",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "A Risk Comparison of Ordinary Least Squares vs Ridge Regression",
      "author" : [ "PS Dhillon", "DP Foster", "SM Kakade", "LH Ungar" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Deep Canonical Correlation Analysis",
      "author" : [ "G Andrew", "R Arora", "J Bilmes", "K Livescu" ],
      "venue" : "ICML",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Randomization has recently been considered as an alternative to optimization that, surprisingly, can yield comparable generalization performance at a fraction of the computational cost [1, 2].",
      "startOffset" : 185,
      "endOffset" : 191
    }, {
      "referenceID" : 1,
      "context" : "Randomization has recently been considered as an alternative to optimization that, surprisingly, can yield comparable generalization performance at a fraction of the computational cost [1, 2].",
      "startOffset" : 185,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "Among several different approaches, the Nyström method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3–5].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : "Among several different approaches, the Nyström method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3–5].",
      "startOffset" : 156,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "Among several different approaches, the Nyström method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3–5].",
      "startOffset" : 156,
      "endOffset" : 161
    }, {
      "referenceID" : 4,
      "context" : "Among several different approaches, the Nyström method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3–5].",
      "startOffset" : 156,
      "endOffset" : 161
    }, {
      "referenceID" : 1,
      "context" : "We investigate two ways of doing so: one based on the Nyström method and another based on random Fourier features (so-called kitchen sinks) [2, 6].",
      "startOffset" : 140,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "We investigate two ways of doing so: one based on the Nyström method and another based on random Fourier features (so-called kitchen sinks) [2, 6].",
      "startOffset" : 140,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "The second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "The second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views.",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "The second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views.",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Recent theoretical work by Bach [5] shows that Nyström views can be expected to contain accurate estimators.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "We perform an extensive evaluation of XNV on 18 real-world datasets, comparing against a modified version of the SSSL (simple semi-supervised learning) algorithm introduced in [10].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 9,
      "context" : "We chose SSSL since it was shown in [10] to outperform a state of the art algorithm, Laplacian Regularized Least Squares [11].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : "We chose SSSL since it was shown in [10] to outperform a state of the art algorithm, Laplacian Regularized Least Squares [11].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "Our approximate version of SSSL outperforms kernel ridge regression (KRR) by > 50% on the 18 datasets on average, in line with the results reported in [10], suggesting that we lose little by replacing the exact SSSL with our approximate implementation.",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 11,
      "context" : "Multiple view learning was first introduced in the co-training method of [12] and has also recently been extended to unsupervised settings [13,14].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "Multiple view learning was first introduced in the co-training method of [12] and has also recently been extended to unsupervised settings [13,14].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "Multiple view learning was first introduced in the co-training method of [12] and has also recently been extended to unsupervised settings [13,14].",
      "startOffset" : 139,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "Our algorithm builds on an elegant proposal for multi-view regression introduced in [7].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "First, given two equally useful but sufficiently different views on a dataset, penalizing regression using the canonical norm (computed via CCA), can substantially improve performance [7].",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 0,
      "context" : "The second is the Nyström method for constructing random features [1], which we use to construct the views.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "Assumption 1 (Multi-view assumption [7]).",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "Canonical correlation analysis [8, 9] extends principal component analysis (PCA) from one to two sets of variables.",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "Canonical correlation analysis [8, 9] extends principal component analysis (PCA) from one to two sets of variables.",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "More formally: Theorem 1 (canonical ridge regression, [7]).",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "To ensure our method scales to large sets of unlabeled data, we use random features generated using the Nyström method [1].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "The idea behind random features is to instead define a lower-dimensional mapping, z(xi) : R → R M through a random sampling scheme such that [K]ii′ ≈ z(xi) z(xi′ ) [6, 15].",
      "startOffset" : 164,
      "endOffset" : 171
    }, {
      "referenceID" : 14,
      "context" : "The idea behind random features is to instead define a lower-dimensional mapping, z(xi) : R → R M through a random sampling scheme such that [K]ii′ ≈ z(xi) z(xi′ ) [6, 15].",
      "startOffset" : 164,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : "The Nyström method [1, 3] constructs a low-rank approximation to the Gram matrix as",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "The Nyström method [1, 3] constructs a low-rank approximation to the Gram matrix as",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 14,
      "context" : "Constructing features in this way reduces the time complexity of learning a non-linear prediction function from O(N) to O(N) [15].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "(6): Proposition 2 (random Nyström view, [3]).",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "However, we reduce the runtime to O(NM) by applying a recently proposed randomized CCA algorithm of [16].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3–5, 15].",
      "startOffset" : 166,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3–5, 15].",
      "startOffset" : 166,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3–5, 15].",
      "startOffset" : 166,
      "endOffset" : 175
    }, {
      "referenceID" : 14,
      "context" : "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3–5, 15].",
      "startOffset" : 166,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "Recent work of Bach [5] provides theoretical guarantees on the quality of Nyström estimates in the fixed design setting that are relevant to our approach.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "1 Theorem 3 (Nyström generalization bound, [5]).",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "For sufficiently large M (depending on η, see [5]), we have",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "We refer to this approach as Correlated Kitchen Sinks (XKS) after [2].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "It turns out that the performance of XKS is consistently worse than XNV, in line with the detailed comparison presented in [3].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 16,
      "context" : "Extending to a random design requires techniques from [17].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "4 A fast approximation to SSSL The SSSL (simple semi-supervised learning) algorithm proposed in [10] finds the first s eigenfunctions φi of the integral operator LN in Eq.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "SSSL outperforms Laplacian Regularized Least Squares [11], a state of the art semi-supervised learning method, see [10].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "SSSL outperforms Laplacian Regularized Least Squares [11], a state of the art semi-supervised learning method, see [10].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : "Second, instead of thresholding eigenfunctions, we use the easier to tune ridge penalty which penalizes directions proportional to the inverse square of their eigenvalues [18].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 14,
      "context" : "As M increases, the span of L̂M tends towards that of LN [15].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "The SSSL algorithm was shown to exhibit state-of-the-art performance over fully and semisupervised methods in scenarios where few labeled training examples are available [10].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 9,
      "context" : "The SSSL algorithm of [10] is not computationally feasible on large datasets, since it has time complexity O(N).",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "We observe dramatic improvements, between 48% and 63%, consistent with the results observed in [10] for the exact SSSL algorithm.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Nyström features significantly outperform Fourier features, in line with observations in [3].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 18,
      "context" : "An interesting research direction is to investigate using the recently developed deep CCA algorithm, which extracts higher order correlations between views [19], as a preprocessing step.",
      "startOffset" : 156,
      "endOffset" : 160
    } ],
    "year" : 2013,
    "abstractText" : "This paper presents Correlated Nyström Views (XNV), a fast semi-supervised algorithm for regression and classification. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude.",
    "creator" : "LaTeX with hyperref package"
  }
}