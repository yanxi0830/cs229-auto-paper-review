{
  "name" : "1506.02649.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Faster SGD Using Sketched Conditioning",
    "authors" : [ "Alon Gonen", "Shai Shalev-Shwartz" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 6.\n02 64\n9v 1\n[ cs\n.N A\n] 8\nJ un"
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider empirical loss minimization problems of the form:\nmin W∈Rp×n\nL(W ) := 1\nm\nm ∑\ni=1\nℓyi(Wxi) , (1)\nwhere for every i, xi is an n-dimensional vector and ℓyi is a loss function from R p to R. For example, in multiclass categorization with the logistic-loss, we have that ℓyi(a) = log (\n∑p j=1 exp(aj − ayi)\n)\n. Later in the paper we will generalize the discussion to the\ncase in which W is the weight matrix of an intermediate layer of a deep neural network. We consider the large data regime, in which m is large. A popular algorithm for this case is Stochastic Gradient Descent (SGD). The basic idea is to initialize W1 to be some matrix, and at each time t to draw an index i ∈ {1, . . . ,m} uniformly at random from the training sequence S = ((x1, y1), . . . , (xm, ym)), and then update Wt+1 based on the gradient of ℓyi(Wxi) at W . When performing this update we would like to decrease the value of ℓyi(Wxi) while bearing in mind that we only look on a single example, and therefore we should not change W too much. This can be formalized by an update of the form\nWt+1 = argmin W∈Rp×n\n1\n2η D(W,Wt) + ℓyi(Wxi) ,\n∗School of Computer Science, The Hebrew University, Jerusalem, Isreal †School of Computer Science, The Hebrew University, Jerusalem, Isreal\nwhere D(·, ·) is some distance measure between matrices and η, the learning rate, controls the tradeoff between the desire to minimize the function and the desire to stay close to Wt. Since we keep W close to Wt, we can further simplify things by using the first-order approximation of ℓyi around Wtxi,\nℓyi(Wxi) ≈ ℓyi(Wtxi) + 〈W −Wt , (∇ℓyi(Wtxi))x⊤i 〉 ,\nwhere ∇ℓyi(Wtxi) ∈ Rp is the (sub)gradient of ℓyi at the p-dimensional vector Wtxi (as a column vector), and for two matricesA,B we use the notation 〈A,B〉 = ∑i,j Ai,jBi,j . Hence, the update becomes\nWt+1 = argmin W∈Rp×n\n1\n2η D(W,Wt) + ℓyi(Wtxi) + 〈W −Wt , (∇ℓyi(Wtxi))x⊤i 〉 (2)\nEquation (2) defines a family of algorithms, where different instances are derived by specifying the distance measure D. The simplest choice of D is the squared Frobenius norm regularizer, namely,\nD(W,Wt) = ‖W −Wt‖2F = 〈W −Wt,W −Wt〉 .\nIt is easy to verify that for this choice of D, the update given in Equation (2) becomes\nWt+1 = Wt − η(∇ℓyi(Wtxi))x⊤i ,\nwhich is exactly the update rule of SGD. Note that the Frobenius norm distance measure can be rewritten as\nD(W,Wt) = 〈I , (W −Wt)⊤(W −Wt)〉\nIn this paper, we consider the family of distance measures of the form\nDA(W,Wt) = 〈A , (W −Wt)⊤(W −Wt)〉\nwhere A is a positive definite matrix. For every such choice of A, the update given in Equation (2) becomes\nWt+1 = Wt − η(∇ℓyi(Wtxi))(A−1xi)⊤ . (3)\nWe refer to the matrix A as a conditioning matrix (for a reason that will become clear shortly) and call the resulting algorithm Conditioned SGD.\nHow should we choose the conditioning matrix A? There are two considerations. First, we would like to choose A so that the algorithm will converge to a solution of Equation (1) as fast as possible. Second, we would like that it will be easy to compute both the matrix A and the update rule given in Equation (3).\nWe start with the first consideration. Naturally, the convergence of the Conditioned SGD algorithm depends on the specific problem at hand. However, we can rely on convergence bounds and picks A that minimizes these bounds. Concretely, assuming that each ℓyi is convex and ρ-Lipschitz, denote C = 1 m ∑m i=1 xix ⊤ i the correlation\nmatrix of the data, and let W ∗ be an optimum of Equation (1), then the sub-optimality of the Conditioned SGD algorithm after performing T iterations is upper bounded by\n1\n2ηT DA(W\n⋆,W1) + ηρ2\n2 tr(A−1C) .\nWe still cannot minimize this bound w.r.t. A as we do not know the value of W ⋆. So, we further upper bound DA(W ⋆,W1) by considering two possible bounds. Denoting the spectral norm and the trace norm by ‖ · ‖sp and ‖ · ‖tr, respectively, we have\n1. DA(W ⋆,W1) ≤ ‖A‖sp ‖(W ⋆ −W1)⊤(W ⋆ −W1)‖tr 2. DA(W ⋆,W1) ≤ ‖A‖tr ‖(W ⋆ −W1)⊤(W ⋆ −W1)‖sp\nInterestingly, for the first possibility above, the optimal A becomes A = I , corresponding to the vanilla SGD algorithm. However, for the second possibility, we show that the optimal A becomes A = C1/2. The ratio between the required number of iterations to achieve ǫ sub-optimality is\n# iterations for A = I # iterations for A = C1/2 = ‖(W ⋆ −W1)⊤(W ⋆ −W1)‖tr ‖C‖tr ‖(W ⋆ −W1)⊤(W ⋆ −W1)‖sp ‖C1/2‖2tr\nThe above ratio is always between 1/n and min{n, p}. We argue that in many typical cases the ratio will be greater than 1, meaning that the conditioner A = C1/2 will lead to faster convergence. For example, suppose that the norm of each row of W ⋆ is order of 1, but the rows are not correlated. Let us also choose W1 = 0 and assume that p = Θ(n). Then, ‖(W\n⋆−W1)⊤(W⋆−W1)‖tr ‖(W⋆−W1)⊤(W⋆−W1)‖sp is order of n. On the other hand, if the\neigenvalues of C decay fast, then ‖C‖tr‖C1/2‖2tr ≈ 1. Therefore, in such scenarios, using the conditioner A = C1/2 will lead to a factor of n less iterations relatively to vanilla SGD.\nGetting back to the question of how to choose A, the second consideration that we have mentioned is the time required to compute A−1 and to apply the update rule given in Equation (3). As we will show later, the time required to compute A−1 is less of an issue relatively to the time of applying the update rule at each iteration, so we focus on the latter.\nObserve that the time required to apply Equation (3) is order of (p+n)n. Therefore, if p ≈ n then we have no significant overhead in applying the conditioner relatively to applying vanilla SGD. If p ≪ n, then the update time is dominated by the time required to compute A−1xi. To decrease this time, we propose to use A of the form QBQ⊤ + a(I − QQ⊤), where Q ∈ Rn×k has orthonormal columns, B ∈ Rk×k is invertible and k ≪ n. We use linear sketching techniques (see [23]) for constructing this conditioner efficiently, and therefore we refer to the resulting algorithm as Sketched Conditioned SGD (SCSGD). Intuitively, the sketched conditioner is a combination of the two conditioners A = I and A = C1/2, where the matrix QBQ⊤ captures the top eigenvalues of C and the matrix a(I − QQ⊤) deals with the smaller eigenvalues of C. We show that if the eigenvalues of C decay fast enough then SCSGD enjoys similar speedup to the full conditioner A = C1/2. The advantage of using the sketched\nconditioner is that the time required to apply Equation (3) becomes (p+k)n. Therefore, if p ≥ k then the runtime per iteration of SCSGD and the runtime per iteration of the vanilla SGD are of the same order of magnitude.\nThe rest of the paper is organized as follows. In the next subsection we survey some related work. In Section 2 we describe in detail our conditioning method. Finally, in Section 3 we discuss variants of the method that are applicable to deep learning problems and report some preliminary experiments showing the merits of conditioning for deep learning problems. Due to the lack of space, proofs are omitted and can be found in Appendix A."
    }, {
      "heading" : "1.1 Related work",
      "text" : "Conditioning is a well established technique in optimization aiming at choosing an “appropriate” coordinate system for the optimization process. For twice differentiable objectives, maybe the most well known approach is Newton’s method which dynamically changes the coordinate system according to the Hessian of the objective around the current solution. There are several problems with utilizing the Hessian. First, in our case, the Hessian matrix is of size (pn)× (pn). Hence, it is computationally expensive to compute and invert it. Second, even for convex problems, the Hessian matrix might be meaningless. For example, for linear regression with the absolute loss the Hessian matrix is the zero matrix almost everywhere. Third, when the number of training examples is very large, stochastic methods are preferable and it is not clear how to adapt Newton method to the stochastic case. The crux of the problem is that while it is easy to construct an unbiased, low variance, estimate of the gradient, based on a single example, it is not clear how to construct a good estimate of the Newton’s direction based on a small mini-batch of examples.\nMany approaches have been proposed for speeding up Newton’s method. For example, the R{·} operator technique [14, 22, 10, 9]. However, these methods are not applicable for the stochastic setting. An obvious way to decrease the storage and computational cost is to only consider the diagonal elements of the Hessian (see [3]). Schraudolph [18] proposed an adaptation of the L-BFGS approach to the online setting, in which at each iteration, the estimation of the inverse of the Hessian is computed based on only the last few noisy gradients. Naturally, this yields a low rank approximation. In [5], the two aforementioned approaches are combined to yield the SGD-QN algorithm. In the same paper, an analysis of second order SGD is described, but with A being always the Hessian matrix at the optimum (which is of course not known). There are various other approximations, see for example [16, 4, 21, 15].\nTo tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian. A somewhat related approach is Amari’s natural gradient descent [1, 2]. See the discussion in [13]. To the best of our knowledge, these methods come with no theoretical guarantees.\nThe aforementioned approaches change the conditioner at each iteration of the algorithm. A general treatment of this approach is described in [11][Section 1.3.1] under the name “Variable Metric”. Maybe the most relevant approach is the Adagrad algorithm [6], which was originally proposed for the online learning setting but can be easily adapted to the stochastic optimization setting. In our notation, the AdaGrad al-\ngorithm uses a (pn) × (pn) conditioning matrix that changes along time and has the form,At = δI+ 1t ∑t i=1 ∇t∇⊤t , where∇t = vec(∇ℓyi(Wtxi))x⊤i ). There are several advantages of our method relatively to AdaGrad. First, the convergence bound we obtain is better than the convergence bound of AdaGrad. Specifically, while both bounds have the sane dependence on ‖C1/2‖2tr, our bound depends on ‖W ∗‖2sp while AdaGrad depends on ‖W ∗‖2F . As we discussed before, there may be a gap of p between ‖W ∗‖2sp and ‖W ∗‖2F . More critically, when using a full conditioner, the storage complexity of our conditioner is n2, while the storage complexity of AdaGrad is (np)2. In addition, the time complexity of applying the update rule is (p+ n)n for our conditioner versus (np)2 for AdaGrad. For this reason, most practical application of AdaGrad relies on a diagonal approximation of At. In contrast, we can use a full conditioner in many practical cases, and even when n is large our sketched conditioner can be applied without a significant increase in the complexity relatively to vanilla SGD. Finally, because we derive our algorithm for the stochastic case (as opposed to the adversarial online optimization setting), and because we bound the component ∇ℓy(Wtx) using the Lipschitzness of ℓy , the conditioner we use is the constant C−1/2 along the entire run of the optimization process, and should only be calculated once. In contrast, AdaGrad replaces the conditioner in every iteration."
    }, {
      "heading" : "2 Conditioning and Sketched Conditioning",
      "text" : "As mentioned previously, the algorithms we consider start with an initial matrix W1 and at each iteration update the matrix according to Equation (3). The following lemma provides an upper bound on the expected sub-optimality of any algorithm of this form.\nLemma 1. Fix a positive definite matrix A ∈ Rn×n. Let W ⋆ be the minimizer of Equation (1), let σ ∈ R be such that σ ≥ ‖W ⋆‖sp and denote C = 1m ∑m i=1 xix ⊤ i . Assume that for every i, ℓyi is convex and ρ-Lipschitz. Then, if we apply the update rule given in Equation (3) using the conditioner A and denote W̄ = 1T ∑T t=1 Wt, then\nE[L(W̄ )− L(W ⋆)] ≤ 1 2ηT tr(AW ⋆⊤W ⋆) + ηρ2 2 E [ tr(A−1C) ]\n≤ σ 2\n2ηT tr(A) +\nηρ2\n2 E [ tr(A−1C) ] .\nIn particular, for η = σ/(ρ √ T ), we obtain\nE[L(W̄ )− L(W ⋆)] ≤ σρ√ T (tr(A) + tr(A−1C)) .\nThe proof of the above lemma can be obtained by replacing the standard inner product with the inner product induced by A. For completeness, we provide a proof in Appendix A.\nIn Appendix A we show that the conditioner which minimizes the bound given in the above Lemma is A = C1/2. This yields:\nTheorem 1. Following the notation of Lemma 1, assume that we run the meta-algorithm with A = C1/2, then\nE[L(W̄ )− L(W ⋆)] ≤ σρ√ T · tr(C1/2) ."
    }, {
      "heading" : "2.1 Sketched Conditioning",
      "text" : "Let k < n and assume that rank(C) ≥ k. Consider the following family of conditioners:\nA = {A = QBQ⊤ + a(I −QQ⊤) : Q ∈ Rn×k, Q⊤Q = I, B ≻ 0 ∈ Rk×k, a > 0} (4) Before proceeding, we show that the conditioners in A are indeed positive definite, and give a formula for their inverse.\nLemma 2. Let A = QBQ⊤+ a(I−QQ⊤) ∈ A. Then, A ≻ 0 and its inverse is given by\nA−1 = QB−1Q⊤ + a−1(I −QQ⊤) .\nInformally, every conditioner A ∈ A is a combination of a low rank conditioner and the identity conditioner. The most appealing property of these conditioners is that we can compute A−1x in time O(nk) and therefore the time complexity of calculating the update given in Equation (3) is O(n(p+ k)).\nIn the next subsections we focus on instances of A which are induced by an approximate best rank-k approximation of C. However, for now, we give an analysis for any choice of A ∈ A.\nTheorem 2. Following the notation of Lemma 1, let A ∈ A and denote C̃ = Q⊤CQ. Then, if a = √\ntr(C)−tr(C̃) n−k , we have\nE[L(W̄ )−L(W ⋆)] ≤ σρ 2 √ T · ( tr(B) + tr(B−1C̃) + 2 √ (n− k)(tr(C)− tr(C̃)) ) ."
    }, {
      "heading" : "2.2 Low-rank conditioning via exact low-rank approximation",
      "text" : "Maybe the most straightforward approach of defining Q and B is by taking the leading eigenvectors of C. Concretely, let C = UDU⊤ be the eigenvalue decomposition of C and denote the diagonal elements of D by λ1 ≥ . . . ≥ λn ≥ 0. Recall that for any k ≤ n, the best rank-k approximation of C is given by Ck = UkDkU⊤k , where Uk ∈ Rn×k consists of the first k columns of U and Dk is the first k × k sub-matrix of D. Denote C̃ = Q⊤CQ and consider the conditioner Ã which is determined from Equation (4) by setting Q = Uk, B = C̃1/2, and a as in Theorem 2.\nTheorem 3. Let Q = Uk, B = C̃1/2 and a as in Theorem 2, and consider the conditioner given in Equation (4). Then,\nE[L(W̄ )− L(W ⋆)] ≤ σρ√ T · ( tr(C 1/2 k ) + √ (n− k)(tr(C)− tr(Ck)) ) .\nIn particular, if √ (n− k)(tr(C) − tr(Ck)) = O(tr(C1/2)), then the obtained bound is of the same order as the bound in Theorem 1.\nWe refer to the condition √ (n− k)(tr(C) − tr(Ck)) = O(tr(C1/2)) as a fast spectral decay property of the matrix C."
    }, {
      "heading" : "2.3 Low-rank conditioning via sketching",
      "text" : "The conditioner defined in the previous subsection requires the exact calculation of the matrix C and its eigenvalue decomposition. In this section we describe a faster technique for calculating a sketched conditioner. Before formally describing the sketching technique, let us try to explain the intuition behind it. Figure 1 depicts a set of 1000 (blue) random points in the plane. Suppose that we represent this sequence by a matrix X ∈ R2×1000. Now we draw a vector ω ∈ R1000×1 whose coordinates are N (0, 1) i.i.d. random variables and consider the vector z = Xω. The vector z is simply a random combination of these points. As we can see, z coincides with the strongest direction of the data. More generally, the idea of sketching is that if we take a matrix X ∈ Rn×m and multiply it from the right by random matrix Ω ∈ Rm×r, then with high probability, we preserve the strongest directions of the column space of X . The above intuition is formalized by the following result, which follows from [17] by setting ǫ = 11.\nLemma 3. Let X ∈ Rn×m. Let r = Θ(k) and let Ω ∈ Rm×r be a random matrix whose elements are i.i.d. N (0, 1/r) random variables. Let P ∈ Rn×r be a matrix whose columns form an orthonormal basis of the column space of XΩ, let U ∈ Rr×k be a matrix whose columns are the top k eigenvectors of the matrix (P⊤X)(P⊤X)⊤, and let Q = PU ∈ Rn×k. Then,\nE‖QQ⊤X −X‖F ≤ 2‖X −Xk‖F . (5)\nLet X ∈ Rn×m be a matrix whose columns are the vectors x1, . . . , xm. Based on Lemma 3, we produce a matrix Q ∈ Rn×k which satisfies the inequality E[‖QQ⊤X−\n1See also [23][Lemmas 4.1,4.2]. In particular, the elements of Ω can be drawn either according to be i.i.d. N (0, 1/r) or zero-mean ±1 random variables. Also, the bounds on the lower dimension in [23] are better in (additive) factor k log k.\nX‖F ] ≤ 2‖X −Xk‖F . Let C̃ = Q⊤CQ. Our sketched conditioner is determined by the matrix Q and the matrix B = C̃1/2. As we show in Algorithm 1, we can compute a factored form of the inverse of the conditioner, Ã−1, in time O(mnk). We turn to\nAlgorithm 1 Sketched Conditioning: Preprocessing\nInput: X ∈ Rn,m , Parameters: k < n, r ∈ Θ(k) Output: Q, B−1, a−1 that determines a conditioner according to Equation (4) Sample each element of Ω ∈ Rm×r i.i.d. from N (0, r−1) Compute Z = XΩ # in time O(mnr) [P,∼] = QR(Z) # in time O(r2n) Compute Y = P⊤X # in time O(mnr) Compute the SVD: Y = U ′Σ′V ′⊤ # in time O(mr2) Compute Q = PU ′k # in time O(nrk) Compute C̃ = Q⊤CQ # in time O(mkn) Compute B−1 = C̃−1/2 # in time O(k3) Compute a−1 = √\nn−k tr(C)−tr(C̃) # in time O(mn+ k)\ndiscuss the performance of this conditioner. Relying on Lemma 3, we start by relating the trace of C̃ = Q⊤CQ to the trace of C.\nLemma 4. We have tr(C)− tr(C̃) ≤ 4(tr(C)− tr(Ck)).\nThe next lemma holds for any choice of Q ∈ Rn×k with orthonormal columns.\nLemma 5. Assume that C is of rank at least k. Let Q ∈ Rn×k with orthonormal columns and define C̃ = Q⊤CQ⊤, B = C̃1/2. Then, tr(B) = tr(B−1C̃) = O(tr(C 1/2 k )).\nCombining the last two lemmas with Theorem 2, we conclude:\nTheorem 4. Consider running SCSGD with the conditioner given in Algorithm 1. Then,\nE[L(W̄ )− L(W ⋆)] ≤ O ( σρ√ T · ( tr(C 1/2 k ) + √ (n− k)(tr(C)− tr(Ck)) ) ) .\nIn particular, if the fast spectral decay property holds, i.e., √ (n− k)(tr(C)− tr(Ck)) = O(tr(C1/2)), then the obtained bound is of the same order as the bound in Theorem 1."
    }, {
      "heading" : "3 Experiments with Deep Learning",
      "text" : "While our theoretical guarantees were derived for convex problems, the conditioning technique can be adapted for deep learning problems, as we outline below.\nA feedforward deep neural network is a function f that can be written as a composition f = f1 ◦ f2 ◦ . . . ◦ fq, where each fi is called a layer function. Some of the layer functions are predefined, while other layer functions are parameterized by\nweights matrices. Training of a network amounts to optimizing w.r.t. the weights matrices. The most popular layer function with weights is the affine layer (a.k.a. “fully connected” layer). This layer performs the transformation y = Wx+b, where x ∈ Rn, W ∈ Rp,n, and b ∈ Rp. The network is usually trained based on variants of stochastic gradient descent, where the gradient of the objective w.r.t. W is calculated based on the backpropagation algorithm, and has the form δx⊤, where δ ∈ Rp.\nTo apply conditioning to an affine layer, instead of the vanilla SGD update W = W −ηδx⊤, we can apply a conditioned update of the form W = W −ηδ(A−1x)⊤. To calculate A we could go over the entire training data and calculate C = 1m ∑m i=1 xix ⊤ i . However, unlike the convex case, now the vectors xi are not constant but depends on weights of previous layers. Therefore, we initialize C = I and update it according to the update rule C = (1 − ν)C + νxix⊤i . for some ν ∈ (0, 1). From time to time, we replace the conditioner to be A = C1/2 for the current value of A. In our experiments, we updated the conditioning matrix after each 50s iterations. Note that the process of calculating A = C1/2 can be performed in a different thread, in parallel to the main stochastic gradient descent process, and therefore it causes no slowdown to the main stochastic gradient descent process.\nThe same technique can be applied to convolutional layers (that also have weights), because it is possible to write a convolutional layer as a composition of a transformation called “Im2Col” and a vanilla affine layer. Besides these changes, the rest of the algorithm is the same as in the convex case.\nBelow we describe two experiments in which we have applied conditioning technique to a popular variant of stochastic gradient descent. In particular, we used stochastic gradient descent with a mini-batch of size 64, a learning rate of ηt = 0.01(1 + 0.0001t)−3/4, and with Nesterov momentum with parameter 0.9, as described in [20]. To initialize the weights we used the so-called Xavier method, namely, chose each element ofW at random according to a uniform distribution over [−a, a], with a = √\n3/n. We chose these parameters because they are the default in the popular Caffe library (http://caffe.berkeleyvision.org), without attempting to tune them. We conducted experiments with the MNIST dataset [8] and with the Street View House Numbers (SVHN) dataset [12].\nMNIST: We used a variant of the LeNet architecture [7]. The input is images of 28 × 28 pixels. We apply the following layer functions: Convolution with kernel size of 5 × 5, without padding, and with 20 output channels. Max-pooling with kernel size of 2 × 2. Again, a convolutional and pooling layers with the same kernel sizes, this time with 50 output channels. Finally, an affine layer with 500 output channels, followed by a ReLU layer and another affine layer with 10 output channels that forms the prediction. In short, the architecture is: conv 5x5x20, maxpool 2x2, conv 5x5x50, maxpool 2x2, affine 500, relu, affine 10.\nFor training, we used the multiclass log loss function. Figure 2 and Figure 3 show the training and the test errors both w.r.t. the multiclass log loss function and the zeroone loss (where the x-axis corresponds to the number of iterations). In both graphs, we can see that SCSGD enjoys a much faster convergence rate.\nSVHN: In this experiment we used a much smaller network. The input is images of size 32×32 pixels. Using the same terminology as above, the architecture is now: conv 5x5x8, relu, conv 5x5x16, maxpool 2x2, conv 5x5x16, maxpool 2x2, affine 32, relu, affine 32, relu, affine 10, avgpool 4x4. The results are summarized in the graphs of Figure 4 and Figure 5. We again see a superior convergence rate of SCSGD relatively to SGD."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."
    }, {
      "heading" : "A Proofs Omitted from The Text",
      "text" : "Proof. (of Lemma 1) Using the notation from Section 1, denote\n∆t = 1\n2 DA(W\n⋆,Wt)− 1\n2 DA(W\n⋆,Wt+1) .\nAs in the standard proof of SGD (for the Lipschitz case), we consider the progress of Wt towards W ⋆. Recall that Wt+1 = Wt − η∇ℓyit (Wtxit)x⊤itA−1, where it ∈ [m] is the random index that is drawn at time t. For simplicity, denote the p × n matrix ∇ℓyit (Wtxit)x⊤it by Gt. Thus, Wt+1 −Wt = GtA−1. By standard algebraic manipulations we have\n∆t = 1 2 tr(A(W ⋆ −Wt+1)⊤(W ⋆ −Wt+1))− 1 2 tr(A(W ⋆ −Wt)⊤(W ⋆ −Wt))\n= tr((W ⋆ −Wt+1)A(Wt+1 −Wt)⊤) + 1 2 tr((Wt+1 −Wt)⊤A(Wt+1 −Wt)) = tr((W ⋆ −Wt+1)A(−ηGtA−1)⊤) + 1\n2 tr((ηGtA\n−1)A(ηGtA −1)⊤)\n= η · tr((Wt+1 −W ⋆)G⊤t ) + η2\n2 tr(GtA\n−1G⊤t )\n= η · tr(G⊤t (Wt+1 −W ⋆)) + η2\n2 tr(GtA\n−1G⊤t )\n= η · tr(G⊤t (Wt −W ⋆)) + η · tr(G⊤t (Wt+1 −Wt)) + η2\n2 tr(GtA\n−1G⊤t )\n= η · tr(G⊤t (Wt −W ⋆))− η2 · tr(GtA−1G⊤t ) + η2\n2 tr(GtA\n−1G⊤t )\n= η · tr(G⊤t (Wt −W ⋆))− η2\n2 tr(GtA\n−1G⊤t )\n≥ η〈Gt,W ⋆ −Wt+1〉 − ρ2η2\n2 tr(x⊤itA −1xit) .\nwhere in the last inequality we used the fact that the loss function is ρ-Lipschitz. Summing over t and dividing by η we obtain\nT ∑\nt=1\n〈Gt,Wt −W ⋆〉 ≤ 1 2η tr(A(W ⋆ −W1)⊤(W ⋆ −W1)) +\nηρ2\n2 tr(A−1\n∑\nt\nxitx ⊤ it) .\nRecall that W1 = 0. Note that the expected value of Gt is the gradient of L at Wt and the expected value of xitx ⊤ it is C. Taking expectation over the choice of it for all t, dividing by T and relying on the fact that L(Wt)− L(W ⋆) ≤ 〈∇L(Wt),Wt −W ⋆〉, we obtain\nL(W̄ )− L(W ⋆) ≤ 1 2ηT tr(AW ⋆⊤W ⋆) + ηρ2 2 tr(A−1C)\nProof. (of Theorem 1) For simplicity, we assume that C has full rank. If this is not the case, one can add a tiny amount of noise to the instances to make sure that C is of full rank.\nWe would like to optimize tr(A) + tr(A−1C) over all positive definite matrics. Since every matrix A ≻ 0 can be written as A = τM , where M ≻ 0, tr(M) = 1 and τ = tr(A), an equivalent objective is given by\nmin τ>0 min M≻0:\ntr(M)=1\nσ2\n2ηT τ +\nηρ2\n2τ tr(M−1C) . (6)\nThe following lemma characterizes the optimizer.\nLemma 6. Let C ≻ 0. Then,\nmin M≻0:\ntr(M)≤1\ntr(M−1C) = (tr(C1/2))2 ,\nand the minimum is attained by M⋆ = (tr(C1/2))−1 · C1/2. Straightforward optimization over τ yields the value τ = tr(C1/2). Subtituitng τ and M in Equation (6) and applying Lemma 1, we conclude the proof of Theorem 1.\nProof. (of Lemma 6) First, it can be seen that M⋆ is feasible and attains the claimed minimal value. We complete the proof by showing the following inequality for any feasible A: tr(M−1C) ≥ (tr(C1/2))2 . We claim the following analogue of Fan’s inequality: For any symmetric matrix M ∈ R\nn×n, tr(M−1C) ≥ 〈λ↑(M−1), λ↓(C)〉 = 〈(λ↓(M))−1, λ↓(C)〉 ,\nwhere ↓ and ↑ are used to represent decreasing and increasing orders, respectively, and for a vector x = (x1, . . . , xn) with positive components, x−1 = (1/x1, . . . , 1/xn). The equality is clear so we proceed by proving the inequality. Let M be a n × n symmetric matrix. Assume that C =\n∑n i=1 λiuiu ⊤ i and M = ∑n i=1 µiviv ⊤ i are the\nspectral decompositions of C and M , respectively. Letting αi,j = 〈ui, vj〉, we have\ntr(M−1C) = ∑\ni,j\nα2i,jλi/µj .\nNote that since both v1, . . . , vn and u1, . . . , un form orthonormal bases, the matrix Z ∈ Rn×n whose (i, j)-th element is α2i,j is doubly stochastic. So, we have\ntr(M−1C) = λ⊤Zµ−1 .\nViewing the right side as a function of Z , we can apply Birkhoff’s theorem and conclude that the minimum is obtained by a permutation matrix. The claimed inequality follows.\nThus, we next consider the objective\nmin µ∈E\nn ∑\ni=1\nλi/µi ,\nwhere E = {µ ∈ Rn+ : ∑n i=1 µi ≤ 1}. The corresponding Lagrangian2 is\nL(µ;α) =\nn ∑\ni=1\nλi/µi − n ∑\ni=1\nαiµi + αn+1(\nn ∑\ni=1\nµi − 1) .\nNext, we compare the differential to zero and rearrange, to obtain\n(λi/µ 2 i ) n i=1 = (αn+1 − αi)ni=1 .\nBy complementary slackness, α1 = . . . = αn = 0. Thus,\nµ2i = cλ 1/2 i ,\nfor some c > 0. The constraint ∑ i=1 µi ≤ 1 implies that c = ∑ i=1 λ 1/2 i . Substituting the minimizer in the objective, we conclude the proof.\nProof. (of Lemma 2) Since B = EE⊤ for some matrix E, it follows that QBQ⊤ = QE(QE)⊤, thus it is positive semidefinite. The matrix a(I −QQ⊤) is clearly positive semidefinite. It remains to show that A is invertible and thus it is positive definite. We have\nAA−1 = (QBQ⊤ + a(I −QQ⊤))(QB−1Q⊤ + 1 a (I −QQ⊤))\n= QBQ⊤QB−1Q⊤ + (I −QQ⊤)(I −QQ⊤) + 0 + 0 = QQ⊤ + I −QQ⊤ = I .\nProof. (of Theorem 2) Recall that\nA = QBQ⊤ + a(I −QQ⊤) .\nAccording to Lemma 1, we need to show that\ntr(A) + tr(A−1C) ≤ tr(B) + tr(B−1C̃) + 2 √ (n− k)(tr(C)− tr(C̃)) .\nSince the trace is invariant to cyclic permutations, we have\ntr(A) = tr(QBQ⊤) + a · tr(I −QQ⊤) = tr(Q⊤QB) + a(n− k) = tr(B) + a(n− k) .\n2The strict inequalities µi > 0 are not allowed, but we can replace them with weak inequalities and let f(µ) = ∞ for any µ whose one of its components is not greater than zero\nUsing Lemma 2, we obtain\ntr(A−1C) = tr(QB−1Q⊤C) + a−1 · tr((I −QQ⊤)C) = tr(B−1Q⊤CQ) + a−1(tr(C) − tr(QQ⊤C)) = tr(B−1C̃) + a−1(tr(C)− tr(C̃)) .\nSubtituting a = √\ntr(C)−tr(C̃) n−k , we complete the proof.\nProof. (of Theorem 3) Note that\nC̃ = Q⊤CQ = U⊤k UDU ⊤Uk = UkDkUk) = Ck .\nB = C̃1/2 = C 1/2 k .\nB−1C̃ = C̃−1/2C̃ = C̃1/2 = C1/2k .\nInvoking Theorem 2, we obtain the desired bound.\nProof. (of Lemma 4) With a alight abuse of notation, we consider the decomposition C = Ck + Cn−k (here, Cn−k corresponds to the last n− k eigenvalues rather than to the first n− k eigenvalues). We need to show that\ntr(C̃) ≥ tr(Ck)− 3tr(Cd−k) .\nLet X̄ = 1√ m X , where X ∈ Rn×m is the matrix whose columns are x1, . . . , xm. Note that C = X̄X̄⊤. Also, since Q satisfies Equation (5) w.r.t. X , it satisfies the same inequality w.r.t. X̄ . Let X̄ = UΣV ⊤ be the SVD of X . Note that the same matrix U participates in the SVD (EVD) of the matrix C = X̄X̄⊤, i.e., we have C = UDU⊤, where D = Σ2. Recall that the best rank-k approximation of X̄ is UkU ⊤ k X̄ = UkΣkV ⊤ k . By assumption,\n‖QQ⊤X̄ − X̄‖2F ≤ 4‖UkU⊤k X̄ − X̄‖2F (7)\nNote that\n‖X̄ −QQ⊤X̄‖2F = tr(X̄⊤X̄) + tr(X̄⊤QQ⊤QQ⊤X̄)− 2tr(X̄⊤QQ⊤X̄) = tr(C)− tr(QQ⊤C) = tr(C) − tr(C̃) .\nSimilarly,\n‖UkU⊤k X̄ − X̄‖2F = tr(C)− tr(UkU⊤k C) = tr(C) − tr(Ck) .\nThus, Equation (7) implies that\ntr(C)− tr(C̃) ≤ 4(tr(C) − tr(Ck)) .\nHence, tr(C̃) ≥ 4tr(Ck)− 3tr(C) = tr(Ck)− 3tr(Cd−k) .\nProof. (of Lemma 5) First, we note that B = B−1C̃ = C̃1/2. Thus, we need to show that tr(C̃1/2) = O(tr(C1/2k )). Second, we observe that for every positive scalar b, we have\ntr(C̃1/2) = O(tr(C 1/2 k )) ⇔ tr(bC̃1/2) = O(tr(bC 1/2 k )) .\nDenote the k top eigenvalues of C and C̃ by λ1, . . . , λk and λ̃1, . . . , λ̃k , respectively. According to the above observation, we may assume w.l.o.g. that λi ≥ 1 for all i ∈ [k] (simply consider b = λ−1k ).\nLet X̄ = UΣV ⊤ be the SVD of X̄ , where X̄ = (1/ √ m)X . Since UkU⊤k X̄ is the\nbest rank-k approximation of X̄ , we have\n‖UkU⊤k X̄ − X̄‖2F ≤ ‖QQ⊤X̄ − X̄‖2F\nfor all Q ∈ Rn×k with orthonormal columns. As in the proof of Lemma 4, this implies that tr(Ck) ≥ tr(C̃) . Therefore,\ntr(C̃1/2)− tr(C1/2k ) = k ∑\ni=1\n(\n√\nλ̃i − √ λi) =\nk ∑\ni=1\nλ̃i − λi √\nλ̃i + √ λi\n≤ k ∑\ni=1\nλ̃i − λi\n= tr(C̃)− tr(Ck)) ≤ 0\nwhere the first inequality follows from the assumption that λi ≥ 1 for all i ∈ [k]."
    } ],
    "references" : [ {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "Shun-Ichi Amari" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1998
    }, {
      "title" : "Adaptive method of realizing natural gradient learning for multilayer perceptrons",
      "author" : [ "Shun-Ichi Amari", "Hyeyoung Park", "Kenji Fukumizu" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2000
    }, {
      "title" : "Improving the convergence of back-propagation learning with second order methods",
      "author" : [ "Sue Becker", "Yann Le Cun" ],
      "venue" : "In Proceedings of the 1988 connectionist models summer school,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1988
    }, {
      "title" : "Practical recommendations for gradient-based training of deep architectures",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "In Neural Networks: Tricks of the Trade,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Sgd-qn: Careful quasinewton stochastic gradient descent",
      "author" : [ "Antoine Bordes", "Léon Bottou", "Patrick Gallinari" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1998
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "Yann LeCun", "Corinna Cortes" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "Deep learning via hessian-free optimization",
      "author" : [ "James Martens" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Exact calculation of the product of the hessian matrix of feedforward network error functions and a vector in 0 (n) time",
      "author" : [ "Martin F Møller" ],
      "venue" : "DAIMI Report Series,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1993
    }, {
      "title" : "Introductory lectures on convex optimization, volume 87",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2004
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng" ],
      "venue" : "In NIPS workshop on deep learning and unsupervised feature learning,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Revisiting natural gradient for deep networks",
      "author" : [ "Razvan Pascanu", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1301.3584,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Fast exact multiplication by the hessian",
      "author" : [ "Barak A Pearlmutter" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1994
    }, {
      "title" : "Topmoumoute online natural gradient algorithm",
      "author" : [ "Nicolas L Roux", "Pierre-Antoine Manzagol", "Yoshua Bengio" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Partial bfgs update and efficient step-length calculation for three-layer neural networks",
      "author" : [ "Kazumi Saito", "Ryohei Nakano" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1997
    }, {
      "title" : "Improved approximation algorithms for large matrices via random projections",
      "author" : [ "Tamas Sarlos" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2006
    }, {
      "title" : "A stochastic quasi-newton method for online convex optimization",
      "author" : [ "Nicol Schraudolph", "Jin Yu", "Simon Günter" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Fast curvature matrix-vector products for second-order gradient descent",
      "author" : [ "Nicol N Schraudolph" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2002
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Krylov subspace descent for deep learning",
      "author" : [ "Oriol Vinyals", "Daniel Povey" ],
      "venue" : "arXiv preprint arXiv:1111.4259,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Backpropagation: Past and future",
      "author" : [ "Paul J Werbos" ],
      "venue" : "In Neural Networks,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "For example, the R{·} operator technique [14, 22, 10, 9].",
      "startOffset" : 41,
      "endOffset" : 56
    }, {
      "referenceID" : 21,
      "context" : "For example, the R{·} operator technique [14, 22, 10, 9].",
      "startOffset" : 41,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "For example, the R{·} operator technique [14, 22, 10, 9].",
      "startOffset" : 41,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "For example, the R{·} operator technique [14, 22, 10, 9].",
      "startOffset" : 41,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "An obvious way to decrease the storage and computational cost is to only consider the diagonal elements of the Hessian (see [3]).",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "Schraudolph [18] proposed an adaptation of the L-BFGS approach to the online setting, in which at each iteration, the estimation of the inverse of the Hessian is computed based on only the last few noisy gradients.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 4,
      "context" : "In [5], the two aforementioned approaches are combined to yield the SGD-QN algorithm.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : "There are various other approximations, see for example [16, 4, 21, 15].",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "There are various other approximations, see for example [16, 4, 21, 15].",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : "There are various other approximations, see for example [16, 4, 21, 15].",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : "There are various other approximations, see for example [16, 4, 21, 15].",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "A somewhat related approach is Amari’s natural gradient descent [1, 2].",
      "startOffset" : 64,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "A somewhat related approach is Amari’s natural gradient descent [1, 2].",
      "startOffset" : 64,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "See the discussion in [13].",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "A general treatment of this approach is described in [11][Section 1.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "Maybe the most relevant approach is the Adagrad algorithm [6], which was originally proposed for the online learning setting but can be easily adapted to the stochastic optimization setting.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "The above intuition is formalized by the following result, which follows from [17] by setting ǫ = 11.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "9, as described in [20].",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "We conducted experiments with the MNIST dataset [8] and with the Street View House Numbers (SVHN) dataset [12].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "We conducted experiments with the MNIST dataset [8] and with the Street View House Numbers (SVHN) dataset [12].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "MNIST: We used a variant of the LeNet architecture [7].",
      "startOffset" : 51,
      "endOffset" : 54
    } ],
    "year" : 2015,
    "abstractText" : "We propose a novel method for speeding up stochastic optimization algorithms via sketching methods, which recently became a powerful tool for accelerating algorithms for numerical linear algebra. We revisit the method of conditioning for accelerating first-order methods and suggest the use of sketching methods for constructing a cheap conditioner that attains a significant speedup with respect to the Stochastic Gradient Descent (SGD) algorithm. While our theoretical guarantees assume convexity, we discuss the applicability of our method to deep neural networks, and experimentally demonstrate its merits.",
    "creator" : "LaTeX with hyperref package"
  }
}