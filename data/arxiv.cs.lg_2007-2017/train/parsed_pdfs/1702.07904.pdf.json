{
  "name" : "1702.07904.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Coarse Grained Exponential Variational Autoencoders",
    "authors" : [ "Ke Sun", "Xiangliang Zhang" ],
    "emails" : [ "sunk@ieee.org", "xiangliang.zhang@kaust.edu.sa" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al., 1999) with strong generalization and a standard learning objective, and deep learning with flexible and scalable representations. They are attracting decent attentions, producing state-of-the-art performance in semi-supervised learning (Kingma et al., 2014) and image generation (Gregor et al., 2015), and are getting applied in diverse areas such as deep generative modeling (Rezende et al., 2014), image segmentation (Sohn et al., 2015), clustering (Dilokthanakul et al., 2017), and future prediction from images (Walker et al., 2016).\nThis paper discusses unsupervised learning with VAE which pipes an inference model q(z |x) with a generative model p(x | z), where x and z are observed and latent variables, respectively. A simple parameter-free prior p(z) combined with p(x | z) parameterized by a deep neural network results in arbitrarily flexible representations. However, its (very complex) posterior p(z |x) must be within the representation power of the inference machine q(z |x), so that the variational bound is tight and variational learning is effective.\nIn the original VAE (Kingma & Welling, 2014), q(z |x) obeys a Gaussian distribution with a diagonal covariance matrix. This is a very simplified assumption, because Gaussian is the maximum entropy (least informative) distribution with respect to prescribed mean and variance and has one single mode, while human inference can be ambiguous and can have a bounded support when we exclude very unlikely cases (de Haan & Ferreira, 2006).\nMany recent works try to tackle this limitation. Jang et al. (2017) extended VAE to effectively use a discrete latent z following a category distribution (e.g. Bernoulli distribution). Kingma et al. (2014) extended the latent structure with a combination of continuous and discrete latent variables (class labels)\nar X\niv :1\n70 2.\n07 90\n4v 1\n[ cs\n.L G\n] 2\n5 Fe\nb 20\n17\nand applied the model into semi-supervised learning. Similarly, Shu et al. (2016) and Dilokthanakul et al. (2017) proposed to use a Gaussian mixture latent model in VAE. Serban et al. (2016) applied a piecewise constant distribution on z.\nThis work contributes a new ingredient in VAE model construction. To tackle the difficulty in dealing with complex probability density function (pdf) p(z) (z ∈ Z), we generate instead a semi-continuous z ∈ Z , by first discretizing the support Z into a grid, then drawing a discrete sample y based on the corresponding probability mass function (pmf), and then reconstruct y into z ∈ Z . This coarse grain (CG) technique can help apply any pdf into VAE. Hence we apply a bounded polynomial exponential family (BPEF) as the underlying p(z), which is a universal pdf generator. This fits in the spirit of neural networks because the prior and posterior are not hand-crafted but learned by themselves.\nThis contribution blends theoretical insights with empirical developments. We present CG, BPEF, information monotonicity, etc., that are useful ingredients for general VAE modeling. Notably, we present a novel application scenario with new analysis on the Gumbel softmax trick (Jang et al., 2017; Maddison et al., 2016). We assemble these components into a machine CG-BPEF-VAE and present empirical results on unsupervised density estimation, showing improvements over vanilla VAE (Kingma & Welling, 2014) and category VAE (Jang et al., 2017). We present a novel perspective with theoretical analysis of VAE learning, with guaranteed bounds derived from information geometry (Amari, 2016).\nThis paper is organized as follows. Sec. 2 reviews the basics of VAE. Sec. 3 introduces CG-VAE and its implementation CG-BPEF-VAE. Sec. 4 performs an empirical study on two different datasets. Sec. 5 gives a theoretical analysis on VAE learning. Sec. 6 states our concluding remarks."
    }, {
      "heading" : "2 Prerequisites: Variational Autoencoders",
      "text" : "This section covers the basics from a brief introduction of variational Bayes to previous works on VAE. A generative model can be specified by a joint distribution between the observables x and the hidden variables z, that is, p(x, z |θ) = p(z |θz)p(x | z,θx|z) where θ = (θz,θx|z). By Jensen’s inequality,\n− log p(x |θ) = − log ∫ q(z |x,ϕ) p(x, z |θ)\nq(z |x,ϕ) dz ≤ ∫ q(z |x,ϕ) log q(z |x,ϕ) p(x, z |θ) dz ( def = L(θ,ϕ) ) , (1)\nfor any q(z |x,ϕ). The upper bound L(θ,ϕ) on the RHS is known as the “variational free energy”. We have\nL(θ,ϕ) = KL(q(z |x,ϕ) : p(z |θz))︸ ︷︷ ︸ term1\n− ∫ q(z |x,ϕ) log p(x | z,θx|z)dz︸ ︷︷ ︸\nterm2\n,\nwhere KL(· : ·) denotes the Kullback-Leibler (KL) divergence. (We will use term1 and term2 as shorthands for the two terms whose sum is L(θ,ϕ). One has to remember that they are functions of θ and ϕ.) We therefore minimize the free energy with respect to both θ and ϕ so as to minimize − log p(x |θ). The gap of the bound in eq. (1) is L(θ,ϕ) − (− log p(x |θ)) = KL (q(z |x,ϕ) : p(z |x,θ)), which can be small as long as the parameter manifold of q(z |x,ϕ) (e.g. constructed based on the mean field technique, Jordan et al. 1999) encompasses a good estimation of the true posterior.\nVAE (Kingma & Welling, 2014) assume the following generative process. The prior p(z |θz) = G(z |0, I) is parameter free, where G(· |µ,Σ) denotes a Gaussian distribution with mean µ and covariance matrix Σ. Denote dimx = D and dim z = d. The conditional mapping p(x | z,θ) =\n∏D i=1 p (xi | f(z,θ)) is parametrized by a neural network f(z,θ) with input z and parameters θ. For binary x, p(xi | ·) is a Bernoulli distribution; for continuous x, p(xi | ·) can be univariate Gaussian. This gives a very flexible p(x |θ) to adapt the complex data manifold.\nIn this case, it is hard to select the parameter form of q(z |x,ϕ), as the posterior p(z |x,ϕ) has no closed form solution. VAE borrows again the representation power of neural networks and lets q(z |x,ϕ) = G(z |µ(x,ϕ), diag(λ(x,ϕ))), where µ(x,ϕ) and λ(x,ϕ) are both neural networks with input x and parameters ϕ, and diag(·) means a diagonal matrix constructed with a given diagonal vector. The assumption of a diagonal covariance is for reducing the network size so as to be efficient and to control overfitting.\nSince the KL of Gaussians is available in closed form, term1 has an analytical solution. In order to solve the integration in term2, VAE employs a reparameterization trick. It draws L i.i.d. samples 1, · · · , L ∼ G( |0, I), where I is the identity matrix. Let zl = µ(x,ϕ) + λ(x,ϕ) ◦ l, where “◦” denotes element-wise product. Then zl ∼ G(z |µ(x,ϕ), diag(λ(x,ϕ))). Hence\nterm2 ≈ − 1\nL L∑ l=1 log p ( x |µ(x,ϕ) + λ(x,ϕ) ◦ l,θ ) .\nThis trick allows error to backpropagate through the random mapping (µ,λ) z. Then L(θ,ϕ) = term1 + term2 can be expressed as simple arithmetic operations of the outputs of the hidden layer and the last layer. It can therefore be optimized e.g. with stochastic gradient descent. The optimization technique is called stochastic gradient variational Bayes (SGVB). The resulting architecture is presented in fig. 1a."
    }, {
      "heading" : "3 CG-BPEF-VAE",
      "text" : "We would like to extend VAE to incorporate a general inference process, where the model can learn by itself a proper p(z |x,ϕ) within a flexible family of distributions, which is not limited to Gaussian or category distributions and can capture higher order moments of the posterior. We will therefore derive in this section a variation of VAE called CG-BPEF-VAE for Coarse-Grained Bounded Polynomial Exponential Family VAE."
    }, {
      "heading" : "3.1 Bounded Polynomial Exponential Family",
      "text" : "We try to model the latent z with a factorable polynomial exponential family (PEF) (Cobb et al., 1983; Nielsen & Nock, 2016) probability density function:\np(z) = d∏ j=1 exp ( M∑ m=1 cjmz m j − ψ(cj) ) , (2)\nwhereM is the polynomial order,C = (cjm)d×M denotes the polynomial coefficients, and ψ is a convex cumulant generating function (Amari, 2016). This PEF family can be regarded as the most general parameterization, because with large enough M it can approximate arbitrary finely any given p(z) satisfying weak regularity conditions (Cobb et al., 1983).\nFurthermore, we constrain z to have a bounded support so that z ∈ [−1, 1]d, a hypercube. This gives z a focused density that is not wasted on unlikely cases, which is in contrast to Gaussian distribution with non-zero probability on the whole real line. This also allows one to easily explore extreme cases by setting zj to ±1 or beyond.\nFor example, if M = 2, then the resulting p(zj) ∝ exp ( cj1zj + cj2z 2 j ) includes the truncated Gaussian distribution (with one mode) as a special case when cj2 < 0. Moreover, the setting cj2 ≥ 0 encompasses more general cases and can have at most two modes.\nThe two important elements in constructing a VAE model are À the KL divergence between q(z |x,ϕ) and p(z |θz) must have a closed form; Á a random sample of q(z |x,ϕ) can be expressed as a simple function between its parameters and some parameter-free random variables. Neither of these conditions are met for BPEF. We will address these difficulties in the remainder of this section."
    }, {
      "heading" : "3.2 Coarse Grain",
      "text" : "Our basic idea is to reduce the BPEF pdf into a discrete distribution, then draw samples based on the pmf, then reconstruct the continuous sample.\nWe sample R points uniformly on the interval [−1, 1]:\nζ = ( −1,−1 + 2\nR− 1 , · · · , 1− 2 R− 1 , 1\n)ᵀ ,\nwhere the r’th discrete value is ζr = 2r−(R+1) R−1 . For example, choosing R = 21 results in a precision of 0.1. In correspondence to these R locations, we assume for the j’th latent dimension a random yj in ∆R−1, the (R− 1)-dimensional probability simplex, so that ∑R r=1 yjr = 1, ∀r, yjr ≥ 0. This yjr means the likelihood for zj taking the value ζr. Intuitively, if we constrain yj to be one-hot (with probability mass only on vertices of ∆R−1), and let P (yjr = 1) ∝ exp( ∑M m=1 cjmζ\nm r ), then the expectation zj =∑R\nr=1 yjrζr ∈ [−1, 1] will be distributed like the BPEF in eq. (2). However, to apply the reparameterization trick, it is not known how to express a random one-hot sample yj as a simple function of the activation probabilities. Nor does Dirichlet distribution as a commonlyused density on ∆R−1 can do the trick.\nThis reparameterization problem of category distribution is studied recently (Jang et al., 2017; Maddison et al., 2016) following earlier developments (Kuzmin & Warmuth, 2005; Maddison et al., 2014) on applying extreme value distributions (de Haan & Ferreira, 2006) to machine learning. Based on these previous studies, we let yj follow a Concrete distribution (Maddison et al., 2016), which is a continuous relaxation of the category distribution, with the key advantage that Concrete samples can be easily drawn to be applied to VAE. Details are explained as follows.\nThe standard Gumbel distribution (Gumbel, 1954) is defined on the support g ∈ < with the cumulative distribution function P (g ≤ x) = e−e−x . Therefore Gumbel samples can be easily obtained by inversion sampling g = − log(− logU), where U is uniform on (0, 1). Let gjr follows standard Gumbel distribution, then the random variable yj ∈ ∆R−1 defined by\nyjr = exp ((gjr + φjr)/T )∑R r=1 exp ((gjr + φjr)/T )\nis said to follow a Concrete distribution with location parameter φj and temperature parameter T : yj ∼ Con(φj , T ). This distribution has a closed-form probability density function (see Maddison et al. 2016)\nand has the following fundamental property ∀r, P (\nlim T→0+ yjr = 1\n) = P (yjr > yjo,∀o 6= r)\n= exp(φjr)/ R∑ r=1 exp(φjr) ( def = αjr ) . (3)\nBasically, at the limit T → 0+, the density will be pushed to the vertices of ∆R−1, and Concrete random vectors yj tend to be onehot, with activation probability of the r’th bit defined by αjr. Hence it can be considered as a relaxation (Maddison et al., 2016) of the category distribution. See fig. 2 for an intuitive view of the Concrete distribution. There are heavy volumes of densities around the vertices.\nIn our case, let φjr = ∑M m=1 cjmζ m r , then the odds for yjr activated (i.e., the probability for zj taking\nthe value ζr) will be proportional to exp (∑M m=1 cjmζ m r ) at the limit T → 0+. This provides a way to simulate the BPEF density."
    }, {
      "heading" : "3.3 The Model",
      "text" : "Based on previous subsections, we assume the following generation process\nαjr = M∑ m=1 ajmζ m r , p(y |a) = d∏ j=1 Con (yj |αj , T ) ,\nzj(a) = y ᵀ j ζ, p(x | z,θ) = D∏ i=1 p (xi | f (z,θ)) ,\nwhere A = (ajm)d×M is the parameters of the prior1, and f is defined by a neural network. One should always chooseM < R−1, because the polynomial ∑R−1 m=1 cjmζ m r withR−1 free parameters can already represent any distribution in ∆R−1. The setting M ≥ R− 1 makes the polynomial structure redundant. The corresponding inference process is given by\nβjr = M∑ m=1 bjm(x,ϕ)ζ m r ,\n1Strictly speaking the prior distribution only contains hyper-parameters that are set a priori. Here the term “prior” is more like a prior structure with learned parameters.\nq(y |x,ϕ) = d∏ j=1 Con (yj |βj , T ) ,\nzj(x,ϕ) = y ᵀ j ζ,\nwhere B(x,ϕ) = (bjm(x,ϕ))d×M is defined by a neural network. By Monte Carlo integration, it is straightforward that\nterm2 ≈ 1\nL L∑ l=1 D∑ i=1 log p\n( xi | f ( R∑ r=1 yl•rζr,θ )) ,\nyljr = exp\n( (gljr + ∑M m=1 bjm(x,ϕ)ζ m r )/T ) ∑R r=1 exp ( (gljr + ∑M m=1 bjm(x,ϕ)ζ m r )/T\n) , where (gljr) is a 3D tensor of independent Gumbel variables, and the approximation becomes accurate when L→∞.\nFor simplicity, we assume T to be the same scalar during generation and inference. We adopt a simple annealing process of T , starting from Tmax, exponentially decaying to Tmin in the first half of training epochs, then keeping Tmin. The study (Jang et al., 2017) implies that Tmin = 0.5 ∼ 1 could be small enough to make the Concrete distribution approximate well a category distribution. The setting of Tmin will affect the computation of term1, which will be explained in the following subsection."
    }, {
      "heading" : "3.4 Information Mononicity",
      "text" : "We need to compute term1 which is the KL divergence between the posterior p(z |x,ϕ) the prior p(z |a). This is the most complex part because these pdfs are not in closed form. However, we know that as T → 0+ they converge to categories distributions over R evenly spanned positions on [−1, 1] (the vector ζ). Therefore we approximate term1 with the KL divergence between the corresponding category distributions, that is,\nterm1 ≈ d∑ j=1 R∑ r=1 [ exp(βjr)∑R r=1 exp(βjr)\n× log exp(βjr)/(\n∑R r=1 exp(βjr))\nexp(αjr)/( ∑R r=1 exp(αjr))\n]\n= d∑ j=1 [∑R r=1 exp(βjr)(βjr − αjr)∑R r=1 exp(βjr)\n+ log R∑ r=1 exp(αjr)− log R∑ r=1 exp(βjr) ] . (4)\nIn the rest of this subsection we give theoretical and empirical justifications of this approximation. KL divergence belongs to Csiszár’s f -divergence family and therefore satisfy the well-known information monotonicity (Amari, 2016). Basically, the support V can be partitioned into subregions {Vr} with zero volume overlap, so that V = ]Vr. Denote by p1(Vr) = ∫ x∈Vr p1(x)dx the probability mass of Vr, then∑\nr p1(Vr) = 1 and the pmf {p1(Vr)} is a coarse grained version of p1(x). The information monotonicity principle states that KL(p1 : p2) ≥ ∑ r p1(Vr) log p1(Vr) p2(Vr)\n. See (Nielsen & Sun, 2016) for an analysis. Based on this principle, we have the following result.\nTheorem 1.\nÀ KL(q(y |x,ϕ) : p(y |a)) ≥ KL(q(z |x,ϕ) : p(z |a)) = term1;\nÁ KL(q(y |x,ϕ) : p(y |a)) is also lower bounded by the discrete KL given by the right hand side of eq. (4).\nBy theorem 1, the KL between two Concrete distributions are lower bounded by À KL between the dimension reduced z (the exact value of term1); Á KL between the corresponding category distributions (our approximation of term1). If one uses Concrete latent variable and uses the category KL as term1 (e.g. in Category VAE, see fig. 1b), this is equivalent to minimizing a lower bound of the free energy, which is not ideal because such learning has less control over the free energy. In contrast, CG-BPEFVAE has a reconstruction layer y → z (see fig. 1c), which reduces the number of dimensions by a factor of R (e.g. in our experiments R ≈ 100). By theorem 1 À, this effectively reduces the KL divergence between the latent posterior and the latent prior. Intuitively, we can expect term1 to be much smaller than KL(q(y |x,ϕ) : p(y |a)) and by minimizing the category KL, we have more faith to bring down term1 rather than KL(q(y |x,ϕ) : p(y |a)).\nHow good is our approximation in eq. (4)? Unfortunately we do not have theoretically guaranteed bounds. Therefore we fall back to an empirical study. We generate category samples α ∈ ∆99, then generate the corresponding Gumbel distribution y, then reduce the dimensionality by z = yᵀζ. Figure 3 shows the KL(α : Uniform) (our approximation) and KL(p(z) : Uniform) (the true latent KL). We repeat 100 experiments for each of two different α generator: a high entropy uniform generator over ∆99, and a low entropy generator based on a Dirichlet distribution with shape parameter α = 0.5. (In practice we expect a low entropy posterior which is close to the latter case). The results suggest that our approximation is roughly an upper bound of the true KL divergence between latent distributions on small temperatures. Therefore we can expect that minimizing L(θ,ϕ) based on eq. (4) will bring down the free energy. See the appendix for more empirical study. A theoretical analysis is left to future work.\nEssentially term1 serves as a regularizor, constraining p(z |x,ϕ) to have enough entropy to respect a common p(z) that does not vary with different samples. An approximated term1 is acceptable in many cases, because one can add a regularization strength parameter to tune the model (e.g. based on validation)."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "We implemented the proposed method using TensorFlow (Abadi, Martı́n et al., 2015) and tested it on two different datasets. The MNIST dataset (LeCun et al.) consists of 70,000 gray scale images of handwritten digits, each of size 28 × 28. The training/validation/testing sets are split according to the ratio 11 : 1 : 2. The SVHN dataset (Netzer et al., 2011) has around 100,000 gray-scale pictures (for simplicity the original 32× 32× 3 RGB images are reduced into 32× 32× 1 by averaging the 3 channels) of door numbers with a train/valid/test split of 10 : 1 : 3.5. These pictures are centered by cropping from real street view images.\nWe only investigate unsupervised density estimation. It is nevertheless meaningful to have unsupervised VAE results on the selected datasets for future references. We compare the proposed CG-BPEFVAE with vanilla VAE (Gauss-VAE) and Category VAE (Cat-VAE) (Jang et al., 2017). For MNIST, the candidate network shapes are 784-400-(10,20,· · · ,80)-400-784 and 784-400-400-(10,20,· · · ,80)-400- 400-784, equipped with densely connected layers and RELU activations (Nair & Hinton, 2010). For SVHN, the encoder network has 5 convolutional layers with fixed size, reducing the images into a 128- dimensional feature space, and a bottleneck layer of size (10,20,30,40,50) (5 different configurations). The decoder network has one RELU layer of size 128, followed by 4 transposed convolutional layers with fixed size. See the appendix for the detailed configurations.\nThe learning rate is γ ∈ {10−4, 5 × 10−4, 10−3, 5 × 10−3}. For Cat-VAE and CG-BPEF-VAE, the initial and final temperature are Tmax ∈ {1, Tmin} and Tmin ∈ {0.5, 0.8}, respectively, with a simple exponential annealing scheme. For Cat-VAE, the number of categories is C ∈ {5, 10, 15, 20}. For CGBPEF-VAE, we set the polynomial order M ∈ {5, 10, 15, 20}, and the precision R ∈ {51, 101}. The mini-batch size is fixed to 100. The maximum number of mini-batch iterations is 10,000. For all methods we adopt the Adam optimizer (Kingma & Ba, 2014) and the Xavier initialization (Glorot & Bengio, 2010), which are commonly recognized to bring improvements.\nThe performance is measured by the per-sample average free energyL(ϕ,θ). The best model with the\nsmallest validated L is selected. Then the we report its L on the testing set, along with the reconstruction error term2 so that one can tell its trade-off between model complexity (term1) and fitness to the data (term2). See table 1 for the results on two different latent sample size L and two different temperatures Tmin.\nWe clearly see that CG-BPEF-VAE shows the best results. Essentially, Gauss-VAE can be considered as a special case of CG-BEPF-VAE when M = 2 therefore cannot model higher order moments. CatVAE has neither a polynomial exponential structure to regulate the discrete variables, nor a dimensionality reduction layer to reduce the free energy. The good results of CG-BPEF-VAE are expected.\nNotice that as we increase the final temperature Tmin, both Cat-VAE and CG-BPEF-VAE will show “better” results. However, the estimation of the free energy will become more and more inaccurate especially for Cat-VAE, whose estimation is a lower bound of the actual free energy by theorem 1 (2). In high temperature, the free energy can be well above its reported L. In contrast, for CG-BPEF-VAE, its estimated L is an empirical upper bound of the free energy, as long as Tmin is set reasonably small (T = 0.5 ∼ 1, see fig. 3).\nAll models prefer deep architectures over shallow ones. There is a significant improvement of CatVAE and CG-BPEF-VAE when L is increased from 1 to 10, when Cat-VAE starts to prefer larger category numbers. A large sample size L is required to model complex multimodal distributions and is recommended for Cat-VAE and CG-BPEF-VAE. As the size of the decoder network scales linearly with L, one will face significantly higher computation cost during increasing L.\nAs compared to MNIST, SVHN is more difficult to get improved over the baseline results by GaussVAE, because its data manifold is much more complex. One has to incorporate supervised information (Kingma et al., 2014) to achieve better results.\nCat-VAE and CG-BPEF-VAE are more computational costly than Gauss-VAE. In Cat-VAE, the tensor z has a size of batch size× L× d× C. In CG-BPEF-VAE, the tensor y have a size of batch size× L× d×R, although this is immediately reduced to batch size×L× d by the mapping y → z. A high precision CG-BPEF-VAE or a Cat-VAE with a large category number will multiply the computational time. Our implementation is available at https://github.com/sunk/cgvae."
    }, {
      "heading" : "5 Information Geometry of VAE",
      "text" : "This is a relatively separate section. We present a geometric theory which can be useful to uncover the intrinsics of general VAE modeling not limited to the proposed CG-BPEF-VAE, so that one can architect useful VAE models not only based on variational inference, but also along another geometric axis. We also use this geometry to discuss advantages of the proposed CG-BPEF-VAE.\nNotice, this geometry is not about the input feature space or the latent space (space of x and z), but about the models (space of θ and ϕ) or information geometry (Amari, 2016).\nWe will consider the cost function L(θ,ϕ) averaged with respect to i.i.d. observations {xk}nk=1. term1 is the average KL divergence between q(z |xk) and p(z). Assume that both p(z) and q(z |xk) are in the same exponential family M(ϕ) so that p(z) = exp(tᵀ(z)ϕz − F (ϕz)) and q(z |xk) = exp(tᵀ(z)ϕk − F (ϕk)), where t(z) is a vector of sufficient statistics (for example in CG-BPEF-VAE, t(z) = (z, z2, z3, · · · )), and F (ϕ) is a convex cumulant generating function2. This M(ϕ) is a statistical manifold, i.e., space of probability distributions where ϕ serves as a coordinate system. The dual parameters (Amari, 2016) ofM(ϕ), which form another coordinate system, are defined by the moments η = E(t(z)) = ∫ p(z)t(z)dz. These two coordinate systems can be transformed back and forth by the Legendre transformations η = F ′(ϕ), ϕ = I ′(η), where I is Shannon’s information (negative entropy). 2 In this section, we will denote p(z |ϕz) instead of p(z |θz) (as in previous sections) to emphasize that p(z) is in the same statistical manifold with q(z |xk).\nBy straightforward derivations,\nterm1 = 1\nn n∑ k=1 [ I(ηk)− (ηk)ᵀϕz ] + F (ϕz).\nNotice that the prior p(z) only appears in term1 but not in term2. We therefore consider a free p(z) which minimizes term1 with {ϕk}nk=1 fixed. We have\n∂term1 ∂ϕz = − 1 n n∑ k=1 ηk + ∂F (ϕz) ∂ϕz = ηz − 1 n n∑ k=1 ηk.\nTherefore the optimal (ηz)? = 1n ∑n k=1 η\nk is the Bregman centroid (Nielsen & Nock, 2009) of {ϕk}nk=1. Geometrically, term1 is the average divergence between ϕk and the Bregman centroid and therefore measures the n-body compactness of {ϕk}nk=1. We can therefore have a lower bound of term1.\nTheorem 2. Given q(z |xk) in an exponential familyM(ϕ), if p(z) is in the same exponential family, then\nterm1 ≥ 1\nn n∑ k=1 I ( ηk ) − I\n( 1\nn n∑ k=1 ηk\n) ≥ 0, (5)\nwhere the first “=” holds if and only if ηz = 1n ∑n k=1 η\nk. If p(z) is non-parametric (not constrained by any parametric structure), then\nterm1 ≥ 1\nn n∑ k=1 I(ηk)− I(m) ≥ 0, (6)\nwhere m(z) = 1n ∑n k=1 q(z |xk) is a mixture model which is outsideM(ϕ).\nComparatively, the non-parametric lower bound eq. (6) is smaller than the parametric bound eq. (5). However it needs to compute the entropy of mixture models (Nielsen & Sun, 2016), which does not have an analytic solution. Essentially, term1 is related to the convexity of Shannon information. In standard VAE, p(z) is fixed to the standard Gaussian distribution, which is not guaranteed to be the Bregman centroid, and does not activate the lower bound in theorem 2. In CG-BPEF-VAE, term1 is closer to this bound because p(z) is set free in our modeling. This hints that as a future work one can directly replace term1 with the lower bound stated in theorem 2 to avoid the model selection of p(z) and to achieve better performance.\nLet µk = µ(xk,ϕ) and V k = V (xk,ϕ) 0 be the mean and covariance matrix of q(z |xk,ϕ), respectively. A Taylor expansion of log p(xk | z,θ) at z = µk gives\nterm2 ≈ 1\nn n∑ k=1 ∫ q(z |xk,ϕ) [ − log p(xk |µk,θ)\n− (z − µk)ᵀ log p(x k |z,θ)\n∂z +\n1 2 (z − µk)ᵀGθ(µk)(z − µk)\n] dz\n= 1\nn n∑ k=1 [ − log p(xk |µk,θ) + 1 2 tr ( Gθ(µk)V k )] ,\nwhere\nGθ(µk) = − ∂2 log p(xk | z,θ)\n∂z2\n∣∣∣∣ z=µk\nis the observed Fisher information metric (FIM)3 (Amari, 2016) wrt z depending on θ. The approximation is accurate when q(z |xk,ϕ) is Gaussian with vanishing centered-moments of order 3 or above.\nAssuming the inference network is flexible enough, minimizing term2 alone gives µk = (zk)?, V k = 0, where (zk)? = arg maxz log p(x\nk | z, θ) is the maximum likelihood estimation wrt xk. This (zk)? is the latent z learned by a plain autoencoder. Hence term2 measures a dissimilarity between q(z |xk,ϕ) and the Dirac delta distribution δ((zk)?). By theorem 2, we get the following approximation of the variational bound.\nCorollary 3. Assume the inference network is flexible enough. Consider a variation of Gaussian VAE, where both p(z) and q(z |xk) are free Gaussian distributions. The optimal L? is given by\nL? = min {µk,V k,θ}\n1\nn n∑ k=1 [ − log p(xk |µk,θ) + 1 2 tr ( Gθ(µk)V k ) − 1\n2 log |V k|\n] + 1 2 log ∣∣∣V k + µk(µk)ᵀ − µk (µk)ᵀ∣∣∣ ,\nwhere “ ·” means averaging over k = 1, · · · , n. Remark 4. Consider roughly V k ≈ V and V z = µk(µk)ᵀ − µk (µk)ᵀ. The term 12 tr ( Gθ(µk)V ) helps to shrink V towards 0 and lets V respect the data manifold encoded in the spectrum of Gθ(µk). The term − 12 log |V |+ 1 2 log |V + Vz| enlarges V and lets V respect the latent manifold of {µ\nk}. This reveals a fundamental trade-off between fitting the input data and generalizing.\nRemark 5. In Gaussian VAE with L = 1, the term 12 tr ( Gθ(µk)V k ) is inaccurate. Approximating this term can potentially give more effective implementations of VAE.\nRemark 6. Using BPEF for q(z |xk,ϕ) has the advantage that the information preserved in higher order differentiations ∂\nd log p(x | z, θ) ∂zd (d ≥ 3) is captured.\nIn summary, fig. 4 presents the information geometric background of VAE on the statistical manifold M(ϕ) which includes p(z), q(z |xk) and δ((zk)?). Note that δ((zk)?) is along the boundary ofM(ϕ), where the variance is 0. For example, a Gaussian distribution G(µ, σ) with σ → 0 becomes δ(µ). {δ((zk)?)} vary according to maximum likelihood learning along another statistical manifold M(θ). The cost function L is interpreted as the geometric compactness of those three sets of distributions. This\n3The FIM is mostly computed for parameters on a statistical manifold to describe parameter sensitivity. In contrast, we compute the FIM with respect to the hidden variable z.\nis essentially related to the theory of minimum description length (Hinton & Zemel, 1994; Sun et al., 2015). Gθ(z) also gives a lower bound (Cramér-Rao bound, Cramér 1946) on the variance of z, which is given by G−1θ (z). In other words, there is a minimum precision (or maximum accuracy) that one can achieve in the inference of z given xk. Although it is hard to compute exactly, it is important to realize the existence of this bound. We give the following rough estimation. Because each z has only single observation, the FIM of z does not scale with n. On the other hand, θ has n repeated observations. Therefore its FIM G(θ) = ∑n k=1Ep ( −∂2 log p(xk | z,θ)/∂θ2 ) scales linearly with n, meaning that the\nprecision of θ scales with 1/ √ n. Therefore the precision of z is roughly √ n times the precision of θ. Hence the estimation of z should indeed be inaccurate as compared to θ. This means that the proposed coarse grain technique is not only for computational convenience, but also has a theoretical background."
    }, {
      "heading" : "6 Concluding Remarks",
      "text" : "Within the variational auto-encoding framework (Kingma & Welling, 2014), this paper proposed a new method CG-BPEF-VAE. Among numerous variations of VAE (Burda et al., 2016; Jang et al., 2017), CG-BPEF-VAE is featured by using a universal BPEF density generator in the inference model, and providing a principled way to simulate continuous densities using discrete latent variables. For example, to apply another sophisticated distribution on the latent variable z, one can employ our CG technique so as to use the reparameterization trick. This study touches a fundamental problem in unsupervised learning: how to build a discrete latent structure to factor information in a continuous representation? We provide preliminary results on unsupervised density estimation, showing performance improvements over the original VAE and category VAE (Jang et al., 2017). An empirical study can be extended to semi-supervised learning. This is ongoing work.\nWe try to picture an information geometric background of VAE. Essentially VAE learns on two manifoldsM(θ) andM(ϕ), where the cost function can be geometrically interpreted as a sum of divergences within a n-body system. This potentially leads to new implementations based on information geometry, e.g., using alternative divergences.\nBPEF uses a linear combination of basis distributions in the θ-coordinates (natural parameters). Another basic way to define probability distributions is mixture modeling, or linear combination in the η-coordinates (moment parameters). The coarse grained technique can be extended to a mixture of BPEF densities, which could more effectively model multi-modal distributions."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is funded by King Abdullah University of Science and Technology. The experiments are conducted on the Manda cluster provided by Computational Bioscience Research Center at KAUST."
    }, {
      "heading" : "A Proof of Theorem 1",
      "text" : "Proof. We first prove (1). The mapping z = yᵀζ is from the simplex\n∆R−1 = { y ∈ <R :\nR∑ i=1 yi = 1; ∀i, yi ≥ 0\n}\nto the line segment [-1,1]. We therefore define the following subset\nSz = { y ∈ ∆R−1 : yᵀζ = z } ⊂ ∆R−1,\nwhere −1 ≤ z ≤ 1. There we have the following partition scheme\n∆R−1 = 1⊎ z=−1 Sz.\nNote KL(q(y |x,ϕ) : p(y |a)) is the KL divergence between two distributions on ∆R−1. By information monotonicity,\nKL(q(y |x,ϕ) : p(y |a)) ≥ KL(q(Sz |x,ϕ) : p(Sz |a)).\nwhere q(Sz |x,ϕ) = ∫ y∈Sz q(y |x,ϕ)dy and p(Sz |a) = ∫ y∈Sz p(y |a)dy are coarse grained distributions defined on [−1, 1], that is, q(z |x,ϕ) and p(z |a). To prove (2), we partition ∆R−1 based on a Voronoi diagram. Let\nVr = {y ∈ ∆R−1 : ‖y − er‖2 ≤ ‖y − eo‖,∀o 6= r}.\nwhere er ∈ ∆R−1 has the r’th bit set to 1 and the rest bits set to 0. By the basic property of Concrete distribution (Eq.(8) in the paper),∫\ny∈Vr q(y |x,ϕ)dy = P (yr ≥ yo,∀o 6= r) = exp(βjr)∑R r=1 exp(βjr)\n,∫ y∈Vr p(y |a)dy = exp(αjr)∑R r=1 exp(αjr) .\nThen (2) follows immediately from information monotonicity."
    }, {
      "heading" : "B Proof of Theorem 2",
      "text" : "Lemma 1. Let p(z) = exp(ϕᵀt(z) − F (ϕ)) be a distribution in an exponential family , then we have I(η)− ηᵀϕ+ F (ϕ) = 0.\nProof. By definition, I(η) = ∫ p(z) log p(z)dz = ∫ p(z) (ϕᵀt(z)− F (ϕ)) dz\n= ϕᵀ ∫ p(z)t(z)dz − F (ϕ) = ϕᵀη − F (ϕ).\nProof. If both q(z |xk) and p(z) are in the same exponential family, we have\np(z) = exp (tᵀ(z)ϕz − F (ϕz)) , q(z |xk) = exp ( tᵀ(z)ϕk − F (ϕk) ) .\nTherefore\nterm1 = 1\nn n∑ k=1 KL ( q(z |xk,ϕ) : p(z |ϕz) ) = 1\nn n∑ k=1 ∫ q(z |xk,ϕ) log q(z |x k,ϕ) p(z |ϕz) dz\n= 1\nn n∑ k=1 [ I(ηk)− ∫ q(z |xk,ϕ)(tᵀ(z)ϕz − F (ϕz))dz ]\n= 1\nn n∑ k=1 [ I(ηk)− (ηk)ᵀ(ϕz) + F (ϕz) ] . (7)\nBecause F (ϕz) is convex with respect to ϕz , setting its derivative\n∂term1 ∂ϕz = 1 n n∑ k=1 [ −ηk + ∂F ∂ϕz ] = 1 n n∑ k=1 [ −ηk + ηz ] (Legendre transformation)\nto zero gives the unique minimizer of term1:\n(ηz)? = 1\nn n∑ k=1 ηk.\nPlugging this into Eq. 7, we get\nterm?1 = 1\nn n∑ k=1 [ I(ηk)− (ηk)ᵀ(ϕz)? + F ((ϕz)?) ] = 1\nn n∑ k=1 [ I(ηk)− ((ϕz)?)ᵀηk + ((ϕz)?)ᵀ(ηz)? − I((ηz)?) ] (by the Lemma)\n= 1\nn n∑ k=1 I(ηk)− ((ϕz)?)ᵀ 1 n n∑ k=1 ηk + ((ϕz)?)ᵀ(ηz)? − I((ηz)?)\n= 1\nn n∑ k=1 I(ηk)− I\n( 1\nn n∑ k=1 ηk\n) .\nBy the above analysis, term1 ≥ term?1, and the “=” holds if and only if ηz = (ηz)?. The second “≥” is straightforward from the fact that I is a convex function in the coordinate system η.\nIf p(z) is non-parametric, then\nterm1 = 1\nn n∑ k=1 ∫ q(z |xk,ϕ) log q(z |x k,ϕ) p(z |ϕz) dz\n= 1\nn n∑ k=1 [ I(ηk)− ∫ q(z |xk,ϕ) log p(z |ϕz)dz ]\n= 1\nn n∑ k=1 I(ηk)− ∫ 1 n n∑ k=1 q(z |xk,ϕ) log p(z |ϕz)dz. (8)\nTherefore, term1 is minimized at p(z | (ϕz)?) = 1n ∑n k=1 q(z |xk,ϕ). Plugging this minimizer into the above Eq. 8, we get\nterm?1 = 1\nn n∑ k=1 I(ηk)− ∫ 1 n n∑ k=1 q(z |xk,ϕ) log [ 1 n n∑ k=1 q(z |xk,ϕ) ] dz.\nNote that the mixture model 1n ∑n k=1 q(z |xk,ϕ) is outside the exponential familyM(ϕ)."
    }, {
      "heading" : "C The Effect of the Dimensionality Reduction Layer of CG-BPEFVAE",
      "text" : "Fig. 5 shows the KL(p(z) : Uniform) (KL(z)) and KL(α : Uniform) (KL(category)) when α is generated by Dirichlet distributions with different configurations. In all cases, KL(α : Uniform) is lower bounded by KL(p(z) : Uniform) for small temperature.\nD Visualization of the Concrete Distribution Fig. 6, Fig. 7 and Fig. 8 show Concrete densities generated by random sampling. For each experiment (sub-figure), we generate 106 Concrete samples and plot the resulting density. There are very high density regions near the corner (the red region), which are cropped so that the visualization is clear.\nAn interesting observation is that the density will “leak” to the simplex faces if T is small, although in this case the density will concentrate on the corners. Therefore it may not always be good to choose a small T . This is ongoing study."
    }, {
      "heading" : "E Details of the Convolutional Layers",
      "text" : "We used convolutional layers on the SVHN dataset. The encoder is specified by\n• Input: 1× 32× 32 (RGB is averaged into 1 channel)\n• Convolutional layer: 32 (5× 5) filters, with ReLU activation and no padding (→ 32× 28× 28)\n• Pooling layer: 2× 2 filter with a stride of 2 and no padding zeros (→ 32× 14× 14)\n• Convolutional layer: 64 (5× 5) filters, with ReLU activation and no padding (→ 64× 10× 10)\n• Pooling layer: 2× 2 filter with a stride of 2 and no padding (→ 64× 5× 5)\n• Convolutional layer: 128 (5× 5) filters, with RELU activation and no padding (→ 128× 1× 1)\nThe decoder is specified by\n• A dense linear layer with RELU activation to transform the dimension to 128\n• Transposed convolutional layer: 64 (5× 5) filters with stride 4; with RELU activation (→ 64× 4× 4)\n• Transposed convolutional layer: 32 (5× 5) filters with stride 2; with RELU activation (→ 32× 8× 8)\n• Transposed convolutional layer: 16 (5× 5) filters with stride 2; with RELU activation (→ 16× 16× 16)\n• Transposed convolutional layer: 1 (5× 5) filters with stride 2; without non-linear activation (→ 1× 32× 32)"
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning",
      "author" : [ "Abadi", "Martı́n" ],
      "venue" : "on heterogeneous systems,",
      "citeRegEx" : "Abadi and Martı́n,? \\Q2015\\E",
      "shortCiteRegEx" : "Abadi and Martı́n",
      "year" : 2015
    }, {
      "title" : "Information Geometry and its Applications, volume 194 of Applied Mathematical Sciences",
      "author" : [ "Amari", "Shun-ichi" ],
      "venue" : null,
      "citeRegEx" : "Amari and Shun.ichi.,? \\Q2016\\E",
      "shortCiteRegEx" : "Amari and Shun.ichi.",
      "year" : 2016
    }, {
      "title" : "Importance weighted autoencoders",
      "author" : [ "Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Burda et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Burda et al\\.",
      "year" : 2016
    }, {
      "title" : "Estimation and moment recursion relations for multimodal distributions of the exponential family",
      "author" : [ "Cobb", "Loren", "Koppstein", "Peter", "Chen", "Neng Hsin" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Cobb et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Cobb et al\\.",
      "year" : 1983
    }, {
      "title" : "Mathematical Methods of Statistics, volume 9 of Princeton Mathematical Series",
      "author" : [ "Cramér", "Harald" ],
      "venue" : null,
      "citeRegEx" : "Cramér and Harald.,? \\Q1946\\E",
      "shortCiteRegEx" : "Cramér and Harald.",
      "year" : 1946
    }, {
      "title" : "Extreme Value Theory: An Introduction. Springer Series in Operations Research and Financial Engineering",
      "author" : [ "de Haan", "Laurens", "Ferreira", "Ana" ],
      "venue" : null,
      "citeRegEx" : "Haan et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Haan et al\\.",
      "year" : 2006
    }, {
      "title" : "Deep unsupervised clustering with Gaussian mixture variational autoencoders",
      "author" : [ "Dilokthanakul", "Nat", "Mediano", "Pedro A.M", "Garnelo", "Marta", "Lee", "Matthew C.H", "Salimbeni", "Hugh", "Arulkumaran", "Kai", "Shanahan", "Murray" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Dilokthanakul et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Dilokthanakul et al\\.",
      "year" : 2017
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Glorot", "Xavier", "Bengio", "Yoshua" ],
      "venue" : "In AISTATS; JMLR W&CP",
      "citeRegEx" : "Glorot et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2010
    }, {
      "title" : "DRAW: A recurrent neural network for image generation",
      "author" : [ "Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan" ],
      "venue" : "In ICML; JMLR W & CP",
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "Statistical theory of extreme values and some practical applications: a series of lectures",
      "author" : [ "Gumbel", "Emil Julius" ],
      "venue" : "Applied mathematics series. U. S. Govt. Print. Office,",
      "citeRegEx" : "Gumbel and Julius.,? \\Q1954\\E",
      "shortCiteRegEx" : "Gumbel and Julius.",
      "year" : 1954
    }, {
      "title" : "Autoencoders, minimum description length and Helmholtz free energy",
      "author" : [ "Hinton", "Geoffrey E", "Zemel", "Richard S" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Hinton et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 1994
    }, {
      "title" : "Categorical reparameterization with Gumbel-softmax",
      "author" : [ "Jang", "Eric", "Gu", "Shixiang", "Poole", "Ben" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Jang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2017
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Jordan et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jordan et al\\.",
      "year" : 1999
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik P", "Ba", "Jimmy" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational Bayes",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised learning with deep generative models",
      "author" : [ "Kingma", "Diederik P", "Mohamed", "Shakir", "Jimenez Rezende", "Danilo", "Welling", "Max" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimum follow the leader algorithm",
      "author" : [ "Kuzmin", "Dima", "Warmuth", "Manfred K" ],
      "venue" : "In COLT, pp",
      "citeRegEx" : "Kuzmin et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kuzmin et al\\.",
      "year" : 2005
    }, {
      "title" : "The MNIST database of handwritten digits",
      "author" : [ "LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher J.C" ],
      "venue" : "In NIPS",
      "citeRegEx" : "LeCun et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2014
    }, {
      "title" : "The Concrete distribution: A continuous relaxation of discrete random variables",
      "author" : [ "Maddison", "Chris J", "Mnih", "Andriy", "Teh", "Yee Whye" ],
      "venue" : "CoRR, abs/1611.00712,",
      "citeRegEx" : "Maddison et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Maddison et al\\.",
      "year" : 2016
    }, {
      "title" : "Rectified linear units improve restricted Boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Nair and Hinton,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair and Hinton",
      "year" : 2010
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y" ],
      "venue" : "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,",
      "citeRegEx" : "Netzer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Netzer et al\\.",
      "year" : 2011
    }, {
      "title" : "Sided and symmetrized Bregman centroids",
      "author" : [ "Nielsen", "Frank", "Nock", "Richard" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Nielsen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nielsen et al\\.",
      "year" : 2009
    }, {
      "title" : "Patch matching with polynomial exponential families and projective divergences",
      "author" : [ "Nielsen", "Frank", "Nock", "Richard" ],
      "venue" : "In International Conference on Similarity Search and Applications (SISAP),",
      "citeRegEx" : "Nielsen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nielsen et al\\.",
      "year" : 2016
    }, {
      "title" : "Guaranteed bounds on information-theoretic measures of univariate mixtures using piecewise log-sum-exp",
      "author" : [ "Nielsen", "Frank", "Sun", "Ke" ],
      "venue" : "inequalities. Entropy,",
      "citeRegEx" : "Nielsen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nielsen et al\\.",
      "year" : 2016
    }, {
      "title" : "Markov chain Monte Carlo and variational inference: Bridging the gap",
      "author" : [ "Salimans", "Tim", "Kingma", "Diederik", "Welling", "Max" ],
      "venue" : "In ICML; JMLR W&CP",
      "citeRegEx" : "Salimans et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Salimans et al\\.",
      "year" : 2015
    }, {
      "title" : "Multi-modal variational encoder-decoders",
      "author" : [ "Serban", "II Iulian Vlad", "Alexander G. Ororbia", "Pineau", "Joelle", "Courville", "Aaron C" ],
      "venue" : null,
      "citeRegEx" : "Serban et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Stochastic video prediction with conditional density estimation",
      "author" : [ "Shu", "Rui" ],
      "venue" : "In ECCV Workshop on Action and Anticipation for Visual Learning,",
      "citeRegEx" : "Shu and Rui,? \\Q2016\\E",
      "shortCiteRegEx" : "Shu and Rui",
      "year" : 2016
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "Sohn", "Kihyuk", "Lee", "Honglak", "Yan", "Xinchen" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Sohn et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2015
    }, {
      "title" : "Information geometry and minimum description length networks",
      "author" : [ "Sun", "Ke", "Wang", "Jun", "Kalousis", "Alexandros", "Marchand-Maillet", "Stéphane" ],
      "venue" : "In ICML; JMLR W&CP",
      "citeRegEx" : "Sun et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2015
    }, {
      "title" : "An uncertain future: Forecasting from static images using variational autoencoders",
      "author" : [ "Walker", "Jacob", "Doersch", "Carl", "Gupta", "Abhinav", "Hebert", "Martial" ],
      "venue" : "In ECCV; LNCS 9911,",
      "citeRegEx" : "Walker et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.",
      "startOffset" : 82,
      "endOffset" : 187
    }, {
      "referenceID" : 24,
      "context" : "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.",
      "startOffset" : 82,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.",
      "startOffset" : 82,
      "endOffset" : 187
    }, {
      "referenceID" : 25,
      "context" : "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.",
      "startOffset" : 82,
      "endOffset" : 187
    }, {
      "referenceID" : 12,
      "context" : ", 2016) combine the two powers of variational Bayesian learning (Jordan et al., 1999) with strong generalization and a standard learning objective, and deep learning with flexible and scalable representations.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "They are attracting decent attentions, producing state-of-the-art performance in semi-supervised learning (Kingma et al., 2014) and image generation (Gregor et al.",
      "startOffset" : 106,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : ", 2014) and image generation (Gregor et al., 2015), and are getting applied in diverse areas such as deep generative modeling (Rezende et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 27,
      "context" : ", 2014), image segmentation (Sohn et al., 2015), clustering (Dilokthanakul et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : ", 2015), clustering (Dilokthanakul et al., 2017), and future prediction from images (Walker et al.",
      "startOffset" : 20,
      "endOffset" : 48
    }, {
      "referenceID" : 29,
      "context" : ", 2017), and future prediction from images (Walker et al., 2016).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : ", 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al., 1999) with strong generalization and a standard learning objective, and deep learning with flexible and scalable representations. They are attracting decent attentions, producing state-of-the-art performance in semi-supervised learning (Kingma et al., 2014) and image generation (Gregor et al., 2015), and are getting applied in diverse areas such as deep generative modeling (Rezende et al., 2014), image segmentation (Sohn et al., 2015), clustering (Dilokthanakul et al., 2017), and future prediction from images (Walker et al., 2016). This paper discusses unsupervised learning with VAE which pipes an inference model q(z |x) with a generative model p(x | z), where x and z are observed and latent variables, respectively. A simple parameter-free prior p(z) combined with p(x | z) parameterized by a deep neural network results in arbitrarily flexible representations. However, its (very complex) posterior p(z |x) must be within the representation power of the inference machine q(z |x), so that the variational bound is tight and variational learning is effective. In the original VAE (Kingma & Welling, 2014), q(z |x) obeys a Gaussian distribution with a diagonal covariance matrix. This is a very simplified assumption, because Gaussian is the maximum entropy (least informative) distribution with respect to prescribed mean and variance and has one single mode, while human inference can be ambiguous and can have a bounded support when we exclude very unlikely cases (de Haan & Ferreira, 2006). Many recent works try to tackle this limitation. Jang et al. (2017) extended VAE to effectively use a discrete latent z following a category distribution (e.",
      "startOffset" : 8,
      "endOffset" : 1693
    }, {
      "referenceID" : 2,
      "context" : ", 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al., 1999) with strong generalization and a standard learning objective, and deep learning with flexible and scalable representations. They are attracting decent attentions, producing state-of-the-art performance in semi-supervised learning (Kingma et al., 2014) and image generation (Gregor et al., 2015), and are getting applied in diverse areas such as deep generative modeling (Rezende et al., 2014), image segmentation (Sohn et al., 2015), clustering (Dilokthanakul et al., 2017), and future prediction from images (Walker et al., 2016). This paper discusses unsupervised learning with VAE which pipes an inference model q(z |x) with a generative model p(x | z), where x and z are observed and latent variables, respectively. A simple parameter-free prior p(z) combined with p(x | z) parameterized by a deep neural network results in arbitrarily flexible representations. However, its (very complex) posterior p(z |x) must be within the representation power of the inference machine q(z |x), so that the variational bound is tight and variational learning is effective. In the original VAE (Kingma & Welling, 2014), q(z |x) obeys a Gaussian distribution with a diagonal covariance matrix. This is a very simplified assumption, because Gaussian is the maximum entropy (least informative) distribution with respect to prescribed mean and variance and has one single mode, while human inference can be ambiguous and can have a bounded support when we exclude very unlikely cases (de Haan & Ferreira, 2006). Many recent works try to tackle this limitation. Jang et al. (2017) extended VAE to effectively use a discrete latent z following a category distribution (e.g. Bernoulli distribution). Kingma et al. (2014) extended the latent structure with a combination of continuous and discrete latent variables (class labels)",
      "startOffset" : 8,
      "endOffset" : 1831
    }, {
      "referenceID" : 11,
      "context" : "Notably, we present a novel application scenario with new analysis on the Gumbel softmax trick (Jang et al., 2017; Maddison et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 137
    }, {
      "referenceID" : 18,
      "context" : "Notably, we present a novel application scenario with new analysis on the Gumbel softmax trick (Jang et al., 2017; Maddison et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "We assemble these components into a machine CG-BPEF-VAE and present empirical results on unsupervised density estimation, showing improvements over vanilla VAE (Kingma & Welling, 2014) and category VAE (Jang et al., 2017).",
      "startOffset" : 202,
      "endOffset" : 221
    }, {
      "referenceID" : 6,
      "context" : "(2016) and Dilokthanakul et al. (2017) proposed to use a Gaussian mixture latent model in VAE.",
      "startOffset" : 11,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "(2016) and Dilokthanakul et al. (2017) proposed to use a Gaussian mixture latent model in VAE. Serban et al. (2016) applied a piecewise constant distribution on z.",
      "startOffset" : 11,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "1 Bounded Polynomial Exponential Family We try to model the latent z with a factorable polynomial exponential family (PEF) (Cobb et al., 1983; Nielsen & Nock, 2016) probability density function:",
      "startOffset" : 123,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : "This PEF family can be regarded as the most general parameterization, because with large enough M it can approximate arbitrary finely any given p(z) satisfying weak regularity conditions (Cobb et al., 1983).",
      "startOffset" : 187,
      "endOffset" : 206
    }, {
      "referenceID" : 11,
      "context" : "This reparameterization problem of category distribution is studied recently (Jang et al., 2017; Maddison et al., 2016) following earlier developments (Kuzmin & Warmuth, 2005; Maddison et al.",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 18,
      "context" : "This reparameterization problem of category distribution is studied recently (Jang et al., 2017; Maddison et al., 2016) following earlier developments (Kuzmin & Warmuth, 2005; Maddison et al.",
      "startOffset" : 77,
      "endOffset" : 119
    }, {
      "referenceID" : 18,
      "context" : "Based on these previous studies, we let yj follow a Concrete distribution (Maddison et al., 2016), which is a continuous relaxation of the category distribution, with the key advantage that Concrete samples can be easily drawn to be applied to VAE.",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : "Hence it can be considered as a relaxation (Maddison et al., 2016) of the category distribution.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "The study (Jang et al., 2017) implies that Tmin = 0.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 20,
      "context" : "The SVHN dataset (Netzer et al., 2011) has around 100,000 gray-scale pictures (for simplicity the original 32× 32× 3 RGB images are reduced into 32× 32× 1 by averaging the 3 channels) of door numbers with a train/valid/test split of 10 : 1 : 3.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "We compare the proposed CG-BPEFVAE with vanilla VAE (Gauss-VAE) and Category VAE (Cat-VAE) (Jang et al., 2017).",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 13,
      "context" : "One has to incorporate supervised information (Kingma et al., 2014) to achieve better results.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 28,
      "context" : "is essentially related to the theory of minimum description length (Hinton & Zemel, 1994; Sun et al., 2015).",
      "startOffset" : 67,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "Among numerous variations of VAE (Burda et al., 2016; Jang et al., 2017), CG-BPEF-VAE is featured by using a universal BPEF density generator in the inference model, and providing a principled way to simulate continuous densities using discrete latent variables.",
      "startOffset" : 33,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "Among numerous variations of VAE (Burda et al., 2016; Jang et al., 2017), CG-BPEF-VAE is featured by using a universal BPEF density generator in the inference model, and providing a principled way to simulate continuous densities using discrete latent variables.",
      "startOffset" : 33,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "This study touches a fundamental problem in unsupervised learning: how to build a discrete latent structure to factor information in a continuous representation? We provide preliminary results on unsupervised density estimation, showing performance improvements over the original VAE and category VAE (Jang et al., 2017).",
      "startOffset" : 301,
      "endOffset" : 320
    } ],
    "year" : 2017,
    "abstractText" : "Variational autoencoders (VAE) often use Gaussian or category distribution to model the inference process. This puts a limit on variational learning because this simplified assumption does not match the true posterior distribution, which is usually much more sophisticated. To break this limitation and apply arbitrary parametric distribution during inference, this paper derives a semi-continuous latent representation, which approximates a continuous density up to a prescribed precision, and is much easier to analyze than its continuous counterpart because it is fundamentally discrete. We showcase the proposition by applying polynomial exponential family distributions as the posterior, which are universal probability density function generators. Our experimental results show consistent improvements over commonly used VAE models.",
    "creator" : "LaTeX with hyperref package"
  }
}