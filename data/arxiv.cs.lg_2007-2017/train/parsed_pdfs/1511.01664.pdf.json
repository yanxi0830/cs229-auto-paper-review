{
  "name" : "1511.01664.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stochastic Proximal Gradient Descent for Nuclear Norm Regularization",
    "authors" : [ "Lijun Zhang", "Tianbao Yang", "Rong Jin", "Zhi-Hua Zhou" ],
    "emails" : [ "zhanglj@lamda.nju.edu.cn", "tianbao-yang@uiowa.edu", "rongjin@cse.msu.edu", "zhouzh@lamda.nju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n01 66\n4v 2\n[ cs\n√ T ) and O(log T/T )\nconvergence rates for general convex functions and strongly convex functions, respectively.\nKeywords: Stochastic Proximal Gradient Descent, Nuclear Norm Regularization"
    }, {
      "heading" : "1. Introduction",
      "text" : "Stochastic optimization has received lots of attention recently in data mining and machine learning communities (Hu et al., 2009; Shalev-Shwartz et al., 2009; Zhang et al., 2013). It is generally formulated as a minimization problem where the objective is an expectation over unknown distributions (Nemirovski et al., 2009), or defined in terms of the access model, which assumes there is a stochastic oracle that generates unbiased estimate of the objective or its gradient (Hazan and Kale, 2011). Algorithms for stochastic optimization, such as the stochastic gradient descent (SGD), utilize stochastic gradients of the objective for updating. Those methods have lightweight computation per iteration, and are widely used to reduce the time complexity of optimization problems.\nIn this paper, we take a different perspective and exploit stochastic optimization as a tool to reduce the space complexity for certain optimization problem over matrices. Specifically, we study the nuclear norm regularized convex optimization problem\nmin W∈W⊆Rm×n\nF (W ) = f(W ) + λ‖W‖∗ (1)\nAlgorithm 1 SPGD for nuclear norm regularization Input: The number of trials T , and the regularization parameter λ\n1: Initialize W1 = 0 2: for t = 1, 2, . . . , T do 3: Construct a low-rank stochastic gradient Ĝt = AtB ⊤ t of f(·) at Wt 4: Calculate Wt+1 according to (2) 5: end for 6: return WT+1\nwhere W is the domain of the solution, f(·) is a convex function, and ‖ · ‖∗ is the nuclear norm of matrices. Note that the above problem is a special case of convex composite optimization (Nesterov, 2013). Due to the nuclear norm regularizer, the optimal solution W∗ to (1) is a low-rank matrix, provided λ is large enough. Here, we consider the large-scale setting: “both m and n are very large such that directly storing an m×n matrix in memory is impossible”. This setting excludes deterministic methods for solving (1) (Nesterov, 2013), as they need to evaluate the gradient of f(·), which needs O(mn) memory. Although various stochastic algorithms has been proposed for convex composite optimization and can also be applied to (1), their primal goal is to reduce the time complexity of evaluating the gradient of f(·) instead of the space complexity (Lin et al., 2011; Lan, 2012). As a result, most of them still have an O(mn) space requirement.\nThe only memory-efficient algorithm for solving (1) that we found in the literature is a heuristic algorithm developed by Avron et al. (2012). Under the condition that it is possible to generate a stochastic gradient of f(·) which is also low-rank, they propose to combine SGD and truncated SVD to solve (1). By representing all the intermediate solutions and stochastic gradients in low-rank factorization forms, the space complexity can be reduced to O(m+n). The major limitation of this approach is that there is no theoretical guarantee about its convergence, due to the fact that the truncated SVD operation introduces a constant error in each iteration. Furthermore, when the objective value is difficult to calculate, it is also unclear which iterate should be used as the final solution.\nInspired by the previous studies, we develop a novel stochastic algorithm that optimizes (1) in a memory-efficient way and is supported by formal theoretical guarantees. Specifically, we propose to use stochastic proximal gradient descent (SPGD) instead of SGD to solve (1), and take the last iterate of SPGD as the final solution. Similar to the heuristic algorithm of Avron et al. (2012), the proposed algorithm always maintains a low-rank factorization of iterates that can be conveniently held in memory, and both the time and space complexities in each iteration are O(m+n). Built upon recent progresses in stochastic optimization (Rakhlin et al., 2012; Shamir and Zhang, 2012), we further analyze the convergence property of the proposed algorithm, and show that the last iterate has an O(log T/ √ T ) convergence rate for general convex functions, and an O(log T/T ) rate for strongly convex functions.\n2. The Algorithm\nOur algorithm is based on the following observation. For any W ∈ W, it is always possible to generate a low-rank matrix Ĝ represented as AB⊤ such that\nE[Ĝ] = E[AB⊤] ∈ ∂f(W )\nand A and B occupy O(m) and O(n) memory, respectively. The existence and construction of such a low-rank matrix can be found in previous studies (Avron et al., 2012; Chen et al., 2014), and will be discussed later.\nDenote by Wt the solution at the t-th iteration, and let Ĝt = AtB ⊤ t be the low-rank stochastic gradient of f(·) at Wt. Then, we update the current solution by the SPGD, which is a stochastic variant of composite gradient mapping Nesterov (2013)\nWt+1\n=argmin W∈W\n1 2 ‖W −Wt‖2F + ηt〈W −Wt, Ĝt〉+ ηtλ‖W‖∗\n=argmin W∈W\n1\n2\n∥∥∥W − ( Wt − ηtĜt )∥∥∥ 2\nF + ηtλ‖W‖∗\n(2)\nwhere ηt > 0 is the step size. Due to the presence of the nuclear norm regularizer, all the iterates Wt’s tend to be low-rank matrices. The fact that both Wt and Ĝt are low-rank matrices can in turn be exploited to accelerate the updating.\nIn the beginning, we simply set W1 be the zero matrix, and take the last iterate WT+1 as the final solution. The complete procedure is summarized in Algorithm 1."
    }, {
      "heading" : "2.1 Construction of the Low-rank Stochastic Gradients",
      "text" : "For a given matrix W , there are various ways to construct a low-rank stochastic gradient Ĝ satisfying E[Ĝ] ∈ ∂f(W ) (Avron et al., 2012; Chen et al., 2014). For brevity, we just describe a general approach. To this end, we need the following definition from Avron et al. (2012).\nDefinition 1 (Probing Matrix) A random n×k matrix Y is a probing matrix if E[Y Y ⊤] = I.\nAvron et al. (2012) also provide several families of distributions that generate probing matrices efficiently. Lemma 2 Let Y = Z/ √ k where Z is a random matrix drawn any one of the following distributions:\n1. Independent entries taking values +1 and −1 with equal probability 1/2.\n2. Independent and identically distributed standard normal entries.\n3. Each column of Z is drawn uniformly at random and independent of each other from {√ne1, . . . , √ ne2} (scaled identity vectors).\nThen, Y is a probing matrix.\nLet G is a subgradient of f(·) at X, i.e., G ∈ ∂f(X). Then, we can construct Ĝ as\nĜ = AB⊤, A = GY, and B = Y.\nThus, as long as we can evaluate the subgradient of f(·), we can construct a low-rank stochastic gradient. Note that when Y is constructed from the scaled identity vectors, A = GY are scaled columns of G. So, in this case, we only need to compute a small portion of G, which could be very efficient."
    }, {
      "heading" : "2.2 Efficient Implementation of the Updating",
      "text" : "During the optimization process, all the iterates Wt are represented in the SVD form Wt = U⊤t ΣtV ⊤ t , which needs O((m+n)rt) memory. Here, rt is the rank of Wt. Then, by exploiting the fact that Ĝt = AtB ⊤ t is also represent in a low-rank factorization form, we can implement the updating rule in (2) efficiently when W = Rm×n or W = {W |W ∈ Rm×n, ‖W‖F ≤ R}. We start with the incremental SVD which is a core operation during the updating."
    }, {
      "heading" : "2.2.1 Incremental SVD",
      "text" : "Let X ∈ Rm×n be a rank-r matrix with economy SVD X = UΣV ⊤. Let A = Rm×c and B ∈ R n×c. Then, the economy SVD of X+AB⊤ can be calculated in O((m+n)(r+c)2+(r+c)3) time with O((m+n)(r+ c)) memory. We provide a detailed procedure below (Brand, 2006, Section 2).\nLet P be an orthogonal basis of the column space of (I − UU⊤)A, and set RA = P⊤(I−UU⊤)A. Note that cols(P ) = rows(RA) = rank((I −UU⊤)A) ≤ c, and may be zero if A lies in the column space of U . Then, we have\n[ U A ] = [ U P ] [ I U⊤A 0 RA ]\nSimilarly, let Q be an orthogonal basis of the column space of (I − V V ⊤)B, and set RB = Q⊤(I − V V ⊤)B. Then, it is easy to verify\nX +AB⊤ = [ U P ] K [ V Q ]⊤\nwhere\nK = [ I U⊤A 0 RA ] [ Σ 0 0 I ] [ I V ⊤B 0 RB ]⊤\n= [ Σ 0 0 0 ] + [ U⊤A RA ] [ V ⊤B RB ]⊤ ∈ R(r+cols(P ))×(r+cols(Q))\nLet the SVD of K be K = Û Σ̂V̂ ⊤. Then, the SVD of X +AB⊤ is given by\nX +AB⊤ = ([ U P ] Û ) Σ̂ ([ V Q ] V̂ )⊤"
    }, {
      "heading" : "2.2.2 Updating for Unbounded Domain",
      "text" : "We first introduce the Singular Value Shrinkage (SVS) operator with threshold λ, which is denoted by Dλ[·] (Cai et al., 2010). For a matrix Y ∈ Rm×n with singular value decomposition UΣV ⊤, where Σ = diag[σ1, . . . , σmin(m,n)], we have\nDλ[Y ] = UDλ[Σ]V ⊤\nwhere Dλ[Σ] = diag [ max(0, σ1 − λ), . . . ,max(0, σmin(m,n) − λ) ] . The following theorem shows that Dλ[Y ] is the optimal solution to a unconstrained least square problem with nuclear norm regularization (Cai et al., 2010, Theorem 2.1).\nTheorem 1 For each λ ≥ 0 and Y ∈ Rm×n, the SVS operator obeys\nDλ[Y ] = argmin X\n1 2 ‖X − Y ‖2F + λ‖X‖∗ (3)\nWhen W = Rm×n, the above theorem implies Wt+1 = Dληt [Wt − ηtĜt]\nwhich can be implemented in the following two steps:\n1. Since Wt = U ⊤ t ΣtV ⊤ t and Ĝt = AtB ⊤ t , we can use the incremental SVD algorithm in\nSection 2.2.1 to find the SVD of Ŵt+1 := Wt−ηtĜt, which is denoted by Ût+1Σ̂t+1V̂ ⊤t+1. Let rt and ct be the rank of Wt and Ĝt, respectively. The time and space complexities of this step are O((m+n)(rt+ ct) 2 +(rt + ct) 3) and O((m+n)(rt+ ct)), respectively.\n2. Based on the SVD factorization Ŵt+1 = Ût+1Σ̂t+1V̂ ⊤ t+1, Wt+1 = Dληt [Ŵt+1] can be\ncalculated directly according to the definition of SVS operator. Furthermore, Wt+1 is also represented in the SVD form as Wt+1 = U ⊤ t+1Σt+1V ⊤ t+1. The time complexity is O(rt + ct), and there is no additional space requirement."
    }, {
      "heading" : "2.2.3 Updating for Frobenius Norm Ball",
      "text" : "To facilitate presentations, we introduce the metric projection operator onto the Frobenius norm ball with radius R:\nPR[Y ] = argmin ‖X‖F≤R\n‖X − Y ‖F = { Y, if ‖Y ‖F ≤ R; R\n‖Y ‖F Y, otherwise.\nTo derive the updating rule when W = {W |W ∈ Rm×n, ‖W‖F ≤ R}, we need the following theorem.\nTheorem 2 The optimal solution X∗ to the following optimization problem\nmin ‖X‖F≤R\n1 2 ‖X − Y ‖2F + λ‖X‖∗ (4)\nis given by\nX∗ = PR [Dλ[Y ]] = { Dλ[Y ], if ‖Dλ[Y ]‖F ≤ R; R\n‖Dλ[Y ]‖F Dλ[Y ], otherwise.\nFrom Theorem 2, we have\nWt+1 = PR [ Dληt [Wt − ηtĜt] ] =\n{ Dληt [Wt − ηtĜt], if ‖Dληt [Wt − ηtĜt]‖F ≤ R;\nR ‖Dληt [Wt−ηtĜt]‖F Dληt [Wt − ηtĜt], otherwise.\nTo calculate Wt+1, we first use the two steps in Section 2.2.2 to get Dληt [Wt − ηtĜt] = U⊤t+1Σt+1V ⊤ t+1, and then use the following additional step to normalize the diagnal matrix Σt+1:\nΣt+1 ← min ( 1,\nR\n‖Σt+1‖F\n) Σt+1\nwhich has O(rt+1) time complexity."
    }, {
      "heading" : "3. Analysis",
      "text" : "The convergence of the last iterate of SGD has been analyzed by Rakhlin et al. (2012) and Shamir and Zhang (2012). By extending their analysis, we provide the convergence of SPGD for solving the nuclear norm regularized problem in (1).\nThe following theorem shows that for general convex functions, the last iterate attends an O(log T/ √ T ) convergence rate.\nTheorem 3 Assume there exists constants G and D such that E[‖Ĝt‖2F ] ≤ G2 for all t and supW,W ′∈W ‖W −W ′‖F ≤ D. Setting ηt = c/ √ T , we have\nE [F (WT )− F (W∗)] ≤ ( D2\nc + c\n( G2 + 16rλ2 + 4λG √ r )) 2 + log T√\nT\nwhere W∗ is the optimal solution to (1) and r ≥ maxt rank(Wt). Next, we study the case that the f(·) is strongly convex. In this case, the optimal solution W∗ is unique, which allows us to bound the difference between Wt and W∗ in each iteration.\nLemma 3 Suppose f(·) is µ-strongly convex, and E[‖Ĝt‖2F ] ≤ G2 for all t. Setting ηt = 1/(µt), we have\nE [ ‖Wt −W∗‖2F ] ≤ 4\nµ2t\n( G2 + 16rλ2 + 4λG √ r )\nwhere W∗ is the optimal solution to (1) and r ≥ maxt rank(Wt). As indicated by the above lemma, the convergence rate in terms of the squared distance between WT and W∗ is O(1/T ).\nBased on Lemma 3, it is also possible to characterizes the performance in terms of the objective function.\nTheorem 4 Suppose f(·) is µ-strongly convex, and E[‖Ĝt‖2F ] ≤ G2 for all t. Setting ηt = 1/(µt), we have\nE [F (WT )− F (W∗)] ≤ 17 ( G2 + 16rλ2 + 4λG √ r ) 1 + log T\nµT\nwhere W∗ is the optimal solution to (1) and r ≥ maxt rank(Wt)."
    }, {
      "heading" : "3.1 Proof of Theorem 3",
      "text" : "The proof is an extension of Theorem 2 in Shamir and Zhang (2012), which establishes a similar guarantee for the last iterate of SGD.\nFrom the property of strongly convex, i.e., (2) of Hazan and Kale (2011), the updating rule in (2) implies\n1 2 ‖Wt+1 −Wt‖2F + ηt〈Wt+1 −Wt, Ĝt〉+ ηtλ‖Wt+1‖∗\n≤1 2 ‖W −Wt‖2F + ηt〈W −Wt, Ĝt〉+ ηtλ‖W‖∗ − 1 2 ‖W −Wt+1‖2F\nfor all W ∈ W. Then, we have 1\n2 ‖W −Wt+1‖2F + ηtλ‖Wt+1‖∗\n≤1 2 ‖W −Wt‖2F − 1 2 ‖Wt+1 −Wt‖2F + ηt〈W −Wt+1, Ĝt〉+ ηtλ‖W‖∗ = 1\n2 ‖W −Wt‖2F + ηt〈W −Wt, Ĝt〉+ ηt〈Wt −Wt+1, Ĝt〉 −\n1 2 ‖Wt+1 −Wt‖2F + ηtλ‖W‖∗\n(5)\nfor all W ∈ W. Combining (5) with the following inequality\nηt〈Wt −Wt+1, Ĝt〉 − 1\n2 ‖Wt+1 −Wt‖2F ≤ max W ηt〈W, Ĝt〉 −\n1 2 ‖W‖2F = η2t 2 ‖Ĝt‖2F\nwe have\n1 2 ‖W −Wt+1‖2F + ηtλ‖Wt+1‖∗\n≤1 2 ‖W −Wt‖2F + ηt〈W −Wt, Ĝt〉+ η2t 2 ‖Ĝt‖2F + ηtλ‖W‖∗\n(6)\nfor all W ∈ W. Since E[Ĝt] ∈ ∂f(Wt), by convexity, we have\nf(Wt) + λ‖Wt+1‖∗ − f(W )− λ‖W‖∗ ≤〈Wt −W,E[Ĝt]〉 − λ‖W‖∗ + λ‖Wt+1‖∗ =〈Wt −W, Ĝt〉+ 〈Wt −W,E[Ĝt]− Ĝt〉 − λ‖W‖∗ + λ‖Wt+1‖∗ (6) ≤ 1 2ηt ‖W −Wt‖2F − 1 2ηt ‖W −Wt+1‖2F + ηt 2 ‖Ĝt‖2F + 〈Wt −W,E[Ĝt]− Ĝt〉\nand thus\nf(Wt) + λ‖Wt‖∗ − f(W )− λ‖W‖∗\n≤ 1 2ηt ‖W −Wt‖2F − 1 2ηt ‖W −Wt+1‖2F + ηt 2 ‖Ĝt‖2F + 〈Wt −W,E[Ĝt]− Ĝt〉\n+ λ‖Wt −Wt+1‖∗\n(7)\nfor all W ∈ W.\nNext, we discuss how to bound ‖Wt −Wt+1‖∗. Choosing W = Wt in (6), we have\n1 2 ‖Wt −Wt+1‖2F + ηtλ‖Wt+1‖∗ ≤ η2t 2 ‖Ĝt‖2F + ηtλ‖Wt‖∗ (8)\nLet r ≥ maxt rank(Wt), we have ‖Wt −Wt+1‖∗ ≤ √ 2r‖Wt −Wt+1‖F . As a result, we have\n‖Wt −Wt+1‖2∗ ≤ 2r‖Wt −Wt+1‖2F (8)\n≤4r ( η2t 2 ‖Ĝt‖2F + ηtλ‖Wt‖∗ − ηtλ‖Wt+1‖∗ ) ≤ 2rη2t ‖Ĝt‖2F + 4rηtλ‖Wt −Wt+1‖∗\n(9)\nRecall that for x, b, c ≥ 0 x2 ≤ bx+ c ⇒ x ≤ 2b+ √ 2c.\nFrom (9), we have\n‖Wt −Wt+1‖∗ ≤ 8rηtλ+ 2ηt‖Ĝt‖F √ r (10)\nCombining (7) and (10), we have\nf(Wt) + λ‖Wt‖∗ − f(W )− λ‖W‖∗\n≤ 1 2ηt ‖W −Wt‖2F − 1 2ηt ‖W −Wt+1‖2F + 〈Wt −W,E[Ĝt]− Ĝt〉\n+ ( ‖Ĝt‖2F + 16rλ2 + 4λ‖Ĝt‖F √ r ) ηt\n2\n(11)\nBy Jensen’s inequality, we have\n(E[‖Ĝt‖F ])2 ≤ E[‖Ĝt‖2F ] ≤ G2 ⇒ E[‖Ĝt‖F ] ≤ G.\nTaking expectation over both sides of (11), we have\nE [F (Wt)− F (W )]\n≤ 1 2ηt\nE [ ‖W −Wt‖2F ] − 1 2ηt E [ ‖W −Wt+1‖2F ] + ηt 2 ( G2 + 16rλ2 + 4λG √ r )\n+ E [ 〈Wt −W,E[Ĝt]− Ĝt〉\n] (12)\nChoosing W = WT−k, for any t ≥ T − k, we have\nE [F (Wt)− F (WT−k)]\n≤ 1 2ηt\nE [ ‖WT−k −Wt‖2F ] − 1 2ηt E [ ‖WT−k −Wt+1‖2F ] + ηt 2 ( G2 + 16rλ2 + 4λG √ r ) (13)\nwhere we use the fact\nE [ 〈Wt −WT−k,E[Ĝt]− Ĝt〉 ] = 0, ∀t ≥ T − k.\nSumming over t = T − k, . . . , T , and rearranging, we get\nE\n[ T∑\nt=T−k F (Wt)− F (WT−k)\n]\n≤ T∑\nt=T−k+1\nE [ ‖WT−k −Wt‖2F ]\n2\n( 1\nηt − 1 ηt−1\n) + 1\n2\n( G2 + 16rλ2 + 4λG √ r ) T∑\nt=T−k ηt\n≤ T∑\nt=T−k+1\nD2\n2\n( 1\nηt − 1 ηt−1\n) + 1\n2\n( G2 + 16rλ2 + 4λG √ r ) T∑\nt=T−k ηt\n= T∑\nt=T−k+1\nD2\n2c\n(√ t− √ t− 1 ) + 1\n2\n( G2 + 16rλ2 + 4λG √ r ) T∑\nt=T−k\nc√ t\n≤D 2\n2c\n(√ T − √ T − k ) + c ( G2 + 16rλ2 + 4λG √ r ) (√ T − √ T − k − k )\n≤ ( D2\n2c + c\n( G2 + 16rλ2 + 4λG √ r ))(√ T − √ T − k − k )\n≤ ( D2\n2c + c\n( G2 + 16rλ2 + 4λG √ r )) k + 1√\nT\n(14)\nwhere in the fifth line we use the inequality ∑T\nt=T−k 1√ t ≤ 2(\n√ T − √ T − k − 1).\nThen, it is straightforward to prove this theorem by following the arguments of Theorem 2 in Shamir and Zhang (2012). Specifically, we just need to replace (5) in Shamir and Zhang (2012) with (14) in this paper, and the rest is identical."
    }, {
      "heading" : "3.2 Proof of Lemma 3",
      "text" : "The proof of is similar to that of Lemma 1 in (Rakhlin et al., 2012), which is devoted to analyze the behavior of SGD.\nUsing the fact that f(·) is µ-strongly convex, (11) becomes\nf(Wt) + λ‖Wt‖∗ − f(W )− λ‖W‖∗\n≤1 2\n( 1\nηt − µ\n) ‖W −Wt‖2F − 1\n2ηt ‖W −Wt+1‖2F + 〈Wt −W,E[Ĝt]− Ĝt〉\n+ ( ‖Ĝt‖2F + 16rλ2 + 4λ‖Ĝt‖F √ r ) ηt\n2\n(15)\nfor all W ∈ W. On the other hand, the strongly convexity convexity also implies\nF (Wt)− F (W∗) ≥ µ\n2 ‖Wt −W∗‖2F (16)\nFrom (15) and (16), we have\n‖W∗ −Wt+1‖2F ≤ (1− 2µηt) ‖W∗ −Wt‖2F + 2ηt〈Wt −W∗,E[Ĝt]− Ĝt〉\n+ ( ‖Ĝt‖2F + 16rλ2 + 4λ‖Ĝt‖F √ r ) η2t\nTaking expectation over both sides and plugging in ηt = 1/(µt), we have\nE [ ‖W∗ −Wt+1‖2F ] ≤ ( 1− 2\nt\n) E [ ‖W∗ −Wt‖2F ] + ( G2 + 16rλ2 + 4λG √ r ) 1 µ2t2\n(17)\nNext, we provide an upper bound for ‖W∗ −W1‖F . From strong convexity and the fact W1 = 0, we have\nµ 2 ‖W1 −W∗‖2F ≤F (W1)− F (W∗) = f(W1) + λ‖W1‖∗ − f(W∗)− λ‖W∗‖∗\n≤f(W1)− f(W∗) ≤ 〈E[Ĝ1],W1 −W∗〉 ≤ ‖E[Ĝ1]‖F ‖W1 −W∗‖F\nwhich implies\n‖W1 −W∗‖2F ≤ 4\nµ2 ‖E[Ĝ1]‖2F ≤\n4\nµ2 E[‖Ĝ1‖2F ] ≤\n4G2\nµ2 (18)\nWe complete the proof by a simple induction argument based on (17) and (18)."
    }, {
      "heading" : "3.3 Proof of Theorem 4",
      "text" : "The proof is similar to that of Theorem 1 in Shamir and Zhang (2012), and thus we just show the difference.\nFollowing the derivation of (14), for all W ∈ W, we have\nE\n[ T∑\nt=T−k F (Wt)− F (W )\n] ≤ E [ ‖W −WT−k‖2F ]\n2ηT−k\n+\nT∑\nt=T−k+1\nE [ ‖W −Wt‖2F ]\n2\n( 1\nηt − 1 ηt−1\n) + 1\n2\n( G2 + 16rλ2 + 4λG √ r ) T∑\nt=T−k ηt\n(19)\nprovided\nE [ 〈Wt −W,E[Ĝt]− Ĝt〉 ] = 0, ∀t ≥ T − k.\nSubstituting ηt = 1/(µt), (19) becomes\nE\n[ T∑\nt=T−k F (Wt)− F (W )\n] ≤ µ(T − k)\n2 E [ ‖W −WT−k‖2F ]\n+ µ\n2\nT∑ t=T−k+1 E [ ‖W −Wt‖2F ] + 1 2µ ( G2 + 16rλ2 + 4λG √ r ) T∑ t=T−k 1 t\n(20)\nThen, Theorem 4 can be proved by replacing (2) in Shamir and Zhang (2012) with (20) in this paper."
    }, {
      "heading" : "4. Conclusion",
      "text" : "In this paper, we propose an memory-efficient algorithm for solving the nuclear norm regularized problem in (1). Specifically, in each iteration it constructs a low-rank stochastic\ngradient and updates the intermediate iterate by SPGD. Compared to previous studies, there are two advantages of the proposed algorithm: i) it space complexity is O(m + n) instead of O(mn) and ii) it is equipped with a formal convergence rate.\nThe space complexity of our algorithm also depends on the rank of the intermediate iterates Wt. AlthoughWt tends to be a low-rank matrix due to the nuclear norm regularizer, an explicit upper bound about its rank is unknown. This issue will be studied as a future work. We will also investigate whether it is possible to derive efficient updating for other domains of matrices, such as the spectral norm ball."
    }, {
      "heading" : "Appendix A. Proof of Theorem 2",
      "text" : "We consider two cases: ‖Dλ[Y ]‖F ≤ R and ‖Dλ[Y ]‖F > R.\nA.1 ‖Dλ[Y ]‖F ≤ R From Theorem 1, we know that Dλ[Y ] is the optimal solution to a unconstrained optimization problem in (3). Thus, if Dλ[Y ] ≤ R, it is also the optimal solution to (4).\nA.2 ‖Dλ[Y ]‖F > R We denote the singular values of Y by σ1, σ2, . . .. From (3), we know\nmin X\n1 2 ‖X − Y ‖2F + λ‖X‖∗\n= 1\n2 ‖Dλ[Y ]− Y ‖2F + λ‖Dλ[Y ]‖∗ =\n∑\nσi≥λ\n( λσi − 1\n2 λ2\n) + ∑\nσi<λ\n1 2 σ2i\n(21)\nFollowing the standard analysis of convex optimization (Boyd and Vandenberghe, 2004), we introduce a dual variable µ for the constraint and obtain the Lagrange dual function\nL(µ) =min X\n1 2 ‖X − Y ‖2F + λ‖X‖∗ + µ(‖X‖2F −R2)\n=(1 + 2µ)min X\n( 1\n2\n∥∥∥∥X − 1\n1 + 2µ Y\n∥∥∥∥ 2\nF\n+ λ\n1 + 2µ ‖X‖∗\n) +\n2µ\n2(1 + 2µ) ‖Y ‖2F − µR2\n(21) =\n1\n1 + 2µ\n ∑\nσi≥λ\n( λσi − 1\n2 λ2\n) + ∑\nσi<λ\n1 2 σ2i\n + 2µ\n2(1 + 2µ) ‖Y ‖2F − µR2\n= 1\n1 + 2µ\n ∑\nσi≥λ\n( λσi − 1\n2 λ2\n) + ∑\nσi<λ\n1 2 σ2i\n − 1\n2(1 + 2µ) ‖Y ‖2F − µR2 +\n1 2 ‖Y ‖2F\n=− 1 1 + 2µ  1 2 ∑\nσi≥λ (σi − λ)2\n − µR2 + 1\n2 ‖Y ‖2F\n=− 1 2(1 + 2µ) ‖Dλ[Y ]‖2F − µR2 + 1 2 ‖Y ‖2F .\nAs a result, the Lagrange dual problem is\nmax µ≥0 − 1 2(1 + 2µ) ‖Dλ[Y ]‖2F − µR2,\nand it is easy to verify the optimal dual solution µ∗ satisfies\n1\n1 + 2µ∗ =\nR\n‖Dλ[Y ]‖F .\nThen, we can recover the optimal primal solution from µ∗ in the following way\nX∗ =argmin X\n1 2 ‖X − Y ‖2F + λ‖X‖∗ + µ∗‖X‖2F\n=argmin X\n1\n2\n∥∥∥∥X − 1\n1 + 2µ∗ Y\n∥∥∥∥ 2\nF\n+ λ\n1 + 2µ∗ ‖X‖∗\n=D λ 1+2µ∗\n[ 1\n1 + 2µ∗ Y\n] =\n1\n1 + 2µ∗ Dλ[Y ] =\nR\n‖Dλ[Y ]‖F Dλ[Y ]."
    } ],
    "references" : [ {
      "title" : "Efficient and practical stochastic subgradient descent for nuclear norm regularization",
      "author" : [ "Haim Avron", "Satyen Kale", "Shiva Kasiviswanathan", "Vikas Sindhwani" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Avron et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Avron et al\\.",
      "year" : 2012
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen Boyd", "Lieven Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2004
    }, {
      "title" : "Fast low-rank modifications of the thin singular value decomposition",
      "author" : [ "Matthew Brand" ],
      "venue" : "Linear Algebra and its Applications,",
      "citeRegEx" : "Brand.,? \\Q2006\\E",
      "shortCiteRegEx" : "Brand.",
      "year" : 2006
    }, {
      "title" : "A singular value thresholding algorithm for matrix completion",
      "author" : [ "Jian-Feng Cai", "Emmanuel J. Candès", "Zuowei Shen" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Cai et al\\.,? \\Q1956\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 1956
    }, {
      "title" : "Efficient low-rank stochastic gradient descent methods for solving semidefinite programs",
      "author" : [ "Jianhui Chen", "Tianbao Yang", "Shenghuo Zhu" ],
      "venue" : "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "In Proceedings of the 24th Annual Conference on Learning Theory,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2011
    }, {
      "title" : "Accelerated gradient methods for stochastic optimization and online learning",
      "author" : [ "Chonghai Hu", "James Kwok", "Weike Pan" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2009
    }, {
      "title" : "An optimal method for stochastic composite optimization",
      "author" : [ "Guanghui Lan" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Lan.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lan.",
      "year" : 2012
    }, {
      "title" : "A sparsity preserving stochastic gradient method for composite optimization",
      "author" : [ "Qihang Lin", "Xi Chen", "Javier Peña" ],
      "venue" : "Computational Optimization and Applications,",
      "citeRegEx" : "Lin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2011
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Nemirovski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nemirovski et al\\.",
      "year" : 2009
    }, {
      "title" : "Gradient methods for minimizing composite functions",
      "author" : [ "Yu. Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Nesterov.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2013
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization",
      "author" : [ "Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Rakhlin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rakhlin et al\\.",
      "year" : 2012
    }, {
      "title" : "Stochastic convex optimization",
      "author" : [ "Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "In Proceedings of the 22nd Annual Conference on Learning Theory,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2009
    }, {
      "title" : "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes",
      "author" : [ "Ohad Shamir", "Tong Zhang" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Shamir and Zhang.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shamir and Zhang.",
      "year" : 2012
    }, {
      "title" : "O(log T ) projections for stochastic optimization of smooth and strongly convex functions",
      "author" : [ "Lijun Zhang", "Tianbao Yang", "Rong Jin", "Xiaofei He" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Stochastic optimization has received lots of attention recently in data mining and machine learning communities (Hu et al., 2009; Shalev-Shwartz et al., 2009; Zhang et al., 2013).",
      "startOffset" : 112,
      "endOffset" : 178
    }, {
      "referenceID" : 12,
      "context" : "Stochastic optimization has received lots of attention recently in data mining and machine learning communities (Hu et al., 2009; Shalev-Shwartz et al., 2009; Zhang et al., 2013).",
      "startOffset" : 112,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : "Stochastic optimization has received lots of attention recently in data mining and machine learning communities (Hu et al., 2009; Shalev-Shwartz et al., 2009; Zhang et al., 2013).",
      "startOffset" : 112,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "It is generally formulated as a minimization problem where the objective is an expectation over unknown distributions (Nemirovski et al., 2009), or defined in terms of the access model, which assumes there is a stochastic oracle that generates unbiased estimate of the objective or its gradient (Hazan and Kale, 2011).",
      "startOffset" : 118,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : ", 2009), or defined in terms of the access model, which assumes there is a stochastic oracle that generates unbiased estimate of the objective or its gradient (Hazan and Kale, 2011).",
      "startOffset" : 159,
      "endOffset" : 181
    }, {
      "referenceID" : 10,
      "context" : "Note that the above problem is a special case of convex composite optimization (Nesterov, 2013).",
      "startOffset" : 79,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "This setting excludes deterministic methods for solving (1) (Nesterov, 2013), as they need to evaluate the gradient of f(·), which needs O(mn) memory.",
      "startOffset" : 60,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "Although various stochastic algorithms has been proposed for convex composite optimization and can also be applied to (1), their primal goal is to reduce the time complexity of evaluating the gradient of f(·) instead of the space complexity (Lin et al., 2011; Lan, 2012).",
      "startOffset" : 241,
      "endOffset" : 270
    }, {
      "referenceID" : 7,
      "context" : "Although various stochastic algorithms has been proposed for convex composite optimization and can also be applied to (1), their primal goal is to reduce the time complexity of evaluating the gradient of f(·) instead of the space complexity (Lin et al., 2011; Lan, 2012).",
      "startOffset" : 241,
      "endOffset" : 270
    }, {
      "referenceID" : 11,
      "context" : "Built upon recent progresses in stochastic optimization (Rakhlin et al., 2012; Shamir and Zhang, 2012), we further analyze the convergence property of the proposed algorithm, and show that the last iterate has an O(log T/ √ T ) convergence rate for general convex functions, and an O(log T/T ) rate for strongly convex functions.",
      "startOffset" : 56,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "Built upon recent progresses in stochastic optimization (Rakhlin et al., 2012; Shamir and Zhang, 2012), we further analyze the convergence property of the proposed algorithm, and show that the last iterate has an O(log T/ √ T ) convergence rate for general convex functions, and an O(log T/T ) rate for strongly convex functions.",
      "startOffset" : 56,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "The only memory-efficient algorithm for solving (1) that we found in the literature is a heuristic algorithm developed by Avron et al. (2012). Under the condition that it is possible to generate a stochastic gradient of f(·) which is also low-rank, they propose to combine SGD and truncated SVD to solve (1).",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "The only memory-efficient algorithm for solving (1) that we found in the literature is a heuristic algorithm developed by Avron et al. (2012). Under the condition that it is possible to generate a stochastic gradient of f(·) which is also low-rank, they propose to combine SGD and truncated SVD to solve (1). By representing all the intermediate solutions and stochastic gradients in low-rank factorization forms, the space complexity can be reduced to O(m+n). The major limitation of this approach is that there is no theoretical guarantee about its convergence, due to the fact that the truncated SVD operation introduces a constant error in each iteration. Furthermore, when the objective value is difficult to calculate, it is also unclear which iterate should be used as the final solution. Inspired by the previous studies, we develop a novel stochastic algorithm that optimizes (1) in a memory-efficient way and is supported by formal theoretical guarantees. Specifically, we propose to use stochastic proximal gradient descent (SPGD) instead of SGD to solve (1), and take the last iterate of SPGD as the final solution. Similar to the heuristic algorithm of Avron et al. (2012), the proposed algorithm always maintains a low-rank factorization of iterates that can be conveniently held in memory, and both the time and space complexities in each iteration are O(m+n).",
      "startOffset" : 122,
      "endOffset" : 1186
    }, {
      "referenceID" : 0,
      "context" : "The existence and construction of such a low-rank matrix can be found in previous studies (Avron et al., 2012; Chen et al., 2014), and will be discussed later.",
      "startOffset" : 90,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : "The existence and construction of such a low-rank matrix can be found in previous studies (Avron et al., 2012; Chen et al., 2014), and will be discussed later.",
      "startOffset" : 90,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "The existence and construction of such a low-rank matrix can be found in previous studies (Avron et al., 2012; Chen et al., 2014), and will be discussed later. Denote by Wt the solution at the t-th iteration, and let Ĝt = AtB ⊤ t be the low-rank stochastic gradient of f(·) at Wt. Then, we update the current solution by the SPGD, which is a stochastic variant of composite gradient mapping Nesterov (2013)",
      "startOffset" : 91,
      "endOffset" : 407
    }, {
      "referenceID" : 0,
      "context" : "1 Construction of the Low-rank Stochastic Gradients For a given matrix W , there are various ways to construct a low-rank stochastic gradient Ĝ satisfying E[Ĝ] ∈ ∂f(W ) (Avron et al., 2012; Chen et al., 2014).",
      "startOffset" : 169,
      "endOffset" : 208
    }, {
      "referenceID" : 4,
      "context" : "1 Construction of the Low-rank Stochastic Gradients For a given matrix W , there are various ways to construct a low-rank stochastic gradient Ĝ satisfying E[Ĝ] ∈ ∂f(W ) (Avron et al., 2012; Chen et al., 2014).",
      "startOffset" : 169,
      "endOffset" : 208
    }, {
      "referenceID" : 0,
      "context" : "1 Construction of the Low-rank Stochastic Gradients For a given matrix W , there are various ways to construct a low-rank stochastic gradient Ĝ satisfying E[Ĝ] ∈ ∂f(W ) (Avron et al., 2012; Chen et al., 2014). For brevity, we just describe a general approach. To this end, we need the following definition from Avron et al. (2012). Definition 1 (Probing Matrix) A random n×k matrix Y is a probing matrix if E[Y Y ⊤] = I.",
      "startOffset" : 170,
      "endOffset" : 331
    }, {
      "referenceID" : 11,
      "context" : "Analysis The convergence of the last iterate of SGD has been analyzed by Rakhlin et al. (2012) and Shamir and Zhang (2012).",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : "Analysis The convergence of the last iterate of SGD has been analyzed by Rakhlin et al. (2012) and Shamir and Zhang (2012). By extending their analysis, we provide the convergence of SPGD for solving the nuclear norm regularized problem in (1).",
      "startOffset" : 73,
      "endOffset" : 123
    }, {
      "referenceID" : 12,
      "context" : "1 Proof of Theorem 3 The proof is an extension of Theorem 2 in Shamir and Zhang (2012), which establishes a similar guarantee for the last iterate of SGD.",
      "startOffset" : 63,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : ", (2) of Hazan and Kale (2011), the updating rule in (2) implies 1 2 ‖Wt+1 −Wt‖F + ηt〈Wt+1 −Wt, Ĝt〉+ ηtλ‖Wt+1‖∗ ≤ 2 ‖W −Wt‖F + ηt〈W −Wt, Ĝt〉+ ηtλ‖W‖∗ − 1 2 ‖W −Wt+1‖F for all W ∈ W.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "Then, it is straightforward to prove this theorem by following the arguments of Theorem 2 in Shamir and Zhang (2012). Specifically, we just need to replace (5) in Shamir and Zhang (2012) with (14) in this paper, and the rest is identical.",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "Then, it is straightforward to prove this theorem by following the arguments of Theorem 2 in Shamir and Zhang (2012). Specifically, we just need to replace (5) in Shamir and Zhang (2012) with (14) in this paper, and the rest is identical.",
      "startOffset" : 93,
      "endOffset" : 187
    }, {
      "referenceID" : 11,
      "context" : "2 Proof of Lemma 3 The proof of is similar to that of Lemma 1 in (Rakhlin et al., 2012), which is devoted to analyze the behavior of SGD.",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 13,
      "context" : "3 Proof of Theorem 4 The proof is similar to that of Theorem 1 in Shamir and Zhang (2012), and thus we just show the difference.",
      "startOffset" : 66,
      "endOffset" : 90
    }, {
      "referenceID" : 13,
      "context" : "Then, Theorem 4 can be proved by replacing (2) in Shamir and Zhang (2012) with (20) in this paper.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "Following the standard analysis of convex optimization (Boyd and Vandenberghe, 2004), we introduce a dual variable μ for the constraint and obtain the Lagrange dual function",
      "startOffset" : 55,
      "endOffset" : 84
    } ],
    "year" : 2015,
    "abstractText" : "In this paper, we utilize stochastic optimization to reduce the space complexity of convex composite optimization with a nuclear norm regularizer, where the variable is a matrix of size m × n. By constructing a low-rank estimate of the gradient, we propose an iterative algorithm based on stochastic proximal gradient descent (SPGD), and take the last iterate of SPGD as the final solution. The main advantage of the proposed algorithm is that its space complexity is O(m + n), in contrast, most of previous algorithms have a O(mn) space complexity. Theoretical analysis shows that it achieves O(log T/ √ T ) and O(log T/T ) convergence rates for general convex functions and strongly convex functions, respectively.",
    "creator" : "LaTeX with hyperref package"
  }
}