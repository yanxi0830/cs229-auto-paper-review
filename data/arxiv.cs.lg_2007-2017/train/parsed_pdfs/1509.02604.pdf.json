{
  "name" : "1509.02604.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Asynchronous Distributed ADMM for Large-Scale Optimization- Part II: Linear Convergence Analysis and Numerical Performance",
    "authors" : [ "Tsung-Hui Chang", "Wei-Cheng Liao", "Mingyi Hong", "Xiangfeng Wang" ],
    "emails" : [ "tsunghui.chang@ieee.org.", "mhong@umn.edu", "mingyi@iastate.edu", "xfwang@sei.ecnu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 9.\n02 60\n4v 1\n[ cs\n.D C\n] 9\nS ep\nKeywords− Distributed optimization, ADMM, Asynchronous, Consensus optimization\n⋆Tsung-Hui Chang is the corresponding author. Address: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 518172, E-mail: tsunghui.chang@ieee.org.\n†Wei-Cheng Liao is with Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455, USA, E-mail: mhong@umn.edu\n†Mingyi Hong is with Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, 50011, USA, E-mail: mingyi@iastate.edu\n‡Xiangfeng Wang is with Shanghai Key Lab for Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, 200062, China, E-mail: xfwang@sei.ecnu.edu.cn\nSeptember 10, 2015 DRAFT\n2 I. INTRODUCTION\nConsider the following optimization problem\nmin x∈Rn\nN∑\ni=1\nfi(x) + h(x), (1)\nwhere each fi : Rn → R is the cost function and h : Rn → R ∪ {∞} is a non-smooth, convex regularization function. The regularization function is used for obtaining structured solutions (e.g., sparsity) and/or is an indicator function which enforces x to lie in a constraint set [2, Section 5]. Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.\nDistributed optimization algorithms that can scale well with large-scale instances of (1) have drawn significant attention in recent years [2], [7]–[14]. Our interest in this paper lies in the distributed optimization method based on the alternating direction method of multipliers (ADMM) [2, Section 7.1.1]. The ADMM is a convenient approach of distributing the computation load of a very large-scale problem to a network of computing nodes. Specifically, consider a computer network with a star topology, where one master node coordinates the computation of a set of N distributed workers. Based on a consensus formulation, the distributed ADMM partitions the original problem into N subproblems, each of which contains either a small set of training samples or a subset of the learning parameters. At each iteration, the distributed workers solve the subproblems based on the local data and send the variable information to the master, who summarizes the variable information and broadcasts it back the workers. Through such iterative variable update and information exchange, the large-scale learning problem can be solved in a distributed and parallel manner.\nThe convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]–[20]. For example, for general convex problems, references [2], [7] showed that the ADMM is guaranteed to converge to an optimal solution and [15] showed that the ADMM has a worst-case O(1/k) convergence rate, where k is the iteration number. Considering non-convex problems with smooth fi’s, reference [16] presented conditions for which the distributed ADMM converges to the set of Karush-KuhnTucker (KKT) points. For problems with strongly convex and smooth fi’s or problems satisfying certain error bound condition, references [17] and [21] respectively showed that the ADMM can even exhibit a linear convergence rate. References [18]–[20] also showed similar linear convergence conditions for some variants of distributed ADMM in a network with a general topology. However, the distributed ADMM in [2], [16] have assumed a synchronous network, where at each iteration, the master always waits until all\nSeptember 10, 2015 DRAFT\n3 the workers report their variable information. Unfortunately, such synchronous protocol does not scale well with the problem size, as the algorithm speed is determined by the “slowest” workers. To improve the time efficiency, the works [22], [23] have generalized the distributed ADMM to an asynchronous network. Specifically, in the asynchronous distributed ADMM (AD-ADMM) proposed in [22], [23], the master does not necessarily wait for all the workers. Instead, the master updates its variable whenever it receives the variable information from a partial set of the workers. This prevents the master and speedy workers from spending most of the time waiting and consequently can improve the time efficiency of distributed optimization. Theoretically, it has been shown in [23] that the AD-ADMM is guaranteed to converge (to a KKT point) even for non-convex problem (1), under a bounded delay assumption only.\nThe contributions of this paper are twofold. Firstly, beyond the convergence analysis in [23], we further present the conditions for which the AD-ADMM can exhibit a linear convergence rate. Specifically, we show that for problem (1) with some structured convex fi’s (e.g., strongly convex), the augmented Lagrangian function of the AD-ADMM can decrease by a constant fraction in every iteration of the algorithm, as long as the algorithm parameters are chosen appropriately according to the network delay. We give explicit expressions on the linear convergence conditions and the linear rate, which illustrate how the algorithm and network parameters impact on the algorithm performance. To the best of our knowledge, our results are novel, and are by no means extensions of the existing analyses [17]–[21] for synchronous ADMM. Secondly, we present extensive numerical results to demonstrate the time efficiency of the AD-ADMM over its synchronous counterpart. In particular, we consider a large-scale LR problem and implement the AD-ADMM on a high-performance computer cluster. The presented numerical results show that the AD-ADMM significantly reduces the practical running time of distributed optimization.\nSynopsis: Section II reviews the AD-ADMM in [23]. The linear convergence analysis is presented in Section III and the proofs are presented in Section IV. Numerical results are given in Section V and conclusions are drawn in Section VI.\nII. ASYNCHRONOUS DISTRIBUTED ADMM\nIn this section, we review the AD-ADMM proposed in [23]. The distributed ADMM [2, Section 7.1.1]\nis derived based on the following consensus formulation of (1):\nmin x0,xi∈Rn, i=1,...,N\nN∑\ni=1\nfi(xi) + h(x0) (2a)\ns.t. xi = x0 ∀i ∈ V , {1, . . . , N}. (2b)\nSeptember 10, 2015 DRAFT\n4 By applying the standard ADMM [7] to problem (2), one obtains the following three simple steps: for iteration k = 0, 1, . . . , update\nxk+10 =arg min x0∈Rn\n{ h(x0)− x T 0 ∑N i=1 λ k i + ρ 2 ∑N i=1 ‖x k i − x0‖ 2 } , (3)\nxk+1i =arg min xi∈Rn fi(xi) + x T i λ k i + ρ 2‖xi − x k+1 0 ‖ 2 ∀i ∈ V, (4) λk+1i = λ k i + ρ(x k+1 i − x k+1 0 ) ∀i ∈ V. (5)\nAs seen, the distributed ADMM is designed for a computing network with a star topology that consists of one master node and a set of N workers (see Fig. 1 in [23]). In particular, the master is responsible for optimizing the variable x0 by (3), while each worker i, i ∈ V , takes charge of optimizing variables xi and λi by (4) and (5), respectively. Once the master updates x0, it broadcasts x0 to the workers; each worker i then updates (xi,λi) based on the received x0, and sends the new (xi,λi) to the master. Through such iterative variable update and message exchange, problem (2) is solved in a fully parallel and distributed fashion.\nHowever, to implement (3)-(5), the master and the workers have to be synchronized with each other. Specifically, according to (3), the master proceeds to update x0 only if it has received update-to-date (xi,λi) from all the workers. This implies that the optimization speed would be determined by the slowest worker in the network. This is in particular the case in a heterogeneous network where the workers experience different computation and communication delays, in which case the master and speedy workers would idle most of the time.\nThe distributed ADMM has been extended to an asynchronous network in [22], [23]. In the ADADMM, the master does not wait for all the workers, but updates the variable x0 as long as it receives variable information from a partial set of workers instead. This would greatly reduce the waiting time of the master, and improve the overall time efficiency of distributed optimization. The AD-ADMM is presented in Algorithm 1, which includes the algorithmic steps of the master and those of the workers. Here, we denote k as the iteration number of the master (i.e., the number of times for which the master updates x0), and assume that, at each iteration k, the master receives variable information from workers in the set Ak ⊆ V , {1, . . . , N}. Worker i is said to be “arrived” at iteration k if i ∈ Ak and unarrived otherwise. Notation Ack denotes the complementary set of Ak, i.e., Ak ∩ A c k = ∅ and Ak ∪ A c k = V . Moreover, variables di’s are used to count the numbers of delayed iterations of the workers. The variables ρ and γ are two penalty parameters.\nSeptember 10, 2015 DRAFT\n5 In the AD-ADMM, the master inevitably uses delayed and old variable information for updating x0. As shown in step 4 of Algorithm of the Master, to ensure the used variable information not too stale, the master would wait until it receives the update-to-date (xi,λi) from all the workers that have di ≥ τ − 1, if any (so all the workers i ∈ Ack must have di < τ − 1). This condition guarantees that the variable information is at most τ iterations old, and is known as the partially asynchronous model [7]:\nAssumption 1 (Bounded delay) Let τ ≥ 1 be a maximum tolerable delay. For all i ∈ V and iteration k, it must be that i ∈ Ak ∪ Ak−1 · · · ∪ Ak−τ+1.\nIn [23, Theorem 1], we have shown that under Assumption 1, some smoothness conditions on the cost functions fi’s (see [23, Assumption 2]) and for sufficiently large ρ and γ, the AD-ADMM in Algorithm 1 is provably convergent to the set of KKT points of problem (2). Notably, this convergence property holds even for non-convex fi’s. In the next section, we focus on convex fi’s, and further characterize the linear convergence conditions of the AD-ADMM.\nIII. LINEAR CONVERGENCE RATE ANALYSIS\nIn this section, we show that the AD-ADMM can achieve linear convergence for some structured convex functions. We first make the following convex assumption on problem (1) (or equivalently, problem (2)).\nAssumption 2 Each function fi is a proper closed convex function and is continuously differentiable; each gradient ∇fi is Lipschitz continuous with a Lipschitz constant L > 0; the function h is convex (not necessarily smooth). Moreover, problem (1) is bounded below, i.e., F ⋆ > −∞ where F ⋆ denotes the optimal objective value of problem (1).\nAssumption 2 is the same as [23, Assumption 2], except that fi’s are assumed convex here. Given this\nconvex property, it is well known that the augmented Lagrangian function, i.e.,\nLρ(x k,xk0 ,λ k) =\nN∑\ni=1\nfi(x k i ) + h(x k 0) +\nN∑\ni=1\n(λki ) T (xki − x k 0)\n+ ρ\n2\nN∑\ni=1\n‖xki − x k 0‖ 2, (12)\nwould converge to F ⋆ whenever the iterates ({xki } N i=1,x k 0 , {λ k i } N i=1) approaches the optimal solution of problem (2). Therefore, our analysis is based on characterizing how Lρ(xk,xk0 ,λ k) can converge to F ⋆ linearly. Let us define\n△k , Lρ(x k,xk0 ,λ k)− F ⋆. (13)\nSeptember 10, 2015 DRAFT\n6 It has been shown in [23, Lemma 3] that △k ≥ 0 for all k as long as ρ ≥ L.\nIn the ensuing analysis, we consider two types of structured convex cost functions, respectively\ndescribed in the following two assumptions.\nAssumption 3 For all i ∈ V, each function fi is strongly convex with modulus σ2 > 0.\nAssumption 4 Each function fi(x) = gi(Aix), ∀i ∈ V, where gi : Rm → R is a strongly convex function with modulus σ2 > 0 and Ai ∈ Rm×n is a nonzero matrix with arbitrary rank. Moreover, h(x) = 0.\nNote that in Assumption 4 matrix Ai can have an arbitrary rank, so fi(x) is not necessarily strongly convex with respect to x. Interestingly, such structured cost function appears in many machine learning problems, for example, the least squared problem and the logistic regression problem [5].\nLet us first consider the strongly convex case. Under Assumption 3, the linear convergence conditions\nof the AD-ADMM are given by the following theorem.\nTheorem 1 Suppose that Assumptions 1, 2 and 3 hold true. Moreover, assume that there exists a constant S ∈ [1, N ] such that |Ak| < S for all k and that\nρ ≥ max\n{ (1 + L2) + √ (1 + L2)2 + 8L2α(τ)\n2 , σ2 +\n1\n8N\n} , (14)\nγ ≥ max { β(ρ, τ) − Nρ\n2 + 1, 8N(ρ − σ2)\n} , (15)\nwhere α(τ) , 1 + 2+2 τ (τ−1) 1+8Nσ2 and β(ρ, τ) , 2(τ − 1)[( (1+ρ2)S+S/N 2 )(2 τ−1 − 1) + (4τ−1 − 1)]. Then, the iterates generated by (6), (7) and (9) satisfy\n0 ≤ △k+1 ≤\n( 1\n1 + 1δγ\n)k+1 △0, (16)\nwhere δ is a constant satisfying\nδ ≥ max { 1, ρN + γ\nσ2N − 1\n} . (17)\nTheorem 1 asserts that, for problem (2) with strongly convex fi’s, the augmented Lagrange function can decrease linearly to zero, as long as ρ and γ are large enough (exponentially increasing with τ ). Equation (16) also implies that the linear rate would decrease with the delay τ and the number of workers in the worst case.\nAnalogous to Theorem 1, the following theorem shows that the AD-ADMM can achieve linear\nconvergence under Assumption 4.\nSeptember 10, 2015 DRAFT\n7 Theorem 2 Suppose that Assumptions 1, 2 and 4 hold true. Moreover, assume that there exists a constant S ∈ [1, N ] such that |Ak| < S for all k and that\nρ ≥ max\n{ (1 + L2) + √ (1 + L2)2 + 8L2α(τ)\n2 , σ2 +\n1\n8N\n} ,\nγ ≥ max { β(ρ, τ) − Nρ\n2 + 1, 8N(ρ − σ2/c) + 4Nσ2\n} ,\nfor some constant c > 0. Then, the iterates generated by (6), (7) and (9) satisfy (16) with δ satisfying\nδ ≥ max { 1, ρN + γ\nNσ2/c − 1\n} .\nSince it has been known that the (synchronous) distributed ADMM [17]–[21] can converge linearly given the same structured cost functions in Assumption 3 and Assumption 4, the convergence results presented above demonstrate that the linear convergence property can be preserved in the asynchronous network. We remark that (14) and (15) are sufficient conditions only. In practice, the AD-ADMM could still exhibit a linear convergence rate without exactly satisfying these conditions.\nThe proofs of Theorem 1 and Theorem 2 are presented in the next section. The readers who are more\ninterested in the numerical performance of the AD-ADMM may jump to Section V.\nIV. PROOFS OF THEOREMS\nA. Preliminaries and Key Lemmas\nLet us present some basic inequalities that will be used frequently in the ensuing analysis and key\nlemmas for proving Theorem 1 and Theorem 2.\nWe will frequently use the following inequality due to Jensen’s inequality: for any ai, i = 1, . . . ,M ,\n‖ ∑M\ni=1 ai‖ 2 ≤ M ∑M i=1 ‖ai‖ 2. (18)\nMoreover, for any a, b and δ > 0,\n‖a + b‖2 ≤ (1 + δ)‖a‖2 + (1 + 1\nδ )‖b‖2. (19)\nThe equality is also known to be true: for any vectors a, b, c and d,\n(a− b)T (c− d) = 1\n2 ‖a− d‖2 −\n1 2 ‖a− c‖2\n+ 1\n2 ‖b− c‖2 −\n1 2 ‖b− d‖2. (20)\nSeptember 10, 2015 DRAFT\n8 We follow [23, Algorithm 3] to write Algorithm 1 from the master’s point of view as follows:\nxk+1i =    arg min xi∈Rn { fi(xi) + x T i λ k i + ρ 2‖xi − x k̄i+1 0 ‖ 2 } , ∀i ∈ Ak\nxki ∀i ∈ A c k\n, (21)\nλk+1i =\n{ λki + ρ(x k+1 i − x k̄i+1 0 ) ∀i ∈ Ak\nλki ∀i ∈ A c k\n, (22)\nxk+10 =arg min x0∈Rn\n{ h(x0)− x T 0 ∑N i=1 λ k+1 i\n+ ρ2 ∑N i=1 ‖x k+1 i − x0‖ 2 + γ2‖x0 − x k 0‖ 2 } . (23)\nHere, index k̄i in (21) and (22) represents the last iteration number before iteration k for which worker i ∈ Ak is arrived, i.e., i ∈ Ak̄i . Under Assumption 1, it must hold\nk − τ ≤ k̄i < k ∀k. (24)\nFurthermore, for workers i ∈ Ack, let us denote k̃i as the last iteration number before iteration k for which worker i is arrived, i.e., i ∈ Ak̃i . Then, under Assumption 1, it must hold\nk − τ < k̃i < k ∀k. (25)\nIn addition, denote k̂i (k̃i− τ ≤ k̂i < k̃i) as the last iteration number before iteration k̃i for which worker i ∈ Ak̃i is arrived, i.e., i ∈ Ak̂i . Then by (21) and (22), for all workers i ∈ A c k, we must have\nxk̃i+1i = x k̃i+2 i = · · · = x k i = x k+1 i , (26) λk̃i+1i = λ k̃i+2 i = · · · = λ k i = λ k+1 i , (27)\nSince i ∈ Ak̃i for all i ∈ A c k and by (26)-(27), we can equivalently write (21) and (22) for all i ∈ A c k as\nxk+1i = x k̃i+1 i\n= arg min xi\nfi(xi) + x T i λ k̃i i + ρ 2‖xi − x k̂i+1 0 ‖ 2, (28)\nλk+1i = λ k̃i+1 i = λ k̃i i + ρ(x k̃i+1 i − x k̂i+1 0 )\n= λk̃ii + ρ(x k+1 i − x k̂i+1 0 ). (29)\nBased on these notations, we have shown in [23, Eqn. (33)] that the following lemma is true.\nSeptember 10, 2015 DRAFT\n9 Lemma 1 Suppose that Assumption 2 holds and ρ ≥ L. Then, for all k = 0, 1, . . . ,\n0 ≤△k+1 ≤ △k +\n( 1 + ρ/ǫ\n2\n)∑\ni∈Ak\n‖xk0 − x k̄i+1 0 ‖ 2\n−\n( 2γ +Nρ\n2\n) ‖xk+10 − x k 0‖ 2 + ( L2 + (ǫ− 1)ρ\n2 +\nL2\nρ\n) ∑\ni∈Ak\n‖xk+1i − x k i ‖ 2, (30)\nwhere ǫ ∈ (0, 1) is a constant.\nIn particular, (30) is the same as [23, Eqn. (33)] except that here we have assumed convex fi’s. Lemma\n1 shows how the gap between the augmented Lagrangian function Lρ(xk+1,x k+1 0 ,λ k+1) and the optimal objective value F ⋆ evolves with the iteration number k. Notice that it follows from [23, Lemma 3] that △k+1 ≥ 0 for all k if ρ ≥ L. As will be seen shortly, Lemma 1 is crucial in the linear convergence analysis.\nSimilar to [23, Lemma 3], we next need to bound the error terms, e.g., (1+ρ 2 2 ) ∑ i∈Ak ‖xk0 − x k̄i+1 0 ‖ 2 in (30), which is caused by asynchrony of the network. Here, we present a more general result for the latter analysis.\nLemma 2 Let η > 0 and j − ν ≤ ji < j where ν ∈ Z++, ji ∈ Z+ and j ∈ {0, 1, . . . , k}. Moreover, let Nj ⊂ V be any index subset satisfying |Nj| ≤ N̄ for some constant N̄ ∈ (1, N ]. Then, the following inequality holds true\nk∑\nj=0\nηj ∑\ni∈Nj\n‖xj0 − x ji+1 0 ‖\n2 ≤ (ν − 1)N̄ k−1∑\nj=0\nηj+1 ( ην−1 − 1\nη − 1\n) ‖xj0 − x j+1 0 ‖ 2. (31)\nProof: See Appendix B.\nNow let us consider Assumption 3. For strongly convex f ′is, it is known that the following first-order\ncondition holds [24]: ∀x,y,\nfi(y) ≥ fi(x) + (∇fi(x)) T (y − x) +\nσ2\n2 ‖y − x‖2. (32)\nBased on this property, we can bound △k+1 as follows.\nLemma 3 Suppose that Assumptions 2 and 3 hold and ρ ≥ σ2. If γ ≥ 8N(ρ− σ2) and δ satisfies (17), then it holds that\n1\nγδ △k+1 ≤\nL2\n4ρ2N\n∑\ni∈Ak\n‖xk+1i − x k i ‖ 2\nSeptember 10, 2015 DRAFT\n10\n+ L2\n4ρ2N\n∑\ni∈Ack\n‖xk̃i+1i − x k̃i i ‖\n2 + 1\n2N\n∑\ni∈Ak\n‖xk0 − x k̄i+1 0 ‖ 2\n+ 1\n2N\n∑\ni∈Ack\n‖xk0 − x k̂i+1 0 ‖ 2 + ‖xk+10 − x k 0‖ 2. (33)\nInstead, if γ = 0 and δ ≥ max{ρ/σ2 − 1, 1}, then it holds (\n1\n4(ρ− σ2)Nδ\n) △k+1 ≤ L2\n2ρ2N\n∑\ni∈Ak\n‖xk+1i − x k i ‖ 2\n+ L2\n2ρ2N\n∑\ni∈Ack\n‖xk̃i+1i − x k̃i i ‖\n2 + 1\nN\n∑\ni∈Ak\n‖xk0 − x k̄i+1 0 ‖ 2\n+ 1\nN\n∑\ni∈Ack\n‖xk0 − x k̂i+1 0 ‖ 2 + ‖xk+10 − x k 0‖ 2. (34)\nProof: See Appendix C.\nB. Proof of Theorem 1\nWe use the lemmas above to prove Theorem 1. Denote η , 1 + 1δγ . By summing (30) and (33), we\nobtain\n△k+1 ≤ 1\nη △k +\n1 η\n[( L2 + (ǫ− 1)ρ+ L 2\n2ρ2N\n2 +\nL2\nρ\n) N∑\ni=1\n‖xk+1i − x k i ‖ 2\n−\n( 2γ +Nρ\n2 − 1\n) ‖xk+10 − x k 0‖ 2 + 1\n2N\n∑\ni∈Ack\n‖xk0 − x k̂i+1 0 ‖ 2\n+ L2\n4ρ2N\n∑\ni∈Ack\n‖xk̃i+1i − x k̃i i ‖ 2 2 +\n( 1 + ρ/ǫ\n2 +\n1\n2N\n) ∑\ni∈Ak\n‖xk0 − x k̄i+1 0 ‖ 2\n] . (35)\nHere, we have used the fact of ∑\ni∈Ak ‖xk+1i −x k i ‖\n2 = ∑N\ni=1 ‖x k+1 i −x k i ‖ 2 as xk+1i = x k i ∀i ∈ A c k. By\ntaking the telescoping sum of (35), we further obtain △k+1 ≤ 1\nηk+1 △0\n+ 1\nη\n[( L2 + (ǫ− 1)ρ+ L 2\n2ρ2N + 2L2 ρ\n2\n) k∑\nℓ=0\n1\nηℓ\nN∑\ni=1\n‖xk−ℓ+1i −x k−ℓ i ‖ 2 −\n( 2γ +Nρ\n2 − 1\n) k∑\nℓ=0\n1\nηℓ ‖xk−ℓ+10 − x k−ℓ 0 ‖ 2\n+\n( 1 + ρ/ǫ\n2 +\n1\n2N\n) k∑\nℓ=0\n1\nηℓ\n∑\ni∈Ak−ℓ\n‖xk−ℓ0 − x (k−ℓ) i +1 0 ‖ 2\n︸ ︷︷ ︸ (36a)\n+ 1\n2N\nk∑\nℓ=0\n1\nηℓ\n∑\ni∈Ack−ℓ\n‖xk−ℓ0 − x (̂k−ℓ) i +1 0 ‖ 2\n︸ ︷︷ ︸ (36b)\n+ L2\n4ρ2N\nk∑\nℓ=0\n1\nηℓ\n∑\ni∈Ack−ℓ\n‖x (̃k−ℓ) i +1 i − x (̃k−ℓ) i i ‖ 2 2\n︸ ︷︷ ︸ (36c)\n] . (36)\nSeptember 10, 2015 DRAFT\n11\nThe three terms (36a), (36b), and (36c) in the right hand side (RHS) of (36) can respectively be bounded as follows, using Lemma 2. Consider the change of variable k − ℓ = j. Then, we have the following chain for (36a):\n(36a) =\nk∑\nℓ=0\n1\nηℓ\n∑\ni∈Ak−ℓ\n‖xk−ℓ0 − x (k−ℓ) i +1 0 ‖ 2\n= 1\nηk\nk∑\nj=0\nηj ∑\ni∈Aj\n‖xj0 − x j̄i+1 0 ‖ 2\n≤ 1\nηk S(τ − 1)\nk−1∑\nj=0\nηj+1 ( ητ−1 − 1\nη − 1\n) ‖xj0 − x j+1 0 ‖ 2\n= S(τ − 1)η\n( ητ−1 − 1\nη − 1\n) k∑\nℓ=1\n1\nηℓ ‖xk−ℓ0 − x k−ℓ+1 0 ‖ 2, (37)\nwhere the inequality is obtained by applying (31) with ν = τ , Nj = Aj , N̄ = S, and ji = j̄i which satisfies j− τ ≤ j̄i < j (see (24)); to obtain the last equality, the change of variable k− ℓ = j is applied again.\nAnalogously, by applying (31) with ν = 2τ − 1, Nj = Acj , N̄ = N , and ji = ĵi (which satisfies j− 2τ +1 ≤ ĵi < j since j̃i− τ ≤ ĵi < j̃i and j− τ < j̃i < j by (24) and (25)), one can bound (36b) as\n(36b) =\nk∑\nℓ=0\n1\nηℓ\n∑\ni∈Ack−ℓ\n‖xk−ℓ0 − x (̂k−ℓ) i +1 0 ‖ 2\n= 1\nηk\nk∑\nj=0\nηj ∑\ni∈Acj\n‖xj0 − x ĵi+1 0 ‖ 2\n≤ 1\nηk 2N(τ − 1)\nk−1∑\nj=0\nηj+1 ( η2(τ−1) − 1\nη − 1\n) ‖xj0 − x j+1 0 ‖ 2\n= 2N(τ − 1)η\n( η2(τ−1) − 1\nη − 1\n) k∑\nℓ=1\n1\nηℓ ‖xk−ℓ0 − x k−ℓ+1 0 ‖ 2. (38)\nThe term (36c) can be bounded as follows\n(36c) =\nk∑\nℓ=0\n1\nηℓ\n∑\ni∈Ack−ℓ\n‖x (̃k−ℓ) i +1 i − x (̃k−ℓ) i i ‖ 2\n= 1\nηk\nk∑\nj=0\nηj ∑\ni∈Acj\n‖xj̃i+1i − x j̃i i ‖ 2\n= 1\nηk\nk∑\nj=0\n∑\ni∈Acj\nηj−j̃i−1ηj̃i+1‖xj̃i+1i − x j̃i i ‖ 2\nSeptember 10, 2015 DRAFT\n12\n≤ ητ−2 1\nηk\nk∑\nj=0\n∑\ni∈Acj\nηj̃i+1‖xj̃i+1i − x j̃i i ‖ 2\n≤ ητ−2(τ − 1) 1\nηk\nN∑\ni=1\nk∑\nj=0\nηj+1‖xj+1i − x j i‖ 2\n= ητ−1(τ − 1) N∑\ni=1\nk∑\nℓ=0\n1\nηℓ ‖xk−ℓ+1i − x k−ℓ i ‖ 2, (39)\nwhere, in the first inequality, we have used the fact of j− τ +1 ≤ j̃i < j from (25). To show the second inequality, notice that for any i ∈ Acj , it also satisfies i ∈ A c ℓ for ℓ = j̃i + 1, . . . , j. So, j̃i = ℓ̃i for ℓ = j̃i + 1, . . . , j. Since j − τ < j̃i < j, each ηj̃+1‖x j̃+1 i − x j̃i i ‖ 2 appears no more than τ − 1 times in the summation ∑k\nj=0 ∑ i∈Acj ηj̃+1‖xj̃+1i − x j̃i i ‖ 2.\nBy substituting (39), (38) and (37) into (36), we obtain\n△k+1 ≤ 1\nηk+1 △0\n+ 1\nη\n[( 1 + ρ/ǫ\n2 +\n1\n2N\n) S(τ − 1)η ( ητ−1 − 1\nη − 1\n)\n+ (τ − 1)η\n( η2(τ−1) − 1\nη − 1\n)\n+ 1−\n( 2γ +Nρ\n2\n)] k∑\nℓ=0\n1\nηℓ ‖xk−ℓ+10 − x k−ℓ 0 ‖ 2\n+ 1\nη\n[( L2 + (ǫ− 1)ρ+ L 2\n2ρ2N + 2L2 ρ\n2\n)\n+ ητ−1(τ − 1) L2\n4ρ2N\n] N∑\ni=1\nk∑\nℓ=0\n1\nηℓ\nN∑\ni=1\n‖xk−ℓ+1i − x k−ℓ i ‖ 2. (40)\nLet ǫ = 1/ρ. Therefore, we see that (16) is true if\nγ ≥ (τ − 1)η\n[( S(1 + ρ2) + S/N\n2\n)( ητ−1 − 1\nη − 1\n)\n+\n( η2(τ−1) − 1\nη − 1\n)] − Nρ\n2 + 1, (41)\nρ ≥ (1 + L2) + 2L2\nρ +\nL2\n2ρ2N\n( 1 + ητ−1(τ − 1) ) . (42)\nLet ρ ≥ 18N + σ 2. Then (42) holds true if\nρ ≥ (1 + L2) + 2L2\nρ\n( 1 + 2 + 2ητ−1(τ − 1)\n1 + 8Nσ2\n) . (43)\nSeptember 10, 2015 DRAFT\n13\nMoreover, since γ ≥ 8N(ρ− σ2) and δ > 1, we see that η has an upper bound\nη = 1 + 1\nδγ < 1 +\n1\n8N(ρ− σ2) < 2. (44)\nTherefore, (14) and (15) are sufficient conditions for (43) and (41), respectively. The proof is thus complete.\nC. Proof of Theorem 2\nThe key is to build a similar result as Lemma 3 under Assumption 4. Now, consider Assumption 4.\nLet x⋆ be an optimal solution to (1), and let\ny⋆i = Aix ⋆, i = 1, . . . , N.\nThen, (y⋆1 , . . . ,y ⋆ N ) is unique since gi’s are strongly convex. So, the optimal solution set to (2) can be defined as\nX ⋆ = { (x0,x1, . . . ,xN ) |   y⋆1 ...\ny⋆N\n  =   A1x1 ...\nANxN\n  ,\nxi = x0, i = 1, . . . , N\n} . (45)\nLet 1N+1 ⊗ P⋆(x̂) be the projection point of x̂ , (xT0 ,x T 1 , . . . ,x T N ) T onto X ⋆, where ⊗ denotes the Kronecker product. It can be shown that the following lemma is true.\nLemma 4 Under Assumption 4, for any x̂ ∈ Rn(N+1), it holds that N∑\ni=1\nfi(P ⋆(x̂)) ≥\nN∑\ni=1\nfi(xi) +\nN∑\ni=1\n(∇fi(xi)) T (P⋆(x̂)− xi)\n+\nN∑\ni=1\nσ2 2c ‖P⋆(x̂)− xi‖ 2 + σ2 2c ‖P⋆(x̂)− x0‖ 2\n− σ2\n2\nN∑\ni=1\n‖xi − x0‖ 2, (46)\nfor some finite constant c > 0.\nProof: See Appendix D.\nLemma 4 implies that the structured fi’s in Assumption 4 own an analogous property as the strongly convex functions in (32). Based on Lemma 4, the next lemma shows that one can still bound △k+1 as in Lemma 3 under Assumption 4.\nSeptember 10, 2015 DRAFT\n14\nLemma 5 Suppose that Assumptions 2 and 4 hold, and assume that γ ≥ 8N(ρ− σ2/c) + 4Nσ2 and δ satisfies\nδ ≥ max { 1, ρN + γ\nNσ2/c − 1\n} . (47)\nThen, (33) holds true. Instead, if γ = 0 and δ ≥ max{(cρ)/σ2 − 1, 1}, then\n△k+1 2N [2(ρ − σ2/c)δ + σ2] ≤ L2 2ρ2N\n∑\ni∈Ak\n‖xk+1i − x k i ‖ 2\n+ L2\n2ρ2N\n∑\ni∈Ack\n‖xk̃i+1i − x k̃i i ‖\n2 + 1\nN\n∑\ni∈Ak\n‖xk0 − x k̄i+1 0 ‖ 2\n+ 1\nN\n∑\ni∈Ack\n‖xk0 − x k̂i+1 0 ‖ 2 + ‖xk+10 − x k 0‖ 2. (48)\nProof: See Appendix E.\nGiven Lemma 5, Theorem 2 can be proved by following exactly the same steps as for Theorem 1 in\nSection IV-B. The details are omitted here.\nV. NUMERICAL RESULTS\nIn this section, we present some simulation results to examine the practical performance of the AD-\nADMM. We consider the following LR problem\nmin w∈W\nm∑\nj=1\nlog ( 1 + exp(−yja T j w) ) (49)\nwhere y1, . . . , ym are the binary labels of the m training data, w ∈ Rn is the regression variable and Ai = [a1, . . . ,am] T ∈ Rm×n is the training data matrix. We used the MiniBooNE particle identification Data Set1 which contains 130065 training samples (m = 130065) and the learning parameter has a size of 50 (n = 50). The constraint set W is set to W = {w ∈ Rn | |wi| ≤ 10 ∀i = 1, . . . , n}. The AD-ADMM is implemented on an HP ProLiant BL280c G6 Linux Cluster (Itasca HPC in University of Minnesota). The n training samples are uniformly distributed to a set of N workers (N = 10, 15, 20). For each worker, we employed the fast iterative shrinkage thresholding algorithm (FISTA) [25] to solve the corresponding subproblem (10). The stepsize of FISTA is set to 0.0001 and the stopping condition is that the 2-norm of the gradient is less than 0.001. The penalty parameter ρ of the AD-ADMM is set to 0.01. Interestingly, while the theoretical convergence conditions in [23, Theorem 1] and Theorem 1\n1https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification\nSeptember 10, 2015 DRAFT\n15\nall suggest that the penalty parameter γ should be large in the worst-case, we find that, for the problem instance we test here, it is also fine to set γ = 0.\nNote that the asynchrony in our setting comes naturally from the heterogeneity of the computation times of computing nodes. In our experiments, analogous to [22], we further constrained the minimum size of the active set Ak by |Ak| ≥ A where A ∈ [1, N ] is an integer. When A = N , it corresponds to the synchronous case where the master is forced to wait for all the workers at every iteration.\nFigure 1(a) and Figure 1(b) respectively display the convergence curves (objective value) of the ADADMM versus the iteration number and the running time (second), for various values of N and τ . Here we\nSeptember 10, 2015 DRAFT\n16\nset A = 1. One can observe from Figure 1(a) that, in terms of the iteration number, the convergence speed of the AD-ADMM slows down when τ increases. However, as seen from Figure 1(b), the AD-ADMM is actually faster than its synchronous counterpart (τ = 1), and the running time of the AD-ADMM can be further reduced with increased τ . We also observe that, when N increases, the advantages of the AD-ADMM compared to its synchronous counterpart reduces. This is because the computation load allocated to each worker decreases with N (as n is fixed), making all the workers experience similar computation delays.\nIn Figure 1(c) and Figure 1(d), we present the convergence curves of AD-ADMM with different values of A. We see from Figure 1(c) that when A increases, it always requires fewer number of iterations to\nSeptember 10, 2015 DRAFT\n17\nachieve convergence for all choices of parameters. From Figure 1(d), however we can observe that a larger value of A is not always beneficial in reducing the running time. Specifically, one can see that for N = 10, the running time of AD-ADMM decreases when one increases A from 1 to 2, whereas the running time increases a lot if one increases A to 4. One can observe similar results for N = 15 and N = 20.\nTo look into how the values of τ and A impact on the algorithm speed, in Figure 2, we respectively plot the computation time and the waiting time of the master node for various pairs of (τ,A). The setting is the same as that in Figure 1, except that here the stopping condition of the AD-ADMM is that the objective value achieves 4.56 × 104. One can observe from these figures that, when τ increases, the computing load of the master also increases but the waiting time is significantly reduced. This explains why in Figure 1(b) the AD-ADMM requires a less running time compared with the synchronous ADMM. On the other hand, when A increases, the computation time of the master always decreases. This is because the master may take a smaller number of iterations to reach the target objective value (see Figure 1(c)) and have to spend more time waiting for slow workers. However, the overall waiting time of the master does not necessarily become larger or smaller with A. As seen from Figure 2(b) and Figure 2(d), when A increases from 1 to 2, the waiting time for N = 10 in Figure 2(b) increases, whereas the waiting time for N = 20 in Figure 2(d) decreases. However, for A = 4, the waiting times always become larger. Nevertheless, when comparing to the synchronous ADMM (i.e., (τ,A) = (1, N)), we can see that the waiting time of the master in the AD-ADMM is always much smaller.\nVI. CONCLUSIONS\nIn this paper, we have analytically studied the linear convergence conditions of the AD-ADMM proposed in [23]. Specifically, we have shown that for strongly convex fi’s (Assumption 3) or for fi’s with the composite form in Assumption 4, the AD-ADMM is guaranteed to converge linearly, provided that the penalty parameter ρ and the proximal parameter γ are chosen sufficiently large depending on the delay τ . When the delay τ is bounded and N is large, we have further shown that linear convergence can be achieved with zero proximal parameter (i.e.,γ = 0), and with a delay-independent ρ. The linear convergence conditions and the linear rate have been given explicitly, which relate the algorithm and network parameters with the algorithm worst-case convergence performance. The presented numerical examples have shown that in practice the AD-ADMM can effectively reduce the waiting time of the master node, and as a consequence improves the overall time efficiency of distributed optimization significantly.\nSeptember 10, 2015 DRAFT\n18\nAPPENDIX A\nBOUND OF CONSENSUS ERROR\nWe bound the size of the consensus error ∑N\ni=1 ‖x k+1 i −x k+1 0 ‖ 2 in the following lemma.\nLemma 6 Under Assumption 2, it holds that\nN∑\ni=1\n‖xk+1i −x k+1 0 ‖\n2 ≤ 2L2\nρ2\n∑\ni∈Ak\n‖xk+1i − x k i ‖ 2\n+ 2L2\nρ2\n∑\ni∈Ack\n‖xk̃i+1i − x k̃i i ‖\n2 + 4 ∑\ni∈Ak\n‖xk̄i+10 −x k 0‖\n2 + 4 ∑\ni∈Ack\n‖xk̂i+10 −x k 0‖ 2 + 4N‖xk+10 − x k 0‖ 2.\n(A.1)\nProof: It follows from (22) and (29) that the following chain is true\nN∑\ni=1\n‖xk+1i −x k+1 0 ‖\n2 = ∑\ni∈Ak\n‖xk+1i − x k̄i+1 0 + x k̄i+1 0 −x k+1 0 ‖ 2\n+ ∑\ni∈Ack\n‖xk+1i − x k̂i+1 0 + x k̂i+1 0 −x k+1 0 ‖ 2\n≤ 2 ∑\ni∈Ak\n‖xk+1i − x k̄i+1 0 ‖\n2 + 2 ∑\ni∈Ack\n‖xk+1i − x k̂i+1 0 ‖ 2\n+ 2 ∑\ni∈Ak\n‖xk̄i+10 −x k+1 0 ‖\n2 + 2 ∑\ni∈Ack\n‖xk̂i+10 −x k+1 0 ‖ 2\n≤ 2\nρ2\n∑\ni∈Ak\n‖λk+1i − λ k i ‖\n2 + 2\nρ2\n∑\ni∈Ack\n‖λk̃i+1i − λ k̃i i ‖ 2\n+ 2 ∑\ni∈Ak\n‖xk̄i+10 − x k 0 + x k 0 −x k+1 0 ‖\n2 + 2 ∑\ni∈Ack\n‖xk̂i+10 − x k 0 + x k 0 −x k+1 0 ‖ 2\n≤ 2\nρ2\n∑\ni∈Ak\n‖λk+1i − λ k i ‖\n2 + 2\nρ2\n∑\ni∈Ack\n‖λk̃i+1i − λ k̃i i ‖ 2\n+ 4 ∑\ni∈Ak\n‖xk̄i+10 −x k 0‖\n2 + 4 ∑\ni∈Ack\n‖xk̂i+10 −x k 0‖ 2 + 4N‖xk+10 − x k 0‖ 2. (A.2)\nRecall from [23, Eqn. (38)] that\n∇fi(x k+1 i ) + λ k+1 i = 0 ∀i ∈ V and ∀k. (A.3)\nBy substituting (A.3) into (A.2) and by the Lipschitz continuity of ∇fi, we obtain (A.1).\nSeptember 10, 2015 DRAFT\n19\nAPPENDIX B\nPROOF OF LEMMA 2\nIt is easy to show the following chain is true\nk∑\nj=0\nηj ∑\ni∈Nj\n‖xj0 − x ji+1 0 ‖ 2 =\nk∑\nj=0\nηj ∑\ni∈Nj\n‖\nj−1∑\nq=ji+1\n(xq0 − x q+1 0 )‖ 2\n≤ k∑\nj=0\nηj ∑\ni∈Nj\n(j − ji − 1)\nj−1∑\nq=ji+1\n‖xq0 − x q+1 0 ‖ 2\n≤ k∑\nj=0\nηj ∑\ni∈Nj\n(ν − 1)\nj−1∑\nq=j−ν+1\n‖xq0 − x q+1 0 ‖ 2\n≤ (ν − 1)N̄\n( k∑\nj=0\nηj j−1∑\nq=j−ν+1\n‖xq0 − x q+1 0 ‖ 2\n) , (A.4)\nwhere the second inequality is owing to j − ν ≤ ji. To proceed, we list ηj ∑j−1 q=j−ν+1 ‖x q 0 − x q+1 0 ‖ 2 for j = 1, . . . , ν, . . ., below\nj = 1, η\n0∑\nq=2−ν\n‖xq0 − x q+1 0 ‖ 2 = η‖x00 − x 1 0‖ 2\nj = 2, η2 1∑\nq=3−ν\n‖xq0 − x q+1 0 ‖ 2 = η2‖x00 − x 1 0‖ 2 + η2‖x10 − x 2 0‖ 2\n... ...\nj = ν − 1, ην−1 ν−2∑\nq=0\n‖xq0 − x q+1 0 ‖ 2 = ην−1‖x00 − x 1 0‖ 2 + ην−1‖x10 − x 2 0‖ 2 + · · ·+ ην−1‖xν−20 − x ν−1 0 ‖ 2\nj = ν, ην ν−1∑\nq=1\n‖xq0 − x q+1 0 ‖ 2 = ην‖x10 − x 2 0‖ 2 + ην‖x20 − x 3 0‖ 2 + · · · + ην‖xν−10 − x ν 0‖ 2 (A.5)\nOne can verify that each ‖xj0 − x j+1 0 ‖ 2 appears no more than ν − 1 times in the summation term ∑k\nj=0 η j ∑j−1 q=j−ν+1 ‖x q 0 − x q+1 0 ‖ 2 and therefore the total contribution of each ‖xj0 − x j+1 0 ‖ 2 can be\nupper bounded by\n(ηj+1 + ηj+2 + · · ·+ ηj+ν−1)‖xj0 − x j+1 0 ‖\n2 = ηj+1 ( ην−1 − 1\nη − 1\n) ‖xj0 − x j+1 0 ‖ 2. (A.6)\nThis shows that k∑\nj=0\nηj j−1∑\nq=j−ν+1\n‖xq0 − x q+1 0 ‖\n2 ≤ k−1∑\nj=0\nηj+1 ( ην−1 − 1\nη − 1\n) ‖xj0 − x j+1 0 ‖ 2. (A.7)\nBy substituting (A.7) into (A.4), we obtain (31).\nSeptember 10, 2015 DRAFT\n20\nAPPENDIX C\nPROOF OF LEMMA 3\nBy the optimality condition of (21) [24] , one has, ∀i ∈ Ak and ∀xi ∈ Rn,\n0 ≥ (∇fi(x k+1 i ) + λ k i + ρ(x k+1 i − x k̄i+1 0 ) T (xk+1i − xi)\n= (∇fi(x k+1 i )) T (xk+1i − xi) + (λ k+1 i ) T (xk+1i − xi), (A.8)\nwhere the equality is due to (22). Similarly, by the optimality condition of (28) and by (29), one has, ∀i ∈ Ack and ∀xi ∈ R n,\n0 ≥ (∇fi(x k+1 i ) + λ k̃i i + ρ(x k+1 i − x k̂i+1 0 ) T (xk+1i − xi)\n= (∇fi(x k+1 i )) T (xk+1i − xi) + (λ k+1 i ) T (xk+1i − xi). (A.9)\nSumming (A.8) and (A.9) for all i ∈ V gives rise to\nN∑\ni=1\n(∇fi(x k+1 i )) T (xk+1i − xi) + N∑\ni=1\n(λk+1i ) T (xk+1i − xi)\n≤ 0 ∀(x1, . . . ,xN ) ∈ R nN . (A.10)\nIn addition, by the optimality condition of (23) [7, Lemma 4.1], one has, ∀x0 ∈ Rn,\nh(xk+10 )− h(x0)− N∑\ni=1\n(λk+1i ) T (xk+10 − x0)\n− ρ N∑\ni=1\n(xk+1i − x k+1 0 ) T (xk+10 − x0)\n+ γ(xk+10 − x k 0) T (xk+10 − x0) ≤ 0. (A.11)\nDenote x⋆ ∈ Rn as an optimal solution to problem (1). Let x1 = · · · = xN = x0 = x⋆ in (A.10) and (A.11), and combine the two equations. We obtain\nN∑\ni=1\n(∇fi(x k+1 i )) T (xk+1i − x ⋆) + h(xk+10 )− h(x ⋆)\n+\nN∑\ni=1\n(λk+1i ) T (xk+1i − x k+1 0 )\n− ρ N∑\ni=1\n(xk+1i − x k+1 0 ) T (xk+10 − x ⋆)\n+ γ(xk+10 − x k 0) T (xk+10 − x ⋆) ≤ 0. (A.12)\nSeptember 10, 2015 DRAFT\n21\nLet y = x⋆ and x = xk+1i in (32) for all i ∈ V , and apply them to (A.12). We have\n0 ≥\n( N∑\ni=1\nfi(x k+1 i ) + h(x k+1 0 )−\nN∑\ni=1\nfi(x ⋆)− h(x⋆)\n)\n+\nN∑\ni=1\n(λk+1i ) T (xk+1i − x k+1 0 ) +\nσ2\n2\nN∑\ni=1\n‖xk+1i − x ⋆‖2\n− ρ N∑\ni=1\n(xk+1i − x k+1 0 ) T (xk+10 − x ⋆)\n+ γ(xk+10 − x k 0) T (xk+10 − x ⋆), (A.13)\nNote that, by (20),\n− ρ N∑\ni=1\n(xk+1i − x k+1 0 ) T (xk+10 − x ⋆)\n= − ρ\n2\nN∑\ni=1\n‖xk+1i − x ⋆‖2 +\nρ 2\nN∑\ni=1\n‖xk+1i − x k+1 0 ‖ 2\n+ ρN\n2 ‖xk+10 − x ⋆‖2, (A.14)\nand that\nγ(xk+10 − x k 0) T (xk+10 − x ⋆) =\nγ 2 ‖xk+10 − x ⋆‖2\n− γ\n2 ‖xk0 − x\n⋆‖2 + γ\n2 ‖xk+10 − x k 0‖ 2. (A.15)\nBy substituting (A.14) and (A.15) into (A.13) and recalling Lρ in (12), we obtain\n△k+1 ≤ ρ− σ2\n2\nN∑\ni=1\n‖xk+1i − x ⋆‖2 +\nγ 2 ‖xk0 − x ⋆‖2\n− γ\n2 ‖xk+10 − x k 0‖\n2 − γ + ρN\n2 ‖xk+10 − x ⋆‖2. (A.16)\nWe bound the term ∑N\ni=1 ‖x k+1 i − x ⋆‖2 as\nN∑\ni=1\n‖xk+1i − x ⋆‖2 =\nN∑\ni=1\n‖xk+1i − x k+1 0 + x k+1 0 − x ⋆‖2\n≤ (1 + 1\nδ )N‖xk+10 − x\n⋆‖22 + (1 + δ) N∑\ni=1\n‖xk+1i − x k+1 0 ‖ 2 (by (20))\nSeptember 10, 2015 DRAFT\n22\n≤ (1 + 1\nδ )N‖xk+10 − x ⋆‖22 + (1 + δ)\n[ 2L2\nρ2\n∑\ni∈Ak\n‖xk+1i − x k i ‖\n2 + 2L2\nρ2\n∑\ni∈Ack\n‖xk̃i+1i − x k̃i i ‖ 2\n+ 4 ∑\ni∈Ak\n‖xk̄i+10 −x k 0‖\n2 + 4 ∑\ni∈Ack\n‖xk̂i+10 −x k 0‖ 2 + 4N‖xk+10 − x k 0‖ 2\n] (by (A.1))\n≤ (1 + 1\nδ )N‖xk+10 − x\n⋆‖22 + 4δL2\nρ2\n∑\ni∈Ak\n‖xk+1i − x k i ‖\n2 + 4δL2\nρ2\n∑\ni∈Ack\n‖xk̃i+1i − x k̃i i ‖ 2\n+ 8δ ∑\ni∈Ak\n‖xk̄i+10 −x k 0‖\n2 + 8δ ∑\ni∈Ack\n‖xk̂i+10 −x k 0‖ 2 + 8δN‖xk+10 − x k 0‖ 2, (A.17)\nwhere the last inequality is obtained by assuming δ > 1. Besides, we bound the term γ2‖x k 0 − x ⋆‖2 in the RHS of (A.16) as\nγ 2 ‖xk0 − x ⋆‖2 = γ 2 ‖xk0 − x k+1 0 + x k+1 0 − x ⋆‖2 ≤ γ\n2 (1 + δ)‖xk0 − x k+1 0 ‖\n2 + γ\n2 (1 +\n1 δ )‖xk+10 − x ⋆‖2. (A.18)\nBy substituting (A.17) and (A.18) into (A.16), one obtains\n△k+1 ≤\n( ρN + γ\n2δ −\nσ2N\n2 (1 +\n1 δ )\n) ‖xk+10 − x ⋆‖2\n+\n( γδ\n2 + 4(ρ− σ2)Nδ\n) ‖xk+10 − x k 0‖ 2 + 2(ρ− σ2)δL2\nρ2\n∑\ni∈Ak\n‖xk+1i − x k i ‖ 2\n+ 2(ρ− σ2)δL2\nρ2\n∑\ni∈Ack\n‖xk̃i+1i − x k̃i i ‖ 2\n+ 4(ρ− σ2)δ ∑\ni∈Ak\n‖xk0 − x k̄i+1 0 ‖\n2 + 4(ρ− σ2)δ ∑\ni∈Ack\n‖xk0 − x k̂i+1 0 ‖ 2. (A.19)\nLet δ > 1 be large enough so that ρN+γ2δ − σ2N 2 (1+ 1 δ ) ≤ 0 and assume that γ ≥ 8(ρ− σ 2)N . Then, one obtains (33) from (A.19).\nTo show (34), let γ = 0 in (A.19) and assume that δ > 1 be large enough so that ρδ − σ 2(1 + 1δ ) ≤ 0.\nAPPENDIX D\nPROOF OF LEMMA 4\nSince X ⋆ is a linear set, according to the Hoffman bound [26], for some constant c > 0,\ndist2(X ⋆, x̂) = N∑\ni=1\n‖P⋆(x̂)− xi‖ 2 + ‖P⋆(x̂)− x0‖ 2\n≤ c N∑\ni=1\n‖Aixi − y ⋆ i ‖ 2 + c\nN∑\ni=1\n‖xi − x0‖ 2. (A.20)\nSeptember 10, 2015 DRAFT\n23\nIn addition, it follows from the strong convexity of gi’s that\nN∑\ni=1\nfi(P ⋆(x̂)) =\nN∑\ni=1\ngi(AiP ⋆(x̂))\n≥ N∑\ni=1\ngi(Aixi) +\nN∑\ni=1\n(∇gi(Aixi)) TAi(P ⋆(x̂)− xi) + N∑\ni=1\nσ2\n2 ‖AiP\n⋆(x̂)−Aixi‖ 2\n=\nN∑\ni=1\nfi(xi) +\nN∑\ni=1\n(∇fi(xi)) T (P⋆(x̂)− xi) +\nN∑\ni=1\nσ2\n2 ‖y⋆i −Aixi‖ 2. (A.21)\nBy substituting (A.20) into (A.21), one obtains (46).\nAPPENDIX E\nPROOF OF LEMMA 5\nBy applying (46) (with xi = x k+1 i ∀i = 0, 1, . . . , N ) to (A.12), and following the same steps as in\n(A.13)-(A.16), we have\n△k+1 ≤ ρ− σ2/c\n2\nN∑\ni=1\n‖xk+1i − P ⋆(x̂)‖2 +\nγ 2 ‖xk0 − P ⋆(x̂)‖2\n− γ\n2 ‖xk+10 − x k 0‖\n2 − γ + σ2/c+ ρN\n2 ‖xk+10 − P ⋆(x̂)‖2\n+ σ2\n2\nN∑\ni=1\n‖xk+1i − x k+1 0 ‖ 2. (A.22)\nRecall (A.17), (A.18) (with x⋆ replaced by P⋆(x̂)) and (A.1) in Lemma 6 and apply them to (A.22).\nOne obtains\n△k+1 ≤\n( ρN + γ\n2δ −\nNσ2/c\n2 (1 +\n1 δ )\n) ‖xk+10 − P ⋆(x̂)‖2\n+\n( γδ\n2 + 4(ρ− σ2/c)Nδ + 2σ2N\n) ‖xk+10 − x k 0‖ 2\n+ (2(ρ − σ2/c)δ + σ2)L2\nρ2\n∑\ni∈Ak\n‖xk+1i − x k i ‖ 2\n+ (2(ρ− σ2/c)δ + σ2)L2\nρ2\n∑\ni∈Ack\n‖xk̃i+1i − x k̃i i ‖ 2\n+ (4(ρ− σ2/c)δ + 2σ2) ∑\ni∈Ak\n‖xk0 − x k̄i+1 0 ‖ 2\n+ (4(ρ− σ2/c)δ + 2σ2) ∑\ni∈Ack\n‖xk0 − x k̂i+1 0 ‖ 2. (A.23)\nSeptember 10, 2015 DRAFT\n24\nLet δ > 1 be large enough so that ρN+γ2δ − Nσ2/c 2 (1+ 1 δ ) ≤ 0 In addition, since γ ≥ 8N(ρ− σ 2/c) + 4Nσ2 implies γ ≥ 8N(ρ− σ2/c) + 4Nσ2/δ, (A.23) infers (33).\nTo obtain (48), let γ = 0 in (A.23) and assume that δ > 1 be large enough so that ρδ − σ2 c (1+ 1 δ ) ≤ 0.\nREFERENCES\n[1] T.-H. Chang, M. Hong, W.-C. Liao, and X. Wang, “Asynchronous distributed alternating direction method of multipliers:\nAlgorithm and convergence analysis,” submitted to NIPS, Montreal, Canada, Dec. 7-12, 2015.\n[2] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical learning via the alternating\ndirection method of multipliers,” Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1–122, 2011.\n[3] R. Tibshirani and M. Saunders, “Sparisty and smoothness via the fused lasso,” J. R. Statist. Soc. B, vol. 67, no. 1, pp.\n91–108, 2005.\n[4] J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in Proc. ACM Int. Conf. on Knowledge Discovery and\nData Mining, New York, NY, USA, June 28 - July 1, 2009, pp. 547–556.\n[5] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction.\nNew York, NY, USA: Springer-Verlag, 2001.\n[6] P. Richtárik, M. Takáč, and S. D. Ahipasaoglu, “Alternating maximization: Unifying framework for 8 sparse PCA\nformulations and efficient parallel codes,” [Online] http://arxiv.org/abs/1212.4137.\n[7] D. P. Bertsekas and J. N. Tsitsiklis, Parallel and distributed computation: Numerical methods. Upper Saddle River, NJ,\nUSA: Prentice-Hall, Inc., 1989.\n[8] F. Niu, B. Recht, C. Re, and S. J. Wright, “Hogwild!: A lock-free approach to parallelizing stochastic gradient descent,”\nProc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 693-701, 2011, [Online] http://arxiv.org/ abs/1106.5730. [9] A. Agarwal and J. C. Duchi, “Distributed delayed stochastic optimization,” Proc. Advances in Neural Information Processing\nSystems (NIPS), vol. 24, pp. 873-881, 2011, [Online] http://arxiv.org/abs/1104.5525.\n[10] M. Li, L. Zhou, Z. Yang, A. Li, F. Xia, D. G. Andersen, and A. Smola, “Parameter server for distributed machine learning,”\n[Online] http://www.cs.cmu.edu/∼muli/file/ps.pdf.\n[11] M. Li, D. G. Andersen, and A. Smola, “Distributed delayed proximal gradient methods,” [Online] http://www.cs.cmu.edu/\n∼muli/file/ddp.pdf.\n[12] J. Liu and S. J. Wright, “Asynchronous stochastic coordinate descent: Parallelism and convergence properties,” SIAM J.\nOptim.,, vol. 25, no. 1, pp. 351–376, Feb. 2015.\n[13] M. Razaviyayn, M. Hong, Z.-Q. Luo, and J. S. Pang, “Parallel successive convex approximation for nonsmooth nonconvex\noptimization,” in the Proceedings of the Neural Information Processing (NIPS), 2014.\n[14] G. Scutari, F. Facchinei, P. Song, D. P. Palomar, and J.-S. Pang, “Decomposition by partial linearization: Parallel\noptimization of multi-agent systems,” IEEE Transactions on Signal Processing, vol. 63, no. 3, pp. 641–656, 2014.\n[15] B. He and X. Yuan, “On the o(1/n) convergence rate of Douglas-Rachford alternating direction method,” SIAM J. Num.\nAnal., vol. 50, 2012.\n[16] M. Hong, Z.-Q. Luo, and M. Razaviyayn, “Convergence analysis of alternating direction method of multipliers for a family\nof nonconvex problems,” technical report; available on http://arxiv.org/pdf/1410.1390.pdf.\nSeptember 10, 2015 DRAFT\n25\n[17] W. Deng and W. Yin, “On the global and linear convergence of the generalized alternating direction method of multipliers,”\nRice CAAM technical report 12-14, 2012.\n[18] W. Shi, Q. Ling, K. Yuan, G. Wu, and W. Yin, “On the linear convergence of the ADMM in decentralized consensus\noptimization,” IEEE Trans. Signal Process., vol. 62, no. 7, pp. 1750–1761, April 2014.\n[19] T.-H. Chang, M. Hong, and X. Wang, “Multi-agent distributed optimization via inexact consensus ADMM,” IEEE Trans.\nSignal Process., vol. 63, no. 2, pp. 482–497, Jan. 2015.\n[20] D. Jakovetić, J. M. F. Moura, and J. Xavier, “Linear convergence rate of class of distributed augmented lagrangian\nalgorithms,” to appear in IEEE Trans. Automatic Control.\n[21] M. Hong and Z.-Q. Luo, “On the linear convergence of the alternating direction method of multipliers,” available on\narxiv.org.\n[22] R. Zhang and J. T. Kwok, “Asynchronous distributed ADMM for consensus optimization,” in Proc. 31th ICML, , 2014.,\nBeijing, China, June 21-26, 2014, pp. 1–9.\n[23] T.-H. Chang, M. Hong, W.-C. Liao, and X. Wang, “Asynchronous distributed ADMM for large-scale optimization- Part I:\nAlgorithm and convergence analysis,” submitted for publication.\n[24] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge, UK: Cambridge University Press, 2004. [25] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding algorithm for linear inverse problems,” SIAM J. Imaging\nSci., vol. 2, no. 1, pp. 183–202, 2009.\n[26] A. J. Hoffman, “On approximate solutions of systems of linear inequalities,” Journal of Research of the National Bureau\nof Standards,, vol. 49, pp. 263–265, 1952.\nSeptember 10, 2015 DRAFT\n26\nAlgorithm 1 Asynchronous Distributed ADMM for (2).\n1: Algorithm of the Master: 2: Given initial variable x0 and broadcast it to the workers. Set k = 0 and d1 = · · · = dN = 0; 3: repeat 4: wait until receiving {x̂i, λ̂i}i∈Ak from workers i ∈ Ak and that di < τ − 1 ∀i ∈ A c k. 5: update\nxk+1i =\n{ x̂i ∀i ∈ Ak\nxki ∀i ∈ A c k\n, (6)\nλk+1i =\n{ λ̂i ∀i ∈ Ak\nλki ∀i ∈ A c k\n, (7)\ndi =\n{ 0 ∀i ∈ Ak\ndi + 1 ∀i ∈ A c k\n, (8)\nxk+10 =arg min x0∈Rn\n{ h(x0)− x T 0 ∑N i=1 λ k+1 i\n+ ρ2 ∑N i=1 ‖x k+1 i − x0‖ 2 + γ2‖x0 − x k 0‖ 2 } , (9)\n6: broadcast xk+10 to the workers in Ak. 7: set k ← k + 1. 8: until a predefined stopping criterion is satisfied.\n1: Algorithm of the ith Worker: 2: Given initial λ0 and set ki = 0. 3: repeat 4: wait until receiving x̂0 from the master node. 5: update\nxki+1i = arg min xi∈Rn fi(xi) + x T i λ ki i + ρ 2‖xi − x̂0‖ 2, (10) λki+1i = λ ki i + ρ(x ki+1 i − x̂0). (11)\n6: send (xki+1i ,λ ki+1 i ) to the master node. 7: set ki ← ki + 1. 8: until a predefined stopping criterion is satisfied.\nSeptember 10, 2015 DRAFT"
    } ],
    "references" : [ {
      "title" : "Asynchronous distributed alternating direction method of multipliers: Algorithm and convergence analysis",
      "author" : [ "T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang" ],
      "venue" : "submitted to NIPS, Montreal, Canada, Dec. 7-12, 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1–122, 2011.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sparisty and smoothness via the fused lasso",
      "author" : [ "R. Tibshirani", "M. Saunders" ],
      "venue" : "J. R. Statist. Soc. B, vol. 67, no. 1, pp. 91–108, 2005.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Large-scale sparse logistic regression",
      "author" : [ "J. Liu", "J. Chen", "J. Ye" ],
      "venue" : "Proc. ACM Int. Conf. on Knowledge Discovery and Data Mining, New York, NY, USA, June 28 - July 1, 2009, pp. 547–556.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "Alternating maximization: Unifying framework for 8 sparse PCA formulations and efficient parallel codes",
      "author" : [ "P. Richtárik", "M. Takáč", "S.D. Ahipasaoglu" ],
      "venue" : "[Online] http://arxiv.org/abs/1212.4137.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1212
    }, {
      "title" : "Parallel and distributed computation: Numerical methods",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Upper Saddle River, NJ, USA: Prentice-Hall, Inc.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1989
    }, {
      "title" : "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "F. Niu", "B. Recht", "C. Re", "S.J. Wright" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 693-701, 2011, [Online] http://arxiv.org/ abs/1106.5730.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Distributed delayed stochastic optimization",
      "author" : [ "A. Agarwal", "J.C. Duchi" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 873-881, 2011, [Online] http://arxiv.org/abs/1104.5525.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Parameter server for distributed machine learning",
      "author" : [ "M. Li", "L. Zhou", "Z. Yang", "A. Li", "F. Xia", "D.G. Andersen", "A. Smola" ],
      "venue" : "[Online] http://www.cs.cmu.edu/∼muli/file/ps.pdf.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Distributed delayed proximal gradient methods",
      "author" : [ "M. Li", "D.G. Andersen", "A. Smola" ],
      "venue" : "[Online] http://www.cs.cmu.edu/ ∼muli/file/ddp.pdf.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Asynchronous stochastic coordinate descent: Parallelism and convergence properties",
      "author" : [ "J. Liu", "S.J. Wright" ],
      "venue" : "SIAM J. Optim.,, vol. 25, no. 1, pp. 351–376, Feb. 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Parallel successive convex approximation for nonsmooth nonconvex optimization",
      "author" : [ "M. Razaviyayn", "M. Hong", "Z.-Q. Luo", "J.S. Pang" ],
      "venue" : "the Proceedings of the Neural Information Processing (NIPS), 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Decomposition by partial linearization: Parallel optimization of multi-agent systems",
      "author" : [ "G. Scutari", "F. Facchinei", "P. Song", "D.P. Palomar", "J.-S. Pang" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 63, no. 3, pp. 641–656, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the o(1/n) convergence rate of Douglas-Rachford alternating direction method",
      "author" : [ "B. He", "X. Yuan" ],
      "venue" : "SIAM J. Num. Anal., vol. 50, 2012.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems",
      "author" : [ "M. Hong", "Z.-Q. Luo", "M. Razaviyayn" ],
      "venue" : "technical report; available on http://arxiv.org/pdf/1410.1390.pdf. September 10, 2015  DRAFT  25",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On the global and linear convergence of the generalized alternating direction method of multipliers",
      "author" : [ "W. Deng", "W. Yin" ],
      "venue" : "Rice CAAM technical report 12-14, 2012.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the linear convergence of the ADMM in decentralized consensus optimization",
      "author" : [ "W. Shi", "Q. Ling", "K. Yuan", "G. Wu", "W. Yin" ],
      "venue" : "IEEE Trans. Signal Process., vol. 62, no. 7, pp. 1750–1761, April 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multi-agent distributed optimization via inexact consensus ADMM",
      "author" : [ "T.-H. Chang", "M. Hong", "X. Wang" ],
      "venue" : "IEEE Trans. Signal Process., vol. 63, no. 2, pp. 482–497, Jan. 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Linear convergence rate of class of distributed augmented lagrangian algorithms",
      "author" : [ "D. Jakovetić", "J.M.F. Moura", "J. Xavier" ],
      "venue" : "to appear in IEEE Trans. Automatic Control.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "On the linear convergence of the alternating direction method of multipliers",
      "author" : [ "M. Hong", "Z.-Q. Luo" ],
      "venue" : "available on arxiv.org.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Asynchronous distributed ADMM for consensus optimization",
      "author" : [ "R. Zhang", "J.T. Kwok" ],
      "venue" : "Proc. 31th ICML, , 2014., Beijing, China, June 21-26, 2014, pp. 1–9.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Asynchronous distributed ADMM for large-scale optimization- Part I: Algorithm and convergence analysis",
      "author" : [ "T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang" ],
      "venue" : "submitted for publication.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM J. Imaging Sci., vol. 2, no. 1, pp. 183–202, 2009.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 4,
      "context" : "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 5,
      "context" : "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.",
      "startOffset" : 255,
      "endOffset" : 258
    }, {
      "referenceID" : 1,
      "context" : "Distributed optimization algorithms that can scale well with large-scale instances of (1) have drawn significant attention in recent years [2], [7]–[14].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 6,
      "context" : "Distributed optimization algorithms that can scale well with large-scale instances of (1) have drawn significant attention in recent years [2], [7]–[14].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "Distributed optimization algorithms that can scale well with large-scale instances of (1) have drawn significant attention in recent years [2], [7]–[14].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]–[20].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]–[20].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]–[20].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]–[20].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "For example, for general convex problems, references [2], [7] showed that the ADMM is guaranteed to converge to an optimal solution and [15] showed that the ADMM has a worst-case O(1/k) convergence rate, where k is the iteration number.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : "For example, for general convex problems, references [2], [7] showed that the ADMM is guaranteed to converge to an optimal solution and [15] showed that the ADMM has a worst-case O(1/k) convergence rate, where k is the iteration number.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "For example, for general convex problems, references [2], [7] showed that the ADMM is guaranteed to converge to an optimal solution and [15] showed that the ADMM has a worst-case O(1/k) convergence rate, where k is the iteration number.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "Considering non-convex problems with smooth fi’s, reference [16] presented conditions for which the distributed ADMM converges to the set of Karush-KuhnTucker (KKT) points.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 16,
      "context" : "For problems with strongly convex and smooth fi’s or problems satisfying certain error bound condition, references [17] and [21] respectively showed that the ADMM can even exhibit a linear convergence rate.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "For problems with strongly convex and smooth fi’s or problems satisfying certain error bound condition, references [17] and [21] respectively showed that the ADMM can even exhibit a linear convergence rate.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : "References [18]–[20] also showed similar linear convergence conditions for some variants of distributed ADMM in a network with a general topology.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 19,
      "context" : "References [18]–[20] also showed similar linear convergence conditions for some variants of distributed ADMM in a network with a general topology.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "However, the distributed ADMM in [2], [16] have assumed a synchronous network, where at each iteration, the master always waits until all",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "However, the distributed ADMM in [2], [16] have assumed a synchronous network, where at each iteration, the master always waits until all",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 21,
      "context" : "To improve the time efficiency, the works [22], [23] have generalized the distributed ADMM to an asynchronous network.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 22,
      "context" : "To improve the time efficiency, the works [22], [23] have generalized the distributed ADMM to an asynchronous network.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 21,
      "context" : "Specifically, in the asynchronous distributed ADMM (AD-ADMM) proposed in [22], [23], the master does not necessarily wait for all the workers.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : "Specifically, in the asynchronous distributed ADMM (AD-ADMM) proposed in [22], [23], the master does not necessarily wait for all the workers.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "Theoretically, it has been shown in [23] that the AD-ADMM is guaranteed to converge (to a KKT point) even for non-convex problem (1), under a bounded delay assumption only.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : "Firstly, beyond the convergence analysis in [23], we further present the conditions for which the AD-ADMM can exhibit a linear convergence rate.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 16,
      "context" : "To the best of our knowledge, our results are novel, and are by no means extensions of the existing analyses [17]–[21] for synchronous ADMM.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 20,
      "context" : "To the best of our knowledge, our results are novel, and are by no means extensions of the existing analyses [17]–[21] for synchronous ADMM.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "Synopsis: Section II reviews the AD-ADMM in [23].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 22,
      "context" : "ASYNCHRONOUS DISTRIBUTED ADMM In this section, we review the AD-ADMM proposed in [23].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "By applying the standard ADMM [7] to problem (2), one obtains the following three simple steps: for iteration k = 0, 1, .",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "1 in [23]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 21,
      "context" : "The distributed ADMM has been extended to an asynchronous network in [22], [23].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : "The distributed ADMM has been extended to an asynchronous network in [22], [23].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "This condition guarantees that the variable information is at most τ iterations old, and is known as the partially asynchronous model [7]: Assumption 1 (Bounded delay) Let τ ≥ 1 be a maximum tolerable delay.",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 4,
      "context" : "Interestingly, such structured cost function appears in many machine learning problems, for example, the least squared problem and the logistic regression problem [5].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 16,
      "context" : "Since it has been known that the (synchronous) distributed ADMM [17]–[21] can converge linearly given the same structured cost functions in Assumption 3 and Assumption 4, the convergence results presented above demonstrate that the linear convergence property can be preserved in the asynchronous network.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 20,
      "context" : "Since it has been known that the (synchronous) distributed ADMM [17]–[21] can converge linearly given the same structured cost functions in Assumption 3 and Assumption 4, the convergence results presented above demonstrate that the linear convergence property can be preserved in the asynchronous network.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 23,
      "context" : "For each worker, we employed the fast iterative shrinkage thresholding algorithm (FISTA) [25] to solve the corresponding subproblem (10).",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 21,
      "context" : "In our experiments, analogous to [22], we further constrained the minimum size of the active set Ak by |Ak| ≥ A where A ∈ [1, N ] is an integer.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : "CONCLUSIONS In this paper, we have analytically studied the linear convergence conditions of the AD-ADMM proposed in [23].",
      "startOffset" : 117,
      "endOffset" : 121
    } ],
    "year" : 2015,
    "abstractText" : "The alternating direction method of multipliers (ADMM) has been recognized as a versatile approach for solving modern large-scale machine learning and signal processing problems efficiently. When the data size and/or the problem dimension is large, a distributed version of ADMM can be used, which is capable of distributing the computation load and the data set to a network of computing nodes. Unfortunately, a direct synchronous implementation of such algorithm does not scale well with the problem size, as the algorithm speed is limited by the slowest computing nodes. To address this issue, in a companion paper, we have proposed an asynchronous distributed ADMM (AD-ADMM) and studied its worst-case convergence conditions. In this paper, we further the study by characterizing the conditions under which the AD-ADMM achieves linear convergence. Our conditions as well as the resulting linear rates reveal the impact that various algorithm parameters, network delay and network size have on the algorithm performance. To demonstrate the superior time efficiency of the proposed AD-ADMM, we test the ADADMM on a high-performance computer cluster by solving a large-scale logistic regression problem. Keywords− Distributed optimization, ADMM, Asynchronous, Consensus optimization ⋆Tsung-Hui Chang is the corresponding author. Address: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 518172, E-mail: tsunghui.chang@ieee.org. Wei-Cheng Liao is with Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455, USA, E-mail: mhong@umn.edu Mingyi Hong is with Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, 50011, USA, E-mail: mingyi@iastate.edu Xiangfeng Wang is with Shanghai Key Lab for Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, 200062, China, E-mail: xfwang@sei.ecnu.edu.cn September 10, 2015 DRAFT",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}