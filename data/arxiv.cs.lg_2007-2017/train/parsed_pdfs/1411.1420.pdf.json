{
  "name" : "1411.1420.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning a Hidden Basis Through Imperfect Measurements: An Algorithmic Primitive",
    "authors" : [ "Mikhail Belkin" ],
    "emails" : [ "mbelkin@cse.ohio-state.edu", "lrademac@cse.ohio-state.edu", "vossj@cse.ohio-state.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n14 20\nv1 [\ncs .L\nG ]\n5 N\nWe describe a new algorithm, “gradient iteration”, for provable recovery of the hidden basis. We provide a complete theoretical analysis of Gradient Iteration both for the exact case as well as for the case when the observed function is a perturbation of the “true” underlying BEF. In both cases we show convergence and complexity bounds polynomial in dimension and other relevant parameters, such as perturbation size. Our perturbation results can be considered as a very general non-linear version of the classical Davis-Kahan theorem for eigenvectors of perturbations of symmetric matrices. In addition we show that in the exact case the algorithm converges superlinearly and give conditions relating the degree of convergence to properties of the Basis Encoding Function. Our algorithm can be viewed as a generalization of the classical power iteration method for eigenanalysis of symmetric matrices as well as a generalization of power iterations for tensors. Moreover, the Gradient Iteration algorithm can be easily and efficiently implemented in practice."
    }, {
      "heading" : "1 Introduction",
      "text" : "A good algoritmic primitive is a procedure which is simple, allows for theoretical analysis and, ideally, for efficient implementation. It should also be applicable to a range of interesting problems. An example of an extremely successful and widely used primitive, both in theory and practice, is diagonalization/eigendecomposition of symmetric matrices.\nThe goal of this paper is to propose learning a hidden basis from noisy observations as a new algorithmic primitive and to provide the underlying algorithmic framework and its theoretical analysis. Our approach can be viewed as a non-linear/non-tensorial generalization of the classical matrix diagonalization results and perturbation analyses. We will show that a number of problems of recent theoretical and practical interest can be viewed within our setting.\nConsider {z1, . . . , zm} a set of orthogonal unit vectors in Rd. Choosing a set of one-dimensional contrast functions1 gi : R → R, we define the Basis Encoding Function (BEF) F : Rd → R as\nF (u) := m ∑\ni=1\ngi(〈u, zi〉) . (1)\nOur goal will be to recover the set {z1, . . . , zm} (fully or partially) through access to F (u) (the exact setting), or to provide a provable approximation to these given vectors given an estimate of F (u) (noisy/perturbation setting). We will see that in a number of different settings, the relevant information about the problem can be encoded as a BEF (Section 2.1).\nIn what follows, we will primarily be interested in the behavior of the BEF F (u) restricted to the unit sphere in Rd. It turns out that the behavior of F (u) on the sphere is closely related to the structure of the basis elements. We will describe a large class of admissible contrast functions gi such that local maxima of F (u) on the sphere are in one-to-one correspondence to the basis vectors zi (or their opposite directions). This perhaps surprising behavior (as F (u) can have lots of minima and other critical points) is due to certain “hidden convexity” of the BEF on the sphere. Moreover, it turns out that these maxima are stable under perturbations of F (u) and no spurious maxima are created.\nWe propose an algorithm for recovering the hidden basis based on what may be called a “gradient iteration” algorithm. The basic algorithm consists simply of replacing the point with the gradient at each step of the iteration using the map u 7→ ∇F (u)/‖∇F (u)‖. We will show that the maxima of F (u) on the sphere are the only stable fixed points of this map. While in general it is possible (although not likely in practice) for the iteration to converge to an unstable fixed point, we will provide a slight modification of this algorithm (by appropriately resetting the starting point) that can be guaranteed to converge to one of the basis vectors (or to approximate such vectors in the noisy case). By repeating the procedure appropriately we can provably recover the basis. We provide complete theoretical analysis in terms of the perturbation size and the computational complexity of the algorithm. Our bounds are low degree polynomial in all relevant parameters, including the dimension, the number of basis elements to be recovered and the perturbation size. They can be considered as a non-linear version of the classical Davis-Kahan perturbation theorem [11] for eigenvectors of symmetric matrices. Moreover, for the non-perturbed case we show superlinear convergence, in contrast to the linear convergence of the standard power iteration for matrices. We provide additional conditions on the contrast functions gi to obtain specific higher orders of convergence.\nWe proceed to show that a number of problems can be viewed in terms of hidden basis recovery. In particular, we briefly discuss how our primitive can be used to recover clusters in spectral clustering, independent components in Independent Component Analysis (ICA), parameters of Gaussian mixtures and certain tensor decompositions.\n1We call gi’s contrast functions following the Independent Component Analysis (ICA) terminology. Note, however, that in the ICA setting our “contrast functions” correspond to different scalings of the ICA contrast function.\nConnection to the power method for symmetric matrices and tensors. Our algorithm can be viewed as a generalization of the classical power iteration method for eigendecomposition of symmetric matrices. Let A be a symmetric matrix. Put F (u) = 〈Au,u〉. From the spectral theorem for matrices, we have F (u) = ∑\ni λi(〈u, zi〉)2. We see that F (u) is a BEF with the hidden basis zi representing the eigenvectors of A and the contrast functions gi(x) = λix2, with λi being the eigenvalues of A. It is easy to see that our gradient iteration is exactly the power method based on the map u 7→ Au/‖Au‖. We note that it is not necessary to know gi(x) to have access to the BEF F (u).\nIn a recent work [2], a form of orthogonal tensor decomposition was proposed for solving a variety of problems by generalizing previous works on learning mixtures of spherical Gaussians [14], latent Dirichlet allocation [1], and learning hidden Markov models [3]. The authors also introduced a tensor power method. We will see (Sections 2.1 and 4.2) their setting also fits within our framework by choosing the contrast functions to be gi(x) = λixr, r ≥ 3.\nPerhaps counter-intuitively, our results imply that the success of these methods for certain problems does not rely on their tensorial structure but on certain “hidden convexity” inherent in the problem."
    }, {
      "heading" : "2 Problem description and main results",
      "text" : "We consider a function optimization framework for hidden basis recovery. More formally, let {z1, . . . , zm} be a non-empty set of orthogonal unit vectors in Rd. These unit vectors form the unseen basis. A function on a closed unit ball F : B(0, 1) → R is defined from “contrast functions” gi : [−1, 1] → R as:\nF (u) :=\nm ∑\ni=1\ngi(〈u, zi〉) . (2)\nWe call F a basis encoding function (BEF) with the associated tuples {(gi, zi) | i ∈ [m]}. The goal is to recover the hidden basis vectors zi for i ∈ [m] up to sign given evaluation access to F and its gradient. We will assume that d ≥ 2 since otherwise the problem is trivial. We only consider contrast functions gi ∈ C(2)([−1, 1]) which satisfy the following assumptions:\nA1. Each gi is either an even or odd function. A2. For each i, either gi( √ x) or −gi( √ x) is strictly convex on [0, 1]. A3. For each i ∈ [m], ddxgi( √ x)|x=0 = 0.\nA4. For each i ∈ [m], gi(0) = 0. From now on F and the term BEF will refer to a BEF with associated zi’s and gi’s satisfying Assumptions A1–A4 unless otherwise stated. Remark: The Assumption A4 is non-essential. If each gi satisfies A1–A3, then x 7→ [gi(x)−gi(0)] satisfies A1–A4 making [F (u)− F (0)] =∑mi=1[gi(〈u, zi〉)− gi(0)] a BEF of the desired form.\nWe shall see that BEFs arise naturally in a number of problems, and also that given a BEF, the directions z1, . . . , zm can be efficiently recovered up to sign."
    }, {
      "heading" : "2.1 Examples of algorithmic problems solvable via basis encoding functions",
      "text" : "Spectral clustering. Spectral clustering is a class of methods for multiway cluster analysis. We describe now a prototypical version of the method that works in two phases [5, 21, 23, 27]. The first phase, spectral embedding, constructs a similarity graph based on the features of the data and then embeds the data in Rd\n(where d is the number of clusters) using the bottom d eigenvectors of the Laplacian matrix of the similarity graph. The second phase clusters the embedded data using a variation of the k-means algorithm. A key aspect in the justification of spectral clustering is the following observation: If the graph has d connected components, then a pair of data points is either mapped to the same vector if they are in the same connected component or mapped to orthogonal vectors if they are in different connected components [26]. If the graph\nis close to this ideal case, which can be interpreted as a realistic graph with d clusters, then the embedding is close to that ideal embedding.\nThis suggests the following alternate approach (introduced in [7]) to the second phase of spectral clustering by interpreting it as a hidden basis recovery problem: Let x1, . . . ,xn ∈ Rd be the embedded points. Let g : R → R be a function satisfying Assumptions A1–A4. Let\nF (u) =\nn ∑\ni=1\ng(〈u, xi〉). (3)\nIn the ideal case we have {x1, . . . ,xn} = {b1Z1, . . . , bdZd}, where {Zj}dj=1 is an orthonormal basis and {bj}dj=1 are positive scalars. Thus, in the ideal case we can write\nF (u) = d ∑\nj=1\najg(bj〈u, Zj〉)\nwhere aj is the number of points in the jth connected component. That is, F is a BEF in the ideal case. In the general case it is a perturbed BEF and the hidden basis can be approximately recovered using our robust algorithm (Section 6). Note that, via (3), F and its derivatives can be evaluated at any u just with the knowledge of the xis, without knowing the hidden basis.\nIndependent component analysis (ICA). In the ICA model, one observes samples of the random vector X = AS where A ∈ Rd×d is a mixing matrix and S = (S1, . . . , Sd) is a latent random vector such that the Sis are mutually independent and non-Gaussian. The goal is to recover the mixing matrix A = [A1| · · · |Ad], typically with the goal of using A−1 to invert the mixing process and recover the original signals. This recovery is possible up to natural indeterminacies, namely the ordering of the columns of A and the choice of the sign of each Ai [9]. ICA has a vast literature (see the books [10, 16] for a broad overview) with numerous applications including speech separation [20], denoising of EEG/MEG brain recordings [24], and various vision tasks [6, 8] to name a few.\nTo demonstrate that ICA fits within our BEF framework, we rely on the properties of the cumulant statistics2 In particular, letting κr(X) denote the rth cumulant of a random variable X, then κr(X) satisfies the following: (1) Homogeneity: κr(αX) = αrκr(X) for any α ∈ R and (2) Additivity: if X and Y are independent, then κr(X+Y ) = κr(X)+κr(Y ). Given an ICA model X = AS, these properties imply that for any u ∈ Rd, κr(〈u, X〉) = κr( ∑d i=1〈u, Ai〉Si) = ∑d i=1〈u, Ai〉rκr(Si). A preprocessing step called whitening (i.e., linearly transforming the observed data to have identity covariance) makes the columns of A into orthogonal unit vectors. Under whitening, the columns of A form a hidden basis of the space. In particular, defining the contrast functions gi(x) := xrκr(Si) and the basis encoding elements zi := Ai, then the function F (u) := κr(〈u, X〉) = ∑d i=1 gi(〈u, zi〉) is a BEF so long as each κr(Si) 6= 0. Further, the cumulants and their derivatives have natural sample estimates (see e.g.,[18, 25] for the third and fourth order estimates), and as such this choice of F will be admissible to our algorithmic framework for basis recovery.\nInterestingly, it has been noted in several places (see e.g., [15, 22, 28]) that cubic convergence rates can be achieved using optimization techniques for recovering the directions Ai, particularly when performing ICA using the fourth cumulant or the closely related fourth moment. One explanation as to why this is possible arises from the dual interpretation of the “gradient iteration” algorithm (discussed at length later in this paper) as both an optimization technique and as a power method. In the ICA setting, the gradient iteration algorithm for cumulants was introduced in our paper [25]. This paper provides a significant generalization of those ideas as well as a theoretical analysis.\n2 An important class of ICA methods with guaranteed convergence to the columns of A are based on the optimization of κ4(〈u, X〉) over Sd−1 (see e.g., [4, 12, 15]). Other contrast functions are also frequently used in the practical implementations of ICA (see e.g., [17]). However these do not have analogous guarantees of the non-existence of spurious maxima.\nOrthogonal tensor decompositions. In a recent work [2], it was shown that a form of orthogonal tensor decomposition applies to a variety of problems including ICA and previous works on learning mixtures of spherical Gaussians [14], latent Dirichlet allocation [1], and learning hidden Markov models [3].\nTheir framework involves using the moments of the various models to obtain a tensor of the form T = ∑m\nk=1wkµ ⊗r k where (1) each wk ∈ R\\{0}, (2) each µk ∈ Rd is a unit vector, and (3) µ⊗rk is the tensor outer power defined by (µ⊗rk )i1...ir = (µk)i1 . . . (µk)ir . The µks may be assumed to have unit norm by changing the wks appropriately. In the special case where the µks are orthogonal, then the directions of µk can be recovered using the tensor power methods introduced in [2]. Treating T as an operator using the definition Tur := ∑\ni1,...,ik∈[d]k Ti1...irui1 · · · uir , it can be seen that Tu r = ∑m k=1wk〈u, µk〉r. In particular, the\nfunction F (u) = Tur is a BEF with the contrasts gi(x) := wixr and hidden basis elements zk := µk. In section 4.2, we will show that the tensor power method is a special case of our gradient iteration.\nParameter estimation in a Spherical Gaussian Mixture Model. A Gaussian Mixture Model (GMM) is a parametric family of probability distributions. A spherical GMM is a distribution whose density can be written in the form f(x) =\n∑k i=1wifi(x), where wi ≥ 0, ∑\niwi = 1 and fi is an d-dimensional Normal density with mean µi and covariance matrix σ 2 i I , for σi > 0. The parameter estimation problem is to estimate wi,µi, σi given i.i.d. samples of random vector x with density f . For clarity of exposition, we only discuss the case k = d and σi = σ for some fixed, unknown σ. Our argument is a variation of the moment method in [14]. As in [14], similar ideas should work for the case k < d and non-identical σis.\nWe explain how to recover the different parameters from observable moments. Firstly, σ2 is the smallest eigenvalue of the covariance matrix of x. This recovers σ. Let v be any unit norm eigenvector corresponding to the eigenvalue σ2. Define M2 = E(xxT ) − σ2I ∈ Rd×d. Then we have M2 = ∑d i=1wiµiµ T i . Denote D = diag(w1, . . . , wd), A = (µ1, . . . ,µd) ∈ Rd×d. With this notation we have M2 = ADAT . Let M = M\n1/2 2 (symmetric). This implies\nM = AD1/2R, (4)\nwhere R is some unitary matrix. We have E(〈x, u〉3) =∑di=1 wi〈µi, u〉3 + 3σ2‖u‖2 E(〈x, u〉). Then\nF (u) := E(〈x, M−1u〉3)− 3σ2‖M−1u‖2 E(〈x, M−1u〉) = d ∑\ni=1\nwi〈µi, M−1u〉3\n= d ∑\ni=1\nwi(u TRTD−1/2ei) 3 = d ∑\ni=1\nw −1/2 i 〈u, Ri·〉3\nis a BEF encoding the rows of R, with basis vectors zi = Ri· and contrasts gi(t) = w −1/2 i t\n3. The recovery of the rows of R allows, via (4), the recovery of the directions of the columns of A, that is, the directions of µis. The actual µis then can be recovered from the identity 〈µi, v〉 = 〈E(x), v〉. Finally, denoting w = (w1, . . . , wd) we have E(x) = Aw and we recover w = A−1 E(x)."
    }, {
      "heading" : "2.2 Summary of the results",
      "text" : "It will be convenient to we append arbitrary directions zm+1, . . . , zd to our hidden basis z1, . . . , zm such that z1, . . . , zd are orthonormal. For the remainder of this paper, we simplify our notation by indexing vectors in Rd with respect to this hidden basis z1, . . . , zd. In particular, for u ∈ Rd we have that ui is shorthand for the unobserved value 〈u, zi〉, and we may thus write F (u) = ∑m i=1 gi(ui).\nWe now state the first result indicating that a BEF may be used to recover the unseen basis.\nTheorem 2.1. The set {±zi | i ∈ [m]} is a complete enumeration of the local maxima of |F | with respect to the domain Sd−1.\nWe note that assumption A1 is stronger than is actually required in Theorem 2.1. In particular, we could replace Assumption A1 with the assumption that x 7→ gi(− √\n|x|) is either strictly convex or strictly concave on [−1, 0] for each i ∈ [m].\nTheorem 2.1 implies that a form of gradient ascent can be used to recover maxima of |F | and hence the hidden basis. However, the performance of gradient ascent is dependent on the choice of a learning the rate parameter. It turns out that there is a simple and practical parameter-free alternative to gradient ascent for finding the hidden basis elements zi in this setting.\nWe associate with F the gradient iteration function G : Sd−1 → Sd−1 defined by\nG(u) := { ∇F (u) ‖∇F (u)‖ if ∇F (u) 6= 0 u otherwise.\nWe would like to treat G as a fixed point method for recovering the hidden basis elements. However, there is a difficulty: At any given step, the derivative ∂iF (u) := ∂∂uiF (u) can be of a different sign than ui causing ui and Gi(u) to have opposite sign. Given a sequence {u(n)}∞n=0 defined recursively by u(n) = G(u(n− 1)), it may happen that for oscillating sign values s(i) ∈ {−1,+1} the sequence s(n)u(n) → zi as n → ∞. Since we do not distinguish between recovery of zi and −zi, the sequence {u(n)}∞i=0 should be viewed as recovering zi even though it is oscillating.\nTo fix this issue, we divide Sd−1 into equivalence classes using the equivalence relation v ∼ u if |vi| = |ui| for each i ∈ [d]. Given v ∈ Sd−1, we denote by [v] its corresponding equivalence class. The resulting quotient space Sd−1/∼ may informally be thought of as an orthant of the sphere, i.e., Qd−1+ := {u ∈ Sd−1 | ui ≥ 0 for each i ∈ [d]}. There is a bijection φ : Sd−1/∼ → Qd−1+ given by φ([u]) = ∑d i=1 |ui|zi, and we treat Sd−1/∼ as a metric space with the metric µ([u], [v]) = ‖φ([v]) − φ([u])‖. Under Assumption A1, if u ∼ v then G(u) ∼ G(v). As such, sequences are consistently defined modulo this equivalence class, and we consider the fixed points of G/∼.\nWe will use the following terminology. A class [v] is a fixed point of G/∼ if G(v) ∼ v. Consider sequences of the form {u(n)}∞n=0 defined recursively by u(n) = G(u(n − 1)). A class [v] is Lyapunov stable if for every neighborhood N of [v] there exists a neighborhood N ′ ⊂ N of [v] such that if [u(0)] ∈ N ′ then [u(n)] ∈ N ′ for every n ∈ N. A class [v] is unstable if it is not Lyapunov stable. Finally, a Lyapunov stable class [v] is an attractor of G/∼ if there exists a neighborhood N of [v] such that for any [u(0)] ∈ N , the sequence [u(n)] → [v] as n → ∞.\nIn addition, we will sometimes refer to a vector v ∈ Sd−1 as a fixed point of G/∼. This is a slight abuse of terminology which should be understood to mean that [v] is a fixed point of G/∼.\nThe following results demonstrate that the attractors of G/∼ are precisely the hidden basis elements, and that convergence to these fixed points is fast (super-linear).\nTheorem 2.2 (Gradient iteration stability). The classes {[zi] | i ∈ [m]} are fixed points of the G/∼. Further, the following hold:\n1. The equivalence classes {[zi] | i ∈ [m]} are attractors of G/∼. 2. All fixed points of G/∼ not in the set {zi | i ∈ [m]} are unstable.\nTheorem 2.3 (Gradient iteration convergence). Let {u(n)}∞n=0 be a sequence defined recursively from a starting u(0) ∈ Sd−1 and u(n) = G(u(n − 1)). Then following hold:\n1. If [u(n)] → [zi] as n → ∞, then the convergence is superlinear. Further, if there exists r ≥ 2 such that x 7→ gi(x1/r) is convex on [0, 1], then the rate of convergence is at least order r − 1. 2. If u(0) ⊥ zi, then [u(n)] 6→ [zi] as n → ∞.\nThe above Theorems imply the following practical algorithm for recovering the hidden basis elements: First choose a vector u ∈ Sd−1 and perform the iteration u ← G(u) until convergence is achieved to\nrecover a single hidden basis direction. Note that convergence can be measured by looking at min(‖G(u)− u‖, ‖−G(u) − u‖). Then, to recover an additional hidden basis direction, one may repeat the procedure with a new starting vector u in the orthogonal complement to previously found hidden basis elements. We refer to this process as the Gradient iteration algorithm.\nThe gradient iteration algorithm forms an interesting link between function optimization and power method techniques. In Section 4.1, we show that the gradient iteration update performs an implicit gradient ascent step with an adaptive learning rate. In Section 4.2, we show that the gradient iteration algorithm is also an extension of the power method for matrices.\nFrom a practical standpoint, the fast convergence properties of the gradient iteration make it an ideal algorithm for hidden basis recovery. However, it is possible (though unlikely) to get stuck in an unstable fixed point of G/∼. However, it is possible to augment the gradient iteration algorithm with Hessian informed restarts in order to achieve guaranteed basis recovery even when we only observe a perturbation of F . More formally, we call F̂ an ǫ-approximation of F if for every u ∈ B(0, 1) the following hold: |F (u)− F̂ (u)| < ǫ, ‖∇F (u) − ∇F̂ (u)‖ < ǫ, and ‖HF (u) − HF̂ (u)‖ < ǫ. If F satisfies a strong version of Assumption A2, namely that for some strictly positive cmin and cmax such that cmin ≤ ∣ ∣ ∣ d2 dx2 gi( √ x)|x=x0 ∣ ∣ ∣ ≤ cmax for each x0 ∈ [−1, 1] and for each i ∈ [m], then we call F a (cmin, cmax)robust BEF and we have the following result.\nTheorem 2.4. There exist positive constants C1, C2 such that the following holds. Suppose that ǫ ≤ C1cmin m3/2d2 · ( cmincmax ) 7/2. If F̂ is an ǫ-approximation to a (cmin, cmax)-robust BEF F , then there exists a deterministic algorithm ROBUSTGI-RECOVERY which approximately recover the hidden basis elements using the following operations: Oracle computations of ∇F̂ (u), oracle eigendecompositions of HF̂ (u), basic arithmetic operations on scalars, the square root, and inner products in Rd. ROBUSTGI-RECOVERY recovers each zi (i ∈ [m]) up to sign within error 4m √ 2d\ncmin ǫ when run for sufficiently many steps N .\n• If m is known, it suffices that N ≥ C2[ c 2 maxm 3d c2min log(2 √ 2mcmax cmin )+log( cminǫ )+md 2]. Further, ROBUSTGI-\nRECOVERY uses at most 4m2 eigendecompositions of HF̂ (u). • If m is unknown, it suffices that N ≥ C2[ c 2 maxmd 3\nc2min log(mcmaxcmin )+ log( cmin ǫ )+d 3]. Further, ROBUSTGI-\nRECOVERY uses at most 4d2 eigendecompositions of HF̂ (u). In section 6, we state the algorithm ROBUSTGI-RECOVERY and give explicit values for the constants\nC1 and C2. It should also be noted that in the special case where we have exact access to F , then F is an ǫ-approximation to itself for any ǫ > 0. As a corollary to Theorem 2.4, given exact access to F and its derivatives we may efficiently recover the hidden basis elements within any error δ."
    }, {
      "heading" : "3 Extrema structure of the optimization framework",
      "text" : "The optima structure of F relies heavily upon a hidden convexity structure implied by Assumption A2. To better capture this structure, we define functions hi : [−1, 1] → R as hi(x) := gi(sign(x) √\n|x|) for i ∈ [m] and hi := 0 for i ∈ [d] \\ [m]. We may thus write\nF (u) =\nm ∑\ni=1\nhi(sign(ui)u 2 i ) . (5)\nNote that on [0, 1] (or respectively on [−1, 0]), each of these functions hi is either strictly convex or strictly concave. The functions hi have the following properties:\nLemma 3.1. Let s ∈ {+1,−1} be a sign value. Let σi+ = 1 if gi if it is convex on [0, 1], σi+ = −1 otherwise. Let σi− = 1 if gi if it is convex on [−1, 0], σi− = −1 otherwise. Let I+ = [0, 1] and let I− = [−1, 0]. For each i ∈ [m], the following hold:\n1. The function σishi is strictly convex on the domain Is. 2. h′i(0) = 0. 3. hi is continuously differentiable. 4. The derivative function σish′i is strictly increasing on Is. In particular, sσish ′ i(x) > 0 for any x ∈\nIs \\ {0}, and the function sσishi is strictly increasing on Is.\nProof. That σishi is strictly convex follows directly from Assumption A2 and (when s = −1) the subsequent definition of σi−. That h′i(0) = 0 is an implication of Assumption A3. That hi is continuously differentiable follows from the fact that\nh′i(x) =\n{\n1 2g ′ i( √ x)/ √ x if x 6= 0\n0 if x = 0\nexists everywhere, and is thus continuous due to the convexity of σishi (see [13, Corollary 4.2.3]). Property 4 can be seen using properties of convex functions. Let D+ denote the right derivative function. By strict convexity, D+(σishi) is a strictly increasing functions on Is. Since σishi is differentiable on Is, σish ′ i(x) and D+(σishi)(x) coincide for each x ∈ Is. As such, σish′i is strictly increasing on Is.\nIn order to avoid dealing with unnecessary sign values, it will be convenient to restrict ourselves to analyzing F over the domain Qd−1+ . We demonstrate that this can be done without loss of generality using Lemma 3.2 below. We identify each orthant by a sign vector v where each vi ∈ {+1,−1}, and in particular we define Qv := {u ∈ Sd−1 | viui ≥ 0 for each i ∈ [d]} as the orthant of Sd−1 containing v.\nLemma 3.2. Let F be a BEF with hidden basis z1, . . . , zd, and let v = ∑d i=1 sizi for sign values si ∈ {±1}. Define an isometry ψ : Sd−1 → Sd−1 by ψi(u) := siui. Then the function F̃ := F◦ψ is a BEF. In particular, F̃ |Qd−1+ is the pullback of F |Qd−1v with respect to ψ.\nProof. To construct F̃ = ∑m i=1 g̃i(〈u, z̃i〉), we set z̃i = sizi and g̃i = gi for each i ∈ [m]. It is easily verified that F̃ is a BEF. Further, our construction implies that for any u ∈ Sd−1 we have that\nF ◦ ψ(u) = m ∑\ni=1\ngi(siui) = m ∑\ni=1\ngi(〈u, z̃i〉) = F̃ (u) .\nLemma 3.2 demonstrates that by a simple manipulation of the signs of zi, we may relabel any of the hidden orthants Qv of Sd−1 as being the all positive orthant Q d−1 + within a new BEF F̃ . Thus when proving Theorem 2.1, it suffices to show that for a BEF F , the vectors z1, z2, . . . , zm give a complete enumeration of the maxima of |F ||Qd−1+ .\nTo characterize the extrema structure of |F ||Qd−1+ , we analyze the Lagrangian function L : B(0, 1) × R defined as: L(u, λ) := F (u) − λ[‖u‖2 − 1]. The following result enumerates the critical points of F with respect to the sphere.\nLemma 3.3. The pair (u, λ) is a critical point of L if and only if λχ[vi 6=0] = h′i(v2i ) for each i ∈ [d].\nProof. We set the derivative ∂\n∂ui L(u, λ) = 2h′i(u2i )ui − 2λui (6)\nequal to 0 to obtain h′i(u 2 i )ui = λiui. Noting that when ui = 0, h ′ i(v 2 i ) = h ′ i(0) = 0 by Assumption A3 gives the result.\nWhile there are potentially exponentially many critical points of L, it turns out that only the hidden basis directions correspond to maxima of F on the sphere.\nProposition 3.4. If j ∈ [m], then zj is a strict local maximum of |F | with respect to Qd−1+ .\nProof. We will prove the case where hj is strictly convex and note that the case hj is strictly concave is exactly the same when replacing F with −F .\nWe first note that F (zj) = hj(1) > 0 since hj is strictly increasing (see Lemma 3.1 property 4). In particular, using continuity of each gi, it follows that F (u) > 0 on a neighborhood of zj , and it suffices to demonstrate that F takes on a maximum with respect to Sd−1 at zj . Continuing from equation (6), we obtain\nD2 u L(u, λ) =\nm ∑\ni=1\nχ[ui 6=0][4h ′′ i (u 2 i )u 2 i + 2h ′ i(ui)]ziz T i − 2λI , (7)\nwhere Du is the derivative operator with respect to the variable u. The main difficulty in this calculation is showing that (∂2i F )(v) = 0 when vi = 0, which gives rise to the indicator function in the formula above. To see this, we note by the definition that for each i ∈ [m],\ng′′i (0) = lim c→0 g′i(c)− g′i(0) c = 2 lim c→0+ 1 2 g′i( √ c)√ c = 2 lim c→0+ ( d dx gi( √ x) ) ∣ ∣ ∣ ∣\nx=c\n= 0 .\nThe second equality uses that g′i(0) = 0, a fact which is implied by Assumption A3. The final equality uses that ddxgi( √ x) is continuous (see [13, Corollary 4.2.3]). As (∂2i F )(v) when vi = 0 is exactly g ′′ i (0), the formula (7) follows as claimed. We now use the Lagrangian criteria for constrained extrema (see e.g., [19, chapter 11] for a discussion\nof the first order necessary and second order sufficient conditions for constrained extrema) to show that zj is a maximum of F |Qd−1+ . From Lemma 3.3, we see that (zj , h ′ j(1)) is a critical point of L. Further, for any non-zero v ∈ Rd ∩ z⊥j , we have that vT (D2uL)(zj , h′j(1))v = −2h′j(1)‖v‖2. As h′j(1) > 0, it follows that vT (D2\nu L)(zj , h′j(1))v < 0. Thus, zj is a local maximum of F .\nProposition 3.5. If v ∈ Qd−1+ is not contained in the set {zi | i ∈ [m]}, then v is not a local maximum of |F | with respect to Qd−1+ .\nProof. We first consider the case in which v 6⊥ zi for at most one i ∈ [m]. We will call this i ∈ [m] for which vi 6= 0 as j if it exists and otherwise let j ∈ [m] be arbitrary. Fix any w ∈ Qd−1+ such that wj > vj and wi = 0 for i ∈ [m] \\ {j}. Such a choice is possible since v 6= zj implies vj < 1. Then, |F (v)| = |hj(v2j )| and |F (w)| = |hj(w2j )|. Since hj is a strictly increasing function on [0, 1] from hj(0) = 0 (see Lemma 3.1), it follows that |F (w)| > |F (v)|. Since w can be constructed in any open neighborhood of v, v is not a local maximum of |F | with respect to Qd−1+ .\nNow suppose that v is an extremum (either a maximum or a minimum) of |F | with respect to Qd−1+ such that there exists j, k ∈ [m] distinct such that vj 6= 0 and vk 6= 0. We will demonstrate that this implies that v is a minimum of |F |. Fix η > 0 sufficiently small that for any choice of δ such that |δ| ∈ (0, η) has w(δ) := (v〈2〉 + δzj − δzk)〈1/2〉 ∈ Qd−1+ . We now consider the difference F (w(δ)) − F (v):\nF (w(δ)) − F (v) = hj(wj(δ)2)− hj(v2j ) + hk(wk(δ)2)− hk(v2k) = h′j(xj(δ) 2)[wj(δ) 2 − v2j ] + h′k(xk(δ)2)[wk(δ)2 − v2k]\n= δ[h′j(xj(δ) 2)− h′k(xk(δ)2)] ,\nwhere xi(δ) ∈ (vj , wj(δ)) and xi(δ) ∈ (wk(δ), vk) under the mean value theorem.\nAs v must be an extremum of F in order to be an extremum of |F |, there exists λ such that the pair (v, λ) is a critical point of L. Let S = {i | vi 6= 0}. Lemma 3.3 implies that λ = h′i(v2i ) for i ∈ S . In particular, sign(h′i(v 2 i )) is the same for each i ∈ S , and we will call this sign value s. Under equation (5), we have F (v) = ∑\ni∈S hi(v 2 i ). By Lemma 3.1, shi is strictly increasing from shi(0) = 0 on [0, 1] for each\ni ∈ S . As such, F (v) is separated from 0 and sign(F (v)) = s. Further,\ns[F (w(δ)) − F (v)] = sδ[h′j(xj(δ)2)− h′k(xk(δ)2)] < sδ[λ− λ] = 0\nholds by noting that each sh′i is strictly increasing on [0, 1]. Thus, v is a minimum of |F |.\nTheorem 2.1 follows by combining Propositions 3.4 and 3.5 with Lemma 3.2."
    }, {
      "heading" : "4 Interpreting the gradient iteration",
      "text" : "In this section, we demonstrate that the gradient iteration algorithm has two main interpretations, first as an adaptive form of gradient ascent (section 4.1) and second as a generalization of the power method (section 4.2). These dual interpretations closely link the gradient iteration and other power methods with hill climbing techniques for finding the maxima of a function3 . Both interpretations of the gradient iteration are most easily understood using a special form of BEF defined below.\nDefinition 4.1. A BEF F (u) = ∑m i=1 gi(ui) is called a positive basis encoding function (PBEF) if x 7→ gi(sign(x) √ |x|) is strictly convex for each i ∈ [m].\nA PBEF has several nice properties not shared by all BEFs. Its name is justified by the fact that for a PBEF F , then for any u ∈ Sd−1, we have that F (u) ≥ 0. Further, when we expand F (u) = ∑m\ni=1 hi(sign(ui)u 2 i ) = ∑m i=1 hi(u 2 i ) under equation 5, we see that each hi is strictly convex over its entire\ndomain. Finally, given a BEF F , we construct a PBEF F̄ (u) := ∑m i=1 ḡi(ui) where ḡi(x) = |gi(x)|. We call F̄ the PBEF associated with F .\nFor PBEF the gradient iteration G becomes a true fixed point method without a need to consider equivalence classes (as in section 2.2). In particular, if φ and µ are defined as in section 2.2, then we have the following.\nLemma 4.2. Let v ∈ Rd be a sign vector (that is, vi ∈ {±1} for each i ∈ [d]). If u,w ∈ Qd−1v , then µ([u], [w]) = ‖u−w‖.\nProof. By direct calculation we see:\nµ([u], [w])2 =\n∥\n∥\n∥\n∥\n∥\nd ∑\ni=1\n|ui|zi − d ∑\ni=1\n|wi|zi ∥ ∥ ∥ ∥\n∥\n2\n= d ∑\ni=1\n(|ui| − |wi|)2 = d ∑\ni=1\n(ui − wi)2 = ‖u−w‖2 .\nThe first equality uses the definition of µ, and the third equality uses that u,w ∈ Qd−1 v\n, i.e., ui and wi share the same sign (up to the possibility of being 0) for each i ∈ [d].\nIn the following proposition, we see that G/∼ and Ḡ|Qd−1+ are equivalent updates on metric spaces which are isometric under the map φ. In particular, these iterations have equivalent fixed point properties, and it will suffice to analyze Ḡ|Qd−1+ in place of G/∼.\nProposition 4.3. Let v be a sign vector in Rd. Then, Ḡ has the following properties: 1. If u ∈ Qd−1\nv , then Ḡ(u) ∈ Qd−1 v .\n2. If u,w ∈ Sd−1 are such that u ∼ w, then G(u) ∼ Ḡ(w). 3We note that in a special setting of recovering a parallelopiped a closely related observation was made in [22].\nProof. We first demonstrate property 1. Letting h̄1, . . . , h̄d be defined for F̄ similarly to h1, . . . , hd from section 3, we have that ∂iF̄ (u) = 2h̄′i(u 2 i )ui. Under Lemma 3.1, we have that h̄ ′ i ≥ 0 on R for each i ∈ [m]. As each h̄i := 0 for each i ∈ [d] \\ [m], it follows that sign(ui)∂iF̄ (u) ≥ 0 for each i ∈ [d]. It follows that Ḡ(u) ∈ Qd−1\nv .\nWe now demonstrate that property 2 holds. Since u ∼ w, there exist sign values si ∈ {+1,−1} such that ui = siwi. By Assumption A1 (i.e., gi and hence its derivative is either an even or odd function), we see that |∂iF (u)| = |g′i(ui)| = |g′i(wi)| = |∂iF̄ (w)|. In particular, it follows that ‖∇F̄ (w)‖ = ‖∇F (u)‖ 6= 0, and that |Ḡi(w)| = |Gi(u)| for each i ∈ [d]. Thus, Ḡ(w) ∼ G(u).\nCorollary 4.4. Given a sequence {u(n)}∞n=0 in Sd−1 defined recursively by u(n) := G(u(n − 1)), then we may consider a parallel sequence {v(n)}∞i=0 in Qd−1+ defined by v(0) := φ([u(0)]) and v(n) := Ḡ(v(n − 1)). Then, for any w ∈ Qd−1+ and any fixed n, µ([u(n)], [w]) = ‖v(n)−w‖."
    }, {
      "heading" : "4.1 Gradient iteration as adaptive gradient ascent",
      "text" : "For the remainder of this section, we take F to be a PBEF. Given a u ∈ Sd−1, the function F = |F | can be maximized on the unit sphere using a variation on gradient ascent. The projected gradient ascent update (with learning rate η) is given in the function GRADASCENTUPDATE.\nAlgorithm 1 A single of projected gradient ascent step for function maximization over Sd−1.\n1: function GRADASCENTUPDATE(u, η) 2: u′ ← u+ ηP\nu⊥ ∇F (u)\n3: return u ′ ‖u′‖ 4: end function\nThe update in GRADASCENTUPDATE differs from the standard gradient ascent in two ways. First, the update occurs in the direction P\nu⊥ ∇F (u) rather than ∇F (u). This takes into account the geometry\nstructure of Sd−1 by updating within the plane tangent to Sd−1 at u. This arises naturally when treating Sd−1 as a manifold with the local coordinate system defined by the projective space centered at u. Then, u′ is projected back onto the sphere in order to stay within Sd−1. We now compare the updates u ← GRADASCENTUPDATE(u, η) and u ← G(u). If P\nu⊥ ∇F (u) = 0, then both updates are the identity map\nand are thus identical. If P u⊥\n∇F (u) 6= 0, then\nG(u) = ∇F (u)‖∇F (u)‖ = 〈∇F (u), u〉u+ P u⊥ ∇F (u) ‖∇F (u)‖ = u+ P u⊥ ∇F (u)/〈∇F (u), u〉 ‖∇F (u)‖/〈∇F (u), u〉 . (8)\nThe numerator of the rightmost fraction can be interpreted as line 2 of GRADASCENTUPDATE(u, η) using the choice η = 〈u, ∇F (u)〉−1. Lemma 3.1 implies that ui > 0 if and only if ∂iF (u) = 2h′i(u2i )ui > 0. As such, η = 〈u, ∇F (u)〉−1 > 0 is a valid learning rate. The denominator of the rightmost fraction in equation (8) gives the normalization to project back onto the unit sphere (line 3 of GRADASCENTUPDATE). We thus have the following result.\nLemma 4.5. The update u ← G(u) is an adaptive form of projective gradient ascent. Specifically, 1. If ∇F (u) 6= 0, then G(u) = GRADASCENTUPDATE(u, 〈u, ∇F (u)〉−1). 2. If ∇F (u) = 0 and η ∈ (0,∞) is fixed, then G(u) = GRADASCENTUPDATE(u, η).\nWe note that the step size chosen by the gradient iteration function is in many ways very good. By Proposition 4.3, we see that G(u) and hence ∇F (u) belong to the same orthant as u, and as such we never overshoot a basis direction zi during the ascent procedure. Further, we will see that the gradient iteration has the fast convergence properties stated in Theorem 2.3."
    }, {
      "heading" : "4.2 Gradient iteration extends the power method",
      "text" : "We now show how our gradient iteration relates to the classic power method for matrix eigenvector recovery. Given a symmetric matrix A ∈ Rd×d, the power iteration update is given by u ← Au/‖Au‖. If A has the eigendecomposition\n∑d i=1 λiviv T i , then the gradient iteration may be rewritten as u ←\n1 ‖Au‖ ∑d i=1 λi〈u, vi〉vi. It can be seen that in the coordinate system of the eigenvectors, the coordinate value 〈u, vi〉 corresponding to the maximum eigenvalue increases the most so long as 〈u, vi〉 6= 0. Given a generic starting point, the power method converges to the top eigenvector of A. Additional eigenvectors can be recovered by choosing a new starting point in the orthogonal complement of previously found eigenvectors.\nConsider the function f : Sd−1 → R defined as f(u) := uTAu. The derivative ∇f(u) = 2Au differs from Au only by the multiplicative constant 2. As such, the gradient iteration u ← ∇f(u)‖∇f(u)‖ and the power iteration u ← Au/‖Au‖ are identical, making the matrix power iteration a special case of gradient iteration.\nFrom the eigendecomposition of A, we obtain f(u) = ∑d i=1 λi〈u, vi〉2. Defining gi(x) := λix2 and zi := vi, we see that f(u) = ∑d i=1 gi(〈u, zi〉) is a basis encoding function that fails to satisfy Assumption A2. The stability structure of the matrix power iteration differs from the stability structure shown for the BEFs satisfying A1–A4 in this paper. In particular, the only attractor of the power method for matrices is the top eigenvector rather than each of the hidden basis elements. Nevertheless, the matrix power method is a border case of our framework. If f(u) =\n∑d i=1 gi(x) had contrasts gi(x) = λi|x|2+ǫ for some ǫ > 0\nrather than contrasts gi(x) = λix2, then f(u) would be a BEF satisfying Assumptions A1–A4. We now switch back to considering PBEFs with Assumptions A1–A4 in place and write the gradient iteration as a generailized power method. Given u ∈ Sd−1 for which ∇F (u) 6= 0, we obtain\nG(u) = ∇F (u)‖∇F (u)‖ = 2 ‖∇F (u)‖\nm ∑\ni=1\nh′i(u 2 i )〈u, zi〉zi .\nThis is the same form as obtained for the matrix power iteration u ← 2‖∇f(u)‖ ∑d i=1 λi〈u, vi〉vi except with the λis replaced by the functions h′i(u 2 i ). These h ′ is are 0 at the origin and strictly increasing (see Lemma 3.1). For any fixed i, there is a neighborhood of zi on Sd−1 such that hi(u2i ) > hj(u 2 j) for any j ∈ [d] \\ {i}. In this neighborhood, the gradient iteration converges to zi. In addition, the strict convexity of the his combined with the fact that h′i(0) = 0 for each i gives rise to the super-linear convergence rates of Theorem 2.3 not achieved in by the matrix power method. We formalize and prove these convergence properties in section 5.\nFinally, we note that Anandkumar et al. [2] have recently proposed a generalization of the matrix power method for decomposing orthogonal, symmetric tensors. It turns out that the tensor power methods are also special cases of gradient iteration. We continue with the notation from the paragraph on orthogonal tensor decompositions in section 2.1. Let T =\n∑m i=1wiµ ⊗r i where r ≥ 2, each wi ∈ R \\ {0}, and each\nµi ∈ Rd. We extend the definition of Txr to include lower powers: For k ∈ [r], we define (Txk)i1···ir := ∑\ni1∈[d] · · · ∑ ik∈[d] Ti1,...,irxi1 · · · xik . It can be seen that Tx r−1 = ∑d i=1 wi〈x, µi〉r−1µi. In [2], the following update x ← Txr−1/‖Txr−1‖ was proposed and analyzed as a generalized power method. As noted in section 2.1, the function F (u) := Tur is a BEF with the hidden contrast functions gi(x) = wix r and basis encoding elements zi = µi. The product rule implies that ∇F (u) = rTur−1. We see that the gradient iteration u ← ∇F (u)‖∇F (u)‖ = rTu r−1 ‖rTur−1‖ is identical to the tensor power iteration u ← Tu r−1 ‖Tur−1‖ ."
    }, {
      "heading" : "5 Fixed point structure of the gradient iteration",
      "text" : "In this section, we proceed with a formal analysis of the gradient iteration algorithm. In particular, in subsection 5.1, we demonstrate that the distinguished basis directions [z1], . . . , [zm] are the only stable\nfixed points of the gradient iteration update (Theorem 2.2). Further, we demonstrate in subsection 5.2 that convergence to these stable fixed points is fast (Theorem 2.3).\nThroughout this section, we will assume that F (u) = ∑m\ni=1 gi(ui) is a PBEF unless otherwise stated and that the functions hi for i ∈ [d] are defined with respect to F as in section 3 unless otherwise stated. We will analyze the associated gradient iteration function G on the domain Qd−1+ . It suffices to analyze this function due to Corollary 4.4.\nThe proofs in this section largely uses the power method interpretation of G."
    }, {
      "heading" : "5.1 Fixed point stability",
      "text" : "We now proceed with the proof of Theorem 2.2. The proof of this theorem has two main parts, namely demonstrating that the directions z1, . . . , zm are stable attractors of G|Qd−1+ and demonstrating that all other fixed points of G|\nQd−1+ are unstable. The first part is the simpler part, and we prove it first. We will make use\nof the following notation: u〈r〉 is the elementwise rth power of u\nProposition 5.1. The directions z1, . . . , zm are attractors of G|Qd−1+ .\nThe proof of the proposition is based on analyzing the properties of the power method iteration in a small neighbourhood of the hidden basis directions.\nProof. It is sufficient to show that z1 is an attractor of G|Qd−1+ . Since h′i is a strictly increasing continuous function which is 0 at the origin, there exists δ ∈ (0, 12 ) such that x ∈ [0, δ) implies h′i(x2) ∈ [0, 14h′1(1)) for each i and h′1((1 − x)2) ∈ (34h′1(1), h′(1)]. Consider the neighborhood N ⊂ Qd−1+ of z1 given by N := {u ∈ Qd−1+ | |(z1 − u〈2〉)i| < δ ∀i ∈ [d]}. For each u ∈ N , taking derivatives of equation (5) yields:\n1 2 ∂1F (u) > 3 4 h′1(1)u1 and\n1 2 ∂iF (u) ≤ 1 4 h′1(1)ui for each i 6= 1 . (9)\nFix a u ∈ N \\ {z1}. Let Λu := {i | i > 1, ui 6= 0}. For each i ∈ Λu, equation (9) implies that\nG1(u) Gi(u) = ∂1F (u) ∂iF (u) > 3 u1 ui . (10)\nIt follows that\n1− G1(u)2 = ∑ i∈Λu Gi(u)2 < G1(u)2 9u21 ∑ i∈Λu u2i = G1(u)2 9u21 (1− u21) ≤ 1 9u21 (1− u21) .\nIn the above, the first inequality is arrived at using equation (10) when multiplying ∑ i∈Λu Gi(u)2 by G1(u) G1(u) . By the assumption u1 > 12 , it follows that\n1− G1(u)2 < 4\n9 (1− u21) (11)\nWe will use this update to demonstrate the stability of z1.\nClaim 5.1.1. If u ∈ N , then G(u) ∈ N .\nProof of Claim. An implication of equation (11) is that 1 − G1(u)2 < 1 − u21, and hence that G1(u) > u1. It follows that 1 − G1(u)2 < 1 − u1 < δ. By reorganizing equation 10, for i ∈ Λu we obtain Gi(u) < G1(u) 3u1 ui < 2 3ui < δ. Finally, for i 6∈ (Λu ∪ {1}), ui = 0 implies that Gi(u) = 0 < δ. △\nWe now consider a sequence u(0),u(1),u(2), . . . formed by choosing u(0) ∈ N and recursively defining u(n) = G(u(n− 1)) for each n ≥ 1. By the preceding Claim and induction, u(n) ∈ N for each n ∈ N. It only remains to be seen that u(n) → z1 as n → ∞.\nBy induction on equation (11), we get 1 − u1(n)2 < ( 4 9 )n (1 − u1(0)2) < ( 4 9 )n δ. It follows that\nu1(n) → 1 as n → ∞. Under the constraint ‖u(n)‖ = 1, it follows that u(n) → z1 as n → ∞.\nWe now wish to demonstrate that stationary points of G|Qd−1+ outside of the set {zi | i ∈ [m]} are unstable. We will actually prove something stronger, namely given v a stable point of G|Qd−1+ with at least two non-zero coordinates vi1 and vi2 and and a neighborhood N of v, then there exists a sequence {u(n)}∞n=0 with u(0) ∈ N defined recursively by the gradient iteration u(n) = G(u(n − 1)) such that ui1(n) → 0 as n → ∞. The following characterization of the stationary points of G will turn out to be useful.\nObservation 5.2. A vector v ∈ Qd−1+ is stationary point of G if and only if there exists λ∗ such that (v, λ∗) is a critical point of the Lagrangian4 function L(u, λ) = F (u) − λ[‖u‖2 − 1]. In particular, if v is a stationary point of G, then λ∗χ[vi 6=0] = h′i(v2i ) for each i ∈ [d].\nProof. This is a result of Lemmas 4.5 and 3.3.\nWith this characterization, we are actually able to characterize the stationary points G. Note that if vi = 0 for each i ∈ [m], then by the definition of G, v is a stationary point. The remaining stationary points are enumerated by the following Lemma.\nLemma 5.3. Let S ⊂ [m] be non-empty. Then there exists exactly one stationary point v of G|Qd−1+ such that vi 6= 0 for each i ∈ S and vi = 0 for each i ∈ [m] \\ S . Further, vi = 0 for each i ∈ [d] \\ S .\nProof. We prove this in two parts. First, we show that a v exists with all of the desired properties. Then, we show uniqueness.\nClaim 5.3.1. There exists v a stationary point of G|Qd−1+ such that vi 6= 0 if and only if i ∈ S .\nProof of Claim. We will construct v as the limit of a sequence. Consider the following construction of an approximation to v whose precision depends on the magnitude of 1N where N ∈ N.\nfunction APPROXFIXPT(N ) u ← 0 for i ← 1 to N do\nj ← arg mink∈S h′k(u2k) uj ← √ u2j + 1 N\nend for return u\nend function Let ǫ0 > 0 be fixed. Let ǫk = 1 k ǫ0 for each k ∈ N. Since [0, 1] is a compact space, the h′is are uniformly equicontinuous on this domain. Thus for each k ∈ N ∪ {0}, there exists δk > 0 such that for x, y ∈ [0, 1], |x− y| ≤ δk implies that |h′i(x)− h′i(y)| ≤ ǫk for each i ∈ S . We fix constants Nk ∈ N∪ {0} such that (1) 1 Nk\n≤ δk for each k and (2) for each k ≥ 1, Nk is an integer multiple of N0. Then we construct a sequence {u(k)}∞k=0 by setting u(k) = APPROXFIXPT(Nk) for each k ∈ N ∪ {0}. It follows by construction that |h′i(u2i (k))− h′j(u2j (k))| ≤ ǫk for each i, j ∈ S .\n4This is the Lagrangian equation which arises from optimizing F over the unit sphere introduced in Section 3.\nIt can be seen that mini∈S h′i(u 2 i (k)) ≥ mini∈S h′i(u2i (0)) > 0 for each k ∈ N. To see the second in-\nequality mini∈S h′i(u 2 i (0)) > 0, we note that the h ′ is are strictly increasing from 0 by Lemma 3.1, and in particular during the first |S| iterations of the loop in APPROXFIXPT, a new coordinate of u will be incremented. To see the second inequality mini∈S h′i(u 2 i (k)) ≥ mini∈S for each k ∈ N, we argue by contradiction. Let j = arg mini∈S h ′ i(u 2 i (k)). If h ′ j(u 2 j (k)) < minmin i∈S h ′ i(u 2 i (0)), then u 2 j(k) < mini∈S u 2 i (0), and thus there exists ℓ ∈ S with ℓ 6= j such that u2ℓ(k) > u2ℓ(0). However, for this to be true, then during course of the execution of APPROXFIXPT(Nk ) the decision must be made at line 4 that ℓ = arg mink∈S h ′ k(u 2 k) when u2ℓ = u 2 ℓ(0) (since Nk is an integer multiple of N0). During this update, strict monotonicity of h ′ i implies that h′j(u 2 j ) ≤ h′j(u2j(k)) < minmin i∈S h′i(u2i (0)) ≤ h′ℓ(u2ℓ ). But this contradicts that ℓ = arg mink∈S h′k(u2k) at line 4. It follows that there exists a ∆ > 0 such that for each i ∈ S and each k ∈ {0, 1, 2, . . . } we have h′i(u 2 i (k)) > ∆, and in particular that u 2 i (k) ≥ minj∈S(h′j)−1(∆) > 0.\nSince Sd−1 has a compact topology, there exists a subsequence i1, i2, i3, . . . of {0, 1, 2, . . . } such that {u(ik)}∞k=1 converges to a vector v ∈ Sd−1. Since each u(ik) ∈ Qd−1+ , v ∈ Qd−1+ . Further, since the u2j(ik)s are bounded from below by a constant ∆ ′ = minj∈S(h′j) −1(∆) > 0 for each j ∈ S , we see that v2j ≥ ∆′ > 0 for each j ∈ S . Thus by construction, vi = 0 if and only if i ∈ S . By continuity of the h′ℓs, it follows that for any j, ℓ ∈ S , h′ℓ(v2ℓ ) − h′j(v2j ) = limk→∞[h′ℓ(u2ℓ (ik)) − h′j(u2j (ik))] = 0, and in particular h′ℓ(v 2 ℓ ) = h ′ j(v 2 j ). Observation 5.2 implies that v is a stationary point of G. △\nClaim 5.3.2. There exists only one stationary point v of G|Qd−1+ such that the following hold: (1) vi 6= 0 if i ∈ S and (2) vi = 0 if i ∈ [m] \\ S . Proof of Claim. We first show that if v is a stationary point of G|Qd−1+ meeting the conditions of the claim, then vi = 0 for each i ∈ [d] \\ [m]. To see this, we use Observation 5.2, and we note that for each i, j ∈ [d] such that ui 6= 0 and uj 6= 0, then h′i(u2i ) = h′j(u2j ). In particular, choosing i ∈ S , we see that h′i(u2i ) > 0. But for each i ∈ [d] \\ [m], hi := 0 implies that h′i(u2i ) = 0. In particular, for i ∈ [d] \\ [m], ui = 0.\nNow suppose that there are two stationary points v and w meeting the requirements of this Claim. By Observation 5.2, there exists λv and λw such that h′i(v 2 i ) = λv and h ′ i(w 2 i ) = λw for each i ∈ S . If λv < λw, then strict monotonicity of each h′i implies that v 2 i < w 2 i for each i ∈ S . But this contradicts that ∑\ni∈S v 2 i = 1 =\n∑\ni∈S w 2 i . By similar reasoning, it cannot be that λw < λv. As such, λv = λw, and further\nfor each i ∈ S it follows that h′i(v2i ) = h′i(w2i ). Using strict monotonicity of the h′is, we see that v = w. Note that the v constructed in Claim 5.3.1 gives the unique solution to this Claim.\nWe now demonstrate that all stationary points except z1, . . . , zm of G|Qd−1+ are unstable. Most of the difficulty will arise when considering a stationary point v of G such that vi 6= 0 for each i ∈ [m]. We will first demonstrate that such a stationary point of G is unstable. Actually we will prove something stronger, namely that within any neighborhood N of such a choice of v, there exists a vector u(0) ∈ N such that the resulting sequence {u(n)}∞n=0 defined recursively by u(n) = G(u(n − 1), then mini∈[m] ui(n) → 0 as n → ∞. We will generalize this result to the other relevant stationary points of G. The following Lemma captures the main technical difficulties.\nLemma 5.4. Let v ∈ Qd−1+ be a stationary point of G such that vi 6= 0 for each i ∈ [m] and vi = 0 for each i ∈ [d] \\ [m]. Let {u(n)}∞n=0 be a sequence defined recursively by u(n) = G(u(n − 1)) with base element u(0) 6= v such that ui(0) 6= 0 for each i ∈ [m] and ui(0) = 0 for each i ∈ [d] \\ [m]. We define the sets Λ+n := {i ∈ [m] | ui(n) ≥ vi} and Λ−n := {i ∈ [m] | ui(n) < vi}. Then, the following hold:\n1. The sets Λ+n and Λ − n are non-empty for each n ∈ N ∪ {0}. 2. Define Mn := max{uj(n)/vjui(n)/vi | j ∈ Λ + n , i ∈ Λ−n }. Then M0 > 1, and there exists a constant C > 1\nsuch that Mn > C nM0 (12)\nfor each n ∈ N. At this point, it is worth noting a key difference in the structure of the sequences {u(n)}∞n=0 in this work and in the most closely related works (i.e., [2, 25]). In these other works, the contrasts gi satisfy a homogeneity assumption, namely that for some fixed r > 2, gi(αx) = αrgi(x) when α ≥ 0 and x ≥ 0. Then, on its positive domain, gi becomes simply gi(x) = xrgi(1). Given any starting point u(0), homogeneity implies an ordering on [m] defined by i & j if |ui|r−2(0)gi(1) > |uj |r−2(0)gj(1) such that |ui(n)| |uj(n)| → ∞ as a strictly increasing if i & j (see e.g., the proof of Theorem 4.2 in the long version of [25]). This ordering provides a complete characterization of the basins of attraction for the gradient iteration with homogeneity. We assume neither homogeneity nor such an ordering. Nevertheless, we are able to obtain the slightly weaker guarantee maxi,j∈[m] |ui(n)| |uj(n)| → ∞ as a strictly increasing sequence as an implication of Lemma 5.4, which is sufficient to show that the Gs only stable fixed points are the hidden basis elements.\nProof of Lemma 5.4. We first prove part 1. Since u(0) 6= v, it follows that there exists i ∈ [m] such that ui(0) 6= vi. Further, since ∑ i∈[m] u 2 i = ∑ i∈[m] v 2 i = 1, the existence of i ∈ [m] such that ui(0) < vi (or ui(0) > vi resp.) implies the existence of j ∈ [m] such that uj(0) > vi (or uj(0) < vj resp.). Thus Λ+0 and Λ−0 are both non-empty.\nWe proceed by induction on n. In particular, assume that Λ+n−1 and Λ − n−1 are non-empty. Let us assume\nthat i ∈ Λ+n−1 and j ∈ Λ−n−1. Then,\nuj(n) ui(n) = Gj(u(n − 1)) Gi(u(n− 1)) = h′j(uj(n− 1)2)uj(n− 1) h′i(ui(n− 1)2)ui(n− 1) .\nBy Observation 5.2, there exists λ 6= 0 such that h′ℓ(v2ℓ ) = λ for each ℓ ∈ [m]. Since each h′ℓ is strictly increasing on [0, 1] from h′ℓ(0) = 0 for each ℓ ∈ [m] (see Lemma 3.1), since uj(n − 1) < vj , and since ui(n − 1) ≥ vi, it follows that uj(n)ui(n) < λuj(n−1) λui(n−1) < vj vi\n. In particular, u(n) 6= v(n). We note that for each ℓ 6∈ [m], uℓ(n) = 0 since hℓ := 0. Then, it follows by the the same reasoning that made both Λ+0 and Λ−0 non-empty that Λ+n and Λ − n are both non-empty.\nWe now prove part 2. By part 1, there exists i ∈ Λ−0 and j ∈ Λ+0 . In particular, M0 ≥ uj(0)/vj ui(0)/vi > 1. There exists η ∈ (0, 12) such that M0 > 1 + η. Claim 5.4.1. Let w 6= v be a vector such that wi 6= 0 for each i ∈ [m] and wi = 0 for each i 6∈ [m]. Let (k, ℓ) = arg max(i,j){ wi/viwj/vj | (i, j) ∈ [m] × [m]}. If wk/vk wℓ/vℓ\n≥ M0 then there exists a constant C > 1 depending only on η such that G(wk)/vkG(wℓ)/vℓ > C wk/vk wℓ/vℓ .\nProof of claim. Explicit calculation yields:\nG(wk)/vk G(wℓ)/vℓ = h′k(w 2 k)wk/vk h′ℓ(w 2 ℓ )wℓ/vℓ . (13)\nPart 1 with the sequence {u(n)}∞n=0 constructed such that u(0) = w implies that wk ≥ vk and wℓ < vℓ. By assumption, wk/vkwℓ/vℓ ≥ M0 > 1 + η. One of the following conditions must hold: either wk/vk > 1 + η/4 or wℓ/vℓ < 1− η/4. In particular, if neither condition holds, then we obtain:\nwk/vk wℓ/vℓ ≤ 1 + η/4 1− η/4 = 1 + η/2 1− η/4 < 1 + η\nusing that 1− η/4 < 12 . This yields a contradiction. Using Observation 5.2, there exists λ such that λ = h′i(v 2 i ) for each i ∈ [m]. Since h′i is a strictly increasing function on [0, 1], there exists C > 1 which depends only on η satisfying\n1. Whenever x > vi + η/4, then h′i(x 2) λ > C for each i ∈ [m]. 2. Whenever x < vi − η/4, then h ′ i(x 2) λ < 1 C for each i ∈ [m].\nWith this choice of C , continuing from equation (13), we obtain:\nG(wk)/vk G(wℓ)/vℓ = h′k(w 2 k)/λ h′ℓ(w 2 ℓ )/λ · wk/vk wℓ/vℓ > C wk/vk wℓ/vℓ . △\nWe now proceed by induction on n in showing that equation (12) holds. Equation (12) holds trivially when n = 0. Now suppose that equation (12) holds for n = N−1. If we let (k, ℓ) = arg max(i,j){ ui(N−1)/viuj(N−1)/vj | (i, j) ∈ [m]× [m]}, then it follows that\nmax (i,j)∈[m]2 ui(N)/vi uj(N)/vj = max (i,j)∈[m]2 G(ui(N − 1))/vi G(uj(N − 1))/vj ≥ G(uk(N − 1))/vkG(uℓ(N − 1))/vℓ > C · CN−1M0 = CNM0 .\nHere, the strict inequality follows from the inductive hypothesis and Claim 5.4.1.\nProposition 5.5. Any stationary point of G|Qd−1+ not contained in the set {zi | i ∈ [m]} is unstable.\nProof. We do this proof in three cases for a choice of stationary point v ∈ Qd−1+ such that v 6∈ {zi | i ∈ [m]}. Case 1. For each i ∈ [m], vi 6= 0.\nLemma 5.3 implies that vi = 0 for each i 6∈ [m]. We assume that m ≥ 2 since otherwise v = z1 and there is nothing to prove. We construct a sequence {u(n)}∞i=0 such that u(0) 6= v, ui(0) 6= 0 for each i ∈ [m], ui(0) = 0 for each i ∈ [d \\ [m], and u(n) = G(u(n − 1)) for each n > 0. Defining Mn := max(i,j) ui(n)/vi(n) uj(n)/vj (n)\nas in Lemma 5.4, we get that Mn → ∞ as n → ∞. Since ui(n)/vi ≤ maxj∈[m](1/vj) is finitely bounded for each i ∈ [m], this implies that mini∈[m] ui(n)/vi → 0 as n → ∞. Hence, mini∈[m] ui(n) → 0 as n → ∞. In particular, v is unstable.\nCase 2. There exists at least one i ∈ [m] such that vi 6= 0.\nWe will reduce this case to that of case 1.\nLet Λ = {i : vi 6= 0} be enumerated as i1, . . . , ik, and let A = {u ∈ Qd−1+ | ui = 0 if i 6∈ Λ}. We define a bijection ψ : Qd−1+ |A → Qk−1+ as ψj(u) = uij . We define F̃ : Rk → R and G̃ : Sk−1 → Sk−1 as F and G similar to before except on the image of ψ:\nF̃ (ũ) =\nk ∑\nj=1\ngij (ũj) G̃(ũ) = { ∇F̃ (ũ) ‖∇F̃ (ũ)‖ if ∇F̃ (ũ) 6= 0 ũ otherwise .\nWe note that F̃ is a PBEF in a lower dimensional space. Since Gi(u) = ∂iF (u)‖F (u)‖ = 2h′i(u 2 i )ui\n‖F (u)‖ , it follows that when ui = 0, then Gi(u) = 0. In particular for any u ∈ A it can be seen that G(u) = ψ−1(G̃(ψ(u))).\nBut by case 1, ψ(v) is an unstable point of G̃. More precisely, the proof of case 1 implies (1) {i1, i2, . . . , ik} ⊂ [m] and (2) Given any neighborhood N of ψ(v), there exists j ∈ [k] and a sequence {x(n)}∞i=0 in Rk defined recursively by the rule x(n) = G̃(x(n− 1)) such that xi(n) → 0 as n → ∞. Note that the sequence {u(n)}∞n=0 defined by u(0) = ψ−1(x(0)) and recursive step u(n) = G(u(n − 1)) also obeys the rule u(n) = ψ−1(x(n)). In particular, uij (n) → 0 as n → 0. Since ‖ψ(v)−x(0)‖ = ‖v−u(0)‖, and since the neighborhood N is arbitrary, this implies that v is unstable.\nAlgorithm 2 A practical algorithm which uses Gradient Iteration to recover the hidden basis. The inputs are m̂ which is the desired number of basis elements to recover, and N which determines how long to run the gradient iteration algorithm when attempting to achieve convergence. If k = min(m, m̂), then the first k outputs µ1, . . . ,µk recover estimates to a subset of z1, . . . , zm.\nfunction PRACTICALGI-RECOVERY(m̂, N ) for i ← 1 to m̂ do\nGenerate u uniformly at random in Sd−1 ∩ span(µ1, . . . ,µi)⊥ repeat\nu ← G(u) until Convergence (up to sign) µi ← u\nend for return µ1, . . .µm̂\nend function\nCase 3. For each i ∈ [m], vi = 0.\nIn this case, ∇F (v) = 0, leading to the degenerate update G(v) = v. We may fix any δ > 0, and we let u = [(1− δ)v〈2〉 + δz1]〈 1 2 〉. Then ∂1F (u) > 0 and ∂iF (u) = 0 for each i 6= 1 implies that G(u) = z1, which is a fixed point of G. In particular, defining a sequence recursively by u(n) = G(u(n − 1)) with base element u(0) = u yields a sequence for which ‖u(n) − v‖ = 1/ √ 2 for each n ≥ 1. As u(0) can\nbe chosen arbitrarily close to v, it follows that v is unstable.\nUnder Corollary 4.4 and the subsequent discussion, Theorem 2.2 is implied by Propositions 5.1 and 5.5.\nThe stability structure the fixed points of G/∼ suggest a very practical algorithm for recovering the hidden basis elements z1, . . . , zm which we outline in PRACTICALGI-RECOVERY (Algorithm 2). The idea is as follows: We first choose a random starting point u on the unit sphere and apply G (or an approximation Ĝ of G) repeatedly. We would expect the resulting sequence to converge to one of the stable points zi for i ∈ [m]. Given an estimate to zi, we may choose a new starting on the orthogonal space Sd−1 ∩ z⊥i , and noting that F |\nz ⊥ i\nis also a BEF encoding the basis elements z1, . . . , zi−1, zi+1, . . . , zm, we would expect a\nsequence starting in Sd−1 ∩ z⊥i to recover one of the other hidden basis elements. In Section 6, we will show that this practical algorithm can be modified to give a deterministic algorithm\nwith full recovery guarantees."
    }, {
      "heading" : "5.2 Fast convergence of the gradient iteration",
      "text" : "We now proceed with the proof of Theorem 2.3. The stability analysis relied on the change of variable u 7→ u〈2〉 (which gave rise to the definitions of hi for i ∈ [d]) due the fact that for each i ∈ [m], gi(x1/2) is convex on [0, 1]. The fast convergence of the gradient iteration algorithm relies on a more general change of variable u 7→ u〈r〉 where r ≥ 2, and in particular it is assumed that gi(x1/r) is convex on [0, 1] for each i ∈ [m]. We encode this potentially stronger convexity constraint within our BEF by extending the definition of the hi’s from section 3 to the more general family of maps γ′ir : [0, 1] → R defined by γir(x) := gi(x 1 r ) for i ∈ [m] and γir = 0 for i 6∈ [m]. We note that hi = γi2 on [0, 1] for each i ∈ [d]. We then write\nF (u) = m ∑\ni=1\ngi(ui) = m ∑\ni=1\nγir(u r i ) , (14)\nwhere each γir is a convex function.\nLemma 5.6. For i ∈ [m], the functions γ′ir and γ′i2 are related by γ′ir(x) = 2rγ′i2(x 2 r )x 2−r r on the domain (0, 1].\nProof. This is by direct computation. We have the formulas:\nγ′i2(x) = 1 2 g′i(x 1 2 )x− 1 2 γ′ir(x) = 1 r g′i(x 1 r )x 1−r r\nWe may rewrite γ′ir(x) as follows:\nγ′ir(x) = 2 r · 1 2 g′i((x 2 r ) 1 2 )(x 2 r )− 1 2x 2−r r = 2 r γ′2(x 2 r )x 2−r r .\nProposition 5.7. Suppose that {u(n)}∞n=0 is a sequence in Qd−1+ defined recursively by u(n) = G(u(n−1)) which converges to a zj for some j ∈ [m]. Then, the following hold:\n1. The sequence {u(n)}∞n=0 converges to zj at a super-linear rate. 2. Fix r ≥ 2. If x 7→ gi(x 1 r ) is convex for every i ∈ [m], then {u(n)}∞n=0 converges to zj with order of\nconvergence at least r − 1.\nProof. It is sufficient to consider a sequence converging to z1. If there exists n0 such that u(n0) = z1, then there is nothing to prove as z1 is a stationary point of G. So, we assume that u(n) 6= z1 for all n ∈ N.\nTaking derivatives of F from equation (14), we get: ∂iF (v) = rγ′ir(v r i )v r−1 i . We will make use of the\nfollowing ratios in analyzing the rate of convergence of u(n):\nρ(i, j;n) := ui(n)\nuj(n) = γ′ir(ui(n− 1)r)ui(n− 1)r−1 γ′jr(uj(n− 1)r)uj(n− 1)r−1 .\nDefine U = γ′1r(1) and L = maxj 6=1{limx→0+ γ′jr(x)}. We note that the strict convexity of x 7→ gi( √ x) (for i ∈ [m]) implies that γ′i2(1) > 0, and since Lemma 5.6 implies γ′ir(1) = 2rγ′i2(1) > 0, it follows that U > 0. Since γir is convex, γ′jr is a non-decreasing function. It follows that L is well defined and is also equal to maxj 6=1{infx>0 γ′jr(x)}. Finally, noting that γ′i2 is non-negative on [0, 1] (indeed, γ′i2 is increasing from γ′i2(0) = 0 by Lemma 3.1), it follows from Lemma 5.6 that γ ′ ir(x) ≥ 0 for all x > 0, and in particular m ≥ 0. Fix ǫ ∈ (0, 12U). There exists δ > 0 such that:\n1. If v ∈ Qd−1+ is such that 1 − v1 < δ, then γ′1r(u1) > U − ǫ. The existence of such a choice for δ is implied by the continuity of g′1 and hence γ ′ 1r near 1.\n2. If v ∈ Qd−1+ is such that vj < δ for some j 6= 1, then γ′jr(uj) < L + ǫ. The existence of such a δ follows from the characterization of L as maxj 6=1{infx>0 γ′jr(x)} and γ′jr being non-increasing on [0, 1].\nFix N sufficiently large that for each n ≥ N , ‖z1 − u(n)‖1 < δ. With any fixed j 6= 1 and n ≥ N + 1, it follows that\nρ(j, 1;n) = γ′jr(uj(n − 1)r)uj(n− 1)r−1 γ′1r(ui(n− 1)r)u1(n− 1)r−1 < L+ ǫ U − ǫ · uj(n− 1)r−1 u1(n− 1)r−1 .\nDenote by u′ the vector ∑d\ni=2 uizi. Then,\n‖z1 − u(n)‖ = ‖z1(1− u1(n))− (u(n)− u1(n)z1)‖ ≤ ‖z1(1− u1(n))‖ + ‖u′(n)‖ = 1− u1(n) + ‖u′(n)‖ .\nSince u is a unit vector, we see that u1(n)+‖u′(n)‖ ≥ u1(n)2+‖u′(n)‖2 = 1. It follows that 1−u1(n) ≤ ‖u′(n)‖. Thus,\n‖z1 − u(n)‖ ≤ 2‖u′(n)‖ ≤ 2‖u′(n)‖1 = 2 d ∑\ni=2\nui(n) ≤ 2ρ(i, 1;n) < 2 · L+ ǫ U − ǫ · uj(n− 1)r−1 u1(n− 1)r−1 .\nIn particular,\n‖z1 − u(n)‖ < 2 · L+ ǫ (U − ǫ)r · uj(n− 1) r−1 .\nSince uj(n− 1) ≤ ‖z1 − u(n− 1)‖, it follows that:\n‖z1 − u(n)‖ ‖z1 − u(n− 1)‖r−1 < 2 · L+ ǫ (U − ǫ)r−1 .\nAs the right hand side is a finite constant, the sequence has order of convergence at least r − 1. In the case where r = 2, Lemma 3.1 combined with the fact that γi2 = 0 for each i ∈ [d] \\ [m] implies that limx→0+ γ ′ i2(x) = 0 for each i ∈ [d]; and in particular, L = 0. Since ǫ can be chosen arbitrarily small, the sequence {u(n)}∞n=0 has super-linear convergence even when r = 2.\nUnder Corollary 4.4 and the subsequent discussion, part 1 of Theorem 2.3 is implied by Proposition 5.7. Part 2 of Theorem 2.3 follows from the fact that for any i such that u ⊥ zi, then ∂iF (u) = 0 implies that G(u) ⊥ zi. In particular, induction implies that for a sequence defined recursively by u(n) = G(n) and u(0) ⊥ zi, then u(n) ⊥ zi for all n ∈ N, and hence u(n) 6→ zi."
    }, {
      "heading" : "6 A robust gradient iteration algorithm",
      "text" : "In section 5, we saw that the only stable fixed points of the gradient iteration correspond to the hidden basis elements zi and that convergence to these points is super-linear. Nevertheless, the analysis is incomplete for two reasons: First, it is possible (though probably unlikely) for the gradient iteration to converge to an unstable fixed point and therefore fail to recover any basis vector zi. Second, in many practical settings, we would have evaluation access to an approximation F̂ of F . In this section, we propose and analyze an algorithm which is guaranteed to approximately recover the hidden basis elements z1, . . . , zm given access to F̂ and its first and second derivatives. Hatted objects such as F̂ and Ĝ will represent the natural estimates of un-hatted objects, and in particular Ĝ(u) := {\n∇F̂ (u)/‖∇F̂ (u)‖ if ∇F̂ (u) 6= 0 u otherwise .\nThroughout this section, we will assume that F is an (cmin, cmax)-robust BEF and that F̂ is an ǫapproximation to F . That is (recalling the definitions from section 2.2), we assume that for strictly positive constants cmin and cmax, each gi satisfies the following robust version assumption A2:\nA2′. For each i ∈ [m] and each x0 ∈ [−1, 1], ∣ ∣ ∣ d2\ndx2 gi( √ x)|x=x0 ∣ ∣ ∣ ∈ [cmin, cmax].\nWe further assume that for some choice of ǫ > 0 and for each u ∈ B(0, 1), we have that |F̂ (u)− F (u)| ≤ ǫ, ‖∇F̂ (u)−∇F (u)‖ ≤ ǫ, and ‖HF̂ (u)−HF (u)‖ ≤ ǫ.\nUnder these assumptions, FINDBASISELEMENT (page 21) robustly recovers a single hidden basis element ±zi using F̂ and its derivatives. Further, FINDBASISELEMENT may be run repeatedly to recover all hidden basis elements. We have the following main theoretical results:\nTheorem 6.1. Suppose ǫ ≤ 7cmin 10240 √ 2m3/2d2 · ( cmin cmax )7/2 . Let k < m be non-negative, let p be a permutation of [m], and let s1, . . . , sk ∈ {−1,+1} be sign values such that ‖siµi − zp(i)‖ ≤ 4m √ 2d cmin ǫ for each i ∈\nAlgorithm 3 Perform the gradient iteration for a predetermined number of iterations. The inputs are u(0) (an initialization vector) and N (the number of iterations). The output is u(N) (the N th element of the resulting gradient iteration sequence).\nfunction GI-LOOP(u(0), N ) for n ← 1 to N do\nu(n) ← Ĝ(u(n− 1)) end for return u(N)\nend function\n[k]. Suppose N1 ≥ log2( cmin8√2m3/2ǫ · ( cmin cmax\n)1/2) and N2 ≥ 320c 2 maxmd\n3c2min loge\n( 2 √ 2mcmax cmin ) + log2( cmin 8 √ 2m3/2ǫ\n· ( cmincmax )\n1/2) + 1. If we execute µk+1 ← FINDBASISELEMENT({µ1, . . . ,µk}, m̂) for any choice of m̂ ≥ m, then there will exist a sign value sk+1 ∈ {+1,−1} and an index j ∈ [m]\\[k] such that ‖sk+1µk+1−zp(j)‖ ≤ 4m √ d\ncmin ǫ.\nTheorem 6.2. Suppose that ǫ ≤ 7cmin 10240 √ 2m3/2d2 · ( cmin cmax )7/2 . Suppose that m̂ ≥ m, that N1 ≥ log2( cmin8√2m3/2ǫ · ( cmincmax ) 1/2), and that N2 ≥ log2( cmin8√2m3/2ǫ · ( cmin cmax )1/2) + 320c 2 maxmd 3c2min loge ( 2 √ 2mcmax cmin ) + 1. If we execute\nµ1, . . . ,µm̂ ← ROBUSTGI-RECOVERY(m̂), then µ1, . . . ,µm forms a 4m √ 2d\ncmin ǫ-approximation to the hid-\nden basis. More precisely, there exists a permutation ω of [m] and signs s1, . . . , sm ∈ {+1,−1} such that ‖siµi − zω(i)‖ ≤ 4m √ 2d cmin ǫ for each i ∈ [m].\nThe parameters N1 and N2 determine the running time of ROBUSTGI-RECOVERY. In particular, ROBUSTGIRECOVERY uses O(m̂2(N1+N2)) oracle steps including O(m̂2) eigendecompositions of HF̂ and O(m̂2[N1+ N2]) evaluations of ∇F̂ to compute gradient iteration updates. When the desired number of basis elements m is known, then m̂ can be chosen as m. When the number of basis elements is unknown, then m̂ may be chosen as d, and in a more practical setting the values of ‖∇F̂ (µℓ)‖ may be thresholded to determine which returned vectors correspond to hidden basis elements. To obtain the time bound seen in Theorem 2.4, we choose N1 = N2, we note that the main loop runs at most m̂2 times, and we use a somewhat larger, simplified lower bound for m̂2N2 + Cm̂d2 as our lower bound for N (with C a constant). The Cm̂d2 portion of this bound comes from step 3, which can be implemented using Gram-Schmidt orthogonalization involving the µis and canonical vectors in the ambient space.\nIn addition, we note that F is an ǫ-approximation to itself for any ǫ > 0. As such, Theorem 6.2 also implies a polynomial time algorithm for recovering each hidden basis element within arbitrary precision. In particular, the following Corollary of Theorem 6.2 characterizes the running time of ROBUSTGI-RECOVERY as a function of the precision of the hidden basis estimate.\nCorollary 6.3. Let δ ∈ (\n0, 7 2560m1/2d · ( cmin cmax\n)7/2 ]\nand let m̂ ≥ m. Suppose that F̂ is a cmin 4m √ 2d -approximation\nof F , N1 ≥ log2( √ cmind 2δ √ cmaxm ), and N2 ≥ log2( √ cmind 2δ √ cmaxm ) + 320c 2 maxmd 3c2min loge ( 2 √ 2mcmax cmin ) + 1. After executing µ1, . . . ,µm̂ ← ROBUSTGI-RECOVERY(m̂), then µ1, . . . ,µm forms a δ-approximation to the hidden basis. More precisely, there exists a permutation ω of [m] and signs s1, . . . , sm ∈ {+1,−1} such that ‖siµi − zω(i)‖ ≤ δ for each i ∈ [m].\nThe proof of Theorem 6.1 has a number of technical details. We define several projections of particular interest: (1) P⊕u := ∑m i=1 uizi and (2) P0u := ∑d i=m+1 uizi. Further, given a set S ⊂ [d], we will denote its complement by S̄ := [d] \\ S and the associated projection PSu := ∑\ni∈S uizi. At a high level, we demonstrate two things. First, we show that starting with step 5, ‖P0u‖ is small for every for every\nAlgorithm 4 A robust extension to the gradient iteration algorithm for guaranteed recovery of a single hidden basis element. In this algorithm, given a vector u ∈ Sd−1, Û(u)Λ̂(u)Û (u)T is the eigendecomposition of ĤF (u) with eigenvalues λ̂i(u) = Λ̂ii(u) ordered as |λ1(u)| ≤ · · · ≤ |λd(u)|. We also define λ̂0(u) = 0 as it will simplify some steps. Inputs: {µ1, . . . ,µk} A (possibly empty) set of approximate hidden basis directions. m̂ The desired number of basis elements. It is required that m̂ ≥ m.\nOutputs: µ An approximate basis element not estimated by any of µ1, . . . ,µk. 1: function FINDBASISELEMENT({µ1, . . . ,µk}, m̂) 2: // Find a starting vector sufficiently outside the subspace span(zm+1, . . . , zd). 3: Let x1, . . . ,xd−k be orthonormal vectors in span(µ1, . . . ,µk)\n⊥. 4: j ← arg maxi∈[d−k]‖∇F̂ (xi)‖ 5: u ← Ĝ(xj) // “Zero” the values of um+1, . . . , ud. 6: u ← GI-LOOP(u, N1) 7: for i ← 1 to m̂− k − 1 do // Start of the main loop 8: if d = arg maxj∈[d][|λ̂j(u)| − |λ̂j−1(u)|] then 9: µ ← GI-LOOP(Ûd(u), N1)\n10: return µ 11: end if 12: // Identify a good new starting location and “zero” one of its coordinates 13: for j ← 1 to 3 do 14: uj ← Û1(u) cos(π3 (j − 1)) + Û2(u) sin(π3 (j − 1)) 15: uj ← GI-LOOP(uj , N2) 16: end for 17: ℓ ← arg minj∈[3] |λ̂k+i(uj)| 18: u ← GI-LOOP(uℓ, N1) 19: end for 20: µ ← u 21: return µ 22: end function\nvector that we run through the gradient iteration. As such, our GI-LOOPs will work essentially within the non-trivial subspace span(z1, . . . , zm). Second, we show that after the ith iteration through the main loop of FINDBASISELEMENT, at least k + i coordinates of u are approximately zeroed with respect to the hidden basis zm+1, . . . , zd. These two parts combine to demonstrate that FINDBASISELEMENT recovers a good approximation of a single hidden basis element. Once Theorem 6.1 is proven, then Theorem 6.2 follows fairly easily by noting that ROBUSTGI-RECOVERY runs FINDBASISELEMENT repeatedly, each time obtaining a new hidden basis element. We now proceed with the proofs."
    }, {
      "heading" : "6.1 Controlling ‖P0u‖",
      "text" : "The following two Lemmas allow us to demonstrate that ‖P0u‖ becomes small during the gradient iteration updates u ← Ĝ(u) which occur in GI-LOOP. Lemma 6.4. Suppose that ‖P0u‖ ≤ 35 and ǫ ≤ 12mcmin. Then, ‖P0Ĝ(u)‖ ≤ 2mcmin ǫ. Proof. The proof uses the following auxiliary result:\nClaim 6.4.1. If ‖P⊕u‖ > 0 and there exists γ ∈ (0, 1] such that ǫ ≤ γmcmin‖P⊕u‖3, then ‖P0Ĝ(u)‖ ≤ γ.\nAlgorithm 5 A robust algorithm to recover approximations to all of the hidden basis elements. Inputs: m̂ The desired number of basis elements to recover. It is required that m̂ ≥ m.\nOutputs: µ1, . . . ,µm̂ The first m of these are approximate hidden basis elements. 1: function ROBUSTGI-RECOVERY(m̂) 2: for i ← 1 to m̂ do 3: µi ← FINDBASISELEMENT({µ1, . . . ,µi−1}, m̂) 4: end for 5: return µ1, . . . ,µm̂ 6: end function\nProof of Claim. Using Lemma B.1, we note that ‖∇F (u)‖ ≥ 2mcmin‖P⊕u‖3. As such, we see:\n‖P0Ĝ(u)‖ = ‖P0∇F̂ (u)‖ ‖∇F̂ (u)‖ ≤ ǫ‖∇F (u)‖ − ǫ ≤ ǫ\n2 mcmin‖P⊕u‖3 − ǫ ≤ ǫ1 mcmin‖P⊕u‖3 .\nIn the last step, we use that ǫ ≤ γmcmin‖P⊕u‖3 ≤ 1mcmin‖P⊕u‖3. Using the given bound on ǫ, we see that ‖P0Ĝ(u)‖ ≤ γ as desired. △\nNow, by the bound on ‖P0u‖, we see that ‖P⊕u‖3 = (1−‖P0u‖2)3/2 ≥ (45)3 ≥ 12 . Choosing γ = 2mǫcmin , we see that ǫ ≤ γmcmin‖P⊕u‖3. Thus, Claim 6.4.1 gives the desired result.\nLemma 6.5. Let S = {µ1, . . . ,µk} be the set of unit vectors passed into FINDBASISELEMENT. Suppose that k < m, that there exists δ ∈ [0, 14dm ), and that there exists a permutation π on [m] and sign values s1, . . . , sk such that for each i ∈ [k], ‖siµi−zπ(i)‖ ≤ δ. Then, at the end of step 5 of FINDBASISELEMENT, the following hold:\n1. If ǫ ∈ [0, cmin md3/2 ], then ‖P0u‖ ≤ md 3/2 cmin ǫ. 2. If ǫ ∈ [0, c 3/2 min\n8(2)3/4md3/2c 1/2 max\n], δ ≤ 4m √ 2d\ncmin ǫ, and i ∈ {π(j) | j ∈ [k]}, then |ui| ≤ 3md\n3/2\ncmin ǫ.\nProof of Lemma 6.5. First, we demonstrate that one of the vectors xi for i ∈ [d − k] from step 3 of FINDBASISELEMENT has ‖P⊕xi‖2 ≥ m−kd . We then use this to demonstrate that for the chosen value of j in step 4, ‖P0Ĝ(xj)‖ is small. Claim 6.5.1. There exists i ∈ [d− k] such that ‖P⊕xi‖2 ≥ m−kd .\nProof of Claim. We define the projection operators PSv := ∑k i=1 vπ(i)zπ(i) and PS̃v := ∑m\ni=k+1 vπ(i)zπ(i). Notice that ‖P⊕v‖ ≥ ‖PS̃v‖ for any v ∈ Rd.\nWe extend the list of vectors x1, . . . ,xd−k to be an orthonormal basis of the space: x1, . . . ,xd. Since each zi is a unit vector, it follows:\n1\nd\n[ d ∑\ni=1\n‖PS̃xi‖2 ] = 1\nd\nm ∑\nj=k+1\n[ d ∑\ni=1\n〈xi, zπ(j)〉2 ] = m− k\nd . (15)\nTreating equation (15) as a sample average, there exists i ∈ [d] such that ‖PS̃xi‖ ≥ m−kd . To complete the proof, we need only demonstrate that for any i > d− k, ‖PS̃u‖ < m−kd . To show this, we first demonstrate that µ1, . . . ,µk span a k dimensional space. Note that this implies that x1, . . . ,xd−k\nspan the space span(µ1, . . . ,µk) ⊥. Therefore, for any i > d− k we have xi ∈ span(µ1, . . . ,µk). Then to complete the proof, we demonstrate that for any v ∈ span(µ1, . . . ,µk), we have ‖PS̃u‖ < m−kd . We now consider the matrices A = A0 = ∑k i=1 µiµ T i and Ã = Ã0 = ∑k i=1 zπ(i)z T π(i). We note:\n‖A0 − Ã0‖ = ∥ ∥ ∥\nk ∑\ni=1\n[(µi − zπ(i))µTi + zi(µi − zi)T ] ∥ ∥ ∥ ≤ 2\nk ∑\ni=1\n‖µi − zπ(i)‖‖µi‖ ≤ 2kδ < k\n2dm .\nIn particular, Weyl’s inequality (reproduced in Theorem C.1) implies that the kth lowest eigenvalue λk(A0) > 1− k2dm > 0. As such, the vectors µ1, . . . ,µk ar linearly independent. As the k eigenvalues of A0 are contained in the interval [1− k2dm , 1+ k2dm ] by Weyl’s inequality, Theorem C.2 (the Davis-Kahan sin Θ theorem) with Ã1 = ∑d i=k+1 0zπ(i)z T π(i), implies that\n[ 1− k 2dm ] ‖Pspan(zπ(k+1),...,zπ(d))Pspan(µ1,...,µk)‖ < k 2dm\n‖Pspan(zπ(k+1),...,zπ(d))Pspan(µ1,...,µk)‖ < k 2dm− k ≤ k dm ≤ 1 d ≤ m− k d .\nAs such, if v ∈ span(µ1, . . . ,µk), then ‖PS̃v‖ ≤ ‖Pspan(zπ(k+1),...,zπ(d))Pspan(µ1,...,µk)‖ < m−k d . △\nWe now fix i ∈ [d − k] such that ‖P⊕xi‖2 ≥ m−kd according to claim 6.5.1, and we fix j according to step 4 from FINDBASISELEMENT. By Lemma B.1, we have ‖∇F (xi)‖ ≥ 2cmin(m−k) 3/2\nmd3/2 . As such,\n‖∇F̂ (xj)‖ ≥ ‖∇F̂ (xi)‖ ≥ 2cmin(m−k) 3/2\nmd3/2 − ǫ. We now show part 1:\n‖P0Ĝ(xj)‖ ≤ ǫ ‖∇F̂ (xj)‖ ≤ md\n3/2ǫ\n2cmin(m− k)3/2 −md3/2ǫ ≤ md\n3/2ǫ\n2cmin −md3/2ǫ ≤ md\n3/2ǫ\ncmin .\nWe now show part 2. We let w = xj . We let ℓ ∈ [k], and noting that w ⊥ µℓ by construction, we obtain the following bound for |wπ(ℓ)|:\n|wπ(ℓ)| = |〈xj , zπ(ℓ) − sℓµℓ + sℓµℓ〉| = |〈xj , zπ(ℓ) − sℓµℓ〉| ≤ ‖xj‖‖zπ(ℓ) − sℓµℓ‖ ≤ 4m\n√ 2d\ncmin ǫ .\nWe now fix i ∈ {π(ℓ) | ℓ ∈ [k]} and bound |ui| = |Ĝi(w)|:\n|Ĝi(w)| ≤ 2|h′i(w2i )wi|+ ǫ ‖∇F̂ (w)‖ ≤ 2cmax|wi| 3 + ǫ 2cmin(m−k)3/2\nmd3/2 − ǫ\n= md3/2[2cmax|wi|3 + ǫ]\n2cmin(m− k)3/2 −md3/2ǫ\n≤ md3/2[2cmax(\n4m √ 2d\ncmin ǫ)3 + ǫ]\ncmin ≤ md\n3/2[2ǫ+ ǫ]\ncmin =\n3md3/2ǫ\ncmin .\nIn the above, the second to last inequality uses that (1) m− k ≥ 1, (2) that ǫ ≤ cmin md3/2 , and (3) our bound on |wi|. The final inequality uses the that ǫ ≤ c 3/2 min\n8(2)3/4md3/2c 1/2 max\n< c 3/2 min\n8(2)3/4m3/2d3/4c 1/2 max\n.\nIn addition to controlling ‖P0u‖ under the Gradient iteration update u ← Ĝ(u), we must also control ‖P0x‖ for any x ∈ span(Ûj(u), . . . , Ûd(u)) (defined as in steps 9 and 14 of FINDBASISELEMENT). For any such x, the following result demonstrates that ‖P0x‖ is small, and in addition gives conditions into which other coordinates of x are small which will be useful later.\nLemma 6.6. Let u ∈ Sd−1 be fixed. Suppose that ‖P⊕u‖ > 0 and that ǫ ≤ dd+1 · ‖P⊕u‖2 dm cmin. Let Ûi(u) and λ̂i(u) for each i ∈ [d] ∪ {0} be defined as in FINDBASISELEMENT. Let j = arg maxi∈[d][|λ̂i(u)| − |λ̂i−1(u)|]. Let X = span(Ûj(u), . . . , Ûd(u)). Let π be a permutation of [d] such that when defining λi := ∂ 2 π(i)F (u) for each i ∈ [m] and λ0 = 0, then |λ0| ≤ |λ1| ≤ · · · ≤ |λd|. Let S = {π(i) | i < j}. The following hold: 1. If x ∈ X , then ‖PSx‖ ≤ dmǫ5‖P⊕u‖2cmin 2. The set [d] \\ [m] ⊂ S . In particular, if x ∈ X , then ‖P0x‖ ≤ dmǫ5‖P⊕u‖2cmin . 3. If S ′ = {i ∈ [m] | u2i < 5‖P⊕u‖2 6dm · cmincmax } ∪ ([d] \\ [m]), then S\n′ ⊂ S . In particular, if x ∈ X then ‖PS′x‖ ≤ dmǫ5‖P⊕v‖2cmin .\nProof. We will make use of the following Claim:\nClaim 6.6.1. |λ̂j | − |λ̂j−1| ≥ 6‖P⊕u‖ 2 dm cmin − ǫd .\nProof of Claim. Since ‖P⊕u‖2 = ∑m ı=1 u 2 i , it follows that there exists ℓ ∈ [m] such that u2ℓ ≥ ‖P⊕u‖2 m . Using Lemma B.1, we see that |λπ−1(ℓ)| = |∂2ℓF (u)| ≥ 6u2ℓcmin ≥ 6‖P⊕u‖2\nm cmin. It follows that ‖[HF̂ (u)]zℓ‖ ≥ ‖[HF (u)]zℓ‖ − ǫ = |λπ−1(ℓ)| − ǫ. Thus, maxi∈[d] |λ̂i| ≥ |λπ−1(ℓ)| − ǫ ≥ 6‖P⊕u‖ 2\nm cmin − ǫ. To complete the proof, we note that |λ̂0|, . . . , |λ̂d| partitions the interval [0,maxi∈[d] |λ̂i|] into d pieces.\nAs such, |λ̂j| − |λ̂j−1| ≥ maxi∈[d] |λ̂i|−0d ≥ 6‖P⊕u‖2 dm cmin − ǫd . △\nWe now prove part 1 using the Davis-Kahan sinΘ theorem [11] (reproduced in Theorem C.2). Following the notation of Theorem C.2, we partition HF̂ (u) into spectral parts A0 = ∑d i=j λ̂iÛi(u)Ûi(u) T and A1 = ∑j−1 i=1 λ̂iÛi(u)Ûi(u) T . We also define the projection operator Π0 := ∑d i=j Ûi(u)Ûi(u) T . We split HF (u) into its spectral parts Ã0 = ∑d i=j λizπ(i)z T π(i) and Ã1 = ∑j−1 i=1 λizπ(i)z T π(i), and we define the projection operators Π̃0 = ∑d i=j zπ(i)z T π(i) and Π̃1 = I − Π̃0 = ∑j−1 i=1 zπ(i)z T π(i). The error matrix for Theorem C.2 is precisely H = HF (u)−HF̂ (u). Note that the eigenvalues of A0 all lie outside the interval\n[−|λ̂j |, |λ̂j |] ⊃ [ − |λ̂j−1| − 6‖P⊕u‖2\ndm cmin +\nǫ d , |λ̂j−1|+ 6‖P⊕u‖2 dm cmin − ǫ d ]\nby Claim 6.6.1. Further, the eigenvalues of Ã1 all lie within [−|λj−1|, |λj−1|] ⊂ [−|λ̂j−1| − ǫ, |λ̂j−1|+ ǫ] by Weyl’s inequality (Theorem C.1). Applying the sin Θ theorem with δ = 6‖P⊕u‖ 2\ndm cmin − ǫd − ǫ yields ‖Π̃1Π0‖ ≤ 1δ ‖H‖ ≤ 1δ ǫ. As such, if x ∈ R(Π0) = span(Û (u)j , . . . , Û (u)d), then ‖Π̃1x‖ ≤ 1δ ǫ.\nBounding 1δ ǫ and use Claims 6.6.2 and 6.6.3 completes the proof of part 1:\n‖Π̃1x‖ ≤ 1\nδ ǫ = ǫ 6‖P⊕u‖2 dm cmin − (d+1d )ǫ ≤ ǫ (5‖P⊕u‖2 dm cmin ) = dmǫ 5‖P⊕u‖2cmin .\nParts 2 and 3 follow from the following claims.\nClaim 6.6.2. If π(i) ∈ [d] \\ [m], then j > i.\nProof of Claim. Note that |λ̂i| ≤ ǫ by Weyl’s inequality (Theorem C.1). Since 0 is a lower bound on the |λ̂ℓ|s, it suffices to show that |λ̂i| < |λ̂j |−|λ̂j−1|. By Claim 6.6.1, it suffices to show that ǫ < 6‖P⊕u‖ 2\ndm cmin− ǫ d , or alternatively ( d+1 d )ǫ < 6‖P⊕u‖2 dm cmin. This follows from the assumptions on ǫ. In particular, d+1 d ǫ ≤ ‖P⊕u‖2 dm cmin < 6‖P⊕u‖2 dm cmin. △\nClaim 6.6.3. If π(i) ∈ [m] is such that u2π(i) ≤ 5‖P⊕u‖2 6dm · cmin cmax , then j > i.\nProof of Claim. By Lemma B.1, we see that |λi| = |∂2π(i)F (u)| ≤ 6u2π(i)cmax. As such, using Weyl’s inequality (see Theorem C.1), we see\n|λ̂i| ≤ 6u2π(i)cmax + ǫ ≤ 5‖P⊕u‖2\nmd cmin +\nd d+ 1 · ‖P⊕u‖ 2 md cmin\n= ‖P⊕u‖2 md cmin [ 6− 1 d ] < 6‖P⊕u‖2 md cmin − ǫ d ≤ |λ̂j| − |λ̂j−1| ,\nusing Claim 6.6.1. Since 0 lower bounds the |λ̂ℓ|s and |λ̂i| < |λ̂j| − |λ̂j−1|, it follows that i < j.\nWe now demonstrate that FINDBASISELEMENT largely works within the non-trivial subspace span(z1, . . . , zm) throughout its execution.\nProposition 6.7. Let v be defined at any step of FINDBASISELEMENT after step 6 among the subset of {u,u1,u2,u3,µ} which has been generated during the execution of FINDBASISELEMENT. Suppose that k < m. Suppose there exists δ ∈ [0, 14dm ), sign values s1, . . . , sk, and a permutation π of [m] such that ‖siµi − zπ(i)‖ ≤ δ for each i ∈ [k]. Suppose that N1 and N2 are strictly positive integers. If ǫ ≤ cmin\n3md3/2 , then ‖P0v‖ ≤ 2dmcmin ǫ. Further, at the start of the execution of each iteration of the main loop of\nFINDBASISELEMENT, ‖P0u‖ ≤ 2mcmin ǫ.\nProof of Proposition 6.7. We first apply Lemma 6.5 to see that at the end of step 5 of FINDBASISELEMENT, ‖P0u‖ ≤ md 3/2\ncmin ǫ ≤ 13 . As such, by Lemma 6.4, it follows that at the end of step 6, ‖P0u‖ ≤ 2mcmin ǫ, which\nis clearly upper bounded by 2dmcmin ǫ.\nNow we let the λ̂i(v) and Û(v) be defined as in FINDBASISELEMENT (page 21). We will make use of the following claims in completing the proof.\nClaim 6.7.1. For a vector y, let j = arg maxi∈[d][|λ̂j(y)| − |λ̂j−1(y)|] and define the subspace X (y) := span(Ûj(y), . . . , Ûd(y)). If ‖P0y‖ ≤ 2mcmin ǫ, then ‖P0x‖ < 2dm cmin ǫ for any x ∈ X (y)\nProof of Claim. We first note that ‖P⊕y‖2 = 1 − ‖P0y‖2 ≥ 1 − ( 2mcmin ǫ) 2 ≥ 1 − (23 )2 ≥ 12 . We see that\nd d+1 · ‖P⊕y‖2 dm cmin ≥ dd+1 · 1/2 dm cmin. Since d ≥ 2, we see that dd+1 ≥ 23 , and hence dd+1 · ‖P⊕y‖2 dm cmin ≥ 1 3 · cmindm ≥ ǫ, which is the required assumption of Lemma 6.6.\nApplying Lemma 6.6, we see that if x ∈ X (y), then ‖P0x‖2 ≤ dmǫ5‖P⊕y‖2cmin ≤ 2dmǫ 5cmin < 2dmcmin ǫ. △\nClaim 6.7.2. For any vector v such that ‖P0v‖ ≤ 2dmcmin ǫ, then ‖P0Ĝ(v)‖ ≤ 2m cmin ǫ.\nProof of Claim. We note (using that d ≥ 2) that ‖P0v‖ ≤ 2dmcmin ǫ ≤ 2 3 √ 2 < 35 . As such, we may apply Lemma 6.4, which yields the claim. △\nCompleting the proof involves tracing what can happen during the execution of the main loop of FINDBASISELEMENT for any generated choice of v ∈ {u,u1,u2,u3,µ}. At the beginning of the first execution of the loop, the vector ‖P0u‖ ≤ 2mcmin ǫ as shown before the above Claims. Then, letting y = u Claim 6.7.1, we note that all the vectors u1,u2,u3 if generated are generated within the subspace of X (u). Therefore, at the step of generation, ‖P0ui‖ ≤ 2dmcmin ǫ for i ∈ [d], and (with u still being its value at the start of the loop) ‖P0Ûd(u)‖ ≤ 2dmcmin ǫ. Finally, we complete the argument for the first run through the loop using Claim 6.7.2. We note that applications of GI-LOOP makes it so that for any y among any of u1, . . . ,u5 which has been generated or Ûd(u), we have that ‖P0(GI-LOOP(y, N))‖ ≤ 2mcmin ǫ whenever N ≥ 1. In\nparticular, at the end of the loop’s execution, if v is among any of u,u1,u2,u3,µ which has been generated, then ‖P0v‖ ≤ 2mcmin ǫ.\nNote that this implies the following loop invariant: If ‖P0u‖ ≤ 2mcmin ǫ at the start of the main loop’s execution, then ‖P0u‖ ≤ 2mcmin ǫ at the end of the main loop’s execution. As all of the other desired bounds internal to the loop follow from the fact that ‖P0u‖ ≤ 2mcmin ǫ at the start of the loop, we obtain the desired bounds during every execution of the loop. Finally, if µ is generated in step 20, then ‖P0µ‖ = ‖P0u‖ ≤ 2m cmin ǫ holds due to the loop invariant."
    }, {
      "heading" : "6.2 Progress of the gradient iteration",
      "text" : "The core idea behind our robust gradient iteration algorithm FINDBASISELEMENT comes from part 2 of Lemma 5.4. There it is seen that for almost every u ∈ Sd−1, repeated application of the update u ← G(u) drives ui → 0 for some non-zero coordinate i of u. In the noisy setting, this zeroing phenomenon has two parts: (1) for coordinates i such that ui is sufficiently close to 0, ui remains “trapped” near zero, and (2) for an appropriate starting choice of u (of which there are many), a new coordinate of u is driven towards under repeated application of the gradient iteration. In this section, we formalize this zero trapping phenomenon. The following Lemma demonstrates the first part of this zero trapping effect.\nLemma 6.8. Suppose that γ0 ∈ [0, 1), that u ∈ Sd−1, and that ǫ ∈ [ 0, c 3/2 min(1−γ0)3/2‖P⊕u‖9/2\n2 √ 2m3/2c 1/2 max\n)\n. If i ∈ [d] is\nsuch that |ui| ≤ √ (1−γ0)cmin‖P⊕u‖3 2mcmax , then |Ĝi(u)| ≤ max ( (1− γ0)|ui|, 2mǫcmin‖P⊕u‖3 ) .\nProof. The proof is based on the following claim.\nClaim 6.8.1. Suppose that γ0 ∈ [0, 1), that u ∈ Sd−1, that M2 ≤ (1−γ0)cmin‖P⊕u‖ 3\n2mcmax , and that ǫ ≤\n1 2mcmin(1− γ0)‖P⊕u‖3M . If i ∈ [d] is such that |ui| ≤ M , then |Ĝi(u)| ≤ (1− γ0)M .\nProof of Claim. From Lemma B.1, we have ‖∇F (u)‖ ≥ 2mcmin‖P⊕u‖3. We note that ǫ can be further bounded as:\nǫ ≤ 1 2m cmin‖P⊕u‖3(1− γ0)M ≤ 1 4 ‖∇F (u)‖(1 − γ0)M (16)\nThus for any i ∈ [d] such that u2i ≤ M2, we have:\n|Ĝi(u)| = |∂iF̂ (u)| ‖∇F̂ (u)‖ ≤ |∂iF (u)|+ ǫ‖∇F (u)‖ − ǫ ≤ 2|h′i(u2i )ui|+ ǫ\n3 4‖∇F (u)‖\n≤ 2cmax|ui| 3\n3 2m −1cmin‖P⊕u‖3 + ǫ 3 4‖∇F (u)‖\n≤ 2 3 (1− γ0)|ui|+ 1 3 (1− γ0)M ≤ (1− γ0)M .\nIn the above, the second inequality uses equation (16) for the denominator noting that both (1 − γ0) and M are at most 1. The third inequality uses the mean value theorem for the numerator and the lower bound ‖∇F (u)‖ ≥ 2mcmin‖P⊕u‖3 from Lemma B.1 for the denominator. The fourth inequality uses the bound u2i ≤ M2 ≤ (1−γ0)cmin‖P⊕u‖3 2mcmax and equation (16). △\nLet i ∈ [d] be fixed. If |ui| ∈ [\n2mǫ cmin(1−γ0)‖P⊕u‖3 ,\n√\n(1−γ0)cmin‖P⊕u‖3 2mcmax\n]\n, then we apply Claim 6.8.1 with\nthe choice M = |ui| to obtain |ui| ≤ (1 − γ0)|ui|. If |ui| < 2mǫcmin(1−γ0)‖P⊕u‖3 , then we apply Claim 6.8.1 with the choice M = 2mǫ\ncmin(1−γ0)‖P⊕u‖3 to obtain |ui| ≤ 2mǫ cmin‖P⊕u‖3 .\nWe now a corresponding time bound for zero trapping sufficiently small coordinates of u.\nLemma 6.9. Let γ0 ∈ (0, 1). Suppose that ǫ ∈ ( 0, c 3/2 min(1−γ0)3/2\n10m3/2c 1/2 max\n)\n. Let {u(n)}∞n=0 be a sequence\nsuch that ‖P0u(0)‖ ≤ 35 . Suppose that i ∈ [m] is such that |ui(0)| ≤ √ (1−γ0)cmin 4mcmax , and suppose N ≥ loge ( (1−γ0)1/2c3/2min\n8m3/2c 1/2 maxǫ\n)\n/ loge( 1 1−γ0 ) . Then, |ui(n)| ≤ 4mǫcmin for every n ≥ N .\nProof. Repeated application of Lemma 6.4 implies that ‖P0u(n)‖ ≤ 2mcmin ǫ < 3 5 for every n ∈ N. This also implies that ‖P⊕u(n)‖3 ≥ (1− (35)2)3/2 = (45)3 > 12 . We have that for every n ∈ N:\nǫ ≤ c 3/2 min(1− γ0)3/2\n10m3/2c 1/2 max\n<\n√ 2c\n3/2 min(1− γ0)3/2‖P⊕u(n)‖9/2\n5m3/2c 1/2 max\n< c 3/2 min(1− γ0)3/2‖P⊕u‖9/2\n2 √ 2m3/2c 1/2 max\n.\nFurther, |ui(0)| ≤ √ (1−γ0)cmin 4mcmax ≤ √ (1−γ0)cmin‖P⊕u(n)‖3 2mcmax for each n ∈ N ∪ {0}. Noting that upon applications of Lemma 6.8 that √\n(1−γ0)cmin 4mcmax and hence √ (1−γ0)cmin‖P⊕u(n)‖3 2mcmax\nfor each n ∈ N remains an upper bound for our |ui(k)|s, it follows that we may apply Lemma 6.8 at will.\nBy repeated application of Lemma 6.8, we see that |u(n)| ≤ max ( (1− γ0)n|ui(0)|, 4mǫcmin ) . It suffices\nto show that when n ≥ N , then (1− γ0)n|ui(0)| ≤ 4mǫcmin . To show this, we note:\n(1− γ0)n|ui(0)| ≤ ( 1\n1− γ0\n)−N |ui(0)| ≤ ( 1\n1− γ0\n)− log1/(1−γ0)\n(\n(1−γ0) 1/2c 3/2 min\n8m3/2c 1/2 maxǫ\n)\n· √\n(1− γ0)cmin 4mcmax\n=\n(\n8m3/2c 1/2 maxǫ\n(1− γ0)1/2c3/2min\n) · √\n(1− γ0)cmin 4mcmax = 4m cmin ǫ .\nThe following result (used in conjunction with the Lemma 6.8) allows us to demonstrate that for an appropriately chosen starting vector u, a new coordinate of u will be driven towards 0 by the gradient iteration. More precisely, it can be used to show that for an appropriately chosen u, the coordinate values of u diverge under the gradient iteration until some coordinate becomes small.\nLemma 6.10. Let u ∈ Sd−1 be such that the set S = {i | |ui| > 4mdcmin ǫ} is a subset of [m] containing at least 2 elements. Let v ∈ Qd−1+ be the fixed point of G/∼ such that vi 6= 0 if and only if i ∈ S . Let γ0 ∈ (0, 1), ℓ0 = arg maxi∈S |ui|vi , and k0 = arg mini∈S |ui| vi . If ǫ ≤ 7(1−γ0)3/2cmin 5120m3/2d2 · ( cmin cmax ) 7 2 , if |uℓ0 | ≥ vℓ0 , and if |ui| > √ (1−γ0)cmin 4mcmax\nfor each i ∈ S , then the following hold: 1. If δ ∈ [ 120d , 12 ] and |uℓ0 |/vℓ0 |uk0 |/vk0 ≥ (1 + δ)2, then maxi,j∈S |Ĝi(u)|/vi|Ĝj(u)|/vj ≥ (1 + 7c2minδ 32c2maxm ) |uℓ0 |/vℓ0 |uk0 |/vk0 2. If ( uℓ0/vℓ0 u0/vk0 )2 ≥ (1 + 14d ), then maxi∈S |Ĝi(u)|/vi ≥ 1.\nProof. We first prove part 1. We will make use of the following claims.\nClaim 6.10.1. Suppose there exists ∆ ∈ (0, 12) such that one of the following holds: (1) h′ℓ0(u2ℓ0) ≥ (1 + ∆)h′ℓ0(v 2 ℓ0 ) or (2) h′k0(u 2 k0 ) ≤ (1 + ∆)−1h′k0(v2k0). Suppose there exists β ∈ (0, 1 8∆] such that ǫ ≤ βmini∈S |∂iF (u)|. Then, maxi,j∈S |Ĝi(u)|/vi|Ĝj(u)|/vj ≥ (1 + 1 4∆) |uℓ0 |/vℓ0 |uk0 |/vk0 .\nProof of Claim. We first bound the error on calculating Gi(u). For each i ∈ S , we have:\n|Ĝi(u)| = |∂iF̂ (u)| ‖∇F̂ (u)‖ ≤ |∂iF (u)|+ ǫ‖∇F (u)‖ − ǫ ≤ 1 + β 1− β · |∂iF (u)| ‖∇F (u)‖ ≤ 1 + β 1− β · |Gi(u)|\n|Ĝi(u)| = |∂iF̂ (u)| ‖∇F̂ (u)‖ ≥ |∂iF (u)| − ǫ‖∇F (u)‖ + ǫ ≥ 1− β 1 + β · |∂iF (u)|‖∇F (u)‖ ≥ 1− β 1 + β · |Gi(u)| .\nWe note that since ∑ i∈S u 2 i ≤ ∑ i∈S v 2 i = 1, it follows that |uk0 | ≤ vk0 . As such, we have both that |uk0 | ≤ vk0 and |uℓ0 | ≥ vℓ0 . We have that\nmax i,j∈S |Ĝi(u)|/vi |Ĝj(u)|/vj ≥ (1− β 1 + β )2 max i,j∈S |Gi(u)|/vi |Gj(u)|/vj = (1− β 1 + β )2 max i,j∈S |h′i(u2i )||ui|/vi |h′j(u2j )||uj|/vj\n≥ (1− β 1 + β )2 |h′ℓ0(u2ℓ0)||uℓ0 |/vℓ0 |h′k0(u2k0)||uk0 |/vk0 ≥ (1− β 1 + β )2 (1 + ∆)|h′ℓ0(v2ℓ0)||uℓ0 |/vℓ0 |h′k0(v2k0)||uk0 |/vk0 ≥ (1 + ∆) (1− β 1 + β )2 |uℓ0 |/vℓ0 |uk0 |/vk0 .\nIn the second to last inequality, we use the monotonicity of h′i (see Lemma 3.1) along with the the assumption that one of the following holds: either (1) h′ℓ0(u 2 ℓ0 ) ≥ (1+∆)h′ℓ0(v2ℓ0) or (2) h′k0(u2k0) ≤ (1+∆)−1h′k0(v2k0). In the last inequality, we use Observation 5.2 to note that h′ℓ0(v 2 ℓ0 ) = h′k0(v 2 k0 ).\nWe now only need bound (1 + ∆) ( 1−β 1+β )2 . We first note that ( 1−β 1+β )2 = ( 1 − 2β1+β )2 ≥ (1 − 2β)2 ≥\n1 − 4β. Thus, (1 + ∆) ( 1−β 1+β )2 ≥ 1 + ∆ − 4β − 4β∆. But since β ≤ 18∆ and since ∆ ≤ 12 , we see that 1 + ∆− 4β − 4β∆ ≥ 1 + 12∆− 12∆2 ≥ 1 + 14∆. Thus, we get that:\nmax i,j∈S |Ĝi(u)|/vi |Ĝj(u)|/vj ≥ (1 + 1 4 ∆)max i,j∈S |ui|/vi |uj|/vj . △\nClaim 6.10.2. Suppose that ∆ > 0 is such that δ ≥ 8∆mc2max 7c2min and |uℓ0 |/vℓ0 |uk0 |/vk0\n≥ (1 + δ)2. Then one of the following holds: either (1) h′ℓ0(u 2 ℓ0 ) ≥ (1 + ∆)h′ℓ0(v2ℓ0) or (2) h′k0(u2k0) ≤ (1 + ∆)−1h′k0(v2k0).\nProof of Claim. By the assumption |uℓ0 |/vℓ0 |uk0 |/vk0\n≥ (1 + δ)2, one of the following must hold: (1) |uℓ0 |/vℓ0 ≥ (1 + δ) or (2) |uk0 |/vk0 ≤ (1 + δ)−1. We consider these cases separately, and demonstrate that in each case one of our desired results holds.\nCase 1. |uℓ0 |/vℓ0 ≥ (1 + δ).\nSince |uℓ0 | ≥ (1 + δ)vℓ0 , we see that |h′ℓ0(u2ℓ0)| ≥ |h′ℓ0((1 + 2δ + δ2)v2ℓ0)| ≥ |h′ℓ0(v2ℓ0)|+ 2δv2ℓ0cmin ≥ |h′ℓ0(v2ℓ0)| + 2δ c2min cmaxm\n, where the last inequality uses Lemma B.2. Noting that |h′ℓ0(v2ℓ0)| ≤ cmax, it suffice to show that 2δ c 2 min\ncmaxm ≥ (1 + ∆)cmax. But by the assumptions on δ, we have that 2δ c\n2 min\ncmaxm ≥\n16 7 ∆cmax > ∆cmax.\nCase 2. |uk0 |/vk0 ≤ (1 − δ)−1.\nWe note that |uk0 | ≤ (1 + δ)−1vk0 . Further, we bound (1 + δ)−1 = 1+δ−δ1+δ ≤ 1 − 12δ. In particular, u2k0 ≤ (1− δ + 1 4δ 2)v2k0 ≤ (1− 7 8δ)v 2 k0 . It follows that\n|h′k0(u2k0)| ≤ |h′k0(v2k0(1− 7 8 δ)| ≤ |h′k0(v2k0)| − 7 8 δv2k0cmin ≤ |h′k0(v2k0)| − 7 8 δ c2min cmaxm ,\nwhere the last inequality uses Lemma B.2. We now note that |h′k0(v2k0)|(1 + ∆)−1 = |h′k0(v2k0)|[1 − ∆\n1+∆ ] ≥ |h′k0(v2k0)|(1 − ∆). Since |h′k0(v2k0)| − 7 8δ c2min cmaxm ≥ |h′k0(u2k0)|, it suffices to show that\n|h′k0(v2k0)|(1 − ∆) ≥ |h′k0(v2k0)| − 7 8δ c2min cmaxm , or alternatively, it suffices to show that 78δ c2min cmaxm ≥ ∆|h′k0(v2k0)|. But by the assumptions on δ, we have that 7 8δ c2min cmaxm ≥ ∆cmax ≥ ∆|h ′ k0 (vk0) 2|. △\nTo use these claims, we set the parameter choices ∆ = 7c 2 minδ\n8c2maxm and β = 18∆. We note that\nβmin i∈S |∂iF (u)| = 2βmin i∈S |h′i(u2i )ui| ≥ 2βcmin min i∈S |ui|3 ≥ 1 4 ∆cmin (1− γ0)cmin 4mcmax )3/2\n= 7(1− γ0)3/2cminδ 256m5/2 · ( cmin cmax )7/2 ≥ ǫ .\nWe apply Claim 6.10.2 followed by Claim 6.10.1 to complete the proof of part 1. We now proceed to prove part 2. Let ℓ1 = arg maxi∈S |Ĝi(u)|, and let k1 = arg mini∈S |Ĝi(u)|. We assume for the sake of contradiction that |Ĝℓ1(u)| < vℓ1 . By part 1 with the choice of δ = 120d , we obtain ( Ĝℓ1(u)/vℓ1 Ĝk1(u)/vk1 )2 ≥ ( uℓ0/vℓ0 uk0/vk0\n)2 ≥ 1 + 14d . In particular, (Ĝk1(u)/vk1)2 < (1 + 14d)−1 = 4d4d+1 , which may alternatively be written Ĝk1(u)2 < 4d4d+1v2k1 .\nWe use the following notation: For sets S ⊂ [d], we have the complement set S̄ := [d] \\ S and the projection PSw := ∑\ni∈S wizi for any w ∈ Rd. Define S1 := {i | |Ĝi(u)| > 4mdcmin ǫ}. We note that ‖P0u‖ ≤ ‖PS̄0u‖ ≤ 4md 3/2 cmin ǫ < 35 . As such, ‖P⊕u‖3 ≥ (1−(35 )2)3/2 > 12 , and thus the Lemmas 6.4 and 6.8 combine to imply that S1 ⊂ S0. As such, ‖PS̄0 Ĝ(u)‖ ≤ ‖PS̄1 Ĝ(u)‖ ≤ 4md 3/2 cmin ǫ < 1√ 5m1/2d1/2 · ( cmincmax ) 1/2, where we use a weak bound on ǫ in the final inequality. In particular, ‖PS0 Ĝ(u)‖2 ≥ 1 − cmin5mdcmax . Expanding, we obtain:\nĜℓ1(u)2 ≥ 1− cmin\n5mdcmax −\n∑\ni∈S0\\{ℓ1} Ĝi(u)2 = v2ℓ1 − cmin 5mdcmax + ∑ i∈S0\\{ℓ1} (v2i − Ĝi(u)2)\n≥ v2ℓ1 − cmin\n5mdcmax + (v2k1 − Ĝk1(u)2) > v2ℓ1 − cmin 5mdcmax + 1 4d+ 1 v2k1\n≥ v2ℓ1 − cmin\n5mdcmax + cmin cmaxm(4d+ 1) ≥ v2l1 .\nIn the above, the second to last inequality uses Lemma B.2. This is a direct direct contradiction to our assumption that |Ĝℓ1(u)| < vl1 .\nFinally, we provide a method to find a good starting point u in order to guarantee progress using the gradient iteration under Lemma 6.10. The idea is captured line 14 of FINDBASISELEMENT. We identify a subspace on which u has large coordinate values using the spectral decomposition of HF̂ (u), and we choose several starting locations within that subspace. The following Lemma shows that one of these choices will be good.\nLemma 6.11. Let X be a great circle of Sd−1. Suppose there exists a set of coordinates S ⊂ [m] such that for any u ∈ X , ‖PS̄u‖2 < 14d . Let p1,p2 be an orthonomral basis of span(X ), and define the angles θk = kπ 3 and vectors uk = p1 cos θk + p2 sin θk for each k ∈ [3]. Let v ∈ Qd−1+ be the stationary point of G/∼ such that vi 6= 0 if and only if i ∈ S . Then there exists i ∈ S and ℓ ∈ [3] such that 〈uℓ, zi〉2 ≥ (1 + 14d )v2i .\nProof. First, we extend our set of candidate vectors u1,u2,u3 to be u1, . . . ,u6 by setting θk = kπ 3 and uk = p1 cos θk+p2 sin θk for each k ∈ [6]. Note that for each k ∈ [3], uk+3 = −uk and hence uk ∼ uk+3. In particular, if k ∈ [3] and j ∈ S are such that 〈uk+3, zj〉2 ≥ (1 + 14d)v2j , then 〈uk, zj〉2 ≥ (1 + 14d)v2j . It suffices to find a j ∈ S and an ℓ ∈ [6] such that 〈uℓ, zj〉2 ≥ (1 + 14d)v2j .\nClaim 6.11.1. There exists j ∈ S and i1, i2 ∈ [6] such that (1) 〈ui1 , zj〉 and 〈ui2 , zj〉 belong to the same interval among [−1, 0] and [0, 1], and (2) |〈ui2 , zj〉 − 〈ui1 , zj〉| ≥ 1√2d .\nProof of Claim. Let j = arg maxi∈S √ 〈p1, zi〉2 + 〈p2, zi〉2, and let c = √ 〈p1, zj〉2 + 〈p2, zj〉2. For each k ∈ [6], we note that 〈uk, zj〉 = 〈p1, zj〉 cos θk + 〈p2, zj〉 sin θk. By a trigonometric identity, there exists an angle ϕ such that 〈uk, zj〉 = c sin(θk + ϕ) for each k ∈ [6].\nBy the pigeon hole principle, there exists indices i1, i2 ∈ [6] such that θi1 + ϕ and θi2 + ϕ belong to the same quadrant. In particular, this choice of i1 and i2 gives part (1) of our Claim. Since mod(|θi1 + ϕ− (θi2 + ϕ)|, 2π) ≤ π2 , it follows that i1 and i2 can be chosen such that i2 ≡ i1 + 1 (mod 6). Under this choice of i1 and i2, mod(θi2 − θi1 , 2π) = π3 . We assume this choice without loss of generality. Then,\n|〈ui2 , zj〉 − 〈ui1 , zj〉| = c| sin(θi2 + ϕ)− sin(θi1 + ϕ)| = 2c| sin( θi2−θi1 2 ) cos( θi1+θi2+2ϕ 2 )| , (17)\nwhere the last equality uses the trigonometric identity sin(x) − sin(y) = 2 sin(x−y2 ) cos( x+y 2 ). We note sin( θi2−θi1\n2 ) = sin( π 6 ) = 1 2 . Bounding | cos(\nθi1+θi2+2ϕ\n2 )| makes use of the fact that θi1 + ϕ and θi2 + ϕ are in the same quadrant. In particular, there exists ω ∈ {0, π2 , π, 3π2 } and δ ∈ [0, π6 ] such that θi1 + ϕ ≡ ω + δ (mod 2π) and θi2 + ϕ ≡ ω + δ + π3 (mod 2π). As such, θi1+θi2+2ϕ\n2 ∈ (ω + [π6 , π3 ] + πr) for some integer r. In particular, | cos(θi1+θi2+2ϕ2 )| ≥ cos π3 ≥ 12 . Continuing from equation (17), we see that |〈ui2 , zj〉 − 〈ui1 , zj〉| ≥ 12c.\nTo complete the result, we only need to lower bound c. We note that since z1, . . . , zd is a basis of the space, we have:\nmax i∈[d]\n(〈p1, zi〉2 + 〈p2, zi〉2) ≥ 1\nd\nd ∑\ni=1\n(〈p1, zi〉2 + 〈p2, zi〉2) = 2\nd .\nHowever, for each i 6∈ S , we have that 〈p1, zi〉2 + 〈p2, zi〉2 < 2 · 14d ≤ 2d by assumption. As such, it follows that c2 ≥ 2d , and in particular |〈ui2 , zj〉 − 〈ui1 , zj〉| ≥ 12c ≥ 1√2d . △\nClaim 6.11.2. Let w ∈ X . If w2j < (1 + 14d )v2j for each j ∈ S , then w2j > v2j − 14d [1 + ∑ i∈S\\{j} v 2 i ] for each j ∈ S .\nProof of Claim. We note ∑ i∈S w 2 i ≥ 1− 14d = ∑ i∈S v 2 i − 14d . In particular, fixing some j ∈ S obtain\nw2j ≥ v2j + ∑ i∈S\\{j} (v2i − w2i )−\n1\n4d > v2j −\n1\n4d\n∑\ni∈S\\{j} v2i −\n1\n4d ,\nwhere the last inequality follows by rewriting the given w2i < (1 + 1 4d )v 2 i as (v 2 i − w2i ) > − 14dv2i . △\nWe let ui1 and ui2 be as in Claim 6.11.1, and let u = ui1 and w = ui2 . If there exists i ∈ S such that u2i ≥ (1 + 14d )v2i , then there is nothing to prove. So we assume that for each i ∈ S , u2i < (1 + 14d )v2i . But letting j be as in Claim 6.11.1 we get:\n|w2j − u2j | = |wj − uj | · |wj + uj| ≥ 1√ 2d · 1√ 2d = 1 2d (18)\nIn the above, the first inequality uses that since wj and uj are in the same half space, |wj + uj | ≥ |wj − uj |. Noting that (1 + 14d)v 2 j − [v2j − 14d [1 + ∑ i∈S\\{j} v 2 i ] = 1 4d [1 + ∑ i∈S v 2 i = 1 2d ]. Claim 6.11.2 implies that u2j ∈ ((1+ 14d )v2j , v2j − 14d [1+ ∑ i∈S\\{j} v 2 i ]) =: Ij . It follows from equation (18) that wj 6∈ Ij . In particular, the contrapositive of Claim 6.11.2 implies the existence of an i ∈ S such that w2i ≥ (1 + 14d)v2i .\nFinally, we provide a time bound for the second part of the zeroing phenomenon.\nLemma 6.12. Suppose that {u(n)}∞n=0 is a sequence defined recursively by u(n) = G(u(n − 1)). Let γ0 ∈ (0, 1). Suppose that ǫ ≤ 7(1−γ0) 3/2cmin 5120m3/2d2 · ( cmin cmax )7/2 . Define the sets S := {i | |ui(n)| > 4mdcmin ǫ} and An := {i ∈ S | |ui(n)| ≤ √ (1−γ0)cmin 4mcmax\n}. Suppose that |S| ≥ 2 and that v ∈ Qd−1+ is the stationary point of G/∼ such that vi 6= 0 if and only if i ∈ S . Let N0 be the first occurrence of k such that Ak 6= ∅. If there exists ℓ ∈ S such that u 2 ℓ(0)\nv2ℓ ≥ 1 + 14d , then N0 ≤ 1 + 320c2maxmd 3c2min loge ( 2mcmax (1−γ0)1/2cmin ) .\nProof. We first choose δ = 120d for use in Lemma 6.10. We note:\n(1 + δ)4 = 1 + 4δ + 6δ2 + 4δ3 + δ4 ≤ 1 + 4δ + 6 20 δ + 1 100 δ + 1 8000 δ < 1 + 5δ ≤ 1 + 1 4d .\nIn particular, we have that u2ℓ (0)\nv2ℓ ≥ (1+δ)2, and thus (noting that there must be an j ∈ S such that |uj | ≤ vj)\nwe obtain maxi,j∈S |ui(0)|/vi |uj(0)|/vj ≥ (1 + δ) 2. Repeated application of Lemma 6.10 implies that\nmax i,j∈S |ui(N0 − 1)|/vi |uj(N0 − 1)|/vj ≥ ( 1 + 7c2minδ 32c2maxm\n)N0−1 u2ℓ(0)\nv2ℓ ≥ ( 1 + 7c2min 640c2maxmd\n)N0−1\nHowever, first using the bounds for the vis from Lemma B.2 and then using the lower bound on the |ui|s from the sets An, we see that maxi,j∈S\n|ui(N0−1)|/vi |uj(N0−1)|/vj ≤ ( mcmax cmin ) 1 2 maxj∈S 1 |uj(N0−1)|/vj ≤ 2mcmax (1−γ0)1/2cmin . It\nfollows (\n1 + 7c2min\n640c2maxmd\n)N0−1 ≤ 2mcmax\n(1− γ0)1/2cmin From this, we obtain:\nN0 ≤ 1 + loge\n(\n2mcmax (1−γ0)1/2cmin\n)\nloge (\n1 + 7c2min\n640c2maxmd\n)\n≤ 1 + 320c 2 maxmd\n3c2min loge\n( 2mcmax\n(1− γ0)1/2cmin\n)\n.\nThe first inequality is obtained by taking logs and rearranging terms. The second inequality uses that loge(1+ x) ≥ x − x2. Then, setting x = 7c 2 min\n640c2maxmd , it can be seen that x − x2 ≥ x − 7640x > 67x. In particular,\nloge(1 + 7c2min\n640c2maxmd ) > 3c2min 320c2maxmd .\nCorollary 6.13. Suppose that {u(n)}∞n=0 is a sequence defined recursively by u(n) = G(u(n − 1)). Let γ0 ∈ (0, 1). Suppose that ǫ ≤ 7(1−γ0) 3/2cmin 5120m3/2d2 · ( cmin cmax )7/2 . Suppose that ‖P0u(0)‖ ≤ 4mdcmin ǫ. Define the set S := {i | |ui(n)| > 4mdcmin ǫ}. Let v ∈ Q d−1 + be the stationary point of G/∼ such that vi 6= 0 if and only if i ∈ S . If there exists ℓ ∈ S such that u 2 ℓ(0)\nv2ℓ ≥ 1 + 14d , and if N ≥ 1 + 320c2max 3c2min loge ( 2mcmax (1−γ0)1/2cmin ) md +\nloge\n( (1−γ)1/2c3/2min 8m3/2c 1/2 maxǫ ) / loge( 1\n1−γ0 ), then there exists j ∈ S such that for every n ≥ N and for every i ∈ ([d] \\ S) ∪ {j} we have that |ui(N)| ≤ 4mǫcmin .\nProof. Using Lemma 6.12, it follows that for some choice of N0 ≤ 1 + 320c 2 max\n3c2min loge\n(\n2mcmax (1−γ0)1/2cmin\n)\nmd,\nthere exists j ∈ S such that |uj(N0)| ≤ √ (1−γ0)cmin 4mcmax . However, if we consider the sequence starting at N0 and set N1 = loge ( (1−γ)1/2c3/2min 8m3/2c 1/2 maxǫ ) / loge( 1\n1−γ0 ), then Lemma 6.9 implies that for any n ≥ N0+N1 we obtain |uj(n)| ≤ 4mcmin ǫ. Further, Lemma 6.4 implies that if i ∈ [d] \\ [m], then |ui(n)| ≤ 2m cmin\nǫ for each n ∈ N; and Lemma 6.8 implies that if i ∈ S̄ ∩ [m], then |ui(n)| ≤ 4mcmin ǫ for every n ∈ N."
    }, {
      "heading" : "6.3 Gradient iteration proof of robustness",
      "text" : "We now have all of the technical tools needed to prove that ROBUSTGI-RECOVERY robustly recovers the hidden basis elements. To do so, we first demonstrate that FINDBASISELEMENT can be used to approximate a single undiscovered basis element. We then show that by repeated application of FINDBASISELEMENT, all hidden basis elements may be recovered. In particular, we now prove this section’s main theoretical results (Theorems 6.1 and 6.2). For the reader’s convenience, we restate each theorem before its proof.\nTheorem 6.14. Suppose ǫ ≤ 7cmin 10240 √ 2m3/2d2 · ( cmin cmax )7/2 . Let k < m be non-negative, let p be a permutation of [m], and let s1, . . . , sk ∈ {−1,+1} be sign values such that ‖siµi − zp(i)‖ ≤ 4m √ 2d cmin ǫ for each i ∈ [k]. Suppose N1 ≥ log2( cmin8√2m3/2ǫ · ( cmin cmax )1/2) and N2 ≥ 320c 2 maxmd 3c2min loge ( 2 √ 2mcmax cmin ) + log2( cmin 8 √ 2m3/2ǫ\n· ( cmincmax )\n1/2) + 1. If we execute µk+1 ← FINDBASISELEMENT({µ1, . . . ,µk}, m̂) for any choice of m̂ ≥ m, then there will exist a sign value sk+1 ∈ {+1,−1} and an index j ∈ [m]\\[k] such that ‖sk+1µk+1−zp(j)‖ ≤ 4m √ d\ncmin ǫ.\nProof. Throughout the proof, we will fix a choice of γ0 = 12 for use in Lemma 6.9. It can be verified that for this choice of γ0, then fixing Nmin1 to be the constant N from Lemma 6.9, we get N min 1 =\nloge\n(\nc 3/2 min\n8 √ 2m3/2c 1/2 maxǫ\n)\n/ loge(2) = log2\n(\nc 3/2 min\n8 √ 2m3/2c 1/2 maxǫ\n)\n≤ N1. Further, setting Nmin2 to be constant N from\nCorollary 6.13 with the same choice of γ0, we see that Nmin2 = 320c2maxmd\n3c2min loge\n( 2 √ 2mcmax cmin ) +log2 (\nc 3/2 min\n8 √ 2m3/2c 1/2 maxǫ\n)\n+\n1 ≤ N2. Claim 6.14.1. Let S = {p(ℓ) | ℓ ∈ [k]}. At the start of the first execution of the main loop of FINDBASISELEMENT, u is such that |uα| ≤ 4mcmin ǫ for each α ∈ ([d] \\ [m]) ∪ S .\nProof of Claim. For each ℓ ∈ [d] \\ [m], this follows directly from Proposition 6.7. Let S = {p(ℓ) | ℓ ∈ [k]}. Immediately following step 5 of FINDBASISELEMENT, Lemma 6.5 implies that (1) ‖P0u‖ ≤ md 3/2\ncmin ǫ and (2) for each ℓ ∈ S , |ui| ≤ 3md\n3/2\ncmin ǫ. Using (weak) bounds on ǫ, we note that\n‖P0u‖ < 35 and |ui| < √ (1−γ0)cmin 4mcmax . In particular, we may apply Lemma 6.9, and we obtain that the end of step 6, |uℓ| ≤ 4mǫcmin for each ℓ ∈ S . △\nBy Proposition 6.7, we see that following step 6 of FINDBASISELEMENT, we have that for any w among u,u1,u2,u3,µ which has been generated, ‖P0w‖ ≤ 2dmcmin ǫ < 3 5 . In particular, we will have the following lower bound for all relevant vectors during the execution of the main loop of FINDBASISELEMENT: ‖P⊕w‖3 ≥ (1− (35)2)3/2 = (45 )3 ≥ 12 . We continue our analysis strictly after step 6 and consider this to be an additional assumption.\nClaim 6.14.2. Define S(u) := {j | |uj | > 4mcmin ǫ} at the start of iteration i of FINDBASISELEMENT’s main loop, and define S ′(u) := {j | |uj | > 4mcmin ǫ} at the end of iteration i of the main loop. Execution of FINDBASISELEMENT’s main loop satisfies the following: If |S(u)| ≤ m̂ − (i + k − 1) and line 10 is not executed during iteration i, then (1) S ′(u) ⊂ S(u) and (2) |S ′(u)| ≤ m̂− (i+ k).\nProof of Claim. For each j ∈ [3] and each ℓ ∈ S̄(u), the following hold: 1. At the end of the execution of line 14, we have |〈uj , zℓ〉| < √\ncmin 8mcmax . To see this, we note that\nu2ℓ ≤ ( 4m cmin )2 < 5‖P⊕u‖ 2 6dm · cmin cmax (for each ℓ ∈ S̄(u)). Applying Lemma 6.6, we obtain |〈uj , zℓ〉| ≤ dmǫ\n5‖P⊕u‖2cmin < √ cmin 8mcmax .\n2. At the end of the execution of line 15, |〈uj , zℓ〉| ≤ 4mcmin ǫ. If ℓ ∈ [m], then this follows from Lemma 6.9 by noting that N2 > Nmin1 . Otherwise, this is a result of Lemma 6.4. 3. Consider uj at the end of the execution of line 15, and let w = GI-LOOP(uj , N1). Then |wℓ| ≤ 4mcmin ǫ. This can be seen using Lemma 6.8 (if ℓ ∈ [m]) or by Lemma 6.4 (if ℓ ∈ [d] \\ [m]). In particular, at the end of the execution of the main loop, |uℓ| ≤ 4mcmin ǫ. Note that the above imply that S ′(u) ⊂ S(u) (which gives the first part of the claim). Now, we show the second part of this claim. If |S(u)| ≤ m̂ − (i + k), then by the first part there is nothing to prove. So, we assume that |S(u)| = m̂− (i+ k − 1). Let j = arg maxℓ∈[d][|λ̂ℓ(u)| − |λ̂ℓ−1(u)|] with the |λ̂ℓ(u)|s defined as in line 8 of FINDBASISELEMENT. Note that for any ℓ ∈ S̄(u), it follows that u2ℓ ≤ 16mǫ 2\nc2min < 512dm ≤ 5‖P⊕u‖3 6dm using a (weak) bound on\nǫ. Defining X = span(Ûj(u), . . . , Ûd(u)), then Lemma 6.6 implies that ‖PS̄(u)x‖ ≤ dmǫ5‖P⊕u‖2cmin for any x ∈ X . In particular, using our choice of ǫ and using that ‖P⊕u‖ ≥ 45 , it can be seen that ‖PS̄(u)x‖2 < 14d . Let v ∈ Qd−1+ be the fixed point of G/∼ such that vℓ = 0 if and only if ℓ ∈ S(u). Lemma 6.11 implies that for some choice α1 ∈ [3] and α2 ∈ S(u), uα1 at the time of generation in line 14 of FINDBASISELEMENT satisfies |〈uα1 , zα2〉|/vα2 ≥ (1 + 14d).\nFor each ℓ ∈ [3], we let {uℓ(n)}∞n=0 be the sequence defined recursively by uℓ(0) = uℓ and uℓ(n) = Ĝ(uℓ(n − 1)). Define S(uℓ(n) := {β | 〈uℓ(n), zβ〉} for each ℓ ∈ [3] and n ∈ N ∪ {0} Noting that each uα1(0) ∈ X and that ‖PS̄(u)uℓ(0)‖ ≤ dmǫ5‖P⊕u‖2cmin ≤ 4md cmin ǫ, then Corollary 6.13 implies that there exists α3 ∈ S(u) such that S(uα1(N2)) ⊂ S(u) \\ {α3}. Further, Lemma 6.9 combined with Lemma 6.4 implies that S(uℓ(N2)) ⊂ S(u) for each ℓ ∈ [3].\nNow we let ℓ = arg minj∈[3] |λ̂k+i(uj)| be defined as in step 17. If ℓ = α1, then Lemmas 6.4 and 6.8 imply that S ′(u) ⊂ S(uα1(N2)), which gives the claim. If ℓ 6= α1, then we note:\n6cmax〈uα1(N2), zα3〉2 + ǫ ≥ |∂2α3F (uα1(N2))|+ ǫ ≥ |λ̂k+i(uα1(N2))| ≥ |λ̂k+i(uℓ(N2))| ≥ |∂2α3F (uℓ(N2))| − ǫ ≥ 6cmin〈uℓ(N2), zα3〉2 − ǫ .\nIn the above, the first inequality uses Lemma B.1, the second inequality uses Weyl’s inequality (Theorem C.1), the third inequality uses the definition of ℓ, the fourth inequality uses Weyl’s inequality again, and the fifth inequality uses Lemma B.1 again. As such,\n|〈uℓ(N2), zα3〉| ≤ √ cmax cmin |〈uα1(N2), zα2〉|2 + ǫ 3 ≤ √ 16m2cmax c3min ǫ2 + ǫ 3 ≤ √ cmin 8mcmax ,\nwhere the last inequality uses a (weak) upper bound on ǫ. It follows by Lemma 6.9 and Lemma 6.4 that S(uℓ(N2)) ⊂ S(u) \\ {α3}. In particular, S ′(u) ⊂ S(uℓ(N2)) ⊂ S(u) \\ {α2}. △\nClaim 6.14.3. If the line 10 of FINDBASISELEMENT is executed, then there exists a sign s ∈ {±1} and α1 ∈ [m] \\ [k] such that the resulting µ satisfies ‖sµ− zp(α1)‖ ≤ 4m √ 2d cmin ǫ.\nProof of Claim. Consider u in its form at the time that the loop is exited. Let the permutation π be defined as in Lemma 6.6, and let A = {π(α) ∈ [d] | α < d}. Then, we obtain:\n‖PAÛd(u)‖ ≤ dmǫ 5‖P⊕u‖2cmin ≤ dmǫ\n5(45 ) 2cmin\n<\n√\n(1− γ0)cmin 4mcmax .\nwhere the first inequality uses Lemma 6.6, and the third inequality uses a (weak) bound on ǫ. As Lemma 6.6 implies that ‖P0Û(u)‖ ≤ dmǫ5‖P⊕u‖2cmin < 3 5 , the Lemmas 6.4 and Lemma 6.9 imply that for µ recovered on\nline 10, we have |µℓ| ≤ 4mǫcmin for each ℓ ∈ A. We note that |A| = d − 1, and in particular there exists only one ℓ ∈ [d] \\ A. For this choice of ℓ, it follows that µ2ℓ ≥ 1 − ∑ α∈A µ 2 α ≥ 1 − d ( 4m cmin ǫ )2 . In particular, there exists a sign value s such that ‖sµ− zℓ‖ ≤ √ [1− µ2ℓ ] + ∑ α∈A µ 2 α ≤ 4m √ 2d cmin ǫ. It remains to be seen that our choice of ℓ recovers a new hidden basis element as opposed to one which has already been found. To see this, we define the sets Si := {α | |uα| > 4mcmin ǫ} at the start of the i\nth iteration of the loop. By Claim 6.14.1, S1 ⊂ {p(α1) | α1 ∈ [m] \\ [k]}. Let t give the iteration on which we exit the loop. Using Claim 6.14.2, we see that St ⊂ St−1 ⊂ · · · ⊂ S1. Finally, after verifying that 4mcmin ǫ < 5‖P⊕u‖2 6dm · cmincmax , Lemma 6.6 implies that for each α ∈ S̄t, we have that α ∈ A. As ∑d\nα=1 u 2 α = 1 > d · 4mcmin ǫ, it follows that\nS̄t is nonempty, and hence that the lone element ℓ ∈ Ā satisfies ℓ ∈ S ′t ⊂ S ′1 ⊂ {p(α1) | α1 ∈ [m]\\[k]}. △\nNote that if the main loop of FINDBASISELEMENT exits at step 10, then by Claim 6.14.3 there is nothing to prove. So, we assume that step 10 is never executed. We let S1,S2, . . . ,Sm̂−k be the sets defined by: (1) If i < m̂ − k, then Si := {ℓ | |uℓ| > 4mcmin ǫ} using u at the start of iteration i of the main loop, and (2) Sm̂−k := {ℓ | |uℓ| > 4mcmin ǫ} using u at the end of the last iteration (i = m̂− k − 1) of the main loop. Then, Claim 6.14.1 implies that S1 ⊂ {p(ℓ) | ℓ ∈ [m] \\ [k]}. Repeated application of Claim 6.14.2 implies that Sm̂−k ⊂ S1 ⊂ {p(ℓ) | ℓ ∈ [m] \\ [k]} and further that |Sm̂−k| ≤ 1. Since u is a unit vector, it is impossible for each |ui| ≤ 4mcmin ǫ, and in particular it follows that |Sm̂−k| = 1.\nLet µ = u be as generated at the end of the execution of FINDBASISELEMENT. Since µ is a unit vector, then, for the choice of ℓ ∈ Sm̂−k we have that µ2ℓ = ∑ α∈S̄m̂−k µ 2 α > 1 − d( 4mcmin ǫ) 2. In particular, there exists a sign value s such that ‖sµ− zℓ‖ ≤ √ [1− µ2ℓ ] + ∑ α∈S̄m̂−k µ 2 α ≤ 4m √ 2d cmin ǫ.\nTheorem 6.15. Suppose that ǫ ≤ 7cmin 10240 √ 2m3/2d2 · ( cmin cmax )7/2 . Suppose that m̂ ≥ m, that N1 ≥ log2( cmin8√2m3/2ǫ · ( cmincmax ) 1/2), and that N2 ≥ log2( cmin8√2m3/2ǫ · ( cmin cmax )1/2) + 320c 2 maxmd 3c2min loge ( 2 √ 2mcmax cmin ) + 1. If we execute\nµ1, . . . ,µm̂ ← ROBUSTGI-RECOVERY(m̂), then µ1, . . . ,µm forms a 4m √ 2d\ncmin ǫ-approximation to the hid-\nden basis. More precisely, there exists a permutation ω of [m] and signs s1, . . . , sm ∈ {+1,−1} such that ‖siµi − zω(i)‖ ≤ 4m √ 2d cmin ǫ for each i ∈ [m].\nProof. We let µ1, . . . ,µm denote the firstm approximate basis elements returned by ROBUSTGI-RECOVERY. We proceed by induction on the following statement (with k ∈ [m] ∪ {0}).\nInductive Hypothesis: There exist sign values s1, . . . , sk and a permutation ωk of [m] such that ‖siµi − zωk(i)‖ ≤ 4m √ 2d cmin ǫ.\nThe base case k = 0 holds trivially. Suppose that the inductive hypothesis holds for some k = n with n < m. Then, by Theorem 6.1, there exists j ∈ [m] \\ {ωn(i) | i ∈ [n]} and a sign s such that ‖sµn+1 − zj‖ ≤ 4m √ 2d\ncmin ǫ. Letting sn+1 = s and letting ωn+1 be a permutation of [m] such that ωn+1(n + 1) = j and\nωn+1(i) = ωn(i) for i ≤ n gives the inductive hypothesis with k = n+ 1."
    }, {
      "heading" : "A Chart of notation",
      "text" : "We use a number of notations throughout this paper, many of which are standard and some of which are not. For the reader’s reference, we list notations used throughout the paper here.\n∇ Gradient operator. H Hessian operator. ∂i The derivative operator with respect to the direction of the ith basis element of the space. When\nworking in [d], this is the derivative with respect to the direction zi. ∼ The equivalence relation defined on Sd−1 given by u ∼ v if for each i ∈ [d], |ui| = |vi|. [u] The equivalence class {v | v ∼ u}. φ The map from Sd−1/∼ to Qd−1+ given by φi([u]∼) = |ui|. µ The distance metric on Sd−1/ ∼ defined by µ([u], [v]) := ‖φ(u)− φ(v)‖. [k] The set {1, 2, . . . , k}. | · | The modulus or absolute value operation. ‖·‖ The standard Euclidean 2-norm. 〈·, ·〉 The standard Euclidean inner product, i.e., the dot product.\nB(x, r) The closed ball centered at x with radius r. d Dimensionality of the ambient space. F A BEF with expanded form F (u) =\n∑m i=1 αig(βiui), defined on page 2.\nF̄ The PBEF associated with BEF F . G The gradient iteration functions associated with a BEF F . I The identity matrix. m Number of distinguished hidden basis vectors z1, . . . , zm. Note that m ≤ d.\nQd−1+ The all positive orthant of S d−1: {u ∈ Sd−1 | ui ≥ 0 for all i ∈ [d]}. Qd−1 v\nIt is assumed that v ∈ Rd is a vector of signs (vi ∈ {+1,−1} for all i ∈ [d]). Then, Qd−1v := {u ∈ Sd−1 | viui ≥ 0} is the orthant of Sd−1 containing v.\nSd−1 The unit sphere in Rd: {u ∈ Rd | ‖u‖ = 1}.\nsign(·) The sign indicator on R defined by sign(x) := { x/|x| if x 6= 0 0 if x = 0 .\nv〈r〉 Vector v taken to the element-wise exponent of r, i.e., (v〈r〉)i = vri . χ[E] The indicator function of the event E. zi The vectors z1, . . . , zm are the hidden basis elements encoded within a BEF. The vectors\nzm+1, . . . , zd are chosen arbitrarily in order to make z1, . . . , zd an orthonormal basis of Rd."
    }, {
      "heading" : "B Function bounds",
      "text" : "In this section, we provide some useful bounds for (cmin, cmax)-robust BEFs and PBEFs.\nLemma B.1. For a (cmin, cmax)-robust BEF F , we have the following bounds for any u ∈ B(0, 1): 1. 2mcmin‖P⊕u‖3 ≤ ‖∇F (u)‖ ≤ 2cmax‖P⊕u‖3. 2. If i ∈ [m], then |∂2i F (u)| ∈ 6u2i [cmin, cmax].\nProof. We first bound ‖∇F (u)‖:\n‖∇F (u)‖2 = m ∑\ni=1\n(2h′i(u 2 i )ui)\n2 ≤ 4 m ∑\ni=1\n(h′′i (xi)u 3 i ) 2\n≤ 4 m ∑\ni=1\nc2maxu 6 i ≤ 4‖P⊕u‖6\nm ∑\ni=1\nc2max ( ui ‖P⊕u‖ )6 ≤ 4c2max‖P⊕u‖6\nwhere xi ∈ (0, u2i ) by the mean value theorem. We now lower bound ‖∇F (u)‖:\n‖∇F (u)‖2 = 4 m ∑\ni=1\nh′i(u 2 i ) 2u2i ≥ 4c2min m ∑\ni=1\nu6i = 4mc 2 min\nm ∑\ni=1\n(u6i /m)\n≥ 4mc2min (\nm ∑\ni=1\nu2i /m )3 = 4m−2c2min‖P⊕u‖6\nwhere the last inequality uses Jensen’s inequality. We now bound ∂2i F (u):\n|∂2i F (u)| = |4h′′i (u2i )u2i + 2h′i(u2i )| = |4h′′i (u2i )u2i + 2h′′i (x)| ∈ 6u2i [cmin, cmax] ,\nwhere x ∈ (0, u2i ) by the mean value theorem.\nLemma B.2. Let F be a (cmin, cmax)-robust BEF, and let v be a fixed point of G/∼. Let S = {i | vi 6= 0}. Suppose that S ⊂ [m]. If i ∈ S , then v2i ≥ cmincmaxm .\nProof. There exists j ∈ S such that v2j ≥ 1|S| . Using Observation 5.2, we see that h′j(v2j ) = h′k(v2k) for any k ∈ S . In particular, h′k(v2k) = hj(v2j ) ≥ cminv2j ≥ cminm . But noting that h′k(v2k) ≤ v2kcmax, it follows that v2k ≥ cmincmaxm ."
    }, {
      "heading" : "C Error bounds on eigenvalues and eigenspaces",
      "text" : "As part of our error analysis of ROBUSTGIRECOVERY in section 6, we require bounds on the error of estimating the eigenvalues and eigenvectors of HF (u) given access to HF̃ (u). The following inequality is a known version of Weyl’s inequality for matrix eigenvalues.\nTheorem C.1 (Weyl’s inequality). Let A, Ã, and H be symmetric (or more generally Hermetian) n × n matrices such that Ã = A+H . Let the eigenvalues of A, Ã, and H be given by λ1, . . . , λn, λ̃1, . . . , λ̃n, and ρ1, . . . , ρn respectively. Assume that the eigenvalues are indexed in decreasing order, i.e., λ1 ≥ · · · ≥ λn. Then, for each i ∈ [n], λi + ρi ≤ λ̃i ≤ λiρn.\nThe next Theorem (namely the Davis-Kahan sinΘ theorem from [11]) allows us to bound the error in eigenvector subspaces of a matrix under a perturbation. This theorem requires a bit more explanation. In particular, we will still assume that we have a Hermitian matrix A which is the matrix we are interested in, and that Ã = A + H is a perturbed version of A (with Ã and H also both Hermitian). Suppose that A =\n∑n i=1 λiviv T i and Ã ∑n i=1 λ̃iṽiṽi T give eigendecompositions with the ordering of the eigenvalues λi not yet determined. We may split the indices at a point k and define the matrices A0 = ∑k i=1 λiviv T i , A1 = ∑n i=k+1 λiviv T i , Ã0 = ∑k i=1 λ̃iṽiṽi T , Ã0 = ∑m i=k+1 λ̃iṽiṽi T .\nTheorem C.2 (Davis-Kahan sinΘ theorem). Suppose that there exists an interval [α, β] and a δ > 0 such that the eigenvalues of A0 lie within [α, β] and the eigenvalues of Ã1 all lie outside the interval (α−δ, β+δ) [or alternatively, the eigenvalues of Ã1 lie within [α, β] and the eigenvalues of A0 all lie outside the interval (α− δ, β + δ)]. Then, δ‖sinΘ0‖ ≤ ‖H‖.\nThe definition of sinΘ0 is somewhat involved and can be found in [11], however for our setting it suffices to note that ‖sinΘ0‖ bounds certain projection operators. In particular, if we define Π0 = ∑k i=1 viv T i and Π̃0 = ∑k i=1 ṽiṽi T , then ‖(I − Π̃0)Π0‖ ≤ ‖sinΘ0‖ ≤ 1δ ‖H‖."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In this paper we formulate the framework of recovering a hidden orthonormal basis given access to a<lb>certain “Basis Encoding Function”. We describe the class of Basis Encoding Functions (BEF), such that<lb>their local maxima on the unit sphere are in one-to-one correspondence with the basis elements. This<lb>description relies on a certain “hidden convexity” property of these functions. A number of theoretical<lb>and practical problems of recent interest can be interpreted as recovering a hidden basis from potentially<lb>noisy observations. Specifically, we show how our simple and general framework applies to Independent<lb>Component Analysis (ICA), tensor decompositions, spectral clustering and Gaussian mixture learning.<lb>We describe a new algorithm, “gradient iteration”, for provable recovery of the hidden basis. We<lb>provide a complete theoretical analysis of Gradient Iteration both for the exact case as well as for the<lb>case when the observed function is a perturbation of the “true” underlying BEF. In both cases we show<lb>convergence and complexity bounds polynomial in dimension and other relevant parameters, such as<lb>perturbation size. Our perturbation results can be considered as a very general non-linear version of<lb>the classical Davis-Kahan theorem for eigenvectors of perturbations of symmetric matrices. In addition<lb>we show that in the exact case the algorithm converges superlinearly and give conditions relating the<lb>degree of convergence to properties of the Basis Encoding Function. Our algorithm can be viewed as a<lb>generalization of the classical power iteration method for eigenanalysis of symmetric matrices as well as<lb>a generalization of power iterations for tensors. Moreover, the Gradient Iteration algorithm can be easily<lb>and efficiently implemented in practice.",
    "creator" : "LaTeX with hyperref package"
  }
}