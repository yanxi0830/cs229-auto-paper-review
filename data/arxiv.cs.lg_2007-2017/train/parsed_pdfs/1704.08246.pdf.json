{
  "name" : "1704.08246.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Relative Error Tensor Low Rank Approximation",
    "authors" : [ "Zhao Song", "David P. Woodruff", "Peilin Zhong" ],
    "emails" : [ "zhaos@utexas.edu", "dpwoodru@us.ibm.com", "peilin.zhong@columbia.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "∏q i=1 ni , output a rank-k tensor B for which\n‖A − B‖2F ≤ (1 + ) OPT, where OPT = infrank-k A′ ‖A − A′‖2F . Despite much success on obtaining relative error low rank approximations for matrices, no such results were known for tensors. One structural issue is that there may be no rank-k tensor Ak achieving the above infinum. Another, computational issue, is that an efficient relative error low rank approximation algorithm for tensors would allow one to compute the rank of a tensor, which is NP-hard. We bypass these two issues via (1) bicriteria and (2) parameterized complexity solutions:\n1. We give an algorithm which outputs a rank k′ = O((k/ )q−1) tensor B for which ‖A − B‖2F ≤ (1+ ) OPT in nnz(A)+n ·poly(k/ ) time in the real RAM model, whenever either Ak exists or OPT > 0. Here nnz(A) denotes the number of non-zero entries in A. If both Ak does not exist and OPT = 0, then B instead satisfies ‖A − B‖2F < γ, where γ is any positive, arbitrarily small function of n.\n2. We give an algorithm for any δ > 0 which outputs a rank k tensor B for which ‖A−B‖2F ≤ (1+ ) OPT and runs in (nnz(A)+n poly(k/ )+exp(k2/ )) ·nδ time in the unit cost RAM model, whenever OPT > 2−O(n δ) and there is a rank-k tensor B = ∑k i=1 ui ⊗ vi ⊗ wi for\nwhich ‖A − B‖2F ≤ (1 + /2) OPT and ‖ui‖2, ‖vi‖2, ‖wi‖2 ≤ 2O(n δ). If OPT ≤ 2−Ω(nδ), then B instead satisfies ‖A−B‖2F ≤ 2−Ω(n δ).\nOur first result is polynomial time, and in fact input sparsity time, in n, k, and 1/ , for any k ≥ 1 and any 0 < < 1, while our second result is fixed parameter tractable in k and 1/ . For outputting a rank-k tensor, or even a bicriteria solution with rank-Ck for a certain constant C > 1, we show a 2Ω(k\n1−o(1)) time lower bound under the Exponential Time Hypothesis. Our results are based on an “iterative existential argument”, and give the first relative error low rank approximations for tensors for a large number of error measures for which nothing was known. In particular, we give the first relative error approximation algorithms on tensors for: column row and tube subset selection, entrywise `p-low rank approximation for 1 ≤ p < 2, low rank approximation with respect to sum of Euclidean norms of faces or tubes, weighted low rank approximation, and low rank approximation in distributed and streaming models. We also obtain several new results for matrices, such as nnz(A)-time CUR decompositions, improving the previous nnz(A) log n-time CUR decompositions, which may be of independent interest.\n∗Work done while visiting IBM Almaden, and supported in part by UTCS TAship (CS361 Spring 17 Introduction to Computer Security). †Supported in part by Simons Foundation, and NSF CCF-1617955.\nar X\niv :1\n70 4.\n08 24\n6v 1\n[ cs\n.D S]\n2 6\nA pr\n2 01"
    }, {
      "heading" : "Contents",
      "text" : ""
    }, {
      "heading" : "1 Introduction 4",
      "text" : "1.1 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Our Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.3 Other Low Rank Approximation Algorithms Following Our Framework. . . . . . . . 11 1.4 An Algorithm and a Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16"
    }, {
      "heading" : "A Notation 17",
      "text" : ""
    }, {
      "heading" : "B Preliminaries 19",
      "text" : "B.1 Subspace Embeddings and Approximate Matrix Product . . . . . . . . . . . . . . . . 20 B.2 Tensor CURT decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.3 Polynomial system verifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.4 Lower bound on the cost of a polynomial system . . . . . . . . . . . . . . . . . . . . 25 B.5 Frobenius norm and `2 relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.6 CountSketch and Gaussian transforms . . . . . . . . . . . . . . . . . . . . . . . . . . 26 B.7 Cauchy and p-stable transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 B.8 Leverage scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.9 Lewis weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.10 TensorSketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30"
    }, {
      "heading" : "C Frobenius Norm for Arbitrary Tensors 31",
      "text" : "C.1 (1 + )-approximate low-rank approximation . . . . . . . . . . . . . . . . . . . . . . . 31 C.2 Input sparsity reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 C.3 Tensor multiple regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 C.4 Bicriteria algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\nC.4.1 Solving a small regression problem . . . . . . . . . . . . . . . . . . . . . . . . 38 C.4.2 Algorithm I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 C.4.3 poly(k)-approximation to multiple regression . . . . . . . . . . . . . . . . . . 44 C.4.4 Algorithm II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\nC.5 Generalized matrix row subset selection . . . . . . . . . . . . . . . . . . . . . . . . . 47 C.6 Column, row, and tube subset selection, (1 + )-approximation . . . . . . . . . . . . 51 C.7 CURT decomposition, (1 + )-approximation . . . . . . . . . . . . . . . . . . . . . . 53\nC.7.1 Properties of leverage score sampling and BSS sampling . . . . . . . . . . . . 53 C.7.2 Row sampling for linear regression . . . . . . . . . . . . . . . . . . . . . . . . 54 C.7.3 Leverage scores for multiple regression . . . . . . . . . . . . . . . . . . . . . . 56 C.7.4 Sampling columns according to leverage scores implicitly, improving polynomial running time to nearly linear running time . . . . . . . . . . . . . . . . . 57 C.7.5 Input sparsity time algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 C.7.6 Optimal sample complexity algorithm . . . . . . . . . . . . . . . . . . . . . . 63\nC.8 Face-based selection and decomposition . . . . . . . . . . . . . . . . . . . . . . . . . 64 C.8.1 Column-row, column-tube, row-tube face subset selection . . . . . . . . . . . 64 C.8.2 CURT decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 C.9 Solving small problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 C.10 Extension to general q-th order tensors . . . . . . . . . . . . . . . . . . . . . . . . . . 70\nC.10.1 Fast sampling of columns according to leverage scores, implicitly . . . . . . . 70 C.10.2 General iterative existential proof . . . . . . . . . . . . . . . . . . . . . . . . . 72\nC.10.3 General input sparsity reduction . . . . . . . . . . . . . . . . . . . . . . . . . 73 C.10.4 Bicriteria algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 C.10.5 CURT decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\nC.11 Matrix CUR decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 C.11.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 C.11.2 Stronger property achieved by leverage scores . . . . . . . . . . . . . . . . . . 77"
    }, {
      "heading" : "D Entry-wise `1 Norm for Arbitrary Tensors 81",
      "text" : "D.1 Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 D.2 Existence results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 D.3 Polynomial in k size reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 D.4 Solving small problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 D.5 Bicriteria algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\nD.5.1 Input sparsity time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 D.5.2 Improving cubic rank to quadratic rank . . . . . . . . . . . . . . . . . . . . . 92\nD.6 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 D.6.1 Input sparsity time algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 D.6.2 Õ(k3/2)-approximation algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 96 D.7 CURT decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96"
    }, {
      "heading" : "E Entry-wise `p Norm for Arbitrary Tensors, 1 < p < 2 100",
      "text" : "E.1 Existence results for matrix case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 E.2 Existence results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 E.3 Polynomial in k size reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 E.4 Solving small problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 E.5 Bicriteria algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 E.6 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 E.7 CURT decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108"
    }, {
      "heading" : "F Robust Subspace Approximation (Asymmetric Norms for Arbitrary Tensors) 111",
      "text" : "F.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 F.2 `1-Frobenius (a.k.a `1-`2-`2) norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\nF.2.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 F.2.2 Sampling and rescaling sketches . . . . . . . . . . . . . . . . . . . . . . . . . . 112 F.2.3 No dilation and no contraction . . . . . . . . . . . . . . . . . . . . . . . . . . 113 F.2.4 Oblivious sketches, MSketch . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 F.2.5 Running time analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 F.2.6 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\nF.3 `1-`1-`2 norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 F.3.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 F.3.2 Projection via Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 F.3.3 Reduction, projection to high dimension . . . . . . . . . . . . . . . . . . . . . 127 F.3.4 Existence results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 F.3.5 Running time analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 F.3.6 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131"
    }, {
      "heading" : "G Weighted Frobenius Norm for Arbitrary Tensors 133",
      "text" : "G.1 Definitions and Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 G.2 r distinct faces in each dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 G.3 r distinct columns, rows and tubes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 G.4 r distinct columns and rows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140"
    }, {
      "heading" : "H Hardness 144",
      "text" : "H.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 H.2 Symmetric tensor eigenvalue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 H.3 Symmetric tensor singular value, spectral norm and rank-1 approximation . . . . . . 146 H.4 Tensor rank is hard to approximate . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\nH.4.1 Cover number . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 H.4.2 Properties of 3SAT instances . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 H.4.3 Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\nH.5 Hardness result for robust subspace approximation . . . . . . . . . . . . . . . . . . . 162 H.6 Extending hardness from matrices to tensors . . . . . . . . . . . . . . . . . . . . . . . 165\nH.6.1 Entry-wise `1 norm and `1-`1-`2 norm . . . . . . . . . . . . . . . . . . . . . . 166 H.6.2 `1-`2-`2 norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167"
    }, {
      "heading" : "I Hard Instance 169",
      "text" : "I.1 Frobenius CURT decomposition for 3rd order tensor . . . . . . . . . . . . . . . . . . 169 I.2 General Frobenius CURT decomposition for q-th order tensor . . . . . . . . . . . . . 171"
    }, {
      "heading" : "J Distributed Setting 174",
      "text" : ""
    }, {
      "heading" : "K Streaming Setting 178",
      "text" : ""
    }, {
      "heading" : "L Extension to Other Tensor Ranks 182",
      "text" : "L.1 Tensor Tucker rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\nL.1.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 L.1.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\nL.2 Tensor Train rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 L.2.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 L.2.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185"
    }, {
      "heading" : "M Acknowledgments 189",
      "text" : "References 190"
    }, {
      "heading" : "1 Introduction",
      "text" : "Low rank approximation of matrices is one of the most well-studied problems in randomized numerical linear algebra. Given an n × d matrix A with real-valued entries, we want to output a rank-k matrix B for which ‖A−B‖ is small, under a given norm. While this problem can be solved exactly using the singular value decomposition for some norms like the spectral and Frobenius norms, the time complexity is still min(ndω−1, dnω−1), where ω ≈ 2.376 is the exponent of matrix multiplication [Str69, CW87, Wil12]. This time complexity is prohibitive when n and d are large. By now there are a number of approximation algorithms for this problem, with the Frobenius norm 1 being one of the most common error measures. Initial solutions [FKV04, AM07] to this problem were based on sampling and achieved additive error in terms of ‖A‖F , where > 0 is an approximation parameter, which can be arbitrarily larger than the optimal cost OPT = minrank-k B ‖A − B‖2F . Since then a number of solutions based on the technique of oblivious sketching [Sar06, CW13, MM13, NN13] as well as sampling based on non-uniform distributions [DMM06b, DMM06a, DMM08, DMIMW12], have been proposed which achieve the stronger notion of relative error, namely, which output a rankk matrix B for which ‖A−B‖2F ≤ (1+ ) OPT with high probability. It is now known how to output a factorization of such a B = U ·V , where U is n×k and V is k×d, in nnz(A)+(n+d) poly(k/ ) time [CW13, MM13, NN13]. Such an algorithm is optimal, up to the poly(k/ ) factor, as any algorithm achieving relative error must read almost all of the entries.\nTensors are often more useful than matrices for capturing higher order relations in data. Computing low rank factorizations of approximations of tensors is the primary task of interest in a number of applications, such as in psychology[Kro83], chemometrics [Paa00, SBG04], neuroscience [AAB+07, KB09, CLK+15], computational biology [CV15, SC15], natural language processing [CYYM14, LZBJ14, LZMB15, BNR+15], computer vision [VT02, WA03, SH05, HPS05, HD08, AFdLGTL09, PLY10, LFC+16, CLZ17], computer graphics [VT04, WWS+05, Vas09], security [AÇKY05, ACY06, KB06], cryptography [FS99, Sch12, KYFD15, SHW+16] data mining [KS08, RST10, KABO10, Mør11], machine learning applications such as learning hidden Markov models, reinforcement learning, community detection, multi-armed bandit, ranking models, neural network, Gaussian mixture models and Latent Dirichlet allocation [MR05, AFH+12, HK13, ALB13, ABSV14, AGH+14, AGHK14, BCV14, JO14a, GHK15, PBLJ15, JSA15, ALA16, AGMR16, ZSJ+17], programming languages [RTP16], signal processing [Wes94, DLDM98, Com09, CMDL+15], and other applications [YCS11, LMWY13, OS14, ZCZJ14, STLS14, YCS16, RNSS16].\nDespite the success for matrices, the situation for order-q tensors for q > 2 is much less understood. There are a number of works based on alternating minimization [CC70, Har70, FMPS13, FT15, ZG01, BS15] gradient descent or Newton methods [ES09, ZG01], methods based on the Higher-order SVD (HOSVD) [LMV00a] which provably incur Ω( √ n)-inapproximability for Frobenius norm error [LMV00b], the power method or orthogonal iteration method [LMV00b], additive error guarantees in terms of the flattened (unfolded) tensor rather than the original tensor [MMD08], tensor trains [Ose11], the tree Tucker decomposition [OT09], or methods specialized to orthogonal tensors [KM11, AGH+14, MHG15, WTSA15, WA16, SWZ16]. There are also a number of works on the problem of tensor completion, that is, recovering a low rank tensor from missing entries [WM01, AKDM10, TSHK11, LMWY13, MHWG14, JO14b, BM16]. There is also another line of work using the sum of squares (SOS) technique to study tensor problems [BKS15, GM15, HSS15, HSSS16, MSS16, PS17, SS17], other recent work on tensor PCA [All12b, All12a, RM14, JMZ15, ADGM16, ZX17], and work applying smoothed analysis to tensor decomposition [BCMV14]. Several previous works also consider more robust norms than\n1Recall the Frobenius norm ‖A‖F of a matrix A is ( ∑n i=1 ∑d j=1 A 2 i,j) 1/2.\nthe Frobenius norm for tensors, e.g., the R1 norm (`1-`2-`2 norm in our work) [HD08], `1-PCA [PLY10], entry-wise `1 regularization [GGH14], M-estimator loss [YFS16], weighted approximation [Paa97, TK11, LRHG13], tensor-CUR [OST08, MMD08, CC10, FMMN11, FT15], or robust tensor PCA [GQ14, LFC+16, CLZ17].\nUnlike for matrices, the above works either do not have provable guarantees or require incoherence or orthogonality assumptions on the underlying tensor to achieve their bounds. A natural question, for example, is why the following guarantee has not been achieved for tensors: given a third order tensor A ∈ Rn×n×n, output a rank-k tensor B for which\n‖A−B‖2F ≤ (1 + ) OPT, (1)\nwhere OPT = infrank-k B′ ‖A−B′‖2F , and where recall the rank of a tensor B is the minimal integer k for which B can be expressed as ∑k i=1 ui⊗ vi⊗wi. For a third order tensor, its rank is an integer in {0, 1, 2, . . . , n2}. For simplicity, in this section we mostly focus the discussion on third order tensors with all dimensions of equal size, but we extend all of our main theorems below to tensors of any constant order q > 3 and dimensions of different sizes.\nThe first caveat regarding (1) for tensors is that an optimal rank-k solution may not even exist! This is a well-known problem for tensors (see, e.g., [KHL89, Paa00, KDS08, Ste06, Ste08] and more details in section 4 of [DSL08]), for which for any rank-k tensor B, there always exists another rank-k tensor B′ for which ‖A − B′‖2F < ‖A − B‖2F . If OPT = 0, then in this case for any rank-k tensor B, necessarily ‖A−B‖2F > 0, and so (1) cannot be satisfied. This fact was known to algebraic geometers as early as the 19th century, which they refer to as the fact that the locus of r-th secant planes to a Segre variety may not define a (closed) algebraic variety [DSL08, Lan12]. It is also known as the phenomenon underlying the concept of border rank2[Bin80, Bin86, BCS97, Knu98, Lan06]. In this case it is natural to allow the algorithm to output an arbitrarily small γ > 0 amount of additive error. Note that unlike several additive error algorithms for matrices, the additive error here can in fact be an arbitrarily small positive function of n. If, however, OPT > 0, then for any > 0, there exists a rank-k tensor B for which ‖A−B‖2F ≤ (1 + ) OPT, and in this case we should still require the algorithm to output a relative-error solution. If an optimal rank-k solution B exists, then as for matrices, it is natural to require the algorithm to output a relative-error solution.\nBesides the above definitional issue, a central reason that (1) has not been achieved is that computing the rank of a third order tensor is well-known to be NP-hard [Hås90, HL13]. Thus, if one had such a polynomial time procedure for solving the problem above, one could determine the rank of A by running the procedure on each k ∈ {0, 1, 2, . . . , n2}, and check for the first value of k for which ‖A − B‖2F = 0, thus determining the rank of A. However, it is unclear if approximating the tensor rank is hard. This question will also be answered in this work.\nThe main question which we address is how to define a meaningful notion of (1) for the case of tensors and whether it is possible to obtain provably efficient algorithms which achieve this guarantee, without any assumptions on the tensor itself. Besides (1), there are many other notions of relative error for low rank approximation of matrices for which provable guarantees for tensors are unknown, such as tensor CURT, R1 norm, and the weighted and `1 norms mentioned above. Our goal is to provide a general technique to obtain algorithms for many of these variants as well."
    }, {
      "heading" : "1.1 Our Results",
      "text" : "To state our results, we first consider the case when a rank-k solution Ak exists, that is, there exists a rank-k tensor Ak for which ‖A−Ak‖2F = OPT.\n2https://en.wikipedia.org/wiki/Tensor_rank_decomposition#Border_rank\nWe first give a poly(n, k, 1/ )-time (1 + )-relative error approximation algorithm for any 0 < < 1 and any k ≥ 1, but allow the output tensor B to be of rank O((k/ )2) (for general q-order tensors, the output rank is O((k/ )q−1), whereas we measure the cost of B with respect to rank-k tensors. Formally, ‖A − B‖2F ≤ (1 + )‖A − Ak‖2F . In fact, our algorithm can be implemented in nnz(A)+n ·poly(k/ ) time in the real-RAM model, where nnz(A) is the number of non-zero entries of A. Such an algorithm is optimal for any relative error algorithm, even bicriteria ones.\nIf Ak does not exist, then our output B instead satisfies ‖A−B‖2F ≤ (1 + ) OPT +γ, where γ is an arbitrarily small additive error. Since γ is arbitrarily small, (1 + ) OPT +γ is still a relative error whenever OPT > 0.\nOur theorem is as follows.\nTheorem 1.1 (A Version of Theorem C.9, bicriteria). Given a 3rd order tensor A ∈ Rn×n×n, if Ak exists then there is a randomized algorithm running in nnz(A) + n · poly(k/ ) time which outputs a (factorization of a) rank-O(k2/ 2) tensor B for which ‖A−B‖2F ≤ (1+ )‖A−Ak‖2F . If Ak does not exist, then the algorithm outputs a rank-O(k2/ 2) tensor B for which ‖A−B‖2F ≤ (1 + ) OPT +γ, where γ > 0 is an arbitrarily small positive function of n. In both cases, the success probability is at least 2/3.\nWe next consider the case when the rank parameter k is small, and we try to obtain rank-k solutions which are efficient for small values of k. As before, we first suppose that Ak exists.\nIf Ak = ∑k\ni=1 ui⊗vi⊗wi and the norms ‖ui‖2, ‖vi‖2, and ‖wi‖2 are bounded by 2poly(n), we can return a rank-k solution B for which ‖A−B‖2F ≤ (1+ )‖A−Ak‖2F +2− poly(n), in f(k, 1/ ) ·poly(n) time in the standard unit cost RAM model with words of size O(log n) bits. Thus, our algorithm is fixed parameter tractable in k and 1/ , and in fact remains polynomial time for any values of k and 1/ for which k2/ = O(log n). This is motivated by a number of low rank approximation applications in which k is typically small. The additive error of 2− poly(n) is only needed in order to write down our solution B in the unit cost RAM model, since in general the entries of B may be irrational, even if the entries of A are specified by poly(n) bits. If instead we only want to output an approximation to the value ‖A − Ak‖2F , then we can output a number Z for which OPT ≤ Z ≤ (1 + ) OPT, that is, we do not incur additive error.\nWhen Ak does not exist, there still exists a rank-k tensor Ã for which ‖A − Ã‖2F ≤ OPT +γ. We require there exists such a Ã for which if Ã = ∑k i=1 ui ⊗ vi ⊗ wi, then the norms ‖ui‖2, ‖vi‖2, and ‖wi‖2 are bounded by 2poly(n). The assumption in the previous two paragraphs that the factors of Ak and of Ã have norm bounded by 2poly(n) is necessary in certain cases, e.g., if OPT = 0 and we are to write down the factors in poly(n) time. An abridged version of our theorem is as follows.\nTheorem 1.2 (Combination of Theorem C.1 and C.2, rank-k). Given a 3rd order tensor A ∈ Rn×n×n, for any δ > 0, if Ak = ∑k i=1 ui⊗vi⊗wi exists and each of ‖ui‖2, ‖vi‖2, and ‖wi‖2 is bounded by 2O(nδ), then there is a randomized algorithm running in O(nnz(A) +n poly(k, 1/ ) + 2O(k2/ )) ·nδ time in the unit cost RAM model with words of size O(log n) bits3, which outputs a (factorization of a) rank-k tensor B for which ‖A−B‖2F ≤ (1 + )‖A−Ak‖2F + 2−O(n\nδ). Further, we can output a number Z for which OPT ≤ Z ≤ (1+ ) OPT in the same amount of time. When Ak does not exist, if there exists a rank-k tensor Ã for which ‖A−Ã‖2F ≤ OPT +2−O(n δ) and Ã = ∑k\ni=1 ui⊗vi⊗wi is such that the norms ‖ui‖2, ‖vi‖2, and ‖wi‖2 are bounded by 2O(nδ), then we can output a (factorization of a) rank-k tensor Ã for which ‖A− Ã‖2F ≤ (1 + ) OPT +2−O(n δ).\n3The entries of A are assumed to fit in nδ words.\nOur techniques for proving Theorem 1.1 and Theorem 1.2 open up avenues for many other problems in linear algebra on tensors. We now define the problems and state our results for them.\nThere is a long line of research on matrix column subset selection and CUR decomposition [DMM08, BMD09, DR10, BDM11, FEGK13, BW14, WS15, ABF+16, SWZ17] under operator, Frobenius, and entry-wise `1 norm. It is natural to consider tensor column subset selection or tensorCURT4, however most previous works either give error bounds in terms of the tensor flattenings [DMM08], assume the original tensor has certain properties [OST08, FT15, TM17], consider the exact case which assumes the tensor has low rank [CC10], or only fit a high dimensional cross-shape to the tensor rather than to all of its entries [FMMN11]. Such works are not able to provide a (1+ )- approximation guarantee as in the matrix case without assumptions. We consider tensor column, row, and tube subset selection, with the goal being to find three matrices: a subset C ∈ Rn×c of columns of A, a subset R ∈ Rn×r of rows of A, and a subset T ∈ Rn×t of tubes of A, such that there exists a tensor U ∈ Rc×r×t for which\n‖U(C,R, T )−A‖ξ ≤ α‖Ak −A‖ξ + γ, (2)\nwhere γ = 0 if Ak exists and γ = 2− poly(n) otherwise, α > 1 is the approximation ratio, ξ is either Frobenius norm or Entry-wise `1 norm, and U(C,R, T ) = ∑c i=1 ∑r j=1 ∑t l=1 Ui,j,l · Ci ⊗Rj ⊗ Tl. In tensor CURT decomposition, we also want to output U . We provide a (nearly) input sparsity time algorithm for this, together with an alternative input sparsity time algorithm which chooses slightly larger factors C,R, and T .\nTheorem 1.3 (Combination of Theorem C.20 and C.21, ‖‖F -norm, column, row, tube subset selection). Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, if Ak exists then there is a randomized algorithm which takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices: a subset C ∈ Rn×c of columns of A, a subset R ∈ Rn×r of rows of A, and a subset T ∈ Rn×t of tubes of A where c = r = t = poly(k, 1/ ), and there exists a tensor U ∈ Rc×r×t such that ‖U(C,R, T ) − A‖2F ≤ (1 + )‖Ak − A‖2F holds. If Ak does not exist, then ‖U(C,R, T ) − A‖2F ≤ (1 + ) OPT +γ where γ is an arbitrarily small positive function of n. In both cases, the algorithm succeeds with probability at least 9/10.\nGiven a factorization of a rank-k tensor B, we can obtain C, U , R, and T in terms of it:\nTheorem 1.4 (Combination of Theorem C.40 and C.41, ‖‖F -norm, CURT decomposition). Given a 3rd order tensor A ∈ Rn×n×n, let k ≥ 1, and let UB, VB,WB ∈ Rn×k be given. There is an algorithm running in O(nnz(A) log n)+Õ(n2) poly(k, 1/ ) time (respectively, O(nnz(A))+n poly(k, 1/ ) time) which outputs a subset C ∈ Rn×c of columns of A, a subset R ∈ Rn×r of rows of A, a subset T ∈ Rn×t of tubes of A, together with a tensor U ∈ Rc×r×t with rank(U) = k such that c = r = t = O(k/ ) (respectively, c = r = t = O(k log k+k/ )), and ‖U(C,R, T )−A‖2F ≤ (1 + )‖UB⊗VB⊗WB−A‖2F holds with probability at least 9/10.\nCombining Theorems 1.2 and 1.4 (with B being a (1 + O( ))-approximation to A) we achieve Equation (2) with α = (1 + ) and ξ = F with the optimal number of columns, rows, tubes, and rank of U (we mention our matching lower bound later).\nWe also obtain several algorithms for tensor entry-wise `p norm low-rank approximation, as well as results for asymmetric tensor norms, which are natural extensions of the matrix `1-`2 norm. Here, for a tensor A, ‖A‖v = ∑ i( ∑ j,k(Ai,j,k) 2) 1 2 and ‖A‖u = ∑ i,j( ∑ k(Ai,j,k) 2) 1 2 .\n4T denotes the tube which is the column in 3rd dimension of tensor.\nTheorem 1.5 (Combination of Theorem D.14 (‖‖1-norm), Theorem E.9 (‖‖p-norm, p ∈ (0, 1)) Theorem F.23 (‖‖v-norm or `1-`2-`2), Theorem F.37 (‖‖u-norm or `1-`1-`2)). Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, let r = Õ(k2). If Ak exists then there is an algorithm which runs in nnz(A) · t + Õ(n) poly(k) time and outputs a (factorization of a) rank-r tensor B for which ‖B − A‖ξ ≤ poly(k, log n) · ‖Ak − A‖ξ holds. If Ak does not exist, we have ‖B − A‖ξ ≤ poly(k, log n)·OPT +γ, where γ is an arbitrarily small positive function of n. The success probability is at least 9/10. For ξ = 1 or p, t = Õ(k); for ξ = v, t = O(1); for ξ = u, t = O(n).\nAs in the case of Frobenius norm, we can get rank-k and CURT algorithms for the above norms. Our results for asymmetric norms can be extended to `p-`2-`2, `p-`p-`2, and families of M-estimators.\nWe also obtain the following result for weighted tensor low-rank approximation.\nTheorem 1.6 (Informal Version of Theorem G.5, weighted). Suppose we are given a third order tensor A ∈ Rn×n×n, as well as a tensor W ∈ Rn×n×n with r distinct rows and r distinct columns. Suppose there is a rank-k tensor A′ ∈ Rn×n×n for which ‖W ◦ (A′−A)‖2F = OPT and one can write A′ = ∑k i=1 ui ⊗ vi ⊗ wi for ‖ui‖2, ‖vi‖2, and ‖wi‖2 bounded by 2n\nδ . Then there is an algorithm running in (nnz(A) + nnz(W ) + n2Õ(r2k2/ )) · nδ time and outputting n× k matrices U1, U2, U3 for which ‖W ◦ (U1 ⊗ U2 ⊗ U3 −A)‖2F ≤ (1 + ) OPT with probability at least 2/3.\nWe next strengthen Håstad’s NP-hardness to show that even approximating tensor rank is hard.\nTheorem 1.7 (Informal Version of Theorem H.42). Let q ≥ 3. Unless the Exponential Time Hypothesis (ETH) fails, there is an absolute constant c0 > 1 for which distinguishing if a tensor in Rnq has rank at most k, or at least c0 · k, requires 2δk1−o(1) time, for a constant δ > 0.\nUnder random-ETH [Fei02, GL04, RSW16], an average case hardness assumption for 3SAT , we can replace the k1−o(1) in the exponent above with a k. We also obtain hardness in terms of :\nTheorem 1.8 (Informal Version of Corollary H.22). Let q ≥ 3. Unless ETH fails, there is no algorithm running in 2o(1/ 1/4) time which, given a tensor A ∈ Rnq , outputs a rank-1 tensor B for which ‖A−B‖2F ≤ (1 + ) OPT.\nAs a side result worth stating, our analysis improves the best matrix CUR decomposition algorithm under Frobenius norm [BW14], providing the first optimal nnz(A)-time algorithm:\nTheorem 1.9 (Informal Version of Theorem C.48, Matrix CUR decomposition). There is an algorithm, which given a matrix A ∈ Rn×d and an integer k ≥ 1, runs in O(nnz(A))+(n+d) poly(k, 1/ ) time and outputs three matrices: C ∈ Rn×c containing c columns of A, R ∈ Rr×d containing r rows of A, and U ∈ Rc×r with rank(U) = k for which r = c = O(k/ ) and ‖CUR − A‖2F ≤ (1 + ) minrank−k Ak ‖Ak −A‖2F , holds with probability at least 9/10."
    }, {
      "heading" : "1.2 Our Techniques",
      "text" : "Many of our proofs, in particular those for Theorem 1.1 and Theorem 1.2, are based on what we call an “iterative existential proof”, which we then turn into an algorithm in two different ways depending if we are proving Theorem 1.1 or Theorem 1.2.\nHenceforth, we assume Ak exists; otherwise replace Ak with a suitably good tensor Ã in what follows. Since Ak = ∑k i=1 U ∗ i ⊗ V ∗i ⊗W ∗i 5, we can create three n × k matrices U∗, V ∗, and W ∗ whose columns are the vectors U∗i , V ∗ i , and W ∗ i , respectively. Now we consider the three different\n5For simplicity, we define U ⊗ V ⊗W = ∑k i=1 Ui ⊗ Vi ⊗Wi, where Ui is the i-th column of U .\nflattenings (or unfoldings) of Ak, which express Ak as an n×n2 matrix. Namely, by thinking of Ak as the sum of outer products, we can write the three flattenings of Ak as U∗ ·Z1, V ∗ ·Z2, andW ∗ ·Z3, where the rows of Z1 are vec(V ∗i ⊗W ∗i ) 6 ( For simplicity, we write Z1 = (V ∗> W ∗>). 7 ), the rows of Z2 are vec(U∗i ⊗W ∗i ), and the rows of Z3 are vec(U∗i ⊗ V ∗i ), for i ∈ [k] def = {1, 2, . . . , k}. Letting the three corresponding flattenings of the input tensor A be A1, A2, and A3, by the symmetry of the Frobenius norm, we have\n‖A−B‖2F = ‖A1 − U∗Z1‖2F = ‖A2 − V ∗Z2‖2F = ‖A3 −W ∗Z3‖2F .\nLet us consider the hypothetical regression problem minU ‖A1 − UZ1‖2F . Note that we do not know Z1, but we will not need to. Let r = O(k/ ), and suppose S1 is an n2 × r matrix of i.i.d. normal random variables with mean 0 and variance 1/r, denoted N(0, 1/r). Then by standard results for regression (see, e.g., [Woo14] for a survey), if Û is the minimizer to the smaller regression problem Û = argminU‖UZ1S1 −A1S1‖2F , then\n‖A1 − ÛZ1‖2F ≤ (1 + )minU‖A1 − UZ1‖2F . (3)\nMoreover,Û = A1S1(Z1S1)†. Although we do not know know Z1, this implies Û is in the column span of A1S1, which we do know, since we can flatten A to compute A1 and then compute A1S1. Thus, this hypothetical regression argument gives us an existential statement - there exists a good rank-k matrix Û in the column span of A1S1. We could similarly define V̂ = A2S2(Z2S2)† and Ŵ = A3S3(Z3S3)\n† as solutions to the analogous regression problems for the other two flattenings of A, which are in the column spans of A2S2 and A3S3, respectively. Given A1S1, A2S2, and A3S3, which we know, we could hope there is a good rank-k tensor in the span of the rank-1 tensors\n{(A1S1)a ⊗ (A2S2)b ⊗ (A3S3)c}a,b,c∈[r]. (4)\nHowever, an immediate issue arises. First, note that our hypothetical regression problem guarantees that ‖A1 − ÛZ1‖2F ≤ (1 + )‖A− Ak‖2F , and therefore since the rows of Z1 are of the special form vec(V ∗i ⊗W ∗i ), we can perform a “retensorization” to create a rank-k tensor B = ∑ i Ûi ⊗ V ∗i ⊗W ∗i from the matrix ÛZ1 for which ‖A−B‖2F ≤ (1 + )‖A− Ak‖2F . While we do not know Û , since it is in the column span of A1S1, it implies that B is in the span of the rank-1 tensors {(A1S1)a ⊗ V ∗b ⊗W ∗c }a∈[r],b,c∈[k]. Analogously, we have that there is a good rank-k tensor B in the span of the rank-1 tensors {U∗a⊗(A2S2)b⊗W ∗c }a,c∈[k],b∈[r], and a good rank-k tensor B in the span of the rank-1 tensors {U∗a ⊗ V ∗b ⊗ (A3S3)c}a,b∈[k],c∈[r]. However, we do not know U∗ or V ∗, and it is not clear there is a rank-k tensor B for which simultaneously its first factors are in the column span of A1S1, its second factors are in the column span of A2S2, and its third factors are in the column span of A3S3, i.e., whether there is a good rank-k tensor B in the span of rank-1 tensors in (4).\nWe fix this by an iterative argument. Namely, we first computeA1S1, and write Û = A1S1(Z1S1)†. We now redefine Z2 with respect to Û , so the rows of Z2 are vec(Ûi ⊗W ∗i ) for i ∈ [k], and consider the regression problem minV ‖A2 − V Z2‖2F . While we do not know Z2, if S2 is an n2 × r matrix of\n6vec(V ∗i ⊗W ∗i ) denotes a row vector that has length n1n2 where V ∗i has length n1 and W ∗i has length n2. 7(V ∗> W ∗>) denotes a k × n1n2 matrix where the i-th row is vec(V ∗i ⊗W ∗i ), where length n1 vector V ∗i is the\ni-th column of n1 × k matrix V ∗, and length n2 vector W ∗i is the i-th column of n2 × k matrix W ∗, ∀i ∈ [k].\ni.i.d. Gaussians, we again have the statement that V̂ = A2S2(Z2S2)† satisfies\n‖A2 − V̂ Z2‖2F ≤ (1 + )minV ‖A2 − V Z2‖2F by the regression guarantee with Gaussians ≤ (1 + )‖A2 − V ∗Z2‖2F since V ∗ is no better than the minimizer V = (1 + )‖A1 − ÛZ1‖2F by retensorizing and flattening along a different dimension ≤ (1 + )2minU‖A1 − UZ1‖2F by (3) = (1 + )2‖A−Ak‖2F by definition of Z1 .\nNow we can retensorize V̂ Z2 to obtain a rank-k tensor B for which ‖A − B‖2F = ‖A2 − V̂ Z2‖2F ≤ (1 + )2‖A− Ak‖2F . Note that since the columns of V̂ are in the span of A2S2, and the rows of Z2 are vec(Ûi ⊗W ∗i ) for i ∈ [k], where the columns of Û are in the span of A1S1, it follows that B is in the span of rank-1 tensors\n{(A1S1)a ⊗ (A2S2)b ⊗ V̂c}a,b∈[r],c∈[k].\nSuppose we now redefine Z3 so that it is now an r2×n2 matrix with rows vec((A1S1)a⊗(A2S2)b) for all pairs a, b ∈ [r], and consider the regression problem minW ‖A3 −WZ3‖2F . Now observe that since we know Z3, and since we can form A3 by flattening A, we can solve for W ∈ Rn×r2 in polynomial time by solving a regression problem. Retensorizing WZ3 to a tensor B, it follows that we have found a rank-r2 = O(k2/ 2) tensor B for which ‖A − B‖2F ≤ (1 + )2‖A − Ak‖2F = (1 +O( ))‖A−Ak‖2F , and the result follows by adjusting by a constant factor.\nTo obtain the nnz(A)+n poly(k/ ) running time guarantee of Theorem 1.1, while we can replace S1 and S2 with compositions of a sparse CountSketch matrix and a Gaussian matrix (see chapter 2 of [Woo14] for a survey), enabling us to compute A1S1 and A2S2 in nnz(A)+n poly(k/ ) time, we still need to solve the regression problem minW ‖A3−WZ3‖2F quickly, and note that we cannot even write down Z3 without spending r2n2 time. Here we use a different random matrix S3 called TensorSketch, which was introduced in [Pag13, PP13], but for which we will need the stronger properties of a subspace embedding and approximate matrix product shown to hold for it in [ANW14]. Given the latter properties, we can instead solve the regression problem minW ‖A3S3 −WZ3S3‖2F , and importantly A3S3 and Z3S3 can be computed in nnz(A) + n poly(k/ ) time. Finally, this small problem can be solved in n poly(k/ ) time.\nIf we want to output a rank-k solution as in Theorem 1.2, then we need to introduce indeterminates at several places in the preceding argument and run a generic polynomial optimization procedure which runs in time exponential in the number of indeterminates. Namely, we write Û as A1S1X1, where X1 is an r×k matrix of indeterminates, we write V̂ as A2S2X2, where X2 is an r×k matrix of indeterminates, and we write Ŵ as A3S3X3, whereX3 is an r×k matrix of indeterminates. When executing the above iterative argument, we let the rows of Z1 be the vectors vec(V ∗i ⊗W ∗i ), the rows of Z2 be the vectors vec(Ûi⊗W ∗i ), and the rows of Z3 be the vectors vec(Ûi⊗Vi). Then Û is a (1+ )-approximate minimizer to minU ‖A1−UZ1‖F , while V̂ is a (1+ )-approximate minimizer to minV ‖A2 − V Z2‖F , while Ŵ is a (1 + )-approximate minimizer to minW ‖A3 −WZ3‖F . Note that by assigning X1 = (Z1S1)†, X2 = (Z2S2)†, and X3 = (Z3S3)†, it follows that the rank-k tensor B = ∑k i=1(A1S1X1)i⊗ (A2S2X2)i⊗ (A3S3X3)i satisfies ‖A−B‖2F ≤ (1 + )3‖A−Ak‖2F , as desired. Note that here the rows of Z2 are a function of X1, while the rows of Z3 are a function of both X1 and X2. What is important for us though is that it suffices to minimize the degree-6 polynomial\n∑\na,b,c∈[n]\n( k∑\ni=1\n(A1S1X1)a,i · (A2S2X2)b,i · (A3S3X3)c,i −Aa,b,c)2,\nover the 3rk = O(k2/ ) indeterminates X1, X2, X3, since we know there exists an assignment to X1, X2, and X3 providing a (1+O( ))-approximate solution, and any solution X1, X2, and X3 found by minimizing the above polynomial will be no worse than that solution. This polynomial can be minimized up to additive 2− poly(n) additive error in poly(n) time [Ren92a, BPR96] assuming the entries of U∗, V ∗, and W ∗ are bounded by 2poly(n), as assumed in Theorem 1.2. Similar arguments can be made for obtaining a relative error approximation to the actual value OPT as well as handling the case when Ak does not exist.\nTo optimize the running time to nnz(A), we can choose CountSketch matrices T1, T2, T3 of t = poly(k, 1/ ) × n dimensions and reapply the above iterative argument. Then it suffices to minimize this small size degree-6 polynomial\n∑\na,b,c∈[t]\n(\nk∑\ni=1\n(T1A1S1X1)a,i · (T2A2S2X2)b,i · (T3A3S3X3)c,i − (A(T1, T2, T3))a,b,c)2,\nover the 3rk = O(k2/ ) indeterminates X1, X2, X3. Outputting A1S1X1, A2S2X2, A3S3X3 then provides a (1 + )-approximate solution.\nOur iterative existential argument provides a general framework for obtaining low rank approximation results for tensors for many other error measures as well."
    }, {
      "heading" : "1.3 Other Low Rank Approximation Algorithms Following Our Framework.",
      "text" : "Column, row, tube subset selection, and CURT decomposition. In tensor column, row, tube subset selection, the goal is to find three matrices: a subset C of columns of A, a subset R of rows of A, and a subset T of tubes of A, such that there exists a small tensor U for which ‖U(C,R, T )−A‖2F ≤ (1+ ) OPT. We first choose two Gaussian matrices S1 and S2 with s1 = s2 = O(k/ ) columns, and form a matrix Z ′3 ∈ R(s1s2)×n\n2 with (i, j)-th row equal to the vectorization of (A1S1)i ⊗ (A2S2)j . Motivated by the regression problem minW ‖A3 −WZ ′3‖F , we sample d3 = O(s1s2/ ) columns from A3 and let D3 denote this selection matrix. There are a few ways to do the sampling depending on the tradeoff between the number of columns and running time, which we describe below. Proceeding iteratively, we write down Z ′2 by setting its (i, j)-th row to the vectorization of (A1S1)i ⊗ (A3D3)j . We then sample d2 = O(s1d3/ ) columns from A2 and let D2 denote that selection matrix. Finally, we define Z ′1 by setting its (i, j)-th row to be the vectorization of (A2D2)i ⊗ (A3D3)j . We obtain C = A1D1, R = A2D2 and T = A3D3. For the sampling steps, we can use a generalized matrix column subset selection technique, which extends a column subset selection technique of [BW14] in the context of CUR decompositions to the case when C is not necessarily a subset of the input. This gives O(nnz(A) log n) + Õ(n2) poly(k, 1/ ) time. Alternatively, we can use a technique we develop called tensor leverage score sampling described below, yielding O(nnz(A)) + n poly(k, 1/ ) time.\nA body of work in the matrix case has focused on finding the best possible number of columns and rows of a CUR decomposition, and we can ask the same question for tensors. It turns out that if one is given the factorization ∑k i=1(UB)i ⊗ (VB)i ⊗ (WB)i of a rank-k tensor B ∈ Rn×n×n with UB, VB,WB ∈ Rn×k, then one can find a set C of O(k/ ) columns, a set R of O(k/ ) rows, and a set T of O(k/ ) tubes of A, together with a rank-k tensor U for which ‖U(C,R, T )−A‖2F ≤ (1 + )‖A−B‖2F . This is based on an iterative argument, where the initial sampling (which needs to be our generalized matrix column subset selection rather than tensor leverage score sampling to achieve optimal bounds) is done with respect to V >B W>B , and then an iterative argument is carried out. Since we show a matching lower bound on the number of columns, rows, tubes and rank of U , these parameters are tight. The algorithm is efficient if one is given a rank-k tensor B\nwhich is a (1 + O( ))-approximation to A; if not then one can use Theorem C.2 and and this step will be exponential time in k. If one just wants O(k log k+k/ ) columns, rows, and tubes, then one can achieve O(nnz(A)) + n poly(k, 1/ ) time, if one is given B.\nColumn-row, row-tube, tube-column face subset selection, and CURT decomposition. In tensor column-row, row-tube, tube-column face subset selection, the goal is to find three tensors: a subset C ∈ Rc×n×n of row-tube faces of A, a subset R ∈ Rn×r×n of tube-column faces of A, and a subset T ∈ Rn×n×t of column-row faces of A, such that there exists a tensor U ∈ Rtn×cn×rn with small rank for which ‖U(T1, C2, R3) − A‖2F ≤ (1 + ) OPT, where T1 ∈ Rn×tn denotes the matrix obtained by flattening the tensor T along the first dimension, C2 ∈ Rn×cn denotes the matrix obtained by flattening the tensor C along the second dimension, and R3 ∈ Rn×rn denotes the matrix obtained by flattening the tensor T along the third dimension.\nWe solve this problem by first choosing two Gaussian matrices S1 and S2 with s1 = s2 = O(k/ ) columns, and then forming matrix U3 ∈ Rn×s1s2 with (i, j)-th column equal to (A1S1)i, as well as matrix V3 ∈ Rn×s1s2 with (i, j)-th column equal to (A2S2)j . Inspired by the regression problem minW∈Rn×s1s2 ‖V3 · (W> U>3 ) − A2‖F , we sample d3 = O(s1s2/ ) rows from A2 and let D3 ∈ Rn×n denote this selection matrix. In other words, D3 selects d3 tube-column faces from the original tensor A. Thus, we obtain a small regression problem: minW ‖D3V3 · (W> U>3 ) − D3A2‖F . By retensorizing the objective function, we obtain the problem minW ‖U3⊗ (D3V3)⊗W − A(I,D3, I)‖F . Flattening the objective function along the third dimension, we obtain minW ‖W · (U>3 (D3V3)>)− (A(I,D3, I))3‖F which has optimal solution (A(I,D3, I))3(U>3 (D3V3)>)†. Let W ′ denote A(I,D3, I))3. In the next step, we fix W2 = W ′(U>3 (D3V3)>)† and U2 = U3, and consider the objective function minV ‖U2 · (V > W>2 ) − A1‖F . Applying a similar argument, we obtain V ′ = (A(D2, I, I))2 and U ′ = (A(I, I,D1)1). Let C denote A(D2, I, I), R denote A(I,D3, I), and T denote A(I, I,D1). Overall, this algorithm selects poly(k, 1/ ) faces from each dimension.\nSimilar to our column-based CURT decomposition, our face-based CURT decomposition has the property that if one is given the factorization ∑k i=1(UB)i ⊗ (VB)i ⊗ (WB)i of a rank-k tensor B ∈ Rn×n×n with UB, VB,WB ∈ Rn×k which is a (1+O( ))-approximation to A, then one can find a set C of O(k/ ) row-tube faces, a set R of O(k/ ) tube-column faces, and a set T of O(k/ ) columnrow faces of A, together with a rank-k tensor U for which ‖U(T1, C2, R3)−A‖2F ≤ (1 + ) OPT.\nTensor multiple regression and tensor leverage score sampling. In the above we need to consider standard problems for matrices in the context of tensors. Suppose we are given a matrix A ∈ Rn1×n2n3 and a matrix B = (V > W>) ∈ Rk×n2n3 with rows (Vi ⊗Wi) for an n2 × k matrix V and n3 × k matrix W . Using TensorSketch [Pag13, PP13, ANW14] one can solve multiple regression minU ‖UB−A‖F without forming B in O(n2+n3) poly(k, 1/ ) time, rather than the naïve O(n2n3) poly(k, 1/ ) time. However, this does not immediately help us if we would like to sample columns of such a matrix B proportional to its leverage scores. Even if we apply TensorSketch to compute a k × k change of basis matrix R in O(n2 + n3) poly(k, log(n2n3)) time, for which the leverage scores of B are (up to a constant factor) the squared column norms of R−1B, there are still n2n3 leverage scores and we cannot write them all down! Nevertheless, we show we can still sample by them. For the i-th row eiR−1 of R−1, we create a matrix V\n′i by scaling each of the columns of V > entrywise by the entries of z. The squared norms of eiR−1B are exactly the squared entries of (V ′i)W>. We cannot compute this matrix product, but we can first sample a column of it proportional to its squared norm and then sample an entry in that column proportional to its square. To sample a column, we compute G(V ′i)W> for a Gaussian matrix G with O(log n3) rows by computing G · V ′i, then computing (G · V ′i) ·W>, which is O(n2 + n3) poly(k, log(n2n3)) total\ntime. After sampling a column, we compute the column exactly and sample a squared entry. We do this for each i ∈ [k], first sampling an i proportional to ‖GV ′iW>‖2F , then running the above scheme on that i. The poly(log n) factor in the running time can be replaced by poly(k) if one wants to avoid a poly(log n) dependence in the running time.\nEntry-wise `1 low-rank approximation. We consider the problem of entrywise `1-low rank approximation of an n×n×n tensor A, namely, the problem of finding a rank-k tensor B for which ‖A − B‖1 ≤ poly(k, log n) OPT, where OPT = infrank-k B ‖A − B‖1, and where for a tensor A, ‖A‖1 = ∑ i,j,k |Ai,j,k|. Our iterative existential argument can be applied in much the same way as for the Frobenius norm. We iteratively flatten A along each of its three dimensions, obtaining A1, A2, and A3 as above, and iteratively build a good rank-k solution B of the form (A1S1X1)⊗(A2S2X2)⊗ (A3S3X3), where now the Si are matrices of i.i.d. Cauchy random variables or sparse matrices of Cauchy random variables and the Xi are O(k log k) × k matrices of indeterminates. For a matrix C and a matrix S of i.i.d. Cauchy random variables with k columns, it is known [SWZ17] that the column span of CS contains a poly(k log n)-approximate rank-k space with respect to the entrywise `1-norm for C. In the case of tensors, we must perform an iterative flattening and retensorizing argument to guarantee there exists a tensor B of the form above. Also, if we insist on outputting a rank-k solution as opposed to a bicriteria solution, ‖(A1S1X1)⊗ (A2S2X2)⊗ (A3S3X3)−A‖1 is not a polynomial of the Xi, and if we introduce sign variables for the n3 absolute values, the running time of the polynomial solver will be 2# of variables = 2Ω(n3). We perform additional dimensionality reduction by Lewis weight sampling [CP15] from the flattenings to reduce the problem size to poly(k). This small problem still has Õ(k3) sign variables, and to obtain a 2Õ(k2) running time we relax the reduced problem to a Frobenius norm problem, mildly increasing the approximation factor by another poly(k) factor.\nCombining the iterative existential argument with techniques in [SWZ17], we also obtain an `1 CURT decomposition algorithm (which is similar to the Frobenius norm result in Theorem 1.4), which can find Õ(k) columns, Õ(k) rows, Õ(k) tubes, and a tensor U . Our algorithm starts from a given factorization of a rank-k tensor B = UB ⊗ VB ⊗WB found above. We compute a sampling and rescaling diagonal matrix D1 according to the Lewis weights of matrix B1 = (V >B W>B ), where D1 has Õ(k) nonzero entries. Then we iteratively construct B2, D2, B3 and D3. Finally we have C = A1D1 (selecting Õ(k) columns from A), R = A2D2 (selecting Õ(k) rows from A), T = A3D3 (selecting Õ(k) tubes from A) and tensor U = ((B1D1)†)⊗ ((B2D2)†)⊗ ((B3D3)†).\nWe have similar results for entry-wise `p, 1 ≤ p < 2, via analogous techniques.\n`1-`2-`2 low-rank approximation (sum of Euclidean norms of faces). For an n × n × n tensor A, in `1-`2-`2 low rank approximation we seek a rank-k tensor B for which ‖A − B‖v ≤ poly(k, log n) OPT, where OPT = infrank-k B ‖A − B‖v and where ‖A‖v = ∑ i( ∑ j,k(Ai,j,k) 2) 1 2 for a tensor A. This norm is asymmetric, i.e., not invariant under permutations to its coordinates, and we cannot flatten the tensor along each of its dimensions while preserving its cost. Instead, we embed the problem to a new problem with a symmetric norm. Once we have a symmetric norm, we apply an iterative existential argument. We choose an oblivious sketching matrix (the M -Sketch in [CW15b]) S ∈ Rs×n with s = poly(k, log n), and reduce the original problem to ‖S(A − B)‖v, by losing a small approximation factor. Because s is small, we can then turn the `1 part of the problem to `2 by losing another √ s in the approximation, so that now the problem is a Frobenius norm problem. We then apply our iterative existential argument to the problem ‖S(∑ki=1 U∗i ⊗ (Â2S2X2)i⊗ (Â3S3X3)i−A)‖F where U∗ is a fixed matrix and Â = SA, and output a bicriteria solution.\n`1-`1-`2 low-rank approximation (sum of Euclidean norms of tubes). For an n×n×n tensor A, in the `1-`1-`2 low rank approximation problem we seek a rank-k tensor B for which ‖A−B‖u ≤ poly(k, log n) OPT, where OPT = infrank-k B ‖A−B‖u and ‖A‖u = ∑ i,j( ∑ k(Ai,j,k) 2) 1 2 . The main difficulty in this problem is that the norm is asymmetric, and we cannot flatten the tensor along all three dimensions. To reduce the problem to a problem with a symmetric norm, we choose random Gaussian matrices S ∈ Rn×s with s = O(n). By Dvoretzky’s theorem [Dvo61], for all tensors A, ‖AS‖1 ≈ ‖A‖u, which reduces our problem to minrank-k B ‖(A−B)S‖1. Via an iterative existential argument, we obtain a generalized version of entrywise `1 low rank approximation, ‖((Â1S1X1) ⊗ (Â2S2X2) ⊗ (A3S3X3) − A)S‖1, where Â = AS is an n × n × s size tensor. Finally, we can either use a polynomial system solver to obtain a rank-k solution, or output a bicriteria solution.\nWeighted low-rank approximation. We also consider weighted low rank approximation. Given an n × n × n tensor A and an n × n × n tensor W of weights, we want to find a rank-k tensor B for which ‖W ◦ (A − B)‖2F ≤ (1 + ) OPT, where OPT = infrank-k B ‖W ◦ (A − B)‖2F and where for a tensor A, ‖W ◦ A‖F = ( ∑ i,j,kW 2 i,j,kA 2 i,j,k) 1 2 . We provide two algorithms based on different assumptions on the weight tensor W . The first algorithm assumes that W has r distinct faces on each of its three dimensions. We flatten A and W along each of its three dimensions, obtaining A1, A2, A3 and W1,W2,W3. Because each Wi has r distinct rows, combining the “guess a sketch” technique from [RSW16] with our iterative argument, we can create matrices U1, U2, and U3 in terms of O(rk2/ ) total indeterminates and for which a solution to the objective function ‖W ◦ (∑ki=1(U1)i ⊗ (U2)i ⊗ (U3)i − A)‖2F , together with O(r) side constraints, gives a (1 + )- approximation. We can solve the latter problem in poly(n) · 2Õ(rk2/ ) time. Our second algorithm assumes W has r distinct faces in two dimensions. Via a pigeonhole argument, the third dimension will have at most 2Õ(r) distinct faces. We again use O(rk2/ ) variables to express U1 and U2, but now express U3 in terms of these variables, which is necessary since W3 could have an exponential number of distinct rows, ultimately causing too many variables needed to express U3 directly. We again arrive at the objective function ‖W ◦ (∑ki=1(U1)i ⊗ (U2)i ⊗ (U3)i −A)‖2F , but now have 2Õ(r) side constraints, coming from the fact that U3 is a rational function of the variables created for U1 and U2 and we need to clear denominators. Ultimately, the running time is 2Õ(r 2k2/ ).\nComputational Hardness. Our 2δk1−o(1) time hardness for c-approximation in Theorem H.42 is shown via a reduction from approximating MAX-3SAT to approximating MAX-E3SAT, where the latter problem has the property that each clause in the satisfiability instance has exactly 3 literals (in MAX-3SAT some clauses may have 2 literals). Then, a reduction [Tre01] from approximating MAXE3SAT to approximating MAX-E3SAT(B) is performed, for a constant B which provides an upper bound on the number of clauses each literal can occur in. Given an instance φ to MAX-E3SAT(B), we create a 3rd order tensor T as Håstad does using φ [Hås90]. While Håstad’s reduction guarantees that the rank of T is at most r if φ is satisfiable, and at least r+ 1 otherwise, we can show that if φ is not satisfiable then its rank is at least the minimal size of a set of variables which is guaranteed to intersect every unsatisfied clause in any unsatisfiable assignment. Since if φ is not satisfiable, there are at least a linear fraction of clauses in φ that are unsatisfied under any assignment by the inapproximability of MAX-E3SAT(B), and since each literal occurs in at most B clauses for a constant B, it follows that the rank of T when φ is not satisfiable is at least c0r for a constant c0 > 1. Further, under ETH , our reduction implies one cannot approximate MAX-E3SAT(B), and thus approximate the rank of a tensor up to a factor c0, in less than 2δk\n1−o(1) time. We need the near-linear size reduction of MAX-3SAT to MAX-E3SAT of [MR10] to get our strongest result.\nAlgorithm 1 Main Meta-Algorithm 1: procedure TensorLowRankApproxBicriteria(A,n, k, ) . Theorem 1.1 2: Choose sketching matrices S2,S3(Composition of Gaussian and CountSketch.) 3: Choose sketching matrices T2,T3(CountSketch.) 4: Compute T2A2S2, T3A3S3. 5: Construct V̂ by setting (i, j)-th column to be (A2S2)i. 6: Construct Ŵ by setting (i, j)-th column to be (A3S3)j . 7: Construct matrix B by setting (i, j)-th row of B is vectorization of (T2A2S2)i ⊗ (T3A3S3)j . 8: Solve minU ‖UB − (A(I, T2, T3))1‖2F . 9: return Û , V̂ , and Ŵ . 10: end procedure 11: procedure TensorLowRankApprox(A,n, k, ) . Theorem 1.2 12: Choose sketching matrices S1,S2,S3(Composition of Gaussian and CountSketch.) 13: Choose sketching matrices T1,T2,T3(CountSketch.) 14: Compute T1A1S1, T2A2S2, T3A3S3. 15: Solve minX1,X2,X3 ‖(T1A1S1X1)⊗ (T2A2S2X2)⊗ (T3A3S3X3)−A(T1, T2, T3)‖2F . 16: return A1S1X1, A2S2X2, and A3S3X3. 17: end procedure\nThe 2Ω(1/ 1/4) time hardness for (1 + )-approximation for rank-1 tensors in Theorem H.21 strengthens the NP-hardness for rank-1 tensor computation in Section 7 of [HL13], where instead of assuming the NP-hardness of the Clique problem, we assume ETH . Also, the proof in [HL13] did not explicitly bound the approximation error; we do this for a poly(1/ )-sized tensor (which can be padded with 0s to a poly(n)-sized tensor) to rule out (1 + )-approximation in 2o(1/ 1/4) time.\nThe same hard instance above shows, assuming ETH , that 2Ω(1/ 1/2) time is necessary for (1+ )- approximation to the spectral norm of a symmetric rank-1 tensor (see Section H.2 and Section H.3).\nAssuming ETH , the 21/ 1−o(1)-hardness [SWZ17] for matrix `1-low rank approximation gives the same hardness for tensor entry-wise `1 and `1-`1-`2 low rank approximation. Also, under ETH , we strengthen the NP-hardness in [CW15a] to a 21/ Ω(1)-hardness for `1-`2-low rank approximation of a matrix, which gives the same hardness for tensor `1-`2-`2 low rank approximation.\nHard Instance. We extend the previous matrix CUR hard instance [BW14] to 3rd order tensors by planting multiple rotations of the hard instance for matrices into a tensor. We show C must select Ω(k/ ) columns from A, R must select Ω(k/ ) rows from A, and T must select Ω(k/ ) tubes from A. Also the tensor U must have rank at least k. This generalizes to q-th order tensors.\nOptimal matrix CUR decomposition. We also improve the nnz(A) log n+(n+d) poly(log n, k, 1/ ) running time of [BW14] for CUR decomposition of A ∈ Rn×d to nnz(A) + (n+ d) poly(k, 1/ ), while selecting the optimal number of columns, rows, and a rank-k matrix U . Using [CW13, MM13, NN13], we find a matrix Û with k orthonormal columns in nnz(A) + n poly(k/ ) time for which minV ‖ÛV − A‖2F ≤ (1 + )‖A − Ak‖2F . Let s1 = Õ(k/ 2) and S1 ∈ Rs1×n be a sampling/rescaling matrix by the leverage scores of Û . By strengthening the affine embedding analysis of [CW13] to leverage score sampling (the analysis of [CW13] gives a weaker analysis for affine embeddings using leverage scores which does not allow approximation in the sketch space to translate to approximation in the original space), with probability at least 0.99, for all X ′ which satisfy ‖S1ÛX ′ − S1A‖2F ≤ (1+ ′) minX ‖S1ÛX−S1A‖2F , we have ‖ÛX ′−A‖2F ≤ (1+ ) minX ‖ÛX−A‖2F , where ′ = 0.0001 .\nApplying our generalized row subset selection procedure, we can find Y,R for which ‖S1ÛY R − S1A‖2F ≤ (1 + ′) minX ‖S1ÛX − S1A‖2F , where R contains O(k/ ′) = O(k/ ) rescaled rows of S1A. A key point is that rescaled rows of S1A are also rescaled rows of A. Then, ‖ÛY R − A‖2F ≤ (1 + ) minX ‖ÛX − A‖2F . Finding Y,R can be done in dpoly(s1/ ) = dpoly(k/ ) time. Now set V̂ = Y R. We can choose S2 to be a sampling/rescaling matrix, and then find C,Z for which ‖CZV̂ S2 − AS2‖2F ≤ (1 + ′) minX ‖XV̂ S2 − AS2‖2F in a similar way, where C contains O(k/ ) rescaled columns of AS2, and thus also of A. We thus have ‖CZY R−A‖2F ≤ (1 +O( ))‖A−Ak‖2F .\nDistributed and streaming settings. Since our algorithms use linear sketches, they are implementable in distributed and streaming models. We use random variables with limited independence to succinctly store the sketching matrices [CW13, KVW14, KN14, Woo14, SWZ17].\nExtension to other notions of tensor rank. This paper focuses on the standard CP rank, or canonical rank, of a tensor. As mentioned, due to border rank issues, the best rank-k solution does not exist in certain cases. There are other notions of tensor rank considered in some applications which do not suffer from this problem, e.g., the tucker rank [KC07, PC08, MH09, ZW13, YC14], and the train rank [Ose11, OTZ11, ZWZ16, PTBD16]). We also show observe that our techniques can be applied to these notions of rank."
    }, {
      "heading" : "1.4 An Algorithm and a Roadmap",
      "text" : "Roadmap Section A introduces notation and definitions. Section B includes several useful tools. We provide our Frobenius norm low rank approximation algorithms in Section C. Section C.10 extends our results to general q-th order tensors. Section D has our results for entry-wise `1 norm low rank approximation. Section E has our results for entry-wise `p norm low rank approximation. Section G has our results for weighted low rank approximation. Section F has our results for asymmetric norm low rank approximation algorithms. We present our hardness results in Section H and Section I. Section J and Section K extend the results to distributed and streaming settings. Section L extends our techniques from tensor rank to other notions of tensor rank including tensor tucker rank and tensor train rank."
    }, {
      "heading" : "A Notation",
      "text" : "For an n ∈ N+, let [n] denote the set {1, 2, · · · , n}. For any function f , we define Õ(f) to be f · logO(1)(f). In addition to O(·) notation, for two functions f, g, we use the shorthand f . g (resp. &) to indicate that f ≤ Cg (resp. ≥) for an absolute constant C. We use f h g to mean cf ≤ g ≤ Cf for constants c, C.\nFor a matrix A, we use ‖A‖2 to denote the spectral norm of A. For a tensor A, let ‖A‖ and ‖A‖2 (which we sometimes use interchangeably) denote the spectral norm of tensor A,\n‖A‖ = sup x,y,z 6=0 |A(x, y, z)| ‖x‖ · ‖y‖ · ‖z‖ .\nLet ‖A‖F denote the Frobenius norm of a matrix/tensor A, i.e., ‖A‖F is the square root of sum of squares of all the entries of A. For 1 ≤ p < 2, we use ‖A‖p to denote the entry-wise `p-norm of a matrix/tensor A, i.e., ‖A‖p is the p-th root of the sum of p-th powers of the absolute values of the entries of A. ‖A‖1 will be an important special case of ‖A‖p, which corresponds to the sum of absolute values of all of the entries.\nLet nnz(A) denote the number of nonzero entries of A. Let det(A) denote the determinant of a square matrix A. Let A> denote the transpose of A. Let A† denote the Moore-Penrose pseudoinverse of A. Let A−1 denote the inverse of a full rank square matrix.\nFor a 3rd order tensor A ∈ Rn×n×n, its x-mode fibers are called column fibers (x = 1), row fibers (x = 2) and tube fibers (x = 3). For tensor A, we use A∗,j,l to denote its (j, l)-th column, we use Ai,∗,l to denote its (i, l)-th row, and we use Ai,j,∗ to denote its (i, j)-th tube.\nA tensor A is symmetric if and only if for any i, j, k, Ai,j,k = Ai,k,j = Aj,i,k = Aj,k,i = Ak,i,j = Ak,j,i.\nFor a tensor A ∈ Rn1×n2×n3 , we use > to denote rotation (3 dimensional transpose) so that A> ∈ Rn3×n1×n2 . For a tensor A ∈ Rn1×n2×n3 and matrix B ∈ Rn3×k, we define the tensor-matrix dot product to be A ·B ∈ Rn1×n2×k.\nWe use ⊗ to denote outer product, ◦ to denote entrywise product, and · to denote dot product. Given two column vectors u, v ∈ Rn, let u⊗ v ∈ Rn×n and (u⊗ v)i,j = ui · vj , u>v = ∑n i=1 uivi ∈ R and (u ◦ v)i = uivi. Definition A.1 (⊗ product for vectors). Given q vectors u1 ∈ Rn1, u2 ∈ Rn2 , · · · , uq ∈ Rnq , we use u1 ⊗ u2 ⊗ · · · ⊗ uq to denote an n1 × n2 × · · · × nq tensor such that, for each (j1, j2, · · · , jq) ∈ [n1]× [n2]× · · · × [nq],\n(u1 ⊗ u2 ⊗ · · · ⊗ uq)j1,j2,··· ,jq = (u1)j1(u2)j2 · · · (uq)jq , where (ui)ji denotes the ji-th entry of vector ui.\nDefinition A.2 (vec(), convert tensor into a vector). Given a tensor A ∈ Rn1×n2×···×nq , let vec(A) ∈ R1× ∏q i=1 ni be a row vector, such that the t-th entry of vec(A) is Aj1,j2,··· ,jq where t =\n(j1 − 1) ∏q i=2 ni + (j2 − 1) ∏q i=3 ni + · · ·+ (jq−1 − 1)nq + jq.\nFor example if u = [ 1 2 ] , v =\n \n3 4 5\n  then vec(u⊗ v) = [ 3 4 5 6 8 10 ] .\nDefinition A.3 (⊗ product for matrices). Given q matrices U1 ∈ Rn1×k, U2 ∈ Rn2×k, · · · , Uq ∈ Rnq×k, we use U1 ⊗ U2 ⊗ · · · ⊗ Uq to denote an n1 × n2 × · · · × nq tensor which can be written as,\nU1 ⊗ U2 ⊗ · · · ⊗ Uq = k∑\ni=1\n(U1)i ⊗ (U2)i ⊗ · · · ⊗ (Uq)i ∈ Rn1×n2×···×nq ,\nwhere (Uj)i denotes the i-th column of matrix Uj ∈ Rnj×k. Definition A.4 ( product for matrices). Given q matrices U1 ∈ Rk×n1, U2 ∈ Rk×n2, · · · , Uq ∈ Rk×nq , we use U1 U2 · · · Uq to denote a k× ∏q j=1 nj matrix where the i-th row of U1 U2 · · · Uq is the vectorization of (U1)i ⊗ (U2)i ⊗ · · · ⊗ (Uq)i, i.e.,\nU1 U2 · · · Uq =   vec((U1) 1 ⊗ (U2)1 ⊗ · · · ⊗ (Uq)1) vec((U1) 2 ⊗ (U2)2 ⊗ · · · ⊗ (Uq)2)\n· · · vec((U1) k ⊗ (U2)k ⊗ · · · ⊗ (Uq)k)\n  ∈ R k× ∏q j=1 nj .\nwhere (Uj)i ∈ Rnj denotes the i-th row of matrix Uj ∈ Rk×nj .\nDefinition A.5 (Flattening vs unflattening/retensorizing). Suppose we are given three matrices U ∈ Rn1×k, V ∈ Rn2×k, W ∈ Rn3×k. Let tensor A ∈ Rn1×n2×n3 denote U ⊗ V ⊗W . Let A1 ∈ Rn1×n2n3 denote a matrix obtained by flattening tensor A along the 1st dimension. Then A1 = U ·B, where B = V > W> ∈ Rk×n2n3 denotes the matrix for which the i-th row is vec(Vi⊗Wi),∀i ∈ [k]. We let the “flattening” be the operation that obtains A1 by A. Given A1 = U · B, we can obtain tensor A by unflattening/retensorizing A1. We let “retensorization” be the operation that obtains A from A1. Similarly, let A2 ∈ Rn2×n1n3 denote a matrix obtained by flattening tensor A along the 2nd dimension, so A2 = V · C, where C = W> U> ∈ Rk×n1n3 denotes the matrix for which the i-th row is vec(Wi ⊗ Ui), ∀i ∈ [k]. Let A3 ∈ Rn3×n1n2 denote a matrix obtained by flattening tensor A along the 3rd dimension. Then, A3 = W ·D, where D = U> V > ∈ Rk×n1n2 denotes the matrix for which the i-th row is vec(Ui ⊗ Vi),∀i ∈ [k]. Definition A.6 ( (·, ·, ·) operator for tensors and matrices). Given tensor A ∈ Rn1×n2×n3 and three matrices B1 ∈ Rn1×d1, B2 ∈ Rn2×d2, B3 ∈ Rn3×d3, we define tensors A(B1, I, I) ∈ Rd1×n2×n3 , A(I,B2, I) ∈ Rn1×d2×n3, A(I, I, B3) ∈ Rn1×n2×d3, A(B1, B2, I) ∈ Rd1×d2×n3 , A(B1, B2, B3) ∈ Rd1×d2×d3 as follows,\nA(B1, I, I)i,j,l =\nn1∑\ni′=1\nAi′,j,l(B1)i′,i, ∀(i, j, l) ∈ [d1]× [n2]× [n3]\nA(I,B2, I)i,j,l =\nn2∑\nj′=1\nAi,j′,l(B2)j′,j , ∀(i, j, l) ∈ [n1]× [d2]× [n3]\nA(I, I, B3)i,j,l =\nn3∑\nl′=1\nAi,j,l′(B3)l′,l, ∀(i, j, l) ∈ [n1]× [n2]× [d3]\nA(B1, B2, I)i,j,l =\nn1∑\ni′=1\nn2∑\nj′=1\nAi′,j′,l(B1)i′,i(B2)j′,j , ∀(i, j, l) ∈ [d1]× [d2]× [n3]\nA(B1, B2, B3)i,j,l =\nn1∑\ni′=1\nn2∑\nj′=1\nn3∑\nl′=1\nAi′,j′,l′(B1)i′,i(B2)j′,j(B3)l′,l, ∀(i, j, l) ∈ [d1]× [d2]× [d3]\nNote that B>1 A = A(B1, I, I), AB3 = A(I, I, B3) and B>1 AB3 = A(B1, I, B3). In our paper, if ∀i ∈ [3], Bi is either a rectangular matrix or a symmetric matrix, then we sometimes use A(B1, B2, B3) to denote A(B>1 , B>2 , B>3 ) for simplicity. Similar to the (·, ·, ·) operator on 3rd order tensors, we can define the (·, ·, · · · , ·) operator on higher order tensors.\nFor the matrix case, min rank−k A′ ‖A − A′‖2F always exists. However, this is not true for tensors [DSL08]. For convenience, we redefine the notation of OPT and min.\nDefinition A.7. Given tensor A ∈ Rn1×n2×n3 , k > 0, if min rank−k A′ ‖A−A′‖2F does not exist, then we define OPT = inf\nrank−k A′ ‖A−A′‖2F +γ for sufficiently small γ > 0, which can be an arbitrarily small\npositive function of n. We let min rank−k A′ ‖A−A′‖2F be the value of OPT, and we let arg min rank−k A′ ‖A−A′‖2F be a rank−k tensor Ak ∈ Rn1×n2×n3 which satisfies ‖A−Ak‖2F = OPT ."
    }, {
      "heading" : "B Preliminaries",
      "text" : "Section B.1 provides the definitions for Subspace Embeddings and Approximate Matrix Product. We introduce the definition for Tensor-CURT decomposition in Section B.2. Section B.3 presents\na tool which we call a “polynomial system verifier”. Section B.4 introduces a tool which is able to determine the minimum nonzero value of the absolute value of a polynomial evaluated on a set, provided the polynomial is never equal to 0 on that set. Section B.5 shows how to relax an `p problem to an `2 problem. We provide definitions for CountSketch and Gaussian transforms in Section B.6. We present Cauchy and p-stable transforms in Section B.7. We introduce leverage scores and Lewis weights in Section B.8 and Section B.9. Finally, we explain an extension of CountSketch, which is called TensorSketch in Section B.10."
    }, {
      "heading" : "B.1 Subspace Embeddings and Approximate Matrix Product",
      "text" : "Definition B.1 (Subspace Embedding). A (1 ± ) `2-subspace embedding for the column space of an n× d matrix A is a matrix S for which for all x ∈ Rd, ‖SAx‖22 = (1± )‖Ax‖22.\nDefinition B.2 (Approximate Matrix Product). Let 0 < < 1 be a given approximation parameter. Given matrices A and B, where A and B each have n rows, the goal is to output a matrix C so that ‖A>B−C‖F ≤ ‖A‖F ‖B‖F . Typically C has the form A>S>SB, for a random matrix S with a small number of rows. See, e.g., Lemma 32 of [CW13] for a number of example matrices S with O( −2) rows for which this property holds."
    }, {
      "heading" : "B.2 Tensor CURT decomposition",
      "text" : "We first review matrix CUR decompositions:\nDefinition B.3 (Matrix CUR, exact). Given a matrix A ∈ Rn×d, we choose C ∈ Rn×c to be a subset of columns of A and R ∈ Rr×n to be a subset of rows of A. If there exists a matrix U ∈ Rc×r such that A can be written as,"
    }, {
      "heading" : "CUR = A,",
      "text" : "then we say C,U,R is matrix A’s CUR decomposition.\nDefinition B.4 (Matrix CUR, approximate). Given a matrix A ∈ Rn×d, a parameter k ≥ 1, an approximation ratio α > 1, and a norm ‖‖ξ, we choose C ∈ Rn×c to be a subset of columns of A and R ∈ Rr×n to be a subset of rows of A. Then if there exists a matrix U ∈ Rc×r such that,\n‖CUR−A‖ξ ≤ α min rank−k Ak ‖Ak −A‖ξ,\nwhere ‖‖ξ can be operator norm, Frobenius norm or Entry-wise `1 norm, we say that C,U,R is matrix A’s approximate CUR decomposition, and sometimes just refer to this as a CUR decomposition.\nDefinition B.5 ([Bou11]). Given matrix A ∈ Rm×n, integer k, and matrix C ∈ Rm×r with r > k, we define the matrix ΠξC,k(A) ∈ Rm×n to be the best approximation to A (under the ξ-norm) within the column space of C of rank at most k; so, ΠξC,k(A) ∈ Rm×n minimizes the residual ‖A − Â‖ξ, over all Â ∈ Rm×n in the column space of C of rank at most k.\nWe define the following notion of tensor-CURT decomposition.\nDefinition B.6 (Tensor CURT, exact). Given a tensor A ∈ Rn1×n2×n3, we choose three sets of pair of coordinates S1 ⊆ [n2]× [n3], S2 ⊆ [n1]× [n3], S3 ⊆ [n1]× [n2]. We define c = |S1|, r = |S2| and t = |S3|. Let C ∈ Rn1×c denote a subset of columns of A, R ∈ Rn2×r denote a subset of rows of A, and T ∈ Rn3×t denote a subset of tubes of A. If there exists a tensor U ∈ Rc×r×t such that A can be written as\n(((U · T>)> ·R>)> · C>)> = A,\nor equivalently,\nU(C,R, T ) = A,\nor equivalently,\n∀(i, j, l) ∈ [n1]× [n2]× [n3], Ai,j,l = c∑\nu1=1\nr∑\nu2=1\nt∑\nu3=1\nUu1,u2,u3Ci,u1Rj,u2Tl,u3 ,\nthen we say C,U,R, T is tensor A’s CURT decomposition.\nDefinition B.7 (Tensor CURT, approximate). Given a tensor A ∈ Rn1×n2×n3, for some k ≥ 1, for some approximation α > 1, for some norm ‖‖ξ, we choose three sets of pair of coordinates S1 ⊆ [n2] × [n3], S2 ⊆ [n1] × [n3], S3 ⊆ [n1] × [n2]. We define c = |S1|, r = |S2| and t = |S3|. Let C ∈ Rn1×c denote a subset of columns of A, R ∈ Rn2×r denote a subset of rows of A, and T ∈ Rn3×t denote a subset of tubes of A. If there exists a tensor U ∈ Rc×r×t such that\n‖U(C,R, T )−A‖ξ ≤ α min rank−k Ak ‖Ak −A‖ξ,\nwhere ‖‖ξ is operator norm, Frobenius norm or Entry-wise `1 norm, then we refer to C,U,R, T as an approximate CUR decomposition of A, and sometimes just refer to this as a CURT decomposition of A.\nRecently, [TM17] studied a very different face-based tensor-CUR decomposition, which selects faces from tensors rather than columns. To achieve their results, [TM17] need to make several incoherence assumptions on the original tensor. Their sample complexity depends on log n, and they only sample two of the three dimensions. We will provide more general face-based tensor CURT decompositions.\nDefinition B.8 (Tensor (face-based) CURT, exact). Given a tensor A ∈ Rn1×n2×n3, we choose three sets of coordinates S1 ⊆ [n1], S2 ⊆ [n2], S3 ⊆ [n3]. We define c = |S1|, r = |S2| and t = |S3|. Let C ∈ Rc×n2×n3 denote a subset of row-tube faces of A, R ∈ Rn1×r×n3 denote a subset of columntube faces of A, and T ∈ Rn1×n2×t denote a subset of column-row faces of A. Let C2 ∈ Rn2×cn3\ndenote the matrix obtained by flattening the tensor C along the second dimension. Let R3 ∈ Rn3×rn1 denote the matrix obtained by flattening the tensor R along the third dimension. Let T1 ∈ Rn1×tn2 denote the matrix obtained by flattening the tensor T along the first dimension. If there exists a tensor U ∈ Rtn2×cn3×rn1 such that A can be written as\ntn2∑\ni=1\ncn3∑\nj=1\nrn1∑\nl=1\nUi,j,l(T1)l ⊗ (C2)i ⊗ (R3)j = A,\nU(T1, C2, R3) = A,\nor equivalently,\n∀(i′, j′, l′) ∈ [n1]× [n2]× [n3], Ai,j,l = tn1∑\ni=1\ncn3∑\nj=1\nrn2∑\nl=1\nUi,j,l(T1)i′,i(C2)j′,j(R3)l′,l,\nthen we say C,U,R, T is tensor A’s (face-based) CURT decomposition.\nDefinition B.9 (Tensor (face-based) CURT, approximate). Given a tensor A ∈ Rn1×n2×n3 , for some k ≥ 1, for some approximation α > 1, for some norm ‖‖ξ,we choose three sets of coordinates S1 ⊆ [n1], S2 ⊆ [n2], S3 ⊆ [n3]. We define c = |S1|, r = |S2| and t = |S3|. Let C ∈ Rc×n2×n3 denote a subset of row-tube faces of A, R ∈ Rn1×r×n3 denote a subset of column-tube faces of A, and T ∈ Rn1×n2×t denote a subset of column-row faces of A. Let C2 ∈ Rn2×cn3 denote the matrix obtained by flattening the tensor C along the second dimension. Let R3 ∈ Rn3×rn1 denote the matrix obtained by flattening the tensor R along the third dimension. Let T1 ∈ Rn1×tn2 denote the matrix obtained by flattening the tensor T along the first dimension. If there exists a tensor U ∈ Rtn2×cn3×rn1 such that\n‖U(T1, C2, R3)−A‖ξ ≤ α min rank−k Ak ‖Ak −A‖ξ,\nwhere ‖‖ξ is operator norm, Frobenius norm or Entry-wise `1 norm, then we refer to C,U,R, T as an approximate CUR decomposition of A, and sometimes just refer to this as a (face-based) CURT decomposition of A."
    }, {
      "heading" : "B.3 Polynomial system verifier",
      "text" : "We use the polynomial system verifiers independently developed by Renegar [Ren92a, Ren92b] and Basu et al. [BPR96].\nTheorem B.10 (Decision Problem [Ren92a, Ren92b, BPR96]). Given a real polynomial system P (x1, x2, · · · , xv) having v variables and m polynomial constraints fi(x1, x2, · · · , xv)∆i0, ∀i ∈ [m], where ∆i is any of the “standard relations”: {>,≥,=, 6=,≤, <}, let d denote the maximum degree of all the polynomial constraints and let H denote the maximum bitsize of the coefficients of all the polynomial constraints. Then in\n(md)O(v) poly(H),\ntime one can determine if there exists a solution to the polynomial system P .\nRecently, this technique has been used to solve a number of low-rank approximation and matrix factorization problems [AGKM12, Moi13, CW15a, BDL16, RSW16, SWZ17].\nB.4 Lower bound on the cost of a polynomial system\nAn important result we use is the following lower bound on the minimum value attained by a polynomial restricted to a compact connected component of a basic closed semi-algebraic subset of Rv.\nTheorem B.11 ([JPT13]). Let T = {x ∈ Rv|f1(x) ≥ 0, · · · , f`(x) ≥ 0, f`+1(x) = 0, · · · , fm(x) = 0} be defined by polynomials f1, · · · , fm ∈ Z[x1, · · · , xv] with n ≥ 2, degrees bounded by an even integer d, and coefficients of absolute value at most H, and let C be a compact connected (in the topological sense) component of T . Let g ∈ Z[x1, · · · , xv] be a polynomial of degree at most d and coefficients of absolute value bounded by H. Then, the minimum value that g takes over C satisfies that if it is not zero, then its absolute value is greater than or equal to\n(24−v/2H̃dv)−v2 vdv ,\nwhere H̃ = max{H, 2v + 2m}.\nWhile the above theorem involves notions from topology, we shall apply it in an elementary way. Namely, in our setting T will be bounded and so every connected component, which is by definition closed, will also be bounded and therefore compact. As the connected components partition T the theorem will just be applied to give a global minimum value of g on T provided that it is non-zero."
    }, {
      "heading" : "B.5 Frobenius norm and `2 relaxation",
      "text" : "Theorem B.12 (Generalized rank-constrained matrix approximations, Theorem 2 in [FT07]). Given matrices A ∈ Rn×d, B ∈ Rn×p, and C ∈ Rq×d, let the SVD of B be B = UBΣBV >B and the SVD of C be C = UCΣCV >C . Then,\nB†(UBU > BAVCC > C )kC † = arg min rank−k X∈Rp×q ‖A−BXC‖F ,\nwhere (UBU>BAVCV > C )k ∈ Rp×q is of rank at most k and denotes the best rank-k approximation to UBU > BAVCV > C ∈ Rp×d in Frobenius norm.\nClaim B.13 (`2 relaxation of `p-regression). Let p ∈ [1, 2). For any A ∈ Rn×d and b ∈ Rn, define x∗ = arg min\nx∈Rd ‖Ax− b‖p and x′ = arg min x∈Rd ‖Ax− b‖2. Then,\n‖Ax∗ − b‖p ≤ ‖Ax′ − b‖p ≤ n1/p−1/2 · ‖Ax∗ − b‖p.\nClaim B.14 ((Matrix) Frobenius norm relaxation of `p-low rank approximation). Let p ∈ [1, 2) and for any matrix A ∈ Rn×d, define A∗ = arg min\nrank−k B∈Rn×d ‖B − A‖p and A′ = arg min rank−k B∈Rn×d ‖B − A‖F .\nThen\n‖A∗ −A‖p ≤ ‖A′ −A‖p ≤ (nd)1/p−1/2‖A∗ −A‖p.\nClaim B.15 ((Tensor) Frobenius norm relaxation of `p-low rank approximation). Let p ∈ [1, 2) and for any matrix A ∈ Rn1×n2×n3, define\nA∗ = arg min rank−k B∈Rn1×n2×n3 ‖B −A‖p\nand\nA′ = arg min rank−k B∈Rn1×n2×n3 ‖B −A‖F .\nThen\n‖A∗ −A‖p ≤ ‖A′ −A‖p ≤ (n1n2n3)1/p−1/2‖A∗ −A‖p."
    }, {
      "heading" : "B.6 CountSketch and Gaussian transforms",
      "text" : "Definition B.16 (Sparse embedding matrix or CountSketch transform). A CountSketch transform is defined to be Π = σ ·ΦD ∈ Rm×n. Here, σ is a scalar, D is an n×n random diagonal matrix with each diagonal entry independently chosen to be +1 or −1 with equal probability, and Φ ∈ {0, 1}m×n is an m × n binary matrix with Φh(i),i = 1 and all remaining entries 0, where h : [n] → [m] is a random map such that for each i ∈ [n], h(i) = j with probability 1/m for each j ∈ [m]. For any matrix A ∈ Rn×d, ΠA can be computed in O(nnz(A)) time. For any tensor A ∈ Rn×d1×d2 , ΠA can be computed in O(nnz(A)) time. Let Π1,Π2,Π3 denote three CountSktech transforms. For any tensor A ∈ Rn1×n2×n3 , A(Π1,Π2,Π3) can be computed in O(nnz(A)) time.\nIf the above scalar σ is not specified in the context, we assume the scalar σ to be 1.\nDefinition B.17 (Gaussian matrix or Gaussian transform). Let S = σ · G ∈ Rm×n where σ is a scalar, and each entry of G ∈ Rm×n is chosen independently from the standard Gaussian distribution. For any matrix A ∈ Rn×d, SA can be computed in O(m ·nnz(A)) time. For any tensor A ∈ Rn×d1×d2, SA can be computed in O(m · nnz(A)) time.\nIf the above scalar σ is not specified in the context, we assume the scalar σ to be 1/ √ m. In\nmost places, we can combine CountSketch and Gaussian transforms to achieve the following:\nDefinition B.18 (CountSketch + Gaussian transform). Let S′ = SΠ, where Π ∈ Rt×n is the CountSketch transform (defined in Definition B.16) and S ∈ Rm×t is the Gaussian transform (defined in Definition B.17). For any matrix A ∈ Rn×d, S′A can be computed in O(nnz(A) + dtmω−2) time, where ω is the matrix multiplication exponent.\nLemma B.19 (Affine Embedding - Theorem 39 in [CW13]). Given matrices A ∈ Rn×r, B ∈ Rn×d, and rank(A) = k, let m = poly(k/ ), S ∈ Rm×n be a sparse embedding matrix (Definition B.16) with scalar σ = 1. Then with probability at least 0.999, ∀X ∈ Rr×d, we have\n(1− ) · ‖AX −B‖2F ≤ ‖S(AX −B)‖2F ≤ (1 + )‖AX −B‖2F .\nLemma B.20 (see, e.g., Lemma 10 in version 1 of [BWZ16]8). Let m = Ω(k/ ), S = 1√ m · G, where G ∈ Rm×n is a random matrix where each entry is an i.i.d Gaussian N(0, 1). Then with probability at least 0.998, S satisfies (1 ± 1/8) Subspace Embedding (Definition B.1) for any fixed matrix C ∈ Rn×k, and it also satisfies O( √ /k) Approximate Matrix Product (Definition B.2) for any fixed matrix A and B which has the same number of rows.\nLemma B.21 (see, e.g., Lemma 11 in version 1 of [BWZ16]8). Let m = Ω(k2 + k/ ), Π ∈ Rm×n, where Π is a sparse embedding matrix (Definition B.16) with scalar σ = 1, then with probability at least 0.998, S satisfies (1±1/8) Subspace Embedding (Definition B.1) for any fixed matrix C ∈ Rn×k, and it also satisfies O( √ /k) Approximate Matrix Product (Definition B.2) for any fixed matrix A and B which has the same number of rows.\nLemma B.22 (see, e.g., Lemma 12 in version 1 of [BWZ16]8). Let m2 = Ω(k2 + k/ ), Π ∈ Rm2×n, where Π is a sparse embedding matrix (Definition B.16) with scalar σ = 1. Let m1 = Ω(k/ ), S = 1√m1 · G, where G ∈ R\nm1×m2 is a random matrix where each entry is an i.i.d Gaussian N(0, 1). Let S′ = SΠ. Then with probability at least 0.99, S′ is a (1 ± 1/3) Subspace Embedding (Definition B.1) for any fixed matrix C ∈ Rn×k, and it also satisfies O( √ /k) Approximate Matrix Product (Definition B.2) for any fixed matrix A and B which have the same number of rows.\nTheorem B.23 (Theorem 36 in [CW13]). Given A ∈ Rn×k, B ∈ Rn×d, suppose S ∈ Rm×n is such that S is a (1± 1√\n2 ) Subspace Embedding for A, and satisfies O(\n√ /k) Approximate Matrix Product\nfor matrices A and C where C with n rows, where C depends on A and B. If\nX̂ = arg min X∈Rk×d\n‖SAX − SB‖2F ,\nthen\n‖AX̂ −B‖2F ≤ (1 + ) min X∈Rk×d ‖AX −B‖2F .\nB.7 Cauchy and p-stable transforms\nDefinition B.24 (Dense Cauchy transform). Let S = σ · C ∈ Rm×n where σ is a scalar, and each entry of C ∈ Rm×n is chosen independently from the standard Cauchy distribution. For any matrix A ∈ Rn×d, SA can be computed in O(m · nnz(A)) time.\nDefinition B.25 (Sparse Cauchy transform). Let Π = σ · SC ∈ Rm×n, where σ is a scalar, S ∈ Rm×n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and C ∈ Rn×n is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution. For any matrix A ∈ Rn×d, ΠA can be computed in O(nnz(A)) time. For any tensor A ∈ Rn×d1×d2, ΠA can be computed in O(nnz(A)) time. Let Π1 ∈ Rm1×n1 ,Π2 ∈ Rm2×n2 ,Π3 ∈ Rm3×n3 denote three sparse Cauchy transforms. For any tensor A ∈ Rn1×n2×n3, A(Π1,Π2,Π3) ∈ Rm1×m2×m3 can be computed in O(nnz(A)) time.\n8 https://arxiv.org/pdf/1504.06729v1.pdf\nDefinition B.26 (Dense p-stable transform). Let p ∈ (1, 2). Let S = σ · C ∈ Rm×n, where σ is a scalar, and each entry of C ∈ Rm×n is chosen independently from the standard p-stable distribution. For any matrix A ∈ Rn×d, SA can be computed in O(mnnz(A)) time.\nDefinition B.27 (Sparse p-stable transform). Let p ∈ (1, 2). Let Π = σ ·SC ∈ Rm×n, where σ is a scalar, S ∈ Rm×n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and C ∈ Rn×n is a diagonal matrix with diagonals chosen independently from the standard p-stable distribution. For any matrix A ∈ Rn×d, ΠA can be computed in O(nnz(A)) time. For any tensor A ∈ Rn×d1×d2, ΠA can be computed in O(nnz(A)) time. Let Π1 ∈ Rm1×n1 ,Π2 ∈ Rm2×n2 ,Π3 ∈ Rm3×n3 denote three sparse p-stable transforms. For any tensor A ∈ Rn1×n2×n3, A(Π1,Π2,Π3) ∈ Rm1×m2×m3 can be computed in O(nnz(A)) time."
    }, {
      "heading" : "B.8 Leverage scores",
      "text" : "Definition B.28 (Leverage scores). Let U ∈ Rn×k have orthonormal columns, and let pi = u2i /k, where u2i = ‖e>i U‖22 is the i-th leverage score of U .\nDefinition B.29 (Leverage score sampling). Given A ∈ Rn×d with rank k, let U ∈ Rn×k be an orthonormal basis of the column space of A, and for each i let pi be the squared row norm of the i-th row of U , i.e., the i-th leverage score. Let k · pi denote the i-th leverage score of U scaled by k. Let β > 0 be a constant and q = (q1, · · · , qn) denote a distribution such that, for each i ∈ [n], qi ≥ βpi. Let s be a parameter. Construct an n × s sampling matrix B and an s × s rescaling matrix D as follows. Initially, B = 0n×s and D = 0s×s. For each column j of B, D, independently, and with replacement, pick a row index i ∈ [n] with probability qi, and set Bi,j = 1 and Dj,j = 1/√qis. We denote this procedure Leverage score sampling according to the matrix A."
    }, {
      "heading" : "B.9 Lewis weights",
      "text" : "We follow the exposition of Lewis weights from [CP15].\nDefinition B.30. For a matrix A, let ai denote the ith row of A, where ai(= (Ai)>) is a column vector. The statistical leverage score of a row ai is\nτi(A) def = a>i (A >A)−1ai = ‖(A>A)−1/2ai‖22.\nFor a matrix A and norm p, the `p Lewis weights w are the unique weights such that for each row i we have\nwi = τi(W 1/2−1/pA).\nor equivalently,\na>i (A >W 1−2/pA)−1ai = w 2/p i .\nLemma B.31 (Lemma 2.4 of [CP15] and Lemma 7 of [CLM+15]). Given a matrix A ∈ Rn×d, n ≥ d, for any constant C > 0, 4 > p ≥ 1, there is an algorithm which can compute C-approximate `p Lewis weights for every row i of A in O((nnz(A) + dω log d) log n) time, where ω < 2.373 is the matrix multiplication exponent[Str69, CW87, Wil12].\nLemma B.32 (Theorem 7.1 of [CP15]). Given matrix A ∈ Rn×d (n ≥ d) with `p (4 > p ≥ 1) Lewis weights w, for any set of sampling probabilities pi, ∑ i pi = N ,\npi ≥ f(d, p)wi,\nif S ∈ RN×n has each row chosen independently as the ith standard basis vector, multiplied by 1/p1/pi , with probability pi/N . Then, overall with probability at least 0.999,\n∀x ∈ Rd, 1 2 ‖Ax‖pp ≤ ‖SAx‖pp ≤ 2‖Ax‖pp.\nFurthermore, if p = 1, N = O(d log d). If 1 < p < 2, N = O(d log d log log d). If 2 ≤ p < 4, N = O(dp/2 log d).\nLemma B.33. Given matrix A ∈ Rn×d (n ≥ d), there is an algorithm to compute a diagonal matrix D = SS1 with N nonzero entries in O(n poly(d)) time such that, with probability at least 0.999, for all x ∈ Rd\n1\n10 ‖DAx‖pp ≤ ‖Ax‖pp ≤ 10‖DAx‖pp,\nwhere S, S1 are two sampling/rescaling matrices. Furthermore, if p = 1, then N = O(d log d). If 1 < p < 2, then N = O(d log d log log d). If 2 ≤ p < 4, then N = O(dp/2 log d).\nGiven a matrix A ∈ Rn×d (n ≥ d), by Lemma B.32 and Lemma B.31, we can compute a sampling/rescaling matrix S in O((nnz(A) + dω log d) log n) time with Õ(d) nonzero entries such that\n∀x ∈ Rd, 1 2 ‖Ax‖pp ≤ ‖SAx‖pp ≤ 2‖Ax‖pp.\nSometimes, poly(d) is much smaller than log n. In this case, we are also able to compute such a sampling/rescaling matrix S in n poly(d) time in an alternative way.\nTo do so, we run one of the input sparsity `p embedding algorithms (see e.g., [MM13]) to compute a well conditioned basis U of the column span of A in n poly(d/ ) time. By sampling according to the well conditioned basis (see e.g. [Cla05, DDH+09, Woo14]), we can compute a sampling/rescaling matrix S1 such that (1 − )‖Ax‖pp ≤ ‖S1Ax‖pp ≤ (1 + )‖Ax‖pp where ∈ (0, 1) is an arbitrary constant. Notice that S1 has poly(d/ ) nonzero entries, and thus S1A has size poly(d/ ). Next, we apply Lewis weight sampling according to S1A, and we obtain a sampling/rescaling matrix S for which\n∀x ∈ Rd, (1− 1 3 )‖S1Ax‖pp ≤ ‖SS1Ax‖pp ≤ (1 + 1 3 )‖S1Ax‖pp.\nThis implies that\n∀x ∈ Rd, 1 2 ‖Ax‖pp ≤ ‖SS1Ax‖pp ≤ 2‖Ax‖pp.\nNote that SS1 is still a sampling/rescaling matrix according to A, and the number of non-zero entries is Õ(d). The total running time is thus n poly(d/ ), as desired.\nB.10 TensorSketch Let φ(v1, v2, · · · , vq) denote the function that maps q vectors(ui ∈ Rni) to the ∏q i=1 ni-dimensional vector formed by v1 ⊗ v2 ⊗ · · · ⊗ uq. We first give the definition of TensorSketch. Similar definitions can be found in previous work [Pag13, PP13, ANW14, WTSA15].\nDefinition B.34 (TensorSketch [Pag13]). Given q points v1, v2, · · · , vq where for each i ∈ [q], vi ∈ Rni , let m be the target dimension. The TensorSketch transform is specified using q 3-wise independent hash functions, h1, · · · , hq, where for each i ∈ [q], hi : [ni]→ [m], as well as q 4-wise independent sign functions s1, · · · , sq, where for each i ∈ [q], si : [ni]→ {−1,+1}.\nTensorSketch applied to v1, · · · , vq is then CountSketch applied to φ(v1, · · · , vq) with hash function H : [ ∏q i=1 ni]→ [m] and sign functions S : [ ∏q i=1 ni]→ {−1,+1} defined as follows:\nH(i1, · · · , iq) = h1(i1) + h2(s2) + · · ·+ hq(iq) (mod m),\nand\nS(i1, · · · , iq) = s1(i1) · s2(i2) · · · · · sq(iq).\nUsing the Fast Fourier Transform, TensorSketch(v1, · · · , vq) can be computed in O( ∑q\ni=1(nnz(vi)+ m logm)) time.\nNote that Theorem 1 in [ANW14] only defines φ(v) = v ⊗ v ⊗ · · · ⊗ v. Here we state a stronger version of Theorem 1 than in [ANW14], though the proofs are identical; a formal derivation can be found in [DW17]. Theorem B.35 (Generalized version of Theorem 1 in [ANW14]). Let S be the ( ∏q i=1 ni)×m matrix such that TensorSketch (v1, v2, · · · , vq) is φ(v1, v2, · · · , vq)S for a randomly selected TensorSketch. The matrix S satisfies the following two properties.\nProperty I (Approximate Matrix Product). Let A and B be matrices with ∏q i=1 ni rows. For\nm ≥ (2 + 3q)/( 2δ), we have\nPr[‖A>SS>B −A>B‖2F ≤ 2‖A‖2F ‖B‖2F ] ≥ 1− δ.\nProperty II (Subspace Embedding). Consider a fixed k-dimensional subspace V . If m ≥ k2(2 + 3q)/( 2δ), then with probability at least 1− δ, ‖xS‖2 = (1± )‖x‖2 simultaneously for all x ∈ V ."
    }, {
      "heading" : "C Frobenius Norm for Arbitrary Tensors",
      "text" : "Section C.1 presents a Frobenius norm tensor low-rank approximation algorithm with (1 + )- approximation ratio. Section C.2 introduces a tool which is able to reduce the size of the objective function from n3 to poly(k, 1/ ). Section C.3 introduces a new problem called tensor multiple regression. Section C.4 presents several bicriteria algorithms. Section C.5 introduces a powerful tool which we call generalized matrix row subset selection. Section C.6 presents an algorithm that is able to select a batch of columns, rows and tubes from a given tensor, and those samples are also able to form a low-rank solution. Section C.7 presents several useful tools for tensor problems, and also two (1 + )-approximation CURT decomposition algorithms: one has the optimal sample complexity, and the other has the optimal running time. Section C.9 shows how to solve the problem if the size of the objective function is small. Section C.10 extends several techniques from 3rd order tensors to general q-th order tensors, for any q ≥ 3. Finally, in Section C.11 we also provide a new matrix CUR decomposition algorithm, which is faster than [BW14].\nFor simplicity of presentation, we assume Ak exists in theorems (e.g., Theorem C.1) which concern outputting a rank-k solution, as well as the theorems (e.g., Theorem C.7, Theorem C.8, Theorem C.13) which concern outputting a bicriteria solution (the output rank is larger than k). For each of the bicriteria theorems, we can obtain a more detailed version when Ak does not exist, like Theorem 1.1 in Section 1 (by instead considering a tensor sufficiently close to Ak in objective function value). Note that the theorems for column, row, tube subset selection Theorem C.20 and Theorem C.21 also belong to this first category. In the second category, for each of the rank-k theorems we can obtain a more detailed version handling all cases, even when Ak does not exist, like Theorem 1.2 in Section 1 (by instead considering a tensor sufficiently close to Ak in objective function value).\nSeveral other tensor results or tools (e.g., Theorem C.4, Lemma C.3, Theorem C.40, Theorem C.41, Theorem C.14, Theorem C.46) that we build in this section do not belong to the above two categories. It means those results do not depend on whether Ak exists or not and whether OPT is zero or not.\nC.1 (1 + )-approximate low-rank approximation\nAlgorithm 2 Frobenius Norm Low-rank Approximation 1: procedure FLowRankApprox(A,n, k, ) . Theorem C.1 2: s1 ← s2 ← s3 ← O(k/ ). 3: Choose sketching matrices S1 ∈ Rn2×s1 , S2 ∈ Rn2×s2 , S3 ∈ Rn2×s3 . . Definition B.18 4: Compute AiSi,∀i ∈ [3]. 5: Y1, Y2, Y3, C ←FInputSparsityReduction(A,A1S1, A2S2, A3S3, n, s1, s2, s3, k, ). .\nAlgorithm 3 6: Create variables for Xi ∈ Rsi×k,∀i ∈ [3]. 7: Run polynomial system verifier for ‖(Y1X1)⊗ (Y2X2)⊗ (Y3X3)− C‖2F . 8: return A1S1X1, A2S2X2, and A3S3X3. 9: end procedure\nTheorem C.1. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, ∈ (0, 1), there exists an algorithm which takes O(nnz(A)) + n poly(k, 1/ ) + 2O(k2/ ) time and outputs three matrices\nU ∈ Rn×k, V ∈ Rn×k, W ∈ Rn×k such that ∥∥∥∥∥ k∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥ 2\nF\n≤ (1 + ) min rank−k Ak ‖Ak −A‖2F\nholds with probability 9/10.\nProof. Given any tensorA ∈ Rn1×n2×n3 , we define three matricesA1 ∈ Rn1×n2n3 , A2 ∈ Rn2×n3n1 , A3 ∈ Rn3×n1n2 such that, for any i ∈ [n1], j ∈ [n2], l ∈ [n3],\nAi,j,l = (A1)i,(j−1)·n3+l = (A2)j,(l−1)·n1+i = (A3)l,(i−1)·n2+j .\nWe define OPT as\nOPT = min rank−k A′\n‖A′ −A‖2F .\nSuppose the optimal Ak = U∗ ⊗ V ∗ ⊗ W ∗. We fix V ∗ ∈ Rn×k and W ∗ ∈ Rn×k. We use V ∗1 , V ∗ 2 , · · · , V ∗k to denote the columns of V ∗ and W ∗1 ,W ∗2 , · · · ,W ∗k to denote the columns of W ∗.\nWe consider the following optimization problem,\nmin U1,··· ,Uk∈Rn ∥∥∥∥∥ k∑\ni=1\nUi ⊗ V ∗i ⊗W ∗i −A ∥∥∥∥∥ 2\nF\n,\nwhich is equivalent to\nmin U1,··· ,Uk∈Rn ∥∥∥∥∥∥∥∥ [ U1 U2 · · · Uk ]   V ∗1 ⊗W ∗1 V ∗2 ⊗W ∗2 · · ·\nV ∗k ⊗W ∗k\n −A ∥∥∥∥∥∥∥∥ 2\nF\n.\nWe use matrix Z1 to denote   vec(V ∗1 ⊗W ∗1 ) vec(V ∗2 ⊗W ∗2 )\n· · · vec(V ∗k ⊗W ∗k )\n  ∈ Rk×n 2 and matrix U to denote [ U1 U2 · · · Uk ] .\nThen we can obtain the following equivalent objective function,\nmin U∈Rn×k\n‖UZ1 −A1‖2F .\nNotice that minU∈Rn×k ‖UZ1 −A1‖2F = OPT, since Ak = U∗Z1. Let S>1 ∈ Rs1×n\n2 be a sketching matrix defined in Definition B.18, where s1 = O(k/ ). We obtain the following optimization problem,\nmin U∈Rn×k\n‖UZ1S1 −A1S1‖2F .\nLet Û ∈ Rn×k denote the optimal solution to the above optimization problem. Then Û = A1S1(Z1S1) †. By Lemma B.22 and Theorem B.23, we have\n‖ÛZ1 −A1‖2F ≤ (1 + ) min U∈Rn×k ‖UZ1 −A1‖2F = (1 + ) OPT,\nwhich implies ∥∥∥∥∥ k∑\ni=1\nÛi ⊗ V ∗i ⊗W ∗i −A ∥∥∥∥∥ 2\nF\n≤ (1 + ) OPT .\nTo write down Û1, · · · , Ûk, we use the given matrix A1, and we create s1 × k variables for matrix (Z1S1)\n†. As our second step, we fix Û ∈ Rn×k and W ∗ ∈ Rn×k, and we convert tensor A into matrix A2.\nLet matrix Z2 denote   vec(Û1 ⊗W ∗1 ) vec(Û2 ⊗W ∗2 )\n· · · vec(Ûk ⊗W ∗k )\n . We consider the following objective function,\nmin V ∈Rn×k\n‖V Z2 −A2‖2F ,\nfor which the optimal cost is at most (1 + ) OPT. Let S>2 ∈ Rs2×n\n2 be a sketching matrix defined in Definition B.18, where s2 = O(k/ ). We sketch S2 on the right of the objective function to obtain the new objective function,\nmin V ∈Rn×k\n‖V Z2S2 −A2S2‖2F .\nLet V̂ ∈ Rn×k denote the optimal solution of the above problem. Then V̂ = A2S2(Z2S2)†. By Lemma B.22 and Theorem B.23, we have,\n‖V̂ Z2 −A2‖2F ≤ (1 + ) min V ∈Rn×k ‖V Z2 −A2‖2F ≤ (1 + )2 OPT,\nwhich implies ∥∥∥∥∥ k∑\ni=1\nÛi ⊗ V̂i ⊗W ∗i −A ∥∥∥∥∥ 2\nF\n≤ (1 + )2 OPT .\nTo write down V̂1, · · · , V̂k, we need to use the given matrix A2 ∈ Rn 2×n, and we need to create s2 × k variables for matrix (Z2S2)†. As our third step, we fix the matrices Û ∈ Rn×k and V̂ ∈ Rn×k. We convert tensor A ∈ Rn×n×n\ninto matrix A3 ∈ Rn2×n. Let matrix Z3 denote   vec(Û1 ⊗ V̂1) vec(Û2 ⊗ V̂2)\n· · · vec(Ûk ⊗ V̂k)\n . We consider the following objective\nfunction,\nmin W∈Rn×k\n‖WZ3 −A3‖2F ,\nwhich has optimal cost at most (1 + )2 OPT. Let S>3 ∈ Rs3×n\n2 be a sketching matrix defined in Definition B.18, where s3 = O(k/ ). We sketch S3 on the right of the objective function to obtain a new objective function,\nmin W∈Rn×k\n‖WZ3S3 −A3S3‖2F .\nLet Ŵ ∈ Rn×k denote the optimal solution of the above problem. Then Ŵ = A3S3(Z3S3)†. By Lemma B.22 and Theorem B.23, we have,\n‖ŴZ3 −A3‖2F ≤ (1 + ) min W∈Rn×k ‖WZ3 −A3‖2F ≤ (1 + )3 OPT .\nThus, we have\nmin X1,X2,X3 ∥∥∥∥∥ k∑\ni=1\n(A1S1X1)i ⊗ (A2S2X2)i ⊗ (A3S3X3)i −A ∥∥∥∥∥ 2\nF\n≤ (1 + )3 OPT .\nLet V1 = A1S1, V2 = A2S2, V3 = A3S3, we then apply Lemma C.3, and we obtain V̂1, V̂2, V̂3, C. We then apply Theorem C.45. Correctness follows by rescaling by a constant factor.\nRunning time. Due to Definition B.18, the running time of line 4 is O(nnz(A)) + n poly(k). The running time of line 5 is shown by Lemma C.3, and the running time of line 7 is shown by Theorem C.45.\nTheorem C.2. Suppose we are given a 3rd order n × n × n tensor A such that each entry can be written using nδ bits, where δ > 0 is a given, value which can be arbitrarily small (e.g., we could have nδ being O(log n)). Define OPT = infrank−k Ak‖Ak −A‖2F . For any k ≥ 1, and for any 0 < < 1, define nδ′ = O(nδ2O(k2/ )). (I) If OPT > 0, and there exists a rank-k Ak = U∗ ⊗ V ∗ ⊗W ∗ tensor, with size n×n×n, such that ‖Ak −A‖2F = OPT, and max(‖U∗‖F , ‖V ∗‖F , ‖W ∗‖F ) ≤ 2O(n\nδ′ ), then there exists an algorithm that takes (nnz(A)+n poly(k, 1/ )+2O(k2/ ))nδ time in the unit cost RAM model with word size O(log n) bits9 and outputs three n× k matrices U, V,W such that\n‖U ⊗ V ⊗W −A‖2F ≤ (1 + ) OPT (5)\nholds with probability 9/10, and each entry of each of U, V,W fits in nδ′ bits. (II) If OPT > 0, and Ak does not exist, and there exist three n×k matrices U ′, V ′,W ′ for which max(‖U ′‖F , ‖V ′‖F , ‖W ′‖F ) ≤ 2O(n δ′ ) and ‖U ′ ⊗ V ′ ⊗W ′ −A‖2F ≤ (1 + /2) OPT, then we can find U, V,W such that (5) holds. (III) If OPT = 0 and Ak does exist, and there exists a solution U∗, V ∗,W ∗ such that each entry can be written by nδ′ bits, then we can obtain (5). (IV) If OPT = 0, and there exist three n×k matrices U, V,W such that max(‖U‖F , ‖V ‖F , ‖W‖F ) ≤ 2O(nδ ′ ) and\n‖U ⊗ V ⊗W −A‖2F ≤ (1 + ) OPT +2−Ω(n δ′ ) = 2−Ω(n δ′ ), (6)\nthen we can output U, V,W such that (6) holds. Further if Ak exists, we can output a number Z for which OPT ≤ Z ≤ (1 + ) OPT. For all the cases above, the algorithm runs in the same time as (I) and succeeds with probability at least 9/10.\nProof. This follows by the discussion in Section 1, Theorem C.1 and Theorem C.45 in Section C.9. Part (I) Suppose δ > 0 and Ak = U∗⊗V ∗⊗W ∗ exists and each of ‖U∗‖F , ‖V ∗‖F , and ‖W ∗‖F is bounded by 2O(nδ ′ ). We assume the computation model is the unit cost RAM model with words of size O(log n) bits, and allow each number of the input tensor A to be written using nδ bits. For the 9The entries of A are assumed to fit in nδ words.\ncase when OPT is nonzero, using the proof of Theorem C.1 and Theorems C.45, B.11, there exists a lower bound on the cost OPT, which is at least 2−O(nδ)2O(k\n2/ ) . We can round each entry of matrices U∗, V ∗,W ∗ to be an integer expressed using O(nδ′) bits to obtain U ′, V ′,W ′. Using the triangle inequality and our lower bound on OPT, it follows that U ′, V ′,W ′ provide a (1 + )-approximation.\nThus, applying Theorem C.1 by fixing U ′, V ′,W ′ and using Theorem C.45 at the end, we can output three matrices U, V,W , where each entry can be written using nδ′ bits, so that we satisfy ‖U ⊗ V ⊗W −A‖2F ≤ (1 + ) OPT.\nFor the running time, since each entry of the input is bounded by nδ bits, due to Theorem C.1, we need (nnz(A) + n poly(k/ )) · nδ time to reduce the size of the problem to poly(k/ ) size (with each number represented using O(nδ) bits). According to Theorem C.45, the running time of using a polynomial system verifier to get the solution is 2O(k2/ )nO(δ′) = 2O(k2/ )nO(δ) time. Thus the total running time is (nnz(A) + n poly(k/ ))nδ + 2O(k2/ ) · nO(δ).\nPart (II) is similar to Part (I). Part (III) is trivial to prove since there exists a solution which can be written down in the bit model, so we obtain a (1 + )-approximation. Part (IV) is also very similar to Part (II).\nC.2 Input sparsity reduction\nAlgorithm 3 Reducing the Size of the Objective Function from poly(n) to poly(k)\n1: procedure FInputSparsityReduction(A, V1, V2, V3, n, b1, b2, b3, k, ) . Lemma C.3 2: c1 ← c2 ← c3 ← poly(k, 1/ ). 3: Choose sparse embedding matrices T1 ∈ Rc1×n, T2 ∈ Rc2×n, T3 ∈ Rc3×n. . Definition B.16 4: V̂i ← TiVi ∈ Rci×bi , ∀i ∈ [3]. 5: C ← A(T1, T2, T3) ∈ Rc1×c2×c3 . 6: return V̂1, V̂2, V̂3 and C. 7: end procedure\nLemma C.3. Let poly(k, 1/ ) ≥ b1b2b3 ≥ k. Given a tensor A ∈ Rn×n×n and three matrices V1 ∈ Rn×b1, V2 ∈ Rn×b2, and V3 ∈ Rn×b3, there exists an algorithm that takes O(nnz(A)+nnz(V1)+ nnz(V2) + nnz(V3)) = O(nnz(A) + n poly(k/ )) time and outputs a tensor C ∈ Rc1×c2×c3 and three matrices V̂1 ∈ Rc1×b1, V̂2 ∈ Rc2×b2 and V̂3 ∈ Rc3×b3 with c1 = c2 = c3 = poly(k, 1/ ), such that with probability at least 0.99, for all α > 0, X1, X ′1 ∈ Rb1×k, X2, X ′2 ∈ Rb2×k, X3, X ′3 ∈ Rb3×k satisfy that,\n∥∥∥∥∥ k∑\ni=1\n(V̂1X ′ 1)i ⊗ (V̂2X ′2)i ⊗ (V̂3X ′3)i − C ∥∥∥∥∥ 2\nF\n≤ α ∥∥∥∥∥ k∑\ni=1\n(V̂1X1)i ⊗ (V̂2X2)i ⊗ (V̂3X3)i − C ∥∥∥∥∥ 2\nF\n,\nthen, ∥∥∥∥∥ k∑\ni=1\n(V1X ′ 1)i ⊗ (V2X ′2)i ⊗ (V3X ′3)i −A ∥∥∥∥∥ 2\nF\n≤ (1 + )α ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ 2\nF\n.\nProof. Let X1 ∈ Rb1×k, X2 ∈ Rb2×k, X3 ∈ Rb3×k. First, we define Z1 = ((V2X2)> (V3X3)>) ∈ Rk×n2 . (Note that, for each i ∈ [k], the i-th row of matrix Z1 is vec((V2X2)i ⊗ (V3X3)i).) Then, by\nflattening we have ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ 2\nF\n= ‖V1X1 · Z1 −A1‖2F .\nWe choose a sparse embedding matrix (Definition B.16) T1 ∈ Rc1×n with c1 = poly(k, 1/ ) rows. Since V1 has b1 ≤ poly(k/ ) columns, according to Lemma B.19 with probability 0.999, for all X1 ∈ Rb1×k, Z ∈ Rk×n2 ,\n(1− )‖V1X1Z −A1‖2F ≤ ‖T1V1X1Z − T1A1‖2F ≤ (1 + )‖V1X1Z −A1‖2F .\nTherefore, we have\n‖T1V1X1 · Z1 − T1A1‖2F = (1± ) ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ 2\nF\n.\nSecond, we unflatten matrix T1A1 ∈ Rc1×n2 to obtain a tensor A′ ∈ Rc1×n×n. Then we flatten A′ along the second direction to obtain A2 ∈ Rn×c1n. We define Z2 = (T1V1X1)> (V3X3)> ∈ Rk×c1n. Then, by flattening,\n‖V2X2 · Z2 −A2‖2F = ‖T1V1X1 · Z1 − T1A1‖2F\n= (1± ) ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ 2\nF\n.\nWe choose a sparse embedding matrix (Definition B.16) T2 ∈ Rc2×n with c2 = poly(k, 1/ ) rows. Then according to Lemma B.19 with probability 0.999, for all X2 ∈ Rb2×k, Z ∈ Rk×c1n,\n(1− )‖V2X2Z −A2‖2F ≤ ‖T2V2X2Z − T2A2‖2F ≤ (1 + )‖V2X2Z −A2‖2F .\nTherefore, we have\n‖T2V2X2 · Z2 − T2A2‖2F = (1± )‖V2X2 · Z2 −A2‖2F\n= (1± )2 ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ 2\nF\n.\nThird, we unflatten matrix T2A2 ∈ Rc2×c1n to obtain a tensor A′′(= A(T1, T2, I)) ∈ Rc1×c2×n. Then we flatten tensor A′′ along the last direction (the third direction) to obtain matrix A3 ∈ Rn×c1c2 . We define Z3 = (T1V1X1)> (T2V2X2)> ∈ Rk×c1c2 . Then, by flattening, we have\n‖V3X3 · Z3 −A3‖2F = ‖T2V2X2 · Z2 − T2A2‖2F\n= (1± )2 ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ 2\nF\n.\nWe choose a sparse embedding matrix (Definition B.16) T3 ∈ Rc3×n with c3 = poly(k, 1/ ) rows. Then according to Lemma B.19 with probability 0.999, for all X3 ∈ Rb3×k, Z ∈ Rk×c1c2 ,\n(1− )‖V3X3Z −A3‖2F ≤ ‖T3V3X3Z − T3A3‖2F ≤ (1 + )‖V3X3Z −A3‖2F .\nTherefore, we have\n‖T3V3X3 · Z3 − T3A3‖2F = (1± )3 ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ 2\nF\n.\nNote that\n‖T3V3X3 · Z3 − T3A3‖2F = ∥∥∥∥∥ k∑\ni=1\n(T1V1X1)i ⊗ (T2V2X2)i ⊗ (T3V3X3)i −A(T1, T2, T3) ∥∥∥∥∥ 2\nF\n,\nand thus, we have ∀X1 ∈ Rb1×k, X2 ∈ Rb2×k, X3 ∈ Rb3×k ∥∥∥∥∥ k∑\ni=1\n(T1V1X1)i ⊗ (T2V2X2)i ⊗ (T3V3X3)i −A(T1, T2, T3) ∥∥∥∥∥ 2\nF\n=(1± )3 ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ 2\nF\n.\nLet V̂i denote TiVi, for each i ∈ [3]. Let C ∈ Rc1×c2×c3 denote A(T1, T2, T3). For α > 1, if ∥∥∥∥∥ k∑\ni=1\n(V̂1X ′ 1)i ⊗ (V̂2X ′2)i ⊗ (V̂3X ′3)i − C ∥∥∥∥∥ 2\nF\n≤ α ∥∥∥∥∥ k∑\ni=1\n(V̂1X1)i ⊗ (V̂2X2)i ⊗ (V̂3X3)i − C ∥∥∥∥∥ 2\nF\n,\nthen ∥∥∥∥∥ k∑\ni=1\n(V1X ′ 1)i ⊗ (V2X ′2)i ⊗ (V3X ′3)i − C ∥∥∥∥∥ 2\nF\n≤ 1 (1− )3 ∥∥∥∥∥ k∑\ni=1\n(V̂1X ′ 1)i ⊗ (V̂2X ′2)i ⊗ (V̂3X ′3)i − C ∥∥∥∥∥ 2\nF\n≤ 1 (1− )3α ∥∥∥∥∥ k∑\ni=1\n(V̂1X1)i ⊗ (V̂2X2)i ⊗ (V̂3X3)i − C ∥∥∥∥∥ 2\nF\n≤ (1 + ) 3 (1− )3α ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i − C ∥∥∥∥∥ 2\nF\nBy rescaling by a constant, we complete the proof of correctness.\nRunning time. According to Section B.6, for each i ∈ [3], TiVi can be computed in O(nnz(Vi)) time, and A(T1, T2, T3) can be computed in O(nnz(A)) time.\nBy the analysis above, the proof is complete."
    }, {
      "heading" : "C.3 Tensor multiple regression",
      "text" : "Theorem C.4. Given matrices A ∈ Rd×n2, U, V ∈ Rn×k, let B ∈ Rk×n2 denote U> V >. There exists an algorithm that takes O(nnz(A) + nnz(U) + nnz(V ) + dpoly(k, 1/ )) time and outputs a matrix W ′ ∈ Rd×k such that,\n‖W ′B −A‖2F ≤ (1 + ) min W∈Rd×k ‖WB −A‖2F .\nAlgorithm 4 Frobenius Norm Tensor Multiple Regression 1: procedure FTensorMultipleRegression(A,U, V, d, n, k) . Theorem C.4 2: s← O(k2 + k/ ). 3: Choose S ∈ Rn2×s to be a TensorSketch. . Definition B.34 4: Compute A · S. 5: Compute B · S. . B = U> V > 6: W ← (AS)(BS)† 7: return W . 8: end procedure\nProof. We choose a TensorSketch (Definition B.34) S ∈ Rn2×s to reduce the problem to a smaller problem,\nmin W∈Rd×k\n‖WBS −AS‖2F .\nLet W ′ denote the optimal solution to the above problem. Following a similar proof to that in Section C.7.3, if S is a (1±1/2)-subspace embedding and satisfies √ /k-approximate matrix product, then W ′ provides a (1 + )-approximation to the original problem. By Theorem B.35, we have s = O(k2 + k/ ).\nRunning time. According to Definition B.34, BS can be computed in O(nnz(U) + nnz(V )) + poly(k/ ) time. Notice that each row of S has exactly 1 nonzero entry, thus AS can be computed in O(nnz(A)) time. Since BS ∈ Rk×s and AS ∈ Rd×s, minW∈Rd×k ‖WBS −AS‖2F can be solved in dpoly(sk) = d poly(k/ ) time."
    }, {
      "heading" : "C.4 Bicriteria algorithms",
      "text" : "C.4.1 Solving a small regression problem\nLemma C.5. Given tensor A ∈ Rn×n×n and three matrices U ∈ Rn×s1 , V ∈ Rn×s2 and W ∈ Rn×s3 , there exists an algorithm that takes O(nnz(A) + n poly(s1, s2, s3, 1/ )) time and outputs α′ ∈ Rs1×s2×s3 such that ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nα′i,j,l · Ui ⊗ Vj ⊗Wl −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + ) min α∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nαi,j,l · Ui ⊗ Vj ⊗Wl −A ∥∥∥∥∥∥ 2\nF\n.\nholds with probability at least .99.\nProof. We define b̃ ∈ Rn3 to be the vector where the i+(j−1)n+(l−1)n2-th entry of b̃ is Ai,j,l. We define Ã ∈ Rn3×s1s2s3 to be the matrix where the (i+(j−1)n+(l−1)n2, i′+(j′−1)s2 +(l′−1)s2s3) entry is Ui′,i · Vj′,j ·Wl′,l. This problem is equivalent to a linear regression problem,\nmin x∈Rs1s2s3\n‖Ãx− b̃‖22,\nwhere Ã ∈ Rn3×s1s2s3 , b̃ ∈ Rn3 . Thus, it can be solved fairly quickly using recent work [CW13, MM13, NN13]. However, the running time of this naïvely is Ω(n3), since we have to write down each entry of Ã. In the next few paragraphs, we show how to improve the running time to nnz(A)+ n poly(s1, s2, s3).\nSince α ∈ Rs1×s2×s3 , α can be always written as α = X1⊗X2⊗X3, where X1 ∈ Rs1×s1s2s3 , X2 ∈ Rs2×s1s2s3 , X3 ∈ Rs3×s1s2s3 , we have\nmin α∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nαi,j,l · Ui ⊗ Vj ⊗Wl −A ∥∥∥∥∥∥ 2\nF = min X1∈Rs1×s1s2s3 X2∈Rs2×s1s2s3 X3∈Rs3×s1s2s3\n‖(UX1)⊗ (V X2)⊗ (WX3)−A‖2F .\nBy Lemma C.3, we can reduce the problem size n × n × n to a smaller problem that has size t1 × t2 × t3,\nmin X1,X2,X3 ∥∥∥∥∥ s1s2s3∑\ni=1\n(T1UX1)i ⊗ (T2V X2)i ⊗ (T3WX3)i −A(T1, T2, T3) ∥∥∥∥∥ 2\nF\nwhere T1 ∈ Rt1×n, T2 ∈ Rt2×n, T3 ∈ Rt3×n, t1 = t2 = t3 = poly(s1s2s3/ ). Notice that\nmin X1,X2,X3 ∥∥∥∥∥ s1s2s3∑\ni=1\n(T1UX1)i ⊗ (T2V X2)i ⊗ (T3WX3)i −A(T1, T2, T3) ∥∥∥∥∥ 2\nF\n= min α∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nαi,j,l · (T1U)i ⊗ (T2V )j ⊗ (T3W )l −A(T1, T2, T3) ∥∥∥∥∥∥ 2\nF\n.\nLet\nα′ = arg min α∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nαi,j,l · (T1U)i ⊗ (T2V )j ⊗ (T3W )l −A(T1, T2, T3) ∥∥∥∥∥∥ 2\nF\n,\nthen we have ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nα′i,j,l · Ui ⊗ Vj ⊗Wl −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + ) min α∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nαi,j,l · Ui ⊗ Vj ⊗Wl −A ∥∥∥∥∥∥ 2\nF\n.\nAgain, according to Lemma C.3, the total running time is then O(nnz(A) + n poly(s1, s2, s3, 1/ )).\nLemma C.6. Given tensor A ∈ Rn×n×n, and two matrices U ∈ Rn×s, V ∈ Rn×s with rank(U) = r1, rank(V ) = r2, let T1 ∈ Rt1×n, T2 ∈ Rt2×n be two sparse embedding matrices (Definition B.16) with t1 = poly(r1/ ), t2 = poly(r2/ ). Then with probability at least 0.99, ∀X ∈ Rn×s,\n(1− )‖U ⊗ V ⊗X −A‖2F ≤ ‖T1U ⊗ T2V ⊗X −A(T1, T2, I)‖2F ≤ (1 + )‖U ⊗ V ⊗X −A‖2F .\nProof. Let X ∈ Rn×s. We define Z1 = (V > X>) ∈ Rs×n2 . We choose a sparse embedding matrix (Definition B.16) T1 ∈ Rt1×n with t1 = poly(r1/ ) rows. According to Lemma B.19 with probability 0.999, for all Z ∈ Rs×n2 ,\n(1− )‖UZ −A1‖2F ≤ ‖T1UZ − T1A1‖2F ≤ (1 + )‖T1UZ −A1‖2F .\nIt means that\n(1− )‖UZ1 −A1‖2F ≤ ‖T1UZ1 − T1A1‖2F ≤ (1 + )‖T1UZ1 −A1‖2F .\nSecond, we unflatten matrix T1A1 ∈ Rt1×n2 to obtain a tensor A′ ∈ Rt1×n×n. Then we flatten A′ along the second direction to obtain A′2 ∈ Rn×t1n. We define Z2 = ((T1U)> X>) ∈ Rs×t1n. Then, by flattening,\n‖V · Z2 −A′2‖2F = ‖T1U · Z1 − T1A1‖2F = (1± )‖U ⊗ V ⊗X −A‖2F .\nWe choose a sparse embedding matrix (Definition B.16) T2 ∈ Rt2×n with t2 = poly(r2/ ) rows. Then according to Lemma B.19 with probability 0.999, for all Z ∈ Rs×t1n,\n(1− )‖V Z −A′2‖2F ≤ ‖T2V Z − T2A′2‖2F ≤ (1 + )‖V Z −A′2‖2F .\nThus,\n‖T2V · Z2 − T2A′2‖2F = (1± )2‖U ⊗ V ⊗X −A‖2F .\nAfter rescaling by a constant, with probability at least 0.99, ∀X ∈ Rn×s,\n(1− )‖U ⊗ V ⊗X −A‖2F ≤ ‖T1U ⊗ T2V ⊗X −A(T1, T2, I)‖2F ≤ (1 + )‖U ⊗ V ⊗X −A‖2F ."
    }, {
      "heading" : "C.4.2 Algorithm I",
      "text" : "We start with a slightly unoptimized bicriteria low rank approximation algorithm.\nAlgorithm 5 Frobenius Norm Bicriteria Low Rank Approximation Algorithm, rank-O(k3/ 3)\n1: procedure FTensorLowRankBicriteriaCubicRank(A,n, k) . Theorem C.7 2: s1 ← s2 ← s3 ← O(k/ ). 3: t1 ← t2 ← t3 ← poly(k/ ). 4: Choose Si ∈ Rn2×si to be a Sketching matrix, ∀i ∈ [3]. . Definition B.18 5: Choose Ti ∈ Rti×n to be a Sketching matrix, ∀i ∈ [3]. . Definition B.16 6: Compute U ← T1 · (A1 · S1), V ← T2 · (A2 · S2), W ← T3 · (A3 · S3). 7: Compute C ← A(T1, T2, T3). 8: X ←FTensorRegression(C,U, V,W, t1, s1, t2, s2, t3, s3). . Linear regression 9: return X(A1S1, A2S2, A3S3).\n10: end procedure\nTheorem C.7. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, ∈ (0, 1), let r = O(k3/ 3). There exists an algorithm that takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices U ∈ Rn×r, V ∈ Rn×r, W ∈ Rn×r such that\n∥∥∥∥∥ r∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥ 2\nF\n≤ (1 + ) min rank−k Ak ‖Ak −A‖2F\nholds with probability 9/10.\nProof. At the end of Theorem C.1, we need to run a polynomial system verifier. This is why we obtain exponential in k running time. Instead of running the polynomial system verifier, we can use Lemma C.5. This reduces the running time to be polynomial in all parameters: n, k, 1/ . However, the output tensor has rank (k/ )3 (Here we mean that we do not obtain a better decomposition than (k/ )3 components). According to Section B.6, for each i, AiSi can be computed in O(nnz(A)) + n poly(k/ ) time. Then Ti(AiSi) can be computed in n poly(k, 1/ ) time and A(T1, T2, T3) also can be computed in O(nnz(A)) time. The running time for the regression is poly(k/ ).\nNow we present an optimized bicriteria algorithm.\nAlgorithm 6 Frobenius Norm Low Rank Approximation Algorithm, rank-O(k2/ 2)\n1: procedure FTensorLowRankBicriteriaQuadraticRank(A,n, k) . Theorem C.8 2: s1 ← s2 ← O(k/ ). 3: Choose Si ∈ Rn2×si to be a sketching matrix, ∀i ∈ [3]. . Definition B.18 4: Compute A1 · S1, A2 · S2. 5: Form Û by using A1S1 according to Equation (9). 6: Form V̂ by using A2S2 according to Equation (10). 7: Ŵ ←FTensorMultipleRegression(A, Û , V̂ , n, n, s1s2). . Algorithm 4 8: return Û , V̂ , Ŵ . 9: end procedure 10: procedure FTensorLowRankBicriteriaQuadraticRank(A,n, k) . Theorem C.8 11: s1 ← s2 ← O(k/ ). 12: t1 ← t2 ← poly(k/ ). 13: Choose Si ∈ Rn2×si to be a Sketching matrix, ∀i ∈ [2]. . Definition B.18 14: Choose Ti ∈ Rti×n to be a Sketching matrix, ∀i ∈ [2]. . Definition B.16 15: Form Û by using A1S1 according to Equation (9). 16: Form V̂ by using A2S2 according to Equation (10). 17: Compute C ← A(T1, T2, I). . C ∈ Rt1×t2×n 18: Compute B ← (T1Û)> (T2V̂ )>. 19: Ŵ ← arg min\nX∈Rn×s1s2 ‖XB − C3‖2F .\n20: return Û , V̂ , Ŵ . 21: end procedure\nTheorem C.8. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, ∈ (0, 1), let r = O(k2/ 2). There exists an algorithm that takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices U ∈ Rn×r, V ∈ Rn×r, W ∈ Rn×r such that\n∥∥∥∥∥ r∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥ 2\nF\n≤ (1 + ) min rank−k Ak ‖Ak −A‖2F\nholds with probability 9/10.\nNote that there are two different ways to implement algorithm FTensorLowRankBicriteriaQuadraticRank. We present the proofs for both of them here.\nApproach I.\nProof. Let OPT = min rank−k Ak ‖Ak − A‖2F . According to Theorem C.1, we know that there exists a sketching matrix S3 ∈ Rn2×s3 where s3 = O(k/ ), such that\nmin X1∈Rs1×k,X2∈Rs2×k,X3∈Rs3×k ∥∥∥∥∥ k∑\nl=1\n(A1S1X1)l ⊗ (A2S2X2)l ⊗ (A3S3X3)l −A ∥∥∥∥∥ 2\nF\n≤ (1 + ) OPT\nNow we fix an l and we have:\n(A1S1X1)l ⊗ (A2S2X2)l ⊗ (A3S3X3)l\n=\n( s1∑\ni=1\n(A1S1)i(X1)i,l\n) ⊗   s2∑\nj=1\n(A2S2)j(X2)j,l  ⊗ (A3S3X3)l\n=\ns1∑\ni=1\ns2∑\nj=1\n(A1S1)i ⊗ (A2S2)j ⊗ (A3S3X3)l(X1)i,l(X2)j,l\nThus, we have\nmin X1,X2,X3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\n(A1S1)i ⊗ (A2S2)j ⊗ ( k∑\nl=1\n(A3S3X3)l(X1)i,l(X2)j,l\n) −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + ) OPT . (7)\nWe use matrices A1S1 ∈ Rn×s1 and A2S2 ∈ Rn×s2 to construct a matrix B ∈ Rs1s2×n2 in the following way: each row of B is the vector corresponding to the matrix generated by the ⊗ product between one column vector in A1S1 and the other column vector in A2S2, i.e.,\nBi+(j−1)s1 = vec((A1S1)i ⊗ (A2S2)j),∀i ∈ [s1], j ∈ [s2], (8)\nwhere (A1S1)i denotes the i-th column of A1S1 and (A2S2)j denote the j-th column of A2S2. We create matrix Û ∈ Rn×s1s2 by copying matrix A1S1 s2 times, i.e.,\nÛ = [ A1S1 A1S1 · · · A1S1 ] . (9)\nWe create matrix V̂ ∈ Rn×s1s2 by copying the i-th column of A2S2 a total of s1 times, into columns (i− 1)s1, · · · , is1 of V̂ , for each i ∈ [s2], i.e.,\nV̂ = [ (A2S2)1 · · · (A2S2)1 (A2S2)2 · · · (A2S2)2 · · · (A2S2)s2 · · · (A2S2)s2 ] . (10)\nThus, we can use Û and V̂ to represent B,\nB = (Û> V̂ >) ∈ Rs1s2×n2 .\nAccording to Equation (7), we have:\nmin W∈Rn×s1s2\n‖WB −A3‖2F ≤ (1 + ) OPT .\nNext, we want to find matrix W ∈ Rn×s1s2 by solving the following optimization problem,\nmin W∈Rn×s1s2\n‖WB −A3‖2F .\nNote that B has size s1s2 × n2. Naïvely writing down B already requires Ω(n2) time. In order to achieve nearly linear time in n, we cannot write down B. We choose S3 ∈ Rn1n2×s3 to be a TensorSketch (Definition B.34). In order to solve multiple regression, we need to set s3 = O((s1s2)\n2 + (s1s2)/ ). Let Ŵ denote the optimal solution to ‖WBS3 − A3S3‖2F . Then Ŵ = (A3S3)(BS3)\n†. Since each row of S3 has exactly 1 nonzero entry, A3S3 can be computed in O(nnz(A)) time. Since B = (Û> V̂ >), according to Definition B.34, BS3 can be computed in n poly(s1s2/ ) = n poly(k/ ) time. By Theorem C.4, we have\n‖ŴB −A3‖2F ≤ (1 + ) min W∈Rn×s1s2 ‖WB −A3‖2F .\nThus, we have\n‖Û ⊗ V̂ ⊗ Ŵ −A‖2F ≤ (1 + ) OPT .\nAccording to Definition B.18, A1S1, A2S2 can be computed in O(nnz(A)+poly(k/ )) time. Te total running time is thus O(nnz(A) + poly(k/ )).\nApproach II.\nProof. Let OPT = min rank−k Ak\n‖Ak −A‖2F . Choose sketching matrices (Definition B.18) S1 ∈ Rn 2×s1 ,\nS2 ∈ Rn2×s2 , S3 ∈ Rn2×s3 , and sketching matrices (Definition B.16) T1 ∈ Rt1×n and T2 ∈ Rt2×n with s1 = s2 = s3 = O(k/ ), t1 = t2 = poly(k/ ). We create matrix Û ∈ Rn×s1s2 by copying matrix A1S1 s2 times, i.e.,\nÛ = [ A1S1 A1S1 · · · A1S1 ] .\nWe create matrix V̂ ∈ Rn×s1s2 by copying the i-th column of A2S2 a total of s1 times, into columns (i− 1)s1, · · · , is1 of V̂ , for each i ∈ [s2], i.e.,\nV̂ = [ (A2S2)1 · · · (A2S2)1 (A2S2)2 · · · (A2S2)2 · · · (A2S2)s2 · · · (A2S2)s2 ] .\nAs we proved in Approach I, we have\nmin X∈Rn×s1s2\n‖Û ⊗ V̂ ⊗X −A‖2F ≤ (1 + ) OPT .\nLet B = ((T1Û)> (T2V̂ )>) ∈ Rs1s2×t1t2 , and flatten A(T1, T2, I) along the third direction to obtain C3 ∈ Rn×t1t2 . Let\nŴ = arg min X∈Rn×s1s2 ‖T1Û ⊗ T2V̂ ⊗X −A(T1, T2, I)‖2F = arg min X∈Rn×s1s2 ‖XB − C3‖2F .\nLet\nW ∗ = arg min X∈Rn×s1s2 ‖Û ⊗ V̂ ⊗X −A‖2F .\nAccording to Lemma C.6,\n‖Û ⊗ V̂ ⊗ Ŵ −A‖2F ≤ 1\n1− ‖T1Û ⊗ T2V̂ ⊗ Ŵ −A(T1, T2, I)‖ 2 F\n≤ 1 1− ‖T1Û ⊗ T2V̂ ⊗W ∗ −A(T1, T2, I)‖2F ≤1 + 1− ‖Û ⊗ V̂ ⊗W ∗ −A‖2F ≤(1 + ) 2\n1− OPT .\nAccording to Definition B.18, A1S1, A2S2 can be computed in O(nnz(A) + poly(k/ )) time. The total running time is thus O(nnz(A) + poly(k/ )). Since T1, T2 are sparse embedding matrices, T1Û , T2V̂ can be computed in O(nnz(A)+poly(k/ )) time. The total running time is in O(nnz(A)+ poly(k/ )).\nTheorem C.9. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1 and any 0 < < 1, if Ak exists then there is a randomized algorithm running in nnz(A) + n · poly(k/ ) time which outputs a rank-O(k2/ 2) tensor B for which ‖A − B‖2F ≤ (1 + )‖A − Ak‖2F . If Ak does not exist, then the algorithm outputs a rank-O(k2/ 2) tensor B for which ‖A−B‖2F ≤ (1 + ) OPT +γ, where γ is an arbitrarily small positive function of n. In both cases, the algorithm succeeds with probability at least 9/10.\nProof. If Ak exists, then the proof directly follows the proof of Theorem C.1 and Theorem C.8. If Ak does not exist, then for any γ > 0, there exist U∗ ∈ Rn×k, V ∗ ∈ Rn×k,W ∗ ∈ Rn×k such that\n‖U∗ ⊗ V ∗ ⊗W ∗ −A‖2F ≤ inf rank−k A′ ‖A−A′‖2F + 1 10 γ.\nThen we just regard U∗ ⊗ V ∗ ⊗W ∗ as the “best” rank k approximation to A, and follow the same argument as in the proof of Theorem C.1 and the proof of Theorem C.8. We can finally output a tensor B ∈ Rn×n×n with rank-O(k2/ 2) such that\n‖B −A‖2F ≤ (1 + )‖U∗ ⊗ V ∗ ⊗W ∗ −A‖2F\n≤ (1 + ) (\ninf rank−k A′\n‖A−A′‖2F + 1\n10 γ\n)\n≤ (1 + ) inf rank−k A′ ‖A−A′‖2F + γ\nwhere the first inequality follows by the proof of Theorem C.1 and the proof of theorem C.8. The second inequality follows by our choice of U∗, V ∗,W ∗. The third inequality follows since 1 + < 2 and γ > 0.\nC.4.3 poly(k)-approximation to multiple regression\nLemma C.10 ((1.4) and (1.9) in [RV09]). Let s ≥ k. Let U ∈ Rn×k denote a matrix that has orthonormal columns, and S ∈ Rs×n denote an i.i.d. N(0, 1/s) Gaussian matrix. Then SU is also an s×k i.i.d. Gaussian matrix with each entry draw from N(0, 1/s), and furthermore, we have with arbitrarily large constant probability,\nσmax(SU) = O(1) and σmin(SU) = Ω(1/ √ s).\nProof. Note that √ s− √ k − 1 = s−k−1√\ns+ √ k−1 = Ω(1/\n√ s).\nLemma C.11. Given matrices A ∈ Rn×k, B ∈ Rn×d, let S ∈ Rs×n denote a standard Gaussian N(0, 1) matrix with s = k. Let X∗ = min\nX∈Rk×d ‖AX − B‖F . Let X ′ = min X∈Rk×d ‖SAX − SB‖F . Then,\nwe have that\n‖AX ′ −B‖F ≤ O( √ k)‖AX∗ −B‖F ,\nholds with probability at least 0.99.\nProof. Let X∗ ∈ Rk×d denote the optimal solution such that\n‖AX∗ −B‖F = min X∈Rk×d ‖AX −B‖F .\nConsider a standard Gaussian matrix S ∈ Rk×n scaled by 1/ √ k with exactly k rows. Then for\nany X ∈ Rk×d, by the triangle inequality, we have\n‖SAX − SB‖F ≤ ‖SAX − SAX∗‖F + ‖SAX∗ − SB‖F ,\nand\n‖SAX − SB‖F ≥ ‖SAX − SAX∗‖F − ‖SAX∗ − SB‖F .\nWe first show how to bound ‖SAX − SAX∗‖F , and then show how to bound ‖SAX∗ − SB‖F . Note that Lemma C.10 implies the following result,\nClaim C.12. For any X ∈ Rk×d, with probability 0.999, we have 1√ k ‖AX −AX∗‖F . ‖SAX − SAX∗‖F . ‖AX −AX∗‖F .\nProof. First, we can write A = UR ∈ Rn×k where U ∈ Rn×k has orthonormal columns and R ∈ Rk×k. It gives,\n‖SAX − SAX∗‖F = ‖SU(RX −RX∗)‖F .\nSecond, applying Lemma C.10 to SU ∈ Rs×k completes the proof.\nUsing Markov’s inequality, for any fixed matrix AX∗ − B, choosing a Gaussian matrix S, we have that\n‖SAX∗ − SB‖2F = O(‖AX∗ −B‖2F )\nholds with probability at least 0.999. This is equivalent to\n‖SAX∗ − SB‖F = O(‖AX∗ −B‖F ), (11)\nholding with probability at least 0.999.\nLet X ′ = arg min X∈Rk×d ‖SAX − SB‖F . Putting it all together, we have\n‖AX ′ −B‖F ≤ ‖AX ′ −AX∗‖F + ‖AX∗ −B‖F by triangle inequality ≤ O( √ k)‖SAX ′ − SAX∗‖F + ‖AX∗ −B‖F by Claim C.12 ≤ O( √ k)‖SAX ′ − SB‖F +O( √ k)‖SAX∗ − SB‖F + ‖AX∗ −B‖F by triangle inequality ≤ O( √ k)‖SAX∗ − SB‖F +O( √ k)‖SAX∗ − SB‖F + ‖AX∗ −B‖F by definition of X ′ ≤ O( √ k)‖AX∗ −B‖F . by Equation (11)"
    }, {
      "heading" : "C.4.4 Algorithm II",
      "text" : "Theorem C.13. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, let r = k2. There exists an algorithm which takes O(nnz(A)k) + n poly(k) time and outputs three matrices U, V,W ∈ Rn×r such that,\n∥∥∥∥∥ r∑\ni=1 Ui ⊗ Vi ⊗Wi −A ∥∥∥∥∥ F ≤ poly(k) min rank−k A′ ‖A′ −A‖F\nholds with probability 9/10.\nProof. Let OPT = min rank−k A′ ‖A′−A‖F , we fix V ∗ ∈ Rn×k,W ∗ ∈ Rn×k to be the optimal solution of the original problem. We use Z1 = (V ∗> W ∗>) ∈ Rk×n2 to denote the matrix where the i-th row is the vectorization of V ∗i ⊗W ∗i . Let A1 ∈ Rn×n\n2 denote the matrix obtained by flattening tensor A ∈ Rn×n×n along the first direction. Then, we have\nmin U ‖UZ1 −A1‖F ≤ OPT .\nChoosing an N(0, 1/k) Gaussian sketching matrix S1 ∈ Rn2×s1 with s1 = k, we can obtain the smaller problem,\nmin U∈Rn×k\n‖UZ1S1 −A1S1‖F .\nDefine Û = A1S1(Z1S1)†. Define α = O( √ k). By Lemma C.11, we have\n‖ÛZ1 −A1‖F ≤ αOPT .\nSecond, we fix Û and W ∗. Define Z2, A2 similarly as above. Choosing an N(0, 1/k) Gaussian sketching matrix S2 ∈ Rn2×s2 with s2 = k, we can obtain another smaller problem,\nmin V ∈Rn×k\n‖V Z2S2 −A2S2‖F .\nDefine V̂ = A2S2(Z2S2)†. By Lemma C.11 again, we have\n‖V̂ Z2 −A2‖F ≤ α2 OPT .\nThus, we now have\nmin X1,X2,W\n‖A1S1X1 ⊗A2S2X2 ⊗W −A‖F ≤ α2 OPT\nWe use a similar idea as in the proof of Theorem C.8. We create matrix Ũ ∈ Rn×s1s2 by copying matrix A1S1 s2 times, i.e.,\nŨ = [ A1S1 A1S1 · · · A1S1 ] .\nWe create matrix Ṽ ∈ Rn×s1s2 by copying the i-th column of A2S2 a total of s1 times, into columns (i− 1)s1, · · · , is1 of Ṽ , for each i ∈ [s2], i.e.,\nṼ = [ (A2S2)1 · · · (A2S2)1 (A2S2)2 · · · (A2S2)2 · · · (A2S2)s2 · · · (A2S2)s2 ] .\nWe have\nmin X∈Rn×s1s2\n‖Ũ ⊗ Ṽ ⊗X −A‖F ≤ α2 OPT .\nChoose Ti ∈ Rti×n to be a sparse embedding matrix (Definition B.16) with ti = poly(k/ ), for each i ∈ [2]. By applying Lemma C.6, we have, if W ′ satisfies,\n‖T1Ũ ⊗ T2Ṽ ⊗W ′ −A(T1, T2, I)‖F = min X∈Rn×s1s2 ‖T1Ũ ⊗ T2Ṽ ⊗X −A(T1, T2, I)‖F\nthen,\n‖Ũ ⊗ Ṽ ⊗W ′ −A‖F ≤ (1 + ) min X∈Rn×s1s2 ‖Ũ ⊗ Ṽ ⊗X −A‖F ≤ (1 + )α2 OPT .\nThus, we only need to solve\nmin X∈Rn×s1s2\n‖T1Ũ ⊗ T2Ṽ ⊗X −A(T1, T2, I)‖F .\nwhich is similar to the proof of Theorem C.8. Therefore, we complete the proof of correctness. For the running time, A1S1, A2S2 can be computed in O(nnz(A)k) time, T1Ũ , T2Ṽ can be computed in n poly(k) time. The final regression problem can be computed in n poly(k) running time.\nC.5 Generalized matrix row subset selection\nNote that in this section, the notation ΠξC,k is given in Definition B.5.\nTheorem C.14. Given matrices A ∈ Rn×m and C ∈ Rn×k, there exists an algorithm which takes O(nnz(A) log n)+(m+n) poly(k, 1/ ) time and outputs a diagonal matrix D ∈ Rn×n with d = O(k/ ) nonzeros (or equivalently a matrix R that contains d = O(k/ ) rescaled rows of A) and a matrix U ∈ Rk×d such that\n‖CUDA−A‖2F ≤ (1 + ) min X∈Rk×m ‖CX −A‖2F\nholds with probability .99.\nAlgorithm 7 Generalized Matrix Row Subset Selection: Constructing R with r = O(k+k/ ) Rows and a rank-k U ∈ Rk×r 1: procedure GeneralizedMatrixRowSubsetSelection(A,C, n,m, k, ) . Theorem C.14 2: Y,Φ,∆← ApproxSubspaceSVD(A,C, k). . Claim C.16 and Lemma 3.12 in [BW14] 3: B ← Y∆. 4: Z2, D ← QR(B). . Z2 ∈ Rm×k, Z>2 Z2 = Ik, D ∈ Rk×k 5: h2 ← 8k ln(20k). 6: Ω2, D2 ← RandSampling(Z2, h2, 1) . Definition 3.6 in [BW14] 7: M2 ← Z>2 Ω2D2 ∈ Rk×h2 . 8: UM2 ,ΣM2 , V > M2 ← SVD(M2). . rank(M2) = k and VM2 ∈ Rh2×k\n9: r1 ← 4k. 10: S2 ← BSSSamplingSparse(VM2 , ((A> −A>Z2Z>2 )Ω2D2)>, r1, 0.5) . Lemma 4.3 in\n[BW14] 11: R1 ← (A>Ω2D2S2)> ∈ Rr1×n containing rescaled rows from A. 12: r2 ← 4820k/ . 13: R2 ← AdaptiveRowsSparse(A,Z2, R1, r2) . Lemma 4.5 in [BW14] 14: R← [R>1 , R>2 ]>. . R ∈ R(r1+r2)×n containing r = 4k + 4820k/ rescaled rows of A. 15: Choose W ∈ Rξ×m to be a randomly chosen sparse subspace embedding with ξ = Ω(k2 −2). 16: U ← Φ−1∆D−1(WCΦ−1∆D−1)†WAR† = Φ−1∆∆>(WC)†WAR†. 17: return R, U . 18: end procedure\nProof. This follows by combining Lemma C.17 and C.18. Let U,R denote the output of procedure GeneralizedMatrixRowSubsetSelection,\n‖A− CUR‖2F ≤ (1 + )‖A− Z2Z>2 AR†R‖2F ≤ (1 + )(1 + 60 )‖A−ΠFC,k(A)‖2F ≤ (1 + 130 )‖A−ΠFC,k(A)‖2F .\nBecause R is a subset of rows of A and R has size O(k/ )×m, there must exist a diagonal matrix D ∈ Rn×n with O(k/ ) nonzeros such that R = DA. This completes the proof.\nCorollary C.15 (A slightly different version of Theorem C.14, faster running time, and small input matrix). Given matrices A ∈ Rn×m and C ∈ Rn×k, if min(m,n) = poly(k, 1/ ), then there exists an algorithm which takes O(nnz(A)) + (m + n) poly(k, 1/ ) time and outputs a diagonal matrix D ∈ Rn×n with d = O(k/ ) nonzeros (or equivalently a matrix R that contains d = O(k/ ) rescaled rows of A) and a matrix U ∈ Rk×d such that\n‖CUDA−A‖2F ≤ (1 + ) min X∈Rk×m ‖CX −A‖2F\nholds with probability .99.\nProof. The log n factor comes from the adaptive sampling where we need to choose a Gaussian matrix with O(log n) rows and compute SA. If A has poly(k, 1/ ) columns, it is sufficient to choose S to be a CountSketch matrix with poly(k, 1/ ) rows. Then, we do not need a log n factor in the running time. If S has poly(k, 1/ ) rows, then we no longer need the matrix S.\nClaim C.16. Given matrices A ∈ Rm×n and C ∈ Rm×c, let Y ∈ Rm×c,Φ ∈ Rc×c and ∆ ∈ Rc×k denote the output of procedure ApproxSubspaceSVD(A,C, k, ). Then with probability .99, we have,\n‖A− Y∆∆>Y >A‖2F ≤ (1 + 30 )‖A−ΠFC,k(A)‖2F .\nProof. This follows by Lemma 3.12 in [BW14].\nLemma C.17. The matrices R and Z2 in procedure GeneralizedMatrixRowSubsetSelection (Algorithm 7) satisfy with probability at least 0.17− 2/n,\n‖A− Z2Z>2 AR†R‖2F ≤ ‖A−ΠFC,k(A)‖2F + 60 ‖A−ΠFC,k(A)‖2F .\nProof. We can show,\n‖A− Z2Z>2 A‖2F + 30 4820 ‖A−AR†1R1‖2F\n= ‖A−BB†A‖2F + 30 4820 ‖A−AR†1R1‖2F ≤ ‖A−BB†A‖2F + 30 ‖A−Ak‖2F ≤ ‖A− Y∆∆>Y A‖2F + 30 ‖A−ΠFC,k(A)‖2F ≤ (1 + 30 )‖A−ΠFC,k(A)‖2F + 30 ‖A−ΠFC,k(A)‖2F ,\nwhere the first step follows by the fact that Z2Z>2 = Z2DD−1Z>2 = (Z2D)(Z2D)† = BB†, the second step follows by ‖A − AR†1R1‖2F ≤ 4820‖A − Ak‖2F , the third step follows by B = Y∆ and B† = (Y∆)† = ∆†Y † = ∆>Y >, and the last step follows by Claim C.16.\nLemma C.18. The matrices C,U and R in procedure GeneralizedMatrixRowSubsetSelection (Algorithm 7) satisfy that\n‖A− CUR‖2F ≤ (1 + )‖A− Z2Z>2 AR†R‖2F\nwith probability at least .99.\nProof. Let UR,ΣR, VR denote the SVD of R. Then VRV >R = R †R.\nWe define Y ∗ to be the optimal solution of\nmin X∈Rk×r\n‖WAVRV >R −WCΦ−1∆D−1Y R‖2F .\nWe define X̂∗ to be Y ∗R ∈ Rk×n, which is also equivalent to defining X̂∗ to be the optimal solution of\nmin X∈Rk×n\n‖WAVRV >R −WCΦ−1∆D−1X‖2F .\nFurthermore, it implies X̂∗ = (WCΦ−1∆D−1)†WAVRV † R.\nWe also define X∗ to be the optimal solution of\nmin X∈Rk×n\n‖AVRV †R − CΦ−1∆D−1X‖2F ,\nwhich implies that,\nX∗ = (CΦ−1∆D−1)†AVRV > R = Z > 2 AVRV > R .\nNow, we start to prove an upper bound on ‖A− CUR‖2F ,\n‖A− CUR‖2F = ‖A− CΦ−1∆D−1Y ∗R‖2F by definition of U = ‖A− CΦ−1∆D−1X̂∗‖2F by X̂∗ = Y ∗R = ‖AVRV >R − CΦ−1∆D−1X̂∗ +A−AVRV >R ‖2F = ‖AVRV >R − CΦ−1∆D−1X̂∗‖2F︸ ︷︷ ︸\nα\n+ ‖A−AVRV >R ‖2F︸ ︷︷ ︸ β , (12)\nwhere the last step follows by X̂∗ = MV >R , A − AVRV >R = A(I − VRV >R ) and the Pythagorean theorem. We show how to upper bound the term α,\nα ≤ (1 + )‖AVRV >R − CΦ−1∆D−1X∗‖2F by Lemma C.19 = ‖AVRV >R − CΦ−1∆D−1X∗‖2F + ‖AVRV >R − CΦ−1∆D−1X∗‖2F = ‖AVRV >R − CΦ−1∆D−1X∗‖2F + ‖AVRV >R − CΦ−1∆D−1(Z>2 AR†R)‖2F . (13)\nBy the Pythagorean theorem and the definition of Z2 (which means Z2 = CΦ−1∆D−1), we have,\n‖AVRV >R − CΦ−1∆D−1(Z>2 AR†R)‖2F + β = ‖AVRV >R − CΦ−1∆D−1(Z>2 AR†R)‖2F + ‖A−AVRV >R ‖2F = ‖A− CΦ−1∆D−1(Z>2 AR†R)‖2F = ‖A− Z2Z>2 AR†R‖2F . (14)\nCombining Equations (12), (13) and (14) together, we obtain,\n‖A− CUR‖2F ≤ ‖AVRV >R − CΦ−1∆D−1X∗‖2F + ‖A− Z2Z>2 AR†R‖2F .\nWe want to show ‖AVRV >R − CΦ−1∆D−1X∗‖2F ≤ ‖A− Z2Z>2 AR†R‖2F ,\n‖AVRV >R − CΦ−1∆D−1X∗‖2F = ‖AVRV >R − CΦ−1∆D−1Z>2 AVRV >R ‖2F by X∗ = Z>2 AVRV >R ≤ ‖A− CΦ−1∆D−1Z>2 A‖2F by properties of projections ≤ ‖A− CΦ−1∆D−1Z>2 AR†R‖2F by properties of projections = ‖A− Z2Z>2 AR†R‖2F . by Z2 = CΦ−1∆D−1\nThis completes the proof.\nLemma C.19 ([CW13]). Let A ∈ Rn×d have rank ρ and B ∈ Rn×r. Let W ∈ Rr×n be a randomly chosen sparse subspace embedding with r = Ω(ρ2 −2). Let X̂∗ = arg min\nX∈Rd×r ‖WAX −WB‖2F and let"
    }, {
      "heading" : "X∗ = arg min",
      "text" : "X∈Rd×r ‖AX −B‖2F . Then with probability at least .99,\n‖AX̃∗ −B‖2F ≤ (1 + )‖AX∗ −B‖2F .\nAlgorithm 8 Frobenius Norm Tensor Column, Row and Tube Subset Selection, Polynomial Time 1: procedure FCRTSelection(A,n, k, ) . Theorem C.20 2: s1 ← s2 ← O(k/ ). 3: Choose a Gaussian matrix S1 with s1 columns. . Definition B.18 4: Choose a Gaussian matrix S2 with s2 columns. . Definition B.18 5: Form matrix Z ′3 by setting the (i, j)-th row to be the vectorization of (A1S1)i ⊗ (A2S2)j . 6: D3 ←GeneralizedMatrixRowSubsetSelection(A>3 , (Z ′3)>,n2,n,s1s2, ). . Algorithm\n7 7: Let d3 denote the number of nonzero entries in D3. . d3 = O(s1s2/ ) 8: Form matrix Z ′2 by setting the (i, j)-th row to be the vectorization of (A1S1)i ⊗ (A3S′3)j . 9: D2 ←GeneralizedMatrixRowSubsetSelection(A>2 , (Z ′2)>,n2,n,s1d3, ). 10: Let d2 denote the number of nonzero entries in D2. . d2 = O(s1d3/ ) 11: Form matrix Z ′1 by setting the (i, j)-th row to be the vectorization of (A2D2)i ⊗ (A3D3)j . 12: D1 ←GeneralizedMatrixRowSubsetSelection(A>1 , (Z ′1)>,n2,n,d2d3, ). 13: Let d1 denote the number of nonzero entries in D1. . d1 = O(d2d3/ ) 14: C ← A1D1, R← A2D2 and T ← A3D3. 15: return C, R and T . 16: end procedure\nC.6 Column, row, and tube subset selection, (1 + )-approximation\nTheorem C.20. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm which takes O(nnz(A)) log n + n2 poly(log n, k, 1/ ) time and outputs three matrices: C ∈ Rn×c, a subset of columns of A, R ∈ Rn×r a subset of rows of A, and T ∈ Rn×t, a subset of tubes of A where c = r = t = poly(k, 1/ ), and there exists a tensor U ∈ Rc×r×t such that\n‖(((U · T>)> ·R>)> · C>)> −A‖2F ≤ (1 + ) min rank−k Ak ‖Ak −A‖2F ,\nor equivalently, ∥∥∥∥∥∥ c∑\ni=1\nr∑\nj=1\nt∑\nl=1\nUi,j,l · Ci ⊗Rj ⊗ Tl −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + ) min rank−k Ak ‖Ak −A‖2F\nholds with probability 9/10.\nProof. We fix V ∗ ∈ Rn×k and W ∗ ∈ Rn×k. We define Z1 ∈ Rk×n2 where the i-th row of Z1 is the vector Vi ⊗Wi. Choose sketching (Gaussian) matrix S1 ∈ Rn2×s1 (Definition B.18), and let Û = A1S1(Z1S1) † ∈ Rn×k. Following a similar argument as in the previous theorem, we have\n‖ÛZ1 −A1‖2F ≤ (1 + ) OPT .\nWe fix Û and W ∗. We define Z2 ∈ Rk×n2 where the i-th row of Z2 is the vector Ûi ⊗W ∗i . Choose sketching (Gaussian) matrix S2 ∈ Rn2×s2 (Definition B.18), and let V̂ = A2S2(Z2S2)† ∈ Rn×k. Following a similar argument as in the previous theorem, we have\n‖V̂ Z2 −A2‖2F ≤ (1 + )2 OPT .\nWe fix Û and V̂ . Note that Û = A1S1(Z1S1)† and V̂ = A2S2(Z2S2)†. We define Z3 ∈ Rk×n2 such that the i-th row of Z3 is the vector Ûi ⊗ V̂i. Let z3 = s1 · s2. We define Z ′3 ∈ Rz3×n 2 such\nthat, ∀i ∈ [s1],∀j ∈ [s2], the i+ (j− 1)s1-th row of Z ′3 is the vector (A1S1)i⊗ (A2S2)j . We consider the following objective function,\nmin W∈Rn×k,X∈Rk×z3 ‖WXZ ′3 −A3‖2F ≤ min W∈Rn×k ‖WZ3 −A3‖2F ≤ (1 + )2 OPT .\nUsing Theorem C.14, we can find a diagonal matrix D3 ∈ Rn2×n2 with d3 = O(z3/ ) = O(k2/ 3) nonzero entries such that\nmin X∈Rd3×z3\n‖A3D3XZ ′3 −A3‖2F ≤ (1 + )3 OPT .\nIn the following, we abuse notation and let A3D3 ∈ Rn×d3 by deleting zero columns. Let W ′ denote A3D3 ∈ Rn×d3 . Then,\nmin X∈Rd3×z3\n‖W ′XZ ′3 −A3‖2F ≤ (1 + )3 OPT .\nWe fix Û and W ′. Let z2 = s1 · d3. We define Z ′2 ∈ Rz2×n 2 such that, ∀i ∈ [s1], ∀j ∈ [d3], the i+ (j − 1)s1-th row of Z ′2 is the vector (A1S1)i ⊗ (A3D3)j . Using Theorem C.14, we can find a diagonal matrix D2 ∈ Rn2×n2 with d2 = O(z2/ ) = O(s1d3/ ) = O(k 3/ 5) nonzero entries such that\nmin X∈Rd2×z2\n‖A2D2XZ ′2 −A2‖2F ≤ (1 + )4 OPT .\nLet V ′ denote A2D2. Then,\nmin X∈Rd2×z2\n‖V ′XZ ′2 −A2‖2F ≤ (1 + )4 OPT .\nWe fix V ′ and W ′. Let z1 = d2 · d3. We define Z ′1 ∈ Rz1×n 2 such that, ∀i ∈ [d2],∀j ∈ [d3], the i+ (j − 1)s1-th row of Z ′1 is the vector (A2D2)i ⊗ (A3D3)j . Using Theorem C.14, we can find a diagonal matrix D1 ∈ Rn2×n2 with d1 = O(z1/ ) = O(d2d3/ ) = O(k 5/ 9) nonzero entries such that\nmin X∈Rd1×z1\n‖A1D1XZ ′1 −A1‖2F ≤ (1 + )5 OPT .\nLet U ′ denote A1D1. Then,\nmin X∈Rd1×z1\n‖U ′XZ ′1 −A1‖2F ≤ (1 + )5 OPT .\nPutting U ′, V ′,W ′ all together, we complete the proof.\nTheorem C.21. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm which takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices: C ∈ Rn×c, a subset of columns of A, R ∈ Rn×r a subset of rows of A, and T ∈ Rn×t, a subset of tubes of A where c = r = t = poly(k, 1/ ), and there exists a tensor U ∈ Rc×r×t such that\n‖U(C,R, T )−A‖2F ≤ (1 + ) min rank−k Ak ‖Ak −A‖2F ,\nor equivalently, ∥∥∥∥∥∥ c∑\ni=1\nr∑\nj=1\nt∑\nl=1\nUi,j,l · Ci ⊗Rj ⊗ Tl −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + ) min rank−k Ak ‖Ak −A‖2F\nholds with probability 9/10.\nAlgorithm 9 Frobenius Norm Tensor Column, Row and Tube Subset Selection, Input Sparsity Time 1: procedure FCRTSelection(A,n, k, ) . Theorem C.21 2: s1 ← s2 ← O(k/ ). 3: 0 ← 0.001. 4: Choose a Gaussian matrix S1 with s1 columns. . Definition B.18 5: Choose a Gaussian matrix S2 with s2 columns. . Definition B.18 6: Form matrix B1 by setting (i, j)-th column to be (A1S1)i. 7: Form matrix B2 by setting (i, j)-th column to be (A2S2)j . . Z ′3 = B>1 B>2 8: d3 ← O(s1s2 log(s1s2) + (s1s2/ )). 9: D3 ←FastTensorLeverageScoreGeneralOrder(B>1 , B>2 , n, n, s1s2, 0, d1). .\nAlgorithm 15 10: Form matrix B1 by setting (i, j)-th column to be (A1S1)i. 11: Form matrix B3 by setting (i, j)-th column to be (A3D3)j . . Z ′2 = B>1 B>3 12: d2 ← O(s1d3 log(s1d3) + (s1d3/ )). 13: D2 ←FastTensorLeverageScoreGeneralOrder(B>1 , B>3 , n, n, s1d3, 0, d2). 14: Form matrix B2 by setting (i, j)-th column to be (A2D2)i. 15: Form matrix B3 by setting (i, j)-th column to be (A3D3)j . . Z ′1 = B>2 B>3 16: d1 ← O(d2d3 log(d2d3) + (d2d3/ )). 17: D1 ←FastTensorLeverageScoreGeneralOrder(B>2 , B>3 , n, n, d2d3, 0, d1). 18: C ← A1D1, R← A2D2 and T ← A3D3. 19: return C, R and T . 20: end procedure\nProof. The proof is similar to the proof of Theorem C.20, except we replace generalized matrix row subset by tensor leverage score sampling (implicitly).\nC.7 CURT decomposition, (1 + )-approximation\nC.7.1 Properties of leverage score sampling and BSS sampling\nNotice that, the BSS algorithm is a deterministic procedure developed in [BSS12] for selecting rows from a matrix A ∈ Rn×d (with ‖A‖2 ≤ 1 and ‖A‖2F ≤ k) using a selection matrix S so that\n‖A>S>SA−A>A‖2 ≤ .\nThe algorithm runs in poly(n, d, 1/ ) time. Using the ideas from [BW14] and [CEM+15], we are able to reduce the number of nonzero entries from O( −2k log k) to O( −2k), and also improve the running time to input sparsity.\nLemma C.22 (Leverage score preserves subspace embedding - Theorem 2.11 in [Woo14]). Given a rank-k matrix A ∈ Rn×d, via leverage score sampling, we can obtain a diagonal matrix D with m nonzero entries such that, letting B = DA, if m = O( −2k log k), then, with probability at least 0.999, for all x ∈ Rd,\n(1− )‖Ax‖2 ≤ ‖Bx‖2 ≤ (1 + )‖Ax‖2\nLemma C.23. Given a rank-k matrix A ∈ Rn×d, there exists an algorithm that runs in O(nnz(A)+ n poly(k, 1/ )) time and outputs a matrix B containing O( −2k log k) re-weighted rows of A, such\nthat with probability at least 0.999, for all x ∈ Rd,\n(1− )‖Ax‖2 ≤ ‖Bx‖2 ≤ (1 + )‖Ax‖2\nProof. We choose a sparse embedding matrix (Definition B.16) Π ∈ Rd×s with s = poly(k/ ). With probability at least 0.999, Π> is a subspace embedding of A>. Thus, rank(AΠ) = rank(A). Also, the leverage scores of AΠ are the same as those of A. Thus, we can compute the leverage scores of AΠ. The running time of computing AΠ is O(nnz(A)). Thus the total running time is O(nnz(A) + n poly(k, 1/ )).\nLemma C.24. Let B denote a matrix which contains O( −2k log k) rows of A ∈ Rn×d. Choosing Π to be a sparse subspace embedding matrix of size d×O( −6(k log k)2), with probability at least 0.999,\n‖BΠΠ>B> −BB>‖2 ≤ ‖B‖22.\nCombining Lemma C.23, C.24 and the BSS algorithm, we obtain:\nLemma C.25. Given a rank-k matrix A ∈ Rn×d, there exists an algorithm that runs in O(nnz(A)+ n poly(k, 1/ )) time and outputs a sampling and rescaling diagonal matrix S that selects O( −2k) re-weighted rows of A, such that, with probability at least 0.999,\n‖A>S>SA−A>A‖2 ≤ ‖A‖22.\nor equivalently, for all x ∈ Rd,\n(1− )‖Ax‖2 ≤ ‖SAx‖2 ≤ (1 + )‖Ax‖2.\nProof. Using Lemma C.23, we can obtain B. Then we apply a sparse subspace embedding matrix Π on the right of B. At the end, we run the BSS algorithm on BΠ and we are able to output O( −2k) re-weighted rows of BΠ. Using these rows, we are able to determine O( −2k) re-weighted rows of A.\nC.7.2 Row sampling for linear regression\nTheorem C.26 (Theorem 5 in [CNW15]). We are given A ∈ Rn×d with ‖A‖22 ≤ 1 and ‖A‖2F ≤ k, and an ∈ (0, 1). There exists a diagonal matrix S with O(k/ 2) nonzero entries such that\n‖(SA)>SA−A>A‖2 ≤ .\nCorollary C.27. Given a rank-k matrix A ∈ Rn×d, vector b ∈ Rn, and parameter > 0, let U ∈ Rn×(k+1) denote an orthonormal basis of [A, b]. Let S ∈ Rn×n denote a sampling and rescaling diagonal matrix according to Leverage score sampling and sparse BSS sampling of U with m nonzero entries. If m = O(k), then S is a (1±1/2) subspace embedding for U ; if m = O(k/ ), then S satisfies√ -operator norm approximate matrix product for U .\nProof. This follows by Lemma C.22, Lemma C.24 and Theorem C.26.\nLemma C.28 ([NW14]). Given A ∈ Rn×d and b ∈ Rn, let S ∈ Rn×n denote a sampling and rescaling diagonal matrix. Let x∗ denote arg minx ‖Ax− b‖22 and x′ denote arg minx ‖SAx− Sb‖22. If S is a (1 ± 1/2) subspace embedding for the column span of A, and ′ (=√ )-operator norm approximate matrix product for U adjoined with b−Ax∗, then, with probability at least .999,\n‖Ax′ − b‖22 ≤ (1 + )‖Ax∗ − b‖22.\nProof. We define OPT = min x ‖Ax−b‖2. We define x′ = arg min x ‖SAx−Sb‖22 and x∗ = arg min x ‖Ax− b‖22. Let w = b − Ax∗. Let U denote an orthonormal basis of A. We can write Ax′ − Ax∗ = Uβ. Then, we have,\n‖Ax′ − b‖22 = ‖Ax′ −Ax∗ +AA†b− b‖22 by x∗ = A†b = ‖Uβ + (UU> − I)b‖22 = ‖Ax∗ −Ax′‖22 + ‖Ax∗ − b‖22 by Pythagorean Theorem = ‖Uβ‖22 + OPT2\n= ‖β‖22 + OPT2 .\nIf S is a (1± 1/2) subspace embedding for U , then we can show\n‖β‖2 − ‖U>S>SUβ‖2 ≤ ‖β − U>S>SUβ‖2 by triangle inequality = ‖(I − U>S>SU)β‖2 ≤ ‖I − U>S>SU‖2 · ‖β‖2 ≤ 1\n2 ‖β‖2.\nThus, we obtain\n‖U>S>SUβ‖2 ≥ ‖β‖2/2.\nNext, we can show\n‖U>S>SUβ‖2 = ‖U>S>S(Ax′ −Ax∗)‖22 = ‖U>S>S(A(SA)†Sb−Ax∗)‖2 by x′ = (SA)†Sb = ‖U>S>S(b−Ax∗)‖2 by SA(SA)† = I = ‖U>S>Sw‖2. by w = b−Ax∗\nWe define U ′ = [ U w/‖w‖2 ] . We define X and y to satisfy U = U ′X and w = U ′y. Then, we have\n‖U>S>Sw‖2 = ‖U>S>Sw − U>w‖2 by U>w = 0 = ‖X>U ′>S>SU ′y −X>U ′>U ′y‖2 = ‖X>(U ′>S>SU ′ − I)y‖2 ≤ ‖X‖2 · ‖U ′>S>SU ′ − I‖2 · ‖y‖2 ≤ ′‖X‖2‖y‖2 = ′‖U‖2‖w‖2 = ′OPT, by ‖U‖2 = 1 and ‖w‖2 = OPT\nwhere the fifth inequality follows since S satisfies ′-operator norm approximate matrix product for the column span of U adjoined with w.\nPutting it all together, we have\n‖Ax′ − b‖22 = ‖Ax∗ − b‖22 + ‖Ax∗ −Ax′‖22 = OPT2 +‖β‖22 ≤ OPT2 +4‖U>S>Sw‖22 ≤ OPT2 +4( ′OPT)2\n≤ (1 + ) OPT2 . by ′ = 1 2\n√ .\nFinally, note that S satisfies ′-operator norm approximate matrix product for U adjoined with w if it is a (1± ′)-subspace embedding for U adjoined with w, which holds using BSS sampling by Theorem 5 of [CNW15] with O(d/ ) samples.\nC.7.3 Leverage scores for multiple regression\nLemma C.29 (see, e.g., Lemma 32 in [CW13] among other places). Given matrix A ∈ Rn×d with orthonormal columns, and parameter > 0, if S ∈ Rn×n is a sampling and rescaling diagonal matrix according to the leverage scores of A where the number of nonzero entries is t = O(1/ 2), then, for any B ∈ Rn×m, we have\n‖A>S>SB −A>B‖2F < 2‖A‖2F ‖B‖2F ,\nholds with probability at least 0.9999.\nCorollary C.30. Given matrix A ∈ Rn×d with orthonormal columns, and parameter > 0, if S ∈ Rn×n is a sampling and rescaling diagonal matrix according to the leverage scores of A with m nonzero entries, then if m = O(d log d), then S is a (1 ± 1/2) subspace embedding for A. If m = O(d/ ), then S satisfies √ /d-Frobenius norm approximate matrix product for A.\nProof. This follows by Lemma C.22 and Lemma C.29.\nLemma C.31 ([NW14]). Given A ∈ Rn×d and B ∈ Rn×m, let S ∈ Rn×n denote a sampling and rescaling matrix according to A. Let X∗ denote arg minX ‖AX−B‖2F and X ′ denote arg minX ‖SAX− SB‖2F . Let U denote an orthonormal basis for A. If S is a (1 ± 1/2) subspace embedding for U , and satisfies ′(= √ /d)-Frobenius norm approximate matrix product for U , then, we have that\n‖AX ′ −B‖2F ≤ (1 + )‖AX∗ −B‖2F\nholds with probability at least 0.999.\nProof. We define OPT = minX ‖AX − B‖F . Let A = UΣV > denote the SVD of A. Since A has rank k, U and V have k columns. We can write A(X ′ −X∗) = Uβ. Then, we have\n‖AX ′ −B‖2F = ‖AX ′ −AX∗ +AA†B −B‖2F by X∗ = A†B = ‖Uβ + (UU> − I)B‖2F = ‖AX∗ −AX ′‖2F + ‖AX∗ −B‖2F by Pythagorean Theorem = ‖Uβ‖2F + OPT2\n= ‖β‖2F + OPT2 . (15)\nIf S is a (1± 1/2) subspace embedding for U , then we can show,\n‖β‖F − ‖U>S>SSUβ‖F ≤ ‖β − U>S>SUβ‖F by triangle inequality = ‖(I − U>S>SU)β‖F ≤ ‖(I − U>S>SU)‖2 · ‖β‖F by ‖AB‖F ≤ ‖A‖2‖B‖F ≤ 1\n2 ‖β‖F . by ‖(I − U>S>SU)‖2 ≤ 1/2\nThus, we obtain\n‖U>S>SUβ‖F ≥ ‖β‖F /2. (16)\nNext, we can show\n‖U>S>SUβ‖F = ‖U>S>S(AX ′ −AX∗)‖F = ‖U>S>S(A(SA)†Sb−AX∗)‖F by X ′ = (SA)†SB = ‖U>S>S(B −AX∗)‖F . by SA(SA)† = I\nThen, we can show\n‖U>S>S(B −AX∗)‖F ≤ ′‖U>‖F ‖B −AX∗‖F by Lemma C.29 = ′ √ dOPT . by ‖U‖F = √ d and ‖B −AX∗‖F = OPT\n(17)\nPutting it all together, we have\n‖AX ′ −B‖2F = ‖AX∗ −B‖2F + ‖AX∗ −AX ′‖2F = OPT2 +‖β‖2F by Equation (15) ≤ OPT2 +4‖U>S>Sw‖2F by Equation (16) ≤ OPT2 +4( ′ √ dOPT)2 by Equation (17)\n≤ (1 + ) OPT2 . by ′ = 1 2\n√ /d\nC.7.4 Sampling columns according to leverage scores implicitly, improving polynomial running time to nearly linear running time\nThis section explains an algorithm that is able to sample from the leverage scores from the product of two matrices U, V without explicitly writing down U V . To build this algorithm we combine TensorSketch, some ideas from [DMIMW12] and some ideas from [AKO11, MW10]. Finally, we are able to improve the running time of sampling columns according to leverage scores from Ω(n2) to Õ(n). Given two matrices U, V ∈ Rk×n, we define A ∈ Rk×n1n2 to be the matrix where the i-th row of A is the vectorization of U i ⊗ V i, ∀i ∈ [k]. Naïvely, in order to sample O(poly(k, 1/ )) rows from A> according to leverage scores, we need to write down n2 leverage scores. This approach will take at least Ω(n2) running time. In the rest of this section, we will explain how to do it in O(n · poly(log n, k, 1/ )) time. In Section C.10.1, we will explain how to extend this idea from 3rd order tensors to general q-th order tensors and remove the poly(log n) factor from running time, i.e., obtain O(n · poly(k, 1/ )) time.\nAlgorithm 10 Fast Tensor Leverage Score Sampling 1: procedure FastTensorLeverageScore(U, V, n1, n2, k, , Rsamples) . Lemma C.32 2: s1 ← poly(k, 1/ ). 3: g1 ← g2 ← g3 ← O( −2 log(n1n2)). 4: Choose Π ∈ Rn1n2×s1 to be a TensorSketch. . Definition B.34 5: Compute R−1 ∈ Rk×k by using (U V )Π. . U ∈ Rk×n1 , V ∈ Rk×n2 6: Choose G1 ∈ Rg1×k to be a Gaussian sketching matrix. 7: for i = 1→ g1 do 8: w ← (GiR−1)> . Gi denotes the i-th row of G 9: for j = 1→ [n1] do . Form matrix U ′i ∈ Rk×n1 10: U ′ij ← w ◦ Uj , ∀j ∈ [n1]. . Uj denotes the j-th column of U ∈ Rk×n1 11: end for 12: end for 13: Choose G2,i ∈ Rg2×n1 to be a Gaussian sketching matrix. 14: for i = 1→ g1 do 15: αi ← ‖(G2,iU ′i>)V ‖2F . 16: Choose G3,i ∈ Rg3×n1 to be a Gaussian sketching matrix. 17: for j2 = 1→ n2 do 18: βi,j ← ‖G3,i(U ′i>)Vj2‖22. 19: end for 20: end for 21: S ← ∅. 22: for r = 1→ Rsamples do 23: Sample i from [g1] with probability αi/ ∑g1 i′=1 αi′ .\n24: Sample j2 from [n2] with probability βi,j2/ ∑n2\nj′2=1 βi,j′2 .\n25: for j1 = 1→ n1 do 26: γj1 ← ((U ′i>)j1Vj2)2. 27: end for 28: Sample j1 from [n1] with probability γj1/ ∑n1 j′1=1\nγj′1 . 29: S ← S ∪ (j1, j2). 30: end for 31: Convert S into a diagonal matrix D with at most Rsamples nonzero entries. 32: return D. . Diagonal matrix D ∈ Rn1n2×n1n2 33: end procedure\nLemma C.32. Given two matrices U ∈ Rk×n1 and V ∈ Rk×n2, there exists an algorithm that takes O((n1 + n2) · poly(log(n1n2), k) ·Rsamples) time and samples Rsamples columns of U V ∈ Rk×n1n2 according to the leverage scores of R−1(U V ), where R is the R of a QR factorization.\nProof. We choose Π ∈ Rn1n2×s1 to be a TensorSketch. Then, according to Section B.10, we can compute R−1 in n · poly(log n, k, 1/ ) time, where R is the R in a QR-factorization. We want to sample columns from U V according to the square of the `2-norms of each column of R−1(U V ). However, explicitly writing down the matrix R−1(U V ) takes kn1n2 time, and the number of columns is already n1n2. The goal is to sample columns from R−1(U V ) without explicitly computing the square of the `2-norm of each column.\nThe first simple observation is that the following two sampling procedures are equivalent in terms of the column samples of a matrix that they take. (1) We sample a single entry from the\nmatrix R−1(U V ) proportional to its squared value. (2) We sample a column from the matrix R−1(U V ) proportional to its squared `2-norm. Let the (i, j1, j2)-th entry denote the entry in the i-th row and the (j1 − 1)n2 + j2-th column. We can show, for a particular column (j1 − 1)n2 + j2,\nPr[sample an entry from the (j1 − 1)n2 + j2 th column of a matrix]\n= k∑\ni=1\nPr[sample the (i, j1, j2)-th entry of matrix]\n=\nk∑\ni=1\n|(R−1(U V ))i,(j1−1)n2+j2 |2 ‖R−1(U V )‖2F\n= ‖(R−1(U V ))(j1−1)n2+j2‖2 ‖R−1(U V )‖2F = Pr[sample the (j1 − 1)n2 + j2 th column of matrix]. (18)\nThus, it is sufficient to show how to sample a single entry from matrix R−1(U V ) proportional to its squared value without writing down all of the entries of a k × n1n2 matrix.\nWe choose a Gaussian matrix G1 ∈ Rg1×k with g1 = O( −2 log(n1n2)). By Claim C.33 we can reduce the length of each column vector of matrixR−1U V from k to g1 while preserving the squared `2-norm of all columns simultaneously. Thus, we obtain a new matrix GR−1(U V ) ∈ Rg1×n1n2 , and sampling from this new matrix is equivalent to sampling from the original matrix R−1(U V ).\nIn the following paragraphs, we explain a sampling procedure (also described in Procedure FastTensorLeverageScore in Algorithm 10) which contains three sampling steps. The first step is sampling i from [g1], the second step is sampling j2 from [n2], and the last step is sampling j1 from [n1].\nFor each j1 ∈ [n1], let Uj1 denote the j1-th column of U . For each i ∈ [g1], let Gi1 denote the i-th row of matrix G1 ∈ Rg1×k, let U ′i ∈ Rk×n1 denote a matrix where the j1-th column is (GiR−1)> ◦Uj1 ∈ Rk, ∀j ∈ [n1]. Then, using Claim C.37, we have that (GiR−1) · (U V ) ∈ Rn1n2 is a row vector where the entry in the (j1−1)n2 +j2-th coordinate is the entry in the j1-th row and j2th column of matrix (U ′i>V ) ∈ Rn1×n2 . Further, the squared `2-norm of vector (GiR−1) · (U V ) is equal to the squared Frobenius norm of matrix (U ′i>V ). Thus, sampling i proportional to the squared `2-norm of vector (GiR−1) · (U V ) is equivalent to sampling i proportional to the squared Frobenius norm of matrix (U ′i>V ). Naïvely, computing the Frobenius norm of an n1 × n2 matrix requires O(n1n2) time. However, we can choose a Gaussian matrix G2,i ∈ Rg2×n1 to sample according to the value ‖(G2,iU ′i>)V ‖2F , which can be computed in O((n1 + n2)g2k) time. By claim C.35, ‖(G2,iU ′i>)V ‖2F ≈ ‖(U ′i>)V ‖2F with high probability. So far, we have finished the first step of the sampling procedure.\nFor the second step of the sampling procedure, we need to sample j2 from [n2]. To do that, we need to compute the squared `2-norm of each column of U ′i>V ∈ Rn1×n2 . This can be done by choosing another Gaussian matrix G3,i ∈ Rg3×n1 . For all j2 ∈ [n2], by Claim C.36, we have ‖G3,iU ′i>Vj2‖22 ≈ ‖U ′i>Vj2‖22. Also, for j2 ∈ [n2], ‖G3,iU ′i>Vj2‖22 can be computed in nearly linear in n1 + n2 time.\nFor the third step of the sampling procedure, we need to sample j1 from [n1]. Since we already have i and j2 from the previous two steps, we can directly compute |(U ′i>)j1Vj2 |2, for all j1. This only takes O(n1k) time.\nOverall, the running time is O((n1 + n2) · poly(log(n1n2), k, 1/ )). Because our estimates are accurate enough, our sampling probabilities are also good approximations to the leverage score sampling probabilities. Putting it all together, we complete the proof.\nClaim C.33. Given matrix R−1(U V ) ∈ Rk×n1n2, let G1 ∈ Rg1×k denote a Gaussian matrix with g1 = ( −2 log(n1n2)). Then with probability at least 1− 1/ poly(n1n2), we have: for all j ∈ [n1n2],\n(1− )‖R−1(U V )j‖22 ≤ ‖G1R−1(U V )j‖22 ≤ (1 + )‖R−1(U V )j‖22.\nProof. This follows by the Johnson-Lindenstrauss Lemma.\nClaim C.34. For a fixed i ∈ [g1], let G2,i ∈ Rg2×n1 denote a Gaussian matrix with g2 = O( −2 log(n1n2)). Then with probability at least 1− 1/ poly(n1n2), we have: for all j2 ∈ [n2],\n(1− )‖U ′i>Vj2‖22 ≤ ‖(G2,iU ′i>)Vj2‖2 ≤ (1 + )‖U ′i>Vj2‖22.\nBy taking the union bound over all i ∈ [g1], we obtain a stronger claim,\nClaim C.35. With probability at least 1−1/ poly(n1n2), we have : for all i ∈ [g1], for all j2 ∈ [n2],\n(1− )‖U ′i>Vj2‖22 ≤ ‖(G2,iU ′i>)Vj2‖2 ≤ (1 + )‖U ′i>Vj2‖22.\nSimilarly, if we choose G3,i to be a Gaussian matrix, we can obtain the same result as for G2,i:\nClaim C.36. With probability at least 1−1/ poly(n1n2), we have : for all i ∈ [g1], for all j2 ∈ [n2],\n(1− )‖U ′i>Vj2‖22 ≤ ‖(G3,iU ′i>)Vj2‖2 ≤ (1 + )‖U ′i>Vj2‖22.\nClaim C.37. For any i ∈ [g1], j1 ∈ [n1], j2 ∈ [n2], let Gi1 denote the i-th row of matrix G1 ∈ Rg1×k. Let (U V )(j1−1)n2+j2 denote the (j1− 1)n2 + j2-th column of matrix Rk×n1n2 . Let (U ′i>)j1 denote the j1-th row of matrix (U ′i>) ∈ Rn1×k. Let Vj2 denote the j2-th column of matrix V ∈ Rk×n2. Then, we have\nGi1R −1(U V )(j1−1)n2+j2 = (U ′i>)j1Vj2 .\nProof. This follows by,\nGi1R −1(U V )(j1−1)n2+j2 = Gi1R−1(Uj1 ◦ Vj2) = (Gi1R−1 ◦ (Uj1)>)Vj2 = (U ′i>)j1Vj2 .\nLemma C.38. Given A ∈ Rn×n2, V,W ∈ Rk×n, for any > 0, there exists an algorithm that runs in O(n · poly(k, 1/ )) time and outputs a diagonal matrix D ∈ Rn2×n2 with m = O(k log k + k/ ) nonzero entries such that,\n‖Û(V W )−A‖2F ≤ (1 + ) min U∈Rn×k ‖U(V W )−A‖2F ,\nholds with probability at least 0.999, where Û denotes the optimal solution to minU ‖U(V W )D− AD‖2F .\nProof. This follows by combining Theorem C.46, Corollary C.30, and Lemma C.31.\nRemark C.39. Replacing Theorem C.46 (Algorithm 15) by Lemma C.32 (Algorithm 10), we can obtain a slightly different version of Lemma C.38 with n poly(log n, k, 1/ ) running time, where the dependence on k is better.\nAlgorithm 11 Frobenius Norm CURT Decomposition Algorithm, Input Sparsity Time and Nearly Optimal Number of Samples 1: procedure FCURTInputSparsity(A,UB, VB,WB, n, k, ) . Theorem C.40 2: d1 ← d2 ← d3 ← O(k log k + k/ ). 3: 0 ← 0.01. 4: Form B1 = V >B W>B ∈ Rk×n\n2 . 5: D1 ←FastTensorLeverageScoreGeneralOrder(V >B ,W>B , n, n, k, 0, d1). .\nAlgorithm 15 6: Form Û = A1D1(B1D1)† ∈ Rn×k. 7: Form B2 = Û> W>B ∈ Rk×n\n2 . 8: D2 ←FastTensorLeverageScoreGeneralOrder(Û>,W>B , n, n, k, 0, d2). 9: Form V̂ = A2D2(B2D2)† ∈ Rn×k.\n10: Form B3 = Û> V̂ > ∈ Rk×n2 . 11: D3 ←FastTensorLeverageScoreGeneralOrder(Û>, V̂ >, n, n, k, 0, d3). 12: C ← A1D1, R← A2D2, T ← A3D3. 13: U ←∑ki=1((B1D1)†)i ⊗ ((B2D2)†)i ⊗ ((B3D3)†)i. 14: return C, R, T and U . 15: end procedure\nC.7.5 Input sparsity time algorithm\nTheorem C.40. Given a 3rd order tensor A ∈ Rn×n×n, let k ≥ 1, and let UB, VB,WB ∈ Rn×k denote a rank-k, α-approximation to A. Then there exists an algorithm which takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices C ∈ Rn×c with columns from A, R ∈ Rn×r with rows from A, T ∈ Rn×t with tubes from A, and a tensor U ∈ Rc×r×t with rank(U) = k such that c = r = t = O(k log k + k/ ), and\n∥∥∥∥∥∥ c∑\ni=1\nr∑\nj=1\nt∑\nl=1\nUi,j,l · Ci ⊗Rj ⊗ Tl −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + )α min rank−k A′ ‖A′ −A‖2F\nholds with probability 9/10.\nProof. We define\nOPT := min rank−k A′\n‖A′ −A‖2F .\nWe already have three matrices UB ∈ Rn×k, VB ∈ Rn×k and WB ∈ Rn×k and these three matrices provide a rank-k, α-approximation to A, i.e.,\n∥∥∥∥∥ k∑\ni=1\n(UB)i ⊗ (VB)i ⊗ (WB)i −A ∥∥∥∥∥ 2\nF\n≤ αOPT . (19)\nLet B1 = V >B W>B ∈ Rk×n 2 denote the matrix where the i-th row is the vectorization of (VB)i ⊗ (WB)i. Let D1 ∈ Rn 2×n2 be a sampling and rescaling matrix corresponding to sampling by the leverage scores of B>1 ; there are d1 nonzero entries on the diagonal of D1. Let Ai ∈ Rn×n 2 denote the matrix obtained by flattening A along the i-th direction, for each i ∈ [3].\nDefine U∗ ∈ Rn×k to be the optimal solution to min U∈Rn×k ‖UB1−A1‖2F , Û = A1D1(B1D1)† ∈ Rn×k,\nand V0 ∈ Rn×k to be the optimal solution to min V ∈Rn×k ‖V · (Û> W>B )−A2‖2F . Due to Lemma C.38, if d1 = O(k log k + k/ ) then with constant probability, we have\n‖ÛB1 −A1‖2F ≤ αD1‖U∗B1 −A1‖2F . (20)\nRecall that (Û> W>B ) ∈ Rk×n 2 denotes the matrix where the i-th row is the vectorization of\nÛi ⊗ (WB)i, ∀i ∈ [k]. Now, we can show, ‖V0 · (Û> W>B )−A2‖2F ≤ ‖ÛB1 −A1‖2F by V0 = arg min\nV ∈Rn×k ‖V · (Û> W>B )−A2‖2F\n≤ αD1‖U∗B1 −A1‖2F by Equation (20) ≤ αD1‖UBB1 −A1‖2F by U∗ = arg min\nU∈Rn×k ‖UB1 −A1‖2F\n≤ αD1αOPT . by Equation (19) (21)\nWe define B2 = Û> W>B . Let D2 ∈ Rn 2×n2 be a sampling and rescaling matrix corresponding to the leverage scores of B>2 . Suppose there are d2 nonzero entries on the diagonal of D2. Define V ∗ ∈ Rn×k to be the optimal solution to minV ∈Rn×k ‖V B2−A2‖2F , V̂ = A2D2(B2D2)† ∈ Rn×k, W0 ∈ Rn×k to be the optimal solution to min W∈Rn×k\n‖W · (Û> V̂ >)−A3‖2F , and V ′ to be the optimal solution to min\nV ∈Rn×k ‖V B2D2 −A2D2‖2F .\nDue to Lemma C.38, with constant probability, we have\n‖V̂ B2 −A2‖2F ≤ αD2‖V ∗B2 −A2‖2F . (22)\nRecall that (Û> V̂ >) ∈ Rk×n2 denotes the matrix where the i-th row is the vectorization of Ûi ⊗ V̂i, ∀i ∈ [k]. Now, we can show, ‖W0 · (Û> V̂ >)−A3‖2F ≤ ‖V̂ B2 −A2‖2F by W0 = arg min\nW∈Rn×k ‖W · (Û> V̂ >)−A3‖2F\n≤ αD2‖V ∗B2 −A2‖2F by Equation (22) ≤ αD2‖V0B2 −A2‖2F by V ∗ = arg min\nV ∈Rn×k ‖V B2 −A2‖2F\n≤ αD2αD1αOPT . by Equation (21) (23)\nWe define B3 = Û> V̂ >. Let D3 ∈ Rn2×n2 denote a sampling and rescaling matrix corresponding to sampling by the leverage scores of B>3 . Suppose there are d3 nonzero entries on the diagonal of D3.\nDefineW ∗ ∈ Rn×k to be the optimal solution to minW∈Rn×k ‖WB3−A3‖2F , Ŵ = A3D3(B3D3)† ∈ Rn×k, and W ′ to be the optimal solution to min\nW∈Rn×k ‖WB3D3 −A3D3‖2F .\nDue to Lemma C.38 with constant probability, we have\n‖ŴB3 −A3‖2F ≤ αD3‖W ∗B3 −A3‖2F . (24) Now we can show,\n‖ŴB3 −A3‖2F ≤ αD3‖W ∗B3 −A3‖2F , by Equation (24) ≤ αD3‖W0B3 −A3‖2F , by W ∗ = arg min\nW∈Rn×k ‖WB3 −A3‖2F\n≤ αD3αD2αD1αOPT . by Equation (23)\nThis implies, ∥∥∥∥∥ k∑\ni=1\nÛi ⊗ V̂i ⊗ Ŵi −A ∥∥∥∥∥ 2\nF\n≤ O(1)αOPT2 .\nwhere Û = A1D1(B1D1)†, V̂ = A2D2(B2D2)†, Ŵ = A3D3(B3D3)†. By Lemma C.38, we need to set d1 = d2 = d3 = O(k log k + k/ ). Note that B1 = (V >B W>B ). Thus D1 can be found in n · poly(k, 1/ ) time. Because D1 has a small number of nonzero entries on the diagonal, we can compute B1D1 quickly without explicitly writing down B1. Also A1D1 can be computed in nnz(A) time. Using (A1D1) and (B1D1), we can compute Û in n poly(k, 1/ ) time. In a similar way, we can compute B2, D2, B3, and D3. Since tensor U is constructed based on three poly(k, 1/ ) size matrices, (B1D1)†, (B2D2)†, and (B3D3)†, the overall running time is O(nnz(A) + n poly(k, 1/ ))\nC.7.6 Optimal sample complexity algorithm\nAlgorithm 12 Frobenius Norm CURT Decomposition Algorithm, Optimal Sample Complexity 1: procedure FCURTOptimalSamples(A,UB, VB,WB, n, k) . Theorem C.41 2: d1 ← d2 ← d3 ← O(k/ ). 3: Form B1 = V >B W>B ∈ Rk×n\n2 . 4: D1 ←GeneralizedMatrixRowSubsetSelection(A>1 , B>1 , n2, n, k, ). . Algorithm 7 5: Let d1 denote the number of nonzero entries in D1. . d1 = O(k/ ) 6: Form Û = A1D1(B1D1)† ∈ Rn×k. 7: Form B2 = Û> W>B ∈ Rk×n\n2 . 8: D2 ←GeneralizedMatrixRowSubsetSelection(A>2 , B>2 , n2, n, k, ). . Algorithm 7 9: Let d2 denote the number of nonzero entries in D2. . d2 = O(k/ ) 10: Form V̂ = A2D2(B2D2)† ∈ Rn×k. 11: Form B3 = Û> V̂ > ∈ Rk×n2 . 12: D3 ←GeneralizedMatrixRowSubsetSelection(A>3 , B>3 , n2, n, k, ). . Algorithm 7 13: d3 denote the number of nonzero entries in D3. . d3 = O(k/ ) 14: C ← A1D1, R← A2D2, T ← A3D3. 15: U ←∑ki=1((B1D1)†)i ⊗ ((B2D2)†)i ⊗ ((B3D3)†)i. 16: return C, R, T and U . 17: end procedure\nTheorem C.41. Given a 3rd order tensor A ∈ Rn×n×n, let k ≥ 1, and let UB, VB,WB ∈ Rn×k denote a rank-k, α-approximation to A. Then there exists an algorithm which takes O(nnz(A) log n+ n2 poly(log n, k, 1/ )) time and outputs three matrices: C ∈ Rn×c with columns from A, R ∈ Rn×r with rows from A, T ∈ Rn×t with tubes from A, and a tensor U ∈ Rc×r×t with rank(U) = k such that c = r = t = O(k/ ), and\n∥∥∥∥∥∥ c∑\ni=1\nr∑\nj=1\nt∑\nl=1\nUi,j,l · Ci ⊗Rj ⊗ Tl −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + )α min rank−k A′ ‖A′ −A‖2F\nholds with probability 9/10.\nProof. The proof is almost the same as the proof of Theorem C.40. The only difference is that instead of using Theorem C.38, we use Theorem C.14."
    }, {
      "heading" : "C.8 Face-based selection and decomposition",
      "text" : "Previously we provided column-based tensor CURT algorithms, which are algorithms that can select a subset of columns from each of the three dimensions. Here we provide two face-based tensor CURT decomposition algorithms. The first algorithm runs in polynomial time and is a bicriteria algorithm (the number of samples is poly(k/ )). The second algorithm needs to start with a rank-k (1+O( ))- approximate solution, which we then show how to combine with our previous algorithm. Both of our algorithms are able to select a subset of column-row faces, a subset of row-tube faces and a subset of column-tube faces. The second algorithm is able to output U , but the first algorithm is not.\nC.8.1 Column-row, column-tube, row-tube face subset selection\nAlgorithm 13 Frobenius Norm Tensor Column-row, Row-tube and Tube-column Face Subset Selection 1: procedure FFaceCRTSelection(A,n, k, ) . Theorem C.42 2: s1 ← s2 ← O(k/ ). 3: Choose a Gaussian matrix S1 with s1 columns. . Definition B.18 4: Choose a Gaussian matrix S2 with s2 columns. . Definition B.18 5: Form matrix V3 by setting the (i, j)-th column to be (A2S2)j . 6: D3 ←GeneralizedMatrixRowSubsetSelection(A2,V3,n,n2,s1s2, ). . Algorithm 7 7: Let d3 denote the number of nonzero entries in D3. . d3 = O(s1s2/ ) 8: Form matrix U2 by setting the (i, j)-th column to be (A1S1)i. 9: D2 ←GeneralizedMatrixRowSubsetSelection(A1,U2,n,n2,s1s2, ). 10: Let d2 denote the number of nonzero entries in D2. . d2 = O(s1s2/ ) 11: Form matrix W1 by setting the (i, j)-th column to be (A(I,D3, I)3)j . 12: D1 ←GeneralizedMatrixRowSubsetSelection(A3,W1,n,n2,s1s2, ). 13: Let d1 denote the number of nonzero entries in D1. . d1 = O(s1s2/ ) 14: T ← A(I, I,D1), C ← A(D2, I, I), and R← A(I,D3, I). 15: return C, R and T . 16: end procedure\nTheorem C.42. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm which takes O(nnz(A)) log n + n2 poly(log n, k, 1/ ) time and outputs three tensors : a subset C ∈ Rc×n×n of row-tube faces of A, a subset R ∈ Rn×r×n of column-tube faces of A, and a subset T ∈ Rn×n×t of column-row faces of A, where c = r = t = poly(k, 1/ ), and for which there exists a tensor U ∈ Rtn×cn×rn for which\n‖U(T1, C2, R3)−A‖2F ≤ (1 + ) min rank−k A′ ‖A′ −A‖2F ,\nor equivalently, ∥∥∥∥∥∥ tn∑\ni=1\ncn∑\nj=1\nrn∑\nl=1\nUi,j,l · (T1)i ⊗ (C2)j ⊗ (R3)l −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + ) min rank−k A′ ‖A′ −A‖2F .\nProof. We fix V ∗ ∈ Rn×k and W ∗ ∈ Rn×k. We define Z1 ∈ Rk×n2 where the i-th row of Z1 is the vector Vi ⊗Wi. Choose a sketching (Gaussian) matrix S1 ∈ Rn2×s1 (Definition B.18), and let\nÛ = A1S1(Z1S1) † ∈ Rn×k. Following a similar argument as in the previous theorem, we have\n‖ÛZ1 −A1‖2F ≤ (1 + ) OPT .\nWe fix Û and W ∗. We define Z2 ∈ Rk×n2 where the i-th row of Z2 is the vector Ûi ⊗W ∗i . Choose a sketching (Gaussian) matrix S2 ∈ Rn2×s2 (Definition B.18), and let V̂ = A2S2(Z2S2)† ∈ Rn×k. Following a similar argument as in the previous theorem, we have\n‖V̂ Z2 −A2‖2F ≤ (1 + )2 OPT .\nWe fix Û and V̂ . Note that Û = A1S1(Z1S1)† and V̂ = A2S2(Z2S2)†. We define Z3 ∈ Rk×n2 such that the i-th row of Z3 is the vector Ûi ⊗ V̂i. Let z3 = s1 · s2. We define Z ′3 ∈ Rz3×n\n2 such that, ∀i ∈ [s1], ∀j ∈ [s2], the i+ (j − 1)s1-th row of Z ′3 is the vector (A1S1)i ⊗ (A2S2)j .\nWe define U3 ∈ Rn×z3 to be the matrix where the i + (j − 1)s1-th column is (A1S1)i and V3 ∈ Rn×z3 to be the matrix where the i+ (j − 1)s1-th column is (A2S2)j . Then Z ′3 = (U>3 V >3 ).\nWe first have,\nmin W∈Rn×k,X∈Rk×z3 ‖WXZ ′3 −A3‖2F ≤ min W∈Rn×k ‖WZ3 −A3‖2F ≤ (1 + )2 OPT .\nNow consider the following objective function,\nmin W∈Rn×z3\n‖V3 · (W> U>3 )−A2‖2F .\nLet D3 denote a sampling and rescaling diagonal matrix according to V1 ∈ Rn×z3 , let d3 denote the number of nonzero entries of D3. Then we have\nmin W∈Rn×z3 ‖D3V3 · (W> U>3 )−D3A2‖2F = min\nW∈Rn×z3 ‖U3 ⊗ (D3V3)⊗W −A(I,D3, I)‖2F\n= min W∈Rn×z3\n‖W · (U>3 (D3V3)>)− (A(I,D3, I))3‖2F ,\nwhere the first equality follows by retensorizing the objective function, and the second equality follows by flattening the tensor along the third dimension.\nLet Z3 denote (U>3 (D3V3)>) ∈ Rz3×nd3 and W ′ = (A(I,D3, I))3 ∈ Rn×nd3 . Using Theorem C.14, we can find a diagonal matrix D3 ∈ Rn2×n2 with d3 = O(z3/ ) = O(k2/ 3) nonzero entries such that\n‖U3 ⊗ V3 ⊗ (W ′Z†3)−A‖2F ≤ (1 + )3 OPT .\nWe define U2 = U3 ∈ Rn×z2 with z2 = z3. We define W2 = W ′Z†3 ∈ Rn×z2 with z2 = z3. We consider,\nmin V ∈Rn×z2\n‖U2 · (V > W>2 )−A1‖2F .\nLet D2 denote a sampling and rescaling matrix according to U2, and let d2 denote the number of nonzero entries of D2. Then, we have\nmin V ∈Rn×z2 ‖D2U2 · (V > W>2 )−D2A1‖2F = min\nV ∈Rn×z2 ‖D2U2 ⊗ V ⊗W2 −A(D2, I, I)‖2F\n= min V ∈Rn×z2\n‖V · (W>2 (D2U2)>)− (A(D2, I, I))2‖2F ,\nwhere the first equality follows by retensorizing the objective function, and the second equality follows by flattening the tensor along the second dimension.\nLet Z2 denote (W>2 (D2U2)>) ∈ Rz2×nd2 and V ′ = (A(D2, I, I))2 ∈ Rn×nd2 . Using Theorem C.14, we can find a diagonal matrix D2 ∈ Rn2×n2 with d2 = O(z2/ ) nonzero entries such that\n‖U2 ⊗ (V ′Z†2)⊗W2 −A‖2F ≤ (1 + )4 OPT .\nWe define W1 = W2 ∈ Rn×z1 with z1 = z2, and define V1 = (V ′Z†2) ∈ Rn×z1 with z1 = z2. Let D1 denote a sampling and rescaling matrix according to W1, and let d1 denote the number\nof nonzero entries of D1. Then we have\nmin U∈Rn×z1 ‖D1W1 · (U> V >1 )−D1A3‖2F = min\nU∈Rn×z1 ‖U ⊗ V1 ⊗ (D1W1)−A(I, I,D1)‖2F\n= min U∈Rn×z1\n‖U · (V >1 (D1W1)>)−A(I, I,D1)1‖2F\nwhere the first equality follows by unflattening the objective function, and second equality follows by flattening the tensor along the first dimension.\nLet Z1 denote (V >1 (D1W1)>) ∈ Rz1×nd1 , and U ′ = A(I, I,D1)1 ∈ Rn×nd1 . Using Theorem C.14, we can find a diagonal matrix D1 ∈ Rn2×n2 with d1 = O(z1/ ) nonzero entries such that\n‖(U ′Z†1)⊗ (V1)⊗W1 −A‖2F ≤ (1 + )5 OPT,\nwhich means,\n‖(U ′Z†1)⊗ (V ′Z † 2)⊗ (W ′Z † 3)−A‖2F ≤ (1 + )5 OPT .\nPutting U ′, V ′,W ′ together completes the proof.\nCorollary C.43. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm which takes O(nnz(A)) + n2 poly(k, 1/ ) time and outputs three tensors : a subset C ∈ Rc×n×n of row-tube faces of A, a subset R ∈ Rn×r×n of column-tube faces of A, and a subset T ∈ Rn×n×t of column-row faces of A, where c = r = t = poly(k, 1/ ), so that there exists a tensor U ∈ Rtn×cn×rn for which\n‖U(T1, C2, R3)−A‖2F ≤ (1 + ) min rank−k A′ ‖A′ −A‖2F ,\nor equivalently, ∥∥∥∥∥∥ tn∑\ni=1\ncn∑\nj=1\nrn∑\nl=1\nUi,j,l · (T1)i ⊗ (C2)j ⊗ (R3)l −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + ) min rank−k A′ ‖A′ −A‖2F\nProof. If we allow a poly(k/ ) factor increase in running time and a poly(k/ ) factor increase in the number of faces selected, then instead of using generalized row subset selection, which has running time depending on log n, we can use the technique in Section C.11 to avoid the log n factor.\nAlgorithm 14 Frobenius Norm (Face-based) CURT Decomposition Algorithm, Optimal Sample Complexity 1: procedure FFaceCURTDecomposition(A,UB, VB,WB, n, k) . Theorem C.44 2: D1 ←GeneralizedMatrixRowSubsetSelection(A3,WB, n, n2, k, ). . Algorithm 7,\nthe number of nonzero entries is d1 = O(k/ ) 3: Form Z1 = V >B (D1WB)>. 4: Form Û = (A(I, I,D1))1Z † 1 ∈ Rn×k. 5: D2 ←GeneralizedMatrixRowSubsetSelection(A1, Û , n, n2, k, ). . The number of nonzero entries is d2 = O(k/ ) 6: Form Z2 = (W>B (D2Û)). 7: Form V̂ = (A(D2, I, I))2Z † 2 ∈ Rn×k. 8: D3 ←GeneralizedMatrixRowSubsetSelection(A2, V̂ , n, n2, k, ). . The number of nonzero entries is d3 = O(k/ )\n9: Form Z3 = Û> (D3V̂ )>. 10: Form Ŵ = (A(I,D3, I))3(Z3)† ∈ Rn×k. 11: T ← A(I, I,D1), C ← A(D2, I, I), R← A(I,D3, I). 12: U ←∑ki=1((Z1)†)i ⊗ ((Z2)†)i ⊗ ((Z3)†)i. 13: return C, R, T and U . 14: end procedure"
    }, {
      "heading" : "C.8.2 CURT decomposition",
      "text" : "Theorem C.44. Given a 3rd order tensor A ∈ Rn×n×n, let k ≥ 1, and let UB, VB,WB ∈ Rn×k denote a rank-k, α-approximation to A. Then there exists an algorithm which takes O(nnz(A)) log n+ n2 poly(log n, k, 1/ ) time and outputs three tensors: C ∈ Rc×n×n with row-tube faces from A, R ∈ Rn×r×n with colum-tube faces from A, T ∈ Rn×n×t with column-row faces from A, and a (factorization of a) tensor U ∈ Rtn×cn×rn with rank(U) = k for which c = r = t = O(k/ ) and\n‖U(T1, C2, R3)−A‖2F ≤ (1 + )α min rank−k A′ ‖A′ −A‖2F ,\nor equivalently, ∥∥∥∥∥∥ tn∑\ni=1\ncn∑\nj=1\nrn∑\nl=1\nUi,j,l · (T1)i ⊗ (C2)j ⊗ (R3)l −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + )α min rank−k A′ ‖A′ −A‖2F\nholds with probability 9/10.\nProof. We already have three matrices UB ∈ Rn×k, VB ∈ Rn×k and WB ∈ Rn×k and these three matrices provide a rank-k, α-approximation to A, i.e.,\n‖UB ⊗ VB ⊗WB −A‖2F ≤ α min rank−k A′ ‖A′ −A‖2F ︸ ︷︷ ︸\nOPT\n.\nWe can consider the following problem,\nmin U∈Rn×k\n‖WB · (U> V >B )−A3‖2F .\nLet D1 denote a sampling and rescaling diagonal matrix according to WB, and let d1 denote the number of nonzero entries of D1. Then we have\nmin U∈Rn×k ‖(D1WB) · (U> V >B )−D1A3‖2F = min U∈Rn×k ‖U ⊗ VB ⊗D1WB −A(I, I,D1)‖2F\n= min U∈Rn×k\n‖U · (V >B (D1WB)>)− (A(I, I,D1))1‖2F ,\nwhere the first equality follows by retensorizing the objective function, and the second equality follows by flattening the tensor along the first dimension. Let Z1 denote V >B (D1WB)> ∈ Rk×nd1 , and define Û = (A(I, I,D1))1Z † 1 ∈ Rn×k. Then we have\n‖Û ⊗ VB ⊗WB −A‖2F ≤ (1 + )αOPT .\nIn the second step, we fix Û and WB, and consider the following objective function,\nmin V ∈Rn×k\n‖Û · (V > WB)−A1‖2F .\nLet D2 denote a sampling and rescaling matrix according to Û , and let d2 denote the number of nonzero entries of D2. Then we have,\nmin V ∈Rn×k\n‖(D2Û) · (V > W>B )−D2A1‖2F\n= min V ∈Rn×k\n‖(D2Û)⊗ V ⊗WB −A(D2, I, I)‖2F\n= min V ∈Rn×k\n‖V · (W>B (D2Û)>)− (A(D2, I, I))2‖2F ,\nwhere the first equality follows by unflattening the objective function, and the second equality follows by flattening the tensor along the second dimension. Let Z2 denote (W>B (D2Û)>) ∈ Rk×nd2 , and define V̂ = (A(D2, I, I))2(Z2)† ∈ Rn×k. Then we have,\n‖Û ⊗ V̂ ⊗WB −A‖2F ≤ (1 + )2αOPT .\nIn the third step, we fix Û and V̂ , and consider the following objective function,\nmin W∈Rn×k\n‖V̂ · (W Û)−A2‖2F .\nLet D3 denote a sampling and rescaling matrix according to V̂ , and let d3 denote the number of nonzero entries of D3. Then we have,\nmin W∈Rn×k\n‖(D3V̂ ) · (W> Û>)−D3A2‖2F\n= min W∈Rn×k\n‖Û ⊗ (D3V̂ )⊗W −A(I,D3, I)‖2F\n= min W∈Rn×k\n‖W · (Û> (D3V̂ )>)− (A(I,D3, I))3‖2F ,\nwhere the first equality follows by retensorizing the objective function, and the second equality follows by flattening the tensor along the third dimension. Let Z3 denote (Û> (D3V̂ )>) ∈ Rk×nd3 , and define Ŵ = (A(I,D3, I))3(Z3)†. Putting it all together, we have,\n‖Û ⊗ V̂ ⊗ Ŵ −A‖2F ≤ (1 + )3αOPT .\nThis implies\n‖(A(I, I,D1))1Z†1 ⊗ (A(D2, I, I))2Z†2 ⊗ (A(I,D3, I))3Z†3 −A‖2F ≤ (1 + )3αOPT ."
    }, {
      "heading" : "C.9 Solving small problems",
      "text" : "Theorem C.45. Let maxi{ti, di} ≤ n. Given a t1 × t2 × t3 tensor A and three matrices: a t1 × d1 matrix T1, a t2 × d2 matrix T2, and a t3 × d3 matrix T3, if for any δ > 0 there exists a solution to\nmin X1,X2,X3 ∥∥∥∥∥ k∑\ni=1\n(T1X1)i ⊗ (T2X2)i ⊗ (T3X3)i −A ∥∥∥∥∥ 2\nF\n:= OPT,\nand each entry of Xi can be expressed using O(nδ) bits, then there exists an algorithm that takes nO(δ)·2O(d1k+d2k+d3k) time and outputs three matrices: X̂1, X̂2, and X̂3 such that ‖(T1X̂1)⊗(T2X̂2)⊗ (T3X̂3)−A‖2F = OPT. Proof. For each i ∈ [3], we can create ti × di variables to represent matrix Xi. Let x denote this list of variables. Let B denote tensor ∑k i=1(T1X1)i ⊗ (T2X2)i ⊗ (T3X3)i and let Bi,j,l(x) denote an entry of tensor B (which can be thought of as a polynomial written in terms of x). Then we can write the following objective function,\nmin x\nt1∑\ni=1\nt2∑\nj=1\nt3∑\nl=1\n(Bi,j,l(x)−Ai,j,l)2.\nWe slightly modify the above objective function to obtain a new objective function,\nmin x,σ\nt1∑\ni=1\nt2∑\nj=1\nt3∑\nl=1\n(Bi,j,l(x)−Ai,j,l)2,\ns.t. ‖x‖22 ≤ 2O(n δ),\nwhere the last constraint is unharmful, because there exists a solution that can be written using O(nδ) bits. Note that the number of inequality constraints in the above system is O(1), the degree is O(1), and the number of variables is v = (d1k+d2k+d3k). Thus by Theorem B.11, the minimum nonzero cost is at least\n(2O(n δ))−2 O(v) .\nIt is clear that the upper bound on the cost is at most 2O(nδ). Thus the number of binary search steps is at most log(2O(nδ))2O(v). In each step of the binary search, we need to choose a cost C between the lower bound and the upper bound, and write down the polynomial system,\nt1∑\ni=1\nt2∑\nj=1\nt3∑\nl=1\n(Bi,j,l(x)−Ai,j,l)2 ≤ C,\n‖x‖22 ≤ 2O(n δ).\nUsing Theorem B.10, we can determine if there exists a solution to the above polynomial system. Since the number of variables is v, and the degree is O(1), the number of inequality constraints is O(1). Thus, the running time is\npoly(bitsize) · (# constraints · degree)# variables = nO(δ)2O(v).\nC.10 Extension to general q-th order tensors\nThis section provides the details for our extensions from 3rd order tensors to general q-th order tensors. In most practical applications, the order q is a constant. Thus, to simplify the analysis, we use Oq(·) to hide dependencies on q.\nC.10.1 Fast sampling of columns according to leverage scores, implicitly\nThis section explains an algorithm that is able to sample from the leverage scores from the product of q matrices U1, U2, · · · , Uq without explicitly writing down U1 U2 · · ·Uq. To build this algorithm we combine TensorSketch, some ideas from [DMIMW12], and some techniques from [AKO11, MW10]. Finally, we improve the running time for sampling columns according to the leverage scores from poly(n) to Õ(n). Given q matrices U1, U2, · · · , Uq, with each such matrix Ui having size k × ni, we define A ∈ Rk× ∏q i=1 ni to be the matrix where the i-th row of A is the vectorization of U i1 ⊗ U i2 ⊗ · · · ⊗ U iq, ∀i ∈ [k]. Naïvely, in order to sample poly(k, 1/ ) rows from A according to the leverage scores, we need to write down ∏q i=1 ni leverage scores. This approach will\ntake at least ∏q i=1 ni running time. In the remainder of this section, we will explain how to do it in Oq(n · poly(k, 1/ )) time for any constant p, and maxi∈[q] ni ≤ n.\nTheorem C.46. Given q matrices U1 ∈ Rk×n1, U2 ∈ Rk×n2, · · · , Uq ∈ Rk×nq , let maxi ni ≤ n. There exists an algorithm that takes Oq(n ·poly(k, 1/ ) ·Rsamples) time and samples Rsamples columns of U1 U2 · · · Uq ∈ Rk× ∏q i=1 ni according to the leverage scores of U1 U2 · · · Uq.\nProof. Let maxi ni ≤ n. First, choosing Π0 to be a TensorSketch, we can compute R−1 in Oq(n poly(k, 1/ )) time, where R is the R in a QR-factorization. We want to sample columns from U1 U2 · · · Uq according to the square of the `2-norm of each column of R−1(U1 U2 · · ·Uq). The issue is the number of columns of this matrix is already ∏q i=1 ni. The goal is to sample columns from R−1(U1 U2 · · ·Uq) without explicitly computing the square of the `2-norm of each column. Similarly as in the proof of Lemma C.32, we have the observation that the following two sampling procedures are equivalent in terms of sampling a column of a matrix: (1) We sample a single entry from matrix R−1(U1 U2 · · · Uq) proportional to its squared value, (2) We sample a column from matrix R−1(U1 U2 · · · Uq) proportional to its squared `2-norm. Let the (i, j1, j2, · · · , jq)-th entry denote the entry in the i-th row and the j-th column, where\nj =\nq−1∑\nl=1\n(jl − 1) q∏\nt=l+1\nnt + jq.\nSimilarly to Equation (18), we can show, for a particular column j,\nPr[we sample an entry from the j-th column of matrix] = Pr[we sample the j-th column of a matrix].\nThus, it is sufficient to show how to sample a single entry from matrix R−1(U1 U2 · · · Uq) proportional to its squared value without writing down all the entries of the k ×∏qi=1 ni matrix.\nLet V0 denote R−1. Let n0 denote the number of rows of V0. In the next few paragraphs, we describe a sampling procedure (procedure FastTensorLeverageScoreGeneralOrder in Algorithm 15) which first samples ĵ0 from [n0], then samples ĵ1 from [n1], · · · , and at the end samples ĵq from [nq].\nIn the first step, we want to sample ĵ0 from [n0] proportional to the squared `2-norm of that row. To do this efficiently, we choose Π1 ∈ R ∏q i=1 ni×s1 to be a TensorSketch to sketch on the\nAlgorithm 15 Fast Tensor Leverage Score Sampling, for General q-th Order 1: procedure FastTensorLeverageScoreGeneralOrder({Ui}i∈[q], {ni}i∈[q], k, , Rsamples) . Theorem C.46\n2: s1 ← poly(k, 1/ ). 3: Choose Π0,Π1 ∈ Rn1n2···nq×s1 to each be a TensorSketch. . Definition B.34 4: Compute R−1 ∈ Rk×k by using (U1 U2 · · · Uq)Π0. . Ui ∈ Rk×ni , ∀i ∈ [q] 5: V0 ← R−1, n0 ← k. 6: for i = 1→ [n0] do 7: αi ← ‖(V0)i((U1 U2 · · · Uq)Π1)‖22. 8: end for 9: for r = 1→ Rsamples do\n10: Sample ĵ0 from [n0] with probability αi/ ∑n0\ni′=1 αi′ . 11: for l = 1→ q − 1 do 12: sl+1 ← Oq(poly(k, 1/ )). 13: Choose Πl+1 ∈ Rnl+1···nq×sl+1 to be a TensorSketch. 14: for jl = 1→ [nl] do . Form Vl ∈ Rnl×k 15: (Vl)\njl ← (Vl−1)ĵl−1 ◦ (Ul)>jl . 16: end for 17: for i = 1→ nq do 18: βi ← ‖(Vl)i((Ul+1 · · · Uq)Πl+1)‖22. 19: end for 20: Sample ĵl from [nl] with probability βi/ ∑nl i′=1 βi′ . 21: end for 22: for i = 1→ nq do 23: βi ← |(Vq−1)ĵq−1(Uq)i|2. 24: end for 25: Sample ĵq from [nq] with probability βi/ ∑nq i′=1 βi′ . 26: S ← S ∪ (ĵ1, · · · , ĵq). 27: end for 28: Convert S into a diagonal matrix D with at most Rsamples nonzero entries. 29: return D. . Diagonal matrix D ∈ Rn1n2···nq×n1n2···nq 30: end procedure\nright of V0(U1 U2 · · · Uq). By Section B.10, as long as s1 = Oq(poly(k, 1/ )), then Π1 is a (1± )-subspace embedding matrix. Thus with probability 1− 1/Ω(q), for all i ∈ [n0],\n‖(V0)i((U1 U2 · · · Uq)Π1)‖22 = (1± )‖(V0)i((U1 U2 · · · Uq))‖22,\nwhich means we can sample ĵ0 from [n0] in Oq(n poly(k, 1/ )) time. In the second step, we have already obtained ĵ0. Using that row of V0 with U1, we can form a new matrix V1 ∈ Rn1×k in the following sense,\n(V1) i = (V0) ĵ0 ◦ (U1)>i ,∀i ∈ [n1],\nwhere (V1)i denotes the i-th row of matrix V1, (V0)ĵ0 denotes the ĵ0-th row of V0 and (U1)i is the i-th column of U1. Another important observation is, the entry in the (j1, j2, · · · , jq)-th coordinate of vector (V0)ĵ0(U1 U2 · · · Uq) is the same as the entry in the j1-th row and (j2, · · · , jq)-th\ncolumn of matrix V1(U2 U3 · · · Uq). Thus, sampling j1 is equivalent to sampling j1 from the new matrix V1(U2 U3 · · · Uq) proportional to the squared `2-norm of that row. We still have the computational issue that the length of the row vector is very long. To deal with this, we can choose Π2 ∈ R ∏q i=2 ni×s2 to be a TensorSketch to multiply on the right of V1(U2 U3 · · · Uq).\nBy Section B.10, as long as s2 = Oq(poly(k, 1/ )), then Π2 is a (1 ± )-subspace embedding matrix. Thus with probability 1− 1/Ω(q), for all i ∈ [n1],\n‖(V1)i((U2 · · · Uq)Π2)‖22 = (1± )‖(V1)i((U2 · · · Uq))‖22,\nwhich means we can sample ĵ1 from [n1] in Oq(n poly(k, 1/ )) time. We repeat the above procedure until we obtain each of ĵ0, ĵ1, · · · , ĵq. Note that the last one, ĵq, is easier, since the length of the vector is already small enough, and so we do not need to use TensorSketch for it.\nBy Section B.10, the time for multiplying by TensorSketch is Oq(n poly(k, 1/ )). Setting to be a small constant, and taking a union bound over O(q) events completes the proof. Lemma C.47. Given A ∈ Rn0× ∏q i=1 ni, U1, U2, · · · , Uq ∈ Rk×n, for any > 0, there exists an\nalgorithm that runs in O(n · poly(k, 1/ )) time and outputs a diagonal matrix D ∈ R ∏q i=1 ni× ∏q i=1 ni with m = O(k log k + k/ ) nonzero entries such that,\n‖Û(U1 U2 · · · Uq)−A‖2F ≤ (1 + ) min U∈Rn×k ‖U(U1 U2 · · · Uq)−A‖2F ,\nholds with probability at least 0.999, where Û denotes the optimal solution of\nmin U∈Rn0×k\n‖U(U1 U2 · · · Uq)D −AD‖2F .\nProof. This follows by combining Theorem C.46, Corollary C.30, and Lemma C.31.\nC.10.2 General iterative existential proof\nAlgorithm 16 General q-th Order Iterative Existential Proof 1: procedure GeneralIterativeExistentialProof(A,n, k, q, ) . Section C.10.2 2: Fix U∗1 , U∗2 , · · · , U∗q ∈ Rn×k. 3: for i = 1→ q do 4: Choose sketching matrix Si ∈ Rnq−1×si with si = Oq(k/ ). 5: Define Zi ∈ Rk×nq−1 to be\nj<i Û>j j′>i U∗>j′ .\n6: Let Ai denote the matrix obtained by flattening tensor A along the i-th dimension. 7: Define Ûi to be AiSi(ZiSi)†. 8: end for 9: return Û1, Û2, · · · , Ûq.\n10: end procedure\nGiven a q-th order tensor A ∈ Rn×n×···×n, we fix U∗1 , U∗2 , · · · , U∗q ∈ Rn×k to be the best rank-k solution (if it does not exist, then we replace it by a good approximation, as discussed). We define OPT = ‖U∗1 ⊗ U∗2 ⊗ · · · ⊗ U∗q − A‖2F . Our iterative proof works as follows. We first obtain the objective function,\nmin U1∈Rn×k\n‖U1 · Z1 −A1‖2F ≤ OPT,\nwhere A1 is a matrix obtained by flattening tensor A along the first dimension, Z1 = (U∗>2 U∗>3 · · · U∗>q ) denotes a k × nq−1 matrix. Choosing S1 ∈ Rn\nq−1×s1 to be a Gaussian sketching matrix with s1 = O(k/ ), we obtain a smaller problem,\nmin U1∈Rn×k\n‖U1 · Z1S1 −A1S1‖2F .\nWe define Û1 to be A1S1(Z1S1)† ∈ Rn×k, which gives,\n‖Û1 · Z1 −A1‖2F ≤ (1 + ) OPT .\nAfter retensorizing the above, we have,\n‖Û1 ⊗ U∗2 ⊗ · · · ⊗ U∗q −A‖2F ≤ (1 + ) OPT .\nIn the second round, we fix Û1, U∗3 , · · · , U∗q ∈ Rn×k, and choose S2 ∈ Rn q−1×s2 to be a Gaussian sketching matrix with s2 = O(k/ ). We define Z2 ∈ Rk×nq−1 to be (Û>1 U∗>3 · · · U∗>q ). We define Û2 to be A2S2(Z2S2)† ∈ Rn×k. Then, we have\n‖Û1 ⊗ Û2 ⊗ U∗3 ⊗ · · · ⊗ U∗q −A‖2F ≤ (1 + )2 OPT .\nWe repeat the above process, where in the i-th round we fix Û1, · · · , Ûi−1, U∗i+1, · · · , U∗q ∈ Rn×k, and choose Si ∈ Rnq−1×si to be a Gaussian sketching matrix with si = O(k/ ). We define Zi ∈ Rk×nq−1 to be (Û>1 · · · Û>i−1 U∗>i+1 · · · U∗>q ). We define Ûi to be AiSi(ZiSi)† ∈ Rn×k. Then, we have\n‖Û1 ⊗ · · · ⊗ Ûi−1 ⊗ Ûi ⊗ U∗i+1 ⊗ · · · ⊗ U∗q −A‖2F ≤ (1 + )2 OPT .\nAt the end of the q-th round, we have\n‖Û1 ⊗ · · · ⊗ Ûq −A‖2F ≤ (1 + )q OPT .\nReplacing = ′/(2q), we obtain\n‖Û1 ⊗ · · · ⊗ Ûq −A‖2F ≤ (1 + ′) OPT .\nwhere for all i ∈ [q], si = O(kq/ ′) = Oq(k/ ′) .\nC.10.3 General input sparsity reduction\nThis section shows how to extend the input sparsity reduction from third order tensors to general q-th order tensors. Given a tensor A ∈ Rn×n×···×n and q matrices, for each i ∈ [q], matrix Vi has size Vi ∈ Rn×bi , with bi ≤ poly(k, 1/ ). We choose a batch of sparse embedding matrices Ti ∈ Rti×n. Define V̂i = TiVi, and C = A(T1, T2, · · · , Tq). Thus we have with probability 99/100, for any α ≥ 0, for all {Xi, X ′i ∈ Rbi×k}i∈[q], if\n‖V̂1X ′1 ⊗ V̂2X ′2 ⊗ · · · ⊗ V̂qX ′q − C‖2F ≤ α‖V̂1X1 ⊗ V̂2X2 ⊗ · · · ⊗ V̂qXq − C‖2F ,\nthen\n‖V1X ′1 ⊗ V2X ′2 ⊗ · · · ⊗ VqX ′q −A‖2F ≤ (1 + )α‖V1X1 ⊗ V2X2 ⊗ · · · ⊗ VqXq −A‖2F ,\nwhere ti = Oq(poly(bi, 1/ )).\nAlgorithm 17 General q-th Order Input Sparsity Reduction 1: procedure GeneralInputSparsityReduction(A, {Vi}i∈[q], n, k, q, ) . Section C.10.3 2: for i = 1→ q do 3: Choose sketching matrix Ti ∈ Rti×n with ti = poly(k, q, 1/ ). 4: V̂i ← TiVi. 5: end for 6: C ← A(T1, T2, · · · , Tq). 7: return {V̂i}i∈[q], C. 8: end procedure"
    }, {
      "heading" : "C.10.4 Bicriteria algorithm",
      "text" : "This section explains how to extend the bicriteria algorithm from third order tensors (Section C.4) to general q-th order tensors. Given any q-th order tensor A ∈ Rn×n×···×n, we can output a rank-r tensor (or equivalently q matrices U1, U2, · · · , Uq ∈ Rn×r) such that,\n‖U1 ⊗ U2 ⊗ · · · ⊗ Uq −A‖2F ≤ (1 + ) OPT,\nwhere r = Oq((k/ )q−1) and the algorithm takes Oq(nnz(A) + n · poly(k, 1/ )).\nAlgorithm 18 General q-th Order Bicriteria Algorithm 1: procedure GeneralBicriteriaAlgorithm(A,n, k, q, ) . Section C.10.4 2: for i = 2→ q do 3: Choose sketching matrix Si ∈ Rnq−1×si with si = O(kq/ ). 4: Choose sketching matrix Ti ∈ Rti×n with ti = poly(k, q, 1/ ). 5: Form matrix Ûi by setting (j2, j3, · · · , jq)-th column to be (AiSi)ji . 6: end for 7: Solve minU1 ‖U1B − (A(I, T2, · · · , Tq))1‖2F . 8: return {Ûi}i∈[q]. 9: end procedure"
    }, {
      "heading" : "C.10.5 CURT decomposition",
      "text" : "This section extends the tensor CURT algorithm from 3rd order tensors (Section C.7) to general q-th order tensors. Given a q-th order tensor A ∈ Rn×n×···×n and a batch of matrices U1, U2, · · · , Uq ∈ Rn×k, we iteratively apply the proof in Theorem C.40 (or Theorem C.41) q times. Then for each i ∈ [q], we are able to select di columns from the i-th dimension of tensor A (let Ci denote those columns) and also find a tensor U ∈ Rd1×d2×···×dq such that,\n‖U(C1, C2, · · · , Cq)−A‖2F ≤ (1 + )‖U1 ⊗ U2 ⊗ · · · ⊗ Uq −A‖2F ,\nwhere either di = Oq(k log k + k/ ) (similar to Theorem C.40) or di = Oq(k/ ) (similar to Theorem C.41)."
    }, {
      "heading" : "C.11 Matrix CUR decomposition",
      "text" : "There is a long line of research on matrix CUR decomposition under operator, Frobenius or recently, entry-wise `1 norm [DMM08, BMD09, DR10, BDM11, BW14, SWZ17]. We provide the first\nAlgorithm 19 General q-th Order CURT Decomposition 1: procedure GeneralCURTDecomposition(A, {Ui}i∈[q], n, k, q, ) . Section C.10.5 2: for i = 1→ q do 3: Form Bi =\nj<i Û>j j>i U>j ∈ Rk×n q−1 .\n4: if fast = true then . Optimal running time 5: 0 ← 0.01. 6: di ← Oq(k log k + k/ ). 7: Di ← FastTensorLeverageScoreGeneralOrder ({Ûj}j<i, {Uj}j>i, n, k, 0, di). . Algorithm 15 8: else . Optimal sample complexity 9: 0 ← Oq( ). 10: Di ← GeneralizedMatrixRowSubsetSelection (A>i , B>i , nq−1, n, k, 0). . Algorithm C.5, di = Oq(k/ ). 11: end if 12: Ûi ← AiDi(BiDi)†. 13: Ci ← AiDi. 14: end for 15: U ← (B1D1)† ⊗ (B2D2)† ⊗ · · · ⊗ (BqDq)†. 16: return {Ci}i∈[q], U . 17: end procedure\nalgorithm that runs in nnz(A) time, which improves the previous best matrix CUR decomposition algorithm under Frobenius norm [BW14]."
    }, {
      "heading" : "C.11.1 Algorithm",
      "text" : "Algorithm 20 Optimal Matrix CUR Decomposition Algorithm 1: procedure OptimalMatrixCUR(A,n, k, ) . Theorem C.48 2: ′ ← 0.1 . ′′ ← 0.001 ′. 3: Û ←SparseSVD(A, k, ′). . Û ∈ Rn×k 4: Choose S1 ∈ Rn×n to be a sampling and rescaling diagonal matrix according to the leverage\nscores of Û with s1 = O( −2k log k) nonzero entries. 5: R, Y ←GeneralizedMatrixRowSubsetSelection(S1A,S1Û , s1, n, k, ′′). .\nAlgorithm 7, R ∈ Rr×n, Y ∈ Rk×r and r = O(k/ ) 6: V̂ ← Y R ∈ Rk×n. 7: Choose S>2 ∈ Rn×n to be a sampling and rescaling diagonal matrix according to the leverage\nscores of V̂ > ∈ Rn×k with s2 = O( −2k log k) nonzero entries. 8: C>, Z> ← GeneralizedMatrixRowSubsetSelection ((AS2)>, (V̂ S2)>, s2, n, k, ′′). .\nAlgorithm 7, C ∈ Rn×c, Z ∈ Rc×k, and c = O(k/ ) 9: U ← ZY . . U ∈ Rc×r and rank(U) = k\n10: return C,U,R. 11: end procedure\nTheorem C.48. Given matrix A ∈ Rn×n, for any k ≥ 1 and ∈ (0, 1), there exists an algorithm that takes O(nnz(A) + n poly(k, 1/ )) time and outputs three matrices C ∈ Rn×c with c columns\nfrom A, R ∈ Rr×n with r rows from A, and U ∈ Rc×r with rank(U) = k such that r = c = O(k/ ) and,\n‖CUR−A‖2F ≤ (1 + ) min rank−k Ak ‖Ak −A‖2F ,\nholds with probability at least 9/10.\nProof. We define\nOPT = min rank−k Ak\n‖Ak −A‖2F .\nWe first compute Û ∈ Rn×k by using the result of [CW13], so that Û satisfies:\nmin X∈Rk×n\n‖ÛX −A‖2F ≤ (1 + ) OPT . (25)\nThis step can be done in O(nnz(A) + n poly(k, 1/ )) time. We choose S1 ∈ Rn×n to be a sampling and rescaling diagonal matrix according to the leverage scores of Û , where here s1 = O( −2k log k) is the number of samples. This step also can be done in O(n poly(k, 1/ )) time.\nWe run GeneralizedMatrixRowSubsetSelection(Algorithm 7) on matrices S1A and S1Û . Then we obtain two new matrices R and Y , where R contains r = O(k/ ) rows of S1A and Y has size k × r. According to Theorem C.14 and Corollary C.15, this step takes n poly(k, 1/ ) time.\nWe construct V̂ = Y R, and choose S>2 to be another sampling and rescaling diagonal matrix according to the leverage scores of V̂ > with s2 = O( −2k log k) nonzero entries. As in the case of constructing S1, this step can be done in O(n poly(k, 1/ )) time.\nWe run GeneralizedMatrixRowSubsetSelection(Algorithm 7) on matrices (AS2)> and (V̂ S2)\n>. Then we can obtain two new matrices C> and Y >, where C> contains c = O(k/ ) rows of (AS2)> and Z> has size k × c. According to Theorem C.14 and Corollary C.15, this step takes n poly(k, 1/ ) time.\nThus, overall the running time is O(nnz(A) + n poly(k, 1/ )).\nCorrectness. Let\nX∗ = arg min X∈Rn×k ‖XV̂ −A‖2F .\nAccording to Corollary C.15,\n‖CZV̂ S2 −AS2‖2F ≤ (1 + ′′) min X∈Rn×k ‖XV̂ S2 −AS2‖2F ≤ (1 + ′′)‖X∗V̂ S2 −AS2‖2F .\nAccording to Theorem C.52, ′′ = 0.001 ′,\n‖CZV̂ −A‖2F ≤ (1 + ′)‖X∗V̂ −A‖2F . (26)\nLet\nX̃ = arg min X∈Rk×n\n‖ÛX −A‖2F .\nAccording to Corollary C.15,\n‖S1ÛY R− S1A‖2F ≤ (1 + ′′) min X∈Rk×n ‖S1ÛX − S1A‖2F ≤ (1 + ′′)‖S1ÛX̃ − S1A‖2F .\nAccording to Theorem C.52, since ′′ = 0.001 ′,\n‖ÛY R−A‖2F ≤ (1 + ′)‖ÛX̃ −A‖2F . (27) Then, we can conclude\n‖CUR−A‖2F = ‖CZY R−A‖2F = ‖CZV̂ −A‖2F ≤ (1 + ′) min\nX∈Rn×k ‖XV̂ −A‖2F\n≤ (1 + ′)‖Û V̂ −A‖2F ≤ (1 + ′)2 min\nX∈Rk×n ‖ÛX −A‖2F\n≤ (1 + ′)3 OPT ≤ (1 + ) OPT .\nThe first equality follows since U = ZY . The second equality follows since Y R = V̂ . The first inequality follows by Equation (26). The third inequality follows by Equation (27). The fourth inequality follows by Equation (25). The last inequality follows since ′ = 0.1 .\nNotice that C has O(k/ ) reweighted columns of AS2, and AS2 is a subset of reweighted columns of A, so C has O(k/ ) reweighted columns of A. Similarly, we can prove that R has O(k/ ) reweighted rows of A. Thus, CUR is a CUR decomposition of A.\nC.11.2 Stronger property achieved by leverage scores\nClaim C.49. Given matrix A ∈ Rn×m, for any distribution p = (p1, p2, · · · , pn) define random variable X such that X = ‖Ai‖22/pi with probability pi, where Ai is the i-th row of matrix A. Then take m independent samples X1, X2, · · · , Xm, and let Y = 1m ∑m j=1X j. We have\nPr[Y ≤ 100‖A‖2F ] ≥ .99. Proof. We can compute the expectation of Xj , for any j ∈ [m],\nE[Xj ] = n∑\ni=1\n‖Ai‖22 pi · pi = ‖A‖2F .\nThen E[Y ] = 1m ∑m j=1 E[X j ] = ‖A‖2F . Using Markov’s inequality, we have\nPr[Y ≥ ‖A‖2F ] ≤ .01.\nTheorem C.50 (The leverage score case of Theorem 39 in [CW13]). Let A ∈ Rn×k, B ∈ Rn×d. Let S ∈ Rn×n denote a sampling and rescaling diagonal matrix according to the leverage scores of A. If the event occurs that S satisfies ( / √ k)-Frobenius norm approximate matrix product for A, and also S is a (1 + )-subspace embedding for A, then let X∗ be the optimal solution of minX ‖AX −B‖2F , and B̃ ≡ AX∗ −B. Then, for all X ∈ Rk×d,\n(1− 2 )‖AX −B‖2F ≤ ‖S(AX −B)‖2F + ‖B̃‖2F − ‖SB̃‖2F ≤ (1 + 2 )‖AX −B‖2F . Furthermore, if S has m = O( −2k log(k)) nonzero entries, the above event happens with probability at least 0.99.\nNote that Theorem 39 in [CW13] is stated in a way that holds for general sketching matrices. However, we are only interested in the case when S is a sampling and rescaling diagonal matrix according to the leverage scores. For completeness, we provide the full proof of the leverage score case with certain parameters.\nProof. Suppose S is a sampling and rescaling diagonal matrix according to the leverage scores of A, and it has m = O( −2k log k) nonzero entries. Then, according to Lemma C.22, S is a (1 + )- subspace embedding for A with probability at least 0.999, and according to Lemma C.29, S satisfies ( / √ k)-Frobenius norm approximate matrix product for A with probability at least 0.999.\nLet U ∈ Rn×k denote an orthonormal basis of the column span of A. Then the leverage scores of U are the same as the leverage scores of A. Furthermore, for any X ∈ Rk×d, there is a matrix Y such that AX = UY , and vice versa, so we can now assume A has k orthonormal columns.\nThen,\n‖S(AX −B)‖2F − ‖SB̃‖2F = ‖SA(X −X∗) + S(AX∗ −B)‖2F − ‖SB̃‖2F = ‖SA(X −X∗)‖2F + ‖S(AX∗ −B)‖2F + 2 tr ( (X −X∗)>A>S>S(AX∗ −B) ) − ‖SB̃‖2F = ‖SA(X −X∗)‖2F + 2 tr ( (X −X∗)>A>S>SB̃ )\n︸ ︷︷ ︸ α\n. (28)\nThe second equality follows using ‖C + D‖2F = ‖C‖2F + ‖D‖2F + 2 tr(C>D). The third equality follows from B̃ = AX∗ −B. Now, let us first upper bound the term α in Equation (28):\n‖SA(X −X∗)‖2F + 2 tr ( (X −X∗)>A>S>SB̃ )\n≤ (1 + )‖A(X −X∗)‖2F + 2‖X −X∗‖F ‖A>S>SB̃‖F ≤ (1 + )‖A(X −X∗)‖2F + 2( / √ k) · ‖X −X∗‖F ‖A‖F ‖B̃‖F ≤ (1 + )‖A(X −X∗)‖2F + 2 ‖A(X −X∗)‖F ‖B̃‖F .\nThe first inequality follows since S is a (1+ ) subspace embedding of A, and tr(C>D) ≤ ‖C‖F ‖D‖F . The second inequality follows since S satisfies ( / √ k)-Frobenius norm approximate matrix product\nfor A. The last inequality follows using that ‖A‖F ≤ √ k since A only has k orthonormal columns. Now, let us lower bound the term α in Equation (28):\n‖SA(X −X∗)‖2F + 2 tr ( (X −X∗)>A>S>SB̃ )\n≥ (1− )‖A(X −X∗)‖2F − 2‖X −X∗‖F ‖A>S>SB̃‖F ≥ (1− )‖A(X −X∗)‖2F − 2( / √ k) · ‖X −X∗‖F ‖A‖F ‖B̃‖F ≥ (1− )‖A(X −X∗)‖2F − 2 ‖A(X −X∗)‖F ‖B̃‖F .\nThe first inequality follows since S is a (1+ ) subspace embedding ofA, and tr(C>D) ≥ −‖C‖F ‖D‖F . The second inequality follows since S satisfies ( / √ k)-Frobenius norm approximate matrix product\nfor A. The last inequality follows using that ‖A‖F ≤ √ k since A only has k orthonormal columns.\nTherefore,\n(1− )‖A(X −X∗)‖2F − 2 ‖A(X −X∗)‖F ‖B̃‖F ≤ ‖S(AX −B)‖2F − ‖SB̃‖2F , (29)\nand\n(1 + )‖A(X −X∗)‖2F + 2 ‖A(X −X∗)‖F ‖B̃‖F ≥ ‖S(AX −B)‖2F − ‖SB̃‖2F . (30)\nNotice that B̃ = AX∗ −B = AA†B −B = (AA† − I)B, so according to the Pythagorean theorem, we have\n‖AX −B‖2F = ‖A(X −X∗)‖2F + ‖B̃‖2F ,\nwhich means that\n‖A(X −X∗)‖2F = ‖AX −B‖2F − ‖B̃‖2F . (31)\nUsing Equation (31), we can rewrite and lower bound the LHS of Equation (29),\n(1− )‖A(X −X∗)‖2F − 2 ‖A(X −X∗)‖F ‖B̃‖F = ‖A(X −X∗)‖2F − ( ‖A(X −X∗)‖2F + 2‖A(X −X∗)‖F ‖B̃‖F ) = ‖AX −B‖2F − ‖B̃‖2F − ( ‖A(X −X∗)‖2F + 2‖A(X −X∗)‖F ‖B̃‖F ) ≥ ‖AX −B‖2F − ‖B̃‖2F − ( ‖A(X −X∗)‖F + ‖B̃‖F )2 ≥ ‖AX −B‖2F − ‖B̃‖2F − 2 ( ‖A(X −X∗)‖2F + ‖B̃‖2F )\n= (1− 2 )‖AX −B‖2F − ‖B̃‖2F . (32)\nThe second step follows by Equation (31). The first inequality follows using a2 + 2ab < (a + b)2. The second inequality follows using (a + b)2 ≤ 2(a2 + b2). The last equality follows using ‖A(X − X∗)‖2F + ‖B̃‖2F = ‖AX−B‖2F . Similarly, using Equation (31), we can rewrite and upper bound the LHS of Equation (30)\n(1 + )‖A(X −X∗)‖2F + 2 ‖A(X −X∗)‖F ‖B̃‖F ≤ (1 + 2 )‖AX −B‖2F − ‖B̃‖2F . (33)\nCombining Equations (29),(32),(30),(33), we conclude that\n(1− 2 )‖AX −B‖2F − ‖B̃‖2F ≤ ‖S(AX −B)‖2F − ‖SB̃‖2F ≤ (1 + 2 )‖AX −B‖2F − ‖B̃‖2F .\nTheorem C.51. Let A ∈ Rn×k, B ∈ Rn×d, and 12 > > 0. Let X∗ be the optimal solution to minX ‖AX −B‖2F , and B̃ ≡ AX∗ −B. Let S ∈ Rn×n denote a sketching matrix which satisfies the following:\n1. ‖SB̃‖2F ≤ 100 · ‖B̃‖2F ,\n2. for all X ∈ Rk×d,\n(1− )‖AX −B‖2F ≤ ‖S(AX −B)‖2F + ‖B̃‖2F − ‖SB̃‖2F ≤ (1 + )‖AX −B‖2F .\nThen, for all X1, X2 ∈ Rk×d satisfying\n‖SAX1 − SB‖2F ≤ ( 1 +\n100\n) · ‖SAX2 − SB‖2F ,\nwe have\n‖AX1 −B‖2F ≤ (1 + 5 ) · ‖AX2 −B‖2F .\nProof. Let A,B, S, be the same as in the statement of the theorem, and suppose S satisfies those two conditions. Let X1, X2 ∈ Rk×d satisfy\n‖SAX1 − SB‖2F ≤ ( 1 +\n100\n) ‖SAX2 − SB‖2F .\nWe have\n‖AX1 −B‖2F ≤ 1\n1− ( ‖S(AX1 −B)‖2F + ‖B̃‖2F − ‖SB̃‖2F )\n≤ 1 1−\n(( 1 +\n100\n) · ‖S(AX2 −B)‖2F + ‖B̃‖2F − ‖SB̃‖2F )\n= 1 1− (( 1 + 100 ) · ( ‖S(AX2 −B)‖2F + ‖B̃‖2F − ‖SB̃‖2F ) − 100 · ( ‖B̃‖2F − ‖SB̃‖2F )) ≤ 1 1− · ( 1 + 100 ) · ‖AX2 −B‖2F − 1 1− · 100 · ( ‖B̃‖2F − ‖SB̃‖2F ) ≤ (1 + 3 )‖AX2 −B‖2F + 1\n1− · 100 ‖SB̃‖2F\n≤ (1 + 3 )‖AX2 −B‖2F + 2 ‖B̃‖2F ≤ (1 + 5 )‖AX2 −B‖2F .\nThe first inequality follows since S satisfies the second condition. The second inequality follows by the relationship between X1 and X2. The third inequality follows since S satisfies the second condition. The fifth inequality follows using that < 12 and that S satisfies the first condition. The last inequality follows using that ‖B̃‖2F = ‖AX∗ −B‖2F ≤ ‖AX2 −B‖2F .\nTheorem C.52. Let A ∈ Rn×k, B ∈ Rn×d, and 12 > > 0. Let S ∈ Rn×n denote a sampling and rescaling diagonal matrix according to the leverage scores of A. If S has at least m = O(k log(k)/ 2) nonzero entries, then with probability at least 0.98, for all X1, X2 ∈ Rk×d satisfying\n‖SAX1 − SB‖2F ≤ (1 + 500 ) · ‖SAX2 − SB‖2F ,\nwe have\n‖AX1 −B‖2F ≤ (1 + ) · ‖AX2 −B‖2F .\nProof. The proof directly follows by Claim C.49, Theorem C.50 and Theorem C.51. Because of Claim C.49, S satisfies the first condition in the statement of Theorem C.51 with probability at least 0.99. According to Theorem C.50, S satisfies the second condition in the statement of Theorem C.51 with probability at least 0.99. Thus, with probability 0.98, by Theorem C.51, we complete the proof."
    }, {
      "heading" : "D Entry-wise `1 Norm for Arbitrary Tensors",
      "text" : "In this section, we provide several different algorithms for tensor `1-low rank approximation. Section D.1 provides some useful facts and definitions. Section D.2 presents several existence results. Section D.3 describes a tool that is able to reduce the size of the objective function from poly(n) to poly(k). Section D.4 discusses the case when the problem size is small. Section D.5 provides several bicriteria algorithms. Section D.6 summarizes a batch of algorithms. Section D.7 provides an algorithm for `1 norm CURT decomposition.\nNotice that if the rank−k solution does not exist, then every bicriteria algorithm in Section D.5 can be stated in a form similar to Theorem 1.1, and every algorithm which can output a rank−k solution in Section D.6 can be stated in a form similar to Theorem 1.2. See Section 1 for more details."
    }, {
      "heading" : "D.1 Facts",
      "text" : "We present a method that is able to reduce the entry-wise `1-norm objective function to the Frobenius norm objective function.\nFact D.1. Given a 3rd order tensor C ∈ Rc1×c2×c3, three matrices V1 ∈ Rc1×b1, V2 ∈ Rc2×b2 , V3 ∈ Rc3×b3, for any k ∈ [1,mini bi], if X ′1 ∈ Rb1×k, X ′2 ∈ Rb2×k, X ′3 ∈ Rb3×k satisfies that,\n‖(V1X ′1)⊗ (V2X ′2)⊗ (V3X ′3)− C‖F ≤ α min X1,X2,X3 ‖(V1X1)⊗ (V2X2)⊗ (V3X3)− C‖F ,\nthen\n‖(V1X ′1)⊗ (V2X ′2)⊗ (V3X ′3)− C‖1 ≤ α √ c1c2c3 min\nX1,X2,X3 ‖(V1X1)⊗ (V2X2)⊗ (V3X3)− C‖1.\nWe extend Lemma C.15 in [SWZ17] to tensors:\nFact D.2. Given tensor A ∈ Rn×n×n, let OPT = min rank−k Ak ‖A − Ak‖1. For any r ≥ k, if rank-r tensor B ∈ Rn×n×n is an f -approximation to A, i.e.,\n‖B −A‖1 ≤ f ·OPT,\nand U, V,W ∈ Rn×k is a g-approximation to B, i.e.,\n‖U ⊗ V ⊗W −B‖1 ≤ g · min rank−k Bk ‖Bk −B‖1,\nthen,\n‖U ⊗ V ⊗W −A‖1 . gf ·OPT .\nProof. We define Ũ , Ṽ , W̃ ∈ Rn×k to be three matrices, such that\n‖Ũ ⊗ Ṽ ⊗ W̃ −B‖1 ≤ g min rank−k Bk ‖Bk −B‖1,\nand also define,\nÛ , V̂ , Ŵ = arg min U,V,W∈Rn×k ‖U ⊗ V ⊗W −B‖1 and U∗, V ∗,W ∗ = arg min U,V,W∈Rn×k ‖U ⊗ V ⊗W −A‖1.\nIt is obvious that,\n‖Û ⊗ V̂ ⊗ Ŵ −B‖1 ≤ ‖U∗ ⊗ V ∗ ⊗W ∗ −B‖1. (34)\nThen,\n‖Ũ ⊗ Ṽ ⊗ W̃ −A‖1 ≤ ‖Ũ ⊗ Ṽ ⊗ W̃ −B‖1 + ‖B −A‖1 by the triangle inequality ≤ g‖Û ⊗ V̂ ⊗ Ŵ −B‖1 + ‖B −A‖1 by definition ≤ g‖U∗ ⊗ V ∗ ⊗W ∗ −B‖1 + ‖B −A‖1 by Equation (34) ≤ g‖U∗ ⊗ V ∗ ⊗W ∗ −A‖1 + g‖B −A‖1 + ‖B −A‖1 by the triangle inequality = gOPT +(g + 1)‖B −A‖1 by definition of OPT ≤ gOPT +(g + 1)f ·OPT since B is an f -approximation to A . gf OPT .\nThis completes the proof.\nUsing the above fact, we are able to optimize our approximation ratio."
    }, {
      "heading" : "D.2 Existence results",
      "text" : "Definition D.3 (`1 multiple regression cost preserving sketch - Definition D.5 in [SWZ17]). Given matrices U ∈ Rn×r, A ∈ Rn×d, let S ∈ Rm×n. If ∀β ≥ 1, V̂ ∈ Rr×d which satisfy\n‖SUV̂ − SA‖1 ≤ β · min V ∈Rr×d ‖SUV − SA‖1,\nit holds that\n‖UV̂ −A‖1 ≤ β · c · min V ∈Rr×d ‖UV −A‖1,\nthen S provides a c-`1-multiple-regression-cost-preserving-sketch for (U,A).\nTheorem D.4. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, there exist three matrices S1 ∈ Rn2×s1 , S2 ∈ Rn2×s2 , S3 ∈ Rn2×s3 such that\nmin X1,X2,X3 ∥∥∥∥∥ k∑\ni=1\n(A1S1X1)i ⊗ (A2S2X2)i ⊗ (A3S3X3)i −A ∥∥∥∥∥\n1\n≤ α min rank−k Ak∈Rn×n×n ‖Ak −A‖1,\nholds with probability 99/100. (I). Using a dense Cauchy transform, s1 = s2 = s3 = Õ(k), α = Õ(k1.5) log3 n. (II). Using a sparse Cauchy transform, s1 = s2 = s3 = Õ(k 5), α = Õ(k13.5) log3 n.\n(III). Guessing Lewis weights, s1 = s2 = s3 = Õ(k), α = Õ(k1.5).\nProof. We use OPT to denote\nOPT := min rank−k Ak∈Rn×n×n\n‖Ak −A‖1.\nGiven a tensor A ∈ Rn1×n2×n3 , we define three matrices A1 ∈ Rn1×n2n3 , A2 ∈ Rn2×n3n1 , A3 ∈ Rn3×n1n2 such that, for any i ∈ [n1], j ∈ [n2], l ∈ [n3],\nAi,j,l = (A1)i,(j−1)·n3+l = (A2)j,(l−1)·n1+i = (A3)l,(i−1)·n2+j .\nWe fix V ∗ ∈ Rn×k and W ∗ ∈ Rn×k, and use V ∗1 , V ∗2 , · · · , V ∗k to denote the columns of V ∗ and W ∗1 ,W ∗ 2 , · · · ,W ∗k to denote the columns of W ∗.\nWe consider the following optimization problem,\nmin U1,··· ,Uk∈Rn ∥∥∥∥∥ k∑\ni=1\nUi ⊗ V ∗i ⊗W ∗i −A ∥∥∥∥∥\n1\n,\nwhich is equivalent to\nmin U1,··· ,Uk∈Rn ∥∥∥∥∥∥∥∥ [ U1 U2 · · · Uk ]   V ∗1 ⊗W ∗1 V ∗2 ⊗W ∗2 · · ·\nV ∗k ⊗W ∗k\n −A ∥∥∥∥∥∥∥∥ 1 .\nWe use matrix Z1 to denote V ∗> W ∗> ∈ Rk×n2 and matrix U to denote [ U1 U2 · · · Uk ] .\nThen we can obtain the following equivalent objective function,\nmin U∈Rn×k\n‖UZ1 −A1‖1.\nChoose an `1 multiple regression cost preserving sketch S1 ∈ Rn2×s1 for (Z>1 , A>1 ). We can obtain the optimization problem,\nmin U∈Rn×k ‖UZ1S1 −A1S1‖1 = min U∈Rn×k\nn∑\ni=1\n‖U iZ1S1 − (A1S1)i‖1,\nwhere U i denotes the i-th row of matrix U ∈ Rn×k and (A1S1)i denotes the i-th row of matrix A1S1. Instead of solving it under the `1-norm, we consider the `2-norm relaxation,\nmin U∈Rn×k ‖UZ1S1 −A1S1‖2F = min U∈Rn×k\nn∑\ni=1\n‖U iZ1S1 − (A1S1)i‖22.\nLet Û ∈ Rn×k denote the optimal solution of the above optimization problem. Then, Û = A1S1(Z1S1)\n†. We plug Û into the objective function under the `1-norm. According to Claim B.13, we have,\n‖ÛZ1S1 −A1S1‖1 = n∑\ni=1\n‖Û iZ1S1 − (A1S1)i‖1 ≤ √ s1 min\nU∈Rn×k ‖UZ1S1 −A1S1‖1.\nSince S1 ∈ Rn2×s1 satisfies Definition D.3, we have\n‖ÛZ1 −A1‖1 ≤ α min U∈Rn×k ‖UZ1 −A1‖1 = αOPT,\nwhere α = √ s1β and β (see Definition D.3) is a parameter which depends on which kind of sketching matrix we actually choose. It implies\n‖Û ⊗ V ∗ ⊗W ∗ −A‖1 ≤ αOPT .\nAs a second step, we fix Û ∈ Rn×k and W ∗ ∈ Rn×k, and convert tensor A into matrix A2. Let matrix Z2 denote Û> W ∗>. We consider the following objective function,\nmin V ∈Rn×k\n‖V Z2 −A2‖1,\nand the optimal cost of it is at most αOPT. Choose an `1 multiple regression cost preserving sketch S2 ∈ Rn2×s2 for (Z>2 , A>2 ), and sketch on the right of the objective function to obtain this new objective function,\nmin V ∈Rn×k ‖V Z2S2 −A2S2‖1 = min U∈Rn×k\nn∑\ni=1\n‖V iZ2S2 − (A2S2)i‖1,\nwhere V i denotes the i-th row of matrix V and (A2S2)i denotes the i-th row of matrix A2S2. Instead of solving this under the `1-norm, we consider the `2-norm relaxation,\nmin U∈Rn×k ‖V Z2S2 −A2S2‖2F = min V ∈Rn×k ‖V i(Z2S2)− (A2S2)i‖22.\nLet V̂ ∈ Rn×k denote the optimal solution of the above problem. Then V̂ = A2S2(Z2S2)†. By properties of the sketching matrix S2 ∈ Rn2×s2 , we have,\n‖V̂ Z2 −A2‖1 ≤ α min V ∈Rn×k ‖V Z2 −A2‖1 ≤ α2 OPT,\nwhich implies\n‖Û ⊗ V̂ ⊗W ∗ −A‖1 ≤ α2 OPT .\nAs a third step, we fix the matrices Û ∈ Rn×k and V̂ ∈ Rn×k. We can convert tensor A ∈ Rn×n×n into matrix A3 ∈ Rn2×n. Let matrix Z3 denote Û> V̂ > ∈ Rk×n2 . We consider the following objective function,\nmin W∈Rn×k\n‖WZ3 −A3‖1,\nand the optimal cost of it is at most α2 OPT. Choose an `1 multiple regression cost preserving sketch S3 ∈ Rn2×s3 for (Z>3 , A>3 ) and sketch on the right of the objective function to obtain the new objective function,\nmin W∈Rn×k\n‖WZ3S3 −A3S3‖1.\nLet Ŵ ∈ Rn×k denote the optimal solution of the above problem. Then Ŵ = A3S3(Z3S3)†. By properties of sketching matrix S3 ∈ Rn2×s3 , we have,\n‖ŴZ3 −A3‖1 ≤ α min W∈Rn×k ‖WZ3 −A3‖1 ≤ α3 OPT .\nThus, we obtain,\nmin X1∈Rs1×k,X2∈Rs2×k,X3∈Rs3×k ∥∥∥∥∥ k∑\ni=1\n(A1S1X1)i ⊗ (A2S2X2)i ⊗ (A3S3X3)i −A ∥∥∥∥∥\n1\n≤ α3 OPT .\nProof of (I) By Theorem C.1 in [SWZ17], we can use dense Cauchy transforms for S1, S2, S3, and then s1 = s2 = s3 = O(k log k) and α = O( √ k log k log n).\nProof of (II) By Theorem C.1 in [SWZ17], we can use sparse Cauchy transforms for S1, S2, S3, and then s1 = s2 = s3 = O(k5 log5 k) and α = O(k4.5 log4.5 k log n).\nProof of (III) By Theorem C.1 in [SWZ17], we can sample by Lewis weights. Then S1, S2, S3 ∈ Rn2×n2 are diagonal matrices, and each of them has O(k log k) nonzero rows. This gives α = O( √ k log k)."
    }, {
      "heading" : "D.3 Polynomial in k size reduction",
      "text" : "Definition D.5 (Definition D.1 in [SWZ17]). Given a matrix M ∈ Rn×d, if matrix S ∈ Rm×n satisfies\n‖SM‖1 ≤ β‖M‖1,\nthen S has at most β dilation on M .\nDefinition D.6 (Definition D.2 in [SWZ17]). Given a matrix U ∈ Rn×k, if matrix S ∈ Rm×n satisfies\n∀x ∈ Rk, ‖SUx‖1 ≥ 1\nβ ‖Ux‖1,\nthen S has at most β contraction on U .\nTheorem D.7. Given a tensor A ∈ Rn1×n2×n3 and three matrices V1 ∈ Rn1×b1 , V2 ∈ Rn2×b2 , V3 ∈ Rn3×b3 , let X∗1 ∈ Rb1×k, X∗2 ∈ Rb2×k, X∗3 ∈ Rb3×k satisfies\nX∗1 , X ∗ 2 , X ∗ 3 = arg min X1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖V1X1 ⊗ V2X2 ⊗ V3X3 −A‖1.\nLet S ∈ Rm×n have at most β1 ≥ 1 dilation on V1X∗1 · ((V2X∗2 )> (V3X∗3 )>) − A1 and S have at most β2 ≥ 1 contraction on V1. If X̂1 ∈ Rb1×k, X̂2 ∈ Rb2×k, X̂3 ∈ Rb3×k satisfies\n‖SV1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − SA‖1 ≤ β min X1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖SV1X1 ⊗ V2X2 ⊗ V3X3 − SA‖1,\nwhere β ≥ 1, then\n‖V1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 −A‖1 . β1β2β min X1,X2,X3 ‖V1X1 ⊗ V2X2 ⊗ V3X3 −A‖1.\nThe proof idea is similar to [SWZ17].\nProof. Let A, V1, V2, V3, S,X∗1 , X∗2 , X∗3 , β1, β2 be the same as stated in the theorem. Let X̂1 ∈ Rb1×k, X̂2 ∈ Rb2×k, X̂3 ∈ Rb3×k satisfy\n‖SV1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − SA‖1 ≤ β min X1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖SV1X1 ⊗ V2X2 ⊗ V3X3 − SA‖1.\nWe have,\n‖SV1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − SA‖1 ≥ ‖SV1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − SV1X∗1 ⊗ V2X∗2 ⊗ V3X∗3‖1 − ‖SV1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 − SA‖1 ≥ 1 β2 ‖V1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3‖1 − β1‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖1 ≥ 1 β2 ‖V1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 −A‖1 − 1 β2 ‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖1\n− β1‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖1 = 1\nβ2 ‖V1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 −A‖1 − (\n1\nβ2 + β1)‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖1. (35)\nThe first and the third inequality follow by the triangle inequalities. The second inequality follows using that\n‖SV1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − SV1X∗1 ⊗ V2X∗2 ⊗ V3X∗3‖1 = ∥∥∥SV1(X̂1 −X∗1 ) · ( (V2(X̂2 −X∗2 ))> (V3(X̂3 −X∗3 ))> )∥∥∥ 1 ≥ 1 β2 ∥∥∥V1(X̂1 −X∗1 ) · ( (V2(X̂2 −X∗2 ))> (V3(X̂3 −X∗3 ))> )∥∥∥ 1 ≥ 1 β2 ‖V1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3‖1,\nand\n‖SV1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 − SA‖1 = ‖S(V1X∗1 · ((V2X∗2 )> (V3X∗3 )>)−A1)‖1 ≤ ‖V1X∗1 · ((V2X∗2 )> (V3X∗3 )>)−A1‖1 = β1‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖1. (36)\nThen, we have\n‖V1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 −A‖1 ≤ β2‖SV1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − SA‖1 + (1 + β1β2)‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖1 ≤ β2β‖SV1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 − SA‖1 + (1 + β1β2)‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖1 ≤ β1β2β‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖1 + (1 + β1β2)‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖1 ≤ β(1 + 2β1β2)‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖1.\nThe first inequality follows by Equation (35). The second inequality follows by\n‖SV1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − SA‖1 ≤ β min X1,X2,X3 ‖SV1X1 ⊗ V2X2 ⊗ V3X3 − SA‖1.\nThe third inequality follows by Equation (36). The final inequality follows using that β ≥ 1.\nLemma D.8. Let min(b1, b2, b3) ≥ k. Given three matrices V1 ∈ Rn×b1, V2 ∈ Rn×b2, and V3 ∈ Rn×b3, there exists an algorithm that takes O(nnz(A)) + n poly(b1, b2, b3) time and outputs a tensor\nAlgorithm 21 Reducing the Size of the Objective Function to poly(k)\n1: procedure L1PolyKSizeReduction(A, V1, V2, V3, n, b1, b2, b3, k) . Lemma D.8 2: for i = 1→ 3 do 3: ci ← Õ(bi). 4: Choose sampling and rescaling matrices Ti ∈ Rci×n according to the Lewis weights of Vi. 5: V̂i ← TiVi ∈ Rci×bi . 6: end for 7: C ← A(T1, T2, T3) ∈ Rc1×c2×c3 . 8: return V̂1, V̂2, V̂3 and C. 9: end procedure"
    }, {
      "heading" : "C ∈ Rc1×c2×c3 and three matrices V̂1 ∈ Rc1×b1, V̂2 ∈ Rc2×b2 and V̂3 ∈ Rc3×b3 with c1 = c2 = c3 =",
      "text" : "poly(b1, b2, b3), such that with probability 0.99, for any α ≥ 1, if X ′1, X ′2, X ′3 satisfy that, ∥∥∥∥∥ k∑\ni=1\n(V̂1X ′ 1)i ⊗ (V̂2X ′2)i ⊗ (V̂3X ′3)i − C ∥∥∥∥∥ 1 ≤ α min X1,X2,X3 ∥∥∥∥∥ k∑ i=1 (V̂1X1)i ⊗ (V̂2X2)i ⊗ (V̂3X3)i − C ∥∥∥∥∥ 1 ,\nthen, ∥∥∥∥∥ k∑\ni=1\n(V1X ′ 1)i ⊗ (V2X ′2)i ⊗ (V3X ′3)i −A ∥∥∥∥∥ 1 . α min X1,X2,X3 ∥∥∥∥∥ k∑ i=1 (V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ 1 .\nProof. For simplicity, we define OPT to be\nmin X1,X2,X3 ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥\n1\n.\nLet T1 ∈ Rc1×n sample according to the Lewis weights of V1 ∈ Rn×b1 , where c1 = Õ(b1). Let T2 ∈ Rc2×n sample according to the Lewis weights of V2 ∈ Rn×b2 , where c2 = Õ(b2). Let T3 ∈ Rc3×n sample according to the Lewis weights of V3 ∈ Rn×b3 , where c3 = Õ(b3).\nFor any α ≥ 1, let X ′1 ∈ Rb1×k, X ′2 ∈ Rb2×k, X ′3 ∈ Rb3×k satisfy\n‖T1V1X ′1 ⊗ T2V2X ′2 ⊗ T3V3X ′3 −A(T1, T2, T3)‖1 ≤ α min\nX1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖T1V1X1 ⊗ T2V2X2 ⊗ T3V3X3 −A(T1, T2, T3)‖1.\nFirst, we regard T1 as the sketching matrix for the remainder. Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have\n‖V1X ′1 ⊗ T2V2X ′2 ⊗ T3V3X ′3 −A(I, T2, T3)‖1 . α min\nX1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖V1X1 ⊗ T2V2X2 ⊗ T3V3X3 −A(I, T2, T3)‖1.\nSecond, we regard T2 as a sketching matrix for V1X1 ⊗ V2X2 ⊗ T3V3X3 − A(I, I, T3). Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have\n‖V1X ′1 ⊗ V2X ′2 ⊗ T3V3X ′3 −A(I, I, T3)‖1 . α min\nX1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖V1X1 ⊗ V2X2 ⊗ T3V3X3 −A(I, I, T3)‖1.\nThird, we regard T3 as a sketching matrix for V1X1 ⊗ V2X2 ⊗ V3X3 −A. Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have ∥∥∥∥∥ k∑\ni=1\n(V1X ′ 1)i ⊗ (V2X ′2)i ⊗ (V3X ′3)i −A ∥∥∥∥∥ 1 . α min X1,X2,X3 ∥∥∥∥∥ k∑ i=1 (V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ 1 .\nLemma D.9. Given tensor A ∈ Rn1×n2×n3, and two matrices U ∈ Rn1×s, V ∈ Rn2×s with rank(U) = r, let T ∈ Rt×n1 be a sampling/rescaling matrix according to the Lewis weights of U with t = Õ(r). Then with probability at least 0.99, for all X ′ ∈ Rn3×s, α ≥ 1 which satisfy\n‖T1U ⊗ V ⊗X ′ − T1A‖1 ≤ α · min X∈Rn3×s ‖T1U ⊗ V ⊗X − T1A‖1,\nit holds that\n‖U ⊗ V ⊗X ′ −A‖1 . α · min X∈Rn3×s ‖U ⊗ V ⊗X −A‖1.\nThe proof is similar to the proof of Lemma D.8.\nProof. Let X∗ = arg min X∈Rn3×s ‖U ⊗ V ⊗ X − A‖1. Then according to Lemma D.11 in [SWZ17], T has at most constant dilation (Definition D.5) on U · (V > (X∗)>) − A1, and has at most constant contraction (Definition D.6) on U . We first look at\n‖TU ⊗ V ⊗X ′ − TA‖1 = ‖TU · (V > (X ′)>)− TA1‖1 ≥ ‖TU · ((V > (X ′)>)− (V > (X∗)>))‖1 − ‖TU · (V > (X∗)>)− TA1‖1 ≥ 1 β2 ‖U · ((V > (X ′)>)−A1‖1 − ( 1 β2 + β1)‖U · (V > (X∗)>)−A1‖1,\nwhere β1 ≥ 1, β2 ≥ 1 are two constants. Then we have:\n‖U ⊗ V ⊗X ′ −A‖1 ≤ β2‖TU · (V > (X ′)>)− TA1‖1 + (1 + β1β2)‖U · (V > (X∗)>)−A1‖1 ≤ αβ2‖TU · (V > (X∗)>)− TA1‖1 + (1 + β1β2)‖U · (V > (X∗)>)−A1‖1 ≤ αβ1β2‖U · (V > (X∗)>)−A1‖1 + (1 + β1β2)‖U · (V > (X∗)>)−A1‖1 . α‖U ⊗ V ⊗X∗ −A‖1.\nCorollary D.10. Given tensor A ∈ Rn×n×n, and two matrices U ∈ Rn×s, V ∈ Rn×s with rank(U) = r1, rank(V ) = r2, let T1 ∈ Rt1×n be a sampling/rescaling matrix according to the Lewis weights of U , and let T2 ∈ Rt2×n be a sampling/rescaling matrix according to the Lewis weights of V with t1 = Õ(r1), t2 = Õ(r2). Then with probability at least 0.99, for all X ′ ∈ Rn×s, α ≥ 1 which satisfy\n‖T1U ⊗ T2V ⊗X ′ −A(T1, T2, I)‖1 ≤ α · min X∈Rn×s ‖T1U ⊗ T2V ⊗X −A(T1, T2, I)‖1,\nit holds that\n‖U ⊗ V ⊗X ′ −A‖1 . α · min X∈Rn×s ‖U ⊗ V ⊗X −A‖1.\nProof. We apply Lemma D.9 twice: if\n‖T1U ⊗ T2V ⊗X ′ −A(T1, T2, I)‖1 ≤ α · min X∈Rn×s ‖T1U ⊗ T2V ⊗X −A(T1, T2, I)‖1,\nthen\n‖U ⊗ T2V ⊗X ′ −A(I, T2, I)‖1 . α · min X∈Rn×s ‖U ⊗ T2V ⊗X −A(I, T2, I)‖1.\nThen, we have\n‖U ⊗ V ⊗X ′ −A‖1 . α · min X∈Rn×s ‖U ⊗ V ⊗X −A‖1."
    }, {
      "heading" : "D.4 Solving small problems",
      "text" : "Theorem D.11. Let maxi{ti, di} ≤ n. Given a t1 × t2 × t3 tensor A and three matrices: a t1 × d1 matrix T1, a t2 × d2 matrix T2, and a t3 × d3 matrix T3, if for δ > 0 there exists a solution to\nmin X1,X2,X3 ∥∥∥∥∥ k∑\ni=1\n(T1X1)i ⊗ (T2X2)i ⊗ (T3X3)i −A ∥∥∥∥∥\n1\n:= OPT,\nsuch that each entry of Xi can be expressed using O(nδ) bits, then there exists an algorithm that takes nO(δ) · 2O(d1k+d2k+d3k) time and outputs three matrices: X̂1, X̂2, and X̂3 such that ‖(T1X̂1)⊗ (T2X̂2)⊗ (T3X̂3)−A‖1 = OPT.\nProof. For each i ∈ [3], we can create ti× di variables to represent matrix Xi. Let x denote the list of these variables. Let B denote tensor ∑k i=1(T1X1)i ⊗ (T2X2)i ⊗ (T3X3)i. Then we can write the following objective function,\nmin x\nt1∑\ni=1\nt2∑\nj=1\nt3∑\nl=1\n|Bi,j,l(x)−Ai,j,l|.\nTo remove the | · |, we create t1t2t3 extra variables σi,j,l. Then we obtain the objective function:\nmin x,σ\nt1∑\ni=1\nt2∑\nj=1\nt3∑\nl=1\nσi,j,l(Bi,j,l(x)−Ai,j,l)\ns.t. σ2i,j,l = 1,\nσi,j,l(Bi,j,l(x)−Ai,j,l) ≥ 0, ‖x‖22 + ‖σ‖22 ≤ 2O(n δ)\nwhere the last constraint is unharmful, because there exists a solution that can be written using O(nδ) bits. Note that the number of inequality constraints in the above system is O(t1t2t3), the degree is O(1), and the number of variables is v = (t1t2t3 +d1k+d2k+d3k). Thus by Theorem B.11, we know that the minimum nonzero cost is at least\n(2O(n δ))−2 Õ(v) .\nIt is immediate that the upper bound on cost is at most 2O(nδ), and thus the number of binary search steps is at most log(2O(nδ))2Õ(v). In each step of the binary search, we need to choose a cost C between the lower bound and the upper bound, and write down the polynomial system,\nt1∑\ni=1\nt2∑\nj=1\nt3∑\nl=1\nσi,j,l(Bi,j,l(x)−Ai,j,l) ≤ C,\nσ2i,j,l = 1, σi,j,l(Bi,j,l(x)−Ai,j,l) ≥ 0, ‖x‖22 + ‖σ‖22 ≤ 2O(n δ).\nUsing Theorem B.10, we can determine if there exists a solution to the above polynomial system. Since the number of variables is v, and the degree is O(1), the number of inequality constraints is t1t2t2. Thus, the running time is\npoly(bitsize) · (# constraints ·degree)# variables = nO(δ)2Õ(v)"
    }, {
      "heading" : "D.5 Bicriteria algorithms",
      "text" : "We present several bicriteria algorithms with different tradeoffs. We first present an algorithm that runs in nearly linear time and outputs a solution with rank Õ(k3) in Theorem D.12. Then we show an algorithm that runs in nnz(A) time but outputs a solution with rank poly(k) in Theorem D.13. Then we explain an idea which is able to decrease the cubic rank to quadratic rank, and thus we can obtain Theorem D.14 and Theorem D.15.\nD.5.1 Input sparsity time\nAlgorithm 22 `1-Low Rank Approximation, Bicriteria Algorithm, rank-Õ(k3), Nearly Input Sparsity Time 1: procedure L1BicriteriaAlgorithm(A,n, k) . Theorem D.12 2: s1 ← s2 ← s3 ← Õ(k). 3: For each i ∈ [3], choose Si ∈ Rn2×si to be a dense Cauchy transform. . Part (I) of\nTheorem D.2 4: Compute A1 · S1, A2 · S2, A3 · S3. 5: Y1, Y2, Y3, C ←L1PolyKSizeReduction(A,A1S1, A2S2, A3S3, n, s1, s2, s3, k) .\nAlgorithm 21 6: Form objective function\nmin X∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nXi,j,l(Y1)i ⊗ (Y2)j ⊗ (Y3)l − C ∥∥∥∥∥∥ 1 .\n7: Run `1-regression solver to find X. 8: return A1S1, A2S2, A3S3 and X. 9: end procedure\nTheorem D.12. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, ∈ (0, 1), let r = Õ(k3). There exists an algorithm which takes nnz(A) · Õ(k)+O(n) poly(k)+poly(k) time and outputs three matrices U, V,W ∈ Rn×r such that\n∥∥∥∥∥ r∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥\n1\n≤ Õ(k3/2) log3 n min rank−k Ak ‖Ak −A‖1\nholds with probability 9/10.\nProof. We first choose three dense Cauchy transforms Si ∈ Rn2×si . According to Section B.7, for each i ∈ [3], AiSi can be computed in nnz(A)·Õ(k) time. Then we apply Lemma D.8 (Algorithm 21). We obtain three matrices Y1, Y2, Y3 and a tensor C. Note that for each i ∈ [3], Yi can be computed in n poly(k) time. Because C = A(T1, T2, T3) and T1, T2, T3 ∈ Rn×Õ(k) are three sampling and rescaling matrices, C can be computed in nnz(A) + Õ(k3) time. At the end, we just need to run an `1-regression solver to find the solution to the problem,\nmin X∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nXi,j,l(Y1)i ⊗ (Y2)j ⊗ (Y3)j ∥∥∥∥∥∥ 1 ,\nwhere (Y1)i denotes the i-th column of matrix Y1. Since the size of the above problem is only poly(k), this can be solved in poly(k) time.\nAlgorithm 23 `1-Low Rank Approximation, Bicriteria Algorithm, rank-poly(k), Input Sparsity Time 1: procedure L1BicriteriaAlgorithm(A,n, k) . Theorem D.13 2: s1 ← s2 ← s3 ← Õ(k5). 3: For each i ∈ [3], choose Si ∈ Rn2×si to be a sparse Cauchy transform. . Part (II) of\nTheorem D.4 4: Compute A1 · S1, A2 · S2, A3 · S3. 5: Y1, Y2, Y3, C ←L1PolyKSizeReduction(A,A1S1, A2S2, A3, S3, n, s1, s2, s3, k) .\nAlgorithm 21 6: Form objective function\nmin X∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nXi,j,l(Y1)i ⊗ (Y2)j ⊗ (Y3)l − C ∥∥∥∥∥∥ 1 .\n7: Run `1-regression solver to find X. 8: return A1S1, A2S2, A3S3 and X. 9: end procedure\nTheorem D.13. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, ∈ (0, 1), let r = Õ(k15). There exists an algorithm that takes nnz(A)+O(n) poly(k)+poly(k) time and outputs three matrices U, V,W ∈ Rn×r such that\n∥∥∥∥∥ r∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥\n1\n≤ poly(k, log n) min rank−k Ak ‖Ak −A‖1\nholds with probability 9/10.\nProof. We first choose three dense Cauchy transforms Si ∈ Rn2×si . According to Section B.7, for each i ∈ [3], AiSi can be computed in O(nnz(A)) time. Then we apply Lemma D.8 (Algorithm 21), and can obtain three matrices Y1, Y2, Y3 and a tensor C. Note that for each i ∈ [3], Yi can be computed in O(n) poly(k) time. Because C = A(T1, T2, T3) and T1, T2, T3 ∈ Rn×Õ(k) are three sampling and rescaling matrices, C can be computed in nnz(A) + Õ(k3) time. At the end, we just need to run an `1-regression solver to find the solution to the problem,\nmin X∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nXi,j,l(Y1)i ⊗ (Y2)j ⊗ (Y3)l − C ∥∥∥∥∥∥ 1 ,\nwhere (Y1)i denotes the i-th column of matrix Y1. Since the size of the above problem is only poly(k), it can be solved in poly(k) time.\nD.5.2 Improving cubic rank to quadratic rank\nAlgorithm 24 `1-Low Rank Approximation, Bicriteria Algorithm, rank-Õ(k2), Nearly Input Sparsity Time 1: procedure L1BicriteriaAlgorithm(A,n, k) . Theorem D.14 2: s1 ← s2 ← s3 ← Õ(k). 3: For each i ∈ [3], choose Si ∈ Rn2×si to be a dense Cauchy transform. . Part (I) of\nTheorem D.2 4: Compute A1 · S1, A2 · S2. 5: For each i ∈ [2], choose Ti to be a sampling and rescaling diagonal matrix according to the\nLewis weights of AiSi, with ti = Õ(k) nonzero entries. 6: C ← A(T1, T2, I). 7: Bi+(j−1)s1 ← vec((T1A1S1)i ⊗ (T2A2S2)j),∀i ∈ [s1], j ∈ [s2]. 8: Form objective function minW ‖WB − C3‖1 9: Run `1-regression solver to find Ŵ .\n10: Construct Û by using A1S1 according to Equation (38). 11: Construct V̂ by using A2S2 according to Equation (39). 12: return Û , V̂ , Ŵ . 13: end procedure\nTheorem D.14. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, ∈ (0, 1), let r = Õ(k2). There exists an algorithm which takes nnz(A) · Õ(k)+O(n) poly(k)+poly(k) time and outputs three matrices U, V,W ∈ Rn×r such that\n∥∥∥∥∥ r∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥\n1\n≤ Õ(k3/2) log3 n min rank−k Ak ‖Ak −A‖1\nholds with probability 9/10.\nProof. Let OPT = min Ak∈Rn×n×n\n‖Ak−A‖1.We first choose three dense Cauchy transforms Si ∈ Rn 2×si ,\n∀i ∈ [3]. According to Section B.7, for each i ∈ [3], AiSi can be computed in nnz(A) · Õ(k) time. Then we choose Ti to be a sampling and rescaling diagonal matrix according to the Lewis weights of AiSi, ∀i ∈ [2].\nAccording to Theorem D.4, we have\nmin X1∈Rs1×k,X2∈Rs2×k,X3∈Rs3×k ∥∥∥∥∥ k∑\nl=1\n(A1S1X1)l ⊗ (A2S2X2)l ⊗ (A3S3X3)l −A ∥∥∥∥∥\n1\n≤ Õ(k1.5) log3 nOPT\nNow we fix an l and we have:\n(A1S1X1)l ⊗ (A2S2X2)l ⊗ (A3S3X3)l\n=\n( s1∑\ni=1\n(A1S1)i(X1)i,l\n) ⊗   s2∑\nj=1\n(A2S2)j(X2)j,l  ⊗ (A3S3X3)l\n=\ns1∑\ni=1\ns2∑\nj=1\n(A1S1)i ⊗ (A2S2)j ⊗ (A3S3X3)l(X1)i,l(X2)j,l\nThus, we have\nmin X1,X2,X3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\n(A1S1)i ⊗ (A2S2)j ⊗ ( k∑\nl=1\n(A3S3X3)l(X1)i,l(X2)j,l\n) −A ∥∥∥∥∥∥ 1 ≤ Õ(k1.5) log3 nOPT .\n(37)\nWe create matrix Û ∈ Rn×s1s2 by copying matrix A1S1 s2 times, i.e., Û = [ A1S1 A1S1 · · · A1S1 ] . (38)\nWe create matrix V̂ ∈ Rn×s1s2 by copying the i-th column of A2S2 a total of s1 times into the columns (i− 1)s1, · · · , is1 of V̂ , for each i ∈ [s2], i.e.,\nV̂ = [ (A2S2)1 · · · (A2S2)1 (A2S2)2 · · · (A2S2)2 · · · (A2S2)s2 · · · (A2S2)s2 . ] (39)\nAccording to Equation (37), we have:\nmin W∈Rn×s1s2\n‖Û ⊗ V̂ ⊗W −A‖1 ≤ Õ(k1.5) log3 n ·OPT .\nLet\nŴ = arg min W∈Rn×s1s2\n‖T1Û ⊗ T2V̂ ⊗W −A(T1, T2, I)‖1.\nDue to Corollary D.10, we have\n‖Û ⊗ V̂ ⊗ Ŵ −A‖1 ≤ Õ(k1.5) log3 n ·OPT .\nPutting it all together, we have that Û , V̂ , Ŵ gives a rank-Õ(k2) bicriteria algorithm to the original problem.\nTheorem D.15. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, ∈ (0, 1), let r = Õ(k10). There exists an algorithm which takes nnz(A) + O(n) poly(k) + poly(k) time and outputs three matrices U, V,W ∈ Rn×r such that\n∥∥∥∥∥ r∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥\n1\n≤ poly(k, log n) min rank−k Ak ‖Ak −A‖1\nholds with probability 9/10.\nAlgorithm 25 `1-Low Rank Approximation, Bicriteria Algorithm, rank-poly(k), Input Sparsity Time 1: procedure L1BicriteriaAlgorithm(A,n, k) . Theorem D.15 2: s1 ← s2 ← s3 ← Õ(k5). 3: For each i ∈ [3], choose Si ∈ Rn2×si to be a sparse Cauchy transform. . Part (II) of\nTheorem D.2 4: Compute A1 · S1, A2 · S2. 5: For each i ∈ [2], choose Ti to be a sampling and rescaling diagonal matrix according to the\nLewis weights of AiSi, with ti = Õ(k) nonzero entries. 6: C ← A(T1, T2, I). 7: Bi+(j−1)s1 ← vec((T1A1S1)i ⊗ (T2A2S2)j),∀i ∈ [s1], j ∈ [s2]. 8: Form objective function minW ‖WB − C3‖1. 9: Run `1-regression solver to find Ŵ .\n10: Construct Û by using A1S1 according to Equation (38). 11: Construct V̂ by using A2S2 according to Equation (39). 12: return Û , V̂ , Ŵ . 13: end procedure\nProof. The proof is similar to the proof of Theorem D.14. The only difference is that instead of choosing dense Cauchy matrices S1, S2, we choose sparse Cauchy matrices.\nNotice that if we firstly apply a sparse Cauchy transform, we can reduce the rank of the matrix to poly(k). Then we apply a dense Cauchy transform and can further reduce the dimension while only incurring another poly(k) factor in the approximation ratio. By combining a sparse Cauchy transform and a dense Cauchy transform, we can improve the running time from nnz(A) · Õ(k) to nnz(A).\nCorollary D.16. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, ∈ (0, 1), let r = Õ(k2). There exists an algorithm which takes nnz(A) + O(n) poly(k) + poly(k) time and outputs three matrices U, V,W ∈ Rn×r such that\n∥∥∥∥∥ r∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥\n1\n≤ poly(k, log n) min rank−k Ak ‖Ak −A‖1\nholds with probability 9/10."
    }, {
      "heading" : "D.6 Algorithms",
      "text" : "In this section, we show two different algorithms by using different kind of sketches. One is shown in Theorem D.17 which gives a fast running time. Another one is shown in Theorem D.19 which gives the best approximation ratio.\nD.6.1 Input sparsity time algorithm\nTheorem D.17. Given a 3rd tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm that takes nnz(A) · Õ(k) + O(n) poly(k) + 2Õ(k2) time and outputs three matrices U, V,W ∈ Rn×k such that,\n‖U ⊗ V ⊗W −A‖1 ≤ poly(k, log n) min rank−k A′ ‖A′ −A‖1.\nAlgorithm 26 `1-Low Rank Approximation, Bicriteria Algorithm, rank-Õ(k2), Input Sparsity Time 1: procedure L1BicriteriaAlgorithm(A,n, k) . Corollary D.16 2: s1 ← s2 ← s3 ← Õ(k). 3: For each i ∈ [3], choose Si ∈ Rn2×si to be the composition of a sparse Cauchy transform and\na dense Cauchy transform. . Part (I,II) of Theorem D.2 4: Compute A1 · S1, A2 · S2. 5: For each i ∈ [2], choose Ti to be a sampling and rescaling diagonal matrix according to the\nLewis weights of AiSi, with ti = Õ(k) nonzero entries. 6: C ← A(T1, T2, I). 7: Bi+(j−1)s1 ← vec((T1A1S1)i ⊗ (T2A2S2)j),∀i ∈ [s1], j ∈ [s2]. 8: Form objective function minW ‖WB − C3‖1. 9: Run `1-regression solver to find Ŵ .\n10: Construct Û by using A1S1 according to Equation (38). 11: Construct V̂ by using A2S2 according to Equation (39). 12: return Û , V̂ , Ŵ . 13: end procedure\nAlgorithm 27 `1-Low Rank Approximation, Input sparsity Time Algorithm 1: procedure L1TensorLowRankApproxInputSparsity(A,n, k) . Theorem D.17 2: s1 ← s2 ← s3 ← Õ(k5). 3: Choose Si ∈ Rn2×si to be a dense Cauchy transform, ∀i ∈ [3]. . Part (I) of Theorem D.4 4: Compute A1 · S1, A2 · S2, and A3 · S3. 5: Y1, Y2, Y3, C ←L1PolyKSizeReduction(A,A1S1, A2S2, A3S3, n, s1, s2, s3, k). .\nAlgorithm 21 6: Create variables s1 × k + s2 × k + s3 × k variables for each entry of X1, X2, X3. 7: Form objective function ‖(Y1X1)⊗ (Y2X2)⊗ (Y3X3)− C‖2F . 8: Run polynomial system verifier. 9: return A1S1X1, A2S2X2, A3S3X3.\n10: end procedure\nholds with probability at least 9/10.\nProof. First, we apply part (II) of Theorem D.4. Then AiSi can be computed in O(nnz(A)) time. Second, we use Lemma D.8 to reduce the size of the objective function from O(n3) to poly(k) in n poly(k) time by only losing a constant factor in approximation ratio. Third, we use Claim B.15 to relax the objective function from entry-wise `1-norm to Frobenius norm, and this step causes us to lose some other poly(k) factors in approximation ratio. As a last step, we use Theorem C.45 to solve the Frobenius norm objective function.\nNotice again that if we first apply a sparse Cauchy transform, we can reduce the rank of the matrix to poly(k). Then as before we can apply a dense Cauchy transform to further reduce the dimension while only incurring another poly(k) factor in the approximation ratio. By combining a sparse Cauchy transform and a dense Cauchy transform, we can improve the running time from nnz(A) · Õ(k) to nnz(A), while losing some additional poly(k) factors in approximation ratio.\nCorollary D.18. Given a 3rd tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm that takes nnz(A) +O(n) poly(k) + 2Õ(k2) time and outputs three matrices U, V,W ∈ Rn×k such that,\n‖U ⊗ V ⊗W −A‖1 ≤ poly(k, log n) min rank−k A′ ‖A′ −A‖1.\nholds with probability at least 9/10."
    }, {
      "heading" : "D.6.2 Õ(k3/2)-approximation algorithm",
      "text" : "Algorithm 28 `1-Low Rank Approximation Algorithm, Õ(k3/2)-approximation\n1: procedure L1TensorLowRankApproxK(A,n, k) . Theorem D.19 2: s1 ← s2 ← s3 ← Õ(k). 3: Guess diagonal matrices Si ∈ Rn2×si with si nonzero entries, ∀i ∈ [3]. . Part (III) of\nTheorem D.4 4: Compute A1 · S1, A2 · S2, and A3 · S3. 5: Y1, Y2, Y3, C ←L1PolyKSizeReduction(A,A1S1, A2S2, A3S3, n, s1, s2, s3, k). .\nAlgorithm 21 6: Create s1 × k + s2 × k + s3 × k variables for each entry of X1, X2, X3. 7: Form objective function ‖(Y1X1)⊗ (Y2X2)⊗ (Y3X3)− C‖1. 8: Run polynomial system verifier. 9: return U, V,W .\n10: end procedure\nTheorem D.19. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm that takes nÕ(k)2Õ(k3) time and output three matrices U, V,W ∈ Rn×k such that,\n‖U ⊗ V ⊗W −A‖1 ≤ Õ(k3/2) min rank−k A′ ‖A′ −A‖1.\nholds with probability at least 9/10.\nProof. First, we apply part (III) of Theorem D.4. Then, guessing Si requires nÕ(k) time. Second, we use Lemma D.8 to reduce the size of the objective from O(n3) to poly(k) in polynomial time while only losing a constant factor in approximation ratio. Third, we use Theorem D.11 to solve the entry-wise `1-norm objective function directly."
    }, {
      "heading" : "D.7 CURT decomposition",
      "text" : "Theorem D.20. Given a 3rd order tensor A ∈ Rn×n×n, let k ≥ 1, let UB, VB,WB ∈ Rn×k denote a rank-k, α-approximation to A. Then there exists an algorithm which takes O(nnz(A)) + O(n2) poly(k) time and outputs three matrices: C ∈ Rn×c with columns from A, R ∈ Rn×r with rows from A, T ∈ Rn×t with tubes from A, and a tensor U ∈ Rc×r×t with rank(U) = k such that c = r = t = O(k log k), and\n∥∥∥∥∥∥ c∑\ni=1\nr∑\nj=1\nt∑\nl=1\nUi,j,l · Ci ⊗Rj ⊗ Tl −A ∥∥∥∥∥∥ 1 ≤ Õ(k1.5)α min rank−k A′ ‖A′ −A‖1\nholds with probability 9/10.\nAlgorithm 29 `1-CURT Decomposition Algorithm 1: procedure L1CURT(A,UB, VB,WB, n, k) . Theorem D.20 2: Form B1 = V >B W>B ∈ Rk×n\n2 . 3: Let D>1 ∈ Rn\n2×n2 be the sampling and rescaling diagonal matrix corresponding to the Lewis weights of B>1 , and let D1 have d1 = O(k log k) nonzero entries.\n4: Form Û = A1D1(B1D1)† ∈ Rn×k. 5: Form B2 = Û> W>B ∈ Rk×n\n2 . 6: Let D>2 ∈ Rn\n2×n2 be the sampling and rescaling diagonal matrix corresponding to the Lewis weights of B>2 , and let D2 have d2 = O(k log k) nonzero entries.\n7: Form V̂ = A2D2(B2D2)† ∈ Rn×k. 8: Form B3 = Û> V̂ > ∈ Rk×n2 . 9: Let D>3 ∈ Rn\n2×n2 be the sampling and rescaling diagonal matrix corresponding to the Lewis weights of B>3 , and let D3 have d3 = O(k log k) nonzero entries.\n10: C ← A1D1, R← A2D2, T ← A3D3. 11: U ←∑ki=1((B1D1)†)i ⊗ ((B2D2)†)i ⊗ ((B3D3)†)i. 12: return C, R, T and U . 13: end procedure\nProof. We define\nOPT := min rank−k A′\n‖A′ −A‖1.\nWe already have three matrices UB ∈ Rn×k, VB ∈ Rn×k and WB ∈ Rn×k and these three matrices provide a rank-k, α approximation to A, i.e.,\n∥∥∥∥∥ k∑\ni=1\n(UB)i ⊗ (VB)i ⊗ (WB)i −A ∥∥∥∥∥\n1\n≤ αOPT (40)\nLet B1 = V >B W>B ∈ Rk×n 2 denote the matrix where the i-th row is the vectorization of (VB)i ⊗ (WB)i. By Section B.3, we can compute D1 ∈ Rn 2×n2 which is a sampling and rescaling matrix corresponding to the Lewis weights of B>1 in O(n2 poly(k)) time, and there are d1 = O(k log k) nonzero entries on the diagonal of D1. Let Ai ∈ Rn×n2 denote the matrix obtained by flattening A along the i-th direction, for each i ∈ [3].\nDefine U∗ ∈ Rn×k to be the optimal solution to min U∈Rn×k ‖UB1−A1‖1, Û = A1D1(B1D1)† ∈ Rn×k,\nV0 ∈ Rn×k to be the optimal solution to min V ∈Rn×k ‖V · (Û> W>B )−A2‖1, and U ′ to be the optimal solution to min\nU∈Rn×k ‖UB1D1 −A1D1‖1.\nBy Claim B.13, we have\n‖ÛB1D1 −A1D1‖1 ≤ √ d1‖U ′B1D1 −A1D1‖1\nDue to Lemma D.11 and Lemma D.8 (in [SWZ17]) with constant probability, we have\n‖ÛB1 −A1‖1 ≤ √ d1αD1‖U∗B1 −A1‖1, (41)\nwhere αD1 = O(1).\nRecall that (Û> W>B ) ∈ Rk×n 2 denotes the matrix where the i-th row is the vectorization of\nÛi ⊗ (WB)i, ∀i ∈ [k]. Now, we can show,\n‖V0 · (Û> W>B )−A2‖1 ≤ ‖ÛB1 −A1‖1 by V0 = arg min V ∈Rn×k ‖V · (Û> W>B )−A2‖1\n. √ d1‖U∗B1 −A1‖1 by Equation (41) ≤ √ d1‖UBB1 −A1‖1 by U∗ = arg min\nU∈Rn×k ‖UB1 −A1‖1\n≤ O( √ d1)αOPT by Equation (40) (42)\nWe define B2 = Û> W>B . We can compute D2 ∈ Rn 2×n2 which is a sampling and rescaling matrix corresponding to the Lewis weights of B>2 in O(n2 poly(k)) time, and there are d2 = O(k log k) nonzero entries on the diagonal of D2.\nDefine V ∗ ∈ Rn×k to be the optimal solution of minV ∈Rn×k ‖V B2 −A2‖1, V̂ = A2D2(B2D2)† ∈ Rn×k, W0 ∈ Rn×k to be the optimal solution of min\nW∈Rn×k ‖W · (Û> V̂ >)− A3‖1, and V ′ to be the\noptimal solution of min V ∈Rn×k ‖V B2D2 −A2D2‖1. By Claim B.13, we have\n‖V̂ B2D2 −A2D2‖1 ≤ √ d2‖V ′B2D2 −A2D2‖1.\nDue to Lemma D.11 and Lemma D.8(in [SWZ17]) with constant probability, we have\n‖V̂ B2 −A2‖1 ≤ √ d2αD2‖V ∗B2 −A2‖1, (43)\nwhere αD2 = O(1). Recall that (Û> V̂ >) ∈ Rk×n2 denotes the matrix for which the i-th row is the vectorization of Ûi ⊗ V̂i, ∀i ∈ [k]. Now, we can show,\n‖W0 · (Û> V̂ >)−A3‖1 ≤ ‖V̂ B2 −A2‖1 by W0 = arg min W∈Rn×k ‖W · (Û> V̂ >)−A3‖1\n. √ d2‖V ∗B2 −A2‖1 by Equation (43) ≤ √ d2‖V0B2 −A2‖1 by V ∗ = arg min\nV ∈Rn×k ‖V B2 −A2‖1\n≤ O( √ d1d2)αOPT by Equation (42) (44)\nWe define B3 = Û> V̂ >. We can compute D3 ∈ Rn2×n2 which is a sampling and rescaling matrix corresponding to the Lewis weights of B>3 in O(n2 poly(k)) time, and there are d3 = O(k log k) nonzero entries on the diagonal of D3.\nDefineW ∗ ∈ Rn×k to be the optimal solution to minW∈Rn×k ‖WB3−A3‖1, Ŵ = A3D3(B3D3)† ∈ Rn×k, and W ′ to be the optimal solution to min\nW∈Rn×k ‖WB3D3 −A3D3‖1.\nBy Claim B.13, we have\n‖ŴB3D3 −A3D3‖1 ≤ √ d3‖W ′B3D3 −A3D3‖1.\nDue to Lemma D.11 and Lemma D.8(in [SWZ17]) with constant probability, we have\n‖ŴB3 −A3‖1 ≤ √ d3αD3‖W ∗B3 −A3‖1, (45)\nwhere αD3 = O(1). Now we can show,\n‖ŴB3 −A3‖1 . √ d3‖W ∗B3 −A3‖1, by Equation (45)\n≤ √ d3‖W0B3 −A3‖1, by W ∗ = arg min\nW∈Rn×k ‖WB3 −A3‖1\n≤ O( √ d1d2d3)αOPT by Equation (44)\nThus, it implies, ∥∥∥∥∥ k∑\ni=1\nÛi ⊗ V̂i ⊗ Ŵi −A ∥∥∥∥∥\n1\n≤ poly(k, log n) OPT .\nwhere Û = A1D1(B1D1)†, V̂ = A2D2(B2D2)†, Ŵ = A3D3(B3D3)†.\nAlgorithm 30 `1-CURT decomposition algorithm\n1: procedure L1CURT+(A,n, k) . Theorem D.21 2: UB, VB,WB ←L1LowRankApproximation(A,n, k). . Corollary D.18 3: C,R, T, U ← L1CURT(A,UB, VB,WB, n, k). . Algorithm 29 4: return C, R, T and U . 5: end procedure\nTheorem D.21. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm which takes O(nnz(A)) + O(n2) poly(k) + 2Õ(k2) time and outputs three matrices C ∈ Rn×c with columns from A, R ∈ Rn×r with rows from A, T ∈ Rn×t with tubes from A, and a tensor U ∈ Rc×r×t with rank(U) = k such that c = r = t = O(k log k), and\n∥∥∥∥∥∥ c∑\ni=1\nr∑\nj=1\nt∑\nl=1\nUi,j,l · Ci ⊗Rj ⊗ Tl −A ∥∥∥∥∥∥ 1 ≤ poly(k, log n) min rank−k A′ ‖A′ −A‖1,\nholds with probability 9/10.\nProof. This follows by combining Corollary D.18 and Theorem D.20."
    }, {
      "heading" : "E Entry-wise `p Norm for Arbitrary Tensors, 1 < p < 2",
      "text" : "There is a long line of research dealing with `p norm-related problems [DDH+09, MM13, CDMI+13, CP15, BCKY16, YCRM16, BBC+17].\nIn this section, we provide several different algorithms for tensor `p-low rank approximation. Section E.1 formally states the `p version of Theorem C.1 in [SWZ17]. Section E.2 presents several existence results. Section E.3 describes a tool that is able to reduce the size of the objective function from poly(n) to poly(k). Section E.4 discusses the case when the problem size is small. Section E.5 provides several bicriteria algorithms. Section E.6 summarizes a batch of algorithms. Section E.7 provides an algorithm for `p norm CURT decomposition.\nNotice that if the rank-k solution does not exist, then every bicriteria algorithm in Section E.5 can be stated in the form as Theorem 1.1, and every algorithm which can output a rank-k solution in Section E.6 can be stated in the form as Theorem 1.2. See Section 1 for more details.\nE.1 Existence results for matrix case\nTheorem E.1 ([SWZ17]). Let 1 ≤ p < 2. Given V ∈ Rk×n, A ∈ Rd×n. Let S ∈ Rn×s be a proper random sketching matrix. Let\nÛ = arg min U∈Rd×k\n‖UV S −AS‖2F ,\ni.e.,\nÛ = AS(V S)†.\nThen with probability at least 0.999,\n‖ÛV −A‖pp ≤ α · min U∈Rd×k ‖UV −A‖pp.\n(I). S denotes a dense p-stable transform, s = Õ(k), α = Õ(k1−p/2) log d.\n(II). S denotes a sparse p-stable transform, s = Õ(k5), α = Õ(k5−5p/2+2/p) log d.\n(III). S> denotes a sampling/rescaling matrix according to the `p Lewis weights of V >, s = Õ(k), α = Õ(k1−p/2).\nWe give the proof for completeness.\nProof. Let S ∈ Rn×s be a sketching matrix which satisfies the property (∗): ∀c ≥ 1, Ũ ∈ Rd×k which satisfy\n‖ŨV S −AS‖pp ≤ c · min U∈Rd×k ‖UV S −AS‖pp,\nwe have\n‖ŨV −A‖pp ≤ cβS · min U∈Rd×k ‖UV −A‖pp,\nwhere βS ≥ 1 only depends on the sketching matrix S. Let\n∀i ∈ [d], (Û i)> = arg min x∈Rk ‖x>V S −AiS‖22,\ni.e.,\nÛ = AS(V S)†.\nLet\nŨ = arg min U∈Rd×k\n‖UV S −AS‖pp.\nThen, we have:\n‖ÛV S −AS‖pp\n= d∑\ni=1\n‖Û iV S −AiS‖pp\n≤ d∑\ni=1\n(s1/p−1/2‖Û iV S −AiS‖2)p\n≤ d∑\ni=1\n(s1/p−1/2‖Ũ iV S −AiS‖2)p\n≤ d∑\ni=1\n(s1/p−1/2‖Ũ iV S −AiS‖p)p\n≤ s1−p/2‖ŨV S −AS‖pp.\nThe first inequality follows using ∀x ∈ Rs, ‖x‖p ≤ s1/p−1/2‖x‖2 since p < 2. The third inequality follows using ∀x ∈ Rs, ‖x‖2 ≤ ‖x‖p since p < 2. Thus, according to the property (∗) of S,\n‖ÛV −A‖pp ≤ s1−p/2βS min U∈Rd×k ‖UV −A‖pp.\nDue to Lemma E.8 and Lemma E.11 of [SWZ17], we have: for (I), s = Õ(k), βS = O(log d), α = s1−p/2βS = Õ(k1−p/2) log d, for (II), s = Õ(k5), βS = Õ(k2/p log d), α = s1−p/2βS = Õ(k5−5p/2+2/p) log d, for (III), s = Õ(k), βS = O(1), α = s1−p/2βS = Õ(k1−p/2)."
    }, {
      "heading" : "E.2 Existence results",
      "text" : "Theorem E.2. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, there exist three matrices S1 ∈ Rn2×s1 , S2 ∈ Rn2×s2 , S3 ∈ Rn2×s3 such that\nmin X1,X2,X3 ∥∥∥∥∥ k∑\ni=1\n(A1S1X1)i ⊗ (A2S2X2)i ⊗ (A3S3X3)i −A ∥∥∥∥∥ p\np\n≤ α min rank−k Ak∈Rn×n×n ‖Ak −A‖pp,\nholds with probability 99/100. (I). Using a dense p-stable transform, s1 = s2 = s3 = Õ(k), α = Õ(k3−1.5p) log3 n. (II). Using a sparse p-stable transform, s1 = s2 = s3 = Õ(k 5), α = Õ(k15−7.5p+6/p) log3 n.\n(III). Guessing Lewis weights, s1 = s2 = s3 = Õ(k), α = Õ(k3−1.5p).\nProof. We use OPT to denote\nOPT := min rank−k Ak∈Rn×n×n\n‖Ak −A‖pp.\nGiven a tensor A ∈ Rn1×n2×n3 , we define three matrices A1 ∈ Rn1×n2n3 , A2 ∈ Rn2×n3n1 , A3 ∈ Rn3×n1n2 such that, for any i ∈ [n1], j ∈ [n2], l ∈ [n3]\nAi,j,l = (A1)i,(j−1)·n3+l = (A2)j,(l−1)·n1+i = (A3)l,(i−1)·n2+j .\nWe fix V ∗ ∈ Rn×k and W ∗ ∈ Rn×k, and use V ∗1 , V ∗2 , · · · , V ∗k to denote the columns of V ∗ and W ∗1 ,W ∗ 2 , · · · ,W ∗k to denote the columns of W ∗.\nWe consider the following optimization problem,\nmin U1,··· ,Uk∈Rn ∥∥∥∥∥ k∑\ni=1\nUi ⊗ V ∗i ⊗W ∗i −A ∥∥∥∥∥ p\np\n,\nwhich is equivalent to\nmin U1,··· ,Uk∈Rn ∥∥∥∥∥∥∥∥ [ U1 U2 · · · Uk ]   V ∗1 ⊗W ∗1 V ∗2 ⊗W ∗2 · · ·\nV ∗k ⊗W ∗k\n −A ∥∥∥∥∥∥∥∥ p\np\n.\nWe use matrix Z1 to denote V ∗> W ∗> ∈ Rk×n2 and matrix U to denote [ U1 U2 · · · Uk ] .\nThen we can obtain the following equivalent objective function,\nmin U∈Rn×k\n‖UZ1 −A1‖pp.\nChoose a sketching matrix (a dense p-stable, a sparse p-stable or an `p Lewis weight sampling/rescaling matrix to Z1) S1 ∈ Rn2×s1 . We can obtain the optimization problem,\nmin U∈Rn×k ‖UZ1S1 −A1S1‖pp = min U∈Rn×k\nn∑\ni=1\n‖U iZ1S1 − (A1S1)i‖pp,\nwhere U i denotes the i-th row of matrix U ∈ Rn×k and (A1S1)i denotes the i-th row of matrix A1S1. Instead of solving it under the `p-norm, we consider the `2-norm relaxation,\nmin U∈Rn×k ‖UZ1S1 −A1S1‖2F = min U∈Rn×k\nn∑\ni=1\n‖U iZ1S1 − (A1S1)i‖22.\nLet Û ∈ Rn×k denote the optimal solution of the above optimization problem. Then, Û = A1S1(Z1S1)\n†. We plug Û into the objective function under the `p-norm. By choosing s1 and by the properties of sketching matrices (a dense p-stable, a sparse p-stable or an `p Lewis weight sampling/rescaling matrix to Z1) S1 ∈ Rn2×s1 , we have\n‖ÛZ1 −A1‖pp ≤ α min U∈Rn×k ‖UZ1 −A1‖pp = αOPT .\nThis implies\n‖Û ⊗ V ∗ ⊗W ∗ −A‖pp ≤ αOPT .\nAs a second step, we fix Û ∈ Rn×k and W ∗ ∈ Rn×k, and convert tensor A into matrix A2. Let matrix Z2 denote Û> W ∗>. We consider the following objective function,\nmin V ∈Rn×k\n‖V Z2 −A2‖pp,\nand the optimal cost of it is at most αOPT. We choose a sketching matrix (a dense p-stable, a sparse p-stable or an `p Lewis weight sampling/rescaling matrix to Z2) S2 ∈ Rn2×s2 and sketch on the right of the objective function to obtain the new objective function,\nmin V ∈Rn×k ‖V Z2S2 −A2S2‖pp = min V ∈Rn×k\nn∑\ni=1\n‖V iZ2S2 − (A2S2)i‖pp,\nwhere V i denotes the i-th row of matrix V and (A2S2)i denotes the i-th row of matrix A2S2. Instead of solving this under the `p-norm, we consider the `2-norm relaxation,\nmin V ∈Rn×k ‖V Z2S2 −A2S2‖2F = min V ∈Rn×k\nn∑\ni=1\n‖V i(Z2S2)− (A2S2)i‖22.\nLet V̂ ∈ Rn×k denote the optimal solution of the above problem. Then V̂ = A2S2(Z2S2)†. By properties of sketching matrix S2 ∈ Rn2×s2 , we have,\n‖V̂ Z2 −A2‖pp ≤ α min V ∈Rn×k ‖V Z2 −A2‖pp ≤ α2 OPT,\nwhich implies\n‖Û ⊗ V̂ ⊗W ∗ −A‖pp ≤ α2 OPT,\nAs a third step, we fix the matrices Û ∈ Rn×k and V̂ ∈ Rn×k. We can convert tensor A ∈ Rn×n×n into matrix A3 ∈ Rn2×n. Let matrix Z3 denote Û> V̂ > ∈ Rk×n2 . We consider the following objective function,\nmin W∈Rn×k\n‖WZ3 −A3‖pp,\nand the optimal cost of it is at most α2 OPT. We choose sketching matrix (a dense p-stable, a sparse p-stable or an `p Lewis weight sampling/rescaling matrix to Z3) S3 ∈ Rn2×s3 and sketch on the right of the objective function to obtain the new objective function,\nmin W∈Rn×k\n‖WZ3S3 −A3S3‖pp.\nInstead of solving this under the `p-norm, we consider the `2-norm relaxation,\nmin W∈Rn×k ‖WZ3S3 −A3S3‖2F = min W∈Rn×k\nn∑\ni=1\n‖W i(Z3S3)− (A3S3)i‖22.\nLet Ŵ ∈ Rn×k denote the optimal solution of the above problem. Then Ŵ = A3S3(Z3S3)†. By properties of sketching matrix S3 ∈ Rn2×s3 , we have,\n‖ŴZ3 −A3‖pp ≤ α min W∈Rn×k ‖WZ3 −A3‖pp ≤ α3 OPT .\nThus, we obtain,\nmin X1∈Rs1×k,X2∈Rs2×k,X3∈Rs3×k ∥∥∥∥∥ k∑\ni=1\n(A1S1X1)i ⊗ (A2S2X2)i ⊗ (A3S3X3)i −A ∥∥∥∥∥ p\np\n≤ α3 OPT .\nAccording to Theorem E.1, we let s = s1 = s2 = s3 and take the corresponding α. We can directly get the results for (I), (II) and (III)."
    }, {
      "heading" : "E.3 Polynomial in k size reduction",
      "text" : "Definition E.3 (Definition E.1 in [SWZ17]). Given a matrix M ∈ Rn×d, if matrix S ∈ Rm×n satisfies\n‖SM‖pp ≤ β‖M‖pp,\nthen S has at most β dilation on M in the `p case.\nDefinition E.4 (Definition E.2 in [SWZ17]). Given a matrix U ∈ Rn×k, if matrix S ∈ Rm×n satisfies\n∀x ∈ Rk, ‖SUx‖pp ≥ 1\nβ ‖Ux‖pp,\nthen S has at most β contraction on U in the `p case.\nTheorem E.5. Given a tensor A ∈ Rn1×n2×n3 and three matrices V1 ∈ Rn1×b1 , V2 ∈ Rn2×b2 , V3 ∈ Rn3×b3 , let X∗1 ∈ Rb1×k, X∗2 ∈ Rb2×k, X∗3 ∈ Rb3×k satisfy\nX∗1 , X ∗ 2 , X ∗ 3 = arg min X1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖V1X1 ⊗ V2X2 ⊗ V3X3 −A‖pp.\nLet S ∈ Rm×n have at most β1 ≥ 1 dilation on V1X∗1 · ((V2X∗2 )> (V3X∗3 )>) − A1 and S have at most β2 ≥ 1 contraction on V1 in the `p case. If X̂1 ∈ Rb1×k, X̂2 ∈ Rb2×k, X̂3 ∈ Rb3×k satisfy\n‖SV1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − SA‖pp ≤ β min X1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖SV1X1 ⊗ V2X2 ⊗ V3X3 − SA‖pp,\nwhere β ≥ 1, then\n‖V1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 −A‖pp . β1β2β min X1,X2,X3 ‖V1X1 ⊗ V2X2 ⊗ V3X3 −A‖pp.\nThe proof is essentially the same as the proof of Theorem D.7:\nProof. Let A, V1, V2, V3, S,X∗1 , X∗2 , X∗3 , β1, β2 be as stated in the theorem. Let X̂1 ∈ Rb1×k, X̂2 ∈ Rb2×k, X̂3 ∈ Rb3×k satisfy\n‖SV1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − SA‖pp ≤ β min X1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖SV1X1 ⊗ V2X2 ⊗ V3X3 − SA‖pp.\nSimilar to the proof of Theorem D.7, we have,\n‖SV1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − SA‖pp = 22−2p 1\nβ2 ‖V1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 −A‖pp − (21−p\n1\nβ2 + β1)‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖pp\nThe only difference from the proof of Theorem D.7 is that instead of using triangle inequality, we actually use ‖x+ y‖pp ≤ 2p−1‖x‖pp + ‖y‖pp. Then, we have\n‖V1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 −A‖pp ≤ 22p−2β2‖SV1X̂1 ⊗ V2X̂2 ⊗ V3X̂3 − SA‖pp + (2p−1 + 22p−2β1β2)‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖pp ≤ 22p−2β2β‖SV1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 − SA‖pp + (2p−1 + 22p−2β1β2)‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖pp ≤ 22p−2β1β2β‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖pp + (2p−1 + 22p−2β1β2)‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖pp ≤ 2p−1β(1 + 2β1β2)‖V1X∗1 ⊗ V2X∗2 ⊗ V3X∗3 −A‖pp.\nLemma E.6. Let min(b1, b2, b3) ≥ k. Given three matrices V1 ∈ Rn×b1, V2 ∈ Rn×b2, and V3 ∈ Rn×b3, there exists an algorithm which takes O(nnz(A))+n poly(b1, b2, b3) time and outputs a tensor C ∈ Rc1×c2×c3 and three matrices V̂1 ∈ Rc1×b1, V̂2 ∈ Rc2×b2 and V̂3 ∈ Rc3×b3 with c1 = c2 = c3 = poly(b1, b2, b3), such that with probability 0.99, for any α ≥ 1, if X ′1, X ′2, X ′3 satisfy that, ∥∥∥∥∥ k∑\ni=1\n(V̂1X ′ 1)i ⊗ (V̂2X ′2)i ⊗ (V̂3X ′3)i − C ∥∥∥∥∥ p\np\n≤ α min X1,X2,X3 ∥∥∥∥∥ k∑\ni=1\n(V̂1X1)i ⊗ (V̂2X2)i ⊗ (V̂3X3)i − C ∥∥∥∥∥ p\np\n,\nthen, ∥∥∥∥∥ k∑\ni=1\n(V1X ′ 1)i ⊗ (V2X ′2)i ⊗ (V3X ′3)i −A ∥∥∥∥∥ p\np\n. α min X1,X2,X3 ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ p\np\n.\nProof. For simplicity, we define OPT to be\nmin X1,X2,X3 ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ p\np\n.\nLet T1 ∈ Rc1×n correspond to sampling according to the `p Lewis weights of V1 ∈ Rn×b1 , where c1 = b̃1. Let T2 ∈ Rc2×n be sampling according to the `p Lewis weights of V2 ∈ Rn×b2 , where c2 = b̃2. Let T3 ∈ Rc3×n be sampling according to the `p Lewis weights of V3 ∈ Rn×b3 , where c3 = b̃3.\nFor any α ≥ 1, let X ′1 ∈ Rb1×k, X ′2 ∈ Rb2×k, X ′3 ∈ Rb3×k satisfy ‖T1V1X ′1 ⊗ T2V2X ′2 ⊗ T3V3X ′3 −A(T1, T2, T3)‖pp\n≤ α min X1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖T1V1X1 ⊗ T2V2X2 ⊗ T3V3X3 −A(T1, T2, T3)‖pp.\nFirst, we regard T1 as the sketching matrix for the remainder. Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have\n‖V1X ′1 ⊗ T2V2X ′2 ⊗ T3V3X ′3 −A(I, T2, T3)‖pp . α min\nX1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖V1X1 ⊗ T2V2X2 ⊗ T3V3X3 −A(I, T2, T3)‖pp.\nSecond, we regard T2 as the sketching matrix for V1X1 ⊗ V2X2 ⊗ T3V3X3 − A(I, I, T3). Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have\n‖V1X ′1 ⊗ V2X ′2 ⊗ T3V3X ′3 −A(I, I, T3)‖pp . α min\nX1∈Rb1×k,X2∈Rb2×k,X3∈Rb3×k ‖V1X1 ⊗ V2X2 ⊗ T3V3X3 −A(I, I, T3)‖pp.\nThird, we regard T3 as the sketching matrix for V1X1 ⊗ V2X2 ⊗ V3X3 − A. Then by Lemma D.11 in [SWZ17] and Theorem D.7, we have ∥∥∥∥∥ k∑\ni=1\n(V1X ′ 1)i ⊗ (V2X ′2)i ⊗ (V3X ′3)i −A ∥∥∥∥∥ p\np\n. α min X1,X2,X3 ∥∥∥∥∥ k∑\ni=1\n(V1X1)i ⊗ (V2X2)i ⊗ (V3X3)i −A ∥∥∥∥∥ p\np\n."
    }, {
      "heading" : "E.4 Solving small problems",
      "text" : "Combining Section B.5 in [SWZ17] and the proof of Theorem D.4, for any p = a/b with a, b are integers, we can obtain the `p version of Theorem D.4."
    }, {
      "heading" : "E.5 Bicriteria algorithm",
      "text" : "We present several bicriteria algorithms with different tradeoffs. We first present an algorithm that runs in nearly linear time and outputs a solution with rank Õ(k3) in Theorem E.7. Then we show an algorithm that runs in nnz(A) time but outputs a solution with rank poly(k) in Theorem E.8. Then we explain an idea which is able to decrease the cubic rank to quadratic, and thus we can obtain Theorem E.9.\nTheorem E.7. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, let r = Õ(k3). There exists an algorithm which takes nnz(A) · Õ(k) +n poly(k) + poly(k) time and outputs three matrices U, V,W ∈ Rn×r such that\n∥∥∥∥∥ r∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥ p\np\n≤ Õ(k3−p/2) log3 n min rank−k Ak ‖Ak −A‖pp\nholds with probability 9/10.\nProof. We first choose three dense Cauchy transforms Si ∈ Rn2×si . According to Section B.7, for each i ∈ [3], AiSi can be computed in nnz(A) · Õ(k) time. Then we apply Lemma E.6. We obtain three matrices Y1 = T1A1S1, Y2 = T2A2S2, Y3 = T3A3S3 and a tensor C = A(T1, T2, T3). Note that for each i ∈ [3], Yi can be computed in n poly(k) time. Because C = A(T1, T2, T3) and T1, T2, T3 ∈ Rn×Õ(k) are three sampling and rescaling matrices, C can be computed in nnz(A)+Õ(k3) time. At the end, we just need to run an `p-regression solver to find the solution for the problem:\nmin X∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nXi,j,l(Y1)i ⊗ (Y2)j ⊗ (Y3)j ∥∥∥∥∥∥ p\np\n,\nwhere (Y1)i denotes the i-th column of matrix Y1. Since the size of the above problem is only poly(k), this can be solved in poly(k) time.\nTheorem E.8. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, let r = Õ(k15). There exists an algorithm that takes nnz(A)+n poly(k)+poly(k) time and outputs three matrices U, V,W ∈ Rn×r such that\n∥∥∥∥∥ r∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥ p\np\n≤ poly(k, log n) min rank−k Ak ‖Ak −A‖pp\nholds with probability 9/10.\nProof. We first choose three sparse p-stable transforms Si ∈ Rn2×si . According to Section B.7, for each i ∈ [3], AiSi can be computed in O(nnz(A)) time. Then we apply Lemma E.6, and can obtain three matrices Y1 = T1A1S1, Y2 = T2A2S2, Y3 = T3A3S3 and a tensor C = A(T1, T2, T3). Note that for each i ∈ [3], Yi can be computed in n poly(k) time. Because C = A(T1, T2, T3) and T1, T2, T3 ∈ Rn×Õ(k) are three sampling and rescaling matrices, C can be computed in nnz(A)+Õ(k3) time. At the end, we just need to run an `p-regression solver to find the solution to the problem,\nmin X∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nXi,j,l(Y1)i ⊗ (Y2)j ⊗ (Y3)l − C ∥∥∥∥∥∥ p\np\n,\nwhere (Y1)i denotes the i-th column of matrix Y1. Since the size of the above problem is only poly(k), it can be solved in poly(k) time.\nTheorem E.9. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, ∈ (0, 1), let r = Õ(k2). There exists an algorithm which takes nnz(A) · Õ(k) + n poly(k) + poly(k) time and outputs three matrices U, V,W ∈ Rn×r such that\n∥∥∥∥∥ r∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥ p\np\n≤ Õ(k3−1.5p) log3 n min rank−k Ak ‖Ak −A‖pp\nholds with probability 9/10.\nProof. The proof is similar to Theorem D.14.\nAlgorithm 31 `p-Low Rank Approximation, Bicriteria Algorithm, rank-Õ(k2), Input Sparsity Time 1: procedure LpBicriteriaAlgorithm(A,n, k) . Corollary E.10 2: s1 ← s2 ← s3 ← Õ(k). 3: For each i ∈ [3], choose Si ∈ Rn2×si to be the composition of a sparse p-stable transform\nand a dense p-stable transform. . Part (I,II) of Theorem E.2 4: Compute A1 · S1, A2 · S2. 5: For each i ∈ [2], choose Ti to be a sampling and rescaling diagonal matrix according to the\nLewis weights of AiSi, with ti = Õ(k) nonzero entries. 6: C ← A(T1, T2, I). 7: Bi+(j−1)s1 ← vec((T1A1S1)i ⊗ (T2A2S2)j),∀i ∈ [s1], j ∈ [s2]. 8: Form objective function minW ‖WB − C3‖1. 9: Run `p-regression solver to find Ŵ .\n10: Construct Û by copying (A1S1)i to the (i, j)-th column of Û . 11: Construct V̂ by copying (A2S2)j to the (i, j)-th column of V̂ . 12: return Û , V̂ , Ŵ . 13: end procedure\nAs for `1, notice that if we first apply a sparse Cauchy transform, we can reduce the rank of the matrix to poly(k). Theyn we can apply a dense Cauchy transform and further reduce the dimension, while only incurring another poly(k) factor in the approximation ratio. By combining sparse p-stable and dense p-stable transforms, we can improve the running time from nnz(A) · Õ(k) to be nnz(A) by losing some additional poly(k) factors in the approximation ratio.\nCorollary E.10. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, ∈ (0, 1), let r = Õ(k2). There exists an algorithm which takes nnz(A) +n poly(k) + poly(k) time and outputs three matrices U, V,W ∈ Rn×r such that\n∥∥∥∥∥ r∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥ p\np\n≤ poly(k, log n) min rank−k Ak ‖Ak −A‖pp\nholds with probability 9/10."
    }, {
      "heading" : "E.6 Algorithms",
      "text" : "In this section, we show two different algorithms by using different kind of sketches. One is shown in Theorem E.11 which gives a fast running time. Another one is shown in Theorem E.12 which gives the best approximation ratio.\nTheorem E.11. Given a 3rd tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm which takes O(nnz(A)) + n poly(k) + 2Õ(k2) time and outputs three matrices U, V,W ∈ Rn×k such that,\n‖U ⊗ V ⊗W −A‖pp ≤ poly(k, log n) min rank−k A′ ‖A′ −A‖pp.\nholds with probability at least 9/10.\nProof. First, we apply part (II) of Theorem E.2. Then AiSi can be computed in O(nnz(A)) time. Second, we use Lemma E.6 to reduce the size of the objective function from O(n3) to poly(k) in n poly(k) time by only losing a constant factor in approximation ratio. Third, we use Claim B.15 to relax the objective function from entry-wise `p-norm to Frobenius norm, and this step causes us to lose some other poly(k) factors in approximation ratio. As a last step, we use Theorem C.45 to solve the Frobenius norm objective function.\nTheorem E.12. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm that takes nÕ(k)2Õ(k3) time and output three matrices U, V,W ∈ Rn×k such that,\n‖U ⊗ V ⊗W −A‖pp ≤ Õ(k3−1.5p) min rank−k A′ ‖A′ −A‖pp.\nholds with probability at least 9/10.\nProof. First, we apply part (III) of Theorem E.2. Then, guessing Si requires nÕ(k) time. Second, we use Lemma E.6 to reduce the size of the objective from O(n3) to poly(k) in polynomial time while only losing a constant factor in approximation ratio. Third, we solve the small optimization problem."
    }, {
      "heading" : "E.7 CURT decomposition",
      "text" : "Theorem E.13. Given a 3rd order tensor A ∈ Rn×n×n, let k ≥ 1, and let UB, VB,WB ∈ Rn×k denote a rank-k, α-approximation to A. Then there exists an algorithm which takes O(nnz(A)) + O(n2) poly(k) time and outputs three matrices C ∈ Rn×c with columns from A, R ∈ Rn×r with rows from A, T ∈ Rn×t with tubes from A, and a tensor U ∈ Rc×r×t with rank(U) = k such that c = r = t = O(k log k log log k), and\n∥∥∥∥∥∥ c∑\ni=1\nr∑\nj=1\nt∑\nl=1\nUi,j,l · Ci ⊗Rj ⊗ Tl −A ∥∥∥∥∥∥ p\np\n≤ Õ(k3−1.5p)α min rank−k A′ ‖A′ −A‖pp\nholds with probability 9/10.\nProof. We define\nOPT := min rank−k A′\n‖A′ −A‖pp.\nWe already have three matrices UB ∈ Rn×k, VB ∈ Rn×k and WB ∈ Rn×k and these three matrices provide a rank-k, α approximation to A, i.e.,\n∥∥∥∥∥ k∑\ni=1\n(UB)i ⊗ (VB)i ⊗ (WB)i −A ∥∥∥∥∥ p\np\n≤ αOPT . (46)\nLet B1 = V >B W>B ∈ Rk×n 2 denote the matrix where the i-th row is the vectorization of (VB)i ⊗ (WB)i. By Section B.3 in [SWZ17], we can compute D1 ∈ Rn 2×n2 which is a sampling and rescaling matrix corresponding to the Lewis weights of B>1 in O(n2 poly(k)) time, and there are d1 = O(k log k log log k) nonzero entries on the diagonal of D1. Let Ai ∈ Rn×n2 denote the matrix obtained by flattening A along the i-th direction, for each i ∈ [3].\nDefine U∗ ∈ Rn×k to be the optimal solution to min U∈Rn×k ‖UB1−A1‖pp, Û = A1D1(B1D1)† ∈ Rn×k,\nV0 ∈ Rn×k to be the optimal solution to min V ∈Rn×k ‖V · (Û> W>B )−A2‖ p p, and U ′ to be the optimal solution to min U∈Rn×k\n‖UB1D1 −A1D1‖pp. By Claim B.13, we have\n‖ÛB1D1 −A1D1‖pp ≤ d1−p/21 ‖U ′B1D1 −A1D1‖pp.\nDue to Lemma E.11 and Lemma E.8 in [SWZ17], with constant probability, we have\n‖ÛB1 −A1‖pp ≤ d1−p/21 αD1‖U∗B1 −A1‖pp, (47)\nwhere αD1 = O(1). Recall that (Û> W>B ) ∈ Rk×n\n2 denotes the matrix where the i-th row is the vectorization of Ûi ⊗ (WB)i, ∀i ∈ [k]. Now, we can show,\n‖V0 · (Û> W>B )−A2‖pp ≤ ‖ÛB1 −A1‖pp by V0 = arg min V ∈Rn×k ‖V · (Û> W>B )−A2‖pp\n. d1−p/21 ‖U∗B1 −A1‖pp by Equation (47) ≤ d1−p/21 ‖UBB1 −A1‖pp by U∗ = arg min\nU∈Rn×k ‖UB1 −A1‖pp\n≤ O(d1−p/21 )αOPT . by Equation (46) (48)\nWe define B2 = Û> W>B . We can compute D2 ∈ Rn 2×n2 which is a sampling and rescaling matrix corresponding to the `p Lewis weights of B>2 in O(n2 poly(k)) time, and there are d2 = O(k log k log log k) nonzero entries on the diagonal of D2.\nDefine V ∗ ∈ Rn×k to be the optimal solution of minV ∈Rn×k ‖V B2 −A2‖pp, V̂ = A2D2(B2D2)† ∈ Rn×k, W0 ∈ Rn×k to be the optimal solution of min\nW∈Rn×k ‖W · (Û> V̂ >)− A3‖pp, and V ′ to be the\noptimal solution of min V ∈Rn×k ‖V B2D2 −A2D2‖pp. By Claim B.13, we have\n‖V̂ B2D2 −A2D2‖pp ≤ d1−p/22 ‖V ′B2D2 −A2D2‖pp.\nDue to Lemma E.11 and Lemma E.8 in [SWZ17], with constant probability, we have\n‖V̂ B2 −A2‖pp ≤ d1−p/22 αD2‖V ∗B2 −A2‖pp, (49)\nwhere αD2 = O(1). Recall that (Û> V̂ >) ∈ Rk×n2 denotes the matrix for which the i-th row is the vectorization of Ûi ⊗ V̂i, ∀i ∈ [k]. Now, we can show,\n‖W0 · (Û> V̂ >)−A3‖pp ≤ ‖V̂ B2 −A2‖pp by W0 = arg min\nW∈Rn×k ‖W · (Û> V̂ >)−A3‖pp\n. d1−p/22 ‖V ∗B2 −A2‖pp by Equation (49) ≤ d1−p/22 ‖V0B2 −A2‖pp by V ∗ = arg min\nV ∈Rn×k ‖V B2 −A2‖pp\n≤ O((d1d2)1−p/2)αOPT . by Equation (48) (50)\nWe define B3 = Û> V̂ >. We can compute D3 ∈ Rn2×n2 which is a sampling and rescaling matrix corresponding to the `p Lewis weights of B>3 in O(n2 poly(k)) time, and there are d3 = O(k log k log log k) nonzero entries on the diagonal of D3.\nDefineW ∗ ∈ Rn×k to be the optimal solution to minW∈Rn×k ‖WB3−A3‖pp, Ŵ = A3D3(B3D3)† ∈ Rn×k, and W ′ to be the optimal solution to min\nW∈Rn×k ‖WB3D3 −A3D3‖pp.\nBy Claim B.13, we have\n‖ŴB3D3 −A3D3‖pp ≤ d1−p/23 ‖W ′B3D3 −A3D3‖pp.\nDue to Lemma E.11 and Lemma E.8 in [SWZ17], with constant probability, we have\n‖ŴB3 −A3‖pp ≤ d1−p/23 αD3‖W ∗B3 −A3‖pp, (51)\nwhere αD3 = O(1). Now we can show,\n‖ŴB3 −A3‖pp . d1−p/23 ‖W ∗B3 −A3‖pp, by Equation (51) ≤ d1−p/23 ‖W0B3 −A3‖pp, by W ∗ = arg min\nW∈Rn×k ‖WB3 −A3‖pp\n≤ O((d1d2d3)1−p/2)αOPT . by Equation (50)\nThus, it implies, ∥∥∥∥∥ k∑\ni=1\nÛi ⊗ V̂i ⊗ Ŵi −A ∥∥∥∥∥ p\np\n≤ poly(k, log n) OPT .\nwhere Û = A1D1(B1D1)†, V̂ = A2D2(B2D2)†, Ŵ = A3D3(B3D3)†."
    }, {
      "heading" : "F Robust Subspace Approximation (Asymmetric Norms for Arbitrary Tensors)",
      "text" : "Recently, [CW15b] and [CW15a] study the linear regression problem and low-rank approximation problem under M-Estimator loss functions. In this section, we extend the matrix version of the low rank approximation problem to tensors, i.e., in particular focusing on tensor low-rank approximation under M-Estimator norms. Note that M-Estimators are very different from Frobenius norm and Entry-wise `1 norm, which are symmetric norms. Namely, flattening the tensor objective function along any of the dimensions does not change the cost if the norm is Frobenius or Entry-wise `1- norm. However, for M-Estimator norms, we cannot flatten the tensor along all three dimensions. This property makes the tensor low-rank approximation problem under M-Estimator norms more difficult. This section can be split into two independent parts. Section F.2 studies the `1-`2-`2 norm setting, and Section F.3 studies the `1-`1-`2 norm setting."
    }, {
      "heading" : "F.1 Preliminaries",
      "text" : "Definition F.1 (Nice functions for M -Estimators,M2, Lp, [CW15a]). We say an M -Estimator is nice if M(x) = M(−x), M(0) = 0, M is non-decreasing in |x|, there is a constant CM > 0 and a constant p ≥ 1 so that for all a, b ∈ R>0 with a ≥ b, we have\nCm |a| |b| ≤ M(a) M(b) ≤ (a b )p,\nand also that M(x) 1 p is subadditive, that is, M(x+ y) 1 p ≤M(x) 1 p +M(y) 1 p .\nLetM2 denote the set of such nice M -estimators, for p = 2. Let Lp denote M -Estimators with M(x) = |x|p and p ∈ [1, 2).\nF.2 `1-Frobenius (a.k.a `1-`2-`2) norm\nSection F.2.1 presents basic definitions and facts for the `1-`2-`2 norm setting. Section F.2.2 introduces some useful tools. Section F.2.3 presents the “no dilation” and “no contraction” bounds, which are the key ideas for reducing the problem to a “generalized” Frobenius norm low rank approximation problem. Finally, we provide our algorithms in Section F.2.6."
    }, {
      "heading" : "F.2.1 Definitions",
      "text" : "We first give the definition for the v-norm of a tensor, and then give the definition of the v-norm for a matrix and a weighted version of the v-norm for a matrix.\nDefinition F.2 (Tensor v-norm). For an n× n× n tensor A, we define the v-norm of A, denoted ‖A‖v, to be\n( n∑\ni=1\nM(‖Ai,∗,∗‖F ) )1/p ,\nwhere Ai,∗,∗ is the i-th face of A (along the 1st direction), and p is a parameter associated with the function M(), which defines a nice M -Estimator.\nDefinition F.3 (Matrix v-norm). For an n×d matrix A, we define the v-norm of A, denoted ‖A‖v, to be\nn∑\ni=1\nM(‖Ai,∗‖2)1/p,\nwhere Ai,∗ is the i-th row of A, and p is a parameter associated with the function M(), which defines a nice M -Estimator.\nDefinition F.4. Given matrix A ∈ Rn×d, let Ai,∗ denote the i-th row of A. Let TS ⊂ [n] denote the indices i such that ei is chosen for S. Using a probability vector q and a sampling and rescaling matrix S ∈ Rn×n from q, we will estimate ‖A‖v using S and a re-weighted version, ‖S · ‖v,w′ of ‖ · ‖v, with\n‖SA‖v,w′ =\n ∑\ni∈TS\nw′iM(‖Ai,∗‖2)\n  1/p\n,\nwhere w′i = wi/qi. Since w ′ is generally understood, we will usually just write ‖SA‖v. We will also need an “entrywise row-weighted” version :\n|||SA||| =\n ∑\ni∈TS\nwi qi ‖Ai,∗‖pM\n  1/p\n=\n  ∑\ni∈TS ,j∈[d]\nwi qi M(Ai,j)\n  1/p\n,\nwhere Ai,j denotes the entry in the i-th row and j-th column of A.\nFact F.5. For p = 1, for any two matrices A and B, we have ‖A+B‖v ≤ ‖A‖v + ‖B‖v. For any two tensors A and B, we have ‖A+B‖v ≤ ‖A‖v + ‖B‖v.\nF.2.2 Sampling and rescaling sketches\nNote that Lemmas 42 and 44 in [CW15a] are stronger than stated. In particular, we do not need to assume X is a square matrix. For any m ≥ z, if X ∈ Rd×m, then we have the same result.\nLemma F.6 (Lemma 42 in [CW15a]). Let ρ > 0 and integer z > 0. For sampling matrix S, suppose for a given y ∈ Rd with failure probability δ it holds that ‖SAy‖M = (1 ± 1/10)‖Ay‖M . There is K1 = O(z2/CM ) so that with failure probability δ(KN /CM )(1+p)d, for a constant KN , any rank-z matrix X ∈ Rd×m has the property that if ‖AX‖v ≥ K1ρ, then ‖SAX‖v ≥ ρ, and that if ‖AX‖v ≤ ρ/K1, then ‖SAX‖v ≤ ρ.\nLemma F.7 (Lemma 44 in [CW15a]). Let δ, ρ > 0 and integer z > 0. Given matrix A ∈ Rn×d, there exists a sampling and rescaling matrix S ∈ Rn×n with r = O(γ(A,M,w) −2dz2 log(z/ ) log(1/δ)) nonzero entries such that, with probability at least 1− δ, for any rank-z matrix X ∈ Rd×m, we have either\n‖SAX‖v ≥ ρ,\nor\n(1− )‖AX‖v − ρ ≤ ‖SAX‖v ≤ (1 + )‖AX‖v + ρ.\nLemma F.8 (Lemma 43 in [CW15a]). For r > 0, let r̂ = r/γ(A,M,w), and let q ∈ Rn have\nqi = min{1, r̂γi(A,M,w)}.\nLet S be a sampling and rescaling matrix generated using q, with weights as usual w′i = wi/qi. Let W ∈ Rd×z, and δ > 0. There is an absolute constant C so that for r̂ ≥ Cz log(1/δ)/ 2, with probability at least 1− δ, we have\n(1− )‖AW‖v,w ≤ ‖SAW‖v,w′ ≤ (1 + )‖AW‖v,w."
    }, {
      "heading" : "F.2.3 No dilation and no contraction",
      "text" : "Lemma F.9. Given matrices A ∈ Rn×m, U ∈ Rn×d, let V ∗ = arg min rank−k V ∈Rd×m ‖UV − A‖v. If S ∈ Rs×n has at most c1-dilation on UV ∗ −A, i.e.,\n‖S(UV ∗ −A)‖v ≤ c1‖UV ∗ −A‖v,\nand it has at most c2-contraction on U , i.e.,\n∀x ∈ Rd, ‖SUx‖v ≥ 1\nc2 ‖Ux‖v,\nthen S has at most (c2, c1 + 1c2 )-contraction on (U,A), i.e.,\n∀ rank−k V ∈ Rd×m, ‖SUV − SA‖v ≥ 1\nc2 ‖UV −A‖v − (c1 +\n1\nc2 )‖UV ∗ −A‖v.\nProof. Let A ∈ Rn×m, U ∈ Rn×d and S ∈ Rs×n be the same as that described in the lemma. Let (V − V ∗)j denote the j-th column of V − V ∗. Then ∀ rank−k V ∈ Rd×m,\n‖SUV − SA‖v ≥ ‖SUV − SUV ∗‖v − ‖SUV ∗ − SA‖v ≥ ‖SUV − SUV ∗‖v − c1‖UV ∗ −A‖v = ‖SU(V − V ∗)‖v − c1‖UV ∗ −A‖v\n=\nm∑\nj=1\n‖SU(V − V ∗)j‖v − c1‖UV ∗ −A‖v\n≥ m∑\nj=1\n1 c2 ‖U(V − V ∗)j‖v − c1‖UV ∗ −A‖v\n= 1\nc2 ‖UV − UV ∗‖v − c1‖UV ∗ −A‖v\n≥ 1 c2 ‖UV −A‖v − 1 c2 ‖UV ∗ −A‖v − c1‖UV ∗ −A‖v = 1\nc2 ‖UV −A‖v −\n( ( 1\nc2 + c2)‖UV ∗ −A‖v\n) ,\nwhere the first inequality follows by the triangle inequality, the second inequality follows since S has at most c1 dilation on UV ∗−A, the third inequality follows since S has at most c2 contraction on U , and the fourth inequality follows by the triangle inequality.\nClaim F.10. Given matrix A ∈ Rn×m, for any distribution p = (p1, p2, · · · , pn) define random variable X such that X = ‖Ai‖2/pi with probability pi where Ai is the i-th row of matrix A. Then take m independent samples X1, X2, · · · , Xm, and let Y = 1m ∑m j=1X j. We have\nPr[Y ≤ 1000‖A‖v] ≥ .999.\nProof. We can compute the expectation of Xj , for any j ∈ [m],\nE[Xj ] = n∑\ni=1\n‖Ai‖2 pi · pi = ‖A‖v.\nThen E[Y ] = 1m ∑m j=1 E[X j ] = ‖A‖v. Using Markov’s inequality, we have\nPr[Y ≥ ‖A‖v] ≤ .001.\nLemma F.11. For any fixed U∗ ∈ Rn×d and rank-k V ∗ ∈ Rd×m with d = poly(k), there exists an algorithm that takes poly(n, d) time to compute a sampling and rescaling diagonal matrix S ∈ Rn×n with s = poly(k) nonzero entries such that, with probability at least .999, we have: for all rank-k V ∈ Rd×m,\n‖U∗V ∗ − U∗V ‖v . ‖SU∗V ∗ − SU∗V ‖v . ‖U∗V ∗ − U∗V ‖v.\nLemma F.12 (No dilation). Given matrices A ∈ Rn×m, U∗ ∈ Rn×d with d = poly(k), define V ∗ ∈ Rd×m to be the optimal solution min\nrank−k V ∈Rd×m ‖U∗V −A‖v. Choose a sampling and rescaling\ndiagonal matrix S ∈ Rn×n with s = poly(k) according to Lemma F.8. Then with probability at least .99, we have: for all rank-k V ∈ Rd×m,\n‖SU∗V − SA‖v . ‖U∗V ∗ − U∗V ‖v +O(1)‖U∗V ∗ −A‖v . ‖U∗V −A‖v.\nProof. Using Claim F.10 and Lemma F.11, we have with probability at least .99, for all rank-k V ∈ Rd×m,\n‖SU∗V − SA‖v ≤ ‖SU∗V − SU∗V ∗‖v + ‖SU∗V ∗ − SA‖v by triangle inequality . ‖SU∗V − SU∗V ∗‖v +O(1)‖U∗V ∗ −A‖v by Claim F.10 . ‖U∗V − U∗V ∗‖v +O(1)‖U∗V ∗ −A‖v by Lemma F.11 . ‖U∗V −A‖v + ‖U∗V ∗ −A‖v +O(1)‖U∗V ∗ −A‖v by triangle inequality . ‖U∗V −A‖v.\nLemma F.13 (No contraction). Given matrices A ∈ Rn×m, U∗ ∈ Rn×d with d = poly(k), define V ∗ ∈ Rd×m to be the optimal solution min\nrank−k V ∈Rd×m ‖U∗V −A‖v. Choose a sampling and rescaling\ndiagonal matrix S ∈ Rn×n with s = poly(k) according to Lemma F.8. Then with probability at least .99, we have: for all rank-k V ∈ Rd×m,\n‖U∗V −A‖v . ‖SU∗V − SA‖v +O(1)‖U∗V ∗ −A‖v.\nProof. This follows by Lemma F.9, Claim F.10 and Lemma F.12.\nF.2.4 Oblivious sketches, MSketch\nIn this section, we recall a concept calledM -sketches forM -estimators which is defined in [CW15b]. M -sketch is an oblivious sketch for matrices.\nTheorem F.14 (Theorem 3.1 in [CW15b]). Let OPT denote minx∈Rd ‖Ax − b‖G. There is an algorithm that in O(nnz(A))+poly(d log n) time, with constant probability finds x′ such that ‖Ax′− b‖G ≤ O(1) OPT.\nDefinition F.15 (M-Estimator sketches or MSketch [CW15b]). Given parameters N,n,m, b > 1, define hmax = blogb(n/m)c, β = (b − b−hmax)/(b − 1) and s = Nhmax. For each p ∈ [n], σp, gp, hp are generated (independently) in the following way,\nσp ← ±1, chosen with equal probability, gp ∈ [N ], chosen with equal probability, hp ← t, chosen with probability 1/(βbt) for t ∈ {0, 1, · · ·hmax}.\nFor each p ∈ [n], we define jp = gp +Nhp. Let w ∈ Rs denote the scaling vector such that, for each j ∈ [s],\nwj = { βbhp , if there exists p ∈ [n] s.t.j = jp, 0 otherwise.\nLet S ∈ RNhmax×n be such that, for each j ∈ [s],for each p ∈ [n],\nSj,p = { σp, if j = gp +N · hp, 0, otherwise.\nLet Dw denote the diagonal matrix where the i-th entry on the diagonal is the i-th entry of w. Let S = DwS. We say (S,w) or S is an MSketch.\nDefinition F.16 (Tensor ‖‖v,w-norm). For a tensor A ∈ Rd×n1×n2 and a vector w ∈ , we define\n‖A‖v,w = d∑\ni=1\nwi‖Ai,∗,∗‖F .\nLet (S,w) denote an MSketch, and let S = DwS. If v corresponds to a scale-invariant MEstimator, then for any three matrices U, V,W , we have the following,\n‖(SU)⊗ V ⊗W‖v,w = ‖(DwSU)⊗ V ⊗W‖v = ‖(SU)⊗ V ⊗W‖v.\nFact F.17. For a tensor A ∈ Rn×n×n, let S ∈ Rs×n denote an MSketch (defined in F.15) with s = poly(k, log n). Then SA can be computed in O(nnz(A)) time.\nLemma F.18. For any fixed U∗ ∈ Rn×d and rank-k V ∗ ∈ Rd×m with d = poly(k), let S ∈ Rs×n denote an MSketch (defined in Definition F.15) with s = poly(k, log n) rows. Then with probability at least .999, we have: for all rank-k V ∈ Rd×m,\n‖U∗V ∗ − U∗V ‖v . ‖SU∗V ∗ − SU∗V ‖v . ‖U∗V ∗ − U∗V ‖v.\nLemma F.19 (No dilation, Theorem 3.4 in [CW15b]). Given matrices A ∈ Rn×m, U∗ ∈ Rn×d with d = poly(k), define V ∗ ∈ Rd×m to be the optimal solution to min\nrank−k V ∈Rd×m ‖U∗V − A‖v. Choose\nan MSketch S ∈ Rs×n with s = poly(k, log n) according to Definition F.15. Then with probability at least .99, we have: for all rank-k V ∈ Rd×m,\n‖SU∗V − SA‖v . ‖U∗V ∗ − U∗V ‖v +O(1)‖U∗V ∗ −A‖v . ‖U∗V −A‖v.\nLemma F.20 (No contraction). Given matrices A ∈ Rn×m, U∗ ∈ Rn×d with d = poly(k), define V ∗ ∈ Rd×m to be the optimal solution to min\nrank−k V ∈Rd×m ‖U∗V −A‖v. Choose an MSketch S ∈ Rs×n\nwith s = poly(k, log n) according to Definition F.15. Then with probability at least .99, we have: for all rank-k V ∈ Rd×m,\n‖U∗V −A‖v . ‖SU∗V − SA‖v +O(1)‖U∗V ∗ −A‖v.\nF.2.5 Running time analysis\nLemma F.21. Given a tensor A ∈ Rn×d×d, let S ∈ Rs×n denote an MSketch with s rows. Let SA denote a tensor that has size s× d× d. For each i ∈ {2, 3}, let (SA)i ∈ Rd×ds denote a matrix obtained by flattening tensor SA along the i-th dimension. For each i ∈ {2, 3}, let Si ∈ Rds×si denote a CountSketch transform with si columns. For each i ∈ {2, 3}, let Ti ∈ Rti×d denote a CountSketch transform with ti rows. Then (I) For each i ∈ {2, 3}, (SA)iSi can be computed in O(nnz(A)) time. (II) For each i ∈ {2, 3}, Ti(SA)iSi can be computed in O(nnz(A)) time.\nProof. Proof of Part (I). First note that (SA)2S2 has size n×S2. Thus for each i ∈ [d], j ∈ [s2], we have,\n((SA)2S2)i,j = ds∑\nx′=1\n((SA)2)i,x′(S2)x′,j by (SA)2 ∈ Rd×ds, S2 ∈ Rds×s2\n=\nd∑\ny=1\ns∑\nz=1\n((SA)2)i,(y−1)s+z(S2)(y−1)s+z,j\n= d∑\ny=1\ns∑\nz=1\n(SA)z,i,y(S2)(y−1)s+z,j by unflattening\n=\nd∑\ny=1\ns∑\nz=1\n( n∑\nx=1\nSz,xAx,i,y ) (S2)(y−1)s+z,j\n=\nd∑\ny=1\ns∑\nz=1\nn∑\nx=1\nSz,x ·Ax,i,y · (S2)(y−1)s+z,j .\nFor each nonzero entry Ax,i,y, there is only one z such that Sz,x is nonzero. Thus there is only one j such that (S2)(y−1)s+z,j is nonzero. It means that Ax,i,y can only affect one entry of ((SA)2S2)i,j . Thus, (SA)2S2 can be computed in O(nnz(A)) time. Similarly, we can compute (SA)3S3 in O(nnz(A)) time.\nProof of Part (II). Note that T2(SA)2S2 has size t2×s2. Thus for each i ∈ [t2], j ∈ [s2], we have,\n(T2(SA)2S2)i,j = d∑\nx=1\nds∑\ny′=1\n(T2)i,x((SA)2)x,y′(S2)y′,j by (SA)2 ∈ Rd×ds\n=\nd∑\nx=1\nd∑\ny=1\ns∑\nz=1\n(T2)i,x((SA)2)x,(y−1)s+z(S2)(y−1)s+z,j\n= d∑\nx=1\nd∑\ny=1\ns∑\nz=1\n(T2)i,x(SA)z,x,y(S2)(y−1)s+z,j by unflattening\n= d∑\nx=1\nd∑\ny=1\ns∑\nz=1\n(T2)i,x\n( n∑\nw=1\nSz,wAw,x,y ) (S2)(y−1)s+z,j\n=\nd∑\nx=1\nd∑\ny=1\ns∑\nz=1\nn∑\nw=1\n(T2)i,x · Sz,w ·Aw,x,y · (S2)(y−1)s+z,j .\nFor each nonzero entry Aw,x,y, there is only one z such that Sz,w is nonzero. There is only one i such that (T2)i,x is nonzero. Since there is only one z to make Sz,w nonzero, there is only one j, such that (S2)(y−1)s+z,j is nonzero. Thus, T2(SA)2S2 can be computed in O(nnz(A)) time. Similarly, we can compute T3(SA)3S3 in O(nnz(A)) time."
    }, {
      "heading" : "F.2.6 Algorithms",
      "text" : "We first give a “warm-up” algorithm in Theorem F.22 by using a sampling and rescaling matrix. Then we improve the running time to be polynomial in all the parameters by using an oblivious sketch, and thus we obtain Theorem F.23.\nAlgorithm 32 `1-Frobenius(`1-`2-`2) Low-rank Approximation Algorithm, poly(k)-approximation\n1: procedure L122TensorLowRankApprox(A,n, k) . Theorem F.22 2: ← 0.1. 3: s← poly(k, 1/ ). 4: Guess a sampling and rescaling matrix S ∈ Rs×n. 5: s2 ← s3 ← O(k/ ). 6: r ← s2s3. 7: Choose sketching matrices S2 ∈ Rns×s2 , S3 ∈ Rns×s3 . 8: Compute (SA)2S2, (SA)3S3. 9: Form Ṽ ∈ Rn×r by repeating (SA)2S2 s3 times according to Equation (59).\n10: Form W̃ ∈ Rn×r by repeating (SA)3S3 s2 times according to Equation (60). 11: Form objective function minU∈Rn×r ‖U · (Ṽ > W̃>)−A1‖F . 12: Use a linear regression solver to find a solution Ũ . 13: Take the best solution found over all guesses. 14: return Ũ , Ṽ , W̃ . 15: end procedure\nTheorem F.22. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, let r = O(k2). There exists\nan algorithm which takes npoly(k) time and outputs three matrices U, V,W ∈ Rn×r such that\n‖U ⊗ V ⊗W −A‖v ≤ poly(k) min rank−k A′ ‖A′ −A‖v,\nholds with probability at least 9/10.\nProof. We define OPT as follows,\nOPT = min U,V,W∈Rn×k ‖U ⊗ V ⊗W −A‖v = min U,V,W∈Rn×k ∥∥∥∥∥ k∑\ni=1 Ui ⊗ Vi ⊗Wi −A ∥∥∥∥∥ v .\nLet A1 ∈ Rn×n2 denote the matrix obtained by flattening tensor A along the 1st dimension. Let U∗ ∈ Rn×k denote the optimal solution. We fix U∗ ∈ Rn×k, and consider this objective function,\nmin V,W∈Rn×k ‖U∗ ⊗ V ⊗W −A‖v ≡ min V,W∈Rn×k ∥∥∥U∗ · (V > W>)−A1 ∥∥∥ v , (52)\nwhich has cost at most OPT, and where V > W> ∈ Rk×n2 denotes the matrix for which the i-th row is a vectorization of Vi ⊗Wi, ∀i ∈ [k]. (Note that Vi ∈ Rn is the i-th column of matrix V ∈ Rn×k). Choose a sampling and rescaling diagonal matrix S ∈ Rn×n according to U∗, which has s = poly(k) non-zero entries. Using S to sketch on the left of the objective function when U∗ is fixed (Equation (52)), we obtain a smaller problem,\nmin V,W∈Rn×k ‖(SU∗)⊗ V ⊗W − SA‖v ≡ min V,W∈Rn×k ∥∥∥SU∗ · (V > W>)− SA1 ∥∥∥ v . (53)\nLet V ′,W ′ denote the optimal solution to the above problem, i.e.,\nV ′,W ′ = arg min V,W∈Rn×k ‖(SU∗)⊗ V ⊗W − SA‖v .\nThen using properties (no dilation Lemma F.12 and no contraction Lemma F.13) of S, we have ∥∥U∗ ⊗ V ′ ⊗W ′ −A ∥∥ v ≤ αOPT .\nwhere α is an approximation ratio determined by S. By definition of ‖ · ‖v and ‖ · ‖2 ≤ ‖ · ‖1 ≤ √ dim‖ · ‖2, we can rewrite Equation (53) in the following way,\n‖(SU∗)⊗ V ⊗W − SA‖v\n=\ns∑\ni=1\n  n∑\nj=1\nn∑\nl=1\n( ((SU∗)⊗ V ⊗W )i,j,l − (SA)i,j,l\n)2   1 2\n≤ √s   s∑\ni=1\nn∑\nj=1\nn∑\nl=1\n( ((SU∗)⊗ V ⊗W )i,j,l − (SA)i,j,l\n)2   1 2\n= √ s ‖(SU∗)⊗ V ⊗W − SA‖F . (54)\nGiven the above properties of S and Equation (54), for any β ≥ 1, let V ′′,W ′′ denote a βapproximate solution of min\nV,W∈Rn×k ‖(SU∗)⊗ V ⊗W − SA‖F , i.e.,\n∥∥(SU∗)⊗ V ′′ ⊗W ′′ − SA ∥∥ F ≤ β · min\nV,W∈Rn×k ‖(SU∗)⊗ V ⊗W − SA‖F . (55)\nThen, ∥∥U∗ ⊗ V ′′ ⊗W ′′ −A ∥∥ v ≤ √sαβ ·OPT . (56)\nIn the next few paragraphs we will focus on solving Equation (55). We start by fixing W ∗ ∈ Rn×k to be the optimal solution of\nmin V,W∈Rn×k\n‖(SU∗)⊗ V ⊗W − SA‖F .\nWe use (SA)2 ∈ Rn×ns to denote the matrix obtained by flattening the tensor SA ∈ Rs×n×n along the second direction. We use Z2 = (SU∗)> (W ∗)> ∈ Rk×ns to denote the matrix where the i-th row is the vectorization of (SU∗)i ⊗W ∗i . We can consider the following objective function,\nmin V ∈Rn×k\n‖V Z2 − (SA)2‖F .\nChoosing a sketching matrix S2 ∈ Rns×s2 with s2 = O(k/ ) gives a smaller problem,\nmin V ∈Rn×k\n‖V Z2S2 − (SA)2S2‖F .\nLetting V̂ = (SA)2S2(Z2S2)† ∈ Rn×k, then\n‖V̂ Z2 − (SA)2‖F ≤ (1 + ) min V ∈Rn×k ‖V Z2 − (SA)2‖F\n= (1 + ) min V ∈Rn×k\n‖V ((SU∗)> (W ∗)>)− (SA)2‖F\n= (1 + ) min V ∈Rn×k\n‖(SU∗)⊗ V ⊗W ∗ − SA‖F by unflattening\n= (1 + ) min V,W∈Rn×k\n‖(SU∗)⊗ V ⊗W − SA‖F . by definition of W ∗ (57)\nWe define D2 ∈ Rn2×n2 to be a diagonal matrix obrained by copying the n× n identity matrix s times on n diagonal blocks of D2. Then it has ns nonzero entries. Thus, D2 also can be thought of as a matrix that has size n2 × ns.\nWe can think of (SA)2S2 ∈ Rn×s2 as follows,\n(SA)2S2 = (A(S, I, I))2S2\n= A2︸︷︷︸ n×n2 · D2︸︷︷︸ n2×n2 · S2︸︷︷︸ ns×s2 by D2 can be thought of as having size n2 × ns\n= A2 ·   c2,1In c2,2In\n. . . c2,nIn\n  · S2\nwhere In is an n× n identity matrix, c2,i ≥ 0 for each i ∈ [n], and the number of nonzero c2,i is s. For the last step, we fix SU∗ and V̂ . We use (SA)3 ∈ Rn×ns to denote the matrix obtained by flattening the tensor SA ∈ Rs×n×n along the third direction. We use Z3 = (SU∗)> V̂ > ∈ Rk×ns\nto denote the matrix where the i-th row is the vectorization of (SU∗)i ⊗ V̂i. We can consider the following objective function,\nmin W∈Rn×k\n‖WZ3 − (SA)3‖F .\nChoosing a sketching matrix S3 ∈ Rns×s3 with s3 = O(k/ ) gives a smaller problem,\nmin W∈Rn×k\n‖WZ3S3 − (SA)3S3‖F .\nLet Ŵ = (SA)3S3(Z3S3)† ∈ Rn×k. Then\n‖ŴZ3 − (SA)3‖F ≤ (1 + ) min W∈Rn×k ‖WZ3 − (SA)3‖F by property of S3\n= (1 + ) min W∈Rn×k\n‖W ((SU∗)> V̂ >)− (SA)3‖F by definition Z3\n= (1 + ) min W∈Rn×k\n‖(SU∗)⊗ V̂ ⊗W − SA‖F by unflattening\n≤ (1 + )2 ‖(SU∗)⊗ V ⊗W − SA‖F . by Equation (57)\nWe define D3 ∈ Rn2×n2 to be a diagonal matrix formed by copying the n× n identity matrix s times on n diagonal blocks of D3. Then it has ns nonzero entries. Thus, D3 also can be thought of as a matrix that has size n2 × ns and D3 is uniquely determined by S.\nSimilarly as to the 2nd dimension, for the 3rd dimension, we can think of (SA)3S3 as follows,\n(SA)3S3 = (A(S, I, I))3S3\n= A3︸︷︷︸ n×n2 · D3︸︷︷︸ n2×n2 · S3︸︷︷︸ ns×s3\nby D3 can be thought of as having size n2 × ns\n= A3 ·   c3,1In c3,2In\n. . . c3,nIn\n  · S3\nwhere In is an n× n identity matrix, c3,i ≥ 0 for each i ∈ [n] and the number of nonzero c3,i is s. Overall, we have proved that,\nmin X2,X3\n‖(SU∗)⊗ (A2D2S2X2)⊗ (A3D3S3X3)− SA‖F ≤ (1 + )2 ‖(SU∗)⊗ V ⊗W − SA‖F , (58)\nwhere diagonal matrix D2 ∈ Rn2×n2 (with ns nonzero entries) and D3 ∈ Rn2×n2 (with ns nonzero entries) are uniquely determined by diagonal matrix S ∈ Rn×n (s nonzero entries). Let X ′2 and X ′3 denote the optimal solution to the above problem (Equation (58)). Let V ′′ = (A2D2S2X ′2) ∈ Rn×k and W ′′ = (A3D3S3X ′3) ∈ Rn×k. Then we have\n∥∥U∗ ⊗ V ′′ ⊗W ′′ −A ∥∥ v ≤ √sαβOPT .\nWe construct matrix Ṽ ∈ Rn×s2s3 by copying matrix (SA)2S2 ∈ Rn×s2 s3 times,\nṼ = [ (SA)2S2 (SA)2S2 · · · (SA)2S2. ] (59)\nWe construct matrix W̃ ∈ Rn×s2s3 by copying the i-th column of matrix (SA)3S3 ∈ Rn×s3 into (i− 1)s2 + 1, · · · , is2 columns of W̃ ,\nW̃ = [((SA)3S3)1 · · · ((SA)3S3)1 ((SA)3S3)2 · · · ((SA)3S3)2 · · · ((SA)3S3)s3 · · · ((SA)3S3)s3 .] (60)\nAlthough we don’t know S, we can guess all of the possibilities. For each possibility, we can find a solution Ũ ∈ Rn×s2s3 to the following problem,\nmin U∈Rn×s2s3 ∥∥∥∥∥∥ s2∑\ni=1\ns3∑\nj=1\nU(i−1)s3+j ⊗ ((SA)2S2)i ⊗ ((SA)3S3)j −A ∥∥∥∥∥∥ v\n= min U∈Rn×s2s3 ∥∥∥∥∥∥ s2∑\ni=1\ns3∑\nj=1\nU(i−1)s3+j · vec(((SA)2S2)i ⊗ ((SA)3S3)j)−A1 ∥∥∥∥∥∥ v\n= min U∈Rn×s2s3 ∥∥∥∥∥∥ s2∑\ni=1\ns3∑\nj=1\nU(i−1)s3+j · (Ṽ > W̃>)(i−1)s3+j −A1 ∥∥∥∥∥∥ v\n= min U∈Rn×s2s3 ∥∥∥U · (Ṽ > W̃>)−A1 ∥∥∥ v\n= min U∈Rn×s2s3\n‖UZ −A1‖v\n= min U∈Rn×s2s3\ns2s3∑\ni=1\n‖U iZ −Ai1‖2,\nwhere the first step follows by flattening the tensor along the 1st dimension, U(i−1)s3+j denotes the (i−1)s3 +j-th column of U ∈ Rn×s2s3 , A1 ∈ Rn×n2 denotes the matrix obtained by flattening tensor A along the 1st dimension, the second step follows since Ṽ > W̃> ∈ Rs2s3∈n2 is defined to be the matrix where the (i− 1)s3 + j-th row is vectorization of ((SA)2S2)i ⊗ ((SA)3S3)j , the fourth step follows by defining Z to be Ṽ > W̃>, and the last step follows by definition of ‖ · ‖v norm. Thus, we obtain a multiple regression problem and it can be solved directly by using [CW13, NN13].\nFinally, we take the best Ũ , Ṽ , W̃ over all the guesses. The entire running time is dominated by the number of guesses, which is npoly(k). This completes the proof.\nTheorem F.23. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, let r = O(k2). There exists an algorithm which takes O(nnz(A)) + n poly(k, log n) time and outputs three matrices U, V,W ∈ Rn×r such that\n‖U ⊗ V ⊗W −A‖v ≤ poly(k, log n) min rank−k A′ ‖A′ −A‖v\nholds with probability at least 9/10.\nProof. We define OPT as follows,\nOPT = min U,V,W∈Rn×k ‖U ⊗ V ⊗W −A‖v = min U,V,W∈Rn×k ∥∥∥∥∥ k∑\ni=1 Ui ⊗ Vi ⊗Wi −A ∥∥∥∥∥ v .\nAlgorithm 33 `1-Frobenius(`1-`2-`2) Low-rank Approximation Algorithm, poly(k, log n)approximation 1: procedure L122TensorLowRankApprox(A,n, k) . Theorem F.23 2: ← 0.1. 3: s← poly(k, log n). 4: Choose S ∈ Rs×n to be an MSketch. . Definition F.15 5: s2 ← s3 ← O(k/ ). 6: t2 ← t3 ← poly(k/ ). 7: r ← s2s3. 8: Choose sketching matrices S2 ∈ Rns×s2 , S3 ∈ Rns×s3 . 9: Choose sketching matrices T2 ∈ Rt2×n, T3 ∈ Rt3×n. 10: Compute (SA)2S2, (SA)3S3. 11: Compute T2(SA)2S2, T3(SA)3S3. 12: Form Ṽ ∈ Rn×r by repeating (SA)2S2 s3 times according to Equation (69). 13: Form W̃ ∈ Rn×r by repeating (SA)3S3 s2 times according to Equation (70). 14: Form V ∈ Rt2×r by repeating T2(SA)2S2 s3 times according to Equation (67). 15: Form W ∈ Rt3×r by repeating T3(SA)3S3 s2 times according to Equation (68). 16: C ← A(I, T2, T3). 17: Form objective function minU∈Rn×r ‖U · (V\n> W>)− C1‖F . 18: Use linear regression solver to find a solution Ũ . 19: return Ũ , Ṽ , W̃ . 20: end procedure\nLet A1 ∈ Rn×n2 denote the matrix obtained by flattening tensor A along the 1st dimension. Let U∗ ∈ Rn×k denote the optimal solution. We fix U∗ ∈ Rn×k, and consider the objective function,\nmin V,W∈Rn×k ‖U∗ ⊗ V ⊗W −A‖v ≡ min V,W∈Rn×k ∥∥∥U∗ · (V > W>)−A1 ∥∥∥ v , (61)\nwhich has cost at most OPT, and where V > W> ∈ Rk×n2 denotes the matrix for which the i-th row is a vectorization of Vi ⊗Wi, ∀i ∈ [k]. (Note that Vi ∈ Rn is the i-th column of matrix V ∈ Rn×k). Choose an (oblivious) MSketch S ∈ Rs×n with s = poly(k, log n) according to Definition F.15. Using MSketch S,w to sketch on the left of the objective function when U∗ is fixed (Equation (61)), we obtain a smaller problem,\nmin V,W∈Rn×k ‖(SU∗)⊗ V ⊗W − SA‖v ≡ min V,W∈Rn×k ∥∥∥SU∗ · (V > W>)− SA1 ∥∥∥ v . (62)\nLet V ′,W ′ denote the optimal solution to the above problem, i.e.,\nV ′,W ′ = arg min V,W∈Rn×k ‖(SU∗)⊗ V ⊗W − SA‖v .\nThen using properties (no dilation Lemma F.19 and no contraction Lemma F.20) of S, we have ∥∥U∗ ⊗ V ′ ⊗W ′ −A ∥∥ v ≤ αOPT .\nwhere α is an approximation ratio determined by S.\nBy definition of ‖ · ‖v and ‖ · ‖2 ≤ ‖ · ‖1 ≤ √ dim‖ · ‖2, we can rewrite Equation (62) in the\nfollowing way,\n‖(SU∗)⊗ V ⊗W − SA‖v\n=\ns∑\ni=1\n  n∑\nj=1\nn∑\nl=1\n( ((SU∗)⊗ V ⊗W )i,j,l − (SA)i,j,l\n)2   1 2\n≤ √s   s∑\ni=1\nn∑\nj=1\nn∑\nl=1\n( ((SU∗)⊗ V ⊗W )i,j,l − (SA)i,j,l\n)2   1 2\n= √ s ‖(SU∗)⊗ V ⊗W − SA‖F (63)\nUsing the properties of S and Equation (63), for any β ≥ 1, let V ′′,W ′′ denote a β-approximation solution of min\nV,W∈Rn×k ‖(SU∗)⊗ V ⊗W − SA‖F , i.e.,\n∥∥(SU∗)⊗ V ′′ ⊗W ′′ − SA ∥∥ F ≤ β · min\nV,W∈Rn×k ‖(SU∗)⊗ V ⊗W − SA‖F . (64)\nThen, ∥∥U∗ ⊗ V ′′ ⊗W ′′ −A ∥∥ v ≤ √sαβ ·OPT . (65)\nLet Â denote SA. Choose Si ∈ Rns×si to be Gaussian matrix with si = O(k/ ), ∀i{2, 3}. By a similar proof as in Theorem F.22, we have if X ′2, X ′3 is a β-approximate solution to\nmin X2,X3\n‖(SU∗)⊗ (Â2S2X2)⊗ (Â3S3X3)− SA‖F ,\nthen,\n‖U∗ ⊗ (Â2S2X2)⊗ (Â3S3X3)−A‖v ≤ √ sαβ.\nTo reduce the size of the objective function from poly(n) to poly(k/ ), we use perform an “input sparsity reduction” (in Lemma C.3). Note that, we do not need to use this idea to optimize the running time in Theorem F.22. The running time of Theorem F.22 is dominated by guessing sampling and rescaling matrices. (That running time is nnz(A).) Choose Ti ∈ Rti×n to be a sparse subspace embedding matrix (CountSketch transform) with ti = poly(k, 1/ ), ∀i ∈ {2, 3}. Applying the proof of Lemma C.3 here, we obtain, if X ′2, X ′3 is a β-approximate solution to\nmin X2,X3\n‖(SU∗)⊗ (T2(SA)2S2X2)⊗ (T3(SA)3S3X3)− SA‖F ,\nthen,\n‖U∗ ⊗ ((SA)2S2X2)⊗ ((SA)3S3X3)−A‖v ≤ √ sαβ. (66)\nSimilar to the bicriteria results in Section C.4, Equation (66) indicates that we can construct a bicriteria solution by using two matrices (SA)2S2 and (SA)3S3. The next question is how to obtain the final results Û , V̂ , Ŵ . We first show how to obtain Û . Then we show to construct V̂ and Ŵ .\nTo obtain Û , we need to solve a regression problem related to two matrices V , Ŵ and a tensor A(I, T2, T3). We construct matrix V ∈ Rt2×s2s3 by copying matrix T2(SA)2S2 ∈ Rt2×s2 s3 times,\nV = [ T2(SA)2S2 T2(SA)2S2 · · · T2(SA)2S2 ] . (67)\nWe construct matrix W ∈ Rt3×s2s3 by copying the i-th column of matrix T3(SA)3S3 ∈ Rt3×s3 into (i− 1)s2 + 1, · · · , is2 columns of W ,\nW = [ F1 · · ·F1 F2 · · ·F2 · · · Fs3 · · ·Fs3 ] , (68)\nwhere F = T3(SA)3S3. Thus, to obtain Ũ ∈ Rs2s3 , we just need to use a linear regression solver to solve a smaller problem,\nmin U∈Rs2s3\n‖U · (V > W>)−A(I, T2, T3)‖F ,\nwhich can be solved in O(nnz(A)) + n poly(k, log n) time. We will show how to obtain Ṽ and W̃ . We construct matrix Ṽ ∈ Rn×s2s3 by copying matrix (SA)2S2 ∈ Rn×s2 s3 times,\nṼ = [ (SA)2S2 (SA)2S2 · · · (SA)2S2. ] (69)\nWe construct matrix W̃ ∈ Rn×s2s3 by copying the i-th column of matrix (SA)3S3 ∈ Rn×s3 into (i− 1)s2 + 1, · · · , is2 columns of W̃ ,\nW̃ = [ F1 · · ·F1 F2 · · ·F2 · · · Fs3 · · ·Fs3 ] , (70)\nwhere F = (SA)3S3.\nF.3 `1-`1-`2 norm\nSection F.3.1 presents some definitions and useful facts for the tensor `1-`1-`2 norm. We provide some tools in Section F.3.2. Section F.3.3 presents a key idea which shows we are able to reduce the original problem to a new problem under entry-wise `1 norm. Section F.3.4 presents several existence results. Finally, Section F.3.6 introduces several algorithms with different tradeoffs."
    }, {
      "heading" : "F.3.1 Definitions",
      "text" : "Definition F.24. (Tensor u-norm) For an n×n×n tensor A, we define the u-norm of A, denoted ‖A‖u, to be\n  n∑\ni=1\nn∑\nj=1\nM(‖Ai,j,∗‖2)\n  1/p\n,\nwhere Ai,j,∗ is the (i, j)-th tube of A, and p is a parameter associated with the function M(), which defines a nice M -Estimator.\nDefinition F.25. (Matrix u-norm) For an n× n matrix A, we define u-norm of A, denoted ‖A‖u, to be\n( n∑\ni=1\nM(‖Ai,∗‖2) )1/p ,\nwhere Ai,∗ is the i-th row of A, and p is a parameter associated with the function M(), which defines a nice M -Estimator.\nFact F.26. For p = 1, for any two matrices A and B, we have ‖A+B‖u ≤ ‖A‖u + ‖B‖u. For any two tensors A and B, we have ‖A+B‖u ≤ ‖A‖u + ‖B‖u."
    }, {
      "heading" : "F.3.2 Projection via Gaussians",
      "text" : "Definition F.27. Let p ≥ 1. Let `Sn−1p be an infinite dimensional `p metric which consists of a coordinate for each vector r in the unit sphere Sn−1. Define function f : Sn−1 → R. The `1-norm of any such f is defined as follows:\n‖f‖1 = (∫\nr∈Sn−1 |f(r)|pdr\n)1/p .\nClaim F.28. Let fv(r) = 〈v, r〉. There exists a universal constant αp such that\n‖fv‖p = αp‖v‖2.\nProof. We have,\n‖fv‖p = (∫\nr∈Sn−1 |〈v, r〉|pdr\n)1/p\n=\n(∫\nθ∈Sn−1 ‖v‖p2 · | cos θ|pdθ\n)1/p\n= ‖v‖2 (∫\nθ∈Sn−1 | cos θ|pdθ\n)1/p\n= αp‖v‖2.\nThis completes the proof.\nLemma F.29. Let G ∈ Rk×n denote i.i.d. random Gaussian matrices with rescaling. Then for any v ∈ Rn, we have\nPr[(1− )‖v‖2 ≤ ‖Gv‖1 ≤ (1 + )‖v‖2] ≥ 1− 2−Ω(k 2).\nProof. For each i ∈ [k], we define Xi = 〈v, gi〉, where gi ∈ Rn is the i-th row of G. Then Xi =∑n j=1 vjgi,j and E[|Xi|] = αp‖v‖2. Define Y = ∑k i=1 |Xi|. We have E[Y ] = kα1‖v‖2 = kα1.\nWe can show\nPr[Y ≥ (1 + )α1k] = Pr[esY ≥ es(1+ )α1k] for all s > 0 ≤ E[esY ]/es(1+ )α1k by Markov’s inequality\n= e−s(1+ )α1k ·E[ k∏\ni=1\nes|Xi|] by Y = k∑\ni=1\n|Xi|\n= e−s(1+ )α1k · (E[es|X1|])k\nIt remains to bound E[es|X1|]. Since X1 ∼ N (0, 1), we have that X1 has density function e−t2/2.\nThus, we have,\nE[es|X1|] = 1√ 2π\n∫ +∞\n−∞ es|t| · e−t2/2dt\n= 1√ 2π\n∫ +∞\n−∞ es 2/2 · e−(|t|−s)2/2dt\n= es 2/2(erf(s/ √ 2) + 1)\n≤ es2/2((1− exp(−2s2/π))1/2 + 1) by 1− exp(−4x2/π) ≥ erf(x)2 ≤ es2/2( √ 2/πs+ 1). by 1− e−x ≤ x\nThus, we have\nPr[Y ≥ (1 + )α1k] ≤ e−s(1+ )keks 2/2(1 + s √ 2/π)k\n= e−s(1+ )α1keks 2/2ek·log(1+s\n√ 2/π)\n≤ e−s(1+ )α1k+ks2/2+k·s √ 2/π ≤ e−Ω(k 2). by α1 ≥ √ 2/π and setting s =\nLemma F.30. For any ∈ (0, 1), let k = O(n/ 2). Let G ∈ Rk×n denote i.i.d. random Gaussian matrices with rescaling. Then for any v ∈ Rn, with probability at least 1− 2−Ω(n/ 2), we have : for all v ∈ Rn,\n(1− )‖v‖2 ≤ ‖Gv‖1 ≤ (1 + )‖v‖2.\nProof. Let S denote {y ∈ Rn | ‖y‖2 = 1}. We construct a γ-net so that for all y ∈ S, there exists a vector w ∈ N for which ‖y − w‖2 ≤ γ. We set γ = 1/2.\nFor any unit vector y, we can write\ny = y0 + y1 + y2 + · · · ,\nwhere ‖yi‖2 ≤ 1/2i and yi is a scalar multiple of a vector in N . Thus, we have\n‖Gy‖1 = ‖G(y0 + y1 + y2 + · · · )‖1\n≤ ∞∑\ni=0\n‖Gyi‖1 by triangle inequality\n≤ ∞∑\ni=0\n(1 + )‖yi‖2\n≤ ∞∑\ni=0\n(1 + ) 1\n2i\n≤ 1 + Θ( ).\nSimilarly, we can lower bound ‖Gy‖1 by 1−Θ( ). By Lemma 2.2 in [Woo14], we know that for any γ ∈ (0, 1), there exists a γ-net N of S for which |N | ≤ (1 + 4/γ)n.\nF.3.3 Reduction, projection to high dimension\nLemma F.31. Given a 3rd order tensor A ∈ Rn×n×n, let S ∈ Rn×s denote a Gaussian matrix with s = O(n/ 2) columns. With probability at least 1− 2−Ω(n/ 2), for any U, V,W ∈ Rn×k, we have\n(1− ) ‖U ⊗ V ⊗W −A‖u ≤ ‖(U ⊗ V ⊗W )S −AS‖1 ≤ (1 + ) ‖U ⊗ V ⊗W −A‖u .\nProof. By definition of the ⊗ product between matrices and · product between a tensor and a matrix, we have (U ⊗ V ⊗W )S = U ⊗ V ⊗ (SW ) ∈ Rn×n×s. We use Ai,j,∗ ∈ Rn to denote the (i, j)-th tube (the column in the 3rd dimension) of tensor A. We first prove the upper bound,\n‖(U ⊗ V ⊗W )S −AS‖1 = n∑\ni=1\nn∑\nj=1\n‖((U ⊗ V ⊗W )i,j,∗ −Ai,j,∗)S‖1\n≤ n∑\ni=1\nn∑\nj=1\n(1 + ) ‖(U ⊗ V ⊗W )i,j,∗ −Ai,j,∗‖2\n= (1 + ) ‖U ⊗ V ⊗W −A‖u ,\nwhere the first step follows by definition of tensor ‖·‖u norm, the second step follows by Lemma F.30, and the last step follows by tensor entry-wise `1 norm. Similarly, we can prove the lower bound,\n‖(U ⊗ V ⊗W )S −AS‖1 ≥ n∑\ni=1\nn∑\nj=1\n(1− ) ‖(U ⊗ V ⊗W )i,j,∗ −Ai,j,∗‖2\n= (1− ) ‖U ⊗ V ⊗W −A‖u .\nThis completes the proof.\nCorollary F.32. For any α ≥ 1, if U ′, V ′,W ′ satisfy\n‖(U ′ ⊗ V ′ ⊗W ′ −A)S‖1 ≤ γ min rank−k Ak ‖(Ak −A)S‖1,\nthen\n‖U ′ ⊗ V ′ ⊗W ′ −A‖u ≤ γ 1 +\n1− minrank−k Ak ‖Ak −A‖u.\nProof. Let Û , V̂ , Ŵ denote the optimal solution to minrank−k Ak ‖(Ak − A)S‖1. Let U∗, V ∗,W ∗ denote the optimal solution to minrank−k Ak ‖Ak −A‖u. Then,\n‖U ′ ⊗ V ′ ⊗W ′ −A‖u ≤ 1 1− ‖(U ′ ⊗ V ′ ⊗W ′ −A)S‖1\n≤ γ 1 1− ‖(Û ⊗ V̂ ⊗ Ŵ −A)S‖1 ≤ γ 1 1− ‖(U ∗ ⊗ V ∗ ⊗W ∗ −A)S‖1 ≤ γ 1 + 1− ‖U ∗ ⊗ V ∗ ⊗W ∗ −A‖u,\nwhich completes the proof."
    }, {
      "heading" : "F.3.4 Existence results",
      "text" : "Theorem F.33 (Existence results). Given a 3rd order tensor A ∈ Rn×n×n and a matrix S ∈ Rn×n, let OPT denote minrank−k Ak∈Rn×n×n ‖(Ak − A)S‖1, let Â = AS ∈ Rn×n×n. For any k ≥ 1, there exist three matrices S1 ∈ Rnn×s1, S2 ∈ Rnn×s2, S3 ∈ Rn2×s3 such that\nmin X1∈Rs1×k,X2∈Rs2×k,X3∈Rs3×k\n∥∥∥(Â1S1X1)⊗ (Â2S2X2)⊗ (Â3S3X3)− Â ∥∥∥\n1 ≤ αOPT,\nor equivalently,\nmin X1∈Rs1×k,X2∈Rs2×k,X3∈Rs3×k\n∥∥∥ ( (Â1S1X1)⊗ (Â2S2X2)⊗ (A3S3X3)−A ) S ∥∥∥\n1 ≤ αOPT,\nholds with probability 99/100. (I). Using a dense Cauchy transform, s1 = s2 = s3 = Õ(k), α = Õ(k1.5) log3 n. (II). Using a sparse Cauchy transform, s1 = s2 = s3 = Õ(k 5), α = Õ(k13.5) log3 n.\n(III). Guessing Lewis weights, s1 = s2 = s3 = Õ(k), α = Õ(k1.5).\nProof. We use OPT to denote the optimal cost,\nOPT := min rank−k Ak∈Rn×n×n\n‖(Ak −A)S‖1.\nWe fix V ∗ ∈ Rn×k and W ∗ ∈ Rn×k to be the optimal solution to\nmin U,V,W\n‖(U ⊗ V ⊗W −A)S‖1.\nWe define Z1 ∈ Rk×nn to be the matrix where the i-th row is the vectorization of V ∗i ⊗ (SW ∗i ). We define tensor\nÂ = AS ∈ Rn×n×n.\nThen we also have Â = A(I, I, S) according to the definition of the · product between a tensor and a matrix.\nLet Â1 ∈ Rn×nn denote the matrix obtained by flattening tensor Â along the first direction. We can consider the following optimization problem,\nmin U∈Rn×k\n∥∥∥UZ1 − Â1 ∥∥∥\n1 .\nChoosing S1 to be one of the following sketching matrices: (I) a dense Cauchy transform, (II) a sparse Cauchy transform, (III) a sampling and rescaling diagonal matrix according to Lewis weights.\nLet αS1 denote the approximation ratio produced by the sketching matrix S1. We use S1 ∈ Rnn×s1 to sketch on right of the above problem, and obtain the problem:\nmin U∈Rn×k ‖UZ1S1 − Â1S1‖1 = min U∈Rn×k\nn∑\ni=1\n‖U iZ1S1 − (Â1S1)i‖1,\nwhere U i denotes the i-th row of matrix U ∈ Rn×k and (Â1S1)i denotes the i-th row of matrix Â1S1. Instead of solving it under `1-norm, we consider the `2-norm relaxation,\nmin U∈Rn×k ‖UZ1S1 − Â1S1‖2F = min U∈Rn×k\nn∑\ni=1\n‖U iZ1S1 − (Â1S1)i‖22.\nLet Û ∈ Rn×k denote the optimal solution of the above optimization problem, so that Û = Â1S1(Z1S1)\n†. We plug Û into the objective function under the `1-norm. By the property of sketching matrix S1 ∈ Rnn×s1 , we have,\n‖ÛZ1 − Â1‖1 ≤ αS1 min U∈Rn×k ‖UZ1 − Â1‖1 = αS1 OPT,\nwhich implies that,\n‖Û ⊗ V ∗ ⊗ (SW ∗)− Â‖1 = ‖(Û ⊗ V ∗ ⊗W ∗)S − Â‖1 ≤ αS1 OPT .\nIn the second step, we fix Û ∈ Rn×k and W ∗ ∈ Rn×k. Let Â2 ∈ Rn×nn denote the matrix obtained by flattening tensor Â ∈ Rn×n×n along the second direction. We choose a sketching matrix S2 ∈ Rnn×s2 . Let Z2 = Û> (SW ∗)> ∈ Rk×nn denote the matrix where the i-th row is the vectorization of Ûi ⊗ (SW ∗i ). Define V̂ = Â2S2(Z2S2)†. By the properties of sketching matrix S2, we have\n‖V̂ Z2 − Â2‖1 ≤ αS2αS1 OPT,\nIn the third step, we fix Û ∈ Rn×k and V̂ ∈ Rn×k. Let Â3 ∈ Rn×n2 denote the matrix obtained by flattening tensor Â ∈ Rn×n×n along the third direction. We choose a sketching matrix S3 ∈ Rn2×s3 . Let Z3 ∈ Rk×n2 denote the matrix where the i-th row is the vectorization of Ûi ⊗ V̂i. Define W ′ = Â3S3(Z3S3)† ∈ Rn×k and Ŵ = A3S3(Z3S3)† ∈ Rn×k. Then we have,\nW ′ = Â3S3(Z3S3) †\n= (A(I, I, S))3S3(Z3S3) † = (S>A3)S3(Z3S3) †\n= S>Ŵ\nBy properties of sketching matrix S3, we have\n‖W ′Z3 − Â3‖1 ≤ αS3αS2αS1 OPT .\nReplacing W ′ by S>Ŵ , we obtain,\n‖W ′Z3 − Â3‖1 = ‖S>ŴZ3 − Â3‖1 = ‖S>ŴZ3 − S>A3‖1 = ‖(Û ⊗ V̂ ⊗ Ŵ −A)S‖1.\nThus, we have\nmin X1∈Rs1×k,X2∈Rs2×k,X3∈Rs3×k\n∥∥∥(Â1S1X1)⊗ (Â2S2X2)⊗ (Â3S3X3)− Â ∥∥∥\n1 ≤ αS1αS2αS3 OPT .\nF.3.5 Running time analysis\nFact F.34. Given tensor A ∈ Rn×n×n and a matrix B ∈ Rn×d with d = O(n), let AB denote an n× n× d size tensor, For each i ∈ [3], let (AB)i denote a matrix obtained by flattening tensor AB along the i-th dimension, then\n(AB)1 ∈ Rn×nd, (AB)2 ∈ Rn×nd, (AB)3 ∈ Rd×n 2 .\nFor each i ∈ [3], let Si ∈ Rnd×si denote a sparse Cauchy transform, Ti ∈ Rti×n. Then we have, (I) If T1 denotes a sparse Cauchy transform or a sampling and rescaling matrix according to the Lewis weights, T1(AB)1S1 can be computed in O(nnz(A)d) time. Otherwise, it can be computed in O(nnz(A)d+ ns1t1). (II) If T2 denotes a sparse Cauchy transform or a sampling and rescaling matrix according to the Lewis weights, T2(AB)2S2 can be computed in O(nnz(A)d) time. Otherwise, it can be computed in O(nnz(A)d+ ns2t2). (III) If T3 denotes a sparse Cauchy transform or a sampling and rescaling matrix according to the Lewis weights, T3(AB)3S3 can be computed in O(nnz(A)d) time. Otherwise, it can be computed in O(nnz(A)d+ ds3t3).\nProof. Part (I). Note that T1(AB)1S1 ∈ Rt1×s1 and (AB)1 ∈ Rn×nd, for each i ∈ [t1], j ∈ [s1],\n(T1(AB)1S1)i,j =\nn∑\nx=1\nnd∑\ny′=1\n(T1)i,x((AB)1)x,y′(S1)y′,j\n= n∑\nx=1\nn∑\ny=1\nd∑\nz=1\n(T1)i,x((AB)1)x,(y−1)d+z(S1)(y−1)d+z,j\n= n∑\nx=1\nn∑\ny=1\nd∑\nz=1\n(T1)i,x(AB)x,y,z(S1)(y−1)d+z,j\n=\nn∑\nx=1\nn∑\ny=1\nd∑\nz=1\n(T1)i,x\nn∑\nw=1\n(Ax,y,wBw,z)(S1)(y−1)d+z,j\n= n∑\nx=1\nn∑\ny=1\n(T1)i,x\nn∑\nw=1\nAx,y,w\nd∑\nz=1\nBw,z(S1)(y−1)d+z,j .\nWe look at a non-zero entry Ax,y,w and the entry Bw,z. If T1 denotes a sparse Cauchy transform or a sampling and rescaling matrix according to the Lewis weights, then there is at most one pair (i, j) such that (T1)i,xAx,y,wBw,z(S1)(y−1)d+z,j is non-zero. Therefore, computing T1(AB)1S1 only needs nnz(A)d time. If T1 is not in the above case, since S1 is sparse, we can compute (AB)1S1 in nnz(A)d time by a similar argument. Then, we can compute T1(AB)1S1 in nt1s1 time.\nPart (II). It is as the same as Part (I).\nPart (III). Note that T3(AB)3S3 ∈ Rt3×s3 and (AB)3 ∈ Rd×n2 . For each i ∈ [t3], j ∈ [s3],\n(T3(AB)3S3)i,j = d∑\nx=1\nn2∑\ny′=1\n(T3)i,x((AB)3)x,y′(S3)y′,j\n= d∑\nx=1\nn∑\ny=1\nn∑\nz=1\n(T3)i,x((AB)3)x,(y−1)n+z(S3)(y−1)n+z,j\n=\nd∑\nx=1\nn∑\ny=1\nn∑\nz=1\n(T3)i,x(AB)y,z,x(S3)(y−1)n+z,j\n= d∑\nx=1\nn∑\ny=1\nn∑\nz=1\n(T3)i,x\nn∑\nw=1\nAy,z,wBw,x(S3)(y−1)n+z,j\nSimilar to Part (I), if T1 denotes a sparse Cauchy transform or a sampling and rescaling matrix according to the Lewis weights, computing T3(AB)3S3 only needs nnz(A)d time. Otherwise, it needs dt3s3 + nnz(A)d running time."
    }, {
      "heading" : "F.3.6 Algorithms",
      "text" : "Algorithm 34 `1-`1-`2-Low Rank Approximation algorithm, input sparsity time 1: procedure L112TensorLowRankApproxInputSparsity(A,n, k) . Theorem F.35 2: n← O(n). 3: s1 ← s2 ← s3 ← Õ(k5). 4: Choose S ∈ Rn×n to be a Gaussian matrix. 5: Choose S1 ∈ Rnn×s1 to be a sparse Cauchy transform. . Part (II) of Theorem F.33 6: Choose S2 ∈ Rnn×s2 to be a sparse Cauchy transform. 7: Choose S3 ∈ Rn2×s3 to be a sparse Cauchy transform. 8: Form Â = AS. 9: Compute Â1S1, Â2S2, and Â3S3 10: Y1, Y2, Y3, C ←L1PolyKSizeReduction(Â, Â1S1, Â2S2, Â3S3, n, n, n, s1, s2, s3, k) . Algorithm 21 11: Create s1k + s2k + s3k variables for each entry of X1, X2, X3. 12: Form objective function ‖(Y1X1)⊗ (Y2X2)⊗ (Y3X3)− C‖2F . 13: Run polynomial system verifier. 14: return A1S1X1, A2S2X2, A3S3X3 15: end procedure\nTheorem F.35. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm which takes O(nnz(A)n) + Õ(n) poly(k) + n2Õ(k2) time and outputs three matrices U, V,W ∈ Rn×k such that,\n‖U ⊗ V ⊗W −A‖u ≤ poly(k, log n) min rank−k A′ ‖A′ −A‖u,\nholds with probability at least 9/10.\nProof. We first choose a Gaussian matrix S ∈ Rn×n with n = O(n). By applying Corollary F.32, we can reduce the original problem to a “generalized” `1 low rank approximation problem. Next, we use the existence results (Theorem F.33) and polynomial in k size reduction (Lemma D.8). At the end, we relax the `1-norm objective function to a Frobenius norm objective function (Fact D.1).\nAlgorithm 35 `1-`1-`2-Low Rank Approximation Algorithm, Õ(k2/3)\n1: procedure L112TensorLowRankApproxK(A,n, k) . Theorem F.36 2: n← O(n). 3: s1 ← s2 ← s3 ← Õ(k). 4: Choose S ∈ Rn×n to be a Gaussian matrix. 5: Guess a diagonal matrix S1 ∈ Rnn×s1 with s1 nonzero entries. . Part (III) of Theorem F.33 6: Guess a diagonal matrix S2 ∈ Rnn×s2 with s2 nonzero entries. 7: Guess a diagonal matrix S3 ∈ Rn2×s3 with s3 nonzero entries. 8: Form Â = AS. 9: Compute Â1S1, Â2S2, and Â3S3 10: Y1, Y2, Y3, C ←L1PolyKSizeReduction(Â, Â1S1, Â2S2, Â3S3, n, n, n, s1, s2, s3, k) . Algorithm 21 11: Create s1k + s2k + s3k variables for each entry of X1, X2, X3. 12: Form objective function ‖(Y1X1)⊗ (Y2X2)⊗ (Y3X3)− C‖1. 13: Run polynomial system verifier. 14: return A1S1X1, A2S2X2, A3S3X3 15: end procedure\nTheorem F.36. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, there exists an algorithm which takes nÕ(k)2Õ(k3) time and outputs three matrices U, V,W ∈ Rn×k such that,\n‖U ⊗ V ⊗W −A‖u ≤ O(k3/2) min rank−k A′ ‖A′ −A‖u,\nholds with probability at least 9/10.\nProof. We first choose a Gaussian matrix S ∈ Rn×n with n = O(n). By applying Corollary F.32, we can reduce the original problem to a “generalized” `1 low rank approximation problem. Next, we use the existence results (Theorem F.33) and polynomial in k size reduction (Lemma D.8). At the end, we solve an entry-wise `1 norm objective function directly.\nTheorem F.37. Given a 3rd order tensor A ∈ Rn×n×n, for any k ≥ 1, let r = Õ(k2). There is an algorithm which takes O(nnz(A)n) + Õ(n) poly(k) time and outputs three matrices U, V,W ∈ Rn×r such that\n‖U ⊗ V ⊗W −A‖u ≤ poly(log n, k) min rank−k Ak ‖Ak −A‖u,\nholds with probability at least 9/10.\nProof. We first choose a Gaussian matrix S ∈ Rn×n with n = O(n). By applying Corollary F.32, we can reduce the original problem to a “generalized” `1 low rank approximation problem. Next, we use the existence results (Theorem F.33) and polynomial in k size reduction (Lemma D.8). At the end, we solve an entry-wise `1 norm objective function directly.\nAlgorithm 36 `1-`1-`2-Low Rank Approximation Algorithm, Bicriteria Algorithm 1: procedure L112TensorLowRankApproxBicteriteria(A,n, k) . Theorem F.37 2: n← O(n). 3: s2 ← s3 ← Õ(k5). 4: t2 ← t3 ← Õ(k). 5: r ← s2s3. 6: Choose S ∈ Rn×n to be a Gaussian matrix. 7: Form Â = AS ∈ Rn×n×n. 8: Choose a sketching matrix S2 ∈ Rnn×s2 with s2 nonzero entries (Sparse Cauchy transform),\nfor each i ∈ {2, 3}. . Part (II) of Theorem F.33 9: Choose a sampling and rescaling diagonal matrix Di according to the Lewis weights of ÂiSi\nwith ti nonzero entries, for each i ∈ {2, 3}. 10: Form V̂ ∈ Rn×r by setting the (i, j)-th column to be (Â2S2)i. 11: Form Ŵ ∈ Rn×r by setting the (i, j)-th column to be (A3S3)j . 12: Form matrix B ∈ Rr×t2t3 by setting the (i, j)-th column to be the vectorization of\n(T2Â2S2)i ⊗ (T3Â3S3)j . 13: Solve minU ‖U ·B − (Â(I, T2, T3))1‖1. 14: return Û , V̂ , Ŵ 15: end procedure"
    }, {
      "heading" : "G Weighted Frobenius Norm for Arbitrary Tensors",
      "text" : "This section presents several tensor algorithms for the weighted case. For notational purposes, instead of using U, V,W to denote the ground truth factorization, we use U1, U2, U3 to denote the ground truth factorization. We use A to denote the input tensor, and W to denote the tensor of weights. Combining our new tensor techniques with existing weighted low rank approximation algorithms [RSW16] allows us to obtain several interesting new results. We provide some necessary definitions and facts in Section G.1. Section G.2 provides an algorithm when W has at most r distinct faces in each dimension. Section G.3 studies relationships between r distinct faces and r distinct columns. Finally, we provides an algorithm with a similar running time but weaker assumption, where W has at most r distinct columns and r distinct rows in Section G.4. The result in Theorem G.2 is fairly similar to Theorem G.5, except for the running time. We only put a very detailed discussion in the statement of Theorem G.5. Note that Theorem G.2 also has other versions which are similar to the Frobnius norm rank-k algorithms described in Section 1. For simplicity of presentation, we only present one clean and simple version (which assumes Ak exists and has factor norms which are not too large)."
    }, {
      "heading" : "G.1 Definitions and Facts",
      "text" : "For a matrix A ∈ Rn×m and a weight matrix W ∈ Rn×m, we define ‖W ◦A‖F as follows,\n‖W ◦A‖F =\n  n∑\ni=1\nm∑\nj=1\nW 2i,jA 2 i,j\n  1 2\n.\nFor a tensor A ∈ Rn×n×n and a weight tensor W ∈ Rn×n×n, we define ‖W ◦A‖F as follows,\n‖W ◦A‖F =\n  n∑\ni=1\nn∑\nj=1\nn∑\nl=1\nW 2i,j,lA 2 i,j,l\n  1 2\n.\nFor three matrices A ∈ Rn×m, U ∈ Rn×k, V ∈ Rk×m and a weight matrix W , from one perspective, we have\n‖(UV −A) ◦W‖2F = n∑\ni=1\n‖(U iV −Ai) ◦W i‖22 = n∑\ni=1\n‖(U iV −Ai)DW i‖22,\nwhere W i denote the i-th row of matrix W , and DW i ∈ Rm×m denotes a diagonal matrix where the j-th entry on diagonal is the j-th entry of vector W i. From another perspective, we have\n‖(UV −A) ◦W‖2F = m∑\nj=1\n‖(UVj −Aj) ◦Wj‖22 = m∑\nj=1\n‖(UVj −Aj)DWj‖22,\nwhere Wj denotes the j-th column of matrix W , and DWj ∈ Rn×n denotes a diagonal matrix where the i-th entry on the diagonal is the i-th entry of vector Wj .\nOne of the key tools we use in this section is,\nLemma G.1 (Cramer’s rule). Let R be an n× n invertible matrix. Then, for each i ∈ [n], j ∈ [n],\n(R−1)ji = det(R ¬i ¬j)/det(R),\nwhere R¬i¬j is the matrix R with the i-th row and the j-th column removed.\nG.2 r distinct faces in each dimension\nNotice that in the matrix case, it is sufficient to assume that ‖A′‖F is upper bounded [RSW16]. Once we have that ‖A′‖F is bounded, without loss of generality, we can assume that U∗1 is an orthonormal basis[CW15a, RSW16]. If U∗1 is not an orthonormal basis, then let U ′1R denote a QR factorization of U∗1 , and then write U ′2 = RU∗2 . However, in the case of tensors we have to assume that each factor ‖U∗i ‖F is upper bounded due to border rank issues (see, e.g., [DSL08]).\nTheorem G.2. Given a 3rd order n×n×n tensor A and an n×n×n tensor W of weights with r distinct faces in each of the three dimensions for which each entry can be written using O(nδ) bits, for δ > 0, define OPT = infrank−k Ak‖W ◦ (Ak −A)‖2F . Let k ≥ 1 be an integer and let 0 < < 1.\nIf OPT > 0, and there exists a rank-k Ak = U∗1 ⊗U∗2 ⊗U∗3 tensor (with size n×n×n) such that ‖W ◦ (Ak − A)‖2F = OPT, and maxi∈[3] ‖U∗i ‖F ≤ 2O(n\nδ), then there exists an algorithm that takes (nnz(A) + nnz(W ) + n2Õ(rk\n2/ ))nO(δ) time in the unit cost RAM model with words of size O(log n) bits10 and outputs three n× k matrices U1, U2, U3 such that\n‖W ◦ (U1 ⊗ U2 ⊗ U3 −A)‖2F ≤ (1 + ) OPT (71)\nholds with probability 9/10. 10The entries of A and W are assumed to fit in nδ words.\nAlgorithm 37 Weighted Tensor Low-rank Approximation Algorithm when the Weighted Tensor has r Distinct Faces in Each of the Three Dimensions. procedure WeightedRDistinctFacesIn3Dimensions(A,W, n, r, k, ) . Theorem G.2\nfor j = 1→ 3 do sj ← O(k/ ). Choose a sketching matrix Sj ∈ Rn2×sj . for i = 1→ r do\nCreate k × s1 variables for matrix Pi,j ∈ Rk×sj . end for for i = 1→ n do\nWrite down (Ûj)i = A j iDW j1 SjP > j,i(Pj,iP > j,i) −1.\nend for end for Form ‖W ◦ (Û1 ⊗ Û2 ⊗ Û3 −A)‖2F . Run polynomial system verifier. return U1, U2, U3\nend procedure\nProof. Note thatW has r distinct columns, rows, and tubes. Hence, each of the matricesW1,W2,W3 ∈ Rn×n2 has at most r distinct columns, and at most r distinct rows. Let U∗1 , U∗2 , U∗3 ∈ Rn×k denote the matrices satisfying ‖W ◦ (U∗1 ⊗ U∗2 ⊗ U∗3 − A)‖2F = OPT. We fix U∗2 and U∗3 , and consider a flattening of the tensor along the first dimension,\nmin U1∈Rn×k\n‖(U1Z1 −A1) ◦W1‖2F = OPT,\nwhere matrix Z1 = U∗>2 U∗>3 has size k×n2 and for each i ∈ [k] the i-th row of Z1 is vec((U∗2 )i⊗ (U∗3 )i). For each i ∈ [n], let W i1 denote the i-th row of n× n2 matrix W1. For each i ∈ [n], let DW i1 denote the diagonal matrix of size n2× n2, where each diagonal entry is from the vector W i1 ∈ Rn\n2 . Without loss of generality, we can assume the first r rows of W1 are distinct. We can rewrite the objective function along the first dimension as a sum of multiple regression problems. For any n×k matrix U1,\n‖(U1Z1 −A1) ◦W1‖2F = n∑\ni=1\n‖U i1Z1DW i1 −A i 1DW i1 ‖22. (72)\nBased on the observation thatW1 has r distinct rows, we can group the n rows ofW 1 into r groups. We use g1,1, g1,2, · · · , g1,r to denote r sets of indices such that, for each i ∈ g1,j , W i1 = W j1 . Thus we can rewrite Equation (72),\n‖(U1Z1 −A1) ◦W1‖2F = n∑\ni=1\n‖U i1Z1DW i1 −A i 1DW i1 ‖22\n=\nr∑\nj=1\n∑\ni∈g1,j\n‖U i1Z1DW i1 −A i 1DW i1 ‖22.\nWe can sketch the objective function by choosing Gaussian matrices S1 ∈ Rn2×s1 with s1 = O(k/ ). n∑\ni=1\n‖U i1Z1DW i1S1 −A i 1DW i1 S1‖22.\nLet Û1 denote the optimal solution of the sketch problem,\nÛ1 = arg min U1∈Rn×k\nn∑\ni=1\n‖U i1Z1DW i1S1 −A i 1DW i1 S1‖22.\nBy properties of S1([RSW16]), plugging Û ∈ Rn×k into the original problem, we obtain, n∑\ni=1\n‖Û i1Z1DW i1 −A i 1DW i1 ‖22 ≤ (1 + ) OPT .\nNote that Û1 ∈ Rn×k also has the following form. For each i ∈ [n],\nÛ i1 = A i 1DW i1 S1(Z1DW i1 S1) †\n= Ai1DW i1 S1(Z1DW i1 S1) >((Z1DW i1 S1)(Z1DW i1 S1) >)−1.\nNote that W1 has r distinct rows. Thus, we only have r distinct DW i1 . This implies that there are r distinct matrices Z1DW i1S1 ∈ R\nk×s1 . Using the definition of g1,j , for j ∈ [r], for each i ∈ g1,j ⊂ [n], we have\nÛ i1 = A i 1DW i1 S1(Z1DW i1 S1) †\n= Ai1DW j1 S1(Z1DW j1 S1) † by W i1 = W j 1 ,\nwhich means we only need to write down r different Z1DW j1S1. For each k × s1 matrix Z1DW j1S1, we create k × s1 variables to represent it. Thus, we need to create rks1 variables to represent r matrices,\n{Z1DW 11 S1, Z1DW 21 S1, · · · , Z1DW r1 S1}.\nFor simplicity, let P1,i ∈ Rk×s1 denote Z1DW i1S1. Then we can rewrite Û i ∈ Rk as follows,\nÛ i1 = A i 1DW i1 S1P > 1,i(P1,iP > 1,i) −1.\nIf P1,iP>1,i ∈ Rk×k has rank k, then we can use Cramer’s rule (Lemma G.1) to write down the inverse of P1,iP>1,i. However, vector W i 1 could have many zero entries. Then the rank of P1,iP>1,i can be smaller than k. There are two different ways to solve this issue. One way is by using the argument from [RSW16], which allows us to assume that P1,iP>1,i ∈ Rk×k has rank k. The other way is straightforward: we can guess the rank. There are k possibilities. Let ti ≤ k denote the rank of P1,i. Then we need to figure out a maximal linearly independent subset of rows of P1,i. There are 2O(k) possibilities. Next, we need to figure out a maximal linearly independent subset of columns of P1,i. We can also guess all the possibilities, which is at most 2O(k). Because we have r different P1,i, the total number of guesses we have is at most 2O(rk). Thus, we can write down (P1,iP>1,i)\n−1 according to Cramer’s rule. After Û1 is obtained, we will fix Û1 and U∗3 in the next round. We consider the flattening of the\ntensor along the second direction,\nmin U2∈Rn×k\n‖(U2Z2 −A2) ◦W2‖2F ,\nwhere n×n2 matrix A2 is obtained by flattening tensor A along the second dimension, k×n2 matrix Z2 denotes Û>1 U∗>3 , and n× n2 matrix W2 is obtained by flattening tensor W along the second dimension. For each i ∈ [n], let W i2 denote the i-th row of n × n2 matrix W2. For each i ∈ [n], let DW i1 denote the diagonal matrix which has size n\n2 × n2 and for which each entry is from vector W i2 ∈ Rn\n2 . Without loss of generality, we can assume the first r rows of W2 are distinct. We can rewrite the objective function along the second dimension as a sum of multiple regression problems. For any n× k matrix U2,\n‖(U2Z2 −A2) ◦W2‖2F = n∑\ni=1\n‖U i2Z2DW i2 −A i 2DW i2 ‖22. (73)\nBased on the observation thatW2 has r distinct rows, we can group the n rows ofW 2 into r groups. We use g2,1, g2,2, · · · , g2,r to denote r sets of indices such that, for each i ∈ g2,j , W i2 = W j2 . Thus we obtain,\n‖(U2Z2 −A2) ◦W2‖2F = n∑\ni=1\n‖U i2Z2DW i2 −A i 2DW i2 ‖22\n=\nr∑\nj=1\n∑\ni∈g2,j\n‖U i2Z2DW i2 −A i 2DW i2 ‖22.\nWe can sketch the objective function by choosing a Gaussian sketch S2 ∈ Rn2×s2 with s2 = O(k/ ). Let Û2 denote the optimal solution to the sketch problem. Then Û2 has the form, for each i ∈ [n],\nÛ i2 = A i 2DW i2 S2(Z2DW i2 S2) †.\nSimilarly as before, we only need to write down r different matrices Z2DW i2S1, and for each of them, create k × s2 variables. Let P2,i ∈ Rk×s2 denote Z2DW i2S2. By our guessing argument, we can obtain Û2.\nIn the last round, we fix Û1 and Û2. We then write down Û3. Overall, by creating l = O(rk2/ ) variables, we have rational polynomials Û1(x), Û2(x), Û3(x). Putting it all together, we can write this objective function,\nmin x∈Rl\n‖(Û1(x)⊗ Û2(x)⊗ Û3(x)−A) ◦W‖2F .\ns.t. h1,i(x) 6= 0, ∀i ∈ [r]. h2,i(x) 6= 0, ∀i ∈ [r]. h3,i(x) 6= 0, ∀i ∈ [r].\nwhere h1,i(x) denotes the denominator polynomial related to a full rank sub-block of P1,i(x). By a perturbation argument in Section 4 in [RSW16], we know that the h1,i(x) are nonzero. By a similar argument as in Section 5 in [RSW16], we can show a lower bound on the cost of the denominator polynomial h1,i(x). Thus we can create new bounded variables xl+1, · · · , x3r+l to rewrite the objective function,\nmin x∈Rl+3r q(x)/p(x).\ns.t. h1,i(x)xl+i = 0, ∀i ∈ [r]. h2,i(x)xl+r+i = 0, ∀i ∈ [r]. h3,i(x)xl+2r+i = 0,∀i ∈ [r].\np(x) =\nr∏\ni=1\nh21,i(x)h 2 2,i(x)h 2 3,i(x)\nNote that the degree of the above system is poly(kr) and all the equality constraints can be merged into one single constraint. Thus, the number of constraints is O(1). The number of variables is O(rk2/ ).\nUsing Theorem B.11 and a similar argument from Section 5 of [RSW16], we have that the minimum nonzero cost is at least 2−nδ2Õ(rk\n2/ ) . Combining the binary search explained in Section C(similar techniques also can be found in Section 6 of [RSW16]) with the lower bound we obtained, we can find the solution for the original problem in time,\n(nnz(A) + nnz(W ) + n2Õ(rk 2/ ))nO(δ).\nG.3 r distinct columns, rows and tubes\nLemma G.3. Let W ∈ Rn×n×n denote a tensor that has r distinct columns and r distinct rows, then W has (I) r distinct column-tube faces. (II) r distinct row-tube faces.\nProof. Proof of Part (I). Without loss of generality, we consider the first (which is the bottom one) column-row face. Assume it has r distinct rows and r distinct columns. We can re-order all the column-tube faces to make sure that all the n columns in the bottom face have been split into r continuous disjoint groups Ci, e.g., {C1, C2, · · · , Cr} = [n]. Next, we can re-order all the row-tube faces to make sure that all the n rows in the bottom face have been split into r continuous disjoint groups Ri, e.g., {R1, R2, · · · , Rr} = [n]. Thus, the new bottom face can be regarded as r×r groups, and the number in each position of the same group is the same.\nSuppose that the tensor has r+ 1 distinct column-tube faces. By the pigeonhole principle there exist two different column-tube faces belonging to the same group Ci, for some i ∈ [r]. Note that these two column-tube faces are the same by looking at the bottom (column-row) face. Since they are distinct faces, there must exist one row vector v which is not in the bottom (column-row) face, and it has a different value in coordinates belong to group Ci. Note that, considering the bottom face, for each row vector, it has the same value over coordinates belonging to group Ci. But v has different values in coordinates belong to group Ci. Also, note that the bottom (column-row) face also has r distinct rows, and v is not one of them. This means there are at least r + 1 distinct rows, which contradicts that there are r distinct rows in total. Thus, there are at most r distinct column-tube faces.\nProof of Part (II). It is similar to Part (I).\nCorollary G.4. Let W ∈ Rn×n×n denote a tensor that has r distinct columns, r distinct rows, and r distinct rubes. Then W has r distinct column-tube faces, r distinct row-tube faces, and r distinct column-row faces.\nProof. This follows by applying Lemma G.3 twice.\nThus, we obtain the same result as in Theorem G.2 by changing the assumption from r distinct faces in each dimension to r distinct columns, r distinct rows and r distinct tubes."
    }, {
      "heading" : "G.4 r distinct columns and rows",
      "text" : "The main difference between Theorem G.2 and Theorem G.5 is the running time. The first one takes 2Õ(rk2/ ) time and the second one is slightly longer, 2Õ(r2k2/ ). By Lemma G.3, r distinct columns in two dimensions implies r distinct faces in two of the three kinds of faces. Thus, the following theorem also holds for r distinct columns in two dimensions.\nAlgorithm 38 Weighted Tensor Low-rank Approximation Algorithm when the Weighted Tensor has r Distinct Faces in Each of the Two Dimensions. procedure WeightedRDistinctFacesIn2Dimensions(A,W, n, r, k, ) . Theorem G.5\nfor j = 1→ 3 do sj ← O(k/ ). Choose a sketching matrix Sj ∈ Rn2×sj . if j 6= 3 then\nfor i = 1→ r do Create k × s1 variables for matrix Pi,j ∈ Rk×sj .\nend for end if for i = 1→ n do\nWrite down (Ûj)i = A j iDW j1 SjP > j,i(Pj,iP > j,i) −1.\nend for end for Form ‖W ◦ (Û1 ⊗ Û2 ⊗ Û3 −A)‖2F . Run polynomial system verifier. return U1, U2, U3\nend procedure\nTheorem G.5. Given a 3rd order n×n×n tensor A and an n×n×n tensor W of weights with r distinct faces in two dimensions (out of three dimensions) such that each entry can be written using O(nδ) bits for some δ > 0, define OPT = infrank−k Ak‖W ◦ (Ak − A)‖2F . For any k ≥ 1 and any 0 < < 1.\n(I) If OPT > 0, and there exists a rank-k Ak = U∗1 ⊗U∗2 ⊗U∗3 tensor (with size n× n× n) such that ‖W ◦ (Ak − A)‖2F = OPT, and maxi∈[3] ‖U∗i ‖F ≤ 2O(n\nδ), then there exists an algorithm that takes (nnz(A) + nnz(W ) + n2Õ(r2k2/ ))nO(δ) time in the unit cost RAM model with words of size O(log n) bits11 and outputs three n× k matrices U1, U2, U3 such that\n‖W ◦ (U1 ⊗ U2 ⊗ U3 −A)‖2F ≤ (1 + ) OPT (74)\nholds with probability 9/10. (II) If OPT > 0, Ak does not exist, and there exist three n × k matrices U ′1, U ′2, U ′3 where each entry can be written using O(nδ) bits and ‖W ◦ (U ′1 ⊗ U ′2 ⊗ U ′3 − A)‖2F ≤ (1 + /2) OPT, then we can find U, V,W such that (74) holds.\n11The entries of A and W are assumed to fit in nδ words.\n(III) If OPT = 0, Ak exists, and there exists a solution U∗1 , U ∗ 2 , U ∗ 3 such that each entry of the matrix can be written using O(nδ) bits, then we can obtain (74). (IV) If OPT = 0, and there exist three n × k matrices U1, U2, U3 such that maxi∈[3] ‖U∗i ‖F ≤ 2O(n δ) and\n‖W ◦ (U1 ⊗ U2 ⊗ U3 −A)‖2F ≤ (1 + ) OPT +2−Ω(n δ), (75)\nthen we can output U1, U2, U3 such that (75) holds. (V) Further if Ak exists, we can output a number Z for which OPT ≤ Z ≤ (1 + ) OPT. For all the cases, the algorithm succeeds with probability at least 9/10.\nProof. By Lemma G.3, we have W has r distinct column-tube faces and r distinct row-tube faces. By Claim G.7, we know that W has R = 2O(r log r) distinct column-row faces.\nWe use the same approach as in proof of Theorem G.2 (which is also similar to Section 8 of [RSW16]) to create variables, write down the polynomial systems and add not equal constraints. Instead of having 3r distinct denominators as in the proof of Theorem G.2, we have 2r +R.\nWe create l = O(rk2/ ) variables for {Z1DW 11 S1, Z1DW 21 S1, · · · , Z1DW r1 S1}. Then we can write down Û1 with r distinct denominators gi(x). Each gi(x) is non-zero in an optimal solution using the perturbation argument in Section 4 in [RSW16]. We create new variables x2l+i to remove the denominators gi(x), ∀i ∈ [r]. Then the entries of Û1 are polynomials as opposed to rational functions.\nWe create l = O(rk2/ ) variables for {Z2DW 12 S2, Z2DW 22 S2, · · · , Z2DW r2 S2}. Then we can write down Û2 with r distinct denominators gr+i(x). Each gr+i(x) is non-zero in an optimal solution using the perturbation argument in Section 4 in [RSW16]. We create new variables x2l+r+i to remove the denominators gr+i(x), ∀i ∈ [r]. Then the entries of Û2 are polynomials as opposed to rational functions.\nUsing Û1 and Û2 we can express Û3 with R distinct denominators fi(x), which are also non-zero by using the perturbation argument in Section 4 in [RSW16], and using that W3 has at most this number of distinct rows. Finally we can write the following optimization problem,\nmin x∈R2l+2r\np(x)/q(x)\ns.t. gi(x)x2l+i − 1 = 0,∀i ∈ [r] gr+i(x)x2l+r+i − 1 = 0,∀i ∈ [r] f2j (x) 6= 0,∀j ∈ [R]\nq(x) =\nR∏\nj=1\nf2j (x)\nWe then determine if there exists a solution to the above semi-algebraic set in time\n(poly(k, r)R)O(rk 2/ ) = 2Õ(r 2k2/ ).\nUsing similar techniques from Section 5 of [RSW16], we can show a lower bound on the cost similar to Section 8.3 of [RSW16], namely, the minimum nonzero cost is at least\n2−n δ2Õ(r 2k2/ ) .\nCombining the binary search explained in Section C (a similar techniques also can be found in Section 6 of [RSW16]) with the lower bound we obtained, we can find a solution for the original problem in time\n(nnz(A) + nnz(W ) + n2Õ(r 2k2/ ))nO(δ).\nRemark G.6. Note that the running time for the Frobenius norm and for the `1 norm are of the form poly(n) + exp(poly(k/ )) rather than poly(n) · exp(k/ ). The reason is, we can use an input sparsity reduction to reduce the size of the objective function from poly(n) to poly(k).\nClaim G.7. Let W ∈ R denote a third order tensor that has r distinct columns and r distinct rows. Then it has 2O(r log r) distinct column-row faces.\nProof. By similar arguments as in the proof of Lemma G.3, the bottom (column-row) face can be split into r groups C1, C2, · · · , Cr based on r columns, and split into r groups R1, R2, · · · , Rr based on rows. Thus, the bottom (column-row) face can be regarded as having r × r groups, and the number in each position of the same group is the same.\nWe can assume that all the r2 blocks in the bottom column-row face have the same size. Otherwise, we can expand the tensor to the situation that all the r2 blocks have the same size. Because this small tensor is a sub-tensor of the big tensor, if the big tensor has at most t distinct column-row faces, then the small tensor has at most t distinct column-row faces.\nBy Lemma G.3, we know that the tensor W has at most r distinct column-tube faces and rowtube faces. Because it has r distinct column-tube faces, then all the faces belonging to coordinates in Cr are the same. Thus, all the columns belonging to Cr and in the second column-row face are the same. Similarly, we have that all the rows belonging to Rr and in the second column-row face are the same. Thus we have that all the entries in block CR ∪ Rr and in the second column-row faces are the same. Further, we can conclude, for every column-row face, for every Ci ∪ Rj block, all the entries in the same block are the same.\nThe next observation is, if there exist r2 +1 different values in the tensor, then there exist either r distinct columns or r distinct rows. Indeed, otherwise since we have r distinct columns, each column has at most r distinct entries given our bound on the nunber of distinct rows. Thus, the r distinct columns could have at most r2 distinct entries in total, a contradiction.\nFor each column-row face, there are at most r2 blocks, and the value in each block can have at most r2 possibilities. Thus, overall we have at most (r2)r2 = 2O(r2 log r) column-row faces.\nBy using different argument, we can improve the above bound. Note that we already show in each column-row face of a tensor, it has r2 blocks, and all the values in each block have to be the same. Since we have r distinct rows, we can fix the those r distinct rows. If we copy row v into one row of Ri, then we have to copy row v into every row of Ri. This is because if Ri contains two distinct rows, then there must exist a block Cj for which the entries in block Ri ∪Cj are not all the same. Thus, for each row group, all the rows in that group are the same.\nNow, for each column-row face, consider the leftmost r blocks, R1 ∪ C1, R2 ∪ C1, · · · , Rr ∪ C1. There are at most r possible values in each block, because we have r distinct rows in total. Overall the total number of possibilities for the leftmost r blocks is at most (r)r = 2O(r log r). Once the leftmost r blocks are determined, the remaining r(r − 1) are also determined. This completes the proof.\nAlso, notice that there is an example that has 2Ω(r log r) distinct column-row faces. For the bottom column-row faces, there are r × r blocks for which all the blocks have the same size, the blocks on the diagonal have all 1s, and all the other blocks contain 0s everywhere. For the later column-row faces, we can arbitrarily permute this block diagonal matrix, and the total number of possibilities is Ω(r!) ≥ 2Ω(r log r)."
    }, {
      "heading" : "H Hardness",
      "text" : "We first provide definitions and results for some fundamental problems in Section H.1. Section H.2 presents our hardness result for the symmetric tensor eigenvalue problem. Section H.3 presents our hardness results for symmetric tensor singular value problems, computing tensor spectral norm, and rank-1 approximation. We improve Håstad’s NP-hardness[Hås90] result for tensor rank in Section H.4. We also show a better hardness result for robust subspace approximation in Section H.5. Finally, we discuss several other tensor hardness results that are implied by matrix hardness results in Section H.6."
    }, {
      "heading" : "H.1 Definitions",
      "text" : "We first provide the definitions for 3SAT , ETH , MAX-3SAT , MAX-E3SAT and then state some fundamental results related to those definitions.\nDefinition H.1 (3SAT problem). Given n variables and m clauses in a conjunctive normal form CNF formula with the size of each clause at most 3, the goal is to decide whether there exists an assignment to the n Boolean variables to make the CNF formula be satisfied.\nHypothesis H.2 (Exponential Time Hypothesis (ETH) [IPZ98]). There is a δ > 0 such that the 3SAT problem defined in Definition H.1 cannot be solved in O(2δn) time.\nDefinition H.3 (MAX-3SAT). Given n variables andm clauses, a conjunctive normal form CNF formula with the size of each clause at most 3, the goal is to find an assignment that satisfies the largest number of clauses.\nWe use MAX-E3SAT to denote the version of MAX-3SAT where each clause contains exactly 3 literals.\nTheorem H.4 ([Hås01]). For every δ > 0, it is NP-hard to distinguish a satisfiable instance of MAX-E3SAT from an instance where at most a 7/8 + δ fraction of the clauses can be simultaneously satisfied.\nTheorem H.5 ([Hås01, MR10]). Assume ETH holds. For every δ > 0, there is no 2o(n1−o(1)) time algorithm to distinguish a satisfiable instance of MAX-E3SAT from an instance where at most a fraction 7/8 + δ of the clauses can be simultaneously satisfied.\nWe use MAX-E3SAT(B) to denote the restricted special case of MAX-3SAT where every variable occurs in at most B clauses. Håstad [Hås00] proved that the problem is approximable to within a factor 7/8 + 1/(64B) in polynomial time, and that it is hard to approximate within a factor 7/8 + 1/(logB)Ω(1). In 2001, Trevisan improved the hardness result, Theorem H.6 ([Tre01]). Unless RP=NP, there is no polynomial time (7/8 + 5/ √ B)-approximate algorithm for MAX-E3SAT(B) . Theorem H.7 ([Hås01, Tre01, MR10]). Unless ETH fails, there is no 2o(n1−o(1)) time (7/8+5/ √ B)approximate algorithm for MAX-E3SAT(B) .\nTheorem H.8 ([LMS11]). Unless ETH fails, there is no 2o(n) time algorithm for the Independent Set problem.\nDefinition H.9 (MAX-CUT decision problem). Given a positive integer c∗ and an unweighted graph G = (V,E) where V is the set of vertices of G and E is the set of edges of G, the goal is to determine whether there is a cut of G that has at least c∗ edges.\nNote that Feige’s original assumption[Fei02] states that there is no polynomial time algorithm for the problem in Assumption H.10. We do not know of any better algorithm for the problem in Assumption H.10 and have consulted several experts12 about the assumption who do not know a counterexample to it.\nAssumption H.10 (Random Exponential Time Hypothesis). Let c > ln 2 be a constant. Consider a random 3SAT formula on n variables in which each clause has 3 literals, and in which each of the 8n3 clauses is picked independently with probability c/n2. Then any algorithm which always outputs 1 when the random formula is satisfiable, and outputs 0 with probability at least 1/2 when the random formula is unsatisfiable, must run in 2c′n time on some input, where c′ > 0 is an absolute constant.\nThe 4SAT-version of the above random-ETH assumption has been used in [GL04] and [RSW16] (Assumption 1.3)."
    }, {
      "heading" : "H.2 Symmetric tensor eigenvalue",
      "text" : "Definition H.11 (Tensor Eigenvalue [HL13]). An eigenvector of a tensor A ∈ Rn×n×n is a nonzero vector x ∈ Rn such that\nn∑\ni=1\nn∑\nj=1\nAi,j,kxixj = λxk,∀k ∈ [n]\nfor some λ ∈ R, which is called an eigenvalue of A.\nTheorem H.12 ([N+03]). Let G = (V,E) on v vertices have stability number (the size of a maximum independent set) α(G). Let n = v+ v(v−1)2 and S\nn−1 = {(x, y) ∈ Rv×Rv(v−1)/2 : ‖x‖22 +‖y‖22 = 1}. Then,\n√ 1− 1\nα(G) = 3 √ 3/2 max (x,y)∈Sn−1\n∑\ni<j,(i,j)/∈E\nxixjyi,j .\nFor any graph G(V,E), we can construct a symmetric tensor A ∈ Rn×n×n. For any 1 ≤ i < j < k ≤ v, let\nAi,j,k = { 1 1 ≤ i < j ≤ v, k = v + φ(i, j), (i, j) /∈ E, 0 otherwise,\nwhere φ(i, j) = (i− 1)v− i(i− 1)/2 + j − i is a lexicographical enumeration of the v(v− 1)/2 pairs i < j. For the other cases i < k < j, · · · , k < j < i, we set\nAi,j,k = Ai,k,j = Aj,i,k = Aj,k,i = Ak,i,j = Ak,j,i.\nIf two or more indices are equal, we set Ai,j,k = 0. Thus tensor T has the following property,\nA(z, z, z) = 6 ∑\ni<j,(i,j)/∈E\nxixjyi,j ,\nwhere z = (x, y) ∈ Rn. 12Personal communication with Russell Impagliazzo and Ryan Williams.\nThus, we have\nλ = max z∈Sn−1 A(z, z, z) = max (x,y)∈Sn−1\n6 ∑\ni<j,(i,j)/∈E\nxixjyi,j .\nFurthermore, λ is the maximum eigenvalue of A.\nTheorem H.13. Unless ETH fails, there is no 2o( √ n) time to approximate the largest eigenvalue of an n-dimensional symmetric tensor within (1±Θ(1/n)) relative error.\nProof. The additive error is at least\n√ 1− 1/v − √ 1− 1/(v − 1) = 1/(v − 1)− 1/v√\n1− 1/v + √ 1− 1/(v − 1) & 1/(v − 1)− 1/v ≥ 1/v2.\nThus, the relative error is (1 ± Θ(1/v2)). By the definition of n, we know n = Θ(v2). Assuming ETH , there is no 2o(v) time algorithm to compute the clique number of G. Because the clique number of G is α(G), there is no 2o(v) time algorithm to compute α(G). Furthermore, there is no 2o(v) time algorithm to approximate the maximum eigenvalue within (1 ± Θ(1/v2)) relative error. Thus, we complete the proof.\nCorollary H.14. Unless ETH fails, there is no polynomial running time algorithm to approximate the largest eigenvalue of an n-dimensional tensor within (1±Θ(1/ log2+γ(n))) relative-error, where γ > 0 is an arbitrarily small constant.\nProof. We can apply a padding argument here. According to Theorem H.13, there is a d-dimensional tensor such that there is no 2o( √ d) time algorithm that can give a (1 + Θ(1/d)) relative error approximation. If we pad 0s everywhere to extend the size of the tensor to n = 2d(1−γ ′)/2 , where γ′ > 0 is a sufficiently small constant, then poly(n) = 2o( √ d), so d = log2+O(γ\n′)(n). Thus, it means that there is no polynomial running time algorithm which can output a (1 + 1/(log2+γ))-relative approximation to the tensor which has size n.\nH.3 Symmetric tensor singular value, spectral norm and rank-1 approximation\n[HL13] defines two kinds of singular values of a tensor. In this paper, we only consider the following kind:\nDefinition H.15 (`2 singular value in [HL13]). Given a 3rd order tensor A ∈ Rn1×n2×n3, the number σ ∈ R is called a singular value and the nonzero u ∈ Rn1 ,v ∈ Rn2,w ∈ Rn3 are called singular vectors of A if\nn2∑\nj=1\nn3∑\nk=1\nAi,j,kvjwk = σui,∀i ∈ [n1]\nn1∑\ni=1\nn3∑\nk=1\nAi,j,kuiwk = σvj ,∀j ∈ [n2]\nn1∑\ni=1\nn2∑\nj=1\nAi,j,kuivj = σwk,∀k ∈ [n3].\nDefinition H.16 (Spectral norm [HL13]). The spectral norm of a tensor A is:\n‖A‖2 = sup x,y,z 6=0 |A(x, y, z)| ‖x‖2‖y‖2‖z‖2\nNotice that the spectral norm is the absolute value of either the maximum value of A(x,y,z)‖x‖2‖y‖2‖z‖2 or the minimum value of it. Thus, it is an `2-singular value of A. Furthermore, it is the maximum `2-singular value of A.\nTheorem H.17 ([Ban38]). Let A ∈ Rn×n×n be a symmetric 3rd order tensor. Then,\n‖A‖2 = sup x,y,z 6=0\nA(x, y, z)\n‖x‖2‖y‖2‖z‖2 = sup x 6=0 |A(x, x, x)| ‖x‖32 .\nIt means that if a tensor is symmetric, then its largest eigenvalue is the same as its largest singular value and its spectral norm. Then, by combining with Theorem H.13, we have the following corollary:\nCorollary H.18. Unless ETH fails,\n1. There is no 2o( √ n) time algorithm to approximate the largest singular value of an n-dimensional\nsymmetric tensor within (1 + Θ(1/n)) relative-error.\n2. There is no 2o( √ n) time algorithm to approximate the spectral norm of an n-dimensional sym-\nmetric tensor within (1 + Θ(1/n)) relative-error.\nBy Corollary H.14, we have:\nCorollary H.19. Unless ETH fails,\n1. There is no polynomial time algorithm to approximate the largest singular value of an ndimensional tensor within (1 + Θ(1/ log2+γ(n))) relative-error, where γ > 0 is an arbitrarily small constant.\n2. There is no polynomial time algorithm to approximate the spectral norm of an n-dimensional tensor within (1+Θ(1/ log2+γ(n))) relative-error, where γ > 0 is an arbitrarily small constant.\nNow, let us consider Frobenius norm rank-1 approximation.\nTheorem H.20 ([Ban38]). Let A ∈ Rn×n×n be a symmetric 3rd order tensor. Then, min\nσ≥0,‖u‖2=‖v‖2=‖w‖2=1 ‖A− σu⊗ v ⊗ w‖F = min λ≥0,‖v‖2=1 ‖A− λv ⊗ v ⊗ v‖F .\nFurthermore, the optimal σ and λ may be chosen to be equal.\nNotice that\n‖A− σu⊗ v ⊗ w‖2F = ‖A‖2F − 2σA(u, v, w) + σ2‖u⊗ v ⊗ w‖2F . Then, if ‖u‖2 = ‖v‖2 = ‖w‖2 = 1, we have:\n‖A− σu⊗ v ⊗ w‖2F = ‖A‖2F − 2σA(u, v, w) + σ2. When A(u, v, w) = σ, then the above is minimized.\nThus, we have:\nmin σ≥0,‖u‖2=‖v‖2=‖w‖2=1\n‖A− σu⊗ v ⊗ w‖2F + ‖A‖22 = ‖A‖2F .\nIt is sufficient to prove the following theorem:\nTheorem H.21. Given A ∈ Rn×n×n, unless ETH fails, there is no 2o( √ n) time algorithm to compute u′, v′, w′ ∈ Rn such that ‖A− u′ ⊗ v′ ⊗ w′‖2F ≤ (1 + ) min\nu,v,w∈Rn ‖A− u⊗ v ⊗ w‖2F ,\nwhere = O(1/n2).\nProof. Let A ∈ Rn×n×n be the same hard instance mentioned in Theorem H.12. Notice that each entry of A is either 0 or 1. Thus, minu,v,w∈Rn ‖A − u ⊗ v ⊗ w‖2F ≤ ‖A‖2F . Notice that Theorem H.12 also implies that it is hard to distinguish the two cases ‖A‖2 ≤ 2 √ 2/3 · √ 1− 1/c or\n‖A‖2 ≥ 2 √ 2/3 · √ 1− 1/(c+ 1) where c is an integer which is no greater than √n. So the difference between (2 √ 2/3 · √ 1− 1/c)2 and (2 √ 2/3 · √ 1− 1/(c+ 1))2 is at least Θ(1/n). Since ‖A‖2F is at most n (see construction of A in the proof of Lemma H.12), Θ(1/n) is an = O(1/n2) fraction of minu,v,w∈Rn ‖A− u⊗ v ⊗ w‖2F . Because\nmin u,v,w∈Rn\n‖A− u⊗ v ⊗ w‖2F + ‖A‖22 = ‖A‖2F ,\nif we have a 2o( √ n) time algorithm to compute u′, v′, w′ ∈ Rn such that ‖A− u′ ⊗ v′ ⊗ w′‖2F ≤ (1 + ) min\nu,v,w∈Rn ‖A− u⊗ v ⊗ w‖2F\nfor = O(1/n2), it will contradict the fact that we cannot distinguish whether ‖A‖2 ≤ 2 √\n2/3 ·√ 1− 1/c or ‖A‖2 ≥ 2 √ 2/3 · √ 1− 1/(c+ 1).\nCorollary H.22. Given A ∈ Rn×n×n, unless ETH fails, for any for which 12 ≥ ≥ c/n2 where c is any constant, there is no 2o( −1/4) time algorithm to compute u′, v′, w′ ∈ Rn such that\n‖A− u′ ⊗ v′ ⊗ w′‖2F ≤ (1 + ) min u,v,w∈Rn ‖A− u⊗ v ⊗ w‖2F .\nProof. If = Ω(1/n2), it means that n = Ω(1/ √ ). Then, we can construct a hard instance B with size m ×m ×m where m = Θ(1/√ ), and we can put B into A, and let A have zero entries elsewhere. Since B is hard, i.e., there is no 2o(m−1/2) = 2o( −1/4) running time to compute a rank-1 approximation to B, this means there is no 2o( −1/4) running time algorithm to find an approximate rank-1 approximation to A.\nCorollary H.23. Unless ETH fails, there is no polynomial time algorithm to approximate the best rank-1 approximation of an n-dimensional tensor within (1 + Θ(1/ log2+γ(n))) relative-error, where γ > 0 is an arbitrarily small constant.\nProof. We can apply a padding argument here. According to Theorem H.21, there is a d-dimensional tensor such that there is no 2o( √ d) time algorithm which can give a (1 + Θ(1/d4)) relative approximation. Then, if we pad with 0s everywhere to extend the size of the tensor to n = 2d(1−γ ′)/2 where γ′ > 0 is a sufficiently small constant, then poly(n) = 2o( √ d), and d4 = log2+O(γ\n′)(n). Thus, it means that there is no polynomial time algorithm which can output a (1+1/(log2+γ))-relative error approximation to the tensor which has size n.\nH.4 Tensor rank is hard to approximate\nThis section presents the hardness result for approximating tensor rank under ETH . According to our new result, we notice that not only deciding the tensor rank is a hard problem, but also approximating the tensor rank is a hard problem. This therefore strengthens Håstad’s NP-Hadness [Hås90] for computing tensor rank."
    }, {
      "heading" : "H.4.1 Cover number",
      "text" : "Before getting into the details of the reduction, we provide a definition of an important concept called the “cover number” and discuss the cover number for the MAX-E3SAT(B) problem.\nDefinition H.24 (Cover number). For any 3SAT instance S with n variables and m clauses, we are allowed to assign one of three values {0, 1, ∗} to each variable. For each clause, if one of the literals outputs true, then the clause outputs true. For each clause, if the corresponding variable of one of the literals is assigned to ∗, then the clause outputs true. We say y ∈ {0, 1}n is a string, and z ∈ {0, 1, ∗}n is a star string. For an instance S, if there exists a string y ∈ {0, 1}n that causes all the clauses to be true, then we say that S is satisfiable, otherwise it is unsatisfiable. For an instance S, let ZS denote the set of star strings which cause all of the clauses of S to be true. For each star string z ∈ {0, 1, ∗}n, let star(z) denote the number of ∗s in the star-string z. We define the “cover number” of instance S to be\ncover-number(S) = min z∈ZS star(z).\nNotice that for a satisfiable 3SAT instance S, the cover number p is 0. Also, for any unsatisfiable 3SAT instance S, the cover number p is at least 1. This is because for any input string, there exists at least one clause which cannot be satisfied. To fix that clause, we have to assign ∗ to a variable\nbelonging to that clause. (Assigning ∗ to a variable can be regarded as assigning both 0 and 1 to a variable)\nLemma H.25. Let S denote a MAX-E3SAT(B) instance with n variables and m clauses and S suppose S is at most 7/8 +A satisfiable, where A ∈ (0, 1/8). Then the cover number of S is at least (1/8−A)m/B.\nProof. For any input string y ∈ {0, 1}n, there exists at least (1/8 − A)m clauses which are not satisfied. Since each variable appears in at most B clauses, we need to assign ∗ to at least (1/8 − A)m/B variables. Thus, the cover number of S is at least (1/8−A)m/B.\nWe say x1, x2, · · · , xn are variables and x1, x1, x2, x2, · · · , xn, xn are literals.\nDefinition H.26. For a list of clauses C and a set of variables P , if for each clause, there exists at least one literal such that the corresponding variable of that literal belongs to P , then we say P covers L.\nH.4.2 Properties of 3SAT instances\nFact H.27. For any 3SAT instance S with n variables and m = Θ(n) clauses, let c > 0 denote a constant. If S is (1−c)m satisfiable, then let y ∈ {0, 1}n denote a string for which S has the smallest number of unsatisfiable clauses. Let T denote the set of unsatisfiable clauses and let b denote the number of variables in T . Then Ω((cm)1/3) ≤ b ≤ O(cm).\nProof. Note that in S, there is no duplicate clause. Let T denote the set of unsatisfiable clauses by assigning string y to S. First, we can show that any two literals xi, xi cannot belong to T at the same time. If xi and xi belong to the same clause, then that clause must be an “always” satisfiable clause. If xi and xi belong to different clauses, then one of the clauses must be satisfiable. This contradicts the fact that that clause belongs to T . Thus, we can assume that literals x1, x2, · · · , xb belong to T .\nThere are two extreme cases: one is that each clause only contains three literals and each literal appears in exactly one clause in T . Then b = 3cm. The other case is that each clause contains 3 literals, and each literal appears in as many clauses as possible. Then ( b 3 ) = cm, which gives b = Θ((cm)1/3).\nLemma H.28. For a random 3SAT instance, with probability 1−2−Ω(logn log logn) there is no literal appearing in at least log n clauses.\nProof. By the property of random 3SAT , for any literal x and any clause C, the probability that x appears in C is 32n , i.e., Pr[x ∈ C] = 32n = Θ(1/n). Let p denote this probability. For any literal x,\nthe probability of x appearing in at least log n clauses (out of m clauses) is\nPr[ x appearing in ≥ log n clauses ]\n=\nm∑\ni=logn\n( m\ni\n) pi(1− p)m−i\n=\nm/2∑\ni=logn\n( m\ni\n) pi(1− p)m−i + m∑\ni=m/2\n( m\ni\n) pi(1− p)m−i\n≤ m/2∑\ni=logn\n(em/i)ipi + m∑\ni=m/2\n( m\ni\n) pi by (1− p) ≤ 1, ( m\ni\n) ≤ (em/i)i\n≤ (Θ(1/ log n))logn + 2 · (2e)m/2 ·Θ(1/n)m/2\n≤ 2−Ω(logn·log logn).\nTaking a union bound over all the literals, we complete the proof,\nPr[ @ x appearing in ≥ log n clauses ] ≥ 1− 2−Ω(logn log logn).\nLemma H.29. For a sufficiently large constant c′ > 0 and a constant c > 0, for any random 3SAT instance which has n variables and m = c′n clauses, suppose it is (1− c)m satisfiable. Then with probability 1 − 2−Ω(logn log logn), for all input strings y, among the unsatisfied clauses, each literal appears in O(log n) places.\nProof. This follows by Lemma H.28.\nNext, we show how to reduce the O(log n) to O(1).\nLemma H.30. For a sufficiently large constant c, for any random 3SAT instance that has n variables and m = cn clauses, for any constant B ≥ 1, b ∈ (0, 1), with probability at least 1− 9mBbn , there exist at least (1− b)m clauses such that each variable (in these (1− b)m clauses) only appears in at most B clauses (out of these (1− b)m clauses).\nProof. For each i ∈ [m], we use zi to denote the indicator variable such that it is 1, if for each variable in the ith clause, it appears in at most a clauses. Let B ∈ [1,∞) denote a sufficiently large constant, which we will decide upon later.\nFor each variable x, the probability of it appearing in the i-th clause is 3n . Then we have\nE[ # clauses that contain x] = m∑\ni=1\nE[i-th clause contains x] = 3m\nn\nBy Markov’s inequality,\nPr[ # clauses that contain x ≥ a] ≤ E[ # clauses that contain x]/B = 3m Bn\nBy a union bound, we can compute E[zi] ,\nE[zi] = Pr[zi = 1]\n≥ 1− 3 Pr[ one variable in i-th clause appearing ≥ B clauses ]\n≥ 1− 9m Bn .\nFurthermore, we have\nE[z] = E[ m∑\ni=1\nzi] = m∑\ni=1\nE[zi] ≥ (1− 9m\nBn )m.\nNote that z ≤ m. Thus E[z] ≤ m. Let b ∈ (0, 1) denote a sufficiently small constant. We can show\nPr[m− z ≥ bm] ≤ E[m− z] bm\n= m−E[z]\nbm\n≤ m− (1− 9m Bn)m\nbm\n= 9m\nBbn .\nThis implies that with probability at least 1 − 9mBbn , we have m − z ≤ bm. Notice that in randomETH , m = cn for a constant c. Thus, by choosing a sufficiently large constant B (which is a function of c, b), we can obtain arbitrarily large constant success probability."
    }, {
      "heading" : "H.4.3 Reduction",
      "text" : "We reduce 3SAT to tensor rank by following the same construction in [Hås90]. To obtain a stronger hardness result, we use the property that each variable only appears in at most B (some constant) clauses and that the cover number of an unsatisfiable 3SAT instance is large. Note that both MAXE3SAT(B) instances and random-ETH instances have that property. Also each MAX-E3SAT(B) is also a 3SAT instance. Thus if the reduction holds for 3SAT , it also holds for MAX-E3SAT(B) , and similarly for random-ETH .\nRecall the definition of 3SAT : 3SAT is the problem of given a Boolean formula of n variables in CNF form with at most 3 variables in each of the m clauses, is it possible to find a satisfying assignment to the formula? We say x1, x2, · · · , xn are variables and x1, x1, x2, x2, · · · , xn, xn are literals. We transform this to the problem of computing the rank of a tensor of size n1 × n2 × n3 where n1 = 2 +n+ 2m, n2 = 3n and n3 = 3n+m. T has the following n3 column-row faces, where each of the faces is an m1 × n2 matrix,\n• n variable matrices Vi ∈ Rn1×n2 . It has a 1 in positions (1, 2i− 1) and (2, 2i) while all other elements are 0.\n• n help matrices Si ∈ Rn1×n2 . It has a 1 position in (1, 2n+ i) and is 0 otherwise.\n• n help matrices Mi ∈ Rn1×n2 . It has a 1 in positions (1, 2i− 1), (2 + i, 2i) and (2 + i, 2n+ i) and is 0 otherwise.\n• m clause matrices Cl ∈ Rn1×n2 . Suppose the clause cl contains the literals ul,1, ul,2 and ul,3. For each j ∈ [3], ul,j ∈ {x1, x2, · · · , xn, x1, x2, · · · , xn}. Note that xi, xi are the literals of the 3SAT formula. We can also think of xi, xi as length 3n vectors. Let xi denote the vector that has a 1 in position 2i− 1, i.e., xi = e2i−1. Let xi denote the vector that has a 1 in positions 2i− 1 and 2i, xi = e2i−1 + e2i.\n– Row 1 is the vector ul,1 ∈ R3n, – Row 2 + n+ 2l − 1 is the vector ul,1 − ul,2 ∈ R3n, – Row 2 + n+ 2l is the vector ul,1 − ul,3 ∈ R3n.\nFirst, we can obtain Lemma H.31 which follows by Lemma 2 in [Hås90]. For completeness, we provide a proof.\nLemma H.31. If the formula is satisfiable, then the constructed tensor has rank at most 4n+ 2m.\nProof. We will construct 4n+ 2m rank-1 matrices V (1)i , V (2) i , S (1) i , M (1) i , C (1) l and C (2) l . Then the goal is to show that for each matrix in the set\n{V1, V2, · · · , Vn, S1, S2, · · · , Sn,M1,M2, · · · ,Mn, C1, C2, · · · , Cm},\nit can be written as a linear combination of these constructed matrices.\n• Matrices V (1)i and V (2) i . V (1) i has the first row equal to xi iff αi = 1 and otherwise xi. All the\nother rows are 0. We set V (2)i = Vi − V (1) i .\n• Matrices S(1)i . S (1) i = Si.\n• Matrices M (1)i .\nM (1) i = { Mi − V (1)i if αi = 1 Mi − V (1)i − Si if αi = 0\n• Matrices C(1)l and C (2) l . Let xi = αi be the assignment that makes the clause cl true. Then\nCl − V (1)i has rank 2, since either it has just two nonzero rows (in the case where xi is the first variable in the clause) or it has three nonzero rows of which two are equal. In both cases we just need two additional rank 1 matrices.\nOnce the 3SAT instance S is unsatisfiable, then its cover number is at least 1. For each unsatisfiable 3SAT instance S with cover number p, we can show that the constructed tensor has rank at most 4n+ 2m+O(p) and also has rank at least 4n+ 2m+ Ω(p). We first prove an upper bound,\nLemma H.32. For a 3SAT instance S, let y ∈ {0, 1} denote a string such that S(y) has a set L that contains unsatisfiable clauses. Let p denote the smallest number of variables that cover all clauses in L. Then the constructed tensor T has rank at most 4n+ 2m+ p.\nProof. Let y denote a length-n Boolean string (α1, α2, · · · , αn). Based on the assignment y, all the clauses of S can be split into two sets: L contains all the unsatisfied clauses and L contains all the satisfied clauses. We use set P to denote a set of variables that covers all the clauses in set L. Let p = |P |. We will construct 4n+ 2m+ p rank-1 matrices V (1)i , V (2) i , S (1) i , M (1) i , ∀i ∈ [n], C (1) l , C (2) l , ∀l ∈ [m], and V (3)j , ∀j ∈ P . Then the goal is to show that the Vi, Si,Mi and Cl can be written as linear combinations of these constructed matrices.\n• Matrices V (1)i and V (2) i . V (1) i has first row equal to xi iff αi = 1 and otherwise xi. All the\nother rows are 0. We set V (2)i = Vi − V (1) i .\n• Matrices V (3)j . For each j ∈ P , V (3) j has the first row equal to xi iff αi = 0 and otherwise xi.\n• Matrices S(1)i . S (1) i = Si.\n• Matrices M (1)i .\nM (1) i = { Mi − V (1)i if αi = 1 Mi − V (1)i − Si if αi = 0\n• Matrices C(1)l and C (2) l .\n– For each l /∈ L, clause cl is satisfied according to assignment y. Let xi = αi be the assignment that makes the clause cl true. Then Cl − V (1)i has rank 2, since either it has just two nonzero rows (in the case where xi is the first variables in the clause) or it has three nonzero rows of which two are equal. In both cases we just need two additional rank 1 matrices.\n– For each l ∈ L. It means clause cl is unsatisfied according to assignment y. Let xj1 = αj1 , xj2 = αj2 , xj3 = αj3 be an assignment that makes the clause cl false. In other words, one of j1, j2, j3 must be P according to the definition that P covers L. Then matrix Cl − V (3)j1 has rank 2, since either it has just two nonzero rows (in the case where xj1 is the first variables in the clause) or it has three nonzero rows of which two are equal. In both cases we just need two additional rank 1 matrices.\nWe finish the proof by taking the P that has the smallest size.\nFurther, we have:\nCorollary H.33. For a 3SAT instance S, let p denote the cover number of S, then the constructed tensor T has rank at most 4n+ 2m+ p.\nProof. This follows by applying Lemma H.32 to all the input strings and the definition of cover number (Definition H.24).\nWe can split the tensor T ∈ R(2+n+3m)×3n×(3n+m) into two sub-tensors, one is T1 ∈ R2×3n×(3n+m) (that contains the first two row-tube faces of T and linear combination of the remaining 2m rowtube faces of T ), and the other is T2 ∈ R(n+2m)×3n×(3n+m) (that contains the next n+ 2m row-tube faces of T ). We first analyze the rank of T1 and then analyze the rank of T2.\nClaim H.34. The rank of T2 is n+ 2m.\nProof. According to Figure 11, the nonzero rows are distributed in n+m fully separated sub-tensors. It is obvious that the rank of each one of those n sub-tensors is 1, and the rank of each of those m sub-tensors is 2. Thus, overall, the rank T2 is n+ 2m.\nTo make sure rank(T ) = rank(T1) + rank(T2), the T1 ∈ R2×3n×(3n+m) can be described as the following 3n+m column-row faces, and each of the faces is a 2× 3n matrix.\n• Matrices Ṽi, ∀i ∈ [n]. The two rows are from the first two rows of Vi in Figure 11, i.e., the first row is e2i−1 and the second row is e2i.\n• Matrices S̃i, ∀i ∈ [n]. The two rows are from the first two rows of Si in Figure 11, i.e., the first row is e2n+i and the second row is zero everywhere else.\n• Matrices M̃i,∀i ∈ [n]. The first row is e2i−1 + βi,1(e2i + e2n+i), while the second row is βi,2(e2i + e2n+i).\n• Matrices C̃l, ∀i ∈ [m]. The first row is (1 + γl,1 + γl,2)ul,1 − γl,1ul,2 − γl,2ul,3 and the second is (γl,3 + γl,4)ul,1 − γl,3ul,2 − γl,4ul,3,\nwhere for each i ∈ [3n], we use vector ei to denote a length 3n vector such that it only has a 1 in position i and 0 otherwise. β, γ are variables. The goal is to show a lower bound for,\nrank β,γ (T1).\nLemma H.35. Let P denote the set {i | the second row of matrix M̃i is nonzero, ∀i ∈ [n]}. Then the rank of T1 is at least 3n+ |P |.\nProof. We define p = |P |. Without loss of generality, we assume that for each i ∈ [p], the second row of matrix M̃i is nonzero.\nNotice that matrices Ṽi, S̃i, M̃i have size 2× 3n, but we only focus on the first 2n+ p columns. Thus, we have n+ p column-row faces (from the 3rd dimension) Aj ∈ R2×(2n+p),\n• Aj , 1 ≤ j ≤ n, Aj is the first 2n+p columns of Ṽj− ∑n\ni=1 αi,jS̃i ∈ R2×3n, where αi,j are some coefficients.\n• An+j , 1 ≤ j ≤ p, Aj is the first 2n + p columns of M̃j − ∑n\ni=1 αi,n+jS̃i ∈ R2×3n, where αi,j are some coefficients.\nConsider the first 2n + p column-tube faces (from 2nd dimension), Bj , ∀j ∈ [2n + p], of T1. Notice that these matrices have size 2× (n+ p).\n• B2i−1, 1 ≤ i ≤ p, it has a 1 in positions (1, i) and (1, n+ i).\n• B2i, 1 ≤ i ≤ p, it has βi,1 in position (1, n+ i), 1 in position (2, i) and βi,2 in position (2, n+ i).\n• B2i−1, p+ 1 ≤ i ≤ n, it has 1 in position (1, i).\n• B2i, p+ 1 ≤ i ≤ n, it has 1 in position (2, i).\n• B2n+i, 1 ≤ i ≤ p, the first row is unknown, the second row has βi,2 in position in (2, n+ i).\nIt is obvious that the first 2n matrices are linearly independent, thus the rank is at least 2n. We choose the first 2n matrices as our basis. For B2n+1, we try to write it as a linear combination of the first 2n matrices {Bi}i∈[2n]. Consider the second row of B2n+1. The first n positions are all 0. The matrices B2i all have disjoint support for the second row of the first n columns. Thus, the matrices B2i should not be used. Consider the second row of B2i−1, ∀i ∈ [n]. None of them has a nonzero value in position n+1. Thus B2n+1 cannot be written as a linear combination of of the first 2n matrices. Thus, we can show for any i ∈ [p], B2n+i cannot be written as a linear combination of matrices {Bi}i∈[2n]. Consider the p matrices {B2n+i}i∈[p]. Each of them has a different nonzero position in the second row. Thus these matrices are all linearly independent. Putting it all together, we know that the rank of matrices {Bi}i∈[2n+p] is at least 2n+ p.\nNext, we consider another special case when βi,2 = 0, for all i ∈ [n]. If we subtract βi,1 times S̃i from M̃i and leave the other column-row faces (from the 3rd dimension) as they are, and we make all column-tube faces(from the 2nd dimension) for j > 2n identically 0, then all other choices do not change the first 2n column-tube faces (from the 2nd dimension) and make some other column-tube faces (from the 2nd dimension) nonzero. Such a choice could clearly only increase the rank of T . Thus, we obtain,\nrank(T ) = 2n+ 2m+ min rank(T3),\nwhere T3 is a tensor of size 2 × 2n × (2n + m) given by the following column-row faces (from 3rd dimension) Ai, ∀i ∈ [2n+m] and each matrix has size 2× 2n (shown in Figure 15).\n• Ai, i ∈ [n], the first 2n columns of Ṽi.\n• An+i, i ∈ [n], the first 2n columns of M̃i. The first row is e2i−1 + βi,1e2i, and the second row is 0.\n• A2n+l, l ∈ [m], the first 2n columns of C̃l. The first row is (1+γl,1 +γl,2)ul,1−γl,1ul,2−γl,2ul,3, and the second row is (γl,3 + γl,4)ul,1 − γl,3ul,2 − γl,4ul,3.\nWe can show\nLemma H.36. Let p denote the cover number of the 3SAT instance. T3 has rank at least 2n+Ω(p).\nProof. First, we can show that all matrices An+i−Ai and An+i (for all i ∈ [n] ) are in the expansion of tensor T3. Thus, the rank of T3 is at least 2n.\nWe need the following claim:\nClaim H.37. For any l ∈ [m], if A2n+l can be written as a linear combination of {An+i −Ai}i∈[n] and {An+i}i∈[n], then the second row of A2n+l is 0, and the first row of one of the An+i is ui where ui is one of the literals appearing in clause cl.\nProof. We prove this for the second row first. For each l ∈ [m], we consider the possibility of using all matrices An+i − Ai and An+i to express matrix A2n+l. If the second row of A2n+l is nonzero, then it must have a nonzero entry in an odd position. But there is no nonzero in an odd position of the second row of any of matrices An+i −Ai and An+i.\nFor the first row. It is obvious that the first row of A2n+l must have at least one nonzero position, for any γl,1, γl,2. Let uj be a literal belonging to the variable xi which appears in the first row of A2n+l with a nonzero coefficient. Since only An+i of all the other An+s,∀s ∈ [n] matrices has nonzero elements in either of the positions (1, 2i − 1) or (1, 2i), then An+i must be used to cancel these elements. Thus, the first row of An+i must be a multiple of uj and since the element in position (1, 2i− 1) of An+i is 1, this multiple must be 1.\nNote that matrices Ai,∀i ∈ [n] have the property that, for any matrix in {An+1, · · · , A2n+m}, it cannot be written as the linear combination of matrices Ai,∀i ∈ [n]. Let Ã ∈ R(n+m)×2n denote a matrix that consists of the first rows of {An+1, · · · , A2n+m}. According to the property of matrices Ai, ∀i ∈ [n], and that the rank of a tensor is always greater than or equal to the rank of any sub-tensor, we know that\nrank(T3) ≥ n+ min rank(Ã).\nClaim H.38. For a 3SAT instance S, for any input string y ∈ {0, 1}n, set β∗,1 to be the entry-wise flipping of y, (I) if the clause l is satisfied, then the (n+ l)-th row of Ã ∈ R(n+m)×2n can be written as a linear combination of the first n rows of Ã. (II) if the clause l is unsatisfied, then the (n+ l)-th row of Ã cannot be written as a linear combination of the first n rows of Ã.\nProof. Part (I), consider a clause l which is satisfied with input string y. Then there must exist a variable xi belonging to clause l (either literal xi or literal xi) and one of the following holds: if xi belongs to clause l, then αi = 1; if xi belongs to clause l, then αi = 0. Suppose clause l contains literal xi. The other case can be proved in a similar way. We consider the (n + l)-th row. One of the following assignments (0, 0), (−1, 0), (0,−1) to γl,1, γl,1 is going to set the (n+ l)-th row of Ã to be vector e2i−1. We consider the i-th row of Ã. Since we set αi = 1, then we set βi,1 = 0, it follows that the i-th row of A becomes e2i−1. Therefore, the (n+ l)-th row of Ã can be written as a linear combination of Ã.\nPart (II), consider a clause l which is unsatisfied with input string y. Suppose that clause contains three literals xi1 , xi2 , xi3 (the other seven possibilities can be proved in a similar way). Then for input string y, we have αi1 = 0, αi2 = 0 and αi3 = 0, otherwise this clause l is satisfied. Consider i1-th row of Ã. It becomes e2i1−1 + e2i1 . Similarly for the i2-th row and i3-th row. Consider the (n + l)-th row. We can observe that all of positions 2i1, 2i2, 2i3 must be 0. Any\nlinear combination formed by the i1, i2, i3-th row of Ã must have one nonzero in one of positions 2i1, 2i2, 2i3. However, if we consider the (n+ l)-th row of Ã, one of the positions 2i1, 2i2, 2i3 must be 0. Also, the remaining n− 3 of the first n rows of Ã also have 0 in positions 2i1, 2i2, 2i3. Thus, we can show that the (n + l)-th row of Ã cannot be written as a linear combination of the first n rows. Similarly, for the other seven cases.\nNote that in order to make sure as many as possible rows in n + 1, · · · , n + m can be written as linear combinations of the first n rows of Ã, the βi,1 should be set to either 0 or 1. Also each possibility of input string y is corresponding to a choice of βi,1. According to the above Claim H.38, let l0 denote the smallest number of unsatisfied clauses over the choices of all the 2n input strings. Then over all choices of β, γ, there must exist at least l0 rows of Ãn+1, · · · Ãn+m, such that each of those rows cannot be written as the linear combination of the first n rows.\nClaim H.39. Let Ã ∈ R(n+m)×2n denote a matrix that consists of the first rows of An+i,∀i ∈ [n] and An+l,∀l ∈ [m]. Let p denote the cover number of 3SAT instance. Then min rank(Ã) ≥ n+Ω(p).\nProof. For any choices of {βi,1}i∈[n], there must exist a set of rows out of the next m rows such that, each of those rows cannot be written as a linear combination of the first n rows. Let L denote the set of those rows. Let t denote the maximum size set of disjoint rows from L. Since those t rows in L all have disjoint support, they are always linearly independent. Thus the rank is at least n+ t.\nNote that each row corresponds to a unique clause and each clause corresponds to a unique row. We can just pick an arbitrary clause l in L, then remove the clauses that are using the same literal as clause l from L. Because each variable occurs in at most B clauses, we only need to remove at most 3B clauses from L. We repeat the procedure until there is no clause L. The corresponding rows of all the clauses we picked have disjoint supports, thus we can show a lower bound for t,\nt ≥ |L|/(3B) ≥ l0/(3B) ≥ p/(9B) & p,\nwhere the second step follows by |L| ≥ l0, the third step follows 3l0 ≥ p, and the last step follows by B is some constant.\nThus, putting it all together, we complete the proof.\nNow, we consider a general case when there are q different i ∈ [n] satisfying that βi,2 6= 0. Similar to tensor T3, we can obtain T4 such that,\nrank(T ) = 2n+ 2m+ min rank(T4)\nwhere T4 is a tensor of size 2 × 2n × (2n + m) given by the following column-row faces (from 3rd dimension) Ai, ∀i ∈ [2n+m] and each matrix has size 2× 2n (shown in Figure 16).\n• Ai, i ∈ [n], the first 2n columns of Ṽi.\n• An+i, i ∈ [q], the first 2n columns of M̃i. The first row is e2i−1 + βi,1e2i, and the second row is βi,2e2i.\n• An+i, i ∈ {q + 1, · · · , n}, the first 2n columns of M̃i. The first row is e2i−1 + βi,1e2i, and the second row is 0.\n• A2n+l, l ∈ [m], the first 2n columns of C̃l. The first row is (1+γl,1 +γl,2)ul,1−γl,1ul,2−γl,2ul,3, and the second row is (γl,3 + γl,4)ul,1 − γl,3ul,2 − γl,4ul,3.\nNote that modifying q entries(from Figure 15 to Figure 16) of a tensor can only decrease the rank by q, thus we obtain\nLemma H.40. Let q denote the number of i such that βi,2 6= 0, and let p denote the cover number of the 3SAT instance. Then T4 has rank at least 2n+ Ω(p)− q.\nCombining the two perspectives we have\nLemma H.41. Let p denote the cover number of an unsatisfiable 3SAT instance. Then the tensor has rank at least 4n+ 2m+ Ω(p).\nProof. Let q denote the q in Figure 16. From one perspective, we know that the tensor has rank at least 4n+ 2m+ Ω(p)− q. From another perspective, we know that the tensor has rank at least 4n+ 2m+ q. Combining them together, we obtain the rank is at least 4n+ 2m+ Ω(p)/2, which is still 4n+ 2m+ Ω(p).\nTheorem H.42. Unless ETH fails, there is a δ > 0 and an absolute constant c0 > 1 such that the following holds. For the problem of deciding if the rank of a q-th order tensor, q ≥ 3, with each dimension n, is at most k or at least c0k, there is no 2δk 1−o(1) time algorithm.\nProof. The reduction can be split into three parts.13 The first part reduces the MAX-3SAT problem to the MAX-E3SAT problem by [MR10]. For each MAX-3SAT instance with size n, the corresponding MAX-E3SAT instance has size n1+o(1). The second part is by reducing the MAX-E3SAT problem to MAX-E3SAT(B) by [Tre01]. For each MAX-E3SAT instance with size n, the corresponding MAX-E3SAT(B) instance has size Θ(n) when B is a constant. The third part is by reducing the MAX-E3SAT(B) problem to the tensor problem. Combining Theorem H.7, Lemma H.25 with this reduction, we complete the proof.\nTheorem H.43. Unless random-ETH fails, there is an absolute constant c0 > 1 for which any deterministic algorithm for deciding if the rank of a q-th order tensor is at most k or at least c0k, requires 2Ω(k) time.\nProof. This follows by combining the reduction with random-ETH and Lemma H.30. 13The first two parts are accomplished by personal communication with Dana Moshkovitz and Govind Ramnarayan.\nNote that, if BPP = P then it also holds for randomized algorithms which succeed with probability 2/3.\nIndeed, we know that any deterministic algorithm requires 2Ω(n) running time on tensors that have size n×n×n. Let g(n) denote a fixed function of n, and g(n) = o(n). We change the original tensor from size n×n×n to 2g(n)×2g(n)×2g(n) by adding zero entries. Then the number of entries in the new tensor is 23g(n) and the deterministic algorithm still requires 2Ω(n) running time on this new tensor. Assume there is a randomized algorithm that runs in 2cg(n) time, for some constant c > 3. Then considering the size of this new tensor, the deterministic algorithm is a super-polynomial time algorithm, but the randomized algorithm is a polynomial time algorithm. Thus, by assuming BPP = P, we can rule out randomized algorithms, which means Theorem H.43 also holds for randomized algorithms which succeed with probability 2/3.\nWe provide some some motivation for the BPP = P assumption: this is a standard conjecture in complexity theory, as it is implied by the existence of strong pseudorandom generators or if any problem in deterministic exponential time has exponential size circuits [IW97].\nH.5 Hardness result for robust subspace approximation\nThis section improves the previous hardness for subspace approximation [CW15a] from 1±1/poly(d) to 1± 1/poly(log d). (Note that, we provide the algorithmic results for this problem in Section F.)\nLemma H.44 ([Dem14]). For any graph G with n nodes, m edges, for which the maximum degree in graph G is d, there exists a d-regular graph G′ with 2nd − 2m nodes such that the clique size of G′ is the same as the clique size of G.\nProof. First we create d copies of the original graph G. For each i ∈ [n], let vi,1, vi,2, · · · , vi,d denote the set of nodes in G′ that are corresponding to vi in G. Let dvi denote the degree of node vi in graph G. In graph G′, we create d− dvi new nodes v′i,1, v′i,2, · · · , v′i,dvi and connect each of them to all of the v1, v2, · · · , vd. Therefore, 1. For each i ∈ [n], j ∈ [dvi ], node v′i,j has degree d. 2. For each i ∈ [n], j ∈ [d], node vi,j has degree dvi (from the original graph), and d−dvi degree (from the edges to all the v′i,1, v\n′ i,2, · · · , v′i,dvi ). Thus, we proved the graph G is d-regular.\nThe number of nodes in the new graph G′ is,\nnd+ n∑\ni=1\n(d− dvi) = 2nd− n∑\ni=1\ndvi = 2nd− 2m.\nIt remains to show the clique size is the same in graph G and G′. Since we can always reorder the indices for all the nodes, without loss of generality, let us assume the the first k nodes v1, v2, · · · , vk forms a k-clique that has the largest size. It is obvious that the clique size k′ in graph G′ is at least k, since we make k copies of the original graph and do not delete any edges and nodes. Then we just need to show k′ ≤ k. By the property of the construction, the node in one copy does not connect to a node in any other copy. Consider the new nodes we created. For each node v′i,j , consider the neighbors of this node. None of them share a edge. Combining the above two properties gives k′ ≤ k. Thus, we finish the proof.\nTheorem H.45 (Theorem 2.6 in [GJS76]). Any n variable m clauses 3SAT instance can be reduced to a graph G with 24m vertices, which is an instance of 10m-independent set. Furthermore G is a 3-regular graph.\nWe give the proof for completeness here.\nProof. Define oi to be the number of occurrences of {xi, xi} in the m clauses. For each variable xi, we construct 2oi vertices, namely vi,1, vi,2, · · · , vi,2oi . We make these 2oi vertices be a circuit, i.e., there are 2oi edges: (vi,1, vi,2), (vi,2, vi,3), · · · , (vi,2oi−1, vi,2oi), (vi,2oi , vi,1). For each clause with 3 literals a, b, c, we create 3 vertices va, vb, vc where they form a triangle, i.e., there are edges (va, vb), (vb, vc), (vc, va). Furthermore, assume a is the jth occurrence of xi (occurrence of xi means a = xi or a = xi). Then if a = xi, we add edge (va, vi,2j), otherwise we add edge (va, vi,2j−1).\nThus, we can see that every vertex in the triangle corresponding to a clause has degree 3, half of vertices of the circuit corresponding to variable xi have degree 3 and the other half have degree 2. Notice that the maximum independent set of a 2oi circuit is at most oi, and the maximum independent set of a triangle is at most 1. Thus, the maximum independent set of the whole graph has size at most m+ ∑n i=1 oi = m+ 3m = 4m. Another observation is that if there is a satisfiable assignment for the 3SAT instance, then we can choose a 4m-independent set in the following way: if xi is true, then we choose all the vertices in set {vi,1, vi,3, · · · , vi,2j−1, · · · vi,2oi−1}; otherwise, we choose all the vertices in set {vi,2, vi,4, · · · , vi,2j , · · · vi,2oi}. For a clause with literals a, b, c: if a is satisfied, it means that vi,t which connected to va is not chosen in the independent set, thus we can pick va.\nThe issue remaining is to reduce the above graph to a 3 regular graph. Notice that there are exactly ∑n i=1 oi = 3m vertices which have degree 2. For each of this kind of vertex u, we construct 5 additional vertices u1, u2, u3, u4, u5 and edges (u1, u2), (u2, u3), (u3, u4), (u4, u5), (u5, u1), (u2, u4), (u3, u5) and (u1, u). Because we can always choose exactly two vertices among u1, u2, · · · , u5 no matter we choose vertex u or not, the value of the maximum independent set will increase the size by exactly 2 ∑n\ni=1 oi = 6m. To conclude, we construct a 3-regular graph reduced from a 3SAT instance. The graph has exactly 24m vertices. Furthermore, if the 3SAT instance is satisfiable, the graph has 10m-independent set. Otherwise, it does not have a 10m-independent set.\nCorollary H.46. There is a constant 0 < c < 1, such that for any > 0, there is no O(2n1− ) time algorithm which can solve k-clique for an n-vertex (n − 3)-regular graph where k = cn unless ETH fails.\nProof. According to Theorem H.45, for a given n variable m = O(n) clauses 3SAT instance, we can reduce it to a 3-regular graph with 24m vertices which is a 10m-independent set instance. If\nthere exists > 0 such that we have an algorithm with running time O(2(24m)1− ) which can solve 10m-clique for a 24m− 3 regular graph with 24m vertices, then we can solve the 3SAT problem in O(2n 1− ′ ) time, where ′ = Θ( ). Thus, it contradicts ETH .\nDefinition H.47. Let V be a k-dimensional subspace of Rd, represented as the column span of a d× k matrix with orthonormal columns. We abuse notation and let V be both the subspace and the corresponding matrix. For a set Q of points, let\nc(Q,V ) = ∑\nq∈Q d(q, V )p =\n∑ q∈Q ‖q>(I − V V >)‖p2 = ∑ q∈Q (‖q‖2 − ‖q>V ‖2)p/2,\nbe the sum of p-th powers of distances of points in Q, i.e., ‖Q−QV V >‖v with associatedM(x) = |x|p.\nLemma H.48. For any k ∈ [d], the k-dimensional subspaces V which minimize c(E, V ) are exactly the ( n k ) subspaces formed by taking the span of k distinct standard unit vectors ei, i ∈ [d]. The cost of any such V is d− k.\nTheorem H.49. Given a set Q of poly(d) points in Rd, for a sufficiently small = 1/poly(d), it is NP-hard to output a k-dimensional subspace V of Rd for which c(Q,V ) ≤ (1 + )c(Q,V ∗), where V ∗ is the k-dimensional subspace minimizing the expression c(Q,V ), that is c(Q,V ) ≥ c(Q,V ∗) for all k-dimensional subspaces V .\nTheorem H.50. For a sufficiently small = 1/ poly(log(d)), there exist 1 ≤ k ≤ d, unless ETH fails, there is no algorithm that can output a k-dimensional subspace V of Rd for which c(Q,V ) ≤ (1 + )c(Q,V ∗), where V ∗ is the k-dimensional subspace minimizing the expression c(Q,V ), that is c(Q,V ) ≥ c(Q,V ∗) for all k-dimensional subspaces V .\nProof. The reduction is from the clique problem of d-vertices (d − 3)-regular graph. We construct the hard instance in the same way as in [CW15a]. Given a d-vertes (d− 3)-regular graph graph G, let B1 = dα, B2 = dβ where β > α ≥ 1 are two sufficiently large constants. Let c be such that\n(1− 1/B1)2 + c2/B1 = 1.\nWe construct a d × d matrix A as the following: ∀i ∈ [d], let Ai,i = 1 − 1/B1 and ∀i 6= j, Ai,j = Aj,i = c/ √ B1r if (i, j) is an edge in G, and Ai,j = Aj,i = 0 otherwise. Let us construct A′ ∈ R2d×d as follows:\nA′ =\n[ A\nB2 · Id\n] ,\nwhere Id ∈ Rd is a d× d identity matrix. Claim H.51 (In proof of Theorem 54 in [CW15a]). Let V ′ ∈ Rd×k satisfy that\nc(A′, V ′) ≤ (1 + 1/dγ)c(A′, V ∗),\nwhere A′ is constructed as the above corresponding to the given graph G, and γ > 1 is a sufficiently large constant, V ∗ is the optimal solution which minimizes c(A′, V ). Then if G has a k-Clique , given V ′, there is a poly(d) time algorithm which can find the clique which has size at least k.\nNow, to apply ETH here, we only need to apply a padding argument. We can construct a matrix A′′ ∈ RN×d as follows:\nA′′ =   A′ A′\n· · · A′\n  .\nBasically, A′′ contains N/(2d) copies of A′ where N = 2d1−α , and 0 < α is a constant which can be arbitrarily small. Notice that ∀V ∈ Rd×k,\nc(V,A′′) = ∑\nq∈A′′ d(q, V )p = N/(2d)\n∑ q∈A′ d(q, V )p = N/(2d)c(V,A′).\nSo if V ′′ gives a (1 + 1/dγ) approximation to A′′, it also gives a (1 + 1/dγ) approximation to A′. So if we can find V ′′ in poly(N, d) time, we can output a k-Clique of G in poly(N, d) time. But unless ETH fails, for a sufficiently small constant α′ > 0 there is no poly(N, d) = O(2d1−α ′ ) time algorithm that can output a k-Clique of G. It means that there is no poly(N, d) time algorithm that can compute a (1 + 1/dγ) = (1 + 1/ poly(log(N))) approximation to A′′. To make A′′ be a square matrix, we can just pad with 0s to make the size of A′′ be N ×N . Thus, we can conclude, unless ETH fails, there is no polynomial algorithm that can compute a (1 + 1/ poly(log(N))) rank-k subspace approximation to a point set with size N .\nH.6 Extending hardness from matrices to tensors\nIn this section, we briefly state some hardness results which are implied by hardness for matrices. The intuition is that, if there is a hard instance for the matrix problem, then we can always construct a tensor hard instance for the tensor problem as follos: the first face of the tensor is the hard instance matrix and it has all 0s elsewhere. We can prove that the optimal tensor solution will always fit the first face and will have all 0s elsewhere. Then the optimal tensor solution gives an optimal matrix solution.\nH.6.1 Entry-wise `1 norm and `1-`1-`2 norm\nIn the following we will show that the hardness for entry-wise `1 norm low rank matrix approximation implies the hardness for entry-wise `1 norm low rank tensor approximation and asymmetric tensor norm (`1-`1-`2) low rank tensor approximation problems.\nTheorem H.52 (Theorem H.13 in [SWZ17]). Unless ETH fails, for an arbitrarily small constant γ > 0, given some matrix A ∈ Rn×n, there is no algorithm that can compute x̂, ŷ ∈ Rn s.t.\n‖A− x̂ŷ>‖1 ≤ ( 1 + 1\nlog1+γ(n) ) min x,y∈Rn ‖A− xy>‖1,\nin poly(n) time.\nWe can get the hardness for tensors directly.\nTheorem H.53. Unless ETH fails, for an arbitrarily small constant γ > 0, given some tensor A ∈ Rn×n×n,\n1. there is no algorithm that can compute x̂, ŷ, ẑ ∈ Rn s.t.\n‖A− x̂⊗ ŷ ⊗ ẑ‖1 ≤ ( 1 + 1\nlog1+γ(n)\n) min\nx,y,z∈Rn ‖A− x⊗ y ⊗ z‖1,\nin poly(n) time.\n2. there is no algorithm can compute x̂, ŷ, ẑ ∈ Rn s.t.\n‖A− x̂⊗ ŷ ⊗ ẑ‖u ≤ ( 1 + 1\nlog1+γ(n)\n) min\nx,y,z∈Rn ‖A− x⊗ y ⊗ z‖u,\nin poly(n) time.\nProof. Let matrix Â ∈ Rn×n be the hard instance in Theorem H.52. We construct tensor A ∈ Rn×n×n as follows: ∀i, j, l ∈ [n], l 6= 1 we let Ai,j,1 = Âi,j , Ai,j,l = 0.\nSuppose x̂, ŷ, ẑ ∈ Rn satisfies\n‖A− x̂⊗ ŷ ⊗ ẑ‖1 ≤ ( 1 + 1\nlog1+γ(n)\n) min\nx,y,z∈Rn ‖A− x⊗ y ⊗ z‖1.\nThen letting z′ = (1, 0, 0, · · · , 0)>, we have\n‖A− x̂⊗ ŷ ⊗ z′‖1 ≤ ‖A− x̂⊗ ŷ ⊗ ẑ‖1 ≤ ( 1 + 1\nlog1+γ(n)\n) min\nx,y,z∈Rn ‖A− x⊗ y ⊗ z‖1.\nThe first inequality follows since ∀i, j, l ∈ [n], l 6= 1, we have Ai,j,l = 0. Let\nx∗, y∗ = arg min x,y∈Rn ‖Â− xy>‖1.\nThen\n‖A− x̂⊗ ŷ ⊗ z′‖1 ≤ ( 1 + 1\nlog1+γ(n)\n) ‖A− x̂⊗ ŷ ⊗ ẑ‖1 ≤ ( 1 +\n1\nlog1+γ(n)\n) ‖A− x∗ ⊗ y∗ ⊗ z′‖1.\nThus, we have\n‖Â− x̂ŷ>‖1 ≤ ( 1 + 1\nlog1+γ(n)\n) ‖Â− x∗(y∗)>‖1.\nCombining with Theorem H.52, we know that unless ETH fails, there is no poly(n) running time algorithm which can output\n‖A− x̂⊗ ŷ ⊗ ẑ‖1 ≤ ( 1 + 1\nlog1+γ(n)\n) min\nx,y,z∈Rn ‖A− x⊗ y ⊗ z‖1.\nSimilarly, we can prove that if x̃, ỹ, z̃ ∈ Rn satisfies:\n‖A− x̃⊗ ỹ ⊗ z̃‖u ≤ ( 1 + 1\nlog1+γ(n)\n) min\nx,y,z∈Rn ‖A− x⊗ y ⊗ z‖u,\nthen\n‖Â− x̃ỹ>‖1 ≤ ( 1 + 1\nlog1+γ(n)\n) ‖Â− x∗(y∗)>‖1.\nWe complete the proof.\nCorollary H.54. Unless ETH fails, for arbitrarily small constant γ > 0,\n1. there is no algorithm that can compute (1+ ) entry-wise `1 norm rank-1 tensor approximation in 2O(1/ 1−γ) running time. (‖ · ‖1-norm is defined in Section D)\n2. there is no algorithm that can compute (1 + ) `u-norm rank-1 tensor approximation in 2O(1/ 1−γ) running time. (‖ · ‖u-norm is defined in Section F.3)\nH.6.2 `1-`2-`2 norm\nTheorem H.55. Unless ETH fails, for arbitrarily small constant γ > 0, given some tensor A ∈ Rn×n×n, there is no algorithm can compute Û , V̂ , Ŵ ∈ Rn×k s.t.\n‖A− Û ⊗ V̂ ⊗ Ŵ‖v ≤ ( 1 + 1\npoly(log n)\n) min\nU,V,W∈Rn×k ‖A− U ⊗ V ⊗W‖v,\nin poly(n) running time. (‖ · ‖v-norm is defined in Section F.2) Proof. Let matrix Â ∈ Rn×n be the hard instance in Theorem H.50. We construct tensor A ∈ Rn×n×n as follows: ∀i, j, l ∈ [n], l 6= 1 we let Ai,j,1 = Âi,j , Ai,j,l = 0.\nSuppose Û , V̂ , Ŵ ∈ Rn×k satisfies\n‖A− Û ⊗ V̂ ⊗ Ŵ‖v ≤ ( 1 + 1\npoly(log n)\n) min\nU,V,W∈Rn×k ‖A− U ⊗ V ⊗W‖v.\nLet W ′ ∈ Rn×k be the following:\nW ′ =   1 1 · · · 1 0 0 · · · 0 0 0 · · · 0 · · · · · · · · · · · · 0 0 · · · 0   ,\nthen we have\n‖A− Û ⊗ V̂ ⊗W ′‖v ≤ ‖A− Û ⊗ V̂ ⊗ Ŵ‖v ≤ ( 1 + 1\npoly(log n)\n) min\nU,V,W∈Rn×k ‖A− U ⊗ V ⊗W‖v.\nThe first inequality follows since ∀i, j, l ∈ [n], l 6= 1, we have Ai,j,l = 0. Let\nU∗, V ∗ = arg min U,V ∈Rn×k ‖Â− UV >‖v.\nThen\n‖A− Û ⊗ V̂ ⊗W ′‖v ≤ ( 1 + 1\npoly(log n)\n) ‖A− Û ⊗ V̂ ⊗ Ŵ‖v\n≤ ( 1 + 1\npoly(log n)\n) ‖A− U∗ ⊗ V ∗ ⊗W ′‖v.\nThus, we have\n‖Â− Û V̂ >‖v ≤ ( 1 + 1\npoly(log n)\n) ‖Â− U∗(V ∗)>‖v.\nCombining with Theorem H.50, we know that unless ETH fails, there is no poly(n) time algorithm which can output\n‖A− Û ⊗ V̂ ⊗ Ŵ‖v ≤ ( 1 + 1\npoly(log n)\n) min\nU,V,W∈Rn×k ‖A− U ⊗ V ⊗W‖v."
    }, {
      "heading" : "I Hard Instance",
      "text" : "This section provides some hard instances for tensor problems.\nI.1 Frobenius CURT decomposition for 3rd order tensor\nIn this section we will prove that a relative-error Tensor CURT is not possible unless C has Ω(k/ ) columns from A, R has Ω(k/ ) rows from A, T has Ω(k/ ) tubes from A and U has rank Ω(k).\nWe use a similar construction from [BW14, BDM11, DR10] and extend it to the tensor setting.\nTheorem I.1. There exists a tensor A ∈ Rn×n×n with the following property. Consider a factorization CURT, with C ∈ Rn×c containing c columns of A, R ∈ Rn×r containing r rows of A, T ∈ Rn×t containing r tubes of A, and U ∈ Rc×r×t, such that\n∥∥∥∥∥∥ A− n∑\ni=1\nn∑\nj=1\nn∑\nl=1\nUi,j,l · Ci ⊗Rj ⊗ Tl ∥∥∥∥∥∥ 2\nF\n≤ (1 + )‖A−Ak‖2F .\nThen, for any < 1 and any k ≥ 1,\nc = Ω(k/ ), r = Ω(k/ ), t = Ω(k/ ) and rank(U) ≥ k/3.\nProof. For any i ∈ [d], let ei ∈ Rd denote the i-th standard basis vector. For α > 0 and integer d > 1, consider the matrix D ∈ R(d+1)×(d+1),\nD = [ e1 + αe2 e1 + αe3 · · · e1 + αed+1 0 ]\n=   1 1 · · · 1 0 α 0 α 0 . . .\n... α 0\n \nWe construct matrix B ∈ R(d+1)k/3×(d+1)k/3 by repeating matrix D k/3 times along its main diagonal,\nB =   D D\n. . . D\n \nLet m = (d+ 1)k/3. We construct a tensor A ∈ Rn×n×n with n = 3m by repeating matrix B three times in the following way,\nA1,j,l = Bj,l, ∀j, l ∈ [m]× [m] Am+i,m+1,m+l = Bi,l,∀i, l ∈ [m]× [m] A2m+i,2m+j,2m+1 = Bi,j ,∀j, i ∈ [m]× [m]\nand 0 everywhere else. We first state some useful properties for matrix D,\nD>D =\n[ 1d1 > d + α\n2Id 0 0 0\n] ∈ R(d+1)×(d+1)\nwhere\nσ21(D) = d+ α 2, σ2i (D) = α 2, ∀i = 2, · · · , d\nσ2d+1(D) = 0.\nBy definition of matrix B, we can obtain the following properties,\nσ2i (B) = d+ α 2, ∀i = 1, · · · , k/3 σ2i (B) = α 2, ∀i = k/3 + 1, · · · , dk/3 σ2i (B) = 0, ∀i = dk + 1, · · · , dk/3 + k/3\nBy definition of A, we can copy B into three disjoint n×n×n sub-tensors on the main diagonal of tensor A. Thus, we have\nσ2i (A) = d+ α 2, ∀i = 1, · · · , k σ2i (A) = α 2, ∀i = k + 1, · · · , dk σ2i (A) = 0, ∀i = dk + 1, · · · , dk + k\nLet A(k) denote the best rank-k approximation to A, and let D1 denote the best rank-1 approximation to D. Using the above properties, for any k ≥ 1, we can compute ‖A−A(k)‖2F ,\n‖A−Ak‖2F = k‖D −D1‖2F = k(d− 1)α2. (76)\nSuppose we have a CUR decomposition with c′ = o(k/ ) columns, r′ = o(k/ ) rows or t′ = o(k/ ) tubes. Since the tensor is equivalent by looking through any of the 3 dimensions/directions, we just need to show why the cost will be at least (1 + )‖A − Ak‖2F if we choose t = o(k/ ) columns and t = o(k/ ) rows.\nLet C ∈ Rn×c denote the optimal solution. Then it should have the following form,\nC =\n  C1\nC2 C3\n \nwhere C1 ∈ Rm×c1 contains c1 columns from A1:m,1:m,1:m ∈ Rm×m×m, C2 ∈ Rm×c2 contains c2 columns from Am+1:2m,m+1:2m,m+1:2m ∈ Rm×m×m, C3 ∈ Rm×c3 contains c3 columns from A2m+1:3m,2m+1:3m,2m+1:3m ∈ Rm×m×m.\nLet R ∈ Rn×r denote the optimal solution. Then it should have the following form,\nR =\n  R1\nR2 R3\n \n‖A−A(CC†, RR†, I)‖2F ≥ ‖B −R1R†1B‖2F + ‖B − C2C†2B‖2F + ‖B> − C3C†3B>‖2F . (77)\nBy the analysis in Proposition 4 of [DV06], we have\n‖B −R1R†1B‖2F ≥ (k/3)(1 + b · α)‖D −D(1)‖2F . (78)\nand\n‖B − C2C†2B‖2F ≥ (k/3)(1 + b · α)‖D −D(1)‖2F . (79)\nLet C3 ∈ Rm×c3 contain any c3 columns from B>. Note that C3 contains c3(≤ t) columns from B>, equivalently C>2 contains c2 rows from B. Recall that B contains k copies of D ∈ R(d+1)×(d+1) along its main diagonal. Even if we choose t columns of B>, the cost is at least\n‖B> − C3C†3B>‖2F ≥ (k/3)‖D −D(t)‖2F ≥ (k/3)(d− t)α2. (80) Combining Equations (76), (77), (78), (79), (80), α = gives,\n‖A− CC†A‖2F ‖A−A(k)‖2F\n≥ ‖B −R1R † 1B‖2F + ‖B − C2C † 2B‖2F + ‖B> − C3C † 3B >‖2F\n‖A−A(k)‖2F by Eq. (77)\n≥ ‖B −R1R † 1B‖2F + ‖B − C2C † 2B‖2F + ‖B> − C3C † 3B >‖2F\nk(d− 1)α2 by Eq. (76)\n≥ 2(k/3)(1 + b )(d− 1) 2 + (k/3)(d− t) 2\nk(d− 1) 2 by Eq. (78),(79),(80) and α =\n= k(d− 1) 2 + (k/3)(−t+ 1) 2 + 2(k/3)b (d− 1) 2\nk(d− 1) 2\n= 1 + (k/3) 2(2b (d− 1)− t+ 1)\nk(d− 1) 2\n= 1 + 2b (d− 1)− t+ 1 3(d− 1) ≥ 1 + (b/3) by 2t ≤ b (d− 1)/2 ≥ 1 + . by b > 3.\nwhich gives a contradiction.\nI.2 General Frobenius CURT decomposition for q-th order tensor\nIn this section, we extend the hard instance for 3rd order tensors to q-th order tensors.\nTheorem I.2. For any constant q ≥ 1, there exists a tensor A ∈ Rn×n×···×n with the following property. Define\nOPT = min rank−k Ak∈Rc1×c2×···×cq\n‖A−Ak‖2F .\nConsider a q-th order factorization CURT, with C1 ∈ Rn×c1 containing c columns from the 1st dimension of A, C2 ∈ Rn×c2 containing c2 columns from the 2nd dimension of A, · · · , Cq ∈ Rn×cq containing cq columns from the q-th dimension of A and a tensor U ∈ Rc1×c2×···×cq , such that\n∥∥∥∥∥∥ A− n∑\ni1=1\nn∑\ni2=1\n· · · n∑\niq=1\nUi1,i2,··· ,iq · C1,i1 ⊗ C2,i2 ⊗ · · · ⊗ Cq,iq ∥∥∥∥∥∥ 2\nF\n≤ (1 + ) OPT .\nThere exists a constant c′ < 1 such that for any < c′ and any k ≥ 1, c1 = Ω(k/ ), c2 = Ω(k/ ), · · · , cq = Ω(k/ ) and rank(U) ≥ c′k.\nProof. We use the same matrixD ∈ R(d+1)×(d+1) as the proof of Theorem I.1. Then we can construct matrix B ∈ R(d+1)k/q×(d+1)k/q by repeating matrix D k/q times along the its main diagonal,\nB =   D D\n. . . D\n \nLet m = (d+ 1)/q. We construct a tensor A ∈ Rn×n×···×n with n = qm by repeating the matrix q times in the following way,\nA[1:m],[1:m],1,1,1,··· ,1,1 = B,\nAm+1,[m+1:2m],[m+1:2m],m+1,m+1,··· ,m+1,m+1 = B >,\nA2m+1,2m+1,[2m+1:3m],[2m+1:3m],2m+1,··· ,2m+1,2m+1 = B, A3m+1,3m+1,3m+1,[3m+1:4m],[3m+1:4m],··· ,2m+1,3m+1 = B >,\n· · · · · · · · · A(q−2)m+1,(q−2)m+1,(q−2)m+1,(q−2)m+1,(q−2)m+1,··· ,[(q−2)m+1:(q−1)m],[(q−2)m+1:(q−1)m] = B,\nA[(q−1)m+1:qm],(q−1)m+1,(q−1)m+1,(q−1)m+1,(q−1)m+1,··· ,(q−1)m+1,[(q−1)m+1:qm] = B >,\nwhere there are q/2 Bs and q/2 B>s on the right when q is even, and there are (q + 1)/2 Bs and (q − 1)/2 Bs on the right when q is odd. Note that this tensor A is equivalent if we look through any of the q dimensions/directions. Similarly as before, we have\n‖A−A(k)‖2F = k‖D −D(1)‖2F = k(d− 1)α2.\nSuppose there is a general CURT decomposition (of this q-th order tensor), with c1 = c2 = · · · cq = o(k/ ) columns from each dimension. Let C1 ∈ Rn×c1 , C2 ∈ Rn×c2 , · · · , Cq ∈ Rn×cq denote the optimal solution. Then the Ci should have the following form,\nC1 =   C1,1 C1,2\n. . . C1,q\n  , C2 =   C2,1 C2,2\n. . . C2,q\n  , · · · , Cq =   Cq,1 Cq,2\n. . . Cq,q\n \n(In the rest of the proof, we focus on the case when q is even. Similarly, we can show the same thing when q is odd.) We have\n‖A−A(C1C†1, C2C†2, · · · , CqC†q)‖2F\n≥ q/2∑\ni=1\n‖B − C2i−1,2i−1C†2i−1,2i−1B‖2F + ‖B> − C2i,2iC † 2i,2iB >‖2F\n≥ (q/2) ( (k/q)(1 + bα)‖D −D(1)‖2F + (k/q)(d− t)α2 ) = (q/2) ( (k/q)(1 + bα)(d− 1)α2 + (k/q)(d− t)α2 )\nwhere the second inequality follows by Equations (79) and (80), and the third step follows by ‖D −D(1)‖2F = (d− 1)α2.\nPutting it all together, we have\n‖A−A(C1C†1, C2C†2, · · · , CqC†q)‖2F ‖A−A(k)‖2F\n≥ (q/2) ( (k/q)(1 + bα)(d− 1)α2 + (k/q)(d− t)α2 )\nk(d− 1)α2\n= k(d− 1)α2 + (k/2)bα(d− 1)α2 + (k/q)(−t+ 1)α2\nk(d− 1)α2\n= 1 + (k/2)bα(d− 1)α2 + (k/q)(−t+ 1)α2\nk(d− 1)α2\n≤ 1 + (k/3)bα(d− 1)α 2 k(d− 1)α2 = 1 + (b/3) by = α > 1 + by b > 3.\nwhich leads to a contradiction. Similarly we can show the rank is at least Ω(k)."
    }, {
      "heading" : "J Distributed Setting",
      "text" : "Input data to large-scale machine learning and data mining tasks may be distributed across different machines. The communication cost becomes the major bottleneck of distributed protocols, and so there is a growing body of work on low rank matrix approximations in the distributed model [TD99, QOSG02, BCL05, BRB08, MBZ10, FEGK13, PMvdG+13, KVW14, BKLW14, BLS+16, BWZ16, WZ16, SWZ17] and also many other machine learning problems such as clustering, boosting, and column subset selection [BBLM14, BLG+15, ABW17]. Thus, it is natural to ask whether our algorithm can be applied in the distributed setting. This section will discuss the distributed Frobenius norm low rank tensor approximation protocol in the so-called arbitrary-partition model (see, e.g. [KVW14, BWZ16]).\nIn the following, we extend the definition of the arbitrary-partition model [KVW14] to fit our tensor setting.\nDefinition J.1 (Arbitrary-partition model [KVW14]). There are s machines, and the ith machine holds a tensor Ai ∈ Rn×n×n as its local data tensor. The global data tensor is implicit and is denoted as A = ∑s i=1Ai. Then, we say that A is arbitrarily partitioned into s matrices distributed in the s machines. In addition, there is also a coordinator. In this model, the communication is only allowed between the machines and the coordinator. The total communication cost is the total number of words delivered between machines and the coordinator. Each word has O(log(sn)) bits.\nNow, let us introduce the distributed Frobenius norm low rank tensor approximation problem in the arbitrary partition model:\nDefinition J.2 (Arbitrary-partition model Frobenius norm rank-k tensor approximation). Tensor A ∈ Rn×n×n is arbitrarily partitioned into s matrices A1, A2, · · · , As distributed in s machines respectively, and ∀i ∈ [s], each entry of Ai is at most O(log(sn)) bits. Given tensor A, k ∈ N+ and an error parameter 0 < < 1, the goal is to find a distributed protocol in the model of Definition J.1 such that\n1. Upon termination, the protocol leaves three matrices U∗, V ∗,W ∗ ∈ Rn×k on the coordinator.\n2. U∗, V ∗,W ∗ satisfies that ∥∥∥∥∥ k∑\ni=1\nU∗i ⊗ V ∗i ⊗W ∗i −A ∥∥∥∥∥ 2\nF\n≤ (1 + ) min rank−k A′ ‖A′ −A‖2F .\n3. The communication cost is as small as possible.\nTheorem J.3. Suppose tensor A ∈ Rn×n×n is distributed in the arbitrary partition model (See Definition J.1). There is a protocol( in Algorithm 39) which solves the problem in Definition J.2 with constant success probability. In addition, the communication complexity of the protocol is s(poly(k/ ) +O(kn)) words.\nProof. Correctness. The correctness is implied by Algorithm 2 and Algorithm 3 (Theorem C.1.) Notice that A1 = ∑s i=1Ai,1, A2 = ∑s i=1Ai,2, A3 = ∑s i=1Ai,3, which means that\nY1 = T1A1S1, Y2 = T2A2S2, Y3 = T3A3S3,\nand\nC = A(T1, T2, T3).\nAccording to line 23,\nX∗1 , X ∗ 2 , X ∗ 3 = arg min\nX1,X2,X3 ∥∥∥∥∥∥ k∑\nj=1\n(Y1X1)j ⊗ (Y2X2)j ⊗ (Y3X3)j − C ∥∥∥∥∥∥ F .\nAccording to Lemma C.3, we have ∥∥∥∥∥∥ k∑\nj=1\n(T1A1S1X ∗ 1 )j ⊗ (T2A2S2X∗2 )j ⊗ (T3A3S3X∗3 )j −A(T1, T2, T3) ∥∥∥∥∥∥ 2\nF\n≤(1 +O( )) min X1,X2,X3 ∥∥∥∥∥∥ k∑\nj=1\n(A1S1X1)j ⊗ (A2S2X2)j ⊗ (A3Y3X3)j −A ∥∥∥∥∥∥ 2\nF\n≤(1 +O( )) min U,V,W ∥∥∥∥∥ k∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥ 2\nF\n,\nwhere the last inequality follows by the proof of Theorem C.1. By scaling a constant of , we complete the proof of correctness.\nCommunication complexity. Since S1, S2, S3 are w1-wise independent, and T1, T2, T3 are w2-wise independent, the communication cost of sending random seeds in line 5 is O(s(w1 + w2)) words, where w1 = O(k), w2 = O(1) (see [KVW14, CW13, Woo14, KN14]). The communication cost in line 18 is s · poly(k/ ) words due to T1Ai,1S1, T2Ai,2S2, T3Ai,3S3 ∈ Rpoly(k/ )×O(k/ ) and Ci = Ai(T1, T2, T3) ∈ Rpoly(k/ )×poly(k/ )×poly(k/ ).\nNotice that, since ∀i ∈ [s] each entry of Ai has at most O(log(sn)) bits, each entry of Y1, Y2, Y3, C has at most O(log(sn)) bits. Due to Theorem J.7, each entry of X∗1 , X∗2 , X∗3 has at most O(log(sn)) bits, and the sizes of X∗1 , X∗2 , X∗3 are poly(k/ ) words. Thus the communication cost in line 24 is s · poly(k/ ) words.\nFinally, since ∀i ∈ [s], U∗i , V ∗i ,W ∗i ∈ Rn×k, the communication here is at most O(skn) words. The total communication cost is s(poly(k/ ) +O(kn)) words.\nRemark J.4. If we slightly change the goal in Definition J.2 to the following: the coordinator does not need to output U∗, V ∗,W ∗, but each machine i holds U∗i , V ∗ i ,W ∗ i such that U ∗ = ∑s i=1 U ∗ i , V\n∗ =∑s i=1 V ∗ i ,W ∗ = ∑s i=1W ∗ i , then the protocol shown in Algorithm 39 does not have to do the line 28. Thus the total communication cost is at most s · poly(k/ ) words in this setting.\nRemark J.5. Algorithm 39 needs exponential in poly(k/ ) running time since it solves a polynomial solver in line 23. Instead of solving line 23, we can solve the following optimization problem:\nα∗ = arg min α∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nαi,j,l · (Y1)i ⊗ (Y2)j ⊗ (Y3)l − C ∥∥∥∥∥∥ F .\nSince it is actually a regression problem, it only takes polynomial running time to get α∗. And according to Lemma C.5,\ns1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nα∗i,j,l · (Y1)i ⊗ (Y2)j ⊗ (Y3)l\nAlgorithm 39 Distributed Frobenius Norm Low Rank Approximation Protocol 1: procedure DistributedFnormLowRankApproxProtocol(A, ,k,s) 2: A ∈ Rn×n×n was arbitrarily partitioned into s matrices A1, · · · , As ∈ Rn×n×n on s machines. 3: Coordinator Machines i 4: Chooses a random seed. 5: Sends it to all machines. 6: −−−−−−−−− > 7: si ← O(k/ ), ∀i ∈ [3]. 8: Agree on Si ∈ Rn2×si , ∀i ∈ [3] 9: which are w1-wise independent random 10: N(0, 1/si) Gaussian matrices. 11: ti ← poly(k/ ), ∀i ∈ [3]. 12: Agree on Ti ∈ Rti×n, ∀i ∈ [3] 13: which are w2-wise independent random 14: sparse embedding matrices. 15: Compute Yi,1 ← T1Ai,1S1, 16: Yi,2 ← T2Ai,2S2, Yi,3 ← T3Ai,3S3. 17: Send Yi,1, Yi,2, Yi,3 to the coordinator. 18: Send Ci ← Ai(T1, T2, T3) to the coordinator. 19: < −−−−−−−−− 20: Compute Y1 ←\ns∑ i=1 Yi,1, Y2 ← s∑ i=1 Yi,2,\n21: Y3 ← s∑ i=1 Yi,3, C ← s∑ i=1 Ci. 22: Compute X∗1 , X∗2 , X∗3 by solving 23: min\nX1,X2,X3 ‖(Y1X1)⊗ (Y2X2)⊗ (Y3X3)− C‖F\n24: Send X∗1 , X∗2 , X∗3 to machines. 25: −−−−−−−−− > 26: Compute U∗i ← Ai,1S1X∗1 , 27: V ∗i ← Ai,2S2X∗2 , W ∗i ← Ai,3S3X∗3 . 28: Send U∗i , V ∗ i ,W ∗ i to the coordinator. 29: < −−−−−−−−− 30: Compute U∗ ←∑si=1 U∗i . 31: Compute V ∗ ←∑si=1 V ∗i . 32: Compute W ∗ ←∑si=1W ∗i . 33: return U∗, V ∗, W ∗. 34: end procedure\ngives a rank-O(k3/ 3) bicriteria solution. Further, similar to Theorem C.8, we can solve\nmin U∈Rn×s2s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\nUi+s1(j−1) ⊗ (Y2)i ⊗ (Y3)j − C ∥∥∥∥∥∥ F ,\nwhere C = ∑\niAi(I, T2, T3). Thus, we can obtain a rank-O(k 2/ 2) in polynomial time.\nRemark J.6. If we select sketching matrices S1, S2, S3, T1, T2, T3 to be random Cauchy matrices,\nthen we are able to compute distributed entry-wise `1 norm rank-k tensor approximation (see Theorem D.17). The communication cost is still s(poly(k/ ) + O(kn)) words. If we only require a bicriteria solution, then it only needs polynomial running time.\nUsing similar techniques as in the proof of Theorem C.45, we can obtain:\nTheorem J.7. Let maxi{ti, di} ≤ n. Given a t1 × t2 × t3 tensor A and three matrices: a t1 × d1 matrix T1, a t2 × d2 matrix T2, and a t3 × d3 matrix T3. For any δ > 0, if there exists a solution to\nmin X1,X2,X3 ∥∥∥∥∥ k∑\ni=1\n(T1X1)i ⊗ (T2X2)i ⊗ (T3X3)i −A ∥∥∥∥∥ 2\nF\n:= OPT,\nand each entry of Xi can be expressed using O(log n) bits, then there exists an algorithm that takes poly(log n) · 2O(d1k+d2k+d3k) time and outputs three matrices: X̂1, X̂2, and X̂3 such that ‖(T1X̂1)⊗ (T2X̂2)⊗ (T3X̂3)−A‖2F = OPT."
    }, {
      "heading" : "K Streaming Setting",
      "text" : "One of the computation models which is closely related to the distributed model of computation is the streaming model. There is a growing line of work in the streaming model. Some problems are very fundamental in the streaming model such like Heavy Hitters [LNNT16, BCI+16, BCIW16], and streaming numerical linear algebra problems [CW09]. Streaming low rank matrix approximation has been extensively studied by previous work like [CW09, KL11, GP14, Lib13, KLM+14, BWZ16, SWZ17]. In this section, we show that there is a streaming algorithm which can compute a low rank tensor approximation.\nIn the following, we introduce the turnstile streaming model and the turnstile streaming tensor Frobenius norm low rank approximation problem. The following gives a formal definition of the computation model we study.\nDefinition K.1 (Turnstile model). Initially, tensor A ∈ Rn×n×n is an all zero tensor. In the turnstile streaming model, there is a stream of update operations, and the ith update operation is in the form (xi, yi, zi, δi) where xi, yi, zi ∈ [n], and δi ∈ R has O(log n) bits. Each (xi, yi, zi, δi) means that Axi,yi,zi should be incremented by δi. And each entry of A has at most O(log n) bits at the end of the stream. An algorithm in this computation model is only allowed one pass over the stream. At the end of the stream, the algorithm stores a summary of A. The space complexity of the algorithm is the total number of words required to compute and store this summary while scanning the stream. Here, each word has at most O(log(n)) bits.\nThe following is the formal definition of the problem.\nDefinition K.2 (Turnstile model Frobenius norm rank-k tensor approximation). Given tensor A ∈ Rn×n×n, k ∈ N+ and an error parameter 1 > > 0, the goal is to design an algorithm in the streaming model of Definition K.1 such that\n1. Upon termination, the algorithm outputs three matrices U∗, V ∗,W ∗ ∈ Rn×k.\n2. U∗, V ∗,W ∗ satisft that\n∥∥∥∥∥ k∑\ni=1\nU∗i ⊗ V ∗i ⊗W ∗i −A ∥∥∥∥∥ 2\nF\n≤ (1 + ) min rank−k A′ ‖A′ −A‖2F .\n3. The space complexity of the algorithm is as small as possible.\nTheorem K.3. Suppose tensor A ∈ Rn×n×n is given in the turnstile streaming model (see Definition K.1), there is an streaming algorithm (in Algorithm 40) which solves the problem in Definition K.2 with constant success probability. In addition, the space complexity of the algorithm is poly(k/ ) +O(nk/ ) words.\nProof. Correctness. Similar to the distributed protocol, the correctness of this streaming algorithm is also implied by Algorithm 2 and Algorithm 3 (Theorem C.1.) Notice that at the end of the stream V1 = A1S1 ∈ Rn×s1 , V2 = A2S2 ∈ Rn×s2 , V3 = A3S3 ∈ Rn×s3 , C = A(T1, T2, T3) ∈ Rt1×t2×t3 . It also means that\nY1 = T1A1S1, Y2 = T2A2S2, Y3 = T3A3S3.\nAccording to line 26 of procedure TurnstileStreaming,\nX∗1 , X ∗ 2 , X ∗ 3 = arg min\nX1∈Rs1×k,X2∈Rs2×k,X3∈Rs3×k ∥∥∥∥∥∥ k∑\nj=1\n(Y1X1)j ⊗ (Y2X2)j ⊗ (Y3X3)j − C ∥∥∥∥∥∥ F\nAccording to Lemma C.3, we have ∥∥∥∥∥∥ k∑\nj=1\n(Y1X1)j ⊗ (Y2X2)j ⊗ (Y3X3)j − C ∥∥∥∥∥∥ 2\nF\n= ∥∥∥∥∥∥ k∑\nj=1\n(T1A1S1X ∗ 1 )j ⊗ (T2A2S2X∗2 )j ⊗ (T3A3S3X∗3 )j −A(T1, T2, T3) ∥∥∥∥∥∥ 2\nF\n≤ (1 +O( )) min X1,X2,X3 ∥∥∥∥∥∥ k∑\nj=1\n(A1S1X1)j ⊗ (A2S2X2)j ⊗ (A3Y3X3)j −A ∥∥∥∥∥∥ 2\nF\n≤ (1 +O( )) min U,V,W ∥∥∥∥∥ k∑\ni=1\nUi ⊗ Vi ⊗Wi −A ∥∥∥∥∥ 2\nF\n,\nwhere the last inequality follows by the proof of Theorem C.1. By scaling a constant of , we complete the proof of correctness.\nSpace complexity. Since S1, S2, S3 are w1-wise independent, and T1, T2, T3 are w2-wise independent, the space needed to construct these sketching matrices in line 3 and line 5 of procedure TurnstileStreaming is O(w1 + w2) words, where w1 = O(k), w2 = O(1) (see [KVW14, CW13, Woo14, KN14]). The cost to maintain V1, V2, V3 is O(nk/ ) words, and the cost to maintain C is poly(k/ ) words.\nNotice that, since each entry of A has at most O(log(sn)) bits, each entry of Y1, Y2, Y3, C has at most O(log(sn)) bits. Due to Theorem J.7, each entry of X∗1 , X∗2 , X∗3 has at most O(log(sn)) bits, and the sizes of X∗1 , X∗2 , X∗3 are poly(k/ ) words. Thus the space cost in line 26 is poly(k/ ) words.\nThe total space cost is poly(k/ ) +O(nk/ ) words.\nRemark K.4. In the Algorithm 40, for each update operation, we need O(k/ ) time to maintain matrices V1, V2, V3, and we need poly(k/ ) time to maintain tensor C. Thus the update time is poly(k/ ). At the end of the stream, the time to compute\nX∗1 , X ∗ 2 , X ∗ 3 = arg min\nX1,X2,X3∈RO(k/ )×k\n∥∥∥∥∥∥ k∑\nj=1\n(Y1X1)j ⊗ (Y2X2)j ⊗ (Y3X3)j − C ∥∥∥∥∥∥ F ,\nis exponential in poly(k/ ) running time since it should use a polynomial system solver. Instead of computing the rank-k solution, we can solve the following:\nα∗ = arg min α∈Rs1×s2×s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nαi,j,l · (Y1)i ⊗ (Y2)j ⊗ (Y3)l − C ∥∥∥∥∥∥ F\nAlgorithm 40 Turnstile Frobenius Norm Low Rank Approximation Algorithm 1: procedure TurnstileStreaming(k,S) 2: s1 ← s2 ← s3 ← O(k/ ). 3: Construct sketching matrices Si ∈ Rn2×si ,∀i ∈ [3] where entries of S1, S2, S3 are w1-wise\nindependent random N(0, 1/si) Gaussian variables. 4: t1 ← t2 ← t3 ← poly(k/ ). 5: Construct sparse embedding matrices Ti ∈ Rti×n, ∀i ∈ [3] where entries are w2-wise inde-\npendent. 6: Initialize matrices: 7: Vi ← {0}n×si ,∀i ∈ [3]. 8: C ← {0}t1×t2×t3 9: for i ∈ [l] do\n10: Receive update operation (xi, yi, zi, δi) from the data stream S. 11: for r = 1→ s1 do 12: (V1)xi,r ← (V1)xi,r + δi · (S1)(yi−1)n+zi,r. 13: end for 14: for r = 1→ s2 do 15: (V2)yi,r ← (V2)yi,r + δi · (S2)(zi−1)n+xi,r. 16: end for 17: for r = 1→ s3 do 18: (V3)zi,r ← (V3)zi,r + δi · (S3)(xi−1)n+yi,r. 19: end for 20: for r = 1→ t1, p = 1→ t2, q = 1→ t3 do 21: Cr,p,q ← Cr,p,q + δi · (T1)r,xi(T2)p,yi(T3)q,zi . 22: end for 23: end for 24: Compute Y1 ← T1V1, Y2 ← T2V2, Y3 ← T3V3. 25: Compute X∗i ∈ Rsi×k,∀i ∈ [3] by solving 26: min\nX1,X2,X3 ‖(Y1X1)⊗ (Y2X2)⊗ (Y3X3)− C‖F\n27: Compute U∗ ← V1X∗1 , V ∗ ← V2X∗2 , W ∗ ← V3X∗3 . 28: return U∗, V ∗,W ∗ 29: end procedure\nwhich will then give\ns1∑\ni=1\ns2∑\nj=1\ns3∑\nl=1\nα∗i,j,l · (Y1)i ⊗ (Y2)j ⊗ (Y3)l\nto be a rank-O(k3/ 3) bicriteria solution. Further, similar to Theorem C.8, we can solve\nmin U∈Rn×s2s3 ∥∥∥∥∥∥ s1∑\ni=1\ns2∑\nj=1\nUi+s1(j−1) ⊗ (Y2)i ⊗ (Y3)j − C ∥∥∥∥∥∥ F\nwhere C = ∑\niAi(I, T2, T3). Thus, we can obtain a rank-O(k 2/ 2) in polynomial time.\nRemark K.5. If we choose S1, S2, S3, T1, T2, T3 to be random Cauchy matrices, then we are able to apply the entry-wise `1 norm low rank tensor approximation algorithm (see Theorem D.17) in turnstile model."
    }, {
      "heading" : "L Extension to Other Tensor Ranks",
      "text" : "The tensor rank studied in the previous sections is also called the CP rank or canonical rank. The tensor rank can be thought of as a direct extension of the matrix rank. We would like to point out that there are other definitions of tensor rank, e.g., the tucker rank and train rank. In this section we explain how to extend our proofs to other notions of tensor rank. Section L.1 provides the extension to tucker rank, and Section L.2 provides the extension to train rank."
    }, {
      "heading" : "L.1 Tensor Tucker rank",
      "text" : "Tensor Tucker rank has been studied in a number of works [KC07, PC08, MH09, ZW13, YC14]. We provide the formal definition here:"
    }, {
      "heading" : "L.1.1 Definitions",
      "text" : "Definition L.1 (Tucker rank). Given a third order tensor A ∈ Rn×n×n, we say A has tucker rank k if k is the smallest integer such that there exist three matrices U, V,W ∈ Rn×k and a (small) tensor C ∈ Rk×k×k satisfying\nAi,j,l =\nk∑\ni′=1\nk∑\nj′=1\nk∑\nl′=1\nCi′,j′,l′Ui,i′Vj,j′Wl,l′ , ∀i, j, l ∈ [n]× [n]× [n],\nor equivalently,\nA = C(U, V,W )."
    }, {
      "heading" : "L.1.2 Algorithm",
      "text" : "Algorithm 41 Frobenius Norm Low (Tucker) Rank Approximation\n1: procedure FLowTuckerRankApprox(A,n, k, ) . Theorem L.2 2: s1 ← s2 ← s3 ← O(k/ ). 3: t1 ← t2 ← t3 ← poly(k, 1/ ). 4: Choose sketching matrices S1 ∈ Rn2×s1 , S2 ∈ Rn2×s2 , S3 ∈ Rn2×s3 . . Definition B.18 5: Choose sketching matrices T1 ∈ Rt1×n, T2 ∈ Rt2×n, T3 ∈ Rt3×n. 6: Compute AiSi,∀i ∈ [3]. 7: Compute TiAiSi, ∀i ∈ [3]. 8: Compute B ← A(T1, T2, T3). 9: Create variables for Xi ∈ Rsi×k,∀i ∈ [3].\n10: Create variables for C ∈ Rk×k×k. 11: Run a polynomial system verifier for ‖C((Y1X1), (Y2X2), (Y3X3))−B‖2F . 12: return C,A1S1X1, A2S2X2, and A3S3X3. 13: end procedure\nTheorem L.2. Given a third order tensor A ∈ Rn×n×n, for any k ≥ 1 and ∈ (0, 1), there exists an algorithm which takes O(nnz(A)) + n poly(k, 1/ ) + 2O(k2/ +k3) time and outputs three matrices U, V,W ∈ Rn×k, and a tensor C ∈ Rk×k×k for which\n‖C(U, V,W )−A‖2F ≤ (1 + ) min tucker rank−k Ak ‖Ak −A‖2F\nholds with probability 9/10.\nProof. We define OPT to be\nOPT = min tucker rank−k A′\n‖A′ −A‖2F .\nSuppose the optimal Ak = C∗(U∗, V ∗,W ∗). We fix C∗ ∈ Rk×k×k, V ∗ ∈ Rn×k and W ∗ ∈ Rn×k. We use V ∗1 , V ∗2 , · · · , V ∗k to denote the columns of V ∗ and W ∗1 ,W ∗2 , · · · ,W ∗k to denote the columns of W ∗.\nWe consider the following optimization problem,\nmin U1,··· ,Uk∈Rn\n‖C∗(U, V ∗,W ∗)−A‖2F ,\nwhich is equivalent to\nmin U1,··· ,Uk∈Rn\n‖U · C∗(I, V ∗,W ∗)−A‖2F ,\nbecause C∗(U, V ∗,W ∗) = U · C∗(I, V ∗,W ∗) according to Definition A.6. Recall that C∗(I, V ∗,W ∗) denotes a k × n × n tensor. Let (C∗(I, V ∗,W ∗))1 denote the matrix obtained by flattening C∗(I, V ∗,W ∗) along the first dimension. We use matrix Z1 to denote (C∗(I, V ∗,W ∗))1 ∈ Rk×n2 . Then we can obtain the following equivalent objective function,\nmin U∈Rn×k\n‖UZ1 −A1‖2F .\nNotice that minU∈Rn×k ‖UZ1 −A1‖2F = OPT, since Ak = U∗Z1. Let S>1 ∈ Rs1×n\n2 be the sketching matrix defined in Definition B.18, where s1 = O(k/ ). We obtain the following optimization problem,\nmin U∈Rn×k\n‖UZ1S1 −A1S1‖2F .\nLet Û ∈ Rn×k denote the optimal solution to the above optimization problem. Then Û = A1S1(Z1S1) †. By Lemma B.22 and Theorem B.23, we have\n‖ÛZ1 −A1‖2F ≤ (1 + ) min U∈Rn×k ‖UZ1 −A1‖2F = (1 + ) OPT,\nwhich implies ∥∥∥C∗(Û , V ∗,W ∗)−A ∥∥∥ 2\nF ≤ (1 + ) OPT .\nTo write down Û1, · · · , Ûk, we use the given matrix A1, and we create s1 × k variables for matrix (Z1S1)\n†. As our second step, we fix Û ∈ Rn×k and W ∗ ∈ Rn×k, and we convert tensor A into matrix A2.\nLet matrix Z2 denote (C∗(Û , I,W ∗))2 ∈ Rk×n2 . We consider the following objective function,\nmin V ∈Rn×k\n‖V Z2 −A2‖2F ,\nfor which the optimal cost is at most (1 + ) OPT.\nLet S>2 ∈ Rs2×n 2 be a sketching matrix defined in Definition B.18, where s2 = O(k/ ). We\nsketch S2 on the right of the objective function to obtain a new objective function,\nmin V ∈Rn×k\n‖V Z2S2 −A2S2‖2F .\nLet V̂ ∈ Rn×k denote the optimal solution to the above problem. Then V̂ = A2S2(Z2S2)†. By Lemma B.22 and Theorem B.23, we have,\n‖V̂ Z2 −A2‖2F ≤ (1 + ) min V ∈Rn×k ‖V Z2 −A2‖2F ≤ (1 + )2 OPT,\nwhich implies ∥∥∥C∗(Û , V̂ ,W ∗)−A ∥∥∥ 2\nF ≤ (1 + )2 OPT .\nTo write down V̂1, · · · , V̂k, we need to use the given matrix A2 ∈ Rn 2×n, and we need to create s2 × k variables for matrix (Z2S2)†. As our third step, we fix the matrices Û ∈ Rn×k and V̂ ∈ Rn×k. We convert tensor A ∈ Rn×n×n into matrix A3 ∈ Rn2×n. Let matrix Z3 denote (C∗(Û , V̂ , I))3 ∈ Rk×n2 . We consider the following objective function,\nmin W∈Rn×k\n‖WZ3 −A3‖2F ,\nwhich has optimal cost at most (1 + )2 OPT. Let S>3 ∈ Rs3×n\n2 be a sketching matrix defined in Definition B.18, where s3 = O(k/ ). We sketch S3 on the right of the objective function to obtain a new objective function,\nmin W∈Rn×k\n‖WZ3S3 −A3S3‖2F .\nLet Ŵ ∈ Rn×k denote the optimal solution of the above problem. Then Ŵ = A3S3(Z3S3)†. By Lemma B.22 and Theorem B.23, we have,\n‖ŴZ3 −A3‖2F ≤ (1 + ) min W∈Rn×k ‖WZ3 −A3‖2F ≤ (1 + )3 OPT .\nThus, we have\nmin X1,X2,X3\n‖C∗((A1S1X1), (A2S2X2), (A3S3X3))−A‖2F ≤ (1 + )3 OPT .\nLet V1 = A1S1, V2 = A2S2, and V3 = A3S3.We then apply Lemma C.3, and we obtain V̂1, V̂2, V̂3, B. We then apply Theorem C.45. Correctness follows by rescaling by a constant factor.\nRunning time. Due to Definition B.18, the running time of line 7 (Algorithm 41) is O(nnz(A))+ n poly(k, 1/ ). Due to Lemma C.3, line 7 and 8 can be executed in nnz(A) + n poly(k, 1/ ) time. The running time of line 11 is given by Theorem C.45. (For simplicity, we ignore the bit complexity in the running time.)"
    }, {
      "heading" : "L.2 Tensor Train rank",
      "text" : ""
    }, {
      "heading" : "L.2.1 Definitions",
      "text" : "The tensor train rank has been studied in several works [Ose11, OTZ11, ZWZ16, PTBD16]. We provide the formal definition here.\nDefinition L.3 (Tensor Train rank). Given a third order tensor A ∈ Rn×n×n, we say A has train rank k if k is the smallest integer such that there exist three tensors U ∈ R1×n×k, V ∈ Rk×n×k, W ∈ Rk×n×1 satisfying:\nAi,j,l =\n1∑\ni1=1\nk∑\ni2=1\nk∑\ni3=1\n1∑\ni4=1\nUi1,i,i2Vi2,j,i3Wi3,l,i4 ,∀i, j, l ∈ [n]× [n]× [n],\nor equivalently,\nAi,j,l = k∑\ni2=1\nk∑\ni3=1\n(U2)i,i2(V2)j,i2+k(i3−1)(W2)l,i3 ,\nwhere V2 ∈ Rn×k2 denotes the matrix obtained by flattening the tensor U along the second dimension, and (V2)i,i1+k(i2−1) denotes the entry in the i-th row and i1 +k(i2−1)-th column of V2. We similarly define U2,W2 ∈ Rn×k.\nAlgorithm 42 Frobenius Norm Low (Train) rank Approximation\n1: procedure FLowTrainRankApprox(A,n, k, ) . Theorem L.4 2: s1 ← s3 ← O(k/ ). 3: s2 ← O(k2/ ). 4: t1 ← t2 ← t3 ← poly(k, 1/ ). 5: Choose sketching matrices S1 ∈ Rn2×s1 , S2 ∈ Rn2×s2 , S3 ∈ Rn2×s3 . . Definition B.18 6: Choose sketching matrices T1 ∈ Rt1×n, T2 ∈ Rt2×n, T3 ∈ Rt3×n. 7: Compute AiSi,∀i ∈ [3]. 8: Compute TiAiSi, ∀i ∈ [3]. 9: Compute B ← A(T1, T2, T3).\n10: Create variables for X1 ∈ Rs1×k. 11: Create variables for X3 ∈ Rs3×k. 12: Create variables for X2 ∈ Rs2×k2 . 13: Create variables for C ∈ Rk×k×k. 14: Run polynomial system verifier for ‖∑ki2=1 ∑k i3=1\n(Y1X1)i2(Y2X2)i2+k(i3−1)(Y3X3)i3 −B‖2F . 15: return A1S1X1, A2S2X2, and A3S3X3. 16: end procedure"
    }, {
      "heading" : "L.2.2 Algorithm",
      "text" : "Theorem L.4. Given a third order tensor A ∈ Rn×n×n, for any k ≥ 1, ∈ (0, 1), there exists an algorithm which takes O(nnz(A)) + n poly(k, 1/ ) + 2O(k4/ ) time and outputs three tensors U ∈ R1×n×k, V ∈ Rk×n×k, W ∈ Rk×n×1 such that\n∥∥∥∥∥∥ k∑\ni=1\nk∑\nj=1\n(U2)i ⊗ (V2)i+k(j−1) ⊗ (W2)j −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + ) min train rank−k Ak ‖Ak −A‖2F\nholds with probability 9/10.\nProof. We define OPT as\nOPT = min train rank−k A′\n‖A′ −A‖2F .\nSuppose the optimal\nAk =\nk∑\ni=1\nk∑\nj=1\nU∗i ⊗ V ∗i+k(j−1) ⊗W ∗j .\nWe fix V ∗ ∈ Rn×k2 and W ∗ ∈ Rn×k. We use V ∗1 , V ∗2 , · · · , V ∗k2 to denote the columns of V ∗, and W ∗1 ,W ∗ 2 , · · · ,W ∗k to denote the columns of W ∗.\nWe consider the following optimization problem,\nmin U∈Rn×k ∥∥∥∥∥∥ k∑\ni=1\nk∑\nj=1\nUi ⊗ V ∗i+k(j−1) ⊗W ∗j −A ∥∥∥∥∥∥ 2\nF\n,\nwhich is equivalent to\nmin U∈Rn×k ∥∥∥∥∥∥∥∥∥∥∥∥∥∥∥ U ·   k∑ j=1 V ∗1+k(j−1) ⊗W ∗j k∑ j=1 V ∗2+k(j−1) ⊗W ∗j · · ·\nk∑ j=1 V ∗k+k(j−1) ⊗W ∗j\n  −A ∥∥∥∥∥∥∥∥∥∥∥∥∥∥∥ 2\nF\n.\nLet A1 ∈ Rn×n2 denote the matrix obtained by flattening the tensor A along the first dimension. We use matrix Z1 ∈ Rk×n2 to denote\n  k∑ j=1 vec(V ∗1+k(j−1) ⊗W ∗j ) k∑ j=1 vec(V ∗2+k(j−1) ⊗W ∗j )\n· · · k∑ j=1 vec(V ∗k+k(j−1) ⊗W ∗j )\n  .\nThen we can obtain the following equivalent objective function,\nmin U∈Rn×k\n‖UZ1 −A1‖2F .\nNotice that minU∈Rn×k ‖UZ1 −A1‖2F = OPT, since Ak = U∗Z1. Let S>1 ∈ Rs1×n\n2 be a sketching matrix defined in Definition B.18, where s1 = O(k/ ). We obtain the following optimization problem,\nmin U∈Rn×k\n‖UZ1S1 −A1S1‖2F .\nLet Û ∈ Rn×k denote the optimal solution to the above optimization problem. Then Û = A1S1(Z1S1) †. By Lemma B.22 and Theorem B.23, we have\n‖ÛZ1 −A1‖2F ≤ (1 + ) min U∈Rn×k ‖UZ1 −A1‖2F = (1 + ) OPT,\nwhich implies ∥∥∥∥∥∥ k∑\ni=1\nk∑\nj=1\nÛi ⊗ V ∗i+k(j−1) ⊗W ∗j −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + ) OPT .\nTo write down Û1, · · · , Ûk, we use the given matrix A1, and we create s1 × k variables for matrix (Z1S1)\n†. As our second step, we fix Û ∈ Rn×k and W ∗ ∈ Rn×k, and we convert the tensor A into matrix A2. Let matrix Z2 ∈ Rk2×n2 denote the matrix where the (i, j)-th row is the vectorization of Ûi ⊗W ∗j . We consider the following objective function,\nmin V ∈Rn×k\n‖V Z2 −A2‖2F ,\nfor which the optimal cost is at most (1 + ) OPT. Let S>2 ∈ Rs2×n\n2 be a sketching matrix defined in Definition B.18, where s2 = O(k2/ ). We sketch S2 on the right of the objective function to obtain the new objective function,\nmin V ∈Rn×k\n‖V Z2S2 −A2S2‖2F .\nLet V̂ ∈ Rn×k denote the optimal solution of the above problem. Then V̂ = A2S2(Z2S2)†. By Lemma B.22 and Theorem B.23, we have,\n‖V̂ Z2 −A2‖2F ≤ (1 + ) min V ∈Rn×k ‖V Z2 −A2‖2F ≤ (1 + )2 OPT,\nwhich implies ∥∥∥∥∥∥ k∑\ni=1\nk∑\nj=1\nÛi ⊗ V̂i+k(j−1) ⊗W ∗ −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + )2 OPT .\nTo write down V̂1, · · · , V̂k, we need to use the given matrix A2 ∈ Rn 2×n, and we need to create s2 × k variables for matrix (Z2S2)†. As our third step, we fix the matrices Û ∈ Rn×k and V̂ ∈ Rn×k. We convert tensor A ∈ Rn×n×n into matrix A3 ∈ Rn2×n. Let matrix Z3 ∈ Rk×n2 denote   ∑k i=1 vec(Ûi ⊗ V̂i+k·0)∑k i=1 vec(Ûi ⊗ V̂i+k·1)\n· · ·∑k i=1 vec(Ûi ⊗ V̂i+k·(k−1))\n  .\nWe consider the following objective function,\nmin W∈Rn×k\n‖WZ3 −A3‖2F ,\nwhich has optimal cost at most (1 + )2 OPT. Let S>3 ∈ Rs3×n\n2 be a sketching matrix defined in Definition B.18, where s3 = O(k/ ). We sketch S3 on the right of the objective function to obtain a new objective function,\nmin W∈Rn×k\n‖WZ3S3 −A3S3‖2F .\nLet Ŵ ∈ Rn×k denote the optimal solution of the above problem. Then Ŵ = A3S3(Z3S3)†. By Lemma B.22 and Theorem B.23, we have,\n‖ŴZ3 −A3‖2F ≤ (1 + ) min W∈Rn×k ‖WZ3 −A3‖2F ≤ (1 + )3 OPT .\nThus, we have\nmin X1,X2,X3 ∥∥∥∥∥∥ k∑\ni=1\nk∑\nj=1\n(A1S1X1)i ⊗ (A2S2X2)i+k(j−1) ⊗ (A3S3X3)j −A ∥∥∥∥∥∥ 2\nF\n≤ (1 + )3 OPT .\nLet V1 = A1S1, V2 = A2S2, and V3 = A3S3.We then apply Lemma C.3, and we obtain V̂1, V̂2, V̂3, B. We then apply Theorem C.45. Correctness follows by rescaling by a constant factor.\nRunning time. Due to Definition B.18, the running time of line 7 (Algorithm 42) is O(nnz(A))+ n poly(k, 1/ ). Due to Lemma C.3, lines 8 and 9 can be executed in nnz(A) + n poly(k, 1/ ) time. The running time of 2O(k4/ ) comes from running Theorem C.45 (For simplicity, we ignore the bit complexity in the running time.)"
    }, {
      "heading" : "M Acknowledgments",
      "text" : "The authors would like to thank Udit Agarwal, Alexandr Andoni, Arturs Backurs, Saugata Basu, Lijie Chen, Xi Chen, Thomas Dillig, Yu Feng, Rong Ge, Daniel Hsu, Chi Jin, Ravindran Kannan, J. M. Landsberg, Qi Lei, Fu Li, Syed Mohammad Meesum, Ankur Moitra, Dana Moshkovitz, Cameron Musco, Richard Peng, Eric Price, Govind Ramnarayan, Ilya Razenshteyn, James Renegar, Rocco Servedio, Tselil Schramm, Clifford Stein, Wen Sun, Yining Wang, Zhaoran Wang, Wei Ye, Huacheng Yu, Huan Zhang, Kai Zhong, David Zuckerman for useful discussions."
    } ],
    "references" : [ {
      "title" : "Multiway analysis of epilepsy tensors",
      "author" : [ "Evrim Acar", "Canan Aykut-Bingöl", "Haluk Bingol", "Rasmus Bro", "Bülent Yener" ],
      "venue" : "In Proceedings 15th International Conference on Intelligent Systems for Molecular Biology (ISMB) & 6th European Conference on Computational Biology (ECCB),",
      "citeRegEx" : "Acar et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Acar et al\\.",
      "year" : 2007
    }, {
      "title" : "Greedy column subset selection: New bounds and distributed algorithms",
      "author" : [ "Jason Altschuler", "Aditya Bhaskara", "Gang Fu", "Vahab Mirrokni", "Afshin Rostamizadeh", "Morteza Zadimoghaddam" ],
      "venue" : "In International Conference on Machine Learning (ICML)",
      "citeRegEx" : "Altschuler et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Altschuler et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning mixtures of ranking models",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet", "Aravindan Vijayaraghavan" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Awasthi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2014
    }, {
      "title" : "General and robust communication-efficient algorithms for distributed clustering",
      "author" : [ "Pranjal Awasthi", "Maria-Florina Balcan", "Colin White" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Awasthi et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling and multiway analysis of chatroom tensors",
      "author" : [ "Evrim Acar", "Seyit A Çamtepe", "Mukkai S Krishnamoorthy", "Bülent Yener" ],
      "venue" : "In International Conference on Intelligence and Security Informatics,",
      "citeRegEx" : "Acar et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Acar et al\\.",
      "year" : 2005
    }, {
      "title" : "Collective sampling and analysis of high order tensors for chatroom communications",
      "author" : [ "Evrim Acar", "Seyit A Camtepe", "Bülent Yener" ],
      "venue" : "In International Conference on Intelligence and Security Informatics,",
      "citeRegEx" : "Acar et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Acar et al\\.",
      "year" : 2006
    }, {
      "title" : "Homotopy analysis for tensor pca",
      "author" : [ "Anima Anandkumar", "Yuan Deng", "Rong Ge", "Hossein Mobahi" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2016
    }, {
      "title" : "Tensors in image processing and computer vision",
      "author" : [ "Santiago Aja-Fernández", "Rodrigo de Luis Garcia", "Dacheng Tao", "Xuelong Li" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Aja.Fernández et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Aja.Fernández et al\\.",
      "year" : 2009
    }, {
      "title" : "A spectral algorithm for latent dirichlet allocation",
      "author" : [ "Anima Anandkumar", "Dean P Foster", "Daniel J Hsu", "Sham M Kakade", "Yi-Kai Liu" ],
      "venue" : "In Advances in Neural Information Processing Systems(NIPS),",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "Animashree Anandkumar", "Rong Ge", "Daniel J. Hsu", "Sham M. Kakade", "Matus Telgarsky" ],
      "venue" : "In Journal of Machine Learning Research,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2014
    }, {
      "title" : "A tensor approach to learning mixed membership community models",
      "author" : [ "Animashree Anandkumar", "Rong Ge", "Daniel J Hsu", "Sham M Kakade" ],
      "venue" : "In Journal of Machine Learning Research,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2014
    }, {
      "title" : "Computing a nonnegative matrix factorization - provably",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ravindran Kannan", "Ankur Moitra" ],
      "venue" : "In Proceedings of the 44th Symposium on Theory of Computing Conference (STOC),",
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "Provable learning of noisy-or networks",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Tengyu Ma", "Andrej Risteski" ],
      "venue" : "In Proceedings of the 49th Annual Symposium on the Theory of Computing (STOC). ACM,",
      "citeRegEx" : "Arora et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2016
    }, {
      "title" : "Scalable Tensor Factorizations for Incomplete Data",
      "author" : [ "E. Acar", "T.G. Kolda", "D.M. Dunlavy", "M. Morup" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Acar et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Acar et al\\.",
      "year" : 2010
    }, {
      "title" : "Streaming algorithms via precision sampling",
      "author" : [ "Alexandr Andoni", "Robert Krauthgamer", "Krzysztof Onak" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Andoni et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Andoni et al\\.",
      "year" : 2011
    }, {
      "title" : "Reinforcement learning of POMDPs using spectral methods",
      "author" : [ "Kamyar Azizzadenesheli", "Alessandro Lazaric", "Animashree Anandkumar" ],
      "venue" : "In 29th Annual Conference on Learning Theory (COLT), pages 193–256",
      "citeRegEx" : "Azizzadenesheli et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Azizzadenesheli et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequential transfer in multi-armed bandit with finite set of models",
      "author" : [ "Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill" ],
      "venue" : "In Advances in Neural Information Processing Systems(NIPS),",
      "citeRegEx" : "Azar et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Azar et al\\.",
      "year" : 2013
    }, {
      "title" : "Sparse higher-order principal components analysis",
      "author" : [ "Genevera Allen" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Allen.,? \\Q2012\\E",
      "shortCiteRegEx" : "Allen.",
      "year" : 2012
    }, {
      "title" : "Regularized tensor factorizations and higher-order principal components analysis",
      "author" : [ "Genevera I Allen" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Allen.,? \\Q2012\\E",
      "shortCiteRegEx" : "Allen.",
      "year" : 2012
    }, {
      "title" : "Fast computation of low-rank matrix approximations",
      "author" : [ "Dimitris Achlioptas", "Frank McSherry" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Achlioptas and McSherry.,? \\Q2007\\E",
      "shortCiteRegEx" : "Achlioptas and McSherry.",
      "year" : 2007
    }, {
      "title" : "Subspace embeddings for the polynomial kernel",
      "author" : [ "Haim Avron", "Huy Nguyen", "David Woodruff" ],
      "venue" : "In Advances in Neural Information Processing Systems(NIPS),",
      "citeRegEx" : "Avron et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Avron et al\\.",
      "year" : 2014
    }, {
      "title" : "Über homogene polynome in (l2)",
      "author" : [ "Stefan Banach" ],
      "venue" : "Studia Mathematica,",
      "citeRegEx" : "Banach.,? \\Q1938\\E",
      "shortCiteRegEx" : "Banach.",
      "year" : 1938
    }, {
      "title" : "Streaming symmetric norms via measure concentration",
      "author" : [ "Jaroslaw Blasiok", "Vladimir Braverman", "Stephen R Chestnut", "Robert Krauthgamer", "Lin F Yang" ],
      "venue" : "In Proceedings of the 49th Annual Symposium on the Theory of Computing(STOC). ACM,",
      "citeRegEx" : "Blasiok et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Blasiok et al\\.",
      "year" : 2017
    }, {
      "title" : "Distributed balanced clustering via mapping coresets",
      "author" : [ "MohammadHossein Bateni", "Aditya Bhaskara", "Silvio Lattanzi", "Vahab Mirrokni" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Bateni et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bateni et al\\.",
      "year" : 2014
    }, {
      "title" : "Bptree: an `2 heavy hitters algorithm using constant memory",
      "author" : [ "Vladimir Braverman", "Stephen R Chestnut", "Nikita Ivkin", "Jelani Nelson", "Zhengyu Wang", "David P Woodruff" ],
      "venue" : "In Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems (PODS)",
      "citeRegEx" : "Braverman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Braverman et al\\.",
      "year" : 2016
    }, {
      "title" : "Beating countsketch for heavy hitters in insertion streams",
      "author" : [ "Vladimir Braverman", "Stephen R Chestnut", "Nikita Ivkin", "David P Woodruff" ],
      "venue" : "In Proceedings of the 48th Annual Symposium on the Theory of Computing (STOC). https://arxiv",
      "citeRegEx" : "Braverman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Braverman et al\\.",
      "year" : 2016
    }, {
      "title" : "Sketches for matrix norms: Faster, smaller and more general",
      "author" : [ "Vladimir Braverman", "Stephen R Chestnut", "Robert Krauthgamer", "Lin F Yang" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Braverman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Braverman et al\\.",
      "year" : 2016
    }, {
      "title" : "Principal component analysis for distributed data sets with updating",
      "author" : [ "Zheng-Jian Bai", "Raymond H Chan", "Franklin T Luk" ],
      "venue" : "In Advanced Parallel Processing Technologies,",
      "citeRegEx" : "Bai et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2005
    }, {
      "title" : "Smoothed analysis of tensor decompositions",
      "author" : [ "Aditya Bhaskara", "Moses Charikar", "Ankur Moitra", "Aravindan Vijayaraghavan" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Bhaskara et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bhaskara et al\\.",
      "year" : 2014
    }, {
      "title" : "Algebraic complexity theory, volume 315",
      "author" : [ "Peter Bürgisser", "Michael Clausen", "Amin Shokrollahi" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Bürgisser et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Bürgisser et al\\.",
      "year" : 1997
    }, {
      "title" : "Uniqueness of tensor decompositions with applications to polynomial identifiability",
      "author" : [ "Aditya Bhaskara", "Moses Charikar", "Aravindan Vijayaraghavan" ],
      "venue" : "In 27th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Bhaskara et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bhaskara et al\\.",
      "year" : 2014
    }, {
      "title" : "Computing approximate PSD factorizations",
      "author" : [ "Amitabh Basu", "Michael Dinitz", "Xin Li" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Basu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Basu et al\\.",
      "year" : 2016
    }, {
      "title" : "Near optimal columnbased matrix reconstruction",
      "author" : [ "Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail" ],
      "venue" : "In IEEE 52nd Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2011
    }, {
      "title" : "Border rank of a p × q × 2 tensor and the optimal approximation of a pair of bilinear forms",
      "author" : [ "Dario Bini" ],
      "venue" : "Automata, languages and programming,",
      "citeRegEx" : "Bini.,? \\Q1980\\E",
      "shortCiteRegEx" : "Bini.",
      "year" : 1980
    }, {
      "title" : "Border rank of m × n × (mn-q) tensors",
      "author" : [ "Dario Bini" ],
      "venue" : "Linear Algebra and Its Applications,",
      "citeRegEx" : "Bini.,? \\Q1986\\E",
      "shortCiteRegEx" : "Bini.",
      "year" : 1986
    }, {
      "title" : "Improved distributed principal component analysis",
      "author" : [ "Maria-Florina Balcan", "Vandana Kanchanapally", "Yingyu Liang", "David Woodruff" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Balcan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2014
    }, {
      "title" : "A distributed frank-wolfe algorithm for communication-efficient sparse learning",
      "author" : [ "Aurélien Bellet", "Yingyu Liang", "Alireza Bagheri Garakani", "Maria-Florina Balcan", "Fei Sha" ],
      "venue" : "In Proceedings of the 2015 SIAM International Conference on Data Mining (ICDM),",
      "citeRegEx" : "Bellet et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bellet et al\\.",
      "year" : 2015
    }, {
      "title" : "Communication efficient distributed kernel principal component analysis",
      "author" : [ "Maria-Florina Balcan", "Yingyu Liang", "Le Song", "David Woodruff", "Bo Xie" ],
      "venue" : "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "Balcan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2016
    }, {
      "title" : "Noisy tensor completion via the sum-of-squares hierarchy",
      "author" : [ "Boaz Barak", "Ankur Moitra" ],
      "venue" : "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,",
      "citeRegEx" : "Barak and Moitra.,? \\Q2016\\E",
      "shortCiteRegEx" : "Barak and Moitra.",
      "year" : 2016
    }, {
      "title" : "An improved approximation algorithm for the column subset selection problem",
      "author" : [ "Christos Boutsidis", "Michael W Mahoney", "Petros Drineas" ],
      "venue" : "In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2009
    }, {
      "title" : "Matrix and tensor factorization methods for natural language processing",
      "author" : [ "Guillaume Bouchard", "Jason Naradowsky", "Sebastian Riedel", "Tim Rocktäschel", "Andreas Vlachos" ],
      "venue" : "In ACL (Tutorial Abstracts),",
      "citeRegEx" : "Bouchard et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bouchard et al\\.",
      "year" : 2015
    }, {
      "title" : "Topics in matrix sampling algorithms",
      "author" : [ "Christos Boutsidis" ],
      "venue" : "In Ph.D. Thesis. arXiv preprint",
      "citeRegEx" : "Boutsidis.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boutsidis.",
      "year" : 2011
    }, {
      "title" : "On the combinatorial and algebraic complexity of quantifier elimination",
      "author" : [ "Saugata Basu", "Richard Pollack", "Marie-Françoise Roy" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Basu et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Basu et al\\.",
      "year" : 1996
    }, {
      "title" : "Distributed principal component analysis for wireless sensor",
      "author" : [ "Yann-Ael Le Borgne", "Sylvain Raybaud", "Gianluca Bontempi" ],
      "venue" : "networks. Sensors,",
      "citeRegEx" : "Borgne et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Borgne et al\\.",
      "year" : 2008
    }, {
      "title" : "A new sampling technique for tensors",
      "author" : [ "Srinadh Bhojanapalli", "Sujay Sanghavi" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Bhojanapalli and Sanghavi.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bhojanapalli and Sanghavi.",
      "year" : 2015
    }, {
      "title" : "Twice-ramanujan sparsifiers",
      "author" : [ "Joshua Batson", "Daniel A Spielman", "Nikhil Srivastava" ],
      "venue" : "In SIAM Journal on Computing,",
      "citeRegEx" : "Batson et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Batson et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimal cur matrix decompositions",
      "author" : [ "Christos Boutsidis", "David P Woodruff" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Boutsidis and Woodruff.,? \\Q2014\\E",
      "shortCiteRegEx" : "Boutsidis and Woodruff.",
      "year" : 2014
    }, {
      "title" : "Optimal principal component analysis in distributed and streaming models",
      "author" : [ "Christos Boutsidis", "David P Woodruff", "Peilin Zhong" ],
      "venue" : "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2016
    }, {
      "title" : "Anaylsis of individual differences in multidimensional scaling via an n-way generalization of eckart-young",
      "author" : [ "J Douglas Carroll", "Jih-Jie Chang" ],
      "venue" : "decomposition. Psychometrika,",
      "citeRegEx" : "Carroll and Chang.,? \\Q1970\\E",
      "shortCiteRegEx" : "Carroll and Chang.",
      "year" : 1970
    }, {
      "title" : "Generalizing the column–row matrix decomposition to multi-way arrays",
      "author" : [ "Cesar F Caiafa", "Andrzej Cichocki" ],
      "venue" : "Linear Algebra and its Applications,",
      "citeRegEx" : "Caiafa and Cichocki.,? \\Q2010\\E",
      "shortCiteRegEx" : "Caiafa and Cichocki.",
      "year" : 2010
    }, {
      "title" : "The fast cauchy transform and faster robust linear regression",
      "author" : [ "Kenneth L Clarkson", "Petros Drineas", "Malik Magdon-Ismail", "Michael W Mahoney", "Xiangrui Meng", "David P Woodruff" ],
      "venue" : "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "Clarkson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Clarkson et al\\.",
      "year" : 2013
    }, {
      "title" : "Dimensionality reduction for k-means clustering and low rank approximation",
      "author" : [ "Michael B Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu" ],
      "venue" : "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Cohen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2015
    }, {
      "title" : "Subgradient and sampling algorithms for `1 regression",
      "author" : [ "Kenneth L Clarkson" ],
      "venue" : "In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms (SODA),",
      "citeRegEx" : "Clarkson.,? \\Q2005\\E",
      "shortCiteRegEx" : "Clarkson.",
      "year" : 2005
    }, {
      "title" : "Tensor decomposition of eeg signals: a brief review",
      "author" : [ "Fengyu Cong", "Qiu-Hua Lin", "Li-Dan Kuang", "Xiao-Feng Gong", "Piia Astikainen", "Tapani Ristaniemi" ],
      "venue" : "Journal of neuroscience methods,",
      "citeRegEx" : "Cong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cong et al\\.",
      "year" : 2015
    }, {
      "title" : "Uniform sampling for matrix approximation",
      "author" : [ "Michael B Cohen", "Yin Tat Lee", "Cameron Musco", "Christopher Musco", "Richard Peng", "Aaron Sidford" ],
      "venue" : "In Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science (ITCS),",
      "citeRegEx" : "Cohen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2015
    }, {
      "title" : "Iterative block tensor singular value thresholding for extraction of low rank component of image data",
      "author" : [ "Longxi Chen", "Yipeng Liu", "Ce Zhu" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Tensor decompositions for signal processing applications: From two-way to multiway component analysis",
      "author" : [ "Andrzej Cichocki", "Danilo Mandic", "Lieven De Lathauwer", "Guoxu Zhou", "Qibin Zhao", "Cesar Caiafa", "Huy Anh Phan" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "Cichocki et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cichocki et al\\.",
      "year" : 2015
    }, {
      "title" : "Optimal approximate matrix product in terms of stable rank",
      "author" : [ "Michael B Cohen", "Jelani Nelson", "David P Woodruff" ],
      "venue" : "In Proceedings of the 43rd International Colloquium on Automata, Languages and Programming (ICALP),",
      "citeRegEx" : "Cohen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2016
    }, {
      "title" : "Tensor Decompositions, State of the Art and Applications",
      "author" : [ "P. Comon" ],
      "venue" : "ArXiv eprints,",
      "citeRegEx" : "Comon.,? \\Q2009\\E",
      "shortCiteRegEx" : "Comon.",
      "year" : 2009
    }, {
      "title" : "`p row sampling by lewis weights",
      "author" : [ "Michael B. Cohen", "Richard Peng" ],
      "venue" : "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Cohen and Peng.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cohen and Peng.",
      "year" : 2015
    }, {
      "title" : "Fastmotif: spectral sequence motif discovery",
      "author" : [ "Nicoló Colombo", "Nikos Vlassis" ],
      "venue" : "Bioinformatics, pages 2623–2631,",
      "citeRegEx" : "Colombo and Vlassis.,? \\Q2015\\E",
      "shortCiteRegEx" : "Colombo and Vlassis.",
      "year" : 2015
    }, {
      "title" : "Matrix multiplication via arithmetic progressions",
      "author" : [ "Don Coppersmith", "Shmuel Winograd" ],
      "venue" : "In Proceedings of the nineteenth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Coppersmith and Winograd.,? \\Q1987\\E",
      "shortCiteRegEx" : "Coppersmith and Winograd.",
      "year" : 1987
    }, {
      "title" : "Numerical linear algebra in the streaming model",
      "author" : [ "Kenneth L. Clarkson", "David P. Woodruff" ],
      "venue" : "In Proceedings of the 41st Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Clarkson and Woodruff.,? \\Q2009\\E",
      "shortCiteRegEx" : "Clarkson and Woodruff.",
      "year" : 2009
    }, {
      "title" : "Low rank approximation and regression in input sparsity time",
      "author" : [ "Kenneth L. Clarkson", "David P. Woodruff" ],
      "venue" : "In Symposium on Theory of Computing Conference,",
      "citeRegEx" : "Clarkson and Woodruff.,? \\Q2013\\E",
      "shortCiteRegEx" : "Clarkson and Woodruff.",
      "year" : 2013
    }, {
      "title" : "Input sparsity and hardness for robust subspace approximation",
      "author" : [ "Kenneth L Clarkson", "David P Woodruff" ],
      "venue" : "In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Clarkson and Woodruff.,? \\Q2015\\E",
      "shortCiteRegEx" : "Clarkson and Woodruff.",
      "year" : 2015
    }, {
      "title" : "Sketching for m-estimators: A unified approach to robust regression",
      "author" : [ "Kenneth L Clarkson", "David P Woodruff" ],
      "venue" : "In Proceedings of the Twenty-Sixth Annual ACMSIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "Clarkson and Woodruff.,? \\Q2015\\E",
      "shortCiteRegEx" : "Clarkson and Woodruff.",
      "year" : 2015
    }, {
      "title" : "Typed tensor decomposition of knowledge bases for relation extraction",
      "author" : [ "Kai-Wei Chang", "Scott Wen-tau Yih", "Bishan Yang", "Chris Meek" ],
      "venue" : "In Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Chang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2014
    }, {
      "title" : "Sampling algorithms and coresets for `p regression",
      "author" : [ "Anirban Dasgupta", "Petros Drineas", "Boulos Harb", "Ravi Kumar", "Michael W Mahoney" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Dasgupta et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2009
    }, {
      "title" : "Algorithmic lower bounds: Fun with hardness proofs, lecture 13",
      "author" : [ "Erik Demaine" ],
      "venue" : "In MIT Course",
      "citeRegEx" : "Demaine.,? \\Q2014\\E",
      "shortCiteRegEx" : "Demaine.",
      "year" : 2014
    }, {
      "title" : "From matrix to tensor: Multilinear algebra and signal processing",
      "author" : [ "Lieven De Lathauwer", "Bart De Moor" ],
      "venue" : "In Institute of Mathematics and Its Applications Conference Series,",
      "citeRegEx" : "Lathauwer and Moor.,? \\Q1998\\E",
      "shortCiteRegEx" : "Lathauwer and Moor.",
      "year" : 1998
    }, {
      "title" : "Fast approximation of matrix coherence and statistical leverage",
      "author" : [ "Petros Drineas", "Malik Magdon-Ismail", "Michael W Mahoney", "David P Woodruff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2012
    }, {
      "title" : "Subspace sampling and relative-error matrix approximation: Column-based methods. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 9th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems",
      "author" : [ "Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan" ],
      "venue" : "APPROX 2006 and 10th International Workshop on Random-",
      "citeRegEx" : "Drineas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2006
    }, {
      "title" : "Subspace sampling and relative-error matrix approximation: Column-row-based methods",
      "author" : [ "Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan" ],
      "venue" : "In Algorithms - ESA",
      "citeRegEx" : "Drineas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2006
    }, {
      "title" : "Relative-error CUR matrix decompositions",
      "author" : [ "Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan" ],
      "venue" : "SIAM J. Matrix Analysis Applications,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient volume sampling for row/column subset selection",
      "author" : [ "Amit Deshpande", "Luis Rademacher" ],
      "venue" : "In 2010 51st Annual IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Deshpande and Rademacher.,? \\Q2010\\E",
      "shortCiteRegEx" : "Deshpande and Rademacher.",
      "year" : 2010
    }, {
      "title" : "Tensor rank and the ill-posedness of the best lowrank approximation problem",
      "author" : [ "Vin De Silva", "Lek-Heng Lim" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Silva and Lim.,? \\Q2008\\E",
      "shortCiteRegEx" : "Silva and Lim.",
      "year" : 2008
    }, {
      "title" : "Adaptive sampling and fast low-rank matrix approximation. In Approximation, Randomization, and Combinatorial Optimization",
      "author" : [ "Amit Deshpande", "Santosh Vempala" ],
      "venue" : "Algorithms and Techniques,",
      "citeRegEx" : "Deshpande and Vempala.,? \\Q2006\\E",
      "shortCiteRegEx" : "Deshpande and Vempala.",
      "year" : 2006
    }, {
      "title" : "Kronecker product and spline regression",
      "author" : [ "Huaian Diao", "David P. Woodruff" ],
      "venue" : null,
      "citeRegEx" : "Diao and Woodruff.,? \\Q2017\\E",
      "shortCiteRegEx" : "Diao and Woodruff.",
      "year" : 2017
    }, {
      "title" : "A newton-grassmann method for computing the best multilinear rank-(r1,r2,r3) approximation of a tensor",
      "author" : [ "Lars Eldén", "Berkant Savas" ],
      "venue" : "SIAM J. Matrix Analysis Applications,",
      "citeRegEx" : "Eldén and Savas.,? \\Q2009\\E",
      "shortCiteRegEx" : "Eldén and Savas.",
      "year" : 2009
    }, {
      "title" : "Distributed column subset selection on mapreduce",
      "author" : [ "Ahmed K Farahat", "Ahmed Elgohary", "Ali Ghodsi", "Mohamed S Kamel" ],
      "venue" : "IEEE 13th International Conference on Data Mining (ICDM),",
      "citeRegEx" : "Farahat et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Farahat et al\\.",
      "year" : 2013
    }, {
      "title" : "Relations between average case complexity and approximation complexity",
      "author" : [ "Uriel Feige" ],
      "venue" : "In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing(STOC),",
      "citeRegEx" : "Feige.,? \\Q2002\\E",
      "shortCiteRegEx" : "Feige.",
      "year" : 2002
    }, {
      "title" : "Fast monte-carlo algorithms for finding low-rank approximations",
      "author" : [ "Alan M. Frieze", "Ravi Kannan", "Santosh Vempala" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Frieze et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Frieze et al\\.",
      "year" : 2004
    }, {
      "title" : "Nkengla. Fast low rank approximations of matrices and tensors",
      "author" : [ "Shmuel Friedland", "V Mehrmann", "A Miedlar" ],
      "venue" : "Electron. J. Linear Algebra,",
      "citeRegEx" : "Friedland et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Friedland et al\\.",
      "year" : 2011
    }, {
      "title" : "On best rank one approximation of tensors",
      "author" : [ "Shmuel Friedland", "Volker Mehrmann", "Renato Pajarola", "Susanne K. Suter" ],
      "venue" : "Numerical Lin. Alg. with Applic.,",
      "citeRegEx" : "Friedland et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Friedland et al\\.",
      "year" : 2013
    }, {
      "title" : "Tensor-based trapdoors for cvp and their application to public key cryptography",
      "author" : [ "Roger Fischlin", "Jean-Pierre Seifert" ],
      "venue" : "Cryptography and Coding,",
      "citeRegEx" : "Fischlin and Seifert.,? \\Q1999\\E",
      "shortCiteRegEx" : "Fischlin and Seifert.",
      "year" : 1999
    }, {
      "title" : "Generalized rank-constrained matrix approximations",
      "author" : [ "Shmuel Friedland", "Anatoli Torokhti" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Friedland and Torokhti.,? \\Q2007\\E",
      "shortCiteRegEx" : "Friedland and Torokhti.",
      "year" : 2007
    }, {
      "title" : "Low-rank approximation of tensors",
      "author" : [ "Shmuel Friedland", "Venu Tammali" ],
      "venue" : null,
      "citeRegEx" : "Friedland and Tammali.,? \\Q2015\\E",
      "shortCiteRegEx" : "Friedland and Tammali.",
      "year" : 2015
    }, {
      "title" : "Robust tensor decomposition with gross corruption",
      "author" : [ "Quanquan Gu", "Huan Gui", "Jiawei Han" ],
      "venue" : "In Advances in Neural Information Processing Systems(NIPS),",
      "citeRegEx" : "Gu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning mixtures of gaussians in high dimensions",
      "author" : [ "Rong Ge", "Qingqing Huang", "Sham M Kakade" ],
      "venue" : "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Ge et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2015
    }, {
      "title" : "Some simplified npcomplete graph problems",
      "author" : [ "Michael R Garey", "David S. Johnson", "Larry Stockmeyer" ],
      "venue" : "Theoretical computer science,",
      "citeRegEx" : "Garey et al\\.,? \\Q1976\\E",
      "shortCiteRegEx" : "Garey et al\\.",
      "year" : 1976
    }, {
      "title" : "An approximation hardness result for bipartite clique",
      "author" : [ "Andreas Goerdt", "André Lanka" ],
      "venue" : "In Electronic Colloquium on Computational Complexity, Report,",
      "citeRegEx" : "Goerdt and Lanka.,? \\Q2004\\E",
      "shortCiteRegEx" : "Goerdt and Lanka.",
      "year" : 2004
    }, {
      "title" : "Decomposing overcomplete 3rd order tensors using sumof-squares algorithms. In The 18th. International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX’2015), and the 19th",
      "author" : [ "Rong Ge", "Tengyu Ma" ],
      "venue" : "International Workshop on Randomization and Computation (RANDOM’2015). https://arxiv.org/pdf/1504.05287,",
      "citeRegEx" : "Ge and Ma.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ge and Ma.",
      "year" : 2015
    }, {
      "title" : "Relative errors for deterministic low-rank matrix approximations",
      "author" : [ "Mina Ghashami", "Jeff M Phillips" ],
      "venue" : "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "Ghashami and Phillips.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ghashami and Phillips.",
      "year" : 2014
    }, {
      "title" : "Robust low-rank tensor recovery: Models and algorithms",
      "author" : [ "Donald Goldfarb", "Zhiwei Qin" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Goldfarb and Qin.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goldfarb and Qin.",
      "year" : 2014
    }, {
      "title" : "Foundations of the parafac procedure: Models and conditions for an “explanatory” multi-modal factor analysis",
      "author" : [ "Richard A Harshman" ],
      "venue" : null,
      "citeRegEx" : "Harshman.,? \\Q1970\\E",
      "shortCiteRegEx" : "Harshman.",
      "year" : 1970
    }, {
      "title" : "Tensor rank is np-complete",
      "author" : [ "Johan Håstad" ],
      "venue" : "Journal of Algorithms,",
      "citeRegEx" : "Håstad.,? \\Q1990\\E",
      "shortCiteRegEx" : "Håstad.",
      "year" : 1990
    }, {
      "title" : "On bounded occurrence constraint satisfaction",
      "author" : [ "Johan Håstad" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "Håstad.,? \\Q2000\\E",
      "shortCiteRegEx" : "Håstad.",
      "year" : 2000
    }, {
      "title" : "Some optimal inapproximability results",
      "author" : [ "Johan Håstad" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Håstad.,? \\Q2001\\E",
      "shortCiteRegEx" : "Håstad.",
      "year" : 2001
    }, {
      "title" : "Robust tensor factorization using r 1 norm",
      "author" : [ "Heng Huang", "Chris Ding" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Huang and Ding.,? \\Q2008\\E",
      "shortCiteRegEx" : "Huang and Ding.",
      "year" : 2008
    }, {
      "title" : "Learning mixtures of spherical gaussians: moment methods and spectral decompositions",
      "author" : [ "Daniel Hsu", "Sham M Kakade" ],
      "venue" : "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science(ITCS),",
      "citeRegEx" : "Hsu and Kakade.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hsu and Kakade.",
      "year" : 2013
    }, {
      "title" : "Most tensor problems are np-hard",
      "author" : [ "Christopher J Hillar", "Lek-Heng Lim" ],
      "venue" : "In Journal of the ACM (JACM),",
      "citeRegEx" : "Hillar and Lim.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hillar and Lim.",
      "year" : 2013
    }, {
      "title" : "Sparse image coding using a 3d non-negative tensor factorization",
      "author" : [ "Tamir Hazan", "Simon Polak", "Amnon Shashua" ],
      "venue" : "In Tenth IEEE International Conference on Computer Vision(ICCV),",
      "citeRegEx" : "Hazan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2005
    }, {
      "title" : "Tensor principal component analysis via sum-of-square proofs",
      "author" : [ "Samuel B Hopkins", "Jonathan Shi", "David Steurer" ],
      "venue" : "In 28th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Hopkins et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hopkins et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors",
      "author" : [ "Samuel B Hopkins", "Tselil Schramm", "Jonathan Shi", "David Steurer" ],
      "venue" : "In Proceedings of the 48th Annual Symposium on the Theory of Computing. ACM,",
      "citeRegEx" : "Hopkins et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hopkins et al\\.",
      "year" : 2016
    }, {
      "title" : "Which problems have strongly exponential complexity",
      "author" : [ "Russell Impagliazzo", "Ramamohan Paturi", "Francis Zane" ],
      "venue" : "In Proceedings. 39th Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Impagliazzo et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Impagliazzo et al\\.",
      "year" : 1998
    }, {
      "title" : "BPP if E requires exponential circuits: Derandomizing the XOR lemma",
      "author" : [ "Russell Impagliazzo", "Avi Wigderson. P" ],
      "venue" : "In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing (STOC),",
      "citeRegEx" : "Impagliazzo and P,? \\Q1997\\E",
      "shortCiteRegEx" : "Impagliazzo and P",
      "year" : 1997
    }, {
      "title" : "Tensor principal component analysis via convex optimization",
      "author" : [ "Bo Jiang", "Shiqian Ma", "Shuzhong Zhang" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Jiang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning mixtures of discrete product distributions using spectral decompositions",
      "author" : [ "Prateek Jain", "Sewoong Oh" ],
      "venue" : "In 27th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Jain and Oh.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jain and Oh.",
      "year" : 2014
    }, {
      "title" : "Provable tensor factorization with missing data",
      "author" : [ "Prateek Jain", "Sewoong Oh" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Jain and Oh.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jain and Oh.",
      "year" : 2014
    }, {
      "title" : "On the minimum of a polynomial function on a basic closed semialgebraic set and applications",
      "author" : [ "Gabriela Jeronimo", "Daniel Perrucci", "Elias Tsigaridas" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Jeronimo et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jeronimo et al\\.",
      "year" : 2013
    }, {
      "title" : "Beating the perils of nonconvexity: Guaranteed training of neural networks using tensor methods",
      "author" : [ "Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Janzamin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Janzamin et al\\.",
      "year" : 2015
    }, {
      "title" : "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering",
      "author" : [ "Alexandros Karatzoglou", "Xavier Amatriain", "Linas Baltrunas", "Nuria Oliver" ],
      "venue" : "In Proceedings of the fourth ACM conference on Recommender systems,",
      "citeRegEx" : "Karatzoglou et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Karatzoglou et al\\.",
      "year" : 2010
    }, {
      "title" : "The tophits model for higher-order web link analysis",
      "author" : [ "Tamara Kolda", "Brett Bader" ],
      "venue" : "In Workshop on link analysis, counterterrorism and security,",
      "citeRegEx" : "Kolda and Bader.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kolda and Bader.",
      "year" : 2006
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "Tamara G. Kolda", "Brett W. Bader" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Kolda and Bader.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kolda and Bader.",
      "year" : 2009
    }, {
      "title" : "Nonnegative tucker decomposition",
      "author" : [ "Yong-Deok Kim", "Seungjin Choi" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).,",
      "citeRegEx" : "Kim and Choi.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kim and Choi.",
      "year" : 2007
    }, {
      "title" : "On the non-existence of optimal solutions and the occurrence of “degeneracy",
      "author" : [ "Wim P Krijnen", "Theo K Dijkstra", "Alwin Stegeman" ],
      "venue" : "in the candecomp/parafac model. Psychometrika,",
      "citeRegEx" : "Krijnen et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Krijnen et al\\.",
      "year" : 2008
    }, {
      "title" : "How 3-mfa data can cause degenerate parafac solutions, among other relationships",
      "author" : [ "JB Kruskal", "RA Harshman", "ME Lundy" ],
      "venue" : "Multiway data analysis,",
      "citeRegEx" : "Kruskal et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Kruskal et al\\.",
      "year" : 1989
    }, {
      "title" : "Spectral sparsification in the semi-streaming setting",
      "author" : [ "J. Kelner", "A. Levin" ],
      "venue" : "In Symposium on Theoretical Aspects of Computer Science (STACS),",
      "citeRegEx" : "Kelner and Levin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kelner and Levin.",
      "year" : 2011
    }, {
      "title" : "Single pass spectral sparsification in dynamic streams",
      "author" : [ "Michael Kapralov", "Yin Tat Lee", "Cameron Musco", "Christopher Musco", "Aaron Sidford" ],
      "venue" : "In 2014 IEEE 55th Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Kapralov et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kapralov et al\\.",
      "year" : 2014
    }, {
      "title" : "Shifted power method for computing tensor eigenpairs",
      "author" : [ "Tamara G Kolda", "Jackson R Mayo" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Kolda and Mayo.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kolda and Mayo.",
      "year" : 2011
    }, {
      "title" : "Sparser johnson-lindenstrauss transforms",
      "author" : [ "Daniel M Kane", "Jelani Nelson" ],
      "venue" : "In Journal of the ACM (JACM),",
      "citeRegEx" : "Kane and Nelson.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kane and Nelson.",
      "year" : 2014
    }, {
      "title" : "Three-mode principal component analysis: Theory and applications, volume 2",
      "author" : [ "Pieter M Kroonenberg" ],
      "venue" : "DSWO press,",
      "citeRegEx" : "Kroonenberg.,? \\Q1983\\E",
      "shortCiteRegEx" : "Kroonenberg.",
      "year" : 1983
    }, {
      "title" : "Scalable tensor decompositions for multi-aspect data mining",
      "author" : [ "Tamara G Kolda", "Jimeng Sun" ],
      "venue" : "In Eighth IEEE International Conference on Data Mining (ICDM),",
      "citeRegEx" : "Kolda and Sun.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kolda and Sun.",
      "year" : 2008
    }, {
      "title" : "Principal component analysis and higher correlations for distributed data",
      "author" : [ "Ravindran Kannan", "Santosh S Vempala", "David P Woodruff" ],
      "venue" : "In Proceedings of The 27th Conference on Learning Theory (COLT),",
      "citeRegEx" : "Kannan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kannan et al\\.",
      "year" : 2014
    }, {
      "title" : "Secure tensor decomposition using fully homomorphic encryption scheme",
      "author" : [ "Liwei Kuang", "Laurence Yang", "Jun Feng", "Mianxiong Dong" ],
      "venue" : "IEEE Transactions on Cloud Computing,",
      "citeRegEx" : "Kuang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kuang et al\\.",
      "year" : 2015
    }, {
      "title" : "The border rank of the multiplication of 2× 2 matrices is seven",
      "author" : [ "J Landsberg" ],
      "venue" : "In Journal of the American Mathematical Society,",
      "citeRegEx" : "Landsberg.,? \\Q2006\\E",
      "shortCiteRegEx" : "Landsberg.",
      "year" : 2006
    }, {
      "title" : "Tensors: geometry and applications, volume 128",
      "author" : [ "Joseph M Landsberg" ],
      "venue" : "American Mathematical Society Providence, RI, USA., http://www.math.tamu.edu/ ~joseph.landsberg/Tbookintro.pdf,",
      "citeRegEx" : "Landsberg.,? \\Q2012\\E",
      "shortCiteRegEx" : "Landsberg.",
      "year" : 2012
    }, {
      "title" : "Tensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimization",
      "author" : [ "Canyi Lu", "Jiashi Feng", "Yudong Chen", "Wei Liu", "Zhouchen Lin", "Shuicheng Yan" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Lu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Simple and deterministic matrix sketching",
      "author" : [ "Edo Liberty" ],
      "venue" : "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD),",
      "citeRegEx" : "Liberty.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liberty.",
      "year" : 2013
    }, {
      "title" : "Lower bounds based on the exponential time hypothesis",
      "author" : [ "Daniel Lokshtanov", "Dániel Marx", "Saket Saurabh" ],
      "venue" : "In Bull. EATCS",
      "citeRegEx" : "Lokshtanov et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lokshtanov et al\\.",
      "year" : 2011
    }, {
      "title" : "A multilinear singular value decomposition",
      "author" : [ "Lieven De Lathauwer", "Bart De Moor", "Joos Vandewalle" ],
      "venue" : "SIAM J. Matrix Analysis Applications,",
      "citeRegEx" : "Lathauwer et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Lathauwer et al\\.",
      "year" : 2000
    }, {
      "title" : "Tensor completion for estimating missing values in visual data",
      "author" : [ "Ji Liu", "Przemyslaw Musialski", "Peter Wonka", "Jieping Ye" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "Liu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2013
    }, {
      "title" : "Heavy hitters via cluster-preserving clustering",
      "author" : [ "Kasper Green Larsen", "Jelani Nelson", "Huy L Nguyen", "Mikkel Thorup" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Larsen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Larsen et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-relational learning using weighted tensor decomposition with modular loss",
      "author" : [ "Ben London", "Theodoros Rekatsinas", "Bert Huang", "Lise Getoor" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "London et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "London et al\\.",
      "year" : 2013
    }, {
      "title" : "Low-rank tensors for scoring dependency structures. In Association for Computational Linguistics(ACL)",
      "author" : [ "Tao Lei", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola" ],
      "venue" : "Best student paper award,",
      "citeRegEx" : "Lei et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2014
    }, {
      "title" : "High-order lowrank tensors for semantic role labeling",
      "author" : [ "Tao Lei", "Yuan Zhang", "Alessandro Moschitti", "Regina Barzilay" ],
      "venue" : "Proceedings of the 2015 Conference of the North America Chapter of the Association For Computational Linguistics– Human Language Technologies (NAACLHLT",
      "citeRegEx" : "Lei et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2015
    }, {
      "title" : "Consensus-based distributed principal component analysis in wireless sensor networks",
      "author" : [ "Sergio V Macua", "Pavle Belanovic", "Santiago Zazo" ],
      "venue" : "In Signal Processing Advances in Wireless Communications (SPAWC),",
      "citeRegEx" : "Macua et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Macua et al\\.",
      "year" : 2010
    }, {
      "title" : "Sparse coding and automatic relevance determination for multi-way models",
      "author" : [ "Morten Mørup", "Lars Kai Hansen" ],
      "venue" : "In SPARS’09-Signal Processing with Adaptive Sparse Structured Representations,",
      "citeRegEx" : "Mørup and Hansen.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mørup and Hansen.",
      "year" : 2009
    }, {
      "title" : "Successive rank-one approximations for nearly orthogonally decomposable symmetric tensors",
      "author" : [ "Cun Mu", "Daniel Hsu", "Donald Goldfarb" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Mu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mu et al\\.",
      "year" : 2015
    }, {
      "title" : "Square deal: Lower bounds and improved relaxations for tensor recovery",
      "author" : [ "Cun Mu", "Bo Huang", "John Wright", "Donald Goldfarb" ],
      "venue" : "In The Thirty-first International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Mu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mu et al\\.",
      "year" : 2014
    }, {
      "title" : "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression",
      "author" : [ "Xiangrui Meng", "Michael W Mahoney" ],
      "venue" : "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Meng and Mahoney.,? \\Q2013\\E",
      "shortCiteRegEx" : "Meng and Mahoney.",
      "year" : 2013
    }, {
      "title" : "Tensor-cur decompositions for tensor-based data",
      "author" : [ "Michael W Mahoney", "Mauro Maggioni", "Petros Drineas" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Mahoney et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mahoney et al\\.",
      "year" : 2008
    }, {
      "title" : "An almost optimal algorithm for computing nonnegative rank",
      "author" : [ "Ankur Moitra" ],
      "venue" : "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "Moitra.,? \\Q2013\\E",
      "shortCiteRegEx" : "Moitra.",
      "year" : 2013
    }, {
      "title" : "Applications of tensor (multiway array) factorizations and decompositions in data mining",
      "author" : [ "Morten Mørup" ],
      "venue" : "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,",
      "citeRegEx" : "Mørup.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mørup.",
      "year" : 2011
    }, {
      "title" : "Learning nonsingular phylogenies and hidden markov models",
      "author" : [ "Elchanan Mossel", "Sébastien Roch" ],
      "venue" : "In Proceedings of the thirty-seventh annual ACM symposium on Theory of computing (STOC),",
      "citeRegEx" : "Mossel and Roch.,? \\Q2005\\E",
      "shortCiteRegEx" : "Mossel and Roch.",
      "year" : 2005
    }, {
      "title" : "Two-query pcp with subconstant error",
      "author" : [ "Dana Moshkovitz", "Ran Raz" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Moshkovitz and Raz.,? \\Q2010\\E",
      "shortCiteRegEx" : "Moshkovitz and Raz.",
      "year" : 2010
    }, {
      "title" : "Polynomial-time tensor decompositions with sum-of-squares",
      "author" : [ "Tengyu Ma", "Jonathan Shi", "David Steurer" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Ma et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2016
    }, {
      "title" : "1-pass relative-error lp-sampling with applications",
      "author" : [ "Morteza Monemizadeh", "David P Woodruff" ],
      "venue" : "In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms,",
      "citeRegEx" : "Monemizadeh and Woodruff.,? \\Q2010\\E",
      "shortCiteRegEx" : "Monemizadeh and Woodruff.",
      "year" : 2010
    }, {
      "title" : "Random walk in a simplex and quadratic optimization over convex polytopes",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "CORE,",
      "citeRegEx" : "Nesterov,? \\Q2003\\E",
      "shortCiteRegEx" : "Nesterov",
      "year" : 2003
    }, {
      "title" : "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings",
      "author" : [ "Jelani Nelson", "Huy L Nguyên" ],
      "venue" : "IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Nelson and Nguyên.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nelson and Nguyên.",
      "year" : 2013
    }, {
      "title" : "Learning mixed multinomial logit model from ordinal data",
      "author" : [ "Sewoong Oh", "Devavrat Shah" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Oh and Shah.,? \\Q2014\\E",
      "shortCiteRegEx" : "Oh and Shah.",
      "year" : 2014
    }, {
      "title" : "Tensor-train decomposition",
      "author" : [ "Ivan V. Oseledets" ],
      "venue" : "SIAM J. Scientific Computing,",
      "citeRegEx" : "Oseledets.,? \\Q2011\\E",
      "shortCiteRegEx" : "Oseledets.",
      "year" : 2011
    }, {
      "title" : "Tucker dimensionality reduction of three-dimensional arrays in linear time",
      "author" : [ "Ivan V Oseledets", "DV Savostianov", "Eugene E Tyrtyshnikov" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Oseledets et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Oseledets et al\\.",
      "year" : 2008
    }, {
      "title" : "Breaking the curse of dimensionality, or how to use svd in many dimensions",
      "author" : [ "Ivan V Oseledets", "Eugene E Tyrtyshnikov" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Oseledets and Tyrtyshnikov.,? \\Q2009\\E",
      "shortCiteRegEx" : "Oseledets and Tyrtyshnikov.",
      "year" : 2009
    }, {
      "title" : "Tensor-train ranks for matrices and their inverses",
      "author" : [ "Ivan Oseledets", "Eugene Tyrtyshnikov", "Nickolai Zamarashkin" ],
      "venue" : "Computational Methods in Applied Mathematics Comput. Methods Appl. Math.,",
      "citeRegEx" : "Oseledets et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Oseledets et al\\.",
      "year" : 2011
    }, {
      "title" : "A weighted non-negative least squares algorithm for threeway “parafac” factor analysis",
      "author" : [ "Pentti Paatero" ],
      "venue" : "Chemometrics and Intelligent Laboratory Systems,",
      "citeRegEx" : "Paatero.,? \\Q1997\\E",
      "shortCiteRegEx" : "Paatero.",
      "year" : 1997
    }, {
      "title" : "Construction and analysis of degenerate parafac models",
      "author" : [ "Pentti Paatero" ],
      "venue" : "Journal of chemometrics,",
      "citeRegEx" : "Paatero.,? \\Q2000\\E",
      "shortCiteRegEx" : "Paatero.",
      "year" : 2000
    }, {
      "title" : "Compressed matrix multiplication",
      "author" : [ "Rasmus Pagh" ],
      "venue" : "ACM Transactions on Computation Theory (TOCT),",
      "citeRegEx" : "Pagh.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pagh.",
      "year" : 2013
    }, {
      "title" : "Rethinking lda: moment matching for discrete ica",
      "author" : [ "Anastasia Podosinnikova", "Francis Bach", "Simon Lacoste-Julien" ],
      "venue" : "In Advances in Neural Information Processing Systems(NIPS),",
      "citeRegEx" : "Podosinnikova et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Podosinnikova et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast and efficient algorithms for nonnegative tucker decomposition",
      "author" : [ "Anh Phan", "Andrzej Cichocki" ],
      "venue" : "Advances in Neural Networks-ISNN",
      "citeRegEx" : "Phan and Cichocki.,? \\Q2008\\E",
      "shortCiteRegEx" : "Phan and Cichocki.",
      "year" : 2008
    }, {
      "title" : "Robust tensor analysis with l1-norm",
      "author" : [ "Yanwei Pang", "Xuelong Li", "Yuan Yuan" ],
      "venue" : "IEEE Transactions on Circuits and Systems for Video Technology,",
      "citeRegEx" : "Pang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2010
    }, {
      "title" : "Elemental: A new framework for distributed memory dense matrix computations",
      "author" : [ "Jack Poulson", "Bryan Marker", "Robert A van de Geijn", "Jeff R Hammond", "Nichols A Romero" ],
      "venue" : "ACM Transactions on Mathematical Software (TOMS),",
      "citeRegEx" : "Poulson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Poulson et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast and scalable polynomial kernels via explicit feature maps",
      "author" : [ "Ninh Pham", "Rasmus Pagh" ],
      "venue" : "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining(KDD),",
      "citeRegEx" : "Pham and Pagh.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pham and Pagh.",
      "year" : 2013
    }, {
      "title" : "Exact tensor completion with sum-of-squares",
      "author" : [ "Aaron Potechin", "David Steurer" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Potechin and Steurer.,? \\Q2017\\E",
      "shortCiteRegEx" : "Potechin and Steurer.",
      "year" : 2017
    }, {
      "title" : "Efficient tensor completion: Low-rank tensor train",
      "author" : [ "Ho N Phien", "Hoang D Tuan", "Johann A Bengua", "Minh N Do" ],
      "venue" : "In arXiv preprint. https://arxiv.org/pdf/",
      "citeRegEx" : "Phien et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Phien et al\\.",
      "year" : 2016
    }, {
      "title" : "Principal component analysis for dimension reduction in massive distributed data sets",
      "author" : [ "Yongming Qu", "George Ostrouchov", "Nagiza Samatova", "Al Geist" ],
      "venue" : "In Proceedings of IEEE International Conference on Data Mining (ICDM),",
      "citeRegEx" : "Qu et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2002
    }, {
      "title" : "On the computational complexity and geometry of the first-order theory of the reals, part I: introduction. preliminaries. the geometry of semi-algebraic sets. the decision problem for the existential theory of the reals",
      "author" : [ "James Renegar" ],
      "venue" : "J. Symb. Comput.,",
      "citeRegEx" : "Renegar.,? \\Q1992\\E",
      "shortCiteRegEx" : "Renegar.",
      "year" : 1992
    }, {
      "title" : "On the computational complexity and geometry of the first-order theory of the reals, part II: the general decision problem. preliminaries for quantifier elimination",
      "author" : [ "James Renegar" ],
      "venue" : "J. Symb. Comput.,",
      "citeRegEx" : "Renegar.,? \\Q1992\\E",
      "shortCiteRegEx" : "Renegar.",
      "year" : 1992
    }, {
      "title" : "A statistical model for tensor pca",
      "author" : [ "Emile Richard", "Andrea Montanari" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Richard and Montanari.,? \\Q2014\\E",
      "shortCiteRegEx" : "Richard and Montanari.",
      "year" : 2014
    }, {
      "title" : "The search problem in mixture models",
      "author" : [ "Avik Ray", "Joe Neeman", "Sujay Sanghavi", "Sanjay Shakkottai" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Ray et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ray et al\\.",
      "year" : 2016
    }, {
      "title" : "Pairwise interaction tensor factorization for personalized tag recommendation",
      "author" : [ "Steffen Rendle", "Lars Schmidt-Thieme" ],
      "venue" : "In Proceedings of the third ACM international conference on Web search and data mining(WSDM),",
      "citeRegEx" : "Rendle and Schmidt.Thieme.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rendle and Schmidt.Thieme.",
      "year" : 2010
    }, {
      "title" : "Weighted low rank approximations with provable guarantees",
      "author" : [ "Ilya Razenshteyn", "Zhao Song", "David P Woodruff" ],
      "venue" : "In Proceedings of the 48th Annual Symposium on the Theory of Computing (STOC),",
      "citeRegEx" : "Razenshteyn et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Razenshteyn et al\\.",
      "year" : 2016
    }, {
      "title" : "Newtonian program analysis via tensor product",
      "author" : [ "Thomas Reps", "Emma Turetsky", "Prathmesh Prabhu" ],
      "venue" : "In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages(POPL),",
      "citeRegEx" : "Reps et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Reps et al\\.",
      "year" : 2016
    }, {
      "title" : "Smallest singular value of a random rectangular matrix",
      "author" : [ "Mark Rudelson", "Roman Vershynin" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "Rudelson and Vershynin.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rudelson and Vershynin.",
      "year" : 2009
    }, {
      "title" : "Improved approximation algorithms for large matrices via random projections",
      "author" : [ "Tamás Sarlós" ],
      "venue" : "Annual IEEE Symposium on Foundations of Computer Science",
      "citeRegEx" : "Sarlós.,? \\Q2006\\E",
      "shortCiteRegEx" : "Sarlós.",
      "year" : 2006
    }, {
      "title" : "Multi-way Analysis with Applications in the Chemical Sciences",
      "author" : [ "K. Smilde", "Rasmus Bro", "Paul Geladi" ],
      "venue" : null,
      "citeRegEx" : "Smilde et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Smilde et al\\.",
      "year" : 2004
    }, {
      "title" : "Spectacle: fast chromatin state annotation using spectral learning",
      "author" : [ "Jimin Song", "Kevin C Chen" ],
      "venue" : "Genome biology,",
      "citeRegEx" : "Song and Chen.,? \\Q2015\\E",
      "shortCiteRegEx" : "Song and Chen.",
      "year" : 2015
    }, {
      "title" : "Cryptography from tensor problems",
      "author" : [ "Leonard J Schulman" ],
      "venue" : "In IACR Cryptology ePrint Archive,",
      "citeRegEx" : "Schulman.,? \\Q2012\\E",
      "shortCiteRegEx" : "Schulman.",
      "year" : 2012
    }, {
      "title" : "Non-negative tensor factorization with applications to statistics and computer vision",
      "author" : [ "Amnon Shashua", "Tamir Hazan" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning(ICML),",
      "citeRegEx" : "Shashua and Hazan.,? \\Q2005\\E",
      "shortCiteRegEx" : "Shashua and Hazan.",
      "year" : 2005
    }, {
      "title" : "Key exchange protocol based on tensor decomposition problem",
      "author" : [ "Mao Shaowu", "Zhang Huanguo", "Wu Wanqing", "Zhang Pei", "Song Jun", "Liu Jinhui" ],
      "venue" : "China Communications,",
      "citeRegEx" : "Shaowu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shaowu et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast and robust tensor decomposition with applications to dictionary learning",
      "author" : [ "Tselil Schramm", "David Steurer" ],
      "venue" : null,
      "citeRegEx" : "Schramm and Steurer.,? \\Q2017\\E",
      "shortCiteRegEx" : "Schramm and Steurer.",
      "year" : 2017
    }, {
      "title" : "Degeneracy in candecomp/parafac explained for p × p × 2 arrays of rank",
      "author" : [ "Alwin Stegeman" ],
      "venue" : "p+1 or higher. Psychometrika,",
      "citeRegEx" : "Stegeman.,? \\Q2006\\E",
      "shortCiteRegEx" : "Stegeman.",
      "year" : 2006
    }, {
      "title" : "Low-rank approximation of generic p × q × 2 arrays and diverging components in the candecomp/parafac model",
      "author" : [ "Alwin Stegeman" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Stegeman.,? \\Q2008\\E",
      "shortCiteRegEx" : "Stegeman.",
      "year" : 2008
    }, {
      "title" : "Learning with tensors: a framework based on convex optimization and spectral regularization",
      "author" : [ "Marco Signoretto", "Dinh Quoc Tran", "Lieven De Lathauwer", "Johan A.K. Suykens" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Signoretto et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Signoretto et al\\.",
      "year" : 2014
    }, {
      "title" : "Gaussian elimination is not optimal",
      "author" : [ "Volker Strassen" ],
      "venue" : "Numerische Mathematik,",
      "citeRegEx" : "Strassen.,? \\Q1969\\E",
      "shortCiteRegEx" : "Strassen.",
      "year" : 1969
    }, {
      "title" : "Sublinear time orthogonal tensor decomposition",
      "author" : [ "Zhao Song", "David P. Woodruff", "Huan Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Song et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2016
    }, {
      "title" : "Low rank approximation with entrywise `1-norm error",
      "author" : [ "Zhao Song", "David P Woodruff", "Peilin Zhong" ],
      "venue" : "In Proceedings of the 49th Annual Symposium on the Theory of Computing (STOC). ACM,",
      "citeRegEx" : "Song et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2017
    }, {
      "title" : "A parallel divide and conquer algorithm for the symmetric eigenvalue problem on distributed memory architectures",
      "author" : [ "Françoise Tisseur", "Jack Dongarra" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "Tisseur and Dongarra.,? \\Q1999\\E",
      "shortCiteRegEx" : "Tisseur and Dongarra.",
      "year" : 1999
    }, {
      "title" : "Weight adjusted tensor method for blind separation of underdetermined mixtures of nonstationary sources",
      "author" : [ "Petr Tichavsky", "Zbyněk Koldovsky" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Tichavsky and Koldovsky.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tichavsky and Koldovsky.",
      "year" : 2011
    }, {
      "title" : "Fast monte carlo algorithms for tensor operations",
      "author" : [ "Davoud Ataee Tarzanagh", "George Michailidis" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Tarzanagh and Michailidis.,? \\Q2017\\E",
      "shortCiteRegEx" : "Tarzanagh and Michailidis.",
      "year" : 2017
    }, {
      "title" : "Non-approximability results for optimization problems on bounded degree instances",
      "author" : [ "Luca Trevisan" ],
      "venue" : "In Proceedings of the thirty-third annual ACM symposium on Theory of computing (STOC),",
      "citeRegEx" : "Trevisan.,? \\Q2001\\E",
      "shortCiteRegEx" : "Trevisan.",
      "year" : 2001
    }, {
      "title" : "Statistical performance of convex tensor decomposition",
      "author" : [ "Ryota Tomioka", "Taiji Suzuki", "Kohei Hayashi", "Hisashi Kashima" ],
      "venue" : "In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems (NIPS). Proceedings of a meeting held 12-14 December",
      "citeRegEx" : "Tomioka et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tomioka et al\\.",
      "year" : 2011
    }, {
      "title" : "A multilinear (tensor) algebraic framework for computer graphics, computer vision, and machine learning",
      "author" : [ "M Alex O Vasilescu" ],
      "venue" : "PhD thesis, Citeseer,",
      "citeRegEx" : "Vasilescu.,? \\Q2009\\E",
      "shortCiteRegEx" : "Vasilescu.",
      "year" : 2009
    }, {
      "title" : "Multilinear analysis of image ensembles: Tensorfaces",
      "author" : [ "M Alex O Vasilescu", "Demetri Terzopoulos" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Vasilescu and Terzopoulos.,? \\Q2002\\E",
      "shortCiteRegEx" : "Vasilescu and Terzopoulos.",
      "year" : 2002
    }, {
      "title" : "Tensortextures: Multilinear imagebased rendering",
      "author" : [ "M Alex O Vasilescu", "Demetri Terzopoulos" ],
      "venue" : "In ACM Transactions on Graphics (TOG),",
      "citeRegEx" : "Vasilescu and Terzopoulos.,? \\Q2004\\E",
      "shortCiteRegEx" : "Vasilescu and Terzopoulos.",
      "year" : 2004
    }, {
      "title" : "Facial expression decomposition",
      "author" : [ "Hongcheng Wang", "Narendra Ahuja" ],
      "venue" : "In Computer Vision,",
      "citeRegEx" : "Wang and Ahuja.,? \\Q2003\\E",
      "shortCiteRegEx" : "Wang and Ahuja.",
      "year" : 2003
    }, {
      "title" : "Online and differentially-private tensor decomposition",
      "author" : [ "Yining Wang", "Animashree Anandkumar" ],
      "venue" : "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Wang and Anandkumar.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang and Anandkumar.",
      "year" : 2016
    }, {
      "title" : "A tensor framework for multidimensional signal processing",
      "author" : [ "Carl-Fredrik Westin" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Westin.,? \\Q1994\\E",
      "shortCiteRegEx" : "Westin.",
      "year" : 1994
    }, {
      "title" : "Multiplying matrices faster than coppersmithwinograd",
      "author" : [ "Virginia Vassilevska Williams" ],
      "venue" : "In Proceedings of the forty-fourth annual ACM symposium on Theory of computing (STOC),",
      "citeRegEx" : "Williams.,? \\Q2012\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 2012
    }, {
      "title" : "Dealing with missing data",
      "author" : [ "B. Walczak", "DL Massart" ],
      "venue" : "Part i. Chemometrics and Intelligent Laboratory Systems,",
      "citeRegEx" : "Walczak and Massart.,? \\Q2001\\E",
      "shortCiteRegEx" : "Walczak and Massart.",
      "year" : 2001
    }, {
      "title" : "Sketching as a tool for numerical linear algebra",
      "author" : [ "David P. Woodruff" ],
      "venue" : "Foundations and Trends in Theoretical Computer Science,",
      "citeRegEx" : "Woodruff.,? \\Q2014\\E",
      "shortCiteRegEx" : "Woodruff.",
      "year" : 2014
    }, {
      "title" : "Column subset selection with missing data via active sampling",
      "author" : [ "Yining Wang", "Aarti Singh" ],
      "venue" : "In The 18th International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Wang and Singh.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang and Singh.",
      "year" : 2015
    }, {
      "title" : "Fast and guaranteed tensor decomposition via sketching",
      "author" : [ "Yining Wang", "Hsiao-Yu Tung", "Alexander J Smola", "Anima Anandkumar" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Out-of-core tensor approximation of multi-dimensional matrices of visual data",
      "author" : [ "Hongcheng Wang", "Qing Wu", "Lin Shi", "Yizhou Yu", "Narendra Ahuja" ],
      "venue" : "ACM Transactions on Graphics (TOG),",
      "citeRegEx" : "Wang et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2005
    }, {
      "title" : "Distributed low rank approximation of implicit functions of a matrix",
      "author" : [ "David P Woodruff", "Peilin Zhong" ],
      "venue" : "IEEE International Conference on Data Engineering (ICDE)",
      "citeRegEx" : "Woodruff and Zhong.,? \\Q2016\\E",
      "shortCiteRegEx" : "Woodruff and Zhong.",
      "year" : 2016
    }, {
      "title" : "Multilinear tensor rank estimation via sparse tucker decomposition",
      "author" : [ "Tatsuya Yokota", "Andrzej Cichocki" ],
      "venue" : "In Soft Computing and Intelligent Systems (SCIS),",
      "citeRegEx" : "Yokota and Cichocki.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yokota and Cichocki.",
      "year" : 2014
    }, {
      "title" : "Weighted sgd for `p regression with randomized preconditioning",
      "author" : [ "Jiyan Yang", "Yin-Lam Chow", "Christopher Ré", "Michael W Mahoney" ],
      "venue" : "In Proceedings of the TwentySeventh Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "Yang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Generalised coupled tensor factorisation",
      "author" : [ "Yusuf Kenan Yilmaz", "Ali Taylan Cemgil", "Umut Simsekli" ],
      "venue" : "In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Yilmaz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Yilmaz et al\\.",
      "year" : 2011
    }, {
      "title" : "Solving a mixture of many random linear equations by tensor decomposition and alternating minimization",
      "author" : [ "Xinyang Yi", "Constantine Caramanis", "Sujay Sanghavi" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Yi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2016
    }, {
      "title" : "Robust low-rank tensor recovery with regularized redescending m-estimator",
      "author" : [ "Yuning Yang", "Yunlong Feng", "Johan AK Suykens" ],
      "venue" : "IEEE transactions on neural networks and learning systems,",
      "citeRegEx" : "Yang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Spectral methods meet em: A provably optimal algorithm for crowdsourcing",
      "author" : [ "Yuchen Zhang", "Xi Chen", "Denny Zhou", "Michael I Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Zhang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    }, {
      "title" : "Rank-one approximation to high order tensors",
      "author" : [ "Tong Zhang", "Gene H. Golub" ],
      "venue" : "SIAM J. Matrix Analysis Applications,",
      "citeRegEx" : "Zhang and Golub.,? \\Q2001\\E",
      "shortCiteRegEx" : "Zhang and Golub.",
      "year" : 2001
    }, {
      "title" : "Recovery guarantees for one-hidden-layer neural networks",
      "author" : [ "Kai Zhong", "Zhao Song", "Prateek Jain", "Peter L. Bartlett", "Inderjit S. Dhillon" ],
      "venue" : null,
      "citeRegEx" : "Zhong et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2017
    }, {
      "title" : "Tensor dictionary learning with sparse tucker decomposition",
      "author" : [ "Syed Zubair", "Wenwu Wang" ],
      "venue" : "In Digital Signal Processing (DSP),",
      "citeRegEx" : "Zubair and Wang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zubair and Wang.",
      "year" : 2013
    }, {
      "title" : "Subspace methods with local refinements for eigenvalue computation using low-rank tensor-train format",
      "author" : [ "Junyu Zhang", "ZaiwenWen", "Yin Zhang" ],
      "venue" : "Journal of Scientific Computing,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Guaranteed tensor pca with optimality in statistics and computation",
      "author" : [ "Anru Zhang", "Dong Xia" ],
      "venue" : "In arXiv preprint",
      "citeRegEx" : "Zhang and Xia.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhang and Xia.",
      "year" : 2017
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We consider relative error low rank approximation of tensors with respect to the Frobenius norm. Namely, given an order-q tensor A ∈ R ∏q i=1 ni , output a rank-k tensor B for which ‖A − B‖F ≤ (1 + ) OPT, where OPT = infrank-k A′ ‖A − A‖F . Despite much success on obtaining relative error low rank approximations for matrices, no such results were known for tensors. One structural issue is that there may be no rank-k tensor Ak achieving the above infinum. Another, computational issue, is that an efficient relative error low rank approximation algorithm for tensors would allow one to compute the rank of a tensor, which is NP-hard. We bypass these two issues via (1) bicriteria and (2) parameterized complexity solutions: 1. We give an algorithm which outputs a rank k′ = O((k/ )q−1) tensor B for which ‖A − B‖F ≤ (1+ ) OPT in nnz(A)+n ·poly(k/ ) time in the real RAM model, whenever either Ak exists or OPT > 0. Here nnz(A) denotes the number of non-zero entries in A. If both Ak does not exist and OPT = 0, then B instead satisfies ‖A − B‖F < γ, where γ is any positive, arbitrarily small function of n. 2. We give an algorithm for any δ > 0 which outputs a rank k tensor B for which ‖A−B‖F ≤ (1+ ) OPT and runs in (nnz(A)+n poly(k/ )+exp(k/ )) ·nδ time in the unit cost RAM model, whenever OPT > 2−O(n ) and there is a rank-k tensor B = ∑k i=1 ui ⊗ vi ⊗ wi for which ‖A − B‖F ≤ (1 + /2) OPT and ‖ui‖2, ‖vi‖2, ‖wi‖2 ≤ 2 δ). If OPT ≤ 2−Ω(nδ), then B instead satisfies ‖A−B‖F ≤ 2−Ω(n δ). Our first result is polynomial time, and in fact input sparsity time, in n, k, and 1/ , for any k ≥ 1 and any 0 < < 1, while our second result is fixed parameter tractable in k and 1/ . For outputting a rank-k tensor, or even a bicriteria solution with rank-Ck for a certain constant C > 1, we show a 2 1−o(1)) time lower bound under the Exponential Time Hypothesis. Our results are based on an “iterative existential argument”, and give the first relative error low rank approximations for tensors for a large number of error measures for which nothing was known. In particular, we give the first relative error approximation algorithms on tensors for: column row and tube subset selection, entrywise `p-low rank approximation for 1 ≤ p < 2, low rank approximation with respect to sum of Euclidean norms of faces or tubes, weighted low rank approximation, and low rank approximation in distributed and streaming models. We also obtain several new results for matrices, such as nnz(A)-time CUR decompositions, improving the previous nnz(A) log n-time CUR decompositions, which may be of independent interest. ∗Work done while visiting IBM Almaden, and supported in part by UTCS TAship (CS361 Spring 17 Introduction to Computer Security). †Supported in part by Simons Foundation, and NSF CCF-1617955. ar X iv :1 70 4. 08 24 6v 1 [ cs .D S] 2 6 A pr 2 01 7",
    "creator" : "LaTeX with hyperref package"
  }
}