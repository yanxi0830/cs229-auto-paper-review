{
  "name" : "1502.07143.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The VC-Dimension of Similarity Hypothesis Spaces",
    "authors" : [ "Mark Herbster", "Paul Rubenstein", "James Townsend" ],
    "emails" : [ "james.townsend.14}@ucl.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 2.\n07 14\n3v 1\n[ cs\n.L G\n] 2\n5 Fe\nby h(s)(w, x) := 1[h(w) = h(x)]. This idea can be extended to a set of functions, or hypothesis space H ⊆ {0, 1}X by defining a similarity hypothesis space H(s) := {h(s) : h ∈ H}. We show that vc-dimension(H(s)) ∈ Θ(vc-dimension(H))."
    }, {
      "heading" : "1 Introduction",
      "text" : "Consider the problem of learning from examples. We may learn by receiving class labels as feedback: ‘this is a dog’, ‘that is a wolf’ , ‘there is a cat’, etc. We may also learn by receiving similarity labels: ‘these are the same’, ‘those are different’ and so forth. In this note we study the problem of learning with similarity versus class labels. Our approach is to use the VC-dimension [VC71] to study the fundamental difficulty of this learning task.\nIn the supervised learning model we are given a training set of patterns and associated labels. The goal is then to find a hypothesis function that maps patterns to labels that will predict with few errors on future data (small generalization error). A classic approach to this problem is empirical risk minimisation. Here the procedure is to choose a hypothesis from a set of hypothesis functions (hypothesis space) that ‘fits’ the data as closely as possible. If the hypothesis is from a hypothesis space with small VC-dimension and fits the data well then we are likely to predict well on future data [VC71, BEHW89]. The number of examples required to have small generalisation error with high probability is called the sample complexity. In the uniform learnability model the VC-dimension gives a nearly matching upper and lower bound on the sample complexity [BEHW89, EHKV89]. In Theorem 1 we demonstrate that the VC-dimension of a hypothesis space with respect to similarity-labels is proportionally bounded by the VC-dimension with respect to class-labels indicating that the sample complexities within the two feedback\nsettings are comparable. That is, the fundamental difficulties of the two learning tasks are comparable.\nRelated work\nWe are motivated by the results of [GHP13]. Here the authors considered the problem of similarity prediction in the online mistake bound model [Lit88]. In [GHP13, Theorem 1] it was found that given a basic algorithm for class-label prediction with a mistake bound there exists an algorithm for similarity-label prediction with a mistake bound which was larger by no more than a constant factor. In this work we find an analogous result in terms of the VC-dimension."
    }, {
      "heading" : "2 The VC-dimension of similarity hypothesis spaces",
      "text" : "A hypothesis space H ⊆ {0, 1}X is a set of functions from some set of patterns X to the set of labels Y = {0, 1} in the two-class setting. The restriction of a function h ∈ {0, 1}X to a subset X ′ ⊆ X is the function h|X′ ∈ {0, 1}\nX′ with h|X′(x) := h(x) for each x ∈ X ′. Analogously, one can define the restriction of a hypothesis space as H|X′ := {h|X′ : h ∈ H}.\nA subset X ′ ⊆ X is said to be shattered by H if H|X′ = {0, 1} X′ , that is if the restriction contains all possible functions from X ′ to {0, 1}. The VC-dimension [VC71] of a hypothesis space H ⊆ {0, 1}X , denoted d(H), is the size of the largest subset of X which is shattered by H, that is\nd(H) := max X′⊆X\n{|X ′| : H|X′ = {0, 1} X′} .\nSauer’s lemma [VC71, Sau72, She72], which gives a lower bound for the VC-dimension of a hypothesis space, will be used for proving our main result. It states that for a hypothesis space H ⊆ {0, 1}X , if\n|H| >\nm−1 ∑\nk=0\n(\n|X|\nk\n)\n(1)\nthen d(H) ≥ m. Given a function h : X −→ {0, 1}, we may define a function h(s) to measure the similarity of pairs of points in X according to h. Specifically, for h ∈ {0, 1}X we define h(s) ∈ {0, 1}X×X by h(s)(w, x) := 1[h(w) = h(x)], where 1 is the indicator function. This idea can be extended to a hypothesis space H by defining the similarity hypothesis space H(s) := {h(s) : h ∈ H}. We now give our central result,\nTheorem 1. Given a hypothesis space H ⊆ {0, 1}X ,\nd(H)− 1 ≤ d(H(s)) ≤ δd(H) ,\nwith δ = 4.55.\nProof. For the left hand inequality, let n := d(H) and pick a set T = {x1, x2, . . . , xn} of size n which is shattered by H. Then let T ′ = {(x1, x2), (x2, x3), . . . , (xn−1, xn)}. To demonstrate that T ′ is shattered by H(s), let g ∈ {0, 1}T ′\nbe any mapping from T ′ to {0, 1}. Then since T is shattered by H we may find a map h ∈ H with h(x1) = 0 and\nh(xi+1) =\n{\nh(xi) if g(xi, xi+1) = 1 1− h(xi) if g(xi, xi+1) = 0\nfor i = 1, . . . , n − 1. Observe that g = h(s)|T ′ . Since g was chosen arbitrarily, we may conclude that T ′ is indeed shattered by H(s), and therefore d(H(s)) ≥ |T ′| = d(H)− 1.\nFor the right hand inequality, first let M := d(H(s)) and then pick a set U = {(w1, x1), (w2, x2), . . . , (wM , xM )} of size M in X ×X which is shattered by H\n(s). Let V = {w1, w2, . . . , wM , x1, x2, . . . , xM} and note that |H|V | ≥ |H (s)|U | = 2 M . This is because any two maps h and g which agree on V will induce maps h(s) and g(s) which agree on U , so H(s)|U cannot possibly contain more maps than H|V . Using this fact, and applying Sauer’s Lemma (see (1)) to H|V , we see that if\n2M >\nm−1 ∑\nk=0\n(\n|V |\nk\n)\nthen d(H) ≥ d(H|V ) ≥ m. Now note the following inequality (see e.g., [FG06, Lemma 16.19]), which bounds a sum of binomial coefficients:\n⌊ǫn⌋ ∑\ni=0\n(\nn\ni\n)\n≤ 2H(ǫ)n (0 < ǫ < 1/2) , (2)\nwhere H(ǫ) := ǫ log2 1 ǫ + (1 − ǫ) log2 1 1−ǫ denotes the binary entropy function. If we set m = 1 + ⌊2ǫM⌋ for some ǫ < 12 such that H(ǫ) < 1 2 , we have\nm−1 ∑\nk=0\n(\n|V |\nk\n)\n=\n⌊2ǫM⌋ ∑\nk=0\n(\n|V |\nk\n)\n≤\n⌊2ǫM⌋ ∑\nk=0\n(\n2M\nk\n)\n≤ 22MH(ǫ) < 2M\nusing (2) and that |V | ≤ 2M from the definition of V . Thus Sauer’s lemma can be applied with the above value of m and hence\nd(H) ≥ 1 + ⌊2ǫM⌋ ≥ 2ǫM = 2ǫd(H(s)) ,\nas long as H(ǫ) < 1/2. Observe that ǫ = .11 satisfies this condition and thus we have that\nd(H(s)) ≤ 4.55d(H) ."
    }, {
      "heading" : "3 Discussion",
      "text" : "In the following, we give a family of examples where the VC-dimension of the similarity hypothesis space is exactly twice that of the original space. We use the following notation for the set of the first n natural numbers [n] := {1, 2, . . . , n}.\nExample 2. For the hypothesis space of k-sparse vectors, Hk := {h ∈ {0, 1} [n] : ∑n i=1 h(i) ≤ k},\nd(Hk) = k and d(H (s) k ) = 2k ,\nprovided that n ≥ 2k + 1.\nProof. Let X := [n]. Firstly note that d(Hk) ≥ k, since any subset T ⊆ X with |T | ≤ k is shattered by Hk. If T\n′ ⊆ X with |T ′| > k then T ′ cannot possibly be shattered by Hk since there is no element in Hk that labels all elements of T ′ as 1. Therefore d(Hk) = k.\nTo see that d(H (s) k ) ≥ 2k, let U = {(x1, x2), (x2, x3), . . . , (x2k, x2k+1)} for any distinct elements x1, x2, . . . , x2k+1 ∈ X and note that |U | = 2k. To show that U is shattered by H (s) k , let g ∈ {0, 1}\nU be any function from U to {0, 1}. We need to find an h ∈ Hk such that g = h(s)|U . Two functions in {0, 1}\nX which satisfy the condition g = h(s)|U are h0 and h1 defined by h0(x1) = 0, h1(x1) = 1 and\nhj(xi+1) =\n{\nhj(xi) if g(xi, xi+1) = 1 1− hj(xi) if g(xi, xi+1) = 0\nhj(x) = 0 ∀x 6∈ {x1, x2, . . . , x2k+1}\nfor i = 1, . . . , 2k and j = 0, 1. Observe that by construction, h0(xi)+h1(xi) = 1 for each i = 1, . . . , 2k+1 and therefore\n∑2k+1 i=1 h0(xi)+ ∑2k+1 i=1 h1(xi) = ∑2k+1 i=1 [h0(xi)+h1(xi)] =\n2k + 1. This means that we must have ∑2k+1\ni=1 hj(xi) ≤ k for some j and hence hj ∈ Hk\nwith h (s) j |U = g. This proves that d(H (s) k ) ≥ 2k.\nNow suppose, for a contradiction, that d(H (s) k ) > 2k. Then there is some set\nE = {(u1, v1), (u2, v2), . . . , (u2k+1, v2k+1)} ⊆ X × X of size 2k + 1 which is shattered by H(s). Let V := {u1, u2, . . . , u2k+1, v1, v2, . . . , v2k+1} (note that in general we do not necessarily have that |V | = 4k + 2 since the ui and vi need not all be distinct).\nLet G be the graph with vertex set V and edge set E. Observe that elements of Hk correspond to {0, 1}-labellings of V and that elements of H (s) k correspond to {0, 1}- labellings of E. Since E is shattered by H (s) k , every labelling of E is realisable as the induced map h(s) of some h ∈ Hk. Note that G cannot contain a cycle since there is no labelling of V which could induce a similarity labelling on a cycle in which exactly one edge is labelled 0 and the rest are labelled 1∗. So the graph is a union of trees, also known as a ‘forest’. Note that in\n∗Indeed, under any such labelling of E any two vertices in the cycle are connected by two paths, one path containing exactly zero edges labelled with a 0 (implying that the two vertices are labelled the same) and one path containing exactly one edge labelled with a 0 (implying that the two vertices are labelled differently).\ngeneral the number of vertices in a forest is |V | = |E| + r, where |E| is the number of edges and r is the number of trees in the forest. In this case we have |V | = 2k + 1 + r.\nNow choose a labelling g, which labels the vertices of each connected component (tree) in G according to the following rule: for each connected component C in G, label ⌊ |C|2 ⌋ vertices v ∈ C with a 1 and the remaining ⌈ |C| 2 ⌉ with a 0. Note that g /∈ Hk since\n∑\nv∈V\ng(v) = ∑\nC\n∑\nv∈C\ng(v) = ∑\nC\n⌊\n|C|\n2\n⌋\n≥ ∑\nC\n|C| − 1\n2 =\n|V | − r\n2 = k +\n1 2 > k.\nConsider the edge labelling g(s)|E. Since E is shattered by H (s) k , there must be some h ∈ Hk such that h (s)|E = g\n(s)|E . But this is not possible, for if it were, then in order for h(s) to agree with g(s) we would need h|C = g|C or h|C = 1− g|C for each connected component C in G. Swapping the labellings between 0 and 1 on one or more of the connected components can only increase the number of 1 labellings and thus\n∑\nv∈V\nh(v) ≥ ∑\nv∈V\ng(v) > k\nso h cannot be in Hk. Thus we have found a labelling of E, namely g (s)|E , which cannot be in H (s) k . But this is a contradiction of our initial assumption that E was shattered by H (s) k . So we have proved that our assumption must have been incorrect and therefore d(H (s) k ) = 2k.\nIn Theorem 1, the lower bound d(H) − 1 ≤ d(H(s))) is tight, for example when H = {0, 1}[n]. However, observe that in Example 2, the hypothesis space of k-sparse vectors, the similarity space “expands” only by a factor of 2, which is less than the factor δ = 4.55 of Theorem 1. We leave as a conjecture that the upper bound in Theorem 1 can be improved to a factor of two.\nAcknowledgements. We would like to thank Shai Ben-David, Ruth Urner and Fabio Vitale for valuable discussions. In particular we would like thank Ruth Urner for proving an initial motivating upper bound of d(H(s)) ≤ 2d(H) log(2d(H))."
    } ],
    "references" : [ {
      "title" : "Learnability and the Vapnik-Chervonenkis dimension",
      "author" : [ "A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Blumer et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Blumer et al\\.",
      "year" : 1989
    }, {
      "title" : "A general lower bound on the number of examples needed for learning",
      "author" : [ "A. Ehrenfeucht", "D. Haussler", "M. Kearns", "L.G. Valiant" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Ehrenfeucht et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Ehrenfeucht et al\\.",
      "year" : 1989
    }, {
      "title" : "Parameterized Complexity Theory (Texts in Theoretical Computer Science. An EATCS Series)",
      "author" : [ "J. Flum", "M. Grohe" ],
      "venue" : null,
      "citeRegEx" : "Flum and Grohe.,? \\Q2006\\E",
      "shortCiteRegEx" : "Flum and Grohe.",
      "year" : 2006
    }, {
      "title" : "Online similarity prediction of networked data from known and unknown graphs",
      "author" : [ "Claudio Gentile", "Mark Herbster", "Stephen Pasteris" ],
      "venue" : "COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14,",
      "citeRegEx" : "Gentile et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gentile et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm",
      "author" : [ "N. Littlestone" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Littlestone.,? \\Q1988\\E",
      "shortCiteRegEx" : "Littlestone.",
      "year" : 1988
    }, {
      "title" : "On the density of families of sets",
      "author" : [ "N. Sauer" ],
      "venue" : "Journal of Combinatorial Theory, Series A,",
      "citeRegEx" : "Sauer.,? \\Q1972\\E",
      "shortCiteRegEx" : "Sauer.",
      "year" : 1972
    }, {
      "title" : "A combinatorial problem; stability and order for models and theories in infinitary languages",
      "author" : [ "Saharon Shelah" ],
      "venue" : "Pacific J. Math.,",
      "citeRegEx" : "Shelah.,? \\Q1972\\E",
      "shortCiteRegEx" : "Shelah.",
      "year" : 1972
    }, {
      "title" : "On the uniform convergence of relative frequencies of events to their probabilities",
      "author" : [ "V.N. Vapnik", "A.Y. Chervonenkis" ],
      "venue" : "Theory of Probab. and its Applications,",
      "citeRegEx" : "Vapnik and Chervonenkis.,? \\Q1971\\E",
      "shortCiteRegEx" : "Vapnik and Chervonenkis.",
      "year" : 1971
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : null,
    "creator" : "LaTeX with hyperref package"
  }
}