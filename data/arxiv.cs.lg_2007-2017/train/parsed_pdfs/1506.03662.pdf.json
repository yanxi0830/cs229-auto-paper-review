{
  "name" : "1506.03662.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neighborhood Watch: Stochastic Gradient Descent with Neighbors",
    "authors" : [ "Thomas Hofmann", "Aurelien Lucchi", "Brian McWilliams" ],
    "emails" : [ "@inf.ethz.ch" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider a general problem that is pervasive in machine learning, namely optimization of an empirical or regularized convex risk function. Given a convex loss l and a strongly convex regularizer Ω, one aims at finding a parameter vector w which minimizes the (empirical) expectation:\nw∗ ∈ argmin w f(w), f(w) := Efi(w) = 1 n n∑ i=1 fi(w), fi(w) := l(w, (xi, yi)) + Ω(w) . (1)\nWe assume throughout the data-dependent functions fi to have Lipschitz-continuous gradients. Steepest descent is a straightforward algorithm to find a minimizer w∗ of f , but it requires the repeated computation of full gradients f ′(w), which becomes prohibitive in the case of massive data sets and which is impossible in the case of streaming data. Stochastic gradient descent (SGD) is a popular alternative, in particular in the context of large-scale learning [2, 9]. The key advantage of SGD is that each update only involves a single example. This is sufficient to compute a stochastic version of the true gradient as f ′i(w), which provides an unbiased estimate since Ef ′ i(w) = f ′(w).\nIt is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation. Instead of O(1/t) or O(log t/t) rates for standard SGD variants, it is possible to obtain geometric rates, i.e. to guarantee exponentially fast convergence. While the classic convergence proof of SGD requires vanishing step sizes, typically at a rate of O(1/t) [7],\nar X\niv :1\n50 6.\n03 66\n2v 1\n[ cs\n.L G\n] 1\n1 Ju\nn 20\nthese more recent methods introduce corrections to the stochastic gradients that ensure convergence for finite step sizes.\nBased on the work mentioned above, the contributions of our paper are as follows: First, we provide a novel, simple, and more general analysis technique for studying variance reduced versions of SGD. Second, based on the above analysis, we present new insights into the trade-offs between freshness and biasedness of the corrections computed from previous stochastic gradients. Third, we present a new class of algorithms that resolves this trade-off by computing corrections based on stochastic gradients at neighboring points. Fourth, we will show experimentally that this new class of algorithms offers advantages in the regime of streaming data and single/few-epoch learning."
    }, {
      "heading" : "2 Algorithms",
      "text" : "Variance Reduced SGD Given an optimization problem as in (1), we investigate a class of stochastic gradient descent algorithms that generates an iterate sequence wt (t ≥ 0) with updates taking the form:\nw+ = w − γgi(w), gi(w) = f ′i(w)− ᾱi (2)\nHere w is the current and w+ the new parameter vector, γ is the step size, and i is an index selected uniformly at random. ᾱi are variance correction terms such that Eᾱi = 0. The aim is to define updates of asymptotically vanishing variance, i.e. gi(w) → 0 as w → w∗, which requires that ᾱi → f ′i(w∗). This implies the corrections need to be designed in a way to exactly cancel out the stochasticity of f ′i(w ∗) at the optimum.\nSAGA The SAGA algorithm [3] maintains variance corrections αi after selecting data point i by memorizing stochastic gradients. The update rule is α+i = f ′ i(w) for the selected i, and α + j = αj , for j 6= i. Note that these corrections will be used, the next time the same index i gets sampled. Bias adjusted versions are then defined via ᾱi := αi − ᾱ, where ᾱ := 1n ∑n i=1 αi. Obviously, ᾱ can be updated incrementally. One convenient property of SAGA is that it can reuse the stochastic gradient f ′i(w) computed at step t to update both, w as well as the correction αi at no additional costs.\nWe also consider a variant of SAGA, which we call q-SAGA that on average updates q ≥ 1 randomly chosen αj variables at each iteration. While this is practically less interesting, it is a convenient reference point in our analysis as well as in the experiments to investigate the advantages of ”fresher” corrections. Note that in SAGA the corrections will be on average n iterations ”old”. In q-SAGA this can be controlled to be n/q.\nSVRG To show the fruitfulness of our framework, we reformulate a variant of Stochastic Variance Reduced Gradient (SVRG) [5]. We use a randomization argument similar to (but much simpler than) the one suggested in [6] and define a real-valued parameter q > 0. In each iteration we sample r ∼ Uniform[0; 1). If r < q/n we perform a complete update of the α variables α+j = f ′j(w) (∀j), otherwise they are left unchanged. For q = 1 in expectation one parameter is updated per step, which is the same as in SAGA. The main difference is that SAGA always updates exactly one αi in each iteration, while SVRG occasionally updates all α parameters by triggering an additional sweep through the data. One can also see that here ᾱ = f ′(w). There is an option to not maintain α variables explicitly and to save on space by storing only ᾱ and the parameter vector w̃ at which the last full re-computation happened.\nUniform Memorization Algorithms Motivated by SAGA and SVRG, we define a class of algorithms, which we call memorization algorithms. Memorization algorithms use the αi variables to keep track of past stochastic gradients, i.e. αti = f ′ j(w\nτi), for some iteration τi < t. Note that for SAGA as well as SVRG i = j. We refer to this special cases as algorithms without sharing. We will go beyond this restriction by considering i 6= j below. It will turn out to be a technical requirement of our analysis that the probability of updating αi is the same for all i. We call memorization algorithms with this property uniform memorization algorithms.\nN -SAGA We assume that we have structured our data set such that for each data index i, there is a set of neighbors Ni available. Specifically, we propose two variants: (i) The balanced case in\nwhich |Ni| = q (∀i). We call this variant qN -SAGA. It is mainly important for the analysis, since it fulfills the requirements of an uniform memorization algorithm. (ii) The case of -neighborhoods where Ni := {j : ‖xi − xj‖ ≤ }. which we refer to as N -SAGA. As the analysis will show, the quantity that matters most is the expectation of the squared pairwise distances ‖f ′i(w) − f ′j(w)‖2. Since computing those explicitly defeats the purpose of sharing stochastic gradients across points, N -SAGA resorts to the heuristic of constructing neighborhoods based on distances in input space. This can be more formally motivated by assuming a Lipschitz condition of the loss with regard to the inputs.\nBased on a neighborhood system, we define a modified version of SAGA updates on selecting i via\nα+j = { f ′i(w) if i ∈ Nj αj otherwise\n(3)\nIntuitively, the stochastic gradient computed for the observed or sampled data point (xi, yi) is propagated to those data points (xj , yj) for which i is a neighbor of j.\nComputing Neighborhoods From a practical point of view it is critical to limit the computational overhead of N -SAGA. However, note that we do not need exactness in finding nearest neighbors. Obviously, the closer the neighbors, the better the expected improvements. Yet, one can be opportunistic here and use plain SAGA as a fallback option, e.g. in the N -variant with occasionally trivial neighborhoods Ni = {i}. From a practical point of view: (i) We should create or maintain a candidate set of k data vectors, where k ∈ o(n), most drastically k ∈ O(1). The simplest way to do this is via uniform subsampling of k points from the training data. More refined methods are available from the literature on core sets, e.g. [4]. How will decrease with k remains data set dependent though. (ii) We can use locality-sensitive hashing to index these k data points, perhaps even using data-dependent hashing [1], whenever this cost can be amortized. Note also that as SGD is a sequential method, increasing the computational cost in a highly parallelizable way may not affect data throughput."
    }, {
      "heading" : "3 Analysis",
      "text" : "Primal Recurrence The evolution equation (2) in expectation implies the recurrence E‖w+−w∗‖2 = ‖w − w∗‖2 − 2γ〈f ′(w), w − w∗〉+ γ2E‖gi(w)‖2 . (4)\nFrom here we utilize a number of well-known bounds (see e.g. [3]), which exploit strong convexity of f (wherever µ appears) as well as Lipschitz continuity of the fi-gradients (wherever L appears):\n〈f ′i(w), w − w∗〉 ≥ f(w)− f(w∗) + µ 2 ‖w − w∗‖2 , (5)\nE‖gi(w)‖2 ≤ (1 + β)E‖f ′i(w)− f ′i(w∗)‖2 − β‖f ′(w)‖2 + ( 1 + β−1 ) E‖ᾱi − f ′i(w∗)‖2 (β > 0), (6)\n‖f ′(w)‖2 ≥ 2µ (f(w)− f(w∗)) , (7) ‖f ′i(w)− f ′i(w∗)‖2 ≤ 2Lhi(w), hi(w) := fi(w)− fi(w∗) + 〈w − w∗, f ′i(w∗)〉 , (8) E‖f ′i(w)−f ′i(w∗)‖2 ≤ 2L(f(w)− f(w∗)) , (9) E‖ᾱi − f ′i(w∗))‖2 = E‖αi − f ′i(w∗)‖2 − ‖ᾱ‖2 ≤ E‖αi − f ′i(w∗)‖2 (10)\nBy applying all of these, we can derive a β-parameterized bound ‖w−w∗‖2 −E‖w+−w∗‖2 ≥ γµ‖w − w∗‖2 − γ2 ( 1 + β−1 ) E‖αi − f ′i(w∗))‖2 + 2γ [1− γ(L(1 + β)− µβ)] (f(w)− f(w∗)) . (11) Note that in the ideal case of perfect asymptotic variance reduction, where αi = f ′i(w\n∗) we can look at the limit of β → 0 and would immediately get a condition for a contraction by choosing γ = 1L , yielding a contraction rate of 1−ρ with ρ = γµ = µL , which is just the condition number. Our main result will show that we are not losing more than a factor of 4 relative to that gold standard.\nExploiting Properties of Uniform Memorization Algorithms How can we further bound E‖αi − f ′i(w∗)‖2 in the case of variance-reducing SGD? A key insights is that for memorization algorithms without sharing, we can apply the same smoothness bound as in Eq. (8)\n‖αi − f ′i(w∗)‖2 ≤ 2Lhi(wτi) . (12)\nFor the N -SAGA family things get slightly more complicated, but we can apply a similar parametrized bound as before (with φ > 0) to split the squared norm\n‖αi − f ′i(w∗)‖2 ≤(1 + φ)‖f ′i(wτi)− f ′i(w∗)‖2 + (1 + φ−1)‖f ′j(wτi)− f ′i(wτi)‖2 . (13) The first of these terms is the same as before, only re-scaled by (1+φ). The second term is the error introduced by making use of neighbor information (i.e. sharing). We assume it can be bounded in expectation by\nE‖f ′j(w)− f ′i(w)‖2 < η (14) where the expectation is over random index pairs {(i, j) : j ∈ Ni} with regard to the distribution induced by the update rule. We require this bound to hold for each w along the iterate sequence.\nLyapunov Function We want to show that for a suitable choice of the step size γ each iteration results in a contraction that brings us closer to the optimum, i.e. E‖w+−w∗‖2 ≤ (1−ρ)‖w−w∗‖2. where 0 < ρ < 1. However, the main challenge arises from the fact that αi store stochastic gradients from previous iterations, i.e. they constitute quantities that are not evaluated at the current iterate w. This requires a somewhat more complex proof technique. Inspired by the Lyapunov function method in [3] we define upper bounds Hi ≥ ‖αi − f ′i(w∗)‖2 such that Hi → 0 as w → w∗. We (conceptually) initialize Hi = 2Lhi(w0), start with α0i = 0 and then update Hi in synch with αi,\nH+i := { 2Lhi(w) if αi is updated Hi otherwise\n(15)\nso that we always maintain valid bounds ‖αi − f ′i(w∗)‖2 ≤ Hi and E‖αi − f ′i(w∗)‖2 ≤ H̄ with H̄ := 1n ∑n i=1Hi. Note that for N -SAGA ‖αi − f ′i(w∗)‖2 ≤ (1 + φ)Hi + (1 + φ−1)η. It should be clear that Hi are quantities showing up in the analysis, but are not computed in the algorithm.\nWe now define a Lyapunov function (which is much simpler than in [3]) L(w,H) = ‖w − w∗‖2 + Sσ H̄, with S := ( γn\nLq\n) and 0 < σ < 1 . (16)\nIn expectation under a random update, the Lyapunov function L changes as EL(w+, H+) = E‖w+ − w∗‖2 + SσEH̄+. The first part is due to the parameter update and we can apply (11) to bound it. The second part is due to the recurrence (15), which mirrors the update of the α variables. For the previously defined uniform memorization algorithms we have\nEH̄+ = ( n− q n ) H̄ + 2Lq n (f(w)− f(w∗)) (17)\nwhere we have made use of the fact that Ehi(w) = f(w)− f(w∗). Note that for this expectation to work out correctly without further complication requires the mentioned uniformity property. Also note that in expectation the shrinkage does not depend on the location of previous iterates wτ and the new increment is always proportional to the sub-optimality of the current iterate w.\nGeneral Convergence Analysis Our main contraction result for uniform memorization algorithms (without sharing) can be stated as follows: Theorem 1. Given an instance of an uniform memorization algorithm without sharing as defined above. For any choice of β > 0 and 0 < σ < 1 there is a step size γ > 0 such that we get a contraction\nL(w,H)−EL(w+, H+) ≥ ρ(σ, β)L(w,H) with\nρ(σ, β) := µ\nL min\n{ σ\nRσ + (1 + β−1) , 1− σ 1 + β\n} , where R := nµ\nqL .\nAs it provides a lot of motivation and insights, we carry out the proof in the main text. From Eq. (5), we can see that it may be reasonable to aim for ρ ≤ γµ based on the ‖w − w∗‖2 part of L. The question then is, how H̄ changes and what constraints that may impose on γ. First of all note that there is a shrinkage of H̄ that only depends on q\n4+H := Sσ H̄ − S (n− q) n σH̄ = S q n σH̄ = γ L σH̄ . (18)\nBut as we are using Hi to bound ‖αi− f ′i(w∗)‖2 as it appears in (11), we need to subtract a suitable term, namely4−H := γ2(1+β−1)H̄ . Combining the two terms, we pull out S and collect constants:\n4H := 4+H −4−H = Sσ ( Lq\nn\n)[ 1\nL − γ\n( 1 + β−1 ) σ ] H̄ . (19)\nAs we aim for a contraction rate of ρ = γµ, this leads to a constraint on the step size\n1 L − γ\n( 1 + β−1 ) σ ≥ ( qL n )−1 µγ = Rγ ⇐⇒ γ ≤ 1 L · σ Rσ + (1 + β−1) , (20)\nwith R as defined in the claim of the theorem.\nWe next investigate terms that can be bounded by sub-optimality, i.e. f(w)− f∗(w). From Eq. (11) we get a factor 41f := 2γ [1− γ(L(1 + β)− µβ)]. However, we also have f(w) − f(w∗) occur in the H̄ recurrence in (17). After working through cancelations of the constants one gets a second factor42f := −2γσ. Combining both terms, we require\n41f +42f ≥ 0 ⇐⇒ σ + γL(1 + β) ≤ 1 + γµβ ⇐⇒ γ ≤ 1− σ\nL+ β(L− µ) . (21)\nHere, we can directly see that σ < 1 is needed in order to get a positive step size as µ ≤ L implies that the denominator is positive. We would like to simplify the bound, which we can get by strengthening as follows:\n1− σ L+ β(L− µ) ≥ 1− σ L(1 + β) ≥ γ (22)\nSo we have derived two bounds, which – together with the identity ρ = µγ – can be summarized in the claim of the theorem.\nThe theorem provides a two-dimensional family of bounds as β > 0 and 0 < σ < 1 can be chosen arbitrarily. The question is, which choice of constant gives the best (maximal) rate ρ∗. The optimal choice of σ is provided by the following corollary. Corollary 1. In Theorem 1, the maximal rate ρ∗(β) = ρ(β, σ∗) = maxσ ρ(σ, β) is obtained at\nσ∗ = 1\n2R\n[ R− (a+ b) + √ R2 + (a+ b)2 − 2R(a− b) ] (23)\nwhere a = (1 + β) and b = (1 + β−1).\nProof. We have to equate both expressions in the minimum of Theorem 1, resulting in\naσ = (Rσ + b)(1− σ) ⇐⇒ Rσ2 + (a+ b−R)σ − b = 0 (24) Applying the standard formula for quadratic equations provides the claim.\nSolving the resulting bound for β yields the best bound. Here, we simply state a specialization to the case of β = 1. Corollary 2. The optimal rate is lower bounded by\nρ∗ = max β ρ∗(β) ≥ ρ∗(1) = µ 4L 1 + 4 R − √ 1 + ( 4 R )2 (25) Note that the constant R is the ratio of n/q and the condition number µ/L. In the large data case where n/q µ/L, we get the following result. Corollary 3. The optimal contraction rate is guaranteed to be at least\nρ∗ ≥ µ 4L\n( 1 + 4\nR\n) +O ( R−2 ) = 1\n4 (µ L + q n ) +O ( R−2 ) (26)\nProof. Set z = 4/R and perform a Taylor approximation of the bound in Corollary 2 around z = 0:\nb(z) := µ\n4L\n[ 1 + z − √ 1 + z2 ] , b′(z) = µ\n4L\n[ 1− z√\n1 + z2\n] (27)\nApproximating b(z) = µ4L + zb ′(0) gives the claim.\nThe effect of q on the rate can be elucidated as follows. Using b′(z) as defined in Eq. (27) and noting that z = 4Lµnq we see that b ′(q) = 1n [ 1− z/ √ 1 + z2 ] ≤ 1n . Moreover it is easy to see that b′′(q) < 0. Hence the effect of increasing q on the bound in Corollary 2 is at most 1n . If the condition number µL 1n , improved freshness has a small effect, yet for small µ (i.e. because of weak regularization) and a regime where µL ≈ 1n , the rate will increase proportionally to q.\nN -SAGA We now provide results for the N -SAGA algorithm in the (easier to analyze) qN - variant. Taking the above analysis as a starting point, we first need to incorporate the additional penalty term (1 + φ) > 1. So wherever we had (1 + β−1) before, we now will have a factor b = (1 + β−1)(1 + φ), e.g. in Corollary 1.\nMore challenging is the error ‖f ′i(w)− f ′j(w)‖2 for i ∈ Nj as it introduces a finite (non-vanishing) bias that can only be controlled indirectly over the granularity of the neighborhoods. What is possible to obtain in this case is a geometric convergence towards a neighborhood of w∗. Theorem 2. Let ρ be a rate guaranteed by Theorem 1 for a uniform memorization algorithm without sharing (e.g. q-SAGA). For a fixed w assume that E‖f ′i(w) − f ′j(w)‖2 < η (∀i ∈ Nj) and define ρ′ = (1 − ζ)ρ, 0 < ζ < 1. Then for qN -SAGA we have: L(w,H) − EL(w+, H+) ≥ ρ′L(w,H) as long as ‖w − w∗‖ ≤ δ with δ := 2(1−ζ)µ √ ηρ ζ .\nProof. Based on the assumptions it is straightforward to derive\nL(w,H)−EL(w+, H+) ≥ ρL(w,H)− 4γ2η (28) by setting φ = 1. In order to get a contraction of ρ′ = (1 − ζ)ρ, it is required that ζρL(w,H) ≥ 4γ2η. As L(w,H) ≥ ‖w − w∗‖2 and with an adjusted step size γ = ρ′µ we get as a sufficient condition\nζρ‖w − w∗‖2 ≥ 4 ( ρ′\nµ\n)2 η ⇐⇒ ‖w − w∗‖2 ≥ 4η\nµ2 (1− ζ)2ρ ζ (29)\nCorollary 4. As a specialization of Theorem 2 we chose ζ = 2 − √\n3 and get a contraction of ρ′ = (1− 2 + √ 3)ρ ≈ 0.732 ρ as long as ‖w − w∗‖ ≥ 2µ √ 2ηρ.\nCorollary 5. Under the assumptions of Theorem 2, if for i ∈ Nj , E‖f ′i(w) − f ′j(w)‖2 < η holds for all iterates w = wt, then wt converges towards a δ-ball around w∗ at a geometric rate of ρ′.\nSo the theoretically expected behavior of qN -SAGA is to achieve very fast initial progress towards w∗ (for large enough q, faster than SAGA) until the iterate reaches a δ-ball around the optimum. The size of this δ-ball scales with √ η. Within this ball, we either get a random walk behavior or need to switch to a SGD-style schedule for the step size to ensure convergence. Alternatively, we can lower q or ultimately switch to SAGA without sharing (effectively for q = 1), if highly accurate convergence towards the empirical risk minimizer is desired.\nIn practice we have found N -SAGA to behave better than qN -SAGA. This is sensible as the constant neighborhood size in qN -SAGA seems more like a technical requirement than a principled feature."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "Algorithms We present experimental results on the performance of the different variants of memorization algorithms for variance reduced SGD as discussed in this paper. Since SAGA has shown uniformly superior behavior than SVRG in our experiments and N -SAGA has been almost uniformly superior to qN -SAGA (albeit, sometimes with small differences), we focus on these algorithms alongside with SGD as a straw man and q-SAGA as a point of reference for speed-ups. We have chosen q = 50 in q-SAGA and (based on some crude experimentation) chose such that the average neighborhood size q ≈ 20. The same setting was used across all data sets and experiments.\nData Sets As special cases for the choice of the loss function and regularizer in Eq. (1), we consider two commonly occurring problems in machine learning, namely least-square regression and `2-regularized logistic regression. We apply least-square regression on the the million song year regression from the UCI repository. This dataset contains n = 515, 345 data points, each described by d = 90 input features. We apply logistic regression on the cov and ijcnn1 datasets obtained from the libsvm website 1. The cov dataset contains n = 581, 012 datapoints, each described by d = 54 input features. The ijcnn1 dataset contains n = 49, 990 datapoints, each described by d = 22 input features. We added an `2-regularizer Ω(w) = µ‖w‖22 with µ = 10−3 to ensure the objective is strongly convex.\nExperimental Protocol We have run the algorithms in question in an i.i.d. sampling setting and averaged the results over 5 runs. Figure 2 shows the evolution of the value of the objective function f as a function of the number of update steps performed. Note that all algorithms compute one stochastic gradient per update step, with the exception of q-SAGA, which is included here not as a practically relevant algorithm, but as an indication of potential improvements that could be achieved by using fresher corrections. A constant step size γ has been used everywhere, expect for plain SGD. γ = 10−3 was found to be roughly the best in the set {10−1, . . . , 10−5}. For plain SGD we used a\n1http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets\nschedule of the form γt = γ0/(T0 + t) with constants optimized coarsely via cross-validation. The x-axis is expressed in units of n (suggestively called ”epochs”).\nSAGA vs. SGD As we can see, if we run SGD with the same constant step size as SAGA, it takes at least 2-4 epochs until SAGA really shows a significant gain. Of course, if the SGD step size is chosen more conservatively, the gains are more significant from the start.\nSAGA vs. q-SAGA q-SAGA outperforms plain SAGA quite consistently when counting stochastic update steps. This establishes optimistic reference curves of what we can expect to achieve with N -SAGA. The actual speed-up is somewhat data set dependent.\nN -SAGA vs. SAGA and q-SAGA N -SAGA can realize quite a fraction of the possible gains and typically traces nicely between the SAGA and q-SAGA curves. On cov we see solid speed-ups in epoch 1 and 2, which then start wearing off. The same is true for ijcnn1. On year the differences are less significant, but note that here SAGA has only a small edge on SGD with constant step size. At least the difference between N -SAGA and SAGA are bigger than those between SAGA and SGD.\nAsymptotics It should be clearly stated that running N -SAGA at a fixed for longer will not result in good asymptotics on the empirical risk. In our experiments, the cross-over point with SAGA was typically after 5 − 10 epochs. Note that the gains are quite significant though for the single epoch learning. We have found very similar results in a streaming setting, i.e. presenting the data once in random order.\nGeneralization Error Although our analysis was carried out purely for the empirical loss, it is important to also take a look at the expected risk on a test set (as a proxy for generalization performance). Here the curves are somewhat more noisy. On cov and ijcnn1 N -SAGA shows some improvements in early epochs, however on year we get a less satisfying result. Note that here already q-SAGA fails."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We proposed a novel analysis method for variance reducing SGD methods that demonstrates geometric convergence rates and provides a number of new insights, in particular about the role of the freshness of stochastic gradients evaluated at previous iterates. We have also investigated the effect of additional errors in the variance correction terms on the convergence behavior. Motivated by this, we have proposed N -SAGA, a modification of SAGA that can achieve consistent and at times significant speed-ups in the initial phase of the optimization. Most remarkably, this algorithm can be run in a streaming mode, which is – to our knowledge – the first of its kind within the family of variance reduced methods."
    } ],
    "references" : [ {
      "title" : "Optimal data-dependent hashing for approximate near neighbors",
      "author" : [ "A. Andoni", "I. Razenshteyn" ],
      "venue" : "arXiv preprint arXiv:1501.01062,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "In COMPSTAT,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "A. Defazio", "F. Bach", "S. Lacoste-Julien" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Scalable training of mixture models via coresets",
      "author" : [ "D. Feldman", "M. Faulkner", "A. Krause" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Semi-stochastic gradient descent methods",
      "author" : [ "J. Konečnỳ", "P. Richtárik" ],
      "venue" : "arXiv preprint arXiv:1312.1666,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : "The annals of mathematical statistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1951
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "M. Schmidt", "N.L. Roux", "F. Bach" ],
      "venue" : "arXiv preprint arXiv:1309.2388,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Stochastic gradient descent (SGD) is a popular alternative, in particular in the context of large-scale learning [2, 9].",
      "startOffset" : 113,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "Stochastic gradient descent (SGD) is a popular alternative, in particular in the context of large-scale learning [2, 9].",
      "startOffset" : 113,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.",
      "startOffset" : 34,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.",
      "startOffset" : 34,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.",
      "startOffset" : 34,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.",
      "startOffset" : 34,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "While the classic convergence proof of SGD requires vanishing step sizes, typically at a rate of O(1/t) [7],",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "SAGA The SAGA algorithm [3] maintains variance corrections αi after selecting data point i by memorizing stochastic gradients.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "SVRG To show the fruitfulness of our framework, we reformulate a variant of Stochastic Variance Reduced Gradient (SVRG) [5].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : "We use a randomization argument similar to (but much simpler than) the one suggested in [6] and define a real-valued parameter q > 0.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "[4].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "(ii) We can use locality-sensitive hashing to index these k data points, perhaps even using data-dependent hashing [1], whenever this cost can be amortized.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "[3]), which exploit strong convexity of f (wherever μ appears) as well as Lipschitz continuity of the fi-gradients (wherever L appears): 〈f ′ i(w), w − w∗〉 ≥ f(w)− f(w∗) + μ 2 ‖w − w∗‖2 , (5) E‖gi(w)‖ ≤ (1 + β)E‖f ′ i(w)− f ′ i(w)‖ − β‖f ′(w)‖2 + ( 1 + β−1 ) E‖ᾱi − f ′ i(w)‖ (β > 0), (6) ‖f ′(w)‖2 ≥ 2μ (f(w)− f(w∗)) , (7) ‖f ′ i(w)− f ′ i(w)‖ ≤ 2Lhi(w), hi(w) := fi(w)− fi(w) + 〈w − w∗, f ′ i(w)〉 , (8) E‖f ′ i(w)−f ′ i(w)‖ ≤ 2L(f(w)− f(w∗)) , (9) E‖ᾱi − f ′ i(w))‖ = E‖αi − f ′ i(w)‖ − ‖ᾱ‖ ≤ E‖αi − f ′ i(w)‖ (10)",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "Inspired by the Lyapunov function method in [3] we define upper bounds Hi ≥ ‖αi − f ′ i(w)‖ such that Hi → 0 as w → w∗.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "We now define a Lyapunov function (which is much simpler than in [3])",
      "startOffset" : 65,
      "endOffset" : 68
    } ],
    "year" : 2017,
    "abstractText" : "Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet it is also known to be slow relative to steepest descent. The variance in the stochastic update directions only allows for sublinear or (with iterate averaging) linear convergence rates. Recently, variance reduction techniques such as SVRG and SAGA have been proposed to overcome this weakness. With asymptotically vanishing variance, a constant step size can be maintained, resulting in geometric convergence rates. However, these methods are either based on occasional computations of full gradients at pivot points (SVRG), or on keeping per data point corrections in memory (SAGA). This has the disadvantage that one cannot employ these methods in a streaming setting and that speed-ups relative to SGD may need a certain number of epochs in order to materialize. This paper investigates a new class of algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points. While not meant to be offering advantages in an asymptotic setting, there are significant benefits in the transient optimization phase, in particular in a streaming or singleepoch setting. We investigate this family of algorithms in a thorough analysis and show supporting experimental results. As a side-product we provide a simple and unified proof technique for a broad class of variance reduction algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}