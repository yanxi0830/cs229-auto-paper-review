{
  "name" : "1605.01832.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cross-Graph Learning of Multi-Relational Associations",
    "authors" : [ "Hanxiao Liu", "Yiming Yang" ],
    "emails" : [ "HANXIAOL@CS.CMU.EDU", "YIMING@CS.CMU.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Many important problems in multi-source relational learning could be cast as joint learning over multiple graphs about how heterogeneous types of objects interact with\neach other. In literature data analysis, for example, publication records provide rich information about how authors collaborate with each other in a co-authoring graph, how papers are linked in citation networks, how keywords are related via ontology, and so on. The challenging question is about how to combine such heterogeneous information in individual graphs for the labeling or scoring of the multi-relational associations in tuples like (author,paper,keyword), given some observed instances of such tuples as the labeled training set. Automated labeling or scoring of unobserved tuples allows us to discover who have been active in the literature on what areas of research, and to predict who would become influential in which areas in the future. In protein data analysis, as another example, a graph of proteins with pairwise sequence similarities is often jointly studied with a graph of chemical compounds with their structural similarities for the discovery of interesting patterns in (compound,protein) pairs. We call the prediction problem in both examples cross-graph learning of multirelational associations, or simply cross-graph relational learning (CGRL), where the multi-relational associations are defined by the tuples of heterogeneous types of objects, and each object type has its own graph with type-specific relational structure as a part of the provided data. The task is to predict the labels or the scores of unobserved multirelational tuples, conditioned on a relatively small set of labeled instances.\nCGRL is an open challenge in machine learning for several reasons. Firstly, the number of multi-relational tuples grows combinatorially in the numbers of individual graphs and the number of nodes in each graph. How to make crossgraph inference computationally tractable for large graphs is a tough challenge. Secondly, how to combine the internal structures or relations in individual graphs for joint inference in a principled manner is an open question. Thirdly, supervised information (labeled instances) is typically extremely sparse in CGRL due to the very large number of all possible combinations of heterogeneous objects in individual graphs. Consequently, the success of cross-graph learning crucially depends on effectively leveraging the massively available unlabeled tuples (and the latent relations\nar X\niv :1\n60 5.\n01 83\n2v 1\n[ cs\n.L G\n] 6\nM ay\n2 01\n6\namong them) in addition to the labeled training data. In other words, how to make the learning transductive is crucial for the true success of CGRL. Research on transdcutive CGRL has been quite limited, to our knowledge.\nExisting approaches in CGRL or CGRL-related areas can be outlined as those using tensors or graph-regularized tensors, and kernel machines that combine multiple kernels.\nTensor methods have been commonly used for combining multi-source evidence of the interactions among multiple types of objects (Nickel et al., 2011; Rendle et al., 2009; Kolda & Bader, 2009) as the combined evidence can be naturally represented as tuples. However, most of the tensor methods do not explicitly model the internal graph structure for each type of objects, although some of those methods implicitly leverage such information via graph-based regularization terms in their objective function that encourage similar objects within each graph to share similar latent factors (Narita et al., 2012; Cai et al., 2011). A major weakness in such tensor methods is the lack of convexity in their models, which leads to ill-posed optimization problems particularly in high-order scenarios. It has also been observed that tensor factorization models suffer from labelsparsity issue, which is typically severe in CGRL.\nKernel machines have been widely studied for supervised classifiers, where a kernel matrix corresponds to a similarity graph among a single type of objects. Multiple kernels can be combined, for example, by taking the tensor product of each individual kernel matrix, which results in a desired kernel matrix among cross-graph multi-relational tuples. The idea has been explored in relational learning combined with SVMs (Ben-Hur & Noble, 2005), perceptions (Basilico & Hofmann, 2004) or Gaussian process (Yu & Chu, 2008) for two types of objects and is generalizable to the multi-type scenario of CGRL. Although being generic, the complexity of such kernel-based methods grows exponentially in the number of individual kernels (graphs) and the size of each individual graph. As a result, kernel machines suffer from poor scalability in general. In addition, kernel machines are purely supervised (not for transductive learning), i.e., they cannot leverage the massive number of available non-observed tuples induced from individual graphs and the latent connections among them. Those limitations make existing kernel methods less powerful for solving the CGRL problem in large scale and under severely datasparse conditions.\nIn this paper, we propose a novel framework for CGRL which can be characterized as follows: (i) It uses graph products to map heterogeneous sources of information and the link structures in individual graphs onto a single homogeneous graph; (ii) It provides a convex formulation and approximation of the CGRL problem that ensure robust optimization and efficient computation; and (iii) It en-\nables transductive learning in the form of label propagation over the induced homogeneous graph so that the massively available non-observed tuples and the latent connections among them can play an important role in effectively addressing the label-sparsity issue.\nThe proposed framework is most related to (Liu & Yang, 2015), where the authors formulated graph products for learning the edges of a bipartite graph. Our new framework is fundamentally different in two aspects. First, our new formulation and algorithms allow the number of individual graphs to be greater than two, while method in (Liu & Yang, 2015) is only applicable to two graphs. Secondly, the algorithms in (Liu & Yang, 2015) suffer from cubic complexity over the graphs sizes (quadratic by using a non-convex approximation), while our new algorithm enjoys both the convexity of the formulation and the low time complexity which is linear over the graph sizes.\nThe paper is organized as follows: Section 2 shows how cross-graph multi-relations can be embedded into the vertex space of a homogeneous graph. Section 3 describes how efficient label propagation among multi-relations can be carried out in such space with approximation. We discuss our optimization algorithm in Section 4 and provide empirical evaluations over real-world datasets in Section 5."
    }, {
      "heading" : "2. The Proposed Method",
      "text" : "We introduce our notation in 2.1 and the notion of graph product (GP) in 2.2. We then narrow down to a specific GP family with desirable computational properties in 2.2, and finally propose our GP-based optimization objective in 2.4."
    }, {
      "heading" : "2.1. Notations",
      "text" : "We are given J heterogeneous graphs where the j-th graph contains nj vertices and is associated with an adjacency matrix G(j) ∈ Rnj×nj . We use ij to index the ij-th vertex of graph j, and use a tuple (i1, . . . , iJ) to index each multi-relation across the J graphs. The system predictions over all possible ∏J j=1 nj multi-relations is summarized in an order-J tensor f ∈ Rn1×···×nJ , where fi1,i2,...,iJ corresponds to the prediction about tuple (i1, . . . , iJ). Denote by ⊗ the Kronecker (Tensor) product. We use⊗J j=1 xj (or simply ⊗ j xj) as the shorthand for x1⊗· · ·⊗ xJ . Denote by×j the j-mode product between tensors. We refer the readers to (Kolda & Bader, 2009) for a thorough introduction about tensor mode product.\nP ( ︸ ︷︷ ︸\nG(1)\n, ︸ ︷︷ ︸ G(2) , ︸ ︷︷ ︸ G(3)\n) =\nFigure 1. Graph product ofG(1),G(2) andG(3). Each vertex in the resulting graph P\n( G(1),G(2),G(3) ) corresponds to a multi-relation\nacross the original graphs. E.g., vertex 3.II.B in P corresponds to multi-relation (3,II,B) across G(1), G(2) and G(3)."
    }, {
      "heading" : "2.2. Graph Product",
      "text" : "In a nutshell, graph product (GP) 1 is a mapping from each cross-graph multi-relation to each vertex in a new graph P , whose edges encode similarities among the multi-relations (illustrated in Fig. 1). A desirable property of GP is it provides a natural reduction from the original multi-relational learning problem over heterogeneous information sources (Task 1) to an equivalent graph-based learning problem over a homogeneous graph (Task 2).\nTask 1. Given J graphs G(1), . . . , G(J) with a small set of labeled multi-relations O = {(i1, . . . , iJ)}, predict labels of the unlabeled multi-relations.\nTask 2. Given the product graph P ( G(1), . . . , G(J) ) with a small set of labeled vertices O = {(i1, . . . , iJ)}, predict labels of its unlabeled vertices."
    }, {
      "heading" : "2.3. Spectral Graph Product",
      "text" : "We define a parametric family of GP operators named the spectral graph product (SGP), which is of particular interest as it subsumes the well-known Tensor GP and Cartesian GP (Table 1), is well behaved (Theorem 1) and allows efficient optimization routines (Section 3).\nLet λ(j)ij and v (j) ij\nbe the ij-th eigenvalue and eigenvector for the graph j, respectively. We construct SGP by defining the eigensystem of its adjacency matrix based on the provided J heterogeneous eigensystems of G(1), . . . , G(J).\nDefinition 1. The SGP of G(1), . . . , G(J) is a graph consisting of ∏ j nj vertices, with its adjacency matrix Pκ :=\nPκ ( G(1), . . . , G(J) ) defined by the following eigensystem{\nκ ( λ (1) i1 , . . . , λ (J) iJ ) , ⊗ j v (j) ij } i1,...,iJ\n(1)\nwhere κ is a pre-specified nonnegative nondecreasing function over λ(j)ij ,∀j = 1, 2, . . . , J .\n1 While traditional GP only applies to two graphs, we generalize it to the case of multiple graphs (Section 2.3).\nIn other words, the (i1, . . . , iJ)-th eigenvalue of Pκ is defined by coupling the λ(1)i1 , . . . , λ (J) iJ\nwith function κ, and the (i1, . . . , iJ)-th eigenvector of Pκ is defined by coupling v(1)i1 , . . . , v (J) iJ via tensor (outer) product.\nRemark 1. If each individual { v (j) ij }nj ij=1 forms an orthog-\nonal basis in Rnj , ∀j ∈ 1, . . . , J , then {⊗\nj v (j) ij } i1,...,iJ\nforms an orthogonal basis in R ∏J j=1 nj .\nIn the following example we introduce two special kinds of SGPs, assuming J = 2 for brevity. Higher-order cases are later summarized in Table 1.\nExample 1. Tensor GP defines κ(λi1 , λi2) = λi1λi2 , and is equivalent to Kronecker product: PTensor ( G(1), G(2) ) =∑\ni1,i2 (λi1λi2) ( v (1) i1 ⊗ v(2)i2 )( v (1) i1 ⊗ v(2)i2 )> ≡ G(1)⊗G(2). Cartesian GP defines κ(λi1 , λi2) = λi1 + λi2 , and is equivalent to the Kronecker sum: PCartesian ( G(1), G(2) ) =∑\ni1,i2 (λi1+λi2) ( v (1) i1 ⊗v(2)i2 )( v (1) i1 ⊗v(2)i2 )> ≡ G(1)⊕G(2). SGP Type κ ( λ\n(1) i1 , · · · , λ(J)iJ ) [Pκ](i1,···iJ ),(i′1,···i′J )\nTensor ∏ j λ (j) ij\n∏ j G (j)\nij ,i ′ j Cartesian ∑ j λ (j) ij ∑ j G (j)\nij ,i ′ j ∏ j′ 6=j δij′=i′j′\nTable 1. Tensor GP and Cartesian GP in higher-orders.\nWhile Tensor GP and Cartesian GP provide mechanisms to associate multiple graphs in a multiplicative/additive manner, more complex cross-graph association patterns can be modeled by specifying κ. E.g., κ (λi1 , λi2 , λi3) = λi1λi2 + λi2λi3 + λi3λi1 indicates pairwise associations are allowed among three graphs, but no triple-wise association is allowed as term λi1λi2λi3 is not involved. Including higher order polynomials in κ amounts to incorporating higher-order associations among the graphs, which can be achieved by simply exponentiating κ.\nSince what the product graph P offers is essentially a similarity measure among multi-relations, shuffling the order of input graphs G(1), . . . , G(J) should not affect P’s topological structure. For SGP, this property is guaranteed by the following theorem: Theorem 1 (The Commutative Property). SGP is commutative (up to graph isomorphism) if κ is commutative.\nWe omit the proof. The theorem suggests the SGP family is well-behaved as long as κ is commutative, which is true for both Tensor and Cartesian GPs as both multiplication and addition operations are order-insensitive."
    }, {
      "heading" : "2.4. Optimization Objective",
      "text" : "It is often more convenient to equivalently write tensor f as a multi-linear map. E.g., when J = 2, tensor (matrix) f ∈ Rn1×n2 defines a bilinear map from Rn1 × Rn2 to R via f(x1, x2) := x>1 fx2 and we have fi1,i2 = f(ei1 , ei2). Such equivalence is analogous to high-order cases where f defines a multi-linear map from Rn1 × · · · × RnJ to R.\nTo carry out transductive learning over Pκ (Task 2), we inject the structure of the product graph into f via a Gaussian random fields prior (Zhu et al., 2003). The negative loglikelihood of the prior − log p (f |Pκ) is the same (up to constant) as the following squared semi-norm\n‖f‖2Pκ = vec(f) >P−1κ vec(f) (2)\n= ∑\ni1,i2,...,iJ\nf ( v (1) i1 , . . . , v (J) iJ )2 κ ( λ (1) i1 , . . . , λ (J) iJ\n) (3) Our optimization objective is therefore defined as\nmin f∈Rn1×···×nJ\n`O (f) + γ\n2 ‖f‖2Pκ (4)\nwhere `O(·) is a loss function to be defined later (Section 4), O is the set of training tuples, and γ is a tuning parameter controlling the strength of graph regularization."
    }, {
      "heading" : "3. Convex Approximation",
      "text" : "The computational bottleneck for optimization (4) lies in evaluating ‖f‖2Pκ and its first-order derivative, due to the extremely large size of Pκ. In section 3.1, we first identify the computation bottleneck of using the exact formulation, based on which we propose our convex approximation scheme in 3.2 that reduces the time complexity of evaluating the semi-norm ‖f‖2Pκ from O ((∑ j nj )(∏ j nj )) to\nO (∏ j dj ) , where dj nj for j = 1, . . . , J ."
    }, {
      "heading" : "3.1. Complexity of the Exact Formulation",
      "text" : "The brute-force evaluation of ‖f‖2Pκ according to (3) costs O ((∏ j nj )2) , as one has to evaluate O (∏ j nj ) terms\ninside the summation where each term costs O (∏ j nj ) . However, redundancies exist and the minimum complexity for the exact evaluation is given as follows Proposition 1. The exact evaluation of semi-norm ‖f‖Pκ takes O ((∑ j nj )(∏ j nj )) flops.\nProof. Notice that the collection of all numerators in (3), namely [ f ( v (1) i1 , . . . , v (J) iJ )] i1,··· ,iJ\n, is a tensor in Rn1×···×nJ that can be precomputed via((\nf ×1 V (1) ) ×2 V (2) ) · · · ×J V (J) (5)\nwhere ×j stands for the j-mode product between a tensor in Rn1×···×nj×···×nJ and V (j) ∈ Rnj×nj . The conclusion follows as the j-th mode product in (5) takes O ( nj ∏ j nj ) flops, and one has to do this for each j = 1, . . . , J . When J = 2, (5) reduces to the multiplication of three matrices V (1) > fV (2) at the complexity of O ((n1 + n2)n1n2)."
    }, {
      "heading" : "3.2. Approximation via Tucker Form",
      "text" : "Equation (5) implies the key for complexity reduction is to reduce the cost of the j-mode multiplications · ×j V (j). Such multiplication costs O ( nj ∏ j nj ) in general, but can be carried out more efficiently if f is structured.\nOur solution is twofold: First, we include only the top-dj eigenvectors in V (j) for each graph G(i), where dj nj . Hence each V (j) becomes a thin matrix in Rnj×dj . Second, we restrict tensor f to be within the linear span of the top∏J j=1 dj eigenvectors of the product graph Pκ\nf = d1,··· ,dJ∑ k1,··· ,kJ=1 αk1,··· ,kJ ⊗ j v (j) kj\n(6)\n= α×1 V (1) ×2 V (2) ×3 · · · ×J V (J) (7)\nThe combination coefficients α ∈ Rd1×···×dJ is known as the core tensor of Tucker decomposition. In the case where J = 2, the above is equivalent to saying f ∈ Rn1×n2 is a low-rank matrix parametrized by α ∈ Rd1×d2 such that f = ∑ k1,k2 αk1,k2v (1) k1 v (2) k2 > = V (1)αV (2) > .\nCombining (6) with the orthogonality property of eigenvectors leads to the fact that f ( v (1) k1 , . . . , v (J) kJ ) = αk1,··· ,kJ . To\nsee this for J = 2, notice f ( v (1) k1 , v (2) k2 ) = v (1) k1 > fv (2) k1 = v (1) k1 > V (1)αV (2) > v (2) k1\n= e>k1αek2 = αk1,k2 . Therefore the semi-norm in (2) can be simplified as\n‖f‖2Pκ = ‖α‖ 2 Pκ = d1,··· ,dJ∑ k1,...,kJ=1\nα2k1,··· ,kJ κ ( λ (1) k1 , . . . , λ (J) kJ ) (8) Comparing (8) with (3), the number of inside-summation terms is reduced from O (∏ j nj ) to O (∏ j dj ) where\nFigure 2. An illustration of the eigenvectors of G(1), G(2) and P\n( G(1), G(2) ) . We plot leading nontrivial eigenvectors of G(1) and\nG(2) in blue and red curves, respectively, and plot the induced leading nontrivial eigenvectors of P\n( G(1), G(2) ) in 3D. IfG(1) andG(2)\nare symmetrically normalized, their eigenvectors (corresponding to eigenvectors of the graph Laplacian) will be ordered by smoothness w.r.t. the graph structures. As a result, eigenvectors of P ( G(1), G(2) ) will also be ordered by smoothness.\ndj nj . In addition, the cost for evaluating each term inside summation is reduced from O (∏ j nj ) to O(1).\nDenote by V (j)ij ∈ R dj the ij-th row of V (j), we obtain the following optimization by replacing f with α in (4)\nmin α∈Rd1×···×dJ\n`O (f) + γ\n2 ‖α‖2Pκ\ns.t. f = α×1 V (1) ×2 · · · ×J V (J) (9)\nOptimization above has intuitive interpretations. In principle, it is natural to emphasis bases in f that are “smooth” w.r.t. the manifold structure of Pκ, and de-emphasis those that are “nonsmooth” in order to obtain a parsimonious hypothesis with strong generalization ability. We claim this is exactly the role of regularizer (8). To see this, note any nonsmooth basis ⊗ j v (j) kj of Pκ is likely to be associated with\nsmall a eigenvalue κ ( λ (1) k1 , . . . , λ (J) kJ ) (illustrated in Fig. 2). The conclusion follows by noticing that αk1,...,kJ is essentially the activation strength of ⊗ j v (j) kj\nin f (implied by (6)), and that (8) is going to give any αk1,...,kJ associated with a small κ ( λ (1) k1 , . . . , λ (J) kJ ) a stronger penalty.\n(9) is a convex optimization problem over α with any convex `O(·). Spectral approximation techniques for graphbased learning has been found successful in standard classification tasks (Fergus et al., 2009), which are special cases under our framework when J = 1. We introduce this technique for multi-relational learning, which is particularly desirable as the complexity reduction will be much more significant for high-order cases (J >= 2).\nWhile f in (6) is assumed to be in the Tucker form, other low-rank tensor representation schemes are potentially applicable. E.g., the Candecomp/Parafac (CP) form that fur-\nther restricts α to be diagonal, which is more aggressive but substantially less expressive. The Tensor-Train decomposition (Oseledets, 2011) offers an alternative representation scheme in the middle of Tucker and CP, but the resulting optimization problem will suffer from non-convexity."
    }, {
      "heading" : "4. Optimization",
      "text" : "Let (x)+ = max (0, 1− x) be the shorthand for hinge loss. We define `O(f) to be the ranking `2-hinge loss\n`O(f) =\n∑ (i1, . . . , iJ ) ∈ O (i′1, . . . , i ′ J ) ∈ Ō ( fi1...iJ − fi′1...i′J )2 +\n|O × Ō| (10)\nwhere Ō is the complement of O w.r.t. all possible multirelations. Eq. (10) encourages the valid tuples in our training set O to be ranked higher than those corrupted ones in Ō, and is known to be a surrogate of AUC.\nWe use stochastic gradient descent for optimization as |O| is usually large. In each iteration, a random valid multirelation (i1, . . . , iJ) is uniformly drawn fromO, a random corrupted multirelation (i′1, . . . , i ′ J) is uniformly drawn from Ō. The associated noisy gradient is computed as\n∇α = ∂`O ∂f ( ∂fi1,...,iJ ∂α − ∂fi′1,...,i′J ∂α ) + γα κ (11)\nwhere we abuse the notation by defining κ ∈ Rd1×···×dJ , κk1,...,kJ := κ ( λ (1) k1 , . . . , λ (J) kJ ) ; is the element-wise di-\nAlgorithm 1: Transductive Learning over Product Graph (TOP) foreach j ∈ 1, . . . , J do{\nv (j) k , λ (j) k }dj k=1 ← APPROX EIGEN(G(j));\nforeach (k1, . . . , kJ) ∈ [d1]× . . . [dJ ] do κk1,...,kJ ← κ(λ (1) k1 , . . . , λ (J) kJ ); α← 0, Z ← 0; while not converge do\n(i1, . . . , iJ) uni∼ O, (i′1, . . . , i′J) uni∼ Ō; fi1,...,iJ ← α×1 V (1) i1 ×2 · · · ×J V (J)iJ ; fi′1,...,i′J ← α×1 V (1) i′1 ×2 · · · ×J V (J)i′J ; δ = fi1,...,iJ − fi′1,...,i′J ; if δ < 1 then ∇α ← 2(δ− 1) (⊗ j V (j) ij − ⊗ j V (j) i′j ) + γα κ;\nelse ∇α ← γα κ;\nZ ← Z +∇ 2α ; α← α− η0Z − 1 2 ∇α;\nreturn α\nvision between tensors. The gradient w.r.t. α in (11) is\n∂fi1,...,iJ ∂α\n= ∂ ( α×1 V (1)i1 ×2 · · · ×J V (J) iJ ) ∂α\n(12) = ⊗ j V (j) ij ∈ Rd1×...dJ (13)\nEach SGD iteration costs O (∏ j dj )\nflops, which is independent from n1, n2, . . . , nJ . After obtaining the solution α̂(κ) of optimization (9) for any given SGP Pκ, our final predictions in f̂(κ) can be recovered via (6).\nFollowing (Duchi et al., 2011), we allow adaptive step sizes for each element in α. That is, in the t-th iteration we use\nη (t) k1,...,kJ = η0 /[∑t τ=1∇α (τ) k1,...,kJ 2] 12 as the step size for\nαk1,...,kJ , where { ∇α(τ)k1,...,kJ }t τ=0\nare historical gradients associated with αk1,...,kJ and η0 is the initial step size (set to be 1). The strategy is particularly efficient with highly redundant gradients, which is our case where the gradient is a regularized rank-2 tensor, according to (11) and (13).\nIn practice (especially for large J), the computation cost of tensor operations involving ⊗J j=1 V (j) ij ∈ Rd1,...,dJ is not ignorable even if d1, d2, . . . , dJ are small. Fortunately, such medium-sized tensor operations in our algorithm are highly parallelable over GPU. The pseudocode for our optimization algorithm is summarized in Alg. 1."
    }, {
      "heading" : "5. Experiments",
      "text" : ""
    }, {
      "heading" : "5.1. Datasets",
      "text" : "We evaluate our method on real-world data in two different domains: the Enzyme dataset (Yamanishi et al., 2008) for compound-protein interaction and the DBLP dataset of scientific publication records. Fig. 3 illustrates their heterogeneous objects and relational structures.\nThe Enzyme dataset has been used for modeling and predicting drug-target interactions, which contains a graph of 445 chemical compounds (drugs) and a graph of 664 proteins (targets). The prediction task is to label the unknown compound-protein interactions based on both the graph structures and a small set of 2,926 known interactions. The graph of compounds is constructed based on the SIMCOMP score (Hattori et al., 2003), and the graph of proteins is constructed based on the normalized SmithWaterman score (Smith & Waterman, 1981). While both graphs are provided in the dense form, we converted them into sparse kNN graphs where each vertex is connected with its top 1% neighbors.\nAs for the DBLP dataset, we use a subset of 34,340 DBLP publication records in the domain of Artificial Intelligence (Tang et al., 2008), from which 3 graphs are constructed as:\n• For the author graph (G(1)) we draw an edge between two authors if they have coauthored an overlapping set of papers, and remove the isolated authors using a DFS algorithm. We then obtain a symmetric kNN graph by connecting each author with her top 0.5% nearest neighbors using the count of co-authored papers as the proximity measure. The resulting graph has 5,517 vertices with 17 links per vertex on average.\n• For the paper graph (G(2)) we connect two papers if both of them cite another paper, or are cited by another paper. Like G(1), we remove isolated papers using DFS and construct a symmetric 0.5%-NN graph. To measure the similarity of any given pair of papers, we represent each paper as a bag-of-citations and compute their cosine similarity. The resulted graph has 11,879 vertices and has an average degree of 50.\n• For the venue graph (G(3)) we connect two venues if they share similar research focus. The venue-venue similarity is measured by the total number of crosscitations in between, normalized by the size of the two venues involved. The symmetric venue graph has 22 vertices and an average degree of 7.\nTuples in the form of (Author,Paper,Venue) are extracted from the publication records, and there are 15,514 tuples (cross-graph interactions) after preprocessing."
    }, {
      "heading" : "5.2. Methods for Comparison",
      "text" : "• Transductive Learning over Product Graph (TOP). The proposed method. We explore the following κ’s for parametrizing the spectral graph product.\nName κ(x, y) (J = 2) κ(x, y, z) (J = 3)\nTensor xy xyz Cartesian x+ y x+ y + z\nExponential ex+y exy+yz+xz\nFlat 1 1\n• Tensor Factorization (TF) and Graph-regularized TF (GRTF). In TF we factorize f ∈ Rn1×···×nJ as a set of dimensionality-reduced latent factors Cd1,×···×dJ , Un1×d11 , . . . , UJ ∈ RnJ×dJ . In GRTF, we further enhanced the traditional TF by adding graph regularizations to the objective function, which enforce the model to be aware of the context information inG(j)’s (Narita et al., 2012; Cai et al., 2011);\n• One-class Nearest Neighbor (NN). We score each tuple (i1, . . . , iJ) in the test set with f̂(i1, . . . , iJ) = max(i′1,...,i′J)∈O ∏J j=1Giji′j . That is, we assume the\ntuple-tuple similarity can be factorized as the product of vertex-level similarities across different graphs. We experimented with several other similarity measures and empirically found the multiplicative similarity leads to the best overall performance. Note it does\nnot rely on the presence of any negative examples.\n• Ranking Support Vector Machines (Joachims, 2002) (RSVM). For the task of completing the missing paper in (Author,?,Venue), we use a Learning-toRank strategy by treating (Author,Venue) as the query and Paper as the document to be retrieved. The query feature is constructed by concatenating the eigen-features of Author and Venue, where we define the eigen-feature of vertex ij in graph j as V (j) ij ∈\nRdj . The feature for each query-document pair is obtained by taking the tensor product of the query feature and document eigen-feature.\n• Low-rank Tensor Kernel Machines (LTKM). While traditional tensor-based kernel construction methods for tuples suffer from poor scalability. We propose to speedup by replacing each individual kernel with its low-rank approximation before tensor product, leading to a low-rank kernel of tuples which allows more efficient optimization routines.\nFor fair comparison, loss functions for TF, GRTF, RSVM and LTKM are set to be exactly the same as that for TOP, i.e. E.q. (10). All algorithms are trained using a minibatched stochastic gradient descent.\nWe use the same eigensystems (eigenvectors and eigenvalues) of the G(j)’s as the input for TOP, RSVM and LTKM. The number of top-eigenvalues/eigenvectors dj for graph j is chosen such that λ(j)1 , . . . , λ (j) dj\napproximately cover 80% of the total spectral energy ofG(j). With respect to this criterion, we choose d1 = 1, 281, d2 = 2, 170, d3 = 6 for DBLP, and d1 = 150, d2 = 159 for Enzyme."
    }, {
      "heading" : "5.3. Experiment Setups",
      "text" : "For both datasets, we randomly sample one third of known interactions for training (denoted by O), one third for validation and use the remaining ones for testing. Known interactions in the test set, denoted by T , are treated as positive examples. All tuples not in T , denoted by T̄ , are treated as\nnegative. Tuples that are already in O are removed from T̄ to avoid misleading results (Bordes et al., 2013).\nWe measure algorithm performance on Enzyme based on the quality of inferred target proteins given each compound, namely by the ability of completing (Compound,?). For DBLP, the performance is measured by the quality of inferred papers given author and venue, namely by the ability of completing (Author,?,Venue). We use Mean Average Prevision (MAP), Area Under the Curve (AUC) and Hits at Top 5 (Hits@5) as our evaluation metrics."
    }, {
      "heading" : "5.4. Results",
      "text" : "Fig. 4 compares the results of TOP with various parameterizations of the spectral graph product (SGP). Among those, Exponential κ works better on average.\nFigs. 5 and 6 show the main results, comparing TOP (with Exponential κ) with other representative baselines. Clearly, TOP outperforms all the other methods on both datasets in all the evaluation metrics of MAP 2, AUC and Hit@5.\nFig. 7 shows the performance curves of TOP on Enzyme over different model sizes (by varying the dj’s). With a relatively small model size compared with using the full spectrum, TOP’s performance converges to the optimal point.\n2MAP scores for random guessing are 0.014 on Enzyme and 0.00072 on DBLP, respectively."
    }, {
      "heading" : "6. Concluding Remarks",
      "text" : "The paper presents a novel convex optimization framework for transductive CGRL and a scalable algorithmic solution with guaranteed global optimum and a time complexity that does not depend on the sizes of input graphs. Our experiments on multi-graph data sets provide strong evidence for the superior power of the proposed approach in modeling cross-graph inference and large-scale optimization."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank the reviewers for their helpful comments. This work is supported in part by the National Science Foundation (NSF) under grants IIS-1216282, 1350364, 1546329."
    } ],
    "references" : [ {
      "title" : "Unifying collaborative and content-based filtering",
      "author" : [ "Basilico", "Justin", "Hofmann", "Thomas" ],
      "venue" : "In Proceedings of the twenty-first international conference on Machine learning,",
      "citeRegEx" : "Basilico et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Basilico et al\\.",
      "year" : 2004
    }, {
      "title" : "Kernel methods for predicting protein–protein interactions",
      "author" : [ "Ben-Hur", "Asa", "Noble", "William Stafford" ],
      "venue" : "Bioinformatics, 21(suppl 1):i38–i46,",
      "citeRegEx" : "Ben.Hur et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ben.Hur et al\\.",
      "year" : 2005
    }, {
      "title" : "Translating embeddings for modeling multi-relational data",
      "author" : [ "Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Graph regularized nonnegative matrix factorization for data representation",
      "author" : [ "Cai", "Deng", "He", "Xiaofei", "Han", "Jiawei", "Huang", "Thomas S" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Cai et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2011
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "Duchi", "John", "Hazan", "Elad", "Singer", "Yoram" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Semisupervised learning in gigantic image collections",
      "author" : [ "Fergus", "Rob", "Weiss", "Yair", "Torralba", "Antonio" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Fergus et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Fergus et al\\.",
      "year" : 2009
    }, {
      "title" : "Heuristics for chemical compound matching",
      "author" : [ "Hattori", "Masahiro", "Okuno", "Yasushi", "Goto", "Susumu", "Kanehisa", "Minoru" ],
      "venue" : "Genome Informatics,",
      "citeRegEx" : "Hattori et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Hattori et al\\.",
      "year" : 2003
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "Joachims", "Thorsten" ],
      "venue" : "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Joachims and Thorsten.,? \\Q2002\\E",
      "shortCiteRegEx" : "Joachims and Thorsten.",
      "year" : 2002
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "Kolda", "Tamara G", "Bader", "Brett W" ],
      "venue" : "SIAM review,",
      "citeRegEx" : "Kolda et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kolda et al\\.",
      "year" : 2009
    }, {
      "title" : "Bipartite edge prediction via transductive learning over product graphs",
      "author" : [ "Liu", "Hanxiao", "Yang", "Yiming" ],
      "venue" : "In Proceedings of The 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Tensor factorization using auxiliary information",
      "author" : [ "Narita", "Atsuhiro", "Hayashi", "Kohei", "Tomioka", "Ryota", "Kashima", "Hisashi" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "Narita et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Narita et al\\.",
      "year" : 2012
    }, {
      "title" : "Identification of common molecular subsequences",
      "author" : [ "Smith", "Temple F", "Waterman", "Michael S" ],
      "venue" : "Journal of molecular biology,",
      "citeRegEx" : "Smith et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 1981
    }, {
      "title" : "Arnetminer: extraction and mining of academic social networks",
      "author" : [ "Tang", "Jie", "Zhang", "Jing", "Yao", "Limin", "Li", "Juanzi", "Su", "Zhong" ],
      "venue" : "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Tang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2008
    }, {
      "title" : "Prediction of drug–target interaction networks from the integration of chemical and genomic spaces",
      "author" : [ "Yamanishi", "Yoshihiro", "Araki", "Michihiro", "Gutteridge", "Alex", "Honda", "Wataru", "Kanehisa", "Minoru" ],
      "venue" : "i232–i240,",
      "citeRegEx" : "Yamanishi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Yamanishi et al\\.",
      "year" : 2008
    }, {
      "title" : "Gaussian process models for link analysis and transfer learning",
      "author" : [ "Yu", "Kai", "Chu", "Wei" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Yu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2008
    }, {
      "title" : "Semi-supervised learning using gaussian fields and harmonic functions",
      "author" : [ "Zhu", "Xiaojin", "Ghahramani", "Zoubin", "Lafferty", "John" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "However, most of the tensor methods do not explicitly model the internal graph structure for each type of objects, although some of those methods implicitly leverage such information via graph-based regularization terms in their objective function that encourage similar objects within each graph to share similar latent factors (Narita et al., 2012; Cai et al., 2011).",
      "startOffset" : 329,
      "endOffset" : 368
    }, {
      "referenceID" : 3,
      "context" : "However, most of the tensor methods do not explicitly model the internal graph structure for each type of objects, although some of those methods implicitly leverage such information via graph-based regularization terms in their objective function that encourage similar objects within each graph to share similar latent factors (Narita et al., 2012; Cai et al., 2011).",
      "startOffset" : 329,
      "endOffset" : 368
    }, {
      "referenceID" : 15,
      "context" : "To carry out transductive learning over Pκ (Task 2), we inject the structure of the product graph into f via a Gaussian random fields prior (Zhu et al., 2003).",
      "startOffset" : 140,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "Spectral approximation techniques for graphbased learning has been found successful in standard classification tasks (Fergus et al., 2009), which are special cases under our framework when J = 1.",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "Following (Duchi et al., 2011), we allow adaptive step sizes for each element in α.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "We evaluate our method on real-world data in two different domains: the Enzyme dataset (Yamanishi et al., 2008) for compound-protein interaction and the DBLP dataset of scientific publication records.",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "The graph of compounds is constructed based on the SIMCOMP score (Hattori et al., 2003), and the graph of proteins is constructed based on the normalized SmithWaterman score (Smith & Waterman, 1981).",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "As for the DBLP dataset, we use a subset of 34,340 DBLP publication records in the domain of Artificial Intelligence (Tang et al., 2008), from which 3 graphs are constructed as:",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "In GRTF, we further enhanced the traditional TF by adding graph regularizations to the objective function, which enforce the model to be aware of the context information inG’s (Narita et al., 2012; Cai et al., 2011);",
      "startOffset" : 176,
      "endOffset" : 215
    }, {
      "referenceID" : 3,
      "context" : "In GRTF, we further enhanced the traditional TF by adding graph regularizations to the objective function, which enforce the model to be aware of the context information inG’s (Narita et al., 2012; Cai et al., 2011);",
      "startOffset" : 176,
      "endOffset" : 215
    }, {
      "referenceID" : 2,
      "context" : "Tuples that are already in O are removed from T̄ to avoid misleading results (Bordes et al., 2013).",
      "startOffset" : 77,
      "endOffset" : 98
    } ],
    "year" : 2016,
    "abstractText" : "Cross-graph Relational Learning (CGRL) refers to the problem of predicting the strengths or labels of multi-relational tuples of heterogeneous object types, through the joint inference over multiple graphs which specify the internal connections among each type of objects. CGRL is an open challenge in machine learning due to the daunting number of all possible tuples to deal with when the numbers of nodes in multiple graphs are large, and because the labeled training instances are extremely sparse as typical. Existing methods such as tensor factorization or tensor-kernel machines do not work well because of the lack of convex formulation for the optimization of CGRL models, the poor scalability of the algorithms in handling combinatorial numbers of tuples, and/or the non-transductive nature of the learning methods which limits their ability to leverage unlabeled data in training. This paper proposes a novel framework which formulates CGRL as a convex optimization problem, enables transductive learning using both labeled and unlabeled tuples, and offers a scalable algorithm that guarantees the optimal solution and enjoys a linear time complexity with respect to the sizes of input graphs. In our experiments with a subset of DBLP publication records and an Enzyme multi-source dataset, the proposed method successfully scaled to the large cross-graph inference problem, and outperformed other representative approaches significantly.",
    "creator" : "LaTeX with hyperref package"
  }
}