{
  "name" : "1612.01663.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Non-oblivious Randomized Reduction for Risk Minimization with Improved Excess Risk Guarantee",
    "authors" : [ "Yi Xu", "Haiqin Yang", "Lijun Zhang", "Tianbao Yang" ],
    "emails" : [ "tianbao-yang}@uiowa.edu,", "hqyang@ieee.org,", "zhanglj@lamda.nju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 2.\n01 66\n3v 1\n[ cs\n.L G\n] 6\nD ec\n2 01"
    }, {
      "heading" : "Introduction",
      "text" : "Recently, the scale and dimensionality of data associated with machine learning and data mining applications have seen unprecedented growth, spurring the BIG DATA research and development. Learning from largescale ultrahigh-dimensional data remains a computationally challenging problem. The big size of data not only increases the memory footprint but also increases the computational costs pertaining to optimization. A popular approach for addressing the high-dimensionality challenge is\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nto perform dimensionality reduction. Nowadays, randomized reduction methods are emerging to be attractive for dimensionality reduction. Compared with traditional dimensionality reduction methods (e.g., PCA and LDA), randomized reduction methods (i) can lead to simpler algorithms that are easier to analyze (Mahoney 2011); (ii) can often be organized to exploit modern computational architectures better than classical dimensional reduction methods (Halko, Martinsson, and Tropp 2011); (iii) can be more efficient without loss in efficacy (Paul et al. 2013).\nGenerally, randomized reduction methods can be cast into two types: the first type of methods reduces a set of high-dimensional vectors into a low dimensional space independent of each other. These methods usually sample a random matrix independent of the data and then use it to reduce the dimensionality of the data. The second type of methods projects a set of vectors (in the form of a matrix) onto a subspace such that the original matrix can be well reconstructed from the projected matrix and the subspace. Therefore, the subspace to which the data is projected depends on the original data. These methods have been deployed for solving many numerical problems related to matrices, e.g., matrix product approximation, low-rank matrix approximation, approximate singular value decomposition (Boutsidis and Gittens 2013a; Halko, Martinsson, and Tropp 2011). To differentiate these two types of randomized reduction methods, we refer to the first type as oblivious randomized reduction, and refer to the second type as non-oblivious randomized reduction. We note that in literature oblivious and non-oblivious are used interchangeably with data-independent and data-dependent. Here, we use the terminology commonly appearing in matrix analysis and numerical linear algebra due to that the general excess risk bound depends on the matrix approximation error.\nHowever, we have not seen any comprehensive study on the statistical property (in particular the excess risk bound) of these randomized reduction methods applied to risk minimization in machine learning. The excess risk bound measures the generalization performance of a learned model compared to the optimal model from a class that has the best generalization performance. The excess risk bounds facilitate a better understanding of different learning algorithms and have the potential to guide us to design bet-\nter algorithms (Kukliansky and Shamir 2015). It is worth noting that several studies have been devoted to understanding the theoretical properties of oblivious randomized reduction methods applied to classification and regression problems. For example, (Blum 2005; Shi et al. 2012; Paul et al. 2013) analyzed the preservation of the margin of SVM based classification methods with randomized dimension reduction. (Zhang et al. 2014; Yang et al. 2015; Pilanci and Wainwright 2015) studied the problem from the perspective of optimization. Nonetheless, these results are limited in the sense that (i) they focus on only oblivious randomized reduction where the data is projected onto a random subspace independent of the data; (ii) they depend heavily on strong assumptions of the training data or the problem, e.g., low-rank of the data matrix, linear separability of training examples, or the sparsity of optimal solution, and (iii) some of these results do not directly carry over to the excess risk bounds.\nTo tackle the above challenges, we propose an algorithmic framework for employing non-oblivious randomized reduction (NOR) method to project the original high-dimensional features onto a random subspace that is derived from the original data. We study and establish the excess risk bound of the presented randomized algorithms for risk minimization. Different from previous results for oblivious randomized reduction methods, our theoretical analysis does not require assumptions of the training data or the problem, such as low-rank of the data matrix, linear separability of training examples, and the sparsity of optimal solution. When the data matrix is of low-rank or has a fast spectral decay, the excess risk bound of NOR is much better than that of oblivious randomized reduction based methods. Empirical studies on synthetic and real data sets corroborate the theoretical results and demonstrate the effectiveness of the proposed methods."
    }, {
      "heading" : "Related Work",
      "text" : "In literature, tremendous studies are devoted to nonoblivious randomized reduction in matrix applications. The focus of these studies is to establish matrix approximation error or the recovery error of the solution (e.g., in leastsquares regression). Few studies have examined their properties for risk minimization in machine learning. For oblivious randomized reduction methods, there exist some theoretical work trying to understand their impact on prediction performance (Blum 2005; Shi et al. 2012; Paul et al. 2013). This work differentiates from these studies in that we focus on the statistical property (the generalization property) of non-oblivious randomized reduction for expected risk minimization.\nWe employ tools in statistical learning theory to study the excess risk bounds of randomized reduction and results from randomized matrix theory to understand the order of the excess risk bound. A popular method for expected risk minimization is regularized empirical risk minimization (Vapnik 1998). The excess risk bounds of regularized empirical risk minimization have been well understood. In general, given a sample of size n it can achieve a risk bound of O(1/ √ n). Under some special conditions\n(e.g., low noise condition) this bound can be further improved (Bousquet, Boucheron, and Lugosi 2003). However, it is still not entirely clear what is the order of excess risk for learning from randomized dimensionality reduced data. The recovery result from (Zhang et al. 2014; Yang et al. 2015) could end up with an order of O(1/ √ m) excess risk for oblivious randomized reduction, where m is the reduced dimensionality. However, it relies on strong assumptions of the data. (Durrant and Kaban 2013) proved the generalization error of the linear classifier trained on randomly projected data by oblivious randomized reduction, which is upper bounded by the training error of the classifier learned in the original feature space by empirical risk minimization plus the VC-complexity in the projection space (proportional to O(1/ √ m) and plus terms depending on the average flipping probabilities on the training points defined as (1/n) ∑n i=1 Pr(sign(w T nA\n⊺Axi) 6= sign(w⊺nxi)), where wn is a model learned from the original data by empirical risk minimization. However, the order of the average flipping probabilities is generally unknown.\nRandom sampling (in particular uniform sampling) has been used in the Nyström method for approximating a big kernel matrix. There are some related work focusing on the statistical properties of the Nyström based kernel method (Yang et al. 2012; Bach 2013; Jin et al. 2013; Alaoui and Mahoney 2015). We note that the presented empirical risk minimization with non-oblivious randomized reduction using random sampling is similar to using the Nyström approximation on the linear kernel. However, in the present work besides random sampling, we also study other efficient randomized reduction methods using different random matrices. By leveraging recent results of these randomized reduction methods we are able to obtain better performance than using random sampling."
    }, {
      "heading" : "Preliminaries",
      "text" : "Let (x, y) denote a feature vector and a label that follow a distribution P = P(x, y), where x ∈ X ⊂ Rd and y ∈ Y . In the sequel, we will focus on Y = {+1,−1} and Y = R. However, we emphasize that the results are applicable to other problems (e.g., multi-class and multi-label classification). We denote by ℓ(z, y) a non-negative loss function that measures the inconsistency between a prediction z and the label y. Let w ∈ Rd, then by assuming a linear model z = w⊺x for prediction, the risk minimization problem in machine learning is to solve following problem:\nw∗ = arg min w∈Rd EP [ℓ(w ⊺ x, y)] (1)\nwhere EP [·] denotes the expectation over (x, y) ∼ P . Let A be an algorithm that learns an approximate solution wn from a sample of size n, i.e., {(x1, y1), . . . , (xn, yn)}. The excess risk of wn is defined as the difference between the expected risk of the solution wn and that of the optimal solution w∗:\nER(wn,w∗) = EP [ℓ(w⊺nx, y)]− EP [ℓ(w⊺∗x, y)] (2) A popular method for learning an approximate solution wn is based on regularized empirical risk minimization (ERM),\ni.e.,\nwn = arg min w∈Rd\n1\nn\nn∑\ni=1\nℓ(w⊺xi, yi) + λ\n2 ‖w‖22 (3)\nThe ERM problem is sometimes solved by solving its dual problem:\nα∗ = arg max α∈Rn − 1 n\nn∑\ni=1\nℓ∗i (αi)− 1\n2λn2 α⊺X⊺Xα (4)\nwhere α is usually called dual variable, ℓ∗i (α) = maxz αz− ℓ(z, yi) is the conjugate dual of the loss function, and X = (x1, . . . ,xn) ∈ Rd×n is the data matrix. With α∗, we have wn = − 1λnXα∗."
    }, {
      "heading" : "Oblivious Randomized Reduction",
      "text" : "In this section, we present an excess risk bound of oblivious randomized reduction building on previous theoretical results to facilitate the comparison with our result of non-oblivious randomized reduction. The idea of oblivious randomized reduction is to reduce a high-dimensional feature vector x ∈ Rd to a low dimensional vector by x̂ = Ax ∈ Rm, where A ∈ Rm×d is a random matrix that is independent of the data. A traditional approach is to use a Gaussian matrix with each entry independently sampled from a normal distribution with mean zero and variance 1/m (Dasgupta and Gupta 2003). Recently, many other types of random matrix A are proposed that lead to much more efficient computation of reduction, including subsampled randomized Hadamard transform (SRHT) (Boutsidis and Gittens 2013a) and random hashing (RH) (Kane and Nelson 2014). The key property of A that plays an important role in the analysis is that it should preserve the Euclidean length of a high-dimensional vector with a high probability, which is stated formally in JohnsonLindenstrauss (JL) lemma below.\nLemma 1 (JL Lemma). For any 0 < ǫ, δ < 1/2, there exists a probability distribution on matrices A ∈ Rm×d such that there exists a small universal constant c > 0 and for any fixed x ∈ Rd, with a probability at least 1− δ, we have\n∣∣‖Ax‖22 − ‖x‖22 ∣∣ ≤ c\n√ log(1/δ)\nm ‖x‖22\nThe key consequence of the JL lemma is that we can reduce a set of d-dimensional vectors into a low dimensional space with a reduced dimensionality independent of d such that the pairwise distance between any two points can be well preserved.\nGiven the JL transform A ∈ Rm×d, the problem can be imposed as,\nmin v∈Rm EP [ℓ(v ⊺Ax, y)] (5)\nPrevious studies have focused on using the ERM of the above problem\nmin v∈Rm\n1\nn\nn∑\ni=1\nℓ(v⊺x̂i, yi) + λ\n2 ‖v‖22 (6)\nto learn a model in the reduced feature space or using its dual solution to recover a model in the original high-dimensional space:\nα̂ = arg max α∈Rn − 1 n\nn∑\ni=1\nℓ∗i (αi)− 1\n2λn2 α⊺X̂⊺X̂α (7)\nwhere X̂ = (x̂1, . . . , x̂n) ∈ Rm×n. For example, (Zhang et al. 2014) proposed a dual recovery approach to recover a model in the original high-dimensional space that is close to the optimal solution wn in (3). The dual recovery approach consists of two steps (i) the first step obtains an approximate dual solution α̂ ∈ Rn by solving the dual problem in (7), and (ii) the second step recovers a highdimensional model by ŵn = − 1λnXα̂. By making a lowrank assumption of the data matrix, they established a recovery error ‖wn− ŵn‖2 in the order of O( √ r/m‖wn‖2), where r represents the rank of the data matrix. The theory has been generalized to full rank data matrix but with an additional assumption that the optimal primal solution wn or the optimal dual solution α∗ is sparse (Zhang et al. 2014; Yang et al. 2015). A similar order O( √ r/m‖wn‖2) of recovery error was established, where r represents the number of non-zero elements in the optimal solution. The proposition below exhibits the excess risk bound building on the recovery error.\nProposition 1. Suppose ℓ(z, y) is Lipschitz continuous and A is a JL transform. Let ŵn denote a recovered model by an ERM approach such that ‖wn−ŵn‖2 ≤ O( √ r/m‖wn‖2)."
    }, {
      "heading" : "Then",
      "text" : "ER(ŵn,w∗) , EP [ℓ(ŵ ⊺ nx, y)]− EP [ℓ(w⊺∗x, y)] ≤ O( √ r/m‖wn‖2 + 1/ √ n)\nRemark: In the above bound, we omit dependence on upper bound of the data norm ‖x‖ ≤ R. Although the synthesis of the proposition and previous recovery error analysis can give us guarantee on the excess risk bound, it relies on certain assumptions of the data, which may not hold in practice."
    }, {
      "heading" : "Non-Oblivious Randomized Reduction",
      "text" : "The key idea of non-oblivious randomized reduction is to compute a subspace Û ∈ Rd×m from the data matrix X ∈ Rd×n such that the projection of the data matrix to the subspace is close to the data matrix. To compute the subspace Û , we first sample a random matrix Ω ∈ Rn×m and compute Y = XΩ ∈ Rd×m. Then let Û be the left singular vector matrix of Y . This technique has been used in low-rank matrix approximation, matrix product approximation and approximate singular value decomposition (SVD) of a large matrix (Halko, Martinsson, and Tropp 2011). Various random matrices Ω can be used as long as the matrix approximation error defined below can be well bounded.\n‖X − Û Û⊺X‖2 = ‖X − PY X‖2 (8)\nwhere PY = Û Û⊺ denotes the projection to the subspace Û . We defer more discussions on different random matrices and\nAlgorithm 1 ERM with Non-Oblivious Randomized Reduction (NOR)\n1: Compute Y = XΩ ∈ Rd×m, where Ω ∈ Rn×m is a random subspace embedding matrix 2: Compute SVD of Y = ÛΣ̂V̂ ⊺, where Û ∈ Rd×m 3: Compute the reduced data by X̂ = Û⊺X ∈ Rm×n 4: Solve the reduced problem in Eqn. (9) 5: Output v̂n\ntheir impact on the excess risk bound to subsection “Matrix Approximation Error”.\nNext, we focus on the non-oblivious reduction defined by Û for risk minimization. Let x̂i = Û⊺xi denote the reduced feature vector and X̂ = Û⊺X ∈ Rm×n be the reduced data matrix. We propose to solve the following ERM problem:\nv̂n = arg min v∈Rm\n1\nn\nn∑\ni=1\nℓ(v⊺x̂i, yi) + λ\n2 ‖v‖22. (9)\nTo understand the non-oblivious randomized reduction for ERM, we first see that the problem above is equivalent to\nmin w=Ûv,v∈Rm\n1\nn\nn∑\ni=1\nℓ(w⊺xi, yi) + λ\n2 ‖w‖22.\ndue to that ‖Ûv‖2 = ‖v‖2. Compared to (3), we can see that the ERM with non-oblivious randomized reduction is restricting the model w to be Ûv. Since Û can capture the top column space of X due to the way it is constructed, and therefore the resulting model ŵn = Û v̂n is close to the top column space of X . Thus, we expect ŵn to be close to wn. The procedure is described in details in Algorithm 1. We note that the SVD in step 2 can be computed efficiently for sparse data. We defer the details into the appendix."
    }, {
      "heading" : "Excess Risk Bound",
      "text" : "Here, we show an excess risk bound of the proposed ERM with non-oblivious randomized reduction. The logic of the analysis is to first derive the optimization error of the approximate model ŵn = Û v̂n and then explore the statistical learning theory to bound the excess risk. In particular, we will show that the optimization error and consequentially the excess risk is bounded by the matrix approximation error in (8). To simplify the presentation, we introduce some notations:\nF (w) = 1\nn\nn∑\ni=1\nℓ(w⊺xi, yi) + λ\n2 ‖w‖2 (10)\nF̄ (w) = EP [ℓ(w ⊺ x, y)] +\nλ 2 ‖w‖22 (11)\nNext, we derive the optimization error of ŵn = Û v̂n.\nLemma 2. Suppose the loss function is G-Lipschitz continuous. Let ŵn = Û v̂n. We have\nF (ŵn) ≤ F (wn) + G2\n2λn ‖X − PY X‖22\nThe lemma below bounds the excess risk by the optimization error.\nLemma 3 (Theorem 1 (Sridharan et al. 2008)). Assume the loss function is G-Lipschitz continuous and ‖x‖2 ≤ R. Then, for any δ > 0 and any a > 0, with probability at least 1− δ, we have that for any w∗ ∈ Rd\nF̄ (ŵn)− F̄ (w∗) ≤ (1 + a)(F (ŵn)− F (wn))\n+ 8(1 + 1/a)G2R2(32 + log(1/δ))\nλn Using the lemma above and the the result in Lemma 2, we have the following theorem about the excess risk bound. Theorem 1. Assume the loss function is G-Lipschitz continuous and ‖x‖2 ≤ R. Then, for any δ > 0 and any a > 0, with probability at least 1− δ, we have that for any w∗ such that ‖w∗‖2 ≤ B\nER(ŵn,w∗) ≤ λB2\n2 +\nG2(1 + a)\n2λn ‖X − PY X‖22\n+ 8(1 + 1/a)G2R2(32 + log(1/δ))\nλn In particular, if we optimize λ over the R.H.S., we obtain\nER(ŵn,w∗) ≤ GB √ (1 + a)√ n ‖X − PY X‖2\n+ 4GRB √ (1 + 1/a)(32 + log(1/δ))√\nn\nRemark: Note that the above theorem bounds the excess risk by the matrix approximation error. Thus, we can leverage state-of-the-art results on the matrix approximation to study the excess risk bound. Importantly, future results about matrix approximation can be directly plugged into the excess risk bound. When the data matrix is of low rank r, then if m ≥ Ω(r log r) the matrix approximation error can be made zero (see below). As a result, the excess risk bound of ŵn is O(1/ √ n), the same to that of wn. In contrast, the excess risk bound in Proposition 1 of oblivious randomized reduction for ERM is O( √ r/m) for the dual recovery approach under the low rank assumption."
    }, {
      "heading" : "Matrix Approximation Error",
      "text" : "In this subsection, we will present some recent results on the matrix approximation error of four commonly used randomized reduction operators Ω ∈ Rn×m, i.e., random sampling (RS), random Gaussian (RG), subsampled randomized Hadamard transform (SRHT), and random hashing (RH), and discuss their impact on the excess risk bound. More details of these four randomized reduction operators can be found in (Yang et al. 2015). We first introduce some notations used in matrix approximation analysis. Let r ≤ min(n, d) denote the rank of X and k ∈ N+ such that 1 ≤ k ≤ r. We write the SVD of X ∈ Rd×n as X = U1Σ1V ⊺1 + U2Σ2V ⊺\n2 , where Σ1 ∈ Rk×k, Σ2 ∈ R(r−k)×(r−k), U1 ∈ R\nd×k, U2 ∈ Rd×(r−k), V1 ∈ Rn×k and V2 ∈ Rn×(r−k). We use σ1, σ2, . . . , σr to denote the singular values of X in the descending order. Let µk denote the coherence measure of V1 defined as µk = nk max1≤i≤n ∑k j=1[V1] 2 ij .\nTheorem 2 (RS (Gittens 2011)). Let Ω ∈ Rn×m be a random sampling matrix corresponding to sampling the columns of X uniformly at random with or without replacement. If for any ǫ ∈ (0, 1) and δ > 0, m satisfies m ≥ 2µk(1−ǫ)2k log kδ , then with a probability at least 1− δ,\n‖X − PY X‖2 ≤ √ 1 + n\nǫm σk+1\nRemark: The matrix approximation error using RS implies the excess risk bound of RS for risk minimization is dominated by O(σk+1√\nm ) provided m ≥ Ω(µkk log k),\nwhich is in the same order in terms of m to that in Proposition 1 of oblivious randomized reduction. However, random sampling is not guaranteed to work in oblivious randomized reduction since it does not satisfy the JL lemma in general (Yang et al. 2015) as required in Proposition 1. Moreover, if the data matrix is low rank such that m ≥ Ω(µrr log r), then the excess risk bound of NOR with RS is O(1/ √ n), the same order to that of wn learned from the original high-dimensional features.\nTheorem 3 (RG (Gittens and Mahoney 2013)). Let Ω ∈ R\nn×m be a random Gaussian matrix. If for any ǫ ∈ (0, 1) and k > 4, m satisfies m ≥ 2ǫ−2k log k, then with a probability at least 1− 2k−1 − 4k−k/ǫ2 ,\n‖X − PY X‖2 ≤O(σk+1) + O (\nǫ√ k log k )√∑ j>k σ2j\nRemark: We are interested in comparing the error bound of RG with that of RS. In the worse case, when the tail singular values are flat, then ‖X − PY X‖2 ≤ O( √ n mσk+1), which is in the same order to that of RS. However, if the tail\neigen-values decay fast such that √∑\nj>k σ 2 j ≪ √ nσk+1,\nthen the matrix approximation error could be much better than O( √ n mσk+1), and consequentially the excess risk\nbound could be much better than O(σk+1/ √ m) that is suffered by RS.\nTheorem 4 (SRHT (Boutsidis and Gittens 2013a)). Let Ω = √ n mDHP ∈ Rn×m be a SRHT with P ∈ Rn×m being a random sampling matrix, D ∈ Rn×n is a diagonal matrix with each entry sampled from {1,−1} with equal probabilities and H ∈ Rn×n is a normalized Hadamard transform. If for any 0 < ǫ < 1/3, 2 ≤ k ≤ r and δ ∈ (0, 1), m satisfies\n6C2ǫ−1[ √ k + √ 8 log(n/δ)]2 log(k/δ) ≤ m ≤ n,\nthen with a probability at least 1− 5δ,\n‖X − PY X‖2 ≤ ( 4 + √ 3 log(n/δ) log(r/δ)\nm\n) σk+1\n+\n√ 3 log(r/δ)\nm\n√∑ j>k σ2j\nwhere C is a universal constant.\nRemark: The order of the matrix approximation error of SRHT is similar to that of RG up to a logarithmic factor.\nFinally, we summarize the matrix approximation error of the RH matrix Ω. This has been studied in (Cohen, Nelson, and Woodruff 2015), in which RH is also referred to as sparse subspace embedding. We first describe the construction of random hashing matrix Ω. Let hk(i) : [n] → [m/s], k = 1, . . . , s denote s independent random hashing functions and let Ω = ((H1D1) ⊺, (H2D2) ⊺, . . . , (HsDs)\n⊺)⊺ ∈ Rm×n be a random matrix with a block of s random hashing matrices, where Dk ∈ Rn×n is a diagonal matrix with each entry sampled from {−1,+1} with equal probabilities, and Hk ∈ Rm/s,n with [Hk]j,i = δj,hk(i). The following theorem below summarizes the matrix approximation error using such a random matrix Ω.\nTheorem 5 (RH (Cohen, Nelson, and Woodruff 2015)). For any δ ∈ (0, 1) and ǫ ∈ (0, 1). If s = 1 and m = O(k/(ǫδ)) or s = O(log3(k/δ)/ √ ǫ) and m = O(k log6(k/δ)/ǫ), then with a probability 1− δ\n‖X − PY X‖2 ≤ (1 + √ ǫ)σk+1 +\n√ ǫ\nk √∑ j>k σ2j\nRemark: With the second choice of s and m, the order of the matrix approximation error of RH is similar to that of SRHT up to a logarithmic factor.\nTo conclude this section, we can see that the excess risk bound of the ERM with non-oblivious randomized reduction is dominated by O(1/ √ m) in the worst case, and could be much better than RG, SRHT and RH if the tail singular values decay fast."
    }, {
      "heading" : "Experiments",
      "text" : "In this section, we provide empirical evaluations in support of the proposed algorithms and the theoretical analysis. We implement and compare the following algorithms: (i) NOR: ERM with non-oblivious randomized reduction; (ii) previous ERM approaches with oblivious randomized reduction, including two dual recovery approaches, namely random projection with dual recovery (RPDR) (Zhang et al. 2014), and dual-sparse regularized randomized (DSRR) approach (Yang et al. 2015), and the pure random projection (RP) (Paul et al. 2013). We also implement and compare three randomized reduction operators for these different approaches, i.e., RH, RG and RS 1. For RH, we use only one block of random hashing matrix (i.e., s = 1). A similar result to Therorem 5 can be established for one-block of random hashing but with a constant success probability (Nelson and Nguyen 2012). The loss function for the binary classification problem is the hinge loss and for the multi-class classification problem is the softmax loss.\nExperiments are conducted on four real-world datasets and three synthetic datasets. The four real-world datasets are described in Table 1. To generate synthetic data, we first draw a random standard Gaussian matrix M ∈ Rd×n and\n1We do not report the performance of SRHT because it has similar performance to RH but it is less efficient than RH.\ncompute its SVD M = USV ⊺. Then we construct singular values following three different decay: an exponential decay (exp-τ ) with σi = e−iτ , (τ = 1) and polynomial decay (poly-τ ) with σi = i−τ , (τ = 0.5, 1). This will generate 3 synthetic datasets. We compute a base data matrix by Xb = √ nUΣV ⊺, where Σ = diag{σ1, . . . , σd}. Then the binary labels are computed by y = sign(X⊺b w), where w ∈ Rd is a standard Gaussian random vector. To increase the difficulty of the problem, we add some Gaussian random features to each data in Xb and form a full data matrix X ∈ R(d+t)×n. We use the first 90% examples as training data and the remaining 10% examples as testing data. In particular, we generate the synthetic datasets with d = 1000, n = 105, t = 10. We note that the synthetic data is not high-dimensional and it is solely for verifying the proposed approach and analysis. We perform data reduction to reduce features to the dimensionality of m = 100.\nWe first compare the performance of NOR, RPDR and DSRR with different randomized reduction operators on the synthetic datasets in order to verify the excess risk bounds established in Proposition 1 and subsection “Matrix Approximation Error” (MAE). The results are shown in Figure 1, where we also include the performance of SVM on the orig-\ninal features (denoted by Org). From the results, we can observe that (i) when the singular values follow an exponential decay (which yields almost low-rank data matrices), NOR performs almost the same to SVM on the original data; however the two recovery approaches RPDR and DSRR perform much worse than SVM, verifying our theoretical analysis in Proposition 1 and subsection “MAE”; (ii) The performance of NOR decreases gradually as the decay of singular values becomes slower, which is consistent with the theoretical results in subsection “MAE”; (iii) for NOR, RS is comparable to RH and RG when the decay of singular-values is fast, but is slightly worse when the decay of singular values becomes slower. This is also expected according to the discussions in subsection “MAE”; (iv) NOR always performs better than RPDR and DSRR. One reason that RPDR and DSRR do not perform well on these synthetic data sets is that the recovered dual solution is not accurate because that the data has noise. In contrast, NOR is much more robust to noise.\nSecondly, we present some experimental results on two binary classification real datasets, namely Reuters Text Categorization both for binary version (RCV1.b) and Splice Site Recognition (Splice), which are tested in previous studies (Sonnenburg and Franc 2010). For Splice dataset, we evaluate different algorithms by computing the same mea-\nsure, namely area under precision recall curve (auPRC), as in (Sonnenburg and Franc 2010). We compare NOR with RPDR, DSRR and RP. The results are shown in Figure 2. We can see that when m increases, the testing error/auPRC is monotonically decreasing/increasing. Comparing with other three algorithms, NOR has the best performance. In addition, RS does not work well for oblivious randomized reduction approaches (RP, RPDR and DSRR), but performs similarly to other randomized operators for NOR, which is consistent with our analysis (i.e., RS is not a JL transform as required in Proposition 1 for RPDR and DSRR; however, RS provides guarantee on the matrix approximation error that renders NOR work).\nFinally, we compare NOR with RP on RCV.m and News20 datasets for multi-class classification. The results are shown in Figure 3 (upper panel). We can see that NOR clearly outperforms RP. In addition, running time results 2 are reported in Figure 3 (lower panel). The running time consists of the reducation time and the optimization time in the reduced feture space. The results show that (i) NOR is more efficient than RP; (ii) RH and/or RS are much more efficient than RG for a certain approach. It is interesting to note that the total running time of NOR is less than RP. The reason is that the optimization of NOR is more efficient than RP 3 due to that the new data of NOR is better suited for classification (higher prediction performance), making the optimization easier, though NOR has slightly higher data reduction time than RP."
    }, {
      "heading" : "Conclusions",
      "text" : "In this paper, we have established the excess risk bound of non-oblivious randomized reduction method for risk minimization problems. More importantly, the new excess risk bound does not require stringent assumptions of the data and the loss functions, which is nontrivial and significant theoretical results. The empirical studies on synthetic datasets and real datasets validate our theoretical analysis and also demonstrate the effectiveness of the proposed non-oblivious randomized reduction approach."
    }, {
      "heading" : "Acknowlegements",
      "text" : "We thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are partially supported by National Science Foundation (IIS-1463988, IIS-1545995). L. Zhang is partially supported by NSFC (61603177) and JiangsuSF (BK20160658)."
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "Proof of Lemma 2",
      "text" : "Since the loss function is G-Lipschitz continuous, i.e., |ℓ′(z, y)| ≤ G, therefore, maxα∈Ω |α| ≤ G. Our proof is built on the dual formulation. First, we have\nF (wn) = max α∈Ωn − 1 n\nn∑\ni=1\nℓ∗i (αi)− 1 2λn2 α⊤X⊤Xα\nF (ŵn) = max α∈Ωn − 1 n\nn∑\ni=1\nℓ∗i (αi)− 1 2λn2 α⊤X⊤Û Û⊤Xα\nThen\nF (ŵn) = max α∈Ωn − 1 n\nn∑\ni=1\nℓ∗i (αi)− 1 2λn2 α⊤X⊤Û Û⊤Xα\n= max α∈Ωn − 1 n\nn∑\ni=1\nℓ∗i (αi)− 1 2λn2 α⊤X⊤Xα\n+ 1\n2λn2 α⊤(X⊤X −X⊤Û Û⊤X)α\n≤ max α∈Ωn − 1 n\nn∑\ni=1\nℓ∗i (αi)− 1 2λn2 α⊤X⊤Xα\n+ max α∈Ωn\n1\n2λn2 α⊤(X⊤X −X⊤Û Û⊤X)α\n≤ F (wn)\n+ 1\n2λn2 max α∈Ωn\n‖α‖22‖X⊤X −X⊤Û Û⊤X‖2\n≤ F (wn) + G2\n2λn ‖X⊤X −X⊤ÛÛ⊤X‖2\nOn the other hand, since PY = ÛÛ⊤ is the projection to the column space of Y = XΩ, we have\n‖X⊤X −X⊤ÛÛ⊤X‖2 = ‖X⊤X −X⊤PY X‖2 = ‖X⊤(I − PY )X‖2 = ‖X⊤(I − PY )2X‖2 (by the property of projection)\n= ‖X − PY X‖22 Combing above results together, we complete the proof."
    }, {
      "heading" : "Proof of Theorem 1",
      "text" : ""
    }, {
      "heading" : "Proof.",
      "text" : "ER(ŵn,w∗) = EP [ℓ(ŵ⊤n x, y)]− EP [ℓ(w⊤∗ x, y)]\n= [ F̄ (ŵn)− λ\n2 ‖ŵn‖22\n] − [ F̄ (w∗)− λ\n2 ‖w∗‖22\n]\n= F̄ (ŵn)− F̄ (w∗) + λ\n2 ‖w∗‖22 −\nλ 2 ‖ŵn‖22\n≤ F̄ (ŵn)− F̄ (w∗) + λ\n2 ‖w∗‖22\n≤ (1 + a)(F (ŵn)− F (wn))\n+ 8(1 + 1/a)G2R2(32 + log(1/δ))\nλn +\nλ 2 ‖w∗‖22\n(by Lemma 3)\n≤ G 2(1 + a)\n2λn ‖X − PY X‖22\n+ 8(1 + 1/a)G2R2(32 + log(1/δ))\nλn +\nλ 2 ‖w∗‖22\n(by Lemma 2)\n≤ G 2(1 + a)\n2λn ‖X − PY X‖22\n+ 8(1 + 1/a)G2R2(32 + log(1/δ))\nλn +\nλB2\n2\nThe last inequality is held because of ‖w∗‖22 ≤ B2."
    }, {
      "heading" : "Proof of Theorem 2",
      "text" : "Proof. Recall that X = U1Σ1V ⊤1 +U2Σ2V ⊤ 2 , and we have Y = XΩ = U1Σ1V ⊤ 1 Ω + U2Σ2V ⊤ 2 Ω. Let’s denote Ω1 = V ⊤1 Ω and Ω2 = V ⊤ 2 Ω, then\nY = XΩ = U1Σ1Ω1 + U2Σ2Ω2\nBased on the results from Theorem 9.1 of (Halko, Martinsson, and Tropp 2011), we have\n‖X − PY X‖22 ≤ ‖Σ2‖22 + ‖Σ2Ω2Ω†1‖22 (12) with a probability at least 1 − δ. In (Gittens 2011), Lemma 1 showed that\n‖Ω†1‖22 ≤ n\nεm\nif assume that Ω1 has full row rank. It is easy to show that ‖Ω2‖22 ≤ ‖V ⊤2 ‖22‖Ω‖22 ≤ 1. Then we have\n‖Σ2Ω2Ω†1‖22 ≤ n\nεm ‖Σ2‖22\nCombining this result with equation (12), we know that\n‖X − PY X‖22 ≤ (1 + n\nεm )‖Σ2‖22\ni.e.\n‖X − PY X‖2 ≤ √ (1 + n\nεm )σk+1\nwith a probability at least 1− δ."
    }, {
      "heading" : "Proof of Theorem 3",
      "text" : "Proof. Our proof is modified from (Gittens and Mahoney 2013). In Section 10 of (Halko, Martinsson, and Tropp 2011), it is showed that if m = k + p with p > 4 and u, t ≥ 1, and Σ2 is a diagonal matrix, then we have\n‖Σ2Ω2Ω†1‖2 ≤ ‖Σ2‖2 (√ 3k\np+ 1 t+\ne √ m p+ 1 tu\n)\n+ ‖Σ2‖F e √ m\np+ 1 t (13)\nwith probability at least 1 − 2t−p − e−u2/s. Since m ≥ 2ǫ−2k log k and m = k + p, we have that p ≥ ǫ−2k log k. Then, the following inequalities hold:\n√ 3k\np+ 1 ≤\n√ 3k\np ≤\n√ 3\nlog k ǫ\n√ m\np+ 1 ≤\n√ k + p\np ≤\n√ ǫ4\nk log2 k +\nǫ2\nk log k ≤\n√ 2\nk log k ǫ\nApply these inequalities and set t = e and u = √ 2 log k in (13), we can obtain\n‖Σ2Ω2Ω†1‖2 ≤ ‖Σ2‖2 ( e √ 3k\np+ 1 +\ne2 √ 2m log k\np+ 1\n)\n+ ‖Σ2‖F e2 √ m\np+ 1\n≤ ( e √ 3\nlog k + 2e2\n√ 1\nk\n) ǫ‖Σ2‖2\n+ e2 √ 2\nk log k ǫ‖Σ2‖F\nCombining this results into equation (12), we have\n‖X − PY X‖22 ≤ ‖Σ2‖22 + ‖Σ2Ω2Ω†1‖22\n≤ ‖Σ2‖22 + ( e √ 3\nlog k + 2e2\n√ 1\nk\n)2 ǫ2‖Σ2‖22\n+ e4 2\nk log k ǫ2‖Σ2‖2F\n≤  1 + ( e √ 3\nlog k + 2e2\n√ 1\nk\n)2 ǫ2  σ2k+1\n+ e4 2 k log k ǫ2 ∑\nj>k\nσ2j\nThus\n‖X − PY X‖2 ≤ √√√√1 + ( e √ 3\nlog k + 2e2\n√ 1\nk\n)2 ǫ2σk+1\n+ e2 √ 2\nk log k ǫ\n√∑\nj>k\nσ2j\n≤ O(σk+1) +O (\nǫ√ k log k\n)√∑\nj>k\nσ2j\nwith a probabilty at least 1− 2k−1 − 4k−k/ǫ2 ."
    }, {
      "heading" : "Proof of Theorem 4",
      "text" : "Proof. In (Boutsidis and Gittens 2013b), Lemma 4.1 showed that\n‖Ω†1‖22 ≤ (1− √ ǫ)−1\nwith probability at least 1 − 3δ. Comsequently, Ω1 has full row rank, and by applying Lemma 5.4 of (Boutsidis and Gittens 2013b) with the same probability, we obtain\n‖X − PY X‖22 ≤ ‖Σ2‖22 + (1− √ ǫ)−1‖Σ2V ⊤2 Ω‖22 (14)\nFrom Lemma 4.8 of (Boutsidis and Gittens 2013b) we have\n(1− √ ǫ)−1‖Σ2V ⊤2 Ω‖22 ≤\n5\n1−√ǫ‖Σ2V ⊤ 2 ‖22\n+ log(r/δ)\n(1−√ǫ)m (‖Σ2V ⊤ 2 ‖F +\n√ 8 log(n/δ)‖Σ2V ⊤2 ‖2)2\nwith probability at least 1−5δ. Since 0 < ǫ < 1/3, then (1−√ ǫ)−1 < 3. Also ‖Σ2V ⊤2 ‖2 = ‖Σ2‖2 and ‖Σ2V ⊤2 ‖F = ‖Σ2‖F . Thus, (1 − √ ǫ)−1‖Σ2V ⊤2 Ω‖22 ≤ 15‖Σ2‖22\n+ 3 log(r/δ)\nm (‖Σ2‖F +\n√ 8 log(n/δ)‖Σ2‖2)2\nPlugging this equation into (14),\n‖X − PY X‖22 ≤ 16‖Σ2‖22+ 3 log(r/δ)\nm (‖Σ2‖F +\n√ 8 log(n/δ)‖Σ2‖2)2\nUse the subadditivity of the square-root function to obtain that\n‖X − PY X‖2 ≤ ( 4 + √ 3 log(n/δ) log(r/δ)\nm\n) σk+1\n+\n√ 3 log(r/δ)\nm\n√∑\nj>k\nσ2j\nwith probability at least 1− 5δ."
    }, {
      "heading" : "Proof of Theorem 5",
      "text" : "Proof. Recall that X = U1Σ1V ⊤1 + U2Σ2V ⊤ 2 and Y = XΩ = Û Σ̂V̂ ⊤. Applying the SVD of PY X = ÛkΣ̂kV̂ ⊤k + Ûk̄Σ̂k̄V̂ ⊤ k̄\nto Theorem 4 of (Cohen, Nelson, and Woodruff 2015), we have\n‖X − ÛkΣ̂kV̂ ⊤k ‖22 ≤ (1 + ǫ)‖X − U1Σ1V ⊤1 ‖22 + ǫ\nk ‖X − U1Σ1V ⊤1 ‖2F\n= (1 + ǫ)‖Σ2‖22 + ǫ\nk ‖Σ2‖2F\nwith probability at least 1− δ. Then we have\n‖X − PY X‖22 = ‖X − ÛkΣ̂kV̂ ⊤k − Ûk̄Σ̂k̄V̂ ⊤k̄ ‖22 ≤ ‖X − ÛkΣ̂kV̂ ⊤k ‖22 + ‖Ûk̄Σ̂k̄V̂ ⊤k̄ ‖22 ≤ ‖X − ÛkΣ̂kV̂ ⊤k ‖22 ≤ (1 + ǫ)‖Σ2‖22 + ǫ\nk ‖Σ2‖2F\nWe complete the proof by using the subadditivity of the square-root function, i.e. we have\n‖X − PY X‖2 ≤ √ (1 + ǫ)‖Σ2‖2 +\n√ ǫ\nk ‖Σ2‖F\n≤ (1 + √ ǫ)σk+1 +\n√ ǫ\nk\n√∑\nj>k\nσ2j\nwith probability at least 1− δ."
    }, {
      "heading" : "An Efficient Implementation of Computing Û in NOR for Sparse Data",
      "text" : "In this section, we present an efficient implementation of NOR for sparse data. We note that when data is sparse, the time complexity of calculating Û⊤X is O(mN), where N ≪ dn is the number of non-zero elements in X . Consequently, computing the left singular vectors of Y could become a significant component of overall computation in Algorithm 2. To harness data sparsity, next we present a fast implementation of Û computation. Let Y = Û Σ̂V̂ ⊤ be the SVD of Y , then V̂ Σ̂2V̂ ⊤ is the singular value decomposition of Km = Y ⊤Y ∈ Rm×m. Then, the projection matrix Û⊤ can be computed by\nÛ⊤ = Σ̂−1V̂ ⊤Y ⊤ (15)\nAlgorithm 2 Fast Projection in NOR\n1: Compute Km = Y ⊤Y ∈ Rm×m 2: Compute eigen-values λi and corresponding eigen-vectors\nVi(i = 1, . . . ,m) of the small matrix Km 3: Let Σ̂ = diag( √ λ1, . . . , √ λm) 4: Compute the projection matrix Û⊤ by Eq. (15)\nTherefore, we can efficiently implement the projection matrix by first computing singular values and corresponding left singular vectors of the small matrix Km = Y ⊤Y ∈ Rm×m and then compute the projection matrix by Eq. (15). We assume the number of non-zero entries in X is N and the number of non-zero entries in Y is Nm. The time complexity consists of (i) O(mNm) for computing Km, (ii) O(m2 logm) for computing left singualr value decomposition of Km by randomized algorithms (Halko, Martinsson, and Tropp 2011), (iii) O(m2 + mNm) for computing Σ̂−1V̂ ⊤Y ⊤, yeilding an overall time complexity ofO(m2 logm+m2+2mNm). Compared to the overall time complexity of O(md logm) by directly computing the left singular vectors of Y , the efficient implementation could be much faster especially when Nm,m ≪ d. We present the detailed steps in Algorithm 2 for computing Û⊤."
    } ],
    "references" : [ {
      "title" : "Fast randomized kernel ridge regression with",
      "author" : [ "W. M" ],
      "venue" : null,
      "citeRegEx" : "M.,? \\Q2015\\E",
      "shortCiteRegEx" : "M.",
      "year" : 2015
    }, {
      "title" : "Introduction to statistical learning theory",
      "author" : [ "Boucheron Bousquet", "O. Lugosi 2003] Bousquet", "S. Boucheron", "G. Lugosi" ],
      "venue" : "In Advanced Lectures on Machine Learning,",
      "citeRegEx" : "Bousquet et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bousquet et al\\.",
      "year" : 2003
    }, {
      "title" : "Improved matrix algorithms via the subsampled randomized hadamard transform",
      "author" : [ "Boutsidis", "C. Gittens 2013a] Boutsidis", "A. Gittens" ],
      "venue" : "SIAM J. Matrix Analysis Applications 34(3):1301–1340",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2013
    }, {
      "title" : "Improved matrix algorithms via the subsampled randomized hadamard transform",
      "author" : [ "Boutsidis", "C. Gittens 2013b] Boutsidis", "A. Gittens" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications 34(3):1301–1340",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2013
    }, {
      "title" : "D",
      "author" : [ "M.B. Cohen", "J. Nelson", "Woodruff" ],
      "venue" : "P.",
      "citeRegEx" : "Cohen. Nelson. and Woodruff 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Gupta",
      "author" : [ "S. Dasgupta" ],
      "venue" : "A.",
      "citeRegEx" : "Dasgupta and Gupta 2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "and Kaban",
      "author" : [ "R.J. Durrant" ],
      "venue" : "A.",
      "citeRegEx" : "Durrant and Kaban 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Mahoney",
      "author" : [ "A. Gittens" ],
      "venue" : "M.",
      "citeRegEx" : "Gittens and Mahoney 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "J",
      "author" : [ "N. Halko", "P.-G. Martinsson", "Tropp" ],
      "venue" : "A.",
      "citeRegEx" : "Halko. Martinsson. and Tropp 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Improved bounds for the nyström method with application to kernel classification",
      "author" : [ "Jin" ],
      "venue" : "IEEE Transactions on Information Theory 59(10):6939–6949",
      "citeRegEx" : "Jin,? \\Q2013\\E",
      "shortCiteRegEx" : "Jin",
      "year" : 2013
    }, {
      "title" : "and Nelson",
      "author" : [ "D.M. Kane" ],
      "venue" : "J.",
      "citeRegEx" : "Kane and Nelson 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and Shamir",
      "author" : [ "D. Kukliansky" ],
      "venue" : "O.",
      "citeRegEx" : "Kukliansky and Shamir 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "M",
      "author" : [ "Mahoney" ],
      "venue" : "W.",
      "citeRegEx" : "Mahoney 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "H",
      "author" : [ "J. Nelson", "Nguyen" ],
      "venue" : "L.",
      "citeRegEx" : "Nelson and Nguyen 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Random projections for support vector machines",
      "author" : [ "Paul" ],
      "venue" : null,
      "citeRegEx" : "Paul,? \\Q2013\\E",
      "shortCiteRegEx" : "Paul",
      "year" : 2013
    }, {
      "title" : "M",
      "author" : [ "M. Pilanci", "Wainwright" ],
      "venue" : "J.",
      "citeRegEx" : "Pilanci and Wainwright 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Is margin preserved after random projection",
      "author" : [ "Shi" ],
      "venue" : null,
      "citeRegEx" : "Shi,? \\Q2012\\E",
      "shortCiteRegEx" : "Shi",
      "year" : 2012
    }, {
      "title" : "and Franc",
      "author" : [ "S. Sonnenburg" ],
      "venue" : "V.",
      "citeRegEx" : "Sonnenburg and Franc 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fast rates for regularized objectives",
      "author" : [ "Sridharan" ],
      "venue" : null,
      "citeRegEx" : "Sridharan,? \\Q2008\\E",
      "shortCiteRegEx" : "Sridharan",
      "year" : 2008
    }, {
      "title" : "V",
      "author" : [ "Vapnik" ],
      "venue" : "N.",
      "citeRegEx" : "Vapnik 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Nyström method vs random fourier features: A theoretical and empirical comparison",
      "author" : [ "Yang" ],
      "venue" : null,
      "citeRegEx" : "Yang,? \\Q2012\\E",
      "shortCiteRegEx" : "Yang",
      "year" : 2012
    }, {
      "title" : "Theory of dual-sparse regularized randomized reduction",
      "author" : [ "Yang" ],
      "venue" : null,
      "citeRegEx" : "Yang,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang",
      "year" : 2015
    }, {
      "title" : "Random projections for classification: A recovery approach",
      "author" : [ "Zhang" ],
      "venue" : "IEEE Transactions on Information Theory 60(11):7300–7316",
      "citeRegEx" : "Zhang,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In this paper, we address learning problems for high dimensional data. Previously, oblivious random projection based approaches that project high dimensional features onto a random subspace have been used in practice for tackling highdimensionality challenge in machine learning. Recently, various non-oblivious randomized reduction methods have been developed and deployed for solving many numerical problems such as matrix product approximation, low-rank matrix approximation, etc. However, they are less explored for the machine learning tasks, e.g., classification. More seriously, the theoretical analysis of excess risk bounds for risk minimization, an important measure of generalization performance, has not been established for non-oblivious randomized reduction methods. It therefore remains an open problem what is the benefit of using them over previous oblivious random projection based approaches. To tackle these challenges, we propose an algorithmic framework for employing non-oblivious randomized reduction method for general empirical risk minimizing in machine learning tasks, where the original high-dimensional features are projected onto a random subspace that is derived from the data with a small matrix approximation error. We then derive the first excess risk bound for the proposed non-oblivious randomized reduction approach without requiring strong assumptions on the training data. The established excess risk bound exhibits that the proposed approach provides much better generalization performance and it also sheds more insights about different randomized reduction approaches. Finally, we conduct extensive experiments on both synthetic and real-world benchmark datasets, whose dimension scales to O(10), to demonstrate the efficacy of our proposed approach. Introduction Recently, the scale and dimensionality of data associated with machine learning and data mining applications have seen unprecedented growth, spurring the BIG DATA research and development. Learning from largescale ultrahigh-dimensional data remains a computationally challenging problem. The big size of data not only increases the memory footprint but also increases the computational costs pertaining to optimization. A popular approach for addressing the high-dimensionality challenge is Copyright c © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. to perform dimensionality reduction. Nowadays, randomized reduction methods are emerging to be attractive for dimensionality reduction. Compared with traditional dimensionality reduction methods (e.g., PCA and LDA), randomized reduction methods (i) can lead to simpler algorithms that are easier to analyze (Mahoney 2011); (ii) can often be organized to exploit modern computational architectures better than classical dimensional reduction methods (Halko, Martinsson, and Tropp 2011); (iii) can be more efficient without loss in efficacy (Paul et al. 2013). Generally, randomized reduction methods can be cast into two types: the first type of methods reduces a set of high-dimensional vectors into a low dimensional space independent of each other. These methods usually sample a random matrix independent of the data and then use it to reduce the dimensionality of the data. The second type of methods projects a set of vectors (in the form of a matrix) onto a subspace such that the original matrix can be well reconstructed from the projected matrix and the subspace. Therefore, the subspace to which the data is projected depends on the original data. These methods have been deployed for solving many numerical problems related to matrices, e.g., matrix product approximation, low-rank matrix approximation, approximate singular value decomposition (Boutsidis and Gittens 2013a; Halko, Martinsson, and Tropp 2011). To differentiate these two types of randomized reduction methods, we refer to the first type as oblivious randomized reduction, and refer to the second type as non-oblivious randomized reduction. We note that in literature oblivious and non-oblivious are used interchangeably with data-independent and data-dependent. Here, we use the terminology commonly appearing in matrix analysis and numerical linear algebra due to that the general excess risk bound depends on the matrix approximation error. However, we have not seen any comprehensive study on the statistical property (in particular the excess risk bound) of these randomized reduction methods applied to risk minimization in machine learning. The excess risk bound measures the generalization performance of a learned model compared to the optimal model from a class that has the best generalization performance. The excess risk bounds facilitate a better understanding of different learning algorithms and have the potential to guide us to design better algorithms (Kukliansky and Shamir 2015). It is worth noting that several studies have been devoted to understanding the theoretical properties of oblivious randomized reduction methods applied to classification and regression problems. For example, (Blum 2005; Shi et al. 2012; Paul et al. 2013) analyzed the preservation of the margin of SVM based classification methods with randomized dimension reduction. (Zhang et al. 2014; Yang et al. 2015; Pilanci and Wainwright 2015) studied the problem from the perspective of optimization. Nonetheless, these results are limited in the sense that (i) they focus on only oblivious randomized reduction where the data is projected onto a random subspace independent of the data; (ii) they depend heavily on strong assumptions of the training data or the problem, e.g., low-rank of the data matrix, linear separability of training examples, or the sparsity of optimal solution, and (iii) some of these results do not directly carry over to the excess risk bounds. To tackle the above challenges, we propose an algorithmic framework for employing non-oblivious randomized reduction (NOR) method to project the original high-dimensional features onto a random subspace that is derived from the original data. We study and establish the excess risk bound of the presented randomized algorithms for risk minimization. Different from previous results for oblivious randomized reduction methods, our theoretical analysis does not require assumptions of the training data or the problem, such as low-rank of the data matrix, linear separability of training examples, and the sparsity of optimal solution. When the data matrix is of low-rank or has a fast spectral decay, the excess risk bound of NOR is much better than that of oblivious randomized reduction based methods. Empirical studies on synthetic and real data sets corroborate the theoretical results and demonstrate the effectiveness of the proposed methods.",
    "creator" : "LaTeX with hyperref package"
  }
}