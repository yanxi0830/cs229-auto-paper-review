{
  "name" : "1311.2854.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Approximate Spectral Clustering via Randomized Sketching",
    "authors" : [ "Alex Gittens", "Prabhanjan Kambadur", "Christos Boutsidis" ],
    "emails" : [ "gittens@caltech.edu", "pkambadu@us.ibm.com", "cboutsi@us.ibm.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Consider clustering the points in Figure 1. The data in this space are non-separable: there is no readily apparent clustering metric which can be optimized over in order to recover these particular clustering structures. In particular, in both cases, the two clusters have the same centers (centroids); hence, distance-based clustering methods such as k-means [32] will fail miserably.\nMotivated by shortcomings such as these, researchers have produced a body of more flexible and dataadaptive clustering approaches, known under the umbrella of Spectral Clustering. The crux of all these approaches is that the points to be clustered are viewed as vertices on a graph, and the weights of the edges between two vertices are assigned according to some similarity measure between the points. A new, hopefully separable, representation for the points is then formed by using the eigenvectors of a Laplacian matrix associated with the similarity graph as the coordinates for the points. We refer the reader to Section 9 in [45] for a discussion on the history of spectral clustering. The first reports in the literature go back to [13, 16]. According to [45], “since then spectral clustering has been discovered, re-discovered, and extended many times”. Shi and Malik brought spectral clustering to the machine learning community in their seminal work on Normalized Cuts and Image Segmentation [41] (see Section 2). Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].\nThe computational bottleneck in spectral clustering is the computation of the eigenvectors of the Laplacian matrix. Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46]. Algorithms from the theory community admit theoretical guarantees, but are complex to describe and implement. On the other hand, algorithms from the machine learning community are empirically sound, but the corresponding theoretical analysis is lacking.\nWe bridge this gap by describing a simple approximation algorithm for spectral clustering with strong theoretical and empirical performance. Our algorithm approximates the key eigenvectors of the Laplacian matrix of the graph representing the data by using a truncated subspace iteration process with a Gaussian random initialization. We show, both in theory and in practice, that the running time of the suggested algorithm is sub-cubic and that it finds clusterings that are as good as those obtained using standard approaches.\n∗Part of this work was done while the author was with IBM Research during summer 2012.\nar X\niv :1\n31 1.\n28 54\nv1 [\ncs .L\nG ]\n1 2\nN ov\n2 01"
    }, {
      "heading" : "2 The spectral clustering problem: the normalized cuts point of",
      "text" : "view\nFirst we review one mathematical formulation of spectral clustering. Denote data points x1,x2, . . . ,xn in d-dimensional space. The goal of clustering is to cluster those points into k disjoint sets. Towards this end, define a weighted undirected graph G(V,E) with |V | = n nodes and |E| edges as follows: each node in G corresponds to a point xi (i = 1 : n) in the data; the weight of each edge encodes the similarity between the corresponding two nodes. The similarity matrix W ∈ Rn×n whose (i, j)th entry gives the similarity between the two corresponding points is,\nWij = e − ‖xi−xj‖\n2\nσ\n(i 6= j), where σ is a tuning parameter, and Wii = 0. Given this setup, the spectral clustering problem for k = 2 can be viewed as a graph partitioning problem (the definition below admits an easy generalization when k > 2).\nDefinition 1 (The Spectral Clustering Problem [41]). Let x1,x2, . . . ,xn ∈ Rd are given. Construct graph G(V,E) as described in the text above. Find subgraphs of G, denoted as A and B, to minimize the objective\nNcut(A,B) = cut(A,B)\nassoc(A, V ) +\ncut(A,B)\nassoc(B, V ) ,\nwhere cut(A,B) = ∑ xi∈A,xj∈B Wij ; assoc(A, V ) = ∑ xi∈A,xj∈V Wij , assoc(B, V ) = ∑ xi∈B,xj∈V Wij .\nMinimizing the normalized cut objective Ncut(.) in a weighted undirected graph is an NP-Complete problem (see the appendix in [41] for the proof). Motivated by this hardness result, Shi and Malik [41] suggested a relaxation to the problem that is tractable in polynomial time through the SVD. First, [41] shows that for any G,A,B and partition vector y ∈ Rn with +1 to the entries corresponding to A and −1 to the entries corresponding to B it is: 4 · Ncut(A,B) = yT (D −W)y/(yTDy). Here, D ∈ Rn×n is the diagonal matrix of degree nodes: Dii = ∑ j Wij . Hence, the spectral clustering problem in Definition 1 can be restated as finding such an optimum partition vector y. The real relaxation for spectral clustering asks for a real-valued y ∈ Rn minimizing the same objective:\nDefinition 2 (The Real Relaxation for the Spectral Clustering Problem [41]). Given graph G with n nodes, adjacency matrix W, and degrees matrix D find y: y = argminy∈Rn,yTD1n yT (D−W)y yTDy .\nOnce such a y is found, we partition the graph into 2 subgraphs by looking at the signs of the elements in y. When k > 2, we first compute k eigenvectors and then apply k-means clustering on the rows of the matrix containing those eigenvectors (see discussion below)."
    }, {
      "heading" : "2.1 An algorithm for spectral clustering",
      "text" : "Motivated by the above observations, which are due to [41], Ng, Jordan, and Weis [30] (see also [47]) suggested the following algorithm for spectral clustering 1, which we consider as the ground-truth algorithm. The input to this spectral clustering algorithm is a set of points x1,x2, . . . ,xn ∈ Rd and the desired number of clusters k. The output is a disjoint partition of the points into k clusters.\nExact Spectral Clustering\n1. Construct the similarity matrix W ∈ Rn×n as Wij = e− ‖xi−xj‖\n2\nσ (for i 6= j) and Wii = 0. 2. Construct D ∈ Rn×n as the diagonal matrix of degree nodes: Dii = ∑ j Wij .\n3. Construct W̃ = D− 1 2 WD− 1 2 ∈ Rn×n.\n4. Find the largest k eigenvectors of W̃ and assign them as columns to a matrix Y ∈ Rn×k.\n5. Apply k-means clustering on the rows of Y, and cluster the original points accordingly.\nIn a nutshell, compute the top k eigenvectors of W̃ and then apply k-means on the rows of the matrix containing those eigenvectors."
    }, {
      "heading" : "3 Main result",
      "text" : "We now describe our algorithm for approximate spectral clustering. Our main idea is to replace step 4 in the above ground-truth algorithm with a procedure that approximates the top k eigenvectors of W̃. Specifically, we use random projections along with the power iteration. This method for approximating the eigenvectors of matrices is not new: it is a classical technique for eigenvector approximation, known as subspace iteration in the numerical linear algebra literature (see Section 8.2.4 in [18]). The linear algebra community is characteristically concerned with obtaining results with as high numerical precision as feasible, so when subspace iteration is applied, the stopping condition usually depends on a priori tests of accuracy rather than a fixed number of iterations. Recently, researchers have realized the benefits of truncating these classical iterative processes after a small number of iterations in large-dimensional settings where each iteration may be quite expensive. The survey [20] gives an overview of these truncated processes in the context of low-rank matrix approximation. Our contribution is the application of this idea of truncating the iterative process of subspace iteration to spectral clustering and a novel analysis that shows that clusterings are preserved remarkably well.\nApproximate Spectral Clustering\n1. Construct the similarity matrix W ∈ Rn×n as Wij = e− ‖xi−xj‖\n2\nσ (for i 6= j) and Wii = 0. 2. Construct D ∈ Rn×n as the diagonal matrix of degree nodes: Dii = ∑ j Wij .\n3. Construct W̃ = D− 1 2 WD− 1 2 ∈ Rn×n.\n4. Let Ỹ ∈ Rn×k contain the left singular vectors of\nB = (W̃W̃ T )pW̃S,\nwith p ≥ 0, and S ∈ Rn×k being a matrix with i.i.d random Gaussian variables.\n5. Apply k-means clustering on the rows of Ỹ, and cluster the original data points accordingly.\n1Precisely, they suggested one more normalization step on Y before applying k-means, i.e., normalize Y to unit row norms, but we ignore this step for simplicity.\nStep 4 can be implemented in O(n2kp + k2n) time, as we need O(n2kp) time to implement all the matrixmatrix multiplications (right-to-left) and another O(k2n) time to find Ỹ. As we discuss in the theorem below, selecting p ≈ ln(kn) and assuming that the multiplicative spectral gap of the kth to the (k + 1)th eigenvalue of W̃ is large suffices to get very accurate clusterings. This leads to an O(kn2 ln(kn)) runtime for this step, which is a considerable improvement over the O(n3) time of the standard approach (see, for example, Section 8.3 in [18]).\nThere are several ways to define the goodness of an approximation algorithm for spectral clustering (see Section 4). Motivated by the work in [22] we adapt the following approach: we assume that if, for two matrices U ∈ Rn×k and V ∈ Rn×k, both with orthonormal columns, ‖U −V‖2 ≤ ε, for some arbitrarily small ε > 0, then clustering the rows of U and the rows of V with the same method should result to the same clustering. Notice that if ‖U−V‖2 is small, then the distance of each row of U to the corresponding row of V is also small, i.e., for all i = 1 : n, let uTi ,v T i ∈ R1×k be the ith rows of U and V, then,\n‖ui − vi‖2 ≤ ‖U−V‖2 ≤ ε.\nHence, a distance-based algorithm such as k-means [32] would lead to the same clustering as ε→ 0 (this is equivalent to saying that k-means is robust to small perturbations to the input).\nGiven this assumption, we argue that the clustering that is obtained by applying k-means on Ỹ ∈ Rn×k is similar to the one obtained by applying k-means on Y ∈ Rn×k. The following theorem is proved in Section 6.\nTheorem 3. Given orthonormal matrices Y and Ỹ in Rn×k, if for some ε > 0 and δ > 0 we choose\np ≥ 1 2\nln(4ε−1δ−1 √ k(n− k)) ln−1  σk ( W̃ )\nσk+1\n( W̃ )  ,\nthen there is an orthonormal matrix Q ∈ Rk×k such that with probability at least 1− e−n − 2.35δ,\n‖Y − ỸQ‖22 ≤ ε2\n1 + ε2 = O(ε2).\nCorollary 4. As ε→ 0, the rows of Ỹ and Y are clustered identically with the k-means method.\nProof. The rows of ỸQ and Ỹ are clustered identically since Q maps Ỹ to a set of coordinates that is almost only rotated (see Fig. 2), by the previous theorem. Thus the rows of Ỹ and Y admit the same clustering, as ε→ 0."
    }, {
      "heading" : "4 Comparison with related work",
      "text" : "The Hunter and Strohmer result [22]. The result that is most relevant to ours appeared in [22]. Firstly, we follow the exact same framework for measuring the approximability of an algorithm for spectral clustering (see Theorem 5 and Corollary 6 in [22]). Hunter and Strohmer do not suggest a specific approximation algorithm; however, they do provide a general theory for approximate spectral clustering under perturbations to the eigenvectors used for the clustering (this theory generalizes other similar perturbation bounds that we disuse below). Specifically, they prove a weaker version of Theorem 7 of our work (see the proof of Theorem 5 in [22]). Putting their bound in our notation translates to the fact that there exists an orthonormal matrix Q ∈ Rk×k such that,\n‖Y − ỸQ‖2 ≤ ‖PY −PỸ‖2 + ‖Y T Ỹ‖2.\nSpectral Clustering via Random Projections. The most relevant algorithmic result to ours is [37]. The authors suggest to reduce the dimension of the data points with random projections before forming the similarity matrix W. Recall that we apply random projections to a scaled version of W. No theoretical results are reported for this approach.\nPower Iteration Clustering. Another clustering method similar to ours is in [26]. The authors use the power iteration, as we do, on W̃ but choose S with just one column. Then, they apply k-means to the resulted vector (even if k > 2). No theoretical convergence results are proven for this technique.\nPerturbation bounds for Spectral Clustering. Define the mis-clustering rate ρ as the number of incorrectly classified data points divided by the total number of points. Suppose L̃ = L + E, where L and L̃ are the Laplacian matrices of two graphs G and G̃, respectively. G is the graph corresponding to the input points that we want to cluster; G̃ is some other graph, close to G. Assume k = 2 and that the clustering is obtained by looking at the signs of the computed eigenvector. Denote by ṽi and vi the ith (smallest) eigenvectors of L̃ and L, respectively. Then, [48, 21, 44] show the following bound: ρ ≤ ‖ṽ2 − v2‖2 ≤ (λ2 − λ3)−1 ‖E‖2 +O(‖E‖2). This bound depends on the “additive” gap of two eigenvalues of the Laplacian matrix. Compare this to our bound in Theorem 3, where we replace this with the “multiplicative gap”.\nThe Spielman-Teng iterative algorithm. Motivated by a result of Mihail [29], Spielman and Teng [43] suggest the following definition for an approximate Fiedler vector: For a Laplacian matrix L and 0 < ε < 1, v ∈ Rm is an ε-approximate Fiedler vector if v is orthogonal to the all-ones vector and vLvvTv ≤ (1 + ε) fLf fTf = (1 + )λn−1(L). Here f ∈ Rm denotes the Fiedler vector (the eigenvector of L corresponding to the second smallest eigenvalue). Theorem 6.2 in [43] gives a randomized algorithm to compute such an ε-approximate Fiedler vector w.p. 1− δ, in time O ( nnz(L) ln27(nnz(L)) ln(1δ ) ln( 1 ε ) −1) . This method is based on the fast solvers for linear systems with Laplacian matrices developed in the same paper. Though this is a very strong theoretical result, we are not aware of any efficient implementation.\nOther approximation algorithms. Yan et al. [48] proposed an algorithm where one first applies k-means to the data to find the so-called centroids and then applies spectral clustering on the centroids and uses this to cluster all the points. Instead of using these centroids, one can select a small set of points from the original points (coreset), apply spectral clustering there and then extend this (e.g. with a nearest neighbor based approach) to all the points [35, 5, 46]. No theoretical results are known for all these approaches. The previous two methods, reduce the dimension of the clustering problem by subsampling data points. Alternatively, one can subsample the similarity matrix W, i.e., use a submatrix of W and hope that it approximates the entire matrix W well. This is known as the Nyström method [31, 3, 17]. The Nyström method relies on a submatrix to approximate the entire similarity matrix W. There, one usually samples blocks or rows/columns at a time. Alternatively, one can sample the entries themselves. This is the approach of the spectral clustering on a budget method [40]. The aim is to randomly select b different entries in the matrix (for some budget constraint b) and store only those. The remaining entries are set to zero, enforcing the approximation to be a sparse matrix whose eigenvectors can be computed efficiently by some iterative eigenvalue solver. For the suggested algorithm, ρ . (λ2(L)− λ3(L))−1 (n 5 3 b− 2 3 + n 3 2 b− 1 2 ), where L is the graph Laplacian matrix.\nRandomized Numerical Linear Algebra (RandNLA). The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34]. (viii) Canonical Correlation Analysis (CCA) [1]. Our algorithm is similar to many of those methods in regard to reducing the dimensions of the input matrix with random projections. However, to prove the theoretical behavior of the algorithm we derive results that could be of independent interest. The main novel technical ingredient is that we show that the difference between Y and an orthonormal transformation of Ỹ is bounded (see Theorem 3). Previous work analyzes the approximation error after projecting W̃ on PỸ. Specifically, [20] argues that ‖W̃ −PỸW̃‖2 is small.\nSubspace Iteration. Finally, there are deep connections of our approach with a classical algorithm in the numerical linear algebra literature, known as “subspace iteration”. See for example Section 8.2.4 in [18]. Theorem 8.2.2 in [18] shows a bound similar to our bound in Theorem 10. We leave it as a topic of future work a detailed discussion of those connections."
    }, {
      "heading" : "5 Preliminaries",
      "text" : "A,B, . . . are matrices; a,b, . . . are column vectors. In is the n×n identity matrix; 0m×n is the m×n matrix of zeros; 1n is the n× 1 vector of ones. The Frobenius and the spectral matrix-norms are ‖A‖2F = ∑ i,j A 2 ij and ‖A‖2 = max‖x‖2=1 ‖Ax‖2, respectively. The SVD of A ∈ Rm×n of rank ρ ≤ min{m,n} is\nA = ( Uk Uρ−k )︸ ︷︷ ︸\nUA∈Rm×ρ\n( Σk 0 0 Σρ−k ) ︸ ︷︷ ︸\nΣA∈Rρ×ρ\n( VTk\nVTρ−k ) ︸ ︷︷ ︸\nVTA∈Rρ×n\n, (1)\nwith singular values σ1 (A) ≥ . . . ≥ σk (A) ≥ σk+1 (A) ≥ . . . ≥ σρ (A) > 0. The matrices Uk ∈ Rm×k and Uρ−k ∈ Rm×(ρ−k) contain the left singular vectors of A; and, similarly, the matrices Vk ∈ Rn×k and Vρ−k ∈ Rn×(ρ−k) contain the right singular vectors. Σk ∈ Rk×k and Σρ−k ∈ R(ρ−k)×(ρ−k) contain the singular values of A. Also, A+ = VAΣ −1 A U T A denotes the Moore-Penrose pseudo-inverse of A. For a symmetric positive definite matrix (SPSD) A = BBT , λi (A) = σ 2 i (B) denotes the i-th eigenvalue of A. Given a matrix A with m rows, PA ∈ Rm×m is the projector matrix that projects in the column span of A, i.e., for any matrix Z that is a basis for span(A), PA = ZZ\n+. For example, PA = UAU T A. For A,B,∥∥PA −PB∥∥2 = ∥∥(I−PA)PB∥∥2 = ∥∥(I−PB)PA∥∥2,"
    }, {
      "heading" : "5.1 Random Matrix Theory",
      "text" : "The main ingredient in our approximate spectral clustering algorithm is to reduce the dimensions of W̃ via post-multiplication with a random Gaussian matrix. Here, we summarize two properties of such matrices that are useful in proving our main results.\nLemma 5 (The norm of a random Gaussian Matrix [11]). Let A ∈ Rn×m be a matrix with i.i.d. standard Gaussian random variables, where n ≥ m. Then, for every t ≥ 4,\nP{σ1(A) ≥ tn 1 2 } ≥ e−nt 2/8.\nLemma 6 (Invertibility of a random Gaussian Matrix [38]). Let A ∈ Rn×n be a matrix with i.i.d. standard Gaussian random variables. Then, for any δ > 0\nP{σn(A) ≤ δn− 1 2 } ≤ 2.35δ."
    }, {
      "heading" : "6 Proof of Theorem 3",
      "text" : "First, we show that there is some orthonormal matrix Q such that ‖Y − ỸQ‖22 ≤ ‖PY −PỸ‖22.\nTheorem 7. Let Y ∈ Rn×k and Ỹ ∈ Rn×k are the matrices described in the exact and the approximate spectral clustering algorithms, respectively. Then, there is an orthonormal matrix Q ∈ Rk×k such that,\n‖Y − ỸQ‖22 ≤ ‖PY −PỸ‖ 2 2\nProof. Observe that if O is a square orthonormal matrix, then OT , YTO, and Ỹ T O are all orthonormal matrices. Recall also that ‖A‖2 = ‖AO‖2 when O is orthonormal. Consequently,\ninf OOT=I ‖Y − ỸO‖2 ≤ inf OOT=I\n‖Y − Ỹ(ỸTO)‖2\n= inf OOT=I\n‖YOT − ỸỸT ‖2\n≤ inf OOT=I\n‖YYTO− ỸỸT ‖2\n≤ ‖YYT − ỸỸT ‖2 = ‖PY −PỸ‖2.\nIt follows that there is an orthonormal matrix Q satisfying the claim of the theorem.\nTo prove Theorem 3 it only remains to show that ‖PY − PỸ‖22 ≤ O(ε2). We show this bound in Corollary 11 (specifically, apply this corollary to A = W̃). Corollary 11 is based on few results of independent interest, which we present next."
    }, {
      "heading" : "7 Error bounds for Subspace Iteration",
      "text" : "We start with a matrix perturbation bound which we use later to prove our main results for approximate spectral clustering.\nLemma 8. Consider three matrices D,C,E ∈ Rm×n; Let D = C + E and PCE = 0m×n. Then,∥∥(I−PD)PC∥∥22 = 1− λrank(C)(C(DTD)†CT ). Proof. Let the columns of U constitute an orthonormal basis for the range of C and those of V constitute an orthonormal basis for the range of D. Denoting the rank of C by k = rank(C), we observe that∥∥(I−PD)PC∥∥22 = ∥∥PC(I−PD)PC∥∥2\n= ∥∥UUT −UUTVVTUUT∥∥\n2 = ∥∥U(I−UTVVTU)UT∥∥\n2 = ∥∥I−UTVVTU∥∥\n2 .\nThe first equality follows from the fact that ∥∥X∥∥2 2 = ∥∥XXT∥∥ 2 , and the idempotence of projections. The last inequality uses the unitary invariance of the spectral norm and the fact that UT maps the unit ball of its domain onto the unit ball of Rk.\nIt follows that∥∥(I−PD)PC∥∥22 = 1− λk(UTVVTU) = 1− λk(VTUUTV) = 1− λk(UUTVVT ), where our manipulations are justified by the fact that, when the two products are well-defined, the eigenvalues of AB are identical to those of BA up to multiplicity of the zero eigenvalues. We further observe that\nλk(UU TVVT ) = λk(PCPD) = λk(PCDD †) = λk(PC(C + E)D †) = λk(CD †)\nbecause PCE = 0. So, ∥∥(I−PD)PC∥∥22 = 1− λk(CD†). (2) Recall one expression for the pseudoinverse, D† = (DTD)†DT . Using this identity, we find that\nλk(CD †) = λk(PCC(D TD)†DT ) = λk(C(D TD)†DTPC) = λk(C(D TD)†(PCD) T ).\nSince PCD = PC(C + E) = C,\nwe conclude that λk(CD †) = λk(C(D TD)†CT ).\nSubstituting this equality into (2) gives the desired identity.\nWe continue with a bound on the the difference of the projection matrices corresponding to two different subspaces. Given a matrix A those two subspaces are the following. The first is the so-called dominant subspace of A, i.e., the subspace spanned by the top k left singular vectors of A. The second is the subspace obtained after applying the truncated power iteration to A.\nLemma 9. Let S ∈ Rn×k with rank(AkS) = k, then for Ω1 = (AAT )pAS and Ω2 = Ak, we obtain∥∥PΩ1 −PΩ2∥∥22 = 1− λk(AkS(STATAS)†STATk ). Proof. In Lemma 8, take C = AkS and E = Aρ−kS so that D = AS = C + E. Indeed, since rank(AkS) = k = rank(Ak), the range spaces of AkS and Ak are identical, so\nPCE = PAkAρ−kS = 0m×n.\nLemma 8 gives ∥∥(I−PD)PC∥∥22 = 1− λk(AkS(STATAS)†STATk ).\nNext, we prove a bound as in the previous lemma, but with the right hand side involving different quantities.\nTheorem 10. Given A ∈ Rm×n, let S ∈ Rn×k be such that rank(AkS) = k. Let p ≥ 0 be an integer and define\nγp = ∥∥Σ2p+1ρ−k VTρ−kS(VTk S)−1Σ−(2p+1)k ∥∥2.\nThen, for Ω1 = (AA T )pAS and Ω2 = Ak, we obtain ∥∥PΩ1 −PΩ2∥∥22 = γ2p1 + γ2p . Proof. Note that AkS has full column rank, so\nSTATkAkS 0\nis invertible. It follows that\nSTATAS = STATkAkS + S TATρ−kAρ−kS 0\nis also invertible. Also VTk S is a k × k random Gaussian matrix (the product of a random Gaussian matrix with an orthonormal matrix is also a random Gaussian matrix) and hence is invertible almost surely according to Lemma 6.\nHence,\nλk(AkS(SA TAS)†STATk ) =\n= λk ( UkΣkV T k S(SA TAS)−1STVkΣkU T k ) = λk ( ΣkV T k S(SA TAS)−1STVkΣk\n) = λmax (( ΣkV T k S(S TATAS)−1STVkΣk\n)−1)−1 = λmax (( STVkΣk )−1 ( STATAS ) ( ΣkV t kS )−1)−1\n= λmax ( Ik + ( STVkΣk )−1 STVρ−kΣ 2 ρ−kV T ρ−kS(ΣkV t kS) −1 )−1\n= ( 1 + ∥∥Σρ−kVTρ−kS(ΣkVTk S)−1∥∥22)−1\n= ( 1 + ∥∥Σρ−kVTρ−kS(VTk S)−1Σ−1k ∥∥22)−1 = (1 + γ2p) −1.\nThe result follows by substituting this expression into the expression given in Lemma 9.\nThe term γp has a compelling interpretation: note that\nγp ≤ ( σk+1(A)\nσk(A) )2p+1 ∥∥VTρ−kS∥∥2∥∥(VTk S)−1∥∥2; the first term, a power of the ratio of the (k+1)th and kth singular values of A, is the inverse of the spectral gap; the larger the spectral gap, the easier it should be to capture the dominant k dimensional range-space of A in the range space of AS. Increasing the power increases the beneficial effect of the spectral gap. The remaining terms reflect the importance of choosing an appropriate sampling matrix S: one desires AS to have a small component in the direction of the undesired range space spanned by Uρ−k; this is ensured if S has a small component in the direction of the corresponding right singular space. Likewise, one desires Vk and S to be strongly correlated so that AS has large components in the direction of the desired space, spanned by Uk.\nTo apply Theorem 10 practically to estimate the dominant k-dimensional left singular space of A, we must choose an S ∈ Rn×k that satisfies rank(AkS) = k and bound the resulting γp. The next corollary gives a bound on γp when S is chosen to be a matrix of i.i.d. standard Gaussian r.v.s.\nCorollary 11. Fix A ∈ Rm×n with rank at least k. Let p ≥ 0 be an integer and draw S ∈ Rn×k, a matrix of i.i.d. standard Gaussian r.v.s. Fix δ ∈ (0, 1) and ε ∈ (0, 1). Let\nΩ1 = (AA T )pAS,\nand Ω2 = Ak.\nIf p satisfies\np ≥ 1 2\nln(4ε−1δ−1 √ k(n− k)) ln−1 ( σk (A)\nσk+1 (A) ) then with probability at least 1− e−n − 2.35δ,\n‖PΩ1 −PΩ2‖22 ≤ ε2\n1 + ε2 = O(ε2).\nProof. Clearly AkS has rank k (the rank of the product of a Gaussian matrix and an arbitrary matrix is the minimum of the ranks of the two matrices), so Theorem 10 is applicable. Estimate γp as\nγp ≤ ∥∥Σ2p+1ρ−k ∥∥2∥∥VTρ−kS∥∥2∥∥(VTk S)−1∥∥2∥∥Σ−(2p+1)k ∥∥2\n= ( σk+1 σk )2p+1 σmax (VTρ−kS) σmin ( VTk S\n) ≤ ( σk+1 σk )2p+1 4 √ n− k δ/ √ k\n= ( σk+1 σk )2p+1 · 4δ−1 √ k(n− k).\nIn the above, the last inequality follows from the fact that VTρ−kS and V T k S are matrices of standard Gaussians, and then using Lemma 5 with t = 4 and Lemma 6. The guarantees of both Lemmas apply with probability at least 1− e−n − 2.35δ, so\nγp ≤ ( σk+1 σk )2p+1 · 4δ−1 · √ k(n− k)\nholds with at least this probability. Hence, to ensure γp ≤ ε, it suffices to choose p satisfying\np ≥ 12\n( ln(4ε−1δ−1 √ k(n− k)) ln−1 ( σk (A)\nσk+1 (A)\n) − 1 ) .\nIn this case,\n‖PY −PỸ‖ 2 2 ≤\nε2\n1 + ε2 = O(ε2)."
    }, {
      "heading" : "8 Experiments",
      "text" : "Setting expectations. The selling point of approximate spectral clustering is the reduction in time to solution without sacrificing the quality of the clustering. Computationally, approximation saves time as we compute the eigenvectors of a smaller matrix — (W̃W̃ T\n)pW̃S ∈ Rn×k — as opposed to W̃ ∈ Rn×n; however, we do incur the cost of computing this smaller matrix itself. The exponent p is a parameter to the spectral clustering algorithm that determines the quality of the approximate solution (see Theorem 3). Accordingly, the goal of our experiments is to: (1) establish that the quality of clustering does not suffer due to the approximation, and (2) show the effect of p on the solution quality and the running time. Note that our experiments are not meant to produce the best quality clusters for the test data; this requires fine-tuning several parameters such as the kernel bandwidth for W, which is out of the scope of this paper.\nreported (in seconds) is to complete the power iterations and the eigen-decomposition of (W̃W̃ T\n)pW̃S with p. A “—” in the right three columns indicates that the best CR% under exact time for the approximate algorithms was also achieved at p = 2 (Vehicle). We see that the approximate algorithms perform at least as good as their exact counterparts. Note that CR% is heavily dependent on the quality of W, which we did not optimize.\nSetup. To conduct our experiments, we developed MATLAB versions of the exact and approximate algorithms (Section 3 and 2.1). To compute W, we use the heat kernel: ln(Wij) = ||xi−xj ||22\nσij , where xi ∈ Rd and\nxj ∈ Rd are the data points and σij is a tuning parameter; σij is determined using the self-tuning method described in [49]. For the partial singular value decomposition of W̃, we used IRLBA [2] due to it’s excellent support for sparse decompositions. To compute W̃S, we used MATLAB’s built-in normrnd function to generate a Gaussian S. For the complete singular value decomposition of (W̃W̃ T\n)pW̃S, we use MATLAB’s built-in svd function. We also use MATLAB’s built-in kmeans function for the final clustering; the options ‘EmptyAction’, ‘singleton’ were turned on in addition to specifying a maximum of 100 iterations and 10 ‘Replicates’. The output labeling order of both the exact and the approximate algorithms do not necessarily match the true labels. To avoid comprehensively testing all k permutations of the output labels for optimality with the true labels, we use the Hungarian algorithm [23]. As a measure of quality, we use the clustering-rate (CR), which can be defined as the (normalized) number of correctly clustered objects; that is CR = 1n ∑n i=1 Ii, where Ii is an indicator variable that is 1 when an object is correctly clustered and 0 when it is not. We ran our experiments on four multi-class datasets from the libSVMTools webpage (Table 1). All experiments were carried out using MATLAB 7.8.0.347 (R2009a) on a 64-bit linux server running 2.6.31-23. Finally, to highlight the timing differences between using the exact and the approximate algorithm, we only report the time taken to compute the singular value decomposition; all the other steps are the same for both algorithms.\nResults. To determine the quality of the clustering obtained by our approximate algorithm, we ran 10 iterations on each of the 4 datasets listed in Table 1, and report results for the one which minimizes the clustering rate CR%. This was done to minimize the effect of the randomness of S. In each iteration, we varied p from [0, 10]. First, we explore how the error in Theorem 3 behaves while p grows. As we are\nnot aware how to find this optimal Q, we compute Q̂ as the solution of the Frobenius norm version of the problem, which is the famous Procrustes problem, Q̂ = argminQTQ=Ik ‖Y − ỸQ‖F, i.e., Q̂ = UBV T B (B = YT Ỹ). Figure 3 confirms that the approximation error decreases with an increase in p. Next, we report results for the clustering rate CR% in Table 2. In columns 2 and 3, we see the results for the exact algorithm, which serves as our baseline for quality and performance. In columns 4 and 5, we see the CR% with 2 power iterations (p = 2) along with the time required for the power iterations and the eigendecomposition. Immediately, we see that at p = 2, the Vehicle dataset achieves good performance by beating or matching the performance of the exact algorithm, resulting in a 2.5x speedup. The only outlier is the Segment dataset, which achieves poor CR% at p = 2. In columns 6—8, we report the best CR% that was achieved by the approximate algorithm while staying under the time taken by the exact algorithm; we also report the value of p and the time taken (in seconds) for this result. A “—” in the\nVariation of Procrustes’ error with p\nright three columns indicates that the best CR% under exact time for the approximate algorithms was also achieved at p = 2 (Vehicle). We see that even when constrained to run in less time than the exact algorithm, the approximate algorithm bests the CR% achieved by the exact algorithm. For example, at p = 10, the approximate algorithm reports 76.46% for Segment, while still providing a 1.4x speedup. We can see that in many cases, 2 or fewer power iterations (p ≤ 2) suffice to get good quality clustering (Segment dataset being the exception). Note that the clustering quality also depends on the quality of W and optimizations in k-means, both of which we did not attempt to optimize.\nNext, we turn our attention to the relation of p to the time to compute the power iterations followed by the eigendecomposition; this is depicted in Figure 4. All the times are normalized by the time taken at p = 0 to enable reporting numbers for all datasets on the same scale; notice that when p = 0, the approximation to W̃ is merely W̃S. As expected, as p increases, we see a linear increase in the time taken.\nFinally, we report the variation of the clustering rate with p in Figure 5; all the rates are normalized by the CR% at p = 0. We observe that the clustering quality is comparable to the ground truth. Although one would expect the clustering CR% to increase monotonically while we increase p, this is not entirely true. Recall that we assumed that Y and Ỹ are clustered identically as ε → 0. However, in our setting ε is not approaching 0 (see Figure 3), hence k-means finds (slightly) different clusters for small ε, e.g., ε = 0.1.\nVariation of time with p\nof (W̃W̃ T\n)pW̃S. The baseline times (at p=0) are SatImage (0.0648 secs), Segment (0.0261 secs), Vehicle (0.0027 secs), and Vowel (0.0012 secs). Times reported are in seconds.\nVariation of Clustering Rate with p"
    } ],
    "references" : [ {
      "title" : "Efficient dimensionality reduction for canonical correlation analysis",
      "author" : [ "H. Avron", "C. Boutsidis", "S. Toledo", "A. Zouzias" ],
      "venue" : "In ICML",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Restarted block lanczos bidiagonalization methods",
      "author" : [ "J. Baglama", "L. Reichel" ],
      "venue" : "Numerical Algorithms, 43(3):251– 272",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The numerical treatment of integral equations",
      "author" : [ "C. Baker" ],
      "venue" : "volume 13. Clarendon Press",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Laplacian eigenmaps and spectral techniques for embedding and clustering",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "NIPS",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Approximate clustering in very large relational data",
      "author" : [ "J. Bezdek", "R. Hathaway", "J. Huband", "C. Leckie", "R. Kotagiri" ],
      "venue" : "International journal of intelligent systems, 21(8):817–841",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Random projections for the nonnegative least-squares problem",
      "author" : [ "C. Boutsidis", "P. Drineas" ],
      "venue" : "Linear Algebra and its Applications, 431(5-7):760–771",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Improved matrix algorithms via the subsampled randomized hadamard transform",
      "author" : [ "C. Boutsidis", "A. Gittens" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Random projections for k-means clustering",
      "author" : [ "C. Boutsidis", "A. Zouzias", "P. Drineas" ],
      "venue" : "In NIPS",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Low rank approximation and regression in input sparsity time",
      "author" : [ "K.L. Clarkson", "D.P. Woodruff" ],
      "venue" : "STOC",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Local Operator Theory",
      "author" : [ "K.R. Davidson", "S.J. Szarek" ],
      "venue" : "Random Matrices and Banach Spaces. In W. B. Johnson and J. Lindenstrauss, editors, Handbook of the Geometry of Banach Spaces, volume 1. Elsevier Science",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Co-clustering documents and words using bipartite spectral graph partitioning",
      "author" : [ "I.S. Dhillon" ],
      "venue" : "KDD, pages 269–274. ACM",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Lower bounds for the partitioning of graphs",
      "author" : [ "W.E. Donath", "A.J. Hoffman" ],
      "venue" : "IBM Journal of Research and Development, 17(5):420–425",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Fast Monte-Carlo algorithms for approximate matrix multiplication",
      "author" : [ "P. Drineas", "R. Kannan" ],
      "venue" : "FOCS",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Faster least squares approximation",
      "author" : [ "P. Drineas", "M. Mahoney", "S. Muthukrishnan", "T. Sarlós" ],
      "venue" : "Numerische Mathematik, 117(2):217–249",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Algebraic connectivity of graphs",
      "author" : [ "M. Fiedler" ],
      "venue" : "Czechoslovak Mathematical Journal, 23(2):298–305",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Spectral grouping using the Nystrom method",
      "author" : [ "C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 26(2):214–225",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Matrix computations",
      "author" : [ "G.H. Golub", "C.F. Van Loan" ],
      "venue" : "volume 3. JHU Press",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "N. Halko", "P. Martinsson", "J. Tropp" ],
      "venue" : "SIAM Review, 53(2):217–288",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "N. Halko", "P.-G. Martinsson", "J.A. Tropp" ],
      "venue" : "SIAM review, 53(2):217–288",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Spectral clustering with perturbed data",
      "author" : [ "L. Huang", "D. Yan", "M. Jordan", "N. Taft" ],
      "venue" : "NIPS",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Performance analysis of spectral clustering on compressed",
      "author" : [ "B. Hunter", "T. Strohmer" ],
      "venue" : "incomplete and inaccurate measurements. Preprint: http://arxiv.org/abs/1011.0997",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Improving the hungarian assignment algorithm",
      "author" : [ "R. Jonker", "T. Volgenant" ],
      "venue" : "Operations Research Letters, 5(4):171–175",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "On clusterings: Good",
      "author" : [ "R. Kannan", "S. Vempala", "A. Vetta" ],
      "venue" : "bad and spectral. Journal of the ACM (JACM), 51(3):497–515",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Approaching optimality for solving SDD systems",
      "author" : [ "I. Koutis", "G. Miller", "R. Peng" ],
      "venue" : "FOCS",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Power iteration clustering",
      "author" : [ "F. Lin", "W.W. Cohen" ],
      "venue" : "ICML",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Segmentation of 3d meshes through spectral clustering",
      "author" : [ "R. Liu", "H. Zhang" ],
      "venue" : "Computer Graphics and Applications, 2004. PG 2004. Proceedings. 12th Pacific Conference on, pages 298–305. IEEE",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A randomized algorithm for the decomposition of matrices",
      "author" : [ "P. Martinsson", "V. Rokhlin", "M. Tygert" ],
      "venue" : "Applied and Computational Harmonic Analysis",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Conductance and convergence of Markov chains-A combinatorial treatment of expanders",
      "author" : [ "M. Mihail" ],
      "venue" : "FOCS",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "et al",
      "author" : [ "A.Y. Ng", "M.I. Jordan", "Y. Weiss" ],
      "venue" : "On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849–856",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Über die praktische Auflösung von Integralgleichungen mit Anwendungen auf Randwertaufgaben",
      "author" : [ "E. Nyström" ],
      "venue" : "Acta Mathematica, 54(1):185–204",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1930
    }, {
      "title" : "The effectiveness of lloyd-type methods for the k-means problem",
      "author" : [ "R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy" ],
      "venue" : "FOCS",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Spectral clustering of protein sequences",
      "author" : [ "A. Paccanaro", "J.A. Casbon", "M.A. Saqi" ],
      "venue" : "Nucleic acids research, 34(5):1571–1580",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Random projections for support vector machines",
      "author" : [ "S. Paul", "C. Boutsidis", "M. Magdon-Ismail", "P. Drineas" ],
      "venue" : "In AISTATS",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient out-of-sample extension of dominant-set clusters",
      "author" : [ "M. Pavan", "M. Pelillo" ],
      "venue" : "NIPS",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A fast randomized algorithm for overdetermined linear least-squares regression",
      "author" : [ "V. Rokhlin", "M. Tygert" ],
      "venue" : "Proceedings of the National Academy of Sciences, 105(36):13212",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Fast spectral clustering with random projection and sampling",
      "author" : [ "T. Sakai", "A. Imiya" ],
      "venue" : "Machine Learning and Data Mining in Pattern Recognition, pages 372–384. Springer",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Smoothed analysis of the condition numbers and growth factors of matrices",
      "author" : [ "A. Sankar", "D.A. Spielman", "S.-H. Teng" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications, 28(2):446–476",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Improved approximation algorithms for large matrices via random projections",
      "author" : [ "T. Sarlós" ],
      "venue" : "In FOCS",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Spectral clustering on a budget",
      "author" : [ "O. Shamir", "N. Tishby" ],
      "venue" : "AISTATS",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(8):888–905",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A spectral clustering approach to finding communities in graphs",
      "author" : [ "S. Smyth", "S. White" ],
      "venue" : "SDM",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Nearly-linear time algorithms for preconditioning and solving symmetric",
      "author" : [ "D. Spielman", "S.-H. Teng" ],
      "venue" : "diagonally dominant linear systems. ArxiV preprint, http://arxiv.org/pdf/cs/0607105v4.pdf",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Introduction to matrix computations",
      "author" : [ "G. Stewart", "G. Stewart" ],
      "venue" : "volume 441. Academic press New York",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "U. Von Luxburg" ],
      "venue" : "Statistics and computing, 17(4):395–416",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Approximate spectral clustering",
      "author" : [ "L. Wang", "C. Leckie", "K. Ramamohanarao", "J. Bezdek" ],
      "venue" : "Advances in Knowledge Discovery and Data Mining, pages 134–146",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Segmentation using eigenvectors: a unifying view",
      "author" : [ "Y. Weiss" ],
      "venue" : "Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 975–982. IEEE",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Fast approximate spectral clustering",
      "author" : [ "D. Yan", "L. Huang", "M. Jordan" ],
      "venue" : "KDD",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Self-tuning spectral clustering",
      "author" : [ "L. Zelnik-Manor", "P. Perona" ],
      "venue" : "NIPS",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "In particular, in both cases, the two clusters have the same centers (centroids); hence, distance-based clustering methods such as k-means [32] will fail miserably.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 44,
      "context" : "We refer the reader to Section 9 in [45] for a discussion on the history of spectral clustering.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "The first reports in the literature go back to [13, 16].",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "The first reports in the literature go back to [13, 16].",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 44,
      "context" : "According to [45], “since then spectral clustering has been discovered, re-discovered, and extended many times”.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 40,
      "context" : "Shi and Malik brought spectral clustering to the machine learning community in their seminal work on Normalized Cuts and Image Segmentation [41] (see Section 2).",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 11,
      "context" : "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 29,
      "context" : "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 23,
      "context" : "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 26,
      "context" : "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 48,
      "context" : "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 41,
      "context" : "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 32,
      "context" : "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 44,
      "context" : "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 42,
      "context" : "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 47,
      "context" : "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].",
      "startOffset" : 181,
      "endOffset" : 200
    }, {
      "referenceID" : 16,
      "context" : "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].",
      "startOffset" : 181,
      "endOffset" : 200
    }, {
      "referenceID" : 34,
      "context" : "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].",
      "startOffset" : 181,
      "endOffset" : 200
    }, {
      "referenceID" : 4,
      "context" : "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].",
      "startOffset" : 181,
      "endOffset" : 200
    }, {
      "referenceID" : 45,
      "context" : "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].",
      "startOffset" : 181,
      "endOffset" : 200
    }, {
      "referenceID" : 40,
      "context" : "Definition 1 (The Spectral Clustering Problem [41]).",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 40,
      "context" : ") in a weighted undirected graph is an NP-Complete problem (see the appendix in [41] for the proof).",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 40,
      "context" : "Motivated by this hardness result, Shi and Malik [41] suggested a relaxation to the problem that is tractable in polynomial time through the SVD.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 40,
      "context" : "First, [41] shows that for any G,A,B and partition vector y ∈ R with +1 to the entries corresponding to A and −1 to the entries corresponding to B it is: 4 · Ncut(A,B) = y (D −W)y/(yDy).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 40,
      "context" : "Definition 2 (The Real Relaxation for the Spectral Clustering Problem [41]).",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 40,
      "context" : "Motivated by the above observations, which are due to [41], Ng, Jordan, and Weis [30] (see also [47]) suggested the following algorithm for spectral clustering , which we consider as the ground-truth algorithm.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 29,
      "context" : "Motivated by the above observations, which are due to [41], Ng, Jordan, and Weis [30] (see also [47]) suggested the following algorithm for spectral clustering , which we consider as the ground-truth algorithm.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 46,
      "context" : "Motivated by the above observations, which are due to [41], Ng, Jordan, and Weis [30] (see also [47]) suggested the following algorithm for spectral clustering , which we consider as the ground-truth algorithm.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "4 in [18]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 19,
      "context" : "The survey [20] gives an overview of these truncated processes in the context of low-rank matrix approximation.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 17,
      "context" : "3 in [18]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 21,
      "context" : "Motivated by the work in [22] we adapt the following approach: we assume that if, for two matrices U ∈ Rn×k and V ∈ Rn×k, both with orthonormal columns, ‖U −V‖2 ≤ ε, for some arbitrarily small ε > 0, then clustering the rows of U and the rows of V with the same method should result to the same clustering.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 31,
      "context" : "Hence, a distance-based algorithm such as k-means [32] would lead to the same clustering as ε→ 0 (this is equivalent to saying that k-means is robust to small perturbations to the input).",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 21,
      "context" : "The Hunter and Strohmer result [22].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 21,
      "context" : "The result that is most relevant to ours appeared in [22].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 21,
      "context" : "Firstly, we follow the exact same framework for measuring the approximability of an algorithm for spectral clustering (see Theorem 5 and Corollary 6 in [22]).",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 21,
      "context" : "Specifically, they prove a weaker version of Theorem 7 of our work (see the proof of Theorem 5 in [22]).",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 36,
      "context" : "The most relevant algorithmic result to ours is [37].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : "Another clustering method similar to ours is in [26].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 47,
      "context" : "Then, [48, 21, 44] show the following bound: ρ ≤ ‖ṽ2 − v2‖2 ≤ (λ2 − λ3) ‖E‖2 +O(‖E‖2).",
      "startOffset" : 6,
      "endOffset" : 18
    }, {
      "referenceID" : 20,
      "context" : "Then, [48, 21, 44] show the following bound: ρ ≤ ‖ṽ2 − v2‖2 ≤ (λ2 − λ3) ‖E‖2 +O(‖E‖2).",
      "startOffset" : 6,
      "endOffset" : 18
    }, {
      "referenceID" : 43,
      "context" : "Then, [48, 21, 44] show the following bound: ρ ≤ ‖ṽ2 − v2‖2 ≤ (λ2 − λ3) ‖E‖2 +O(‖E‖2).",
      "startOffset" : 6,
      "endOffset" : 18
    }, {
      "referenceID" : 28,
      "context" : "Motivated by a result of Mihail [29], Spielman and Teng [43] suggest the following definition for an approximate Fiedler vector: For a Laplacian matrix L and 0 < ε < 1, v ∈ R is an ε-approximate Fiedler vector if v is orthogonal to the all-ones vector and vLv vTv ≤ (1 + ε) fLf fTf = (1 + )λn−1(L).",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 42,
      "context" : "Motivated by a result of Mihail [29], Spielman and Teng [43] suggest the following definition for an approximate Fiedler vector: For a Laplacian matrix L and 0 < ε < 1, v ∈ R is an ε-approximate Fiedler vector if v is orthogonal to the all-ones vector and vLv vTv ≤ (1 + ε) fLf fTf = (1 + )λn−1(L).",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 42,
      "context" : "2 in [43] gives a randomized algorithm to compute such an ε-approximate Fiedler vector w.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 47,
      "context" : "[48] proposed an algorithm where one first applies k-means to the data to find the so-called centroids and then applies spectral clustering on the centroids and uses this to cluster all the points.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "with a nearest neighbor based approach) to all the points [35, 5, 46].",
      "startOffset" : 58,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "with a nearest neighbor based approach) to all the points [35, 5, 46].",
      "startOffset" : 58,
      "endOffset" : 69
    }, {
      "referenceID" : 45,
      "context" : "with a nearest neighbor based approach) to all the points [35, 5, 46].",
      "startOffset" : 58,
      "endOffset" : 69
    }, {
      "referenceID" : 30,
      "context" : "This is known as the Nyström method [31, 3, 17].",
      "startOffset" : 36,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "This is known as the Nyström method [31, 3, 17].",
      "startOffset" : 36,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "This is known as the Nyström method [31, 3, 17].",
      "startOffset" : 36,
      "endOffset" : 47
    }, {
      "referenceID" : 39,
      "context" : "This is the approach of the spectral clustering on a budget method [40].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 35,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 283,
      "endOffset" : 298
    }, {
      "referenceID" : 5,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 283,
      "endOffset" : 298
    }, {
      "referenceID" : 14,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 283,
      "endOffset" : 298
    }, {
      "referenceID" : 9,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 283,
      "endOffset" : 298
    }, {
      "referenceID" : 24,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 366,
      "endOffset" : 370
    }, {
      "referenceID" : 27,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 412,
      "endOffset" : 427
    }, {
      "referenceID" : 18,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 412,
      "endOffset" : 427
    }, {
      "referenceID" : 6,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 412,
      "endOffset" : 427
    }, {
      "referenceID" : 9,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 412,
      "endOffset" : 427
    }, {
      "referenceID" : 13,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 455,
      "endOffset" : 463
    }, {
      "referenceID" : 38,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 455,
      "endOffset" : 463
    }, {
      "referenceID" : 7,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 489,
      "endOffset" : 492
    }, {
      "referenceID" : 33,
      "context" : "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].",
      "startOffset" : 524,
      "endOffset" : 528
    }, {
      "referenceID" : 0,
      "context" : "(viii) Canonical Correlation Analysis (CCA) [1].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 19,
      "context" : "Specifically, [20] argues that ‖W̃ −PỸW̃‖2 is small.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 17,
      "context" : "4 in [18].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 17,
      "context" : "2 in [18] shows a bound similar to our bound in Theorem 10.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 10,
      "context" : "Lemma 5 (The norm of a random Gaussian Matrix [11]).",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 37,
      "context" : "Lemma 6 (Invertibility of a random Gaussian Matrix [38]).",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "Table 1: Datasets from the libSVM multi-class classification page [9] that were used in our experiments.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 48,
      "context" : "To compute W, we use the heat kernel: ln(Wij) = ||xi−xj ||22 σij , where xi ∈ R and xj ∈ R are the data points and σij is a tuning parameter; σij is determined using the self-tuning method described in [49].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 1,
      "context" : "For the partial singular value decomposition of W̃, we used IRLBA [2] due to it’s excellent support for sparse decompositions.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 22,
      "context" : "To avoid comprehensively testing all k permutations of the output labels for optimality with the true labels, we use the Hungarian algorithm [23].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 9,
      "context" : "In each iteration, we varied p from [0, 10].",
      "startOffset" : 36,
      "endOffset" : 43
    } ],
    "year" : 2017,
    "abstractText" : "Spectral clustering is arguably one of the most important algorithms in data mining and machine intelligence; however, its computational complexity makes it a challenge to use it for large scale data analysis. Recently, several approximation algorithms for spectral clustering have been developed in order to alleviate the relevant costs, but theoretical results are lacking. In this paper, we present a novel approximation algorithm for spectral clustering with strong theoretical evidence of its performance. Our algorithm is based on approximating the eigenvectors of the Laplacian matrix using random projections, a.k.a randomized sketching. Our experimental results demonstrate that the proposed approximation algorithm compares remarkably well to the exact algorithm.",
    "creator" : "TeX"
  }
}