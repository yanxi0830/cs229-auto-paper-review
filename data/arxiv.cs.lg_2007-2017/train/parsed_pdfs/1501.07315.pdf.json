{
  "name" : "1501.07315.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Konstantinos Slavakis", "Georgios B. Giannakis" ],
    "emails" : [ "kslavaki@umn.edu", "georgios@umn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 1.\n07 31\n5v 1\n[ cs\n.L G\n] 2\n9 Ja\nn 20"
    }, {
      "heading" : "1 Introduction",
      "text" : "Aiming at succinct representations of large-scale data, models relying on non-convex functions have\nemerged as a prominent tool to learn low-dimensional structure from (possibly high-dimensional) data.\nAreas of interest span signal processing and machine learning applications including dictionary learning\n(DL) [1–3], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor\n(PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.\nConsider DL for specificity, where a given M ×1 vector yt is modeled as the product of an unknown over-complete dictionary D := [d1, . . . ,dQ], Q ≥ M , times an unknown sparse coefficient vector st [2, 3]. With Yt := [y1, . . . ,yt], and likewise for S, DL solves\nmin (S,D)\n1\n2t ‖Yt −DS‖2F + λs‖S‖1 + ιD(D) (1)\n∗Preliminary results of this work appear in the Proc. of the IEEE Intern. Conf. Acoustics, Speech, and Signal Process. (ICASSP), Florence, Italy, May 4–9, 2014. This work was supported by the NSF grant Eager 1343860.\n†The authors are with the Dept. of ECE and the Digital Technology Center, Univ. of Minnesota, 117 Pleasant St. SE, Minneapolis, MN 55455, USA. Tel: (612) 625-0763; Emails: {kslavaki,georgios}@umn.edu\nwhere ‖·‖ F denotes the Frobenius norm; the scale λs > 0 controls the sparsity effected by the ℓ1-norm ‖S‖1 := ∑\nq,t|sq,t|; the set indicator is ιD(D) = 0 if D ∈ D, and ιD(D) = +∞ otherwise, where the set D := {D ∈ RM×Q | ‖dq‖ ≤ 1, q ∈ {1, . . . , Q}} confines the dictionary to have bounded-norm columns. This constraint fixes the inherent scale ambiguity of the bilinear fit DS, and also ensures that the\nsolution of (1) remains bounded. Sparsity on the other hand, renders DL representations identifiable\neven when yt has missing entries [8], due to e.g., malfunctioning, privacy reasons, or, high cost of data gathering.\nDue to the bilinear term DS, the three-summand cost Ft(S,D;Ot) := ft(S,D;Ot)+ g1(S)+ g2(D) in (1) is non-convex (set Ot := {y1, . . . ,yt} collects observations up to t.) However, Ft is clearly “perblock-convex,” as it is convex in either S or D, if the other one is fixed. Related multilinear forms emerge\nalso with NMF, SSC, PARAFAC, and TLS models. Mainly for offline optimization, block coordinate\ndescent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity\nof the cost functions involved [3, 9–19]. For online DL, BCDMs alternate between two iterations to\nupdate current estimates (St−1 := [s1, . . . , st−1],Dt−1) as follows [3]\nst ∈ argmins Ft ( [St−1, s],Dt−1;Ot ) (2a) Dt ∈ argminD Ft(St,D;Ot) . (2b)\nGiven Ot, each step in (2) is a convex optimization task: Basis pursuit [20] in (2a), and constrained leastsquares (LS) in (2b). However, the per-block minimizations in BCD may not be affordable by todays big\ndata applications, where the sheer volume and dimensionality of Ot strain computing resources [21,22]. Further, as data are streaming, analytics must often be performed in real time, without a chance to\nrevisit past entries – a feature common to stochastic approximation (SA) setups [23].\nIn the spirit of SA, the present paper deals with minimizing expected value costs of the form\nminx EO{Ft(x;Ot)} (3)\nwhere x comprises all blocks of variables, e.g., x := (S,D) in (1), and expectation E is over the random\nOt, whose probability density function (pdf) is unknown. The goal is to develop a modular algorithmic framework for solving (3) that: i) Leverages per-block-convexity of Ft as in BCDMs; ii) it operates online with streaming data Ot of unknown pdf; iii) relies only on first-order (sub)gradient information of Ft, bypassing the need for (almost) exact minimizers per block as in (2); iv) it incurs affordable complexity per iteration, at most linear with respect to (w.r.t.) the number of unknowns; and v)\niterations converge quadratically to a solution of (3), which is optimal among first-order methods in\nthe sense of [24].\nTo place our contributions i)-v) in context, related first-order online BCDMs include the proximal\nstochastic (sub)gradient iterations [23, 25–28], whose convergence tends to be slow even for convex\nproblems, on top of being challenged by step-size choices. A relevant stochastic algorithm is the SA-\nbased alternating-direction method of multipliers (ADMM) [29–32], that is known to be sublinearly\nconvergent for convex costs [33–36], but no similar results are available for per-block-convex functions.\nOn the other hand, accelerated first-order quadratically convergent iterations are available for off-line\nconvex optimization [24, 37–40]; see also [41] for related SA-based minimizers of convex costs. Even\nthough [42] deals with non-convex costs, it requires bounds on the (primal) variables, knowledge of a\nbound on the Lipschitz coefficient of the gradient operator for the algorithm to operate, and does not\nexploit the modularity offered by per-block-convexity.\nOur work markedly broadens the offline acceleration technique introduced for convex costs in [39],\nto per-block-convex and to online SA setups. Unless the per-block optimization task can be solved in\nsimple closed-form, there is no need for exact minimizers per block. Without knowing the data pdf and\nby relying only on first-order information of the instantaneous cost Ft, at linear complexity per iteration, we prove that the expected cost converges quadratically. Neither bounds on the block variables nor\nknowledge of bounds on Lipschitz coefficients are required. Under minimal assumptions and without\nimposing any block-wise strong convexity on the cost, performance analysis is carried out both for the\ncost values and the block variables of task (3). Specifically, we establish that the expected limit cost is\nan accumulation point of (per-block) minima, and that subgradients of the expected cost asymptotically\nvanish in the mean-squared sense. The analytical results are tested on two instances of broad practical\ninterest: (i) Online, robust and sparsity-aware linear TLS regression, using synthetic data; and (ii)\nonline semi-supervised DL for network-wide link load tracking and imputation of real data. Numerical\ntests corroborate our analytical claims, and demonstrate that under a linear computational complexity\nfootprint the proposed algorithm outperforms BCDMs and the computationally heavier ADMM-based\nalternatives [8].\nThe rest of the manuscript is organized as follows. Preliminaries are given in Sec. 2, while the\nproposed algorithm is developed in Sec. 3. Performance analysis is the subject of Sec. 4, with proofs\ndelegated to Appendix A. Two examples of principal practical interest are provided in Sec. 5. Numerical\ntests both on synthetic and real data are presented in Sec. 6, while the manuscript is concluded in Sec. 7.\nPreliminary results were presented in [43], and outlined in [21]."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "A first-order algorithm for the off-line minimization of a convex cost ϕ(x) := f(x) + g(x), x ∈ M, was studied in [39] (presented for convenience in Table 1), where M is a linear vector space; f is convex as\nwell as L-Lipschitz continuously differentiable; and g is convex but possibly non-smooth, e.g., the ℓ1norm. Auxiliary variables {ψi, ζi} ⊂ M are utilized to generate a sequence (ϕ(xi))i∈Z≥0 that converges as i → +∞ to the (global) minimum of ϕ with quadratic rate. The engine under the hood is the forward-backward (FB) [44] or proximal-gradient iteration of line 5, where the proximal mapping is defined as Proxβig : M → M : x 7→ argminξ∈M‖x − ξ‖2/2 + βig(ξ) for any βi ∈ R>0 [44]. The FB iteration splits operation on ϕ into two concatenated stages: Firstly on the differentiable f through the classical steepest-descent operator (Id−βi∇f), and secondly on g via Proxβig, which usually obtains closed-forms for the majority of regularizers g, e.g., Prox‖·‖1 boils down to the soft-thresholding operator [45]. If the FB iteration were performed with ψi+1 taking the place of ζi in line 5, then (ψi)i∈Z≥0 would converge to a minimizer of ϕ [44], but with no claims on quadratic rate of convergence. Towards establishing such claims, ζi of line 5 is convexly combined with xi−1 to form (1− λi)xi−1 + λiζi, which, together with line 6, guarantee that the values of ϕ are monotonically non-increasing: ϕ(xi) ≤ ϕ(xi−1). Parameters {ηi+1, λi+1} in line 2 are used to define stepsize βi+1 through line 3, offering the flexibility of a variable stepsize from the interval [(1− √ 1− ηi+1λi+1L)/L, (1+ √\n1− ηi+1λi+1L)/L] per iteration, as opposed to the rigid βi+1 = 1/L in [37, 38]. Instrumental in establishing quadratic rate of convergence\nis the sequence of positive coefficients (µi)i∈Z>0 of line 4 (cf. Fact 1 in Appendix A.3 where limi→∞ µi = +∞). Finally, line 7 links variables {xi−1, xi, ψi, ψi+1, ζi} together, and, as it will be shown later on, it facilitates performance analysis via helpful telescoping terms. Under proper parameter selection\n(ηi = βi = 1/L, λi = 1), the algorithm of Table 1 boils down to [38]. Moreover, with its guaranteed monotonically non-increasing behavior of cost values through line 6, and the flexibility offered by the\nvariable step-sizes (βi)i∈Z>0 in line 3, the algorithm in Table 1 has merits over [37]. Notwithstanding, neither [37, 38] nor [39] can offer guarantees on the convergence of the (primal) variables {xi, ψi, ζi}. The purpose of this study is to extend the merits of the algorithm in Table 1 to the much more general\nsetting where not only the underlying cost is time-varying, but it is per-block-convex, and several of its\nparameters are of stochastic nature.\nTo this end, and with reference to (3), consider a sequence of functions Ft of composite structure\nFt(x ;Ot) := ft(x ;Ot) + ∑B b=1 gb(x (b)), where x gathers all unknowns, split in B blocks of variables x := (x(1), . . . , x(B)), with x(b) belonging to a finite-dimensional linear space Mb with inner-product 〈· | ·〉 Mb ; ft exhibits per-block-convexity, i.e., ft is convex w.r.t. each of the blocks x (b) whenever the rest of them are fixed; and gb is a convex function used to regularize and account for prior information on each x(b). Symbol M stands for the Cartesian product M := ×Bb=1Mb. An inner product on M is defined as 〈x1 | x2〉M := ∑B b=1〈x (b) 1 | x (b) 2 〉Mb , ∀(x1,x2) ∈ M 2. Whenever clear from the context, subscripts Mb will be dropped from the inner-product symbols for notational convenience.\nFor any function Φ on M, notation Φ(x(b) | x(−b)) stresses the dependence of Φ onto the bth block of variables x(b), whenever the rest of them are fixed, with x(−b) denoting all but the bth blocks contained in x. Term Φ(x | x(−b)) serves also the previous purpose, but with superscript (b) dropped from x(b) to avoid overloading notation.\nLet Γ0(Mb) denote all proper, convex, and lower-semicontinuous (l.s.c.) functions defined on Mb with values in R∪{+∞} [44]. For any ϕ ∈ Γ0(Mb), the subdifferential ∂ϕ(x(b)) is defined as the set of all subgradients ϕ′(x(b)) of ϕ at x(b): ∂ϕ(x(b)) := {ϕ′(x(b)) ∈ Mb | 〈ϕ′(x(b)) | ξ−x(b)〉+ϕ(x(b)) ≤ ϕ(ξ),∀ξ ∈\nMb} [44]. If ϕ is differentiable at x(b), ∂ϕ(x(b)) = {∇bϕ(x(b))}, with ∇b denoting the gradient operator w.r.t. the bth block in x. In this context, per realization of the r.vs. in Ot, ft(· | x(−b) ;Ot) ∈ Γ0(Mb) is assumed [L\n(b) t := L (b) t (x (−b),Ot)]-Lipschitz continuously differentiable. Moreover, gb ∈ Γ0(Mb). Data Ot are considered to be r.vs. defined on a probability space (Ω,A,Pr) with E{·} := EA{·} denoting expectation w.r.t. the σ-algebra A [46]. IfO is the σ-subalgebra of A including all events related to data {Ot}t∈Z>0 , EO{·} stands for expectation w.r.t. O. Whenever block x(b) is viewed as an r.v., it is assumed to have finite second-order moment. In this context, Hb := {x : Ω → Mb | E{‖x‖2Mb} < +∞} turns out to be a Hilbert space with inner-product E{〈x1 | x2〉Mb}, ∀(x1, x2) ∈ H 2 b [44, Example 2.6]. To generalize, H := ×Bb=1Hb is also a Hilbert space with inner-product E{〈x1 | x2〉M}, ∀(x1,x2) ∈ H2. If X denotes a σ-subalgebra of A which includes all events related to all blocks of variables, then EX{·} stands for expectation w.r.t. X [46]. Moreover, EO|X{·} denotes conditional expectation w.r.t. O, conditioned on X [46]. Hereafter, it is assumed that X ∪O = A, so that E{·} = EA{·} = EX,O{·}."
    }, {
      "heading" : "3 Algorithm",
      "text" : "The algorithm of this section incorporates the acceleration module of Table 1 into the online learning setup of (3). Given that variables are split in blocks x = (x(1), . . . , x(b), . . . , x(B)), the proposed\nalgorithm takes advantage of the per-block convexity of the cost Ft and visits blocks of variables in a Gauss-Seidel or successive fashion. The basic principles of this modular algorithm are depicted\nin the block diagram of Fig. 1. Per iteration (time slot) t and given observed data Ot, the algorithm visits all blocks of variables successively to solve the per-block b convex minimization task minx(b)∈Mb ft(x (b) | x(−b)t )+ gb(x(b)). Symbol Ot is dropped from ft(x(b) | x (−b) t ;Ot) for notational convenience, and x (−b) t := (x (1) t , . . . , x (b−1) t , x (b+1) t−1 , . . . , x (B) t−1) comprises all updated blocks up to the (b− 1)st one, as well as the {b + 1, . . . , B} unvisited ones. If the solution to the previous minimization task\nis affordable both w.r.t. time and computational resources, then block b is updated by the obtained\nminimizer; otherwise, the acceleration module of Table 1 is run only for a finite number of iterations Rb, i.e., i ∈ {1, . . . , Rb} in the context of Table 1, and not infinitely often (i → +∞) as in the batch and off-line mode of [39]. Having the bth block updated, effort is put on the next (b+ 1)st one. Once\nall blocks have been updated, the previous procedure is repeated for the (t+ 1)st time instant, and so\non.\nSince the acceleration module of Table 1 is allowed to be employed for Rb times per (b, t), the time index τ (b) := (t− 1)Rb + rb, with rb ∈ {1, . . . , Rb}, is introduced here to account for this “overclocking” or finer -time-scaling of the original t-axis. Because data are originally observed according to the “t-\nclicks,” to abide by the τ -click notation, define Oτ (b) = O(t−1)Rb+rb := Ot, as well as the induced functions Fτ (b) = F(t−1)Rb+rb := Ft and fτ (b) = f(t−1)Rb+rb := ft.\nA more detailed version of the block diagram of Fig. 1, equipped with the τ -time notation, is given in Table 2. The acceleration module of Table 1, called here Accel(·), has been revised in Table 2b to abide by the notation which pertains to per-block b operation. Overclocking and acceleration can be\nseen in lines 7–13 of Table 2a. Symbol x (−b) τ in line 9 (the (b) superscript from τ (b) is omitted for notational convenience) stands for all but the bth blocks of variables, where only blocks {1, . . . , b− 1} have been updated. On the other hand, x (b)\nτ (b) in line 12 collects x\n(−b) τ (b) and the recently updated bth\nblock of variables. Symbol Iτ (b) stands for the input arguments of the acceleration module Accel(Iτ (b)) in line 11, which is expanded in Table 2b. These input arguments consist of only those parts of the cost which are affected by the bth block update; the value of x (b)\nτ (b)−1 at the previous time instant τ (b) − 1; as\nwell as the Accel(·)’s intrinsic variables {ψ(b) τ (b) , η (b) τ (b) }. Finally, xt in line 16 collects all B updated blocks at time t.\nImplementing only the first-order information of ft(· | x(−b)τ ) in (M5) equips the algorithm in Table 2 with a low computational footprint which scales linearly w.r.t. the number of unknown parameters. For\nspecificity, the computational complexities on two practical examples will be provided in Sec. 5."
    }, {
      "heading" : "4 Main Result",
      "text" : "The following assumptions will be instrumental in the subsequent discussion.\n[As0] (Stationarity.) Expectation F (x) := EO{Ft(x ;Ot)} is time-invariant.\n[As1] (Boundedness from below.) Functions Ft(x ;Ot) are bounded from below almost surely (a.s.).\n[As2] (Coercivity.) If limk→∞ E{‖ξk‖2} = +∞ for any (ξk)k∈Z≥0 ⊂ H, then limk→∞ E{F (ξk)} = +∞ [44].\n[As3] With (xτ )τ∈Z≥0 standing for the sequence of estimates of the algorithm in Table 2, and F∗\ndenoting the limit which appears in Thm. 1.1, ∃x(b)∗ ∈ Hb s.t. lim supτ→∞ E{F (x(b)∗ | x(−b)τ )} ≤ F∗.\n[As4] Given the sequence of Lipschitz coefficients (L (b) τ )τ∈Z≥0 produced by the algorithm in Table 2,\nthere exists L̂ ∈ R≥0 s.t. L(b)τ ≤ L̂ a.s. ∀(τ, b). Moreover, there exist τ (b)∗ ∈ Z≥0 and a sufficiently small δ(b) ∈ R>0 s.t. η(b)τ λ(b)τ L̂ ≤ 1− δ(b), ∀τ ≥ τ (b)∗ .\n[As5] If w (b) τ ∈ argminx(b)∈Hb E{F (x(b) | x (−b) τ )}, then (E{‖w(b)τ ‖2})τ∈Z≥0 is bounded.\nComments on the previous assumptions are in order. First, As0 can be recognized as one of the principal hypotheses in SA. As1 will be used to prevent cost values from sinking to −∞, and it is usually met in practice, e.g., any quadratic data-fit term as well as any vector-norm satisfy As1 due to non-\nnegativity. As2 will be used to prevent the proposed algorithm from generating unbounded sequences of\nestimates, without any a-priori enforcement of hard bounds on the variables, as in [42]. Examples where coercivity is introduced via {gb}Bb=1 will be given shortly in Sec. 5. With regard to As3, it will be shown in Lemma 3 that it is a necessary condition for properties related to (weak/strong sequential) cluster\npoints of (xτ )τ∈Z≥0 , as well as to the boundedness of (sub)gradients of the expected cost. The existence of a sufficiently large L̂, which upper-bounds the data-dependent Lipschitz coefficients in As4, is well-\nmotivated by the coercivity assumption As2 that promotes bounded sequences of iterates (cf. Thm. 1.2).\nThe clarification of the previous statement will be given through concrete examples in Remark 2, where\nthe boundedness of the resultant iterates, as well as an assumption on the boundedness of the moments\nof the observed data, justify the existence of L̂. As4 and the related performance analysis suggest also strategies for selecting {η(b)τ , λ(b)τ } (cf. Remark 1). It is important to stress here that the stepsize β (b) τ of the forward-backward iteration relies on the “local” Lipschitz coefficient L (b) τ and not on L̂, e.g., β (b) τ := 1/L (b) τ . Finally, As5 imposes a uniform bound, across time, on the second-order moment of (per-block) minimizers of the expected cost. This is the case if moments of the observed data (Ot)t∈Z>0 and block variables are bounded. In this sense, As5 is necessary here since the present framework does\nnot enforce hard bounds on block variables and follows the more relaxed coercivity assumption of As2.\nIt is also important to stress here that As5 is a condition on existence; there is no need of constructing\nsuch minimizers for the algorithm to operate.\nThe following lemma gathers a few helpful properties on the function E{F (· | x(−b))}.\nLemma 1. Per block b and for any realization of x(−b), function x(b) 7→ E{F (x(b) | x(−b))} is convex on Hb. Moreover, EO|X{F ′t (x(b);Ot | x(−b))} ∈ ∂ E{F (x(b) | x(−b))}. In other words, EO|X{F ′t (x(b);Ot | x(−b))} is a subgradient of E{F (· | x(−b))} at x(b). Further, under As0 and As1, E{F (·)} is bounded from below on H.\nProof. See Appendix A.1.\nThe following lemma sheds light on the connection between the selection of {η(b)τ , λ(b)τ } and the negativity of the quadratic polynomial in (M3).\nLemma 2. Under As4 there exists a sequence of stepsizes (β (b) τ )τ∈Z>0 and a δ̌ (b) ∈ R>0 s.t.\nL(b)τ β (b) τ 2 − 2β(b)τ + η(b)τ λ(b)τ ≤ −δ̌(b) , ∀τ ≥ τ∗ .\nProof. See Appendix A.2.\nThe main results of this paper are summarized in the following theorem.\nTheorem 1.\n1. Under As0 and As1, there exists F∗ to which (E{F (xt)})t∈Z>0 converges. In other words, ∃F∗ ∈ R s.t. F∗ = limt→∞ E{F (xt)} = limτ→∞ E{F (x(b)τ )}, ∀b ∈ {1, . . . , B}, where x(b)τ is defined in line 12 of Table 2a.\n2. Under As0, As1, and As2, sequences (xt)t∈Z>0 and (x (b) τ (b) )τ (b)∈Z≥0 inH, as well as (x (b) τ (b) )τ (b)∈Z≥0 in Hb\nare bounded. Consequently, the sets of weak sequential cluster points W{(xt)t∈Z>0}, W{(x (b) τ (b) )τ (b)∈Z≥0}, and W{(x(b) τ (b) )τ (b)∈Z≥0} are non-empty [44, Lem. 2.37]. Moreover, according to (M6) define\nT (b) ζ :=\n{ τ (b) ∈ Z≥0 ∣ ∣ ∣ x (b)\nτ (b) 6= x(b) τ (b)−1\n}\n.\nThen, sequence (ζ (b)\nτ (b) ) τ (b)∈T\n(b) ζ ⊂ Hb is bounded and W{(ζ(b)τ (b))τ (b)∈T(b) ζ } 6= ∅.\n3. Under As0–As3, E{F (x(b)τ )} enjoys a quadratic rate of convergence to F∗. More precisely, for any arbitrarily fixed ǫ ∈ R>0, there exists τ ′0 ∈ Z>0 s.t. ∀τ > τ ′0,\nE{F (x(b)τ )} − F∗\n≤ 4 λ (b) 1 2λ̌(b)2η̌(b)(1 + τ)2\n[\nη(b)τ0 µ (b) τ0 2 ( E{F (x(b)τ0 )} − F∗ )\n+ 1\n2 E {∥ ∥ ∥ ∥ ∥ η (b) τ0 λ (b) τ0 µ (b) τ0\nβ (b) τ0\nζ(b)τ0 + v (b) τ0 − λ (b) 1 x (b) ∗ ∥ ∥ ∥ ∥ ∥ 2}] + ǫ .\n4. Under As0, As1, and As4, ∑∞ τ=0 E{‖ζ (b) τ −ψ(b)τ ‖2} < +∞. Necessarily, limτ→∞ E{‖ζ(b)τ −ψ(b)τ ‖2} = 0.\n5. Under As0, As1, and As4,\nlim τ→∞\nE { ‖EO|X{F ′τ (ζτ | x(−b)τ ;Oτ )}‖2 } = 0.\nIn other words, according to Lemma 1, the subgradient EO|X{F ′t (ζ (b) τ | x(−b)τ ;Oτ )} of E{F (· | x(−b)τ )} at ζ (b) τ converges to 0 in the mean-squared sense.\n6. Under As0–As2, and As4–As5,\nF∗ = lim τ∈T\n(b) ζ\nE{F (ζ(b)τ | x(−b)τ )}\n= lim sup τ→∞ min x(b)∈Hb\nE{F (x(b) | x(−b)τ )} .\nThe last equation implies that there exists a subsequence (x (b) τk )k∈Z≥0 that satisfies the following property: For any arbitrarily small ǫ ∈ R>0, there exists a k0 s.t. ∀k ≥ k0,\nE{F (x(b)τk | x (−b) τk )} − min x(b)∈Hb E{F (x(b) | x(−b)τk )} ≤ ǫ .\nIn other words, there exists (x (b) τk )k∈Z≥0 onto which E{F (x (b) τk | x (−b) τk )} approximate arbitrarily close the per-block minima minx(b)∈Hb E{F (x (b) | x(−b)τk )}.\nProof. The proof is given in Appendix A.3.\nTo show that As3 is a rather weak assumption, the following Lemma 3 demonstrates that As3 is\nnecessary to the more “conventional” As6a and As6b1 on weak and strong sequential cluster points of\nthe sequence of r.vs. (xt)t∈Z>0 . More specifically, As6b assumes existence of a strong sequential cluster point and bounds a sequence of subgradients of the expected cost, similarly to the bound on gradients\nintroduced in [47].\nLemma 3. Under As0–As2, if E{F (·)} is also l.s.c. on H, then As3 is necessary to As6a. Moreover, under As0–As2, As3 is also necessary to As6b.\n1Both As6a and As6b are placed in Appendix A.4 for not disrupting the flow of the present discussion.\nProof. See Appendix A.4.\nRemark 1. Setting λ (b) τ := 1 for simplicity, As4 suggests that η (b) τ should be sufficiently small for η (b) τ ≤ (1 − δ(b))/L̂, given δ(b) ∈ (0, 1) and assuming that L̂ is available. Without having knowledge of L̂, selection rules for η (b) τ are (i) η (b) τ := η̌(b), and (ii) η (b) τ := η̌(b)+1/τ , after choosing a sufficiently small η̌(b) > 0 [cf. (M2)]. Notice that the previous rules abide by the monotonicity of η (b) τ , i.e., η (b) τ+1 ≤ η (b) τ , in (M2). In practice, and in the numerical tests of Sec. 6, the following rule is adopted:\nλ(b)τ := 1, η (b) τ := min\n{\nη (b) τ−1,\n1− δ(b)\nL (b) τ\n, η̌(b) + 1\nτ\n}\n. (4)\nIt is important to stress here that the selection of stepsize β (b) τ , in the forward-backward iteration of (M3), is based on the “local” coefficient L (b) τ and not on L̂. Further discussion on L̂ is provided in Remark 2."
    }, {
      "heading" : "5 Examples",
      "text" : "Two concrete examples of practical interest follow."
    }, {
      "heading" : "5.1 Total least-squares",
      "text" : "Data (yt)t∈Z>0 are generated by yt = u ⊤ ∗ts∗+vt, where (u∗t, s∗) ∈ RQ×RQ; the unknown s∗ is sparse; ⊤ denotes transposition; and vt stands for noise. Observed data are Ot := {yt,ut}, where ut := u∗t−et is a noisy version of u∗t, and no statistical information on the process (et)t∈Z>0 is available. Motivated by the TLS criterion and the resultant errors-in-variables (EIV) modeling approach [7, 16], the following\nsequence of per-block-convex costs is considered:\nFt(s,e;Ot) := 1\n2\n[ yt − (ut + e)⊤s ]2 + λs2 2 ‖s‖2 ︸ ︷︷ ︸\n=: ft(s,e;Ot)\n+ λs1‖s‖1 ︸ ︷︷ ︸\n=: g1(s)\n+ λe 2 ‖e‖2\n︸ ︷︷ ︸\n=: g2(e)\n, (5)\nwhere the first quadratic term in (5) quantifies fitness to the observed data, with e modeling EIV; ‖ · ‖1 promotes sparsity on s; ‖e‖2 penalizes large entries of e; and ‖s‖2 is used to regularize the cost in (5) by imposing coercivity (cf. As2). To draw connections with Sec. 2, M1 := M2 := R Q, and x(1) := s, x(2) := e.\nIf {yt,ut} are (jointly) wide sense stationary, then As0 holds with F (s,e) := EO{Ft(s,e;Ot)}. Due to the non-negativity of all terms in (5), it can be readily verified that As1 is also satisfied.\nIn the case where both s and e are considered as r.vs., x := (s,e) ∈ H, λs2 6= 0, λe 6= 0, and (5) suggest that Ft(x;Ot) ≥ λs2‖s‖2/2 + λe‖e‖2/2, and under As0, E{F (x)} = EX{EO|X{Ft(x;Ot)}} ≥ λs2 E{‖s‖2}/2 + λe E{‖e‖2}/2; hence, As2 holds.\nMoreover, it can be verified by standard algebraic manipulations that a Lipschitz coefficient of ∇sft is L\n(s) t (e,ut) = ‖(ut+ e)(ut+ e)⊤ +λs2IQ‖ ≤ ‖(ut+ e)(ut+ e)⊤‖F +λs2‖IQ‖F = ‖ut+ e‖2+λs2\n√ Q,\nwhere ‖A‖ denotes the spectral norm of a matrix A. Moreover, a Lipschitz coefficient of ∇eft is L (e) t (s) = ‖ss⊤‖ ≤ ‖ss⊤‖F = ‖s‖2."
    }, {
      "heading" : "5.2 Semi-supervised dictionary learning",
      "text" : "Following [8], consider an undirected graph G(V, E), where V denotes the set of all vertices or nodes,\nwith cardinality V, and E is the set of all edges. Connectivity and edge strengths of G are described by the adjacency matrix W ∈ RV×V, where [W ]ij > 0 if nodes νi and νj are connected, while [W ]ij = 0 otherwise. Per t and node ν, r.v. χtν : Ω → R describes a network-wide dynamical process of interest, e.g., traffic load. All r.vs. are collected in χt := [χt1, . . . , χtV]. A succinct representation of the process over G models χt as a superposition of “few” atoms in a dictionary D ∈ RV×Q, Q ≥ V: χt = Dst, where st ∈ RQ is sparse. Further, only a few entries of χt are observed. Such a missing-entries scenario is conceivable in cases where not all of {χtν}Vν=1 are observable due to privacy constraints, severely corrupted measurements, node failures, or, data collection costs. To this end, let the random masking matrix Mt ∈ RM×V, M < V, whose mth row is the transpose of a canonical basis vector for RV; in other words, Mtχt selects M out of V entries of χt. To summarize, yt = MtDst + vt, with observed data Ot := {yt,Mt} and vt denoting noise. To enable imputation of missing entries, the topology of G is utilized. Spatial correlation of the network is captured by the Laplacian matrix L := diag(W1V)−W , where diag(a) defines the diagonal matrix whose main diagonal entries are those of vector a, and 1V ∈ RV is the all-one vector. Given a “forgetting factor” δ ∈ (0, 1] to gradually diminish the effect of past data, define the per-block-convex cost\nFt(s,D)\n:=\n=: ft(s,D;Ot) ︷ ︸︸ ︷ t∑\nτ=1\nδt−τ‖yτ −MτDs‖2 2∆t + λL 2 s⊤D⊤LDs+ λs2 2 ‖s‖2\n+ λs1‖s‖1 ︸ ︷︷ ︸\n=: g1(s)\n+ ιD(D) ︸ ︷︷ ︸\n=: g2(D)\n, (6)\nwhere ∆t := ∑t τ=1 δ t−τ ; ‖s‖2 and ‖s‖1 are as in (5), while the term including L quantifies prior knowledge on the topology of G, promotes “smooth” solutions over strongly connected nodes of G, and\nis instrumental in imputing missing entries [8].\nTo establish links with the introductory discussion, M1 := R Q (x(1) := s), with 〈· | ·〉\nM1 being the\ndot-vector product, and M2 := R V×Q (x(2) := D), with 〈D1 | D2〉M2 := trace(D⊤1 D2), ∀(D1,D2) ∈ M22 . If expectations in (6) are invariant w.r.t. t, then As0 holds with F (s,D) := EO{Ft(s,D;Ot)}. The non-negativity of all terms in (6) guarantees that As1 is also satisfied. Further, by following a similar argument as in Sec. 5.1, for any λs2 6= 0, E{F (·)} satisfies As2. Standard algebra suggests that a Lipschitz coefficient of ∇sft is L(s)t (D,At) = ‖D⊤AtD+λs2IQ‖ ≤\n‖D⊤AtD+λs2IQ‖F ≤ ‖D‖2F‖At‖F+λs2 √ Q, whereAt := ∑t τ=1 δ\nt−τM⊤τ Mτ/∆t+λLL. By∇Dft(D) = AtDss ⊤−∑tτ=1 δt−τM⊤τ yτs⊤/∆t, it can be also verified that a Lipschitz constant of∇Dft is L (D) t (s,At) = ‖s‖2‖At‖F.\nThe computational complexities of the algorithm in Table 2 on examples of Secs. 5.1 and 5.2,\nincluding computations of Lipschitz constants and function evaluations, are linear w.r.t. to the number\nof unknown variables, and more specifically, in the order of O[(R1 +R2)Q] and O[(R1 +R2)(Q+V)V] per t, respectively.\nRemark 2. With regard to the selection of L̂ in As4, recall that Markov’s inequality dictates that Pr(L (b) τ ≥ L̂) ≤ E{L(b)τ }/L̂, for any L̂ [46]. Provided that there exists Λ̂ s.t. E{L(b)τ } < Λ̂, one can arbitrarily decrease the measure of the event {ω ∈ Ω | L(b)τ (ω) ≥ L̂} by choosing a sufficiently large L̂. Examples for which E{L(b)τ } < Λ̂ can be found in this section; regarding Sec. 5.1, it is straightforward to verify that E{L(s)τ (eτ ,uτ )} ≤ 2E{‖uτ‖2} + 2E{‖eτ‖2} + λs2 √ Q. Hence, under Thm. 1.2 and the assumption that E{‖uτ‖2} < +∞, there exists Λ̂ < ∞ s.t. E{L(s)τ (eτ ,uτ )} ≤ Λ̂, ∀τ . It can be also verified that the previous discussion carries over to the Lipschitz coefficients of Sec. 5.2 in a similar way."
    }, {
      "heading" : "6 Numerical Tests",
      "text" : ""
    }, {
      "heading" : "6.1 Synthetic data",
      "text" : "To validate the algorithm of Table 2 on the example of Sec. 5.1, entries of (u∗t := u∗, s∗) are drawn independently from a zero-mean, unit-variance Gaussian r.v. To make s∗ sparse, its nonzero entries are placed randomly in s∗ following a uniform distribution under two scenarios, a low-dimensional one corresponding to (Q, ‖s∗‖0) = (100, 10), and a high-dimensional one with (Q, ‖s∗‖0) = (103, 100), tagged “low-d” and “high-d” in Figs. 2a, 2b, and 2c, respectively. Noise vt is considered to be zero-mean and i.i.d. Gaussian, with variance 10−2. Regressor vectors (ut)t∈Z>0 are observed after an i.i.d. zeromean Gaussian process with variance 10−4 is added to u∗. Parameters (λs1, λs2, λe) = (10 −5, 10−1, 1) are used in (5), common to all employed methods. Minimization w.r.t. block e accepts a closed-form solution; given s, the minimizer of (5) w.r.t. e is ê = (yt − u⊤t s)(ss⊤ + λeIQ)−1s, where inversion is facilitated by the matrix inversion lemma as (ss⊤ + λeIQ)\n−1 = [IQ − ss⊤/(λe + ‖s‖2)]/λe. It is worth noticing here that R1 = 3 for the inner loop in Table 2 (lines 7–13). Parameters {η(1)τ , λ(1)τ } follow (4).\nThe algorithm in Table 2 is tested against a block-version of the classical online (sub)gradient\ndescent method [25], tagged as BOGD in Fig. 2. BOGD adopts the Gauss-Seidel strategy of visiting\nblocks; per t, the standard subgradient descent step is applied first w.r.t. s with constant step size 10−3, followed by a minimization step w.r.t. block e which is given also here by the closed-form solution\nemployed in the proposed scheme. Moreover, a block coordinate descent (BCD) strategy is validated,\nwhere (5) is “maximally” separated in scalar-valued blocks w.r.t. s. More specifically, following the Gauss-Seidel scheme and letting index b ∈ {1, . . . , Q} visit the bth entry of s successively, having fixed e and the entries {sj}j 6=b of s, minimization of (5) w.r.t. sb amounts to the scalar-valued optimization task ŝb := argminsb [yt − ∑\nj 6=b(utj + ej)sj − (utb + eb)sb]2/2 + λs2s2b + λs1|sb|, which can be solved in closed form using the soft-thresholding operator as\nŝb =\n{ (utb+eb)θb−sgn[(utb+eb)θb]λs1\n(utb+eb)2+λs2 , |(utb + eb)θb − λs1| > 0\n0, otherwise .\nNotice that θb := yt− ∑ j 6=b(utj + ej)sj , while utj denotes the jth entry of ut. After all entries of s are updated, the closed form solution ê, leveraged in the proposed and BOGD techniques, updates block\n(a) Cost function values vs. time.\n(b) Normalized deviation on the complement of supp(s∗).\ne to conclude step t of BCD. For fairness, both BOGD and BCD run three consecutive iterations per t\nto meet the computational load of the proposed scheme where R1 = 3.\nFigs. 2a, 2b, and 2c illustrate the performance of the employed methods. Fig. 2a depicts the cost\nfunction values (5) across time; Fig. 2b shows the per entry deviation [ ∑\nj /∈supp(s∗) (stj − s∗j)2]1/2/(Q−\n‖s∗‖0) across t, where supp(s∗) stands for the support of s∗, i.e., all those indexes j s.t. s∗j 6= 0; and Fig. 2c plots the time-variations of the subgradient norm of the cost. Curves in Figs. 2a, 2b, and 2c\nare obtained after averaging uniformly 100 realizations, and are illustrated in log-log scale for easily\nidentifying the rate of convergence. Numerical results corroborate Thm. 1.3 which states that there\nexists a time instant after which the proposed algorithm converges with quadratic rate to an arbitrarily\nsmall neighborhood around F∗. The behavior of BOGD confirms the fact that (sub)gradient techniques are in general slow convergent."
    }, {
      "heading" : "6.2 Real data",
      "text" : "In the context of Sec. 5.2, the advocated algorithm is validated on estimating and tracking network-wide\nlink loads taken from the Internet2 measurement archive [48]. Analyzing the Internet2 backbone network\nyields a graph Gwith V = 54 number of vertices. Using the network topology and routing information, network-wide link loads (χt) 30,000 t=1 ⊂ RV become available (in Gbps). Per time slot t, only M = 30 of the χt components, chosen randomly via Mt ∈ RM×V, are observed in yt ∈ RM . The cardinality of\nthe time-varying dictionaries is set constant to Q = 80. To cope with pronounced temporal variations\nof the Internet2 link loads, the forgetting factor δ in Sec. 5.2 is set equal to 0.95. Initial values for both\n(s,D) are randomly drawn from the feasibility regions seen in Sec. 5.2. Parameters in (6) are defined as (λL, λs1, λs2) = (10 −3, 10−3, 10−3). Moreover, as in Sec. 6.1, {η(b)τ , λ(b)τ } follow (4).\nThe advocated algorithm is tested against the state-of-the-art scheme in [8] which relies on a Gauss-\nSeidel alternating minimization scheme: (i) ADMM [29,30] is employed to minimize a cost closely related\nto (6) w.r.t. s, with the same parameters (λL, λs1, λs2) as in (6), and (ii) BCD iterations requiring matrix inversions are leveraged to optimize the associated loss w.r.t. D. Fig. 2d depicts the normalized squared estimation error between the true χt and the inferred χ̂t, namely ‖χt− χ̂t‖2/‖χt‖2, versus time t for a randomly chosen network link. For visualization reasons, only a small portion of the data is shown in\nFig. 2d. To obtain computationally light recursions, the number of inner loops in Table 2 w.r.t. s is set\nequal to R1 = 2, while R2 = 5 w.r.t. D. It is worth noticing here that ADMM in [8] requires multiple iterations to achieve a prescribed estimation accuracy, and that no matrix inversion was incorporated in\nthe realization of Table 2. The proposed method and [8] perform similarly, scoring mean (normalized)\nestimation errors of 0.1166 and 0.1161 on the entire dataset of cardinality 30, 000, respectively."
    }, {
      "heading" : "7 Conclusions",
      "text" : "This manuscript presented a modular online learning algorithm which extended arguments, originally\ndeveloped for accelerating first-order methods in batch convex optimization tasks, to the per-block-\nconvex and stochastic approximation context. The proposed framework showed a computational com-\nplexity that scales linearly w.r.t. the number of unknowns. Assuming no knowledge of the underlying\ndata statistics, the convergence rate of the expected loss on the resultant iterates was proved to be\nquadratic. Rigorous theoretical analysis was performed in the Hilbert space of r.vs. of finite second-order\nmoments. The framework was tested on two instances of broad practical interest: (i) Sparsity-aware\nregression based on the TLS criterion; and (ii) semi-supervised DL for network-wide link load tracking\nand imputation. Numerical tests on synthetic and real data demonstrated that the proposed algorithm\nperforms better than BCDMs and comparably to state-of-the-art but computationally heavier ADMM-\nbased methods. Future directions include the extension of the proposed framework from Gauss-Seidel\nstrategies of visiting blocks of variables to parallel and random ones. Moreover, to study the effect\nof data non-stationarities, a regret analysis on the per-block convex loss will be presented in a future\nsubmission."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Proof of Lemma 1\nSince x(b) 7→ Ft(x(b) | x(−b);Ot) is convex on Mb for any realizations of x(−b) and Ot, then ∀λ ∈ [0, 1], and ∀(x(b)1 , x (b) 2 ) ∈ M2b , Ft(λx (b) 1 + (1 − λ)x (b) 2 | x(−b);Ot) ≤ λFt(x (b) 1 | x(−b);Ot) + (1 − λ)Ft(x (b) 2 | x(−b);Ot). Applying EX{EO|X{·}} to both sides of the previous inequality yields the convexity of E{F (· | x(−b))} on Hb.\nThe convexity of Ft(· | x(−b);Ot) implies that for any x(b) ∈ Mb, Ft(x(b) | x(−b);Ot) +〈F ′t(x(b) | x(−b);Ot) | ξ − x(b)〉 ≤ Ft(ξ | x(−b);Ot), ∀ξ ∈ Mb. Application of EO|X{·} to both sides of\nthe previous inequality results in F (x(b) | x(−b)) + 〈EO|X{F ′t (x(b) | x(−b);Ot)} | ξ − x(b)〉 ≤ F (ξ | x(−b)). An additional application of E{·} yields E{F (x(b) | x(−b))}+E{〈EO|X{F ′t (x(b) | x(−b);Ot)} | ξ−x(b)〉} ≤ E{F (ξ | x(−b))}, or, EO|X{F ′t (x(b) | x(−b);Ot) ∈ ∂ E{F (x(b) | x(−b))}. Notice finally that it can be trivially verified by As1 that E{F (·)} is bounded from below on H.\nA.2 Proof of Lemma 2\nIt can be verified that minβ∈R{L(b)τ β2 − 2β + η(b)τ λ(b)τ } = (η(b)τ λ(b)τ L(b)τ − 1)/L(b)τ , which is attained at β∗ = 1/L (b) τ . Due to As4, (η (b) τ λ (b) τ L (b) τ − 1)/L(b)τ ≤ (η(b)τ λ(b)τ L̂− 1)/L(b)τ ≤ (η(b)τ λ(b)τ L̂− 1)/L̂ ≤ −δ(b)/L̂. Hence, the choices of β (b) τ := 1/L (b) τ and δ̌(b) := δ(b)/L̂ are sufficient to establish the claim of Lemma 2. Moreover, due to the continuity of the function L (b) τ β2 − 2β + η(b)τ λ(b)τ w.r.t. β, one can always find a neighborhood of 1/L (b) τ onto which the claim of Lemma 2 also holds for, let’s say, δ̌(b)/2.\nA.3 Proof of Theorem 1\nFirst, a fact is in order.\nFact 1 ([39, Lem. 1]).\n1. Let ϕ := f + g, where (f, g) ∈ Γ0(M)2, and f is Lf -Lipschitz continuously differentiable. For any β ∈ R>0, define ζψ := Proxβg[ψ − β∇f(ψ)], ∀ψ ∈ M. Assume that ∃(ψ, ξ, w) ∈ M3, ∃β ∈ R>0, and ∃λ ∈ [0, 1] s.t. ϕ(ξ) ≤ ϕ((1 − λ)w + λζψ). Then, ∀x ∈ M, ∀L ≥ Lf ,\nϕ(ξ) ≤ (1− λ)ϕ(w) + λϕ(x)− λ β 〈ζψ − ψ | ψ − x〉\n+ λ (L 2 − 1 β ) ‖ζψ − ψ‖2 .\n2. Given (λi)i∈Z>0 ⊂ (0, 1], the sequence (µi)i∈Z>0 defined in line 4 of Table 1 satisfies µi+1 > µi and µi ≥ λ1(1 + ∑i i′=1 λi′)/2, ∀i ∈ Z>0.\nRemark 3. Fact 1.1 holds for any over-estimate L of the Lipschitz coefficient Lf . This flexibility is inherited by the subsequent performance analysis, and it facilitates computations in Table 2 in cases\nwhere computing Lf requires considerable effort; cf. Sec. 5 where the smallest Lipschitz coefficients requires computation of the spectral norm of a matrix, while an over-estimate is provided by the\nmanageable Frobenius norm. Under these considerations, it will be assumed that in all of the subsequent discussion there exists a sufficiently small Ľ ∈ R>0 that stands as a lower bound on all employed Lipschitz coefficients.\nThe proof of Thm. 1 now follows. Symbol Ot is omitted to avoid overloading notations. For the\nsame reason, superscript (b) is often omitted from τ (b). 1) By (M6), and the definition of x (−b) τ given in Table 2a (line 9),\nFt ( x(b)τ | x(−b)τ ) ≤ Ft ( x (b) τ−1 | x(−b)τ ) = Ft ( x (b) τ−1 | x (−b) τ−1 ) . (7)\nFor τ ∈ {(t−1)Rb+2, . . . , tRb}, the previous inequality yields Ft(x(b)tRb | x (−b) tRb ) ≤ Ft(x(b)(t−1)Rb+1 | x (−b) (t−1)Rb+1 ). Moreover, by Table 2a (line 11), Ft(x (b) (t−1)Rb+1 | x(−b)(t−1)Rb+1) ≤ Ft(x (b) (t−1)Rb | x(−b)(t−1)Rb+1). Notice now by\nthe definition in Table 2a (line 12), that if {x(b)(t−1)Rb} is combined with x (−b) (t−1)Rb+1 , then x (b−1) tRb−1 is obtained. Hence, the previous arguments summarize to Ft(x (b) tRb ) = Ft(x (b) tRb | x(−b)tRb ) ≤ Ft(x (b−1) tRb−1 ). If b assumes all consecutive values in {1, . . . , B}, then the previous inequality yields Ft(xt) = Ft(x(B)tRB ) ≤ Ft(x (0) tR0 ) = Ft(xt−1), where x (0) tR0 := xt−1 was used in the last equality. Consequently, by As0, E{F (xt)} = EX EO|X{Ft(xt)} ≤ EX EO|X{Ft(xt−1)} = E{F (xt−1)}. This suggests that the boundedfrom-below sequence (E{F (xt)})t∈Z>0 is non-increasing; thus convergent. In other words, ∃F∗ ∈ R s.t. F∗ = limt→∞ E{F (xt)}.\nWith reference to (7) and the preceding arguments, ∀τ ∈ {(t− 1)Rb + rb | rb ∈ {1, . . . , Rb}},\nE{F (xt)} ≤ E { F ( x(b)τ | x(−b)τ )} = E { F ( x(b)τ )}\n≤ E { F ( x (b) τ−1 | x (−b) τ−1 )} = E { F ( x (b) τ−1 )} ≤ E{F (xt−1)} .\nHence, since (E{F (xt)})t∈Z>0 converges to F∗, so does also the non-increasing (E{F (x (b) τ )})τ∈Z≥0 . 2) Due to As2 and the existence of F∗ by Thm. 1.1, the sequences (xt)t∈Z>0 and (x (b)\nτ (b) )τ (b) are necessarily\nbounded. Moreover, due to the definition of H, boundedness of (x (b)\nτ (b) )τ (b) implies also boundedness of\nthe block-sequence (x (b) τ (b) )τ (b) ⊂ Hb, ∀i. Now, ∀τ ∈ T (b) ζ ,\nE { ‖ζ(b)τ ‖2 }\n= E\n \n ∥ ∥ ∥ ∥ ∥ 1\nλ (b) τ\n( x(b)τ − (1− λ(b)τ )x(b)τ−1 ) ∥ ∥ ∥ ∥ ∥\n2 \n\n\n≤ 2 (λ (b) τ )2 E { ‖x(b)τ ‖2 } +\n2(1 − λ(b)τ )2\n(λ (b) τ )2\nE { ‖x(b)τ−1‖2 }\n≤ 2∆̂ (λ (b) τ )2 ≤ 2∆̂ (λ̌(b))2 ,\nwhere ∆̂ ≥ supτ∈Z≥0 E{‖x (b) τ ‖2}. As such, (ζ(b)τ )τ∈T(b)\nζ is bounded, and, consequently, W{(ζ(b)τ )τ∈T(b) ζ } 6= ∅ [44, Lem. 2.37]. 3) The following proof is based on the one developed in [39] for the off-line, convex analytic case. The\nsubsequent one offers a generalization in the context of the present online/stochastic setup.\nBy (M6) in Table 2, Fτ+1(xτ+1 | x(−b)τ+1 ) ≤ Fτ+1((1 − λτ+1)xτ + λτ+1ζτ+1 | x (−b) τ+1 ). Fact 1.1 will be applied here with the convex Fτ+1(· | x(−b)τ+1 ) taking the place of ϕ, and (ψτ+1, xτ+1, xτ , ζτ+1) that of (ψ, ξ, w, ζψ). Let also x := xτ and an arbitrarily fixed x̄ (b) ∈ Mb in Fact 1.1 to obtain\nFτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )\n≤ Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )\n− λτ+1 βτ+1 〈ζτ+1 − ψτ+1 |ψτ+1 − xτ 〉\n+ λτ+1 (Lτ+1 2 − 1 βτ+1 ) ‖ζτ+1 − ψτ+1‖2. (8)\nAnother application of Fact 1.1 with x = x̄(b) yields\nFτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )\n≤ (1− λτ+1) [ Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )]\n− λτ+1 βτ+1\n〈 ζτ+1 − ψτ+1 ∣ ∣ ∣ψτ+1 − x̄(b) 〉\n+ λτ+1 (Lτ+1 2 − 1 βτ+1 ) ‖ζτ+1 − yτ+1‖2 . (9)\nMultiplying (8) by µτ+1(µτ+1 − λ1) ≥ 0 and (9) by µτ+1λ1 ≥ 0, and adding the resultant inequalities,\nµ2τ+1\n[\nFτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1\n)]\n≤ µτ+1(µτ+1 − λ1λτ+1) × [ Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )]\n− 〈 λτ+1µτ+1\nβτ+1\n( ζτ+1 − ψτ+1 )\n︸ ︷︷ ︸ =: aτ\n∣ ∣ ∣ ∣\n(µτ+1 − λ1) ( ψτ+1 − xτ ) + λ1 ( ψτ+1 − x̄(b) ) ︸ ︷︷ ︸\n=: bτ\n〉\n+ µ2τ+1λτ+1 (Lτ+1 2 − 1 βτ+1 ) ‖ζτ+1 − ψτ+1‖2 .\nApplication of 〈aτ | bτ 〉 = (‖ηaτ + bτ‖2 − ‖bτ‖2 − ‖ηaτ‖2)/(2η), ∀η ∈ R>0, to the previous inequality and vτ := µτ (1− ητλτ/βτ )ψτ − (µτ − λ1)xτ−1 yield\nµ2τ+1\n[\nFτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1\n)]\n≤ µτ+1(µτ+1 − λ1λτ+1) × [ Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )]\n− 1 2ητ+1\n∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1\nζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n+ 1\n2ητ+1\n∥ ∥ ∥µτ+1ψτ+1 − (µτ+1 − λ1)xτ − λ1x̄(b) ∥ ∥ ∥ 2\n+ 1\n2ητ+1\n∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1 ( ζτ+1 − ψτ+1\n) ∥ ∥ ∥ ∥ 2\n+ µ2τ+1λτ+1 (Lτ+1 2 − 1 βτ+1 ) ‖ζτ+1 − ψτ+1‖2 . (10)"
    }, {
      "heading" : "It can be verified by (M7) that µτ+1ψτ+1 − (µτ+1 − λ1)xτ = ητλτµτζτ/βτ + µτ (1 − ητλτ/βτ )ψτ −",
      "text" : "(µτ − λ1)xτ−1. Notice also that (M4) is equivalent to µ2τ = µτ+1(µτ+1 − λ1λτ+1). Incorporating these\narguments and (M3) into (10),\nητ+1µ 2 τ+1\n[\nFτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1\n)]\n≤ ητ+1µ2τ [ Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )]\n− 1 2 ∥ ∥ ∥ ∥ ητ+1λτ+1µτ+1 βτ+1 ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n+ 1\n2\n∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n+ ητ+1µ\n2 τ+1λτ+1\n2β2τ+1 (Lτ+1β\n2 τ+1 − 2βτ+1 + ητ+1λτ+1)\n× ‖ζτ+1 − ψτ+1‖2 (11) ≤ ητ+1µ2τ [ Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )]\n− 1 2 ∥ ∥ ∥ ∥ ητ+1λτ+1µτ+1 βτ+1 ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n+ 1\n2\n∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2 .\nThe previous inequality suggests that\nητ+1µ 2 τ+1\n[\nEO|X { Fτ+1 ( xτ+1 | x(−b)τ+1 )} − EO|X { Fτ+1 ( x̄(b) | x(−b)τ+1 )}]\n+ 1\n2 EO|X\n{∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1 ζτ+1\n+ vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2}\n≤ ητ+1µ2τ [ EO|X { Fτ+1 ( xτ | x(−b)τ+1 )}\n− EO|X { Fτ+1 ( x̄(b) | x(−b)τ+1\n)}]\n+ 1\n2 EO|X\n{∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}\nwhich, according to As0, results in\nητ+1µ 2 τ+1\n[\nF ( xτ+1 | x(−b)τ+1 ) − F∗ + F∗ − F ( x̄(b) | x(−b)τ+1\n)]\n+ 1\n2 EO|X\n{∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1 ζτ+1\n+ vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2}\n≤ ητ+1µ2τ [ F ( xτ | x(−b)τ+1 ) − F∗ + F∗ − F ( x̄(b) | x(−b)τ+1 )]\n+ 1\n2 EO|X\n{∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2} .\nAfter some elementary algebra,\nητ+1µ 2 τ+1\n[\nF ( xτ+1 | x(−b)τ+1 ) − F∗\n]\n+ 1\n2 EO|X\n{∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1 ζτ+1\n+ vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2}\n≤ ητ+1µ2τ [ F ( xτ | x(−b)τ+1 ) − F∗ ]\n+ 1\n2 EO|X\n{∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}\n+ ητ+1(µ 2 τ+1 − µ2τ )\n[\nF ( x̄(b) | x(−b)τ+1 ) − F∗\n]\n≤ ητ+1µ2τ [ F ( xτ | x(−b)τ+1 ) − F∗ ]\n+ 1\n2 EO|X\n{∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}\n+ η0(µ 2 τ+1 − µ2τ )\n[\nF ( x̄(b) | x(−b)τ+1 ) − F∗\n]\n.\nConsequently,\nητ+1µ 2 τ+1\n[ E{F (x(b)τ+1)} − F∗ ]\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1\nζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2}\n≤ ητ+1µ2τ [ E { F ( x(b)τ | x(−b)τ+1 )} − F∗ ]\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}\n+ η0(µ 2 τ+1 − µ2τ )\n[\nE { F ( x̄(b) | x(−b)τ+1 )} − F∗\n]\n≤ ητ+1µ2τ [ E{F (x(b−1)τ+1 )} − F∗ ]\n(12a)\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}\n+ η0(µ 2 τ+1 − µ2τ )\n[\nE { F ( x̄(b) | x(−b)τ+1 )} − F∗\n]\n≤ ητ+1µ2τ [ E{F (x(b)τ )} − F∗ ]\n(12b)\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}\n+ η0(µ 2 τ+1 − µ2τ )\n[\nE { F ( x̄(b) | x(−b)τ+1 )} − F∗\n]\n≤ ητµ2τ [ E{F (x(b)τ )} − F∗ ]\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2}\n+ η0(µ 2 τ+1 − µ2τ )\n[\nE { F ( x̄(b) | x(−b)τ+1 )} − F∗\n]\n, (12c)\nwhere Table 2a (line 12) and the monotonicity in Thm. 1.1 were used in (12a) and (12b), while (M2)\nwas utilized in (12c).\nLet now ϑ(x (b) ∗ ) := lim supτ→∞ E{F (x(b)∗ | x(−b)τ )}, where x(b)∗ was defined in As3. Since (12c) holds for any x̄(b), by x̄(b) := x (b) ∗ and the definition of lim sup, it is straightforward to verify that ∀ǫ > 0, there exists τ ′0 ∈ Z≥0 s.t. ∀τ ≥ τ ′0,\nητ+1µ 2 τ+1\n[ E{F (x(b)τ+1)} − F∗ ]\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1\nζτ+1 + vτ+1 − λ1x(b)∗ ∥ ∥ ∥ ∥\n2 }\n≤ ητµ2τ [ E{F (x(b)τ )} − F∗ ]\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x(b)∗ ∥ ∥ ∥ ∥\n2 }\n+ η0(µ 2 τ+1 − µ2τ )\n[\nϑ ( x (b) ∗ ) − F∗ + ǫ′\n]\n,\nwhere ǫ′ := ǫη̌/η0. Hence, by As3,\n0 ≤ ητ+1µ2τ+1 [ E{F (x(b)τ+1)} − F∗ ]\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1\nζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥\n2 }\n≤ ητ0µ2τ0 [ E{F (x(b)τ0 )} − F∗ ]\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητ0λτ0µτ0 βτ0\nζτ0 + vτ0 − λ1x̄(b) ∥ ∥ ∥ ∥\n2 }\n+ η0ǫ ′\nτ∑\nτ ′=τ ′0\n( µ2τ ′+1 − µ2τ ′ ) ,\nand\n0 ≤ [ E{F ( x (b) τ+1 ) } − F∗ ]\n+ 1\n2ητ+1µ2τ+1 E\n{∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1 ζτ+1\n+ vτ+1 − λ1x(b)∗ ∥ ∥ ∥ ∥\n2 }\n≤ 1 ητ+1µ2τ+1\n[\nητ0µ 2 τ0 ( E{F (x(b)τ0 )} − F∗ )\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητ0λτ0µτ0 βτ0 ζτ0 + vτ0 − λ1x (b) ∗\n∥ ∥ ∥ ∥\n2 }]\n+ η0ǫ ′µ 2 τ+1 − µ2τ0 ητ+1µ2τ+1\n≤ 4 λ21η̌ ( 1 + ∑τ+1 τ ′=1 λτ ′ )2\n[\nητ0µ 2 τ0 ( E{F (x(b)τ0 )} − F∗ ) (13a)\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητ0λτ0µτ0 βτ0 ζτ0 + vτ0 − λ1x (b) ∗\n∥ ∥ ∥ ∥\n2 }]\n+ η0 η̌ ǫ′ ( 1− µ 2 τ0\nµ2t+1\n)\n≤ 4 λ21η̌ ( 1 + λ̌(1 + τ) )2\n[\nητ0µ 2 τ0 ( E{F (x(b)τ0 )} − F∗ )\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητ0λτ0µτ0 βτ0 ζτ0 + vτ0 − λ1x (b) ∗\n∥ ∥ ∥ ∥\n2 }]\n+ η0 η̌ ǫ′\n≤ 4 λ21λ̌ 2η̌(1 + τ)2\n[\nητ0µ 2 τ0 ( E{F (x(b)τ0 )} − F∗ )\n+ 1\n2 E\n{∥ ∥ ∥ ∥\nητ0λτ0µτ0 βτ0 ζτ0 + vτ0 − λ1x (b) ∗\n∥ ∥ ∥ ∥\n2 }]\n+ η0 η̌ ǫ′ , (13b)\nwhere Fact 1.2 was utilized in (13a). The previous inequality establishes the claim of Thm. 1.3.\n4) By (M3),\n1− √\n1− ητλτL(b)τ L (b) τ ≤ βτ ≤ 1 +\n√\n1− ητλτL(b)τ L (b) τ . (14)\nMoreover, since the preceding discussion holds for any over-estimate L (b) τ of the underlying Lipschitz constants (cf. Remark 3), one can always set a sufficiently small Ľ ∈ R>0 as a lower-bound on all L(b)τ , i.e., L (b) τ ≥ Ľ, ∀(τ, i). Accordingly, the right hand side of (14) suggests that βτ ≤ 2/Ľ. Given also that ητ ≥ η̌, λτ ≥ λ̌, and As4, then there exists δ > 0 s.t. −ητλτ (L(b)τ β2τ − 2βτ + ητλτ )/(2β2τ ) ≥ δ, ∀τ . As a result, (11) implies that for any x̄(b),\nµ2τ+1δ‖ζτ+1 − ψτ+1‖2\n≤ − ητ+1µ 2 τ+1λτ+1\n2β2τ+1\n( L (b) τ+1β 2 τ+1 − 2βτ+1 + ητ+1λτ+1 )\n× ‖ζτ+1 − ψτ+1‖2\n≤ ητ+1µ2τ [ Fτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )]\n− ητ+1µ2τ+1 [ Fτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1 )]\n− 1 2 ∥ ∥ ∥ ∥ ητ+1λτ+1µτ+1 βτ+1 ζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n+ 1\n2\n∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2 ,\nand\n‖ζτ+1 − ψτ+1‖2\n≤ητ+1µ 2 τ\nδµ2τ+1\n[\nFτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1\n)]\n− ητ+1 δ\n[\nFτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1\n)]\n− 1 2δµ2τ+1\n∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1\nζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n+ 1\n2δµ2τ+1\n∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n≤ητ+1 δ\n[\nFτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1\n)]\n− ητ+1 δ\n[\nFτ+1 ( xτ+1 | x(−b)τ+1 ) − Fτ+1 ( x̄(b) | x(−b)τ+1\n)]\n− 1 2δµ2τ+1\n∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1\nζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n+ 1\n2δµ2τ\n∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n= ητ+1 δ\n[\nFτ+1 ( xτ | x(−b)τ+1 ) − Fτ+1 ( xτ+1 | x(b)τ+1\n)]\n− 1 2δµ2τ+1\n∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1\nζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n+ 1\n2δµ2τ\n∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n≤ητ+1 δ\n[\nFτ+1 ( x (b−1) τ+1 ) − Fτ+1 ( x (b) τ+1\n)]\n− 1 2δµ2τ+1\n∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1\nζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥ 2\n+ 1\n2δµ2τ\n∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥ 2 .\nApplying expectations to the previous inequality yields\nE { ‖ζτ+1 − ψτ+1‖2 } ≤ η0 δ [ E { F ( x (b−1) τ+1 )} − E { F ( x (b) τ+1 )}]\n− 1 2δµ2τ+1 E\n{∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1\nζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥\n2 }\n+ 1\n2δµ2τ E\n{∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥\n2 }\n≤ η0 δ\n[\nE { F ( x(b)τ )} − E { F ( x (b) τ+1\n)}]\n− 1 2δµ2τ+1 E\n{∥ ∥ ∥ ∥\nητ+1λτ+1µτ+1 βτ+1\nζτ+1 + vτ+1 − λ1x̄(b) ∥ ∥ ∥ ∥\n2 }\n+ 1\n2δµ2τ E\n{∥ ∥ ∥ ∥\nητλτµτ βτ\nζτ + vτ − λ1x̄(b) ∥ ∥ ∥ ∥\n2 }\n.\nFor arbitrarily fixed (τ̌ , τ̂) ∈ Z2≥0 s.t. τ̌ ≤ τ̂ , adding the previous inequality for τ ∈ {τ̌ , τ̌ + 1, . . . , τ̂} results in\nτ̂∑\nτ=τ̌\nE\n{ ‖ζτ+1 − ψτ+1‖2 }\n≤ η0 δ\n[\nE { F ( x (b) τ̌ )} − E { F ( x (b) τ̂+1\n)}]\n+ 1\n2δµ2τ̌ E\n{∥ ∥ ∥ ∥\nητ̌λτ̌µτ̌ βτ̌\nζτ̌ + vτ̌ − λ1x̄(b) ∥ ∥ ∥ ∥\n2 }\n.\nHence, by applying limτ̂→∞ to the previous inequality,\n∞∑\nτ=τ̌\nE\n{ ‖ζτ+1 − ψτ+1‖2 }\n≤ η0 δ\n[\nE { F ( x (b) τ̌ )} − F∗\n]\n+\nE {∥ ∥ ∥ ητ̌λτ̌µτ̌ βτ̌ ζτ̌ + vτ̌ − λ1x̄(b) ∥ ∥ ∥ 2 }\n2δµ2τ̌ ."
    }, {
      "heading" : "5) By the definition of Proxβτgb(h) as the (unique) minimizer Proxβτgb(h) = argminξ[‖h − ξ‖2/2 +",
      "text" : "βτgb(ξ)], there exists g ′ b(Proxβτgb(h)) ∈ ∂gb(Proxβτgb(h)) s.t. Proxβτgb(h) − h + βτg′b(Proxβτgb(h)) = 0. Following the notation of (M5), a rearrangement of the previous equality yields that there exists g′b(ζτ ) ∈ ∂gb(ζτ ) s.t. ψτ − ζτ = βτ [∇bfτ ( ψτ | x(−b)τ ) + g′b(ζτ )] . (15)\nBy definition, fτ (· | x(−b)τ )’s range is included in R; hence, the relative interior [44, p. 91] of its domain ri dom fτ (· | x(−b)τ ) = Mb. Consequently, the definition Fτ = fτ + ∑B b=1 gb, and the elementary ri dom[fτ (· | x(−b)τ )]∩ ri dom gb(·) = ri dom gb 6= ∅, suggest that ∂bFτ (· | x(−b)τ ) = ∇bfτ (· | x(−b)τ )+∂gb(·) [44, Cor. 16.38.iv]. As a result, the existence of g′b(ζτ ) and (15) establish that for any (τ, b), there exists F ′τ (ζτ | x(−b)τ ) ∈ ∂bFτ (ζτ | x(−b)τ ) s.t. ψτ − ζτ = βτ [F ′τ (ζτ | x(−b)τ ) +∇bfτ (ψτ | x(−b)τ )−∇bfτ (ζτ | x(−b)τ )]. Hence,\n∥ ∥ ∥F ′τ ( ζτ | x(−b)τ )∥∥ ∥ 2\n≤ 2 β2τ ‖ψτ − ζτ‖2\n+ 2 ∥ ∥ ∥∇bfτ ( ζτ | x(−b)τ ) −∇bfτ ( ψτ | x(−b)τ ) ∥ ∥ ∥ 2\n≤ 2 β2τ ‖ψτ − ζτ‖2 + 2L2τ‖ψτ − ζτ‖2 ≤ 4L2τ‖ψτ − ζτ‖2 ≤ 4L̂‖ψτ − ζτ‖2 , (16)\nwhere (16) utilized As4 and the fact that due to (14), βτ ≥ (1− (1−ητλτL(b)τ )1/2)/L(b)τ ≥ 1/L(b)τ ≥ 1/L̂. Assuming that F ′τ (ζτ | x(−b)τ ) ∈ Hb and applying expectations to (16) yields E{‖F ′τ (ζτ | x(−b)τ )‖2} ≤ 4L̂E{‖ψτ − ζτ‖2}, which together with Thm. 1.4 establish\nlim τ→∞ E\n{∥ ∥ ∥F ′τ ( ζτ | x(−b)τ ) ∥ ∥ ∥ 2 } = 0 . (17)\nFurther, due to the convexity of ‖ · ‖2, Jensen’s inequality for conditional expectations [46, § IV.3] suggests that\nE {∥ ∥ ∥F ′τ ( ζτ | x(−b)τ ) ∥ ∥ ∥ 2 }\n= EX\n{\nEO|X {∥ ∥ ∥F ′τ ( ζτ | x(−b)τ ) ∥ ∥ ∥ 2 }}\n≥ EX {∥ ∥ ∥EO|X { F ′τ ( ζτ | x(−b)τ )} ∥ ∥ ∥ 2 } .\nAccordingly, the previous inequality together with (17) establish Thm. 1.5. 6) By (M6), ∀τ ∈ T(b)ζ , where T (b) ζ is defined in Thm. 1.2, Fτ (xτ | x (−b) τ ) ≤ λτFτ (ζτ | x(−b)τ ) + (1 − λτ )Fτ (xτ−1 | x(−b)τ ). Hence, ∀τ ∈ T(b)ζ ,\nE { F ( ζτ | x(−b)τ )} = E { Fτ ( ζτ | x(−b)τ )} ≥ 1 λτ E { Fτ ( xτ | x(−b)τ )} − 1− λτ λτ E { Fτ ( xτ−1 | x(−b)τ )} = 1\nλτ E { F ( x(b)τ\n)} − 1− λτ\nλτ E { F ( xτ−1 | x(−b)τ )}\n≥ 1 λτ E { F ( x(b)τ )} − 1− λτ λτ E { F ( x(b−1)τ )} (18a) = 1\nλτ\n[\nE { F ( x(b)τ )} − E { F ( x(b−1)τ\n)}]\n+ E { F ( x(b−1)τ )}\n≥ 1 λ̌\n[\nE { F ( x(b)τ )} − E { F ( x(b−1)τ\n)}]\n+ E { F ( x(b−1)τ )} (18b)\n≥ 1 λ̌\n[\nE { F ( x(b)τ )} − E { F ( x(b−1)τ\n)}]\n+ F∗ , (18c)\nwhere λ̌ ≤ λτ , ∀τ , and the monotonicity properties E{F (x(b−1)τ )} ≤ E{F (xτ−1 | x(−b)τ )}, as well as F∗ ≤ E{F (x(b)τ )} ≤ E{F (x(b−1)τ )}, established in Thm. 1.1, were used in (18a), (18b), and (18c). Since limτ→∞[E{F (x(b)τ )} − E{F (x(b−1)τ )}] = 0 by Thm. 1.1, given any ǫ ∈ R>0, there exists τ ′0 ∈ Z≥0 s.t. ∀τ ∈ T(b)ζ ∩ [τ ′0,+∞),\nE{F (ζτ | x(−b)τ )} ≥ − 1\nλ̌\nλ̌ǫ\n2 + F∗ = F∗ −\nǫ 2 . (19)\nDue to the elementary E{‖ζ(b)τ − w(b)τ ‖2} ≤ 2E{‖ζ(b)τ ‖2} + 2E{‖w(b)τ ‖2}, Thm. 1.2 and As5 suggest that there exists ∆̂ s.t. E{‖ζ(b)τ − w(b)τ ‖2} ≤ ∆̂2.\nNow, according to Lemma 1, the subgradient inequality for function E{F (· | x(−b)τ )} yields ∀τ ,\nE { F ( ζτ | x(−b)τ )} = E { Fτ ( ζτ | x(−b)τ )}\n≤ E { Fτ ( wτ | x(−b)τ )}\n+ E {〈\nEO|X { F ′τ ( ζτ | x(−b)τ )} ∣ ∣ ∣ ζτ − wτ 〉}\n≤ E { Fτ ( wτ | x(−b)τ )}\n+ [ E { ‖ζτ − wτ‖2 }] 1\n2 × [\nE {∥ ∥ ∥EO|X { F ′τ ( ζτ | x(−b)τ )}∥∥ ∥ 2 }] 1 2\n≤ E { Fτ ( wτ | x(−b)τ )}\n+ ∆̂\n[\nE {∥ ∥ ∥EO|X { F ′τ ( ζτ | x(−b)τ )} ∥ ∥ ∥ 2 }] 1 2 , (20)\nwhere ∆̂ bounds ([E{‖ζ(b)τ − w(b)τ ‖2}]1/2)τ∈Z>0 . Since (20) holds for any subgradient, it holds also for the specific EO|X{F ′τ (ζτ | x (−b) τ )} involved in Thm. 1.5. Moreover, by the definition of lim sup, ∃τ ′′0 ∈ Z≥0 ∩ [τ ′0,+∞) s.t. ∀τ ∈ T (b) ζ ∩ [τ ′′0 ,+∞),\nE { F ( ζτ | x(−b)τ )}\n≤ lim sup τ→∞\nE { F ( wτ | x(−b)τ )} + ǫ\n4 + ∆̂\nǫ\n4∆̂\n= lim sup τ→∞ min x∈Hb\nE { F ( x | x(−b)τ )}\n︸ ︷︷ ︸\n=: F̄b\n+ ǫ\n2\n≤ lim sup τ→∞\nE { F ( x(b)τ | x(−b)τ )} + ǫ\n2 = F∗ +\nǫ 2 . (21)\nTo summarize, (19) and (21) suggest that ∀τ ∈ T(b)ζ ∩ [τ ′′0 ,+∞), F∗ ≤ E{F (ζτ | x (−b) τ )}+ ǫ/2 ≤ F̄b+ ǫ ≤ F∗ + ǫ, and since ǫ was chosen arbitrarily, F∗ = F̄b.\nThe previous result together with the definition of lim sup suggest that there exists a subsequence\n(x (b) τk )k∈Z≥0 s.t. limk→∞minx∈Hb E{F (x | x (−b) τk )} = F∗. In other words, given any arbitrarily small ǫ ∈ R>0, there exists k ′ 0 s.t. ∀k ≥ k′0, |F∗−minx∈Hb E{F (x | x (−b) τk )}| ≤ ǫ/2. Moreover, Thm. 1.1 has already demonstrated that F∗ = limτ→∞ E{F (x(b)τ | x(−b)τ )}; hence, F∗ = limk→∞ E{F (x(b)τk | x (−b) τk )}, and there exists k′′0 s.t. ∀k ≥ k′′0 , |F∗−E{F (x (b) τk | x (−b) τk )}| ≤ ǫ/2. Putting everything together, by choosing any k0 ≥ max{k′0, k′′0}, it can be readily deduced that ∀k ≥ k0, E{F (x (b) τk | x (−b) τk )}−minx∈Hb E{F (x | x (−b) τk )} ≤ ǫ.\nA.4 Proof of Lemma 3\nBy Thm. 1.2, consider x (b) ∗ ∈ W{(x(b)τ )τ∈Z≥0} 6= ∅. In other words, there exists a subsequence (τk)k∈Z≥0 s.t. w-limk→∞ x (b) τk = x (b) ∗ , where w-lim denotes the limit in the weak topology of Hb [44, §2.4].\nFix now arbitrarily ǫ ∈ R>0. By the definition of lim sup, there exists τ ′0 ∈ Z≥0 s.t. ∀τ ≥ τ ′0, lim supτ→∞ E{F ( x (b) ∗ | x(−b)τ )} ≤ E{F (x(b)∗ | x(−b)τ )}+ǫ/2. By assumption, E{F (· | x(−b)τ )} is l.s.c. on Hb, and, consequently, it is also weakly sequentially l.s.c. [44, Thm. 9.1]. Hence, there exists a neighborhood Vτ (x (b) ∗ ) of x (b) ∗ in the weak topology of Hb s.t. ∀x ∈ Vτ (x(b)∗ ), E{F (x(b)∗ | x(−b)τ )} ≤ E{F (x | x(−b)τ )}+ǫ/2 [44, Def. 1.21].\n[As6a] There exists an open neighborhood V(x (b) ∗ ) of x (b) ∗ in the weak topology of Hb and a τ# ∈ Z≥0\ns.t. V(x (b) ∗ ) ⊂ ∩+∞τ=τ# Vτ (x (b) ∗ ).\nSince w-limk→∞ x (b) τk = x (b) ∗ , there exists k ′ 0 ∈ Z≥0 s.t. ∀k ≥ k′0, xτk ∈ V(x (b) ∗ ). The previous arguments and As6a suggest that there exists a sufficiently large k′′0 ∈ Z≥0 s.t. τk′′0 ≥ max{τ#, τ ′ 0}, k′′0 ≥ k′0, and consequently ∀k ≥ k′′0 ,\nlim sup τ→∞\nE { F ( x (b) ∗ | x(−b)τ )}\n≤ E { F ( x (b) ∗ | x(−b)τk )} + ǫ\n2\n≤ E { F ( x(b)τk | x (−b) τk )} + ǫ\n2 +\nǫ\n2\n= E { F ( x(b)τk )} + ǫ.\nTherefore,\nlim sup τ→∞\nE { F ( x (b) ∗ | x(−b)τ )}\n= lim k→∞ lim sup τ→∞\nE { F ( x (b) ∗ | x(−b)τ )}\n≤ lim k→∞\nE { F ( x(b)τk )} + ǫ = F∗ + ǫ .\nThe previous inequality holds for any ǫ; hence lim supτ→∞ E{F (x(b)∗ | x(−b)τ )} ≤ F∗.\n[As6b] There exists a strong sequential cluster point x (b) ∗ ∈ C{(x(b)τ )τ∈Z≥0}, a ∆̂ ∈ R>0, and a sequence\nof subgradients (EO|X{F ′t(x(b)∗ | x(−b)τ )})τ∈Z>0 ⊂ Hb (cf. Lemma 1) s.t.\nlim sup τ→∞ E\n{∥ ∥ ∥EO|X { F ′t(x (b) ∗ | x(−b)τ ) } ∥ ∥ ∥ 2 } ≤ ∆̂.\nDue to x (b) ∗ ∈ C{(x(b)τ )τ∈Z≥0}, there exists a subsequence (x (b) τk )k∈Z≥0 s.t. limk→∞[E{‖x (b) ∗ −x(b)τk ‖2}]1/2 =\n0. According to Lemma 1, the subgradient inequality for function E{F (· | x(−b)τ )} yields E{F (x(b)∗ | x(−b)τ )} ≤ E{F (x(b)τ | x(−b)τ )}+ E{〈EO|X{F ′t (x(b)∗ | x(−b)τ )} | x(b)∗ − x(b)τ 〉}. Similarly to a previous discussion, by the definition of lim sup, ∃τ ′0 ∈ Z≥0 s.t. ∀τ ≥ τ ′0, lim supτ→∞ E{F ( x (b) ∗ | x(−b)τ )} ≤ E{F (x(b)∗ | x(−b)τ )}+ ǫ/2.\nHence, ∀τ ≥ τ ′0,\nlim sup τ→∞\nE { F ( x (b) ∗ | x(−b)τ )}\n≤ E { F ( x (b) ∗ | x(−b)τ )} + ǫ ≤ E { F ( x(b)τ | x(−b)τ ) }+ ǫ\n+ E {〈 EO|X{F ′t (x(b)∗ | x(−b)τ )} ∣ ∣ ∣ x (b) ∗ − x(b)τ 〉}\n≤ E { F ( x(b)τ ) }+ ǫ\n+\n[\nE {∥ ∥ ∥x (b) ∗ − x(b)τ ∥ ∥ ∥ 2 }] 1 2\n× [\nE {∥ ∥ ∥EO|X { F ′t ( x (b) ∗ | x(−b)τ )} ∥ ∥ ∥ 2 }] 1 2\n≤ E { F ( x(b)τ ) }+\n√\n∆̂\n[\nE {∥ ∥ ∥x (b) ∗ − x(b)τ ∥ ∥ ∥ 2 }] 1 2 + ǫ ,\nand ∀k ≥ k′′0 , for a sufficiently large k′′0 ∈ Z>0,\nlim sup τ→∞\nE { F ( x (b) ∗ | x(−b)τ )}\n≤ E { F ( x(b)τk ) }+\n√\n∆̂\n[\nE {∥ ∥ ∥x\n(b) ∗ − x(b)τk\n∥ ∥ ∥\n2 }] 1 2\n+ ǫ .\nBy applying limk→∞ to both sides of the previous inequality, it can be verified that lim supτ→∞ E{F (x(b)∗ | x(−b)τ )} ≤ F∗+ǫ, and since ǫ was chosen arbitrarily, lim supτ→∞ E{F (x(b)∗ | x(−b)τ )} ≤ F∗."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Dr. P. Forero of SPAWAR for providing the code of the method in [8]."
    } ],
    "references" : [ {
      "title" : "Dictionary learning",
      "author" : [ "I. Tošić", "P. Frossard" ],
      "venue" : "IEEE Signal Process. Magaz., vol. 28, no. 2, pp. 27–38, Mar. 2011.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Online learning for matrix factorization and sparse coding",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "J. Machine Learn. Research, vol. 11, pp. 19–60, Mar. 2010.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning the parts of objects by non-negative matrix factorization",
      "author" : [ "D.D. Lee", "H.S. Seung" ],
      "venue" : "Nature, vol. 401, no. 6755, pp. 788–791, Oct. 1999.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Subspace clustering",
      "author" : [ "R. Vidal" ],
      "venue" : "IEEE Signal Process. Magaz., vol. 28, no. 2, pp. 52–68, Mar. 2011.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "T.G. Kolda", "B.W. Bader" ],
      "venue" : "SIAM Review, vol. 51, no. 3, pp. 455–500, 2009. 27",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Prediction of partially observed dynamical processes over networks via dictionary learning",
      "author" : [ "P. Forero", "K. Rajawat", "G.B. Giannakis" ],
      "venue" : "IEEE Trans. Signal Process., vol. 62, no. 13, pp. 3305–3320, July 2014.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Convergence of block coordinate decent method for nondifferentiable minimization",
      "author" : [ "P. Tseng" ],
      "venue" : "J. Optim. Theory Appl., vol. 109, pp. 475–494, June 2001.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A coordinate gradient descent method for nonsmooth separable minimization",
      "author" : [ "P. Tseng", "S. Yun" ],
      "venue" : "Math. Program., Ser. B, vol. 117, pp. 387–423, 2009.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Accelerated block-coordinate relaxation for regularized optimization",
      "author" : [ "S.J. Wright" ],
      "venue" : "SIAM J. Optim., vol. 22, no. 1, pp. 159–186, 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Y. Nesterov" ],
      "venue" : "SIAM J. Optim., vol. 22, no. 2, pp. 341–362, 2012.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion",
      "author" : [ "Y. Xu", "W. Yin" ],
      "venue" : "SIAM J. Imaging, vol. 6, no. 3, pp. 1758– 1789, 2013.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
      "author" : [ "P. Richtárik", "M. Takáč" ],
      "venue" : "Math. Program., Ser. A, pp. 1–38, Dec. 2012.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sparse reconstruction by separable approximation",
      "author" : [ "S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo" ],
      "venue" : "IEEE Trans. Signal Process., vol. 57, no. 7, pp. 2479–2493, July 2009.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Sparsity-cognizant total least-squares for perturbed compressive sampling",
      "author" : [ "H. Zhu", "G. Leus", "G.B. Giannakis" ],
      "venue" : "IEEE Trans. Signal Process., vol. 59, no. 5, pp. 2002–2016, May 2011.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A unified convergence analysis of block successive minimization methods for nonsmooth optimization",
      "author" : [ "M. Razaviyayn", "M. Hong", "Z.-Q. Luo" ],
      "venue" : "SIAM J. Optim., vol. 23, no. 2, pp. 1126–1153, 2013.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Stochastic coordinate descent methods for regularized smooth and nonsmooth losses",
      "author" : [ "Q. Tao", "K. Kong", "D. Chu", "G. Wu" ],
      "venue" : "Lecture Notes in Computer Science, ser. Machine Learning and Knowledge Discovery in Databases, P. A. Flach, T. de Bie, and N. Cristianini, Eds., vol. 7523. Springer, 2012, pp. 537–552.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Decentralized sparsity-regularized rank minimization: Algorithms and applications",
      "author" : [ "M. Mardani", "G. Mateos", "G.B. Giannakis" ],
      "venue" : "IEEE Trans. Signal Process., vol. 61, no. 11, pp. 5374–5388, Nov. 2013.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Compressed sensing",
      "author" : [ "D.L. Donoho" ],
      "venue" : "IEEE Trans. Inform. Theory, vol. 52, pp. 1289–1306, 2006.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Modeling and optimization for big data analytics",
      "author" : [ "K. Slavakis", "G.B. Giannakis", "G. Mateos" ],
      "venue" : "IEEE Signal Process. Magaz., vol. 31, no. 5, pp. 18–31, Sept. 2014.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stochastic approximation vis-à-vis online learning for big data analytics",
      "author" : [ "K. Slavakis", "S.-J. Kim", "G. Mateos", "G.B. Giannakis" ],
      "venue" : "IEEE Signal Process. Magaz., vol. 31, no. 6, pp. 124–129, Nov. 2014.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stochastic Approximation and Recursive Algorithms and Applications, 2nd ed",
      "author" : [ "H.J. Kushner", "G.G. Yin" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2003
    }, {
      "title" : "A method for solving the convex programming problem with convergence rate O(1/k)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Dokl. Akad. Nauk SSSR, vol. 269, pp. 543–547, 1983, in Russian.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "S. Shalev-Shwartz" ],
      "venue" : "Foundations and Trends in Machine Learning, vol. 4, no. 2, pp. 107–194, 2012.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "J. Machine Learn. Research, vol. 12, pp. 2121–2159, 2011.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization",
      "author" : [ "S. Ghadimi", "G. Lan", "H. Zhang" ],
      "venue" : "Aug. 2013, arXiv:1308.6594. 28",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A stochastic successive minimization method for nonsmooth nonconvex optimization with applications to transceiver design in wireless communication networks",
      "author" : [ "M. Razaviyayn", "M. Sanjabi", "Z.-Q. Luo" ],
      "venue" : "Jul. 2013, arXiv:1307.4457.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sur l’approximation par éléments finis et la résolution par pénalisationdualité d’une classe de problèmes de Dirichlet non linéaires",
      "author" : [ "R. Glowinski", "A. Marrocco" ],
      "venue" : "Rev. Francaise d’Aut. Inf. Rech. Oper., vol. 9, no. 2, pp. 41–76, 1975.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "A dual algorithm for the solution of nonlinear variational problems via finiteelement approximations",
      "author" : [ "D. Gabay", "B. Mercier" ],
      "venue" : "Comp. Math. Appl., vol. 2, pp. 17–40, 1976.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Distributed LMS for consensus-based in-network adaptive processing",
      "author" : [ "I.D. Schizas", "G. Mateos", "G.B. Giannakis" ],
      "venue" : "IEEE Trans. Signal Process., vol. 57, no. 6, pp. 2365–2381, June 2009.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Performance analysis of the consensus-based distributed LMS algorithm",
      "author" : [ "G. Mateos", "I.D. Schizas", "G.B. Giannakis" ],
      "venue" : "EURASIP Journal on Advances in Signal Processing, vol. 2009, Dec. 2009.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Stochastic alternating direction method of multipliers",
      "author" : [ "H. Ouyang", "N. He", "L.Q. Tran", "A. Gray" ],
      "venue" : "Proc. ICML, Atlanta, Georgia: USA, June 2013.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On the O(1/t) convergence rate of the Douglas-Rachford alternating direction method",
      "author" : [ "B. He", "X. Yuan" ],
      "venue" : "SIAM J. Numerical Analysis, vol. 50, no. 2, pp. 700–709, 2012.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the o(1/k) convergence and parallelization of the alternating direction method of multipliers",
      "author" : [ "W. Deng", "M.-J. Lai", "W. Yin" ],
      "venue" : "2013, arXiv:1312.3040.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On the linear convergence of the alternating direction method of multipliers",
      "author" : [ "M. Hong", "Z.-Q. Luo" ],
      "venue" : "2012, arXiv:1208.3922.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM J. Imaging Sciences, vol. 2, no. 1, pp. 183–202, 2009.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems",
      "author" : [ "——" ],
      "venue" : "IEEE Trans. Image Process., vol. 18, pp. 2419–2439, 2009.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Over-relaxation of the fast iterative shrinkage-thresholding algorithm with variable stepsize",
      "author" : [ "M. Yamagishi", "I. Yamada" ],
      "venue" : "Inverse Problems, vol. 27, no. 10, 2011.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Acceleration of adaptive proximal forward-backward splitting method and its application to sparse system identification",
      "author" : [ "M. Yamagishi", "M. Yukawa", "I. Yamada" ],
      "venue" : "Proc. ICASSP, Prague: Czech Republic, May 22–27 2011, pp. 4296–4299.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Accelerated gradient methods for stochastic optimization and online learning",
      "author" : [ "C. Hu", "J.T. Kwok", "W. Pan" ],
      "venue" : "Proc. NIPS, 2009.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Accelerated gradient methods for nonconvex nonlinear and stochastic programming",
      "author" : [ "S. Ghadimi", "G. Lan" ],
      "venue" : "Oct. 2013, arXiv:1310.3787.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Online dictionary learning from big data using accelerated stochastic approximation algorithms",
      "author" : [ "K. Slavakis", "G.B. Giannakis" ],
      "venue" : "Proc. ICASSP, Florence: Italy, May 2014.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Convex Analysis and Monotone Operator Theory in Hilbert Spaces",
      "author" : [ "H.H. Bauschke", "P.L. Combettes" ],
      "venue" : "New York: Springer,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2011
    }, {
      "title" : "Proximal splitting methods in signal processing",
      "author" : [ "P.L. Combettes", "J.-C. Pesquet" ],
      "venue" : "Fixed-Point Algorithms for Inverse Problems in Science and Engineering. Springer-Verlag, 2011, pp. 185–212.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Mathematical Foundations of the Calculus of Probability",
      "author" : [ "J. Neveu" ],
      "venue" : "San Francisco: Holden-Day,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 1965
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditski", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM J. Optim., vol. 19, no. 4, pp. 1574–1609, 2009.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1–3], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.",
      "startOffset" : 110,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1–3], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.",
      "startOffset" : 110,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1–3], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1–3], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 4,
      "context" : "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1–3], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.",
      "startOffset" : 254,
      "endOffset" : 257
    }, {
      "referenceID" : 0,
      "context" : ",dQ], Q ≥ M , times an unknown sparse coefficient vector st [2, 3].",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : ",dQ], Q ≥ M , times an unknown sparse coefficient vector st [2, 3].",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "Sparsity on the other hand, renders DL representations identifiable even when yt has missing entries [8], due to e.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 6,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 9,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 10,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 11,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 12,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 13,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 14,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 15,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 16,
      "context" : "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9–19].",
      "startOffset" : 182,
      "endOffset" : 191
    }, {
      "referenceID" : 1,
      "context" : ", st−1],Dt−1) as follows [3] st ∈ argmins Ft ( [St−1, s],Dt−1;Ot ) (2a) Dt ∈ argminD Ft(St,D;Ot) .",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "Given Ot, each step in (2) is a convex optimization task: Basis pursuit [20] in (2a), and constrained leastsquares (LS) in (2b).",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "However, the per-block minimizations in BCD may not be affordable by todays big data applications, where the sheer volume and dimensionality of Ot strain computing resources [21,22].",
      "startOffset" : 174,
      "endOffset" : 181
    }, {
      "referenceID" : 19,
      "context" : "However, the per-block minimizations in BCD may not be affordable by todays big data applications, where the sheer volume and dimensionality of Ot strain computing resources [21,22].",
      "startOffset" : 174,
      "endOffset" : 181
    }, {
      "referenceID" : 20,
      "context" : "Further, as data are streaming, analytics must often be performed in real time, without a chance to revisit past entries – a feature common to stochastic approximation (SA) setups [23].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 21,
      "context" : ") the number of unknowns; and v) iterations converge quadratically to a solution of (3), which is optimal among first-order methods in the sense of [24].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 20,
      "context" : "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25–28], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.",
      "startOffset" : 135,
      "endOffset" : 146
    }, {
      "referenceID" : 22,
      "context" : "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25–28], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.",
      "startOffset" : 135,
      "endOffset" : 146
    }, {
      "referenceID" : 23,
      "context" : "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25–28], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.",
      "startOffset" : 135,
      "endOffset" : 146
    }, {
      "referenceID" : 24,
      "context" : "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25–28], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.",
      "startOffset" : 135,
      "endOffset" : 146
    }, {
      "referenceID" : 25,
      "context" : "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25–28], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.",
      "startOffset" : 135,
      "endOffset" : 146
    }, {
      "referenceID" : 26,
      "context" : "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29–32], that is known to be sublinearly convergent for convex costs [33–36], but no similar results are available for per-block-convex functions.",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 27,
      "context" : "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29–32], that is known to be sublinearly convergent for convex costs [33–36], but no similar results are available for per-block-convex functions.",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 28,
      "context" : "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29–32], that is known to be sublinearly convergent for convex costs [33–36], but no similar results are available for per-block-convex functions.",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 29,
      "context" : "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29–32], that is known to be sublinearly convergent for convex costs [33–36], but no similar results are available for per-block-convex functions.",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 30,
      "context" : "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29–32], that is known to be sublinearly convergent for convex costs [33–36], but no similar results are available for per-block-convex functions.",
      "startOffset" : 167,
      "endOffset" : 174
    }, {
      "referenceID" : 31,
      "context" : "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29–32], that is known to be sublinearly convergent for convex costs [33–36], but no similar results are available for per-block-convex functions.",
      "startOffset" : 167,
      "endOffset" : 174
    }, {
      "referenceID" : 32,
      "context" : "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29–32], that is known to be sublinearly convergent for convex costs [33–36], but no similar results are available for per-block-convex functions.",
      "startOffset" : 167,
      "endOffset" : 174
    }, {
      "referenceID" : 33,
      "context" : "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29–32], that is known to be sublinearly convergent for convex costs [33–36], but no similar results are available for per-block-convex functions.",
      "startOffset" : 167,
      "endOffset" : 174
    }, {
      "referenceID" : 21,
      "context" : "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37–40]; see also [41] for related SA-based minimizers of convex costs.",
      "startOffset" : 126,
      "endOffset" : 137
    }, {
      "referenceID" : 34,
      "context" : "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37–40]; see also [41] for related SA-based minimizers of convex costs.",
      "startOffset" : 126,
      "endOffset" : 137
    }, {
      "referenceID" : 35,
      "context" : "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37–40]; see also [41] for related SA-based minimizers of convex costs.",
      "startOffset" : 126,
      "endOffset" : 137
    }, {
      "referenceID" : 36,
      "context" : "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37–40]; see also [41] for related SA-based minimizers of convex costs.",
      "startOffset" : 126,
      "endOffset" : 137
    }, {
      "referenceID" : 37,
      "context" : "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37–40]; see also [41] for related SA-based minimizers of convex costs.",
      "startOffset" : 126,
      "endOffset" : 137
    }, {
      "referenceID" : 38,
      "context" : "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37–40]; see also [41] for related SA-based minimizers of convex costs.",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 39,
      "context" : "Even though [42] deals with non-convex costs, it requires bounds on the (primal) variables, knowledge of a",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 36,
      "context" : "Our work markedly broadens the offline acceleration technique introduced for convex costs in [39], to per-block-convex and to online SA setups.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "Numerical tests corroborate our analytical claims, and demonstrate that under a linear computational complexity footprint the proposed algorithm outperforms BCDMs and the computationally heavier ADMM-based alternatives [8].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 40,
      "context" : "Preliminary results were presented in [43], and outlined in [21].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "Preliminary results were presented in [43], and outlined in [21].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 36,
      "context" : "A first-order algorithm for the off-line minimization of a convex cost φ(x) := f(x) + g(x), x ∈ M, was studied in [39] (presented for convenience in Table 1), where M is a linear vector space; f is convex as well as L-Lipschitz continuously differentiable; and g is convex but possibly non-smooth, e.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 41,
      "context" : "The engine under the hood is the forward-backward (FB) [44] or proximal-gradient iteration of line 5, where the proximal mapping is defined as Proxβig : M → M : x 7→ argminξ∈M‖x − ξ‖/2 + βig(ξ) for any βi ∈ R>0 [44].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 41,
      "context" : "The engine under the hood is the forward-backward (FB) [44] or proximal-gradient iteration of line 5, where the proximal mapping is defined as Proxβig : M → M : x 7→ argminξ∈M‖x − ξ‖/2 + βig(ξ) for any βi ∈ R>0 [44].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 42,
      "context" : ", Prox‖·‖1 boils down to the soft-thresholding operator [45].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 41,
      "context" : "If the FB iteration were performed with ψi+1 taking the place of ζi in line 5, then (ψi)i∈Z≥0 would converge to a minimizer of φ [44], but with no claims on quadratic rate of convergence.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 34,
      "context" : "Parameters {ηi+1, λi+1} in line 2 are used to define stepsize βi+1 through line 3, offering the flexibility of a variable stepsize from the interval [(1− √ 1− ηi+1λi+1L)/L, (1+ √ 1− ηi+1λi+1L)/L] per iteration, as opposed to the rigid βi+1 = 1/L in [37, 38].",
      "startOffset" : 249,
      "endOffset" : 257
    }, {
      "referenceID" : 35,
      "context" : "Parameters {ηi+1, λi+1} in line 2 are used to define stepsize βi+1 through line 3, offering the flexibility of a variable stepsize from the interval [(1− √ 1− ηi+1λi+1L)/L, (1+ √ 1− ηi+1λi+1L)/L] per iteration, as opposed to the rigid βi+1 = 1/L in [37, 38].",
      "startOffset" : 249,
      "endOffset" : 257
    }, {
      "referenceID" : 36,
      "context" : "Table 1: Minimizing the convex cost φ := f + g [39] Require: λ̌, η̌ ∈ R>0; μ1 := λ1 ∈ [λ̌, 1]; Lipschitz coeff.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 35,
      "context" : "Under proper parameter selection (ηi = βi = 1/L, λi = 1), the algorithm of Table 1 boils down to [38].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 34,
      "context" : "Moreover, with its guaranteed monotonically non-increasing behavior of cost values through line 6, and the flexibility offered by the variable step-sizes (βi)i∈Z>0 in line 3, the algorithm in Table 1 has merits over [37].",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 34,
      "context" : "Notwithstanding, neither [37, 38] nor [39] can offer guarantees on the convergence of the (primal) variables {xi, ψi, ζi}.",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 35,
      "context" : "Notwithstanding, neither [37, 38] nor [39] can offer guarantees on the convergence of the (primal) variables {xi, ψi, ζi}.",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 36,
      "context" : "Notwithstanding, neither [37, 38] nor [39] can offer guarantees on the convergence of the (primal) variables {xi, ψi, ζi}.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 41,
      "context" : ") functions defined on Mb with values in R∪{+∞} [44].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 41,
      "context" : "Mb} [44].",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 43,
      "context" : "the σ-algebra A [46].",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 43,
      "context" : "X [46].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 43,
      "context" : "O, conditioned on X [46].",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 36,
      "context" : ", Rb} in the context of Table 1, and not infinitely often (i → +∞) as in the batch and off-line mode of [39].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 41,
      "context" : ") If limk→∞ E{‖ξk‖} = +∞ for any (ξk)k∈Z≥0 ⊂ H, then limk→∞ E{F (ξk)} = +∞ [44].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 39,
      "context" : "As2 will be used to prevent the proposed algorithm from generating unbounded sequences of estimates, without any a-priori enforcement of hard bounds on the variables, as in [42].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 44,
      "context" : "More specifically, As6b assumes existence of a strong sequential cluster point and bounds a sequence of subgradients of the expected cost, similarly to the bound on gradients introduced in [47].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 13,
      "context" : "Motivated by the TLS criterion and the resultant errors-in-variables (EIV) modeling approach [7, 16], the following sequence of per-block-convex costs is considered:",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "Following [8], consider an undirected graph G(V, E), where V denotes the set of all vertices or nodes, with cardinality V, and E is the set of all edges.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 5,
      "context" : "where ∆t := ∑t τ=1 δ t−τ ; ‖s‖ and ‖s‖1 are as in (5), while the term including L quantifies prior knowledge on the topology of G, promotes “smooth” solutions over strongly connected nodes of G, and is instrumental in imputing missing entries [8].",
      "startOffset" : 243,
      "endOffset" : 246
    }, {
      "referenceID" : 43,
      "context" : "With regard to the selection of L̂ in As4, recall that Markov’s inequality dictates that Pr(L (b) τ ≥ L̂) ≤ E{L τ }/L̂, for any L̂ [46].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 22,
      "context" : "The algorithm in Table 2 is tested against a block-version of the classical online (sub)gradient descent method [25], tagged as BOGD in Fig.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "The advocated algorithm is tested against the state-of-the-art scheme in [8] which relies on a GaussSeidel alternating minimization scheme: (i) ADMM [29,30] is employed to minimize a cost closely related to (6) w.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "The advocated algorithm is tested against the state-of-the-art scheme in [8] which relies on a GaussSeidel alternating minimization scheme: (i) ADMM [29,30] is employed to minimize a cost closely related to (6) w.",
      "startOffset" : 149,
      "endOffset" : 156
    }, {
      "referenceID" : 27,
      "context" : "The advocated algorithm is tested against the state-of-the-art scheme in [8] which relies on a GaussSeidel alternating minimization scheme: (i) ADMM [29,30] is employed to minimize a cost closely related to (6) w.",
      "startOffset" : 149,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "It is worth noticing here that ADMM in [8] requires multiple iterations to achieve a prescribed estimation accuracy, and that no matrix inversion was incorporated in the realization of Table 2.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : "The proposed method and [8] perform similarly, scoring mean (normalized) estimation errors of 0.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 36,
      "context" : "3) The following proof is based on the one developed in [39] for the off-line, convex analytic case.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "Forero of SPAWAR for providing the code of the method in [8].",
      "startOffset" : 57,
      "endOffset" : 60
    } ],
    "year" : 2017,
    "abstractText" : "Applications involving dictionary learning, non-negative matrix factorization, subspace clustering, and parallel factor tensor decomposition tasks motivate well algorithms for per-block-convex and non-smooth optimization problems. By leveraging the stochastic approximation paradigm and first-order acceleration schemes, this paper develops an online and modular learning algorithm for a large class of non-convex data models, where convexity is manifested only per-block of variables whenever the rest of them are held fixed. The advocated algorithm incurs computational complexity that scales linearly with the number of unknowns. Under minimal assumptions on the cost functions of the composite optimization task, without bounding constraints on the optimization variables, or any explicit information on bounds of Lipschitz coefficients, the expected cost evaluated online at the resultant iterates is provably convergent with quadratic rate to an accumulation point of the (perblock) minima, while subgradients of the expected cost asymptotically vanish in the mean-squared sense. The merits of the general approach are demonstrated in two online learning setups: (i) Robust linear regression using a sparsity-cognizant total least-squares criterion; and (ii) semi-supervised dictionary learning for network-wide link load tracking and imputation with missing entries. Numerical tests on synthetic and real data highlight the potential of the proposed framework for streaming data analytics by demonstrating superior performance over block coordinate descent, and reduced complexity relative to the popular alternating-direction method of multipliers.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}