{
  "name" : "1412.2457.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Weighted Polynomial Approximations: Limits for Learning and Pseudorandomness",
    "authors" : [ "Mark Bun", "Thomas Steinke" ],
    "emails" : [ "mbun@seas.harvard.edu", "tsteinke@seas.harvard.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 2.\n24 57\nv1 [\ncs .C\nC ]\n8 D\nFirstly, the “polynomial regression” algorithm of Kalai et al. (SIAM J. Comput. 2008) shows that halfspaces can be learned with respect to log-concave distributions on Rn in the challenging agnostic learning model. The power of this algorithm relies on the fact that under log-concave distributions, halfspaces can be approximated arbitrarily well by low-degree polynomials. We ask whether this technique can be extended beyond log-concave distributions, and establish a negative result. We show that polynomials of any degree cannot approximate the sign function to within arbitrarily low error for a large class of non-log-concave distributions on the real line, including those with densities proportional to exp(−|x|0.99). This impossibility result extends to multivariate distributions, and thus gives a strong limitation on the power of the polynomial regression algorithm for halfspaces.\nSecondly, we investigate the derandomization of Chernoff-type concentration inequalities. Chernoff-type tail bounds on sums of independent random variables have pervasive applications in theoretical computer science. Schmidt et al. (SIAM J. Discrete Math. 1995) showed that these inequalities can be established for sums of random variables with only O(log(1/δ))-wise independence, for a tail probability of δ. We show that their results are tight up to constant factors.\nThese results rely on techniques from weighted approximation theory, which studies how well functions on the real line can be approximated by polynomials under various distributions. We believe that these techniques will have further applications in other areas of theoretical computer science.\n∗Harvard University, School of Engineering and Applied Sciences. Supported by an NDSEG Fellowship and NSF grant CNS-1237235.\n†Harvard University, School of Engineering and Applied Sciences. Supported by NSF grant CCF-1116616 and the Lord Rutherford Memorial Research Fellowship."
    }, {
      "heading" : "1 Introduction",
      "text" : "Approximation theory is a classical area of mathematics that studies how well functions can be approximated by simpler ones. It has found many applications in computer science. Most of these applications of approximation theory focus on the approximation of functions by polynomials in the uniform norm (or infinity norm). For instance, approximate degree, which captures how well a boolean function can be approximated by low-degree polynomials in the uniform norm, underlies important lower bounds in circuit complexity [Bei93, Bei94, She09], quantum query complexity [BBC+01, AS04], and communication complexity [She08]. It also underlies state-of-the art algorithms in learning theory [KKMS08, KS04], streaming [HNO08], and in spectral methods [SV14].\nWhile it is compelling to study polynomial approximations under the uniform norm, there are scenarios where it is more natural to study weighted polynomial approximations, where error is measured in terms of an Lp norm under some distribution. For instance, in agnostic learning, the polynomial regression algorithm of Kalai et al. [KKMS08] has guarantees based on how well functions in a concept class of interest can be approximated by low-degree polynomials in L1 distance.\nIn this work, we show how ideas from weighted approximation theory can yield tight lower bounds for several problems in theoretical computer science. As our first application, we establish a strong limitation on the distributions under which halfspaces can be learned using the polynomial regression algorithm of Kalai et al. Second, in the area of derandomization, we give a tight characterization of the amount of k-wise independence necessary to establish Chernoff-like concentration inequalities."
    }, {
      "heading" : "1.1 Agnostically Learning Halfspaces",
      "text" : "Halfspaces are a fundamental concept class in machine learning, both in theory and in practice.1 Their study dates back to the Perceptron algorithm of the 1950s. Halfspaces serve as building blocks in many applications, including boosting and kernel methods.\nHalfspaces can be learned in the PAC model [Val84] either by solving a linear program, or via simple iterative update algorithms (e.g. the Perceptron algorithm). However, learning halfspaces with classification noise is a much more difficult problem, and often needs to be dealt with in practice.\nIn this work, we study a challenging model of adversarial noise – the agnostic learning model of Kearns et al. [KSSH94]. In this model, a learner has access to examples drawn from a distribution D on X × {±1} and must output a hypothesis h : X → {±1} such that\nP (x,y)∼D\n[h(x) 6= y] ≤ opt + ε,\nwhere opt is the error of the best concept in the concept class – that is, opt = minf∈C P (x,y)∼D [f(x) 6= y]. The theory of agnostic learning is not well-understood, even in the case of halfspaces. Positive results for efficient agnostic learning of high-dimensional halfspaces are restricted to limited classes of distributions.2 For instance, halfspaces can be learned under the uniform distribution over the\n1A halfspace is a function f : Rn → {±1} given by f(x) = sgn(w · x− θ) for w ∈ Rn and θ ∈ R, where sgn(x) = 1 if x ≥ 0 and sgn(x) = −1 otherwise.\n2An efficient algorithm is one which runs in time polynomial in the dimension n for any constant ε > 0 – that is, time nOε(1).\nhypercube or the unit sphere, or on any log-concave distribution [KKMS08]. On the negative side, a variety of both computational and information-theoretic hardness results are known. For instance, proper agnostic learning of halfspaces (where the learner is required to output a hypothesis that is itself a halfspace) is known to be NP-hard [FGKP06]. Moreover, agnostically learning halfspaces under arbitrary distributions is as hard as PAC learning DNFs [LBW95], which is a longstanding open problem.\nThere is essentially only one known technique for agnostically learning high-dimensional halfspaces: the L1 regression algorithm [KKMS08], which we discuss in more detail in Section 2.2. In its most general form, the algorithm selects a linear space of functions H ⊂ {h : X → R}. After drawing a number of examples (xi, yi) from D, it computes\nh∗ = argmin h∈H\n∑\ni\n|h(xi)− yi|.\nThe output of the algorithm is sgn(h∗(x)− t) for some t. We need to ensure that the minimisation can be computed efficiently (e.g. by linear programming) and that every concept f ∈ C can be approximated by some h ∈ H – that is E\nx∼D [|h(x) − f(x)|] ≤ ε. If this is the case, then C is\nagnostically learnable in time poly(|H|). Kalai et al. (and most subsequent work on learning using L1 regression, e.g. [KOS08, GKK08, BOW10, KKM13, FK14]) chose H to be the class of low-degree polynomials. They showed that under certain classes of distributions, every halfspace can be approximated by a polynomial of degree Oε(1), and hence halfspaces are agnostically learnable in time n\nOε(1). Distributional assumptions arise because we use an L1 approximation measure (namely E\nx∼D [|h(x) − f(x)|]),\nwhich depends on the distribution. A distribution-independent approximation would require an L∞ approximation, which is too much to hope for in many circumstances."
    }, {
      "heading" : "1.1.1 Our Results",
      "text" : "Can we weaken the distributional assumptions required for learning halfspaces using current techniques? Our result addressing this question (Theorem 2) is a negative one. We show that polynomial approximations to halfspaces do not exist for a large class of distributions, namely:\nDefinition 1. An absolutely continuous distribution D on R is a log-superlinear (LSL) distribution if there exist C > 0 and γ ∈ (0, 1) such that the density w of D satisfies w(x) ≥ C exp(−|x|γ).3\nTheorem 2. For any LSL distribution D, there exists ε > 0 such that no polynomial (of any degree) can approximate the sign function with L1 error less than ε with respect to D.\nIn particular, this implies that the polynomial regression algorithm is not able to agnostically learn thresholds on the real line to within arbitrarily small error. Note that this result does not rule out the possibility that halfspaces can be agnostically learned by other techniques. Indeed, the classic approach of empirical risk minimization (see [KSSH94] and the references therein) gives an efficient algorithm for learning thresholds (which are halfspaces in one dimension) under arbitrary distributions. Thus the problem of learning real thresholds under LSL distributions is an explicit example for which polynomial regression fails while other techniques can succeed.\n3The name log-superlinear comes from the fact that the tails of the probability density function of a LSL distribution are heavier than that of the log-linear Laplace distribution.\nIf we were to take γ ≥ 1, the probability density function C(γ)e−|x|γ (where C(γ) is a normalising constant) would give a log-concave distribution, in which case Kalai et al. [KKMS08] show that good polynomial approximations to halfspaces exist. Thus our result gives a threshold between where polynomial approximations to halfspaces exist and where they do not.\nOur result for thresholds extends readily to an impossibility result for learning halfspaces over Rn:\nTheorem 3. For any product distribution D on Rn with a LSL marginal distribution on some coordinate, there exists ε > 0 and a halfspace h such that no polynomial can approximate h with L1 error less than ε with respect to D.\nOur result echoes prior work establishing the limits of uniform polynomial approximations for various concept classes. For instance, the seminal work of Minsky and Papert [MP72] showed that there is an intersection of two halfpsaces over Rn which cannot be represented as the sign of any polynomial. Building on work of Nisan and Szegedy [NS94], Paturi [Pat92] gave tight lower bounds for uniform approximations to symmetric boolean functions. This, and subsequent work on lower bounds for approximate degree, immediately imply limitations for distributionindependent agnostic learning via polynomial regression. Klivans and Sherstov [KS10] also showed a strong generalization of Paturi’s result to disjunctions, giving limitations on how well they can be approximated by linear combinations of arbitrary features. By contrast to all of these results, our work shows a strong limitation for certain distribution-dependent polynomial approximations.\nIn the distribution-dependent setting, Feldman and Kothari [FK14] showed that polynomial regression cannot be used to learn disjunctions with respect to symmetric distributions on the hypercube. Recent work of Daniely et al. [DLS14] also uses ideas from approximation theory to show limitations on broad class of regression and kernel-based methods for learning halfspaces, even under a margin assumption. While our results only apply to polynomial regression, they hold for approximations of arbitrarily high complexity (i.e. degree), and for a large class of natural distributions.\nThe proof of Theorem 2 relies on several Markov-type inequalities for weighted polynomial approximations. These are generalizations of the classical Markov inequality for uniform approximations, which gives a bound on the derivative of a low-degree polynomial that is bounded on the unit interval:\nTheorem 4 ([Mar90]). Let p be a polynomial of degree d with |p(x)| ≤ 1 on the interval [−1, 1]. Then |p′(x)| ≤ d2 on [−1, 1].\nEarly work on the approximate degree of boolean functions [NS94, Pat92] used Markov’s inequality to get tight lower bounds on the degree of uniform approximations to symmetric functions. For weighted approximations under LSL distributions, we actually get a much stronger statement. It turns out that under LSL distributions, the derivative of a bounded polynomial near the origin is at most a constant independent of degree. With this powerful fact in hand, the proof of Theorem 2 is quite simple. Consider the threshold function f(t) = sgn(t). Since f has a “jump” at zero, any good polynomial approximation to f must be bounded and have a large derivative near zero. The higher quality the approximation, the larger a derivative we need. But since the derivative of any polynomial is bounded by a constant, we cannot get arbitrarily good approximations to f using polynomials.\nWe give the full proof in Section 2.4, and discuss the multivariate generalization in Section 2.5."
    }, {
      "heading" : "1.1.2 Related Work",
      "text" : "There is a rich literature on lower bounds for agnostic learning. In the case of proper agnostic learning Feldman et. al [FGKP06] gave an optimal NP-hardness result for even weakly agnostically learn halfspaces over Qn. Guruswami and Raghavendra [GR06] showed that the same is true even for halfspaces on the boolean hypercube.\nThere has also been a line of work giving representation-independent hardness of learning halfspaces based on cryptographic assumptions. Feldman et. al [FGKP06] and Klivans and Sherstov [KS09] showed that, assuming the security of certain public key encryptions schemes, it is hard to even PAC learn thresholds and intersections of halfspaces, respectively. These results imply that it is hard to agnostically learn a single halfspace in the harsh noise regime, i.e. when opt is very close to 12 . Shalev-Schwartz et al. [SSSS11] further showed that halfspaces cannot be efficiently learned even under a large margin assumptions.\nThere has also been extensive work proving unconditional lower bounds for restricted learning algorithms. One well-studied restriction on a learner is that it operates in Kearns’ statistical query (SQ) model [Kea98, BFJ+94, KS07, FLS11]. This model captures L1 regression, as well as essentially every technique known for learning (besides Gaussian elimination). Very recently, Dachman-Soled et al. [DFT+14] showed that polynomial regression is in fact essentially the optimal SQ algorithm for agnostic learning with respect to product distributions on the hypercube.\nThe limitations we prove for polynomial regression do not rule out the existence of other agnostic learning algorithms, including those using L1 regression with different feature spaces. Wimmer [Wim10] showed how to use a different family of basis functions to learn halfspaces over symmetric distributions on the hypercube. Subsequent work of Feldman and Kothari [FK14] improved the running time in the special case of disjunctions. We leave it as an intriguing open question to determine whether other basis functions can be used to learn halfspaces under LSL distributions."
    }, {
      "heading" : "1.2 Tail Bounds for Limited Independence",
      "text" : "The famous Hoeffding bound [Hoe63] implies that if X ∈ {±1}n is a uniform random variable and r ∈ Rn is fixed, then, for all T ≥ 0,\nP X [|X · r| ≥ T ] ≤ 2e\n− T2 2||r||22 .\nWe ask the following question:\nFor what pseudorandom X is the Hoeffding bound true?\nMore precisely, given T and δ, can we construct a pseudorandom X such that P X [|X · r| ≥ T ] ≤ δ for all r ∈ {±1}n?4 Of particular interest is the parameter regime δ = 1/poly(n) and T = Θ(||r||2 √\nlog(1/δ)). The probabilistic method gives a non-constructive proof that there exists such an X which can be sampled with seed length O(log(n/δ)). The challenge is to give an explicit construction of such an X which can be efficiently sampled with a short seed.\nThis is a very natural pseudorandomness question: Concentration of measure is a fundamental property of independent random variables and one of the key objectives of pseudorandomness research is to replicate such properties for variables with low entropy. Finding a pseudorandom X\n4For simplicity we restrict our attention to r ∈ {±1}n.\nexhibiting good concentration is also a relaxation of a more general and well-studied pseudorandomness question, namely constructing pseudorandom generators that fool linear threshold functions [DGJ+09, MZ10, GOWZ10, DSTW10]. This can also be viewed as a special case of constructing pseudorandom generators for space-bounded computation [Nis92, INW94, Rei08, BRRY10, BV10, KNP11, RSV13].\nFor δ = 1/poly(n) and T = Θ( √\nn log(1/δ)), we can construct generators X with seed length O(log2 n) using a variety of methods (including [Nis92, MZ10]). In particular, it suffices for X to be O(log(1/δ))-wise independent:\nTheorem 5 (Tail Bound for Limited Independence). Let n ≥ 1, η > 0, and δ ∈ (0, 1) be given. Let X ∈ {±1}n be k-wise independent for k = 2⌈η loge(1/δ)⌉. Set T = e(η+1)/2η √ k ||r||2. Then, for all r ∈ Rn, P [|X · r| ≥ T ] ≤ δ.\nA k-wise independentX ∈ {±1}n can be sampled with seed length O(k ·log n) [ABI85]. Another construction which achieves seed length O(log n · log(1/δ)) is to sample X from a small-bias space [NN93]. Very recently, Gopalan et al. [GKM14] constructed a new generator with seed length Õ(log(n/δ)), which is nearly optimal.\nIn this work, we ask whether the tail bound of Theorem 5 for k-wise independence is tight. That is, can we prove stronger tail bounds for k-wise independent X?\nQuestion 6. How much independence is needed for X to satisfy a Hoeffding-like tail bound? That is, what is the minimum k = k(n, δ, T ) for which any k-wise independent X ∈ {±1}n satisfies\nP X [|X · r| ≥ T ] ≤ δ\nfor all r ∈ {−1, 1}n, where · denotes the inner product."
    }, {
      "heading" : "1.2.1 Our Results",
      "text" : "Theorem 5 shows that k(n, δ, T ) ≤ O(log(1/δ)) for T = O( √\nn log(1/δ)). In this work, we show that this is essentially tight:\nTheorem 7. For T = c √\nn log(1/δ) (c > 5), we have k(n, δ, T ) = Ω(logc(1/δ)) for sufficiently large n.\nThe only previous lower bound was\nk(n, δ, T )) ≥ Ω ( log(1/δ)\nlog n\n)\n,\nwhich holds for any T ≤ n and is due to [SSS95]. This is useful if δ < n−ω(1), but the lower bound is constant in our parameter regime. This lower bound follows from the fact that a random variable X with support size s cannot give a tail bound with δ < 1/s, and that there exist k-wise independent distributions with support size s ≤ O(nk).\nThe most natural way to prove Theorem 7 would be to construct a family of k-wise independent distributions that do not satisfy the required tail bound. However, we instead study the dual formulation of the problem (following [Baz09, DETT10]) and then use lower bound techniques\nfrom approximation theory. To the best of our knowledge, this indirect approach is novel. Our results imply the existence of k-wise independent distributions with poor tail bounds, but give no immediate indication as to how to construct them!\nWe now describe the proof idea in slightly more detail. The answer to Question 6 can be posed in terms of the value of a certain linear program. The variables represent the probability distribution of the random variable X and the constraints force X to be k-wise independent. The objective of the linear program is maximize P [|X · r| ≥ T ]. Thus, the value of the program is at most δ if and only if k ≥ k(n, δ, T ). Taking the dual of this linear program and appealing to strong duality yields an alternative characterization of k(n, δ, T ). Namely, k(n, δ, T ) is the smallest k for which the threshold function FT (x) = 1(|x| ≥ T ) admits an upper sandwiching polynomial of degree k and expectation at most δ. Here, an upper sandwiching polynomial is simply a polynomial p for which p(x) ≥ FT (x) pointwise.\nWe then use ideas from weighted approximation theory to give a lower bound on k for which such sandwiching polynomials exist. In order to apply these ideas, we make a few symmetrization and approximation arguments to reduce the problem to a continuous one-dimensional problem: Find a degree lower bound for a univariate polynomial that is a good upper sandwich for the function fT (x) = sgn(|x|−T ), with respect to a Gaussian distribution. As in our proof of Theorem 2, the solution of this problem appeals to a weighted Markov-type inequality. Again, the idea is that an upper sandwich for fT must have a large jump at the threshold T , which is impossible for low-degree polynomials. The formal proof of this claim is based on a variant of an “infinite-finite range” inequality, which asserts that the weighted norm of a polynomial on the real line is bounded by its norm on a finite interval."
    }, {
      "heading" : "2 Agnostically Learning Halfspaces",
      "text" : "The class of log-concave distributions over Rn (defined below) is essentially the broadest under which we know how to agnostically learn halfspaces. While many distributions used in machine learning are log-concave, such as the normal, Laplace, beta, and Dirichlet distributions, log-concave distributions do not capture everything. For instance, the log-normal distribution and heavier-tailed exponential power law distributions are not log-concave. The main motivating question for this section is whether we can relax the assumption of log-concavity for agnostically learning halfspaces. To this end, we show a negative result: for LSL distributions, agnostic learning of halfspaces will require new techniques."
    }, {
      "heading" : "2.1 Background",
      "text" : "Our starting point is the work of Kalai et al. [KKMS08]. Among their results is the following.\nTheorem 8 ([KKMS08]). The concept class of halfspaces over Rn is agnostically learnable in time poly(nOε(1)) under log-concave distributions.\nA log-concave distribution is an absolutely continuous probability distribution such that the logarithm of the probability density function is concave. For example, the standard multivariate Gaussian distribution on Rn has the probability density function x 7→ e−||x||22/2/(2π)n/2. The natural logarithm of this is −||x||22/2 − n/2 · log(2π), which is concave. The class of log-concave distributions also includes the Laplace distribution and other natural distributions. However, it\ndoes not contain heavy-tailed distributions (such as power laws) nor non-smooth distributions (such as discrete probability distributions).\nKalai et al. also show that we can agnostically learn halfspaces under the uniform distribution over the hypercube {±1}n or over the unit sphere {x ∈ Rn : ||x||2 = 1}."
    }, {
      "heading" : "2.2 The L1 Regression Algorithm",
      "text" : "The results of Kalai et al. are based on the so-called L1 regression algorithm, which relies on being able to approximate the concept class in question by a low-degree polynomial:\nTheorem 9 ([KKMS08]). Fix a distribution D on X × {±1} and a concept class C ⊂ {f : X → {±1}}.5 Suppose that, for all f ∈ C, there exists a polynomial p : X → R of degree at most d such that E\nx∼DX [|p(x)− f(x)|] ≤ ε, where DX is the marginal distribution of D on X. Then, with\nprobability 1− δ the L1 regression algorithm outputs a hypothesis h such that\nP (x,y)∼D [h(x) 6= y] ≤ min f∈C P (x,y)∼D [f(x) 6= y] + ε\nin time poly(nd, 1/ε, log(1/δ)) with access only to examples drawn from D.\nThe L1 regression algorithm solves a linear program to find a polynomial p of degree at most d that minimises ∑\ni |p(xi)− yi|, where (xi, yi) are the examples sampled from D. The hypothesis is then h(x) = sgn(p(x)− t), where t ∈ [−1, 1] is chosen to minimise the error of h on the examples.\nGiven Theorem 9, proving Theorem 8 reduces to showing that halfspaces can be approximated by low-degree polynomials under the distributions we are interested in. It is important to note that making assumptions on the distribution is necessary (barring a major breakthrough): Agnostically learning halfspaces under arbitrary distributions is at least as hard as PAC learning DNF formulas [LBW95]. Moreover, proper learning of halfspaces under arbitrary distributions is known to be NP-hard [FGKP06].\nIn fact, we can reduce the task of approximating a halfspace to a one-dimensional problem. A halfspace is given by f(x) = sgn(w · x − θ) for some w ∈ Rn and θ ∈ R. It suffices to find a univariate polynomial p of degree at most d such that E\nx∼Dw,θ [|p(x)− sgn(x)|] ≤ ε, where Dw,θ is\nthe distribution of w · x− θ when x is drawn from DX . If DX is log-concave, then so is Dw,θ."
    }, {
      "heading" : "2.3 On the Density of Polynomials",
      "text" : "In this section, we give some intuition for why one might expect that polynomial approximations do not suffice for learning under LSL distributions. It turns out that under a LSL distribution w, polynomials actually fail to be dense in the space C0[w] of continuous functions vanishing at infinity when weighted by w. This is in stark contrast to the classical Weierstrass approximation theorem, which asserts that the polynomials are dense in C0 under the uniform weight. These kinds of results address Bernstein’s approximation problem [Ber24], a precise statement of which is as follows.\n5Here X = Rn.\nQuestion 10. Let w : R → [0, 1] be a measurable function. Let C0[w] denote the space of continuous functions f for which lim|x|→∞ f(x)w(x) = 0. Under what conditions on w is it true that for every f ∈ C0[w], there is a sequence of polynomials {pn}∞n=1 for which\nlim n→∞\n‖(pn − f)w‖∞ = 0?\n(The choice of the L∞ norm here appears to make very little difference). If Bernstein’s problem admits a positive resolution, we say that the polynomials are dense in C0[w]. The excellent survey of Lubinsky [Lub07] presents a number of criteria for when polynomials are dense. The one that is most readily applied was proved by Carleson [Car51] (but appears to be implicit in [IK37]):\nTheorem 11. Let w be even and positive with log(w(ex)) concave. Then the polynomials are dense in C0[w] iff\n∫ ∞\n0\nlogw(x)\n1 + x2 dx = −∞.\nThis immediately yields the following dichotomy result for exponential power distributions:\nCorollary 12. For γ > 0 and wγ(x) = exp(−|x|γ), the polynomials are dense in C0[wγ ] iff γ ≥ 1.\nIn particular, this justifies our assertion that the polynomials fail to be dense in the continuous functions under LSL distributions.\nSo what does this have to do with agnostically learning halfspaces? Recall that the analysis of the L1-regression algorithm of Kalai et al. [KKMS08] reduces approximating a halfspace under a distribution D to the problem of approximating each threshold function sgn(x − θ) under each marginal distribution of D. So for the algorithm to work, we require D to have marginals w under which sgn(x − θ) can be approximated arbitrarily well by polynomials. Now if the polynomials are dense in C0[w], then threshold functions can also be approximated arbitrarily well (since C0[w] is in turn dense in L1[w]). Such an appeal to density actually underlies Kalai et al.’s proof of approximability under log-concave distributions. On the other hand, if the polynomials fail to be dense, then one might conjecture that thresholds cannot be arbitrarily well approximated.\nOur result, presented in the next section, confirms the conjecture that even the sign function cannot be approximated arbitrarily well by polynomials under LSL distributions."
    }, {
      "heading" : "2.4 Lower Bound for One Variable",
      "text" : "Consider the LSL density function\nwγ(x) := C(γ) exp(−|x|γ)\non the reals for γ ∈ (0, 1), where C(γ) is a normalizing constant. Define the sign function sgn(x) = 1 if x ≥ 0 and sgn(x) = −1 otherwise. In this section, we show that for sufficiently small ε, the sign function does not have an L1 approximation under the distribution wγ . More formally,\nProposition 13. For any γ ∈ (0, 1), there exists an ε = ε(γ) such that for any polynomial p, ∫\nR\n|p(x)− sgn(x)|wγ(x) dx > ε.\nThe proof is based on the following Markov-type inequality, which roughly says that a bounded polynomial cannot have a large derivative (under the weight wγ). This implies the claim, since the sign function we are trying to approximate has a large “jump” at the origin.\nLemma 14. For γ ∈ (0, 1) there is a constant M(γ) such that\nsup x∈R\n(|p′(x)|wγ(x)) ≤ M(γ) ∫\nR\n|p(x)|wγ(x) dx.\nProof. The lemma is a combination of a Markov-type inequality and a Nikolskii-type, available in a survey of Nevai [Nev86]:\nTheorem 15 ([NT86], [Nev86, Theorem 4.17.4]). There exists a constant C1(γ) such that for any polynomial p,\n∫\nR\n|p′(x)|wγ(x) dx ≤ C1(γ) ∫\nR\n|p(x)|wγ(x) dx.\nTheorem 16 ([NT87], [Nev86, Theorem 4.17.5]). There exists a constant C2(γ) such that for any polynomial p,\nsup x (|p(x)|wγ(x)) ≤ C2(γ)\n∫\nR\n|p(x)|wγ(x) dx.\nProof of Proposition 13. Fix ε ∈ (0, 1) and suppose p is a polynomial satisfying ∫\nR\n|p(x)− sgn(x)|wγ(x) dx ≤ ε.\nSince the absolute value of the sign function integrates to 1, this forces ∫\nR\n|p(x)|wγ(x) dx ≤ 1 + ε ≤ 2.\nTherefore, we have by Lemma 14 that |p′(x)|wγ(x) ≤ 2M(γ) for every x. The idea is now to show that there is some x0 for which |p′(x0)|wγ(x0) ≥ Ω(1/ε). To see this, let δ = 4ε/C(γ) and observe that there must exist some x+ ∈ [0, δ] such that p(x+) ≥ 1/2. If this were not the case, then we would have\n∫\nR\n|p(x)|wγ(x) dx ≥ 1\n2\n∫ δ\n0 C(γ) exp(−δγ) ≥ ε\nfor δ small enough to make exp(−δγ) ≥ 1/2, yielding a contradiction. A similar argument shows that there is some x− ∈ [−δ, 0] with p(x−) ≤ −1/2. Therefore, by the mean value theorem, there is some x0 ∈ [x−, x+] with p′(x0) ≥ 1/2δ = C(γ)/8ε. Moreover, because we took δ small enough, we also have p′(x0)w(x0) ≥ C(γ)/16ε. This shows that no polynomial ε-approximates sgn as long as ε < C/32M .\nMoreover, the proposition shows that it is impossible to get arbitrarily close polynomial approximations to halfspaces under densities w for which there are constants C and γ ∈ (0, 1) with w(x) ≥ C exp(−|x|γ) for all x ∈ R. This shows that LSL distributions on R do not support polynomial approximations to halfspaces."
    }, {
      "heading" : "2.5 Extending the Lower Bound to Multivariate Distributions",
      "text" : "It is straightforward to extend the lower bound from the previous section to product distributions with LSL marginals.\nTheorem 17. Let X = (X1, . . . ,Xn) be a random variable over R n with density fX(x) = w(x1)f(x2, . . . , xn). Suppose the density w specifies a univariate γ-LSL distribution. Then there exists an ε = ε(γ) such that for any polynomial p,\n∫\nRn |p(x1, . . . , xn)− sgn(x1)|fX(x1, . . . , xn) dx1dx2 . . . dxn > ε.\nThat is, the linear threshold function sgn(x1) cannot be approximated arbitrarily well by polynomials.\nProof. Let p(x1, . . . , xn) be a polynomial, and define a univariate polynomial q by “averaging out” the variables x2, . . . , xn:\nq(x1) :=\n∫\nRn−1 p(x1, . . . , xn)f(x2, . . . , xn) dx2 . . . dxn.\nThen we have ∫\nR\n|q(x1)− sgn(x1)|w(x1) dx1 = ∫\nR\n∣ ∣ ∣ ∣ ∫\nRn−1 (p(x1, . . . , xn)− sgn(x1))f(x2, . . . , xn) dx2 . . . dxn\n∣ ∣ ∣ ∣ w(x1) dx1\n≤ ∫\nR\n(∫\nRn−1 |p(x1, . . . , xn)− sgn(x1)|f(x2, . . . , xn) dx2 . . . dxn\n)\nw(x1) dx1\n=\n∫\nRn\n∫\nRn |p(x1, . . . , xn)− sgn(x1)|fX(x1, . . . , xn) dx1dx2 . . . dxn.\nBy Proposition 13, the latter quantity must be at least ε(γ).\nLet wnγ (x) ∝ exp(−(|x1|γ + · · · + |xn|γ)) denote the density of the prototypical multivariate LSL distribution, with each marginal having the same exponential power law distribution. Our impossibility result holds uniformly for every distribution in the sequence {wnγ }. That is, for every γ ∈ (0, 1), there exists ε = ε(γ) for which halfspaces cannot be learned by polynomials under any of the distributions specified by {wnγ }.\nAs a consequence, we get inapproximability results for several natural classes of distributions that dominate {wnγ } by constant factors (i.e. not growing with n).\n1. Any power-law distribution, i.e. a distribution with density ∝ ‖x‖−M for some constant M , since such a distribution dominates every wnγ .\n2. Multivariate generalizations of the log-normal distribution, i.e. any distribution with density ∝ exp(− polylog(‖x‖)).\n3. Multivariate exponential power distributions, which have densities ∝ exp(−‖x‖γ) for γ ∈ (0, 1). These distributions dominate the prototypical wnγ by the inequality of ℓp-norms:\n‖x‖γ ≤ |x1|γ + · · ·+ |xn|γ\nfor every 0 ≤ γ ≤ 2."
    }, {
      "heading" : "3 Tail Bounds for Limited Independence",
      "text" : "Our proof consists of three steps:\n§3.1 First we reformulate the question of tail bounds for k-wise independent distributions using linear programming duality and symmetrisation. This reduces the problem to proving a degree lower bound on univariate polynomials. Namely we need to give a lower bound on the degree of a polynomial p : {0, 1, · · · n} → R such that p(i) ≥ 0 for all i, p(i) ≥ 1 if |i− n/2| ≥ T , and E [p(i)] ≤ δ, where i is drawn from the binomial distribution.\n§3.2 We then transform the problem from one about polynomials with a discrete domain to one about polynomials with a continuous domain. This amounts to showing that, since E [p(i)] ≤ δ with respect to the binomial distribution, we can bound E [p(x+ n/2)] with respect to a\ntruncated Gaussian distribution on x.\n§3.3 Finally we can apply the tools of weighted approximation theory. We know that p(x+ n/2) is small for x near the origin, but p(T + n/2) ≥ 1. We show that any low-degree polynomial that is bounded near the origin cannot grow too quickly. This implies that p must have high degree."
    }, {
      "heading" : "3.1 Dual Formulation",
      "text" : "Question 6 from the introduction is equivalent to finding the smallest k for which the value of the following linear program is at most δ.\nLinear Program Formulation of Question 6\nmax ψ\n∑\nx∈{−1,1}n ψ(x)FT (x)\ns.t. ∑\nx∈{−1,1}n ψ(x)χS(x) = 0 for all |S| ≤ k\n∑\nx∈{−1,1}n ψ(x) = 1 0 ≤ ψ(x) ≤ 1 for all x ∈ {−1, 1}n.\nHere, FT (x) = 1 if |x| ≥ T and is 0 otherwise, and χS(x) is the Fourier character corresponding to S ⊆ [n].\nIf we set P X [X = x] = φ(x), then the constraints impose that X is a k-wise independent distri-\nbution, while the objective function is P X\n[∣\n∣ ∣\n∑ i∈[n]Xi ∣ ∣ ∣ ≥ T ] . Thus the above linear program finds\nthe k-wise independent distribution with the worst tail bound. If the value of the program is at most δ, then all k-wise independent distributions satisfy the tail bound, as required.\nTaking the dual of the above linear program yields the following.\nDual Formulation of Question 6\nmin p\n2−n ∑\nx∈{−1,1}n p(x)\ns.t. deg(p) ≤ k p(x) ≥ FT (x) for all x ∈ {−1, 1}n.\nBy strong duality, the value of the dual linear program is the same as that of the primal. The multilinear polynomial p as an “upper sandwich” of FT – that is, p ≥ FT and E\nX∈{±1}n [p(X)]\nis minimal. Therefore, k(n, δ, T ) is the smallest k for which FT admits an upper sandwiching polynomial of degree k with expectation δ.\nConsider the shifted univariate symmetrization of FT\nF ′T (x) =\n{\n1 if |x− n/2| ≥ T 0 otherwise.\nBy applying the well-known Minsky-Papert symmetrization [MP72] to the dual formulation above, we get the following characterization.\nTheorem 18. The quantity k(n, δ, T ) from Question 6 is the smallest k for which there exists a degree-k univariate polynomial p : {0, . . . , n} → R such that\n1. p(i) ≥ F ′T (i) for all 0 ≤ i ≤ n and 2. 2−n ∑n\ni=0 (n i ) p(i) ≤ δ. The upper bound on k(n, δ, T ) (Theorem 5) is proved (in the appendix) by showing that\np(i) =\n(\ni− n/2 T\n)k\nsatisfies the requirements of Theorem 18 for an appropriate even k.6 So this characterisation does in fact capture how upper bounds are proved. The fact that it is a tight characterisation allows us to prove that a barrier to the technique is in fact an impossibility result.\nWith this characterisation of our problem, we may move on to proving inapproximability results."
    }, {
      "heading" : "3.2 A Continuous Version",
      "text" : "To apply techniques from the theory of weighted polynomial approximations, we move to polynomials on a continuous domain. We replace the binomial distribution upon which Theorem 18 evaluates p with a Gaussian distribution.\nDefine the probability density function\nw(x) = 1√ π e−x 2 .\nWe define the L∞ norm with respect to the weight w:\n‖g‖L∞(S) = sup x∈S |g(x)|w(x).\nNow we can give the continuous version of the problem:\n6While our results show that this polynomial is asymptotically optimal, numerical experiments have shown that it is not exactly optimal.\nTheorem 19. Let T = c √ n log(1/δ) for c ≥ 5, and d = k(n, δ, T ). Assume n ≥ (12c)2(3 log(1/δ))3. Then for T ′ = 4cT/ √ n, there is a degree d polynomial q such that\n1. q(T ′) = q(−T ′) ≥ 1 and\n2. ‖q‖L∞[−√d,√d] ≤ δ 0.9(n+ 1).\nThe following lemma is key to moving from the discrete to the continous setting. It shows that if a polynomial is bounded at evenly spaced points, then it must also be bounded between those points, assuming the number of points is sufficiently large relative to the degree.\nLemma 20. [EZ64, RC66, NS94] Let q be a polynomial of degree d such that |q(i)| ≤ 1 for i = 0, 1, . . . ,m, where 3d2 ≤ m. Then |q(x)| ≤ 32 for all x ∈ [0,m].\nProof. Let a = maxx∈[0,m] |q′(x)|. Then by the mean value theorem, |q(x)| ≤ 1+a/2 for x ∈ [0,m]. By Markov’s inequality ([Mar90], see also [Che82]),\na ≤ 2d 2(1 + a/2)\nm .\nRearranging gives a\n2 + a ≤ d\n2\nm ≤ 1 3 .\nTherefore, a ≤ 1, and hence |q(x)| ≤ 32 for x ∈ [0,m].\nWe also require the following anti-concentration lemma.\nLemma 21. (\nn\nn/2 + α √ n\n) ≥ 2 n−6α2\nn+ 1 .\nProof. It is well known via Stirling’s approximation that (n k ) ≥ 2nH(k/n)/(n+1), whereH(·) denotes the binary entropy function. We estimate\nH\n(\n1 2 + α√ n\n) ≥ ( 1\n2 + α√ n\n)(\n1− 2α (log 2) √ n\n)\n+\n(\n1 2 − α√ n\n)(\n1 + 2α\n(log 2) √ n\n)\n≥ 1− 4α 2\n(log 2)n ,\nwhich concludes the proof.\nProof of Theorem 19. Let p be the polynomial promised by Theorem 18. By Theorem 5, we know that d ≤ 3 log(1/δ). Define\nq(x) = p(x √ n/4c+ n/2).\nThen q(±T ′) = p(±T + n/2) ≥ FT (±T + n/2) = 1, dispensing with the first claim. Now for all integers i in the interval n/2± √ nd/4c, we have\n2−n ( n\ni\n)\n|p(i)| ≤ δ\nand hence, by Lemma 21,\n|p(i)| ≤ 2 nδ\n( n n/2+ √ nd/4c\n) ≤ (n+ 1)δ26d/16c2 ≤ (n+ 1)δ1−18/16c2 ≤ (n+ 1)δ0.9.\nBy Lemma 20, |p(x)| ≤ 32(n+1)δ0.9 on the whole interval n/2± √ nd/4c. Thus |q(x)| ≤ 32(n+1)δ0.9\non [− √ d, √ d], completing the proof."
    }, {
      "heading" : "3.3 The Lower Bound",
      "text" : "Now we state the result we need from approximation theory. The following “infinite-finite range inequality” shows that the norm of weighted polynomial on the real line is determined by its norm on a finite interval around the origin. Thus, an upper bound on the magnitude of a polynomial near the origin yields a bound on its growth away from the origin.. We will apply this to the polynomial given to us in Theorem 19.\nTheorem 22. For any polynomial p of degree d and B > 1,\n‖p‖L∞(R\\[−B√d,B√d]) ≤ (2eB) d exp(−B2d)‖p‖L∞[−√d,√d].\nThe proof follows [Lub07, Theorem 6.1] and [Nev86, Theorem 4.16.12].\nProof. Let p̃ be a polynomial of degree d. Let Td(x) denote the dth Chebyshev polynomial of the first kind [Che82]. By the extremal properties of Td, we have\n|p̃(x)| ≤ |Td(x)| (\nmax t∈[−1,1]\n|p̃(t)| ) ≤ (2|x|)d (\nmax t∈[−1,1]\n|p̃(t)| )\nfor |x| ≥ 1. Rescaling p(x) = p̃(x/ √ d) yields\n|p(x)| ≤ ( 2|x|√ d\n)d (\nmax t∈[− √ d, √ d] |p(t)|\n)\n≤ √ πed ( 2|x|√ d )d ‖p‖L∞[−√d,√d]\nfor |x| ≥ √ d. Now let |x| = B √ d for some B > 1. Then\n|p(x)|w(x) ≤ ed(2B)d exp(−B2d)‖p‖L∞ [−√d,√d].\nSince the coefficient (2eB)d exp(−B2d) is decreasing in B, this proves the claim.\nThe above approximation theory result, combined with our continuous formulation Theorem 19, enables us to complete the proof.\nTheorem 23. Let T = c √ n log(1/δ) for c ≥ 5. Assume n ≥ (12c)2(3 log(1/δ))3. Then k(n, δ, T ) > log(1/δ)/9 log c. Proof. Let q be the polynomial given by Theorem 19. Let T ′ = 4cT/ √ n, d = log(1/δ)/9 log c, and B = T ′/ √ d = 12c2 √ log c. For the sake of contradiction, we suppose that q satisfies the conditions of Theorem 19, but deg(q) ≤ d. Then\n‖q‖L∞(R\\[−B√d,B√d]) = ‖q‖L∞(R\\[−T ′,T ′]) ≥ exp(−T ′2)√\nπ .\nOn the other hand, applying Theorem 22, gives\n‖q‖L∞(R\\[−B√d,B√d]) ≤ (2eB) d exp(−T ′2)δ0.9(n+ 1).\nCombining the two inequalities gives\n1√ π ≤ (2eB)dδ0.9(n+ 1) ≤ ( 24ec2 √ log(c) )log(1/δ)/9 log(c) δ0.9(n+ 1) ≤ δ1/3(n+ 1),\nwhich is a contradiction.\nTheorem 23 yields Theorem 7."
    }, {
      "heading" : "4 Further Work",
      "text" : "Our negative results naturally suggest a number of directions for future work. Are halfspaces agnostically learnable under LSL distributions? Our negative result does not even necessarily rule out the use of L1 regression for this task: The polynomial regression algorithm of Kalai et al. [KKMS08] is in fact quite flexible. Nothing is really special about the basis of lowdegree monomials, and the algorithm works equally well over any small, efficiently evaluable “feature space”. That is, if we can show that halfspaces are well-approximated by linear combinations of features from a feature space F under a distribution D, then we can agnostically learn halfspaces with respect to D in time proportional to |F|. Could one hope for such approximations? Wimmer [Wim10] and Feldman and Kothari [FK14] have shown how to use non-polynomial basis functions to obtain faster learning algorithms on the boolean hypercube. On the other hand, recent work of Dachman-Soled et al. [DFT+14] shows that, at least for product distributions on the hypercube, polynomials yield the best basis for L1 regression.\nAre there other suitable derandomizations of concentration inequalities? In this work, we focused on understanding the limits of k-wise independent distributions. Gopalan et al. [GKM14] gave a much more sophisticated generator with nearly optimal seed length. But could simple, natural pseudorandom distributions, such as small-bias spaces, give strong tail bounds themselves?"
    }, {
      "heading" : "5 Acknowledgements",
      "text" : "We thank Varun Kanade, Scott Linderman, Raghu Meka, Jelani Nelson, Justin Thaler, Salil Vadhan, Les Valiant, and several anonymous reviewers for helpful discussions and comments."
    }, {
      "heading" : "A Upper Bound for Limited Independence",
      "text" : "Theorem 5 follows from the following well-known [SSS95, BR94] lemma, which we prove for completeness.\nLemma 24. Let X ∈ {±1}n be uniform and r ∈ Rn. For all even k ≥ 2,\nE\n[ (X · r)k ] ≤ ( e ||r||22 k )k/2 .\nAn even stronger form of Lemma 24 follows immediately from the hypercontractivity theorem [Bon70] [O’D14, §9]: Letting f(x) = x · r, we have\nE\n[ (X · r)k ] = ||f ||kk ≤ ( (k − 1)deg(f)/2 ||f ||2 )k = (√ k − 1 ||r||2 )k ,\nas required. A self-contained proof follows.\nProof. We start by bounding the moment generating function of X · r: Let t ∈ R be fixed later. For any i ∈ [n], we have\nE [ etriXi ] = 1\n2\n( etri + e−tri ) = ∞ ∑\nk=0\n(tri) k + (−tri)k 2k!\n= ∞ ∑\nk=0\n(tri) 2k\n(2k)! ≤\n∞ ∑\nk=0\n(t2r2i ) k\n2kk! = et 2r2i /2.\nBy independence,\nE\n[ et(X·r) ] = n ∏\ni=1\nE [ etriXi ] ≤ n ∏\ni=1\net 2r2i /2 = et 2||r||22/2.\nNow we have\nE\n[ et(X·r) ] =\n∞ ∑\nk=0\ntk k! E [ (X · r)k ] ≤ et2||r||22/2.\nWe wish to bound a single moment, namely E [ (X · r)k∗ ]\nfor an even k∗. We do this by picking\none term out of the above infinite sum. We have E [ (X · r)k ] ≥ 0 for even k, so these terms can be removed from the sum without increasing it. By changing the sign of t, we can ensure that the sum of the odd terms is positive and thus\ntk∗ k∗! E [ (X · r)k∗ ]\n≤ ∞ ∑\nk=0\ntk k! E [ (X · r)k ] = E [ et(X·r) ] ≤ et2||r||22/2.\nRearranging and setting t = ± √ k∗/ ||r||2, we obtain\nE\n[ (X · r)k∗ ] ≤ k∗! tk∗ et 2||r||22/2 = k∗! ||r||k∗2 ek∗/2√ k∗ k∗ ≤ ( k2∗ ||r||22 e k∗ )k∗/2 = (e ||r||22 k∗)k∗/2,\nas required.\nNow we can prove the upper bound for k-wise independence using the connection between moment bounds and tail bounds [SSS95].\nProof of Theorem 5. Note that, if X ∈ {±1}n is k-wise independent, then\nE\n[ (X · r)k ] = ∑\ni1···ik∈[n]\n\n\nk ∏\nj=1\nrij\n\n · E\n\n\nk ∏\nj=1\nXij\n\n\nis the same as for uniform X, as this is the expectation of a degree-k polynomial. By Lemma 24 and Markov’s inequality, we have (assuming k is even),\nP [|X · r| ≥ T ] = P [ (X · r)k ≥ T k ] ≤ E [ (X · r)k ]\nT k ≤\n(\ne ||r||22 k T 2\n)k/2\n.\nSubstituting k = 2⌈η loge(1/δ)⌉ and T = e(η+1)/2η √ k ||r||2, we have\nP [|X · r| ≥ T ] ≤ ( e ||r||22 k (e(η+1)/2η √ k ||r||2)2\n)⌈η loge(1/δ)⌉\n= e−⌈η loge(1/δ)⌉/η ≤ δ."
    } ],
    "references" : [ {
      "title" : "A fast and simple randomized parallel algorithm for the maximal independent set problem",
      "author" : [ "Noga Alon", "Laszlo Babai", "Alon Itai" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Alon et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 1985
    }, {
      "title" : "Quantum lower bounds for the collision and the element distinctness problems",
      "author" : [ "Scott Aaronson", "Yaoyun Shi" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Aaronson and Shi.,? \\Q2004\\E",
      "shortCiteRegEx" : "Aaronson and Shi.",
      "year" : 2004
    }, {
      "title" : "Polylogarithmic independence can fool DNF formulas",
      "author" : [ "Louay M.J. Bazzi" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Bazzi.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bazzi.",
      "year" : 2009
    }, {
      "title" : "Quantum lower bounds by polynomials",
      "author" : [ "Robert Beals", "Harry Buhrman", "Richard Cleve", "Michele Mosca", "Ronald de Wolf" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Beals et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Beals et al\\.",
      "year" : 2001
    }, {
      "title" : "The polynomial method in circuit complexity. In Structure in Complexity Theory Conference, pages 82–95",
      "author" : [ "Richard Beigel" ],
      "venue" : "IEEE Computer Society,",
      "citeRegEx" : "Beigel.,? \\Q1993\\E",
      "shortCiteRegEx" : "Beigel.",
      "year" : 1993
    }, {
      "title" : "Perceptrons, PP, and the polynomial hierarchy",
      "author" : [ "Richard Beigel" ],
      "venue" : "Computational Complexity,",
      "citeRegEx" : "Beigel.,? \\Q1994\\E",
      "shortCiteRegEx" : "Beigel.",
      "year" : 1994
    }, {
      "title" : "Le problème de l’approximation des fonctions continues sur tout l’axe",
      "author" : [ "S.N. Bernstein" ],
      "venue" : "réel et l’une de ses applications. Bull. Math. Soc. France,",
      "citeRegEx" : "Bernstein.,? \\Q1924\\E",
      "shortCiteRegEx" : "Bernstein.",
      "year" : 1924
    }, {
      "title" : "Weakly learning DNF and characterizing statistical query learning using Fourier analysis",
      "author" : [ "Avrim Blum", "Merrick Furst", "Jeffrey Jackson", "Michael Kearns", "Yishay Mansour", "Steven Rudich" ],
      "venue" : "In Proceedings of the twenty-sixth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Blum et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 1994
    }, {
      "title" : "Étude des coefficients de fourier des fonctions de lp(g)",
      "author" : [ "Aline Bonami" ],
      "venue" : "Annales de l’institut Fourier,",
      "citeRegEx" : "Bonami.,? \\Q1970\\E",
      "shortCiteRegEx" : "Bonami.",
      "year" : 1970
    }, {
      "title" : "Polynomial regression under arbitrary product distributions",
      "author" : [ "Eric Blais", "Ryan O’Donnell", "Karl Wimmer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Blais et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Blais et al\\.",
      "year" : 2010
    }, {
      "title" : "Randomness-efficient oblivious sampling",
      "author" : [ "M. Bellare", "J. Rompel" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Bellare and Rompel.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bellare and Rompel.",
      "year" : 1994
    }, {
      "title" : "Pseudorandom generators for regular branching programs",
      "author" : [ "Mark Braverman", "Anup Rao", "Ran Raz", "Amir Yehudayoff" ],
      "venue" : null,
      "citeRegEx" : "Braverman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Braverman et al\\.",
      "year" : 2010
    }, {
      "title" : "The coin problem and pseudorandomness for branching programs",
      "author" : [ "Joshua Brody", "Elad Verbin" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Brody and Verbin.,? \\Q2010\\E",
      "shortCiteRegEx" : "Brody and Verbin.",
      "year" : 2010
    }, {
      "title" : "Bernstein’s approximation problem",
      "author" : [ "Lennart Carleson" ],
      "venue" : "Proc. Amer. Math. Soc.,",
      "citeRegEx" : "Carleson.,? \\Q1951\\E",
      "shortCiteRegEx" : "Carleson.",
      "year" : 1951
    }, {
      "title" : "Introduction to Approximation Theory",
      "author" : [ "E.W. Cheney" ],
      "venue" : "AMS Chelsea Publishing Series. AMS Chelsea Pub.,",
      "citeRegEx" : "Cheney.,? \\Q1982\\E",
      "shortCiteRegEx" : "Cheney.",
      "year" : 1982
    }, {
      "title" : "Improved pseudorandom generators for depth 2 circuits",
      "author" : [ "Anindya De", "Omid Etesami", "Luca Trevisan", "Madhur Tulsiani" ],
      "venue" : "Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques,",
      "citeRegEx" : "De et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "De et al\\.",
      "year" : 2010
    }, {
      "title" : "Approximate resilience, monotonicity, and the complexity of agnostic learning",
      "author" : [ "Dana Dachman-Soled", "Vitaly Feldman", "Li-Yang Tan", "Andrew Wan", "Karl Wimmer" ],
      "venue" : "CoRR, abs/1405.5268,",
      "citeRegEx" : "Dachman.Soled et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dachman.Soled et al\\.",
      "year" : 2014
    }, {
      "title" : "Bounded independence fools halfspaces",
      "author" : [ "Ilias Diakonikolas", "Parikshit Gopalan", "Ragesh Jaiswal", "Rocco A. Servedio", "Emanuele Viola" ],
      "venue" : "In In Proc. 50th Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Diakonikolas et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Diakonikolas et al\\.",
      "year" : 2009
    }, {
      "title" : "The complexity of learning halfspaces using generalized linear methods",
      "author" : [ "Amit Daniely", "Nati Linial", "Shai Shalev-Shwartz" ],
      "venue" : "CoRR, abs/1211.0616,",
      "citeRegEx" : "Daniely et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Daniely et al\\.",
      "year" : 2014
    }, {
      "title" : "A regularity lemma, and low-weight approximators, for low-degree polynomial threshold functions",
      "author" : [ "Ilias Diakonikolas", "Rocco A. Servedio", "Li-Yang Tan", "Andrew Wan" ],
      "venue" : "In Proceedings of the 2010 IEEE 25th Annual Conference on Computational Complexity,",
      "citeRegEx" : "Diakonikolas et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Diakonikolas et al\\.",
      "year" : 2010
    }, {
      "title" : "Schwankung von polynomen zwischen gitterpunkten",
      "author" : [ "H. Ehlich", "K. Zeller" ],
      "venue" : "Mathematische Zeitschrift,",
      "citeRegEx" : "Ehlich and Zeller.,? \\Q1964\\E",
      "shortCiteRegEx" : "Ehlich and Zeller.",
      "year" : 1964
    }, {
      "title" : "New results for learning noisy parities and halfspaces",
      "author" : [ "Vitaly Feldman", "Parikshit Gopalan", "Subhash Khot", "Ponnuswami" ],
      "venue" : "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Feldman et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2006
    }, {
      "title" : "Agnostic learning of disjunctions on symmetric distributions",
      "author" : [ "Vitaly Feldman", "Pravesh Kothari" ],
      "venue" : "CoRR, abs/1405.6791,",
      "citeRegEx" : "Feldman and Kothari.,? \\Q2014\\E",
      "shortCiteRegEx" : "Feldman and Kothari.",
      "year" : 2014
    }, {
      "title" : "Lower bounds and hardness amplification for learning shallow monotone formulas",
      "author" : [ "V. Feldman", "H. Lee", "R. Servedio" ],
      "venue" : "Journal of Machine Learning Research - COLT Proceedings,",
      "citeRegEx" : "Feldman et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2011
    }, {
      "title" : "Agnostically learning decision trees",
      "author" : [ "Parikshit Gopalan", "Adam Tauman Kalai", "Adam R. Klivans" ],
      "venue" : "In Cynthia Dwork, editor,",
      "citeRegEx" : "Gopalan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Gopalan et al\\.",
      "year" : 2008
    }, {
      "title" : "Pseudorandomness for concentration bounds and signed majorities",
      "author" : [ "Parikshit Gopalan", "Daniel Kane", "Raghu Meka" ],
      "venue" : "CoRR, abs/1411.4584,",
      "citeRegEx" : "Gopalan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gopalan et al\\.",
      "year" : 2014
    }, {
      "title" : "Fooling functions of halfspaces under product distributions",
      "author" : [ "Parikshit Gopalan", "Ryan O’Donnell", "Yi Wu", "David Zuckerman" ],
      "venue" : "In Proceedings of the 2010 IEEE 25th Annual Conference on Computational Complexity,",
      "citeRegEx" : "Gopalan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gopalan et al\\.",
      "year" : 2010
    }, {
      "title" : "Hardness of learning halfspaces with noise",
      "author" : [ "V. Guruswami", "P. Raghavendra" ],
      "venue" : "In Proceedings of FOCS",
      "citeRegEx" : "Guruswami and Raghavendra.,? \\Q2006\\E",
      "shortCiteRegEx" : "Guruswami and Raghavendra.",
      "year" : 2006
    }, {
      "title" : "Sketching and streaming entropy via approximation theory",
      "author" : [ "Nicholas J.A. Harvey", "Jelani Nelson", "Krzysztof Onak" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Harvey et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Harvey et al\\.",
      "year" : 2008
    }, {
      "title" : "Quasi-analytic class and closure of {tn} in the interval (−∞,∞)",
      "author" : [ "S. Izumi", "T. Kawata" ],
      "venue" : "Tohoku Math. J.,",
      "citeRegEx" : "Izumi and Kawata.,? \\Q1937\\E",
      "shortCiteRegEx" : "Izumi and Kawata.",
      "year" : 1937
    }, {
      "title" : "Pseudorandomness for network algorithms",
      "author" : [ "Russell Impagliazzo", "Noam Nisan", "Avi Wigderson" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Impagliazzo et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Impagliazzo et al\\.",
      "year" : 1994
    }, {
      "title" : "Efficient noise-tolerant learning from statistical queries",
      "author" : [ "M. Kearns" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Kearns.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kearns.",
      "year" : 1998
    }, {
      "title" : "Learning halfspaces under logconcave densities: Polynomial approximations and moment matching",
      "author" : [ "Daniel M. Kane", "Adam Klivans", "Raghu Meka" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Kane et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kane et al\\.",
      "year" : 2013
    }, {
      "title" : "Agnostically learning halfspaces",
      "author" : [ "Adam Tauman Kalai", "Adam R. Klivans", "Yishay Mansour", "Rocco A. Servedio" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Kalai et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kalai et al\\.",
      "year" : 2008
    }, {
      "title" : "Pseudorandom generators for group products",
      "author" : [ "Michal Koucký", "Prajakta Nimbhorkar", "Pavel Pudlák" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Koucký et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Koucký et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning geometric concepts via gaussian surface area",
      "author" : [ "Adam R. Klivans", "Ryan O’Donnell", "Rocco A. Servedio" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Klivans et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Klivans et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning DNF in time 2õ(n 1/3)",
      "author" : [ "Adam R. Klivans", "Rocco A. Servedio" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Klivans and Servedio.,? \\Q2004\\E",
      "shortCiteRegEx" : "Klivans and Servedio.",
      "year" : 2004
    }, {
      "title" : "Unconditional lower bounds for learning intersections of halfspaces",
      "author" : [ "Adam R Klivans", "Alexander A Sherstov" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Klivans and Sherstov.,? \\Q2007\\E",
      "shortCiteRegEx" : "Klivans and Sherstov.",
      "year" : 2007
    }, {
      "title" : "Cryptographic hardness for learning intersections of halfspaces",
      "author" : [ "Adam R. Klivans", "Alexander A. Sherstov" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Klivans and Sherstov.,? \\Q2009\\E",
      "shortCiteRegEx" : "Klivans and Sherstov.",
      "year" : 2009
    }, {
      "title" : "Lower bounds for agnostic learning via approximate rank",
      "author" : [ "Adam R. Klivans", "Alexander A. Sherstov" ],
      "venue" : "Computational Complexity,",
      "citeRegEx" : "Klivans and Sherstov.,? \\Q2010\\E",
      "shortCiteRegEx" : "Klivans and Sherstov.",
      "year" : 2010
    }, {
      "title" : "Toward efficient agnostic learning",
      "author" : [ "Michael Kearns", "Robert E. Schapire", "Linda M. Sellie", "Lisa Hellerstein" ],
      "venue" : "In Machine Learning,",
      "citeRegEx" : "Kearns et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 1994
    }, {
      "title" : "On efficient agnostic learning of linear combinations of basis functions",
      "author" : [ "Wee Sun Lee", "Peter L. Bartlett", "Robert C. Williamson" ],
      "venue" : "In Proceedings of the Eighth Annual Conference on Computational Learning Theory, COLT",
      "citeRegEx" : "Lee et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 1995
    }, {
      "title" : "A survey of weighted polynomial approximation with exponential weights",
      "author" : [ "Doron Lubinsky" ],
      "venue" : "Surveys in Approximation Theory,",
      "citeRegEx" : "Lubinsky.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lubinsky.",
      "year" : 2007
    }, {
      "title" : "Perceptrons: An Introduction to Computational Geometry",
      "author" : [ "Marvin Minsky", "Seymour Papert" ],
      "venue" : null,
      "citeRegEx" : "Minsky and Papert.,? \\Q1972\\E",
      "shortCiteRegEx" : "Minsky and Papert.",
      "year" : 1972
    }, {
      "title" : "Pseudorandom generators for polynomial threshold functions",
      "author" : [ "Raghu Meka", "David Zuckerman" ],
      "venue" : "In Proceedings of the Forty-second ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Meka and Zuckerman.,? \\Q2010\\E",
      "shortCiteRegEx" : "Meka and Zuckerman.",
      "year" : 2010
    }, {
      "title" : "Géza Freud, orthogonal polynomials and Christoffel functions. A case study",
      "author" : [ "Paul Nevai" ],
      "venue" : "Journal of Approximation Theory,",
      "citeRegEx" : "Nevai.,? \\Q1986\\E",
      "shortCiteRegEx" : "Nevai.",
      "year" : 1986
    }, {
      "title" : "Small-bias probability spaces: Efficient constructions and applications",
      "author" : [ "Joseph Naor", "Moni Naor" ],
      "venue" : "SIAM J. Computing,",
      "citeRegEx" : "Naor and Naor.,? \\Q1993\\E",
      "shortCiteRegEx" : "Naor and Naor.",
      "year" : 1993
    }, {
      "title" : "On the degree of boolean functions as real polynomials",
      "author" : [ "N. Nisan", "M. Szegedy" ],
      "venue" : "Computational Complexity,",
      "citeRegEx" : "Nisan and Szegedy.,? \\Q1994\\E",
      "shortCiteRegEx" : "Nisan and Szegedy.",
      "year" : 1994
    }, {
      "title" : "Weighted polynomial inequalities",
      "author" : [ "Paul Nevai", "Vilmos Totik" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "Nevai and Totik.,? \\Q1986\\E",
      "shortCiteRegEx" : "Nevai and Totik.",
      "year" : 1986
    }, {
      "title" : "Sharp Nikolskii inequalities with exponential weights",
      "author" : [ "P. Nevai", "V. Totik" ],
      "venue" : "Analysis Mathematica,",
      "citeRegEx" : "Nevai and Totik.,? \\Q1987\\E",
      "shortCiteRegEx" : "Nevai and Totik.",
      "year" : 1987
    }, {
      "title" : "Analysis of Boolean Functions",
      "author" : [ "Ryan O’Donnell" ],
      "venue" : null,
      "citeRegEx" : "O.Donnell.,? \\Q2014\\E",
      "shortCiteRegEx" : "O.Donnell.",
      "year" : 2014
    }, {
      "title" : "On the degree of polynomials that approximate symmetric boolean functions (preliminary version)",
      "author" : [ "Ramamohan Paturi" ],
      "venue" : null,
      "citeRegEx" : "Paturi.,? \\Q1992\\E",
      "shortCiteRegEx" : "Paturi.",
      "year" : 1992
    }, {
      "title" : "A comparison of uniform approximations on an interval and a finite subset thereof",
      "author" : [ "T.J. Rivlin", "E.W. Cheney" ],
      "venue" : "SIAM J. Numer. Anal.,",
      "citeRegEx" : "Rivlin and Cheney.,? \\Q1966\\E",
      "shortCiteRegEx" : "Rivlin and Cheney.",
      "year" : 1966
    }, {
      "title" : "Undirected connectivity in log-space",
      "author" : [ "Omer Reingold" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Reingold.,? \\Q2008\\E",
      "shortCiteRegEx" : "Reingold.",
      "year" : 2008
    }, {
      "title" : "Pseudorandomness for regular branching programs via fourier analysis",
      "author" : [ "Omer Reingold", "Thomas Steinke", "Salil Vadhan" ],
      "venue" : "In APPROX-RANDOM,",
      "citeRegEx" : "Reingold et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Reingold et al\\.",
      "year" : 2013
    }, {
      "title" : "Communication lower bounds using dual polynomials",
      "author" : [ "Alexander A. Sherstov" ],
      "venue" : "Bulletin of the EATCS,",
      "citeRegEx" : "Sherstov.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sherstov.",
      "year" : 2008
    }, {
      "title" : "Separating AC0 from depth-2 majority circuits",
      "author" : [ "Alexander A. Sherstov" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Sherstov.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sherstov.",
      "year" : 2009
    }, {
      "title" : "Chernoff–Hoeffding bounds for applications with limited independence",
      "author" : [ "J. Schmidt", "A. Siegel", "A. Srinivasan" ],
      "venue" : "SIAM J. Discrete Mathematics,",
      "citeRegEx" : "Schmidt et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 1995
    }, {
      "title" : "Learning kernel-based halfspaces with the 0-1 loss",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "K. Sridharan" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Faster algorithms via approximation theory",
      "author" : [ "Sushant Sachdeva", "Nisheeth K. Vishnoi" ],
      "venue" : "Foundations and Trends in Theoretical Computer Science,",
      "citeRegEx" : "Sachdeva and Vishnoi.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sachdeva and Vishnoi.",
      "year" : 2014
    }, {
      "title" : "A theory of the learnable",
      "author" : [ "Leslie G. Valiant" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "Valiant.,? \\Q1984\\E",
      "shortCiteRegEx" : "Valiant.",
      "year" : 1984
    }, {
      "title" : "Agnostically learning under permutation invariant distributions",
      "author" : [ "Karl Wimmer" ],
      "venue" : "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Wimmer.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wimmer.",
      "year" : 2010
    } ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "Polynomial approximations to boolean functions have led to many positive results in computer science. In particular, polynomial approximations to the sign function underly algorithms for agnostically learning halfspaces, as well as pseudorandom generators for halfspaces. In this work, we investigate the limits of these techniques by proving inapproximability results for the sign function. Firstly, the “polynomial regression” algorithm of Kalai et al. (SIAM J. Comput. 2008) shows that halfspaces can be learned with respect to log-concave distributions on Rn in the challenging agnostic learning model. The power of this algorithm relies on the fact that under log-concave distributions, halfspaces can be approximated arbitrarily well by low-degree polynomials. We ask whether this technique can be extended beyond log-concave distributions, and establish a negative result. We show that polynomials of any degree cannot approximate the sign function to within arbitrarily low error for a large class of non-log-concave distributions on the real line, including those with densities proportional to exp(−|x|0.99). This impossibility result extends to multivariate distributions, and thus gives a strong limitation on the power of the polynomial regression algorithm for halfspaces. Secondly, we investigate the derandomization of Chernoff-type concentration inequalities. Chernoff-type tail bounds on sums of independent random variables have pervasive applications in theoretical computer science. Schmidt et al. (SIAM J. Discrete Math. 1995) showed that these inequalities can be established for sums of random variables with only O(log(1/δ))-wise independence, for a tail probability of δ. We show that their results are tight up to constant factors. These results rely on techniques from weighted approximation theory, which studies how well functions on the real line can be approximated by polynomials under various distributions. We believe that these techniques will have further applications in other areas of theoretical computer science. Harvard University, School of Engineering and Applied Sciences. Supported by an NDSEG Fellowship and NSF grant CNS-1237235. Harvard University, School of Engineering and Applied Sciences. Supported by NSF grant CCF-1116616 and the Lord Rutherford Memorial Research Fellowship.",
    "creator" : "LaTeX with hyperref package"
  }
}