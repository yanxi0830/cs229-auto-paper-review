{
  "name" : "1306.2347.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Auditing: Active Learning with Outcome-Dependent Query Costs",
    "authors" : [ "Sivan Sabato", "Anand Sarwate", "Nathan Srebro" ],
    "emails" : [ "sivan.sabato@microsoft.com", "asarwate@ttic.edu", "nati@ttic.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Active learning algorithms seek to mitigate the cost of learning by using unlabeled data and sequentially selecting examples to query for their label to minimize total number of queries. In some cases, however, the actual cost of each query depends on the true label of the example and is thus not known before the label is requested. For instance, in detecting fraudulent credit transactions, a query with a positive answer is not wasteful, whereas a negative answer is the result of a wasteful investigation of an honest transaction, and perhaps a loss of good-will. More generally, in a multiclass setting, different queries may entail different costs, depending on the outcome of the query. In this work we focus on the binary case, and on the extreme version of the problem, as described in the example of credit frauds, in which the algorithm only pays for queries which return a negative label. We term this setting auditing, and the cost incurred by the algorithm its auditing complexity.\nThere are several natural ways to measure performance for auditing. For example, we may wish for the algorithm to maximize the number of positive labels it finds for a fixed “budget” of negative labels, or to minimize the number of negative labels while finding a certain number or fraction of positive labels. In this work we focus on the classical learning problem, in which one attempts to learn a classifier from a fixed hypothesis class, with an error close to the best possible. Similar to active learning, we assume we are given a large set of unlabeled examples, and aim to learn with minimal labeling cost. But unlike active learning, we only incur a cost when requesting the label of an example that turns out to be negative.\n∗Microsoft Research New England, Cambridge, MA USA, sivan.sabato@microsoft.com †Toyota Technological Institute at Chicago, Chicago, IL USA, asarwate@ttic.edu ‡Toyota Technological Institute at Chicago, Chicago, IL USA, nati@ttic.edu\nar X\niv :1\n30 6.\n23 47\nv4 [\ncs .L\nG ]\n1 2\nJu l 2\n01 5\nThe close relationship between auditing and active learning raises natural questions. Can the auditing complexity be significantly better than the label complexity in active learning? If so, should algorithms be optimized for auditing, or do optimal active learning algorithms also have low auditing complexity? To answer these questions, and demonstrate the differences between active learning and auditing, we study the simple hypothesis classes of thresholds and of axis-aligned rectangles in Rd, in both the realizable and the agnostic settings. We then also consider a general competitive analysis for arbitrary hypothesis classes.\nExisting work on active learning with costs (Margineantu, 2007; Kapoor et al., 2007; Settles et al., 2008; Golovin and Krause, 2011) typically assumes that the cost of labeling each point is known a priori, so the algorithm can use the costs directly to select a query. Our model is significantly different, as the costs depend on the outcome of the query itself. Kapoor et al. (2007) do mention the possibility of class-dependent costs, but this possibility is not studied in detail. An unrelated game-theoretic learning model addressing “auditing” was proposed by Blocki et al. (2011)."
    }, {
      "heading" : "Notation and Setup",
      "text" : "For an integer m, let [m] = {1, 2, . . . ,m}. The function I[A] is the indicator function of a set A. For a function f and a sub-domain X, f |X is the restriction of f to X. For vectors a and b in Rd, the inequality a ≤ b implies ai ≤ bi for all i ∈ [d].\nWe assume a data domain X and a distribution D over labeled data points in X ×{−1,+1}. A learning algorithm may sample i.i.d. pairs (X,Y ) ∼ D. It then has access to the value of X, but the label Y remains hidden until queried. The algorithm returns a labeling function ĥ : X → {−1,+1}. The error of a function h : X → {−1,+1} on D is err(D,h) = E(X,Y )∼D[h(X) 6= Y ]. The error of h on a multiset S ⊆ X ×{−1,+1} is given by err(S, h) = 1|S| ∑ (x,y)∈S I[h(x) 6= y]. The passive sample complexity of an algorithm is the number of pairs it draws from D. The active label complexity of an algorithm is the total number of label queries the algorithm makes. Its auditing complexity is the number of queries the algorithm makes on points with negative labels.\nWe consider guarantees for learning algorithms relative to a hypothesis class H ⊆ {−1,+1}X . We denote the error of the best hypothesis in H on D by err(D,H) = minh∈H err(D,h). Similarly, err(S,H) = minh∈H err(S, h). We usually denote the best error for D by η = err(D,H).\nTo describe our algorithms it will be convenient to define the following sample sizes, using universal constants C, c > 0. Let δ ∈ (0, 1) be a confidence parameter, and let ∈ (0, 1) be an error parameter. Let mag( , δ, d) = C(d + ln(c/δ))/ 2. If a sample S is drawn from D with |S| = mag( , δ, d) then with probability 1 − δ, ∀h ∈ H, err(D,h) ≤ err(S, h) + and err(S,H) ≤ err(D,H)+ (Bartlett and Mendelson, 2002). Let mν( , δ, d) = C(d ln(c/ν )+ln(c/δ))/ν2 . Results of Vapnik and Chervonenkis (1971) show that if H has VC dimension d and S is drawn from D with |S| = mν , then for all h ∈ H,\nerr(S, h) ≤ max {err(D,h)(1 + ν), err(D,h) + ν } and (1) err(D,h) ≤ max {err(S, h)(1 + ν), err(S, h) + ν } ."
    }, {
      "heading" : "2 Active Learning vs. Auditing: Summary of Results",
      "text" : "The main point of this paper is that the auditing complexity can be quite different from the active label complexity, and that algorithms tuned to minimizing the audit label complexity give improvements over standard active learning algorithms. Before presenting these differences, we note that in some regimes, neither active learning nor auditing can improve significantly over the passive sample complexity. In particular, a simple adaptation of a result of Beygelzimer et al. (2009) establishes the following lower bound.\nLemma 2.1. Let H be a hypothesis class with VC dimension d > 1. If an algorithm always finds a hypothesis ĥ with err(D, ĥ) ≤ err(D,H) + for > 0, then for any η ∈ (0, 1) there is a distribution D with η = err(D,H) such that the auditing complexity of this algorithm for D is Ω(dη2/ 2).\nThat is, when η is fixed while → 0, the auditing complexity scales as Ω(d/ 2), similar to the passive sample complexity. Therefore the two situations which are interesting are the realizable case, corresponding to η = 0, and the agnostic case, when we want to guarantee an excess error such that η/ is bounded. We provide results for both of these regimes.\nWe will first consider the realizable case, when η = 0. Here it is sufficient to consider the case where a fixed pool S of m points is given and the algorithm must return a hypothesis ĥ such that err(S, ĥ) = 0 with probability 1. A pool labeling algorithm can be used to learn a hypothesis which is good for a distribution by drawing and labeling a large enough pool. We define auditing complexity for an unlabeled pool as the minimal number of negative labels needed to perfectly classify it. It is easy to see that there are pools with an auditing complexity at least the VC dimension of the hypothesis class.\nFor the agnostic case, when η > 0, we denote α = /η and say that an algorithm (α, δ)-learns a class of distributions D with respect to H if for all D ∈ D, with probability 1 − δ, ĥ returned by the algorithm satisfies err(D, ĥ) ≤ (1 + α)η. By Lemma 2.1 an auditing complexity of Ω(d/α2) is unavoidable, be we can hope to improve over the passive sample complexity lower bound of Ω(d/ηα2) (Devroye and Lugosi, 1995) by avoiding the dependence on η.\nOur main results are summarized in Table 1, which shows the auditing and active learning complexities in the two regimes, for thresholds on [0, 1] and axis-aligned rectangles in Rd, where we assume that the hypotheses label the points in the rectangle as negative and points outside as positive.\nIn the realizable case, for thresholds, the optimal active learning algorithm performs binary search, resulting in Ω(lnm) labels in the worst case. This is a significant improvement over the\npassive label complexity of m. However, a simple auditing procedure that scans from right to left queries only a single negative point, achieving an auditing complexity of 1. For rectangles, we present a simple coordinate-wise scanning procedure with auditing complexity of at most 2d, demonstrating a huge gap versus active learning, where the labels of all m points might be required. Not all classes enjoy reduced auditing complexity: we also show that for rectangles with positive points on the inside, there exists pools of size m with an auditing complexity of m.\nIn the agnostic case we wish to (α, δ)-learn distributions with a true error of η = err(D,H), for constant α, δ. For active learning, it has been shown that in some cases, the Ω(d/η) passive sample complexity can be replaced by an exponentially smaller O(d ln(1/η)) active label complexity (Hanneke, 2011), albeit sometimes with a larger polynomial dependence on d. In other cases, an Ω(1/η) dependence exists also for active learning. Our main question is whether the dependence on η in the active label complexity can be further reduced for auditing.\nFor thresholds, active learning requires Ω(ln(1/η)) labels (Kulkarni et al., 1993). Using auditing, we show that the dependence on η can be completely removed, for any true error level η > 0, if we know η in advance. We also show that if η is not known at least approximately, the logarithmic dependence on 1/η is also unavoidable for auditing. For rectangles, we show that the active label complexity is at least Ω(d/η). In contrast, we propose an algorithm with an auditing complexity of O(d2 ln2(1/η)), reducing the linear dependence on 1/η to a logarithmic dependence. We do not know whether a linear dependence on d is possible with a logarithmic dependence on 1/η.\nMost of the proofs are provided in Appendix A."
    }, {
      "heading" : "3 Auditing for Thresholds on the Line",
      "text" : "The first question to ask is whether the audit label complexity can ever be significantly smaller than the active or passive label complexities, and whether a different algorithm is required to achieve this improvement. The following simple case answers both questions in the affirmative. Consider the hypothesis class of thresholds on the line, defined over the domain X = [0, 1]. A hypothesis with threshold a is ha(x) = I[x−a ≥ 0]. The hypothesis class is Ha = {ha | a ∈ [0, 1]}. Consider the pool setting for the realizable case. The optimal active label complexity of Θ(log2m) can be achieved by a binary search on the pool. The auditing complexity of this algorithm can also be as large as Θ(log2(m)). However, auditing allows us to beat this barrier. This case exemplifies an interesting contrast between auditing and active learning. Due to information-theoretic considerations, any algorithm which learns an unlabeled pool S has an active label complexity of at least log2 |H|S | (Kulkarni et al., 1993), where H|S is the set of restrictions of functions in H to the domain S. For Ha, the active label complexity is thus log2 |Ha|S | = Ω(log2m). However, the same considerations are invalid for auditing.\nWe showed that for the realizable case, the auditing label complexity for Ha is a constant. We now provide a more complex algorithm that guarantees this for (α, δ)-learning in the agnostic case. The intuition behind our approach is that in a pool with at most k errors, querying from highest to lowest until observing k+ 1 negative points, and finding the minimal error threshold on the labeled points, yields the optimal threshold.\nLemma 3.1. Let S be a pool of size m in [0, 1], and assume that err(S,Ha) ≤ k/m. Then the procedure above finds ĥ such that err(S, ĥ) = err(S,Ha) with an auditing complexity of k + 1.\nProof. Denote the last queried point by x0, and let ha∗ = argminh∈Ha err(S,Ha). Since err(S, ha∗) ≤\nk/m, a∗ > x0. Denote by S ′ ⊆ S the set of points queried by the procedure. For any a > x0, err(S′, ha) = err(S, ha) + |{(x, y) ∈ S | x < x0, y = 1}|/m. Therefore, minimizing the error on S′ results in a hypothesis that minimizes the error on S.\nTo learn from a distribution, one can draw a random sample and use it as the pool in the procedure above. However, the sample size required for passive (α, δ)-learning of thresholds is Ω(ln(1/η)/η). Thus, the number of errors in the pool would be k = η ·Ω(ln(1/η)/η) = Ω(ln(1/η)), which depends on η. To avoid this dependence, the auditing algorithm we propose uses Alg. 1 below to select a subset of the random sample, which still represents the distribution well, but its size is only Ω(1/η).\nAlgorithm 1: Representative Subset Selection\n1: Input: pool S = (x1, . . . , xm) (with hidden labels), xi ∈ [0, 1], ηmax ∈ (0, 1], δ ∈ (0, 1). 2: T ← max{b1/3ηmaxc, 1}. 3: Let U = {x1, . . . , x1︸ ︷︷ ︸\nT copies , . . . , xm, . . . , xm︸ ︷︷ ︸ T copies } be the multiset with T copies of each point in S.\n4: Sort and rename the points in U such that x′i ≤ x′i+1 for all i ∈ [Tm]. 5: Let Sq be an empty multiset. 6: for t = 1 to T do 7: S(t)← {x′(t−1)m+1, . . . , x ′ tm}. 8: Draw 14 ln(8/δ) random points from S(t) independently uniformly at random and add them to Sq (with duplications).\n9: end for 10: Return Sq (with the corresponding hidden labels).\nLemma 3.2. Let δ, ηmax ∈ (0, 1). Let S be a pool such that err(S,Ha) ≤ ηmax. Let Sq be the output of Alg. 1 with inputs S, ηmax, δ, and let ĥ = argminh∈Ha err(Sq,Ha). Then with probability 1− δ,\nerr(Sq, ĥ) ≤ 6ηmax and err(S, ĥ) ≤ 17ηmax.\nAlgorithm 2: Auditing for Thresholds with a constant α\n1: Input: ηmax, δ, α ∈ (0, 1), access to distribution D such that err(D,Ha) ≤ ηmax. 2: ν ← α/5. 3: Draw a random labeled pool (with hidden labels) S0 of size mν(η, δ/2, 1) from D. 4: Draw a random sample S of size mag((1 + ν)ηmax, δ/2, 1) uniformly from S0. 5: Get a subset Sq using Alg. 1 with inputs S, 2(1 + ν)ηmax, δ/2. 6: Query points in Sq from highest to lowest. Stop after d12|Sq|(1 + ν)ηmaxe+ 1 negatives. 7: Find â such that hâ minimizes the error on the labeled part of Sq. 8: Let S1 be the set of the 36(1 + ν)ηmax|S0| closest points to â in S from each side of â. 9: Draw S2 of size m\nag(ν/72, δ/2, 1) from S1 (see definition on page 2). 10: Query all points in S2, and return ĥ that minimizes the error on S2.\nThe algorithm for auditing thresholds on the line in the agnostic case is listed in Alg. 2. This algorithm first achieves (C, δ) learning of Ha for a fixed C (in step 7, based on Lemma 3.2 and\nLemma 3.1, and then improves its accuracy to achieve (α, δ)-learning for α > 0, by additional passive sampling in a restricted region. The following theorem provides the guarantees for Alg. 2.\nTheorem 3.3. Let ηmax, δ, α ∈ (0, 1). Let D be a distribution with error err(D,Ha) ≤ ηmax. Alg. 2 with input ηmax, δ, α has an auditing complexity of O(ln(1/δ)/α\n2), and returns ĥ such that with probability 1− δ, err(D, ĥ) ≤ (1 + α)ηmax.\nIt immediately follows that if η = err(D,H) is known, (α, δ)-learning is achievable with an auditing complexity that does not depend on η. This is formulated in the following corollary.\nCorollary 3.4 ((α, δ)-learning for Ha). Let η, α, δ ∈ (0, 1]. For any distribution D with error err(D,Ha) = η, Alg. 2 with inputs ηmax = η, α, δ (α, δ)-learns D with respect to Ha with an auditing complexity of O(ln(1/δ)/α2).\nA similar result holds if the error is known up to a multiplicative constant. But what if no bound on η is known? The following lower bound shows that in this case, the best active complexity for threshold this similar to the best active label complexity.\nTheorem 3.5 (Lower bound on auditing Ha without ηmax). Consider any constant α ≥ 0. For any δ ∈ (0, 1), if an auditing algorithm (α, δ)-learns any distribution D such that err(D,Ha) ≥ ηmin, then the algorithm’s auditing complexity is Ω(ln(1−δδ ) ln(1/ηmin)).\nNonetheless, in the next section show that there are classes with a significant gap between active and auditing complexities even without an upper bound on the error."
    }, {
      "heading" : "4 Axis Aligned Rectangles",
      "text" : "A natural extension of thresholds to higher dimension is the class of axis-aligned rectangles, in which the labels are determined by a d-dimensional hyperrectangle. This hypothesis class, first introduced in Blumer et al. (1989), has been studied extensively in different regimes (Kearns, 1998; Long and Tan, 1998), including active learning (Hanneke, 2007b). An axis-aligned-rectangle hypothesis is a disjunction of 2d thresholds. For simplicity of presentation, we consider here the slightly simpler class of disjunctions of d thresholds over the positive orthant Rd+. It is easy to reduce learning of an axis-aligned rectangle in Rd to learning of a disjunction of thresholds in R2d, by mapping each point x ∈ Rd to a point x̃ ∈ R2d such that for i ∈ [d], x̃[i] = max(x[i], 0) and x̃[i + d] = max(0,−x[i])). Thus learning the class of disjunctions is equivalent, up to a factor of two in the dimensionality, to learning rectangles.1 Because auditing costs are asymmetric, we consider two possibilities for label assignment. For a vector a = (a[1], . . . , a[d]) ∈ Rd+, define the hypotheses ha and h−a by\nha(x) = 2I[∃i ∈ [d], x[i] ≥ a[i]]− 1, and h−a (x) = −ha(x).\nDefine H2 = {ha | a ∈ Rd+} and H−2 = {h−a | a ∈ Rd+}. In H2 the positive points are outside the rectangle and in H−2 the negatives are outside. Both classes have VC dimension d. All of our results for these classes can be easily extended to the corresponding classes of general axis-aligned rectangles on Rd, with at most a factor of two penalty on the auditing complexity.\n1This reduction suffices if the origin is known to be in the rectangle. Our algorithms and results can all be extended to the case where rectangles are not required to include the origin. To keep the algorithm and analysis as simple as possible, we state the result for this special case."
    }, {
      "heading" : "4.1 The Realizable Case",
      "text" : "We first consider the pool setting for the realizable case, and show a sharp contrast between the auditing complexity and the active label complexity for H2 and H−2 . Assume a pool of size m. While the active learning complexity for H2 and H−2 can be as large as m, the auditing complexities for the two classes are quite different. For H−2 , the auditing complexity can be as large as m, but for H2 it is at most d. We start by showing the upper bound for auditing of H2.\nTheorem 4.1 (Pool auditing upper bound for H2). The auditing complexity of any unlabeled pool Su of size m with respect to H2 is at most d.\nProof. The method is a generalization of the approach to auditing for thresholds. Let h∗ ∈ H2 such that err(S, h∗) = 0. For each i ∈ [d], order the points x in S by the values of their i-th coordinates x[i]. Query the points sequentially from largest value to the smallest (breaking ties arbitrarily) and stop when the first negative label is returned, for some point xi. Set a[i]← xi[i], and note that h∗ labels all points in {x | x[i] > a[i]} positive. Return the hypothesis ĥ = ha. This procedure clearly queries at most d negative points and agrees with the labeling of h∗.\nIt is easy to see that for full Axis-Aligned Rectangles, an auditing complexity of 2d can be achieved in a similar fashion. We now show the lower bound for the auditing complexity of H−2 . This immediately implies the same lower bound for active label complexity of H−2 and H2.\nTheorem 4.2 (Pool auditing lower bound for H−2). For any m and any d ≥ 2, there is a pool Su ⊆ Rd+ of size m such that its auditing complexity with respect to H−2 is m.\nProof. The construction is a simple adaptation of a construction due to Dasgupta (2004), originally showing an active learning lower bound for the class of hyperplanes. Let the pool be composed of m distinct points on the intersection of the unit circle and the positive orthant: Su = {(cos θj , sin θj)} for distinct θj ∈ [0, π/2]. Any labeling which labels all the points in Su negative except any one point is realizable forH−2 , and so is the all-negative labeling. Thus, any algorithm that distinguishes between these different labelings with probability 1 must query all the negative labels.\nCorollary 4.3 (Realizable active label complexity of H2 and H−2). For H2 and H−2 , there is a pool of size m such that its active label complexity is m."
    }, {
      "heading" : "4.2 The Agnostic Case",
      "text" : "We now consider H2 in the agnostic case, where η > 0. The best known algorithm for active learning of rectangles (2, δ)-learns a very restricted class of distributions (continuous product distributions which are sufficiently balanced in all directions) with an active label complexity of Õ(d3p(ln(1/η)p(ln(1/δ))), where p(·) is a polynomial (Hanneke, 2007b). However, for a general distribution, active label complexity cannot be significantly better than passive label complexity. This is formalized in the following theorem.\nTheorem 4.4 (Agnostic active label complexity of H2). Let α, η > 0, δ ∈ (0, 12). Any learning algorithm that (α, δ)-learns all distributions such that err(D,H) = η for η > 0 with respect to H2 has an active label complexity of Ω(d/η).\nIn contrast, the auditing complexity of H2 can be much smaller, as we show for Alg. 3 below.\nAlgorithm 3: Auditing for H2 1: Input: ηmin > 0, α ∈ (0, 1], access to distribution D over Rd+ × {−1,+1}. 2: ν ← α/25. 3: for t = 0 to blog2(1/ηmin)c do 4: ηt ← 2−t. 5: Draw a sample St of size mν(ηt, δ/ log2(1/ηmin), 10d) with hidden labels. 6: for i = 1 to d do 7: j ← 0 8: while j ≤ d(1 + ν)ηt|St|e+ 1 do 9: If unqueried points exist, query the unqueried point with highest i’th coordinate;\n10: If query returned −1, j ← j + 1. 11: end while 12: bt[i]← the i’th coordinate of the last queried point, or 0 if all points were queried. 13: end for 14: Set Sbt to St, with unqueried labels set to −1. 15: Vt ← Vν(Sbt , ηt,H2[bt]). 16: η̂t ← maxh∈Vt errneg(Sbt , h). 17: if η̂t > ηt/4 then 18: Skip to step 21 19: end if 20: end for 21: Return ĥ ≡ argminh∈H2[bt] err(Sbt , h).\nTheorem 4.5 (Auditing complexity of H2). For ηmin, α, δ ∈ (0, 1), Alg. 3 (α, δ)-learns all distributions with η ≥ ηmin with respect to H2 with an auditing complexity of O(d 2 ln(1/αδ) α2 ln2(1/ηmin)).\nIf ηmin is polynomially close to the true η, we get an auditing complexity of O(d 2 ln2(1/η)), compared to the active label complexity of Ω(d/η), an exponential improvement in η. It is an open question whether the quadratic dependence on d is necessary here.\nAlg. 3 implements a ‘low-confidence’ version of the realizable algorithm. It sequentially queries points in each direction, until enough negative points have been observed to make sure the threshold in this direction has been overstepped. To bound the number of negative labels, the algorithm iteratively refines lower bounds on the locations of the best thresholds, and an upper bound on the negative error, defined as the probability that a point from D with negative label is classified as positive by a minimal-error classifier. The algorithm uses queries that mostly result in positive labels, and stops when the upper bound on the negative error cannot be refined. The idea of iteratively refining a set of possible hypotheses has been used in a long line of active learning works (Cohn et al., 1994; Balcan et al., 2006; Hanneke, 2007a; Dasgupta et al., 2008). Here we refine in a particular way that uses the structure of H2, and allows bounding the number of negative examples we observe.\nWe use the following notation in Alg. 3. The negative error of a hypothesis is errneg(D,h) = P(X,Y )∼D[h(X) = 1 and Y = −1]. It is easy to see that the same convergence guarantees that hold for err(·, ·) using a sample size mν( , δ, d) hold also for the negative error errneg(·, ·) (see Lemma A.4). For a labeled set of points S, an ≤ (0, 1) and a hypothesis class H, denote\nVν(S, ,H) = {h ∈ H | err(S, h) ≤ err(S,H) + (2ν + ν2) ·max(err(S,H), )}. For a vector b ∈ Rd+, define H2[b] = {ha ∈ H2 | a ≥ b}.\nTheorem 4.5 is proven in Section A.4.3. The proof idea is to show that at each round t, Vt includes any h∗ ∈ argminh∈H err(D,h), and η̂t is an upper bound on errneg(D,h∗). Further, at any given point minimizing the error on Sbt is equivalent to minimizing the error on the entire (unlabeled) sample. We conclude that the algorithm obtains a good approximation of the total error. Its auditing complexity is bounded since it queries a bounded number of negative points at each round."
    }, {
      "heading" : "5 Outcome-dependent Costs for a General Hypothesis Class",
      "text" : "In this section we return to the realizable pool setting and consider finite hypothesis classes H. We consider general outcome-dependent costs and a general space of labels Y, so that H ⊆ YX . Let S ⊆ X be an unlabeled pool, and let cost : S × H → R+ denote the cost of a query: For x ∈ S and h ∈ H, cost(x, h) is the cost of querying the label of x given that h is the true (unknown) hypothesis. In the auditing setting, Y = {−1,+1} and cost(x, h) = I[h(x) = −1]. For active learning, cost ≡ 1. Note that under this definition of cost function, the algorithm may not know the cost of the query until it reveals the true hypothesis.\nDefine OPTcost(S) to be the minimal cost of an algorithm that for any labeling of S which is consistent with some h ∈ H produces a hypothesis ĥ such that err(S, ĥ) = 0. In the active learning setting, where cost ≡ 1, it is NP-hard to obtain OPTcost(S) for general H and S. This can be shown by a reduction to set-cover (Hyafil and Rivest, 1976). A simple adaptation of the reduction for the auditing complexity, which we defer to the full version of this work, shows that it is also NP-hard to obtain OPTcost(S) in the auditing setting.\nFor active learning, and for query costs that do not depend on the true hypothesis (that is cost(x, h) ≡ cost(x)), Golovin and Krause (2011) showed an efficient greedy strategy that achieves a cost of O(OPTcost(S) · ln(|H|)) for any S. This approach has also been shown to provide considerable performance gains in practical settings (Gonen et al., 2013). The greedy strategy consists of iteratively selecting a point whose label splits the set of possible hypotheses as evenly as possible, with a normalization proportional on the cost of each query.\nWe now show that for outcome-dependent costs, if there are two labels and the cost depends only on the label, then another greedy strategy provides similar approximation guarantees for OPTcost(S). The algorithm is defined as follows: Suppose that so far the algorithm requested labels for x1, . . . , xt and received the corresponding labels y1, . . . , yt. Letting St = {(x1, y1), . . . , (xt, yt)}, denote the current version space by V (St) = {h ∈ H|S | ∀(x, y) ∈ St, h(x) = y}. The next query selected by the algorithm is\nx ∈ argmax x∈S min h∈H |V (St) \\ V (St ∪ {(x, h(x))})| cost(x, h) .\nThat is, the algorithm selects the query that in the worst-case over the possible hypotheses, would remove the most hypotheses from the version spaces, when normalizing by the outcome-dependent cost of the query. The algorithm terminates when |V (St)| = 1, and returns the single hypothesis in the version space.\nTheorem 5.1. For any hypothesis class H with |Y| = 2, any pool S, and any true hypothesis h ∈ H, if cost(x, h) ≡ cost(x, h(x)), then the cost of the proposed algorithm is at most (ln(|H|S | − 1) + 1) ·OPT.2\nIf cost is the auditing cost, the proposed algorithm is mapped to the following intuitive strategy: At every round, select a query such that, if its result is a negative label, then the number of hypotheses removed from the version space is the largest. This strategy is consistent with a simple principle based on a partial ordering of the points: For points x, x′ in the pool, define x′ x if {h ∈ H | h(x′) = −1} ⊇ {h ∈ H | h(x) = −1}, so that if x′ has a negative label, so does x. In the auditing setting, it is always preferable to query x before querying x′. Therefore, for any realizable auditing problem, there exists an optimal algorithm that adheres to this principle. It is thus encouraging that our greedy algorithm is also consistent with it.\nAn O(ln(|H|S |)) approximation factor for auditing is less appealing than the same factor for active learning. By information-theoretic arguments, active label complexity is at least log2(|H|S |) (and hence the approximation at most squares the cost), but this does not hold for auditing. Nonetheless, hardness of approximation results for set cover (Feige, 1998), in conjunction with the reduction to set cover of Hyafil and Rivest (1976) mentioned above, imply that such an approximation factor cannot be avoided for a general auditing algorithm."
    }, {
      "heading" : "6 Conclusion and Future Directions",
      "text" : "In this paper we propose a model of active learning with query costs that depend on the outcome of the query. We show that the auditing complexity can be significantly different from the active learning complexity, and that tailoring algorithms for auditing can be beneficial. Our algorithms take advantage of the fact that positive labels are free, to improve error at less cost than in active learning. We also described a general approach to designing auditing procedures for finite hypothesis classes, based on a greedy strategy and on a partial ordering of points, which takes advantage of the asymmetric label costs.\nThere are many interesting directions suggested by this work. First, it is known that for some hypothesis classes, active learning cannot improve over passive learning for certain distributions (Dasgupta, 2004), and the same is true for auditing. However, exponential speedups are possible for active learning on certain classes of distributions (Balcan et al., 2006; Dasgupta et al., 2008), in particular ones with a small disagreement coefficient (Hanneke, 2007a). This quantity is related to the Alexander capacity function (Koltchinskii, 2010), which appears in lower bounds for active learning (Raginsky and Rakhlin, 2011). It would be interesting if a similar property of the distribution can guarantee an improvement with auditing over active or passive learning.\nInvestigating such a general property might shed light on auditing for other important hypothesis classes such as decision trees or halfspaces. It is well known that for some important settings, such as learning with hyperplanes, there are distributions which resist any improvement using active learning (Dasgupta, 2004). Recent work by Gonen et al. (2013) has shown that both theoretically and empirically, more aggressive learning strategies can be effective in the realizable case. These strategies are based on heuristics (Tong and Koller, 2001) that query the most “informative” points. It would be interesting to see how such approaches should change in the presence of asymmetric label costs.\n2A stronger version was erroneously given in the short version of this paper. However, our proof holds only for this weaker version.\nThe name “auditing” suggests an imbalance in the number of points per class (this is the case in fraud). Prior work on learning from unbalanced data was surveyed by He and Garcia (2009). Some of these approaches (Ertekin et al., 2007) use the same active learning heuristics (Tong and Koller, 2001), and it would be interesting to see how these apply to auditing.\nIn this work we considered algorithms which aim to minimize the number of negative labels needed to classify all of the points accurately, but this is not the only way to measure the performance in an auditing setting. For example, we may wish to maximize the number of positive points the algorithm finds subject to a bound on the number of negative labels encountered. In addition, auditing is an extreme version of asymmetric label costs – positive labels are free – but it would be interesting to study more general label costs. An interesting generalization along these lines is a multiclass setting with a different cost for each label. These measures and costs are different from those studied in active learning, and may lead to new algorithmic insights."
    }, {
      "heading" : "A Proofs omitted from the text",
      "text" : ""
    }, {
      "heading" : "A.1 Additional notation",
      "text" : "We use C,C1, C2, . . . , c, c ′ etc. to denote positive numerical constants. Their values may change between expressions. We use the shorthand ∀δ to say that a statement holds with probability at least 1 − cδ, for some constant c. Denote a multiplicative/additive upper bound for a by aJn, λK = max{na, a+ (n− 1)λ}. We will use the following easy fact.\nFact 1. For any non-negative numbers a, b, c, , n,m, if a ≤ bJn, K then aJm,λK ≤ bJmn, λK.\nA.2 Standard results from probability\nThese are included for the ease of the reader.\nTheorem A.1 (Hoeffding’s Inequality (Hoeffding, 1963)). Let the random variables X1, . . . , Xn be independent with Xi ∈ [0, 1], and let X = 1n ∑ i∈[n]Xi. Then for any t > 0,\nP[X > E[X] + t] ≤ exp ( −2nt2 ) .\nTheorem A.2 (Bernstein’s Inequality (Bernstein, 1946)). Let the random variables X1, . . . , Xn be independent with Xi−E[Xi] ≤ 1. Let X = 1n ∑n i=1Xi and let σ 2 = 1n ∑n\ni=1 Var(Xi) be the variance of X. Then for any t > 0,\nP[X > E[X] + t] ≤ exp ( − nt 2\n2(σ2 + t/3)\n) .\nIn particular, by setting the right hand side to δ and solving for t, we get that with probability 1− δ,\nX ≤ E[X] + 2 3\nln(1/δ)/n+ √ 2σ2 ln(1/δ)/n."
    }, {
      "heading" : "A.3 Proofs for Section 3",
      "text" : "Proof of Lemma 3.2. We start with the first inequality. If ηmax ≥ 1/6 then the inequality trivially holds. Thus assume ηmax < 1/6. Let W = 14 ln(8/δ) and let N = WT be the size of the subset Sq. Let h\n∗ ∈ argminh∈Ha err(S,Ha) be a minimizer of the error on S. By assumption err(S, h∗) ≤ ηmax. For each t, let {Xt(l) | l ∈ [W ]} be the W points drawn from S(t) by the procedure and Yt(l) be their corresponding labels given by S. Let Zt(l) = I[Yt(l) 6= h∗(Xt(l))] and note that {Zt(l)} for l ∈ [W ] are i.i.d. random variables, and Zt(l) − E[Zt(l)] ≤ 1. Furthermore, we have Var[Zt(l)] ≤ E[Z2t (l)] ≤ E[Zt(l)]. Therefore\n1\nN ∑ t∈[T ],l∈[W ] Var[Zt(l)] ≤ 1 N ∑ t∈[T ],l∈[W ] E[Zt(l)] ≤ err(S, h∗) ≤ ηmax.\nTherefore by Bernstein’s inequality (Bernstein, 1946, see Theorem A.2), with probability 1− δ,\nerr(Sq, h ∗) =\n1\nN ∑ t∈[T ],l∈[W ] Zt(l) ≤ ηmax + 2 3 ln(1/δ)/N + √ 2ηmax ln(1/δ)/N.\nBecause T = max{b1/3ηc, 1}, for ηmax < 1/6 we have T ≥ 1/6η. Therefore N ≥ 14 ln(8/δ)/6η. Substituting N and δ in the inequality above we get that with probability 1 − δ/8, err(Sq, ĥ) ≤ err(Sq, h\n∗) ≤ 6ηmax. For the second claim, if ηmax > 1/17 the claim trivially holds. Thus assume ηmax ≤ 1/17. We now show that the error of a threshold ĥ ∈ argminh∈Ha err(Sq,Ha) on the original set S is at most 17ηmax. Let A(h) = {x ∈ U | h(x) = 1} be the points in the set U labeled 1 by a hypothesis h, and let gi = hx′\n(i−1)m+1 be the hypothesis corresponding to the threshold at the leftmost point of S(i). Note that A(gi) = ⋃ j>i S(j).\nWe claim that for any hypothesis h such that err(S, h) > err(S, h∗) + 4/T , the error on the sampled set Sq must satisfy err(Sq, h) > err(Sq, h ∗) with high probability, and therefore h cannot be a minimizer ĥ. We consider two cases, depending on whether the threshold for h is larger or smaller than h∗. First suppose that the threshold is larger so that A(h) ⊆ A(h∗). Let i be the smallest index such that A(gi) ⊆ A(h∗) and j be the largest index such that A(h) ⊆ A(gj). The situation is illustrated in Figure 1. Since err(S, h) > err(S, h∗) + 4/T , there must be three full intervals S(t) between gi and gj , so |j − i| ≥ 3. Define ∆ = |j − i|.\nThen using the fact that a single S(t) can contribute at most 1/T to the error on Sq, we can\nbound the gap:\nerr(Sq, h)− err(Sq, h∗) = err(Sq, h)− err(Sq, gj) + err(Sq, gj)− err(Sq, gi) + err(Sq, gi)− err(Sq, h∗) ≥ err(Sq, gj)− err(Sq, gi)− 2/T.\nTherefore for any h whose threshold is between those for gj and gj+1, in order to show that err(Sq, h) > err(Sq, h\n∗) with high probability it is sufficient to show that err(Sq, gj)− err(Sq, gi) ≥ 2/T with high probability.\nNote that the number of points in Sq on which gi and gj disagree is W∆, corresponding to the subsamples in the ∆ segments S(i+ 1), S(i+ 2), . . . , S(j) in Algorithm 1. For each pair (x, y) in those segments, either gi or gj errs, and err((x, y), gj) − err((x, y), gi) = 1 − 2I[gi(x) 6= y]. Let Zit(l) = I[Yt(l) 6= hi(Xt(l))]. Then\nerr(Sq, gj)− err(Sq, gi) = 1\nWT j∑ t=i+1 ∑ l∈[W ] (1− 2Zit(l)) = ∆ T − 2 WT j∑ t=i+1 ∑ l∈[W ] Zit(l).\nThe event that this difference is smaller than 2/T is equivalent to\n1\nW∆ j∑ t=i+1 ∑ l∈[W ] Zit(l) ≥ ∆− 2 2∆ ≥ 1 6 .\nNote that hi agrees with h ∗ on ⋃j t=i+1 S(t), so\nE  1 W∆ j∑ t=i+1 ∑ l∈[W ] Zit(l)  ≤ err(S, h∗) ≤ ηmax. By Hoeffding’s inequality (Hoeffding, 1963, see Theorem A.1), and since ηmax ≤ 1/17,\nP[err(Sq, gj)− err(Sq, gi) ≤ 2/T ] ≤ P  1 W∆ j∑ t=i+1 ∑ l∈[W ] Z ′t(l) ≥ 1 6  ≤ exp ( −2W∆ ( 1\n6 − ηmax )2) ≤ exp (−W∆/42) .\nNow taking a union bound over all j such that j > i+ 3, we have\nP[∀j ≥ i+ 3, err(Sq, gj)− err(Sq, gi) ≤ 2/T ] ≤ T∑\n∆=3\nexp (−W∆/42)\n≤ exp(−W/14)− exp(−W (T + 1)/42) 1− exp(−W/42) ≤ exp(−W/14) 1− exp(−W/42) .\nThe other case when h < h∗ is symmetric, so we see that if err(S, h) > err(S, h∗) + 4/T then\nP[err(Sq, h) > err(Sq, h∗)] ≤ 2 exp(−W/14)\n1− exp(−W/42) .\nSince W = 14 ln(8/δ), we get that the total probability is bounded by δ/2. Since T > 13ηmax − 1, we have for ηmax ≤ 1/17 that T > 13ηmax − 1 17ηmax\n≥ 1/4ηmax. Therefore for ĥ which minimizes the error on Sq we have err(S, ĥ) < err(S, h\n∗) + 4/T < 17ηmax. To prove Theorem 3.3, we require the following lemma.\nLemma A.3. For S0 and hâ in Alg. 2, if err(S0,Ha) ≤ (1 + ν)ηmax, then the auditing complexity of step 6 of Alg. 2 is at most 85 ln(16/δ) and with probability 1− δ, err(S0, hâ) ≤ 35(1 + ν)ηmax.\nProof. Denote γ = (1 + ν)γ. Let h∗ ∈ argminh∈H err(S0,H). Since |S| = mag(γ, δ/2, 1), with probability 1− δ/2, err(S,Ha) ≤ err(S0,H) +γ ≤ 2γ. By Lemma 3.2, the total number of negative errors in Sq is at most d12γ · |Sq|e+ 1. Therefore, by Lemma 3.1, step 6 finds a hypothesis hâ, that minimizes the error on Sq. By Lemma 3.2, with probability 1 − δ/2, err(S, hâ) ≤ 34γ. Therefore, due to the size of |S| again, with probability 1− δ, err(S0, hâ) ≤ 35γ.\nThe auditing complexity of step 6 is at most 6γ · |Sq|+ 1. Now, from Alg. 1, |Sq| ≤ 14 ln(16/δ) · max{b1/3γc, 1}. Since γ · max{b1/3γc, 1} ≤ 1, the auditing complexity of Alg. 2 is at most d6γ · |Sq|e+ 1 ≤ 85 ln(16/δ).\nWe are now ready to prove the theorem. Proof of Theorem 3.3. We first bound err(D, ĥ). Let h∗ ∈ argminh∈H err(D,h), and h∗0 ∈ argminh∈H err(S0, h). Since |S0| = mν(ηmax, δ/2, 1), with probability 1− δ/2,\nerr(S0, h ∗ 0) ≤ err(S0, h∗) ≤ err(D, ηmax)J(1 + ν), ηmaxK ≤ (1 + ν)ηmax. (2)\nTherefore, by Lemma A.3, hâ satisfies ∀δ, err(S0, ĥ) ≤ 35(1 + ν)ηmax. It follows that\nP(X,Y )∼S0 [h ∗ 0(X) 6= ĥ(X)] ≤ err(S0, ĥ) + err(S0, h∗0) ≤ 36(1 + ν)ηmax.\nIn other words, h∗0 classifies at most 36(1 + ν)ηmax|S0| points differently from hâ. Therefore h∗0 ∈ argminh∈H err(S1, h), where S1 is defined in step 8, since all points in S0 \\S1 are classified the same by all possible candidates for h∗0.\nWe have\nerr(S1, h ∗ 0) ≤ |S0| |S1| err(S0, h ∗ 0) ≤ |S0| 2 · 36(1 + ν)ηmax|S0| (1 + ν)ηmax ≤ 1 72 . (3)\nSince |S2| = mag(ν/72, δ/2, 1), with probability 1− δ/2,\nerr(S1, ĥ) ≤ err(S2, ĥ) + ν/72 ≤ err(S2, h∗0) + ν/72,\nand err(S2, h ∗ 0) ≤ err(S1, h∗0)J(1 + ν), 172K. Therefore\n∀δ, err(S1, ĥ) ≤ err(S1, h∗0)J(1 + ν), 1 72 K + ν/72 ≤ err(S1, h∗0) + ν/36,\nwhere the last inequality follows from Eq. (3). Note also that err(S0 \\ S1, ĥ) = err(S0 \\ S1, h∗0).\nerr(S0, ĥ) = |S0| − |S1| |S0| err(S0 \\ S1, h∗0) + |S1| |S0| err(S1, ĥ)\n≤ |S0| − |S1| |S0| err(S0 \\ S1, h∗0) + |S1| |S0| (err(S1, h ∗ 0) + ν/36) = err(S0, h ∗ 0) + 72(1 + ν)ηmax(ν/36) ≤ err(S0, h∗0) + 4νηmax.\nIn the last inequality we used the fact that ν ≤ 1. Therefore err(S0, ĥ) ≤ err(S0, h∗0) + 4νηmax. Combining this with Eq. (2) we conclude that with probability 1 − δ, err(S0, ĥ) ≤ ηmax(1 + 5ν). Since ν = α/5, this proves the lemma.\nThe auditing complexity of Alg. 2 is at most the auditing complexity of step 6, which is O(ln(1/δ)) by Lemma A.3, plus mag(ν/72, δ/2, 1) = O(ln(1/δ)/ν2)O(ln(1/δ)/α2). Thus the total auditing complexity is also O(ln(1/δ)/α2). Proof of Theorem 3.5. Fix ηmin and define β = α+ 1. Assume without loss of generality that the algorithm returns a hypothesis ĥ after having queried exactly M negative labels. We will define a finite set of distributions such that if the algorithm emits a correct answer for all of them, then the given lower bound on M must hold.\nWe consider distributions with a uniform marginal over [0, 1], and define several conditional labeling distributions for points on [0, 1]. First, we define the distribution D−, which assigns −1 to all x ∈ [0, 1−2ηmin]∪[1−ηmin, 1], and +1 to x ∈ (1−2ηmin, 1−ηmin). Note that err(D−,Ha) = ηmin, so the guarantee of the algorithm is that err(ĥ, D−) ≤ βηmin with probability 1− δ. Thus for D− the algorithm produces a hypothesis ĥ = ha for some threshold value a ≥ 1 − (1 + β)ηmin with probability 1− δ.\nSecond, we define a family of distributions D1, . . . , DN , for N = bln(1/2ηβ)/ ln(4β)c, such that for each Di, the algorithm cannot return ha with a ≥ 1− (β + 1)η with probability more than δ.\nLet λ = 1/8β. Define a0 = 1 − (1 + β)ηmin, and for i ∈ [N ] define li = β(4β)iηmin and ai = a0 − ∑ j≤i lj . Define the distribution Di as follows (See Figure 2):\nPDi [Y = +1 | X = x] =  0 x ∈ [0, ai] ∪ [ai−1, 1− 2ηmin] ∪ [1− ηmin, 1] 1 x ∈ (1− 2ηmin, 1− ηmin) 1− λ x ∈ (ai, ai−1).\nThe distribution Di agrees with the distribution D− except on the interval (ai, ai−1), where it is positive with probability 1 − λ and negative with probability λ. We claim that if the algorithm returns a threshold greater than a0 on Di with probability more than δ, it violates the (α, δ)-learning\nguarantee. For a0, and β ≥ 1,\nerr(Di, ha0) ≥ (1− λ)li > 7\n8 β(4β)iηmin.\nFor ai,\nerr(Di, hai) = βηmin + ∑ j<i lj + λli\n=  i−1∑ j=0 β(4β)j + 1 8β β(4β)i  ηmin Hence\nerr(Di, hai) = β ( (4β)i − 1 4β − 1 + 1 8 (4β)i ) ηmin\n< 7\n8 β(4β)i\n( 8\n7(4β − 1) +\n1\n7\n) ηmin\n< 7\n8 (4β)i.\nFrom this we can see that err(Di, ha0) > βerr(Di, hai), violating the guarantee of the algorithm. It follows that for any i, if the true labeling is Di, then the probability that the algorithm returns ha for a ≥ 1− (β + 1)η is at most δ. We now show that this implies a lower bound on M .\nFirst, since all distributions label [a0, 1] in the same way, we may assume without loss of generality that the algorithm never queries points in this segment. It follows that if the true distribution is D−, the algorithm observes only negative labels.\nDenote by Yt the random variable whose value is the label the algorithm receives for its t’th query, or 0 if the algorithm stopped before querying t points. Denote by Zt the random variable whose value is j if on round t, the algorithm queries a point in [aj , aj−1], and −1 if the algorithm stops before round t. Denote by At the event that ∀i ∈ [t], Yi = −1. Also denote ptj = P[Zt = j | At−1]. We will show a lower bound on P[AM ], that is the probability that all first M queries return a negative label. Since in this case the algorithm cannot distinguish Dj from D−, this probability must be small, which implies a lower bound on M .\nBy definition, ∑\nj∈[N ] p t j = 1 for all t ≤ M . Thus, there exists some j ∈ [N ] such that∑\nt∈[M ] p t j ≤M/N . Fix j to one such value. Assume that the true labeling is Dj . Then\nP[A1] = λp1j + (1− p1j ) = 1− (1− λ)p1j , P[At] = P[At−1]P[Yt = −1 | At−1] = P[At−1](1− (1− λ)ptj).\nIt follows that if the true distribution is Dj , then P[AM ] = ∏ t∈[M ] (1− (1− λ)ptj). We consider two kinds of indices t ∈ [M ]. First let Ij = {t ∈ [M ] | ptj > 1/2(1− λ)}. Since∑ t∈[M ] p t j ≤ M/N , we have |Ij | ≤ 2(1− λ)M/N . For t ∈ I we use the bound 1 − (1 − λ)ptj ≥ λ.\nFor t /∈ I, we use the bound 1 − (1 − λ)ptj ≥ exp(−2(1 − λ)ptj). This follows from the inequality exp(−2x) ≤ 1− x, which holds for x ∈ [0, 12 ]. Combining the two cases, we get\nP[AM ] = ∏ i∈[M ] (1− (1− λ)ptj) ≥ exp\n( −2(1− λ)\n∑ t/∈I ptj\n) λ|I|\n≥ exp ( −2(1− λ)M\nN\n) λ2(1−λ)(M/N)\n= exp ( −2(1− λ)M\nN (1 + ln(1/λ))\n) . (4)\nThe algorithm must stop after seeing M negative labels, thus it must return an answer at time M if AM occurs. If the true distribution is D−, then AM occurs with probability 1. Therefore, if AM occurs the algorithm must return ha for a ≥ a0 with probability at least 1 − δ. Thus, if the true distribution is Dj , the probability that the algorithm errs is at least P[AM ](1− δ). Since the algorithm errs with probability at most δ, we have\nδ ≥ P[AM ](1− δ),\nSolving for M using Eq. (4), we get\nM ≥ N ln(1−δδ )\n2(1− λ)(1 + ln(1/λ)) .\nTreating β, and hence λ, as constants, we get that N ≥ C ln(1/ηmin) − C ′, therefore M ≥ C ln(1−δδ ) ln(1/ηmin)− C ′ for some positive constants C,C ′."
    }, {
      "heading" : "A.4 Proofs for Section 4",
      "text" : "Here we gather proof details for the hypothesis class H2 and H−2 of axis-aligned rectangles."
    }, {
      "heading" : "A.4.1 Proof of Theorem 4.4",
      "text" : "Proof of Theorem 4.4. We will show that in the realizable case, an algorithm that returns ĥ such that ∀δerr(D, ĥ) ≤ requires Ω(d/ ) labels. The statement of the theorem follows by adding an unavoidable error of η to all distributions.\nWithout loss of generality, suppose d is even and 1/4 is an integer, and partition the d dimensions in d/2 pairs of coordinates {(1, 2), (3, 4), . . . , (d− 1, d)}. For each coordinate pair (2i− 1, 2i) choose 14 distinct points Si on the unit circle in the subspace spanned by the i and (i + 1)-th coordinates, as in the proof of Theorem 4.2. Consider distributions D with a uniform marginal over the points in S1, . . . , Sd/2, so that the probability of each point is 4 /d. Any distribution such that all points are labeled negative, except perhaps a single point in every Si, is realizable. To get err(D, ĥ) ≤ with probability more than half, the algorithm must find whether there is a positive point in at least half of the Si’s.\nLet Ti be the number of points queried by the algorithm in set Si. If the total number of queries that the algorithm makes is less than d|Si|/16, then E[Ti] < |Si|/8 for at least half of the i’s. If E[Ti] < |Si|/8 then with probability at least 1/2, Ti ≤ 1/4. Thus there exists a point in Si such that with probability at least 1/2 the algorithm does not query this point, and therefore cannot tell whether it is positive. It follows that the algorithm must make at least d|Si|/16 = Ω(d/ ) queries on negative points.\nA.4.2 Approximation bounds for error on samples\nLemma A.4. Let H be a hypothesis class with VC dimension d ≥ 1, and let S be a sample of size mν( , δ, d) drawn i.i.d. from a distribution D. With probability 1− δ, ∀h ∈ H,\nerrneg(S, h) ≤ errneg(D,h)J(1 + ν), K and errneg(D,h) ≤ errneg(S, h)J(1 + ν), K.\nProof. Let f [h] : (Rd+ × {−1,+1})→ {−1,+1} such that f [h](x, y) = I[h(x) = 1 and y = −1]. For any distribution over Rd+×{−1,+1}, consider a distribution D′ over (Rd+×{−1,+1})→ {−1,+1} that draws ((X,Y ), Z) ∼ D′ such that (X,Y ) is drawn from D and Z is deterministically 1. Then errneg(D,h) = err(D\n′, f [h]). The VC-dimension of F = {f [h] | h ∈ H} is at most that of H: Any set ((x1, y1), . . . , (xn, yn)) shattered by F must have yi = −1 for all i ∈ [n]. Therefore ∀h ∈ H, f [h](xi, yi) = h(xi), hence x1, . . . , xd is shattered by H. The result follows by applying Eq. (1) to err(D′, f [h])."
    }, {
      "heading" : "A.4.3 Proof of Theorem 4.5",
      "text" : "Theorem 4.5 is proven using several lemmas. We will need the following auxiliary result.\nLemma A.5. Let H be a hypothesis class of VC-dimension d, and let f [h1, h2] : (Rd+×{−1,+1})→ {−1,+1} be the function f [h1, h2](x) = I[h1(x) = y or h2(x) = 1]. The VC-dimension of F = {f [h] | h ∈ H} is at most 10d.\nProof. Let S = ((x1, y1), . . . , (xn, yn)) be a set shattered by F . Then |F|S | = 2n. In addition, |F|S | ≤ |H|S × H|S | ≤ |H|S |2. By Sauer’s lemma, |H|S | ≤ (en/d)d. Therefore 2n ≤ (en/d)2d. It follows that n ≤ 10d.\nThe next lemma will help prove that the set of hypotheses maintained by the algorithm includes the best hypothesis for the distribution.\nLemma A.6. Let ν, > 0 and δ ∈ (0, 1). Let S be a random labeled sample of size mν( , δ, 10d) drawn from D . For b ∈ Rd+, let Sb be identical to sample S except that if (x, y) ∈ S and x ≤ b, then (x,−1) ∈ Sb instead of (x, y). Let h∗ ∈ argminh∈H2 err(D,h), and let a ∗ such that h∗ = ha∗. Let ĥb = argminh∈H2[b] err(Sb, h). Then ∀ δ, for all b ≤ a∗,\nh∗ ∈ Vν(Sb, ,H2[b]), and err(D, ĥb) ≤ err(D,h∗)J(1 + ν)2, K.\nProof of Lemma A.6. For the first claim, it suffices to show that ∀δ, for all b ≤ a∗,\nerr(Sb, h ∗) ≤ err(Sb, ĥb)J(1 + ν)2, K. (5)\nDefine fa,b : Rd+ × {−1,+1} → {−1,+1} such that fa,b(x, y) = I[ha(x) = y or hb(x) = −1]. Let S′ = {((x, y), 1) | (x, y) ∈ S}, and let D′ be a distribution over (Rd+ × {−1,+1}) × {−1,+1} generated by drawing ((X,Y ), Z) ∼ D′ where (X,Y ) ∼ D and Z = 1. Then S′ is drawn i.i.d. from D′. Note that for any a ≥ b, ha classifies all points x ≤ b as negative. It follows that there is some λ > 0 such that for all a ≥ b, λ = err(D,ha)− err(D′, fa,b).\nThe VC-dimension of F = {fa,b | a ≥ b} is at most 10d (see Lemma A.5 in the appendix). Since |S′| ≥ mν( , δ, 10d), ∀δ,∀f ∈ F , err(S′, f) ≤ err(D′, f)J1 + ν, K and err(D′, f) ≤ err(S′, f)J1 + ν, K.\nLet âb ∈ Rd+ such that ĥb = hâb . We have err(D,ha∗) ≤ err(D,hâb), therefore err(D′, fa∗,b) ≤ err(D′, hâb,b). Combining these inequalities and using Fact 1, we get\n∀δ, ∀b ∈ Rd+, err(S′, fa∗,b) ≤ err(D′, fa∗,b)J1 + ν, K ≤ err(D′, fâb,b)J1 + ν, K ≤ err(S′, fâb,b)J(1 + ν)2, K.\nNoting that for a ≥ b, err(S′, fa,b) = err(Sb, ha), this proves Eq. (5). For the second claim,\n∀δ,∀b ∈ Rd+, err(D′, fâb,b) ≤ err(S ′, fâb,b)J1 + ν, K\n≤ err(S′, fa∗,b)J1 + ν, K ≤ err(D′, fa∗,b)J(1 + ν)2, K.\nDenoting λ = err(D,ha∗)− err(D′, fa∗,b) = err(D,hâb)− err(D′, fâb,b), we get\nerr(D,hâb)− λ ≤ (err(D,ha∗)− λ)J(1 + ν)2, K.\nSince λ > 0, this implies err(D,hâb) ≤ err(D,ha∗)J(1 + ν)2, K. The following lemma shows that ηt is indeed an upper bound for the negative error of the best hypothesis.\nLemma A.7. If the algorithm reaches round T , then ∀δ, ∀t ≤ T, ∀h∗ ∈ argminh∈H2 err(D,h), the following claims hold:\n• Claim A(t): errneg(D,h∗) ≤ ηt.\n• Claim B(t): h∗ ∈ Vt, where Vt is defined in step 15 of Algorithm 3\n• Claim C(t): errneg(D,h∗) ≤ η̂tJ1 + ν, ηtK.\nProof of Lemma A.7. We will prove the claims by induction on t. At each round t ≤ T ≤ 1/ log2(1/ηmin), |St| ≥ m(ηt, δ/ log2(1/ηmin), d), thus ∀δ, uniform convergence as stated in Eq. (1) holds for all rounds simultaneously. We assume this for the rest of the proof .\nFirst, claim A(0) trivially holds since η0 = 1. It is also easy to see that if claim C(t) holds, and the algorithm reaches round t+1, then claim A(t+1) holds: If Alg. 3 reached round t+1, then the condition in step 17 failed at time t, meaning η̂t ≤ ηt/4. By C(t), errneg(D,h∗) ≤ η̂tJ1 + ν, ηtK ≤ ηt/2 = ηt+1 (since ν ≤ 1), which proves A(t+ 1).\nWe have left to show that claim A(t) implies claims C(t) and B(t). Assume that A(t) holds. First, suppose not all the points in St are queried. To prove B(t), note that errneg(St, h\n∗) ≤ errneg(D,h\n∗)J1 + ν, ηtK. Since errneg(D,h∗) ≤ ηt, this implies an upper bound errneg(St, h∗) ≤ (1 + ν)ηt.\nWe now show that h∗ ∈ H2[bt]. If all the points in St are queried, then bt is the zero vector, thus H2[bt] = H2 and h∗ ∈ H2[bt]. If not all the points in St are queried, then the algorithm queried more than (1 + ν)ηt|St| negative points in each direction, thus at least one of those points is labeled negative by h∗. The smallest value of coordinate i queried in iteration i of round t is bt[i]. Therefore the threshold of h ∗ in direction i is at most bt[i]. This implies h ∗ ∈ H2[bt] and\nfurthermore that h∗ = ha∗ for some a ∗ ≥ bt. By Lemma A.6, h∗ ∈ Vν(Sbt , 4, ,H2[bt]) = Vt. This proves B(t). For C(t), note that η̂t = maxh∈Vt errneg(Sbt , h), hence byB(t), η̂t ≥ errneg(Sbt , h∗) = errneg(St, h∗). The claim follows since errneg(D,h ∗) ≤ errneg(St, h∗)J1 + ν, ηtK.\nThe last lemma provides a the stopping condition of the algorithm.\nLemma A.8. If err(D,H) > ηmin then ∀δ the algorithm stops at round at least log2(1/8err(D,H)).\nProof. Let T = log2(1/8err(D,H)). We show that ∀δ the algorithm does not stop at any t ≤ T , by showing that the condition in step 17 does not hold, that is η̂t = maxh∈Vt errneg(Sbt , h) ≤ ηt/4. By Lemma A.7, claim B(t), h∗ ∈ Vt. Therefore, by definition of Vt, for all h ∈ Vt,\nerrneg(Sbt , h) ≤ err(Sbt , h) ≤ err(Sbt , h∗)J(1 + ν)2, ηtK ≤ err(St, h∗)J(1 + ν)2, ηtK.\nDue to the size of St we also have ∀δ,∀t ≤ T, err(St, h∗) ≤ err(D,h∗)J1 + ν, ηtK. Combining these inequalities we get η̂t ≤ err(D,h∗)J(1 + ν)3, ηtK. For t ≤ T , err(D,h∗) ≤ 2−t/8 = ηt/8. η̂t ≤ err(D,h∗)+((1+ν3)−1)ηt ≤ ηt(1/8+((1+ν3)−1)). Since ν ≤ 1/25, one can check that η̂t ≤ ηt/4.\nWe are finally ready to prove Theorem 4.5. Proof of Theorem 4.5. Let T be the round in which the algorithm returns ĥ. The number of negative labels N observed by the algorithm satisfies\nN ≤ T∑ t=0 d(1 + ν)ηt(dmν(ηt, δ/ log2(1/ηmin), 10d)e+ 1)\nBy the definition on page 2, mν(η, δ, d) = C(d ln(c/νη) + ln(c/δ))/ν 2η. Also 1 + ν ≤ 2. Therefore\nN ≤ C(T + d ν2 T∑ t=0 (d ln(c/νηt) + ln(c log2(1/ηmin)/δ))\n≤ Cd(d T∑ t=0 ln(c/νηt) + T ln(c ln(1/ηmin)/δ)).\nWe have ∑T t=0 ln(c/ηt) ≤ C ∑T\nt=0 t+ C ≤ CT 2 + C. In addition, T ≤ log2(1/ηmin). Therefore the algorithm observes at most Cd2 ln2(1/ηmin) ln(c/νδ)/ν\n2 negative examples. Since ν = α/25, we get the same auditing complexity for α.\nFor the second part of the theorem, by Lemma A.7, ∀δ argminh∈H2 err(D,h) ∈ VT = V (SbT , ηT ,H2[bT ]). Therefore, by Lemma A.6, err(D, ĥ) ≤ err(D,H2)J(1 + ν)2, ηT K. By Lemma A.8, we have that T ≥ min{log2(1/8err(D,H), log2(1/2ηmin)}, and therefore ηT ≤ max{8err(D,H2), 2ηmin}. It follows that\n∀δ err(D, ĥ) ≤ max {\n(1 + ν)2err(D,H2), err(D,H2) + ((1 + ν)2 − 1) ·max{8err(D,H2), 2ηmin} } ≤ max{(1 + 8((1 + ν)2 − 1))err(D,H2), err(D,H2) + 2((1 + ν)2 − 1)ηmin\n} ≤ err(D,H2)J(1 + 8((1 + ν)2 − 1)), ηminK.\nNow, since ν ≤ 1 and ν = α/25, we have 8((1 + ν)2 − 1) = 8(2ν + ν2) ≤ 24ν ≤ α. The statement of the theorem immediately follows."
    }, {
      "heading" : "A.5 Proofs for Section 5",
      "text" : "Proof of Theorem 5.1. Assume without loss of generality that H|S = H. For an algorithm A, let QkA,h = (q 1 A,h, . . . , q k A,h) be the sequence of first k queries the algorithm makes if h is the true hypothesis. QA,h stands for the entire sequence until the algorithm stops with V (QA,h, h) = {h}. Denote by ◦ the concatenation of two sequences. Let cost(Q, h) be the total cost of a set or sequence of queries Q if the true hypothesis is h. For a set of points X ⊆ X and a hypothesis h ∈ H, let V (X,h) be the set of hypotheses that are consistent with the labeling of h on X, that is V (X,h) = {g ∈ H | ∀x ∈ S, g(x) = h(x)}.\nBy definition, there exists an algorithm A such that for any h ∈ H, cost(QA,h) ≤ OPTcost. Denote OPTcost by OPT for brevity. Now, consider a greedy algorithm B. If h is the true hypothesis then after k queries, the version space is V (QkB,h, h). Consider running A after executing QkB,h. Let the hypothesis h̄ ∈ V (QkB,h, h) be such that for every length of sub-sequence n ≤ |QA,h̄|, and for every y ∈ Y,\n|V (QkB,h ◦Q n−1 A,h̄ , h̄) \\ V (Q k B,h ◦QnA,h̄, h̄)|\ncost(qnA,h̄, h̄(q n A,h̄))\n≤ |V (QkB,h ◦Q n−1 A,h̄ , g) \\ V (Q k B,h ◦QnA,h̄, g)|\ncost(qnA,h̄, y) , (6)\nwhere g is equal to h̄ on QkB,h ◦Q n−1 A,h̄ but labels q n A,h̄ differently. Such a hypothesis clearly exists if there are two possible labels: it can be found by selecting, at each iteration, the hypothesis with the label for qnA,h̄ that would incur the smaller ratio.\nBy the definition of OPT,\n|QA,h̄|∑ n=1 cost(qnA,h̄, h̄) = cost(QA,h̄) ≤ OPT.\nAlso |V (QkB,h ◦QA,h̄, h̄)| = 1. Therefore\n|QA,h̄|∑ n=1 |V (QkB,h ◦Qn−1A,h̄ , h̄) \\ V (Q k B,h ◦QnA,h̄, h̄)| = |V (Q k B,h, h̄) \\ V (QkB,h ◦QA,h̄, h̄)|\n= |V (QkB,h, h̄)| − 1 = |V (QkB,h, h)| − 1.\nIt follows that there exists at least one n such that\n|V (QkB,h ◦Q n−1 A,h̄ , h̄) \\ V (Q k B,h ◦QnA,h̄, h̄)|\ncost(qnA,h̄, h̄) ≥ |V (QkB,h, h)| − 1 OPT .\nMoreover, for this n, due to Eq. (6),\nmin y∈Y\n|V (QkB,h ◦Q n−1 A,h̄ , g) \\ V (Q k B,h ◦QnA,h̄, g)|\ncost(qnA,h̄, y) ≥ |V (QkB,h, h)| − 1 OPT .\nIt follows that\nmin y∈Y |V (QkB,h, g) \\ V (QkB,h ◦ qnA,h̄, g)| cost(qnA,h̄, y) ≥ |V (QkB,h, h)| − 1 OPT .\nTherefore, the query qk+1B,h , selected by the greedy algorithm at step k + 1, satisfies\n|V (QkB,h, h) \\ V (Q k+1 B,h , h)| cost(qk+1B,h , h) ≥ |V (QkB,h, h)| − 1 OPT .\nIt follows that cost(qk+1B,h , h) ≤ OPT. In addition, it follows that\n|V (Qk+1B,h , h)| − 1 ≤ (|V (Q k B,h, h)| − 1)(1− cost(qk+1B,h , h)/OPT)\n≤ (|V (QkB,h, h)| − 1) exp(−cost(qk+1B,h , h)/OPT).\nThis analysis holds for every length k of a sub-sequence QB,h. Therefore by induction\n|V (QkB,h, h)| − 1 ≤ (|H| − 1) k∏ i=1 exp(−cost(qiB,h, h)/OPT)\n= (|H| − 1) exp(−cost(QkB,h, h)/OPT).\nB terminates at the minimal k such that |V (QkB,h, h)| − 1 < 1. This holds for any k such that exp(−cost(QkB,h, h)/OPT) < 1/(|H|− 1), which means cost(QkB,h, h) > ln(|H|− 1) ·OPT. Let k′ be the minimal integer that satisfies this inequality. Then cost(Qk\n′−1 B,h , h) ≤ ln(|H| − 1) · OPT. Since\ncost(qk ′ B,h, h) ≤ OPT, it follows that cost(Qk ′ B,h, h) ≤ (ln(|H|− 1) + 1) ·OPT. This analysis holds for any h ∈ H, thus the worst-case cost of the greedy algorithm is at most (ln(|H| − 1) + 1) ·OPT."
    } ],
    "references" : [ {
      "title" : "Agnostic active learning",
      "author" : [ "M.F. Balcan", "A. Beygelzimer", "J. Langford" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning (ICML),",
      "citeRegEx" : "Balcan et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2006
    }, {
      "title" : "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bartlett and Mendelson.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bartlett and Mendelson.",
      "year" : 2002
    }, {
      "title" : "The Theory of Probabilities",
      "author" : [ "S. Bernstein" ],
      "venue" : "Gastehizdat Publishing House, Moscow,",
      "citeRegEx" : "Bernstein.,? \\Q1946\\E",
      "shortCiteRegEx" : "Bernstein.",
      "year" : 1946
    }, {
      "title" : "Importance weighted active learning",
      "author" : [ "A. Beygelzimer", "S. Dasgupta", "J. Langford" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2009
    }, {
      "title" : "Regret minimizing audits: A learning-theoretic basis for privacy protection",
      "author" : [ "J. Blocki", "N. Christin", "A. Dutta", "A. Sinha" ],
      "venue" : "In Proceedings of 24th IEEE Computer Security Foundations Symposium,",
      "citeRegEx" : "Blocki et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Blocki et al\\.",
      "year" : 2011
    }, {
      "title" : "Learnability and the VapnikChervonenkis dimension",
      "author" : [ "A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Blumer et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Blumer et al\\.",
      "year" : 1989
    }, {
      "title" : "Improving generalization with active learning",
      "author" : [ "D. Cohn", "L. Atlas", "R. Ladner" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cohn et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Cohn et al\\.",
      "year" : 1994
    }, {
      "title" : "Analysis of a greedy active learning strategy",
      "author" : [ "S. Dasgupta" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Dasgupta.,? \\Q2004\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 2004
    }, {
      "title" : "A general agnostic active learning algorithm",
      "author" : [ "S. Dasgupta", "D. Hsu", "C. Monteleoni" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Dasgupta et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2008
    }, {
      "title" : "Lower bounds in pattern recognition and learning",
      "author" : [ "L. Devroye", "G. Lugosi" ],
      "venue" : "Pattern recognition,",
      "citeRegEx" : "Devroye and Lugosi.,? \\Q1995\\E",
      "shortCiteRegEx" : "Devroye and Lugosi.",
      "year" : 1995
    }, {
      "title" : "Learning on the border: Active learning in imbalanced data classification",
      "author" : [ "Şeyda. Ertekin", "J. Huang", "L. Bottou", "C.L. Giles" ],
      "venue" : "In Proceedings of the ACM Sixteenth Conference on Information and Knowledge Management (CIKM",
      "citeRegEx" : "Ertekin et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ertekin et al\\.",
      "year" : 2007
    }, {
      "title" : "A threshold of ln n for approximating set cover",
      "author" : [ "U. Feige" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Feige.,? \\Q1998\\E",
      "shortCiteRegEx" : "Feige.",
      "year" : 1998
    }, {
      "title" : "Adaptive submodularity: Theory and applications in active learning and stochastic optimization",
      "author" : [ "D. Golovin", "A. Krause" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Golovin and Krause.,? \\Q2011\\E",
      "shortCiteRegEx" : "Golovin and Krause.",
      "year" : 2011
    }, {
      "title" : "Efficient active learning of halfspaces: an aggressive approach",
      "author" : [ "A. Gonen", "S. Sabato", "S. Shalev-Shwartz" ],
      "venue" : "In The 30th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Gonen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gonen et al\\.",
      "year" : 2013
    }, {
      "title" : "A bound on the label complexity of agnostic active learning",
      "author" : [ "S. Hanneke" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "Hanneke.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hanneke.",
      "year" : 2007
    }, {
      "title" : "Teaching dimension and the complexity of active learning. In Learning Theory, pages 66–81",
      "author" : [ "S. Hanneke" ],
      "venue" : null,
      "citeRegEx" : "Hanneke.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hanneke.",
      "year" : 2007
    }, {
      "title" : "Rates of convergence in active learning",
      "author" : [ "S. Hanneke" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Hanneke.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hanneke.",
      "year" : 2011
    }, {
      "title" : "Learning from imbalanced data",
      "author" : [ "H. He", "E.A. Garcia" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "He and Garcia.,? \\Q2009\\E",
      "shortCiteRegEx" : "He and Garcia.",
      "year" : 2009
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "W. Hoeffding" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Hoeffding.,? \\Q1963\\E",
      "shortCiteRegEx" : "Hoeffding.",
      "year" : 1963
    }, {
      "title" : "Constructing optimal binary decision trees is NP-complete",
      "author" : [ "L. Hyafil", "R.L. Rivest" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "Hyafil and Rivest.,? \\Q1976\\E",
      "shortCiteRegEx" : "Hyafil and Rivest.",
      "year" : 1976
    }, {
      "title" : "Selective supervision: Guiding supervised learning with decision-theoretic active learning",
      "author" : [ "A. Kapoor", "E. Horvitz", "S. Basu" ],
      "venue" : "In Proceedings of IJCAI,",
      "citeRegEx" : "Kapoor et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kapoor et al\\.",
      "year" : 2007
    }, {
      "title" : "Efficient noise-tolerant learning from statistical queries",
      "author" : [ "M. Kearns" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Kearns.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kearns.",
      "year" : 1998
    }, {
      "title" : "Rademacher complexities and bounding the excess risk of active learning",
      "author" : [ "V. Koltchinskii" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Koltchinskii.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koltchinskii.",
      "year" : 2010
    }, {
      "title" : "Active learning using arbitrary binary valued queries",
      "author" : [ "S.R. Kulkarni", "S.K. Mitter", "J.N. Tsitsiklis" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 1993
    }, {
      "title" : "On the uniform convergence of relative frequencies of events",
      "author" : [ "V.N. Vapnik", "A.Y. Chervonenkis" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Vapnik and Chervonenkis.,? \\Q2001\\E",
      "shortCiteRegEx" : "Vapnik and Chervonenkis.",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Existing work on active learning with costs (Margineantu, 2007; Kapoor et al., 2007; Settles et al., 2008; Golovin and Krause, 2011) typically assumes that the cost of labeling each point is known a priori, so the algorithm can use the costs directly to select a query.",
      "startOffset" : 44,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : "Existing work on active learning with costs (Margineantu, 2007; Kapoor et al., 2007; Settles et al., 2008; Golovin and Krause, 2011) typically assumes that the cost of labeling each point is known a priori, so the algorithm can use the costs directly to select a query.",
      "startOffset" : 44,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : ", 2008; Golovin and Krause, 2011) typically assumes that the cost of labeling each point is known a priori, so the algorithm can use the costs directly to select a query. Our model is significantly different, as the costs depend on the outcome of the query itself. Kapoor et al. (2007) do mention the possibility of class-dependent costs, but this possibility is not studied in detail.",
      "startOffset" : 8,
      "endOffset" : 286
    }, {
      "referenceID" : 4,
      "context" : "An unrelated game-theoretic learning model addressing “auditing” was proposed by Blocki et al. (2011).",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "If a sample S is drawn from D with |S| = mag( , δ, d) then with probability 1 − δ, ∀h ∈ H, err(D,h) ≤ err(S, h) + and err(S,H) ≤ err(D,H)+ (Bartlett and Mendelson, 2002).",
      "startOffset" : 139,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "If a sample S is drawn from D with |S| = mag( , δ, d) then with probability 1 − δ, ∀h ∈ H, err(D,h) ≤ err(S, h) + and err(S,H) ≤ err(D,H)+ (Bartlett and Mendelson, 2002). Let mν( , δ, d) = C(d ln(c/ν )+ln(c/δ))/ν2 . Results of Vapnik and Chervonenkis (1971) show that if H has VC dimension d and S is drawn from D with |S| = mν , then for all h ∈ H, err(S, h) ≤ max {err(D,h)(1 + ν), err(D,h) + ν } and (1) err(D,h) ≤ max {err(S, h)(1 + ν), err(S, h) + ν } .",
      "startOffset" : 140,
      "endOffset" : 258
    }, {
      "referenceID" : 3,
      "context" : "In particular, a simple adaptation of a result of Beygelzimer et al. (2009) establishes the following lower bound.",
      "startOffset" : 50,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "1 an auditing complexity of Ω(d/α2) is unavoidable, be we can hope to improve over the passive sample complexity lower bound of Ω(d/ηα2) (Devroye and Lugosi, 1995) by avoiding the dependence on η.",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 16,
      "context" : "For active learning, it has been shown that in some cases, the Ω(d/η) passive sample complexity can be replaced by an exponentially smaller O(d ln(1/η)) active label complexity (Hanneke, 2011), albeit sometimes with a larger polynomial dependence on d.",
      "startOffset" : 177,
      "endOffset" : 192
    }, {
      "referenceID" : 23,
      "context" : "For thresholds, active learning requires Ω(ln(1/η)) labels (Kulkarni et al., 1993).",
      "startOffset" : 59,
      "endOffset" : 82
    }, {
      "referenceID" : 23,
      "context" : "Due to information-theoretic considerations, any algorithm which learns an unlabeled pool S has an active label complexity of at least log2 |H|S | (Kulkarni et al., 1993), where H|S is the set of restrictions of functions in H to the domain S.",
      "startOffset" : 147,
      "endOffset" : 170
    }, {
      "referenceID" : 21,
      "context" : "(1989), has been studied extensively in different regimes (Kearns, 1998; Long and Tan, 1998), including active learning (Hanneke, 2007b).",
      "startOffset" : 58,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "This hypothesis class, first introduced in Blumer et al. (1989), has been studied extensively in different regimes (Kearns, 1998; Long and Tan, 1998), including active learning (Hanneke, 2007b).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "The construction is a simple adaptation of a construction due to Dasgupta (2004), originally showing an active learning lower bound for the class of hyperplanes.",
      "startOffset" : 65,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "The idea of iteratively refining a set of possible hypotheses has been used in a long line of active learning works (Cohn et al., 1994; Balcan et al., 2006; Hanneke, 2007a; Dasgupta et al., 2008).",
      "startOffset" : 116,
      "endOffset" : 195
    }, {
      "referenceID" : 0,
      "context" : "The idea of iteratively refining a set of possible hypotheses has been used in a long line of active learning works (Cohn et al., 1994; Balcan et al., 2006; Hanneke, 2007a; Dasgupta et al., 2008).",
      "startOffset" : 116,
      "endOffset" : 195
    }, {
      "referenceID" : 8,
      "context" : "The idea of iteratively refining a set of possible hypotheses has been used in a long line of active learning works (Cohn et al., 1994; Balcan et al., 2006; Hanneke, 2007a; Dasgupta et al., 2008).",
      "startOffset" : 116,
      "endOffset" : 195
    }, {
      "referenceID" : 19,
      "context" : "This can be shown by a reduction to set-cover (Hyafil and Rivest, 1976).",
      "startOffset" : 46,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "This approach has also been shown to provide considerable performance gains in practical settings (Gonen et al., 2013).",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "For active learning, and for query costs that do not depend on the true hypothesis (that is cost(x, h) ≡ cost(x)), Golovin and Krause (2011) showed an efficient greedy strategy that achieves a cost of O(OPTcost(S) · ln(|H|)) for any S.",
      "startOffset" : 115,
      "endOffset" : 141
    }, {
      "referenceID" : 11,
      "context" : "Nonetheless, hardness of approximation results for set cover (Feige, 1998), in conjunction with the reduction to set cover of Hyafil and Rivest (1976) mentioned above, imply that such an approximation factor cannot be avoided for a general auditing algorithm.",
      "startOffset" : 61,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "Nonetheless, hardness of approximation results for set cover (Feige, 1998), in conjunction with the reduction to set cover of Hyafil and Rivest (1976) mentioned above, imply that such an approximation factor cannot be avoided for a general auditing algorithm.",
      "startOffset" : 62,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "First, it is known that for some hypothesis classes, active learning cannot improve over passive learning for certain distributions (Dasgupta, 2004), and the same is true for auditing.",
      "startOffset" : 132,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "However, exponential speedups are possible for active learning on certain classes of distributions (Balcan et al., 2006; Dasgupta et al., 2008), in particular ones with a small disagreement coefficient (Hanneke, 2007a).",
      "startOffset" : 99,
      "endOffset" : 143
    }, {
      "referenceID" : 8,
      "context" : "However, exponential speedups are possible for active learning on certain classes of distributions (Balcan et al., 2006; Dasgupta et al., 2008), in particular ones with a small disagreement coefficient (Hanneke, 2007a).",
      "startOffset" : 99,
      "endOffset" : 143
    }, {
      "referenceID" : 22,
      "context" : "This quantity is related to the Alexander capacity function (Koltchinskii, 2010), which appears in lower bounds for active learning (Raginsky and Rakhlin, 2011).",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "It is well known that for some important settings, such as learning with hyperplanes, there are distributions which resist any improvement using active learning (Dasgupta, 2004).",
      "startOffset" : 161,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "However, exponential speedups are possible for active learning on certain classes of distributions (Balcan et al., 2006; Dasgupta et al., 2008), in particular ones with a small disagreement coefficient (Hanneke, 2007a). This quantity is related to the Alexander capacity function (Koltchinskii, 2010), which appears in lower bounds for active learning (Raginsky and Rakhlin, 2011). It would be interesting if a similar property of the distribution can guarantee an improvement with auditing over active or passive learning. Investigating such a general property might shed light on auditing for other important hypothesis classes such as decision trees or halfspaces. It is well known that for some important settings, such as learning with hyperplanes, there are distributions which resist any improvement using active learning (Dasgupta, 2004). Recent work by Gonen et al. (2013) has shown that both theoretically and empirically, more aggressive learning strategies can be effective in the realizable case.",
      "startOffset" : 100,
      "endOffset" : 882
    }, {
      "referenceID" : 10,
      "context" : "Some of these approaches (Ertekin et al., 2007) use the same active learning heuristics (Tong and Koller, 2001), and it would be interesting to see how these apply to auditing.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "Prior work on learning from unbalanced data was surveyed by He and Garcia (2009). Some of these approaches (Ertekin et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "1 (Hoeffding’s Inequality (Hoeffding, 1963)).",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "2 (Bernstein’s Inequality (Bernstein, 1946)).",
      "startOffset" : 26,
      "endOffset" : 43
    } ],
    "year" : 2015,
    "abstractText" : "We propose a learning setting in which unlabeled data is free, and the cost of a label depends on its value, which is not known in advance. We study binary classification in an extreme case, where the algorithm only pays for negative labels. Our motivation are applications such as fraud detection, in which investigating an honest transaction should be avoided if possible. We term the setting auditing, and consider the auditing complexity of an algorithm: the number of negative labels the algorithm requires in order to learn a hypothesis with low relative error. We design auditing algorithms for simple hypothesis classes (thresholds and rectangles), and show that with these algorithms, the auditing complexity can be significantly lower than the active label complexity. We also discuss a general competitive approach for auditing and possible modifications to the framework.",
    "creator" : "LaTeX with hyperref package"
  }
}