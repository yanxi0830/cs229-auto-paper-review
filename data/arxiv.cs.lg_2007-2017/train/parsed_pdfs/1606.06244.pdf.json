{
  "name" : "1606.06244.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Convergence of Common Learning Algorithms in Games",
    "authors" : [ "Dylan J. Foster", "Zhiyuan Li", "Thodoris Lykouris", "Karthik Sridharan" ],
    "emails" : [ "djfoster@cs.cornell.edu.", "lizhiyuan13@mails.tsinghua.edu.cn.", "teddlyk@cs.cornell.edu.", "sridharan@cs.cornell.edu.", "eva@cs.cornell.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n06 24\n4v 1\n[ cs\n.G T\n] 2\n0 Ju\nOur framework applies to dynamic population games via a low approximate regret property for shifting experts. Here we strengthen the results of [LST16] in two ways: We allow players to select learning algorithms from a larger class, which includes a minor variant of the basic Hedge algorithm, and we increase the maximum churn in players for which approximate optimality is achieved.\nIn the bandit setting we present a novel algorithm which provides a “small loss”-type bound with improved dependence on the number of actions and is both simple and efficient. This result may be of independent interest.\n∗Cornell University, djfoster@cs.cornell.edu. Work supported under NSF grant CDS&E-MSS 1521544. †Tsinghua University, lizhiyuan13@mails.tsinghua.edu.cn. Research performed while author was visiting Cornell University. ‡Cornell University, teddlyk@cs.cornell.edu. Work supported under ONR grant N00014-08-1-0031, and a Google faculty research award. §Cornell University, sridharan@cs.cornell.edu. Work supported by NSF grant CDS&E-MSS 1521544. ¶Cornell University, eva@cs.cornell.edu. Work supported in part by NSF grant CCF-1563714, ONR grant N00014-081-0031, and a Google faculty research award."
    }, {
      "heading" : "1 Introduction",
      "text" : "Consider players repeatedly playing a game, all acting independently to minimize their costs or maximize their utility. It is natural in this setting for each player to use a learning algorithm that guarantees small regret to decide on their strategies , as the environment is constantly changing due to strategy choices of the other players. It is well known that such decentralized no-regret dynamics are guaranteed to converge to a form of equilibrium of the game. Furthermore, in a large class of games, termed smooth games by Roughgarden [Rou15], they converge to outcomes with approximately optimal social welfare matching the price of anarchy, the welfare loss of Nash equilibria. In cost minimization games the overall cost is λ/(1−µ) times the minimal cost, while in smooth mechanisms [ST13] (such as auctions) it is λ/max(1, µ) times the maximum total utility, for parameters λ and µ of the smoothness conditions. Examples of smooth games and mechanisms include routing games, andmany forms of auction games, including ad-auctions, see [Rou15, ST13].\nThe speed at which the game outcome converges to this approximately optimal welfare is governed by individual players’ regret bounds. There is a large number of simple regret minimization algorithms (multiplicative weights, mirror decent, follow the regularized leader, see e.g. [Haz16]) guaranteeing that the average regret goes down as O(1/ √ T) with time T, which is tight in adversarial settings.\nTaking advantage of the fact that playing a game against opponents who themselves are also using regretminimization innot a truly adversarial setting, a sequence of papers [DDK15,RS13b, SALS15] showed that byusing specific learning algorithms, the dependence onTof this convergence rate can be improved inO(1/T). Concretely, Syrgkanis et al [SALS15] show this for all algorithms satisfying the so-called RVU property (regret by variation in utilities), which include the Optimistic Mirror Descent of [RS13b], converge at a O(1/T) rate with a fixed number of players.\nOne issue with the works of [DDK15, RS13b, SALS15] is that they use expected loss as their feedback model to the players. In each round, each player receives the expected cost of each of their actions in expectation over the current strategies of all other players. This clearly represents more information than is realistically available to players in games – at most each player sees the cost of each of her actions given the actions taken by the other players (realized feedback). In fact, even if each player had access to the action distributions of the other players, simply computing this expectation is generally intractable when n, the number of players, is large.\nWe improve the result of [SALS15] on the convergence to approximate optimality in smooth games in a number of different aspects. To achieve this, we relax the quality of approximation from the bound guaranteed by smoothness. Typical smoothness bounds on the price of anarchy in auctions are small constants, such a factor of 1.58 or 2 in item auctions. The simple idea of increasing the approximation we consider by an arbitrarily small constant ǫ > 0 enables the following results:\n• We show that learning algorithms obtaining fast convergence are ubiquitous. • We improve the speed of convergence by a factor of n, the number of players. • For all our results, players only need feedback based on realized outcomes, not expected\noutcomes. • We show that convergence occurs with high probability in most settings. • We further extend the results to show that it is enough for the players to observe realized\nbandit feedback, only seeing the outcome of the action they play.\n• Our results apply to settings where the set of players in the game changes over time [LST16]. We strengthen previous results in such settings by showing that a broader class of algorithms achieves approximate efficiency under significant churn.\nWe achieve these results by introducing a propertywe termLowApproximate Regret in Section 2.1, which simply states that an online learning algorithm achieves good regret against a multiplicative approximation of the best action in hindsight. This property is satisfied bymanyknown algorithms, including even the vanilla Hedge algorithm, as well as Optimistic Hedge (via a novel analysis). The crux of our analysis technique is the simple observation that formany types of data-dependent regret boundswe can fold part of the regret bound into the comparator term, allowing us to explore the trade-off between additive and multiplicative approximation.\nIn Section 3, we show that LowApproximateRegret implies fast convergence to the price of anarchy guaranteed by the smoothness property, and only requires feedback from the realization of actions played by other players, not their action distribution, or the expectation over their actions. We further show that this convergence occurs with high probability in most settings, and not only in expectation. For games with a large number of players, we also improve the speed of convergence. [SALS15] shows that players using Optimistic Hedge in a repeated game with n players, converge to the approximately optimal outcome guaranteed by smoothness at a rate of O(n2/T). They also offer an analysis guaranteeing O(n/T) speed of convergence, at the expense of a constant factor decrease in the quality of approximation (e.g., a factor of 4 in atomic congestion games with affine congestion). We achieve the convergence bound of O(n/T) with only an arbitrarily small loss in the approximation bound.\nIn Section 3, we show that in the full information setting, algorithms that satisfy the Low Approximate Regret property are ubiquitous, including simple, efficient algorithms such as Hedge and its variants. Armed with the knowledge that this broad class of algorithms experiences fast convergence in realistic settings, one can more reliably predict when fast convergence will occur in practice.\nComparing our work to [SALS15] in more detail, we note that our Low Approximate Regret property is broader than their RVU property. Further, LowApproximate Regret algorithms require only realized feedback, while the analysis of the RVU property in [SALS15] requires expected feedback. To see the contrast, consider the load balancing game introduced in [KP09] with two players and two bins, where each player selects a bin and observes cost given by the number of players in that bin. Initialized at the uniform distribution, any learning algorithmwith expectation feedback (particularly those considered in [SALS15]) will simply stay at the uniform distribution forever, because the expected cost vector distributes cost equally across the two bins. This gives low regret under expected costs, but suppose we were interested in realized costs: The only “black box” way to lift the [SALS15] to this case would be to simply evaluate the regret bound above under realized costs, but here players will experience Θ(1/ √ T) variation because they select bins uniformly at random, ruining the fast convergence. Our analysis essentially sidesteps this issue because individual players naturally concentrate at Low Approximate Regret.\nIn Section 4 we consider games where players can only observe the loss of the action they played given the actions taken by the other players, and receive no feedback for actions not played (bandit feedback). [RS13b] analyzed zero-sum games to bandit feedback, but assumed players receive expected cost over the strategies of all other players. We propose a new algorithm based on log-\nbarrier regularizationwith importance sampling that guarantees fast convergence ofO(d logT/ǫ) in this setting, where d is the number of actions. Known techniques such as adaptations of SCRiBLe (see [RS13a]), would result in a convergence rate of O(d3 logT). Our technique improving the dependence of the regret bounds on the number of experts d is of independent interest.\nFinally, in Section 5, we consider the dynamic population game setting of Lykouris et al. [LST16], where players are entering and leaving the game over time. [LST16] showed that regret bounds for shifting experts directly influence the rate at which players can turn over and still guarantee close to optimal solution on average. We show that a number of learning algorithms have the Low Approximate Regret property even in the shifting experts setting, allowing us to extend the fast convergence result to dynamic games. Such learning algorithms include a noisy version of Hedge, as well as AdaNormalHedge [LS15] which was previously analyzed in the dynamic setting in [LST16]. The smaller approximate regret error allows us to increase the turnover rate from the one in [LST16], while also widening and simplifying the class of learning algorithms that players can use to guarantee the close to optimal average welfare."
    }, {
      "heading" : "2 Repeated Games and Learning Dynamics",
      "text" : "We consider a game G among a set of n players. Each player i has an action space Si and a cost function costi : S1×· · ·×Sn → [0, 1] thatmaps an action profile s = (s1, . . . , sn) to the cost costi(s) that player experiences. We assume that the action space of each player has cardinality d, i.e. |Si| = d. We let w = (w1, . . . ,wn) denote a list of probability distributions over all players’ actions, where wi ∈ ∆(Si) and wi,x is the probability of action x ∈ Si. Analogous quantities for utility maximization games are defined in Appendix D.\nWe consider the setting where the game G is played repeatedly for T time steps. At each time step t each player i picks a probability distribution wt\ni ∈ ∆(Si) over actions and draws their action sti\nfrom this distribution. Depending on the game playing environment under consideration, players will receive different types of feedback after each step. Most of this paper considers feedback where at the end of the iteration each player i observes the utility she would have received had she played any possible action x ∈ Si given the actions taken by the other players. More formally let ct i,x = costi(x, s t −i), where s t −i is the set of strategies of all but the i th player at step t, and let ct i = (ct i,x )x∈Si . Note that the expected cost of player i at iteration t (conditioned on the other players’ actions) is simply the inner product 〈wt\ni , ct i 〉.\nWe refer to this form of feedback as realized feedback since it only depends on the realized actions st−i sampled by the oponents – it does not directly depend on their distributions wt−i. This should be contrasted with the expectation feedback used by [SALS15, DDK15, RS13b], where player i observes Est−i∼w t −i [costi(x, s t −i)] for each x.\nSections 4 and 5 consider extensions of our repeated game model. In Section 4 we examine partial information (“bandit”) feedback, where players observe only the cost of their own realized actions. In Section 5 we consider a setting where the player set is evolving over time. Here we use the dynamic populationmodel of Lykouris et al. [LST16], where at each round t each player i is replaced (“turns over”) with some probability p. The new player has cost function costt\ni (·) and action space\nSt i which may change arbitrarily subject to certain constraints. We will formalize this notion later on."
    }, {
      "heading" : "2.1 Learning Dynamics",
      "text" : "We assume that players select their actions using a learning algorithm satisfying a property we call Low Approximate Regret, which simply requires that the cumulative cost of the learner multiplicatively approximates the cost of the best action they could have chosen in hindsight. We will see in subsequent sections that this property is ubiquitous and leads to fast convergence in a robust range of settings. Definition 1. (Low Approximate Regret) A learning algorithm satisfies the Low Approximate Regret property for parameter ǫ and function A if for all action distributions f ∈ ∆(Si),\n(1 − ǫ) T∑\nt=1\n〈wti , c t i〉 ≤\nT∑\nt=1\n〈 f , cti〉 + A(d,T)\nǫ . (1)\nAlearning algorithm satisfiesLowApproximateRegret against shifting experts if for all sequences f 1 , . . . , f T ∈ ∆(Si), letting K be the number of shifts, i.e. K = |{i > 2 : f t−1 , f t}|,\n(1 − ǫ) T∑\nt=1\n〈wti , c t i〉 ≤\nT∑\nt=1\n〈 f t, cti〉 + (1 + K) A(d,T)\nǫ . (2)\nIn the bandit feedback setting, we require (1) or (2) to hold in expectations over the realized strategies of player i.\nWe will use the version of the Low Approximate Regret property with shifting experts when consideringplayers in a dynamic population games in Section 5. In this case, the game environment can be constantly changing due to churn in the population, and we need the players to have low approximate regret with shifting experts to guarantee high social welfare despite the churn.\nAll algorithms we are aware of that satisfy Low Approximate Regret can be made to do so for any choice of the approximation factor ǫ via an appropriate selection of parameters. Many algorithms have an even stronger property: They satisfy (1) or (2) for all ǫ > 0 simultaneously. We say that such algorithms satisfy the Strong Low Approximate Regret property. We shall see later that this property has favorable consequences in the context of repeated games.\nThe Low Approximate Regret property differs from previous properties (e.g. RVU) in that it only requires that the learner’s cost be close to a multiplicative approximation to the cost of the best action in hindsight. We can generally achieve much lower regret relative to the multiplicative approximation, and which is significant because low regret implies fast convergence. For instance, if we consider only uniform (i.e. not data-dependent) regret bounds the Hedge algorithm can only achieve O( √ T log d) exact regret, but can achieve ǫ-approximate regret of log d/ǫ for any ǫ. Remark. In Appendix D we show that the Low Approximate Regret property and our subsequent results naturally extend to utility maximization games."
    }, {
      "heading" : "2.2 Smooth Games",
      "text" : "It is well-known that in a large class of games, termed smooth games by Roughgarden [Rou15], traditional learning dynamics converge to approximately optimal social welfare. In subsequent sections we analyze the convergence of Low Approximate Regret learning dynamics in such smooth games. We will see that Low Approximate Regret (for sufficiently small A) coupled with smoothness of the game implies fast convergence of learning dynamics to desirable social welfare under a variety of conditions. Before proving this result we review social welfare and smooth games. For a given action profile s, the social cost is C(s) = ∑n\ni=1 costi(s). To bound the efficiency loss due to the selfish behavior of the players, we compare to the optimal social cost that can be achieved by some centralized entity, i.e.\nOpt = min so\nn∑\ni=1\ncosti(s o).\nDefinition 2. (Smooth game [Rou15]) A cost minimization game is called (λ, µ)-smooth if for all strategy profiles s and s∗: ∑ i costi(s ∗ i , s−i) ≤ λcosti(s∗) + µcosti(s). This property is typically applied using a (close to) optimal action profile s∗ = so. For this case the property implies that if s is an action profile with very high cost, then some player deviating to her share of the optimal profile s∗\ni will improve her cost.\nFor smooth games, the price of anarchy is at most λ1−µ , meaning that Nash equilibria of the game, as well as no-regret learning outcomes in the limit, have social cost at most a factor of λ1−µ above the optimum. Smooth cost minimization games include congestion games such as routing or load balancing. For example, atomic congestion games with affine cost functions are ( 53 , 1 3 )-smooth [CK05], non-atomic games are (1, 14 ) smooth [RT02], implying a price of anarchy of 2.5 and 1.33 respectively. Whilewe focus on cost-minimization games for simplicity of exposition, an analogous definition also applies for utility maximization, including smooth mechanisms [ST13], which we will elaborate on in Appendix D. Smooth mechanisms include most simple auctions. For example, the first price item auction is (1 − 1/e, 1)-smooth and all-pay actions are (1/2, 1)-smooth, implying a price of anarchy of 1.58 and 2 respectively. All of our results extend to such mechanisms."
    }, {
      "heading" : "3 Learning in Games with Full Information Feedback",
      "text" : "We now analyze the efficiency of algorithms with the Low Approximate Regret property in our full information setting. Our first proposition shows that, for smooth games with full information feedback, learners with the Low Approximate Regret property converge to efficient outcomes. Proposition 1. In any (λ, µ)-smooth game, if all players use LowApproximate Regret algorithms satisfying Eq. (1) with parameters ǫ and A, then\n1\nT\n∑\nt\nE [ C(st) ] ≤ λ\n1 − µ − ǫOpt + n T · 1 1 − µ − ǫ · A(d,T) ǫ ,\nwhere st is the action profile drawn on round t from the corresponding mixed actions of the players.\nProof. This proof is a straightforward modification of the usual price of anarchy proof for smooth games. We get the claimed bound by writing ∑ tE [ C(st) ] = ∑ i ∑ t E [ costi(s t) ] ; using the Low Approximate Regret property with f = s∗ i for each player i for the optimal solution s∗; then using\nthe smoothness property for each time t to bound ∑\ni costi(s ∗ i , st−i), and rearranging terms.\nNote that for ǫ << (1 − µ) the approximation factor of λ/(1 − µ − ǫ) is very close to the price of anarchy of λ/(1 − µ), showing that Low Approximate Regret learning quickly leads to outcomes with social welfare as high as the welfare of exact Nash equilibria. A simple corollary of this proposition is that when learners satisfy the Strong Low Approximate Regret property the bound above can be taken to depend on Opt, even though this value is unknown to the players.\nWhenever the Low Approximate Regret property is satisfied, a high probability version of the propertywith similar dependence on ǫ andA is also satisfied (see Appendix A.1). This implies that in addition to quickly converging to efficient outcomes in expectation, Low Approximate Regret learners experience fast convergence in high probability. Proposition 2. In any (λ, µ)-smooth game, if all players use LowApproximate Regret algorithms satisfying Eq. (1) for parameters ǫ and A, then ∀δ > 0, with probability at least 1 − δ,\n1\nT\n∑\nt\nC(st) ≤ λ 1 − µ − γOpt + n T · 1 1 − µ − γ ·\n[ 4A(d,T)\nγ +\n12 log(n log2(T)/δ))\nγ\n] ,\nwhere st is the action profile drawn on round t from the players’ mixed actions and γ = 2ǫ/(1+ ǫ)."
    }, {
      "heading" : "3.1 Examples of Simple Low Approximate Regret Algorithms",
      "text" : "Propositions 1 and 2 are of course only informative if they are applied with algorithms for which A is sufficiently small. One would further hope that such algorithms are relatively simple and easy to find. We show now that the well-known Hedge algorithm as well as basic variants such as Optimistic Hedge and Hedge with online learning rate tuning satisfy the property with A = O(log d), which will lead to fast convergence both in terms of n and T. It is important to note that for all the algorithms considered in this paper, we can achieve the Low Approximate Regret property for any fixed ǫ > 0 of choice for an appropriate parameter setting. In Appendix A.2, we provide full descriptions of these algorithms and proofs. Example 1. Hedge satisfies the Low Approximate Regret property with A(d,T) = log(d). In particular one can achieve the property for any fixed ǫ > 0 by selecting an appropriate learning rate. Example 2. Hedge with online learning rate tuning satisfies the Strong Low Approximate Regret property with A(d,T) = 8 log(d). Example 3. Optimistic Hedge satisfies the Low Approximate Regret property with A(d,T) = 8 log(d). As with vanilla Hedge, we can choose the learning rate to achieve the property with any ǫ. Example 4. Any algorithm satisfying a “small loss” regret bound of the form √ (Learner’s cost) · A or√\n(Cost of best action) · A satisfies Strong Low Approximate Regret via the AM-GM inequality, i.e. √ (Learner’s cost) · A ∝ inf\nǫ>0 [ǫ · (Learner’s cost) + A/ǫ].\nIn particular, this implies that the following algorithms are Strong Low Approximate Regret:\n• Canonical small loss and self-confident algorithms, e.g. [FS97, ACBG02, YEYS04]. • Algorithm of [CBMS07]. • Variation MW [HK10]. • AEG-Path [SL14]. • AdaNormalHedge [LS15]. • Squint [KVE15]. • Optimistic PAC-Bayes [FRS15].\nThis example shows that the Strong Low Approximate Regret property in fact is ubiquitous, as it is satisfied by any algorithm that provides small loss regret bounds or one of many variants on this type of bound. Moreover, all algorithms that can satisfy the LowApproximate Regret property for all fixed ǫ via appropriate choice of parameters can be made to satisfy the strong property using the doubling trick."
    }, {
      "heading" : "3.2 Main Result for Full Information Games",
      "text" : "We now summarize our results so far into a single theorem about behavior of Low Approximate Regret learners in full information games with realized feedback. Theorem 3. In any (λ, µ)-smooth game, if all players use Low Approximate Regret algorithms satisfying (1) for parameter ǫ1 and A(d,T) = O(log d), then\n1\nT\n∑\nt\nE [ C(st) ] ≤ λ\n1 − µ − ǫOpt + n T · 1 1 − µ − ǫ · O(log d) ǫ\nand furthermore, ∀δ > 0, with probability at least 1 − δ,\n1\nT\n∑\nt\nE [ C(st) ] ≤ λ\n1 − µ − ǫOpt + n T · 1 1 − µ − ǫ ·\n[ O(log d)\nǫ +\nO(log(n log2(T)/δ))\nǫ\n] .\nCorollary 4. If all players use algorithms with the Strong Low Approximate Regret property, then\n1. The above results hold for all ǫ > 0 simultaneously 2. Individual players have regret bounded by O ( 1/ √ T ) , even in adversarial settings.\n3. The players approach a coarse correlated equilibrium asymptotically.\nComparison with Syrgkanis et al. [SALS15]. By relaxing the standard λ1−µ price of anarchy bound, Theorem 3 substantially broadens the class of algorithms that experience fast convergence to include even the common Hedge algorithm. The main result of [SALS15] shows that learning algorithms that satisfy their RVU (regret by variation in utilities) property converge to the price of\n1We can also show that the theorem holds if players satisfy the property for different values of ǫ, but with a dependence on the worst case value of ǫ across all players.\nanarchy bound λ1−µ at the rate of n2 log d T . They further show how to achieve a worse approximation of λ(1+µ)\nµ(1−µ) at the improved rate (in terms of n) of n logd T . We come close to achieving the best of both\nworlds in that we converge to an approximation arbitrarily close to λ1+µ at a rate of n log d T . Note that in atomic congestion games with affine congestion function µ = 1/3, so their bound of λ(1+µ)\nµ(1−µ) loses\na factor of 4 compared to the price of anarchy.\nWe show that Strong Low Approximate Regret algorithms such as Hedge with learning rate tuning simultaneously experience both fast O(n/T) convergence in games and an O(1/ √ T) bound on individual regret in adversarial settings. In contrast [SALS15] are only able to simultaneously show O ( n/ √ T ) individual regret and O(n3/T) convergence to price of anarchy.\nLow Approximate Regret algorithms only need realized feedback, whereas [SALS15] require expectation feedback. Having players receive expected feedback is unrealistic in terms of both information and computation. Indeed, even if the necessary information were available, computing expectations over discrete probability distributions is not tractable in the general case unless n is taken to be constant.\nOur results imply that Optimistic Hedge enjoys the best of two worlds: It enjoys fast convergence to the exact λ1−µ price of anarchy using expectation feedback as well as fast convergence to the ǫ-approximate price of anarchy using realized feedback. Our new analysis of Optimistic Hedge ( Appendix A.2.2) sheds light on another desirable property of this algorithm: Its regret on any cost sequence is bounded in terms of the net cost the Hedge strategy would have produced, had it been selected on that sequence instead, meaning it will have good performance any time Hedge does.\nThe main differences between our results are summarized in Figure 1."
    }, {
      "heading" : "4 Bandit Feedback",
      "text" : "In many realistic scenarios, players of a game might not even know what they would have lost or gained if they had deviated from the action they played. We model this lack of information with bandit feedback, in which each player observes a single scalar costi(s\nt) = 〈st i , ct i 〉 per round. When the\ngame we consider is smooth, one can use the Low Approximate Regret property just as we did for the full information setting to show that players quickly converge to efficient solutions. Our results here hold with the same generality as in the full information setting: As long as learners satisfy the Low Approximate Regret property (1), Proposition 1 holds.\nIn contrast to the full information setting where the most common algorithm, Hedge, achieves Low Approximate Regret with competitive parameters, the most common adversarial bandit algorithm Exp3 does not seem to satisfy Low Approximate Regret. There are however known bandit algorithms that satisfy Low Approximate Regret, typically with A(d,T) = poly(d) log(T). For instance, [Sto05] provides the Exp3Light algorithm which has A(d,T) = d2 logT. The SCRiBLe algorithm introduced in [AHR08] (via the analysis in [RS13a]) enjoys the LowApproximate Regret property with A(d,T) = d3 log(dT). [AB10] provide a small loss bound for bandits which would be sufficient for Low Approximate Regret, but their strategy requires prior knowledge on the loss of the best expert (or a bound on it), which is not appropriate in our game setting. Similarly, the small loss bound in [Neu15] is not applicable in our setting as the work assumes oblivious adversary which is not the case in the games we consider.\nWe present a simple new bandit algorithm that achieves the Low Approximate Regret property with A(d,T) = d log(T/d), improving on the state of the art. Our algorithm uses a log barrier regularizer as in SCRiBLe, but uses classical importance sampling to produce unbiased estimates for cost vectors.\nAt each round the algorithm produces an importance weighted unbiased estimator of the costs: ĉt i [ j] = ct i [st i ]/wt i [st i ] when j = st i and ĉt i [ j] = 0 for j , st i . This estimator is used as the loss vector while\nperforming a Mirror Descent update with regularizer R(w) = ∑\ni log(1/wi).\nWe bound the regret for mirror descent in terms of local norms as in [AHR08]. However, counter to earlier algorithms using log-barrier type functions we don’t use the Dikin ellipsoid to perform exploration, and our analysis does not go through self-concordance. Insteadwe show that classical importanceweighted sampling approach, in fact, yields a favorable boundon the local normarising from the log barrier regularizer. The update step for our algorithm has the form:\n∀ j , st wt+1i [ j] = wt i [ j]\n1 + γwt i [ j]\n& wt+1i [s t] =\nwt i [st]\n1 + ηct i [st] + γwt i [st]\n(3)\nwhere γ < 0 is such that ∑d\nj=1 w t+1 i [ j] = 1. This method can be implemented in O(d) time per round (using line search to find γ). See Appendix B for a more detailed discussion. Lemma 5. The bandit algorithm proposed in (3) with η = ǫ/(1 + ǫ) achieves Low Approximate Regret given by:\n(1 − ǫ)E   ∑\nt\n〈sti , c t i〉   ≤ ∑\nt\n〈 f , ct〉 + d ( (1 + ǫ) log(T/d)\nǫ + 1\n) .\nA direct consequence of this lemma is that when all the players play our algorithm in a smooth game, they converge quickly to approximately efficient solution. Theorem 6. Consider a (λ, µ)-smooth game. If all players use the learning algorithm proposed in Eq. 3 with learning rate η = ǫ/(1+ ǫ), then\n1 T E\n  ∑\nt\nC(st)   ≤\nλ 1 − µ − ǫOpt + n T · 1 1 − µ − ǫ · 2(1 + ǫ)d logT ǫ .\nProof. Simply use the Low Approximate Regret property proved in Lemma 5 along with smoothness of the game. We obtain the final statement as in Proposition 1."
    }, {
      "heading" : "5 Dynamic Population Games",
      "text" : "In this section we consider the dynamic population repeated game setting introduced in [LST16]. Given a gameG as described in Section 2, a dynamic population gamewith stage gameG, is a repeated game where at each round t game G is played, and at each step, every player i is replaced by a new player with a turnover probability p. Concretely, when a player turns over, their strategy set and cost function are changed arbitrarily subject to the rules of the game. This models a repeated game setting where players have to adapt to an adversarially changing environment. We denote the cost function of player i at round t as costt\ni (·). As in Section 3, we assume that the players are\nreceiving full information feedback. That is, at the end of each round they observe the entire cost vector ct\ni = costt i (·, st−i), and know their own cost function, but are not aware of the costs of the other\nplayers in the game.\nLearning in Dynamic Population Games and the Price of Anarchy To guarantee small overall cost, players need to exhibit low regret against a shifting benchmark that corresponds to the optimal social cost Optt = mins∗t ∑ i cost t i (s∗t) at each step. With even a small probability p of change, the sequence of optimal solutions can have too many changes to be able to get low regret. In spite of this apparent difficulty, [LST16] prove that achieving at least a ρλ/(1−µ−ǫ) fraction of the optimal welfare is guaranteed if i) players are using low adaptive regret algorithms (see [HS09, LS15]) and ii) for the underlying optimization problem there exists a relatively stable sequence of solutions which at each step approximate the optimal solution by a factor of ρ. This holds as long as the turnover probability p is upper bounded by a function of ǫ, as well as other terms depending on the game, such as the stability of the close to optimal solution.\nWe consider games with dynamic population where each player uses a learning algorithm satisfying Low Approximate Regret for shifting experts (2). This shifting version of Low Approximate Regret naturally implies a dynamic game analog of our main efficiency result, Proposition 1. We prove that a simple variation of Hedge which mixes the prediction weights at each step with a very small amount of uniform noise satisfies the Low Approximate Regret property for shifting experts with small A. Moreover, we show that all the algorithms that satisfy a small loss version of the adaptive regret property considered in [LST16] satisfy the Strong Low Approximate Regret property. This strengthens the results in [LST16] by both weakening the behavioral assumption on the players, allowing them to use much simpler learning algorithms, and allowing a higher turnover probability.\nWe first present the extension of Proposition 1 to the dynamic population game setting. Let s∗1:T denote a sequence of near-optimal solutions s∗t, so that ∑\ni cost t i (s∗t) ≤ ρOptt for all time steps t,\nwhere s∗t i ∈ ∆(Si), the action of each player i in s∗t, may be randomized.\nWe use Ki to denote the random variable of the number of changes of strategies in player i in the strategy sequence s∗1:T. The stability of a sequence of solutions is determined by the expected number of changes that occur in it, i.e. E[ ∑ i Ki]. Theorem 7. (PoA with Dynamic Population) If all players use a Low Approximate Regret algorithm satisfying (2) in a dynamic population game, where the stage game is (λ, µ)-smooth game, and Ki denotes the number of changes of strategies in player i in a sequence s∗1:T of ρ-approximately minimum cost solution,\nthen 1\nT\n∑\nt\nE [ C(st) ] ≤ 1\nT λ · ρ 1 − µ − ǫ\n∑\nt\nE [ Optt ] +\nn + E [∑ i Ki ]\nT · 1 1 − µ − ǫ · A(d,T) ǫ , (4)\nwhere expectation is taken over the random turnover in the population playing the game, as well as the random choices of the players on the left hand side.\nThis result follows from using s∗1:T i\n– the action sequence of player i in the stable solution – as the shifting comparator in the Low Approximate Regret inequality, and then applying smoothness. See the proof in Appendix C.1. We can also extend this theorem to guarantee a high probability bound, analogous to the proof of Proposition 2.\nTo claim a price of anarchy bound, we need to make sure that the additive term in (4) is a small fraction of the optimal cost. The challenge here is that high turnover probability p reduces stability, increasing E [∑ i Ki ] . By using algorithms with smaller A, we can allow for higher E [∑ i Ki ] and hence higher turnover probability.\nAlgorithms with Low Approximate Regret for Shifting Experts A variant of the basic Hedge algorithmwith a small uniform noise added to its predictions, whichwe termNoisy Hedge, satisfies Low Approximate Regret for shifting experts with A = Õ(1). Letting wt be the Hedge distribution and π be the uniform distribution, Noisy Hedge predicts according to (1−θ)wt+θπwith θ at most O(1/T) in our applications. Proposition 8. Noisy Hedge with a constant learning rate ηt = ǫ satisfies the Low Approximate Regret property for shifting experts with A(d,T) = log(dT) + 1.\nProof of this proposition and details on the algorithm are in Appendix C.2. A similar result can be proven for a noisy version of Optimistic Hedge. In addition, all learning algorithms satisfying the small loss version of the adaptive regret property of [HS09] have low approximate regret with shifting experts.\nIn their results for dynamic population games, [LST16] use the AdaNormalHedge algorithm of [LS15]. Thismore complex algorithm satisfies the adaptive regret property of [HS09], but hasO(dT) space complexity. In contrast Noisy Hedge is as simple as Hedge, and requires space complexity of just O(d).\nResults in Dynamic Population Games Using Theorem 7 with Low Approximate Regret algorithms such as Noisy Hedge that have A(d,T) = log(dT) improves the turnover rate p in the results of [LST16].\nLet κ denote the expected number of players whose strategy in s∗1:T changes as one player turns over, so E [∑ i Ki ] = pnTκ, as each step pn players turn over in expectation. The parameter κ as defined here depends on the concrete game; it is a parameter of a high stability approximate optimization method used as in [LST16]. Let γ > 0 be a lower bound on the average cost of each player, at each time step, giving a lower bound on the optimum cost of ∑\nt E\n[ Optt ] ≥ γnT.\nUsing these two parameters, [LST16] show a price of anarchy bound of λρ/(1 − µ − ǫ), assuming the turnover probability p ≤ ǫ2γ2/(κ log(dT)). Using Theorem 7 we get the same price of anarchy bound, yet allow higher turnover probability by a factor of 1/γ: p ≤ ǫ2γ/(κ log(dT)).\nAs an illustrative example of the improvement, consider matching markets. Suppose n players are each bidding in first price item auction for one of many items (the winner paying her own bid for each item). Suppose vi j, the player i’s value for item j is in the range of vi j ∈ [γ, 1], and the players are unit-demand, bidding to win one high value items at low price. In this mechanism, we will use SW(s) to denote the social welfare achieved by action profile s, the sum of player utilities plus the auctioneer’s revenue, and use Optt is the maximum social welfare possible with players in round t.\nFirst price item auction is a (1−1/e, 1) smooth mechanism, hence has a price of anarchy of e/(e−1) ≈ 1.58. Lykouris et al [LST16] prove that a price of anarchy of 1.58(1 + η) is guaranteed if players use adaptive learning and the turnover probability is at most p ≤ γ2/(log(dT) log(1/γ)). Using the proof from [LST16] with the improved A(d,T) term of Proposition 8, we get an γ−1 improvement in the probability term: Theorem 9. If all players use a Low Approximate Regret algorithm satisfying (2) in a dynamic population matching market with first price item auctions, then\n1.58(1+ η) ∑\nt\nE [ SW(st) ] ≥ ∑\nt\nE [ Optt ] (5)\nassuming the turnover probability p is at most p ≤ γ/(log(dT) log(1/γ)). Considering other games and mechanisms [LST16], we get analogously improved version of all of them, including congestion games, bandwidth-sharing, as well as large markets."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank Vasilis Syrgkanis for sharing his simulation software and for helpful discussions."
    }, {
      "heading" : "APPENDIX",
      "text" : ""
    }, {
      "heading" : "A Proofs from Section 3",
      "text" : ""
    }, {
      "heading" : "A.1 Proof of Proposition 2",
      "text" : "To get the with high probability bounds we will need to use a concentration bound. We will use a refinement of Freedman’s martingale Bernstein inequality due to [BDH+08] which can be used to obtain high-probability versions of data-dependent regret bounds. Lemma 10 ([BDH+08]). Let X1, . . . ,XT be a martingale difference sequence with |Xt| ≤ b. Let σ̄2 =∑T\nt=1 Var(Xt | X1, . . . ,Xt−1) be the sum of conditional variances for a particular outcome X1, . . . ,XT. For all δ < 1/e, T ≥ 4 we have\nP   T∑\nt=1\nXt > 4 √ σ̄2 log(1/δ) + 2b log(1/δ)   ≤ log2(T)δ. (6)\nEquipped with the refined Freedman inequality, we show that a low approximate regret algorithm for the simplex concentrate against an adaptive adversary. Lemma 11 (High-Probability Regret Bound Against Fixed Expert). Let wt ∈ ∆(d) be selected by an algorithm satisfying the LAR property (1) for ǫ > 0 given ct selected by an adaptive adversary, and let st ∼ wt be the algorithm’s realized actions. Then with probability at least 1 − δ,\n(1 − γ) T∑\nt=1\n〈st, ct〉 ≤ T∑\nt=1\n〈 s⋆, ct 〉 + 4A(d,T)\nγ +\n12 log(log2(T)/δ)\nγ , (7)\nwhere ǫ = γ/(2 − γ).\nProof. Let Zt = (1 − ǫ)〈st, ct〉 − 〈s⋆, ct〉 be a random process indexed by t ∈ [T], where s⋆ = argmin f∈∆(d) ∑T t=1〈 f , ct〉. Let Xt = Zt − E [ Zt | s1, . . . , st−1 ] be the associated martingale difference sequence. Note that |Xt| ≤ (1 − ǫ) ≤ 1. That X is a martingale comes from the fact that while the adversary is adaptive, it cannot react to the learner’s randomness at time t in selecting ct. It should be noted that E[st | s1, . . . , st−1] = wt. Lemma 10 and (1) now imply that for a given draw of s1, . . . , sT, with probability at least 1 − δ,\n(1 − ǫ) T∑\nt=1\n〈st, ct〉 − T∑\nt=1\n〈 s⋆, ct 〉 ≤ A(d,T)\nǫ + 4\n√ σ̄2 log(log2(T)/δ) + 2 log(log2(T)/δ).\nTo complete this bound we must provide a bound on the conditional variance σ̄2. To this end note\nthat\nσ̄2 =\nT∑\nt=1\nE [( Xts )2 | s1, . . . , st−1 ]\n= (1 − ǫ)2 T∑\nt=1\nE [( 〈st, ct〉 − 〈wt, ct〉 )2 | s1, . . . , st−1 ]\n≤ (1 − ǫ)2 T∑\nt=1\nE [( 〈st, ct〉 )2 | s1, . . . , st−1 ]\n≤ (1 − ǫ)2 T∑\nt=1\nE [ 〈st, ct〉 | s1, . . . , st−1 ]\n= (1 − ǫ)2 T∑\nt=1\n〈wt, ct〉.\nHence, with probability at least 1 − δ.\n(1 − ǫ) T∑\nt=1\n〈st, ct〉 − T∑\nt=1\n〈 s⋆, ct 〉\n≤ A(d,T) ǫ + 4\n√\n(1 − ǫ)2   T∑\nt=1\n〈wt, ct〉   log ( log2(T)/δ ) + 2 log ( log2(T)/δ )\nNow for all ǫ′ > 0 we have:\n≤ A(d,T) ǫ + ǫ′(1 − ǫ)2   T∑\nt=1\n〈wt, ct〉   + 4 log ( log2(T)/δ ) /ǫ′ + 2 log ( log2(T)/δ ) ,\nand so the LAR property (1) implies\n≤ A(d,T) ǫ + ǫ′(1 − ǫ)   T∑\nt=1\n〈 s⋆, ct 〉 + A(d,T)\nǫ\n  + 4 log ( log2(T)/δ ) /ǫ′ + 2 log ( log2(T)/δ ) ,\nRearranging,\n(1 − ǫ) (1 + ǫ′(1 − ǫ))\nT∑\nt=1\n〈st, ct〉\n≤ T∑\nt=1\n〈 s⋆, ct 〉 +\n1\n1 + ǫ′(1 − ǫ)\n[ A(d,T)\nǫ + 4 log\n( log2(T)/δ ) /ǫ′ + 2 log ( log2(T)/δ )] + A(d,T)\nǫ .\nTaking ǫ′ = ǫ we have\n(1 − ǫ) 1 + ǫ − ǫ2\nT∑\nt=1\n〈st, ct〉 ≤ T∑\nt=1\n〈 s⋆, ct 〉 + 2 A(d,T)\nǫ +\n6 log(log2(T)/δ)\nǫ .\nWe simplify to a slightly weaker bound,\n(1 − ǫ) (1 + ǫ)\nT∑\nt=1\n〈st, ct〉 ≤ T∑\nt=1\n〈 s⋆, ct 〉 + 2 A(d,T)\nǫ +\n6 log(log2(T)/δ)\nǫ .\nNow setting ǫ = γ/(1 − γ) we arrive at\n(1 − 2γ) T∑\nt=1\n〈st, ct〉 ≤ T∑\nt=1\n〈 s⋆, ct 〉 + 2 A(d,T)\nγ +\n6 log(log2(T)/δ)\nγ .\nFinally reparameterizing with γ′ = 2γwe have\n(1 − γ′) T∑\nt=1\n〈st, ct〉 ≤ T∑\nt=1\n〈 s⋆, ct 〉 + 4A(d,T)\nγ′ +\n12 log(log2(T)/δ)\nγ′ .\nTo get the the high probability efficiency result of Proposition 2 we simply apply this lemma to the individual players, apply the union bound to get a regret statement that holds for all players simultaneously, then finally apply the same smoothness argument as in the expectation case."
    }, {
      "heading" : "A.2 Low Approximate Regret of Specific Algorithms",
      "text" : "In this section, we present the proof of the Low Approximate Regret property for Hedge (Example 1) and Optimistic Hedge (3). The first proof is given mainly for completeness but may be helpful as subsequent proofs follow the same framework. The proof for Optimistic Hedge gives a new analysis that relates the performance of Optimistic Hedge to the performance of hedge. We omit the proof of Example 2 and instead refer the reader to Corollary 2.4 of [CBL06]."
    }, {
      "heading" : "A.2.1 Hedge (Example 1)",
      "text" : "Proposition 12. Hedge with a constant learning rate and uniform prior over actions satisfies the Low Approximate Regret property with A(d,T) = log(d).\nHedge with some constant learning rate η is an algorithm for Online Linear Optimization over the simplex with update rule wt+1 ∝ wte−ηct . We will write this proof of why it satisfies the Low Approximate Regret property for completeness purposes aswewill use variants of it in subsequent proofs.\nHedge can be understood as an instance of Online Mirror Descent with the negative entropy regularizer R(w) = ∑d\ni=1 wi log(wi). It is defined by a two-step update rule at time t given c t:\n1. Let w̃t satisfy ∇R(w̃t+1) = ∇R(wt) − ηct. 2. wt+1 = argmin f∈∆(d) DR( f |w̃t+1)\nHere DR( f |1) = R( f ) − R(1) − 〈∇R(1), f − 1〉 is the Bregman divergence for the specific regularizer.\nUsing the standard mirror descent proof, we have that at every step t, for any f ∈ ∆(d):\n〈wt − f , ct〉 ≤ 〈wt − w̃t+1, ct〉 + 1 η\n( DR( f |wt) −DR( f |wt+1) −DR(wt|w̃t+1) )\n≤ 〈wt − w̃t+1, ct〉 + 1 η\n( DR( f |wt) −DR( f |wt+1) ) . (8)\nFor the first term in the sum above, we can prove:\n〈wt − w̃t+1, ct〉 ≤ η〈wt, ct〉 (9)\nTo prove this note that∇R(w) = log(w)+1 and hence (∇R)−1( f ) = e f−1. This implies that (∇R)−1( f ) = e f−1 which means that we can express w̃t+1 = wte−ηc t .\n〈wt − w̃t+1, ct〉 = ∑\nj∈[d] wtjc t j(1 − e\n−ηct j ) ≤ η\n∑ j∈[d] wtj(c t j) 2 ≤ η〈wt, ct〉, (10)\nwhere the first inequality uses the fact that 1 − e−ηx ≤ ηx for x > 0 and the second inequality uses that the losses lie in [0, 1].\nUsing relations (8) and (9), and summing over t:\n∑\nt\n〈wt − f , ct〉 ≤ η ∑\nt\n〈wt, ct〉 + 1 η DR( f |w1). (11)\nUsing the fact that w1 is the uniform distribution, henceforth, DR( f |w1) ≤ log(d) and rearranging we get the claimed result."
    }, {
      "heading" : "A.2.2 Optimistic Hedge (Example 3)",
      "text" : "Proposition 13. Optimistic Hedge with a constant learning rate η = ǫ/8 < 1/4 satisfies the Low Approximate Regret property with A(d,T) = 8 log(d).\nOptimistic Hedge is described by two update sequences: The first one is the Hedge update: 1t+1 ∝ 1te−ηc t and the second one an alternative that gives more weight to the last step: wt+1 ∝ 1t+1e−ηct . Let the regularizer R and its Bregman divergenceDR be the same as in the proof of Proposition 12.\nLet ∇2R(w) denote the Hessian of the regularizer R. The local norm with respect to w is ‖ f ‖w =√ f T∇2R(w) f and its dual norm is ‖x‖∗w = √ xT(∇2R(w))−1x. We will now introduce a useful lemma due to [RS13a]. Lemma 14. (Lemma 3 in [RS13a]) Optimistic Hedge enjoys for any f ∈ ∆(S)\nT∑\nt=1\n〈wt − f , ct〉 ≤ 2η T∑\nt=1\n(‖ct − ct−1‖∗wt) 2 +\nlog(d)\nη .\nas long as η‖ct − ct−1‖∞ ≤ 1/4 at every step. We will focus on the first term of the RHS in this inequality and prove that for all t,\n(‖ct − ct−1‖∗wt) 2 ≤ 2〈wt, ct〉 + 2〈1t−1, ct−1〉. (12)\nThis holds as\n(‖ct − ct−1‖∗wt) 2 ≤ 2 ( (‖ct‖∗wt) 2 + (‖ct−1‖∗wt) 2 )\n= 2 ( d∑\nj=1\nwtj(c t j) 2 +\nd∑\nj=1\nwtj(c t−1 j )\n2) )\n(13)\n≤ 2 ( 〈wt, ct〉 + 〈wt, ct−1〉 ) (14) = 2 ( 〈wt, ct〉 + 〈1t−1, ct−1〉 + 〈wt − 1t, ct−1〉 + 〈1t − 1t−1, ct−1〉 ) ≤ 2 ( 〈wt, ct〉 + 〈1t−1, ct−1〉 ) . (15)\nHere (13) holds by the definition of the local norm, (14) holds as the costs are in [0, 1] and (15) holds by the Pythagorean theorem for Bregman divergences.\nFrom Lemma 14 and inequality (12), we get, for η < 1/4:\nT∑\nt=1\n〈wt − f , ct〉 ≤ 4η T∑\nt=1\n〈wt, ct〉 + 4η T∑\nt=1\n〈1t−1, ct−1〉 + log(d)\nη . (16)\nObserve now that 1t are the weights selected by the basic Hedge algorithm on the sequence {ct} (setting c0 = 0). Hence by the Low Approximate Regret property for Hedge (Example 1) we have\nT∑\nt=1\n〈wt − f , ct〉 ≤ 4η T∑\nt=1\n〈wt, ct〉 + 4η ( 1 1 − η T∑\nt=1\n〈 f , ct−1〉 + log(d) η(1 − η) ) + log(d) η .\nRearranging,\n(1 − 4η) T∑\nt=1\n〈wt, ct〉 ≤ 1 + 3η 1 − η ( T∑\nt=1\n〈 f , ct〉 + log(d)\nη\n) .\nThis gives the claimed bound as 1 + 3η ≤ 11−3η for η ≤ 1/3 and 1 − 8η ≤ (1 − 4η)(1 − η)(1 − 3η)."
    }, {
      "heading" : "A.3 Proof of Theorem 3",
      "text" : "This theorem follows from Propositions 1 and 2. Robustness against adversary holds as 1) we can\nset ǫ =\n√ log(d)\nT since the Strong LowApproximate Regret property implies the property for all ǫ > 0\nand 2) ∑\nt〈wti , c t i 〉 ≤ T (the losses are in [0, 1])."
    }, {
      "heading" : "B Proofs from Section 4",
      "text" : ""
    }, {
      "heading" : "B.1 Proof of Lemma 5",
      "text" : "The proof follows a common scheme for proving regret bounds in partial information settings (see [AHR08] for a nice discussion): We first provide a regret bound for the algorithm in the full information setting, then for the partial information settingwe run the algorithmusing anunbiased estimator ĉt for the cost. An unbiased estimator ĉt satisfies Est∼wt [ ĉt ] = ct, where st is the action selected by the algorithm at step t. We use the well-known importance-weighted estimator: ĉt i (st) = ct i\nwt i · est , where st is the action selected by the algorithm and ei denotes the ith standard basis vector."
    }, {
      "heading" : "Regret Bound with Estimated Costs",
      "text" : "We now show a regret bound for Mirror Descent with the log-barrier regularizer when this algorithm is run using our unbiased estimator for the cost. We will first show that, for any fixed comparator f ∗:\nT∑\nt=1\n〈wt − f ∗, ĉt〉 ≤ η\n1 − η\nT∑\nt=1\n(ctst) 2 +\n1 η DR\n( f ∗ |w1 ) (17)\nNote that the first three lines of our bound hold generically for any cost, and the last three are specialized to our estimator. Starting from the standard Mirror Descent bound we have that for any t:\n〈wt − f ∗, ĉt〉 ≤ 〈wt − w̃t+1, ĉt〉 + 1 η\n( DR ( f ∗ |wt ) −DR ( f ∗ |wt+1 ))\n≤ ‖wt − w̃t+1‖wt‖ĉt‖∗wt + 1\nη\n( DR ( f ∗ |wt ) −DR ( f ∗|wt+1 ))\n=\n√√ d∑\nj=1\n(wt j − w̃t+1 j )2\n(wt j )2\n·\n√√ d∑\nj=1\n(wt j )2(ĉt j )2 +\n1\nη\n( DR ( f ∗|wt ) −DR ( f ∗ |wt+1 ))\n= η\n√ (ctst) 2\n(1 − ηct st )2\n·\n√√ d∑\nj=1\n(wt j ĉt j )2 +\n1\nη\n( DR ( f ∗ |wt ) −DR ( f ∗|wt+1 ))\n= η\n√ (ct\nst )2\n(1 − ηct st )2\n· √ (ct\nst )2 +\n1\nη\n( DR ( f ∗|wt ) −DR ( f ∗|wt+1 ))\n≤ η\n1 − η (c t st)\n2 + 1\nη\n( DR ( f ∗|wt ) −DR ( f ∗|wt+1 )) ,\nwhere in the second inequality we are using Holder’s inequality with local norm as defined in Appendix A.2.2. In the third inequality we simply use form of the local norm for log barrier function. In the next equality we use the update form for mirror descent which for this case yields\nthat 1 − w̃t j /wt i j = 0 if j , st and 1 − w̃t j /wt j = −ηct st /(1 − ηct st ) when j = st. Subsequent inequalities use the form of unbiased estimate ĉt st mentioned earlier and the fact that |ct st | ≤ 1. Summing then over all t and by the non-negativity of the Bregman divergence, we get inequality (17)."
    }, {
      "heading" : "From Full Information to Partial Information",
      "text" : "Next note that,\nE [ 〈est − f ∗, ct〉 ] = 〈wt − f ∗, ct〉 = 〈wt − f ∗,E [ ĉt ] 〉 = E [ 〈wt − f ∗, ĉt〉 ] .\nBy Inequalities (17) and the above inequality, we get:\nE   T∑\nt=1\n〈est − f ∗, ct〉   ≤ η 1−η E [∑T t=1(c t st )2 ] + 1ηDR ( f ∗ |w1 ) . (18)\nWhat is left is to bound the Bregman divergence term. Unfortunately,DR( f |w1) can be unbounded for some f although w1 is the uniform distribution. To get our final bound against f , we will use an auxiliary f ∗ ∈ ∆(d) such that f ∗ = (1 − ∆) f + ∆π where π is the uniform distribution. This has the advantage of having at least ∆/dweight at each expert and hence,\nDR ( f ∗ |w1 ) ≤ d log(1/∆) (19)\nBy Inequalities (18) and (19), we get:\nE   ∑\nt\n〈est , ct〉   − ∑\nt\n〈 f ∗, ct〉 ≤ η\n1 − η Est∼wt\n  ∑\nt\n(ct st )2   + d log(1/∆)\nη (20)\nCombining Inequality (20) and the fact that the costs are in [0, 1], we get:\nE   ∑\nt\n〈est , ct〉   − ∑\nt\n〈 f , ct〉 = E   ∑\nt\n〈est , ct〉   − ∑\nt\n〈 f ∗, ct〉 + ∑\nt\n〈 f ∗ − f , ct〉\n≤ η 1 − η E   ∑\nt\nctst   + d log(1/∆) η + ∑\nt\n〈 f ∗ − f , ct〉\n= η 1 − η E   ∑\nt\n〈est , ct〉   + d log(1/∆) η + ∑\nt\n〈∆(π − f ), ct〉\n≤ η 1 − η E   ∑\nt\n〈est , ct〉   + d log(1/∆)\nη + ∆\nT∑\nt=1\n‖ct‖∞.\nSetting ∆ = d/T and noting that ‖ct‖∞ ≤ 1 we have\nE   ∑\nt\n〈est , ct〉   − ∑\nt\n〈 f , ct〉 ≤ η 1 − η E   ∑\nt\n〈est , ct〉   + d log(T/d)\nη + d.\nNow to prove the theorem, first use η = ǫ/(1 + ǫ) and we notice that the Low approximate rerget property is satified by our bandit algorithm as:\n(1 − ǫ)E   ∑\nt\n〈est , ct〉   ≤ ∑\nt\n〈 f , ct〉 + d(1 + ǫ) log(T/d)\nǫ + d.\nThis concludes the proof of Lemma 5."
    }, {
      "heading" : "C Proofs from Section 5",
      "text" : ""
    }, {
      "heading" : "C.1 Proof of Theorem 7",
      "text" : "Here we sketch the proof of Theorem 7, which is analogous to the proof of Proposition 1.\nProof. Recall that s∗1:T is a solution sequence with cost at most ρ times the minimum cost, and Ki denotes the number of changes of strategies in player i in this sequence.\n(1 − ǫ) ∑\nt\nE [ C(st) ] = (1 − ǫ) ∑\ni\n∑\nt\nE [ costi(s t) ]\n≤ ∑\ni\n  ∑\nt\nE [ costi(s ∗t i , s t −i) ] + 1 + E[Ki]\nǫ A(d,T)\n \n≤ ∑\nt\n(λE [ C(s∗t) ] + µE [ C(st) ] ) +\nn + E [∑ i Ki ]\nǫ A(d,T).\nwhere the second inequality is using the smoothness property, the first inequality is by Low Approximate Regret algorithm satisfying (2) for each player i against the shifting expert of s∗1:T\ni and\ntaking expectation over the randomness in s∗1:T i\ndue to players turning over or due to randomness in the approximate minimization algorithm.\nThe claimed bound follows by rearranging terms."
    }, {
      "heading" : "C.2 Proof of Proposition 8",
      "text" : "NoisyHedge is a slightmodificationofHedge thatmixes theHedgedistributionwith small uniform noise at each step. Let 1t+1 ∝ 1te−ηct . Then the weight at each step t is wt+1 ∝ (1− θ)1t+1 + θπwhere π is the uniform distribution and θ ∈ [0, 1]. The rest of the notation will be identical as in the proof of Low Approximate Regret for Hedge in Appendix A.2.1\nWe will follow a similar approach to this proof. Similarly to equation (8) at each step for any f t ∈ ∆(d) and using that wt = (1 − θ)1t + θπ:\n〈wt − f t, ct〉 = 〈1t − w̃t+1, ct〉 + 〈wt − 1t, ct〉 + 〈w̃t+1 − f t, ct〉. (21)\nFor the first term, we already have, from the Hedge proof, a bound of\n〈1t − w̃t+1, ct〉 ≤ η〈wt, ct〉. (22)\nFor the second term, given that wt and 1t are probability distributions, we have that:\n〈wt − 1t, ct〉 ≤ θ〈π − 1t, ct〉 ≤ θ‖ct‖∞. (23)\nFor the third term, we will use again the inequality:\n〈w̃t+1 − f t, ct〉 ≤ 1 η\n( DR( f t|wt) −DR( f t|wt+1) ) .\nSumming over all t, we have:\nT∑\nt=1\n〈wt − f t, ct〉 = η ∑\nt\n〈wt, ct〉 + θT‖ct‖∞ + 1\nη\nT∑\nt=1\n( DR( f t|wt) −DR( f t|wt+1) ) .\nObserve that the latter term is a telescopic sum where subsequent steps with the same comparator cancel. Hence, letting the comparator sequence be f ∗\n1 , · · · , f ∗ Ki and t( j) be the step at which the j-th\nchange occurs, the last term can be upper bounded as:\n1\nη\nT∑\nt=1\n( DR( f t|wt) −DR( f t|wt+1) ) ≤ DR( f ∗1 |w 1) + Ki+1∑\nj=2\n( DR( f ∗ j |w t( j)) −DR( f ∗j−1|w t( j)) ) . (24)\nMixing the uniform distribution into the prediction distribution guarantees that we can bound the differences in Bregman divergences at steps where changes occur. By Lemma 15, the difference is bounded as\nDR ( f ∗j |w t( j) ) −DR( f ∗j−1|w\nt( j)) ≤ log(d/θ) + 1. (25) Combining inequalities (24), (25), and the fact that DR( f ∗ 1 ) ≤ log(d),\n1\nη\nT∑\nt=1\n( DR( f t|wt) −DR( f t|wt+1) ) ≤ log(d) + Ki ( log(d/θ)+ 1 ) . (26)\nCombining inequalities (21), (22), (23), (26), and setting θ = 1/T, the result follows. Lemma 15. Assume that R is the negative entropy regularizer (so that DR is the KL divergence). Let r ∈ ∆(d) and let r̂ = θr + (1 − θ)π for θ ∈ [0, 1]. Then for all p, q ∈ ∆(d),\n|DR(p | r̂) −DR(q | r̂)| ≤ ( log ( d\nθ\n) + 1 ) ‖p − q‖1.\nProof. Since DR(·) is convex in its first argument we have\nDR(p | r̂) −DR(q | r̂) ≤ 〈 ∇qDR(q | r̂), p − q 〉 ≤ ‖∇qDR(q | r̂)‖∞‖p − q‖1.\nso it remains to bound the gradient’s norm. For each j ∈ [d] we have\n∇q jDR(q | r̂) = log(qi/r̂ j) + 1 = log (\nq j\n(1 − θ)r̂ + θ/d\n) + 1 ≤ log(d/θ) + 1,\nwhich establishes the result."
    }, {
      "heading" : "D Utility Maximization Games and Mechanisms",
      "text" : "In this section, we show how all our results extend to utility maximization games andmechanisms.\nConsider a static game G among a set of n players. Each player i has an action space Si and a utility function utilityi : S1 × · · · × Sn → [0, 1] that maps an action profile s = (s1, . . . , sn) to a utility utilityi(s). The goal of each player is to maximize their utility. Roughly speaking we can think of utility as a negative cost, an can adapt the definition is smoothness and low approximate regret to have essentially all our results go through.\nSimilarly as in the cost minimization setting, we will assume that at each time step t, player i picks a probability distribution wt\ni and draws her acion st i from this distribution. The utility she is\nreceiving when playing action x is ut i,x = utilityi(x, s t −i) where s t −i is the set of strategies of all but i th player. Let ut i = (ut i,x )x∈Si .\nAn important class of utility maximization games are mechanisms, such as auctions, where money plays special role. The players’ actions si typically involve bidding on items, and the outcome of an action profile s comes in two parts: vi : S1 × · · · × Sn → [0, 1], which is the resulting value for player i, and pi : S1 × · · · × Sn → [0, 1], which is the price player i has to pay. Her utility is then utilityi(s) = vi(s) − pi(s). We evaluate such mechanisms via the notion of social welfare SW(s) = ∑ i vi(s), the sum of the utilities of the players plus all the payments, which is the revenue of the mechanism. A simple example of such a mechanism is the first price auction: The player’s strategy is a bid, the highest bidder wins the item, and pays her own bid.\nWe use the the smooth mechanism definition of [ST13].2 Definition 3. (Smooth mechanism [ST13]) A utility maximization mechanism is called (λ, µ)-smooth if there exists a strategy profile s∗, such that for all strategy profiles s: ∑ i ui(s ∗ i , s−i) ≥ λOpt−µ ∑ i pi(s), where\nOpt = maxso ∑n i=1 utilityi(s o).\nRecall from section 2 that first price item auctions are (1−1/e, 1)-smooth, all-pay actions are (1/2, 1)- smooth, and smooth mechanisms have a price of anarchy of at most max(µ, 1)/λ, as we will show in Proposition 16. Remark 1. Note the slight difference in the definition to the one we used for smoothness in games. In proving the price of anarchy property we used the game smoothness property with s∗ being the action profile resulting in Opt total cost. In the above definition for mechanisms, we do not insist that SW(s∗) = Opt.\nThe natural extension of the low approximate regret property for utility maximization is the following: Definition 4. (Low Approximate Regret for utility maximization) A learning algorithm that uses action distributions wt\ni in step t satisfies the LowApproximateRegret property\nfor a parameter ǫ, and a function A if for all action distributions f ∈ ∆(Si):\n(1 + ǫ)\nT∑\nt=1\n〈wti , u t i〉 ≥\nT∑\nt=1\n〈 f , uti〉 − A(d,T)\nǫ . (27)\n2For the dynamic population game setting, we use a variant of this definition, solution-based smoothnesswhere Opt in the RHS is replaced by the social welfare of a near-optimal solution as in [LST16].\nAnalgorithm satisfies LowApproximateRegret for the shifting experts setting if for all sequences f 1, . . . , f T ∈ ∆(Si), letting K be the number of shifts, i.e. K = |{t > 2 : f t−1 , f t}|:\n(1 + ǫ)\nT∑\nt=1\n〈wti , u t i〉 ≥\nT∑\nt=1\n〈 f t, uti〉 − (1 + K) A(d,T)\nǫ . (28)\nWe say that an algorithm satisfies the strong Low Approximate Regret property if it satisfies (27) or (28) for all ǫ > 0 simultaneously. In the bandit feedback case, we require the property to hold in expectations over the realized strategies of player i.\nNow we are ready to prove the utility maximization analog of Proposition 1 Proposition 16. (Mechanisms PoA in expectation) Consider a (λ, µ)-smooth mechanism. If all players use Low Approximate Regret algorithms satisfying Eq. (27) for some parameter ǫ, then\n1\nT\n∑\nt\nE [ SW(st) ] ≥ λ\nmax(µ, 1 + ǫ) Opt +\nn T · 1 max(µ, 1 + ǫ) · A(d,T) ǫ .\nwhere st is the action profile drawn on round t from the corresponding mixed actions of the players.\nProof. We get the claimed bound by considering (1 + ǫ) ∑\ntE [∑ i utilityi(s t) ] ; using the low approx-\nimate regret property with f = s∗ i for each player i for the action s∗in the smoothness property;\nthen using the smoothness property for each time t to bound ∑\ni utilityi(s ∗ i , st−i), and rearranging\nterms.\nFollowing, we show how we can get the Low Approximate Regret property for Hedge. Proposition 17. Hedge with a constant learning rate and uniform prior over actions satisfies the utility version of the Low Approximate Regret property with A(d,T) = (e − 1) log(d). We mirror the proof of Proposition 12 with ct = −ut. The only place where the analysis does not automatically go through is where we need that the costs are in [0, 1], namely relation 10. Note that the first inequality there ceases to hold when ct < 0. However it is still the case that 1 − e−ηx ≤ (e − 1)ηx for x ∈ [−1, 0]. Hence we have:\n〈wt − w̃t+1, ct〉 ≤ η(e − 1) ∑\nj∈[d] wtj(c t j) 2 ≤ η(e − 1)\n∑ j∈[d] wtj(−c t j).\nThe last inequality holds as −ct j ∈ [0, 1]. From this, combined with the rest of the proof in Proposition 12, we have: ∑\nt〈wt − f , ct〉 ≤ η(e − 1) ∑ t〈wt,−ct〉 + log(d) η . Setting ǫ = η(e − 1) and substituting ct yields\n(1 + ǫ)\nT∑\nt=1\n〈wt, ut〉 ≥ T∑\nt=1\n〈 f , ut〉 − (e − 1) log(d)\nǫ .\nwhich proves the claim.\nThe same idea can be applied in all the other proofs we have and one can verify that all our results for cost minimization games extend to utility maximization mechanisms."
    } ],
    "references" : [ {
      "title" : "Regret bounds and minimax policies under partial monitoring",
      "author" : [ "Jean-Yves Audibert", "Sébastien Bubeck" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Audibert and Bubeck.,? \\Q2010\\E",
      "shortCiteRegEx" : "Audibert and Bubeck.",
      "year" : 2010
    }, {
      "title" : "Adaptive and self-confident on-line learning algorithms",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Claudio Gentile" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Competing in the dark: An efficient algorithm for bandit linear optimization",
      "author" : [ "Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin" ],
      "venue" : "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2008
    }, {
      "title" : "High-probability regret bounds for bandit online linear optimization",
      "author" : [ "Peter L Bartlett", "Varsha Dani", "Thomas Hayes", "Sham Kakade", "Alexander Rakhlin", "Ambuj Tewari" ],
      "venue" : "In Proceedings of 21st Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2008
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolo Cesa-Bianchi", "Gabor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "Improved second-order bounds for prediction with expert advice",
      "author" : [ "Nicolo Cesa-Bianchi", "Yishay Mansour", "Gilles Stoltz" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2007
    }, {
      "title" : "The price of anarchy of finite congestion games",
      "author" : [ "Giorgos Christodoulou", "Elias Koutsoupias" ],
      "venue" : "In Proceedings of the 37th Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Christodoulou and Koutsoupias.,? \\Q2005\\E",
      "shortCiteRegEx" : "Christodoulou and Koutsoupias.",
      "year" : 2005
    }, {
      "title" : "Near-optimal no-regret algorithms for zero-sum games",
      "author" : [ "Constantinos Daskalakis", "Alan Deckelbaum", "Anthony Kim" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "Daskalakis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Daskalakis et al\\.",
      "year" : 2015
    }, {
      "title" : "Adaptive online learning",
      "author" : [ "Dylan J Foster", "Alexander Rakhlin", "Karthik Sridharan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Foster et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Foster et al\\.",
      "year" : 2015
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E Schapire" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Freund and Schapire.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1997
    }, {
      "title" : "Introduction to Online Convex Optimization",
      "author" : [ "Elad Hazan" ],
      "venue" : "Foundations and Trends in Optimization,",
      "citeRegEx" : "Hazan.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hazan.",
      "year" : 2016
    }, {
      "title" : "Extracting certainty from uncertainty: Regret bounded by variation in costs",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2010
    }, {
      "title" : "Efficient learning algorithms for changing environments",
      "author" : [ "Elad Hazan", "C. Seshadhri" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Hazan and Seshadhri.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hazan and Seshadhri.",
      "year" : 2009
    }, {
      "title" : "Worst-case equilibria",
      "author" : [ "Elias Koutsoupias", "Christos Papadimitriou" ],
      "venue" : "Computer science review,",
      "citeRegEx" : "Koutsoupias and Papadimitriou.,? \\Q2009\\E",
      "shortCiteRegEx" : "Koutsoupias and Papadimitriou.",
      "year" : 2009
    }, {
      "title" : "Second-order quantile methods for experts and combinatorial games",
      "author" : [ "Wouter M Koolen", "Tim Van Erven" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory (COLT),",
      "citeRegEx" : "Koolen and Erven.,? \\Q2015\\E",
      "shortCiteRegEx" : "Koolen and Erven.",
      "year" : 2015
    }, {
      "title" : "Achieving all with no parameters: Adanormalhedge",
      "author" : [ "Haipeng Luo", "Robert E Schapire" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory (COLT),",
      "citeRegEx" : "Luo and Schapire.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luo and Schapire.",
      "year" : 2015
    }, {
      "title" : "Learning and efficiency in games with dynamic population",
      "author" : [ "Thodoris Lykouris", "Vasilis Syrgkanis", "Éva Tardos" ],
      "venue" : "InProceedings of the Twenty-SeventhAnnualACM-SIAMSymposium onDiscrete Algorithms (SODA),",
      "citeRegEx" : "Lykouris et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lykouris et al\\.",
      "year" : 2016
    }, {
      "title" : "First-order regret bounds for combinatorial semi-bandits",
      "author" : [ "Gergely Neu" ],
      "venue" : "In Proceedings of the 27th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Neu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neu.",
      "year" : 2015
    }, {
      "title" : "Intrinsic robustness of the price of anarchy",
      "author" : [ "Tim Roughgarden" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Roughgarden.,? \\Q2015\\E",
      "shortCiteRegEx" : "Roughgarden.",
      "year" : 2015
    }, {
      "title" : "Online learning with predictable sequences",
      "author" : [ "Alexander Rakhlin", "Karthik Sridharan" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "Rakhlin and Sridharan.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rakhlin and Sridharan.",
      "year" : 2013
    }, {
      "title" : "Optimization, learning, and games with predictable sequences",
      "author" : [ "Alexander Rakhlin", "Karthik Sridharan" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Rakhlin and Sridharan.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rakhlin and Sridharan.",
      "year" : 2013
    }, {
      "title" : "How bad is selfish routing",
      "author" : [ "Tim Roughgarden", "Eva Tardos" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Roughgarden and Tardos.,? \\Q2002\\E",
      "shortCiteRegEx" : "Roughgarden and Tardos.",
      "year" : 2002
    }, {
      "title" : "Fast convergence of regularized learning in games",
      "author" : [ "Vasilis Syrgkanis", "Alekh Agarwal", "Haipeng Luo", "Robert E Schapire" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Syrgkanis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Syrgkanis et al\\.",
      "year" : 2015
    }, {
      "title" : "Adaptivity and optimism: An improvedexponentiated gradient algorithm",
      "author" : [ "Jacob Steinhardt", "PercyLiang" ],
      "venue" : "In Proceedings of the 31st International Conference onMachine Learning (ICML),",
      "citeRegEx" : "Steinhardt and PercyLiang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Steinhardt and PercyLiang.",
      "year" : 2014
    }, {
      "title" : "Composable and efficient mechanisms",
      "author" : [ "Vasilis Syrgkanis", "Éva Tardos" ],
      "venue" : "In ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Syrgkanis and Tardos.,? \\Q2013\\E",
      "shortCiteRegEx" : "Syrgkanis and Tardos.",
      "year" : 2013
    }, {
      "title" : "Incomplete information and internal regret in prediction of individual sequences",
      "author" : [ "Gilles Stoltz" ],
      "venue" : "PhD thesis, Universite Paris-Sud,",
      "citeRegEx" : "Stoltz.,? \\Q2005\\E",
      "shortCiteRegEx" : "Stoltz.",
      "year" : 2005
    }, {
      "title" : "How to better use expert advice",
      "author" : [ "Rani Yaroshinsky", "Ran El-Yaniv", "Steven S. Seiden" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Yaroshinsky et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Yaroshinsky et al\\.",
      "year" : 2004
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We show that learning algorithms satisfying a low approximate regret property experience fast convergence to approximate optimality in a large class of repeated games. Our property, which simply requires that each learner has small regret compared to a (1 + ǫ)-multiplicative approximation to the best action in hindsight, is ubiquitous among learning algorithms — it is satisfied evenby the vanillaHedge forecaster. Our results improveupon recentwork of Syrgkanis et al. [SALS15] in a number of ways. We improve upon the speed of convergence by a factor of n, the number of players, and require only that the players observe payoffs under other players’ realized actions, as opposed to expected payoffs. We further show that convergence occurs with high probability, and under certain conditions show convergence under bandit feedback. Both the scope of settings and the class of algorithms for which our analysis provides fast convergence are considerably broader than in previous work. Our framework applies to dynamic population games via a low approximate regret property for shifting experts. Here we strengthen the results of [LST16] in two ways: We allow players to select learning algorithms from a larger class, which includes a minor variant of the basic Hedge algorithm, and we increase the maximum churn in players for which approximate optimality is achieved. In the bandit setting we present a novel algorithm which provides a “small loss”-type bound with improved dependence on the number of actions and is both simple and efficient. This result may be of independent interest. ∗Cornell University, djfoster@cs.cornell.edu. Work supported under NSF grant CDS&E-MSS 1521544. †Tsinghua University, lizhiyuan13@mails.tsinghua.edu.cn. Research performed while author was visiting Cornell University. ‡Cornell University, teddlyk@cs.cornell.edu. Work supported under ONR grant N00014-08-1-0031, and a Google faculty research award. §Cornell University, sridharan@cs.cornell.edu. Work supported by NSF grant CDS&E-MSS 1521544. ¶Cornell University, eva@cs.cornell.edu. Work supported in part by NSF grant CCF-1563714, ONR grant N00014-081-0031, and a Google faculty research award.",
    "creator" : "LaTeX with hyperref package"
  }
}