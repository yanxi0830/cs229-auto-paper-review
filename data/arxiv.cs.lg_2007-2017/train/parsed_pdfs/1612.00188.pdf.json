{
  "name" : "1612.00188.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Efficient Orthogonal Parametrisation of Recurrent Neural Networks  Using Householder Reflections",
    "authors" : [ "Zakaria Mhammedi", "Andrew Hellicar", "Ashfaqur Rahman" ],
    "emails" : [ "ZMHAMMEDI@STUDENT.UNIMELB.EDU.AU", "ANDREW.HELLICAR@DATA61.CSIRO.AU", "ASHFAQUR.RAHMAN@DATA61.CSIRO.AU", "JAMES.BAILEY@UNIMELB.EDU.AU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "†The Department of Computing and Information Systems. ‡Data61.\nparametrisation on problems with long-term dependencies. Our results suggest that the orthogonal constraint on the transition matrix has similar benefits to the unitary constraint."
    }, {
      "heading" : "1. Introduction",
      "text" : "Recurrent Neural Networks (RNNs) have been successfully used in many applications involving time series. This is because RNNs are well suited for sequential data as they process inputs one element at a time and store relevant information in their hidden state. In practice, however, training simple RNNs can be challenging due to the problem of exploding and vanishing gradients (Hochreiter et al., 2001). It has been shown that the exploding gradient problem is likely to occur when the transition matrix of an RNN has a spectral norm larger than one (Glorot & Bengio, 2010). This results in an error surface, associated with some objective function, having very steep walls (Pascanu et al., 2013). Landing on one of these walls during training will result in a very large gradient step, which can disrupt training. On the other hand, when the spectral norm of the transition matrix is less than one, the information at one time step tend to vanish quickly after a few time steps. This makes it challenging to learn long-term dependencies in sequential data.\nDifferent methods have been suggested to solve either the vanishing or exploding gradient problem. The LSTM has been specifically designed to help with the vanishing gradient (Hochreiter & Schmidhuber, 1997). This is achieved by using gate vectors which allow a linear flow of information through the hidden state. However, the LSTM does not di-\nar X\niv :1\n61 2.\n00 18\n8v 1\n[ cs\n.L G\n] 1\nrectly address the exploding gradient problem. In fact, the longer the input sequence is, the more prone the LSTM is to the exploding gradient. One approach to solving this issue is to clip the gradients (Pascanu et al., 2013) when their norm exceeds some threshold value. This has the negative side effect of adding an additional hyperparameter to the model. Furthermore, if the exploding gradient occurs within some parameter search space, the associated error surface will still have steep walls. This makes training challenging even with gradient clipping.\nAnother way to approach this problem is to improve the shape of the error surface directly by making it smoother, which can be achieved by constraining the spectral norm of the transition matrix to be less than or equal to one. However, a value of exactly one is best for the vanishing gradient problem. A good choice of the activation function between hidden states is also crucial in this case. These ideas have been investigated in recent work. In particular, the unitary RNN (Arjovsky et al., 2015) constrains the transition matrix to be in the unitary group U(n) using a special parametrisation. This ensures that the norm of the transition matrix is exactly equal to one. This parametrisation and other similar ones (Hyland & Rätsch, 2016; Wisdom et al., 2016) have some advantages and drawbacks which we will discuss in more details in the next section.\nThe main contributions of this work are as follows:\n• We first show that, constraining the search space of the transition matrix of an RNN to the unitary group U(n) is equivalent to limiting the search space to a subset of the orthogonal group O(2n) of a new RNN with twice the hidden size. This suggests that it may not be necessary to work with complex valued matrices.\n• We present a simple and efficient way to parametrise orthogonal transition matrices. We show that the time and space complexities of using this parametrisation are, in the worse case, the same as those of the simple RNN. This new parametrisation can be used in other types of neural networks, not just the recurrent ones."
    }, {
      "heading" : "2. Related Work",
      "text" : "Throughout this work we will refer to elements of the following simple RNN architecture.\nht = φ(Wht−1 + V xt), (1) ot = Y ht, (2)\nwhere W , V and Y are the hidden-to-hidden, input-tohidden, and hidden-to-output weight matrices. ht−1 and ht are the hidden vectors at time steps t − 1 and t respectively. Finally, φ is a non-linear activation function. We have omitted the bias terms for simplicity.\nWhen looking at the structure of the transition matrix W , recent research has explored how their initialisation influences training and the ability to learn long-term dependencies. In particular, initialising with specific structures such as the identity or an orthogonal matrix can greatly improve performance (Le et al., 2015). In addition to these initialisation methods, one study has also considered removing the non-linearity between the hidden-to-hidden connections (Henaff et al., 2016), i.e. the termWht−1 in Equation 1 is outside the activation function φ. This method showed good results when compared to the LSTM on pathologically synthetic problems exhibiting long-term dependencies.\nAfter training a model for a few iterations using gradient descent, nothing guarantees that the initial structures of the transition matrix will be kept. In fact, its spectral norm can deviate from one, and the exploding and vanishing gradients can be a problem again. It is possible to constrain the transition matrix to be orthogonal during training using more sophisticated methods (Wisdom et al., 2016). However, this comes at the cost of time-complexity. A naive approach is to use gradient descent and project the updated transition matrix onto the orthogonal group using the QR decomposition for example. This incurs a O(n3) cost at each iteration, where n is the size of the hidden layer. This can be an issue when the batch size and the sequence length T are small compared to n.\nRecently, new parametrisations of the transition matrix have been suggested (Arjovsky et al., 2015), which ensure that its spectral norm is always equal to one. This condition is sufficient to ensure that exploding gradients do not occur. Methods such as gradient clipping are made unnecessary. The unitary RNN (uRNN) is one example where the hidden matrix W ∈ Cn×n is the product of elementary matrices, consisting of reflection, diagonal, and Fourier transform matrices. When the size of hidden layer is equal to n, the transition matrix has a total of only 7n parameters. Another advantage of this parametrisation is computational efficiency. In particular, the matrix-vector product Wv, for some vector v, can be calculated in time complexity O(n log n). However, it has been shown that this parametrisation does not allow the transition matrix to span the full unitary group (Wisdom et al., 2016), which may limit the model expressiveness.\nAnother interesting parametrisation (Hyland & Rätsch, 2016) has been suggested which takes advantage of the algebraic properties of the unitary group U(n). The idea is to use the corresponding matrix Lie algebra u(n) of skew hermitian matrices. In particular, the transition matrix can\nbe written as follows:\nW = exp  n2∑ i=1 λiTi  , (3) where exp is the exponential matrix map and {Ti}n 2\ni=1 are predefined n × n matrices forming a bases of the Lie algebra u(n). The learning parameters are the weights {λi}. The fact that the matrix Lie algebra u(n) is closed and connected ensures that the exponential mapping from u(n) to U(n) is surjective. Therefore, with this parametrisation the search space of the transition matrix spans the whole unitary group. This is one advantage over the original unitary parametrisation (Arjovsky et al., 2015). However, the cost of computing the matrix exponential in Equation (3) is O(n3), where n is the dimension of the hidden state. Evaluating ∑n2 i=1 λiTi can also be computationally expensive.\nA more recent method (Wisdom et al., 2016) performs optimisation directly of the Stiefel manifold. The corresponding model was called full-capacity unitary RNN. Using this approach, the transition matrix can span the full unitary group. However, this method involves a matrix inverse as well as matrix-matrix products which have time complexity O(n3). This can be problematic when the size of the hidden layer is large. This method is somewhat similar to the projection approach discussed earlier.\nAll the methods discussed above, except for the original unitary RNN, involve a step that requires at least a O(n3) time complexity. Furthermore, they all require the use of complex matrices. Table 2 summarises the time complexities of various methods, including our approach, for one online gradient step. In the next section, we show that imposing a unitary constraint on a transition matrix W ∈ Cn×n is equivalent to imposing a constraint in a subset of the orthogonal group O(2n) on a new RNN with twice the hidden size. Furthermore, since the norm of orthogonal matrices is also always equal to one, using the latter has the same theoretical advantages as using unitary matrices when it comes to exploding gradient problem."
    }, {
      "heading" : "3. Complex unitary versus orthogonal",
      "text" : "In is possible to show that when the transition matrix W ∈ Cn×n of an RNN is unitary, there exists an equivalent representation of this RNN with an orthogonal transition matrix Ŵ ∈ R2n×2n.\nIn fact, consider a complex unitary transition matrix W = A + iB ∈ Cn×n, where A and B are now real valued matrices in Rn×n. Furthermore, consider the following new variables\n∀t, ĥt = [ < (ht) = (ht) ] , V̂ = [ < (V ) = (V ) ] , Ŵ = [ A −B B A ] ,\nwhere < and = denote the real and imaginary parts of a complex number. Note that ĥt ∈ R2n, Ŵ ∈ R2n×2n, V̂ ∈ R2n×nx , where nx is the dimension of the input vector xt in Equation (1).\nAssuming that the activation function φ applies to the real and imaginary parts separately, it is easy to show that the update equation of the complex hidden state ht of the unitary RNN can be written in the real space as follows\nĥt = φ(Ŵ ĥt−1 + V̂ xt). (4)\nEven when the activation function φ does not apply to the real and imaginary parts separately, it is still possible to find an equivalent real space representation. Take for example the activation function proposed by (Arjovsky et al., 2015)\nσmodRelU(z) = { (|z|+ b) z|z| , if |z|+ b > 0\n0, otherwise (5)\nwhere b is a bias vector. For a hidden state ĥ ∈ R2n, the equivalent activation function in the real space representation (4) is given by\nφ(ĥi) =  √ ĥ2i+ĥ 2 ki +bk′ i√ ĥ2i+ĥ 2 ki ĥi, if √ ĥ2i + ĥ 2 ki + bk′i > 0\n0, otherwise (6)\nwhere ki = (i + n) mod 2n and k′i = i mod n for all i ∈ {1, . . . , 2n}. This activation is no longer applied to hidden units independently.\nNow we will show that the matrix Ŵ is orthogonal. By definition of a unitary matrix, we have WW ∗ = I where the ∗ symbol indicates the complex conjugate. This equation implies that\nA2 +B2 = I, (7) BA−AB = 0. (8)\nOn the other hand, we have\nŴŴT = [ A2 +B2 AB −BA BA−AB B2 +A2 ] . (9)\nUsing Equations (7) and (8), it follows that ŴŴT = I . Also note that the matrix W has a special structure. It is a block-matrix and its blocks are the matricesA,−B, A, and B.\nThe discussion above shows that using a complex unitary matrix in Cn×n is equivalent to using an orthogonal matrix, belonging to a subset of O(2n), in a new RNN with twice the hidden size. This is why in this work we focus on parametrising orthogonal matrices directly.\nIn the next section, we present an efficient, and flexible parametrisation of the transition matrix which allows it to span the full orthogonal group."
    }, {
      "heading" : "4. Parametrisation of the transition matrix",
      "text" : "Before discussing the details of our parametrisation, we need to introduce a few notations.\n• For n ∈ N and 2 ≤ k ≤ n, we define the mappings\nHk : Rk → Rn×n u 7→ [ In−k 0 0 Ik − 2uuT ] ,\n(10)\nwhere Ik denotes the k-dimensional identity matrix. Note that, when ‖v‖ = 1, Hk(u) is simply the Householder Reflection in Rn×n, which is orthogonal.\n• For the special case of k = 1, we define H1 as\nH1 : R→ Rn×n u 7→ [ In−1 0 0 u ] ,\n(11)\nwhere u is simply a scalar in this case. Note that, when u ∈ {1,−1}, H1(u) is orthogonal.\nWe propose to parametrise the transition matrix of an RNN using the vectors {ui}, which we will refer to as the reflection vectors in what follows. This parametrisation has many advantage as the following theorem will help us show.\nTheorem 1. Let n ∈ N and 1 ≤ k ≤ n we define the mappingMk as follows\nMk : Rk × · · · × Rn → Rn×n\n(uk, . . . , un) 7→ Hn(un) . . . Hk(uk),\nwhere the functions {Hk}nk=1 are defined in Equations (10) and (11). Then we have,\n• The mappingsMk are smooth for all 1 ≤ k ≤ n.\n• The image of M1 includes the orthogonal group O(n), i.e.\nO(n) ⊂M1 [R× · · · × Rn] .\nSketch of the proof. The first point of Theorem 1 is straightforward. The mapping Mk is the product of n − k + 1 smooth maps. Therefore,Mk is also a smooth map.\nNow we need to the show that for every Q′ ∈ O(n), there exits a tuple of vectors (u1, . . . , un) ∈ R × · · · × Rn such that Q′ = M1(u1, . . . , un). Algorithm 1 show how a QR decomposition can be performed using the matrices {Hk(uk)}nk=1 while insuring that the upper triangular matrix R has positive diagonal elements. If we apply this algorithm to an orthogonal matrix Q′, we get a tuple (u1, . . . , un) which satisfies\nQR = Hn(un) . . . H1(u1)R = Q ′.\nNote that the matrix R must be orthogonal since R = QTQ′. Therefore, R = I , since the only upper triangular matrix with positive diagonal elements is the identity matrix. Hence, we have\nM1(u1, . . . , un) = Hn(un) . . . H1(u1) = Q′.\nWhen using m reflection vectors, the parametrisation can be expressed as\nW =Mn−m+1(un−m+1, . . . , vn) = Hn(un) . . . Hn−m+1(un−m+1), (12)\nThis parametrisation has the following advantages\n• The parametrisation is smooth, which is convenient for training with gradient descent.\nAlgorithm 1 QR decomposition using the mappings {Hk}nk=1. For a matrix R ∈ Rn×n, {Rk,k}1≤k≤n denote its diagonal elements, and Rk..n,k ∈ Rn−k+1 is a vector representing the part of the k-th column below the k-th row (including the element of the k-th row ). Require: A ∈ Rn×n is a full-rank matrix. Ensure: Q and R where Q = Hn(un) . . . H1(u1) and R\nand is upper triangular with positive diagonal elements such that A = QR R← A Q← I {Initialise Q to the identity matrix} for k = 1 to n− 1 do\nif Rk,k == ‖Rk..n,k‖ then un−k+1 = [0, . . . , 0, 1]\nT ∈ Rn−k+1 else un−k+1 ← Rk..n,k − ‖Rk..n,k‖ [1, 0, . . . , 0]T un−k+1 ← un−k+1/ ‖un−k+1‖ end if R← Hn−k+1(un−k+1)R Q← QHn−k+1(un−k+1)\nend for u1 = sgn(Rn,n) ∈ R R← H1(u1)R Q← QH1(u1)\n• Whenm = n, the transition matrix can span the whole orthogonal group O(n)\n• The only thing needed for the matrixW to be orthogonal is for the vectors {ui} to have norm one. To ensure that this is always the case, one method would be to project the vectors onto the unit ball after each gradient step. This would take O(nm) time complexity.\n• The time and space complexities involved in one gradient step are, in the worst case, the same as that of the simple RNN with the same number of hidden units. This is mainly due to the fact that the transition matrix W does not need to be generated explicitly. In fact, one only needs the matrix-vector products Wv, which are cheaper to compute using our parametrisation.\n• The parametrisation is flexible. A good trade-off between expressiveness and speed can be found by selecting the right number of reflection vectors.\n• When the goal is to ensure that the transition matrix W can span the whole orthogonal group, the number of reflection vectors and hidden units be the same. In this case, the number of redundant parameters associated with the transition matrix W is small. In fact, when m = n the total number of parameters needed for W is n(n+ 1)/2. There are only n redundant parameters in this case, since the orthogonal group O(n) is a n(n− 1)/2 manifold.\nNote that H1(u1) is not the standard Householder reflection. The second point of Theorem 5.2.2 (i.e. the subjectivity) would not be true had we used a Householder reflection instead."
    }, {
      "heading" : "4.1. Time complexity",
      "text" : "When using the parametrisation in (12), we do not need to calculate the matrix W explicitly. We only need the matrix-vector products Wv. The cost of calculating Wv is proportional to O(nm), where m and n are the number of reflection vectors used and the size of the hidden state respectively. Consequently, given a sequence of length T , the time complexity of Forward Propagation (FP) is O(Tmn). Back Propagation Through Time (BPTT) can be achieved within the same time complexity as FP. Note that for a simple RNN the complexity of FP and BPTT are equal to O(Tn2) for one sequence of length T . Therefore, training an RNN using our parametrisation incurs, in the worst case, the same time complexity as the simple RNN. The worst case being when the number of reflection vectors is equal to the size of the hidden state."
    }, {
      "heading" : "4.2. Space complexity",
      "text" : "Parametrising the transition matrix of an RNN using (12) withm reflections is equivalent to adding m−1 intermediate hidden states between any two consecutive time steps. For time step t, these vectors are\nht,k = Hn−m+k(un−m+k) . . . Hn−m+1(un−m+1)ht, (13)\nfor 1 ≤ k ≤ m− 1, with the conventions ht,m = ht+1,0 = ht+1.\nUsing an algorithm such as BPTT to calculate the gradients, requires the values of the hidden states, including the intermediate ones, in order to propagate the gradients. A naive approach would be to store the values of ht,k for all t ≤ T and 1 ≤ k ≤ m − 1. This method would increase the memory storage by a factor of m compared to the case of a simple RNN. This may be acceptable if m is fixed irrespective of the size of the hidden state n. However, when m = n, it is clearly a problem.\nIn order to circumvent this issue, note that the ht,k for 1 ≤ k ≤ m − 1 do not need to be explicitly stored during the forward propagation. In fact, they can be calculated ”on the fly” during BPTT, which would only add O(mn) operations at each time step. In fact, from Equation (13), the only ingredients needed to generate the intermediate hidden states at time step t are the reflection vectors {ui}mi=1 and the hidden state ht. Note that having to generate the intermediate vectors locally does not affect the overall time complexity of the BPTT algorithm. AddingO(mn) operations at each time step still results in an overall complexity\nof O(Tmn) for one sequence of length T .\nHowever, unless one implements the BPTT algorithm from scratch, it would be challenging to modify any pre-existing library to manage memory in the way needed for our parametrisation."
    }, {
      "heading" : "4.3. Compact WY representation",
      "text" : "In order to make the implementation of our method feasible we need to derive the expanded mathematical expressions of the following gradients with respect to some scalar loss function L\n∂L ∂U =\n[ ∂(Wh)\n∂U\n]T ∂L\n∂(Wh) , (14)\n∂L ∂h =\n[ ∂(Wh)\n∂h\n]T ∂L\n∂(Wh) , (15)\nwhere W is the transition matrix and U = (un| . . . |un−m+1) is the matrix whose columns are the reflection vectors. Furthermore, we need to be able to compute these gradients in a O(mn) time complexity in order to achieve the desired global time and space complexities discussed in the previous subsections. Having the explicit expressions of ∂L∂U , ∂L ∂h will allow an implementation that does not require the storage of any intermediate hidden states for BPTT.\nNote that in the case of an RNN, ∂L∂U represents the ”immediate” gradient at any time step t. In which case, the hidden states {hk}k≥t are affected by small perturbations of U only through the hidden state ht.\nIn order to write the mathematical expressions of the gradients in Equations (14) and (15), we will use the compact WY representation (Joffrain et al., 2006) of the product of Householder Reflections, which is described by the following proposition. Proposition 1. (Joffrain et al., 2006) Let n ∈ N and m ≤ n − 1. For n − m + 1 ≤ i ≤ n, let vi ∈ Rn−i+1 and U = (un| . . . |un−m+1) be the matrix whose columns are the {vi} vectors. Then\nHn(un), . . . ,Hn−m+1(un−m+1) = In − UT−1UT , (16)\nwhere\nT = striu(UTU) + 1\n2 diag(UTU), (17)\nwith striu(UTU), and diag(UTU) representing the strictly upper part and the diagonal of the matrix UTU , respectively. This is the compact WY representation.\nFor the particular case of m = n, in Proposition 1. The compact WY representation can be written as\nHn(un) . . . H1(u1) = ( In − UT−1UT ) H1(u1), (18)\nwhere U = (un| . . . |u2), and H1(u1) is defined in Equation (11).\nTheorem 2. Letm ≤ n−1 andW = I−UT−1UT , where U = (un| . . . |un−m+1). Let Jn ∈ Rn×n be the matrix of all ones, and L be a scalar loss function. We have\n∂L ∂U\n=U [ (h̃C̃T ) ◦BT + (C̃h̃T ) ◦B ] − Ch̃T − hC̃T ,\n(19) ∂L ∂h =C − UC̃, (20)\nwhere\nC =Wh, C = ∂L ∂C , C̃ = T−TUTC, h̃ = T−1UTh, B = striu(Jn) + 12In,\nand ◦ denote the Hadamard product.\nThe gradients ∂L∂U , and ∂L ∂h as defined in Equations (19) and (20) can be calculated in O(mn) time and space complexities.\nThe proof of Equations (19) and (20) is provided in the appendix. Algorithms 2 and 3 show routines which can calculate T−1v and T−T v in time complexity O(mn) for any vector v. Algorithm 4 shows a routine which calculates U [ (vwT ) ◦BT + (wvT ) ◦B ] for any vectors v and w, also in time complexity O(mn). Using these routines the gradients ∂L∂U and ∂L ∂h can be calculated in time complexity O(mn).\nAlgorithm 2 Calculating T−1v for any vector v. For a matrix U ∈ Rn×m, U∗,k denotes its k-th column. Require: U = (un| . . . |un−m+1) ∈ Rn×m and v ∈ Rm. Ensure: o = T−1v, where o ∈ Rm o← [0, . . . , 0]T ∈ Rm {initialising o to zeros} t← [0, . . . , 0]T ∈ Rn {temporary vector} for k = m to 1 do ok ← 2(vk − UT∗,kt)/ ‖U∗,m‖ t← t+ okU∗,k\nend for\nAlgorithm 3 Calculating T−T v for any vector v. For a matrix U ∈ Rn×m, U∗,k denotes its k-th column. Require: U = (un| . . . |un−m+1) ∈ Rn×m and v ∈ Rm. Ensure: o = T−T v, where o ∈ Rm o← [0, . . . , 0]T ∈ Rm {initialising o to zeros} t← [0, . . . , 0]T ∈ Rn {temporary vector} for k = 1 to m do ok ← 2(vk − UT∗,kt)/ ‖U∗,m‖ t← t+ okU∗,k\nend for\nAlgorithm 4 Calculating U [ (vwT ) ◦BT + (wvT ) ◦B ] for any vector v and w in Rm. For a matrix U ∈ Rn×m, U∗,k denotes its k-th column. Require: U = (un| . . . |un−m+1) ∈ Rn×m and v, w ∈ Rm.\nEnsure: Z = U [ (vwT ) ◦BT + (wvT ) ◦B ] ∈ Rn×m,\nwhere o ∈ Rn Z ← 0n,m ∈ Rn×m {initialising Z with zeros} t← [0, . . . , 0]T ∈ Rn {temporary vector} l← [0, . . . , 0]T ∈ Rn {temporary vector} for k = 1 to m− 1 do t← t+ U∗,kvk Z∗,k+1 ← Z∗,k+1 + wk+1t l← l + U∗,m−k+1wm−k+1 Z∗,m−k+1 ← Z∗,m−k+1 + vm−k+1l end for l← l + U∗,1w1 Z∗,1 ← Z∗,1 + v1l"
    }, {
      "heading" : "5. Experiments",
      "text" : "We have implemented and tested our method using the python library theano (Theano Development Team, 2016). We implemented a tensor operator F that takes a matrix W and a vector h and returns the product Wh. In order to enable automatic differentiation, we also implemented a gradient method associated with the operator F which calculates the gradients in Equations 19 and 20, using Algorithms 2, 3, and 4."
    }, {
      "heading" : "5.1. Scalability",
      "text" : "In this first experiment we tested the scalability of our method by measuring the time required for one gradient calculation. The input consisted of one randomly generated 2d sequence. The target output was a fixed scalar value at the end of the sequence. We compared our method to a naive approach where the orthogonal matrix is generated explicitly before the FP using the compact representation W = I − UT−1UT . Note that even though the matrix T is triangular and its inverse can be calculated in time complexity O(m2), generating W would still require O(nm2) complexity because of the matrix-matrix products involved. Our method, on the other hand, does not require the explicit computation of W .\nWe considered the following situations:\n• We fixed the length T = 10 of the input sequence and varied the number of hidden units n of the oRNN in the set {128, 256, 512, 1024, 2048} (see Figure 1(a)). The number of reflection vectors was equal to the hidden size (i.e m = n). Our method scales in O(n2), compared with O(n3) for the naive approach as ex-\npected. Our method is faster than the later when n ≥ 250.\n• Here, we fixed the length of the input sequence to T = 10, and the number of hidden units to n = 2048 and we varied the number of reflections m = {128, 256, 512, 1024, 2048} (See Figure 1(b)). Our method scales in O(m).\n• The last case consisted of fixing the number of hidden units and reflections to n = m = 128 and varying the length of the input sequence in {64, 128, 256, 512, 1024, 2048, 4096}. The computational time increased linearly with the length of the sequence (See Figure 1(c)).\nThis first experiment, confirms that, using our approach, BPTT can be performed in O(Tmn) time complexity for an input sequence of length T ."
    }, {
      "heading" : "5.2. Application to problems with long-term dependencies",
      "text" : "We have tested the new parametrisation on the three different datasets all having long-term dependencies. We will refer to the RNN parametrised using the Householder reflections as the oRNN. We set its activation function to the leaky Rectified Linear Unit (Relu) defined as\nφ(x) =\n{ x, if x > 0\n0.1x, otherwise (21)\nFor all experiments, we used the adam method for stochastic gradient descent. We tested different learning rates in the set {10−1, 10−2, 10−3, 10−4} for every model and selected the ones which resulted in the fastest convergence on the training sets. We initialised all the parameters using uniform distributions similar to (Arjovsky et al., 2015). The biases of all model were set to zero, except for the forget bias of the LSTM, which we set to 5 in order to facilitate the learning of long-term dependencies (Koutnı́k et al., 2014).\nTo ensure that the transition matrix of the oRNN is always orthogonal, we projected each reflection vector ui onto the unit i-sphere after each gradient iteration."
    }, {
      "heading" : "5.2.1. SEQUENCE GENERATION",
      "text" : "In this experiment, we followed a similar setting to (Koutnı́k et al., 2014) where we trained RNNs to encode song excerpts. We used the track Manyrista from album Musica Deposita by Cuprum. We extracted five consecutive excerpts around the beginning of the song, each having 800 data points and corresponding to 18ms with a 44.1Hz sampling frequency. We trained an sRNN, LSTM,\nand oRNN for 5000 epochs on each of the pieces with five random seeds. For each run, the lowest Normalised Mean Squared Error (NMSE) during the 5000 epochs was recorded. For each model, we tested three different hidden sizes. The total number of parameters corresponding to these hidden sizes was approximately equal to 250, 500, and 1000. For the oRNN, we chose the number of reflection vectors to be equal to the hidden size for each case, so that the transition matrix is allowed to span the full orthogonal group. The results are shown in Figures 2 and 3. All the learning rate were set to 0.001.\nThe orthogonal parametrisation outperformed the simple RNN and performed on average better than the LSTM on the sequence encoding task."
    }, {
      "heading" : "5.2.2. PIXEL MNIST",
      "text" : "We used the MNIST image dataset in this experiment. We split the dataset into train set (55000 instances), validation (5000 instances), and test set (10000 instances). We trained an oRNN with n = 128 hidden units and m = 16 reflections, which corresponds to approximately 3600 parameters, to minimise the cross-entropy. We set the batch size to 64, and we selected the best model based on the lowest validation cost after 20 epochs. The later was evaluated at every 20 training iterations. All the learning rates were set to 0.001 in this experiment.\nTable 5.2.2 compares our test performance against the uRNN results available in the literature. The models we used for comparison all have about 16K parameters, which is more than four time the number of parameters of our tested model. Despite this, our model still achieved a 95.6% test accuracy which is the second best result."
    }, {
      "heading" : "5.2.3. ADDING TASK",
      "text" : "In this experiment, we follow a similar setting to (Arjovsky et al., 2015), where the goal of the RNN is to output the sum of two elements in the first dimension of a 2d sequence. The location of the two elements to be summed are specified by the entries in the second dimension of the input sequence. In particular, the first dimension of every input sequence consists of random numbers between 0 and 1. The second dimension has all zeros except for two elements which are equal to 1. The first unit entry is located in the first half of the sequence, and the second one in the second half. We tested two different lags T = 400 and T = 800. All models were trained to minimise the Mean Squared Error (MSE). The baseline MSE for this task is 0.167 corresponding to a model that always outputs 1.\nWe trained an oRNN with n = 128 hidden units and m = 16 reflections similar to the MNIST experiment. We trained an LSTM and an sRNN with hidden sizes 28 and 54, respectively, corresponding to a total number of parameters ' 3600 (i.e. same as the oRNN model). We chose a batch size of 50, and after each iteration, a new set of sequences was generated randomly. The learning rate for the oRNN was set to 0.01. Figure 5(a) and 5(b) displays the results.\nThe oRNN was able to beat the baseline MSE in less than 5000 iterations for both lags and for two different random initialisation seeds. This is in line with the results of the unitary RNN (Arjovsky et al., 2015)."
    }, {
      "heading" : "6. Discussion",
      "text" : "In this work, we presented a new efficient way to parametrise transition matrices of a recurrent neural network using Householder reflections. This method allows an easy and computationally efficient way to enforce an orthogonal constraint on the transition matrix which then ensures that exploding gradients do not occur during training. We have also shown that enforcing a unitary constraint on the transition matrix is a special case of the orthogonal one. Experimental results show that the orthogonal parametrisation has similar advantages to the unitary one."
    }, {
      "heading" : "Acknowledgment",
      "text" : "The authors would like to acknowledge Department of State Growth Tasmania for funding this work through SenseT."
    }, {
      "heading" : "A. Proof of Theorem 2",
      "text" : "Lemma 1. (Giles, 2008) Let A, B, and C be real or complex matrices, such that C = f(A,B) where f is some differentiable mapping. Let s be some scalar quantity that is dependent on A, and B through C. Then we have the following identity\nTr(C T dC) = Tr(A T dA) + Tr(B T dB),\nwhere dA, dB, and dC represent infinitesimal perturbations and X = ∂s∂X , for X = A,B,C.\nProof of Theorem 2. Let C = h − UT−1UTh where (U, h) ∈ Rn×m×Rn and T = striu(UTU)+ 12diag(U\nTU). Notice that the matrix T can be written using the Hadamard product as follows\nT = B ◦ (UTU),\nwhere B = striu(Jn) + 12In and Jn is the matrix of all ones.\nCalculating the infinitesimal perturbations of C gives\ndC =(I − UT−1UT )dh − dUT−1UTh− UT−1dUTh + UT−1dTT−1UTh.\nUsing Equation (??) we can write\ndT = B ◦ (dUTU + UT dU).\nBy substituting this back into the expression of dC, multiplying the left and right-hand sides by C T , and applying the trace we get\nTr(C T dC) = Tr(C T (I − UT−1UT )dh)\n− Tr(CT dUT−1UTh)− Tr(CTUT−1dUTh)\n+ Tr(C T UT−1(B ◦ (dUTU + UT dU))T−1UTh).\nNow using the identity Tr(AB) = Tr(BA), where the second dimension of A agrees with the first dimension of B, we can rearrange the expression of Tr(C T dC) as follows\nTr(C T dC) = Tr(C T (I − UT−1UT )dh)\n− Tr(T−1UThCT dU)− Tr(hCTUT−1dUT )\n+ Tr(T−1UThC T UT−1(B ◦ (dUTU + UT dU))).\nTo simplify the expression, we will use the short notations\nC̃ = T−TUTC,\nh̃ = T−1UTh,\nTr(C T dC) becomes\nTr(C T dC) = Tr((C T − C̃TUT )dh)\n− Tr(h̃CT dU)− Tr(hC̃T dUT ) + Tr(h̃C̃T (B ◦ (dUTU + UT dU))).\nNow using the two following identities of the trace\nTr(AT ) = Tr(A),\nTr(A(B ◦ C)) = Tr((A ◦BT )C)),\nwe can rewrite Tr(C T dC) as follows\nTr(C T dC) =Tr((C T − C̃TUT )dh)\n− Tr(h̃CT dU)− Tr(hC̃T dUT ) + Tr((h̃C̃T ◦BT )dUTU) + Tr((h̃C̃T ◦BT )UT dU).\nBy rearranging and taking the transpose of the third and fourth term of the right-hand side we obtain\nTr(C T dC) =Tr((C T − C̃TUT )dh)\n− Tr(h̃CT dU)− Tr(C̃hT dU) + Tr(((C̃h̃T ) ◦B)UT dU) + Tr(((h̃C̃T ) ◦BT )UT dU).\nFactorising by dU inside the Tr we get\nTr(C T dC) = Tr((C T − C̃TUT )dh) − Tr (( h̃C T + C̃hT − [ (C̃h̃T ) ◦B + (h̃C̃T ) ◦BT ] UT ) dU ) .\nUsing lemma 1 we conclude that\n∂L ∂U\n=U [ (h̃C̃T ) ◦BT + (C̃h̃T ) ◦B ] − Ch̃T − hC̃T ,\n∂L ∂h =C − UC̃."
    } ],
    "references" : [ {
      "title" : "Unitary evolution recurrent neural networks",
      "author" : [ "Arjovsky", "Martin", "Shah", "Amar", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1511.06464,",
      "citeRegEx" : "Arjovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Arjovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "An extended collection of matrix derivative results for forward and reverse mode automatic differentiation",
      "author" : [ "Giles", "Mike B" ],
      "venue" : null,
      "citeRegEx" : "Giles and B.,? \\Q2008\\E",
      "shortCiteRegEx" : "Giles and B.",
      "year" : 2008
    }, {
      "title" : "Orthogonal rnns and long-memory tasks",
      "author" : [ "Henaff", "Mikael", "Szlam", "Arthur", "LeCun", "Yann" ],
      "venue" : "arXiv preprint arXiv:1602.06662,",
      "citeRegEx" : "Henaff et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Henaff et al\\.",
      "year" : 2016
    }, {
      "title" : "Gradient flow in recurrent nets: the difficulty of learning",
      "author" : [ "Hochreiter", "Sepp", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "Jürgen" ],
      "venue" : "long-term dependencies,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 2001
    }, {
      "title" : "Learning unitary operators with help from u (n)",
      "author" : [ "Hyland", "Stephanie L", "Rätsch", "Gunnar" ],
      "venue" : "arXiv preprint arXiv:1607.04903,",
      "citeRegEx" : "Hyland et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hyland et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving training of deep neural networks via singular value bounding",
      "author" : [ "Jia", "Kui" ],
      "venue" : "arXiv preprint arXiv:1611.06013,",
      "citeRegEx" : "Jia and Kui.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jia and Kui.",
      "year" : 2016
    }, {
      "title" : "Accumulating householder transformations, revisited",
      "author" : [ "Joffrain", "Thierry", "Low", "Tze Meng", "Quintana-Ortı", "Enrique S", "Geijn", "Robert van de", "Zee", "Field G Van" ],
      "venue" : "ACM Transactions on Mathematical Software (TOMS),",
      "citeRegEx" : "Joffrain et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Joffrain et al\\.",
      "year" : 2006
    }, {
      "title" : "A clockwork RNN",
      "author" : [ "Koutnı́k", "Jan", "Greff", "Klaus", "Gomez", "Faustino J", "Schmidhuber", "Jürgen" ],
      "venue" : "CoRR, abs/1402.3511,",
      "citeRegEx" : "Koutnı́k et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Koutnı́k et al\\.",
      "year" : 2014
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E" ],
      "venue" : "arXiv preprint arXiv:1504.00941,",
      "citeRegEx" : "Le et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2015
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua" ],
      "venue" : "ICML (3),",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Full-capacity unitary recurrent neural networks",
      "author" : [ "Wisdom", "Scott", "Powers", "Thomas", "Hershey", "John R", "Roux", "Jonathan Le", "Atlas", "Les" ],
      "venue" : "arXiv preprint arXiv:1611.00035,",
      "citeRegEx" : "Wisdom et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wisdom et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "In practice, however, training simple RNNs can be challenging due to the problem of exploding and vanishing gradients (Hochreiter et al., 2001).",
      "startOffset" : 118,
      "endOffset" : 143
    }, {
      "referenceID" : 9,
      "context" : "This results in an error surface, associated with some objective function, having very steep walls (Pascanu et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "One approach to solving this issue is to clip the gradients (Pascanu et al., 2013) when their norm exceeds some threshold value.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "In particular, the unitary RNN (Arjovsky et al., 2015) constrains the transition matrix to be in the unitary group U(n) using a special parametrisation.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "This parametrisation and other similar ones (Hyland & Rätsch, 2016; Wisdom et al., 2016) have some advantages and drawbacks which we will discuss in more details in the next section.",
      "startOffset" : 44,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "In particular, initialising with specific structures such as the identity or an orthogonal matrix can greatly improve performance (Le et al., 2015).",
      "startOffset" : 130,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "In addition to these initialisation methods, one study has also considered removing the non-linearity between the hidden-to-hidden connections (Henaff et al., 2016), i.",
      "startOffset" : 143,
      "endOffset" : 164
    }, {
      "referenceID" : 10,
      "context" : "It is possible to constrain the transition matrix to be orthogonal during training using more sophisticated methods (Wisdom et al., 2016).",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 0,
      "context" : "Recently, new parametrisations of the transition matrix have been suggested (Arjovsky et al., 2015), which ensure that its spectral norm is always equal to one.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "However, it has been shown that this parametrisation does not allow the transition matrix to span the full unitary group (Wisdom et al., 2016), which may limit the model expressiveness.",
      "startOffset" : 121,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "This is one advantage over the original unitary parametrisation (Arjovsky et al., 2015).",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "A more recent method (Wisdom et al., 2016) performs optimisation directly of the Stiefel manifold.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "Take for example the activation function proposed by (Arjovsky et al., 2015)",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "Methods Constraint on the Time complexity of one Search space of the transition matrix online gradient step transition matrix uRNN (Arjovsky et al., 2015) ‖W‖ = 1 O(Tn log(n)) A subset of U(n)",
      "startOffset" : 131,
      "endOffset" : 154
    }, {
      "referenceID" : 10,
      "context" : "Full-capacity uRNN ‖W‖ = 1 O(Tn + n) The full U(n) group (Wisdom et al., 2016) Full-capacity uRNN ‖W‖ = 1 O(Tn + n) The full U(n) group (Hyland & Rätsch, 2016) oRNN ‖W‖ = 1 O(Tnm) The full O(n) group (Our approach) where m ≤ n when m = n",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "In order to write the mathematical expressions of the gradients in Equations (14) and (15), we will use the compact WY representation (Joffrain et al., 2006) of the product of Householder Reflections, which is described by the following proposition.",
      "startOffset" : 134,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "(Joffrain et al., 2006) Let n ∈ N and m ≤ n − 1.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "We initialised all the parameters using uniform distributions similar to (Arjovsky et al., 2015).",
      "startOffset" : 73,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "The biases of all model were set to zero, except for the forget bias of the LSTM, which we set to 5 in order to facilitate the learning of long-term dependencies (Koutnı́k et al., 2014).",
      "startOffset" : 162,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "In this experiment, we followed a similar setting to (Koutnı́k et al., 2014) where we trained RNNs to encode song excerpts.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "6 % uRNN (Arjovsky et al., 2015) 512 ' 16K - 95.",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "In this experiment, we follow a similar setting to (Arjovsky et al., 2015), where the goal of the RNN is to output the sum of two elements in the first dimension of a 2d sequence.",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "This is in line with the results of the unitary RNN (Arjovsky et al., 2015).",
      "startOffset" : 52,
      "endOffset" : 75
    } ],
    "year" : 2016,
    "abstractText" : "Recurrent Neural Networks (RNNs) have been successfully used in many applications. However, the problem of learning long-term dependencies in sequences using these networks is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training, which ensures that its norm is exactly equal to one. These methods either have limited expressiveness or scale poorly with the size of the network when compared to the simple RNN case, especially in an online learning setting. Our contributions are as follows. We first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Therefore, it may not be necessary to work with complex valued matrices. Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while insuring that the transition matrix is always orthogonal. Using our approach, one online gradient step can, in the worst case, be performed in time complexity O(Tn), where T and n are the length of the input sequence and the size of the hidden layer respectively. This time complexity is the same as the simple RNN case. Finally, we test our new †The Department of Computing and Information Systems. ‡Data61. parametrisation on problems with long-term dependencies. Our results suggest that the orthogonal constraint on the transition matrix has similar benefits to the unitary constraint.",
    "creator" : "LaTeX with hyperref package"
  }
}