{
  "name" : "1610.02072.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "An efficient high-probability algorithm for linear bandits",
    "authors" : [ "Gábor Braun", "Sebastian Pokutta" ],
    "emails" : [ "gabor.braun@isye.gatech.edu", "sebastian.pokutta@isye.gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n02 07\n2v 1\n[ cs\n.D S]\n6 O\nFor the linear bandit problem, we extend the analysis of algorithm CombEXP from Combes et al. [2015] to the high-probability case against adaptive adversaries, allowing actions to come from an arbitrary polytope. We prove a high-probability regret of O(T2/3) for time horizon T. While this bound is weaker than the optimal O( √\nT) bound achieved by GeometricHedge in Bartlett et al. [2008], CombEXP is computationally efficient, requiring only an efficient linear optimization oracle over the convex hull of the actions."
    }, {
      "heading" : "1 Introduction",
      "text" : "We study sequential prediction problemswith linear losses and bandit feedback against an adaptive adversary. At every round t the forecaster chooses an action xt, and the adversary chooses a loss function Lt, and the forecaster suffers the loss Lt(xt). The forecaster learns only the suffered loss after each round, while the adversary learns the forecaster’s action xt. The forecaster’s aim is to minimize regret, which is the difference between the incurred loss and the loss of the best single action in hindsight:\n∑ t∈T Lt(xt)− min x∈A ∑t∈T Lt(x).\nIn this work we focus on establishing regret bounds holding with high-probability with an efficient algorithm.\nFor algorithms with bandit feedback, exploration (occasionally playing random actions for learning) is a crucial feature, however it does not have to be explicit as recently shown in Neu [2015], where exploration is achieved via skewing loss estimators. One of the most studied regret minimization algorithm is EXP, which iteratively updates the probabilities of each action via multiplication with factors exponential in its (estimated) loss. The variant EXP3 for multi-armed bandit problems first appeared in Auer et al. [2002], however optimal high-probability regret bounds were first achieved in Dani and Hayes [2006]. The linear bandit setting is a generalization of the multi-armed bandit setting where, utilizing the linearity of losses, the goal is to improve the dependence on the number of actions in the regret bound, which might be exponential in the dimension n. At the same time linear losses come naturally into play when considering actions with a combinatorial structure, such as e.g., matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al. [2013] for an extensive discussion. For the linear bandit setting, the\nEXP-variant ComBand (Combinatorial Bandit) from Cesa-Bianchia and Lugosi [2012] has optimal O( √\nT) expected regret, and in Bartlett et al. [2008] the modified version GeometricHedge achieves O( √ T) regret with high probability. While these regret bounds practically do not depend on the number of actions, both maintain a distribution over the (possibly exponentially large) action set A, which is infeasible in general due to the large data size, even though ComBand is still efficient for many specific problems. Recently, a modification of the ComBand algorithm called CombEXP (see Algorithm 1) was derived in Combes et al. [2015], which achieves general computational efficiency by not maintaining a distribution of xt, but only the desired expectation x̂t of the distribution, and generating a new sparse approximate distribution at every round.\nIn this work we provide a high-probability regret bound of O(T2/3) for CombEXP against adaptive adversaries, while generalizing it to general polytopes. The obtained bounds are any-time, i.e., the parameter choice is independent\nof the time horizon T. Finally, our algorithmmaintains computational efficiency given an efficient linear programming oracle over the underlying polytope (the convex hull of actions). For comparison, we also show an O(T2/3) regret in the high-probability setting for the original ComBand.\nThe maximal matching problem is a good example where the linear programming oracle approach is useful, as it has a polynomial time linear optimization algorithm Edmonds [1965], but no polynomial-size polyhedral description Rothvoß [2014].\nRelated work\nOur work is most closely related to the line of works on combinatorial bandit problems. The algorithm ComBand first appeared in Cesa-Bianchia and Lugosi [2012], while GeometricHedge comes from Bartlett et al. [2008], and Comb-\nEXP appeared in Combes et al. [2015]. Using interior point methods, an efficient algorithm with O( √\nT) expected regret for linear bandit problems has been established in Abernethy et al. [2008].\nFor multiarmed bandit problems, the original version of EXP3 has high-probability regret Ω(T2/3) against some\nadaptive adversaries [Dani and Hayes, 2006, Theorem 1.2], however variants with optimal O( √\nT) regret exists, e.g., using accountants to control the exploration rate (see Dani and Hayes [2006]), or via the recent EXP3-IX with implicit exploration (see Neu [2015]).\nFor convex loss functions, optimal regret bounds have been obtained in Hazan and Li [2016] with running time being poly-exponential in the dimension, and in [Bubeck et al., 2016, Theorem 1] with polynomial running time provided the number of constraints of the underlying polytope is polynomial in the dimension. However the case of convex loss does not subsume the combinatorial/linear case, as with convex loss all inner points of the convex set are actions; with linear losses the actions are limited to the vertices of the underlying polytope in most cases.\nWe refer the interested reader to the excellent survey of Bubeck and Cesa-Bianchi [2012] on bandit problems.\nContribution\nOur main contribution is a high-probability regret bound for CombEXP from Combes et al. [2015] for adaptive adversaries over actions coming from arbitrary polytopes P ⊆ Rn. Our algorithm, being a slight generalization of CombExp, maintains computational efficiency. In particular, our contribution can be summarized as follows:\n(i) High-probability bounds for an efficient algorithm. For CombEXP we establish a high-probability regret of\nO\n( B2 + nB\nmin{λ, 1} · ln 2n + 2 δ\n)\nT2/3,\nwith probability 1− δ, where B is the ℓ2-diameter of P, and λ is a lower bound on the smallest eigenvalue of the exploration covariance matrix, see Theorem 3.1 for the exact regret bound.\nFor comparison we show that the same method already provides a high-probability regret bound of O(T2/3) for the original ComBand, albeit a suboptimal one as GeometricHedge achieves O( √ T) regret.\n(ii) Generalization of CombEXP and computational efficiency. We generalize CombEXP to actions arising from ar-\nbitrary polytopes contained in Rn and to the case of adaptive adversaries. We maintain computational efficiency of CombExp providing running times relative to a linear programming oracle over the underlying polytope P, separating the complexity for learning from the complexity of linear optimization over P.\nAll our bounds are any-time, i.e., holding uniformly for all times T. In particular, our parameter choices are independent of T.\nOutline\nAfter a brief summary of the regret minimization framework in Section 2, we reanalyze CombEXP in Section 3. For completeness we present a similar analysis for ComBand in Section 4.\nWe relegated various related materials to the the Appendix. In Section A we provide an any-time version of EXP with time-varying parameters maintaining generalized distributions, defined by an arbitrary convex set in the positive\northant, instead of the probability simplex. We prove an O( √ T) regret bound in the full information case by standard\narguments, which forms the basis for our regret bounds for the bandit case. In Section B we recall concentration inequalities that we use to establish high-probability bounds. Finally, in Sections C and D we provide (already known) efficient algorithms for projection and distribution generation, which are key components in our algorithms. We include those for completeness of exposition and to make parameters explicit."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We will briefly recall the regret minimization framework to define our notation. In the sequential prediction problem with linear losses, at every round t the forecaster chooses an action xt from a finite set A ⊆ Rn and the adversary chooses a loss vector Lt ∈ Rn. The forecaster suffers the loss ℓt := L⊺t xt. The goal of the forecaster is to minimize the regret\nT\n∑ t=1\nL ⊺ t xt − min x∈A\nT\n∑ t=1\nL ⊺\nt x.\nAgainst an oblivious adversary, who chooses the Lt independently of the forecaster’s actions, this is the extra loss suffered by not playing the best single action in hindsight. However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas.\nWith bandit feedback the forecaster learns only the loss ℓt but not the actual loss vector Lt. An adaptive adversary learns the forecaster’s action xt after round t, and can use it in later rounds to choose his actions.\nWe make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L⊺t x| ≤ 1 for all x ∈ A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that ‖x‖2 ≤ B and ‖x‖1 ≤ B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions.\nFollowing Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution µ on A for exploration, whose fitness for exploration is measured by a positive lower bound λ on the smallest eigenvalue of its covariance matrix J: J := Ey∼µ [yy⊺] λI. Here and below we denote by M N that N − M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then µ is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John’s decomposition (John’s exploration; see Dani et al. [2007]), transferred to A. In the latter two cases, J = I and λ = 1/n using the scalar product on R n induced by the additional structure. John’s ellipsoid can be approximately estimated with a worse lower bound λ = 1/n3/2 by Grötschel et al. [1993], however a constant factor approximation is NP-hard by Nemirovski [2006]. Recall that a barycentric spanner is a linear basis v1, . . . , vn in P (the convex hull of A), such that every element of P is a linear combination of the vi with coefficients from [−1,+1]. The basis v1, . . . , vn is a C-approximate barycentric spanner for some C > 1 if every element of P is a linear combination of the vi with coefficients from [−C,+C]. A Capproximate barycentric spanners can be efficiently computed by O(n2 ln n/ ln C) calls to a linear optimization oracle over P by Awerbuch and Kleinberg [2004], which actually computes a spanner consisting of vertices of P. In this paper we deliberately avoid using the scalar product induced by the structure to be able to directly use the bounds available in the original space of the problem. Fortunately, the uniform distribution on an approximate barycentric spanner has a close to optimal minimal eigenvalue even in the original space, see Lemma E.1, which allows us to preserve sparsity of the original space. As such we assume that we have access to an exploration distribution over actions with sparse support of size n, where n is the dimension of the vector space, from which we can efficiently sample. Note that for specific problems exploration distributions with better minimal eigenvalue can be explicitly given; we refer the interested reader to Cesa-Bianchia and Lugosi [2012] and follow-up work for a large set of such examples.\nLet u := Ey∼µ [y] denote the expectation of µ and let e denote the Euler constant. Instead of dealing directly with A, it will be more convenient to use the convex hull P of A, then A contains the vertex set of P (and in many applications the two are equal). We shall use the Kullback–Leibler divergence as Bregman divergence of the function\nf (x1, . . . , xn) = ∑ n i=1 xi ln xi for projection:\nKL(x, y) = n\n∑ i=1 xi ln xi yi − n ∑ i=1 xi + n ∑ i=1 yi.\nIn the following, for a vector a ∈ Rn wewill useRn>a := {x ∈ Rn | xi > ai for all i ∈ [n]} to denote the a-positive orthant. Moreover, a linear optimization oracle (or LP oracle) over a polytope P ⊆ Rn finds for any linear objective c ∈ Rn a vertex x of P minimizing c⊺x.\nIn all our bounds below, the O-notation only hides an absolute constant, i.e., all parameters of the algorithms are explicit. However, in Section 1 the O-notation hides also other parameters, like the dimension n."
    }, {
      "heading" : "3 A high-probability regret bound for CombEXP",
      "text" : "We provide an adaptation of CombEXP (Algorithm 1) with an O(T2/3) regret with high probability against adaptive adversaries, while maintaining computational efficiency. In a nutshell, EXP is run on the coordinates of the desired expectation x̂t of xt, and a new distribution over vertices xt of P is generated in every round. In order to obtain an efficient algorithm, we allow errors in the most resource-consuming components of the algorithm: the projection step and the distribution generation. The accuracy of distribution generation is controlled by a parameter ε, and helps maintaining a distribution with sparse support, to allow fast sampling and fast computation of the covariance matrix Ct. The positive parameters ηt, γt control the learning rate and exploration rate of the algorithm. The role of the shifting vector a ∈ Rn is to avoid singularity issues with Kullback–Leibler divergence. Except for the shifting vector a, these ideas already appeared in Combes et al. [2015].\nThe algorithmcontains four resource-consumingsteps: (1) projection (Line 10), (2) distributiongeneration (Line 3), (3) sampling from the distribution, and (4) computing the covariance matrix. All the other steps are fast, depending only polynomially on the dimension.\nThemajor factor for the running time of sampling from the distribution (3), and computing the covariancematrix (4) is the sparsity of the generated distribution, i.e., the number of possible outcomes. Sparse distributions (number of outcomes polynomial in the dimension) of sufficient accuracy can be efficiently generated by the decomposition algorithm fromMirrokni et al. [2015], which we summarize as Algorithm 5 in Section D for the reader’s convenience. Common choices of the exploration distribution µ are sparse, as discussed above, notwithstanding non-sparse distributions for µ are also acceptable which have an efficient sampling method and a precomputed covariance matrix. Therefore we will disregard the complexity of sampling and computation of the covariance matrix.\nFinally, the projection step (Line 10) can be efficiently accomplished by the Frank–Wolfe algorithm (also called conditional gradient), which we recall in Algorithm 4 in Section C. Note that if Algorithm 4 is used for the projection step, it already provides a sparse linear decomposition of the desired expectation x̂t+1 with accuracy ε = 0, and therefore makes a separate linear decomposition step unnecessary. Nevertheless it might be advantageous for specific polytopes to use a specialized, more efficient projection algorithm and/or decomposition algorithm.\nAll in all, we measure complexity of only the most time-consuming tasks: projection and linear decomposition, requiring the linear decomposition to be sparse. We report complexity of Algorithms 4 and 5 mentioned above in the total number of linear optimization oracle calls over P. This relative complexity is often useful in applications where fast linear programming oracles are available.\nNow we are ready to state our main theorem on the regret and complexity of CombEXP.\nTheorem 3.1 (High-probability regret bound for CombEXP for adaptive adversaries). For n ≥ 1 and with the choice\nγt := t−1/3\n2 and ηt := min{γ2t , γtλ}\nAlgorithm 1 achieves for any time T ≥ 1 the following regret: With probability at least 1 − δ, for any x ∈ P we have T\n∑ t=1\n(L⊺t xt − L ⊺ t x) ≤ ( 4 KL(a + x, a + x̂1) min{1, 2λT1/3} + B1 √ 3 λ ln n + 2 δ + ( (e − 2)‖a‖1 + B1 λ + 2 + B(B + ε) λ ) 3 4 ) T2/3\n+O\n(\nmax\n{\n1, B2 max{(‖a‖1 + B1)/λ, ε}\nλ , B1 max{1, B} λ , B + ε√\nλ\n}√ T ) ln 2n + 2\nδ . (1)\nIn particular, assuming α − ai ≤ zi ≤ β − ai for some 0 < α < β for all z ∈ P:\nAlgorithm 1 CombEXP\nRequire: polytope P ⊆ Rn>−a, positive parameters ε, η1 ≥ η2 ≥ . . ., and 1/2 ≥ γ1, γ2, . . . Ensure: vertices xt of P as actions 1: x̂1 ∈ P arbitrary 2: for t = 1 to T do 3: Find distribution pt with ‖Ex∼pt [x]− x̂t‖2 ≤ γtε {approximate distribution} 4: qt ← (1 − γt)pt + γtµ 5: Sample xt ∼ qt. 6: Observe loss ℓt := L ⊺\nt xt 7: Ct ← Ex∼qt [xx⊺] 8: L̂t ← ℓtC−1t xt 9: yt+1,i ← (ai + x̂1,i)1−ηt+1/ηt(ai + x̂t,i)ηt+1/ηt exp(−ηt+1L̂t,i)− ai for all i ∈ [n] 10: Find x̂t+1 ∈ P with KL(a + z, a + x̂t+1) ≤ KL(a + z, a + yt+1) + γtηt+1 for all z ∈ P {approximate\nprojection}\n11: end for\n(i) Regret boundWe have KL(a + x, a + x̂1) ≤ 4B 2 α so that the upper bound on the regret is proportional to T 2/3.\nWith probability at least 1 − δ, for any x ∈ P we have T\n∑ t=1\n(L⊺t xt − L ⊺ t x) ≤ O ( B2\nα +\nB1 + (B + ε) 2 + ‖a‖1\nmin{λ, √ λ} · ln 2n + 2 δ\n)\nT2/3.\n(ii) Complexity. Using Algorithm 4 both for projection and distribution generation (Lines 3 and 10) with ε = 0, the\nalgorithm makes altogether O (\nB4β α3 min{1,λ2}\n)\nT3 oracle calls to a linear optimization oracle over P.\nAlternatively using a specialized projection algorithm in Line 10, and Algorithm 5 for distribution generation in Line 3, then Algorithm 5 calls a linear optimization oracle over P at most O(B2/ε2)T5/3 times across all rounds.\nObviously, the O(B2/ε2)T5/3 oracle calls in the last sentence does not contain the complexity of the specialized projection algorithm.\nNote that the bounds in Theorem 3.1 are any-time guarantees as the parameters of the algorithm do not depend on the time horizon T. The constant factor in the regret bound can be slightly improved by a more sophisticated choice of the γt and ηt, however, we preferred simple formulae for these parameters. Just as for EXP3, the choice of parameters is different for the best expected regret and the best high-probability regret."
    }, {
      "heading" : "3.1 Proof of Theorem 3.1",
      "text" : "In this section we will prove Theorem 3.1. We focus on the main regret bound, Equation (1), the other results easily follow from it. See Propositions C.1 and D.1 for the complexity of Algorithms 4 and 5. The inequality KL(a + y, a + x̂1) ≤ 4B 2 α is derived using ln z ≤ z − 1:\nKL(a + y, a + x̂1) = n\n∑ i=1\n(ai + yi) ln ai + yi\nai + x̂1,i −\nn\n∑ i=1\n(ai + yi) + n\n∑ i=1 (ai + x̂1,i)\n≤ n\n∑ i=1 (ai + yi)\n( ai + yi\nai + x̂1,i − 1\n)\n− n\n∑ i=1\n(ai + yi) + n\n∑ i=1\n(ai + x̂1,i) = n\n∑ i=1 (yi − x̂1,i)2 ai + x̂1,i ≤ ‖y − x̂1‖ 2 2 α ≤ 4B 2 α .\nThe proof of Equation (1) follows the standard approach, whereby we break-up the regret estimation into various\npieces, which we estimate separately:\nT\n∑ t=1\n(L⊺t xt − L ⊺ t x) ≤ T\n∑ t=1\n( L ⊺\nt xt − L̂ ⊺ t x̂t )\n︸ ︷︷ ︸\nLemma 3.5\n+ T\n∑ t=1\n(L̂⊺t x̂t − L̂ ⊺ t x)\n︸ ︷︷ ︸\nLemmas 3.3 and 3.4\n+ T\n∑ t=1\n( L̂ ⊺\nt x − L ⊺ t x )\n︸ ︷︷ ︸\nLemma 3.6\n. (2)\nLet Et [−] := E [− | x1, L1, . . . , xt−1, Lt−1, Lt] denote the conditional expectation operator given the history preceding round t and also the adversary’s action in round t. In particular, Ct = Et [ xtx ⊺\nt\n] = (1 − γt)Pt + γt J, with\nPt := Ex∼pt [xx ⊺]. We first establish some basic bounds on quantities occurring in Algorithm 1.\nLemma 3.2 (Basic bounds). Let y ∈ P be arbitrary.\n‖C−1t ‖2 ≤ 1\nγtλ (3)\n‖L̂t‖2 ≤ B\nγtλ (4)\n‖Lt‖2 ≤ B\nλ (5)\nProof. Equation (3) follows from Ct γt J γtλI. Inequality (4) follows via\n|L̂t| = |ℓt · C−1t xt| ≤ |ℓt| · ‖C−1t ‖2 · ‖xt‖2 ≤ B\nγtλ .\nFinally, (5) follows from the estimation\n‖L⊺t ‖2 = ‖L ⊺ t J J −1‖2 = ∥ ∥ ∥Ey∼µ [ L⊺t yy ⊺ J−1 ]∥ ∥ ∥\n2 ≤ Ey∼µ\n[ ‖L⊺t y · y⊺ J−1‖2 ] ≤ Ey∼µ\n\n ‖L⊺t y‖2 ︸ ︷︷ ︸\n≤1\n‖y‖2 ︸︷︷︸\n≤B\n· ‖J−1‖2 ︸ ︷︷ ︸\n≤1/λ\n\n  ≤ B\nλ .\nWe now estimate the pieces of Equation (2). The following series of upper bounds are independent of the concrete choice of the parameters γt, ηt. However, for the reader’s convenience in the last inequality of each estimation we make the bound explicit by substituting the values for γt, ηt by the choices given in Theorem 3.1. We will tacitly use the following inequality to estimate sums like ∑ T t=1 γt:\nT\n∑ t=1\ntα ≤ { Tα+1+α α+1 ≤ T α+1\nα+1 , −1 < α < 0 Tα+1−1\nα+1 + T α ≤ Tα+1α+1 + Tα, α > 0\nWe first estimate the regret when using the loss estimators L̂t. For this we use a generalized variant of EXP (see Lemma A.1), which works with arbitrary convex sets contained in the positive orthant.\nLemma 3.3.\nT\n∑ t=1\n(L̂⊺t x̂t − L̂ ⊺ t x) ≤ KL(a + x, a + x̂1)\nηT + T−1 ∑ t=1 γt + (e − 2) T ∑ t=1 ηt n ∑ i=1 (ai + x̂t,i)L̂ 2 t,i\n≤ 4 KL(a + x, a + x̂1)T 2/3 min{1, 2λT1/3} + 3 4 T2/3 + (e − 2) T\n∑ t=1\nηt n\n∑ i=1\n(ai + x̂t,i)L̂ 2 t,i.\n(6)\nProof. This follows from Lemma A.1 with the L̂t as loss vectors and the a + x̂t as played actions. Note that a cancels on the left hand side in (L̂⊺t (a + x̂t)− L̂ ⊺ t (a + x)).\nIn a next step we estimate the last term of Equation (6).\nLemma 3.4. With probability at least 1 − δ\nT\n∑ t=1\nηt n\n∑ i=1\n(ai + x̂t,i)L̂ 2 t,i ≤ ‖a‖1 + B1 λ T\n∑ t=1 ηt γt + (‖a‖1 + B1)B2 λ2\n√ √ √ √1\n2\nT\n∑ t=1 η2t γ4t · ln 1 δ\n≤ ‖a‖1 + B1 λ 3 4 T2/3 + (‖a‖1 + B1)B2 λ2\n√\n1 2 T ln 1 δ .\n(7)\nProof. This is a special case of the Azuma–Hoeffding inequality (recalled in Theorem B.1) using the bounds\n0 ≤ n\n∑ i=1\n(ai + x̂t,i)L̂ 2 t,i ≤\nn\n∑ i=1 (ai + x̂t,i)\n( B\nγtλ\n)2\n≤ (‖a‖1 + B1)B 2\n(γtλ)2\nand\nEt\n[ n\n∑ i=1\n(ai + x̂t,i)L̂ 2 t,i\n]\n= Et\n[ n\n∑ i=1\n(ai + x̂t,i)ℓ 2 t e ⊺ i C −1 t xtx ⊺ t C −1 t ei\n]\n= n\n∑ i=1\n(ai + x̂t,i)ℓ 2 t e ⊺ i C −1 t CtC −1 t ei\n≤ n\n∑ i=1\n(ai + x̂t,i)e ⊺ i C −1 t ei ≤ ‖a‖1 + B1 γtλ .\nNext we bound the difference between the true loss L ⊺ t xt and the expected estimated loss L̂ ⊺ t x̂t.\nLemma 3.5. With probability at least 1 − δ\nT\n∑ t=1\n( L⊺t xt − L̂ ⊺ t x̂t ) ≤\n(\n1 + B(B + ε)\nλ\n) T\n∑ t=1 γt +\n√ √ √ √2 ( T + (3B + ε)(B + ε)\nλ\nT\n∑ t=1 γt (1 − γt)2\n)\nln 1\nδ\n+ 1\n3\n(\nB √\nγT(1 − γT)λ + 3 +\nB2ε\nλ\n)\nln 1\nδ\n≤ ( 1 + B(B + ε)\nλ\n) T\n∑ t=1\nγt + B\n3 √ γT(1 − γT)λ ln\n1 δ +\n[\n3 + B2ε\nλ + O\n(\nmax\n{\n1, B + ε√\nλ\n}√ T )] max { 1, ln 1\nδ\n}\n≤ ( 1 + B(B + ε)\nλ\n) 3\n4 T2/3 +\n√ 2B 3 √ λ (T1/6 +T−1/6) ln 1 δ + [ 3 + B2ε λ + O ( max { 1, B + ε√ λ }√ T )] max { 1, ln 1 δ } .\n(8)\nProof. Let x̃t := Ex∼pt [x] and xt := Et [xt] = (1 − γt)x̃t + γtu. As also ‖x̂t − x̃t‖2 ≤ γtε, we have xt = (1 − γt)x̂t + γtv for v := u + 1−γtγt (x̃t − x̂t) with ‖v‖2 ≤ B + ε(1 − γt) ≤ B + ε. We consider the martingale difference sequence\nXt := L ⊺ t xt − L̂ ⊺ t x̂t − Et [ L⊺t xt − L̂ ⊺ t x̂t ] = L⊺t xt − L̂ ⊺ t x̂t − L ⊺ t xt + L ⊺ t x̂t = L ⊺ t xt − L̂ ⊺ t x̂t + γtL ⊺ t (x̂t − v).\nNote that as x̃tx̃ ⊺ t = Ex∼pt [x]Ex∼pt [x] ⊺ Ex∼pt [xx⊺] = Pt ≤ Ct/(1 − γt)\n( L̂⊺t x̃t )2 = L̂⊺t x̃t x̃ ⊺ t L̂t = ℓ 2 t x ⊺ t C −1 t x̃t x̃ ⊺ t C −1 t xt ≤ x ⊺ t C −1 t PtC −1 t xt ≤\nx ⊺ t C −1 t xt\n1 − γt ≤ B\n2\nγtλ(1 − γt) ≤ B\n2\nγTλ(1 − γT) ,\nand\n|L̂⊺t x̂t − L̂ ⊺ t x̃t| ≤ ‖L̂t‖2 · ‖x̂t − x̃t‖2 ≤ B2\nγtλ γtε =\nB2ε\nλ ,\nhence\n|Xt| ≤ ∣ ∣L⊺t xt ∣ ∣+ ∣ ∣L⊺t xt ∣ ∣+ ∣ ∣L⊺t x̂t ∣ ∣+ ∣ ∣L̂⊺t x̂t − L̂ ⊺ t x̃t ∣ ∣+ ∣ ∣L̂⊺t x̃t ∣ ∣ ≤ 3 + B\n2ε\nλ +\nB √\nγT(1 − γT)λ ,\nand the variance of Xt is easily bounded by:\nVart [Xt] ≤ Et [ (L⊺t xt − L̂ ⊺ t x̂t) 2 ] = Et [ (ℓt · (1 − x⊺t C−1t x̂t))2 ]\n≤ Et [ (1 − x⊺t C−1t x̂t)2 ] = Et [ 1 − 2x⊺t C−1t x̂t + x̂ ⊺ t C −1 t xtx ⊺ t C −1 t x̂t ]\n= 1− 2xt⊺C−1t x̂t + x̂ ⊺ t C −1 t x̂t = 1− 1 − 2γt (1 − γt)2 xt ⊺C−1t xt + γ2t (1 − γt)2 (v − 2xt)⊺ C−1t v ≤ 1+ γt(3B + ε)(B + ε) (1 − γt)2λ .\nHence Benett’s inequality (Theorem B.2, [Fan et al., 2012, (18)]) applied to the martingale difference sequence Xt provides\nT\n∑ t=1\n( L⊺t xt − L̂ ⊺ t x̂t + γtL ⊺ t (x̂t − v) ) ≤ 1\n3\n(\nB √\nγT(1 − γT)λ + 3 +\nB2ε\nλ\n)\nln 1\nδ\n+ √ √ √ √2 ( T + (3B + ε)(B + ε)\nλ\nT\n∑ t=1 γt (1 − γt)2\n)\nln 1\nδ .\nThe claim follows by using |L⊺t (x̂t − v)| ≤ 1 + B(B + ε)/λ.\nFinally, we bound the difference between the true loss L⊺t x and the estimated loss L̂ ⊺ t x for any point x ∈ P.\nLemma 3.6. For all 0 < δ < 1 with probability at least 1 − δ for every x ∈ Rn simultaneously\nT\n∑ t=1\n( L̂ ⊺\nt x − L ⊺ t x ) ≤ ‖x‖1\n3\n( B\nλ +\n1\nγTλ\n)\nln 2n\nδ + ‖x‖1\n√ √ √ √ 2\nλ\nT\n∑ t=1\n1\nγt ln\n2n\nδ ."
    }, {
      "heading" : "In particular, with probability at least 1 − δ, for all x ∈ P simultaneously",
      "text" : "T\n∑ t=1\n( L̂ ⊺\nt x − L ⊺ t x ) ≤ B1\n3λ\n( B + 2T1/3 ) ln 2n\nδ + B1T\n2/3\n√\n1 + 4\n3T\n√\n3 λ ln 2n δ . (9)\nRemark 3.7. Restricting the statement for all x ≥ 0, the ln(2n/δ) can be replaced by ln(n/δ).\nProof. Let x = ±ei be a coordinate vector or its negation. Then\nVart [ L̂⊺t x − L ⊺ t x ] ≤ Et\n[ (L̂⊺t x) 2 ] ≤ Et [ x⊺C−1t xtx ⊺ t C −1 t x ] = x⊺C−1t x ≤ 1\nγtλ ,\nand\n|L̂⊺t x − L ⊺ t x| ≤ B\nλ +\n1\nγtλ .\nHence by Benett’s inequality (Theorem B.2, [Fan et al., 2012, (18)]) the claim follows for a fixed vector x = ±ei with probability at least 1 − δ/(2n). Hence by the union bound, it holds for all x = ±ei simultaneously with probability at least 1 − δ. Finally, the inequality for a general x follows by taking linear combinations with the absolute values of the coefficients of x.\nSumming up (6), (7), (8) (substituting δ/(2n + 2) for δ in the latter two) and (9) (substituting 2nδ/(2n + 2) for δ), with probability at least 1 − δ yields (1) of Theorem 3.1.\n4 A high-probability regret bound for ComBand\nIn this section we will show that ComBand of Cesa-Bianchia and Lugosi [2012] achieves a high-probability regret bound of O(T2/3) without any modifications. While this is worse than the optimal regret of O( √\nT) obtained by GeometricHedge in Bartlett et al. [2008], it shows that already Algorithm 2, the vanilla version of ComBandwithout any correction terms suffices to achieve a high-probability regret bound.\nTheorem 4.1. With the choice\nηt := γtλ\nB2 and γt :=\nt−1/3\n2\nAlgorithm 2 ComBand\nRequire: Losses Lt, action set A ⊆ Rn, positive parameters η1 ≥ η2 ≥ . . ., 1/2 ≥ γ1 ≥ γ2 ≥ . . . Ensure: actions xt ∈ A 1: for t = 1 to T do 2: wt(x) ← ∑t−1i=1 L̂ ⊺\ni x for all x 3: Wt ← ∑x wt(x) 4: pt(x) ← wt(x)/Wt for all x 5: qt ← (1 − γt)pt + γtµ 6: Sample xt ∼ qt. 7: Observe loss ℓt := L ⊺\nt xt. 8: Ct ← Ex∼qt [xx⊺] 9: L̂t ← ℓtC−1t xt 10: end for\nAlgorithm 2 achieves regret (√\n3B√ λ\n√\nln N + 2\nδ + n 3(e − 2)λ 4B2 + 3 2\n)\nT2/3 +O\n(\nn λ\nB2 +\n(\n1 + B2\nλ\n)\nln N + 2\nδ\n)√ T (10)\n≤ O (\nB√ λ\n√\nln N + 2\nδ + n\nλ\nB2\n)\nT2/3\nwith probability at least 1 − δ for 0 < δ < 1. Remark 4.2. Similar to Theorem 3.1, it is possible to change the ln((N + 2)/δ) in the coefficient of T2/3 to the possibly much smaller ln((n + 2)/δ) with a suitable altering of the other constants. However, since an T1/3 ln N term will still remain in the regret bound, this does not seem to be a significant improvement.\nWe use the same notation as in Section 3.1 for CombEXP, which we recall here for the reader’s convenience. Let Et [−] := E [− | x1, L1, . . . , xt−1, Lt−1, Lt] denote the conditional expectation operator given the history preceding round t and also the adversary’s action in round t. Let x̃t := Ex∼pt [x] and Pt := Ex∼pt [xx ⊺] denote the expectation and variance of distribution pt, respectively. Note that Ct = Et [ xtx ⊺\nt\n] = (1 − γt)Pt + γt J.\nLemma 4.3 (Basic bounds). Let x, y1, and y2 be arbitrary actions.\n(i) Bounds on size\n|y⊺1 C−1t y2| ≤ B2\nγtλ (11)\n|L̂⊺t x| ≤ B2\nγtλ (12)\n(ii) Bounds on expectation\nEt\n[\nx⊺t C −1 t xt\n]\n= n (13)\nProof. Equation (11) follows from the bounds ‖y1‖2, ‖y2‖2 ≤ B and ‖C−1t ‖2 ≤ 1/(γtλ), as Ct γt J γtλI. Inequality (12) follows via\n|L̂⊺t x| = |ℓt · x ⊺ t C −1 t x| ≤\nB2\nγtλ .\nTo prove (13), we use a trick using the trace function to compute the expectation:\nEt\n[\nx⊺t C −1 t xt\n] = Et [ Tr(C−1t xtx ⊺ t ) ] = Tr(C−1t Ct) = Tr(I) = n.\nRemark 4.4. One can similarly prove Ey∼pt [ y⊺P−1t y ] = n, but it will not be used in the following.\nAs in the case of CombEXP, the lemmas below are independent of the choice of the γt, ηt except for the last formula in each lemma, where we particularize the bounds by substituting parameters.\nFirst instead of the real regret, we estimate the regret computed using the estimators L̂t.\nLemma 4.5. With probability at least 1 − δ\nT\n∑ t=1\n(L̂⊺t x̃t − L̂ ⊺ t x) ≤ ln N\nηT + (e − 2)\n n T\n∑ t=1 ηt 1 − γt + B2 λ\n√ √ √ √1\n2\nT\n∑ t=1 η2t γ2t (1 − γt)2 · ln 1 δ\n\n\n≤ 2B 2 ln N\nλ T1/3 + (e − 2)\n(\nn 3λ\n4B2 (T2/3 + 2T1/3) + B√ λ\n√\n2T ln 1\nδ\n)\n.\nProof. By Lemma A.1,\nT\n∑ t=1\n( L̂ ⊺\nt x̃t − L̂ ⊺ t x ) ≤ ln N\nηT + (e − 2)\nT\n∑ t=1\nηt Ey∼pt [ (L̂⊺t y) 2 ] .\nTo estimate the last term, first note that\nEy∼pt [ (L̂⊺t y) 2 ] = Ey∼pt [ L̂⊺t yy ⊺L̂t ] = L̂⊺t Pt L̂t = ℓ 2 t x T t C −1 t PtC −1 t xt ≤ x ⊺ t C −1 t xt\n1 − γt .\nSo far combining our estimates provides\nT\n∑ t=1\n(L̂⊺t x̃t − L̂ ⊺ t x) ≤ ln N\nηT +\nT\n∑ t=1\nηt x⊺t C −1 t xt\n1 − γt =\n2B2 ln N\nλ T1/3 + (e − 2)\nT\n∑ t=1\nηt x⊺t C −1 t xt\n1 − γt . (14)\nTo estimate the last term on the right-hand side, we apply the Azuma–Hoeffding inequality using (11) and (13) for bounding the summands and their expectation, which readily proves the lemma:\nT\n∑ t=1\nηt x⊺t C −1 t xt\n1 − γt ≤ n\nT\n∑ t=1 ηt 1 − γt + B2 λ\n√ √ √ √1\n2\nT\n∑ t=1 η2t γ2t (1 − γt)2 · ln 1 δ .\nWe turn our attention to the difference between the real loss vectors Lt and their estimators L̂t. We start by comparing the loss of the played action.\nLemma 4.6. With probability at least 1 − δ\nT\n∑ t=1\n( L ⊺\nt xt − L̂ ⊺ t x̃t ) ≤ 2\nT\n∑ t=1\nγt + 1\n3\n(\n2 + B √\nγTλ(1 − γT)\n)\nln 1\nδ +\n√ √ √ √2 ( T + 3B2\nλ\nT\n∑ t=1 γt (1 − γt)2\n)\nln 1\nδ\n≤ 3 2 T2/3 + 1 3\n(\n2 + √ 2B√ λ (T1/6 + T−1/6) ) ln 1 δ + √ 2 ( T + 9B2 λ T2/3 ) ln 1 δ .\n(15)\nProof. Let xt := Et [xt] = (1 − γt)x̃t + γtu denote the conditional expectation of xt given the history before round t and loss Lt. The statement is a special case of Benett’s inequality (see Theorem B.2) for the martingale\nXt := L ⊺ t xt − L̂ ⊺ t x̃t − Et [ L⊺t xt − L̂ ⊺ t x̃t ] = L⊺t xt − L̂ ⊺ t x̃t − L ⊺ t xt + L ⊺ t x̃t = L ⊺ t xt − L̂ ⊺ t x̃t + γtL ⊺ t (x̃t − u).\nNote that x̃t x̃t ⊺ Pt by Jensen’s inequality, therefore\n( L̂⊺t x̃t )2 = L̂⊺t x̃tx̃ ⊺ t L̂t = ℓ 2 t x ⊺ t C −1 t x̃t x̃ ⊺ t C −1 t xt ≤ x ⊺ t C −1 t PtC −1 t xt ≤\nx⊺t C −1 t xt\n1 − γt ≤ B\n2\nγtλ(1 − γt) ,\nhence\n|Xt| ≤ 1 + B √\nγtλ(1 − γt) + 2γt ≤ 2 +\nB √\nγTλ(1 − γT) ,\nand the variance of Xt is easily bounded by:\nVart [Xt] ≤ Et [ (L⊺t xt − L̂ ⊺ t x̃t) 2 ] = Et [ (ℓt · (1 − x⊺t C−1t x̃t))2 ]\n≤ Et [ (1 − x⊺t C−1t x̃t)2 ] = Et [ 1 − 2x⊺t C−1t x̃t + x̃ ⊺ t C −1 t xtx ⊺ t C −1 t x̃t ]\n= 1 − 2xt⊺C−1t x̃t + x̃ ⊺ t C −1 t x̃t = 1 − 1 − 2γt (1 − γt)2 xt ⊺C−1t xt + γ2t (1 − γt)2 (u − 2xt)⊺ C−1t u ≤ 1 + 3γtB 2 (1 − γt)2λ .\nBenett’s inequality provides\nT\n∑ t=1\n( L ⊺\nt xt − L̂ ⊺ t x̃t + γtL ⊺ t (xt − u) ) ≤ 1\n3\n(\n2 + B √\nγTλ(1 − γT)\n)\nln 1\nδ +\n√ √ √ √2 ( T + 3B2\nλ\nT\n∑ t=1\nγt\n(1 − γt)2\n)\nln 1\nδ .\nThe claim follows by using |L⊺t (xt − u)| ≤ 2.\nNow we compare the losses Lt with their estimator L̂t for all fixed actions.\nLemma 4.7. For all 0 < δ < 1 with probability at least 1 − δ for every x ∈ A simultaneously\nT\n∑ t=1\n( L̂⊺t x − L ⊺ t x ) ≤ 1\n3\n(\n1 + B2\nγTλ\n)\nln N\nδ +\n√ √ √ √2B 2\nλ\nT\n∑ t=1\n1 γt · ln N δ\n≤ 1 3\n(\n1 + 2B2\nλ T1/3\n)\nln N\nδ + √ 3B√ λ T2/3 √ 1 + 4 3T √ ln N δ .\n(16)\nProof. As customary for concentration inequalities, we start by a variance and size estimate:\nVart [ L̂ ⊺ t x − L ⊺ t x ] ≤ Et\n[ (L̂⊺t x) 2 ] ≤ Et [ x⊺C−1t xtx ⊺ t C −1 t x ] = x⊺C−1t x ≤ B2\nγtλ ,\nand\n|L̂⊺t x − L ⊺ t x| ≤ 1 + B2\nγtλ .\nAlso note that L̂ ⊺ t x − L ⊺\nt x is a martingale difference sequence. Hence by Benett’s inequality (see Theorem B.2) the claim follows for a fixed action x with probability at least 1 − δ/N. Therefore by the union bound, it holds for all x ∈ A simultaneously with probability at least 1 − δ.\nSumming up (14), (15) (substituting δ/(N + 2) for δ), and (16) (substituting Nδ/(N + 2) for δ), we obtain (10) with probability at least 1 − δ."
    }, {
      "heading" : "5 Concluding remarks",
      "text" : "We would like to mention that our method could be immediately strengthened to provide an optimal high-probability regret of O( √ T) using the correction term of GeometricHedge (see Bartlett et al. [2008]) and the identity\nEn\n\n ∑ i∈[d]\nM̃i(n)X̃ 2 i (n)\n\n = En [ X(n)⊺M(n)M(n)⊺Σ+n−1M̃(n)M̃(n) ⊺Σ+n−1 M(n)M(n) ⊺X(n) ] ,\nused for establishing theO( √\nT) regret bound for the expected case under oblivious adversaries in [Combes et al., 2015, supplementary material, proof of Theorem 6]. However, we were unable to verify this identity 1, which is equivalent to\nEn\n\n ∑ i∈[d]\nM̃i(n)X̃i(n) 2\n\n = En\n\n \n\n ∑ i∈[d] M̃i(n)X̃i(n)\n\n\n2 \n  ,\nand as such we only claim the weaker bound of O(T2/3). This is the only obstacle to combining CombEXP with GeometricHedge to obtain an efficient algorithm with optimal high-probability regret O( √\nT) for the adaptive case using our method.\nTo put this into context, without the above identity also for the expected regret case under oblivious adversaries we were only able to establish an O(T2/3) regret bound, matching our high-probability regret bound for adaptive adversaries."
    }, {
      "heading" : "B Concentration inequalities",
      "text" : "We will use the following concentration inequalities.\nTheorem B.1 (Azuma–Hoeffding inequality). For a martingale difference sequence Xt with at ≤ Xt ≤ bt almost surely for constants at, bt, we have with probability at least 1 − δ\nT\n∑ t=1\nXt ≤\n√\n∑ T t=1(bt − at)2 ln(1/δ)\n2 .\nWhile the following inequality is stated only for b = 1 in [Fan et al., 2012, (18)] it easily generalizes via scaling to arbitrary b > 0.\nTheorem B.2 (Benett’s inequality [Fan et al., 2012, (18)]). For a supermartingale difference sequence Xt bounded above by a positive constant Xt ≤ b, for any v ≥ 0 with probability at least 1 − δ:\nT\n∑ t=1\nVart [Xt] ≥ v or T\n∑ t=1\nXt ≤ b ln(1/δ)\n3 +\n√\n2v ln(1/δ)."
    }, {
      "heading" : "C Projection for Kullback–Leibler divergence",
      "text" : "We will now describe a generic, efficient, simple Frank–Wolfe algorithm for the projection step in Line 10 of Algorithm 1. We remark that there are many possibilities for improvements, such as, e.g., employing advanced variants of the Frank–Wolfe algorithm (see e.g., Lacoste-Julien and Jaggi [2015]) or using customized algorithms for specific polytopes. For example, in the case of the simplex P = {x ≥ 0 |∑i xi = 1}, the projection of x is simply x/ ∑ni=1 xi and for the the permutahedron there exist very fast, specialized projection methods (see e.g., Lim and Wright [2016]).\nAlgorithm 4 Projection for KL\nRequire: linear optimization oracle over a polytope P ⊆ [α, β]n, α > 0, upper bound B for the ℓ2-diameter of P, accuracy ε > 0, point x ∈ Rn >0 Ensure: yK ∈ P with KL(z, yK) ≤ KL(z, x) + ε for all z ∈ P\ny0 ∈ P any point K ←\n⌈ 4B4β α3ε2 ⌉\nfor k = 1 to K do s ∈ arg minz∈P ∑ni=1 zi ln(yk−1,i/xi) {Linear optimization oracle call} yk ← ((k − 1)yk−1 + 2s)/(k + 1) end for return yK\nProposition C.1. Given a polytope P ⊆ [α, β]n with α > 0, an upper bound B for the ℓ2-diameter of P, as well as an accuracy ε > 0, Algorithm 4 computes an approximate projection with O ( B4 β\nα3ε2\n)\noracle calls.\nProof. As the algorithm calls the oracle once per iteration, the bound on the number of oracle calls is immediate. To prove the claimed accuracy of the returned point yK, note that the algorithm is the Frank–Wolfe algorithm for the function f (z) := KL(z, x). Recall that the gradient∇ f (z) of f at z is given by (∇ f (z))i = ln(zi/xi) and the Hessian is a diagonalmatrix∇2 f (z) = diag(1/z1, 1/z2, . . . , 1/zn). As 1/β ≤ 1/zi ≤ 1/α for z ∈ P, the function f is 1/αsmooth and 1/β-strongly convex on P in the ℓ2-norm, and has curvature C f ≤ B2/α. Let x∗ := arg minz∈P f (z), i.e., the Bregman projection of x to P. By [Jaggi, 2013, Theorem 1], f (yK)− f (x∗) ≤ 2C f /(K + 2), therefore by strong convexity\n1\n2β ‖yK − x∗‖22 ≤ KL(yK, x)− KL(x∗, x) ≤\n2B2\nα(K + 2) .\nLet z ∈ P be arbitrary. By the Pythagorean Theorem we have KL(z, x∗) ≤ KL(z, x) and thus\nKL(z, yK)− KL(z, x) ≤ KL(z, yK)− KL(z, x∗) = n\n∑ i=1\nzi ln x∗i\nyK,i −\nn\n∑ i=1\nx∗i + n\n∑ i=1 yK,i\n≤ n\n∑ i=1 zi\n( x∗i\nyK,i − 1\n)\n− n\n∑ i=1\nx∗i + n\n∑ i=1\nyK,i = n\n∑ i=1 zi − yK,i yK,i (x∗i − yK,i) ≤ B α ‖x∗ − yK‖2\n≤ B √ 2β\nα\n√ KL(yK, x)− KL(x∗, x) ≤ 2B2\n√ β\nα3/2 √ K + 2 ≤ ε.\nPlugging in K = ⌈\n4B4β α3ε2\n⌉\nas set by the algorithm provides the result."
    }, {
      "heading" : "D Linear decomposition",
      "text" : "For the convenience of the reader, we briefly recall the decomposition algorithm (Algorithm5) ofMirrokni et al. [2015] that for a polytope P approximately decomposes any point x ∈ P into a convex combination of vertices of P, using a linear optimization oracle over P. The algorithm uses Mirror Descent (see Nemirovski [1979]) to find a convex combination.\nProposition D.1 ([Mirrokni et al., 2015, Theorem 3.5]). Given a polytope P with diameter at most 2D in ℓ2-norm, and a point x ∈ P, Algorithm 5 computes with O(D2/ε2) calls to a linear optimization oracle over P a multiset x1, . . . , xk of vertices for k = ⌈4D2/ε2⌉ such that ‖∑ki=1 xi/k − x‖2 ≤ ε.\nAlgorithm 5 Linear decomposition Require: linear optimization oracle over polytope P, an inner point x ∈ P, precision ε Ensure: vertices x1, . . . , xk ∈ P such that ‖x − ∑i λixi/k‖2 ≤ ε\nk ← ⌈4D2/ε2⌉ η ← 4ε(p − 1) y1 ← 0; z1 ← 0 for t = 1 to k do Choose vertex xt ∈ arg miny∈P y ⊺ t y {Linear optimization oracle call}\nzt+1 ← zt − η(x − xt) if ‖zt+1‖2 > 1 then\nyt+1 ← zt+1/‖zt+1‖2 else\nyt+1 ← zt+1 end if\nend for return x1, . . . , xk"
    }, {
      "heading" : "E Fitness of barycentric spanners for exploration",
      "text" : "Let λmin(µ) denote the minimal eigenvalue of the covariance matrix Ex∼µ [xx⊺] of a distribution µ. For exploration one wishes to find a µ with a high minimal eigenvalue λmin(µ). Here we show that a uniform distribution on any approximate barycentric spanner achieves within anO(n2) factor the best possible minimal eigenvalue using any scalar product on Rn. The free choice of scalar product and hence orthonormal basis allows preserving sparse representation of a polytope P.\nLemma E.1. Let v1, . . . , vn be a C-approximate barycentric spanner of a polytope P ⊆ Rn. Then the uniform distribution µv1,...,vn on the spanner satisfies\nλmin(µv1,...,vn) ≥ λmin(µ)\nC2n2\nfor any distribution µ over P.\nProof. Using that the vi form a barycentric spanner, there are coefficients λx,i for all x ∈ P satisfying\nx = ∑ i\nλx,ivi, |λx,i| ≤ C.\nIn particular, with αx := ∑ n i=1|λx,i| ≤ Cn by Jensen’s inequality\nxx⊺ n\n∑ i=1\nαx|λx,i|viv⊺i C2n n\n∑ i=1\nviv ⊺ i .\nHence Ex∼µ [xx⊺] C2n ∑ni=1 viv ⊺ i = C 2n2 Ex∼µv1,...,vn [xx ⊺], from which the claim follows."
    } ],
    "references" : [ {
      "title" : "Competing in the dark: An efficient algorithm for bandit linear optimization",
      "author" : [ "J. Abernethy", "E. Hazan", "A. Rakhlin" ],
      "venue" : "In Proceedings of the 21st Annual Conference on Learning Theory (COLT,",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2008
    }, {
      "title" : "Online bandit learning against an adaptive adversary: from regret to policy regret",
      "author" : [ "R. Arora", "O. Dekel", "A. Tewari" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "Regret in online combinatorial optimization",
      "author" : [ "J.-Y. Audibert", "S. Bubeck", "G. Lugosi" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Audibert et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2013
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire" ],
      "venue" : "Siam J. Comput.,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches",
      "author" : [ "B. Awerbuch", "R.D. Kleinberg" ],
      "venue" : "In Proceedings of the thirty-sixth annual ACM symposium on Theory of computin,",
      "citeRegEx" : "Awerbuch and Kleinberg.,? \\Q2004\\E",
      "shortCiteRegEx" : "Awerbuch and Kleinberg.",
      "year" : 2004
    }, {
      "title" : "High-probability regret bounds for bandit online linear optimization",
      "author" : [ "P.L. Bartlett", "V. Dani", "T. Hayes", "S. Kakade", "A. Rakhlin", "A. Tewari" ],
      "venue" : "In 21th Annual Conference on Learning Theory (COLT",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2008
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Kernel-based methods for bandit convex optimization",
      "author" : [ "S. Bubeck", "R. Eldan", "Y.T. Lee" ],
      "venue" : "arXiv preprint arXiv:1607.03084,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2016
    }, {
      "title" : "Combinatorial bandits",
      "author" : [ "N. Cesa-Bianchia", "G. Lugosi" ],
      "venue" : "Journal of Computer and System Sciences (Special Issue: Cloud Computing",
      "citeRegEx" : "Cesa.Bianchia and Lugosi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cesa.Bianchia and Lugosi.",
      "year" : 2012
    }, {
      "title" : "Combinatorial bandits revisited",
      "author" : [ "R. Combes", "M.S. Talebi Mazraeh Shahi", "A. Proutiere", "M. Lelarge" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Combes et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Combes et al\\.",
      "year" : 2015
    }, {
      "title" : "How to beat the adaptive multi-armed bandit",
      "author" : [ "V. Dani", "T.P. Hayes" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "Dani and Hayes.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dani and Hayes.",
      "year" : 2006
    }, {
      "title" : "The price of bandit information for online optimization",
      "author" : [ "V. Dani", "T.P. Hayes", "S. Kakade" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dani et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2007
    }, {
      "title" : "Maximummatching and a polyhedronwith 0,1-vertices",
      "author" : [ "J. Edmonds" ],
      "venue" : "J. Res. Nat. Bur. Standards B,",
      "citeRegEx" : "Edmonds.,? \\Q1965\\E",
      "shortCiteRegEx" : "Edmonds.",
      "year" : 1965
    }, {
      "title" : "Hoeffding’s inequality for supermartingales",
      "author" : [ "X. Fan", "I. Grama", "Q. Liu" ],
      "venue" : "Stochastic Processes and their Applications,",
      "citeRegEx" : "Fan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2012
    }, {
      "title" : "Geometric algorithms and combinatorial optimization, volume 2 of Algorithms and Combinatorics",
      "author" : [ "M. Grötschel", "L. Lovász", "A. Schrijver" ],
      "venue" : "Springer-Verlag, Berlin, second edition,",
      "citeRegEx" : "Grötschel et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Grötschel et al\\.",
      "year" : 1993
    }, {
      "title" : "An optimal algorithm for bandit convex optimization",
      "author" : [ "E. Hazan", "Y. Li" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "Hazan and Li.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hazan and Li.",
      "year" : 2016
    }, {
      "title" : "Volumetric spanners: an efficient exploration basis for learning",
      "author" : [ "E. Hazan", "Z. Karnin", "R.Meka" ],
      "venue" : "In JMLR:Workshop and Conference Proceedings,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2014
    }, {
      "title" : "Revisiting Frank–Wolfe: Projection-free sparse convex optimization",
      "author" : [ "M. Jaggi" ],
      "venue" : "InProceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Jaggi.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jaggi.",
      "year" : 2013
    }, {
      "title" : "On the global linear convergence of Frank–Wolfe optimization variants",
      "author" : [ "S. Lacoste-Julien", "M. Jaggi" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lacoste.Julien and Jaggi.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lacoste.Julien and Jaggi.",
      "year" : 2015
    }, {
      "title" : "Efficient Bregman projections onto the permutahedron and related polytopes",
      "author" : [ "C.H. Lim", "S.J. Wright" ],
      "venue" : "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Lim and Wright.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lim and Wright.",
      "year" : 2016
    }, {
      "title" : "Tight bounds for approximate Carathéodory and beyond",
      "author" : [ "V.S. Mirrokni", "R.P. Leme", "A. Vladu", "S.C. wai Wong" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "Mirrokni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mirrokni et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient methods for large-scale convex optimization problems",
      "author" : [ "A. Nemirovski" ],
      "venue" : "Ekonomika i MatematicheskieMetody,",
      "citeRegEx" : "Nemirovski.,? \\Q1979\\E",
      "shortCiteRegEx" : "Nemirovski.",
      "year" : 1979
    }, {
      "title" : "Advances in convex optimization: Conic programming",
      "author" : [ "A. Nemirovski" ],
      "venue" : "In Proceedings of the International Congress of Mathematicians. EMS-European Mathematical Society Publishing House,",
      "citeRegEx" : "Nemirovski.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nemirovski.",
      "year" : 2006
    }, {
      "title" : "Improved high-probability regret bounds for non-stochastic bandits",
      "author" : [ "G. Neu" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Neu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neu.",
      "year" : 2015
    }, {
      "title" : "The matching polytope has exponential extension complexity",
      "author" : [ "T. Rothvoß" ],
      "venue" : "Proceedings of STOC, pages 263–272,",
      "citeRegEx" : "Rothvoß.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rothvoß.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "For the linear bandit problem, we extend the analysis of algorithm CombEXP from Combes et al. [2015] to the high-probability case against adaptive adversaries, allowing actions to come from an arbitrary polytope.",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "While this bound is weaker than the optimal O( √ T) bound achieved by GeometricHedge in Bartlett et al. [2008], CombEXP is computationally efficient, requiring only an efficient linear optimization oracle over the convex hull of the actions.",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 17,
      "context" : "For algorithms with bandit feedback, exploration (occasionally playing random actions for learning) is a crucial feature, however it does not have to be explicit as recently shown in Neu [2015], where exploration is achieved via skewing loss estimators.",
      "startOffset" : 183,
      "endOffset" : 194
    }, {
      "referenceID" : 2,
      "context" : "The variant EXP3 for multi-armed bandit problems first appeared in Auer et al. [2002], however optimal high-probability regret bounds were first achieved in Dani and Hayes [2006].",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "The variant EXP3 for multi-armed bandit problems first appeared in Auer et al. [2002], however optimal high-probability regret bounds were first achieved in Dani and Hayes [2006]. The linear bandit setting is a generalization of the multi-armed bandit setting where, utilizing the linearity of losses, the goal is to improve the dependence on the number of actions in the regret bound, which might be exponential in the dimension n.",
      "startOffset" : 67,
      "endOffset" : 179
    }, {
      "referenceID" : 2,
      "context" : "The variant EXP3 for multi-armed bandit problems first appeared in Auer et al. [2002], however optimal high-probability regret bounds were first achieved in Dani and Hayes [2006]. The linear bandit setting is a generalization of the multi-armed bandit setting where, utilizing the linearity of losses, the goal is to improve the dependence on the number of actions in the regret bound, which might be exponential in the dimension n. At the same time linear losses come naturally into play when considering actions with a combinatorial structure, such as e.g., matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al.",
      "startOffset" : 67,
      "endOffset" : 631
    }, {
      "referenceID" : 2,
      "context" : ", matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al. [2013] for an extensive discussion.",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : ", matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al. [2013] for an extensive discussion. For the linear bandit setting, the EXP-variant ComBand (Combinatorial Bandit) from Cesa-Bianchia and Lugosi [2012] has optimal O( √ T) expected regret, and in Bartlett et al.",
      "startOffset" : 74,
      "endOffset" : 241
    }, {
      "referenceID" : 2,
      "context" : ", matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al. [2013] for an extensive discussion. For the linear bandit setting, the EXP-variant ComBand (Combinatorial Bandit) from Cesa-Bianchia and Lugosi [2012] has optimal O( √ T) expected regret, and in Bartlett et al. [2008] the modified version GeometricHedge achieves O( √ T) regret with high probability.",
      "startOffset" : 74,
      "endOffset" : 308
    }, {
      "referenceID" : 2,
      "context" : ", matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al. [2013] for an extensive discussion. For the linear bandit setting, the EXP-variant ComBand (Combinatorial Bandit) from Cesa-Bianchia and Lugosi [2012] has optimal O( √ T) expected regret, and in Bartlett et al. [2008] the modified version GeometricHedge achieves O( √ T) regret with high probability. While these regret bounds practically do not depend on the number of actions, both maintain a distribution over the (possibly exponentially large) action set A, which is infeasible in general due to the large data size, even though ComBand is still efficient for many specific problems. Recently, a modification of the ComBand algorithm called CombEXP (see Algorithm 1) was derived in Combes et al. [2015], which achieves general computational efficiency by not maintaining a distribution of xt, but only the desired expectation x̂t of the distribution, and generating a new sparse approximate distribution at every round.",
      "startOffset" : 74,
      "endOffset" : 797
    }, {
      "referenceID" : 12,
      "context" : "The maximal matching problem is a good example where the linear programming oracle approach is useful, as it has a polynomial time linear optimization algorithm Edmonds [1965], but no polynomial-size polyhedral description Rothvoß [2014].",
      "startOffset" : 161,
      "endOffset" : 176
    }, {
      "referenceID" : 12,
      "context" : "The maximal matching problem is a good example where the linear programming oracle approach is useful, as it has a polynomial time linear optimization algorithm Edmonds [1965], but no polynomial-size polyhedral description Rothvoß [2014].",
      "startOffset" : 161,
      "endOffset" : 238
    }, {
      "referenceID" : 4,
      "context" : "The algorithm ComBand first appeared in Cesa-Bianchia and Lugosi [2012], while GeometricHedge comes from Bartlett et al.",
      "startOffset" : 40,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "The algorithm ComBand first appeared in Cesa-Bianchia and Lugosi [2012], while GeometricHedge comes from Bartlett et al. [2008], and CombEXP appeared in Combes et al.",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "The algorithm ComBand first appeared in Cesa-Bianchia and Lugosi [2012], while GeometricHedge comes from Bartlett et al. [2008], and CombEXP appeared in Combes et al. [2015]. Using interior point methods, an efficient algorithm with O( √ T) expected regret for linear bandit problems has been established in Abernethy et al.",
      "startOffset" : 105,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "Using interior point methods, an efficient algorithm with O( √ T) expected regret for linear bandit problems has been established in Abernethy et al. [2008]. For multiarmed bandit problems, the original version of EXP3 has high-probability regret Ω(T2/3) against some adaptive adversaries [Dani and Hayes, 2006, Theorem 1.",
      "startOffset" : 133,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "Using interior point methods, an efficient algorithm with O( √ T) expected regret for linear bandit problems has been established in Abernethy et al. [2008]. For multiarmed bandit problems, the original version of EXP3 has high-probability regret Ω(T2/3) against some adaptive adversaries [Dani and Hayes, 2006, Theorem 1.2], however variants with optimal O( √ T) regret exists, e.g., using accountants to control the exploration rate (see Dani and Hayes [2006]), or via the recent EXP3-IX with implicit exploration (see Neu [2015]).",
      "startOffset" : 133,
      "endOffset" : 462
    }, {
      "referenceID" : 0,
      "context" : "Using interior point methods, an efficient algorithm with O( √ T) expected regret for linear bandit problems has been established in Abernethy et al. [2008]. For multiarmed bandit problems, the original version of EXP3 has high-probability regret Ω(T2/3) against some adaptive adversaries [Dani and Hayes, 2006, Theorem 1.2], however variants with optimal O( √ T) regret exists, e.g., using accountants to control the exploration rate (see Dani and Hayes [2006]), or via the recent EXP3-IX with implicit exploration (see Neu [2015]).",
      "startOffset" : 133,
      "endOffset" : 532
    }, {
      "referenceID" : 0,
      "context" : "Using interior point methods, an efficient algorithm with O( √ T) expected regret for linear bandit problems has been established in Abernethy et al. [2008]. For multiarmed bandit problems, the original version of EXP3 has high-probability regret Ω(T2/3) against some adaptive adversaries [Dani and Hayes, 2006, Theorem 1.2], however variants with optimal O( √ T) regret exists, e.g., using accountants to control the exploration rate (see Dani and Hayes [2006]), or via the recent EXP3-IX with implicit exploration (see Neu [2015]). For convex loss functions, optimal regret bounds have been obtained in Hazan and Li [2016] with running time being poly-exponential in the dimension, and in [Bubeck et al.",
      "startOffset" : 133,
      "endOffset" : 625
    }, {
      "referenceID" : 0,
      "context" : "Using interior point methods, an efficient algorithm with O( √ T) expected regret for linear bandit problems has been established in Abernethy et al. [2008]. For multiarmed bandit problems, the original version of EXP3 has high-probability regret Ω(T2/3) against some adaptive adversaries [Dani and Hayes, 2006, Theorem 1.2], however variants with optimal O( √ T) regret exists, e.g., using accountants to control the exploration rate (see Dani and Hayes [2006]), or via the recent EXP3-IX with implicit exploration (see Neu [2015]). For convex loss functions, optimal regret bounds have been obtained in Hazan and Li [2016] with running time being poly-exponential in the dimension, and in [Bubeck et al., 2016, Theorem 1] with polynomial running time provided the number of constraints of the underlying polytope is polynomial in the dimension. However the case of convex loss does not subsume the combinatorial/linear case, as with convex loss all inner points of the convex set are actions; with linear losses the actions are limited to the vertices of the underlying polytope in most cases. We refer the interested reader to the excellent survey of Bubeck and Cesa-Bianchi [2012] on bandit problems.",
      "startOffset" : 133,
      "endOffset" : 1185
    }, {
      "referenceID" : 9,
      "context" : "Our main contribution is a high-probability regret bound for CombEXP from Combes et al. [2015] for adaptive adversaries over actions coming from arbitrary polytopes P ⊆ Rn.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation).",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster’s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L⊺t x| ≤ 1 for all x ∈ A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that ‖x‖2 ≤ B and ‖x‖1 ≤ B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution μ on A for exploration, whose fitness for exploration is measured by a positive lower bound λ on the smallest eigenvalue of its covariance matrix J: J := Ey∼μ [yy] λI.",
      "startOffset" : 114,
      "endOffset" : 1003
    }, {
      "referenceID" : 1,
      "context" : "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster’s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L⊺t x| ≤ 1 for all x ∈ A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that ‖x‖2 ≤ B and ‖x‖1 ≤ B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution μ on A for exploration, whose fitness for exploration is measured by a positive lower bound λ on the smallest eigenvalue of its covariance matrix J: J := Ey∼μ [yy] λI. Here and below we denote by M N that N − M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then μ is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John’s decomposition (John’s exploration; see Dani et al.",
      "startOffset" : 114,
      "endOffset" : 1519
    }, {
      "referenceID" : 1,
      "context" : "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster’s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L⊺t x| ≤ 1 for all x ∈ A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that ‖x‖2 ≤ B and ‖x‖1 ≤ B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution μ on A for exploration, whose fitness for exploration is measured by a positive lower bound λ on the smallest eigenvalue of its covariance matrix J: J := Ey∼μ [yy] λI. Here and below we denote by M N that N − M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then μ is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John’s decomposition (John’s exploration; see Dani et al. [2007]), transferred to A.",
      "startOffset" : 114,
      "endOffset" : 1705
    }, {
      "referenceID" : 1,
      "context" : "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster’s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L⊺t x| ≤ 1 for all x ∈ A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that ‖x‖2 ≤ B and ‖x‖1 ≤ B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution μ on A for exploration, whose fitness for exploration is measured by a positive lower bound λ on the smallest eigenvalue of its covariance matrix J: J := Ey∼μ [yy] λI. Here and below we denote by M N that N − M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then μ is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John’s decomposition (John’s exploration; see Dani et al. [2007]), transferred to A. In the latter two cases, J = I and λ = 1/n using the scalar product on R n induced by the additional structure. John’s ellipsoid can be approximately estimated with a worse lower bound λ = 1/n3/2 by Grötschel et al. [1993], however a constant factor approximation is NP-hard by Nemirovski [2006].",
      "startOffset" : 114,
      "endOffset" : 1948
    }, {
      "referenceID" : 1,
      "context" : "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster’s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L⊺t x| ≤ 1 for all x ∈ A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that ‖x‖2 ≤ B and ‖x‖1 ≤ B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution μ on A for exploration, whose fitness for exploration is measured by a positive lower bound λ on the smallest eigenvalue of its covariance matrix J: J := Ey∼μ [yy] λI. Here and below we denote by M N that N − M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then μ is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John’s decomposition (John’s exploration; see Dani et al. [2007]), transferred to A. In the latter two cases, J = I and λ = 1/n using the scalar product on R n induced by the additional structure. John’s ellipsoid can be approximately estimated with a worse lower bound λ = 1/n3/2 by Grötschel et al. [1993], however a constant factor approximation is NP-hard by Nemirovski [2006]. Recall that a barycentric spanner is a linear basis v1, .",
      "startOffset" : 114,
      "endOffset" : 2021
    }, {
      "referenceID" : 1,
      "context" : "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster’s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L⊺t x| ≤ 1 for all x ∈ A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that ‖x‖2 ≤ B and ‖x‖1 ≤ B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution μ on A for exploration, whose fitness for exploration is measured by a positive lower bound λ on the smallest eigenvalue of its covariance matrix J: J := Ey∼μ [yy] λI. Here and below we denote by M N that N − M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then μ is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John’s decomposition (John’s exploration; see Dani et al. [2007]), transferred to A. In the latter two cases, J = I and λ = 1/n using the scalar product on R n induced by the additional structure. John’s ellipsoid can be approximately estimated with a worse lower bound λ = 1/n3/2 by Grötschel et al. [1993], however a constant factor approximation is NP-hard by Nemirovski [2006]. Recall that a barycentric spanner is a linear basis v1, . . . , vn in P (the convex hull of A), such that every element of P is a linear combination of the vi with coefficients from [−1,+1]. The basis v1, . . . , vn is a C-approximate barycentric spanner for some C > 1 if every element of P is a linear combination of the vi with coefficients from [−C,+C]. A Capproximate barycentric spanners can be efficiently computed by O(n2 ln n/ ln C) calls to a linear optimization oracle over P by Awerbuch and Kleinberg [2004], which actually computes a spanner consisting of vertices of P.",
      "startOffset" : 114,
      "endOffset" : 2542
    }, {
      "referenceID" : 1,
      "context" : "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster’s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L⊺t x| ≤ 1 for all x ∈ A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that ‖x‖2 ≤ B and ‖x‖1 ≤ B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution μ on A for exploration, whose fitness for exploration is measured by a positive lower bound λ on the smallest eigenvalue of its covariance matrix J: J := Ey∼μ [yy] λI. Here and below we denote by M N that N − M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then μ is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John’s decomposition (John’s exploration; see Dani et al. [2007]), transferred to A. In the latter two cases, J = I and λ = 1/n using the scalar product on R n induced by the additional structure. John’s ellipsoid can be approximately estimated with a worse lower bound λ = 1/n3/2 by Grötschel et al. [1993], however a constant factor approximation is NP-hard by Nemirovski [2006]. Recall that a barycentric spanner is a linear basis v1, . . . , vn in P (the convex hull of A), such that every element of P is a linear combination of the vi with coefficients from [−1,+1]. The basis v1, . . . , vn is a C-approximate barycentric spanner for some C > 1 if every element of P is a linear combination of the vi with coefficients from [−C,+C]. A Capproximate barycentric spanners can be efficiently computed by O(n2 ln n/ ln C) calls to a linear optimization oracle over P by Awerbuch and Kleinberg [2004], which actually computes a spanner consisting of vertices of P. In this paper we deliberately avoid using the scalar product induced by the structure to be able to directly use the bounds available in the original space of the problem. Fortunately, the uniform distribution on an approximate barycentric spanner has a close to optimal minimal eigenvalue even in the original space, see Lemma E.1, which allows us to preserve sparsity of the original space. As such we assume that we have access to an exploration distribution over actions with sparse support of size n, where n is the dimension of the vector space, from which we can efficiently sample. Note that for specific problems exploration distributions with better minimal eigenvalue can be explicitly given; we refer the interested reader to Cesa-Bianchia and Lugosi [2012] and follow-up work for a large set of such examples.",
      "startOffset" : 114,
      "endOffset" : 3376
    }, {
      "referenceID" : 9,
      "context" : "Except for the shifting vector a, these ideas already appeared in Combes et al. [2015]. The algorithmcontains four resource-consumingsteps: (1) projection (Line 10), (2) distributiongeneration (Line 3), (3) sampling from the distribution, and (4) computing the covariance matrix.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "Except for the shifting vector a, these ideas already appeared in Combes et al. [2015]. The algorithmcontains four resource-consumingsteps: (1) projection (Line 10), (2) distributiongeneration (Line 3), (3) sampling from the distribution, and (4) computing the covariance matrix. All the other steps are fast, depending only polynomially on the dimension. Themajor factor for the running time of sampling from the distribution (3), and computing the covariancematrix (4) is the sparsity of the generated distribution, i.e., the number of possible outcomes. Sparse distributions (number of outcomes polynomial in the dimension) of sufficient accuracy can be efficiently generated by the decomposition algorithm fromMirrokni et al. [2015], which we summarize as Algorithm 5 in Section D for the reader’s convenience.",
      "startOffset" : 66,
      "endOffset" : 737
    }, {
      "referenceID" : 7,
      "context" : "In this section we will show that ComBand of Cesa-Bianchia and Lugosi [2012] achieves a high-probability regret bound of O(T2/3) without any modifications.",
      "startOffset" : 45,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "While this is worse than the optimal regret of O( √ T) obtained by GeometricHedge in Bartlett et al. [2008], it shows that already Algorithm 2, the vanilla version of ComBandwithout any correction terms suffices to achieve a high-probability regret bound.",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "We would like to mention that our method could be immediately strengthened to provide an optimal high-probability regret of O( √ T) using the correction term of GeometricHedge (see Bartlett et al. [2008]) and the identity",
      "startOffset" : 181,
      "endOffset" : 204
    } ],
    "year" : 2017,
    "abstractText" : "For the linear bandit problem, we extend the analysis of algorithm CombEXP from Combes et al. [2015] to the high-probability case against adaptive adversaries, allowing actions to come from an arbitrary polytope. We prove a high-probability regret of O(T2/3) for time horizon T. While this bound is weaker than the optimal O( √ T) bound achieved by GeometricHedge in Bartlett et al. [2008], CombEXP is computationally efficient, requiring only an efficient linear optimization oracle over the convex hull of the actions.",
    "creator" : "LaTeX with hyperref package"
  }
}