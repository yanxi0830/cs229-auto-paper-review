{
  "name" : "1602.01690.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Minimizing the Maximal Loss: How and Why",
    "authors" : [ "Shai Shalev-Shwartz", "Yonatan Wexler" ],
    "emails" : [ "SHAIS@CS.HUJI.AC.IL", "YONATAN.WEXLER@ORCAM.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In a typical supervised learning scenario, we have training examples, S = ((x1, y1), . . . , (xm, ym)) ∈ (X ×Y)m, and our goal is to learn a function h : X → Y . We focus on the case in which h is parameterized by a vectorw ∈ W ⊂ Rd, and we use hw to denote the function induced by w. The performance of w on an example (x, y) is assessed using a loss function, ` :W ×X × Y → [0, 1]. A commonly used learning rule is to approximately minimize the average loss, namely,\nmin w∈W\nLavg(w) := 1\nm\nm∑\ni=1\n`(w, xi, yi) . (1)\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nAnother option is to approximately minimize the maximal loss, namely,\nmin w∈W Lmax(w) := max i∈[m] `(w, xi, yi) . (2)\nObviously, if there existsw∗ ∈ W such that `(w∗, xi, yi) = 0 for every i then the minimizers of both problems coincide. However, approximate solutions can be very different. In particular, since Lmax(w) ≥ Lavg(w) for every w, the guarantee Lmax(w) < is stronger than the guarantee Lavg(w) < . Furthermore, for binary classification with the zero-one loss, any vector for which Lmax(w) < 1 must predict all the labels on the training set correctly, while the guarantee Lavg(w) < 1 is meaningless.\nSome classical machine learning algorithms can be viewed as approximately minimizing Lmax. For example, HardSVM effectively solves Lmax with respect to the loss function `(w, xi, yi) = λ‖w‖2 + 1[yi〈w, xi〉 < 1]. However, minimizingLavg is a more popular approach, especially for deep learning problems, in whichw is the vector of weights of a neural network and the optimization is performed using variants of stochastic gradient descent (SGD). There are several reasons to prefer Lavg over Lmax:\n1. If m is very large, it is not practical to perform operations on the entire training set. Instead, we prefer iterative algorithms that update w based on few examples at each iteration. This can be easily done for Lavg by observing that if we sample i uniformly at random from [m], then the gradient of `(w, xi, yi) with respect to w is an unbiased estimator of the gradient of Lavg(w). This property, which lies at the heart of the SGD algorithm, does not hold for Lmax.\n2. Our ultimate goal is not to minimize the loss on the training set but instead to have a small loss on unseen examples. As argued before, approximately minimizing Lmax can lead to a smaller loss on the training set, but it is not clear if this added accuracy will also be reflected in performance on unseen examples. Formal\nar X\niv :1\n60 2.\n01 69\n0v 2\n[ cs\n.L G\n] 2\n2 M\nay 2\n01 6\narguments of this nature were given in (Bousquet & Bottou, 2008; Shalev-Shwartz & Srebro, 2008).\n3. The objective Lmax is not robust to outliers. It is easy to see that even a single outlier can make the minimizer of Lmax meaningless.\nIn this paper we tackle the aforementioned disadvantages of Lmax, and by doing so, we show cases in which Lmax is preferable. In particular:\n1. We describe and analyze a meta algorithm that can take any online learner for w and convert it to a minimizer of Lmax. A detailed description of our meta algorithm, its analysis, and a comparison to other approaches, are given in Section 2.\n2. The arguments in (Bousquet & Bottou, 2008; ShalevShwartz & Srebro, 2008) rely on a comparison of upper bounds. We show that these upper bounds are not tight in many cases. Furthermore, we analyze the sample complexity of learning in situations where the training examples are divided to “typical” scenarios and “rare” scenarios. We argue that in many practical cases, our goal is to have a high accuracy on both typical and rare examples. We show conditions under which minimizing even few rare examples suffice to guarantee good performance on unseen examples from the rare scenario. In other words, few examples can have a dramatic effect on the performance of the learnt classifier on unseen examples. This is described and analyzed in Section 3.\n3. Finally, in Section 4 we review standard techniques for generalizing the results from realizable cases to scenarios in which there might be outliers in the data.\nTo summarize, we argue that in some situations minimizing Lmax is better than minimizing Lavg. We address the “how” question in Section 2, the “why” question in Section 3, and the issue of robustness in Section 4. Finally, in Section 5 we provide some empirical evidence, showing the effectiveness of our algorithm on real world learning problems."
    }, {
      "heading" : "2. How",
      "text" : "In this section we describe and analyze an algorithmic framework for approximately solving the optimization problem given in (2).\nDenote by Sm = {p ∈ [0, 1]m : ‖p‖1 = 1} the probabilistic simplex over m items. We also denote by Λ : W → [0, 1]m the function defined by\nΛ(w) = (`(w, x1, y1), . . . , `(w, xm, ym)) .\nThe first step is to note that the optimization problem given in (2) is equivalent to\nmin w∈W max p∈Sm\n〈p,Λ(w)〉 . (3)\nThis is true because for every w, the p that maximizes the inner optimization is the all zeros vector except 1 in the coordinate for which `(w, xi, yi) is maximal.\nWe can now think of (3) as a zero-sum game between twoplayers. The p player tries to maximize 〈p,Λ(w)〉 while the w player tries to minimize 〈p,Λ(w)〉. The optimization process is comprised of T game rounds. At round t, the p player defines pt ∈ Sm and the w player defines wt ∈ W . We then sample it ∼ pt and define the value of the round to be `(wt, xit , yit).\nTo derive a concrete algorithm we need to specify how player p picks pt and how player w picks wt. For the w player one can use any online learning algorithm. We specify the requirement from the algorithm below.\nDefinition 1 (Mistake bound for the w player) We say that the w player enjoys a mistake bound of C if for every sequence of indices (i1, . . . , iT ) ∈ [m]T we have that\nT∑\nt=1\n`(wt, xit , yit) ≤ C . (4)\nExample 1 Consider a binary classification problem in which the data is linearly separable by a vector w∗ with a margin of 1. Let the loss function be the zero-one loss, namely, `(w, x, y) = 1[y〈w, x〉 ≤ 0], where 1[boolean expression] is 1 if the boolean expression holds and 0 otherwise. We can use the online Perceptron algorithm as our w learner and it is well known that the Perceptron enjoys the mistake bound of C = ‖w∗‖2 maxi∈[m] ‖xi‖2 (for a reference, see for example (Shalev-Shwartz, 2011)).\nFor the p player, we use the seminal work of (Auer et al., 2002). In particular, recall that the goal of the p player is to maximize the loss, `(wt, xit , yit , where it ∼ pt. The basic idea of the construction is therefore to think of the m examples as m slot machines, where at round t the gain of pulling the arms of the different machines is according to Λ(wt) ∈ [0, 1]m. Crucially, the work of (Auer et al., 2002) does not assume that Λ(wt) are sampled from a fixed distribution, but rather the vectors Λ(wt) can be chosen by an adversary. As observed in Auer et al. (2002, Section 9), this naturally fits zero-sum games, as we consider here.\nIn (Auer et al., 2002) it is proposed to rely on the algorithm EXP3.P.1 as the strategy for the p-player. The acronym EXP3 stands for Exploration-Exploitation-Exponent, because the algorithm balances between exploration and ex-\nploitation and rely on an exponentiated gradient framework. The “P” in EXP3.P.1 stands for a regret bound that holds with high probability. This is essential for our analysis because we will later apply a union bound over the m examples. While the EXP3.P.1 algorithm gives the desired regret analysis, the runtime per iteration of this algorithm scales with m. Here, we propose another variant of EXP3 for which the runtime per iteration is O(log(m)).\nTo describe our strategy for the p player, recall that it maintains pt ∈ Sm. We will instead maintain another vector, qt ∈ Sm, and will set pt to be the vector such that pt,i = 1 2qt,i + 1 2m . That is, pt is a half-half mix of qt with the uniform distribution. While in general such a strong mix with the uniform distribution can hurt the regret, in our case it only affects the convergence rate by a constant factor. On the up side, this strong exploration helps us having an update step that takes O(log(m)) per iteration.\nRecall that at round t of the algorithm, we sample it ∼ pt and the value of the round is `(wt, xit , yit). Denote zt = − `it (wt)pit eit , then it is easy to verify that Eit∼pt [zt] = −Λ(wt). Therefore, applying gradient descent with respect to the linear function 〈·, zt〉 is in expectation equivalent to applying gradient descent with respect to the linear function −〈·,Λ(wt)〉, which is the function the p player aims at minimizing. Instead of gradient descent, we use the exponentiated gradient descent approach which applies gradient descent in the log space, namely, the update can be written as log(qt+1) = log(qt) + ηzt.\nA pseudo-code of the resulting algorithm is given in Section 2.3. Observe that we use a tree structure to hold the vector q, and since all but the it coordinate of zt are zeros, we can implement the update of q in O(log(m)) time per iteration. The following theorem summarizes the convergence of the resulting algorithm.\nTheorem 1 Suppose the we have an oracle access to an online algorithm that enjoys a mistake bound of C with respect to the training examples (x1, y1), . . . , (xm, ym). Fix , δ, and suppose we run the FOL algorithm with T, k such that C/T ≤ /8, T = Ω(m log(m/δ)/ ), and k = Ω(log(m/δ)/ ), and with η = 1/(2m). Then, with probability of at least 1− δ,\nmax i\n1\nk\nk∑\nj=1\n`(wtj , xi, yi) ≤ .\nThe proof of the theorem is given in Appendix A.\nThe above theorem tells us that we can find an ensemble of O(log(m)/ ) predictors, such that the ensemble loss is smaller than for all of the examples.\nWe next need to show that we can construct a single pre-\ndictor with a small loss. To do so, we consider two typical scenarios. The first is classification settings, in which `(w, x, y) is the zero-one loss and the second is convex losses in which `(w, x, y) has the form φy(hw(x)), where for every y, φy is a convex function."
    }, {
      "heading" : "2.1. Classification",
      "text" : "In classification, `(w, x, y) is the zero-one loss, namely, it equals to zero if hw(x) = y and it equals to 1 if hw(x) 6= y. We will take to be any number strictly smaller than 1/2, say 0.499.\nObserve that Theorem 1 tells us that the average loss of the classifiers wt1 , . . . , wtk is smaller than = 0.499. Since the values of the loss are either 1 or 0, it means that the loss of more than 1/2 of the classifiers is 0, which implies that the majority classifier has a zero loss.\nCorollary 1 Assume that `(w, x, y) is the zero-one loss function, namely, `(w, x, y) = 1[hw(x) 6= y]. Apply Theorem 1 with = 0.49. Then, with probability of at least 1 − δ, the majority classifier of hwt1 , . . . , hwtk is consistent, namely, it makes no mistakes on the entire training set.\nExample 2 Consider again the linear binary classification problem given in Example 1, where we use the online Perceptron algorithm as our w learner, and its mistake bound is C as given in Example 1. Then, after Õ (m+ C) iterations, we will find an ensemble of O(log(m)) halfspaces, whose majority vote is consistent with all the examples. In Section 2.4 we compare the runtime of the method to stateof-the-art approaches. Here we just note that to obtain a consistent hypothesis using SGD one needs order of mC iterations, which is significantly larger in most scenarios."
    }, {
      "heading" : "2.2. Convex Losses",
      "text" : "Consider now the case in which `(w, x, y) has the form φy(hw(x)), where for every y, φy is a convex function. Note that this assumption alone does not imply that ` is a convex function of w (this will be true only if hw(x) is an affine function).\nIn the case of convex φy , combining Theorem 1 with Jensen’s inequality we obtain:\nCorollary 2 Under the assumptions of Theorem 1, if `(w, x, y) has the form φy(hw(x)), where for every y, φy is a convex function, then the predictor h(x) = 1 k ∑k j=1 hwtj (x) satisfies ∀i, φyi(h(xi)) ≤ . If we further assume that hw(x) is an affine function of w, and let w = 1k ∑k j=1 wtj , then we also have that\n∀i, φyi(hw(xi)) ≤ ."
    }, {
      "heading" : "2.3. Pseudo-code",
      "text" : "Below we describe a pseudo-code of the algorithm. We rely on a tree data structure for maintaining the probability of the p-player. It is easy to verify the correctness of the implementation. Observe that the runtime of each iteration is the time required to perform one step of the online learner plusO(log(m)) for sampling from pt and updating the tree structure.\nFocused Online Learning (FOL)\nInput: Training examples (x1, y1), . . . , (xm, ym) Loss function ` :W ×X × Y → [0, 1] Parameters η, T, k Oracle access to online learning algorithm OLA Initialization: Tree.initialize(m) (see the Tree pseudo-code) w1 = OLA.initialize() Loop over t ∈ {1, . . . , T}: (it, pit) = Tree.sample(1/2) OLA.step(xit , yit) Tree.update(it, exp(η `(wt, xit , yit)/pit)) Output: Sample (t1, . . . , tk) indices uniformly from [T ] Output Majority/Average of (hwt1 , . . . , hwtk )\nTree\ninitialize(m) Build a full binary tree of height h = dlog2(m)e Set value of the first m leaves to 1 and the rest to 0 Set the value of each internal node to be\nthe sum of its two children Let qi be the value of the i’th leaf divided by\nthe value of the root sample(γ)\nSample b ∈ {0, 1} s.t. P[b = 0] = γ If b = 0\nSample i uniformly at random from [m] Else\nSet v to be the root node of the tree While v is not a leaf:\nGo to the left/right child by sampling according to their values\nLet i be the obtained leaf Return: (i, γ/m+ (1− γ)qi)\nupdate(i, f ) Let v be the current value of the i’th leaf of the tree Let δ = f v − v Add δ to the values of all nodes on\nthe path from the i’th leaf to the root"
    }, {
      "heading" : "2.4. Related Work",
      "text" : "As mentioned before, our algorithm is a variant of the approach given in Auer et al. (2002, Section 9), but has the advantage that the update of the p player at each iteration scales with log(m) rather than with m. Phrasing the maxloss minimization as a two players game has also been proposed by (Clarkson et al., 2012; Hazan et al., 2011). These works focus on the specific case of binary classification with a linear predictor, namely, they tackle the problem minw∈Rd:‖w‖2≤1 maxp∈Sm ∑ i pi〈w, xi〉. Assuming the setup of Example 1, (Clarkson et al., 2012) presents an algorithm that finds a consistent hypothesis in runtime of Õ((m + d) · C). For the same problem, our algorithm (with the Perceptron as the weak learner) finds a consistent hypothesis in runtime of Õ((m + C) · d). Furthermore, if the instances are d̄-sparse (meaning that the number of nonzeros in each xi is at most d̄), then the term d in our bound can be replaced by d̄. In any case, our bound is sometimes better and sometimes worse than the one in (Clarkson et al., 2012). We note that we can also use AdaBoost (Freund & Schapire, 1995) on top of the Perceptron algorithm for the same problem. It can be easily verified that the resulting runtime will be identical to our bound. In this sense, our algorithm can be seen as an online version of AdaBoost.\nFinally, several recent works use sampling strategies for speeding up optimization algorithms for minimizing the average loss. See for example (Bengio & Senécal, 2008; Bouchard et al., 2015; Zhao & Zhang, 2014; Allen-Zhu & Yuan, 2015)."
    }, {
      "heading" : "3. Why",
      "text" : "In this section we tackle the “Why” question, namely, why should we prefer minimizing the maximal loss instead of the average loss. For simplicity of presentation, throughout this section we deal with binary classification problems with the zero-one loss, in the realizable setting. In this context, minimizing the maximal loss to accuracy of < 1 leads to a consistent hypothesis1. On the other hand, minimizing the average loss to any accuracy of > 1/m does not guarantee to return a consistent hypothesis. Therefore, in this context, the “why” question becomes: why should we find a consistent hypothesis and not be satisfied with a hypothesis with Lavg(h) ≤ for some > 1/m. In the usual PAC learning model (see (Shalev-Shwartz & Ben-David, 2014) for an overview), there is a distribution D over X ×Y and the training examples are assumed to be\n1Recall that a consistent hypothesis is a hypothesis that makes no mistakes on the training set. We also use the term Empirical Risk Minimization (ERM) to describe the process of finding a consistent hypothesis, and use ERM(S) to denote any hypothesis which is consistent with a sample S.\nsampled i.i.d. from D. The goal of the learner is to minimize LD(h) := E(x,y)∼D[`(h, x, y)] = P(x,y)∼D[h(x) 6= y]. For a fixed h ∈ H, the random variable Lavg(h) is an unbiased estimator ofLD(h). Furthermore, it can be shown (Boucheron et al. (2005, Section 5.1.2)) that with probability of at least 1− δ over the choice of the sample S ∼ Dm we have that:\n∀h ∈ H, LD(h) ≤ Lavg(h)+\nÕ (√ Lavg(h)\nVC(H)− log(δ) m + VC(H)− log(δ) m\n)\nwhere VC(H) is the VC dimension of the class H and the notation Õ hides constants and logarithmic terms.\nFrom the above bound we get that any h with Lavg(h) = 0 (i.e., a consistent h) guarantees that LD(h) = Õ (\nVC(H)+log(1/δ) m\n) . However, we will obtain the same\nguarantee (up to constants) if we will choose any h with Lavg(h) ≤ , for = Õ ( VC(H)+log(1/δ)\nm\n) . Based on this\nobservation, it can be argued that it is enough to minimize Lavg to accuracy of = Õ ( VC(H)+log(1/δ)\nm\n) > 1m , be-\ncause a better accuracy on the training set will in any case get lost by the sampling noise.\nFurthermore, because of either computational reasons or high dimensionality of the data, we often do not directly minimize the zero-one loss, and instead minimize a convex surrogate loss, such as the hinge-loss. In such cases, we often rely on a margin based analysis, which means that the term VC(H) is replaced byB2, whereB is an upper bound on the norm of the weight vector that defines the classifier. It is often the case that the convergence rate of SGD is of the same order, and therefore there is no added value of solving the ERM problem over performing a single SGD pass over the data (or few epochs over the data). Formal arguments of this nature were given in (Bousquet & Bottou, 2008; Shalev-Shwartz & Srebro, 2008).\nDespite of these arguments, we show below reasons to prefer the max loss formulation over the average loss formulation. The first reason is straightforward: arguments that are based on worst case bounds are problematic, since in many cases the behavior is rather different than the worst case bounds. In subsection 3.1 we present a simple example in which there is a large gap between the sample complexity of SGD and the sample complexity of ERM, and we further show that the runtime of our algorithm will be much better than the runtime of SGD for solving this problem.\nNext, we describe a family of problems in which the distribution from which the training data is being sampled is a mix of “typical” examples and “rare” examples. We show that in such a case, few “rare” examples may be sufficient for learning a hypothesis that has a high accuracy on both\nthe “typical” and “rare” examples, and therefore, it is really required to solve the ERM problem as opposed to being satisfied with a hypothesis for which Lavg(h) is small."
    }, {
      "heading" : "3.1. A Simple Example of a Gap",
      "text" : "Consider the following distribution. Let z1 = (α, 1) and z2 = (α,−2α) for some small α > 0. To generate an example (x, y) ∼ D, we first sample a label y uniformly at random from {±1}, then we set x = yz1 with probability 1 − and set x = yz2 with probability . The hypothesis class is halfspaces: H = {x→ sign(〈w, x〉) : w ∈ R2}. The following three lemmas, whose proofs are given in the appendix, establish the gap between the different approaches.\nLemma 1 For every δ ∈ (0, 1), if m ≥ 2 log(4/δ) then, with probability of at least 1 − δ over the choice of the training set, S ∼ Dm, any hypothesis in ERM(S) has a generalization error of 0.\nLemma 2 Suppose we run SGD with the hinge-loss and any η > 0 for less than T = Ω(1/(α )) iterations. Then, with probability of 1−O( ) we have that SGD will not find a solution with error smaller than .\nLemma 3 Running FOL (with the Perceptron as its w player) takes Õ ( 1 +\n1 α\n) iterations."
    }, {
      "heading" : "3.2. Typical vs. Rare Distributions",
      "text" : "To motivate the learning setting, consider the problem of face detection, in which the goal is to take an image crop and determine whether it is an image of a face or not. An illustration of typical random positive and negative examples is given in Figure 1 (top row). By having enough training examples, we can learn that the discrimination between face and non-face is based on few features like “an ellipse shape”, “eyes”, “nose”, and “mouth”. However, from the typical examples it is hard to tell whether an image of a watermelon is a face or not — it has the ellipse shape like a face, and something that looks like eyes, but it doesn’t have a nose, or a mouth. The bottom row of Figure 1 shows some additional “rare” examples.\nSuch a phenomenon can be formally described as follows. There are two distributions over the examples, D1 and D2. Our goal is to have an error of at most on both distributions, namely, we would like to find h such thatLD1(h) ≤ and LD2(h) ≤ . However, the training examples that we observe are sampled i.i.d. from a mixed distribution, D = λ1D1+λ2D2, where λ1, λ2 ∈ (0, 1) and λ1+λ2 = 1. We assume that λ2 λ1, namely, typical examples in the training set are from D1 while examples from D2 are rare. Fix some . If λ2 < , then a hypothesis with Lavg(h) ≤\nMinimizing the Maximal Loss: How and WhyMinimizing the Maximal Loss: How and Why\nFigure 1. Top: typical positive (left) and negative (right) examples. Bottom: rare negative examples.\nmight err on most of the “rare” examples, and is therefore likely to have LD2(h) > ✏. If we want to guarantee a good performance on D2 we must optimize to a very high accuracy, or put another way, we would like to minimize Lmax instead of Lavg. The question is how many examples do we need in order to guarantee that a consistent hypothesis on S will have a small error on both D1 and D2. A naive approach is to require order of VC(H)/( 2✏) examples, thus ensuring that we have order of VC(H)/✏ examples from both D1 and D2. However, this is a rough estimate and the real sample complexity might be much smaller. Intuitively, we can think of the typical examples from D1 as filtering out most of the hypotheses in H, and the goal of the rare examples is just to fine tune the exact hypothesis. In the example of face detection, the examples from D1 will help us figure out what is an “ellipse like shape”, what is an “eye”, and what is a “mouth” and a “nose”. After we understand all this, the rare examples from D2 will tell us the exact requirement of being a face (e.g., you need an ellipse like shape and either eyes or a mouth). We can therefore hope that the number of required “rare” examples is much smaller than the number of required “typical” examples. This intuition is formalized in the following theorem.\nTheorem 2 Fix ✏, 2 (0, 1), distributions D1, D2, and let D = 1D1 + 2D2 where 1 + 2 = 1, 1, 2 2 [0, 1], and 2 < 1. Define H1,✏ = {h 2 H : LD1(h)  ✏} and c = max{c0 2 [✏, 1) : 8h 2 H1,✏, LD2(h)  c0 ) LD2(h)  ✏}. Then, if\nm ⌦ ✓\nVC(H) log(1/✏) + log(1/ ) ✏ +\nVC(H1,✏) log(1/c) + log(1/ ) c 2\n◆\nwe have, with probability of at least 1 over the sampling of a sample S ⇠ Dm:\nLD1(ERM(S))  ✏ and LD2(ERM(S))  ✏\nThe proof of the theorem is given in the appendix. The first term in the sample complexity is a standard VC-based sample complexity. The second term makes two crucial\nimprovement. First, we measure the VC dimension of a reduced class (H1,✏), containing only those hypotheses in H that have a small error on the “typical” distribution. Intuitively, this will be a much smaller hypothesis class compared to the original class. Second, we apply an analysis of the sample complexity similar to the “shell analysis” of (Haussler et al., 1996), and assume that the error of all hypotheses in H1,✏ on D2 is either smaller than ✏ or larger than c, where we would like to think of c as being significantly larger than ✏. Naturally, this will not always be the case. But, Theorem 2 provides data dependent conditions, under which a much smaller number of examples from D2 is sufficient. As a motivation, consider again Figure 1, and suppose H1,✏ contains conjunctions over all subsets of the features “has eyes”, “has nose”, “has mouth”, “has skin color”. Let h⇤ be the conjunction of all these 4 features. It is reasonable to assume that examples in D2 lack one of these features. Let us also assume for simplicity that each lacking feature takes at least 1/8 of the mass of D2. Hence, the error of all “wrong” functions in H1,✏ on D2 is at least 1/8, while the error of h⇤ is 0. We see that in this simple example, c = 1/8.\nAll in all, the theorem shows that a small number of “rare” examples in the training set can have a dramatic effect on the performance of the algorithm on the rare distribution D2. But, we will see this effect only if we will indeed find a hypothesis consistent with all (or most) examples from D2, which requires an algorithm for minimizing Lmax and not Lavg.\n4. Robustness In the previous section we have shown cases in which minimizing Lmax is better than minimizing Lavg. However, in the presence of outliers, minimizing Lmax might lead to meaningless results — even a single outlier can change the value of Lmax and might lead to a trivial, non-interesting, solution. In this section we describe two tricks for addressing this problem. The first trick replaces the original sample with a new sample whose examples are sampled from the original sample. The second trick relies on slack variables. We note that these tricks are not new and appears in the literature in various forms. See for example (Huber & Ronchetti, 2009; Maronna et al., 2006). The goal of this section is merely to show how to apply known tricks to the max loss problem.\nRecall that in the previous section we have shown that a small amount of “rare” examples can have a dramatic effect on the performance of the algorithm on the “rare” distribution. Naturally, if the number of outliers is larger than the number of rare examples we cannot hope to enjoy the benefit of rare examples. Therefore, throughout this section we assume that the number of outliers, denoted k, is smaller\nFigure 1. op: t i l iti l ft) and negative (right) examples. Botto : r r ti l s.\nmight err on ost of the “rare” exa ples, and is therefore likely to have LD2(h) > . If we want to guarantee a good performance on D2 we must optimize to a very high accuracy, or put another way, we would like to minimize Lmax instead of Lavg. The question is how many examples do we need in order to guarantee that a consistent hypothesis on S will have a small error on both D1 and D2. A naive approach is to require order of VC(H)/(λ2 ) examples, thus ensuring that we have order of VC(H)/ examples from both D1 and D2. However, this is a rough estimate and the real sample complexity might be much smaller. Intuitively, we can think of the typical examples from D1 as filtering out most of the hypotheses in H, and the goal of the rare examples is just to fine tune the exact hypothesis. In the example of face detection, the examples from D1 will help us figure out what is an “ellipse like shape”, what is an “eye”, and what is a “mouth” and a “nose”. After we understand all this, the rare examples from D2 will tell us the exact requirement of being a face (e.g., you need an ellipse like shape and either eyes or a mouth). We can therefore hope that the number of required “rare” examples is much smaller than the number of required “typical” examples. This intuition is formalized in the following theor m.\nTheorem 2 Fix , δ ∈ (0, 1), distributions D1, D2, and let D = λ1D1 + λ2D2 where λ1 + λ2 = 1, λ1, λ2 ∈ [0, 1], and λ2 < λ1. Define H1, = {h ∈ H : LD1(h) ≤ } and c = max{c′ ∈ [ , 1) : ∀h ∈ H1, , LD2(h) ≤ c′ ⇒ LD2(h) ≤ }. Then, if\nm ≥ Ω ( VC(H) log(1/ ) + log(1/δ) +\nVC(H1, ) log(1/c) + log(1/δ) c λ2\n)\nwe have, with probability of at least 1−δ over the sampling of a sample S ∼ Dm:\nLD1(ERM(S)) ≤ and LD2(ERM(S)) ≤\nThe proof of the theorem is given in the appendix. The first term in the sample complexity is a standard VC-based sample complexity. The second term makes two crucial\nimprovement. First, we measure the VC dimension of a reduced class (H1, ), containing only those hypotheses in H that have a small error on the “typical” distribution. Intuitively, this will be a much smaller hypothesis class compared to the original class. Second, we apply an alysis of the sample complexity similar to he “shell analysis” of (Haussler et al., 1996), and assume that the erro f all hypotheses in H1, on D2 is either smaller than or large than c, wher we would like to think of c as being sign ficantly larger than . Naturally, this will not always be the case. But, Theorem 2 provides data dep ndent conditions, under which a much smaller number of examples from D2 is sufficient. As a motivation, consider again Figure 1, and suppose H1, contains conjunctions over all subsets of the features “has eyes”, “has nose”, “has mouth”, “has skin color”. Let h∗ be the conjunction of all these 4 features. It is reasonable to assume that examples in D2 lack one of these features. Let us also assume for simplicity that each lacking feature takes at least 1/8 of the mass ofD2. Hence, the error of all “wrong” functions in H1, on D2 is at least 1/8, while the error of h∗ is 0. We see that in this simple example, c = 1/8.\nAll in all, the theorem shows that a small number of “rare” examples in the training set can have a dramatic effect on the performance of the algorithm on the rare distribution D2. But, we will see this effect only if we will indeed find a hypothesis consistent with all (or most) examples from D2, which requires an algorithm for minimizing Lmax and not Lavg."
    }, {
      "heading" : "4. Robustness",
      "text" : "In the previous section we have shown cases in which minimizing Lmax is better than minimizing Lavg. However, in the presence of outliers, mi i izing Lmax might lead to meaningles results — ev n a si gle outlier can change the value of Lmax and might lead to a trivial, non-interesting, solution. In this section we describe two tricks for addressing this problem. The first trick replaces the original sample with a new sample whose examples are sampled from the original sample. Th sec nd trick relies on slack variables. We ote that these tricks are not new and appears in the literature in various forms. See for example (Huber & Ronchetti, 2009; Maronna et al., 2006). The goal of this section is merely to show how to apply known tricks to the max loss problem.\nRecall that in the previous section we have shown that a small amount of “rare” examples can have a dramatic effect on the performance of the algorithm on the “rare” distribution. Naturally, if the number of outliers is larger than the number of rare examples we cannot hope to enjoy the benefit of rare examples. Therefore, throughout this section we assume that the number of outliers, denoted k, is smaller\nthan the number of “rare” examples, which we denote by m2."
    }, {
      "heading" : "4.1. Sub-sampling with repetitions",
      "text" : "The first trick we consider is to simply take a new sample of n examples, where each example in the new sample is sampled independently according to the uniform distribution over the original m examples. Then, we run our algorithm on the obtained sample of n examples.\nIntuitively, if there are k outliers, and the size of the new sample is significantly smaller than m/k, then there is a good chance that no outliers will fall into the new sample. On the other hand, we want that enough “rare” examples will fall into the new sample. The following theorem, whose proof is in the appendix, shows for which values of k and m2 this is possible.\nTheorem 3 Let k be the number of outliers, m2 be the number of rare examples, m be the size of the original sample, and n be the size of the new sample. Assume that m ≥ 10k. Then, the probability that the new sample contains outliers and/or does not contain at least m2/2 rare examples is at most 0.01 + 0.99kn/m+ e−0.1nm2/m.\nFor example, if n = m/(100k) and m2 ≥ 1000 log(100) k, then the probability of the bad event is at most 0.03."
    }, {
      "heading" : "4.2. Slack variables",
      "text" : "Another common trick, often used in the SVM literature, is to introduce a vector of slack variables, ξ ∈ Rm, such that ξi > 0 indicates that example i is an outlier. We first describe the ideal version of outlier removal. Suppose we restrict ξi to take values in {0, 1}, and we restrict the number of outliers to be at most K. Then, we can write the following optimization problem:\nmin w∈W,ξ∈Rm max i∈[m]\n(1− ξi) `(w, xi, yi) s.t.\nξ ∈ {0, 1}m, ‖ξ‖1 ≤ K .\nThis optimization problem minimizes the max loss over a subset of examples of size at leastm−K. That is, we allow the algorithm to refer to at most K examples as outliers.\nNote that the above problem can be written as a max-loss minimization:\nmin w̄∈W̄ max i\n¯̀(w̄, xi, yi) where\nW̄ = {(w, ξ) : w ∈ W, ξ ∈ {0, 1}m, ‖ξ‖1 ≤ K} and ¯̀((w, ξ), xi, yi) = (1− ξi)`(w, xi, yi)\nWe can now apply our framework on this modified problem. The p player remains as before, but now the w̄\nplayer has a more difficult task. To make the task easier we can perform several relaxations. First, we can replace the non-convex constraint ξ ∈ {0, 1}m with the convex constraint ξ ∈ [0, 1]m. Second, we can replace the multiplicative slack with an additive slack, and re-define: ¯̀((w, ξ), xi, yi) = `(w, xi, yi) − ξi. This adds a convex term to the loss function, and therefore, if the original loss was convex we end up with a convex loss. The new problem can often be solved by combining gradient updates with projections of ξ onto the set ξ ∈ [0, 1]m, ‖ξ‖1 ≤ K. For efficient implementations of this projection see for example (Duchi et al., 2008). We can further replace the constraint ‖ξ‖1 ≤ K with a constraint of ‖ξ‖22 ≤ K, because projection onto the Euclidean ball is a simple scaling, and the operation can be done efficiently with an adequate data structure (as described, for example, in (Shalev-Shwartz et al., 2011))."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section we demonstrate several merits of our approach on the well studied problem of face detection. Detection problems in general have a biased distribution as they are often expected to detect few needles in a haystack. Furthermore, a mix of typical and rare distributions is to be expected. For example, users of smartphones won’t be in the same continent as the manufacturers who collect data for training. This domain requires weighting of examples, and therefore is a good playground to examine our algorithm.\nTo create a dataset we downloaded 30k photos from Google images that are tagged with “face”. We then applied an off-the-shelf face detector, and it found 32k rectangles that aligned on faces. This was the base of our positive examples. For negative examples we randomly sampled 250k rectangles in the same images that do not overlap faces. Each rectangle was cropped and scaled to 28×28 pixels. Using a fixed size simplifies the experiments so we can focus on the merits of our method rather than justify various choices that are not relevant here, such as localization and range of scale.\nRecall that our FOL algorithm relies on an online algorithm as the w player. In the experiments, w is the vector containing all the weights of a convolutional neural network (CNN) with a variant of the well known LeNet architecture. The layers of the network are as follows. Convolution with a 5×5 kernel, stride of 1, and 40 output channels, followed by ReLU and max-pooling. Then a convolution with a 5×5 kernel, stride of 1, and 80 output channels, followed by ReLU and max-pooling. Then a convolution with a 7×7 kernel, stride of 1, and 160 output channels, followed by ReLU. Finally, a linear prediction over the resulting 160 channels yields the binary prediction for the input 28×28\nimage crop. Overall the model has 710, 642 weights. We denote byH the resulting hypothesis class. In the comparison, we focus on the case in which the data is realizable by H. To guarantee that, we first used vanilla SGD to find a network in H. We then kept only samples that were labeled correctly by the network. This yielded 28k positive examples and 246k negative examples. This set was then randomly mixed and split 9 : 1 for train and test sets.\nFor the w player we used the SGD algorithm with Nesterov’s momentum, as this is a standard solver for learning convolutional neural networks. The parameters we used are a batch size of 64, an `2 regularization of 0.0005, momentum parameter of 0.9, and a learning rate of ηt = 0.01(1 + 0.0001 t)−0.75. We used the logistic loss as a surrogate loss for the classification error.\nWe performed two experiments to highlight different properties of our algorithm. The first experiment shows that FOL is much faster than SGD, and this is reflected both in train and test errors. The second experiment compares FOL to the application of AdaBoost on top of the same base learner.\nExperiment 1: Convergence speed In this experiment we show that FOL is faster than SGD. Figure 2 shows the train and test errors of SGD and FOL. Both models were initialized with the same randomly selected weights. As mentioned before, FOL relies on SGD as its w player. Observe that FOL essentially solves the problem (zero training error) after 30 epochs, whereas SGD did not converge to a zero training error even after 14, 000 epochs and achieved 0.1313% error. While the logarithmic scale in the figure shows that SGD is still improving, it is doing so at a decreasing rate. This is reflected by our theoretical analysis.\nTo understand why SGD slows down, observe that when the error of SGD is as small as here (0.13%), only one example in 769 is informative. Even when at classification error of 0.4% (left-side of the graph), only 4 in 1, 000 examples are informative. Hence, even with batch size of 64, SGD picks one useful sample only once every fifteen iterations. FOL expects an average of 32 useful samples in every iteration so every iteration is informative. In our case, since the training set size is 246k, only 984 examples are informative and FOL makes sure to focus on these rather than waste time on solved examples.\nAs can be seen in Figure 2, the faster convergence of FOL on the training set is also translated to a better test error. Indeed, FOL achieves a test error of 0.14% (after 27 epochs) whereas even after 14k epochs SGD results 0.35% error.\nExperiment 2: Comparison to AdaBoost As mentioned in Section 2.4, FOL can be seen as an online version of AdaBoost. Specifically, we can apply the AdaBoost algorithm while using SGD as its weak learner. For concreteness and reproducibility of our experiment, we briefly describe the resulting algorithm. We initialize a uniform distribution over the m examples, p = (1/m, . . . , 1/m). At iteration t of AdaBoost, we run one epoch of SGD over the data, while sampling examples according to p. Let ht be the resulting classifier. We then calculate ht(xi) over all the m examples, calculate the averaged zero-one error of ht, with weights based on p, define a weight αt = 0.5 log(1/ t−1), and update p such that pi ∝ pi exp(−αtyiht(xi)). We repeat this process for T iterations, and output the hypothesis h(x) = sign( ∑T t=1 αtht(x)).\nObserve that each such iteration of AdaBoost is equivalent to 2 epochs of our algorithm. In Figure 3 we show the train error of AdaBoost and FOL as a function of the number of epochs over the data. The behavior on the test set shows a similar trend. As can be seen in the figure, AdaBoost finds a consistent hypothesis after 20 epochs, while FOL requires 27 epochs to converge to a consistent hypothesis. However, once FOL converged, its last hypothesis has a zero training error. In contrast, the output hypothesis of AdaBoost is a weighted majority of T hypotheses (T = 10 in this case). It follows that at prediction time, applying AdaBoost’s predictor is 10 times slower than applying FOL’s predictor. Often, we prefer to spend more time during training, for the sake of finding a hypothesis which can be evaluated faster at test time. While based on our theory, the output hypothesis of FOL should also be a majority of log(m) hypotheses, we found out that in practice, the last hypothesis of FOL converges to a zero classification error at almost the same rate as the majority classifier.\nAcknowledgements: S. Shalev-Shwartz is supported by ICRI-CI and by the European Research Council (Theo-\nryDL project)."
    }, {
      "heading" : "A. Proof of Theorem 1",
      "text" : "A.1. Background\nBernstein’s type inequality for martingales: A sequence B1, . . . , BT of random variables is Markovian if for every t, given Bt−1 we have that Bt is independent of B1, . . . , Bt−2. A sequence A1, . . . , AT of random variables is a martingale difference sequence with respect to B1, . . . , BT if for every t we have E[At|B1, . . . , Bt] = 0.\nLemma 4 (Hazan et al. (2011, Lemma C.3) and Fan et al. (2012, Theorem 2.1)) Let B1, . . . , BT be a Markovian sequence and let A1, . . . , AT be a martingale difference sequence w.r.t. B1, . . . , BT . Assume that for every t we have |At| ≤ V and E[A2t |B1, . . . , Bt] ≤ s. Then, for every α > 0 we have\nP\n( 1\nT\nT∑\nt=1\nAt ≥ α ) ≤ exp ( −T α 2/2\ns+ αV/3\n)\nIn particular, for every δ ∈ (0, 1), if\nT ≥ 2(s+ αV/3) log(1/δ) α2 ,\nthen with probability of at least 1− δ we have that 1T ∑T t=1At ≤ α.\nThe EG algorithm: Consider a sequence of vectors, z1, . . . , zT , where every zt ∈ Rm. Consider the following sequence of vectors, parameterized by η > 0. The first vector is q̃1 = (1, . . . , 1) ∈ Rm and for t ≥ 1 we define q̃t+1 to be such that:\n∀i ∈ [m], q̃t+1,i = q̃t,i exp(−ηzt,i) .\nIn addition, for every t define qt = q̃/( ∑m i=1 q̃i) ∈ Sm. The algorithm that generates the above sequence is known as the EG algorithm (Kivinen & Warmuth, 1997).\nLemma 5 (Theorem 2.22 in (Shalev-Shwartz, 2011)) Assume that ηzt,i ≥ −1 for every t and i. Then, for every u ∈ Sm we have:\nT∑\nt=1\n〈qt − u, zt〉 ≤ log(m)\nη + η\nT∑\nt=1\nm∑\ni=1\nqt,iz 2 t,i .\nA.2. Proof\nTo simplify our notation we denote `i(wt) = `(wt, xi, yi). We sometimes omit the time index t when it is clear from the context (e.g., we sometime use qi instead of qt,i).\nA.2.1. THE w PLAYER\nBy our assumption that C/T ≤ /8 we have that, for every i1, . . . , iT ,\n1\nT\nT∑\nt=1\n`it(wt) ≤ /8 (5)\nA.2.2. THE p PLAYER\nRecall that pi = 12m + qi 2 . Note that, for every i,\n1 pi ≤ 2m and qi pi ≤ 2\nDefine zt = − `it (wt)pit eit . Observe that the p player applies the EG algorithm w.r.t. the sequence z1, . . . , zT . Since\nzt,i ≥ −2m we obtain from Lemma 5 that if η ≤ 1/(2m) then, for every u ∈ Sm,\n1\nT\nT∑\nt=1\n〈qt − u, zt〉 ≤ log(m) ηT + η T\nT∑\nt=1\nm∑\ni=1\nqt,iz 2 t,i\n≤ log(m) ηT + η T\nT∑\nt=1\nqt,it `it(wt)\n2\np2t,it\n≤ log(m) ηT + η T\nT∑\nt=1\n4m`it(wt) 2\n≤ log(m) ηT + η4m T\nT∑\nt=1\n`it(wt)\n≤ /8 + 2 T\nT∑\nt=1\n`it(wt) ,\nwhere in the last inequality we used η = 1/(2m) and T = Ω(m log(m)/ ). Rearranging, and combining with (5) we obtain\n1\nT\nT∑\nt=1\n〈u, 1 pt,it `it(wt)eit〉 ≤ 1 T\nT∑\nt=1\n( qt,it pt,it + 2 ) `it(wt) + /8 ≤ 4 T T∑\nt=1\n`it(wt) + /8 ≤ 5\n8 . (6)\nA.2.3. MEASURE CONCENTRATION\nNote that, if u = ei, then\nE[〈u, 1 pt,it\n`it(wt)eit〉2|qt, wt] = m∑\nj=1\npt,j p2t,j `j(wt) 2uj ≤ 1 pt,i ≤ 2m .\nDefine the martingale difference sequence A1, . . . , AT where At = `i(wt) − 〈u, 1pt,it `it(wt)eit〉. We have that |At| ≤ (2m + 1) and E[A2t |qt, wt] ≤ 2m. Therefore, the conditions of Lemma 4 holds and we obtain that if T ≥ 6m log(m/δ)/( /8)2 then with probability of at least 1 − δ/m we have that 1T ∑ tAt ≤ /8. Applying a union bound over i ∈ [m] we obtain that with probability of at least 1− δ it holds that\n∀i ∈ [m], 1 T\n∑\nt\n`i(wt) ≤ 1\nT\n∑\nt\n〈ei, 1\npit `it(wt)eit〉+ /8 .\nCombining with (6) we obtain that, with probability of at least 1− δ,\n∀i ∈ [m], 1 T\nT∑\nt=1\n`i(wt) ≤ 6\n8 .\nFinally, relying on Bernstein’s inequality (see Lemma B.10 in (Shalev-Shwartz & Ben-David, 2014)), it is not hard to see that if k = Ω(log(m/δ)/ ) then, with probability of at least 1− δ we have that\n∀i ∈ [m], 1 k\nk∑\nj=1\n`i(wtj ) ≤ 1\nT\nT∑\nt=1\n`i(wt) + 4 ,\nand this concludes our proof."
    }, {
      "heading" : "B. Proofs of Lemmas in Section 3.1",
      "text" : "Proof [Proof of Lemma 1] There are only 4 possible examples, so an ERM will have a generalization error of 0 provided we see all the 4 examples. By a simple direct calculation together with the union bound over the 4 examples it\nis easy to verify that the probability not to see all the examples is at most 4(1− /2)m ≤ 4e−m /2, and the claim follows.\nProof [Proof of Lemma 2] For SGD, we can assume (due to symmetry) that y is always 1. Therefore, there are only two possible examples z1, z2. With probability 1− the first examples is z1. Also, wT has always the form\nη(kz1 + rz2) = η((k + r)α, k − 2rα) ,\nwhere k is the number of times we had a margin error on z1 and r is the number of times we had a margin error on z2. To make sure that 〈wT , z2〉 > 0 we must have that\n(k + r)α2 − 2(k − 2rα)α > 0 ⇒ r > k 2α− α 2 5α2 ≈ k 2 5α (7)\nNote that the first example is z1 with probability of 1 − , hence we have that k ≥ 1 with probability of at least 1 − . In addition, r is upper bounded by the number of times we saw z2 as the example, and by Chernoff’s bound we have that that the probability that this number is greater than 2m is at most e− /3 ≈ (1− /3). Therefore, with probability of 1−O( ) we have the requirement that m must be at least Ω(1/(α )), which concludes our proof.\nProof [Proof of Lemma 3] We have shown that m = 1/ examples suffices. Specifying our general analysis to classification with the zero-one loss, it suffices to ensure that the regret of both players will be smaller than 1/2. The regret of the sampling player is bounded by O(m log(m)). As for the halfspace player, to simplify the derivation, lets use the Perceptron as the underlying player. It is easy to verify that the vector wT has the form kz1 + rz2 = ((k + r)α, k − 2rα), for some integers k, r. Lets consider two regimes. The first is the first time when r, k satisfies 〈wT , z2〉 > 0. As we have shown before, this happens when r is roughly 2k/(5α). Once this happens we also have that\n〈wT , z1〉 = ((k + r)α2 + k − 2rα) ≈ (k − 2rα) ≈ > 4k/5 > 0 ,\nSo, the Perceptron will stop making changes and will give us an optimal halfspace. Next, suppose that we have a pair r, k for which 〈wT , z2〉 ≤ 0. If we now encounter z2 then we increase r. If we encounter z1 then\n〈wT , z1〉 ≈ (k − 2rα) ≈ > 4k/5 > 0 ,\nso we’ll not increase k. Therefore, k will increase only up to a constant, while r will continue to increase until roughly 2k/(5α), and then the Perceptron will stop making updates. This implies that the mistake bound of the Perceptron is bounded by O(1/α), which concludes our proof."
    }, {
      "heading" : "C. Proof of Theorem 2",
      "text" : "We can think of the ERM algorithm as following the following three steps. First, we sample (i1, . . . , im) ∈ {1, 2}m, where P[ir = j] = λj . Let m1 be the number of indices for which ir = 1 and let m2 = m−m1. Second, we sample S1 ∼ Dm11 , and define Ĥ1 to be all hypotheses in H which are consistent with S1. Last, we sample S2 ∼ Dm22 and set the output hypotheses to be some hypothesis in Ĥ1 which is consistent with S2. The proof relies on the following three claims, where we use C to denote a universal constant:\n• Claim 1: With probability of at least 1 − δ/3 over the choice of (i1, . . . , im) we have that both m1 ≥ λ1m/2 and m2 ≥ λ2m/2. • Claim 2: Assuming that m1 ≥ C ( VC(H) log(1/ )+log(1/δ) ) , then with probability of at least 1− δ/3 over the choice\nof S1 we have that Ĥ1 ⊆ H1, .\n• Claim 3: Assume that m2 ≥ C ( VC(H1, ) log(1/c)+log(1/δ) c ) , then with probability of at least 1− δ/3 over the choice\nof S2, any hypothesis inH1, which is consistent with S2 must have LD2(h) ≤ c.\nClaim 1 follows directly from Chernoff’s bound, while Claim 2-3 follows directly from standard VC bounds (see for example Shalev-Shwartz & Ben-David (2014, Theorem 6.8)).\nEquipped with the above three claims we are ready to prove the theorem. First, we apply the union bound to get that with probability of at least 1 − δ, the statements in all the above three claims hold. This means that Ĥ1 ⊆ H1, hence LD1(ERM(S)) ≤ . It also means that ERM(S) must be inH1, , and therefore from the third claim and the assumption in the theorem we have that LD2(ERM(S)) ≤ as well, which concludes our proof."
    }, {
      "heading" : "D. Proof of Theorem 3",
      "text" : "The probability that all of the outliers do not fall into the sample of n examples is\n(1− k/m)n ≥ 0.99 e−kn/m .\nTherefore, the probability that at least one outlier falls into the sample is at most\n1− 0.99 e−kn/m ≤ 1− 0.99(1− kn/m) = 0.01 + 0.99kn/m\nOn the other hand, the expected number of rare examples in the sample is nm2/m and by Chernoff’s bound, the probability that less than half of the rare examples fall into the sample is at most exp(−0.1nm2/m). Applying the union bound we conclude our proof."
    } ],
    "references" : [ {
      "title" : "Even faster accelerated coordinate descent using non-uniform sampling",
      "author" : [ "Allen-Zhu", "Zeyuan", "Yuan", "Yang" ],
      "venue" : "arXiv preprint arXiv:1512.09103,",
      "citeRegEx" : "Allen.Zhu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Allen.Zhu et al\\.",
      "year" : 2015
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Adaptive importance sampling to accelerate training of a neural probabilistic language model",
      "author" : [ "Bengio", "Yoshua", "Senécal", "Jean-Sébastien" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2008
    }, {
      "title" : "Accelerating stochastic gradient descent via online learning to sample",
      "author" : [ "Bouchard", "Guillaume", "Trouillon", "Théo", "Perez", "Julien", "Gaidon", "Adrien" ],
      "venue" : "arXiv preprint arXiv:1506.09016,",
      "citeRegEx" : "Bouchard et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bouchard et al\\.",
      "year" : 2015
    }, {
      "title" : "Theory of classification: A survey of some recent advances",
      "author" : [ "Boucheron", "Stéphane", "Bousquet", "Olivier", "Lugosi", "Gábor" ],
      "venue" : "ESAIM: probability and statistics,",
      "citeRegEx" : "Boucheron et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Boucheron et al\\.",
      "year" : 2005
    }, {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "Bousquet", "Olivier", "Bottou", "Léon" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Bousquet et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bousquet et al\\.",
      "year" : 2008
    }, {
      "title" : "Sublinear optimization for machine learning",
      "author" : [ "Clarkson", "Kenneth L", "Hazan", "Elad", "Woodruff", "David P" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Clarkson et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Clarkson et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient projections onto the l 1-ball for learning in high dimensions",
      "author" : [ "Duchi", "John", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2008
    }, {
      "title" : "Hoeffding’s inequality for supermartingales",
      "author" : [ "Fan", "Xiequan", "Grama", "Ion", "Liu", "Quansheng" ],
      "venue" : "Stochastic Processes and their Applications,",
      "citeRegEx" : "Fan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2012
    }, {
      "title" : "A desicion-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Freund", "Yoav", "Schapire", "Robert E" ],
      "venue" : "In Computational learning theory,",
      "citeRegEx" : "Freund et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 1995
    }, {
      "title" : "Rigorous learning curve bounds from statistical mechanics",
      "author" : [ "Haussler", "David", "Kearns", "Michael", "Seung", "H Sebastian", "Tishby", "Naftali" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Haussler et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Haussler et al\\.",
      "year" : 1996
    }, {
      "title" : "Beating sgd: Learning svms in sublinear time",
      "author" : [ "Hazan", "Elad", "Koren", "Tomer", "Srebro", "Nati" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2011
    }, {
      "title" : "Robust Statistics (second edition)",
      "author" : [ "Huber", "Peter J", "Ronchetti", "Elvezio M" ],
      "venue" : "J. Wiley,",
      "citeRegEx" : "Huber et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Huber et al\\.",
      "year" : 2009
    }, {
      "title" : "Exponentiated gradient versus gradient descent for linear predictors",
      "author" : [ "J. Kivinen", "M. Warmuth" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Kivinen and Warmuth,? \\Q1997\\E",
      "shortCiteRegEx" : "Kivinen and Warmuth",
      "year" : 1997
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "Shalev-Shwartz", "Shai" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Shalev.Shwartz and Shai.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Shai.",
      "year" : 2011
    }, {
      "title" : "Understanding Machine Learning: From Theory to Algorithms",
      "author" : [ "Shalev-Shwartz", "Shai", "Ben-David" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2014
    }, {
      "title" : "On the equivalence of weak learnability and linear separability: New relaxations and efficient boosting algorithms",
      "author" : [ "Shalev-Shwartz", "Shai", "Singer", "Yoram" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2010
    }, {
      "title" : "Svm optimization: inverse dependence on training set size",
      "author" : [ "Shalev-Shwartz", "Shai", "Srebro", "Nathan" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2008
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan", "Cotter", "Andrew" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Stochastic optimization with importance sampling",
      "author" : [ "Zhao", "Peilin", "Zhang", "Tong" ],
      "venue" : "arXiv preprint arXiv:1401.2753,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2014
    }, {
      "title" : "AT be a martingale difference sequence w.r.t",
      "author" : [ "Hazan" ],
      "venue" : "Lemma C.3) and Fan et al. (2012,",
      "citeRegEx" : "Hazan,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan",
      "year" : 2011
    }, {
      "title" : "The algorithm that generates the above sequence is known as the EG algorithm (Kivinen",
      "author" : [ "∈ Sm" ],
      "venue" : "(Shalev-Shwartz,",
      "citeRegEx" : "Sm.,? \\Q1997\\E",
      "shortCiteRegEx" : "Sm.",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "For the p player, we use the seminal work of (Auer et al., 2002).",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "Crucially, the work of (Auer et al., 2002) does not assume that Λ(wt) are sampled from a fixed distribution, but rather the vectors Λ(wt) can be chosen by an adversary.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "In (Auer et al., 2002) it is proposed to rely on the algorithm EXP3.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "Phrasing the maxloss minimization as a two players game has also been proposed by (Clarkson et al., 2012; Hazan et al., 2011).",
      "startOffset" : 82,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "Phrasing the maxloss minimization as a two players game has also been proposed by (Clarkson et al., 2012; Hazan et al., 2011).",
      "startOffset" : 82,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "Assuming the setup of Example 1, (Clarkson et al., 2012) presents an algorithm that finds a consistent hypothesis in runtime of Õ((m + d) · C).",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : "In any case, our bound is sometimes better and sometimes worse than the one in (Clarkson et al., 2012).",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "See for example (Bengio & Senécal, 2008; Bouchard et al., 2015; Zhao & Zhang, 2014; Allen-Zhu & Yuan, 2015).",
      "startOffset" : 16,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : "Second, we apply an analysis of the sample complexity similar to the “shell analysis” of (Haussler et al., 1996), and assume that the error of all hypotheses in H1,✏ on D2 is either smaller than ✏ or larger than c, where we would like to think of c as being significantly larger than ✏.",
      "startOffset" : 89,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "Second, we apply an alysis of the sample complexity similar to he “shell analysis” of (Haussler et al., 1996), and assume that the erro f all hypotheses in H1, on D2 is either smaller than or large than c, wher we would like to think of c as being sign ficantly larger than .",
      "startOffset" : 86,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "For efficient implementations of this projection see for example (Duchi et al., 2008).",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "We can further replace the constraint ‖ξ‖1 ≤ K with a constraint of ‖ξ‖2 ≤ K, because projection onto the Euclidean ball is a simple scaling, and the operation can be done efficiently with an adequate data structure (as described, for example, in (Shalev-Shwartz et al., 2011)).",
      "startOffset" : 247,
      "endOffset" : 276
    } ],
    "year" : 2016,
    "abstractText" : "A commonly used learning rule is to approximately minimize the average loss over the training set. Other learning algorithms, such as AdaBoost and hard-SVM, aim at minimizing the maximal loss over the training set. The average loss is more popular, particularly in deep learning, due to three main reasons. First, it can be conveniently minimized using online algorithms, that process few examples at each iteration. Second, it is often argued that there is no sense to minimize the loss on the training set too much, as it will not be reflected in the generalization loss. Last, the maximal loss is not robust to outliers. In this paper we describe and analyze an algorithm that can convert any online algorithm to a minimizer of the maximal loss. We prove that in some situations better accuracy on the training set is crucial to obtain good performance on unseen examples. Last, we propose robust versions of the approach that can handle outliers.",
    "creator" : "LaTeX with hyperref package"
  }
}