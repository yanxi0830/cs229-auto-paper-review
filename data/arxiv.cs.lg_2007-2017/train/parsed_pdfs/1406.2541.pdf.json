{
  "name" : "1406.2541.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions",
    "authors" : [ "José Miguel Hernández-Lobato", "Matthew W. Hoffman", "Zoubin Ghahramani" ],
    "emails" : [ "jmh233@cam.ac.uk", "mwh30@cam.ac.uk", "zoubin@eng.cam.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Bayesian optimization techniques form a successful approach for optimizing black-box functions [5]. The goal of these methods is to find the global maximizer of a nonlinear and generally nonconvex function f whose derivatives are unavailable. Furthermore, the evaluations of f are usually corrupted by noise and the process that queries f can be computationally or economically very expensive. To address these challenges, Bayesian optimization devotes additional effort to modeling the unknown function f and its behavior. These additional computations aim to minimize the number of evaluations that are needed to find the global optima.\nOptimization problems are widespread in science and engineering and as a result so are Bayesian approaches to this problem. Bayesian optimization has successfully been used in robotics to adjust the parameters of a robot’s controller to maximize gait speed and smoothness [16] as well as parameter tuning for computer graphics [6]. Another example application in drug discovery is to find the chemical derivative of a particular molecule that best treats a given disease [20]. Finally, Bayesian optimization can also be used to find optimal hyper-parameter values for statistical [28] and machine learning techniques [24].\nAs described above, we are interested in finding the global maximizer x? = arg maxx∈X f(x) of a function f over some bounded domain, typically X ⊂ Rd. We assume that f(x) can only be evaluated via queries to a black-box that provides noisy outputs of the form yi ∼ N (f(xi), σ2). We note, however, that our framework can be extended to other non-Gaussian likelihoods. In this setting, we describe a sequential search algorithm that, after n iterations, proposes to evaluate f at some location xn+1. To make this decision the algorithm conditions on all previous observations Dn = {(x1, y1), . . . , (xn, yn)}. AfterN iterations the algorithm makes a final recommendation x̃N for the global maximizer of the latent function f .\nWe take a Bayesian approach to the problem described above and use a probabilistic model for the latent function f to guide the search and to select x̃N . In this work we use a zero-mean Gaussian\nar X\niv :1\n40 6.\n25 41\nv1 [\nst at\n.M L\n] 1\n0 Ju\nn 20\nAlgorithm 1 Generic Bayesian optimization Input: a black-box with unknown mean f 1: for n = 1, . . . , N do 2: select xn = argmaxx∈X αn−1(x) 3: query the black-box at xn to obtain yn 4: augment data Dn = Dn−1 ∪ {(xn, yn)} 5: end for 6: return x̃N = argmaxx∈X µN (x)\nAlgorithm 2 PES acquisition function Input: a candidate x; data Dn 1: sample M hyperparameter values {ψ(i)} 2: for i = 1, . . . ,M do 3: sample f (i) ∼ p(f |Dn,φ,ψ(i)) 4: set x(i)? ← argmaxx∈X f (i)(x) 5: compute m(i)0 , V (i) 0 and m̃ (i), ṽ(i)\n6: compute v(i)n (x) and v (i) n (x|x(i)? ) 7: end for 8: return αn(x) as in (10)\nprecom puted\nprocess (GP) prior for f [22]. This prior is specified by a positive-definite kernel function k(x,x′). Given any finite collection of points {x1, . . . ,xn}, the values of f at these points are jointly zeromean Gaussian with covariance matrix Kn, where [Kn]ij = k(xi,xj). For the Gaussian likelihood described above, the vector of concatenated observations yn is also jointly Gaussian with zero-mean. Therefore, at any location x, the latent function f(x) conditioned on past observations Dn is then Gaussian with marginal mean µn(x) and variance vn(x) given by\nµn(x) = kn(x) T(Kn + σ 2I)−1yn , vn(x) = k(x,x)− kn(x)T(Kn + σ2I)−1kn(x) , (1)\nwhere kn(x) is a vector of cross-covariance terms between x and {x1, . . . ,xn}. Bayesian optimization techniques use the above predictive distribution p(f(x)|Dn) to guide the search for the global maximizer x?. In particular, p(f(x)|Dn) is used during the computation of an acquisition function αn(x) that is optimized at each iteration to determine the next evaluation location xn+1. This process is shown in Algorithm 1. Intuitively, the acquisition function αn(x) should be high in areas where the maxima is most likely to lie given the current data. However, αn(x) should also encourage exploration of the search space to guarantee that the recommendation x̃N is a global optimum of f , not just a global optimum of the posterior mean. Several acquisition functions have been proposed in the literature. Some examples are the probability of improvement [14], the expected improvement [19] or upper confidence bounds [26]. Alternatively, one can combine several of these acquisition functions [10].\nThe acquisition functions described above are based on optimistic estimates of the latent function f which implicitly trade off between exploiting the posterior mean and exploring based on the uncertainty. We instead follow the approach described in [9] and aim to maximize the expected gain of information on the posterior distribution of the global maximizer x?. In Section 2 we derive a rearrangement of this acquisition function and a corresponding approximation that we call Predictive Entropy Search (PES). PES is more accurate than the approximation used in [9]. In Section 3 we empirically evaluate this claim on both synthetic and real-world problems and show that this leads to real gains in performance."
    }, {
      "heading" : "2 Predictive entropy search",
      "text" : "We propose to follow the information-theoretic method for active data collection described in [17]. We are interested in maximizing information about the location x? of the global maximum, whose posterior distribution is p(x?|Dn). Our current information about x? can be measured in terms of the negative differential entropy of p(x?|Dn). Therefore, our strategy is to select xn+1 which maximizes the expected reduction in this quantity. The corresponding acquisition function is\nαn(x) = H[p(x?|Dn)]− Ep(y|Dn,x)[H[p(x?|Dn ∪ {(x, y)})]] , (2) where H[p(x)] = − ∫ p(x) log p(x)dx represents the differential entropy of its argument and the expectation above is taken with respect to the posterior predictive distribution of y given x. The exact evaluation of (2) is infeasible in practice. The main difficulties are i) p(x?|Dn ∪ {(x, y)}) must be computed for many different values of x and y during the optimization of (2) and ii) the entropy computations themselves are not analytical. In practice, a direct evaluation of (2) is only possible after performing many approximations [9]. To avoid this, we follow the approach described in [11] by noting that (2) can be equivalently written as the mutual information between x? and y\ngiven Dn. Since the mutual information is a symmetric function, αn(x) can be rewritten as\nαn(x) = H[p(y|Dn,x)]− Ep(x?|Dn)[H[p(y|Dn,x,x?)]] , (3)\nwhere p(y|Dn,x,x?) is the posterior predictive distribution for y given the observed data Dn and the location of the global maximizer of f . Intuitively, conditioning on the location x? pushes the posterior predictions up in locations around x? and down in regions away from x?. Note that, unlike the previous formulation, this objective is based on the entropies of predictive distributions, which are analytic or can be easily approximated, rather than on the entropies of distributions on x? whose approximation is more challenging.\nThe first term in (3) can be computed analytically using the posterior marginals for f(x) in (1), that is, H[p(y|Dn,x)] = 0.5 log[2πe (vn(x) + σ2)], where we add σ2 to vn(x) because y is obtained by adding Gaussian noise with variance σ2 to f(x). The second term, on the other hand, must be approximated. We first approximate the expectation in (3) by averaging over samples x(i)? drawn approximately from p(x?|Dn). For each of these samples, we then approximate the corresponding entropy function H[p(y|Dn,x,x(i)? )] using expectation propagation [18]. The code for all these operations is publicly available at http://tobediscloseduponacceptance.org."
    }, {
      "heading" : "2.1 Sampling from the posterior over global maxima",
      "text" : "In this section we show how to approximately sample from the conditional distribution of the global maximizer x? given the observed data Dn, that is,\np(x?|Dn) = p ( f(x?) = max\nx∈X f(x) ∣∣Dn) . (4) If the domain X is restricted to some finite set of m points, the latent function f takes the form of an m-dimensional vector f . The probability that the ith element of f is optimal can then be written as ∫ p(f |Dn) ∏ j≤m I[fi ≥ fj ] df . This suggests the following generative process: i) draw a sample from the posterior distribution p(f |Dn) and ii) return the index of the maximum element in the sampled vector. This process is known as Thompson sampling or probability matching when used as an arm-selection strategy in multi-armed bandits [8]. This same approach could be used for sampling the maximizer over a continuous domainX . At first glance this would require constructing an infinite-dimensional object representing the function f . To avoid this, one could sequentially construct f while it is being optimized. However, evaluating such an f would ultimately have cost O(m3) where m is the number of function evaluations necessary to find the optimum. Instead, we propose to sample and optimize an analytic approximation to f . We will briefly derive this approximation below, but more detail is given in Appendix A.\nGiven a shift-invariant kernel k, Bochner’s theorem [4] asserts the existence of its Fourier dual s(w), which is equal to the spectral density of k. Letting p(w) = s(w)/α be the associated normalized density, we can write the kernel as the expectation\nk(x,x′) = αEp(w)[e−iw T(x−x′)] = 2αEp(w,b)[cos(wTx + b) cos(wTx′ + b)] , (5)\nwhere b ∼ U [0, 2π]. Let φ(x) = √\n2α/m cos(Wx+b) denote an m-dimensional feature mapping where W and b consist of m stacked samples from p(w, b). The kernel k can then be approximated by the inner product of these features, k(x,x′) ≈ φ(x)Tφ(x′). This approach was used by [21] as an approximation method in the context of kernel methods. The feature mapping φ(x) allows us to approximate the Gaussian process prior for f with a linear model f(x) = φ(x)Tθ where θ ∼ N (0, I) is a standard Gaussian. By conditioning on Dn, the posterior for θ is also multivariate Gaussian, θ|Dn ∼ N (A−1ΦTyn, σ2A−1) where A = ΦTΦ + σ2I and ΦT = [φ(x1) . . .φ(xn)].\nLet φ(i) and θ(i) be a random set of features and the corresponding posterior weights sampled both according to the generative process given above. They can then be used to construct the function f (i)(x) = φ(i)(x)Tθ(i), which is an approximate posterior sample of f—albeit one with a finite parameterization. We can then maximize this function to obtain x(i)? = arg maxx∈X f\n(i)(x), which is approximately distributed according to p(x?|Dn). Note that for early iterations when n < m, we can efficiently sample θ(i) with cost O(n2m) using the method described in Appendix B.2 of [23]. This allows us to use a large number of features in φ(i)(x)."
    }, {
      "heading" : "2.2 Approximating the predictive entropy",
      "text" : "We now show how to approximate H[p(y|Dn,x,x?)] in (3). Note that we can write the argument to H in this expression as p(y|Dn,x,x?) = ∫ p(y|f(x))p(f(x)|Dn,x?) df(x). Here p(f(x)|Dn,x?) is the posterior distribution on f(x) given Dn and the location x? of the global maximizer of f . When the likelihood p(y|f(x)) is Gaussian, we have that p(f(x)|Dn) is analytically tractable since it is the predictive distribution of a Gaussian process. However, by further conditioning on the location x? of the global maximizer we are introducing additional constraints, namely that f(z) ≤ f(x?) for all z ∈ X . These constraints make p(f(x)|Dn,x?) intractable. To circumvent this difficulty, we instead use the following simplified constraints:\nC1. x? is a local maximum. This is achieved by letting ∇f(x?) = 0 and ensuring that ∇2f(x?) is negative definite. We further assume that the non-diagonal elements of ∇2f(x?), denoted by upper[∇2f(x?)], are known, for example they could all be zero. This simplifies the negative-definite constraint. We denote by C1.1 the constraint given by ∇f(x?) = 0 and upper[∇2f(x?)] = 0. We denote by C1.2 the constraint that forces the elements of diag[∇2f(x?)] to be negative.\nC2. f(x?) is larger than past observations. We also assume that f(x?) ≥ f(xi) for all i ≤ n. However, we only observe f(xi) noisily via yi. To avoid making inference on these latent function values, we approximate the above hard constraints with the soft constraint f(x?) > ymax + , where ∼ N (0, σ2) and ymax is the largest yi seen so far.\nC3. f(x) is smaller than f(x?). This simplified constraint only conditions on the given x rather than requiring f(x?) ≤ f(z) for all z ∈ X .\nWe incorporate these simplified constraints into p(f(x)|Dn) to approximate p(f(x)|Dn,x?). This is achieved by multiplying p(f(x)|Dn) with specific factors that encode the above constraints. In what follows we briefly show how to construct these factors; more detail is given in Appendix B.\nConsider the latent variable z = [f(x?); diag[∇2f(x?)]]. To incorporate constraint C1.1 we can condition on the data and on the “observations” given by the constraints ∇f(x?) = 0 and upper[∇2f(x?)] = 0. Since f is distributed according to a GP, the joint distribution between z and these observations is multivariate Gaussian. The covariance between the noisy observations yn and the extra noise-free derivative observations can easily be computed [25]. The resulting conditional distribution is also multivariate Gaussian with mean m0 and covariance V0. These computations are similar to those performed in (1). Constraints C1.2 and C2 can then be incorporated by writing\np(z|Dn,C1,C2) ∝ Φσ2(f(x?)− ymax) [∏d i=1 I ( [∇2f(x?)]ii ≤ 0 )] N (z|m0,V0) , (6)\nwhere Φσ2 is the cdf of a zero-mean Gaussian distribution with variance σ2. The first new factor in this expression guarantees that f(x?) > ymax + , where we have marginalized out, and the second set of factors guarantees that the entries in diag[∇2f(x?)] are negative. Later integrals that make use of p(z|Dn,C1,C2), however, will not admit a closed-form expression. As a result we compute a Gaussian approximation q(z) to this distribution using Expectation Propagation (EP) [18]. The resulting algorithm is similar to the implementation of EP for binary classification with Gaussian processes [22]. EP approximates each non-Gaussian factor in (6) with a Gaussian factor whose mean and variance are m̃i and ṽi, respectively. The EP approximation can then be written as q(z) ∝ [ ∏d+1 i=1 N (zi|m̃i, ṽi)]N (z|m0,V0). Note that these computations have so far not depended on x, so we can compute {m0,V0, m̃, ṽ} once and store them for later use, where m̃ = (m̃1, . . . , m̃d+1) and ṽ = (ṽ1, . . . , ṽd+1).\nWe will now describe how to compute the predictive variance of some latent function value f(x) given these constraints. Let f = [f(x); f(x?)] be a vector given by the concatenation of the values of the latent function at x and x?. The joint distribution between f , z, the evaluations yn collected so far and the derivative “observations” ∇f(x?) = 0 and upper[∇2f(x?)] = 0 is multivariate Gaussian. Using q(z), we then obtain the following approximation:\np(f |Dn,C1,C2) ≈ ∫ p(f |z,Dn,C1.1) q(z) dz = N (f |mf ,Vf ) . (7)\nImplicitly we are assuming above that f depends on our observations and constraint C1.1, but is independent of C1.2 and C2 given z. The computations necessary to obtain mf and Vf are similar\nto those used above and in (1). The required quantities are similar to the ones used by EP to make predictions in the Gaussian process binary classifier [22]. We can then incorporate C3 by multiplying N (f |mf ,Vf ) with a factor that guarantees f(x) < f(x?). The predictive distribution for f(x) given Dn and all the constraints can be approximated as\np(f(x)|Dn,C1,C2,C3) ≈ Z−1 ∫ I(f1 < f2)N (f |mf ,Vf ) df2 , (8)\nwhere Z is a normalization constant. The variance of the right hand size of (8) is given by vn(x|x?) = [Vf ]1,1 − s−1β(β + α){[Vf ]1,1 − [Vf ]1,2}2 , (9) where s = [−1, 1]TVf [−1, 1], α = µ/ √ s, µ = [−1, 1]Tmf , β = φ(α)/Φ(α), and φ(·) and Φ(·) are the standard Gaussian density function and cdf, respectively. By further approximating (8) by a Gaussian distribution with the same mean and variance we can write the entropy as H[p(y|Dn,x,x?)] ≈ 0.5 log[2πe(vn(x|x?) + σ2)]. The computation of (9) can be numerically unstable when s is very close to zero. This occurs when [Vf ]1,1 is very similar to [Vf ]1,2. To avoid these numerical problems, we multiply [Vf ]1,2 by the largest 0 ≤ κ ≤ 1 that guarantees that s > 10−10. This can be understood as slightly reducing the amount of dependence between f(x) and f(x?) when x is very close to x?. Finally, fixing upper[∇2f(x?)] to be zero can also produce poor predictions when the actual f does not satisfy this constraint. To avoid this, we instead fix this quantity to upper[∇2f (i)(x?)], where f (i) is the ith sample function optimized in Section 2.1 to sample x(i)? ."
    }, {
      "heading" : "2.3 Hyperparameter learning and the PES acquisition function",
      "text" : "We now show how the previous approximations are integrated to compute the acquisition function used by predictive entropy search (PES). This acquisition function performs a formal treatment of the hyperparameters. Let ψ denote a vector of hyperparameters which includes any kernel parameters as well as the noise variance σ2. Let p(ψ|Dn) ∝ p(ψ) p(Dn|ψ) denote the posterior distribution over these parameters where p(ψ) is a hyperprior and p(Dn|ψ) is the GP marginal likelihood. For a fully Bayesian treatment of ψ we must marginalize the acquisition function (3) with respect to this posterior. The corresponding integral has no analytic expression and must be approximated using Monte Carlo. This approach is also taken in [24].\nWe draw M samples {ψ(i)} from p(ψ|Dn) using slice sampling [27]. Let x(i)? denote a sampled global maximizer drawn from p(x?|Dn,ψ(i)) as described in Section 2.1. Furthermore, let v(i)n (x) and v(i)n (x|x(i)? ) denote the predictive variances computed as described in Section 2.2 when the model hyperparameters are fixed to ψ(i). We then write the marginalized acquisition function as\nαn(x) = 1 M ∑M i=1 { 0.5 log[v (i) n (x) + σ2]− 0.5 log[v(i)n (x|x(i)? ) + σ2] } . (10)\nNote that PES is effectively marginalizing the original acquisition function (2) over p(ψ|Dn). This is a significant advantage with respect to other methods that optimize the same information-theoretic acquisition function but do not marginalize over the hyper-parameters. For example, the approach of [9] approximates (2) only for fixed ψ. The resulting approximation is computationally very expensive and recomputing it to average over multiple samples from p(ψ|Dn) is infeasible in practice. Algorithm 2 shows pseudo-code for computing the PES acquisition function. Note that most of the computations necessary for evaluating (10) can be done independently of the input x, as noted in the pseudo-code. This initial cost is dominated by a matrix inversion necessary to pre-compute V for each hyperparameter sample. The resulting complexity isO[M(n+d+d(d−1)/2)3]. This cost can be reduced to O[M(n + d)3] by ignoring the derivative observations imposed on upper[∇2f(x?)] by constraint C1.1. Nevertheless, in the problems that we consider d is very small (less than 20). After these precomputations are done, the evaluation of (10) is O[M(n+ d+ d(d− 1)/2)]."
    }, {
      "heading" : "3 Experiments",
      "text" : "In our experiments, we use Gaussian process priors for f with squared-exponential kernels k(x,x′) = γ2 exp{−0.5 ∑ i(xi−x′i)2/`2i }. The corresponding spectral density is zero-mean Gaussian with covariance given by diag([`−2i ]) and normalizing constant α = γ 2. The model hyperparameters are {γ, `1, . . . , `d, σ2}. We use broad, uninformative Gamma hyperpriors.\nFirst, we analyze the accuracy of PES in the task of approximating the differential entropy (2). We compare the PES approximation (10), with the approximation used by the entropy search (ES) method [9]. We also compare with the ground truth for (2) obtained using a rejection sampling (RS) algorithm based on (3). For this experiment we generate the data Dn using an objective function f sampled from the Gaussian process prior. The domain X of f is fixed to be [0, 1]2 and data are generated using γ2 = 1, σ2 = 10−6, and `2i = 0.1. To compute (10) we avoid sampling the hyperparameters and use the known values directly. We further fix M = 200 and m = 1000.\nThe ground truth rejection sampling scheme works as follows. First,X is discretized using a uniform grid. The expectation with respect to p(x?|Dn) in (3) is then approximated using sampling. For this, we sample x? by evaluating a random sample from p(f |Dn) on each grid cell and then selecting the cell with highest value. Given x?, we then approximate H[p(y|Dn,x,x?)] by rejection sampling. We draw samples from p(f |Dn) and reject those whose corresponding grid cell with highest value is not x?. Finally, we approximate H[p(y|Dn,x,x?)] by first, adding zero-mean Gaussian noise with variance σ2 to the the evaluations at x of the functions not rejected during the previous step and second, we estimate the differential entropy of the resulting samples using kernels [1].\nFigure 1 shows the objective functions produced by RS, ES and PES for a particular Dn with 10 measurements whose locations are selected uniformly at random in [0, 1]2. The locations of the collected measurements are displayed with an “x” in the plots. The particular objective function used to generate the measurements in Dn is displayed in the left part of Figure 2. The plots in Figure 1 show that the PES approximation to (2) is more similar to the ground truth given by RS than the approximation produced by ES. In this figure we also see a discrepancy between RS and PES at locations near x = (0.572, 0.687). This difference is an artifact of the discretization used in RS. By zooming in and drawing many more samples we would see the same behavior in both plots.\nWe now evaluate the performance of PES in the task of finding the optimum of synthetic black-box objective functions. For this, we reproduce the within-model comparison experiment described in [9]. In this experiment we optimize objective functions defined in the 2-dimensional unit domain X = [0, 1]2. Each objective function is generated by first sampling 1024 function values from the GP prior assumed by PES, using the same γ2, `i and σ2 as in the previous experiment. The objective function is then given by the resulting GP posterior mean. We generated a total of 1000 objective functions by following this procedure. The left plot in Figure 2 shows an example function.\nIn these experiments we compared the performance of PES with that of ES [9] and expected improvement (EI) [13], a widely used acquisition function in the Bayesian optimization literature. We again assume that the optimal hyper-parameter values are known to all methods. Predictive performance is then measured in terms of the immediate regret (IR) |f(x̃n) − f(x?)|, where x? is the known location of the global maximum and x̃n is the recommendation of each algorithm had we stopped at step n—for all methods this is given by the maximizer of the posterior mean. The right plot in Figure 2 shows the decimal logarithm of the median of the IR obtained by each method across the 1000 different objective functions. Confidence bands equal to one standard deviation are obtained using the bootstrap method. Note that while averaging these results is also interesting, corresponding to the expected performance averaged over the prior, here we report the median IR\nbecause the empirical distribution of IR values is very heavy-tailed. In this case, the median is more representative of the exact location of the bulk of the data. These results indicate that the best method in this setting is PES, which significantly outperforms ES and EI. The plot also shows that in this case ES is significantly better than EI.\nWe perform another series of experiments in which we optimize well-known synthetic benchmark functions including a mixture of cosines [2] and Branin-Hoo (both functions defined in [0, 1]2) as well as the Hartmann-6 (defined in [0, 1]6) [15]. In all instances, we fix the measurement noise to σ2 = 10−3. For both PES and EI we marginalize the hyperparameters ψ using the approach described in Section 2.3. ES, by contrast, cannot average its approximation of (2) over the posterior on ψ. Instead, ES works by fixing ψ to an estimate of its posterior mean (obtained using slice sampling) [27]. To evaluate the gains produced by the fully Bayesian treatment ofψ in PES, we also compare with a version of PES (PES-NB) which performs the same non-Bayesian (NB) treatment of ψ as ES. In PES-NB we use a single fixed hyperparameter as in previous sections with value given by the posterior mean of ψ. All the methods are initialized with three random measurements collected using latin hypercube sampling [5].\nThe plots in Figure 3 show the median IR obtained by each method on each function across 250 random initializations. Overall, PES is better than PES-NB and ES. Furthermore, PES-NB is also significantly better than ES in most of the cases. These results show that the fully Bayesian treatment of ψ in PES is advantageous and that PES can produce better approximations than ES. Note that PES performs better than EI in the Branin and cosines functions, while EI is significantly better on the Hartmann problem. This appears to be due to the fact that entropy-based strategies explore more aggressively which in higher-dimensional spaces takes more iterations. The Hartmann problem, however, is a relatively simple problem and as a result the comparatively more greedy behavior of EI does not result in significant adverse consequences. Note that the synthetic functions optimized in the previous experiment were much more multimodal that the ones considered here."
    }, {
      "heading" : "3.1 Experiments with real-world functions",
      "text" : "We finally optimize different real-world cost functions. The first one (NNet) returns the predictive accuracy of a neural network on a random train/test partition of the Boston Housing dataset [3].\nThe variables to optimize are the weight-decay parameter and the number of training iterations for the neural network. The second function (Hydrogen) returns the amount of hydrogen production of a particular bacteria in terms of the PH and Nitrogen levels of the growth medium [7]. The third one (Portfolio) returns the ratio of the mean and the standard deviation (the Sharpe ratio) of the 1-year ahead returns generated by simulations from a multivariate time-series model that is adjusted to the daily returns of stocks AXP, BA and HD. The time-series model is formed by univariate GARCH models connected with a Student’s t copula [12]. These three functions (NNet, Hydrogen and Portfolio) have as domain [0, 1]2. Furthermore, in these examples, the ground truth function that we want to optimize is unknown and is only available through noisy measurements. To obtain a ground truth, we approximate each cost function as the predictive distribution of a GP that is adjusted to data sampled from the original function (1000 uniform samples for NNet and Portfolio and all the available data for Hydrogen [7]). Finally, we also consider another real-world function that returns the walking speed of a bipedal robot [29]. This function is defined in [0, 1]8 and its inputs are the parameters of the robot’s controller. In this case the ground truth function is noiseless and can be exactly evaluated through expensive numerical simulation. We consider two versions of this problem (Walker A) with zero-mean, additive noise of σ = 0.01 and (Walker B) with σ = 0.1.\nFigure 4 shows the median IR values obtained by each method on each function across 250 random initializations, except in Hydrogen where we used 500 due to its higher level of noise. Overall, PES, ES and PES-NB perform similarly in NNet, Hydrogen and Portfolio. EI performs rather poorly in these first three functions. This method seems to make excessively greedy decisions and fails to explore the search space enough. This strategy seems to be advantageous in Walker A, where EI obtains the best results. By contrast, PES, ES and PES-NB tend to explore more in this latter dataset. This leads to worse results than those of EI. Nevertheless, PES is significantly better than PES-NB and ES in both Walker datasets and better than EI in the noisier Walker B. In this case, the fully Bayesian treatment of hyper-parameters performed by PES produces improvements in performance."
    }, {
      "heading" : "4 Conclusions",
      "text" : "We have proposed a novel information-theoretic approach for Bayesian optimization. Our method, predictive entropy search (PES), greedily maximizes the amount of one-step information on the location x? of the global maximum using its posterior differential entropy. Since this objective function is intractable, PES approximates the original objective using a reparameterization that measures entropy in the posterior predictive distribution of the function evaluations. PES produces more accurate approximations than Entropy Search (ES), a method based on the original, non-transformed acquisition function. Furthermore, PES can easily marginalize its approximation with respect to the posterior distribution of its hyper-parameters, while ES cannot. Experiments with synthetic and real-world functions show that PES often outperforms ES in terms of immediate regret. In these experiments, we also observe that PES often produces better results than expected improvement (EI), a popular heuristic for Bayesian optimization. EI often seems to make excessively greedy decisions, while PES tends to explore more. As a result, EI seems to perform better for simple objective functions while often getting stuck with noisier objectives or for functions with many modes."
    }, {
      "heading" : "A Details on approximating GP sample paths",
      "text" : "In this section we give further details about the approach used in Section 2.1 to approximate a GP using random features. These random features can be used to approximate sample paths from the GP posterior. By optimizing these sample paths we obtain posterior samples over the global maxima x?. We derive in more detail the kernel approximation from (5). Formally, the theorem of [4] states Theorem 1 (Bochner’s theorem). A continuous, shift-invariant kernel is positive definite if and only if it is the Fourier transform of a non-negative, finite measure.\nAs a result given some kernel k(x,x′) = k(x− x′,0) there must exist an associated density s(w), known as its spectral density, which is the Fourier dual of k. This can be written as\nk(x,x′) = ∫ e−iw T(x−x′)s(w) dw,\ns(w) = 1\n(2π)d\n∫ eiw Tτk(τ ,0) dτ .\nFurther, we can treat this measure as a probability density p(w) = s(w)/α where α = ∫ s(w) dw is the normalizing constant. Consequently, the kernel can be written as\nk(x,x′) = αEp(w)[e−iw T(x−x′)]\nand due to the symmetry of p(w) [see 22] we can write the expectation as\n= αEp(w)[ 12 (e −iwT(x−x′) + eiw T(x−x′))] = αEp(w)[cos(wTx−wTx′)]\nWe can then note that ∫ 2π 0\ncos(a + 2b) db = 0 for any constant offset a ∈ R. As a result, for b uniformly distributed between 0 and 2π we can write\n= αEp(w)[cos(wTx−wTx′) + Ep(b)[cos(wTx + wTx′ + 2b)]] = αEp(w,b)[cos(wTx + b−wTx′ − b) + cos(wTx + b+ wTx′ + b)] = 2αEp(w,b)[cos(wTx + b) cos(wTx′ + b)]\nThe last equality can be derived from the sum of angles formula, which leads to the identity: 2 cos(x) cos(y) = cos(x− y) + cos(x+ y). Finally, we can average over m weights and phases\n= 2α\nm Ep(W,b)[cos(Wx + b)T cos(Wx′ + b)]\nwhere [W]i ∼ p(w) and [b]i ∼ p(b) are stacked versions of the original random variables. The resulting quantity has the same expectation but results in a lower variance estimator. If we let φ(x) = √ 2α/m cos(Wx + b) denote a random m-dimensional feature generated by this model we can also write the kernel as k(x,x′) = Ep(φ)[φ(x)Tφ(x′)].\nWe now briefly show the equivalence between a Bayesian linear model using random features φ and a GP with kernel k. Consider now a linear model f(x) = φ(x)Tθ where θ ∼ N (0, I) has a standard Gaussian distribution and observations Dn = {(xi, yi)}i≤n of the form yi ∼ N (f(xi), σ2). The posterior of θ given (Dn,φ) is also be normal N (m,V) where\nm = (ΦTΦ + σ2I)−1ΦTy,\nV = (ΦTΦ + σ2I)−1σ2,\nand where [Φ]i = φ(xi) and [y]i = yi consist of the stacked features and observations respectively. We can also easily write the predictive distribution over f evaluated at a test point x, which is Gaussian distributed with mean and variance given by\nµn(x) = φ(x) T(ΦTΦ + σ2I)−1ΦTy,\nvn(x) = φ(x) T(ΦTΦ + σ2I)−1φ(x)σ2.\nBy a simple application of the matrix-inversion lemma these quantities can be rewritten in terms which only make use of the inner products between features,\nµn(x) = φ(x) TΦT(ΦΦT + σ2I)−1y1:t\nvn(x) = φ(x) Tφ(x)− φ(x)TΦT(ΦΦT + σ2I)−1Φφ(x).\nthe expectations of which are equivalent to the kernel k and we obtain the same expressions as that in (1)."
    }, {
      "heading" : "B Details on approximating the predictive variance",
      "text" : "We now provide further details on approximating the predictive variance vn(x|x?) of inputs x given the position of the global optimizer x?. In particular we include all steps omitted in the presentation of Section 2.2.\nB.1 Incorporating the analytic latent constraints (C1.1)\nWe first turn to the random variables\nz = [f(x?); diag[∇2f(x?)]], c = [yn;∇f(x?); upper[∇2f(x?)]] = [yn; 0; 0].\nHere c contains the random variables that we will condition on in order to enforce constraint C1.1. Given the input locations x and x? we can construct a kernel matrix K containing the covariance evaluated on the stacked vector [z; c]. We again refer to [25] in constructing this matrix which includes derivative observations, the computations of which are tedious but not overly complicated. Note also that the portions of K which correspond to yi will have an additional σ2 due to the observation noise. Next let Kz, Kc, and Kzc denote the corresponding diagonal and off-diagonal blocks of the kernel matrix. We can now condition on the observed values of c to write\np(z|Dn,C1.1) = p(z|c) = N (z|m0,V0) where m0 = KzcK−1c c and V0 = Kz −KzcK−1c KTzc.\nB.2 Incorporating the non-analytic latent constraints (C1.2 and C2)\nThe additional constraints C1.2 and C2 can be introduced explicitly as in (6), which takes the form of a single Gaussian factor and d+ 1 non-Gaussian factors\np(z|Dn,C1,C2) ∝ N (z|m0,V0) [ d+1∏ i=1 ti(zi) ] .\nWe approximate this distribution using a single multivariate Gaussian q(z) where each non-Gaussian factor is replaced by a Gaussian approximation t̃i(zi) = N (zi; m̃i, ṽi) such that\nq(z) = N (z|m,V) ∝ N (z|m0,V0) [ d+1∏ i=1 N (zi; m̃i, ṽi) ]\nwhere this approximation is parameterized by m = V[Ṽ−1m̃ + V−10 m0] and V = (Ṽ −1 + V−10 ) −1. The parameters of the approximate factors are combined to form the vector [m̃]i = m̃i and the diagonal matrix [Ṽ]ii = ṽi.\nTo compute the approximate factors we use expectation propagation (EP). EP is a procedure that starts from some initial values for the approximate factors (m̃i, ṽi) and iteratively refines these quantities; here we initialize m̃i = 0 and ṽi =∞ which corresponds to m = m0 and V = V0. At each iteration, for every factor i, we remove the contribution of the ith approximate factor to form the cavity distribution q\\i(z) ∝ q(z)/t̃i(zi). Given the independent factors we consider here we can focus on each individual component q\\i(zi) separately with mean and variance\nm̄i = v̄i(mi/vii − m̃i/ṽi), v̄i = (v −1 ii − ṽ −1 i ) −1.\nLet q̂(zi) ∝ q\\i(zi)ti(zi) denote the tilted distribution where the ith approximate factor has been replaced by the corresponding real factor. EP proceeds by finding the approximation qi that minimizes the KL-divergence D[q̂i||qi] where qi is restricted to be Gaussian. This amounts to matching the first two moments. Finally, by removing the influence of the cavity distribution and setting t̃i(zi) ∝ qi(zi)/q\\i(zi) we can update the approximate factors. This can be performed using the same procedure which forms the cavity distribution.\nFor both sets of constraints used in this work the moments can easily be obtained by computing the normalizing constant Z̄i = ∫ N (zi|m̄i, v̄i) ti(zi) dzi and using the following identities:\nEq̂(zi) = m̄i + v̄i ∂Z̄i ∂m̄i , Varq̂(zi) = v̄i − v̄2i ( ∂Z̄i ∂m̄i − 2∂Z̄i ∂v̄i ) .\nFor the factors corresponding to constraints on the diagonal Hessian, i.e. where ti(zi) = I[zi < 0], the distribution is also simply a truncated Gaussian. Given these moments we can remove the contribution of the cavity distribution as above and write\nm̃i ← m̄i − κ−1, where α = − m̄i√ v̄i ,\nṽi ← β−1 − v̄i, β = φ(α)\nΦ(α)\n[ φ(α)\nΦ(α) + α\n] 1\nv̄i ,\nκ =\n[ φ(α)\nΦ(α) − α ] 1√ v̄i .\nFor the final soft-maximum constraint, Φ ( (zi−ymax)/σ ) , the moments can be calculated in a similar fashion. Using the same procedure as above we arrive at very similar updates:\nm̃i ← m̄i + κ−1, where α = m̄i − ymax√ v̄i + σ2 ,\nṽi ← β−1 − v̄i, β = φ(α)\nΦ(α)\n[ φ(α)\nΦ(α) + α\n] 1\nv̄i + σ2 ,\nκ =\n[ φ(α)\nΦ(α) + α\n] 1√\nv̄i + σ2 .\nB.3 Incorporating the prediction constraint (C3)\nGiven some test input x we now turn to the problem of making predictions about f(x). We again note that both the “prior” terms m0, V0 and the EP factors, m̃ and Ṽ, are independent of x and can be precomputed once for later use at prediction time.\nLet f = [f(x); f(x?)] be a vector given by the concatenation of the latent function at x and x?. The distribution for f given the first two constraints can be written as\np(f |Dn,C1,C2) ≈ ∫ p(f |z, c) q(z) dz = N (f |mf ,Vf ) . (11)\nBy writing p(f |z, c) above we are assuming that f is independent of C1.2 and C2 given z and as a result the above is simply an integral over the product of two Gaussians. Let K† be the crosscovariance matrix evaluated between f and [z; c] and Kf the covariance matrix associated with f . The posterior above will then be Gaussian with mean and variance\nmf = K†[K + W̃] −1[c; m̃]\nVf = Kf −K†[K + W̃]−1KT† ,\nwhere W̃ is a block-diagonal matrix where the first block is zero and the second is Ṽ (note this matrix is also diagonal since Ṽ is diagonal). Finally, these values can be plugged into (8–9) in order to arrive at vn(x|x?)."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "<lb>We propose a novel information-theoretic approach for Bayesian optimization<lb>called Predictive Entropy Search (PES). At each iteration, PES selects the next<lb>evaluation point that maximizes the expected information gained with respect to<lb>the global maximum. PES codifies this intractable acquisition function in terms<lb>of the expected reduction in the differential entropy of the predictive distribu-<lb>tion. This reformulation allows PES to obtain approximations that are both more<lb>accurate and efficient than other alternatives such as Entropy Search (ES). Fur-<lb>thermore, PES can easily perform a fully Bayesian treatment of the model hy-<lb>perparameters while ES cannot. We evaluate PES in both synthetic and real-<lb>world applications, including optimization problems in machine learning, finance,<lb>biotechnology, and robotics. We show that the increased accuracy of PES leads to<lb>significant gains in optimization performance.<lb>",
    "creator" : "LaTeX with hyperref package"
  }
}