{
  "name" : "1506.04557.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Deep Generative Models with Doubly Stochastic MCMC",
    "authors" : [ "Chao Du", "Jun Zhu", "Bo Zhang" ],
    "emails" : [ "DU-C14@MAILS.TSINGHUA.EDU.CN", "DCSZJ@MAIL.TSINGHUA.EDU.CN", "DCSZB@MAIL.TSINGHUA.EDU.CN" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Learning deep models that consist of multi-layered representations has obtained state-of-the-art performance in many tasks (Bengio et al., 2014; Hinton et al., 2006), partly due to their ability on capturing high-level abstractions. As an important family of deep models, deep generative models (DGMs) (Hinton et al., 2006; Salakhutdinov & Hinton, 2009) can answer a wide range of queries by performing probabilistic inference, such as inferring the missing values of input data, which is beyond the scope of recognition networks such as deep neural networks.\nHowever, probabilistic inference with DGMs is challenging, especially when a Bayesian formalism is adopted, which is desirable to protect the DGM from overfitting (MacKay, 1992; Neal, 1995) and to perform sparse Bayesian inference (Gan et al., 2015b) or nonparametric inference (Adams et al., 2010) to learn the network structure. For Bayesian methods in general, the posterior inference often involves intractable integrals because of several potential factors, such as that the space is extremely high-\ndimensional and that the Bayesian model is non-conjugate. To address the challenges, approximate methods have to be adopted, including variational (Jordan et al., 1999; Saul et al., 1996) and Markov chain Monte Carlo (MCMC) methods (Robert & Casella, 2005).\nMuch progress has been made on stochastic variational methods for DGMs (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2014), under some mean-field or parameterization assumptions. One key feature of such variational methods is that they marry ideas from deep neural networks to parameterize the variational distribution by a recognition network and jointly learn the parameters by optimizing a variational bound. In contrast, little work has been done on extending MCMC methods to learn DGMs in a Bayesian setting, which are often more accurate, except a few exceptions. Gan et al. (2015b) present a Gibbs sampler for deep sigmoid belief networks with a sparsity-inducing prior via data augmentation, Adams et al. (2010) present a Metropolis-Hastings method for cascading Indian buffet process and Li et al. (2016) develop a high-order stochastic gradient MCMC method and apply to deep Poisson factor analysis (Gan et al., 2015a).\nIn this paper, we present a simple and generic method, named doubly stochastic gradient MCMC, to improve the efficiency of performing Bayesian inference on DGMs. By drawing samples in the collapsed parameter space, our method extends the recent work on stochastic gradient MCMC (Welling & Teh, 2011; Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014) to deal with the challenging task of posterior inference with DGMs. Besides the stochasticity of randomly drawing a mini-batch of samples in stochastic approximation, our algorithm introduces an extra dimension of stochasticity to estimate the intractable gradients by randomly drawing the hidden variables in DGMs. The sampling can be done via a Gibbs sampler, which however has a low mixing rate in high dimensional spaces. To address that, we develop a neural adaptive importance sampler (NAIS), where the adaptive proposal is parameterized by a recognition network and the parameters are optimized ar X\niv :1\n50 6.\n04 55\n7v 4\n[ cs\n.L G\n] 7\nM ar\n2 01\nby descending inclusive KL-divergence. By combining the two types of stochasticity, we construct an asymptotically unbiased estimate of the gradient in the continuous parameter space. Then, a stochastic gradient MCMC method is applied with guarantee to (approximately) converge to the target posterior when the learning rates are set under some proper annealing scheme.\nOur method can be widely applied to the DGMs with either discrete or continuous hidden variables. In experiments, we demonstrate the efficacy on learning various DGMs, such as deep sigmoid belief networks (Mnih & Gregor, 2014), for density estimation, data generation and missing value imputation. Our results show that we can outperform many strong competitors for learning DGMs."
    }, {
      "heading" : "2. Related Work",
      "text" : "Recently, there has been a lot of interest in developing variational methods for DGMs. One common strategy for dealing with the intractable posterior distribution is to approximate it with a recognition (or inference) network, and a variational lower bound is then optimized (Kingma & Welling, 2014; Mnih & Gregor, 2014). Note in these methods the gradients are also estimated doubly stochastically. Kingma & Welling (2014) and Mnih & Gregor (2014) adopt variance reduction techniques to make these methods practically applicable. Titsias & Lázaro-Gredilla (2014) propose a so-called “doubly stochastic variational inference” method for non-conjugate Bayesian inference. We are inspired by these methods when naming ours.\nThe reweighted wake-sleep (RWS) (Bornschein & Bengio, 2015) and importance weighted autoencoder (IWAE) (Burda et al., 2015) directly estimate the log-likelihood (as well as its gradient) via importance sampling, where the proposal distribution is characterized by a recognition model. These methods reduce the gap between the variational bound and the log-likelihood, which is shown much tighter than that in Kingma & Welling (2014). Such tighter bound results in an asymptotically unbiased estimator of its gradient. We draw inspiration from these variational methods to build our MCMC samplers.\nOur work is closely related to the recent progress on neural adaptive proposals for sequential Monte Carlo (NASMC) (Gu et al., 2015). Different from our work, NASMC deals with dynamical models such as Hidden Markov models and adopts recurrent neural network as the proposal. We use a similar KL-divergence as NASMC to learn the proposal.\nFinally, Gan et al. (2015a) adopt a Monte Carlo estimate via Gibbs sampling to the intractable gradients under a stochastic MCMC method particularly for topic models. Besides a general perspective which is applicable to various types of DGM models, we propose a neural adaptive importance\nsampler which is more efficient than Gibbs sampling and leads to better estimates."
    }, {
      "heading" : "3. Doubly Stochastic Gradient MCMC for Deep Generative Models",
      "text" : "We now present the doubly stochastic gradient MCMC for deep generative models."
    }, {
      "heading" : "3.1. Deep Generative Models",
      "text" : "Let X = {xn}Nn=1 be a given dataset with N i.i.d. samples. A deep generative model (DGM) assumes that each sample xn ∈ RD is generated from a vector of hidden variables zn ∈ RH , which itself follows some prior distribution p(z|α). Let p(x|z,β) be the likelihood model. The joint probability of a DGM is as follows:\np(X,Z|θ) = N∏ n=1 p(zn|α)p(xn|zn,β), (1)\nwhere θ := (α,β). Depending on the structure of z, various DGMs have been developed, such as deep belief networks (Hinton et al., 2006), deep sigmoid belief networks (Mnih & Gregor, 2014), and deep Boltzmann machines (Salakhutdinov & Hinton, 2009).\nFor most DGMs, the hidden variables z are often assumed to have a directed multi-layer representation z = {z(l)}Ll=1, where L is the number of hidden layers. Then the prior distribution has the factorization form:\np(z|θ) = p(z(L)|θ) L−1∏ l=1 p(z(l)|z(l+1),θ), (2)\nwhere p(z(l)|z(l+1),θ) is defined by some conditional stochastic layer that takes z(l+1) as input and generates samples z(l). The likelihood model is further assumed to be a conditional stochastic layer again:\np(x|z,θ) = p(x|z(1),θ), (3) where the samples are generated conditioned on the lowest hidden layer only.\nVarious conditional stochastic layers have been developed. In the following we briefly summarize the layers used in our experiments:\nSigmoid Belief Network layer (SBN): A SBN layer (Saul et al., 1996) is a directed graphical model that defines the conditional probability of each independent binary variable z (l) i given the upper layer z (l+1) as follows:\np(z (l) i = 1|z (l+1)) = σ(Wi,:z (l+1) + bi), (4)\nwhere σ(x) = 1/(1 + e−x) is the sigmoid function, Wi,: denotes the i-th row of the weight matrix and bi is the bias.\nDeep Autoregressive Network layer (DARN): A DARN (Gregor et al., 2014) layer assumes in-layer connections on the SBN layer. It defines the probability of each binary variable z(l)i conditioned on both the upper\nlayer z(l+1) and the previous z(l)<i in the same layer:\np(z (l) i = 1|z (l) <i, z (l+1)) = σ(Ui,:z (l+1) +Wi,<iz (l) <i + bi),\nwhere z(l)<i refers to (z (l) 1 , · · · , z (l) i−1) > and Wi,<i denotes the first i elements of the i-th row of the in-layer connection weight matrix.\nConditional NADE layer: The NADE (Larochelle & Murray, 2011) models the distribution of high-dimensional discrete variables x autoregressively with an internal MLP (Bengio & Bengio, 2000). The dependency between the variables is captured by a single-hidden-layer feedforward neural network:\np(xi = 1|x<i) = σ(Vi,:σ(W:,<ix<i + a) + bi).\nBoulanger-Lewandowski et al. (2012) and Bornschein & Bengio (2015) amend this model to a conditional NADE layer:\np(z (l) i = 1|z (l) <i, z (l+1)) = σ ( Vi,:σ(W:,<iz (l) <i+\nUz(l+1) + a) +Ri,:z (l+1) + bi ) ,\n(5)\nwhere we use W:,<i to refer the sub-matrix consisting the first i columns of W.\nVariational Auto-Encoder layer (VAE): VAE (Kingma & Welling, 2014) differs from the above layers in that its output can be binary or real-valued variables. It contains an internal MLP f(z(l+1)) which encodes the parameters of the distribution p(z(l)|z(l+1)). The MLP may itself contain multiple deterministic layers. For binary output variable, the distribution of each individual variable is:\np(z (l) i = 1|z (l+1)) = σ(Wi,:f(z (l+1)) + bi). (6)\nFor real-value output variable, the distribution of each independent variable z(l)i is a normal distribution whose mean and variance are as follows:\nµi =Wµi,:f(z (l+1)) + bµi,\nlog σ2i =Wσi,:f(z (l+1)) + bσi.\n(7)\nTop layer and Likelihood model: The likelihood model in Eqn. (3) can be obtained by treating x as z(0). The distribution of top layer p(z(L)|θ), which has no ancestral layer, can be obtained by simply treating the input as 0 vector or setting to fixed distribution, e.g., standard normal for the VAE layer (Kingma & Welling, 2014). Detailed description of the layers and the model construction can be found in Supplementary Material."
    }, {
      "heading" : "3.2. Variational MLE for DGMs",
      "text" : "Learning DGMs is often very challenging due to the intractability of posterior inference. One popular type of methods resort to stochastic variational methods under the maximum likelihood estimation (MLE) framework, θ̂ =\nargmaxθ log p(X|θ). These methods commonly utilize some variational distribution q(z|x;φ) to approximate the true posterior p(z|x,θ). For DGMs, the variational distribution q(z|x;φ) can be formalized as a recognition model (or inference network) (Kingma & Welling, 2014; Mnih & Gregor, 2014; Bornschein & Bengio, 2015), which takes x as inputs and outputs z stochastically. Specifically, for the DGMs with multi-layer representation z = {z(l)}Ll=1 described in Sec. 3.1, the variational distribution can be formulated as:\nq(z|x,φ) = q(z(1)|x,φ) L−1∏ l=1 q(z(l+1)|z(l),φ), (8)\nwhere each q(z(l+1)|z(l),φ) and q(z(1)|x,φ) are again defined by some stochastic layers parametrized by φ. With the variational distribution, a variational bound of the loglikelihood log p(X|θ) can be derived and optimized, e.g., the variational lower bound in (Kingma & Welling, 2014) and a tighter bound in (Burda et al., 2015).\nHowever, the variational bound is often intractable to compute analytically for DGMs. To address this challenge, recent progress (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014) has adopted hybrid Monte Carlo and variational methods, which approximate the intractable expectations and their gradients over the parameters (θ,φ) via some unbiased Monte Carlo estimates. Furthermore, to handle large-scale datasets, stochastic optimization (Robbins & Monro, 1951; Bottou, 1998) of the variational objective can be used with a suitable learning rate annealing scheme. Note variance reduction is a key part of these methods to have fast and stable convergence."
    }, {
      "heading" : "3.3. Doubly Stochastic Gradient MCMC",
      "text" : "We consider the Bayesian setting to infer the posterior distribution p(θ,Z|X) ∝ p0(θ)p(Z|θ)p(X|Z,θ) or its marginal distribution p(θ|X), by assuming some prior p0(θ). A Bayesian formalism of deep learning enjoys several advantages, such as preventing the model from overfitting and performing sparse/nonparametric Bayesian inference, as mentioned before. However, except a handful of special examples, the posterior distribution is intractable to infer. Though variational methods can be developed as in (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014; Bornschein & Bengio, 2015), under some mean-field or parameterization assumptions, they often require non-trivial model-specific deviations and may lead to inaccurate approximation when the assumptions are not properly made. Here, we consider MCMC methods, which are more generally applicable and can asymptotically approach the target posterior.\nA straightforward application of MCMC methods can be Gibbs sampling or stochastic gradient MCMC (Welling & Teh, 2011; Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014). However, a Gibbs sampler can suffer from the\nrandom-walk behavior in high-dimensional spaces. Furthermore, a Gibbs sampler would need to process all data at each iteration, which is prohibitive when dealing with large-scale datasets. The stochastic gradient MCMC methods can lead to significant speedup by exploring statistical redundancy in large datasets; but they require that the sample space is continuous, which is not true for many DGMs, such as deep sigmoid belief networks that have discrete hidden variables. Below, we present a doubly stochastic gradient MCMC with general applicability."
    }, {
      "heading" : "3.3.1. GENERAL PROCEDURE",
      "text" : "We make the mildest assumption that the parameter space is continuous and the log joint distribution log p(x, z|θ) is differentiable with respect to the model parameters θ almost everywhere except a zero-mass set. Such an assumption is true for almost all existing DGMs. Then, our method draws samples in a collapsed space that involves the model parameters θ only, by integrating out the hidden variables z:\np(θ|X) = 1 p(X) p0(θ) N∏ n=1 ∫ p(xn, zn|θ) dzn, (9)\nwhere for discrete variables the integral will be a summation. Then the gradient of the log-posterior is ∇θ log p(θ|X) = ∇θ log p0(θ) + ∑N n=1∇θ log p(xn|θ), where the second term can be calculated as:\n∇θ log p(x|θ) = 1 p(x|θ) ∂ ∂θ\n∫ p(x, z|θ) dz\n= ∫ p(x, z|θ) p(x|θ) ∂ ∂θ log p(x, z|θ) dz\n= Ep(z|x,θ) [ ∂\n∂θ log p(x, z|θ)\n] . (10)\nWith the above gradient, we can adopt a stochastic gradient MCMC (SG-MCMC) method to draw samples of θ. We consider the stochastic gradient Nosé-Hoover thermostat (SGNHT) (Ding et al., 2014). Note our method can be naturally extended to other SG-MCMC methods, e.g., stochastic gradient Langevin dynamics (Welling & Teh, 2011), stochastic gradient Hamiltonian Monte Carlo (Chen et al., 2014) and high-order stochastic gradient thermostats (Li et al., 2016). SGNHT defines a potential energy U(θ) = − log p(θ|X) where p(θ|X) is the target posterior distribution, and use a random mini-batch B of the data X to approximate the true gradient of the potential energy:\n∇θŨ(θ)=−∇θ log p0(θ)− N |B| ∑ n∈B ∇θ log p(xn|θ). (11)\nWe follow Gan et al. (2015a) to use the multivariate version of SGNHT that generate samples by simulating the dynamics as follows:\nθt+1 = θt + λpt, (12) pt+1 = pt − λξt pt − λ∇θŨ(θt+1) + √ 2AN (0, λI),\nξt+1 = ξt + λ(pt+1 pt+1 − I),\nwhere represent element-wise product, p are the augmented momentum variables, ξ are the diffusion factors, λ is the step size andA is a constant that controls the noise injected. With a proper annealing scheme over the step size λ, the Hamiltonian dynamics will converge to the target posterior."
    }, {
      "heading" : "3.3.2. NEURAL ADAPTIVE IMPORTANCE SAMPLER",
      "text" : "The remaining challenge is to compute the gradient as the expectation in Eqn. (10) is often intractable for DGMs. Here, we construct an unbiased estimate of the gradient by a set of samples {z(s)}Ss=1 from the posterior p(z|x,θ):\n∇θ log p(x|θ) ≈ 1\nS S∑ s=1 ( ∂ ∂θ log p(x, z(s)|θ) ) . (13)\nTo draw the samples z(s), a straightforward strategy is Gibbs sampling. Gibbs samplers are simple and applicable to both discrete and continuous hidden variables. However, it may be hard to develop Gibbs samplers for most DGMs, as the highly complicated models often result in non-conjugacy. More importantly, a Gibbs sampler can be slow to mix in high-dimensional spaces. Below, we present a neural adaptive importance sampler (NAIS), which again applies to both discrete and continuous hidden variables but with faster mixing rates.\nLet q(z|x;φ) be a proposal distribution which satisfies q(z|x;φ) > 0 wherever p(z|x,θ) > 0, we then have ∇θ log p(x|θ) = Eq(z|x;φ) [ p(z|x,θ) q(z|x;φ) ∂ ∂θ log p(x, z|θ) ] ,\nfrom which an unbiased importance sampling estimator can be derived with the sample weights being p(z|x,θ)q(z|x;φ) . However, computing p(z|x,θ) is often hard for most DGMs. By noticing that p(z|x,θ) ∝ p(x, z|θ) and computing p(x, z|θ) is easy, we derive a self-normalized importance sampling estimate as follows: ∇θ log p(x|θ) ≈ ∑S s=1 ( ∂ ∂θ log p(x, z (s)|θ) ) · ω(s)∑S\ns=1 ω (s)\n, (14)\nwhere {z(s)}Ss=1 is a set of samples drawn from the proposal q(z|x;φ) and ω(s) = p(x,z\n(s)|θ) q(z(s)|x;φ) is the unnormalized\nlikelihood ratio. This estimate is asymptotically consistent (Owen, 2013), and its slight bias decreases as drawing more samples.\nNeural Adaptive Proposals: To reduce the variance of the estimator in Eqn. (14) and get accurate gradient estimates, q(z|x;φ) should be as close to p(z|x,θ) as possible. Here, we draw inspirations from variational methods and learn adaptive proposals (Gu et al., 2015) by minimizing some criterion. Specifically, we build a recognition model (or inference network) to represent the proposal distribution q(z|x;φ) of hidden variables, as in the variational methods (Kingma & Welling, 2014; Bornschein & Bengio, 2015). Such a recognition model takes x as input and outputs {z(s)} as samples from q(z|x;φ), as described in Sec. 3.2. We optimize the quality of the proposal distribution by minimizing the inclusive KL-divergence between the target posterior distribution and the proposal Ep(z|x,θ)[log p(z|x,θ)q(z|x;φ) ] (Bornschein & Bengio, 2015; Gu et al., 2015) or equivalently maximizing the expected loglikelihood of the recognition model\nJ (φ;θ,x) = Ep(z|x,θ)[log q(z|x;φ)]. (15)\nWe choose this objective due to the following reasons. If the target posterior belongs to the family of proposal distributions, maximizing J (φ;θ,x) leads to the optimal solution that is the target posterior; otherwise, minimizing the inclusive KL-divergence tends to find proposal distributions that have higher entropy than the target posterior. Such a property is advantageous for importance sampling as we require that q(z|x;φ) > 0 wherever p(z|x,θ) > 0. In contrast, the exclusive KL-divergence L(φ;θ,x) := Eq(z|x;φ)[log q(z|x;φ)p(z|x,θ) ], as widely adopted in the variational methods (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014), does not have such a property — It can happen that q(z|x;φ) = 0 when p(z|x,θ) > 0; therefore unsuitable for importance sampling.\nThe gradient of J (φ;θ,x) with respect to the parameters of the proposal distribution is\n∇φJ (φ;θ,x) = Ep(z|x,θ)[∇φ log q(z|x;φ)], (16)\nwhich can be estimated using importance sampling similar as in Eqn. (14):\n∇φJ (φ;θ,x) ≈\n∑S s=1 ( ∂ ∂φ log q(z (s);x,θ) ) · ω(s)∑S\ns=1 ω (s)\n, (17)\nwhere {z(s)}Ss=1 are samples from the latest proposal distribution q(z|x;φ) and the weights are the same as in Eqn. (14). To improve the efficiency, we adopt stochastic gradient descent methods to optimize the objective J (φ;θ,X) := ∑N n=1 J (φ;θ,xn), with the gradient being estimated by a random mini-batch of data points B at each iteration:\n∇φJ (φ;θ,X) ≈ N |B| ∑ n∈B ∇φJ (φ;θ,xn), (18)\nAlgorithm 1 Doubly Stochastic Gradient MCMC with Neural Adaptive Proposals\nInput: data X Initialize θ, φ for epoch = 1, 2, · · · do\nfor mini-batch Bi ⊂ {1, · · · , N} do Sample {z(s)n } ∼ q(z|xn;φ), n ∈ Bi Estimate ∇ log p(xn|θ) with Eqn. (14), n ∈ Bi Update θ with Eqn. (29) Sample {z(s)n } ∼ q(z|xn;φ), n ∈ Bi (optionally) Update φ with the gradient in Eqn. (18)\nend for end for Output: samples of θ\nwhere each term ∇φJ (φ;θ,xn) is further estimated by samples as in Eqn. (17).\nWith the above gradient estimates, we get the overall algorithm with neural adaptive importance sampling, as outlined in Alg. 1, where we adaptively update the proposal distribution by performing one step of recognition model update after each step of SGNGT simulation. Practically, re-sampling the hidden variables before each updating is helpful to get more accurate estimations. The more detailed version of Alg. 1 is included in Supplementary Material."
    }, {
      "heading" : "4. Experiments",
      "text" : "We now present a series of experimental results of our doubly stochastic MCMC method on several representative deep generative models. We use the doubly stochastic gradient Nosé-Hoover thermostat with a neural adaptive importance sampler (DSGNHT-NAIS) in the experiments. In Sec. 4.2, various DGMs with discrete hidden variables, such as sigmoid belief networks (Neal, 1992), are trained on the binarized MNIST (Salakhutdinov & Murray, 2008) and the Caltech 101 Silhouettes (Marlin et al., 2010) datasets. We compare the predictive performance with state-of-the-art methods in terms of the estimated loglikelihood (Est. LL.) on the test set. We also demonstrate the generative performance and analyze the sensitivity to main hyperparameters. In Sec. 4.3, we train variational auto-encoders (Kingma & Welling, 2014) on the binarized MNIST and the Omniglot (Lake et al., 2013) datasets."
    }, {
      "heading" : "4.1. Setup",
      "text" : "In our experiments, all models (including recognition models) are initialized following the heuristic of Glorot & Bengio (2010). We set the Student-t prior to all the model parameters. We use the reformulated form of multivariate SGNHT as described in Supplementary Material. The perbatch learning rate γ is set among {0.01, 0.005, 0.001},\nTable 1. MNIST results of various methods on five benchmark architectures. “Dim” denotes the number of hidden variables in each layer, with layer closest to the data laying left. Values within brackets are variational lower bounds, values without brackets are estimated loglikelihoods. (?) Use NADE layers for recognition model. The results of NVIL are from Mnih & Gregor (2014); the results of Wake-sleep and RWS are from Bornschein & Bengio (2015); and the results of Data Augmentation (DA) are from Gan et al. (2015b).\nModel Dim NVIL Wake-Sleep RWS DA DSGNHT-Gibbs DSGNHT-NAIS SBN 200 (−113.1) −116.3(−120.7) −103.1 (−113.02) −102.9 −101.8 SBN 200-200 (−99.8) −106.9(−109.4) −93.4 (−110.74) −100.6 −92.5 SBN 200-200-200 (−96.7) −101.3(−104.4) −90.1 – −97.5 −89.9 DARN 200 – – −89.2? (−102.11) −101.1 −89.3? NADE 200 – – −86.8? – – −83.7?\n10 0\n10 1\n10 2\n−102.3 −102.1 −101.9 −101.7\nSBN 200\nE st\n. t es\nt L L.\n10 0\n10 1\n10 2\n−90.4 −90.2\n−90 −89.8\nSBN 200−200−200\n10 0\n10 1\n10 2\n−91.5\n−90.5\n−89.5\nDARN 200\nE st\n. t es\nt L L.\n# samples 10\n0 10 1 10 2\n−85.5\n−84.5\n−83.5\nNADE 200\n# samples\nFigure 1. Log-likelihood estimation on MNIST for different models w.r.t number of posterior samples M used for posterior mean estimator. Dotted line marks the results reported in Table 1.\nfrom which we report the experiment with best performance. If not noted otherwise, the number of samples used during training is set to S = 5. The mini-batch size |B| is set to 100 for all experiments. The parameters of recognition model are updated using the Adam (Kingma & Ba, 2015) optimizer with step sizes of {1, 3, 5} × 10−4.\nAs our method infers the posterior p(θ|X), we adopt the posterior mean estimator for model evaluation:\nθ̂ = Ep(θ|X)[θ] ≈ 1\nM M∑ m=1 θ(m). (19)\nTo compute the posterior mean, we start to collect posterior samples when we observe the Est. LL. on the validation set does not increase for 10 consecutive epochs. Then M samples {θ(m)}Mm=1 fromM more epochs are averaged for final evaluation. If not mentioned otherwise, the number of samples used for computing the posterior mean is set to M = 100. We will also show howM influences the results.\nTo evaluate the inferred model θ̂ in terms of Est. LL., we adopt the K-sample importance weighting estimation LK :\nLK = Ez(k)∼q(z|x;φ)\n[ log 1\nK K∑ k=1 p(x, z(k)) q(z(k)|x)\n] . (20)\nSuch estimation is also used by Bornschein & Bengio (2015) and Burda et al. (2015). We will clarify how we set the number of samples K used for estimating the loglikelihoods and investigate how it influences the quality of the estimator.\nDARN and NADE are not permutation-invariant models. In our experiments, the ordering is simply determined by the original order in the dataset.\nSee Supplementary Material for more details about the experimental setting."
    }, {
      "heading" : "4.2. Discrete Hidden Variable Models",
      "text" : ""
    }, {
      "heading" : "4.2.1. BINARIZED MNIST",
      "text" : "The binarized MNIST dataset consists of 50, 000 training samples, 10, 000 validation samples and 10, 000 test samples.\nWe consider five benchmark models: three SBN models, one DARN model and one NADE model. For the three models with SBN layers, we also use SBN layers to construct the recognition model; for the two models with DARN layer and NADE layer, we follow Bornschein & Bengio (2015) to use NADE layer for the recognition model. The model sizes and the results are summarized in Table 1. Details of the construction of the models are summarized in Supplementary Material.\nWe first investigate the effect of the neural adaptive importance sampler (NAIS). For comparison, we also estimate the gradient Eqn. (10) by directly sampling from p(z|x,θ) using a Gibbs sampler. (The derivation for the Gibbs sampler is included in Supplementary Material.) We denote the resulting method by DSGNHT-Gibbs. In Table 1 we observe that the DSGNHT using a NAIS consistently outper-\nforms the DSGNHT using a Gibbs sampler, especially for deeper models and autoregressive models, since the model parameters are higher-dimensional and highly correlated. We then compare our method to several other state-of-theart methods on the five benchmark models. We observe that our method outperforms RWS almost on all models, except for DARN 200, on which we are slightly worse than RWS.\nWe compare our best result to the state-of-the-art results on binarized MNIST in Table 2. The NADE/NADE 200 model achieves an Est. LL. of −83.67, which outperforms most published results. Gregor et al. (2015) give a lower bound −80.97, which exploits spatial structure. IWAE\n(Burda et al., 2015) achieves −82.90, which is trained on the original MNIST dataset (Lecun et al., 1998) and thus not directly comparable. We cite their results on the binarized MNIST in Table 2.\nIn Fig. 1, we investigate the influence of the number of samples M on the posterior mean estimator Eqn. (19). We can observe that on all models using more samples for posterior mean brings consistent improvements. On SBN/SBN models using M = 100 samples improves around 0.6 nat than using M = 1 (in which case one posterior sample is estimated). On autoregressive models, using M = 100 samples brings an improvement more than 2 nats.\nWe show the influence of the number of samples S used during training in Fig. 2(a). we observe that the Est. LL. on test data improves as S grows up. Fig. 2(b) presents the curves of the final estimated test log-likelihood with re-\nspect to the number of samples K used for the estimator Eqn. (20). We observe that K = 100, 000 and K = 500 are large enough to get a good Est. LL. for SBN and DARN(NADE) models, respectively.\nFig. 3 visualizes the generative performance of the learned models. In Fig. 3(a), we show the randomly sampled training data of MNIST and Caltech 101 Silhouettes. Fig. 3(b) displays the examples generated from the learned models. We observe that the generated samples are visually good.\nOne advantage of Bayesian framework is that we can specify sparsity-encouraging priors on the model parameters explicitly, e.g., the Student-t prior in our experiments. Fig. 3(c) and Fig. 3(d) demonstrate the difference between features learned with a sparse (Student-t prior) prior and a non-sparse (Gaussian) prior. We observe that the features learned with a sparse prior appear more localized.\nWe further demonstrate the ability of the learned models on predicting missing data. For each test image, the lower half is assumed missing and the upper half is used to inference the hidden units (Gan et al., 2015b). Then, with the hidden units, the lower half is reconstructed. Prediction is done by repeating this procedure and finally sampling from the generative model with the inferred hidden units. Fig. 4 demonstrates some example completions for the missing data on MNIST."
    }, {
      "heading" : "4.2.2. CALTECH 101 SILHOUETTES",
      "text" : "The Caltech 101 Silhouettes dataset consists of 4, 100 training samples, 2, 264 validation samples and 2, 307 test samples. We first compare our method to RWS on two benchmark models in Table 3 (Top) and observe that our method achieves significant improvements. On SBN/SBN 200- 200, we get a test Est. LL. of −108.0 which improves over RWS for 18 nats.\nTable 3 (Bottom) summarizes ours best results and other state-of-the-art results. Our NADE/NADE 150 network\nreaches a test Est. LL. of −100.0, which improves RWS on the same model for 4.3 nats. We observe a remarkable effect of increasing the number of samples M for posterior mean: the test Est. LL. of −100.0 at M = 100 improves 5.3 nats compared to −105.3 at M = 1. Gan et al. (2015b) achieve −96.40 by training FVSBN (Frey, 1998) with both training data and validation data. A latest work by Goessling & Amit (2015) achieves −88.48 by developing a mixture model of sparse autoregressive network. Fig. 3(b) visualizes the samples drawn from the learned models."
    }, {
      "heading" : "4.3. Variational Auto-Encoders",
      "text" : "Finally, we consider the DGMs with continuous hidden variables. One popular example is the variational autoencoder (VAE) (Kingma & Welling, 2014). Intuitively, the posterior of DGMs with continuous hidden variables is harder to capture, as the hidden variables have much more freedom compared to that of DGMs with discrete hidden variables. Such freedom potentially results in high variance of the gradients estimation. VAE and the importance weighted auto-encoders (IWAE) (Burda et al., 2015) alleviate this problem by adopting a reparametrization trick. Then the variational parameters φ can be optimized tying to the generative model.\nIn our DSGNHT-NAIS, we indeed observe high variance of the gradients estimation. However in theory, any distribution that satisfies q(z|x;φ) > 0 wherever p(z|x,θ) > 0 can be used as a proposal. Such a property makes any other reasonable objectives (instead of the inclusive KLdivergence described in Sec. 3.3) for q(z|x;φ) is adoptable. We adopt the same objective in IWAE for the proposal distribution 1 and find it works well in practice.\nWe follow IWAE to train a single-stochastic-layer VAE with 50 hidden units. In between the data and the hidden variables are two deterministic layers with tanh activation. The model is trained on the binarized MNIST and the Omniglot datasets. We use the Omniglot dataset downloaded from Burda et al. (2015) which consists of 23, 000 training samples, 1, 345 validation samples and 87, 00 test samples. We use M = 200 for the posterior mean estimator and follow IWAE to use K = 5, 000 to evaluate the test Est. LL.. Since IWAE also adopts an importance sampler to estimate\n1We use the objective in IWAE for optimizing the proposal distribution q(z|x;φ) only, leaving other part of our method unchanged.\nthe objective (as well as its gradient), we compare the results using S = 5 samples during training for both IWAE and our method. We achieve comparable or better results as summarized in Table 4."
    }, {
      "heading" : "5. Conclusions and Future Work",
      "text" : "We propose a powerful Bayesian inference method based on stochastic gradient MCMC for deep generative models with continuous parameter space. It enjoys several advantages of Bayesian formalism such as sparse Bayesian inference. Our results include state-of-the-art performance on standard published datasets.\nFor future work we like to investigate the performance on learning sparse Bayesian models. Also, learning nonparametric Bayesian DGMs is another interesting challenge."
    }, {
      "heading" : "A. Model Setup",
      "text" : "We describe how the models are constructed with the conditional stochastic layers. Each model should consists of a top layer p(z(L)|θ), a data layer p(x|z(1),θ) and (optionally) several intermediate layer p(z(l)|z(1+1),θ). The output of the layer (l + 1) (random samples) is passed as the input to the layer l and thus a full generative process for the data is built. In principle, the type of each layer can be chosen arbitrarily, as long as the input dimension and the output dimension of adjacent layers match to each other.\nThe recognition model can be constructed in a similar way. Given a generative model with L hidden layers, the recognition model should also contains L stochastic layers. The first layer takes x as input and outputs random samples of z(1). The output of the layer l (random samples of z(l)) is passed as the input to the layer l+1. Note the type of each layer can also be chosen arbitrarily, as long as the dimensions of adjacent layers match to each other and the output dimension of the l-th layer matches the input dimension of the l-th layer of the generative model.\nIn our experiments, all tested models and their recognition models consist only one type of stochastic layer. In the following we describe the detailed architectures of our tested models.\nSBN/SBN models:\nFor all models with SBN layers we construct the recognition model with SBN layers too. We use four SBN/SBN architectures in the experiments: 1-hidden-layer with 200 hidden units (SBN/SBN 200); 2-hidden-layer with 200 hidden units in each hidden layer (SBN/SBN 200-200); 3-hidden-layer with 200 hidden units in each hidden layer (SBN/SBN 200-200-200); and 4-hidden-layer with 300(closest to data), 100, 50, 10 hidden units (SBN/SBN 300-100-50-10).\nWe use the following top layer (equivalent to factorized Bernoulli distribution):\np(z (L) i = 1|θ) = σ(bi), (21)\nand the likelihood model:\np(z (l) i = 1|z (l+1),θ) = σ(W (l) i,: z (l+1) + b (l) i ), (22)\nwhere we define z(0) = x. The model parameters are θ = {W(l),b(l)}L−1l=0 ∪ {b(L)}.\nDARN/NADE models:\nwe follow Bornschein & Bengio (2015) to use NADE layers in the recognition model for the DARN models. We test a shallow model (1-hidden-layer DARN/NADE 200) in our experiments. We use the following top layer (equivalent to\nFVSBN (Frey, 1998)):\np(z (L) i = 1|z (L) <i ,θ) = σ(W (L) i,<iz (L) <i + b (L) i ), (23)\nand the likelihood model:\np(z (l) i = 1|z (l) <i, z (l+1),θ) =\nσ(U (l) i,: z (l+1) +W (l) i,<iz (l) <i + b (l) i ).\n(24)\nThe model parameters are θ = {U(l)}L−1l=0 ∪ {W(l),b(l)}Ll=0.\nNADE/NADE models:\nWe test a shallow model (1-hidden-layer NADE/NADE 200) in our experiments. We use the following top layer:\np(z (L) i = 1|z (L) <i ,θ) = σ ( V\n(L) i,: σ(W (L) :,<iz (L) <i + a (L)) + b (L) i\n) ,\n(25)\nand the likelihood model:\np(z (l) i = 1|z (l) <i, z\n(l+1),θ) = σ ( V\n(l) i,: σ(W (l) :,<iz (l) <i\n+U(l)z(l+1) + a(l)) +R (l) i,: z (l+1) + b (l) i\n) , (26)\nThe model parameters are θ = {U(l),R(l)}L−1l=0 ∪ {V(l),W(l),a(l),b(l)}Ll=0.\nVAE/VAE models:\nFor the VAE model, we follow Kingma & Welling (2014) to use an isotropic multivariate Gaussian top layer:\np(z (L) i = 1|θ) = N (0, I). (27)\nThe VAE stochastic layer itself contains an internal MLP. In our experiments, we train single-stochastic-layer VAE with 50 hidden units. In between the data and the hidden variables are two deterministic layers with tanh activation. The dimension of the two deterministic layers are both 100. The recognition model is a stochastic VAE layer within which are two 100-dimensional deterministic layers. Such an architecture is used in Burda et al. (2015).\nIn experiments of training VAE, we adopt the objective in IWAE (Burda et al., 2015) for learning the proposal distribution q(z|x;φ):\nLK = Ez(k)∼q(z|x;φ)\n[ log 1\nK K∑ k=1 p(x, z(k)) q(z(k)|x)\n] . (28)\nThen the gradient can be evaluated by adopting the\nreparametrization trick (Kingma & Welling, 2014): ∇φLK =∇φEz(k)∼q(z|x;φ) [ log 1\nK ∑K k=1 p(x, z(k)) q(z(k)|x) ] =∇φE (k)∼p( ) [ log 1\nK ∑K k=1 w(x, z( (k),x,φ)) ] =E (k)∼p( ) [ ∇φ log 1\nK ∑K k=1 w(x, z( (k),x,φ)) ] =E (k)∼p( ) [∑K k=1 w̃k∇φ logw(x, z( (k),x,φ)) ] ,\nwhere we have omitted the model parameters θ in the above gradients, since θ is fixed when learning the proposal distribution. In the above derivations, (1), · · · , (K) are the auxiliary variables as defined in VAE. wk = w(x, z( (k),x,φ)) = p(x,z(\n(k),x,φ)) q(z( (k),x,φ)|x) are the importance\nweight and w̃i are the normalized importance weights as defined in IWAE."
    }, {
      "heading" : "B. Experimental Setup",
      "text" : "We describe our experimental setup here, including the parameter setting and implementation details.\nIn our implementation, we use the reformulated form of multivariate SGNHT (Ding et al., 2014):\nθt+1 = θt + ut, (29)\nut+1 = ut − ξt ut − η∇θŨ(θt+1) +N (0, 2aηI), αt+1 = αt + (pt+1 pt+1 − ηI),\nwhere we have setting u = λp, η = λ2, α = λξ and a = Aλ. This reformulation is cleaner and easier to implement. In analog to SGD with momentum, η is called the learning rate and 1 − α are the momentum terms (Chen et al., 2014). The initialization of SGNHT is as follows: u is random sampled from N (0, ηI) and α is initialized as aI. There are three parameters for SGNHT: the learning rate η, the momentum decay a, and the mini-batch size B. In our implementation we choose the mini-batch size B = 100, the momentum decay a = {0.1, 0.01}. For numerical stability, we choose η = γN , where γ is called the “per-batch learning rate” (Chen et al., 2014). The perbatch learning rate γ is chosen from {0.01, 0.005, 0.001} with best performance.\nFor the recognition model, we use Adam (Kingma & Ba, 2015) to learn the parameters φ. There are four parameters for Adam: the stepsize η′, the exponential decay rates {β1, β2} and which is used to prevent division by zero. In our implementation we choose β1 = 0.9, β2 = 0.999 and = 10−10. The stepsize η′ is chosen from {1, 3, 5}×10−4 with best performance.\nThe model parameters are initialized following the heuristic of Glorot & Bengio (2010). The Student-t’s prior for all\nAlgorithm 2 A Detailed Version of Doubly Stochastic Gradient MCMC with Neural Adaptive Proposals Input: X = {x1, · · · ,xN}: the dataset Input: S: number of samples used during training Input: M : number of samples used for computing the posterior mean estimation Eqn. (19) Input: γ: per-batch learning rate for SGNHT, |B|: mini-batch size, a: momentum decay Input: η′: step size for Adam Input: nθ, nφ: number of θ or φ update during each mini-batch Initialize θ, φ: following the heuristic of Glorot & Bengio (2010) Initialize u ∼ N (0, ηI), α = aI for epoch = 1, 2, · · · do\nRandomly split the data X into mini-batches B1, · · · , BN/|B| for mini-batch Bi in {B1, · · · , BN/|B|} do\nfor iθ = 1, · · · , nθ do Sample {z(s)n }Ss=1 from the proposal q(z|xn;φ), n ∈ Bi Estimate the gradients∇ log p(xn|θ) with Eqn. (14), n ∈ Bi Update θ with Eqn. (29) for iφ = 1, · · · , nφ do\nSample {z(s)n }Ss=1 from the proposal q(z|xn;φ), n ∈ Bi Estimate the gradients∇φJ (φ;θ,X) with Eqn. (18), n ∈ Bi Update φ using Adam optimizer with∇φJ (φ;θ,X)\nend for end for\nend for end for Run another M epochs to estimate the posterior mean θ̂ Output: θ̂\nmodel parameters are set with a scale parameter σ = 0.09, location parameter µ = 0 and degrees of freedom ν = 2.2.\nOur method involves updating the generative model parameters θ and the recognition model parameters φ together (one step of θ update and one step of φ update within each mini-batch). One natural extension is to make the numbers of the two type of updates adjustable. We thus set the parameter nθ which controls the number of steps of θ update within each mini-batch, and parameter nφ which controls the number of steps of φ update following each step of θ update. Larger nθ can potentially make the samples of θ less correlated. Larger nφ can potentially make the proposal distribution more accurate. We set nθ = 10 and nφ = 1 in our implementation.\nFinally, in Alg. 2 we summarize a detailed version of our method."
    }, {
      "heading" : "C. Derivations",
      "text" : "We provide the derivations of the Gibbs sampler for the DSGNHT-Gibbs. The hidden variables are sampled layer-wisely and dimension-wisely. We define z(0) = x and z(L+1) = 0 for convenience. Then the probability p(z\n(l) i |z (l) ¬i ,x, z (¬l)) can be written as p(z(l)i |z (l) ¬i , z (¬l)).\nWe have the following Gibbs sampler:\np(z (l) i |z (l) ¬i , z (¬l))\n=p(z (l) i |z (l) ¬i , z (<l), z(>l))\n∝p(z(<l)|z(l)i , z (l) ¬i , z (>l)) · p(z(l)i |z (l) ¬i , z (>l))\n=p(z(<l)|z(l)) · p(z(l)i |z (l+1)) ∝p(z(l−1)|z(l)) · p(z(l)i |z (l+1))\n= D(l−1)∏ i′=1 exp [ (W (l−1)> i,: z (l)+b (l) i′ )z (l−1) i′ −log(1+e (W (l−1)> i,: z (l)+b (l) i′ )) ] ×exp [ (W (l)>\ni,: z (l+1)+b (l) i )z (l) i −log(1+e\n(W (l−1)> i,: z (l+1)+b (l−1) i ))\n]\n∝ exp [ ( D(l−1)∑ i′=1 W (l−1) i′i z (l−1) i′ + (W (l)> i,: z (l+1) + b (l) i ))z (l) i\n− D(l−1)∑ i′=1 log(1 + e (W (l−1)> i′,: z (l)+b (l) i′ )) ] .\nA Gibbs sampler for DARN can be derived similarly."
    } ],
    "references" : [ {
      "title" : "Learning the structure of deep sparse graphical models",
      "author" : [ "R. Adams", "H. Wallach", "Z. Ghahramani" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Adams et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Adams et al\\.",
      "year" : 2010
    }, {
      "title" : "Bayesian posterior sampling via stochastic gradient fisher scoring",
      "author" : [ "S. Ahn", "A. Korattikara", "M. Welling" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ahn et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ahn et al\\.",
      "year" : 2012
    }, {
      "title" : "Modeling high-dimensional discrete data with multi-layer neural networks",
      "author" : [ "Y. Bengio", "S. Bengio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Bengio and Bengio,? \\Q2000\\E",
      "shortCiteRegEx" : "Bengio and Bengio",
      "year" : 2000
    }, {
      "title" : "Deep generative stochastic networks trainable by backprop",
      "author" : [ "Y. Bengio", "E. Laufer", "G. Alain", "J. Yosinski" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2014
    }, {
      "title" : "Reweighted wake-sleep",
      "author" : [ "J. Bornschein", "Y. Bengio" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Bornschein and Bengio,? \\Q2015\\E",
      "shortCiteRegEx" : "Bornschein and Bengio",
      "year" : 2015
    }, {
      "title" : "Online Algorithms and Stochastic Approximations. Online Learning and Neural Networks, Edited by David Saad",
      "author" : [ "L. Bottou" ],
      "venue" : null,
      "citeRegEx" : "Bottou,? \\Q1998\\E",
      "shortCiteRegEx" : "Bottou",
      "year" : 1998
    }, {
      "title" : "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription",
      "author" : [ "N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Boulanger.Lewandowski et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Boulanger.Lewandowski et al\\.",
      "year" : 2012
    }, {
      "title" : "Stochastic gradient hamiltonian monte carlo",
      "author" : [ "T. Chen", "E. Fox", "C. Guestrin" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Enhanced gradient for training restricted boltzmann machines",
      "author" : [ "K. Cho", "T. Raiko", "A. Ilin" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Cho et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2013
    }, {
      "title" : "Bayesian sampling using stochastic gradient thermostats",
      "author" : [ "N. Ding", "Y. Fang", "R. Babbush", "C. Chen", "R. Skeel", "H. Neven" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ding et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2014
    }, {
      "title" : "Graphical models for machine learning and digital communication",
      "author" : [ "B.J. Frey" ],
      "venue" : null,
      "citeRegEx" : "Frey,? \\Q1998\\E",
      "shortCiteRegEx" : "Frey",
      "year" : 1998
    }, {
      "title" : "Scalable deep poisson factor analysis for topic modeling",
      "author" : [ "Z. Gan", "C. Chen", "R. Henao", "D. Carlson", "L. Carin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Gan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gan et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning deep sigmoid belief networks with data augmentation",
      "author" : [ "Z. Gan", "R. Henao", "D. Carlson", "L. Carin" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Gan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gan et al\\.",
      "year" : 2015
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "In AISTATS, pp",
      "citeRegEx" : "Glorot and Bengio,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot and Bengio",
      "year" : 2010
    }, {
      "title" : "Sparse autoregressive networks",
      "author" : [ "M. Goessling", "Y. Amit" ],
      "venue" : "arXiv preprint arXiv:1511.04776,",
      "citeRegEx" : "Goessling and Amit,? \\Q2015\\E",
      "shortCiteRegEx" : "Goessling and Amit",
      "year" : 2015
    }, {
      "title" : "Deep autoregressive networks",
      "author" : [ "K. Gregor", "I. Danihelka", "A. Mnih", "C. Blundell", "D. Wierstra" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2014
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "K. Gregor", "I. Danihelka", "A. Graves", "D. Rezende", "D. Wierstra" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural adaptive sequential monte carlo",
      "author" : [ "S. Gu", "Z. Ghahramani", "R.E. Turner" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Gu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2015
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y. Teh" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "M. Jordan", "Z. Ghahramani", "T. Jaakkola", "L. Saul" ],
      "venue" : "MLJ, 37(2):183–233,",
      "citeRegEx" : "Jordan et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jordan et al\\.",
      "year" : 1999
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J.L. Ba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma and Ba,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba",
      "year" : 2015
    }, {
      "title" : "Auto-encoding variational Bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma and Welling,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Welling",
      "year" : 2014
    }, {
      "title" : "Oneshot learning by inverting a compositional causal process",
      "author" : [ "B.M. Lake", "R. Salakhutdinov", "J. Tenenbaum" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Lake et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2013
    }, {
      "title" : "The neural autoregressive distribution estimator",
      "author" : [ "H. Larochelle", "I. Murray" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Larochelle and Murray,? \\Q2011\\E",
      "shortCiteRegEx" : "Larochelle and Murray",
      "year" : 2011
    }, {
      "title" : "Gradientbased learning applied to document recognition",
      "author" : [ "Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Lecun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Lecun et al\\.",
      "year" : 1998
    }, {
      "title" : "High-order stochastic gradient thermostats for bayesian learning of deep models",
      "author" : [ "C. Li", "C. Chen", "K. Fan", "L. Carin" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A practical bayesian framework for backpropagation networks",
      "author" : [ "D. MacKay" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "MacKay,? \\Q1992\\E",
      "shortCiteRegEx" : "MacKay",
      "year" : 1992
    }, {
      "title" : "Inductive principles for restricted boltzmann machine learning",
      "author" : [ "B. Marlin", "K. Swersky", "B. Chen", "N. Freitas" ],
      "venue" : "In AISTATS, pp",
      "citeRegEx" : "Marlin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Marlin et al\\.",
      "year" : 2010
    }, {
      "title" : "Neural variational inference and learning in belief networks",
      "author" : [ "A. Mnih", "K. Gregor" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Mnih and Gregor,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih and Gregor",
      "year" : 2014
    }, {
      "title" : "Evaluating probabilities under high-dimensional latent variable models",
      "author" : [ "I. Murray", "R. Salakhutdinov" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Murray and Salakhutdinov,? \\Q2009\\E",
      "shortCiteRegEx" : "Murray and Salakhutdinov",
      "year" : 2009
    }, {
      "title" : "Connectionist learning of belief networks",
      "author" : [ "R.M. Neal" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Neal,? \\Q1992\\E",
      "shortCiteRegEx" : "Neal",
      "year" : 1992
    }, {
      "title" : "Bayesian learning for neural networks",
      "author" : [ "R.M. Neal" ],
      "venue" : "PhD thesis, University of Toronto,",
      "citeRegEx" : "Neal,? \\Q1995\\E",
      "shortCiteRegEx" : "Neal",
      "year" : 1995
    }, {
      "title" : "Iterative neural autoregressive distribution estimator nade-k",
      "author" : [ "T. Raiko", "Y. Li", "K. Cho", "Y. Bengio" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Raiko et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Raiko et al\\.",
      "year" : 2014
    }, {
      "title" : "Black box variational inference",
      "author" : [ "R. Ranganath", "S. Gerrish", "D.M. Blei" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ranganath et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ranganath et al\\.",
      "year" : 2014
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "D.J. Rezende", "S. Mohamed", "D. Wierstra" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Robbins and Monro,? \\Q1951\\E",
      "shortCiteRegEx" : "Robbins and Monro",
      "year" : 1951
    }, {
      "title" : "On the quantitative analysis of deep belief networks",
      "author" : [ "R. Salakhutdinov", "I. Murray" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Salakhutdinov and Murray,? \\Q2008\\E",
      "shortCiteRegEx" : "Salakhutdinov and Murray",
      "year" : 2008
    }, {
      "title" : "Mean field theory for sigmoid belief networks",
      "author" : [ "L. Saul", "T. Jaakkola", "M. Jordan" ],
      "venue" : "Journal of AI Research,",
      "citeRegEx" : "Saul et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Saul et al\\.",
      "year" : 1996
    }, {
      "title" : "Doubly stochastic variational bayes for non-conjugate inference",
      "author" : [ "M.K. Titsias", "M. Lázaro-Gredilla" ],
      "venue" : "In ICML, pp. 1971–1979,",
      "citeRegEx" : "Titsias and Lázaro.Gredilla,? \\Q2014\\E",
      "shortCiteRegEx" : "Titsias and Lázaro.Gredilla",
      "year" : 2014
    }, {
      "title" : "A deep and tractable density estimator",
      "author" : [ "B. Uria", "I. Murray", "H. Larochelle" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Uria et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Uria et al\\.",
      "year" : 2014
    }, {
      "title" : "Bayesian learning via stochastic gradient Langevin dynamics",
      "author" : [ "M. Welling", "Y.W. Teh" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Welling and Teh,? \\Q2011\\E",
      "shortCiteRegEx" : "Welling and Teh",
      "year" : 2011
    }, {
      "title" : "The stepsize η′ is chosen from {1, 3, 5}×10−4 with best performance. The model parameters are initialized following the heuristic of Glorot",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "10−10.,? \\Q2010\\E",
      "shortCiteRegEx" : "10−10.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Learning deep models that consist of multi-layered representations has obtained state-of-the-art performance in many tasks (Bengio et al., 2014; Hinton et al., 2006), partly due to their ability on capturing high-level abstractions.",
      "startOffset" : 123,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : "Learning deep models that consist of multi-layered representations has obtained state-of-the-art performance in many tasks (Bengio et al., 2014; Hinton et al., 2006), partly due to their ability on capturing high-level abstractions.",
      "startOffset" : 123,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : "As an important family of deep models, deep generative models (DGMs) (Hinton et al., 2006; Salakhutdinov & Hinton, 2009) can answer a wide range of queries by performing probabilistic inference, such as inferring the missing values of input data, which is beyond the scope of recognition networks such as deep neural networks.",
      "startOffset" : 69,
      "endOffset" : 120
    }, {
      "referenceID" : 26,
      "context" : "However, probabilistic inference with DGMs is challenging, especially when a Bayesian formalism is adopted, which is desirable to protect the DGM from overfitting (MacKay, 1992; Neal, 1995) and to perform sparse Bayesian inference (Gan et al.",
      "startOffset" : 163,
      "endOffset" : 189
    }, {
      "referenceID" : 31,
      "context" : "However, probabilistic inference with DGMs is challenging, especially when a Bayesian formalism is adopted, which is desirable to protect the DGM from overfitting (MacKay, 1992; Neal, 1995) and to perform sparse Bayesian inference (Gan et al.",
      "startOffset" : 163,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : ", 2015b) or nonparametric inference (Adams et al., 2010) to learn the network structure.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 19,
      "context" : "To address the challenges, approximate methods have to be adopted, including variational (Jordan et al., 1999; Saul et al., 1996) and Markov chain Monte Carlo (MCMC) methods (Robert & Casella, 2005).",
      "startOffset" : 89,
      "endOffset" : 129
    }, {
      "referenceID" : 37,
      "context" : "To address the challenges, approximate methods have to be adopted, including variational (Jordan et al., 1999; Saul et al., 1996) and Markov chain Monte Carlo (MCMC) methods (Robert & Casella, 2005).",
      "startOffset" : 89,
      "endOffset" : 129
    }, {
      "referenceID" : 34,
      "context" : "Much progress has been made on stochastic variational methods for DGMs (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2014), under some mean-field or parameterization assumptions.",
      "startOffset" : 71,
      "endOffset" : 141
    }, {
      "referenceID" : 33,
      "context" : "Much progress has been made on stochastic variational methods for DGMs (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2014), under some mean-field or parameterization assumptions.",
      "startOffset" : 71,
      "endOffset" : 141
    }, {
      "referenceID" : 10,
      "context" : "Gan et al. (2015b) present a Gibbs sampler for deep sigmoid belief networks with a sparsity-inducing prior via data augmentation, Adams et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "(2015b) present a Gibbs sampler for deep sigmoid belief networks with a sparsity-inducing prior via data augmentation, Adams et al. (2010) present a Metropolis-Hastings method for cascading Indian buffet process and Li et al.",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "(2015b) present a Gibbs sampler for deep sigmoid belief networks with a sparsity-inducing prior via data augmentation, Adams et al. (2010) present a Metropolis-Hastings method for cascading Indian buffet process and Li et al. (2016) develop a high-order stochastic gradient MCMC method and apply to deep Poisson factor analysis (Gan et al.",
      "startOffset" : 119,
      "endOffset" : 233
    }, {
      "referenceID" : 1,
      "context" : "By drawing samples in the collapsed parameter space, our method extends the recent work on stochastic gradient MCMC (Welling & Teh, 2011; Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014) to deal with the challenging task of posterior inference with DGMs.",
      "startOffset" : 116,
      "endOffset" : 193
    }, {
      "referenceID" : 7,
      "context" : "By drawing samples in the collapsed parameter space, our method extends the recent work on stochastic gradient MCMC (Welling & Teh, 2011; Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014) to deal with the challenging task of posterior inference with DGMs.",
      "startOffset" : 116,
      "endOffset" : 193
    }, {
      "referenceID" : 9,
      "context" : "By drawing samples in the collapsed parameter space, our method extends the recent work on stochastic gradient MCMC (Welling & Teh, 2011; Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014) to deal with the challenging task of posterior inference with DGMs.",
      "startOffset" : 116,
      "endOffset" : 193
    }, {
      "referenceID" : 17,
      "context" : "Our work is closely related to the recent progress on neural adaptive proposals for sequential Monte Carlo (NASMC) (Gu et al., 2015).",
      "startOffset" : 115,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : "Finally, Gan et al. (2015a) adopt a Monte Carlo estimate via Gibbs sampling to the intractable gradients under a stochastic MCMC method particularly for topic models.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "Depending on the structure of z, various DGMs have been developed, such as deep belief networks (Hinton et al., 2006), deep sigmoid belief networks (Mnih & Gregor, 2014), and deep Boltzmann machines (Salakhutdinov & Hinton, 2009).",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 37,
      "context" : "Sigmoid Belief Network layer (SBN): A SBN layer (Saul et al., 1996) is a directed graphical model that defines the conditional probability of each independent binary variable z (l) i given the upper layer z (l+1) as follows: p(z (l) i = 1|z ) = σ(Wi,:z (l+1) + bi), (4) where σ(x) = 1/(1 + e−x) is the sigmoid function, Wi,: denotes the i-th row of the weight matrix and bi is the bias.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "Deep Autoregressive Network layer (DARN): A DARN (Gregor et al., 2014) layer assumes in-layer connections on the SBN layer.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "Boulanger-Lewandowski et al. (2012) and Bornschein & Bengio (2015) amend this model to a conditional NADE layer:",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "Boulanger-Lewandowski et al. (2012) and Bornschein & Bengio (2015) amend this model to a conditional NADE layer:",
      "startOffset" : 0,
      "endOffset" : 67
    }, {
      "referenceID" : 34,
      "context" : "To address this challenge, recent progress (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014) has adopted hybrid Monte Carlo and variational methods, which approximate the intractable expectations and their gradients over the parameters (θ,φ) via some unbiased Monte Carlo estimates.",
      "startOffset" : 43,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "Furthermore, to handle large-scale datasets, stochastic optimization (Robbins & Monro, 1951; Bottou, 1998) of the variational objective can be used with a suitable learning rate annealing scheme.",
      "startOffset" : 69,
      "endOffset" : 106
    }, {
      "referenceID" : 34,
      "context" : "Though variational methods can be developed as in (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014; Bornschein & Bengio, 2015), under some mean-field or parameterization assumptions, they often require non-trivial model-specific deviations and may lead to inaccurate approximation when the assumptions are not properly made.",
      "startOffset" : 50,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "A straightforward application of MCMC methods can be Gibbs sampling or stochastic gradient MCMC (Welling & Teh, 2011; Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014).",
      "startOffset" : 96,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "A straightforward application of MCMC methods can be Gibbs sampling or stochastic gradient MCMC (Welling & Teh, 2011; Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014).",
      "startOffset" : 96,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "A straightforward application of MCMC methods can be Gibbs sampling or stochastic gradient MCMC (Welling & Teh, 2011; Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014).",
      "startOffset" : 96,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "We consider the stochastic gradient Nosé-Hoover thermostat (SGNHT) (Ding et al., 2014).",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : ", stochastic gradient Langevin dynamics (Welling & Teh, 2011), stochastic gradient Hamiltonian Monte Carlo (Chen et al., 2014) and high-order stochastic gradient thermostats (Li et al.",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 25,
      "context" : ", 2014) and high-order stochastic gradient thermostats (Li et al., 2016).",
      "startOffset" : 55,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "We follow Gan et al. (2015a) to use the multivariate version of SGNHT that generate samples by simulating the dynamics as follows: θt+1 = θt + λpt, (12) pt+1 = pt − λξt pt − λ∇θŨ(θt+1) + √ 2AN (0, λI), ξt+1 = ξt + λ(pt+1 pt+1 − I),",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "Here, we draw inspirations from variational methods and learn adaptive proposals (Gu et al., 2015) by minimizing some criterion.",
      "startOffset" : 81,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "We optimize the quality of the proposal distribution by minimizing the inclusive KL-divergence between the target posterior distribution and the proposal Ep(z|x,θ)[log p(z|x,θ) q(z|x;φ) ] (Bornschein & Bengio, 2015; Gu et al., 2015) or equivalently maximizing the expected loglikelihood of the recognition model",
      "startOffset" : 188,
      "endOffset" : 232
    }, {
      "referenceID" : 34,
      "context" : "In contrast, the exclusive KL-divergence L(φ;θ,x) := Eq(z|x;φ)[log q(z|x;φ) p(z|x,θ) ], as widely adopted in the variational methods (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014), does not have such a property — It can happen that q(z|x;φ) = 0 when p(z|x,θ) > 0; therefore unsuitable for importance sampling.",
      "startOffset" : 133,
      "endOffset" : 200
    }, {
      "referenceID" : 30,
      "context" : "2, various DGMs with discrete hidden variables, such as sigmoid belief networks (Neal, 1992), are trained on the binarized MNIST (Salakhutdinov & Murray, 2008) and the Caltech 101 Silhouettes (Marlin et al.",
      "startOffset" : 80,
      "endOffset" : 92
    }, {
      "referenceID" : 27,
      "context" : "2, various DGMs with discrete hidden variables, such as sigmoid belief networks (Neal, 1992), are trained on the binarized MNIST (Salakhutdinov & Murray, 2008) and the Caltech 101 Silhouettes (Marlin et al., 2010) datasets.",
      "startOffset" : 192,
      "endOffset" : 213
    }, {
      "referenceID" : 22,
      "context" : "3, we train variational auto-encoders (Kingma & Welling, 2014) on the binarized MNIST and the Omniglot (Lake et al., 2013) datasets.",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "The results of NVIL are from Mnih & Gregor (2014); the results of Wake-sleep and RWS are from Bornschein & Bengio (2015); and the results of Data Augmentation (DA) are from Gan et al. (2015b). Model Dim NVIL Wake-Sleep RWS DA DSGNHT-Gibbs DSGNHT-NAIS SBN 200 (−113.",
      "startOffset" : 173,
      "endOffset" : 192
    }, {
      "referenceID" : 36,
      "context" : "Results are taken from [1] Bornschein & Bengio (2015), [2] Larochelle & Murray (2011), [3] Uria et al. (2014), [4] Gregor et al.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "(2014), [4] Gregor et al. (2014), [5] Salakhutdinov & Murray (2008), [6] Raiko et al.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "(2014), [4] Gregor et al. (2014), [5] Salakhutdinov & Murray (2008), [6] Raiko et al.",
      "startOffset" : 12,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : "(2014), [4] Gregor et al. (2014), [5] Salakhutdinov & Murray (2008), [6] Raiko et al. (2014), [7] Murray & Salakhutdinov (2009), [8] Burda et al.",
      "startOffset" : 12,
      "endOffset" : 93
    }, {
      "referenceID" : 15,
      "context" : "(2014), [4] Gregor et al. (2014), [5] Salakhutdinov & Murray (2008), [6] Raiko et al. (2014), [7] Murray & Salakhutdinov (2009), [8] Burda et al.",
      "startOffset" : 12,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "(2014), [4] Gregor et al. (2014), [5] Salakhutdinov & Murray (2008), [6] Raiko et al. (2014), [7] Murray & Salakhutdinov (2009), [8] Burda et al. (2015). Trained on the original MNIST dataset.",
      "startOffset" : 12,
      "endOffset" : 153
    }, {
      "referenceID" : 24,
      "context" : "90, which is trained on the original MNIST dataset (Lecun et al., 1998) and thus not directly comparable.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "Gregor et al. (2015) give a lower bound −80.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "Results are taken from [1] Bornschein & Bengio (2015), [2] Cho et al. (2013), [3] Raiko et al.",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : "Results are taken from [1] Bornschein & Bengio (2015), [2] Cho et al. (2013), [3] Raiko et al. (2014). † Results are produced using the authors’ published code.",
      "startOffset" : 59,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "40 by training FVSBN (Frey, 1998) with both training data and validation data.",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "Gan et al. (2015b) achieve −96.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "40 by training FVSBN (Frey, 1998) with both training data and validation data. A latest work by Goessling & Amit (2015) achieves −88.",
      "startOffset" : 22,
      "endOffset" : 120
    } ],
    "year" : 2016,
    "abstractText" : "We present doubly stochastic gradient MCMC, a simple and generic method for (approximate) Bayesian inference of deep generative models (DGMs) in a collapsed continuous parameter space. At each MCMC sampling step, the algorithm randomly draws a mini-batch of data samples to estimate the gradient of log-posterior and further estimates the intractable expectation over hidden variables via a neural adaptive importance sampler, where the proposal distribution is parameterized by a deep neural network and learnt jointly. We demonstrate the effectiveness on learning various DGMs in a wide range of tasks, including density estimation, data generation and missing data imputation. Our method outperforms many state-of-the-art competitors.",
    "creator" : "LaTeX with hyperref package"
  }
}