{
  "name" : "1706.01604.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hyperplane Clustering Via Dual Principal Component Pursuit",
    "authors" : [ "Manolis C. Tsakiris", "René Vidal" ],
    "emails" : [ "M.TSAKIRIS@JHU.EDU", "RVIDAL@JHU.EDU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "and low-rank representation theory. Existing results guaranteeing the correctness of such methods require the dimension of the subspaces to be small relative to the dimension of the ambient space. When this assumption is violated, as is, for example, in the case of hyperplanes, existing methods are either computationally too intense (e.g., algebraic methods) or lack theoretical support (e.g., K-hyperplanes or RANSAC). The main theoretical contribution of this paper is to extend the theoretical analysis of a recently proposed single subspace learning algorithm, called Dual Principal Component Pursuit (DPCP), to the case where the data are drawn from of a union of hyperplanes. To gain insight into the expected properties of the non-convex `1 problem associated with DPCP (discrete problem), we develop a geometric analysis of a closely related continuous optimization problem. Then transferring this analysis to the discrete problem, our results state that as long as the hyperplanes are sufficiently separated, the dominant hyperplane is sufficiently dominant and the points are uniformly distributed (in a deterministic sense) inside their associated hyperplanes, then the non-convex DPCP problem has a unique (up to sign) global solution, equal to the normal vector of the dominant hyperplane. This suggests a sequential hyperplane learning algorithm, which first learns the dominant hyperplane by applying DPCP to the data. In order to avoid hard thresholding of the points which is sensitive to the choice of the thresholding parameter, all points are weighted according to their distance to that hyperplane, and a second hyperplane is computed by applying DPCP to the weighted data, and so on. Experiments on corrupted synthetic data show that this DPCP-based sequential algorithm dramatically improves over similar sequential algorithms, which learn the dominant hyperplane via state-of-the-art single subspace learning methods (e.g., with RANSAC or REAPER). Finally, 3D plane clustering experiments on real 3D point clouds show that a K-Hyperplanes DPCP-based scheme, which computes the normal vector of each cluster via DPCP, instead of the classic SVD, is very competitive to state-of-the-art approaches (e.g., RANSAC or SVD-based K-Hyperplanes)."
    }, {
      "heading" : "1. Introduction",
      "text" : "Subspace Clustering. Over the past fifteen years the model of a union of linear subspaces, also called a subspace arrangement (Derksen, 2007), has gained significant popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classical model of a single linear subspace, associated to the well-known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002). This has led to a variety of algorithms that attempt to cluster a collection of data drawn from a subspace arrangement, giving rise to the challenging field of subspace clustering (Vidal, 2011). Such techniques can be iterative (Bradley and Mangasarian, 2000; Tseng, 2000;\nar X\niv :1\n70 6.\n01 60\n4v 1\n[ cs\n.C V\n] 6\nJ un\n2 01\nZhang et al., 2009), statistical (Tipping and Bishop, 1999b; Gruber and Weiss, 2004), informationtheoretic (Ma et al., 2007), algebraic (Vidal et al., 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al., 2010; Favaro et al., 2011; Liu et al., 2013; Vidal and Favaro, 2014) representation theory.\nHyperplane Clustering. A special class of subspace clustering is that of hyperplane clustering, which arises when the data are drawn from a union of hyperplanes, i.e., a hyperplane arrangement. Prominent applications include projective motion segmentation (Vidal et al., 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005). Even though in some ways hyperplane clustering is simpler than general subspace clustering, since, e.g., the dimensions of the subspaces are equal and known a-priori, modern self-expressiveness-based subspace clustering methods, such as Liu et al. (2013); Lu et al. (2012); Elhamifar and Vidal (2013), in principle do not apply in this case, because they require small relative subspace dimensions. 1\nFrom a theoretical point of view, one of the most appropriate methods for hyperplane clustering is Algebraic Subspace Clustering (ASC) (Vidal et al., 2003, 2005, 2008; Tsakiris and Vidal, 2014, 2015b, 2017c,a), which gives closed-form solutions by means of factorization (Vidal et al., 2003) or differentiation (Vidal et al., 2005) of polynomials. However, the main drawback of ASC is its exponential complexity2 in the number n of hyperplanes and the ambient dimension D, which makes it impractical in many settings. Another method that is theoretically justifiable for clustering hyperplanes is Spectral Curvature Clustering (SCC) (Chen and Lerman, 2009), which is based on computing a D-fold affinity between all D-tuples of points in the dataset. As in the case of ASC, SCC is characterized by combinatorial complexity and becomes cumbersome for large D; even though it is possible to reduce its complexity, this comes at the cost of significant performance degradation. On the other hand, the intuitive classical method of K-hyperplanes (KH) (Bradley and Mangasarian, 2000), which alternates between assigning clusters and fitting a new normal vector to each cluster with PCA, is perhaps the most practical method for hyperplane clustering, since it is simple to implement, it is robust to noise and its complexity depends on the maximal allowed number of iterations. However, KH is sensitive to outliers and is guaranteed to converge only to a local minimum; hence multiple restarts are in general required. Median K-Flats (MKF) (Zhang et al., 2009) shares a similar objective function as KH, but uses the `1-norm instead of the `2- norm, in an attempt to gain robustness to outliers. MKF minimizes its objective function via a stochastic gradient descent scheme, and searches directly for a basis of each subspace, which makes it slower to converge for hyperplanes. Finally, we note that any single subspace learning method, such as RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al., 2015), can be applied in a sequential fashion to learn a union of hyperplanes, by first learning the first dominant hyperplane, then removing the points lying close to it, then learning a second dominant hyperplane, and so on.\nDPCP: A single subspace method for high relative dimensions. Recently, an `1 method was introduced in the context of single subspace learning with outliers, called Dual Principal Component Pursuit (DPCP) (Tsakiris and Vidal, 2015a, 2017b), which aims at recovering the orthogonal complement of a subspace in the presence of outliers. Since the orthogonal complement of a hyperplane is one-dimensional, DPCP is particularly suited for hyperplanes. DPCP searches for the\n1. The relative dimension of a linear subspace is the ratio d/D, where d is the dimension of the subspace and D is the ambient dimension. 2. The issue of robustness to noise for ASC has been recently addressed in Tsakiris and Vidal (2015b, 2017c).\nnormal to a hyperplane by solving a non-convex `1 minimization problem on the sphere, or alternatively a recursion of linear programming relaxations. Assuming the dataset is normalized to unit `2-norm and consists of points uniformly distributed on the great circle defined by a hyperplane (inliers), together with arbitrary points uniformly distributed on the sphere (outliers), Tsakiris and Vidal (2015a, 2017b) gave conditions under which the normal to the hyperplane is the unique global solution to the non-convex `1 problem, as well as the limit point of a recursion of linear programming relaxations, the latter being reached after a finite number of iterations.\nContributions. Motivated by the robustness of DPCP to outliers–DPCP was shown to be the only method capable of recovering the normal to the hyperplane in the presence of about 70% outliers inside a 30-dimensional ambient space 3 –one could naively use it for hyperplane clustering by recovering the normal to a hyperplane one at a time, while treating points from other hyperplanes as outliers. However, such a scheme is not a-priori guaranteed to succeed, since the outliers are now clearly structured, contrary to the theorems of correctness of Tsakiris and Vidal (2015a, 2017b) that assume that the outliers are uniformly distributed on the sphere. It is precisely this theoretical gap that we bridge in this paper: we show that as long as the hyperplanes are sufficiently separated, the dominant hyperplane is sufficiently dominant and the points are uniformly distributed (in a deterministic sense) inside their associated hyperplanes, then the non-convex DPCP problem has a unique (up to sign) global solution, equal to the normal vector of the dominant hyperplane. This suggests a sequential hyperplane learning algorithm, which first learns the dominant hyperplane, and weights all points according to their distance to that hyperplane. Then DPCP applied on the weighted data yields the second dominant hyperplane, and so on. Experiments on corrupted synthetic data show that this DPCP-based sequential algorithm dramatically improves over similar sequential algorithms, which learn the dominant hyperplane via state-of-the-art single subspace learning methods (e.g., with RANSAC). Finally, 3D plane clustering experiments on real 3D point clouds show that a K-Hyperplanes DPCP-based scheme, which computes the normal vector of each cluster via DPCP, instead of the classic SVD, is very competitive to state-of-the-art approaches (e.g., RANSAC or SVD-based K-Hyperplanes).\nNotation. For a positive integer n, [n] := {1, . . . , n}. For a vector w ∈ RD we let ŵ = w/ ‖w‖2 if w 6= 0, and ŵ = 0 otherwise. SD−1 is the unit sphere of RD. For two vectors w1,w2 ∈ SD−1, the principal angle between w1 and w2 is the unique angle θ ∈ [0, π/2] with cos(θ) = |w>1 w2|. 1 denotes the vector of all ones, LHS stands for left-hand-side and RHS stands for right-hand-side. Finally, for a set X we denote by Card(X ) the cardinality of X .\nPaper Organization. The rest of the paper is organized as follows. In §2 we review the priorart in generic hyperplane clustering. In §3 we discuss the theoretical contributions of this paper; proofs are given in §4. §5 describes the algorithmic contributions of this paper, and §6 contains experimental evaluations of the proposed methods."
    }, {
      "heading" : "2. Prior Art",
      "text" : "Suppose given a dataset X = [x1, . . . ,xN ] of N points of RD, such that Ni points of X , say X i ∈ RD×Ni , lie close to a hyperplane Hi = { x : b>i x = 0 } , where bi is the normal vector to\n3. We note that the problem of robust PCA or subspace clustering for subspaces of large relative dimension becomes very challenging as the ambient dimension increases; see Section 6.1.\nthe hyperplane. Then the goal of hyperplane clustering is to identify the underlying hyperplane arrangement ⋃n i=1Hi and cluster the dataset X to the subsets (clusters) X 1, . . . ,X n.4\nRANSAC. A traditional way of clustering points lying close to a hyperplane arrangement is by means of the RANdom SAmpling Consensus algorithm (RANSAC) (Fischler and Bolles, 1981), which attempts to identify a single hyperplaneHi at a time. More specifically, RANSAC alternates between randomly selecting D − 1 points from X and counting the number of points in the dataset that are within distance δ from the hyperplane generated by the selectedD−1 points. After a certain number of trials is reached, a first hyperplane Ĥ1 is selected as the one that admits the largest number of points in the dataset within distance δ. These points are then removed and a second hyperplane Ĥ2 is obtained from the reduced dataset in a similar fashion, and so on. Naturally, RANSAC is sensitive to the thresholding parameter δ. In addition, its efficiency depends on how big the probability is, that D − 1 randomly selected points lie close to the same underlying hyperplane Hi, for some i ∈ [n]. This probability depends on how large D is as well as how balanced or unbalanced the clusters are. If D is small, then RANSAC is likely to succeed with few trials. The same is true if one of the clusters, say X 1, is highly dominant, i.e., N1 >> Ni, ∀i ≥ 2, since in such a case, identifying H1 is likely to be achieved with only a few trials. On the other hand, if D is large and the Ni are of the same order of magnitude, then exponentially many trials are required (see §6 for some numerical results), and RANSAC becomes inefficient.\nK-Hyperplanes (KH). Another very popular method for hyperplane clustering is the so-called K-hyperplanes (KH), which was proposed by Bradley and Mangasarian (2000). KH attempts to minimize the non-convex objective function\nJKH(f1, . . . ,fn; s1, . . . , sN ) := n∑ i=1  N∑ j=1 sj(i) ( f>i xj )2 , (1) where sj : [n] → {0, 1} is the hyperplane assignment of point xj , i.e., sj(i) = 1 if and only if point xj has been assigned to hyperplane i, and f i ∈ SD−1 is the normal vector to the estimated i-th hyperplane. Because of the non-convexity of (1), the typical way to perform the optimization is by alternating between assigning clusters, i.e., given the f i assigning xj to its closest hyperplane (in the euclidean sense), and fitting hyperplanes, i.e., given the segmentation {sj}, computing the best `2 hyperplane for each cluster by means of PCA on each cluster. Because of this iterative refinement of hyperplanes and clusters, this method is sometimes also called Iterative Hyperplane Learning (IHL). The theoretical guarantees of KH are limited to convergence to a local minimum in a finite number of steps. Even though the alternating minimization in KH is computationally efficient, in practice several restarts are typically used, in order to select the best among multiple local minima. In fact, the higher the ambient dimension D is the more restarts are required, which significantly increases the computational burden of KH. Moreover, KH is robust to noise but not to outliers, since the update of the normal vectors is done by means of standard (`2-based) PCA.\nMedian K Flats (MKF). It is precisely the sensitivity to outliers of KH that Median K Flats (MKF) or Median K Hyperplanes (Zhang et al., 2009) attempts to address, by minimizing the non-\n4. We are assuming here that there is a unique hyperplane arrangement associated to the data X , for more details see §3.1.\nconvex and non-smooth objective function\nJMKF(f1, . . . ,fn; s1, . . . , sN ) := n∑ i=1  N∑ j=1 sj(i) ∣∣∣f>i xj∣∣∣  . (2) Notice that (2) is almost identical to the objective (1) of KH, except that the distances of the points to their assigned hyperplanes now appear without the square. This makes the optimization problem harder, and Zhang et al. (2009) propose to solve it by means of a stochastic gradient approach, which requires multiple restarts, as KH does. Even though conceptually MKF is expected to be more robust to outliers than KH, we are not aware of any theoretical guarantees surrounding MKF that corroborate this intuition. Moreover, MKF is considerably slower than KH, since MKF searches directly for a basis of the hyperplanes, rather than the normals to the hyperplanes. We note here that MKF was not designed specifically for hyperplanes, rather for the more general case of unions of equi-dimensional subspaces. In addition, it is not trivial to adjust MKF to search for the orthogonal complement of the subspaces, which would be the efficient approach for hyperplanes.\nAlgebraic Subspace Clustering (ASC). ASC was originally proposed in Vidal et al. (2003) precisely for the purpose of provably clustering hyperplanes, a problem which at the time was handled either by the intuitive RANSAC or K-Hyperplanes. The idea behind ASC is to fit a polynomial p(x1, . . . , xD) ∈ R[x1, . . . , xD] of degree n to the data, where n is the number of hyperplanes, and x1, . . . , xD are polynomial indeterminates. In the absence of noise, this polynomial can be shown to have up to scale the form\np(x) = (b>1 x) · · · (b>n x), x := [ x1 · · ·xD ]> , (3)\nwhere bi ∈ SD−1 is the normal vector to hyperplane Hi. This reduces the problem to that of factorizing p(x) to the product of linear factors, which was elegantly done in Vidal et al. (2003). When the data are contaminated by noise, the fitted polynomial need no longer be factorizable; this apparent difficulty was circumvented in Vidal et al. (2005), where it was shown that the gradient of the polynomial evaluated at point xj is a good estimate for the normal vector of the hyperplane Hi that xj lies closest to. Using this insight, one may obtain the hyperplane clusters by applying standard spectral clustering (von Luxburg, 2007) on the angle-based affinity matrix\nAjj′ = ∣∣∣∣∣〈 ∇p|xj||∇p|xj ||2 , ∇p|xj′||∇p|xj′ ||2 〉 ∣∣∣∣∣ , j, j′ ∈ [N ]. (4)\nThe main bottleneck of ASC is computational: at least ( n+D−1\nn\n) − 1 points are required in order to\nfit the polynomial, which yields prohibitive complexity in many settings when n or D are large. A second issue with ASC is that it is sensitive to outliers; this is because the polynomial is fitted in an `2 sense through SVD (notice the similarity with KH).\nSpectral Curvature Clustering (SCC). Another yet conceptually distinct method from the ones discussed so far is SCC, whose main idea is to build a D-fold tensor as follows. For each D-tuple of distinct points in the dataset, say xj1 , . . . ,xjD , the value of the tensor is set to\nA(j1, . . . , jD) = exp ( −(cp(xj1 , . . . ,xjD)) 2\n2σ2\n) , (5)\nwhere cp(xj1 , . . . ,xjD) is the polar curvature of the points xj1 , . . . ,xjD (see Chen and Lerman (2009) for an explicit formula) and σ is a tuning parameter. Intuitively, the polar curvature is a multiple of the volume of the simplex of the D points, which becomes zero if the points lie in the same hyperplane, and the further the points lie from any hyperplane the larger the volume becomes. SCC obtains the hyperplane clusters by unfolding the tensor A to an affinity matrix, upon which spectral clustering is applied. As with ASC, the main bottleneck of SCC is computational, since in principle all ( N D ) entries of the tensor need to be computed. Even though the combinatorial complexity of SCC can be reduced, this comes at the cost of significant performance degradation. RANSAC/KH Hybrids. Generally speaking, any single subspace learning method that is robust to outliers and can handle subspaces of high relative dimensions, can be used to perform hyperplane clustering, either via a RANSAC-style or a KH-style scheme or a combination of both. For example, ifM is a method that takes a dataset and fits to it a hyperplane, then one can useM to compute the first dominant hyperplane, remove the points in the dataset lying close to it, compute a second dominant hyperplane and so on (RANSAC-style). Alternatively, one can start with a random guess for n hyperplanes, cluster the data according to their distance to these hyperplanes, and then use M (instead of the classic SVD) to fit a new hyperplane to each cluster, and so on (KH-style). Even though a large variety of single subspace learning methods exist, e.g., see references in Lerman and Zhang (2014), only few such methods are potentially able to handle large relative dimensions and in particular hyperplanes. In addition to RANSAC, in this paper we will consider two other possibilities, i.e., REAPER and DPCP, which are described next.5\nREAPER. A recently proposed single subspace learning method that admits an interesting theoretical analysis is the so-called REAPER (Lerman et al., 2015). REAPER is inspired by the nonconvex optimization problem\nmin L∑ j=1 ‖(ID −Π)xj‖2 , s.t. Π is an orthogonal projection, Trace (Π) = d, (6)\nwhose principle is to minimize the sum of the euclidean distances of all points to a single ddimensional linear subspace U ; the matrix Π appearing in (6) can be thought of as the product Π = UU>, where U ∈ RD×d contains in its columns an orthonormal basis for U . As (6) is non-convex, Lerman et al. (2015) relax it to the convex semi-definite program\nmin L∑ j=1 ‖(ID − P )xj‖2 , s.t. 0 ≤ P ≤ ID, Trace (P ) = d, (7)\nwhich is the optimization problem that is actually solved by REAPER; the orthogonal projection matrix Π∗ associated to U , is obtained by projecting the solution P ∗ of (7) onto the space of rank d orthogonal projectors. A limitation of REAPER is that the semi-definite program (7) may become prohibitively large even for moderate values ofD. This difficulty can be circumvented by solving (7) in an Iteratively Reweighted Least Squares (IRLS) fashion, for which convergence of the objective value to a neighborhood of the optimal value has been established in Lerman et al. (2015).\nDual Principal Component Pursuit (DPCP). Similarly to RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al., 2015), DPCP (Tsakiris and Vidal, 2015a, 2017b) is another, recently proposed, single subspace learning method that can be applied to hyperplane clustering.\n5. Regression-type methods such as the one proposed in Wang et al. (2015) are also a possibility.\nThe key idea of DPCP is to identify a single hyperplane H that is maximal with respect to the data X . Such a maximal hyperplane is defined by the property that it must contain a maximal number of points NH from the dataset, i.e., NH′ ≤ NH for any other hyperplaneH′ of RD. Notice that such a maximal hyperplane can be characterized as a solution to the problem\nmin b ∥∥∥X>b∥∥∥ 0 s.t. b 6= 0, (8)\nsince ∥∥X>b∥∥\n0 counts precisely the number of points in X that are orthogonal to b, and hence,\ncontained in the hyperplane with normal vector b. Problem (8) is naturally relaxed to\nmin b ∥∥∥X>b∥∥∥ 1 s.t. ‖b‖2 = 1, (9)\nwhich is a non-convex non-smooth optimization problem on the sphere. In the case where there is no noise and the dataset consists of N1 ≥ D inlier points X 1 drawn from a single hyperplane H1 ∩ SD−1 with normal vector b1 ∈ SD−1, together with M outlier points O ⊂ SD−1 in general position, i.e. X = [X 1 O] Γ, where Γ is an unknown permutation matrix, then there is a unique maximal hyperplane that coincides with H1. Under certain uniformity assumptions on the data points and abundance of inliers X 1, Tsakiris and Vidal (2015a, 2017b) asserted that b1 is the unique6 up to sign global solution of (9), i.e., the combinatorial problem (8) and its non-convex relaxation (9) share the same unique global minimizer. Moreover, it was shown that under the additional assumption that the principal angle of the initial estimate n̂0 from b1 is not large, the sequence {n̂k} generated by the recursion of linear programs 7\nnk+1 := argmin b: b>n̂k=1\n∥∥∥X>b∥∥∥ 1 , (11)\nconverges up to sign to b1, after finitely many iterations. Alternatively, one can attempt to solve problem (9) by means of an IRLS scheme, in a similar fashion as for REAPER. Even though no theory has been developed for this approach, experimental evidence in Tsakiris and Vidal (2017b) indicates convergence of such an IRLS scheme to the global minimizer of (9).\nOther Methods. In general, there is a large variety of clustering methods that can be adapted to perform hyperplane clustering, and the above list is by no means exhaustive; rather contains the methods that are intelluctually closest to the proposal of this paper. Important examples that we do not compare with in this paper are the statistical-theoretic Mixtures of Probabilistic Principal Component Analyzers (Tipping and Bishop, 1999a), as well as the information-theoretic Agglomerative Lossy Compression (Ma et al., 2007). For an extensive account of these and other methods the reader is referred to Vidal et al. (2016).\n6. The theorems in Tsakiris and Vidal (2015a, 2017b) are given for inliers lying in a proper subspace of arbitrary dimension d; the uniqueness follows from specializing d = D − 1. 7. Notice that problem\nmin b ∥∥∥X>b∥∥∥ 1 , s.t. b>n̂k = 1 (10)\nmay admit more than one global minimizer. Here and in the rest of the paper we denote by nk+1 the solution obtained via the simplex method."
    }, {
      "heading" : "3. Theoretical Contributions",
      "text" : "In this section we develop the main theoretical contributions of this paper, which are concerned with the properties of the non-convex `1 minimization problem (9), as well as with the recursion of linear programs (11) in the context of hyperplane clustering. More specifically, we are particularly interested in developing conditions under which every global minimizer of the non-convex problem (9) is the normal vector to one of the hyperplanes of the underlying hyperplane arrangement. Towards that end, it is insightful to study an associated continuous problem, which is obtained by replacing each finite cluster within a hyperplane by the uniform measure on the unit sphere of the hyperplane (§3.2). The main result in that direction is Theorem 4. Next, by introducing certain uniformity parameters which measure the deviation of discrete quantities from their continuous counterparts, we adapt our analysis to the discrete case of interest (§3.1). This furnishes Theorem 6, which is the discrete analogue of Theorem 4, and gives global optimality conditions for the `1 non-convex DPCP problem (9) (§3.3). Finally, Theorem 7 gives convergence guarantees for the linear programming recursion (11). The proofs of all results are deferred to §4."
    }, {
      "heading" : "3.1 Data model and the problem of hyperplane clustering",
      "text" : "Consider given a collection X = [x1, . . . ,xN ] ∈ RD×N ofN points of the unit sphere SD−1 of RD, that lie in an arrangement A of n hyperplanes H1, . . . ,Hn of RD, i.e., X ⊂ A = ⋃n i=1Hi, where each hyperplaneHi is the set of points of RD that are orthogonal to a normal vector bi ∈ SD−1, i.e., Hi = { x ∈ RD : x>bi = 0 } , i ∈ [n] := {1, . . . , n}. We assume that the data X lie in general position inA, by which we mean two things. First, we mean that there are no linear relations among the points other than the ones induced by their membership to the hyperplanes. In particular, every (D − 1) points coming from Hi form a basis for Hi and any D points of X that come from at least two distinct Hi,Hi′ are linearly independent. Second, we mean that the points X uniquely define the hyperplane arrangement A, in the sense that A is the only arrangement of n hyperplanes that contains X . This can be verified computationally by checking that there is only one up to scale homogeneous polynomial of degree n that fits the data, see Vidal et al. (2005); Tsakiris and Vidal (2017c) for details. We assume that for every i ∈ [n], precisely Ni points of X , denoted by X i = [x(i)1 , . . . ,x (i) Ni ], belong toHi, with ∑n\ni=1Ni = N . With that notation, X = [X 1, . . . ,X n]Γ, where Γ is an unknown permutation matrix, indicating that the hyperplane membership of the points is unknown. Moreover, we assume an ordering N1 ≥ N2 ≥ · · · ≥ Nn, and we refer to H1 as the dominant hyperplane. After these preparations, the problem of hyperplane clustering can be stated as follows: given the data X , find the number n of hyperplanes associated to X , a normal vector to each hyperplane, and a clustering of the data X = ⋃n i=1X i according to hyperplane membership."
    }, {
      "heading" : "3.2 Theoretical analysis of the continuous problem",
      "text" : "As it turns out, certain important insights regarding problem (9) with respect to hyperplane clustering can be gained by examining an associated continuous problem. To see what that problem is, let Ĥi = Hi ∩ SD−1, and note first that for any b ∈ SD−1 we have\n1\nNi Ni∑ j=1 ∣∣∣b>x(i)j ∣∣∣ ' ∫ x∈Ĥi ∣∣∣b>x∣∣∣ dµĤi , (12)\nwhere the LHS of (12) is precisely 1Ni ∥∥X>i b∥∥1 and can be viewed as an approximation (') via the point set X i of the integral on the RHS of (12), with µĤi denoting the uniform measure on Ĥi. Letting θi be the principal angle between b and bi, i.e., the unique angle θi ∈ [0, π/2] such that cos(θi) = |b>bi|, and πHi : RD → Hi the orthogonal projection ontoHi, for any x ∈ Hi we have\nb>x = b>πHi(x) = (πHi(b)) > x = h>i,bx = sin(θi) ĥ > i,bx. (13)\nHence, ∫ x∈Ĥi ∣∣∣b>x∣∣∣ dµĤi = [∫ x∈Ĥi ∣∣∣ĥ>i,bx∣∣∣ dµĤi ] sin(θi) (14)\n= [∫ x∈SD−2 |x1|dµSD−2 ] sin(θi) (15) = c sin(θi). (16)\nIn the second equality above we made use of the rotational invariance of the sphere, as well as the fact that Ĥi ∼= SD−2, which leads to (for details see the proof of Proposition 4 and Lemma 7 in Tsakiris and Vidal (2017b))∫\nx∈Ĥi ∣∣∣ĥ>i,bx∣∣∣ dµĤi = ∫ x∈SD−2 |x1|dµSD−2 =: c, (17)\nwhere x1 is the first coordinate of x and c is the average height of the unit hemisphere in RD. As a consequence, we can view the objective function of (9), which is given by\n∥∥∥X>b∥∥∥ 1 = n∑ i=1 ∥∥∥X>i b∥∥∥ 1 = n∑ i=1 Ni  1 Ni Ni∑ j=1 ∣∣∣b>x(i)j ∣∣∣  , (18)\nas a discretization via the point set X of the function\nJ (b) := n∑ i=1 Ni (∫ x∈Ĥi ∣∣∣b>x∣∣∣ dµĤi ) (16) = n∑ i=1 Ni c sin(θi). (19)\nIn that sense, the continuous counterpart of problem (9) is\nmin b J (b) = N1 c sin(θ1) + · · ·+Nn c sin(θn), s.t. b ∈ SD−1. (20)\nNote that sin(θi) is the distance between the line spanned by b and the line spanned by bi.8 Thus, every global minimizer b∗ of problem (20) minimizes the sum of the weighted distances of Span(b∗) from Span(b1), . . . ,Span(bn), and can be thought of as representing a weighted median of these lines. Medians in Riemmannian manifolds, and in particular in the Grassmannian manifold, are an active subject of research (Draper et al., 2014; Ghalieh and Hajja, 1996). However, we are not aware of any work in the literature that defines a median by means of (20), nor any work that studies (20).\nThe advantage of working with (20) instead of (9), is that the solution set of the continuous problem (20) depends solely on the weights N1 ≥ N2 ≥ · · · ≥ Nn assigned to the hyperplane\n8. Recall that θi is a principal angle, i.e., θi ∈ [0, π/2].\narrangement, as well as on the geometry of the arrangement, captured by the principal angles θij between bi and bj . In contrast, the solutions of the discrete problem (9) may also depend on the distribution of the points X . From that perspective, understanding when problem (20) has a unique solution that coincides with the normal ±b1 to the dominant hyperplane H1 is essential for understanding the potential of (9) for hyperplane clustering. Towards that end, we next provide a series of results pertaining to (20). The first configuration that we examine is that of two hyperplanes. In that case the weighted geometric median of the two lines spanned by the normals to the hyperplanes always corresponds to one of the two normals:\nTheorem 1 Let b1, b2 be an arrangement of two hyperplanes in RD, with weights N1 ≥ N2. Then the set B∗ of global minimizers of (20) satisfies:\n1. If N1 = N2, then B∗ = {±b1,±b2}.\n2. If N1 > N2, then B∗ = {±b1}.\nNotice that when N1 > N2, problem (20) recovers the normal b1 to the dominant hyperplane, irrespectively of how separated the two hyperplanes are, since, according to Proposition 1, the principal angle θ12 between b1, b2 does not play a role. The continuous problem (20) is equally favorable in recovering normal vectors as global minimizers in the dual situation, where the arrangement consists of up to D perfectly separated (orthogonal) hyperplanes, as asserted by the next Theorem.\nTheorem 2 Let b1, . . . , bn be an orthogonal hyperplane arrangement (θij = π/2, ∀i 6= j) of RD, with n ≤ D, and weights N1 ≥ N2 ≥ · · · ≥ Nn. Then the set B∗ of global minimizers of (20) can be characterized as follows:\n1. If N1 = Nn, then B∗ = {±b1, . . . ,±bn}.\n2. If N1 = · · · = N` > N`+1 ≥ · · ·Nn, for some ` ∈ [n− 1], then B∗ = {±b1, . . . ,±b`}.\nTheorems 1 and 2 are not hard to prove, since for two hyperplanes the objective can be shown to be a strictly concave function, while for orthogonal hyperplanes the objective is separable. In contrast, the problem becomes considerably harder for n > 2 non-orthogonal hyperplanes. Even when n = 3, characterizing the global minimizers of (20) as a function of the geometry and the weights seems challenging. Nevertheless, when the three hyperplanes are equiangular and their weights are equal, the symmetry of the configuration allows to analytically characterize the median as a function of the angle of the arrangement.\nTheorem 3 Let b1, b2, b3 be an equiangular hyperplane arrangement of RD, D ≥ 3, with θ12 = θ13 = θ23 = θ ∈ (0, π/2] and weights N1 = N2 = N3. Let B∗ be the set of global minimizers of (20). Then B∗ satisfies the following phase transition:\n1. If θ > 60◦, then B∗ = {±b1,±b2,±b3}.\n2. If θ = 60◦, then B∗ = { ±b1,±b2,±b3,± 1√31 } .\n3. If θ < 60◦, then B∗ = { ± 1√ 3 1 } .\nProposition 3, whose proof uses nontrivial arguments from spherical and algebraic geometry, is particularly enlightening, since it suggests that the global minimizers of (20) are associated to normal vectors of the underlying hyperplane arrangement when the hyperplanes are sufficiently separated, while otherwise they seem to be capturing the median hyperplane of the arrangement. This is in striking similarity with the results regarding the Fermat point of planar and spherical triangles (Ghalieh and Hajja, 1996). However, when the symmetry in Theorem 3 is removed, by not requiring the principal angles θij or/and the weights Ni to be equal, then our proof technique no longer applies, and the problem seems even harder. Even so, one intuitively expects an interplay between the angles and the weights of the arrangement under which, if the hyperplanes are sufficiently separated and H1 is sufficiently dominant, then (20) should have a unique global minimizer equal to b1. Our next theorem formalizes this intuition.\nTheorem 4 Let b1, . . . , bn be an arrangement of n ≥ 3 hyperplanes in RD, with pairwise principal angles θij . Let N1 ≥ N2 ≥ · · · ≥ Nn be positive integer weights assigned to the arrangement. Suppose that N1 is large enough, in the sense that\nN1 > √ α2 + β2, (21)\nwhere\nα := ∑ i>1 Ni sin(θ1,i)− √∑ i>1 N2i − σmax ( [NiNj cos(θij)]i,j>1 ) ≥ 0, (22)\nβ := √∑ i>1 N2i + 2 ∑ i 6=j, i,j>1 NiNj cos(θij), (23)\nwith σmax (\n[NiNj cos(θij)]i,j>1\n) denoting the maximal eigenvalue of the (n−1)× (n−1) matrix,\nwhose (i − 1, j − 1) entry is NiNj cos(θij) and 1 < i, j ≤ n. Then any global minimizer b∗ of problem (20) must satisfy b∗ = ±bi, for some i ∈ [n]. If in addition,\nγ := min i′ 6=1 ∑ i 6=i′ Ni sin(θi′,i)− ∑ i>1 Ni sin(θ1,i) > 0, (24)\nthen problem (20) admits a unique up to sign global minimizer b∗ = ±b1.\nLet us provide some intuition about the meaning of the quantities α, β and γ in Theorem 4. To begin with, the first term in α is precisely equal to J (b1), while the second term in α is a lower bound on the objective function N2 sin(θ2) + · · · + Nn sin(θn), if one discards hyperplane H1. Moving on, under the hypothesis that N1 > √ α2 + β2, the quantity βN1 admits a nice geometric interpretation:\ncos−1 ( β N1 ) is a lower bound on how small the principal angle of a critical point b† from b1 can be, if b† 6= ±b1. Interestingly, this means that the larger N1 is, the larger this minimum angle is, which shows that critical hyperplanes H† (i.e., hyperplanes defined by critical points b†) that are distinct from H1, must be sufficiently separated from H1. Finally, the first term in γ is J (b1), while the second term is the smallest objective value that corresponds to b = bi, i > 1, and so (24) simply guarantees that J (b1) < J (bi), ∀i > 1.\nNext, notice that condition N1 > √ α2 + β2 of Theorem 4 is easier to satisfy when H1 is close to the rest of the hyperplanes (which leads to small α), while the rest of the hyperplanes are sufficiently separated (which leads to small α and small β). Here the notion of close and separated is to be interpreted relatively toH1 and its assigned weight N1. Regardless, one can show that\n√ 2 ∑ i>1 Ni ≥ √ α2 + β2, (25)\nand so if\nN1 > √ 2 ∑ i>1 Ni, (26)\nthen any global minimizer of (20) has to be one of the normals, irrespectively of the θij . Finally, condition (24) is consistent with condition (21) in that it requires H1 to be close to Hi, i > 1 and Hi,Hj to be sufficiently separated for i, j > 1. Once again, (24) can always be satisfied irrespectively of the θij , by choosing N1 sufficiently large, since only the positive term in the definition of γ depends on N1, once again manifesting that the terms close and separated are used relatively to H1 and its assigned weight N1.\nRemoving the term N1 sin(θ1) from the objective function, which corresponds to having identified H1 and removing its associated points, one may re-apply Theorem 4 to the remaining hyperplanes H2, . . . ,Hn to obtain conditions for recovering b2 and so on. Notice that these conditions will be independent of N1, rather they will be relative to H2 and its assigned weight N2, and can always be satisfied for large enough N2. Finally, further recursive application of Theorem 4 can furnish conditions for sequentially recovering all normals b1, . . . , bn. However, we should point out that the conditions of Theorem 4 are readily seen to be stronger than necessary. For example, we already know from Theorem 2 that when the arrangement is orthogonal, i.e., θij = π/2, ∀i 6= j, then problem (20) has a unique minimizer ±b1 as soon as N1 > Ni,∀i > 1. On the contrary, Theorem 4 applied to that case requires N1 to be unnecessarily large, i.e., condition (21) becomes\nN21 > (∑ i>1 Ni )2 + 2 ∑ i>1 N2i − 2 (∑ i>1 Ni )(∑ i>1 N2i )1/2 , (27)\nwhich in the special case N2 = · · · = Nn reduces to N1 > (n− 1)N2. This is clearly an artifact of the techniques used to prove Theorem 4, and not a weakness of problem (20) in terms of its global optimality properties. Improving the proof technique of Theorem 4 is an open problem."
    }, {
      "heading" : "3.3 Theoretical analysis of the discrete problem",
      "text" : "We now turn our attention to the discrete problem of hyperplane clustering via DPCP, i.e., to problems (9) and (11), for the case where X = [X 1, . . . ,X n]Γ, with X i being Ni points in Hi, as described in §3.1. As a first step of our analysis we define certain uniformity parameters i, which serve as link between the continuous and discrete domains. Towards that end, note that for any i ∈ [n] and b ∈ SD−1, the quantity ||X>i b||1 can be written as∥∥∥X>i b∥∥∥\n1 = Ni∑ j=1 ∣∣∣b>x(i)j ∣∣∣ = b> Ni∑ j=1 Sign ( b>x (i) j ) x (i) j = Ni b >χi,b, (28)\nwhere\nχi,b := 1\nNi Ni∑ j=1 Sign ( b>x (i) j ) x (i) j (29)\nis the average point of X i with respect to the orthogonal projection hi,b := πHi(b) of b onto Hi. Notice that χi,b can be seen as an average of the function y ∈ Ĥi 7→ Sign ( b>y ) y ∈ Ĥi through the point sent X i ⊂ Ĥi. In other words, χi,b can be seen as an approximation of the integral9∫ x∈Ĥi Sign ( b>x ) x dµĤi = c ĥi,b, (30)\nwhere c was defined in (17). To remove the dependence on b we define the approximation error i associated to hyperplaneHi as\ni := max b∈SD−1 ∥∥∥χi,b − c ĥi,b∥∥∥ 2 . (31)\nThen it can be shown (see Tsakiris and Vidal (2017b) §4.2) that when the points X i are uniformly distributed in a deterministic sense (Grabner et al., 1997; Grabner and Tichy, 1993), i is small and in particular i → 0 as Ni →∞.\nWe are now almost ready to state our main results, before doing so though we need a rather technical, yet necessary, definition.\nDefinition 5 For a set Y = [y1, . . . ,yL] ⊂ SD−1 and positive integer K ≤ L, define RY,K to be the maximum circumradius among the circumradii of all polytopes {∑K i=1 αjiyji : αji ∈ [−1, 1] } , where j1, . . . , jK are distinct integers in [L], and the circumradius of a closed bounded set is the minimum radius among all spheres that contain the set. We now define the quantity of interest as\nR := max K1+···+Kn=D−1\n0≤Ki≤D−2\nn∑ i=1 RX i,Ki . (32)\nWe note that it is always the case that RX i,Ki ≤ Ki, with this upper bound achieved when X i contains Ki colinear points. Combining this fact with the constraint ∑ iKi = D−1 in (32), we get that R ≤ D − 1, and the more uniformly distributed are the points X inside the hyperplanes, the smallerR is (even thoughR does not go to zero).\nRecalling the definitions of c, i andR in (17), (31) and (32), respectively, we have the following result about the non-convex problem (9). Theorem 6 Let b∗ be a solution of (9) with X = [X 1, . . . ,X n]Γ, and suppose that c > √ 2 1. If\nN1 >\n√ ᾱ2 + β̄2, where (33)\nᾱ := α+ c−1 ( 1N1 + 2\n∑ i>1 iNi\n) , and (34)\nβ̄ := β + c−1 ( R+ ∑ iNi ) , (35)\n9. For details regarding the evaluation of this integral see Lemma 9 and its proof in Tsakiris and Vidal (2017b).\nwith α, β as in Theorem 4, then b∗ = ±bi for some i ∈ [n]. Furthermore, b∗ = ±b1, if γ̄ := γ − c−1 ( 1N1 + 2N2 + 2\n∑ i>2 iNi\n) > 0. (36)\nNotice the similarity of conditions N1 > √ ᾱ2 + β̄2, γ̄ > 0 of Theorem 6 with conditions N1 >√\nα2 + β2, γ > 0 of Theorem 4. In fact ᾱ > α, β̄ > β, γ̄ < γ, which implies that the conditions of Theorem 6 are strictly stronger than those of Theorem 4. This is no surprise since, as we have already remarked, the solution set of (9) depends not only on the geometry (θij) and the weights (Ni) of the arrangement, but also on the distribution of the data points (parameters i andR).\nWe note that in contrast to condition (21) of Theorem 4, N1 now appears in both sides of condition (33) of Theorem 6. Nevertheless, under the assumption c > √ 2 1, (33) is equivalent to the positivity of a quadratic polynomial in N1, whose leading coefficient is positive, and hence it can always be satisfied for sufficiently large N1.\nAnother interesting connection of Theorem 4 to Theorem 6, is that the former can be seen as a limit version of the latter : dividing (33) and (36) by N1, letting N1, . . . , Nn go to infinity while keeping each ratio Ni/N1 fixed, and recalling that i → 0 as Ni →∞ andR ≤ D − 1, we recover the conditions of Theorem 4.\nNext, we consider the linear programming recursion (11). At a conceptual level, the main difference between the linear programming recursion in (11) and the continuous and discrete problems (20) and (9), respectively, is that the behavior of (11) depends highly on the initialization n̂0. Intuitively, the closer n̂0 is to b1, the more likely the recursion will converge to b1, with this likelihood becoming larger for larger N1. The precise technical statement is as follows.\nTheorem 7 Let {n̂k} be the sequence generated by the linear programming recursion (11) by means of the simplex method, where n̂0 ∈ SD−1 is an initial estimate for b1, with principal angle from bi equal to θi,0. Suppose that c > √ 5 1, and let θ (1) min = mini>1 {θ1i}. If θ1,0 is small enough, i.e.,\nsin(θ1,0) < min { sin ( θ (1) min ) − 2 1, √ 1− (c−1 1)2 − 2c−1 1 } , (37)\nand N1 is large enough in the sense that\nN1 > max\n{ µ, ν + √ ν2 + 4ρτ\n2τ\n} , where (38)\nµ := max j 6=1\n{∑ i>1Ni sin(θi,0) + c −1 jNj + ∑ i 6=1,j Ni [ 2c−1 i − sin(θij) ] sin(θ1j)− sin(θ1,0)− 2c−1 1 } , (39)\nν := 2c−1 1 ( β + c−1R+ c−1\n∑ i>1 iNi\n) + 2 [ sin(θ1,0) + 2c −1 1 ](\nα+ 2c−1 ∑ i>1 iNi\n) ,\n(40)\nρ := ( α+ 2c−1\n∑ i>1 iNi\n)2 + ( β + c−1R+ c−1\n∑ i>1 iNi\n)2 , (41)\nτ := cos2(θ1,0)− 4c−1 1 sin(θ1,0)− 5(c−1 1)2, (42)\nwith α, β as in Theorem 4, then {nk} converges to either b1 or −b1 in a finite number of steps.\nThe quantities appearing in Theorem 7 are harder to interpret than those of Theorem 6, but we can still give some intuition about their meaning. To begin with, the two inequalities in (38) represent two distinct requirements that we enforced in our proof, which when combined, guarantee that the limit point of (11) is ±b1.\nThe first requirement is that no ±bi can be the limit point of (11) for i > 1; this is captured by a linear inequality of the form\nµN1 + (terms not depending on N1) > 0, (43)\nwhich is satisfied either for N1 sufficiently large (if µ > 0) or for N1 sufficiently small (if µ < 0). To avoid pathological situations where N1 is required to be negative or less than D− 1, it is natural to enforce µ to be positive. This is precisely achieved by inequality sin(θ1,0) < sin ( θ (1) min ) − 2 1 in (37), which is a quite natural condition itself: the initial estimate n̂0 needs to be closer to b1 than any other normal bi for i > 1, and the more well-distributed the data X 1 are insideH1 (smaller 1), the further n̂0 can be from b1.\nThe second requirement that we employed in our proof is that the limit point of (11) is one of the ±b1, . . . ,±bn; this is captured by requiring that a certain quadratic polynomial\np(N1) := τ N 2 1 − ν N1 − ρ (44)\nin N1 is positive. To avoid situations where the positivity of this polynomial contradicts the relation N1 > µ, it is important that we ask its leading coefficient τ to be positive, so that the second requirement is satisfied for N1 large enough, and thus is compatible with N1 > µ. As it turns out, τ is positive only if the data X 1 are sufficiently well distributed in H1, which is captured by condition c > √ 5 1 of Theorem 7. Even so, this latter condition is not sufficient; instead\nsin(θ1,0) < √\n1− (c−1 1)2 − 2c−1 1 is needed (as in (37)), which is once again very natural: the more well-distributed the data X 1 are insideH1 (smaller 1), the further n̂0 from b1 can be.\nNext, notice that the conditions of Theorem 7 are not directly comparable to those of Theorem 6. Indeed, it may be the case that ±b1 is not a global minimizer of the non-convex problem (9), yet the recursions (11) do converge to b1, simply because n̂0 is close to b1. In fact, by (37) n̂0 must be closer to b1 than bi to b1 for any i > 1, i.e., θ (1) min > θ1,0. Similarly to Theorems 4 and 6, the more separated the hyperplanes Hi,Hj are for i, j > 1, the easier it is to satisfy condition (38). In contrast, H1 needs to be sufficiently separated from Hi for i > 1, since otherwise µ becomes large. This has an intuitive explanation: the less separated H1 is from the rest of the hyperplanes, the less resolution the linear program (11) has in distinguishing b1 from bi, i > 1. To increase this resolution, one needs to either select n̂0 very close to b1, or select N1 very large. The acute reader may recall that the quantity α appearing in (41) becomes larger when H1 becomes separated from Hi, i > 1. Nevertheless, there are no inconsistency issues in controlling the size of µ and ρ. This is because α is always bounded from above by ∑ i>1Ni, i.e., α does not increase arbitrarily as the θ1i increase. Another way to look at the consistency of condition (38), is that its RHS does not depend on N1; hence one can always satisfy (38) by selecting N1 large enough."
    }, {
      "heading" : "4. Proofs",
      "text" : "In this Section we prove Theorems 1-3 associated to the continuous problem (20), as well as Theorems 6 and 7 associated to the discrete non-convex `1 minimization problem (9) and the recursion of linear programs (11) respectively."
    }, {
      "heading" : "4.1 Preliminaries on the continuous problem",
      "text" : "We start by noting that the objective function (20) is everywhere differentiable except at the points ±b1, . . . ,±bn, where its partial derivatives do not exist. For any b ∈ SD−1 distinct from ±bi, the gradient at b is given by\n∇bJ = − n∑ i=1 b>i b( 1− (b>i b)2 ) 1 2 bi. (45)\nNow let b∗ be a global solution of (20) and suppose that b∗ 6= ±bi, ∀i ∈ [n]. Then b∗ must satisfy the first order optimality condition\n∇bJ |b∗ + λ∗ b∗ = 0, (46)\nwhere λ∗ is a Lagrange multiplier. Equivalently, we have\n− n∑ i=1 Ni ( b>i b ∗ )( 1− ( b>i b ∗ )2)− 12 bi + λ ∗ b∗ = 0, (47)\nwhich implies that\nn∑ i=1 Ni ( b>i b ∗ )2( 1− ( b>i b ∗ )2)− 12 b∗ = n∑ i=1 Ni ( b>i b ∗ )( 1− ( b>i b ∗ )2)− 12 bi, (48)\nfrom which the next Lemma follows.\nLemma 8 Let b∗ be a global solution of (20). Then b∗ ∈ Span (b1, . . . , bn).\nProof If b∗ is equal to some ±bi, then the statement of the Lemma is certainly true. If b∗ 6= ±bi, ∀i ∈ [n], then b∗ satisfies (48), from which again the statement is true."
    }, {
      "heading" : "4.2 Proof of Theorem 1",
      "text" : "By Lemma 8 any global solution must lie in the plane Span(b1, b2), and so our problem becomes planar, i.e., we may as well assume that the hyperplane arrangement b1, b2 is a line arrangement of R2. Note that b1, b2 ∈ S1 partition S1 in two arcs, and among these, only one arc has length θ strictly less than π; we denote this arc by a. Next, recall that the continuous objective function for two hyperplanes can be written as\nJ (b) = N1 ( 1− (b>1 b)2 ) 1 2 +N2 ( 1− (b>2 b)2 ) 1 2 , b ∈ S1. (49)\nLet b∗ be a global solution, and suppose that b∗ 6∈ a. If −b∗ ∈ a, then we can replace b1, b2 by −b1,−b2, an operation that does not change neither the arrangement nor the objective. After this replacement, we have that b∗ ∈ a. Finally suppose that neither b∗ nor −b∗ are inside a. Then replacing either b1 with −b1 or b2 with −b2, leads to b∗ ∈ a. Consequently, without loss of generality we may assume that b∗ lies in a. Moreover, subject to a rotation and perhaps exchanging b1 with b2, we can assume that b1 is aligned with the positive x-axis and that the angle θ between b1 and b2, measured counter-clockwise, lies in (0, π). Then b∗ is a global solution to\nJ (b) = N1 ( 1− (b>1 b)2 ) 1 2 +N2 ( 1− (b>2 b)2 ) 1 2 , b ∈ S1 ∩ a. (50)\nNow, for any vector b ∈ S1 ∩ a, let θ1, θ2 = θ − θ1 be the angle between b and b1, b2 respectively. Then our objective can be written as\nJ (b) = J̃ (θ1) = N1 sin(θ1) +N2 sin(θ − θ1), θ1 ∈ [0, θ]. (51)\nTaking first and second derivatives, we have\n∂J̃ ∂θ1 = N1 cos(θ1)−N2 cos(θ − θ1) (52)\n∂2J̃ ∂θ21 = −N1 sin(θ1)−N2 sin(θ − θ1). (53)\nSince the second derivative is everywhere negative on [0, θ], J̃ (θ1) is strictly concave on [0, θ] and so its minimum must be achieved at the boundary θ1 = 0 or θ1 = θ. This means that either b∗ = b1 or b∗ = b2."
    }, {
      "heading" : "4.3 Proof of Theorem 2",
      "text" : "For the sake of simplicity we assume n = 3, the general case follows in a similar fashion. Letting xi := b > i b and yi := √ 1− x2i , (48) can be written as(\nN1 x21 y1 +N2 x22 y2 +N3 x23 y3\n) b∗ = N1\nx1 y1 b1 +N2 x2 y2 b2 +N3 x3 y3 b3. (54)\nTaking inner products of (54) with b1, b2, b3 we respectively obtain( N1\nx21 y1 +N2 x22 y2 +N3 x23 y3\n) x1 = N1\nx1 y1 +N2 x2 y2 (b>1 b2) +N3 x3 y3 (b>1 b3), (55)( N1\nx21 y1 +N2 x22 y2 +N3 x23 y3\n) x2 = N1\nx1 y1 (b>2 b1) +N2 x2 y2 +N3 x3 y3 (b>2 b3), (56)( N1\nx21 y1 +N2 x22 y2 +N3 x23 y3\n) x3 = N1\nx1 y1 (b>3 b1) +N2 x2 y2 (b>3 b2) +N3 x3 y3 . (57)\nSince by Lemma 8 b∗ is a linear combination of b1, b2, b3, we can assume that D = 3. Suppose that b∗ 6= ±bi, ∀i ∈ [n]. Now, suppose that x3 = 0. Then we can not have either x1 = 0 or x2 = 0, otherwise b∗ = b2 or b∗ = b1 respectively. Hence x1, x2 6= 0. Then equations (55)-(57) imply that\nN1 y1 = N2 y2 and x21 + x 2 2 = 1. (58)\nTaking into consideration the relations x2i + y 2 i = 1, we deduce that\ny1 = N1√\nN21 +N 2 2\n, y2 = N2√\nN21 +N 2 2\n. (59)\nThen\nJ (b∗) = N1y1 +N2y2 +N3y3 = √ N21 +N 2 2 +N3 > J (b1) = N2 +N3, (60)\nwhich is a contradiction on the optimality of b∗. Similarly, none of the x1, x2 can be zero, i.e. x1, x2, x3 6= 0. Then equations (55)-(57) imply that\nx21 + x 2 2 + x 2 3 = 1, N1 y1 = N2 y2 = N3 y3 , (61)\nwhich give\nyi = Ni √ 2√ N21 +N 2 2 +N 2 3 , i = 1, 2, 3. (62)\nBut then J (b∗) = √ 2 ( N21 +N 2 2 +N 2 3 ) > J (b1) = N2 +N3. This contradiction shows that our hypothesis b∗ 6= ±bi, ∀i ∈ [n] is not valid, i.e., B∗ ⊂ {±b1,±b2,±b3}. The rest of the theorem follows by comparing the values J (bi), i ∈ [3]."
    }, {
      "heading" : "4.4 Proof of Theorem 3",
      "text" : "Without loss of generality, we can describe an equiangular arrangement of three hyperplanes of RD, with an equiangular arrangement of three planes of R3, with normals b1, b2, b3 given by\nb1 := µ [ 1 + α α α ]> (63) b2 := µ [ α 1 + α α\n]> (64) b3 := µ [ α α 1 + α\n]> (65) µ := [ (1 + α)2 + 2α2 ]− 1 2 , (66)\nwith α a positive real number that determines the angle θ ∈ (0, π/2] of the arrangement, given by\ncos(θ) := 2α(1 + α) + α2\n(1 + α)2 + 2α2 =\n2α+ 3α2\n1 + 2α+ 3α2 . (67)\nSince N1 = N2 = N3, so our objective function essentially becomes\nJ (b) = ( 1− (b>1 b)2 ) 1 2 + ( 1− (b>2 b)2 ) 1 2 + ( 1− (b>3 b)2 ) 1 2 , b ∈ S2 (68)\n= sin(θ1) + sin(θ2) + sin(θ3), (69)\nwhere θi is the principal angle of b from bi. The next Lemma shows that any global minimizer b∗ must have equal principal angles from at least two of the b1, b2, b3.\nLemma 9 Let b1, b2, b3 be an arrangement of equiangular planes in R3, with angle θ and weights N1 = N2 = N3. Let b∗ be a global minimizer of (20) and let xi := b>i b ∗, yi = √\n1− x2i i = 1, 2, 3. Then either y1 = y2 or y1 = y3 or y2 = y3.\nProof If b∗ is one of ±b1,±b2,±b3, then the statement clearly holds, since if say b∗ = b1, then y2 = y3 = sin(θ). So suppose that b∗ 6= ±bi, ∀i ∈ [3]. Then xi, yi must satisfy equations (55)-(57), together with x2i + y 2 i = 1. Allowing for yi to take the value zero, the xi, yi must satisfy\np1 := x1y1y2y3 + x2y3[z − x1x2] + x3y2[z − x1x3] = 0, (70) p2 := x1y3[z − x1x2] + x2y1y2y3 + x3y1[z − x2x3] = 0, (71) p3 := x1y2[z − x1x3] + x2y1[z − x2x3] + x3y1y2y3 = 0, (72) q1 := x 2 1 + y 2 1 − 1, (73) q2 := x 2 2 + y 2 2 − 1, (74) q3 := x 2 3 + y 2 3 − 1, (75)\nwhere z := cos(θ). Viewing the above system of equations as polynomial equations in the variables x1, x2, x3, y1, y2, y3, z, standard Groebner basis (Cox et al., 2007) computations reveal that the polynomial\ng := (1− z)(y21 − y22)(y21 − y23)(y22 − y23)(y1 + y2 + y3) (76)\nlies in the ideal generated by pi, qi, i = 1, 2, 3. In simple terms, this means that b∗ must satisfy g(xi, yi, z = cos(θ)) = 0. However, the yi are by construction non-negative and can not be all zero. Moreover, θ > 0 so 1− z 6= 0. This implies that\n(y21 − y22)(y21 − y23)(y22 − y23) = 0, (77)\nwhich in view of the non-negativity of the yi implies\n(y1 − y2)(y1 − y3)(y2 − y3) = 0. (78)\nThe next Lemma says that a global minimizer of J (b) is not far from the arrangement.\nLemma 10 Let b1, b2, b3 be an arrangement of equiangular planes in R3, with angle θ and weights N1 = N2 = N3. Let Ci be the spherical cap with center bi and radius θ. Then any global minimizer of (69) must lie (up to a sign) either on the boundary or the interior of C1 ∩ C2 ∩ C3.\nProof First of all notice that b1, b2, b3 lie on the boundary of C1 ∩ C2 ∩ C3. Let b∗ be a global minimizer. If θ = π/2, we have already seen in Proposition 2 that b∗ has to be one of the vertices b1, b2, b3 (up to a sign); so suppose that θ < π/2. Let θ∗i be the principal angle of b\n∗ from bi. Then at least two of θ∗1, θ ∗ 2, θ ∗ 3 must be less or equal to θ; for if say θ ∗ 1, θ ∗ 2 > θ, then b3 would give a smaller objective than b∗. Hence, without loss of generality we may assume that θ∗1, θ ∗ 2 ≤ θ. In addition, because of Lemma 9, we can further assume without loss of generality that θ∗1 = θ ∗ 2. Let ζ be the vector in the small arc that joins 1√ 3 1 and b3 and has angle from b1, b2 equal to θ∗1. Since\nJ (b∗) ≤ J (ζ), it must be the case that the principal angle θ∗3 is less or equal to θ (because the angle of ζ from b3 is ≤ θ). We conclude that θ∗1, θ∗2, θ∗3 ≤ θ. Consequently, there exist i 6= j such that up to a sign b∗ ∈ Ci ∩ Cj . Let us assume without loss of generality that b∗ ∈ C1 ∩ C2, i.e., θ∗1, θ∗2 are the angles of b∗ from b1, b2 (notice that now it may no longer be the case that θ∗1 = θ ∗ 2).\nNotice that the boundaries of C1 and C2 intersect at two points: b3 and its reflection b̃3 with respect to the plane H12 spanned by b1, b2. In fact, H12 divides C1 ∩ C2 in two halves, Y, Ỹ , with Y being the reflection of Ỹ with respect to H12. Letting C̃3 be the spherical cap of radius θ around b̃3, we can write\nC1 ∩ C2 = (C1 ∩ C2 ∩ C3) ∪ (C1 ∩ C2 ∩ C̃3). (79)\nIf b∗ ∈ C1 ∩C2 ∩C3 we are done, so let us assume that b∗ ∈ C1 ∩C2 ∩ C̃3. Let b̃ ∗\nbe the reflection of b∗ with respect to H12. This reflection preserves the angles from b1 and b2. We will show that b̃ ∗\nhas a smaller principal angle θ̃∗3 from b3 than b ∗. In fact the spherical angle of b̃ ∗ from b3 is θ̃∗3 itself, and this is precisely the angle of b∗ from b̃3. Denote by H3,3̃ the plane spanned by b3 and b̃3, b̄ ∗ the spherical projection of b∗ onto H3,3̃, γ the angle between b̄ ∗ and b∗, α the angle between b̄∗ and b3, and α̃ the angle between b̄ ∗ and b̃3. Then the spherical law of cosines gives\ncos(θ̃∗3) = cos(α̃) cos(γ), (80) cos(θ∗3) = cos(α) cos(γ). (81)\nLetting 2ψ be the angle between b3 and b̃3, we have that\nα = ψ + (ψ − α̃). (82)\nBy hypothesis α̃ < ψ and so α > ψ. If 2ψ ≤ π/2, then α is an acute angle and cos(α̃) > cos(α). If 2ψ > π/2, then cos(α̃) ≤ cos(α) only when π − (2ψ − α̃) ≤ α̃⇔ ψ ≥ π/2. But by construction ψ ≤ π/2 and equality is achieved only when θ = π/2. Hence, we conclude that cos(α̃) > |cos(α)|, which implies that cos(θ̃3) > |cos(θ3)|. This in turn means that J (b̃ ∗ ) < J (b∗), which is a contradiction.\nLemma 11 Let b1, b2, b3 be an arrangement of equiangular planes in R3, with angle θ and weights N1 = N2 = N3. Let b∗ be a global minimizer of (20) and let xi := b>i b\n∗, i = 1, 2, 3. Then either x1, x2, x3 are all non-negative or they are all non-positive.\nProof By Lemma 10, we know that either b∗ ∈ C1 ∩ C2 ∩ C3 or −b∗ ∈ C1 ∩ C2 ∩ C3. In the first case, the angles of b∗ from b1, b2, b3 are less or equal to θ ≤ π/2.\nNow, Lemmas 9 and 11 show that b∗ is a global minimizer of problem\nmin b∈S2\n{√ 1− (b>1 b)2 + √ 1− (b>2 b)2 + √ 1− (b>3 b)2 } (83)\nif and only if it is a global minimizer of problem\nmin b∈S2\n{√ 1− (b>1 b)2 + √ 1− (b>2 b)2 + √ 1− (b>3 b)2 } , (84)\ns.t. b>i b = b > j b, for some i 6= j ∈ [3]. (85)\nSo suppose without loss of generality that b∗ is a global minimizer of (85) corresponding to indices i = 1, j = 2. Then b∗ lives in the vector space\nV12 = Span 11 0  , 00 1  , (86) which consists of all vectors that have equal angles from b1 and b2. Taking into consideration that b∗ also lies in S2, we have the parametrization\nb∗ = 1√\n2v2 + w2 vv w  . (87) The choice v = 0, corresponding to b∗ = e3 (the third standard basis vector), can be excluded, since b3 always results in a smaller objective: moving b from e3 to b3 while staying in the plane V12 results in decreasing angles of b from b1, b2, b3. Consequently, we can assume v = 1, and our problem becomes an unconstrained one, with objective\nJ (w) = 2 [ (2 + w2)(1 + 2α+ 3α2)− (αw + 2α+ 1)2 ]1/2 + √\n2 |a− aw + 1| [(2 + w2)(1 + 2α+ 3α2)]1/2 . (88)\nNow, it can be shown that:\n• The following quantity is always positive\nu := (2 + w2)(1 + 2α+ 3α2)− (αw + 2α+ 1)2. (89)\n• The choice w = 1 + 1/α corresponds to b∗ = b3, and that is precisely the only point where J (w) is non-differentiable.\n• The choice w = 1 corresponds to b∗ = 1√ 3 1.\n• The choice α = 1/3 corresponds to θ = 60◦.\n• J (b3) = J ( 1√ 3 1 ) precisely for α = 1/3.\nSince for α = 0 the theorem has already been proved (orthogonal case), we will assume that α > 0. We proceed by showing that for α ∈ (0, 1/3) and for w 6= 1 + 1/a, it is always the case that J (w) > J (1 + 1/a). Expanding this last inequality, we obtain\n2 [ (2 + w2)(1 + 2α+ 3α2)− (αw + 2α+ 1)2 ]1/2 + √\n2 |a− aw + 1| [(2 + w2)(1 + 2α+ 3α2)] 1/2 >\n2 √ 1 + 4α+ 6α2\n1 + 2α+ 3α2 , (90)\nwhich can be written equivalently as\np1 < 4 √\n2u1/2 |α− αw + 1| (1 + 2α+ 3α2), where (91) p1 := 4(2 + w 2)(1 + 4α+ 6α2)− (1 + 2α+ 3α2) [ 4u+ 2(α− αw + 1)2 ] , (92)\nand u has been defined in (89). Viewing p1 as a polynomial in w, p1 has two real roots given by\nr(1)p1 := 1 + 1/α > r (2) p1 :=\n−1− 7α+ α2 + 15α3\nα(7 + 22α+ 15α2) . (93)\nSince the leading coefficient of p1 is always a negative function of α (for α > 0), (91) will always be true for w 6∈ [r(2)p1 , r (1) p1 ], in which interval p1 is strictly negative. Consequently, we must show that as long as α ∈ (0, 1/3), (91) is true for every w ∈ [r(2)p1 , r (1) p1 ). For such w, p1 is non-negative and by squaring (91), we must show that\np2 >0, ∀w ∈ [r(2)p1 , r (1) p1 ), ∀α ∈ (0, 1/3), (94) p2 :=32u(α− αw + 1)2(1 + 2α+ 3α2)− p21. (95)\nInterestingly, p2 admits the following factorization\np2 =− 4(−1− α+ αw)2p3, (96) p3 :=− 7− 18α− 49α2 − 204α3 − 441α4 − 162α5 + 81α6\n+ (30α+ 238α2 + 612α3 + 468α4 − 162α5 − 162α6)w + (−8− 48α− 111α2 − 12α3 + 270α4 + 324α5 + 81α6)w2 (97)\nThe discriminant of p3 is the following 10-degree polynomial in α:\n∆(p3) = 32(−7− 60α− 226α2 − 312α3 + 782α4 + 5160α5 + 13500α6+ + 21816α7 + 22761α8 + 14580α9 + 4374α10). (98)\nBy Descartes rule of signs, ∆(p3) has precisely one positive root. In fact this root is equal to 1/3. Since the leading coefficient of ∆(p3) is positive, we must have that ∆(p3) < 0, ∀α ∈ (0, 1/3), and so for such α, p3 has no real roots, i.e. it will be either everywhere negative or everywhere positive. Since p3(α = 1/4, w = 1) = −80327/4096, we conclude that as long as α ∈ (0, 1/3), p3 is everywhere negative and as long as w 6= 1 + 1/α, p2 is positive, i.e. we are done.\nMoving on to the case α = 1/3, we have\np2(α = 1/3, w) = 128\n9 (w − 4)2(w − 1)2, (99)\nwhich shows that for such α the only global minimizers are ±b3 and ± 1√31. In a similar fashion, we can proceed to show that J (w) > J ( 1√ 3 1 )\n, for all w 6= 1 and all α ∈ (1/3,∞). However, the roots of the polynomials that arise are more complicated functions of α and establishing the inequality J (w) > J ( 1√ 3 1 )\nanalytically, seems intractable; instead this can be done if one allows for numeric computation of polynomial roots."
    }, {
      "heading" : "4.5 Proof of Theorem 4",
      "text" : "We begin with two Lemmas.\nLemma 12 Let b1, . . . , bn be vectors of SD−1, with pairwise principal angles θij . Then\nmax b>b=1\n[ N1 ∣∣∣b>1 b∣∣∣+ · · ·+Nn ∣∣∣b>n b∣∣∣] ≤ ∑\ni N2i + 2 ∑ i 6=j NiNj cos(θij) 1/2 . (100) Proof Let b† be a maximizer of N1\n∣∣b>1 b∣∣ + · · · + Nn ∣∣b>n b∣∣. Then b† must satisfy the first order optimality condition, which is\nλ†b† = ∑ i Ni Sgn(b > i b †)bi, (101)\nwhere λ† is a Lagrange multiplier and Sgn(b>i b †) is the subdifferential of ∣∣b>i b†∣∣. Then λ†b† = N1s † 1b1 + · · ·+Nns † nbn, (102)\nwhere s†i = Sign(b > i b †), if b>i b † 6= 0, and s†i ∈ [−1, 1] otherwise. Recalling that ∥∥b†∥∥ 2 = 1, and taking equality of 2-norms on both sides of (103), we get\nb† = N1s\n† 1b1 + · · ·+Nns † nbn∥∥∥N1s†1b1 + · · ·+Nns†nbn∥∥∥\n2\n. (103)\nNow\n∑ i Ni ∣∣∣b>i b†∣∣∣ = ∑ i:b† 6⊥bi Ni ∣∣∣b>i b†∣∣∣ = ∑ i:b† 6⊥bi Nis † ib > i b † =\n( b† )> ∑\ni:b† 6⊥bi\nNis † ibi  = ( b† )> ∑\ni:b† 6⊥bi\nNis † ibi + ∑ i:b†⊥bi Nis † ibi  = (b†)>(∑ i Nis † ibi )\n(103) = ∥∥∥N1s†1b1 + · · ·+Nns†nbn∥∥∥\n2 = ∑ i ( s†iNi )2 + 2 ∑ i 6=j NiNjs † is † jb > i bj 1/2\n≤ ∑ i N2i + 2 ∑ i 6=j NiNj cos(θij) 1/2 . (104)\nLemma 13 Let b1, . . . , bn be a hyperplane arrangement of RD with integer weights N1, . . . , Nn assigned. For b ∈ SD−1, let θi be the principal angle between b and bi. Then\nmin b∈SD−1\n∑ Ni sin(θi) ≥ √∑ N2i − σmax ( [NiNj cos(θij)]i,j ) , (105)\nwhere σmax (\n[NiNj cos(θij)]i,j\n) denotes the maximal eigenvalue of the n× n matrix, whose (i, j)\nentry is NiNj cos(θij) and 1 ≤ i, j ≤ n.\nProof For any vector ξ we have that ‖ξ‖1 ≥ ‖ξ‖2. Let ψi ∈ [0, 180◦] be the angle between b and bi. Then ∑\nNi sin(θi) = ∑ Ni |sin(ψi)| ‖·‖1≥‖·‖2 ≥ √∑ N2i sin 2(ψi) (106)\n= √∑ N2i − ∑ N2i cos 2(ψi). (107)\nHence Ni sin(θi) is minimized when ∑ N2i cos\n2(ψi) is maximized. But∑ N2i cos 2(ψi) = b > (∑ N2i bib > i ) b, (108)\nand the maximum value of ∑ N2i cos\n2(ψi) is equal to the maximal eigenvalue of the matrix∑ N2i bib > i = [ N1b1 · · · Nnbn ] [ N1b1 · · · Nnbn ]> , (109)\nwhich is the same as the maximal eigenvalue of the matrix[ N1b1 · · · Nnbn ]> [ N1b1 · · · Nnbn ] = [NiNj cos(ψij)]i,j , (110)\nwhere ψij is the angle between bi, bj . Now, if A is a matrix and we denote by |A| the matrix that arises by taking absolute values of each element in the matrixA, then it is known that σmax(|A|) ≥ σmax(A). Hence the result follows by recalling that |cos(ψij)| = cos(θij).\nNow, let b∗ be a global solution of (20). Suppose for the sake of a contradiction that b∗ 6⊥ Hi, ∀i ∈ [n], i.e., b∗ 6= ±bi, ∀i ∈ [n]. Consequently, J is differentiable at b∗ and so b∗ must satisfy (47), which we repeat here for convenience:\n− n∑ i=1 Ni ( b>i b ∗ )( 1− ( b>i b ∗ )2)− 12 bi + λ ∗ b∗ = 0. (111)\nProjecting (111) orthogonally onto the hyperplaneH∗ defined by b∗ we get\n− n∑ i=1 Ni ( b>i b ∗ )( 1− ( b>i b ∗ )2)− 12 πH∗ (bi) = 0. (112)\nSince b∗ 6= bi, ∀i ∈ [n], it will be the case that hi := πH∗ (bi) 6= 0, ∀i ∈ [n]. Since\n‖πH∗ (bi)‖2 = ( 1− ( b>i b ∗ )2) 12 > 0, (113)\nequation (112) can be written as n∑ i=1 Ni ( b>i b ∗ ) ĥi = 0, (114)\nwhich in turn gives\nN1 ∣∣∣b>1 b∗∣∣∣ ≤ ∥∥∥∥∥∑ i>1 Ni ( b>i b ∗ ) ĥi ∥∥∥∥∥ 2 ≤ ∑ i>1 Ni ∣∣∣b>i b∗∣∣∣ ≤ max b>b=1 ∑ i>1 Ni ∣∣∣b>i b∣∣∣ Lem.12≤ β. (115)\nSince by hypothesis N1 > β, we can define an angle θ † 1 by\ncos(θ†1) := β\nN1 , (116)\nand so (115) says that θ1 can not drop below θ † 1. Hence J (b ∗) can be bounded from below as follows:\nJ (b∗) = N1 sin(θ∗1) + ∑ i>1 Ni sin(θ ∗ i ) ≥ N1 sin(θ † 1) + min b>b=1 ∑ i>1 Ni sin(θi) (117)\nLem.13 ≥ N1 sin(θ†1) + √∑ i>1 N2i − σmax ( [NiNj cos(θij)]i,j>1 ) . (118)\nBy the optimality of b∗, we must also have J (b1) ≥ J (b∗), which in view of (118) gives∑ i>1 Ni sin(θ1i) ≥ N1 sin(θ†1) + √∑ i>1 N2i − σmax ( [NiNj cos(θij)]i,j>1 ) . (119) Now, a little algebra reveals that this latter inequality is precisely the negation of hypothesis N1 >√ α2 + β2. This shows that b∗ has to be ±bi, for some i ∈ [n]. For the last statement of the Theorem, notice that condition γ > 0 is equivalent to saying that J (b1) < J (bi), ∀i > 1."
    }, {
      "heading" : "4.6 Proof of Theorem 6",
      "text" : "Let us first derive an upper bound θ(1)max on how large θ∗1 can be. Towards that end, we derive a lower bound on the objective function J (b) in terms of θ1: For any vector b ∈ SD−1 we can write\nJ (b) = ∥∥∥X>b∥∥∥ 1 = ∑∥∥∥X>i b∥∥∥ 1 = ∑ Nib >χi,b (120)\n= ∑ cNi sin(θi) + ∑ Nib >ηi,b, ∥∥ηi,b∥∥2 ≤ i (121) ≥ c ∑ Ni sin(θi)− ∑ iNi (122)\n= cN1 sin(θ1) + c ∑ i>1 Ni sin(θi)− ∑ iNi (123)\n≥ cN1 sin(θ1) + c min b>b=1 [∑ i>1 Ni sin(θi) ] − ∑ iNi (124)\nLem.13 ≥ cN1 sin(θ1) + c √∑ i>1 N2i − σmax ( [NiNj cos(θij)]i,j>1 ) − ∑ iNi. (125)\nNext, we derive an upper bound on J (b1): J (b1) = ∑ i>1 ∥∥∥X>i b1∥∥∥ 1 = ∑ i>1 Nib > 1 χi,b1 (126)\n= ∑ i>1 cNi sin(θ1i) + ∑ i>1 Nib > 1 ηi,b1 , ∥∥ηi,b1∥∥2 ≤ i (127) ≤ c\n∑ i>1 Ni sin(θ1i) + ∑ i>1 iNi. (128)\nSince any vector b for which the corresponding lower bound (125) on J (b) is strictly larger than the upper bound (128) on J (b1), can not be a global minimizer (because it gives a larger objective than b1), θ∗1 must be bounded above by θ (1) max, where the latter is defined, in view of (33), by\nsin ( θ(1)max ) := α+ c−1 ( 1N1 + 2 ∑ i>1 iNi ) N1 , (129)\nwhere α is as in Theorem 6. Now let b∗ be a global minimizer, and suppose for the sake of contradiction that b∗ 6⊥ Hi,∀i ∈ [n]. We will show that there exists a lower bound θ(1)min on θ1, such that θ(1)min > θ (1) max, which is of course a contradiction. Towards that end, the first order optimality condition for b∗ can be written as\n0 ∈ X Sgn(X>b∗) + λb∗, (130)\nwhere λ is a Lagrange multiplier and Sgn(α) = Sign(α) if α 6= 0 and Sgn(0) = [−1, 1], is the subdifferential of the function | · |. Since the points X are general, any hyperplaneH of RD spanned by D − 1 points of X such that at most D − 2 points come from X i, ∀i ∈ [n], does not contain any of the remaining points of X . Consequently, by Lemma 14 b∗ will be orthogonal to precisely D− 1 points { ξ1, . . . , ξD−1 } ⊂ X , from which at most Ki ≤ D− 2 lie inHi. Thus, we can write relation (130) as D−1∑ j=1 αjξj + n∑ i=1 Niχi,b∗ + λb ∗ = 0, (131)\nfor real numbers −1 ≤ αj ≤ 1, ∀j ∈ [D − 1]. Using the definition of i, we can write\nχi,b∗ = c ĥi,b∗ + ηi,b∗ , ∀i ∈ [n], (132) with ∥∥ηi,b∗∥∥2 ≤ i. Note that since b∗ 6⊥ Hi, ∀i ∈ [n], we have ĥi,b∗ 6= 0. Substituting (132) in (131) we get\nD−1∑ j=1 αjξj + c n∑ i=1 Ni ĥi,b∗ + n∑ i=1 Ni ηi,b∗ + λb ∗ = 0, (133)\nand projecting (133) onto the hyperplaneHb∗ with normal b∗, we obtain\nπHb∗ D−1∑ j=1 αjξj + c n∑ i=1 NiπHb∗ ( ĥi,b∗ ) + n∑ i=1 Ni πHb∗ ( ηi,b∗ ) = 0. (134)\nLet us analyze the term πHb∗ ( ĥi,b∗ ) . We have\nπHb∗ ( ĥi,b∗ ) = πHb∗ ( πHi(b ∗)\n‖πHi(b∗)‖2\n) = πHb∗ ( b∗ − ( b>i b ∗) bi∥∥b∗ − (b>i b∗) bi∥∥2 )\n(135)\n= πHb∗\n( b∗ − ( b>i b ∗) bi sin(θi) ) = b∗ − ( b>i b ∗) bi sin(θi) − ( 1− cos2(θi) sin(θi) ) b∗ (136)\n=\n( b>i b ∗) ((b>i b∗) b∗ − bi) sin(θi) = − ( b>i b ∗ ) ζ̂i, ζi = πHb∗ (bi). (137)\nUsing (137), (134) becomes\nπHb∗ D−1∑ j=1 αjξj − n∑ i=1 Ni c ( b>i b ∗ ) ζ̂i + n∑ i=1 Ni πHb∗ ( ηi,b∗ ) = 0. (138)\nIsolating the term that depends on i = 1 to the LHS and moving everything else to the RHS, and taking norms, we get\ncN1 cos(θ1) ≤ n∑ i>1 cNi cos(θi)+\n+ ∥∥∥∥∥∥πHb∗  K∑ j=1 αjξj ∥∥∥∥∥∥ 2 + n∑ i=1 Ni ∥∥πHb∗ (ηi,b∗)∥∥2 . (139)\nSince ∥∥ηi,b∗∥∥2 ≤ i, we have that ∥∥πHb∗ (ηi,b∗)∥∥2 ≤ i. Next, the quantity ∑Kj=1 αjξj can be decomposed along the index i, based on the hyperplane membership of the ξj . For instance, if ξ1 ∈ H1, then replace the term α1ξ1 with α (1) 1 ξ (1) 1 , where the superscript ·(1) denotes association to hyperplaneH1. Repeating this for all ξj and after a possible re-indexing, we have\nD−1∑ j=1 αjξj = n∑ i=1 Ki∑ j=1 α (i) j ξ (i) j . (140)\nNow, by Definition 5 we have that ∥∥∥∥∥∥ Ki∑ j=1 α (i) j ξ (i) j ∥∥∥∥∥∥ 2 ≤ Ri,Ki , (141)\nand as a consequence, the upper bound (139) can be extended to\ncN1 cos(θ1) ≤ n∑ i>1 cNi cos(θi) + ∑ i iNi +R. (142)\nFinally, Lemma 12 provides a bound n∑ i>1 Ni cos(θi) ≤ β, (143)\nwhere β is as in Theorem 4. In turn, this can be used to extend (142) to\ncos(θ1) ≤ β + c−1 (R+\n∑ iNi)\nN1 =: cos\n( θ (1) min ) . (144)\nNote that the angle θ(1)min of (144) is well-defined, since by hypothesis N1 > β̄, and that what (144) effectively says, is that θ1 never drops below θ (1) min. It is then straightforward to check that hypothesis\nN1 > √ ᾱ2 + β̄2 implies θ(1)min > θ (1) max, which is a contradiction. In other words, b∗ must be equal up to sign to one of the bi, which proves the first part of the Theorem. The second part follows from noting that condition γ̄ > 0 guarantees that J (b1) < mini>1 J (bi)."
    }, {
      "heading" : "4.7 Proof of Theorem 7",
      "text" : "First of all, it follows from the theory of the simplex method, that ifnk+1 is obtained via the simplex method, then it will satisfy the conclusion of Lemma 15 in Appendix A. Then Lemma 16 guarantees that {nk} converges to a critical point of problem (9) in a finite number of steps; denote that point by n∞. In other words, n∞ will satisfy equation (111) and it will have unit `2 norm. Now, if n∞ = ±bj for some j > 1, then\nJ (n̂0) ≥ J (bj), (145)\nor equivalently ∑ Nin̂ > 0 χi,n̂0 ≥ ∑ i 6=j Nib > j χi,bj . (146)\nSubstituting the concentration model\nχi,n̂0 = c ̂πHi (n0) + ηi,0, ∥∥ηi,0∥∥2 ≤ i, (147) χi,bj = c ̂πHi (bj) + ηij , ∥∥ηij∥∥2 ≤ i, (148)\ninto (146), we get∑ Nic sin(θi,0) + ∑ Nin̂ > 0 ηi,0 ≥ ∑ i 6=j Nic sin(θij) + ∑ Nib > j ηij . (149)\nBounding the LHS of (149) from above and the RHS from below, we get∑ Ni c sin(θi,0) + ∑ iNi ≥ ∑ i 6=j Ni c sin(θij)− ∑ iNi. (150)\nBut this very last relation is contradicted by hypothesis N1 > µ, i.e., none of the ±bj for j > 1 can be n∞. We will show that n∞ has to be ±b1. So suppose for the sake of a contradiction that that n∞ is not colinear with b1, i.e., n∞ 6⊥ Hi, ∀i ∈ [n]. Since n∞ satisfies (111), we can use part of the proof of Theorem 6, according to which the principal angle θ1,∞ of n∞ from b1 does not become less than θ(1)min, where θ (1) min is as in (144). Consequently, and using once again the concentration model, we obtain∑ Ni c sin(θi,0) + ∑ iNi ≥ J (n̂0) ≥ J (n∞) ≥ ∑ Ni c sin(θi,∞)− ∑ iNi\n≥ N1 c sin ( θ (1) min ) + c √∑ i>1 N2i − σmax ( [NiNj cos(θij)]i,j>1 ) − ∑ iNi. (151)\nNow, a little algebra reveals that the outermost inequality in (151) contradicts (38)."
    }, {
      "heading" : "5. Algorithmic Contributions",
      "text" : "There are at least two ways in which DPCP can be used to learn a hyperplane arrangement; either through a sequential (RANSAC-style) scheme, or through an iterative (K-Subspaces-style) scheme. These two cases are described next.\nAlgorithm 1 Sequential Hyperplane Learning via DPCP 1: procedure SHL-DPCP(X = [x1, x2, . . . , xN ] ∈ RD×N , n) 2: i← 0; 3: wj ← 1, j = 1, . . . , N ; 4: for i = 1 : n do 5: Y ← [w1x1 · · · wNxN ]; 6: bi ← argminb∈RD [∥∥Y>b∥∥ 1 , s.t. b>b = 1 ] ;\n7: wj ← mink=1,...,i ∣∣b>k xj∣∣ , j = 1, . . . , N ; 8: end for 9: Ci ← { xj ∈ X : i = argmink=1,...,n\n∣∣b>k xj∣∣} , i = 1, . . . , n; 10: return {(bi,Ci)}ni=1; 11: end procedure"
    }, {
      "heading" : "5.1 Sequential hyperplane learning via DPCP",
      "text" : "Since at its core DPCP is a single subspace learning method, we may as well use it to learn n hyperplanes in the same way that RANSAC (Fischler and Bolles, 1981) is used: learn one hyperplane from the entire dataset, remove the points close to it, then learn a second hyperplane and so on. The main weakness of this technique is well known, and consists of its sensitivity to the thresholding parameter, which is necessary in order to remove points. To alleviate the need of knowing a good threshold, we propose to replace the process of removing points by a process of appropriately weighting the points. In particular, suppose we solve the DPCP problem (9) on the entire dataset X and obtain a unit `2-norm vector b1. Now, instead of removing the points of X that are close to the hyperplane with normal vector b1 (which would require a threshold parameter), we weight each and every point xj of X by its distance\n∣∣b>1 xj∣∣ from that hyperplane. Then to compute a second hyperplane with normal b2 we apply DPCP on the weighted dataset\n{∣∣b>1 xj∣∣xj}. To compute a third hyperplane, the weight of point xj is chosen as the smallest distance of xj from the already computed two hyperplanes, i.e., DPCP is now applied to { mini=1,2\n∣∣b>i xj∣∣xj}. After n hyperplanes have been computed, the clustering of the points is obtained based on their distances to the n hypeprlanes; see Algorithm 1."
    }, {
      "heading" : "5.2 Iterative hyperplane learning via DPCP",
      "text" : "Another way to do hyperplane clustering via DPCP, is to modify the classic K-Subspaces (Bradley and Mangasarian, 2000; Tseng, 2000; Zhang et al., 2009) by computing the normal vector of each cluster by DPCP. We call the resulting method IHL-DPCP; see Algorithm 2. It is worth noting that since DPCP minimizes the `1-norm of the distances of the points to a hyperplane, consistency dictates that the stopping criterion for IHL-DPCP be governed by the sum over all points of the distance of each point to its assigned hyperplane (instead of the traditional sum of squares (Bradley and Mangasarian, 2000; Tseng, 2000)); in other words the global objective function minimized by IHL-DPCP is the same as that of Median K-Flats (Zhang et al., 2009).\nAlgorithm 2 Iterative Hyperplane Learning via Dual Principal Component Pursuit 1: procedure IHL-DPCP(X = [x1, x2, . . . , xN ] , b1, . . . , bn, ε, Tmax) 2: Jold ←∞, ∆J ← ∞, t← 0; 3: while t < Tmax and ∆J > ε do 4: Jnew ← 0, t = t+ 1; 5: Ci ← { xj ∈ X : i = argmink=1,...,n\n∣∣b>k xj∣∣} , i = 1, . . . , n; 6: Jnew = ∑n i=1 ∑ xj∈Ci\n∣∣b>i xj∣∣; 7: ∆J ← (Jold − Jnew)/(Jold + 10−9), Jold ← Jnew ; 8: bi ← argminb\n[∥∥C>i b∥∥1 , s.t. b>b = 1] , i = 1, . . . , n; 9: end while\n10: return {(bi,Ci)}ni=1; 11: end procedure\nAlgorithm 3 Relaxed Dual Principal Component Pursuit 1: procedure DPCP-r(X , ε, Tmax) 2: k ← 0; ∆J ← ∞; 3: n̂0 ← argmin‖b‖2=1 ∥∥X>b∥∥ 2 ;\n4: while k < Tmax and ∆J > ε do 5: k ← k + 1; 6: nk ← argminb>n̂k−1=1 ∥∥X>b∥∥ 1 ;\n7: ∆J ← (∥∥X>n̂k−1∥∥1 − ∥∥X>n̂k∥∥1) / (∥∥X>n̂k−1∥∥1 + 10−9); 8: end while 9: return n̂k;\n10: end procedure\nAlgorithm 4 Dual Principal Component Pursuit via Iteratively Reweighted Least Squares 1: procedure DPCP-IRLS(X , c, ε, Tmax, δ) 2: k ← 0; ∆J ← ∞; 3: B0 ← argminB∈RD×c ∥∥X>B∥∥ 2 , s.t. B>B = Ic;\n4: while k < Tmax and ∆J > ε do 5: k ← k + 1; 6: wx ← 1/max { δ, ∥∥B>k−1x∥∥2} , x ∈ X ;\n7: Bk ← argminB∈RD×c ∑ x∈X wx ∥∥B>x∥∥2 2 s.t. B>B = Ic;\n8: ∆J ← (∥∥X>Bk−1∥∥1 − ∥∥X>Bk∥∥1) / (∥∥X>Bk−1∥∥1 + 10−9);\n9: end while 10: returnBk; 11: end procedure"
    }, {
      "heading" : "5.3 Solving the DPCP problem",
      "text" : "Recall that the DPCP problem (9) that appears in Algorithms 1 and 2 (with data matrices Y andCi, respectively) is non-convex. In Tsakiris and Vidal (2017b) we described four distinct methods for solving it, which we briefly review here.\nAlgorithm 5 Denoised Dual Principal Component Pursuit 1: procedure DPCP-d(X , ε, Tmax, δ, τ ) 2: Compute a Cholesky factorization LL> = XX> + δID; 3: k ← 0; ∆J ← ∞; 4: b← argminb∈RD: ‖b‖2=1 ∥∥X>b∥∥ 2 ;\n5: J0 ← τ ∥∥X>b∥∥\n1 ;\n6: while k < Tmax and ∆J > ε do 7: k ← k + 1; 8: y ← Sτ ( X>b ) ;\n9: b← solution of LL>ξ = Xy by backward/forward propagation; 10: b← b/ ‖b‖2; 11: Jk ← τ ‖y‖1 + 1 2 ∥∥y −X>b∥∥2 2 ;\n12: ∆J ← (Jk−1 − Jk) / ( Jk−1 + 10−9 ) ; 13: end while 14: return (y, b); 15: end procedure\nThe first method, which was first proposed in Späth and Watson (1987), consists of solving the recursion of linear programs (11) using any standard solver, such as Gurobi (Gurobi Optimization, 2015); we refer to such a method as DPCP-r, standing for relaxed DPCP (see Algorithm 3). A second approach, called DPCP-IRLS, is to solve (9) using a standard Iteratively Reweighted LeastSquares (IRLS) technique ((Candès et al., 2008; Daubechies et al., 2010; Chartrand and Yin, 2008)) as in Algorithm 4. A third method, first proposed in Qu et al. (2014), is to solve (9) approximately by applying alternative minimization on its denoised version\nmin b,y: ||b||2=1\n[ τ ‖y‖1 + 1\n2 ∥∥∥y −X>b∥∥∥2 2 ] . (152)\nWe refer to such a method as DPCP-d, standing for denoised DPCP; see Algorithm 5. Finally, the fourth method is relaxed and denoised DPCP (DPCP-r-d), which replaces each problem of recursion (11) with its denoised version\nmin y,b\n[ τ ‖y‖1 + 1\n2 ∥∥∥y −X>b∥∥∥2 2 ] , s.t. b>n̂k−1 = 1; (153)\nwhich is in turn solved via alternating minimization; see Tsakiris and Vidal (2017b) for details."
    }, {
      "heading" : "6. Experimental evaluation",
      "text" : "In this section we evaluate experimentally Algorithms 1 and 2 using both synthetic (§6.1) and real data (§6.2)."
    }, {
      "heading" : "6.1 Synthetic data",
      "text" : "Dataset design. We begin by evaluating experimentally the sequential hyperplane learning Algorithm 1 using synthetic data. The coordinate dimension D of the data is inspired by major applications where hyperplane arrangements appear. In particular, we recall that\n• In 3D point cloud analysis, the coordinate dimension is 3, but since the various planes do not necessarily pass through a common origin, i.e., they are affine, one may work with homo-\ngeneous coordinates, which increases the coordinate dimension of the data by 1 (see ?), i.e., D = 4.\n• In two-view geometry one works with correspondences between pairs of 3D points. Each such correspondence is treated as a point itself, equal to the tensor product of the two 3D corresponding points, thus having coordinate dimension D = 9.\nAs a consequence, we choose D = 4, 9 as well as D = 30, where the choice of 30 is a-posteriori justified as being sufficiently larger than 4 or 9, so that the clustering problem becomes more challenging. For each choice of D we randomly generate n = 2, 3, 4 hyperplanes of RD and sample each hyperplane as follows. For each choice of n the total number of points in the dataset is set to 300n, and the number Ni of points sampled from hyperplane i > 1 is set to Ni = αi−1Ni−1, so that\nn∑ i=1 Ni = (1 + α+ · · ·+ αn−1)N1 = 300n. (154)\nHere α ∈ (0, 1] is a parameter that controls the balancing of the clusters: α = 1 means the clusters are perfectly balanced, while smaller values of α lead to less balanced clusters. In our experiment we try α = 1, 0.8, 0.6. Having specified the size of each cluster, the points of each cluster are sampled from a zero-mean unit-variance Gaussian distribution with support in the corresponding hyperplane. To make the experiment more realistic, we corrupt points from each hyperplane by adding white Gaussian noise of standard deviation σ = 0.01 with support in the direction orthogonal\nto the hyperplane. Moreover, we corrupt the dataset by adding 10% outliers sampled from a standard zero-mean unit-variance Gaussian distribution with support in the entire ambient space.\nAlgorithms and parameters. In Algorithm 1 we solve the DPCP problem by using all four variations DPCP-r, DPCP-r-d, DPCP-d and DPCP-IRLS (see Section 5.3), thus leading to four\ndifferent versions of the algorithm. All DPCP algorithms are set to terminate if either a maximal number of 20 iterations for DPCP-r or 100 iterations for DPCP-r-d,DPDP-d, DPCP-IRLS is reached, or if the algorithm converges within accuracy of 10−3. We also compare with the REAPER analog of Algorithm 1, where the computation of each normal vector is done by the IRLS version of REAPER (see Section 2) instead of DPCP. As with the DPCP algorithms, its maximal number of iterations is 100 and its convergence accuracy is 10−3.\nFinally, we compare with RANSAC, which is the predominant method for clustering hyperplanes in low ambient dimensions (e.g., for D = 4, 9). For fairness, we implement a version of RANSAC which involves the same weighting scheme as Algorithm 1, but instead of weighting the points, it uses the normalized weights as a discrete probability distribution on the data points; thus points that lie close to some of the already computed hyperplanes, have a low probability of being selected. Moreover, we control the running time of RANSAC so that it is as slow as DPCP-r, the latter being the slowest among the four DPCP algorithms.\nResults. Since not all results can fit in a single figure, we show the mean clustering accuracy over 50 independent experiments in Fig. 1 only for RANSAC, REAPER, DPCP-r and DPDP-IRLS (i.e., not including DPCP-r-d and DPCP-d), but for all values α = 1, 0.8, 0.6, as well as in Fig. 2 for all methods but only for α = 0.8. The accuracy is normalized to range from 0 to 1, with 0 corresponding to black color, and 1 corresponding to white.\nFirst, observe that the performance of almost all methods improves as the clusters become more unbalanced (α = 1→ α = 0.6). This is intuitively expected, as the smaller α is the more dominant is the i-th hyperplane with respect to hyperplanes i + 1, . . . , n, and so the more likely it is to be identified at iteration i of the sequential algorithm. The only exception to this intuitive phenomenon\nis RANSAC, which appears to be insensitive to the balancing of the data. This is because RANSAC is configured to run a relatively long amount of time, approximately equal to the running time of DPCP-r, and as it turns out this compensates for the unbalancing of the data, since many different samplings take place, thus leading to approximately constant behavior across different α.\nIn fact, notice that RANSAC is the best among all methods when D = 4, with mean clustering accuracy ranging from 99% when n = 2, to 97% when n = 4. On the other hand, RANSAC’s performance drops dramatically when we move to higher coordinate dimensions and more than 2 hyperplanes. For example, for α = 0.8 and n = 4, the mean clustering accuracy of RANSAC drops from 97% for D = 4, to 44% for D = 9, to 28% for D = 30. This is due to the fact that the probability of sampling D − 1 points from the same hyperplane decreases as D increases.\nSecondly, the proposed Algorithm 1 using DPCP-r is uniformly the best method (and using DPCP-IRLS is the second best), with the slight exception of D = 4, where its clustering accuracy ranges for α = 0.8 from 99% for n = 2 (same as RANSAC), to 89% for n = 4, as opposed to the 97% of RANSAC for the latter case. In fact, all DPCP variants were superior than RANSAC or REAPER in the challenging scenario of D = 30, n = 4, where for α = 0.6, DPCP-r, DPCP-IRLS, DPCP-d and DPCP-r-d gave 86%, 81%, 74% and 52% accuracy respectively, as opposed to 28% for RANSAC and 42% for REAPER.\nTable 1 reports running times in seconds for α = 1 and n = 2, 4. It is readily seen that DPCPr is the slowest among all methods (recall that RANSAC has been configured to be as slow as DPCP-r). Remarkably, DPCP-d and REAPER are the fastest among all methods with a difference of approximately two orders of magnitude from DPCP-r. However, as we saw above, none of these methods performs nearly as well as DPCP-r. From that perspective, DPCP-IRLS is interesting, since it seems to be striking a balance between running time and performance.\nMoving on, we fix D = 9 and vary the outlier ratio as 10%, 30%, 50% (in the previous experiment the outlier ratio was 10%). Then the mean clustering accuracy over 50 independent trials is shown in Fig. 3 and Fig. 4. In this experiment the hierarchy of the methods is clear: Algorithm 1 using DPCP-r and using DPCP-IRLS are the best and second best methods, respectively, while the rest of the methods perform equally poorly. As an example, in the challenging scenario of n = 4, D = 30 and 50% outliers, for α = 0.6, DPCP-r gives 74% accuracy, while the next best method is DPCP-IRLS with 58% accuracy; in that scenario RANSAC and REAPER give 38% and 41% accuracy respectively, while DPCP-r-d and DPCP-d give 41% and 40% respectively. Moreover, for n = 2, D = 30 and α = 0.8 DPCP-r and DPCP-IRLS give 95% and 86% accuracy, while all other methods give about 65%."
    }, {
      "heading" : "6.2 3D Plane clustering of real Kinect data",
      "text" : "Dataset and objective. In this section we explore various Iterative Hyperplane Clustering10 (IHL) algorithms using the benchmark dataset NYUdepthV2 Silberman et al. (2012). This dataset consists of 1449 RGBd data instances acquired using the Microsoft kinect sensor. Each instance of NYUdepthV2 corresponds to an indoor scene, and consists of the 480× 640× 3 RGB data together with depth data for each pixel, i.e., a total of 480 · 640 depth values. In turn, the depth data can be used to reconstruct a 3D point cloud associated to the scene. In this experiment we use such 3D point clouds to learn plane arrangements and segment the pixels of the corresponding RGB images based on their plane membership. This is an important problem in robotics, where estimating the geometry of a scene is essential for successful robot navigation.\nManual annotation. While the coarse geometry of most indoor scenes can be approximately described by a union of a few (≤ 9) planes, many points in the scene do not lie in these planes, and may thus be viewed as outliers. Moreover, it is not always clear how many planes one should select or which these planes are. In fact, NYUdepthV2 does not contain any ground truth annotation based on planes, rather the scenes are annotated semantically with a view to object recognition. For this reason, among a total of 1449 scenes, we manually annotated 92 scenes, in which the dominant planes are well-defined and capture most of the scene; see for example Figs. 7(a)-7(b) and 5(a)-5(b). Specifically, for each of the 92 images, at most 9 dominant planar regions were manually marked in the image and the set of pixels within these regions were declared inliers, while the remaining pixels were declared outliers. For each planar region a ground truth normal vector was computed using DPCP-r. Finally, two planar regions that were considered distinct during manual annotation, were merged if the absolute inner product of their corresponding normal vectors was above 0.999.\nPre-processing. For computational reasons, the hyperplane clustering algorithms that we use (to be described in the next paragraph) do not act directly on the original 3D point cloud, rather on a weighted subset of it, corresponding to a superpixel representation of each image. In particular, each 480 × 640 × 3 RGB image is segmented to about 1000 superpixels and the entire 3D point sub-cloud corresponding to each superpixel is replaced by the point in the geometric center of the superpixel. To account for the fact that the planes associated with an indoor scene are affine, i.e., they do not pass through a common origin, we work in homogeneous coordinates, i.e., we append a fourth coordinate to each 3D point representing a superpixel and normalize it to unit `2-norm. Finally, a weight is assigned to each representative 3D point, equal to the number of pixels in the underlying superpixel. The role of this weight is to regulate the influence of each point in the modeling error (points representing larger superpixels should have more influence).\nAlgorithms. The first algorithm that we test is the sequential RANSAC algorithm (SHLRANSAC), which identifies one plane at a time. Secondly, we explore a family of variations of the IHL algorithm (see §2) based on SVD, DPCP, REAPER and RANSAC. In particular, IHL(2)SVD indicates the classic IHL algorithm which computes normal vectors through the Singular Value Decomposition (SVD), and minimizes an `2 objective (this is K-Hyperplanes). IHL(1)DPCP-r-d, IHL(1)-DPCP-d and IHL(1)-DPCP-IRLS, denote IHL variations of DPCP according to Algorithm 2, depending on which method is used to solve the DPCP problem (9) 11. Similarly,\n10. Recall from §2 and §5.2 that by iterative hyperplane clustering, we mean the process of estimating n hyperplanes, then assigning each point to its closest hyperplane, then refining the hyperplanes associated to a cluster only from the points of the cluster, re-assigning points to hyperplanes and so on. 11. IHL(1)-DPCP-r was not included since it was slowing down the experiment considerably, while its performance was similar to the rest of the DPCP methods.\nIHL(1)-REAPER and IHL(1)-RANSAC denote the obvious adaptation of IHL where the normals are computed with REAPER and RANASC, respectively, and an `1 objective is minimized.\nA third method that we explore is a hybrid between Algebraic Subspace Clustering (ASC), RANSAC and IHL, (IHL-ASC-RANSAC). First, the vanishing polynomial associated to ASC (see §2) is computed with RANSAC instead of SVD, which is the traditional way; this ensures robustness to outliers. Then spectral clustering applied on the angle-based affinity associated to ASC (see equation (4)) yields n clusters. Finally, one iteration of IHL-RANSAC refines these clusters and yields a normal vector per cluster (the normal vectors are necessary for the post-processing step).\nPost-processing. The algorithms described above, are generic hyperplane clustering algorithms. On the other hand, we know that nearby points in a 3D point cloud have a high chance of lying in the same plane, simply because indoor scenes are spatially coherent. Thus to associate a spatially smooth image segmentation to each algorithm, we use the normal vectors b1, . . . , bn that the algorithm produced to minimize a Conditional-Random-Field (Sutton and McCallum, 2006) type of energy function, given by\nE(y1, . . . , yN ) := N∑ j=1 d(byj ,xj) + λ ∑ k∈Nj w(xj ,xk)δ(yj 6= yk). (155)\nIn (155) yj ∈ {1, . . . , n} is the plane label of point xj , d(byj ,xj) is a unary term that measures the cost of assigning 3D point xj to the plane with normal byj , w(xj ,xk) is a pairwise term that measures the similarity between points xj and xk, λ > 0 is a chosen parameter, Nj indexes the neighbors of xj , and δ(·) is the indicator function. The unary term is defined as d(byj ,xj) = |b>yjxj |, which is the Euclidean distance from pointxj to the plane with normal byj , and the pairwise\nterm is defined as\nw(xj ,xk) := CBj,k exp ( −‖xj − xk‖ 2 2\n2σ2d\n) , (156)\nwhere ‖xj − xk‖2 is the Euclidean distance from xj to xk, and CBj,k is the length of the common boundary between superpixels j and k. The minimization of the energy function is done via GraphCuts (Boykov et al., 2001).\nParameters. For the thresholding parameter of SHL-RANSAC, denoted by τ , we test the values 0.1, 0.01, 0.001. For the parameter τ of IHL(1)-DPCP-d and IHL(1)-DPCP-r-d we test the values\n0.1, 0.01, 0.001. We also use the same values for the thresholding parameter of SHL-RANSAC, which we also denote by τ . The rest of the parameters of DPCP and REAPER are set as in Section 6.1. The convergence accuracy of the IHL algorithms is set to 10−3. Moreover, the IHL algorithms are configured to allow for a maximal number of 10 random restarts and 100 iterations per restart,\nbut the overall running time of each IHL algorithm should not exceed 5 seconds; this latter constraint is also enforced to SHL-RANSAC and IHL-ASC-RANSAC.\nThe parameter σd in (155) is set to the mean distance between 3D points representing neighboring superpixels. The parameter λ in (155) is set to the inverse of twice the maximal row-sum of the pairwise matrix {w(xj ,xk)}; this is to achieve a balance between unary and pairwise terms.\nEvaluation. Recall that none of the algorithms considered in this section is explicitly configured to detect outliers, rather it assigns each and every point to some plane. Thus we compute the clustering error as follows. First, we restrict the output labels of each algorithm to the indices of the dominant ground-truth cluster, and measure how far are these restricted labels from being identical (identical labels would signify that the algorithm identified perfectly well the plane); this is done by computing the ratio of the restricted labels that are different from the dominant label. Then the dominant label is disabled and a similar error is computed for the second dominant ground-truth plane, and so on. Finally the clustering error is taken to be the weighted sum of the errors associated with each dominant plane, with the weights proportional to the size of the ground-truth cluster.\nWe evaluate the algorithms in several different settings. First, we test how well the algorithms can cluster the data into the first n dominant planes, where n is 2, 4 or equal to the total number of annotated planes for each scene. Second, we report the clustering error before spatial smoothing, i.e., without refining the clustering by minimizing (155), and after spatial smoothing. The former case is denoted by GC(0), indicating that no graph-cuts takes place, while the latter is indicated by GC(1). Finally, to account for the randomness in RANSAC as well as the random initialization of IHL, we average the clustering errors over 10 independent experiments.\nResults. The results are reported in Table 2, where the clustering error of the methods that depend on τ is shown for each value of τ individually, as well as averaged over all three values.\nNotice that spatial smoothing improves the clustering accuracy considerably (GC(0) vs GC(1)); e.g., the clustering error of the traditional IHL(2)-SVD for all ground-truth planes drops from 26.22% to 16.71%, when spatial smoothing is employed. Moreover, as it is intuitively expected, the clustering error increases when fitting more planes (larger n) is required; e.g., for the GC(1) case, the error of IHL(2)-SVD increases from 9.96% for n = 2 to 16.71% for all planes (n ≈ 9).\nNext, we note the remarkable insensitivity of the DPCP-based methods IHL(1)-DPCP-d and IHL(1)-DPCP-r-d to variations of the parameter τ . In sharp contrast, SHL-RANSAC is very sensitive to τ ; e.g., for τ = 0.01 and n = 2, SHL-RANSAC is the best method with 6.27%, while for τ = 0.1, 0.001 its error increases to 16.01% and 15.26% respectively. Interestingly, the hybrid IHL(1)-RANSAC is significantly more robust; in fact, in terms of clustering error it is the best method. On the other hand, by looking at the lower part of Table 2, we conclude that on average the rest of the methods have very similar behavior.\nFigs. 5-8 show some segmentation results for two scenes, with and without spatial smoothing. It is remarkable that, even though the segmentation in Fig. 5 contains artifacts, which are expected due to the lack of spatial smoothing, its quality is actually very good, in that most of the dominant planes have been correctly identified. Indeed, applying spatial smoothing (Fig. 6) further drops the error for most methods only by about 1%."
    }, {
      "heading" : "7. Conclusions",
      "text" : "We studied theoretically and algorithmically the application of the recently proposed single subspace learning method Dual Principal Component Pursuit (DPCP) to the problem of clustering\ndata that lie close to a union of hyperplanes. We gave theoretical conditions under which the nonconvex cosparse problem associated with DPCP admits a unique (up to sign) global solution equal to the normal vector of the underlying dominant hyperplane. We proposed sequential and parallel hyperplane clustering methods, which on synthetic data dramatically improved upon state-of-the-art methods such as RANSAC or REAPER, while were competitive to the latter in the case of learning unions of 3D planes from real Kinect data. Future research directions include analysis in the presence of noise, generalizations to unions of subspaces of arbitrary dimensions, even more scalable algorithms, and applications to deep networks."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work was supported by grant NSF 1447822. The authors thank Prof. Daniel P. Robinson of the Applied Mathematics and Statistics department of the Johns Hopkins University for many useful conversations, as well as for his many comments that helped improve this manuscript.\nAppendix A. Results on Problems (9) and (11) following Späth and Watson (1987)\nIn this Section we state three results that are important for our mathematical analysis, already known in Späth and Watson (1987); detailed proofs can be found in Tsakiris and Vidal (2017b). Let Y be a D ×N matrix of full rank D. Then we have the following.\nLemma 14 Any global solution b∗ to minb>b=1 ∥∥Y>b∥∥ 1 , must be orthogonal to (D − 1) linearly independent points of Y .\nLemma 15 Problem minb>n̂k=1 ∥∥Y>b∥∥ 1 admits a computable solution nk+1 that is orthogonal to (D − 1) linearly independent points of Y .\nLemma 16 Suppose that for each problem minb>n̂k=1 ∥∥Y>b∥∥ 1 , a solution nk+1 is chosen such that nk+1 is orthogonal to D − 1 linearly independent points of Y , in accordance with Lemma 15. Then the sequence {nk} converges to a critical point of problem minb>b=1 ∥∥Y>b∥∥ 1\nin a finite number of steps."
    } ],
    "references" : [ {
      "title" : "Identification of switched linear systems via sparse optimization",
      "author" : [ "L. Bako" ],
      "venue" : null,
      "citeRegEx" : "Bako.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bako.",
      "year" : 2011
    }, {
      "title" : "Factorization-based segmentation of motions",
      "author" : [ "T.E. Boult", "L.G. Brown" ],
      "venue" : "In IEEE Workshop on Motion Understanding,",
      "citeRegEx" : "Boult and Brown.,? \\Q1991\\E",
      "shortCiteRegEx" : "Boult and Brown.",
      "year" : 1991
    }, {
      "title" : "Fast approximate energy minimization via graph cuts",
      "author" : [ "Y. Boykov", "O. Veksler", "R. Zabih" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Boykov et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Boykov et al\\.",
      "year" : 2001
    }, {
      "title" : "k-plane clustering",
      "author" : [ "P.S. Bradley", "O.L. Mangasarian" ],
      "venue" : "Journal of Global Optimization,",
      "citeRegEx" : "Bradley and Mangasarian.,? \\Q2000\\E",
      "shortCiteRegEx" : "Bradley and Mangasarian.",
      "year" : 2000
    }, {
      "title" : "Enhancing sparsity by reweighted `1 minimization",
      "author" : [ "E. Candès", "M. Wakin", "S. Boyd" ],
      "venue" : "Journal of Fourier Analysis and Applications,",
      "citeRegEx" : "Candès et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Candès et al\\.",
      "year" : 2008
    }, {
      "title" : "Iteratively reweighted algorithms for compressive sensing",
      "author" : [ "R. Chartrand", "W. Yin" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "Chartrand and Yin.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chartrand and Yin.",
      "year" : 2008
    }, {
      "title" : "Spectral curvature clustering (SCC)",
      "author" : [ "G. Chen", "G. Lerman" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Chen and Lerman.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chen and Lerman.",
      "year" : 2009
    }, {
      "title" : "A multibody factorization method for independently moving objects",
      "author" : [ "J. Costeira", "T. Kanade" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Costeira and Kanade.,? \\Q1998\\E",
      "shortCiteRegEx" : "Costeira and Kanade.",
      "year" : 1998
    }, {
      "title" : "Ideals, Varieties, and Algorithms",
      "author" : [ "D.A. Cox", "J. Little", "D. O’Shea" ],
      "venue" : null,
      "citeRegEx" : "Cox et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cox et al\\.",
      "year" : 2007
    }, {
      "title" : "Iteratively reweighted least squares minimization for sparse recovery",
      "author" : [ "I. Daubechies", "R. DeVore", "M. Fornasier", "C.S. Güntürk" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "Daubechies et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Daubechies et al\\.",
      "year" : 2010
    }, {
      "title" : "Hilbert series of subspace arrangements",
      "author" : [ "H. Derksen" ],
      "venue" : "Journal of Pure and Applied Algebra,",
      "citeRegEx" : "Derksen.,? \\Q2007\\E",
      "shortCiteRegEx" : "Derksen.",
      "year" : 2007
    }, {
      "title" : "A flag representation for finite collections of subspaces of mixed dimensions",
      "author" : [ "B. Draper", "M. Kirby", "J. Marks", "T. Marrinan", "C. Peterson" ],
      "venue" : "Linear Algebra and its Applications,",
      "citeRegEx" : "Draper et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Draper et al\\.",
      "year" : 2014
    }, {
      "title" : "Sparse subspace clustering",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Elhamifar and Vidal.,? \\Q2009\\E",
      "shortCiteRegEx" : "Elhamifar and Vidal.",
      "year" : 2009
    }, {
      "title" : "Clustering disjoint subspaces via sparse representation",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Elhamifar and Vidal.,? \\Q2010\\E",
      "shortCiteRegEx" : "Elhamifar and Vidal.",
      "year" : 2010
    }, {
      "title" : "Sparse subspace clustering: Algorithm, theory, and applications",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Elhamifar and Vidal.,? \\Q2013\\E",
      "shortCiteRegEx" : "Elhamifar and Vidal.",
      "year" : 2013
    }, {
      "title" : "A closed form solution to robust subspace estimation and clustering",
      "author" : [ "P. Favaro", "R. Vidal", "A. Ravichandran" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Favaro et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Favaro et al\\.",
      "year" : 2011
    }, {
      "title" : "RANSAC random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography",
      "author" : [ "M.A. Fischler", "R.C. Bolles" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Fischler and Bolles.,? \\Q1981\\E",
      "shortCiteRegEx" : "Fischler and Bolles.",
      "year" : 1981
    }, {
      "title" : "The fermat point of a spherical triangle",
      "author" : [ "K. Ghalieh", "M. Hajja" ],
      "venue" : "The Mathematical Gazette,",
      "citeRegEx" : "Ghalieh and Hajja.,? \\Q1996\\E",
      "shortCiteRegEx" : "Ghalieh and Hajja.",
      "year" : 1996
    }, {
      "title" : "Spherical designs, discrepancy and numerical integration",
      "author" : [ "P.J. Grabner", "R.F. Tichy" ],
      "venue" : "Math. Comp.,",
      "citeRegEx" : "Grabner and Tichy.,? \\Q1993\\E",
      "shortCiteRegEx" : "Grabner and Tichy.",
      "year" : 1993
    }, {
      "title" : "Discrepancies of point sequences on the sphere and numerical integration",
      "author" : [ "P.J. Grabner", "B. Klinger", "R.F. Tichy" ],
      "venue" : "Mathematical Research,",
      "citeRegEx" : "Grabner et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Grabner et al\\.",
      "year" : 1997
    }, {
      "title" : "Multibody factorization with uncertainty and missing data using the EM algorithm",
      "author" : [ "A. Gruber", "Y. Weiss" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Gruber and Weiss.,? \\Q2004\\E",
      "shortCiteRegEx" : "Gruber and Weiss.",
      "year" : 2004
    }, {
      "title" : "Analysis of a complex of statistical variables into principal components",
      "author" : [ "H. Hotelling" ],
      "venue" : "Journal of Educational Psychology,",
      "citeRegEx" : "Hotelling.,? \\Q1933\\E",
      "shortCiteRegEx" : "Hotelling.",
      "year" : 1933
    }, {
      "title" : "Principal Component Analysis",
      "author" : [ "I. Jolliffe" ],
      "venue" : "Springer-Verlag, 2nd edition,",
      "citeRegEx" : "Jolliffe.,? \\Q2002\\E",
      "shortCiteRegEx" : "Jolliffe.",
      "year" : 2002
    }, {
      "title" : "Motion segmentation by subspace separation and model selection",
      "author" : [ "K. Kanatani" ],
      "venue" : "In IEEE International Conference on Computer Vision,",
      "citeRegEx" : "Kanatani.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kanatani.",
      "year" : 2001
    }, {
      "title" : "`p-recovery of the most significant subspace among multiple subspaces with outliers",
      "author" : [ "G. Lerman", "T. Zhang" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "Lerman and Zhang.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lerman and Zhang.",
      "year" : 2014
    }, {
      "title" : "Robust computation of linear models by convex relaxation",
      "author" : [ "G. Lerman", "M.B. McCoy", "J.A. Tropp", "T. Zhang" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Lerman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lerman et al\\.",
      "year" : 2015
    }, {
      "title" : "Robust subspace segmentation by low-rank representation",
      "author" : [ "G. Liu", "Z. Lin", "Y. Yu" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Liu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2010
    }, {
      "title" : "Robust recovery of subspace structures by low-rank representation",
      "author" : [ "G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Ma" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Liu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2013
    }, {
      "title" : "Robust and efficient subspace segmentation via least squares regression",
      "author" : [ "C-Y. Lu", "H. Min", "Z-Q. Zhao", "L. Zhu", "D-S. Huang", "S. Yan" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Lu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2012
    }, {
      "title" : "Identification of deterministic switched ARX systems via identification of algebraic varieties. In Hybrid Systems: Computation and Control, pages 449–465",
      "author" : [ "Y. Ma", "R. Vidal" ],
      "venue" : null,
      "citeRegEx" : "Ma and Vidal.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ma and Vidal.",
      "year" : 2005
    }, {
      "title" : "Segmentation of multivariate mixed data via lossy coding and compression",
      "author" : [ "Y. Ma", "H. Derksen", "W. Hong", "J. Wright" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Ma et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2007
    }, {
      "title" : "On lines and planes of closest fit to systems of points in space",
      "author" : [ "K. Pearson" ],
      "venue" : "The London, Edinburgh and Dublin Philosphical Magazine and Journal of Science,",
      "citeRegEx" : "Pearson.,? \\Q1901\\E",
      "shortCiteRegEx" : "Pearson.",
      "year" : 1901
    }, {
      "title" : "Finding a sparse vector in a subspace: Linear sparsity using alternating directions",
      "author" : [ "Q. Qu", "J. Sun", "J. Wright" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Qu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2014
    }, {
      "title" : "Segmentation and reconstruction of polyhedral building roofs from aerial lidar point clouds",
      "author" : [ "A. Sampath", "J. Shan" ],
      "venue" : "IEEE Transactions on Geoscience and Remote Sensing,",
      "citeRegEx" : "Sampath and Shan.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sampath and Shan.",
      "year" : 2010
    }, {
      "title" : "Indoor segmentation and support inference from rgbd images",
      "author" : [ "N. Silberman", "P. Kohli", "D. Hoiem", "R. Fergus" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Silberman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Silberman et al\\.",
      "year" : 2012
    }, {
      "title" : "On orthogonal linear `1 approximation",
      "author" : [ "H. Späth", "G.A. Watson" ],
      "venue" : "Numerische Mathematik,",
      "citeRegEx" : "Späth and Watson.,? \\Q1987\\E",
      "shortCiteRegEx" : "Späth and Watson.",
      "year" : 1987
    }, {
      "title" : "An introduction to conditional random fields for relational learning, volume 2. Introduction to statistical relational learning",
      "author" : [ "C. Sutton", "A. McCallum" ],
      "venue" : null,
      "citeRegEx" : "Sutton and McCallum.,? \\Q2006\\E",
      "shortCiteRegEx" : "Sutton and McCallum.",
      "year" : 2006
    }, {
      "title" : "Probabilistic principal component analysis",
      "author" : [ "M. Tipping", "C. Bishop" ],
      "venue" : "Journal of the Royal Statistical Society,",
      "citeRegEx" : "Tipping and Bishop.,? \\Q1999\\E",
      "shortCiteRegEx" : "Tipping and Bishop.",
      "year" : 1999
    }, {
      "title" : "Mixtures of probabilistic principal component analyzers",
      "author" : [ "M. Tipping", "C. Bishop" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Tipping and Bishop.,? \\Q1999\\E",
      "shortCiteRegEx" : "Tipping and Bishop.",
      "year" : 1999
    }, {
      "title" : "Abstract algebraic-geometric subspace clustering",
      "author" : [ "M.C. Tsakiris", "R. Vidal" ],
      "venue" : "In Asilomar Conference on Signals, Systems and Computers,",
      "citeRegEx" : "Tsakiris and Vidal.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tsakiris and Vidal.",
      "year" : 2014
    }, {
      "title" : "Algebraic clustering of affine subspaces",
      "author" : [ "M.C. Tsakiris", "R. Vidal" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Tsakiris and Vidal.,? \\Q2017\\E",
      "shortCiteRegEx" : "Tsakiris and Vidal.",
      "year" : 2017
    }, {
      "title" : "Dual principal component pursuit. arXiv:1510.04390v2 [cs.CV], 2017b",
      "author" : [ "M.C. Tsakiris", "R. Vidal" ],
      "venue" : null,
      "citeRegEx" : "Tsakiris and Vidal.,? \\Q2017\\E",
      "shortCiteRegEx" : "Tsakiris and Vidal.",
      "year" : 2017
    }, {
      "title" : "Filtrated algebraic subspace clustering",
      "author" : [ "M.C. Tsakiris", "R. Vidal" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "Tsakiris and Vidal.,? \\Q2017\\E",
      "shortCiteRegEx" : "Tsakiris and Vidal.",
      "year" : 2017
    }, {
      "title" : "Dual principal component pursuit",
      "author" : [ "M.C. Tsakiris", "R. Vidal" ],
      "venue" : "In ICCV Workshop on Robust Subspace Learning and Computer Vision,",
      "citeRegEx" : "Tsakiris and Vidal.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tsakiris and Vidal.",
      "year" : 2015
    }, {
      "title" : "Filtrated spectral algebraic subspace clustering",
      "author" : [ "M.C. Tsakiris", "R. Vidal" ],
      "venue" : "In ICCV Workshop on Robust Subspace Learning and Computer Vision,",
      "citeRegEx" : "Tsakiris and Vidal.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tsakiris and Vidal.",
      "year" : 2015
    }, {
      "title" : "Nearest q-flat to m points",
      "author" : [ "P. Tseng" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "Tseng.,? \\Q2000\\E",
      "shortCiteRegEx" : "Tseng.",
      "year" : 2000
    }, {
      "title" : "Subspace clustering",
      "author" : [ "R. Vidal" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "Vidal.,? \\Q2011\\E",
      "shortCiteRegEx" : "Vidal.",
      "year" : 2011
    }, {
      "title" : "Low rank subspace clustering (LRSC)",
      "author" : [ "R. Vidal", "P. Favaro" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "Vidal and Favaro.,? \\Q2014\\E",
      "shortCiteRegEx" : "Vidal and Favaro.",
      "year" : 2014
    }, {
      "title" : "Three-view multibody structure from motion",
      "author" : [ "R. Vidal", "R. Hartley" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Vidal and Hartley.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vidal and Hartley.",
      "year" : 2008
    }, {
      "title" : "Generalized Principal Component Analysis (GPCA)",
      "author" : [ "R. Vidal", "Y. Ma", "S. Sastry" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Vidal et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Vidal et al\\.",
      "year" : 2003
    }, {
      "title" : "Generalized Principal Component Analysis (GPCA)",
      "author" : [ "R. Vidal", "Y. Ma", "S. Sastry" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Vidal et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Vidal et al\\.",
      "year" : 2005
    }, {
      "title" : "Two-view multibody structure from motion",
      "author" : [ "R. Vidal", "Y. Ma", "S. Soatto", "S. Sastry" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Vidal et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Vidal et al\\.",
      "year" : 2006
    }, {
      "title" : "Multiframe motion segmentation with missing data using PowerFactorization, and GPCA",
      "author" : [ "R. Vidal", "R. Tron", "R. Hartley" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Vidal et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vidal et al\\.",
      "year" : 2008
    }, {
      "title" : "Generalized Principal Component Analysis",
      "author" : [ "R. Vidal", "Y. Ma", "S. Sastry" ],
      "venue" : null,
      "citeRegEx" : "Vidal et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vidal et al\\.",
      "year" : 2016
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "U. von Luxburg" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "Luxburg.,? \\Q2007\\E",
      "shortCiteRegEx" : "Luxburg.",
      "year" : 2007
    }, {
      "title" : "Self scaled regularized robust regression",
      "author" : [ "Y. Wang", "C. Dicle", "M. Sznaier", "O. Camps" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Median k-flats for hybrid linear modeling with many outliers",
      "author" : [ "T. Zhang", "A. Szlam", "G. Lerman" ],
      "venue" : "In Workshop on Subspace Methods,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Over the past fifteen years the model of a union of linear subspaces, also called a subspace arrangement (Derksen, 2007), has gained significant popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classical model of a single linear subspace, associated to the well-known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002).",
      "startOffset" : 105,
      "endOffset" : 120
    }, {
      "referenceID" : 46,
      "context" : "Over the past fifteen years the model of a union of linear subspaces, also called a subspace arrangement (Derksen, 2007), has gained significant popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classical model of a single linear subspace, associated to the well-known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002).",
      "startOffset" : 199,
      "endOffset" : 212
    }, {
      "referenceID" : 21,
      "context" : "Over the past fifteen years the model of a union of linear subspaces, also called a subspace arrangement (Derksen, 2007), has gained significant popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classical model of a single linear subspace, associated to the well-known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002).",
      "startOffset" : 343,
      "endOffset" : 391
    }, {
      "referenceID" : 31,
      "context" : "Over the past fifteen years the model of a union of linear subspaces, also called a subspace arrangement (Derksen, 2007), has gained significant popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classical model of a single linear subspace, associated to the well-known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002).",
      "startOffset" : 343,
      "endOffset" : 391
    }, {
      "referenceID" : 22,
      "context" : "Over the past fifteen years the model of a union of linear subspaces, also called a subspace arrangement (Derksen, 2007), has gained significant popularity in pattern recognition and computer vision (Vidal, 2011), often replacing the classical model of a single linear subspace, associated to the well-known Principal Component Analysis (PCA) (Hotelling, 1933; Pearson, 1901; Jolliffe, 2002).",
      "startOffset" : 343,
      "endOffset" : 391
    }, {
      "referenceID" : 46,
      "context" : "This has led to a variety of algorithms that attempt to cluster a collection of data drawn from a subspace arrangement, giving rise to the challenging field of subspace clustering (Vidal, 2011).",
      "startOffset" : 180,
      "endOffset" : 193
    }, {
      "referenceID" : 20,
      "context" : ", 2009), statistical (Tipping and Bishop, 1999b; Gruber and Weiss, 2004), informationtheoretic (Ma et al.",
      "startOffset" : 21,
      "endOffset" : 72
    }, {
      "referenceID" : 30,
      "context" : ", 2009), statistical (Tipping and Bishop, 1999b; Gruber and Weiss, 2004), informationtheoretic (Ma et al., 2007), algebraic (Vidal et al.",
      "startOffset" : 95,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al.",
      "startOffset" : 51,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al.",
      "startOffset" : 51,
      "endOffset" : 140
    }, {
      "referenceID" : 23,
      "context" : ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al.",
      "startOffset" : 51,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al.",
      "startOffset" : 51,
      "endOffset" : 140
    }, {
      "referenceID" : 26,
      "context" : ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al., 2010; Favaro et al., 2011; Liu et al., 2013; Vidal and Favaro, 2014) representation theory.",
      "startOffset" : 214,
      "endOffset" : 295
    }, {
      "referenceID" : 15,
      "context" : ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al., 2010; Favaro et al., 2011; Liu et al., 2013; Vidal and Favaro, 2014) representation theory.",
      "startOffset" : 214,
      "endOffset" : 295
    }, {
      "referenceID" : 27,
      "context" : ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al., 2010; Favaro et al., 2011; Liu et al., 2013; Vidal and Favaro, 2014) representation theory.",
      "startOffset" : 214,
      "endOffset" : 295
    }, {
      "referenceID" : 47,
      "context" : ", 2003, 2005; Tsakiris and Vidal, 2017c), spectral (Boult and Brown, 1991; Costeira and Kanade, 1998; Kanatani, 2001; Chen and Lerman, 2009), or based on sparse (Elhamifar and Vidal, 2009, 2010, 2013) and low-rank (Liu et al., 2010; Favaro et al., 2011; Liu et al., 2013; Vidal and Favaro, 2014) representation theory.",
      "startOffset" : 214,
      "endOffset" : 295
    }, {
      "referenceID" : 51,
      "context" : "Prominent applications include projective motion segmentation (Vidal et al., 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005).",
      "startOffset" : 62,
      "endOffset" : 107
    }, {
      "referenceID" : 48,
      "context" : "Prominent applications include projective motion segmentation (Vidal et al., 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005).",
      "startOffset" : 62,
      "endOffset" : 107
    }, {
      "referenceID" : 33,
      "context" : ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005).",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005).",
      "startOffset" : 116,
      "endOffset" : 148
    }, {
      "referenceID" : 29,
      "context" : ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005).",
      "startOffset" : 116,
      "endOffset" : 148
    }, {
      "referenceID" : 49,
      "context" : ", 2003, 2005, 2008; Tsakiris and Vidal, 2014, 2015b, 2017c,a), which gives closed-form solutions by means of factorization (Vidal et al., 2003) or differentiation (Vidal et al.",
      "startOffset" : 123,
      "endOffset" : 143
    }, {
      "referenceID" : 50,
      "context" : ", 2003) or differentiation (Vidal et al., 2005) of polynomials.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "Another method that is theoretically justifiable for clustering hyperplanes is Spectral Curvature Clustering (SCC) (Chen and Lerman, 2009), which is based on computing a D-fold affinity between all D-tuples of points in the dataset.",
      "startOffset" : 115,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : "On the other hand, the intuitive classical method of K-hyperplanes (KH) (Bradley and Mangasarian, 2000), which alternates between assigning clusters and fitting a new normal vector to each cluster with PCA, is perhaps the most practical method for hyperplane clustering, since it is simple to implement, it is robust to noise and its complexity depends on the maximal allowed number of iterations.",
      "startOffset" : 72,
      "endOffset" : 103
    }, {
      "referenceID" : 56,
      "context" : "Median K-Flats (MKF) (Zhang et al., 2009) shares a similar objective function as KH, but uses the `1-norm instead of the `2norm, in an attempt to gain robustness to outliers.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "Finally, we note that any single subspace learning method, such as RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al.",
      "startOffset" : 74,
      "endOffset" : 101
    }, {
      "referenceID" : 25,
      "context" : "Finally, we note that any single subspace learning method, such as RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al., 2015), can be applied in a sequential fashion to learn a union of hyperplanes, by first learning the first dominant hyperplane, then removing the points lying close to it, then learning a second dominant hyperplane, and so on.",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005). Even though in some ways hyperplane clustering is simpler than general subspace clustering, since, e.g., the dimensions of the subspaces are equal and known a-priori, modern self-expressiveness-based subspace clustering methods, such as Liu et al. (2013); Lu et al.",
      "startOffset" : 117,
      "endOffset" : 405
    }, {
      "referenceID" : 0,
      "context" : ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005). Even though in some ways hyperplane clustering is simpler than general subspace clustering, since, e.g., the dimensions of the subspaces are equal and known a-priori, modern self-expressiveness-based subspace clustering methods, such as Liu et al. (2013); Lu et al. (2012); Elhamifar and Vidal (2013), in principle do not apply in this case, because they require small relative subspace dimensions.",
      "startOffset" : 117,
      "endOffset" : 423
    }, {
      "referenceID" : 0,
      "context" : ", 2006; Vidal and Hartley, 2008), 3D point cloud analysis (Sampath and Shan, 2010) and hybrid system identification (Bako, 2011; Ma and Vidal, 2005). Even though in some ways hyperplane clustering is simpler than general subspace clustering, since, e.g., the dimensions of the subspaces are equal and known a-priori, modern self-expressiveness-based subspace clustering methods, such as Liu et al. (2013); Lu et al. (2012); Elhamifar and Vidal (2013), in principle do not apply in this case, because they require small relative subspace dimensions.",
      "startOffset" : 117,
      "endOffset" : 451
    }, {
      "referenceID" : 16,
      "context" : "A traditional way of clustering points lying close to a hyperplane arrangement is by means of the RANdom SAmpling Consensus algorithm (RANSAC) (Fischler and Bolles, 1981), which attempts to identify a single hyperplaneHi at a time.",
      "startOffset" : 143,
      "endOffset" : 170
    }, {
      "referenceID" : 3,
      "context" : "Another very popular method for hyperplane clustering is the so-called K-hyperplanes (KH), which was proposed by Bradley and Mangasarian (2000). KH attempts to minimize the non-convex objective function",
      "startOffset" : 113,
      "endOffset" : 144
    }, {
      "referenceID" : 56,
      "context" : "It is precisely the sensitivity to outliers of KH that Median K Flats (MKF) or Median K Hyperplanes (Zhang et al., 2009) attempts to address, by minimizing the non-",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 50,
      "context" : "This makes the optimization problem harder, and Zhang et al. (2009) propose to solve it by means of a stochastic gradient approach, which requires multiple restarts, as KH does.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 46,
      "context" : "ASC was originally proposed in Vidal et al. (2003) precisely for the purpose of provably clustering hyperplanes, a problem which at the time was handled either by the intuitive RANSAC or K-Hyperplanes.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 46,
      "context" : "This reduces the problem to that of factorizing p(x) to the product of linear factors, which was elegantly done in Vidal et al. (2003). When the data are contaminated by noise, the fitted polynomial need no longer be factorizable; this apparent difficulty was circumvented in Vidal et al.",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 46,
      "context" : "This reduces the problem to that of factorizing p(x) to the product of linear factors, which was elegantly done in Vidal et al. (2003). When the data are contaminated by noise, the fitted polynomial need no longer be factorizable; this apparent difficulty was circumvented in Vidal et al. (2005), where it was shown that the gradient of the polynomial evaluated at point xj is a good estimate for the normal vector of the hyperplane Hi that xj lies closest to.",
      "startOffset" : 115,
      "endOffset" : 296
    }, {
      "referenceID" : 25,
      "context" : "A recently proposed single subspace learning method that admits an interesting theoretical analysis is the so-called REAPER (Lerman et al., 2015).",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 6,
      "context" : ",xjD (see Chen and Lerman (2009) for an explicit formula) and σ is a tuning parameter.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : ",xjD (see Chen and Lerman (2009) for an explicit formula) and σ is a tuning parameter. Intuitively, the polar curvature is a multiple of the volume of the simplex of the D points, which becomes zero if the points lie in the same hyperplane, and the further the points lie from any hyperplane the larger the volume becomes. SCC obtains the hyperplane clusters by unfolding the tensor A to an affinity matrix, upon which spectral clustering is applied. As with ASC, the main bottleneck of SCC is computational, since in principle all ( N D ) entries of the tensor need to be computed. Even though the combinatorial complexity of SCC can be reduced, this comes at the cost of significant performance degradation. RANSAC/KH Hybrids. Generally speaking, any single subspace learning method that is robust to outliers and can handle subspaces of high relative dimensions, can be used to perform hyperplane clustering, either via a RANSAC-style or a KH-style scheme or a combination of both. For example, ifM is a method that takes a dataset and fits to it a hyperplane, then one can useM to compute the first dominant hyperplane, remove the points in the dataset lying close to it, compute a second dominant hyperplane and so on (RANSAC-style). Alternatively, one can start with a random guess for n hyperplanes, cluster the data according to their distance to these hyperplanes, and then use M (instead of the classic SVD) to fit a new hyperplane to each cluster, and so on (KH-style). Even though a large variety of single subspace learning methods exist, e.g., see references in Lerman and Zhang (2014), only few such methods are potentially able to handle large relative dimensions and in particular hyperplanes.",
      "startOffset" : 10,
      "endOffset" : 1600
    }, {
      "referenceID" : 25,
      "context" : "As (6) is non-convex, Lerman et al. (2015) relax it to the convex semi-definite program",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 16,
      "context" : "Similarly to RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al.",
      "startOffset" : 20,
      "endOffset" : 47
    }, {
      "referenceID" : 25,
      "context" : "Similarly to RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al., 2015), DPCP (Tsakiris and Vidal, 2015a, 2017b) is another, recently proposed, single subspace learning method that can be applied to hyperplane clustering.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "This difficulty can be circumvented by solving (7) in an Iteratively Reweighted Least Squares (IRLS) fashion, for which convergence of the objective value to a neighborhood of the optimal value has been established in Lerman et al. (2015). Dual Principal Component Pursuit (DPCP).",
      "startOffset" : 218,
      "endOffset" : 239
    }, {
      "referenceID" : 16,
      "context" : "Similarly to RANSAC (Fischler and Bolles, 1981) or REAPER (Lerman et al., 2015), DPCP (Tsakiris and Vidal, 2015a, 2017b) is another, recently proposed, single subspace learning method that can be applied to hyperplane clustering. 5. Regression-type methods such as the one proposed in Wang et al. (2015) are also a possibility.",
      "startOffset" : 21,
      "endOffset" : 304
    }, {
      "referenceID" : 30,
      "context" : "Important examples that we do not compare with in this paper are the statistical-theoretic Mixtures of Probabilistic Principal Component Analyzers (Tipping and Bishop, 1999a), as well as the information-theoretic Agglomerative Lossy Compression (Ma et al., 2007).",
      "startOffset" : 245,
      "endOffset" : 262
    }, {
      "referenceID" : 36,
      "context" : "Even though no theory has been developed for this approach, experimental evidence in Tsakiris and Vidal (2017b) indicates convergence of such an IRLS scheme to the global minimizer of (9).",
      "startOffset" : 85,
      "endOffset" : 112
    }, {
      "referenceID" : 30,
      "context" : "Important examples that we do not compare with in this paper are the statistical-theoretic Mixtures of Probabilistic Principal Component Analyzers (Tipping and Bishop, 1999a), as well as the information-theoretic Agglomerative Lossy Compression (Ma et al., 2007). For an extensive account of these and other methods the reader is referred to Vidal et al. (2016).",
      "startOffset" : 246,
      "endOffset" : 362
    }, {
      "referenceID" : 40,
      "context" : "This can be verified computationally by checking that there is only one up to scale homogeneous polynomial of degree n that fits the data, see Vidal et al. (2005); Tsakiris and Vidal (2017c) for details.",
      "startOffset" : 143,
      "endOffset" : 163
    }, {
      "referenceID" : 39,
      "context" : "(2005); Tsakiris and Vidal (2017c) for details.",
      "startOffset" : 8,
      "endOffset" : 35
    }, {
      "referenceID" : 39,
      "context" : "(16) In the second equality above we made use of the rotational invariance of the sphere, as well as the fact that Ĥi ∼= SD−2, which leads to (for details see the proof of Proposition 4 and Lemma 7 in Tsakiris and Vidal (2017b)) ∫",
      "startOffset" : 201,
      "endOffset" : 228
    }, {
      "referenceID" : 11,
      "context" : "Medians in Riemmannian manifolds, and in particular in the Grassmannian manifold, are an active subject of research (Draper et al., 2014; Ghalieh and Hajja, 1996).",
      "startOffset" : 116,
      "endOffset" : 162
    }, {
      "referenceID" : 17,
      "context" : "Medians in Riemmannian manifolds, and in particular in the Grassmannian manifold, are an active subject of research (Draper et al., 2014; Ghalieh and Hajja, 1996).",
      "startOffset" : 116,
      "endOffset" : 162
    }, {
      "referenceID" : 17,
      "context" : "This is in striking similarity with the results regarding the Fermat point of planar and spherical triangles (Ghalieh and Hajja, 1996).",
      "startOffset" : 109,
      "endOffset" : 134
    }, {
      "referenceID" : 19,
      "context" : "2) that when the points X i are uniformly distributed in a deterministic sense (Grabner et al., 1997; Grabner and Tichy, 1993), i is small and in particular i → 0 as Ni →∞.",
      "startOffset" : 79,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "2) that when the points X i are uniformly distributed in a deterministic sense (Grabner et al., 1997; Grabner and Tichy, 1993), i is small and in particular i → 0 as Ni →∞.",
      "startOffset" : 79,
      "endOffset" : 126
    }, {
      "referenceID" : 37,
      "context" : "Then it can be shown (see Tsakiris and Vidal (2017b) §4.",
      "startOffset" : 26,
      "endOffset" : 53
    }, {
      "referenceID" : 39,
      "context" : "For details regarding the evaluation of this integral see Lemma 9 and its proof in Tsakiris and Vidal (2017b).",
      "startOffset" : 83,
      "endOffset" : 110
    }, {
      "referenceID" : 8,
      "context" : "Viewing the above system of equations as polynomial equations in the variables x1, x2, x3, y1, y2, y3, z, standard Groebner basis (Cox et al., 2007) computations reveal that the polynomial g := (1− z)(y 1 − y 2)(y 1 − y 3)(y 2 − y 3)(y1 + y2 + y3) (76) lies in the ideal generated by pi, qi, i = 1, 2, 3.",
      "startOffset" : 130,
      "endOffset" : 148
    }, {
      "referenceID" : 16,
      "context" : "Since at its core DPCP is a single subspace learning method, we may as well use it to learn n hyperplanes in the same way that RANSAC (Fischler and Bolles, 1981) is used: learn one hyperplane from the entire dataset, remove the points close to it, then learn a second hyperplane and so on.",
      "startOffset" : 134,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "Another way to do hyperplane clustering via DPCP, is to modify the classic K-Subspaces (Bradley and Mangasarian, 2000; Tseng, 2000; Zhang et al., 2009) by computing the normal vector of each cluster by DPCP.",
      "startOffset" : 87,
      "endOffset" : 151
    }, {
      "referenceID" : 45,
      "context" : "Another way to do hyperplane clustering via DPCP, is to modify the classic K-Subspaces (Bradley and Mangasarian, 2000; Tseng, 2000; Zhang et al., 2009) by computing the normal vector of each cluster by DPCP.",
      "startOffset" : 87,
      "endOffset" : 151
    }, {
      "referenceID" : 56,
      "context" : "Another way to do hyperplane clustering via DPCP, is to modify the classic K-Subspaces (Bradley and Mangasarian, 2000; Tseng, 2000; Zhang et al., 2009) by computing the normal vector of each cluster by DPCP.",
      "startOffset" : 87,
      "endOffset" : 151
    }, {
      "referenceID" : 3,
      "context" : "It is worth noting that since DPCP minimizes the `1-norm of the distances of the points to a hyperplane, consistency dictates that the stopping criterion for IHL-DPCP be governed by the sum over all points of the distance of each point to its assigned hyperplane (instead of the traditional sum of squares (Bradley and Mangasarian, 2000; Tseng, 2000)); in other words the global objective function minimized by IHL-DPCP is the same as that of Median K-Flats (Zhang et al.",
      "startOffset" : 306,
      "endOffset" : 350
    }, {
      "referenceID" : 45,
      "context" : "It is worth noting that since DPCP minimizes the `1-norm of the distances of the points to a hyperplane, consistency dictates that the stopping criterion for IHL-DPCP be governed by the sum over all points of the distance of each point to its assigned hyperplane (instead of the traditional sum of squares (Bradley and Mangasarian, 2000; Tseng, 2000)); in other words the global objective function minimized by IHL-DPCP is the same as that of Median K-Flats (Zhang et al.",
      "startOffset" : 306,
      "endOffset" : 350
    }, {
      "referenceID" : 56,
      "context" : "It is worth noting that since DPCP minimizes the `1-norm of the distances of the points to a hyperplane, consistency dictates that the stopping criterion for IHL-DPCP be governed by the sum over all points of the distance of each point to its assigned hyperplane (instead of the traditional sum of squares (Bradley and Mangasarian, 2000; Tseng, 2000)); in other words the global objective function minimized by IHL-DPCP is the same as that of Median K-Flats (Zhang et al., 2009).",
      "startOffset" : 458,
      "endOffset" : 478
    }, {
      "referenceID" : 39,
      "context" : "In Tsakiris and Vidal (2017b) we described four distinct methods for solving it, which we briefly review here.",
      "startOffset" : 3,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "A second approach, called DPCP-IRLS, is to solve (9) using a standard Iteratively Reweighted LeastSquares (IRLS) technique ((Candès et al., 2008; Daubechies et al., 2010; Chartrand and Yin, 2008)) as in Algorithm 4.",
      "startOffset" : 124,
      "endOffset" : 195
    }, {
      "referenceID" : 9,
      "context" : "A second approach, called DPCP-IRLS, is to solve (9) using a standard Iteratively Reweighted LeastSquares (IRLS) technique ((Candès et al., 2008; Daubechies et al., 2010; Chartrand and Yin, 2008)) as in Algorithm 4.",
      "startOffset" : 124,
      "endOffset" : 195
    }, {
      "referenceID" : 5,
      "context" : "A second approach, called DPCP-IRLS, is to solve (9) using a standard Iteratively Reweighted LeastSquares (IRLS) technique ((Candès et al., 2008; Daubechies et al., 2010; Chartrand and Yin, 2008)) as in Algorithm 4.",
      "startOffset" : 124,
      "endOffset" : 195
    }, {
      "referenceID" : 31,
      "context" : "The first method, which was first proposed in Späth and Watson (1987), consists of solving the recursion of linear programs (11) using any standard solver, such as Gurobi (Gurobi Optimization, 2015); we refer to such a method as DPCP-r, standing for relaxed DPCP (see Algorithm 3).",
      "startOffset" : 46,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "A second approach, called DPCP-IRLS, is to solve (9) using a standard Iteratively Reweighted LeastSquares (IRLS) technique ((Candès et al., 2008; Daubechies et al., 2010; Chartrand and Yin, 2008)) as in Algorithm 4. A third method, first proposed in Qu et al. (2014), is to solve (9) approximately by applying alternative minimization on its denoised version",
      "startOffset" : 125,
      "endOffset" : 267
    }, {
      "referenceID" : 39,
      "context" : "which is in turn solved via alternating minimization; see Tsakiris and Vidal (2017b) for details.",
      "startOffset" : 58,
      "endOffset" : 85
    }, {
      "referenceID" : 34,
      "context" : "In this section we explore various Iterative Hyperplane Clustering10 (IHL) algorithms using the benchmark dataset NYUdepthV2 Silberman et al. (2012). This dataset consists of 1449 RGBd data instances acquired using the Microsoft kinect sensor.",
      "startOffset" : 125,
      "endOffset" : 149
    }, {
      "referenceID" : 36,
      "context" : ", bn that the algorithm produced to minimize a Conditional-Random-Field (Sutton and McCallum, 2006) type of energy function, given by",
      "startOffset" : 72,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "The minimization of the energy function is done via GraphCuts (Boykov et al., 2001).",
      "startOffset" : 62,
      "endOffset" : 83
    } ],
    "year" : 2017,
    "abstractText" : "State-of-the-art methods for clustering data drawn from a union of subspaces are based on sparse and low-rank representation theory. Existing results guaranteeing the correctness of such methods require the dimension of the subspaces to be small relative to the dimension of the ambient space. When this assumption is violated, as is, for example, in the case of hyperplanes, existing methods are either computationally too intense (e.g., algebraic methods) or lack theoretical support (e.g., K-hyperplanes or RANSAC). The main theoretical contribution of this paper is to extend the theoretical analysis of a recently proposed single subspace learning algorithm, called Dual Principal Component Pursuit (DPCP), to the case where the data are drawn from of a union of hyperplanes. To gain insight into the expected properties of the non-convex `1 problem associated with DPCP (discrete problem), we develop a geometric analysis of a closely related continuous optimization problem. Then transferring this analysis to the discrete problem, our results state that as long as the hyperplanes are sufficiently separated, the dominant hyperplane is sufficiently dominant and the points are uniformly distributed (in a deterministic sense) inside their associated hyperplanes, then the non-convex DPCP problem has a unique (up to sign) global solution, equal to the normal vector of the dominant hyperplane. This suggests a sequential hyperplane learning algorithm, which first learns the dominant hyperplane by applying DPCP to the data. In order to avoid hard thresholding of the points which is sensitive to the choice of the thresholding parameter, all points are weighted according to their distance to that hyperplane, and a second hyperplane is computed by applying DPCP to the weighted data, and so on. Experiments on corrupted synthetic data show that this DPCP-based sequential algorithm dramatically improves over similar sequential algorithms, which learn the dominant hyperplane via state-of-the-art single subspace learning methods (e.g., with RANSAC or REAPER). Finally, 3D plane clustering experiments on real 3D point clouds show that a K-Hyperplanes DPCP-based scheme, which computes the normal vector of each cluster via DPCP, instead of the classic SVD, is very competitive to state-of-the-art approaches (e.g., RANSAC or SVD-based K-Hyperplanes).",
    "creator" : "LaTeX with hyperref package"
  }
}