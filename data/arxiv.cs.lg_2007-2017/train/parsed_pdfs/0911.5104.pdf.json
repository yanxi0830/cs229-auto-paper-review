{
  "name" : "0911.5104.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Bayesian Rule for Adaptive Control based on Causal Interventions",
    "authors" : [ "Pedro A. Ortega", "Daniel A. Braun" ],
    "emails" : [ "peortega@dcc.uchile.cl", "dab54@cam.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :0\n91 1.\n51 04\nv2 [\ncs .A\nI] 3\n0 D\nec 2\n00 9\nKeywords: Adaptive behavior, Intervention calculus, Bayesian control, Kullback-Leibler-divergence"
    }, {
      "heading" : "Introduction",
      "text" : "The ability to adapt to unknown environments is often considered a hallmark of intelligence [Beer, 1990, Hutter, 2004]. Agent and environment can be conceptualized as two systems that exchange symbols in every time step [Hutter, 2004]: the symbol issued by the agent is an action, whereas the symbol issued by the environment is an observation. Thus, both agent and environment can be conceptualized as probability distributions over sequences of actions and observations (I/O streams). If the environment is perfectly known then the I/O probability distribution of the agent can be tailored to suit this particular environment. However, if the environment is unknown, but known to belong to a set of possible environments, then the agent faces an adaptation problem. Consider, for example, a robot that has been endowed with a set of behavioral primitives\nand now faces the problem of how to act while being ignorant as to which is the correct primitive. Since we want to model both agent and environment as probability distributions over I/O sequences, a natural way to measure the degree of adaptation would be to measure the ‘distance’ in probability space between the I/O distribution represented by the agent and the I/O distribution conditioned on the true environment. A suitable measure (in terms of its information-theoretic interpretation) is readily provided by the KL-divergence [MacKay, 2003]. In the case of passive prediction, the adaptation problem has a well-known solution. The distribution that minimizes the KL-divergence is a Bayesian mixture distribution over all possible environments [Haussler and Opper, 1997, Opper, 1998]. The aim of this paper is to extend this result for distributions over both inputs and outputs. The main result of this paper is that this extension is only possible if we consider the special syntax of actions in probability theory as it has been suggested by proponents of causal calculus [Pearl, 2000]."
    }, {
      "heading" : "Preliminaries",
      "text" : "We restrict the exposition to the case of discrete time with discrete stochastic observations and control signals. Let O and A be two finite sets, the first being the set of observations and the second being the set of actions. We use a≤t ≡ a1a2 . . . at, ao≤t ≡ a1o1 . . . atot etc. to simplify the notation of strings. Using A and O, a set of interaction sequences is constructed. Define the set of interactions as Z ≡ A×O. A pair (a, o) ∈ Z is called an interaction. The set of interaction strings of length t ≥ 0 is denoted by Zt. Similarly, the set of (finite) interaction strings is Z∗ ≡ ⋃\nt≥0 Z t and the set\nof (infinite) interaction sequences is Z∞ ≡ {w : w = a1o1a2o2 . . .}, where each (at, ot) ∈ Z. The interaction string of length 0 is denoted by ǫ. Agents and environments are formalized as I/O systems. An I/O system is a probability distribution Pr over interaction sequences Z∞. Pr is uniquely determined by the conditional probabilities\nPr(at|ao<t), Pr(ot|ao<tat) (1)\nfor each ao≤t ∈ Z ∗. However, the semantics of the\nprobability distribution Pr are only fully defined once it is coupled to another system. Let P, Q be two I/O systems. An interaction system (P,Q) is a coupling of the two systems giving rise to the generative distribution G that describes the probabilities that actually govern the I/O stream once the two systems are coupled. G is specified by the equations\nG(at|ao<t) = P(at|ao<t)\nG(ot|ao<tat) = Q(ot|ao<tat)\nvalid for all aot ∈ Z ∗. Here, G models the true probability distribution over interaction sequences that arises by coupling two systems through their I/O streams. More specifically, for the system P, P(at|ao<t) is the probability of producing action at ∈ A given history ao<t and P(ot|ao<tat) is the predicted probability of the observation ot ∈ O given history ao<tat. Hence, for P, the sequence o1o2 . . . is its input stream and the sequence a1a2 . . . is its output stream. In contrast, the roles of actions and observations are reversed in the case of the system Q. Thus, the sequence o1o2 . . . is its output stream and the sequence a1a2 . . . is its input stream. This model of interaction is very general in that it can accommodate many specific regimes of interaction. Note that an agent P can perfectly predict its environment Q iff for all ao≤t ∈ Z ∗,\nP(ot|ao<tat) = Q(ot|ao<tat).\nIn this case we say that P is tailored to Q."
    }, {
      "heading" : "Adaptive Systems: Näıve Construction",
      "text" : "Throughout this paper, we use the convention that P is an agent to be constructed by a designer, which is then going to be interfaced with a preexisting but unknown environment Q. The designer assumes that Q is going to be drawn with probability P (m) from a set Q ≡ {Qm}m∈M of possible systems before the interaction starts, where M is a countable set. Consider the case when the designer knows beforehand which environment Q ∈ Q is going to be drawn. Then, not only can P be tailored to Q, but also a custom-made policy for Q can be designed. That is, the output stream P(at|ao<t) is such that the true probability G of the resulting interaction system (P,Q) gives rise to interaction sequences that the designer considers desirable. Consider now the case when the designer does not know which environment Qm ∈ Q is going to be drawn, and assume he has a set P ≡ {Pm}m∈M of systems such that for each m ∈ M, Pm is tailored to Qm and the interaction system (Pm,Qm) has a generative distribution Gm that produces desirable interaction sequences. How can the designer construct a system P such that its behavior is as close as possible to the custom-made system Pm under any realization of Qm ∈ Q? A convenient measure of how much P deviates from Pm is given by the KL-divergence. A first approach would be to construct an agent P̃ so as to minimize\nthe total expected KL-divergence to Pm. This is constructed as follows. Define the history-dependent KLdivergences over the action at and observation ot as\nDatm (ao<t) ≡ ∑\nat\nPm(at|ao<t) log2 Pm(at|ao<t)\nPr(at|ao<t)\nDotm(ao<tat) ≡ ∑\not\nPm(ot|ao<tat) log2 Pm(ot|ao<tat)\nPr(ot|ao<tat) ,\nwhere Pr is a given arbitrary agent. Then, define the average KL-divergences over at and ot as\nDatm = ∑\nao <t\nPm(ao<t)D at m (ao<t)\nDotm = ∑\nao <t at\nPm(ao<tat)D ot m(ao<tat).\nFinally, we define the total expected KL-divergence of Pr to Pm as\nD ≡ lim sup t→∞\n∑\nm\nP (m) t ∑\nτ=1\n(\nDaτm +D oτ m\n)\n.\nWe construct the agent P̃ as the system that minimizes D = D(Pr):\nP̃ ≡ argmin Pr D(Pr). (2)\nThe solution to Equation 2 is the system P̃ defined by the set of equations\nP̃(at|ao<t) = ∑\nm\nPm(at|ao<t)wm(ao<t)\nP̃(ot|ao<tat) = ∑\nm\nPm(ot|ao<tat)wm(ao<tat) (3)\nvalid for all ao≤t ∈ Z ∗, where the mixture weights are\nwm(ao<t) ≡ P (m)Pm(ao<t) ∑\nm′ P (m ′)Pm′(ao<t)\nwm(ao<tat) ≡ P (m)Pm(ao<tat) ∑\nm′ P (m ′)Pm′(ao<tat)\n.\n(4)\nFor reference, see Haussler and Opper [1997], Opper\n[1998]. It is clear that P̃ is just the Bayesian mixture over the agents Pm. If we define the conditional probabilities\nP (at|m, ao<t) ≡ Pm(at|ao<t)\nP (ot|m, ao<tat) ≡ Pm(at|ao<tat) (5)\nfor all ao≤t ∈ Z ∗, then Equation 3 can be rewritten as\nP̃(at|ao<t) = ∑\nm\nP (at|m, ao<t)P (m|ao<t)\nP̃(ot|ao<tat) = ∑\nm\nP (ot|m, ao<tat)P (m|ao<tat) (6)\nwhere the P (m|ao<t) = wm(ao<t) and P (m|ao<tat) = wm(ao<tat) are just the posterior probabilities over the\nelements in M given the past interactions. Hence, the conditional probabilities in Equation 5, together with the prior probabilities P (m), define a Bayesian model over interaction sequences with hypotheses m ∈ M. The behavior of P̃ can be described as follows. At any given time t, P̃ maintains a mixture over systems Pm. The weighting over them is given by the mixture coefficients wm. Whenever a new action at or a new observation is produced (by the agent or the environment respectively), the weights wm are updated according to Bayes’ rule. In addition, P̃ issues an action at suggested by a system Pm drawn randomly according to the weights wt. However, there is an important problem with P̃ that arises due to the fact that it is not only a system that is passively observing symbols, but also actively generating them. Therefore, an action that is generated by the agent should not provide the same information than an observation that is issued by its environment. Intuitively, it does not make any sense to use one’s own actions to do inference. In the following section we illustrate this problem with a simple statistical example."
    }, {
      "heading" : "The Problem of Causal Intervention",
      "text" : "Suppose a statistician is asked to design a model for a given data set D and she decides to use a Bayesian method. She computes the posterior probability density function (pdf) over the parameters θ of the model given the data using Bayes’ rule:\np(θ|D) = p(D|θ)p(θ) ∫\np(D|θ′)p(θ′) dθ′ ,\nwhere p(D|θ) is the likelihood of D given θ and p(θ) is the prior pdf of θ. She can simulate the source by drawing a sample data set S from the predictive pdf\np(S|D) =\n∫\np(S|D, θ)p(θ|D) dθ,\nwhere p(S|D, θ) is the likelihood of S given D and θ. She decides to do so, obtaining a sample set S′. She understands that the nature of S′ is very different from"
    }, {
      "heading" : "D: while D is informative and does change the belief state of the Bayesian model, S′ is non-informative and",
      "text" : "thus is a reflection of the model’s belief state. Hence, she would never use S′ to further condition the Bayesian model. Mathematically, she seems to imply that\np(θ|D,S′) = p(θ|D)\nif S′ has been generated from p(S|D) itself. But this simple independence assumption is not correct as the following elaboration of the example will show. The statistician is now told that the source is waiting for the simulation results S′ in order to produce a next data set D′ which does depend on S′. She hands in S′ and obtains a new data set D′. Using Bayes’ rule, the posterior pdf over the parameters is now\np(D′|D,S′, θ)p(D|θ)p(θ) ∫\np(D′|D,S′, θ′)p(D|θ′)p(θ′) dθ′ (7)\nwhere p(D′|D,S′, θ) is the likelihood of the new data D′ given the old data D, the parameters θ and the simulated data S′. Notice that this looks almost like the posterior pdf p(θ|D,S′,D′) given by\np(D′|D,S′, θ)p(S′|D, θ)p(D|θ)p(θ) ∫\np(D′|D,S′, θ′)p(S′|D, θ′)p(D|θ′)p(θ′) dθ′\nwith the exception that now the Bayesian update contains the likelihoods of the simulated data p(S′|D, θ). This suggests that Equation 7 is a variant of the posterior pdf p(θ|D,S′,D′) but where the simulated data S′ is treated in a different way than the data D and D′. Define the pdf p′ such that the pdfs p′(θ), p′(D|θ), p′(D′|D,S′, θ) are identical to p(θ), p(D|θ) and p(D′|D,S′, θ) respectively, but differ in p′(S|D, θ):\np′(S|D, θ) =\n{\n1 if S′ = S,\n0 else.\nThat is, p′ is identical to p but it assumes that the value of S is fixed to S′ given D and θ. For p′, the simulated data S′ is non-informative:\n− log 2 p(S′|D, θ) = 0.\nIf one computes the posterior pdf p′(θ|D,S′,D′), one obtains the result of Equation 7:\np′(D′|D,S′, θ)p′(S′|D, θ)p′(D|θ)p′(θ) ∫\np′(D′|D,S′, θ′)p′(S′|D, θ′)p′(D|θ′)p′(θ′) dθ′\n= p(D′|D,S′, θ)p(D|θ)p(θ) ∫\np(D′|D,S′, θ′)p(D|θ′)p(θ′) dθ′ .\nThus, in order to explain Equation 7 as a posterior pdf given the data sets D, D′ and the simulated data S′, one has to intervene p in order to account for the fact that S′ is non-informative given D and θ. In statistics, there is a rich literature on causal intervention. In particular, we will use the formalism developed by Pearl [2000], because it suits the needs to formalize interactions in systems and has a convenient notation—compare Figures 1a & b. Given a causal model1 variables that are intervened are denoted by a hat as in Ŝ. In the previous example, the causal model of the joint pdf p(θ,D,S,D′) is given by the set of conditional pdfs\nCp = { p(θ), p(D|θ), p(S|D, θ), p(D′|D,S, θ) } .\nIf D and D′ are observed from the source and S is intervened to take on the value S′, then the posterior pdf over the parameters θ is given by p(θ|D, Ŝ′,D′) which is just\np(D′|D, Ŝ′, θ)p(Ŝ′|D, θ)p(D|θ)p(θ) ∫\np(D′|D, Ŝ′, θ′)p(Ŝ′|D, θ′)p(D|θ′)p(θ′) dθ′\n= p(D′|D,S′, θ)p(D|θ)p(θ) ∫\np(D′|D,S′, θ′)p(D|θ′)p(θ′) dθ′ .\n1For our needs, it is enough to think about a causal model as a complete factorization of a probability distribution into conditional probability distributions representing the causal structure.\nbecause p(D′|D, Ŝ′, θ) = p(D′|D,S′, θ), which corresponds to applying rule 2 in Pearl’s intervention calculus, and because p(Ŝ′|D, θ′) = p′(S′|D, θ′) = 1."
    }, {
      "heading" : "Adaptive Systems: Causal Construction",
      "text" : "Following the discussion in the previous section, we want to construct an adaptive agent P by minimizing the KL-divergence to the Pm, but this time treating actions as interventions. Based on the definition of the conditional probabilities in Equation 5, we construct now the KL-divergence criterion to characterize P using intervention calculus. Importantly, interventions index a set of intervened probability distribution derived from an initial probability distribution. Hence, the set of fixed intervention sequences of the form â1â2 . . . indexes probability distributions over observation sequences o1o2 . . .. Because of this, we are going to construct a set of criteria indexed by the intervention sequences, but we will see that they all have the same solution. Define the history-dependent intervened KLdivergences over the action at and observation ot as\nCatm (âo<t) ≡ ∑\nat\nP (at|m, âo<t) log2 P (at|m, âo<t)\nPr(at|ao<t)\nCotm (âo<tât) ≡ ∑\not\nP (ot|m, âo<tât) log2 P (ot|m, âo<tât)\nPr(ot|ao<tat) ,\nwhere Pr is a given arbitrary agent. Note that past actions are treated as interventions. Then, define the average KL-divergences over at and ot as\nCatm = ∑\nao <t\nP (âo<t|m)C at m (âo<t)\nCotm = ∑\nao <t at\nP (âo<tat|m)C ot m (âo<tât).\nFinally, we define the total expected KL-divergence of P to Pm as\nC ≡ lim sup t→∞\n∑\nm\nP (m)\nt ∑\nτ=1\n(\nCaτm + C oτ m\n)\n. (8)\nWe construct the agent P as the system that minimizes C = C(Pr):\nP ≡ argmin Pr C(Pr). (9)\nThe solution to Equation 9 is the system P defined by the set of equations\nP(at|ao<t) = P (at|âo<t)\n= ∑\nm\nP (at|m, âo<t)vm(âo<t)\nP(ot|ao<tat) = P (ot|âo<tât)\n= ∑\nm\nP (ot|m, âo<tât)vm(âo<tât)\n(10)\nvalid for all ao≤t ∈ Z ∗, where the mixture weights are\nvm(ao<tat) = vm(ao<t) ≡ P (m)P (âo<t|m) ∑\nm′ P (m ′)P (âo<t|m)\n= P (m)\n∏t−1\nτ=1 P (oτ |m, âo<τ âτ ) ∑\nm′ P (m ′) ∏t−1 τ=1 P (oτ |m ′, âo<τ âτ )\n.\n(11) The proof follows the same line of argument as the solution to Equation 2 with the crucial difference that actions are treated as interventions. Consider without loss of generality the summand ∑\nm P (m)C at m in Equa-\ntion 8. Note that the KL-divergence can be written as a difference of two logarithms, where only one term depends on Pr that we want to vary. Therefore, we can integrate out the other term and write it as a constant c. Then we get\nc− ∑\nm\nP (m) ∑\nâo <t\nP (âo<t|m)\n· ∑\nat\nP (at|m, âo<t) lnPr(at|âo<t).\nSubstituting P (âo<t|m) by P (m|âo<t)P (âo<t)/P (m) and identifying P characterized by Equations 10 and 11 we obtain\nc− ∑\nâo <t\nP (âo<t) ∑\nat\nP(at|âo<t) lnPr(at|âo<t).\nThe inner sum has the form − ∑\nx p(x) ln q(x), i.e. the cross-entropy between q(x) and p(x), which is minimized when q(x) = p(x) for all x. By choosing this optimum one obtains Pr(at|âo<t) = P(at|âo<t) for all at. Note that the solution to this variational problem is independent of the weighting P (âo<t). Since the same argument applies to any summand ∑\nm P (m)C aτ m and\n∑\nm P (m)C oτ m in Equation 8, their variational problems are mutually independent. The behavior of P differs in an important aspect from\nP̃. At any given time t, P maintains a mixture over systems Pm. The weighting over these systems is given by the mixture coefficients vm. In contrast to P̃, P updates the weights vm only whenever a new observation ot is produced by the environment respectively. The update follows Bayes’ rule but treating past actions as interventions, i.e. dropping the evidence they provide. In addition, P issues an action at suggested by an system m drawn randomly according to the weights vm—see Figures 1c & d. If we use the following equalities connecting the weights and the intervened posterior distributions\nvm(ao<t) = P (m|âo<t) = P (m|âo<tât) = vm(ao<tat)\nand substitute interventions by observations in the conditionals\nP (at|m, âo<t) = P (at|m, ao<t)\nP (ot|m, âo<tât) = P (ot|m, ao<tat)\nwhich corresponds to rule 2 of Pearl’s intervention calculus, we can rewrite Equations 10 and 11 as\nP(at|ao<t) = P (at|âo<t)\n= ∑\nm\nP (at|m, ao<t)P (m|âo<t) (12)\nP(ot|ao<tat) = P (ot|âo<tât)\n= ∑\nm\nP (ot|m, ao<tat)P (m|âo<t) (13)\nwhere the intervened posterior probabilities are\nP (m|âo<t) = P (m)\n∏t−1\nτ=1 P (oτ |m, ao<τaτ ) ∑\nm′ P (m ′) ∏t−1 τ=1 P (oτ |m ′, ao<τaτ )\n.\n(14) Equations 12, 13 and 14 are important because they describe the behavior of P only in terms of known probabilities, i.e. probabilities that are computable from the causal model associated to P given by\nCP = { P (m), P (at|m, ao<t), P (ot|m, ao<tat) : t ≥ 1 } .\nImportantly, Equation 12 describes a stochastic method to produce desirable actions that differs fundamentally from an agent that is constructed by choosing an optimal policy with respect to a given utility criterion. We call this action selection rule the Bayesian control rule."
    }, {
      "heading" : "Experimental Results",
      "text" : "Here we design a very simple toy experiment to illustrate the behavior of an agent P̃ based on a Bayesian mixture compared to an agent P based on the Bayesian control rule. Let Q0, Q1, P0 and P1 be four agents with binary I/O sets A = O = {0, 1} defined as follows. P1 is such that P1(at|ao<t) = P1(at) and P1(ot|ao<tat) = P1(ot) for all ao≤t ∈ Z ∗, where\nP1(at) =\n{\n0.1 if at = 0 0.9 if at = 1 , P1(ot) =\n{\n0.4 if at = 0 0.6 if at = 1 .\nLet P0 be such that\nP0(at|ao<t) = 1− P1(at|ao<t)\nP0(ot|ao<tat) = 1− P0(ot|ao<tat)\nfor all ao≤t ∈ Z ∗. Thus, P0 and P1 are agents that are biased towards observing and acting 0’s and 1’s respectively. Furthermore, Q0 = P0 and Q1 = P1. Assume a uniform distribution over Q = {Q0,Q1}, i.e. P (m = 0) = P (m = 1) = 1\n2 .\nAssume Q0 ∈ Q is drawn. In this case, one wants the agents P̃ and P to minimize the deviation from P0. Consider the following instantaneous measure\nd(t) ≡ ∑\na′ t\nP0(a ′ t) log2\nP0(a ′ t)\nPr(a′t|ao<t)\n+ ∑\no′ t\nP0(o ′ t) log2\nP0(o ′ t)\nPr(o′t|ao<tat)\nwhere a1o1a2o2 . . . is a realization of the interaction system (Pr,Q0). d(t) measures how much Pr’s action and observation probabilities deviate from P0 at time t. Recall that both P̃ and P maintain a mixture over P0 and P1. The instantaneous I/O probabilities of such a system can always be written as\nwP0(at) + (1 − w)P1(at)\nwP0(ot) + (1− w)P1(ot).\nwhere w ∈ [0, 1]. Thus, it is easy to see that the instantaneous I/O deviation takes on the minimum value when w = 1 and the maximum value when w = 0: In the case w = 1, d(t) = 0 bits; In the case w = 0, d(t) ≈ 2.653. We have simulated realizations of the instantaneous I/O deviation using the agents P̃ and P. The results\nare summarized in Figure 2. For P̃, d(t) happens to be non-ergodic: it either converges to d(t) → 0 or to\nd(t) →≈ 2.654, implying that either P̃ → P0 or P̃ → P1 respectively. In contrast, d(t) → 0 always for P, implying that P → P0. Analogous results are obtained when Q1 ∈ Q is drawn instead: For P̃, d(t) converges either to 0 or to ≈ 2.654, whereas for P, d(t) →≈ 2.654 always implying that P → P1. Hence, P shows the correct adaptive behavior while P̃ does not."
    }, {
      "heading" : "Conclusions",
      "text" : "We propose a Bayesian rule for adaptive control. The key feature of this rule is the special treatment of actions based on causal calculus and the decomposition of agents into Bayesian mixture of I/O distributions. The question of how to integrate information generated by an agent’s probabilistic model into the agent’s information state lies at the very heart of adaptive agent design. We show that the näıve application of Bayes’ rule to I/O distributions leads to inconsistencies, because outputs don’t provide the same type of information as genuine observations. Crucially, these inconsistencies vanish if intervention calculus is applied [Pearl, 2000]. Some of the presented key ideas are not unique to the Bayesian control rule. The idea of representing agents and environments as I/O streams has been proposed by a number of other approaches, such as predictive state representation (PSR) [Littman et al., 2002] and the universal AI approach by Hutter [2004]. The idea of breaking down a control problem into a superposition of controllers has been previously evoked in the context of “mixture of experts”-models like the MOSAICarchitecture Haruno et al. [2001]. Other stochastic action selection approaches are found in exploration strategies for (PO)MDPs [Wyatt, 1997], learning automata [Narendra and Thathachar, 1974] and in probability matching [R.O. Duda, 2001] amongst others. The usage of compression principles to select actions has been proposed by AI researchers, for example Schmidhuber [2009]. The main contribution of this paper is the derivation of a stochastic action selection and inference rule by minimizing KL-divergences of intervened I/O distributions. An important potential application of the Bayesian control rule would naturally be the realm of adaptive control problems. Since it takes on a similar form to Bayes’ rule, the adaptive control problem could then be translated into an on-line inference problem where actions are sampled stochastically from a posterior distribution. It is important to note, however, that the problem statement as formulated here and the usual\nBayes-optimal approach in adaptive control are not the same. In the future the relationship between these two problem statements deserves further investigation."
    }, {
      "heading" : "M. Haruno, D.M. Wolpert, and M. Kawato. Mosaic model",
      "text" : "for sensorimotor learning and control. Neural Computation, 13:2201–2220, 2001."
    }, {
      "heading" : "D. Haussler and M. Opper. Mutual information, metric",
      "text" : "entropy and cumulative relative entropy risk. The Annals of Statistics, 25:2451–2492, 1997.\nMarcus Hutter. Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer, Berlin, 2004.\nM. Littman, R. Sutton, and S. Singh. Predictive representations of state. In Neural Information Processing Systems (NIPS), number 14, pages 1555–1561, 2002.\nDavid J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003."
    }, {
      "heading" : "K. Narendra and M.A.L. Thathachar. Learning automata",
      "text" : "- a survey. IEEE Transactions on Systems, Man, and Cybernetics, SMC-4(4):323–334, July 1974.\nM. Opper. A bayesian approach to online learning. Online Learning in Neural Networks, pages 363–378, 1998.\nJ. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, Cambridge, UK, 2000.\nD.G. Stork R.O. Duda, P.E. Hart. Pattern Classification. Wiley & Sons, Inc., second edition, 2001.\nJ. Schmidhuber. Simple algorithmic theory of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes. Journal of SICE, 48(1):21–32, 2009.\nJ. Wyatt. Exploration and Inference in Learning from Reinforcement. PhD thesis, Department of Artificial Intelligence, University of Edinburgh, 1997."
    } ],
    "references" : [ {
      "title" : "Intelligence as Adaptive Behavior",
      "author" : [ "Randall Beer" ],
      "venue" : null,
      "citeRegEx" : "Beer.,? \\Q1990\\E",
      "shortCiteRegEx" : "Beer.",
      "year" : 1990
    }, {
      "title" : "Mosaic model for sensorimotor learning and control",
      "author" : [ "M. Haruno", "D.M. Wolpert", "M. Kawato" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Haruno et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Haruno et al\\.",
      "year" : 2001
    }, {
      "title" : "Mutual information, metric entropy and cumulative relative entropy risk",
      "author" : [ "D. Haussler", "M. Opper" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Haussler and Opper.,? \\Q1997\\E",
      "shortCiteRegEx" : "Haussler and Opper.",
      "year" : 1997
    }, {
      "title" : "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability",
      "author" : [ "Marcus Hutter" ],
      "venue" : null,
      "citeRegEx" : "Hutter.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2004
    }, {
      "title" : "Predictive representations of state",
      "author" : [ "M. Littman", "R. Sutton", "S. Singh" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Littman et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Littman et al\\.",
      "year" : 2002
    }, {
      "title" : "Information Theory, Inference, and Learning Algorithms",
      "author" : [ "David J.C. MacKay" ],
      "venue" : null,
      "citeRegEx" : "MacKay.,? \\Q2003\\E",
      "shortCiteRegEx" : "MacKay.",
      "year" : 2003
    }, {
      "title" : "Learning automata - a survey",
      "author" : [ "K. Narendra", "M.A.L. Thathachar" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics,",
      "citeRegEx" : "Narendra and Thathachar.,? \\Q1974\\E",
      "shortCiteRegEx" : "Narendra and Thathachar.",
      "year" : 1974
    }, {
      "title" : "A bayesian approach to online learning",
      "author" : [ "M. Opper" ],
      "venue" : "Online Learning in Neural Networks,",
      "citeRegEx" : "Opper.,? \\Q1998\\E",
      "shortCiteRegEx" : "Opper.",
      "year" : 1998
    }, {
      "title" : "Causality: Models, Reasoning, and Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl.,? \\Q2000\\E",
      "shortCiteRegEx" : "Pearl.",
      "year" : 2000
    }, {
      "title" : "Simple algorithmic theory of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Journal of SICE,",
      "citeRegEx" : "Schmidhuber.,? \\Q2009\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 2009
    }, {
      "title" : "Exploration and Inference in Learning from Reinforcement",
      "author" : [ "J. Wyatt" ],
      "venue" : "PhD thesis, Department of Artificial Intelligence, University of Edinburgh,",
      "citeRegEx" : "Wyatt.,? \\Q1997\\E",
      "shortCiteRegEx" : "Wyatt.",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Agent and environment can be conceptualized as two systems that exchange symbols in every time step [Hutter, 2004]: the symbol issued by the agent is an action, whereas the symbol issued by the environment is an observation.",
      "startOffset" : 100,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "A suitable measure (in terms of its information-theoretic interpretation) is readily provided by the KL-divergence [MacKay, 2003].",
      "startOffset" : 115,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "The main result of this paper is that this extension is only possible if we consider the special syntax of actions in probability theory as it has been suggested by proponents of causal calculus [Pearl, 2000].",
      "startOffset" : 195,
      "endOffset" : 208
    }, {
      "referenceID" : 2,
      "context" : "For reference, see Haussler and Opper [1997], Opper [1998].",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "For reference, see Haussler and Opper [1997], Opper [1998]. It is clear that P̃ is just the Bayesian mixture over the agents Pm.",
      "startOffset" : 19,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "In particular, we will use the formalism developed by Pearl [2000], because it suits the needs to formalize interactions in systems and has a convenient notation—compare Figures 1a & b.",
      "startOffset" : 54,
      "endOffset" : 67
    } ],
    "year" : 2009,
    "abstractText" : "Explaining adaptive behavior is a central problem in artificial intelligence research. Here we formalize adaptive agents as mixture distributions over sequences of inputs and outputs (I/O). Each distribution of the mixture constitutes a ‘possible world’, but the agent does not know which of the possible worlds it is actually facing. The problem is to adapt the I/O stream in a way that is compatible with the true world. A natural measure of adaptation can be obtained by the KullbackLeibler (KL) divergence between the I/O distribution of the true world and the I/O distribution expected by the agent that is uncertain about possible worlds. In the case of pure input streams, the Bayesian mixture provides a well-known solution for this problem. We show, however, that in the case of I/O streams this solution breaks down, because outputs are issued by the agent itself and require a different probabilistic syntax as provided by intervention calculus. Based on this calculus, we obtain a Bayesian control rule that allows modeling adaptive behavior with mixture distributions over I/O streams. This rule might allow for a novel approach to adaptive control based on a minimum KLprinciple.",
    "creator" : "LaTeX with hyperref package"
  }
}