{
  "name" : "1507.00418.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "No-Regret Learning in Repeated Bayesian Games",
    "authors" : [ "Jason Hartline", "Vasilis Syrgkanis Éva Tardos" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 7.\n00 41\n8v 1\n[ cs\n.G T\n] 2\nJ ul\n2 01\n5\nRecent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal welfare. This work provides two main technical results that lift this conclusion to games of incomplete information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public. Second, no-regret learning dynamics converge to Bayesian coarse correlated equilibrium in these incomplete information games. These results are enabled by interpretation of a Bayesian game as a stochastic game of complete information."
    }, {
      "heading" : "1 Introduction",
      "text" : "A recent confluence of results from game theory and learning theory gives a simple explanation for why good outcomes in large classes of games can be expected, in spite of the inherent complexity of these games and their equilibria. Many of games of interest in the computer science literature, from traffic routing to auctions, model repeated interactions. Nash equilibria of the game being repeated correspond to a stable outcome of the repeated game dynamic. However, it is well known that in most games natural game play does not lead to such stable outcomes (Nash equilibria), under any definition of “natural play”.\nBlum et al. [3] suggested modeling game player as no-regret learners. Modeling game participants as no-regret learners is very appealing for a number of reasons. There are a number of simple and natural algorithm that guarantee vanishing regret for a player. Any of a number of multi-armed bandit learning algorithms can guarantee that no-regret in the limit. No-regret learning outcomes circumvent the computational difficulty of finding Nash equilibria in games, and can do this with the minimal feedback typically available in large games arising in most online scenarios, such as online auction platforms: just the player’s own payoff. When repeated interaction is between the exact same set of players, the limit points of noregret learning form the coarse correlated equilibria of the game, an equilibrium notion that relaxes the notion of Nash equilibrium.\nTo measure the quality of outcomes in games Koutsoupias and Papadimitriou [8] introduced the Price of anarchy, the ratio of the quality of the worst Nash equilibrium over a socially optimal solution focusing on pure Nash equilibria of full information games. Price of anarchy results have been shown for many games from routing through simple auctions. Roughgarden [10] identifies a simple smoothness condition, exhibited by many games of interest, that implies that all Nash equilibria have near-optimal welfare. Syrgkanis and Tardos [13] extended this to mechanisms.\nOver the last decade the work on understanding the quality of outcomes in games has been extended in two orthogonal directions. Bayesian games model really large games, situations when players do not know the exact identities and properties of all opponents, rather need to make decisions optimizing their expected utility based on the distribution of possible opponents. Roughgarden [11] and Syrgkanis [12] showed that price of anarchy results obtained via the smoothness framework also extend to Bayesian versions of the games. The second direction concerns full information games, and is extending the concept\nof game outcomes to learning outcomes. Blum et al. [3] introduced the price of total anarchy for the worst case welfare at a limit point of no-regret learning compared to the socially optimum outcome. Roughgarden [10] showed that price of anarchy analysis obtained via the smoothness framework extends to such learning outcomes, bounding also the price of total anarchy in full information games.\nOur results. In this paper we consider learning outcomes in Bayesian games. Many important games model repeated interactions between an uncertain set of participants. Sponsored search, and more generally, online ad-auction market places, are important examples of such games. Platforms are running millions of auctions, with each individual auction slightly different, and of only very small value, but such market places have high enough volume to be the financial basis of large industries. This online auction environment is best modeled by a repeated Bayesian game: the auction game is repeated over time, with the set of participants slightly different each time, depending on many factors from budgets of the players through subtle differences in the opportunities.\nOur main result is to show that the price of anarchy bounds achieved via the smoothness property of games or mechanisms extends to results on the quality of learning outcomes in Bayesian games. We achieve this by interpreting a Bayesian game as a stochastic game of complete information: there is a large population of possible participants. The game is played by randomly selecting players from the player pool, where players know only the distribution of participants, and not the actual set selected, when choosing their strategies. Concretely, we show two results. First, we show that if all players use a form of no-regret learning in the stochastic game just defined, then the outcome converges to a form of Bayesian coarse correlated equilibrium. There has been several different notions of correlated equilibrium introduced for incomplete information games, the version we use here is the analogue of the Bayesian correlated equilibrium introduced by Forges [6], and is the coarse correlated equilibrium of the corresponding stochastic full information game. Second, we show that if a game or mechanism is smooth, then the corresponding stochastic game is also smooth with the same smoothness parameters. Putting these two results together, our main contribution is to show that the price of anarchy results obtained via the smoothness technique extend to outcomes of no-regret learning in Bayesian games."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "This section describes a general game theoretic environment which includes auctions and resource allocation mechanisms. For this general environment we review the results from the literature for analyzing the social welfare that arises from no-regret learning dynamics in repeated game play. The subsequent sections of the paper will generalize this model and these results to Bayesian games, a.k.a., games of incomplete information.\nGeneral Game Form. A general gameM is specified by a mapping from a profile a ∈ A ≡ A1×· · ·×An of allowable actions of players to an outcome. Player i’s utility in this game is determined by a profile of individual values v ∈ V ≡ V1 × · · · × Vn and the (implicit) outcome of the game and is denoted U M i (a; vi). In games with a social planner or principal who does not take an action in the game, the utility of the principal is RM(a). In many games of interest, such as auctions or allocation mechanisms, the utility of the principal is the revenue from payments from the players. We will use the term mechanism and game interchangeably.\nIn the complete information game the valuation profile of the players is common knowledge. Subsequent sections will consider games of incomplete information in the independent private value model, i.e., where player i’s value is known only privately to player i.\nSocial Welfare. We will be interested in analyzing the quality of the outcome of the game as defined by the social welfare, which is the sum of the utilities of the players and the principal. We will denote by SWM(a; v) = ∑\ni∈[n] U M i (a; vi)+R M(a) the expected social welfare of mechanism M under a randomized action profile a. For any valuation profile v ∈ V we will denote the optimal social welfare, i.e, the maximum over outcomes of the game of the sum of utilities, by Opt(v).\nNo-regret Learning and Coarse Correlated Equilibria. For complete information games, i.e., fixed valuation profile v, Blum et al. [3] analyzed repeated play of players using no-regret learning algorithms, and showed that this play converges to a relaxation of Nash equilibrium, namely, coarse correlated equilibrium.\nDefinition 1 (no-regret). A player achieves no-regret in a sequence of play a1, . . . , aT if his regret against any fixed strategy a′i vanishes to zero:\nlim T→∞\n1\nT\nT ∑\nt=1\nUMi (a ′ i, a t −i; vi)− U M i (a t; vi) = 0. (1)\nDefinition 2 (Coarse correlated equilibrium - CCE). A randomized action profile a ∈ ∆(A) is a coarse correlated equilibrium of a (static) complete information game if for every player i and a′i ∈ Ai:\nEa [Ui(a;vi)] ≥ Ea [Ui(a ′ i, a−i;vi)] (2)\nTheorem 3 ([3]). The empirical distribution of actions of any no-regret sequence in a repeated game converges to the set of CCE of the static game.\nPrice of Anarchy of CCE. Roughgarden [10] gave a unifying framework for comparing the social welfare, under various equilibrium notions including coarse correlated equilibrium, to the optimal social welfare by defining the notion of a smooth game. This framework was extended to games like auctions and allocation mechanisms by Syrgkanis [13].\nDefinition 4 (Smooth Mechanism [13]). A mechanism M is (λ, µ)-smooth for some λ, µ ≥ 0, if for any valuation profile v ∈ V and for each player i ∈ [n] there exists a randomized action a∗i (v), such that for any action profile a ∈ A:\n∑\ni∈[n] U M i (a ∗ i (v), a−i; vi) ≥ λ ·Opt(v) − µ · R M(a) (3)\nMany important games and mechanisms satisfy this smoothness definition for various parameters of λ and µ (see Figure 1); the following theorem shows that the welfare of any coarse correlated equilibrium in any of these games is nearly optimal.\nTheorem 5 (Efficiency of CCE [13]). If a mechanism is (λ, µ)-smooth then the social welfare of any course correlated equilibrium at least λmax{1,µ} of the optimal welfare, i.e., the price of anarchy satisfies PoA ≤ max{1,µ} λ .\nPrice of Anarchy of No-regret Learning. Following Blum et al.[3], Theorem 3 and Theorem 5 imply that no-regret learning dynamics have near-optimal social welfare.\nCorollary 6 (Efficiency of no-regret dynamics [13]). If a mechanism is (λ, µ)-smooth then the average welfare of any no-regret dynamics of the repeated game with a fixed player set and valuation profile, achieves average social welfare at least λmax{1,µ} of the optimal welfare, i.e., the price of anarchy satisfies PoA ≤ max{1,µ}\nλ .\nImportantly, Corollary 6 holds the valuation profile v ∈ V fixed throughout the repeated game play. The main contribution of this paper is in extending this theory to games of incomplete information, e.g., where the values of the players are drawn at random in each round of game play."
    }, {
      "heading" : "3 Learning in Repeated Bayesian Game",
      "text" : "We consider the following setting of a repeated Bayesian game: there exist n equally sized populations of players. For each i ∈ [n] population Pi consists of a finite set of r players. Each player q ∈ Pi has some valuation Vi(q). We denote with Fi ∈ ∆(Vi) the empirical distribution of values in population Pi and with vi a random sample from Fi. In other words, the value of a randomly chosen player from population Pi is distributed according to Fi.\nWe consider the repeated game where at each iteration one player q from each population is sampled uniformly and independently from other populations. The set of chosen players then participate in an instance of a mechanism M. We assume that each player qi ∈ Pi, uses some no-regret learning rule to play in this repeated game. An equivalent way of viewing the game is that the set of players is fixed, but their valuations are changing and are drawn independently and identically from distributions Fi at each iteration. Moreover, each player uses a no-regret algorithm for each possible valuation he can have. However, throughout the paper we will adopt the population interpretation of the model. In Definition 7, we describe the structure of the game and our notation more elaborately.\nDefinition 7. The repeated Bayesian game of M proceeds as follows. In iteration t:\n1. Each player q ∈ Pi in each population i picks an action a t iq. For each population i we denote with\nµti : A Pi i a function that takes as input a player q ∈ Pi and outputs his action µ t i(q) = a t iq.\n2. From each population i one player qti ∈ Pi is selected uniformly at random. Let q t = (qt1, . . . , q t n) be\nthe chosen profile of players and µt(qt) = (µt1(q t 1), . . . , µ t n(q t n)) be the profile of chosen actions.\n3. Each player qti participates in an instance of game M, in the role of player i ∈ [n], with action µ t i(q t i)\nand experiences a utility of UMi (µ t(qt);Vi(q t i)). The rest of the players experience zero utility.\nRemark. We point out that for each player in each population to achieve no-regret he does not need to know the distribution of values in the remainder populations. There exist algorithms that can achieve the\nno-regret property and simply require an oracle that returns the utility of a player at each iteration. Thus all we need to assume is that each player receives as feedback his utility at each iteration.\nRemark. We also note that our results would extend to the case where at each period multiple matchings are sampled independently and players potentially participate in more than one instance of the mechanism M and potentially with different players from the remaining population. The only thing that the players need to observe in such a setting is their average utility that resulted from their action µti(q) ∈ Ai from all the instances that they participated at the given period. Such a scenario seems an appealing model in online ad auction marketplaces where players receive only average utility feedback from their bids.\nBayesian Price of Total Anarchy In this repeated game setting we want to compare the average social welfare of any sequence of play where each player uses a vanishing regret algorithm versus the average optimal welfare. Moreover, we want to quantify the worst-case such average welfare over all possible valuation distributions within each population:\nsup F1,...,Fn lim sup T→∞\n1 T ∑T t=1 Opt(V (q t)) 1 T ∑T t=1 SW M(µt(qt);V (qt)) (4)\nWe will refer to this quantity as the Bayesian price of total anarchy. Since the valuation of the chosen player is re-drawn independently at every time step, the average optimal welfare will converge almost surely to the expected ex-post optimal welfare Ev[Opt(v)] of the static incomplete information setting.\nOur main theorem is that smooth mechanisms guarantee approximate efficiency even in repeated Bayesian games and not only in the fixed stationary environment.\nTheorem 8 (Main Theorem). If a mechanism is (λ, µ)-smooth then the Bayesian price of total anarchy of any vanishing regret sequence of play of the repeated Bayesian game is at most max{1,µ} λ .\nRoadmap of the proof. In Section 4 we analyze static Bayesian games and define Bayesian generalizations of coarse correlated equilibria. The repeated Bayesian game can be viewed as a repetition of a static game of incomplete information. In Section 5, we show that any vanishing regret sequence of play of the repeated Bayesian game, will converge almost surely to the Bayesian version of a coarse correlated equilibrium of the static incomplete information game. Therefore the Bayesian price of total anarchy will be upper bounded by the efficiency of guarantee of any Bayesian coarse correlated equilibrium. Finally, in Section 6 we show that the price of anarchy bound of smooth mechanisms directly extends to Bayesian coarse correlated equilibria, thereby providing an upper bound on the Bayesian price of total anarchy of the repeated game."
    }, {
      "heading" : "4 Bayesian Coarse Correlated Equilibria",
      "text" : "Observe that the game that is being repeated at each time step can be viewed as Bayesian game among a set of n players where the valuation of each player is private and known only by the player and is drawn from a distribution Fi. Under this interpretation, prior to making her decision on which action to play, each player learns his own valuation vi and nothing else. A player’s strategy in this Bayesian game, is a mapping si : A Vi i from a valuation vi ∈ Vi to an action ai ∈ Ai. We will denote with Σi = A Vi i the strategy space of each player and with Σ = Σ1 × . . .×Σn. In an incomplete information game we want to measure welfare as compared to the expected ex-post optimal social welfare Ev[Opt(v)].\nWhen the game that is being repeated is a complete information game where no player has any private information then Theorem 3 gives a strong connection between no-regret learning and the notion of a\ncoarse correlated equilibrium of the static game. In this work we show an analogue of this connection for the case of Bayesian games.\nDefining a coarse correlated equilibrium in an incomplete information game (and its sibling notion of correlated equilibrium) is not such a straightforward task and several notions have been introduced (c.f. Forges [6], Bergemann and Morris [2], Caragiannis et al. [4]). The Bayesian coarse correlated equilibrium that we define here is the analogue of the Bayes correlated equilibrium defined in Section 4.1 of [6] and corresponds to the coarse correlated equilibrium of a corresponding complete information game defined as follows: the utility of a player i in the complete information game is his ex-ante expected utility from the mechanism. The strategy space of player i is Σi = A Vi i . For a strategy profile s ∈ Σ, the utility of a player in the complete information game is then:\nUexi (s) = Ev [Ui(s(v);vi)] (5)\nA Bayes-CCE is simply a CCE of this complete information game. For completeness we provide the formal definition below.\nDefinition 9 (Bayesian coarse correlated equilibrium - Bayes-CCE). A randomized strategy profile s ∈ ∆(Σ) is a Bayesian coarse correlated equilibrium if for every s′i ∈ Σi:\nEsEv [Ui(s(v);vi)] ≥ EsEv [Ui(s ′ i(vi), s−i(v−i);vi)] (6)\nWe will refer to the worst-case ratio of the expected optimal social welfare over the expected social welfare of any Bayes-CCE as Bayes-CCE-PoA\nRemark. In the above definitions we assumed that the space Σ = AV11 × . . . × A Vn n admits probability distributions. This is not always obvious, since this is a space of functions. When the valuation spaces Vi are finite and probability distributions on the action spaces are well defined, then this is true, and hence in the above and subsequence definitions that involve probability distributions over functions AVii we will assume that the valuation space Vi is finite. For continuous valuation spaces, the issue is a special case of the problem of defining mixed strategies in extensive form games with infinite information sets and the reader is directed to the classic work of Aumann [1] for one approach.\nRemark. We also point out that our definition of Bayes-CCE is inherently different and more restricted than the one defined in Caragiannis et al. [4]. There, a Bayes-CCE is defined as a joint distribution D over V ×A, such that if (v, a) ∼ D then for any vi ∈ Vi and a ′ i(vi) ∈ Ai:\nE(v,a) [Ui(a; vi)] ≥ E(v,a) [Ui(a ′ i(vi), a−i; vi)] (7)\nThe main difference is that the product distribution defined by a distribution in ∆(Σ) and the distribution of values, cannot produce any possible joint distribution over (V ,A), but the type of joint distributions are restricted to satisfy a conditional independence property described by Forges [6]. Namely that a player i’s action is conditionally independent of some other player j’s value, given player i’s type. Such a conditional independence property seems essential for the guarantees that we will present in this work to extend to a Bayes-CCE and hence do not seem to extend to the notion given in Caragiannis et al. [4].\nHowever, as we will show in Section 5, the no-regret dynamics that we analyze, which are mathematically equivalent to the dynamics in Caragiannis et al. [4], do converge to the smallest set of Bayes-CCE that we define and for which our efficiency guarantees will extend. This extra convergence property is not needed when the mechanism satisfies the stronger semi-smoothness property defined in [4] and thereby was not needed to show efficiency bounds in their setting."
    }, {
      "heading" : "5 Convergence of Bayesian No-Regret to Bayes-CCE",
      "text" : "For any given sequence of play of the repeated Bayesian game, which we defined in Definition 7, we define the following sequence of strategy-value pairs (st, vt) where s : V → A:\nst(v) =\n{\nµt(qt) if V (qt) = v\narbitrary a ∈ A o.w. (8)\nand vt = V (qt). Then observe that all that matters to compute the average social welfare of the game for any given time step T , is the empirical distribution of (s, v), up till time step T , denoted as DT , i.e. if (sT ,vT ) is a random sample from DT :\n1\nT\nT ∑\nt=1\nSW (µt(qt);V (qt)) = E(sT ,vT ) [ SW (sT (vT );vT ) ]\n(9)\nLemma 10 (Almost sure convergence to Bayes-CCE). Let D ∈ ∆(Σ × V) be a joint distribution of (strategy, valuation) profile pairs. Consider a sequence of play of the random matching game, where each player uses a vanishing regret algorithm and let DT be the empirical distribution of strategy, valuation profile pairs up till time step T . Suppose that there exists a subsequence of {DT }T that converges in distribution to D. Then, almost surely, D is a product distribution, i.e. D = Ds×Dv, with Ds ∈ ∆(Σ) and Dv ×∆(V) such that Dv = F and Ds ∈ Bayes-CCE of the static incomplete information game with distributional beliefs F .\nProof. We will denote with ri(a ∗ i , a; vi) = Ui(a ∗ i , a−i; vi)− Ui(a; vi),\nthe regret of a player for action a∗i at action profile a, when he has value vi. For a q ∈ Pi let x t i(q) = 1qti=q. Since the sequence has vanishing regret for each player in population Pi, it must be that for any qi ∈ Pi and any s∗i : V → A:\nT ∑\nt=1\nxti(qi) · ri ( s∗i (Vi(qi)), µ t(qi, q t −i);Vi(qi) ) ≤ o(T )\nSumming over all players in Pi we get:\nT ∑\nt=1\nri ( s∗i (Vi(q t i)), µ t(qt);Vi(q t i) ) ≤ o(T )\nUsing the definition of st from Equation (8) and vti = Vi(q t i), we can rewrite the above as:\nT ∑\nt=1\nri ( s∗i (v t i), s t(vt); vti ) ≤ o(T )\nFor any fixed T , let DTs ∈ ∆(Σ) denote the empirical distribution of s t and let s be a random sample from DTs . For each s ∈ Σ, let Ts ⊂ [T ] denote the time steps such that s t = s for each t ∈ Ts. Then we get:\nEs\n[\n1\n|Ts|\n∑\nt∈Ts\nri ( s∗i (v t i), s(v t); vti )\n]\n≤ o(T )\nT\nFor any s ∈ Σ, let Ts,v = {t ∈ Ts : v t = v}. Then we can re-write:\nEs\n[\n∑\nv∈V\n|Ts,v|\n|Ts| ri (s\n∗ i (vi), s(v); vi)\n]\n≤ o(T )\nT (10)\nNow we observe that |Ts,v| |Ts| is the empirical frequency of the valuation vector v ∈ V , when filtered at time steps where the strategy vector was s. Since at each time step t the valuation vector vt is picked independently from the distribution of valuation profiles F , this is the empirical frequency of Ts independent samples from F .\nBy standard arguments from empirical processes theory, if Ts → ∞ then this empirical distribution converges almost surely to the distribution F . On the other hand if Ts doesn’t go to ∞, then the empirical frequency of strategy s vanishes to 0 as T → ∞ and therefore has measure zero in the above expectation as T → ∞. Thus for any convergent subsequence of {DT }, if D is the limit distribution, then if s is in the support of D, then almost surely the distribution of v conditional on strategy s is F . Thus we can write D as a product distribution Ds ×F .\nMoreover, if we denote with v the random variable that follows distribution F , then the limit of Inequality (10) for any convergent subsequence, will give that:\na.s.: EsEv [ri (s ∗ i (vi), s(v);vi)] ≤ 0\nThus DTs is in the set of Bayes-CCE of the static incomplete incomplete information game among n players, where the valuation profile is drawn from F .\nTheorem 11. The Bayesian price of total anarchy is upper bounded by the Bayesian price of anarchy of Bayesian coarse correlated equilibria."
    }, {
      "heading" : "6 Efficiency of Smooth Mechanisms at Bayes Coarse Correlated",
      "text" : "Equilibria\nIn this section we show that smoothness of a mechanismM implies that any Bayes-CCE of the incomplete information setting achieves at least λmax{1,µ} of the expected optimal welfare. To show this we will adopt the interpretation of Bayes-CCE that we used in the previous section, as coarse correlated equilibria of a more complex normal form game. We can interpret this complex normal form game as the game that arises from a complete information mechanism Mex. The main theorem that we will show is that whenever mechanism M is (λ, µ)-smooth, then also mechanism Mex is (λ, µ)-smooth. Then we will invoke a theorem of [13, 10], which shows that any coarse correlated equilibria of a mechanism in the complete information setting achieve at least λmax{1,µ} of the optimal welfare.\nMore formally, we define the mechanism Mex where the action space of a player is the set of functions from a type to an action of the complete information setting, Aexi , (Vi → Ai). The utility of a player in this mechanism is his ex-ante expected utility, Uexi (s) = Ev [ UMi (s(v)) ]\n. Observe that the utility Uexi does not depend on any private information of the player. It only depends on the underlying utility function UMi and on the distributions Fi which are all common knowledge. The set of possible outcomes in this ex-ante game corresponds to the set of mappings from an allocation profile to an outcome in the underlying mechanism M. The optimal welfare of this game, is then the expected ex-post optimal welfare Opt\nex = Ev [Opt(v)]. The main result of this section is to show that if a mechanism M is smooth according to definition 4 then Mex is also a smooth mechanism and therefore by [13, 10] every coarse correlated equilibria of Mex,\nwhich corresponds to a Bayes-CCE of the incomplete information game, will achieve at least λmax{1,µ} of the expected optimal welfare.\nTheorem 12 (From complete information to Bayesian smoothness). If a mechanism M is (λ, µ)-smooth, then for any vector of independent valuation distributions F = (F1, . . . ,Fn), the ex-ante complete information mechanism Mex defined by the incomplete information setting is also (λ, µ)-smooth.\nProof. Consider the following randomized deviation for each player i that depends only on the information that he has which is his own value vi: He random samples a valuation profile w ∼ F . Then he plays according to the randomized action s∗i (vi,w−i), i.e., the player deviates using the randomized action guaranteed by the smoothness property for his true type vi and the random sample of the types of the others w−i. Since this is not a profitable deviation for player i:\nEv,w [Ui(s ∗ i (vi,w−i), s−i(v−i);vi)] = Ev,w [Ui(s ∗ i (wi,w−i), s−i(v−i);wi)]\n= Ev,w [Ui(s ∗ i (w), s−i(v−i);wi)] ,\nwhere the first equation is an exchange of variable names and regrouping using independence. Summing over players and using smoothness of M:\n∑\ni∈[n]\nUexi (s ∗ i , s−i) = Ev,w\n\n\n∑\ni∈[n]\nUi(s ∗ i (w), s−i(v−i);wi)\n\n\n≥ Ev,w [ λOpt(w)− µRM(s(v)) ] = λEw [Opt(w)]− µR ex(s)\nCorollary 13. Every Bayes-CCE of the incomplete information setting of a smooth mechanism M, achieves welfare at least λmax{1,µ} of the expected optimal welfare."
    }, {
      "heading" : "7 Finite Time Analysis and Convergence Rates",
      "text" : "In the previous section we argued about the limit average efficiency of the game as time goes to infinity. In this section we analyze the convergence rate to Bayes-CCE and we show approximate efficiency results even for finite time, when players are allowed to have some ǫ-regret.\nTheorem 14. Consider the repeated matching game with a (λ, µ)-smooth mechanism. Suppose that for any T ≥ T 0, each player in each of the n populations has regret at most ǫ\nn . Then for every δ and ρ, there\nexists a T ∗(δ, ρ), such that for any T ≥ min{T 0, T ∗}, with probability 1− ρ:\n1\nT\nT ∑\nt=1\nSW (st(vt); vt) ≥ λ\nmax{1, µ} Ev [Opt(v)] − δ − µ · ǫ (11)\nMoreover, T ∗(δ, ρ) ≤ 54·n 3·|Σ|·|V|2·H3\nδ3 log\n(\n2 ρ\n)\n."
    }, {
      "heading" : "A Proof of Theorem 11",
      "text" : "Let D ∈ ∆(Σ × V) be a joint distribution, such that there is a subsequence of {DT }T , converging in distribution to D. Then by Lemma 10, almost surely, D is a product distribution, i.e. D ∈ ∆(Σ) ×∆(V) and that the marginal on V is equal to F and the marginal on Σ is a Bayes-CCE of the static incomplete information game with distributional beliefs F .\nTherefore, if ρ is the Bayes-CCE − PoA of the mechanism, and if (s,v) is a random sample from D, then almost surely:\nEs,v [SW (s(v);v)] ≥ 1\nρ Ev [Opt(v)] (12)\nThus the limit average social welfare of any convergent subsequence will be at least 1 ρ Ev [Opt(v)], which then implies that almost surely:\nlim inf T→∞\n1\nT\nT ∑\nt=1\nSW (µt(qt);V (qt)) ≥ 1\nρ Ev [Opt(v)] =\n1 ρ lim T→∞ 1 T\nT ∑\nt=1\nOpt(V (qt))\nThus for any non-measure zero event, for any ǫ, there exists a f(ǫ) such that for any T ≥ f(ǫ):\n1\nT\nT ∑\nt=1\nSW (µt(qt);V (qt)) ≥ 1\nρ\n1\nT\nT ∑\nt=1\nOpt(V (qt))− ǫ\nWith no loss of generality we can assume that Ev [Opt(v)] > 0 (o.w. valuations are all zero and theorem holds trivially). The average optimal welfare converges almost surely to Ev [Opt(v)], we get that for any non-measure zero event, there exists a g(δ) such that for T ≥ g(δ), 1 T ∑T t=1 Opt(V (q\nt)) is bounded away from zero. Thereby, we can turn the additive error into a multiplicative one, i.e. for any non-measure zero event and for any ǫ′ there exists w(ǫ′) such that for any T ≥ w(ǫ′):\n1\nT\nT ∑\nt=1\nSW (µt(qt);V (qt)) ≥ 1\nρ (1 + ǫ′)\n1\nT\nT ∑\nt=1\nOpt(V (qt))\nThis implies that almost surely:\nlim sup T→∞\n1 T ∑T t=1 Opt(V (q t)) 1 T ∑T t=1 SW (µ t(qt);V (qt)) ≤ ρ = Bayes-CCE-PoA"
    }, {
      "heading" : "B Proof of Theorem 14",
      "text" : "Fix a population i and a bayesian strategy s∗i ∈ Σi, as well as a bayesian strategy profile s ∈ Σ. For shorter notation we will denote:\nπi(s ∗ i , s, v) = Ui(s ∗ i (vi), s−i(v−i); vi).\nFor a time step T , let pT (s) = |Ts| T be the empirical distribution of a bayesian strategy s and with pT (v|s) = |Ts,v| |Ts| be the empirical distribution of values conditional on a bayesian strategy s. The average utility of a population i up till time step T , when switching to a fixed bayesian strategy s∗i , can be written as:\n1\nT\nT ∑\nt=1\nπ(s∗i , s t, vt) =\n∑\ns∈Σ\npT (s) ∑\nv∈V\npT (v|s) · πi(s ∗ i , s, v) (13)\nWe will show that for any s∗i , there exists a T ∗(δ, ρ) such that for any T ≥ T ∗(δ, ρ), with probability\n1− ρ: ∑\ns∈Σ\npT (s) ∑\nv∈V\npT (v|s) · πi(s ∗ i , s, v) ≥\n∑\ns∈Σ\npT (s)Ev [πi(s ∗ i , s,v)]− δ (14)\nwhere v is a random variable drawn from the distribution of valuation profiles F . We will denote with p(v) the density function implied by distribution F .\nIn what follows we will denote with H = maxi∈[n],vi∈Vi,xi∈Xi vi(xi) the maximum possible value of any player. Thus observe that the utility of any player is upper bounded by H and that the revenue collected by any player at equilibrium is upper bounded by H .\nFor a time period T , let G = {s ∈ Σ : pT (s) ≥ ζ}. Then observe that:\n∑\ns∈Σ\npT (s) ∑\nv∈V\n( pT (v|s)− p(v) ) · πi(s ∗ i , s, v) ≥\n∑\ns∈G\npT (s) ∑\nv∈V\n( pT (v|s)− p(v) ) · πi(s ∗ i , s, v)− ζ · |Σ| ·H\nObserve that for any s ∈ G, |Ts| ≥ ζ · T . Thus p T (v|s) is the empirical mean of at least ζ · T independent random samples of a Bernoulli trial with success probability p(v). Hence, by Hoeffding bounds, we have that |pT (v|s)−p(v)| ≤ t with probability at least 1−2 exp ( −2 · ζ · T · t2 )\n. Thus with that much probability we get:\n∑\ns∈Σ\npT (s) ∑\nv∈V\n( pT (v|s)− p(v) ) · πi(s ∗ i , s, v) ≥ −t · |V| ·H − ζ · |Σ| ·H\nBy setting t = δ2·|V|·H , ζ = δ 2·|Σ|·H and T ∗(δ, ρ) = 16·|Σ|·|V|\n2·H3\nδ3 log\n(\n2 ρ\n)\n, we get the claimed property in\nEquation (14). Now suppose that after time step T 0 each player in a population has regret ǫ/n. Thus the average utility of the population is at least the utility from switching to any fixed bayesian strategy s∗i , minus an error term of ǫ/n:\n∑\ns∈Σ\npT (s) ∑\nv∈V\npT (v|s)πi(si, s, v) ≥ ∑\ns∈Σ\npT (s) ∑\nv∈V\npT (v|s)πi(s ∗ i , s, v)−\nǫ n (15)\nFrom the previous analysis, for any T ≥ min{T 0, T ∗( 2δ3·n , ρ)}, we get that with probability 1− ρ:\n∑\ns∈Σ\npT (s) ∑\nv∈V\npT (v|s)πi(si, s, v) ≥ ∑\ns∈Σ\npT (s)Ev [πi(s ∗ i , s,v)]−\n2δ 3n − ǫ n (16)\nSumming over all populations and using the bayesian smoothness property of the mechanism from Theorem 12, we have that with probability 1− ρ:\n∑\ns∈Σ\npT (s) ∑\nv∈V\npT (v|s) ∑\ni\nπi(si, s, v) ≥ ∑\ns∈Σ\npT (s) (λEv [Opt(v)] − µR ex(s))−\n2δ\n3 − ǫ\n≥ λEv [Opt(v)] − µ ∑\ns∈Σ\npT (s)Rex(s)− 2δ\n3 − ǫ\nTo conclude the theorem we observe that since for any s ∈ Σ, |pT (v|s)− p(v)| ≤ δ3·n·|V|·H , we get that:\nRex(s) = ∑\nv∈V\np(v)R(s(v)) ≤ ∑\nv∈V\npT (v|s)R(s(v)) + δ\n3 (17)\nSince, the revenue collected by a player at any action in the support of an equilibrium is at most H . By the latter we can combine the revenue on the right hand side with the utility on the left hand side. We can also bound the remaining (µ − 1) of the revenue, by (µ − 1) of the average welfare minus ǫ, since each player in each population can always drop out of the auction and therefore his average utility at an ǫ n -regret sequence must be at least − ǫ n .\nHence, we get that:\n∑\ns∈Σ\npT (s) ∑\nv∈V\npT (v|s)SW (s(v); v) ≥ λ\nmax{1, µ} Ev [Opt(v)]− δ − µ · ǫ (18)\nThus choosing T ∗(ρ, 2δ3·n ) = 54·n3·|Σ|·|V|2·H3 δ3 log\n(\n2 ρ\n)\n, we get the conditions of the theorem."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>Recent price-of-anarchy analyses of games of complete information suggest that coarse correlated<lb>equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal<lb>welfare. This work provides two main technical results that lift this conclusion to games of incomplete<lb>information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian games follows directly from<lb>the smoothness-based proof of near-optimal welfare in the same game when the private information<lb>is public. Second, no-regret learning dynamics converge to Bayesian coarse correlated equilibrium in<lb>these incomplete information games. These results are enabled by interpretation of a Bayesian game<lb>as a stochastic game of complete information.",
    "creator" : "LaTeX with hyperref package"
  }
}