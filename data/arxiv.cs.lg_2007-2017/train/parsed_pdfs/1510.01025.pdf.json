{
  "name" : "1510.01025.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Quadratic Optimization with Orthogonality Constraints: Explicit Lojasiewicz Exponent and Linear Convergence of Line-Search Methods",
    "authors" : [ "Huikang Liu", "Weijie Wu", "Anthony Man–Cho" ],
    "emails" : [ "hkliu@se.cuhk.edu.hk", "wwu@se.cuhk.edu.hk", "manchoso@se.cuhk.edu.hk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 0.\n01 02\n5v 1\n[ m\nat h.\nO C"
    }, {
      "heading" : "1 Introduction",
      "text" : "Quadratic optimization problems with orthogonality constraints constitute an important class of matrix optimization problems that have found applications in areas such as combinatorial optimization, data mining, dynamical systems, multivariate statistical analysis, and signal processing, just to mention a few (see, e.g., [6, 13, 3, 8, 10, 16, 21, 25]). A prototypical form of such problems is\nmin X∈St(m,n)\n{ F (X) = tr ( XTAXB )} , (1)\nwhere St(m,n) = { X ∈ Rm×n | XTX = In }\n(with m ≥ n and In being the n×n identity matrix) is the compact Stiefel manifold and A ∈ Sm, B ∈ Sn are given symmetric matrices. Despite its simplicity, Problem (1) already has many applications, a most prominent of which is Principal\n∗Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong. E–mail: hkliu@se.cuhk.edu.hk\n†Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong. E–mail: wwu@se.cuhk.edu.hk\n‡Department of Systems Engineering and Engineering Management, and, by courtesy, CUHK–BGI Innovation Institute of Trans–omics, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong. E–mail: manchoso@se.cuhk.edu.hk\nComponent Analysis (PCA). One of the algorithmic approaches for solving (1) is to apply linesearch methods on the manifold St(m,n). The update formula of this family of methods takes the form\nXk+1 = R (Xk, αkξk) for k = 0, 1, . . . , (2)\nwhere αk ≥ 0 is the step size, ξk is a search direction in the tangent space to St(m,n) at Xk, and R(Xk, ·) is a so-called retraction that maps a vector in the tangent space to St(m,n) at Xk into a point on St(m,n). In particular, the iterates produced by (2) are all feasible for Problem (1). Naturally, the choice of step sizes, search directions and the retraction will affect the convergence and efficiency of the resulting method. For the general problem of optimizing a smooth function over the Stiefel manifold (which includes Problem (1) as a special case), various choices have been proposed over the years, and the convergence properties of the resulting methods are relatively well understood; see, e.g., [1, 3, 4, 24, 7]. However, very little is known about the convergence rates of these methods, even when they are applied to the much more structured problem (1). Part of the difficulty is due to the fact that optimization problems over the Stiefel manifold are non-convex in general. This implies that much of the existing analysis machinery, which heavily exploits convexity, cannot be applied to such problems. Currently, convergence rates of linesearch methods for solving Problem (1) are established only under quite restrictive conditions. For instance, Absil et al. [3, Theorem 4.6.3] showed that when n = 1 and B = In = 1 (and hence Problem (1) corresponds to minimizing the Rayleigh quotient on the unit sphere in Rm), a certain line-search method will converge linearly to an eigenvector corresponding to the smallest eigenvalue λ of A, provided that λ is simple. More recently, Shamir [19] developed a stochastic line-search method for Problem (1) when n = 1, B = In = 1, and A is negative semidefinite. He showed that if the smallest eigenvalue λ is simple and certain boundedness assumptions hold, then his proposed method converges linearly to an eigenvector corresponding to λ. However, it is not clear how to extend the above results to handle the case where n > 1 and/or the multiplicity of λ is greater than one. On another front, Smith [20] showed that when used to optimize a smooth function over a Riemannian manifold, the method of steepest descent will converge linearly to a critical point if the function is strongly convex on the manifold. However, such a notion of convexity is much stronger than that on the Euclidean space. In particular, it is known that every smooth function that is convex on a compact Riemannian manifold (such as the Stiefel manifold) is constant [5]. Therefore, one cannot hope to obtain linear convergence results for Problem (1) using the convexity-based approach in [20]. Recently, there have been some endeavors to analyze the convergence rates of line-search methods for solving optimization problems over embedded submanifolds using the so-called Lojasiewicz inequality ; see, e.g., [2, 14, 17]. Although such an approach is extremely powerful, it has a severe limitation; namely, the exponent in the Lojasiewicz inequality is often hard to determine explicitly. Without the knowledge of such exponent, one cannot determine the exact rate of convergence of a given method. As it turns out, the Lojasiewicz exponent for general polynomial systems is known (see, e.g., [11]) and can in principle be applied to Problem (1). However, the exponent depends on the dimensions of the problem and leads only to very weak convergence rate results.\nIn view of the above discussion, our main contribution of this paper is to give a significantly sharper estimate of the Lojasiewicz exponent for the non-convex problem (1). In particular, it is independent of the dimensions of the problem. We achieve this by establishing a local Lipschitzian error bound for the (non-convex) set of critical points of Problem (1), which may be of independent interest. By combining our estimate of the Lojasiewicz exponent with a\nwell-established analysis framework in the literature [17], we conclude that a host of line-search methods for solving Problem (1) converge linearly to a critical point. It should be noted that our convergence result does not require any restriction on the eigenvalues of A and B. Thus, it is qualitatively different from those in [3, 19]. Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.\nBesides the notations introduced earlier, we shall use On to denote the set of n×n orthogonal matrices (in particular, we have On = St(n, n)); Diag(x1, . . . , xn) to denote the diagonal matrix with x1, . . . , xn on the diagonal; BlkDiag(A1, . . . , An) to denote the block diagonal matrix whose diagonal blocks are A1, . . . , An. Given a matrix Y ∈ R\nm×n and a non-empty closed set X ⊂ R m×n, we shall use dist(Y,X ) to denote the distance of Y to X ; i.e., dist(Y,X ) = minX∈X ‖X − Y ‖F . Other notations are standard."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 First-Order Optimality Condition and Descent Directions",
      "text" : "To begin, let us introduce some basic definitions and concepts. We view St(m,n) as an embedded submanifold of Rm×n with the inherited Riemannian metric 〈·, ·〉 given by 〈X,Y 〉 = tr ( XTY )\n. For any X ∈ St(m,n), the tangent space to St(m,n) at X is given by T (X) = {\nY ∈ Rm×n | XTY + Y TX = 0 } . The gradient of F (X) = tr ( XTAXB )\nis ∇F (X) = 2AXB, and its orthogonal projection onto T (X) is given by\ngradF (X) = ( Im −XX T ) ∇F (X) + 1\n2 X\n( XT∇F (X)−∇F (X)TX )\n= 2AXB −XXTAXB −XBXTAX.\nLet X = {X ∈ St(m,n) | gradF (X) = 0} be the set of critical points of Problem (1). The following proposition gives a characterization of X :\nProposition 1 Let X ∈ St(m,n) be given. Then, the following are equivalent:\n(i) gradF (X) = 0.\n(ii) ∇F (X)−X∇F (X)TX = 0.\n(iii) For any ρ > 0, Dρ(X) = ∇F (X)−X ( 2ρ∇F (X)TX + (1− 2ρ)XT∇F (X) ) = 0.\nProof The equivalence between (ii) and (iii) is established in [7, Lemma 2.1]. To prove the equivalence between (i) and (ii), observe that\ngradF (X) =\n(\nIm − 1\n2 XXT\n)\n∇F (X)− 1\n2 X∇F (X)TX\n=\n(\nIm − 1\n2 XXT\n)\n( ∇F (X)−X∇F (X)TX ) .\nNow, it remains to note that Im − (1/2)XX T is invertible. ⊔⊓ It is easy to verify that Dρ(X) ∈ T (X) for any ρ > 0. Moreover, as shown in [7, Lemma 3.1], −Dρ(X) is a descent direction at X ∈ St(m,n) for any ρ > 0. Hence, in the sequel, we shall focus on line-search methods that use −Dρ(·) as the search direction."
    }, {
      "heading" : "2.2 Retraction",
      "text" : "Another ingredient in line-search methods for optimizing over St(m,n) is a retraction:\nDefinition 1 (Retraction) A map R : ⋃\nX∈St(m,n){X} × T (X) → St(m,n) will be called a retraction, if for any fixed X ∈ St(m,n) and ξ ∈ T (X) it holds that ξ 7→ R(X, ξ) is continuous on T (X), and for all X ∈ St(m,n),\nlim T (X)∋ξ→0 ‖R(X, ξ)− (X + ξ)‖F ‖ξ‖F = 0. (3)\nVarious smooth retractions on the Stiefel manifold have been proposed in the literature. These include the polar decomposition-based retraction, the QR-decomposition-based retraction, the Cayley transform, and the Riemannian exponential mapping. We refer the reader to [3, 9] for details of these retractions. In Section 4, we shall conduct numerical experiments with these four retractions."
    }, {
      "heading" : "2.3 Step Sizes",
      "text" : "To complete the specification of a line-search method, it remains to choose the step sizes. This is done in the following:\nDefinition 2 (Armijo Point) Let γ > 0, β, c ∈ (0, 1) be given constants. The number\nα = max { βnγ | n ≥ 0, F (R (X,−βnγDρ(X)))− F (X) ≤ −cβ nγ∇F (X)TDρ(X) }\n(4)\nis called the Armijo point at X ∈ St(m,n) with parameters (γ, β, c).\nSince the smooth retraction (3) is a first-order approximation, the left hand side approximate the first-order derivative along −βnγDρ when m is large enough. Consequently, the Armijo point exists. We refer the reader to [17] for details.\nWe summarize the line-search method in Algorithm 1.\nAlgorithm 1 Line-Search Method on the Stiefel manifold\nRequire: Select X0 ∈ St(m,n), ρ > 0, β, c ∈ (0, 1). 1: for k = 0, 1, 2, . . . do 2: Calculate the descent direction −Dρ(Xk) at Xk. 3: Choose β̄k ≥ 1 and find the Armijo point αk at Xk with parameters (β̄k, β, c). 4: Set Xk+1 = R (Xk,−αkDρ(Xk)). 5: end for"
    }, {
      "heading" : "2.4 Convergence Analysis Framework for the Line-Search Method",
      "text" : "To analyze the convergence properties of Algorithm 1, we adopt the framework introduced in [17]. It has been shown in [17, Corollary 2.9] that Algorithm 1 has the following properties:\n• (Primary Descent) There exists a constant σ > 0 such that for all k large enough,\nF (Xk+1)− F (Xk) ≤ −σ ‖Dρ(Xk)‖F ‖Xk+1 −Xk‖F .\n• (Stationarity) For all k large enough,\n‖Dρ(Xk)‖F = 0 =⇒ Xk+1 = Xk.\nMoreover, we show in the appendix that Algorithm 1 has the following property:\nProposition 2 (Asymptotic Small Step Size Safeguard) There exists a constant κ > 0 such that for all k large enough,\n‖Xk+1 −Xk‖F ≥ κ ‖Dρ(Xk)‖F . (5)\nThus, by [17, Theorem 2.3], in order to establish the linear convergence of Algorithm 1 to a critical point of Problem (1), it remains to prove the following theorem:\nTheorem 1 ( Lojasiewicz Inequality for Quadratic Optimization with Orthogonality Constraints) There exist constants δ, η > 0 such that for all X ∈ St(m,n) and X∗ ∈ X with ‖X −X∗‖F ≤ δ,\n|F (X) − F (X∗)|1/2 ≤ η ‖Dρ(X)‖F .\nThe proof of Theorem 1 is based on the following two results:\nTheorem 2 (Local Error Bound for Quadratic Optimization with Orthogonality Constraints) There exist constants δ, η > 0 such that\ndist(X,X ) ≤ η‖Dρ(X)‖F whenever X ∈ St(m,n) and dist(X,X ) ≤ δ.\nWe defer the proof of Theorem 2 to Section 3.\nProposition 3 (2-Hölder Continuity of F ) There exists a constant η > 0 such that for all X ∈ St(m,n) and X∗ ∈ X ,\n|F (X) − F (X∗)| ≤ η‖X −X∗‖2F .\nProof Observe that F , when viewed as a function on Rm×n, is continuously differentiable with Lipschitz continuous gradient. Thus, we have\n|F (X) − F (X∗)− 〈∇F (X∗),X −X∗〉| ≤ L\n2 ‖X −X∗‖2F , (6)\nwhere L > 0 is the Lipschitz constant of ∇F ; see, e.g., [15]. Now, by Proposition 1, we have ∇F (X∗) = X∗∇F (X∗)TX∗. This implies that\n〈∇F (X∗),X −X∗〉 = 〈 X∗∇F (X∗)TX∗,X −X∗ 〉 = 〈 ∇F (X∗)TX∗, (X∗)TX − In 〉 . (7)\nOn the other hand,\n〈\n∇F (X∗)TX∗, In −X TX∗\n〉 = 〈 (X∗)T∇F (X∗), (X∗)TX∗ −XTX∗ 〉\n= 〈 X∗∇F (X∗)TX∗,X∗ −X 〉 = −〈∇F (X∗),X −X∗〉. (8)\nUpon adding (7) and (8) and using the fact that (X −X∗)T (X−X∗) = 2In− (X ∗)TX−XTX∗, we obtain 2〈∇F (X∗),X −X∗〉 = − 〈 ∇F (X∗)TX∗, (X −X∗)T (X −X∗) 〉 ,\nor equivalently,\n|〈∇F (X∗),X −X∗〉| ≤ 1\n2\n∥ ∥∇F (X∗)TX∗ ∥ ∥ F ‖X −X∗‖2F .\nThis, together with (6), yields the desired inequality with η = ( L+ ∥ ∥∇F (X∗)TX∗ ∥ ∥\nF\n)\n/2. ⊔⊓ To complete the proof of Theorem 1, let X ∈ St(m,n) and X∗ ∈ X be such that dist(X,X ) = ‖X −X∗‖F ≤ δ, where δ > 0 is given by Theorem 2. Then, by Proposition 3, we obtain\n|F (X) − F (X∗)| ≤ η1‖X −X ∗‖2F = η1 · dist(X,X ) 2 ≤ η1η2‖Dρ(X)‖ 2 F\nfor some constants η1, η2 > 0, as desired."
    }, {
      "heading" : "3 Proof of Theorem 2",
      "text" : "We now prove Theorem 2, which is the main result of this paper. The proof can be divided into four steps."
    }, {
      "heading" : "3.1 Preliminary Observations",
      "text" : "Let A = UAΣAU T A and B = UBΣBU T B be spectral decompositions of A and B, respectively. It is straightforward to verify that tr ( XTAXB ) = tr ( X̄TΣAX̄ΣB )\n, where X̄ = UTAXUB ∈ St(m,n). Thus, we may assume without loss of generality that\nA = Diag(a1, . . . , am) ∈ S m and B = Diag(b1, . . . , bn) ∈ S n,\nwhere a1 ≥ a2 ≥ · · · ≥ am and b1 ≥ b2 ≥ · · · ≥ bn. By Proposition 1, we can write\nX = { X ∈ St(m,n) | AXB −XBXTAX = 0 } . (9)\nNow, it can be verified that\nDρ(X) = ( Im − (1− 2ρ)XX T ) ( ∇F (X)−X∇F (X)TX ) .\nSince ρ > 0, we see that Im − (1− 2ρ)XX T is invertible and\n∥ ∥∇F (X)−X∇F (X)TX ∥ ∥\nF ≤\n∥ ∥ ∥ ( Im − (1− 2ρ)XX T )−1 ∥ ∥ ∥ · ‖Dρ(X)‖F ≤ 1\n2ρ ‖Dρ(X)‖F .\nIn particular, in order to prove Theorem 2, it suffices to prove the following:\nTheorem 2’. There exist constants δ, η > 0 such that\ndist(X,X ) ≤ η ∥ ∥AXB −XBXTAX ∥ ∥\nF whenever X ∈ St(m,n) and dist(X,X ) ≤ δ."
    }, {
      "heading" : "3.2 Characterizing the Set of Critical Points when B has Full Rank",
      "text" : "Consider first the case where B has full rank; i.e., bi 6= 0 for i = 1, . . . , n. Let nA and nB be the number of distinct eigenvalues of A and B, respectively. Then, there exist indices s0, s1, . . . , snA and t0, t1, . . . , tnB such that 0 = s0 < s1 < · · · < snA = m and 0 = t0 < t1 < · · · < tnB = n, and\nas0+1 = · · · = as1 > as1+1 = · · · = as2 > · · · > asnA−1+1 = · · · = asnA , bt0+1 = · · · = bt1 > bt1+1 = · · · = bt2 > · · · > btnB−1+1 = · · · = btnB .\nLet U1, . . . , UnA and V1, . . . , VnB be the eigenspaces of A andB, respectively. Note that dim(Ui) = si − si−1 for i = 1, . . . , nA and dim(Vj) = tj − tj−1 for j = 1, . . . , nB. Furthermore, let\nH =\n{\n(h1, . . . , hnA)\n∣ ∣ ∣ ∣ ∣ nA ∑\ni=1\nhi = n, hi ∈ {0, 1, . . . , si − si−1} for i = 1, . . . , nA\n}\nand {ei} m i=1 be the standard basis of R m. Given any h = (h1, . . . , hnA) ∈ H, define\nEi(h) = [esi−1+1 · · · esi−1+hi ] ∈ R m×hi for i = 1, . . . , nA,\nE(h) = [E1(h) · · · EnA(h)] ∈ R m×n. (10)\nWe then have the following characterization of the set X of critical points of Problem (1), whose proof can be found in the appendix:\nProposition 4 The following holds:\nX = {X ∈ St(m,n) | X = BlkDiag(P1, . . . , PnA) · E(h) · BlkDiag(Q1, . . . , QnB )\nfor some Pi ∈ O si−si−1 (i = 1, . . . , nA), Qj ∈ O tj−tj−1 (j = 1, . . . , nB), and h ∈ H }\n. (11)\nRemarks. (i) Essentially, Proposition 4 states that every X ∈ X can be factorized as X = PQ, where P ∈ St(m,n) and Q ∈ On, and the columns of P (resp. Q) are the eigenvectors of A (resp. B). Indeed, observe that for i = 1, . . . , nA, the (si−1 + 1)-st to si-th columns of BlkDiag(P1, . . . , PnA) form an orthonormal basis of Ui. Similarly, for j = 1, . . . , nB , the (tj−1 + 1)-st to tj-th columns of BlkDiag(Q1, . . . , QnB ) form an orthonormal basis of Vj . To specify which n of the m eigenvectors of A are chosen to form P , we use the matrix E(h), where h = (h1, . . . , hnA) ∈ H and hi is the number of eigenvectors chosen from the eigenspace Ui.\n(ii) A result similar to Proposition 4 has appeared in [3, Section 4.8.2]. However, the proof therein contains a small gap. Specifically, from the properties that B is diagonal and commutes with XTAX, it is claimed in [3, Section 4.8.2] that XTAX is also diagonal. However, this is not true unless the diagonal entries of B are all distinct.\nProposition 4 suggests that we can partition X into disjoint subsets {Xh}h∈H, where\nXh = {X ∈ St(m,n) | X = BlkDiag(P1, . . . , PnA) · E(h) · BlkDiag(Q1, . . . , QnB )\nfor some Pi ∈ O si−si−1 (i = 1, . . . , nA), Qj ∈ O tj−tj−1 (j = 1, . . . , nB) } .\nConsequently, in order to prove Theorem 2’, it suffices to bound dist(X,Xh) for any X ∈ St(m,n) and h ∈ H."
    }, {
      "heading" : "3.3 Estimating the Distance to the Set of Critical Points",
      "text" : "Let X ∈ St(m,n) and h = (h1, . . . , hnA) ∈ H be arbitrary. By definition,\ndist(X,Xh) = min { ‖X − BlkDiag (P1, . . . , PnA) ·E(h) · BlkDiag (Q1, . . . , QnB )‖F |\nPi ∈ O si−si−1 for i = 1, . . . , nA; Qj ∈ O tj−tj−1 for j = 1, . . . , nB } . (12)\nLet ( P ∗1 , . . . , P ∗ nA , Q ∗ 1, . . . , Q ∗ nB ) be an optimal solution to (12). Upon letting\nP ∗ = BlkDiag ( P ∗1 , . . . , P ∗ nA ) ∈ Om, Q∗ = BlkDiag ( Q∗1, . . . , Q ∗ nB ) ∈ On,\nand X̄ = (P ∗)TX(Q∗)T , it is clear that dist2(X,Xh) = ∥ ∥X̄ − E(h) ∥ ∥ 2\nF . To bound this quantity,\nconsider the decompositions\nX̄ = [ X̄1 · · · X̄nB ] and E(h) = [ Ē1(h) · · · ĒnB(h) ] , (13)\nwhere X̄j , Ēj(h) ∈ R m×(tj−tj−1). We then have the following result, whose proof can be found in the appendix:\nProposition 5 For j = 1, . . . , nB and k = 1, . . . ,m, denote the k-th row of X̄j and Ēj(h) by [\nX̄j ]\nk and\n[ Ēj(h) ]\nk , respectively. Suppose that dist(X,Xh) ≤ δ for some δ ∈ (0, 1). Then,\ndist2(X,Xh) =\nnB ∑\nj=1\n∑\nk∈Ij\nΘ ( ∥ ∥ [ X̄j ]\nk\n∥ ∥ 2\n2\n)\n,\nwhere Ij = { k ∈ {1, . . . ,m} : [ Ēj(h) ]\nk = 0\n}\n.\nTo establish the desired error bound, we need to link ∥ ∥AXB −XBXTAX ∥ ∥\nF to the bound on\ndist2(X,Xh) in Proposition 5. This is achieved in two steps. First, we prove the following result:\nProposition 6 Consider the decomposition of X̄ in (13). Then,\n∥ ∥AXB −XBXTAX ∥ ∥\n2 F = Ω\n\n\nnB ∑\nj=1\n∥ ∥AX̄j − X̄jX̄ T j AX̄j ∥ ∥\n2 F\n\n .\nIn view of Proposition 6, we then proceed to prove the following bound:\nProposition 7 Suppose that dist(X,Xh) ≤ δ for some δ ∈ (0, 1). Then,\nnB ∑\nj=1\n∥ ∥AX̄j − X̄jX̄ T j AX̄j ∥ ∥\n2 F =\nnB ∑\nj=1\n∑\nk∈Ij\nΩ ( ∥ ∥ [ X̄j ]\nk\n∥ ∥ 2\n2\n)\n.\nThe proofs of Propositions 6 and 7 can be found in the appendix. Now, observe that whenever X ∈ St(m,n) and dist(X,X ) ≤ δ, then there exists an h ∈ H such that dist(X,Xh) ≤ δ. Hence, by combining Propositions 5, 6, and 7, we obtain Theorem 2’."
    }, {
      "heading" : "3.4 Removing the Full Rank Assumption on B",
      "text" : "Consider now the case where B does not have full rank. Without loss of generality, we assume that B = BlkDiag(B̄,0), where B̄ = Diag(b1, . . . , bp) ∈ S\np has full rank. Then, using (9), it can be shown that\nX = { X = [X1 X2] ∈ St(m,n) | X1 ∈ R m×p,X2 ∈ R m×(n−p), AX1B̄ −X1B̄X T 1 AX1 = 0 } .\nIt follows that for any X = [X1 X2] ∈ St(m,n) with X1 ∈ R m×p and X2 ∈ R m×(n−p), we have dist(X,X ) = dist(X1, X̄ ), where\nX̄ = { X ∈ St(m, p) | AXB̄ −XB̄XTAX = 0 } .\nBy our previous result, there exist constants δ, η > 0 such that\ndist(X1, X̄ ) ≤ η ∥ ∥AXB̄ −XB̄XTAX ∥ ∥\nF\nwhenever X1 ∈ St(m, p) and dist(X1, X̄ ) ≤ δ. To complete the proof, it remains to observe that\n∥ ∥AXB −XBXTAX ∥ ∥\n2 F = ∥ ∥AX1B̄ −X1B̄X T 1 AX1 ∥ ∥ 2 F + ∥ ∥X1B̄X T 1 AX2 ∥ ∥ 2 F\n= ∥ ∥AX1B̄ −X1B̄X T 1 AX1 ∥ ∥\n2 F + ∥ ∥XT2 ( AX1B̄ −X1B̄X T 1 AX1 ) XT1 ∥ ∥ 2 F\n= Θ ( ∥\n∥AX1B̄ −X1B̄X T 1 AX1\n∥ ∥\n2 F\n)\n."
    }, {
      "heading" : "4 Numerical Experiments",
      "text" : "In this section, we perform numerical experiments to investigate the convergence rate of the retracted line-search algorithm for problem (1) on synthetic datasets. As we shall see, the results consistency with the theoretical analysis in previous sections. In particular, we consider the four retractions mentioned above.\nFirst, we generate our diagonal matrices A ∈ Sm and B ∈ Sn, whose diagonal elements are sampled randomly from the uniform distribution. The starting point X0 is chosen from the uniform distribution and get the orthonormal basis for the range of X0 to keep the feasibility. In the setting of Armijo point, we fix γ = 1, β = 0.5 and c = 0.001. We stop the algorithm when F (Xk)− F (Xk+1) < 10\n−8. In practical computations, the orthogonality constraint may be violated after several iterations, which is mainly due to numerical errors incurred in the multiplication. In the numerical experiments, we follow the technique introduced in [7] and use (XTX)−1 to control feasibility error.\nFigure (1) illustrates the convergence performance of the four retractions with the relative “Thin” matrix: (1(a))m = 20, n = 10, (1(b)) m = 30, n = 10, (1(c)) m = 100, n = 10. Figure (2) illustrates the convergence performance with the relative “Fat” matrix: (2(a)) m = 20, n = 15, (2(b)) m = 50, n = 40, (2(c)) m = 100, n = 80. It can be seen that as long as the iterates are close enough to the optimal set, both the objective values and the solutions converge linearly."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we gave an explicit estimate of the exponent in a Lojasiewicz inequality for the (non-convex) set of critical points of Problem (1). Such an estimate was obtained by establishing a local error bound for the aforementioned set of critical points. Together with known arguments, our result implies the linear convergence of a large class of line-search methods on the Stiefel manifold. An interesting future direction would be to extend our techniques to analyze the convergence rates of first-order methods for solving structured non-convex optimization problems.\nAppendix"
    }, {
      "heading" : "A Proof of Proposition 2",
      "text" : "The proof of Proposition 2 is based on the following lemma:\nLemma 1 The Armijo points {αk}k≥0 satisfy limk→0 αk ‖Dρ(Xk)‖F = 0.\nProof The Armijo point exists in each step, which guarantees a sufficient decrease. We add all the decrease together and the sum must be finite, since there is a lower bound on the function value; i.e.\n+∞ ∑\nk=0\ncαk‖Dρ(Xk)‖ 2 F < +∞,\nwhich implies that lim k→0 αk‖Dρ(Xk)‖ 2 F = 0. Here, all the Armijo points have an upper bound γ. Thus,\nlim k→0\nα2k‖Dρ(Xk)‖ 2 ≤ lim\nk→0 γαk‖Dρ(Xk)‖\n2 = 0.\nThus, we have lim k→0 αk‖Dρ(Xk)‖ = 0, as desired. ⊔⊓ Proof of Proposition 2. By construction of the algorithm, with Vk = −αkDρ(Xk), we have\nXk+1 −Xk = R(Xk, Vk)−Xk = R(Xk, Vk)− (Xk + Vk)− Vk,\nwhich implies that\n‖Xk+1 −Xk‖F ≥ ‖Vk‖F − ‖R(Xk, Vk)− (Xk + Vk)‖F .\nWe divide by ‖Vk‖F on both sides to obtain\n‖Xk+1 −Xk‖F ‖Vk‖F ≥ 1− ‖R(Xk, Vk)− (Xk + Vk)‖F ‖Vk‖F .\nIt follows that\nlim k→∞ ‖Xk+1 −Xk‖F ‖Vk‖F ≥ 1− lim ‖Vk‖F→0 ‖R(Xk, Vk)− (Xk + Vk)‖F ‖Vk‖F .\nAccording to the definition of smooth retraction (3), the last term is equal to 0. Thus,\nlim k→∞ ‖Xk+1 −Xk‖F ‖Vk‖F ≥ 1.\nTherefore, there exists a large enough k to make sure (5) hold if we choose 1/2."
    }, {
      "heading" : "B Proof of Proposition 4",
      "text" : "Let X ∈ X be arbitrary. Using (9) and the fact that XTX = In, we have X TAXB = BXTAX. Since both XTAX and B are symmetric, this implies that XTAX and B are simultaneously diagonalizable. In particular, there exist orthogonal matrices Qj ∈ O\ntj−tj−1 and diagonal matrices Σj ∈ S\ntj−tj−1 , where j = 1, . . . , nB , such that the columns of BlkDiag(Q1, . . . , QnB) are the eigenvectors of B, and that\nXTAX = BlkDiag ( QT1 Σ1Q1, . . . , Q T nB ΣnBQnB ) . (14)\nNow, using (9) again, we have ( AX −XXTAX )\nB = 0. Since B has full rank and hence invertible, this yields AX = XXTAX. Upon letting Y = X ·BlkDiag (\nQT1 , . . . , Q T nB\n)\n∈ St(m,n) and using (14), we obtain AY = Y · BlkDiag(Σ1, . . . ,ΣnB). As Σ1, . . . ,ΣnB are diagonal, this implies that each of the n columns of Y is an eigenvector of A. To see that X can be expressed in the form given on the right-hand side of (11), it remains to note that A has m eigenvectors in total, and that any set of m eigenvectors of A can be expressed as BlkDiag(P1, . . . , PnA) for some Pi ∈ O\nsi−si−1 , where i = 1, . . . , nA. The converse is rather easy to verify. Hence, the proof is completed."
    }, {
      "heading" : "C Proof of Proposition 5",
      "text" : "Using (12) and (13), it can be verified that\ndist2(X,Xh) = ∥ ∥X̄ − E(h) ∥ ∥ 2\nF\n= min { ∥\n∥X̄ − E(h) · BlkDiag(Q1, . . . , QnB ) ∥ ∥ 2\nF\n∣ ∣ ∣ Qj ∈ O tj−tj−1 for j = 1, . . . , nB }\n=\nnB ∑\nj=1\nmin { ∥ ∥X̄j − Ēj(h)Qj ∥ ∥ 2\nF\n∣ ∣ ∣ Qj ∈ O tj−tj−1 } .\nFrom the definitions of E(h) in (10) and Ēj(h) in (13), we see that up to a rearrangement\nof the rows, Ēj(h) takes the form Ēj(h) =\n[\nItj−tj−1 0\n]\n. Thus, to obtain the desired bound on\ndist2(X,Xh), it remains to prove the following:\nLemma 2 Let S =\n[\nS1 S2\n]\n∈ St(p, q) be given, with S1 ∈ R q×q and S2 ∈ R (p−q)×q. Consider the\nfollowing problem:\nv∗ = min\n{\n∥ ∥ ∥ ∥ S − [ Iq 0 ] X ∥ ∥ ∥ ∥\n2\nF\n∣ ∣ ∣ ∣ ∣ X ∈ Oq } .\nSuppose that v∗ < 1. Then, we have v∗ = Θ ( ‖S2‖ 2 F ) .\nProof Since ∥\n∥ ∥ ∥ S −\n[\nIq 0\n]\nX\n∥ ∥ ∥ ∥\n2\nF\n= ‖S1 −X‖ 2 F + ‖S2‖ 2 F ,\nit suffices to consider the problem\nmin { ‖S1 −X‖ 2 F | X ∈ O q } . (15)\nProblem (15) is an instance of the orthogonal Procrustes problem, whose optimal solution is given by X∗ = UV T , where S1 = UΣV\nT is the singular value decomposition of S1 [18]. It follows that\nv∗ = ‖Σ− Iq‖ 2 F + ‖S2‖ 2 F .\nNow, since S ∈ St(p, q), we have STS = ST1 S1 + S T 2 S2 = Iq, or equivalently,\nΣ2 + V TST2 S2V = Iq.\nThis implies that 0 Σ Iq and\nIq − Σ = (Iq +Σ) −1\n( V TST2 S2V ) .\nIt follows that 1\n4 ‖S2‖\n4 F + ‖S2‖ 2 F ≤ v ∗ ≤ ‖S2‖ 4 F + ‖S2‖ 2 F .\nThis, together with the fact that ‖S2‖ 2 F ≤ v ∗ < 1, yields v∗ = Θ ( ‖S2‖ 2 F ) , as desired. ⊔⊓"
    }, {
      "heading" : "D Proof of Proposition 6",
      "text" : "Recall that\nP ∗ = BlkDiag ( P ∗1 , . . . , P ∗ nA ) ∈ Om, Q∗ = BlkDiag ( Q∗1, . . . , Q ∗ nB ) ∈ On, X̄ = (P ∗)TX(Q∗)T .\nUpon observing that AP ∗ = P ∗A, BQ∗ = Q∗B, B = BlkDiag (\nbt1It1−t0 , . . . , btnB ItnB−tnB−1\n)\nand using (13), we compute\n∥ ∥AXB −XBXTAX ∥ ∥\n2 F = ∥ ∥AP ∗X̄Q∗B − P ∗X̄Q∗B(Q∗)T X̄T (P ∗)TAP ∗X̄Q∗ ∥ ∥ 2 F\n= ∥ ∥P ∗ ( AX̄B − X̄BX̄TAX̄ ) Q∗ ∥ ∥\n2 F\n= ∥ ∥AX̄B − X̄BX̄TAX̄ ∥ ∥\n2 F\n=\nnB ∑\nj=1\n∥ ∥ ∥ ∥ ∥ btjAX̄j − nB ∑\nk=1\nbtkX̄k ( X̄Tk AX̄j )\n∥ ∥ ∥ ∥ ∥ 2\nF\n. (16)\nNow, observe that the columns of X̄ are orthonormal and span an n-dimensional subspace L. In particular, for j = 1, . . . , nB , each column of AX̄j can be decomposed as u+ v, where u is a linear combination of the columns of X̄ and v ∈ L⊥, the orthogonal complement of L. In view of the structure of X̄ in (13), this leads to\nAX̄j =\nnB ∑\nk=1\nX̄k ( X̄Tk AX̄j ) + Tj,\nwhere Tj ∈ R m×(tj−tj−1) is formed by projecting the columns of AX̄j onto L ⊥. Hence,\n∥ ∥ ∥ ∥ ∥ btjAX̄j − nB ∑\nk=1\nbtkX̄k ( X̄Tk AX̄j )\n∥ ∥ ∥ ∥ ∥ 2\nF\n= ∑\nk 6=j\n(btj − btk) 2 ∥ ∥X̄k ( X̄Tk AX̄j )∥ ∥\n2 F + b2tj‖Tj‖ 2 F\n= Ω\n\n\n∑\nk 6=j\n∥ ∥X̄k ( X̄Tk AX̄j )∥ ∥\n2 F + ‖Tj‖ 2 F\n\n (17)\n= Ω ( ∥\n∥AX̄j − X̄jX̄ T j AX̄j\n∥ ∥\n2 F\n)\n,\nwhere (17) follows from the fact that btj 6= btk whenever j 6= k and btj 6= 0 since B is assumed to have full rank. By combining the above with (16), the proof is completed."
    }, {
      "heading" : "E Proof of Proposition 7",
      "text" : "Consider a fixed j ∈ {1, . . . , nB}. Let x̄k be the k-th column of X̄j and (x̄k)α be the α-th entry of x̄k, where k = 1, . . . , tj − tj−1 and α = 1, . . . ,m. Since dist(X,Xh) = ∥ ∥X̄ − E(h) ∥ ∥\nF ≤ δ, using\nthe definition of E(h) in (10), we have\n(x̄k)α =\n{\n1 +O(δ) if α = π(k), O(δ) otherwise,\nwhere π(k) is the coordinate of the k-th column of Ēj(h) that equals 1. Since π(k) 6= π(ℓ) whenever k 6= ℓ, it follows that\nx̄TkAx̄ℓ =\n{\naπ(k) +O(δ) if k = ℓ,\nO(δ) otherwise.\nNow, let ∆k be the k-th column of AX̄j − X̄jX̄ T j AX̄j , where k = 1, . . . , tj − tj−1. Then,\n∆k = Ax̄k −\ntj−tj−1 ∑\nℓ=1\nx̄ℓ ( x̄Tℓ Ax̄k ) = ( A− aπ(k)Im ) x̄k −O(δ) ·\n\n\ntj−tj−1 ∑\nℓ=1\nx̄ℓ\n\n .\nLet ΠIj be the projector onto the coordinates in Ij. By Proposition 5 and the assumption that dist(X,Xh) ≤ δ, we have\ntj−tj−1 ∑\nℓ=1\n∥ ∥ΠIj (x̄ℓ) ∥ ∥ 2 2 =\n∑\nk∈Ij\n∥ ∥ [ X̄j ]\nk\n∥ ∥ 2\n2 = O(δ).\nHence,\n∥ ∥ΠIj(∆k) ∥ ∥ 2 ≥ ∥ ∥ΠIj (( A− aπ(k)Im ) x̄k )∥ ∥ 2 −O(δ) ·\n\n\ntj−tj−1 ∑\nℓ=1\n∥ ∥ΠIj (x̄ℓ) ∥ ∥\n2\n\n\n≥ ∥ ∥ΠIj (( A− aπ(k)Im ) x̄k )∥ ∥ 2 −O(δ2). (18)\nLet i′ ∈ {0, 1, . . . , nA − 1} be such that si′ + 1 ≤ π(k) ≤ si′+1. Then, we have\n∥ ∥ΠIj (( A− aπ(k)Im ) x̄k )∥ ∥ 2 2 =\n∑\ni 6=i′\n∑\nα∈Ij∩{si+1,...,si+1}\n(( asi+1 − aπ(k) ) (x̄k)α )2\n= ∑\ni 6=i′\n∑\nα∈Ij∩{si+1,...,si+1}\nΩ ( (x̄k) 2 α )\n= Ω ( ∥\n∥ΠIj (x̄k) ∥ ∥ 2\n2\n)\n−O\n(\n∥ ∥ ∥ ΠIj∩{si′+1,...,si′+1}(x̄k) ∥ ∥ ∥ 2\n2\n)\n. (19)\nTo bound the term ∥ ∥\n∥ ΠIj∩{si′+1,...,si′+1}(x̄k)\n∥ ∥ ∥ 2\n2 , we proceed as follows. Let Y = X(Q∗)T ∈ St(m,n)\nand decompose it as\nY =\n\n  Y11 · · · Y1nA ... . . . ...\nYnA1 · · · YnAnA\n\n  ,\nwhere Yii ∈ R (si−si−1)×hi , for i = 1, . . . , nA. Observe that\ndist2(X,Xh) = min { ‖Y − BlkDiag(P1, . . . , PnA) · E(h)‖ 2 F | Pi ∈ O si−si−1 for i = 1, . . . , nA }\n= ∑\n1≤i 6=j≤nA\n‖Yij‖ 2 F +\nnA ∑\ni=1\nmin\n{\n∥ ∥ ∥ ∥ Yii − Pi [ Ihi 0 ]∥ ∥ ∥ ∥\n2\nF\n∣ ∣ ∣ ∣ ∣ Pi ∈ O si−si−1 } . (20)\nThe following lemma establishes a bound on the second term in (20):\nLemma 3 For i = 1, . . . , nA, let\nv∗i = min\n{\n∥ ∥ ∥ ∥ Yii − Pi [ Ihi 0 ]∥ ∥ ∥ ∥ 2\nF\n∣ ∣ ∣ ∣ ∣ Pi ∈ O si−si−1 } . (21)\nThen, we have\nv∗i = Θ\n\n\n∥ ∥ ∥ ∥ ∥ ∥ ∑\nj 6=i\nY Tji Yji\n∥ ∥ ∥ ∥ ∥ ∥ 2\nF\n\n .\nLet us defer the proof of Lemma 3 to the end of this section. Together with (20), Lemma 3 implies that\ndist2(X,Xh) = ∑\n1≤i 6=j≤nA\n‖Yij‖ 2 F +\nnA ∑\ni=1\nΘ\n\n\n∥ ∥ ∥ ∥ ∥ ∥ ∑\nj 6=i\nY Tji Yji\n∥ ∥ ∥ ∥ ∥ ∥ 2\nF\n\n .\nSince dist(X,Xh) ≤ δ for some δ ∈ (0, 1), we have ∑\n1≤i 6=j≤nA ‖Yij‖ 2 F = O(δ 2). This implies that\nv∗i = O\n\n\n\n\n∑\nj 6=i\n‖Yji‖ 2 F\n\n\n2\n = O(δ4)\nfor i = 1, . . . , nA. Now, decompose X̄ = (P ∗)TY as\n\n  X̄11 · · · X̄1nA ... . . . ...\nX̄nA1 · · · X̄nAnA\n\n  ,\nwhere X̄ii = (P ∗ i ) TYii ∈ R (si−si−1)×hi for i = 1, . . . , nA. Note that for i = 1, . . . , nA, we have\nv∗i =\n∥ ∥ ∥ ∥ X̄ii − [ Ihi 0 ]∥ ∥ ∥ ∥ 2\nF\n.\nMoreover, observe that ΠIj∩{si′+1,...,si′+1}(x̄k) is part of X̄i′+1,i′+1 and does not intersect the diagonal of the top hi′+1 × hi′+1 block of X̄i′+1,i′+1. Thus, by Lemma 3,\n∥ ∥ ∥ΠIj∩{si′+1,...,si′+1}(x̄k) ∥ ∥ ∥ 2 2 ≤ v∗i′+1 = O(δ 4).\nTogether with (18) and (19), this yields\n∥ ∥ΠIj (∆k) ∥ ∥ 2 2 ≥ Ω\n(\n∥ ∥ΠIj (x̄k) ∥ ∥ 2\n2\n)\n−O(δ3).\nIt follows that\n∥ ∥AX̄j − X̄jX̄ T j AX̄j ∥ ∥\n2 F =\ntj−tj−1 ∑\nk=1\n‖(∆k)‖ 2 2\n≥\ntj−tj−1 ∑\nk=1\n∥ ∥ΠIj(∆k) ∥ ∥ 2\n2\n≥\ntj−tj−1 ∑\nk=1\nΩ ( ∥\n∥ΠIj (x̄k) ∥ ∥ 2\n2\n)\n−O(δ3)\n= ∑\nk∈Ij\nΩ ( ∥ ∥ [ X̄j ]\nk\n∥ ∥ 2\n2\n)\n−O(δ3).\nUpon summing over j = 1, . . . , nB and using Proposition 5, we obtain the desired bound. To complete the proof, it remains to prove Lemma 3. Proof of Lemma 3. Consider a fixed i ∈ {1, . . . , nA}. Note that Problem (21) is again an instance of the orthogonal Procrustes problem. Hence, by the result in [18], an optimal solution to Problem (21) is given by\nP ∗i = Hi\n[\nW Ti 0 0 Isi−si−1−hi\n]\n,\nwhere Yii = Hi\n[\nΣi 0\n]\nW Ti is a singular value decomposition of Yii with Hi ∈ O si−si−1 , Wi ∈ O hi ,\nand Σi ∈ S hi being diagonal. It follows from (21) that\nv∗i =\n∥ ∥ ∥ ∥ Yii − P ∗ i [ Ihi 0 ]∥ ∥ ∥ ∥ 2\nF\n= ‖Σi − Ihi‖ 2 F .\nNow, since Y ∈ St(m,n), we have\nY Tii Yii + ∑\nj 6=i\nY Tji Yji = WiΣ 2 iW T i +\n∑\nj 6=i\nY Tji Yji = Ihi ,\nor equivalently,\nΣ2i +W T i\n\n\n∑\nj 6=i\nY Tji Yji\n\nWi = Ihi .\nBy following the arguments in the proof of Lemma 2, we conclude that\n‖Σi − Ihi‖ 2 F = Θ\n\n\n∥ ∥ ∥ ∥ ∥ ∥ ∑\nj 6=i\nY Tji Yji\n∥ ∥ ∥ ∥ ∥ ∥ 2\nF\n\n ,\nas desired."
    } ],
    "references" : [ {
      "title" : "Steepest Descent Algorithms for Optimization under Unitary Matrix Constraint",
      "author" : [ "T.E. Abrudan", "J. Eriksson", "V. Koivunen" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Convergence of the Iterates of Descent Methods for Analytic Cost Functions",
      "author" : [ "P.-A. Absil", "R. Mahony", "B. Andrews" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Optimization Algorithms on Matrix Manifolds",
      "author" : [ "P.-A. Absil", "R. Mahony", "R. Sepulchre" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Projection–Like Retractions on Matrix Manifolds",
      "author" : [ "P.-A. Absil", "J. Malick" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Manifolds of Negative Curvature",
      "author" : [ "R.L. Bishop", "B. O’Neill" ],
      "venue" : "Transactions of the American Mathematical Society,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1969
    }, {
      "title" : "Extrema of Sums of Heterogeneous Quadratic Forms",
      "author" : [ "M. Bolla", "G. Michaletzky", "G. Tusnády", "M. Ziermann" ],
      "venue" : "Linear Algebra and Its Applications,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "A Framework of Constraint Preserving Update Schemes for Optimization on Stiefel Manifold. Accepted for publication",
      "author" : [ "B. Jiang", "Y.-H. Dai" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Generalized Power Method for Sparse Principal Component Analysis",
      "author" : [ "M. Journée", "Yu. Nesterov", "P. Richtárik", "R. Sepulchre" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Empirical Arithmetic Averaging over the Compact Stiefel Manifold",
      "author" : [ "T. Kaneko", "S. Fiori", "T. Tanaka" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Trace Optimization and Eigenproblems in Dimension Reduction Methods",
      "author" : [ "E. Kokiopoulou", "J. Chen", "Y. Saad" ],
      "venue" : "Numerical Linear Algebra with Applications,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "New Fractional Error Bounds for Polynomial Systems with Applications to Hölderian Stability in Optimization and Spectral Theory of Tensors",
      "author" : [ "G. Li", "B.S. Mordukhovich", "T.S.  Pha.m" ],
      "venue" : "Accepted for publication in Mathematical Programming, Series A,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Error Bounds and Convergence Analysis of Feasible Descent Methods: A General Approach",
      "author" : [ "Z.-Q. Luo", "P. Tseng" ],
      "venue" : "Annals of Operations Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1993
    }, {
      "title" : "Optimization Algorithms Exploiting Unitary Constraints",
      "author" : [ "J.H. Manton" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Convergence to Equilibrium for Discretizations of Gradient",
      "author" : [ "B. Merlet", "T.N. Nguyen" ],
      "venue" : "Like Flows on Riemannian Manifolds. Differential and Integral Equations,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Introductory Lectures on Convex Optimization: A Basic Course",
      "author" : [ "Yu. Nesterov" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "Numerical Methods for Large Eigenvalue Problems",
      "author" : [ "Y. Saad" ],
      "venue" : "Classics in Applied Mathematics. Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania, revised edition,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Convergence Results for Projected Line–Search Methods on Varieties of Low–Rank Matrices via Lojasiewicz Inequality",
      "author" : [ "R. Schneider", "A. Uschmajew" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "A Generalized Solution of the Orthogonal",
      "author" : [ "P.H. Schönemann" ],
      "venue" : "Procrustes Problem. Psychometrika,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1966
    }, {
      "title" : "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate",
      "author" : [ "O. Shamir" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Optimization Techniques on Riemannian Manifolds",
      "author" : [ "S.T. Smith" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1994
    }, {
      "title" : "Moment Inequalities for Sums of Random Matrices and Their Applications in Optimization",
      "author" : [ "A.M.-C. So" ],
      "venue" : "Mathematical Programming, Series A,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Non–Asymptotic Convergence Analysis of Inexact Gradient Methods for Machine Learning Without Strong Convexity",
      "author" : [ "A.M.-C. So" ],
      "venue" : "Preprint, available at http://www.se.cuhk.edu.hk/~manchoso/papers/inexact_GM_conv.pdf,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Approximation Accuracy, Gradient Methods, and Error Bound for Structured Convex Optimization",
      "author" : [ "P. Tseng" ],
      "venue" : "Mathematical Programming, Series B,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "A Feasible Method for Optimization with Orthogonality Constraints",
      "author" : [ "Z. Wen", "W. Yin" ],
      "venue" : "Mathematical Programming, Series A,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Adaptive Canonical Correlation Analysis Based on Matrix Manifolds",
      "author" : [ "F. Yger", "M. Berar", "G. Gasso", "A. Rakotomamonjy" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning (ICML",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "l1,p–Norm Regularization: Error Bounds and Convergence Rate Analysis of First–Order Methods",
      "author" : [ "Z. Zhou", "Q. Zhang", "A.M.-C. So" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : ", [6, 13, 3, 8, 10, 16, 21, 25]).",
      "startOffset" : 2,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : ", [6, 13, 3, 8, 10, 16, 21, 25]).",
      "startOffset" : 2,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : ", [6, 13, 3, 8, 10, 16, 21, 25]).",
      "startOffset" : 2,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : ", [6, 13, 3, 8, 10, 16, 21, 25]).",
      "startOffset" : 2,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : ", [6, 13, 3, 8, 10, 16, 21, 25]).",
      "startOffset" : 2,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : ", [6, 13, 3, 8, 10, 16, 21, 25]).",
      "startOffset" : 2,
      "endOffset" : 31
    }, {
      "referenceID" : 20,
      "context" : ", [6, 13, 3, 8, 10, 16, 21, 25]).",
      "startOffset" : 2,
      "endOffset" : 31
    }, {
      "referenceID" : 24,
      "context" : ", [6, 13, 3, 8, 10, 16, 21, 25]).",
      "startOffset" : 2,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : ", [1, 3, 4, 24, 7].",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : ", [1, 3, 4, 24, 7].",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : ", [1, 3, 4, 24, 7].",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 23,
      "context" : ", [1, 3, 4, 24, 7].",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 6,
      "context" : ", [1, 3, 4, 24, 7].",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 18,
      "context" : "More recently, Shamir [19] developed a stochastic line-search method for Problem (1) when n = 1, B = In = 1, and A is negative semidefinite.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 19,
      "context" : "On another front, Smith [20] showed that when used to optimize a smooth function over a Riemannian manifold, the method of steepest descent will converge linearly to a critical point if the function is strongly convex on the manifold.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "In particular, it is known that every smooth function that is convex on a compact Riemannian manifold (such as the Stiefel manifold) is constant [5].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 19,
      "context" : "Therefore, one cannot hope to obtain linear convergence results for Problem (1) using the convexity-based approach in [20].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : ", [2, 14, 17].",
      "startOffset" : 2,
      "endOffset" : 13
    }, {
      "referenceID" : 13,
      "context" : ", [2, 14, 17].",
      "startOffset" : 2,
      "endOffset" : 13
    }, {
      "referenceID" : 16,
      "context" : ", [2, 14, 17].",
      "startOffset" : 2,
      "endOffset" : 13
    }, {
      "referenceID" : 10,
      "context" : ", [11]) and can in principle be applied to Problem (1).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 16,
      "context" : "well-established analysis framework in the literature [17], we conclude that a host of line-search methods for solving Problem (1) converge linearly to a critical point.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "Thus, it is qualitatively different from those in [3, 19].",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "Thus, it is qualitatively different from those in [3, 19].",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 11,
      "context" : "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.",
      "startOffset" : 52,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.",
      "startOffset" : 52,
      "endOffset" : 68
    }, {
      "referenceID" : 21,
      "context" : "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.",
      "startOffset" : 52,
      "endOffset" : 68
    }, {
      "referenceID" : 25,
      "context" : "Moreover, although our work is similar in spirit as [12, 23, 22, 26], there is a crucial difference: While the latter deals exclusively with convex optimization problems, the former considers an optimization problem in which neither the objective function nor the constraint is convex.",
      "startOffset" : 52,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "We refer the reader to [3, 9] for details of these retractions.",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "We refer the reader to [3, 9] for details of these retractions.",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 16,
      "context" : "We refer the reader to [17] for details.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "4 Convergence Analysis Framework for the Line-Search Method To analyze the convergence properties of Algorithm 1, we adopt the framework introduced in [17].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 14,
      "context" : ", [15].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "In the numerical experiments, we follow the technique introduced in [7] and use (XTX)−1 to control feasibility error.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 17,
      "context" : "(15) Problem (15) is an instance of the orthogonal Procrustes problem, whose optimal solution is given by X∗ = UV T , where S1 = UΣV T is the singular value decomposition of S1 [18].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 17,
      "context" : "Hence, by the result in [18], an optimal solution to Problem (21) is given by P ∗ i = Hi [ W T i 0 0 Isi−si−1−hi ]",
      "startOffset" : 24,
      "endOffset" : 28
    } ],
    "year" : 2015,
    "abstractText" : "A fundamental class of matrix optimization problems that arise in many areas of science and engineering is that of quadratic optimization with orthogonality constraints. Such problems can be solved using line-search methods on the Stiefel manifold, which are known to converge globally under mild conditions. To determine the convergence rate of these methods, we give an explicit estimate of the exponent in a Lojasiewicz inequality for the (non-convex) set of critical points of the aforementioned class of problems. By combining such an estimate with known arguments, we are able to establish the linear convergence of a large class of line-search methods. A key step in our proof is to establish a local error bound for the set of critical points, which may be of independent interest.",
    "creator" : "LaTeX with hyperref package"
  }
}