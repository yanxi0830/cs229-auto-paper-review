{
  "name" : "1411.1490.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Representations for Life-Long Learning and Autoencoding",
    "authors" : [ "Maria-Florina Balcan", "Avrim Blum" ],
    "emails" : [ "ninamf@cs.cmu.edu", "avrim@cs.cmu.edu", "vempala@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n14 90\nv2 [\ncs .L\nG ]\n4 D\nec 2\n01 4"
    }, {
      "heading" : "1 Introduction",
      "text" : "Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that, like humans [13], improve their ability to learn as they do so, needing less data (per task) as they learn more. A natural approach for tackling this goal (called “life-long learning” [23, 25] or “transfer learning” [1, 19] or “learning to learn” [7, 24]) is to use information from previously-learned tasks to improve the underlying representation used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist. These commonalities could be a single low-dimensional or sparse representation, a collection of multiple low-dimensional or sparse representations, or some combination or hierarchy, such as in Deep Learning [9, 10]. In this paper, we develop algorithms with provable efficiency and sample size guarantees for several interesting categories of commonalities, considering both linear and Boolean transfer functions, under natural distributions on the data points.\nSpecifically, we consider a setting where we are trying to solve a large number of binary classification problems that arrive one at a time. Each classification problem will individually be learnable from a polynomialsize sample,1 but our goal will be to learn new internal representations that will allow us to learn new target functions faster and from less data. We will furthermore aim to do this in a streaming setting in which we cannot keep the labeled data for problem t in memory when we move on to problem t+ 1, only the learned hypotheses (which will be required to have a compact description) and the current internal representation.\nWe start by considering a conceptually simple case that each classification problem is a linear separator, and that what their associated target vectors share in common is they lie in a low k dimensional subspace of the ambient space <n (equivalently, there exist a small number of hidden linear metafeatures and each target is a linear separator over these metafeatures). This case has been considered in the “batch” setting in which one has data available for all target functions at the same time and therefore can solve a joint optimization problem [1, 19]. However, for the online setting, a key challenge is that we won’t have perfectly learned the previous target functions when we set out to learn our next one.2 For this problem we provide a sampleefficient polynomial time algorithm that, when the underlying data distributions for ourm learning problems are log-concave, has labeled sample complexity much better than the Ω(nm/ ) sample complexity required for learning the tasks separately.\nWe then consider scenarios where the commonalities require a representation with more than one level of metafeatures and provide efficient algorithms for these settings as well. For linear metafeatures, we provide a natural framework where two levels of metafeatures can be efficiently extracted and provide substantial benefit: specifically, we analyze a scenario where the target functions all lie in a k dimensional space and furthermore within that k-dimensional space, each target lies in one of r different constant dimensional spaces, where r could be large. This models situations where there are really r different types of learning problems but they do share some commonalities across types (given by a low k-dimensional subspace).\nIn Section 4 we develop algorithms for a scenario where the metafeatures are non-linear, in particular where features are boolean and the metafeatures are products. We give an efficient algorithm for finding the fewest product-based metafeatures for a given set of target monomials under an “anchor-variable” assumption analogous to the anchor-word assumption of [2], and prove bounds on its performance for learning a series of target functions arriving online. We then give an extension that learns an approximately-optimal overcomplete sparse representation (we may have more metafeatures than input features, but each target should have a sparse representation) under a weaker form of assumption we call the “anchor-set” assumption (anchor\n1Except in Section 5 where we consider learning multivariate polynomials and allow membership queries. 2In particular, because of this we will need to be particularly careful with which targets we use in constructing our metafeatures,\nas well as in controlling the propagation of errors. See, e.g., Lemma 3 and Figure 1.\nvariables no longer make sense in the overcomplete case). These results can be viewed as giving efficient algorithms for a Boolean autoencoding where given a set of black-and-white pixel images (vectors in {0, 1}n) we want to find either (a) the fewest “basic objects” (also vectors in {0, 1}n) such that each given image can be reconstructed by superimposing some subset of them (taking their bitwise-OR), or (b) a larger number of such objects such that each image can be reconstructed by superimposing only a few of them. In the first case our algorithm finds the optimal solution under the anchor-variable assumption (the problem is NP-hard in general) and in the second case it finds a bicriteria approximation (for a given sparsity level, approximates both the number and the sparsity to logarithmic factors) under the weaker anchor-set assumption.\nIn Section 5 we show how our results can be applied to the case that target functions are polynomials of low L1 norm whose terms share pieces in common (within and across polynomials), a scenario that can be expressed via two levels of product-based metafeatures. Interestingly, as opposed to the algorithms for linear metafeatures, this algorithm periodically re-compactifies its current representation. In particular, whenever a new polynomial cannot be learned using the current representation and must be learned from scratch, we then revisit the previously-learned polynomials and optimally “compactify” them into the fewest number of (possibly overlapping) conjunctive metafeatures that can be used to recreate all their monomials."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Most related work in multi-task or transfer learning considers the case that all target functions are present simultaneously or that target functions are drawn from some easily learnable distribution. Baxter [7, 6] developed some of the earliest foundations for transfer learning, by providing sample complexity results for achieving low average error in such settings. Other related sample complexity results appear in [8].\nRecent work of [19, 16] considers the problem of learning multiple linear separators that share a common low-dimensional subspace in the batch setting where all tasks are given up front. They specifically provide guarantees for a natural ERM algorithm with trace norm regularization. There has also been work on applying the Group Lasso method to batch multi-task learning which solves a specific multi-task optimization problem [20]. By contrast with these results, our setting is more demanding since we aim to achieve small error on all tasks and to do so online without keeping all training data from past learning tasks in memory.\n[11] considers multi-task learning where explicit known relationships among tasks are exploited for faster learning. In their setting each learning problem is an online problem but the collection of learning problems are all occurring simultaneously. Discussion in [26] hints toward the type of the algorithms we analyze in Section 3, but without formal analysis about how the error accumulation could harm the sample complexity (which, as we will see, is one of the central challenges in this setting).\nThe problem of trying to learn invariants or other commonalities when faced with a series of learning tasks arriving over time has a long history in applied machine learning (e.g., [23, 25]). Our work is the first to give provable efficiency guarantees for learning multi-layer representations in this life-long learning setting."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We assume we have m learning (binary classification) problems that arrive online over time. The learning problems are all over a common instance space X of dimension n (e.g., we will consider X = <n and X = {0, 1}n) but each has its own target function and potentially its own distribution Di over X . Formally, learning problem i is defined by a distribution Pi over X × Y where Y = {−1, 1} is the label space and Di is the marginal over X of Pi, and the goal of the learning algorithm on problem i is to produce a hypothesis function hi of small error, where err(hi) = errPi(hi) = P(x,y)∼Pi [hi(x) 6= y]."
    }, {
      "heading" : "3 Life-long Learning of Halfspaces",
      "text" : "We consider here the natural case that X = <n and each target function is a linear separator going through the origin; that is, for all i there exists ai of unit length such that for all (x, y) drawn from Pi we have sign(ai · x) = y. To begin, we will assume that what the target functions have in common is that they all lie in some common k-dimensional subspace of <n for k min(n,m). In particular, let A be a m by n matrix whose rows are a1, . . . , am; then our assumption is that A has rank k. This implies that there exist a decomposition A = CW , where W is a k × n matrix and C is a m × k matrix. The rows w1, . . . , wk of W can be viewed as k linear metafeatures that are sufficient to describe all m learning problems, or equivalently we can view this as a network with one middle layer of k hidden linear units. In fact, our algorithms will work under a more robust condition that allows for the ai to be “near” to a low-dimensional subspace (see Theorem 1).\nIn this section we analyze the following online algorithm for this setting; note, the algorithm is very natural but the challenge will be to analyze it and control the propagation of error at reasonable sample sizes. Let εacc be a quantity to be determined later. For the first learning problem we just learn it to error εacc using the original input features and let the resulting weight vector be w̃1. Suppose now we have produced weight vectors w̃1, . . . , w̃k′ and we are considering problem i. We will first see if we can learn problem i well (to error ) as a linear combination of the w̃j . If so, then we mark this as a success and go on to problem i+ 1. If not, then we will learn it to error εacc using the input features and add the hypothesis weight vector as w̃k′+1. (See Algorithm 1 for formal details). The challenge is how small εacc needs to be for this to succeed.\nWe show in the following that if theDi are isotropic log-concave (which includes many distributions such as Gaussian and uniform, see, e.g., [18]), the above procedure will be successful and learn the target functions with much fewer labeled examples in total than by learning each function separately. We start with some useful facts and present a lemma (Lemma 3) that is crucial for our analysis.\nGiven two vectors a and b and a distribution D̃, let dD̃(a, b) = Px∼D̃(sign(u · x) 6= sign(v · x)). Let θ(a, b) be the angle between two vectors a and b. For a vector a and a subspace V , let θ(a, V ) = minb∈V θ(a, b) be the angle between a and its closest vector in V (in angle). For subspaces U and V , let θ(U, V ) = maxu∈U θ(u, V ). That is, θ(U, V ) ≤ α iff for all u ∈ U there exists v ∈ V such that θ(u, v) ≤ α.\nLemma 1 Assume D is an isotropic log-concave in Rn. Then there exist constants c and c′ such that for any two unit vectors u and v in Rd we have cθ(v, u) ≤ dD(u, v) ≤ c′θ(v, u).\nProof: The proof of the lower bound appears in [5]. The proof of the upper bound is implicit in the earlier work of [27] – we provide it here for completeness. The key idea is to project the region of disagreement in the space given by the two normal vectors, and then using properties of log-concave distributions in 2- dimensions. Specifically, consider the plane determined by u and v, and let g be the 2-dimensional marginal of the density function over this plane. Then g is an isotropic and log-concave density function over R2.\nIt is known [15] that for some constants k3, k4 we have g(z) ≤ k3e−k4||z||. Given this fact, we just need to show that the integral of k3e−k4||z|| over the region {z : u · z ≥ 0, v · z ≤ 0} is at most c′α for some constant c′, where α is the angle between u and v (the integral over {z : u · z ≤ 0, v · z ≥ 0} is analogous). In particular, using polar coordinates, we can write the integral as:∫ α\nθ=0 ∫ ∞ r=0 f(r cos θ, r sin θ) r drdθ ≤ ∫ α θ=0 ∫ ∞ r=0 rk3e −k4rdrdθ.\nThe inner integral evaluates to a constant and therefore the entire integral is bounded by c′α for some constant c′ as desired.\nLemma 1 implies that if we learn some target ai to error acc, then the angle between our learned vector and the target will be O( acc). In the other direction, if the target lies in subspace W and we have learned a subspace W̃ such that θ(W, W̃ ) is small, then there will exist a low-error weight vector in W̃ . Ideally, we would therefore like to say that if we construct a subspace W̃ out of vectors w̃i that individually are close to their associated targets wi, then W̃ is close to the spanW of the wi. Unfortunately, this is not in general true if the targets are close to each other, e.g., see Figure 1(a). We will address this by using the fact that each w̃i was not learnable using the span of the previous w̃j . We begin with a helper lemma (Lemma 2), which can be viewed as addressing the special case that all previous targets have been learned perfectly, and then present our main lemma (Lemma 3).\nLemma 2 Let U = span{a1, . . . , ak−1}, V = span{a1, a2, . . . , ak−1, b} and Ṽ = span{a1, . . . , ak−1, b̃} be sets of vectors in <n. Then,\nθ(V, Ṽ ) ≤ π 2 θ(b̃, b) θ(b̃, U) .\nProof: (For intuition, see Figure 1(b).) First, we may assume b, b̃ 6∈ U because if b ∈ U then θ(V, Ṽ ) = 0 and if b̃ ∈ U then θ(b̃, U) = 0. Additionally we may assume b and b̃ are unit-length vectors since we are interested only in angles. Next, let u ∈ V be the unit-length vector in V of farthest angle from Ṽ , i.e., θ(u, Ṽ ) = θ(V, Ṽ ). We can write u as a linear combination of b and some vector u1 ∈ U , and will prove the lemma by showing there must be some nearby vector in the span of u1 and b̃. Specifically, using the fact that θ(V, Ṽ ) = θ(u, Ṽ ) ≤ θ(span(u1, b), span(u1, b̃)) and the fact that θ(b̃, U) ≤ θ(b̃, u1), it is sufficient to prove that:\nθ(span(u1, b), span(u1, b̃)) ≤ π\n2\nθ(b, b̃)\nθ(b̃, u1) ,\nwhich is just a statement about 3-d space. Let α = θ(span(u1, b), span(u1, b̃)) and β = θ(b̃, u1). We can wlog write u1 = (1, 0, 0) and assume span(u1, b) is the x-y plane. Since span(u1, b̃) has angle α with the x-y plane and intersects the x-axis, we can write b̃ = cos(β)u1 + sin(β)u2 for u2 = (0, cos(α), sin(α)). Now, θ(b, b̃) is at least sin−1 of the Euclidean distance of b̃ to the x-y plane, which is sin(α) sin(β). The lemma then follows from the fact that αβ ≤ π2 sin\n−1(sin(α) sin(β)) for 0 ≤ α, β ≤ π2 . The key point of the next lemma is that the errors (angles between ai and ãi) only contribute additively to the overall angle gap between subspaces so long as each new learned vector is sufficiently far from the previously-learned subspace. In contrast, a difficulty with the usual analysis of perturbation of matrices is that while we can assume that each new ãi is far from the span of the previous ã1, . . . , ãi−1, we do\nnot have control over its distance to the span of the past and future vectors {ã1, . . . , ãi−1, ãi+1, . . . , ãk} as in the definition of the height of a matrix (e.g., [22]). Note also that even adding the same vector to two different subspaces can potentially increase their angle (e.g., in Figure 1(a), θ(w1, w̃1) < 0.11 but θ(span(w1, w2), span(w̃1, w2) = π/2).\nLemma 3 Let Vk = span{a1, . . . , ak} and Ṽk = span{ã1, . . . , ãk}. Let acc, γ ≥ 0 and acc ≤ γ2/(10k). Assume for i = 2, . . . , k that θ(ãi, Ṽi−1) ≥ γ, and for i = 1, . . . , n, θ(ai, ãi) ≤ acc. Then\nθ(Vk, Ṽk) ≤ 2k acc γ .\nProof: The proof is by induction on k, on the stronger hypothesis that the conclusion holds for Vk = span{W,a1, . . . , ak} and Ṽk = span{W, ã1, . . . , ãk} for any fixed subspace W . Note that the base case (k = 1), follows directly from Lemma 2, using W = Ṽk−1 = U , ã1 = b̃, and a1 = b. Now, let V ′k = span(Vk−1, ãk). Then we have:\nθ(Vk, Ṽk) ≤ θ(Vk, V ′k) + θ(V ′k, Ṽk) [by triangle inequality]\n≤ π 2 θ(ãk, ak) θ(ãk, Vk−1) + 2(k − 1) acc γ\n[the first term is by Lemma 2, and the second term is by induction using W = span(ãk)]\n≤ π 2\nacc\nθ(ãk, Ṽk−1)− θ(Vk−1, Ṽk−1) + 2(k − 1) acc γ\n[by triangle inequality: θ(ãk, Ṽk−1) ≤ θ(ãk, Vk−1) + θ(Vk−1, Ṽk−1)]\n≤ π 2\nacc\nγ − 2(k−1) accγ +\n2(k − 1) acc γ\n[by assumption and by induction]\n≤ acc γ\n( π\n2\nγ2\nγ2 − 2(k − 1) acc + 2(k − 1) ) ≤ 2k acc/γ,\nwhere the last step comes from using acc ≤ γ2/(10(k − 1)). We now put these together to analyze Algorithm 1 when target functions lie on, or close to, a low-dimensional subspace. Specifically, say that a subsequence of target functions ai1 , ai2 , . . . is γ-separated if each aij has angle greater than γ from the span of the previous ai1 , . . . , aij−1 . Define the γ-effective dimension of targets a1, a2, . . . , am as the size of the largest γ-separated subsequence. Our assumption will be that the γ-effective dimension of the targets is at most k for γ = c for some absolute constant c > 0, where is our desired error rate per target. Note that for γ = 0, γ-effective dimension equals the dimension of the subspace spanned, and for γ > 0 this allows the targets to just be “near” to a low-dimensional subspace.\nTheorem 1 Assume that all marginals Di are isotropic log-concave. Choose γ = c1 and acc s.t. 2k εaccγ + γ = c2 for sufficiently small constants c1, c2 > 0. Consider running Algorithm 1 with parameters and acc on any sequence of targets whose γ-effective dimension is at most k. Then k̃ ≤ k (the rank of Ã is at most k). Moreover the total number of labeled examples needed to learn all the problems to error is Õ(nk/ acc + km/ ) = Õ(nk 2/ 2 + km/ ).\nProof: We divide problems in two types: problems of type (a) are those for which we can learn a classifier of error at most by using the previously learnt problems; the rest are of type (b).\nAlgorithm 1 Life-long learning of halfspaces sharing a common low-dimensional subspace Input: n,m,k, access to labeled examples for problems i ∈ {1, . . . ,m}, parameters and acc.\n1. Learn the first target to error acc to get an n-dimensional vector α1. Set w̃1 = α1; k̃ = 1 and i1 = 1.\n2. For the learning problem i = 2 to m • Attempt to learn using the representation v → (w̃1 · v, ..., w̃k̃ · v). I.e., check if for learning\nproblem i there exists a hypothesis sign(αi,1(w̃1 · v) + · · ·+ αi,k̃(w̃k̃ · v)) of error at most . (a) If yes, set c̃i = (αi,1, . . . , αi,k̃, 0, . . . , 0). (b) If not, learn a classifier αi for problem i of accuracy acc by using the original features. Set\nk̃ = k̃ + 1, ik̃ = i , w̃k̃ = αi, and c̃i = ek̃.\n3. Let W̃ be an k̃ × n matrix whose rows are w̃1, . . . , w̃k̃ and let C̃ be the matrix m × k̃ matrix whose rows are c̃1, . . . , c̃k̃. Compute Ã = C̃W̃ .\nOutput: m predictors; predictor i is v → sign(Ãi · v)\nFor problems of type (a) we achieve error by design. For each problem i of type (b) we open a new row in W̃ , and set w̃k̂ = αi, where k̂ is such that ik̂ = i. We also set c̃i = ek̂, so ãi = αi. Since αi has error at most acc, we have θ(w̃k̂, aik̂) ≤ acc/c for some absolute constant c (by Lemma 1).\nWe next show that k̃ ≤ k. We prove by induction that for each w̃k̂ we create for a problem i = ik̂, we have both (1) aik̂ is γ-far from span{ai1 , · · · , aik̂−1} and (2) w̃k̂ is γ-far from span(w̃1, ..., w̃k̂−1).\nStep k̂ = 1 follows immediately. For the inductive step k̂ > 1: if we create w̃k̂ for a problem i = ik̂, this only happens if there is no vector in the span of the previous metafeatures w̃j , j < i that has error less than for problem ik̂. 3 That is aik̂ is at least /c ′-far from the span{w̃1, ..., w̃k̂−1} for some absolute constant c ′ (by Lemma 1). We also have θ(w̃k̂, aik̂) ≤ εacc/c, therefore, by triangle-inequality, we obtain\nθ(w̃k̂, span(w̃1, ...w̃k̂−1)) ≥ /c ′ − εacc/c ≥ γ.\nThus w̃k̂ is γ-far from span{w̃1, ..., w̃k̂−1}. It remains to show that aik̂ is γ-far from the span of {ai1 , · · · , aik̂−1}. Suppose for contradiction that θ(aik̂ , {ai1 , · · · , aik̂−1}) ≤ γ. We will show that this implies there exists b̃ik̂ ∈ span{w̃1, · · · , w̃k̂−1} with error at most , contradicting the fact that no such vector exists.\nBy construction we have θ(aij , w̃j) ≤ acc/c for j ∈ {1, . . . , k̂ − 1}; also by induction we have w̃j is γ-far from the span of {w̃1, · · · , w̃j−1} for j ∈ {1, . . . , k̂ − 1}. By Lemma 3 we obtain that\nθ(span{ai1 , · · · , aik̂−1}, span{w̃1, · · · , w̃k̂−1}) ≤ 2kεacc/(cγ).\nThese together with triangle inequality imply that\nθ(aik̂ , span{w̃1, · · · , w̃k̂−1}) ≤ γ + 2k εacc cγ ≤ /c′.\nSo by Lemma 1 there exist b̃ik̂ ∈ span{w̃1, · · · , w̃k̂−1} of error at most , which contradicts our assumption. Therefore, our induction is maintained (by condition (2)) and so we have k̃ ≤ k (by condition (1) and our assumption on the γ-effective dimension of the targets).\nBy setting γ = O( ) and acc = O( 2/k) the total number of labeled examples needed to learn all the problems to error is Õ(nk2/ 2 + km/ ), which could be significantly lower than learning each problem separately. In this case the sample complexity would be Ω(mn/ ) even under log-concave distributions [5].\n3Technically, since we are learning over a finite sample, we can only be confident that there is no vector in the span of error at most /2. However, we can absorb these factors of 2 into the constants c, c′.\nNote 1 As stated, Algorithm 1 is not efficient because it requires finding an optimal linear separator in Step 2, which in general is hard. However, for log-concave distributions, there exist algorithms running in time poly(k, 1/ ) that find a near-optimal linear separator: in particular, one of error under the assumption that the optimal separator has error η = / log2(1/ ) [4], and with near-optimal sample complexity [14, 28]. Thus, by reducing εacc by an O(log2(1/ )) factor, one can achieve the bounds of Theorem 1 efficiently."
    }, {
      "heading" : "3.1 Halfspaces with more complex common structure",
      "text" : "In this section we consider life-long learning of halfspaces with more complex common structure, corresponding to a multi-layer network of linear metafeatures. It is at first not obvious how multiple levels of linear nodes could help: if the target vectors span a k-dimensional subspace, then to represent them with a multi-layer linear network, each layer would need to have at least k nodes. However, the numbers of nodes in the network do not tell the whole story: sample complexity of learning can also be reduced via sparsity.\nSpecifically, we assume now that the target functions all lie in a k dimensional space and that furthermore within that k-dimensional space, each target lies in one of r different τ -dimensional spaces. This naturally models settings where there are really r different types of learning problems but they share some commonality across type (given by the common k-dimensional subspace).4 We can view this as a network with two hidden layers: the first layer given by vectors w1, w2, . . . , wk, and the second layer given by r τ -tuples of vectors, u11, . . . , u τ 1 , ..., u 1 r , . . . , u τ r , where u 1 i , . . . , u τ i span one of τ -dimensional spaces. In other words, the first hidden layer captures the overall low dimensionality and the second hidden layer captures sparsity. We assume r m and k n and that τ is a constant. Algorithmically, given a new problem we first try to learn well via a sparse linear combination of only τ second level metafeatures If we fail, we try to learn based on the first level metafeatures and if successful we add a new second level metafeature corresponding to this target. If that fails, we learn using the input features and then we add both a first and second level metafeature corresponding to this target. For logconcave distributions, by using the subspace lemma and an error analysis similar to that for Theorem 1 we can show we have k̃ ≤ k and r̃ ≤ τr. Formally:\nTheorem 2 Assume all marginals Di are isotropic log-concave and the target functions satisfy the above conditions. Consider γ̃ ≤ c , ̃acc ≤ c γ̃ τ , γ ≤ c̃acc, and acc ≤ c γ̃acc k for (sufficiently small) constant c > 0. Consider running Algorithm 2 (see appendix) with parameters , acc, and ̃acc. Then k̃ ≤ k and r̃ ≤ τr. Moreover the total number of examples needed to learn all the problems to error is Õ(nk/ acc + kr/̃acc +m log(r)/ ).\n(Proof in appendix). By setting γ̃ = /2, ̃acc = Θ( 2/τ), γ = Θ( 2/τ), acc = Θ( 4/τ2k) we get that the total number of labeled examples needed to learn all the problems to error is Õ(nk2τ2/ 4 + krτ2/ 2 + mτ log(r)/ ). This could be significantly lower than learning each problem separately or by learning the problems together but only using one layer of metafeatures. Specifically, if we used one layer of metafeatures as in Theorem 1 (corresponding to the k-dimensional subspace) the sample complexity would be O(nk2/ 2 + mk/ ). Alternatively we could have just one middle layer of size rτ and learn sparsely within that, but this would also give worse bounds if r is large. As a concrete example, if is constant,\n4For instance, imagine a job-placement company whose goal is to decide which people would do well in which job. In this setting, we can measure a large number of features for each person (e.g., based on how well they do on various tests). There are then k “intrinsic qualities” that are linear combinations of these features. E.g., “quantitative reasoning” might be one linear combination, “people skills” and “time management” might be others, etc., and really what is important about each person is where they sit in this k-dimensional subspace. Then, different jobs might belong in different low-dimensional spaces within this k-dimensional space, based on what is important for that job. I.e., there are r “kinds” of jobs, each of which has a τ -dimensional subspace that is relevant for it.\nk = √ n, r = n2 and m = n2.5, we get that the two-layer algorithm requires only O(τ2/ 2 + τ log(r)/ ) examples per target. On the other hand, the other two options require at least O(k/ ) examples per target, which could be much worse."
    }, {
      "heading" : "4 Life-long Learning of Monomials",
      "text" : "We now consider a nonlinear case where the metafeatures will be products and combined via products. Specifically, we assume that the instance space is X = {0, 1}n, that the m target functions are conjunctions (i.e., products) of features, and that there exist k monomial metafeatures such that all the target functions can be expressed as conjunctions (products) over them. Our goal will be to learn them efficiently.\nIf the metafeatures do not overlap, then this can be viewed as an instance of the linear case. Each target function can be described by an indicator vector with coefficients in {0, 1} (plus a threshold that can be converted to an integer weight for a dummy variable x0). More importantly, if the metafeatures do not overlap, then the indicator vectors for all the targets are in a space of rank k with basis given by the indicator vectors of the metafeatures. If furthermore the underlying distribution is one for which, when learning from scratch, we can learn the target functions exactly (e.g., a product distribution where each variable is set to 0 some non-negligible fraction of the time) then we can directly apply the analysis for linear case. In fact, the overall analysis is much simpler since we have the targets exactly that were learned from scratch.\nSo, the interesting case is when metafeatures may overlap (it is easy to construct examples where this produces a space of dimension Θ(2k)). Unfortunately, without any additional assumptions, even just the consistency problem is now NP-hard. That is, given a collection of conjunctions, it is NP-hard to determine whether there exist k monomials such that each can be written as a product of subsets of those monomials (it is called the “set-basis problem” [12]). For this reason, we will make a natural anchor-variable assumption that each metafeature mi has at least one variable (call it yi) that is not in any other metafeature mj . So this is a generalization of the disjoint case where every variable in mi is not inside any other mj . We can think of yi as an “anchor variable” for metafeature mi.\nWe now show how with this assumption we can efficiently solve the consistency problem (and find the smallest set of monomials for which one can reconstruct each target). Using this as a subroutine, we then show how to solve an abstract online learning problem where at each stage we must propose a set of at most k monomial metafeatures and then pay a cost of 1 if the next target cannot be written as a product over them. This can then be applied to give efficient life-long learning of related conjunctions over product distributions. In Section 4.4 we give an application to constructing Boolean superimposition-based autoencoders. We then relax the anchor-variable assumption and show how under this relaxed condition we can solve for near-optimal sparse autoencoders as well as life-long learning of conjunctions under relaxed conditions. In Section 5, we build on some of these results to give an algorithm for life-long learning of polynomials."
    }, {
      "heading" : "4.1 Solving the Consistency Problem",
      "text" : "We now show that we can use Algorithm 3 (below) for solving the consistency problem under the anchor variable assumption. That is, given a collection of conjunctions, the goal is to find the fewest monomial metafeatures needed to reconstruct all of them as products of subsets of the metafeatures. Given a conjunction T we denote by vars(T ) the variables appearing in T . Given a variable z and a set of conjunctions TS we denote by N(TS, z) the set of conjunctions in TS that contain z.\nLemma 4 Let TS be a set of conjunctions such that each of them is a conjunction of some subset of metafeatures m1, . . . ,mk satisfying the anchor variable condition. We can use Algorithm 3 to find m̃1, . . . , m̃i,\nAlgorithm 3 Consistency problem for monomial metafeatures with anchor variables Input: set TS = {T1, . . . , Tr} of conjunctions.\n1. Let i = 0. 2. Let h(T ) denote the conjunction of all metafeatures m̃j produced so far that are fully contained in T .\nI.e., vars(h(T )) = ∪{vars(m̃j) : vars(m̃j) ⊆ vars(T )}. 3. While there exists T ∈ TS s.t. vars(T ) 6= vars(h(T )) do:\n(1) Let T be the target of least index in TS s.t. vars(T ) 6= vars(h(T )). (2) Choose zi+1 to be a minimal variable in vars(T ) \\ vars(h(T )); that is, there is no other variable\nz′ ∈ vars(T ) \\ vars(h(T )) s.t. N(TS, z′) ⊂ N(TS, z). If there are multiple options, choose zi+1 to be the option of least index.\n(3) Let vars(m̃i+1) be the intersection of vars(T ) for all T in TS that contain zi+1. That is vars(m̃i+1) = ⋂ T∈TS,zi+1∈vars(T ) vars(T ).\n(4) i=i+1\nOutput: Conjunctions m̃1, . . . , m̃i s.t. each Tj is a conjunction of a subset of them.\ni ≤ k s.t. each Tj ∈ TS is a conjunction of a subset of them. Moreover each m̃i is associated to a metafeature mti s.t. the following conditions are satisfied:\n(a) vars(mti) ⊆ vars(m̃i); that is, m̃i is more specific than mti . (b) For all targets T in TS such that vars(mti) ⊆ vars(T ) we have vars(m̃i) ⊆ vars(T ); that is, m̃i is\nnot too specific. (c) For any j, if yj ∈ vars(m̃i) then vars(mj) ⊆ vars(m̃i).\nProof: Note that for any i for any T ∈ TS we have vars(h(T )) = ∪{vars(m̃j) : j ≤ i, vars(m̃j) ⊆ vars(T )}; that is, vars(h(T )) represents all variables from T that are already used by the previous hypothesized metafeatures m̃j whose relevant variables are contained in T .\nWe prove the desired statement by induction. Assume inductively that m̃1, . . . , m̃i satisfy conditions (a),(b),(c). We show that m̃i+1 satisfies these conditions as well.\nConsider the target T we choose in step 3(1) in round i+ 1. We know zi+1 ∈ vars(T )\\vars(h(T )) and that T is a conjunction of the true metafeatures. So zi+1 belongs to some metafeature mti+1 s.t. vars(mti+1) ⊆ vars(T ) . From the induction hypothesis, by conditions (a),(b) we know that mti+1 6= mti′ for i\n′ ≤ i. To see this assume by contradiction that mti+1 = mti′ for i\n′ ≤ i; so zi+1 ∈ mti′ . By condition (a) we know vars(mti′ ) ⊆ vars(m̃ti′ ) and since vars(mti′ ) ⊆ vars(T ) by condition (b) we have vars(m̃ti′ ) ⊆ vars(T ), so zi+1 ∈ vars(h(T )), contradiction. Consider T ∈ TS such that vars(mti+1) ⊆ vars(T ). Since zi+1 ∈ vars(mti+1) and we create m̃i+1 by intersecting the variables in every target T containing zi+1, we clearly have vars(m̃i+1) ⊆ vars(T ), satisfying condition (b). Also if any target T contains an anchor variable yj , then it must contain mj , so condition (c) is satisfied as well.\nWe now show that (a) is satisfied, namely that vars(mti+1) ⊆ vars(m̃i+1). This could only fail if zi+1 is not an anchor for mti+1 , so in step 2 of the algorithm we intersected some target T that contains zi+1 but does not contain mti+1 . This can only happen if zi+1 also belongs to some other mj . But then zi+1 is not minimal since yti+1 (the true anchor variable for mti+1 , which is also contained in vars(T ) \\ vars(h(T )) by (c)) satisfies N(TS, yti+1) ⊂ N(TS, zi+1), and so would have been chosen instead of zi+1 in step 3(1)."
    }, {
      "heading" : "4.2 An Abstract Online Problem",
      "text" : "Building on Algorithm 3 and Lemma 4, we now describe an algorithm for the following abstract online setting. At each time-step r we propose a set M̃ of at most k hypothesized metafeatures and are provided with a target conjunction Tr. If Tr can be written as a conjunction of metafeatures in M̃ then we pay 0. If not, then we pay 1 and may update our set M̃ using Tr (this corresponds to the case of learning Tr from scratch). Our goal is to bound our total cost, under the assumption that there exists a set of k metafeatures for all targets. To do so we need to argue that each time we pay 1, we can use Tr to make progress.\nAlgorithm 4 Lifelong Learning of Conjunctions with Monomial Metafeatures Input: Targets T1, T2, . . . , Tm provided online.\n1. Initialize TS = ∅ and M̃ = ∅. 2. For r = 1 to m do:\n• If we cannot represent Tr as conjunction of hypothesized metafeatures M̃ then • Add Tr to TS. • Run Algorithm 3 with input TS to produce hypothesized metafeatures M̃ .\nOutput: Hypothesized metafeatures M̃ .\nTheorem 3 The number of targets that need to be learned from scratch in in Algorithm 4 is at most n2 + k.\nProof: For any given set of targets TS learnt from scratch, we define a directed graph GTS on the variables, by adding an edge (xi, xj) if every target in TS that has xi also has xj . Note that if TS ⊆ T̃S we have E(GTS) ⊆ E(GT̃S). We start with the complete directed graph (corresponding to TS = ∅), and then we argue that each time we are forced to learn a new target from scratch and increase TS we either delete at least one edge from the graph or we increment the number of hypothesized metafeatures by 1.\nSuppose the new target Tr cannot be represented using the current hypothesis metafeatures. So we add Tr into TS and re-run Algorithm 3 . Let us look at the first time the new run differs from the old run. There are three possibilities for this difference.\n(1) It could be that we choose a different zi+1 in step 3(2) of Algorithm 3. There are two ways this can happen: (a) the old zi+1 is not minimal any more or (b) it could be some z′ (of lower index than the old zi+1) was not minimal before but is minimal now. In case (a) we have some z′ is now in a strict subset of the targets in TS that contain zi+1 but this was not the case before adding Tr. This means the new target Tr must contain the old zi+1 but not z′, and all previous targets that contained either z′ or zi+1 contained both of them. That means we cut the edge (zi+1, z′). In case (b), some z′ (of lower index than the old zi+1) was not minimal before but is minimal now. This means that before there was some z′′ that was in a strict subset of the targets as z′, but it is not anymore. Now, z′ is minimal, z′′ is no longer in a strict subset of the targets containing z′; so the new target contains z′′ but not z′. So we cut the edge (z′′, z′).\n(2) It could be that we get the same zi+1 but different m̃i+1 in step 3(3); this means vars(m̃i+1) is smaller. Thus we cut the edges between zi+1 and all the variables in the old m̃i+1 that are not in the new m̃i+1.\n(3) It could be that we use the new target Tr in step 3(1). Since we go through the targets in order, the only way that the first difference can be when the new target is used in 3(1) is if every previous metafeature is created the same as before. Therefore, in this case we create a new metafeature. So, the number of metafeatures is increasing and we make progress as desired."
    }, {
      "heading" : "4.3 Applications",
      "text" : "As one immediate application of the above abstract online problem, since conjunctions over {0, 1}n can be exactly learned in the Equivalence Query model with at most n equivalence queries (and conjunctions over {0, 1}k can be learned from at most k equivalence queries), we immediately have the following:\nCorollary 1 Let TS be a sequence of m conjunctions such that each is a conjunction of some subset of metafeatures m1, . . . ,mk satisfying the anchor variable condition. Then this sequence can be learned using only O(mk + n3) equivalence queries total.\nAs another application of the above abstract online problem, we now show we can learn with good sample complexity over any product distribution D.\nTheorem 4 Assume that all Dr = D which is a product distribution, that the metafeatures mi satisfy the anchor variable assumption and all the target functions cr are balanced. We can learn hypotheses h1, . . . , hm of error at most by using Algorithm 5 with parameters s1(n, , δ) = O(n/ log(n/δ)), s2(n, , δ) = k/ log(m/δ), and s3(n, , δ) = n/ log(nk/δ). The total number of labeled examples needed is Õ((n2 + k)n/ log(n/δ) + km/ ).\nProof: Let us call a variable i insignificant if over a sample of size Θ((n/ ) log(n/δ)) appears set to 0 less than /4n fraction of the time. Let I be the set of insignificant variables and let S be the set of significant variables. Let DS be the distribution D restricted to examples that are set to 1 on all variables in I . We can show that error at most /2 over DS implies error at most over D. This is true, since by Chernoff bounds for every variable i we have Px∼D[xi = 0] ≤ /2n if i appears set to 0 less than /4n fraction of the time over a sample of size Θ(n log(n)/δ) . So, by union bound Px∼D[∃i ∈ I, xi = 0] ≤ /2. It remains to show that hypotheses h1, . . . , hm have error at most /2 over DS . First note that for any label r if xi /∈ cr and i ∈ S, then Px∼DS [xi = 0|cr(x) = 1] = Px∼DS [xi = 0]. This follows from two facts. First, since the target cr is a conjunction we have Px∼DS [xi = 0|cr(x) = 1] = Px∼DS [xi = 0|xj = 1∀xj ∈ cr]. Second, because D is a product distribution and DS be the distribution D restricted to examples that are set to 1 on all variables in I , we have Px∼DS [xi = 0|xj = 1∀xj ∈ cr] = Px∼DS [xi = 0]. Furthermore since cr is balanced over D and so over DS we get Px∼DS [xi = 0, cr(x) = 1] ≥ c /n. Note that every time we learn we learn a problem from scratch (by using the original variables), we get n/ log(n/δ) labeled examples from DS . Therefore significant variables that are not in the target will appear set to 0 in at least one positive example. Therefore for every problem i learned based on the original features (via case 1 or 3(b)), we learn the target, that is hi = ci.\nThese together with the argument in the Theorem 3 gives the desired result."
    }, {
      "heading" : "4.4 Sparse Boolean Autoencoders and Relaxing the Anchor-Variable Assumption",
      "text" : "The above results (and in particular, Lemma 4) have an interesting interpretation as constructing a minimal feature space for Boolean, or superimposition-based, autoencoding.\nSpecifically, consider a collection of black-and-while pixel images {Tr} where each Tr ∈ {0, 1}n. Our goal is to contruct a 2-level auto-encoder A (for each r, we want A(Tr) = Tr) with as few nodes in the middle (hidden) level as possible, such that nodes in the hidden level compute the AND of their inputs, and nodes in the output level compute the OR of their inputs. We can view each hidden node in such a network as representing a “piece” of an image, with the autoencoding property requiring that each Tr should be equal to the bitwise-OR of all pieces contained within it (i.e., superimposing them together). Formally, for each\nAlgorithm 5 Transfer Learning of Conjunctions with Monomial Metafeatures Input: parameters n,m,k, , δ; s1(n, , δ), s2(n, , δ), s3(n, , δ), access to unlabeled examples fromDi and label oracles for problems r ∈ {1, . . . ,m}, .\n1. Draw s1(n, , δ) unlabeled examples and identify the set of variables I that are set to 0 less than /4n fraction of the times.\n2. Draw a set S1 of s1(n, , δ) examples from D1, remove from S1 those examples for which not all features in I are set to 1. Label S1 according to problem 1. Find a conjunction h1 consistent with S1. Initialize TS = {h1}.\n3. Run Algorithm 3 with input TS to produce hypothesized metafeatures M̃ .\n4. For the learning problem r = 2 to m\n• Draw a set Sr of s2(n, , δ), examples from Dr, remove from Sr those examples in Sr for which not all features in I is set to 1; re-represent each example in Sr using meta-features in M̃ and check if we can find a conjunction consistent with Sr,\n(a) If yes, let hr be its representation over the original features and record it. (b) If not, draw a set Sr of s3(n, , δ), examples from Dr, remove from Sr those examples for\nwhich a feature in I is set to 1; find a conjunction mhr consistent with Sr. • Add hr to TS. • Run Algorithm 3 with input TS to produce hypothesized metafeatures M̃ .\nOutput: Conjunctions h1, . . . , hm.\nhidden node j, let mj ∈ {0, 1}n denote the indicator vector for the set of inputs to that node (which without loss of generality will also be the set of outputs of that node), and say that mj Tr if each bit set to 1 in mj is also set to 1 in Tr; we then require Tr to be the bitwise-OR of all mj Tr. Lemma 4 then shows that given a collection of images {Tr}, Algorithm 3 finds the smallest number of hidden nodes needed to perform this autoencoding, under the assumption that each metafeature mj contains some anchor-variable (some pixel set to 1 that no other metafeature sets to 1).\nWe now consider the problem of sparse Boolean autoencoding. That is, given a set TS = {Tr}, with each Tr ∈ {0, 1}n, our goal is to find a collection of metafeatures m̃j (perhaps more than n of them) such that each Tr ∈ TS can be written as the bitwise-OR of at most k of the m̃j (where k n). Clearly this is trivial by having one metafeature m̃j for each Tr, so our goal will be to have the (approximately) fewest of them subject to this condition. Additionally, because we want sparse reconstruction, we want for each Tr that |{j : m̃j Tr}| should be small as well. This problem has two motivations. From the perspective of autoencoding, this corresponds to finding a sparse autoencoder (viewing the Tr as pixel images). From the perspective of life-long learning, if this can be done online then (viewing the Tr as conjunctions) it will allow for fast learning, since conjunctions of k out of N variables can be learned with sample complexity (or equivalence queries) only O(k logN); in this case we would actually not need the additional “sparse reconstruction” property above.\nTo solve this problem, we make a relaxed version of the anchor-variable assumption (anchor-variables no longer make sense once the number of metafeatures exceeds the number of input features n) which is that each metafeature should have a set of ≤ c variables (for some constant c) such that any Tr containing that set should have the metafeature as one of its k “relevant metafeatures”. We call this the c-anchor-set\nassumption. Note that metafeatures satisfying the anchor-variable assumption will also satisfy the c-anchorset assumption for c = 1. Note also that in general the c-anchor-set assumption is a requirement on both the metafeatures and on the set TS. Formally, we make the following definition:\nDefinition 1 A set of metafeatures M = {mj} and set of targets TS = {Tr} satisfy the c-anchor-set assumption at sparsity level k if\n1. for each Tr ∈ TS there exists a set Rr of at most k “relevant” metafeatures in M such that Tr is the bitwise-OR of the metafeatures in Rr, and\n2. For each mj ∈ M there exists yj mj of Hamming weight at most c such that for all r, if yj Tr then mj ∈ Rr. Note that in particular this implies that |{j : mj Tr}| ≤ k.\nWe now prove that under this assumption, we can solve for a near-optimal set of metafeatures {m̃j}.\nTheorem 5 Given a set of targets TS = {Tr} in {0, 1}n, suppose there exists a set of metafeatures M satisfying the c-anchor-set assumption at sparsity level k. Then in time poly(nc) we can:\n1. Find a set of O(nc) metafeatures such that each Tr ∈ TS can be written as the bitwise-OR of at most k of them, and\n2. Find a set of O(|M | log(n|TS|)) metafeatures that satisfy the c-anchor-set assumption with respect to TS at sparsity level O(k log(n|TS|)).\nProof: Item (1) is the easier of the two. For each y ∈ {0, 1}n of Hamming weight at most c, define m̃y to be the bitwise-AND of all Tr ∈ TS such that y Tr. By definition of the anchor-set assumption, for each mj ∈ M there exists yj mj of Hamming weight at most c such that for all r, if yj Tr then mj ∈ Rr. Therefore we have both (a) mj m̃yj and (b) m̃yj Tr for all r such that mj ∈ Rr. Therefore each Tr is the bitwise-OR of the (at most k) metafeatures m̃yj such that mj ∈ Rr. For item (2), we begin by creating O(nc) metafeatures m̃y as above. We next set up a linear program to find an optimal fractional subset of these metafeatures, and then round this fractional solution to a set of metafeatures M̃ satisfying (2). Specifically, the LP has one variable Zy for each m̃y with objective\nMinimize: ∑ y Zy,\nSubject to : (1) for all y: 0 ≤ Zy ≤ 1 (2) for all r, i: ∑ y:ei m̃y Tr Zy ≥ 1 (ei is the unit vector in coordinate i)\n(3) for all r: ∑\ny:m̃y Tr Zy ≤ k\nHere, constraint (2) ensures that each Tr is fractionally covered by all the metafeatures contained inside it, and constraint (3) ensures that each Tr fractionally contains at most k metafeatures. Note also that setting Zyj = 1 for each mj ∈M (and setting all other Zy = 0) satisfies all constraints at objective value |M |.\nWe now produce our output set of metafeatures M̃ by independently rounding each Zy to 1 with probability min[1, Zy ln(n\n2|TS|)]. Clearly E[|M̃ |] = O(|M | log(n|TS|)) so the key issue is the coverage of each Tr and the size of the set R̃r = {m̃y ∈ M̃ : m̃y Tr}. Note that item (2) of Definition 1 will be satisfied by how the m̃y were constructed (taking the bitwise-AND of all Tr such that y Tr). First, for coverage, for each r and i such that variable i is set to 1 by Tr, the probability that M̃ does not contain some m̃y such that ei m̃y Tr is maximized when constraint (2) is satisfied at equality and all associated Zy are equal (by concavity). This in turn is at most lim →0(1 − ln(n2|TS|))1/ε = 1/(n2|TS|). Thus, by the union bound, the probability that any Tr fails to be completely covered by R̃r is at most 1/n. Now, to\naddress the size of the sets R̃r, the expected size of each R̃r by constraint (3) and the rounding step is at most k ln(n2|TS|) ≤ max[k, 3] ln(n2|TS|). By Chernoff bounds, the probability any given R̃r has size more than twice this value is at most e−max[k,3] ln(n\n2|TS|)/3 ≤ 1/(n2|TS|). So, by the union bound, the probability that any R̃r is too large is at most 1/n2.\nTheorem 5 shows that we can efficiently find a near-optimal sparse autoencoder for any set of targets in {0, 1}n having an optimal encoder satisfying the c-anchor-set assumption for constant c. Theorem 5 also has the following corollary for online learning from equivalence queries, similar to Corollary 1.\nCorollary 2 Let TS be a sequence ofm conjunctions for which there exists a setM of conjunctive metafeatures satisfying the c-anchor-set assumption at sparsity-level k for some constant c. Then this sequence can be efficiently learned using only O(mk log(n) + n2|M |) equivalence queries total.\nProof: We instantiate O(nc) metafeatures m̃y, one for each y ∈ {0, 1}n of Hamming weight at most c, setting each m̃y initially to the conjunction of all variables. Given a new target Tr, we try to learn it as a conjunction of at most k of these metafeatures using at most O(k log nc) equivalence queries using the Winnow algorithm. If we are unsuccessful, we learn Tr from scratch using at most n equivalence queries. We then (viewing Tr and the m̃y as their indicator vectors) let m̃y ← m̃y & Tr (where “&” denotes bitwiseAND) for all m̃y such that y Tr. This maintains the invariant that for each mj ∈M , we have mj m̃yj , which implies that each time we learn some Tr from scratch we shrink at least one m̃yj by at least one variable. This can happen at most n|M | times."
    }, {
      "heading" : "5 Life-long Learning of Polynomials",
      "text" : "We now show an application of the results in Section 4 to the case where the target functions are polynomials from {0, 1}n to R, whose terms “share” a not too large number of pieces. Specifically, we assume there exist k distinguished monomials (which might overlap) such that each monomial in each target polynomial can be written as a product of some subset of them. For example, if our distinguished monomials are {x1x2x3, x3x4x5, x5x6x7, x7x8x1} then we might have polynomials such as 4x3x4x5x6x7−2x5x6x7x8x1 and 3x1x2x3x4x5 + 3x1x2x3x7x8. If the target polynomials use r distinct monomials in total, then viewed as a network we have k nodes in a first hidden layer, where each is a product of some of the inputs, r nodes in a second hidden layer, where each is a product of outputs of the first hidden layer, and then the final outputs (our target functions) are weighted linear functions of the second hidden layer. Efficiently learning polynomials requires membership queries (under the assumption that juntas are hard to learn) in addition to equivalence queries or random examples even in the single task setting [21]. So we will assume access to membership queries as well. However, our goal will be to use these sparingly, only when we need to learn a new function from scratch. When learning from scratch we use an algorithm of Schapire and Sellie [21] that learns polynomials exactly. Any function from {0, 1}n to R has a unique representation as a polynomial over {0, 1}n, so learning exactly means learning the exact functional form of the target function as a polynomial. As a warmup, let’s first consider a simple case. Assume that the target functions are polynomials that simply use at most k distinct monomials in total. This corresponds to a network with with only one hidden layer of k nodes. In this case, there is a very simple algorithm that exploits the structure of the problem. Let M̃ be the set of hypothesized monomials. Given a new target function, we try to learn a linear function over the monomials in M̃ . If we succeed, we are done and move on to the next problem. If not, we learn from scratch using queries; we will clearly get at least one new monomial we have not seen, and add it to the set M̃ . So, we only need to learn k problems from scratch.\nWe now provide an algorithm for the general, more interesting case. Our theoretical guarantees are under the assumptions that each polynomial in our family has L1 norm bounded by B and the number of terms in\neach is bounded by t. If the target function has an L1 norm bounded by B and its monomials can indeed be written as products of our metafeatures, then by considering all products of metafeatures and running an L1-based algorithm for learning linear functions [17], we can achieve low mean squared error using only O(B2 log(2k)) = O(B2k) examples.\nAlgorithm 6 Multi-task learning for polynomial target functions Input: n,m.\n1. Let M̃ = ∅. M̃ is the set of hypothesized metafeatures for the first hidden layer. Let TS = ∅. TS is the set of terms used to create the hypothesized metafeatures in M̃ .\n2. For the learning problem r = 1 to m\n(a) Create the set conj(M̃) of terms obtained by taking all possible conjunctions of the hypothesized metafeatures in M̃ .\n(b) Attempt to learn problem r as a linear function over the terms in conj(M̃) to low mean squared error (quadratic loss) using O(B2k) examples.\n• If we succeed, record the hypothesis. • Otherwise, run the algorithm of Schapire and Sellie [21] to learn the target Tr for problem\nr exactly based on the original feature representation with equivalence and membership queries.\ni. Expand TS by adding any term in Tr that was not in TS. ii. Run Algorithm 3 with input TS to “compactify” it into the fewest number of (possibly\noverlapping) conjunctive metafeatures that can be used to recreate all the terms in TS. Let M̃ be the resulting metafeatures.\nOutput: Hypothesis functions of low error for each learning task.\nTheorem 6 Assume that the monomials corresponding to the first network layer satisfy the anchor assumption and the L1 norm of the target polynomials is bounded byB. Consider running Algorithm 6. The number of targets needed to learn from scratch is n2 + k. Furthermore the number of hypothesized metafeatures satisfies |M̃ | ≤ k at any time, thus the sample complexity of learning problems in Step 2(b) is O(B2k) per problem.\nProof: In Algorithm 6, M̃ represents the set of hypothesized metafeatures for the first hidden layer – they are learned using Algorithm 3; let k′ = |M̃ |. Let conj(M̃)=all possible conjunctions of hypothesized metafeatures in M̃ ; so TS ⊆ conj(M̃), |conj(M̃)| = 2k′ . We know that in the true underlying network the metafeatures in the first middle layer are monomials satisfying the anchor assumption and the metafeatures in the second middle layer are monomials of meta-features in the first layer. Note that every time we fail to learn in Step 2(b) we know that at least one of the monomials that can make up the target polynomial (which is a metafeature second level of the true network) cannot be written as a conjunction of hypothesized first level metafeatures M̃ . Since we create M̃ by using Algorithm 3, by Theorem 3 we only need to learn at most n2 + k problems from from scratch (that is |TS| ≤ n2 + k), and furthermore, k′ ≤ k. Note that while the sample complexity of Algorithm 6 is linear in k for problems learned from scratch, its running time is exponential in k, due to the work in creating the set conj(M̃). However, a poly(k) bound seems unachievable because it would require solving the junta learning problem. In particular, the problem of learning polynomials over k metafeatures is at least as hard as learning polynomials over {0, 1}k (because\neven if the true metafeatures were given to us in advance, one possibility is that the targets could be arbitrary polynomials over x1, . . . , xk). Thus, for this problem one should think of k as small."
    }, {
      "heading" : "6 Discussion and Open Problems",
      "text" : "In this work we present algorithms for learning new internal representations when presented with a series of learning problems arriving online that share different types of commonalities. For the case of linear threshold functions sharing linear subspaces, we require log-concave distributions to ensure that error can be both upper-bounded and lower-bounded by some “nice” function of angle: the lower bound helps to ensure that the span of accurate hypotheses is close to the span of their corresponding true targets (though one must be careful with error accumulation), and the upper-bound ensures that a sufficiently-close approximation to the span of the true targets is nearly as good as the span itself. It is an interesting question whether one can extend these results to distributions that do not have such properties while still maintaining the streaming nature of the algorithms (i.e., remembering only the learned rules and not the data from which they were generated). For the case of product metafeatures, our results have natural interpretations as autoencoders, which interestingly do not require assumptions such as the problem matrix being incoherent or a generative model, only the anchor-variable or anchor-set assumption. It would be interesting to see whether an analog of the anchor-set assumption could be applied to dictionary learning problems such as in [3].\nAcknowledgements This work was supported in part by NSF grants CCF-0953192, CCF-1451177, CCF1422910, IIS-1065251, ONR grant N00014-09-1-0751, AFOSR grant FA9550-09-1-0538, and a Microsoft Research Faculty Fellowship."
    }, {
      "heading" : "A Proofs for halfspaces with more complex common structure",
      "text" : "We now provide the algorithm and proof for Theorem 2.\nTheorem 2 Assume all marginals Di are isotropic log-concave and the target functions satisfy the above conditions. Consider γ̃ ≤ c , ̃acc ≤ c γ̃ τ , γ ≤ c̃acc, and acc ≤ c γ̃acc k for (sufficiently small) constant c > 0. Consider running Algorithm 2 with parameters , acc, and ̃acc. Then k̃ ≤ k and r̃ ≤ τr. Moreover the total number of examples needed to learn all the problems to error is Õ(nk/ acc+kr/̃acc+m log(r)/ ).\nProof: We divide problems in two types: problems of type (a) are those for which we can learn a classifier of desired error at most by using the previously learnt metafeatures at the second middle level; the rest are of type (b).\nFor problems of type (a) we achieve error at most by design. For each problem i of type (b) we have either opened a new row in Ũ , and we have set w̃r̂ = αi, where r̂ is such that jr̂ = i or we have opened both a new row in r̂ in Ũ and a new row k̂ in W̃ , and set jr̂ = i and ik̂ = i. In both cases, by design and Lemma 1 (and the fact that acc ≤ ̃acc) we have θ(ũr̂W̃ , ajr̂) = O(̃acc); furthermore since c̃i = er̂ we also have θ(ãi, ai) = O(̃acc). Furthermore, for each ũr̂ we create for a problem jr̂ we have that ũr̂W̃ is γ̃-far from the span of those vectors in {ũ1, ..., ũjr̂−1} whose corresponding targets lie in space Us, where Us is one of the r τ -dimensional subspaces that ajr̂ belongs to. (Otherwise if ũr̂ is γ̃-close we would have been able to learn sparsely to error based on the second level metafeatures.)\nUsing this together with the fact that ̃acc = O( γ̃ τ ), we obtain (by Lemma 3) that once we have τ second level meta-features ũjl1 , . . . , ũjlτ whose corresponding targets al1 , . . . , alτ lie in the same τ -dimensional\nspace Us, we have θ(Us, span(ũjl1W̃ , . . . , ũjlτ W̃ )) = O(τ ̃acc/γ̃) ≤ .\nTherefore we will be able to learn based on second level metafeatures any future target belonging to that subspace. This implies r̃ ≤ τr. Using the fact that acc ≤ cγ̃acck , as in the proof of Theorem 1, we can prove by induction that for each w̃k̂ we create for a problem ik̂, we have aik̂ is γ-far from span{ai1 , · · · , aik̂−1} and w̃k̂ is γ-far from span(w̃1, ..., w̃k̂−1); this implies k̃ ≤ k."
    } ],
    "references" : [ {
      "title" : "Convex multi-task feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "Machine Learning Journal",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning topic models - going beyond SVD",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ankur Moitra" ],
      "venue" : "In 53rd Annual IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "New algorithms for learning incoherent and overcomplete dictionaries",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ankur Moitra" ],
      "venue" : "In Proceedings of The 27th Conference on Learning Theory (COLT),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "The power of localization for efficiently learning linear separators with noise",
      "author" : [ "Pranjal Awasthi", "Maria-Florina Balcan", "Philip M. Long" ],
      "venue" : "In Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Active and passive learning of linear separators under log-concave distributions",
      "author" : [ "M.-F. Balcan", "P.M. Long" ],
      "venue" : "Proceedings of the 26th Annual Conference on Learning Theory",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A model of inductive bias learning",
      "author" : [ "J. Baxter" ],
      "venue" : "Journal of Artificial Intelligence Research",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A bayesian/information theoretic model of learning to learn via multiple task sampling",
      "author" : [ "Jonathan Baxter" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1997
    }, {
      "title" : "Exploiting task relatedness for multiple task learning",
      "author" : [ "S. Ben-David", "R. Schuller" ],
      "venue" : "COLT",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Deep learning of representations",
      "author" : [ "Y. Bengio" ],
      "venue" : "Looking forward,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "On the expressive power of deep architectures",
      "author" : [ "Y. Bengio", "O. Delalleau" ],
      "venue" : "ALT",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Linear algorithms for online multitask classification",
      "author" : [ "G. Cavallanti", "N. Cesa-Bianchi", "C. Gentile" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Computers and Intractability: A Guide to the Theory of NP-Completeness",
      "author" : [ "Michael R. Garey", "David S. Johnson" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1979
    }, {
      "title" : "How babies think",
      "author" : [ "A. Gopnik", "A. Meltzoff", "P. Kuhl" ],
      "venue" : "Orion",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Baum’s algorithm learns intersections of halfspaces with respect to log-concave distributions",
      "author" : [ "A.R. Klivans", "P.M. Long", "A. Tang" ],
      "venue" : "RANDOM",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning task grouping and overlap in multi-task learning",
      "author" : [ "A. Kumar", "H. Daume III" ],
      "venue" : "NIPS",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On-line learning of linear functions",
      "author" : [ "Nick Littlestone", "Philip M. Long", "Manfred K. Warmuth" ],
      "venue" : "Computational Complexity,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1995
    }, {
      "title" : "The geometry of logconcave functions and sampling algorithms",
      "author" : [ "László Lovász", "Santosh Vempala" ],
      "venue" : "Random Structures & Algorithms,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Excess risk bounds for multitask learning with trace norm regularization",
      "author" : [ "A. Maurer", "M. Pontil" ],
      "venue" : "Proceedings of the 26th Annual Conference on Learning Theory",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A complete analysis of the l 1, p group-lasso",
      "author" : [ "Volker Roth", "Julia E Vogt" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Learning sparse multivariate polynomials over a field with queries and counterexamples",
      "author" : [ "R.E. Schapire", "L.M. Sellie" ],
      "venue" : "Proceedings of the 6th Annual Conference on Computational Learning Theory",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Lecture notes for 18.409: The behavior of algorithms in practice. Lecture 2: On the condition",
      "author" : [ "D. Spielman" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2002
    }, {
      "title" : "Explanation-Based Neural Network Learning: A Lifelong Learning Approach",
      "author" : [ "S. Thrun" ],
      "venue" : "Kluwer Academic Publishers, Boston, MA",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "editors",
      "author" : [ "S. Thrun", "L.Y. Pratt" ],
      "venue" : "Learning To Learn. Kluwer Academic Publishers, Boston, MA",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Lifelong robot learning",
      "author" : [ "Sebastian Thrun", "Tom M. Mitchell" ],
      "venue" : "Robotics and Autonomous Systems,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1995
    }, {
      "title" : "A neuroidal architecture for cognitive computation",
      "author" : [ "L.G. Valiant" ],
      "venue" : "Journal of the ACM",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A random-sampling-based algorithm for learning intersections of halfspaces",
      "author" : [ "S. Vempala" ],
      "venue" : "JACM, 57(6)",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Mathematical Theories of Interaction with Oracles",
      "author" : [ "L. Yang" ],
      "venue" : "PhD thesis, CMU Dept. Machine Learning",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that, like humans [13], improve their ability to learn as they do so, needing less data (per task) as they learn more.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 21,
      "context" : "A natural approach for tackling this goal (called “life-long learning” [23, 25] or “transfer learning” [1, 19] or “learning to learn” [7, 24]) is to use information from previously-learned tasks to improve the underlying representation used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "A natural approach for tackling this goal (called “life-long learning” [23, 25] or “transfer learning” [1, 19] or “learning to learn” [7, 24]) is to use information from previously-learned tasks to improve the underlying representation used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "A natural approach for tackling this goal (called “life-long learning” [23, 25] or “transfer learning” [1, 19] or “learning to learn” [7, 24]) is to use information from previously-learned tasks to improve the underlying representation used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 17,
      "context" : "A natural approach for tackling this goal (called “life-long learning” [23, 25] or “transfer learning” [1, 19] or “learning to learn” [7, 24]) is to use information from previously-learned tasks to improve the underlying representation used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "A natural approach for tackling this goal (called “life-long learning” [23, 25] or “transfer learning” [1, 19] or “learning to learn” [7, 24]) is to use information from previously-learned tasks to improve the underlying representation used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 22,
      "context" : "A natural approach for tackling this goal (called “life-long learning” [23, 25] or “transfer learning” [1, 19] or “learning to learn” [7, 24]) is to use information from previously-learned tasks to improve the underlying representation used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "These commonalities could be a single low-dimensional or sparse representation, a collection of multiple low-dimensional or sparse representations, or some combination or hierarchy, such as in Deep Learning [9, 10].",
      "startOffset" : 207,
      "endOffset" : 214
    }, {
      "referenceID" : 9,
      "context" : "These commonalities could be a single low-dimensional or sparse representation, a collection of multiple low-dimensional or sparse representations, or some combination or hierarchy, such as in Deep Learning [9, 10].",
      "startOffset" : 207,
      "endOffset" : 214
    }, {
      "referenceID" : 0,
      "context" : "This case has been considered in the “batch” setting in which one has data available for all target functions at the same time and therefore can solve a joint optimization problem [1, 19].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 17,
      "context" : "This case has been considered in the “batch” setting in which one has data available for all target functions at the same time and therefore can solve a joint optimization problem [1, 19].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 1,
      "context" : "We give an efficient algorithm for finding the fewest product-based metafeatures for a given set of target monomials under an “anchor-variable” assumption analogous to the anchor-word assumption of [2], and prove bounds on its performance for learning a series of target functions arriving online.",
      "startOffset" : 198,
      "endOffset" : 201
    }, {
      "referenceID" : 6,
      "context" : "Baxter [7, 6] developed some of the earliest foundations for transfer learning, by providing sample complexity results for achieving low average error in such settings.",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 5,
      "context" : "Baxter [7, 6] developed some of the earliest foundations for transfer learning, by providing sample complexity results for achieving low average error in such settings.",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 7,
      "context" : "Other related sample complexity results appear in [8].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : "Recent work of [19, 16] considers the problem of learning multiple linear separators that share a common low-dimensional subspace in the batch setting where all tasks are given up front.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "Recent work of [19, 16] considers the problem of learning multiple linear separators that share a common low-dimensional subspace in the batch setting where all tasks are given up front.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 18,
      "context" : "There has also been work on applying the Group Lasso method to batch multi-task learning which solves a specific multi-task optimization problem [20].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 10,
      "context" : "[11] considers multi-task learning where explicit known relationships among tasks are exploited for faster learning.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "Discussion in [26] hints toward the type of the algorithms we analyze in Section 3, but without formal analysis about how the error accumulation could harm the sample complexity (which, as we will see, is one of the central challenges in this setting).",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 21,
      "context" : ", [23, 25]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 23,
      "context" : ", [23, 25]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 16,
      "context" : ", [18]), the above procedure will be successful and learn the target functions with much fewer labeled examples in total than by learning each function separately.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "Proof: The proof of the lower bound appears in [5].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 25,
      "context" : "The proof of the upper bound is implicit in the earlier work of [27] – we provide it here for completeness.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "It is known [15] that for some constants k3, k4 we have g(z) ≤ k3e4.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 20,
      "context" : ", [22]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "In this case the sample complexity would be Ω(mn/ ) even under log-concave distributions [5].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "However, for log-concave distributions, there exist algorithms running in time poly(k, 1/ ) that find a near-optimal linear separator: in particular, one of error under the assumption that the optimal separator has error η = / log(1/ ) [4], and with near-optimal sample complexity [14, 28].",
      "startOffset" : 236,
      "endOffset" : 239
    }, {
      "referenceID" : 26,
      "context" : "However, for log-concave distributions, there exist algorithms running in time poly(k, 1/ ) that find a near-optimal linear separator: in particular, one of error under the assumption that the optimal separator has error η = / log(1/ ) [4], and with near-optimal sample complexity [14, 28].",
      "startOffset" : 281,
      "endOffset" : 289
    }, {
      "referenceID" : 11,
      "context" : "That is, given a collection of conjunctions, it is NP-hard to determine whether there exist k monomials such that each can be written as a product of subsets of those monomials (it is called the “set-basis problem” [12]).",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 19,
      "context" : "Efficiently learning polynomials requires membership queries (under the assumption that juntas are hard to learn) in addition to equivalence queries or random examples even in the single task setting [21].",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 19,
      "context" : "When learning from scratch we use an algorithm of Schapire and Sellie [21] that learns polynomials exactly.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "If the target function has an L1 norm bounded by B and its monomials can indeed be written as products of our metafeatures, then by considering all products of metafeatures and running an L1-based algorithm for learning linear functions [17], we can achieve low mean squared error using only O(B2 log(2k)) = O(B2k) examples.",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 19,
      "context" : "• Otherwise, run the algorithm of Schapire and Sellie [21] to learn the target Tr for problem r exactly based on the original feature representation with equivalence and membership queries.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "It would be interesting to see whether an analog of the anchor-set assumption could be applied to dictionary learning problems such as in [3].",
      "startOffset" : 138,
      "endOffset" : 141
    } ],
    "year" : 2014,
    "abstractText" : "It has been a long-standing goal in machine learning, as well as in AI more generally, to develop lifelong learning systems that learn many different tasks over time, and reuse insights from tasks learned, “learning to learn” as they do so. In this work we pose and provide efficient algorithms for several natural theoretical formulations of this goal. Specifically, we consider the problem of learning many different target functions over time, that share certain commonalities that are initially unknown to the learning algorithm. Our aim is to learn new internal representations as the algorithm learns new target functions, that capture this commonality and allow subsequent learning tasks to be solved more efficiently and from less data. We develop efficient algorithms for two very different kinds of commonalities that target functions might share: one based on learning common low-dimensional and unions of low-dimensional subspaces and one based on learning nonlinear Boolean combinations of features. Our algorithms for learning Boolean feature combinations additionally have a dual interpretation, and can be viewed as giving an efficient procedure for constructing near-optimal sparse Boolean autoencoders under a natural “anchor-set” assumption. ar X iv :1 41 1. 14 90 v2 [ cs .L G ] 4 D ec 2 01 4",
    "creator" : "LaTeX with hyperref package"
  }
}