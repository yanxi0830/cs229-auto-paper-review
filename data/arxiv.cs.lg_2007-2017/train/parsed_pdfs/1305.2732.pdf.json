{
  "name" : "1305.2732.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Efficient Algorithm for Learning with Semi-Bandit Feedback",
    "authors" : [ "Gergely Neu", "Gábor Bartók" ],
    "emails" : [ "gergely.neu@gmail.com", "bartok@inf.ethz.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 5.\n27 32\nv1 [\ncs .L\nG ]\n1 3\n√ dT log d). As a side result, we also improve\nthe best known regret bounds for FPL in the full information setting to O(m3/2 √ T log d), gaining a factor of √ d/m over previous bounds for this algorithm.\nKeywords: Follow-the-perturbed-leader, bandit problems, online learning, combinatorial optimization"
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper, we consider a special case of online linear optimization known as online combinatorial optimization (see Figure 1). In every time step t = 1, 2, . . . , T of this sequential decision problem, the learner chooses an action Vt from the finite action set S ⊆ {0, 1}d, where ‖v‖1 ≤ m holds for all v ∈ S. At the same time, the environment fixes a loss vector ℓt ∈ [0, 1]d and the learner suffers loss V ⊤t ℓt. We allow the loss vector ℓt to depend on the previous decisions V1, . . . ,Vt−1 made by the learner, that is, we consider non-oblivious environments. The goal of the learner is to minimize the cumulative loss ∑T\nt=1 V ⊤ t ℓt.\nThen, the performance of the learner is measured in terms of the total expected regret\nRT = max v∈S E\n[ T∑\nt=1\n(Vt − v)⊤ ℓt ] = E [ T∑\nt=1\nV ⊤t ℓt\n] −min\nv∈S E\n[ T∑\nt=1\nv⊤ℓt ] , (1)\nNote that, as indicated in Figure 1, the learner chooses its actions randomly, hence the expectation.\nThe framework described above is general enough to accommodate a number of interesting problem instances such as path planning, ranking and matching problems, finding minimum-weight spanning trees and cut sets. Accordingly, different versions of this general learning problem have drawn considerable attention in the past few years. These versions differ in the amount of information made available to the learner after each round t. In the simplest setting, called the full-information setting, it is assumed that the learner gets to observe the loss vector ℓt regardless of the choice of Vt. However, this assumption does not hold for many practical applications, so it is more interesting to study the problem under partial information, meaning that the learner only gets some limited feedback based on its own decision. In particular, in some problems it is realistic to assume that the learner observes the vector (Vt,1ℓt,1, . . . , Vt,dℓt,d), where Vt,i and ℓt,i are the i\nth components of the vectors Vt and ℓt, respectively. This information scheme is called semi-bandit information. An even more challenging variant is the full bandit scheme where all the learner observes after time t is its own loss V ⊤t ℓt.\nThe most well-known instance of our problem is the (adversarial) multiarmed bandit problem considered in the seminal paper of Auer et al. (2002): in each round of this problem, the learner has to select one of N arms and minimize regret against the best fixed arm, while only observing the losses of the chosen arm. In our framework, this setting corresponds to setting d = N and m = 1, and assuming either full bandit or semi-bandit feedback. Among other contributions concerning this problem, Auer et al. propose an algorithm called Exp3 (Exploration and Exploitation using Exponential weights) based on\nconstructing loss estimates ℓ̂t,i for each component of the loss vector and playing arm i with probability proportional to exp(−η∑t−1s=1 ℓ̂s,i) at time t (η > 0)4. This algorithm is known as the Exponentially Weighted Average (EWA) forecaster\n4 In fact, Auer et al. mix the resulting distribution with a uniform distribution over the arms with probability γ > 0. However, this modification is not needed when one is concerned with the total expected regret, see e.g., Bartók et al. (2011, Chapter 15).\nin the full information case. Besides proving that the total expected regret of this algorithm is O( √ NT logN), Auer et al. also provide a general lower bound\nof Ω( √ NT ) on the regret of any learning algorithm on this particular problem. This lower bound was later matched by the Implicitly Normalized Forecaster (INF) of Audibert and Bubeck (2009, 2010) by using the same loss estimates in a more refined way.\nThe most popular example of online learning problems with actual combinatorial structure is the shortest path problem first considered by Takimoto and Warmuth (2003) in the full information scheme. The same problem was considered by György et al. (2007), who proposed an algorithm that works with semi-bandit information. Since then, we have come a long way in understanding the “price of information” in online combinatorial optimization—see Audibert et al. (2013) for a complete overview of results concerning all of the presented information schemes. The first algorithm directly targeting general online combinatorial optimization problems is due to Koolen et al. (2010): their method named Component Hedge guarantees an optimal regret of O(m √ T log d) in the full information setting. In particular, this algorithm is an instance of the more general algorithm class known as Online Stochastic Mirror Descent (OSMD) or Follow-The-Regularized-Leader (FTRL) methods. Audibert et al. (2013) show that OSMD/FTRL-based methods can also be used for proving optimal regret bounds of O( √ mdT ) for the semi-bandit setting. Finally, Bubeck et al. (2012) show that the natural extension of the EWA forecaster (coupled with an intricate exploration scheme) can be applied to obtain a O(m3/2 √ dT log d) upper bound on the regret when assuming full bandit feedback. This upper bound is off by a factor of √ m log d from the lower bound proved by Audibert et al. (2013). For completeness, we note that the EWA forecaster attains a regret of O(m3/2 √ T log d) in the full information case and O(m √ dT log d) in the semibandit case.\nWhile the results outlined above suggest that there is absolutely no work left to be done in the full information and semi-bandit schemes, we get a different picture if we restrict our attention to computationally efficient algorithms. First, methods based on exponential weighting of each decision vector can only be efficiently implemented for a handful of decision sets S—see Koolen et al. (2010) and Cesa-Bianchi and Lugosi (2012) for some examples. Furthermore, as noted by Audibert et al. (2013), OSMD/FTRL-type methods can be efficiently implemented by convex programming if the convex hull of the decision set can be described by a polynomial number of constraints. Details of such an efficient implementation are worked out by Suehiro et al. (2012), whose algorithm runs in O(d6) time, which can still be prohibitive in practical problems. While Koolen et al. (2010) list some further examples where OSMD/FTRL can be implemented efficiently, we conclude that results concerning general efficient methods for online combinatorial optimization are lacking for (semi or full) bandit information problems.\nThe Follow-the-Perturbed-Leader (FPL) prediction method (first proposed by Hannan (1957) and later rediscovered by Kalai and Vempala (2005)) method\noffers a computationally efficient solution for the online combinatorial optimization problem given that the static combinatorial optimization problemminv∈S v ⊤ℓ admits computationally efficient solutions for any ℓ ∈ Rd. FPL, however, is usually relatively overlooked due to many “reasons”, some of them listed below:\n– The best known bound for FPL in the full information setting is O(m √ dT ),\nwhich is worse than the bounds for both EWA and OSMD/FTRL. However, this result was recently improved to O(m2 √ T polylog d) by Devroye et al.\n(2013). – It is commonly believed that the standard proof techniques for FPL do not\napply directly against adaptive adversaries (see, e.g, the comments of Audibert et al. (2013, Section 2.3) or Cesa-Bianchi and Lugosi (2006, Section 4.3)). On the other hand, a direct analysis for non-oblivious adversaries is given by Poland (2005) in the multi-armed bandit setting. – Considering bandit information, no efficient FPL-style algorithm is known to achieve a regret of O( √ T ). Awerbuch and Kleinberg (2004) and McMahan\nand Blum (2004) proposed FPL-based algorithms for learning with full bandit feedback in shortest path problems, and proved O(T 2/3) bounds on the regret (1). Poland (2005) proved bounds of O( √ NT logN) in the N -armed bandit setting, however, the proposed algorithm requires O(T 2) computations per time step.\nIn this paper, we offer an efficient FPL-based algorithm for regret minimization under semi-bandit feedback. Our approach relies on a novel method for estimating components of the loss vector. The method, called geometric resampling (GR), is based on the idea that the reciprocal of the probability of an event can be estimated by measuring the reoccurrence time. We show that the regret of FPL coupled with GR attains a regret of O(m √ dT log d) in the semi-bandit case. To the best of our knowledge, our algorithm is the first computationally efficient learning algorithm for this learning problem. As a side result, we also improve the regret bounds of FPL in the full information setting to O(m3/2 √ T log d), that is, we close the gaps between the performance bounds of FPL and EWA under both full information and semi-bandit feedback."
    }, {
      "heading" : "2 Loss estimation by geometric resampling",
      "text" : "For a gentle start, consider the problem of regret minimization in N -armed bandits where d = N , m = 1 and the learner has access to the basis vectors {ei}di=1. In each time step, the learner specifies a distribution pt over the arms, where pt,i = P [It = i| Ft−1], where Ft−1 is the history of the learner’s observations and choices up to the end of time step t− 1. Most bandit algorithms rely on feeding some loss estimates to a black-box prediction algorithm. It is commonplace to consider loss estimates of the form\nℓ̂t,i = ℓt,i pt,i I {It = i} , (2)\nwhere pt,i = P [It = i |Ft−1 ], where Ft−1 is the history of observations and internal random variables used by the algorithm up to time t− 1. It is very easy to show that ℓ̂t,i is an unbiased estimate of the loss ℓt,i for all t, i such that pt,i\nis positive. For all other i and t, E [ ℓ̂t,i ∣∣∣Ft−1 ] = 0 ≤ ℓt,i.\nTo our knowledge, all existing bandit algorithms utilize some version of the loss estimates described above. While for many algorithms (such as the Exp3 algorithm of Auer et al. (2002) and the Green algorithm of Allenberg et al. (2006)), the probabilities pt,i are readily available and the estimates (2) can be computed efficiently, this is not necessarily the case for all algorithms. In particular, FPL is notorious for not being able to handle bandit information efficiently since the probabilities pt,i cannot be expressed in closed form. To overcome this difficulty, we propose a different loss estimate that can be efficiently computed even when pt,i is not available for the learner.\nThe estimation procedure executed after each time step t is described below.\n1. The learner draws It ∼ pt. 2. For n = 1, 2, . . .\n(a) Let n ← n+ 1. (b) Draw I ′t(n) ∼ pt. (c) If I ′t(n) = It, break.\n3. Let Kt = n.\nClearly, Kt is a geometrically distributed random variable given It and Ft−1. Consequently, we have E [Kt |Ft−1, It ] = 1/pt,It . We use this property to construct the estimates\nℓ̂t,i = ℓt,iI {It = i}Kt (3)\nfor all arms i. We can easily show that the above estimate is conditionally unbiased whenever pt,i > 0:\nE [ ℓ̂t,i ∣∣∣Ft−1 ] = ∑\nj\npt,jE [ ℓ̂t,i ∣∣∣Ft−1, It = j ]\n= pt,iE [ℓt,iKt |Ft−1, It = i ] = pt,iℓt,iE [Kt |Ft−1, It = i ] = ℓt,i.\nClearly E [ ℓ̂t,i ∣∣∣Ft−1 ] = 0 still holds whenever pt,i = 0.\nThe main problem with the above sampling procedure is that its worstcase running time is unbounded: while the expected number of necessary samples Kt is clearly N , the actual number of samples might be much larger. To overcome this problem, we maximize the number of samples by M and use K̃t = min {Kt,M} instead of Kt in (3). While this capping obviously introduces some bias, we will show later that for appropriate values of M , this bias does not hurt the performance too much.\nAlgorithm 1: FPL with GR\nInput: S = {v(1),v(2), . . . ,v(N)} ⊆ {0, 1}d, η ∈ R+, M ∈ Z+; Initialization: L̂(1) = · · · = L̂(d) = 0; for t=1,. . . ,T do\nDraw Z(1), . . . ,Z(d) independently from distribution Exp(η);\nChoose action I = argmin i∈{1,2,...,N}\n{ v(i)⊤ ( L̂−Z )} ;\nK(1) = · · · = K(d) = M ; k = 0; /* Counter for reoccurred indices */ for n=1,. . . ,M-1 do /* Geometric Resamplig */\nDraw Z′(1), . . . ,Z′(d) independently from distribution Exp(η);\nI(n) = argmin i∈{1,2,...,N}\n{ v(i)⊤ ( L̂−Z′ )} ;\nfor j=1,. . . ,d do\nif v(I(n))(j) = 1 & K(j) = M then K(j) = n; k = k + 1; if k =\n∥∥v(I) ∥∥ 1 then break; /* All indices reoccurred */\nend\nend\nend\nfor j=1,. . . ,d do L̂(j) = L̂(j) +K(j)v(I)(j)ℓ(j) ; /* Update */\nend"
    }, {
      "heading" : "3 An efficient algorithm for learning with semi-bandit feedback",
      "text" : "First, we generalize the geometric resampling method for constructing loss estimates in the semi-bandit case. To this end, let pt,i = P [It = i |Ft−1 ] and qt,j = E [Vt,j |Ft−1 ]. First, the learner plays the decision vector with index It ∼ pt. Then, it draws M additional indices I ′t(1), I ′t(2), . . . , I ′t(M) ∼ pt independently of each other and It. For each j = 1, 2, . . . , d, we define the random variables Kt,j = min {1 ≤ s ≤ M : vj(I ′t(s)) = vj(It)} , with the convention that min {∅} = M . We define the components of our loss estimates ℓ̂t as\nℓ̂t,j = Kt,jVt,jℓt,j (4)\nfor all j = 1, 2, . . . , d. Since Vt,j are nonzero only for coordinates for which ℓt,j is observed, these estimates are well-defined. Letting L̂t = ∑t\ns=1 ℓ̂s, at time step t the algorithm draws the components of the perturbation vectorZt independently from an exponential distribution with parameter η and selects the index\nIt = argmin i∈{1,2,...,N}\n{ v(i)⊤ ( L̂t−1 −Zt )} .\nAs noted earlier, the distribution pt, while implicitly specified by Zt and the estimated cumulative losses L̂t, cannot be expressed in closed form for FPL. However, sampling the indices I ′t(1), I ′ t(2), . . . , I ′ t(M) can be carried out by drawing additional perturbation vectors Z ′t(1),Z ′ t(2), . . . ,Z ′ t(M) independently from the same distribution as Zt. We emphasize that the above additional indices are never actually played by the algorithm, but are only necessary for constructing the loss estimates. We also note that in general, drawing as much as M samples is usually not necessary since the sampling procedure can be terminated as soon as the values of Kt,i are fixed for all i such that Vt,i = 1. We point the reader to Section 3.1 for a more detailed discussion of the running time of the sampling procedure.\nPseudocode for the algorithm can be found in Algorithm 1. We start analyzing our method by proving a simple lemma on the bias of the estimates.\nLemma 1. For all j ∈ {1, 2, . . . , d} and t = 1, 2, . . . , T such that qt,j > 0, the loss estimates (4) satisfy\nE [ ℓ̂t,j ∣∣∣Ft−1 ] = ( 1− (1− qt,j)M ) ℓt,j.\nProof. Fix any j, t satisfying the condition of the lemma. By elementary calculations,\nE [ ℓ̂t,j ∣∣∣Ft−1 ] = qt,jℓt,jE [Kt,j| Ft−1, Vt,j = 1] .\nSetting q = qt,j for simplicity, we have\nE [Kt,j |Ft−1, Vt,j = 1] = ∞∑\nn=1\nn(1− q)n−1q − ∞∑\nn=M\n(n−M)(1− q)n−1q\n=\n∞∑\nn=1\nn(1− q)n−1q − (1− q)M ∞∑\nn=m\n(n−M)(1− q)n−M−1q\n= ( 1− (1− q)M ) ∞∑\nn=1\nn(1− q)n−1q = 1− (1− q) M\nq .\nPutting the two together proves the statement. ⊓⊔ The following theorem gives an upper bound on the total expected regret of the algorithm.\nTheorem 1. The total expected regret of FPL with geometric resampling satisfies\nRn ≤ m (log d+ 1)\nη + ηmdT +\ndT\neM\nunder semi-bandit information. In particular, setting η = √ (log d+ 1) /(dT ) and M ≥ √ dT/(em √ log d+ 1), the regret can be upper bounded as\nRn ≤ 3m √ dT (log d+ 1).\nNote that regret bound stated above holds for any non-oblivious adversary since the decision It only depends on the previous decisions It−1, . . . , I1 through the loss estimates ℓ̂t−1, . . . , ℓ̂1. While the main ingredients of the proof presented below are rather common (we borrow several ideas from Poland (2005), the proofs of Theorems 3 and 8 of Audibert et al. (2013) and the proof of Corollary 4.5 in Cesa-Bianchi and Lugosi (2006)), these elements are carefully combined in our proof to get the desired result.\nProof. Let Z̃ be a perturbation vector drawn independently from the same distribution as Z1 and\nĨt = argmin i∈{1,2,...,N}\n{ v(i)⊤ ( L̂t − Z̃ )} .\nIn what follows, we will crucially use that Ṽt = v(Ĩt) and Vt+1 = v(It+1) are conditionally independent and identically distributed given Fs for any s ≥ t. In particular, introducing the notations\nqt,k = E [Vt,k| Ft−1] q̃t,k = E [ Ṽt,k ∣∣∣Ft ] pt,i = P [It = i| Ft−1] p̃t,i = P [ Ĩt = i ∣∣∣Ft ] ,\nwe will exploit the above property by using qt,k = q̃t−1,k and pt,i = p̃t−1,i numerous times below.\nFirst, let us address the bias of the loss estimates generated by GR. By Lemma 1, we have that E [ ℓ̂t,k ∣∣∣Ft−1 ] ≤ ℓt,k for all k and t, and thus E [ v⊤ℓ̂t ∣∣∣Ft−1 ] ≤ v⊤ℓt holds for any fixed v ∈ S. Furthermore, we have\nE [ Ṽ ⊤t−1ℓ̂t ∣∣∣Ft−1 ] = E\n[ d∑\nk=1\nṼt−1,k ℓ̂t,k ∣∣∣∣∣Ft−1 ]\n=\nd∑\nk=1\nq̃t−1,kE [ ℓ̂t,k ∣∣∣Ft−1 ]\n=\nd∑\nk=1\nq̃t−1,k ( 1− (1 − qt,k)M ) ℓt,k,\nwhere we used the fact that Ṽt−1 is independent of ℓ̂t in the second line and Lemma 1 in the last line. Now using that q̃t−1,k = qt,k for all k and t and noticing that E [ V ⊤t ℓt ∣∣Ft−1 ] = ∑d k=1 qt,kℓt,k, we get that\nE [ V ⊤t ℓt ∣∣Ft−1 ] ≤ E [ Ṽ ⊤t−1ℓ̂t ∣∣∣Ft−1 ] + d∑\ni=1\nqt,k(1 − qt,k)M . (5)\nTo control ∑\nk qt,k(1 − qt,k)M , note that qt,k(1 − qt,k)M ≤ qt,ke−Mqt,k . Since f(q) = qe−Mq takes its maximum at q = 1/M , we get\nd∑\nk=1\nqt,k(1− qt,k)M ≤ d\neM .\nUsing Lemma 3.1 of Cesa-Bianchi and Lugosi (2006) (sometimes referred to\nas the “be-the-leader” lemma) for the sequence ( ℓ̂1 − Z̃, ℓ̂2, . . . , ℓ̂T ) , we obtain\nT∑\nt=1\nṼ ⊤t ℓ̂t − Ṽ ⊤1 Z̃ ≤ T∑\nt=1\nv⊤ℓ̂t − v⊤Z̃\nfor any v ∈ S. Reordering and taking expectations gives\nE\n[ T∑\nt=1\n( Ṽt − v )⊤ ℓ̂t ] ≤ E [( Ṽt − v )⊤ Z̃ ] ≤ m (log d+ 1)\nη , (6)\nwhere we used E [‖Zt‖∞] ≤ log d + 1. To proceed, we study the relationship between p̃t,i and p̃t−1,i = pt,i. To this end, we introduce the “sparse loss vector” ℓ̂′t(i) with components ℓ̂ ′ t,k(i) = vk(i)ℓ̂t,k and\nĨ ′t(i) = argmin i∈{1,2,...,N}\n{ v(i)⊤ ( L̂t−1 + ℓ̂ ′ t(i)− Z̃ )} .\nUsing the notation p̃′t,i = P [ Ĩ ′t(i) = i ∣∣∣Ft ] , we show in Lemma 2 (stated and proved after the proof of the theorem) that p̃′t,i ≤ p̃t,i.5 Also, define\nJ(z) = argmin j∈{1,2,...,N}\n{ v(j)⊤ ( L̂t−1 − z )} .\nLetting f(z) be the density of the perturbations, we clearly have\np̃t−1,i =\n∫\nz∈[0,∞]d\nI {J(z) = i} f(z) dz\n= eη‖ℓ̂ ′ t(i)‖ 1\n∫\nz∈[0,∞]d\nI {J(z) = i} f ( z + ℓ̂′t(i) ) dz\n= eη‖ℓ̂ ′ t(i)‖ 1 ∫ · · · ∫\nzi∈[ℓ̂′t,i,∞]\nI { J ( z − ℓ̂′t(i) ) = i } f(z) dz\n≤ eη‖ℓ̂ ′ t(i)‖ 1\n∫\nz∈[0,∞]d\nI { J ( z − ℓ̂′t(i) ) = i } f(z) dz\n= eη‖ℓ̂ ′ t(i)‖ 1 p̃′t,i ≤ eη‖ℓ̂ ′ t(i)‖ 1 p̃t,i,\n5 Note that a similar trick was used in the proof Corollary 4.5 in Cesa-Bianchi and Lugosi (2006). Also note that this trick only applies in the case of non-negative losses.\nwhere we used f(z) = η exp(−η‖z‖1) for z ∈ [0,∞]d. Now notice that ∥∥ℓ̂′t(i) ∥∥ 1 = v(i)⊤ℓ̂′t(i) = v(i) ⊤ℓ̂t, which yields\np̃t,i ≥ p̃t−1,ie−ηv(i) ⊤ ℓ̂t ≥ p̃t−1,i ( 1− ηv(i)⊤ℓ̂t ) .\nIt follows that\nE [ Ṽ ⊤t−1ℓ̂t ∣∣∣Ft ] = N∑\ni=1\np̃t−1,iv(i) ⊤ℓ̂t ≤\nN∑\ni=1\np̃t,iv(i) ⊤ℓ̂t + η\nN∑\ni=1\np̃t−1,i ( v(i)⊤ℓ̂t )2\n= E [ Ṽ ⊤t ℓ̂t ∣∣∣Ft ] + η N∑\ni=1\np̃t−1,i ( v(i)⊤ℓ̂t )2 ,\n(7)\nwhere we used E [ Ṽt−1 ∣∣∣Ft ] = E [ Ṽt−1 ∣∣∣Ft−1 ] in the second equality. Similarly\nto the proof of Theorem 8 of Audibert et al. (2013), the last term can be upper bounded as\nE\n[ N∑\ni=1\np̃t−1,i ( v(i)⊤ℓ̂t )2 ∣∣∣∣∣Ft−1 ] = E   d∑\nj=1\nd∑\nk=1\n( Ṽt−1,j ℓ̂t,j )( Ṽt−1,k ℓ̂t,k ) ∣∣∣∣∣∣ Ft−1  \n≤ E\n  d∑\nj=1\nℓ̂t,j\nd∑\nk=1\n( Ṽt−1,kKt,kVt,kℓt,k ) ∣∣∣∣∣∣ Ft−1  \n≤ E\n  d∑\nj=1\nℓ̂t,j\nd∑\nk=1\nVt,kℓt,k ∣∣∣∣∣∣ Ft−1  \n≤ mE\n  d∑\nj=1\nℓ̂t,j ∣∣∣∣∣∣ Ft−1   ≤ md,\nwhere we used that Ṽt−1 is independent of Vt, ℓ̂t andKt, so E [ Ṽt−1,kKt,k ∣∣∣Ft−1 ] ≤ 1 in the second inequality, and E [ ℓ̂t,j ∣∣∣Ft−1 ] ≤ 1 in the last inequality. That is,\nwe have proved\nE\n[ T∑\nt=1\nṼ ⊤t−1ℓ̂t\n] ≤ E [ T∑\nt=1\nṼ ⊤t ℓ̂t\n] + ηmd. (8)\nPutting Equations (5), (6) and (8) together, we obtain\nE\n[ T∑\nt=1\n(Vt − v)⊤ ℓt ] ≤ m (log d+ 1)\nη + ηmdT +\ndT\neM\nas stated in the theorem. ⊓⊔\nIn the next lemma, we prove that p̃′t,i ≤ p̃t,i holds for all t and i. While this statement is rather intuitive, we include its simple proof for completeness.\nLemma 2. Fix any i ∈ {1, 2, . . . , N} and any vectors L ∈ Rd and ℓ ∈ [0,∞)d. Furthermore, define the vector ℓ′ with components ℓ′k = vk(i)ℓk and the perturbation vector Z with independent components. Then,\nP [ v(i)⊤ (L+ ℓ′ −Z) ≤ v(j)⊤ (L+ ℓ′ −Z) (∀j ∈ {1, 2, . . . , N}) ]\n≤ P [ v(i)⊤ (L+ ℓ−Z) ≤ v(j)⊤ (L+ ℓ−Z) (∀j ∈ {1, 2, . . . , N}) ] .\nProof. Fix any ∀j ∈ {1, 2, . . . , N} \\ i and define the vector ℓ′′ = ℓ − ℓ′. Define the events\nA′j = { ω : v(i)⊤ (L+ ℓ′ −Z) ≤ v(j)⊤ (L+ ℓ′ −Z) }\nand Aj = { ω : v(i)⊤ (L+ ℓ−Z) ≤ v(j)⊤ (L+ ℓ−Z) } .\nWe have\nA′j = { ω : (v(i)− v(j))⊤ Z ≥ (v(i)− v(j))⊤ (L+ ℓ′) }\n⊆ { ω : (v(i)− v(j))⊤ Z ≥ (v(i)− v(j))⊤ (L+ ℓ′)− v(j)⊤ℓ′′ } = { ω : (v(i)− v(j))⊤ Z ≥ (v(i)− v(j))⊤ (L+ ℓ) } = Aj ,\nwhere we used v(i)ℓ′′ = 0 and v(j)ℓ′′ ≥ 0. Now, since A′j ⊆ Aj , we have ∩Nj=1A′j ⊆ ∩Nj=1Aj , thus proving P [ ∩Nj=1A′j ] ≤ P [ ∩Nj=1Aj ] as requested. ⊓⊔"
    }, {
      "heading" : "3.1 Running time",
      "text" : "Let us now turn our attention to computational issues. As mentioned earlier, since we cut off the number of times we resample the decision vectors, the maximum number of times an arm has to be drawn per time step is M = √ dT . This implies an O(T 3/2d1/2) worst-case running time. However, the expected running time is much more comforting.\nTheorem 2. The expected number of times the algorithm draws an action up to time step T can be upper bounded by dT .\nProof. Let us first modify our algorithm so that it draws even more actions! Let us assume for now that for each coordinate that the original arm had 1, we keep sampling until we get 1 in the same coordinate again. Also let us assume that we do not use cutoff. Instead, we always keep sampling until the desired 1 reoccurs.\nAt time step t, for a given coordinate k with 1, the expected number of samples needed is 1/qt,k, while the probability of coordinate k being 1 is qt,k. Thus, the expected number of samples is\nd∑\nk=1\nqt,k 1\nqt,k = d."
    }, {
      "heading" : "4 Improved bounds for learning with full information",
      "text" : "Our proof technique also enables us to tighten the guarantees for FPL in the full information setting. In particular, we consider the algorithm choosing the index\nIt = argmin i∈{1,2,...,N}\n{ v(i)⊤ (Lt−1 −Zt) } ,\nwhere Lt = ∑t\ns=1 ℓt and the components of Zt are drawn independently from an exponential distribution with parameter η. We state our improved regret bounds concerning this algorithm in the following theorem.\nTheorem 3. Let CT = ∑T t=1 E [ V ⊤t ℓt ] . Then the total expected regret of FPL satisfies\nRn ≤ m (log d+ 1)\nη + ηmCT\nunder full information. In particular, setting η = √ (log d+ 1) /(mT ), the regret can be upper bounded as\nRn ≤ 2m3/2 √ T (log d+ 1).\nNote that the above bound can be further tightened if some upper bound C∗T ≥ CT is available a priori. Once again, these regret bounds hold for any non-oblivious adversary since the decision It depends on the previous decisions It−1, . . . , I1 only through the loss vectors ℓt−1, . . . , ℓ1.\nProof. The statement follows from a simplification of the proof of Theorem 1 when using ℓ̂t = ℓt. First, identically to Equation (6), we have\nE\n[ T∑\nt=1\n( Ṽt − v )⊤ ℓt ] ≤ E [( Ṽt − v )⊤ Z̃ ] ≤ m (log d+ 1)\nη .\nFurther, it is easy to see that the conditions of Lemma 2 are satisfied and, similarly to Equation (7), we also have\nE [ Ṽ ⊤t−1ℓt ] ≤ E [ Ṽ ⊤t ℓt ] + η N∑\ni=1\np̃t−1,i ( v(i)⊤ℓt )2\n≤ E [ Ṽ ⊤t ℓt ] + ηm N∑\ni=1\np̃t−1,iv(i) ⊤ℓt.\nUsing that Vt and Ṽt−1 have the same distribution, we obtain the statement of the theorem. ⊓⊔"
    }, {
      "heading" : "5 Conclusions and open problems",
      "text" : "In this paper, we have described the first general efficient algorithm for online combinatorial optimization under semi-bandit feedback. We have proved that the regret of our algorithm is O(m √ dT log d) in this setting, and have also shown that FPL can achieve O(m3/2 √ T log d) in the full information case when tuned properly. While these bounds are off by a factor of √ m log d and √ m from the respective minimax results, they exactly match the best known regret bounds for the well-studied Exponentially Weighted Forecaster (EWA). Whether the gaps mentioned above can be closed for FPL-style algorithms (e.g., by using more intricate perturbation schemes) remains an important open question. Nevertheless, we regard our contribution as a significant step towards understanding the inherent trade-offs between computational efficiency and performance guarantees in online combinatorial optimization and, more generally, in online linear optimization.\nThe efficiency of our method rests on a novel loss estimation method called geometric resampling (GR). Obviously, this estimation method is not specific to the proposed learning algorithm. While GR has no immediate benefits for OSMD/FTRL-type algorithms where the probabilities qt,k are readily available, it is possible to think about problem instances where EWA can be efficiently implemented while the values of qt,k are difficult to compute. A particular online learning problem where GR can be useful is the problem of online learning in Markovian decision processes (Neu et al., 2010a,b), where computing qt,k can be computationally expensive when the underlying Markovian environment is complicated. This computational burden can be lightened by using GR if the learner has access to a generative model of the environment.6\nThe most important open problem left is the case of efficient online linear optimization with full bandit feedback. Learning algorithms for this problem usually require that the pseudoinverse of the covariance matrix Pt = E [ VtV ⊤ t ∣∣Ft−1 ] is readily available for the learner at each time step (see, e.g., McMahan and Blum 2004; Bartlett et al. 2008; Dani et al. 2008; Cesa-Bianchi and Lugosi 2012; Bubeck et al. 2012). While for most problems, this inverse matrix cannot be computed efficiently, it can be efficiently approximated by geometric resampling when Pt is positive definite as the limit of the matrix geometric series∑∞\nn=1(I−Pt)n. While this knowledge should be enough to construct an efficient FPL-based method for online combinatorial optimization under full bandit feedback, we have to note that the analysis presented in this paper does not carry through directly in this case: as usual loss estimates might take negative values in the full bandit setting, proving a bound similar to Equation (7) cannot be performed in the presented manner."
    }, {
      "heading" : "6 In particular, for an MDP with state and action spaces X and A and worst-case",
      "text" : "mixing time τ > 0, computing the probabilities qt,k can take up to O(|X |3|A|) time, GR returns sufficiently good estimates by generating O(|X ||A|) trajectories of length τ . Deciding which approach is more efficient depends on the problem parameters X , A and τ ."
    } ],
    "references" : [ {
      "title" : "Hannan consistency in on-line learning in case of unbounded losses under partial monitoring",
      "author" : [ "C. Allenberg", "P. Auer", "L. Györfi", "Ottucsák", "Gy" ],
      "venue" : "In ALT,",
      "citeRegEx" : "Allenberg et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Allenberg et al\\.",
      "year" : 2006
    }, {
      "title" : "Minimax policies for bandits games",
      "author" : [ "Audibert", "J.-Y", "S. Bubeck" ],
      "venue" : "COLT",
      "citeRegEx" : "Audibert et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2009
    }, {
      "title" : "Regret bounds and minimax policies under partial monitoring",
      "author" : [ "Audibert", "J.-Y", "S. Bubeck" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Audibert et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2010
    }, {
      "title" : "Regret in online combinatorial optimization",
      "author" : [ "J.Y. Audibert", "S. Bubeck", "G. Lugosi" ],
      "venue" : "To appear in Mathematics of Operations Research",
      "citeRegEx" : "Audibert et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2013
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Adaptive routing with end-to-end feedback: distributed learning and geometric approaches",
      "author" : [ "B. Awerbuch", "R.D. Kleinberg" ],
      "venue" : "In Proceedings of the 36th ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Awerbuch and Kleinberg,? \\Q2004\\E",
      "shortCiteRegEx" : "Awerbuch and Kleinberg",
      "year" : 2004
    }, {
      "title" : "High probability regret bounds for online optimization",
      "author" : [ "P. Bartlett", "V. Dani", "T. Hayes", "S. Kakade", "A. Rakhlin", "A. Tewari" ],
      "venue" : "In Proceedings of the 21st Annual Conference on Learning Theory (COLT)",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2008
    }, {
      "title" : "Towards minimax policies for online linear optimization with bandit feedback",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi", "S.M. Kakade" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2012
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi",
      "year" : 2006
    }, {
      "title" : "Combinatorial bandits",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Cesa.Bianchi and Lugosi,? \\Q2012\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi",
      "year" : 2012
    }, {
      "title" : "The price of bandit information for online optimization",
      "author" : [ "V. Dani", "T. Hayes", "S. Kakade" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "Prediction by random-walk perturbation",
      "author" : [ "L. Devroye", "G. Lugosi", "G. Neu" ],
      "venue" : "Accepted to the Twenty-Sixth Conference on Learning Theory",
      "citeRegEx" : "Devroye et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Devroye et al\\.",
      "year" : 2013
    }, {
      "title" : "The on-line shortest path problem under partial monitoring",
      "author" : [ "A. György", "T. Linder", "G. Lugosi", "Ottucsák", "Gy" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "György et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "György et al\\.",
      "year" : 2007
    }, {
      "title" : "Approximation to Bayes risk in repeated play. Contributions to the theory of games, 3:97–139",
      "author" : [ "J. Hannan" ],
      "venue" : null,
      "citeRegEx" : "Hannan,? \\Q1957\\E",
      "shortCiteRegEx" : "Hannan",
      "year" : 1957
    }, {
      "title" : "Efficient algorithms for online decision problems",
      "author" : [ "A. Kalai", "S. Vempala" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Kalai and Vempala,? \\Q2005\\E",
      "shortCiteRegEx" : "Kalai and Vempala",
      "year" : 2005
    }, {
      "title" : "Hedging structured concepts",
      "author" : [ "W. Koolen", "M. Warmuth", "J. Kivinen" ],
      "venue" : "In Proceedings of the 23rd Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Koolen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koolen et al\\.",
      "year" : 2010
    }, {
      "title" : "Online geometric optimization in the bandit setting against an adaptive adversary",
      "author" : [ "H.B. McMahan", "A. Blum" ],
      "venue" : "In Proceedings of the Eighteenth Conference on Computational Learning Theory,",
      "citeRegEx" : "McMahan and Blum,? \\Q2004\\E",
      "shortCiteRegEx" : "McMahan and Blum",
      "year" : 2004
    }, {
      "title" : "The online loop-free stochastic shortest-path problem",
      "author" : [ "G. Neu", "A. György", "Szepesvári", "Cs" ],
      "venue" : "In Proceedings of the Twenty-Third Conference on Computational Learning Theory,",
      "citeRegEx" : "Neu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Neu et al\\.",
      "year" : 2010
    }, {
      "title" : "Online Markov decision processes under bandit feedback",
      "author" : [ "G. Neu", "A. György", "Szepesvári", "Cs", "A. Antos" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Neu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Neu et al\\.",
      "year" : 2010
    }, {
      "title" : "FPL analysis for adaptive bandits",
      "author" : [ "J. Poland" ],
      "venue" : "Symposium on Stochastic Algorithms, Foundations and Applications (SAGA’05),",
      "citeRegEx" : "Poland,? \\Q2005\\E",
      "shortCiteRegEx" : "Poland",
      "year" : 2005
    }, {
      "title" : "Online prediction under submodular constraints. In Algorithmic Learning Theory, volume 7568 of Lecture Notes in Computer Science, pages 260–274",
      "author" : [ "D. Suehiro", "K. Hatano", "S. Kijima", "E. Takimoto", "K. Nagano" ],
      "venue" : null,
      "citeRegEx" : "Suehiro et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Suehiro et al\\.",
      "year" : 2012
    }, {
      "title" : "Paths kernels and multiplicative updates",
      "author" : [ "E. Takimoto", "M. Warmuth" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Takimoto and Warmuth,? \\Q2003\\E",
      "shortCiteRegEx" : "Takimoto and Warmuth",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "The most well-known instance of our problem is the (adversarial) multiarmed bandit problem considered in the seminal paper of Auer et al. (2002): in each round of this problem, the learner has to select one of N arms and minimize regret against the best fixed arm, while only observing the losses of the chosen arm.",
      "startOffset" : 126,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : "The most popular example of online learning problems with actual combinatorial structure is the shortest path problem first considered by Takimoto and Warmuth (2003) in the full information scheme.",
      "startOffset" : 138,
      "endOffset" : 166
    }, {
      "referenceID" : 8,
      "context" : "The same problem was considered by György et al. (2007), who proposed an algorithm that works with semi-bandit information.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "Since then, we have come a long way in understanding the “price of information” in online combinatorial optimization—see Audibert et al. (2013) for a complete overview of results concerning all of the presented information schemes.",
      "startOffset" : 121,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "Since then, we have come a long way in understanding the “price of information” in online combinatorial optimization—see Audibert et al. (2013) for a complete overview of results concerning all of the presented information schemes. The first algorithm directly targeting general online combinatorial optimization problems is due to Koolen et al. (2010): their method named Component Hedge guarantees an optimal regret of O(m √ T log d) in the full information setting.",
      "startOffset" : 121,
      "endOffset" : 353
    }, {
      "referenceID" : 1,
      "context" : "Since then, we have come a long way in understanding the “price of information” in online combinatorial optimization—see Audibert et al. (2013) for a complete overview of results concerning all of the presented information schemes. The first algorithm directly targeting general online combinatorial optimization problems is due to Koolen et al. (2010): their method named Component Hedge guarantees an optimal regret of O(m √ T log d) in the full information setting. In particular, this algorithm is an instance of the more general algorithm class known as Online Stochastic Mirror Descent (OSMD) or Follow-The-Regularized-Leader (FTRL) methods. Audibert et al. (2013) show that OSMD/FTRL-based methods can also be used for proving optimal regret bounds of O( √ mdT ) for the semi-bandit setting.",
      "startOffset" : 121,
      "endOffset" : 671
    }, {
      "referenceID" : 1,
      "context" : "Since then, we have come a long way in understanding the “price of information” in online combinatorial optimization—see Audibert et al. (2013) for a complete overview of results concerning all of the presented information schemes. The first algorithm directly targeting general online combinatorial optimization problems is due to Koolen et al. (2010): their method named Component Hedge guarantees an optimal regret of O(m √ T log d) in the full information setting. In particular, this algorithm is an instance of the more general algorithm class known as Online Stochastic Mirror Descent (OSMD) or Follow-The-Regularized-Leader (FTRL) methods. Audibert et al. (2013) show that OSMD/FTRL-based methods can also be used for proving optimal regret bounds of O( √ mdT ) for the semi-bandit setting. Finally, Bubeck et al. (2012) show that the natural extension of the EWA forecaster (coupled with an intricate exploration scheme) can be applied to obtain a O(m √ dT log d) upper bound on the regret when assuming full bandit feedback.",
      "startOffset" : 121,
      "endOffset" : 829
    }, {
      "referenceID" : 1,
      "context" : "Since then, we have come a long way in understanding the “price of information” in online combinatorial optimization—see Audibert et al. (2013) for a complete overview of results concerning all of the presented information schemes. The first algorithm directly targeting general online combinatorial optimization problems is due to Koolen et al. (2010): their method named Component Hedge guarantees an optimal regret of O(m √ T log d) in the full information setting. In particular, this algorithm is an instance of the more general algorithm class known as Online Stochastic Mirror Descent (OSMD) or Follow-The-Regularized-Leader (FTRL) methods. Audibert et al. (2013) show that OSMD/FTRL-based methods can also be used for proving optimal regret bounds of O( √ mdT ) for the semi-bandit setting. Finally, Bubeck et al. (2012) show that the natural extension of the EWA forecaster (coupled with an intricate exploration scheme) can be applied to obtain a O(m √ dT log d) upper bound on the regret when assuming full bandit feedback. This upper bound is off by a factor of √ m log d from the lower bound proved by Audibert et al. (2013). For completeness, we note that the EWA forecaster attains a regret of O(m √ T log d) in the full information case and O(m √ dT log d) in the semibandit case.",
      "startOffset" : 121,
      "endOffset" : 1138
    }, {
      "referenceID" : 10,
      "context" : "First, methods based on exponential weighting of each decision vector can only be efficiently implemented for a handful of decision sets S—see Koolen et al. (2010) and Cesa-Bianchi and Lugosi (2012) for some examples.",
      "startOffset" : 143,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "(2010) and Cesa-Bianchi and Lugosi (2012) for some examples.",
      "startOffset" : 11,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, as noted by Audibert et al. (2013), OSMD/FTRL-type methods can be efficiently implemented by convex programming if the convex hull of the decision set can be described by a polynomial number of constraints.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, as noted by Audibert et al. (2013), OSMD/FTRL-type methods can be efficiently implemented by convex programming if the convex hull of the decision set can be described by a polynomial number of constraints. Details of such an efficient implementation are worked out by Suehiro et al. (2012), whose algorithm runs in O(d) time, which can still be prohibitive in practical problems.",
      "startOffset" : 25,
      "endOffset" : 304
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, as noted by Audibert et al. (2013), OSMD/FTRL-type methods can be efficiently implemented by convex programming if the convex hull of the decision set can be described by a polynomial number of constraints. Details of such an efficient implementation are worked out by Suehiro et al. (2012), whose algorithm runs in O(d) time, which can still be prohibitive in practical problems. While Koolen et al. (2010) list some further examples where OSMD/FTRL can be implemented efficiently, we conclude that results concerning general efficient methods for online combinatorial optimization are lacking for (semi or full) bandit information problems.",
      "startOffset" : 25,
      "endOffset" : 421
    }, {
      "referenceID" : 13,
      "context" : "The Follow-the-Perturbed-Leader (FPL) prediction method (first proposed by Hannan (1957) and later rediscovered by Kalai and Vempala (2005)) method",
      "startOffset" : 75,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "The Follow-the-Perturbed-Leader (FPL) prediction method (first proposed by Hannan (1957) and later rediscovered by Kalai and Vempala (2005)) method",
      "startOffset" : 75,
      "endOffset" : 140
    }, {
      "referenceID" : 5,
      "context" : "However, this result was recently improved to O(m √ T polylog d) by Devroye et al. (2013). – It is commonly believed that the standard proof techniques for FPL do not apply directly against adaptive adversaries (see, e.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "g, the comments of Audibert et al. (2013, Section 2.3) or Cesa-Bianchi and Lugosi (2006, Section 4.3)). On the other hand, a direct analysis for non-oblivious adversaries is given by Poland (2005) in the multi-armed bandit setting.",
      "startOffset" : 19,
      "endOffset" : 197
    }, {
      "referenceID" : 1,
      "context" : "g, the comments of Audibert et al. (2013, Section 2.3) or Cesa-Bianchi and Lugosi (2006, Section 4.3)). On the other hand, a direct analysis for non-oblivious adversaries is given by Poland (2005) in the multi-armed bandit setting. – Considering bandit information, no efficient FPL-style algorithm is known to achieve a regret of O( √ T ). Awerbuch and Kleinberg (2004) and McMahan and Blum (2004) proposed FPL-based algorithms for learning with full bandit feedback in shortest path problems, and proved O(T ) bounds on the regret (1).",
      "startOffset" : 19,
      "endOffset" : 371
    }, {
      "referenceID" : 1,
      "context" : "g, the comments of Audibert et al. (2013, Section 2.3) or Cesa-Bianchi and Lugosi (2006, Section 4.3)). On the other hand, a direct analysis for non-oblivious adversaries is given by Poland (2005) in the multi-armed bandit setting. – Considering bandit information, no efficient FPL-style algorithm is known to achieve a regret of O( √ T ). Awerbuch and Kleinberg (2004) and McMahan and Blum (2004) proposed FPL-based algorithms for learning with full bandit feedback in shortest path problems, and proved O(T ) bounds on the regret (1).",
      "startOffset" : 19,
      "endOffset" : 399
    }, {
      "referenceID" : 1,
      "context" : "g, the comments of Audibert et al. (2013, Section 2.3) or Cesa-Bianchi and Lugosi (2006, Section 4.3)). On the other hand, a direct analysis for non-oblivious adversaries is given by Poland (2005) in the multi-armed bandit setting. – Considering bandit information, no efficient FPL-style algorithm is known to achieve a regret of O( √ T ). Awerbuch and Kleinberg (2004) and McMahan and Blum (2004) proposed FPL-based algorithms for learning with full bandit feedback in shortest path problems, and proved O(T ) bounds on the regret (1). Poland (2005) proved bounds of O( √ NT logN) in the N -armed bandit setting, however, the proposed algorithm requires O(T ) computations per time step.",
      "startOffset" : 19,
      "endOffset" : 552
    }, {
      "referenceID" : 3,
      "context" : "While for many algorithms (such as the Exp3 algorithm of Auer et al. (2002) and the Green algorithm of Allenberg et al.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "(2002) and the Green algorithm of Allenberg et al. (2006)), the probabilities pt,i are readily available and the estimates (2) can be computed efficiently, this is not necessarily the case for all algorithms.",
      "startOffset" : 34,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "While the main ingredients of the proof presented below are rather common (we borrow several ideas from Poland (2005), the proofs of Theorems 3 and 8 of Audibert et al.",
      "startOffset" : 104,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "While the main ingredients of the proof presented below are rather common (we borrow several ideas from Poland (2005), the proofs of Theorems 3 and 8 of Audibert et al. (2013) and the proof of Corollary 4.",
      "startOffset" : 153,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : "While the main ingredients of the proof presented below are rather common (we borrow several ideas from Poland (2005), the proofs of Theorems 3 and 8 of Audibert et al. (2013) and the proof of Corollary 4.5 in Cesa-Bianchi and Lugosi (2006)), these elements are carefully combined in our proof to get the desired result.",
      "startOffset" : 153,
      "endOffset" : 241
    }, {
      "referenceID" : 8,
      "context" : "1 of Cesa-Bianchi and Lugosi (2006) (sometimes referred to as the “be-the-leader” lemma) for the sequence ( l̂1 − Z̃, l̂2, .",
      "startOffset" : 5,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "5 in Cesa-Bianchi and Lugosi (2006). Also note that this trick only applies in the case of non-negative losses.",
      "startOffset" : 5,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "Similarly to the proof of Theorem 8 of Audibert et al. (2013), the last term can be upper bounded as",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "Learning algorithms for this problem usually require that the pseudoinverse of the covariance matrix Pt = E [ VtV ⊤ t ∣Ft−1 ] is readily available for the learner at each time step (see, e.g., McMahan and Blum 2004; Bartlett et al. 2008; Dani et al. 2008; Cesa-Bianchi and Lugosi 2012; Bubeck et al. 2012).",
      "startOffset" : 181,
      "endOffset" : 305
    }, {
      "referenceID" : 10,
      "context" : "Learning algorithms for this problem usually require that the pseudoinverse of the covariance matrix Pt = E [ VtV ⊤ t ∣Ft−1 ] is readily available for the learner at each time step (see, e.g., McMahan and Blum 2004; Bartlett et al. 2008; Dani et al. 2008; Cesa-Bianchi and Lugosi 2012; Bubeck et al. 2012).",
      "startOffset" : 181,
      "endOffset" : 305
    }, {
      "referenceID" : 9,
      "context" : "Learning algorithms for this problem usually require that the pseudoinverse of the covariance matrix Pt = E [ VtV ⊤ t ∣Ft−1 ] is readily available for the learner at each time step (see, e.g., McMahan and Blum 2004; Bartlett et al. 2008; Dani et al. 2008; Cesa-Bianchi and Lugosi 2012; Bubeck et al. 2012).",
      "startOffset" : 181,
      "endOffset" : 305
    }, {
      "referenceID" : 7,
      "context" : "Learning algorithms for this problem usually require that the pseudoinverse of the covariance matrix Pt = E [ VtV ⊤ t ∣Ft−1 ] is readily available for the learner at each time step (see, e.g., McMahan and Blum 2004; Bartlett et al. 2008; Dani et al. 2008; Cesa-Bianchi and Lugosi 2012; Bubeck et al. 2012).",
      "startOffset" : 181,
      "endOffset" : 305
    } ],
    "year" : 2013,
    "abstractText" : "We consider the problem of online combinatorial optimization under semi-bandit feedback. The goal of the learner is to sequentially select its actions from a combinatorial decision set so as to minimize its cumulative loss. We propose a learning algorithm for this problem based on combining the Follow-the-Perturbed-Leader (FPL) prediction method with a novel loss estimation procedure called Geometric Resampling (GR). Contrary to previous solutions, the resulting algorithm can be efficiently implemented for any decision set where efficient offline combinatorial optimization is possible at all. Assuming that the elements of the decision set can be described with d-dimensional binary vectors with at most m non-zero entries, we show that the expected regret of our algorithm after T rounds is O(m √ dT log d). As a side result, we also improve the best known regret bounds for FPL in the full information setting to O(m √ T log d), gaining a factor of √ d/m over previous bounds for this algorithm.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}