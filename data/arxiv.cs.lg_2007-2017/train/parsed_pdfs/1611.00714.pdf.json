{
  "name" : "1611.00714.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Scalable Semi-Supervised Learning over Networks using Nonsmooth Convex Optimization",
    "authors" : [ "Alexander Jung", "Alfred O. Hero III", "Alexandru Mara", "Sabeur Aridhi" ],
    "emails" : [ "hero@eecs.umich.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n00 71\n4v 1\n[ cs\n.L G\n] 2\nN ov\n2 01\n6\nI. INTRODUCTION\nModern technological systems generate (heterogeneous) data at unprecedented scale, i.e., “Big Data” [9], [11], [16], [23]. While lacking a precise formal definition, Big Data problems typically share four main characteristics: (i) large data volume, (ii) high speed of data generation, (iii) data is heterogeneous, i.e., partially labeled or unlabeled, mixture of audio, video and text data and (iv) data is noisy, i.e., there are statistical variations due to missing labels, labeling errors, or poor data curation [23]. Moreover, in a wide range of big data applications, e.g., social networks, sensor networks, communication networks, and biological networks an intrinsic graph (or network) structure is present. This graph structure reflects either the physical properties of a system (e.g., public transportation networks) or statistical dependencies (e.g., probabilistic graphical models for bioinformatics). Quite often, these two notions of graph structure coincide: in a wireless sensor network, the graph modeling the communication links between nodes and the graph formed by statistical dependencies between sensor measurements resemble each other since both graphs are induced by the nodes mutual proximity [17], [21], [24].\nOn the algorithmic side, having a graph model for the observed datapoints faciliates scalable distributed data processing, in the form of message passing on the graph. On a higher-level, graph models are suitable to deal with data of diverse nature, since they only require a weak notion of similarity between datapoints. Moreover, graph models allow to capitalize on massive amounts of unlabeled data via semi-supervised learning. In particular, semi-supervised learning exploits the information contained in large amounts of unlabeled datapoints by considering their similarities to a small number of labeled datapoints.\nIn this paper, we consider the problem of semi-supervised learning using a graph model for the raw data. The observed data consists of a small number of labeled datapoints and a huge amount of unlabeled datapoints. We tackle this learning problem by casting the dataset as a graph signal. In this graph signal model, the different dimensions of the data are identified as variables and the observed values of these variables are called signals. These signals are represented by nodes of a (empirical) graph whose edges represent pairwise dependencies between signals. Imposition of such graph signal structure on the data is\nThis work was partially supported by the Vienna Science and Technology Fund (WWTF) under grant ICT15-119, Army Research Office grants W911NF-15-1-0479 and W911NF-15-1-0241 and US Department of Energy grant DE-NA0002534.\nanalogous to making the smoothness assumption of semi-supervised learning [7]: signals that are connected by an edge in the graph have similar labels. In other words, the graph signal is expected to reflect the underlying graph structure in the sense that the labels of signals on closely connected nodes have high mutual correlation and thus these signals form close-knit clusters or communities [12]. In order to quantify the smoothness assumption underlying semi-supervised learning, one can use different measures to incorporate the topological dependency structure of graphs signals. For example, one can project the signals onto the column space of the graph Laplacian matrix, using the squared norm of the projected signals, i.e., the graph Laplacian form, as a measure of smoothness. This is the basis for many well-known label propagation methods [7].\nIn contrast, the approach proposed in this paper is based on using (graph) total variation [22], which provides a more natural match between smoothness and the community structure of the data, i.e., input or feature signal nodes forming a community or cluster should yield similar output values or labels."
    }, {
      "heading" : "Contributions and Outline",
      "text" : "In Section II, we formulate semi-supervised learning using a graph model for the observed data as a convex optimization problem. By adapting Nesterov’s method for nonsmooth convex optimization, which is reviewed in Section III, we propose an efficient learning algorithm in Section IV. We then present a message passing formulation of our learning algorithm in Section V, which only requires local information updating. We also discuss how to implement the message passing formulation for graphs of massive size."
    }, {
      "heading" : "Notation",
      "text" : "Matrices are denoted by boldcase uppercase letters (e.g. A) and column vectors are denoted by boldface lowercase letters (e.g. x). The ith entry of the vector x is denoted by xi. and the entry in the ith row and jth column of matrix A is Ai,. For vectors x,y ∈ RN and matrices X,Y ∈ RN×N , we define the inner products 〈x,y〉2 := ∑ i xiyi and 〈X,Y〉F := ∑ i,j Xi,jYi,j with induced norms ‖x‖2 := √\n〈x,x〉2 and ‖X‖F := √\n〈X,X〉F. For a generic Hilbert space H, we denote its inner product by 〈·, ·〉H. Given a linear operator B mapping the Hilbert space H1 into the Hilbert space H2, we denote its adjoint by B∗ and by ‖B‖op := sup‖x‖H1≤1 ‖Bx‖H2 its operator norm. The operator norm of a matrix A ∈ R\nM×N , interpreted as a mapping from Hilbert space RM to RN , reduces to the spectral norm ‖A‖2 := supx∈RN\\{0} ‖Ax‖2/‖x‖2. The ith column of the identity matrix I is denoted by ei. Given a closed convex subset C ⊆ H of a Hilbert space, we denote by πC(x) = arg min\nz∈C ‖z − x‖H the orthogonal projection on C. For a diagonal matrix\nD ∈ RN×N with non-negative main diagonal entries di,i, we denote by D1/2 the diagonal matrix with main diagonal entries √ di,i."
    }, {
      "heading" : "II. PROBLEM FORMULATION",
      "text" : "We consider a heterogeneous dataset D={zi}Ni=1⊆Z consisting of N datapoints zi∈Z , which might be of significantly different nature, e.g., z1 ∈ Rd, z2 is a continuous-time signal (i.e., z2 : R→R) and z3 might represent the bag-of-words histogram of a text document. Thus, we assume the input space Z rich enough to accomodate for strongly heterogeneous data. Associated with the dataset D is an undirected empirical graph G = (V, E ,W) with node set V = {1, . . . , N}, edge set E ⊆ V × V and symmetric weight matrix W ∈ RN×N . The nodes represent the datapoints, i.e., node i corresponds to the datapoint zi. An undirected edge (i, j) ∈ E encodes some notion of (physical or statistical) similarity from datapoint zi to datapoint zj . Moreover, the presence of an edge (i, j) ∈ E between nodes i, j ∈ V is indicated by a nonzero entry Wi,j = Wj,i of the weight matrix W. Given an edge (i, j) ∈ E , the nonzero value Wi,j > 0 represents the strength of the connection from node i to node j. We assume the empirical graph to be simple, i.e., it contains no self-loops (Wi,i =0 for all i∈V). The neighborhood N (i) and degree di of node i ∈ V is defined, respectively, as\nN (i) := {j ∈ V : (i, j)∈E} (1)\nand di := ∑\nj∈N (i)\nWi,j. (2)\nAn key parameter for the characterization of a graph is the maximum node degree [20]\ndmax := max i∈V di. (3)\nWithin supervised machine learning, we assign to each datapoint zi ∈ D an output value or label xi ∈ R.1 We emphasize that the label xi of node i ∈ V can take on binary values (i.e., xi∈{0, 1}), multi-level discrete values (i.e., xi ∈{1, . . . , K}, with K being the number of classes or clusters), or continuous values in R. We can represent the entire labeling of the empirical graph conveniently by a vector x∈RN whose ith entry is the label xi of node i ∈ V . For a small subset S of datapoints zi we are provided with initial labels yi. With slight abuse of notation, we refer by S also to the subset of nodes i ∈ V representing the datapoints zi for which initial labels yi are available. We refer to the set S ⊆ V as the sampling set, where typically M := |S| ≪ N .\nIn order to learn the entire labeling x from the initial labels {yi}i∈S , we invoke the basic smoothness assumption for semi-supervised learning [7]: If two points z1, z2 are close, with respect to a given topology on the input space Z , then so should be the corresponding labels x1, x2, with respect to some distance measure on the label space R. For quantifying the smoothness of a labeling, we appeal to the discrete calculus for graph signals, which rests on the concept of a gradient for graph signals [7, Sec. 13.2]. In order to draw on discrete calculus for quantifying smoothness of a labeling, we interpret the labels xi, for i ∈ V , as the values of a graph signal, i.e., a mapping x[·] : V → R which maps node i∈V to graph signal value x[i]=xi. Using this interpretation, we measure the smoothness of the labels via the (local) gradient ∇ix at node i∈V , given as [22] (\n∇ix )\nj :=\n√\nWi,j(xj−xi). (4)\nThe norm ‖∇ix‖2= √∑\nj∈V Wi,j(xj−xi)2 provides a measure for the local variation of the graph signal x at node i∈V . The (global) smoothness of the labels xi is then quantified by the total variation [22]:\n‖x‖TV := ∑\ni∈V\n‖∇ix‖2= ∑\ni∈V\n√ ∑\nj∈V\nWi,j(xj−xi)2. (5)\nNote that the total variation (5) is a seminorm, being equal to 0 for labelings that are constant over connected graph components.\nThe basic idea of semi-supervised learning is to find a labeling x of the datapoints zi by balancing the empirical error\nErr[x] :=\n√\n(1/2|S|) ∑\ni∈S\n(xi−yi)2, (6)\nwhich represents the deviation of the learned labels xi from the initial labels yi, with the smoothness ‖x‖TV. If we fix a maximum level ε > 0 tolerated for the empirical error Err[x], we can formulate semi-supervised learning as the optimization problem\nx̂∈arg min x∈Q ‖x‖TV\nwith Q :={x ∈ RN : Err[x]≤ε}. (7) 1We highlight that the term “label” is typically reserved for discrete-valued or categorial output variables xi [3]. Since we can always represent the values of categorial output variables by real numbers, we will formulate our learning method for real-valued ouput variables xi ∈ R. Our learning method Alg. 3, which is based on using the squared error loss to quantify the empirical error, can also be used for classification by suitably quantizing the predicted ouput values. Extensions to other loss functions, more suitable to characterize the empirical error for discrete-valued or categorial labels, will be a focus of future work.\nSince the objective function in (7) is the seminorm ‖z‖TV, which is a convex function and also the constraint set Q is a convex set,2 problem (7) is a convex optimization problem. As the notation in (7) suggests, and which can be verified by simple examples, there typically exist several solutions for this optimization problem. However, the methods we consider for solving (7) in the following do not require uniqueness of the solution, i.e., they work even if there are multiple optimal labelings x̂.\nFor completeness, we also mention an alternative convex formulation of the recovery problem (7), based on using a penalty term for the total variation instead of constraining the empirical error:\nx̂ ∈ arg min x∈RN Err[x] + λ‖x‖TV. (8)\nThe regularization parameter λ>0 trades off small empirical risk Err[x̂] against small total variation ‖x̂‖TV of the learned labeling x̂.\nThe convex optimization problems (7) and (8) are related by convex duality [2], [5]: For each choice for ε there is a choice for λ (and vice-versa) such that the solutions of (7) and (8) coincide. However, the relation between ε and λ for this equivalence to hold is non-trivial and determining the corresponding λ for a given ε is as challenging as solving the problem (7) itself [1].\nFrom a practical viewpoint, an advantage of the formulations (7) is that the parameter ε may be interpreted as a noise level, which can be estimated or adjusted more easily than the parameter λ of the learning problem (8). For the rest of the paper, we will focus on the learning problem (7).\nFinally, for a dataset D whose empirical graph G is composed of several (weakly connected) components [20], the learning problem (7) decompose into independent subproblems, i.e., one learning problem of the form (7) for each of the components. Therefore, we will henceforth, without loss of generality, consider datasets whose empirical graph G is (weakly) connected."
    }, {
      "heading" : "III. OPTIMAL NONSMOOTH CONVEX OPTIMIZATION",
      "text" : "We will now briefly review a recently proposed method [19] for solving nonsmooth convex optimization problems, i.e., optimization problems with a non-differentiable objective function, such as (7). This method exploits a particular structure, which is present in the problems (7). In particular, this optimization method is based on (i) approximating a nonsmooth objective function by a smooth proxy and (ii) then applying an optimal first order (gradient based) method for minimizing this proxy.\nConsider a structured convex optimization problem of the generic form\nx̂∈arg min x∈Q1 f(x) := f̂(x)+max u∈Q2 〈u,Bx〉H2−ĝ(u) ︸ ︷︷ ︸\n:=h0(x)\n. (9)\nHere, B : H1 → H2 is a linear operator from a finite dimensional Hilbert space H1 to another finite dimensional Hilbert space H2, both defined over the real numbers. The set Q1 ⊆ H1 is required to be a closed convex set and the set Q2 ⊂ H2 is a bounded, closed convex set. The functions f̂ and ĝ in (9) are required to be continuous and convex on Q1 and Q2, respectively. Moreover, the function f̂ is assumed differentiable with gradient ∇f̂ being Lipschitz-continuous with constant L ≥ 0, i.e.,\n‖∇f̂(y)−∇f̂(x)‖H1 ≤ L‖y−x‖H1 . (10)"
    }, {
      "heading" : "Smooth Approximation of Nonsmooth Objective",
      "text" : "In order to solve the nonsmooth problem (9), we approximate the non-differentiable component h0(x) by the smooth function\nhµ(x) :=max u∈Q2\n〈u,Bx〉H2−ĝ(u)−(µ/2)‖u‖2H2 (11)\n2The seminorm ‖x‖TV is convex since it is homogeneous (‖αx‖TV= |α|‖x‖TV for α ∈ R) and satisfies the triangle inequality (‖x+y‖TV≤ ‖x‖TV+‖y‖TV). These two properties imply convexity [5, Section 3.1.5].\nwith the smoothing parameter µ > 0, yielding\nfµ(x) := f̂(x)+max u∈Q2\n〈u,Bx〉H2−ĝ(u)−(µ/2)‖u‖2H2. (12)\nThe objective function f(x) of the original problem (9) is obtained formally from (12) for the choice µ = 0, i.e., f(x) = f0(x). Since the function g(u)=‖u‖2H2 is strongly convex, the optimization problem (11) has a unique optimal point\nuµ(x)=argmax u∈Q2\n〈u,Bx〉H2−ĝ(u)−(µ/2)‖u‖2H2. (13)\nAccording to [19, Theorem 1], the function hµ(x) (cf. (11)) is differentiable with gradient\n∇hµ(x) = B∗uµ(x), which can be shown to be Lipschitz continuous with constant (1/µ)‖B‖2op. Since the gradient ∇f̂(x) of f̂(x) is assumed Lipschitz continuous with constant L (cf. (10)), the function fµ(x) (cf. (12)) has gradient\n∇fµ(x) = ∇f̂(x) +B∗uµ(x) (14) which is Lipschitz continuous with constant\nLµ := L+ (1/µ)‖B‖2op. (15) Furthermore, by evaluating [19, Eq. (2.7)], we have\nfµ(x) ≤ f0(x) = f(x) ≤ fµ(x) + (µ/2)max u∈Q2 ‖u‖2H2 , (16)\nwhich verifies that fµ(x) is a uniform smooth approximation of the objective function f(x) in (9). By replacing the objective f(x) in (9) with its smooth approximation fµ(x), we obtain the smooth optimization problem x̂µ ∈ arg min\nx∈Q1⊆H1\nfµ(x). (17)\nThe original nonsmooth problem (9) is obtained formally from the smooth approximation (17) for the particular choice µ = 0. For nonzero µ > 0, the solutions x̂ of (9) will be different from the solutions x̂µ of (17) in general. However, for sufficiently small µ any solution x̂µ of (17) will be also an approximate solution to (9). We can relate the optimal values f(x̂) and fµ(x̂µ) of the original problem (9) and its smooth approximation (17), respectively, with the help of (16). Indeed, by inserting the optimal points x̂µ and x̂ into the corresponding objective functions in (16), we obtain\nfµ(x̂µ)≤f(x̂) ≤ fµ(x̂µ)+(µ/2)max u∈Q2 ‖u‖2H2. (18)\nThus, the optimal value fµ(x̂µ) of the smoothed problem (12) provides an estimate for the optimal value f(x̂) of the original problem (9)."
    }, {
      "heading" : "Optimal Gradient Method for Smooth Minimization",
      "text" : "For solving the smooth optimization problem (17), being a proxy for the original nonsmooth problem (9), we apply an optimal first-order method [1], [19]. This method achieves the optimal worst-case rate of convergence among all gradient based methods [18], [19]. We summarize this method for solving (17) in Alg. 1, which requires as input the smoothing parameter µ > 0, an initial guess x0 and a valid Lipschitz constant L̂ for the gradient (14), i.e., satisfying L̂ ≥ Lµ = L + (1/µ)‖B‖2op. For a particular stopping criterion of Alg. 1, one can monitor the relative decrease in the objective function fµ(x̂k) [1, Sec. 3.5.]. Another option, which is used in our numerical experiments (cf. Section VI), is to run Alg. 1 for a fixed number of iterations.\nAlgorithm 1 Nesterov’s algorithm for solving (17)\nInput: smoothing parameter µ, initial guess x0, Lipschitz constant L̂ ≥ Lµ=L+(1/µ)‖B‖2op (cf. (15)) Initialize: iteration counter k :=0 1: repeat 2: gk :=∇fµ(xk)=∇f̂(x)+B∗uµ(x) with uµ(x) given by (13) 3: x̂k :=arg min\nx∈Q1\n(L̂/2)‖x−xk‖2H1+〈gk,x−xk〉H1\n4: zk :=arg min x∈Q1\n(L̂/2)‖x−x0‖2H1+ ∑k l=0 l+1 2 〈gl,x−xl〉H1\n5: xk+1 := 2\nk+3 zk+\n(\n1− 2 k+3\n)\nx̂k\n6: k :=k+1 7: until stopping criterion is satisfied\nOutput: x̂k\nThe steps 2 and 3 of Alg. 1 amount to computing the projected gradient descent step for the smooth optimization problem (17). However, what sets Alg. 1 apart from standard gradient descent methods is step 4. This step uses all previous gradient information in order to compute a projected minimizer zk of an increasingly more accurate approximation of the objective function fµ(x). The new iterate xk+1 is then obtained in step 5 as a convex combination of the projected gradient descent step x̂k and the minimizer zk of the approximation to the objective function.\nThe output x̂k of Alg. 1 satisfies [19, Theorem 2]\nfµ(x̂k)− fµ(x̂µ) ≤ 2L̂‖x̂µ − x0‖22 (k + 1)(k + 2)\n(19)\nfor any optimal point x̂µ of (17). The convergence rate predicted by (19), i.e., the error fµ(x̂k) − fµ(x̂µ) decaying proportional to 1/k2 with the iteration counter k, is optimal among all gradient-based minimization methods for the class of continuously differentiable functions with Lipschitz continuous gradient [18, Theorem 2.1.7].\nThe characterization (19) can be used to bound the number of iterations needed to run Alg. 1 such that it delivers an approximate solution x̂k ∈ Q1 for the nonsmooth problem (9) with prescribed accuracy δ, i.e., the output x̂k satisfies f(x̂k)− f(x̂) ≤ δ. Lemma III.1. Let x̂ ∈ RN and x̂µ ∈ RN be optimal points of the original problem (9) and its smoothed proxy (17), respectively. Denote D := max\nu∈Q2 ‖u‖2H2 and assume Alg. 1 is used with Lipschitz constant\nL̂=L+(1/µ)‖B‖2op. Then, the output x̂k after k iterations of Alg. 1 satisfies f(x̂k)− f(x̂) ≤ fµ(x̂k)− fµ(x̂µ) + (µ/2)D. (20)\nFor the choice µ = δ/D, Alg. 1 delivers a solution x̂k for the non-smooth problem (9) with accuracy δ, i.e.,\nf(x̂k)−f(x̂) ≤ δ for all k≥kδ (21) kδ :=(2/δ)‖x̂µ−x0‖2 √ Lδ+D‖B‖2op.\nProof: By combining (16) (for the choice x = x̂k) with (18), we have\nf(x̂k)− f(x̂) ≤ fµ(x̂k)− fµ(x̂µ) + (µ/2)D. (22)\nChoosing µ=δ/D and using (19) for the particular choice L̂=L+(1/µ)‖B‖2op we obtain f(x̂k)−f(x̂) ≤ (2/δ)‖x̂µ−x0‖22(Lδ+D‖B‖2op)(1/k2)+δ/2, (23) which implies (21).\nAccording to Lemma III.1 we need k ∝ 1/δ iterations of Alg. 1 for solving the nonsmooth optimization problem (9) with accuracy δ (cf. [19, Theorem 3]). This iteration complexity is essentially optimal for any first-order (sub-)gradient method solving problems of the form (9) [14].\nThe lower bound (21) on the iteration complexity of Alg. 1 depends on both the desired accuracy δ (which is enforced by choosing the smoothing parameter as µ = δ/D) and the choice for the inital guess via ‖x̂µ − x0‖2. As discussed in [1], an effective approach to speed up the convergence of Alg. 1 is to run it repeatedly with increasing accuracy (corresponding to decreasing values of the smoothing parameter µ) and using the output of Alg. 1 in a particular run as initial guess for the next run. Since the inital guesses used for Alg. 1 in a new run becomes more accurate, it is possible to use a smaller value for the smooting parameter µ, which effects an increased accuracy of the ouput of Alg. 1 according to (21). However, a simpler option is to adapt the smoothing parameter directly “on-the-fly” within the iterations of Alg. 1. This results in Alg. 2 being an accelerated version of Alg. 1.\nAlgorithm 2 Accelerated Nesterov for solving (17)\nInput: initial smoothing parameter µ0, decreasing factor κ, initial guess x0 Initialize: iteration counter k = 0 1: repeat 2: µ :=µ0κ k\n3: L̂ :=L+(1/µ)‖B‖2op 4: gk :=∇fµ(xk)=∇f̂(x)+B∗uµ(x) with uµ(x) given by (13) 5: x̂k :=arg min\nx∈Q1\n(L̂/2)‖x−xk‖2H1+〈gk,x−xk〉H1\n6: zk :=arg min x∈Q1\n(L̂/2)‖x−x0‖2H1+ ∑k l=0 l+1 2 〈gl,x−xl〉H1\n7: xk+1 := 2\nk+3 zk+\n(\n1− 2 k+3\n)\nx̂k\n8: k :=k+1 9: until stopping criterion is satisfied\nOutput: x̂k"
    }, {
      "heading" : "IV. EFFICIENT LEARNING OF GRAPH SIGNALS",
      "text" : "We will now show that the semi-supervised learning problem (7) can be rephrased in the generic form (9). This will then allow us to apply Alg. 1 for semi-supervised learning from big data, i.e., from highdimensional heterogeneous data, over networks. To this end, we need to introduce the graph gradient operator ∇G as a mapping from the Hilbert space RN endowed with inner product 〈a,b〉2=aTb into the Hilbert space RN×N endowed with inner product 〈A,B〉F=Tr{ABT} [13], [15]. In particular, the gradient operator ∇G maps a graph signal x ∈ RN to the matrix\n∇Gx := ( ∇1x, . . . ,∇Nx )T ∈ RN×N . (24) The ith row of the matrix ∇Gx is given by the local gradient ∇ix of the graph signal x at node i ∈ V (cf. (4)). Let us also highlight the close relation between the gradient operator ∇G : RN → RN×N and the\nnormalized graph Laplacian matrix L [8], defined element-wise as\n( L )\ni,j :=\n \n\n1 if i = j and di 6= 0, −1/ √ didj if (i, j) ∈ E ,\n0 otherwise,\n(25)\nwith di being the degree of node i ∈ V (cf. (2)). If we define the diagonal matrix D with diagonal elements di,i = di, we have for any graph signal x (cf. [22, Eq. (6)]) the identity\n‖∇Gx‖2F = xTD1/2LD1/2x. (26) We then define the divergence operator divG : RN×N → RN as the negative adjoint of the gradient operator ∇G : RN → RN×N (cf. [7, Chapter 13]) , i.e., divG :=−∇∗G . (27)\nA straightforward calculation (cf. [7, Proposition 13.4]) reveals that the operator divG maps a matrix P ∈ R N×N to the vector divG P ∈ RN with entries\n(divG P)i = ∑\nj∈V\n√ Wi,jPi,j − √ Wj,iPj,i\n(1) =\n∑\nj∈N (i)\n√ Wi,jPi,j − √ Wj,iPj,i. (28)\nWe highlight the fact that both, the gradient ∇G : RN → RN×N as well as the divergence operator divG depend on the graph structure due to the presence of the weights Wi,j in (4) and (28). Moreover, the above definitions for the gradient and divergence operator over complex networks are straightforward generalizations of the well-known gradient and divergence operator for grid graphs representing 2D-images [6].\nUsing the identity ‖∇ix‖2 = max ‖pi‖2≤1 〈pi,∇ix〉2, we can represent the total variation (5) as\n‖x‖TV = ∑\ni∈V\nmax ‖pi‖2≤1 〈pi,∇ix〉2 = max P∈P 〈P,∇Gx〉F (29)\nwith the closed convex set\nP := {P = (p1, . . . ,pN)T ∈ RN×N : (30) ‖pi‖2 ≤ 1 for every i = 1, . . . , N}.\nUsing (29), the learning problem (7) can be written as\nmin x∈Q f0(x) with f0(x) := max P∈P\n〈P,∇Gx〉F (31)\nwith constraint set Q={x ∈ RN : Err[x] ≤ ε} (cf. (6) and (7)). The optimization problem (31) is exactly of the form (9) with the linear operator B = ∇G , the functions f̂(x) ≡ 0 and ĝ(u) ≡ 0, and the sets Q1 = Q and Q2 = P . The smoothed version (cf. (12)) of the problem (7) is then obtained as\nmin x∈Q fµ(x) with (32)\nfµ(x) := max P∈P\n( 〈P,∇Gx〉F−(µ/2)‖P‖2F ) .\nIn order to apply Alg. 1 to the smoothed version (32) of the learning problem (7), we have to determine the gradient ∇fµ(x) and a corresponding valid Lipschitz constant L̂ ≥ Lµ (cf. (15)). The gradient ∇fµ(x)\nis obtained by specializing (14) for the objective in (32), yielding\n∇fµ(x) = − divG Pµ(x) (33) with\nPµ(x) = argmax P∈P\n( 〈P,∇Gx〉F − (µ/2)‖P‖2F ) . (34)\nBy the KKT conditions for constrained convex optimization problems [5], [15],\nPµ(x) = (q1, . . . ,qN) T (35)\nwith qi = 1\nmax{µ, ‖∇ix‖2} ∇ix\nA particular Lipschitz constant for the gradient ∇fµ(x) is obtained, by specializing (15) to B = ∇G , as\nLµ = (1/µ)‖∇G‖2op (27) = (1/µ)‖ divG ‖2op. (36)\nHowever, since evaluating the exact operator norm of the gradient (or divergence) operator is difficult for an arbitrary large-scale graph,3 we will rely on a simple upper bound.\nLemma IV.1. Let G = (V, E ,W) be a weighted undirected graph and let ∇G denote the corresponding gradient operator (24). The norm of the gradient operator satisfies\n‖∇G‖op ≤ √ 2dmax (37)\nwith the maximum node degree dmax (cf. (3)).\nProof: Due to (26), we have ‖∇G‖2op = ‖D1/2LD1/2‖2, (38)\nwhich, since obviously ‖D‖2 ≤ dmax, implies ‖∇G‖2op ≤ dmax‖L‖2. (39)\nThe bound (37) follows from the well-known upper bound ‖L‖2 ≤ 2 for the maximum eigenvalue (which is equal to the spectral norm) of the normalized Laplacian matrix L (cf. [8, Lemma 1.7])\nAccording to Lemma IV.1, the gradient ∇fµ(x) (cf. (33)) of fµ(x) is Lipschitz with constant L̂=2dmax/µ, (40)\nwhich we can use as input to Alg. 1. In order to apply Alg. 1 to the smoothed learning problem (32), we now present closed-form expressions for the updates in step 3 and 4 of Alg. 1 for Q1 = Q (cf. (7)). Lemma IV.2. Consider the convex set Q = {x : Err[x] ≤ ε} and let y denote any labeling which is consistent with the initial labels yi, i.e., ( y )\ni = yi for all i ∈ S. Then, using the shorthand D(S) :=∑\ni∈S eie T i , the solution x̂k of the optimization problem\nx̂k=arg min x∈Q\n(L̂/2)‖x−xk‖22+gTk (x−xk) (41)\n3In many big data applications it is not possible to have a complete description of the graph, e.g. in form of an edge list, available. Instead, one typically has only knowledge about some basic parameters, e.g., the maximum node degree dmax (cf. (3)).\nis given by\nx̂k= ( I+λεD(S) )−1 (q+λεD(S)y) (42) = (I−D(S))q+ { D(S) [ y+(ε/r)(q−y) ] if r>ε\nD(S)q otherwise with\nq :=xk−(1/L̂)gk, r :=Err[q−y] (43) , and λε :=max{0, (r/ε)−1}.\nIn a similar manner, the solution zk of the optimization problem\nzk=arg min x∈Q\n(L̂/2)‖x−x0‖22+(1/2) k∑\nl=0\n(l+1)gTl (x−xl) (44)\nis given by\nzk= ( I+ λ̃εD(S) )−1 (q̃+λ̃εD(S)y) (45) =(I−D(S))q̃+ { D(S) [ y+(ε/r)(q̃−y) ] if r>ε\nD(S)q̃ otherwise with\nq̃ :=x0−(1/2L̂) k∑\nl=0\n(l+1)gl, r̃ :=Err[q̃−y] (46)\n, and λ̃ε = max{0, (r̃/ε)−1}. Proof: see Appendix A.\nThe closed-form expressions (42) and (45) are suitable modifications of those presented in [1, Sec. 3] to our setting of semi-supervised learning over complex networks. We are now in the position to specialize Alg. 1 to the smoothed learning problem (32) by using the closed-form expressions (42) and (45) for step 3 and 4 of Alg. 1. This results in Alg. 3 for semi-supervised learning from big data over networks.\nThe steps 2 and 3 of Alg. 3 amount to computing the gradient gk = ∇fµ(xk) of the objective function fµ(x) in the learning problem (32). The steps 4-6 of Alg. 3 implement a projected gradient descent step, while steps 7-9 amount to computing the minimizer zk of the approximation in step 4 of Alg. 1.\nCombining (19) with (40), yields the following characterization of the convergence rate of Alg. 3:\nfµ(x̂k)− fµ(x̂µ) ≤ 4dmax‖x̂µ − x0‖22 µ(k + 1)(k + 2)\n(47)\nfor any solution x̂µ of (32). The bound (47) suggests that the convergence is faster for graphs which are more sparse, i.e., have a smaller maximum node degree dmax. As for Alg. 1, the convergence speed of Alg. 3 depends on the accuracy of the inital guess as well as on the smoothing parameter µ. The accelerated version of Alg. 3 is then obtained from Alg. 2, yielding Alg. 4."
    }, {
      "heading" : "V. MESSAGE PASSING FORMULATIONS",
      "text" : "In order to make semi-supervised learning via the optimization problem (32) feasible for massive (internetscale) datasets, we will now discuss a message passing formulation of Alg. 3, which is summarized as Alg. 5 (being an adaption of [13, Alg. 2] to undirected graphs): The steps 2-6 of Alg. 5 amount to computing the (scaled) gradient ∇fµ(x) (cf. (33)) of the objective function fµ(x) for the problem (32) in a distributed manner. The quantities Pi,j are the entries of the matrix Pµ(x) (cf. (34)). The steps 11-15 and 17-21 of\nAlgorithm 3 Semi-Supervised Learning via Nesterov’s Method\nInput: dataset D with empirical graph G, subset S = {i1, . . . , iM} of datapoints with initial labels {yj}j∈S , error level ε, smoothing parameter µ, initial guess for the labeling x0 ∈ RN\nInitialize: k :=0, Lipschitz constant L̂ :=(2/µ)dmax, q̃0 :=x0, α :=1/2 1: repeat 2: ∀i ∈ V : pi := 1max{µ,‖∇ixk‖2}∇ixk 3: gk :=− divG(P) with P=(p1, . . . ,pN )T 4: qk :=xk−(1/L̂)gk 5: r :=Err[qk] (cf. (6))\n6: ∀i ∈ V : x̂k,i := {\nyi + (ε/r)(qk,i − yi) if i ∈ S and r > ε qk,i otherwise\n7: q̃k := q̃k−(α/L̂)gk 8: r̃ := Err[q̃k] 9: ∀i ∈ V : zk,i := {\nyi+(ε/r̃)(q̃k,i−yi) if i ∈ S and r̃ > ε q̃k,i otherwise\n10: xk+1 := 2 k+3 zk+(1− 2k+3)x̂k\n11: k := k+1 12: α := α + 1/2 13: until stopping criterion is satisfied Output: learned labeling x̂k for all datapoints\nAlgorithm 4 Semi-Supervised Learning via Accelerated Nesterov\nInput: dataset D with empirical graph G, subset S = {i1, . . . , iM} of datapoints with initial labels {yj}j∈S , error level ε, initial smoothing parameter µ0, decreasing factor κ, initial guess for the labeling x0 ∈ RN\nInitialize: k :=0, q̃0 :=x0, α :=1/2 1: repeat 2: µ :=µ0κ k\n3: L̂ :=(2/µ)dmax 4: ∀i ∈ V : pi := 1max{µ,‖∇ixk‖2}∇ixk 5: gk :=− divG(P) with P=(p1, . . . ,pN )T 6: qk :=xk−(1/L̂)gk 7: r :=Err[qk] (cf. (6)) 8: ∀i ∈ V : x̂k,i := {\nyi + (ε/r)(qk,i − yi) if i ∈ S and r > ε qk,i otherwise\n9: q̃k := q̃k−(α/L̂)gk 10: r̃ := Err[q̃k] 11: ∀i ∈ V : zk,i := {\nyi+(ε/r̃)(q̃k,i−yi) if i ∈ S and r̃ > ε q̃k,i otherwise\n12: xk+1 := 2 k+3 zk+(1− 2k+3)x̂k\n13: k := k+1 14: α := α + 1/2 15: until stopping criterion is satisfied Output: learned labeling x̂k for all datapoints\nAlg. 5 implement a finite number K of iterations of the average consensus algorithm, using MetropolisHastings weights [26] , for (approximately) computing the sums (1/N) ∑\nj∈V bj = (1/N) ∑\nj∈S(yj − qj)2 and (1/N) ∑\nj∈V b̃j = (1/N) ∑\nj∈S(yj − q̃j)2, respectively. In particular, for sufficiently large K, the results ri and r̃i in step 16 and 22 of Alg. 5 satisfy ri ≈ r and r̃i ≈ r̃ for every node i ∈ V (cf. step 6,7 of Alg. 3). The choice for the number K of avarage consensus iterations can be guided by a wide range of results characterzing the convergence rate of average conensus [4], [10], [26]. As a rule of thumb, K should be significantly larger than the diameter d(G) of the underlying graph G [10, Thm. 4.3.]. We highlight that Alg. 5 requires each node i ∈ V to have access to local information only. In particular, to implement Alg. 5 on a given node i ∈ V , the measurement yj , the value xj , the matrix entries Pi,j and the edge weights Wi,j are required only for node i itself and its neighborhood N (i).\nWe implemented Alg. 5 using the big data framework AKKA [25], which is a toolkit for building distributed and resilient message-driven applications. The AKKA implementation was run on a computing cluster composed of nine virtual machines (one master and eight slave workers) obtained via the cloud computing service Amazon EC2. Each virtual machine has been configured with a 64-bit CPU, 3.75 GB of main memory, and 8 GB of local disk storage. In Figure 1 we sketch the basic architecture of the AKKA implementation of our graph learning methods. First, we partition the graph in a simple uniform manner, i.e., node i ∈ {1, . . . , N} is assigend to partition (i mod 8)+1. After partitioning G, the master machine assigns the obtained partitions to the eight workers and manages the execution of the message passing algorithm between the workers. There are two alternating phases in the execution of the AKKA implementation: the master phase, where the states of the worker machines are synchronized and the worker phase. Two types of operations are executed in the worker phase:\n• intra-block operations: each worker performs local computations within its associated partition, and • inter-block operations: workers exchange messages across their partitions.\nWe then compared the runtime of the AKKA implementation to the centralized implementation in MATLAB used in [13]. The results indicate a runtime reduction by almost a factor 10 which is reasonable since we are using a cluster of nine machines."
    }, {
      "heading" : "VI. NUMERICAL EXPERIMENTS",
      "text" : "We assess the performance of (the accelerated version of) the proposed learning algorithm Alg. 4 empirically by applying it to an synthetic dataset with empirical graph G = (V, E ,W), depicted in Fig. 2, whose nodes are made up of 2 disjoint clusters Cc of same size |Cc|=100 giving a total graph size of\nAlgorithm 5 Distributed Semi-Supervised Learning via Nesterov’s Method\nInput: sampling set S = {i1, . . . , iM}, samples {yj}j∈S , error level ε, edge weights {Wi,j}i,j∈V , smoothing parameter µ, initial guess {x0,i}i∈V , number K of average consensus iterations\nInitialize: L̂ :=(2/µ)dmax, ∀i ∈ V : xi=x0,i, α :=1/2, τ :=2/3, {ui,j :=1/(max{di, dj}+1)}j∈N (i) 1: repeat 2: ∀i ∈ V : broadcast xi to neighbors N (i)/ collect {xj}j∈N (i) from neighbors N (i) 3: ∀i ∈ V : update γi = √∑\nj∈N (i)(xj − xi)2Wi,j 4: ∀i ∈ V : for neighbor j ∈ N (i) update Pi,j = 1max{µ,γi}(xj − xi) √ Wi,j 5: ∀i ∈ V : broadcast Pi,j to neighbors N (i)/ collect {Pj,i}j∈N (i) from neighbors N (i) 6: ∀i ∈ V : update gi = (1/L̂) (∑\nj∈N (i)\n√ Wj,iPj,i − ∑\nj∈N (i)\n√ Wi,jPi,j )\n7: ∀i ∈ V : update qi = xi − gi 8: ∀i ∈ V : update gi = gi − αgi 9: ∀i ∈ V : update α = α + 1/2\n10: ∀i ∈ V : update q̃i = x0,i−gi 11: ∀i ∈ V : update bi = {\n(yi − qi)2 for i ∈ S 0 else\n12: for l = 1:K do 13: ∀i ∈ V : broadcast bi to neighbors N (i)/ collect {bj}j∈N (i) from N (i) 14: ∀i ∈ V : update bi = ( 1−∑j∈N (i) ui,j ) bi + ∑\nj∈N (i) ui,jbj 15: end for 16: ∀i ∈ V : update ri = √ Nbi 17: ∀i ∈ V : update b̃i = {\n(yi − q̃i)2 for i ∈ S 0 else\n18: for l = 1:K do 19: ∀i ∈ V : broadcast b̃i to neighbors N (i)/collect {b̃j}j∈N (i) from N (i) 20: ∀i ∈ V : update b̃i = ( 1−∑j∈N (i) ui,j ) b̃i + ∑\nj∈N (i) ui,j b̃j 21: end for 22: ∀i ∈ V : update r̃i = √ Nb̃i 23: ∀i ∈ V : update x̂i = {\nyi + (ε/r)(qi − yi) if i ∈ S and r > ε qi otherwise\n24: ∀i ∈ V : update zi = {\nyi + (ε/r̃)(q̃i − yi) if i ∈ S and r̃ > ε q̃i otherwise\n25: ∀i ∈ V : update xi = τzi + (1− τ)x̂i 26: ∀i ∈ V : update τ = ( (1/τ) + (1/2) )−1\n27: until stopping criterion is satisfied Output: x̂i\nN=2 ·100 nodes. The clusters are connected through few “gate” nodes. The maximum node degree of G is dmax=8. Given the empirical graph G, we generated a labeling x(g) by labeling the nodes for each cluster Cc by a random number tc ∼ N (0, 1), i.e., x(g)i = tc for all nodes i ∈ Cc in the cluster Cc. For each cluster Cc we assume that we are provided initial labels yi = xi for 100 randomly choosen nodes i ∈ Cc, giving rise to an overall sampling set S with M=2 · 100 nodes. We run Alg. 4 with initial smoothing parameter µ=1, decreasing factor κ=(2 · 10−5)1/2000, error bound ε :=‖x(g)‖2/105 and a fixed number of 2000 iterations, to obtain the learned labels x̂i for every node i ∈ V . In Fig. 3, we show the learned labeling x̂i output by Alg. 4. We also show the learned labeling x̂LPi obtained using the well-known label progagation (LP)\n.\nalgorithm [7, Alg. 11.1] which is run for the same number of iterations. From Fig. 3, it is evident that Alg.\n.\n4 yields better learning accuracy compared to plain LP, which is also reflected in the empirical normalized MSEs NMSEnest ≈ 2.1× 10−4 and NMSELP ≈ 2.4× 10−3 obtained by averaging ‖x̂−x(g)‖22/‖x(g)‖22 and ‖x̂LP−x(g)‖22/‖x(g)‖22 over 100 independent Monte Carlo runs. We have also depicted the dependence of the NMSE of Alg. 4 and LP on the iteration number k in Fig. 4, which shows that after some inital phase, which comprises ≈ 100 iterations, the NMSE obtained by Alg. 4 converges quickly to its stationary value.\nRemarkably, According to Fig. 4, the simple LP method provides smaller NMSE for the first few iterations. However, the comparison of the convergence speed of Alg. 4 and LP should be interpreted carefully, since the optimization problem underlying LP is based on the smooth Laplacian quadratic form [7, Sec. 11.3.], whereas Alg. 4 amounts to solving the non-smooth problem (7).\n."
    }, {
      "heading" : "VII. CONCLUSIONS",
      "text" : "The problem of semi-supervised learning from massive datastes over networks has been formulated as a nonsmooth convex optimization problem based on penalizing the total variation of the labeling. We applied the smoothing technique of Nesterov to this optimization problem for obtaining an efficient learning algorithm which can capitalize on huge amounts of unlabeled data using only few labeled datapoints. Moreover, we proposed an implementation of the learning method as message passing over the underlying data graph. This message passing algorithm can be easily implemented in a big data platform such as AKKA to allow for scalable learning algorithms. Future work includes the extension of the optimization framework to accomodate loss functions, different from the mean squared error, that better characterize the training error for discrete valued or categorial labels."
    }, {
      "heading" : "APPENDIX A PROOF OF LEMMA IV.2",
      "text" : "We only detail the derivation of (42), since the derviation of (45) is very similar. Our argument closely follows the derivations used in [1, Sec. 3]. Consider the constrained convex optimization problem (41), which we repeat here for convenience:\nx̂k = arg min x∈Q\n(L̂/2)‖x− xk‖22+gTk (x−xk) (48)\nwith constraint set Q := {x : Err[x] ≤ ε} = {x : ( Err[x] )2 ≤ ε2}. The Lagrangian associated with (48) is\nL(x, λ)=(L̂/2)‖x−xk‖22+gTk (x−xk) +λ( ( Err[x] )2−ε2), (49)\nand the corresponding KKT conditions for x̂k and λε to be primal and dual optimal read [5, Section 5.5.3]\nL̂(x̂k−xk)+gk+(λǫ/|S|)D(S)(x̂k−y)=0, (50) λε(Err[x̂k]− ε) = 0, (51)\nErr[x̂k] ≤ ε, (52) λε ≥ 0, (53)\nwith the diagonal matrix D(S) = ∑i∈S eieTi . From condition (50), we obtain\nx̂k= ( I+λ̃εD(S) )−1 (xk−(1/L̂)gk+λ̃εD(S)y). (54)\nwith λ̃ε := λǫ\nL̂|S| (55)\nUsing the elementary identity (I+ aD(S))−1 = I− a\n1 + a D(S) (56)\nwhich is valid for any a ≥ 0, we can develop (54) further to\nx̂k = ( I− λ̃ε 1 + λ̃ε D(S) ) (xk − (1/L̂)gk + λ̃εD(S)y). (57)\nInserting (57) into (52) yields\nErr[x̂k] (57) = Err[ ( I− λ̃ε\n1 + λ̃ε D(S)\n) (xk − (1/L̂)gk\n+ λ̃εD(S)y)] (6) = 1\n(1 + λ̃ε)2 Err[xk − (1/L̂)gk − y)]\n(52) ≤ ε. (58)\nFrom (58), we have λ̃ε ≥ (1/ε)Err[xk−(1/L̂)gk−y)]−1. (59)\nThus if (1/ε)Err[xk−(1/L̂)gk−y)]>1, then (59) implies λε > 0, which, via (51), requires the inequality (58) to become an equality, i.e.,\nλ̃ε=(1/ε)Err[xk−(1/L̂)gk−y)]−1 (60) This equality holds also if Err[xk−(1/L̂)gk−y)]/ε = 1. For Err[xk−(1/L̂)gk−y)]/ε < 1, complementary slackness (51) requires λ̃ε = 0. Thus the optimal dual variable λ̃ε is fully determined by the quantity Err[xk − (1/L̂)gk − y)] via\nλ̃ε = max{0, (1/ε)Err[xk − (1/L̂)gk − y)]−1}. (61)"
    } ],
    "references" : [ {
      "title" : "NESTA: a fast and accurate first-order method for sparse recovery",
      "author" : [ "S. Becker", "J. Bobin", "E.J. Candès" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "C.M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Fastest mixing markov chain on a graph",
      "author" : [ "S. Boyd", "P. Diaconis", "L. Xiao" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2004
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "An algorithm for total variation minimization and applications",
      "author" : [ "A. Chambolle" ],
      "venue" : "Journal of Mathematical imaging and vision,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Semi-Supervised Learning",
      "author" : [ "O. Chapelle", "B. Schölkopf", "A. Zien", "editors" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Big Data over Networks",
      "author" : [ "S. Cui", "A. Hero", "Z.-Q. Luo", "J. Moura", "editors" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "What do we know about the metropolis algorithm",
      "author" : [ "P. Diaconis" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1998
    }, {
      "title" : "High-dimensional data analysis: The curses and blessings of dimensionality",
      "author" : [ "D.L. Donoho" ],
      "venue" : "In Amer. Math. Soc. Lecture:“Math challenges of the 21st century”,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2000
    }, {
      "title" : "Community detection",
      "author" : [ "S. Fortunato" ],
      "venue" : "in graphs. arXiv,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Efficient graph signal recovery over big networks",
      "author" : [ "G. Hannak", "P. Berger", "G. Matz", "A. Jung" ],
      "venue" : "In Proc. Asilomar Conf. Signals, Sstems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "First-order methods for nonsmooth convex large-scale optimization, I: General purpose methods",
      "author" : [ "A. Juditsky", "A. Nemirovski" ],
      "venue" : "Optimization for Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Scalable graph signal recovery for big data over networks",
      "author" : [ "A. Jung", "P. Berger", "G. Hannak", "G. Matz" ],
      "venue" : "In 2016 IEEE 17th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Big data: The next frontier for innovation, competition, and productivity",
      "author" : [ "J. Mayika", "B. Brown", "J. Bughin", "R. Dobbs", "C. Roxburgh", "A.H. Byers" ],
      "venue" : "McKinsey Global Institute,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Distributed decorrelation in sensor networks with application to distributed particle filtering",
      "author" : [ "M. Moldaschl", "W.N. Gansterer", "O. Hlinka", "F. Meyer", "F. Hlawatsch" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Introductory lectures on convex optimization, volume 87 of Applied Optimization",
      "author" : [ "Y. Nesterov" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Math. Program.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Networks: An Introduction",
      "author" : [ "M. Newman" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Innovations diffusion: A spatial sampling scheme for distributed estimation and detection",
      "author" : [ "Z. Quan", "W.J. Kaiser", "A.H. Sayed" ],
      "venue" : "IEEE Trans. Signal Processing,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
      "author" : [ "D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Hadoop: The Definitive Guide",
      "author" : [ "T. White" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "Distributed covariance estimation in Gaussian graphical models",
      "author" : [ "A. Wiesel", "A.O. Hero" ],
      "venue" : "IEEE Trans. Signal Processing,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Akka Concurrency",
      "author" : [ "D. Wyatt" ],
      "venue" : "Artima Incorporation,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Distributed average consensus with least-mean-square deviation",
      "author" : [ "L. Xiao", "S. Boyd", "S.-J. Kim" ],
      "venue" : "Journal of Parallel and Distributed Computing,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : ", “Big Data” [9], [11], [16], [23].",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : ", “Big Data” [9], [11], [16], [23].",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : ", “Big Data” [9], [11], [16], [23].",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 20,
      "context" : ", “Big Data” [9], [11], [16], [23].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : ", there are statistical variations due to missing labels, labeling errors, or poor data curation [23].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 14,
      "context" : "Quite often, these two notions of graph structure coincide: in a wireless sensor network, the graph modeling the communication links between nodes and the graph formed by statistical dependencies between sensor measurements resemble each other since both graphs are induced by the nodes mutual proximity [17], [21], [24].",
      "startOffset" : 304,
      "endOffset" : 308
    }, {
      "referenceID" : 18,
      "context" : "Quite often, these two notions of graph structure coincide: in a wireless sensor network, the graph modeling the communication links between nodes and the graph formed by statistical dependencies between sensor measurements resemble each other since both graphs are induced by the nodes mutual proximity [17], [21], [24].",
      "startOffset" : 310,
      "endOffset" : 314
    }, {
      "referenceID" : 21,
      "context" : "Quite often, these two notions of graph structure coincide: in a wireless sensor network, the graph modeling the communication links between nodes and the graph formed by statistical dependencies between sensor measurements resemble each other since both graphs are induced by the nodes mutual proximity [17], [21], [24].",
      "startOffset" : 316,
      "endOffset" : 320
    }, {
      "referenceID" : 5,
      "context" : "analogous to making the smoothness assumption of semi-supervised learning [7]: signals that are connected by an edge in the graph have similar labels.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "In other words, the graph signal is expected to reflect the underlying graph structure in the sense that the labels of signals on closely connected nodes have high mutual correlation and thus these signals form close-knit clusters or communities [12].",
      "startOffset" : 246,
      "endOffset" : 250
    }, {
      "referenceID" : 5,
      "context" : "This is the basis for many well-known label propagation methods [7].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 19,
      "context" : "In contrast, the approach proposed in this paper is based on using (graph) total variation [22], which provides a more natural match between smoothness and the community structure of the data, i.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "An key parameter for the characterization of a graph is the maximum node degree [20]",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "In order to learn the entire labeling x from the initial labels {yi}i∈S , we invoke the basic smoothness assumption for semi-supervised learning [7]: If two points z1, z2 are close, with respect to a given topology on the input space Z , then so should be the corresponding labels x1, x2, with respect to some distance measure on the label space R.",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 19,
      "context" : "Using this interpretation, we measure the smoothness of the labels via the (local) gradient ∇ix at node i∈V , given as [22] ( ∇ix )",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 19,
      "context" : "The (global) smoothness of the labels xi is then quantified by the total variation [22]: ‖x‖TV := ∑",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "We highlight that the term “label” is typically reserved for discrete-valued or categorial output variables xi [3].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : "The convex optimization problems (7) and (8) are related by convex duality [2], [5]: For each choice for ε there is a choice for λ (and vice-versa) such that the solutions of (7) and (8) coincide.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "However, the relation between ε and λ for this equivalence to hold is non-trivial and determining the corresponding λ for a given ε is as challenging as solving the problem (7) itself [1].",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 17,
      "context" : "Finally, for a dataset D whose empirical graph G is composed of several (weakly connected) components [20], the learning problem (7) decompose into independent subproblems, i.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "OPTIMAL NONSMOOTH CONVEX OPTIMIZATION We will now briefly review a recently proposed method [19] for solving nonsmooth convex optimization problems, i.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "Optimal Gradient Method for Smooth Minimization For solving the smooth optimization problem (17), being a proxy for the original nonsmooth problem (9), we apply an optimal first-order method [1], [19].",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 16,
      "context" : "Optimal Gradient Method for Smooth Minimization For solving the smooth optimization problem (17), being a proxy for the original nonsmooth problem (9), we apply an optimal first-order method [1], [19].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 15,
      "context" : "This method achieves the optimal worst-case rate of convergence among all gradient based methods [18], [19].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "This method achieves the optimal worst-case rate of convergence among all gradient based methods [18], [19].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "This iteration complexity is essentially optimal for any first-order (sub-)gradient method solving problems of the form (9) [14].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "As discussed in [1], an effective approach to speed up the convergence of Alg.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "To this end, we need to introduce the graph gradient operator ∇G as a mapping from the Hilbert space R endowed with inner product 〈a,b〉2=ab into the Hilbert space R endowed with inner product 〈A,B〉F=Tr{AB} [13], [15].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 12,
      "context" : "To this end, we need to introduce the graph gradient operator ∇G as a mapping from the Hilbert space R endowed with inner product 〈a,b〉2=ab into the Hilbert space R endowed with inner product 〈A,B〉F=Tr{AB} [13], [15].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 4,
      "context" : "Moreover, the above definitions for the gradient and divergence operator over complex networks are straightforward generalizations of the well-known gradient and divergence operator for grid graphs representing 2D-images [6].",
      "startOffset" : 221,
      "endOffset" : 224
    }, {
      "referenceID" : 3,
      "context" : "By the KKT conditions for constrained convex optimization problems [5], [15],",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "By the KKT conditions for constrained convex optimization problems [5], [15],",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 23,
      "context" : "5 implement a finite number K of iterations of the average consensus algorithm, using MetropolisHastings weights [26] , for (approximately) computing the sums (1/N) ∑ j∈V bj = (1/N) ∑ j∈S(yj − qj) and (1/N) ∑ j∈V b̃j = (1/N) ∑ j∈S(yj − q̃j), respectively.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "The choice for the number K of avarage consensus iterations can be guided by a wide range of results characterzing the convergence rate of average conensus [4], [10], [26].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : "The choice for the number K of avarage consensus iterations can be guided by a wide range of results characterzing the convergence rate of average conensus [4], [10], [26].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 23,
      "context" : "The choice for the number K of avarage consensus iterations can be guided by a wide range of results characterzing the convergence rate of average conensus [4], [10], [26].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 22,
      "context" : "5 using the big data framework AKKA [25], which is a toolkit for building distributed and resilient message-driven applications.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : "We then compared the runtime of the AKKA implementation to the centralized implementation in MATLAB used in [13].",
      "startOffset" : 108,
      "endOffset" : 112
    } ],
    "year" : 2016,
    "abstractText" : "We propose a scalable method for semi-supervised (transductive) learning from massive network-structured datasets. Our approach to semi-supervised learning is based on representing the underlying hypothesis as a graph signal with small total variation. Requiring a small total variation of the graph signal representing the underlying hypothesis corresponds to the central smoothness assumption that forms the basis for semi-supervised learning, i.e., input points forming clusters have similar output values or labels. We formulate the learning problem as a nonsmooth convex optimization problem which we solve by appealing to Nesterov’s optimal first-order method for nonsmooth optimization. We also provide a message passing formulation of the learning method which allows for a highly scalable implementation in big data frameworks.",
    "creator" : "LaTeX with hyperref package"
  }
}