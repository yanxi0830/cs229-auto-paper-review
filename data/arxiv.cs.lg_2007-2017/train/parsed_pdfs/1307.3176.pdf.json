{
  "name" : "1307.3176.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online gradient descent for least squares regression: Non-asymptotic bounds and application to bandits",
    "authors" : [ "Nathaniel Korda" ],
    "emails" : [ "nathaniel.korda@inria.fr", "prashanth.la@inria.fr", "remi.munos@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 7.\n31 76\nv1 [\ncs .L\nG ]\n1 1\nJu l 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Often in learning algorithms an unknown parameter must be estimated from data arriving sequentially in pairs, (xn, yn). We consider settings where the points xn are chosen by a higher level algorithm and the outputs yn satisfy the dynamics yn = xTnθ\n∗ + ξn, where ξn is i.i.d., zero-mean noise, and θ∗ is the unknown parameter. Typically, in such cases a least squares regression estimate is used for θ∗, and finding this estimate is often the most computationally intensive part of the higher level algorithm. We provide two Stochastic Gradient Descent (SGD) algorithms that achieve a significant saving in complexity in big data settings where either the dimension of the data, d, is very large, or both d and the number of samples, n, is large. Furthermore we provide an application of both of these algorithms to significantly reduce complexity in linear bandit problems.\nThe solution to the least squares regression problem is the minimizer of the least square error, i.e.,\nθ̂n = argmin θ\n1\n2\nn∑\ni=1\n(yi − θTxi)2.\nIt is well-known that θ̂n = Ā−1n b̄n, where\nĀn = 1\nn\nn−1∑\ni=1\nxix T i and b̄n = 1\nn\nn−1∑\ni=1\nxiyi.\n1 Õ(·) is the usual O(.) notation with the log factors hidden.\nAssuming that the features xi evolve in compact subset D of Rd, the complexity of solving one regression problem with the above approach is O(d2), where the inverse of Ān is computed iteratively using the Sherman-Morrison lemma (using the Strassen algorithm or the Coppersmith-Winograd algorithm gives a complexity of O(d2.807) and O(d2.375) respectively).\nAn alternative to this approach is to use an online SGD scheme to approximate θ̂n. The advantage of such an approach is that the costly inversion of the Ān matrix is replaced by an iterative scheme. We randomise the samples in order to apply online SGD schemes and this reduces the complexity of each iteration to O(d). Furthermore, to cope with situations where strong convexity of the Ān matrix cannot be guaranteed by the higher level algorithm, we propose an adaptive regularisation. For both schemes we provide error bounds both in expectation and in high probability. Such bounds are essential for giving theoretical guarantees when using a randomised SGD scheme to replace the matrix inversion approach to the regression problem in a higher level learning algorithm.\nUnlike the traditional SGD setting where the pairs (xn, yn) are samples drawn from some unknown joint probability distribution, we assume that the samples, xn, are chosen by a higher level learning algorithm and the problem is to find a good enough approximation to θ∗ for its purposes. This poses a new difficulty for the analysis of such SGD schemes, and we propose here two solutions to this problem. First we show that under a strong convexity assumption on the matrices Ān, the analysis can be achieved by adapting methods from the literature. In this case we bound directly the error in approximating θ∗.\nWhen no strong convexity can be guaranteed we choose to regularise the regression problem. However since our data is growing with time we introduce a regularisation, λn, that adapts to the sample size n as follows:\nθ̃n := argmin θ\n1\n2n\nn∑\ni=1\n(yi − θTxi)2 + λn ‖θ‖22 .\nOur second iteration scheme tracks the regression solutions, θ̃n, and we bound the error in approximating these solutions. This introduces a drift error into the overall error of the iterative scheme that reflects the fact that the regularisation λn changes at each time step.\nAs examples of a higher level learning algorithm using regression as a subroutine, we consider two linear bandit algorithms, the PEGE algorithm of [7] and the ConfidenceBall algorithm of [2]. In a linear bandit problem the values xn represent actions taken by an agent and the values yn = xTnθ\n∗ + ξn are interpreted as losses with unknown parameter θ∗. At each time the agent can choose to take any action x ∈ D, where D is some compact subset of Rd, and the agent’s goal is to maximise the expected sum of losses. This goal would be achieved by choosing xn = x∗ := argminx{xTθ∗}, ∀n. However, since one does not know the value of θ∗ one needs to estimate it, and a tradeoff appears between sampling pairs (xn, yn) that will improve the estimate of θ∗, and gaining the best short term losses possible by exploiting the current information available about θ∗. Typically the performance of a bandit algorithm is measured by its expected cumulative regret: Rn = ∑n i=1(x\n∗ − xi)Tθ∗. The PEGE algorithm of [7] is designed for action sets D satisfying a strong convexity property (see assumption (A4) in Section 3). The algorithm splits time into exploration phases and exploitation phases. During the exploitation phases the algorithm acts greedily using least squares estimates of θ∗ calculated from data gathered during the exploration phases. During the exploration phases data is gathered in such a way that Ān matrices are always strongly convex. The regret performance of this algorithm is O(dn1/2), and we find that if we replace the least squares estimates with SGD iterates with no regularisation, we achieve an improvement of order O(d) in complexity, while suffering a loss of only log factors in the regret performance.\nThe ConfidenceBall algorithm requires only that D be compact, i.e., the action sets are not necessarily strongly convex. It continually updates a least squares estimate of θ∗, and a confidence region, Bn, around\nthat estimate based on all the past data. It then chooses xn = argmaxxmaxθ∈Bn{xTθ∗}, and its regret performance is Õ(dn1/2). Since strong convexity of the matrices Ān cannot be guaranteed for this algorithm, we incorporate adaptive regularisation based SGD scheme and the resulting drift error due to the timevarying regularisation parameter λn induces a loss in the regret performance. Using our algorithm we are able to achieve an O(d) improvement in complexity with an order O(n1/5) deterioration in the regret."
    }, {
      "heading" : "Main Contributions",
      "text" : "• We propose two online algorithms - both employing randomisation of samples, one without regularization and another with adaptive regularization. These algorithms track the corresponding least squares solution θ̂n and can be viewed as SGD schemes. The former converges at the rate of O ( n−1/2 ) .\nThe rate of convergence of the latter depends on the rate of convergence of the regularised least squares solution, which in turn is problem dependent. Both incur a computational cost of only O(d) per iteration.\n• We demonstrate the usefulness of our algorithms by applying them to the linear bandit setting, using them instead of solving the least squares regression steps. For the PEGE algorithm of [7] our result achieves an O(d) improvement in complexity, while incurring only a logarithmic deterioration in the regret. For the ConfidenceBall algorithm of [2] we again achieve a complexity improvement of O(d), while incurring an O(n1/5) deterioration in the regret.\n• We provide finite time analysis of all our schemes including bounds both in high probability as well as in expectation. In particular, we observe that under a step-size choice of O(n−1), the non regularised algorithm is within O ( n1/2 ) of the least squares solution θ̂n under a strong convexity assumption on\nĀn. By contrast, using larger step sizes O(n−α), α ∈ (1/2, 1), the regularised algorithm exhibits slightly worse bounds in expectation, and bounds in high probability dependent on the rate of convergence of the regularised least squares estimates, but does not require the strong convexity assumption.\nRelated work Stochastic gradient descent is a popular approach for optimizing a function given noisy observations, while incurring low computational complexity. Non-asymptotic bounds in expectation for SGD schemes have been provided in [1]. In the machine learning community, several algorithms have been proposed for minimizing the regret, for instance, [9, 5, 6] and these can be converted to find the minimizer of a (usually convex) function. A closely related field is stochastic approximation (SA) and concentration bounds for SA algorithms have been provided in [4]. Adaptive regularisation in the context of least squares regression has been analysed in [8].\nIn general, none of the schemes proposed above are directly applicable in our setting due to two difficulties: (a) our data {(xi, yi)}ni=1 do not arrive from a distribution, but instead are chosen by a higher level algorithm, and (b) an efficient online scheme is required to track the solution of a least squares regression problem with growing data to be used by the higher level algorithm. This is often true in practice, for instance, in linear bandit problems which are used to model news-recommendation engines.\nThe rest of the paper is organized as follows: In Section 2 we present the online algorithms - with and without regularisation - for solving least squares regression. In Section 3, we apply the first algorithm in a linear bandit setting with strongly convex arms, while in Section 4, we apply the regularised variant (the second algorithm) to a linear bandit setting where the arms are not necessarily from a strongly convex space. Finally, in Section 5 we provide the concluding remarks."
    }, {
      "heading" : "2 Algorithms",
      "text" : "We first present in Section 2.1 an online incremental algorithm that solves a least squares regression problem at each step. We next extend this algorithm to include (adaptive) regularization in Section 2.2. For each of these algorithms, we provide the non-asymptotic bounds that includes both high probability as well as bounds in expectation on the distance of online algorithm iterate from the (unknown) parameter θ∗."
    }, {
      "heading" : "2.1 Random Online",
      "text" : "Recall that θ̂n := min θ 1 2\nn∑ i=1 (yi − θTxi)2."
    }, {
      "heading" : "Assumptions",
      "text" : "(A1) Boundedness of xn, i.e., supn ‖xn‖2 ≤ 1.\n(A2) The noise {ξn} is i.i.d. and |ξn| ≤ 1,∀n2.\n(A3) For all n, λmin(Ān) ≥ µ, where λmin(·) denotes the smallest eigenvalue of a matrix. While the first two assumptions are standard in the context of least squares, the third assumption is made necessary due to the fact that we do not regularise the problem.\nUpdate Rule The online SGD procedure attempts to shadow θ̂n using a stochastic gradient scheme which updates the iterate θn as follows:\nθn = θn−1 + γn(yin − θTn−1xin)xin where in ∼ U({1, . . . , n})3, (1) and θ0 is an arbitrary fixed value. Note that the samples (xin , yin) passed to (1) are uniformly randomly from the set {(x1, y1), . . . , (xn, yn)}. We establish in the following that the iterate θn obtained after running n steps of (1) is close to the least squares solution θ̂n.\nResults We let zn := θn − θ∗. Then Theorem 1. Under (A1)-(A2), for all n ≥ 1, we have\nP (‖zn‖2 − E ‖zn‖2 ≥ ǫ) ≤ exp\n\n   − ǫ\n2\n2 n∑\ni=1 L2i\n\n   , where Li △ = γi\n\n\nn−1∏\nj=i\n(1 + γ2j+1 − 2µγj+1)\n\n\n1/2\n.\nTheorem 2. Under (A1)-(A3), for all n ≥ 1, we have\nE ‖zn‖2 ≤ exp(−µΓn) ‖z0‖2 ︸ ︷︷ ︸\ninitial error\n+\n( n−1∑\nk=1\nγ2k+1h(k) exp(−2µ(Γn − Γk+1)) ) 1 2\n︸ ︷︷ ︸\nsampling error\nwhere h(k) = 2 [ σ2ξ + 2(‖z0‖2 + Γk)2 ] , and σξ = Eξ[ξ2] < ∞ is the variance of the noise.\nThe starting point θ0 of the algorithm impacts the initial error, while the sampling error comes from a martingale difference sequence.\n2We believe that the analysis of this paper can be extended to a setting with unbounded noise that satisfies a sub-Gaussian property.\n3 U(S) denotes the uniform distribution on the set S.\nChoice of step-sizes Fix γn = c/n. For this choice of step size, in Theorem 1\nn∑\ni=1\nL2i =\nn∑\ni=1\nc2 i2\nn∏\nj=i\n(1 + c2 j2 − 2µc j ) ≤\nn∑\ni=1\nKc i2 exp\n −2µc n∑\nj=i\n1\nj\n  ≤ Kcn−2µc ∞∑\ni=1\ni−2(1−µc),\nwhere Kc := c2 exp( c 2π2\n6 ). We now have three regimes for the rate of convergence, based on the choice of\nc: ∑n\ni=1 L 2 i ≤\n[\n1 + 11−2µc\n]\nKcn −2µc when µc ∈ (0, 1/2), ∑ni=1 L2i ≤ Kcn−1 lnn when µc = 1/2, and\n∑n i=1 L 2 i ≤ 2 2µc−1 2µc−1Kcn −1 when µc ∈ (1/2, 1). (We have used comparisons with integrals to bound the summations.) Thus, the optimal rate for the high probability bound from Theorem 1 with (µc > 1/2) is\nP (‖zn‖2 − E ‖zn‖2 ≥ ǫ) ≤ exp ( − ǫ 2n\n2Kµ,c\n)\n, where Kµ,c := 22µc−1\n2µc− 1Kc. (2)\nThe sampling error (second term in Theorem 2) is bounded as follows:\nn−1∑\nk=1\nh(k)γ2k+1 exp (−2µ(Γn − Γk+1)) ≤ c2n−2µc n∑\nk=1\nh(k)k−2(1−µc)\nComparing the sum with the integral yields the following rates for the bound in expectation:\nE ‖zn‖2 ≤ (‖z0‖2\nnµc +\nh1(n)\nnµc∧1/2\n)\n4, (3)\nwhere h1(n) hides ln factors. Thus, the initial error goes down at an exponential rate, while the sampling error decays at the rate n−1/2.\nRemark 1. Ensuring the optimal rate here requires knowledge of the strong convexity constant µ. In our application to linear bandits in Section 3 we know this constant. However, we can use Polyak averaging together with the step size γn = cn−α to arrive at an optimal rate independent of the choice of c. We give further details in Appendix A.4"
    }, {
      "heading" : "2.2 Random Online with Regularisation",
      "text" : "Ideally an online algorithm would not need to satisfy an assumption such as (A3). Indeed we can get rid of this dependency by introducing a regularisation parameter. In an offline setting the natural regularisation parameter would be λ/T for some λ > 0, where T is the size of the batch. However in an online setting we envisage obtaining arbitrary amounts of information, and so we need to regularize adaptively at each time step by λn. Recall θ̃n := argminθ 12n ∑n i=1(yi − θTxi)2 + λn ‖θ‖22.\nUpdate Rule The following algorithm attempts to shadow the solutions θ̃n more and more closely as n → ∞:\nθn = θn−1 + γn((yin − θTn−1xin)xin − λnθn−1), where in ∼ U(1, . . . , n). (4) 4 a ∧ b denotes the minimum of a and b.\nResults Once again, we let zn := θn − θ∗. Then, the high probability bound in Theorem 1 holds with\nthe Lipschitz constants Li := γi\n( n−1∏\nj=i (1 + γ2j+1 − 2λj+1γj+1)\n)1/2\n. However, it is difficult to obtain an\nanalogue of Theorem 2 that bounds zn directly owing to the fact that we regularise with a time-varying λn. Instead, we first bound the distance of the iterate θn to the least squares solution θ̃n below. The final bound on zn is provided indirectly by a combination of triangle inequality and bounds on θn− θ̃n and θ̃n− θ∗ (see Theorem 4 below).\nTheorem 3. Let z̃n = θn − θ̃n. Then, under (A1)-(A2), for all n ≥ 1, we have\nE ‖z̃n‖2 ≤‖z̃0‖2 exp(−Γ′n) ︸ ︷︷ ︸\ninitial error\n+ ∥ ∥ ∥(θ̃n − θ̃0) ∥ ∥ ∥ 2 + n∑\nk=1\nγkλk ∥ ∥ ∥θ̃k − θ̃0 ∥ ∥ ∥ 2 exp(−(Γ′n − Γ′k))\n︸ ︷︷ ︸\ndrift error\n+\n( n∑\nk=1\nh(k)γ2k exp(−2(Γ′n − Γ′k)) )1/2\n︸ ︷︷ ︸\nsampling error\n,\nwhere Γ′n := ∑n k=1 λkγk, θ̃0 is arbitrary and h(n) = 2σ 2 ξ + 8\n( (g(n))2 + λ2n ‖θ∗‖22 + 2λn ‖θ∗‖2 g(n) )2 ,\nwith g(n) := ‖z̃0‖2 + (∑n k=1 γ 2 k exp(−2(Γ′n − Γ′k)) ) 1 2 σξ + Γ ′ n ‖θ∗‖2.\nThe initial and sampling errors are of the same nature as in the random online algorithm, whereas the drift error is caused by adaptively regularizing the problem using a varying λn. This is evident from the following observation: the drift error is obtained by a discrete integration by parts of ∥ ∥ ∥ ∑n\nk=1 Π̃nΠ̃ −1 k (θ̂k − θ̂k−1) ∥ ∥ ∥ 2 ,\nwhere Π̃n := ∏n\nk=1\n( I − γk(Āk + λkI) ) (See Appendix A.2).\nChoice of step-sizes We note two difficulties in the choice of step-sizes and regularisation parameters: (a) In order to recover exactly the Theorem 1 and 2 we would need to set λn = µ. However this regularisation would be too strong, and the solutions θ̃n could fail to converge to the true θ∗. We also require Γ′n → ∞ for Theorem 3 to be useful. (b) We fix γn = c/nα and λn = µ/n1−α, with α ∈ (1/2, 1). While this choice takes care of the initial and sampling errors in the bound in expectation, handling the drift error is still problematic. We overcome this by bounding the drift error with high probability and this turns out to be sufficient for analysis of the linear bandit problems (see Section 4).\nUnder the choice (b) above, we have ∑n\ni=1 L 2 i ≤ n−2µc ∑n i=1 i\n−2(α−µc). When c and µ are chosen so that µc > α− 1/2, we obtain the following high probability bound:\nP (‖z̃n‖2 − E ‖z̃n‖2 ≥ ǫ) ≤ exp ( −ǫ 2n2α−1\nαKµ,c\n)\n, (5)\nwhere Kµ,c = Kµ,c(α) := 22µc−(2α−1) 2µc− (2α − 1)Kc, and Kc = exp (∑∞ i=1 ci −2α ) .\nBound on E ‖z̃n‖2: The initial and sampling errors (first and the last terms in (3)) of the bound in expectation can be seen to be of the order O (n−µc) and O ( h1(n)\nnα−1/2\n)\n, respectively. The drift error is seen to\nbe of the order O ( h2(n)\nn−α/4+1/2\n)\nwith high probability. Thus, we have with probability 1− δ,\nE ‖zn‖2 ≤ (‖z̃0‖2\nnµc +\nh2(n)\nn−α/4+1/2 +\nh1(n)\nnα−1/2\n)\n.\nFor the bandit application, we need to bound the distance between the iterate θn and the unknown parameter θ∗ in the An norm, where An = ∑n−1 i=1 xix T\ni + nλnId. We provide this bound in Theorem 4 below. It is possible to migrate to l2 norm from this result and we skip this owing to space constraints.\nTheorem 4. Recall zn := θn− θ∗. Under (A1)-(A2), with θ0 = 0 and step-sizes γn = c\nnα with c >\n1\n2µ and\nregularisation parameter λn = µ/n1−α, with α ∈ (1/2, 1), we have for any δ > 0\nP ( ‖θn − θ∗‖An,2 ≤ √ 2nκn + √ βn + n α/4 ) ≥ 1− δ,\nwhere\nκn =\n√\nKµ,c n2α−1 log 1 δ +\n(\nCθ∗√ n +\n√\nβn n + h2(n) n−α/4+1/2 + h1(n) nα−1/2\n)\n,\nCθ∗ bounds ‖θ∗‖2, h2(n) = 2( √ βnn −α/4 + 1) and βn = max\n(\n128d log n log n2\nδ ,\n( 8\n3 log\nn2\nδ\n)2 )\n.\nThe proof of this theorem is a combination of a simple triangle inequality and bounds on ∥ ∥ ∥θn − θ̂n ∥ ∥ ∥ An,2\nand ∥ ∥ ∥θ̂n − θ∗ ∥ ∥ ∥ An,2 provided by Lemmas 5 and 6 below. The detailed proof is provided in Appendix A.3.\nLemma 5. Under the conditions of Theorem 4, we have\nP (∥ ∥ ∥θn − θ̂n ∥ ∥ ∥ An,2 ≤ √ 2nκn ) ≥ 1− δ.\nLemma 6. Under the conditions of Theorem 4, we have\nP (∥ ∥ ∥θ̂n − θ∗ ∥ ∥ ∥ An,2 ≤ √ βn + n α/4 ) ≥ 1− δ."
    }, {
      "heading" : "3 Linear Bandits with Strongly Convex Arms",
      "text" : "In a linear bandit problem the set D ⊂ Rd is interpreted as an action set from which the higher level algorithm chooses an action xn at each time instant. The observations yn = ln(xn) are interpreted as loses satisfying E[ ln(xn)| xn] = xTnθ∗, where θ∗ is an unknown parameter. The aim is to minimise the expected cumulative regret: Rn = ∑n i=1 x T i θ ∗ −minx∈D xTθ∗ .\nIn this section, we assume that D is a strongly convex set (see (A3’) below) and the ”best action” function, denoted by G(θ), is assumed to be smooth in the unknown parameter θ that governs the losses of the bandit algorithm (see (A4) below).\nPhased Exploration and Greedy Exploitation (PEGE) of [7] is a well-known algorithm in this setting. PEGE consists of exploration phases and exploitation phases: during each exploration phase the algorithm samples once each element of a basis for D and then computes a least squares estimate of θ∗ using all the data gathered during exploration phases; the exploration phases are separated by exploitation phases of growing length during which the algorithm acts greedily according to the most recently calculated estimate for θ∗.\nSince strong convexity in the regression problem is artificially guaranteed by the algorithm we propose a new algorithm which replaces the calculation of the least squares estimate with the Random Online algorithm from Section 2.1 (see Algorithm 3 below). Whereas, after m exploration phases, PEGE has incurred a complexity of O(md3), our algorithm has incurred an improved complexity of only O(md3). Further, the computational gains come at only logarithmic cost in the order of the regret.\nAlgorithm 1 PEGE with Online GD Input and Initialisation:\nGet a basis {b1, . . . , bd} ∈ D for Rd. Set c = 4d3λmin(∑di=1 bibTi ) and θ0 = 0.\nfor m = 1 to ∞ do Exploration Phase for n = (m− 1)d to md− 1 do\nChoose arm xn = bn mod md and observe yn. Update θ as follows: θn = θn−1 + cn((yj − θTn−1xj)xj), where j ∼ U(1, . . . , n).\nend for Exploitation Phase Find x = G(θmd) := argminx∈D{θTmdx}. Choose arm x m times consecutively.\nend for"
    }, {
      "heading" : "3.1 Analysis",
      "text" : "We require the following extra assumptions from [7]:\n(A3’) A basis {b1, . . . , bd} ∈ D for Rd is made known to the algorithm.\n(A4) The function G : θ → argminx∈D{θTx} is J-Lipschitz. The assumption (A4) is satisfied, for example, when D is the unit sphere. However it is not satisfied when D is discrete. We provide a slightly modified version of Theorem 3.1 of [7]: Theorem 7. Under the assumptions (A1), (A2), (A3’), and (A4), the cumulative regret of PEGE with Online GD satisfies\nRn ≤ C1(‖θ∗‖2 + ‖θ∗‖ −1 2 )h3(n)dn 1/2,\nwhere C1 is a constant depending on λmin( ∑d i=1 bib T i ) and J , and h3 hides log factors.\nProof. Note that for all n > d,\nλmin(Ān) ≥ λmin ( (n mod d) ∑d i=1 bib T i\n[(n mod d) + 1]d\n)\n≥ λmin\n( ∑d\ni=1 bib T i\n)\n2d .\nSo from equation (3), taking γn = 4d3nλmin( ∑d i=1 bib T i ) we have for any n ≥ d\nE ‖θn − θ∗‖22 ≤ (‖z0‖2 + h1(n))2/n. (6) Now to complete the proof we only need to reprove Lemma 3.6 of [7]:\nLemma 8. For all n ≥ d, E ‖θ∗(G(θ∗)−G(θmd))‖2 ≤ h3(n) m‖θ∗‖\n2\nwhere h3(n) hides log factors.\nProof. Note that\n‖θ∗(G(θ∗)−G(θmd))‖2 = ‖(θ∗ − θmd)TG(θ∗) + (G(θ∗)−G(θmd))Tθmd + (θmd − θ∗)G(θmd)‖2\n≤ ‖(θ∗ − θmd)T(G(θ∗)−G(θmd))‖2 = ∥ ∥ ∥ ∥ G ( θ∗\n‖θ∗‖2\n) −G (\nθmd ‖θmd‖2 )∥ ∥ ∥ ∥ 2 ≤ 2J ‖θ ∗ − θmd‖22 ‖θ∗‖2 ,\nwhere the second equality follows from the fact that G(θ) = G(xθ) for all x > 0, and for the second inequality we have used (A4) and Lemma 3.5 of [7]. The lemma follows from (6).\nThe rest of the proof follows the scheme of the proof of Theorem 3.1 of [7]."
    }, {
      "heading" : "4 Linear Bandits with Non-Strongly Convex Arms",
      "text" : "The setting here is similar to that of the previous section, except that assumptions (A3’) and (A4) do not hold. In other words, the arms are not assumed to belong to a strongly convex subset of Rd - a property that is not known to hold in many practical settings such as news recommendation engines where the arms are discrete. The well-known confidence ball algorithm of [2] works by constructing a least squares estimate θ̂ using {(xi, yi)}ni=1 around θ∗ and using this estimate to pick an arm greedily, i.e., argminx∈D minv∈B2n{v\nTx}. The regret Rn with this algorithm was shown to be of the order O( √ dn lnn) in [2].\nWe propose an enhancement to the confidence ball algorithm, where at each step we use the regularized version of the random online algorithm to shadow the empirical estimator (which is the solution to a least squares problem). This new algorithm (see Algorithm 4 below for details) has a complexity O(nd), where d is the dimension of the arm space, D = Rd. A vanilla confidence ball algorithm has a complexity O(nd2) per time step, and so our proposed enhancement has significantly improved complexity. On the other hand, the computational gains come at the cost of a loss of n1/5 in the regret Rn.\nAlgorithm 2 Confidence Ball with Online GD Input and Initialisation:\nChoose µ and c so that µc > 1/2, α ∈ (1/2, 1) and set θ0 = 0. for n = 1 to ∞ do\nFind xn = argminx∈D minv∈B2n v Tx, where B2n =\n{ v : ‖v − θt‖An,2 ≤ √ 2nκn + √ βn + n α/4 } .\nChoose arm xt and observe yt. Update θn: θn = θn−1 + cn−α((yin − θTn−1xin)xin − µnα−1θn−1) where in ∼ U(1, . . . , n).\nend for"
    }, {
      "heading" : "4.1 Analysis",
      "text" : "We make the following extra assumption: (A4’) An upper bound, Cθ∗ , for ‖θ∗‖2 is known. Choose the step-sizes to be γn = cn−α with µc > 1/2, and adaptive regularisation parameters λn = µ/n1−α, where α ∈ (1/2, 1). Then using Theorem 4 we can define a confidence region,\nB2n = { v : ‖v − θn‖An,2 ≤ ηn } ,\nwhere ηn = √ 2nκn + √ βn + n α/4. In our setting, Theorem 4 serves as an analogue to Theorem 5 of [2].\nTheorem 9. Under (A1), (A2), and (A4’), with probability 1− δ, we have Rn ≤ (n4dηn lnn) 1 2 .\nProof. We break the proof into three steps:\nStep 1: Bound the instantaneous regret Let An be defined as in Section 2.2. First note that, for all θ ∈ B2n,\n|(θ − θ̂n)Tx| ≤ ∥ ∥ ∥ ∥ ( A − 1 2 n (θ − θ̂n) ) T ∥ ∥ ∥ ∥ 2 ∥ ∥ ∥ ∥ A − 1 2 n x ∥ ∥ ∥ ∥ 2 ≤ ηn √ xTA−1n x\nwhere we have used Cauchy-Schwarz for the first inequality, and the definition of B2n, for the second. Define wn := (x T nA −1 n xn) 1/2. Then we have\nrn := x T nθ ∗ −min x∈C xTθ∗ ≤ xTn(θ∗ − θ̃) + xTn(θ̂n − θ̃) ≤ 2ηnwn where θ̃ := arg min θ∈B2n xTnθ.\nStep2: Bound the widths For this part we first note that det(An+1) = ∏t\nτ=1(1 + n(λn+1 − λn) + w2n): We can show this from\ndet(An+1) = det(An) det((1 + n(λn+1 − λn))I +A − 1 2 n xn(A − 1 2 n xn) T)\n= det(An) det((1 + n(λn+1 − λn))I + vnvTn),\nwhere vn := A − 1 2 n xn. Now vn is an eigenvector of (1 + n(λn+1 − λn))I + vnvTn, and its corresponding eigenvalue is 1 + n(λn+1 − λn) + w2n. Moreover all the other eigenvalues of this matrix are 1, since the matrix has rank one. Furthermore, trace(An) = dλn + ∑n k=1 ‖xk‖22 ≤ 2dn. Using this as a constraint on the eigenvalues of An we can derive that det(An) ≤ n2d. Hence, using that for all 0 ≤ y ≤ 1, ln(1 + y) ≥ y/2, we have that n∑\nk=1\nw2k ∧ 1 ≤ n∑\nk=1\n2 ln(1 + w2k) ≤ n∑\nk=1\n2 ln(1 + n(λn+1 − λn) + w2k) = 2 ln(det(An+1)) ≤ 4d ln n.\nStep 3: Bound the regret From the above we know that when θ∗ ∈ B2n, for all n,\nRT ≤ ( T T∑\nn=1\nr2n\n) 1 2 ≤ ( T T∑\nn=1\n2η2n(w 2 n ∧ 1)\n) 1 2\n≤ ( T4dη2T lnT ) 1 2 .\nApplying Theorem 4 gives the result.\nCorollary 10. Under the conditions of Theorem 9, setting γn = c/nα and λn = µ/n1−α, where α = 4/5, the regret RT ≤ 2d √ lnT T 1/2+1/5.\nProof. Follows from the analysis in Section 2.2 and Theorem 9."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We proposed two SGD schemes with randomisation for the problem of reducing complexity in least squares regression problems where the samples do not come from a probability distribution, but are instead given to us by a higher level learning algorithm. In particular, when the higher level algorithm can guarantee strong convexity in the data, we proposed a non-regularised scheme, and otherwise we proposed an adaptively regularised scheme. We provided error bounds both in expectation and in high probability for the two schemes. To demonstrate the applicability of these results we applied both schemes to the linear bandit problem, the non-regularised scheme in the PEGE algorithm, and the regularised scheme in the ConfidenceBall algorithm. We found that since strong convexity of the data is guaranteed by the PEGE algorithm we obtained a speed up of O(d) at a cost of only logarithmic factors in the regret. However for the ConfidenceBall algorithm, although we achieved a speed up of O(d) here as well, we lost O(n1/5) in the regret.\nThe difference between the two regret performances appears to be a result of adaptive regularisation. PEGE is only applicable when the action set of the linear bandit is strongly convex, which allows the algorithm to guarantee strong convexity in the data set. This is a strong condition, making the learning problem an easier one to solve. ConfidenceBall is applicable in any compact action set, but cannot guarantee strong convexity in the data as a result. One solution is to regularise the problem, however this introduces a drift error into the regression problem that is difficult to control. Whether this gap in the regret bound for linear bandits with compact action sets can be eliminated remains an open question for future research."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    }, {
      "heading" : "A.1 Random Online",
      "text" : ""
    }, {
      "heading" : "Proof of Theorem 1",
      "text" : "We use the proof technique from [4] in arriving at the high-probability bounds.\nStep 1 Let Hi := {(xj , yj)}ij=1 ∪ {θ0}. We decompose ‖zn‖22 − E ‖zn‖ 2 2 into a sum of martingale differences as follows:\n‖zn‖2 − E ‖zn‖2 = n∑\ni=1\nE[‖zn‖2 |Hi ]− E[‖zn‖2 |Hi−1 ]\n=\nn∑\ni=1\nE[‖zn‖2 |θi ]− E[E(‖zn‖2 |θi ) |Hi−1 ] = n∑\ni=1\nDi, (7)\nwhere Di △ = gi − E[gi |Hi−1 ] and gi =\nn∑ i=1 E[‖zn‖2 |θi ].\nStep 2 We establish that the functions gi are Lipschitz continuous with constants Li.\nProposition 1. The functions gi are Lipschitz continuous with constants\nLi := γ 1/2 i\nn−1∏ j=i (1 + γ2j+1 − 2µγj+1)1/2.\nProof. Denote fn(θ) := 12 (ξin − (θ − θ∗)Txin)2. The update (1) can be re-written as\nθn = θn−1 − γn(F ′n(θn−1)−∆Mn),\nwhere Fn(θ) := Ein [fn(θ) | Hn] and ∆Mn+1 is the associated martingale difference sequence defined as ∆Mn+1(θ) = F ′ n(θ)− f ′n(θ).\nLet Θij(θ) denote the mapping that returns the value of the iterate updated according to (1) at instant j, θj , given that θi = θ. Then, we have\n∥ ∥Θij+1(θ) −Θij+1(θ′) ∥ ∥ 2 2 = ∥ ∥Θij(θ)−Θij(θ′)− γj+1[f ′j+1(Θij(θ))− f ′j+1(Θij(θ′))] ∥ ∥ 2 2\n≤(1 + γ2j+1) ∥ ∥Θij(θ)−Θij(θ′) ∥ ∥ 2\n2\n− 2γj+1〈Θij(θ)−Θij(θ′), F ′n(Θij(θ))− F ′n(Θij(θ′))〉 + 2γj+1〈Θij(θ)−Θij(θ′),∆Mj+1(θ)−∆Mj+1(θ′)〉,\nThe inequality above follows from the Lipschitz continuity of f ′j in the noise, as shown below:\n∀θ, ∥ ∥(ξj − (θ − θ∗)Txj)xj)− (ξ′j − (θ − θ∗)Txj)xj) ∥ ∥ 2 ≤ ∥ ∥(ξj − ξ′j) ∥ ∥ 2 ,\nwhere the inequality follows from (A1). As a consequence of (A3), we obtain F ′′n (θ) ≥ µI (or F is µstrongly convex), and hence\n∥ ∥Θij+1(θ)−Θij+1(θ′) ∥ ∥ 2 2 ≤(1 + γ2j+1 − 2γj+1µ) ∥ ∥Θij(θ)−Θij(θ′) ∥ ∥ 2 2\n+ 2γj+1〈Θij(θ)−Θij(θ′),∆Mj+1(θ)−∆Mj+1(θ′)〉,\nA repeated application of this inequality, followed by taking expectations yields the following\nE [∥ ∥Θin(θ)−Θin(θ′) ∥ ∥ 2\n2\n]\n≤ ∥ ∥θ − θ′ ∥ ∥2\n2\nn−1∏\nj=i\n(1 + γ2j+1 − 2γj+1µ).\nFinally putting all this together we have\n‖E [‖θn − θ∗‖2 |θi−1, ξii = ξ ] −E [ ‖θn − θ∗‖2 ∣ ∣θi−1, ξii = ξ ′ ]∥ ∥ 2\n≤ E [∥ ∥Θin (θ)−Θin ( θ′ )∥ ∥ 2 ] ≤\n\nγi\nn−1∏\nj=i\n(1 + γ2j+1 − 2γj+1µ) 1 2\n\n ∥ ∥ξ − ξ′ ∥ ∥ 2 = Li ∥ ∥ξ − ξ′ ∥ ∥ 2 .\nStep 3 The last step of the proof is to invoke a concentration bound for sum of martingale differences Di.\nP (‖zn‖2 − E ‖zn‖2 ≥ ǫ) =P ( n∑\ni=1\nDi ≥ ǫ ) ≤ exp(−λǫ)E ( exp ( λ n∑\ni=1\nDi\n))\n= exp(−λǫ)E ( exp ( λ n−1∑\ni=1\nDi\n)\nE\n( exp(λDn |Hn−1 ) ))\nThe first equality above follows from (7), while the inequality follows from Markov inequality. Since ξi are bounded by (A2), we have the following property that holds for every 1-Lipschitz function g, we have\nE (exp(λg(ξ1))) ≤ exp ( λ2\n2\n)\n.\nNoting that gi is Lipschitz with constant Li by Proposition 1, we apply the above inequality to obtain\nE (exp(λDn |Hn−1 )) ≤ exp ( λ2L2n 2 ) ,\nand so\nP (‖zn‖2 − E ‖zn‖2 ≥ ǫ) ≤ exp(−λǫ) exp ( αλ2\n2\nn∑\ni=1\nL2i\n)\nThe claim follows by optimizing over λ in the above."
    }, {
      "heading" : "Proof of Theorem 2",
      "text" : "We define F̃n(θ) △ = Eξ(Fn(θ)) = Eξn,in [f ′ n(θ) | Hn], and ∆M̃n+1(θ) = F̃ ′n(θ) − fn(θ). First we extract a martingale from the sequence (zn)n≥1, and recover a recursive procedure for zn involving the martingale differences ∆M̃n:\nzn =zn−1 − γn ( F̃ ′n(θn−1)−∆M̃n ) = zn−1 − γn ( Ānzn −∆M̃n )\n= ( 1− γnĀn ) zn−1 + γn∆M̃n =\nn∏\nk=1\n( I − γkĀk ) z0 −\nn∑\nk=1\nγk\n\n\nn∏\nj=k+1\n( I − γjĀj )\n\n∆M̃k,\nBy Jensen’s inequality\nE ∥ ∥ ∥ ∥ ∥ Π̃nz0 + n∑\nk=1\nγkΠ̃nΠ̃ −1 k ∆M̃k ∥ ∥ ∥ ∥ ∥ 2 ≤ ( ∥ ∥ ∥Π̃nz0 ∥ ∥ ∥ 2 2 + n∑ k=1 γ2kE ∥ ∥ ∥Π̃nΠ̃ −1 k ∆M̃k ∥ ∥ ∥ 2 2 )1/2\n(8)\nwhere Π̃n := ∏n\nk=1\n( I − γkĀk ) . Notice that Ān − µI is positive definite by (A3) and hence\n∥ ∥ ∥Π̃nΠ̃ −1 k ∥ ∥ ∥ 2 = ∥ ∥ ∥ ∥ ∥ ∥ n∏\nj=k+1\n( I − γjĀj ) ∥ ∥ ∥ ∥ ∥ ∥ 2 ≤ n∏ j=k+1 ∥ ∥(1− γjµ)I − γj(Āj − µI) ∥ ∥ 2\n≤ n∏\nj=k+1\n‖(1− γjµ)I‖2 ≤ n∏\nj=k+1\n(1− γjµ) ≤ exp (−µ(Γn − Γk)) , (9)\nFinally we need to bound square the martingale difference:\nE[ ∥ ∥ ∥∆M̃n ∥ ∥ ∥ 2\n2 ] = Eξ,it〈f ′it(θt−1), f ′it(θt−1)〉 − 〈F̃ ′n(θt−1), F̃ ′n(θt−1)〉\nUsing (A1) and (A2), a calculation shows that\nEξ,it〈f ′it(θt−1), f ′it(θt−1)〉 ≤σ2ξ + ‖zt‖ 2 2 & 〈F̃ ′n(θt−1), F̃ ′n(θt−1)〉 ≤ ‖zt‖ 2 2\nwhere σ2ξ := V ar(ξ) < ∞ (ξ is distributed according to the noise distribution). Now\n‖zt‖2 = ∥ ∥ ∥ ∥ ∥ ∥ [ t∏\nk=1\n(I − γkxikxTk) ] z0 + t∑\nk=1\nγk\n\n\nt∏\nj=k\n(I − γjxijxTj )\n\n ξkxk ∥ ∥ ∥ ∥ ∥ ∥ 2\n≤‖z0‖2 + t∑\nk=1\nγk ≤ ‖z0‖2 + ln t.\nand so E[ ∥ ∥ ∥∆M̃t ∥ ∥ ∥ 2\n2 ] ≤ h(t) where h(t) = 2\n[ σ2ξ + 2(‖z0‖2 + ln t)2 ] .\nWe conclude that\nE ‖zn‖2 ≤ ( ‖z0‖22 exp(−2µΓn) + n∑\nk=1\nh(k)γ2k exp(−2µ(Γn − Γk)) )1/2 ."
    }, {
      "heading" : "A.2 Regularized Random Online",
      "text" : "The proof of the high probability bound for the (adaptive) regularized setting follows in a similar manner as the proof of Theorem 1 and uses the fact that each F ′n is λn-strongly convex. On the other hand, the proof of the bound in expectation in this setting is more complicated than that of Theorem 1 owing to the adaptive regularization parameters λn. We first prove that the iterate θn is close to the corresponding regularized solution θ̃n at instant n. Let zn = θn − θ̃n. Then,\nzn =θn − θ̃n−1 + θ̃n−1 − θ̃n =zn−1 − γn ( F̃ ′n(θn−1)−∆M̃n ) + (θ̃n−1 − θ̃n)\n=zn−1 − γn ( (Ān + λnI)zn−1 −∆M̃n ) + (θ̃n−1 − θ̃n) = ( 1− γn(Ān + λnI) ) zn−1 + γn∆M̃n + (θ̃n−1 − θ̃n)\n=\nn∏\nk=1\n( I − γk(Āk + λkI) ) z0 −\nn∑\nk=1\n\n\nn∏\nj=k+1\n( I − γj(Āj + λkI) )\n\n (γk∆M̃k + (θ̃k − θ̃k−1))\n=Π̃nz0 − n∑\nk=1\nΠ̃nΠ̃ −1 k (θ̃k − θ̃k−1) +\nn∑\nk=1\nγkΠ̃nΠ̃ −1 k ∆M̃k,\nwhere Π̃n := ∏n\nk=1\n( I − γk(Āk + λkI) ) . By Jensen’s inequality, we obtain\nE ‖zn‖2 ≤ ∥ ∥ ∥ ∥ ∥ Π̃nz0 + n∑\nk=1\nΠ̃nΠ̃ −1 k (θ̃k+1 − θ̃k) ∥ ∥ ∥ ∥ ∥ 2 + ( n∑ k=1 γ2kE ∥ ∥ ∥Π̃nΠ̃ −1 k ∆M̃k ∥ ∥ ∥ 2 2 )1/2\n(10)\nUsing the fact that Ān − λnI is positive definite, we obtain\n∥ ∥ ∥Π̃nΠ̃ −1 k ∥ ∥ ∥ 2 = ∥ ∥ ∥ ∥ ∥ ∥ n∏\nj=k+1\n( I − γj(Āj + λjI) ) ∥ ∥ ∥ ∥ ∥ ∥ 2 ≤ n∏ j=k+1 ‖(1− γjλj)I‖2 ≤ exp ( −(Γ′n − Γ′k) ) , (11)\nWe now bound each of the terms in (10) as follows: First term It is easy to see that ∥ ∥ ∥Π̃nz0\n∥ ∥ ∥ 2 ≤ ‖z0‖2 exp(−Γ′n).\nSecond term Using a discrete integration by parts, we obtain\nn∑\nk=1\nΠ̃nΠ̃ −1 k (θ̃k − θ̃k−1) = (θ̃n − θ̃0) +\nn∑\nk=1\nγkλk(θ̃k − θ̃0)Π̃nΠ̃−1k , (12)\nwhere θ̃0 is chosen arbitrarily. Thus, ∥ ∥ ∥ ∥ ∥ n∑\nk=1\nΠ̃nΠ̃ −1 k (θ̃k − θ̃k−1) ∥ ∥ ∥ ∥ ∥ 2 ≤ ∥ ∥ ∥(θ̃n − θ̃0) ∥ ∥ ∥ 2 + n∑ k=1 γkλk ∥ ∥ ∥θ̃k − θ̃0 ∥ ∥ ∥ 2 exp(−(Γ′n − Γ′k)), (13)\nLast term The martingale difference (last term in (10)) is bounded as below:\nE[ ∥ ∥ ∥∆M̃n ∥ ∥ ∥ 2\n2 ] ≤ 2\n( E〈f ′in(θn−1), f ′in(θn−1)〉+ E〈F̃ ′n(θn−1), F̃ ′n(θn−1)〉 )\nUsing (A1) and (A2), a simple calculation shows that\nE〈f ′in(θn−1), f ′in(θn−1)〉 ≤σ2ξ + 2(E ‖zn‖ 2 2 + λ 2 n ‖θ∗‖22 + 2λn ‖θ∗‖2 E ‖zn‖2)\n& E〈F̃ ′n(θn−1), F̃ ′n(θn−1)〉 ≤2(E ‖zn‖22 + λ2n ‖θ∗‖ 2 2 + 2λn ‖θ∗‖2 E ‖zn‖2)\nwhere σ2ξ := V ar(ξ) < ∞ (ξ is distributed according to the noise distribution). Now\nE ‖zn‖2 =E ∥ ∥ ∥ ∥ ∥ [ n∏\nk=1\n(I − γk(xikxTik + λkI)) ] z0\n+ n∑\nk=1\nγk\n\n\nn∏\nj=k+1\n(I − γj(xijxTij + λjI))\n\n (ξkxik + λkθ ∗) ∥ ∥ ∥ ∥ ∥ ∥ 2\n≤‖z0‖2 + ( n∑\nk=1\nγ2k exp(−2(Γ′n − Γ′k)) ) 1 2 σξ + Γ ′ n ‖θ∗‖2 =: g(n).\nand so E[ ∥ ∥ ∥∆M̃n ∥ ∥ ∥ 2\n2 ] ≤ h(n) where h(n) = 2σ2ξ + 8\n( (g(n))2 + λ2n ‖θ∗‖22 + 2λn ‖θ∗‖2 g(n) )2 .\nPutting it all together, (10) simplifies to the following form:\nE ‖zn‖2 ≤‖z0‖2 exp(−Γ′n) + ∥ ∥ ∥(θ̃n − θ̃0) ∥ ∥ ∥ 2 + n∑\nk=1\nγkλk ∥ ∥ ∥θ̃k − θ̃0 ∥ ∥ ∥ 2 exp(−(Γ′n − Γ′k))\n+\n( n∑\nk=1\nh(k)γ2k exp(−2(Γ′n − Γ′k)) )1/2 ."
    }, {
      "heading" : "A.3 Proof of Theorem 4",
      "text" : ""
    }, {
      "heading" : "Proof of Lemma 5",
      "text" : "Proof. Using the analysis from Section 2.1 for the choice of step-sizes γn = c\nnα with c >\n1\n2µ and λn =\nµ/n1−α, with α ∈ (1/2, 1), we obtain the following bound from (2.2)\nP (∥ ∥ ∥θn − θ̃n ∥ ∥ ∥ 2 − E ∥ ∥ ∥θn − θ̃n ∥ ∥ ∥ 2 ≥ ǫ ) ≤ exp ( −ǫ 2n2α−1\nKµ,c\n)\n⇔P ( ∥ ∥ ∥θn − θ̃n\n∥ ∥ ∥ 2 − E ∥ ∥ ∥θn − θ̃n ∥ ∥ ∥ 2 ≤\n√\nKµ,c n2α−1 log 1 δ\n)\n≥ 1− δ\nLetting θ̃0 = θ∗, the drift error (second term in the bound in expectation from Theorem 3) can be simplified as follows: With probability 1− δ, we have\n∥ ∥ ∥(θ̃n − θ∗) ∥ ∥ ∥ 2 +\nn∑\nk=1\nγkλk ∥ ∥ ∥θ̃k − θ∗ ∥ ∥ ∥ 2 exp(−(Γ′n − Γ′k))\n≤ √\nβn n\n+ nα/4−1/2 + n−µc n∑\nk=1\n1 k (\n√\nβk k + kα/4−1/2)kµc ≤ h1(n)nα/4−1/2\nwhere the first inequality follows from Lemma 6 and the following change of norm inequality ‖zn‖2An,2 ≤ 2n ‖zn‖22. The latter is true because ‖zn‖ 2 An,2 = zTnAnzn = un diag(An)un, where An is factorized as P TnAnPn and un = Pnzn. Thus, ‖zn‖2An,2 ≤ λmax ‖un‖ 2 2 = λmax ‖zn‖ 2 2, where λmax is the largest eigen value of An. By the definition of An, it is evident that λmax ≤ 2n. The other two terms in Theorem 3 can be bound in a straightforward manner to finally obtain with probability 1− δ the following:\nE ‖zn‖2 ≤ ( Cθ∗\nnµc +\nh2(n)\nn−α/4+1/2 +\nh1(n)\nnα−1/2\n)\nThus, we obtain\nP (‖θn − θ∗‖2 ≤ κn) ≥ 1− δ. (14)\nThe claim now follows by a changing to An norm in (14)."
    }, {
      "heading" : "Proof of Lemma 6",
      "text" : "Proof. Let Vn = An(θ̃n − θ∗). Note that Vn can also be written as Vt = n−1∑\ni=1 ηixi − λ̃nθ∗, where ηi =\nyn − xTnθ∗ and λ̃n △ = nλn. We first bound z̃n △ = ∥ ∥ ∥θ̃n − θ∗ ∥ ∥ ∥ An,2 = (θ̃n − θ∗)An(θ̃t − θ∗) below:\nz̃n+1 =Vn+1A−1n+1Vn+1 =(Vn + ηnxn − dn+1θ∗)TA−1n+1(Vn + ηnxn − dn+1θ∗) =VTnA−1n+1Vn + 2ηnxTnA−1n+1Vn + η2nx2Tn A−1n+1xn + d2n+1θ∗TA−1n+1θ∗ − 2dn+1A−1n+1(Vn + ηnxn)\n≤VTnA−1n+1Vn + 2ηnxTnA−1n+1Vn + η2nx2Tn A−1n+1xn − d2n+1\nn+ λ̃n+1 ‖θ‖22 − 2dn+1θ∗T(θ̃n − θ∗), (15)\nwhere the last inequality holds because ‖An+1‖2 n+ λ̃n+1 by (A2). A−1n+1 can be simplified as follows:\nA−1n+1 = (An + xnx T n + dn+1Id) −1 = (An + xnx T n) −1 − dn+1(An + xnxTn)−1A−1n+1\n≤ (\n1− dn+1 n+ λ̃n+1\n)\n(An + xnx T n) −1 ≤\n(\n1− dn+1 n+ λ̃n+1\n)(\nA−1n − A−1n xnx T nA −1 n\n1 + w2n\n)\n(16)\nwhere the second equality follows from the identity (M+cI)−1 = M−1−cM−1(M+cI)−1 for any square matrix M and constant c, the first inequality follows from the fact that ‖An+1‖2 n + λ̃n+1, and the second inequality follows by Sherman-Morrison lemma. Note that wn △ = xTnA −1 n xn. Plugging (16) into (15), we obtain\nz̃n+1 ≤ (\n1− dn+1 n+ λ̃n+1\n)[\nz̃n + 2ηn xTn(θ̃n − θ∗)\n1 + w2t +\nη2nw 2 n\n1 + w2n\n]\n− d 2 n+1\nn+ λ̃n+1 ‖θ∗‖22 − 2dn+1θ∗T(θ̃n − θ∗).\nUnrolling the above recursion and noting that z̃1 ≤ d, we obtain\nz̃n ≤d+ n∑\nk=1\n(\n2ηk xTk(θ̃k − θ∗)\n1 + w2k + η2k w2k 1 + w2k\n)\n− n∑\nk=1\nd2k+1\nk + λ̃k+1 ‖θ∗‖22 − 2\nn∑\nk=1\ndk+1θ ∗T(θ̃k − θ∗).\n≤d+ n∑\nk=1\n(\n2ηk xTk(θ̃k − θ∗)\n1 + w2k + η2k w2k 1 + w2k\n)\n− 2 n∑\nk=1\n((k + 1)α − kα))θ∗T(θ̃k − θ∗). (17)\nIn arriving at the first equality, we have used the fact that n∏\nk=1\n(\n1− dk+1 k+λ̃k+1\n)\n≤ 1, while the second inequality (17) follows since the term removed is positive. In (17), we have also used for the regularisation parameter λ̃n, the choice of λ̃n = nα, for some α ∈ (1/2, 1).\nWe now bound each of the terms in (17) below:\n• Let Mk = ηk xTk(θ̃k − θ∗)\n1 + w2k . It can be seen that Mk is a martingale difference sequence. An application of\nFreedman’s concentration result to n−1∑\nk=1\nMk establishes that with high probability, say 1−δ, n−1∑\nk=1 Mk ≤ βk/2, for all k. • By assumption, we have ‖ηi‖2 ≤ 1. Further, w2k\n1 + w2k ≤ 2d ln n.\n• We bound the last term in (17) as follows: ∥ ∥ ∥dk+1θ\n∗T(θ̃k − θ∗) ∥ ∥ ∥ 2 C-S ≤ dk+1 ‖θ∗‖2 ∥ ∥ ∥θ̃k − θ∗ ∥ ∥ ∥ 2 ≤ dk+1√\nλ̃k+1\n‖θ∗‖2 √ βk\nThe last inequality above holds because ∥ ∥ ∥θ̃n − θ∗ ∥ ∥ ∥ An,2 ≤ √βn and ‖z‖2 ≤ ‖z‖An,2 √ λ̃n since ‖An‖2 ≥ λ̃n by definition. By comparing the summation with the integral, we claim n∑\nk=1\n((k + 1)α − kα) (k + 1)α/2 √ βk ≤ C3(n+ 1)α/2,\nwhere C3 hides log factors. Putting it all together, we have the following simplified form of (17):\nz̃n ≤ d+ βn 2 + 2d ln n+ C3(n+ 1) α/2 (18)\nThe claim follows.\nA.4 Iterate Averaging\nHere we incorporate the well-known Polyak-Ruppert scheme to average the iterates, which when coupled with larger step-sizes cn−α with α ∈\n( 1 2 , 1 ) leads to a convergence rate of the order O(n−1/2) irrespective\nof the choice of c in the step-sizes:\nDefine θ̄n+1 △ = (θ1 + . . . + θn)/n and let zn = θ̄n+1 − θ∗ denote the approximation error as before.\nThen, we have the following bound in high probability:\nTheorem 11. Under (A1)-(A3) we have, for Li △ = γi n\n(\n1 + n−1∑\nl=i+1\nl∏\nj=i\n( 1 + γ2j+1 − 2µγj+1) )1/2\n)\n,\n∀n ≥ 1,∀ǫ ≥ 0, P (‖zn‖2 − E ‖zn‖2 ≥ ǫ) ≤ exp\n\n   − ǫ\n2\n2 n∑\ni=1 L2i\n\n   .\nChoice of step-sizes Fix γn := cn−α, for some α ∈ (1/2, 1). Then we have in Theorem 11:\nn∑\ni=1\nL2i ≤ Kµ,c,α\nn where Kµ,c,α = c\n3 exp\n( c2\n2α− 1\n) \n\n( √ 2\nµ3/2c exp\n(√ 2\nµ\n))2\n+\n(\n3 + 2\nc\n)2 \n\n(See Lemma 21 below for a derivation). So, the high probability bound from Theorem 11 is\nP (‖zn‖2 − E ‖zn‖2 ≥ ǫ) ≤ exp ( − ǫ 2n\n2Kµ,c,α\n)\n.\nAveraging the bound in expectation obtained in Theorem 2, we obtain the following: ∀η > 0,\nE ‖zn‖2 ≤ K̃µ,c,α ‖z0‖2 n + h̃(n) η2nα−1/2−η\nwhere K̃µ,c,α := ∑∞\nk=1 exp(−µck1−α/(1 − α)). Thus, it is possible to remove the dependency on the knowledge of µ for the choice of c through averaging of the iterates. However, as suggested by earlier works on stochastic approximation, it is preferred to average after a few iterations since the initial condition ‖z0‖2 is not forgotten exponentially fast with averaging.\nProof of Theorem 11. As in Theorem 1, we first decompose ‖zn‖22 − E ‖zn‖ 2 2 into a sum of martingale differences as follows:\n‖zn‖2 − E ‖zn‖2 = n∑\ni=1\nDi, (19)\nwhere Di △ = gi − E[gi |Hi−1 ] and gi =\nn∑ i=1 E[‖zn‖2 ∣ ∣ζi = (ζ 1 i , ζ 2 i ) ]. Here ζ 1 i is the value of the averaged\niterate θ̄i+1 at instant i and ζ2i is the value of the iterate θi at instant i. The next step is to prove that the functions gi are Lipschitz continuous with constants\nLi △ = γi n\n\n1 +\nn−1∑\nl=i+1\nl∏\nj=i\n( 1 + γ2j+1 − 2µγj+1) )1/2\n\n .\nLet Θ̄ij(ζ) denote the mapping that returns the value of the averaged iterate at instant j, θj , given that θ̄i+1 = ζ 1 and θi = ζ2. Then, we have\nE [∥ ∥Θ̄in(ζ)− Θ̄in(ζ ′) ∥ ∥ 2 ] ≤ i+ 1\nn\n∥ ∥ζ1 − ζ ′1 ∥ ∥ 2 + 1\nn\nn−1∑\nl=i+1\nl∏\nj=i\n( 1 + γ2j+1 − 2µγj+1) )1/2 ∥ ∥ζ2 − ζ ′2 ∥ ∥ 2 (20)\nUsing the sub-Gaussian property of noise ξ, we obtain for every λ > 0\nE ( exp(λgi(θ̄i+1, θi) ∣ ∣ζi−1 = (ζ 1, ζ2) ) =E\n(\nexp(λgi( i\ni+ 1 θ̄i +\n1\ni+ 1 θi, θi)\n∣ ∣(ζ1, ζ2)\n)\n≤ exp(λgi) exp (\nλ2L2f 2\n)\n,\nwhere Lf is the Lipschitz constant of the function f defined as\nf(ξ) = gi\n( i\ni+ 1 ζ1 +\n1 i+ 1 ζ2 − γi i+ 1 f ′i(ζ\n2), ζ2 − γif ′i(ζ2) ) .\nAs a consequence of (20), we obtain Lf = γi n\n(\n1 + n−1∑\nl=i+1\nl∏\nj=i\n( 1 + γ2j+1 − 2µγj+1) )1/2\n)\n. The rest of the\nproof follows in a similar manner to the proof of Theorem 1.\nLemma 12. When γi = cnα for some α ∈ ( 1 2 , 1 ) , we have\nn∑\ni=1\nL2i ≤ cKc,α µ\n(\n2\n( 2α\nµ )1−α (1− α µc ) exp\n(( 2α\nµ\n)1−α )\n1 n + 3 + 2 c\n)2 1\nn (21)\nProof. The main ingredients of this derivation can be found in the argument of page 15 in [3], however here we manage to give all the constants explicitly. We perform the calculation:\nn∑\ni=1\nL2i = n∑\ni=1\n\n γi n\n 1 + n−1∑\nl=i+1\nl∏\nj=i\n( 1 + γ2j+1 − 2µγj+1) )1/2\n\n\n\n\n2\n≤ exp\n(\n2 ∑\nγ2j\n)\n(µn)2\nn∑\ni=1\n[\nγi\n(\n1 + n−1∑\nl=i+1\nexp\n(\n−µ n∑\nl=i+1\nγl\n))]2\n≤ exp\n(\n2 ∑\nγ2j\n)\n(µn)2\nn∑\ni=1\n[\nγiγ −1 i+2 + γi\nn−1∑\nl=i+1\nexp(−µ n∑\nl=i+1\nγl)(γ −1 l+2 − γ−1l+1)\n]2\n≤ K2µ,c (µn)2 n∑\ni=1\n[( i+ 2\ni\n)α\n+ 1\niα\nn−1∑\nl=i+1\nexp\n(\n−µ(l 1−α − i1−α) 1− α\n) ((l + 2)α − (l + 1)α) ]2\n≤ K2µ,c (µn)2\n \n\n⌊2α/µ⌋ ∑\ni=1\n[\n3α + 2 ( µ\n2α )α ( 1− α µc )2 exp\n(( 2α\nµ\n)1−α )]2\n+ n∑\n⌈2α/µ⌉\n[\n3α +\n( 2\nc\n)α]2  \n\nIn the second inequality we have used an Abel transform (see page 15 in [3], display (2.2), for details). For the third inequality we have substituted γi = cn−α, set Kµ,c = exp ( ∑ γj), and used a comparison with an integral. For the last inequality we have noted, as in page 15 in [3], that\n(A) :=\nn−1∑\nl=i+1\nexp\n(\n−µ l1−α−i1−α\n1− α\n)\n((l + 2)α − (l + 1)α)\n≤ exp ( µi1−α\n1− α\n)∫ n1−α\n(i+1)1−α exp\n( µl\n1− α\n)\nl 2α−1 1−α dl.\nNow, by taking the derivative and setting it to zero, we find that l 7→ exp (\nµl 1−α\n)\nl 2α 1−α is decreasing on\n[2α/µ,∞), and so we deduce that (A) ≤ (i + 1)α; this provides the bound on (A) when i ≥ 2α/µ. When i < 2α/µ we upper bound (A) by substituting worst case values for i in each term."
    } ],
    "references" : [ {
      "title" : "Non-asymptotic analysis of stochastic approximation algorithms for machine learning",
      "author" : [ "Francis Bach", "Eric Moulines" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "Varsha Dani", "Thomas P Hayes", "Sham M Kakade" ],
      "venue" : "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Transport-entropy inequalities and deviation estimates for stochastic approximation schemes",
      "author" : [ "Max Fathi", "Noufel Frikha" ],
      "venue" : "arXiv preprint arXiv:1301.7740,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Concentration Bounds for Stochastic Approximations",
      "author" : [ "Noufel Frikha", "Stphane Menozzi" ],
      "venue" : "Electron. Commun. Probab.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "Journal of Machine Learning Research-Proceedings Track,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization",
      "author" : [ "Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan" ],
      "venue" : "arXiv preprint arXiv:1109.5647,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Linearly parameterized bandits",
      "author" : [ "Paat Rusmevichientong", "John N. Tsitsiklis" ],
      "venue" : "Math. Oper. Res.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Online learning as stochastic approximation of regularization paths",
      "author" : [ "Pierre Tarrès", "Yuan Yao" ],
      "venue" : "arXiv preprint arXiv:1103.5538,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "As examples of a higher level learning algorithm using regression as a subroutine, we consider two linear bandit algorithms, the PEGE algorithm of [7] and the ConfidenceBall algorithm of [2].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 1,
      "context" : "As examples of a higher level learning algorithm using regression as a subroutine, we consider two linear bandit algorithms, the PEGE algorithm of [7] and the ConfidenceBall algorithm of [2].",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 6,
      "context" : "The PEGE algorithm of [7] is designed for action sets D satisfying a strong convexity property (see assumption (A4) in Section 3).",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "For the PEGE algorithm of [7] our result achieves an O(d) improvement in complexity, while incurring only a logarithmic deterioration in the regret.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "For the ConfidenceBall algorithm of [2] we again achieve a complexity improvement of O(d), while incurring an O(n1/5) deterioration in the regret.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "Non-asymptotic bounds in expectation for SGD schemes have been provided in [1].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "In the machine learning community, several algorithms have been proposed for minimizing the regret, for instance, [9, 5, 6] and these can be converted to find the minimizer of a (usually convex) function.",
      "startOffset" : 114,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : "In the machine learning community, several algorithms have been proposed for minimizing the regret, for instance, [9, 5, 6] and these can be converted to find the minimizer of a (usually convex) function.",
      "startOffset" : 114,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "A closely related field is stochastic approximation (SA) and concentration bounds for SA algorithms have been provided in [4].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "Adaptive regularisation in the context of least squares regression has been analysed in [8].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "Phased Exploration and Greedy Exploitation (PEGE) of [7] is a well-known algorithm in this setting.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : "1 Analysis We require the following extra assumptions from [7]: (A3’) A basis {b1, .",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "1 of [7]: Theorem 7.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "6 of [7]: Lemma 8.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "5 of [7].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : "1 of [7].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "The well-known confidence ball algorithm of [2] works by constructing a least squares estimate θ̂ using {(xi, yi)}i=1 around θ∗ and using this estimate to pick an arm greedily, i.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "The regret Rn with this algorithm was shown to be of the order O( √ dn lnn) in [2].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "In our setting, Theorem 4 serves as an analogue to Theorem 5 of [2].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "References [1] Francis Bach, Eric Moulines, et al.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] Varsha Dani, Thomas P Hayes, and Sham M Kakade.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Max Fathi and Noufel Frikha.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Noufel Frikha and Stphane Menozzi.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Elad Hazan and Satyen Kale.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Paat Rusmevichientong and John N.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Pierre Tarrès and Yuan Yao.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "1 Random Online Proof of Theorem 1 We use the proof technique from [4] in arriving at the high-probability bounds.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "The main ingredients of this derivation can be found in the argument of page 15 in [3], however here we manage to give all the constants explicitly.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "In the second inequality we have used an Abel transform (see page 15 in [3], display (2.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "For the last inequality we have noted, as in page 15 in [3], that",
      "startOffset" : 56,
      "endOffset" : 59
    } ],
    "year" : 2017,
    "abstractText" : "We improve the computational complexity of online learning algorithms that require to often recompute least squares regression estimates of parameters. We propose two stochastic gradient descent schemes with randomisation in order to efficiently track the true solutions of the regression problems achieving an O(d) improvement in complexity, where d is the dimension of the data. The first algorithm assumes strong convexity in the regression problem, and we provide bounds on the error both in expectation and high probability (the latter is often needed to provide theoretical guarantees for higher level algorithms). The second algorithm deals with cases where strong convexity of the regression problem cannot be guaranteed and uses adaptive regularisation. We again give error bounds in both expectation and high probability. We apply our approaches to the linear bandit algorithms PEGE and ConfidenceBall and demonstrate significant gains in complexity in both cases. Since strong convexity is guaranteed by the PEGE algorithm, we lose only logarithmic factors in the regret performance of the algorithm. On the other hand, in the ConfidenceBall algorithm we adaptively regularise to ensure strong convexity, and this results in an Õ(n1/5)1 deterioration of the regret.",
    "creator" : "LaTeX with hyperref package"
  }
}