{
  "name" : "1705.07404.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CrossNets : A New Approach to Complex Learning",
    "authors" : [ "Chirag Agarwal", "Mehdi Sharifzhadeh", "Dan Schonfeld" ],
    "emails" : [ "cagarw2@uic.edu,", "mshari5@uic.edu,", "dans@uic.edu", "klobuj@rpi.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Cross-Connections, Directed Acyclic Graphs (DAG), Image classification, Convergence\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "N EURAL networks have recently enjoyed an accelerationin popularity, with new research adding to several decades of foundational work. From multilayer perceptron (MLP) networks to the more prominent recurrent neural networks (RNN) and convolutional neural networks (CNN), neural networks have become a dominant force in the fields of computer vision, speech recognition, and machine translation [2]. Increase in computational speed and data have legitimized the training of deep networks. Each type of network consists of several layers of neurons with activation units, usually interconnected in a feed-forward fashion. However, Arbitrary Networks (AC) networks have been shown to train and perform better than the commonly used feed-forward architectures [3], [4]. Neural networks with lateral connections (another special case of AC networks) between hidden layer neurons are also found to be more efficient than MLP networks [5], [6]. Currently, a wide array of problems such as herd effect [7], vanishing gradients [8], [9], diminishing feature reuse, and ”fooling images” [10] have emerged in regards to deep networks. Although most of the works in neural networks have been restricted to a strictly feed-forward network, recent works like Deep Residual Networks (ResNet) [11], Highway Networks [12], FractalNets [13], and DenseNets [1]* broke the general feedforward architecture to use ”short connections” to connect nonconsecutive layers.\nAll feed-forward architectures for neural networks take the form of directed acyclic graphs (DAG). In this paper, we present a theoretical understanding of the proposed network with cross-connections by considering convergence over DAGs for backpropagation with momentum. This is\n• C. Agarwal, M. Sharifzadeh, and D. Schonfeld are with the Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, 60607. E-mail: cagarw2@uic.edu, mshari5@uic.edu, and dans@uic.edu • J. Klobusicky is with the Department of Mathematical Sciences, Rensselaer Polytechnic Institute, Troy, NY, 12180. E-mail: klobuj@rpi.edu\n*This paper was developed independently. Huang et al. [1] performed experiments independently on this subject similar to ours.\nfirst shown with a toy example containing two single-node hidden layers with one ”cross connection” between them. We then consider convergence in the general case of a DAG, which can always be decomposed into ordered layers.\nThe purpose for adopting cross connections is that they enable efficient reuse of features throughout the network. This aids in better training of the network. We believe that deep neural networks tend to learn more low and middle-level features rather than the global structure of the respective images. Cross connections in a feed-forward network, however, consider global features in determining output, leading to efficient and robust learning. We provide evidence for the claim by evaluating the efficiency of our proposed architecture on four competitive datasets (MNIST, CIFAR-10, CIFAR-100, and SVHN). We show improved performance of cross connected networks compared to various other proposed architectures."
    }, {
      "heading" : "1.1 Prior and Related Work",
      "text" : "Increasingly complex versions of DAG architectures have been explored in previous works. Kothari and Agyepong [5], for instance, introduced simple lateral connections in the form of a chain, where each unit in a hidden layer is connected to the next. In one example, by adding these lateral connections, a small network with one hidden layer of 12 units approximated a complex function more accurately. The problem of vanishing gradients were addressed in [14], [15] by connecting some hidden layers directly to the classifier layer. ResNets incorporated additional ”short connection” paths, and thus increased the connections within a given network. ResNets are built from dense blocks and pooling operations, and they combine features through summation before passing them to subsequent layers. Similar to our work, DenseNets [1]* recently connected all layers (with matching feature-map sizes) directly with each other resulting in feature reuse, all layers can easily access their preceding layers making it easy to reuse the information from previously computed feature maps. The architectures ar X\niv :1\n70 5.\n07 40\n4v 1\n[ cs\n.C V\n] 2\n1 M\nay 2\n01 7\n2 of all these examples may be seen as special cases of a DAG, and thus our general proof of convergence, under our boundedness assumptions for hidden weights and activations functions, is applicable to all of these models.\nMany different architectures like Maxout Networks [16], Network In Network (NIN) [17], Deeply Supervised Networks (DSN) [14], and FractalNets are being proposed which give competitive results on the above mentioned datasets. Maxout Networks are simple feed-forward network (a traditional MLP or a deep convolutional neural network), using a maxout activation function which returns the maximum of a set of inputs. NIN uses micro-neural networks to abstract data within a receptive field. These micro-networks (multilayer perceptrons) aid in extracting more complex features. DSN introduced the idea that the supervision of layers should not be constrained to just the ouput layer but should be propagated to the early hidden layers as well. FractalNets proposed combining several sequences of a number of convolutional blocks. They obtained a large depth in their network model, and in order to prevent co-adaptation of parallel paths, they introduced Drop-path."
    }, {
      "heading" : "1.2 Contribution",
      "text" : "In this paper, we propose an architecture that is a combination of the traditional feed-forward network, along with higher order cross layer connections among the neurons in different layers. This enables the global features learned in the early layers to be preserved in the later layers. We concatenate feature maps from early layers by using pooling layers between them. One of the problems mentioned by [10] was that Deep Neural Networks (DNN) tend to learn low and middle-level features rather than the global structure of objects due to which the network was easily fooled on showing only subcomponents of an object. Using our architecture, the final classification decision incorporates all low, middle, and global level features. This leads to higher robustness of our architecture. Another aim of this paper is to generalize the results in [11] to a more general case, that is, cross-connected neural networks. Besides the experimental results, we show the convergence of our proposed network using back-propagation with a momentum term. We show the convergence of the proposed neural network which has every layer connected with all subsequent layer\nAnother advantage of our architecture is seen in terms of training accuracy and loss. Our experiments show that any architecture (MLP or CNN) with any number of layers and neurons is trained better(faster training and lower training loss) using cross connections. We believe this is due to sharing of information between different layers leading to efficient learning. The contributions of this paper can be summarized as follows:\n• We present a unified theory with a proof of optimal updates for our proposed network, whereby all of the prior work forms various special cases of the unified theory.\n• We show that the optimal architecture of crossconnected neural networks perform no worse, and often better, than other feed forward architectures through a series of experiments. Our network can"
    }, {
      "heading" : "2 ARCHITECTURE: FEED-FORWARD NETWORK WITH CROSS-LAYER CONNECTIVITY",
      "text" : ""
    }, {
      "heading" : "2.1 Convergence under a single lateral weight",
      "text" : "For a simplified example, we demonstrate convergence to a local minimum working with the architecture type shown in Figure 1. The proof is similar to [18]. Nodes consist of n input nodes, two hidden nodes, and a single output node. Weight values consist of a matrix V = (vi,j)n×2 of weights from inputs to hidden nodes, a vector (w1, w2) of weights from the hidden layer to the output, and a lateral weight z from the first hidden node to the second. The addition of weight z, in fact, separates the second hidden node as an additional hidden layer for the network.\nLet g : R→ R denote an activation function. For an initial input x = (x1, . . . , xn), the input values of the are\ns1 = ∑ j≤n xjvj,1 = x · v1, (1)\ns2 = zh1 + ∑ j≤n xjvj,2 = zh1 + x · v2, (2)\nsy = w1h1 + w2h2 = w · h. (3)\nOutput values are then\nh1 = g(s1), h2 = g(s2), y = g(sy) (4)\nOver a collection of observations x1, . . . , xJ ∈ Rn, with corresponding desired output di and actual output of yi as determined by the architecture above, we will use the total quadratic error\nE = J∑ j=1 (dj − yj)2/2 := J∑ j=1 gj(yj). (5)\n3\nFor the proof, we will need gradients of the error with respects to weights. They are\n∂E ∂w = J∑ j=1 g′j(s j y) · h := p, (6)\n∂E ∂z = J∑ j=1 g′j(s j y)w2g ′(sj2)h1 := r, (7)\n∂E ∂v2 = J∑ j=1 g′j(s j y)w2g ′(sj2)x j := q2, (8)\n∂E ∂v1 = J∑ j=1 g′j(s j y)(w1g ′(sj1)x j + w2g ′(sj2)zg ′(sj1)x j) := q1.\n(9)\nThe iteration of weights is done through gradient descent with momentum. Here and in the future, a superscript k is used as an iteration variable, and ∆xk = xk − xk−1 for any quantity x. For η ∈ (0, 1), weights will be updated according to,\n∆wk+1 = τk∆w k − ηpk, ∆zk+1 = βk∆zk − ηrk,\n(10)\n∆vk+11 = γk,1∆v k 1 − ηqk1 , ∆vk+12 = γk,2∆vk2 − ηqk2 .\n(11)\nHere, for a fixed τ ∈ (0, 1), and the Euclidean metric ‖ · ‖, adaptive momentum is selected to normalize with respect to iteration differences in weights, given by\nτk =\n{ τ‖pk‖ ‖∆wk‖ ‖∆w\nk‖ 6= 0, 0 otherwise, βk =\n{ τ‖rk‖ ‖∆zk‖ ‖∆z\nk‖ 6= 0, 0 otherwise,\n(12)\nγk,i =\n{ τ‖qki ‖ ‖∆vki ‖\n‖∆vki ‖ 6= 0, 0 otherwise, i = 1, 2. (13)\nThis choice of momentum was also used in [19]. Our major theorem is then a statement of convergence under this iteration method, the proof of which is provided\nin Appendix A.1. This proof will require some regularity and boundedness assumptions [18]\nAssumptions 1.\n1) The function g, and its first two derivatives g′ and g′′ are bounded in R. 2) Hidden weights wk and rk are uniformly bounded over all iterations k = 1, 2, . . . . 3) The gradient ∇E(v1, v2, z, w) vanishes only at a finite set of points.\nThis set of assumptions is equivalent to that used in [18], and may also be found in other nonlinear optimization problems such as [20].\nTheorem 1. Under assumptions (1) and (2), for any s ∈ (0, 1) and τ = sη,∃C > 0 such that if\nη < 1− s\nC(s2 + 1) , (14)\nthen\nEk = E(vk1 , v k 2 , z k, wk)→ E∗, (15) qk1 → 0, qk2 → 0, rk → 0, pk → 0. (16)\nIf assumption (3) is also satisfied, then weights (vk1 , v k 2 , z k, wk)→ (v∗1 , v ∗ 2 , z ∗, w∗), and E∗ = E(v∗1 , v ∗ 2 , z ∗, w∗) is a local minimum."
    }, {
      "heading" : "2.2 DAG architecture and statement of theorem",
      "text" : "We may generalize the proof for our simple case to show the convergence of the gradient method with momentum for any multilayer neural network with an architecture defined by a DAG. Nodes of a DAG can always be ordered into layers 0, . . . , L, in which directed edges always point to larger layers. Layer 0, having l0 nodes, accepts the input training values xp ∈ Rl0 , over p = 1, . . . , J . For each layer i from 1 through L − 1, there have li nodes, and layer L contains a single output node. Under this ordering, define vl,m(i,j) as the weight between node l in layer i and node m in layer j, where i < j. Let v(i,j) denote the matrix of weights\n4 from layer i to j. Over all nodes, we use a single activation function g : R→ R for the determination of output values.\nThe explicit output values of node i in layer j, Hij , and final output HL = y, are defined recursively as\nH0 = x, H1 = g ( H0v(0,1) ) , (17)\nHj = g ∑ i<j Hiv(i,j)  , y = g(∑ i<L Hiv(i,L) ) . (18)\nNode inputs are defined as Sj = ∑ i<j Hiv(i,j). (19)\nError is again given by (5). Gradients for weights are then defined as\n∂E\n∂vl,m(i,j) = ql,m(i,j). (20)\nAs in the last section, a superscript xk for any variable x (now sometimes placed after a semicolon to distinguish between node indices) denotes its value after the kth iteration. Iteration through gradient descent with momentum is defined as\n∆vm,l;k+1(i,j) = τ m,l;k (i,j) ∆v m,l;k (i,j) − ηq m,l;k (i,j) , (21)\nwhere for a predetermined τ ∈ (0, 1), the adaptive momentum\nτm,l;k(i,j) =  τ‖qm,l;k (i,j) ‖ ‖∆vm,l;k (i,j) ‖ ‖∆vm;k(i,j)‖ 6= 0,\n0 otherwise. (22)\nThe norm ‖v‖ denotes the usual Euclidean norm for a vector v. When the norm acts on a matrix A = (ai,j)n×m, it is treated as a length nm vector, with ‖A‖2 = ∑ i,j a 2 i,j .\nOur major theorem is then a statement of convergence under this iteration method. This will require some regularity and boundedness assumptions similar to Assumptions 1:\nAssumptions 2.\n1) The function g, and its first two derivatives g′ and g′′ are bounded in R. 2) The weights vk(i,j) are uniformly bounded over layers 1 ≤ i < j and iterations k = 1, 2, . . . . 3) The gradient ∇E vanishes only at a finite set of points. It readily follows from these two assumptions that we\nmay also uniformly bound qk(i,j), H k i , gp, g ′ p, and g ′′ p . Note that, similar to the two node case, we do not assume boundedness of weights connected to the input layer\nTheorem 2. Under assumptions (1) and (2), for any s ∈ (0, 1) and τ = sη, there exists C > 0 such that if\nη < 1− s\nC(s2 + 1) , (23)\nthen\nEk = E(vk(i,j))→ E∗ 1 ≤ i < j ≤ L, (24) qk1 → 0, qk2 → 0, rk → 0, pk → 0. (25)\nIf assumption (3) is satisfied, weights vk(i,j) → v∗(i,j), and E∗ = E(v∗(i,j)) is a local minimum.\nA complete proof for Theorems 1 and 2 are provided in Appendix A."
    }, {
      "heading" : "2.3 CrossNets Implementation Details:",
      "text" : "From equation (18), each hidden layer is recursively computed using the activated outputs from all previous layers neurons plus the weighted sum of the input from the input layer. The outputs of all the previous layers are stacked together and then are fed as input to the subsequent layer.\nTraditional convolutional neural networks connect the output feature map of jth convolutional layer to the input of (j + 1)th convolutional layer. For a convolutional network version of CrossNets, we connect the output feature map of the jth convolutional layer to all inputs of the ith convolutional layers, for all i > j. These cross connections between non-adjacent layers leads to Cross Network, or CrossNet, architecture. Rather than using simple convolutional layers, we used a combination of three consecutive operations: Batch Normalization, rectified linear unit (ReLU), and finally a 3×3 convolutional layer. Pooling layers are also used to reduce the dimensions within the network. We followed the same configuration in our experiments as [21] and define each stack of these three layers as a composite unit. We use a 2× 2 maxpooling layer after each N composite units. Implementation details specific to individual experiments are provided in Appendix B."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : "We evaluate the effectuality of our proposed architecture by implementing the cross-layer connections on various feed-forward and convolutional neural networks. The main purpose of the following experiments was not to design the architecture which gives state-of-the-art performance for the different datasets, but to introduce the idea of using cross connections to both multi layer perceptron and convolutional network. We compare the result of crossconnected architectures with their feed-forward counterparts, and also with other previously proposed architectures."
    }, {
      "heading" : "3.1 Datasets",
      "text" : ""
    }, {
      "heading" : "3.1.1 MNIST",
      "text" : "We performed experiments on hand-written digit classification problem [22] where input patterns are digits. The samples are 28x28 binary pixel images. The train and test sets contain 60,000 and 10,000 images respectively, and we hold out 10,000 training images as a validation set. The task is to classify the images into 10 digit classes. Table 1 compares the performance of adding cross-connections to different MLP and convolutional architectures. The best performing neural networks for the permutation invariant setting achieved an error of about 1.60% [23]. With the addition of two cross-connections, one between the input and later hidden layer and the second between the early hidden layer and the output, we achieve an error of about 1.26% . A neural net (3 layers and 1024 neurons without max-constraints) with cross connections achieves an error of about 1.10% as compared to a 1.25% error for a traditional feed forward network with same architecture using dropouts [24]. A 2-Convolutional layer network gave an error rate of 1.41%, whereas a CrossNet version of the same achieved a large improvement by reducing the error to 1.15%. One thing we noticed during these experiments was that having a\n5 0 50 100 150 200 250 300 350 400\nEpochs\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nC ro ss E n tr o p y L o ss\nTraining Loss vs Epoch | 2-Layer 25 Nodes each\nCrossNet MLP Traditional MLP\n0 50 100 150 200 250 300 350 400\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nC ro ss E n tr o p y L o ss\nTraining Loss vs Epoch | 2-Layer 50 Nodes each\nCrossNet MLP Traditional MLP\n(a) (b)\n0 50 100 150 200 250 300 350 400\nEpochs\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nC ro\nss E\nn tr\no p y L\no ss\nTraining Loss vs Epoch | 2-Layer 800 Nodes each\nCrossNet MLP Traditional MLP\n0 50 100 150 200 250 300\nEpochs\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nC ro\nss E\nn tr\no p y L\no ss\nTraining Loss vs Epoch | 3-Layer 1024 Nodes each\nCrossNet MLP Traditional MLP\n(c) (d)\nFig. 3. Comparison of the cross entropy training loss between various architectures and their CrossNet counterparts. The different architectures experimented with are (a) 2-layer-25-neurons, (b) 2-layer-50-neurons, (c) 2-layer-800-neurons, and (d) 3-layer-1024-neurons.\ncomplete architecture as CrossNets reduces overfitting. More experimental details on the architecture and training can be found in Appendix B.1\nTable 2 illustrates a comprehensive result on the MNIST dataset without data augmentation. Training such big networks with MNIST generally gives generalization error. An error rate of 0.53% was achieved in [25] using a 2- layer CNN+2-layer NN configuration (0.11M trainable parameters). Network in Network [17] surpassed this result by achieving an error of 0.47%. By using maxout activation function [16] reduced the error to 0.45%. As per our knowledge the current state-of-the-art result for MNIST classification without data augmentation is 0.39% achieved by [14]. We used a 10-layer(0.12M trainable parameters) CrossNet (details in Appendix B.1) to reach an error rate of 0.36%.\nThese experiments were mainly to demonstrate the effectiveness of our proposed architecture as compared to its feed-forward couterpart. We have not exhaustively explored other commonly used tricks such as weight constraints, hidden unit sparsity, and adding noise. All the experiments on this dataset were performed without data augmentation. In Figure 3 we see the superior performance of CrossNets MLP architectures as compared to a feed-forward MLP architecture. We see considerably lower training error of\nCrossNets as compared to their feed-forward MLP versions. We also observe a large reduction in terms of misclassification rate when the networks are shallow, and similar nature is observed for deep networks. We also observe that the performance of the CrossNets when initialized randomly give a relatively uniform result compared to MLP networks.\nCross Weights: As is observed in Table 1, the misclas-\n6\nsification rate is lower in CrossNets as compared to the feed-forward networks. One of the main reasons for this is better information flow throughout the architecture, and also that the final classifier takes into account the global features along with the local features. In Figure 4, we visualize the CrossWeights learned while training various architectures with the MNIST data. Figure 4 (a) and (b) shows some of the weights learned in the second and third hidden layer of the 3-layer-1024-neurons network when the input layer is cross connected to them respectively. We also visualized the cross connections in the 2-layer-800 neurons network. In Figure 4 (c), we see the learned cross weights between the input layer and the second hidden layer. It is clearly seen that cross weights reflect actual digit classes to a great extent. We also see the weights learned are more effective in the shallower structure as compared to the larger structure (1024 neurons). There is noise in some of the visualized weights in the 3-layer-1024-neurons structure. We believe this is due to the fact that MNIST is a relatively easy dataset consisting of simple handwritten digits, hence not requiring a very complex network."
    }, {
      "heading" : "3.1.2 CIFAR-10 and CIFAR-100",
      "text" : "The CIFAR datasets [26] consist of 32x32 RGB color images taken from various classes. CIFAR-10 and CIFAR-100 comprise of images taken from 10 and 100 classes respectively. Both the dataset consist of 60,000 images split into 50,000 training and 10,000 testing images. Of the 50,000 training images, 5,000 were kept for cross-validation to optimize hyperparameters. As we see, each class has 6000 and 600 images in CIFAR-10 and CIFAR-100 datasets respectively. The input images were normalized and experiments were performed with and without augmentation. A more detailed description of the experiment setup and architectures can be found in Appendix B.2.\nTable 3 shows the classification error of different methods on the CIFAR-10 and CIFAR-100 datasets. We see that CrossNets performance is superior to most of the previously proposed architectures. It is clearly observed that CrossNets v2 (64 layers) (6.59%) outperforms both ResNet (13.63%) and Stochastic depth (11.66%) [27] without any data augmentation by a large margin for CIFAR-10 dataset. A similar performance was observed for CIFAR-100 dataset where CrossNets v2 (28.10%) outperformed both ResNets(44.76%) and Stochastic depth (37.80%). With data augmentation we see similar performances for the above architectures. Results from other notable methods are also mentioned in Table 3.\nThe three different designs of CrossNets are explained in Appendix B.2. CrossNet v3 outperforms the earlier versions. The primary difference between CrossNet v3 with the prior versios are that it preserves the global structure more. Due to computation limitation we were not able to test with larger architectures, and hence, we experimented with relatively shallow CrossNets to showcase the effectiveness of the architecture. It is to be noted that networks like FractalNets use 22.9M and 38.6M trainable parameters for achieving low misclassification rates, whereas CrossNets only use 1.2M, 4.9M, and 2.7M trainable parameters for achieving the results stated in Table 3.\n7"
    }, {
      "heading" : "3.1.3 Street View House Numbers",
      "text" : "The Street View House Numbers (SVHN) dataset [32] consists of 3232 color RGB images. The dataset comes with 73,257 samples for training, 26,032 samples for testing, and an extra 531,131 samples for training. It comprises of 10 digit classes from 0-9. We take out 6000 samples (600 from each class - 400 from the training set and 200 from the extra training set) and used it as validation dataset. Preprocessing was done on the samples in terms of global contrast normalization. As in the CIFAR dataset, we chose the best hyperparameters by monitoring the validation performance. The performance of DropConnect was a result of their voting scheme. ResNets and FractalNets comparatively did not perform well with SVHN dataset, reaching a 2.01% and 1.87% error rate respectively. We trained the same three different versions of CrossNets as used for CIFAR datasets. CrossNet v1 (32 layers, 1.2M parameters) did not perform well and achieved an error rate of 2.92%. CrossNet v2 (64 layers, 4.9M parameters) performed better than CrossNet v1 surpassing most of the previous state-of-the-art results by reducing the error to 1.85%. CrossNet v3 (48 layers, 2.7M parameters) achieved an error rate of 1.71%. Table 4 shows recent comparable results using no data augmentation. Details on experimental setup are found in Appendix B.3."
    }, {
      "heading" : "4 DISCUSSION",
      "text" : "CrossNets give rise to larger networks using fewer neurons (parameters and connections). This was evident from the MNIST experiments, as performance of a cross connected 2-layer-25 neurons(40k trainable parameters) was superior to that of a feed-forward 2-layer-50 neurons network (42k trainable parameters). Similar trend was observed in case of a larger network. Performance of a cross connected 2-layer, 800 neuron network(1.9M trainable parameters) was equivalent to that of a 3-layer, 1024 neuron feed-forward network(2.9M\ntrainable parameters). CrossNets enable cross information flow between non-adjacent layers enabling more efficient learning. Figure 5 illustrates the misclassification rate of CIFAR-10 dataset without data augmentation for different state-of-the-art architectures along with their respective number of trainable parameters. It is clearly observed that CrossNet v1(1.2M) performs better than ResNet(1.7M) and Stochastic Depth(1.7M) using less number of parameters. FractalNets(38.6M) use large number of training parameters and performs better than both ResNets and Stochastic Depths. The performance difference between CrossNet v1 and CrossNet v2 validates the fact that increasing the size of CrossNets leads to better performance.\nThe motivation for CrossNets comes from completely connected Directed Acyclic Graphs. Feed-forward networks like MLP and convolutional neural networks come under the category of layered networks, which we believe limit the learning capabilities of neurons. By cross connecting a layer to all its subsequent layers this learning barrier can be overcome. The three different versions of CrossNets(v1, v2, and v3) presented in this paper are just specific cases of the proposed architecture. More experiments using bigger network size would substantiate the superior performance of CrossNets.\nIn [3], [4] it was proposed that Arbitrary Connected (AC) networks are much more powerful than commonly used feed-forward networks. With CrossNets these arbitrary connections between any two layers are achieved. We believe this is one of the reasons for the improved accuracy of CrossNets over other networks.\nAs seen in Figure 4, the visualization of weights show that features are reused in the subsequent layers due to the cross connections. We used a learning rate scheduler for training all CrossNets. It was designed in such a way that the learning rate was small for major part of the training process. This was done so as not to saturate any neuron by the cross\n8\nconnection weight of previous neurons.\nThe difference between CrossNets version 1 and 2, with CrossNets version 3 was that the latter used three pooling layers. We observed that CrossNet v3 performed better with respect to the other two versions. We believe that more number of pooling layers gives rise to smaller feature maps, which was not suitable for the experimented datasets. Hence, validating the point that deep neural networks will give superior performance when they take into account global structure of the images too. Using fewer pooling layers helped us preserve the global structure of the feature maps.\nCrossNets are only one specific case of a completely DAG. We believe that instead of hard coding these connections (either cross or feed-forward connections), some criteria should be designed which would allow the breaking or making of these connections between different layers. This would give rise to adaptive neural network architectures."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper we introduced a feed-forward network architecture with cross layer connections. The network was trained using the traditional back-propagation algorithm. However, the back-propagation algorithm is slower than more advanced second-order algorithms [33], [34]. Training of our proposed network architecture using these secondorder algorithms would make it faster. The cross layer connections enable the neurons to learn more efficiently. This was validated in the performance results for MNIST, CIFAR-10, CIFAR-100, and SVHN datasets. CrossNets can be considered as more of a complete connected network. The question still remains as to whether we should cross connect all layers, or cross connect only some specific layers. We plan to explore this question in our future work. As mentioned in Section 4, this would give rise to adaptive neural networks which can make decisions pertaining to making or breaking the cross connections.\n9"
    }, {
      "heading" : "APPENDIX A CONVERGENCE PROOF",
      "text" : "A.1 Estimates on two hidden nodes\nThe main tool used in the proof of Theorem 1 is Taylor’s theorem. For sufficiently small learning rates, we show that the difference iteration in error produces a negative term a sum of error gradient terms from the first order Taylor approximation which can be controlled by a sufficiently small learning rate, and also quadratic terms. Specifically,\nLemma 1. There exists a constant C > 0 such that for all k = 1, 2, . . . ,\n∆Ek+1 ≤ (τ − η)(‖pk‖2 + ‖qk2‖2 + |rk|2 + ‖qk1‖2) (26) + C(η2 + τ2)(‖p‖2 + ‖qk1‖2 + ‖qk2‖2 + |rk|2). (27)\nLater, we’ll show that this type of decomposition of error is not specific to this specific example, but in fact holds in for all DAG architectures.\nTo arrive at Lemma 1, we first look at the error difference for a single initial input. This is done with Taylor’s theorem by expanding sj,k+1y about s j,k y , which yields\ngj(s j,k+1 y ) = gj(s j,k y ) + g ′ j(s j,k y )(∆s j,k+1 y ) +\n1 2 g′′j (t k,j)(∆sj,k+1y ) 2,\n(28)\nwhere tk,j lies between sj,ky and s j,k+1 y . To handle the linear terms, note that\n∆sj,k+1y = h j,k ·∆wk+1+∆(hj,k+1)·wk+∆(hj,k+1)·∆wk+1 (29) From (6) ,(10), and (29), we sum over all inputs to find\nJ∑ j=1 g′j(s j,k y )(∆s j,k+1 y ) = J∑ j=1 g′j(s j,k y )h j,k · (τk∆wk − ηpk)\n(30)\n+ J∑ j=1 g′j(s j,k y )∆h j,k+1 · wk+1 + ∆(hj,k+1) ·∆wk+1 (31) = −η‖pk‖2 + τkpk ·∆wk + J∑ j=1 g′j(s j,k y )∆h j,k+1 · wk+1\n(32)\n+ ∆(hj,k+1) ·∆wk+1 (33)\nThus, using (5), (29), and (30), we may write the iteration of error as\n∆Ek+1 =− η‖pk‖2 + τkpk ·∆wk + J∑ j=1 g′j(s j,k y )∆h j,k+1 · wk\n(34)\n+ J∑ j=1 1 2 g′′j (t k,j)(∆sj,k+1y ) 2 + ∆(hj,k+1) ·∆wk+1.\n(35)\nOur next estimates are for ∆hj,k+1. The inequalities that will arise frequently rely on Assumptions 1 and 2. Also, we will use a universal constant C > 0 that will hold for all inequalities shown. We again use Taylor’s theorem with\nregards to the first hidden node: for t̂1,j,k between s k+1,j 1 and sk,j1 ,\n∆hj,k+11 = g ′(sj,k1 )∆s j,k+1 1 +\n1 2 g′′(t̂1,j,k)(∆s j,k+1 1 ) 2 (36)\nFrom (1), (10), and Assumption (1),\n∆hj,k+11 ≤ g′(s j,k 1 )(γk,1∆v k 1 − ηqk1 )xj + C(∆s j,k+1 1 ) 2. (37)\nA similar calculation follows for the second hidden node. From (1) and another Taylor expansion, for t̂2,j,k between sk+1,j2 and s k,j 2 ,\n∆hj,k+12 = g ′(sj,k2 )∆s j,k+1 2 +\n1 2 g′′(t̂2,j,k)(∆s j,k+1 2 ) 2 (38)\n= g′(sj,k2 ) ( ∆zk+1(hj,k1 ) + z k∆hj,k+11 (39)\n+ ∆zk+1∆hj,k+11 + ∆v k+1 2 · xj\n) (40)\n+ 1\n2 g′′(t̂2,j,k)(∆h j,k+1 2 ) 2. (41)\nFor the first term in (40), we substitute with (36), and once more use (1) to obtain\n∆hj,k+12 = g ′(sj,k2 ) ( ∆zk+1(hj,k1 ) (42)\n+ zkg′(sj,k1 )∆v k+1 1 · xj +\n1 2 zkg′′(t̂1,j,k)(∆s j,k+1 1 ) 2 (43)\n+ ∆zk+1∆hj,k+11 + ∆v k+1 2 · xj\n) + 1\n2 g′′(t̂2,j,k)(∆h j,k+1 2 ) 2.\n(44)\nFinally, from the Assumptions, ∆hj,k+12 ≤ g′(s j,k 2 ) ( ∆zk+1(hj,k1 ) (45)\n+ zkg′(sj,k1 )∆v k+1 1 · xj + ∆v k+1 2 · xj\n) (46)\n+ C((∆sj,k+11 ) 2 + (∆zk+1)2 + (∆hj,k+11 ) 2 + (∆hj,k+12 ) 2)\n(47)\nUsing (37) and (47), along with the explicit gradient expressions (7-9), we sum over observations in (34) to obtain\nJ∑ j=1 g′j(s j,k y )∆h j,k+1 · wk (48)\n≤ qk2 ·∆vk+12 + qk1 ·∆v k+1 1 + r k∆zk+1 (49) + C((∆sj,k+11 ) 2 + (∆zk+1)2 + (∆hj,k+11 )\n2 + (∆hj,k+12 ) 2). (50)\nFrom (10) and (12),\nJ∑ j=1 g′j(s j,k y )∆h j,k+1 · wk (51)\n≤ (−η + τ)(‖q1‖2 + |rk|2 + ‖qk2‖2) (52) + C((∆sj,k+11 ) 2 + (∆zk+1)2 + (∆hj,k+11 ) 2 + (∆hj,k+12 )\n2) (53)\nIt remains to estimate the quadratic terms in (53). This is contained in\nLemma 2. The following inequalities hold:\n10\n1)\n|(∆zk+1)2 +(∆vk+11 )2| ≤ C(η2 + τ2)(‖qk1‖2 + |rk|2) (54)\n2)\n‖∆hj,k+1‖2 ≤ C(η2 + τ2)(‖qk1‖2 + ‖qk2‖2 + |rk|2) (55)\n3) ∣∣∣(∆sj,k+11 )2∣∣∣ ≤ C(η2 + τ2)‖qk1‖2 (56) Proof. First, (1) and (3) follow directly from (10) and (12). To show (2), note that from (36), and Assumption 1,\n|∆hj,k+11 | ≤ |g′(vk1 · ξj)∆v k+1 1 · xj | (57)\n+ |1 2 g′′(t̂1,j,k)(∆v k+1 1 · xj)2| (58) ≤ C((η + τ)‖qk1‖+ (τ2 + η2)‖qk1‖2) (59) ≤ C(η + τ + τ2 + η2)‖qk1‖. (60)\nThe last inequality uses the fact that qi are uniformly bounded, which can be derived from the Assumptions. Since η, τ < 1,\n|∆hj,k+11 |2 ≤ C(τ2 + η2)‖qk1‖2 (61)\nFor the second node, by Taylor’s theorem\n|∆hj,k+12 | ≤ |g′(s j,k 2 )∆s k+1,j 2 |+ |\n1 2 g′′(t̂2,j,k)(∆s j,k+1 2 )\n2| (62)\nAgain, from the Assumptions,∣∣∣∆sj,k+12 ∣∣∣ = ∣∣∣∆(zk+1g(sj,k+11 )) + ξj ·∆vk+12 ∣∣∣ (63) ≤ C(|∆zk+1|+ ‖∆vk+12 ‖). (64)\nSince\n|∆zk+1| ≤ C(η + τ)|rk|, |∆vk+12 | ≤ C(η + τ)‖qk2‖, (65) we may write,∣∣∣∆sj,k+12 ∣∣∣ ≤ C(η + τ)(|rk|+ ‖qk2‖). (66) Thus, from (62)∣∣∣(∆hj,k+12 )2∣∣∣ ≤ C(η2 + τ2)(|rk|2 + ‖qk2‖2). (67) Combining (61) and (67) then shows part (2).\nA bound for the remaining terms in (34) are provided in\nLemma 3. The following bounds hold:\n1)\n|τkpk ·∆wk| ≤ τ‖pk‖. (68)\n2)\n‖∆wk+1 ·∆hj,k+1‖2 ≤ (69) C(η2 + τ2)(‖pk‖+ ‖qk1‖2 + ‖qk2‖2 + |rk|2). (70)\n3)\n| J∑ j=1 g′′j (tk,j)(∆s j,k+1 y ) 2| (71)\n≤ C(η2 + τ2)(‖pk‖2 + ‖qk1‖2 + ‖qk2‖2 + |rk|2). (72)\nProof. To show (1), we can apply (10) and (12). For part (2), we also use (10) and (12), along with (55), giving\n‖∆wk+1∆hj,k+1‖ ≤ C(‖∆wk+1‖2 + ‖∆hj,k+1‖2) (73) ≤ C(η2 + τ2)(‖pk‖2 + ‖qk1‖2 + ‖qk2‖2 + |rk|2) (74)\nFor (3), recall\n|∆sj,k+1y | = |hj,k ·∆wk+1|+ |∆(hj,k+1) · wk+1| (75)\nFrom Assumption 1, it follows that hj,k is uniformly bounded, so that\n|hj,k ·∆wk+1| ≤ C‖∆wk+1‖ ≤ C(η + τ)‖pk‖. (76)\nFor the second term in (75), notice from Lemma 2 that\n|∆(hj,k+1) · wk+1|2 ≤ C‖∆(hj,k+1)‖2 (77) ≤ C(η2 + τ2)(‖qk1‖2 + ‖qk2‖2 + |rk|2). (78)\nCombining (76) and (77) then give the result.\nProof of Lemma 1. From (34), we may bound the first two terms using part (1) of Lemma (3), the third term with (51) and Lemma 2, the fourth term using part (3) of Lemma 3 part , and the final terms using part 2 of Lemma 3 part along with the identity |x · y| ≤ C(‖x‖2 + ‖y‖2).\nA.2 Proof of convergence\nWe now use the a Lemma found in [35]:\nLemma 4. Let f ∈ C1(Rn,R), and suppose that ∇f vanishes at a finite set of points. Then, for a sequence {xk} if ‖∆xk‖ → 0 and ‖∇f(xk)‖ → 0, then for some x∗ ∈ Rn, xk → x∗ and ∇f(x∗) = 0.\nThrough Lemmas 2 and 3, and (51), we can now bound the iteration of error by\n∆Ek+1 ≤ (τ − η)(‖pk‖2 + ‖qk2‖2 + |rk|2 + ‖qk1‖2) (79) + C(η2 + τ2)(‖p‖2 + ‖qk1‖2 + ‖qk2‖2 + |rk|2) (80) = (−η + τ + C(η2 + τ2))(‖p‖2 + ‖qk1‖2 + ‖qk2‖2 + |rk|2)\n(81)\nAssuming η = sτ , it is straightforward to show that the term in front of the norms are negative when\nη < 1− s\nC(s2 + 1) . (82)\nUnder this constraint, Ek is decreasing under each iteration. The summability for ‖qk2‖2 also follows, since\n∞∑ k=1 ‖qk2‖2 ≤ 1 (η − τ − C(τ2 + η2)) ∞∑ k=1 ∆Ek <∞. (83)\nSimilar calculations show summability of ‖qk1‖2, |rk|2, and ‖pk‖2. Thus\n‖qk1‖, ‖qk2‖, |rk|2, ‖pk‖2 → 0, (84) ‖∆vk1‖, ‖∆vk2‖, ‖∆zk‖, ‖∆wk‖ → 0. (85)\nLemma 4 and Assumption 3 imply a set of minimum weights v∗1 , v ∗ 2 , z ∗, w∗, which determine a local minimum of E.\n11\nA.3 Proof of convergence for DAG architectures\nIn what follows, we will need some notation for tensor manipulation. For a real valued function f , and a vector v = (v1, . . . , vn), we’ll use the notation f(v) = (f(v1), . . . , f(vn)). We’ll also use the matrix inner product, which for two matrices A = (ai,j)n×m, B = (bi,j)n×m, is defined as A : B = ∑ i,j ai,jbi,j . Finally, a matrix gradient of a real (vector) valued function is matrix (tensor) valued, with an\nelement-wise representation as ∂f ∂vk\n(i,j)\n=\n( ∂f\n∂va,b;k (i,j) ) li×lj and\n∂Hkm ∂vk\n(i,j)\n= ( ∂Hc;km ∂va,b;k\n(i,j) ) li×lj×lm .\nA.4 Estimates on DAGs\nOur major technical theorem is a generalization of Lemma 1. Specifically, we show that the difference operator ∆Hk+1 is similar, up to first order, to Qk(Hk), where Qk denotes the differential operator\nQk = ∑\ni<j≤L ∆vk+1(i,j) :\n∂\n∂vk(i,j) . (86)\nNote that when Qk acts on a vector, the matrix inner product in (86) is between a matrix and a tensor, and is vector valued.\nThe major utility of introducing Qk is that it provides a simple bound when acting on Ek. Specifically, using (20) , (21), and (22), it is straightforward to show\nQk(Ek) ≤ (−η + τ) ∑\ni<j≤L ‖qk(i,j)‖2. (87)\nTheorem 3. There exists a universal constant C > 0 such that\n|Qk(Ek)−∆Ek+1| ≤ C ∑ j≤L ‖∆Hkj ‖2 + ∑ i<j≤L ‖∆vk(i,j)‖2  .\n(88)\nProof. We first note that since Hkl only depends on layers 1 through l − 1, we may truncate the sum of Qk and write\nQk(Hkl ) = ∑ i<j≤l ∆vk+1(i,j) : ∂Hkl ∂vk(i,j) . (89)\nApplying the chain rule to (5), using (18), and rearranging sums,\nQk(Ek) = g′p(SL) ∑\ni<j≤L ∆vk+1(i,j) :\n∂\n∂vk(i,j) (∑ m<L Hkmv k (m,L) ) (90) (91)\n= g′p(SL) ∑ m<L ∑ i<j≤L ∆vk+1(i,j) : ∂ ∂vk(i,j) ( Hkmv k (m,L) ) .\n(92)\nDifferentiating with respect to one entry of the matrix derivative in (92) gives\n∂\n∂va,b;k(i,L)\n( Hkmv k (m,L) ) = ∂Hkm\n∂va,b;k(i,L) vk(m,L) +H k m\n∂vk(m,L) ∂va,b;k(i,L) (93)\n= ∂Hkm\n∂va,b;k(i,L) vk(m,L) + (0, . . . , δi,mH a;k m︸ ︷︷ ︸\nbth entry\n, . . . , 0) (94)\n:= Aa,b;ki,m +B a,b;k i,m . (95)\nSumming Aki,m over the second term in (92) gives\n∑ m<L ∑ i<L ∆vk+1(i,L) : A k i,m = ∑ m<L ∑ i<L ∑ a<li b<lL ∆va,b;k+1(i,L) ∂Hkm ∂va,b;k(i,L) vk(m,L)\n(96)\n= ∑ m<L ( ∆vk+1(i,j) : ∂Hkm ∂vk(i,j) ) vk(m,L).\n(97)\nFor Bki,m,∑ m<L ∑ i<L ∆vk+1(i,L) : B k i,m = ∑ m<L ∑ a<li b<lL ∆va,b;k+1(m,L) B a,b;k m,m (98) = ∑ m<L ∑ b<lL H1,km ∆v 1,b;k+1 (m,L) , . . . , ∑ b<lL H lm,km ∆v lm,b;k+1 (m,L)  (99)\n= ∑ m<L Hkm∆v k+1 (m,L). (100)\nCalculations for the second part of the double sum in (92) for the case for j < L are similar to the case j = L, except that there is no corresponding Bki,m term. Indeed,\n∑ m<L ∑ i<j<m ∆vk+1(i,j) : ∂ ∂vk(i,j) ( Hkmv k (m,L) ) (101)\n= ∑ m<L ∑ i<j<m ∆vk+1(i,j) : ( ∂Hkm ∂vk(i,j) vk(m,L) ) . (102)\nCombining (100) and (102),\n∑ m<L ∑ i<j≤L ∆vk+1(i,j) : ∂ ∂vk(i,j) ( Hkmv k (m,L) ) (103)\n= ∑ m<L Hkm∆v k+1 (m,L) + ∑ m<L ∑ i<j≤m ∆vk+1(i,j) : ( ∂Hkm ∂vk(i,j) vk(m,L) ) (104)\n= ∑ m<L ( Hkm∆v k+1 (m,L) +Q k(Hkm)v k (m,L) ) . (105)\nThis can be directly substituted into (92) to yield\nQ(Ek) = g′p ( SkL ) ∑ m<L ( Hkm∆v k+1 (m,L) +Q k(Hkm)v k (m,L) ) .\n(106)\n12\nFrom similar calculations, the formula over a node Hkn , with n < L, is\nQ(Hkn) = g ′ ( Skn ) ∑ m<n ( Hkm∆v k+1 (m,n) +Q k(Hkm)v k (m,n) ) .\n(107)\nWe may also derive a recursive formula for ∆Hk+1n . This is done through Taylor’s theorem, where there exists tpk between Ek and Ek+1 with\n∆Ek+1 = g′p ( SkL )( ∑ m<L ∆Hk+1i v k (i,L) (108)\n+Hk∆vk+1(i,L) + ∑ m<L ∆vk+1(i,L) ·∆H k+1 i ) (109)\n+ g′′p (t p k) (∑ m<L ∆(vk+1(i,L) ·H k+1 i ) )2 . (110)\nSimilarly,there exist tpl,k = (t p,1 l,k , . . . , t p,ll l,k ) where each t p n,k lies between Hkl and H k+1 l , so that\n∆Hk+1l = g ′ ( Skl )(∑ i<l ∆Hk+1i v k (i,l) +H k ·∆vk+1(i,l) (111)\n+ ∑ i<l ∆Hk+1i ∆v k (i,l) ) + 1 2 g′′(tpl,k) (∑ i<l Hk+1i ∆v k+1 (i,l) )2 .\n(112)\nTo bound the quadratic terms, we use Assumptions 1 and 2, along with the simple bound\n‖∆Hk+1i ∆v k+1 (i,l)‖ ≤ C(‖∆v k+1 (i,l)‖ 2 + ‖∆Hk+1i ‖ 2) (113)\nfor some constant C > 0. Taking differences of (112) and (107), for any l < L, we then obtain the recurrence inequality\n‖Q(Hkl )−∆Hk+1l ‖ ≤ C (∑ i<l ‖Q(Hki )−∆Hk+1i ‖ ) (114)\n+ C ∑ i<l (‖∆vk+1(i,l)‖ 2 + ‖∆Hk+1l ‖ 2). (115)\nReplacing Hkl with E k in (114) produces the same type inequality, with the sum in (115) now ranging from i = 1, . . . , L− 1. Repeated applications of (117), to the Ek, and subsequently to Hkl , for l = 1, . . . , L− 1, result in\n|Qk(Ek)−∆Ek+1| ≤ C ( ‖Q(Hk0 )−∆Hk+10 ‖ ) (116)\n+ C ∑ j<L ‖∆Hkj ‖2 + ∑ i<j<L ‖∆vk(i,j)‖2  . (117)\nTo complete the proof, we note that the input data x does not change under iterations, so\nQ(Hk0 )−∆Hk+10 ≡ 0. (118)\nWe now bound the quadratic terms in (88).\nLemma 5. For some constant C > 0,\n1) ‖∆vk(i,j)‖ ≤ (η + τ)‖qk(i,j)‖ (119)\n2) ‖∆Hkl ‖ ≤ C(η + τ) ∑ i<l ‖qk(i,l)‖ (120)\nProof. The first inequality follows immediately from (21) and (22). For the next inequality, note that from Taylor’s theorem, and the boundedness of g′ and g′′:\n‖∆Hk+1l ‖ ≤ |g ′(Skl )| ∥∥∥∥∥∆ (∑ i<l Hk+1i v k (i,l) )∥∥∥∥∥ (121) + 1\n2 |g′′(tkl )| ∥∥∥∥∥∥∆ (∑ i<l Hk+1i v k (i,l) )2∥∥∥∥∥∥ (122) From Assumptions 1 and 2, we may rewrite this, using\n(119)\n‖∆Hk+1l ‖ ≤ C ∑ i<l ‖∆vk(i,l)‖+ ‖∆vk(i,l)‖2\n(123) ≤ C(τ + η)( ∑ i<l ‖qk(i,l)‖+ ‖qk(i,l)‖2) ≤ C(τ + η) ∑ i<l ‖qk(i,l)‖\n(124)\nThe last inequality uses the fact that from Assumption 2, we may uniformly bound gradients.\nNow, combining Theorem 3, (87), and Lemma 5, the iteration of error may then be estimated as\n∆Ek+1 ≤ C ∑ j≤L ‖∆Hkj ‖2 + ∑ i<j≤L ‖∆vk(i,j)‖2 +Q(Ek)\n(125) ≤ ( −η + τ + C(τ2 + η2) ) ∑ i<j≤L ‖qk(i,j)‖2. (126)\nFor some s ∈ (0, 1), assume η = sτ . The proof of Theorem 2 is then identical to that of the two node case."
    }, {
      "heading" : "APPENDIX B DETAILED DESCRIPTION OF EXPERIMENTS",
      "text" : "This section describes the different network architectures and details for the experimental results.\nB.1 MNIST\nAll the different networks were tuned on the validation dataset. This tuned network was used to evaluate the performance on the test images. The performance of our proposed architecture was evaluated over six architectures overall. Stochastic Gradient Descent (SGD) with momentum of 0.95 was used for training all the architectures. For the shallow networks (25 and 50 neurons case), we used an intial learning rate of 0.1 and dropped it by 0.75 every 10 epochs until 125 epochs, and then the learning rate was dropped by 0.998 times its previous value. For all other MLP architectures we used an initial learning rate of 0.01 and dropped it by 0.5 at every quarter of the total number of epochs. In regards to CrossNet version of the MLP network, the input layer was connected to all the subsequent hidden layers (excluding the output layer), and the hidden layers were connected to all\n13\nthe subsequent hidden layers (including the output layer). For the 2-layer CNN architecture we used a fairly standard network. Both convolutional layers use 5× 5 filter size with 16 and 32 channels, respectively. A dropout of 0.5 is used after the second convolutional layer. All the layers used ’ReLU’ activations. For the CrossNet version of this architecture the activated outputs of both the convolutional layers were stacked together and fed to the output softmax classifier. The 2-CNN-2-fully connected architecture was designed as per [25] The first convolutional produces 32 feature maps, and the second convolutional layer produces 64 feature maps, each of using 5 × 5 filters followed by 2 × 2 pooling. The classifier comprises of 2-layer fully-connected neural network with 200 hidden units, and 10 outputs. A 10-layer CrossNet was designed for the experiment. We use 12 composite units (defined in Section.2.3). A 2× 2 average pooling layer was used after every 4 composite units. We used a step wise learning rate for training, starting from an initial learning rate of 0.1. We trained the network for 30 epochs.\nB.2 CIFAR-10 and CIFAR-100 Both CIFAR-10 and CIFAR-100 contains 32× 32 RGB images belonging to 10 and 100 classes respectively. The raw input images pre-processed using global contrast normalization and ZCA whitening. ZCA whitening means that we center the data around its mean, rotate it onto its principle components, normalize individual components and then rotate them back. We held out 5,000 random training images for validation. The network was tuned to give the best performance on this validation dataset. The presented results are obtained from this tuned network. We use three different versions of CrossNets for our experiments. CrossNet v1, CrossNet v2, and CrossNet v3 consisted of 32, 64, and 48 composite units respectively. The pre-processed images were preactivated using a convolutional layer with 16 filters of 3 × 3 kernel size each. A 2 × 2 average pooling layer was used after 8, 16, and 16 composite units in each of the above mentioned CrossNets respectively. Finally, a global averaging pooling layer and a softmax classifier was used at the end. We use a step wise learning rate schedule for training, where we start with an initial learning rate of 0.1 and reduced 10 times after each 10% of the total number of epochs. We train the network for 300 epochs. No dropouts or dropconnects were used in the training. Standard data augmentation scheme in the form of translation and horizontal flips were used.\nB.3 SVHN SVHN comprises of 32× 32 RGB images of digit classes. As before hyperparameters for the training were tuned on the validation set, and the tuned network was used to evaluate the performance on the test dataset. We held out 6,000 images from the training dataset for validation. The SVHN dataset is a relatively easier task as compared to CIFAR. Hence, pre-processing was only done in terms of global contrast normalization. The same three network architectures for CrossNets were used as that for CIFAR. The only difference is in terms of the total number of epochs. We train the network for 40 epochs. The learning rate schedule starts with an initial learning rate of 0.1 and is reduced 10 times after every 10 epochs. No dropouts or dropconnects were used in the training. No data augmentation was performed."
    } ],
    "references" : [ {
      "title" : "Densely connected convolutional networks",
      "author" : [ "G. Huang", "Z. Liu", "K.Q. Weinberger", "L. van der Maaten" ],
      "venue" : "arXiv preprint arXiv:1608.06993, 2016.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Principles of neurodynamics. perceptrons and the theory of brain mechanisms",
      "author" : [ "F. Rosenblatt" ],
      "venue" : "DTIC Document, Tech. Rep., 1961.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1961
    }, {
      "title" : "Solving the n-bit parity problem using neural networks",
      "author" : [ "M.E. Hohil", "D. Liu", "S.H. Smith" ],
      "venue" : "Neural Networks, vol. 12, no. 9, pp. 1321–1323, 1999.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Solving parityn problems with feedforward neural networks",
      "author" : [ "B.M. Wilamowski", "D. Hunter", "A. Malinowski" ],
      "venue" : "Neural Networks, 2003. Proceedings of the International Joint Conference on, vol. 4. IEEE, 2003, pp. 2546–2551.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "On lateral connections in feedforward neural networks",
      "author" : [ "R. Kothari", "K. Agyepong" ],
      "venue" : "Neural Networks, 1996., IEEE International Conference on, vol. 1. IEEE, 1996, pp. 13–18.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Layered neural networks with horizontal connections can reduce the number of units",
      "author" : [ "J. Smid" ],
      "venue" : "Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 3. IEEE, 1994, pp. 1346–1350.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "The cascade-correlation learning architecture",
      "author" : [ "S.E. Fahlman", "C. Lebiere" ],
      "venue" : "1990.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Y. Bengio", "P. Simard", "P. Frasconi" ],
      "venue" : "IEEE transactions on neural networks, vol. 5, no. 2, pp. 157–166, 1994.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks.",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "in Aistats, vol",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "author" : [ "A. Nguyen", "J. Yosinski", "J. Clune" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 427–436.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770–778.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Training very deep networks",
      "author" : [ "R.K. Srivastava", "K. Greff", "J. Schmidhuber" ],
      "venue" : "Advances in neural information processing systems, 2015, pp. 2377–2385.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fractalnet: Ultra-deep neural networks without residuals",
      "author" : [ "G. Larsson", "M. Maire", "G. Shakhnarovich" ],
      "venue" : "arXiv preprint arXiv:1605.07648, 2016.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deeplysupervised nets",
      "author" : [ "C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu" ],
      "venue" : "Artificial Intelligence and Statistics, 2015, pp. 562–570.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1–9.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Maxout networks",
      "author" : [ "I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1302.4389, 2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Network in network",
      "author" : [ "M. Lin", "Q. Chen", "S. Yan" ],
      "venue" : "arXiv preprint arXiv:1312.4400, 2013.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convergence of gradient method with momentum for back-propagation neural networks",
      "author" : [ "W. Wu", "N. Zhang", "Z. Li", "L. Li", "Y. Liu" ],
      "venue" : "Journal of Computational Mathematics, pp. 613–623, 2008.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Convergence of gradient method with momentum for two-layer feedforward neural networks",
      "author" : [ "N. Zhang", "W. Wu", "G. Zheng" ],
      "venue" : "IEEE Transactions on Neural Networks, vol. 17, no. 2, pp. 522–525, 2006.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Optimal convergence of on-line backpropagation",
      "author" : [ "M. Gori", "M. Maggini" ],
      "venue" : "IEEE transactions on neural networks, vol. 7, no. 1, pp. 251–254, 1996.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "European Conference on Computer Vision. Springer, 2016, pp. 630–645.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "Y. LeCun", "C. Cortes", "C.J. Burges" ],
      "venue" : "1998.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Best practices for convolutional neural networks applied to visual document analysis.",
      "author" : [ "P.Y. Simard", "D. Steinkraus", "J.C. Platt" ],
      "venue" : "in ICDAR,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2003
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting.",
      "author" : [ "N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "What is the best multistage architecture for object recognition?",
      "author" : [ "K. Jarrett", "K. Kavukcuoglu", "Y. LeCun" ],
      "venue" : "in Computer Vision,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky", "G. Hinton" ],
      "venue" : "2009.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Deep networks with stochastic depth",
      "author" : [ "G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger" ],
      "venue" : "European Conference on Computer Vision. Springer, 2016, pp. 646–661.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML-13), 2013, pp. 1058–1066.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree",
      "author" : [ "C.-Y. Lee", "P.W. Gallagher", "Z. Tu" ],
      "venue" : "International conference on artificial intelligence and statistics, 2016.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Resnet in resnet: generalizing residual architectures",
      "author" : [ "S. Targ", "D. Almeida", "K. Lyman" ],
      "venue" : "arXiv preprint arXiv:1603.08029, 2016.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Wide residual networks",
      "author" : [ "S. Zagoruyko", "N. Komodakis" ],
      "venue" : "arXiv preprint arXiv:1605.07146, 2016.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng" ],
      "venue" : "NIPS workshop on deep learning and unsupervised feature learning, vol. 2011, no. 2, 2011, p. 5.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Two highly efficient second-order algorithms for training feedforward networks",
      "author" : [ "N. Ampazis", "S.J. Perantonis" ],
      "venue" : "IEEE Transactions on Neural Networks, vol. 13, no. 5, pp. 1064–1074, 2002.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Neural network architectures and learning algorithms",
      "author" : [ "B.M. Wilamowski" ],
      "venue" : "IEEE Industrial Electronics Magazine, vol. 3, no. 4, 2009.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Optimization theory and methods: nonlinear programming",
      "author" : [ "W. Sun", "Y.-X. Yuan" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "From multilayer perceptron (MLP) networks to the more prominent recurrent neural networks (RNN) and convolutional neural networks (CNN), neural networks have become a dominant force in the fields of computer vision, speech recognition, and machine translation [2].",
      "startOffset" : 260,
      "endOffset" : 263
    }, {
      "referenceID" : 2,
      "context" : "However, Arbitrary Networks (AC) networks have been shown to train and perform better than the commonly used feed-forward architectures [3], [4].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "However, Arbitrary Networks (AC) networks have been shown to train and perform better than the commonly used feed-forward architectures [3], [4].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "Neural networks with lateral connections (another special case of AC networks) between hidden layer neurons are also found to be more efficient than MLP networks [5], [6].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "Neural networks with lateral connections (another special case of AC networks) between hidden layer neurons are also found to be more efficient than MLP networks [5], [6].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "Currently, a wide array of problems such as herd effect [7], vanishing gradients [8], [9], diminishing feature reuse, and ”fooling images” [10] have emerged in regards to deep networks.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "Currently, a wide array of problems such as herd effect [7], vanishing gradients [8], [9], diminishing feature reuse, and ”fooling images” [10] have emerged in regards to deep networks.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "Currently, a wide array of problems such as herd effect [7], vanishing gradients [8], [9], diminishing feature reuse, and ”fooling images” [10] have emerged in regards to deep networks.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "Currently, a wide array of problems such as herd effect [7], vanishing gradients [8], [9], diminishing feature reuse, and ”fooling images” [10] have emerged in regards to deep networks.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "Although most of the works in neural networks have been restricted to a strictly feed-forward network, recent works like Deep Residual Networks (ResNet) [11], Highway Networks [12], FractalNets [13], and DenseNets [1]* broke the general feedforward architecture to use ”short connections” to connect nonconsecutive layers.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 11,
      "context" : "Although most of the works in neural networks have been restricted to a strictly feed-forward network, recent works like Deep Residual Networks (ResNet) [11], Highway Networks [12], FractalNets [13], and DenseNets [1]* broke the general feedforward architecture to use ”short connections” to connect nonconsecutive layers.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 12,
      "context" : "Although most of the works in neural networks have been restricted to a strictly feed-forward network, recent works like Deep Residual Networks (ResNet) [11], Highway Networks [12], FractalNets [13], and DenseNets [1]* broke the general feedforward architecture to use ”short connections” to connect nonconsecutive layers.",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 0,
      "context" : "Although most of the works in neural networks have been restricted to a strictly feed-forward network, recent works like Deep Residual Networks (ResNet) [11], Highway Networks [12], FractalNets [13], and DenseNets [1]* broke the general feedforward architecture to use ”short connections” to connect nonconsecutive layers.",
      "startOffset" : 214,
      "endOffset" : 217
    }, {
      "referenceID" : 0,
      "context" : "[1] performed experiments independently on this subject similar to ours.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Kothari and Agyepong [5], for instance, introduced simple lateral connections in the form of a chain, where each unit in a hidden layer is connected to the next.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "The problem of vanishing gradients were addressed in [14], [15] by connecting some hidden layers directly to the classifier layer.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "The problem of vanishing gradients were addressed in [14], [15] by connecting some hidden layers directly to the classifier layer.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "Similar to our work, DenseNets [1]* recently connected all layers (with matching feature-map sizes) directly with each other resulting in feature reuse, all layers can easily access their preceding layers making it easy to reuse the information from previously computed feature maps.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "Many different architectures like Maxout Networks [16], Network In Network (NIN) [17], Deeply Supervised Networks (DSN) [14], and FractalNets are being proposed which give competitive results on the above mentioned datasets.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "Many different architectures like Maxout Networks [16], Network In Network (NIN) [17], Deeply Supervised Networks (DSN) [14], and FractalNets are being proposed which give competitive results on the above mentioned datasets.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "Many different architectures like Maxout Networks [16], Network In Network (NIN) [17], Deeply Supervised Networks (DSN) [14], and FractalNets are being proposed which give competitive results on the above mentioned datasets.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "One of the problems mentioned by [10] was that Deep Neural Networks (DNN) tend to learn low and middle-level features rather than the global structure of objects due to which the network was easily fooled on showing only subcomponents of an object.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "Another aim of this paper is to generalize the results in [11] to a more general case, that is, cross-connected neural networks.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "The proof is similar to [18].",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "This choice of momentum was also used in [19].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "This proof will require some regularity and boundedness assumptions [18] Assumptions 1.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 17,
      "context" : "This set of assumptions is equivalent to that used in [18], and may also be found in other nonlinear optimization problems such as [20].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 19,
      "context" : "This set of assumptions is equivalent to that used in [18], and may also be found in other nonlinear optimization problems such as [20].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 20,
      "context" : "We followed the same configuration in our experiments as [21] and define each stack of these three layers as a composite unit.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "1 MNIST We performed experiments on hand-written digit classification problem [22] where input patterns are digits.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "60% [23].",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 23,
      "context" : "25% error for a traditional feed forward network with same architecture using dropouts [24].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 24,
      "context" : "53% was achieved in [25] using a 2layer CNN+2-layer NN configuration (0.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 16,
      "context" : "Network in Network [17] surpassed this result by achieving an error of 0.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "By using maxout activation function [16] reduced the error to 0.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "39% achieved by [14].",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 25,
      "context" : "2 CIFAR-10 and CIFAR-100 The CIFAR datasets [26] consist of 32x32 RGB color images taken from various classes.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 26,
      "context" : "66%) [27] without any data augmentation by a large margin for CIFAR-10 dataset.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 27,
      "context" : "DropConnect[28] 18.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 26,
      "context" : "32% ResNet[27] 13.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 26,
      "context" : "22% Stochastic Depth[27] 11.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 28,
      "context" : "39% Generalized Pooling[29] 7.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 29,
      "context" : "37% ResNet in ResNet [30] - 5.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 30,
      "context" : "90% Wide ResNet [31] - 4.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 31,
      "context" : "3 Street View House Numbers The Street View House Numbers (SVHN) dataset [32] consists of 3232 color RGB images.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "In [3], [4] it was proposed that Arbitrary Connected (AC) networks are much more powerful than commonly used feed-forward networks.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "In [3], [4] it was proposed that Arbitrary Connected (AC) networks are much more powerful than commonly used feed-forward networks.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 32,
      "context" : "However, the back-propagation algorithm is slower than more advanced second-order algorithms [33], [34].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 33,
      "context" : "However, the back-propagation algorithm is slower than more advanced second-order algorithms [33], [34].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 34,
      "context" : "2 Proof of convergence We now use the a Lemma found in [35]: Lemma 4.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 24,
      "context" : "The 2-CNN-2-fully connected architecture was designed as per [25] The first convolutional produces 32 feature maps, and the second convolutional layer produces 64 feature maps, each of using 5 × 5 filters followed by 2 × 2 pooling.",
      "startOffset" : 61,
      "endOffset" : 65
    } ],
    "year" : 2017,
    "abstractText" : "We propose a novel neural network structure called CrossNets, which considers architectures on directed acyclic graphs. This structure builds on previous generalizations of feed forward models, such as ResNets, by allowing for all forward cross connections between layers (both adjacent and non-adjacent). The addition of cross connections among the network increases information flow across the whole network, leading to better training and testing performances. The superior performance of the network is tested against four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. We conclude with a proof of convergence for Crossnets to a local minimum for error, where weights for connections are chosen through backpropagation with momentum.",
    "creator" : "LaTeX with hyperref package"
  }
}