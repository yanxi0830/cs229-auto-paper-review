{
  "name" : "1104.4803.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Clustering Partially Observed Graphs via Convex Optimization",
    "authors" : [ "Yudong Chen", "Ali Jalali", "Sujay Sanghavi", "Huan Xu", "Marina Meila" ],
    "emails" : [ "YDCHEN@UTEXAS.EDU", "ALIJ@MAIL.UTEXAS.EDU", "SANGHAVI@MAIL.UTEXAS.EDU", "MPEXUH@NUS.EDU.SG" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We take a novel yet natural approach to this problem, by focusing on finding the clustering that minimizes the number of “disagreements”—i.e., the sum of the number of (observed) missing edges within clusters, and (observed) present edges across clusters. Our algorithm uses convex optimization; its basis is a reduction of disagreement minimization to the problem of recovering an (unknown) low-rank matrix and an (unknown) sparse matrix from their partially observed sum. We evaluate the performance of our algorithm on the classical Planted Partition/Stochastic Block Model. Our main theorem provides sufficient conditions for the success of our algorithm as a function of the minimum cluster size, edge density and observation probability; in particular, the results characterize the tradeoff between the observation probability and the edge density gap. When there are a constant number of clusters of equal size, our results are optimal up to logarithmic factors.\nKeywords: graph clustering, convex optimization, sparse and low-rank decomposition"
    }, {
      "heading" : "1. Introduction",
      "text" : "This paper is about the following task: given partial observation of an undirected unweighted graph, partition the nodes into disjoint clusters so that there are dense connections within clusters, and sparse connections across clusters. By partial observation, we mean that for some node pairs we know if there is an edge or not, and for the other node pairs we do not know—these pairs are unobserved. This problem arises in several fields across science and engineering. For example, in sponsored search, each cluster is a submarket that represents a specific group of advertisers that do most of their spending on a group of query phrases—see e.g., Yahoo!-Inc (2009) for such a project at Yahoo. In VLSI and design automation, it is useful in minimizing signaling between components (Kernighan and Lin, 1970). In social networks, clusters may represent groups of people with\nc©2014 Yudong Chen, Ali Jalali, Sujay Sanghavi and Huan Xu.\nar X\niv :1\n10 4.\n48 03\nv4 [\ncs .L\nG ]\nsimilar interest or background; finding clusters enables better recommendations and link prediction (Mishra et al., 2007). In the analysis of document databases, clustering the citation graph is often an essential and informative first step (Ester et al., 1995). In this paper, we will focus not on specific application domains, but rather on the basic graph clustering problem itself.\nPartially observed graphs appear in many applications. For example, in online social networks like Facebook, we observe an edge/no edge between two users when they accept each other as a friend or explicitly decline a friendship suggestion. For the other user pairs, however, we simply have no friendship information between them, which are thus unobserved. More generally, we have partial observations whenever obtaining similarity data is difficult or expensive (e.g., because it requires human participation). In these applications, it is often the case that most pairs are unobserved, which is the regime we are particularly interested in.\nAs with any clustering problem, we need a precise mathematical definition of the clustering criterion with potentially a guaranteed performance. There is relatively few existing results with provable performance guarantees for graph clustering with partially observed node pairs. Many existing approaches to clustering fully observed graphs either require an additional input (e.g., the number of clusters k required for spectral or k-means clustering methods), or do not guarantee the performance of the clustering. We review existing related work in Section 1.2."
    }, {
      "heading" : "1.1 Our Approach",
      "text" : "We focus on a natural formulation, one that does not require any other extraneous input besides the graph itself. It is based on minimizing disagreements, which we now define. Consider any candidate clustering; this will have (a) observed node pairs that are in different clusters, but have an edge between them, and (b) observed node pairs that are in the same cluster, but do not have an edge between them. The total number of node pairs of types (a) and (b) is the number of disagreements between the clustering and the given graph. We focus on the problem of finding the optimal clustering—one that minimizes the number of disagreements. Note that we do not pre-specify the number of clusters. For the special case of fully observed graphs, this formulation is exactly the same as the problem of correlation clustering, first proposed by Bansal et al. (2002). They show that exact minimization of the above objective is NP-hard in the worst case—we survey and compare with this and other related work in Section 1.2. As we will see, our approach and results are different.\nWe aim to achieve the combinatorial disagreement minimization objective using matrix splitting via convex optimization. In particular, as we show in Section 2 below, one can represent the adjacency matrix of the given graph as the sum of an unknown low-rank matrix (corresponding to “ideal” clusters) and a sparse matrix (corresponding to disagreements from this “ideal” in the given graph). Our algorithm either returns a clustering, which is guaranteed to be disagreement minimizing, or returns a “failure”—it never returns a sub-optimal clustering. For our main analytical result, we evaluate our algorithm’s performance on the natural and classical planted partition/stochastic block model with partial observations. Our analysis provides stronger guarantees than are current results on general matrix splitting (Candès et al., 2011; Hsu et al., 2011; Li, 2013; Chen et al., 2013). The algorithm, model and results are given in Section 2. We prove our theoretical results in Section 3 and provide empirical results in Section 4."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "Our problem can be interpreted in the general clustering context as one in which the presence of an edge between two points indicates a “similarity”, and the lack of an edge means “no similarity”. The general field of clustering is of course vast, and a detailed survey of all methods therein is beyond our scope here. We focus instead on the three sets of papers most relevant to the problem here: the work on correlation clustering, on the planted partition/stochastic block model, and on graph clustering with partial observations."
    }, {
      "heading" : "1.2.1 CORRELATION CLUSTERING",
      "text" : "As mentioned, for a completely observed graph, our problem is mathematically precisely the same as correlation clustering formulated in Bansal et al. (2002); in particular a “+” in correlation clustering corresponds to an edge in the graph, a “-” to the lack of an edge, and disagreements are defined in the same way. Thus, this paper can equivalently be considered as an algorithm, and guarantees, for correlation clustering under partial observations. Since correlation clustering is NP-hard, there has been much work on devising alternative approximation algorithms (Bansal et al., 2002; Emmanuel and Fiat, 2003). Approximations using convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed. We emphasize that we use a different convex relaxation, and we focus on understanding when our convex program yields an optimal clustering without further rounding.\nWe note that Mathieu and Schudy (2010) use a convex formulation with constraints enforcing positive semi-definiteness, triangle inequalities and fixed diagonal entries. For the fully observed case, their relaxation is at least as tight as ours, and since they add more constraints, it is possible that there are instances where their convex program works and ours does not. However, this seems hard to prove/disprove. Indeed, in the full observation setting they consider, their exact recovery guarantee is no better than ours. Moreover, as we argue in the next section, our guarantees are order-wise optimal in some important regimes and thus cannot be improved even with a tighter relaxation. Practically, our method is faster since, to the best of our knowledge, there is no lowcomplexity algorithm to deal with the Θ(n3) triangle inequality constraints required by Mathieu and Schudy (2010). This means that our method can handle large graphs while their result is practically restricted to small ones (∼ 100 nodes). In summary, their approach has higher computational complexity, and does not provide significant and characterizable performance gain in terms of exact cluster recovery."
    }, {
      "heading" : "1.2.2 PLANTED PARTITION MODEL",
      "text" : "The planted partition model, also known as the stochastic block-model (Condon and Karp, 2001; Holland et al., 1983), assumes that the graph is generated with in-cluster edge probability p and inter-cluster edge probability q (where p > q) and fully observed. The goal is to recover the latent cluster structure. A class of this model with τ , max{1 − p, q} < 12 is often used as benchmark for average case performance for correlation clustering (see, e.g., Mathieu and Schudy, 2010). Our theoretical results are applicable to this model and thus directly comparable with existing work in this area. A detailed comparison is provided in Table 1. For fully observed graphs, our result matches the previous best bounds in both the minimum cluster size and the difference between in-cluster/inter-cluster densities. We would like to point out that nuclear norm minimization has\nbeen used to solve the closely related planted clique problem (Alon et al., 1998; Ames and Vavasis, 2011)."
    }, {
      "heading" : "1.2.3 PARTIALLY OBSERVED GRAPHS",
      "text" : "The previous work listed in Table 1, except Oymak and Hassibi (2011), does not handle partial observations directly. One natural way to proceed is to impute the missing observations with noedge, or random edges with symmetric probabilities, and then apply any of the results in Table 1. This approach, however, leads to sub-optimal results. Indeed, this is done explicitly by Oymak and Hassibi (2011). They require the probability of observation p0 to satisfy p0 & √ Kmin n , where n is the number of nodes and Kmin is the minimum cluster size; in contrast, our approach only needs p0 & nK2min (both right hand sides have to be less than 1, requiring Kmin & √ n, so the right hand side of our condition is order-wise smaller and thus less restrictive.) Shamir and Tishby (2011) deal with partial observations directly and shows that p0 & 1n suffices for recovering two clusters of size Ω(n). Our result is applicable to much smaller clusters of size Ω̃( √ n). In addition, a nice feature of\nour result is that it explicitly characterizes the tradeoffs between the three relevant parameters: p0, τ , and Kmin; theoretical result like this is not available in previous work.\nThere exists other work that considers partial observations, but under rather different settings. For example, Balcan and Gupta (2010), Voevodski et al. (2010) and Krishnamurthy et al. (2012) consider the clustering problem where one samples the rows/columns of the adjacency matrix rather than its entries. Hunter and Strohmer (2010) consider partial observations in the features rather than in the similarity graph. Eriksson et al. (2011) show that Ω̃(n) actively selected pairwise similarities are sufficient for recovering a hierarchical clustering structure. Their results seem to rely on the hierarchical structure. When disagreements are present, the first split of the cluster tree can recovers clusters of size Ω(n); our results allow smaller clusters. Moreover, they require active control over the observation process, while we assume random observations."
    }, {
      "heading" : "2. Main Results",
      "text" : "Our setup for the graph clustering problem is as follows. We are given a partially observed graph of n nodes, whose adjacency matrix is A ∈ Rn×n, which has ai,j = 1 if there is an edge between nodes i and j, ai,j = 0 if there is no edge, and ai,j =“?” if we do not know. (Here we follow the convention that ai,i = 0 for all i.) Let Ωobs , {(i, j) : ai,j 6=?} be the set of observed node pairs. The goal to find the optimal clustering, i.e., the one that has the minimum number of disagreements (defined in Section 1.1) in Ωobs.\nIn the rest of this section, we present our algorithm for the above task and analyze its performance under the planted partition model with partial observations. We also study the optimality of the performance of our algorithm by deriving a necessary condition for any algorithm to succeed."
    }, {
      "heading" : "2.1 Algorithm",
      "text" : "Our algorithm is based on convex optimization, and either (a) outputs a clustering that is guaranteed to be the one that minimizes the number of observed disagreements, or (b) declares “failure”. In particular, it never produces a suboptimal clustering.1 We now briefly present the main idea and then describe the algorithm.\nConsider first the fully observed case, i.e., every ai,j = 0 or 1. Suppose also that the graph is already ideally clustered—i.e., there is a partition of the nodes such that there is no edge between clusters, and each cluster is a clique. In this case, the matrix A + I is now a low-rank matrix, with the rank equal to the number of clusters. This can be seen by noticing that if we re-order the rows and columns so that clusters appear together, the result would be a block-diagonal matrix, with each block being an all-ones submatrix—and thus rank one. Of course, this re-ordering does not change the rank of the matrix, and hence A + I is exactly low-rank.\nConsider now any given graph, still fully observed. In light of the above, we are looking for a decomposition of its A + I into a low-rank part K∗ (of block-diagonal all-ones, one block for each cluster) and a remaining B∗ (the disagreements), such that the number of non-zero entries in B∗ is as small as possible; i.e., B∗ is sparse. Finally, the problem we look at is recovery of the best K∗ when we do not observe all entries of A + I. The idea is depicted in Figure 1.\n1. In practice, one might be able to use the “failed” output with rounding as an approximate solution. In this paper, we focus on the performance of the unrounded algorithm.\nWe propose to perform the matrix splitting using convex optimization (Chandrasekaran et al., 2011; Candès et al., 2011). Our approach consists of dropping any additional structural requirements, and just looking for a decomposition of the given A + I as the sum of a sparse matrix B and a low-rank matrix K. Recall that Ωobs is the set of observed entries, i.e., the set of elements of A that are known to be 0 or 1; we use the following convex program:\nmin B,K λ ‖B‖1 + ‖K‖∗\ns.t. PΩobs(B + K) = PΩobs(A + I). (1)\nHere, for any matrix M, the term PΩobs(M) keeps all elements of M in Ωobs unchanged, and sets all other elements to 0; the constraints thus state that the sparse and low-rank matrix should in sum be consistent with the observed entries of A + I. The term ‖B‖1 = ∑ i,j |bi,j | is the `1 norm of the entries of the matrix B, which is well-known to be a convex surrogate for the number of nonzero entries ‖B‖0. The second term ‖K‖∗ = ∑ s σs(K) is the nuclear norm (also known as the trace norm), defined as the sum of the singular values of K. This has been shown to be the tightest convex surrogate for the rank function for matrices with unit spectral norm (Fazel, 2002). Thus our objective function is a convex surrogate for the (natural) combinatorial objective λ‖B‖0 + rank(K). The optimization problem (1) is, in fact, a semidefinite program (SDP) (Chandrasekaran et al., 2011).\nWe remark on the above formulation. (a) This formulation does not require specifying the number of clusters; this parameter is effectively learned from the data. The tradeoff parameter λ is artificial and can be easily determined: since any desired K∗ has trace exactly equal to n, we simply choose the smallest λ such that the trace of the optimal solution is at least n. This can be done by, e.g., bisection, which is described below. (b) It is possible to obtain tighter convex relaxations by adding more constraints, such as the diagonal entry constraints ki,i = 1, ∀i, the positive semidefinite constraint K 0, or even the triangular inequalities ki,j + kj,k − ki,k ≤ 1. Indeed, this is done by Mathieu and Schudy (2010). Note that the guarantees for our formulation (to be presented in the next subsection) automatically imply guarantees for any other tighter relaxations. We choose to focus on our (looser) formulation for two reasons. First, and most importantly, even with the extra\nconstraints, Mathieu and Schudy (2010) do not deliver better exact recovery guarantees (cf. Table 1). In fact, we show in Section 2.3 that our results are near optimal in some important regimes, so tighter relaxations do not seem to provide additional benefits in exact recovery. Second, our formulation can be solved efficiently using existing Augmented Lagrangian Multiplier methods (Lin et al., 2009). This is no longer the case with the Θ(n3) triangle inequality constraints enforced by Mathieu and Schudy (2010), and solving it as a standard SDP is only feasible for small graphs.\nWe are interested in the case when the convex program (1) produces an optimal solution K that is a block-diagonal matrix and corresponds to an ideal clustering.\nDefinition 1 (Validity) The convex program (1) is said to produce a valid output if the low-rank matrix part K of the optimal solution corresponds to a graph of disjoint cliques; i.e., its rows and columns can be re-ordered to yield a block-diagonal matrix with all ones for each block.\nValidity of a given K can be easily checked via elementary re-ordering operations.2 Our first simple but useful insight is that whenever the convex program (1) yields a valid solution, it is the disagreement minimizer.\nTheorem 2 For any λ > 0, if the solution of (1) is valid, then it is the clustering that minimizes the number of observed disagreements.\nOur complete clustering procedure is given as Algorithm 1. It takes the adjacency matrix of the graph A and outputs either the optimal clustering or declares failure. Setting the parameter λ is done via binary search. The initial value of λ is not crucial; we use λ = 1\n32 √ p̄0n based on our theoretical analysis in the next sub-section, where p̄0 is the empirical fraction of observed pairs. To solve the optimization problem (1), we use the fast algorithm developed by Lin et al. (2009), which is tailored for matrix splitting and takes advantage of the sparsity of the observations. By Theorem 2, whenever the algorithm results in a valid K, we have found the optimal clustering.\nAlgorithm 1 Optimal-Cluster(A) λ← 1\n32 √ p̄0n\nwhile not terminated do Solve (1) to obtain the solution (B,K) if K is valid then\nOutput the clustering in K and EXIT. else if trace(K) > n then λ← λ/2 else if trace(K) < n then λ← 2λ\nend if end while Declare Failure.\n2. If we re-order a valid K such that identical rows and columns appear together, it will become block-diagonal."
    }, {
      "heading" : "2.2 Performance Analysis",
      "text" : "For the main analytical contribution of this paper, we provide conditions under which the above algorithm will find the clustering that minimizes the number of disagreements among the observed entries. In particular, we characterize its performance under the standard and classical planted partition/stochastic block model with partial observations, which we now describe.\nDefinition 3 (Planted Partition Model with Partial Observations) Suppose that n nodes are partitioned into r clusters, each of size at least Kmin. Let K∗ be the low-rank matrix corresponding to this clustering (as described above). The adjacency matrix A of the graph is generated as follows: for each pair of nodes (i, j) in the same cluster, ai,j =? with probability 1 − p0, ai,j = 1 with probability p0p, or aij = 0 otherwise, independent of all others; similarly, for (i, j) in different clusters, ai,j =? with probability 1− p0, ai,j = 1 with probability p0q, or ai,j = 0 otherwise.\nUnder the above model, the graph is observed at locations chosen at random with probability p0. In expectation a fraction of 1 − p of the in-cluster observations are disagreements; similarly, the fraction of disagreements in the across-cluster observations is q. Let B∗ = PΩobs(A + I − K∗) be the matrix of observed disagreements for the original clustering; note that the support of B∗ is contained in Ωobs. The following theorem provides a sufficient condition for our algorithm to recover the original clustering (B∗,K∗) with high probability. Combined with Theorem 2, it also shows that under the same condition, the original clustering is disagreement minimizing with high probability.\nTheorem 4 Let τ = max{1 − p, q}. There exist universal positive constants c and C such that, with probability at least 1− cn−10, the original clustering (B∗,K∗) is the unique optimal solution of (1) with λ = 132√np0 provided that\np0 (1− 2τ)2 ≥ C n log2 n\nK2min . (2)\nNote that the quantity τ is (an upper bound of) the probability of having a disagreement, and 1− 2τ is (a lower bound of) the density gap p− q. The sufficient condition in the theorem is given in terms of the three parameters that define problem: the minimum cluster sizeKmin, the density gap 1−2τ , and the observation probability p0. We remark on these parameters.\n• Minimum cluster size Kmin. Since the left hand side of the condition (2) in Theorem 4 is no more than 1, it imposes a lower-bound Kmin = Ω̃( √ n) on the cluster sizes. This means that\nour method can handle a growing number Õ( √ n) of clusters. The lower-bound on Kmin is attained when 1 − 2τ and p0 are both Θ(1), i.e., not decreasing as n grows. Note that all relevant works require a lower-bound at least as strong as ours (cf. Table 1). • Density gap 1− 2τ . When p0 = Θ(1), our result allows this gap to be vanishingly small, i.e., Ω̃ ( √\nn Kmin\n) , where a larger Kmin allows for a smaller gap. As we mentioned in Section 1.2,\nthis matches the best available results (cf. Table 1), including those in Mathieu and Schudy (2010) and Oymak and Hassibi (2011), which use tighter convex relaxations that are more computationally demanding. We note that directly applying existing results in the low-rankplus-sparse literature (Candès et al., 2011; Li, 2013) leads to weaker results, where the gap be bounded below by a constant.\n• Observation probability p0. When 1 − 2τ = Θ(1), our result only requires a vanishing fraction of observations, i.e., p0 can be as small as Θ̃\n( n\nK2min\n) ; a larger Kmin allows for a\nsmaller p0. As mentioned in Section 1.2, this scaling is better than prior results we know of.\n• Tradeoffs. A novel aspect of our result is that it shows an explicit tradeoff between the observation probability p0 and the density gap 1 − 2τ . The left hand side of (2) is linear in p0 and quadratic in 1 − 2τ . This means if the number of observations is four times larger, then we can handle a 50% smaller density gap. Moreover, p0 can go to zero quadratically faster then 1 − 2τ . Consequently, treating missing observations as disagreements would lead to quadratically weaker results. This agrees with the intuition that handling missing entries with known locations is easier than correcting disagreements whose locations are unknown.\nWe would like to point out that our algorithm has the capability to handle outliers. Suppose there are some isolated nodes which do not belong to any cluster, and they connect to each other and each node in the clusters with probability at most τ , with τ obeying the condition (2) in Theorem 4. Our algorithm will classify all these edges as disagreements, and hence automatically reveal the identity of each outlier. In the output of our algorithm, the low rank part K will have all zeros in the columns and rows corresponding to outliers—all their edges will appear in the disagreement matrix B."
    }, {
      "heading" : "2.3 Lower Bounds",
      "text" : "We now discuss the tightness of Theorem 4. Consider first the case where Kmin = Θ(n), which means there are a constant number of clusters. We establish a fundamental lower bound on the density gap 1−2τ and the observation probability p0 that are required for any algorithm to correctly recover the clusters.\nTheorem 5 Under the planted partition model with partial observations, suppose the true clustering is chosen uniformly at random from all possible clusterings with equal cluster size K. If K = Θ(n) and τ = 1 − p = q > 1/100, then for any algorithm to correctly identify the clusters with probability at least 34 , we need\np0(1− 2τ)2 ≥ C 1\nn ,\nwhere C > 0 is an absolute constant.\nTheorem 5 generalizes a similar result in Chaudhuri et al. (2012), which does not consider partial observations. The theorem applies to any algorithm regardless of its computational complexity, and characterizes the fundamental tradeoff between p0 and 1 − 2τ . It shows that when Kmin = Θ(n), the requirement for 1− 2τ and p0 in Theorem 4 is optimal up to logarithmic factors, and cannot be significantly improved by using more complicated methods.\nFor the general case with Kmin = O(n), only part of the picture is known. Using non-rigorous arguments, Decelle et al. (2011) show that 1 − 2τ & √ n\nKmin is necessary when τ = Θ(1) and the\ngraph is fully observed; otherwise recovery is impossible or computationally hard. According to this lower-bound, our requirement on the density gap 1 − 2τ is probably tight (up to log factors) for all Kmin. However, a rigorous proof of this claim is still lacking, and seems to be a difficult problem. Similarly, the tightness of our condition on p0 and the tradeoff between p0 and τ is also unclear in this regime."
    }, {
      "heading" : "3. Proofs",
      "text" : "In this section, we prove Theorems 2 and 4. The proof of Theorem 5 is deferred to Appendix B."
    }, {
      "heading" : "3.1 Proof of Theorem 2",
      "text" : "We first prove Theorem 2, which says that if the optimization problem (1) produces a valid matrix, i.e., one that corresponds to a clustering of the nodes, then this is the disagreement minimizing clustering. Consider the following non-convex optimization problem\nmin B,K λ ‖B‖1 + ‖K‖∗\ns.t. PΩobs(B + K) = PΩobs(I + A), K is valid,\n(3)\nand let (B,K) be any feasible solution. Since K represents a valid clustering, it is positive semidefinite and has all ones along its diagonal. Therefore, it obeys ‖K‖∗ = trace(K) = n. On the other hand, because both K−I and A are adjacency matrices, the entries of B = I+A−K in Ωobs must be equal to −1, 1 or 0 (i.e., it is a disagreement matrix). Clearly any optimal B must have zeros at the entries in Ωcobs. Hence ‖B‖1 = ‖PΩobs(B)‖0 when K is valid. We thus conclude that the above optimization problem (3) is equivalent to minimizing ‖PΩobs(B)‖0 subject to the same constraints. This is exactly the minimization of the number of disagreements on the observed edges. Now notice that (1) is a relaxed version of (3). Therefore, if the optimal solution of (1) is valid and thus feasible to (3), then it is also optimal to (3), the disagreement minimization problem."
    }, {
      "heading" : "3.2 Proof of Theorem 4",
      "text" : "We now turn to the proof of Theorem 4, which provides guarantees for when the convex program (1) recovers the true clustering (B∗,K∗)."
    }, {
      "heading" : "3.2.1 PROOF OUTLINE AND PRELIMINARIES",
      "text" : "We overview the main steps in the proof of Theorem 4; details are provided in Sections 3.2.2–3.2.4 to follow. We would like to show that the pair (B∗,K∗) corresponding to the true clustering is the unique optimal solution to our convex program (1). This involves the following three steps.\nStep 1: We show that it suffices to consider an equivalent model for the observation and disagreements. This model is easier to handle, especially when the observation probability and density gap are vanishingly small, which is the regime of interest in this paper.\nStep 2: We write down the sub-gradient based first-order sufficient conditions that need to be satisfied for (B∗,K∗) to be the unique optimum of (1). In our case, this involves showing the existence of a matrix W—the dual certificate—that satisfies certain properties. This step is technical—requiring us to deal with the intricacies of sub-gradients since our convex function is not smooth—but otherwise standard. Luckily for us, this has been done previously (Chandrasekaran et al., 2011; Candès et al., 2011; Li, 2013).\nStep 3: Using the assumptions made on the true clustering K∗ and its disagreements B∗, we construct a candidate dual certificate W that meets the requirements in step 2, and thus certify (B∗,K∗) as being the unique optimum.\nThe crucial Step 3 is where we go beyond the existing literature on matrix splitting (Chandrasekaran et al., 2011; Candès et al., 2011; Li, 2013). These results assume the observation probability and/or density gap is at least a constant, and hence do not apply to our setting. Here we provide a refined analysis, which leads to better performance guarantees than those that could be obtained via a direct application of existing sparse and low-rank matrix splitting results.\nNext, we introduce some notations used in the rest of the proof of the theorem. The following definitions related to K∗ are standard. By symmetry, the SVD of K∗ has the form UΣUT , where U ∈ Rn×r contains the singular vectors of K∗. We define the subspace\nT , { UXT + YUT : X,Y ∈ Rn×r } ,\nwhich is spanned of all matrices that share either the same column space or the same row space as K∗. For any matrix M ∈ Rn×n, its orthogonal projection to the space T is given by PT (M) = UUTM + MUUT −UUTMUUT . The projection onto T ⊥, the complement orthogonal space of T , is given by PT ⊥(M) = M− PT (M).\nThe following definitions are related to B∗ and partial observations. Let Ω∗ = {(i, j) : b∗i,j 6= 0} be the set of matrix entries corresponding to the disagreements. Recall that Ωobs is the set of observed entries. For any matrix M and entry set Ω0, we let PΩ0 (M) ∈ Rn×n be the matrix obtained from M by setting all entries not in the set Ω0 to zero. We write Ω0 ∼ Ber0(p) if the entry set Ω0 does not contain the diagonal entries, and each pair (i, j) and (j, i) (i 6= j) is contained in Ω0 with probability p, independent all others; Ω0 ∼ Ber1(p) is defined similarly except that Ω0 contains all the diagonal entries. Under our partially observed planted partition model, we have Ωobs ∼ Ber1(p0) and Ω∗ ∼ Ber0(τ).\nSeveral matrix norms are used. ‖M‖ and ‖M‖F represent the spectral and Frobenius norms of the matrix M, respectively, and ‖M‖∞ , maxi,j |mi,j | is the matrix infinity norm."
    }, {
      "heading" : "3.2.2 STEP 1: EQUIVALENT MODEL FOR OBSERVATIONS AND DISAGREEMENTS",
      "text" : "It is easy show that increasing p or decreasing q can only make the probability of success higher, so without loss of generality we assume 1 − p = q = τ . Observe that the probability of success is completely determined by the distribution of (Ωobs,B∗) under the planted partition model with partial observations. The first step is to show that it suffices to consider an equivalent model for generating (Ωobs,B∗), which results in the same distribution but is easier to handle. This is in the same spirit as Candès et al. (2011, Theorems 2.2 and 2.3) and Li (2013, Section 4.1). In particular, we consider the following procedure:\n1. Let Γ ∼ Ber1 (p0(1− 2τ)), and Ω ∼ Ber0 (\n2p0τ 1−p0+2p0τ\n) . Let Ωobs = Γ ∪ Ω.\n2. Let S be a symmetric random matrix whose upper-triangular entries are independent and satisfy P(si,j = 1) = P(si,j = −1) = 12 .\n3. Define Ω′ ⊆ Ω as Ω′ = { (i, j) : (i, j) ∈ Ω, si,j = 1− 2k∗i,j }\n. In other words, Ω′ is the entries of S whose signs are consistent with a disagreement matrix.\n4. Define Ω∗ = Ω′\\Γ, and Γ̃ = Ωobs\\Ω∗.\n5. Let B∗ = PΩ∗(S).\nIt is easy to verify that (Ωobs,B∗) has the same distribution as in the original model. In particular, we have P[(i, j) ∈ Ωobs] = p0, P[(i, j) ∈ Ω∗, (i, j) ∈ Ωobs] = p0τ and P[(i, j) ∈ Ω∗, (i, j) /∈ Ωobs] = 0, and observe that given K∗, B∗ is completely determined by its support Ω∗.\nThe advantage of the above model is that Γ and Ω are independent of each other, and S has random signed entries. This facilitates the construction of the dual certificate, especially in the regime of vanishing p0 and ( 1 2 − τ ) considered in this paper. We use this equivalent model in the rest of the proof."
    }, {
      "heading" : "3.2.3 STEP 2: SUFFICIENT CONDITIONS FOR OPTIMALITY",
      "text" : "We state the first-order conditions that guarantee (B∗,K∗) to be the unique optimum of (1) with high probability. Here and henceforth, by with high probability we mean with probability at least 1− cn−10 for some universal constant c > 0. The following lemma follows from Theorem 4.4 in Li (2013) and the discussion thereafter.\nLemma 6 (Optimality Condition) Suppose ∥∥∥ 1(1−2τ)p0PT PΓPT − PT ∥∥∥ ≤ 12 . Then (B∗,K∗) is the unique optimal solution to (1) with high probability if there exists W ∈ Rn×n such that\n1. ∥∥PT (W + λPΩS−UU>)∥∥F ≤ λn2 ,\n2. ‖PT ⊥(W + λPΩS)‖ ≤ 14 ,\n3. PΓc(W) = 0,\n4. ‖PΓ(W)‖∞ ≤ λ 4 . Lemma 9 in the appendix guarantees that the condition ∥∥∥ 1(1−2τ)p0PT PΓPT − PT ∥∥∥ ≤ 12 is satisfied with high probability under the assumption of Theorem 4. It remains to show the existence of a desired dual certificate W which satisfies the four conditions in Lemma 6 with high probability."
    }, {
      "heading" : "3.2.4 STEP 3: DUAL CERTIFICATE CONSTRUCTION",
      "text" : "We use a variant of the so-called golfing scheme (Candès et al., 2011; Gross, 2011) to construct W. Our application of golfing scheme, as well as its analysis, is different from previous work and leads to stronger guarantees. In particular, we go beyond existing results by allowing the fraction of observed entries and the density gap to be vanishing.\nBy definition in Section 3.2.2, Γ obeys Γ ∼ Ber1(p0(1−2τ)). Observe that Γ may be considered to be generated by Γ = ⋃ 1≤k≤k0 Γk, where the sets Γk ∼ Ber1(t) are independent; here the parameter t obeys p0(1 − 2τ) = 1 − (1 − t)k0 , and k0 is chosen to be d5 log ne. This implies t ≥ p0(1−2τ)/k0 ≥ C0 n lognK2min for some constantC0, with the last inequality holds under the assumption of Theorem 4. For any random entry set Ω0 ∼ Ber1(ρ), define the operator RΩ0 : Rn×n 7→ Rn×n by\nRΩ0(M) = n∑ i=1 mi,ieie T i + ρ −1 ∑ 1≤i<j≤n δijmi,j ( eie T j + eje T i ) ,\nwhere δij is the indicator random variable with δij = 1 if (i, j) ∈ Ω0 and 0 otherwise, and ei is the i-th standard basis in Rn×n, i.e., the column vector with 1 in its i-th entry and 0 elsewhere.\nWe now define our dual certificate. Let W = Wk0 , where Wk0 is defined recursively by setting W0 = 0 and for all k = 1, 2, . . . , k0,\nWk = Wk−1 +RΓkPT ( UUT − λPT (PΩ(S))−Wk−1 ) .\nClearly the equality condition in Lemma 6 is satisfied. It remains to show that W also satisfies the inequality conditions with high probability. The proof makes use of the technical Lemmas 9– 12 given in the appendix. For convenience of notation, we define the quantity ∆k = UUT − λPT (PΩ(S))− PT (Wk), and use the notation\nk∏ i=1 (PT − PTRΓiPT ) = (PT − PTRΓkPT ) · · · (PT − PTRΓ1PT ),\nwhere the order of multiplication is important. Observe that by construction of W, we have\n∆k = k∏ i=1 (PT − PTRΓiPT )(UUT − λPT PΩ(S)), k = 1, . . . , k0, (4)\nWk0 = k0∑ k=1 RΓk∆k−1. (5)\nWe are ready to prove that W satisfies inequalities 1, 2 and 4 in Lemma 6. Inequality 1: Thanks to (4), we have the following geometric convergence :\n∥∥∥PT (W + λPΩS−UU>)∥∥∥ F = ‖∆k0‖F\n≤ ( k0∏ k=1 ‖PT − PTRΓkPT ‖ )∥∥UUT − λPT PΩ(S)∥∥F (a)\n≤ e−k0( ∥∥UUT∥∥\nF + λ ‖PT PΩ(S)‖F )\n(b) ≤ n−5(n+ λ · n) ≤ (1 + λ)n−4 (c) ≤ 1 2n2 λ.\nHere, the inequality (a) follows Lemma 9 with 1 = e−1, (b) follows from our choices of λ and k0 and the fact that ‖PT PΩ(S)‖F ≤ ‖PΩ(S)‖F ≤ n, and (c) holds under the assumption λ ≥ 1 32 √ n in the theorem. Inequality 4: We have\n‖PΓ(W)‖∞ = ‖PΓ(Wk0)‖∞ ≤ k0∑ k=1 ‖RΓi∆i−1‖∞ ≤ t −1 k0∑ k=1 ‖∆k−1‖∞ ,\nwhere the first inequality follows from (5) and the triangle inequality. We proceed to obtain\nk0∑ k=1 ‖∆k−1‖∞ (a) = k0∑ k=1 ∥∥∥∥∥ k−1∏ i=1 (PT − PTRΓiPT )(UUT − λPT PΩ(S)) ∥∥∥∥∥ ∞\n(b) ≤ k0∑ k=1 ( 1 2 )k ∥∥UUT − λPT PΩ(S)∥∥∞ (c) ≤ 1 Kmin + λ √ p0n log n K2min , (6)\nwhere (a) follows from (4), (b) follows from Lemma 11 and (c) follows from Lemma 12. It follows that\n‖PΓ(W)‖∞ ≤ 1\nt\n( 1\nKmin + n log n K2min λ ) ≤ k0 p0(1− 2τ) ( 1 Kmin + λ √ p0n log n K2min ) ≤ 1 4 λ,\nwhere the last inequality holds under the assumption of Theorem 4 Inequality 2: Observe that by the triangle inequality, we have\n‖PT ⊥(W + λPΩ(S))‖ ≤ λ ‖PT ⊥(PΩ(S))‖+ ‖PT ⊥(Wk0)‖ .\nFor the first term, standard results on the norm of a matrix with i.i.d. entries (e.g., see Vershynin 2010) give\nλ ‖PT ⊥(PΩ(S))‖ ≤ λ ‖PΩ(S)‖ ≤ 1 32 √ p0n · 4 √\n2p0τn 1− p0 + 2p0τ ≤ 1 8\nIt remains to show that the second term is bounded by 18 . To this end, we observe that\n‖PT ⊥(Wk0)‖ (a) = k0∑ k=1 ‖PT ⊥ (RΓk∆k−1 −∆k−1)‖\n≤ k0∑ k=1 ‖(RΓk − I) ∆k−1‖\n(b) ≤ C √ n log n\nt\nk0∑ k=1 ‖∆k−1‖∞\n(c) ≤ C\n√ k0n log n\np0(1− 2τ)\n( 1\nKmin + λ\n√ p0n log n\nK2min\n) ≤ 1\n8 ,\nwhere in (a) we use (5) and the fact that ∆k ∈ T , (b) follows from Lemma 10, and (c) follows from (6). This completes the proof of Theorem 4."
    }, {
      "heading" : "4. Experimental Results",
      "text" : "We explore via simulation the performance of our algorithm as a function of the values of the model parameters (n,Kmin, p0, τ). We see that the performance matches well with the theory.\nIn the experiment, each test case is constructed by generating a graph with n nodes divided into clusters of equal sizeKmin, and then placing a disagreement on each pair of node with probability τ independently. Each node pair is then observed with probability p0. We then run Algorithm 1, where the optimization problem (1) is solved using the fast algorithm in Lin et al. (2009).. We check if the algorithm successfully outputs a solution that equals to the underlying true clusters. In the first set of experiments, we fix τ = 0.2 and Kmin = n/4 and vary p0 and n. For each (p0, n), we repeat the experiment for 5 times and plot the probability of success in the left pane of Figure 2.\nOne observes that our algorithm has better performance with larger p0 and n, and the success probability exhibits a phase transition. Theorem 4 predicts that, with τ fixed and Kmin = n/4, the transition occurs at p0 ∝ n log\n2 n K2min ∝ log 2 n n ; in particular, if we plot the success probability against\nthe control parameter p0n log2 n , all curves should align with each other. Indeed, this is precisely what we see in the right pane of Figure 2 where we use p0nlogn as the control parameter. This shows that Theorem 4 gives the correct scaling between p0 and n up to an extra log factor.\nIn a similar fashion, we run another three sets of experiments with the following settings: (1) n = 1000 and τ = 0.2 with varying (p0,Kmin); (2) Kmin = n/4 and p0 = 0.2 with varying (τ, n); (3) n = 1000 and p0 = 0.6 with varying (τ,Kmin). The results are shown in Figures 3, 4 and 5; note that each x-axis corresponds to a control parameter chosen according to the scaling predicted by Theorem 4. Again we observe that all the curves roughly align, indicating a good match with the theory. In particular, by comparing Figures 2 and 4 (or Figures 3 and 5), one verifies the quadratic tradeoff between observations and disagreements (i.e., p0 vs. 1− 2τ ) as predicted by Theorem 4.\nFinally, we compare the performance of our method with spectral clustering, a popular method for graph clustering. For spectral clustering, we first impute the missing entries of the adjacency\nmatrix with either zeros or random 1/0’s. We then compute the first k principal components of the adjacency matrix, and run k-means clustering on the principal components (von Luxburg, 2007); here we set k equal to the number of clusters. The adjacency matrix is generated in the same fashion as before using the parameters n = 2000,Kmin = 200 and τ = 0.1. We vary the observation probability p0 and plot the success probability in Figure 6. It can be observed that our method outperforms spectral clustering with both imputation schemes; in particular, it requires fewer observations."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We proposed a convex optimization formulation, based on a reduction to decomposing low-rank and sparse matrices, to address the problem of clustering partially observed graphs. We showed that under a wide range of parameters of the planted partition model with partial observations, our method is guaranteed to find the optimal (disagreement-minimizing) clustering. In particular, our method succeeds under higher levels of noise and/or missing observations than existing methods in\nthis setting. The effectiveness of the proposed method and the scaling of the theoretical results are validated by simulation studies.\nThis work is motivated by graph clustering applications where obtaining similarity data is expensive and it is desirable to use as few observations as possible. As such, potential directions for future work include considering different sampling schemes such as active sampling, as well as dealing with sparse graphs with very few connections."
    }, {
      "heading" : "Acknowledgments",
      "text" : "S. Sanghavi would like to acknowledge DTRA grant HDTRA1-13-1-0024 and NSF grants 1302435, 1320175 and 0954059. H. Xu is partially supported by the Ministry of Education of Singapore\nthrough AcRF Tier Two grant R-265-000-443-112. The authors are grateful to the anonymous reviewers for their thorough reviews of this work and valuable suggestions on improving the manuscript."
    }, {
      "heading" : "Appendix A. Technical Lemmas",
      "text" : "In this section, we provide several auxiliary lemmas required in the proof of Theorem 4. We will make use of the non-commutative Bernstein inequality. The following version is given by Tropp (2012).\nLemma 7 (Tropp, 2012) Consider a finite sequence {Mi} of independent, random n×n matrices that satisfy the assumption EMi = 0 and ‖Mi‖ ≤ D almost surely. Let\nσ2 = max {∥∥∥∥∥∑ i E [ MiM > i ]∥∥∥∥∥ , ∥∥∥∥∥∑ i E [ M>i Mi ]∥∥∥∥∥ } .\nThen for all θ > 0 we have\nP [∥∥∥∑Mi∥∥∥ ≥ θ] ≤ 2n exp(− θ2\n2σ2 + 2Dθ/3\n) .\n≤\n{ 2n exp ( − 3θ2\n8σ2 ) , for θ ≤ σ2D ;\n2n exp ( − 3θ8D ) , for θ ≥ σ2D .\n(7)\nRemark 8 When n = 1, this becomes the standard two-sided Bernstein inequality.\nWe will also make use of the following estimate, which follows from the structure of U.∥∥∥PT (eie>j )∥∥∥2 F = ∥∥UUT ei∥∥2 + ∥∥UUT ej∥∥2 − ∥∥UUT ei∥∥2 ∥∥UUT ej∥∥2 ≤ 2n K2min , ∀1 ≤ i, j ≤ n.\nThe first auxiliary lemma controls the operator norm of certain random operators. A similar result was given in Candès et al. (2011, Theorem 4.1). Our proof is different from theirs.\nLemma 9 Suppose Ω0 is a set of entries obeying Ω0 ∼ Ber1(ρ). Consider the operator PT − PTRΩ0PT . For some constant C0 > 0, we have\n‖PT − PTRΩ0PT ‖ < 1\nwith high probability provided that ρ ≥ C0 n logn 21K2min and 1 ≤ 1.\nProof For each (i, j), define the indicator random variable δij = 1{(i,j)∈Ω0}. We observe that for any matrix M ∈ T ,\n(PTRΩ0PT − PT ) M = ∑\n1≤i<j≤n Sij(M)\n, ∑\n1≤i<j≤n\n( ρ−1δij − 1 ) 〈 PT (eie>j ), M 〉 PT (eie>j + eje>i ).\nHere Sij : Rn×n 7→ Rn×n is a linear self-adjoint operator with E [Sij ] = 0. Using the fact that PT (eie>j ) = ( PT (eje>i ) )> and M is symmetric, we obtain the bounds ‖Sij‖ ≤ ρ−1 ∥∥∥PT (eie>j )∥∥∥ F ∥∥∥PT (eie>j + eje>i )∥∥∥ F\n≤ ρ−1 · 2 ∥∥∥PT (eie>j )∥∥∥2\nF ≤ 4n K2minρ ,\nand∥∥∥∥∥∥E  ∑ 1≤i<j≤n S2ij(M) ∥∥∥∥∥∥ F\n= ∥∥∥∥∥∥ ∑ 1≤i<j≤n E [ (ρ−1δ (k) ij − 1) 2 ] 〈 PT (eie>j ), M 〉〈 PT (eie>j + eje>i ), eie>j 〉 PT (eie>j + eje>i ) ∥∥∥∥∥∥ F = ( ρ−1 − 1 ) ∥∥∥∥∥∥ ∑ 1≤i<j≤n 2 ∥∥∥PT (eie>j )∥∥∥2 F mi,jPT (eie>j + eje>i ) ∥∥∥∥∥∥ F ≤ ( ρ−1 − 1 ) ∥∥∥∥∥∥ ∑ 1≤i<j≤n 2 ∥∥∥PT (eie>j )∥∥∥2 F mi,j(eie > j + eje > i ) ∥∥∥∥∥∥ F ≤ ( ρ−1 − 1\n) 4n K2min ∥∥∥∥∥∥ ∑ 1≤i<j≤n mi,j(eie > j + eje > i ) ∥∥∥∥∥∥ F\n= ( ρ−1 − 1 ) 4n K2min ‖M‖F ,\nwhich means ∥∥∥E [∑1≤i<j≤n S2ij]∥∥∥ ≤ 4nK2minρ . Applying the first inequality in the Bernstein inequality (7) gives\nP [∥∥∥∑1≤i<j≤n Sij∥∥∥ ≥ 1] ≤ 2n2−2β\nprovided ρ ≥ 64βn logn 3K2min 2 1 and 1 < 1.\nThe next lemma bounds the spectral norm of certain symmetric random matrices. A related result for non-symmetric matrices appeared in Candès and Recht (2009, Theorem 6.3).\nLemma 10 Suppose Ω0 is a set of entries obeying Ω0 ∼ Ber1(ρ), and M is a fixed n×n symmetric matrix. Then for some constant C0 > 0, we have\n‖(I −RΩ0)M‖ < √ C0 n log n\nρ ‖M‖∞,\nwith high probability provided that ρ ≥ C0 lognn .\nProof Define δij as before. Notice that\nRΩ0(M)−M = ∑ i<j Sij , ∑ i<j (ρ−1δij − 1)mi,j ( eie > j + eje > i ) .\nHere the symmetric matrix Sij ∈ Rn×n satisfies E [Sij ] = 0, ‖Sij‖ ≤ 2ρ−1 ‖M‖∞ and the bound\n∥∥∥E [∑i<j S2ij]∥∥∥ = (ρ−1 − 1) ∥∥∥∥∥∥ ∑ i<j m2i,j ( eie > i + eje > j )∥∥∥∥∥∥ ≤ ( ρ−1 − 1 ) ∥∥∥∥∥∥diag ∑\nj m21,j , . . . , ∑ j m2n,j ∥∥∥∥∥∥ ≤ ( ρ−1 − 1 ) n ‖M‖2∞ ≤ 2ρ −1n ‖M‖2∞ .\nWhen ρ ≥ 16β logn3n , we apply the first inequality in the Bernstein inequality (7) to obtain\nP [∥∥∥∑i<j Sij∥∥∥ ≥ √ 16βn log n\n3ρ ‖M‖∞\n] ≤ 2n exp ( − 3 · 16βn logn3ρ ‖M‖ 2 ∞\n8 · 2nρ ‖M‖ 2 ∞\n) ≤ 2n1−β.\nThe conclusion follows by choosing a sufficiently large constant β.\nThe third lemma bounds the infinity norm of certain random symmetric matrices. A related result is given in Candès et al. (2011, Lemma 3.1).\nLemma 11 Suppose Ω0 is a set of entries obeying Ω0 ∼ Ber1(ρ), and M ∈ T is a fixed symmetric n× n matrix. Then for some constant C0 > 0, we have\n‖(PT − PTRΩ0PT )M‖∞ < 3‖M‖∞,\nwith high probability provided that ρ ≥ C0 n logn 23K2min and 3 ≤ 1.\nProof Define δij as before. Fix an entry index (a, b). Notice that\n(PTRΩ0PTM− PTM)a,b = ∑ i<j ξij , ∑ i<j 〈 (ρ−1δ (k) ij − 1)mi,jPT ( eie > j + eje > i ) , eae > b 〉 .\nThe random variable ξij satisfies E [ξij ] = 0 and obeys the bounds\n|ξij | ≤ 2p−1 ∥∥∥PT (eie>j )∥∥∥\nF ∥∥∥PT (eae>b )∥∥∥ F |mi,j | ≤ 4n K2minρ ‖M‖∞\nand ∣∣∣∣∣∣E ∑ i<j ξ2ij ∣∣∣∣∣∣ = ∣∣∣∣∣∣ ∑ i<j E [ (ρ−1δ (k) ij − 1) 2 ] m2i,j 〈 PT ( eie > j + eje > i ) , eae > b 〉2∣∣∣∣∣∣ ≤ ( ρ−1 − 1 ) ‖M‖2∞\n∑ i<j 〈 eie > j + eje > i , PT (eae>b ) 〉2 ≤ 2 ( ρ−1 − 1 ) ‖M‖2∞ ∥∥∥PT (eae>b )∥∥∥2 F\n≤ 2 ( ρ−1 − 1 ) 2n K2min ‖M‖2∞ ≤ 4n K2minρ ‖M‖2∞ .\nWhen ρ ≥ 64βn logn 3K2min 2 3 and 3 ≤ 1, we apply the first inequality in the Bernstein inequality (7) with n = 1 to obtain\nP [∣∣∣(PTRΩ0PTM− PTM)a,b∣∣∣ ≥ 3 ‖M‖∞] ≤ 2 exp − 3 23 ‖M‖2∞ 8 4n K2minρ ‖M‖2∞  ≤ 2n−2β. Applying the union bound then yields\nP [‖PTRΩ0PTM− PTM‖∞ ≥ 3 ‖M‖∞] ≤ 2n 2−2β.\nThe last lemma bounds the matrix infinity norm of PT PΩ(S) for a ±1 random matrix S. Lemma 12 Suppose Ω ∼ Ber0 (\n2p0τ 1−p0+2p0τ\n) and S ∈ Rn×n has i.i.d. symmetric ±1 entries .\nUnder the assumption of Theorem 4, for some constant C0, we have with high probability\n‖PT PΩ(S)‖∞ ≤ C0\n√ p0n log n\nK2min .\nProof By triangle inequality, we have ‖PT PΩ(S)‖∞ ≤ ∥∥UUTPΩ(S)∥∥∞ + ∥∥PΩ(S)UUT∥∥∞ + ∥∥UUTPΩ(S)UUT∥∥∞ ,\nso it suffices to show that each of these three terms are bounded by C √\np0n logn K2min w.h.p. for some constant C. Under the assumption on Ω and S in the lemma statement, each pair of symmetric entries of PΩ(S) equals ±1 with probability ρ , p0τ1−p0+2p0τ and 0 otherwise; notice that ρ ≤ p0 2\nsince τ ≤ 12 . Let ( s(i) )T\nbe the ith row of UUT . From the structure of U, we know that for all i and j, ∣∣∣s(i)j ∣∣∣ ≤ 1Kmin ,\nand for all i,\nn∑ j=1 ( s (i) j )2 ≤ 1 Kmin .\nWe now bound ∥∥UUTPΩ(S)∥∥∞. For simplicity, we focus on the (1, 1) entry of UUTPΩ(S) and\ndenote this random variable asX . We may writeX as X = ∑\ni s (1) i (PΩ(S))i,1 , for which we have\nE [ s\n(1) i (PΩ(S))i,1 ] = 0,∣∣∣s(1)i (PΩ(S))i,1∣∣∣ ≤ ∣∣∣s(1)i ∣∣∣ ≤ 1Kmin , a.s.\nVar (X) = ∑\ni:(i,1)∈Ω\n(s (1) i ) 2 · 2ρ ≤ p0 Kmin .\nApplying the standard Bernstein inequality then gives\nP [ |X| > C √ p0n log n\nK2min\n] ≤ 2 exp [ − ( C2 p0n log n\nK2min\n) / ( 2 p0 Kmin + 2C √ p0n log n 3K2min )] .\nUnder the assumption of Theorem 4, the right hand side above is bounded by 2n−12. It follows from the union bound that ∥∥UUTPΩ(S)∥∥∞ ≤ C√p0n lognK2min w.h.p. Clearly, the same bound holds for ∥∥PΩ(S)UUT∥∥∞. Finally, let K be the size of the cluster that node j is in. Observe that due to the structure of UU>, we have( UUTPΩ(S)UUT ) i,j\n= ∑ l ( UU>PΩ(S) ) i,l ( UU> ) l,j ≤ 1 K ·K · ∥∥∥UU>PΩ(S)∥∥∥ ∞ ,\nwhich implies ∥∥UUTPΩ(S)UUT∥∥∞ ≤ ∥∥UU>PΩ(S)∥∥∞ . This completes the proof of the lemma."
    }, {
      "heading" : "Appendix B. Proof of Theorem 5",
      "text" : "We use a standard information theoretical argument, which improves upon a related proof by Chaudhuri et al. (2012). Let K be the size of the clusters (which are assumed to have equal size). For simplicity we assume n/K is an integer. Let F be the set of all possible partition of n nodes into n/K clusters of equal size K. Using Stirling’s approximation, we have\nM , |F| = 1 (n/K)!\n( n\nK )( n−K K ) · · · ( K K ) ≥ ( n 3K )n(1− 1 K ) ≥ c 1 2 n 1 ,\nwhich holds for K = Θ(n). Suppose the clustering Y is chosen uniformly at random from F , and the graph A is generated from Y according to the planted partition model with partial observations, where we use aij =? for\nunobserved pairs. We use PA|Y to denote the distribution of A given Y. Let Ŷ be any measurable function of the observation A. A standard application of Fano’s inequality and the convexity of the mutual information (Yang and Barron, 1999) gives\nsup Y ∈F\nP [ Ŷ 6= Y|Y ] ≥ 1−\nM−2 ∑ Y(1),Y(2)∈F D ( PA|Y(1)‖PA|Y(2) ) + log 2\nlogM , (8)\nwhere D(·‖·) denotes the KL-divergence. We now upper bound this divergence. Given Y(l), l = 1, 2, the ai,j’s are independent of each other, so we have\nD ( PA|Y(1)‖PA|Y(2) ) = ∑ i,j D ( Pai,j |Y(1)‖Pai,j |Y(2) ) .\nFor each pair (i, j), the KL-divergence is zero if y(1)i,j = y (2) i,j , and otherwise satisfies\nD ( Pai,j |Y(1)‖Pai,j |Y(2) ) ≤ p0(1− τ) log\np0(1− τ) p0τ + p0τ log p0τ p0(1− τ) + (1− p0) log 1− p0 1− p0\n= p0(1− 2τ) log 1− τ τ\n≤ p0(1− 2τ) ( 1− τ τ − 1 )\n≤ c2p0(1− 2τ)2,\nwhere c2 > 0 is a universal constant and the last inequality holds under the assumption τ > 1/100. Let N be the number of pairs (i, j) such that y(1)i,j 6= y (2) i,j . When K = Θ(n), we have\nN ≤ |{(i, j) : y(1)i,j = 1} ∪ {(i, j) : y (2) i,j = 1}| ≤ n 2.\nIt follows that D ( PA|Y(1)‖PA|Y(2) ) ≤ N · c2p0(1− 2τ)2 ≤ c2n2p0(1− 2τ)2. Combining pieces, for the left hand side of (8) to be less than 1/4, we must have p0(1− 2τ)2 ≥ C 1n ."
    } ],
    "references" : [ {
      "title" : "Finding a large hidden clique in a random graph",
      "author" : [ "N. Alon", "M. Krivelevich", "B. Sudakov" ],
      "venue" : "In Proceedings of the 9th annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Alon et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 1998
    }, {
      "title" : "Nuclear norm minimization for the planted clique and biclique problems",
      "author" : [ "B. Ames", "S. Vavasis" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Ames and Vavasis.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ames and Vavasis.",
      "year" : 2011
    }, {
      "title" : "Robust hierarchical clustering",
      "author" : [ "M.F. Balcan", "P. Gupta" ],
      "venue" : "In Proceedings of the Conference on Learning Theory (COLT),",
      "citeRegEx" : "Balcan and Gupta.,? \\Q2010\\E",
      "shortCiteRegEx" : "Balcan and Gupta.",
      "year" : 2010
    }, {
      "title" : "Correlation clustering",
      "author" : [ "N. Bansal", "A. Blum", "S. Chawla" ],
      "venue" : "In Proceedings of the 43rd Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Bansal et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2002
    }, {
      "title" : "Max cut for random graphs with a planted partition",
      "author" : [ "B. Bollobás", "A.D. Scott" ],
      "venue" : "Combinatorics, Probability and Computing,",
      "citeRegEx" : "Bollobás and Scott.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bollobás and Scott.",
      "year" : 2004
    }, {
      "title" : "Eigenvalues and graph bisection: An average-case analysis",
      "author" : [ "R.B. Boppana" ],
      "venue" : "In Proceedings of the 28th Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Boppana.,? \\Q1987\\E",
      "shortCiteRegEx" : "Boppana.",
      "year" : 1987
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E. Candès", "B. Recht" ],
      "venue" : "Foundations of Computational mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "E. Candès", "X. Li", "Y. Ma", "J. Wright" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Candès et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Candès et al\\.",
      "year" : 2011
    }, {
      "title" : "Hill-climbing finds random planted bisections",
      "author" : [ "T. Carson", "R. Impagliazzo" ],
      "venue" : "In Proceedings of the 12th annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Carson and Impagliazzo.,? \\Q2001\\E",
      "shortCiteRegEx" : "Carson and Impagliazzo.",
      "year" : 2001
    }, {
      "title" : "Rank-sparsity incoherence for matrix decomposition",
      "author" : [ "V. Chandrasekaran", "S. Sanghavi", "P. Parrilo", "A. Willsky" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Chandrasekaran et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chandrasekaran et al\\.",
      "year" : 2011
    }, {
      "title" : "Clustering with qualitative information",
      "author" : [ "M. Charikar", "V. Guruswami", "A. Wirth" ],
      "venue" : "In Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Charikar et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Charikar et al\\.",
      "year" : 2003
    }, {
      "title" : "Spectral clustering of graphs with general degrees in the extended planted partition model",
      "author" : [ "K. Chaudhuri", "F. Chung", "A. Tsiatas" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2012
    }, {
      "title" : "Low-rank matrix recovery from errors and erasures",
      "author" : [ "Y. Chen", "A. Jalali", "S. Sanghavi", "C. Caramanis" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Chen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "Algorithms for graph partitioning on the planted partition model",
      "author" : [ "A. Condon", "R.M. Karp" ],
      "venue" : "Random Structures and Algorithms,",
      "citeRegEx" : "Condon and Karp.,? \\Q2001\\E",
      "shortCiteRegEx" : "Condon and Karp.",
      "year" : 2001
    }, {
      "title" : "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications",
      "author" : [ "A. Decelle", "F. Krzakala", "C. Moore", "L. Zdeborová" ],
      "venue" : "Physical Review E,",
      "citeRegEx" : "Decelle et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Decelle et al\\.",
      "year" : 2011
    }, {
      "title" : "Correlation clustering with partial information. Approximation, Randomization, and Combinatorial Optimization: Algorithms and Techniques",
      "author" : [ "E.D. Demaine", "N. Immorlica" ],
      "venue" : null,
      "citeRegEx" : "Demaine and Immorlica.,? \\Q2003\\E",
      "shortCiteRegEx" : "Demaine and Immorlica.",
      "year" : 2003
    }, {
      "title" : "Correlation clustering in general weighted graphs",
      "author" : [ "E.D. Demaine", "D. Emanuel", "A. Fiat", "N. Immorlica" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Demaine et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Demaine et al\\.",
      "year" : 2006
    }, {
      "title" : "Correlation clustering minimizing disagreements on arbitrary weighted graphs",
      "author" : [ "D. Emmanuel", "A. Fiat" ],
      "venue" : "In Proceedings of the 11th Annual European Symposium on Algorithms,",
      "citeRegEx" : "Emmanuel and Fiat.,? \\Q2003\\E",
      "shortCiteRegEx" : "Emmanuel and Fiat.",
      "year" : 2003
    }, {
      "title" : "Active clustering: Robust and efficient hierarchical clustering using adaptively selected similarities",
      "author" : [ "B. Eriksson", "G. Dasarathy", "A. Singh", "R. Nowak" ],
      "venue" : "Arxiv preprint arXiv:1102.3887,",
      "citeRegEx" : "Eriksson et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Eriksson et al\\.",
      "year" : 2011
    }, {
      "title" : "A database interface for clustering in large spatial databases",
      "author" : [ "M. Ester", "H. Kriegel", "X. Xu" ],
      "venue" : "In Proceedings of the International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Ester et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Ester et al\\.",
      "year" : 1995
    }, {
      "title" : "Matrix Rank Minimization with Applications",
      "author" : [ "M. Fazel" ],
      "venue" : "PhD thesis, Stanford University,",
      "citeRegEx" : "Fazel.,? \\Q2002\\E",
      "shortCiteRegEx" : "Fazel.",
      "year" : 2002
    }, {
      "title" : "Heuristics for semirandom graph problems",
      "author" : [ "U. Feige", "J. Kilian" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Feige and Kilian.,? \\Q2001\\E",
      "shortCiteRegEx" : "Feige and Kilian.",
      "year" : 2001
    }, {
      "title" : "Reconstructing many partitions using spectral techniques",
      "author" : [ "J. Giesen", "D. Mitsche" ],
      "venue" : "In Fundamentals of Computation Theory,",
      "citeRegEx" : "Giesen and Mitsche.,? \\Q2005\\E",
      "shortCiteRegEx" : "Giesen and Mitsche.",
      "year" : 2005
    }, {
      "title" : "Recovering low-rank matrices from few coefficients in any basis",
      "author" : [ "D. Gross" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Gross.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gross.",
      "year" : 2011
    }, {
      "title" : "Stochastic blockmodels: Some first steps",
      "author" : [ "P.W. Holland", "K.B. Laskey", "S. Leinhardt" ],
      "venue" : "Social networks,",
      "citeRegEx" : "Holland et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Holland et al\\.",
      "year" : 1983
    }, {
      "title" : "Robust matrix decomposition with sparse corruptions",
      "author" : [ "D. Hsu", "S.M. Kakade", "T. Zhang" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Hsu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2011
    }, {
      "title" : "Spectral clustering with compressed, incomplete and inaccurate measurements",
      "author" : [ "B. Hunter", "T. Strohmer" ],
      "venue" : "Available at https://www.math.ucdavis.edu/ ̃strohmer/papers/ 2010/SpectralClustering.pdf,",
      "citeRegEx" : "Hunter and Strohmer.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hunter and Strohmer.",
      "year" : 2010
    }, {
      "title" : "The metropolis algorithm for graph bisection",
      "author" : [ "M. Jerrum", "G.B. Sorkin" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "Jerrum and Sorkin.,? \\Q1998\\E",
      "shortCiteRegEx" : "Jerrum and Sorkin.",
      "year" : 1998
    }, {
      "title" : "An efficient heuristic procedure for partitioning graphs",
      "author" : [ "B.W. Kernighan", "S. Lin" ],
      "venue" : "Bell System Technical Journal,",
      "citeRegEx" : "Kernighan and Lin.,? \\Q1970\\E",
      "shortCiteRegEx" : "Kernighan and Lin.",
      "year" : 1970
    }, {
      "title" : "Efficient active algorithms for hierarchical clustering",
      "author" : [ "A. Krishnamurthy", "S. Balakrishnan", "M. Xu", "A. Singh" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Krishnamurthy et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krishnamurthy et al\\.",
      "year" : 2012
    }, {
      "title" : "Compressed sensing and matrix completion with constant proportion of corruptions",
      "author" : [ "X. Li" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "Li.,? \\Q2013\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2013
    }, {
      "title" : "The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices",
      "author" : [ "Z. Lin", "M. Chen", "L. Wu", "Y. Ma" ],
      "venue" : "UIUC Technical Report UILU-ENG-09-2215,",
      "citeRegEx" : "Lin et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2009
    }, {
      "title" : "Correlation clustering with noisy input",
      "author" : [ "C. Mathieu", "W. Schudy" ],
      "venue" : "In Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Mathieu and Schudy.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mathieu and Schudy.",
      "year" : 2010
    }, {
      "title" : "Spectral partitioning of random graphs",
      "author" : [ "F. McSherry" ],
      "venue" : "In Proceedings of the 42nd IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "McSherry.,? \\Q2001\\E",
      "shortCiteRegEx" : "McSherry.",
      "year" : 2001
    }, {
      "title" : "Clustering social networks. In Algorithms and Models for Web-Graph",
      "author" : [ "N. Mishra", "I. Stanton R. Schreiber", "R.E. Tarjan" ],
      "venue" : null,
      "citeRegEx" : "Mishra et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2007
    }, {
      "title" : "Finding dense clusters via low rank + sparse decomposition",
      "author" : [ "S. Oymak", "B. Hassibi" ],
      "venue" : "Available on arXiv:1104.5186v1,",
      "citeRegEx" : "Oymak and Hassibi.,? \\Q2011\\E",
      "shortCiteRegEx" : "Oymak and Hassibi.",
      "year" : 2011
    }, {
      "title" : "Spectral clustering and the high-dimensional stochastic blockmodel",
      "author" : [ "K. Rohe", "S. Chatterjee", "B. Yu" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Rohe et al\\.,? \\Q1878\\E",
      "shortCiteRegEx" : "Rohe et al\\.",
      "year" : 1878
    }, {
      "title" : "Spectral clustering on a budget",
      "author" : [ "O. Shamir", "N. Tishby" ],
      "venue" : "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Shamir and Tishby.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shamir and Tishby.",
      "year" : 2011
    }, {
      "title" : "Improved algorithms for the random cluster graph model",
      "author" : [ "R. Shamir", "D. Tsur" ],
      "venue" : "Random Structures & Algorithms,",
      "citeRegEx" : "Shamir and Tsur.,? \\Q2007\\E",
      "shortCiteRegEx" : "Shamir and Tsur.",
      "year" : 2007
    }, {
      "title" : "Correlation clustering: maximizing agreements via semidefinite programming",
      "author" : [ "C. Swamy" ],
      "venue" : "In Proceedings of the 15th Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Swamy.,? \\Q2004\\E",
      "shortCiteRegEx" : "Swamy.",
      "year" : 2004
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "J.A. Tropp" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Tropp.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tropp.",
      "year" : 2012
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : "Arxiv preprint arxiv:1011.3027,",
      "citeRegEx" : "Vershynin.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vershynin.",
      "year" : 2010
    }, {
      "title" : "Efficient clustering with limited distance information",
      "author" : [ "K. Voevodski", "M.F. Balcan", "H. Roglin", "S.H. Teng", "Y. Xia" ],
      "venue" : "arXiv preprint arXiv:1009.5168,",
      "citeRegEx" : "Voevodski et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Voevodski et al\\.",
      "year" : 2010
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "U. von Luxburg" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "Luxburg.,? \\Q2007\\E",
      "shortCiteRegEx" : "Luxburg.",
      "year" : 2007
    }, {
      "title" : "Information-theoretic determination of minimax rates of convergence",
      "author" : [ "Y. Yang", "A. Barron" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Yang and Barron.,? \\Q1999\\E",
      "shortCiteRegEx" : "Yang and Barron.",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "In VLSI and design automation, it is useful in minimizing signaling between components (Kernighan and Lin, 1970).",
      "startOffset" : 87,
      "endOffset" : 112
    }, {
      "referenceID" : 34,
      "context" : "similar interest or background; finding clusters enables better recommendations and link prediction (Mishra et al., 2007).",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "In the analysis of document databases, clustering the citation graph is often an essential and informative first step (Ester et al., 1995).",
      "startOffset" : 118,
      "endOffset" : 138
    }, {
      "referenceID" : 7,
      "context" : "Our analysis provides stronger guarantees than are current results on general matrix splitting (Candès et al., 2011; Hsu et al., 2011; Li, 2013; Chen et al., 2013).",
      "startOffset" : 95,
      "endOffset" : 163
    }, {
      "referenceID" : 25,
      "context" : "Our analysis provides stronger guarantees than are current results on general matrix splitting (Candès et al., 2011; Hsu et al., 2011; Li, 2013; Chen et al., 2013).",
      "startOffset" : 95,
      "endOffset" : 163
    }, {
      "referenceID" : 30,
      "context" : "Our analysis provides stronger guarantees than are current results on general matrix splitting (Candès et al., 2011; Hsu et al., 2011; Li, 2013; Chen et al., 2013).",
      "startOffset" : 95,
      "endOffset" : 163
    }, {
      "referenceID" : 12,
      "context" : "Our analysis provides stronger guarantees than are current results on general matrix splitting (Candès et al., 2011; Hsu et al., 2011; Li, 2013; Chen et al., 2013).",
      "startOffset" : 95,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "For the special case of fully observed graphs, this formulation is exactly the same as the problem of correlation clustering, first proposed by Bansal et al. (2002). They show that exact minimization of the above objective is NP-hard in the worst case—we survey and compare with this and other related work in Section 1.",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "Since correlation clustering is NP-hard, there has been much work on devising alternative approximation algorithms (Bansal et al., 2002; Emmanuel and Fiat, 2003).",
      "startOffset" : 115,
      "endOffset" : 161
    }, {
      "referenceID" : 17,
      "context" : "Since correlation clustering is NP-hard, there has been much work on devising alternative approximation algorithms (Bansal et al., 2002; Emmanuel and Fiat, 2003).",
      "startOffset" : 115,
      "endOffset" : 161
    }, {
      "referenceID" : 10,
      "context" : "Approximations using convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed.",
      "startOffset" : 66,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "Approximations using convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed.",
      "startOffset" : 66,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : "Approximations using convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed.",
      "startOffset" : 66,
      "endOffset" : 140
    }, {
      "referenceID" : 39,
      "context" : ", 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed.",
      "startOffset" : 27,
      "endOffset" : 66
    }, {
      "referenceID" : 32,
      "context" : ", 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed.",
      "startOffset" : 27,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "1 CORRELATION CLUSTERING As mentioned, for a completely observed graph, our problem is mathematically precisely the same as correlation clustering formulated in Bansal et al. (2002); in particular a “+” in correlation clustering corresponds to an edge in the graph, a “-” to the lack of an edge, and disagreements are defined in the same way.",
      "startOffset" : 161,
      "endOffset" : 182
    }, {
      "referenceID" : 3,
      "context" : "1 CORRELATION CLUSTERING As mentioned, for a completely observed graph, our problem is mathematically precisely the same as correlation clustering formulated in Bansal et al. (2002); in particular a “+” in correlation clustering corresponds to an edge in the graph, a “-” to the lack of an edge, and disagreements are defined in the same way. Thus, this paper can equivalently be considered as an algorithm, and guarantees, for correlation clustering under partial observations. Since correlation clustering is NP-hard, there has been much work on devising alternative approximation algorithms (Bansal et al., 2002; Emmanuel and Fiat, 2003). Approximations using convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed. We emphasize that we use a different convex relaxation, and we focus on understanding when our convex program yields an optimal clustering without further rounding. We note that Mathieu and Schudy (2010) use a convex formulation with constraints enforcing positive semi-definiteness, triangle inequalities and fixed diagonal entries.",
      "startOffset" : 161,
      "endOffset" : 1104
    }, {
      "referenceID" : 3,
      "context" : "1 CORRELATION CLUSTERING As mentioned, for a completely observed graph, our problem is mathematically precisely the same as correlation clustering formulated in Bansal et al. (2002); in particular a “+” in correlation clustering corresponds to an edge in the graph, a “-” to the lack of an edge, and disagreements are defined in the same way. Thus, this paper can equivalently be considered as an algorithm, and guarantees, for correlation clustering under partial observations. Since correlation clustering is NP-hard, there has been much work on devising alternative approximation algorithms (Bansal et al., 2002; Emmanuel and Fiat, 2003). Approximations using convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed. We emphasize that we use a different convex relaxation, and we focus on understanding when our convex program yields an optimal clustering without further rounding. We note that Mathieu and Schudy (2010) use a convex formulation with constraints enforcing positive semi-definiteness, triangle inequalities and fixed diagonal entries. For the fully observed case, their relaxation is at least as tight as ours, and since they add more constraints, it is possible that there are instances where their convex program works and ours does not. However, this seems hard to prove/disprove. Indeed, in the full observation setting they consider, their exact recovery guarantee is no better than ours. Moreover, as we argue in the next section, our guarantees are order-wise optimal in some important regimes and thus cannot be improved even with a tighter relaxation. Practically, our method is faster since, to the best of our knowledge, there is no lowcomplexity algorithm to deal with the Θ(n3) triangle inequality constraints required by Mathieu and Schudy (2010). This means that our method can handle large graphs while their result is practically restricted to small ones (∼ 100 nodes).",
      "startOffset" : 161,
      "endOffset" : 1960
    }, {
      "referenceID" : 13,
      "context" : "2 PLANTED PARTITION MODEL The planted partition model, also known as the stochastic block-model (Condon and Karp, 2001; Holland et al., 1983), assumes that the graph is generated with in-cluster edge probability p and inter-cluster edge probability q (where p > q) and fully observed.",
      "startOffset" : 96,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "2 PLANTED PARTITION MODEL The planted partition model, also known as the stochastic block-model (Condon and Karp, 2001; Holland et al., 1983), assumes that the graph is generated with in-cluster edge probability p and inter-cluster edge probability q (where p > q) and fully observed.",
      "startOffset" : 96,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "been used to solve the closely related planted clique problem (Alon et al., 1998; Ames and Vavasis, 2011).",
      "startOffset" : 62,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "been used to solve the closely related planted clique problem (Alon et al., 1998; Ames and Vavasis, 2011).",
      "startOffset" : 62,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al.",
      "startOffset" : 48,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al.",
      "startOffset" : 48,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al.",
      "startOffset" : 48,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al.",
      "startOffset" : 48,
      "endOffset" : 195
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al.",
      "startOffset" : 48,
      "endOffset" : 235
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al.",
      "startOffset" : 48,
      "endOffset" : 267
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al.",
      "startOffset" : 290,
      "endOffset" : 316
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al.",
      "startOffset" : 290,
      "endOffset" : 359
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al.",
      "startOffset" : 290,
      "endOffset" : 403
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al.",
      "startOffset" : 290,
      "endOffset" : 450
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al. (2011) Ω̃(n3/4) Ω̃( 3/4 K ) Oymak and Hassibi (2011) Ω̃( √ n) Ω̃( √ n K ) Chaudhuri et al.",
      "startOffset" : 290,
      "endOffset" : 484
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al. (2011) Ω̃(n3/4) Ω̃( 3/4 K ) Oymak and Hassibi (2011) Ω̃( √ n) Ω̃( √ n K ) Chaudhuri et al.",
      "startOffset" : 290,
      "endOffset" : 530
    }, {
      "referenceID" : 4,
      "context" : "Paper Cluster size K Density difference (1− 2τ) Boppana (1987) n/2 Ω̃( 1 √ n ) Jerrum and Sorkin (1998) n/2 Ω̃( 1 n1/6− ) Condon and Karp (2001) Ω̃(n) Ω̃( 1 n1/2− ) Carson and Impagliazzo (2001) n/2 Ω̃( 1 √ n ) Feige and Kilian (2001) n/2 Ω̃( 1 √ n ) McSherry (2001) Ω̃(n2/3) Ω̃( √ n2 K3 ) Bollobás and Scott (2004) Ω̃(n) Ω̃( √ 1 n) Giesen and Mitsche (2005) Ω̃( √ n) Ω̃( √ n K ) Shamir and Tsur (2007) Ω̃( √ n) Ω̃( √ n K ) Mathieu and Schudy (2010) Ω̃( √ n) Ω̃(1) Rohe et al. (2011) Ω̃(n3/4) Ω̃( 3/4 K ) Oymak and Hassibi (2011) Ω̃( √ n) Ω̃( √ n K ) Chaudhuri et al. (2012) Ω̃( √ n) Ω̃( √ n K )",
      "startOffset" : 290,
      "endOffset" : 575
    }, {
      "referenceID" : 35,
      "context" : "The previous work listed in Table 1, except Oymak and Hassibi (2011), does not handle partial observations directly.",
      "startOffset" : 44,
      "endOffset" : 69
    }, {
      "referenceID" : 35,
      "context" : "The previous work listed in Table 1, except Oymak and Hassibi (2011), does not handle partial observations directly. One natural way to proceed is to impute the missing observations with noedge, or random edges with symmetric probabilities, and then apply any of the results in Table 1. This approach, however, leads to sub-optimal results. Indeed, this is done explicitly by Oymak and Hassibi (2011). They require the probability of observation p0 to satisfy p0 & √ Kmin n , where n is the number of nodes and Kmin is the minimum cluster size; in contrast, our approach only needs p0 & n K2 min (both right hand sides have to be less than 1, requiring Kmin & √ n, so the right hand side of our condition is order-wise smaller and thus less restrictive.",
      "startOffset" : 44,
      "endOffset" : 401
    }, {
      "referenceID" : 35,
      "context" : "The previous work listed in Table 1, except Oymak and Hassibi (2011), does not handle partial observations directly. One natural way to proceed is to impute the missing observations with noedge, or random edges with symmetric probabilities, and then apply any of the results in Table 1. This approach, however, leads to sub-optimal results. Indeed, this is done explicitly by Oymak and Hassibi (2011). They require the probability of observation p0 to satisfy p0 & √ Kmin n , where n is the number of nodes and Kmin is the minimum cluster size; in contrast, our approach only needs p0 & n K2 min (both right hand sides have to be less than 1, requiring Kmin & √ n, so the right hand side of our condition is order-wise smaller and thus less restrictive.) Shamir and Tishby (2011) deal with partial observations directly and shows that p0 & 1 n suffices for recovering two clusters of size Ω(n).",
      "startOffset" : 44,
      "endOffset" : 780
    }, {
      "referenceID" : 2,
      "context" : "For example, Balcan and Gupta (2010), Voevodski et al.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "For example, Balcan and Gupta (2010), Voevodski et al. (2010) and Krishnamurthy et al.",
      "startOffset" : 13,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "For example, Balcan and Gupta (2010), Voevodski et al. (2010) and Krishnamurthy et al. (2012) consider the clustering problem where one samples the rows/columns of the adjacency matrix rather than its entries.",
      "startOffset" : 13,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "For example, Balcan and Gupta (2010), Voevodski et al. (2010) and Krishnamurthy et al. (2012) consider the clustering problem where one samples the rows/columns of the adjacency matrix rather than its entries. Hunter and Strohmer (2010) consider partial observations in the features rather than in the similarity graph.",
      "startOffset" : 13,
      "endOffset" : 237
    }, {
      "referenceID" : 2,
      "context" : "For example, Balcan and Gupta (2010), Voevodski et al. (2010) and Krishnamurthy et al. (2012) consider the clustering problem where one samples the rows/columns of the adjacency matrix rather than its entries. Hunter and Strohmer (2010) consider partial observations in the features rather than in the similarity graph. Eriksson et al. (2011) show that Ω̃(n) actively selected pairwise similarities are sufficient for recovering a hierarchical clustering structure.",
      "startOffset" : 13,
      "endOffset" : 343
    }, {
      "referenceID" : 9,
      "context" : "We propose to perform the matrix splitting using convex optimization (Chandrasekaran et al., 2011; Candès et al., 2011).",
      "startOffset" : 69,
      "endOffset" : 119
    }, {
      "referenceID" : 7,
      "context" : "We propose to perform the matrix splitting using convex optimization (Chandrasekaran et al., 2011; Candès et al., 2011).",
      "startOffset" : 69,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "This has been shown to be the tightest convex surrogate for the rank function for matrices with unit spectral norm (Fazel, 2002).",
      "startOffset" : 115,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "The optimization problem (1) is, in fact, a semidefinite program (SDP) (Chandrasekaran et al., 2011).",
      "startOffset" : 71,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "The optimization problem (1) is, in fact, a semidefinite program (SDP) (Chandrasekaran et al., 2011). We remark on the above formulation. (a) This formulation does not require specifying the number of clusters; this parameter is effectively learned from the data. The tradeoff parameter λ is artificial and can be easily determined: since any desired K∗ has trace exactly equal to n, we simply choose the smallest λ such that the trace of the optimal solution is at least n. This can be done by, e.g., bisection, which is described below. (b) It is possible to obtain tighter convex relaxations by adding more constraints, such as the diagonal entry constraints ki,i = 1, ∀i, the positive semidefinite constraint K 0, or even the triangular inequalities ki,j + kj,k − ki,k ≤ 1. Indeed, this is done by Mathieu and Schudy (2010). Note that the guarantees for our formulation (to be presented in the next subsection) automatically imply guarantees for any other tighter relaxations.",
      "startOffset" : 72,
      "endOffset" : 828
    }, {
      "referenceID" : 31,
      "context" : "Second, our formulation can be solved efficiently using existing Augmented Lagrangian Multiplier methods (Lin et al., 2009).",
      "startOffset" : 105,
      "endOffset" : 123
    }, {
      "referenceID" : 30,
      "context" : "constraints, Mathieu and Schudy (2010) do not deliver better exact recovery guarantees (cf.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : "Second, our formulation can be solved efficiently using existing Augmented Lagrangian Multiplier methods (Lin et al., 2009). This is no longer the case with the Θ(n3) triangle inequality constraints enforced by Mathieu and Schudy (2010), and solving it as a standard SDP is only feasible for small graphs.",
      "startOffset" : 106,
      "endOffset" : 237
    }, {
      "referenceID" : 30,
      "context" : "To solve the optimization problem (1), we use the fast algorithm developed by Lin et al. (2009), which is tailored for matrix splitting and takes advantage of the sparsity of the observations.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "We note that directly applying existing results in the low-rankplus-sparse literature (Candès et al., 2011; Li, 2013) leads to weaker results, where the gap be bounded below by a constant.",
      "startOffset" : 86,
      "endOffset" : 117
    }, {
      "referenceID" : 30,
      "context" : "We note that directly applying existing results in the low-rankplus-sparse literature (Candès et al., 2011; Li, 2013) leads to weaker results, where the gap be bounded below by a constant.",
      "startOffset" : 86,
      "endOffset" : 117
    }, {
      "referenceID" : 30,
      "context" : "Table 1), including those in Mathieu and Schudy (2010) and Oymak and Hassibi (2011), which use tighter convex relaxations that are more computationally demanding.",
      "startOffset" : 29,
      "endOffset" : 55
    }, {
      "referenceID" : 30,
      "context" : "Table 1), including those in Mathieu and Schudy (2010) and Oymak and Hassibi (2011), which use tighter convex relaxations that are more computationally demanding.",
      "startOffset" : 29,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "Theorem 5 generalizes a similar result in Chaudhuri et al. (2012), which does not consider partial observations.",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "Theorem 5 generalizes a similar result in Chaudhuri et al. (2012), which does not consider partial observations. The theorem applies to any algorithm regardless of its computational complexity, and characterizes the fundamental tradeoff between p0 and 1 − 2τ . It shows that when Kmin = Θ(n), the requirement for 1− 2τ and p0 in Theorem 4 is optimal up to logarithmic factors, and cannot be significantly improved by using more complicated methods. For the general case with Kmin = O(n), only part of the picture is known. Using non-rigorous arguments, Decelle et al. (2011) show that 1 − 2τ & √ n Kmin is necessary when τ = Θ(1) and the graph is fully observed; otherwise recovery is impossible or computationally hard.",
      "startOffset" : 42,
      "endOffset" : 575
    }, {
      "referenceID" : 9,
      "context" : "Luckily for us, this has been done previously (Chandrasekaran et al., 2011; Candès et al., 2011; Li, 2013).",
      "startOffset" : 46,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "Luckily for us, this has been done previously (Chandrasekaran et al., 2011; Candès et al., 2011; Li, 2013).",
      "startOffset" : 46,
      "endOffset" : 106
    }, {
      "referenceID" : 30,
      "context" : "Luckily for us, this has been done previously (Chandrasekaran et al., 2011; Candès et al., 2011; Li, 2013).",
      "startOffset" : 46,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "The crucial Step 3 is where we go beyond the existing literature on matrix splitting (Chandrasekaran et al., 2011; Candès et al., 2011; Li, 2013).",
      "startOffset" : 85,
      "endOffset" : 145
    }, {
      "referenceID" : 7,
      "context" : "The crucial Step 3 is where we go beyond the existing literature on matrix splitting (Chandrasekaran et al., 2011; Candès et al., 2011; Li, 2013).",
      "startOffset" : 85,
      "endOffset" : 145
    }, {
      "referenceID" : 30,
      "context" : "The crucial Step 3 is where we go beyond the existing literature on matrix splitting (Chandrasekaran et al., 2011; Candès et al., 2011; Li, 2013).",
      "startOffset" : 85,
      "endOffset" : 145
    }, {
      "referenceID" : 30,
      "context" : "4 in Li (2013) and the discussion thereafter.",
      "startOffset" : 5,
      "endOffset" : 15
    }, {
      "referenceID" : 7,
      "context" : "4 STEP 3: DUAL CERTIFICATE CONSTRUCTION We use a variant of the so-called golfing scheme (Candès et al., 2011; Gross, 2011) to construct W.",
      "startOffset" : 89,
      "endOffset" : 123
    }, {
      "referenceID" : 23,
      "context" : "4 STEP 3: DUAL CERTIFICATE CONSTRUCTION We use a variant of the so-called golfing scheme (Candès et al., 2011; Gross, 2011) to construct W.",
      "startOffset" : 89,
      "endOffset" : 123
    }, {
      "referenceID" : 30,
      "context" : "We then run Algorithm 1, where the optimization problem (1) is solved using the fast algorithm in Lin et al. (2009)..",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 40,
      "context" : "The following version is given by Tropp (2012).",
      "startOffset" : 34,
      "endOffset" : 47
    }, {
      "referenceID" : 40,
      "context" : "Lemma 7 (Tropp, 2012) Consider a finite sequence {Mi} of independent, random n×n matrices that satisfy the assumption EMi = 0 and ‖Mi‖ ≤ D almost surely.",
      "startOffset" : 8,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "Proof of Theorem 5 We use a standard information theoretical argument, which improves upon a related proof by Chaudhuri et al. (2012). Let K be the size of the clusters (which are assumed to have equal size).",
      "startOffset" : 110,
      "endOffset" : 134
    }, {
      "referenceID" : 44,
      "context" : "A standard application of Fano’s inequality and the convexity of the mutual information (Yang and Barron, 1999) gives",
      "startOffset" : 88,
      "endOffset" : 111
    } ],
    "year" : 2014,
    "abstractText" : "This paper considers the problem of clustering a partially observed unweighted graph—i.e., one where for some node pairs we know there is an edge between them, for some others we know there is no edge, and for the remaining we do not know whether or not there is an edge. We want to organize the nodes into disjoint clusters so that there is relatively dense (observed) connectivity within clusters, and sparse across clusters. We take a novel yet natural approach to this problem, by focusing on finding the clustering that minimizes the number of “disagreements”—i.e., the sum of the number of (observed) missing edges within clusters, and (observed) present edges across clusters. Our algorithm uses convex optimization; its basis is a reduction of disagreement minimization to the problem of recovering an (unknown) low-rank matrix and an (unknown) sparse matrix from their partially observed sum. We evaluate the performance of our algorithm on the classical Planted Partition/Stochastic Block Model. Our main theorem provides sufficient conditions for the success of our algorithm as a function of the minimum cluster size, edge density and observation probability; in particular, the results characterize the tradeoff between the observation probability and the edge density gap. When there are a constant number of clusters of equal size, our results are optimal up to logarithmic factors.",
    "creator" : "LaTeX with hyperref package"
  }
}