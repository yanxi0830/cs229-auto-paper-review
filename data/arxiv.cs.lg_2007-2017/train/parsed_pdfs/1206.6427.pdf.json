{
  "name" : "1206.6427.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced Mixing Coefficients",
    "authors" : [ "Iftekhar Naim", "Daniel Gildea" ],
    "emails" : [ "inaim@cs.rochester.edu", "gildea@cs.rochester.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Clustering is a widely used exploratory data analysis tool that has been successfully applied to biology, social science, information retrieval, signal processing, and many other fields (Jain et al., 1999). In many of these applications (for example, biological data analysis, anomaly detection, image segmentation, etc.), the goal is to identify rare groups or small clusters (in terms of number of members) in the presence of other\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nlarger clusters. In this paper, we focus on the particular problem of clustering large datasets with high dynamic range in cluster sizes.\nThe Gaussian mixture model is a powerful model for data clustering (McLachlan & Peel, 2000). It models the data as a mixture of multiple Gaussian distributions where each Gaussian component corresponds to one cluster. Let X = {x1,x2, . . . ,xN} be N i.i.d. random vectors that follow a K component Gaussian mixture distribution. The jth Gaussian component in the mixture is defined by its mean µj , covariance Σj , and the mixing coefficient αj (αj > 0 and ∑K\nj=1 αj = 1). These parameters together are represented as the parameter vector Θ = [αj ,µj ,Σj ] K j=1. Our goal is to estimate model parameters Θ given the data X. The parameter estimation is typically accomplished by the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). In this paper, we empirically show that EM exhibits slow convergence if one of the Gaussian mixture components has a very small mixing coefficient compared to others, and there exists some overlap among the mixture components. We explain this slow convergence of EM for a mixture with unbalanced mixing coefficients using the convergence analysis framework presented by Xu and Jordan (1996), and Ma et al. (2000).\nWe present a solution to this problem based on Deterministic Annealing (Rose, 1998; Ueda & Nakano, 1998). It is well known that deterministic annealing can help prevent the EM algorithm from getting trapped in local optima (Ueda & Nakano, 1998). Traditional deterministic annealing follows a monotonically decreasing temperature schedule by slowly increasing the inverse temperature parameter (β) from 0 to 1. We propose a novel non-monotonic temperature schedule that can improve the speed of convergence as well. We call this modified annealing schedule Antiannealing. We start with a traditional temperature\nschedule by slowly increasing β from 0 to 1. Next we continue increasing β beyond 1, up to a chosen upper bound, and finally slowly decrease β down to 1. Our experiments demonstrate the effectiveness of the proposed Anti-annealing schedule to improve the speed of convergence of the EM algorithm for unbalanced mixtures. Finally, we extend our results to the Dirichlet Process Gaussian Mixture Models (DP-GMM)."
    }, {
      "heading" : "2. Convergence of EM Algorithm",
      "text" : ""
    }, {
      "heading" : "2.1. Related Work",
      "text" : "The EM algorithm is guaranteed to monotonically converge to local optima under mild continuity conditions (Dempster et al., 1977; Wu, 1983). Redner and Walker (1984) show that EM has linear rate of convergence and suggest that Newton’s or Quasi-Newton methods should be preferred over the EM algorithm. They also experimentally show that EM converges slowly in the presence of overlapping clusters. Xu and Jordan (1996) analyze the rate of convergence of the EM algorithm and show that EM exhibits a super-linear rate of convergence as the overlap among the mixture components goes to zero (Xu & Jordan, 1996). They forge a connection between the EM algorithm and gradient ascent and prove that rate of convergence of the EM algorithm depends on the condition number of a projected Hessian matrix ETP (Θ∗)H(Θ∗)E, where Θ∗ is the optimum parameter value, E = [e1, . . . , em] is a set of unit basis vectors spanning the constrained parameter space (satisfying the constraint ∑K\nj=1 αj = 1), P (Θ ∗) is a projection\nmatrix, and H(Θ∗) is the Hessian of the log-likelihood function. Later, Ma et al. (2000) extend this result and show that the rate of convergence of EM is a higher order infinitesimal of maximum pairwise overlap among the mixture components. Salakhutdinov et al. (2003) propose the Expectation Conjugate Gradient (ECG) algorithm for highly overlapping Gaussian mixtures. ECG allows faster convergence than the traditional EM algorithm in the presence of high overlap among mixture components. Therefore, the impact of component overlap on the convergence of EM has been known for more than a decade. While overlap among components is the major factor that influences rate of convergence of EM, we show that mixing coefficients can also significantly influence the speed of convergence in the presence of some overlap. To our knowledge, this is the first work that addresses the impact of mixing coefficients on the rate of convergence of EM."
    }, {
      "heading" : "2.2. Convergence of EM for Small Clusters",
      "text" : "We investigate the convergence of the EM algorithm for Gaussian mixtures with some small mixing coefficients. We start with 3 simulation examples. First, we create synthetic data with a 2-component univariate Gaussian mixture with the parameters α = (0.025, 0.975)T , (µ1, µ2) = (−5.0, 5.0), (Σ1,Σ2) = (6.25, 6.25) (as shown in Figure 1(a)). We perform 2-component EM fitting and after 100 iterations, we get a parameter estimate of α̂ = (0.059, 0.9410)T , (µ̂1, µ̂2) = (−0.497, 5.08) , (Σ̂1, Σ̂2) = (23.15, 5.92). In this simple setting, the inaccurate result is due to the slow speed of convergence. If we allow EM to run for many more iterations, it usually converges to the true parameter values.\nNext, we create another dataset (Figure 2(a)) with balanced mixing coefficients (α = (0.5, 0.5)T ) and leave the other parameters the same. Under the same overlap (since means are not changed), EM converges to almost accurate parameters much faster, after only 12 iterations. This simulation implies that under the same amount of overlap, more balanced mixing coefficients yield faster convergence of EM. Finally, we create a synthetic dataset with skewed mixing coefficients, but relatively less overlap (Figure 3(a)). Means are set at points (µ1, µ2) = (−10.0, 10.0), mixing coefficients at α = (0.025, 0.0975)T , and covariance values at (Σ1,Σ2) = (6.25, 6.25). Despite having very\nsmall mixing coefficient values, EM converged to the true parameters in only 23 iterations. This example emphasizes the critical impact of overlaps on speed of convergence. However, the first example also implies that under some overlap, mixing coefficients have strong influence on the speed of convergence."
    }, {
      "heading" : "2.3. Why Slow Convergence?",
      "text" : "Next, we explore the reason that EM shows slow convergence in the presence of small clusters. We explain it using the framework proposed by Xu and Jordan (1996). Xu and Jordan (and later Ma et al. (2000)) have shown that the rate of convergence of EM is upper bounded by the norm ∥ ∥I + ETP (Θ∗)H(Θ∗)E) ∥ ∥ i.e. the condition number of the projected Hessian κ [ ETP (Θ∗)H(Θ∗)E ]\n. As the condition number κ ≃ 1, the log-likelihood surface is spherical and allows fast convergence. On the other hand, a larger value of the condition number of the effective Hessian implies an elongated log-likelihood function which can cause slow convergence for any linearly convergent method. Our simulation results show that, in the presence of some overlap, the condition number of the Hessian matrix increases as one of the mixing coefficients decreases. We verify this by computing the condition number of the projected Hessian matrix ETP (Θ∗)H(Θ∗)E at the true parameter values for 3 different 2-component mixture configurations. For all the mixture configurations, we keep one of the means fixed at µ2 = 0.0 and then we vary the mean of the other components to µ1 = 10, 20, and 30. The variance of the components are set to 9. Then, we vary the mixing coefficients of the first component α1 from 0.01 to 0.99 and compute the condition numbers for each case and plot them in Figure 4. The results are quite intuitive. In the case of nonoverlapping clusters (µ1 = 20 or µ1 = 30), the condition number did not change much for extreme values of α1. However, for the overlapping case (µ1 = 10.0), the condition number of the projected Hessian became a lot larger for extreme values (both higher and smaller values) of α1. To confirm this intuition, we performed\nanother simulation where we generated 2 different 2- component Gaussian mixtures with varying mixing coefficients and looked at their log-likelihood surface. For simplicity, we plot log-likelihood as a function of only the means of the mixture components, and assume the mixing coefficients and covariances to be fixed. Figure 5 shows the log-likelihood surfaces (on the right) for the corresponding Gaussian mixtures (on the left). As one of the mixing coefficients becomes significantly smaller compared to the others, the log-likelihood surface tends to become flatter and elongated, which is in agreement with our intuition."
    }, {
      "heading" : "3. Proposed Solution",
      "text" : "In this section, we propose a variation of the Deterministic Annealing EM (Ueda & Nakano, 1998) to address the speed of convergence issue explained above. We compare the result with two other well-known optimization techniques: Expectation Conjugate Gradient (ECG) (Salakhutdinov et al., 2003) and QuasiNewton Method (BFGS) (Liu & Nocedal, 1989)."
    }, {
      "heading" : "3.1. Deterministic Anti-Annealing Expectation-Maximization",
      "text" : "Deterministic Annealing is a well-known technique to improve the convergence behavior for non-convex optimization problems. Ueda and Nakano (1998) proposed a Deterministic Annealing Expectation Maximization (DAEM) algorithm that varies the temperature from high to low temperature, and deterministically optimizes the objective function at each temperature. Conventionally, DAEM is used to provide reliable global convergence. In EM, we estimate the\nposterior membership probabilities hj(t) (the probability that xt belongs to the j\nth Gaussian component) using the following equation:\nhj(t) = αjP (xt|µj ,Σj)\n∑K i=1 αiP (xt|µi,Σi) (1)\nThe DAEM algorithm modifies the E-step update rule using the scheduling parameter β:\nhj(t) =\n( αjP (xt|µj ,Σj) )β\n∑K i=1 (αiP (xt|µi,Σi)) β\n(2)\nIn the M-step, the model parameters are estimated using this hj(t) values in exactly the same manner as the traditional EM algorithm. The parameter β can roughly be interpreted as the inverse of temperature. DAEM typically starts at β ≃ 0 and slowly increases β up to 1. At each β value, DAEM iterates the E-step (2) and the M -step until convergence. The algorithm can be described as follows:\n1. Initialize:\n• Set β ← βmin (0 < βmin ≪ 1) • Start with a random initial parameter Θ(0)\n2. Iterate until convergence:\n• E-step: estimate posterior probabilities hj(t) using Equation 2. • M-step: estimate Θ(new) using hj(t) values estimated in E-step\n3. Increase β\n4. If β ≤ 1, go back to step 2.\n5. If β > 1, return.\nInitially, for β → 0, all the clusters are overlapping and the posterior probability becomes uniform, i.e. hj(t) ∼ 1/K for all j. On the other hand, as β becomes larger, DAEM allows less and less overlap and for β → ∞ it becomes analogous to the winner-takeall (Kearns et al., 1998) approach, i.e. hj(t) = 1 for only one j, and hi(t) = 0, ∀i 6= j. For β = 1, it is reduced to the original posterior distribution given current Θ.\nTraditionally, DAEM starts with β ≃ 0 and slowly monotonically increases to β = 1. For tiny clusters, we observe that EM gives highly overlapping parameter estimates even after hundreds of iterations, and converges very slowly. We can improve the speed by starting from β > 1 and slowly decreasing it down to 1.0, where the objective function is same as that for EM. Let us call this scheduling strategy Anti-annealing scheduling. Although this improves the speed of convergence by restricting the amount of overlaps, the objective function becomes more irregular and EM tends to converge to poor local optima more frequently. In order to solve this problem, we follow a hybrid schedule, where we start with βmin < 1 and slowly increase it to a value βmax > 1 and then again decrease back to β = 1.\nThe proposed anti-annealing schedule (Figure 6(b)) improves the speed of convergence and helps avoid poor local optima as well. However, it is necessary to choose a slow enough temperature schedule, particularly for complicated data with a large number of\nclusters. We also need to perturb the estimated parameters after each iteration. At lower β values, the effective number of clusters is often less than K, as many components share the same parameters. As we increase β, the clusters start to move around and effective number of clusters increases as a result of splitting. We must perturb the estimated parameters with a small noise term to enable the clusters to split. In our implementation, we add a small amount of random noise to the mean of each of the Gaussian components along the first principal component dimension of the associated cluster.\nHow Does Anti-Annealing Speed up\nConvergence?\nUsing the results shown by Ma et al. (2000), we explain the faster convergence of the proposed Anti-annealing method. Let eij(Θ\n∗) be the measure of overlap between the ith and jth Gaussian component:\neij(Θ ∗) =\n\n    \n    \nlimN→∞ 1 N\n∑N\nt=1 hi(t)hj(t), if i 6= j\nlimN→∞ 1 N\n∑N\nt=1 (1− hi(t))hi(t), if i = j\nand let the maximum component-wise overlap be: e(Θ∗) = maxi,j eij(Θ\n∗). Ma et al. (2000) show that the rate of convergence r is a higher order infinitesimal of e(Θ∗):\nr ≤ lim N→∞\n∥ ∥I + ETP (Θ∗)H(Θ∗)E ∥ ∥ = o(e0.5−ǫ(Θ∗))\nDuring the Anti-annealing phase (β > 1), the pairwise overlap values eij(Θ\n∗) decreases due to relatively harder membership probabilities. For the extreme case β → ∞, the posterior probabilities hi(t) values are either zero or one, which causes eij(Θ\n∗) ≃ 0 for all i, j ∈ {1, . . . ,K} leading to superlinear convergence. In general, it can be easily shown that e(Θ∗) decreases as β increases."
    }, {
      "heading" : "3.2. Expectation Conjugate Gradient (ECG)",
      "text" : "The Conjugate Gradient method is known to outperform gradient descent methods for special cases when the objective function is elongated, and the conjugate direction is a better direction than the steepest gradient direction. Salakhutdinov et al. (2003) proposed an expectation conjugate gradient (ECG) method for optimizing the log-likelihood functions in the case of highly overlapping clusters. We evaluate the performance of the ECG algorithm for the small cluster scenario. The EM algorithm automatically satisfies several constraints on the parameter space: αi ≥ 0,\n∑\ni αi = 1, and Σ 0. To ensure that the ECG algorithm satisfies the same set of constraints, Salakhutdinov et al. (2003) propose to re-parameterize the model parameters: αj = e λj P\nl eλl\nand Σj = LjL ∗ j , where Lj\nis the upper triangular matrix obtained by Cholesky decomposition of Σj . Due to space limitation, exact formulation of gradients under the re-parameterization is presented in the Appendix. We apply the standard conjugate gradient optimization algorithm using these gradient values and a cubic line search (Matlab Library, Carl E. Rasmussen, 2006)."
    }, {
      "heading" : "3.3. Quasi-Newton Method: BFGS",
      "text" : "Quasi-Newton methods (for example, BFGS, LBFGS, etc.) approximate the Hessian using gradient values and usually converge faster than first order gradient based methods. However, these methods (without line search) lack the convergence guarantee of EM and require line search that introduces additional computation. We implement BFGS using the same gradient functions as for conjugate gradient and the Matlab Optimization Toolbox, and compare with our algorithm."
    }, {
      "heading" : "4. Extension to Dirichlet Process Mixture Model",
      "text" : "The Dirichlet process mixture model (DPMM) (Rasmussen, 2000) has gained significant attention for flexible density estimation. Unlike the traditional mixture models, DPMM does not assume a fixed number of density components and allows the model complexity to grow as more data points are seen. We have extended the variational Bayesian Dirichlet process mixture model (Blei & Jordan, 2006) using Deterministic Anti-annealing. The variational Bayesian DPMM is based on the truncated stickbreaking representation. Let X represent the random variable that follows the Dirichlet process mixture model, and let the latent variables be W = {V,η∗,Z} representing the stick lengths, individual mixture component parameters, and cluster memberships respectively. Let q(v,η∗, z) denote the factorized variational distribution and T be the truncation parameter. The posterior responsibility of a data point xt to a mixture component j ∈ {1, . . . , T} is represented as φij , which is computed as:\nφij = exp(Sj) ∑\nl exp(Sl) (3)\nwhere Sj = Eq[log Vj ] + ∑j−1\nl=1 Eq[log(1 − Vl)] + Eq[log p(xi|zi)].\nThe deterministic annealing approach can be easily\nextended to DPMM by a straight-forward modification of equation (3) (Katahira et al., 2008):\nφij = exp(Sj)\nβ\n∑\nl exp(Sl) β\n(4)"
    }, {
      "heading" : "5. Results and Discussion",
      "text" : ""
    }, {
      "heading" : "5.1. Datasets",
      "text" : "We experiment with three different datasets.1 The first dataset consists of samples drawn from a two component mixture of Gaussians, where the cluster sizes are 200k and 200 data points respectively (Figure 7(a)). We deliberately set the means and covariances so that there exists some overlap among the mixture components. The second dataset is also composed of a synthetic mixture of 4 Gaussians (Figure 7(b)), having 150k, 100k, 50k, and 150 data points each. Finally, we experiment with the MNIST handwritten digits dataset, which consists of high dimensional images of handwritten digits (0-9). We have randomly selected 5000 images of handwritten digit ‘4’, and 250 images of handwritten digit ‘8’. Then we combine these samples and reduce the dimensionality of the combined dataset down to two dimensions using PCA. We choose these two particular digits because of their nice elliptical shape in 2D PCA projection (Figure 7(c)). We can approximate the density of this dataset by fitting a 2-component mixture of Gaussians."
    }, {
      "heading" : "5.2. Experimental Results",
      "text" : "We observe the convergence behavior of all the four different algorithms: EM, Deterministic Anti-annealing, BFGS, and ECG, on all the three datasets described above. Each algorithm terminates when it satisfies the stopping criterion: |L(Θk+1) − L(Θk)|/|L(Θk+1| < τ . Here, L(Θk) represents the log-likelihood value at the kth iteration and τ is the tolerance parameter. For all the algorithms except Determinisitc Anti-annealing, we set the tolerance variable τ = 10−10. For Deterministic Anti-annealing, we set the tolerance parameter to 10−6. Since anti-annealing is capable of speeding up convergence at the later scheduling stages, we do not need a conservative tolerance. For the first and third experiment, the temperature schedule is set to β = [0.8, 1.0, 1.2, 1.0], and for the second dataset we set β = [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.0]. As expected, we require slower temperature scheduling for a larger number of clusters. We estimate error with respect to the true parameter values by summing up the symmetric KL divergence between estimated and true Gaussian\n1We also present additional results for image segmentation task in Appendix A.3.\nparameters for each mixture component. In order to measure the accuracy of clustering, we need to perform an assignment task to match clusters. For this task, we use the symmetric KL divergence between multivariate Gaussians and then perform a minimum weighted bipartite matching that finds a one-to-one mapping that minimizes the cumulative error with respect to true clustering. The symmetric KL divergence is defined as:\nDs [p, q] = DKL(p, q) +DKL(q, p). (5)\nFor two Gaussian distributions,\nDs [N (x|µi,Σi),N (x|µj ,Σj)] = 1\n2 Tr\n[\nΣ−1i Σj +Σ −1 j Σi\n]\n+ 1\n2 (µi − µj)\nT [ Σ−1i +Σ −1 j ] (µi − µj)− d\nThe approximate error for the estimated parameter Θ̂ = {µ̂j , Σ̂j , α̂j} K j=1 with respect to the true parameters Θ∗ is estimated as follows:\nerr(Θ̂,Θ∗) =\nK ∑\nj=1\nDs [ N (x|µ̂j , Σ̂j),N (x|µ ∗ πj ,Σ∗πj ) ]\n(6) where {πj} K j=1 is the one-to-one mapping estimated by minimum weight bipartite graph matching.\nFor each of the datasets, we apply all four algorithms for Gaussian mixture model fitting. We execute each algorithm 10 times on each dataset and observe the rate of convergence both for the best case (minimum error) and average case, as shown in Figure 7.2 For EM and deterministic anti-annealing, we initialize the means of mixture components with randomly chosen sample points from the data. The mixing coefficients and covariances are initialized to the uniform distribution (1/K) and the covariance of entire data respectively. For BFGS and ECG, we initialize the parameters with the outcome of a few EM iterations.\nWe experimented with the Dirichlet process Gaussian mixture models on synthetic Gaussian mixture models with high dynamic ranges in cluster sizes. We created a two-component Gaussian mixture, where the larger component consists of 50k points, and the smaller component consists of only 100 points. Then, we applied both the variational Bayes DPMM and the Deterministic Anti-annealing based DPMM. The VB-DPMM incorrectly estimated five components, where four components were fitted to the same Gaussian component. On the other hand, Anti-annealing DPMM resulted in the correct estimation of two components. The reason is the winner-take-all behavior during the anti-annealing.\n2The plot showing the distribution of final log-likelihood values is presented in the Appendix A.1."
    }, {
      "heading" : "5.3. Discussion",
      "text" : "The experimental results demonstrate that both the deterministic anti-annealing method and BFGS significantly improve the speed of convergence compared to traditional EM. For the minimum error case scenario, BFGS outperforms deterministic anti-annealing by a small margin. However, deterministic anti-annealing is more stable on average and has better convergence behavior. Both BFGS and ECG exhibit significant amount of variability, particularly for the second (4 Gaussians) dataset. They often end up in poor degenerated local optima, where one or more of the mixing coefficients are clamped to zero. Therefore, deterministic anti-annealing outperforms BFGS on average. Salakhutdinov et al. (2003) proposed a hybrid EMECG algorithm, that estimates the entropy of cluster memberships as a measure of missing information (in other words, cluster overlap), and chooses to apply ECG if the entropy value is larger than certain threshold. Although the entropy-based method works well for balanced but highly overlapping mixtures, it is not general enough for the case of unbalanced mixtures. The entropy value decreases with the increasing skew in the mixing coefficients. Moreover, our experimental results show that neither ECG nor EM works well for such unbalanced overlapping mixtures."
    }, {
      "heading" : "6. Conclusion",
      "text" : "The proposed Deterministic Anti-annealing scheme has lots of potential for faster convergence for datasets with smaller clusters. It offers several key advantages such as: 1) more robust global convergence, 2) faster convergence for small clusters via anti-annealing, 3) simple to implement, no line search required, 4) parameter constraints are satisfied without requiring reparameterization. Our experimental results demonstrate that deterministic anti-annealing EM outperforms all the other three algorithms on average, both in terms of speed and correctness of convergence. However, the temperature scheduling of Deterministic Annealing often requires some tuning (Rangarajan, 2000). A thorough study of temperature schedule can be an interesting future direction. As a general guideline, the schedule should be guided by the complexity of the data. The more complex the data, the more slowly we should vary the temperature parameter."
    }, {
      "heading" : "7. Acknowledgement",
      "text" : "We would like to thank Daniel Štefankovič, Gaurav Sharma, and Suprakash Datta for many useful comments and feedback. Funded in part by NSF award IIS-0910611."
    } ],
    "references" : [ {
      "title" : "Variational inference for dirichlet process mixtures",
      "author" : [ "D.M. Blei", "M.I. Jordan" ],
      "venue" : "Bayesian Analysis,",
      "citeRegEx" : "Blei and Jordan,? \\Q2006\\E",
      "shortCiteRegEx" : "Blei and Jordan",
      "year" : 2006
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Dempster et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "Data clustering: a review",
      "author" : [ "A.K. Jain", "M.N. Murty", "P.J. Flynn" ],
      "venue" : "ACM computing surveys (CSUR),",
      "citeRegEx" : "Jain et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 1999
    }, {
      "title" : "Deterministic annealing variant of variational bayes method",
      "author" : [ "K. Katahira", "K. Watanabe", "M. Okada" ],
      "venue" : "In Journal of Physics: Conference Series,",
      "citeRegEx" : "Katahira et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Katahira et al\\.",
      "year" : 2008
    }, {
      "title" : "An informationtheoretic analysis of hard and soft assignment methods for clustering",
      "author" : [ "M.J. Kearns", "Y. Mansour", "A.Y. Ng" ],
      "venue" : "Nato ASI Series, Series D: Behavioural and Social Sciences,",
      "citeRegEx" : "Kearns et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 1998
    }, {
      "title" : "On the limited memory BFGS method for large scale optimization",
      "author" : [ "D.C. Liu", "J. Nocedal" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Liu and Nocedal,? \\Q1989\\E",
      "shortCiteRegEx" : "Liu and Nocedal",
      "year" : 1989
    }, {
      "title" : "Asymptotic convergence rate of the EM algorithm for Gaussian mixtures",
      "author" : [ "J. Ma", "L. Xu", "M.I. Jordan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Ma et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2000
    }, {
      "title" : "Finite Mixture Models",
      "author" : [ "G. McLachlan", "D. Peel" ],
      "venue" : "New York: Wiley Interscience,",
      "citeRegEx" : "McLachlan and Peel,? \\Q2000\\E",
      "shortCiteRegEx" : "McLachlan and Peel",
      "year" : 2000
    }, {
      "title" : "Self-annealing and self-annihilation: unifying deterministic annealing and relaxation labeling",
      "author" : [ "A. Rangarajan" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Rangarajan,? \\Q2000\\E",
      "shortCiteRegEx" : "Rangarajan",
      "year" : 2000
    }, {
      "title" : "The infinite Gaussian mixture model",
      "author" : [ "C.E. Rasmussen" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Rasmussen,? \\Q2000\\E",
      "shortCiteRegEx" : "Rasmussen",
      "year" : 2000
    }, {
      "title" : "Mixture densities, maximum likelihood and the EM algorithm",
      "author" : [ "R A Redner", "Walker", "H F" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Redner et al\\.,? \\Q1984\\E",
      "shortCiteRegEx" : "Redner et al\\.",
      "year" : 1984
    }, {
      "title" : "Deterministic annealing for clustering, compression, classification, regression, and related optimization problems",
      "author" : [ "K. Rose" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Rose,? \\Q1998\\E",
      "shortCiteRegEx" : "Rose",
      "year" : 1998
    }, {
      "title" : "Optimization with EM and Expectation-ConjugateGradient",
      "author" : [ "R. Salakhutdinov", "S. Roweis", "Z. Ghahramani" ],
      "venue" : "Proceedings of ICML,",
      "citeRegEx" : "Salakhutdinov et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Salakhutdinov et al\\.",
      "year" : 2003
    }, {
      "title" : "Deterministic annealing EM algorithm",
      "author" : [ "N. Ueda", "R. Nakano" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Ueda and Nakano,? \\Q1998\\E",
      "shortCiteRegEx" : "Ueda and Nakano",
      "year" : 1998
    }, {
      "title" : "On the convergence properties of the EM algorithm",
      "author" : [ "Wu", "CF" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Wu and CF.,? \\Q1983\\E",
      "shortCiteRegEx" : "Wu and CF.",
      "year" : 1983
    }, {
      "title" : "On convergence properties of the EM algorithm for Gaussian mixtures",
      "author" : [ "L. Xu", "M.I. Jordan" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Xu and Jordan,? \\Q1996\\E",
      "shortCiteRegEx" : "Xu and Jordan",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Clustering is a widely used exploratory data analysis tool that has been successfully applied to biology, social science, information retrieval, signal processing, and many other fields (Jain et al., 1999).",
      "startOffset" : 186,
      "endOffset" : 205
    }, {
      "referenceID" : 1,
      "context" : "The parameter estimation is typically accomplished by the Expectation-Maximization (EM) algorithm (Dempster et al., 1977).",
      "startOffset" : 98,
      "endOffset" : 121
    }, {
      "referenceID" : 11,
      "context" : "We present a solution to this problem based on Deterministic Annealing (Rose, 1998; Ueda & Nakano, 1998).",
      "startOffset" : 71,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : "The parameter estimation is typically accomplished by the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). In this paper, we empirically show that EM exhibits slow convergence if one of the Gaussian mixture components has a very small mixing coefficient compared to others, and there exists some overlap among the mixture components. We explain this slow convergence of EM for a mixture with unbalanced mixing coefficients using the convergence analysis framework presented by Xu and Jordan (1996), and Ma et al.",
      "startOffset" : 99,
      "endOffset" : 514
    }, {
      "referenceID" : 1,
      "context" : "The parameter estimation is typically accomplished by the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). In this paper, we empirically show that EM exhibits slow convergence if one of the Gaussian mixture components has a very small mixing coefficient compared to others, and there exists some overlap among the mixture components. We explain this slow convergence of EM for a mixture with unbalanced mixing coefficients using the convergence analysis framework presented by Xu and Jordan (1996), and Ma et al. (2000). We present a solution to this problem based on Deterministic Annealing (Rose, 1998; Ueda & Nakano, 1998).",
      "startOffset" : 99,
      "endOffset" : 536
    }, {
      "referenceID" : 1,
      "context" : "Related Work The EM algorithm is guaranteed to monotonically converge to local optima under mild continuity conditions (Dempster et al., 1977; Wu, 1983).",
      "startOffset" : 119,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "Related Work The EM algorithm is guaranteed to monotonically converge to local optima under mild continuity conditions (Dempster et al., 1977; Wu, 1983). Redner and Walker (1984) show that EM has linear rate of convergence and suggest that Newton’s or Quasi-Newton methods should be preferred over the EM algorithm.",
      "startOffset" : 120,
      "endOffset" : 179
    }, {
      "referenceID" : 1,
      "context" : "Related Work The EM algorithm is guaranteed to monotonically converge to local optima under mild continuity conditions (Dempster et al., 1977; Wu, 1983). Redner and Walker (1984) show that EM has linear rate of convergence and suggest that Newton’s or Quasi-Newton methods should be preferred over the EM algorithm. They also experimentally show that EM converges slowly in the presence of overlapping clusters. Xu and Jordan (1996) analyze the rate of convergence of the EM algorithm and show that EM exhibits a super-linear rate of convergence as the overlap among the mixture components goes to zero (Xu & Jordan, 1996).",
      "startOffset" : 120,
      "endOffset" : 433
    }, {
      "referenceID" : 1,
      "context" : "Related Work The EM algorithm is guaranteed to monotonically converge to local optima under mild continuity conditions (Dempster et al., 1977; Wu, 1983). Redner and Walker (1984) show that EM has linear rate of convergence and suggest that Newton’s or Quasi-Newton methods should be preferred over the EM algorithm. They also experimentally show that EM converges slowly in the presence of overlapping clusters. Xu and Jordan (1996) analyze the rate of convergence of the EM algorithm and show that EM exhibits a super-linear rate of convergence as the overlap among the mixture components goes to zero (Xu & Jordan, 1996). They forge a connection between the EM algorithm and gradient ascent and prove that rate of convergence of the EM algorithm depends on the condition number of a projected Hessian matrix EP (Θ∗)H(Θ∗)E, where Θ∗ is the optimum parameter value, E = [e1, . . . , em] is a set of unit basis vectors spanning the constrained parameter space (satisfying the constraint ∑K j=1 αj = 1), P (Θ ∗) is a projection matrix, and H(Θ∗) is the Hessian of the log-likelihood function. Later, Ma et al. (2000) extend this result and show that the rate of convergence of EM is a higher order infinitesimal of maximum pairwise overlap among the mixture components.",
      "startOffset" : 120,
      "endOffset" : 1115
    }, {
      "referenceID" : 1,
      "context" : "Related Work The EM algorithm is guaranteed to monotonically converge to local optima under mild continuity conditions (Dempster et al., 1977; Wu, 1983). Redner and Walker (1984) show that EM has linear rate of convergence and suggest that Newton’s or Quasi-Newton methods should be preferred over the EM algorithm. They also experimentally show that EM converges slowly in the presence of overlapping clusters. Xu and Jordan (1996) analyze the rate of convergence of the EM algorithm and show that EM exhibits a super-linear rate of convergence as the overlap among the mixture components goes to zero (Xu & Jordan, 1996). They forge a connection between the EM algorithm and gradient ascent and prove that rate of convergence of the EM algorithm depends on the condition number of a projected Hessian matrix EP (Θ∗)H(Θ∗)E, where Θ∗ is the optimum parameter value, E = [e1, . . . , em] is a set of unit basis vectors spanning the constrained parameter space (satisfying the constraint ∑K j=1 αj = 1), P (Θ ∗) is a projection matrix, and H(Θ∗) is the Hessian of the log-likelihood function. Later, Ma et al. (2000) extend this result and show that the rate of convergence of EM is a higher order infinitesimal of maximum pairwise overlap among the mixture components. Salakhutdinov et al. (2003) propose the Expectation Conjugate Gradient (ECG) algorithm for highly overlapping Gaussian mixtures.",
      "startOffset" : 120,
      "endOffset" : 1296
    }, {
      "referenceID" : 14,
      "context" : "We explain it using the framework proposed by Xu and Jordan (1996). Xu and Jordan (and later Ma et al.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "Xu and Jordan (and later Ma et al. (2000)) have shown that the rate of convergence of EM is upper bounded by the norm ∥",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : "We compare the result with two other well-known optimization techniques: Expectation Conjugate Gradient (ECG) (Salakhutdinov et al., 2003) and QuasiNewton Method (BFGS) (Liu & Nocedal, 1989).",
      "startOffset" : 110,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "Ueda and Nakano (1998) proposed a Deterministic Annealing Expectation Maximization (DAEM) algorithm that varies the temperature from high to low temperature, and deterministically optimizes the objective function at each temperature.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "On the other hand, as β becomes larger, DAEM allows less and less overlap and for β → ∞ it becomes analogous to the winner-takeall (Kearns et al., 1998) approach, i.",
      "startOffset" : 131,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "Using the results shown by Ma et al. (2000), we explain the faster convergence of the proposed Anti-annealing method.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "Ma et al. (2000) show that the rate of convergence r is a higher order infinitesimal of e(Θ∗):",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 12,
      "context" : "Salakhutdinov et al. (2003) proposed an expectation conjugate gradient (ECG) method for optimizing the log-likelihood functions in the case of highly overlapping clusters.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : "To ensure that the ECG algorithm satisfies the same set of constraints, Salakhutdinov et al. (2003) propose to re-parameterize the model parameters: αj = e λj P",
      "startOffset" : 72,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "The Dirichlet process mixture model (DPMM) (Rasmussen, 2000) has gained significant attention for flexible density estimation.",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "extended to DPMM by a straight-forward modification of equation (3) (Katahira et al., 2008):",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "Salakhutdinov et al. (2003) proposed a hybrid EMECG algorithm, that estimates the entropy of cluster memberships as a measure of missing information (in other words, cluster overlap), and chooses to apply ECG if the entropy value is larger than certain threshold.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "However, the temperature scheduling of Deterministic Annealing often requires some tuning (Rangarajan, 2000).",
      "startOffset" : 90,
      "endOffset" : 108
    } ],
    "year" : 2012,
    "abstractText" : "The speed of convergence of the Expectation Maximization (EM) algorithm for Gaussian mixture model fitting is known to be dependent on the amount of overlap among the mixture components. In this paper, we study the impact of mixing coefficients on the convergence of EM. We show that when the mixture components exhibit some overlap, the convergence of EM becomes slower as the dynamic range among the mixing coefficients increases. We propose a deterministic anti-annealing algorithm, that significantly improves the speed of convergence of EM for such mixtures with unbalanced mixing coefficients. The proposed algorithm is compared against other standard optimization techniques like BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, we propose a similar deterministic antiannealing based algorithm for the Dirichlet process mixture model and demonstrate its advantages over the conventional variational Bayesian approach.",
    "creator" : "LaTeX with hyperref package"
  }
}