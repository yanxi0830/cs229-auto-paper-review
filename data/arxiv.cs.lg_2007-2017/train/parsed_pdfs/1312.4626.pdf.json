{
  "name" : "1312.4626.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Compact Random Feature Maps",
    "authors" : [ "Raffay Hamid", "Ying Xiao" ],
    "emails" : [ "raffay@gmail.com", "ying.xiao@gatech.edu", "agittens@ebay.com", "ddecoste@ebay.com" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Kernel methods allow implicitly learning nonlinear functions using explicit linear feature spaces (Schölkopf et al., 1999). These explicit feature spaces are typically high dimensional, and often pose what is called the curse of dimensionality. One solution to this problem is the well known kernel trick (Aizerman et al., 1964), where instead of directly learning a hyperplane classifier in Rd, one considers a non-linear mapping Φ : Rd → H, such that for all x,y ∈ Rd, 〈Φ(x),Φ(y)〉H = K(x,y) for some kernel\nK(x,y). One then learns a classifier H : x 7→ wTΦ(x) for some w ∈ H.\nIt has been observed however that with increase in training data size, the support of the vector w can undergo unbounded growth, which can result in increased training as well as testing time (Steinwart, 2003) (Bengio et al., 2006). Previous approaches to address this curse of support have mostly focused on embedding the non-linear feature space H into a low dimensional Euclidean space while incurring an arbitrarily small distortion in the inner product values (Rahimi & Recht, 2007) (Kar & Karnick, 2012) (Pham & Pagh, 2013). One way to do this is to construct a randomized feature map Z : Rd → RD such that for all x,y ∈ Rd, 〈Z(x),Z(y)〉 = K(x,y). Each component of Z(x) can be computed by first projecting x onto a set of randomly generated d dimensional vectors sampled from a zero-mean distribution, followed by computing the dot-products of the projections. While randomized feature maps are applicable to approximate the more general class of dot-product kernels, in this work we focus on analyzing polynomial kernels, where K(x,y) is of the form (〈x,y〉+q)r, with q ∈ N0 and r ∈ R+.\nIt has been shown that |〈Z(x),Z(y)〉 − K(x,y)| reduces exponentially as a function of D (Kar & Karnick, 2012) (Pham & Pagh, 2013). However in practice, to approximate K(x,y) well, D can still need to be increased to values that may not be amenable from the perspective of learning a classifier in RD. This is especially true for higher values of r. Furthermore, we show that the feature spaces constructed by random feature maps are over complete and rank deficient. This rank deficiency can in turn result in the under-utilization of the projected feature space from a learning perspective where the model parameters learned in RD can have a\nar X\niv :1\n31 2.\n46 26\nv1 [\nst at\n.M L\n] 1\n7 D\nsignificant number of components very close to zero.\nThis presents us with the dilemma whether to create feature maps that approximate exact kernel values accurately, or ones that enable efficient classifier learning. To resolve this dilemma, we propose compact random feature maps (CRAFTMaps) as a more concise representation of random feature maps that can approximate polynomial kernels more accurately. We show that the information content of Z : Rd → RD can be captured more compactly by generating an alternate random feature map Q : RD → RE, such that E < D, and 〈Q(Z(x)), Q(Z(y))〉 approximates 〈Z(x),Z(y)〉. CRAFTMaps are therefore constructed by first up projecting the original data non-linearly to RD in order to minimize |〈Z(x),Z(y)〉 − K(x,y)|. This is followed by linearly down projecting the upprojected vectors to RE with E < D in order to capture the underlying structure in RD more compactly. We present both analytical as well as empirical evidence of the fact that the “up/down” projections employed by CRAFTMaps approximate K(x,y) better than a direct random polynomial feature map Z : Rd → RE.\nThe additional cost of down projecting from RD to RE incurred by CRAFTMaps is well-justified by the efficiency gains they offer in terms of training in RE. To further improve the efficiency of CRAFTMaps, we show how they can be generated using structured random matrices, in particular Hadamard transform, that reduces the cost of multiplying two n×n matrices from O(n3) to O(n2log(n)). This gain is exploited for both up as well as down projection steps of CRAFTMaps. Note that while down-projection using structured random matrices is straight forward (Tropp, 2011), we need to incorporate a few novel modifications to previous structured random projection approaches before they can be used for the up-projection step (see § 3.5).\nThe compactness of CRAFTMaps makes them particularly suitable for using Hessian based methods to learn classifiers in a single pass over the data. Moreover, we show how CRAFTMaps can be used to learn multi-class classifiers in a streaming manner, using the previously proposed framework of error correcting output codes (ECOCs) (Dietterich & Bakiri, 1994), to minimize the least square error between the predicted and the true class labels. This combination of CRAFTMaps and ECOCs is particularly powerful as it can be formalized as a matrix-matrix multiplication, and can therefore maximally exploit the multi-core processing power of modern hardware using BLAS3 (Golub & Van Loan, 2012). Finally, by requiring minimal communication among mappers, this framework is well-suited for map-reduce based settings."
    }, {
      "heading" : "2. Related Work",
      "text" : "Extending the kernel machines framework to large scale learning has been explored in a variety of ways (Bottou et al., 2007). The most popular of these approaches are decomposition methods for solving Support Vector Machines (Platt, 1999) (Chang & Lin, 2011). While in general extremely useful, these methods do not always scale well to problems with more than a few hundreds of thousand data-points.\nTo solve this challenge, several schemes have been proposed to explicitly approximate the kernel matrix, including low-rank approximations (Blum, 2006) (Bach & Jordan, 2005), sampling individual entries (Achlioptas et al., 2002), or discarding entire rows (Drineas & Mahoney, 2005). Similarly, fast nearest neighbor lookup methods have been used to approximate multiplication operations with the kernel matrix (Shen et al., 2005). Moreover, formulations leveraging concepts from computational geometry have been explored to obtain efficient approximate solutions for SVM learning (Tsang et al., 2006).\nAn altogether different approximation approach that has recently gained much interest is to approximate the kernel function directly as opposed to explicitly operating on the kernel matrix. This can be done by embedding the non-linear kernel space into a low dimensional Euclidean space while incurring an arbitrarily small additive distortion in the inner product values (Rahimi & Recht, 2007). By relying only on the embedded space dimensionality, this approach presents a potential solution to the aforementioned curse of support, and is similar in spirit to previous efforts to avoid the curse of dimensionality in nearest neighbor problems (Indyk & Motwani, 1998).\nBesides (Rahimi & Recht, 2007), there have been several approaches proposed to approximate other kernels such as group invariant (Li et al., 2010), intersection (Maji & Berg, 2009), and RBF kernels (Vempati et al., 2010). More recently, there has been an interest in approximating polynomial kernels using random feature maps (Kar & Karnick, 2012) and random tensor products (Pham & Pagh, 2013). Our work builds on these approaches and provides a more compact representation of approximating polynomial kernels more accurately."
    }, {
      "heading" : "3. Compact Random Feature Maps",
      "text" : "We begin by demonstrating that previous approaches for approximating polynomial kernels (Kar & Karnick, 2012) (Pham & Pagh, 2013) construct rank-deficient spaces. As a solution to this challenges, we present the framework of CRAFTMaps, followed by proving their\nAlgorithm 1 – Random Feature Maps (RFM)\nInput: Kernel parameters q and r, output dimensionality D, sampling parameter p > 0 Output: Random feature map Z : Rd → RD such that 〈Z(x),Z(y)〉 ≈ K(x,y) 1: Set f(x) =\n∞∑ n=0 anx n where an = fn(0) n!\n2: for each i = 1 to D do 3: Set N ∈ N0 for P [N = n] = 1pn+1 4: Sample w1, · · ·,wN ∈ {−1, 1}d\n5: Set Zi : x 7→ √ aNpN+1 N∏ j=1 wTj x 6: Construct Z : x 7→ 1√ D (Z1, · · ·, ZD)\nerror bounds and explaining how to generate them efficiently using randomized Hadamard transform."
    }, {
      "heading" : "3.1. Preliminaries",
      "text" : "Following (Kar & Karnick, 2012), consider a positive definite kernel K : (x,y) 7→ f(〈x,y〉), where f admits a Maclaurin expansion with only non-negative coefficients, i.e., f(x) = ∑∞ n=0 anx\nn, where an ≥ 0. An example of such a kernel is the polynomial kernel K(x,y) = (〈x,y〉+q)r, with q ∈ N0 and r ∈ R+. By defining estimators for each individual term of the kernel expansion, one can approximate the exact kernel dot-products. To this end, let w ∈ Rd be a Rademacher vector, i.e., each of its components are chosen independently using a fair coin toss from the set {−1, 1}. It can be shown that the feature map Z : Rd → RD, Z : x 7→ √ aNpN+1 ∏N j=1 w T j x gives an unbiased estimate of the polynomial kernel. Here P[N = n] = 1/(pn+1), and w1, · · ·,wN are N independent Rademacher vectors. Generating D such feature maps independently and concatenating them together constructs a multi-dimensional feature map Z : Rd → RD,Z : x 7→ 1/ √ D(Z1(x), · · ·, ZD(x)), such that E (〈Z(x),Z(y)〉) = K(x,y). The procedure for generating random feature maps for polynomial kernels is listed in Algorithm 1 and illustrated in Figure 1.\n\n1 3 10 9 7\n1 3 10 9 7 a = {0, 0, 2}n q = 0, r = 2, D = 4\n2 -10 -28 16 -4 24 -14 2\n-10 -224 -48 -14\n-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n-1 -1 -1 -1\n-1 -1 -1\n1 1 1 1 1 1 1 1 1\n1 1 1 1 1 1 1\n1 1 1\n\n\n\n\nInput Vector Random Projections Projection Products\nRandom Feature\nMap\nD\nr\nD d\nd\nFigure 1. shows Algorithm 1 projecting a 5 dimensional input vector to a random feature map for a 2nd order homogenous polynomial kernel in 4 dimensions.\nAlgorithm 2 – CRAFTMaps using RFM\nInput: Kernel parameters q and r, up and down projection dimensionalities D and E such that E < D, sampling parameter p > 0 Output: CRAFTMap G : Rd → RE, such that 〈G(x),G(y)〉 ≈ K(x,y)\n1: Up Project: Using Algorithm 1, construct random feature map Z : Rd → RD, such that 〈Z(x),Z(y)〉 ≈ K(x,y) 2: Down Project: Using Johnson-Lindenstrauss random projection, linearly down-project Z to construct G : RD → RE such that 〈G(Z(x)),G(Z(y)〉 ≈ 〈Z(x),Z(y)〉."
    }, {
      "heading" : "3.2. Limitations of Random Feature Maps",
      "text" : "Random feature maps are an efficient means to approximate the underlying eigen structure of the exact kernel space. However, their efficiency can come at the cost of their rank deficiency. Consider for instance Figure 2(a) where the black graph shows the log-scree plot of the exact 7th order polynomial kernel (q = 1) obtained using 1000 randomly selected set of points from MNIST data. The red graph shows the log-scree plot for the random feature map (Kar & Karnick, 2012) in a 212 dimensional space. It can be observed that the red plot is substantially lower than the black one for majority of the spectrum range. Note that this rank deficiency is also true for the space generated by random tensor products (Pham & Pagh, 2013) whose log-scree plot is shown in green in Figure 2(a).\nThis rank deficiency can result in the under-utilization of the projected feature space. Figure 2(b) shows the histogram of the linear weight vector learned in a 212 dimensional random feature map (Kar & Karnick, 2012) for a 7th order polynomial kernel (q = 1). The plot was obtained for 1000 randomly selected points from MNIST data for two class-sets. The spike at zero shows that a majority of the learned weight components do not play any role in classification."
    }, {
      "heading" : "3.3. CRAFTMaps using Up/Down Projections",
      "text" : "To address the limitations of random feature maps, we propose CRAFTMaps as a more accurate approximation of polynomial kernels. The intuition behind CRAFTMaps is to first capture the eigen structure of the exact kernel space comprehensively, followed by representing it in a more concise form. CRAFTMaps are therefore generated in the following two steps:\nUp Projection: Since the difference between 〈Z(x),Z(y)〉 and K(x,y) reduces exponentially as a function of the dimensionality of Z (Kar & Karnick,\n2012) (Pham & Pagh, 2013), we first up project the original data non-linearly from Rd to a substantially higher dimensional space RD to maximally capture the underlying eigen structure of the exact kernel space.\nDown Projection: Since the randomized feature map Z : Rd → RD generated as a result of the upprojection step is fundamentally rank-deficient (§ 3.2), we linearly down project Z to a lower-dimensional map G : RD → RE, such that E < D, and 〈G(Z(x)), G(Z(y))〉 ≈ 〈Z(x),Z(y)〉. The procedure to generate CRAFTMaps is listed in Algorithm 2. Note that while Algorithm 2 uses random feature maps (Kar & Karnick, 2012) for up-projection, one could also use tensor products (Pham & Pagh, 2013) instead to generate Z.\nThe rank improvement brought about by using CRAFTMaps for random feature maps and tensor sketch is shown in Figure 2-a by the dotted red and green plots respectively. The improved utilization of the projected space of random feature maps due to CRAFTMaps is demonstrated in Figure 2(c)."
    }, {
      "heading" : "3.4. Error Bounds for CRAFTMaps",
      "text" : "Recall that the following result obtained using an application of the Hoeffding inequality (Hoeffding, 1963) is central to the analysis of (Kar & Karnick, 2012):\nPr (|〈Z(x),Z(y)〉 −K(x,y)| > ε) ≤ 2 exp ( −Dε 2\n8C2Ω\n) (1)\nWe first examine this inequality more closely for homogenous polynomial kernels K(x,y) = 〈x,y〉r for all points on the unit sphere. In that case we have,\nC2Ω = (pf(pR 2))2 =\n( 1\n2r+1\n)2 d2r (2)\nwhere R = max ‖x‖`1 = √ d and a suitable choice for p is 1/2. We only get a non-trivial bound when D & ε−2d2r. Note however that if we used explicit kernel expansion, we would need substantially fewer features (at most ( d+r−1 r ) ). The same holds for (Pham & Pagh, 2013) since they apply the same Hoeffding inequality, and the analysis produces the same asymptotics.\nWe therefore first present an improved error analysis of (Kar & Karnick, 2012), focusing on homogeneous polynomial kernels. We then use this analysis to prove error bounds of CRAFTMaps. Note that these bounds are independent of the dimensionality of the input space, which is a significant improvement over both (Kar & Karnick, 2012) and (Pham & Pagh, 2013).\nLemma 3.1. Fix an integer r ≥ 2, and define SD as:\nSD = D∑ i=1 r∏ j=1 〈x, ωi,j〉〈x′, ωi,j〉\nwhere x,x′ are vectors of unit Euclidean length, and ωi,j ∼ N (0, Id) are independent Gaussian vectors. Then whenever D ≥ 3 · 4r+2ε−2,\nPr (∣∣∣∣ 1DSD − 〈x,x′〉r ∣∣∣∣ ≥ ε) ≤ cr exp ( −1 2 ( Dε2 11 ) 1 2r+2 ) where 0 < c < 0.766 is a universal constant.\nProof: Let Yi = ∏r j=1〈x, ωi,j〉〈x′, ωi,j〉, then the deviation of SD from its mean is estimated by the rate at which the tails of Yi decay, which is in turn determined by the rates at which the moments of Yi grow. We first verify that the expectation of the summands indeed equals 〈x,x′〉r:\nE (Yi) = r∏ j=1 E ( xTωi,jω T i,jx ′) = 〈x,x′〉r\nSimilarly, the kth moment of Yi can be determined as: E ( |Yi|k ) = r∏ j=1 E ( |tr ( xTωi,jω T i,jx ′) |k)\n≤ r∏ j=1 [∥∥x′xT∥∥k 2 E ( tr ( xTωi,jω T i,jx )k)]\n= r∏ j=1 E ( |ωTi,jx|2k ) = r∏ j=1 E ( |γj |2k ) = [( 1\n2\n)k (2k)!\nk!\n]r ≤ ( √ 2)r ( 2k\ne\n)rk ≤ crkrk\nHere γj ∼ N (0, 1), c = √\n2(2/e)k, and the last three expressions above follow from the formula for the moments of a standard Gaussian random variables (Patel\n& Read, 1996). We now estimate moments of feature map approximation error.\nQ = 1\nDk E (∣∣∣ D∑ i=1 (Yi − E (Yi)) ∣∣∣k)\nAssuming k ≥ 2, and using Marcinkiewicz–Zygmund inequality (Burkholder, 1988) we have:\nQ ≤ (\nk√ D\n)k E ( |Yi − E (Yi) |k ) A standard estimate of the right-hand quantity using Jenson’s inequality allows us to conclude that\nQ ≤ (\n2k√ D\n)k E ( |Yi|k ) ≤ cr ( 2k√\nD\n)k krk\nFinally, we apply Markov’s inequality to bound the tails of the approximation error:\nPr (∣∣∣∣∣ 1D D∑ i=1 Yi − 〈x,x′〉r ∣∣∣∣∣ ≥ ε ) ≤ Q εk ≤ cr ( 2k ε √ D )k krk\n= cr exp ( k [ log(2kr+1)− log(ε √ D) ])\nFixing α > 0 and assuming that D > e2α4r+2ε−2 and k = b(ε2De−2α/4)1/(2r+2)c ensures that\nlog(2kr+1)− log(ε √ D) ≤ −α\nand k ≥ 2, so our earlier assumption when applying Marcinkiewicz–Zygmund inequality is valid. Thus\nPr (∣∣∣∣∣ 1D D∑ i=1 Yi − 〈x,x′〉r ∣∣∣∣∣ ≥ ε ) ≤ cr exp ( −α ( Dε2 4e2α )ρ) where ρ = 1/(2r+ 2) and c ≤ √\n2(2/e)2 < 0.766. Take α = 1/2 to reach the bound in the theorem.\nApplying Lemma 3.1, the following corollary follows:\nCorollary 3.2. Let X ⊂ Rd be a set of n unit vectors. Let ωi,j ∼ N(0, Id) be a set of r ·D independent Gaussian random vectors. If D & 4r+1 log(n)2r+2ε−2 then we have with high probability:∣∣∣∣∣∣ 1D D∑ i=1 r∏ j=1 〈x, ωi,j〉 〈x′, ωi,j〉 − 〈x,x′〉 r\n∣∣∣∣∣∣ ≤ ε which holds simultaneously ∀ x,x′ ∈ X.\nProof: We apply the Lemma 3.1 along with the trivial union bound over O(n2) points. Thus, we require\nexp(log(n2)− (Dε2)1/(2r+2)) to be small. In this case, picking D ≥ log(n2)(2r+2)ε−2 suffices.\nAn alternate way to view this is to fix D, in which case the final approximation error will be bounded by:\nε . log(n2)r+1/ √ D (3)\nWe can combine this with a usual JohnsonLindenstrauss (Johnson & Lindenstrauss, 1984) random projection as follows:\nTheorem 3.3. Let X ⊂ Rd be a set of n unit vectors. Suppose we map these vectors using a random feature map Z : Rd → RD composed with a JohnsonLindenstrauss map Q : RD → RE, where D ≥ E, to obtain Z′, then the following holds: ∣∣〈x,x′〉r − 〈Z′(x),Z′(y)〉∣∣ . 2r+1 log(n)r+1 D1/2 + log(n)1/2 E1/2\nwith high probability ∀ x,x′ ∈ X simultaneously.\nProof: A Johnson-Lindenstrauss projection from RD to RE preserves with high probability all pairwise inner products of the n points {Z(x) : x ∈ X} in RD to within an additive factor of ε′ . log(n)1/2/E1/2. Applying the triangle inequality:\n|〈x,y〉r − 〈Z′(x),Z′(y)〉| ≤ |〈x,y〉r − 〈Z(x),Z(y)〉|+ |〈Z(x),Z(y)〉 − 〈Z′(x),Z′(y)〉| := ε+ ε′\nReferring to Equation 3 to bound ε, we obtain the final error bound:\nε+ ε′ . 2r+1 log(n)r+1\nD1/2 +\nlog(n)1/2\nE1/2\nIn particular, the error is lower than random feature maps (Kar & Karnick, 2012) whenever:\n2r+1 log(n)r+1\nD1/2 +\nlog(n)1/2\nE1/2 .\n2r+1 log(n)r+1\nE1/2\nFixing D = g(r)E for some constant g(r) ≥ 1, CRAFTMaps provide a better error bound when:\ng(r) &\n( log(n)r+1/2\nlog(n)(r+1/2) − 2−(r+1)\n)2 ≈ 1"
    }, {
      "heading" : "3.5. Efficient CRAFTMaps Generation",
      "text" : "Recall that for Hessian based optimization of linear regression problems, the dominant cost of O(nD2) is spent calculating the Hessian. By compactly representing random feature maps in RE as opposed to RD for E < D, CRAFTMaps provide a factor of D2/E2\ngain in the complexity of Hessian computation. A straightforward version of CRAFTMaps would incur an additional cost of O(nDE) for the down-projection step. However, since for problems at scale n >> D, the gains CRAFTMaps provide for classifier learning over random feature maps is well worth the relatively small additional cost they incur.\nThese gains can be further improved by using structured random matrices for the up/down projections of CRAFTMaps. One way to do this is to use the Hadamard matrix as a set of orthonormal bases, as opposed to using a random bases-set sampled from a zero mean distribution. The structured nature of Hadamard matrices enables efficient recursive matrixmatrix multiplication that only requires O(n2log(n)) operations compared to the O(n3) operations needed for the product of two n × n non-structured matrices. Constructing CRAFTMaps using Hadamard transform can therefore reduce the complexity of up projection from O(nDd) to O(nDlog(d)), and that of down projection from O(nD2) to O(nDlog(D)) respectively. To employ Hadmard matrices for efficient CRAFTMaps generation, we use the sub-sampled randomized Hadamard transform (SRHT) (Tropp, 2011).\nWhile SRHT can be used directly for the downprojection step, we need to incorporate a few novel modifications to it before it can be used for upprojection. In particular, given a kernel function K : (x,y) 7→ f(〈x,y〉) and a d dimensional1 vector x, we first construct T = d ∑D i=1 Ni)/de copies of x, where N is defined in Algorithm 1. Each copy xt is multiplied by a diagonal matrix Mt whose entries are set to +1 or −1 with equal probability. Each matrix Mtxt is implicitly multiplied by the d × d Hadamard matrix H. All rows of HMtxt for all t = {1, · · ·,T} are first concatenated, and then randomly permuted, to be finally used according to Algorithm 1 to non-linearly up-project x from Rd to RD (see Figure 3)."
    }, {
      "heading" : "4. Classification Using ECOCs",
      "text" : "To solve multi-class classification problems, we use error correcting output codes (ECOCs) (Dietterich & Bakiri, 1994) which employ a unique binary “codeword” of length c for each of the k classes, and learn c binary functions, one for each bit position in the codewords. For training, using an example from class i, the required outputs of the c binary functions are specified by the codeword for class i. Given a test instance x, each of the c binary functions are evaluated to compute a c-bit string s. This string is compared to the k codewords, assigning x to the class whose codeword is\n1As Hadamards exist in powers of 2, usually x needs to be zero-padded to the closest higher power of 2.\nclosest to s according to some distance.\nOverall, given d dimensional data from k classes, we first use up/down projections to construct its CRAFTMap representation in RE. We then use the framework of ECOCs to learn c binary linear regressors in RE. We perform multi-fold cross validation on the training data to select one regularization parameter λ that is used for all the c codeword classifiers. To test a d dimensional example, it is first up/down projected to RE, and then passed through ECOCs to be classified to one of the k classes."
    }, {
      "heading" : "5. Experiments and Results",
      "text" : "We now present reconstruction and classification results of CRAFTMaps on multiple data-sets."
    }, {
      "heading" : "5.1. Reconstruction Error",
      "text" : "Figure 4 shows the normalized root mean square (nrms) errors obtained while reconstructing the polynomial kernel with r = 7 and q = 1 using random feature maps (Kar & Karnick, 2012) and tensor sketching (Pham & Pagh, 2013) versus their respective CRAFTMap representations. Results over 6 different data-sets are presented. All graphs in each plot were obtained using 10 folds of 1000 randomly selected data points from a particular data-set. As shown, CRAFTMaps provide a significant reconstruction improvements for random maps and tensor sketching.\nFigure 5 shows the reconstruction improvements due to CRAFTMaps as a function of polynomial degree. These results were obtained using 10 sets of 1000 randomly picked points from MNIST data. As shown, CRAFTMaps consistently improve the reconstruction error over a range of polynomial degrees."
    }, {
      "heading" : "5.2. Classification Error",
      "text" : "Table 1 shows the test classification errors obtained using random feature maps (Kar & Karnick, 2012) and\ntensor sketching (Pham & Pagh, 2013) versus their CRAFTMap representations. Results over 4 different data-sets are presented, on which CRAFTMaps consistently delivered improved classification performance.\nWe now explain results for CRAFTMaps on MNIST data for small and substantially large projected feature spaces. We also explain CRAFTMaps results on very large amounts of training data using MNIST8M.\nSmall Feature Spaces: Table 1-a shows MNIST results on feature space sizes 300 to 700 dimensions. Note that for RE < d (which for MNIST is 784 ), the random feature maps cannot use the H-0/1 heuristic of (Kar & Karnick, 2012). CRAFTMaps however do not have this limitation as even for E < d, D can still be >> d. This allows CRAFTMaps to use the H-0/1 heuristic in RD, which in turn reflects in RE. This results in substantial classification gains achieved by CRAFTMaps for small-sized feature spaces, and highlights their usefulness in applications with low memory footprint such as mobile phone apps.\nLarge Feature Spaces: Table 1-b shows the MNIST results on feature space sizes 212 to 216 dimensions. It can be seen that CRAFTMaps consistently gave improved test error and achieved 1.12% test classification rate using the original 60K training data (unitlength normalized, non-jittered and non-deskewed).\nResults on MNIST8M Data Figure 6 shows the performance of CRAFTMaps in comparison to random feature maps for a given sized RE (214) as the number of examples vary from 60 thousand to 8.1 million. This experiment uses the same set of 10 thousand test points as used for the experiments with MNIST data. It can be seen that CRAFTMaps on random feature maps converge the fastest, and consistently gives better classification performance compared to the other representations. These results were obtained using a polynomial kernel with r = 7, q = 1, D = 217,\nE = 214, and ECOCs equal to 200. As we increase E to 216 and D to 219 using CRAFTMaps on RFM for 7th order polynomial kernel (q = 1), we achieved test classification error of 0.91% on MNIST8M data-set."
    }, {
      "heading" : "5.3. Run-Time Analysis",
      "text" : "Figure 7 shows the log-log scatter plot of the compute times (projection + Hessian) for random feature maps (Kar & Karnick, 2012), tensor sketching (Pham & Pagh, 2013), and CRAFTMaps using random feature maps (with H-01 heuristic). These times were recorded for MNIST data using a 40-core machine. Notice that CRAFTMaps show significant per unittime classification improvements towards the right end of the x-axis. This is because as the size of the projected space increases, the Hessian computation cost becomes dominant. This naturally gives CRAFTMaps an edge given their ability to encode information more compactly. The performance gain of CRAFTMaps are expected to grow even more as training size increases."
    }, {
      "heading" : "6. Conclusions and Future Work",
      "text" : "In this work, we proposed CRAFTMaps to approximate polynomial kernels more concisely and accurately compared to previous approaches. We theoretically proved error bounds of CRAFTMaps and presented empirical results to demonstrate their effectiveness.\nAn important context where CRAFTMaps are particularly useful is the map-reduce setting. By computing a single Hessian matrix (with different gradients for each ECOC) in a concise feature space, CRAFTMaps provide an effective way to learn multi-class classifiers in a single-pass over large amounts of data. Moreover, their ability to compactly capture the eigen structure of the kernel space makes CRAFTMaps suitable for smaller scale applications such as mobile phone apps."
    } ],
    "references" : [ {
      "title" : "Sampling Techniques for Kernel Methods",
      "author" : [ "D. Achlioptas", "F. McSherry", "B. Schölkopf" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Achlioptas et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Achlioptas et al\\.",
      "year" : 2002
    }, {
      "title" : "Theoretical foundations of the potential function method in pattern recognition learning",
      "author" : [ "A. Aizerman", "E.M. Braverman", "L.I. Rozoner" ],
      "venue" : "Automation and Remote Control,",
      "citeRegEx" : "Aizerman et al\\.,? \\Q1964\\E",
      "shortCiteRegEx" : "Aizerman et al\\.",
      "year" : 1964
    }, {
      "title" : "Predictive low-rank decomposition for kernel methods",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "In Proceedings of the 22nd International Conference on Machine Learning,",
      "citeRegEx" : "Bach and Jordan,? \\Q2005\\E",
      "shortCiteRegEx" : "Bach and Jordan",
      "year" : 2005
    }, {
      "title" : "The Curse of Highly Variable Functions for Local Kernel Machines",
      "author" : [ "Y. Bengio", "O. Delalleau", "N. Le Roux" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bengio et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2006
    }, {
      "title" : "Random Projection, Margins, Kernels, and Feature-Selection",
      "author" : [ "A. Blum" ],
      "venue" : "In Subspace, Latent Structure and Feature Selection. Springer,",
      "citeRegEx" : "Blum,? \\Q2006\\E",
      "shortCiteRegEx" : "Blum",
      "year" : 2006
    }, {
      "title" : "Sharp inequalities for martingales and stochastic integrals",
      "author" : [ "D.L. Burkholder" ],
      "venue" : "Asterisque,",
      "citeRegEx" : "Burkholder,? \\Q1988\\E",
      "shortCiteRegEx" : "Burkholder",
      "year" : 1988
    }, {
      "title" : "LIBSVM: A Library for Support Vector Machines",
      "author" : [ "C. Chang", "C. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Chang and Lin,? \\Q2011\\E",
      "shortCiteRegEx" : "Chang and Lin",
      "year" : 2011
    }, {
      "title" : "Solving Multiclass Learning Problems via Error-Correcting Output Codes",
      "author" : [ "T.G. Dietterich", "G. Bakiri" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Dietterich and Bakiri,? \\Q1994\\E",
      "shortCiteRegEx" : "Dietterich and Bakiri",
      "year" : 1994
    }, {
      "title" : "On the Nyström method for approximating a Gram matrix for improved kernel-based learning",
      "author" : [ "P. Drineas", "M.W. Mahoney" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Drineas and Mahoney,? \\Q2005\\E",
      "shortCiteRegEx" : "Drineas and Mahoney",
      "year" : 2005
    }, {
      "title" : "Matrix Computations",
      "author" : [ "G.H. Golub", "C.F. Van Loan" ],
      "venue" : null,
      "citeRegEx" : "Golub and Loan,? \\Q2012\\E",
      "shortCiteRegEx" : "Golub and Loan",
      "year" : 2012
    }, {
      "title" : "Probability Inequalities for Sums of Bounded Random Variables",
      "author" : [ "W. Hoeffding" ],
      "venue" : "Journal of the American Satistical Association,",
      "citeRegEx" : "Hoeffding,? \\Q1963\\E",
      "shortCiteRegEx" : "Hoeffding",
      "year" : 1963
    }, {
      "title" : "Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality",
      "author" : [ "P. Indyk", "R. Motwani" ],
      "venue" : "In Proceedings of the 30th annual ACM Symposium on the Theory of Computing,",
      "citeRegEx" : "Indyk and Motwani,? \\Q1998\\E",
      "shortCiteRegEx" : "Indyk and Motwani",
      "year" : 1998
    }, {
      "title" : "Extensions of Lipschitz mappings into a Hilbert space",
      "author" : [ "W.B. Johnson", "J. Lindenstrauss" ],
      "venue" : "In Conference on Modern Analysis and Probability,",
      "citeRegEx" : "Johnson and Lindenstrauss,? \\Q1984\\E",
      "shortCiteRegEx" : "Johnson and Lindenstrauss",
      "year" : 1984
    }, {
      "title" : "Random Feature Maps for Dot Product Kernels",
      "author" : [ "P. Kar", "H. Karnick" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Kar and Karnick,? \\Q2012\\E",
      "shortCiteRegEx" : "Kar and Karnick",
      "year" : 2012
    }, {
      "title" : "Random Fourier Approximations for Skewed Multiplicative Histogram Kernels",
      "author" : [ "F. Li", "C. Ionescu", "C. Sminchisescu" ],
      "venue" : "In Pattern Recognition. Springer,",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Max-Margin Additive Classiers for Detection",
      "author" : [ "S. Maji", "A.C. Berg" ],
      "venue" : "In IEEE 12th International Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Maji and Berg,? \\Q2009\\E",
      "shortCiteRegEx" : "Maji and Berg",
      "year" : 2009
    }, {
      "title" : "Fast and Scalable Polynomial Kernels via Explicit Feature Maps",
      "author" : [ "N. Pham", "R. Pagh" ],
      "venue" : "In Proceedings of the 19th International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Pham and Pagh,? \\Q2013\\E",
      "shortCiteRegEx" : "Pham and Pagh",
      "year" : 2013
    }, {
      "title" : "Using Analytic QP and Sparseness to Speed Training of Support Vector Machines",
      "author" : [ "J.C. Platt" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Platt,? \\Q1999\\E",
      "shortCiteRegEx" : "Platt",
      "year" : 1999
    }, {
      "title" : "Random Features for LargeScale Kernel Machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Rahimi and Recht,? \\Q2007\\E",
      "shortCiteRegEx" : "Rahimi and Recht",
      "year" : 2007
    }, {
      "title" : "Advances in Kernel Methods: Support Vector Learning",
      "author" : [ "B. Schölkopf", "C.J.C. Burges", "Smola", "A.J. (eds" ],
      "venue" : null,
      "citeRegEx" : "Schölkopf et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 1999
    }, {
      "title" : "Fast Gaussian Process Regression using KD-Trees",
      "author" : [ "Y. Shen", "A. Ng", "M. Seeger" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Shen et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2005
    }, {
      "title" : "Sparseness of Support Vector Machines",
      "author" : [ "I. Steinwart" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Steinwart,? \\Q2003\\E",
      "shortCiteRegEx" : "Steinwart",
      "year" : 2003
    }, {
      "title" : "Improved analysis of the subsampled randomized Hadamard transform",
      "author" : [ "J.A. Tropp" ],
      "venue" : "Advances in Adaptive Data Analysis,",
      "citeRegEx" : "Tropp,? \\Q2011\\E",
      "shortCiteRegEx" : "Tropp",
      "year" : 2011
    }, {
      "title" : "Core vector machines: Fast SVM training on very large data sets",
      "author" : [ "I.W. Tsang", "J.T. Kwok", "P. Cheung" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Tsang et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Tsang et al\\.",
      "year" : 2006
    }, {
      "title" : "Generalized RBF feature maps for efficient detection",
      "author" : [ "S. Vempati", "A. Vedaldi", "A. Zisserman", "C.V. Jawahar" ],
      "venue" : "In 21st British Machine Vision Conference,",
      "citeRegEx" : "Vempati et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vempati et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Kernel methods allow implicitly learning nonlinear functions using explicit linear feature spaces (Schölkopf et al., 1999).",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "One solution to this problem is the well known kernel trick (Aizerman et al., 1964), where instead of directly learning a hyperplane classifier in R, one considers a non-linear mapping Φ : R → H, such that for all x,y ∈ R, 〈Φ(x),Φ(y)〉H = K(x,y) for some kernel K(x,y).",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "It has been observed however that with increase in training data size, the support of the vector w can undergo unbounded growth, which can result in increased training as well as testing time (Steinwart, 2003) (Bengio et al.",
      "startOffset" : 192,
      "endOffset" : 209
    }, {
      "referenceID" : 3,
      "context" : "It has been observed however that with increase in training data size, the support of the vector w can undergo unbounded growth, which can result in increased training as well as testing time (Steinwart, 2003) (Bengio et al., 2006).",
      "startOffset" : 210,
      "endOffset" : 231
    }, {
      "referenceID" : 22,
      "context" : "Note that while down-projection using structured random matrices is straight forward (Tropp, 2011), we need to incorporate a few novel modifications to previous structured random projection approaches before they can be used for the up-projection step (see § 3.",
      "startOffset" : 85,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "The most popular of these approaches are decomposition methods for solving Support Vector Machines (Platt, 1999) (Chang & Lin, 2011).",
      "startOffset" : 99,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "To solve this challenge, several schemes have been proposed to explicitly approximate the kernel matrix, including low-rank approximations (Blum, 2006) (Bach & Jordan, 2005), sampling individual entries (Achlioptas et al.",
      "startOffset" : 139,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "To solve this challenge, several schemes have been proposed to explicitly approximate the kernel matrix, including low-rank approximations (Blum, 2006) (Bach & Jordan, 2005), sampling individual entries (Achlioptas et al., 2002), or discarding entire rows (Drineas & Mahoney, 2005).",
      "startOffset" : 203,
      "endOffset" : 228
    }, {
      "referenceID" : 20,
      "context" : "Similarly, fast nearest neighbor lookup methods have been used to approximate multiplication operations with the kernel matrix (Shen et al., 2005).",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 23,
      "context" : "Moreover, formulations leveraging concepts from computational geometry have been explored to obtain efficient approximate solutions for SVM learning (Tsang et al., 2006).",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 14,
      "context" : "Besides (Rahimi & Recht, 2007), there have been several approaches proposed to approximate other kernels such as group invariant (Li et al., 2010), intersection (Maji & Berg, 2009), and RBF kernels (Vempati et al.",
      "startOffset" : 129,
      "endOffset" : 146
    }, {
      "referenceID" : 24,
      "context" : ", 2010), intersection (Maji & Berg, 2009), and RBF kernels (Vempati et al., 2010).",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "Recall that the following result obtained using an application of the Hoeffding inequality (Hoeffding, 1963) is central to the analysis of (Kar & Karnick, 2012):",
      "startOffset" : 91,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "Assuming k ≥ 2, and using Marcinkiewicz–Zygmund inequality (Burkholder, 1988) we have:",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : "To employ Hadmard matrices for efficient CRAFTMaps generation, we use the sub-sampled randomized Hadamard transform (SRHT) (Tropp, 2011).",
      "startOffset" : 123,
      "endOffset" : 136
    } ],
    "year" : 2013,
    "abstractText" : "Kernel approximation using randomized feature maps has recently gained a lot of interest. In this work, we identify that previous approaches for polynomial kernel approximation create maps that are rank deficient, and therefore do not utilize the capacity of the projected feature space effectively. To address this challenge, we propose compact random feature maps (CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efficiently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class classifiers. We present experiments on multiple standard data-sets with performance competitive with state-of-the-art results.",
    "creator" : "LaTeX with hyperref package"
  }
}