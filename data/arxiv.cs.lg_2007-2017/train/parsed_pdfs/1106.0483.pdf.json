{
  "name" : "1106.0483.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning unbelievable marginal probabilities",
    "authors" : [ "Xaq Pitkow", "Yashar Ahmadian" ],
    "emails" : [ "xaq@post.harvard.edu", "ya2005@columbia.edu", "ken@neurotheory.columbia.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Calculating marginal probabilities for a graphical model generally requires summing over exponentially many states, and is NP-hard in general [1]. A variety of approximate methods have been used to circumvent this problem. One popular technique is belief propagation (BP), in particular the sumproduct rule, which is a message-passing algorithm for performing inference on a graphical model [2]. Though exact and efficient on trees, it is merely an approximation when applied to graphical models with loops.\nA natural question is whether one can compensate for the shortcomings of the approximation by setting the model parameters appropriately. In this paper, we prove that some sets of marginals simply cannot be achieved by belief propagation. For these cases we provide a new algorithm that can achieve much better results by using an ensemble of parameters rather than a single instance.\nWe are given a set of variables x with a given probability distribution P (x) of some data. We would like to construct a model that reproduces certain of its marginal probabilities, in particular those over individual variables pi(xi) = ∑ x\\xi P (x) for nodes i ∈ V , and those over some relevant clusters\nof variables, pα(xα) = ∑ x\\xα P (x) for α = {i1, . . . , idα}. We will write the collection of all these marginals as a vector p.\nar X\niv :1\n10 6.\n04 83\nv1 [\ncs .A\nI] 2\nWe assume a model distribution Q0(x) in the exponential family taking the form\nQ0(x) = e −E(x)/Z (1) with normalization constant Z = ∑ x e −E(x) and energy function\nE(x) = − ∑ α θα · φα(xα) (2)\nHere, α indexes sets of interacting variables (factors in the factor graph [3]), and xα is a subset of variables whose interaction is characterized by a vector of sufficient statistics φα(xα) and corresponding natural parameters θα. We assume without loss of generality that each φα(xα) is irreducible, meaning that it cannot be written as a sum of any linearly independent functions that themselves do not depend on any xi for i ∈ α. We collect all these sufficient statistics and natural parameters in the vectors φ and θ.\nNormally when learning a graphical model, one would fit its parameters so the marginal probabilities match the target. Here, however, we will not use exact inference to compute the marginals. Instead we will use approximate inference via loopy belief propagation to match the target."
    }, {
      "heading" : "2 Learning in Belief Propagation",
      "text" : ""
    }, {
      "heading" : "2.1 Belief propagation",
      "text" : "The sum-product algorithm for belief propagation on a graphical model with energy function (2) uses the following equations [4]:\nmi→α(xi) ∝ ∏\nβ∈Ni\\α mβ→i(xi) mα→i(xi) ∝ ∑ xα\\xi eθα·φα(xα) ∏ j∈Nα\\i mj→α(xj) (3)\nwhere Ni and Nα are the neighbors of node i or factor α in the factor graph. Once these messages converge, the single-node and factor beliefs are given by\nbi(xi) ∝ ∏ α∈Ni mα→i(xi) bα(xα) ∝ eθα·φα(xα) ∏ i∈Nα mi→α(xi) (4)\nwhere the beliefs must each be normalized to one. For tree graphs, these beliefs exactly equal the marginals of the graphical model Q0(x). For loopy graphs, the beliefs at fixed points are often good approximations of the marginals. While they are guaranteed to be locally consistent,∑ xα\\xi bα(xα) = bi(xi), they are not necessarily globally consistent: There may not exist a single joint distribution B(x) of which the beliefs are the marginals [5]. This is why the resultant beliefs are called pseudomarginals, rather than simply marginals. We use a vector b to refer to the set of both node and factor beliefs produced by belief propagation."
    }, {
      "heading" : "2.2 Bethe free energy",
      "text" : "Despite its limitations, BP is found empirically to work well in many circumstances. Some theoretical justification for loopy belief propagation emerged with proofs that its stable fixed points are local minima of the Bethe free energy [6, 7]. Free energies are important quantities in machine learning because the Kullback-Leibler divergence between the data and model distributions can be expressed in terms of free energies, so models can be optimized by minimizing free energies appropriately.\nGiven an energy function E(x) from (2), the Gibbs free energy of a distribution Q(x) is\nF [Q] = U [Q]− S[Q] (5) where U is the average energy of the distribution\nU [Q] = ∑ x E(x)Q(x) = − ∑ α θα · ∑ xα φα(xα)qα(xα) (6)\nwhich depends on the marginals qα(xα) of Q(x), and S is the entropy S[Q] = − ∑ x Q(x) logQ(x) (7)\nMinimizing the Gibbs free energy F [Q] recovers the distribution Q0(x) for the graphical model (1).\nThe Bethe free energy F β is an approximation to the Gibbs free energy,\nF β [Q] = U [Q]− Sβ [Q] (8) in which the average energy U is exact, but the true entropy S is replaced by an approximation, the Bethe entropy Sβ , which is a sum over the factor and node entropies [6]:\nSβ [Q] = ∑ α Sα[qα] + ∑ i (1− di)Si[qi] (9)\nSα[qα] = − ∑ xα qα(xα) log qα(xα) Si[qi] = − ∑ xi qi(xi) log qi(xi) (10)\nThe coefficients di = |Ni| are the number of factors neighboring node i, and compensate for the overcounting of single-node marginals due to overlapping factor marginals. For tree-structured graphical models, which factorize asQ(x) = ∏ α qα(xα) ∏ i qi(xi)\n1−di , the Bethe entropy is exact, and hence so is the Bethe free energy. On loopy graphs, the Bethe entropy Sβ isn’t really even an entropy (e.g. it may be negative) because it neglects all statistical dependencies other than those present in the factor marginals. Nonetheless, the Bethe free energy is often close enough to the Gibbs free energy that its minima approximate the true marginals [8]. Since stable fixed points of BP are minima of the Bethe free energy [6, 7], this helped explain why belief propagation is often so successful.\nTo emphasize that the Bethe free energy directly depends only on the marginals and not the joint distribution, we will write F β [q] where q is a vector of pseudomarginals qα(xα) for all α and all xα. Pseudomarginal space is the convex set [5] of all q that satisfy the positivity and local consistency constraints,\n0 ≤ qα(xα) ≤ 1 ∑ xα\\xi qα(xα) = qi(xi) ∑ xi qi(xi) = 1 (11)"
    }, {
      "heading" : "2.3 Pseudo-moment matching",
      "text" : "We now wish to correct for the deficiencies of belief propagation by identifying the parameters θ so that BP produces beliefs b matching the true marginals p of the target distribution P (x). Since the fixed points of BP are stationary points of F β [6], one may simply try to find parameters θ that produce a stationary point in pseudomarginal space at p, which is a necessary condition for BP to reach a fixed point there. Simply evaluate the gradient at p, set it to zero, and solve for θ.\nNote that in principle this gradient could be used to directly minimize the Bethe free energy, but F β [q] is a complicated function of q that usually cannot be minimized analytically [8]. In contrast, here we are using it to solve for the parameters needed to move beliefs to a target location. This is much easier, since the Bethe free energy is linear in θ. This approach to learning parameters has been described as ‘pseudo-moment matching’ [9, 10, 11].\nThe Lq-element vector q is an overcomplete representation of the pseudomarginals because it must obey the local consistency constraints (11). It is convenient to express the pseudomarginals in terms of a minimal set of parameters η with the smaller dimensionality Lθ as θ and φ, using an affine transform q = Wη + k (12) where W is an Lq × Lθ rectangular matrix. One example is the expectation parameters ηα =∑ xα qα(xα)φα(xα) [5], giving the energy simply as U = −θ · η. The gradient with respect to those minimal parameters is\n∂F β\n∂η = ∂U ∂η − ∂S\nβ\n∂q\n∂q ∂η = −θ − ∂S\nβ\n∂q W (13)\nThe Bethe entropy gradient is simplest in the overcomplete representation q,\n∂Sβ\n∂qα(xα) = −1− log qα(xα)\n∂Sβ\n∂qi(xi) = (−1− log qi(xi))(1− di) (14)\nSetting the gradient (13) to zero, we have a simple linear equation for the parameters θ that tilt the Bethe free energy surface (Figure 1A) enough to place a stationary point at the desired marginals p:\nθ = − ∂S β\n∂q ∣∣∣∣ p W (15)"
    }, {
      "heading" : "2.4 Unbelievable marginals",
      "text" : "It is well known that BP may converge on fixed points that cannot be realized as marginals of any joint distribution. In this section we show that the converse is also true: There are some distributions whose marginals cannot be realized as beliefs for any set of couplings. In these cases, existing methods for learning often yield poor results, sometimes even worse than performing no learning at all. This is surprising in view of claims to the contrary: [9, 5] state that belief propagation run after pseudo-moment matching can always reach a fixed point that reproduces the target marginals. While BP does technically have such fixed points, they are not always stable and thus may not be reachable by running belief propagation. Definition 1. A set of marginals are ‘unbelievable’ if belief propagation cannot converge to them for any set of parameters.\nFor belief propagation to converge to the target — namely, the marginals p — a zero gradient is not sufficient: The Bethe free energy must also be a local minimum [7].1 This requires a positivedefinite Hessian of F β (the ‘Bethe Hessian’H) in the subspace of pseudomarginals that satisfies the local consistency constraints. Since the energy U is linear in the pseudomarginals, the Hessian is given by the second derivative of the Bethe entropy,\nH = ∂2F β ∂η2 = −W> ∂ 2Sβ ∂q2 W (16)\nwhere projection byW constrains the derivatives to the subspace spanned by the minimal parameters η. If this Hessian is positive definite when evaluated at p then the parameters θ given by (15) give F β a minimum at the target p. If not, then the target cannot be a stable fixed point of loopy belief propagation. In Section 3, we calculate the Bethe Hessian explicitly for a binary model with pairwise interactions. Theorem 1. Unbelievable marginal probabilities exist.\nProof. Proof by example. The simplest unbelievable example is a binary graphical model with pairwise interactions between four nodes, x ∈ {−1,+1}4, and the energy E(x) = −J∑(ij) xixj . By symmetry and (1), marginals of this target P (x) are the same for all nodes and pairs: pi(xi) = 12 and pij(xi = xj) = ρ = (2 + 4/(1 + e2J − e4J + e6J))−1. Substituting these marginals into the appropriate Bethe Hessian (22) gives a matrix that has a negative eigenvalue for all ρ > 38 , or J > 0.316. The associated eigenvector u has the same symmetry as the marginals, with singlenode components ui = 12 (−2 + 7ρ − 8ρ2 + √ 10− 28ρ+ 81ρ2 − 112ρ3 + 64ρ4) and pairwise components uij = 1. Thus the Bethe free energy does not have a minimum at the marginals of these P (x). Stable fixed points of BP occur only at local minima of the Bethe free energy [7], and so BP cannot reproduce the marginals p for any parameters. Hence these marginals are unbelievable.\nNot only do unbelievable marginals exist, but they are actually quite common, as we will see in Section 3. Graphical models with multinomial or gaussian variables and at least two loops always have some pseudomarginals for which the Hessian is not positive definite [12]. On the other hand, all marginals with sufficiently small correlations are believable because they are guaranteed to have a positive-definite Bethe Hessian [12]. Stronger conditions have not yet been described."
    }, {
      "heading" : "2.5 Bethe wake-sleep algorithm",
      "text" : "When pseudo-moment matching fails to reproduce unbelievable marginals, an alternative is to use a gradient descent procedure for learning, analagous to the wake-sleep algorithm used to train Boltzmann machines [13]. The original rule can be derived as gradient descent of the Kullback-Leibler\n1Even this is not sufficient, but it is necessary.\ndivergence between the target P (x) and the graphical model Q(x) (1), DKL[P ||Q] = ∑ s P (x) log P (x) Q(x) = F [P ]− F [Q] (17)\nwhere F is the Gibbs free energy (5) using the energy function (2). Here we use a new cost function, the ‘Bethe divergence’ Dβ [p||b], by replacing these free energies by Bethe free energies [14] evaluated at the true marginals p and at the beliefs b obtained from BP fixed points,\nDβ [p||b] = F β [p]− F β [b] (18) We use gradient descent to optimize this cost, with gradient\ndDβ dθ = ∂Dβ ∂θ + ∂Dβ ∂b ∂b ∂θ (19)\nThe data’s free energy does not depend on the beliefs, so ∂F β [p]/∂b = 0, and fixed points of belief propagation are stationary points of the Bethe free energy, so ∂F β [b]/∂b = 0. Consequently ∂Dβ/∂b = 0. Furthermore, the entropy terms of the free energies do not depend explicitly on θ, so\ndDβ dθ = ∂U(p) ∂θ − ∂U(b) ∂θ = −η(p) + η(b) (20)\nwhere η(q) = ∑ x q(x)φ(x) are the expectations of the sufficient statistics φ(x) under the pseudomarginals q. This gradient forms the basis of a simple learning algorithm. At each step in learning, belief propagation is run, obtaining beliefs b for the current parameters θ. The parameters are then changed in the opposite direction of the gradient,\n∆θ = − dDβ dθ = (η(p)− η(b)) (21)\nwhere is a learning rate. This generally increases the Bethe free energy for the beliefs while decreasing that of the data, hopefully allowing BP to draw closer to the data marginals. We call this learning rule the Bethe wake-sleep algorithm.\nWithin this algorithm, there is still the freedom of how to choose initial messages for BP at each learning iteration. The result depends on these initial conditions because BP can have several stable\nfixed points. One might re-initialize the messages to a fixed starting point for each run of BP, choose random initial messages for each run, or restart the messages where they stopped on the previous learning step. In our experiments we use the first approach, initializing to constant messages at the beginning of each BP run.\nThe Bethe wake-sleep learning rule sometimes places a minimum of F β at the true data distribution, such that belief propagation can give the true marginals as one of its (possibly multiple) fixed points. However, for the reasons provided above, this cannot occur where the Bethe Hessian is not positive definite."
    }, {
      "heading" : "2.6 Ensemble belief propagation",
      "text" : "When the Bethe wake-sleep algorithm attempts to learn unbelievable marginals, the parameters and beliefs do not reach a fixed point but instead continue to vary over time (Figure 2A,B). Still, if learning reaches equilibrium, then the temporal average of beliefs is equal to the unbelievable marginals.\nTheorem 2. If the Bethe wake-sleep algorithm reaches equilibrium, then unbelievable marginals are matched by the belief propagation fixed points averaged over the equilibrium ensemble of parameters.\nProof. At equilibrium, the time average of the parameter changes is zero by definition, 〈∆θ〉t = 0. Substitution of the Bethe wake-sleep equation, ∆θ = (η(p) − η(b(t))) (20), directly implies that 〈η(b(t))〉t = η(p). The deterministic mapping (12) from the minimal representation to the pseudomarginals gives 〈b(t)〉t = p.\nAfter learning has equilibrated, fixed points of belief propagation occur with just the right frequency so that they can be averaged together to reproduce the target distribution exactly (Figure 2C). Note that none of the individual fixed points may be close to the true marginals. We call this inference algorithm ensemble belief propagation (eBP).\nEnsemble BP produces perfect marginals by exploiting a constant, small amplitude learning, and thus assumes that the correct marginals are perpetually available. Yet it also works well when learning is turned off, if parameters are drawn randomly from a gaussian distribution with mean and covariance matched to the equilibrium distribution, θ ∼ N (θ̄,Σθ). In the simulations below (Figures 2C–D, 3B–C), Σθ was always low-rank, and only one or two principle components were needed for good performance. The gaussian ensemble is not quite as accurate as continued learning (Figure 3B,C), but the performance is still markedly better than any of the available fixed points.\nIf the target is not within a convex hull of believable pseudomarginals, then learning cannot reach equilibrium: Eventually BP gets as close as it can but there remains a consistent difference η(p) − η(b), so θ must increase without bound. Though possible in principle, we did not observe this effect in any of our experiments. There may also be no equilibrium if belief propagation at each learning iteration fails to converge."
    }, {
      "heading" : "3 Experiments",
      "text" : "The experiments in this section concentrate on the Ising model: N binary variables, s ∈ {−1,+1}N , with factors comprising individual variables xi and pairs xi, xj . The energy function is E(x) = −∑i hixi −∑(ij) Jijxixj . Then the sufficient statistics are the various first and second moments, xi and xixj , and the natural parameters are hi, Jij . We use this model both for the target distributions and the model.\nWe parameterize pseudomarginals as {q+i , q++ij }where q+i = qi(xi = +1) and q++ij = qij(xi = xj = +1) [8]. The remaining probabilities are linear functions of these values. Positivity constraints and local consistency constraints then appear as 0 ≤ q+i ≤ 1 and max(0, q+i + q+j − 1) ≤ q++ij ≤ min(q+i , q + j ). If all the interactions are finite, then the inequality constraints are not active [15]. In\nthis parameterization, the elements of the Bethe Hessian (16) are\n− ∂ 2Sβ\n∂q+i ∂q + j\n= δi,j(1− di) [ (q+i ) −1 + (1− q+i )−1 ] + δj∈Ni [ (1− q+i − q+j + q++ij )−1 ] (22a)\n+ δi,j ∑ k∈Ni [ (q+i − q++ik )−1 + (1− q+i − q+k + q++ik )−1 ] − ∂ 2Sβ\n∂q+i ∂q ++ jk\n=− δi,j [ (q+i − q++ik )−1 + (1− q+i − q+k + q++ik )−1 ] (22b)\n− δi,k [ (q+i − q++ij )−1 + (1− q+i − q+j + q++ij )−1 ] − ∂ 2Sβ\n∂q++ij ∂q ++ k`\n= δij,k` [ (q++ij ) −1 + (q+i − q++ij )−1 + (q+j − q++ij )−1 + (1− q+i − q+j + q++ij )−1 ]\n(22c)\nFigure 3A shows the fraction of marginals that are unbelievable for 8-node, fully-connected Ising models with random coupling parameters hi ∼ N (0, 13 ) and Jij ∼ N (0, σJ). For σJ & 14 , most marginals cannot be reproduced by belief propagation with any parameters, because the Bethe Hessian (22) has a negative eigenvalue.\nWe generated 500 Ising model targets using σJ = 13 , selected the unbelievable ones, and evaluated the performance of BP and ensemble BP for various methods of choosing parameters θ. Each run of BP used exponential temporal message damping of 5 time steps [16], mt+1 = amt + (1 − a)mundamped with a = e−1/5. Fixed points were declared when messages changed by less than 10−9 on a single time step. We evaluated BP performance for the actual parameters that generated the target (1), pseudomoment matching (15), and at best-matching beliefs obtained at any time during Bethe wake-sleep learning. We also measured eBP performance for two parameter ensembles: the last 100 iterations of Bethe wake-sleep learning, and parameters sampled from a gaussian N (θ̄,Σθ) with the same mean and covariance as that ensemble. Belief propagation gave a poor approximation of the target marginals, as expected for a model with many strong loops. Even with learning, BP could never get the correct marginals, which was guaranteed by selection of unbelievable targets. Yet ensemble belief propagation gave excellent results. Using the exact parameter ensemble gave orders of magnitude improvement, limited by the\nnumber of beliefs being averaged. The gaussian parameter ensemble also did much better than even the best results of BP."
    }, {
      "heading" : "4 Discussion",
      "text" : "Other studies have also made use of the Bethe Hessian to draw conclusions about belief propagation. For instance, the Hessian reveals that the Ising model’s paramagnetic state becomes unstable in BP for large enough couplings [17]. For another example, when the Hessian is positive definite throughout pseudomarginal space, then the Bethe free energy is convex and thus BP has a unique fixed point [18]. Yet the stronger interpretation appears to be underappreciated: When the Hessian is not positive definite for some pseudomarginals, then BP can never have a fixed point there, for any parameters.\nOne might hope that by adjusting the parameters of belief propagation in some systematic way, θ → θBP, one could fix the approximation and so perform exact inference. In this paper we proved that this is a futile hope, because belief propagation simply can never converge to certain marginals. However, we also provided an algorithm that does work: Ensemble belief propagation uses BP on several different parameters with different fixed points and averages the results. This approach preserves the locality and scalability which make BP so popular, but corrects for some of its defects at the cost of running the algorithm a few times. Additionally, it raises the possibility that a systematic compensation for the flaws of BP might exist, but only as a mapping from individual parameters to an ensemble of parameters θ → {θeBP} that could be used in eBP. An especially clear application of eBP is to discriminative models like Conditional Random Fields [19]. These models are trained so that known inputs produce known inferences, and then generalize to draw novel inferences from novel inputs. When belief propagation is used during learning, then the model will fail even on known training examples if they happen to be unbelievable. Overall performance will suffer. Ensemble BP can remedy those training failures and thus allow better performance and more reliable generalization.\nThis paper addressed learning in fully-observed models only, where marginals for all variables were available during training. Yet unbelievable marginals exist for models with hidden variables as well. Ensemble BP should work as in the fully-observed case, but training will require inference over the hidden variables during both wake and sleep phases.\nOne important inference engine is the brain. When inference is hard, neural computations may resort to approximations, perhaps including belief propagation [20, 21, 22, 23, 24]. It would be undesirable for neural circuits to have big blind spots, i.e. reasonable inferences it cannot draw, yet that is precisely what occurs in BP. By averaging over models with eBP, this blind spot can be eliminated. In the brain, synaptic weights fluctuate due to a variety of mechanisms. Perhaps such fluctuations allow averaging over models and thereby reach conclusions unattainable by a deterministic mechanism."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors thank Greg Wayne for helpful conversations."
    } ],
    "references" : [ {
      "title" : "The computational complexity of probabilistic inference using bayesian belief networks",
      "author" : [ "G Cooper" ],
      "venue" : "Artificial intelligence",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1990
    }, {
      "title" : "Probabilistic reasoning in intelligent systems: networks of plausible inference",
      "author" : [ "J Pearl" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1988
    }, {
      "title" : "Factor graphs and the sum-product algorithm",
      "author" : [ "F Kschischang", "B Frey", "H Loeliger" ],
      "venue" : "IEEE Transactions on Information Theory",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "Pattern recognition and machine learning",
      "author" : [ "C Bishop" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "M Wainwright", "M Jordan" ],
      "venue" : "Foundations and Trends in Machine Learning",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Generalized belief propagation",
      "author" : [ "JS Yedidia", "WT Freeman", "Y Weiss" ],
      "venue" : "IN NIPS",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2000
    }, {
      "title" : "Stable fixed points of loopy belief propagation are minima of the Bethe free energy",
      "author" : [ "T Heskes" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Belief optimization for binary networks: A stable alternative to loopy belief propagation",
      "author" : [ "M Welling", "Y Teh" ],
      "venue" : "Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2001
    }, {
      "title" : "Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudo-moment matching",
      "author" : [ "MJ Wainwright", "TS Jaakkola", "AS Willsky" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2003
    }, {
      "title" : "Approximate inference in Boltzmann machines",
      "author" : [ "M Welling", "Y Teh" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "Learning in markov random fields: An empirical study",
      "author" : [ "S Parise", "M Welling" ],
      "venue" : "Joint Statistical Meeting",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "Loopy belief propagation, Bethe free energy and graph zeta function",
      "author" : [ "Y Watanabe", "K Fukumizu" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Analyzing cooperative computation",
      "author" : [ "G Hinton", "T Sejnowski" ],
      "venue" : "Proceedings of the Fifth Annual Cognitive Science",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1983
    }, {
      "title" : "Learning in markov random fields with contrastive free energies",
      "author" : [ "M Welling", "C Sutton" ],
      "venue" : "In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "Constructing free-energy approximations and generalized belief propagation algorithms",
      "author" : [ "J Yedidia", "W Freeman", "Y Weiss" ],
      "venue" : "IEEE Transactions on Information Theory",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2005
    }, {
      "title" : "On the properties of the Bethe approximation and loopy belief propagation on binary networks. Journal of Statistical Mechanics: Theory and Experiment : P11012",
      "author" : [ "J Mooij", "H Kappen" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2005
    }, {
      "title" : "Validity estimates for loopy belief propagation on binary real-world networks",
      "author" : [ "J Mooij", "H Kappen" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2005
    }, {
      "title" : "On the uniqueness of loopy belief propagation fixed points",
      "author" : [ "T Heskes" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J Lafferty", "A McCallum", "F Pereira" ],
      "venue" : "Proceedings of the 18th International Conference on Machine",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2001
    }, {
      "title" : "Cortical circuitry implementing graphical models",
      "author" : [ "S Litvak", "S Ullman" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "Belief propagation in networks of spiking neurons",
      "author" : [ "A Steimer", "W Maass", "R Douglas" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "The neurodynamics of belief propagation on binary markov random fields",
      "author" : [ "T Ott", "R Stoop" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Implementing belief propagation in neural circuits",
      "author" : [ "A Shon", "R Rao" ],
      "venue" : "Neurocomputing 65–66:",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "Towards a mathematical theory of cortical micro-circuits. PLoS computational biology",
      "author" : [ "D George", "J Hawkins" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Calculating marginal probabilities for a graphical model generally requires summing over exponentially many states, and is NP-hard in general [1].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "One popular technique is belief propagation (BP), in particular the sumproduct rule, which is a message-passing algorithm for performing inference on a graphical model [2].",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 2,
      "context" : "Here, α indexes sets of interacting variables (factors in the factor graph [3]), and xα is a subset of variables whose interaction is characterized by a vector of sufficient statistics φα(xα) and corresponding natural parameters θα.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "The sum-product algorithm for belief propagation on a graphical model with energy function (2) uses the following equations [4]: mi→α(xi) ∝ ∏",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "While they are guaranteed to be locally consistent, ∑ xα\\xi bα(xα) = bi(xi), they are not necessarily globally consistent: There may not exist a single joint distribution B(x) of which the beliefs are the marginals [5].",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 5,
      "context" : "Some theoretical justification for loopy belief propagation emerged with proofs that its stable fixed points are local minima of the Bethe free energy [6, 7].",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "Some theoretical justification for loopy belief propagation emerged with proofs that its stable fixed points are local minima of the Bethe free energy [6, 7].",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "in which the average energy U is exact, but the true entropy S is replaced by an approximation, the Bethe entropy S , which is a sum over the factor and node entropies [6]: S [Q] = ∑",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "Nonetheless, the Bethe free energy is often close enough to the Gibbs free energy that its minima approximate the true marginals [8].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 5,
      "context" : "Since stable fixed points of BP are minima of the Bethe free energy [6, 7], this helped explain why belief propagation is often so successful.",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "Since stable fixed points of BP are minima of the Bethe free energy [6, 7], this helped explain why belief propagation is often so successful.",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "Pseudomarginal space is the convex set [5] of all q that satisfy the positivity and local consistency constraints,",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : "Since the fixed points of BP are stationary points of F β [6], one may simply try to find parameters θ that produce a stationary point in pseudomarginal space at p, which is a necessary condition for BP to reach a fixed point there.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "Note that in principle this gradient could be used to directly minimize the Bethe free energy, but F β [q] is a complicated function of q that usually cannot be minimized analytically [8].",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 8,
      "context" : "This approach to learning parameters has been described as ‘pseudo-moment matching’ [9, 10, 11].",
      "startOffset" : 84,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "This approach to learning parameters has been described as ‘pseudo-moment matching’ [9, 10, 11].",
      "startOffset" : 84,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "This approach to learning parameters has been described as ‘pseudo-moment matching’ [9, 10, 11].",
      "startOffset" : 84,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "One example is the expectation parameters ηα = ∑ xα qα(xα)φα(xα) [5], giving the energy simply as U = −θ · η.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "This is surprising in view of claims to the contrary: [9, 5] state that belief propagation run after pseudo-moment matching can always reach a fixed point that reproduces the target marginals.",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "This is surprising in view of claims to the contrary: [9, 5] state that belief propagation run after pseudo-moment matching can always reach a fixed point that reproduces the target marginals.",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "For belief propagation to converge to the target — namely, the marginals p — a zero gradient is not sufficient: The Bethe free energy must also be a local minimum [7].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "Stable fixed points of BP occur only at local minima of the Bethe free energy [7], and so BP cannot reproduce the marginals p for any parameters.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "Graphical models with multinomial or gaussian variables and at least two loops always have some pseudomarginals for which the Hessian is not positive definite [12].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 11,
      "context" : "On the other hand, all marginals with sufficiently small correlations are believable because they are guaranteed to have a positive-definite Bethe Hessian [12].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 12,
      "context" : "When pseudo-moment matching fails to reproduce unbelievable marginals, an alternative is to use a gradient descent procedure for learning, analagous to the wake-sleep algorithm used to train Boltzmann machines [13].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 13,
      "context" : "Here we use a new cost function, the ‘Bethe divergence’ Dβ [p||b], by replacing these free energies by Bethe free energies [14] evaluated at the true marginals p and at the beliefs b obtained from BP fixed points, Dβ [p||b] = F β [p]− F β [b] (18) We use gradient descent to optimize this cost, with gradient dDβ dθ = ∂Dβ ∂θ + ∂Dβ ∂b ∂b ∂θ (19)",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "We parameterize pseudomarginals as {q i , q ij }where q i = qi(xi = +1) and q ij = qij(xi = xj = +1) [8].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 14,
      "context" : "If all the interactions are finite, then the inequality constraints are not active [15].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 15,
      "context" : "Each run of BP used exponential temporal message damping of 5 time steps [16], m = am + (1 − a)mundamped with a = e−1/5.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "For instance, the Hessian reveals that the Ising model’s paramagnetic state becomes unstable in BP for large enough couplings [17].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 17,
      "context" : "For another example, when the Hessian is positive definite throughout pseudomarginal space, then the Bethe free energy is convex and thus BP has a unique fixed point [18].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 18,
      "context" : "An especially clear application of eBP is to discriminative models like Conditional Random Fields [19].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "When inference is hard, neural computations may resort to approximations, perhaps including belief propagation [20, 21, 22, 23, 24].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "When inference is hard, neural computations may resort to approximations, perhaps including belief propagation [20, 21, 22, 23, 24].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 21,
      "context" : "When inference is hard, neural computations may resort to approximations, perhaps including belief propagation [20, 21, 22, 23, 24].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 22,
      "context" : "When inference is hard, neural computations may resort to approximations, perhaps including belief propagation [20, 21, 22, 23, 24].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 23,
      "context" : "When inference is hard, neural computations may resort to approximations, perhaps including belief propagation [20, 21, 22, 23, 24].",
      "startOffset" : 111,
      "endOffset" : 131
    } ],
    "year" : 2011,
    "abstractText" : "Loopy belief propagation performs approximate inference on graphical models with loops. One might hope to compensate for the approximation by adjusting model parameters. Learning algorithms for this purpose have been explored previously, and the claim has been made that every set of locally consistent marginals can arise from belief propagation run on a graphical model. On the contrary, here we show that many probability distributions have marginals that cannot be reached by belief propagation using any set of model parameters or any learning algorithm. We call such marginals ‘unbelievable.’ This problem occurs whenever the Hessian of the Bethe free energy is not positive-definite at the target marginals. All learning algorithms for belief propagation necessarily fail in these cases, producing beliefs or sets of beliefs that may even be worse than the pre-learning approximation. We then show that averaging inaccurate beliefs, each obtained from belief propagation using model parameters perturbed about some learned mean values, can achieve the unbelievable marginals.",
    "creator" : "LaTeX with hyperref package"
  }
}