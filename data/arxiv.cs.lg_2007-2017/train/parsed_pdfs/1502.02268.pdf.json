{
  "name" : "1502.02268.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization",
    "authors" : [ "Zheng Qu", "Peter Richtárik" ],
    "emails" : [ "ZHENG.QU@ED.AC.UK", "PETER.RICHTARIK@ED.AC.UK", "TAKAC.MT@GMAIL.COM", "OLIVIER.FERCOQ@TELECOM-PARISTECH.FR" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Empirical risk minimization (ERM) is a fundamental paradigm in the theory and practice of statistical inference and machine learning (Shalev-Shwartz & Ben-David, 2014). In the “big data” era it is increasingly common in practice to solve ERM problems with a massive number of examples, which leads to new algorithmic challenges.\nState-of-the-art optimization methods for ERM include i) stochastic (sub)gradient descent (Shalev-Shwartz et al.,\n2011; Takáč et al., 2013), ii) methods based on stochastic estimates of the gradient with diminishing variance such as SAG (Schmidt et al., 2013), SVRG (Johnson & Zhang, 2013), S2GD (Konečný & Richtárik, 2014), proxSVRG (Xiao & Zhang, 2014), MISO (Mairal, 2014), SAGA (Defazio et al., 2014), minibatch S2GD (Konečný et al., 2014a), S2CD (Konečný et al., 2014b), and iii) variants of stochastic dual coordinate ascent (Shalev-Shwartz & Zhang, 2013d; Zhao & Zhang, 2014; Takáč et al., 2013; Shalev-Shwartz & Zhang, 2013b;a; Lin et al., 2014; Qu et al., 2014; Shalev-Shwartz & Zhang, 2013c).\nThere have been several attempts at designing methods that combine randomization with the use of curvature (secondorder) information. For example, methods based on running coordinate ascent in the dual such as those mentioned above and also (Richtárik & Takáč, 2014; 2012; Fercoq & Richtárik, 2013b; Tappenden et al., 2014; Richtárik & Takáč, 2013a;b; Fercoq & Richtárik, 2013a; Fercoq et al., 2014; Qu et al., 2014; Qu & Richtárik, 2014a) use curvature information contained in the diagonal of a bound on the Hessian matrix. Block coordinate descent methods, when equipped with suitable data-dependent norms for the blocks, use information contained in the block diagonal of the Hessian (Tappenden et al., 2013).\nA more direct route to incorporating curvature information was taken by Schraudolph et al. (2007) in their stochastic L-BFGS method, by Byrd et al. (2014) and Sohl-Dickstein et al. (2014) in their stochastic quasi-Newton methods and by Fountoulakis & Tappenden (2014) who proposed a stochastic block coordinate descent methods. While typically efficient in practice, none of the methods mentioned above are equipped with complexity bounds (bounds on the number of iterations). An exception in this regard is the work of Bordes et al. (2009), who give a O(1/ ) complex-\nar X\niv :1\n50 2.\n02 26\n8v 1\n[ cs\n.L G\n] 8\nity bound for a Quasi-Newton SGD method."
    }, {
      "heading" : "1.1. Contributions",
      "text" : "The main contribution of this paper is the design and analysis of a new algorithm—stochastic dual Newton ascent (SDNA)—for solving a regularized ERM problem with smooth loss functions and a strongly convex regularizer (primal problem). Our method is stochastic in nature and has the capacity to utilize all curvature information inherent in the data. While we do our analysis for an arbitrary strongly convex regularizer, for the purposes of the introduction we shall describe the method in the case of the L2 regularizer. In this case, the dual problem is a concave quadratic maximization problem with a strongly concave separable penalty.\nSDNA in each iteration picks a random subset of the dual variables (which corresponds to picking a minibatch of examples in the primal problem), following an arbitrary probability law, and maximizes, exactly, the dual objective restricted to the random subspace spanned by the coordinates. Equivalently, this can be seen as the solution of a proximal subproblem involving a random principal submatrix of the Hessian of the quadratic function. Hence, SDNA utilizes all curvature information available in the random subspace in which it operates. Note that this is very different from the update strategy of parallel / minibatch coordinate descent methods. Indeed, while these methods also update a random subset of variables in each iteration, they instead only utilize curvature information present in the diagonal of the Hessian.\nAs we will explain in detail in the text, SDCA-like methods need more iterations (and hence more passes through data) to convergence as the minibatch size increases. However, SDNA enjoys the opposite behavior: with increasing minibatch size, SDNA needs fewer iterations (and hence fewer passes over data) to convergence. This observation can be deduced from the complexity results we prove for SDNA, and is also confirmed by our numerical experiments. In particular, we show that the expected duality gap decreases at a geometric rate which i) is better than that of SDCA-like methods such as SDCA (ShalevShwartz & Zhang, 2013d) and QUARTZ (Qu et al., 2014), and ii) improves with increasing minibatch size. This improvement does not come for free: as we increase the minibatch size, the subproblems grow in size as they involve larger portions of the Hessian. We find through experiments that for some, especially dense problems, even relatively small minibatch sizes lead to dramatic speedups in actual runtime.\nWe show that in the case of quadratic loss, and when viewed as a primal method, SDNA can be interpreted as a variant of the recently introduced Iterative Hessian\nSketch algorithm (Pilanci & Wainwright, 2014).\nEn route to developing SDNA which we describe in Section 4, we also develop several other new algorithms: two in Section 2 (where we focus on smooth problems), one in Section 3 (where we focus on composite problems). Besides SDNA, we also develop and analyze a novel minibatch variant of SDCA in Section 4, for the sake of finding suitable method to compare SDNA to. SDNA is equivalent to applying the new method developed in Section 3 to the dual of the ERM problem. However, as we are mainly interested in solving the ERM (primal) problem, we additionally prove that the expected duality gap decreases at a geometric rate. Our technique for doing this is a variant of the one use by Shalev-Shwartz & Zhang (2013d), but generalized to an arbitrary sampling."
    }, {
      "heading" : "1.2. Notation",
      "text" : "Vectors. By e1, . . . , en we denote the standard basis vectors in Rn. For any x ∈ Rn, we denote by xi the ith element of x, i.e., xi = e>i x. For any two vectors x, y of equal size, we write 〈x, y〉 = x>y = ∑ i xiyi, and by x ◦ y we denote their Hadamard (i.e., elementwise) product. We also write u−1 = (u−11 , . . . , u −1 n ).\nMatrices. I is the identity matrix in Rn×n and D(w) is the diagonal matrix in Rn×n with w ∈ Rn on its diagonal. We will write M 0 (resp. M 0) to indicate that M is positive semidefinite (resp. positive definite).\nSubsets of coordinates. Let S be a nonempty subset of [n] := {1, 2, . . . , n}. For any matrix M ∈ Rn×n we write MS for the matrix obtained from M by retaining elements Mij for which both i ∈ S and j ∈ S and zeroing out all other elements. Clearly, MS = ISMIS . Moreover, for any vector h ∈ Rn we write\nhS := ISh = ∑n i=1 hiei. (1)\nNote that we can thus write\n(hS) >MhS = h >ISMISh = h >MSh, (2)\nand that for x, y ∈ Rn we have\n〈xS , y〉 = 〈ISx, y〉 = 〈x, ISy〉 = 〈x, yS〉. (3)\nBy (MS)−1 we denote the matrix in Rn×n for which\n(MS) −1MS = MS(MS) −1 = IS . (4)"
    }, {
      "heading" : "2. Minimization of a Smooth Function",
      "text" : "In this section we consider unconstrained minimization of a differentiable convex function:\nmin x∈Rn f(x). (5)\nIn particular, we shall assume smoothness (Lipschitz continuity of the gradient) and strong convexity of f :\nAssumption 1 (Smoothness). There is a positive definite matrix M ∈ Rn×n such that for all x, h ∈ Rn,\nf(x+ h) ≤ f(x) + 〈∇f(x), h〉+ 1 2 〈Mh, h〉 (6)\nAssumption 2 (Strong convexity). There is a positive definite matrix G ∈ Rn×n such that for all x, h ∈ Rn,\nf(x) + 〈∇f(x), h〉+ 1 2 〈Gh, h〉 ≤ f(x+ h). (7)"
    }, {
      "heading" : "2.1. Three stochastic algorithms",
      "text" : "We now describe three algorithmic strategies for solving problem (5), the first two of which are new. All these methods have the form\nxk+1 ← xk + hk, (8)\nwhere hki is only allowed to be nonzero for i ∈ Sk, where {Sk}k≥0 are i.i.d. random subsets of [n] := {1, 2, . . . , n} (“samplings”). That is, all methods in each iteration update a random subset of the variables. The four methods will only differ in how the update elements hki for i ∈ Sk are computed. If we wish the methods to work, we necessarily need to require that every coordinate has a positive probability of being sampled. For certain technical reasons that will be apparent later, we will also assume that Sk is nonempty with probability 1.\nAssumption 3 (Samplings). The random sets {Sk}k≥0 are i.i.d., proper (i.e., Prob(i ∈ Sk) > 0 for all i ∈ [n]) and nonvacuous (i.e., Prob(Sk = ∅) = 0).\nMuch of our discussion will depend on the distribution of Sk rather than on k. As {Sk}k≥0 are i.i.d., we will write Ŝ for a sampling which shares their distribution. We will write p = (p1, . . . , pn) where\npi := Prob(i ∈ Ŝ), i ∈ [n]. (9)\nBy Assumption 3, we have pi > 0 for all i. We now describe the methods.\nMethod 1. We compute (MSk)−1 and set\nhk = −(MSk)−1∇f(xk). (Method 1)\nNote that the update only involves the inversion of a random principal submatrix of M of size |Sk| × |Sk|. Also, we only need to compute elements i ∈ Sk of the gradient ∇f(xk). If |Sk| is reasonably small, the update step is cheap.\nMethod 2. We compute the inverse of E[MŜ ] and set\nhk = −ISk(E[MŜ ]) −1D(p)∇f(xk). (Method 2)\nThis strategy easily implementable when |Ŝ| = 1 with probability 1 (i.e., if we update a single variable only). This is because then E[MŜ ] is a diagonal matrix with the (i, i) element equal to piMii. Hence, the update step simplifies to hki = − 1Mii 〈ei,∇f(x\nk)〉 for i ∈ Sk and hki = 0 for i /∈ Sk. For more complicated samplings Ŝ, however, the matrix E[MŜ ] will be as hard to invert as M.\nMethod 3. We compute a vector v ∈ Rn for which\nE[MŜ ] D(p)D(v) (10)\nand then set\nhk = −ISk(D(v))−1∇f(xk). (Method 3)\nAssuming v is easily computable (this should be done before the methods starts), the update is clearly very easy to perform. Indeed, the update can be equivalently written as hki = − 1vi 〈ei,∇f(x\nk)〉 for i ∈ Sk and hki = 0 for i /∈ Sk. Method 3 is known as NSync (Richtárik & Takáč, 2013b). For a calculus allowing the computation of closed form formulas for v as a function of the sampling Ŝ we refer the reader to (Qu & Richtárik, 2014b).\nNote that all three methods coincide if |Ŝ| = 1 with probability 1."
    }, {
      "heading" : "2.2. Three linear convergence rates",
      "text" : "We shall now show that, putting the issue of the cost of each iteration of the three methods aside, all enjoy a linear rate of convergence. In particular, we shall show that Method 1 has the fastest rate, followed by Method 2 and finally, Method 3.\nTheorem 1. Let Assumptions 1, 2 and 3 be satisfied. Let {xk}k≥0 be the sequence of random vectors produced by Method m, for m = 1, 2, 3 and let x∗ be the optimal solution of (5). Then\nE[f(xk+1)− f(x∗)] ≤ (1− σm)E[f(xk)− f(x∗)],\nwhere\nσ1 := λmin\n( G1/2 E [( MŜ )−1] G1/2 ) , (11)\nσ2 := λmin\n( G1/2D(p) ( E [ MŜ ])−1 D(p)G1/2 ) , (12)\nσ3 := λmin\n( G1/2D(p)D(v−1)G1/2 ) . (13)\nThe above result means that the number of iterations sufficient for Method m to obtain an -solution (in expectation) is O( 1σm log(1/ )).\nIn the above theorem (which we prove in Section 2.4), λmin(X) refers to the smallest eigenvalue of matrix X. It turns out that in all three cases, the matrix X involved is\npositive definite. However, for the matrices in (11) and (12) this will only be apparent if we show that E[MŜ ] 0 and E[(MŜ) −1] 0, which we shall do next.\nLemma 1. If Ŝ is a proper sampling, then E [ MŜ ] 0.\nProof. Denote supp{x} = {i ∈ [n] : xi 6= 0}. Since M 0, any principal submatrix of M is also positive definite. Hence for any x ∈ Rn\\{0}, x>MSx = 0 implies that supp{x} ∩ S = ∅ for all S ⊆ [n]. If x ∈ Rn is such that\nx> E [ MŜ ] x = ∑ S⊆[n] Prob(Ŝ = S)x >MSx = 0,\nthen Prob(supp{x}∩ Ŝ = ∅) = 1. Since Ŝ is proper, this only happens when x = 0. Therefore, E[MŜ ] 0.\nLemma 2. If Ŝ is proper and nonvacuous, then\n0 ≺ D(p) ( E [ MŜ ])−1 D(p) E [( MŜ )−1] . (14)\nProof. The first inequality follows from Lemma 1 and the fact for proper Ŝ we have p > 0 and hence D(p) 0. We now turn to the second inequality. Fix h ∈ Rn. For arbitrary ∅ 6= S ⊆ [n] and y ∈ Rn we have:\n1 2h > (MS) −1 h = 12h > S (MS) −1 hS\n= max x∈Rn 〈x, hS〉 − 12x >MSx ≥ 〈y, hS〉 − 12y >MSy.\nSubstituting S = Ŝ and taking expectations, we obtain\n1 2 E [ h> ( MŜ )−1 h ] ≥ E [ 〈y, hŜ〉 − 1 2y >MŜy ] = y>D(p)h− 12y > E [ MŜ ] y.\nTherefore, 12h > E [( MŜ )−1] h ≥ maxy∈Rn y>D(p)h − 1 2y > E [ MŜ ] y = 12h >D(p) ( E [ MŜ ])−1 D(p)h.\nWe now establish an important relationship between the quantities σ1, σ2 and σ3, which sheds light on the convergence rates of the three methods.\nTheorem 2. 0 < σ3 ≤ σ2 ≤ σ1 ≤ 1.\nProof. We have σm > 0 for all m since σm is the smallest eigenvalue of a positive definite matrix. That σm ≤ 1 follows as a direct corollary Theorem 1. Finally, D(p)D(v−1) = D(p)D(p−1)D(v−1)D(p) (10)\nD(p) ( E [ MŜ ])−1 D(p) (14) E [( MŜ )−1] ."
    }, {
      "heading" : "2.3. Example",
      "text" : "Consider the function f : R3 → R given by\nf(x) = 12x TMx, M = ( 1.0000 0.9900 0.9999 0.9900 1.0000 0.9900 0.9999 0.9900 1.0000 ) .\nNote that Assumption 1 holds, and Assumption 2 holds with G = M. Let Ŝ be the “2-nice sampling” on [n] = {1, 2, 3}. That is, we set Prob(Ŝ = {i, j}) = 13 . for (i, j) = (1, 2), (2, 3), (3, 1). A straightforward calculation reveals that:\nE [( MŜ )−1] ≈ ( 1683.50 −16.58 −1666.58−16.58 33.50 −16.58 −1666.58 −16.58 1683.50 ) ,\nD(p) ( E [ MŜ ])−1 D(p) ≈ ( 0.9967 −0.3268 −0.3365 −0.3268 0.9902 −0.3268 −0.3365 −0.3268 0.9967 ) .\nIt can be verified that (10) holds with v = (2, 2, 2); see (Richtárik & Takáč, 2012) or (Qu & Richtárik, 2014b). Therefore, D(p)D(v−1) = 13I. Finally, we obtain:\nσ1 ≈ 0.3350, σ2 ≈ 1.333 · 10−4, σ2 ≈ 0.333 · 10−4.\nNote that: theoretical rate, σ1, of Method 1 is 10,000 times better than the rate, σ3, of parallel coordinate descent (Method 3)."
    }, {
      "heading" : "2.4. Proof of Theorem 1",
      "text" : "Proof. By minimizing both sides of (7) in h, we get:\nf(x)− f(x∗) ≤ 1 2 〈∇f(x),G−1∇f(x)〉. (15)\nIn view of (6) and (2), for for all h ∈ Rn we have:\nf(xk + ISkh) ≤ f(xk) + 〈∇f(xk), ISkh〉+ 1\n2 〈MSkh, h〉.\n(16) Method 1: If we use (16) with h ← hk := −(MSk)−1∇f(xk), and apply (4), we get:\nf(xk+1) ≤ f(xk)− 1 2 〈∇f(xk), (MSk)−1∇f(xk)〉.\nTaking expectations on both sides with respect to Sk yields:\nEk[f(xk+1)]\n≤ f(xk)− 1 2 〈∇f(xk),E[ ( MŜ )−1 ]∇f(xk)〉 (11) ≤ f(xk)− σ1\n2 〈∇f(xk),G−1∇f(xk)〉\n(15) ≤ f(xk)− σ1 ( f(xk)− f(x∗) ) ,\nwhere Ek denotes the expectation with respect to Sk. It remains to rearrange the inequality and take expectation.\nMethod 2: Let D = D(p). Taking expectations on both sides of (16) with respect to Sk, we see that for all h ∈ Rn the following holds: Ek[f(xk + ISkh)] ≤ f(xk) + 〈D∇f(xk), h〉 + 12 〈E[MSk ]h, h〉. Note that the choice h̃k := −(E[MŜ ])−1D∇f(xk) minimizes the right hand side of the inequality in h. Since hk = ISk h̃ k,\nEk[f(xk+1)]\n≤ f(xk)− 1 2 〈∇f(xk),D ( E[MŜ ] )−1 D∇f(xk)〉 (12) ≤ f(xk)− σ2\n2 〈∇f(xk),G−1∇f(xk)〉\n(15) ≤ f(xk)− σ2 ( f(xk)− f(x∗) ) .\nMethod 3: The proof is the same as that for Method 2, except in the first inequality we replace E[MSk ] by D(p)D(v) (see (10))."
    }, {
      "heading" : "3. Minimization of a Composite Function",
      "text" : "In this section we consider the following composite minimization problem:\nmin x∈Rn F (x) ≡ f(x) + n∑ i=1 ψi(xi). (17)\nWe assume that f satisfies Assumptions 6 and 7. The difference from the setup in the previous section is in the inclusion of the separable term ∑ i ψi. Assumption 4. For each i, ψi : R → R ∪ {+∞} is closed and γi-strongly convex for some γi ≥ 0. Let γ = (γ1, . . . , γn) ∈ Rn+.\nFor ease of presentation, in this section we only consider uniform sampling Ŝ, which means that Prob(i ∈ Ŝ) = Prob(j ∈ Ŝ) for all i, j ∈ [n]. In particular, this implies that Prob(i ∈ Ŝ) = E[|Ŝ|]n for all i. Let τ := E[Ŝ]."
    }, {
      "heading" : "3.1. New algorithm",
      "text" : "We now propose Algorithm 1, which a variant of Method 1 applicable to problem (17). If ψi ≡ 0 for all i, the methods coincide. The following result states that the method converges at a geometric rate, in expectation.\nTheorem 3. Let Assumptions 1, 2, 3 and 4 be satisfied. Then the output sequence {xk}k≥0 of Algorithm 1 satisfies:\nE[F (xk+1)− F (x∗)] ≤ (1− σprox1 )E[F (xk)− F (x∗)],\nwhere x∗ is the solution of (17), σprox1 := τ min(1,s1) n and\ns1 := λmin [(n τ E[MŜ ] + D(γ) )−1 (D(γ) + G) ] .\nAlgorithm 1 Proximal version of Method 1 1: Parameters: uniform sampling Ŝ 2: Initialization: choose initial point x0 ∈ Rn 3: for k = 0, 1, 2, . . . do 4: Generate a random set of blocks Sk ∼ Ŝ 5: Compute: hk = arg minh∈Rn〈∇f(xk), hSk〉 +\n1 2 〈h,MSkh〉+ ∑ i∈Sk ψi(x k i + hi)\n6: Update: xk+1 := xk + hkSk 7: end for\nNote for positive definite matrices X,Y, we have λmin(X −1Y) = λmin(Y 1/2X−1Y1/2). It is this latter form we have used in the formulation of Theorem 1. In the special case when γ ≡ 0 (ψi are merely convex), we have σprox1 = min{ τn , τ2 n2λmin(G 1/2(E[MŜ ])\n−1G1/2)}. Note that while this rate applies to a proximal/composite variant of Method 1, its rate is best compared to the rate σ2 of Method 2. Indeed, looking at (12), and realizing that for uniform Ŝ we have D(p) = τnI, we get\nσ1 ≥ σ2 = τ 2 n2λmin(G 1/2(E[MŜ ]) −1G1/2) ≥ σprox1 .\nSo, the rate we can prove for the composite version of Method 1 (σprox1 ) is weaker than the rate we get for Method 2 (σ2), which by Theorem 2 is weaker than the rate of Method 1 (σ1). We believe this is a byproduct of our analysis rather than the weakness of Algorithm 1."
    }, {
      "heading" : "3.2. PCDM",
      "text" : "We will now compare our new Algorithm 1 with the Parallel Coordinate Descent Method (PCDM) of Richtárik & Takáč (2012), which can also be applied to problem (17).\nAlgorithm 2 PCDM (Richtárik & Takáč, 2012) 1: Parameters: uniform sampling Ŝ; v ∈ Rn++ 2: Initialization: choose initial point x0 ∈ Rn 3: for k = 0, 1, 2, . . . do 4: Generate a random set of blocks Sk ∼ Ŝ 5: Compute for i ∈ Sk\nhki = arg min hi∈R e>i ∇f(xk)hi+ vi 2 |hi|2+ψi(xki +hi)\n6: Update: xk+1 := xk + ∑ i∈Sk h k i ei 7: end for\nProposition 1. Let the same assumptions as those in Theorem 3 be satisfied. Moreover, assume v ∈ Rn++ is a vector satisfying (10). Then the output sequence {xk}k≥0 of Algorithm 2 satisfies\nE[F (xk+1)− F (x∗)] ≤ (1− σprox3 )E[F (xk)− F (x∗)],\nwhere σprox3 := τ min(1,s3) n and\ns3 := λmin\n[ (D(v + γ)) −1 (D(γ) + G) ] .\nProof. Sketch: The proof is a minor modification of the arguments in (Richtárik & Takáč, 2012)."
    }, {
      "heading" : "3.3. Comparison of the rates of Algorithms 1 and 2",
      "text" : "We now show that the rate of linear (geometric) convergence of our method is better than that of PCDM.\nProposition 2. σprox1 ≥ σ prox 3 .\nProof. Since pi = τn for all i, we have D(p) = τ nI and hence from (10) we deduce that:\nn τ E[MŜ ] + D(γ) (10) D(v) + D(γ) = D(v + γ),\nwhence s1 ≥ s3, and the claim follows."
    }, {
      "heading" : "4. Empirical Risk Minimization",
      "text" : "We now turn our attention to the empirical risk minimization problem:\nmin w∈Rd\nP (w) := 1n n∑ i=1 φi(a > i w) + λg(w). (18)\nWe assume that g : Rd → R is a 1-strongly convex function with respect to the L2 norm and each loss function φi : R → R is convex and 1/γ-smooth. Each ai is a ddimensional vector and for ease of presentation we write A = (a1, . . . , an) = ∑n i=1 aie > i . Let g\n∗ and {φ∗i }i be the Fenchel conjugate functions of g and {φi}i, respectively. In the case of g, for instance, we have g∗(s) = supw∈Rd〈w, s〉−g(w). The (Fenchel) dual problem of (18) can be written as:\nmax α∈Rn\nD(α) := 1n n∑ i=1 −φ∗i (−αi)− λg∗ ( 1 λnAα ) . (19)"
    }, {
      "heading" : "4.1. SDNA: A new algorithm for ERM",
      "text" : "Note that the dual problem has the form (17)\nmin α∈Rn F (α) ≡ f(α) + n∑ i=1 ψi(αi), (20)\nwhere F (α) = −D(α), f(α) = λg∗( 1λnAα) and ψ(αi) = 1 nφ ∗ i (−αi). It is easy to see that f satisfies Assumption 1 with M := 1nX, where X := 1 λnA\n>A. Moreover, ψi is γ n -strongly convex. We can therefore apply Algorithm 1 to solve the dual (20). This is what Algorithm 3 does.\nIf α∗ is the optimal solution of (19), then the optimal solution of (18) is given by:\nw∗ = ∇g∗ (\n1 λnAα\n∗) . (21)\nAlgorithm 3 Stochastic Dual Newton Ascent (SDNA) 1: Parameters: proper nonvacuous sampling Ŝ 2: Initialization: α0 ∈ Rn; ᾱ0 = 1λnAα 0\n3: for k = 0, 1, 2, . . . do 4: Primal update: wk = ∇g∗(ᾱk) 5: Generate a random set of blocks Sk ∼ Ŝ 6: Compute:\n∆αk = arg min h∈Rn 〈(A>wk)Sk , h〉+ 12h >XSkh + ∑ i∈Sk φ ∗ i (−αki − hi)\n7: Dual update: αk+1 := αk + (∆αk)Sk 8: Average update: ᾱk+1 = ᾱk + 1λn ∑ i∈Sk ∆α k i ai 9: end for\nWith each proper sampling Ŝ we associate the number:\nθ(Ŝ) := min i\npiλγn\nvi + λγn , (22)\nwhere (p1, . . . , pn) is the vector of probabilities defined in (9) and v = (v1, . . . , vn) ∈ Rn++ is a vector satisfying:\nE[(A>A)Ŝ ] D(p)D(v). (23)\nClosed-form expressions for v satisfying this inequality, as a function of the sampling Ŝ chosen, can be found in (Qu & Richtárik, 2014b). A rather conservative choice which works for any Ŝ, irrespective of its distribution, is vi = min{τ, λ′(A>A)}‖ai‖2, where λ′(Y) := maxh{h>Yh : h>D(Y)h ≤ 1} and τ is a number for which |Ŝ| ≤ τ with probability 1 (see Theorem 5.1 in the aforementioned reference). Better bounds (with smaller v) can be derived for special classes of samplings.\nNow we can state the main result of this section: Theorem 4 (Complexity of SDNA). Let Ŝ be a uniform sampling and let τ := E[|Ŝ|]. The output sequence {wk, αk}k≥0 of Algorithm 3 satisfies:\nE[P (wk)−D(αk)] ≤ (1− σ prox 1 ) k\nθ(Ŝ) (D(α∗)−D(α0)),\nwhere σprox1 := τ min(1,s1) n and\ns1 = λmin\n[( 1\nτγλ E[(A>A)Ŝ ] + I\n)−1] . (24)\nIn the case of quadratic losses and quadratic regularizer, we can sharpen the complexity bound: Theorem 5. When both φi and g are quadratic functions, the output sequence {wk, αk}k≥0 of Algorithm 3 satisfies:\nE[P (wk)−D(αk)] ≤ (1− σ1) k\nθ(Ŝ)\n( D(α∗)−D(α0) ) where\nσ1 := λmin\n[ E [((\n1 λnA >A + γI ) Ŝ )−1 ( 1 λnA >A + γI )]] ."
    }, {
      "heading" : "4.2. Complexity analysis",
      "text" : "We first establish that SDNA is able to solve the dual.\nLemma 3. Let Ŝ be a uniform sampling and τ := E[|Ŝ|]. The output sequence {αk}k≥0 of Algorithm 3 satisfies:\nE[D(α∗)−D(αk)] ≤ (1− σprox1 ) k (D(α∗)−D(α0)),\nwhere σprox1 is as in Theorem 4.\nProof. If Ŝ is uniform, then the output of Algorithm 3 is equivalent to the output of Algorithm 1 applied to (20). Therefore, the result is obtained by applying Theorem 3 with M = 1λn2A >A, G = 0 and γi = γn for all i.\nWe now prove a sharper result in the case of quadratic loss and quadratic regularizer.\nLemma 4. If {φi}i and g are quadratic, then the output sequence {αk}k≥0 of Algorithm 3 satisfies:\nE[D(α∗)−D(αk)] ≤ (1− σ1)k(D(α∗)−D(α0)),\nwhere σ1 is as in Theorem 5.\nProof. If {φi}i and g are all quadratic functions, then the dual objective function is quadratic with Hessian matrix given by ∇2D(α) ≡ 1λn2A\n>A + γnI. It suffices to apply Theorem 1(11), with M = G = ∇2D(α).\nWe now prove a more general version of a classical result in dual coordinate ascent methods which bounds the duality gap from above by the expected dual increase.\nLemma 5. The output sequence {wk, αk}k≥0 of Algorithm 3 satisfies:\nEk[D(αk+1)−D(αk)] ≥ θ(Ŝ)(P (wk)−D(αk)).\nThe proof of the this lemma is provided in the supplementary material. Theorem 4 (resp. Theorem 5) now follows by combining Lemma 3 (resp. Lemma 4) and Lemma 5."
    }, {
      "heading" : "4.3. New Algorithm: SDCA with Arbitrary Sampling",
      "text" : "When |Ŝ| = 1 with probability 1, SDNA reduces to a proximal variant of stochastic dual coordinate ascent (SDCA) (Shalev-Shwartz & Zhang, 2013d). However, a minibatch version of standard SDCA in the ERM setup we consider here has not been previously studied in the literature. Takáč et al. (2013) developed such a method but in the special case of hinge-loss (which is not smooth and hence does not fit our setup). Shalev-Shwartz & Zhang (2013b) studied minibatching but in conjunction with acceleration and the QUARTZ method of Qu et al. (2014), which has been analyzed for an arbitrary sampling Ŝ, uses\na different primal update than SDNA. Hence, in order to compare SDNA with an SDCA-like method which is as close a match to SDNA as possible, we need to develop a new method. Algorithm 4 is an extension of SDCA to allow it handle an arbitrary uniform sampling Ŝ.\nThe complexity of Minibatch SDCA (we henceforth just write SDCA) is given in Theorem 6. Theorem 6. If (23) holds, then the output sequence {wk, αk}k≥0 of Algorithm 4 satisfies:\nE[P (wk)−D(αk)] ≤ (1− θ(Ŝ)) k\nθ(Ŝ)\n( D(α∗)−D(α0) ) .\nAlgorithm 4 Minibatch SDCA 1: Parameters: uniform sampling Ŝ, vector v ∈ Rn++ 2: Initialization: α0 ∈ Rn; set ᾱ0 = 1λnAα 0\n3: for k = 0, 1, 2, . . . do 4: Primal update: wk = ∇g∗(ᾱk) 5: Generate a random set of blocks Sk ∼ Ŝ 6: Compute for each i ∈ Sk\nhki = arg min hi∈R hi(a > i w k) + vi 2 |hi|2 +φ∗i (−αki −hi)\n7: Dual update: αk+1 := αk + ∑ i∈Sk h k i ei\n8: Average update: ᾱk+1 = ᾱk + 1λn ∑ i∈Sk h k i ai 9: end for"
    }, {
      "heading" : "4.4. SDNA vs SDCA",
      "text" : "We now compare the rates of SDNA and SDCA. The next result says that the rate of SDNA is always superior to that of SDCA. We also see that the rate is better in the quadratic case covered by Theorem 5. Theorem 7. If Ŝ is uniform sampling with τ = E[|Ŝ|], then\nθ(Ŝ) ≤ σprox1 ≤ σ1.\nProof. Since Ŝ is a uniform sampling, we have pi = τn for all i ∈ [n]. In view of (22), we have 1 ≤ nτ θ(Ŝ). Next,\ns1 (24)+(23) ≥ λmin\n( 1\nτλγ D(v)D(p) + I\n)−1 (22) = n\nτ θ(Ŝ).\nTherefore, σprox1 = τ n min(1, s1) ≥ θ(Ŝ). In order to establish σprox1 ≤ σ1, we use Lemma 2 and the fact that E[IŜ ] = τ nI to obtain\nτ n ( 1 τγλ E[(A >A)Ŝ ] + I )−1 = τ 2 n2 ( E [( 1 γλnA >A + I ) Ŝ ])−1 (Lemma 2) E [(( 1 γλnA >A + I ) Ŝ\n)−1] E [(( A>A + γλnI ) Ŝ )−1 (A>A + γλnI) ] ,\nThe rest of the argument is similar."
    }, {
      "heading" : "5. SDNA as Iterative Hessian Sketch",
      "text" : "We now apply SDNA to the least squares problem:\nmin w∈Rd\n1 2n ‖A>w − b‖2 + λ 2 ‖w‖2, (25)\nand show that the resulting primal update can be interpreted as an iterative Hessian sketch, alternative to the one proposed by Pilanci & Wainwright (2014). We first need to establish a simple duality result.\nLemma 6. Let α∗ be the optimal solution of\nmin α∈Rn\n1 2n ‖α‖2 − 1 n 〈b, α〉+ 1 2λn2 ‖Aα‖2, (26)\nthen the optimal solution w∗ of (25) is w∗ = 1λnAα ∗.\nProof. Problem (25) is a special case of (18) for g(w) ≡ 1 2‖w‖ 2 and φi(a) ≡ 12 (a−bi) 2 for all i ∈ [n]. Problem (26) is the dual of (25) and the result follows from (21).\nThe interpretation of SDNA as a variant of the Iterative Hessian sketch method of Pilanci & Wainwright (2014) follows immediately from the following theorem.\nTheorem 8. The output sequence {wk, αk}k≥0 of Algorithm 3 applied on problem (25) satisfies:\nwk+1 = arg min w∈Rd { 1 2n ‖S>k (A>w − b)‖2 + λ 2 ‖w‖2\n+ 〈w, 1 n AISkα k − λwk〉}, (27)\nwhere Sk denotes the n-by-|Sk| submatrix of the identity matrix In with columns in the random subset Sk.\nProof. We know that S>k ∆α k is the optimal solution of\nmin h∈Rτ\n1 2 ‖h‖2 + 〈S>k (A>wk +αk− b), h〉+ 1 2λn ‖ASkh‖2\nLet τ = |Sk|. By Lemma 6, the optimal solution of\nmin w∈Rd\n1\n2|Sk| ‖S>kA>w+S>k (A>wk+αk−b)‖2+\nλn\n2|Sk| ‖w‖2,\nis given by 1λnASkS > k∇αk, which equals ᾱk+1 − ᾱk and thus equals wk+1 − wk. Hence,\nwk+1 = arg min w∈Rd { 12n‖S > k (A >w+αk−b)‖2+λ2 ‖w−w k‖2},\nwhich is equivalent to (27) since (In)Sk = SkS > k ."
    }, {
      "heading" : "6. Numerical Experiments",
      "text" : "In our first experiment (Figure 1) we compare SDNA and our new minibatch version of SDCA on one real (mushrooms; d = 112, n = 8, 124) and one synthetic (d = 1, 024, n = 2, 048) dataset. In both cases, we used λ = 1/n as the regularization parameter and g(w) = 1 2‖w‖\n2. As τ increases, SDNA requires less passes over data (epochs), while SDCA requires more passes over data. It can be shown that this behavior can be predicted from the complexity results for these two methods. The difference in performance depends on the choice of the dataset and can be quite dramatic.\nIn the second experiment (Figure 2), we investigate how much time it takes for the methods to process a single epoch, using the same datasets as before. As τ increases, SDNA does more work as the subproblems it needs to solve in each iteration involve a τ × τ submatrix of the Hessian of the smooth part of the dual objective function. On the other hand, the work SDCA needs to do is much smaller, and does nearly does not increase with the minibatch size τ . This is because the subproblems are separable. As before, all experiments are done using a single core (however, both methods would benefit from a parallel implementation).\nFinally, in Figure 3 we put the insights gained from the previous two experiments together: we look at the perfor-\nmance of SDNA for various choices of τ by comparing runtime and duality gap error. We should expect that increasing τ would lead to faster method in terms of passes over data, but that this would also lead to slower iterations. The question is, is does the gain outweight the loss? The answer is: yes, for small enough minibatch sizes. Indeed, looking at Figure 3, we see that the runtime of SDNA improved up to the point τ = 16 for both datasets, and then starts to deteriorate. In situations where it is costly to fetch data from memory to a (fast) processor, much larger minibatch sizes would be optimal."
    }, {
      "heading" : "APPENDIX: Proof of Theorem 3",
      "text" : "It follows directly from Assumption 1 and the update rule xk+1 = xk + (hk)Sk in Algorithm 1 that:\nLHS := f(xk+1) + n∑ i=1 ψi(x k+1 i )− f(x k)− ∑ i/∈Sk ψi(x k i )\n≤ 〈∇f(xk), (hk)Sk〉+ 1\n2 〈hk,XSkhk〉+ ∑ i∈Sk ψi(x k i + h k i ).\nSince hk is defined as the minimizer of the right hand side in the last inequality, we can further bound this term by replacing hk with h = λ(x∗ − xk) for arbitrary λ ∈ [0, 1]:\nLHS ≤ λ〈(∇f(xk))Sk , x∗ − xk〉+ ∑ i∈Sk ψi(x k i + λ(x ∗ i − xki )) + λ2 2 〈x∗ − xk,XSk(x∗ − xk)〉. (28)\nNow we use the fact that ψi is γi-strongly convex to obtain:\nF (xk+1)− F (xk) = f(xk+1) + n∑ i=1 ψi(x k+1 i )− f(x k)− n∑ i=1 ψi(x k i )\n(28) ≤ λ〈(∇f(xk))Sk , x∗ − xk〉+ λ ∑ i∈Sk [ψi(x ∗ i )− ψi(xki )]\n−λ(1− λ) 2 〈x∗ − xk,D(γ)Sk(x∗ − xk)〉+ λ2 2 〈x∗ − xk,XSk(x∗ − xk)〉.\nBy taking expectations in Sk on both sides of the last inequality, we see that for any λ ∈ [0, 1], the following holds:\nEk[F (xk+1)− F (xk)] ≤ λτ\nn\n( 〈(∇f(xk)), x∗ − xk〉+\nn∑ i=1 ( ψi(x ∗ i )− ψi(xki )\n))\n−λ(1− λ) 2 〈x∗ − xk,E [ D(γ)Ŝ ] (x∗ − xk)〉+ λ 2 2 〈x∗ − xk,E [ XŜ ] (x∗ − xk)〉\n≤ λτ n\n( F (x∗)− F (xk)− 1\n2 〈x∗ − xk,G(x∗ − xk)〉 ) + λ2\n2 〈x∗ − xk,E\n[ XŜ + D(γ)Ŝ ] (x∗ − xk)〉 − λ\n2 〈x∗ − xk,E\n[ D(γ)Ŝ ] (x∗ − xk)〉\n≤ λτ n\n( F (x∗)− F (xk) ) − λ\n2 〈x∗ − xk, τ n (D(γ) + G)(x∗ − xk)〉\n+ λ2\n2 〈x∗ − xk,E\n[ XŜ + τ\nn D(γ)\n] (x∗ − xk)〉,\nwhere the second to last inequality follows from Assumption 7 and in the last one we used the fact that E[D(γ)Ŝ ] = τ nD(γ). It remains to replace λ by min(1, s)."
    }, {
      "heading" : "APPENDIX: Proof of Lemma 5",
      "text" : "Recall that M = 1nX, where X = 1 λnA >A.\nFor simplicity in this proof we write θ = θ(Ŝ). First, by the 1-strong convexity of the function g we obtain the 1-smoothness of the function g∗, from which we deduce:\n−λg∗(ᾱk+1) + λg∗(ᾱk) + λ〈∇g∗(ᾱk), ᾱk+1 − ᾱk〉 ≥ −λ 2 〈ᾱk+1 − ᾱk, ᾱk+1 − ᾱk〉.\nNow we replace ∇g∗(ᾱk) by wk and ᾱ by 1λnAα to obtain:\nD(αk+1)−D(αk) ≥ 1 n ∑ i∈Sk [ −φ∗i (−αk+1i ) + φ ∗ i (−αki ) ] − 1 n 〈A>wk, αk+1 − αk〉 − 1 2λn2 (αk+1 − αk)>A>A(αk+1 − αk)\n= max h∈Rn\n{ 1\nn ∑ i∈Sk [ −φ∗i (−αki − hi) + φ∗i (−αki ) ] − 1 n 〈(A>wk)Sk , h〉 − 1 2n h>XSkh\n} ,\nwhere in the last equality we used the dual update rules in Algorithm 3, as well as relations (3) and (2). Therefore, for arbitrary h ∈ Rn,\nEk[D(αk+1)−D(αk)] ≥ Ek\n[ 1\nn ∑ i∈Sk [ −φ∗i (−αki − hi) + φ∗i (−αki )\n]] − Ek [ 1\nn 〈(A>wk)Sk , h〉 −\n1\n2n h>XSkh\n]\n= 1\nn n∑ i=1 pi [ −φ∗i (−αki − hi) + φ∗i (−αki )− (a>i wk)hi ] − 1 2n h> E[XŜ ]h.\nLet uk ∈ Rn such that uki = ∇φi(a>i wk) ∈ R for all i ∈ [n]. Let s = (s1, . . . , sn) ∈ [0, 1]n with si = θp −1 i for all i ∈ [n], where θ is given in (22). By using hi = −si(αki + uki ) for all i ∈ [n] in (29), we get:\nEk[D(αk+1)−D(αk)] ≥ 1\nn n∑ i=1 pi[−φ∗i ( −(1− si)αki + siuki ) + φ∗i (−αki ) + si〈a>i wk, αki + uki 〉]\n− 1 2n (αk + uk)>D(s)E[XŜ ]D(s)(α k + uk)\nFrom γ-strong convexity of the functions φ∗i we deduce that:\n−φ∗i ((1− si)(−αki ) + siuki ) + φ∗i (−αki ) ≥ siφ∗i (−αki )− siφ∗i (uki ) + γsi(1− si)\n2 |uki + αki |2.\nConsequently,\nEk[D(αk+1)−D(αk)] ≥ 1\nn n∑ i=1 pisi [ φ∗i (−αki )− φ∗i (uki ) + 〈a>i wk, αki + uki 〉 ] + 1 n n∑ i=1 γpisi(1− si) 2 |uki + αki |2\n− 1 2n (αk + uk)>D(s)E[XŜ ]D(s)(α k + uk)\n= θ\nn n∑ i=1 [ φ∗i (−αki ) + φi(a>i wk) + 〈a>i wk, αki 〉 ] + γθ 2n 〈αk + uk, (I−D(s))(αk + uk)〉\n− 1 2n 〈αk + uk,D(s)E[XŜ ]D(s)(α k + uk)〉\nwhere the equality follows from uki = ∇φi(a>i wk). Next, by the definition of θ in (22), we know that:\nγI θγD(p−1) + θ λn D(v ◦ p−1)\n= γD(s) + 1\nθλn D(s)D(v ◦ p)D(s)\n(23) γD(s) + 1\nθ D(s)E[XŜ ]D(s).\nFinally, it follows that\nEk[D(αk+1)−D(αk)] ≥ θ\nn n∑ i=1 [ φ∗i (−αki ) + φi(a>i wk) + 〈a>i wk, αki 〉 ] = θ(P (wk)−D(αk))."
    }, {
      "heading" : "APPENDIX: More insight into the relationship between σ2 and σ3",
      "text" : "In the main text we have shown that σ2 ≥ σ3, where σ2 is the rate of Method 2 and σ3 is the rate of Method 3: NSync (Richtárik & Takáč, 2013b). In this section we give a more detailed description of the relationship between these two quantities in the case when Ŝ is the τ -nice sampling (Richtárik & Takáč, 2012). That is, Ŝ picks subsets of [n] of cardinality τ , uniformly at random. For this sampling,\npi := Prob(i ∈ Ŝ) = τ\nn .\nProposition 3. Suppose that G = M and Ŝ be the τ -nice sampling. Then there exists β ∈ [1, τ ] such that one can choose vi = βMi,i and\nσ2 = βσ3\n(1− τ−1n−1 ) + n τ τ−1 n−1βσ3\n.\nProof. As explained in (Richtárik & Takáč, 2012), (10) is always true if we take vi = βMi,i with β = τ but smaller values (leading to a faster algorithm) may be computable if the problem exhibits a property called “partial separability”.\nLet us denote by D the diagonal matrix whose entries are the diagonal entries of M.\n(M[Ŝ])i,i = { Mi,i = Di,i if i ∈ Ŝ (probability τn ) 0 otherwise\n(M[Ŝ,Ŝ])i,j = { Mi,j if i ∈ Ŝ and j ∈ Ŝ (probability τ(τ−1)n(n−1) ) 0 otherwise.\nHence,\nE[MŜ ] = τ\nn D + τ(τ − 1) n(n− 1) (M−D) = τ n\n( (1− τ − 1\nn− 1 )D + τ − 1 n− 1\nM ) .\nLet us denote A = M−1/2DM−1/2 and α = τ−1n−1 .\nσ3 (13) =\nτ n β−1λmin(M 1/2D−1M1/2) = τ n β−1\n( λmax(A) )−1\nσ2 (12) =\nτ2 n2 λmin(M 1/2(E[MŜ ]) −1M1/2) = τ2 n2 λmin(M 1/2n τ ((1− α)D + αM)−1M1/2)\n= τ\nn\n( λmax(M −1/2((1− α)D + αM)M−1/2) )−1 = τ\nn\n( λmax((1− α)A + αI) )−1 But we have λmax((1− α)A + αI) = (1− α)λmax(A) + α, so\nτ nσ2 = (1− α) τ nβσ3 + α nσ2 τ = 1\n(1− α) τnβσ3 + α\nσ2 = σ3\n(1− α)β−1 + αnτ σ3 =\nβσ3\n(1− τ−1n−1 ) + τ−1 n−1 n τ βσ3\nNote that if σ3 is small, then σ2 is of the order of βσ31− τ−1n−1 > βσ3 ."
    } ],
    "references" : [ {
      "title" : "Sgdqn: Careful quasi-newton stochastic gradient descent",
      "author" : [ "Bordes", "Antoine", "Bottou", "Léon", "Gallinari", "Patrick" ],
      "venue" : "JMLR, 10:1737–1754,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2009
    }, {
      "title" : "A stochastic quasi-newton method for largescale optimization",
      "author" : [ "R.H. Byrd", "S.L. Hansen", "Nocedal", "Jorge", "Singer", "Yoram" ],
      "venue" : null,
      "citeRegEx" : "Byrd et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Byrd et al\\.",
      "year" : 2014
    }, {
      "title" : "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon" ],
      "venue" : null,
      "citeRegEx" : "Defazio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Defazio et al\\.",
      "year" : 2014
    }, {
      "title" : "Accelerated, parallel and proximal coordinate descent",
      "author" : [ "Fercoq", "Olivier", "Richtárik", "Peter" ],
      "venue" : "SIAM Journal on Optimization (after minor revision),",
      "citeRegEx" : "Fercoq et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fercoq et al\\.",
      "year" : 2013
    }, {
      "title" : "Smooth minimization of nonsmooth functions by parallel coordinate descent",
      "author" : [ "Fercoq", "Olivier", "Richtárik", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Fercoq et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fercoq et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast distributed coordinate descent for minimizing non-strongly convex losses",
      "author" : [ "Fercoq", "Olivier", "Qu", "Zheng", "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : "IEEE International Workshop on Machine Learning for Signal Processing,",
      "citeRegEx" : "Fercoq et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Fercoq et al\\.",
      "year" : 2014
    }, {
      "title" : "Robust block coordinate descent",
      "author" : [ "Fountoulakis", "Kimon", "Tappenden", "Rachael" ],
      "venue" : null,
      "citeRegEx" : "Fountoulakis et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Fountoulakis et al\\.",
      "year" : 2014
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Johnson", "Rie", "Zhang", "Tong" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2013
    }, {
      "title" : "S2GD: Semistochastic gradient descent methods",
      "author" : [ "Konečný", "Jakub", "Richtárik", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Konečný et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Konečný et al\\.",
      "year" : 2014
    }, {
      "title" : "mS2GD: Mini-batch semi-stochastic gradient descent in the proximal setting",
      "author" : [ "Konečný", "Jakub", "Lu", "Jie", "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : null,
      "citeRegEx" : "Konečný et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Konečný et al\\.",
      "year" : 2014
    }, {
      "title" : "An accelerated proximal coordinate gradient method and its application to regularized empirical risk minimization",
      "author" : [ "Lin", "Qihang", "Lu", "Zhaosong", "Xiao" ],
      "venue" : "Technical Report MSR-TR-2014-94, Microsoft Research,",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Incremental majorization-minimization optimization with application to large-scale machine learning",
      "author" : [ "Mairal", "Julien" ],
      "venue" : null,
      "citeRegEx" : "Mairal and Julien.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mairal and Julien.",
      "year" : 2014
    }, {
      "title" : "Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares",
      "author" : [ "Pilanci", "Mert", "Wainwright", "Martin J" ],
      "venue" : null,
      "citeRegEx" : "Pilanci et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pilanci et al\\.",
      "year" : 2014
    }, {
      "title" : "Coordinate descent with arbitrary sampling I: Algorithms and complexity",
      "author" : [ "Qu", "Zheng", "Richtárik", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Qu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2014
    }, {
      "title" : "Coordinate descent methods with arbitrary sampling II: Expected separable overapproximation",
      "author" : [ "Qu", "Zheng", "Richtárik", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Qu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2014
    }, {
      "title" : "Randomized dual coordinate ascent with arbitrary sampling",
      "author" : [ "Qu", "Zheng", "Richtárik", "Peter", "Zhang", "Tong" ],
      "venue" : null,
      "citeRegEx" : "Qu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2014
    }, {
      "title" : "Distributed coordinate descent method for learning with big data",
      "author" : [ "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : null,
      "citeRegEx" : "Richtárik et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Richtárik et al\\.",
      "year" : 2013
    }, {
      "title" : "On optimal probabilities in stochastic coordinate descent methods",
      "author" : [ "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : null,
      "citeRegEx" : "Richtárik et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Richtárik et al\\.",
      "year" : 2013
    }, {
      "title" : "Parallel coordinate descent methods for big data optimization problems",
      "author" : [ "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : "Mathematical Programming (after minor revision),",
      "citeRegEx" : "Richtárik et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Richtárik et al\\.",
      "year" : 2012
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
      "author" : [ "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Richtárik et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Richtárik et al\\.",
      "year" : 2014
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Schmidt", "Mark", "Le Roux", "Nicolas", "Bach", "Francis" ],
      "venue" : null,
      "citeRegEx" : "Schmidt et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2013
    }, {
      "title" : "A stochastic quasi-newton method for online convex optimization",
      "author" : [ "Schraudolph", "Nicol N", "Yu", "Jin", "Günter", "Simon" ],
      "venue" : "In AISTATS, pp",
      "citeRegEx" : "Schraudolph et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Schraudolph et al\\.",
      "year" : 2007
    }, {
      "title" : "Understanding Machine Learning: From Theory to Algorithms",
      "author" : [ "Shalev-Shwartz", "Shai", "Ben-David" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2014
    }, {
      "title" : "Accelerated minibatch stochastic dual coordinate ascent",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2013
    }, {
      "title" : "Accelerated minibatch stochastic dual coordinate ascent",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2013
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2013
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for SVM",
      "author" : [ "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nati", "Cotter", "Andrew" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods",
      "author" : [ "Sohl-Dickstein", "Jascha", "Poole", "Ben", "Ganguli", "Surya" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Sohl.Dickstein et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sohl.Dickstein et al\\.",
      "year" : 2014
    }, {
      "title" : "Mini-batch primal and dual methods for SVMs",
      "author" : [ "Takáč", "Martin", "Bijral", "Avleen", "Richtárik", "Peter", "Srebro", "Nathan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Takáč et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Takáč et al\\.",
      "year" : 2013
    }, {
      "title" : "Inexact block coordinate descent method: complexity and preconditioning",
      "author" : [ "Tappenden", "Rachael", "Richtárik", "Peter", "Gondzio", "Jacek" ],
      "venue" : null,
      "citeRegEx" : "Tappenden et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Tappenden et al\\.",
      "year" : 2013
    }, {
      "title" : "Separable approximations and decomposition methods for the augmented lagrangian",
      "author" : [ "Tappenden", "Rachael", "Richtárik", "Peter", "Büke", "Burak" ],
      "venue" : "Optimization Methods and Software,",
      "citeRegEx" : "Tappenden et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tappenden et al\\.",
      "year" : 2014
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "Xiao", "Lin", "Zhang", "Tong" ],
      "venue" : null,
      "citeRegEx" : "Xiao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2014
    }, {
      "title" : "Stochastic optimization with importance sampling",
      "author" : [ "Zhao", "Peilin", "Zhang", "Tong" ],
      "venue" : null,
      "citeRegEx" : "Zhao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "State-of-the-art optimization methods for ERM include i) stochastic (sub)gradient descent (Shalev-Shwartz et al., 2011; Takáč et al., 2013), ii) methods based on stochastic estimates of the gradient with diminishing variance such as SAG (Schmidt et al.",
      "startOffset" : 90,
      "endOffset" : 139
    }, {
      "referenceID" : 29,
      "context" : "State-of-the-art optimization methods for ERM include i) stochastic (sub)gradient descent (Shalev-Shwartz et al., 2011; Takáč et al., 2013), ii) methods based on stochastic estimates of the gradient with diminishing variance such as SAG (Schmidt et al.",
      "startOffset" : 90,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : ", 2013), ii) methods based on stochastic estimates of the gradient with diminishing variance such as SAG (Schmidt et al., 2013), SVRG (Johnson & Zhang, 2013), S2GD (Konečný & Richtárik, 2014), proxSVRG (Xiao & Zhang, 2014), MISO (Mairal, 2014), SAGA (Defazio et al.",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : ", 2013), SVRG (Johnson & Zhang, 2013), S2GD (Konečný & Richtárik, 2014), proxSVRG (Xiao & Zhang, 2014), MISO (Mairal, 2014), SAGA (Defazio et al., 2014), minibatch S2GD (Konečný et al.",
      "startOffset" : 130,
      "endOffset" : 152
    }, {
      "referenceID" : 29,
      "context" : ", 2014b), and iii) variants of stochastic dual coordinate ascent (Shalev-Shwartz & Zhang, 2013d; Zhao & Zhang, 2014; Takáč et al., 2013; Shalev-Shwartz & Zhang, 2013b;a; Lin et al., 2014; Qu et al., 2014; Shalev-Shwartz & Zhang, 2013c).",
      "startOffset" : 65,
      "endOffset" : 235
    }, {
      "referenceID" : 10,
      "context" : ", 2014b), and iii) variants of stochastic dual coordinate ascent (Shalev-Shwartz & Zhang, 2013d; Zhao & Zhang, 2014; Takáč et al., 2013; Shalev-Shwartz & Zhang, 2013b;a; Lin et al., 2014; Qu et al., 2014; Shalev-Shwartz & Zhang, 2013c).",
      "startOffset" : 65,
      "endOffset" : 235
    }, {
      "referenceID" : 13,
      "context" : ", 2014b), and iii) variants of stochastic dual coordinate ascent (Shalev-Shwartz & Zhang, 2013d; Zhao & Zhang, 2014; Takáč et al., 2013; Shalev-Shwartz & Zhang, 2013b;a; Lin et al., 2014; Qu et al., 2014; Shalev-Shwartz & Zhang, 2013c).",
      "startOffset" : 65,
      "endOffset" : 235
    }, {
      "referenceID" : 31,
      "context" : "For example, methods based on running coordinate ascent in the dual such as those mentioned above and also (Richtárik & Takáč, 2014; 2012; Fercoq & Richtárik, 2013b; Tappenden et al., 2014; Richtárik & Takáč, 2013a;b; Fercoq & Richtárik, 2013a; Fercoq et al., 2014; Qu et al., 2014; Qu & Richtárik, 2014a) use curvature information contained in the diagonal of a bound on the Hessian matrix.",
      "startOffset" : 107,
      "endOffset" : 305
    }, {
      "referenceID" : 5,
      "context" : "For example, methods based on running coordinate ascent in the dual such as those mentioned above and also (Richtárik & Takáč, 2014; 2012; Fercoq & Richtárik, 2013b; Tappenden et al., 2014; Richtárik & Takáč, 2013a;b; Fercoq & Richtárik, 2013a; Fercoq et al., 2014; Qu et al., 2014; Qu & Richtárik, 2014a) use curvature information contained in the diagonal of a bound on the Hessian matrix.",
      "startOffset" : 107,
      "endOffset" : 305
    }, {
      "referenceID" : 13,
      "context" : "For example, methods based on running coordinate ascent in the dual such as those mentioned above and also (Richtárik & Takáč, 2014; 2012; Fercoq & Richtárik, 2013b; Tappenden et al., 2014; Richtárik & Takáč, 2013a;b; Fercoq & Richtárik, 2013a; Fercoq et al., 2014; Qu et al., 2014; Qu & Richtárik, 2014a) use curvature information contained in the diagonal of a bound on the Hessian matrix.",
      "startOffset" : 107,
      "endOffset" : 305
    }, {
      "referenceID" : 30,
      "context" : "Block coordinate descent methods, when equipped with suitable data-dependent norms for the blocks, use information contained in the block diagonal of the Hessian (Tappenden et al., 2013).",
      "startOffset" : 162,
      "endOffset" : 186
    }, {
      "referenceID" : 0,
      "context" : ", 2013), SVRG (Johnson & Zhang, 2013), S2GD (Konečný & Richtárik, 2014), proxSVRG (Xiao & Zhang, 2014), MISO (Mairal, 2014), SAGA (Defazio et al., 2014), minibatch S2GD (Konečný et al., 2014a), S2CD (Konečný et al., 2014b), and iii) variants of stochastic dual coordinate ascent (Shalev-Shwartz & Zhang, 2013d; Zhao & Zhang, 2014; Takáč et al., 2013; Shalev-Shwartz & Zhang, 2013b;a; Lin et al., 2014; Qu et al., 2014; Shalev-Shwartz & Zhang, 2013c). There have been several attempts at designing methods that combine randomization with the use of curvature (secondorder) information. For example, methods based on running coordinate ascent in the dual such as those mentioned above and also (Richtárik & Takáč, 2014; 2012; Fercoq & Richtárik, 2013b; Tappenden et al., 2014; Richtárik & Takáč, 2013a;b; Fercoq & Richtárik, 2013a; Fercoq et al., 2014; Qu et al., 2014; Qu & Richtárik, 2014a) use curvature information contained in the diagonal of a bound on the Hessian matrix. Block coordinate descent methods, when equipped with suitable data-dependent norms for the blocks, use information contained in the block diagonal of the Hessian (Tappenden et al., 2013). A more direct route to incorporating curvature information was taken by Schraudolph et al. (2007) in their stochastic L-BFGS method, by Byrd et al.",
      "startOffset" : 131,
      "endOffset" : 1263
    }, {
      "referenceID" : 0,
      "context" : "(2007) in their stochastic L-BFGS method, by Byrd et al. (2014) and Sohl-Dickstein et al.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "(2007) in their stochastic L-BFGS method, by Byrd et al. (2014) and Sohl-Dickstein et al. (2014) in their stochastic quasi-Newton methods and by Fountoulakis & Tappenden (2014) who proposed a stochastic block coordinate descent methods.",
      "startOffset" : 45,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "(2007) in their stochastic L-BFGS method, by Byrd et al. (2014) and Sohl-Dickstein et al. (2014) in their stochastic quasi-Newton methods and by Fountoulakis & Tappenden (2014) who proposed a stochastic block coordinate descent methods.",
      "startOffset" : 45,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "An exception in this regard is the work of Bordes et al. (2009), who give a O(1/ ) complexar X iv :1 50 2.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "In particular, we show that the expected duality gap decreases at a geometric rate which i) is better than that of SDCA-like methods such as SDCA (ShalevShwartz & Zhang, 2013d) and QUARTZ (Qu et al., 2014), and ii) improves with increasing minibatch size.",
      "startOffset" : 188,
      "endOffset" : 205
    }, {
      "referenceID" : 13,
      "context" : "In particular, we show that the expected duality gap decreases at a geometric rate which i) is better than that of SDCA-like methods such as SDCA (ShalevShwartz & Zhang, 2013d) and QUARTZ (Qu et al., 2014), and ii) improves with increasing minibatch size. This improvement does not come for free: as we increase the minibatch size, the subproblems grow in size as they involve larger portions of the Hessian. We find through experiments that for some, especially dense problems, even relatively small minibatch sizes lead to dramatic speedups in actual runtime. We show that in the case of quadratic loss, and when viewed as a primal method, SDNA can be interpreted as a variant of the recently introduced Iterative Hessian Sketch algorithm (Pilanci & Wainwright, 2014). En route to developing SDNA which we describe in Section 4, we also develop several other new algorithms: two in Section 2 (where we focus on smooth problems), one in Section 3 (where we focus on composite problems). Besides SDNA, we also develop and analyze a novel minibatch variant of SDCA in Section 4, for the sake of finding suitable method to compare SDNA to. SDNA is equivalent to applying the new method developed in Section 3 to the dual of the ERM problem. However, as we are mainly interested in solving the ERM (primal) problem, we additionally prove that the expected duality gap decreases at a geometric rate. Our technique for doing this is a variant of the one use by Shalev-Shwartz & Zhang (2013d), but generalized to an arbitrary sampling.",
      "startOffset" : 189,
      "endOffset" : 1487
    }, {
      "referenceID" : 26,
      "context" : "Takáč et al. (2013) developed such a method but in the special case of hinge-loss (which is not smooth and hence does not fit our setup).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 26,
      "context" : "Takáč et al. (2013) developed such a method but in the special case of hinge-loss (which is not smooth and hence does not fit our setup). Shalev-Shwartz & Zhang (2013b) studied minibatching but in conjunction with acceleration and the QUARTZ method of Qu et al.",
      "startOffset" : 0,
      "endOffset" : 169
    }, {
      "referenceID" : 13,
      "context" : "Shalev-Shwartz & Zhang (2013b) studied minibatching but in conjunction with acceleration and the QUARTZ method of Qu et al. (2014), which has been analyzed for an arbitrary sampling Ŝ, uses a different primal update than SDNA.",
      "startOffset" : 114,
      "endOffset" : 131
    } ],
    "year" : 2015,
    "abstractText" : "We propose a new algorithm for minimizing regularized empirical loss: Stochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each iteration we update a random subset of the dual variables. However, unlike existing methods such as stochastic dual coordinate ascent, SDNA is capable of utilizing all curvature information contained in the examples, which leads to striking improvements in both theory and practice – sometimes by orders of magnitude. In the special case when an L2-regularizer is used in the primal, the dual problem is a concave quadratic maximization problem plus a separable term. In this regime, SDNA in each step solves a proximal subproblem involving a random principal submatrix of the Hessian of the quadratic function; whence the name of the method. If, in addition, the loss functions are quadratic, our method can be interpreted as a novel variant of the recently introduced Iterative Hessian Sketch.",
    "creator" : "LaTeX with hyperref package"
  }
}