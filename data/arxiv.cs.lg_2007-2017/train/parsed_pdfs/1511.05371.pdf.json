{
  "name" : "1511.05371.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CONSTANT TIME EXPECTED SIMILARITY ESTIMATION USING STOCHASTIC OPTIMIZATION",
    "authors" : [ "markus schneider" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "A new algorithm named EXPected Similarity Estimation (EXPoSE) was recently proposed to solve the problem of large-scale anomaly detection. It is a non-parametric and distribution free kernel method based on the Hilbert space embedding of probability measures. Given a dataset of n samples, EXPoSE needs only O(n) (linear time) to build a model and O(1) (constant time) to make a prediction. In this work we improve the linear computational complexity and show that an -accurate model can be estimated in constant time, which has significant implications for large-scale learning problems. To achieve this goal, we cast the original EXPoSE formulation into a stochastic optimization problem. It is crucial that this approach allows us to determine the number of iteration based on a desired accuracy , independent of the dataset size n. We will show that the proposed stochastic gradient descent algorithm works in general (possible infinite-dimensional) Hilbert spaces, is easy to implement and requires no additional step-size parameters."
    }, {
      "heading" : "1 Introduction",
      "text" : "EXPected Similarity Estimation (EXPoSE) was recently proposed to solve the problem of large-scale anomaly detection, where the number of training samples n and the dimension of the data d are too high for most other algorithms [SEP15]. Here, “anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. These non-conforming patterns are often referred to as anomalies” [CBK09].\nAs explained later in detail, the EXPoSE anomaly detection classifier\nη(y) = 〈φ(y),µ[P]〉\ncalculates a score (the likelihood of y belonging to the class of normal data) using the inner product between a feature map φ and the kernel mean map µ[P] of the distribution P (Fig. 1). Given a training dataset of size n, the authors provide a methodology to train this classifier in O(n) time and show that calculating a score for a query point can be done in O(1) time. The question arises if it is possible to improve on the linear training time and create an algorithm which is\n? Institute of Neural Information Processing, University of Ulm, Germany\n† Institute for Artificial Intelligence, Ravensburg-Weingarten University of Applied Sciences, Germany\ncompletely independent of the dataset size.\nThe answer to this question is positive if a high accuracy sample estimate of µ[P] does not improve the anomaly detection performance. As Bousquet and Bottou [BB08] observed, for most machine learning applications there is no need to optimize below the statistical error. The authors argue that accurately minimizing an empirical cost function does not gain much since it is itself an approximation of the expected costs and therefore contains errors. We will see that it is possible to determine the number of samples needed to achieve a desired accuracy (the maximal deviation from the optimal model) of EXPoSE without any dependence on the datasets size n.\nar X\niv :1\n51 1.\n05 37\n1v 1\n[ cs\n.L G\n] 1\n7 N\nov 2\n01 5"
    }, {
      "heading" : "1.1 CONTRIBUTIONS & RELATED WORK",
      "text" : "In this work we derive a methodology to build an - accurate model w of µ[P] using only a random subset of the training data by means of stochastic optimization.\nDefinition 1: We say an algorithm finds an -accurate solution w of an objective function f if\nf(w) 6 inf f+\nfor a given > 0. «\nWe will show that for the proposed objective function E[f(wt) − f(µ[P])] 6 O(1/t), where wt only needs access to t random dataset elements, t ∈ {1, 2, . . . ,n}. The key observation is that for a given an > 0 we can reach ‖wt − µ[P]‖ < in a fixed number of iterations independent of the dataset size. Moreover, it can be shown that (without further assumptions) the O(1/t) rate is optimal for stochastic optimization [Aga+11].\nDue to the low iteration costs, stochastic optimization and especially stochastic gradient (SG) methods [BB08; RSB12], are widely used for training machine learning models on very large-scale datasets. Such algorithms are used for example to train support vector machines [SS+11], logistic regression [Bac14] and lasso models [SST11]. However, this is the first time that EXPoSE is considered as an optimization problem and we will show that the derived algorithms is of general interest for applications of the kernel mean map µ[P].\nOther optimization techniques such as projected gradient decent [BV04] or Nesterov’s accelerated gradient descent [Nes83; Nes04] are also applicable in principle, however a single gradient evaluation takes already O(n) time and hence would be slower than the originally proposed EXPoSE approach. Other stochastic gradient methods [RSB12] can obtain a better convergence rate than O(1/t) for an objective composed of a sum of smooth functions. However this requires multiple passes over the datasets is therefor of no benefit."
    }, {
      "heading" : "2 Problem Description",
      "text" : "EXPoSE is a probabilistic approach which assumes that the normal, non-anomalous data is distributed according to some measure P. More formally, let X be a random variable taking values in a measure space (X, X ) with distribution P. We denote the reproducing kernel Hilbert space (RKHS) associated with the kernel k : X×X→ R with (H, 〈·, ·〉). A RKHS is a Hilbert space of functions g : X→ R, where the evaluation functional δx : g 7→ g(x) is continuous. The function φ : X → H\nwith\nk(x,y) = 〈φ(x),φ(y)〉\nis called feature map denoted by φ(x) = k(x, ·). Throughout the paper, we use ‖·‖H to denote the norm induced by the inner product defined as ‖g‖H = √ 〈g,g〉.\nEXPoSE calculates a score which can be interpreted as the likelihood of a query point belonging to the distribution of normal data P. This is done in the following way.\nDefinition 2: The expected similarity of y ∈ X to the (probability) distribution P is defined as\nη(y) = ∫ X k(y, x)dP(x),\nwhere k : X×X→ R is a reproducing kernel. «\nIntuitively speaking the query point y is compared to all other points of the distribution P. It can be shown [SEP15] that this equation can be rewritten as an inner product between the feature map φ(y) and the kernel embedding µ[P] of P as\nη(y) = ∫ X k(y, x)dP(x)\n= 〈φ(y),µ[P]〉,\nwhere the kernel embedding is defined as follows.\nDefinition 3: The kernel embedding or kernel mean map µ[P] associated with the continuous, bounded and positive-definite kernel function k is\nµ[P] = ∫ X φ(x)dP(x),\nwhere P is a Borel probability measure on X. «\nTo facilitate the further analysis, we assume that the kernel k is measurable and bounded such that µ[P] exists for all P ∈M1+(X) [SFL11]. Since the underlying distribution P is in general unknown and only a set of n ∈ N samples {x1, . . . , xn} from P is available for analysis, the empirical measure\nPn = 1\nn n∑ i=1 δxi ,\nact as a surrogate, where δx is the Dirac measure. Pn can be used to construct an approximation µ[Pn] of µ[P] as\nµ[P] ≈ µ[Pn] = ∫ X φ(x) dPn(x) = 1 n n∑ i=1 φ(xi)\nwhich is called empirical kernel embedding [Smo+07].\nThe consequence of the equation above is, that the empirical kernel embedding µ[Pn] has a computational complexity with linear dependence on n and responsible for the linear EXPoSE training time. Next, we will look at the EXPoSE classifier from the perspective of a stochastic optimization problem to deliver an -accurate approximation of µ[P] in constant time. A reduction of the computationally complexity from linear to constant for the empirical kernel mean map has significant impact on a variety of applications based on the kernel embedding such as for example statistical hypotheses testing [Gre+12] or independence testing [Gre+05].\nHowever the main focus of this work is to improve the EXPoSE training time from linear to constant."
    }, {
      "heading" : "3 Stochastic Optimization",
      "text" : "This sections derives the stochastic optimization problem together with some general conditions which will be necessary at a later stage. Obviously µ[P] ∈ H is the solution of the following unconstrained optimization problem\nmin w∈H g(w) = min w∈H ‖µ[P] −w‖2H = min w∈H 〈w,w〉− 2〈µ[P],w〉+ 〈µ[P],µ[P]〉\n= min w∈H\n1 2 〈w,w〉− 〈µ[P],w〉.\nThis is equivalent to the stochastic optimization problem, where we minimize over the expectation of an objective function\nmin w∈H E[f(w)] = min w∈H ∫ X f(w) dP(x),\nwith\nf(w) = 1\n2 〈w,w〉− 〈φ(X),w〉,\nwhere the expectation is taken with respect to the random variable X.\nWe will assume that we can generate independent samples from P and furthermore require an oracle which returns a stochastic subgradient ∇̃f(w) of f at w. A stochastic subgradient has the property that\nE[∇̃f(w)] = ∇f(w) ∈ ∂f(w)\nwhich means its expectation is equal to a subgradient ∇f(w). Here ∂f(w) denotes the set of all subgradients at w called the subdifferential which is a subset of the dual\nH∗ of H defined by\n∂f(x) = { ξ∗ ∈ H∗ ∣∣ f(y) − f(x) > ξ∗(y− x)}. Proposition 1: The random variable\n∇̃f(w) = w−φ(X)\nis a stochastic unbiased gradient of f at w. «\nProof: The expectation of ∇̃f(w) is given by\nE[∇̃f(w)] = ∫ w−φ(X)dP(x)\n= w− µ[P] ∈ ∂f(w)\nwhich is a stochastic unbiased (sub)gradient by definition.\nWe are going to solve this optimization problem with the stochastic approximation algorithm [RM51] described next."
    }, {
      "heading" : "3.1 STOCHASTIC APPROXIMATION",
      "text" : "Let H be a Hilbert space, H ⊆ H be a subset and f : H→ R some objective function. Furthermore let\nΠH(w) = arg min v∈H ‖w− v‖H\nbe the metric projection operator. ΠH is in general nonexpanding such that\n‖ΠH(w) −ΠH(w ′)‖H 6 ‖w−w ′‖H\nholds. Then the classic stochastic approximation algorithm [RM51] creates the sequence (wt) as\nwt+1 = ΠH ( wt − γt∇̃f(wt) ) ,\nto solve the stochastic optimization problem\nmin w∈H E[f(w)]\nstarting at some w1 ∈ H. Here (γt) is a sequence of positive step sizes and the optimal solution to the problem is denoted by w?.\nNemirovski et al. [Nem+09] considered H = Rd and showed that stochastic approximation can obtain a O(1/t) convergence rate if the objective function f is differentiable and α-strongly convex on H. Here, αstrongly convex means there exists a constant α > 0 such that\nf(y) > f(x) + 〈∇f(x),y− x〉+ 1 2 α‖y− x‖2H\nfor all x,y ∈ H. An additional requirement is that the stochastic subgradient has to be bounded in expectation\nE [ ‖∇̃f(w)‖2H ] 6M2 ∀w ∈ H,M > 0\nand the step sizes need to be γt = θt for some θ > 1 2α . Under these conditions, Nemirovski et al. demonstrated that\nE [ ‖wt −w?‖2H ] 6 Q(θ)\nt (1)\nwhere Q(θ) = max { θ2M2(2αθ− 1)−1, ‖w1 −w?‖2H } .\nFurthermore, if the gradient is Lipschitz continuous, i.e. there is a constant β > 0 such that\n‖∇f(x) −∇f(y)‖H 6 β‖x− y‖H\nfor all x,y ∈ H, then\nE [ f(wt) − f(w ?) ] 6 1\n2\nβQ(θ)\nt . (2)\nFor Lipschitz continuous strongly convex functions, the O(1/t) rate of convergence is unimprovable [Aga+11].\nWe will see that the bound from Nemirovski et al. does also hold when H is a (possibly infinite-dimensional) Hilbert space as in the problem considered in this work. However, some care has to be taken since, unlike in finite-dimensional spaces, being closed and bounded does not imply that a set is compact when H is infinitedimensional. We also refer to [JN10] for a discussion on primal-dual subgradient methods in non-Euclidean spaces."
    }, {
      "heading" : "4 Stochastic Optimization of EXPoSE",
      "text" : "In this section we show the existence and uniqueness of a solution for the previously defined stochastic optimization problem of EXPoSE and also that it meets all requirements for a O(1/t) convergence rate.\nIn the following let H be a RKHS space with a bounded kernel k such that ‖k(x,y)‖ 6 M2. Let H ⊆ H be a weakly sequentially closed and bounded set with ‖H‖H 6 M. It is not hard to show the existence of a minimizer of\nmin w∈H E[f(w)] = min w∈H\n∫ 1\n2 〈w,w〉− 〈φ(X),w〉 dP, (3)\nsince we already know the solution w? = µ[P] assuming\nthat µ[P] ∈ H. This assumption holds since\n‖µ[P]‖2H = ∥∥∥∫\nX\nφ(x)dP(x) ∥∥∥2 H\n6 ∫ X ‖φ(x)‖2H dP(x)\n= ∫ X k(x, x)dP(x) 6M2.\nThis solution is also unique. The proof requires f(w) to be strongly convex, which is subject of the following property:\nProposition 2: The objective function f(w) is α-strongly convex and its gradient is β-Lipschitz with α = β = 1.«\nProof: A function f is α-strongly convex if and only if w 7→ f(w) − α2 ‖w‖ 2 H is convex.\nf(w) − 1\n2 ‖w‖2H =\n1 2 〈w,w〉− 〈φ(X),w〉− 1 2 ‖w‖2H\n= −〈φ(X),w〉\nwhich is convex in w. Hence α = 1.\nFurthermore Df(w) : z 7→ 〈w − φ(X), z〉 is the Fréchet derivative of f at w since\nlim h→0 ‖f(w+ h) − f(w) − 〈Df(w)|h〉‖ ‖h‖H = 0\nwith dual pairing 〈·|·〉. The gradient ∇f(w) = w−φ(X) is β-Lipschitz since\n‖∇f(w) −∇f(v)‖H = ‖w−φ(X) − v+φ(X)‖H = ‖w− v‖H\nfor all w, v ∈ H due to Riesz representation. Besides the existence of a minimizer, its uniqueness plays an important role. The sufficient conditions for w? to be unique are given by [Pey15, Corollary 2.19] which states the following: Corollary 1: Let H be reflexive. If f : H → R ∪ {+∞} is proper, convex, coercive and lower-semicontinuous, then arg min f is nonempty and weakly compact. If, moreover f is strictly convex, then arg min f is a singleton. «\nProof: w? is unique: All Hilbert spaces are reflexive. Since f is continuous, proper (dom(f) 6= {}) and strongly convex it is also convex, coercive and lowersemicontinuous.\nNext we state the two main theorems of this paper.\nTheorem 1: Using the sequence\nwt+1 = ΠH ( wt − γt∇̃f(wt) ) ,\nwith f given by Eq. (3) we have\nE [ ‖wt −w?‖2H ] 6 M2\nt\nfor all t ∈N. « Proof: Since Q(θ) attains its optimal value at θ = 1/α we get from Eq. (1) that\nE [ ‖wt −w?‖2H ] 6 t−1max { α−2M2, ‖w1 −w?‖2H } and we have\nE [ ‖w1 −w?‖2H ] 6 M2\nα2\nsince strong convexity implies\n〈w−w?,∇f(w)〉 > α‖w−w?‖2H 〈w−w?,∇f(w)〉2 > α2‖w−w?‖4H\nand by Cauchy-Schwartz inequality we get\n‖w−w?‖2H · ‖∇f(w)‖2H > 〈w−w?,∇f(w)〉2\nwhich yields\n‖w−w?‖2H · ‖∇f(w)‖2H > α2‖w−w?‖4H ‖∇f(w)‖2H > α2‖w−w?‖2H\nfor all w. Taking exponents on both sides and the bound ‖∇f(w)‖2H 6M2 we get\nE [ ‖w−w?‖2H ] 6 α−2M2\nwhich concludes the proof using α = 1 (Proposition 2).\nNotice that ‖∇f(w)‖2H 6 M2 does indeed hold since ∇f(w) = w − φ(X) ∈ H. The following theorems describes the convergence rate of the objective function f in terms of the number of iterations t.\nTheorem 2: Under the prerequisites of Theorem 1 it holds that\nE [ f(wt) − f(w ?) ] 6 1\n2\nM2\nt .\nProof: Using Eq. (2) and the bound for Q(θ) derived before yields the desired result.\nWe showed above that (in expectation) the distance between the optimal objective f(w?) and f(wt) decays as O(1/t). Another question is how this effects the EXPoSE decision rule η(y) = 〈φ(y),µ[P]〉. By definition and the application of the Cauchy–Schwarz inequality\nit holds that\n‖〈φ(y),µ[P]〉− 〈φ(y),wt〉‖ = ‖〈φ(y),µ[P] −wt〉‖ 6 ‖φ(y)‖H · ‖µ[P] −wt‖H\nfor all y ∈ H. Taking expectations yields\nE [ ‖〈φ(y),µ[P]〉− 〈φ(y),wt〉‖2 ] 6 ‖φ(y)‖2H M2\nt\nfor all t ∈N.\nAlgorithm 1 EXPoSE using Stochastic Optimization\nRequire: 1: T : the number of iterations or : accuracy Algorithm: 2: Set w1 ← 0 3: for t← 1, 2, . . . , T do 4: Sample xt uniformly from P 5: Set γt ← 1t 6: Set ∇̃f(wt)← wt −φ(xt) 7: Update wt+1 ← wt − γt∇̃f(wt) 8: Project wt+1 ← wt+1 ·max{1,M‖wt+1‖}−1\n9: return wT+1\nThe stochastic optimization procedure for EXPoSE is summarized in Algorithm 1. Please note that the stochastic optimization procedure presented here is relatively simple and requires only a few lines of code to implement. It also does not introduce additional parameters since the optimal step-size in known. Step-sizes are crucial and difficult to determine in most optimization algorithms as they have a significant effect on the results. The bound M of the kernel is typically known and the number of iterations T determines the computing time and accuracy. Alternatively, the number of iterations T can be calculated given a desired accuracy using Theorem 1. The projection operator ΠH(w) in the last step takes a form which can efficiently be computed, projecting w onto the sphere H.\nWe emphasize that the stochastic optimization procedure introduce here does not improve on the O(1/ √ t)) convergence rate of the empirical kernel mean map as demonstrated in Theorem 1, but introduces a methodology to reduce the computational complexity from linear to constant."
    }, {
      "heading" : "4.1 CONVERGENCE OF EXPOSE",
      "text" : "Since w → w? converges, this implies also the weak convergence [Pey15] from w⇀ w? namely\nlim t→∞〈u,wt〉 = 〈u,w?〉, ∀u ∈ H\nand especially\nlim t→∞〈φ(y),wt〉 = 〈φ(y),µ[P]〉, ∀y ∈ X\nwhich justifies the use of wt as a surrogate for µ[P]."
    }, {
      "heading" : "4.2 REGULARIZATION",
      "text" : "We would like to mention that the reformulation of EXPoSE as an optimization problem also introduces the opportunity to add constraints or similar properties to the objective function. One approach is to define a general regularizer λΩ(w) on H replacing 12 〈w,w〉 in Eq. (3) which yields\nmin w∈H E[f(w)] = min w∈H\n∫ λΩ(w) − 〈φ(X),w〉 dP\nwith some regularization parameter λ > 0. An example would be to add a roughness penalty to the space of functions setting\nλΩ(w) = λ〈D2w,D2w〉\nwhere D denote the differential operator. Another possibility is to places a sparsity constraint on w. If H admits it, we can use\nλΩ(w) = λ‖w‖1,\nwhere ‖·‖1 is the l1-norm.\nThe disadvantage of other objective functions is, that these are in general not strongly-convex and hence yielding a slower convergence rate and may require additional parameters which are difficult to tune."
    }, {
      "heading" : "5 Experimental Evaluation",
      "text" : "We present experimental results demonstrating the benefit of the proposed approach. Since the true distribution P is often unknown and a closed form solution of µ[P] is not available, we will use the empirical distribution Pn as its surrogate in the objective function and measure the behavior of\n‖wt − µ[Pn]‖H\nas t increases. For sufficiently large sample sizes n we can expect µ[Pn] to be a good proxy for µ[P] by the law of large numbers. Besides the convergence of the model wt → µ[P], we will examine and compare the anomaly\ndetection scores\nηn(y) = 〈φ(y),µ[Pn]〉 and ηt(y) = 〈φ(y),wt〉\ncalculated by the empirical distribution (which is the original EXPoSE predictor proposed in [SEP15]) and the stochastic optimization approximation, respectively."
    }, {
      "heading" : "5.1 APPROXIMATE FEATURE MAPS",
      "text" : "While it is theoretically possible to calculate quantities like ‖wt − µ[Pn]‖H for any kernel k, this is extremely slow and intractable for most large-scale datasets. For datasets with a small sample size n we cannot expect µ[Pn] to be a good proxy for µ[P]. We therefore omit an experiment with explicit features as we either cannot compute µ[Pn] (large n) or µ[Pn] is not a good estimate for µ[P] (small n).\nIn order to overcome this problem, EXPoSE exploits the idea of approximate feature maps for its computational efficiency. The aim is to find approximations φ̂ : X→ Rr of φ such that\nk(x,y) ≈ 〈φ̂(x), φ̂(y)〉\nfor all x,y ∈ X and r ∈ N. We will utilize the Random Kitchen Sinks (RKS) approach [RR07; RR08] which is based on Bochner’s theorem for translation invariant kernels (such as the Gaussian RBF, Laplace, Matérn covariance, etc.). For example in the following experiments we will use the Gaussian RBF kernel k(x,y) = exp ( − 1 2σ2 ‖x − y‖2 ) , which can be approximated by\nZ ∈ Rr×d with Zij ∼ N(0,σ2)\nφ̂(x) = 1√ r exp(ıZx),\nwhere d is the dimension of X ⊆ Rd. The parameter r ∈ N determines the number of kernel expansions and is typically around 20,000. The specific choice of approximate feature map does not affect the previous theoretical analysis and other feature map approximations [LIS10; VZ12; KK12] can be used as well."
    }, {
      "heading" : "5.2 DATASETS",
      "text" : "The following datasets, which all have purposely very different feature characteristics, are used to perform anomaly detection. We refer to [SEP15] for a detailed description of the datasets and feature characteristic.\n• The MNIST database contains 70,000 images of\nhandwritten digits. Using the raw pixel values yield an input space dimension of 784.\n• KDD-CUP 99 is an intrusion detection dataset which contains 4,898,431 connection records of network traffic. As in [SEP15] we rescale the 34 continuous features to [0, 1] and apply a binary encoding for the 7 symbolic features.\n• The third dataset contains 600,000 instances of the Google Street View House Numbers (SVHN) [Net+11] where we use the Histogram of Oriented Gradients (HOG) with a cell size of 3 to get a 2592- dimensional feature vector.\nThe kernel bandwidth σ2 used for these datasets are 7.0, 5.6 and 7.8 respectively, which we found to yield a reasonable anomaly detection performance.\nSince SVHN and MNIST are multi-class and not anomaly detection datasets we use the digit 1 as normal class and all other digits as anomaly instances.1 At each iteration of Algorithm 1 we uniformly choose an instance from the (training) dataset not used previously. We then update the model wt according to the algorithm. Every 200 iterations, wt is used to calculate an anomaly detection score for 10,000 dedicated random instances of the (test) dataset using the full model ηn(y) and the stochastic optimization approximation ηt(y)."
    }, {
      "heading" : "5.3 DISCUSSION",
      "text" : "The experimental results with approximate feature maps are shown in Fig. 2. The first row contains traces of the objective function f(wt) − f(w?), where w? ≈ µ[Pn] for all three datasets. The stochastic optimization algorithm already reaches a reasonable low objective after a few hundred iterations. A further improvement is only visible on a logarithmic scale (dashed blue) on the second y-axis on the right. More important, we observe a similar effect in the second row when comparing ‖wt −w?‖. We get near to w? relatively fast, but it takes much more samples to estimate w? with a high accuracy. However, we will see that a high accuracy estimation is necessary for a good anomaly detection performance. To measure the anomaly detection rate, we first plug wt and w? into the EXPoSE estimators ηt(y) and ηn(y) respectively and calculate scores for all instances in the test dataset. The difference of these scores are shown in row number three. We see again, that the stochastic optimization approximation ηt(y) yields similar scores as the full ηn(y). The last row illustrates the development of the classification error as\n1A different normal/anomaly setup had no significant impact on the experimental results.\nmore iterations are performed2. After only a few hundred iterations ηt(y) reaches the same classification error as the original EXPoSE predictor ηn(y). This confirms that a high accuracy approximation of w? does not necessarily lead to a better predictor. The key is, that for a given we can reach ‖wt −w?‖ < in a fixed number of iterations, independent of the dataset size n which reduced the computational complexity from O(n) to O(1).\nWe emphasize that, unlike other regularized risk minimization problems, EXPoSE does not have a regularization parameter. This is important as the authors of Pegasos noticed that “[. . .] the runtime to achieve a predetermined suboptimality threshold would increase in proportion to λ [the regularization parameter]. Very small values of λ (small amounts of regularization) result in rather long runtimes” [SS+11]."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work we cast the EXPoSE anomaly detection algorithm into a stochastic optimization problem. This enables us to fine an -accurate approximation of the kernel mean map µ[P] in constant time, independent of the training dataset size n. In particular, this approximations reduces the computational complexity of EXPoSE and the empirical kernel mean map from the previous O(n) to O(1) whenever an -accurate estimation is sufficient. More precisely, we are able to determine the number of necessary stochastic optimization iterations T for a user defined error threshold such that ‖wT −w?‖ < . The intuition is that a very high accuracy estimation w? does not necessarily result in a better anomaly detection performance and hence there is no benefit in spending more computational resources. This intuition is also confirmed experimentally on three large-scale datasets, where we reach the same anomaly detection performance long before all data is incorporated into the model. This is the first time that an optimization routine is used for EXPoSE and we provide a detailed theoretical analysis of this algorithm. We emphasize that the proposed algorithm does not introduce any additional parameters which have to be tuned and the gradient descent step-sizes are determined automatically. This has significant implications for large-scale applications such as anomaly detection problems and other techniques which are based on the kernel mean embedding.\n2The prediction score threshold is determined by means of crossvalidation."
    } ],
    "references" : [ {
      "title" : "Information-Theoretic Lower Bounds on the Oracle Complexity of Convex Optimization Convex optimization",
      "author" : [ "A. Agarwal" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Agarwal,? \\Q2011\\E",
      "shortCiteRegEx" : "Agarwal",
      "year" : 2011
    }, {
      "title" : "Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression",
      "author" : [ "F. Bach" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "Bach.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bach.",
      "year" : 2014
    }, {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "O. Bousquet", "L. Bottou" ],
      "venue" : "Advances in neural information processing systems",
      "citeRegEx" : "Bousquet and Bottou.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bousquet and Bottou.",
      "year" : 2008
    }, {
      "title" : "Convex optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge university press",
      "citeRegEx" : "BV04",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Anomaly detection: A survey",
      "author" : [ "V. Chandola", "A. Banerjee", "V. Kumar" ],
      "venue" : "ACM Computing Surveys (CSUR)",
      "citeRegEx" : "Chandola et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chandola et al\\.",
      "year" : 2009
    }, {
      "title" : "Measuring statistical dependence with Hilbert-Schmidt norms",
      "author" : [ "A. Gretton" ],
      "venue" : "Algorithmic learning theory. Springer",
      "citeRegEx" : "Gretton,? \\Q2005\\E",
      "shortCiteRegEx" : "Gretton",
      "year" : 2005
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "A. Gretton" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "Gretton,? \\Q2012\\E",
      "shortCiteRegEx" : "Gretton",
      "year" : 2012
    }, {
      "title" : "Primal-dual subgradient methods for minimizing uniformly convex functions",
      "author" : [ "A. Juditsky", "Y. Nesterov" ],
      "venue" : null,
      "citeRegEx" : "Juditsky and Nesterov.,? \\Q2010\\E",
      "shortCiteRegEx" : "Juditsky and Nesterov.",
      "year" : 2010
    }, {
      "title" : "Random feature maps for dot product kernels",
      "author" : [ "P. Kar", "H. Karnick" ],
      "venue" : "In: International Conference on Artificial Intelligence and Statistics",
      "citeRegEx" : "Kar and Karnick.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kar and Karnick.",
      "year" : 2012
    }, {
      "title" : "Random Fourier approximations for skewed multiplicative histogram kernels",
      "author" : [ "F. Li", "C. Ionescu", "C. Sminchisescu" ],
      "venue" : "Pattern Recognition. Springer,",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski" ],
      "venue" : "SIAM Journal on Optimization",
      "citeRegEx" : "Nemirovski,? \\Q2009\\E",
      "shortCiteRegEx" : "Nemirovski",
      "year" : 2009
    }, {
      "title" : "Introductory lectures on convex optimization",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Vol. 87. Springer Science & Business Media",
      "citeRegEx" : "Nes04",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate O (1/k2)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Soviet Mathematics Doklady",
      "citeRegEx" : "Nesterov.,? \\Q1983\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 1983
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Y. Netzer" ],
      "venue" : "NIPS workshop on deep learning and unsupervised feature learning",
      "citeRegEx" : "Netzer,? \\Q2011\\E",
      "shortCiteRegEx" : "Netzer",
      "year" : 2011
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : null,
      "citeRegEx" : "Robbins and Monro.,? \\Q1951\\E",
      "shortCiteRegEx" : "Robbins and Monro.",
      "year" : 1951
    }, {
      "title" : "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning",
      "author" : [ "A Rahimi", "B Recht" ],
      "venue" : "Advances in neural information processing systems",
      "citeRegEx" : "Rahimi and Recht.,? \\Q2008\\E",
      "shortCiteRegEx" : "Rahimi and Recht.",
      "year" : 2008
    }, {
      "title" : "A stochastic gradient method with an exponential convergence _rate for finite training sets",
      "author" : [ "N.L. Roux", "M. Schmidt", "F.R. Bach" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Roux et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2012
    }, {
      "title" : "Expected Similarity Estimation for Large Scale Anomaly Detection",
      "author" : [ "M. Schneider", "W. Ertel", "G. Palm" ],
      "venue" : "In: International Joint Conference on Neural Networks. IEEE,",
      "citeRegEx" : "Schneider et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schneider et al\\.",
      "year" : 2015
    }, {
      "title" : "Universality, characteristic kernels and RKHS embedding of measures",
      "author" : [ "B.K. Sriperumbudur", "K. Fukumizu", "G.R.G. Lanckriet" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "Sriperumbudur et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sriperumbudur et al\\.",
      "year" : 2011
    }, {
      "title" : "A Hilbert space embedding for distributions",
      "author" : [ "A.J. Smola" ],
      "venue" : "Algorithmic Learning Theory. Springer",
      "citeRegEx" : "Smola,? \\Q2007\\E",
      "shortCiteRegEx" : "Smola",
      "year" : 2007
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for SVM",
      "author" : [ "S. Shalev-Shwartz" ],
      "venue" : "Mathematical Programming. Vol. 127",
      "citeRegEx" : "Shalev.Shwartz,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz",
      "year" : 2011
    }, {
      "title" : "Stochastic methods for l 1-regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "Shalev.Shwartz and Tewari.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Tewari.",
      "year" : 2011
    }, {
      "title" : "Efficient Additive Kernels via Explicit Feature Maps",
      "author" : [ "A. Vedaldi", "A. Zisserman" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on 34.3",
      "citeRegEx" : "Vedaldi and Zisserman.,? \\Q2012\\E",
      "shortCiteRegEx" : "Vedaldi and Zisserman.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Other optimization techniques such as projected gradient decent [BV04] or Nesterov’s accelerated gradient descent [Nes83; Nes04] are also applicable in principle, however a single gradient evaluation takes already O(n) time and hence would be slower than the originally proposed EXPoSE approach.",
      "startOffset" : 64,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "Other optimization techniques such as projected gradient decent [BV04] or Nesterov’s accelerated gradient descent [Nes83; Nes04] are also applicable in principle, however a single gradient evaluation takes already O(n) time and hence would be slower than the originally proposed EXPoSE approach.",
      "startOffset" : 114,
      "endOffset" : 128
    } ],
    "year" : 2015,
    "abstractText" : "A new algorithm named EXPected Similarity Estimation (EXPoSE) was recently proposed to solve the problem of large-scale anomaly detection. It is a non-parametric and distribution free kernel method based on the Hilbert space embedding of probability measures. Given a dataset of n samples, EXPoSE needs only O(n) (linear time) to build a model and O(1) (constant time) to make a prediction. In this work we improve the linear computational complexity and show that an -accurate model can be estimated in constant time, which has significant implications for large-scale learning problems. To achieve this goal, we cast the original EXPoSE formulation into a stochastic optimization problem. It is crucial that this approach allows us to determine the number of iteration based on a desired accuracy , independent of the dataset size n. We will show that the proposed stochastic gradient descent algorithm works in general (possible infinite-dimensional) Hilbert spaces, is easy to implement and requires no additional step-size parameters.",
    "creator" : "LaTeX with hyperref package"
  }
}