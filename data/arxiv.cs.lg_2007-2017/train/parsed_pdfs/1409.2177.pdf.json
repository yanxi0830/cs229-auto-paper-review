{
  "name" : "1409.2177.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Large Margin Mechanism for Differentially Private Maximization",
    "authors" : [ "Kamalika Chaudhuri", "Daniel Hsu", "Shuang Song" ],
    "emails" : [ "kamalika@cs.ucsd.edu,", "djhsu@cs.columbia.edu,", "shs037@eng.ucsd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 9.\n21 77\nv1 [\ncs .L\nG ]\n7 S\nPrevious algorithms for this problem are either range-dependent—i.e., their utility diminishes with the size of the universe—or only apply to very restricted function classes. This work provides the first general-purpose, range-independent algorithm for private maximization that guarantees approximate differential privacy. Its applicability is demonstrated on two fundamental tasks in data mining and machine learning."
    }, {
      "heading" : "1 Introduction",
      "text" : "Differential privacy [17] is a cryptographically-motivated definition of privacy that has recently gained significant attention in the data mining and machine learning communities. An algorithm for processing sensitive data enforces differential privacy by ensuring that the likelihood of any outcome does not change by much when a single individual’s private data changes. Privacy is typically guaranteed by adding noise either to the sensitive data, or to the output of an algorithm that processes the sensitive data. For many machine learning tasks, this leads to a corresponding degradation in accuracy or utility. Thus a central challenge in differentially private learning is to design algorithms with better tradeoffs between privacy and utility for a wide variety of statistics and machine learning tasks.\nIn this paper, we study the private maximization problem, a fundamental problem that arises while designing privacy-preserving algorithms for a number of statistical and machine learning applications. We are given a sensitive dataset D ⊆ Xn comprised of records from n individuals. We are also given a datadependent objective function f : U ×Xn → R, where U is a universe of K items to choose from, and f(i, ·) is (1/n)-Lipschitz for all i ∈ U . That is, |f(i,D′)−f(i,D′′)| ≤ 1/n for all i and for any D′, D′′ ∈ Xn differing in just one individual’s entry. Always selecting an item that exactly maximizes f(·, D) is generally non-private, so the goal is to select, in a differentially private manner, an item i ∈ U with as high an objective f(i,D) as possible. This is a very general algorithmic problem that arises in many applications, include private PAC learning [25] (choosing the most accurate classifier), private decision tree induction [21] (choosing the most informative split), private frequent itemset mining [5] (choosing the most frequent itemset), private validation [12] (choosing the best tuning parameter), and private multiple hypothesis testing [32] (choosing the most likely hypothesis).\nE-mail: kamalika@cs.ucsd.edu, djhsu@cs.columbia.edu, shs037@eng.ucsd.edu\nThe most common algorithms for this problem are the exponential mechanism [28], and a computationally efficient alternative from [5], which we call the max-of-Laplaces mechanism. These algorithms are general— they do not require any additional conditions on f to succeed—and hence have been widely applied. However, a major limitation of both algorithms is that their utility suffers from an explicit range-dependence: the utility deteriorates with increasing universe size. The range-dependence persists even when there is a single clear maximizer of f(·, D), or a few near maximizers, and even when the maximizer remains the same after changing the entries of a large number of individuals in the data. Getting around range-dependence has therefore been a goal for designing algorithms for this problem.\nThis problem has also been addressed by recent algorithms of [3, 31], who provide algorithms that are range-independent and satisfy approximate differential privacy, a relaxed version of differential privacy. However, none of these algorithms is general; they explicitly fail unless additional special conditions on f hold. For example, the algorithm from [31] provides a range-independent result only when there is a single clear maximizer i∗ such that f(i∗, D) is greater than the second highest value by some margin; the algorithm from [3] also has restrictive conditions that limit its applicability (see Section 2.2). Thus, a challenge is to develop a private maximization algorithm that is both range-independent and free of additional conditions; this is necessary to ensure that an algorithm is widely applicable and provides good utility when the universe size is large.\nIn this work, we provide the first such general purpose range-independent private maximization algorithm. Our algorithm is based on two key insights. The first is that private maximization is easier when there is a small set of near maximizing items j ∈ U for which f(j,D) is close to the maximum value maxi∈U f(i,D). A plausible algorithm based on this insight is to first find a set of near maximizers, and then run the exponential mechanism on this set. However, finding this set directly in a differentially private manner is very challenging. Our second insight is that only the number ℓ of near maximizers needs to be found in a differentially private manner – a task that is considerably easier. Provided there is a margin between the maximum value and the (ℓ + 1)-th maximum value of f(i,D), running the exponential mechanism on the items with the top ℓ values of f(i,D) results in approximate differential privacy as well as good utility.\nOur algorithm, which we call the large margin mechanism, automatically exploits large margins when they exist to simultaneously (i) satisfy approximate differential privacy (Theorem 2), as well as (ii) provide a utility guarantee that depends (logarithmically) only on the number of near maximizers, rather than the universe size (Theorem 3). We complement our algorithm with a lower bound, showing that the utility of any approximate differentially private algorithm must deteriorate with the number of near maximizers (Theorem 1). A consequence of our lower bound is that range-independence cannot be achieved with pure differential privacy (Proposition 1), which justifies our relaxation to approximate differential privacy.\nFinally, we show the applicability of our algorithm to two problems from data mining and machine learning: frequent itemset mining and private PAC learning. For the first problem, an application of our method gives the first algorithm for frequent itemset mining that simultaneously guarantees approximate differential privacy and utility independent of the itemset universe size. For the second problem, our algorithm achieves tight sample complexity bounds for private PAC learning analogous to the shell bounds of [26] for non-private learning."
    }, {
      "heading" : "2 Background",
      "text" : "This section reviews differential privacy and introduces the private maximization problem."
    }, {
      "heading" : "2.1 Definitions of Differential Privacy and Private Maximization",
      "text" : "For the rest of the paper, we consider randomized algorithms A : Xn → ∆(S) that take as input datasets D ∈ Xn comprised of records from n individuals, and output values in a range S. Two datasets D,D′ ∈ Xn are said to be neighbors if they differ in a single individual’s entry. A function φ : Xn → R is L-Lipschitz if |φ(D) − φ(D′)| ≤ L for all neighbors D,D′ ∈ Xn.\nThe following definitions of (approximate) differential privacy are from [17] and [20].\nDefinition 1 (Differential Privacy). A randomized algorithmA : Xn → ∆(S) is said to be (α, δ)-approximate differentially private if, for all neighbors D,D′ ∈ Xn and all S ⊆ S,\nPr(A(D) ∈ S) ≤ eα Pr(A(D′) ∈ S) + δ.\nThe algorithm A is α-differentially private if it is (α, 0)-approximate differentially private.\nSmaller values of the privacy parameters α > 0 and δ ∈ [0, 1] imply stronger guarantees of privacy.\nDefinition 2 (Private Maximization). In the private maximization problem, a sensitive dataset D ⊆ Xn comprised of records from n individuals is given as input; there is also a universe U := {1, . . . ,K} of K items, and a function f : U ×Xn → R such that f(i, ·) is (1/n)-Lipschitz for all i ∈ U . The goal is to return an item i ∈ U such that f(i,D) is as large as possible while satisfying (approximate) differential privacy.\nAlways returning the exact maximizer of f(·, D) is non-private, as changing a single individuals’ private values can potentially change the maximizer. Our goal is to design a randomized algorithm that outputs an approximate maximizer with high probability. (We loosely refer to the expected f(·, D) value of the chosen item as the utility of the algorithm.)\nNote that this problem is different from private release of the maximum value of f(·, D); a solution for the latter is easily obtained by adding Laplace noise with standard deviation O(1/(αn)) to maxi∈U f(i,D) [17]. Privately returning a nearly maximizing item itself is much more challenging.\nPrivate maximization is a core problem in the design of differentially private algorithms, and arises in numerous statistical and machine learning tasks. The examples of frequent itemset mining and PAC learning are discussed in Sections 4.1 and 4.2."
    }, {
      "heading" : "2.2 Previous Algorithms for Private Maximization",
      "text" : "The standard algorithm for private maximization is the exponential mechanism [28]. Given a privacy parameter α > 0, the exponential mechanism randomly draws an item i ∈ U with probability pi ∝ enαf(i,D)/2; this guarantees α-differential privacy. While the exponential mechanism is widely used because of its generality, a major limitation is its range-dependence—i.e., its utility diminishes with the universe size K. To be more precise, consider the following example where X := U = [K] and\nf(i,D) := 1\nn |{j ∈ [n] : Dj ≥ i}| (1)\n(where Dj is the j-th entry in the dataset D). When D = (1, 1, . . . , 1), there is a clear maximizer i ∗ = 1, which only changes when the entries of at least n/2 individuals in D change. It stands to reason that any algorithm should report i = 1 in this case with high probability. However, the exponential mechanism outputs i = 1 only with probability enα/2/(K − 1 + enα/2), which is small unless n = Ω(log(K)/α). This implies that the utility of the exponential mechanism deteriorates with K.\nAnother general purpose algorithm is the max-of-Laplaces mechanism from [5]. Unfortunately, this algorithm is also range-dependent. Indeed, our first observation is that all α-differentially private algorithms that succeed on a wide class of private maximization problems share this same drawback.\nProposition 1 (Lower bound for differential privacy). Let A be any α-differentially private algorithm for private maximization, α ∈ (0, 1), and n ≥ 2. There exists a domain X , a function f : U ×Xn → R such that f(i, ·) is (1/n)-Lipschitz for all i ∈ U , and a dataset D ∈ Xn such that:\nPr ( f(A(D), D) > max\ni∈U f(i,D)− log\nK−1 2\nαn\n) < 1\n2 .\nWe remark that results similar to Proposition 1 have appeared in [2, 7, 10, 11, 23]; we simply re-frame those results here in the context of private maximization.\nProposition 1 implies that in order to remove range-dependence, we need to relax the privacy notion. We consider a relaxation of the privacy constraint to (α, δ)-approximate differential privacy with δ > 0.\nThe approximate differentially private algorithm from [31] applies in the case where there is a single clear maximizer whose value is much larger than that of the rest. This algorithm adds Laplace noise with standard deviation O(1/(αn)) to the difference between the largest and the second-largest values of f(·, D), and outputs the maximizer if this noisy difference is larger than O(log(1/δ)/(αn)); otherwise, it outputs Fail. Although this solution has high utility for the example in (1) with D = (1, 1, . . . , 1), it fails even when there is a single additional item j ∈ U with f(j,D) close to the maximum value; for instance, D = (2, 2, . . . , 2).\n[3] provides an approximate differentially private algorithm that applies when f satisfies a condition called ℓ-bounded growth. This condition entails the following: first, for any i ∈ U , adding a single individual to any dataset D can either keep f(i,D) constant, or increase it by 1/n; and second, f(i,D) can only increase in this case for at most ℓ items i ∈ U . The utility of this algorithm depends only on log ℓ, rather than logK. In contrast, our algorithm does not require the first condition. Furthermore, to ensure that our algorithm only depends on log ℓ, it suffices that there only be ≤ℓ near maximizers, which is substantially less restrictive than the ℓ-bounded growth condition.\nAs mentioned earlier, we avoid range-dependence with an algorithm that finds and optimizes over near maximizers of f(·, D). We next specify what we mean by near maximizers using a notion of margin."
    }, {
      "heading" : "3 The Large Margin Mechanism",
      "text" : "We now our new algorithm for private maximization, called the large margin mechanism, along with its privacy and utility guarantees."
    }, {
      "heading" : "3.1 Margins",
      "text" : "We first introduce the notion of margin on which our algorithm is based. Given an instance of the private maximization problem and a positive integer ℓ ∈ N, let f (ℓ)(D) denote the ℓ-th highest value of f(·, D). We adopt the convention that f (K+1)(D) = −∞.\nCondition 1 ((ℓ, γ)-margin condition). For any ℓ ∈ N and γ > 0, we say a dataset D ∈ Xn satisfies the (ℓ, γ)-margin condition if f (ℓ+1)(D) < f (1)(D)− γ (i.e., there are at most ℓ items within γ of the top item according to f(·, D)).1\nBy convention, every dataset satisfies the (K, γ)-margin condition. Intuitively, a (ℓ, γ)-margin condition with a relatively large γ implies that there are ≤ℓ near maximizers, so the private maximization problem is easier when D satisfies an (ℓ, γ)-margin condition with small ℓ.\nHow large should γ be for a given ℓ? The following lower bound suggests that in order to have n = O(log(ℓ)/α), we need γ to be roughly log(ℓ)/(αn).\nTheorem 1 (Lower bound for approximate differential privacy). Fix any α ∈ (0, 1), ℓ > 1, and δ ∈ [0, (1 − exp(−α))/(2(ℓ − 1))]; and assume n ≥ 2. Let A be any (α, δ)-approximate differentially private algorithm, and γ := min{1/2, log((ℓ − 1)/2)/(nα)}. There exists a domain X , a function f : U × Xn → R such that f(i, ·) is (1/n)-Lipschitz for all i ∈ U , and a dataset D ∈ Xn such that:\n1. D satisfies the (ℓ, γ)-margin condition. 2. Pr ( f(A(D), D) > f (1)(D)− γ ) < 1\n2 .\n1Our notion of margins here is different from the usual notion of margins from statistical learning that underlies linear prediction methods like support vector machines and boosting. In fact, our notion is more closely related to the shell decomposition bounds of [26], which we discuss in Section 4.2.\nAlgorithm 1 The large margin mechanism lmm(α, δ,D) input Privacy parameters α > 0 and δ ∈ (0, 1), database D ∈ Xn. output Item I ∈ U . 1: For each r = 1, 2, . . . ,K, let\nt(r) := 6\nn\n( 1 + ln(3r/δ)\nα\n) = O ( 1\nn +\n1\nnα log\nr\nδ\n) ,\nT (r) := 3\nnα ln\n3\n2δ +\n6\nnα ln\n3 δ + 12 nα ln 3r(r + 1) δ + t(r) = O\n( 1\nn +\n1\nnα log\nr\nδ\n) .\n2: Draw Z ∼ Lap(3/α). 3: Let m := f (1)(D) + Z/n. {Estimate of max value.} 4: Draw G ∼ Lap(6/α) and Z1, Z2, . . . , ZK−1 iid∼ Lap(12/α). 5: Let ℓ := 1. {Adaptively determine value ℓ such that D satisfies (ℓ, t(ℓ))-margin condition.} 6: while ℓ < K do 7: if m− f (ℓ+1)(D) > (Zℓ +G)/n+ T (ℓ) then 8: Break out of while-loop with current value of ℓ. 9: else\n10: Let ℓ := ℓ+ 1. 11: end if 12: end while 13: Let Uℓ be the set of ℓ items in U with highest f(i,D) value (ties broken arbitrarily). 14: Draw I ∼ p where pi ∝ 1{i ∈ Uℓ} exp(nαf(i,D)/6). {Exponential mechanism on top ℓ items.} 15: return I.\nA consequence of Theorem 1 is that complete range-independence for all (1/n)-Lipschitz functions f is not possible, even with approximate differential privacy. For instance, if D satisfies an (ℓ,Ω(log(ℓ)/(αn)))margin condition only when ℓ = Ω(K), then nmust be Ω(log(K)/α) in order for an approximate differentially private algorithm to be useful."
    }, {
      "heading" : "3.2 Algorithm",
      "text" : "The lower bound in Theorem 1 suggests the following algorithm. First, privately determine a pair (ℓ, γ), with ℓ is as small as possible and γ = Ω(log(ℓ)/(αn)), such that D satisfies the (ℓ, γ)-margin condition. Then, run the exponential mechanism on the set Uℓ ⊆ U of items with the ℓ highest f(·, D) values. This sounds rather natural and simple, but a knee-jerk reaction to this approach is that the set Uℓ itself depends on the sensitive dataset D, and it may have high sensitivity in the sense that membership of many items in Uℓ can change when a single individual’s private value is changed. Thus differentially private computation of Uℓ appears challenging.\nIt turns out we do not need to guarantee the privacy of the set Uℓ, but rather just of a valid (ℓ, γ) pair. This is essentially because when D satisfies the (ℓ, γ)-margin condition, the probability that the exponential mechanism picks an item i that occurs in Uℓ when the sensitive dataset is D but not in Uℓ when the sensitive dataset is its neighbor D′ is very small.\nMoreover, we can find such a valid (ℓ, γ) pair using a differentially private search procedure based on the sparse vector technique [22]. Combining these ideas gives a general (and adaptive) algorithm whose loss of utility due to privacy is only O(log(ℓ/δ)/αn) when the dataset satisfies a (ℓ, O(log(ℓ/δ)/(αn))-margin condition. We call this general algorithm the large margin mechanism (Algorithm 1), or lmm for short."
    }, {
      "heading" : "3.3 Privacy and Utility Guarantees",
      "text" : "We first show that lmm satisfies approximate differential privacy.\nTheorem 2 (Privacy guarantee). lmm(α, δ, ·) satisfies (α, δ)-approximate differential privacy.\nThe proof of Theorem 2 is in Appendix A. The following theorem, proved in Appendix B, provides a guarantee on the utility of lmm.\nTheorem 3 (Utility guarantee). Pick any η ∈ (0, 1). Suppose D ∈ Xn satisfies the (ℓ∗, γ∗)-margin condition with\nγ∗ = 21\nnα ln\n3 η + T (ℓ ∗).\nThen with probability at least 1− η, I := lmm(α, δ,D) satisfies\nf(I,D) ≥ f (1)(D)− 6 ln(2ℓ ∗/η)\nnα .\n(Above, T (ℓ ∗) is as defined in Algorithm 1.)\nRemark 1. Fix some α, δ ∈ (0, 1). Theorem 3 states that if the dataset D satisfies the (ℓ∗, γ∗)-margin condition, for some positive integer ℓ∗ and γ∗ = C log(ℓ∗/δ)/(nα) for some universal constant C > 0, then the value f(I,D) of the item I returned by lmm is within O(log(ℓ∗)/(nα)) of the maximum, with high probability. There is no explicit dependence on the cardinality K of the universe U ."
    }, {
      "heading" : "4 Illustrative Applications",
      "text" : "We now describe applications of lmm to problems from data mining and machine learning."
    }, {
      "heading" : "4.1 Private Frequent Itemset Mining",
      "text" : "Frequent Itemset Mining (FIM) is the following popular data mining problem: given the purchase lists of users (say, for an online grocery store), the goal is to find the sets of items that are purchased together most often. The work of [5] provides the first differentially private algorithms for FIM. However, as these algorithms rely on the exponential mechanism and the max-of-Laplaces mechanism, their utilities degrade with the total number of possible itemsets. Subsequent algorithms exploit other properties of itemsets or avoid directly finding the most frequent itemset [8, 15, 27, 34].\nLet I be the set of items that can be purchased, and let B be the maximum length of an user’s purchase list. Let U ⊆ 2I be the family of itemsets of interest. For simplicity, we let U := ( I r ) —i.e., all itemsets of size r—and consider the problem of picking the itemset with the (approximately) highest frequency. This is a private maximization problem where D is the users’ lists of purchased items, and f(i,D) is the fraction of users who purchase an itemset i ∈ U . Let fmax be the highest frequency of an itemset in D. Let L be the total number of itemsets with non-zero frequency, so L ≤ n ( B r ) , which is ≪ |I|r whenever B ≪ |I|. Applying lmm gives the following guarantee.\nCorollary 1. Suppose we use lmm(α, δ, ·) on the FIM problem above. Then there exists a constant C > 0 such that the following holds. If fmax ≥ C · log(L/δ)/(nα), then with probability ≥ 1 − δ, the frequency of the itemset Ilmm output by lmm is\nf(Ilmm, D) ≥ fmax −O ( log(L/δ)\nnα\n) .\nIn contrast, the itemset IEM returned by the exponential mechanism is only guaranteed to satisfy\nf(IEM, D) ≥ fmax −O ( r log(|I|/δ)\nnα\n) ,\nwhich is significantly worse than Corollary 1 whenever L ≪ |I|r (as is typically the case). Second, to ensure differential privacy by running the exponential mechanism, one needs a priori knowledge of the set U (and thus the universe of items I) independently of the observed data; otherwise the process will not be end-toend differentially private. In contrast, our algorithm does not need to know I in order to provide end-to-end differential privacy. Finally, unlike [31], our algorithm does not require a gap between the top two itemset frequencies."
    }, {
      "heading" : "4.2 Private PAC Learning",
      "text" : "We now consider private PAC learning with a finite hypothesis class H with bounded VC dimension d [25]. Here, the dataset D consists of n labeled training examples drawn iid from a fixed distribution. The error err(h) of a hypothesis h ∈ H is the probability that it misclassifies a random example drawn from the same distribution. The goal is to return a hypothesis h ∈ H with error as low as possible. A standard procedure that has been well-studied in the literature simply returns the minimizer ĥ ∈ H of the empirical error êrr(h,D) computed on the training data D, but this does not guarantee (approximate) differential privacy. The work of [25] instead uses the exponential mechanism to select a hypothesis hEM ∈ H. With probability ≥ 1− δ0,\nerr(hEM) ≤ min h∈H err(h) +O\n(√ d log(n/δ0)\nn + log |H|+ log(1/δ0) αn\n) . (2)\nThe dependence on log |H| is improved to d log |Σ| by [7] when the data entries come from a finite set Σ. The subsequent work of [4] introduces the notion of representation dimension, and shows how it relates to differentially private learning in the discrete and finite case, and [3] provides improved convergence bounds with approximate differential privacy that exploit the structure of some specific hypothesis classes. For the case of infinite hypothesis classes and continuous data distributions, [10] shows that distribution-free private PAC learning is not generally possible, but distribution-dependent learning can be achieved under certain conditions.\nWe provide a sample complexity bound of a rather different character compared to previous work. Our bound only relies on uniform convergence properties of H, and can be significantly tighter than the bounds from [25] when the number of hypotheses with error close to minh∈H err(h) is small. Indeed, the bounds are a private analogue of the shell bounds of [26], which characterize the structure of the hypothesis class as a function of the properties of a decomposition based on hypotheses’ error rates. In many situation, these bounds are significantly tighter than those that do not involve the error distributions.\nFollowing [26], we divide the hypothesis class H into R = O( √\nn/(d logn)) shells; the t-th shell H(t) is defined by\nH(t) := { h ∈ H : err(h) ≤ min\nh′∈H err(h′) + C0t\n√ d log(n/δ0)\nn\n} .\nAbove, C0 > 0 is the constant from uniform convergence bounds—i.e., C0 is the smallest c > 0 such that for all h ∈ H, with probability ≥ 1 − δ0, we have |êrr(h,D) − err(h)| ≤ c √ d log(n/δ0)/n. Observe that H(t+ 1) ⊆ H(t); and moreover, with probability ≥ 1 − δ0, all h ∈ H(t) have êrr(h,D) ≤ minh′∈H err(h′) + C0 · (t+ 1) √ d log(n/δ0)/n.\nLet t∗(n) as the smallest integer t ∈ N such that log(|H(t+ 1)|) + log(1/δ)\nt ≤ C0α\n√ dn logn\nC\nwhere C > 0 is the constant from Remark 1. Then, with probability ≥ 1−δ0, the dataset D with f = 1− êrr satisfies the (ℓ, γ)-margin condition, with ℓ = |H(t∗(n)+1)| and γ = C log(|H(t∗(n)+1)|/δ)/(αn). Therefore, we have the following guarantee for applying lmm to this problem.\nCorollary 2. Suppose we use lmm(α, δ, ·) on the learning problem above (with U = H and f = 1 − êrr). Then, with probability ≥ 1− δ0 − δ, the hypothesis hlmm returned by lmm satisfies\nerr(hlmm) ≤ min h∈H err(h) +O\n(√ d log(n/δ0)\nn + log(|H(t∗(n) + 1)|/δ) αn\n) .\nThe dependence on log |H| from (2) is replaced here by log(|H(t∗(n)+1)|/δ), which can be vastly smaller, as discussed in [26]."
    }, {
      "heading" : "5 Additional Related Work",
      "text" : "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29]. In particular, algorithms for the private maximization problem (and variants) have been used as subroutines in many applications; examples include PAC learning [25], principle component analysis [14], performance validation [12], and multiple hypothesis testing [32].\nA separation between pure and approximate differential privacy has been shown in several previous works [3, 19, 31]. The first approximate differentially private algorithm that achieves a separation is the Propose-Test-Release (PTR) framework [19]. Given a function, PTR determines an upper bound on its local sensitivity at the input dataset through a search procedure; noise proportional to this upper bound is then added to the actual function value. We note that the PTR framework does not directly apply to our setting as the sensitivity is not generally defined for a discrete universe.\nIn the context of private PAC learning, the work of [3] gives the first separation between pure and approximate differential privacy. In addition to using the algorithm from [31], they devise two additional algorithmic techniques: a concave maximization procedure for learning intervals, and an algorithm for the private maximization problem under the ℓ-bounded growth condition discussed in Section 2.2. The first algorithm is specific to their problem and does not appear to apply to general private maximization problems. The second algorithm has a sample complexity bound of n = O(log(ℓ)/α) when the function f satisfies the ℓ-bounded growth condition.\nLower bounds for approximate differential privacy have been shown by [7, 9, 11, 16], and the proof of our Theorem 1 borrows some techniques from [11]."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this paper, we have presented the first general and range-independent algorithm for approximate differentially private maximization. The algorithm automatically adapts to the available large margin properties of the sensitive dataset, and reverts to worst-case guarantees when such properties are lacking. We have illustrated the applicability of the algorithm in two fundamental problems from data mining and machine learning; in future work, we plan to study other applications where range-independence is a substantial boon.\nAcknowledgments. We thank an anonymous reviewer for suggesting the simpler variant of lmm based on the exponential mechanism. (The original version of lmm used a max of truncated exponentials mechanism, which gives the same guarantees up to constant factors.) This work was supported in part by the NIH under U54 HL108460 and the NSF under IIS 1253942."
    }, {
      "heading" : "A Privacy Analysis",
      "text" : "In this section, we present the proof of Theorem 2. We rely on composition results for approximate differential privacy to analyze the three parts of Algorithm 1:\n• Differential privacy of releasing m after Step 3.\n• Differential privacy of releasing ℓ after Step 12.\n• Approximate differential privacy of releasing I after Step 15.\nWe make this explicit by encapsulating these parts in Algorithm 2 (M), Algorithm 3 (S), and Algorithm 4 (A), so we can write Algorithm 1 as follows (after the definitions of T (r) and t(r)):\n1. m := M(α/3, D).\n2. ℓ := S(α/3,m, T (1), T (2), . . . , T (K−1), D).\n3. I := A(α/3, ℓ,D).\nA.1 max Estimation\nThe first part of Algorithm 1 is a standard application of the Laplace mechanism; it is detailed in Algorithm 2.\nLemma 1 ([17]). M(α, ·) is α-differentially private.\nLemma 2. With probability at least 1− δ,\nM(α,D) ≤ f (1)(D) + 1 nα ln 1 2δ .\nProof. This follows from the tail properties of the Laplace distribution.\nA.2 Certifying the Margin Condition\nThe second part of Algorithm 1 is an application of the “sparse vector technique” to certify the margin condition; it is detailed in Algorithm 3.\nLemma 3. For any m, θ1, θ2, . . . , θK−1 ∈ R, S(α,m, θ1, θ2, . . . , θK−1, ·) is α-differentially private.\nProof. This is an application of the sparse vector technique from [22] that halts as soon as the first “query” is answered positively. We give the privacy analysis for completeness. For clarity, we suppress the dependence of S on all inputs except D, and define F (r+1) := m−f (r+1)−θr, which inherits the (1/n)-Lipschitz property from f (r+1).\nPick any neighboring datasets D and D′, and pick any ℓ ∈ {1, 2, . . . ,K}. We use the notation Pr|G(·) for conditional probabilities where the value of G is fixed, so Pr(·) = E(Pr|G(·)), where the expectation is taken with respect to G. Observe that\nPr|G(S(D) = ℓ) = Pr|G(S(D) ≤ ℓ|S(D) > ℓ− 1) ℓ−1∏\nr=1\nPr|G(S(D) > r|S(D) > r − 1). (3)\nFrom the definition of S and F (r+1),\nPr|G(S(D) > r|S(D) > r − 1) = Pr|G ( F (r+1)(D) ≤ Zr +G\nn\n) ∀r ∈ {1, 2, . . . , ℓ− 1},\nand\nPr|G(S(D) ≤ ℓ|S(D) > ℓ− 1) = Pr|G ( F (ℓ+1)(D) > Zℓ +G\nn\n) .\nWrite Z1:ℓ−1 := (Z1, Z2, . . . , Zℓ−1), and define for any g ∈ R,\nZg(D) := { z ∈ Rℓ−1 : F (r+1)(D) ≤ zr + g\nn ∀r ∈ {1, 2, . . . , ℓ− 1}\n} ,\nso that\nℓ−1∏\nr=1\nPr|G(S(D) > r|S(D) > r − 1) = ℓ−1∏\nr=1\nPr|G\n( F (r+1)(D) ≤ Zr +G\nn\n)\n= Pr|G (Z1:ℓ−1 ∈ ZG(D)) .\nHence, substituting into (3), we have\nPr|G(S(D) = ℓ) = Pr|G\n( F (ℓ+1)(D) > Zℓ +G\nn\n) Pr|G(Z1:ℓ−1 ∈ ZG(D)).\nLetting p denote the density of G, we have the following chain of inequalities:\nPr(S(D) = ℓ) = E(Pr|G(S(D) = ℓ))\n=\n∫ ∞\n−∞\nPr|G\n( F (ℓ+1)(D) > Zℓ + g\nn\n) Pr|G(Z1:ℓ−1 ∈ Zg(D))p(g)dg\n≤ exp(α/2) ∫ ∞\n−∞\nPr|G\n( F (ℓ+1)(D) > Zℓ + g\nn\n) Pr|G(Z1:ℓ−1 ∈ Zg(D))p(g + 1)dg (4)\n= exp(α/2)\n∫ ∞\n−∞\nPr|G\n( F (ℓ+1)(D) >\nZℓ + g − 1 n\n) Pr|G(Z1:ℓ−1 ∈ Zg−1(D))p(g)dg\n≤ exp(α/2) ∫ ∞\n−∞\nPr|G\n( F (ℓ+1)(D) >\nZℓ + g − 1 n\n) Pr|G(Z1:ℓ−1 ∈ Zg(D′))p(g)dg (5)\n≤ exp(α) ∫ ∞\n−∞\nPr|G\n( F (ℓ+1)(D′) > Zℓ + g\nn\n) Pr|G(Z1:ℓ−1 ∈ Zg(D′))p(g)dg (6)\n= exp(α) Pr(S(D′) = ℓ).\nTo prove (4), we use the fact p(g) ≤ exp(α/2)p(g + 1) since p is the Laplace density with scale parameter α/2. To prove (5), observe that for all r ∈ {1, 2, . . . , ℓ− 1}, the (1/n)-Lipschitz property of F (r+1) implies\nF (r+1)(D) ≤ Zr + g − 1 n =⇒ F (r+1)(D′) ≤ Zr + g n .\nThis, in turn, implies Zg−1(D) ⊆ Zg(D′), so (5) follows. To prove (6), we use the following. Observe that\nF (ℓ+1)(D) > Zℓ + g − 1 n =⇒ F (ℓ+1)(D′) > Zℓ + g − 2 n\nby the (1/n)-Lipschitz property of F (ℓ+1). Therefore\nPr|G\n( F (ℓ+1)(D) >\nZℓ + g − 1 n\n) ≤ Pr|G ( F (ℓ+1)(D′) >\nZℓ + g − 2 n\n)\n≤ exp(α/2)Pr|G ( F (ℓ+1)(D′) > Zℓ + g\nn\n)\nwhere we use the fact that Zℓ ∼ Lap(α/4) for the last step, so (6) follows.\nLemma 4. With probability at least 1− δ, if S(α,m, θ1, θ2, . . . , θK−1, D) = r then\nm− f (r+1)(D) > θr − 2\nnα ln\n1 δ − 4 nα ln r(r + 1) δ .\nProof. Using the tail bound for the Laplace distribution,\nPr ( G < − 2\nα ln\n1\nδ\n) ≤ δ\n2\nand\nPr ( Zr < − 4\nα ln\nr(r + 1)\nδ\n) ≤ δ\n2r(r + 1)\nfor each r ∈ {1, 2, . . . ,K − 1}. Therefore, by a union bound, with probability at least 1− δ,\nG ≥ − 2 α ln 1 δ and Zr ≥ − 4 α ln r(r + 1) δ ∀r ∈ {1, 2, . . . ,K − 1}.\nThe claim follows.\nAlgorithm 4 A(α, ℓ,D) input Privacy parameter α > 0, number of items ℓ > 0, database D ∈ Xn. output Item I ∈ U . 1: Let Uℓ be the set of ℓ items in U with highest f(i,D) value, ties broken arbitrarily. 2: Draw I ∼ p where pi ∝ 1{i ∈ Uℓ} exp(nαf(i,D)/2). 3: return I.\nA.3 Restricted Exponential Mechanism\nThe third part of Algorithm 1 uses the exponential mechanism on the top ℓ items to select one of these items; it is detailed in Algorithm 4.\nLemma 5. Assume D satisfies the (ℓ, γ)-margin condition with\nγ ≥ 2 n\n( 1 + ln(ℓ/β)\nα\n) .\nThen for any neighbor D′ ∈ Xn of D, and any S ⊆ U ,\nPr(A(α,D) ∈ S) ≤ exp(α) · Pr(A(α,D′) ∈ S) + β.\nProof. For any r ∈ {1, 2, . . . ,K} and dataset D̃ ∈ Xn, let HD̃ ⊆ U denote the r items of highest f(·, D̃) value (ties broken arbitrarily). (In Algorithm 4, we have Uℓ = HD.) It suffices to show that\nPr(A(α, ℓ,D′) = i) ≤ max {Pr(A(α, ℓ,D) = i) exp(α), β/ℓ} , ∀i ∈ HD′ .\nThis is because Pr(A(α, ℓ,D′) /∈ HD′) = 0 and |HD′ | = ℓ. Fix any i ∈ HD′ . Because f(j, ·) is (1/n)-Lipschitz for every j ∈ U , so is f (r)(·) for every r ∈ [K]. Therefore ℓ∑\nr=1\nexp (nα\n2 f (r)(D′)\n) ≥ ℓ∑\nr=1\nexp (nα\n2 f (r)(D)\n) exp(−α/2).\nAlso by the (1/n)-Lipschitz property,\nexp (nα\n2 f(i,D′)\n) ≤ exp (nα 2 f(i,D) ) exp(α/2).\nTherefore, combining the two displayed equations above gives\nPr(A(α, ℓ,D′) = i) = exp\n( nα 2 f(i,D ′) )\n∑ℓ r=1 exp ( nα 2 f (r)(D′) ) ≤\nexp ( nα 2 f(i,D) ) ∑ℓ\nr=1 exp ( nα 2 f (r)(D) ) exp(α). (7)\nIf i ∈ HD, then (7) reads Pr(A(α, ℓ,D′) = i) ≤ Pr(A(α, ℓ,D) = i) exp(α).\nIf i /∈ HD, then the assumption that D satisfies the (ℓ, γ)-margin condition implies\nf(i,D) ≤ f (1)(D)− γ;\nso combining the above inequality with (7), as well as the assumption γ ≥ (2/n)(1 + ln(ℓ/β)/α), gives\nPr(A(α, ℓ,D′) = i) ≤ exp ( nα 2 ( f (1)(D)− γ ))\nexp ( nα 2 f (1)(D) ) exp(α) ≤ β/ℓ.\nA.4 Privacy of Algorithm 1\nFor clarity, we suppress the privacy parameter inputs to the algorithms. By standard composition results for differential privacy [17], Lemma 1, and Lemma 3, the release ofM(D) and S(M(D), D) is (2α/3)-differentially private. Define the shorthand MS(D) := (M(D), S(M(D), D)), and let µD denote the corresponding probability measure over the range of MS(D).\nFor a dataset D ∈ Xn, let VD be set of (m̃, ℓ̃) pairs (i.e., possible outputs of MS) such that\nm̃ ≤ f (1)(D) + 3 nα ln 3 2δ and m̃− f (ℓ̃+1)(D) > T (ℓ̃) − 12 nα ln 3ℓ̃(ℓ̃+ 1) δ − 6 nα ln 3 δ .\nIf (m, ℓ) ∈ VD, then the values of T (ℓ) and t(ℓ) certify that D satisfies the (ℓ, t(ℓ))-margin condition. Lemma 2 and Lemma 4 imply that\nµD(VD) ≥ 1− 2δ\n3 .\nAlso, observe that if β := δ exp(−2α/3)/3, then\nt(ℓ) = 2\nn\n( 1 + ln(ℓ/β)\nα/3\n) .\nTherefore, for any neighbor D′ ∈ Xn of D, and any S ⊆ U ,\nPr(lmm(D) ∈ S) = ∫ Pr(A(ℓ,D) ∈ S |MS(D) = (m, ℓ))dµD\n≤ ∫\nVD\nPr(A(ℓ,D) ∈ S |MS(D) = (m, ℓ))dµD + 2δ\n3\n≤ ∫\nVD\n( eα/3 Pr(A(ℓ,D′) ∈ S |MS(D) = (m, ℓ)) + β ) e2α/3dµD′ + 2δ\n3\n=\n∫\nVD\n( eα/3 Pr(A(ℓ,D′) ∈ S |MS(D′) = (m, ℓ)) + δe −2α/3\n3\n) e2α/3dµD′ + 2δ\n3\n≤ ∫ ( eα/3 Pr(A(ℓ,D′) ∈ S |MS(D′) = (m, ℓ)) + δe −2α/3\n3\n) e2α/3dµD′ + 2δ\n3\n= eα Pr(lmm(D′) ∈ S) + δ.\nAbove, the second inequality follows from Lemma 5 and the (2α/3)-differential privacy of MS."
    }, {
      "heading" : "B Utility Analysis",
      "text" : "Proof of Theorem 3. Using tail bounds for the Laplace distribution, it follows that with probability at least 1− η/2,\nZ ≥ − 3 α ln 3 η , G ≤ 6 α ln 3 η , Zℓ∗ ≤ 12 α ln 3 η .\nIn this event, the assumption that D satisfies the (ℓ∗, γ∗)-margin condition implies that\n( f (1)(D) + Z/n ) − f (ℓ∗+1)(D) > (Zℓ∗ +G)/n+ T (ℓ ∗),\nso the while-loop terminates with ℓ ≤ ℓ∗. Also, the probability distribution p in Step 14 of Algorithm 1 assigns probability mass at most η/2 to the set of items i with\nf(i,D) ≤ f (1)(D)− 6 ln(2ℓ/η) nα .\nTherefore, by a union bound, the item I returned by Algorithm 1 satisfies\nf(I,D) > f (1)(D)− 6 ln(2ℓ ∗/η)\nnα\nwith probability at least 1− η."
    }, {
      "heading" : "C Proofs of Lower Bounds",
      "text" : "Proof of Theorem 1. We construct the private maximization problem as follows. Let the domain X := 2U (subsets of items), and define f : U × Xn → R by\nf(i,D) := 1\nn\nn∑\ns=1\n1{i ∈ Ds}.\nIn other words, the function f(i, ·) is the fraction of entries containing i. It is easy to see that f(i, ·) is (1/n)-Lipschitz for all i ∈ U .\nLet m := min{n/2, log((ℓ− 1)/2)/α}. We define a collection of ℓ datasets D1, D2, . . . , Dℓ ∈ Xn with the following properties:\n1. For each i, the first n/2 entries of Di are equal to [ℓ] := {1, 2, . . . , ℓ}, the next n/2−m are equal of Di are equal to ∅, and the last m entries of Di are equal to {i}. Therefore\nf(j,Di) =   \n0 if j /∈ [ℓ], 1 2 if j ∈ [ℓ] \\ {i}, 1 2 + m n if j = i,\nso f(i,Di) = f (1)(Di) and Di satisfies the (ℓ,m/n)-margin condition.\n2. For each i 6= j, the datasets Di and Dj differ only in (the last) m entries.\nLet A be (α, δ)-approximate differentially private. Assume for sake of contradiction that\nPr ( f(A(Di), Di) > f (1)(Di)− m\nn\n) ≥ 1\n2\nfor all i ∈ [ℓ]. Since only i satisfies f(i,Di) > f (1)(Di)−m/n, this is the same as Pr(A(Di) = i) ≥ 1/2 for all i ∈ [ℓ]. This then implies the following chain of inequalities leading to a contradiction:\n1 2 > Pr(A(Di) 6= i)\n≥ ∑\nj∈[ℓ]\\{i}\nPr(A(Di) = j)\n≥ ∑\nj∈[ℓ]\\{i}\ne−αm Pr(A(Dj) = j)− δ 1− e−α\n≥ (ℓ − 1) ( e−αm\n2 − δ 1− e−α ) ≥ 1 2 .\nThe first inequality above is by assumption; the third inequality follows from Lemma 6; the fourth inequality again uses the assumption; and the final inequality follows by the definition of m and the condition on δ. Since a contradiction is reached, there must exist some i ∈ [ℓ] such that Pr(f(A(Di), Di) > f (1)(Di)−m/n) < 1/2.\nLemma 6 ([11]). Let D and D′ be any two datasets that differ in at most k entries, and let A be any (α, δ)-approximate differentially private algorithm with range S. Then, for any S ⊆ S,\nPr(A(D) ∈ S) ≥ e−kα Pr(A(D′) ∈ S)− δ 1− e−α ."
    } ],
    "references" : [ {
      "title" : "Private empirical risk minimization, revisited",
      "author" : [ "Raef Bassily", "Adam Smith", "Abhradeep Thakurta" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Bounds on the sample complexity for private learning and private data release",
      "author" : [ "Amos Beimel", "Shiva Prasad Kasiviswanathan", "Kobbi Nissim" ],
      "venue" : "In Theory of Cryptography,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Private learning and sanitization: Pure vs. approximate differential privacy",
      "author" : [ "Amos Beimel", "Kobbi Nissim", "Uri Stemmer" ],
      "venue" : "In RANDOM,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Characterizing the sample complexity of private learners",
      "author" : [ "Amos Beimel", "Kobbi Nissim", "Uri Stemmer" ],
      "venue" : "In ITCS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Discovering frequent patterns in sensitive data",
      "author" : [ "Raghav Bhaskar", "Srivatsan Laxman", "Adam Smith", "Abhradeep Thakurta" ],
      "venue" : "In KDD,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Practical privacy: the SuLQ framework",
      "author" : [ "A. Blum", "C. Dwork", "F. McSherry", "K. Nissim" ],
      "venue" : "In PODS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "A learning theory approach to noninteractive database privacy",
      "author" : [ "Avrim Blum", "Katrina Ligett", "Aaron Roth" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Mining frequent patterns with differential privacy",
      "author" : [ "Luca Bonomi", "Li Xiong" ],
      "venue" : "Proceedings of the VLDB Endowment,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Fingerprinting codes and the price of approximate differential privacy",
      "author" : [ "Mark Bun", "Jonathan Ullman", "Salil Vadhan" ],
      "venue" : "In STOC,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Sample complexity bounds for differentially private learning",
      "author" : [ "Kamalika Chaudhuri", "Daniel Hsu" ],
      "venue" : "In COLT,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Convergence rates for differentially private statistical estimation",
      "author" : [ "Kamalika Chaudhuri", "Daniel Hsu" ],
      "venue" : "In ICML,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "A stability-based validation procedure for differentially private machine learning",
      "author" : [ "Kamalika Chaudhuri", "Staal A Vinterbo" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Differentially private empirical risk minimization",
      "author" : [ "Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Near-optimal differentially private principal components",
      "author" : [ "Kamalika Chaudhuri", "Anand D. Sarwate", "Kaushik Sinha" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Publishing set-valued data via differential privacy",
      "author" : [ "Rui Chen", "Noman Mohammed", "Benjamin CM Fung", "Bipin C Desai", "Li Xiong" ],
      "venue" : "In VLDB,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Lower bounds in differential privacy",
      "author" : [ "Anindya De" ],
      "venue" : "In Ronald Cramer, editor, Theory of Cryptography,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "C. Dwork", "F. McSherry", "K. Nissim", "A. Smith" ],
      "venue" : "In Theory of Cryptography,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2006
    }, {
      "title" : "Differential privacy: A survey of results",
      "author" : [ "Cynthia Dwork" ],
      "venue" : "In Theory and Applications of Models of Computation,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Differential privacy and robust statistics",
      "author" : [ "Cynthia Dwork", "Jing Lei" ],
      "venue" : "In Proceedings of the 41st annual ACM symposium on Theory of computing,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Our data, ourselves: Privacy via distributed noise generation",
      "author" : [ "Cynthia Dwork", "Krishnaram Kenthapadi", "Frank McSherry", "Ilya Mironov", "Moni Naor" ],
      "venue" : "In Advances in Cryptology-EUROCRYPT",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "Data mining with differential privacy",
      "author" : [ "A. Friedman", "A. Schuster" ],
      "venue" : "In KDD,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "A multiplicative weights mechanism for privacy-preserving data analysis",
      "author" : [ "Moritz Hardt", "Guy N Rothblum" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "On the geometry of differential privacy",
      "author" : [ "Moritz Hardt", "Kunal Talwar" ],
      "venue" : "In Proceedings of the 42nd ACM symposium on Theory of computing,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Differentially private online learning",
      "author" : [ "Prateek Jain", "Pravesh Kothari", "Abhradeep Thakurta" ],
      "venue" : "In COLT,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "What can we learn privately",
      "author" : [ "Shiva Prasad Kasiviswanathan", "Homin K Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Computable shell decomposition bounds",
      "author" : [ "John Langford", "David McAllester" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2004
    }, {
      "title" : "Privbasis: frequent itemset mining with differential privacy",
      "author" : [ "Ninghui Li", "Wahbeh Qardaji", "Dong Su", "Jianneng Cao" ],
      "venue" : "In VLDB,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Mechanism design via differential privacy",
      "author" : [ "Frank McSherry", "Kunal Talwar" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2007
    }, {
      "title" : "Signal processing and machine learning with differential privacy: Algorithms and challenges for continuous data",
      "author" : [ "A.D. Sarwate", "K. Chaudhuri" ],
      "venue" : "Signal Processing Magazine, IEEE,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    }, {
      "title" : "Privacy-preserving statistical estimation with optimal convergence rates",
      "author" : [ "Adam Smith" ],
      "venue" : "In STOC,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2011
    }, {
      "title" : "Differentially private feature selection via stability arguments, and the robustness of the lasso",
      "author" : [ "Adam Smith", "Abhradeep Thakurta" ],
      "venue" : "In COLT,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "Privacy-preserving data sharing for genome-wide association studies",
      "author" : [ "Caroline Uhler", "Aleksandra B. Slavkovic", "Stephen E. Fienberg" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2012
    }, {
      "title" : "A statistical framework for differential privacy",
      "author" : [ "Larry Wasserman", "Shuheng Zhou" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Differential privacy [17] is a cryptographically-motivated definition of privacy that has recently gained significant attention in the data mining and machine learning communities.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 24,
      "context" : "This is a very general algorithmic problem that arises in many applications, include private PAC learning [25] (choosing the most accurate classifier), private decision tree induction [21] (choosing the most informative split), private frequent itemset mining [5] (choosing the most frequent itemset), private validation [12] (choosing the best tuning parameter), and private multiple hypothesis testing [32] (choosing the most likely hypothesis).",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : "This is a very general algorithmic problem that arises in many applications, include private PAC learning [25] (choosing the most accurate classifier), private decision tree induction [21] (choosing the most informative split), private frequent itemset mining [5] (choosing the most frequent itemset), private validation [12] (choosing the best tuning parameter), and private multiple hypothesis testing [32] (choosing the most likely hypothesis).",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 4,
      "context" : "This is a very general algorithmic problem that arises in many applications, include private PAC learning [25] (choosing the most accurate classifier), private decision tree induction [21] (choosing the most informative split), private frequent itemset mining [5] (choosing the most frequent itemset), private validation [12] (choosing the best tuning parameter), and private multiple hypothesis testing [32] (choosing the most likely hypothesis).",
      "startOffset" : 260,
      "endOffset" : 263
    }, {
      "referenceID" : 11,
      "context" : "This is a very general algorithmic problem that arises in many applications, include private PAC learning [25] (choosing the most accurate classifier), private decision tree induction [21] (choosing the most informative split), private frequent itemset mining [5] (choosing the most frequent itemset), private validation [12] (choosing the best tuning parameter), and private multiple hypothesis testing [32] (choosing the most likely hypothesis).",
      "startOffset" : 321,
      "endOffset" : 325
    }, {
      "referenceID" : 31,
      "context" : "This is a very general algorithmic problem that arises in many applications, include private PAC learning [25] (choosing the most accurate classifier), private decision tree induction [21] (choosing the most informative split), private frequent itemset mining [5] (choosing the most frequent itemset), private validation [12] (choosing the best tuning parameter), and private multiple hypothesis testing [32] (choosing the most likely hypothesis).",
      "startOffset" : 404,
      "endOffset" : 408
    }, {
      "referenceID" : 27,
      "context" : "The most common algorithms for this problem are the exponential mechanism [28], and a computationally efficient alternative from [5], which we call the max-of-Laplaces mechanism.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "The most common algorithms for this problem are the exponential mechanism [28], and a computationally efficient alternative from [5], which we call the max-of-Laplaces mechanism.",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "This problem has also been addressed by recent algorithms of [3, 31], who provide algorithms that are range-independent and satisfy approximate differential privacy, a relaxed version of differential privacy.",
      "startOffset" : 61,
      "endOffset" : 68
    }, {
      "referenceID" : 30,
      "context" : "This problem has also been addressed by recent algorithms of [3, 31], who provide algorithms that are range-independent and satisfy approximate differential privacy, a relaxed version of differential privacy.",
      "startOffset" : 61,
      "endOffset" : 68
    }, {
      "referenceID" : 30,
      "context" : "For example, the algorithm from [31] provides a range-independent result only when there is a single clear maximizer i such that f(i, D) is greater than the second highest value by some margin; the algorithm from [3] also has restrictive conditions that limit its applicability (see Section 2.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "For example, the algorithm from [31] provides a range-independent result only when there is a single clear maximizer i such that f(i, D) is greater than the second highest value by some margin; the algorithm from [3] also has restrictive conditions that limit its applicability (see Section 2.",
      "startOffset" : 213,
      "endOffset" : 216
    }, {
      "referenceID" : 25,
      "context" : "For the second problem, our algorithm achieves tight sample complexity bounds for private PAC learning analogous to the shell bounds of [26] for non-private learning.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : "The following definitions of (approximate) differential privacy are from [17] and [20].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "The following definitions of (approximate) differential privacy are from [17] and [20].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "Smaller values of the privacy parameters α > 0 and δ ∈ [0, 1] imply stronger guarantees of privacy.",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : ") Note that this problem is different from private release of the maximum value of f(·, D); a solution for the latter is easily obtained by adding Laplace noise with standard deviation O(1/(αn)) to maxi∈U f(i,D) [17].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 27,
      "context" : "2 Previous Algorithms for Private Maximization The standard algorithm for private maximization is the exponential mechanism [28].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "Another general purpose algorithm is the max-of-Laplaces mechanism from [5].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "We remark that results similar to Proposition 1 have appeared in [2, 7, 10, 11, 23]; we simply re-frame those results here in the context of private maximization.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "We remark that results similar to Proposition 1 have appeared in [2, 7, 10, 11, 23]; we simply re-frame those results here in the context of private maximization.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "We remark that results similar to Proposition 1 have appeared in [2, 7, 10, 11, 23]; we simply re-frame those results here in the context of private maximization.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "We remark that results similar to Proposition 1 have appeared in [2, 7, 10, 11, 23]; we simply re-frame those results here in the context of private maximization.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "We remark that results similar to Proposition 1 have appeared in [2, 7, 10, 11, 23]; we simply re-frame those results here in the context of private maximization.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 30,
      "context" : "The approximate differentially private algorithm from [31] applies in the case where there is a single clear maximizer whose value is much larger than that of the rest.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "[3] provides an approximate differentially private algorithm that applies when f satisfies a condition called l-bounded growth.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 25,
      "context" : "In fact, our notion is more closely related to the shell decomposition bounds of [26], which we discuss in Section 4.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 21,
      "context" : "Moreover, we can find such a valid (l, γ) pair using a differentially private search procedure based on the sparse vector technique [22].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "The work of [5] provides the first differentially private algorithms for FIM.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 7,
      "context" : "Subsequent algorithms exploit other properties of itemsets or avoid directly finding the most frequent itemset [8, 15, 27, 34].",
      "startOffset" : 111,
      "endOffset" : 126
    }, {
      "referenceID" : 14,
      "context" : "Subsequent algorithms exploit other properties of itemsets or avoid directly finding the most frequent itemset [8, 15, 27, 34].",
      "startOffset" : 111,
      "endOffset" : 126
    }, {
      "referenceID" : 26,
      "context" : "Subsequent algorithms exploit other properties of itemsets or avoid directly finding the most frequent itemset [8, 15, 27, 34].",
      "startOffset" : 111,
      "endOffset" : 126
    }, {
      "referenceID" : 30,
      "context" : "Finally, unlike [31], our algorithm does not require a gap between the top two itemset frequencies.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 24,
      "context" : "2 Private PAC Learning We now consider private PAC learning with a finite hypothesis class H with bounded VC dimension d [25].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 24,
      "context" : "The work of [25] instead uses the exponential mechanism to select a hypothesis hEM ∈ H.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 6,
      "context" : "The dependence on log |H| is improved to d log |Σ| by [7] when the data entries come from a finite set Σ.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "The subsequent work of [4] introduces the notion of representation dimension, and shows how it relates to differentially private learning in the discrete and finite case, and [3] provides improved convergence bounds with approximate differential privacy that exploit the structure of some specific hypothesis classes.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "The subsequent work of [4] introduces the notion of representation dimension, and shows how it relates to differentially private learning in the discrete and finite case, and [3] provides improved convergence bounds with approximate differential privacy that exploit the structure of some specific hypothesis classes.",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "For the case of infinite hypothesis classes and continuous data distributions, [10] shows that distribution-free private PAC learning is not generally possible, but distribution-dependent learning can be achieved under certain conditions.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 24,
      "context" : "Our bound only relies on uniform convergence properties of H, and can be significantly tighter than the bounds from [25] when the number of hypotheses with error close to minh∈H err(h) is small.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 25,
      "context" : "Indeed, the bounds are a private analogue of the shell bounds of [26], which characterize the structure of the hypothesis class as a function of the properties of a decomposition based on hypotheses’ error rates.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "Following [26], we divide the hypothesis class H into R = O( √ n/(d logn)) shells; the t-th shell H(t) is defined by H(t) := { h ∈ H : err(h) ≤ min h∈H err(h) + C0t √ d log(n/δ0) n } .",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 25,
      "context" : "The dependence on log |H| from (2) is replaced here by log(|H(t∗(n)+1)|/δ), which can be vastly smaller, as discussed in [26].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].",
      "startOffset" : 142,
      "endOffset" : 168
    }, {
      "referenceID" : 5,
      "context" : "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].",
      "startOffset" : 142,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].",
      "startOffset" : 142,
      "endOffset" : 168
    }, {
      "referenceID" : 20,
      "context" : "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].",
      "startOffset" : 142,
      "endOffset" : 168
    }, {
      "referenceID" : 23,
      "context" : "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].",
      "startOffset" : 142,
      "endOffset" : 168
    }, {
      "referenceID" : 29,
      "context" : "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].",
      "startOffset" : 142,
      "endOffset" : 168
    }, {
      "referenceID" : 32,
      "context" : "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].",
      "startOffset" : 142,
      "endOffset" : 168
    }, {
      "referenceID" : 17,
      "context" : "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 28,
      "context" : "There has been a large amount of work on differential privacy for a wide range of statistical and machine learning tasks over the last decade [1, 6, 13, 21, 24, 30, 33]; for overviews, see [18] and [29].",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 24,
      "context" : "In particular, algorithms for the private maximization problem (and variants) have been used as subroutines in many applications; examples include PAC learning [25], principle component analysis [14], performance validation [12], and multiple hypothesis testing [32].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "In particular, algorithms for the private maximization problem (and variants) have been used as subroutines in many applications; examples include PAC learning [25], principle component analysis [14], performance validation [12], and multiple hypothesis testing [32].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 11,
      "context" : "In particular, algorithms for the private maximization problem (and variants) have been used as subroutines in many applications; examples include PAC learning [25], principle component analysis [14], performance validation [12], and multiple hypothesis testing [32].",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 31,
      "context" : "In particular, algorithms for the private maximization problem (and variants) have been used as subroutines in many applications; examples include PAC learning [25], principle component analysis [14], performance validation [12], and multiple hypothesis testing [32].",
      "startOffset" : 262,
      "endOffset" : 266
    }, {
      "referenceID" : 2,
      "context" : "A separation between pure and approximate differential privacy has been shown in several previous works [3, 19, 31].",
      "startOffset" : 104,
      "endOffset" : 115
    }, {
      "referenceID" : 18,
      "context" : "A separation between pure and approximate differential privacy has been shown in several previous works [3, 19, 31].",
      "startOffset" : 104,
      "endOffset" : 115
    }, {
      "referenceID" : 30,
      "context" : "A separation between pure and approximate differential privacy has been shown in several previous works [3, 19, 31].",
      "startOffset" : 104,
      "endOffset" : 115
    }, {
      "referenceID" : 18,
      "context" : "The first approximate differentially private algorithm that achieves a separation is the Propose-Test-Release (PTR) framework [19].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "In the context of private PAC learning, the work of [3] gives the first separation between pure and approximate differential privacy.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 30,
      "context" : "In addition to using the algorithm from [31], they devise two additional algorithmic techniques: a concave maximization procedure for learning intervals, and an algorithm for the private maximization problem under the l-bounded growth condition discussed in Section 2.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "Lower bounds for approximate differential privacy have been shown by [7, 9, 11, 16], and the proof of our Theorem 1 borrows some techniques from [11].",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "Lower bounds for approximate differential privacy have been shown by [7, 9, 11, 16], and the proof of our Theorem 1 borrows some techniques from [11].",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "Lower bounds for approximate differential privacy have been shown by [7, 9, 11, 16], and the proof of our Theorem 1 borrows some techniques from [11].",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 15,
      "context" : "Lower bounds for approximate differential privacy have been shown by [7, 9, 11, 16], and the proof of our Theorem 1 borrows some techniques from [11].",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "Lower bounds for approximate differential privacy have been shown by [7, 9, 11, 16], and the proof of our Theorem 1 borrows some techniques from [11].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "[1] Raef Bassily, Adam Smith, and Abhradeep Thakurta.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Amos Beimel, Kobbi Nissim, and Uri Stemmer.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Amos Beimel, Kobbi Nissim, and Uri Stemmer.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Raghav Bhaskar, Srivatsan Laxman, Adam Smith, and Abhradeep Thakurta.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Avrim Blum, Katrina Ligett, and Aaron Roth.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Luca Bonomi and Li Xiong.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Mark Bun, Jonathan Ullman, and Salil Vadhan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Kamalika Chaudhuri and Daniel Hsu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Kamalika Chaudhuri and Daniel Hsu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Kamalika Chaudhuri and Staal A Vinterbo.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Kamalika Chaudhuri, Claire Monteleoni, and Anand D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Kamalika Chaudhuri, Anand D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Rui Chen, Noman Mohammed, Benjamin CM Fung, Bipin C Desai, and Li Xiong.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Anindya De.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] C.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Cynthia Dwork.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Cynthia Dwork and Jing Lei.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] Moritz Hardt and Guy N Rothblum.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] Moritz Hardt and Kunal Talwar.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] Prateek Jain, Pravesh Kothari, and Abhradeep Thakurta.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[25] Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] John Langford and David McAllester.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[27] Ninghui Li, Wahbeh Qardaji, Dong Su, and Jianneng Cao.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[28] Frank McSherry and Kunal Talwar.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[29] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[30] Adam Smith.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[31] Adam Smith and Abhradeep Thakurta.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[32] Caroline Uhler, Aleksandra B.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[33] Larry Wasserman and Shuheng Zhou.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Lemma 1 ([17]).",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 21,
      "context" : "This is an application of the sparse vector technique from [22] that halts as soon as the first “query” is answered positively.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 16,
      "context" : "By standard composition results for differential privacy [17], Lemma 1, and Lemma 3, the release ofM(D) and S(M(D), D) is (2α/3)-differentially private.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "Lemma 6 ([11]).",
      "startOffset" : 9,
      "endOffset" : 13
    } ],
    "year" : 2014,
    "abstractText" : "A basic problem in the design of privacy-preserving algorithms is the private maximization problem: the goal is to pick an item from a universe that (approximately) maximizes a data-dependent function, all under the constraint of differential privacy. This problem has been used as a sub-routine in many privacy-preserving algorithms for statistics and machine-learning. Previous algorithms for this problem are either range-dependent—i.e., their utility diminishes with the size of the universe—or only apply to very restricted function classes. This work provides the first general-purpose, range-independent algorithm for private maximization that guarantees approximate differential privacy. Its applicability is demonstrated on two fundamental tasks in data mining and machine learning.",
    "creator" : "LaTeX with hyperref package"
  }
}