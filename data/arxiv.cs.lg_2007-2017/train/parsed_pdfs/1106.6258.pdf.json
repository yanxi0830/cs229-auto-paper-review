{
  "name" : "1106.6258.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Note on Improved Loss Bounds for Multiple Kernel Learning",
    "authors" : [ "Zakria Hussain" ],
    "emails" : [ "z.hussain@cs.ucl.ac.uk", "jst@cs.ucl.ac.uk", "mario.marchand@ift.ulaval.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 6.\n62 58"
    }, {
      "heading" : "1 Introduction",
      "text" : "We refer to [4] for the motivation and definitions of multiple kernel learning. It presents a number of results, including a new Rademacher complexity bound on the generalisation error of classifiers learned from a multiple kernel class with a logarithmic dependence on the number of kernels used and with that logarithm entering additively into the bound—that is, independently\nof the complexity of the individual kernels or the margin of the classifier on the training set.\nIn this paper, we follow the approach presented in [4] but correct some of the errors that are present. Unfortunately, the Rademacher complexity risk bound turns out to exhibit a multiplicative dependence on the logarithm of the number of kernels and the margin achieved by the classifier."
    }, {
      "heading" : "2 Detailed proof",
      "text" : ""
    }, {
      "heading" : "2.1 Preliminaries",
      "text" : "Let z = {(xi, yi)}mi=1 be an m-sample where xi ∈ X ⊂ Rn and yi ∈ Y = {−1,+1}, with Z = X ×Y. Let x = {x1, . . . , xm} contain the input vectors. Definition 1 ([1]). A kernel is a function κ that for all x, x′ ∈ X satisfies\nκ ( x, x′ ) = 〈φ(x), φ(x′)〉,\nwhere φ is a mapping from X to an (inner product) Hilbert space H\nφ : X 7→ H.\nKernel learning algorithms [7, 8] make use of the m ×m kernel matrix K = [κ(xi, xi′)] m i,i′=1 defined using the training inputs x. When using the kernel representation it is not always possible to represent the weight vector w explicitly and so we can use the function f directly as the predictor:\nf(x) = m ∑\ni=1\nαiyiκ(xi, x) = 〈w,φ(x)〉,\nwhere α = (α1, . . . , αm) is the dual weight vector and the corresponding norm of the weight vector is\n‖w‖2 = m ∑\ni,j=1\nαiyiαjyjκ(xi, xj).\nGiven a kernel κ, we will use φκ(·) to denote a feature space mapping satisfying κ(x, x′) = 〈φκ(x), φκ(x′)〉. Hence, learning with a kernel κ can be described as finding a function from the class of functions [9]\nFκ = {x 7→ 〈w,φκ(x)〉 | ‖w‖2 ≤ 1, }\nminimising the empirical average of the hinge loss\nhγ(yf(x)) = max\n(\n1− yf(x) γ , 0\n)\n.\nwhere we call γ ∈ [0, 1] the margin. For multiple kernel learning we consider a family of kernels K and the corresponding function class"
    }, {
      "heading" : "FK = {x 7→ 〈w,φκ(x)〉 | ‖w‖2 ≤ 1, for some κ ∈ K} .",
      "text" : "For a distributionD, we use the notation ED[f(x)] to denote the expected value of f(x) when x ∼ D. Given a training set x we denote Ê[f ] to denote its empirical average over the sample x.\nFor the generalisation error bounds we assume that the data are generated iid from a fixed but unknown probability distribution D over the joint space X × Y. Given the true error of a function f :\nerr(f) = E(x,y)∼D(yf(x) ≤ 0) = ED[yf(x)],\nthe empirical margin error of f with margin γ > 0:\nêrrγ(f) = 1\nm\nm ∑\ni=1\nI(yif(xi) < γ) = Ê[I(yif(xi) < γ)] ,\nwhere I is the indicator function, and the estimation error estγ(f) is defined as estγ(f) = |err(f)− êrrγ(f)|, we would like to find an upper bound for estγ(f). In the sequel we will state the bounds in standard form, where the true error err(f) of a function f is upper bounded by the empirical margin error êrrγ(f) plus the estimation error estγ(f):\nerr(f) ≤ êrrγ(f) + estγ(f). (1)\nWe further consider the clipped hinge function:\nAγ(s) =\n\n\n 0; if s ≥ γ 1− s/γ; if 0 ≤ s ≤ γ; 1; otherwise,\nand its empirical estimation Ê[Aγ(yf(x))]. Note that err(f) ≤ ED[Aγ(yf(x))], Ê[Aγ(yf(x))] ≤ êrrγ(f) and Ê[Aγ(yf(x))] ≤ Ê[hγ(yf(x))).\nLet K = {κ1, . . . , κp} denote a family of kernels, where each kernel κj is called the jth base kernel. The following kernel family is formed using a convex combination of base kernels:\nKcon(κ1, . . . , κp) =\n\n\n\nκλ =\np ∑\nj=1\nλjκj | λj ≥ 0, p ∑\nj=1\nλj = 1\n\n\n\n.\nNote, p is the complexity of the kernel family (i.e., cardinality of the set of base kernels)."
    }, {
      "heading" : "2.2 Rademacher complexity bound for MKL",
      "text" : "In this section we correct the MKL risk bound of [4]. We begin by the following definition of Rademacher complexity.\nDefinition 2 (Rademacher complexity). For a sample x = {x1, . . . , xm} generated by a distribution DX on a set X and a real-valued function class F with domain X , the empirical Rademacher complexity of F is the random variable\nR̂m(F) = Eσ [\nsup f∈F\n2\nm\nm ∑\ni=1\nσif(xi) | x1, . . . , xm ] .\nwhere σ = (σ1, . . . , σm) are independent uniform {±1}-valued (Rademacher) random variables. The (true) Rademacher complexity is:\nRm(F) = Ex [ R̂m(F) ] = Exσ\n[\nsup f∈F\n2\nm\nm ∑\ni=1\nσif(xi)\n]\n.\nThe standard Rademacher bound for function classes is given in the following theorem.\nTheorem 1 ([3]). Fix δ ∈ (0, 1), and let F be a class of functions mapping from Z = X ×Y to [0, 1]. Let z = {zi}mi=1 be drawn independently according to a probability distribution D. Then with probability 1 − δ over random draws of samples of size m, every f ∈ F satisfies\nED(f) ≤ Ê(f) + R̂m(F) + 3 √ ln(2/δ)\n2m .\nWe have attributed this bound to [3], though, strictly speaking, they used the slightly weaker version of Rademacher complexity including an absolute value of the sum. This version is obtained by a slight tightening of the argument. This bound is quite general and applicable to various learning algorithms if a tight upper bound of empirical Rademacher complexity R̂m(F) of the function class F can be found. For kernel methods, a wellknown result uses the trace of the kernel matrix to bound the empirical Rademacher complexity.\nTheorem 2 ([3]). If κ : X ×X 7→ R is a kernel, and x = {x1, . . . , xm} is a sample of points from X , then the empirical Rademacher complexity of the class Fκ satisfies\nR̂m(Fκ) ≤ 2\nm\n√ √ √ √ m ∑\ni=1\nκ(xi, xi).\nFurthermore, if R2 ≥ κ(x, x) for all x ∈ X and κ is a normalised kernel such that\n∑m i=1 κ(xi, xi) = m, then we have\n2\nm\n√ √ √ √ m ∑\ni=1\nκ(xi, xi) ≤ 2R√ m .\nThe problem of learning kernels from a convex combination of base kernels is related to using the convex hull of a set of functions. Consider\ncon(F) =\n \n\n∑\nj\najfj | fj ∈ F , aj ≥ 0, ∑\nj\naj ≤ 1\n \n\n. (2)\nSince adding kernels corresponds to concatenating feature spaces, it is clear that (here wj is the restriction of w to the feature space defined by the mapping φκj (·) corresponding to kernel κj)\nFKcon(κ1,...,κp) =\n\n\n\nx 7→ 〈w,φκ(x)〉 | ‖w‖2 ≤ 1, κ = p ∑\nj=1\nλjκj ,\np ∑\nj=1\nλj = 1\n\n\n\n=\n\n\n\nx 7→ p ∑\nj=1\n√ λj‖wj‖ 〈 wj ‖wj‖ , φκj (x) 〉 | ‖w‖2 ≤ 1, p ∑\nj=1\nλj = 1\n\n\n\n= con\n\n\np ⋃\nj=1\nFκj\n\n , (3)\nsince, by the Cauchy Schwartz inequality, we have\np ∑\nj=1\n√\nλj‖wj‖ ≤\n√ √ √ √ p ∑\nj=1\nλj\n√ √ √ √ p ∑\nj=1\n‖wj‖2 ≤ 1.\nHence, we are interested in the empirical Rademacher complexity of a convex hull as given by Equation (2), which is well known to satisfy\nR̂m(con(F)) = R̂m(F) . (4)\nFurthermore, following [5] and [2], we have the following result.\nTheorem 3 ([5]). The empirical Rademacher complexity of the function class L(F) where L(·) is Lipschitz function with Lipschitz constant L is bounded by\nR̂m(L(F)) ≤ LR̂m(F).\nGiven all these results, we are now in a position to state the following theorem, which proves a high probability upper bound for the empirical Rademacher complexity of a union of function classes\n⋃p j=1Fj = F .\nTheorem 4. Let x = {x1, . . . , xm} be an m-sample of points from X , then the empirical Rademacher complexity R̂m of the class F = ∪pj=1Fj, where the range of all the functions in F is [0, 1], satisfies:\nR̂m(F) ≤ max 1≤j≤p\nR̂m(Fj) + √ 8 ln(p)\nm .\nProof. Since F is the union of p function classes, we have\nR̂m(F) = Eσ max 1≤j≤p sup f∈Fj\n2\nm\nm ∑\ni=1\nσif(xi) .\nFrom Jensen’s inequality, we have, for any λ ≥ 0, that\nexp ( λR̂m(F) ) ≤ Eσ exp ( λ [\nmax 1≤j≤p sup f∈Fj\n2\nm\nm ∑\ni=1\nσif(xi)\n])\n= Eσ max 1≤j≤p exp\n(\nλ\n[\nsup f∈Fj\n2\nm\nm ∑\ni=1\nσif(xi)\n])\n≤ p ∑\nj=1\nEσ exp\n(\nλ\n[\nsup f∈Fj\n2\nm\nm ∑\ni=1\nσif(xi)\n])\n. (5)\nNow, for any fixed function class Fj and any fixed training sample, let\nξ(σ1, . . . , σm) def = sup\nf∈Fj\n2\nm\nm ∑\ni=1\nσif(xi) .\nA basic result of McDiarmid [6] states that for any λ ≥ 0, we have\nEeλξ ≤ eλ 2 8 ∑m i=1 c 2 i · eλEξ ,\nwhere, for all i, we have\nsup σ1,...,σm,σ̂i\n|ξ(σ1, . . . , σm)− ξ(σ1, . . . , σi−1, σ̂i, σi+1, . . . , σm)| ≤ ci .\nIn our case, we have that ci ≤ 4/m ∀i ∈ {1, . . . ,m}. Hence, from Equation (5), we have\nexp ( λR̂m(F) ) ≤ e2λ2/m p ∑\nj=1\neλR̂m(Fj) .\nBy taking the logarithm on both sides of this equation, we obtain\nλR̂m(F) ≤ 2λ2\nm + ln\n\n\np ∑\nj=1\neλR̂m(Fj )\n\n\n≤ 2λ 2\nm + ln\n[\np · max 1≤j≤p\neλR̂m(Fj) ]\n≤ 2λ 2\nm + ln(p) + max 1≤j≤p λR̂m(Fj) .\nHence, we have\nR̂m(F) ≤ 2λ\nm +\n1 λ ln(p) + max 1≤j≤p R̂m(Fj) .\nThe theorem then follows from this equation by choosing\nλ =\n√\nm\n2 ln p .\nRecall the function Aγ(·) and the properties err(f) ≤ ED[Aγ(yf(x))] and E[Aγ(yf(x))] ≤ errγ(f). Therefore we have the following generalization error bound for MKL in the case of a convex combination of kernels.\nTheorem 5. Fix γ > 0 and δ ∈ (0, 1). Let K = {κ1, . . . , κp} be a family of kernels containing p base kernels and let z = {zi}mi=1 be a randomly generated sample from distribution D. Then with probability 1 − δ over the random draws of samples of size m, every f ∈ FKcon satisfies\nerr(f) ≤ Ê[Aγ(yf(x))] + 1 γ\n\nmax 1≤j≤p\n2\nm\n√ √ √ √ m ∑\ni=1\nκj(xi, xi) +\n√\n8 ln p\nm\n\n\n+ 3\n√\nln(2/δ)\n2m .\nAlso, if each kernel κj is normalised and bounded by R 2 ≥ κj(x, x) for all x ∈ X and j ∈ {1, . . . , p}, we have\nerr(f) ≤ Ê[Aγ(yf(x))] + 1 γ\n[\n2R√ m +\n√\n8 ln p\nm\n]\n+ 3\n√\nln(2/δ)\n2m .\nProof. Each kernel κj defines the class Fj = {x 7→ 〈w,φκj (x)〉 : ‖w‖ ≤ 1}. Hence, applying Theorem 1 to the class Aγ(FK) = {Aγ ◦ f : f ∈ FK}, we have\nerr(f) ≤ ED[Aγ(yf(x))]\n≤ Ê[Aγ(yf(x))] + R̂m(Aγ(FKcon)) + 3 √ ln(2/δ)\n2m\n≤ Ê[Aγ(yf(x))] + 1 γ R̂m(FKcon) + 3\n√\nln(2/δ)\n2m\n= Ê[Aγ(yf(x))] + 1 γ R̂m\n\n\np ⋃\nj=1\nFκj\n\n+ 3\n√\nln(2/δ)\n2m\n≤ Ê[Aγ(yf(x))] + 1 γ\n[\nmax 1≤j≤p\nR̂m(Fj) + √ 8 ln p\nm\n]\n+ 3\n√\nln(2/δ)\n2m\n≤ Ê[Aγ(yf(x))] + 1 γ\n\nmax 1≤j≤p\n2\nm\n√ √ √ √ m ∑\ni=1\nκj(xi, xi) +\n√\n8 ln p\nm\n\n+ 3\n√\nln(2/δ)\n2m\n≤ Ê[Aγ(yf(x))] + 1 γ\n[\nmax 1≤j≤p\n2\nm\n√ mR2 + √ 8 ln p\nm\n]\n+ 3\n√\nln(2/δ)\n2m\n= Ê[Aγ(yf(x))] + 1 γ\n[\n2R√ m +\n√\n8 ln p\nm\n]\n+ 3\n√\nln(2/δ)\n2m ,\nwhere the third line comes from applying Theorem 3 with with Lipschitz constant L = 1/γ. The forth line comes by applying Equation (4). The fifth line comes by applying Theorem 4. The 6th line follows from Theorem 2. Finally, the 7th line follows from the hypothesis that κj(x, x) ≤ R2 ∀x."
    }, {
      "heading" : "3 Discussion",
      "text" : "Using the notation from above, the un-normalized version of the bound of Theorem 8 of [4] is\nerr(f) ≤ êrrγ(f) + 2 γm max 1≤j≤p\n√ √ √ √ m ∑\ni=1\nκj(xi, xi) + 5\n√\nln((p + 3)/δ)\n2m .\nComparing this to Theorem 5 (the corrected version), we can see that the major difference is the fact that 1/γ is multiplying ln p in Theorem 5, while it is not in the Theorem 8 of [4]. However, the latter was obtained by incorrectly assuming that R̂m(Aγ(con(F))) is upper bounded by R̂m(con(Aγ(F))).\nWhile Theorem 2 of [4] shows an additive dependence on the logarithm of the number of kernels it has an additional term that includes the number of kernels d involved in the final solution and this number is also multiplied by the logarithm of the number of kernels. However, these quantities are separate from the main margin complexity term. A similar result could be obtained for the Rademacher bound given here resulting in a partial independence between the complexity and number of kernel terms, but with the final number of active kernels entering as an additional complexity term.\nAcknowldgements\nWe thank an anonymous reviewer from the Journal of Machine Learning Research who pointed out the flaw in the original proof of Theorem 5."
    } ],
    "references" : [ {
      "title" : "Theoretical foundations of the potential function method in pattern recognition learning",
      "author" : [ "M. Aizerman", "E. Braverman", "L. Rozonoer" ],
      "venue" : "Automation and Remote Control, 25:821 – 837",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "Complexity of pattern classes and lipschitz property",
      "author" : [ "A. Ambroladze", "J. Shawe-Taylor" ],
      "venue" : "Algorithmic Learning Theory, volume 3244 of Lecture Notes in Computer Science, pages 181–193. Springer",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Rademacher and gaussian complexities: risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "Journal of Machine Learning Research, 3:463–482",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Improved loss bounds for multiple kernel learning",
      "author" : [ "Z. Hussain", "J. Shawe-Taylor" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Rademacher composition",
      "author" : [ "S. Kakade", "A. Tewari" ],
      "venue" : "CMSC 35900 Learning Theory, pages 1–22",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "On the method of bounded differences",
      "author" : [ "C. McDiarmid" ],
      "venue" : ". L. M. S. L. N. Series, editor, Surveys in Combinatorics 1989, pages 148–188. Cambridge University Press, Cambridge",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Learning with Kernels",
      "author" : [ "B. Schölkopf", "A. Smola" ],
      "venue" : "MIT Press, Cambridge, MA",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : "Cambridge University Press, Cambridge, U.K.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning bounds for support vector machines with learned kernels",
      "author" : [ "N. Srebro", "S. Ben-David" ],
      "venue" : "Computational Learning Theory, volume 4005 of Lecture Notes in Computer Science, pages 169–183. Springer",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Abstract In this paper, we correct an upper bound, presented in [4], on the generalisation error of classifiers learned through multiple kernel learning.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "The bound in [4] uses Rademacher complexity and has anadditive dependence on the logarithm of the number of kernels and the margin achieved by the classifier.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "1 Introduction We refer to [4] for the motivation and definitions of multiple kernel learning.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "In this paper, we follow the approach presented in [4] but correct some of the errors that are present.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "Definition 1 ([1]).",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 6,
      "context" : "Kernel learning algorithms [7, 8] make use of the m ×m kernel matrix K = [κ(xi, xi′)] m i,i=1 defined using the training inputs x.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "Kernel learning algorithms [7, 8] make use of the m ×m kernel matrix K = [κ(xi, xi′)] m i,i=1 defined using the training inputs x.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "Hence, learning with a kernel κ can be described as finding a function from the class of functions [9] Fκ = {x 7→ 〈w,φκ(x)〉 | ‖w‖2 ≤ 1, } 2",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "where we call γ ∈ [0, 1] the margin.",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "2 Rademacher complexity bound for MKL In this section we correct the MKL risk bound of [4].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "Theorem 1 ([3]).",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "Fix δ ∈ (0, 1), and let F be a class of functions mapping from Z = X ×Y to [0, 1].",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "We have attributed this bound to [3], though, strictly speaking, they used the slightly weaker version of Rademacher complexity including an absolute value of the sum.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "Theorem 2 ([3]).",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "(4) Furthermore, following [5] and [2], we have the following result.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "(4) Furthermore, following [5] and [2], we have the following result.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "Theorem 3 ([5]).",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : ", xm} be an m-sample of points from X , then the empirical Rademacher complexity R̂m of the class F = ∪pj=1Fj, where the range of all the functions in F is [0, 1], satisfies: R̂m(F) ≤ max 1≤j≤p R̂m(Fj) + √ 8 ln(p) m .",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 5,
      "context" : "A basic result of McDiarmid [6] states that for any λ ≥ 0, we have Ee ≤ eλ 2 8 ∑m i=1 c 2 i · e , where, for all i, we have sup σ1,.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "3 Discussion Using the notation from above, the un-normalized version of the bound of Theorem 8 of [4] is err(f) ≤ êrr(f) + 2 γm max 1≤j≤p √ √",
      "startOffset" : 99,
      "endOffset" : 102
    } ],
    "year" : 2014,
    "abstractText" : "In this paper, we correct an upper bound, presented in [4], on the generalisation error of classifiers learned through multiple kernel learning. The bound in [4] uses Rademacher complexity and has anadditive dependence on the logarithm of the number of kernels and the margin achieved by the classifier. However, there are some errors in parts of the proof which are corrected in this paper. Unfortunately, the final result turns out to be a risk bound which has a multiplicative dependence on the logarithm of the number of kernels and the margin achieved by the classifier.",
    "creator" : "LaTeX with hyperref package"
  }
}