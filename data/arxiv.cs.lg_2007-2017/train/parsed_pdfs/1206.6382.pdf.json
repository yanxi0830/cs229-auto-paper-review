{
  "name" : "1206.6382.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "High-Dimensional Covariance Decomposition into Sparse Markov and Independence Domains",
    "authors" : [ "Majid Janzamin", "Animashree Anandkumar" ],
    "emails" : [ "mjanzami@uci.edu", "a.anandkumar@uci.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: High-dimensional covariance estimation, sparse graphical model selection, sparse covariance\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nmodels, sparsistency, convex optimization."
    }, {
      "heading" : "1. Introduction",
      "text" : "Covariance estimation is a classical problem in multivariate statistics. The idea that second-order statistics capture important and relevant relationships between a given set of variables is natural. Finding the sample covariance matrix based on observed data is straightforward and widely used (Anderson, 1984). However, the sample covariance matrix is ill-behaved in highdimensions, where the number of dimensions p is typically much larger than the number of available samples n (p n). Here, the problem of covariance estimation is ill-posed since the number of unknown parameters is larger than the number of available samples, and the sample covariance matrix becomes singular in this regime.\nVarious solutions have been proposed for highdimensional covariance estimation. Intuitively, by restricting the class of covariance models to those with a limited number of free parameters, we can successfully estimate the models in high dimensions. A natural mechanism to achieve this is to impose a sparsity constraint on the covariance matrix, which implies that the variables under consideration satisfy marginal independence, corresponding to the zero pattern of the covariance matrix (Kauermann, 1996) (and we refer to such models as independence models). In many settings, however, marginal independence is too restrictive and does not hold. For instance, consider the dependence between the monthly stock returns of various companies listed on the S&P 100 index. It is quite possible that a wide range of complex (and unobserved) factors such as the economic climate, interest rates etc., affect the returns of all the companies. Thus, it is not realistic to model the stock returns of various companies through a sparse covariance model.\nA popular alternative sparse model, based on conditional independence relationships, has gained\nwidespread acceptance in recent years (Lauritzen, 1996). In this case, sparsity is imposed not on the covariance matrix, but on the inverse covariance or the precision matrix. It can be shown that the zero pattern of the precision matrix corresponds to a set of conditional-independence relationships and such models are referred to as graphical or Markov models. Going back to the stock market example, a first-order approximation is to model the companies in different divisions1 as conditionally independent given the S&P 100 index variable, which captures the overall trends of the stock returns, and thus removes much of the dependence between the companies in different divisions. However, sparse Markov models may not be always sufficient to capture all the relationships between the variables. Going back to the stock market example, the approximation of using the S&P index node to capture the dependence between companies of different divisions may not be enough. For instance, there can still be a large residual dependence between the companies in manufacturing and mining divisions, which cannot be accounted by the S&P index node.\nIn this paper, we make the above notion precise, and model the variables as a linear combination of samples from an independence and a Markov model. In other words, the covariance matrix of the resulting model is a linear combination of a sparse covariance and a sparse precision matrix, see Fig.1. This forms a richer class of models which can faithfully capture complex relationships, such as in the stock market example above, and yet retain parsimony in the representation.\nSummary of Contributions\nWe consider joint estimation of Markov and independence models, given observed data in a high dimensional setting. Our contributions in this paper are three fold. First, we derive a set of sufficient restrictions, under which there is a unique decomposition into the two domains, viz., the Markov and the independence domains, thereby leading to an identifiable\n1 See http://www.osha.gov/pls/imis/sic_manual.html for classifi-\ncations of the companies.\nmodel. Second, we propose novel and efficient estimators for obtaining the decomposition, under both exact and sample statistics. Third, we provide strong theoretical guarantees for high-dimensional learning, both in terms of norm guarantees and sparsistency in each domain, viz., the Markov and the independence domain.\nOur learning method is based on convex optimization. We adapt the popular `1-penalized maximum likelihood estimator (MLE), proposed originally for sparse Markovmodel selection. This estimator is widely used, and theoretical guarantees on consistent estimation have been proven. Here, an `1 penalty is imposed on the precision matrix, which is a convex relaxation of the `0 penalty, in order to encourage sparsity in the precision matrix. It is well known that the Lagrangian dual of this program is a maximum entropy solution which approximately fits the given sample covariance matrix. We modify this program to our setting as follows: we incorporate an additional `1 penalty term involving the residual covariance matrix (corresponding to the independence model) in the max-entropy program. This term can be viewed as encouraging sparsity in the independence domain, while fitting a maximum entropy Markov model to the rest of the sample correlations (after incorporating the independence model). We characterize the optimal solution of the above program, and also provide intuitions on the class of Markov and independence model combinations which can be incorporated under this framework. As a byproduct of this analysis, we obtain a set of conditions for identifiability of the two model components.\nWe provide strong theoretical guarantees for our proposed method under a set of sufficient conditions. We establish that it is possible to obtain sparsistency and norm guarantees in both the Markov and the independence domains, which is somewhat surprising. We establish that the number of samples n is required to scale as n = Ω(d2 log p) for consistency, where p is the number of variables, and d is the maximum degree in the Markov graph. The set of sufficient conditions for successful recovery are based on the so-called notion of mutual incoherence, which controls the dependence between different sets of variables, See (Ravikumar et al., 2011). Our conditions are similar to those previously characterized for consistent graphical model selection, and are only slightly stronger. Our consistency proofs borrow ideas from (Ravikumar et al., 2011), and at the same time, require new ideas to carefully control the errors in the two domains, viz., the Markov and the independence domains. This is because the proposed optimization method only ensures that the overall combination of the Markov and the independence\nmodels is close to the sample covariance matrix, and does not limit the individual perturbations in the two domains. We consider a careful partitioning of the variables, and impose a set of mutual incoherence conditions, and provide consistency guarantees in high dimensions.\nThe idea that a combination of Markov and independence models can provide good model-fitting is not by itself new, see (Choi et al., 2010). However, the previous approach has several deficiencies, including lack of theoretical guarantees, assumption of a known sparsity support for the Markov model, use of expectation maximization (EM) which has convergence issues, and so on. In contrast, we develop convex optimization methods for decomposition , and also provide theoretical guarantees for successful recovery. In summary, in this paper, we provide an in-depth study of efficient methods and guarantees for joint estimation of a combination of Markov and independence models.\nOur experiments validate our theoretical results and demonstrate that our method is able to learn a richer class of models, compared to sparse graphical model selection, while requiring similar number of samples. In particular, our method is able to provide better estimates for the overall precision matrix, which is dense in general, while the performance of `1-based optimization is worse since it attempts to approximate the dense matrix via a sparse estimate. Additionally, we demonstrate that our estimated models have better accuracy under simple distributed inference algorithms such as loopy belief propagation (LBP). This is because the Markov components of the estimated models tend to be more walk summable (Malioutov et al., 2006), since some of the correlations can be “transferred” to the residual matrix. Thus, in addition to learning a richer model class, incorporating sparsity in both covariance and precision domains, we also learn models amenable to efficient inference."
    }, {
      "heading" : "2. Background and Problem Statement",
      "text" : "Notation: For any vector v ∈ Rp and a real number a ∈ [1,∞), the notation ‖v‖a refers to the `a norm of vector v given by ‖v‖a := (∑p i=1 |vi| a ) 1\na . For any matrix U ∈ Rp×p, the operator norm is given by |||U |||a,b := max‖z‖a=1 ‖Uz‖b for parameters a, b ∈ [1,∞). Specifically, we use the `∞ operator norm which is equivalent to |||U |||∞ = maxi=1,...,p ∑p j=1 |Uij |. We also have |||U |||1 = |||U T |||∞. We also use the `∞ element-wise norm notation ‖U‖∞ to refer to the maximum absolute value of the entries of matrix U . The trace inner product of two matrices is denoted by 〈U, V 〉 :=\nTr(UTV ) = ∑\ni,j UijVij . Finally, we use the usual notation for asymptotics: f(n) = Ω(g(n)) if f(n) ≥ cg(n) for some constant c > 0 and f(n) = O(g(n)) if f(n) ≤ c′g(n) for some constant c′ < ∞."
    }, {
      "heading" : "2.1. Gaussian Graphical Models",
      "text" : "A Gaussian graphical model is a family of jointly Gaussian distributions which factor in accordance to a given graph. Given a graph G = (V,E), with V = {1, . . . , p}, consider a vector of Gaussian random variables X = [X1, X2, . . . , Xp], where each node i ∈ V is associated with a scalar Gaussian random variable Xi. A Gaussian graphical model Markov on G has a probability density function (pdf) that may be parameterized as fX(x) ∝ exp [ − 12x T Jx+ hTx ] , where J is a positive-definite symmetric matrix whose sparsity pattern corresponds to that of the graph G. More precisely, J(i, j) = 0 iff (i, j) /∈ G. The matrix J is known as the potential or concentration matrix, the non-zero entries J(i, j) as the edge potentials, and the vector h as the potential vector. This parameterization is known as the information form and is related to the standard mean-covariance parameterization of the Gaussian as µ = J−1h, Σ = J−1, where µ := E[X] is the mean vector and Σ := E[(X − µ)(X − µ)T ] is the covariance matrix.\nWe say that a jointly Gaussian random vector X with joint pdf f(x) satisfies local Markov property with respect to a graph G if f(xi|xN (i)) = f(xi|xV \\i) holds for all nodes i ∈ V , where N (i) denotes the set of neighbors of node i ∈ V and, V \\ i denotes the set of all nodes excluding i. More generally, we say that X satisfies the global Markov property, if for all disjoint sets A,B ⊂ V , we have\nf(xA,xB |xS) = f(xA|xS)f(xB |xS). (1)\nwhere set S is a separator2 of A and B. The local and global Markov properties are equivalent for nondegenerate Gaussian distributions (Lauritzen, 1996).\nIn this paper, we consider models which are characterized by a combination of Markov and independence graphs. In particular, we model the covariance matrix as a linear combination of Markov and independence models:\nΣ∗ = J∗M −1+Σ∗R, Supp(J ∗ M ) = GM , Supp(Σ ∗ R) = GR, (2) where Supp(·) denotes the set of non-zero (offdiagonal) entries, GM denotes the Markov graph and GR, the independence graph.\n2 A set S ⊂ V is a separator for sets A and B if the removal of\nnodes in S partitions A and B into distinct components."
    }, {
      "heading" : "2.2. Problem Statement",
      "text" : "We now give a detailed description of our problem statement, which consists of the covariance decomposition problem (given exact statistics) and covariance estimation problem (given a set of samples).\nCovariance Decomposition Problem: A fundamental question to be addressed is the identifiability of the model parameters.\nDefinition 1 (Identifiability). A parametric model {Pθ : θ ∈ Θ} is identifiable with respect to a measure µ if there do not exist two distinct parameters θ1 6= θ2 such that Pθ1 = Pθ2 almost everywhere w.r.t. to µ.\nThus, if a model is not identifiable, there is no hope of estimating the model parameters from observed data. A Gaussian graphical model (with no hidden variables) belongs to the family of standard exponential distributions (Wainwright & Jordan, 2008). Under nondegeneracy conditions, it is also in the minimal form, and as such is identifiable (Brown, 1986). In our setting in (2), however, identifiability is not straightforward to address, and forms an important part of the covariance decomposition problem, described below.\nDecomposition Problem: Given the covariance matrix Σ∗ = J∗M −1 + Σ∗R as in (2), where J ∗ M is an unknown concentration matrix and Σ∗R is an unknown residual covariance matrix, how and under what conditions can we uniquely recover J∗M and Σ ∗ R from Σ ∗?\nIn other words, we want to address whether the matrices J∗M and Σ ∗ R are identifiable, given Σ\n∗, and if so, how can we design efficient methods to recover them. If we do not impose any additional restrictions, there exists an equivalence class of models which form solutions to the decomposition problem. For instance, we can model Σ∗ entirely through an independence model (Σ∗ = Σ∗R), or through a Markov model (Σ ∗ = J∗M −1). However, in most scenarios, these extreme cases are not desirable, since they result in dense models, while we are interested in sparse representations with a parsimonious use of edges in both the graphs, viz., the Markov and the independence graphs. In Section 3.1, we provide a sufficient set of structural and parametric conditions to guarantee identifiability of the Markov and the independence components, and in Section 3.2, we propose an optimization program to obtain them.\nCovariance Estimation Problem: In the above decomposition problem, we assume that the exact covariance matrix Σ∗ is known. However, in practice, we only have access to samples, and we describe this setting below.\nDenote Σ̂n as the sample covariance matrix Σ̂n :=\n1 n ∑n k=1 x(k)x T (k),where x(k), k = 1, ..., n are n i.i.d. observations of a zero mean Gaussian random vector X ∼ N (0,Σ∗), where X := (X1, ..., Xp). Now the estimation problem is described below.\nEstimation Problem: Assume that there exists a unique decomposition Σ∗ = J∗M −1 + Σ∗R where J ∗ M is an unknown concentration matrix with bounded entries and Σ∗R is an unknown sparse residual covariance matrix given a set of constraints. Given the sample covariance matrix Σ̂n, our goal is to find estimates of J∗M and Σ ∗ R with provable guarantees.\nIn the sequel, we relate the exact and the sample versions of the decomposition problem. In Section 4, we propose a modified optimization program to obtain efficient estimates of the Markov and independence components. Under a set of sufficient conditions, we provide guarantees in terms of sparsistency, sign consistency, and norm guarantees, defined below.\nDefinition 2 (Estimation Guarantees). We say that\nan estimate (ĴM , Σ̂R) to the decomposition problem in (2), given a sample covariance matrix Σ̂ n , is sparsis-\ntent or model consistent, if the supports of ĴM and Σ̂R coincide with the supports of J∗M and Σ ∗ R respectively. It is said to be sign consistent, if additionally, the respective signs coincide. The norm guarantees on the estimates is in terms of bounds on ‖ĴM − J∗M‖ and ‖Σ̂R −Σ ∗ R‖, under some norm ‖·‖."
    }, {
      "heading" : "3. Analysis under Exact Statistics",
      "text" : ""
    }, {
      "heading" : "3.1. Assumptions under Exact Statistics",
      "text" : "We first provide a set of sufficient conditions under which we can guarantee that the decomposition of Σ∗ in (2) into concentration matrix J∗M and residual matrix Σ∗R is unique. We impose the following set of constraints on the two matrices:\n(A.0) J∗M is a positive definite matrix: J ∗ M 0.\n(A.1) Off-diagonal entries of J∗M are bounded from above, i.e., ‖J∗M‖∞,off ≤ λ ∗, for some λ∗ > 0.\n(A.2) Diagonal entries of Σ∗R are zero: (Σ ∗ R)ii = 0, and\nthe support of its off-diagonal entries satisfies ( Σ∗R ) ij 6= 0 ⇐⇒ | ( J∗M ) ij | = λ∗, ∀ i 6= j. (3)\n(A.3) For any i, j, we have sign (( Σ∗R ) ij ) . sign (( J∗M ) ij ) ≤ 0, i.e., the signs\nare opposite to one another.\nIn the sequel, we propose an efficient method to recover the respective matrices J∗M and Σ ∗ R under conditions (A.0)-(A.3) and then establish the uniqueness\nof the decomposition. Finally, note that we do not impose any sparsity constraints on the concentration matrix J∗M , and in fact, our method and guarantees allow for dense matrices J∗M , when the exact covariance matrix Σ∗ is available. However, when only samples are available, we limit ourselves to sparse J∗M and provide learning guarantees in the high-dimensional regime, where the number of samples can be much smaller than the number of variables."
    }, {
      "heading" : "3.2. Formulation of the Optimization Program",
      "text" : "We now propose a method based on convex optimization for obtaining (J∗M ,Σ ∗ R) given the covariance matrix Σ∗ in (2). Consider the following program ( Σ̂M , Σ̂R ) := arg max\nΣM 0,ΣR log detΣM − λ‖ΣR‖1,off\ns. t. ΣM +ΣR = Σ ∗, (ΣR)d = 0, (4)\nwhere ‖·‖1,off denotes the `1 norm of the off-diagonal entries, which is the sum of the absolute values of the off-diagonal entries, and (·)d denotes the diagonal entries. Intuitively, the parameter λ imposes a penalty on large residual covariances, and under favorable conditions, can encourage sparsity in the residual matrix. The program in (4) can be recast\n( Σ̂M , Σ̂R ) := arg max\nΣM 0,ΣR log det ΣM (5)\ns. t. ΣM +ΣR = Σ ∗, (ΣR)d = 0, ‖ΣR‖1,off ≤ C(λ),\nfor some constant C(λ) depending on λ. The objective function in the above program corresponds to the entropy of the Markov model (modulo a scaling and a shift factor) (Cover & Thomas, 2006), and thus, intuitively, the above program looks for the optimal Markov model with maximum entropy subject to an `1 constraint on the residual matrix.\nWe declare the optimal solution Σ̂R in (4) as the estimate of the residual matrix Σ∗R, and ĴM := Σ̂ −1\nM as the estimate of the Markov concentration matrix J∗M . The justification behind these estimates is based on the fact that the Lagrangian dual of the program in (4) is (see long version on arXiv)\nĴM := arg min JM 0\n〈Σ∗, JM 〉 − log det JM (6)\ns. t. ‖JM‖∞,off ≤ λ,\nwhere ‖·‖∞,off denotes the `∞ element-wise norm of the off-diagonal entries, which is the maximum absolute value of the off-diagonal entries. Further, we show in the long version that the following relations exist between the optimal primal 3 solution ĴM and\n3 Henceforth, we refer to the program in (6) as the primal pro-\ngram and the program in (4) as the dual program.\nthe optimal dual solution ( Σ̂M , Σ̂R ) : ĴM = Σ̂ −1 M , and thus Ĵ−1M + Σ̂R = Σ ∗ is a valid decomposition of the covariance matrix Σ∗.\nRemark: Notice that when the `∞ constraint is removed in the primal program in (6), which is equivalent to letting λ → ∞, the program corresponds to the maximum likelihood estimate, and the optimal solution in this case is ĴM = Σ\n∗−1 and Σ̂R = 0. At the other extreme, when λ → 0, ĴM is a diagonal matrix, and the residual matrix Σ̂R is in general, a full matrix. (except for the diagonal entries). Thus, the parameter λ allows us to carefully tune the contributions of the Markov and residual components."
    }, {
      "heading" : "3.3. Guarantees and main results",
      "text" : "The main decomposition result is as follows. The proofs can be found in the extended version on arXiv.\nTheorem 1 (Uniqueness of Decomposition). Under (A.0)–(A.3), given a covariance matrix Σ∗, if we set the parameter λ = ‖J∗M‖∞,off in the optimization program in (4), then the optimal solutions of primaldual optimization programs (6) and (4) are given by ( ĴM , Σ̂R ) = ( J∗M ,Σ ∗ R ) , and the decomposition is unique.\nThus, we establish that the proposed optimization programs in (4) and (6) uniquely recover the Markov concentration matrix J∗M and the residual covariance matrix Σ∗R given Σ ∗ under conditions (A.0)–(A.3)."
    }, {
      "heading" : "4. Sample Analysis of the Algorithm",
      "text" : ""
    }, {
      "heading" : "4.1. Optimization Program",
      "text" : "We have so far provided guarantees on unique decomposition given the exact covariance matrix Σ∗. We now consider the case, when n i.i.d. samples are available from N (0,Σ∗). We now modify the primal-dual pair (6) and (4), considered in the previous section, to incorporate the sample covariance matrix Σ̂ n .\nĴM := arg min JM 0\n〈Σ̂n, JM 〉 − log detJM + γ‖JM‖1,off\ns. t. ‖JM‖∞,off ≤ λ, (7)\nIt is shown that the dual of above program is ( Σ̂M , Σ̂R ) := arg max\nΣM 0,ΣR log detΣM − λ‖ΣR‖1,off\ns. t. ‖Σ̂n − ΣM − ΣR‖∞,off ≤ γ, (8) ( ΣM ) d = ( Σ̂n ) d , ( ΣR ) d = 0.\nWe further establish that Σ̂M = Ĵ −1 M and thus,\n‖Σ̂n − Ĵ−1M − Σ̂R‖∞,off ≤ γ. (9)\nComparing the above with the exact decomposition Σ∗ = J∗M\n−1 + Σ∗R in (2), we note that for the sample version, we do not exactly fit the Markov and the residual models with the sample covariance matrix Σ̂ n , but allow for some divergence, depending on γ. Similarly, the primal program in (7) has an additional `1 penalty term on ĴM , which is absent in (6). Having a non-zero γ in the above programs enables us to impose a sparsity constraint on ĴM , which in turn, enables us to estimate the matrices in the high dimensional regime, under a set of sufficient conditions given below."
    }, {
      "heading" : "4.2. Assumptions under Sample Statistics",
      "text" : "The additional assumptions for successful recovery in high dimensions are based on the Hessian of the objective function in the optimization program in (7), with respect to the variable JM , evaluated at the true Markov model J∗M . The Hessian of this function is given by (Boyd & Vandenberghe, 2004)\nΓ∗ = J∗M −1 ⊗ J∗M −1 = Σ∗M ⊗ Σ ∗ M , (10)\nwhere ⊗ denotes the Kronecker matrix product. Thus Γ∗ is a p2×p2 matrix indexed by the node pairs. Based on the results for exponential families (Brown, 1986), Γ∗(i,j),(k,l) = Cov{XiXj , XkXl}, and hence it can be interpreted as an edge-based alternative to the usual covariance matrix Σ∗M . Define KM as the `∞ operator norm of the covariance matrix of the Markov model: KM := |||Σ∗M |||∞. We now denote the supports of the Markov and residual models. Denote EM := {(i, j) ∈ V × V |i 6= j, ( J∗M ) ij 6= 0} as the edge set of Markov matrix J∗M . Define\nSM := EM ∪ {(i, i)|i = 1, ..., p}, (11) SR := {(i, j) ∈ V × V | ( Σ∗R ) ij 6= 0}. (12)\nThus, the set SM includes diagonal entries and also all edges of the Markov graph corresponding to J∗M . Also, recall from (A.2) that the diagonal entries of Σ∗R are set to zero, and that the support set SR is contained in SM , i.e., SR ⊂ SM . Let ScM and S c R denote the respective complement sets. Define S := SM ∩ ScR, so that {SR, S, ScM} forms a partition of {(1, ..., p)×(1, ..., p)}. This partitioning plays a crucial role in being able to provide learning guarantees. Define the maximum node degree for Markov model J∗M as\nd := max j=1,...,p |{i : (i, j) ∈ EM}|. (13)\nFinally, for any two subsets T and T ′ of V × V , Γ∗TT ′ denotes the submatrix of Γ∗ indexed by T as rows and T ′ as columns. We now impose various constraints on the submatrices of the Hessian in (10), limited to each of the sets {SR, S, ScM}.\n(A.4) Mutual Incoherence: These conditions impose mutual incoherence among three partitions of Γ∗\nindexed by SR, S c M and S.\nmax{|||Γ∗Sc M S ( Γ∗SS )−1 Γ∗SSR − Γ ∗ Sc M SR |||∞,\n|||Γ∗Sc M S ( Γ∗SS )−1 |||∞} ≤ (1− α) (14) for some α ∈ (0, 1],\nKSSR := ||| ( Γ∗SS )−1 Γ∗SSR |||∞ < 1\n4 . (15)\n(A.5) Covariance control: For the same α specified above, we have the bound:\nKSS :=||| ( Γ∗SS )−1 |||∞ ≤\n(m− 4)α\n4(m− (m− 1)α) (16)\nfor some m > 4.\nAssumption (A.4) controls the pairwise effects of edges in different sets S, SR and S c M to each other."
    }, {
      "heading" : "4.3. Guarantees and Main Results",
      "text" : "We are now ready to provide the main result.\nTheorem 2. Consider a Gaussian distribution with covariance matrix Σ∗ = J∗M\n−1 + Σ∗R satisfying conditions (A.0)-(A.5). Given a sample covariance matrix Σ̂ n\nusing n i.i.d. samples from the Gaussian model, let ( ĴM , Σ̂R ) denote the unique optimal solutions of the primal-dual pair (7) and (8), with pa-\nrameters γ = C1 √ log p n and λ = λ∗ + C2 √ log p n for some constants C1, C2 > 0, where λ ∗ := ‖J∗M‖∞,off .\nSuppose that ( Σ∗R ) min := min(i,j)∈SR | ( Σ∗R ) ij | scales as ( Σ∗R ) min = Ω (√ log p n ) and the sample size n is lower bounded as\nn = Ω ( d2 log p ) , (17)\nthen with probability greater than 1 − 1/pc → 1 (for some c > 0), we have:\na) The estimates ĴM and Σ̂R satisfy `∞ bounds\n‖ĴM − J ∗ M‖∞ = O\n(√ log p\nn\n) , (18)\n‖Σ̂R − Σ ∗ R‖∞ = O\n(√ log p\nn\n) . (19)\nb) The estimate Σ̂R is sparsistent and sign consistent with Σ∗R.\nc) If in addition, ( J∗M ) min := min(i,j)∈EM | ( J∗M ) ij |\nscales as ( J∗M ) min\n= Ω (√\nlog p n\n) , then the esti-\nmate ĴM is sparsistent and sign consistent with J∗M .\nRemark 1 (Non-asymptotic sample complexity bounds): In the above theorem, we establish that the number of samples is required to scale as n = Ω(d2 log p). In fact, the result is non-asymptotic which is provided in the extended version on arXiv.\nRemark 2 (Comparison with sparse graphical model selection): The high dimensional covariance estimation problem which is investigated in (Ravikumar et al., 2011) involves similar mutual incoherence conditions and gives similar consistency results. Regarding the final result, sample complexity and convergence rate of estimated models are exactly the same as results in (Ravikumar et al., 2011) with only some minor differences in coefficients. But regarding the incoherence conditions, since their program is a special case of ours, the required conditions are less restrictive in their case. It is natural that we need some more incoherence conditions in order to be able to recover both the Markov and residual models."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section we provide experimental results for the proposed algorithm. We term our proposed optimization program as `1 + `∞ method and compare it with the well-known `1 method which is a special case of the proposed algorithm when λ = ∞. The optimization programs are implemented by YALMIP (Lofberg, 2004) and SDPT3 (Toh et al., 1999) packages for MATLAB. We also compare the performance of applying belief propagation to exact models.\nSynthetic Data: We build a Markov + residual synthetic model in the following way. The underlying graph for the Markov part is an 8 × 8 2-D grid structure (4-nearest neighbor grid). We choose the offdiagonal nonzero entries in J∗M (corresponding to the grid edges) randomly from set {−0.5, 0.5}. Then we ensure that J∗M is positive definite by adding some uniform diagonal weighting. We choose 0.2 fraction of Markov edges randomly to introduce residual edges. The value of these nonzero entries in Σ∗R are chosen from {−0.2, 0.2} such that the sign of residual entry is opposite of the sign of overlapping Markov entry (assumption (A.3)). We also generate a random mean in the interval [0, 1] for each node.\nWe apply `1+`∞ and `1 methods to a random realiza-\ntion of the above described model Σ∗ = J∗M −1 + Σ∗R. The edit distance between estimated and exact Markov model ĴM and J ∗ M is plotted in figure 2.a for different number of samples. First observation is that by increasing the number of samples, the edit distance decreases which is consistent with theoretical results. We also see that the behaviour of `1 + `∞ method is very close to `1 method which suggests that sparsity pattern of J∗M can be estimated efficiently under either methods. The edit distance between Σ̂R and Σ∗R is plotted in figure 2.b. We see again decreasing trend for `1 + `∞ method here with increasing number of samples. But since there is not any off-diagonal `∞ constraints in `1 method, it can not recover the residual matrix Σ∗R. Finally the `∞-elementwise norm of error between estimated precision matrix Ĵ and the exact precision matrix J∗ is sketched for both methods in figure 2.c. We observe the advantage of proposed `1 + `∞ method in estimating the overall model precision matrix J∗ = Σ∗−1.\nNext the results of running Loopy Belief Propagation (LBP) on the models are presented. We compare the result of applying LBP to the same J∗ and J∗M models generated in the learning discussion above. The log of average mean and variance errors over all nodes are sketched in figure 3 throughout the iterations. We observe that LBP does not converge for J∗ model. It is shown by Malioutov et al. (2006) that if a model is walk-summable then the mean estimates under LBP converge and are accurate. The spectral norms of the partial correlation matrices are ‖RM‖ = 0.9443 and ‖R‖ = 6.9191 for J∗M and J\n∗ models respectively. Thus, the matrix J∗ is not walk-summable and therefore its convergence under LBP is not guaranteed and this is seen in figure 3. On the other hand, LBP is accurate for J∗M matrix. Thus, our method learns models which are better suited for inference under loopy belief propagation."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this paper, we provided an in-depth study of convex optimization methods and guarantees for high-dimensional covariance decomposition into sparse Markov and independence domains. We provide consistency guarantees for estimation in both the Markov and the independence domains, and establish efficient sample complexity results for our method. These findings open up many future directions to explore. One important aspect is to relax the sparsity constraints imposed in the two domains, and to develop new methods to enable decomposition of such models. Other considerations include extension to discrete models\nand other models for the residual covariance matrix (e.g. low rank matrices). Such findings will push the envelope of efficient models for high-dimensional estimation. It is worth mentioning while in many scenarios it is important to incorporate latent variables, in our framework it is challenging to incorporate both latent variables as well as marginal independencies, and provide learning guarantees, and we defer it to future work."
    } ],
    "references" : [ {
      "title" : "An Introduction to Multivariate Statistical Analysis",
      "author" : [ "T.W. Anderson" ],
      "venue" : null,
      "citeRegEx" : "Anderson,? \\Q1984\\E",
      "shortCiteRegEx" : "Anderson",
      "year" : 1984
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S.P. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe",
      "year" : 2004
    }, {
      "title" : "Fundamentals of statistical exponential families: with applications in statistical decision theory",
      "author" : [ "L.D. Brown" ],
      "venue" : "Lecture Notes-Monograph Series, Institute of Mathematical Statistics,",
      "citeRegEx" : "Brown,? \\Q1986\\E",
      "shortCiteRegEx" : "Brown",
      "year" : 1986
    }, {
      "title" : "Gaussian multiresolution models: Exploiting sparse Markov and covariance structure",
      "author" : [ "M.J. Choi", "V. Chandrasekaran", "A.S. Willsky" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "Choi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2010
    }, {
      "title" : "On a dualization of graphical gaussian models",
      "author" : [ "G. Kauermann" ],
      "venue" : "Scandinavian journal of statistics,",
      "citeRegEx" : "Kauermann,? \\Q1996\\E",
      "shortCiteRegEx" : "Kauermann",
      "year" : 1996
    }, {
      "title" : "Graphical models: Clarendon Press",
      "author" : [ "S.L. Lauritzen" ],
      "venue" : null,
      "citeRegEx" : "Lauritzen,? \\Q1996\\E",
      "shortCiteRegEx" : "Lauritzen",
      "year" : 1996
    }, {
      "title" : "Yalmip: A toolbox for modeling and optimization in matlab",
      "author" : [ "J. Lofberg" ],
      "venue" : "In IEEE international symposium on Computer Aided Control Systems Design (CACSD),",
      "citeRegEx" : "Lofberg,? \\Q2004\\E",
      "shortCiteRegEx" : "Lofberg",
      "year" : 2004
    }, {
      "title" : "WalkSums and Belief Propagation in Gaussian Graphical Models",
      "author" : [ "D.M. Malioutov", "J.K. Johnson", "A.S. Willsky" ],
      "venue" : "J. of Machine Learning Research,",
      "citeRegEx" : "Malioutov et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Malioutov et al\\.",
      "year" : 2006
    }, {
      "title" : "High-dimensional covariance estimation by minimizing `1-penalized log-determinant divergence",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "Ravikumar et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ravikumar et al\\.",
      "year" : 2011
    }, {
      "title" : "Sdpt3 - a matlab software package for semidefinite programming",
      "author" : [ "K.C. Toh", "M.J. Todd", "R.H. Tutuncu" ],
      "venue" : "Optimization Methods and Software,",
      "citeRegEx" : "Toh et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Toh et al\\.",
      "year" : 1999
    }, {
      "title" : "Graphical Models, Exponential Families, and Variational Inference",
      "author" : [ "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Wainwright and Jordan,? \\Q2008\\E",
      "shortCiteRegEx" : "Wainwright and Jordan",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Finding the sample covariance matrix based on observed data is straightforward and widely used (Anderson, 1984).",
      "startOffset" : 95,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "A natural mechanism to achieve this is to impose a sparsity constraint on the covariance matrix, which implies that the variables under consideration satisfy marginal independence, corresponding to the zero pattern of the covariance matrix (Kauermann, 1996) (and we refer to such models as independence models).",
      "startOffset" : 240,
      "endOffset" : 257
    }, {
      "referenceID" : 5,
      "context" : "widespread acceptance in recent years (Lauritzen, 1996).",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "The set of sufficient conditions for successful recovery are based on the so-called notion of mutual incoherence, which controls the dependence between different sets of variables, See (Ravikumar et al., 2011).",
      "startOffset" : 185,
      "endOffset" : 209
    }, {
      "referenceID" : 8,
      "context" : "Our consistency proofs borrow ideas from (Ravikumar et al., 2011), and at the same time, require new ideas to carefully control the errors in the two domains, viz.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "The idea that a combination of Markov and independence models can provide good model-fitting is not by itself new, see (Choi et al., 2010).",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 7,
      "context" : "This is because the Markov components of the estimated models tend to be more walk summable (Malioutov et al., 2006), since some of the correlations can be “transferred” to the residual matrix.",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "The local and global Markov properties are equivalent for nondegenerate Gaussian distributions (Lauritzen, 1996).",
      "startOffset" : 95,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "Under nondegeneracy conditions, it is also in the minimal form, and as such is identifiable (Brown, 1986).",
      "startOffset" : 92,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "Based on the results for exponential families (Brown, 1986), Γ(i,j),(k,l) = Cov{XiXj , XkXl}, and hence it can be interpreted as an edge-based alternative to the usual covariance matrix ΣM .",
      "startOffset" : 46,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "Remark 2 (Comparison with sparse graphical model selection): The high dimensional covariance estimation problem which is investigated in (Ravikumar et al., 2011) involves similar mutual incoherence conditions and gives similar consistency results.",
      "startOffset" : 137,
      "endOffset" : 161
    }, {
      "referenceID" : 8,
      "context" : "Regarding the final result, sample complexity and convergence rate of estimated models are exactly the same as results in (Ravikumar et al., 2011) with only some minor differences in coefficients.",
      "startOffset" : 122,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "The optimization programs are implemented by YALMIP (Lofberg, 2004) and SDPT3 (Toh et al.",
      "startOffset" : 52,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "The optimization programs are implemented by YALMIP (Lofberg, 2004) and SDPT3 (Toh et al., 1999) packages for MATLAB.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "It is shown by Malioutov et al. (2006) that if a model is walk-summable then the mean estimates under LBP converge and are accurate.",
      "startOffset" : 15,
      "endOffset" : 39
    } ],
    "year" : 2012,
    "abstractText" : "In this paper, we present a novel framework incorporating a combination of sparse models in different domains. We posit the observed data as generated from a linear combination of a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse Gaussian independence model (with a sparse covariance matrix). We provide efficient methods for decomposition of the data into two domains, viz., Markov and independence domains. We characterize a set of sufficient conditions for identifiability and model consistency. Our decomposition method is based on a simple modification of the popular `1-penalized maximumlikelihood estimator (`1-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples n scales as n = Ω(d log p), where p is the number of variables and d is the maximum node degree in the Markov model. Our conditions for recovery are comparable to those of `1-MLE for consistent estimation of a sparse Markov model, and thus, we guarantee successful high-dimensional estimation of a richer class of models under comparable conditions. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation.",
    "creator" : "dvips(k) 5.98 Copyright 2009 Radical Eye Software"
  }
}