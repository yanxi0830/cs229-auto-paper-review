{
  "name" : "1312.7606.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distributed Policy Evaluation Under Multiple Behavior Strategies",
    "authors" : [ "Sergio Valcarcel" ],
    "emails" : [ "sergio@gaps.ssr.upm.es;", "santiago@gaps.ssr.upm.es).", "cjs09@ucla.edu;", "sayed@ee.ucla.edu)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n31 2.\n76 06\nv2 [\ncs .M\nA ]\n5 N\nov 2\nWe apply diffusion strategies to develop a fully-distributed cooperative reinforcement learning algorithm in which agents in a network communicate only with their immediate neighbors to improve predictions about their environment. The algorithm can also be applied to off-policy learning, meaning that the agents can predict the response to a behavior different from the actual policies they are following. The proposed distributed strategy is efficient, with linear complexity in both computation time and memory footprint. We provide a mean-square-error performance analysis and establish convergence under constant step-size updates, which endow the network with continuous learning capabilities. The results show a clear gain from cooperation: when the individual agents can estimate the solution, cooperation increases stability and reduces bias and variance of the prediction error; but, more importantly, the network is able to approach the optimal solution even when none of the individual agents can (e.g., when the individual behavior policies restrict each agent to sample a small portion of the state space).\nIndex Terms\nAdaptive networks, Arrow-Hurwicz algorithm, diffusion strategies, distributed processing, gradient\ntemporal difference, mean-square-error, reinforcement learning, saddle-point problem\nThis work was supported in part by the Spanish Ministry of Science and Innovation in the program CONSOLIDER-INGENIO 2010 under the Grant CSD2008-00010 COMONSENS and by the NSF grants CCF-1011918 and ECCS-1407712. A short preliminary version dealing with a special case of this work appears in the conference publication [1].\nS. V. Macua and S. Zazo are with the Department of Signals, Systems and Radiocommunications, Escuela Técnica Superior de Ingenieros de Telecomunicaión, Universidad Politécnica de Madrid, Madrid 28040, Spain (e-mail: sergio@gaps.ssr.upm.es; santiago@gaps.ssr.upm.es).\nJ. Chen and A. H. Sayed are with the Department of Electrical Engineering, University of California, Los Angeles, CA 90095 USA (e-mail: cjs09@ucla.edu; sayed@ee.ucla.edu).\nNovember 6, 2014 DRAFT\n2 I. INTRODUCTION\nConsider the problem in which a network of autonomous agents collaborate to predict the response of the environment to their actions. The network forms a connected graph, where there is at least one path between every pair of nodes. The agents learn locally from their individual interactions with the environment and share knowledge with their neighbors. Only direct neighborhood communication is allowed. We assume the environment can be modeled as a Markov decision process. The agents do not have access to the actual state of the environment, but just to feature vectors representing it. The feature representation is convenient in problems with very large state dimensions since it is computationally more efficient to work with features of smaller dimension than the size of the original state-space.\nIn the scenario under study in this work, every agent takes actions according to an individual policy, which is possibly different from that of every other agent. The objective of the agents is to assess the response of the environment to a common hypothetical behavior, the target policy, which is the same for every agent but different from the actual behavior policies they are following. This problem of predicting the response to a target policy different from the behavior policy is commonly referred as off-policy learning [2]. Off-policy learning has been claimed to be necessary when the agents need to perform tasks in complex environments because they could perform many different predictions in parallel from a single stream of data [3]–[5].\nThe predictions by the agents are made in the form of value functions [2], [6], [7]. The gradienttemporal-difference (GTD) algorithm is one useful method for computing approximate value functions. It was originally proposed for the single agent scenario in [8], [9], and derived by means of the stochastic optimization of a suitable cost function. The main advantages of this single-agent GTD are its low complexity and its convergence guarantees (for diminishing step-sizes) under the off-policy setting. In Section III of this work we apply diffusion strategies to develop a distributed GTD algorithm that extends the single-agent GTD to multi-agent networks. There are several distributed strategies that can be used for this purpose, such as consensus [10]–[14] and diffusion strategies [15]–[18]. Consensus strategies have been successfully applied to the solution of static optimization problems, where the objective does not drift with time. They have been studied largely under diminishing step-size conditions to ensure agreement among cooperating agents. Diffusion strategies, on the other hand, have been proved to be particularly apt at endowing networks with continuous adaptation and learning abilities to enable tracking of drifting conditions. There are several forms of diffusion; recent overviews appear in [19]–[21]. It has been shown in [22] that the dynamics of diffusion networks leads to enhanced stability and lower mean-\nNovember 6, 2014 DRAFT\n3 square-error (MSE) than consensus networks. In particular, the analysis in [20]–[22] shows that consensus networks combine local data and in-neighborhood information asymmetrically, which can make the state of consensus networks grow unbounded even when all individual agents are mean stable in isolation. This behavior does not happen in diffusion networks, in which local and external information are symmetrically combined by construction, enhancing the stability of the network. For these reasons, we focus in the remainder of this article on the derivation of a diffusion strategy for GTD over multi-agent networks. As a byproduct of this derivation, we show that the GTD algorithm, motivated as a two time-scales stochastic approximation in [9], is indeed a stochastic Arrow-Hurwicz algorithm applied to the dual problem of the original formulation.\nThe convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23]. For a distributed algorithm, the analysis becomes more demanding because the estimation process at each node is influenced by the estimates at the other nodes, so the error propagates across the network. Another difficulty in the distributed case is that the agents may follow different behavior policies and, thus, their individual cost functions could have different minimizers. In Section IV, we will analyze the steady-state and transient behavior of the proposed distributed algorithm, deriving closed-form expressions that characterize the network performance for sufficiently small constant step-sizes. We employ constant, as opposed to decaying step-sizes, because we are interested in distributed solutions that are able to continually adapt and learn. The performance analysis will reveal that when the agents follow the same behavior policy, they will be able to find an unbiased estimator for the centralized solution. On the other hand, when the agents behave differently, they will approach, up to some bias, the solution of a convex combination of their individual problems. This bias is proportional to the step-size, so it becomes negligible when the step-size is sufficiently small. One important benefit that results when the agents behave differently is that, although the agents do not directly share their samples, the in-network experience becomes richer in a manner that the diffusion strategy is able to exploit. In particular, in the reinforcement learning literature, it is customary to assume that the behavior policy must allow the agents to visit every possible state infinitely often. We will relax this assumption and show that the distributed algorithm is able to perform well even when the individual agents only visit small portions of the state-space, as long as there are other agents that explore the remaining regions. Therefore, even though none of the agents can find the optimal estimate of the value function by itself, they can achieve it through cooperation. This is an interesting capability that emerges from the networked solution.\nIn this work, we consider a setting in which the agents can communicate with their neighbors, but they\nNovember 6, 2014 DRAFT\n4 operate without influencing each other. This setup is meaningful in many real applications. Consider, for example, a water purification plant controlled and monitored by a wireless actuator-sensor network, in which each device is attached to a different water-tank. The quality of the water (e.g., the amount of bacteria) in one tank will be influenced by the decisions (e.g., delivering some amount of chlorine) made by the device controlling that tank, independently of what other devices do. Still, since all water tanks behave similarly under similar circumstances, the devices in the network can benefit from sharing their individual knowledge."
    }, {
      "heading" : "A. Related works",
      "text" : "There are several insightful works in the literature that address issues pertaining to distributed learning albeit under different scenarios and conditions than what is studied in this article. For example, the work in [24] proposes a useful algorithm, named QD-learning, which is a distributed implementation of Q-learning using consensus-based stochastic approximation. The diffusion strategy proposed herein is different in several respects. QD-learning asymptotically solves the optimal control problem, learning the policy that maximizes the long-term reward of the agents. Here, we focus on predicting the long-term reward for a given policy, which is an important part of the control problem. However, QD-learning is developed in [24] under the assumption of perfect-knowledge of the state. Here, we study the case in which the agents only know a feature representation of the state, which is used to build a parametric approximation of the value function, allowing us to tackle large problems, for which Q-learning schemes can diverge [25], [26]. Finally, we enforce constant step-sizes in order to enable continuous adaptation and learning. In comparison, the analysis in [24] employs a diminishing step-size that dies out as time progresses and, therefore, turns off adaptation and is not able to track concept drifts in the data.\nAnother related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26]. In addition, although the algorithm in [27] is distributed, in the sense that there is no fusion center, it requires full connectivity (i.e., every node must be able to exchange information with every other node in the network), which is a restrictive assumption that prevents the algorithm from large-scale deployments. In this article, we focus on fully distributed solutions that only require the network of agents to be connected (but not necessarily fully connected). Other related—but more heuristic—approaches include [28], [29].\nNovember 6, 2014 DRAFT\n5"
    }, {
      "heading" : "B. Notation",
      "text" : "Lower case letters are used to denote both scalar values and vectors. Matrices are denoted by upper case letters. Boldface notation denotes random variables (e.g., s is a realization for s). The state of the environment and the action taken by an agent are denoted by s and a, respectively. With a slight abuse of notation, s(i) and a(i) denote the state and action variables at time i. Moreover, whenever a variable is specific to some agent k we add a subscript (e.g., sk(i) = s means that the environment seen by agent k is at state s at time i).\nAll vectors are column vectors. Superscript ·⊤ denotes transposition. The identity matrix of size S is denoted by IS , the null matrix of size M × L is denoted by 0M×L, and 1M and 0M stand for vectors of ones and zeros of length M , respectively. The Kronecker product operation is denoted by ⊗. The spectrum, m-th eigenvalue and spectral radius of a matrix are denoted by λ(·), λm(·) and ρ(·), respectively. The operator col{·} stacks vectors (or matrices) into a long vector (or a tall matrix); while vec[·] stacks the columns of a matrix, one beneath the other, into a long vector. The operator diag{·} creates a diagonal matrix (a block-diagonal matrix) from a given vector (a set of square matrices). The Euclidean (semi)norm is given by ‖y‖2D , y⊤Dy, where D is a positive (semi)definite matrix. The expected value operator with respect to probability distribution d is denoted by Ed[·]; we use multiple sub-indexes (e.g., Ed,φ,P [·]) when the expectation is taken with regard to multiple distributions."
    }, {
      "heading" : "II. BELLMAN EQUATION AND VALUE FUNCTIONS",
      "text" : ""
    }, {
      "heading" : "A. Markov decision processes (MDP)",
      "text" : "We consider Markov decision processes (MDP) [6], [7] that are characterized by a finite set of states S of size S , |S|; a finite set of actions A; the kernel of transition probabilities P(s′|s, a), which gives the probability of going from one state s to another state s′, given an action a; and the reward function r : S ×A× S → R that the agent wants to predict, which is associated with every transition, such that r (s, a, s′) denotes the reward received by a generic agent for the transition from s to s′ after taking action a.\nThe agents want to predict the response of their environment when they follow some stationary policy π, such that π(a|s) stands for the probability of an agent choosing action a when the environment is at state s. We assume that the finite-state Markov chain resulting from the MDP is irreducible and aperiodic under any policy of interest. Thus, it has a unique positive stationary probability distribution of visiting each state [6, App. A] [30] denoted by dπ = [dπ(1), . . . , dπ(S)]⊤, such that dπ(s) > 0, for all 1 ≤ s ≤ S.\nNovember 6, 2014 DRAFT\n6 The state transition probabilities of the Markov chain, from initial state s to destination s′ are given by\npπss′ , P ( s′ | s ) = ∑ a∈A P ( s′|s, a ) π(a|s) (1)\nWe collect pπss′ into an S × S matrix P π as its (s, s′)-th entry.\nB. Value function\nIn order to make predictions of the reward signal, we use state value functions, v : S → R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]–[7]. This time window spans from i = 0 to i = ∞, but it has an effective length controlled by a constant γ ∈ (0, 1), which trades short-sighted (γ → 0) vs. long-term planning (γ → 1). The value function for target policy π, starting from some initial state s ∈ S at time i, is defined as:\nvπ(s) , Eπ,P\n[ ∞∑\nt=1\nγt−1r(i+ t) ∣∣∣∣∣ s(i) = s ]\n(2)\nwhere r(i + 1) , r(s(i),a(i), s(i + 1)), and the expectation is taken with regard to all possible statetransitions. Note that a(i) is random because it is drawn from a probability distribution π, which together with the probabilistic transition dictated by P, leads to a random future state s(i+1). Let s′ denote the destination state after transitioning from s. Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]:\nvπ(s) = Eπ,P [r(i+ 1) + γr(i+ 2) + . . . | s(i) = s]\n= Eπ,P [r(i+ 1) | s(i) = s] + γEπ,P [r(i+ 2) + γr(i+ 3) + . . . | s(i) = s]\n= rπ(s) + γEπ,P\n[ ∞∑\nt=1\nγt−1r(i+ 1 + t) | s(i) = s ]\n= rπ(s) + γEπ,P [ vπ(s′) | s(i+ 1) = s′ ( ght] = rπ(s) + γ ∑\ns′∈S\n∑ a∈A P ( s′|s, a ) π(a|s)vπ(s′)\n= rπ(s) + γ ∑\ns′∈S pπss′v\nπ(s′) (3)\nwhere rπ(s) denotes the expected reward that can be collected over the next transition when the agent is currently at state s:\nrπ(s) , Eπ,P [r(s,a, s′)] = ∑\na∈A π(a|s)\n∑ s′∈S P(s′|s, a)r(s, a, s′) (4)\nNovember 6, 2014 DRAFT\n7 Let vπ and rπ be the vectors of length S that collect the values vπ(s) and rπ(s) for all s ∈ S, respectively:\nvπ ,   vπ(1) ...\nvπ(S)\n  ∈ RS , rπ ,   rπ(1) ...\nrπ(S)\n  ∈ RS (5)\nThen, Eq. (3) can be written in vector form as the linear system of equations:\n(IS − γP π)vπ = rπ (6)\nWe shall refer to vπ as the value vector. There are two challenges when we aim to obtain vπ from (6). The first challenge is that the size of the state-space can be very large (e.g., the chess game has 1047 possible states, making (6) computationally intractable). The second challenge arises when the agents do not know anything about the environment, thus P π and rπ are unavailable. In the following subsections we review how to address these two issues."
    }, {
      "heading" : "C. Approximate value function as a saddle-point problem",
      "text" : "For the single agent scenario, references [8], [9] introduced efficient algorithms with convergence guarantees under general conditions. The algorithms save on computations by relying on features that span a space of much lower dimensionality than the size of the original state space. More formally, let x : S → RM be some mapping from states to features, such that xs is the feature vector of length M ≪ S that represents the state s. Now, it would be efficient to approximate the original value function vπ(s) as a parametric function of xs, for some parameter vector w ∈ RM . When this is done, the problem of making a prediction (i.e., estimating the value vector vπ) becomes equivalent to seeking a parameter vector w⋆ that is optimal in a certain sense. Among many parametrizations, a linear approximation of the form\nvπ(s) ≈ x⊤s w (7)\nhas been extensively studied in the literature (see, e.g., [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands. Moreover, it is expected that if one chooses the mapping of features carefully, then the linear approximation model will generally provide good results (see, e.g., [33]–[38] ). Let X be the matrix of size S × M formed by stacking the transposed feature\nNovember 6, 2014 DRAFT\n8 vectors, x⊤s , on top of each other:\nX ,   x⊤1 ...\nx⊤S\n  ∈ RS×M (8)\nThen, the linear approximation (7) can be expressed in vector form as:\nvπ ≈ Xw (9)\nBy modeling the value function in the form (9), we solve for w by using the approximation (9) in (6). Doing so leads to the approximate Bellman equation:\nXw = rπ + γP πXw (10)\nIn this paper, we assume that the features available for the agents constitute a linearly independent set of basis functions, which effectively represent the states. Thus, X is full rank by construction. However, the fixed point equation (10) may not have a solution w in general because the right-hand side need not lie in the range space of X, which we denote by X. To address this issue, one approach is to solve instead the projected Bellman equation [26]:\nXw = Π(rπ + γP πXw) (11)\nwhere Π is a projection operator onto X. Since X is a linear space, the projection operator with respect to some metric norm ‖ · ‖D is defined as:\nΠx , argmin x̄∈X\n‖x− x̄‖2D (12)\nwhere D is a symmetric positive-definite matrix. The matrix Π is given by\nΠ = X(X⊤DX)−1X⊤D (13)\nTherefore, for different choices of D, we have different projection operators. However, some choices for D will lead to simpler solutions, as we will reveal in Subsection II-D.\nEquation (11) is now an over-determined consistent linear system of equations. To solve for w, reference\nNovember 6, 2014 DRAFT\n9 [9] considered the weighted least-squares problem:\nminimize w\nJPB(w) , ‖Π(rπ + γP πXw)−Xw‖2D (14)\nwhere the cost function JPB(w) is referred to as the projected Bellman error. Since Xw already lies in X and D is positive definite, it can be verified that\nJPB(w) = ‖Πrπ −Π(IS − γP π)Xw‖2D\n= (rπ − (IS − γP π)Xw)⊤Π⊤DΠ(rπ − (IS − γP π)Xw) = ( X⊤Drπ −Bw )⊤ ( X⊤DX )−1 ( X⊤Drπ −Bw ) (15)\nwhere B , X⊤D(IS − γP π)X. Using (15), it can also be verified that the solution w⋆ that minimizes JPB(w) satisfies the following normal equations [39]:\nB⊤(X⊤DX)−1Bw⋆ = B⊤(X⊤DX)−1X⊤Drπ (16)\nSince ‖P π‖∞ = 1 and γ < 1, we can bound the spectral radius of γP π by\nρ(γP π) ≤ ‖γP π‖∞ = γ < 1 (17)\nThus, the inverse (IS − γP π)−1 exists. In addition, since the matrices D and X have full-rank by assumption, we conclude that matrix B is invertible, so the minimizer w⋆ is given by\nw⋆ = ( X⊤D (IS − γP π)X )−1 X⊤Drπ (18)\nIf the quantities {P π, rπ} were known, one would proceed to solve (18) and determine the desired vector w⋆ and the sought-after value vector vπ from (9). However, we want the agents to learn vπ without any prior knowledge of the environment. In other words, we cannot assume P π and rπ are known. For this reason, we need to develop an alternative solution method. In the process of doing so, first for single-agents, we shall arrive at the same gradient temporal difference method of [9] albeit by using a fundamentally different approach involving a primal-dual argument. The approach will subsequently enable us to generalize to a fully distributed solution.\nSo let us continue with the single-agent case for now. Our first step relies on relating Eq. (15) to the saddle-point conditions of a convex optimization problem. Indeed, minimizing JPB(w) in (14) is\nNovember 6, 2014 DRAFT\n10\nequivalent to the following quadratic programming problem:\nminimize ε,w\n1 2 ε⊤(X⊤DX)−1ε\ns.t. ε = X⊤Drπ −Bw (19)\nwhere we have introduced the splitting variable ε. Since problem (19) is convex and satisfies Slater’s condition [40], strong duality holds and the primal and dual optimal values are attained and equal and they form a saddle-point of the Lagrangian. Specifically, the Lagrangian of (19) is\nL(ε, w, θ)= 1 2 ‖ε‖2(X⊤DX)−1 + θ⊤\n( X⊤Drπ −Bw − ε ) (20)\nwhere θ is the Lagrange multiplier. By minimizing L(ε, w, θ) over ε and w, we obtain that the dual function is g(θ) = −∞ unless B⊤θ = 0M , in which case we have\ng(θ) = −1 2 θ⊤X⊤DXθ + θ⊤X⊤Drπ (21)\nTherefore, the dual problem of (19) is given by\nminimize θ\n1 2 θ⊤X⊤DXθ − θ⊤X⊤Drπ\ns.t. B⊤θ = 0M\n(22)\nThe main reason to solve (22) instead of the primal problem (19) is that the dual formulation removes the inverse in the weighting matrix, X⊤DX. This transformation brings two benefits. First, in Sec. II-D, we will see that it is straightforward to optimize (22) from samples. Second, as it is explained in Sections III and IV-B, problem (22) leads to a distributed algorithm in which the agents are able to combine their individual experience to solve the problem.\nHad we assumed P π and rπ to be known, problem (22) would be trivial, with unique solution θ = 0M . However, since we do not assume any prior knowledge, we are going to employ instead a primal-dual algorithm that leads to an iterative stochastic-approximation mechanism to learn from samples. First, we derive the Lagrangian of (22) as\nL(θ,w) = 1 2 θ⊤X⊤DXθ − θ⊤X⊤Drπ + w⊤B⊤θ\n= θ⊤X⊤D ( 1\n2 Xθ + (IS − γP π)Xw − rπ\n) (23)\nwhere w denotes the Lagrange multiplier. We use the same notation w to denote the dual variable for (23) because it can be verified that by computing the dual of the dual problem (22) we recover the original\nNovember 6, 2014 DRAFT\n11\nproblem (14), which is equivalent to (19). Thus, the optimal dual variable w⋆ of (23) is also the optimal solution to (14). To find a saddle-point {θ⋆, w⋆} of the Lagrangian (23) we alternate between applying gradient descent to L(θ,w) with respect to θ and gradient ascent with respect to w:\nθi+1 = θi − µθX⊤D (Xθi + (IS − γP π)Xwi − rπ) (24a)\nwi+1 = wi + µwX ⊤(IS − γP π)⊤DXθi (24b)\nwhere µθ and µw are positive step-sizes.\nConstruction (24a)–(24b) is the well-known Arrow-Hurwicz algorithm (see, e.g., [41], [42, Ch. 9.3.3]\nand [43, Ch. 10])."
    }, {
      "heading" : "D. Primal-dual stochastic optimization",
      "text" : "As mentioned before, since the agents do not have prior knowledge of the environment, we need to replace (24a)–(24b) by constructions that do not depend on the quantities {P π, rπ}. In order to find the solution directly from samples, we need to convert these gradient iterations into stochastic approximations. The selection of an appropriate weighted norm ‖ · ‖D in (14) now becomes relevant. If we choose a weighting matrix D that represents the probability distribution of visiting each state, then we can express the terms that appear in (24a)–(24b) as expectations that we can substitute with their sample estimates. We proceed to explain the details.\nLet us set the weighting matrix in (14) equal to the state-visitation probability induced by the behavior policy (which we emphasize with the corresponding superscript), i.e., D , Dφ , diag{dφ}. Equations (24a)–(24b) depend on P π and rπ, meaning that the agent aims to predict the value vector along the expected trajectory that would have been induced by the target policy π. However, the state-visitation distribution of this trajectory, dπ , does not match the distribution of the samples actually gathered by the agent, given by dφ. Importance sampling [44, Ch. 9.7] is a technique for estimating properties of a particular distribution, while only having samples generated from a different distribution. Let us introduce importance weights that measure the dissimilarity between the target (π) and behavior (φ) policies.\nξ(a, s) , π(a|s) φ(a|s) (25)\nBy using importance sampling, reference [23] showed that we can write the gradient inside (24a) in terms of moment values of the behavior policy as follows:\nX⊤Dφ (Xθi + (IS − γP π)Xwi − rπ)\nNovember 6, 2014 DRAFT\n12\n= ∑\ns∈S dφ(s)xs\n x⊤s θi + ( x⊤s − γ ∑\ns′∈S pπss′x ⊤ s′\n) wi − ∑\na∈A\n∑ s′∈S P(s′|s, a)π(a|s)r(s, a, s′)\n \n= ∑\ns∈S\n∑\na∈A\n∑ s′∈S P(s′|s, a)π(a|s)dφ(s) · xs ( x⊤s θi + (xs − γxs′)⊤wi − r(s, a, s′) )\n= ∑\ns∈S\n∑\na∈A\n∑ s′∈S P(s′|s, a)φ(a|s)ξ(a, s)dφ(s) · xs ( x⊤s θi + (xs − γxs′)⊤wi − r(s, a, s′) )\n= Edφ,φ,P [ xs ( x⊤s θi + (xs − γxs′)⊤wi − r(s,a, s′) ) ξ(a, s) ] (26)\nSimilarly, we can express the gradient inside (24b) as\nX⊤(IS − γP π)⊤DφXθi = Edφ,φ,P [ (xs − γxs′)x⊤s ξ(a, s) ] θi (27)\nThe agent does not know these expected values though. Rather, at every time-step, the agent observes transitions of the form {xi, a(i), xi+1, r(i+1)}, where xi , xs(i) denotes the feature vector observed at time i.\nIn addition, the agent knows both its behavior policy φ and the target policy π that it wants to evaluate so it can compute the importance weight. Nevertheless, in an actual implementation, the agent need not know the states but just features, hence, the actual policies must be conditioned on the feature vectors. Slightly abusing notation, we introduce the importance weight that the node computes at every time step:\nξ(i) , π(a(i)|xi) φ(a(i)|xi) ≈ π(a(i)|s(i)) φ(a(i)|s(i)) , ξ(a(i), s(i)) (28)\nSince a sample of a random variable is an unbiased estimator of its expected value, we can build a pair of stochastic approximation recursions from (24a)–(24b) and (26)–(27):\nθi+1 = θi − µθxi ( x⊤i θi + δ ⊤ i+1wi − r(i+ 1) ) ξ(i) (29a)\nwi+1 = wi + µwδi+1x ⊤ i θiξ(i) (29b)\nwhere we introduced δi+1 , xi−γxi+1. Recursions (29a)–(29b) coincide with the single-agent gradienttemporal difference (GTD2) algorithm, which was derived in [9] using a different approach. The above derivation from (19) to (29b) shows that GTD2 is a stochastic Arrow-Hurwicz algorithm applied to the dual problem of (14). More importantly, as we will see in the following sections, the primal-dual approach is convenient for a multi-agent formulation, since it leads to a meaningful in-network statevisitation distribution that combines the individual stationary distributions of the agents, thus overcoming\nNovember 6, 2014 DRAFT\n13\nnon-exploratory individual behavior policies."
    }, {
      "heading" : "III. MULTI-AGENT LEARNING",
      "text" : "We now consider a network of N connected agents that operate in similar but independent MDPs. The state-space S, action-space A, and transition probabilities P are the same for every node, but their actions do not influence each other. Thus, the transition probabilities seen by each agent k are only determined by its own actions, ak(i) ∈ A, and the previous state of its environment, sk(i) ∈ S:\nsk(i+ 1) ∼ P(·|sk(i), ak(i)), k = 1 . . . N (30)\nThis assumption is convenient because it makes the problem stationary without forcing each agent to know the actions and feature vectors of every other agent in the network. The agents aim to predict the response of their environment to a common target policy π while they follow different behavior policies, denoted by φk(a|s) each. Motivated by recent results on network behavior in [21], [45], we note that, through collaboration, each agent may contribute to the network with its own experience. Let Dφk be the diagonal matrix that represents the stationary state-visitation distribution for agent k. We then introduce the following global problem in place of (22) with D substituted by Dφk :\nminimize θ\nN∑\nk=1\nτk\n( 1\n2 θ⊤X⊤DφkXθ − θ⊤X⊤Dφkrπ\n)\ns.t.\nN∑\nk=1\nτk ( X⊤Dφk(IS − γP π)X )⊤ θ = 0\n(31)\nwhere τ = [τ1, . . . , τN ]⊤ is a vector of non-negative parameters whose purpose is to weight the contribution of each agent’s local problem to the global problem, such that τ⊤1N = 1. Since the dual problem (22) removes the inverse of the weighting matrices X⊤DφkX, we can introduce the in-network stationary distribution\nDφ ,\nN∑\nk=1\nτkD φk (32)\nNote that solving the aggregated problem (31) is effectively solving the single-agent problem (22) with D replaced by Dφ. The Lagrangian of (31) is given by\nL(θ,w) =\nN∑\nk=1\nτkLk(θ,w) (33)\nNovember 6, 2014 DRAFT\n14\nwhere the individual Lagrangians are given by\nLk(θ,w) = θ ⊤X⊤Dφk\n( 1\n2 Xθ + (IS − γP π)Xw − rπ\n) (34)\nwhich are similar to (23) but with stationary distribution Dφk . In order to find the global saddle-point of the aggregate Lagrangian (33) in a cooperative and stochastic manner, we apply diffusion strategies [19]–[21]. We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45]. The algorithm consists of two-steps: the adaptation step, at which every agent updates its own intermediate estimate independently of the other agents; and the combination step, at which every agent combines its neighbors’ estimates. Similar to the derivation of the single-agent algorithm (29a)–(29b), we can express the gradient of the individual Lagrangians (34) in terms of moment values (i.e., replacing Dφ by Dφk into (26)–(27)). We then follow a primal-dual approach and apply ATC twice: i) for minimizing Lk(θ,w) in (34) over θ through stochastic gradient descent:\nθ̂k,i+1=θk,i − µθxk,i ( x⊤k,iθk,i + δ ⊤ k,i+1wk,i − rk(i+ 1) ) ξk(i)\n(35a)\nθk,i+1= ∑\nl∈Nk clk θ̂l,i+1 (35b)\nand ii) for maximizing Lk(θ,w) in (34) over w through stochastic gradient ascent:\nŵk,i+1 = wk,i + µwδk,i+1x ⊤ k,iθk,iξk(i) (35c) wk,i+1 = ∑\nl∈Nk clk ŵl,i+1 (35d)\nwhere Nk stands for the neighborhood of agent k (i.e., the set of agents that are able to communicate with agent k in a single hop, including k itself), θ̂ and ŵ correspond to the locally adapted estimates, and θ and w correspond to the combined estimates for the adapt-then-combine strategy. The combination coefficients {clk} define the weights on the links in the network and can be chosen freely by the designer, as long as they satisfy:\nclk ≥ 0, ∑\nl∈Nk clk = 1, clk = 0 if l /∈ Nk (36)\nckk > 0 for at least one agent k (37)\nLet C , [clk] be the combination matrix. Then, condition (36) implies that C is left-stochastic. Condition (37) means that there is at least one agent that trusts its local measurements and is able to perform its own\nNovember 6, 2014 DRAFT\n15\nadaptation step. We also assume that the topology of the network is connected (i.e., there is at least one path between any pair of nodes) and that the combination matrix C remains fixed over time. Therefore, conditions (36)–(37) ensure that C is a primitive matrix (i.e., there exists j > 0 such that all entries of Cj are strictly positive) [19], [46]. It follows from the Perron-Frobenius Theorem [47] that C has a unique eigenvalue at one, while all other eigenvalues are strictly inside the unit circle. We normalize the entries of the eigenvector that is associated with the eigenvalue at one to add up to one and refer to it as the Perron eigenvector of C . All its entries will be strictly positive. We we will show in Sec. IV-G and App. B that the values for {τk} turn out to be determined by this Perron eigenvector.\nIterations (35a)–(35d) constitute the proposed diffusion off-policy GTD algorithm, which we remark\nis a fully distributed algorithm because the combination step is taken only over Nk.\nAlgorithm 1: Diffusion off-policy GTD algorithm. This procedure runs in parallel at every node k.\nInputs: Target π and behavior φk policies, neighborhood Nk, weights {clk, l = 1, . . . , N}, and step-sizes µθ, µw Initialize estimates θk,0, wk,0 for every time-step i = 1 to T do\nTake action ak(i) ∼ φk(·|xk,i) Observe feature vector xk,i+1 and reward rk(i+ 1) Perform local adaptation steps (35a) and (35c) Combine in-neighborhood estimates into θk,i+1, wk,i+1 using (35b) and (35d)\nend for Return: wk,T+1\nNovember 6, 2014 DRAFT\n16"
    }, {
      "heading" : "IV. PERFORMANCE ANALYSIS",
      "text" : "In this section we analyze the existence and uniqueness of the optimal solution to the multi-agent learning problem (31). We extend the energy conservation arguments of [16]–[19] to perform a mean-squareerror (MSE) analysis of the diffusion GTD algorithm (35a)–(35d) and provide convergence guarantees under sufficiently small step-sizes. We also obtain closed form expressions of the mean-square-deviation (MSD) and analyze the bias of the algorithm. We will rely on some reasonable conditions on the data, as explained next."
    }, {
      "heading" : "A. Data model",
      "text" : "To begin with, we model the quantities appearing in (35a)–(35d) as instantaneous realizations of random variables, which we denote by using boldface notation. We aggregate the variables into vectors of length 2M each:\nαk,i ,   θk,i\nwk,i\n  , ψk,i ,   θ̂k,i\nŵk,i\n  (38)\ngk,i+1 ,   −ηxk,i · ξk(i) · rk(i+ 1)\n0M\n  (39)\nwhere we are now writing µw , µ and µθ , ηµw, such that η > 0 is the step-size ratio between the two adaptation steps. We further introduce the following 2M × 2M coefficient matrix:\nGk,i+1 ,   ηxk,ix ⊤ k,iξk(i) ηxk,iδ ⊤ k,i+1ξk(i)\n−δk,i+1x⊤k,iξk(i) 0M×M\n  (40)\nThen, the diffusion algorithm (35a)–(35d) with stochastic variables can be expressed as\nψk,i+1 = αk,i − µ (Gk,i+1αk,i + gk,i+1) (41a) αk,i+1 = ∑\nl∈Nk clkψl,i+1 (41b)\nWe assume the following conditions for (41a)–(41b):\nAssumption 1. The state transitions {(sk(i), sk(i+1))} visited by each agent k are i.i.d. samples, with initial states {sk(i)} drawn from the stationary distribution dφk .\nNovember 6, 2014 DRAFT\n17\nAssumption 2. There is some positive probability that every state is visited by at least one agent, thus Dφ in (32) is positive-definite.\nAssumption 3. The feature matrix X and the expected reward signal rπ are bounded from below and from above.\nGiven the sequence of states visited by each agent {sk(1), sk(2), . . . , sk(i), . . .}, the segments that start and end at the same state are independent of one another. When the Markov chain that defines these state transitions has short mixing time, these segments tend to be short (see, e.g., [30]). Assumption 1 approximates these independent segments with sequences of just one step. This is a customary approximation (see, e.g., [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.i.d. samples, rendering Gk,i+1 and gk,i+1 independent of αk,i.\nAssumption 2 refers to a property of the network. For a single-agent algorithm, the agent should visit every state with positive probability; otherwise it may not be able to approach the value function. Here, we impose the milder condition that every state must be visited by at least one agent.\nAssumption 3 holds for most practical implementations, and will be used in the stability analysis."
    }, {
      "heading" : "B. Existence and uniqueness of solution",
      "text" : "Solving the aggregated dual problem (31) is equivalent to finding the saddle-points {wo, θo} of the global Lagrangian (33). A saddle-point of the Lagrangian must satisfy [40]:\nL(θo, wo) = min θ max w L(θ,w) = max w min θ L(θ,w) (42)\nThese conditions are equivalent to the following system of linear equations:\n∇θL(θ,w)=X⊤Dφ (Xθ − rπ + (IS − γP π)Xw) = 0M (43)\n∇wL(θ,w)=X⊤(IS − γP π)⊤DφXθ = 0M (44)\nTo find the saddle-point {θo, wo}, we solve for θ in (44) first. Since Assumption 2 establishes that Dφ has full-rank, we recall from (16)–(18) that X⊤Dφ(γP π−IS)X is invertible and, hence, θo = 0M . Then, substituting θo into (43) yields:\nwo = ( X⊤Dφ (IS − γP π)X )−1 X⊤Dφrπ (45)\nNovember 6, 2014 DRAFT\n18\nEquation (45) therefore illustrates one clear benefit of cooperation. If the behavior policy of some agent prevents him from exploring the entire state-space, then some of the entries of its corresponding dφk will be zero and the agent may be unable to estimate the value vector on its own. Nevertheless, as long as any other agent in the network can visit these unexplored states, the matrix Dφ will be positive-definite, guaranteeing the existence and uniqueness of a solution wo.\nWe remark that the off-policy solution wo in (45) is in fact an approximation to the on-policy solution\nthat the agents wish to predict, which is given by (18) when D , Dπ:\nwπ = ( X⊤Dπ (IS − γP π)X )−1 X⊤Dπrπ (46)\nThat is, the obtained solution (45) is still an approximation of (46) because Dφ̄ is not necessarily the same as Dπ. However, it is interesting to realize that, by using diffusion strategies, the agents can estimate the exact on-policy solution if the scalars {τk} could be set to satisfy N∑\nk=1\nτkd φk = dπ ⇔ wo = wπ (47)\nIn the next subsections, we analyze the conditions that allow diffusion GTD to converge to (45)."
    }, {
      "heading" : "C. Error recursion",
      "text" : "We introduce the following error measures, which measure the difference between the estimates\n{αk,i,ψk,i} at time i and the optimal solution αo = col{θo, wo} for each agent k:\nψ̃k,i , α o −ψk,i (48)\nα̃k,i , α o −αk,i (49)\nThen, subtracting both sides of (41a)–(41b) from αo, we obtain\nψ̃k,i+1=(I2M − µGk,i+1) α̃k,i + µ (Gk,i+1αo + gk,i+1) (50)\nUsing the fact that clk = 0 if l /∈ Nk, the error recursion for the combination step becomes\nα̃k,i = ∑\nl∈Nk clkψ̃l,i =\nN∑\nl=1\nclkψ̃l,i (51)\nWe collect the error variables from across the network into block vectors of size 2MN :\nψ̃i , col{ψ̃1,i, . . . , ψ̃N,i} (52)\nNovember 6, 2014 DRAFT\n19\nα̃i , col{α̃1,i, . . . , α̃N,i} (53)\nLet C and Ri be matrices of size 2MN × 2MN defined by\nC , C ⊗ I2M (54)\nRi , diag{G1,i, . . . ,GN,i} (55)\nand let Gi be the matrix of size 2MN × 2M defined by\nGi , col{G1,i, . . . ,GN,i} (56)\nWe also introduce the vectors of length 2MN :\ngi , col{g1,i, . . . ,gN,i} (57)\nni , Giα o + gi (58)\nThen, the individual error recursions in (48)–(49) lead to the following network recursion:\nα̃i+1 = C⊤ ( I2MN − µRi+1 ) α̃i + µC⊤ni+1 (59)\nThis recursion shows how the error dynamics evolves over the network over time."
    }, {
      "heading" : "D. Convergence in the mean",
      "text" : "Introduce the following expected values for each agent:\nGk , EGk,i =  \nηX⊤DφkX ηX⊤Dφk(IS − γP π)X\n−X⊤(IS − γP π)⊤DφkX 0M×M\n  (60)\ngk , Egk,i =   −ηX⊤Dφkrπ\n0M\n  (61)\nSince Assumption 1 implies that the variables Ri+1 and α̃i are independent of each other, then by taking expectations of both sides of (59) we obtain\nEα̃i+1 = C⊤ ( I2MN − µR ) Eα̃i + µC⊤(Gαo + g) (62)\nNovember 6, 2014 DRAFT\n20\nwhere\nR , ERi = diag{G1, . . . , GN} (63)\nG = EGi = col{G1, . . . , GN} (64)\ng = Egi = col{g1, . . . , gN} (65)\nTherefore, the convergence of (62) is guaranteed when the matrix C⊤(I2MN − µR) is stable.\nTheorem 1 (Mean convergence). For the data model of Section IV-A, there exists small enough step-sizes, say 0 < µ < µo (for some µo > 0 given by (122) in Appendix A), such that the matrix C⊤(I2MN −µR) is stable and, therefore, the mean-error recursion (62) is stable for every agent k = 1, . . . , N and converges to the bias value given by\nα̃∞ , lim i→∞\nEα̃i = ( I2MN − C⊤ (I2MN − µR) )−1 µC⊤ (Gαo + g)\nProof: See Appendix A.\nAs it is explained in Appendix A, the value µo only depends on the inputs of the algorithm, namely, data-samples (state-features and transition rewards), the weighted-topology matrix C , the cost-weights {τk}, and the step-size ratio parameter η."
    }, {
      "heading" : "E. Mean-square stability",
      "text" : "Although the error vector converges in the mean, we still need to ensure that it has bounded fluctuations around its fixed point value. To do so, we study the evolution and steady-state value of the variance E‖α̃i‖2. By computing the weighted squared Euclidean (semi)norm of both sides of (59)— using an arbitrary positive (semi)definite weighting matrix Σ that we are free to choose—and applying the expectation operator, we obtain the following variance relation:\nE‖α̃i+1‖2Σ = E ‖α̃i‖2Σ′ + 2b⊤Σ Eα̃i +Tr ( µ2ΣC⊤RnC ) (66)\nwhere\nΣ′ , (I2MN − µR⊤)CΣC⊤(I2MN − µR) + µ2E [ (Ri+1 −R)⊤CΣC⊤(Ri+1 −R) ] (67) bΣ , µE [( I2MN − µR⊤i+1 ) CΣC⊤ni+1 ] (68)\nRn , E [ nin ⊤ i ] = E [ (Giα o + gi)(Giα o + gi) ⊤ ]\n(69)\nNovember 6, 2014 DRAFT\n21\nLet σ = vec(Σ). Using the Kronecker product property vec(Y ΣZ) = (Z⊤ ⊗ Y )vec(Σ) [39], we can vectorize Σ′ in (67) and find that its vector form is related to Σ via the following linear relation: σ′ , vec(Σ′) = Fσ, where the matrix F is given by\nF , (( I2MN − µR⊤ ) C ) ⊗ (( I2MN − µR⊤ ) C ) + µ2E [( (R⊤i+1 −R⊤)C ) ⊗ ( (R⊤i+1 −R⊤)C )]\n(70)\nFurthermore, using the property Tr(ΣY ) = (vec[Y ⊤])⊤σ, we can rewrite (66) as:\nE‖α̃i+1‖2σ = E‖α̃i‖2Fσ + 2σ⊤U · Eα̃i + h⊤σ (71)\nwhere\nU , µE [ ( C⊤ni+1 ) ⊗ ( C⊤ (I2MN − µRi+1) ) ] (72) h , µ2vec [ C⊤RnC ] (73)\nIn (71) we are using the notation ‖x‖2σ to represent ‖x‖2Σ. Note that (71) is not a true recursion because the weighting matrices corresponding to σ and Fσ are different. Moreover, recursion (71) is coupled with the mean-error recursion (62). To study the convergence of (71) we will expand it into a state-space model following [39], [49]. Let L , 2MN and let p(x) denote the characteristic polynomial of the L2 × L2 matrix F , given by\np(x) , det(xI −F) = xL2 + pL2−1xL 2−1 + . . .+ p0 (74)\nBy the Cayley-Hamilton Theorem [39], we know that every matrix satisfies its characteristic equation (i.e., p(F) = 0), so that\nFL2 = −p0IL2 − p1F − . . .− pL2−1FL 2−1 (75)\nReplacing σ in (71) by F jσ, j = 0, . . . , L2 − 1, we can derive the following state-space model:   E‖α̃i+1‖ 2 σ E‖α̃i+1‖ 2 Fσ\n... E‖α̃i+1‖ 2\nFL 2−1σ\n \n︸ ︷︷ ︸ Wi+1\n=   0 1 0 ··· 0 0 0 1 ··· 0 ... . .. 0\n0 0 0 ··· 1\n−p0 −p1 −p2 ··· −pL2−1\n \n︸ ︷︷ ︸ T\n  E‖α̃i‖ 2 σ E‖α̃i‖ 2 Fσ\n... E‖α̃i‖ 2\nFL 2−1σ\n \n︸ ︷︷ ︸ Wi\n+2\n  σ⊤U σ⊤FU\n... σ⊤FL 2−1U\n \n︸ ︷︷ ︸ Q\nEα̃i +\n  h⊤σ h⊤Fσ\n... h⊤FL 2−1σ\n \n︸ ︷︷ ︸ Y\n(76)\nNovember 6, 2014 DRAFT\n22\nWe combine (76) with the mean-recursion (62) and rewrite them more compactly as:   Wi+1\nEα̃i+1\n  =   T 2Q\n0 C⊤(I2MN − µR)\n    Wi\nEα̃i\n +  \nY\nC⊤Gαo + g\n  (77)\nTheorem 2 (Mean-square stability). Assume the step-size parameter µ is sufficiently small so that terms that depend on higher-order powers of µ can be ignored. Then, for the data model of Section IV-A, there exists 0 < µoMS ≤ µo (for µo used in Theorem 1 and given by (122) in Appendix A), such that when 0 < µ < µoMS, the variance recursion (77) is mean-square stable.\nProof: Observe that the stability of the joint recursion (77) is equivalent to the stability of the\nmatrices T and C⊤(I2MN −µR), which is further equivalent to the following conditions on their spectral radii:\nρ ( C⊤(I2MN − µR) ) < 1, ρ (T ) < 1 (78)\nThe first condition is the same mean-stability condition that was discussed in Theorem 1. For the second condition, we note from (76) that T is in companion form, and it is known that its eigenvalues are the roots of p(x), which are also the eigenvalues of F . Therefore, a necessary and sufficient condition for the stability of T is the stability of the matrix F . When the step-sizes are small enough, the last term in (70) can be ignored since it depends on µ2 and we can write\nF ≈ ( C⊤ (I2MN − µR) )⊤ ⊗ ( C⊤ (I2MN − µR) )⊤ (79)\nwhich is stable if C⊤ (I2MN − µR) is stable. We remark that 0 < µoMS ≤ µo is chosen to dismiss higher-order powers of µ, and that µo (see Appendix A) only depends on the inputs of the algorithm (i.e., data-samples, the weighted-topology matrix C , the weights {τk} and the parameter η)."
    }, {
      "heading" : "F. Mean-square performance",
      "text" : "Taking the limit of both sides of (71) we obtain:\nlim i→∞ E‖α̃i+1‖2σ = lim i→∞ E‖α̃i‖2Fσ + 2σ⊤U lim i→∞ Eα̃i + h ⊤σ (80)\nTheorem 1 guarantees that limi→∞Eα̃i = α̃∞, so the steady-state variance recursion in (80) leads to\nlim i→∞\nE‖α̃i‖2σ = q⊤(I −F)−1σ (81)\nNovember 6, 2014 DRAFT\n23\nwhere q , h+ 2U α̃∞. Result (81) is useful because it allows us to derive several performance metrics through the proper selection of the free weighting parameter vector σ (or, equivalently, the parameter matrix Σ). For example, the network mean-square-deviation (MSD) is defined as the average of the MSD of all the agents in the network:\nMSD network , lim i→∞ 1 N\nN∑\nk=1\nE‖α̃k,i‖2 = lim i→∞ E‖α̃i‖21 N I2MN\n(82)\nChoosing the weighting matrix in (81) as Σ = I2MN/N , we get:\nMSD network =\n1\nN q⊤(I −F)−1vec(I2MN ) (83)\nWe can also obtain the MSD of any particular node k, as\nMSDk , lim i→∞\nE‖α̃i‖2Jk (84)\nwhere Jk is a block-diagonal matrix of N blocks of size 2M × 2M , such that all blocks in the diagonal are zero except for block k which is the identity matrix. Following the same procedure as with the network MSD we obtain\nMSDk = q ⊤(I −F)−1vec(Jk) (85)"
    }, {
      "heading" : "G. Bias analysis",
      "text" : "We showed in (45) that, under Assumption 2, there exists a unique solution αo for the global optimization problem (31). On the other hand, the error recursion (62) converges in the mean-square sense to some bias value α̃∞. Now, we examine under which conditions α̃∞ is small when the step-size is small. The analysis of the bias value α̃∞ in (66) is similar to the examination developed in [18, Theorem 3] for multi-objective optimization. The main difference lies in the fact that we do not assume the matrix R in (63) to be symmetric.\nTheorem 3 (Bias at small step-size). Consider the data model of Section IV-A, where the combination matrix C is primitive left-stochastic (it satisfies (36)–(37)). Suppose the Perron eigenvector that corresponds to the eigenvalue of C at one is equal to the vector of weights {τk} in the global problem (31) (i.e., τ⊤C = τ⊤ and τ⊤1N = 1). Assume further that the step-size µ is sufficiently small to ensure mean-square stability. Then, it holds that\nα̃∞ = O(µ) (86)\nNovember 6, 2014 DRAFT\n24\nProof: See Appendix B.\nWe remark that the bias in (86) comes from agents following different behavior policies, which means that they are solving different optimization problems, with different minimizer each. When they use diffusion strategies, the combination step pulls them toward the global solution. Nevertheless, the adaptation step pushes each agent towards the minimizer of its individual cost function. Note, however, that if all agents followed the same behavior policy, their individual optimization problems would be identical, therefore, both the adaptation and the combination steps would pull them toward the global solution and their fixed-point estimates would be unbiased with respect to the solution of the global optimization problem (31), as stated in [1]. More formally, Gk , Ḡ and gk , ḡ would be the same for every agent k, and the saddle-point conditions of the Lagrangian of the global problem (33) would not depend on the combination weights {τk}: N∑\nk=1\nτk(Ḡα o + ḡ) = Ḡαo + ḡ = 02M\nTherefore, if all the agents followed the same behavioral policy, then Gαo + g = 02MN and, from (66), we would conclude that α̃∞ = 0."
    }, {
      "heading" : "V. SIMULATIONS",
      "text" : "Consider a group of animals foraging in a 2D-world (see Figure 2). The group forms a network of N = 15 agents with arbitrarily connected topology and neighborhood size |N |k varying between 2 and 9. The weights of the links (i.e., the clk elements of C) are obtained independently by each node following an averaging rule [19], [50], such that equal weight is given to any member of the neighborhood, including itself (i.e., clk = 1/|Nk|, l ∈ Nk). Note this rule leads to a left (rather than doubly) stochastic combination matrix C that satisfies (36)–(37). We assume that the combination matrix C—and, hence, the network topology—remains fixed.\nThe world is a discrete, bounded square with 20 rows and 20 columns, which amounts to S = 400 states. Each agent self-localizes itself in the grid by sensing a Gaussian radial basis function of its distance to M = 64 fixed markers (i.e., each of these values is a feature). The agents move in four possible directions, namely, A = {north, south, east, west}. At every time step, the agents move and consume some energy (i.e., they receive some negative reward). In the north-east corner of the world, there is food, which the agents understand as positive reward. However, there is a large area below the food with a predator that is harmful to go through, so agents receive large negative reward if they visit these states (see caption of Figure 2 to see the exact numerical values).\nNovember 6, 2014 DRAFT\n25\nSince the agents are getting negative reward at every time step (because of energy consumption) they want to know how to reach the food, while losing as less energy as possible. A natural policy, denoted π1, could be to go straight to the food with high (0.8) probability and low (0.2) probability of going in another direction, but then the agents would face the harmful predator and the total expected reward may be low. Thus, we say that π1 is a myopic policy. Another more insightful policy, denoted π2, could be to take a detour and avoid the predator’s area with very high (0.95) probability. Nevertheless, if the detour takes too long, then the agents would consume too much energy and it may not be worth trying. In order to evaluate which policy is better (myopic π1 or detour π2), the agents have to learn the value vector of each candidate-policy from samples. If the agents were learning on-policy, they would have to follow one candidate policy for long enough so they could apply stochastic optimization over the samples, then they would have to start again but following the other candidate policy. In other words, on-policy learning does not allow to reuse samples while evaluating different policies. The benefit of the off-policy formulation is that the agents can evaluate several target policies in parallel from a single data stream.\nWe consider the case in which the agents are territorial and tend to settle in different regions each. In other words, the behavior policies of the agents {φk} are all different and constrain exploration to some regions of the state-space. At every time-step, each agent is attracted to the state at the center of its territory with 0.8 probability, and it moves in a different direction with 0.2 probability. Since the agents only have samples of state-transitions in their respective territories, it is difficult for them to predict the value vector for each of the target policies (π1 and π2). However, since they sample complementary regions of the state-space, they can collaborate, applying diffusion strategies, to learn the value vector of the two target policies and evaluate which one is better.\nFigure 3 shows1 the exact value vector for the myopic and detour policies, as well as its cooperative– using the proposed diffusion GTD algorithm–and non-cooperative approximation for one agent, under the considered constrained-exploration off-policy multi-agent setting. Figure 4 shows the learning curve of the algorithm. Since the agents have only samples from small portions of the state-space, the noncooperative algorithm may diverge. On the other hand, when the agents cooperate (i.e., communicate their estimates to their neighbors), the diffusion algorithm allows them to benefit from the experience from other agents in the network, and they approach the same solution as a centralized architecture (i.e., with a fusion center that gathers all the samples from every node) would achieve but more efficiently,\n1Code available at http://gaps.ssr.upm.es/images/sergio/tsp-coop-pred-20131004.zip\nNovember 6, 2014 DRAFT\n26\nby communicating only within neighborhoods."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "Diffusion GTD maintains the efficiency of the single-agent GTD2 [9], with linear complexity in both computation time and memory footprint. With diffusion GTD, the agents learn directly from samples (without any apriori knowledge of the environment) and cooperate to improve the stability and accuracy of their prediction. We remark that cooperation is fully distributed with communications only within each agent’s neighborhood; neither fusion-center, nor multi-hop communications are required.\nWe provided conditions that guarantee convergence of the proposed diffusion GTD and derived performance bounds for sufficiently small step-sizes. Although our analysis assumes stationarity, constant step-sizes are a desirable feature for an adaptive network, since it allows the network to learn continuously, and to track concept drifts in the data."
    }, {
      "heading" : "APPENDIX A",
      "text" : "PROOF OF THEOREM 1\nTo study the spectrum of C⊤ (I2MN − µR), we express the combination coefficient matrix in its Jordan canonical form:\nC⊤ = YCJCY −1 C (87)\nNovember 6, 2014 DRAFT\n27\nUsing the property (U ⊗ V )(Y ⊗ Z) = (UY )⊗ (V Z), we obtain\nC⊤(I2MN − µR) = (YC ⊗ I2M )(JC ⊗ I2M )(Y −1C ⊗ I2M ) ( I2MN − µR )\n= (YC ⊗ I2M )(JC ⊗ I2M ) ( I2MN − µ(Y −1C ⊗ I2M )R(YC ⊗ I2M ) ) (Y −1C ⊗ I2M ) (88)\nso that, by similarity,\nλ ( C⊤ (I2MN − µR) ) = λ ((JC ⊗ I2M ) (I2MN − µE)) (89)\nwhere\nE , (Y −1C ⊗ I2M )R(YC ⊗ I2M ) (90)\nNovember 6, 2014 DRAFT\n28\nAs stated in Section III, conditions (36)–(37) ensure that C⊤ is a primitive right-stochastic matrix. Hence, from the Perron-Frobenius Theorem [47], the Jordan canonical form of C can be expressed as\nJC = diag { 1, J0C } (91)\nwhere all the eigenvalues of J0C are strictly inside the unit circle. Moreover, since C ⊤ is right-stochastic it has one right-eigenvector of all ones associated with its unit eigenvalue, and its corresponding left eigenvector, p, has positive entries (i.e., pk > 0, 1 ≤ k ≤ N ):\nC⊤1N = 1N , p⊤C⊤ = p⊤, 1⊤Np = 1 (92)\nWe therefore decompose\nY −1C = col { p⊤, Y lC } , YC = [1N Y r C ] (93)\nand partition E as\nE =   Ḡ E12\nE21 E22\n  (94)\nNovember 6, 2014 DRAFT\n29\nḠ , ( p⊤ ⊗ I2M ) R (1N ⊗ I2M ) = N∑\nk=1\npkGk (95)\nE12 , ( p⊤ ⊗ I2M ) R (Y rC ⊗ I2M ) (96) E21 , ( Y lC ⊗ I2M ) R (1N ⊗ I2M ) (97) E22 , ( Y lC ⊗ I2M ) R (Y rC ⊗ I2M ) (98)\nIntroduce the following shorthand in (89):\nS , (JC ⊗ I2M ) (I2MN − µE) (99)\nThen, expanding (91) and (94)-(98) into (99) we have\nS =  \nI2M − µḠ −µE12\n−µ ( J0C ⊗ I2M ) E21 ( J0C ⊗ I2M ) (IL − µE22)\n  (100)\nwhere L , 2M(N − 1). Using the same technique proposed in [51], [52], we appeal to eigenvalue perturbation analysis to examine the spectral radius of (100). We introduce the N ×N diagonal matrix ΩǫN , diag{ǫ, ǫ2, ǫ3, . . . , ǫN} with parameter ǫ > 0. Let JḠ = Y −1Ḡ ḠYḠ be the Jordan canonical form of Ḡ and introduce the similarity transformation\nΦ =   YḠΩ ǫ 2M 02M×L\n0L×2M √ µ σ ΩβN−1 ⊗ I2M\n  (101)\nwith parameters ǫ, β, and σ. We apply the similarity transformation (101) to S:\nΦ−1SΦ =   I2M − µJ ǫḠ − µ √ µ σ S12\n−σ√µ S21 J0βC ⊗ I2M − µS22\n  (102)\nwhere\nS12 , (Ωǫ2M )−1Y −1Ḡ E12 ( ΩβN−1 ⊗ I2M ) (103) S21 , (( (ΩβN−1) −1J0C ) ⊗ I2M ) E21YḠΩǫ2M (104) S22 , (( (ΩβN−1) −1J0C ) ⊗ I2M ) E22 ( ΩβN−1 ⊗ I2M ) (105)\nand J ǫ Ḡ and J0βC have the same form as the (upper triangular) Jordan canonical forms JḠ and J 0 C , except that the unit entries are replaced by ǫ and β, respectively. By applying Gerschgorin theorem [47] to (102),\nNovember 6, 2014 DRAFT\n30\nwe can identify the regions where the eigenvalues of S should lie: ∣∣∣λ(S)− ( 1− µλm(Ḡ) )∣∣∣ ≤ µǫ+ µ √ µ\nσ\nL∑\nq=1\n∣∣∣[S12]mq ∣∣∣ (106)\n∣∣∣λ(S)− ( λk+1(C)− µ [S22]mm )∣∣∣ ≤ β + σ√µ 2M∑\nq=1\n∣∣∣[S21]mq ∣∣∣+ µ L∑\nq=1 q 6=m\n∣∣∣[S22]mq ∣∣∣ (107)\nwhere [·]mq stands for the element at row m and column q of a matrix. Although we use the same subscript m in both equations, note that 1 ≤ m ≤ 2M in (106) while 1 ≤ m ≤ L in (107); in addition, recall that C is an N ×N matrix, hence, we use subscript k = ceil{m/(2M)}, where ceil{·} rounds a real number to its nearest greater or equal integer.\nWe are looking for sufficient conditions that guarantee that the mean recursion (62) converges for small step-size. Recall from (89) that (62) converges when |λ(S)| < 1. Let us solve for |λ(S)| in (106) first. Since |z| − |y| ≤ |z − y|, we obtain\n|λ(S)| − ∣∣1− µλm(Ḡ) ∣∣ ≤ ∣∣λ(S)− (1− µλm(Ḡ)) ∣∣ ≤ µǫ+ µ √ µ\nσ χ(12) (108)\nwhere χ(12) , ∑L\nq=1 ∣∣∣[S12]mq ∣∣∣. Therefore,\n|λ(S)| ≤ µǫ+ µ √ µ\nσ χ(12) +\n∣∣1− µλm(Ḡ) ∣∣ (109)\nSince S , Ḡ and C are not generally guaranteed to be symmetric, their eigenvalues may be complex. Using the fact that 1− z ≤ ( 1− 12z )2 for z ∈ R, we obtain\n∣∣1− µλm(Ḡ) ∣∣ ≤ 1− µRe{λm(Ḡ)}+ µ2\n2\n∣∣λm(Ḡ) ∣∣2 (110)\nwhere Re{·} denotes the real part of a complex number. Combining (109) and (110), the stability condition implied by (106) requires finding small step-sizes µ such that\nµ |λm(Ḡ)|2\n2 +\n√ µ χ(12)\nσ + ǫ− Re{λm(Ḡ)} < 0 (111)\nwhich leads to\n0< µ <\n − χ(12) σ + √(χ(12) σ )2 + 2|λm(Ḡ)|2(Re{λm(Ḡ)} − ǫ) |λm(Ḡ)|2   2\n(112)\nwhere, in order to guarantee that the term inside the square root in the right side of (112) is positive, we\nNovember 6, 2014 DRAFT\n31\nchoose\n0 < ǫ < min 1≤m≤2M\nRe{λm(Ḡ)} (113)\nWe now show that Re{λm(Ḡ)} is always positive. If we transform Ḡ into a similar matrix:\nḠ√η ,\n  IM 0M×M\n0M×M √ ηIM\n  Ḡ   IM 0M×M\n0M×M 1√ η IM\n  =  \n√ ηX⊤DφX X⊤Dφ(IS − γPπ)X\n−X(IS − γPπ)⊤DφX⊤ 0M×M\n  (114)\nand use [41, Theorem 3.6] on Ḡ√η, we can establish that Re{λm(Ḡ)} > 0. Now, we solve for |λ(S)| from (107). Let us abbreviate the sums in the right side of (107) as\nχ(21) ,\n2M∑\nq=1\n|[S21]mq| , χ(22) , L∑\nq=1 q 6=m\n|[S22]mq| (115)\nIn a manner similar to (108), we have\n|λ(S)| − |λk+1(C)− µ[S22]mm| ≤ β + σ √ µχ(21) + µχ(22) (116)\nUsing (116) and the fact |z − y| ≤ |z|+ |y| yields the following condition on µ for stability:\n√ µ (√ µ ( χ(22) + |[S22]mm| ) +σχ(21) ) < 1− |λk+1(C)| − β (117)\nThe following conditions on the step-size are jointly sufficient to satisfy (117):\n0 < √ µ < 1− |λk+1(C)| − β (118)\n0 < √ µ < 1− σχ(21) χ(22) + |[S22]mm|\n(119)\nFrom the Perron-Frobenius, we know that λk+1(C) < 1, for 1 ≤ k ≤ N − 1. Moreover, Assumption 3 guarantees that any element of S is bounded from below and above. Therefore, there exist parameters 0 < β < 1 − |λk+1(C)| and 0 < σ < 1/χ(21) that make the right side of (118) and (119) positive, respectively. Hence, we can square both inequalities and obtain the following conditions:\n0 < µ < (1− |λk+1(C)| − β)2 (120)\n0 < µ <\n( 1− σχ(21)\nχ(22) + |[S22]mm|\n)2 (121)\nNovember 6, 2014 DRAFT\n32\nLet us bound (112), (120) and (121) by\nµo = min    (1− |λk+1(C)| − β)2 ,   1− σχ(21) χ(22) + |[S22]jj|   2 ,\n − χ(12) σ + √(χ(12) σ )2 + 2|λm(Ḡ)|2(Re{λm(Ḡ)} − ǫ) |λm(Ḡ)|2   2   \n(122)\nfor 1 ≤ m ≤ 2M , 1 ≤ k ≤ N − 1 and 1 ≤ j ≤ L. We conclude that if the step-size 0 < µ < µo, then diffusion GTD is mean-stable.\nAs a final remark, note that µo depends on the eigenvalues of Ḡ, the eigenvalues of the weightedtopology matrix C , and the constructed matrix S in (100) (note that σ, ǫ and β are similarity parameters, and the terms χ(22) and χ(22) are defined in (115) simply as a short hand of sums of the elements in S). Recall that Ḡ is given by (95) as the weighted sum of the individual Gk, which only depend on the data samples, the importance weights and the step-size ratio parameter η. Finally, recall that S depends on the Jordan canonical form of C and R, where the latter is defined in (63) from the individual Gk. Thus, all the terms involved in µo are input data to the algorithm."
    }, {
      "heading" : "APPENDIX B",
      "text" : "PROOF OF THEOREM 3\nWe follow an argument similar to [18], [53]. It suffices to show that limµ→0 ‖α̃∞‖\nµ = εo, where εo is\na constant independent of µ. Substituting (88), (90) and (100) into (66) yields\nα̃∞ = µ (YC ⊗ I2M ) (I2MN − S)−1 (Y −1C ⊗ I2M )C⊤ (Gαo + g) (123)\nExpanding (54), (87) and (93) into (123) leads to\nα̃∞ = µ(YC ⊗ I2M )(I2MN − S)−1(JC ⊗ I2M )   ( p⊤ ⊗ I2M ) (Gαo + g)\n( Y lC ⊗ I2M ) (Gαo + g)\n  (124)\nFrom now on, assume that the weights used in (31) in defining the global cost are the entries of the Perron eigenvector of C , i.e., p , τ . Then, the first row of the last term in (124) stands for the saddle-point conditions of the Lagrangian of the global problem (33):\n(τ⊤⊗I2M )(Gαo + g) = N∑\nk=1\nτk (Gkα o + gk) =\n[ ηX⊤Dφ(Xθo + (IS − γPπ)Xwo − rπ)\n−X⊤(IS − γPπ)⊤DφXθo\n] = 02M (125)\nNovember 6, 2014 DRAFT\n33\nTherefore, expanding JC into (124) yields\nα̃∞ = µ(YC ⊗ I2M )(I2MN − S)−1  \n02M\n(J0C ⊗ I2M )(Y lC ⊗ I2M ) (Gαo + g)\n  (126)\nFrom (100), we know that the upper-left block of I2MN − S is given by µḠ. By using [41, Theorem 3.6], we establish that the similar matrix Ḡ√η in (114) is invertible. Therefore, Ḡ is also invertible so we can use the following relation\n[ Z11 Z12\nZ21 Z22\n]−1 = [ Z−1 11 + Z−1 11 Z12UZ21Z −1 11 −Z−1 11 Z12U\n−UZ21Z−111 U\n]\nwhere U = (Z22 − Z21Z−111 Z12)−1, to write\n(I2MN − S)−1 =   H11 H12\nH21 H22\n  (127)\nIn this way, relation (126) simplifies to\nα̃∞ = µ (YC ⊗ I2M )   H12(J 0 CY l C ⊗ I2M )(Gαo + g)\nH22(J 0 CY l C ⊗ I2M )(Gαo + g)\n  (128)\nThe only terms in (128) that depend on µ are H12 and H22. In the limit, when µ → 0 these two terms become independent of µ:\nlim µ→0\nH12 = −Ḡ−1E12(I2MN − J0C ⊗ I2M )−1 (129)\nlim µ→0\nH22 = (I2MN − J0C ⊗ I2M )−1 (130)\nHence, we can conclude that limµ→0 ‖α̃∞‖\nµ = εo."
    } ],
    "references" : [ {
      "title" : "Cooperative off-policy prediction of Markov decision processes in adaptive networks",
      "author" : [ "S.V. Macua", "J. Chen", "S. Zazo", "A.H. Sayed" ],
      "venue" : "Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP), Vancouver, British Columbia, Canada, May 2013, pp. 4539–4543.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1998
    }, {
      "title" : "Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "author" : [ "R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup" ],
      "venue" : "Proc. Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS), vol. 2, Taipei, Taiwan, 2011, pp. 761–768. November 6, 2014  DRAFT  34",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Scaling-up knowledge for a cognizant robot",
      "author" : [ "T. Degris", "J. Modayil" ],
      "venue" : "Notes AAAI Spring Symposium Series, Palo Alto, CA, USA, 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multi-timescale nexting in a reinforcement learning robot",
      "author" : [ "J. Modayil", "A. White", "R.S. Sutton" ],
      "venue" : "Adaptive Behavior, vol. 22, no. 2, pp. 146–160, 2014.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "author" : [ "L. M" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1994
    }, {
      "title" : "Dynamic Programming and Optimal Control, 4th ed",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "A convergent O(n) temporal-difference algorithm for off-policy learning with linear function approximation",
      "author" : [ "R.S. Sutton", "C. Szepesvari", "H.R. Maei" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NIPS) 21, Vancouver, British Columbia, Canada, 2008, pp. 1609–1616.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesvari", "E. Wiewiora" ],
      "venue" : "Proc. Int. Conf. on Machine Learning (ICML), Montreal, Quebec, Canada, 2009, pp. 993–1000.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Distributed asynchronous deterministic and stochastic gradient optimization algorithms",
      "author" : [ "J. Tsitsiklis", "D. Bertsekas", "M. Athans" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 31, no. 9, pp. 803–812, 1986.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Distributed subgradient methods for multi-agent optimization",
      "author" : [ "A. Nedic", "A. Ozdaglar" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 54, no. 1, pp. 48–61, 2009.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Convergence rate analysis of distributed gossip (linear parameter) estimation: Fundamental limits and tradeoffs",
      "author" : [ "S. Kar", "J. Moura" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing, vol. 5, no. 4, pp. 674–690, 2011.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Decentralized parameter estimation by consensus based stochastic approximation",
      "author" : [ "S. Stankovic", "M. Stankovic", "D. Stipanovic" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 56, no. 3, pp. 531–543, 2011.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Consensus problems in networks of agents with switching topology and time-delays",
      "author" : [ "R. Olfati-Saber", "R. Murray" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 49, pp. 1520–1533, Sep 2004.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis",
      "author" : [ "C. Lopes", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3122 –3136, July 2008.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Diffusion LMS Strategies for Distributed Estimation",
      "author" : [ "F.S. Cattivelli", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 58, no. 3, pp. 1035–1048, March 2010.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Diffusion adaptation strategies for distributed optimization and learning over networks",
      "author" : [ "J. Chen", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 60, no. 8, pp. 4289–4305, Aug. 2012.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Distributed Pareto optimization via diffusion strategies",
      "author" : [ "——" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing, vol. 7, no. 2, pp. 205–220, Apr. 2013.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Diffusion adaptation over networks",
      "author" : [ "A.H. Sayed" ],
      "venue" : "Academic Press Library in Signal Processing, R. Chellapa and S. Theodoridis, Eds. Elsevier, 2014, vol. 3, pp. 323–454. Also available as arXiv:1205.4220v1, May 2012.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Diffusion strategies for adaptation and learning over networks",
      "author" : [ "A.H. Sayed", "S.-Y. Tu", "J. Chen", "X. Zhao", "Z.J. Towfic" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 30, no. 3, pp. 155–171, May 2013.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Adaptive networks",
      "author" : [ "A.H. Sayed" ],
      "venue" : "Proceedings of the IEEE, vol. 102, no. 4, pp. 460–497, April 2014.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Diffusion strategies outperform consensus strategies for distributed estimation over adaptive networks",
      "author" : [ "S.-Y. Tu", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 60, no. 12, pp. 6217–6234, 2012.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "GQ(λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces",
      "author" : [ "H.R. Maei", "R.S. Sutton" ],
      "venue" : "Proc. Conference on Artificial General Intelligence (AGI), vol. 1, Lugano, Switzerland, 2010, pp. 91–96. November 6, 2014  DRAFT  35",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "QD-learning: A collaborative distributed strategy for multi-agent reinforcement learning through consensus + innovations",
      "author" : [ "S. Kar", "J.M.F. Moura", "H.V. Poor" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 61, no. 7, pp. 1848–1862, 2013.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1848
    }, {
      "title" : "Residual algorithms: Reinforcement learning with function approximation",
      "author" : [ "L. Baird" ],
      "venue" : "Proc. Int. Conf. on Machine Learning (ICML), Tahoe City, CA, USA, 1995, pp. 30–37.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "An analysis of temporal-difference learning with function approximation",
      "author" : [ "J.N. Tsitsiklis", "B. Van Roy" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 42, no. 5, pp. 674–690, 1997.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "The Borkar-Meyn theorem for asynchronous stochastic approximations",
      "author" : [ "S. Bhatnagar" ],
      "venue" : "Systems and Control Letters, vol. 60, no. 7, pp. 472–478, 2011.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Distributed value functions",
      "author" : [ "J. Schneider", "W.-K. Wong", "A. Moore", "M. Riedmiller" ],
      "venue" : "Proc. Int. Conf. on Machine Learning (ICML), Bled, Slovenia, 1999, pp. 371–378.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Efficient distributed reinforcement learning through agreement",
      "author" : [ "P. Varshavskaya", "L. Kaelbling", "D. Rus" ],
      "venue" : "Distributed Autonomous Robotic Systems 8, H. Asama, H. Kurokawa, J. Ota, and K. Sekiyama, Eds. Springer Berlin Heidelberg, 2009, pp. 367–378.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Should one compute the temporal difference fix point or minimize the Bellman residual? The unified oblique projection view",
      "author" : [ "B. Scherrer" ],
      "venue" : "Proc. Int. Conf. on Machine Learning (ICML), Haifa, Israel, 2010, pp. 959–966.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Parametric value function approximation: A unified view",
      "author" : [ "M. Geist", "O. Pietquin" ],
      "venue" : "IEEE Symp. on Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), Paris, France, 2011, pp. 9–16.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Basis function adaptation in temporal difference reinforcement learning",
      "author" : [ "I. Menache", "S. Mannor", "N. Shimkin" ],
      "venue" : "Annals of Operations Research, vol. 134, pp. 215–238, 2005.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning",
      "author" : [ "R. Parr", "L. Li", "G. Taylor", "C. Painter-Wakefield", "M. Littman" ],
      "venue" : "Proc. Int. Conf. on Machine Learning (ICML), Helsinki, Finland, 2008, pp. 752–759.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Basis function adaptation methods for cost approximation in MDP",
      "author" : [ "H. Yu", "D.P. Bertsekas" ],
      "venue" : "Proc. IEEE Symp. on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), Nashville, TN, USA, 2009, pp. 74–81.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning representation and control in Markov decision processes: New frontiers",
      "author" : [ "S. Mahadevan" ],
      "venue" : "Foundations and Trends in Machine Learning, vol. 1, no. 4, pp. 403–565, Apr. 2009.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Predictive state temporal difference learning",
      "author" : [ "B. Boots", "G.J. Gordon" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NIPS) 23, 2010, pp. 271–279.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Sketch-based linear value function approximation",
      "author" : [ "M.G. Bellemare", "J. Veness", "M. Bowling" ],
      "venue" : "Proc. Advances in Neural Information Processing Systems (NIPS) 25, 2012, pp. 2222–2230.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Adaptive Filters",
      "author" : [ "A.H. Sayed" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2008
    }, {
      "title" : "Numerical solution of saddle point problems",
      "author" : [ "M. Benzi", "G.H. Golub", "J. Liesen" ],
      "venue" : "Acta Numerica, vol. 14, pp. 1–137, 2005.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Introduction to Optimization",
      "author" : [ "B.T. Polyak" ],
      "venue" : "Optimization Software Inc.,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 1987
    }, {
      "title" : "Studies in Linear and Non-linear Programming",
      "author" : [ "K.J. Arrow", "L. Hurwicz", "H. Uzawa" ],
      "venue" : null,
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1958
    }, {
      "title" : "On the limiting behavior of distributed optimization strategies",
      "author" : [ "J. Chen", "A.H. Sayed" ],
      "venue" : "Proc. Annual Allerton Conference on Communication, Control, and Computing, Monticello, IL, USA, October 2012, pp. 1535–1542.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Non-negative Matrices and Markov Chains",
      "author" : [ "E. Seneta" ],
      "venue" : null,
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2006
    }, {
      "title" : "The O.D.E. method for convergence of stochastic approximation and reinforcement learning",
      "author" : [ "V.S. Borkar", "S. Meyn" ],
      "venue" : "SIAM Journal on Control and Optimization, vol. 38, pp. 447–469, 1999.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Transient analysis of data-normalized adaptive filters",
      "author" : [ "T.Y. Al-Naffouri", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 51, no. 3, pp. 639–652, 2003.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Convergence in multiagent coordination, consensus, and flocking",
      "author" : [ "V. Blondel", "J. Hendrickx", "A. Olshevsky", "J. Tsitsiklis" ],
      "venue" : "Proc. IEEE Conf. on Decision and Control, and European Control Conf. (CDC-ECC), Seville, Spain, 2005, pp. 2996– 3000.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Asynchronous adaptation and learning over networks — Part II: Performance analysis",
      "author" : [ "X. Zhao", "A.H. Sayed" ],
      "venue" : "submitted for publication. Also available as arXiv:1312.5438, Dec. 2013.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The learning behavior of adaptive networks — Part I: Transient analysis",
      "author" : [ "J. Chen", "A.H. Sayed" ],
      "venue" : "submitted for publication. Also available as arXiv:1312.7581, Dec. 2013.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Performance limits for distributed estimation over LMS adaptive networks",
      "author" : [ "X. Zhao", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 60, no. 10, pp. 5107–5124, 2012. November 6, 2014  DRAFT",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "A short preliminary version dealing with a special case of this work appears in the conference publication [1].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 1,
      "context" : "This problem of predicting the response to a target policy different from the behavior policy is commonly referred as off-policy learning [2].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "Off-policy learning has been claimed to be necessary when the agents need to perform tasks in complex environments because they could perform many different predictions in parallel from a single stream of data [3]–[5].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 4,
      "context" : "Off-policy learning has been claimed to be necessary when the agents need to perform tasks in complex environments because they could perform many different predictions in parallel from a single stream of data [3]–[5].",
      "startOffset" : 214,
      "endOffset" : 217
    }, {
      "referenceID" : 1,
      "context" : "The predictions by the agents are made in the form of value functions [2], [6], [7].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "The predictions by the agents are made in the form of value functions [2], [6], [7].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "The predictions by the agents are made in the form of value functions [2], [6], [7].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "It was originally proposed for the single agent scenario in [8], [9], and derived by means of the stochastic optimization of a suitable cost function.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "It was originally proposed for the single agent scenario in [8], [9], and derived by means of the stochastic optimization of a suitable cost function.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "There are several distributed strategies that can be used for this purpose, such as consensus [10]–[14] and diffusion strategies [15]–[18].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : "There are several distributed strategies that can be used for this purpose, such as consensus [10]–[14] and diffusion strategies [15]–[18].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "There are several distributed strategies that can be used for this purpose, such as consensus [10]–[14] and diffusion strategies [15]–[18].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 17,
      "context" : "There are several distributed strategies that can be used for this purpose, such as consensus [10]–[14] and diffusion strategies [15]–[18].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 18,
      "context" : "There are several forms of diffusion; recent overviews appear in [19]–[21].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 20,
      "context" : "There are several forms of diffusion; recent overviews appear in [19]–[21].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "It has been shown in [22] that the dynamics of diffusion networks leads to enhanced stability and lower mean-",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 19,
      "context" : "In particular, the analysis in [20]–[22] shows that consensus networks combine local data and in-neighborhood information asymmetrically, which can make the state of consensus networks grow unbounded even when all individual agents are mean stable in isolation.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 21,
      "context" : "In particular, the analysis in [20]–[22] shows that consensus networks combine local data and in-neighborhood information asymmetrically, which can make the state of consensus networks grow unbounded even when all individual agents are mean stable in isolation.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "As a byproduct of this derivation, we show that the GTD algorithm, motivated as a two time-scales stochastic approximation in [9], is indeed a stochastic Arrow-Hurwicz algorithm applied to the dual problem of the original formulation.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : "The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23].",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 22,
      "context" : "The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23].",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 23,
      "context" : "For example, the work in [24] proposes a useful algorithm, named QD-learning, which is a distributed implementation of Q-learning using consensus-based stochastic approximation.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 23,
      "context" : "However, QD-learning is developed in [24] under the assumption of perfect-knowledge of the state.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 24,
      "context" : "Here, we study the case in which the agents only know a feature representation of the state, which is used to build a parametric approximation of the value function, allowing us to tackle large problems, for which Q-learning schemes can diverge [25], [26].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 25,
      "context" : "Here, we study the case in which the agents only know a feature representation of the state, which is used to build a parametric approximation of the value function, allowing us to tackle large problems, for which Q-learning schemes can diverge [25], [26].",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 23,
      "context" : "In comparison, the analysis in [24] employs a diminishing step-size that dies out as time progresses and, therefore, turns off adaptation and is not able to track concept drifts in the data.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : "Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26].",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 24,
      "context" : "Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26].",
      "startOffset" : 305,
      "endOffset" : 309
    }, {
      "referenceID" : 25,
      "context" : "Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26].",
      "startOffset" : 311,
      "endOffset" : 315
    }, {
      "referenceID" : 26,
      "context" : "In addition, although the algorithm in [27] is distributed, in the sense that there is no fusion center, it requires full connectivity (i.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 27,
      "context" : "Other related—but more heuristic—approaches include [28], [29].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 28,
      "context" : "Other related—but more heuristic—approaches include [28], [29].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "Markov decision processes (MDP) We consider Markov decision processes (MDP) [6], [7] that are characterized by a finite set of states S of size S , |S|; a finite set of actions A; the kernel of transition probabilities P(s′|s, a), which gives the probability of going from one state s to another state s′, given an action a; and the reward function r : S ×A× S → R that the agent wants to predict, which is associated with every transition, such that r (s, a, s′) denotes the reward received by a generic agent for the transition from s to s′ after taking action a.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "Markov decision processes (MDP) We consider Markov decision processes (MDP) [6], [7] that are characterized by a finite set of states S of size S , |S|; a finite set of actions A; the kernel of transition probabilities P(s′|s, a), which gives the probability of going from one state s to another state s′, given an action a; and the reward function r : S ×A× S → R that the agent wants to predict, which is associated with every transition, such that r (s, a, s′) denotes the reward received by a generic agent for the transition from s to s′ after taking action a.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "Value function In order to make predictions of the reward signal, we use state value functions, v : S → R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]–[7].",
      "startOffset" : 214,
      "endOffset" : 217
    }, {
      "referenceID" : 4,
      "context" : "Value function In order to make predictions of the reward signal, we use state value functions, v : S → R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]–[7].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 6,
      "context" : "Value function In order to make predictions of the reward signal, we use state value functions, v : S → R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]–[7].",
      "startOffset" : 223,
      "endOffset" : 226
    }, {
      "referenceID" : 1,
      "context" : "Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = Eπ,P [r(i+ 1) + γr(i+ 2) + .",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = Eπ,P [r(i+ 1) + γr(i+ 2) + .",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = Eπ,P [r(i+ 1) + γr(i+ 2) + .",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "Approximate value function as a saddle-point problem For the single agent scenario, references [8], [9] introduced efficient algorithms with convergence guarantees under general conditions.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "Approximate value function as a saddle-point problem For the single agent scenario, references [8], [9] introduced efficient algorithms with convergence guarantees under general conditions.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 25,
      "context" : ", [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 29,
      "context" : ", [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 30,
      "context" : ", [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 31,
      "context" : ", [33]–[38] ).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 36,
      "context" : ", [33]–[38] ).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 25,
      "context" : "To address this issue, one approach is to solve instead the projected Bellman equation [26]:",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "[9] considered the weighted least-squares problem:",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 37,
      "context" : "Using (15), it can also be verified that the solution w that minimizes JPB(w) satisfies the following normal equations [39]: B⊤(X⊤DX)−1Bw⋆ = B⊤(X⊤DX)−1X⊤Drπ (16) Since ‖P ‖∞ = 1 and γ < 1, we can bound the spectral radius of γP π by ρ(γP ) ≤ ‖γP ‖∞ = γ < 1 (17) Thus, the inverse (IS − γP π)−1 exists.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "In the process of doing so, first for single-agents, we shall arrive at the same gradient temporal difference method of [9] albeit by using a fundamentally different approach involving a primal-dual argument.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 38,
      "context" : ", [41], [42, Ch.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 22,
      "context" : "By using importance sampling, reference [23] showed that we can write the gradient inside (24a) in terms of moment values of the behavior policy as follows: X⊤Dφ (Xθi + (IS − γP )Xwi − r)",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 8,
      "context" : "Recursions (29a)–(29b) coincide with the single-agent gradienttemporal difference (GTD2) algorithm, which was derived in [9] using a different approach.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 20,
      "context" : "Motivated by recent results on network behavior in [21], [45], we note that, through collaboration, each agent may contribute to the network with its own experience.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 41,
      "context" : "Motivated by recent results on network behavior in [21], [45], we note that, through collaboration, each agent may contribute to the network with its own experience.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "In order to find the global saddle-point of the aggregate Lagrangian (33) in a cooperative and stochastic manner, we apply diffusion strategies [19]–[21].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 20,
      "context" : "In order to find the global saddle-point of the aggregate Lagrangian (33) in a cooperative and stochastic manner, we apply diffusion strategies [19]–[21].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 16,
      "context" : "We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 41,
      "context" : "We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 18,
      "context" : ", there exists j > 0 such that all entries of C are strictly positive) [19], [46].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 42,
      "context" : ", there exists j > 0 such that all entries of C are strictly positive) [19], [46].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "We extend the energy conservation arguments of [16]–[19] to perform a mean-squareerror (MSE) analysis of the diffusion GTD algorithm (35a)–(35d) and provide convergence guarantees under sufficiently small step-sizes.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 18,
      "context" : "We extend the energy conservation arguments of [16]–[19] to perform a mean-squareerror (MSE) analysis of the diffusion GTD algorithm (35a)–(35d) and provide convergence guarantees under sufficiently small step-sizes.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : ", [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 8,
      "context" : ", [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 43,
      "context" : ", [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 37,
      "context" : "Using the Kronecker product property vec(Y ΣZ) = (Z⊤ ⊗ Y )vec(Σ) [39], we can vectorize Σ′ in (67) and find that its vector form is related to Σ via the following linear relation: σ′ , vec(Σ′) = Fσ, where the matrix F is given by F , (( I2MN − μR⊤ ) C ) ⊗ (( I2MN − μR⊤ ) C ) + μE [( (Ri+1 −R⊤)C ) ⊗ ( (Ri+1 −R⊤)C )]",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 37,
      "context" : "To study the convergence of (71) we will expand it into a state-space model following [39], [49].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 44,
      "context" : "To study the convergence of (71) we will expand it into a state-space model following [39], [49].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 37,
      "context" : "By the Cayley-Hamilton Theorem [39], we know that every matrix satisfies its characteristic equation (i.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "Note, however, that if all agents followed the same behavior policy, their individual optimization problems would be identical, therefore, both the adaptation and the combination steps would pull them toward the global solution and their fixed-point estimates would be unbiased with respect to the solution of the global optimization problem (31), as stated in [1].",
      "startOffset" : 361,
      "endOffset" : 364
    }, {
      "referenceID" : 18,
      "context" : ", the clk elements of C) are obtained independently by each node following an averaging rule [19], [50], such that equal weight is given to any member of the neighborhood, including itself (i.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 45,
      "context" : ", the clk elements of C) are obtained independently by each node following an averaging rule [19], [50], such that equal weight is given to any member of the neighborhood, including itself (i.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "CONCLUSION Diffusion GTD maintains the efficiency of the single-agent GTD2 [9], with linear complexity in both computation time and memory footprint.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 46,
      "context" : "Using the same technique proposed in [51], [52], we appeal to eigenvalue perturbation analysis to examine the spectral radius of (100).",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 47,
      "context" : "Using the same technique proposed in [51], [52], we appeal to eigenvalue perturbation analysis to examine the spectral radius of (100).",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "APPENDIX B PROOF OF THEOREM 3 We follow an argument similar to [18], [53].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 48,
      "context" : "APPENDIX B PROOF OF THEOREM 3 We follow an argument similar to [18], [53].",
      "startOffset" : 69,
      "endOffset" : 73
    } ],
    "year" : 2014,
    "abstractText" : "We apply diffusion strategies to develop a fully-distributed cooperative reinforcement learning algorithm in which agents in a network communicate only with their immediate neighbors to improve predictions about their environment. The algorithm can also be applied to off-policy learning, meaning that the agents can predict the response to a behavior different from the actual policies they are following. The proposed distributed strategy is efficient, with linear complexity in both computation time and memory footprint. We provide a mean-square-error performance analysis and establish convergence under constant step-size updates, which endow the network with continuous learning capabilities. The results show a clear gain from cooperation: when the individual agents can estimate the solution, cooperation increases stability and reduces bias and variance of the prediction error; but, more importantly, the network is able to approach the optimal solution even when none of the individual agents can (e.g., when the individual behavior policies restrict each agent to sample a small portion of the state space).",
    "creator" : "LaTeX with hyperref package"
  }
}