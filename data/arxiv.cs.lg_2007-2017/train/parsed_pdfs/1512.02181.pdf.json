{
  "name" : "1512.02181.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Teaching Dimension of Linear Learners",
    "authors" : [ "Ji Liu" ],
    "emails" : [ "jliu@cs.rochester.edu,", "jerryzhu@cs.wisc.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 2.\n02 18\n1v 1\n[ cs\n.L G\n] 7\nD ec"
    }, {
      "heading" : "1 Introduction",
      "text" : "Consider a teacher who knows both a target model and the learning algorithm used by a machine learner. The teacher wants to teach the target model to the learner by constructing a training set. The training set does not need to contain independent and identically distributed items drawn from some distribution. Furthermore, the teacher can construct any item in the input space. How many training items are needed? This is the question addressed by the teaching dimension [13, 23]. We give the precise definition in section 2, but first illustrate the intuition with an example.\nConsider integers x ∈ {1 . . . 10} and threshold classifiers hθ on them, so that hθ(x) returns -1 if x < θ and 1 if x ≥ θ. Now let the hypothesis space H consist of eleven classifiers H = {hθ | θ ∈ {1 . . . 11}}. Let the learner be a version-space learner, namely it maintains a version space {hθ ∈ H | hθ consistent with the training set}. If we want to teach a target model (in this paper we use hypothesis and model exchangeably), say h9, to such a learner, we can construct a training set that results in a singleton version space {h9}. It is easy to see that the training set D = {(x1 = 8, y1 = −1), (x2 = 9, y2 = 1)} is the smallest set for this purpose. We say that the teaching dimension of h9 with respect to H is TD(h9) = |D| = 2. Similarly, TD(h11) = 1 because D = {(x1 = 10, y1 = −1)} suffices. In fact, TD(h∗θ) = 1 for target model θ∗ = 1 or 11, and 2 for θ∗ = 2, 3, . . . , 10.\nThe astute reader may notice that this example does not apply to continuous spaces. To see this, let us extend x ∈ R and H = {hθ | θ ∈ R}. The learner’s version space under any linearly separable training set would now be represented by the interval between the two closest oppositely labeled items. It is impossible for the version-space learner to pick out a unique target model hθ∗ with a finite training set. In other words, TD(hθ∗) = ∞ for all target models θ∗. This is counterintuitive\nbecause ostensibly we can teach any one of the “modern” machine learning algorithms such as a support vector machine (SVM) with only two training items: D = {(x1 = θ∗ − ǫ, y1 = −1), (x2 = θ∗ + ǫ, y2 = 1)} with any ǫ > 0.\nThe issue here is that a version-space learner is not equipped with the ability to pick the maxmargin (or any other specific) hypothesis from the version space. In contrast, an SVM is not a version-space learner in our terminology; we have stronger knowledge from optimization on how it picks a specific hypothesis from the hypothesis space. This paper will utilize such knowledge to derive teaching dimensions that are distinct from classic teaching dimension analysis (e.g. [12]). Specifically, we extend teaching dimension to linear learners that learn by regularized empirical risk minimization:\nAopt(D) := Argminθ∈Rd n∑\ni=1\nℓ(x⊤i θ, yi) + λ\n2 ‖θ‖2A\n︸ ︷︷ ︸\n=:f(θ)\n. (1)\nHere, we identify H with Rd, h with θ, the loss function ℓ is either smooth or convex in the first argument, λ > 0 is the regularization coefficient, and A is a positive semidefinite matrix. ‖ · ‖A is the Mahalanobis norm: ‖θ‖A := √ θ⊤Aθ. We follow the convention in optimization when we use the capitalized Argmin to emphasize that it returns a set that achieves the minimum. The teacher can construct a training set with any items in Rd. The alternative pool-based teaching setting, where the teacher is given a finite pool of candidate training items and must select items from that pool, is not studied in this paper. By linear learners we mean the input x and the parameter θ interact only via their inner product x⊤θ. Linear learners include SVMs, logistic regression, and linear regression. Our analysis technique involves a novel application of the Karush-Kuhn-Tucker (KKT) conditions.\nTo our knowledge, this paper gives the first known values of teaching dimension for ridge regression, SVM, and logistic regression. We summarize our main results in Table 1. The table separately lists homogeneous (without a bias term) and inhomogeneous (with a bias term) versions of the linear learners. The teaching goal refers to the intention of the teacher: is teaching considered successful only when the learner learns the exact target parameter, or when the learner learns the correct decision boundary (which can be achieved by any positive scaling of the target parameter). See section 3 for definition of the target parameters θ∗,w∗ and the constant τmax. The target parameters are assumed to be nonzero. We will also present the corresponding minimum teaching set construction in section 3."
    }, {
      "heading" : "2 Classic Teaching Dimension and its Limitations",
      "text" : "Let X denote the input space and Y ⊆ R the output space. A hypothesis is a function h : X → Y. In this paper we identify a hypothesis hθ with its model parameter θ. The hypothesis space H is a set of hypotheses. By training item we mean a pair (x, y) ∈ X × Y. A training set is a multiset D = {(x1, y1) . . . (xn, yn)} where repeated items are allowed. Importantly, for the purpose of teaching we do not assume that D be drawn i.i.d. from a distribution. Let D denote the set of all training sets of all sizes. A learning algorithm A : D → 2H takes in a training set D ∈ D and outputs a subset of the hypothesis space H. That is, A does not necessarily return a unique hypothesis.\nClassic teaching dimension analysis is restricted to the version-space learner Avs:\nAvs(D) = {h ∈ H | h is consistent with D }. (2)\nThat is, the learner Avs keeps track of the version space. Let the target model be hθ∗ ∈ H. Teaching is successful if the teacher identifies a training set D ∈ D such that Avs(D) = {hθ∗} the singleton set. Such a D is called a teaching set of hθ∗ with respect to H. The teaching dimension of the hypothesis hθ∗ is the minimum size of the teaching set:\nTD(hθ∗) =\n{ minD∈D |D|, for D a teaching set of hθ∗\n∞, if no teaching set exists (3)\nFurthermore, the teaching dimension of the whole hypothesis space H is defined by the hardest hypothesis: TD(H) = maxh∈H TD(h). In this paper we will focus on the fine-grained teaching dimension of individual hypothesis TD(h).\nClassic teaching dimension analysis has several limitations: the learner is assumed to be a version-space learner Avs, and the hypothesis space is typically finite or countably infinite. As the example in section 1 showed, these fail to capture the teaching dimension of “modern” machine learners which has Rd as input space and picks a unique hypothesis via regularized empirical risk minimization (1). Furthermore, the target model can be ambiguous when the learner is a classifier: should the learner learn the exact target parameter θ∗, or the target decision boundary? In linear models any scaled parameter cθ∗ with c > 0 produces the same target decision boundary. These limitations motivate us to generalize the teaching dimension in the next section."
    }, {
      "heading" : "3 Main Results",
      "text" : "To make teaching dimension’s dependency on the learning algorithm explicit, henceforth we write teaching dimension with two arguments as\nTD(h∗,A) (4)\nwhere h∗ ∈ H is the target model, and A : D → 2H is the learning algorithm which given a training set D ∈ D returns a set of hypotheses A(D). We define teaching dimension to be the size of the smallest training set D such that A(D) = {h∗}, the singleton set containing the target model. With this notation, the classic teaching dimension is TD(h∗,Avs) where Avs is the version space learning algorithm (2). In this paper we focus on Aopt in (1) instead, namely linear learners in Rd. Linear learners include many popular members such as both homogeneous (without a\nbias term) and inhomogeneous (with a bias term) versions of linear regression, SVM, and logistic regression. In addition, the linear interaction between x and θ makes the loss function subgradient easy to compute, though in principle our analysis technique is applicable to other optimizationbased learners, too. In this section our goal is to teach the exact parameter θ∗, consequently our teaching dimension of interest is TD(θ∗,Aopt). (5) Later in section 4 for classification we will teach the decision boundary instead.\nHow to reason about our teaching dimension TD(θ∗,Aopt)? It is the size of the smallest training set D with which (1) has a unique solution θ∗. Our strategy is to first establish a number of lower bounds LB ≤ TD(θ∗,Aopt) by showing that any training set with which (1) has a unique solution θ∗ must have at least LB items. Section 3.1 is devoted to such lower bounds. The actual teaching dimension is learner dependent. In sections 3.2 and 3.3 we construct specific teaching sets for three popular learners: ridge regression, SVM, and logistic regression. These teaching sets uniquely returns θ∗ via (1). By definition, the size of these teaching sets is an upper bound on TD(θ∗,Aopt), respectively. If the lower and upper bounds match, we would have identified the teaching dimension TD(θ∗,Aopt)."
    }, {
      "heading" : "3.1 Lower Bounds on Teaching Dimension TD(θ∗,Aopt)",
      "text" : "In this section we provide three general lower bounds on the teaching dimension. These lower bounds capture different aspects of a teaching set, and should be used in conjunction (i.e. taking the maximum) when applicable. We will instantiate these lower bounds for specific learners in section 3.2. In the following let X and Y be the feasible region of all xi’s and yi’s respectively. We will use the notation ∂1ℓ(·, ·) in the following way: if ℓ(·, ·) is smooth, then it denotes a singleton set only containing the gradient w.r.t. the first argument; if ℓ(·, ·) is convex, then it denotes the subdifferential w.r.t the first argument.\nLB1 comes from a degree-of-freedom perspective. It is necessary to have this amount of training items for a unique solution to exist in (1).\nTheorem 1. Given any target model θ∗, there is a degree-of-freedom lower bound on the number of training items to obtain a unique solution θ∗ from solving (1):\nLB1 =\n{\nd− Rank(A) + 1, if Aθ∗ 6= 0 d− Rank(A), otherwise.\n(6)\nProof. Let n∗ be the minimal number of training items to ensure a unique solution θ∗. First consider the case n∗ = 0. It happens if and only if θ∗ = 0 and Rank(A) = d, which is a special case of Aθ∗ = 0. Clearly, this case is consistent with LB1. Next consider the case n∗ ≥ 1. Since θ∗ solves (1), the KKT condition holds:\n−λAθ∗ ∈ n∗∑\ni=1\n∂1ℓ(x ⊤ i θ ∗, yi)xi. (7)\nWe seek all δ such that θ∗ + δ satisfies\nA(θ∗ + δ) = Aθ∗ and x⊤i (θ ∗ + δ) = x⊤i θ ∗ ∀i = 1, · · · , n∗, (8)\nFor any such δ , simple algebra verifies that θ∗+ tδ satisfies the KKT condition (7) for any t ∈ [0, 1]. Consequently, θ∗ + δ also solves the problem in (1). To see this, we consider two situations:\n• If the loss function ℓ(·, ·) is convex in the first argument, the KKT condition is a sufficient optimality condition, which means that θ∗ + δ solves (1).\n• If the loss function ℓ(·, ·) is smooth (not necessary convex) in the first argument, we have f(θ∗) = f(θ∗ + δ) by using the Taylor expansion (recall f is defined in equation 1):\nf(θ∗ + δ) =f(θ∗) + 〈∇f(θ∗ + tδ), δ〉 (for some t ∈ [0, 1])\n=f(θ∗) +\n〈 n∗∑\ni=1\n∇1ℓ(x⊤i (θ∗ + tδ), yi)xi + λA(θ∗ + tδ)), δ 〉\n=f(θ∗) +\n〈 n∗∑\ni=1\n∇1ℓ(x⊤i θ∗, yi)xi + λAθ∗\n︸ ︷︷ ︸\n=0 due to the KKT condition (7)\n, δ\n〉\n=f(θ∗).\nTherefore, θ∗ + δ also solves (1). However, the uniqueness of θ∗ requires δ = 0 to be the only value satisfying (8). This is equivalent to say\nNull(A) ∩Null(Span{x1, · · · ,xn∗}) = {0}. (9) It indicates that Rank(A) + Dim(Span{x1, · · · ,xn∗}) ≥ d. From n∗ ≥ Dim(span{x1, · · · ,xn∗}), we have n∗ ≥ d − Rank(A). We proved the general case for LB1.\nIf we have Aθ∗ 6= 0, we can further improve LB1. Let g∗ = (g∗1 , . . . , g∗n∗)⊤ be the vector satisfying\n−λAθ∗ = n∗∑\ni=1\ng∗i xi and g ∗ i ∈ ∂1ℓ(x⊤i θ∗, yi) ∀i = 1, 2, · · · , n∗. (10)\nSince θ∗ satisfies the KKT condition, such vector g∗ must exist. Applying Aθ∗ 6= 0 to (10), we have g∗ 6= 0 and Dim (Span{A.1, A.2, · · · , A.d} ∩ Span{x1, · · · ,xn∗}) ≥ 1. (11) To satisfy (9), we must have\nd = Dim (Span{A.1, A.2, · · · , A.d,x1, · · · ,xn∗}) . Using the fact in linear algebra\nDim (Span{A.1, A.2, · · · , A.d,x1, · · · ,xn∗}) =Dim (Span{A.1, A.2, · · · , A.d}) ︸ ︷︷ ︸\n=Rank(A)\n+\nDim (Span{x1, · · · ,xn∗}) ︸ ︷︷ ︸ ≤n∗ − Dim (Span{A.1, A.2, · · · , A.d} ∩ Span{x1, · · · ,xn∗}) ︸ ︷︷ ︸\n≥1 (from (11))\nWe conclude that n∗ ≥ d− Rank(A) + 1. We completed the proof for LB1.\nLB2 observes that the regularizer acts as a prior. If λ is large, more items are needed to sway the prior toward the target θ∗.\nTheorem 2. Given any target model θ∗, there is a strength-of-regularization lower bound on the required number of training items to obtain a unique solution θ∗ from solving (1):\nLB2 =\n \n\n⌈\nλ ( supα∈R,y∈Y ,g∈−∂1ℓ(α‖θ∗‖2A,y) αg )−1\n⌉\n, if A has full rank and θ∗ 6= 0\n0, otherwise.\n(12)\nProof. When A has full rank we have an equivalent expression for the KKT condition (7):\n−λA 12θ∗ ∈ n∗∑\ni=1\nA− 1 2xi∂1ℓ(x ⊤ i θ ∗, yi) ∀i = 1, · · · , n∗. (13)\nLet us decompose A− 1 2xi for all i = 1, · · · , n∗ into A− 1 2xi = αiA 1 2θ∗ + ui, where ui is orthogonal to A 1 2θ∗: u⊤i A 1 2θ∗ = 0. Equivalently xi = αiAθ∗ +A 1 2ui. Applying this decomposition, we have\nx⊤i θ ∗ = αi‖θ∗‖2A + u⊤i A 1 2θ∗ = αi‖θ∗‖2A.\nPutting it back in (13) we obtain\n−λA 12θ∗ ∈ n∗∑\ni=1\n(\nαiA 1 2θ∗ + ui\n)\n∂1ℓ(αi‖θ∗‖2A, yi) ∀i = 1, · · · , n∗. (14)\nSince ui is orthogonal to A 1 2θ∗, (14) can be rewritten as\n∃αi ∈ R, ∃yi ∈ Y, ∃gi ∈ ∂1ℓ(αi‖θ∗‖2A, yi) ∀i = 1, · · · , n∗ (15)\nsatisfying\nn∗∑\ni=1\ngiui = 0\n− λA 12θ∗ = A 12θ∗ n∗∑\ni=1\nαigi (16)\nSince Aθ∗ 6= 0, we have A 12θ∗ 6= 0 and (16) is equivalent to −λ = ∑n∗i=1 αigi. It follows that\nλ = − n∗∑\ni=1 αigi ≤ n∗ sup α∈R,y∈Y ,g∈∂1ℓ(α‖θ∗‖2A,y) −αg = n∗ sup α∈R,y∈Y ,g∈−∂1ℓ(α‖θ∗‖2A,y) αg\nIt indicates the lower bound for n∗\nn∗ ≥ ⌈\nλ\nsupα∈R,y∈Y ,g∈−∂1ℓ(α‖θ∗‖2A,y) αg\n⌉\n.\nLB1 and LB2 apply to all generalized linear learners. Due to the popularity of inhomogeneous margin-based linear learners (which include the standard form of SVM and logistic regression), we provide a tighter lower bound LB3 for such learners in Theorem 3. For inhomogeneous margin-based linear learners the learning algorithm Aopt solves a special form of (1):\nAopt(D) = Argminw,b n∑\ni=1\nℓ(yi(x ⊤ i w + b)) +\nλ 2 ‖w‖2A. (17)\nLB3 will prove to be instrumental in computing the teaching dimension for those learners. Following standard notation, we define θ = [w; b] wherew ∈ Rd is the weight vector and b ∈ R the bias (offset) term. Note θ ∈ Rd+1 now. The d × d regularization matrix A applies only to w while b is not regularized. Furthermore, margin-based linear learners have loss functions defined on the margin y(x⊤w + b). This loss function structure will play a key role in obtaining LB3.\nTheorem 3. Assume matrix A in (17) has full rank and w∗ 6= 0. Given any target model [w∗; b∗], there is an inhomogeneous-margin lower bound on the required number of training items to obtain a unique solution [w∗; b∗] from solving (17):\nLB3 =\n\n   λ\n(\nsup α∈R,g∈−∂ℓ(α‖w∗‖2\nA )\nαg\n)−1\n   . (18)\nProof. Let D = {xi, yi}i=1,··· ,n be a teaching set for [w∗; b∗]. The following KKT condition needs to be satisfied:\n0 ∈ n∑\ni=1\n∂ℓ(yi(x ⊤ i w ∗ + b∗))yi [ xi 1 ] + [ λAw∗ 0 ] . (19)\nIf we construct a new training set\nD̂ =\n{\nx̂i = xi + b∗\n‖w∗‖2A Aw∗, ŷi = yi\n}\ni=1,··· ,n\nthen [w∗; 0] satisfies the KKT condition defined on D̂. This can be verified as follows:\nn∑\ni=1\n∂ℓ(ŷi(x̂ ⊤ i w ∗))ŷi [ x̂i 1 ] + [ λAw∗ 0 ]\n=\nn∑\ni=1\n∂ℓ(yi(x ⊤ i w ∗ + b∗))yi\n[\nxi + b∗\n‖w∗‖2 A\nAw∗\n1\n]\n+\n[ λAw∗\n0\n]\n=\nn∑\ni=1\n∂ℓ(yi(x ⊤ i w ∗ + b∗))yi [ xi 1 ] + [ λAw∗ 0 ] ︸ ︷︷ ︸\n∋0 from (19)\n+\n[ b∗\n‖w∗‖2 A\nAw∗\n0\n] n∑\ni=1\n∂ℓ(yi(x ⊤ i w ∗ + b∗))yi\n︸ ︷︷ ︸\n∋0 from (19)\n∋0 (20)\nwhere 0 ∈ ∑ni=1 ∂ℓ(yi(x⊤i w∗ + b∗))yi is from the bias dimension in (19). It follows that\n0 ∈ n∑\ni=1\n∂ℓ(ŷix̂ ⊤ i w ∗)ŷix̂i + λAw ∗\nwhich is equivalent to\n0 ∈ n∑\ni=1\n∂ℓ(ŷix̂ ⊤ i w ∗)A− 1 2 ŷix̂i ︸︷︷︸\n=:zi\n+λA 1 2w∗\n=\nn∑\ni=1\n∂ℓ(z⊤i w ∗)A− 1 2zi + λA 1 2w∗. (21)\nWe decompose A− 1 2zi = αiA 1 2w∗+ui where ui satisfies u⊤i A 1 2w∗ = 0. Applying this decomposition to (21), we have\nλA 1 2w∗ ∈\nn∑\ni=1\n−∂ℓ(αi‖w∗‖2A)(αiA 1 2w∗ + ui). (22)\nSince ui is orthogonal to A 1 2w∗, (22) implies that\nλA 1 2w∗ ∈\nn∑\ni=1\n−∂ℓ(αi‖w∗‖2A)αiA 1 2w∗\nSince w∗ 6= 0 we have\nλ ∈ n∑\ni=1\n−∂ℓ(αi‖w∗‖2A)αi\nTogether with n∑\ni=1\n−∂ℓ(αi‖w∗‖2A)αi ≤ n sup α∈R,g∈−∂ℓ(α‖w∗‖2\nA )\nαg,\nwe obtain LB3."
    }, {
      "heading" : "3.2 The Teaching Dimension TD(θ∗,Aopt) of Three Homogeneous Learners",
      "text" : "We now turn to upper bounding teaching dimension by constructing teaching sets. To prove that we indeed have a teaching set for a target θ∗, we need to show that θ∗ is a solution of (1), and the solution is unique. The size of any such teaching set is an upper bound on the teaching dimension. The teaching dimension itself is determined if such an upper bound matches the corresponding lower bound. We show that this is indeed the case for our constructed teaching sets. For the sake of reference we preview in Table 2 the instantiated lower bounds that we will use in this section; their derivation will be shown below.\nTeaching dimension is learner-dependent. We choose three learners to study their teaching dimension due to these learners’ popularity in machine learning: ridge regression, SVM, and logistic regression. It turns out that homogeneous and inhomogeneous versions of these learners require different analysis. We devote this section to the homogeneous version where the regularizer matrix\nA = I the identity matrix, and the next section to the inhomogeneous version. It is possible to extend our analysis to other linear learners of the form (1).\nIt is easy to see that if the target model θ∗ = 0, we do not need any training data to uniquely obtain the target model from (1). In the following, we only consider the nontrivial case θ∗ 6= 0.\nHomogeneous ridge regression solves the following optimization problem:\nmin θ∈Rd\nn∑\ni=1\n1 2 (x⊤i θ − yi)2 + λ 2 ‖θ‖2 (23)\nWe only need one training item to uniquely obtain any nonzero target model θ∗, as the following construction shows.\nProposition 1. Given any target model θ∗ 6= 0, the following is a teaching set for homogeneous ridge regression (23):\nx1 = aθ ∗, y1 = λ+ ‖x1‖2 a\n(24)\nwhere a can be any nonzero real number.\nProof. We simply verify the KKT condition to see that θ∗ is a solution to (23) by applying the construction in (24). The uniqueness of θ∗ is guaranteed by the strong convexity of (23).\nWe encourage the reader to distinguish two senses of uniqueness. The teaching set itself is not necessarily unique. In the construction (24), any a 6= 0 leads to a valid teaching set. Nonetheless, any one of the teaching sets will lead to the unique solution θ∗ in (23).\nCorollary 1. The teaching dimension TD(θ∗,Ahomridge) = 1 for homogeneous ridge regression and target θ∗ 6= 0.\nProof. Substituting A by I in LB1 (6), we obtain the lower bound d − Rank(I) + 1 = 1 which matches the teaching set size in (24).\nHomogeneous SVM solves the problem:\nmin θ∈Rd\nn∑\ni=1\nmax(1− yix⊤i θ, 0) + λ\n2 ‖θ‖2. (25)\nTo teach this learner one training item is in general not enough: we will show that we need ⌈ λ‖θ∗‖2 ⌉ training items. In fact, we will construct such a teaching set consisting of identical training items. It is well-known in the teaching literature that a teaching set does not need to consist of i.i.d. samples from a distribution, and can look unusual. It is possible to incorporate additional constraints into a teaching problem if one wants the training items to be diverse, but we do not consider that in the present paper.\nProposition 2. Given any target model θ∗ 6= 0, the following is a teaching set for homogeneous SVM (25). There are n = ⌈ λ‖θ∗‖2 ⌉ identical training items, each taking the form\nxi = λθ∗\n⌈λ‖θ∗‖2⌉ , yi = 1. (26)\nProof. We only need to verify that the KKT condition holds for θ∗. Due to the strong convexity of (25) uniqueness is guaranteed automatically. We denote the subgradient ∂amax(1 − a, 0) = −∂1 max(1− a, 0) = −I(a), where\nI(a) =\n \n\n1, if a < 1\n[0, 1], if a = 1\n0, otherwise\n. (27)\nThe KKT condition is\nn∑\ni=1\n−yixi∂1 max(1− yix⊤i θ∗, 0) + λθ∗\n=\nn∑\ni=1\n−yixiI(yix⊤i θ∗) + λθ∗\n=− n λθ ∗ ⌈λ‖θ∗‖2⌉I ( λ‖θ∗‖2 ⌈λ‖θ∗‖2⌉ ) + λθ∗ =− λθ∗I (\nλ‖θ∗‖2 ⌈λ‖θ∗‖2⌉\n)\n+ λθ∗\n∋0\nwhere the last line is due to I (\nλ‖θ∗‖2 ⌈λ‖θ∗‖2⌉\n)\ngiving either the set [0, 1] or the value 1.\nCorollary 2. The teaching dimension TD(θ∗,Ahomsvm) = ⌈ λ‖θ∗‖2 ⌉ for homogeneous SVM and target θ∗ 6= 0. Proof. We show this number matches LB2. Let A = I, ℓ(a, b) = max(1 − ab, 0), and consider the denominator of (12):\nsup α∈R,y∈Y ,g∈−∂1ℓ(α‖θ∗‖2,y) αg = sup α,y∈{−1,1},g∈yI(yα‖θ∗‖2) αg\n= sup α,g∈I(α‖θ∗‖2) αg = 1\n‖θ∗‖2\nwhere the first equality is due to ∂1ℓ(a, b) = −bI(ab). Therefore, LB2 = ⌈ λ‖θ∗‖2 ⌉ which matches the construction in (26).\nHomogeneous logistic regression solves the problem:\nmin θ∈Rd\nn∑\ni=1\nlog(1 + exp{−yix⊤i θ}) + λ\n2 ‖θ‖2 (28)\nThe situation is similar to homogeneous SVM. However, due to the negative log likelihood term we have a coefficient defined by the Lambert W function [11], which we denote by Wlam. Recall the defining equation for Lambert W function is Wlam(x)e Wlam(x) = x. We further define\nτmax := max t\nt\n1 + et = Wlam(1/e) ≈ 0.2785. (29)\nFor any value a ≤ τmax, we define τ−1(a) as the solution to a = t1+et . By using the Lambert W function τ−1(a) can be expressed as τ−1(a) ≡ a−Wlam(−aea).\nProposition 3. Given any target model θ∗ 6= 0, the following is a teaching set for homogeneous logistic regression (28). There are n =\n⌈ λ‖θ∗‖2 τmax ⌉ identical training items, each takes the form\nxi = τ −1\n(\nλ‖θ∗‖2 ⌈ λ‖θ∗‖2 τmax\n⌉−1) θ∗\n‖θ∗‖2 , yi = 1. (30)\nProof. We first verify that θ∗ is a solution to (28) based on the teaching set construction in (30). We only need to verify the gradient of (28) is zero. Computing the gradient of (28), we have\nn∑\ni=1\n−yixi 1 + exp{yix⊤i θ∗} + λθ∗\n=− n xi 1 + exp { τ−1 (\nλ‖θ∗‖2 ⌈ λ‖θ∗‖2 τmax\n⌉−1 )} + λθ∗\n=− n τ−1\n(\nλ‖θ∗‖2 ⌈ λ‖θ∗‖2 τmax\n⌉−1 )\n1 + exp\n{ τ−1 ( λ‖θ∗‖2 ⌈ λ‖θ∗‖2 τmax\n⌉−1)} θ∗ ‖θ∗‖2 + λθ ∗\n=− nλ‖θ∗‖2 ⌈ λ‖θ∗‖2 τmax ⌉−1 θ∗ ‖θ∗‖2 + λθ ∗\n=0,\nwhere the third equality uses the fact λ‖θ∗‖2 ⌈ λ‖θ∗‖2 τmax ⌉−1 ≤ τmax and the property a = τ −1(a) 1+eτ−1(a) . The strong convexity of (28) automatically implies uniqueness.\nCorollary 3. The teaching dimension TD(θ∗,Ahomlog ) = ⌈ λ‖θ∗‖2 τmax ⌉ for homogeneous logistic regression and target θ∗ 6= 0.\nProof. We show that the number matches LB2. In (12) let A = I and ℓ(a, b) = log(1+ exp{−ab}). The denominator of LB2 is:\nsup α∈R,y∈Y ,g∈−∂1ℓ(α‖θ∗‖2,y) αg = sup α,y∈{−1,1},g=y(1+exp{yα‖θ∗‖2})−1 αg\n= sup α,g=(1+exp{α‖θ∗‖2})−1 αg\n=sup α\nα\n1 + exp{α‖θ∗‖2} =‖θ∗‖−2 sup\nt\nt\n1 + exp{t} =\nτmax ‖θ∗‖2 ,\nwhich implies LB2 = ⌈ λ‖θ∗‖2 τmax ⌉ ."
    }, {
      "heading" : "3.3 The Teaching Dimension TD(θ∗,Aopt) of Three Inhomogeneous Learners",
      "text" : "Inhomogeneous learners are defined by θ = [w; b] where the weight vector w ∈ Rd and the scalar offset b ∈ R. The offset b is not regularized. Similar to the previous section, we need to instantiate the teaching dimension lower bounds and design the teaching sets. We show that the size of our teaching set exactly matches the lower bound for inhomogeneous ridge regression, and differs from the lower bound of inhomogeneous SVM and logistic regression by at most one due to rounding. Therefore, up to rounding we also establish the teaching dimension for these inhomogeneous learners.\nInhomogeneous ridge regression solves the problem:\nmin w∈Rd,b∈R\nn∑\ni=1\n1 2 (x⊤i w + b− yi)2 + λ 2 ‖w‖2 (31)\nProposition 4. Given any target model [w∗; b∗], if w∗ = 0 (b∗ can be an arbitrary value), the following is a teaching set for inhomogeneous ridge regression (31) with n = 1:\nx1 = 0, y1 = b ∗. (32)"
    }, {
      "heading" : "If w∗ 6= 0, any n = 2 items satisfying the following are a teaching set for a 6= 0:",
      "text" : "x1 − x2 = aw∗, y1 = x⊤1 w∗ + b∗ + λ a , y2 = y1 − a‖w∗‖2 − 2 λ a . (33)\nProof. We first prove the case for w∗ = 0. We can verify that the KKT condition is satisfied by designing x1 and y1 as in (32):\n(x⊤1 w ∗ + b∗ − y1)x1 + λw∗ =0\nx⊤1 w ∗ + b∗ − y1 =0.\nThe uniqueness of [w∗; b∗] is indicated by the strong convexity of (31) when n = 1.\nWe then prove the case for w∗ 6= 0. With simple algebra, we can verify the KKT condition holds via the construction in (33):\n(x⊤1 w ∗ + b∗ − y1)x1 + (x⊤2 w∗ + b∗ − y2)x2 + λw∗ =0\n(x⊤1 w ∗ + b∗ − y1) + (x⊤2 w∗ + b∗ − y2) =0.\nSimilarly, the uniqueness is implied by the strong convexity of (31) when n = 2.\nCorollary 4. The teaching dimension for inhomogeneous ridge regression with target θ∗ = [w∗; b∗] is TD(θ∗,Ainhridge) = 1 if target w∗ = 0, or TD(θ∗,Ainhridge) = 2 if w∗ 6= 0, regardless of the target offset b∗.\nProof. We match the lower bound LB1 in (6). Note θ∗ = [w∗; b∗] ∈ Rd+1, and A in this case is a (d + 1) × (d + 1) matrix with the d × d identity matrix Id padded with one additional row and column of zeros for the offset. Therefore Rank(A) = Rank(Id) = d. When w\n∗ = 0, Aθ∗ = 0 and LB1 = (d + 1) − Rank(A) = 1. When w∗ 6= 0, Aθ∗ 6= 0 and LB1 = (d + 1) − Rank(A) + 1 = 2. These lower bounds match the teaching set sizes in (32) and (33), respectively.\nInhomogeneous SVM solves the problem:\nmin w∈Rd,b∈R\nn∑\ni=1\nmax(1− yi(x⊤i w + b), 0) + λ\n2 ‖w‖2 (34)\nProposition 5. Given any target model [w∗; b∗] with w∗ 6= 0, the following is a teaching set for inhomogeneous SVM (34). We need n = 2 ⌈ λ‖w∗‖2\n2\n⌉\ntraining items, half of which are identical pos-\nitive items xi = x+, yi = 1, ∀i ∈ { 1, · · · , n2 } and half identical negative items xi = x−, yi = −1, ∀i ∈ { n 2 + 1, · · · , n } . x+ and x− can be designed as any vectors satisfying\nx⊤+w ∗ = 1− b∗, x− = x+ −\n2w∗\n‖w∗‖2 . (35)\nProof. Unlike in previous learners (including homogeneous SVM), we no longer have strong convexity w.r.t. b. In order to prove that (35) is a teaching set, we need to verify the KKT condition and verify solution uniqueness.\nWe first verify the KKT condition to show that the solution under (35) includes the target model [w∗; b∗]. From (35), we have\nx⊤+w ∗ + b∗ = 1, x⊤−w ∗ + b∗ = −1. (36)\nApplying them to the KKT condition and using the notation in (27) we obtain\n− n 2 I(x⊤+w ∗ + b∗) [ x+ 1 ] + n 2 I(−x⊤−w∗ − b∗) [ x− 1 ] + [ λw∗ 0 ]\n=− n 2 I(1) [ x+ 1 ] + n 2 I(1) [ x− 1 ] + [ λw∗ 0 ]\n⊃n 2 I(1)\n[ x− − x+\n0\n]\n+\n[ λw∗\n0\n]\nsetting the last dimension to 0\n=I(1)\n[\n− n‖w∗‖2w∗ 0\n]\n+\n[ λw∗\n0\n]\napplying (35)\n⊇I(1) [ −λw∗\n0\n]\n+\n[ λw∗\n0\n]\nobserving n ≥ λ‖w∗‖2\n∋0.\nIt proves that [w∗; b∗] solves (34) by our teaching set construction. Next we prove uniqueness by contradiction. We use f(w, b) to denote the objective function in (34) under the teaching set. It is easy to verify that f(w∗, b∗) = λ2‖w∗‖2. Assume that there exists another solution [w̄; b̄] different from [w∗; b∗]. We can obtain ‖w̄‖2 ≤ ‖w∗‖2 due to\nλ 2 ‖w∗‖2 = f(w∗, b∗) = f(w̄, b̄) ≥ λ 2 ‖w̄‖2.\nThe second equality is due to [w̄; b̄] being a solution; the inequality is due to whole-part relationship. Therefore, there are only two possibilities for the norm of w̄: ‖w̄‖ = ‖w∗‖ or ‖w̄‖ = t‖w∗‖ for some 0 ≤ t < 1. Next we will show that both cases are impossible.\n(Case 1) For the case ‖w̄‖ = ‖w∗‖, we have\nf(w̄, b̄) = n\n2 max\n( 1− (x⊤+w̄ + b̄), 0 ) + n\n2 max\n( 1 + (x⊤−w̄ + b̄), 0 ) + λ\n2 ‖w̄‖2\n= n\n2 max\n\n x ⊤ +(w ∗ − w̄) + (b∗ − b̄) ︸ ︷︷ ︸\n=:∆+\n, 0\n\n  +\nn 2 max\n\n −x⊤−(w∗ − w̄)− (b∗ − b̄) ︸ ︷︷ ︸\n=:∆−\n, 0\n\n \n+ λ 2 ‖w∗‖2\n= n\n2 max (∆+, 0) +\nn 2 max (∆−, 0) + f(w ∗, b∗).\nFrom f(w̄, b̄) = f(w∗, b∗), it follows ∆+ ≤ 0 and ∆− ≤ 0. Since\n0 ≥ ∆+ +∆− = (x+ − x−)⊤(w∗ − w̄) = 2(w∗)⊤(w∗ − w̄) ‖w∗‖2 = 2− 2 w̄⊤w∗ ‖w∗‖2 ,\nwe have w̄⊤w∗ ≥ ‖w∗‖2. But because ‖w̄‖ = ‖w∗‖, we must have w̄ = w∗. Applying this new observation to ∆+ ≤ 0 and ∆− ≤ 0, we obtain b∗ = b̄. It means that [w∗; b∗] = [w̄; b̄], contradicting our assumption [w∗; b∗] 6= [w̄; b̄].\n(Case 2) Next we turn to the case ‖w̄‖ = t‖w∗‖ for some t ∈ [0, 1). Recall our assumption that [w̄; b̄] solves (34). Then it follows that the following specific construction [ŵ, b̂] solves (34) as well:\nŵ = tw∗, b̂ = tb∗. (37)\nTo see this, we consider the following optimization problem:\nmin w,b\nL(w, b) := n 2 max(1− (x⊤+w + b), 0) + n 2 max(1 + (x⊤−w + b), 0)\ns.t. ‖w‖ ≤ t‖w∗‖. (38)\nSince [w̄; b̄] solves (34), it is easy to see that [w̄; b̄] solves (38) too, otherwise there exists a solution for (38) which gives a lower function value on (34). Then we can verify that [ŵ; b̂] solves (38) as well by showing the following optimality condition holds:\n− [ ∂L(w,b)\n∂w ∂L(w,b)\n∂b ]∣ ∣ ∣ ∣ ∣ [ŵ;b̂] ∩ N‖w‖≤t‖w∗‖(ŵ, b̂) ︸ ︷︷ ︸\nNormal cone to the set {[w; b] : ‖w‖ ≤ t‖w∗‖} at [ŵ; b̂]\n6= ∅ (39)\nBecause of (36) and (37), we have x⊤+ŵ + b̂ = t < 1. Thus at [ŵ; b̂] the subgradient is\n− [ ∂L(w,b)\n∂w ∂L(w,b)\n∂b ]∣ ∣ ∣ ∣ ∣ [ŵ;b̂] = n 2 [ x+ − x− 0 ] = [ nw∗ ‖w∗‖2 0 ]\n(40)\nAnd the normal cone is\nN‖w‖≤t‖w∗‖(ŵ, b̂) = { s [ w∗\n0 ] ∣ ∣ ∣ ∣ ∣ s ≥ 0 } . (41)\nThe intersection is non-empty by choosing s = n‖w∗‖2 . Since both [ŵ; b̂] and [w̄; b̄] solve (38), we have L(ŵ, b̂) = L(w̄, b̄). Together with ‖ŵ‖ = ‖w̄‖, we have\nf(ŵ, b̂) = L(ŵ, b̂) + λ 2 ‖ŵ‖2 = f(w̄, b̄) = f(w∗, b∗).\nTherefore, we proved that [ŵ; b̂] solves (34) as well. To see the contradiction, let us check the function value of f(ŵ, b̂) via a different route:\nf(ŵ, b̂) =f(tw∗, tb∗)\n=\nn\n2∑\ni=1\nmax ( 1− t(x⊤+w∗ + b∗), 0 ) +\nn\n2∑\ni=1\nmax (\n1 + t(x⊤−w ∗ + b∗), 0\n)\n+ λ 2 ‖w∗‖2t2\n=\nn\n2∑\ni=1\nmax (1− t, 0) + n 2∑\ni=1\nmax (1− t, 0) + λ 2 ‖w∗‖2t2\n=n(1− t)− λ 2 ‖w∗‖2(1− t2) + λ 2 ‖w∗‖2 ≥n(1− t)− n 2 (1− t2) + λ 2 ‖w∗‖2 = n\n2 (1− t)2 + f(w∗, b∗)\n>f(w∗, b∗),\nwhere the first inequality uses the fact that n ≥ λ‖w∗‖2. It contradicts our early assertion f(ŵ, b̂) = f(w∗, b∗). Putting cases 1 and 2 together we prove uniqueness.\nOur construction of the teaching set in (35) requires n = 2 ⌈ λ‖w∗‖2\n2\n⌉\ntraining items. This is an\nupper bound on the teaching dimension. Meanwhile, we show below that the inhomogeneous SVM lower bound is LB3 = ⌈ λ‖w∗‖2 ⌉ . There can be a difference of at most one between the lower and upper bounds, which we call the “rounding effect.” We suspect that this small gap is a technicality and not intrinsic. However, at present we do not have a teaching set construction that bridges this gap. Therefore, we state the teaching dimension as an interval in the following corollary and leave the precise value as an open question for future research.\nCorollary 5. The teaching dimension for inhomogeneous SVM and target θ∗ = [w∗; b∗] where w∗ 6= 0 is in the interval ⌈ λ‖w∗‖2 ⌉ ≤ TD(θ∗,Ainhsvm) ≤ 2 ⌈ λ‖w∗‖2\n2\n⌉\n.\nProof. The upper bound directly follows Proposition 5. We only need to show the lower bound LB3 = ⌈ λ‖w∗‖2 ⌉ in Theorem 3. Let A = I, ℓ(a) = max(1 − a, 0), and consider the denominator of (18):\nsup α∈R,g∈−∂ℓ(α‖w∗‖2) αg = sup α,g∈I(α‖w∗‖2)\nαg = 1\n‖w∗‖2\nwhere the first equality is due to ∂ℓ(a) = −I(a). Therefore, LB3 = ⌈ λ‖w∗‖2 ⌉ which proves the lower bound.\nInhomogeneous logistic regression solves the problem\nmin w∈Rd,b∈R\nn∑\ni=1\nlog(1 + exp{−yi(x⊤i w + b)}) + λ\n2 ‖w‖2 (42)\nProposition 6. To create a teaching set for target model [w∗; b∗] with nonzero w∗ for inhomogeneous logistic regression (42), we can use n = 2 ⌈ λ‖w∗‖2 2τmax ⌉ training items where xi = x+, yi = 1, ∀i ∈ { 1, · · · , n2 } and xi = x−, yi = −1, ∀i ∈ { n 2 + 1, · · · , n } . x+ and x− can be designed as any vectors satisfying\nx⊤+w ∗ = t− b∗, x− = x+ −\n2t\n‖w∗‖2w ∗, (43)\nwhere the constant t is defined by t := τ−1 ( λ‖w∗‖2\nn\n)\n.\nProof. We first point out that for t to be well-defined the argument to τ−1() has to be bounded λ‖w∗‖2\nn ≤ τmax. This implies n ≥ λ‖w ∗‖2 τmax\n. The size of our proposed teaching set is the smallest among all such symmetric construction that satisfy this constraint.\nWe verify that the KKT condition to show the construction in (43) includes the solution [w∗; b∗]. From (43), we have\nx⊤+w ∗ + b∗ = t x⊤−w ∗ + b∗ = −t.\nWe apply them and the teaching set construction to compute the gradient of (42):\n− n 2\n1\n1 + exp{x⊤+w∗ + b∗}\n[ x+ 1 ] + n 2\n1\n1 + exp{−x⊤−w∗ − b∗}\n[ x− 1 ] + [ λw∗ 0 ]\n=− n 2\n1\n1 + exp{t} [ x+ 1 ] + n 2\n1\n1 + exp{t} [ x− 1 ] + [ λw∗ 0 ]\n=− n‖w∗‖2 t 1 + exp{t}\n[ w∗\n0\n]\n+\n[ λw∗\n0\n]\n=− n‖w∗‖2 λ‖w∗‖2 n [ w∗ 0 ] + [ λw∗ 0 ]\n=0.\nThis verifies the KKT condition. Finally we show uniqueness. The Hessian matrix of the objective function (42) under our training set (43) is:\nn\n2 exp{t} (1 + exp{t})2 ︸ ︷︷ ︸\n:=a\n[ x+x ⊤ + + x−x ⊤ − x+ + x−\nx⊤+ + x ⊤ − 2\n]\n︸ ︷︷ ︸\n:=A\n+λ [ I 0 0⊤ 0 ]\n︸ ︷︷ ︸\n:=B\n.\nNote a > 0 and A = [ x+ 1 ] [ x+ 1 ] + [ x− 1 ] [ x− 1 ] is positive semi-definite. We show that aA+λB is positive definite. Suppose not. Then there exists [u; v] 6= 0 such that [u; v]⊤(aA+λB)[u; v] = 0. This implies [u; v]⊤(aA)[u; v] + λu⊤u = 0. Since the first term is non-negative due to A being positive semi-definite, u = 0. But then we have 2av2 = 0 which implies [u; v] = 0, a contradiction. Therefore uniqueness is guaranteed.\nCorollary 6. The teaching dimension for inhomogeneous logistic regression and target θ∗ = [w∗; b∗] where w∗ 6= 0 is in the interval ⌈ λ‖w∗‖2 τmax ⌉ ≤ TD(θ∗,Ainhlog ) ≤ 2 ⌈ λ‖w∗‖2 2τmax ⌉ .\nProof. The upper bound directly follows Proposition 6. We only need to show the lower bound⌈ λ‖w∗‖2 τmax ⌉\nby applying LB3 in Theorem 3. Let A = I and ℓ(a) = log(1 + exp{−a}) and consider the denominator of (18):\nsup α∈R,g∈∂ℓ(−α‖w∗‖2) αg = sup α,g=(1+exp{α‖w∗‖2})−1 αg\n=sup α\nα\n1 + exp{α‖w∗‖2} =‖w∗‖−2 sup\nt\nt\n1 + exp{t} =\nτmax ‖w∗‖2 ,\nwhich implies LB3 = ⌈ λ‖w∗‖2 τmax ⌉ ."
    }, {
      "heading" : "4 Teaching a Decision Boundary Instead of a Parameter",
      "text" : "In section 3 we considered the teaching goal where the learner is required to learn the exact target parameter θ∗. But when the learner is a classifier often a weaker teaching goal is sufficient, namely teaching the learner a target decision boundary. In this section we consider this teaching goal. Equivalently, such a goal is defined by the set of parameters that produce the target decision boundary. Teaching is successful if the learner arrives at any one parameter within that set.\nIn the case of inhomogeneous linear learners, the linear decision boundary {x | x⊤w∗ + b∗ = 0} is identified with the parameter set {t[w∗; b∗] : t > 0}. Here we assume w∗ is nonzero. The parameter θ∗ = [w∗; b∗] is just a representative member of the set. Homogeneous linear learners are similar without b∗. We denote the corresponding “decision boundary” teaching dimension by TD({tθ∗},Aopt). This notation extends our earlier definition of TD by allowing the first argument to be a set, with the understanding that the teaching goal is for the learned model to be an element in the set. It immediately follows that\nTD({tθ∗},Aopt) = min t>0 TD(tθ∗,Aopt). (44)\nSince it is sufficient to teach the parameter tθ∗ for some t > 0 in order to teach the decision boundary, we can choose the best t that minimizes TD(tθ∗,Aopt). For SVM and logistic regression – either homogeneous or inhomogeneous – the teaching dimension TD(tθ∗,Aopt) depends on ‖tθ∗‖ (see Table 1). We can choose t sufficiently small to drive down the teaching set size toward its minimum (which is nonzero because of the ceiling function). Specifically, for any fixed parameter θ∗ representing the target decision boundary:\n• (homogeneous SVM): we can choose t ≤ 1√ λ‖θ∗‖ so that TD({tθ ∗},Ahomsvm) = 1; • (homogeneous logistic regression): we can choose t ≤ √ τmax√ λ‖θ∗‖ so that TD({tθ ∗},Ahomlog ) = 1; • (inhomogeneous SVM): we can choose t ≤ √ 2√\nλ‖w∗‖ so that TD({tθ ∗},Ainhsvm) = 2;\n• (inhomogeneous logistic regression): we can choose t ≤ √ 2τmax√ λ‖w∗‖ so that TD({tθ ∗},Ainhlog ) = 2.\nThe resulting teaching dimension TD({tθ∗},Aopt) is listed in Table 1 on the row marked by “decision boundary.” The teaching set construction is the same as in sections 3.2 and 3.3, respectively, but with tθ∗."
    }, {
      "heading" : "5 Related Work",
      "text" : "Teaching dimension as a learning-theoretic quantity has attracted a long history of research. It was proposed independently in [13, 23]. Subsequent theoretical developments can be found in e.g. [26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12]. Most of them assume little extra knowledge on the learner other than that it is consistent with the training data. While such version-space learners are elegant object of theoretical study, they diverge from the practice of modern machine learning. Our present paper is among the first to extend teaching dimension to optimization-based machine learners.\nTeaching dimension is distinct from VC dimension. For a finite hypothesis space H, Goldman and Kearns [13] proved the relation\nV C(H)/ log(|H|) ≤ TD(H) ≤ V C(H) + |H| − 2V C(H). (45)\nThese inequalities are somewhat weak, as Goldman and Kearns had shown both cases where one quantity is much larger than the other. The distinction between TD and VC dimension is also present in our setting. For example, by inspecting the inhomogeneous SVM column in Table 1 we note that TD does not depend on the dimensionality d of the feature space Rd. To see why this makes intuitive sense, note two d-dimensional points are sufficient to specify any bisecting hyperplane in Rd. On the other hand, recall that the VC dimension for inhomogeneous hyperplanes in Rd is d + 1. Further quantification of the relation between TD and VC (and other capacity measures) remains an open research question.\nThe teaching setting we considered is also distinct from active learning. In teaching the teacher knows the target model a priori and her goal is to encode the target model as a training set, knowing that the decoder is special (namely a specific machine learning algorithm). This communication perspective highlights the difference to active learning, which must explore the hypothesis space to find the target model. Consequently, the teaching dimension can be dramatically smaller than the active learning query complexity for the same learner and hypothesis space. For example, Zhu [24] demonstrated that to learn a 1D threshold classifier within ǫ error, the teaching dimension is a constant TD=2 regardless of ǫ, while active learning would require O(log 1\nǫ ) queries which can be\narbitrarily larger than TD. While the present paper focused on the theory of optimal teaching, there are practical applications, too. One such application is computer-aided personalized education. The human student is modeled by a computational cognitive model, or equivalently the learning algorithm. The educational goal is encoded in the target model. The optimal teaching set is then well-defined, and represents the best personalized lesson for the student [25, 24, 16]. Patil et al. showed that human students learn statistically significantly better under such optimal teaching set compared to an i.i.d. training set [21]. Because contemporary cognitive models often employ optimization-based machine learners, our teaching dimension study helps to characterize these optimal lessons.\nAnother application of optimal teaching is in computer security. In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1]. Here the “teacher” is an attacker who has a nefarious target model in mind. The “student” is a learning agent (such as a spam filter) which accepts data and adapts itself. The attacker wants to minimally manipulate the input data in order to manipulate the learning agent toward the attacker’s target model. Teaching dimension quantifies the difficulty of data-poisoning attacks, and enables research on defenses.\nTeaching dimension also has applications in interactive machine learning to quantify the minimum human interaction necessary [10], and in formal synthesis to generate computer programs satisfying a specification [15]."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have presented a generalization on teaching dimension to optimization-based learners. To the best of our knowledge, our teaching dimension for ridge regression, SVM, and logistic regression is new; so are the lower bounds and our analysis technique in general.\nThere are many possible extensions to the present work. For example, one may extend our analysis to nonlinear learners. This can potentially be achieved by using the kernel trick on the linear learners. As another example, one may allow “approximate teaching” by relaxing the teaching goal, such that teaching is considered successful if the learner arrives at a model close enough to the target model. Taken together, the present paper and its extensions are expected to enrich our understanding of optimal teaching and enable novel applications."
    } ],
    "references" : [ {
      "title" : "Data poisoning attacks against autoregressive models",
      "author" : [ "S. Alfeld", "X. Zhu", "P. Barford" ],
      "venue" : "AAAI,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Queries revisited",
      "author" : [ "D. Angluin" ],
      "venue" : "Theoretical Computer Science, 313(2):175–194,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Teachers, learners and black boxes",
      "author" : [ "D. Angluin", "M. Krikis" ],
      "venue" : "COLT,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning from different teachers",
      "author" : [ "D. Angluin", "M. Krikis" ],
      "venue" : "Machine Learning, 51(2):137–163,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Measuring teachability using variants of the teaching dimension",
      "author" : [ "F.J. Balbach" ],
      "venue" : "Theor. Comput. Sci., 397(1-3):94–113, May",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Teaching randomized learners",
      "author" : [ "F.J. Balbach", "T. Zeugmann" ],
      "venue" : "COLT, pages 229–243,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Recent developments in algorithmic teaching",
      "author" : [ "F.J. Balbach", "T. Zeugmann" ],
      "venue" : "Proceedings of the 3rd International Conference on Language and Automata Theory and Applications, pages 1–18,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The security of machine learning",
      "author" : [ "M. Barreno", "B. Nelson", "A.D. Joseph", "J.D. Tygar" ],
      "venue" : "Machine Learning Journal, 81(2):121–148,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Self-directed learning and its relation to the VC-dimension and to teacher-directed learning",
      "author" : [ "S. Ben-David", "N. Eiron" ],
      "venue" : "Machine Learning, 33(1):87–104,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Mixed-initiative active learning",
      "author" : [ "M. Cakmak", "A. Thomaz" ],
      "venue" : "ICML Workshop on Combining Learning Strategies to Reduce Label Cost,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the LambertW function",
      "author" : [ "R. Corless", "G. Gonnet", "D. Hare", "D. Jeffrey", "D. Knuth" ],
      "venue" : "Advances in Computational Mathematics, 5(1):329–359,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Recursive teaching dimension, VC-dimension and sample compression",
      "author" : [ "T. Doliwa", "G. Fan", "H.U. Simon", "S. Zilles" ],
      "venue" : "Journal of Machine Learning Research, 15:3107–3131,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the complexity of teaching",
      "author" : [ "S. Goldman", "M. Kearns" ],
      "venue" : "Journal of Computer and Systems Sciences, 50(1):20–31,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Teaching a smarter learner",
      "author" : [ "S. Goldman", "H. Mathias" ],
      "venue" : "Journal of Computer and Systems Sciences, 52(2):255267,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "A theory of formal synthesis via inductive learning",
      "author" : [ "S. Jha", "S.A. Seshia" ],
      "venue" : "CoRR, abs/1505.03953,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "How do humans teach: On curriculum learning and teaching dimension",
      "author" : [ "F. Khan", "X. Zhu", "B. Mutlu" ],
      "venue" : "NIPS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Complexity of teaching by a restricted number of examples",
      "author" : [ "H. Kobayashi", "A. Shinohara" ],
      "venue" : "COLT, pages 293–302,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A model of interactive teaching",
      "author" : [ "H.D. Mathias" ],
      "venue" : "J. Comput. Syst. Sci., 54(3):487–501,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "The security of latent Dirichlet allocation",
      "author" : [ "S. Mei", "X. Zhu" ],
      "venue" : "AISTATS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Using machine teaching to identify optimal training-set attacks on machine learners",
      "author" : [ "S. Mei", "X. Zhu" ],
      "venue" : "AAAI,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Optimal teaching for limited-capacity human learners",
      "author" : [ "K. Patil", "X. Zhu", "L. Kopec", "B. Love" ],
      "venue" : "NIPS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Being taught can be faster than asking questions",
      "author" : [ "R.L. Rivest", "Y.L. Yin" ],
      "venue" : "COLT,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Teachability in computational learning",
      "author" : [ "A. Shinohara", "S. Miyano" ],
      "venue" : "New Generation Computing, 8(4):337–348,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Machine teaching for Bayesian learners in the exponential family",
      "author" : [ "X. Zhu" ],
      "venue" : "NIPS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Machine teaching: an inverse problem to machine learning and an approach toward optimal education",
      "author" : [ "X. Zhu" ],
      "venue" : "AAAI,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Models of cooperative teaching and learning",
      "author" : [ "S. Zilles", "S. Lange", "R. Holte", "M. Zinkevich" ],
      "venue" : "Journal of Machine Learning Research, 12:349–384,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "How many training items are needed? This is the question addressed by the teaching dimension [13, 23].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 22,
      "context" : "How many training items are needed? This is the question addressed by the teaching dimension [13, 23].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "[12]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "For any such δ , simple algebra verifies that θ∗+ tδ satisfies the KKT condition (7) for any t ∈ [0, 1].",
      "startOffset" : 97,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "• If the loss function l(·, ·) is smooth (not necessary convex) in the first argument, we have f(θ∗) = f(θ∗ + δ) by using the Taylor expansion (recall f is defined in equation 1): f(θ∗ + δ) =f(θ∗) + 〈∇f(θ∗ + tδ), δ〉 (for some t ∈ [0, 1]) =f(θ∗) + 〈 n ∑",
      "startOffset" : 230,
      "endOffset" : 236
    }, {
      "referenceID" : 0,
      "context" : "I(a) =    1, if a < 1 [0, 1], if a = 1 0, otherwise .",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "where the last line is due to I ( λ‖θ∗‖2 ⌈λ‖θ∗‖2⌉ ) giving either the set [0, 1] or the value 1.",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "However, due to the negative log likelihood term we have a coefficient defined by the Lambert W function [11], which we denote by Wlam.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 12,
      "context" : "It was proposed independently in [13, 23].",
      "startOffset" : 33,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : "It was proposed independently in [13, 23].",
      "startOffset" : 33,
      "endOffset" : 41
    }, {
      "referenceID" : 25,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 13,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 16,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 21,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "For a finite hypothesis space H, Goldman and Kearns [13] proved the relation V C(H)/ log(|H|) ≤ TD(H) ≤ V C(H) + |H| − 2 C(H).",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 23,
      "context" : "For example, Zhu [24] demonstrated that to learn a 1D threshold classifier within ǫ error, the teaching dimension is a constant TD=2 regardless of ǫ, while active learning would require O(log 1 ǫ ) queries which can be arbitrarily larger than TD.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 24,
      "context" : "The optimal teaching set is then well-defined, and represents the best personalized lesson for the student [25, 24, 16].",
      "startOffset" : 107,
      "endOffset" : 119
    }, {
      "referenceID" : 23,
      "context" : "The optimal teaching set is then well-defined, and represents the best personalized lesson for the student [25, 24, 16].",
      "startOffset" : 107,
      "endOffset" : 119
    }, {
      "referenceID" : 15,
      "context" : "The optimal teaching set is then well-defined, and represents the best personalized lesson for the student [25, 24, 16].",
      "startOffset" : 107,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "training set [21].",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 18,
      "context" : "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "Teaching dimension also has applications in interactive machine learning to quantify the minimum human interaction necessary [10], and in formal synthesis to generate computer programs satisfying a specification [15].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "Teaching dimension also has applications in interactive machine learning to quantify the minimum human interaction necessary [10], and in formal synthesis to generate computer programs satisfying a specification [15].",
      "startOffset" : 212,
      "endOffset" : 216
    } ],
    "year" : 2015,
    "abstractText" : "Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners.",
    "creator" : "LaTeX with hyperref package"
  }
}