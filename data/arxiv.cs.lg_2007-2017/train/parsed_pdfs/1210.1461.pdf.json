{
  "name" : "1210.1461.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound∗",
    "authors" : [ "Shusen Wang", "Zhihua Zhang", "Jian Li" ],
    "emails" : [ "wss@zju.edu.cn", "zhzhang@zju.edu.cn", "lijian@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n21 0.\n14 61\nv1 [\ncs .L\nKeywords: Large-scale matrix computations, low-rank matrix approximation, CUR matrix decomposition, randomized algorithms"
    }, {
      "heading" : "1. Introduction",
      "text" : "Large-scale matrices emerging from stocks, genomes, web documents, web images and videos everyday bring new challenges in modern data analysis. Most efforts have been focused on manipulating, understanding and interpreting large-scale data matrices. In many cases, matrix factorization methods are employed to construct compressed and informative representations to facilitate computation and interpretation. A principled approach is the truncated singular value decomposition (SVD) which finds the best low-rank approximation of a data matrix. Applications of SVD such as eigenface (Sirovich and Kirby, 1987, Turk and Pentland, 1991) and latent semantic analysis (Deerwester et al., 1990) have been illustrated to be very successful.\nHowever, the basis vectors resulting from SVD have little concrete meaning, which makes it very difficult for us to understand and interpret the data in question. An example\n∗. An extended abstract of this paper has been accepted by NIPS 2012.\nin (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age− (1/ √ 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people’s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: “it would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.” Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix.\nThe CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that Ã = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1.\nThe CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + ǫ) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel.\nUnfortunately, all the existing CUR algorithms require a large number of columns and rows to be chosen. For example, for an m× n matrix A and a target rank k ≤ min{m,n}, the state-of-the-art CUR algorithm — the subspace sampling algorithm in Drineas et al. (2008) — requires exactly O(k4ǫ−6) rows or O(kǫ−4 log2 k) rows in expectation to achieve (1 + ǫ) relative-error ratio w.h.p. Moreover, the computational cost of this algorithm is at least the cost of the truncated SVD of A, that is, O(min{mn2, nm2}).1 The algorithms are therefore impractical for large-scale matrices.\nIn this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory and experiments. In particular, we show in Theorem 9 a novel randomized CUR algorithm with lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR algorithm in Drineas et al. (2008).\nThe rest of this paper is organized as follows. Section 2 lists some notations that will be used in this paper and Section 3 reviews two classes of CUR algorithms. Section 4 mainly introduces a column selection algorithm to which our work is closely related. Section 5 describes and analyzes our novel CUR algorithm. Section 6 empirically compares our proposed algorithm with the state-of-the-art algorithm. All proofs are deferred to Appendix B.\n1. Although some partial SVD algorithms, such as Krylov subspace methods, require only O(mnk) time, they are all numerical unstable. See Halko et al. (2011) for more discussions."
    }, {
      "heading" : "2. Notations",
      "text" : "For a matrix A = [aij ] ∈ Rm×n, let a(i) be its i-th row and aj be its j-th column. Let ‖A‖1 = ∑ i,j |aij | be the ℓ1-norm, ‖A‖F = ( ∑ i,j a 2 ij)\n1/2 be the Frobenius norm, and ‖A‖2 = max‖x‖2=1 ‖Ax‖2 be the spectral norm. Moreover, let Im denote the m×m identity matrix, and 0 denotes the zero matrix whose size dependents on the context. Let ρ = rank(A) and k ≤ ρ, the SVD of A can be written as\nA =\nρ ∑\ni=0\nσA,iuA,iv T A,i = UAΣAV T A = UA,kΣA,kV T A,k +UA,k⊥ΣA,k⊥V T A,k⊥,\nwhere UA,k, ΣA,k, and VA,k correspond to the top k singular values. We denote Ak = UA,kΣA,kV T A,k. Furthermore, let A † = UA,ρΣ −1 A,ρV T A,ρ be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003).\nGiven matrices A ∈ Rm×n, X ∈ Rm×p, and Y ∈ Rq×n, XX†A = UXUTXA ∈ Rm×n is the projection of A onto the column space of X, and AY†Y = AVYV T Y\n∈ Rm×n is the projection of A onto the row space of Y. Finally, given an integer k ≤ p, we define the matrix ΠX,k(A) ∈ Rm×n as the best approximation to A within the column space of X that has rank at most k. We have ΠX,k(A) = XẐ where Ẑ = argminrank(Z)≤k ‖A−XZ‖F . We also have that ‖A−XX†A‖F ≤ ‖A−ΠX,k(A)‖F ."
    }, {
      "heading" : "3. Previous Work in CUR Matrix Decomposition",
      "text" : "This section discusses two recent developments of the CUR algorithms. Section 3.1 introduces an additive-error CUR algorithm in Drineas et al. (2006), and Section 3.2 describes two relative-error CUR algorithms in Drineas et al. (2008)."
    }, {
      "heading" : "3.1 The Linear-Time CUR Algorithm",
      "text" : "The linear-time CUR algorithm is proposed by Drineas et al. (2006). It is a highly efficient algorithm. Given a matrix A and a constant k < rank(A), by sampling c = 64kǫ−4 columns and r = 4kǫ−2 rows of A and computing an intersection matrix U, the resulting CUR decomposition satisfies the following additive-error bound\nE‖A−CUR‖F ≤ ‖A−Ak‖F + ǫ‖A‖F .\nFurthermore, the decomposition also satisfies rank(CUR) ≤ k. Here we give its main results (Theorem 4 of Drineas et al., 2006) in the following proposition.\nProposition 1 (The Linear-Time CUR Algorithm) Given a matrix A ∈ Rm×n, we let pi = ‖a(i)‖22/‖A‖2F and qj = ‖aj‖22/‖A‖2F . The linear-time CUR algorithm randomly samples c columns of A with probabilities {qj}nj=1 and r rows of A with probabilities {pi}mi=1. Then\nE‖A−CUR‖F ≤ ‖A−Ak‖F + ( (4k/c)1/4 + (k/r)1/2 ) ‖A‖F .\nThe algorithm costs O(mc2 +nr+ c2r+ c3) time, which is linear in (m+ n) by assuming c and r are constants."
    }, {
      "heading" : "3.2 The Subspace Sampling CUR Algorithm",
      "text" : "Drineas et al. (2008) proposed a two-stage randomized CUR algorithm which has a relativeerror bound w.h.p. In the first stage the algorithm samples c columns of A to construct C, and in the second stage it samples r rows from A and C simultaneously to construct R and U†. In the first stage the sampling probabilities are proportional to the squared ℓ2-norm of the rows of VA,k, in the second stage the sampling probabilities are proportional to the squared ℓ2-norm of the rows of UC,k. That is why it is called the “subspace sampling algorithm”. Here we show the main results of the subspace sampling algorithms in the following proposition.\nProposition 2 (The Subspace Sampling CUR Algorithm) Given a matrix A ∈ Rm×n and an integer k ≪ min{m,n}, the subspace sampling algorithm uses exactly sampling to select exactly c = O(k2ǫ−2 log(1/δ)) columns of A to construct C, and then exactly r = O(c2ǫ−2 log(1/δ)) rows of A to construct R, or uses expected sampling to select c = O(kǫ−2 log k log(1/δ)) columns and r = O(cǫ−2 log c log(1/δ)) rows in expectation. Then with probability at least (1− δ),\n‖A−CUR‖F ≤ (1 + ǫ)‖A−Ak‖F .\nHere, the matrix U is a weighted Moore-Penrose inverse of the intersection between C and R. The running time of both algorithms is dominated by the truncated SVD of A.\nAlthough the algorithm is ǫ-optimal with high probability, it requires too many rows get chosen: at least r = O(kǫ−4 log2 k) rows in expectation. In this paper we seek to devise an algorithm with mild requirement on column and row numbers."
    }, {
      "heading" : "4. Theoretical Backgrounds",
      "text" : "Section 4.1 considers the connections between the column selection problem and the CUR matrix decomposition problem. Section 4.2 introduces a near-optimal relative-error column selection algorithm. Our proposed CUR algorithm is motivated by and partly based on the near-optimal column selection algorithm."
    }, {
      "heading" : "4.1 Connections between Column Selection and CUR Matrix Decomposition",
      "text" : "Column selection is a well-established problem which has been widely studied in the literature: (Frieze et al., 2004, Deshpande et al., 2006, Drineas et al., 2008, Deshpande and Rademacher, 2010, Boutsidis et al., 2011b, Guruswami and Sinop, 2012).\nGiven a matrix A ∈ Rm×n, column selection aims to choose c columns of A to construct C ∈ Rm×c so that ‖A − CC†A‖F achieves the minimum. Since there are (nc ) possible choices of constructing C, so selecting the best subset is a hard problem. In recent years, many polynomial-time approximate algorithms have been proposed, among which we are particularly interested in those algorithms with relative-error bounds; that is, with c ≥ k columns selected from A, there is a constant η such that\n‖A−CC†A‖F ≤ η‖A−Ak‖F .\nWe call η the relative-error ratio. For some randomized algorithms, the inequality holds either w.h.p. or in expectation w.r.t. C.\nThe CUR matrix decomposition problem has a close connection with the column selection problem. As aforementioned, the first stage of existing CUR algorithms is simply a column selection procedure. However, the second stage is more complicated. If the second stage is näıvely solved by a column selection algorithm on AT , then the error ratio will trivially be 2η.\nFor a relative-error CUR algorithm, the first stage seeks to bound a construction error\nratio of ‖A−CC † A‖F ‖A−Ak‖F , while the section stage seeks to bound ‖A−CC † AR † R‖F ‖A−CC†A‖F given C. Actually, the first stage is a special case of the second stage where C = Ak. Given a matrix A, if an algorithm solving the second stage results in a bound ‖A−CC †AR†R‖F\n‖A−CC†A‖F ≤ η, then this\nalgorithm also solves the column selection problem for AT with an η relative-error ratio. Thus the second stage of CUR is a generalization of the column selection problem."
    }, {
      "heading" : "4.2 The Near-Optimal Column Selection Algorithm",
      "text" : "Recently, Boutsidis et al. (2011a) proposed a randomized algorithm which selects only c = 2kǫ−1(1 + o(1)) columns to achieve the expected relative-error ratio (1 + ǫ). Boutsidis et al. (2011a) also proved the lower bound of the column selection problem; that is, at least c = kǫ−1 columns are selected to achieve the (1 + ǫ) ratio. Thus this algorithm is near optimal. Though an optimal algorithm recently proposed by Guruswami and Sinop (2012) achieves the the lower bound, the optimal algorithm is quite inefficient compared with the near-optimal algorithm.\nThe near-optimal algorithm has three steps: the approximate SVD via random projection (Halko et al., 2011), the dual set sparsification algorithm (Boutsidis et al., 2011a), and the adaptive sampling algorithm (Deshpande et al., 2006). Here we present the main results of this algorithm in Lemma 3. To better understand the algorithm, we also give the details of the three steps, respectively.\nLemma 3 (Near-Optimal Column Selection Algorithm) Given a matrix A ∈ Rm×n of rank ρ, a target rank k (2 ≤ k < ρ), and 0 < ǫ < 1, there exists a randomized algorithm to select at most\nc = 2k\nǫ\n( 1 + o(1) )\ncolumns of A to form a matrix C ∈ Rm×c such that\nE 2‖A−CC†A‖F ≤ E‖A−CC†A‖2F ≤ (1 + ǫ)‖A−Ak‖2F ,\nwhere the expectations are taken w.r.t. C. Furthermore, the matrix C can be obtained in O((mnk + nk3)ǫ−2/3).\nThe dual set sparsification algorithm requires the top k right singular vectors of A as inputs. Since SVD is time consuming, Boutsidis et al. (2011a) employed an approximation SVD algorithm (Halko et al., 2011) to speedup computation. We give the theoretical analysis of the approximation SVD via random projection in Lemma 4. The resulting matrix Z approximates VA,k.\nLemma 4 (Randomized SVD via Random Projection) Given a matrix A ∈ Rm×n of rank ρ, a target rank k (k < ρ), and 0 < ǫ0 < 1, the algorithm computes a factorization A = BZT +E with B = AZ, ZTZ = Ik, and EZ = 0 such that\nE‖E‖2F ≤ (1 + ǫ0)‖A−Ak‖2F .\nThe algorithm runs in O(mnkǫ−10 ) time.\nThe second step of the near-optimal column selection algorithm is the dual set sparsification proposed by Boutsidis et al. (2011a). When ones take A and the top k (approximate) right singular vectors of A as inputs, the dual set sparsification algorithm can deterministically selects c1 columns of A to construct C1. We present their results in Lemma 5 and attach the concrete algorithm in Appendix A.\nLemma 5 (Column Selection via Dual Set Sparsification Algorithm) Given a matrix A ∈ Rm×n of rank ρ and a target rank k (< ρ), the dual set spectral-Frobenius sparsification algorithm deterministically selects c1 (> k) columns of A to form a matrix C1 ∈ Rm×c1 such that\n∥ ∥ ∥ A−ΠC1,k(A) ∥ ∥ ∥\nF ≤\n√\n1 + 1\n(1− √ k/c1)2\n∥ ∥ ∥ A−Ak ∥ ∥ ∥\nF .\nMoreover, the matrix C1 can be computed in TVA,k + O(mn + nc1k2), where TVA,k is the time needed to compute the top k right singular vectors of A.\nAfter sampling c1 columns of A, the near-optimal column selection algorithm uses the adaptive sampling of Deshpande et al. (2006) to select c2 columns of A to further reduce the construction error. We present Theorem 2.1 in Deshpande et al. (2006) in the following lemma.\nLemma 6 (The Adaptive Sampling Algorithm) Given a matrix A ∈ Rm×n, we let C1 ∈ Rm×c1 consists of c1 columns of A, and define the residual B = A−C1C†1A. Additionally, for i = 1, · · · , n, we define\npi = ‖bi‖22/‖B‖2F .\nWe further sample c2 columns i.i.d. from A, in each trial of which the i-th column is chosen with probability pi. Let C2 ∈ Rm×c2 contain the c2 sampled rows and let C = [C1,C2] ∈ R m×(c1+c2). Then, for any integer k > 0, the following inequality holds:\nE‖A−CC†A‖2F ≤ ‖A−Ak‖2F + k\nr2 ‖A−C1C†1A‖2F ,\nwhere the expectation is taken w.r.t. C2.\nAlgorithm 1 The Fast CUR Algorithm.\n1: Input: a real matrixA ∈ Rm×n, target rank k, ǫ ∈ (0, 1], target column number c = 2kǫ ( 1+o(1) ) ,\ntarget row number r = 2cǫ ( 1 + o(1) )\n; 2: // Stage 1: select c columns of A to construct C ∈ Rm×c 3: Compute approximate truncated SVD via random projection such that Ak ≈ ŨkΣ̃kṼk; 4: Construct U1 ← columns of (A− ŨkΣ̃kṼk); V1 ← columns of ṼTk ; 5: Compute s1 ← Dual Set Spectral-Frobenius Sparsification Algorithm (U1, V1, c− 2k/ǫ); 6: Construct C1 ← ADiag(s1), and then delete the all-zero columns; 7: Residual matrix D ← A−C1C†1A; 8: Compute sampling probabilities: pi = ‖di‖22/‖D‖2F , i = 1, · · · , n; 9: Sampling c2 = 2k/ǫ columns from A with probability {p1, · · · , pn} to construct C2; 10: // Stage 2: select r rows of A to construct R ∈ Rr×n 11: Construct U2 ← columns of (A− ŨkΣ̃kṼk)T ; V2 ← columns of ŨTk ; 12: Compute s2 ← Dual Set Spectral-Frobenius Sparsification Algorithm (U2, V2, r − 2c/ǫ); 13: Construct R1 ← Diag(s2)A, and then delete the all-zero rows; 14: Residual matrix B ← A−AR†1R1; 15: Compute sampling probabilities: qj = ‖b(j)‖22/‖B‖2F , j = 1, · · · ,m; 16: Sampling r2 = 2c/ǫ rows from A with probability {q1, · · · , qm} to construct R2; 17: return C = [C1,C2], R = [R T 1 ,R T 2 ] T , and U = C†AR†."
    }, {
      "heading" : "5. Main Results",
      "text" : "In this section we develop a novel CUR algorithm that we call the fast CUR algorithm due to its lower time complexity in comparison with SVD. We describe the procedure in Algorithm 1 and give theoretical analysis in Theorem 9.\nThe main results of our work are formally shown in three theorems in this section. The proofs are deferred to Appendix B. Theorem 9 relies on Lemma 3 and Theorem 8, and Theorem 8 relies on Theorem 7. Theorem 7 is a generalization of Lemma 6, and Theorem 8 is a generalization of Lemma 3."
    }, {
      "heading" : "5.1 Adaptive Sampling",
      "text" : "The relative-error adaptive sampling algorithm is established in Theorem 2.1 of Deshpande et al. (2006). The algorithm is based on the following idea: after selecting a proportion of columns from A to form C1 by an arbitrary algorithm, the algorithms randomly samples additional c2 columns according to the residual A − C1C†1A. Boutsidis et al. (2011a) used the adaptive sampling algorithm to decrease the residual of the dual set sparsification algorithm and obtained an (1 + ǫ) relative-error ratio. Here we prove a new bound for the same adaptive sampling algorithm. Interestingly, this new bound is a generalization of the original one in Theorem 2.1 of Deshpande et al. (2006). In other words, Theorem 2.1 of Deshpande et al. (2006) is a direct corollary of our following theorem when C = Ak.\nTheorem 7 (The Adaptive Sampling Algorithm) Given a matrix A ∈ Rm×n and a matrix C ∈ Rm×c such that rank(C) = rank(CC†A) = ρ (ρ ≤ c ≤ n), we let R1 ∈ Rr1×n consist of r1 rows of A, and define the residual B = A − AR†1R1. Additionally, for i = 1, · · · ,m, we define\npi = ‖b(i)‖22/‖B‖2F .\nWe further sample r2 rows i.i.d. from A, in each trial of which the i-th row is chosen with probability pi. Let R2 ∈ Rr2×n contain the r2 sampled rows and let R = [RT1 ,RT2 ]T ∈ R (r1+r2)×n. Then the following inequality holds:\nE‖A−CC†AR†R‖2F ≤ ‖A−CC†A‖2F + ρ\nr2 ‖A−AR†1R1‖2F ,\nwhere the expectation is taken w.r.t. R2."
    }, {
      "heading" : "5.2 The Fast CUR Algorithm",
      "text" : "Based on the randomized SVD algorithm of Lemma 4, the dual set sparsification algorithm of Lemma 5, and the adaptive sampling algorithm of Theorem 7, we develop a randomized algorithm to solve the second stage of the CUR problem. We present the results of the algorithm in the following theorem.\nTheorem 8 (The Fast Row Selection Algorithm) Given a matrix A ∈ Rm×n and a matrix C ∈ Rm×c such that rank(C) = rank(CC†A) = ρ (ρ ≤ c ≤ n), and a target rank k (≤ ρ), the proposed randomized algorithm selects r = 2ρǫ (1 + o(1)) rows of A to construct R ∈ Rr×n, such that\nE‖A−CC†AR†R‖2F ≤ ‖A−CC†A‖2F + ǫ‖A−Ak‖2F ,\nwhere the expectation is taken w.r.t. R. Furthermore, the matrix R can be computed in O((mnk +mk3)ǫ−2/3) time.\nNote that Lemma 3, i.e., Theorem 5 of Boutsidis et al. (2011a), is a special case of Theorem 8 when C = Ak. Based on Lemma 3 and Theorem 8, we have the main theorem for the fast CUR algorithm as follows.\nTheorem 9 (The Fast CUR Algorithm) Given a matrix A ∈ Rm×n and a positive integer k ≪ min{m,n}, the fast CUR algorithm described in Algorithm 1 randomly selects c = 2kǫ (1+o(1)) columns of A to construct C ∈ Rm×c with the near-optimal column selection algorithm of Lemma 3, and then selects r = 2cǫ (1 + o(1)) rows of A to construct R ∈ Rr×n with the fast row selection algorithm of Theorem 8. Then we have\nE‖A−CUR‖F = E‖A−C(C†AR†)R‖F ≤ (1 + ǫ)‖A−Ak‖F .\nMoreover, the algorithm runs in time O ( mnkǫ−2/3 + (m+ n)k3ǫ−2/3 +mk2ǫ−2 + nk2ǫ−4 ) .\nSince k, c, r ≪ min{m,n} by the assumption, so the time complexity of the fast CUR algorithm is lower than that of the SVD of A. This is the main reason why we call it the fast CUR algorithm.\nAnother advantage of this algorithm is that it can avoid loading the whole m× n data matrix A into main memory. None of three steps — the randomized SVD, the dual set sparsification algorithm, and the adaptive sampling — requires loading the whole of A into memory. The most memory-expensive operation throughout the fast CUR Algorithm is computing the Moore-Penrose inverses of C and R, which requires maintaining an m × c matrix or an r×n matrix in memory. In contrast, the subspace sampling algorithm requires loading the whole matrix into memory to compute its truncated SVD."
    }, {
      "heading" : "6. Empirical Analysis",
      "text" : "In this section we conduct empirical comparisons among the relative-error CUR algorithms on several datasets. We report the relative-error ratio and the running time of each algorithm on each data set. The relative-error ratio is defined by\nRelative-error ratio = ‖A−CUR‖F ‖A−Ak‖F ,\nwhere k is a specified target rank."
    }, {
      "heading" : "6.1 Datasets",
      "text" : "We implement experiments on five datasets, including natural images, biology data, and bags of words. Table 1 briefly summarizes some information of the datasets. The Redrock and Edinburgh (Agarwala, 2007) are two large size natural images. Arcene and Dexter are both from the UCI datasets (Frank and Asuncion, 2010). Arcene is a biology dataset with 900 instances and 10000 attributes. Dexter is a bag of words dataset with a 20000- vocabulary and 2600 documents. PicasaWeb image dataset (Wang et al., 2012) contains 6.8 million PicasaWeb images. We use the HMAX features (Serre et al., 2007) and the SIFT features (Lowe, 1999) of the first 50000 images; the features provided by Wang et al. (2012) are all of 3000 dimensions. Each dataset is actually represented as a data matrix, upon which we apply the CUR algorithms.\nWhen the data matrices become very large, e.g., say 8K × 3K, the truncated SVD and the standard SVD are both infeasible in our experiment environment, and so is the subspace sampling algorithm. Therefore we do not conduct experiments on larger data matrices. In contrast, our fast CUR algorithm actually works well even for 30K × 3K matrices."
    }, {
      "heading" : "6.2 Setup",
      "text" : "We implement the subspace sampling algorithm and our fast CUR algorithm in MATLAB 7.10.0. We do not compare with the linear-time CUR algorithm for the following reason. There is an implicit projection operation in the linear-time CUR algorithm, so the result satisfies rank(CUR) ≤ k. However, this inequality does not hold for the subspace sampling algorithm and the fast CUR algorithm. Thus, comparing the construction error among the three CUR algorithm is very unfair for the linear-time CUR algorithm. Actually, the construction error of the linear-time CUR algorithm is much worse than the other two algorithms.\nWe conduct experiments on a workstation with 12 Intel Xeon 3.47GHz CPUs, 12GB memory, and Ubuntu 10.04 system. According to the analysis in Drineas et al. (2008) and this paper, k, c, and r should be integers much less than m and n. For each data set and each algorithm, we set k = 10, 20, or 50, and c = αk, r = αc, where α ranges in each set of experiments. We repeat each set of experiments for 20 times and report the average and the standard deviation of the error ratios. The results are depicted in Figures 1, 2, 3, 4, 5, and 6."
    }, {
      "heading" : "6.3 Result Analysis",
      "text" : "The results show that the fast CUR algorithm has much lower relative-error ratio than the subspace sampling algorithm. The experimental results well match our theoretical analyses in Section 5. As for the running time, the fast CUR algorithm is more efficient when c and r are small. When c and r become large, the fast CUR algorithm becomes less efficient. This is because the time complexity of the fast CUR algorithm is linear in ǫ−4 and large c and r imply small ǫ. However, the purpose of CUR is to select a small number of columns and rows from the data matrix, that is, c ≪ n and r ≪ m. Thus we are not interested in the cases where c and r are large compared with m and n, e.g., say k = 20 and α = 10."
    }, {
      "heading" : "7. Discussions",
      "text" : "In this paper we have proposed a novel randomized algorithm for the CUR matrix decomposition problem. This algorithm is faster, more scalable, and more accurate than the state-of-the-art algorithm, i.e., the subspace sampling algorithm. Our algorithm requires only c = 2kǫ−1(1+o(1)) columns and r = 2cǫ−1(1+o(1)) rows to achieve (1+ǫ) relative-error ratio. To achieve the same relative-error bound, the subspace sampling algorithm requires c = O(kǫ−2 log k) columns and r = O(cǫ−2 log c) rows selected from the original matrix. Our algorithm also beats the subspace sampling algorithms in time-complexity. Our algorithm costs O(mnkǫ−2/3 + (m + n)k3ǫ−2/3 +mk2ǫ−2 + nk2ǫ−4) time, which is lower than O(min{mn2,m2n}) of the subspace sampling algorithms when k is small. Moreover, our algorithm enjoys another advantage of avoiding loading the whole data matrix into main memory, which also makes our algorithm more scalable. Finally, the empirical comparisons have also demonstrated the effectiveness and efficiency of our algorithm.\nHowever, there are several open questions involving the lower bound of the CUR matrix decomposition problem. First, what is the lower bound for the CUR problem? Second, is there any algorithm achieving such a lower bound? Boutsidis et al. (2011b) proved a lower bound for the column selection problem: ‖A−CC†A‖2F ‖A−Ak‖ 2\nF\n≥ 1+ kc . We thus wonder if there is a\nsimilar lower bound on the ratio ‖A−CC†AR†R‖2F\n‖A−CC†A‖2 F\n, e.g., say (1 + rank(C)r ). We shall address\nthese questions in future work.\nAcknowledgments\nAlgorithm 2 Deterministic Dual Set Spectral-Frobenius Sparsification Algorithm.\n1: Input: U = {xi}ni=1 ⊂ Rl, (l < n); V = {vi}ni=1 ⊂ Rk, with ∑n i=1 viv T i = Ik (k < n); k < r < n; 2: Initialize: s0 = 0, A0 = 0; 3: Compute ‖xi‖22 for i = 1, · · · , n, and then compute δU = ∑ n i=1 ‖xi‖ 2 2\n1− √ k/r ;\n4: for τ = 0 to r − 1 do 5: Compute the eigenvalue decomposition of Aτ ; 6: Find an index j in {1, · · · , n} and compute a weight t > 0 such that\nδ−1U ‖xj‖22 ≤ t−1 ≤ vTj\n( Aτ − (Lτ + 1)Ik )−2 vj\nφ(Lτ + 1,Aτ )− φ(Lτ ,Aτ ) − vTj\n( Aτ − (Lτ + 1)Ik )−1 vj ;\nwhere\nφ(L,A) =\nk ∑\ni=1\n( λi(A)− L )−1 , Lτ = τ − √ rk;\n7: Update the j-th component of sτ and Aτ : sτ+1[j] = sτ [j] + t, Aτ+1 = Aτ + tvjv T j ; 8: end for 9: return s = 1− √ k/r\nr sr.\nThis work has been supported in part by the Natural Science Foundations of China (No. 61070239) and the Google visiting faculty program."
    }, {
      "heading" : "Appendix A. The Dual Set Sparsification Algorithm",
      "text" : "For the sake of completeness, we attach the dual set sparsification algorithm here and describe some implementation details. The dual set sparsification algorithms are deterministic algorithms established in Boutsidis et al. (2011a). The fast CUR algorithm calls the dual set spectral-Frobenius sparsification algorithm (Lemma 13 in Boutsidis et al., 2011a) in both stages. We show this algorithm in Algorithm 2 and its bounds in Lemma 10.\nLemma 10 (Dual Set Spectral-Frobenius Sparsification) Let U = {x1, · · · ,xn} ⊂ R l (l < n) contain the columns of an arbitrary matrix X ∈ Rl×n. Let V = {v1, · · · ,vn} ⊂ Rk (k < n) be a decompositions of the identity, i.e., ∑n\ni=1 viv T i = Ik. Given an integer r with\nk < r < n, Algorithm 2 deterministically computes a set of weights si ≥ 0 (i = 1, · · · , n) at most r of which are non-zero, such that\nλk\n( n ∑\ni=1\nsiviv T i\n) ≥ ( 1− √ k\nr\n)2 and tr (\nn ∑\ni=1\nsixix T i\n)\n≤ ‖X‖2F .\nThe weights si can be computed deterministically in O(rnk2 + nl) time.\nHere we would like to mention the implementation of Algorithm 2, which is not described by Boutsidis et al. (2011a) in details. In each iteration the algorithm performs once eigenvalue decomposition: Aτ = WΛW\nT . Here Aτ is guaranteed to be positive semi-definite in each iteration. Since\n( Aτ − αIk )q = WDiag ( (λ1 − α)q, · · · , (λk − α)q ) WT ,\nwe can efficiently compute (Aτ − (Lτ +1)Ik)q based on the eigenvalue decomposition of Aτ . With the eigenvalues at hand, φ(L,Aτ ) can also be computed directly.\nThe algorithm runs in r iterations. In each iteration, the eigenvalue decomposition of Aτ requires O(k3), and the n comparisons in Line 6 each requires O(k2). Moreover, computing ‖xi‖22 for each xi requires O(nl). Overall, the running time of Algorithm 2 is at most O(rk3) +O(rnk2) +O(nl) = O(rnk2 + nl)."
    }, {
      "heading" : "Appendix B. Proofs",
      "text" : "B.1 The Proof of Theorem 7\nTheorem 7 can be equivalently expressed in Theorem 11. In order to stick to the column space convention of Boutsidis et al. (2011a), we prove Theorem 11 instead of Theorem 7.\nTheorem 11 (Adaptive Sampling Algorithm) Given a matrix A ∈ Rm×n and a matrix R ∈ Rr×n such that rank(R) = rank(AR†R) = ρ (ρ ≤ r ≤ m), let C1 ∈ Rm×c1 consist of c1 columns of A, and define the residual B = A−C1C†1A. For i = 1, · · · , n, let\npi = ‖bi‖22/‖B‖2F ,\nwhere bi is the i-th column of the matrix B. Sample further c2 columns from A in c2 i.i.d. trials, where in each trial the i-th column is chosen with probability pi. Let C2 ∈ Rm×c2 contain the c2 sampled columns and C = [C1,C2] ∈ Rm×(c1+c2) contain the columns of both C1 and C2, all of which are columns of A. Then the following inequality holds:\nE‖A−CC†AR†R‖2F ≤ ‖A−AR†R‖2F + ρ\nc2 ‖A−C1C†1A‖2F .\nwhere the expectation is taken w.r.t. C2.\nProof With a little abuse of symbols, we use bold uppercase letters to denote matrix random variables and bold lowercase to denote vector random variables, without distinguishing between matrix/vector random variables and constant matrices/vectors.\nWe denote the j-th column of VAR†R,ρ ∈ Rn×ρ as vj, and the (i, j)-th entry of VAR†R,ρ as vij . Define vector random variables xj,(l) ∈ Rm such that for j = 1, · · · , n and l = 1, · · · , c2,\nxj,(l) = vij pi bi = vij pi ( ai −C1C†1ai ) with probability pi, for i = 1, · · · , n,\nNote that xj,(l) is a linear function of a column of A sampled from the above defined distribution. We have that\nE[xj,(l)] =\nn ∑\ni=1\npi vij pi bi = Bvj ,\nE‖xj,(l)‖22 = n ∑\ni=1\npi v2ij p2i ‖bi‖22 = n ∑\ni=1\nv2ij ‖bi‖22/‖B‖2F ‖bi‖22 = ‖B‖2F .\nThen we let xj = 1 c2 ∑c2 l=1 xj,(l), we have\nE[xj ] = E[xj,(l)] = Bvj ,\nE‖xj −Bvj‖22 = E ∥ ∥ ∥ xj − E[xj ] ∥ ∥ ∥ 2\n2 =\n1\nc2 E\n∥ ∥ ∥ xj,(l) − E[xj,(l)] ∥ ∥ ∥ 2\n2 =\n1 c2 E‖xj,(l) −Bvj‖22.\nAccording to the construction of x1, · · · ,xρ, we define the c2 columns of A to be C2 ∈ R m×c2 . Note that all the random variables x1 · · · ,xρ lie in the subspace span(C1) + span(C2). We define random variables\nwj = C1C † 1AR †Rvj + xj = C1C † 1Avj + xj, for j = 1, · · · , ρ,\nwhere the second equality follows from Lemma 12 that AR†Rvj = Avj if vj is one of the top ρ right singular vectors of AR†R. Then we have that any set of random variables {w1, · · · ,wρ} lies in span(C) = span(C1) + span(C2). Let W = [w1, · · · ,wρ] be a matrix random variable, we have that span(W) ⊂ span(C). The expectation of wj is\nE[wj] = C1C † 1Avj + E[xj] = C1C † 1Avj +Bvj = Avj,\ntherefore we have that wj −Avj = xj −Bvj .\nThe expectation of ‖wj −Avj‖22 is\nE‖wj −Avj‖22 = E‖xj −Bvj‖22 = 1\nc2 E‖xj,(l) −Bvj‖22\n= 1\nc2 E‖xj,(l)‖22 −\n2 c2 (Bvj) T E[xj,(l)] + 1 c2 ‖Bvj‖22\n= 1\nc2 E‖xj,(l)‖22 −\n1 c2 ‖Bvj‖22 =\n1\nc2 ‖B‖2F −\n1 c2 ‖Bvj‖22\n≤ 1 c2 ‖B‖2F (1)\nTo complete the proof, we let the matrix variable\nF = (\nρ ∑\nq=1\nσ−1q wqu T q )AR †R,\nwhere σq is the q-th largest singular value of AR †R and uq is the corresponding left singular vector of AR†R. The column space of F is contained in span(W) ⊂ span(C), and thus\n‖AR†R−CC†AR†R‖2F ≤ ‖AR†R−WW†AR†R‖2F ≤ ‖AR†R− F‖2F .\nWe use F to bound the error ‖AR†R−CC†AR†R‖2F :\nE‖A−CC†AR†R‖2F = E‖A−AR†R+AR†R−CC†AR†R‖2F = E [ ‖A−AR†R‖2F + ‖AR†R−CC†AR†R‖2F ]\n(2)\n≤ ‖A−AR†R‖2F + E‖AR†R− F‖2F ,\nwhere (2) follows from that A(I −R†R) is orthogonal to (I −CC†)AR†R. Since AR†R and F both lies on the space spanned by the right singular vectors of AR†R, i.e. {vj}ρj=1, we decompose AR†R− F along {vj}ρj=1:\nE‖A−CC†AR†R‖2F ≤ ‖A−AR†R‖2F + E‖AR†R− F‖2F ,\n= ‖A−AR†R‖2F + ρ ∑\nj=1\nE\n∥ ∥ ∥ (AR†R− F)vj ∥ ∥ ∥ 2\n2\n= ‖A−AR†R‖2F + ρ ∑\nj=1\nE\n∥ ∥ ∥ AR†Rvj − (\nρ ∑\nq=1\nσ−1q wqu T q )σjuj\n∥ ∥ ∥ 2\n2\n= ‖A−AR†R‖2F + ρ ∑\nj=1\nE\n∥ ∥ ∥ AR†Rvj −wj ∥ ∥ ∥ 2\n2\n= ‖A−AR†R‖2F + ρ ∑\nj=1\nE‖Avj −wj‖22 (3)\n≤ ‖A−AR†R‖2F + ρ\nc2 ‖B‖2F , (4)\nwhere (3) follows from Lemma 12 and (4) follows from (1).\nLemma 12 We are given a matrix A ∈ Rm×n and a matrix R ∈ Rr×n such that rank(AR†R) = rank(R) = ρ (ρ ≤ r ≤ m). Letting vj ∈ Rn be the j-th top right singular vector of AR†R, we have that\nAR†Rvj = Avj, for j = 1, · · · , ρ.\nProof First let VR,ρ ∈ Rn×ρ contain the top ρ right singular vectors of R, then the projection of A onto the row space of R is AR†R = AVR,ρV T R,ρ. Let the thin SVD of AVR,ρ ∈ Rm×ρ be ŨΣ̃ṼT , where Ṽ ∈ Rρ×ρ. Then the compact SVD of AR†R is\nAR†R = AVR,ρV T R,ρ = ŨΣ̃Ṽ TVTR,ρ.\nAccording to the definition, vj is the j-th column of (VR,ρṼ) ∈ Rn×ρ, and thus vj lies on the column space of VR,ρ, and vj is orthogonal to VR,ρ⊥. Finally, since A − AR†R = AVR,ρ⊥V T R,ρ⊥, we have that vj is orthogonal to A−AR†R, that is, (A−AR†R)vj = 0, which directly proves the lemma.\nB.2 The Proof of Theorem 8\nBoutsidis et al. (2011a) proposed a randomized algorithm which achieves the expected relative-error bound in Lemma 13. This algorithm is described in Line 3 to 6 of Algorithm 1. Lemma 13 is a direct corollary of Lemma 4 and Lemma 5. If we apply the same algorithm to AT to select c rows of A to form R1, that is, Line 11 to 13 of Algorithm 1, then a very similar bound is guaranteed.\nLemma 13 (Boutsidis et al. (2011a), Theorem 4) Given a matrix A ∈ Rm×n of rank ρ, a target rank 2 ≤ k < ρ, and 0 < ǫ0 < 1, there is a randomized algorithm to select c1 > k columns of A and form a matrix C1 ∈ Rm×c1 such that\nE‖A−C1C†1A‖2F ≤ (1 + ǫ0) ( 1 + 1\n(1− √ k/c1)2\n)\n‖A−Ak‖2F ,\nwhere the expectation is taken w.r.t. C1. The matrix C1 can be computed in O(mnkǫ−10 + nc1k 2) time.\nWith Theorem 7 and Lemma 13, we now prove Theorem 8 as follows. Proof This randomized algorithm has three steps: approximate SVD via randomized projection (Halko et al., 2011), deterministic column selection via dual set sparsification algorithm (Boutsidis et al., 2011a) shown in Lemma 5, and the adaptive sampling algorithm of Theorem 7 proved in this paper. This algorithm is a generalization of the near-optimal column selection algorithm of Lemma 3.\nGiven A ∈ Rm×n and a target rank k < r1, step 1 (Line 3 of Algorithm 1) compute an approximate truncated SVD of A in O(mnk/ǫ0) time such that Ak ≈ Ãk = ŨkΣ̃kṼTk . Lemma 4 shows that\nE‖A− ŨkΣ̃kṼTk ‖2F ≤ (1 + ǫ0)‖A−Ak‖2F .\nStep 2 (Line 11 to 13 of Algorithm 1) selects r1 rows of A to construct R1 by the dual set sparsification algorithm taking U and V as input, where U contains all the m columns of (AT − ÃTk ) ∈ Rn×m, V contains all the m columns of ŨTA,k ∈ Rk×m. Lemma 13 shows that\nE‖A−AR†1R1‖2F ≤ (1 + ǫ0) ( 1 + 1\n(1− √ k/r1)2\n)\n‖A−Ak‖2F ,\nwhere the expectation is taken w.r.t. R1. Step 2 costs O(mr1k2 +mn) time. Step 3 (Line 14 to 16 of Algorithm 1) samples additional r2 rows of A to construct R2 ∈ R r2×n by the adaptive sampling algorithm of Theorem 7. Let R = [RT1 ,R T 2 ]\nT ∈ R(r1+r2)×n. We apply Theorem 7 and have that\nER‖A−CC†AR†R‖2F = ER1 [ ER2 [ ‖A−CC†AR†R‖2F ∣ ∣ ∣ R1 ]]\n≤ ER1 [ ‖A−CC†A‖2F + ρ\nr2 ‖A−AR†1R1‖2F\n]\n≤ ‖A−CC†A‖2F + ρ\nr2 (1 + ǫ0)\n( 1 + 1\n(1− √ k/r1)2\n)\n‖A−Ak‖2F .\nBy setting r1 = O(kǫ−2/3), r2 ≈ 2ρǫ , and ǫ0 = ǫ2/3, we conclude that\nE‖A−CC†AR†R‖2F ≤ ‖A−CC†A‖2F + ǫ‖A−Ak‖2F .\nThe total computation time of the three steps is O(mnk/ǫ0 +mr1k2 +mn) = O((mnk + mk3)ǫ−2/3)\nB.3 The Proof of Theorem 9\nProof Since C is constructed by columns of A, the column space of C is contained in the column space of A, so rank(CC†A) = rank(C) = ρ ≤ c, and thus the assumptions of Theorem 8 are satisfied. Lemma 3 and Theorem 8 together prove Theorem 9:\nE 2‖A−CUR‖F ≤ E‖A−CUR‖2F = EC,R‖A−CC†AR†R‖2F\n= EC\n[\nER\n[ ‖A−CC†AR†R‖2F ∣ ∣ ∣ C ]]\n≤ EC [ ‖A−CC†A‖2F + ǫ‖A−Ak‖2F ]\n≤ (1 + 2ǫ)‖A−Ak‖2k.\nFinally we have E‖A−CUR‖F ≤ (1 + ǫ)‖A−Ak‖k because 1 + 2ǫ ≤ (1 + ǫ)2. The time cost of the fast CUR algorithm is the sum of Stage 1, Stage 2, and the MoorePenrose inverse of C and R, i.e. O((mnk+nk3)ǫ−2/3)+O((mnk+mk3)ǫ−2/3)+O(mc2)+ O(nr2) = O(mnkǫ−2/3 + (m+ n)k3ǫ−2/3 +mk2ǫ−2 + nk2ǫ−4)."
    } ],
    "references" : [ {
      "title" : "Efficient gradient-domain compositing using quadtrees",
      "author" : [ "Aseem Agarwala" ],
      "venue" : "SIGGRAPH",
      "citeRegEx" : "Agarwala.,? \\Q2007\\E",
      "shortCiteRegEx" : "Agarwala.",
      "year" : 2007
    }, {
      "title" : "Generalized Inverses: Theory and Applications",
      "author" : [ "Adi Ben-Israel", "Thomas N.E. Greville" ],
      "venue" : "Second Edition. Springer,",
      "citeRegEx" : "Ben.Israel and Greville.,? \\Q2003\\E",
      "shortCiteRegEx" : "Ben.Israel and Greville.",
      "year" : 2003
    }, {
      "title" : "Near-optimal column-based matrix reconstruction",
      "author" : [ "Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail" ],
      "venue" : "CoRR, abs/1103.0995,",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2011
    }, {
      "title" : "Near optimal column-based matrix reconstruction",
      "author" : [ "Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail" ],
      "venue" : "In Proceedings of the 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2011
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman" ],
      "venue" : "Journal of The American Society for Information Science,",
      "citeRegEx" : "Deerwester et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "Efficient volume sampling for row/column subset selection",
      "author" : [ "Amit Deshpande", "Luis Rademacher" ],
      "venue" : "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Deshpande and Rademacher.,? \\Q2010\\E",
      "shortCiteRegEx" : "Deshpande and Rademacher.",
      "year" : 2010
    }, {
      "title" : "Matrix approximation and projective clustering via volume sampling",
      "author" : [ "Amit Deshpande", "Luis Rademacher", "Santosh Vempala", "Grant Wang" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "Deshpande et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Deshpande et al\\.",
      "year" : 2006
    }, {
      "title" : "Pass-efficient algorithms for approximating large matrices",
      "author" : [ "Petros Drineas" ],
      "venue" : "Proceeding of the 14th Annual ACM-SIAM Symposium on Dicrete Algorithms,",
      "citeRegEx" : "Drineas.,? \\Q2003\\E",
      "shortCiteRegEx" : "Drineas.",
      "year" : 2003
    }, {
      "title" : "On the Nyström method for approximating a gram matrix for improved kernel-based learning",
      "author" : [ "Petros Drineas", "Michael W. Mahoney" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Drineas and Mahoney.,? \\Q2005\\E",
      "shortCiteRegEx" : "Drineas and Mahoney.",
      "year" : 2005
    }, {
      "title" : "Fast monte carlo algorithms for matrices iii: Computing a compressed approximate matrix decomposition",
      "author" : [ "Petros Drineas", "Ravi Kannan", "Michael W. Mahoney" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2006
    }, {
      "title" : "Relative-error CUR matrix decompositions",
      "author" : [ "Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2008
    }, {
      "title" : "Fast monte-carlo algorithms for finding low-rank approximations",
      "author" : [ "Alan Frieze", "Ravi Kannan", "Santosh Vempala" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Frieze et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Frieze et al\\.",
      "year" : 2004
    }, {
      "title" : "A theory of pseudoskeleton approximations",
      "author" : [ "S.A. Goreinov", "E.E. Tyrtyshnikov", "N.L. Zamarashkin" ],
      "venue" : "Linear Algebra and Its Applications,",
      "citeRegEx" : "Goreinov et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Goreinov et al\\.",
      "year" : 1997
    }, {
      "title" : "Pseudo-skeleton approximations by matrices of maximal volume",
      "author" : [ "S.A. Goreinov", "N.L. Zamarashkin", "E.E. Tyrtyshnikov" ],
      "venue" : "Mathematical Notes,",
      "citeRegEx" : "Goreinov et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Goreinov et al\\.",
      "year" : 1997
    }, {
      "title" : "Optimal column-based low-rank matrix reconstruction",
      "author" : [ "Venkatesan Guruswami", "Ali Kemal Sinop" ],
      "venue" : "In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Guruswami and Sinop.,? \\Q2012\\E",
      "shortCiteRegEx" : "Guruswami and Sinop.",
      "year" : 2012
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Halko et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Halko et al\\.",
      "year" : 2011
    }, {
      "title" : "Computer Science Theory for the Information Age",
      "author" : [ "John Hopcroft", "Ravi Kannan" ],
      "venue" : null,
      "citeRegEx" : "Hopcroft and Kannan.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hopcroft and Kannan.",
      "year" : 2012
    }, {
      "title" : "Vector algebra in the analysis of genome-wide expression data",
      "author" : [ "Finny G. Kuruvilla", "Peter J. Park", "Stuart L. Schreiber" ],
      "venue" : "Genome Biology,",
      "citeRegEx" : "Kuruvilla et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kuruvilla et al\\.",
      "year" : 2002
    }, {
      "title" : "Object recognition from local scale-invariant features",
      "author" : [ "David G. Lowe" ],
      "venue" : "In Proceedings of the International Conference on Computer Vision, ICCV",
      "citeRegEx" : "Lowe.,? \\Q1999\\E",
      "shortCiteRegEx" : "Lowe.",
      "year" : 1999
    }, {
      "title" : "Divide-and-conquer matrix factorization",
      "author" : [ "Lester Mackey", "Ameet Talwalkar", "Michael I. Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mackey et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mackey et al\\.",
      "year" : 2011
    }, {
      "title" : "CUR matrix decompositions for improved data analysis",
      "author" : [ "Michael W. Mahoney", "Petros Drineas" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Mahoney and Drineas.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mahoney and Drineas.",
      "year" : 2009
    }, {
      "title" : "Robust object recognition with cortex-like mechanisms",
      "author" : [ "Thomas Serre", "Lior Wolf", "Stanley Bileschi", "Maximilian Riesenhuber", "Tomaso Poggio" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Serre et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Serre et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Applications of SVD such as eigenface (Sirovich and Kirby, 1987, Turk and Pentland, 1991) and latent semantic analysis (Deerwester et al., 1990) have been illustrated to be very successful.",
      "startOffset" : 119,
      "endOffset" : 144
    }, {
      "referenceID" : 20,
      "context" : "The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009).",
      "startOffset" : 129,
      "endOffset" : 156
    }, {
      "referenceID" : 7,
      "context" : "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age− (1/ √ 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people’s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: “it would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.",
      "startOffset" : 4,
      "endOffset" : 288
    }, {
      "referenceID" : 7,
      "context" : "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age− (1/ √ 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people’s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: “it would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.” Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that Ã = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound.",
      "startOffset" : 4,
      "endOffset" : 1620
    }, {
      "referenceID" : 7,
      "context" : "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age− (1/ √ 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people’s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: “it would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.” Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that Ã = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows.",
      "startOffset" : 4,
      "endOffset" : 1704
    }, {
      "referenceID" : 7,
      "context" : "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age− (1/ √ 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people’s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: “it would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.” Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that Ã = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + ǫ) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel.",
      "startOffset" : 4,
      "endOffset" : 1930
    }, {
      "referenceID" : 7,
      "context" : "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age− (1/ √ 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people’s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: “it would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.” Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that Ã = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + ǫ) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel. Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be chosen. For example, for an m× n matrix A and a target rank k ≤ min{m,n}, the state-of-the-art CUR algorithm — the subspace sampling algorithm in Drineas et al. (2008) — requires exactly O(k4ǫ−6) rows or O(kǫ−4 log k) rows in expectation to achieve (1 + ǫ) relative-error ratio w.",
      "startOffset" : 4,
      "endOffset" : 2276
    }, {
      "referenceID" : 7,
      "context" : "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age− (1/ √ 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people’s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: “it would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.” Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that Ã = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + ǫ) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel. Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be chosen. For example, for an m× n matrix A and a target rank k ≤ min{m,n}, the state-of-the-art CUR algorithm — the subspace sampling algorithm in Drineas et al. (2008) — requires exactly O(k4ǫ−6) rows or O(kǫ−4 log k) rows in expectation to achieve (1 + ǫ) relative-error ratio w.h.p. Moreover, the computational cost of this algorithm is at least the cost of the truncated SVD of A, that is, O(min{mn2, nm2}).1 The algorithms are therefore impractical for large-scale matrices. In this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory and experiments. In particular, we show in Theorem 9 a novel randomized CUR algorithm with lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR algorithm in Drineas et al. (2008). The rest of this paper is organized as follows.",
      "startOffset" : 4,
      "endOffset" : 2908
    }, {
      "referenceID" : 7,
      "context" : "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age− (1/ √ 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people’s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: “it would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.” Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that Ã = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + ǫ) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel. Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be chosen. For example, for an m× n matrix A and a target rank k ≤ min{m,n}, the state-of-the-art CUR algorithm — the subspace sampling algorithm in Drineas et al. (2008) — requires exactly O(k4ǫ−6) rows or O(kǫ−4 log k) rows in expectation to achieve (1 + ǫ) relative-error ratio w.h.p. Moreover, the computational cost of this algorithm is at least the cost of the truncated SVD of A, that is, O(min{mn2, nm2}).1 The algorithms are therefore impractical for large-scale matrices. In this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory and experiments. In particular, we show in Theorem 9 a novel randomized CUR algorithm with lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR algorithm in Drineas et al. (2008). The rest of this paper is organized as follows. Section 2 lists some notations that will be used in this paper and Section 3 reviews two classes of CUR algorithms. Section 4 mainly introduces a column selection algorithm to which our work is closely related. Section 5 describes and analyzes our novel CUR algorithm. Section 6 empirically compares our proposed algorithm with the state-of-the-art algorithm. All proofs are deferred to Appendix B. 1. Although some partial SVD algorithms, such as Krylov subspace methods, require only O(mnk) time, they are all numerical unstable. See Halko et al. (2011) for more discussions.",
      "startOffset" : 4,
      "endOffset" : 3513
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, let A † = UA,ρΣ −1 A,ρV T A,ρ be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003).",
      "startOffset" : 77,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, let A † = UA,ρΣ −1 A,ρV T A,ρ be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003). Given matrices A ∈ Rm×n, X ∈ Rm×p, and Y ∈ Rq×n, XX†A = UXUXA ∈ Rm×n is the projection of A onto the column space of X, and AY†Y = AVYV T Y ∈ Rm×n is the projection of A onto the row space of Y. Finally, given an integer k ≤ p, we define the matrix ΠX,k(A) ∈ Rm×n as the best approximation to A within the column space of X that has rank at most k. We have ΠX,k(A) = XẐ where Ẑ = argminrank(Z)≤k ‖A−XZ‖F . We also have that ‖A−XXA‖F ≤ ‖A−ΠX,k(A)‖F . 3. Previous Work in CUR Matrix Decomposition This section discusses two recent developments of the CUR algorithms. Section 3.1 introduces an additive-error CUR algorithm in Drineas et al. (2006), and Section 3.",
      "startOffset" : 78,
      "endOffset" : 755
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, let A † = UA,ρΣ −1 A,ρV T A,ρ be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003). Given matrices A ∈ Rm×n, X ∈ Rm×p, and Y ∈ Rq×n, XX†A = UXUXA ∈ Rm×n is the projection of A onto the column space of X, and AY†Y = AVYV T Y ∈ Rm×n is the projection of A onto the row space of Y. Finally, given an integer k ≤ p, we define the matrix ΠX,k(A) ∈ Rm×n as the best approximation to A within the column space of X that has rank at most k. We have ΠX,k(A) = XẐ where Ẑ = argminrank(Z)≤k ‖A−XZ‖F . We also have that ‖A−XXA‖F ≤ ‖A−ΠX,k(A)‖F . 3. Previous Work in CUR Matrix Decomposition This section discusses two recent developments of the CUR algorithms. Section 3.1 introduces an additive-error CUR algorithm in Drineas et al. (2006), and Section 3.2 describes two relative-error CUR algorithms in Drineas et al. (2008). 3.",
      "startOffset" : 78,
      "endOffset" : 841
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, let A † = UA,ρΣ −1 A,ρV T A,ρ be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003). Given matrices A ∈ Rm×n, X ∈ Rm×p, and Y ∈ Rq×n, XX†A = UXUXA ∈ Rm×n is the projection of A onto the column space of X, and AY†Y = AVYV T Y ∈ Rm×n is the projection of A onto the row space of Y. Finally, given an integer k ≤ p, we define the matrix ΠX,k(A) ∈ Rm×n as the best approximation to A within the column space of X that has rank at most k. We have ΠX,k(A) = XẐ where Ẑ = argminrank(Z)≤k ‖A−XZ‖F . We also have that ‖A−XXA‖F ≤ ‖A−ΠX,k(A)‖F . 3. Previous Work in CUR Matrix Decomposition This section discusses two recent developments of the CUR algorithms. Section 3.1 introduces an additive-error CUR algorithm in Drineas et al. (2006), and Section 3.2 describes two relative-error CUR algorithms in Drineas et al. (2008). 3.1 The Linear-Time CUR Algorithm The linear-time CUR algorithm is proposed by Drineas et al. (2006). It is a highly efficient algorithm.",
      "startOffset" : 78,
      "endOffset" : 943
    }, {
      "referenceID" : 3,
      "context" : "2 The Subspace Sampling CUR Algorithm Drineas et al. (2008) proposed a two-stage randomized CUR algorithm which has a relativeerror bound w.",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "The near-optimal algorithm has three steps: the approximate SVD via random projection (Halko et al., 2011), the dual set sparsification algorithm (Boutsidis et al.",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 6,
      "context" : ", 2011a), and the adaptive sampling algorithm (Deshpande et al., 2006).",
      "startOffset" : 46,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "2 The Near-Optimal Column Selection Algorithm Recently, Boutsidis et al. (2011a) proposed a randomized algorithm which selects only c = 2kǫ−1(1 + o(1)) columns to achieve the expected relative-error ratio (1 + ǫ).",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "2 The Near-Optimal Column Selection Algorithm Recently, Boutsidis et al. (2011a) proposed a randomized algorithm which selects only c = 2kǫ−1(1 + o(1)) columns to achieve the expected relative-error ratio (1 + ǫ). Boutsidis et al. (2011a) also proved the lower bound of the column selection problem; that is, at least c = kǫ−1 columns are selected to achieve the (1 + ǫ) ratio.",
      "startOffset" : 56,
      "endOffset" : 239
    }, {
      "referenceID" : 2,
      "context" : "2 The Near-Optimal Column Selection Algorithm Recently, Boutsidis et al. (2011a) proposed a randomized algorithm which selects only c = 2kǫ−1(1 + o(1)) columns to achieve the expected relative-error ratio (1 + ǫ). Boutsidis et al. (2011a) also proved the lower bound of the column selection problem; that is, at least c = kǫ−1 columns are selected to achieve the (1 + ǫ) ratio. Thus this algorithm is near optimal. Though an optimal algorithm recently proposed by Guruswami and Sinop (2012) achieves the the lower bound, the optimal algorithm is quite inefficient compared with the near-optimal algorithm.",
      "startOffset" : 56,
      "endOffset" : 491
    }, {
      "referenceID" : 15,
      "context" : "(2011a) employed an approximation SVD algorithm (Halko et al., 2011) to speedup computation.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "Since SVD is time consuming, Boutsidis et al. (2011a) employed an approximation SVD algorithm (Halko et al.",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "The second step of the near-optimal column selection algorithm is the dual set sparsification proposed by Boutsidis et al. (2011a). When ones take A and the top k (approximate) right singular vectors of A as inputs, the dual set sparsification algorithm can deterministically selects c1 columns of A to construct C1.",
      "startOffset" : 106,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "After sampling c1 columns of A, the near-optimal column selection algorithm uses the adaptive sampling of Deshpande et al. (2006) to select c2 columns of A to further reduce the construction error.",
      "startOffset" : 106,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "After sampling c1 columns of A, the near-optimal column selection algorithm uses the adaptive sampling of Deshpande et al. (2006) to select c2 columns of A to further reduce the construction error. We present Theorem 2.1 in Deshpande et al. (2006) in the following lemma.",
      "startOffset" : 106,
      "endOffset" : 248
    }, {
      "referenceID" : 4,
      "context" : "1 of Deshpande et al. (2006). The algorithm is based on the following idea: after selecting a proportion of columns from A to form C1 by an arbitrary algorithm, the algorithms randomly samples additional c2 columns according to the residual A − C1C†1A.",
      "startOffset" : 5,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "Boutsidis et al. (2011a) used the adaptive sampling algorithm to decrease the residual of the dual set sparsification algorithm and obtained an (1 + ǫ) relative-error ratio.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "Boutsidis et al. (2011a) used the adaptive sampling algorithm to decrease the residual of the dual set sparsification algorithm and obtained an (1 + ǫ) relative-error ratio. Here we prove a new bound for the same adaptive sampling algorithm. Interestingly, this new bound is a generalization of the original one in Theorem 2.1 of Deshpande et al. (2006). In other words, Theorem 2.",
      "startOffset" : 0,
      "endOffset" : 354
    }, {
      "referenceID" : 2,
      "context" : "Boutsidis et al. (2011a) used the adaptive sampling algorithm to decrease the residual of the dual set sparsification algorithm and obtained an (1 + ǫ) relative-error ratio. Here we prove a new bound for the same adaptive sampling algorithm. Interestingly, this new bound is a generalization of the original one in Theorem 2.1 of Deshpande et al. (2006). In other words, Theorem 2.1 of Deshpande et al. (2006) is a direct corollary of our following theorem when C = Ak.",
      "startOffset" : 0,
      "endOffset" : 410
    }, {
      "referenceID" : 2,
      "context" : ", Theorem 5 of Boutsidis et al. (2011a), is a special case of Theorem 8 when C = Ak.",
      "startOffset" : 15,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "The Redrock and Edinburgh (Agarwala, 2007) are two large size natural images.",
      "startOffset" : 26,
      "endOffset" : 42
    }, {
      "referenceID" : 21,
      "context" : "We use the HMAX features (Serre et al., 2007) and the SIFT features (Lowe, 1999) of the first 50000 images; the features provided by Wang et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 18,
      "context" : ", 2007) and the SIFT features (Lowe, 1999) of the first 50000 images; the features provided by Wang et al.",
      "startOffset" : 30,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "The Redrock and Edinburgh (Agarwala, 2007) are two large size natural images. Arcene and Dexter are both from the UCI datasets (Frank and Asuncion, 2010). Arcene is a biology dataset with 900 instances and 10000 attributes. Dexter is a bag of words dataset with a 20000vocabulary and 2600 documents. PicasaWeb image dataset (Wang et al., 2012) contains 6.8 million PicasaWeb images. We use the HMAX features (Serre et al., 2007) and the SIFT features (Lowe, 1999) of the first 50000 images; the features provided by Wang et al. (2012) are all of 3000 dimensions.",
      "startOffset" : 27,
      "endOffset" : 535
    }, {
      "referenceID" : 7,
      "context" : "According to the analysis in Drineas et al. (2008) and this paper, k, c, and r should be integers much less than m and n.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "First, what is the lower bound for the CUR problem? Second, is there any algorithm achieving such a lower bound? Boutsidis et al. (2011b) proved a lower bound for the column selection problem: ‖A−CC†A‖2F ‖A−Ak‖ 2 F ≥ 1+ kc .",
      "startOffset" : 113,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "The dual set sparsification algorithms are deterministic algorithms established in Boutsidis et al. (2011a). The fast CUR algorithm calls the dual set spectral-Frobenius sparsification algorithm (Lemma 13 in Boutsidis et al.",
      "startOffset" : 83,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "Here we would like to mention the implementation of Algorithm 2, which is not described by Boutsidis et al. (2011a) in details.",
      "startOffset" : 91,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "In order to stick to the column space convention of Boutsidis et al. (2011a), we prove Theorem 11 instead of Theorem 7.",
      "startOffset" : 52,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "2 The Proof of Theorem 8 Boutsidis et al. (2011a) proposed a randomized algorithm which achieves the expected relative-error bound in Lemma 13.",
      "startOffset" : 25,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "Lemma 13 (Boutsidis et al. (2011a), Theorem 4) Given a matrix A ∈ Rm×n of rank ρ, a target rank 2 ≤ k < ρ, and 0 < ǫ0 < 1, there is a randomized algorithm to select c1 > k columns of A and form a matrix C1 ∈ Rm×c1 such that E‖A−C1C†1A‖F ≤ (1 + ǫ0) (",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 15,
      "context" : "Proof This randomized algorithm has three steps: approximate SVD via randomized projection (Halko et al., 2011), deterministic column selection via dual set sparsification algorithm (Boutsidis et al.",
      "startOffset" : 91,
      "endOffset" : 111
    } ],
    "year" : 2012,
    "abstractText" : "The CUR matrix decomposition is an important extension of Nyström approximation to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR algorithm with an expected relative-error bound. The proposed algorithm has the advantages over the existing relative-error CUR algorithm that it possesses tighter theoretical bound and lower time complexity, and that it can avoid maintaining the whole data matrix in main memory. Finally, experiments on several real-world datasets demonstrate significant improvement over the existing relative-error algorithms.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}