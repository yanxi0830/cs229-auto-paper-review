{
  "name" : "1506.02582.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Convergence of Emphatic Temporal-Difference Learning∗",
    "authors" : [ "Huizhen Yu" ],
    "emails" : [ "JANEY.HZYU@GMAIL.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Markov decision processes; approximate policy evaluation; reinforcement learning; temporal difference methods; importance sampling; stochastic approximation; convergence\n∗ This research was supported by a grant from Alberta Innovates – Technology Futures. A shorter version of this paper is to appear at the 28th Annual Conference on Learning Theory (COLT), Paris, France, 2015.\nar X\niv :1\n50 6.\n02 58\n2v 1\n[ cs\n.L G\n] 8\nJ un\n2 01\n5"
    }, {
      "heading" : "Contents",
      "text" : ""
    }, {
      "heading" : "1 Introduction 3",
      "text" : ""
    }, {
      "heading" : "2 Emphatic TD Algorithms: ETD(λ) and ELSTD(λ) 4",
      "text" : "2.1 A Policy Evaluation Problem in Off-Policy Learning . . . . . . . . . . . . . . . . 4 2.2 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3 Convergence Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"
    }, {
      "heading" : "3 Properties of Trace Iterates and Convergence Analysis of ELSTD(λ) 8",
      "text" : "3.1 Properties of Trace Iterates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.2 Main Results on L1 and Almost Sure Convergence . . . . . . . . . . . . . . . . . 10"
    }, {
      "heading" : "4 Convergence Analysis of ETD(λ) 11",
      "text" : "4.1 Convergence of Constrained ETD(λ) . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.2 Convergence of ETD(λ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nReferences 14\nAppendices 17"
    }, {
      "heading" : "A Proof Details for Section 3 17",
      "text" : "A.1 Some Basic Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.2 Properties of the Trace Iterates {(et, Ft)} . . . . . . . . . . . . . . . . . . . . . . 19 A.3 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 A.4 Handling Noisy Rewards: Proof of Prop. 3.1 . . . . . . . . . . . . . . . . . . . . . 25 A.5 Proof of Theorem 2.1 on the Convergence of ELSTD(λ) . . . . . . . . . . . . . . 28 A.6 Related Result: Alternative Proof of Existence of an Invariant Probability Measure 32"
    }, {
      "heading" : "B Proofs for Section 4 35",
      "text" : ""
    }, {
      "heading" : "C Negative Definiteness of the Matrix C 38",
      "text" : "C.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 C.2 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39"
    }, {
      "heading" : "1. Introduction",
      "text" : "We consider discounted finite-spaces Markov decision processes (MDPs) and the problem of learning an approximate value function for a given policy from off-policy data, that is, from data due to a different policy. The first policy is called the target policy and the second is called the behavior policy. For example, one may want to learn value functions for many target policies in parallel from one (exploratory) behavior; this requires off-policy learning.\nWe focus on temporal-difference (TD) methods with linear function approximation (Sutton, 1988). Such methods are typically convergent when the target and behavior policies are the same (the on-policy case), but not in the off-policy case (Tsitsiklis and Van Roy, 1997). This difficulty is intrinsic to sampling states according to an arbitrary policy.1 Gradient-based or least squares-based approaches have been used to avoid this difficulty.2\nRecently, Sutton, Mahmood, and White (2015) proposed a new approach to address this issue more directly. They introduced an emphatic TD(λ) algorithm, or ETD(λ) as we call it here. The approach is related to the early work on episodic off-policy TD(λ) (Precup et al., 2001), and is based on the idea of re-weighting the states when forming the eligibility traces in TD(λ), so that the weights reflect the occupation frequencies of the target policy rather than the behavior policy. The result of this weighting scheme is that the “mean updates” associated with ETD(λ) now involve a negative definite matrix, similar to the convergent on-policy TD algorithms. This is a salient feature of the emphatic TD method.\nThe purpose of this paper is to investigate the convergence properties of ETD(λ) and its leastsquares version, ELSTD(λ). Under general conditions, we show that (see Theorems 2.1, 2.2):\n(i) for stepsizes decreasing as t−c, c ∈ (1/2, 1], the matrix and vector iterates generated by ELSTD(λ) converge in L1 to the desired limits, which define a projected Bellman equation; (ii) for stepsizes decreasing as t−1, both algorithms generate approximate value functions that converge almost surely to the desired solution of an associated projected Bellman equation.\nThese results show that the new emphatic TD algorithms are sound for off-policy learning. Regarding proof techniques, we note that although the “mean updates” of ETD(λ) involve a negative definite matrix, it is still difficult to directly apply results from stochastic approximation theory to establish rigorously the association between the “mean updates” and the ETD(λ) iterates, thereby obtaining the desired convergence. The stability criterion of (Borkar and Meyn, 2000) (see also (Borkar, 2008, Chap. 3)) and the “natural averaging” argument in (Borkar, 2008, Chap. 6) seem suitable, but they require a certain tightness condition that is hard to verify in the general off-policy learning setting where the variances of the trace iterates can grow to infinity with time.3 The analysis of (Tsitsiklis and Van Roy, 1997) has a strong condition (Condition (6), p. 683, in particular), which is difficult to satisfy unless the trace iterates are uniformly bounded. But in general, this would impose a strong restriction on the behavior policy (cf. Yu, 2012, Prop. 3.1, Footnote 3, and the discussion in p. 3320-3322).\nFor regular off-policy LSTD(λ) and TD(λ) (Bertsekas and Yu, 2009), it has been shown by Yu (2012) that the associated joint process of states and trace iterates exhibit useful properties, by which convergence results for LSTD(λ) can be derived. Subsequently, the results can be used to furnish\n1. See the papers (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2015) and the books (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) for related examples and discussion. 2. See e.g., (Maei, 2011; Bertsekas and Yu, 2009; Geist and Scherrer, 2014; Dann et al., 2014). 3. Related examples can be found in (Glynn and Iglehart, 1989; Randhawa and Juneja, 2004; Sutton et al., 2015).\nthe conditions of a convergence theorem from stochastic approximation theory (Kushner and Yin, 2003) and yield convergence results for TD(λ). In this paper we will take the proof approach used in (Yu, 2012). We note, however, that most of the intermediate results needed in our case require different and more involved proofs, due to the complexity of the emphatic TD method. Furthermore, we will give a new argument to prove the almost sure convergence of ETD(λ), which applies also to the regular off-policy TD(λ) of (Bertsekas and Yu, 2009) for λ near 1. This improves a result of (Yu, 2012), which only dealt with a constrained version of TD(λ) that restricts the iterates to lie in a bounded set.\nThis paper is organized as follows. In Section 2 we formulate the approximate policy evaluation problem, and we describe the ETD(λ) and ELSTD(λ) algorithms, and the approximate Bellman equations they aim to solve. We also state our main convergence results in this section. In Section 3 we prove our convergence theorem for ELSTD(λ), and prepare results needed for analyzing ETD(λ) with a “mean ODE”4 method. In Section 4 we prove our convergence theorem for ETD(λ). We collect long proofs, technical lemmas and other related results in Appendices A-C."
    }, {
      "heading" : "2. Emphatic TD Algorithms: ETD(λ) and ELSTD(λ)",
      "text" : ""
    }, {
      "heading" : "2.1. A Policy Evaluation Problem in Off-Policy Learning",
      "text" : "Let S = {1, . . . , N} be the state space, and letA be a finite set of actions. We assume, without loss of generality, that for every state, all actions are feasible. If we take action a ∈ A at state s ∈ S, the system moves from state s to state s′ with probability p(s′ | s, a), and we receive a random reward with mean r(s, a, s′) and bounded variance, according to a probability distribution q(· | s, a, s′).\nWe are interested in evaluating the performance of a given stationary policy5 π, the target policy, without knowledge of the MDP model. The evaluation is to be done by using just observations of state transitions and rewards, while following a stationary policy πo 6= π, the behavior policy.\nStarting from time t = 0, applying π would generate a sequence of rewards R0, R1, . . .. The performance of π will be measured in terms of the expected total rewards attained under π up to a random termination time τ ≥ 1 that depends on the states in a Markovian way. In particular, if at time t ≥ 1, the state is s and termination has not occurred yet, then the probability of τ = t (terminating at time t) is 1− γ(s), for a given parameter γ(s) ∈ [0, 1].\nLet Pπ denote the transition matrix of the Markov chain on S induced by π. Let Γ denote the N × N diagonal matrix with diagonal entries γ(s), s ∈ S. Denote by π(a | s) and πo(a | s) the probability of taking action a at state s under the policy π and πo, respectively.\nAssumption 2.1 (Conditions on the target and behavior policies) (i) The target policy π is such that (I − PπΓ)−1 exists (equivalently, termination occurs with\nprobability 1 under π, for any initial state). (ii) The behavior policy πo induces an irreducible Markov chain on S , and moreover, for all\n(s, a) ∈ S ×A, πo(a | s) > 0 if π(a | s) > 0.\nUnder Assumption 2.1(i), we define the value function of the target policy π by vπ : S → R, vπ(s) = Eπ [∑τ−1 t=0 Rt ∣∣∣S0 = s], where Eπ denotes expectation with respect to the probability 4. ODE stands for ordinary differential equation. 5. A stationary policy is a decision rule that specifies the probability of taking action a at state s for every s ∈ S.\ndistribution of the process of states, actions and rewards, (St, At, Rt), t ≥ 0, induced by the policy π. Let rπ be the expected one-stage reward function under π; i.e., rπ(s) = Eπ [ R0 | S0 = s ] for s ∈ S. Then the desired function vπ can be seen to satisfy uniquely the Bellman equation6\nvπ = rπ + PπΓ vπ, i.e., vπ = (I − PπΓ)−1rπ."
    }, {
      "heading" : "2.2. Algorithms",
      "text" : "We consider computing vπ with the ETD(λ) algorithm (Sutton et al., 2015) and its least-squares version, ELSTD(λ), using linear function approximation, while following the behavior policy πo. Let E ⊂ RN be the approximation subspace of dimension n, and let Φ be an N × n matrix whose columns form a basis of E. The approximation problem is to find a parameter vector θ ∈ Rn such that v = Φθ ∈ E approximates vπ well.\nWe express v = Φθ as v(s) = φ(s)>θ, s ∈ S, where the superscript > stands for transpose, and φ(s) ∈ Rn is the transposed s-th row of Φ and represents the “features” of state s. Like standard TD(λ), if a transition (s, s′) occurs with reward r′, ETD(λ) and ELSTD(λ) use the “temporal difference” term, r′ + γ(s′)φ(s′)>θ − φ(s)>θ, to adjust the parameter θ for the approximate value function. Also like standard TD(λ), these algorithms aim to solve a projected (single-step or multistep) Bellman equation; but we shall defer the discussion of this until after describing the ETD(λ) algorithm.\nWe focus on a general form of the ETD(λ) algorithm, which uses state-dependent λ values specified by a function λ : S → [0, 1]. Inputs to the algorithm are the states, actions and rewards, {(St, At, Rt), t ≥ 0}, generated under the behavior policy πo, where Rt is the random reward received upon the transition from state St to St+1 with action At. The algorithm can access the following functions, in addition to the features φ(s):\n(i) γ : S → [0, 1], which specifies the termination probabilities (or equivalently, the statedependent discount factors) that define vπ, as described earlier; (ii) λ : S → [0, 1], which determines the single or multi-step Bellman equation for the algorithm [cf. the subsequent Eqs. (2.5)-(2.6)]; (iii) ρ : S × A → R+ given by ρ(s, a) = π(a | s)/πo(a | s) (with 0/0 = 0), which gives the likelihood ratios for action probabilities that can be used to compensate for sampling states and actions according to the behavior policy πo instead of the target policy π;\n(iv) i : S → R+, which gives the algorithm additional flexibility to weigh states according to the degree of “interest” indicated by i(s).\nThe ETD(λ) algorithm does the following. For each t ≥ 0, let αt ∈ (0, 1] be a stepsize parameter, and to simplify notation, let\nρt = ρ(St, At), γt = γ(St), λt = λ(St).\nETD(λ) calculates recursively θt ∈ Rn, t ≥ 0, according to\nθt+1 = θt + αt et · ρt ( Rt + γt+1φ(St+1) >θt − φ(St)>θt ) , (2.1)\n6. One can verify this Bellman equation directly. It also follows from the standard MDP theory (see e.g., Puterman, 1994), as by definition vπ here can be related to a value function in a discounted MDP where the discount factors depend on state transitions, similar to discounted semi-Markov decision processes.\nwhere et ∈ Rn (called the “eligibility trace”) is calculated together with two nonnegative scalar iterates (Ft,Mt) according to:7\nFt = γt ρt−1 Ft−1 + i(St), (2.2) Mt = λt i(St) + (1− λt)Ft, (2.3) et = λt γt ρt−1 et−1 +Mt φ(St). (2.4)\nFor t = 0, (e0, F0, θ0) are given as an initial condition of the algorithm. We recognize that the iteration (2.1) has the same form as standard TD, but the trace et is calculated differently, involving an “emphasis” weight Mt on the state St, which itself evolves along with the iterate Ft, called the “follow-on” trace. If Mt is always set to 1 regardless of Ft and i(·), then the iteration (2.1) reduces to the standard TD(λ) in the case where γ and λ are constants.\nTo explain at a high level what ETD(λ) aims to achieve with the weighting scheme (2.2)-(2.4), let us discuss the approximate Bellman equation it aims to solve. Associated with ETD(λ) is a generalized Bellman equation of which vπ is the unique solution (Sutton, 1995):8\nv = rλπ,γ + P λ π,γ v. (2.5)\nHere P λπ,γ is an N × N substochastic matrix, and rλπ,γ ∈ RN is a vector of expected total rewards attained by π up to some random time depending on the functions γ and λ, given by\nP λπ,γ = I − (I − PπΓΛ)−1 (I − PπΓ), rλπ,γ = (I − PπΓΛ)−1 rπ, (2.6)\nwhere Λ is a diagonal matrix with diagonal entries λ(s), s ∈ S. ETD(λ) aims to solve a projected version of the Bellman equation (2.5) (see Sutton et al., 2015):\nv = Π ( rλπ,γ + P λ π,γ v ) , v ∈ E, ⇐⇒ Cθ + b = 0, θ ∈ Rn. (2.7)\nIn the above, Π is the projection onto E with respect to a weighted Euclidean norm or seminorm. The weights that define this norm also define the diagonal entries of a diagonal matrix M̄ , and are given by\ndiag(M̄) = d>πo,i(I − P λπ,γ)−1, with dπo,i ∈ RN , dπo,i(s) = dπo(s) · i(s), s ∈ S, (2.8)\nwhere dπo(s) > 0 denotes the steady state probability of state s for the behavior policy πo, under Assumption 2.1(ii). For the corresponding linear equation in the θ-space in Eq. (2.7),\nC = −Φ>M̄ (I − P λπ,γ) Φ, b = Φ>M̄ rλπ,γ . (2.9)\nImportant for the convergence of ETD(λ) is the negative definiteness of C. It can be shown that under Assumption 2.1, C is negative definite whenever C is nonsingular.9 By comparison, if we set Mt = 1 regardless of Ft and i(·), the weights that define the projection norm and diag(M̄)\n7. For insights about ETD(λ), see (Sutton et al., 2015; Mahmood et al., 2015). Our definition (2.4) of {et} differs slightly from its original definition, but the two are equivalent; ours appears to be more convenient for our analysis. 8. For the details of this Bellman equation, we refer the readers to the early work (Sutton, 1995; Sutton and Barto, 1998) and the recent work (Sutton et al., 2015). 9. The negative definiteness of C is proved for positive i(·) under Assumption 2.1 by Sutton et al. (2015), and their result extends to nonnegative i(·), as long as C is nonsingular (see our Prop. C.1 in Appendix C).\nwould simply become dπo , the same as in the regular off-policy TD(λ). If we set Mt = i(s), then the weights are given by dπo,i. Neither of these cases guarantees C to be negative definite, unless λ is sufficiently close to 1. Having the desirable negative definiteness property of C is one of the motivations for introducing the weighting scheme (2.2)-(2.4) in ETD(λ) (Sutton et al., 2015).\nFor the convergence analysis in this paper, we shall assume:\nAssumption 2.2 (Nonsingularity condition) The matrix C given in Eq. (2.9) is nonsingular.\nWe remark that for ETD(λ) under Assumption 2.1, C is always negative semidefinite (Sutton et al., 2015) (cf. our Prop. C.1, Appendix C), so the nonsingularity condition above is equivalent to C being negative definite, as noted earlier. This condition is fairly mild and allows i(s) = 0 for some states s. Specifically, as we prove in Appendix C, Assumption 2.2 is equivalent to a condition on the approximation subspace (Prop. C.2), which requires merely that the set of feature vectors of those states with positive emphasis weights contains n linearly independent vectors (cf. Remark C.2). Moreover, this requirement can be fulfilled easily without knowledge of the model (see Cor. C.1, Remark C.2). We also note that when C is negative definite, the projection Π in Eq. (2.7) is welldefined (with respect to a seminorm if in Eq. (2.8) some diagonal entries of M̄ equal zero), the projected Bellman equation (2.7) has a unique solution, and bounds on the approximation error of ETD(λ) can be derived using the approach of Scherrer (2010). (For details of this discussion, see Remark C.1 in Appendix C.)\nThe ELSTD(λ) algorithm aims to solve the same projected Bellman equation (2.7) as ETD(λ). ELSTD(λ) calculates iteratively an n× n matrix Ct and a vector bt ∈ Rn according to\nCt+1 = (1− αt)Ct + αt et · ρt ( γt+1φ(St+1) > − φ(St)> ) , (2.10)\nbt+1 = (1− αt) bt + αt et · ρtRt, (2.11)\nwhere the trace et is calculated according to Eqs. (2.2)-(2.4) as in ETD(λ). ELSTD(λ) sets θt = −C−1t bt, the solution to Ctθ + bt = 0, when Ct is invertible.\nLike ETD(λ), without the weighting scheme (2.2)-(2.4), ELSTD(λ) would reduce essentially to the regular LSTD(λ) (see e.g., (Boyan, 1999; Yu, 2012) for on-policy and off-policy LSTD(λ))."
    }, {
      "heading" : "2.3. Convergence Results",
      "text" : "We analyze ETD(λ) and ELSTD(λ) with diminishing stepsizes. Summarized below are their convergence properties, which we will establish in the rest of this paper. In what follows, we denote by ‖ · ‖ the infinity norm for both vectors and matrices (viewed as vectors). For different stepsize conditions, our results will involve different convergence modes: convergence in L1,10 in probability, or almost sure (a.s.) convergence (we write a.s.→ for “converges almost surely”). First, we state a general stepsize condition that we will use.\nAssumption 2.3 (Stepsize condition) The stepsize sequence {αt} is deterministic and eventually nonincreasing, and satisfies αt ∈ (0, 1], ∑ t αt =∞, ∑ t α 2 t <∞.\nUnder the above condition we may take αt = t−c, c ∈ (1/2, 1]. However, stepsizes decreasing as t−1 will be required in our almost sure convergence results; some cases will require αt = O(1/t) with αt−αt+1αt = O(1/t). 11 (For instance, αt = c1/(c2 + t) for some constants c1, c2 > 0.)\n10. For vector-valued random variables X , Xt, t ≥ 0, by “{Xt} converges to X in L1” we mean E[‖Xt −X‖] t→∞→ 0. 11. We write δt = O(1/t) for a scalar sequence {δt}, if for some c > 0, 0 ≤ δt ≤ c/t for all t.\nOur results are as follows. Let θ∗ denote the desired limit for ETD(λ):\nθ∗ = −C−1b, for C, b defined by Eq. (2.9) under Assumptions 2.1, 2.2.\nTheorem 2.1 (L1 and almost sure convergence of ELSTD(λ) Iterates) Under Assumptions 2.1, 2.3, for any given initial (e0, F0, C0, b0), the sequence {(Ct, bt)} generated by the ELSTD(λ) algorithm (2.10)-(2.11) converges in L1:\nlim t→∞\nE [∥∥Ct − C∥∥] = 0, lim t→∞ E [∥∥bt − b∥∥] = 0.\nIf in addition the stepsize is given by αt = 1/(t+ 1), then Ct a.s.→ C, bt a.s.→ b.\nThe preceding theorem yields immediately the convergence of the parameter sequence {θt} generated by ELSTD(λ):\nCorollary 2.1 (Convergence of ELSTD(λ)) Let Assumptions 2.1-2.3 hold. Let {θt} be generated by the ELSTD(λ) algorithm (2.10)-(2.11) as θt = −C−1t bt. Then for any given initial (e0, F0, C0, b0), {θt} converges to θ∗ in probability; if in addition αt = 1/(t+ 1), then θt a.s.→ θ∗.\nTheorem 2.2 (Almost sure convergence of ETD(λ)) Let Assumptions 2.1-2.3 hold. Let {θt} be generated by the ETD(λ) algorithm (2.1) with stepsizes satisfying αt = O(1/t) and\nαt−αt+1 αt =\nO(1/t). Then for any given initial (e0, F0, θ0), θt a.s.→ θ∗.\nRemark 2.1 (On stepsizes) We believe that the range of stepsizes for the a.s. convergence of ELSTD(λ) can be enlarged. If additional conditions on the behavior policy are imposed to restrict the variances of the trace iterates, it should also be possible to enlarge the range of stepsizes for ETD(λ). These topics, as well as the use of random stepsizes, are under active investigation.\nRemark 2.2 (On variances) The preceding convergence results hold under almost minimal conditions on the behavior policy (Assumption 2.1(ii)). However, unless we restrict sufficiently the behavior policy (which is difficult to do without knowledge of the model, when γ 6< 1), the variances of the trace iterates can grow unboundedly (cf. Remark A.1), significantly affecting the speed of convergence. This is a main difficulty in off-policy methods in general. Further research is required to overcome it. For a recent work in this direction, see (Mahmood et al., 2014)."
    }, {
      "heading" : "3. Properties of Trace Iterates and Convergence Analysis of ELSTD(λ)",
      "text" : "In this section we analyze the trace iterates and convergence properties of ELSTD(λ) iterates. The analysis not only leads to Theorem 2.1 on the convergence of ELSTD(λ), but also prepares the stage for the subsequent ODE-based convergence proof for ETD(λ), by ensuring that “local averaging” gives the desired “mean dynamics,” as will be seen in Section 4.\nThe structure of our analysis will be similar to that of (Yu, 2012) for regular off-policy LSTD(λ), but the proofs at intermediate steps are new and more involved. We will explain the key proof arguments in this section, and give the proof details and related results in Appendix A."
    }, {
      "heading" : "3.1. Properties of Trace Iterates",
      "text" : "Let Zt = (St, At, et, Ft) for t ≥ 0; they form a Markov chain on S × A × Rn+1. First, we observe several important properties of the trace iterates {(et, Ft)} and the Markov chain {Zt}, under Assumption 2.1:\n(i) For any given initial (e0, F0), supt≥0 E [∥∥(et, Ft)∥∥] <∞. (See Prop. A.1.)\n(ii) Let {(et, Ft)} and {(êt, F̂t)} be defined by the same recursion (2.2)-(2.4), using the same state and action random variables, but with different initial conditions (e0, F0) 6= (ê0, F̂0). Then Ft − F̂t a.s.→ 0 and et − êt a.s.→ 0 (the zero vector in Rn). (See Prop. A.2.) (iii) We can approximate the traces (et, Ft), which depend on the entire history of past states and actions, by similarly defined “truncated traces” (ẽt,K , F̃t,K) which depend on the most recent 2K states and actions only [cf. Eqs. (A.13)-(A.15)]. The expected approximation “error” can be bounded uniformly in t, by a constant LK which decreases to 0 as K → ∞. (See Prop. A.3.) (iv) {Zt} is a weak Feller Markov chain12 and bounded in probability,13 and hence it has at least one invariant probability measure.14 Furthermore, as we will show in Theorem 3.2 below, {Zt} has a unique invariant probability measure and is ergodic.\nThese properties suggest that despite the growing variances, the trace iterates are well-behaved. Figure 1 shows how the convergence results of this section, to be introduced next, will depend on these properties.\n12. A Markov chain {Xt} on a metric space is weak Feller if E[f(X1) | X0 = x] is continuous in x for every bounded continuous function f on the state space (Meyn and Tweedie, 2009, Prop. 6.1.1(i)). Using this and the fact that (e1, F1) depends continuously on (e0, F0) [cf. Eqs. (2.2)-(2.4)], the weak Feller property of {Zt} can be seen. 13. A Markov chain {Xt} on a topological space is bounded in probability if, for each initial state x and each > 0, there exists a compact subset D of the state space such that lim inft→∞Px(Xt ∈ D) ≥ 1 − , where Px denotes the probability of events conditional on X0 = x (Meyn and Tweedie, 2009, p. 142). In our case, since S and A are finite, the property (i) above together with the Markov inequality implies that {Zt} is bounded in probability (cf. Yu, 2012, Lemma 3.4). 14. By (Meyn and Tweedie, 2009, Theorem 12.1.2(ii)), a weak Feller Markov chain bounded in probability has at least one invariant probability measure. We mention that there is also an alternative, direct proof of the existence of an invariant probability measure for {Zt}, which does not rely on the weak Feller property (see Appendix A.6)."
    }, {
      "heading" : "3.2. Main Results on L1 and Almost Sure Convergence",
      "text" : "We formulate our convergence results in terms of a general recursion that can be specialized to the ELSTD(λ) iteration. This generality is needed in order to make the results useful for other proofs, specifically, for proving the uniqueness of the invariant probability measure of {Zt}, and for establishing convergence conditions required by an ODE-based analysis for ETD(λ), as those proofs will rely on the convergence properties of certain iterates that are different from ELSTD(λ).\nWe define the general recursion just mentioned as follows. Denote y = (e, F ); thus y ∈ Rn+1. Consider a vector-valued function h : Rn+1×S ×A×S → Rm such that h(y, s, a, s′) is Lipschitz continuous in y for each (s, a, s′); i.e., there exists some constant Lh such that for any y, ŷ ∈ Rn+1,∥∥h(y, s, a, s′)− h(ŷ, s, a, s′)∥∥ ≤ Lh‖y − ŷ‖, ∀ (s, a, s′) ∈ S ×A× S. (3.1) Given h, {Zt} and the stepsizes {αt}, we define a recursion as follows:\nGt+1 = (1− αt)Gt + αt h(Yt, St, At, St+1). (3.2)\nThe ELSTD(λ) iterates Ct and bt correspond to the following choices of h, respectively:\nh1(y, s, a, s ′) = e · ρ(s, a) ( γ(s′)φ(s′)> − φ(s)> ) , h2(y, s, a, s ′) = e · ρ(s, a) r(s, a, s′). (3.3)\nHere h1 is matrix-valued (we view it as an Rm-valued function with m = n × n), and h2 is Rnvalued. As just mentioned, we will also need to consider other choices of h in our proofs later.\nWe first show that {Gt} converges in L1 to some constant vector. The proof (given in Appendix A.3) exploits the property (iii) of truncated traces mentioned earlier: this property allows us to obtain the desired result by working with simple finite-state Markov chains.\nTheorem 3.1 (L1-convergence of {Gt}) Let h be a vector-valued function satisfying the Lipschitz condition (3.1), and let {Gt} be defined by the recursion (3.2), using the process {Zt}. Then under Assumptions 2.1, 2.3, there exists a constant vector G∗ (independent of the stepsizes) such that for any given initial Y0 = (e0, F0) and G0, limt→∞ E\n[∥∥Gt −G∗∥∥] = 0. Next we analyze the a.s. convergence of {Gt}, by using ergodicity properties of the infinitespace Markov chain {Zt} that we establish first. For each initial condition Z0 = z, define the occupation probability measures µz,t for t ≥ 1, by µz,t(B) = 1t ∑t k=1 1B(Zk) for any Borel subset B of S×A×Rn+1, where 1B denotes the indicator function for the setB (i.e., 1B(x) = 1 if x ∈ B, and 1B(x) = 0 otherwise). Let Eµ denote expectation with respect to the probability distribution of the process {Zt} with µ as the initial distribution of Z0.\nTheorem 3.2 (Ergodicity of {Zt}) Under Assumption 2.1, the Markov chain {Zt} has a unique invariant probability measure ζ, and moreover, the following hold: (i) For each initial condition Z0 = z, the sequence {µz,t} of occupation measures converges weakly15 to ζ, almost surely. (ii) Eζ\n[∥∥h(Z0, S1)∥∥] <∞ for any function h satisfying the Lipschitz condition (3.1). 15. For probability measures µ, µt, t ≥ 0, on a metric space X , {µt} is said to converge weakly to µ if for all bounded\ncontinuous functions f on X , ∫ fdµt → ∫ fdµ as t→∞ (Dudley, 2002, Chap. 9.3).\nThe preceding theorem follows from the properties of trace iterates given earlier and Theorem 3.1 (cf. Figure 1). The proof is the same as the corresponding proofs of (Yu, 2012, Theorem 3.2 and Prop. 3.2) for the case of off-policy LSTD. In particular, to prove the uniqueness of the invariant probability measure (which is not as easy to prove as the existence given in the property (iv) earlier), we use the property (ii) and the convergence in L1 result given in Theorem 3.1.16\nWe can now show that {Gt} converges a.s. for stepsize αt = 1/(t+ 1), by using the preceding results (cf. Figure 1), together with a strong law of large numbers for stationary processes (Doob, 1953, Chap. X, Theorem 2.1) (see also Meyn and Tweedie, 2009, Theorem 17.1.2). The proof is a verbatim repetition of the proof of (Yu, 2012, Theorem 3.3) and is therefore omitted.\nTheorem 3.3 (Almost sure convergence of {Gt}) Let h and {Gt} be as in Theorem 3.1, and let the stepsize be αt = 1/(t+ 1). Then, under Assumption 2.1, for any given initial Y0 = (e0, F0) and G0, Gt a.s.→ G∗, where G∗ = Eζ [ h(Y0, S0, A0, S1) ] is the constant vector in Theorem 3.1.\nFinally, we also need to analyze the cumulative effects of noise in the observed rewards Rt and show that they diminish asymptotically. To this end, consider the following recursion: W0 = 0 and\nWt+1 = (1− αt)Wt + αt et ρt · ωt+1, t ≥ 0, (3.4)\nwhere ωt+1 = Rt − r(St, At, St+1) are noise variables.\nProposition 3.1 (Effects of noise in random rewards) Under Assumptions 2.1, 2.3, for any given initial (e0, F0), we have (i) E [ ‖Wt‖ ] → 0; and (ii) if, in addition, the stepsize is αt = 1/(t + 1), then Wt a.s.→ 0.\nThe proof of the preceding proposition is given in Appendix A.4. The proof of part (i) uses the property (iii) of truncated traces, similarly to the proof of Theorem 3.1, and the proof of part (ii) is similar to that of Theorem 3.3 (cf. Figure 1).\nThe convergence of ELSTD(λ) stated in Theorem 2.1 now follows from the preceding results (cf. Figure 1). Specifically, we calculate the limit G∗ in Theorem 3.1 for the two functions h1, h2 in Eq. (3.3), which are associated with the ELSTD(λ) iterates {Ct}, {bt}, respectively, and we show that G∗ = C for h = h1 and G∗ = b for h = h2. We also write the iterates {bt} equivalently as bt+1 = Gt+1 + Wt+1 with h = h2 in the definition of {Gt}. Then, the L1-convergence part of Theorem 2.1 follows from Theorem 3.1 and Prop. 3.1(i), and the a.s. convergence part of Theorem 2.1 follows from Theorem 3.3 and Prop. 3.1(ii). The complete proof with all the details is given in Appendix A.5."
    }, {
      "heading" : "4. Convergence Analysis of ETD(λ)",
      "text" : "Recall that ETD(λ) calculates iteratively θt, t ≥ 0, according to θt+1 = θt + αt et · ρt ( Rt + γt+1φ(St+1) >θt − φ(St)>θt ) . (4.1)\nUsing the results of Section 3, we can now analyze its convergence by applying a “mean ODE” method from stochastic approximation theory (Kushner and Yin, 2003).\n16. Theorem 3.1 is useful here because on the separable metric space S×A×Rn+1, bounded Lipschitz continuous functions are convergence-determining for weak convergence of probability measures (Dudley, 2002, Theorem 11.3.3).\nDenoting ω̃t+1 = ρt (Rt − r(St, At, St+1)), let us write the iteration (4.1) equivalently as\nθt+1 = θt + αt h(θt, ξt) + αt et · ω̃t+1, (4.2)\nwhere ξt = (et, St, At, St+1) and h : Rn × Rn × S ×A× S → Rn is given by h(θ, ξ) = e · ρ(s, a) ( r(s, a, s′) + γ(s′)φ(s′)>θ − φ(s)>θ ) , for ξ = (e, s, a, s′). (4.3)\nWe will apply (Kushner and Yin, 2003, Theorem 6.1.1) to analyze the convergence of {θt} generated by (4.1). The “mean ODE” associated with ETD(λ) (4.1) is\nẋ = h̄(x), where h̄(x) = Cx+ b. (4.4)\nWhen C is negative definite, the above ODE has a unique bounded (constant) solution x(·) ≡ θ∗ = −C−1b on the time interval (−∞,+∞), and θ∗ is globally asymptotically stable for (4.4) in the sense of Liapunov (cf. Kushner and Clark, 1978, p. 23-24). (A Liapunov function in this case is given by ‖θ − θ∗‖22, where ‖ · ‖2 denotes the Euclidean norm.)\nHowever, the a.s. boundedness of {θt} is not easy to prove directly, which has prevented us from getting the desired convergence θt\na.s.→ θ∗ from (Kushner and Yin, 2003, Theorem 6.1.1) directly. For this reason, we analyze first a constrained version of (4.1) and establish its convergence. The result will then help the convergence analysis of the unconstrained algorithm (4.1) in Section 4.2."
    }, {
      "heading" : "4.1. Convergence of Constrained ETD(λ)",
      "text" : "Consider the following constrained ETD(λ) algorithm:\nθt+1 = ΠB ( θt + αt h(θt, ξt) + αt et · ω̃t+1 ) , (4.5)\nwhere B is a closed ball in Rn with a sufficiently large radius r: B = {θ ∈ Rn | ‖θ‖2 ≤ r}, and ΠB is the Euclidean projection onto B. The “mean ODE” associated with the constrained algorithm (4.5) is the projected ODE\nẋ = h̄(x) + z, z ∈ −NB(x), (4.6)\nwhereNB(x) is the normal cone ofB at x, and z is the boundary reflection term that cancels out the component of h̄(x) inNB(x) and is the “minimal force” needed to keep the solution in B (Kushner and Yin, 2003, Chap. 4.3). The negative definiteness of the matrixC implies that the projected ODE (4.6) has no stationary points other than θ∗ if the radius of B is sufficiently large:\nLemma 4.1 Let c > 0 be such that x>Cx ≤ −c‖x‖22 for all x ∈ Rn. Suppose B has a radius r > ‖b‖2/c. Then θ∗ lies in the interior of B, and the only solution x(t), t ∈ (−∞,+∞), of the projected ODE (4.6) in B is x(·) ≡ θ∗.\nThe proof of Lemma 4.1 is given in Appendix B. We now apply (Kushner and Yin, 2003, Theorem 6.1.1) and Lemma 4.1 to prove the a.s. convergence of the constrained ETD(λ) as stated in the theorem below. The proof is given in Appendix B, and it uses the results of Section 3 to verify the conditions required by (Kushner and Yin, 2003, Theorem 6.1.1).\nTheorem 4.1 (Almost sure convergence of constrained ETD(λ)) Let Assumptions 2.1-2.3 hold. Let {θt} be the sequence generated by the constrained ETD(λ) algorithm (4.5) with stepsizes satisfying αt = O(1/t) and\nαt−αt+1 αt = O(1/t), and with the radius r of B exceeding the threshold\ngiven in Lemma 4.1. Then, for any given initial (e0, F0, θ0), θt a.s.→ θ∗."
    }, {
      "heading" : "4.2. Convergence of ETD(λ)",
      "text" : "We now prove the convergence theorem, Theorem 2.2, for the unconstrained ETD(λ) algorithm by using the convergence of the constrained algorithm we just established. In particular, we shall compare the iterates generated by the unconstrained algorithm with those generated by the constrained one, and show that the difference between them diminishes asymptotically with probability one.\nLet B = { θ ∈ Rn | ‖θ‖2 ≤ r } with its radius r satisfying the condition of Lemma 4.1. Note that to project θ onto B is simply to scale θ: ΠBθ = θ if ‖θ‖2 ≤ r; and ΠBθ = r · θ/‖θ‖2 if ‖θ‖2 > r. More concisely,\nΠBθ = η θ, where η = min{1, r/‖θ‖2}.\nTo simplify notation, define matrix Ht and vector gt by\nHt = et · ρt ( γt+1 φ(St+1)− φ(St) )> , gt = et · ρtRt.\nLet us write the constrained algorithm (4.5) equivalently as\nθ̃t+1 = (I + αtHt) · ηt θ̃t + αt gt, (4.7)\nwhere η0 = 1 and ηt = min{1, r/‖θ̃t‖2} for t ≥ 1. (For t ≥ 1, ηt θ̃t corresponds to the projected iterate in (4.5), and θ̃t the iterate just before the projection.) The unconstrained algorithm (4.1) can be equivalently written as\nθt+1 = (I + αtHt) · θt + αt gt. (4.8)\nLemma 4.2 Under the conditions of Theorem 4.1, for any given initial (e0, F0), almost surely, the sequence of matrices, ∏t k≥t̄ (I + αkHk), t = t̄, t̄ + 1, . . ., converges to the n × n zero matrix as t→∞, for all t̄ ≥ 0.\nProof It is sufficient to consider a given (arbitrary) vector y ∈ Rn and prove that for each initial (e0, F0) and each t̄ ≥ 0, ∏t k≥t̄ (I + αkHk) y\na.s.→ 0. To this end, consider generating the iterates θ̃t̄, θ̃t̄+1, . . . , starting from time t̄ and θ̃t̄ = y, by using the constrained algorithm (4.7) as follows:\nθ̃k+1 = (I + αkHk) · ηk θ̃k, k ≥ t̄.\nIn the above, we calculate (ek, Fk) and Hk as before starting from time 0 and the given initial condition (e0, F0), and we have set gk = Rk = 0 for all k. Notice that since the stepsize sequence {αt} satisfies the condition of Theorem 4.1, so does the stepsize sequence, αt̄+1, αt̄+2, . . .. Then, in view of the Markovian property of {(St, At, et, Ft)}, we can apply Theorem 4.1 to the above iteration starting from time t̄ for each possible value of (et̄, Ft̄), thereby concluding that for the given (e0, F0) and t̄, θ̃t\na.s.→ 0 (because Rk = 0 for all k and the solution to Cθ = 0 is 0). On the other hand,\nθ̃t+1 = (∏t k≥t̄ (I + αkHk) ) · (∏t k≥t̄ ηk ) · y. (4.9)\nSince the solution 0 lies in the interior of B, if θ̃t → 0, then ηk = 1 for all k sufficiently large. Thus the convergence θ̃t a.s.→ 0 implies that as t → ∞, ∏t k≥t̄ ηk converges a.s. to a strictly positive number that depends on the sample path and the vector y. Consequently, from Eq. (4.9) and the\nconvergence θ̃t a.s.→ 0, we obtain that (∏t k≥t̄ (I + αkHk) ) y a.s.→ 0 as t → ∞. Now this holds for any given vector y, so by letting y be each column of the identity matrix, it follows that as t→∞, the matrix ∏t k≥t̄ (I + αkHk) converges a.s. to the zero matrix.\nFinally, we prove the a.s. convergence of the unconstrained ETD(λ) as stated by Theorem 2.2: Proof of Theorem 2.2 Let {θ̃t} be the iterates generated by the constrained algorithm (4.7) using the same trajectory of states, actions and rewards that are used by the unconstrained algorithm (4.1) to generate {θt}. By Theorem 4.1 and Lemma 4.2, there exists a set Ω1 of sample paths such that Ω1 has probability one and on Ω1,\nθ̃t → θ∗ and lim t→0\n∏t k≥t̄ (I + αkHk) = 0n×n, ∀ t̄ ≥ 0,\nwhere 0n×n denotes the n×n zero matrix. Consider each path in Ω1. By our choice of the constraint set B, θ∗ lies in the interior of B (Lemma 4.1), so the convergence θ̃t → θ∗ implies the existence of a path-dependent time t′ <∞ such that ηk = 1 for all k ≥ t′. Then\nθ̃k+1 = (I + αkHk) · θ̃k + αk gk, ∀ k ≥ t′,\nand consequently, θk+1 − θ̃k+1 = (I + αkHk) · ( θk − θ̃k ) , ∀ k ≥ t′,\nθt+1 − θ̃t+1 = (∏t k≥t′ (I + αkHk) ) · ( θt′ − θ̃t′ ) , ∀ t ≥ t′. (4.10)\nAs t→∞, the matrix ∏t k≥t′ (I + αkHk)→ 0n×n for the sample path under consideration. Thus, from Eq. (4.10) we obtain θt − θ̃t → 0; since θ̃t → θ∗, this implies θt → θ∗.\nRemark 4.1 (Almost sure convergence of regular off-policy TD(λ)) If λ is a constant sufficiently close to 1, the matrix associated with the “mean updates” of the regular off-policy TD(λ) algorithm is also negative definite (Bertsekas and Yu, 2009). In that case, (Yu, 2012, Prop. 4.1) established the a.s. convergence but only for a constrained version of the algorithm, similar to our Theorem 4.1. The proofs given in this subsection, combined with (Yu, 2012, Prop. 4.1), can be used to establish the desired a.s. convergence for the unconstrained off-policy TD(λ) in that case."
    }, {
      "heading" : "Acknowledgments",
      "text" : "I thank Prof. Richard Sutton for discussions on the ETD algorithm, and Dr. Joseph Modayil, Ashique Mahmood, and several anonymous reviewers for helpful comments on the first version of this paper. This research was supported by a grant from Alberta Innovates – Technology Futures."
    }, {
      "heading" : "Appendix A. Proof Details for Section 3",
      "text" : "In this appendix we give proof details and related results for Section 3. Assumption 2.1 on the target and behavior policies will be in force throughout, so it will not be mentioned explicitly in intermediate technical results."
    }, {
      "heading" : "A.1. Some Basic Technical Lemmas",
      "text" : "We prove three basic lemmas that will be useful later. First, recall that Γ and Λ are diagonal matrices with γ(s) (discount factors) and λ(s), s ∈ S , on their diagonals, respectively. Note also that under Assumption 2.1, the inverse (I −PπΓ)−1 exists. This implies that (I −PπΓΛ)−1 also exists. Then, since\n(I − PπΓ)−1 = ∞∑ t=0 (PπΓ) t, (I − PπΓΛ)−1 = ∞∑ t=0 (PπΓΛ) t,\nboth (PπΓ)t and (PπΓΛ)t converge to the zero matrix as t→∞. We now specify some notation. In what follows, let 1 denote the vector of all ones. For an expression H that results in a vector in RN , we will write (H)(s) for the s-th entry of the resulting vector. (For example, (Pπ1)(s) and (1>Pπ)(s) represent the s-th entry of the vector Pπ1 and 1>Pπ, respectively.)\nLet Ft = σ ( S0, A0, . . . , St ) be the σ-algebra generated by the states and actions up to time t, including the state St but excluding the action At. Recall some shorthand notation we defined earlier:\nρt = ρ(St, At) = π(At|St) πo(At|St) , γt = γ(St), λt = λ(St).\nTo simplify notation, let us also define for t ≥ 1,\nβt = ρt−1 γt λt.\nLemma A.1 For all t > k ≥ 0,\nE [ ρkγk+1 · · · ρt−1γt | Fk ] = ( (PπΓ) t−k1 ) (Sk) ≤ 1, (A.1)\nE [ βk+1βk+2 · · ·βt | Fk ] = ( (PπΓΛ) t−k1 ) (Sk) ≤ 1. (A.2)\nFurthermore, as t→∞, ∏t k=1 ( ρk−1γk ) a.s.→ 0 and∏tk=1 βk a.s.→ 0. Proof The first two equations follow simply from a direct calculation.\nLet ∆t = ∏t k=1 ( ρk−1γk ) . To prove ∆t a.s.→ 0, consider equivalently the iterates\n∆0 = 1, ∆t = (ρt−1γt)∆t−1, t ≥ 1.\nClearly ∆t is Ft-measurable, and by Eq. (A.1) with k = t− 1,\nE [ ∆t | Ft−1 ] = ∆t−1 · E [ ρt−1γt | Ft−1 ] ≤ ∆t−1.\nSo {(∆t,Ft)} is a nonnegative supermartingale with E[∆0] = 1 < ∞. By a convergence theorem for nonnegative supermartingales (Neveu, 1975, Theorem II-2-9), ∆t\na.s.→ ∆∞ for some nonnegative random variable ∆∞ satisfying E [ ∆∞ ] ≤ lim inft→∞ E [ ∆t ] . From Eq. (A.1) with k = 0, we have\nE [ ∆t ] ≤ 1>(PπΓ)t1 → 0 as t → ∞; therefore, E [ ∆∞ ] = 0. This implies ∆∞ = 0 a.s., i.e., ∆t a.s.→ 0. The assertion ∏t k=1 βk\na.s.→ 0 follows similarly by considering the iterates ∆t = βt∆t−1 with ∆0 = 1, and by using Eq. (A.2) together with the nonnegative supermartingale convergence argument.\nLemma A.2 For k ≥ 0, let Yk be anFk-measurable nonnegative random variable. Then for t > k,\nE [ Yk · (ρkγk+1 · · · ρt−1γt )] ≤ E[Yk] · ( 1>(PπΓ) t−k1 ) , (A.3)\nE [ Yk · ( βk+1βk+2 · · ·βt )] ≤ E[Yk] · ( 1>(PπΓΛ) t−k1 ) . (A.4)\nHence, if for some constant L <∞, E[Yk] ≤ L for all k, then\nE\n[ t∑\nk=0\nYk · (ρkγk+1 · · · ρt−1γt )] ≤ L · 1> ( t∑\nk=0\n(PπΓ) k ) 1 <∞, (A.5)\nE\n[ t∑\nk=0\nYk · ( βk+1βk+2 · · ·βt )] ≤ L · 1> ( t∑\nk=0\n(PπΓΛ) k ) 1 <∞. (A.6)\nProof For any k ≥ 0, ( (PπΓ) t−k1 ) (Sk) ≤ 1>(PπΓ)t−k1. Using this, Eq. (A.1) in Lemma A.1, and the assumption that Yk is Fk-measurable, we have\nE [ Yk · ( ρkγk+1 · · · ρt−1γt )] = E [ Yk · E [ (ρkγk+1 · · · ρt−1γt ) | Fk ] ] ≤ E[Yk] · ( 1>(PπΓ) t−k1 ) .\nThis proves Eqs. (A.3), (A.5). Similarly, Eqs. (A.4), (A.6) are obtained by using Eq. (A.2) in Lemma A.1 and a direct calculation.\nLemma A.3 Let {ak} and {ck} be two sequences of nonnegative numbers with ∑∞\nk=1 ak < ∞ and ∑∞ k=1 ck <∞. Then limt→∞ ∑t k=1 ak ct−k = 0.\nProof For any m ≤ t,\nt∑ k=1 ak ct−k = m∑ k=1 ak ct−k + t∑ k=m+1 ak ct−k ≤ ( max k ak ) · t−1∑ k=t−m ck + (max k ck) · t∑ k=m+1 ak.\nSince {ak} and {ck} are summable by assumption, we have maxk ak < ∞, maxk ck < ∞, and limt→∞ ∑t−1 k=t−m ck = 0. So if we fix m and let t go to infinity in the preceding inequality, we have\nlim sup t→∞ t∑ k=1 ak ct−k ≤ (max k ck) · ∞∑ k=m+1 ak.\nSince limm→∞ ∑∞\nk=m+1 ak = 0 by the summable assumption, by letting m go to infinity in the right-hand side above, we obtain limt→∞ ∑t k=1 ak ct−k = 0.\nA.2. Properties of the Trace Iterates {(et, Ft)}\nIn this subsection we state formally the properties of trace iterates which we mentioned in Section 3.1, and we give their proofs. These properties will be used frequently in obtaining some of our main convergence theorems.\nThe following proposition is the property (i) mentioned in Section 3.1. First, let us express the traces et, Ft, by using their definitions [cf. Eqs. (2.2)-(2.4)], as\nFt = F0 · ( ρ0γ1 · · · ρt−1γt ) + t∑ k=1 i(Sk) · ( ρkγk+1 · · · ρt−1γt ) , (A.7)\net = e0 · ( β1 · · ·βt ) + t∑ k=1 Mk · φ(Sk) · ( βk+1 · · ·βt ) , (A.8)\nwhere βk = ρk−1γkλk as defined in the previous subsection, and\nMk = λk i(Sk) + (1− λk)Fk. Let Ft = σ ( S0, A0, . . . , St ) for t ≥ 0, throughout this subsection.\nProposition A.1 For any given initial (e0, F0), supt≥0 E [∥∥(et, Ft)∥∥] <∞.\nProof Let us calculate E [ Ft ] and E [ ‖et‖ ] . Since the number of states is finite, there exists a finite constant L > 0 such that L ≥ ‖(e0, F0)‖ and L ≥ i(s), L ≥ ‖φ(s)‖ for all states s. Using the expression (A.7) for Ft and applying Eq. (A.5) in Lemma A.2 (with Y0 = F0, Yk = i(Sk), k ≥ 1), we have the bound\nE [ Ft ] ≤ L · 1>\n( t∑\nk=0\n(PπΓ) k ) 1 ≤ L · 1>(I − PπΓ)−11.\nThus supt≥0 E [ Ft ] < ∞. We now calculate E [ ‖et‖ ] . Using the expression (A.8) for et, and using also the fact Mk ≤ L+ Fk, we can bound ‖et‖ by\n‖et‖ ≤ L · ( β1 · · ·βt) + L · t∑ k=1 ( L+ Fk ) · ( βk+1βk+2 · · ·βt ) .\nUsing the fact supk≥0 E [ Fk ] ≤ L′ for some finite constant L′ as we just proved, and using also Eq. (A.6) in Lemma A.2 (with Y0 = L, Yk = L(L+ Fk), k ≥ 1), we obtain\nE [ ‖et‖ ] ≤ L(L+ L′ + 1) · 1>\n( t∑\nk=0\n(PπΓΛ) k ) 1 ≤ L(L+ L′ + 1) · 1>(I − PπΓΛ)−11.\nHence supt≥0 E [ ‖et‖ ] <∞. Since ∥∥(et, Ft)∥∥ ≤ ‖et‖+ Ft, this shows that sup t≥0 E [∥∥(et, Ft)∥∥] ≤ sup t≥0 E [ ‖et‖ ] + sup t≥0 E [ Ft ] <∞.\nThe proof is complete.\nRecall that {Zt} with Zt = (St, At, et, Ft) denotes the Markov chain on the joint space S × A × Rn+1 of states, actions and traces, and it is a weak Feller Markov chain (cf. Footnote 12). As explained in Section 3.1, since S and A are finite, the preceding proposition implies that {Zt} is bounded in probability and hence, by its weak Feller property, has at least one invariant probability measure. We will need the following result (which is the property (ii) in Section 3.1) to prove that {Zt} has a unique invariant probability measure.\nLet (êt, F̂t), t ≥ 1, be defined by the same recursion (2.2)-(2.4) that defines (et, Ft), using the same state and action random variables, but with a different initial condition (ê0, F̂0). We write a zero vector in any Euclidean space as 0.\nProposition A.2 For any two given initial conditions (e0, F0) and (ê0, F̂0),\nFt − F̂t a.s.→ 0, et − êt a.s.→ 0.\nProof Using the expression (A.7) for Ft and F̂t, we have Ft − F̂t = (F0 − F̂0) · ∏t k=1(ρk−1γk).\nSince ∏t k=1(ρk−1γk) a.s.→ 0 by Lemma A.1, it follows that Ft − F̂t a.s.→ 0.\nThe proof of Lemma A.1 also shows E [∣∣Ft − F̂t∣∣] ≤ ∣∣F0 − F̂0∣∣ · 1>(PπΓ)t1. (A.9)\nWe will need this inequality below. We now prove et− êt\na.s.→ 0. To simplify the derivation, we first observe that if ê0 6= e0 but F̂0 = F0, then F̂t = Ft for all t, so the expression (A.8) for et and êt gives ‖et−êt‖ = ‖e0−ê0‖· ∏t k=1 βk.\nSince ∏t k=1 βk a.s.→ 0 by Lemma A.1, it follows immediately that in this case ‖et − êt‖ a.s.→ 0.\nThus, for the general case, we can focus on the difference between et and êt that is due to the difference between the initial F0 and F̂0. In particular, define another sequence of iterates (ẽt, F̃t) using the same recursion (2.2)-(2.4) but with the initial condition (ẽ0, F̃0) = (e0, F̂0). Since∥∥et− êt∥∥ ≤ ∥∥et− ẽt∥∥+∥∥ẽt− êt∥∥ and ∥∥ẽt− êt∥∥ a.s.→ 0 by what we just proved, to show et− êt a.s.→ 0, it is sufficient to prove ‖et − ẽt‖\na.s.→ 0. Since F̃0 = F̂0, the sequence {F̃t} coincides with {F̂t}. Then by the definition of et and ẽt,\net − ẽt = βt ( et−1 − ẽt−1 ) + (1− λt) ( Ft − F̂t ) · φ(St).\nSince 0 ≤ E [ βt | Ft−1 ] ≤ 1 (Lemma A.1) and 0 ≤ 1− λt ≤ 1, it follows that\nE [∥∥et − ẽt∥∥ | Ft−1] ≤ ∥∥et−1 − ẽt−1∥∥+ Yt−1, where Yt−1 = E[∣∣Ft − F̂t∣∣ · ‖φ(St)‖ | Ft−1].\n(A.10) Let us show ∑∞ t=0 Yt <∞ a.s. In view of Eq. (A.10), this will then imply, by a convergence theorem in (Neveu, 1975, Ex. II-4, p. 33-34) for nonnegative random processes (which is a consequence of the nonnegative supermartingale convergence theorem), that\n∥∥et−ẽt∥∥ converges a.s. to a finite limit. To prove ∑∞ t=0 Yt <∞ a.s., it is sufficient to show E [∑∞ t=0 Yt ] <∞. LetL = maxs∈S ‖φ(s)‖.\nBy Eq. (A.9), for each t, E [ Yt ] = E [∣∣Ft+1 − F̂t+1∣∣ · ‖φ(St+1)‖ ] ≤ L∣∣F0 − F̂0∣∣ · 1>(PπΓ)t+11.\nSince ∑∞\nt=0(1 >(PπΓ) t+11) ≤ 1>(I − PπΓ)−11 < ∞, we obtain E [∑∞ t=0 Yt ] = ∑∞ t=0 E [ Yt ] <\n∞. Hence ∑∞\nt=0 Yt <∞ a.s., and as discussed earlier, this implies that∥∥et − ẽt∥∥ a.s.→ ∆∞\nfor a nonnegative real-valued random variable ∆∞. What remains to be proved is ∆∞ = 0 a.s. By Fatou’s lemma (Dudley, 2002, Theorem 4.3.3),\nE[∆∞] ≤ lim inf t→∞\nE [∥∥et − ẽt∥∥]. (A.11)\nWe show that the right-hand side equals 0. By a direct calculation (using Eq. (A.8) and the fact ẽ0 = e0), we can write\net − ẽt = t∑\nk=1\nφ(Sk) · (Fk − F̂k) · (1− λk) · ( βk+1 · · ·βt ) .\nFor each k ≥ 1, using Lemma A.2 and Eq. (A.9), we have E [∣∣Fk − F̂k∣∣ · (1− λk) · (βk+1 · · ·βt)] ≤ E[∣∣Fk − F̂k∣∣] · (1>(PπΓΛ)t−k1)\n≤ ∣∣F0 − F̂0∣∣ · (1>(PπΓ)k1) · (1>(PπΓΛ)t−k1).\nFrom the preceding two relations, it follows that\nE [∥∥et − êt∥∥] ≤ L∣∣F0 − F̂0∣∣ · t∑\nk=1\n( 1>(PπΓ) k1 ) · ( 1>(PπΓΛ) t−kPπ1 ) . (A.12)\nFrom Lemma A.3 with {ak} and {ck} defined as ak = 1>(PπΓ)k1 and ck = 1>(PπΓΛ)k1 for k ≥ 1, we have\nlim t→∞ t∑ k=1 ( 1>(PπΓ) k1 ) · ( 1>(PπΓΛ) t−k1 ) = 0.\nCombining this with Eq. (A.12) gives lim inft→∞ E [∥∥et− ẽt∥∥] = 0, and consequently, E[∆∞] = 0\nby Eq. (A.11). This implies ∆∞ = 0 a.s., i.e., ∥∥et − ẽt∥∥ a.s.→ 0.\nThe next proposition is the property (iii) mentioned in Section 3.1, which concerns approximating the trace iterates (et, Ft) by truncated traces that depend on a fixed number of the most recent states and actions only. We will use this proposition subsequently to prove Theorem 3.1: it allows us to work with simple finite-space Markov chains, instead of working with the infinite-space Markov chain {Zt} directly.\nFor each integer K ≥ 1, we define the truncated traces Yt,K = (ẽt,K , F̃t,K) as follows:\nYt,K = (et, Ft) for t ≤ K,\nand for t ≥ K + 1,\nF̃t,K = t∑\nk=t−K i(Sk) ·\n( ρkγk+1 · · · ρt−1γt ) , (A.13)\nM̃t,K = λt i(St) + (1− λt)F̃t,K , (A.14)\nẽt,K = t∑\nk=t−K M̃k,K · φ(Sk) ·\n( βk+1 · · ·βt ) . (A.15)\nDenote the original traces by Yt = (et, Ft); recall that they can be expressed as in Eqs. (A.7)(A.8). We have the following result, in which the notation “LK ↓ 0” means that LK decreases monotonically to 0 as K →∞, and in which Z0 = (S0, A0, e0, F0) as we recall:"
    }, {
      "heading" : "Proposition A.3",
      "text" : "(i) For any given initial Y0 = (e0, F0), there exist constants LK ,K ≥ 1, with LK ↓ 0, such that\nE [∥∥Yt − Yt,K∥∥] ≤ LK , ∀ t ≥ 0.\n(ii) There exist constants LK ,K ≥ 1, independent of the given initial value of Z0, such that LK ↓ 0 and\nE [∥∥Yt,K′ − Yt,K∥∥] ≤ LK , ∀K ′ ≥ K, t > 2K ′.\nProof Let L = max{F0, maxs∈S i(s)}. We first calculate Ft − F̃t,K . By definition Ft − F̃t,K = 0 for t ≤ K. For t ≥ K + 1, using the expressions (A.7), (A.13) of Ft and F̃t,K , we have\nFt − F̃t,K = F0 · ( ρ0γ1 · · · ρt−1γt ) + t−K−1∑ k=1 i(Sk) · ( ρkγk+1 · · · ρt−1γt ) ,\nfrom which it follows by applying Eq. (A.3) in Lemma A.2 that E [∣∣Ft − F̃t,K∣∣] ≤ L · 1>( t∑\nk=K+1\n(PπΓ) k ) 1 ≤ L · 1> ( ∞∑ k=K+1 (PπΓ) k ) 1 def = L (1) K . (A.16)\nSimilarly we bound et − ẽt,K . By definition et = ẽt,K for t ≤ K. For t ≥ K + 1, using the expressions (A.8), (A.15) of et and ẽt,K , and using also the expressions (2.3), (A.14) of Mt and M̃t,K , we have\net − ẽt,K = e0 · ( β1 · · ·βt ) + t−K−1∑ k=1 Mk · φ(Sk) · ( βk+1 · · ·βt ) +\nt∑ k=t−K φ(Sk) · ( Fk − F̃k,K ) · (1− λk) · ( βk+1 · · ·βt ) .\nBy Prop. A.1, supk≥0 E[Mk] < ∞, so we can find a constant L′ < ∞ that is greater than ‖e0‖, maxs∈S ‖φ(s)‖ and ( maxs∈S ‖φ(s)‖ ) · supk≥0 E[Mk]. Then applying Eq. (A.4) in Lemma A.2, and using also Eq. (A.16), we obtain\nE [∥∥et − ẽt,K∥∥] ≤ L′ · 1>(t−K−1∑\nk=0\n(PπΓΛ) t−k ) 1 + L′ ·\nt∑ k=t−K E [∣∣Fk − F̃k,K∣∣] · (1>(PπΓΛ)t−k1)\n≤ L′ · 1> (\nt∑ k=K+1 (PπΓΛ) k ) 1 + L′ · L(1)K · ( 1> K∑ k=0 (PπΓΛ) k1 )\n≤ L′ · 1> ( ∞∑ k=K+1 (PπΓΛ) k ) 1 + L′ · L(1)K · ( 1>(I − PπΓΛ)−11 ) def = L (2) K .\nNow let LK = L (1) K +L (2) K . From the expressions of L (1) K and L (2) K given above, clearly, LK ↓ 0 as K →∞. Then, in view of the relation ∥∥Yt − Yt,K∥∥ ≤ ∣∣Ft − F̃t,K∣∣+ ∥∥et − ẽt,K∥∥, we obtain the\ndesired bound E [∥∥Yt − Yt,K∥∥] ≤ E [∣∣Ft − F̃t,K∣∣]+ E [∥∥et − ẽt,K∥∥] ≤ LK . This proves part (i).\nPart (ii) is proved similarly. By the definition of the truncated traces, for t > 2K ′ ≥ 2K,\nF̃t,K′ − F̃t,K = t−K−1∑ k=t−K′ i(Sk) · ( ρkγk+1 · · · ρt−1γt ) ,\nand\nẽt,K′ − ẽt,K = t−K−1∑ k=t−K′ M̃k,K′ · φ(Sk) · ( βk+1 · · ·βt ) +\nt∑ k=t−K φ(Sk) · ( F̃k,K′ − F̃k,K ) · (1− λk) · ( βk+1 · · ·βt ) .\nWe then apply the same calculation as in the proof of part (i). When t > 2K ′, the truncated traces do not depend on the initial condition (e0, F0). Since the state and action spaces are finite, we can set the constants L,L′ to be independent of the initial condition of Z0. Part (ii) then follows.\nRemark A.1 (On the behavior of trace iterates) From the properties of {(et, Ft)} given above and the ergodicity of the Markov chain {(St, At, et, Ft)} shown in Theorem 3.2, we see that these trace iterates are well-behaved. On the other hand, like in regular off-policy algorithms, these iterates can be unbounded almost surely and their variances can grow to infinity with time. There are no contradictions here. To illustrate this point, let us consider a simple example with just 1 state and 2 actions, S = {1},A = {a1, a2}, where all actions result in a self-transition at state 1. Let π(a1 | 1) = 1 for the target policy π, and let πo(a1 | 1) = q < 1 for the behavior policy πo. Let the discount factor be a constant γ < 1. Then for all t,\nE[ γ2t ρ 2 t−1 | Ft−1 ] = γ2/q.\nSuppose γ2/q > 1. Then even with i(1) = 0, if F0 > 0, the definition Ft = γtρt−1Ft−1 implies that\nE[F 2t ] = E [ E[ γ2t ρ 2 t−1 | Ft−1 ] · F 2t−1 ] = (γ2/q)t · F 20 →∞,\nyet since i(1) = 0, {Ft} is also a supermartingale converging to 0 a.s. (cf. the proof of Lemma A.1). For the case i(1) > 0, again E[F 2t ] → ∞ if γ2/q > 1, and by (Yu, 2012, Prop. 3.1) the sequence {Ft} is almost surely unbounded if γ/q > 1, yet {Ft} is bounded in probability in the sense described by Prop. A.1.\nAs mentioned earlier in Remark 2.2, it can be desirable to restrict the behavior policy so that the variances of the trace iterates do not grow to infinity. In the simple example above, this can be easily arranged. In the general case, however, if the state-dependent discount factor γ(·) can take the value 1 for some states, then without knowledge of the MDP model, to sufficiently restrict the behavior policy seems to be a difficult task."
    }, {
      "heading" : "A.3. Proof of Theorem 3.1",
      "text" : "For convenience, we restate Theorem 3.1 here. Recall that the theorem concerns the recursion\nGt+1 = (1− αt)Gt + αt h(Yt, St, At, St+1),\nwhere Yt = (et, Ft), and the function h is Lipschitz continuous in y: for some constant Lh,∥∥h(y, s, a, s′)− h(ŷ, s, a, s′)∥∥ ≤ Lh‖y − ŷ‖, ∀ y, ŷ ∈ Rn+1, ∀ (s, a, s′) ∈ S ×A× S. Theorem 3.1 (L1-convergence of {Gt}) Let h be a vector-valued function satisfying the Lipschitz condition (3.1), and let {Gt} be defined by the recursion (3.2), using the process {Zt}. Then under Assumptions 2.1, 2.3, there exists a constant vector G∗ (independent of the stepsizes) such that for any given initial Y0 = (e0, F0) and G0, limt→∞ E\n[∥∥Gt −G∗∥∥] = 0. Proof The proof proceeds in three steps: (i) For each K ≥ 1, we consider the truncated traces Yt,K = (ẽt,K , F̃t,K), t = 0, 1, . . ., defined by Eqs. (A.13)-(A.15). Correspondingly, we define iterates G̃0,K = G0 and\nG̃t+1,K = (1− αt) G̃t,K + αt h(Yt,K , St, At, St+1).\nFor each t, Yt,K is a function of (St−2K , At−2K , . . . , St), so h(Yt,K , St, At, St+1) can be viewed as a function of Xt = (St−2K , At−2K , . . . , St+1), where {Xt} is a finite state Markov chain with a single recurrent class by Assumption 2.1(ii). Then, with E0 denoting the expectation under the stationary distribution of the Markov chain {(St, At)}, we have, by a result from stochastic approximation theory (Borkar, 2008, Chap. 6, Theorem 7 and Cor. 8), that under Assumption 2.3 on the stepsizes,\nG̃t,K a.s.→ G∗K , where G∗K = E0 [ h(Yk,K , Sk, Ak, Sk+1) ] ∀ k > 2K. (A.17)\nClearly, the vector G∗K does not depend on the initial condition (Y0, G0) and the stepsizes {αt}. Since for all t, ‖G̃t,K‖ ≤ L for some constant L < ∞, we also have by the bounded convergence theorem\nlim t→∞\nE [∥∥G̃t,K −G∗K∥∥] = 0. (A.18)\n(ii) We show that as K → ∞, G∗K converges to some vector G∗. For any K ′ > K, using the Lipschitz property of h and Prop. A.3(ii), we have that for k > 2K ′,∥∥G∗K′ −G∗K∥∥ = ∥∥E0[h(Yk,K′ , Sk, Ak, Sk+1)− h(Yk,K , Sk, Ak, Sk+1)]∥∥\n≤ Lh E0 [∥∥Yk,K′ − Yk,K∥∥] ≤ Lh LK ,\nwhere LK is some constant with LK ↓ 0 as K →∞. This shows that {G∗K} is a Cauchy sequence and hence converges to some G∗. (iii) We establish the theorem by bounding the differences between Gt and G̃t,K for an increasing K. For each K,\nlim sup t→∞\nE [∥∥Gt −G∗∥∥] ≤ lim sup t→∞ E [∥∥Gt − G̃t,K∥∥]+ lim sup t→∞ E [∥∥G̃t,K −G∗K∥∥]+ ∥∥G∗K −G∗∥∥.\nIn the right-hand side, the second term equals 0 by Eq. (A.18), and the last term converges to 0 as K →∞, as we just showed in step (ii). Consider now the first term. Since\nGt+1 − G̃t+1,K = (1− αt) ( Gt − G̃t,K ) + αt ( h(Yt, St, At, St+1)− h(Yt,K , St, At, St+1) ) and\n∥∥h(Yt, St, At, St+1)−h(Yt,K , St, At, St+1)∥∥ ≤ Lh‖Yt−Yt,K‖ by the Lipschitz property of h, we have\nE [∥∥Gt+1 − G̃t+1,K∥∥] ≤ (1− αt)E[∥∥Gt − G̃t,K∥∥]+ αtLhE[‖Yt − Yt,K‖]\n≤ (1− αt)E [∥∥Gt − G̃t,K∥∥]+ αtLhLK , (A.19)\nwhere the second inequality follows from Prop. A.3(i), which gives the constants LK ,K ≥ 1, with LK ↓ 0. For each K, in view of Assumption 2.3 on the stepsize, the inequality (A.19) implies that\nlim sup t→∞\nE [∥∥Gt − G̃t,K∥∥] ≤ LhLK .\nThen, since LK ↓ 0, letting K go to infinity in the right-hand side of the preceding inequality, it follows that limt→∞ E [∥∥Gt −G∗∥∥] = 0."
    }, {
      "heading" : "A.4. Handling Noisy Rewards: Proof of Prop. 3.1",
      "text" : "For convenience, we restate Prop. 3.1 below. For each t ≥ 0, let ωt+1 = Rt − r(St, At, St+1), the noise in the observed reward Rt. We consider the recursion (3.4): W0 = 0 and\nWt+1 = (1− αt)Wt + αt et ρt · ωt+1, t ≥ 0. (A.20)\nRecall that it is assumed in our MDP model thatRt has mean r(St, At, St+1) and bounded variance; specifically, let Ft = σ(S0, A0, . . . , St+1) in what follows, and we have that for some constant L <∞,\nE [ ωt+1 | Ft ] = 0, E [ ω2t+1 | Ft ] < L. (A.21)\nProposition 3.1 (Effects of noise in random rewards) Under Assumptions 2.1, 2.3, for any given initial (e0, F0), we have (i) E [ ‖Wt‖ ] → 0; and (ii) if, in addition, the stepsize is αt = 1/(t + 1), then Wt a.s.→ 0.\nBecause the proofs of part (i) and part (ii) use quite different arguments, we give them separately below. Proof of Prop. 3.1(i) To simplify notation, denote ω̃t+1 = ρt ωt+1. Similarly to the proof of Theorem 3.1, we first consider for each K ≥ 1, the truncated traces {ẽt,K , t ≥ 0} given by Eq. (A.15), and we replace {et} in the recursion (A.20) by {ẽt,K} to define iterates W̃0,K = 0 and\nW̃t+1,K = (1− αt) W̃t,K + αt ẽt,K · ω̃t+1, t ≥ 0.\nSince the number of states and actions is finite, we can bound ‖ẽt,K‖ by some finite constant for all t. Then, using Eq. (A.21), we have that for all t ≥ 0,\nE [ ẽt,K · ω̃t+1 | Ft] = 0, E [ ‖ẽt,K‖2 · ω̃2t+1 | Ft ] ≤ L′ for some constant L′ <∞.\nUnder Assumption 2.3 on the stepsize {αt}, this implies by (Tsitsiklis, 1994, Lemma 1) that W̃t,K\na.s.→ 0. Next we show limt→∞ E [ ‖W̃t,K‖ ] = 0. Since αt ∈ (0, 1] for all t and W̃0,K = 0, for each t ≥ 0, W̃t+1,K can be expressed as a convex combination of ej,K ω̃j+1, j ≤ t, with coefficients ct,j (each ct,j is a function of (α0, . . . , αt)). Consequently, ‖W̃t+1,K‖ ≤ ∑t j=0 ct,j‖ej,K‖ · |ω̃j+1|, and by the convexity of the function x2,\n‖W̃t+1,K‖2 ≤ ∑t j=0ct,j‖ej,K‖2 · |ω̃j+1|2.\nAs discussed earlier, the variance of ej,K · ω̃j+1 can be bounded uniformly for all j, so the preceding inequality implies that there exists some constant L′ <∞ with\nE [ ‖W̃t+1,K‖2 ] ≤ L′, ∀ t ≥ 0. (A.22)\nThis in turn implies that the sequence {‖W̃t,K‖, t ≥ 0} is uniformly integrable (see e.g., Billingsley, 1968, p. 32); i.e.,\nsup t≥0\nE [ ‖W̃t,K‖ · 1 ( ‖W̃t,K‖ ≥ a )] → 0 as a→ +∞,\n(where 1 ( ‖W̃t,K‖ ≥ a ) is the indicator for the event ‖W̃t,K‖ ≥ a). By (Neveu, 1975, Lemma IV-25, p. 66), every uniformly integrable sequence of random variables which converges almost surely also converges in L1. Therefore, since W̃t,K a.s.→ 0 as proved earlier, we have limt→∞ E [ ‖W̃t,K‖ ] = 0. We now prove limt→∞ E [ ‖Wt‖ ] = 0 similarly to the proof step (iii) for Theorem 3.1. For each\nK ≥ 1, since limt→∞ E [ ‖W̃t,K‖ ] = 0, we have\nlim sup t→∞\nE [∥∥Wt∥∥] ≤ lim sup t→∞ E [∥∥Wt − W̃t,K∥∥]+lim sup t→∞ E [∥∥W̃t,K∥∥] = lim sup t→∞ E [∥∥Wt − W̃t,K∥∥] .\nThus it is sufficient to prove that\nlim K→∞ lim sup t→∞\nE [∥∥Wt − W̃t,K∥∥] = 0. (A.23)\nTo this end, let us write Wt+1 − W̃t+1,K = (1− αt) ( Wt − W̃t,K ) + αt ( et − ẽt,K ) · ω̃t+1.\nSince the number of states and actions is finite, Eq. (A.21) implies that for all t, E [ |ω̃t+1| | Ft ] ≤ L′ for some constant L′ <∞. Consequently,\nE [∥∥Wt+1 − W̃t+1,K∥∥] ≤ (1− αt) E [∥∥Wt − W̃t,K∥∥]+ αt L′ · E [∥∥et − ẽt,K∥∥]\n≤ (1− αt) E [∥∥Wt − W̃t,K∥∥]+ αt L′ · LK ,\nwhere the second inequality follows from Prop. A.3(i), and LK ,K ≥ 1, are constants with the property that LK ↓ 0 as K → ∞. By Assumption 2.3 on the stepsize, the preceding inequality implies that for each K,\nlim sup t→∞\nE [∥∥Wt − W̃t,K∥∥] ≤ L′ · LK .\nLetting K go to infinity and using the fact LK ↓ 0, we obtain the desired equality (A.23), which implies limt→∞ E [‖Wt‖] = 0 as discussed earlier.\nProof of Prop. 3.1(ii) Note that with αt = 1/(t+ 1), the convergence Wt a.s.→ 0 we want to prove is equivalent to the convergence of the time average, 1t+1 ∑t k=0 ek · ρkωk+1 a.s.→ 0, where each term in the sum is a function of (ek, Sk, Ak, Sk+1, Rk):\nek · ρkωk+1 = ek · ρ(Sk, Ak) · ( Rk − r(Sk, Ak, SK+1) ) .\nBy Theorem 3.2, the Markov chain {Zt} = {(St, At, et, Ft)} has a unique invariant probability measure ζ. Consequently, the Markov chain {Z ′t} := {(St, At, et, Ft, St+1, Rt)} has a unique invariant probability measure ζ ′, determined by ζ together with the probabilities of the successor state St+1 given (St, At) and the conditional distribution of the rewardRt given (St, At, St+1), which are specified by the MDP model. Let Eζ′ denote expectation with respect to the probability distribution of the stationary Markov chain {Z ′t} with the initial distribution being ζ ′. From Theorem 3.2(ii) and the relation between ζ ′ and ζ, we have\nEζ′ [ ‖e0‖ · ρ0 |ω1| ] ≤ L′Eζ [ ‖e0‖ ] <∞ (A.24)\nfor some constant L′ <∞. Specifically, in the above, we obtain the first inequality by bounding the conditional expectation of |ω1|, conditioned on (e0, S0, A0), by some finite constant [cf. Eq. (A.21)], and we then obtain the second inequality by applying Theorem 3.2(ii).\nGiven the finite expectation in Eq. (A.24), we can apply to the stationary process {Z ′t} with initial distribution ζ ′ a strong law of large numbers for stationary processes [(Doob, 1953, Chap. X, Theorem 2.1); see also (Meyn and Tweedie, 2009, Theorem 17.1.2)]. By this theorem, there exists a nonempty subset D1 of the state space of {Z ′t} such that:\n(i) D1 has ζ ′-measure 1, and (ii) for each initial condition Z ′0 = z\n′ ∈ D1, {Wt} converges a.s. (with respect to the probability measure induced by the initial condition Z ′0 = z\n′ for the process {Z ′t}). In view of the dependence relations between the variables (St, At, St+1, Rt) given by the MDP model, and also in view of the finiteness of the state-action space S × A and the irreducibility property of the behavior policy πo (Assumption 2.1(ii)), the preceding properties of the set D1 imply that there exists a nonempty subsetD2 of the space Rn+1 (which is the space of (et, Ft)) such that:\n(i) D2 has measure 1 with respect to the marginal on Rn+1 of the invariant probability measure ζ, and\n(ii) for each initial condition (e0, F0, S0) = (e, F, s) ∈ D2 × S , {Wt} converges a.s. But the limit of {Wt} cannot differ from 0 by Prop. 3.1(i) proved earlier (since for the given initial condition, E[‖Wt‖] → 0 implies the existence of a subsequence of {Wt} converging to 0 a.s. (Dudley, 2002, Theorem 9.2.1)). Thus, we conclude that for each initial condition (e, F, s) ∈ D2 × S , Wt\na.s.→ 0. Now to establish Prop. 3.1(ii), we only need to show that for any given initial condition (e0, F0) =\n(e, F ) 6∈ D2, Wt a.s.→ 0 as well. To prove this, let s ∈ S be an arbitrary given state. Consider the sequence {(et, Ft,Wt)} with (e0, F0) = (e, F ) and S0 = s. Consider also a second sequence {(ẽt, F̃t, W̃t)} which is generated by the same recursion and the same trajectory of states, actions and rewards that define the first sequence {(et, Ft,Wt)}, but with a pair of initial traces\n(ẽ0, F̃0) = (ẽ, F̃ ) ∈ D2, possibly different from (e, F ). By what we proved earlier, W̃t a.s.→ 0. On the other hand, by definition\nWt+1 − W̃t+1 = 1\nt+ 1 t∑ k=0 ( ek − ẽk ) · ρkωk+1.\nIn the right-hand side, we have ek − ẽk a.s.→ 0 (as k → ∞) by Prop. A.2, and we also have\n1 t+1 ∑t k=0 ρkωk+1\na.s.→ 0 by the properties of {ωt} and (Tsitsiklis, 1994, Lemma 1).17 Consequently, Wt+1 − W̃t+1 a.s.→ 0; since W̃t a.s.→ 0, this implies Wt a.s.→ 0. The proof is now complete."
    }, {
      "heading" : "A.5. Proof of Theorem 2.1 on the Convergence of ELSTD(λ)",
      "text" : "The proof proceeds by calculating the limit G∗ in Theorem 3.1 for the two functions h1, h2 in Eq. (3.3): with y = (e, F ) ∈ Rn+1,\nh1(y, s, a, s ′) = e ·ρ(s, a) ( γ(s′)φ(s′)>−φ(s)> ) , h2(y, s, a, s ′) = e ·ρ(s, a) r(s, a, s′), (A.25)\nwhich are associated with the ELSTD(λ) iterates Ct, bt, respectively. Specifically, based on the proof of Theorem 3.1, we first calculate for each K, the limit G∗K given in Eq. (A.17), which is associated with the truncated traces (ẽt,K , F̃t,K). We then take K to∞ to get the expression of G∗ since G∗ = limK→∞G∗K , as shown in the step (ii) of the proof of Theorem 3.1. The details of this calculation are given below, and the subsequent Lemma A.4 establishes that\nG∗ = C for h = h1; G∗ = b for h = h2. (A.26)\nLet us give now the rest of the proof of Theorem 2.1, assuming for the moment that Eq. (A.26) has been proved. Then, with h = h1, Theorem 3.1 yields the L1-convergence of {Ct} to C, and Theorem 3.3 yields Ct\na.s.→ C for stepsizes αt = 1/(t+ 1). For the iterates {bt} [cf. Eq. (2.11)], we also need to take care of the noise in the rewards Rt, by\nusing Prop. 3.1. Specifically, with W0 = 0, let\nωt+1 = Rt − r(St, At, St+1), Wt+1 = (1− αt)Wt + αt et ρt · ωt+1, t ≥ 0,\n[cf. Eq. (3.4)]. By definition, bt+1 = (1− αt) bt + αt et · ρtRt = (1− αt) bt + αt et · ρt ( r(St, At, St+1) + ωt+1 ) ,\nso the iteration for {bt} can be equivalently expressed as\nbt+1 = Gt+1 +Wt+1,\n17. Specifically, we write Xt+1 := 1t+1 ∑t k=0 ρkωk+1 equivalently as the recursion,\nXt+1 = (1− 1t+1 )Xt + 1 t+1 ρtωt+1 with X0 = 0,\nand apply (Tsitsiklis, 1994, Lemma 1) to obtain Xt a.s.→ 0. To apply the latter lemma, observe that since S andA are finite spaces, ρt is bounded for all t, and the weighted noise variables {ρtωt+1} thus have conditional zero mean and uniformly bounded variances, conditioned on Ft, similar to the properties of {ωt} shown in Eq. (A.21). Then, the conditions of (Tsitsiklis, 1994, Lemma 1) are satisfied and its conclusion applies.\nwhereGt+1 is given by the recursion (3.2) with h = h2 andG0 = b0, andWt+1 is as defined above. Then by Theorem 3.1, Eq. (A.26) and Prop. 3.1(i), we have\nlim t→∞\nE [∥∥bt − b∥∥] ≤ lim t→∞ E [∥∥Gt −G∗∥∥]+ lim t→∞ E [∥∥Wt∥∥] = 0.\nThis proves the L1-convergence of {bt} to b. Similarly, its a.s. convergence in the second part of Theorem 2.1 follows from Theorem 3.3, Eq. (A.26) and Prop. 3.1(ii) as\nGt a.s.→ G∗ = b and Wt a.s.→ 0 =⇒ bt = Gt +Wt a.s.→ b.\nThus Theorem 2.1 is proved. In the rest of this subsection, we verify Eq. (A.26), which we used in the proof above."
    }, {
      "heading" : "Computing the Limiting Matrix and Vector for ELSTD(λ)",
      "text" : "The desired limits for ELSTD(λ) are the matrix C and vector b given in Eqs. (2.6)-(2.9), Section 2:\nC = −Φ>M̄ (I − P λπ,γ) Φ = −Φ>M̄ (I − PπΓΛ)−1 (I − PπΓ) Φ, (A.27)\nb = Φ>M̄ rλπ,γ = Φ >M̄ (I − PπΓΛ)−1 rπ, (A.28)\nwhere M̄ is a diagonal matrix with\ndiag(M̄) = d>πo,i (I − P λπ,γ)−1, dπo,i ∈ RN , dπo,i(s) = dπo(s) · i(s), s ∈ S,\nand dπo(s) is the steady state probability of state s under the behavior policy πo (cf. Assumption 2.1(ii)), and i(s) is the “interest” weight for state s. By the definition (2.6) of P λπ,γ , we can also write\ndiag(M̄) = d>πo,i (I − PπΓ)−1 (I − PπΓΛ). (A.29)\nRecall that Eζ denotes expectation with respect to the stationary Markov chain {Zt}, where Zt = (St, At, et, Ft), with its unique invariant probability measure ζ as the initial distribution (cf. Theorem 3.2). We denote Yt = (et, Ft).\nLemma A.4 Under Assumption 2.1,\nEζ [ h1(Y0, S0, A0, S1) ] = C, Eζ [ h2(Y0, S0, A0, S1) ] = b.\nProof The proof proceeds as follows. We first calculate the limit vectorG∗K defined in Eq. (A.17) in the proof of Theorem 3.1, for the two choices of the function h given in Eq. (A.25): h = h1, h = h2. We then calculate G∗ by its definition given in the proof of Theorem 3.1: G∗ = limK→∞G∗K . Theorem 3.3 shows Eζ [ h(Y0, S0, A0, S1) ] = G∗, so the lemma follows if we prove that G∗ = C for h = h1, and G∗ = b for h = h2. Let E0 denotes expectation with respect to the probability measure of the stationary Markov chain {(St, At)}. Recall that for each K ≥ 1, G∗K is defined by Eq. (A.17) as\nG∗K = E0 [ h(Yt,K , St, At, St+1) ] , ∀ t > 2K, (A.30)\nwhere Yt,K = (ẽt,K , F̃t,K) are the truncated traces defined together with M̃t,K in Eqs. (A.13)(A.15). To simplify the calculation of G∗K , we first calculate E0 [ M̃t,KΨ(St) ] for any given t > K and any matrix or vector-valued function Ψ on S. By Eqs. (A.13)-(A.14),\nM̃t,K = λt i(St) + (1− λt) F̃t,K , F̃t,K = t∑\nk=t−K i(Sk) ·\n( ρkγk+1 · · · ρt−1γt ) .\nThus, M̃t,K can be equivalently expressed as\nM̃t,K = i(St) + K∑ k=1 i(St−k) · ( ρt−kγt−k+1 · · · ρt−1γt ) · (1− λt).\nTo calculate E0 [ M̃t,KΨ(St) ] , we calculate the expectation for each term in the above summation separately. In what follows, for each s ∈ S , let 1s(·) denote the indicator for state s. For an expression H that results in an N -dimensional vector, we write (H)(s) for the s-th entry of the resulting vector. Under Assumption 2.1(ii), we have\nE0 [ i(St) · 1s(St) ] = dπo(s) i(s) = ( d > πo,i I)(s),\nand for k = 1, 2, . . . ,K,\nE0 [ i(St−k) · ( ρt−kγt−k+1 · · · ρt−1γt ) · (1− λt) · 1s(St) ] = ( d>πo,i(PπΓ) k(I − Λ) ) (s).\nHence\nE0 [ M̃t,K · 1s(St) ] = ( d>πo,i ( I + K∑ k=1 (PπΓ) k(I − Λ) )) (s),\nand consequently,\nE0 [ M̃t,K ·Ψ(St) ] = ∑ s∈S E0 [ M̃t,K · 1s(St) ] ·Ψ(s)\n= ∑ s∈S ( d>πo,i ( I + K∑ k=1 (PπΓ) k(I − Λ) )) (s) ·Ψ(s). (A.31)\nLet us now calculateG∗K for h = h1 or h2 simultaneously, using Eq. (A.30) and the expressions of h1, h2 given in Eq. (A.25). Let t > 2K, and let Ft = σ(S0, A0, . . . , St). For the term appearing after e in the expression of h1, we have\nE0 [ ρt ( γt+1φ(St+1) > − φ(St)> ) | Ft ] = Ψ1(St),\nwhere Ψ1 maps each s to the s-th row of the matrix (PπΓ− I)Φ. For the term appearing after e in the expression of h2, we have\nE0 [ ρt r(St, At, St+1) | Ft ] = Ψ2(St),\nwhere Ψ2 maps each s to the s-th entry of the vector rπ. Denote Ψo1 = (PπΓ− I)Φ,Ψo2 = rπ. Corresponding to h = h1 or h2, let Ψ = Ψ1 or Ψ2, and let Ψo = Ψo1 or Ψ o 2, respectively. Then, by Eqs. (A.30) and (A.25), we have\nG∗K = E0 [ E0[h(Yt,K , St, At, St+1) | Ft ] ]\n= E0 [ ẽt,K ·Ψ(St) ]\n= E0\n[ t∑\nk=t−K M̃k,K · φ(Sk) · (βk+1 · · ·βt) ·Ψ(St)\n] (A.32)\n= t∑ k=t−K E0 [ M̃k,K · φ(Sk) · E0 [ (βk+1 · · ·βt) ·Ψ(St) | Fk ] ]\n= t∑ k=t−K E0 [ M̃k,K · φ(Sk) · ( (PπΓΛ) t−kΨo ) r (Sk) ] . (A.33)\nIn the above we used the definition (A.15) of ẽt,K in Eq. (A.32), and in Eq. (A.33), the term (· · · )r(Sk) inside the expectation denotes the Sk-th row of the matrix or vector given by the expression inside the parentheses (· · · ). We shall also use this notational convention in the proof below.\nFrom Eq. (A.33), using the fact that the expectation is with respect to the stationary Markov chain {(St, At)}, we obtain\nG∗K = t∑ k=t−K E0 [ M̃t,K · φ(St) · ( (PπΓΛ) t−kΨo ) r (St) ]\n= E0 [ M̃t,K · φ(St) · ( K∑ k=0 (PπΓΛ) k ·Ψo ) r (St) ] . (A.34)\nCorresponding to the last two terms inside the expectation above, define a function ΨK on S by\nΨK(s) = φ(s) · ( K∑ k=0 (PπΓΛ) k ·Ψo ) r (s), s ∈ S.\nThen by combining Eq. (A.34) with (A.31), we have\nG∗K = E0 [ M̃t,K ·ΨK(St) ] = ∑ s∈S ( d>πo,i ( I + K∑ k=1 (PπΓ) k(I − Λ) )) (s) ·ΨK(s). (A.35)\nWe now take K to infinity to get the expression of G∗. For h = h1, Ψo in the definition of ΨK is given by Ψo = (PπΓ− I)Φ. Using the equality relations,\nI + ∞∑ k=1 (PπΓ) k(I − Λ) = (I − PπΓ)−1(I − PπΓΛ),\n∞∑ k=0 (PπΓΛ) k = (I − PπΓΛ)−1,\nwe obtain from the expression (A.35) of G∗K that\nG∗ = ∑ s∈S ( d>πo,i(I − PπΓ)−1(I − PπΓΛ) ) (s) · φ(s) · ( (I − PπΓΛ)−1 · (PπΓ− I)Φ ) r (s)\n= Φ>M̄ (I − PπΓΛ)−1 (PπΓ− I) Φ = C,\nwhere the second equality follows from the expression (A.29) for diag(M̄), and the third equality follows from Eq. (A.27). For h = h2, Ψo in the definition of ΨK is given by Ψo = rπ, and a similar calculation gives\nG∗ = ∑ s∈S ( d>πo,i(I − PπΓ)−1(I − PπΓΛ) ) (s) · φ(s) · ( (I − PπΓΛ)−1 · rπ ) r (s)\n= Φ>M̄ (I − PπΓΛ)−1 rπ = b,\nwhere the last equality follows from Eq. (A.28)."
    }, {
      "heading" : "A.6. Related Result: Alternative Proof of Existence of an Invariant Probability Measure",
      "text" : "Consider the Markov chain {Zt}, where Zt = (St, At, et, Ft). In Section 3, we used, among others, the weak Feller property of the Markov chain {Zt} to establish the existence of at least one invariant probability measure for {Zt} (the property (iv) in Section 3.1). We now give an alternative proof for this statement, by constructing directly an invariant probability measure. This proof is similar to that of (Yu, 2012, Lemma 4.2), and it was motivated by an analysis of the LSTD(1) algorithm by Meyn (2008, Chap. 11.5.2). The proof will also yield directly that under that invariant probability measure,\n∥∥(e0, F0)∥∥ has a finite expectation, which was established in Theorem 3.2(ii) earlier by using different arguments.\nProposition A.4 Under Assumption 2.1, the Markov chain {Zt} has at least one invariant probability measure ζ with Eζ [∥∥(e0, F0)∥∥] <∞. Proof Consider a double-ended stationary Markov chain {(St, At) | −∞ < t <∞} with transition probability matrix Pπo and probability distribution Po. In this proof, let E0 denote expectation with respect to Po. Let Xt = ( (St, At), (St−1, At−1), . . . ) , and denote by PX the probability distribution of Xt, which is a probability measure on (S × A)∞ and is the same for all t due to stationarity.\nWe will first define two functions, f : (S ×A)∞ → R+ and ψ : (S ×A)∞ → Rn, which relate to the traces F, e, respectively. We will then show that the distribution of ( S0, A0, ψ(X0), f(X0) ) is an invariant probability measure of {Zt}.\nLet us introduce some notation. For x ∈ (S × A)∞, we index the components of x as x = ( (s0, a0), (s−1, a−1), . . . ) , and we denote by x(−k) the tail of x starting from s−k, i.e.,\nx(−k) = ( (s−k, a−k), (s−k−1, a−k−1), . . . ) . Recall that βt = ρt−1γtλt. Correspondingly, we define a function β : S ×A× S → R+ by\nβ(s, a, s′) = ρ(s, a) γ(s′)λ(s′).\nFor an expression H that results in a vector in RN , we write (H)(s) for the s-th entry of that vector.\nWe now define the function f .18 Since\n∞∑ k=0 E0 [ i(S−k) · ( ρ−kγ−k+1 · · · ρ−1γ0 )] = ∞∑ k=0 E0 [ i(S−k) · ( (PπΓ) k1 ) (S−k) ] = d>πo,i\n∞∑ k=0 (PπΓ) k1 <∞, (A.36)\nwe can define a nonnegative real-valued measurable function f on (S ×A)∞ such that\nf(x) =\n{∑∞ k=0 i(s−k) · ( ρ(s−k, a−k)γ(s−k+1) · · · ρ(s−1, a−1)γ(s0) ) , if x ∈ D1,\n0, otherwise,\nwhere D1 is a subset of (S ×A)∞ with PX(D1) = 1. By Eq. (A.36),∫ f(x)PX(dx) = E0 [ f(X0)\n] = ∞∑ k=0 E0 [ i(S−k) · ( ρ−kγ−k+1 · · · ρ−1γ0 )] <∞.\nWe now define the function ψ. Define two constants L′, L as follows. Let L′ = E0 [ f(X0) ] ;\nequivalently, L′ = E0 [ f(X−k) ] for all k by stationarity. Let L ≥ max { i(s), ‖φ(s)‖ } for all s ∈ S. By taking conditional expectation similarly to the proof of Lemma A.2, we have the following bound:\n∞∑ k=0 E0 [∥∥∥(λ−k · i(S−k) + (1− λ−k)f(X−k)) · φ(S−k) · (β−k+1 · · ·β0)∥∥∥]\n= ∞∑ k=0 E0 [( λ−k · i(S−k) + (1− λ−k)f(X−k) ) · ∥∥φ(S−k)∥∥ · (β−k+1 · · ·β0)]\n≤ (L+ L′) · L · ∞∑ k=0 1>(PπΓΛ) k1 <∞.\nTherefore, by a theorem on integration (Rudin, 1966, Theorem 1.38, p. 28-29), we can define a measurable function ψ on (S ×A)∞ such that the following hold:\n(i) on a set D2 ⊂ (S ×A)∞ with PX(D2) = 1,\nψ(x) = ∞∑ k=0 ( λ(s−k) i(s−k) + (1− λ(s−k)) f(x(−k)) ) · φ(s−k)\n· ( β(s−k, a−k, s−k+1) · · ·β(s−1, a−1, s0) ) ,\nwhere the infinite series on the right-hand side converges to a vector in Rn; (ii) outside D2, ψ(x) = 0; and\n18. We note that to gain intuition about the proof, it will be helpful to compare our definition of f with the expression of Ft in Eq. (A.7), and compare our subsequent definition of ψ with the expression of et in Eq. (A.8).\n(iii) ψ(x) is integrable with\nE0 [ ‖ψ(X0)‖ ] = ∫ ‖ψ(x)‖PX(dx) <∞\nand∫ ψ(x)PX(dx) = ∞∑ k=0 E0 [( λ−ki(S−k) + (1− λ−k)f(X−k) ) · φ(S−k) · ( β−k+1 · · ·β0 )] .\nLet Y o0 = (ψ(X0), f(X0)). We now show that the probability distribution of (S0, A0, Y o 0 ) is an invariant probability measure of the Markov chain {Zt}. To this end, considerX1 = ( (S1, A1), X0 ) , and let us define Y o1 = (e o 1, F o 1 ) based on Y o 0 and (S0, A0, S1), using the same recursion that defines (et, Ft) [cf. Eqs. (2.2)-(2.4)]:\nF o1 = γ1 ρ0 · f(X0) + i(S1), eo1 = β1 ψ(X0) + ( λ1 i(S1) + (1− λ1)F o1 ) · φ(S1).\nIf (S0, A0, Y o0 ) and (S1, A1, Y o 1 ) have the same distribution, then this distribution must be an invariant probability measure of {Zt} because the stochastic kernel that governs the transition from (S0, A0, Y o0 ) to (S1, A1, Y o 1 ) is the same as that from Z0 = ( S0, A0, (e0, F0) ) to Z1 =(\nS1, A1, (e1, F1) ) .\nNow define a set D ⊂ (S ×A)∞ by D = D1 ∩D2 ∩ ( S ×A× (D1 ∩D2) ) , where D1, D2 are the sets in the definitions of the functions f and ψ, respectively. Since PX(D1) = PX(D2) = 1, we have\nPX(D) = P o(X1 ∈ D) = Po(X1 ∈ D1 ∩D2, X0 ∈ D1 ∩D2) = 1.\nConsider the case X1 ∈ D. Then both X0, X1 ∈ D1 ∩D2. By the definition of f on D1, it follows that\nF o1 = i(S1) + ∞∑ k=0 i(S−k) · ( ρ−kγ−k+1 · · · ρ−1γ0 ) · ρ0γ1 = f(X1),\nand from this and the definition of ψ on D2, it also follows that eo1 = ( λ1 i(S1) + (1− λ1) f(X1) ) · φ(S1)\n+ ∞∑ k=0 ( λ−k · i(S−k) + (1− λ−k)f(X−k) ) · φ(S−k) · ( β−k+1 · · ·β0 ) · β1\n= ψ(X1). By stationarity, ( S0, A0, ψ(X0), f(X0) ) and (S1, A1, ψ(X1), f(X1) ) have the same distribution. Denote this distribution by ζ. Since (eo1, F o 1 ) differs from ( ψ(X1), f(X1) ) only when X1 6∈ D, an event with Po-probability 0, we conclude that (S0, A0, Y o0 ) and (S1, A1, Y o\n1 ) have the same distribution ζ, which is an invariant probability measure of {Zt} as discussed earlier. Then, from the integrability property of ψ and f shown earlier, we have\nEζ [∥∥(e0, F0)∥∥] ≤ Eζ[∥∥e0∥∥]+ Eζ[F0] = E0[∥∥ψ(X0)∥∥]+ E0[f(X0)] <∞.\nThis completes the proof."
    }, {
      "heading" : "Appendix B. Proofs for Section 4",
      "text" : "In this appendix we prove Lemma 4.1 and Theorem 4.1 for the constrained ETD(λ) algorithm (4.5). We will restate both theorems for convenience.\nRecall that the constrained ETD(λ) calculates θt, t ≥ 0, all restricted to be in a closed ball with radius r, B = {θ ∈ Rn | ‖θ‖2 ≤ r}, according to\nθt+1 = ΠB ( θt + αt h(θt, ξt) + αt et · ω̃t+1 ) ,\nwhere ω̃t+1 = ρt ( Rt−r(St, At, St+1) ) is noise, ξt = (et, St, At, St+1), and the function h is given by Eq. (4.3) as\nh(θ, ξ) = e · ρ(s, a) ( r(s, a, s′) + γ(s′)φ(s′)>θ − φ(s)>θ ) , for ξ = (e, s, a, s′).\nThe “mean ODE” associated with this algorithm is the projected ODE (4.6):\nẋ = h̄(x) + z, z ∈ −NB(x),\nwhere h̄(x) = Cx + b, NB(x) is the normal cone of B at x, and z is the boundary reflection term that keeps the solution in B (Kushner and Yin, 2003). The solution of h̄(x) = 0 is denoted θ∗; i.e., θ∗ = −C−1b.\nLemma 4.1 Let c > 0 be such that x>Cx ≤ −c‖x‖22 for all x ∈ Rn. Suppose B has a radius r > ‖b‖2/c. Then θ∗ lies in the interior of B, and the only solution x(t), t ∈ (−∞,+∞), of the projected ODE (4.6) in B is x(·) ≡ θ∗.\nProof By the definition of θ∗, Cθ∗ + b = 0. Therefore,\n0 = 〈θ∗, Cθ∗ + b〉 = 〈θ∗, Cθ∗〉+ 〈θ∗, b〉 ≤ −c‖θ∗‖22 + ‖b‖2‖θ∗‖2,\nwhich implies ‖θ∗‖2 ≤ b‖2/c < r, i.e., θ∗ lies in the interior of B. For a point x on the boundary of B, ‖x‖2 = r and the normal cone NB(x) = {ax | a ≥ 0}. Since r > ‖b‖2/c, we have\n〈x, h̄(x)〉 = 〈x,Cx〉+ 〈x, b〉 ≤ −c‖x‖22 + ‖x‖2‖b‖2 = r (−c r + ‖b‖2) < 0.\nThis shows that for any x on the boundary of B, h̄(x) points inside B and hence at x, the boundary reflection term z ∈ −NB(x) that keeps the solution in B is the zero vector. Consequently, any solution of the projected ODE (4.6) in B is a solution of the ODE (4.4), which is x(·) ≡ θ∗.\nNext we prove Theorem 4.1.\nTheorem 4.1 (Almost sure convergence of constrained ETD(λ)) Let Assumptions 2.1-2.3 hold. Let {θt} be the sequence generated by the constrained ETD(λ) algorithm (4.5) with stepsizes satisfying αt = O(1/t) and\nαt−αt+1 αt = O(1/t), and with the radius r of B exceeding the threshold\ngiven in Lemma 4.1. Then, for any given initial (e0, F0, θ0), θt a.s.→ θ∗.\nProof The desired conclusions will follow immediately from (Kushner and Yin, 2003, Theorem 6.1.1) and Lemma 4.1, if we can show that the conditions of (Kushner and Yin, 2003, Theorem 6.1.1) are met. Relevant here are the conditions A.6.1.1-A.6.1.4 and A.6.1.6-A.6.1.7 in (Kushner and Yin, 2003, p. 165). We first adapt these six conditions to our problem, and by using stronger forms of the conditions A.6.1.6-A.6.1.7 given in (Kushner and Yin, 2003, Eq. (6.1.10), p. 166), we obtain the conditions (i)-(vi) below.\nThe first two conditions are for the functions h, h̄ [cf. Eqs. (4.3), (4.4)] and the noise {ω̃t}: (i) supt≥0 E [ ‖h(θt, ξt) + et · ω̃t+1‖ ] <∞.\n(ii) h̄(θ) is continuous, and h(θ, ξ) is continuous in θ for each ξ. Condition (i) is satisfied here. Indeed, we have supt≥0 E [ ‖h(θt, ξt)‖ ] < ∞, in view of Prop. A.1, the Lipschitz continuity of h in e, and the fact that ‖θt‖2 ≤ r for all t by the definition of the constrained algorithm. Since the rewards Rt have bounded variances by assumption and the noise variable ω̃t+1 = ρt ( Rt − r(St, At, St+1) ) by definition, we can bound E [ |ω̃t+1| | Ft ] by some\nconstant for all t, where Ft = σ(S0, A0, . . . , St+1), and consequently, we also have supt≥0 E [ ‖et ·\nω̃t+1‖ ] <∞ by Prop. A.1. Hence condition (i) holds. Condition (ii) is also clearly satisfied here.\nThe four remaining conditions to be introduced are of the same type and relate to the asymptotic rate of change conditions introduced by (Kushner and Clark, 1978). These conditions can guarantee that the effects caused by the noises ω̃t+1 or by the discrepancies between h and h̄ asymptotically “average out” so that the desired convergence can take place.\nFor any real T ′ > 0, define integerm(T ′) = min{t ≥ 0 | ∑t\nk=0 αk > T ′}. Conditions (iii)-(vi)\nbelow are required to hold for each a ≥ 0 and some T > 0 (here a and T are real numbers): (iii) For each θ,\nlim t→∞ P supj≥t max0≤T ′≤T ∥∥∥∥∥∥ m(jT+T ′)−1∑ k=m(jT ) αk ( h(θ, ξk)− h̄(θ) )∥∥∥∥∥∥ ≥ a  = 0. (B.1)\n(iv)\nlim t→∞ P supj≥t max0≤T ′≤T ∥∥∥∥∥∥ m(jT+T ′)−1∑ k=m(jT ) αk ek · ω̃k+1 ∥∥∥∥∥∥ ≥ a  = 0. (B.2)\n(v) There exist nonnegative measurable functions g1(θ), g2(ξ) such that\n‖h(θ, ξ)‖ ≤ g1(θ) g2(ξ),\nwhere g1 is bounded on each bounded set of θ, and g2 satisfies that supt≥0 E [ g2(ξt) ] < ∞ and\nlim t→∞ P supj≥t max0≤T ′≤T ∣∣∣∣∣∣ m(jT+T ′)−1∑ k=m(jT ) αk ( g2(ξk)− E [ g2(ξk) ])∣∣∣∣∣∣ ≥ a  = 0. (B.3)\n(vi) There exist nonnegative measurable functions g3(θ), g4(ξ) such that for each θ, θ′,\n‖h(θ, ξ)− h(θ′, ξ)‖ ≤ g3(θ − θ′) g4(ξ),\nwhere g3 is bounded on each bounded set of θ, with g3(θ)→ 0 as θ → 0, and g4 satisfies that supt≥0 E [ g4(ξt) ] <∞ and\nlim t→∞ P supj≥t max0≤T ′≤T ∣∣∣∣∣∣ m(jT+T ′)−1∑ k=m(jT ) αk ( g4(ξk)− E [ g4(ξk) ])∣∣∣∣∣∣ ≥ a  = 0. (B.4)\nOne method given in (Kushner and Yin, 2003, Chap. 6.2, p. 170-171) of verifying the conditions (B.1)-(B.4) above is to show that a strong law of large numbers hold for the processes involved. In particular, let ψk represent h(θ, ξk)− h̄(θ) for condition (iii), ek · ω̃k+1 for condition (iv), g2(ξk)− E [ g2(ξk) ] for condition (v), and g4(ξk)− E [ g4(ξk) ] for condition (vi). If\n1\nt+ 1 t∑ k=0 ψk a.s.→ 0 (B.5)\nfor the respective {ψk}, then the conditions (B.1)-(B.4) hold for stepsizes satisfying αt = O(1/t) and αt−αt+1αt = O(1/t) (see Kushner and Yin, 2003, Example 6.1, p. 171).\nWe now apply the convergence results given earlier in this paper to show that the desired convergence (B.5) holds for the processes involved in conditions (iii)-(vi). In particular, for each fixed θ, the almost sure convergence part of Theorem 2.1 implies that\n1\nt+ 1 t∑ k=0 h(θ, ξk) a.s.→ Eζ [ h(θ, ξ0) ] = h̄(θ).\nThus, condition (iii) holds, as just discussed. By Prop. 3.1(ii), 1t+1 ∑t k=0 ek · ω̃k+1 a.s.→ 0, so condition (iv) is also met. We verify now conditions (v)-(vi). For condition (v), we take g1(θ) = ‖θ‖ + 1, and we bound the function h by\n‖h(θ, ξ)‖ ≤ ( ‖θ‖+ 1 ) g2(ξ), where g2(ξ) = L‖e‖,\nand L > 0 is some constant. (This bound can be verified directly using the expression of h and the fact that the sets S and A are finite.) Similarly, for condition (vi), we take g3(θ) = ‖θ‖, and we bound the change in h(θ, ξ) in terms of the change in θ as follows: for any θ, θ′ ∈ Rn,∥∥h(θ, ξ)− h(θ′, ξ)∥∥ ≤ ‖θ − θ′‖ g4(ξ), where g4(ξ) = L′‖e‖, and L′ > 0 is some constant. Now the functions g2, g4 are Lipschitz continuous in e. Hence, for j = 2, 4, it follows from Prop. A.1 that supt≥0 E [ gj(ξt) ] < ∞, and it follows from Theorems 3.3 and 3.1 that\n1\nt+ 1 t∑ k=0 gj(ξk) a.s.→ Eζ [ gj(ξ0) ] , and\n1\nt+ 1 t∑ k=0 E [ gj(ξk) ] → Eζ [ gj(ξ0) ] , as t→∞.\nThe preceding two relations imply the desired convergence:\n1\nt+ 1 t∑ k=0 ( gj(ξk)− E [ gj(ξk) ]) a.s.→ 0, j = 2, 4.\nThis shows that conditions (v)-(vi) are met. The theorem now follows by combining (Kushner and Yin, 2003, Theorem 6.1.1) with the characterization of the solution of the projected ODE (4.6) given by Lemma 4.1, using the fact that under Assumptions 2.1 and 2.2, the matrix C is negative definite (Prop. C.1)."
    }, {
      "heading" : "Appendix C. Negative Definiteness of the Matrix C",
      "text" : "In this appendix we prove a necessary and sufficient condition (Prop. C.2 below) for the matrix C associated with ETD(λ) to be negative definite. Recall from Eqs. (2.8)-(2.9) that\nC = −Φ> M̄(I − P λπ,γ) Φ\nwhere Φ is the feature matrix with full column rank, P λπ,γ is a substochatic matrix, and M̄ is a nonnegative diagonal matrix with its diagonal, diag(M̄), given by\ndiag(M̄) = d>πo,i(I − P λπ,γ)−1, d>πo,i = ( dπo(1) i(1), . . . , dπo(N) i(N) ) .\nHere Assumption 2.1 is in force and ensures that (I − P λπ,γ)−1 exists and dπo(s) > 0 for all s ∈ S. The negative definiteness of C is important for the a.s. convergence of ETD(λ). It is known to hold if i(s) > 0 for all s ∈ S (Sutton et al., 2015). In general, C is always negative semidefinite for nonnegative i(·), and thus C is negative definite whenever it is nonsingular.\nIn what follows, we first include a proof of the fact just mentioned, for completeness (see Prop. C.1). We then give explicitly a condition on the approximation subspace which we will prove to be equivalent to the nonsingularity/negative definiteness of C (Prop. C.2). We also show, by specializing this subspace condition, that if those states s of interest (i.e., i(s) > 0) are represented by features φ(s) that are rich enough, then C can be made negative definite, without knowledge of the model (See Cor. C.1, Remark C.2). In addition, we discuss the connection of this subspace condition to seminorm projections, and show that when C is nonsingular, the ETD(λ) solution can be viewed as the solution of a projected Bellman equation involving a seminorm projection (see Remark C.1)."
    }, {
      "heading" : "C.1. Preliminaries",
      "text" : "First, recall that the matrix C is said to be negative definite if there exists c > 0 such that\ny>Cy ≤ −c ‖y‖22, ∀ y ∈ Rn,\nand negative semidefinite if c = 0 in the preceding inequality. The negative definiteness of C is equivalent to that of the symmetric matrix\nC + C> = −Φ> ( M̄(I − P λπ,γ) + (I − P λπ,γ)>M̄ ) Φ.\nSimilarly to (Sutton, 1988; Sutton et al., 2015), our analysis will focus on the N × N symmetric matrix\nG = M̄(I −Q) + (I −Q)>M̄\nfor the substochastic matrix Q = P λπ,γ and the nonnegative diagonal matrix M̄ as given above. We will use a theorem from (Varga, 2000, Cor. 1.22, p. 23), according to which a symmetric real matrix\nwith positive diagonal entries is positive definite if it is strictly diagonally dominant or irreducibly diagonally dominant. Note that by definition, G is irreducibly diagonally dominant if G is irreducible19 and satisfies the following diagonally dominant conditions for every row of G, with strict inequality holding for at least one row:\n|Gss| ≥ ∑ s̄ 6=s |Gss̄|, s = 1, . . . , N,\nwhereas G is strictly diagonally dominant if it satisfies the above inequalities strictly for all rows. We now give a proof of the fact about the relation between the nonsingularity and the negative definiteness of C mentioned at the beginning. This result is due to (Sutton et al., 2015). Regarding notation, in the proofs below, for v ∈ RN , we write v(s) for the s-th entry of v, and for an expression H that results in a vector in RN , we write (H)(s) for the s-th entry of that vector. For an expression H that results in an N ×N matrix, we write [H]ss̄ for its (s, s̄)-th element. We write 0 for a zero vector in any Euclidean space.\nProposition C.1 Let Assumption 2.1 hold. Then, C is negative definite if C is nonsingular.\nProof We show first that if i(s) > 0 for all s ∈ S, then G is strictly diagonally dominant, and hence positive definite; and that if i(s) ≥ 0 for all s ∈ S, then G is positive semidefinite.\nLet J = {s ∈ S | i(s) = 0}. Suppose J = ∅. By definition M̄ss = ( d>πo,i(I − Q)−1 ) (s). Using this together with the fact that Q is substochastic, by a direct calculation as in (Sutton et al., 2015), we have that for each s ∈ S,\nGss − ∑ s̄ 6=s |Gss̄| = M̄ss ·\n( 1−\nN∑ s̄=1 Qss̄\n) +\nN∑ s̄=1 M̄s̄s̄ · [ I −Q ] s̄s\n(C.1)\n≥ 0 + ( d>πo,i(I −Q)−1 · (I −Q) ) (s)\n= 0 + dπo,i(s) (C.2)\n> 0,\nwhere in the last strict inequality, we used the fact that i(s) > 0 implies dπo,i(s) > 0 under Assumption 2.1(ii). This shows thatG is strictly diagonally dominant with positive diagonal entries, and hence positive definite by (Varga, 2000, Cor. 1.22).\nConsider now the case J 6= ∅. For all s ∈ J , perturb i(s) to δ > 0, and denote by Gδ the matrix G corresponding to the perturbed i(·). Then Gδ is positive definite by the preceding proof. So for the original G, by continuity, G = limδ→0Gδ is positive semidefinite. It then follows that the matrix Φ>GΦ = −C −C> is positive semidefinite. Hence C is negative semidefinite; but C is nonsingular by assumption, so C must be negative definite."
    }, {
      "heading" : "C.2. Main Result",
      "text" : "We now give the main result of this section. It expresses the nonsingularity condition onC explicitly in terms of a condition on the approximation subspace E (the column space of Φ).\n19. A symmetric matrix G is irreducible if it corresponds to a connected (undirected) graph when the indices are viewed as the nodes of the graph, and the nonzero entries of G are viewed as edges of the graph.\nProposition C.2 Let Assumption 2.1 hold, and let J0 = {s ∈ S | M̄ss = 0}. Suppose the approximation subspace E ⊂ RN is such that\nv ∈ E and v(s) = 0, ∀ s 6∈ J0 =⇒ v = 0. (C.3)\nThen the matrix C is negative definite. Furthermore, C is nonsingular if and only if the condition (C.3) holds.\nThe corollary below gives a sufficient condition (C.4) for C being negative definite, which can be fulfilled without knowledge of the model, as we will elaborate in Remark C.2. This corollary is a direct consequence of the preceding proposition, and follows from the observation that since i(s) > 0 implies M̄ss > 0, the condition (C.4) implies the condition (C.3) in Prop. C.2.\nCorollary C.1 Let Assumption 2.1 hold, and let J = {s ∈ S | i(s) = 0}. Suppose the approximation subspace E ⊂ RN is such that\nv ∈ E and v(s) = 0, ∀ s 6∈ J =⇒ v = 0. (C.4)\nThen the matrix C is negative definite.\nWe now proceed to prove Prop. C.2. Roughly speaking, the method of proof is to decompose the matrixG into irreducible diagonal blocks and use, among others, the theorem (Varga, 2000, Cor. 1.22, p. 23) on irreducibly diagonally dominant matrices mentioned earlier.\nIn the two technical lemmas that follow, we let the matrixG and the nonnegative diagonal matrix M̄ take a slightly more general form:\nG = M̄(I −Q) + ( M̄(I −Q) )> , diag(M̄) = d>πo,i (I −Q)−1,\nwhere Q is a substochastic matrix (not necessarily P λπ,γ), and dπo,i is a nonnegative vector (for notational simplicity, we keep using dπo,i instead of introducing new notation).\nLemma C.1 Suppose the matrix (I − Q) is invertible. Then the s-th diagonal entry M̄ss = 0 if and only if the s-th row and s-th column of G contain all zeros. Proof We have G = M̄(I −Q) + ( M̄(I −Q)\n)>. Suppose s is a state with M̄ss 6= 0. Then the s-th row of the matrix M(I −Q) is nonzero (because the s-th row of I −Q is nonzero, given that (I − Q)−1 exists). The nonzero entries of this row cannot be canceled out by the corresponding entries from the s-th row of ( M̄(I − Q)\n)>, because Q is a substochastic matrix and M̄ is nonnegative. Therefore, the s-th row of G must also be nonzero. This proves the “if” part.\nFor the “only if” part, suppose s is a state with M̄ss = 0. Then the s-th row of the matrix M̄(I −Q) contains all zeros, so, since G = M̄(I −Q) + ( M̄(I −Q) )> and is symmetric, to prove the “only if” part, we only need to show that the s-th column of M̄(I − Q) is a zero column. We prove this by contradiction.\nSuppose for some state s̄ 6= s, the (s̄, s)-entry of the matrix M̄(I −Q) is nonzero. Then using the definition of M̄s̄s̄, this entry can be expressed as\nMs̄s̄ · [ I −Q ] s̄s = − ( d>πo,i (I −Q)−1 ) (s̄) ·Qs̄s 6= 0,\nwhich, in view of the equality (I −Q)−1 = ∑\nk≥0Q k and the nonnegativity of Q, implies that(\nd>πo,iQ k ) (s̄) ·Qs̄s > 0 for some k ≥ 0.\nThis in turn implies that for the state s,( d>πo,iQ k ) (s) > 0 for some k ≥ 0,\nand hence M̄ss = ( d>πo,i (I −Q)−1 ) (s) ≥ ( d>πo,iQ k ) (s) > 0,\ncontradicting the assumption M̄ss = 0. Thus the s-th column of M̄(I−Q) must be a zero column.\nLemma C.2 Suppose that the matrix (I − Q) is invertible and the matrix G is irreducible. Then the diagonal entries of M̄ must be positive, and G is irreducibly diagonally dominant with positive diagonal entries, and hence positive definite.\nProof If s is a state with M̄ss = 0, by Lemma C.1, the s-th row and s-th column ofG would contain all zeros, which cannot happen if G is irreducible. Thus M̄ss > 0 for all s ∈ S.\nWe have calculated in the proof of Prop. C.1 [cf. Eqs. (C.1)-(C.2)] that for nonnegative i(·),\nGss − ∑ s̄ 6=s |Gss̄| = M̄ss ·\n( 1−\nN∑ s̄=1 Qss̄\n) +\nN∑ s̄=1 M̄s̄s̄ · [ I −Q ] s̄s ≥ 0\nfor all rows s. The strict inequality Gss − ∑\ns̄ 6=s |Gss̄| > 0 must hold for some s. To see this, note that the invertibility of (I − Q) implies that 1 − ∑N s̄=1Qss̄ > 0 for some s, which together\nwith M̄ss > 0 implies that the first term in the right-hand side above, M̄ss · ( 1− ∑N s̄=1Qss̄ ) , must be positive for at least one row s, whereas the second term in the right-hand side above equals dπo,i(s) ≥ 0 [cf. Eqs. (C.1)-(C.2)]. Since G is irreducible by assumption, this proves that G is irreducibly diagonally dominant.\nFinally, since Q is substochastic and (I −Q)−1 exists, the diagonals of I −Q must be positive. The diagonals of M̄ are also positive, as proved earlier. Thus the diagonal entries Gss > 0 for all rows s. It then follows from (Varga, 2000, Cor. 1.22) that G is positive definite.\nWe are now ready to prove Prop. C.2. Regarding notation, in the proof, if G1, G2, . . . , GL are L square matrices (which can have different sizes), we will write diag ( G1, G2, . . . , GL ) for the block-diagonal matrix that has Gk as its k-th diagonal block. However, for a single square matrix G1, we will keep using diag(G1) to mean the diagonal of G1.\nProof of Prop. C.2 By Assumption 2.1(i), (I − PπΓ)−1 exists, which implies that for the substochastic matrix Q = P λπ,γ [cf. Eq. (2.6)], (I −Q)−1 also exists. So the matrices M̄ , C and G are well defined. By reordering the states if necessary, we can arrange G into a block-diagonal matrix with L blocks,\nG = diag ( G(1), . . . , G(L−1), G(L) ) (C.5)\nsuch that:\n(i) for each ` = 1, . . . , L− 1, the `th-block G(`) is irreducible; and (ii) the L-th block G(L) is a zero matrix (if G does not have a zero block, we will treat G(L) as a\nmatrix of size zero, and this will not affect the proof below). Note that by Lemma C.1, the row/column indices associated with the zero block G(L) are exactly those in the set\nJ0 = {s ∈ S | M̄ss = 0}.\nSince the condition (C.3) rules out the case J0 = S, G cannot be a zero matrix, so it must have at least one irreducible block.\nWe prove next that the matrix Q has the following structure, matching the block-diagonal structure of G:\nQ =  Q(1) Q(2) HH Q(L−1)\n∗ ∗ · · · ∗ ∗  (C.6) where the blocks Q(`), ` ≤ L − 1, on the diagonal correspond to the blocks G(`), ` ≤ L − 1, on the diagonal of G, the unmarked blocks contain all zeros, and the ∗-blocks can have both zeros and positive entries.\nTo prove Eq. (C.6) by contradiction, suppose it does not hold. This means that there must exist two states s 6= s̄ with Qss̄ > 0, but the entry Qss̄ lies inside an unmarked block of the matrix on the right-hand side of Eq. (C.6). This position of Qss̄ implies Gss̄ = 0, which is possible only if M̄ss = 0 (otherwise, Qss̄ 6= 0 would force Gss̄ 6= 0). But if M̄ss = 0, s ∈ J0, which is the set of indices associated with the last zero block, as shown earlier. Consequently, the entry Qss̄ cannot lie inside an unmarked block as we assumed. This contradiction proves that Eq. (C.6) must hold.\nFrom the structure of Q shown in (C.6), it follows that (I −Q)−1 has the same structure:\n( I −Q )−1 =  ( I −Q(1) )−1 ( I −Q(2) )−1 HH ( I −Q(L−1) )−1\n∗ ∗ · · · ∗ ∗\n . (C.7)\nSince G = M̄(I − Q) + (I − Q)>M̄ , Eqs. (C.5), (C.6) and (C.7) together imply that for each ` ≤ L− 1, the matrix G(`) can be expressed as\nG(`) = M̄ (`) ( I −Q(`) ) + ( I −Q(`) )> M̄ (`),\nwhere M̄ (`) is the `-th diagonal block in the corresponding decomposition of M̄ as\nM̄ = diag ( M̄ (1), . . . , M̄ (L) ) ,\nand if we decompose the vector dπo,i similarly as dπo,i = ( d (1) πo,i, . . . , d (L) πo,i ) , then for each ` ≤ L−1, the diagonal block M̄ (`) has its diagonal entries given by\ndiag ( M̄ (`) ) = ( d\n(`) πo,i\n)>( I −Q(`) )−1 , ` ≤ L− 1.\nIn the above expression, we also used the fact d(L)πo,i = 0, which is implied by M̄ (L) being a zero matrix (which we showed at the beginning of this proof).20\nWe now apply Lemma C.2 to each irreducible block G(`), ` ≤ L − 1 (with M̄ = M̄ (`) and Q = Q(`), a substochastic matrix). This yields that each of these G(`) is positive definite, and consequently, the block-diagonal matrix\nĜ = diag ( G(1), . . . , G(L−1) ) is positive definite.\nFinally, we prove the statement of the proposition. For the block-diagonal decomposition of G as G = diag(Ĝ,G(L)), write a point y ∈ RN correspondingly as y = (y1, y0). I.e., the indices of the components of y0 are those in J0 = {s ∈ S | M̄ss = 0}, and the dimension of y1 is N̂ = N − |J0|.\nSince Ĝ is positive definite, there exists some c > 0 such that\ny1 >Ĝ y1 ≥ c ‖y1‖22, ∀ y1 ∈ RN̂ . (C.8)\nConsider a point y = (y1, y0) ∈ E with y1 = 0. Then y0 = 0 by the assumption (C.3). Since E is a subspace, this implies that there exists some constant δ > 0 such that\ninf y∈E, ‖y‖2=1\n‖y1‖2 ≥ δ. (C.9)\nUsing Eqs. (C.8)-(C.9), we have\ninf y∈E, ‖y‖2=1 y>Gy = inf y∈E, ‖y‖2=1 y1 >Ĝ y1 ≥ inf y∈E, ‖y‖2=1 c ‖y1‖22 ≥ c δ2 > 0. (C.10)\nSinceE is the column space of Φ and Φ has linearly independent columns by definition, the inequality (C.10) establishes that the matrix Φ>GΦ = −C − C> is positive definite, and consequently, C is negative definite.\nThe preceding proof also shows that C is nonsingular if the condition (C.3) holds. To complete the proof, let us assume that the condition (C.3) does not hold and show that C must be singular. Let y = (y1, y0) ∈ E be such that y1 = 0 and y0 6= 0. Then since G(L) is a zero block, y>Gy = 0, which implies that the matrix Φ>GΦ = −C − C> is singular. If C were nonsingular, then by Prop. C.1, −C − C> would be positive definite and hence nonsingular, a contradiction. Therefore, C must be singular.\nFinally, we make two remarks on the conditions (C.3) and (C.4) in Prop. C.2 and Cor. C.1.\nRemark C.1 (Seminorm projection) Using seminorm projections to formulate the projected Bellman equations associated with TD methods is introduced in (Yu and Bertsekas, 2012). There, conditions of the form (C.3) or (C.4) are used to define a projection on the approximation subspace with respect to a seminorm. We can use this formulation here to interpret the solution of ETD(λ) 20. Using the expression (I − Q)−1 = ∑ k≥0Q\nk, it can be seen from the definition of M̄ss that M̄ss ≥ dπo,i(s). Therefore, M̄ss = 0 implies that dπo,i(s) = 0.\nand ELSTD(λ). Specifically, define a weighted Euclidean seminorm ‖ · ‖M̄ on RN , using diag(M̄) as the weights, as\n‖v‖2M̄ = ∑ s∈S M̄ss · v(s)2.\nCondition (C.3) ensures that the projection ΠM̄ onto E with respect to the seminorm ‖ · ‖M̄ is well-defined and has the matrix representation\nΠM̄ = Φ ( Φ>M̄Φ )−1 Φ>M̄\n(cf. Yu and Bertsekas, 2012, Sec. 2.1). So by Prop. C.2 and the convergence results of this paper, when C is nonsingular, ETD(λ) and ELSTD(λ) solve in the limit the projected Bellman equation\nv = ΠM̄ ( rλπ,γ + P λ π,γ v ) .\nThe relation between the solution v = Φθ∗ of this equation and the desired value function vπ, in particular, the approximation error, can be analyzed then, using the oblique projection viewpoint (Scherrer, 2010) (for details, see also (Yu and Bertsekas, 2012)).\nRemark C.2 (Equivalent conditions in terms of features) The condition (C.3) can be paraphrased in terms of the features φ(s) as follows:\n∀ s ∈ S with M̄ss = 0, φ(s) ∈ span { φ(s̄) ∣∣ s̄ ∈ S and M̄s̄s̄ > 0}; (C.11) namely, from those states with positive emphasis weights M̄s̄s̄ > 0, n linearly independent feature vectors can be found. Similarly, the condition (C.4) can be paraphrased as:\n∀ s ∈ S with i(s) = 0, φ(s) ∈ span { φ(s̄) ∣∣ s̄ ∈ S and i(s̄) > 0}; (C.12) namely, from the states with positive interest weights, n linearly independent feature vectors can be found. This shows that even without knowing Pπ and M̄ , by designing a rich enough set of features for states of interest beforehand, we can ensure the sufficient condition (C.4) for the nonsingularity and negative definiteness of the matrix C.\nConditions like (C.11), (C.12) [or equivalently, (C.3), (C.4)] are naturally satisfied in the case where the approximate values of the policy π at certain states (e.g., those states s with M̄ss = 0 or i(s) = 0) are interpolated or extrapolated from the approximate values of π at some other “representative” states, based on the “proximity” of the former states to the representative ones."
    } ],
    "references" : [ {
      "title" : "Projected equation methods for approximate solution of large linear systems",
      "author" : [ "D.P. Bertsekas", "H. Yu" ],
      "venue" : "Journal of Computational and Applied Mathematics,",
      "citeRegEx" : "Bertsekas and Yu,? \\Q2009\\E",
      "shortCiteRegEx" : "Bertsekas and Yu",
      "year" : 2009
    }, {
      "title" : "Convergence of Probability Measures",
      "author" : [ "P. Billingsley" ],
      "venue" : null,
      "citeRegEx" : "Billingsley,? \\Q1968\\E",
      "shortCiteRegEx" : "Billingsley",
      "year" : 1968
    }, {
      "title" : "Stochastic Approximation: A Dynamic Viewpoint",
      "author" : [ "V.S. Borkar" ],
      "venue" : null,
      "citeRegEx" : "Borkar,? \\Q2008\\E",
      "shortCiteRegEx" : "Borkar",
      "year" : 2008
    }, {
      "title" : "The O.D.E. method for convergence of stochastic approximation and reinforcement learning",
      "author" : [ "V.S. Borkar", "S.P. Meyn" ],
      "venue" : "SIAM J. Control Optim.,",
      "citeRegEx" : "Borkar and Meyn,? \\Q2000\\E",
      "shortCiteRegEx" : "Borkar and Meyn",
      "year" : 2000
    }, {
      "title" : "Least-squares temporal difference learning",
      "author" : [ "J.A. Boyan" ],
      "venue" : "In Proc. The 16th Int. Conf. Machine Learning,",
      "citeRegEx" : "Boyan,? \\Q1999\\E",
      "shortCiteRegEx" : "Boyan",
      "year" : 1999
    }, {
      "title" : "Policy evaluation with temporal differences: A survey and comparison",
      "author" : [ "C. Dann", "G. Neumann", "J. Peters" ],
      "venue" : "Journal of Machine Learning Res.,",
      "citeRegEx" : "Dann et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dann et al\\.",
      "year" : 2014
    }, {
      "title" : "Stochastic Processes",
      "author" : [ "J.L. Doob" ],
      "venue" : null,
      "citeRegEx" : "Doob,? \\Q1953\\E",
      "shortCiteRegEx" : "Doob",
      "year" : 1953
    }, {
      "title" : "Real Analysis and Probability",
      "author" : [ "R.M. Dudley" ],
      "venue" : null,
      "citeRegEx" : "Dudley,? \\Q2002\\E",
      "shortCiteRegEx" : "Dudley",
      "year" : 2002
    }, {
      "title" : "Off-policy learning with eligibility traces: A survey",
      "author" : [ "M. Geist", "B. Scherrer" ],
      "venue" : "Journal of Machine Learning Res.,",
      "citeRegEx" : "Geist and Scherrer,? \\Q2014\\E",
      "shortCiteRegEx" : "Geist and Scherrer",
      "year" : 2014
    }, {
      "title" : "Importance sampling for stochastic simulations",
      "author" : [ "P.W. Glynn", "D.L. Iglehart" ],
      "venue" : "Management Science,",
      "citeRegEx" : "Glynn and Iglehart,? \\Q1989\\E",
      "shortCiteRegEx" : "Glynn and Iglehart",
      "year" : 1989
    }, {
      "title" : "Stochastic Approximation Methods for Constrained and Unconstrained Systems",
      "author" : [ "H.J. Kushner", "D.S. Clark" ],
      "venue" : null,
      "citeRegEx" : "Kushner and Clark,? \\Q1978\\E",
      "shortCiteRegEx" : "Kushner and Clark",
      "year" : 1978
    }, {
      "title" : "Stochastic Approximation and Recursive Algorithms and Applications",
      "author" : [ "H.J. Kushner", "G.G. Yin" ],
      "venue" : null,
      "citeRegEx" : "Kushner and Yin,? \\Q2003\\E",
      "shortCiteRegEx" : "Kushner and Yin",
      "year" : 2003
    }, {
      "title" : "Gradient Temporal-Difference Learning Algorithms",
      "author" : [ "H.R. Maei" ],
      "venue" : "PhD thesis, University of Alberta,",
      "citeRegEx" : "Maei,? \\Q2011\\E",
      "shortCiteRegEx" : "Maei",
      "year" : 2011
    }, {
      "title" : "Weighted importance sampling for off-policy learning with linear function approximation",
      "author" : [ "A.R. Mahmood", "H. van Hasselt", "R.S. Sutton" ],
      "venue" : "In Proc. Conf. Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Mahmood et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mahmood et al\\.",
      "year" : 2014
    }, {
      "title" : "Emphatic temporal-difference learning",
      "author" : [ "A.R. Mahmood", "H. Yu", "M. White", "R.S. Sutton" ],
      "venue" : "In European Workshops on Reinforcement Learning, Lille, France,",
      "citeRegEx" : "Mahmood et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mahmood et al\\.",
      "year" : 2015
    }, {
      "title" : "Control Techniques for Complex Networks",
      "author" : [ "S. Meyn" ],
      "venue" : null,
      "citeRegEx" : "Meyn,? \\Q2008\\E",
      "shortCiteRegEx" : "Meyn",
      "year" : 2008
    }, {
      "title" : "Markov Chains and Stochastic Stability",
      "author" : [ "S. Meyn", "R.L. Tweedie" ],
      "venue" : null,
      "citeRegEx" : "Meyn and Tweedie,? \\Q2009\\E",
      "shortCiteRegEx" : "Meyn and Tweedie",
      "year" : 2009
    }, {
      "title" : "Discrete-Parameter Martingales",
      "author" : [ "J. Neveu" ],
      "venue" : null,
      "citeRegEx" : "Neveu,? \\Q1975\\E",
      "shortCiteRegEx" : "Neveu",
      "year" : 1975
    }, {
      "title" : "Off-policy temporal-difference learning with function approximation",
      "author" : [ "D. Precup", "R.S. Sutton", "S. Dasgupta" ],
      "venue" : "In Proc. The 18th Int. Conf. Machine Learning,",
      "citeRegEx" : "Precup et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Precup et al\\.",
      "year" : 2001
    }, {
      "title" : "Markov decision processes: Discrete stochastic dynamic programming",
      "author" : [ "M.L. Puterman" ],
      "venue" : null,
      "citeRegEx" : "Puterman,? \\Q1994\\E",
      "shortCiteRegEx" : "Puterman",
      "year" : 1994
    }, {
      "title" : "Combining importance sampling and temporal difference control variates to simulate Markov chains",
      "author" : [ "R.S. Randhawa", "S. Juneja" ],
      "venue" : "ACM Trans. Modeling and Computer Simulation,",
      "citeRegEx" : "Randhawa and Juneja,? \\Q2004\\E",
      "shortCiteRegEx" : "Randhawa and Juneja",
      "year" : 2004
    }, {
      "title" : "Should one compute the temporal difference fix point or minimize the Bellman residual? The unified oblique projection view",
      "author" : [ "B. Scherrer" ],
      "venue" : "In Proc. The 27th Int. Conf. Machine Learning,",
      "citeRegEx" : "Scherrer,? \\Q2010\\E",
      "shortCiteRegEx" : "Scherrer",
      "year" : 2010
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Sutton,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton",
      "year" : 1988
    }, {
      "title" : "TD models: Modeling the world at a mixture of time scales",
      "author" : [ "R.S. Sutton" ],
      "venue" : "In Proc. The 12th Int. Conf. Machine Learning,",
      "citeRegEx" : "Sutton,? \\Q1995\\E",
      "shortCiteRegEx" : "Sutton",
      "year" : 1995
    }, {
      "title" : "Reinforcement Learning",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "An emphatic approach to the problem of off-policy",
      "author" : [ "R.S. Sutton", "A.R. Mahmood", "M. White" ],
      "venue" : "temporal-difference learning,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous stochastic approximation and Q-learning",
      "author" : [ "J.N. Tsitsiklis" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Tsitsiklis,? \\Q1994\\E",
      "shortCiteRegEx" : "Tsitsiklis",
      "year" : 1994
    }, {
      "title" : "An analysis of temporal-difference learning with function approximation",
      "author" : [ "J.N. Tsitsiklis", "B. Van Roy" ],
      "venue" : "IEEE Trans. Automat. Contr.,",
      "citeRegEx" : "Tsitsiklis and Roy,? \\Q1997\\E",
      "shortCiteRegEx" : "Tsitsiklis and Roy",
      "year" : 1997
    }, {
      "title" : "Matrix Iterative Analysis",
      "author" : [ "R.S. Varga" ],
      "venue" : "Springer-Verlag, Berlin, 2nd edition,",
      "citeRegEx" : "Varga,? \\Q2000\\E",
      "shortCiteRegEx" : "Varga",
      "year" : 2000
    }, {
      "title" : "Least squares temporal difference methods: An analysis under general conditions",
      "author" : [ "H. Yu" ],
      "venue" : "SIAM J. Control Optim.,",
      "citeRegEx" : "Yu,? \\Q2012\\E",
      "shortCiteRegEx" : "Yu",
      "year" : 2012
    }, {
      "title" : "Weighted Bellman equations and their applications in approximate dynamic programming",
      "author" : [ "H. Yu", "D.P. Bertsekas" ],
      "venue" : "LIDS Technical Report 2876,",
      "citeRegEx" : "Yu and Bertsekas,? \\Q2012\\E",
      "shortCiteRegEx" : "Yu and Bertsekas",
      "year" : 2012
    }, {
      "title" : "imply, by a convergence theorem in (Neveu, 1975, Ex. II-4, p. 33-34) for nonnegative random processes (which is a consequence of the nonnegative supermartingale convergence theorem)",
      "author" : [ "∞ a.s" ],
      "venue" : null,
      "citeRegEx" : "a.s.,? \\Q1975\\E",
      "shortCiteRegEx" : "a.s.",
      "year" : 1975
    }, {
      "title" : "Roughly speaking, the method of proof is to decompose the matrixG into irreducible diagonal blocks and use, among others, the theorem (Varga, 2000, Cor",
      "author" : [ "Prop. C" ],
      "venue" : null,
      "citeRegEx" : "C.2.,? \\Q2000\\E",
      "shortCiteRegEx" : "C.2.",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Such algorithms were recently proposed by Sutton, Mahmood, and White (2015) as an improved solution to the problem of divergence of off-policy temporal-difference learning with linear function approximation.",
      "startOffset" : 42,
      "endOffset" : 76
    }, {
      "referenceID" : 22,
      "context" : "We focus on temporal-difference (TD) methods with linear function approximation (Sutton, 1988).",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "The approach is related to the early work on episodic off-policy TD(λ) (Precup et al., 2001), and is based on the idea of re-weighting the states when forming the eligibility traces in TD(λ), so that the weights reflect the occupation frequencies of the target policy rather than the behavior policy.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "The stability criterion of (Borkar and Meyn, 2000) (see also (Borkar, 2008, Chap.",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "For regular off-policy LSTD(λ) and TD(λ) (Bertsekas and Yu, 2009), it has been shown by Yu (2012) that the associated joint process of states and trace iterates exhibit useful properties, by which convergence results for LSTD(λ) can be derived.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "We focus on temporal-difference (TD) methods with linear function approximation (Sutton, 1988). Such methods are typically convergent when the target and behavior policies are the same (the on-policy case), but not in the off-policy case (Tsitsiklis and Van Roy, 1997). This difficulty is intrinsic to sampling states according to an arbitrary policy.1 Gradient-based or least squares-based approaches have been used to avoid this difficulty.2 Recently, Sutton, Mahmood, and White (2015) proposed a new approach to address this issue more directly.",
      "startOffset" : 81,
      "endOffset" : 488
    }, {
      "referenceID" : 0,
      "context" : "For regular off-policy LSTD(λ) and TD(λ) (Bertsekas and Yu, 2009), it has been shown by Yu (2012) that the associated joint process of states and trace iterates exhibit useful properties, by which convergence results for LSTD(λ) can be derived.",
      "startOffset" : 42,
      "endOffset" : 98
    }, {
      "referenceID" : 25,
      "context" : "See the papers (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton et al., 2015) and the books (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) for related examples and discussion.",
      "startOffset" : 15,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : ", 2015) and the books (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) for related examples and discussion.",
      "startOffset" : 22,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : ", (Maei, 2011; Bertsekas and Yu, 2009; Geist and Scherrer, 2014; Dann et al., 2014).",
      "startOffset" : 2,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : ", (Maei, 2011; Bertsekas and Yu, 2009; Geist and Scherrer, 2014; Dann et al., 2014).",
      "startOffset" : 2,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : ", (Maei, 2011; Bertsekas and Yu, 2009; Geist and Scherrer, 2014; Dann et al., 2014).",
      "startOffset" : 2,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : ", (Maei, 2011; Bertsekas and Yu, 2009; Geist and Scherrer, 2014; Dann et al., 2014).",
      "startOffset" : 2,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "Related examples can be found in (Glynn and Iglehart, 1989; Randhawa and Juneja, 2004; Sutton et al., 2015).",
      "startOffset" : 33,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "Related examples can be found in (Glynn and Iglehart, 1989; Randhawa and Juneja, 2004; Sutton et al., 2015).",
      "startOffset" : 33,
      "endOffset" : 107
    }, {
      "referenceID" : 25,
      "context" : "Related examples can be found in (Glynn and Iglehart, 1989; Randhawa and Juneja, 2004; Sutton et al., 2015).",
      "startOffset" : 33,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "the conditions of a convergence theorem from stochastic approximation theory (Kushner and Yin, 2003) and yield convergence results for TD(λ).",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : "In this paper we will take the proof approach used in (Yu, 2012).",
      "startOffset" : 54,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, we will give a new argument to prove the almost sure convergence of ETD(λ), which applies also to the regular off-policy TD(λ) of (Bertsekas and Yu, 2009) for λ near 1.",
      "startOffset" : 143,
      "endOffset" : 167
    }, {
      "referenceID" : 29,
      "context" : "This improves a result of (Yu, 2012), which only dealt with a constrained version of TD(λ) that restricts the iterates to lie in a bounded set.",
      "startOffset" : 26,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "Algorithms We consider computing vπ with the ETD(λ) algorithm (Sutton et al., 2015) and its least-squares version, ELSTD(λ), using linear function approximation, while following the behavior policy πo.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : "Associated with ETD(λ) is a generalized Bellman equation of which vπ is the unique solution (Sutton, 1995):8 v = r π,γ + P λ π,γ v.",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 25,
      "context" : "For insights about ETD(λ), see (Sutton et al., 2015; Mahmood et al., 2015).",
      "startOffset" : 31,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "For insights about ETD(λ), see (Sutton et al., 2015; Mahmood et al., 2015).",
      "startOffset" : 31,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "For the details of this Bellman equation, we refer the readers to the early work (Sutton, 1995; Sutton and Barto, 1998) and the recent work (Sutton et al.",
      "startOffset" : 81,
      "endOffset" : 119
    }, {
      "referenceID" : 24,
      "context" : "For the details of this Bellman equation, we refer the readers to the early work (Sutton, 1995; Sutton and Barto, 1998) and the recent work (Sutton et al.",
      "startOffset" : 81,
      "endOffset" : 119
    }, {
      "referenceID" : 25,
      "context" : "For the details of this Bellman equation, we refer the readers to the early work (Sutton, 1995; Sutton and Barto, 1998) and the recent work (Sutton et al., 2015).",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 13,
      "context" : ", 2015; Mahmood et al., 2015). Our definition (2.4) of {et} differs slightly from its original definition, but the two are equivalent; ours appears to be more convenient for our analysis. 8. For the details of this Bellman equation, we refer the readers to the early work (Sutton, 1995; Sutton and Barto, 1998) and the recent work (Sutton et al., 2015). 9. The negative definiteness of C is proved for positive i(·) under Assumption 2.1 by Sutton et al. (2015), and their result extends to nonnegative i(·), as long as C is nonsingular (see our Prop.",
      "startOffset" : 8,
      "endOffset" : 461
    }, {
      "referenceID" : 25,
      "context" : "4) in ETD(λ) (Sutton et al., 2015).",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : "1, C is always negative semidefinite (Sutton et al., 2015) (cf.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : ", (Boyan, 1999; Yu, 2012) for on-policy and off-policy LSTD(λ)).",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 29,
      "context" : ", (Boyan, 1999; Yu, 2012) for on-policy and off-policy LSTD(λ)).",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 20,
      "context" : "7) has a unique solution, and bounds on the approximation error of ETD(λ) can be derived using the approach of Scherrer (2010). (For details of this discussion, see Remark C.",
      "startOffset" : 111,
      "endOffset" : 127
    }, {
      "referenceID" : 13,
      "context" : "For a recent work in this direction, see (Mahmood et al., 2014).",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 29,
      "context" : "The structure of our analysis will be similar to that of (Yu, 2012) for regular off-policy LSTD(λ), but the proofs at intermediate steps are new and more involved.",
      "startOffset" : 57,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "Using the results of Section 3, we can now analyze its convergence by applying a “mean ODE” method from stochastic approximation theory (Kushner and Yin, 2003).",
      "startOffset" : 136,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "1 (Almost sure convergence of regular off-policy TD(λ)) If λ is a constant sufficiently close to 1, the matrix associated with the “mean updates” of the regular off-policy TD(λ) algorithm is also negative definite (Bertsekas and Yu, 2009).",
      "startOffset" : 214,
      "endOffset" : 238
    }, {
      "referenceID" : 11,
      "context" : "ẋ = h̄(x) + z, z ∈ −NB(x), where h̄(x) = Cx + b, NB(x) is the normal cone of B at x, and z is the boundary reflection term that keeps the solution in B (Kushner and Yin, 2003).",
      "startOffset" : 152,
      "endOffset" : 175
    }, {
      "referenceID" : 10,
      "context" : "The four remaining conditions to be introduced are of the same type and relate to the asymptotic rate of change conditions introduced by (Kushner and Clark, 1978).",
      "startOffset" : 137,
      "endOffset" : 162
    }, {
      "referenceID" : 25,
      "context" : "It is known to hold if i(s) > 0 for all s ∈ S (Sutton et al., 2015).",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : "Similarly to (Sutton, 1988; Sutton et al., 2015), our analysis will focus on the N × N symmetric matrix G = M̄(I −Q) + (I −Q)>M̄ for the substochastic matrix Q = P λ π,γ and the nonnegative diagonal matrix M̄ as given above.",
      "startOffset" : 13,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "Similarly to (Sutton, 1988; Sutton et al., 2015), our analysis will focus on the N × N symmetric matrix G = M̄(I −Q) + (I −Q)>M̄ for the substochastic matrix Q = P λ π,γ and the nonnegative diagonal matrix M̄ as given above.",
      "startOffset" : 13,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "This result is due to (Sutton et al., 2015).",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 25,
      "context" : "Using this together with the fact that Q is substochastic, by a direct calculation as in (Sutton et al., 2015), we have that for each s ∈ S,",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 30,
      "context" : "1 (Seminorm projection) Using seminorm projections to formulate the projected Bellman equations associated with TD methods is introduced in (Yu and Bertsekas, 2012).",
      "startOffset" : 140,
      "endOffset" : 164
    }, {
      "referenceID" : 21,
      "context" : "The relation between the solution v = Φθ∗ of this equation and the desired value function vπ, in particular, the approximation error, can be analyzed then, using the oblique projection viewpoint (Scherrer, 2010) (for details, see also (Yu and Bertsekas, 2012)).",
      "startOffset" : 195,
      "endOffset" : 211
    }, {
      "referenceID" : 30,
      "context" : "The relation between the solution v = Φθ∗ of this equation and the desired value function vπ, in particular, the approximation error, can be analyzed then, using the oblique projection viewpoint (Scherrer, 2010) (for details, see also (Yu and Bertsekas, 2012)).",
      "startOffset" : 235,
      "endOffset" : 259
    } ],
    "year" : 2015,
    "abstractText" : "We consider emphatic temporal-difference learning algorithms for policy evaluation in discounted Markov decision processes with finite spaces. Such algorithms were recently proposed by Sutton, Mahmood, and White (2015) as an improved solution to the problem of divergence of off-policy temporal-difference learning with linear function approximation. We present in this paper the first convergence proofs for two emphatic algorithms, ETD(λ) and ELSTD(λ). We prove, under general off-policy conditions, the convergence in L for ELSTD(λ) iterates, and the almost sure convergence of the approximate value functions calculated by both algorithms using a single infinitely long trajectory. Our analysis involves new techniques with applications beyond emphatic algorithms leading, for example, to the first proof that standard TD(λ) also converges under off-policy training for λ sufficiently large.",
    "creator" : "LaTeX with hyperref package"
  }
}