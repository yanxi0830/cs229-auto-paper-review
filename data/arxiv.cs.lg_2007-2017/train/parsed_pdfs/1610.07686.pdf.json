{
  "name" : "1610.07686.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Co-Occuring Directions Sketching for Approximate Matrix Multiply",
    "authors" : [ "Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n07 68\n6v 1\n[ cs\n.L G\n] 2\n5 O\nct 2\n01 6\nWe introduce co-occurring directions sketching, a deterministic algorithm for approximate matrix product (AMM), in the streaming model. We show that co-occuring directions achieves a better error bound for AMM than other randomized and deterministic approaches for AMM. Co-occurring directions gives a (1 + ε)-approximation of the optimal low rank approximation of a matrix product. Empirically our algorithm outperforms competing methods for AMM, for a small sketch size. We validate empirically our theoretical findings and algorithms."
    }, {
      "heading" : "1 Introduction",
      "text" : "The vast and continuously growing amount of multimodal content poses some challenges with respect to the collection and the mining of this data. Multimodal datasets are often viewed as multiple large matrices describing the same content with different modality representations (multiple views) such as images and their textual descriptions. The product of large multimodal matrices is of practical interest as it models the correlation between different modalities. Methods such as Partial Least Squares (PLS) [Weg00], Canonical Correlation Analysis (CCA)[Hot36], Spectral Co-Clustering [Dhi01], exploit the low rank structure of the correlation matrix to mine the hidden joint factors, by computing the truncated singular value decomposition of a matrix product.\nThe data streaming paradigm assumes a single pass over the data and a small memory footprint, resulting in a space/accuracy tradeoff. Multimodal data can occupy a large amount of memory or may be generated sequentially, hence it is important for the streaming\nPreliminary work. Under review by AISTATS 2017. Do not distribute.\nmodel to capture the data correlation .\nApproximate Matrix Multiplication (AMM), is gaining an increasing interest in streaming applications (See the recent monograph [Woo14] for more details ). In AMM we are given matrices X ,Y , with a large number of columns n, and the goal is to compute matrices BX , BY , with smaller number of columns ℓ, such that ||XY ⊤ − BXB⊤Y ||Z is small for some norm ‖.‖Z . In streaming AMM, columns of BX , BY , need to be updated as the data arrives sequentially. We refer to BX and BY as sketches of X and Y .\nRandomized approaches for AMM were pioneered by the work of [DKM06]. The approach of [DKM06] is based on the sampling of ℓ columns of X and Y . [DKM06] shows that by choosing an appropriate sampling matrix Π ∈ Rn×ℓ, we obtain a Frobenius error guarantee (‖.‖Z = ‖.‖F ):\n∥ ∥XY ⊤ −XΠ(Y Π)⊤ ∥ ∥ F ≤ ε ‖X‖F ‖Y ‖F , (1)\nfor ℓ = Ω(1/ε2), with high probability. The same guarantee of Eq. (1) was achieved in [Sar06], by using a random projection Π ∈ Rn×ℓ that satisfies the guarantees of a Johnson- Lindenstrauss (JL) transform (∀x ∈ Rn ‖Πx‖2 ∼ (1 ± ε) ‖x‖2 , with probability 1 − δ), where ℓ = O(1/ε2 log(1/δ)). Other randomized approaches focused on error guarantees given in spectral norm (‖.‖Z = ‖.‖) , such as JL embeddings or efficient subspace embeddings [Sar06, MZ11, ATKZ14, CNW15] that can be applied to any type of matrices X in input sparisty time [CW13]. [CNW15] showed that using a subspace embedding Π ∈ Rn×ℓ we have with a probability 1− δ:\n∥ ∥XY ⊤ −XΠ(Y Π)⊤ ∥ ∥ ≤ ε ‖X‖ ‖Y ‖ , (2)\nfor ℓ = O((sr(X) + sr(Y ) + log(1/δ))/ε2), where sr(X) = ‖X‖2F ‖X‖2 is the stable rank of X . Note that sr(X) ≤ rank(X), hence results stated in term of stable rank are sharper and more robust than the one stated with the rank [Sar06, MZ11, ATKZ14].\nCovariance sketching refers to AMM for X = Y . An elegant deterministic approach for covariance sketching called frequent directions was introduced recently\nin [Lib13, GLPW15], drawing the connection between covariance matrix sketching, and the classic problem of estimation of frequent items [MG82]. Another approach for AMM, consists of concatenating matrices X and Y, and of applying a covariance sketch technique on the resulting matrix, this approach results in a looser guarantee; The right hand side in Equations (1),(2) is replaced by ε(‖X‖2F + ‖Y ‖ 2 F ). Based on this observation, [YLZ16] proposed to use the frequent directions algorithm of [Lib13] to perform AMM in a deterministic way, we refer to this approach as FDAMM. FD-AMM [YLZ16] outputs BX , BY such that\n∥ ∥XY ⊤ −BXB⊤Y ∥ ∥ ≤ ε(‖X‖2F + ‖Y ‖ 2 F ), (3)\nfor ℓ = ⌈ 1ε⌉. The sketch length ℓ dependency on ε in randomized methods is quadratic, FD-AMM improves this dependency to linear.\nIn this paper we introduce co-occuring directions, a deterministic algorithm for AMM. Our algorithm is inspired by frequent directions and enables similar guarantees to (2) in spectral norm, but with a linear dependency of ℓ on ε as in FD-AMM. Given with stable ranks, co-occuring direction achieves the guarantee of (2) for ℓ = O( √ sr(X)sr(Y )/ε).\nThe paper is organized as follows: In Section 2 we review frequent directions, introduce our co-occuring directions sketching algorithm, and give error bounds analysis in AMM and in low rank approximation of a matrix product. We state our proofs in Section 3. In section 2.2.2 and Section 4 we discuss error bounds, space and time requirements, and compare our approach to related work on AMM and low rank approximation. Finally we validate the empirical performance of co-occuring directions in Section 5, on both synthetic and real world multimodal datasets.\nNotation. We note by C = UΣV ⊤, the thin svd of C, and by σmax(C) the maximum singular value, Tr refers to the trace. σj are the singular values that are assumed to be given in decreasing order. Note that for C ∈ Rmx×my the spectral norm is defined as follows ‖C‖ = maxu,v,‖u‖=‖v‖=1 ∣ ∣u⊤Cv ∣\n∣ = σmax(C). The nuclear norm (known also as trace or 1− schatten norm) is defined as follows: ‖C‖∗ = Tr(Σ). sr(C) = ‖C‖2F ‖C‖2 is the stable rank of C. Assume C and D have the same number of column, [C;D] denotes their concatenation on their row dimensions. For n ∈ N, [n] = {1, . . . n}."
    }, {
      "heading" : "2 Sketching from Covariance to Correlation",
      "text" : "In this section we review covariance sketching with the frequent directions algorithm of [Lib13] and state its\ntheoretical guarantees [Lib13, GLPW15]. We then introduce correlation sketching and present and analyze our co-occuring directions algorithm.\n2.1 Covariance Sketching: Frequent Directions\nLet X ∈ Rmx×n, where n is the number of samples and mx the dimension. We assume that n > mx. The goal of covariance sketching is to find a small matrix DX ∈ Rmx×ℓ, where ℓ << n (ℓ is assumed to be an even number ), such that XX⊤ ≈ DXD⊤X . Frequent directions algorithm introduced in [Lib13] (Algorithm 1) achieves this goal. Intuitively frequent directions algorithm sets a noise level using the median of the spectrum of the covariance of the sketch DX . It then discards directions below that level and replaces them with fresh samples. This results in the updated the covariance estimate. This process is repeated as the data is streaming.\nAlgorithm 1 Frequent Directions\n1: procedure FD(X ∈ Rmx×n) 2: DX ← 0 ∈ Rmx×ℓ . 3: for i ∈ [n] do 4: Insert column Xi into a zero column of DX 5: if DX has no zero valued column then 6: [U,Σ, V ] ← SVD(DX) 7: δ ← σ2ℓ/2 ⊲ median value of Σ2 8: Σ̃ ← √\nmax(Σ2 − δIℓ, 0) ⊲ shrinkage 9: DX ← U Σ̃\n10: end if 11: end for 12: return DX 13: end procedure\nTheorem 1 ([Lib13]) DX the output of algorithm 1 satisfies:\n∥ ∥XX⊤ −DXD⊤X ∥\n∥ ≤ 2 ‖X‖ 2 F\nℓ . (4)\n2.2 Correlation Sketching: Co-occuring Directions\nWe start by defining correlation sketching:\nDefinition 1 (Correlation Sketching/AMM) Let X ∈ Rmx×n, Y ∈ Rmy×n, where n > max(mx,my). Let BX ∈ Rmx×ℓ and BY ∈ Rmy×ℓ (ℓ < n, ℓ ≤ min(mx,my)). Let η > 0 . The matrix pair (BX , BY ) is called an η-correlation sketch of (X,Y ) if it satisfies in spectral norm:\n∥ ∥XY ⊤ −BXB⊤Y ∥ ∥ ≤ η.\nWe now present our co-occuring directions algorithm (Algorithm 2). Intuitively Algorithm 2 sets a noise level using the median of the singular values of the correlation matrix of the sketch BXB ⊤ Y . The SVD of BXB ⊤ Y is computed efficiently in lines 8,9 and 10 of Algorithm 2 using QR decomposition. Left and right singular vectors below this noise threshold are replaced by fresh samples from X and Y , correlation sketches are updated and the process continues. Theorem 2 shows that our co-occuring directions algorithm outputs (BX , BY ) a correlation sketch of (X,Y ) as defined above in Definition 1.\nAlgorithm 2 Co-occuring Directions\n1: procedure Co-D(X ∈ Rmx×n, Y ∈ Rmy×n) 2: BX ← 0 ∈ Rmx×ℓ . 3: BY ← 0 ∈ Rmy×ℓ . 4: for i ∈ [n] do 5: Insert a column Xi into a zero valued col-\numn of BX 6: Insert a column Yi into a zero valued col-\numn of BY 7: if BX , BY have no zero valued column then 8: [Qx, Rx] ← QR(BX) 9: [Qy, Ry] ← QR(BY ) 10: [U,Σ, V ] ← SVD(RxR⊤y ) 11: ⊲ Qx ∈ Rmx×ℓ, Rx ∈ Rℓ×ℓ, 12: ⊲ Qy ∈ Rmy×ℓ, Ry ∈ Rℓ×ℓ, U,Σ, V ∈ Rℓ×ℓ. 13: Cx ← QxU √ Σ\n14: Cy ← QyV √ Σ 15: ⊲ Cx, Cy not computed 16: δ ← σℓ/2(Σ) ⊲ the median value of Σ 17: Σ̃ ← max(Σ− δIℓ, 0) ⊲ shrinkage 18: BX ← QxU √ Σ̃ 19: BY ← QyV √\nΣ̃ 20: ⊲ at least last ℓ/2 columns are zero 21: end if 22: end for 23: return BX , BY 24: end procedure\nIt is important to see that while frequent directions shrinks Σ2, co-occuring directions filters Σ. We prove in the following an approximation bound in spectral norm for co-occurring directions."
    }, {
      "heading" : "2.2.1 Main Results",
      "text" : "We give in the following our main results, on the approximation error of co-occurring direction in AMM (Theorem 2), and in the k−th rank approximation of a matrix product (Theorem 3). Proofs are given in Section 3.\nTheorem 2 (AMM) The output of co-occuring directions (Algorithm 2) gives a correlation sketch (BX , BY ) of (X,Y ), for ℓ ≤ min(mx,my) satisfying: For a correlation sketch of length ℓ, we have:\n∥ ∥XY ⊤ −BXB⊤Y ∥ ∥ ≤ 2 ‖X‖F ‖Y ‖F ℓ .\n2) Algorithm 2 runs in O(n(mx +my + ℓ)ℓ) time and requires a space of O((mx +my + ℓ)ℓ).\nTheorem 3 (Low Rank Product Approximation) Let (BX , BY ) be the output of Algorithm 2. Let k ≤ ℓ. Let Uk, Vk be the matrices whose columns are the k-th largest left and right singular vectors of BXB ⊤ Y . Let πkU (X) = UkU ⊤ k X, π k V (Y ) = VkV ⊤ k Y . Let\nε > 0, for ℓ ≥ 8 √ sr(X)sr(Y )\nε ||X||||Y || σk+1(XY ⊤) we have:\n∥ ∥XY ⊤ − πkU (X)πkV (Y )⊤ ∥ ∥ ≤ σk+1(XY ⊤)(1 + ε)."
    }, {
      "heading" : "2.2.2 Discussion of Main Results",
      "text" : "For ℓ = ⌈ 1ε⌉, ε ∈ [ 1min(mx,my) , 1] from Theorem 2 we see that (BX , BY ) produced by Algorithm 2 is an ηcorrelation sketch of (X,Y ) for η = 2ε ‖X‖F ‖Y ‖F . In AMM, bounds are usually stated in term of the product of spectral norms ofX an Y as in Equation (2). Let sr(X) = ‖X‖2F ‖X‖2 be the stable rank ofX . It is easy to see\nthat co-occuring directions for ℓ = 2 √ sr(X)sr(Y )\nε , gives an error bound of ε ‖X‖ ‖Y ‖. While in randomized methods the error is O(1/ √ ℓ), co-occuring direction’s error is O(1/ℓ). Moreover the dependency on stable ranks in co-occuring directions is 2 √\nsr(X)sr(Y ) ≤ sr(X) + sr(Y ), the lattter appears in subspace embedding based AMM [CNW15, MZ11, ATKZ14]. For X = Y co-occuring directions reduces to frequent directions of [Lib13], and Theorem 2 recovers Theorem 1 of [Lib13].\nStronger bounds for frequent directions were given in [GLPW15] where the bound in Equation (4) is improved, for ℓ > 2k, for any k:\n∥ ∥XX⊤ −DXD⊤X ∥ ∥ ≤ 2 ℓ− 2k ‖X −Xk‖ 2 F ,\nwhere Xk is the k−th rank approximation of X (with X0 = 0). Hence by defining Z = [X ;Y ] ∈ R\n(mx+my)×n and applying frequent directions to Z (FD-AMM [YLZ16]), we obtain BX , BY satisfying: ∥ ∥XY ⊤ −BXB⊤Y ∥ ∥ ≤ 2ℓ−2k ‖Z − Zk‖ 2 F , hence the perfomance of FD-AMM depends on the low rank structure of Z. A sharper analysis for co-occuring directions remains an open question, but the following discussion of Theorem 3 will shed some light on the advantages of co-occuring directions on FD-AMM [YLZ16].\nTheorem 3 shows that co-occuring directions sketching gives a (1+ ε)- approximation of the optimal low rank approximation of the matrix product XY ⊤. Note that σk+1(XY ⊤) ≤ ‖XY ⊤‖ ∗\nk+1 . Hence for ℓ ≥ 8(k + 1)/ε, we obtain a 1 + ε- approximation of the optimal k rank approximation of XY ⊤. This highlights the relation between the sketch length in co-occurring directions ℓ and the rank of XY ⊤. Note that the maximum rank of XY ⊤ is min(rank(X), rank(Y )). When using FD-AMM, based on the covariance sketch of the concatenation of X and Y , the sketch length ℓ is related to the rank of Z = [X ;Y ]. Note that the maximum rank of the concatenation (Z) is bounded by rank(X) + rank(Y ). Hence we see that co-occuring directions guarantees a 1+ε approximation of the optimal k-rank approximation ofXY ⊤ for a smaller sketch size then FD-AMM (min(rank(X), rank(Y )) for cooccuring directions versus rank(X)+ rank(Y ) for FDAMM).\nIn the following we comment on the running time of co-occuring directions."
    }, {
      "heading" : "2.2.3 Running Time Analysis and Parralelization.",
      "text" : "Running Time. We compare the space and the running time of our sketch to to a naive implementation of the correlation sketch. 1) Naive Correlation Sketch: In the if statement of Algorithm 2, compute the ℓ thin svd SVD(BXB ⊤ Y ) = [U,Σ, V ], BX ← U √ Σ̃, BY ← V √\nΣ̃. We need a space O(mxmy) to store BXB ⊤ y . The running time is dominated by computing an ℓ thin svd O(mxmyℓ) each n ℓ/2 that is O(nmxmy), hence no gain with respect to brute force. 2) Co-occuring Directions: Algorithm 2 avoids computing BXB ⊤ Y by using the QR decomposition of BX and BY . The space needed is O(ℓ(mx + my + ℓ)). We have a computation done every nℓ/2 , that is dominated by computing QR factorization and svd : O((mx +my + ℓ)ℓ 2) (computing RxR ⊤ y requires O(ℓ\n3) operations). This results in a total running time : O(n(mx + my + ℓ)ℓ). There is a computational and memory advantage when ℓ <\nmxmy mx+my .\nParallelization of Co-occuring Directions (Sketches of Sketches). Similarly to the frequent directions [Lib13], co-occuring directions algorithm is simply parallelizable. Let X = [X1, X2] ∈ R\nmx×(n1+n2), and Y = [Y1, Y2] ∈ Rmx×(n1+n2). Let (B1X , B 1 Y ) be the correlation sketch of (X1, Y1), and (B2X , B 2 Y ) be the correlation sketch of (X2, Y2). Then the correlation sketch (CX , CY ) of ([B1X , B 2 X ], [B 1 Y , B 2 Y ]) is a correlation sketch of (X,Y ), and is as good as (BX , BY ) the correlation\nsketch of (X,Y ). Hence we can sketch the data in M -independent chunks on M machines then merge by concatenating the sketches and performing another sketch on the concatenation, by doing so we divide the running time by M ."
    }, {
      "heading" : "3 Proofs",
      "text" : "In this Section we give proofs of our main results:\nProof 1 (Proof of Theorem 2) By construction we have:\nCxC ⊤ y =\n( QxU √ Σ )( QyV √ Σ )⊤\n= Qx ( UΣV ⊤ ) Q⊤y = Qx ( RxR ⊤ y ) Q⊤y = (QxRx) (QyRy) ⊤ .\nHence the algorithm is computing a form of R-SVD of BXB ⊤ Y , followed by a shrinkage of the correlation matrix. Let Bix, B i y, C i x, C i y,Σ i, Σ̃i, δi, the values of BX , BY , Cx, Cy,Σ, Σ̃, δ after the execution of the main loop. δi = 0 if we don’t enter the if statement (Bix = C i x and B i y = C i y if we don’t enter the if statement). Hence we have at an iteration i:\nCixC i,⊤ y = B i−1 x B i−1,⊤ y +XiY ⊤ i .\nNote that:\nXY ⊤ −BXB⊤Y = XY ⊤ −BnxBn,⊤y\n=\nn ∑\ni=1\n(\nXiY ⊤ i +B i−1 x B i−1,⊤ y −BixBi,⊤y\n)\n=\nn ∑\ni=1\n(\nCixC i,⊤ y −BixBi,⊤y\n)\n.\nBy the triangular inequality we can bound the spectral norm:\n∥ ∥XY ⊤ −BXB⊤Y ∥\n∥ ≤ n ∑\ni=1\n∥ ∥CixC i,⊤ y −BixBi,⊤y ∥ ∥ .\nWe are left with bounding ∥ ∥CixC i,⊤ y −BixBi,⊤y ∥ ∥:\nCixC i,⊤ y =\n( QixU i ) Σi ( QiyV i )⊤ , BixB i,⊤ y = ( QixU i ) Σ̃i ( QiyV i )⊤ .\nNote that: ∥ ∥CixC i,⊤ y −BixBi,⊤y ∥ ∥ = ∥ ∥ ∥(QixU i)(Σi − Σ̃i)(QiyV i)⊤ ∥ ∥ ∥\n= ∥ ∥ ∥Σi − Σ̃i ∥ ∥ ∥\n≤ δi,\nwhere the first equality follows from the fact that, QixU i, QiyV i, are orthonormal. And Σi − Σ̃i is a diagonal matrix with at least ℓ/2 entries equal δi or 0,\nand the other entries are less than δi. It follows that we have in spectral norm:\n∥ ∥XY ⊤ −BXB⊤Y ∥\n∥ ≤ n ∑\ni=1\nδi. (5)\nNow we want to relate ∑n\ni=1 δi to ℓ, and propreties of X,Y . Let ‖.‖∗, the 1− schatten norm. For a matrix A of rank r, and singular values σi : ‖A‖∗ = ∑r i=1 σi(A). We have: ∥\n∥BXB ⊤ Y\n∥ ∥\n∗ =\n∥ ∥BnxB n,⊤ y ∥ ∥\n∗\n=\nn ∑\ni=1\n∥ ∥BixB i,⊤ y ∥ ∥ ∗ − ∥ ∥Bi−1x B i−1,⊤ y ∥ ∥ ∗\n=\nn ∑\ni=1\n(\n∥ ∥CixC i,⊤ y ∥ ∥ ∗ − ∥ ∥Bi−1x B i−1,⊤ y ∥ ∥ ∗\n)\n− n ∑\ni=1\n(\n∥ ∥CixC i,⊤ y ∥ ∥ ∗ − ∥ ∥BixB i,⊤ y ∥ ∥ ∗\n)\n(6)\nWe have at an iteration i, the R-SVD of CixC i,⊤ y and Bi,xB i,⊤ y :\n∥ ∥CixC i,⊤ y ∥ ∥ ∗ = Tr(Σi) and ∥ ∥BixB i,⊤ y ∥ ∥ ∗ = Tr(Σ̃i).\nHence we have by the definition of the shrinking operation: ∥\n∥CixC i,⊤ y\n∥ ∥ ∗ − ∥ ∥BixB i,⊤ y ∥ ∥ ∗\n= Tr(Σi − Σ̃i) = ℓ ∑\nj=1\nσij − σ̃ij\n= ∑\nj,σi j >δi\nδi + ∑\nj,σi j ≤δi\nσij ≥ ℓ\n2 δi. (7)\nOn the other hand using the reverse triangle inequality for the 1− shatten norm we have: ∥ ∥CixC i,⊤ y ∥ ∥ ∗ − ∥ ∥Bi−1x B i−1,⊤ y ∥ ∥ ∗ ≤ ∥ ∥CixC i,⊤ y −Bi−1x Bi−1,⊤y ∥ ∥ ∗\nRecall that: CixC i,⊤ y = B i−1 x B i−1,⊤ y +XiY ⊤ i , hence we have: ∥ ∥CixC i,⊤ y ∥ ∥ ∗ − ∥ ∥Bi−1x B i−1,⊤ y ∥ ∥ ∗ ≤ ∥ ∥XiY ⊤ i ∥ ∥ ∗ = ‖Xi‖2 ‖Yi‖2 , (8) since XiY ⊤ i is rank one. Finally putting together Equations (6), (7),(8), we have:\n∥ ∥BXB ⊤ Y ∥ ∥ ∗ ≤\nn ∑\ni=1\n‖Xi‖2 ‖Yi‖2 − ℓ\n2\nn ∑\ni=1\nδi. (9)\nIt follows from Equation (9) that:\nn ∑\ni=1\nδi ≤ 2\nℓ\n(\nn ∑\ni=1\n‖Xi‖2 ‖Yi‖2 − ∥ ∥BXB ⊤ Y ∥ ∥\n∗\n)\n≤ 2 ℓ\n\n\n√ √ √ √ n ∑\ni=1\n‖Xi‖22\n√ √ √ √ n ∑\ni=1\n‖Yi‖22\n\n\n= 2\nℓ ‖X‖F ‖Y ‖F , (10)\nwhere in the last inequality we used the CauchySchwarz inequality. Putting together Equations (5) and (10) we have finally:\n∥ ∥XY ⊤ −BXB⊤Y ∥ ∥ ≤ 2 ℓ ‖X‖F ‖Y ‖F . (11)\n2) Refer to Section 2.2.3.\nProof 2 (Proof of Theorem 3) Let πkU (X) = UkU ⊤ k X, π k V (Y ) = VkV ⊤ k Y . Let Hxk be the span of {u1, . . . uk}, and Hxmx−k be the orthogonal of Hxk. Similarly define Hyk the span of {v1, . . . vk}, and H y my−k its orthogonal. For all u ∈ Rmx , ‖u‖ = 1, there exits ax, bx ∈ R, a2x + b2x = 1, such that u = axwx + bxzx, where wx ∈ Hxk , ||wx|| = 1 and zx ∈ Hxmx−k, ||zx|| = 1. Similarly for v ∈ Rmy , ‖v‖ = 1 there exits ay, by ∈ R, a2y + b 2 y = 1, such that v = aywy + byzy, where wy ∈ Hyk, ||wy|| = 1 and zy ∈ H y my−k , ||vy|| = 1 .\nLet ∆ = XY ⊤ − πkU (X)πkV (Y )⊤, we have ‖∆‖ = maxu∈Rmx ,v∈Rmy ,||u||=||v||=1 |u⊤∆v|\n|u⊤∆v| = |(axwx + bxzx)⊤∆(aywy + byzy)| ≤ |axay||w⊤x ∆wy |+ |bxby||z⊤x ∆zy| + |axby||w⊤x ∆zy|+ |bxay||z⊤x ∆wy|\nSince wx ∈ Hxk , wy ∈ Hyk, we have w⊤x ∆wy = 0. Since zx ∈ Hxmx−k, zy ∈ H y my−k , z⊤x ∆zy = z ⊤ x XY ⊤zy. Similarly w⊤x ∆zy = w ⊤ x XY ⊤zy, and z ⊤ x ∆wy = z ⊤ x XY\n⊤wy. Note that |ax|, |bx|, |ay|, |by| are bounded by 1. Hence we have (maximum is taken on each appropriate set defined above, all vectors are unit norm):\nmax u,v |u⊤∆v| ≤ max zx,zy |z⊤x XY ⊤zy|+ max wx,zy |w⊤x XY ⊤zy|\n+ max zx,wy\n|z⊤x XY ⊤wy|\nFor zx ∈ Hxmx−k, zy ∈ H y my−k we have:\n|z⊤x XY ⊤zy| ≤ |z⊤x (XY ⊤ −BXB⊤Y )zy|+ |z⊤x BXB⊤Y zy| ≤ ∥\n∥XY ⊤ −BXB⊤Y ∥ ∥+ σk+1(BXB ⊤ Y )\n≤ 2 ∥ ∥XY ⊤ −BXB⊤Y ∥ ∥+ σk+1(XY ⊤),\nwhere we used that maxzx∈Hxmx−k,zy∈H y my−k |z⊤x BXB⊤Y zy| = σk+1(BXB ⊤ Y ) by definition of σk+1. The last inequality follows from weyl inequality |σk+1(BXB⊤Y )− σk+1(XY ⊤)| ≤ ∥ ∥XY ⊤ −BXB⊤Y ∥ ∥. Note that for wx ∈ Hxk and zy ∈ Hymy−k we have w⊤x BXB ⊤ Y zy = 0. To see that, note that wx ∈ span{u1, . . . uk} , zy ∈ span{vk+1, . . . vℓ}. There exists βj, such that zy = ∑ℓ j=k+1 βjvj, hence BXB ⊤ Y zy = ∑ℓ i=1 ∑ℓ j=k+1 σiβjuiv ⊤ i vj = ∑ℓ j=k+1 σjβjuj ⊥ wx. Hence we have:\n|w⊤x XY ⊤zy| = |w⊤x (XY ⊤ −BXB⊤Y )zy| ≤ ∥\n∥XY ⊤ −BXB⊤Y ∥ ∥ .\nSimilarly for for zx ∈ Hxmx−k and wy ∈ H y k we conclude that: |w⊤x XY ⊤zy| ≤ ∥ ∥XY ⊤ −BXB⊤Y ∥\n∥. Finally we have:\n‖∆‖ ≤ 4 ∥ ∥XY ⊤ −BXB⊤Y ∥ ∥+ σk+1(XY ⊤)\n≤ 8 ‖X‖F ‖Y ‖F ℓ + σk+1(XY ⊤) ≤ σk+1(XY ⊤)(1 + 8 √ sr(X)sr(Y )\nℓ\n||X ||||Y || σk+1(XY ⊤) )\nFor ℓ ≥ 8 √ sr(X)sr(Y )\nε ||X||||Y || σk+1(XY ⊤) , we have: ‖∆‖ ≤\nσk+1(XY ⊤)(1 + ε)."
    }, {
      "heading" : "4 Previous Work on Approximate Matrix Multilply",
      "text" : "We list here a catalog of baselines for AMM:\nBrute Force. We keep a running correlation C ← C+XiY ⊤ i . We perform an ℓ thin svd at the end of the stream. Space O(mxmy), running time: O(nmxmy) + O(mxmyℓ), the cost of the sketch update and the ℓ thin svd.\nSampling [DKM06]. We define a distribution over [n], pi = ‖Xi‖‖Yi‖ S , where S = ∑n\ni=1 ‖Xi‖ ‖Yi‖. Form BX and BY by taking ℓ iids samples (column indices), using pi. In the streaming model, since S is not known, we use ℓ independent reservoir samples. Hence the space needed is O(ℓ(mx + my)), the running time is O(ℓ(mx +my)n).\nRandom Projection [Sar06]. BX , BY are of the form XΠ and YΠ, where Π ∈ Rn×ℓ , and Πij ∈ {−1/ √ ℓ, 1/ √ ℓ}, uniformly. This is easily implemented in the streaming model and requires O(ℓ(mx + my)) space and O(ℓ(mx +my)n) time.\nHashing [CW13]. Let h : [n] → [ℓ], and s : [n] → {−1, 1} be perfect hash functions. We initialize BX , BY to all zeros matrices. When processing columns of X and Y we update columns of BX and BY as follows: BX,h(i) ← BX,h(i) + s(i)Xi, BY,h(i) ← BY,h(i)+s(i)Yi. Hashing requires O(ℓ(mx+my)) space and O(n(mx +my)) time.\nFD-AMM [YLZ16]. Let Z = [X ;Y ] ∈ R(mx+my)×n, let DZ be the output of frequent directions (Algoritm 1). We partition DZ = [BX ;BY ], and use BX and BY in AMM. This requires O(ℓ(mx +my)) space and O(n(mx +my)ℓ) time."
    }, {
      "heading" : "5 Experiments",
      "text" : "AMM of Low Rank Matrices. We consider X ∈ R\nmx×n and Y ∈ Rmy×n, generated using a non-noisy low rank model [GLPW15] as follows: X = VxSxU ⊤ x ,\nwhere Ux ∈ Rn×kx , (Ux)i,j ∼ N (0, 1), Sx ∈ Rkx×kx is a diagonal matrix with (Sx)jj = 1−(j−1)/kx, and Vx ∈ R\nmx×kx is such that V ⊤x Vx = Ikx . Similarly we generate Y = VySyU ⊤ y , Uy ∈ Rn×ky , Sy ∈ Rky×ky , Vy ∈ R my×ky . Hence X and Y are at most rank kx, and ky respectively. We consider n = 10000, mx = 1000, my = 2000, and three regimes: both matrices have a large rank (kx = 400, ky = 400), one matrix has a smaller rank then the other (kx = 400, ky = 40), and both matrices have a small rank (kx = 40, ky = 40). We compare the performance of co-occuring directions to baselines given in Section 4 in those three regimes. For randomized baselines we run each experiments 50 times and report mean and standard deviations of performances. Experiments were conducted on a single core Intel Xeon CPU E5-2667, 3.30GHz, with 265 GB of RAM and 25.6 MB of cache.\nWe see in Figure 1, that hashing timing is, as expected, independent from the sketch length. Random projection requires the most amount of time. Cooccuring directions timing is on par with sampling and slightly better than FD-AMM. From Figure 2 1 we see that the deterministic baselines (a,c,e) consistently outperform the randomized baselines (b,d,f) in all three regimes. As discussed previously randomized methods error bound are of the order of O(1/ √ ℓ), while both co-occuring directions and FD-AMM have an error bound order O(1/ℓ). Note that the brute force error becomes zero (up to machine precision) when ℓ exceeds min(rank(X), rank(Y )). When comparing co-occuring direction to FD-AMM we see a clear phase transition for co-occuring direction as ℓ exceeds O(min(rank(X), rank(Y ))). For FD-AMM the phase transition happens when ℓ exceeds O(rank(X)+ rank(Y )). The phase transition happens earlier for cooccuring directions and hence co-occuring directions outperforms FD-AMM for a smaller sketch size. This is in line with our discussion in Section 2.2.2. For in-\n1Better seen in color.\nstance plot (c) illustrates this effect, kx = 400, ky = 40, as ℓ exceeds 50, the error of co-occuring directions sharply decreases , while FD-AMM error is still high.\nThe latter starts a steep decreasing tendency when ℓ exceeds 400. We give plots for the low rank approximation as given in Theorem 3 for k = min(kx, ky) in the\nappendix, we see a similar trend in the approximation error.\nAMM of Noisy Low Rank Matrices (Robustness). We consider the same model as before but we add a gaussian noise to the low rank matrices, i.e X = VxSxU ⊤ x + Nx/ζx, where ζx > 0, and Nx ∈ Rmx×n, (Nx)i,j ∼ N (0, 1). Similarly for Y = VySyU⊤y +Ny/ζy. In this scenario X and Y have still decaying singular values but with non zeros tails. We consider ζx = 1000, and ζy = 100. We compare here deterministic baselines in Figures 3,4, and 5, in the three scenarios we see that co-occuring directions still outperforms FDAMM, but the gap between the two approaches becomes smaller in the low rank regimes (Figures 4, and 5), this hints to a weakness in the shrinking of singular values in both algorithms getting affected by the noise (Step 17 in Alg. 2). We give plots for the low rank approximation in the appendix.\nMultimodal Data Experiments. In this section we study the empirical performance of co-occuring directions in approximating correlation between images and captions. We consider Microsoft COCO [LMB+14] dataset. For visual features we use the residual CNN Resnet101, [HZRS16]. The last layer of Resnet results in a feature vector of dimension mx = 2048. For text we use the Hierarchical Kernel Sentence Embed-\nding HSKE of [MMG16] that results in a feature vector of dimension my = 3000. The training set size is n = 113287. We see in Fig. 6 that co-occuring directions outperforms FD-AMM in this case as well (timing experiment is given in the appendix)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we introduced a deterministic sketching algorithm for AMM that we termed co-occuring directions . We showed its error bounds (in spectral norm) for AMM and the low rank approximation of a product. We showed empirically that co-occuring directions outperforms deterministic and randomized baselines in the streaming model. Indeed co-occuring direction has the best error/space tradeoff among known baselines with errors given in spectral norm in the streaming model. We are left with two open questions. First, whether guarantees of Theorem 2 can be improved akin to the improved guarantees for frequent directions given [GLPW15]. This would give an explicit link of the sketch length ℓ, to the low rank structure of the matrix product XY ⊤, and/or the low rank structure of the individual matrices. Second, whether robustness of co-occuring directions can be improved using\nrobust shrinkage operators as in [GDP14]."
    }, {
      "heading" : "A Low Rank product Approximation",
      "text" : "B MS-Coco Timing Experiments"
    } ],
    "references" : [ {
      "title" : "Approximate matrix multiplication with application to linear embeddings",
      "author" : [ "Michail Vlachos Anastasios T. Kyrillidis", "Anastasios Zouzias" ],
      "venue" : "Corr,",
      "citeRegEx" : "ATKZ14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Optimal approximate matrix product in terms of stable rank",
      "author" : [ "Michael B. Cohen", "Jelani Nelson", "David P. Woodruff" ],
      "venue" : "CoRR,",
      "citeRegEx" : "CNW15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Low rank approximation and regression in input sparsity time",
      "author" : [ "Kenneth L. Clarkson", "David P. Woodruff" ],
      "venue" : "STOC,",
      "citeRegEx" : "CW13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Co-clustering documents and words using bipartite spectral graph partitioning",
      "author" : [ "Inderjit S. Dhillon" ],
      "venue" : "KDD,",
      "citeRegEx" : "Dhi01",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Fast monte carlo algorithms for matrices i: Approximating matrix multiplication",
      "author" : [ "Petros Drineas", "Ravi Kannan", "Michael W. Mahoney" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "DKM06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Phillips",
      "author" : [ "Mina Ghashami", "Amey Desai", "Jeff M" ],
      "venue" : "Improved Practical Matrix Sketching with Guarantees.",
      "citeRegEx" : "GDP14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Frequent directions : Simple and deterministic matrix sketching",
      "author" : [ "Mina Ghashami", "Edo Liberty", "Jeff M. Phillips", "David P. Woodruff" ],
      "venue" : "CoRR,",
      "citeRegEx" : "GLPW15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Relations between two sets of variates",
      "author" : [ "Harold Hotteling" ],
      "venue" : "Biometrika,",
      "citeRegEx" : "Hot36",
      "shortCiteRegEx" : null,
      "year" : 1936
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "CVPR,",
      "citeRegEx" : "HZRS16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "In KDD",
      "author" : [ "Edo Liberty. Simple", "deterministic matrix sketching" ],
      "venue" : "ACM,",
      "citeRegEx" : "Lib13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Finding repeated elements",
      "author" : [ "J. Misra", "David Gries" ],
      "venue" : "Science of Computer Programming,",
      "citeRegEx" : "MG82",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Multimodal retrieval with asymmetrically weighted CCA and hierarchical kernel sentence embedding",
      "author" : [ "Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel" ],
      "venue" : "ArXiv,",
      "citeRegEx" : "MMG16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Low rank matrix-valued chernoff bounds and approximate matrix multiplication",
      "author" : [ "Avner Magen", "Anastasios Zouzias" ],
      "venue" : "SODA,",
      "citeRegEx" : "MZ11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Improved approximation algorithms for large matrices via random projections",
      "author" : [ "Tamas Sarlos" ],
      "venue" : null,
      "citeRegEx" : "Sarlos.,? \\Q2006\\E",
      "shortCiteRegEx" : "Sarlos.",
      "year" : 2006
    }, {
      "title" : "A survey of partial least squares (pls) methods",
      "author" : [ "Jacob A. Wegelin" ],
      "venue" : "with emphasis on the two-block case. Technical report,",
      "citeRegEx" : "Weg00",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Comput",
      "author" : [ "David P. Woodruff. Sketching as a tool for numerical linear algebra. Found. Trends Theor" ],
      "venue" : "Sci.,",
      "citeRegEx" : "Woo14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Frequent direction algorithms for approximate matrix multiplication with applications in CCA",
      "author" : [ "Qiaomin Ye", "Luo Luo", "Zhihua Zhang" ],
      "venue" : "IJCAI,",
      "citeRegEx" : "YLZ16",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Methods such as Partial Least Squares (PLS) [Weg00], Canonical Correlation Analysis (CCA)[Hot36], Spectral Co-Clustering [Dhi01], exploit the low rank structure of the correlation matrix to mine the hidden joint factors, by computing the truncated singular value decomposition of a matrix product.",
      "startOffset" : 44,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "Methods such as Partial Least Squares (PLS) [Weg00], Canonical Correlation Analysis (CCA)[Hot36], Spectral Co-Clustering [Dhi01], exploit the low rank structure of the correlation matrix to mine the hidden joint factors, by computing the truncated singular value decomposition of a matrix product.",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "Methods such as Partial Least Squares (PLS) [Weg00], Canonical Correlation Analysis (CCA)[Hot36], Spectral Co-Clustering [Dhi01], exploit the low rank structure of the correlation matrix to mine the hidden joint factors, by computing the truncated singular value decomposition of a matrix product.",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "Approximate Matrix Multiplication (AMM), is gaining an increasing interest in streaming applications (See the recent monograph [Woo14] for more details ).",
      "startOffset" : 127,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "Randomized approaches for AMM were pioneered by the work of [DKM06].",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "The approach of [DKM06] is based on the sampling of l columns of X and Y .",
      "startOffset" : 16,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "[DKM06] shows that by choosing an appropriate sampling matrix Π ∈ R, we obtain a Frobenius error guarantee (‖.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 2,
      "context" : "‖) , such as JL embeddings or efficient subspace embeddings [Sar06, MZ11, ATKZ14, CNW15] that can be applied to any type of matrices X in input sparisty time [CW13].",
      "startOffset" : 158,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "[CNW15] showed that using a subspace embedding Π ∈ R we have with a probability 1− δ:",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 10,
      "context" : "in [Lib13, GLPW15], drawing the connection between covariance matrix sketching, and the classic problem of estimation of frequent items [MG82].",
      "startOffset" : 136,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : "Based on this observation, [YLZ16] proposed to use the frequent directions algorithm of [Lib13] to perform AMM in a deterministic way, we refer to this approach as FDAMM.",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "Based on this observation, [YLZ16] proposed to use the frequent directions algorithm of [Lib13] to perform AMM in a deterministic way, we refer to this approach as FDAMM.",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "FD-AMM [YLZ16] outputs BX , BY such that",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 9,
      "context" : "2 Sketching from Covariance to Correlation In this section we review covariance sketching with the frequent directions algorithm of [Lib13] and state its theoretical guarantees [Lib13, GLPW15].",
      "startOffset" : 132,
      "endOffset" : 139
    }, {
      "referenceID" : 9,
      "context" : "Frequent directions algorithm introduced in [Lib13] (Algorithm 1) achieves this goal.",
      "startOffset" : 44,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "Theorem 1 ([Lib13]) DX the output of algorithm 1 satisfies:",
      "startOffset" : 11,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "For X = Y co-occuring directions reduces to frequent directions of [Lib13], and Theorem 2 recovers Theorem 1 of [Lib13].",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "For X = Y co-occuring directions reduces to frequent directions of [Lib13], and Theorem 2 recovers Theorem 1 of [Lib13].",
      "startOffset" : 112,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "Stronger bounds for frequent directions were given in [GLPW15] where the bound in Equation (4) is improved, for l > 2k, for any k:",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "Hence by defining Z = [X ;Y ] ∈ R (mx+my)×n and applying frequent directions to Z (FD-AMM [YLZ16]), we obtain BX , BY satisfying:",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 16,
      "context" : "A sharper analysis for co-occuring directions remains an open question, but the following discussion of Theorem 3 will shed some light on the advantages of co-occuring directions on FD-AMM [YLZ16].",
      "startOffset" : 189,
      "endOffset" : 196
    }, {
      "referenceID" : 9,
      "context" : "Similarly to the frequent directions [Lib13], co-occuring directions algorithm is simply parallelizable.",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "Sampling [DKM06].",
      "startOffset" : 9,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "Hashing [CW13].",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 16,
      "context" : "FD-AMM [YLZ16].",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 6,
      "context" : "We consider X ∈ R mx×n and Y ∈ Ry, generated using a non-noisy low rank model [GLPW15] as follows: X = VxSxU ⊤ x , where Ux ∈ Rx , (Ux)i,j ∼ N (0, 1), Sx ∈ Rxx is a diagonal matrix with (Sx)jj = 1−(j−1)/kx, and Vx ∈ R mx×kx is such that V ⊤ x Vx = Ikx .",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "For visual features we use the residual CNN Resnet101, [HZRS16].",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "ding HSKE of [MMG16] that results in a feature vector of dimension my = 3000.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "First, whether guarantees of Theorem 2 can be improved akin to the improved guarantees for frequent directions given [GLPW15].",
      "startOffset" : 117,
      "endOffset" : 125
    } ],
    "year" : 2016,
    "abstractText" : "We introduce co-occurring directions sketching, a deterministic algorithm for approximate matrix product (AMM), in the streaming model. We show that co-occuring directions achieves a better error bound for AMM than other randomized and deterministic approaches for AMM. Co-occurring directions gives a (1 + ε)-approximation of the optimal low rank approximation of a matrix product. Empirically our algorithm outperforms competing methods for AMM, for a small sketch size. We validate empirically our theoretical findings and algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}