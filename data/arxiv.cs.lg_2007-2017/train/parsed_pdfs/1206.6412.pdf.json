{
  "name" : "1206.6412.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound",
    "authors" : [ "Ming Ji", "Tianbao Yang", "Binbin Lin", "Rong Jin", "Jiawei Han" ],
    "emails" : [ "mingji1@illinois.edu", "yangtia1@msu.edu", "binbinlin@zju.edu.cn", "rongjin@cse.msu.edu", "hanj@illinois.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Although numerous algorithms have been developed for semi-supervised learning (Zhu (2008) and references therein), most of them do not have theoretical guarantee on improving the generalization performance of supervised learning. A number of theories have been proposed for semi-supervised learning, and most of them are based on one of the two assumptions: (1) the cluster assumption (Seeger, 2001; Rigollet, 2007; Lafferty & Wasserman, 2007; Singh et al., 2008; Sinha & Belkin, 2009) which assumes that two data points should have the same class label or similar values if they are connected by a path passing through a high density region; (2) the manifold assumption (Lafferty & Wasserman, 2007; Niyogi, 2008)\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nwhich states that the prediction function lives in a low dimensional manifold of the marginal distribution PX .\nIt has been pointed out by several studies (Lafferty & Wasserman, 2007; Nadler et al., 2009) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning. However, on the other hand, it was found in (Niyogi, 2008) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn well with relatively few labeled examples. Compared to the manifold assumption, theoretical results based on cluster assumption appear to be more encouraging. In the early studies (Castelli & Cover, 1995; 1996), the authors show that under the assumption that the marginal distribution PX is a mixture of class conditional distributions, the generalization error will be reduced exponentially in the number of labeled examples if the mixture is identifiable. Rigollet (2007) defines the cluster assumption in terms of density level sets, and shows a similar exponential convergence rate given a sufficiently large number of unlabeled examples. Furthermore, Singh et al. (2008) show that the mixture components can be identified if PX is a mixture of a finite number of smooth density functions and the separation/overlap between different mixture components is significantly large. Despite the encouraging results, one major problem of the cluster assumption is that it is difficult to be verified given a limited number of labeled examples. In addition, the learning algorithms suggested in (Rigollet, 2007; Singh et al., 2008; Zhang & Ando, 2005) are difficult to implement efficiently even if the cluster assumption holds, making them unpractical\nfor real-world problems.\nIn this work, we aim to develop a simple algorithm for semi-supervised learning that on one hand is easy to implement, and on the other hand is guaranteed to improve the generalization performance of supervised learning under appropriate assumptions. The main idea of the proposed algorithm is to estimate the top eigenfunctions of the integral operator from the both labeled and unlabeled examples, and learn from the labeled examples the best prediction function in the subspace spanned by the estimated eigenfunctions. Unlike the previous studies of exploring eigenfunctions for semi-supervised learning (Fergus et al., 2009; Sinha & Belkin, 2009), we show that under appropriate assumptions, the proposed algorithm achieves a better generalization error bound than supervised learning algorithms.\nTo derive the generalization error bound, we make a different set of assumptions from previous studies. First, we assume a skewed eigenvalue distribution and bounded eigenfunctions of the integral operator. The assumption of skewed eigenvalue distributions has been verified and used in multiple studies of kernel learning (Koltchinskii, 2011; Steinwart et al., 2006; Minh, 2010; Zhang & Ando, 2005), while the assumption of bounded eigenvectors was mostly found in the study of compressive sensing (Candès & Tao, 2006). Second, we assume that a sufficient number of labeled examples are available, which is also used by the other analysis of semi-supervised learning (Rigollet, 2007). It is the combination of these assumptions that allow us to derive better generalization error bound for semi-supervised learning.\nThe rest of the paper is arranged as follows. Section 2 presents the proposed algorithm and verifies its effectiveness by an empirical study. Section 3 shows the improved generalization error bound for the proposed semi-supervised learning, and Section 4 outlines the proofs. Section 5 concludes with future work."
    }, {
      "heading" : "2. Algorithm and Empirical Validation",
      "text" : "Let X be a compact domain or a manifold in the Euclidean space Rd. Let D = {xi, i = 1, . . . , N |xi ∈ X} be a collection of training examples. We randomly select n examples from D for labeling. Without loss of generality, we assume that the first n examples are labeled by yl = (y1, . . . , yn)\n⊤ ∈ Rn. We denote by y = (y1, . . . , yN )\n⊤ ∈ RN the true labels for all the examples in D. In this study, we assume y = f(x) is decided by an unknown deterministic function f(x). Our goal is to learn an accurate prediction function by\nAlgorithm 1 A Simple Algorithm for Semisupervised Learning\n1: Input • D = {x1, . . . ,xN}: labeled and unlabeled examples\n• yl = (y1, . . . , yn)⊤: labels for the first n examples in D\n• s: the number of eigenfunctions to be used 2: Compute (ϕ̂i, λ̂i), i = 1, . . . , s, the first s eigen-\nfunctions and eigenvalues for the integral operator L̂N defined in (4). 3: Compute the prediction ĝ(x) in (5), where γ∗ = (γ∗1 , . . . , γ ∗ s )\n⊤ is given by solving the following regression problem\nγ∗ = argmin γ∈Rs n∑ i=1  s∑ j=1 γj ϕ̂j(xi)− yi 2 (1) 4: Output prediction function ĝ(·)\nexploiting both labeled and unlabeled examples. Below we first present our algorithm and then verify its empirical performance by comparing to the state-ofthe-art algorithms for supervised and semi-supervised learning."
    }, {
      "heading" : "2.1. A Simple algorithm for Semi-Supervised Learning",
      "text" : "Let κ(·, ·) : X × X → R be a Mercer kernel, and let Hκ be a Reproducing Kernel Hilbert space (RKHS) of functions X → R endowed with kernel κ(·, ·). We assume that κ is a bounded function, i.e., |κ(x,x)| ≤ 1, ∀x ∈ X . Similar to most semi-supervised learning algorithms, in order to effectively exploit the unlabeled data, we need to relate the prediction function f(x) to the unlabeled examples (or the marginal distribution PX ). To this end, we assume there exists an accurate prediction function g(x) ∈ Hκ with ∥g∥Hκ ≤ R. More specifically, we define\nε2 = min h∈Hκ,∥h∥Hκ≤R Ex[(f(x)− h(x))2], (2)\ng(x) = argmin h∈Hκ,∥h∥Hκ≤R\nEx[(f(x)− h(x))2]. (3)\nOur basic assumption (A0) is that the regression error ε2 ≪ R2 is small, and the maximum regression error of g(x) for any x ∈ X is also small, i.e.,\nsup x∈X\n(f(x)− g(x))2 , ε2max = O(nε2/ lnN).\nTo present our algorithm, we define an integral oper-\nator over the examples in D:\nL̂N (f)(·) = 1\nN N∑ i=1 κ(xi, ·)f(xi), (4)\nwhere f ∈ Hκ. Let (ϕ̂i(x), λ̂i), i = 1, 2, . . . , N be the eigenfunctions and eigenvalues of L̂N ranked in the descending order of eigenvalues, where ⟨ϕ̂i(·), ϕ̂j(·)⟩Hκ = δ(i, j) for any 1 ≤ i, j ≤ N . According to (Guo & Zhou, 2011), the prediction function g(x) can be well approximated by a function in the subspace spanned by the top eigenfunctions of L̂N . Hence, we propose to learn a target prediction function ĝ(x) as a linear combination of the first s eigenfunctions, i.e.,\nĝ(x) = s∑\nj=1\nγ∗j ϕ̂j(x), (5)\nwhere s is a parameter that needs to be determined empirically. Coefficients {γ∗i }si=1 in (5) are learned through a simple regression by minimizing the squared error of the labeled examples as shown in (1). Algorithm 1 shows the basic steps of the proposed algorithm.\nImplementation In step 2 of Algorithm 1, we need to compute the eigenvalues and eigenfunctions of L̂N , which is given as follows (Smale & Zhou, 2009). Let K = [κ(xi,xj)]N×N be the kernel matrix for the examples in D, and let {(vi, σi)}si=1 be the first s eigenvectors and eigenvalues of K. Then, the eigenvalues and eigenfunctions of L̂N are given by\nλ̂i = σi N , ϕ̂i(·) = 1 √ σi N∑ j=1 vijκ(xj , ·), i = 1, . . . , s,\nwhere vij is the j-th element of vector v i. Finally, in step 3 of Algorithm 1, we need to compute the optimal coefficient γ∗, which, according to (Bishop, 2006), is given by\nγ∗ = D1/2[V ⊤KBK ⊤ BV ] −1V ⊤KByl,\nwhere D = diag(σ1, . . . , σs), KB = [κ(xi,xj)]N×n includes the kernel similarity between all the examples in D and labeled examples, and V = (v1, . . . ,vs)."
    }, {
      "heading" : "2.2. Empirical study",
      "text" : "Three real-world data sets, i.e., insurance, wine, and temperature 1, are used in our empirical study. The statistics of these datasets are given in Table 1. The first two datasets are from the UC Irvine Machine\n1http://www.remss.com/msu\nLearning Repository (Frank & Asuncion, 2010), while the task of the last dataset is to predict the temperature based on the coordinates (latitude, longitude) on the earth surface. All three datasets are designed for regression tasks with real-valued outputs. We choose these three datasets because they fit in with our assumptions that will be elaborated in section 3.2.\nWe randomly choose 90% of the data for training, and use the rest 10% for testing. We randomly select 2%, 3%, . . . , 9% of the entire dataset as labeled examples. We evaluate the performance by measuring the regression error of the testing data. Each experiment is repeated ten times and the regression errors averaged over the ten trials are reported. Two supervised regression algorithms, i.e., Kernel Ridge Regression (KRR) (Saunders et al., 1998) and Support Vector Regression (SVR) (Drucker et al., 1996), and a state-of-the-art algorithm for semi-supervised regression, i.e., Laplacian Regularized Least Squares (LapRLS) (Belkin et al., 2006), are used as the baselines. We did not include other baseline algorithms for semi-supervised learning because Laplacian regularization yields the state-of-the-art performance of semi-supervised learning. More importantly, our goal is to verify that the proposed algorithm can effectively improve the generalization performance of supervised learning. We refer to the proposed algorithm as Simple Semi-Supervised Learning, or SSSL for short. A RBF kernel function is used for all algorithms, and all the parameters are chosen by cross validation.\nTables 2-4 show the regression errors for the three datasets, respectively. First, as we expected, the performance of all learning algorithms improves as the number of labeled examples increases. It is also not surprising to see that the two semi-supervised learning algorithms perform better than the two supervised learning algorithms. Second, the proposed algorithm (SSSL) outperforms the baseline semi-supervised learning algorithm for almost all the cases, indicating that it is effective for semi-supervised learning. Note that SVR does not perform well on the temperature dataset since this dataset has a perfect manifold structure (the earth surface is a sphere), and SVR fails to capture the manifold structure when the percentage of labeled data is very small."
    }, {
      "heading" : "3. Generalization Error Bounds",
      "text" : "To analyze the generalization performance of the proposed algorithm, we first consider the simple scenario where we have access to an infinite number of unlabeled examples (i.e., the marginal distribution PX ). We then present the generalization error bound for a finite number of unlabeled examples. Detailed analysis can be found in Section 4."
    }, {
      "heading" : "3.1. Generalization error for an infinite number of unlabeled examples",
      "text" : "Given the marginal distribution PX , we define an integral operator L as L(f)(·) = Ex[κ(x, ·)f(x)]. We denote by {(ϕi(·), λi), i = 1, 2, . . .} the eigenfunctions and eigenvalues of L ranked in the descending order of the eigenvalues, where the eigenfunctions are normalized according to the distribution, i.e.,∫ x∈X ϕi(x)ϕj(x)dPX = δij . We note that L̂N , defined in (4), is the empirical version of L, and ∥L− L̂N∥HS approaches to zero as the number of examples goes to infinity, where ∥ · ∥HS is Hilbert Schmidt norm of a linear operator (Smale & Zhou, 2009).\nIn order to achieve a better generalization error bound\nfor the proposed semi-supervised learning algorithm, we make the following assumptions about eigenvalues and eigenfunctions:\n• A1 Skewed eigenvalue distribution. Similar to many studies (Koltchinskii & Yuan, 2010; Steinwart et al., 2006; Minh, 2010), we assume the eigenvalues follow a power law distribution, i.e., there exists a small constant a > 0 and a power index p > 2, such that\nλk ≤ a2k−p, k = 1, 2, . . . .\n• A2 Bounded eigenfunctions. There exists a small constant C such that maxx∈X max\ni |ϕi(x)| ≤ C.\nThis is similar to the incoherence condition specified in compressive sensing (Candès & Tao, 2006).\n• A3 Sufficient number of labeled examples. We require the number of labeled examples to be larger than n0 which is defined as\nn0 = 64C 2 ln2(2N3)\n( Ra\nε\n)4/(p−1) , (6)\nwhere N > 0 is some large number that corresponds to the number of unlabeled examples when we come to the case of finite samples.\nRemark 1 Assumption (A1) ensures that the target function can be approximated, with a small error, by a function in the subspace spanned by the top eigenfunctions of L. This is the foundation behind Algorithm 1.\nRemark 2 Assumptions (A2) and (A3) are introduced to ensure that all the coefficients {γ∗i }si=1 in (5) can be estimated accurately. More specifically, assumption (A3) makes it possible to obtain an accurate estimation of the coefficients {γ∗i }si=1. Assumption A2 ensures that labeled examples are associated with all the top eigenfunctions, and therefore a reliable estimation can be obtained for all the coefficients through the regression analysis. Intuitively, assumption (A2) ensures that |ϕi(xj)|, j ∈ [n] on the labeled examples are not zeros, which is due to E[ϕi(x)] is fixed and maxx |ϕi(x)| is small, otherwise we cannot obtain an accurate estimation of γ∗. Actually, it is notable that we only need to bound the first s eigenfunctions in M(s) = maxx ∑s i=1 ϕ 2 i (x), a key quantity in Proposition 2. From another point of view, if we bound maxx |ϕi(x)| ≤ ∥ϕi∥Hκ = 1/ √ λi (Smale & Zhou, 2009, pg. 9), then if the first s eigenvalues are large, we can expect the maximum value of the first s eigenfuncitons is small. An example satisfying this property is the Sobolev space of functions defined on the domain [0, 1]d with uniform distribution (see (Koltchinskii, 2011, pg. 16)).\nThe following theorem shows the generalization error of Algorithm 1 for an infinite number of unlabeled examples provided that assumptions (A0∼A3) hold. Theorem 1. Assume (A0 ∼ A3) hold. Set s = (aR/ε)2/(p−1). Then, with a probability 1− 2N−3, we have\nEx\n[ (ĝ(x)− f(x))2 ] ≤ O(ε2),\nwhere ĝ(·) is the function learned by Algorithm 1.\nRemark 3 According to (2), ε2 is the optimal regression error that can be achieved by a prediction function in Hκ. Hence, Theorem 1 shows that given an infinite number of unlabeled examples, the prediction function learned by Algorithm 1 achieves almost the optimal performance (up to a constant).\nRemark 4 It is also useful to compare the bound in Theorem 1 to the generalization error bound of supervised learning. According to (Tsybakov, 2008), the\nminimax optimal error if supervised regression (i.e., the best possible regression error of the worst possible distribution) is bounded by Ω(n−p/(p+1)) 2. So if we take the value in assumption (A3) for n ∝ ϵ−4/(p−1), then the generalization error for supervised regression is Ω(ε4p/(p\n2−1)). Compared to our bound (i.e., O(ε2)), when p > 1 + √ 2, we have 4p/(p2 − 1) < 2, implying that the generalization error bound of Algorithm 1 is better than that for supervised regression."
    }, {
      "heading" : "3.2. Generalization error for a finite number of unlabeled examples",
      "text" : "We now consider the scenario where only a finite number (i.e., N) of unlabeled examples are available. The key challenge arising from the finite sample analysis is that we do not have access to the eigenfunctions and eigenvalues of L. Instead, we have to approximate the eigenfunctions and eigenvalues of L by its empirical counterpart L̂N . These approximation errors make the analysis more involved. To ensure that the approximation does not significantly increase the regression error, we make the following assumptions:\n• B1 Skewed eigenvalue distribution of L̂N . We assume eigenvalues λ̂i, i = 1, 2, . . . follow a power law distribution, i.e., there exists a small constant a and power index p > 2, such that\nλ̂k ≤ a2k−p, k = 1, 2, . . . ..\n• B2 Bounded eigenfunctions. There exists a small constant Ĉ such that maxx∈X max\ni |ϕ̂i(x)/\n√ λi| ≤\nĈ.\n• B3 Sufficient number of labeled examples. We require the number of labeled examples to be larger than n0 where n0 is defined as\nn0 = 64Ĉ 2 ln2(2N3)\n( Ra\nε\n)4/(p−1) .\n• B4 Sufficiently large eigengap. Let rs = λs−λs+1 be the gap between the s-th eigenvalue and (s+1)th eigenvalue of L. We assume the eigengap rs is sufficiently large for s = (Ra/ε)2/(p−1), i.e., rs ≥ 3τ\n2/3 N , where τN = 12 lnN√ N .\nRemark 5 Assumptions (B1∼B3) are the “empirical” versions of assumptions (A1∼A3). Note that unlike assumption (A2) where |ϕi(x)| is assumed to be\n2We use Ω(·), instead of O(·), since it is a minimax optimal bound.\nbounded, in assumption (B2), we assume |ϕ̂i(x)/ √ λi| to be bounded. This is because ϕi(x) is normalized with respect to the distribution PX , while ϕ̂i(x) is normalized with respect to the functional norm since the marginal distribution PX is unknown. The most important feature of the finite sample analysis is that we introduce a new assumption (B4), where the number of unlabeled examples N plays an important role to bound the eigengap. This additional assumption is designed to address the approximation error in replacing the eigenfunctions of L with the eigenfunctions of L̂N . Theorem 2. Assume (A0) and (B1∼B3) hold. Set s = (aR/ε)2/(p−1), and assume\nN ≥ max ( 144R2[lnN ]2r−2s ε −2, 144R4a2[lnN ]2ε−4 ) .\nThen, with a probability 1− 4N−3, we have\nEx[(ĝ(x)− f(x))2] ≤ O(ε2).\nAs indicated by Theorem 2, the prediction function learned by Algorithm 1 achieves almost the optimal regression error (up to a constant) provided that all the assumptions hold and the number of unlabeled examples is sufficiently large.\nFinally, to partially verify the assumptions, we examine the eigenvalue distributions for the chosen datasets (described in Section 2.2), as shown in Figure 1. Due to space limitation, we put the figure for the temperature dataset in the supplementary material. We also show in Figure 1 the curves of a2k−p with p = 2.1. It is very clear that the eigenvalues follow a skewed distribution with the power index p > 2."
    }, {
      "heading" : "4. Analysis",
      "text" : "We present the full analysis for the case of infinite number of unlabeled examples, and only sketch the analysis for finite number of unlabeled examples due to lack of space. More detailed analysis can be found in the supplementary materials."
    }, {
      "heading" : "4.1. Analysis for an infinite number of unlabeled examples",
      "text" : "When we have an infinite number of unlabeled examples, the learned prediction function is given by ĝ(x) = ∑s j=1 γ ∗ j ϕj(x), where γ\n∗ = (γ1, · · · , γ∗s )⊤ is obtained by solving the following optimization problem:\nγ∗ = argmin γ L(γ) = n∑ i=1  s∑ j=1 γjϕj(xi)− f(xi) 2  . (7)\nUsing the eigenfunctions of L, we write g(x), the optimal prediction function defined in (3), as g(x) =∑\nj αjϕj(x). We define gs(x), the projection of g(x) into the subspace spanned by the top s eigenfunctions, as\ngs(x) = s∑\nj=1\nαjϕj(x).\nUsing gs(x), we decompose the generalization error of ĝ(x) into two parts, i.e.,\nEx[(ĝ(x)− f(x))2] ≤ 2Ex[(ĝ(x)− gs(x))2] + 2Ex[(gs(x)− f(x))2].\nThe following lemmas bound the two terms on the R.H.S. of the above inequality, separately.\nLemma 1. Under assumption (A1), for any s ≥ 1, we have\nEx [ (gs(x)− f(x))2 ] ≤ 2ε2 + 2a 2R2\nsp−1 , ε2s.\nLemma 2. Under assumptions (A2∼A3) and s = (aR/ϵ)2/(p−1), with a probability at least 1−2N−3, we have\nEx [ (ĝ(x)− gs(x))2 ] ≤ 2η2,\nwhere η2 = 2 ( ε2s + 2εsεmax √ 3 lnN\nn +\nε2max lnN\nn\n) .\nAs indicated by Lemma 1, assumption (A1) guarantees an additional small regression error when constraining the solution to the subspace spanned by the top eigenfunctions of L. As indicated by Lemma 2, assumptions (A2∼A3) ensure that gs(x), the projection of g(x) into the subspace spanned by the top eigenfunctions, can be accurately estimated from the labeled examples. It is easy to see that Theorem 1 immediately follows Lemma 1 and Lemma 2 by noting that ε2s = O(ε\n2) and η2 = O(ε2) when we set s = (Ra/ε)2/(p−1). Below, we show how to prove both lemmas.\nProof of Lemma 1 We first show that ∑∞\ni=s+1 α 2 i\nis bounded. Since ∥g∥Hκ ≤ R, we have\nR2 ≥ ⟨g, g⟩Hκ = ∞∑ i=1 α2i ∥ϕi∥2Hκ = ∞∑ i=1 α2i λi ,\nand therefore\n∞∑ i=s+1 α2i ≤ R2 +∞∑ i=s+1 λi ≤ a2R2 (p− 1)sp−1 ≤ a 2R2 sp−1 .\nThen we bound the regression error of gs(x) as follows: Ex [ (gs(x)− f(x))2 ] ≤ 2Ex [ (g(x)− f(x))2 ] +2Ex\n ∞∑ i,j=s+1 αiαjϕi(x)ϕj(x)  = 2ε2 + 2\n∞∑ i=s+1 α2i ≤ 2ε2 + 2a2R2 sp−1 , ε2s.\nProof of Lemma 2 The proof of Lemma 2 is significantly more involved. We first introduce some notations. Let zi = (ϕ1(xi), . . . , ϕs(xi))\n⊤ be the vector representation of xi derived from the first s eigenfunctions. Let Z = (z1, . . . , zn) include the representations of all labeled examples, and let yl = (f(x1), . . . , f(xn))\n⊤. Using Z, we rewrite L(γ) in (7) as L(γ) = γ⊤ZZ⊤γ − 2γ⊤Zyl + ∥yl∥22. The following proposition bounds Ex[(ĝ(x)− gs(x))2] using the minimum eigenvalue of ZZ⊤. Proposition 1. Assume ZZ⊤ is nonsingular. With a probability at least 1−N−3, we have\nEx [ (ĝ(x)− gs(x))2 ] = ∥αs − γ∗∥22 ≤\nnη2\nλmin(ZZ⊤) .\nThe following proposition bounds the minimum eigenvalue of ZZ⊤. Proposition 2. With a probability at least 1 −N−3, where N > 0 is a large number, we have\n1 n λmin(ZZ ⊤) ≥ 1− 4M(s) ln(2N 3)√ n ,\nwhere M(s) = maxx∈X ∑s i=1 ϕ 2 i (x).\nThe proof for Proposition 1 and 2 can be found in the supplementary materials. Now we are ready to prove Lemma 2.\nAccording to assumptions A2∼A3 and Proposition 2, we have, with a probability at least 1−N−3\n1 n λmin(ZZ ⊤) ≥ 1− 4M(s) ln(2N 3)√ n ≥ 1 2 .\nCombining the above inequality with Proposition 1, we have, with a probability at least 1− 2N−3,\nEx [ (ĝ(x)− gs(x))2 ] ≤ 2η2."
    }, {
      "heading" : "4.2. Analysis for a finite number of unlabeled examples",
      "text" : "Define γ∗ the optimal solution that minimizes the regression error using the eigenfunctions of L̂N , i.e.,\nγ∗ = argmin γ∈Rs n∑ i=1\n( f(xi)−\ns∑ k=1 γkϕ̂k(xi)\n)2 .\nWe further define γ̂∗i = γ ∗ i √ λi, i = 1, · · · , s, and write ĝ(x) learned in the presence of a finite number of un-\nlabeled examples as ĝ(x) = ∑s\ni=1 γ̂ ∗ i ϕ̂i(x)√\nλi . We also in-\ntroduce hs(x) as follows\nhs(x) = s∑\ni=1\nαi ϕ̂i(x)√\nλi .\nwhere {αi}si=1 are the coefficients defined in g(x). Similar to the previous analysis, we bound the generalization error of ĝ(x) by\nEx[(ĝ(x)− f(x))2] ≤ 2Ex[(ĝ(x)− hs(x))2] + 2Ex[(hs(x)− f(x))2].\nWe follow the same path as in the infinite case and present two lemmas to bound the two terms on R.H.S. of the above inequality.\nLemma 3. Under assumptions B1, B3 and N ≥ 144s2p−2[lnN ]2a−2, with a probability at least 1 − 2N−3, we have\nEx[(hs(x)− f(x))2] ≤ 4ε2s + 36R2τ2N\nr2s , ε̂2s.\nLemma 4. Under assumptions B1∼B3, with a probability at least 1− 4N−3, we have\nEx [ (ĝ(x)− hs(x))2 ] ≤ 4η̂2.\nwhere η̂2 = 2 ( ε̂2s + 2ε̂sεmax √ 3 lnN\nn +\nε2max lnN\nn\n) .\nThe proof for Lemma 4 and Lemma 3 can be found in the supplementary materials.\nProof of Theorem 2. Using the condition N ≥ 144R2[lnN ]2/[r2sε 2], we have 36R2τ2N/r 2 s ≤ O(ε2). When we set s = (Ra/ε)2/(p−1), we have ε2s = O(ε 2), ε̂2s = O(ε 2) and η̂ = O(ε2) . By Lemma 3 and Lemma 4, we have, with a probability 1− 4N−3,\nEx[(ĝ(x)− f(x))2] ≤ 2ε̂2s + 8η̂2 = O(ε2)."
    }, {
      "heading" : "5. Conclusions",
      "text" : "In this work, we present a very simple algorithm for semi-supervised learning. Our analysis shows that under appropriate assumptions about the integral operator, the proposed algorithm achieves a better generalization error than a supervised learning algorithm. In the future, we plan to further improve the scalability of the proposed algorithm by exploring different approaches (e.g., the Nyström method) for efficiently estimating eigenfunctions from a large number of unlabeled examples."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The work was supported in part by the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-09-2-0053 (NS-CTA), NSF IIS-0905215, NSF IIS-0643494, U.S. Air Force Office of Scientific Research MURI award FA9550-08-1-0265, and Office of Navy Research (ONR Award N00014-09-1-0663 and N00014-12-1-0431). The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies."
    } ],
    "references" : [ {
      "title" : "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples",
      "author" : [ "Belkin", "Mikhail", "Niyogi", "Partha", "Sindhwani", "Vikas" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Belkin et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Belkin et al\\.",
      "year" : 2006
    }, {
      "title" : "Pattern Recognition and Machine Learning (Information Science and Statistics)",
      "author" : [ "Bishop", "Christopher M" ],
      "venue" : null,
      "citeRegEx" : "Bishop and M.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bishop and M.",
      "year" : 2006
    }, {
      "title" : "Near-optimal signal recovery from random projections: Universal encoding strategies",
      "author" : [ "Candès", "Emmanuel J", "Tao", "Terence" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Candès et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Candès et al\\.",
      "year" : 2006
    }, {
      "title" : "On the exponential value of labeled samples",
      "author" : [ "Castelli", "Vittorio", "Cover", "Thomas M" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "Castelli et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Castelli et al\\.",
      "year" : 1995
    }, {
      "title" : "The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter",
      "author" : [ "Castelli", "Vittorio", "Cover", "Thomas M" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Castelli et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Castelli et al\\.",
      "year" : 1996
    }, {
      "title" : "Support vector regression machines",
      "author" : [ "Drucker", "Harris", "Burges", "Christopher J. C", "Kaufman", "Linda", "Smola", "Alex J", "Vapnik", "Vladimir" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Drucker et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Drucker et al\\.",
      "year" : 1996
    }, {
      "title" : "Semisupervised learning in gigantic image collections",
      "author" : [ "Fergus", "Rob", "Weiss", "Yair", "Torralba", "Antonio" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Fergus et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Fergus et al\\.",
      "year" : 2009
    }, {
      "title" : "An empirical featurebased learning algorithm producing sparse approximations",
      "author" : [ "Guo", "Xin", "Zhou", "Ding-Xuan" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "Guo et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2011
    }, {
      "title" : "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems",
      "author" : [ "Koltchinskii", "Vladimir" ],
      "venue" : null,
      "citeRegEx" : "Koltchinskii and Vladimir.,? \\Q2011\\E",
      "shortCiteRegEx" : "Koltchinskii and Vladimir.",
      "year" : 2011
    }, {
      "title" : "Sparsity in multiple kernel learning",
      "author" : [ "Koltchinskii", "Vladimir", "Yuan", "Ming" ],
      "venue" : "Annuals of Statistics,",
      "citeRegEx" : "Koltchinskii et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koltchinskii et al\\.",
      "year" : 2010
    }, {
      "title" : "Statistical analysis of semi-supervised regression",
      "author" : [ "Lafferty", "John D", "Wasserman", "Larry A" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lafferty et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2007
    }, {
      "title" : "Some properties of gaussian reproducing kernel hilbert spaces and their implications for function approximation and learning theory",
      "author" : [ "Minh", "Ha Quang" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "Minh and Quang.,? \\Q2010\\E",
      "shortCiteRegEx" : "Minh and Quang.",
      "year" : 2010
    }, {
      "title" : "Statistical analysis of semi-supervised learning: The limit of infinite unlabelled data",
      "author" : [ "Nadler", "Boaz", "Srebro", "Nathan", "Zhou", "Xueyuan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Nadler et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nadler et al\\.",
      "year" : 2009
    }, {
      "title" : "Manifold regularization and semi-supervised learning: Some theoretical analyses",
      "author" : [ "P. Niyogi" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Niyogi,? \\Q2008\\E",
      "shortCiteRegEx" : "Niyogi",
      "year" : 2008
    }, {
      "title" : "Generalization error bounds in semisupervised classification under the cluster assumption",
      "author" : [ "Rigollet", "Philippe" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Rigollet and Philippe.,? \\Q2007\\E",
      "shortCiteRegEx" : "Rigollet and Philippe.",
      "year" : 2007
    }, {
      "title" : "Ridge regression learning algorithm in dual variables",
      "author" : [ "Saunders", "Craig", "Gammerman", "Alexander", "Vovk", "Volodya" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Saunders et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Saunders et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning with labeled and unlabeled data",
      "author" : [ "Seeger", "Matthias" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Seeger and Matthias.,? \\Q2001\\E",
      "shortCiteRegEx" : "Seeger and Matthias.",
      "year" : 2001
    }, {
      "title" : "Unlabeled data: Now it helps, now it doesn’t",
      "author" : [ "Singh", "Aarti", "Nowak", "Robert D", "Zhu", "Xiaojin" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Singh et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2008
    }, {
      "title" : "Semi-supervised learning using sparse eigenfunction bases",
      "author" : [ "Sinha", "Kaushik", "Belkin", "Mikhail" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Sinha et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2009
    }, {
      "title" : "Geometry on probability spaces",
      "author" : [ "Smale", "Steve", "Zhou", "Ding-Xuan" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "Smale et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Smale et al\\.",
      "year" : 2009
    }, {
      "title" : "An explicit description of the reproducing kernel hilbert spaces of gaussian rbf kernels",
      "author" : [ "Steinwart", "Ingo", "Hush", "Don R", "Scovel", "Clint" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Steinwart et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Steinwart et al\\.",
      "year" : 2006
    }, {
      "title" : "Introduction to Nonparametric Estimation",
      "author" : [ "Tsybakov", "Alexandre B" ],
      "venue" : "Springer, 1st edition,",
      "citeRegEx" : "Tsybakov and B.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tsybakov and B.",
      "year" : 2008
    }, {
      "title" : "Analysis of spectral kernel design based semi-supervised learning",
      "author" : [ "Zhang", "Tong", "Ando", "Rie Kubota" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Zhang et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2005
    }, {
      "title" : "Semi-supervised learning literature survey",
      "author" : [ "Zhu", "Xiaojin" ],
      "venue" : "Technical report, Computer Sciences, University of Wisconsin-Madison,",
      "citeRegEx" : "Zhu and Xiaojin.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhu and Xiaojin.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "A number of theories have been proposed for semi-supervised learning, and most of them are based on one of the two assumptions: (1) the cluster assumption (Seeger, 2001; Rigollet, 2007; Lafferty & Wasserman, 2007; Singh et al., 2008; Sinha & Belkin, 2009) which assumes that two data points should have the same class label or similar values if they are connected by a path passing through a high density region; (2) the manifold assumption (Lafferty & Wasserman, 2007; Niyogi, 2008)",
      "startOffset" : 155,
      "endOffset" : 255
    }, {
      "referenceID" : 13,
      "context" : ", 2008; Sinha & Belkin, 2009) which assumes that two data points should have the same class label or similar values if they are connected by a path passing through a high density region; (2) the manifold assumption (Lafferty & Wasserman, 2007; Niyogi, 2008)",
      "startOffset" : 215,
      "endOffset" : 257
    }, {
      "referenceID" : 12,
      "context" : "It has been pointed out by several studies (Lafferty & Wasserman, 2007; Nadler et al., 2009) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning.",
      "startOffset" : 43,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "However, on the other hand, it was found in (Niyogi, 2008) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn well with relatively few labeled examples.",
      "startOffset" : 44,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "In addition, the learning algorithms suggested in (Rigollet, 2007; Singh et al., 2008; Zhang & Ando, 2005) are difficult to implement efficiently even if the cluster assumption holds, making them unpractical",
      "startOffset" : 50,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "It has been pointed out by several studies (Lafferty & Wasserman, 2007; Nadler et al., 2009) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning. However, on the other hand, it was found in (Niyogi, 2008) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn well with relatively few labeled examples. Compared to the manifold assumption, theoretical results based on cluster assumption appear to be more encouraging. In the early studies (Castelli & Cover, 1995; 1996), the authors show that under the assumption that the marginal distribution PX is a mixture of class conditional distributions, the generalization error will be reduced exponentially in the number of labeled examples if the mixture is identifiable. Rigollet (2007) defines the cluster assumption in terms of density level sets, and shows a similar exponential convergence rate given a sufficiently large number of unlabeled examples.",
      "startOffset" : 72,
      "endOffset" : 932
    }, {
      "referenceID" : 12,
      "context" : "It has been pointed out by several studies (Lafferty & Wasserman, 2007; Nadler et al., 2009) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning. However, on the other hand, it was found in (Niyogi, 2008) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn well with relatively few labeled examples. Compared to the manifold assumption, theoretical results based on cluster assumption appear to be more encouraging. In the early studies (Castelli & Cover, 1995; 1996), the authors show that under the assumption that the marginal distribution PX is a mixture of class conditional distributions, the generalization error will be reduced exponentially in the number of labeled examples if the mixture is identifiable. Rigollet (2007) defines the cluster assumption in terms of density level sets, and shows a similar exponential convergence rate given a sufficiently large number of unlabeled examples. Furthermore, Singh et al. (2008) show that the mixture components can be identified if PX is a mixture of a finite number of smooth density functions and the separation/overlap between different mixture components is significantly large.",
      "startOffset" : 72,
      "endOffset" : 1134
    }, {
      "referenceID" : 6,
      "context" : "Unlike the previous studies of exploring eigenfunctions for semi-supervised learning (Fergus et al., 2009; Sinha & Belkin, 2009), we show that under appropriate assumptions, the proposed algorithm achieves a better generalization error bound than supervised learning algorithms.",
      "startOffset" : 85,
      "endOffset" : 128
    }, {
      "referenceID" : 20,
      "context" : "The assumption of skewed eigenvalue distributions has been verified and used in multiple studies of kernel learning (Koltchinskii, 2011; Steinwart et al., 2006; Minh, 2010; Zhang & Ando, 2005), while the assumption of bounded eigenvectors was mostly found in the study of compressive sensing (Candès & Tao, 2006).",
      "startOffset" : 116,
      "endOffset" : 192
    }, {
      "referenceID" : 15,
      "context" : ", Kernel Ridge Regression (KRR) (Saunders et al., 1998) and Support Vector Regression (SVR) (Drucker et al.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : ", 1998) and Support Vector Regression (SVR) (Drucker et al., 1996), and a state-of-the-art algorithm for semi-supervised regression, i.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : ", Laplacian Regularized Least Squares (LapRLS) (Belkin et al., 2006), are used as the baselines.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 20,
      "context" : "Similar to many studies (Koltchinskii & Yuan, 2010; Steinwart et al., 2006; Minh, 2010), we assume the eigenvalues follow a power law distribution, i.",
      "startOffset" : 24,
      "endOffset" : 87
    } ],
    "year" : 2012,
    "abstractText" : "In this work, we develop a simple algorithm for semi-supervised regression. The key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression. We show that under appropriate assumptions about the integral operator, this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning. We also verify the effectiveness of the proposed algorithm by an empirical study.",
    "creator" : " TeX output 2012.05.20:2113"
  }
}