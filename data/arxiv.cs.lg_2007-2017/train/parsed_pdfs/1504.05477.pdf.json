{
  "name" : "1504.05477.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stronger Approximate Singular Value Decomposition via the Block Lanczos and Power Methods",
    "authors" : [ "Cameron Musco" ],
    "emails" : [ "cnmusco@mit.edu", "cpmusco@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 4.\n05 47\n7v 1\n[ cs\n.D S]\n2 1\nSimultaneous Iteration is known to give a low rank approximation within (1 + ǫ) of optimal for spectral norm error in Õ(1/ǫ) iterations. We strengthen this result, proving that it finds approximate principal components very close in quality to those given by an exact SVD. Our work bridges a divide between classical analysis, which can give similar bounds but depends critically on singular value gaps, and more recent work, which only focuses on low rank approximation\nFurthermore, we extend our bounds to the Block Lanczos method, which we show obtains the same approximation guarantees in just Õ(1/ √ ǫ) iterations, giving the fastest known algorithm for spectral norm low rank approximation and principal component approximation. Despite their popularity, Krylov subspace methods like Block Lanczos previously seemed more difficult to analyze and did not come with rigorous gap-independent guarantees.\nFinally, we give insight beyond the worst case, justifying why Simultaneous Power Iteration and Block Lanczos can run much faster in practice than predicted. We clarify how simple techniques can potentially accelerate both algorithms significantly."
    }, {
      "heading" : "1 Introduction",
      "text" : "Any matrix A ∈ Rn×d with rank r can be written using a singular value decomposition (SVD) as A = UΣV⊤. U ∈ Rn×r and V ∈ Rd×r have orthonormal columns (A’s left and right singular vectors) and Σ ∈ Rr×r is a positive diagonal matrix containing A’s singular values: σ1 ≥ . . . ≥ σr.\nAmong countless applications, the SVD is often used in machine learning and dimensionality reduction to provide an optimal low rank approximation to A. Specifically, the Eckart-YoungMirsky theorem guarantees that A’s partial SVD can be used to construct a rank k approximation Ak such that both ‖A −Ak‖F and ‖A −Ak‖2 are as small as possible. Ak is simply equal to A projected onto the space spanned by its top k singular vectors. That is, Ak = UkU ⊤ k A.\nThe SVD is also used for principal component analysis (PCA). A’s top singular vector u1 provides a top principal component, which describes the direction of greatest variance within A. The ith singular vector ui provides the i\nth principal component, which is the direction of greatest variance orthogonal to all higher principal components. In other words,\nu⊤i AA ⊤ui = σ 2 i = max\nx:‖x‖2=1 x⊥uj∀j<i\nx⊤AA⊤x,\nwhere AA⊤ is the covariance matrix of A and σi is its ith singular value. Since classical methods for computing a partial SVD are expensive, substantial research has focused on fast, randomized approximation algorithms that seek nearly optimal low rank approximation and PCA [Sar06, MRT06, RST09, HMT11, CW13]. These algorithms have proven to be very fast in practice, especially for large data problems [HMST11, SKT14, IBM14, Tul14].\nIdeally, an approximate partial SVD algorithm will provide good theoretical guarantees for both k rank approximation and PCA. For a specified error ǫ, we hope to return a rank k matrix Z with orthonormal columns z1, . . . , zk satisfying:\nFrobenius Norm Error: ‖A− ZZ⊤A‖2F ≤ (1 + ǫ)‖A−Ak‖2F (1) Spectral Norm Error: ‖A− ZZ⊤A‖22 ≤ (1 + ǫ)‖A−Ak‖2 (2) Per Vector Error: ∀i, ∣ ∣\n∣ u⊤i AA ⊤ui − z⊤i AA⊤zi ∣ ∣ ∣ ≤ ǫσ2k+1 (3)\nFurthermore, we seek runtime bounds that do not depend on properties of A. While substantial literature exists on the convergence of iterative and approximate SVD algorithms, nearly all runtime guarantees depend on the gaps between A’s singular values and become useless when these gaps are small. This limitation is due to a focus on how quickly approximate singular vectors converge to the actual singular vectors of A. When two singular vectors have nearly identical values they are difficult to distinguish, so convergence inherently depends on singular value gaps.\nOnly recently has a shift in approximation goal allowed for algorithms that avoid this dependence, and thus run provably fast for any matrix. For low rank approximation and PCA, we are only concerned with finding a subspace that captures nearly as much information in A as its top singular vectors – distinguishing between two close singular values is overkill."
    }, {
      "heading" : "1.1 Comparing Guarantees",
      "text" : "The Frobenius norm guarantee (1) is well studied and there now exist algorithms achieving (1 + ǫ) error in O(nnz(A)) time, plus lower order terms depending on ǫ, where nnz(A) is the number of nonzero entries inA [CW13]. However, as highlighted in prior work [RST09, HMT11, SKT14] Frobenius\nnorm error is often insufficient, especially in applications to data analysis and machine learning. WhenA has a “heavy-tail” of singular values, as is common for noisy data, ‖A−Ak‖2F = ∑r i=k+1 σ 2 i can be huge, potentially larger than even A’s largest singular value. This renders (1) meaningless since Z does not need to align with any large singular vectors to obtain good multiplicative error.\nTo address this shortcoming, a number of papers [Sar06, Woo14, SKT14] suggest targeting spectral norm error (2). When looking for a rank k approximation, A’s top k singular vectors are usually considered data and the remaining tail is noise. Intuitively, a spectral norm guarantee ensures that ZZ⊤A recovers A up to this noise threshold. A series of work [RST09, HMT11, WC14, BDMI14, Woo14] shows that classical Simultaneous Power Iteration, implemented with randomized start vectors, achieves (1 + ǫ) spectral error.\nHowever, while intuitively stronger for many matrices, (1 + ǫ) spectral error does not imply (1 + ǫ) Frobenius error. Even more concerning, we can construct matrices for which neither low rank approximation guarantee implies any sort of accuracy in Z. Consider an A with its top k+1 squared singular values all equal to 10 followed by a tail of smaller singular values (e.g. 1000k at 1). ‖A−Ak‖22 = 10 but in fact ‖A−ZZ⊤A‖22 = 10 for any rank k matrix Z, leaving the spectral norm bound useless. At the same time, ‖A − Ak‖2F is large, so (1 + ǫ) multiplicative Frobenius error is meaningless as well. For example, any Z obtains ‖A− ZZ⊤A‖2F ≤ (1.01)‖A −Ak‖2F .\nWe address this concern by introducing a per vector guarantee (3) which requires each approximate singular vector z1, . . . , zk to capture nearly as much variance as the corresponding true singular vector. The error bound is very strong in that it depends on ǫσ2k+1 instead of multiplicatively like ∣\n∣u⊤i AA ⊤ui − z⊤i AA⊤zi ∣ ∣ ≤ ǫσ2i , which would be weaker for A’s larger singular vectors. While (3) is reminiscent of the bounds sought in classical numerical analysis [Saa80], we stress that it does not require each zi to converge to ui in the presence of small singular values gaps."
    }, {
      "heading" : "1.2 Our Contributions",
      "text" : "In this paper we re-analyze the decades old Simultaneous Power Iteration and the Block Lanczos methods, combined with simple randomized initializations, for guarantees (1), (2), and (3).\nAlgorithm 1 Simultaneous Iteration input: A ∈ Rn×d, error ǫ ∈ (0, 1), rank k ≤ n, d output: Z ∈ Rn×k. 1: q = Θ(log d/ǫ), Π ∼ N (0, 1)d×k 2: Set K = ( AA⊤ )q AΠ\n3: Orthonormalize the columns of K to obtain Q ∈ Rn×k. 4: Compute M = Q⊤AA⊤Q ∈ Rk×k. 5: Set Ūk to the top k singular vectors of M. 6: return Z = QŪk.\nAlgorithm 2 Block Lanczos input: A ∈ Rn×d, error ǫ ∈ (0, 1), rank k ≤ n, d output: Z ∈ Rn×k. 1: q = Θ(log d/ √ ǫ), Π ∼ N (0, 1)d×k\n2: Set K = [ AΠ, (AA⊤)AΠ, ..., (AA⊤)qAΠ ] 3: Orthonormalize the columns of K to obtain Q ∈ Rn×qk. 4: Compute M = Q⊤AA⊤Q ∈ Rqk×qk. 5: Set Ūk to the top k singular vectors of M. 6: return Z = QŪk.\nTheorem 1 (Main Theorem). With high probability, Algorithms 1 and 2 find approximate singular vectors Z = [z1, . . . , zk] satisfying low rank approximation guarantees (1) and (2), and PCA guarantee (3). For error ǫ, Algorithm 1 requires q = O(log d/ǫ) iterations while Algorithm 2 requires q = O(log d/ √ ǫ) iterations. Excluding lower order terms, both algorithms run in time O(nnz(A)kq).\nOur proof appears in parts as Theorems 6 and 7 (runtime) and Theorems 10, 11, and 12 (accuracy).\nWhile Simultaneous Iteration was known to achieve (2) [Woo14], surprisingly no bounds comparable to (1) and (3) are known. In fact, our analysis is the first to show that an approximation algorithm can achieve per vector guarantees like (3) in runtime independent of singular value gaps.\nPerhaps of greater interest is the fact that our analysis naturally applies to Krylov subspace methods like Block Lanczos. The theory for these methods is more limited, even though they have been proposed, discussed, and tested as a potential improvements over randomized power methods [RST09, HMST11, Hal12]. As highlighted in [SKT14],\n“Despite decades of research on Lanczos methods, the theory for the randomized algorithms is more complete and provides strong guarantees of excellent accuracy, whether or not there exist any gaps between the singular values.”\nTheorem 1 addresses this issue by giving the first gap independent bound for a Krylov subspace method. For guarantees (2) and (3), randomized Block Lanczos gives the fastest known algorithm, improving on the ǫ dependence of Simultaneous Iteration (substantially for small ǫ).\nFinally, in Section 5.3 we use our results to give a very simple alternative analysis that does depend on singular value gaps and can offer significantly faster convergence when A has decaying singular values. It is possible to take further advantage of this result by running Algorithms 1 and 2 with a Π that has > k columns, a very simple modification for accelerating either method."
    }, {
      "heading" : "2 Background and Intuition",
      "text" : "The goal of this section is to 1) provide background on algorithms for approximate singular value decomposition and 2) give intuition for Simultaneous Power Iteration and the Block Lanczos method, justifying why they can give strong gap-independent error guarantees."
    }, {
      "heading" : "2.1 Frobenius Norm Error",
      "text" : "Progress on algorithms for Frobenius norm error low rank approximation (1) has been most considerable. Work in this direction dates back to the strong rank-revealing QR factorizations of Gu and Eisenstat [GE96]. They give deterministic algorithms that run in approximately O(ndk) time, vs. O(ndmin(n, d))1 for a full SVD, and roughly achieve constant factor Frobenius norm error.\nRecently, randomization has been applied to achieve even faster algorithms with (1 + ǫ) error. The paradigm is to compute a linear sketch of A into very few dimensions using either a column sampling matrix or Johnson-Lindenstrauss random projection matrix Π. Typically AΠ has at most poly(k/ǫ) columns and can be used to quickly find Z using a number of methods [Sar06, CEM+15].\nAn×d ×Πd×poly(k/ǫ) = AΠn×poly(k/ǫ)\nThis approach was developed and refined in several pioneering results, including [FKV04, DFK+04, DKM06, DV06] for column sampling, [PTRV00, MRT06] for random projection, and definitive work by Sarlós [Sar06]. Recent work on sparse Johnson-Lindenstrauss type matrices [CW13, MM13, NN13] has brought the cost of Frobenius error low rank approximation down to\n1 By the Abel-Ruffini Theorem, an exact SVD is incomputable even with exact arithmetic – see [TB97]. Accordingly, all SVD algorithm are inherently iterative. Nevertheless, classical methods such as the QR algorithm obtain superlinear convergence rates for the low rank approximation and PCA problems and in any reasonable computing environment, can be taken to run in O(ndmin(n, d)) time.\nO(nnz(A) + n poly(k/ǫ)) time, where the first term is the number of non-zero entries in A and is considered to dominate since typically k ≪ n, d.\nThe sketch-and-solve method is very popular, largely because the the computation of AΠ is easily parallelized and, regardless, pass-efficient in a single processor setting. Furthermore, once a small compression of A is obtained, it can be manipulated in fast memory. This is not typically true of A itself, making it difficult to directly process the original matrix at all. Fast implementations of random projection methods are available through [ME11], [IBM14], and [SKT14]."
    }, {
      "heading" : "2.2 Spectral Norm Error",
      "text" : "Unfortunately, as discussed, Frobenius norm error is often insufficient when A has a heavy singular value tail. Furthermore, it seems an inherent limitation of sampling or random projection methods. The noise from A’s lower r − k singular values corrupts AΠ, making it impossible to extract a good partial SVD if the sum of these singular values (i.e. ‖A−Ak‖2F ) is too large. In other words, any error inherently depends on the size of this tail.\nThis raises a natural question – is there any way to reduce this noise down to the scale of σk+1 = ‖A−Ak‖2 and thus achieve a spectral norm bound like (2)? The answer is yes, and in fact this is exactly the intuition behind the famed power method.\nSimultaneous Power Iteration (Algorithm 1), also known as subspace iteration, or orthogonal iteration, denoises A by working with the powered matrix Aq [Bau57, Rut70]. By the spectral theorem, Aq has exactly the same singular vectors as A, but its singular values are equal to the singular values of A raised to the qth power2. Powering spreads the values apart and accordingly, Aq’s lower singular values are relatively much smaller than its top singular values (see Figure 1a). This effectively reduces the noise in our problem – if we use a sketching method to find a good Z for approximating Aq, even up to Frobenius error, Z will have to align very well with Aq’s large singular vectors.\nSpecifically, q = Õ(1/ǫ) is sufficient to increase any singular value ≥ (1 + ǫ)σk+1 to be significantly (i.e. poly(d) times) larger than any value ≤ σk+1. So ‖Aq − Aqk‖2F is extremely small compared to the top singular values of Aq. In order to achieve even rough multiplicative approximation to this error, Z must align extremely well with every singular vector with value ≥ (1+ ǫ)σk+1. It thus provides an accurate basis for approximating A up to small spectral norm error.\nComputing Aq directly is costly, so AqΠ is computed iteratively. We start with a random Π and repeatedly multiply by A on the left. Since even a rough Frobenius norm approximation for Aq suffices, Π is often chosen to have just k columns. Each each iteration thus takes O(nnz(A)k) time. After AqΠ is computed, Z can simply be set to a basis for its column span. Note that per vector guarantees will require a more careful choice of this basis.\nTo the best of our knowledge, this approach to analyzing Simultaneous Iteration without dependence on singular value gaps began with [RST09]. The technique was popularized in [HMT11] and its analysis improved in [WC14] and [BDMI14]. [Woo14] gives the first bound that directly achieves (2), showing that O(log d/ǫ) power iterations is sufficient for (1 + ǫ) error. All of these papers rely on an improved understanding of the benefits of starting with a randomized Π, which has developed from work on the sketch-and-solve paradigm.\n2 For nonsymmetric matrices, we will work with (AA⊤)qA."
    }, {
      "heading" : "2.3 Beating Simultaneous Iteration with Lanczos",
      "text" : "Numerous papers hint at the possibility of beating Simultaneous Iteration with the Block Lanczos method [CD74, GU77, GLO81], a well studied variant of Lanczos iteration [Lan50], which is the canonical Krylov subspace method for large singular value problems. In particular, [RST09], [HMST11] and [Hal12] suggest and experimentally confirm the potential of randomized Block Lanczos (Algorithm 2) for beating Simultaneous Iteration for low rank approximation. [ME11] also notes the difficulty of beating state-of-the-art Lanczos implementations [Lar01, Lar05] with Simultaneous Iteration.\nThe intuition behind Block Lanczos matches that of many accelerated iterative methods. Simply put, there are better polynomials than Aq for denoising tail singular values. In particular, we can use lower degree polynomials, allowing us to compute fewer powers of A and thus leading to an algorithm with fewer iterations. For example, an appropriately shifted √ q degree Chebyshev polynomial can push the tail of A nearly as close to zero as Aq, even if the long run growth of the polynomial is much lower (see Figure 1b). Specifically, q = Õ(1/ √ ǫ) will increase any singular value ≥ (1 + ǫ)σk+1 to be significantly larger than any singular value below σk+1 – enough to achieve near optimal spectral norm error using a sketching method.\nBlock Lanczos takes advantage of such polynomials by working with the block Krylov subspace,\nK = [ Π AΠ A2Π A3Π . . . A √ qΠ ] ,\nfrom which we can construct p√q(A)Π for any polynomial p√q(·) of degree √ q. Since an effective polynomial for denoising A must be scaled and shifted based on the value of σk+1, we cannot easily compute p√q(A)Π directly. Instead, we argue that the best k rank approximation to A lying in the span of K at least matches the approximation achieved by projecting onto the span of p√q(A)Π. Finding this best approximation will therefore give a nearly optimal low rank approximation to A.\nUnfortunately, there’s a catch. Perhaps surprisingly, it is not clear how to efficiently compute the best spectral norm error low rank approximation to A lying in a specific subspace (e.g. K’s span)\n[BDMI14, SR10]. This challenge precludes an analysis of Krylov methods parallel to the recent work on simultaneous power iteration. Nevertheless, we show that computing the best Frobenius error low rank approximation in the span of K, exactly the post-processing step taken by classic Block Lanczos, will give a good enough spectral norm approximation for achieving (1 + ǫ) error."
    }, {
      "heading" : "2.4 Per Vector Error",
      "text" : "Achieving the per vector guarantee of (3) requires a more nuanced understanding of how Simultaneous Iteration and Block Lanczos denoise the spectrum ofA. The analysis for spectral norm low rank approximation relies on the fact that Aq and p√q(A) blow up any singular value ≥ (1 + ǫ)σk+1 to much larger than any singular value ≤ σk+1. This ensures that the Z outputted by both algorithms aligns very well with the singular vectors corresponding to these large singular values.\nIf σk ≥ (1 + ǫ)σk+1, then Z aligns well with all top k singular vectors of A and we get good Frobenius norm error and the per vector guarantee (3). Unfortunately, when there is a small gap between σk and σk+1, Z could miss intermediate singular vectors whose values lie between σk+1 and (1+ ǫ)σk+1. This is the case where gap dependent guarantees of classical analysis break down.\nNevertheless, we can argue that Aq or, for Block Lanczos, another √ q-degree polynomial in our Krylov subspace, significantly separates singular values > σk+1 from those < (1 − ǫ)σk+1. Thus, each column of Z will align with A at least nearly as well as uk+1. There may be a large subspace of singular vectors with values in the intermediate range [(1− ǫ)σk+1, (1+ ǫ)σk+1]. Our polynomial cannot spread apart these values significantly, so we cannot characterize how Z aligns with this space. However, as long as it avoids singular values below this range, we can guarantee (3).\nFor Frobenius norm low rank approximation, we can argue that the degree to which Z falls outside of the space spanned by the top k singular vectors depends on the number of intermediate singular values between σk+1 and (1− ǫ)σk+1. These are the singular values that may be ‘swapped in’ for the true top k singular values. Since their weight counts towards A’s tail, we can show that the total loss compared to optimal is at worst ǫ‖A−Ak‖2F ."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "Before proceeding to the full technical analysis, we overview required results from linear algebra, polynomial approximation, and randomized low rank approximation."
    }, {
      "heading" : "3.1 Singular Value Decomposition and Low Rank Approximation",
      "text" : "As mentioned, the singular value decomposition can be used to write any A ∈ Rn×d as A = UΣV⊤, where U ∈ Rn×r and V ∈ Rd×r have orthonormal columns and Σ ∈ Rr×r is a positive diagonal matrix containing the singular values of A: σ1 ≥ σ2 ≥ . . . ≥ σr. The singular value decomposition exists for all matrices. The pseudoinverse of A is given by A+ = VΣ−1U⊤. Additionally, for any polynomial p(x), we define p(A) = Up(Σ)V⊤. Note that, since singular values are always take to be non-negative, p(A)’s singular values are given by |p(Σ)|.\nLet Σk be Σ with all but its largest k singular values zeroed out. Let Uk and Vk be U and V with all but their first k columns zeroed out. For any k, Ak = UΣkV\n⊤ = UkΣkV⊤k is the closest rank k approximation to A for any unitarily invariant norm, including the Frobenius norm and spectral norm [Mir60]. The squared Frobenius norm is given by ‖A‖2F = ∑ i,j A 2 i,j = tr(AA ⊤) =\n∑ i σ 2 i . The spectral norm is given by ‖A‖2 = σ1.\n‖A−Ak‖F = min B|rank(B)=k ‖A−B‖F and ‖A−Ak‖2 = min B|rank(B)=k ‖A−B‖2.\nWe often work with the remainder matrixA−Ak and label itAr\\k. Its singular value decomposition is given by Ar\\k = Ur\\kΣr\\kV ⊤ r\\k where Ur\\k, Σr\\k, and V ⊤ r\\k have their first k columns zeroed.\nWhile the SVD gives a globally optimal rank k approximation for A, both Simultaneous Iteration and Block Lanczos will return the best k rank approximation falling within some fixed subspace spanned by a basis Q (with rank ≥ k). For the Frobenius norm, this simply requires projecting A to Q and taking the best rank k approximation of the resulting matrix using an SVD. Lemma 2 (Lemma 4.1 of [Woo14]). Given A ∈ Rn×d and Q ∈ Rm×n with orthonormal columns,\n‖A− ( QQ⊤A )\nk ‖F = ‖A−Q\n( Q⊤A )\nk ‖F = min C|rank(C)=k ‖A−QC‖F .\nThis low rank approximation can be obtained using an SVD (equivalently, eigendecomposition) of the m×m matrix M = Q⊤ ( AA⊤ )\nQ. Specifically, letting M = ŪΣ̄2Ū⊤, then: (\nQŪk ) ( QŪk )⊤ A = Q ( Q⊤A )\nk .\nProof. The first fact is well known in the literature and is given as Lemma 4.1 of [Woo14]. The second fact just follows from noting that, if the SVD of Q⊤A is given by Q⊤A = ŪΣ̄V̄⊤ then M = Q⊤ ( AA⊤ ) Q = ŪΣ̄2Ū⊤. So Q ( Q⊤A )\nk = QŪkΣ̄kV̄ ⊤ k = Q ( ŪkŪ ⊤ k ) ŪΣ̄V̄⊤ = QŪkŪ⊤k Q ⊤A.\nNote that QŪk has orthonormal columns since Ū ⊤ k Q ⊤QŪk = Ū⊤k IŪk = Ik.\nIn general, this rank k approximation does not give the best spectral norm approximation to A falling within Q [BDMI14]. A closed form solution for the best spectral norm approximation can be obtained using the results of [SR10], which are related to Parrott’s theorem. However we do not know an efficient way to compute this solution without essentially performing an SVD of A. It is simple to show at least that, given a rank k basis, the optimal spectral norm approximation for A spanned by that basis is obtained by projecting A to the basis: Lemma 3 (Lemma 4.14 of [Woo14]). For A ∈ Rn×d and Q ∈ Rn×k with orthonormal columns, ‖A−QQ⊤A‖2 = min\nC\n‖A−QC‖2."
    }, {
      "heading" : "3.2 Other Linear Algebra Tools",
      "text" : "Throughout this paper we will use span(M) to denote the column span of the matrix M and will say that a matrix Q is an orthonormal basis for the column span of M if Q has orthonormal columns and QQ⊤M = M. That is, projecting the columns of M to Q fully recovers those columns. QQ⊤ is the orthogonal projection matrix onto the span of Q. (QQ⊤)(QQ⊤) = QIQ⊤ = QQ⊤. If two matrices M and N have the same dimensions and MN⊤ = 0 then ‖M + N‖2F = ‖M‖2F + ‖N‖2F . This matrix Pythagorean theorem follows from the fact that ‖M+N‖2F = tr((M+N)(M+N)⊤). As an example, note that for any orthogonal projection QQ⊤A, A⊤(I − QQ⊤)QQ⊤A = 0 so:\n‖A−QQ⊤A‖2F = ‖A‖2F − ‖QQ⊤A‖2F . This implies for example, that since Ak = UkU ⊤ k A minimizes ‖A − Ak‖2F over all rank k\nmatrices, UkU ⊤ k A maximizes ‖UkU⊤k A‖2F over all rank k orthogonal projections."
    }, {
      "heading" : "3.3 Randomized Low Rank Approximation",
      "text" : "As mentioned, our proofs build on well known sketch-based algorithms for low rank approximation with Frobenius norm error. A short proof of the following Lemma is in Appendix A:\nLemma 4 (Frobenius Norm Low Rank Approximation). Take any A ∈ Rn×d and Π ∈ Rd×k where the entries of Π are independent Gaussians drawn from N (0, 1). If we let Z be an orthonormal basis for span (AΠ), then with probability at least 99/100, for some fixed constant c,\n‖A− ZZ⊤A‖2F ≤ c · dk‖A−Ak‖2F ."
    }, {
      "heading" : "3.4 Chebyshev Polynomials",
      "text" : "As outlined in Section 2.3, our proof also requires polynomials to more effectively denoise the tail of A. As is standard for Krylov subspace methods, we use a variation on the Chebyshev polynomials. The proof of the following Lemma is relegated to Appendix A.\nLemma 5 (Chebyshev Minimizing Polynomial). Given a specified value α > 0, gap γ ∈ (0, 1], and degree q ≥ 1, there exists a degree q polynomial p(x) such that:\n1. p((1 + γ)α) = (1 + γ)α\n2. p(x) ≥ x for all x ≥ (1 + γ)α\n3. p(x) ≤ α 2q √ γ−1 for all x ∈ [0, α]"
    }, {
      "heading" : "4 Implementation and Runtimes",
      "text" : "In this section we briefly discuss runtime and implementation considerations for Algorithms 1 and 2, our randomized variants of Simultaneous Power Iteration and the Block Lanczos methods."
    }, {
      "heading" : "4.1 Simulatenous Iteration",
      "text" : "Algorithm 1 can be modified in a number of ways. Π can be replaced by a random sign matrix, or any matrix achieving the guarantee of Lemma 4. Π may also be chosen with p > k columns. We will discuss in detail how this approach can give improved accuracy in Section 5.3.\nIn our implementation we set Z = QŪk. This ensures that, for all l ≤ k, Zl gives the best rank l Frobenius norm approximation to A within the span of K (See Lemma 2). This is necessary for achieving per vector guarantees for approximate PCA. However, if we are only interested in computing a near optimal low rank approximation, we can simply set Z = Q. Projecting A to QŪk is equivalent to projecting to Q as these two matrices have the same spans.\nTheorem 6 (Simultaneous Iteration Runtime). Algorithm 1 runs in time\nO(nnz(A)k log d/ǫ+ nk2).\nProof. Computing K requires first multiplying A by Π, which takes O(nnz(A)k) time. Computing (\nAA⊤ )i AΠ given ( AA⊤ )i−1\nAΠ then takes O(nnz(A)k) time to first multiply our (n×k) matrix by A⊤ and then by A. This gives a total runtime of O(nnz(A)kq) for computing K.\nFinding Q via Gram-Schmidt orthogonalization or Householder reflections takes O(nk2) time. Computing M by multiplying from left to right requires O(nnz(A)k + nk2) time. M’s SVD then requires O(k3) time using classical techniques (e.g. the QR algorithm). Finally, multiplying Ūk by Q takes time O(nk2). Since we set q = Θ(log d/ǫ), our total runtime is O (\nnnz(A)k log dǫ + nk 2 ) ."
    }, {
      "heading" : "4.2 Block Lanczos",
      "text" : "As with Simultaneous Iteration, we can replace Π with any matrix achieving the guarantee of Lemma 4 and can use p > k columns to improve accuracy (see Section 5.3). Additionally, Q can be computed in a number of ways. In the traditional Block Lanczos algorithm, one starts by computing an orthonormal basis for AΠ, the first block in the Krylov subspace. Bases for subsequent blocks are computed from previous blocks using a three term recurrence that ensures Q⊤AA⊤Q is block tridiagonal, with k × k sized blocks [GU77]. This technique can be useful if qk is large, since it is faster to compute the top singular vectors of a block tridiagonal matrix. However, computing Q using a recurrence can introduce a number of stability issues, and additional steps may be required to ensure that the matrix remains orthogonal [GVL96].\nAn alternative is to compute K explicitly and then compute Q using a QR decomposition. This method is used in [RST09] and [HMST11]. It does not guarantee that Q⊤AA⊤Q is block tridiagonal, but helps avoid a number of stability issues. Furthermore, if qk is small, taking the SVD of Q⊤AA⊤Q will still be fast and typically dominated by the cost of computing K.\nTheorem 7 (Block Lanczos Runtime). Algorithm 2 runs in time\nO\n(\nnnz(A) k log d√\nǫ + n\nk2 log2 d\nǫ +\nk3 log3 d\nǫ3/2\n)\n.\nProof. Computing K requires O(nnz(A)kq) time just like computing K for Simultaneous Iteration (see Theorem 6). The remaining steps are analogous to those in Simultaneous Iteration except somewhat more costly as we work an k ·q dimensional rather than k dimensional subspace. Finding Q takes O(n(kq)2) time. Computing M take O(nnz(A)(kq) + n(kq)2) time and its SVD then requires O((kq)3) time. Finally, multiplying Ūk by Q takes time O(nk(kq)). Plugging in q = Θ(log d/ √ ǫ) gives the claimed runtime."
    }, {
      "heading" : "5 Error Bounds",
      "text" : "We now prove that both Algorithms 1 and 2 return a basis Z that gives relative error Frobenius (1) and spectral norm (2) low rank approximation error as well as the per vector guarantees (3)."
    }, {
      "heading" : "5.1 Main Approximation Lemma",
      "text" : "We first prove a general approximation lemma, which gives three guarantees formalizing the intuition given in Section 2. All other proofs follow nearly immediately from this lemma.\nFor simplicity we assume throughout that k ≤ r ≤ n, d. However, if k is greater than r = rank(A) it can be seen that both algorithms still return a basis satisfying the proven guarantees. We start with a definition:\nDefinition 8. For a given matrix Z ∈ Rn×k with orthonormal columns, letting Zl ∈ Rn×l be the first l columns of Z, we define the error function:\nE(Zl,A) = ‖Al‖2F − ‖ZlZ⊤l A‖2F = ‖A− ZlZ⊤l A‖2F − ‖A−Al‖2F\nRecall that Al is the best rank l approximation to A. This error function measures how well ZlZ ⊤ l A approximates A in comparison to the optimal.\nLemma 9 (Main Approximation Lemma). Let m be the number of singular values σi of A with σi ≥ (1+ǫ/2)σk+1. Let w be the number of singular values with 11+ǫ/2σk ≤ σi < σk. With probability 99/100 Algorithms 1 and 2 return Z satisfying:\n1. ∀l ≤ m, E(Zl,A) ≤ (ǫ/2) · σ2k+1.\n2. ∀l ≤ k, E(Zl,A) ≤ E(Zl−1,A) + 3ǫ · σ2k+1.\n3. ∀l ≤ k, E(Zl,A) ≤ (w + 1) · 3ǫ · σ2k+1.\nProperty 1 captures the intuition given in Section 2.2. Both algorithms return Z with Zl equal to the best Frobenius norm low rank approximation in span(K). Since σ1 ≥ . . . ≥ σm ≥ (1+ ǫ/2)σk+1 and our polynomials separate any values above this threshold from anything below σk+1, Z must align very well with A’s top m singular vectors. Thus E(Zl,A) is very small for all l ≤ m.\nProperty 2 captures the intuition of Section 2.4 – outside of the largest m singular values, Z still performs well. We may fail to distinguish between vectors with values between 11+ǫ/2σk and (1 + ǫ/2)σk+1. However, aligning with the smaller vectors in this range rather than the larger vectors can incur a cost of at most O(ǫ)σ2k+1. Since every column of Z outside of the first m may incur such a cost, there is a linear accumulation as characterized by Property 2.\nFinally, Property 3 captures the intuition that the total error in Z is bounded by the number of singular values falling in the range 11+ǫ/2σk ≤ σi < σk. This is the total number of singular vectors that aren’t necessarily separated from and can thus be ‘swapped in’ for any of the (k − m) true top vectors with singular value < (1 + ǫ/2)σk+1. Property 3 is critical in achieving near optimal Frobenius norm low rank approximation.\nProof. Proof of Property 1\nAssumem ≥ 1. If m = 0 then Property 1 trivially holds. We will prove the statement for Algorithm 2, since this is the more complex case, and then explain how the proof extends to Algorithm 1.\nLet p1 be the polynomial from Lemma 5 with α = σk+1, γ = ǫ/2, and q ≥ c log(d/ǫ)/ √ ǫ for\nsome fixed constant c. We can assume 1/ǫ = O(poly d) and thus q = O(log d/ √ ǫ). Otherwise our Krylov subspace would have as many columns as A and we may as well use a classical algorithm to compute A’s partial SVD directly. Let Y1 ∈ Rn×k be an orthonormal basis for the span of p1(A)Π. Recall that we defined p1(A) = Up1(Σ)V\n⊤. As long as we choose q to be odd, by the recursive definition of the Chebyshev polynomials, p1(A) only contains odd powers of A. Any odd power i can be evaluted as ( AA⊤ )(i−1)/2\nA. Accordingly, p1(A)Π and thus Y1 have columns falling within the span of the Krylov subspace from Algorithm 2 (and hence its column basis Q).\nBy Lemma 4 we have with probability 99/100:\n‖p1(A)−Y1Y⊤1 p1(A)‖2F ≤ cdk‖p1(A)− p1(A)k‖2F . (4)\nFurthermore, one possible rank k approximation of p1(A) is p1(Ak). By the optimality of p1(A)k,\n‖p1(A)− p1(A)k‖2F ≤ ‖p1(A)− p1(Ak)‖2F ≤ d ∑\ni=k+1\np1(σi) 2 ≤ d ·\n(\nσ2k+1\n22q √ ǫ/2−2\n)\n≤ O ( ǫ\n2d2 σ2k+1\n)\n.\nThe last inequalities follow from setting q = Θ(log(d/ǫ)/ √ ǫ) and from the fact that σi ≤ σk+1 =\nα for all i ≥ k + 1 and thus by property 3 of Lemma 5, p1(σi) ≤ σk+1 2q √ ǫ/2−1 . Noting that k ≤ d, we can plug this bound into (4) to get\n‖p1(A)−Y1Y⊤1 p1(A)‖2F ≤ ǫ\n2 σ2k+1. (5)\nApplying the Pythagorean theorem and the invariance of the Frobenius norm under rotation gives\n‖p1(Σ)‖2F − ǫσ2k+1 2 ≤ ‖Y1Y⊤1 Up1(Σ)‖2F .\nY1 falls within A’s column span, and therefore U’s column span. So we can write Y1 = UC for some C ∈ Rr×k. Since Y1 and U have orthonormal columns, so must C. We can now write\n‖p1(Σ)‖2F − ǫσ2k+1 2 ≤ ‖UCC⊤U⊤Up1(Σ)‖2F = ‖UCC⊤p1(Σ)‖2F = ‖C⊤p1(Σ)‖2F .\nLetting ci be the i th row of C, expanding out these norms gives\nr ∑\ni=1\np1(σi) 2 − ǫσ\n2 k+1 2 ≤ r ∑\ni=1\n‖ci‖22p1(σi)2. (6)\nSince C’s columns are orthonormal, its rows all have norms upper bounded by 1. So ‖ci‖22p1(σi)2 ≤ p1(σi) 2 for all i. So for all l ≤ r, (6) gives us\nl ∑\ni=1\n(1− ‖ci‖22)p1(σi)2 ≤ r ∑\ni=1\n(1− ‖ci‖22)p1(σi)2 ≤ ǫσ2k+1 2 .\nRecall that m is the number of singular values with σi ≥ (1 + ǫ/2)σk+1. By Property 2 of Lemma 5, for all i ≤ m we have σi ≤ p1(σi). This gives, for all l ≤ m:\nl ∑\ni=1\n(1− ‖ci‖22)σ2i ≤ ǫσ2k+1 2 and so\nl ∑\ni=1\nσ2i − ǫσ2k+1 2 ≤ r ∑\ni=1\n‖ci‖22σ2i .\nConverting these sums back to norms yields ‖Σl‖2F − ǫσ2k+1 2 ≤ ‖C⊤Σl‖2F and therefore ‖Al‖2F − ǫσ2k+1\n2 ≤ ‖Y1Y⊤1 Al‖2F and\n‖Al‖2F − ‖Y1Y⊤1 Al‖2F ≤ ǫσ2k+1 2 . (7)\nNow Y1Y ⊤ 1 Al is a rank l approximation to A falling within the column span of Y and hence within the column span of Q. By Lemma 2, the best rank l Frobenius approximation to A within Q is given by QŪl(QŪl) ⊤A. So we have\n‖Al‖2F − ‖QŪl(QŪl)⊤A‖2F = E(Zl,A) ≤ ǫσ2k+1 2 ,\ngiving Property 1.\nFor Algorithm 1, we instead choose p1(x) = (1+ǫ/2)σk+1 · (\nx (1+ǫ/2)σk+1\n)2q+1 . For q = Θ(log d/ǫ),\nthis polynomial satisfies the necessary properties: for all i ≥ k + 1, p1(σi) ≤ O ( ǫ 2d2σ 2 k+1 )\nand for all i ≤ m, σi ≤ p1(σi). Further, up to a rescaling, p1(A)Π = K so Y1 spans the same space as K. Therefore since Algorithm 1 returns Z with Zl equal to the best rank l Frobenius norm approximation to A within the span of K, for all l we have:\n‖QŪl(QŪl)⊤A‖2F ≥ ‖Y1Y⊤1 Al‖2F ≥ ‖Al‖2F − ǫσ2k+1 2 ,\ngiving the proof.\nProof of Property 2\nProperty 1 and the fact that E(Zl,A) is always positive immediately gives Property 2 for l ≤ m. So we need to show that it holds for m < l ≤ k. Note that if w, the number of singular values with\n1 1+ǫ/2σk ≤ σi < σk is equal to 0, then σk+1 < 11+ǫ/2σk, so m = k and we are done. So we assume w ≥ 1 henceforth. Again, we first prove the statement for Algorithm 2 and then explain how the proof extends to the simpler case of Algorithm 1.\nIntuitively, Property 1 follows from the guarantee that there is a rank m subspace of span(K) that aligns with A nearly as well as the space spanned by A’s top m singular vectors. To prove Property 2 we must show that there is also some rank k subspace in span(K) whose components all align nearly as well with A as uk, the k\nth singular vector of A. The existence of such a subspace ensures that Z performs well, even on singular vectors in the intermediate range [σk, (1+ ǫ/2)σk+1].\nLet p2 be the polynomial from Lemma 5 with α = 1 1+ǫ/2σk, γ = ǫ/2, and q ≥ c log(d/ǫ)/ √ ǫ for some fixed constant c. Let Y2 ∈ Rn×k be an orthonormal basis for the span of p2(A)Π. Again, as long as we choose q to be odd, p2(A) only contains odd powers of A and so Y2 falls within the span of the Krylov subspace from Algorithm 2. We wish to show that for every unit vector x in the column span of Y2, ‖x⊤A‖2 ≥ 11+ǫ/2σk.\nLet Ainner = Ar\\k −Ar\\(k+w). Ainner = UΣinnerV⊤ where Σinner contains only the singular values σk+1, . . . , σk+w. These are the w intermediate singular values of A falling in the range [\n1 1+ǫ/2σk, σk\n)\n. Let Aouter = A−Ainner = UΣouterV⊤. Σouter contains all large singular values of A with σi ≥ σk and all small singular values with σi < 11+ǫ/2σk.\nLet Yinner ∈ Rn×min{k,w} be an orthonormal basis for the columns of p2(Ainner)Π. Similarly let Youter ∈ Rn×k, be an orthonormal basis for the columns of p2(Aouter)Π.\nEvery column of Yinner falls in the column span of Ainner and hence the column span of Uinner ∈ Rn×w, which contains only the singular vectors of A corresponding to the inner singular values. Similarly, the columns of Youter fall within the span of Uouter ∈ Rn×r−w, which contains the remaining left singular vectors of A. So the columns of Yinner are orthogonal to those of Youter and [Yinner,Youter] forms an orthogonal basis. For any unit vector x ∈ span(p2(A)Π) = span(Y2) we can write x = xinner + xouter where xinner and xouter are orthogonal vectors in the spans of Yinner and Youter respectively. We have:\n‖x⊤A‖22 = ‖x⊤innerA‖22 + ‖x⊤outerA‖22. (8)\nWe will lower bound ‖x⊤A‖22 by considering each contribution separately. First, any unit vector x′ ∈ Rn in the column span of Yinner can be written as x′ = Uinnerz where z ∈ Rw is a unit vector.\n‖x′⊤A‖22 = z⊤U⊤innerAA⊤Uinnerz = z⊤Σ2innerz ≥ (\n1\n1 + ǫ/2 σk\n)2\n≥ (1− ǫ)σ2k. (9)\nNote that we’re abusing notation slightly, using Σinner ∈ Rw×w to represent the diagonal matrix containing all singular values of A with 11+ǫ/2σk ≤ σi ≤ σk without diagonal entries of 0.\nWe next apply the argument used to prove Property 1 to p2(Aouter)Π. The (k + 1) th singular\nvalue of Aouter is equal to σk+w+1 ≤ 11+ǫ/2σk = α. So applying (7) we have for all l ≤ k,\n‖Al‖2F − ‖ (Youter)l (Youter)⊤l Al‖2F ≤ ǫσ2k 2 . (10)\nNote that Aouter has the same top k singular vectors at A so (Aouter)l = Al. Let x ′ ∈ Rn be any unit vector within the column space of Youter and let Youter = (I − x′x′⊤)Youter, i.e the matrix with x′ projected off each column. We can use (10) and the optimality of the SVD for low rank approximation to obtain:\n‖Ak‖2F − ‖YouterY⊤outerAk‖2F = ‖Ak‖2F − ‖YouterY ⊤ outerAk‖2F − ‖x′x′⊤Ak‖2F ≤ ǫσ2k 2\n‖Ak‖2F − ‖Ak−1‖2F − ǫσ2k 2 ≤ ‖x′x′⊤Ak‖2F (1− ǫ/2)σ2k ≤ ‖x′ ⊤ A‖22. (11)\nPlugging (9) and (11) into (8) yields that, for any x in span(Y2), i.e. span(p2(A)Π),\n‖x⊤A‖22 = ‖x⊤innerA‖22 + ‖x⊤outerA‖22 ≥ ( ‖xinner‖22 + ‖xouter‖22 ) (1− ǫ)σ2k ≥ (1− ǫ)σ2k. (12)\nSo, we have identified a rank k subspace Y2 within our Krylov subspace such that every vector in its span aligns at least as well with A as uk.\nNow, for any m ≤ l ≤ k, consider E(Zl,A). We know that given Zl−1, we can form a rank l matrix Zl in our Krylov subspace simply by appending a column x orthogonal to the l− 1 columns of Zl−1 but falling in the span of Y2. Since Y2 has rank k, finding such a column is always\npossible. Since Zl is the optimal rank l Frobenius norm approximation to A falling within our Krylov subspace we have:\nE(Zl,A) ≤ E(Zl,A) = ‖Al‖2F − ‖ZlZ ⊤ l A‖2F\n= σ2l + ‖Al−1‖2F − ‖Zl−1Z⊤l−1A‖2F − ‖xx⊤A‖2F = E(Zl−1,A) + σ2l − ‖xx⊤A‖2F ≤ E(Zl−1,A) + (1 + ǫ/2)2σ2k+1 − (1− ǫ)σ2k+1 ≤ E(Zl−1,A) + 3ǫ · σ2k+1,\nwhich gives Property 2.\nAgain, a nearly identical proof applies for Algorithm 1. We just choose p2(x) = σk\n(\nx σk\n)2q+1 . For\nq = Θ(log d/ǫ) this polynomial satisfies the necessary properties: for all i ≥ k, p1(σi) ≤ O ( ǫ 2d2 σ2k ) and for all i ≤ k, σi ≤ p2(σi).\nProof of Property 3\nBy Properties 1 and 2 we already have, for all l ≤ k, E(Zl,A) ≤ ǫσ2k+1 + (l − m) · 3ǫσ2k+1 ≤ (1 + k −m) · 3ǫ · σ2k+1. So if k −m ≤ w then we immediately have Property 3.\nOtherwise, w < k −m so w < k and thus p2(Ainner)Π ∈ Rn×k only has rank w. It has a null space of dimension k − w. Choose any z in this null space. Then p2(A)Πz = p2(Ainner)Πz + p2(Aouter)Πz = p2(Aouter)Πz. In other words, p2(A)Πz falls entirely within the span of Youter. So, there is a k − w dimensional subspace of span(Y2) that is entirely contained in span(Youter).\nFor l ≤ m+ w, then Properties 1 and 2 already give us E(Zl,A) ≤ ǫσ2k+1 + (l −m) · 3ǫσ2k+1 ≤ (w+1) · 3ǫ · σ2k+1. So consider m+w ≤ l ≤ k. Given Zm, to form a rank l matrix Zl in our Krylov subspace we need to append l −m orthonormal columns. We can choose min{k − w −m, l −m} columns, X1, from the k − w dimensional subspace within span(Y2) that is entirely contained in span(Youter). If necessary (i.e. k−w−m ≤ l−m), We can then choose the remaining l− (k−w) columns X2 from the span of Y2.\nSimilar to our argument when considering a single vector in the span of Youter, letting Youter = (\nI−X1X⊤1 ) Youter, we have by (10):\n‖Ak‖2F − ‖YouterY⊤outerAk‖2F ≤ ǫσ2k 2\n‖Ak‖2F − ‖YouterY ⊤ outerAk‖2F − ‖X1X⊤1 Ak‖2F ≤ ǫσ2k 2\n‖Ak‖2F − ‖Ak−min{k−w−m,l−m}‖2F − ǫσ2k 2\n≤ ‖X1X⊤1 Ak‖2F k ∑\ni=k−min{k−w−m,l−m}+1 σ2i −\nǫσ2k 2 ≤ ‖X1X⊤1 A‖2F .\nBy applying (12) directly to each column of X2 we also have:\n(l + w − k)σ2k − (l + w − k)ǫσ2k ≤ ‖X2X⊤2 A‖2F (l + w − k)σ2k+1 − (l + w − k)ǫσ2k+1 ≤ ‖X2X⊤2 A‖2F .\nAssume that min{k − w −m, l −m} = k − w −m. Similar calculations show the same result when min{k − w −m, l −m} = l −m. We can use the above two bounds to obtain:\nE(Zl,A) ≤ E(Zl,A) = ‖Al‖2F − ‖ZlZ ⊤ l A‖2F\n=\nl ∑\ni=m+1\nσ2i + ‖Am‖2F − ‖ZmZ⊤mA‖2F − ‖X1X⊤1 A‖2F − ‖X2X⊤2 A‖2F\n≤ E(Zm,A) + l ∑\ni=m+1\nσ2i − k ∑\ni=w+m+1\nσ2i + ǫσ2k 2 − (l + w − k)σ2k+1 + (l + w − k)ǫσ2k+1\n≤ m+w ∑\ni=m+1\nσ2i − wσ2k+1 + (l + w − k + 3/2)ǫσ2k+1\n≤ (l + 3w − k + 3/2)ǫσ2k+1 ≤ (w + 1) · 3ǫ · σ2k+1,\ngiving Property 3 for all l ≤ k."
    }, {
      "heading" : "5.2 Error Bounds for Simultaneous Iteration and Block Lanczos",
      "text" : "With Lemma 9 in place, we can easily prove that Simultaneous Iteration and Block Lanczos both achieve the low rank approximation and PCA guarantees (1), (2), and (3).\nTheorem 10 (Near Optimal Spectral Norm Error Approximation). With probability 99/100, Algorithms 1 and 2 return Z satisfying (2):\n‖A− ZZ⊤A‖2 ≤ (1 + ǫ)‖A−Ak‖2.\nProof. Let m be the number of singular values with σi ≥ (1 + ǫ/2)σk+1. If m = 0 then we are done since any Z will satisfy ‖A − ZZ⊤A‖2 ≤ ‖A‖2 = σ1 ≤ (1 + ǫ/2)σk+1 ≤ (1 + ǫ)‖A −Ak‖2. Otherwise, by Property 1 of Lemma 9,\nE(Zm,A) ≤ ǫσ2k+1 2\n‖A− ZmZ⊤mA‖2F ≤ ‖A−Am‖2F + ǫσ2k+1 2 .\nAdditive error in Frobenius norm directly translates to additive spectral norm error. Specifically, applying Theorem 3.4 of [Gu14], which we also prove as Lemma 15 in Appendix A,\n‖A− ZmZ⊤mA‖22 ≤ ‖A−Am‖22 + ǫσ2k+1 2 ≤ σ2m+1 + ǫσ2k+1 2\n≤ (1 + ǫ/2)σ2k+1 + ǫσ2k+1 2 ≤ (1 + ǫ)‖A−Ak‖22. (13)\nFinally, ZmZ ⊤ mA = ZZ ⊤ mA and so by Lemma 3 we have ‖A− ZZ⊤A‖22 ≤ ‖A− ZmZ⊤mA‖22, which combines with (13) to give the result.\nTheorem 11 (Near Optimal Frobenius Norm Error Approximation). With probability 99/100, Algorithms 1 and 2 return Z satisfying (1):\n‖A− ZZ⊤A‖F ≤ (1 + ǫ)‖A−Ak‖F .\nProof. By Property 3 of Lemma 9 we have:\nE(Zl,A) ≤ (w + 1) · 3ǫ · σ2k+1 ‖A− ZZ⊤A‖2F ≤ ‖A−Ak‖2F + (w + 1) · 3ǫ · σ2k+1. (14)\nw is defined as the number of singular values with 11+ǫ/2σk ≤ σi < σk. So ‖A − Ak‖2F ≥ w · (\n1 1+ǫ/2σk\n)2 . Plugging into (14) we have:\n‖A− ZZ⊤A‖2F ≤ ‖A−Ak‖2F + (w + 1) · 3ǫ · σ2k+1 ≤ (1 + 10ǫ)‖A −Ak‖2F .\nAdjusting constants on the ǫ gives us the result.\nTheorem 12 (Per Vector Quality Guarantee). With probability 99/100, Algorithms 1 and 2 return Z satisfying (3):\n∀i, ∣ ∣ ∣ u⊤i AA ⊤ui − z⊤i AA⊤zi ∣ ∣ ∣ ≤ ǫσ2k+1.\nProof. First note that z⊤i AA ⊤zi ≤ u⊤i AA⊤ui. This is because z⊤i AA⊤zi = z⊤i QQ⊤AA⊤QQ⊤zi = σi(QQ ⊤A)2 by our choice of zi. σi(QQ⊤A)2 ≤ σi(A)2 since applying a projection to A will decrease each of its singular values (which follows for example from the Courant-Fischer min-max principle). Then by Property 2 of Lemma 9 we have, for all i ≤ k,\n‖Ai‖2F − ‖ZiZ⊤i ‖2F ≤ ‖Ai−1‖2F − ‖Zi−1Z⊤i−1‖2F + 3ǫσ2k+1 σ2i ≤ ‖ziz⊤i A‖2F + 3ǫσ2k+1 = z⊤i AA⊤zi + 3ǫσ2k+1.\nσ2i = u ⊤ i AA ⊤ui, so simply adjusting constants on ǫ gives the result."
    }, {
      "heading" : "5.3 Improved Convergence With Spectral Decay",
      "text" : "In addition to the traditional Simultaneous Iteration and Block Lanczos methods (Algorithms 1 and 2), our analysis applies to the common modification of running the algorithms with Π ∈ Rn×p for p ≥ k [RST09, HMST11, HMT11]. This technique can significantly accelerate both methods for matrices with decaying singular values. For simplicity, we focus on Block Lanczos, although all arguments immediately extend to the simpler Simultaneous Iteration.\nIn order to avoid inverse dependence on the potentially small singular value gap σkσk+1 − 1, the number of iterations of Block Lanczos inherently depends on 1/ √ ǫ. This ensures that our matrix polynomial sufficiently separates small singular values from larger ones. However, when\nσk > (1 + ǫ)σk+1 we can actually use q = Θ\n(\nlog(d/ǫ)/ √ min{1, σkσk+1 − 1} ) iterations, which\nis sufficient for separating the top k singular values significantly from the lower values. Specifically, in the Block Lanczos case, if we set α = σk+1 and γ =\nσk σk+1 − 1, we know that with\nq = Θ\n(\nlog(d/ǫ)/ √ min{1, σkσk+1 − 1} ) , (5) still holds. We can then just follow the proof of Lemma\n9 and show that Property 1 holds for all l ≤ k (not just for l ≤ m as originally proven). This gives Property 2 and Property 3 trivially.\nFurthermore, for p ≥ k, the exact same analysis shows that q = Θ ( log(d/ǫ)/ √ min{1, σkσp+1 − 1} )\nsuffices. When A’s spectrum decays rapidly, so σp+1 ≤ c · σk for some constant c < 1 and some p not much larger than k, we can obtain significantly faster runtimes. Our ǫ dependence becomes logarithmic, rather than polynomial:\nTheorem 13 (Gap Dependent Convergence). With probability 99/100, for any p ≥ k, Algorithm 1 or 2 initialized with Π ∼ N (0, 1)d×p returns Z satisfying guarantees (1), (2), and (3) as long as we set q = Θ ( log(d/ǫ)/ (\nmin{1, σkσp+1 − 1} )) or Θ\n(\nlog(d/ǫ)/ √ min{1, σkσp+1 − 1} ) , respectively.\nThis theorem may prove especially useful in practice because, on many architectures, multiplying a large A by 2k or even 10k vectors is not much more expensive than multiplying by k vectors. Additionally, it should still be possible to perform all steps for post-processing K in memory, again limiting additional runtime costs due to its larger size.\nFinally, we note that while Theorem 13 is more reminiscent of classical gap-dependent bounds, it still takes substantial advantage of the fact that we’re looking for nearly optimal low rank approximations and principal components instead of attempting to converge precisely to A’s true singular values. This allows the result to avoid dependence on the gap between adjacent singular values, instead varying only with σkσp+1 , which should be much larger."
    }, {
      "heading" : "6 Acknowledgements",
      "text" : "We thank David Woodruff, Aaron Sidford, and Richard Peng for several valuable conversations. Additionally, Michael Cohen was very helpful in discussing many details of this project, including the ultimate form of Lemma 9. This work was partially supported by NSF Graduate Research Fellowship Grant No. 1122374, AFOSR grant FA9550-13-1-0042, DARPA grant FA8650-11-C-7192, and the NSF Center for Science of Information."
    }, {
      "heading" : "A Appendix",
      "text" : "Frobenius Norm Low Rank Approximation\nWe first give a deterministic Lemma, from which the main approximation result follows.\nLemma 14 (Special case of Lemma 4.4 of [Woo14], originally proven in [BDMI14]). Let A ∈ Rn×d have SVD A = UΣV⊤, let S ∈ Rd×k be any matrix such that rank (\nV⊤k S ) = k, and let C ∈ Rn×k be an orthonormal basis for the column span of AS. Then:\n‖A−CC⊤A‖2F ≤ ‖A−Ak‖2F + ‖ (A−Ak)S ( V⊤k S )+ ‖2F .\nLemma 4 (Frobenius Norm Low Rank Approximation). Take any A ∈ Rn×d and Π ∈ Rd×k where the entries of Π are independent Gaussians drawn from N (0, 1). If we let Z be an orthonormal basis for span (AΠ), then with probability at least 99/100, for some fixed constant c,\n‖A− ZZ⊤A‖2F ≤ c · dk‖A−Ak‖2F .\nProof. We follow [Woo14]. Apply Lemma 14 with S = Π. With probability 1, V⊤k S has full rank. So, to show the result we need to show that ‖ (A−Ak)S ( V⊤k S )+ ‖2F ≤ c‖A−Ak‖2F for some fixed c. For any two matrices M and N, ‖MN‖F ≤ ‖M‖F ‖N‖2. This property is known as spectral submultiplicativity. Noting that ‖Ur\\kΣr\\k‖2F = ‖A−Ak‖2F and applying submultiplicativity,\n‖ (A−Ak)S ( V⊤k S )+ ‖2F ≤ ‖Ur\\kΣr\\k‖2F ‖V⊤r\\kS‖22‖ ( V⊤k S )+ ‖22.\nBy the rotational invariance of the Gaussian distribution, since the rows of V⊤ are orthonormal, the entries of V⊤k S and V ⊤ r\\kS are independent Gaussians. By standard Gaussian matrix concentration results (Fact 6 of [Woo14], also in [RV10]), with probability at least 99/100, ‖V⊤r\\kS‖22 ≤ c1 ·max{k, r − k} ≤ c1ḋ and ‖ ( V⊤k S )+ ‖22 ≤ c2k for some fixed constants c1, c2. So,\n‖Ur\\kΣr\\k‖2F ‖V⊤r\\kS‖22‖ ( V⊤k S )+ ‖22 ≤ c · dk‖A−Ak‖2F\nfor some fixed c, yielding the result. Note that we choose probability 99/100 for simplicity – we can obtain a result with higher probability by simply allowing for a higher constant c, which in our applications of Lemma 4 will only factor into logarithmic terms.\nChebyshev Polynomials\nLemma 5 (Chebyshev Minimizing Polynomial). Given a specified value α > 0, gap γ ∈ (0, 1], and degree q ≥ 1, there exists a degree q polynomial p(x) such that:\n1. p((1 + γ)α) = (1 + γ)α\n2. p(x) ≥ x for all x ≥ (1 + γ)α\n3. p(x) ≤ α 2q √ γ−1 for all x ∈ [0, α]\nProof. The required polynomial can be constructed using a standard Chebyshev polynomial of degree q, Tq(x), which is defined by the three term recurrence:\nT0(x) = 1\nT1(x) = x Tq(x) = 2xTq−1(x)− Tq−2(x)\nEach Chebyshev polynomial satisfies the well known property that Tq(x) ≤ 1 for all x ∈ [−1, 1] and we can write the polynomials in closed form [MH02]:\nTq(x) = (x+\n√ x2 − 1)q + (x− √ x2 − 1)q\n2 . (15)\nFor Lemma 5, we simply set:\np(x) = (1 + γ)α Tq(x/α)\nTq(1 + γ) , (16)\nwhich is clearly of degree q and well defined since, referring to (15), Tq(x) > 0 for all x > 1. Now,\np((1 + γ)α) = (1 + γ)α Tq(1 + γ)\nTq(1 + ǫ) = (1 + γ)α,\nso p(x) satisfies property 1. With property 1 in place, to prove that p(x) satisfies property 2, it suffices to show that p′(x) ≥ 1 for all x ≥ (1 + γ)α. By chain rule,\np′(x) = (1 + γ) Tq(1 + γ) T ′q(x/α).\nThus, it suffices to prove that, for all x ≥ (1 + γ),\n(1 + γ)T ′q(x) ≥ Tq(1 + γ). (17)\nWe do this by showing that (1 + γ)T ′q(1 + γ) ≥ Tq(1 + γ) and then claim that T ′′q (x) ≥ 0 for all x > (1 + γ), so (17) holds for x > (1 + γ) as well. A standard form for the derivative of the Chebyshev polynomial is\nT ′q =\n{\n2q (Tq−1 + Tq−3 + . . .+ T1) if q is even, 2q (Tq−1 + Tq−3 + . . .+ T2) + q if q is odd. (18)\n(18) can be verified via induction once noting that the Chebyshev recurrence gives T ′q = 2xT ′ q−1 + 2Tq−1 − T ′q−2. Since Ti(x) > 0 when x ≥ 1, we can conclude that T ′q(x) ≥ 2qTq−1(x). So proving (17) for x = (1 + γ) reduces to proving that\n(1 + γ)2qTq−1(1 + γ) ≥ Tq(1 + γ). (19)\nNoting that, for x ≥ 1, (x+ √ x2 − 1) > 0 and (x− √ x2 − 1) > 0, it follows from (15) that\nTq−1(x) ( (x+ √ x2 − 1) + (x− √ x2 − 1) ) ≥ Tq(x),\nand thus\nTq(x)\nTq−1(x) ≤ 2x.\nSo, to prove (19), it suffices to show that 2(1 + γ) ≤ (1 + γ)2q, which is true whenever q ≥ 1. So (17) holds for all x = (1 + γ).\nFinally, referring to (18), we know that T ′′q must be some positive combination of lower degree Chebyshev polynomials. Again, since Ti(x) > 0 when x ≥ 1, we conclude that T ′′q (x) ≥ 0 for all x ≥ 1. It follows that T ′q(x) does not decrease above x = (1+γ), so (17) also holds for all x > (1+γ) and we have proved property 2.\nTo prove property 3, we first note that, by the well known property that Ti(x) ≤ 1 for x ∈ [−1, 1], Tq(x/α) ≤ 1 for x ∈ [0, α]. So, to prove p(x) ≤ α2q√γ−1 , we just need to show that\n1 Tq(1 + γ) ≤ 1 2q √ γ−1 . (20)\nEquation (15) gives Tq(1+γ) ≥ 12(1+γ+ √ (1 + γ)2 − 1)q ≥ 12(1+ √ γ)q. When γ ≤ 1, (1+√γ)1/ √ γ ≥\n2. Thus, (1 + √ γ)q ≥ 2q √ γ . Dividing by 2 gives Tq(1 + γ) ≥ 2q √ γ−1, which gives (20) and thus property 3.\nAdditive Frobenius Norm Error Implies Additive Spectral Norm Error\nLemma 15 (Theorem 3.4 of [Gu14]). For any A ∈ Rn×d, let B ∈ Rn×d be any rank k matrix satisfying ‖A−B‖2F ≤ ‖A−Ak‖2F + η. Then\n‖A−B‖22 ≤ ‖A−Ak‖22 + η.\nProof. We follow the proof given in [Gu14] nearly exactly, including it for completeness. By Weyl’s monotonicity theorem (Theorem 3.2 in [Gu14]), for any two matrices X,Y ∈ Rn×d with n ≥ d, for all i, j with i+ j − 1 ≤ n we have σi+j−1(X+Y) ≤ σi(X) + σj(X). If we write A = (A−B) +B and apply this theorem, then for all 1 ≥ i ≥ n− k,\nσi+k(A) ≤ σi(A−B) + σk+1(B).\nNote that if n < d, we can just work with A⊤ and B⊤. Now, σk+1(B) = 0 since B is rank k, so:\n‖A−B‖2F ≤ ‖A−Ak‖2F + η n ∑\ni=1\nσ2i (A−B) ≤ n ∑\ni=k+1\nσ2i (A) + η\nn−k ∑\ni=1\nσ2i (A−B) ≤ n ∑\ni=k+1\nσ2i (A) + η\nσ21(A−B) + n−k ∑\ni=2\nσ2i (A) ≤ n ∑\ni=k+1\nσ2i (A) + η\nσ21(A−B) ≤ n ∑\ni=k+1\nσ2i (A)− n−k ∑\ni=2\nσ2i (A) + η\nσ21(A−B) ≤ σ2k+1(A) + η."
    } ],
    "references" : [ {
      "title" : "Das verfahren der treppeniteration und verwandte verfahren zur lösung algebraischer eigenwertprobleme",
      "author" : [ "Friedrich L. Bauer" ],
      "venue" : "Zeitschrift für angewandte Mathematik und Physik ZAMP,",
      "citeRegEx" : "Bauer.,? \\Q1957\\E",
      "shortCiteRegEx" : "Bauer.",
      "year" : 1957
    }, {
      "title" : "Near-optimal columnbased matrix reconstruction",
      "author" : [ "Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Boutsidis et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Boutsidis et al\\.",
      "year" : 2014
    }, {
      "title" : "A block Lanczos algorithm for computing the q algebraically largest eigenvalues and a corresponding eigenspace of large, sparse, real symmetric matrices",
      "author" : [ "Jane Cullum", "W.E. Donath" ],
      "venue" : "In IEEE Conference on Decision and Control including the 13th Symposium on Adaptive Processes,",
      "citeRegEx" : "Cullum and Donath.,? \\Q1974\\E",
      "shortCiteRegEx" : "Cullum and Donath.",
      "year" : 1974
    }, {
      "title" : "Dimensionality reduction for k-means clustering and low rank approximation",
      "author" : [ "Michael B. Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu" ],
      "venue" : "In Proceedings of the 47th Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Cohen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2015
    }, {
      "title" : "Low rank approximation and regression in input sparsity time",
      "author" : [ "Kenneth L. Clarkson", "David P. Woodruff" ],
      "venue" : "In Proceedings of the 45th Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Clarkson and Woodruff.,? \\Q2013\\E",
      "shortCiteRegEx" : "Clarkson and Woodruff.",
      "year" : 2013
    }, {
      "title" : "Clustering large graphs via the singular value decomposition",
      "author" : [ "Petros Drineas", "Alan Frieze", "Ravi Kannan", "Santosh Vempala", "V Vinay" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2004
    }, {
      "title" : "Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix",
      "author" : [ "Petros Drineas", "Ravi Kannan", "Michael W. Mahoney" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2006
    }, {
      "title" : "Adaptive sampling and fast low-rank matrix approximation",
      "author" : [ "Amit Deshpande", "Santosh Vempala" ],
      "venue" : "In Proceedings of the 10th International Workshop on Randomization and Computation (RANDOM),",
      "citeRegEx" : "Deshpande and Vempala.,? \\Q2006\\E",
      "shortCiteRegEx" : "Deshpande and Vempala.",
      "year" : 2006
    }, {
      "title" : "Fast Monte Carlo algorithms for finding low-rank approximations",
      "author" : [ "Alan Frieze", "Ravi Kannan", "Santosh Vempala" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Frieze et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Frieze et al\\.",
      "year" : 2004
    }, {
      "title" : "Efficient algorithms for computing a strong rankrevealing qr factorization",
      "author" : [ "Ming Gu", "Stanley C. Eisenstat" ],
      "venue" : "SIAM J. Sci. Comput.,",
      "citeRegEx" : "Gu and Eisenstat.,? \\Q1996\\E",
      "shortCiteRegEx" : "Gu and Eisenstat.",
      "year" : 1996
    }, {
      "title" : "A block Lanczos method for computing the singular values and corresponding singular vectors of a matrix",
      "author" : [ "Gene H. Golub", "Franklin T. Luk", "Michael L. Overton" ],
      "venue" : "ACM Trans. Math. Softw.,",
      "citeRegEx" : "Golub et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Golub et al\\.",
      "year" : 1981
    }, {
      "title" : "The block Lanczos method for computing eigenvalues",
      "author" : [ "Gene Golub", "Richard Underwood" ],
      "venue" : "Mathematical Software,",
      "citeRegEx" : "Golub and Underwood.,? \\Q1977\\E",
      "shortCiteRegEx" : "Golub and Underwood.",
      "year" : 1977
    }, {
      "title" : "Subspace iteration randomization and singular value problems",
      "author" : [ "Ming Gu" ],
      "venue" : "Computing Research Repository (CoRR),",
      "citeRegEx" : "Gu.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gu.",
      "year" : 2014
    }, {
      "title" : "Matrix Computations",
      "author" : [ "G.H. Golub", "C.F. Van Loan" ],
      "venue" : null,
      "citeRegEx" : "Golub and Loan.,? \\Q1996\\E",
      "shortCiteRegEx" : "Golub and Loan.",
      "year" : 1996
    }, {
      "title" : "Randomized methods for computing low-rank approximations of matrices",
      "author" : [ "Nathan P Halko" ],
      "venue" : "PhD thesis, University of Colorado,",
      "citeRegEx" : "Halko.,? \\Q2012\\E",
      "shortCiteRegEx" : "Halko.",
      "year" : 2012
    }, {
      "title" : "An algorithm for the principal component analysis of large data sets",
      "author" : [ "Nathan Halko", "Per-Gunnar Martinsson", "Yoel Shkolnisky", "Mark Tygert" ],
      "venue" : "SIAM J. Sci. Comput.,",
      "citeRegEx" : "Halko et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Halko et al\\.",
      "year" : 2011
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Halko et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Halko et al\\.",
      "year" : 2011
    }, {
      "title" : "An iteration method for the solution of the eigenvalue problem of linear differential and integral operators1",
      "author" : [ "Cornelius Lanczos" ],
      "venue" : "Journal of Research of the National Bureau of Standards,",
      "citeRegEx" : "Lanczos.,? \\Q1950\\E",
      "shortCiteRegEx" : "Lanczos.",
      "year" : 1950
    }, {
      "title" : "PROPACK: Software for large and sparse SVD calculations",
      "author" : [ "Rasmus Munk Larsen" ],
      "venue" : "Stanford University,",
      "citeRegEx" : "Larsen.,? \\Q2005\\E",
      "shortCiteRegEx" : "Larsen.",
      "year" : 2005
    }, {
      "title" : "Fast algorithms for approximating the singular value decomposition",
      "author" : [ "Aditya Krishna Menon", "Charles Elkan" ],
      "venue" : "ACM Transactions on Knowledge Discovery from Data,",
      "citeRegEx" : "Menon and Elkan.,? \\Q2011\\E",
      "shortCiteRegEx" : "Menon and Elkan.",
      "year" : 2011
    }, {
      "title" : "Symmetric gauge functions and unitarily invariant norms",
      "author" : [ "L. Mirsky" ],
      "venue" : "The Quarterly Journal of Mathematics,",
      "citeRegEx" : "Mirsky.,? \\Q1960\\E",
      "shortCiteRegEx" : "Mirsky.",
      "year" : 1960
    }, {
      "title" : "Low-distortion subspace embeddings in inputsparsity time and applications to robust linear regression",
      "author" : [ "Michael W Mahoney", "Xiangrui Meng" ],
      "venue" : "In Proceedings of the 45th Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Mahoney and Meng.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mahoney and Meng.",
      "year" : 2013
    }, {
      "title" : "A randomized algorithm for the approximation of matrices",
      "author" : [ "Per-Gunnar Martinsson", "Vladimir Rokhlin", "Mark Tygert" ],
      "venue" : "Technical Report 1361,",
      "citeRegEx" : "Martinsson et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Martinsson et al\\.",
      "year" : 2006
    }, {
      "title" : "OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings",
      "author" : [ "Jelani Nelson", "Huy L. Nguyen" ],
      "venue" : "In Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Nelson and Nguyen.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nelson and Nguyen.",
      "year" : 2013
    }, {
      "title" : "Latent semantic indexing: A probabilistic analysis",
      "author" : [ "Christos H. Papadimitriou", "Hisao Tamaki", "Prabhakar Raghavan", "Santosh Vempala" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Papadimitriou et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Papadimitriou et al\\.",
      "year" : 2000
    }, {
      "title" : "A randomized algorithm for principal component analysis",
      "author" : [ "Vladimir Rokhlin", "Arthur Szlam", "Mark Tygert" ],
      "venue" : "SIAM J. Matrix Anal. Appl.,",
      "citeRegEx" : "Rokhlin et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rokhlin et al\\.",
      "year" : 2009
    }, {
      "title" : "Simultaneous iteration method for symmetric matrices",
      "author" : [ "H. Rutishauser" ],
      "venue" : "Numerische Mathematik,",
      "citeRegEx" : "Rutishauser.,? \\Q1970\\E",
      "shortCiteRegEx" : "Rutishauser.",
      "year" : 1970
    }, {
      "title" : "Non-asymptotic theory of random matrices: extreme singular values",
      "author" : [ "Mark Rudelson", "Roman Vershynin" ],
      "venue" : "In Proceedings of the International Congress of Mathematicians 2010 (ICM),",
      "citeRegEx" : "Rudelson and Vershynin.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rudelson and Vershynin.",
      "year" : 2010
    }, {
      "title" : "On the rates of convergence of the Lanczos and the Block-Lanczos methods",
      "author" : [ "Y. Saad" ],
      "venue" : "SIAM Journal on Numerical Analysis,",
      "citeRegEx" : "Saad.,? \\Q1980\\E",
      "shortCiteRegEx" : "Saad.",
      "year" : 1980
    }, {
      "title" : "Improved approximation algorithms for large matrices via random projections",
      "author" : [ "Támas Sarlós" ],
      "venue" : "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Sarlós.,? \\Q2006\\E",
      "shortCiteRegEx" : "Sarlós.",
      "year" : 2006
    }, {
      "title" : "An implementation of a randomized algorithm for principal component analysis",
      "author" : [ "Arthur Szlam", "Yuval Kluger", "Mark Tygert" ],
      "venue" : "Computing Research Repository (CoRR),",
      "citeRegEx" : "Szlam et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Szlam et al\\.",
      "year" : 2014
    }, {
      "title" : "On the minimum rank of a generalized matrix approximation problem in the maximum singular value norm",
      "author" : [ "Kin Cheong Sou", "Anders Rantzer" ],
      "venue" : "In Proceedings of the 19th International Symposium on Mathematical Theory of Networks and Systems (MTNS),",
      "citeRegEx" : "Sou and Rantzer.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sou and Rantzer.",
      "year" : 2010
    }, {
      "title" : "Numerical Linear Algebra",
      "author" : [ "Lloyd N. Trefethen", "David Bau" ],
      "venue" : "Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "Trefethen and Bau.,? \\Q1997\\E",
      "shortCiteRegEx" : "Trefethen and Bau.",
      "year" : 1997
    }, {
      "title" : "Randomized algorithms for low-rank matrix factorizations: Sharp performance",
      "author" : [ "Rafi Witten", "Emmanuel J. Candès" ],
      "venue" : "bounds. Algorithmica,",
      "citeRegEx" : "Witten and Candès.,? \\Q2014\\E",
      "shortCiteRegEx" : "Witten and Candès.",
      "year" : 2014
    }, {
      "title" : "Sketching as a tool for numerical linear algebra",
      "author" : [ "David P. Woodruff" ],
      "venue" : "Foundations and Trends in Theoretical Computer Science,",
      "citeRegEx" : "Woodruff.,? \\Q2014\\E",
      "shortCiteRegEx" : "Woodruff.",
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We re-analyze Simultaneous Power Iteration and the Block Lanczos method, two classical iterative algorithms for the singular value decomposition (SVD). We are interested in convergence bounds that do not depend on properties of the input matrix (e.g. singular value gaps). Simultaneous Iteration is known to give a low rank approximation within (1 + ǫ) of optimal for spectral norm error in Õ(1/ǫ) iterations. We strengthen this result, proving that it finds approximate principal components very close in quality to those given by an exact SVD. Our work bridges a divide between classical analysis, which can give similar bounds but depends critically on singular value gaps, and more recent work, which only focuses on low rank approximation Furthermore, we extend our bounds to the Block Lanczos method, which we show obtains the same approximation guarantees in just Õ(1/ √ ǫ) iterations, giving the fastest known algorithm for spectral norm low rank approximation and principal component approximation. Despite their popularity, Krylov subspace methods like Block Lanczos previously seemed more difficult to analyze and did not come with rigorous gap-independent guarantees. Finally, we give insight beyond the worst case, justifying why Simultaneous Power Iteration and Block Lanczos can run much faster in practice than predicted. We clarify how simple techniques can potentially accelerate both algorithms significantly.",
    "creator" : "LaTeX with hyperref package"
  }
}