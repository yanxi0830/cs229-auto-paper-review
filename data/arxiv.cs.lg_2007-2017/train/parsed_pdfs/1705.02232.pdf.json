{
  "name" : "1705.02232.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Spherical Wards clustering and generalized Voronoi diagrams",
    "authors" : [ "Marek Śmieja", "Jacek Tabor" ],
    "emails" : [ "marek.smieja@ii.uj.edu.pl", "jacek.tabor@ii.uj.edu.pl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nDistribution-based clustering, such as Gaussian mixture model (GMM), has been proven to be very useful in many practical problems [1]. This technique has been widely applied in object detection [2], learning and modeling [3], feature selection [4] or classification [5]. The constructed groups are described by optimally fitted probability distributions. Nevertheless, this kind of methods is limited for the case of Euclidean spaces and the clustering of data with respect to Gaussian-like probability distributions in arbitrary data spaces where only distance or (dis)similarity measure is provided still remains a challenge.\nIn this paper we show how to partially overcome this problem and propose a spherical Wards clustering (SWARDS) which divides data sets with respect to arbitrary dissimilarity measure into groups described by spherical Gaussian-like distributions. Figure 1 shows the relationship between SWARDS and related methods. Moreover, we extend the notion of Voronoi diagram to the case of arbitrary criterion function in non Euclidean spaces and apply it for SWARDS clustering.\nIntroduced method permits an informal interpretation of the notion of spherical Gaussian probability distribution in non Euclidean spaces. The algorithm is capable of discovering spherically-shaped groups of arbitrary sizes (see Example 5.2). Moreover the clustering results are invariant with respect to the scaling of data (see Example 5.1). In fact, data sets with unbalanced groups appear very often in practice, e.g in chemoinformatics where finding of chemical compounds acting on specific disease is rare [6], [7] or in Natural Language Processing where the numbers of documents that belong\nto particular domains are different [8]. Our method can be successfully applied in discovering of populations districts in biological systems modeled by a random walk procedure (see Examples 5.5, 5.6). The method is easy to implement and has the same numerical complexity as the k-means version adapted to non Euclidean spaces [9]. Moreover, our algorithm automatically finds the resultant number of groups by reducing unnecessary clusters on-line. Voronoi diagrams for SWARDS, k-means and their kernelized versions for a mouse-like set with non Euclidean distance function are presented in Figure 2.\nProposed SWARDS method is a combination of spherical variant of Cross-Entropy Clustering (CEC) [10] with the generalized Wards approach [9], [11]. Generally, spherical CEC describes clusters by optimally fitted spherical Gaussian distributions while Wards method allows for its adaptation to non Euclidean case. Spherical CEC performs a clustering by optimizing a cross-entropy criterion function (4). Its form is very flexible since it is based on the within clusters sums of squares, the cardinalities of clusters and the dimension of space.\nApplied Wards approach allows for a generalization of the notion of within cluster sum of squares for the case of any dissimilarity measure [9], [11]. The key lies in the observation that this quantity can be rewritten in Euclidean space without\nCopyright notice: 978-1-4673-8273-1/15/$31.00 c©2015 IEEE\nar X\niv :1\n70 5.\n02 23\n2v 1\n[ cs\n.L G\n] 4\nM ay\n2 01\nthe use of a mean mY of a cluster Y in the form:∑ y∈Y d2(y,mY ) = 1 2|Y | ∑ y,z∈Y d2(y, z).\nOn the other hand, note that a dimension in arbitrary space does not have to be defined. Therefore, to adapt spherical CEC criterion function to general case we recommend to estimate its value from data with use of Maximum Likelihood Estimator of intrinsic dimension [12], [13].\nTo graphically represent and interpret the results of clustering the notion of Voronoi diagram is widely applied. Its construction requires the answer for the question: to which cluster we should associate an arbitrary unclustered point? In the case of classical k-means the answer is simple: we assign the point to the cluster with the nearest center. In the Wards method we replace it by a generalization of distance of point x from the center of cluster Y given by [9]\nd2(x;Y ) := 1 |Y | ∑ y∈Y d2(x, y)− 1 |Y | ss(Y ). (1)\nIn our work we calculate the analogue of above formula (1) for the case of SWARDS criterion function (5) (see (8) for precise formula and Figure 2 for sample effects).\nThe practical properties of proposed method are illustrated and examined on synthetic data sets and examples retrieved form the UCI repository [14]. We compare SWARDS with similar methods which can be applied for non Euclidean data as k-means, Spectral Clustering and their kernelized versions. Our tests demonstrate that introduced method can be applied for populations detection in simple biological systems.\nThe paper is organized as follows. Next section gives a brief description of related clustering methods. In section 3 we recall Wards approach to k-means and present its application for spherical CEC criterion function. Section 4 demonstrates the generalization of Voronoi diagrams to the case of arbitrary criterion functions in non Euclidean data paying particular attention on SWARDS method. The results of experiments and potential applications are given in section 5 while section 6 contains the conclusion."
    }, {
      "heading" : "II. RELATED WORKS",
      "text" : "The hierarchical clustering is probably one of the most popular methods to partition data based on any kind of (dis)similarity measure [15]. The well-known k-means algorithm [16] can also be adapted to non Euclidean data by defining a medoid [17] which plays a role of a generalized notion of mean or by using the Wards method [9], [11] which reformulates the within cluster sum of squares without the notion of the cluster mean. Despite the wide use of these methods, they are sometimes unable to discover groups with complex structures and different sizes. A lot of modifications were also considered to describe clusters with arbitrary shapes [18], [19]. Spectral Clustering uses eigenvectors of similarity matrix to divide elements into groups [20].\nAnother issue of clustering non Euclidean data sets is the appropriate selection of dissimilarity measure. Examples showed that interesting effects can be obtained by applying Gaussian radial basis function (RBF) [21]. The difficulty is that there is no unified methodology how to choose the radius of this function for particular situation [22], [23].\nIn order to perform a distribution-based clustering a GMM is widely used in Euclidean space [1]. Nevertheless it cannot be directly generalized to arbitrary data sets with dissimilarity measures. On the other hand, a family of density based clustering such as DBSCAN [24] can be applied for non Euclidean data. Although the method is capable of discovering clusters of arbitrary shapes and does not require the specification of the number of groups, it does not adopt well to clusters with large differences in densities.\nProposed SWARDS method joins the simplicity and flexibility of k-means with the effects of GMM. Its can be applied in non Euclidean spaces and is based on Gaussian-like probability distributions."
    }, {
      "heading" : "III. CLUSTERING METHOD",
      "text" : "The proposed SWARDS clustering is a combination of spherical Cross-Entropy Clustering (SCEC) [10] with a generalized Wards approach [9], [11]. In this section we first\nintroduce a basic notation and recall the Wards version of kmeans. Then, we show how SCEC can be generalized to non Euclidean data sets via Wards method."
    }, {
      "heading" : "A. Wards method",
      "text" : "Generally, k-means method aims at producing a splitting of data set which optimizes a squared error criterion function. For a group Y ⊂ RN the within cluster sum of squares is defined as:\nss(Y ) = ∑ y∈Y ‖y −mY ‖2,\nwhere mY is a mean of Y . The k-means looks for a partition of X ⊂ RN into k pairwise disjoint sets Y1, . . . , Yk such that the function\nk∑ j=1 ss(Yj)\nis minimal.\nNote that the above formulas cannot be used directly for non vector data since the mean is not well-defined for general data sets. There are several alternatives [25], [26], [27], [28], [29] which allow to partially overcome this difficulty as kmedoids [30] or k-clustering [31]. The technique related to kclustering and k-means is the generalized Wards method [9], [11] which plays the basic role in our investigations. The key idea is the observation that the within cluster sum of squares in Euclidean space can be formulated equivalently without the notion of the center of cluster:\nProposition 3.1: [11] If Y ⊂ RN , then∑ y∈Y ‖y −mY ‖2 = 12|Y | ∑ y∈Y ∑ z∈Y ‖y − z‖2 ,\nwhere |Y | is a cardinality of Y .\nThis allows to reasonably generalize the within cluster sum of squares to general non Euclidean data set. For this purpose let X be an arbitrary data set and let d : X × X → [ 0 , + ∞ ) be a symmetric dissimilarity measure on X , i.e,\n• d(y, y) = 0,\n• d(y, z) = d(z, y),\nfor y, z ∈ X . Given two subsets Y,Z of X we define a function [31] connected with the average linkage function (also called average neighbor function) [32], [27] as:\nD〈Y, Z〉 := ∑ y∈Y ∑ z∈Z d2(y, z) .\nAs a generalized within cluster Y ⊂ X sum of squares we put [9]:\nss(Y ) := 12|Y |D〈Y, Y 〉 = 1 2|Y | ∑ y∈Y ∑ z∈Y d2(y, z) . (2)\nThen, the goal of Wards method is formulated as follows:\nWards Optimization Problem [9]. Let X be a data set with a dissimilarity measure d and let k ∈ N. Find a splitting\nof X into k pairwise disjoint sets Y1, . . . , Yk which minimizes the generalized squared error function:\nEWards(Y1, . . . , Yk) := k∑ i=1 ss(Yi), (3)\nwhere ss(·) is defined by (2)."
    }, {
      "heading" : "B. Spherical Wards criterion function",
      "text" : "The Cross-Entropy Clustering (CEC) is a kind of distribution-based clustering which divides an Euclidean data set into groups such that each group is described by optimally fitted Gaussian probability distribution [10]. The effects of the clustering are similar to those obtained by GMM, but the optimizing criterion function is different. Its value determines the statistical code length of memorization of an arbitrary element of a data set in the case when each cluster uses its own coding algorithm. In particular, the introducing of one more cluster (coding algorithm) requires an additional cost of its identification (increase of the entropy). In consequence, the maintaining of too many clusters is not optimal and it allows for the automatic reduction of unnecessary groups. Another advantage of CEC is that the clustering is performed in a comparable time to computationally efficient k-means method. For more details the reader is referred to [10], [33], [34].\nSpherical Cross-Entropy Clustering (SCEC) is a variant of CEC which takes into account the family of spherical Gaussian distributions. Since for every group the optimal spherical Gaussian distribution is matched, then data set is partitioned into spherically-shaped clusters. For a splitting Y1, . . . , Yk of X the associated criterion function is defined by [10]\nN 2 ln( 2πe N ) + k∑ i=1 |Yi| |X| · [ − ln |Yi||X| + N 2 ln ( |X| |Yi| tr(ΣYi) )] ,\n(4) where ΣY is a covariance matrix of group Y and tr(ΣY ) is a trace of ΣY .\nLet us first observe that the notion of covariance matrix can be easily removed from the expression (4).\nProposition 3.2: If Y ⊂ RN then [10]: tr(ΣY ) = ss(Y ).\nIn consequence the application of Wards approach (2) facilitates its interpretation in non Euclidean case for a fixed N > 0.\nFor fully explanation of the formula (4) in the context of non Euclidean space, the value of dimension N has to be specified. As the most reasonable way to set this value we recommend to use the estimation of a dimension of X . In the present study we apply the Maximum Likelihood Estimation (MLE) of intrinsic dimension of X proposed in [12] and modified in [13]. More precisely, given X = {x1, . . . , xn} the maximum likelihood estimator of a dimension N of X calculated for each x ∈ X equals [12]:\nN̂k(x) = 1\nk − 1 k−1∑ j=1 log d(x, xk) d(x, xj) ,\nfor k ∈ {1, . . . , n}. Since the above value is dependent on the choice of k and x, then one should average the results over\nx ∈ X and K̃ ⊂ {1, . . . , n} to obtain the final estimator of N [13].\nNevertheless, one can tune this value in the learning process as well as may set it to any positive number. In the experimental section we show that for high values of N more clusters are created in the clustering while for low values of N the method prefers to reduce a number of groups. From now on, N will be treated as a free parameter selected by the user, but we keep in mind that the easiest way to tune this value is to use the MLE procedure described above.\nAll in all, the generalized Wards approach and the appropriate choice of the dimension parameter N allow for the understanding of spherical cross-entropy criterion function in arbitrary data set with a dissimilarity measure. In consequence, the informal notion of spherical Gaussian probability distribution based on any dissimilarity measure could be considered. We conclude this subsection with a formulation of spherical Wards (SWARDS) optimization problem:\nSpherical Wards Optimization Problem. Let X be a data set with a dissimilarity measure d, n ∈ N be an initial number of clusters and N > 0 be a free parameter. Find k ≤ n and a partition Y1, . . . , Yk of X which minimizes spherical Wards criterion function\nEsWards(Y1, . . . , Yk;N) :=\nN 2 ln( 2πe N ) + k∑ i=1 |Yi| |X| · [ N 2 ln(ss(Yi))− N+2 2 ln ( |Yi| |X| )] ,\n(5) where ss(·) is defined by (2)."
    }, {
      "heading" : "C. Clustering algorithm",
      "text" : "One can show that the natural modification of the Hartigan algorithm [9], [16], [10] can be used to minimize the SWARDS criterion function (5). We will now discuss its technical aspects.\nThe procedure can be divided into two parts: initialization and iteration. In the initialization phase n ∈ N groups are created randomly. During iteration the algorithm reassigns elements between clusters in order to minimize the SWARDS criterion function (5).\nMore precisely, in the iteration part we repeatedly go over all elements of X applying the following steps:\n1) Reassign x ∈ X to this cluster for which the decrease of energy (5) is maximal, 2) If a probability of some cluster is less than a fixed number ε > 0, then remove this cluster and assign its elements to these groups for which the increase of energy (5) is minimal,\nuntil no group membership has been changed.\nThe number ε was introduced to speed up the reduction of redundant clusters. In our experiments we always use the value ε = 1%. Thus, the group is removed if it contains less than 1% of all elements of X . Clearly, the procedure is not deterministic and leads to a local minimum of (5) [25]. Therefore, to provide the satisfactory results the algorithm\nshould be evaluated several times – the final result is that which gives the minimal value of SWARDS criterion function.\nThe above algorithm can be seen as an online version of standard partitional clustering procedure which is able to reduce unnecessary groups. Every time the element is processed the clusters parameters are recalculated. This implies that to efficiently apply this procedure we have to recompute\nss(Y ∪ {x}) and ss(Y \\ {x}). For this purpose the following formulas are useful:\nProposition 3.3: [11] Let Y ⊂ X and x ∈ X . a) If x 6∈ Y , then\nss(Y ∪ {x}) = |Y | |Y |+ 1 ss(Y ) + 1 |Y |+ 1 D〈{x}, Y 〉.\nb) If x ∈ Y , then\nss(Y \\ {x}) = |Y | |Y | − 1 ss(Y )− 1 |Y | − 1 D〈{x}, Y 〉.\nGiven k clusters, the computational complexity of one iteration of standard Hartigan procedure requires about k ·N · |X| operations (for data sets contained in RN ). When applying the Wards approach this complexity changes to k ·|X|2 operations. Since the mean of cluster is not defined in general situation, one has to pay an additional cost of recalculating the within cluster sum of squares during every reassigning. However, we do not need to recalculate the distance between the reassigning elements and the mean of a cluster which decreases the computational cost N times."
    }, {
      "heading" : "IV. GENERALIZED VORONOI DIAGRAM",
      "text" : "There arises a natural problem how to graphically present the clustering results. Clearly, we can mark the elements of each cluster with different label. However, in practice it is usually more clear to show the division of the whole space. In this section we show that we can naturally obtain an equivalence of the Voronoi diagram for any criterion function in non Euclidean space. In particular we apply these results to define the Voronoi diagram for SWARDS."
    }, {
      "heading" : "A. Classical diagram",
      "text" : "Let us recall that in the case of classical version of Voronoi diagram (k-means method) the point x is associated with this cluster whose center is the closest to x. More precisely, it is classified to this cluster Yi which minimizes d(x; mi), where mi is a mean of Yi. We would like to mention that one can consider the alternative to the Voronoi diagrams as described in [35]. It provides the partition of data but does not induce a natural partition of the space (see [35] for more details).\nTo generalize the notion of the Voronoi diagram to non Euclidean space (Wards k-means), we need to be able to compute the distance of a point from the center of the cluster (without using it in the computations).\nProposition 4.1: [11] Let x ∈ RN be fixed and Y ⊂ RN be a subset of RN with mean mY . Then\n‖x−mY ‖2 = 1 |Y | ∑ y∈Y ‖x− y‖2 − 1 2|Y |2 ∑ y∈Y ∑ z∈Y ‖y − z‖2.\nThe above allows the formulation of the analogue of the square of the “classical” distance of a point x from the center of Y . Let Y be a subset of data space X with a dissimilarity measure d and let x ∈ X be fixed. We define the mean square distance of x from Y by\nd2(x;Y ) := 1\n|Y | (D〈{x}, Y 〉 − ss(Y )). (6)\nApplying the above formula one can draw the equivalence of the Voronoi diagram for Wards k-means, i.e. an element x ∈ X is classified to this cluster which minimizes (6)."
    }, {
      "heading" : "B. Diagram for arbitrary criterion function",
      "text" : "We are now going to present a reasoning which allows to create a kind of Voronoi diagram for arbitrary criterion function. This will be useful for constructing a division of the space for the case of SWARDS method. Obtained results are consistent with the classical Voronoi diagram in the case of Wards k-means presented in previous section.\nLet X be a space with a dissimilarity measure d and let Y ⊂ X represent our data. We extend X by introducing a weight function\nw : X 3 x→ {\nw(x) ∈ [0,+∞) , x ∈ Y, 0 , x ∈ X \\ Y,\nwhich assigns a weight to every element of X . Then we consider an extended data set\nY w = {(y, w(y)) : y ∈ Y }.\nWe define the operations D〈·, ·〉 and ss(·) adapted for Y w. Given Z, Y1, Y2 ⊂ Y we put:\n1) |Zw| := ∑ z∈Z w(z),\n2) D〈Y w1 , Y w2 〉 := ∑\ny1∈Y1 ∑ y2∈Y2 d2(y1, y2)w(y1)w(y2),\n3) ss(Zw) := 12|Zw|D〈Z w, Zw〉.\nThen the analogue of k-means criterion function equals:\nEWards(Y w 1 , . . . , Y w k ) = k∑ i=1 ss(Y wi ), (7)\nwhere Y1, . . . , Yk is a splitting of Y . If w|Y ≡ 1 then (7) coincides with (3).\nIn order to explain our technique assume that Y1, . . . , Yk is a splitting of data set Y and E is an arbitrary criterion function. For a fixed point x ∈ X we consider a mapping\nEix,[Y w1 ,...,Y wk ] : h→ E(Y w1 , . . . , Y w i−1, (Yi ∪ {x})w+hδx , Y wi+1, . . . , Y wk ),\nwhere h ≥ 0 and i ∈ {1, . . . , k}. It determines the value of criterion function E when x ∈ X is associated with i-th cluster with a weight increased by h.\nWe define the functions (wherever they exist)\n∂iE(x, [Y w 1 , . . . , Y w k ]) := (E i x,[Y w1 ,...,Y w k ] )′(0),\nfor i ∈ {1, . . . , k}. Observe that ∂iE coincides with the infinitesimal change in energy when we add x to the i-th cluster.\nThus, in Voronoi diagram the point x ∈ X should be assigned to this cluster which minimizes ∂iE(x, [Y w1 , . . . , Y w k ]).\nLet us show that the above reasoning is consistent with the classical results (6) for Wards k-means criterion function (7):\nTheorem 4.1: Let Y be a subset of a space X with a dissimilarity measure d and let w(y) = 1, for all y ∈ Y , be a weight function. If E denotes the squared error function (7) and Y1, . . . , Yk is a fixed splitting of Y then\n∂iE(x, [Y w 1 , . . . , Y w k ]) = d 2(x;Yi),\nfor x ∈ X and i ∈ {1, . . . , k}.\nProof: Let h > 0. By Corollary 3.3, we have 1 h [E(Y w 1 , . . . , Y w i−1, Y w i ∪ {(x, h)}, Y wi+1, . . . , Y wk )\n−E(Y w1 , . . . , Y wk )] = 1h [ ss((Yi ∪ {x})w+hδx)− ss(Y wi ) ] = 1h [ |Y wi |ss(Y w i )+D〈{(x,h)},Y w i 〉 |Y wi |+h − ss(Y wi )\n] = 1h |Y wi |ss(Y w i )+hD〈{(x,1)},Y w i 〉−(|Y w i |+h)ss(Y w i )\n|Y wi |+h\n= D〈(x,1),Y wi 〉−ss(Y w i )\n|Y wi |+h .\nSince w|Y ≡ 1 then D〈(x,1),Y wi 〉−ss(Y w i )\n|Y wi |+h = D〈x,Yi〉−ss(Yi)|Y wi |+h → 1 |Yi| (D〈x, Yi〉 − ss(Yi)) , as h→ 0,\nwhich yields the assertion of the theorem.\nC. Voronoi diagram for SWARDS\nThe following theorem presents how to create the Voronoi diagram for SWARDS criterion function:\nTheorem 4.2: Let Y be a subset of a space X with a dissimilarity measure d and let w(y) = 1, for all y ∈ Y , be a weight function. If E denotes the SWARDS criterion function for a data set with weights and Y1, . . . , Yk is a fixed splitting of Y then\n∂iE(x, [Y w 1 , . . . , Y w k ])\n= 1|X| [ N 2 ( ln(ss(Yi)) + |Yi|d 2(x;Yi) ss(Yi) ) − N+22 (ln |Yi|+ 1) ] .\nProof: Roughly speaking, Theorem 4.1 says that ∂iss(Y w i ) = d\n2(x;Yi). Moreover, ∂i|Y wi | = 1. Applying the operator ∂i and the above to (5) we easily get the assertion of the theorem.\nConsequently, given a partition Y1, . . . , Yk of Y , to associate a point x ∈ X to a cluster it is sufficient to find i ∈ {1, . . . , k} which minimizes\nEYi(x) = ln(ss(Yi)) + |Yi| d2(x;Yi) ss(Yi) − (1 + 2 N ) ln |Yi|. (8)\nIf X is infinite, then one can apply its quantization into a finite number of regions before applying a Voronoi diagram. The reader is referred to Figure 3 for more detailed explanation of the above described procedure."
    }, {
      "heading" : "V. EXPERIMENTS",
      "text" : "In this section we discuss some fundamental properties as well as the potential applications of proposed clustering method and present a short evaluation study. The implementation of SWARDS is available from http://www.ii.uj.edu.pl/˜smieja/sWards-app.zip1."
    }, {
      "heading" : "A. Synthetic data sets",
      "text" : "In order to show the capabilities of SWARDS we examined its resistance on the change of scale and its sensitivity on the unbalanced data. We compared the clustering results with the ones obtained with use of related methods which can be applied for non Euclidean spaces: Wards k-means and Spectral Clustering (kernlab R package was used for the implementations of this algorithm [36]). Since SWARDS automatically detects the resultant number of groups, then we ran it with 10 initial clusters while the other methods used the number of groups returned by SWARDS2. The value of parameter N (dimension of space) for SWARDS was set automatically with use of MLE method [12], [13]. To provide more stable results, each algorithm was run 10 times and the result with the lowest value of criterion function was chosen.\nExample 5.1: Scale invariance\nIn the first experiment we examined the invariance of algorithms on the change of scale. A data set was generated from the mixture of two spherical Gaussian distributions,\n1 2 G1(r) + 1 2 G2(1− r)\nwith different covariance matrices\nC1 = ( r 0 0 r ) , C2 = ( 1− r 0 0 1− r ) , for r ∈ (0, 1),\ncentered at m1 = (−1, 0) , m2 = (1, 0).\nThe parameter r controls the width of Gaussians.\n1Contact the first author for the explanations. 2Such a technique for a detection of clusters number was chosen in order\nto provide the correspondence between clustering results for all methods.\nThe Figure 4 presents the ratios of resulted clusters sizes. The SWARDS method is robust to the change of scale – the clusters remained almost equally-sized for all r ∈ (0, 1). The clustering result was the most dependent on the widths of Gaussians in the case of k-means.\nExample 5.2: Unbalanced data\nWe have also tested how the number of elements generated from the individual distributions affects the clustering results. For this purpose data was generated from the mixture of two Gaussians\nω · G1 + (1− ω) · G2 , for ω ∈ (0, 1),\nwith identical covariance matrices\nC1 = C2 = ( 1 2 0 0 12 ) ,\nbut different centers\nm1 = (−1, 0) , m2 = (1, 0).\nThe number of elements generated from each Gaussian is determined by the value of parameter ω.\nThe ratios of clusters sizes are shown in the Figure 5. One can observe that the proportions specified by ω was preserved by SWARDS method. In the Spectral Clustering the results are less stable. On the other hand Wards k-means has a tendency to build equally-sized clusters."
    }, {
      "heading" : "B. Dimension estimation",
      "text" : "To apply the SWARDS criterion function in the case of arbitrary non Euclidean space the value of dimension parameter N needs to be specified. In the previous subsection we showed that the reasonable clustering results can be obtained calculating this value using MLE method [12], [13]. We will experimentally show how the clustering effects differ when the value of N changes.\nExample 5.3: Clusters number detection\nLet us first examine the impact of the value of parameter N on the detection of the resultant number of groups. For this purpose a mouse-like set (see Figure 2) was clustered with\ndifferent values of N starting from 100 initial groups. The resultant number of groups are illustrated in the Figure 6.\nThe immediate observation is that the increase of the value of N results in the increase of the detected number of groups. One can observe that for N < 1 the entire data set was recognized as one group. For N ∈ (1, 2) the mouse-like set was partitioned into three groups which seems to be the most appropriate partitioning. For N > 2 the number of groups began to grow rapidly.\nExample 5.4: Shape of criterion function\nTo get more insight into the influence of dimension parameter on the discovered number of clusters, we analyzed the shape of SWARDS criterion function for different values of N . Since the SWARDS automatically reduces unnecessary clusters, it is not possible to directly specify the number of groups. Therefore, a mouse-like data set (see Figure 2) was first partitioned into expected number of groups with use of k-means. Then, the SWARDS criterion function was calculated for each partition.\nIt is clear from Figure 8 that the criterion function yields\na global minimum for 3 clusters when N = 2. Therefore, in most cases the algorithm ends with 3 groups. For N = 1 the cost of maintaining clusters increases and the algorithm generally includes all elements into one group. The function is decreasing for N = 3. It means that the method rarely reduces clusters. The last case can be a very useful variant of SWARDS when the resulting number of groups should not be discovered by the algorithm but specified directly by the user."
    }, {
      "heading" : "C. Applications",
      "text" : "In this section we show that the proposed method is very useful in the analysis of biological models of populations. It is assumed that a population follows a random walk model P (x, n, t) on a plane [37], where at each unit of time an instance moves randomly in one of four directions: left, right, up or down. More precisely, given a starting point (seed) x ∈ X , n-instances are generated from a random walk model assuming t time units. It is worth to mention that a probability distribution of a population converges to spherical Gaussian one [37]. Given a data set consisting of k populations we would like to discover them during a clustering process. Constructed Voronoi diagram determines the corresponding populations districts in the whole space.\nLet us observe that, in practice the environment does not represent an Euclidean space. Indeed, a plane is usually crossed by rivers and barriers. Moreover, the environment can be divided into various regions, e.g. meadows, seas, forests etc., which changes the speed of movement of individuals. These modifications change the classical Euclidean metric – the distance between elements has to take into account all the aforementioned circumstances. In the experiments we analyze two cases of populations environments.\nExample 5.5: Environment with barriers. Let us consider three populations living in the environment showed in Figure 7 crossed by two barriers which modify Euclidean distance function. Basically, the distance between elements located on the opposite sides of barrier is calculated as a shortest path which does not cross the barrier.\nRegions occupied by populations can be obtained with use of Voronoi diagram. Is is clear from Figure 7(a) that Wards k-means discovered populations districts as horizontal stripes which is not an appropriate model. More accurate partition results from SWARDS (see Figure 7(b)), where detected regions form circular shapes. Partitions agreement measured by Rand\nindex [38] for Wards k-means equals 96% while for SWARDS is 98%.\nExample 5.6: Environment with regions. In the second example let us assume that a data space X is divided into two regions X1 and X5. In X5 the individuals moves 5 times faster than in X1. This inducts a dissimilarity measure on X by:\nd(x, y) :=  dE(x, y), x, y ∈ X5, 5dE(x, y), x, y ∈ X1, inf{5dE(x, z) + dE(z, y) : border point z},\nx ∈ X1, y ∈ X5,\nwhere dE(·, ·) denotes the Euclidean distance. We consider two populations showed in Figure 9 with starting points marked with white dots in X1 and X5 respectively.\nOne can observe in Figure 9(b) that despite the form of the above dissimilarity measure, SWARDS detected the circularlike districts of populations very well. This result can be compared with k-means clustering (see Figure 9(a)) where a produced partition does not match populations distributions. The value of Rand index for SWARDS equals 92% while for Wards k-means is 61%."
    }, {
      "heading" : "D. Evaluation",
      "text" : "After establishing the properties as well as demonstrating basic capabilities and potential applications of introduced method we present a short evaluation. We carried out the experiments on selected UCI data sets [14]. In all experiments the initial number of clusters for SWARDS was fixed two times higher than the actual number of groups. In order to provide the correspondence between the clustering results the other examined methods assumed the number of groups returned by SWARDS as the input clusters number.\nAs a measure of agreements between partitions the Rand index (RI) was used [38]. It is defined as a ratio between pairs of true positives and false negatives, and all pairs of examples. The values close to 1 indicate that two partitions are very similar. MLE was used to calculate the optimal value of parameter N . Two kinds of dissimilarity measures were considered: the Euclidean distance and the dissimilarity determined by the Gaussian radial basis function (RBF). The value of sigma for RBF was estimated as a median of the squared distances between all pairs of data set elements [39].\nThe results presented in Table I show that SWARDS reasonably well determined the final number of groups. The advantage of our method over k-means and Spectral Clustering is the most evident for the case of Ecoli data set and Euclidean distance. The worst results were obtained for Ionosphere data set. The use of RBF similarity rarely improved the accuracy of clustering. It could be caused by the fact that it is very difficult to set the optimal value for RBF sigma parameter in particular situation.\nTo extend the above evaluation, in the Figure 10 we present the clustering accuracies of UCI data sets for a wide range of dimension parameter N ∈ (0, 10). One can observe that in most cases the best results were obtained when N was estimated as a dimension of data. The exceptions are Glass and Yeast data sets where a slight improvement was achieved for higher values of N ."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "In this paper a generalization of spherical Cross-Entropy Clustering to non Euclidean spaces was presented. The proposed method uses a Wards approach to modify the crossentropy criterion function for the case of arbitrary data sets. In consequence, obtained method allows for partitioning of non vector data into spherically-shaped clusters of arbitrary sizes. It is scale invariant technique which detects the final number of groups automatically. Our method works in comparable time to generalized Wards method while the clustering effects are similar to those produced by GMM when focusing on spherical Gaussian distributions in Euclidean spaces.\nMoreover, we generalized the notion of Voronoi diagram for the case of arbitrary criterion function based on Wards approach. This leads to identical results in the case of classical methods as k-means while it allows for formal division of data space when focusing on non Euclidean methods as SWARDS."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This work was partially funded by the Polish Ministry of Science and Higher Education from the budget for sci-\nence in the years 2013–2015, Grant no. IP2012 055972 and by the National Science Centre (Poland), Grant No. 2014/13/B/ST6/01792."
    } ],
    "references" : [ {
      "title" : "The EM algorithm and extensions",
      "author" : [ "G. McLachlan", "T. Krishnan" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Unsupervised learning of finite mixture models",
      "author" : [ "M.A.T. Figueiredo", "A.K. Jain" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 24, no. 3, pp. 381–396, 2002.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Waveform quantization of speech using Gaussian mixture models",
      "author" : [ "J. Samuelsson" ],
      "venue" : "Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, 2004, vol. 1. IEEE, 2004, pp. I–165.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Feature selection for high-dimensional genomic microarray data",
      "author" : [ "E.P. Xing", "M.I. Jordan", "R.M. Karp" ],
      "venue" : "Proceedings of the 18th International Conference on Machine Learning, vol. 1. Citeseer, 2001, pp. 601–608.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Time series classification using Gaussian mixture models of reconstructed phase spaces",
      "author" : [ "R.J. Povinelli", "M.T. Johnson", "A.C. Lindgren", "J. Ye" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on, vol. 16, no. 6, pp. 779–783, 2004.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Handbook of chemoinformatics",
      "author" : [ "J. Gasteiger" ],
      "venue" : "Wiley Online Library,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "Investigation of classification methods for the prediction of activity in diverse chemical libraries",
      "author" : [ "S.L. Dixon", "H.O. Villar" ],
      "venue" : "Journal of Computer-Aided Molecular Design, vol. 13, no. 5, pp. 533–545, 1999.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Web document clustering: A feasibility demonstration",
      "author" : [ "O. Zamir", "O. Etzioni" ],
      "venue" : "Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1998, pp. 46–54.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Generalized Ward and related clustering problems",
      "author" : [ "V. Batagelj" ],
      "venue" : "Classification and Related Methods of Data Analysis, pp. 67–74, 1988.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Cross-entropy clustering",
      "author" : [ "J. Tabor", "P. Spurek" ],
      "venue" : "Pattern Recognition, vol. 47, no. 9, pp. 3046–3059, 2014.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Cluster-analyse-algorithmen",
      "author" : [ "H. Spath" ],
      "venue" : "München und Wien, 1975.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Maximum likelihood estimation of intrinsic dimension",
      "author" : [ "E. Levina", "P. Bickel" ],
      "venue" : "Ann Arbor MI, vol. 48109, p. 1092, 2004.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Comments on ”Maximum likelihood estimation of intrinsic dimension” by E. Levina and P. Bickel (2004)",
      "author" : [ "D.J.C. MacKay", "Z. Ghahramani" ],
      "venue" : "2012, available from http://www.inference.phy.cam.ac.uk/mackay/dimension/.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "UCI machine learning repository",
      "author" : [ "A. Asuncion", "D. Newman" ],
      "venue" : "2007.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Hierarchical clustering schemes",
      "author" : [ "S.C. Johnson" ],
      "venue" : "Psychometrika, vol. 32, no. 3, pp. 241–254, 1967.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "Algorithm AS 136: A k-means clustering algorithm",
      "author" : [ "J.A. Hartigan", "M.A. Wong" ],
      "venue" : "Applied Statistics, pp. 100–108, 1979.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "A simple and fast algorithm for k-medoids clustering",
      "author" : [ "H.S. Park", "C.H. Jun" ],
      "venue" : "Expert Systems with Applications, vol. 36, no. 2, pp. 3336– 3341, 2009.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Kernel k-means: spectral clustering and normalized cuts",
      "author" : [ "I.S. Dhillon", "Y. Guan", "B. Kulis" ],
      "venue" : "Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2004, pp. 551–556.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Constrained kmeans clustering with background knowledge",
      "author" : [ "K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schrödl" ],
      "venue" : "Proceedings of the 18th International Conference on Machine Learning, vol. 1, 2001, pp. 577–584.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Spectral clustering and transductive learning with multiple views",
      "author" : [ "D. Zhou", "C.J.C. Burges" ],
      "venue" : "Proceedings of the 24th International Conference on Machine learning. ACM, 2007, pp. 1159–1166.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A clustering technique for digital communications channel equalization using radial basis function networks",
      "author" : [ "S. Chen", "B. Mulgrew", "P.M. Grant" ],
      "venue" : "Neural Networks, IEEE Transactions on, vol. 4, no. 4, pp. 570–590, 1993.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Regularization in the selection of radial basis function centers",
      "author" : [ "M.J.L. Orr" ],
      "venue" : "Neural Computation, vol. 7, no. 3, pp. 606–623, 1995.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "An algorithm for selecting a good value for the parameter c in radial basis function interpolation",
      "author" : [ "S. Rippa" ],
      "venue" : "Advances in Computational Mathematics, vol. 11, no. 2-3, pp. 193–210, 1999.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Density-based clustering",
      "author" : [ "H.P. Kriegel", "P. Kröger", "J. Sander", "A. Zimek" ],
      "venue" : "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 1, no. 3, pp. 231–240, 2011.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Data clustering: a review",
      "author" : [ "A.K. Jain", "M.N. Murty", "P.J. Flynn" ],
      "venue" : "ACM Computing Surveys (CSUR), vol. 31, no. 3, pp. 264–323, 1999.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "K-means clustering: A half-century synthesis",
      "author" : [ "D. Steinley" ],
      "venue" : "British Journal of Mathematical and Statistical Psychology, vol. 59, no. 1, pp. 1–34, 2006.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Data clustering: 50 years beyond k-means",
      "author" : [ "A.K. Jain" ],
      "venue" : "Pattern Recognition Letters, vol. 31, no. 8, pp. 651–666, 2010.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Clustering by means of medoids",
      "author" : [ "L. Kaufman", "P. Rousseeuw" ],
      "venue" : "1987.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "A sublinear time approximation scheme for clustering in metric spaces",
      "author" : [ "P. Indyk" ],
      "venue" : "Foundations of Computer Science, 1999. 40th Annual Symposium on. IEEE, 1999, pp. 154–159.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A statistical method for evaluating systematic relationships",
      "author" : [ "R.R. Sokal", "C.D. Michener" ],
      "venue" : "Univ. Kans. Sci. Bull., vol. 38, pp. 1409–1438, 1958.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1958
    }, {
      "title" : "Image segmentation with use of cross-entropy clustering",
      "author" : [ "M. Śmieja", "J. Tabor" ],
      "venue" : "Proceedings of the 8th International Conference on Computer Recognition Systems CORES 2013. Springer, 2013, pp. 403–409.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Detection of elliptical shapes via cross-entropy clustering",
      "author" : [ "J. Tabor", "K. Misztal" ],
      "venue" : "Pattern Recognition and Image Analysis. Springer, 2013, pp. 656–663.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Hartigan’s Method: k-means Clustering without Voronoi",
      "author" : [ "M. Telgarsky", "A. Vattani" ],
      "venue" : "Proceedings of International Conference on Artifcial Intelligence and Statistics (AISTATS), 2010.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "kernlab-an S4 package for kernel methods in r",
      "author" : [ "A. Karatzoglou", "A. Smola", "K. Hornik", "A. Zeileis" ],
      "venue" : "2004.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Random walk models in biology",
      "author" : [ "E.A. Codling", "M.J. Plank", "S. Benhamou" ],
      "venue" : "Journal of the Royal Society Interface, vol. 5, no. 25, pp. 813–834, 2008.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Objective criteria for the evaluation of clustering methods",
      "author" : [ "W.M. Rand" ],
      "venue" : "Journal of the American Statistical Association, vol. 66, no. 336, pp. 846–850, 1971.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Appearance-based Object Recognition using SVMs: Which Kernel Should I Use?",
      "author" : [ "B. Caputo", "K. Sim", "F. Furesjo", "A. Smola" ],
      "venue" : "Proceedings of NIPS workshop on Statistical Methods for Computational Experiments in Visual Processing and Computer",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Distribution-based clustering, such as Gaussian mixture model (GMM), has been proven to be very useful in many practical problems [1].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "This technique has been widely applied in object detection [2], learning and modeling [3], feature selection [4] or classification [5].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "This technique has been widely applied in object detection [2], learning and modeling [3], feature selection [4] or classification [5].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "This technique has been widely applied in object detection [2], learning and modeling [3], feature selection [4] or classification [5].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "This technique has been widely applied in object detection [2], learning and modeling [3], feature selection [4] or classification [5].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "g in chemoinformatics where finding of chemical compounds acting on specific disease is rare [6], [7] or in Natural Language Processing where the numbers of documents that belong Data set with dissimilarity measure",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "g in chemoinformatics where finding of chemical compounds acting on specific disease is rare [6], [7] or in Natural Language Processing where the numbers of documents that belong Data set with dissimilarity measure",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 7,
      "context" : "to particular domains are different [8].",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "The method is easy to implement and has the same numerical complexity as the k-means version adapted to non Euclidean spaces [9].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "Proposed SWARDS method is a combination of spherical variant of Cross-Entropy Clustering (CEC) [10] with the generalized Wards approach [9], [11].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "Proposed SWARDS method is a combination of spherical variant of Cross-Entropy Clustering (CEC) [10] with the generalized Wards approach [9], [11].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 10,
      "context" : "Proposed SWARDS method is a combination of spherical variant of Cross-Entropy Clustering (CEC) [10] with the generalized Wards approach [9], [11].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "Applied Wards approach allows for a generalization of the notion of within cluster sum of squares for the case of any dissimilarity measure [9], [11].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "Applied Wards approach allows for a generalization of the notion of within cluster sum of squares for the case of any dissimilarity measure [9], [11].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 11,
      "context" : "Therefore, to adapt spherical CEC criterion function to general case we recommend to estimate its value from data with use of Maximum Likelihood Estimator of intrinsic dimension [12], [13].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "Therefore, to adapt spherical CEC criterion function to general case we recommend to estimate its value from data with use of Maximum Likelihood Estimator of intrinsic dimension [12], [13].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 8,
      "context" : "In the Wards method we replace it by a generalization of distance of point x from the center of cluster Y given by [9]",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "The practical properties of proposed method are illustrated and examined on synthetic data sets and examples retrieved form the UCI repository [14].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 14,
      "context" : "The hierarchical clustering is probably one of the most popular methods to partition data based on any kind of (dis)similarity measure [15].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "The well-known k-means algorithm [16] can also be adapted to non Euclidean data by defining a medoid [17] which plays a role of a generalized notion of mean or by using the Wards method [9], [11] which reformulates the within cluster sum of squares without the notion of the cluster mean.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "The well-known k-means algorithm [16] can also be adapted to non Euclidean data by defining a medoid [17] which plays a role of a generalized notion of mean or by using the Wards method [9], [11] which reformulates the within cluster sum of squares without the notion of the cluster mean.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "The well-known k-means algorithm [16] can also be adapted to non Euclidean data by defining a medoid [17] which plays a role of a generalized notion of mean or by using the Wards method [9], [11] which reformulates the within cluster sum of squares without the notion of the cluster mean.",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 10,
      "context" : "The well-known k-means algorithm [16] can also be adapted to non Euclidean data by defining a medoid [17] which plays a role of a generalized notion of mean or by using the Wards method [9], [11] which reformulates the within cluster sum of squares without the notion of the cluster mean.",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 17,
      "context" : "A lot of modifications were also considered to describe clusters with arbitrary shapes [18], [19].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : "A lot of modifications were also considered to describe clusters with arbitrary shapes [18], [19].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : "Spectral Clustering uses eigenvectors of similarity matrix to divide elements into groups [20].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "Examples showed that interesting effects can be obtained by applying Gaussian radial basis function (RBF) [21].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 21,
      "context" : "The difficulty is that there is no unified methodology how to choose the radius of this function for particular situation [22], [23].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 22,
      "context" : "The difficulty is that there is no unified methodology how to choose the radius of this function for particular situation [22], [23].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "In order to perform a distribution-based clustering a GMM is widely used in Euclidean space [1].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : "On the other hand, a family of density based clustering such as DBSCAN [24] can be applied for non Euclidean data.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "The proposed SWARDS clustering is a combination of spherical Cross-Entropy Clustering (SCEC) [10] with a generalized Wards approach [9], [11].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "The proposed SWARDS clustering is a combination of spherical Cross-Entropy Clustering (SCEC) [10] with a generalized Wards approach [9], [11].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "The proposed SWARDS clustering is a combination of spherical Cross-Entropy Clustering (SCEC) [10] with a generalized Wards approach [9], [11].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "There are several alternatives [25], [26], [27], [28], [29] which allow to partially overcome this difficulty as kmedoids [30] or k-clustering [31].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : "There are several alternatives [25], [26], [27], [28], [29] which allow to partially overcome this difficulty as kmedoids [30] or k-clustering [31].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 26,
      "context" : "There are several alternatives [25], [26], [27], [28], [29] which allow to partially overcome this difficulty as kmedoids [30] or k-clustering [31].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 27,
      "context" : "There are several alternatives [25], [26], [27], [28], [29] which allow to partially overcome this difficulty as kmedoids [30] or k-clustering [31].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 28,
      "context" : "There are several alternatives [25], [26], [27], [28], [29] which allow to partially overcome this difficulty as kmedoids [30] or k-clustering [31].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "The technique related to kclustering and k-means is the generalized Wards method [9], [11] which plays the basic role in our investigations.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "The technique related to kclustering and k-means is the generalized Wards method [9], [11] which plays the basic role in our investigations.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "1: [11] If Y ⊂ R , then ∑",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 28,
      "context" : "Given two subsets Y,Z of X we define a function [31] connected with the average linkage function (also called average neighbor function) [32], [27] as:",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 29,
      "context" : "Given two subsets Y,Z of X we define a function [31] connected with the average linkage function (also called average neighbor function) [32], [27] as:",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "As a generalized within cluster Y ⊂ X sum of squares we put [9]:",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "Wards Optimization Problem [9].",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "The Cross-Entropy Clustering (CEC) is a kind of distribution-based clustering which divides an Euclidean data set into groups such that each group is described by optimally fitted Gaussian probability distribution [10].",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 9,
      "context" : "For more details the reader is referred to [10], [33], [34].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 30,
      "context" : "For more details the reader is referred to [10], [33], [34].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 31,
      "context" : "For more details the reader is referred to [10], [33], [34].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : ", Yk of X the associated criterion function is defined by [10]",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "2: If Y ⊂ R then [10]:",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "In the present study we apply the Maximum Likelihood Estimation (MLE) of intrinsic dimension of X proposed in [12] and modified in [13].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "In the present study we apply the Maximum Likelihood Estimation (MLE) of intrinsic dimension of X proposed in [12] and modified in [13].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : ", xn} the maximum likelihood estimator of a dimension N of X calculated for each x ∈ X equals [12]:",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : ", n} to obtain the final estimator of N [13].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 8,
      "context" : "One can show that the natural modification of the Hartigan algorithm [9], [16], [10] can be used to minimize the SWARDS criterion function (5).",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "One can show that the natural modification of the Hartigan algorithm [9], [16], [10] can be used to minimize the SWARDS criterion function (5).",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "One can show that the natural modification of the Hartigan algorithm [9], [16], [10] can be used to minimize the SWARDS criterion function (5).",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 24,
      "context" : "Clearly, the procedure is not deterministic and leads to a local minimum of (5) [25].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "3: [11] Let Y ⊂ X and x ∈ X .",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 32,
      "context" : "We would like to mention that one can consider the alternative to the Voronoi diagrams as described in [35].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 32,
      "context" : "It provides the partition of data but does not induce a natural partition of the space (see [35] for more details).",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 10,
      "context" : "1: [11] Let x ∈ R be fixed and Y ⊂ R be a subset of R with mean mY .",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 33,
      "context" : "We compared the clustering results with the ones obtained with use of related methods which can be applied for non Euclidean spaces: Wards k-means and Spectral Clustering (kernlab R package was used for the implementations of this algorithm [36]).",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 11,
      "context" : "The value of parameter N (dimension of space) for SWARDS was set automatically with use of MLE method [12], [13].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "The value of parameter N (dimension of space) for SWARDS was set automatically with use of MLE method [12], [13].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 11,
      "context" : "In the previous subsection we showed that the reasonable clustering results can be obtained calculating this value using MLE method [12], [13].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "In the previous subsection we showed that the reasonable clustering results can be obtained calculating this value using MLE method [12], [13].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 34,
      "context" : "It is assumed that a population follows a random walk model P (x, n, t) on a plane [37], where at each unit of time an instance moves randomly in one of four directions: left, right, up or down.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 34,
      "context" : "It is worth to mention that a probability distribution of a population converges to spherical Gaussian one [37].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 35,
      "context" : "Partitions agreement measured by Rand index [38] for Wards k-means equals 96% while for SWARDS is 98%.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "We carried out the experiments on selected UCI data sets [14].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 35,
      "context" : "As a measure of agreements between partitions the Rand index (RI) was used [38].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 36,
      "context" : "The value of sigma for RBF was estimated as a median of the squared distances between all pairs of data set elements [39].",
      "startOffset" : 117,
      "endOffset" : 121
    } ],
    "year" : 2017,
    "abstractText" : "Gaussian mixture model is very useful in many practical problems. Nevertheless, it cannot be directly generalized to non Euclidean spaces. To overcome this problem we present a spherical Gaussian-based clustering approach for partitioning data sets with respect to arbitrary dissimilarity measure. The proposed method is a combination of spherical Cross-Entropy Clustering with a generalized Wards approach. The algorithm finds the optimal number of clusters by automatically removing groups which carry no information. Moreover, it is scale invariant and allows for forming of spherically-shaped clusters of arbitrary sizes. In order to graphically represent and interpret the results the notion of Voronoi diagram was generalized to non Euclidean spaces and applied for introduced clustering method.",
    "creator" : "TeX"
  }
}