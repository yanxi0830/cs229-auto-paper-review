{
  "name" : "1201.6604.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "tjung@cs.utexas.edu", "pstone@cs.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 1.\n66 04\nv1 [\ncs .A\nI] 3\n1 Ja"
    }, {
      "heading" : "1 Introduction",
      "text" : "In reinforcement learning (RL), an agent interacts with an environment and attempts to choose its actions such that an externally defined performance measure, the accumulated per-step reward, is maximized over time. One defining characteristic of RL is that the environment is unknown and that the agent has to learn how to act directly from experience. In practical applications, e.g., in robotics, obtaining this experience means having a physical system interact with the physical environment in real time. Therefore, RL methods that are able to learn quickly and minimize the amount of time the robot needs to interact with the environment until good or optimal behavior is learned, are highly desirable.\nIn this paper we are interested in online RL for tasks with continuous state spaces and smooth transition dynamics that are typical for robotic control domains. Our primary goal is to have an algorithm which keeps sample complexity as low as possible."
    }, {
      "heading" : "1.1 Overview of the contribution",
      "text" : "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5]. As in RMAX and related methods, our algorithm, GP-RMAX, consists of two parts: a model-learner and a planner. The model-learner estimates the dynamics of the environment from the sample transitions the agent experiences while interacting with the environment. The planner is used to find the best possible action, given the current model. As the predictions of the model-learner become increasingly more accurate, the actions derived become increasingly closer to optimal. To control the amount of exploration, the “optimism in the face of uncertainty” principle is employed which makes the agent visit unexplored states first. In our algorithm, the model-learner is implemented by Gaussian process (GP) regression; being non-parametric, GPs give us enhanced modeling flexibility. GPs allow Bayesian model selection and automatic relevance determination. In addition, GPs provide a natural way to determine the uncertainty of predictions, which allows us to implement the “optimism in the face of uncertainty” exploration of RMAX in a principled way. The planner uses the estimated transition function (as estimated by the model) to solve the Bellman equation via value iteration on a uniform grid.1\nThe key point of our algorithm is that we separate the steps estimating a function from samples in the model-learner from solving the Bellman equation in the planner. The rationale behind this is that, if the transition function is relatively simple, it can be estimated accurately from only few sample transitions. On the other hand, the optimal value function, due to the inclusion of the max operator, often is a complex function with sharp discontinuities. Solving the Bellman equation, however, does not require actual “samples”; instead, we must only be able to evaluate the Bellman operator in arbitrary points of the state space. This way, when the transition function can be learned from only a few samples, large gains in sample efficiency are possible. Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.\nConceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner. The primary contribution of this paper is to use GPs instead. Doing this means we are willing to trade off theoretical analysis with practical performance. For example, unlike the recent ARL [1], for which PAC-style performance bounds could be derived (because of its grid-based implementation of model-learning), a GP is much better able to handle generalization and as a consequence can achieve much lower sample complexity.\n1 While certainly more advanced methods exist, e.g., [9,14], for our purpose here, a uniform grid is sufficient as proof of concept."
    }, {
      "heading" : "1.2 Assumptions and limitations",
      "text" : "Our approach makes the following assumptions (most of which are also made in related work, even if it is not always explicitly stated):\n– Low dimensionality of the state space. With a uniform grid, the number of grid points for solving the Bellman equation scales exponentially with the dimensionality. While more advanced methods, such as sparse grids or adaptive grids, may allow us to somewhat reduce this exponential increase, at the end they do not break the curse of dimensionality. Alternatively, one can use nonlinear function approximation; however, despite some encouraging results, it is unclear as to whether this approach would really do any better in general applications. Today, breaking the curse of dimensionality is still an open research problem. – Discrete actions. While continuous actions may be discretized, in practice, for higher dimensional action spaces this becomes infeasible. – Smooth transition function. Performing an action from states that are “close” must lead to successor states that are “close”. (Otherwise both the generalization in the model learner and the interpolation in the value function approximation would not work). – Deterministic transitions. This is not a fundamental requirement of our approach, since GPs can also learn noisy functions (either due to observation noise or random disturbances with small magnitude), and the Bellman operator can be evaluated in the resulting predictive distribution. Rather it is one taken for convenience. – Known reward function. Assuming that the reward function is known and only the transition function needs to be learned is what is different from most comparable work. While it is not a fundamental requirement of our approach (since we could learn the reward function as well), it is an assumption that we think is well justified: for one, reward is the performance criterion and specifies the goal. For the type of control problems we consider here, reward is always externally defined and never something that is “generated” from within the environment. Two, reward sometimes is a discontinuous function, e.g., +1 at the goal state and 0 elsewhere. Which makes it not very amenable for function approximation."
    }, {
      "heading" : "2 Background: Planning when the model is exact",
      "text" : "Consider the reinforcement learning problem for MDPs with continuous state space, finite action space, discounted reward criterion and deterministic dynamics [19]. In this section we assume that dynamics and rewards are available to the learning agent. Let state space X be a hyperrectangle in Rd (this assumption is justified if, for example, the system is a motor control task), A be the finite action space (assuming continuous controls are discretized), xt+1 = f(xt, at) be the transition function (assuming that continuous time problems are discretized in time), and r(x, a) be the reward function. For the following theoretical argument\nwe require that both transition and reward function are Lipschitz continuous in the actions; i.e., there exist constants Lf , Lr such that ‖f(x, a)− f(x\n′, a)‖ ≤ Lf ‖x− x′‖, and |r(x, a) − r(x′, a)| ≤ Lr ‖x− x′‖, ∀x, x′ ∈ X , a ∈ A. In addition, we assume that the reward is bounded, |r(x, a)| ≤ RMAX, ∀x, a. Note that in practice, while the first condition, continuity in the transition function, is usually fulfilled for domains derived from physical systems, the second condition, continuity in the rewards, is often violated (e.g. in the mountain car domain, reward is 0 in the goal and −1 everywhere else). Despite that we find that in many of these cases the outlined procedure may still work well enough.\nFor any state x, we are interested in determining a sequence of actions a0, a1, a2, . . . such that the accumulated reward is maximized,\nV ∗(x) := max a0,a1,...\n{ ∞ ∑\nt=0\nγtr(xt, at) | x0 = x, xt+1 = f(xt, at) } ,\nwhere 0 < γ < 1. Using the Q-notation, where Q∗(x, a) := r(x, a)+γV ∗(f(x, a)), the optimal decision policy π∗ is found by first solving the Bellman equation in the unknown function Q,\nQ(x, a) = r(x, a) + γmax a′\nQ(f(x, a), a′) ∀x ∈ X , a ∈ A (1)\nto yield Q∗, and then choosing the action with the highest Q-value,\nπ∗(x) = argmax a′ Q∗(x, a′).\nThe Bellman operator T related to (1) is defined by\n( TQ )\n(x, a) := r(x, a) + γmax a′\nQ(f(x, a), a′). (2)\nIt is well known that T is a contraction and Q∗ the unique bounded solution to the fixed point problem Q(x, a) = ( TQ ) (x, a), ∀x, a.\nIn order to solve the infinite dimensional problem in (1) numerically, we have to reduce it to a finite dimensional problem. This is done by introducing a discretization Γ of X into a finite number of elements, applying the Bellman operator to only the nodes and interpolating in between.\nIn the following we will consider a uniform grid Γh with N vertices ξi and d-dimensional tensor B-spline interpolation of order 1. The solution of (1) is then obtained in the space of piecewise affine functions.\nFor a fixed action a′, the value QΓh(z, a′) of any state z with respect to grid Γh can be written as a convex combination of the vertices ξj of the grid cell enclosing z with coefficients wij (see Figure 1a). For example, consider the 2-dimensional case (bilinear interpolation) in Figure 1b. Let z = (x, y) ∈ R2. To determine QΓh(z, a′), we find the four vertices ξ00, ξ01, ξ10, ξ11 ∈ R2 of the enclosing cell with known function values qa ′\n00 := Q Γh(ξ00, a ′), . . . etc. We then perform two linear interpolations along the x-coordinate (order invariant) in the auxilary points x0, x1 to obtain\nQΓh(x0, a ′) = (1− λ0)q\na′ 00 + λ0q a′ 01\nQΓh(x1, a ′) = (1− λ0)q\na′ 10 + λ0q a′ 11\nwhere λ0 := dx/hx (see Figure 1b for a definition of dx, hx, x0, x1). We then perform another linear interpolation in x0, x1 along the y-coordinate to obtain\nQΓh(z, a′) = (1− λ1)(1− λ0)q a′ 00 + (1− λ1)λ0q a′ 01 + λ1(1− λ0)q a′ 10 + λ1λ0q a′ 11 (3)\nwhere λ1 := dy/hy. Weights wij now correspond to the coefficients in (3). An analogous procedure applies to higher dimensions.\nLet Qa ′ be the N×1 vector with entries [Qa ′ ]i = Q Γh(ξi, a ′). Let za1 , . . . , z a N ∈ R d denote the successor state we obtain when we apply the transition function f to vertices ξi using action a, i.e., z a i := f(ξi, a). Let [w a i ]j = w a ij denote the 1×N vector of coefficients for zai from (3). The Q-value of z a i for any action a ′ with respect to grid Γh can thus be written as Q Γh(zai , a ′) = ∑N j=1[w a i ]j [Q\na′ ]j . Let W a with rows [wai ] be the N ×N matrix of all coefficients. (Note that this matrix is sparse: each row contains only 2d nonzero entries).\nLet Ra be the N × 1 vector of associated rewards, [Ra]i := r(ξi, a). Now we can use (2) to obtain a fixed point equation in the vertices of the grid Γh,\nQΓh(ξi, a) = ( T ΓhQΓh ) (ξi, a) i = 1, . . . , N, a = 1, . . . , |A|, (4)\nwhere (\nT ΓhQΓh )\n(ξi, a) := r(ξi, a) + γmax a′\nQΓh(f(ξi, a), a ′).\nSlightly abusing the notation, we can write this more compactly in terms of matrices and vectors,\nT ΓhQΓh := Ra + γmax a′\n{ W aQa ′} ∀a. (5)\nThe Q-function is now represented by |A| N -dimensional vectors Qa ′\n, each containing the values for the vertices ξi. The discretized Bellman operator T Γh is\na contraction in Rd × A and therefore has a unique fixed point Q∗ ∈ Rd × A. Let function Q∗,Γh : (Rd × A) → R be the Q-function obtained by linear interpolation of vector Q∗ along states. The function Q∗,Γh can now be used to determine (approximately) optimal control actions: for any state x ∈ X , we simply determine\nπ∗,Γh(x) = argmax a′ Q∗,Γh(x, a′).\nIn order to estimate how well function Q∗,Γh approximates the true Q∗, a posteriori estimates can be defined that are based on local errors, i.e. the maximum of residual in each grid cell. The local error in a grid cell in turn depends on the granularity of the grid, h, and the modulus of continuity Lf , Lg (e.g., see [9,14] for details)."
    }, {
      "heading" : "3 Our algorithm: GP-RMAX",
      "text" : "In the last section we have seen how, for a continuous state space, optimal behavior of an agent can be obtained in a numerically robust way, given that the transition function xt+1 = f(xt, at) is known. 2\nFor model-based RL we are now interested in solving the same problem for the case that the transition function is not known. Instead, the agent has to interact with the environment, and only use the samples it observes to compute optimal behavior. Our goal in this paper is to develop a learning framework\n2 Remember our working assumption: reward as a performance criterion is externally given and does not need to be estimated by the agent. Also note that discretization (even with more advanced methods like adaptive or sparse grids) is likely to be feasible only in state spaces with low to medium dimensionality. Breaking the curse of dimensionality is an open research problem.\nwhere this number is kept as small as possible. This will be done by using the samples to learn an estimate f̃(x, a) of f(x, a) and then use this estimate f̃ in place of f in the numerical procedure outlined in the previous section."
    }, {
      "heading" : "3.1 Overview",
      "text" : "A sketch of our architecture is shown in Figure 2. GP-RMAX consists of the two parts model learning and planning which are interwoven for online learning. The model-learner estimates the dynamics of the environment from the sample transitions the agent experiences while interacting with the environment. The planner is used to find the best possible action, given the current model. As the predictions of the model-learner become increasingly more accurate, the actions derived from the planner become increasingly closer to optimal. Below is a highlevel overview of the algorithm:\n– Input: • Reward function r(x, a) • Discount factor γ • Performance parameters:\n∗ planning and model-update frequency K ∗ model accuracy δM1 , δ M 2 (stopping criterion for model-learning) ∗ discretization of planner N\n– Initialize: • Model M1, Q-function Q1, observed transitions D1\n– Loop: t = 1, 2, . . . • Interact with system:\n∗ observe current state xt ∗ choose action at greedy with respect to Qt\nat = argmax a′\nQt(xt, a ′)\n∗ execute action at, observe next state xt+1, store transition Dt+1 = Dt ∪ {xt, at, xt+1} • Model learning: (see Section 3.2) ∗ only every K steps, and only if Mt is not sufficiently exact (as determined by evaluating the stopping criterion) · Mt+1 = update model (Mt,Dt+1) · evaluate stopping criterion (Mt+1,Mt, δM1 , δ M 2 )\n∗ else · Mt+1 = Mt\n• Planning with model: (see Section 3.3) ∗ only every K steps, and only if Mt is not sufficiently exact (as determined by evaluating the stopping criterion) · Qt+1 = augmented value iteration (Q⊔,Mt+1,@r(x, u), γ,N)\n∗ else · Qt+1 = Qt\nNext, we will explain in more detail how each of the two functional modules “model-learner” and “planner” is implemented."
    }, {
      "heading" : "3.2 Model learning with GPs",
      "text" : "In essence, estimating f̃ from samples is a regression problem. While in theory any nonlinear regression algorithm could serve this purpose, we believe that GPs are particularly well-suited: (1) being non-parametric means great modeling flexibility; (2) setting the hyperparameters can be done automatically (and in a principled way) via optimization of the marginal likelihood and allows automatic determination of relevant inputs; and (3) GPs provide a natural way to determine the uncertainty of its predictions which will be used to guide exploration. Furthermore, uncertainty in GPs is supervised in that it depends on the target function that is estimated (because of (2)); other methods only consider the density of the data (unsupervised) and will tend to overexplore if the target function is simple.\nAssume we have observed a number of transitions, given as triplets of state, performed action, and resulting successor state, e.g., D = {xt, at, xt+1}t=1,2,... where xt+1 = f(xt, at). Note that f is a d-dimensional function, f(xt, at) = [\nf1(xt, at), . . . , fd(xt, at) ]T\n. Instead of trying to estimate f directly (which corresponds to absolute transitions), we try to estimate the relative change xt+1−xt as in [10]. The effect of each action on each state variable will be treated independently: we train multiple univariate GPs and combine the individual predictions afterwards. Each individual GP ij is trained in the respective subset of data in D, e.g., GP ij is trained on all xt as input, and x (i) t+1 − x (i) t as output, where at = j. Each individual GP ij has its own set of hyperparameters obtained from optimizing the marginal likelihood.\nThe details of working3 with GPs can be found in [17]; using GPs to learn a model for RL was previously also studied in [6] (for offline RL and without uncertainty-guided exploration). One characteristic of GPs is that their functional form is given in terms of a parameterized covariance function. Here we use the squared exponential,\nk(x, x′; v0, b, θ) = v0 exp { −0.5(x− x′)TΩ(x− x′) } + b,\nwhere matrix Ω is either one of the following: (1) Ω = θI (uniform), (2) Ω = diag(θ1, . . . , θd) (axis aligned ARD), (3) Ω = MkM T k (factor analysis). Scalars v0, b and the (Ω-dependent number of) entries of θ constitute the hyperparameters of the GP and are adapted from the training data (likelihood\n3 There is also the problem of implementing GPs efficiently when dealing with a possible large number of data points. For the lack of space we can only sketch our particular implementation, see [16] for more detailed information. Our GP implementation is based on the subset of regressors approximation. The elements of the subset are chosen by a stepwise greedy procedure aimed at minimizing the error incurred from using a low rank approximation (incomplete Cholesky decomposition). Optimization of the likelihood is done on random subsets of the data of fixed size. To avoid a degenerate predictive variance, the projected process approximation was used.\noptimization). Note that variant (2) and (3) implement automatic relevance determination: relevant inputs or linear projections of inputs are automatically identified, whereby model complexity is reduced and generalization sped up.\nOnce trained, for any testpoint x, GP ij provides a distribution over target values, N (µij(x), σ2ij(x)), with mean µij(x) and variance σ 2 ij(x) (exact formulas for µ and σ can be found in [17]). Each individual mean µij predicts the change in the i-th coordinate of the state under the j-th action. Each individual variance σ2ij can be interpreted as the associated uncertainty; it will be close to 0 if GPij is certain, and close to k(x, x) if it is uncertain (the value of k(x, x) depends on the hyperparameters of GP ij). Stacking the individual predictions together, our model-learner produces in summary\nf̃(x, a) :=\n\n \nx(1)\n... x(d)\n\n  +\n\n  µ1a(x) ...\nµda(x)\n\n  , c(x, a) := max\ni=1,...,d\n(\nnormalizeia(σ 2 ia)\n)\n, (6)\nwhere f̃(x, a) is the predicted successor state and c(x, a) the associated uncertainty (taken as maximum over the normalized per-coordinate uncertainties, where normalization ensures that the values lie between 0 and 1)."
    }, {
      "heading" : "3.3 Planning with a model",
      "text" : "At any time t, the planner receives as input model Mt. For any state x and action a, model Mt can be evaluated to “produce” the transition f̃(x, a) along with normalized scalar uncertainty c(x, a) ∈ [0, 1], where 0 means maximally certain and 1 maximally uncertain (see Section 3.2)\nLet Γh be the discretization of the state space X with nodes ξi, i = 1, . . . , N . We now solve the planning stage by plugging f̃ into the procedure described in Section 2. First, we compute z̃ai = f̃(ξi, a), c(ξi, a) from (6) and the associated interpolation coefficients waij from (3) for each node ξi and action a.\nLet Ca denote the N × 1 vector corresponding to the uncertainties, [Ca]i = c(ξi, a); and R\na be theN×1 vector corresponding to the rewards, [Ra]i = r(ξi, a). To solve the discretized Bellman equation in Eq. (4), we perform basic Jacobi iteration:\n– Initialize [Qa0 ]i, i = 1, . . . , N , a = 1, . . . , |A| – Repeat for k = 0, 1, 2, . . .\n[Qak+1]i = [R a]i + γmax\na′\n\n\n\nN ∑\nj=1\nwaij [Q a′ k ]j\n\n\n\n∀i, a (7)\nuntil |Qak+1−Q a k|∞ < tol, ∀a, or a maximum number of iterations is reached.\nTo reduce the number of iterations necessary, we adapt Grüne’s increasing coordinate algorithm [9] to the case of Q-functions: instead of Eq. (7), we perform\nupdates of the form\n[Qak+1]i = [1− γw a ii] −1\n\n[Ra]i + γmax a′\n\n\n\nN ∑\nj=1,j 6=i\nwaij [Q a′ k ]j\n\n\n\n\n . (7’)\nIn [9] it was proved that Eq. (7’) converges to the same fixed point as Eq. (7), and it was empirically demonstrated that convergence can occur in significantly fewer iterations. The exact reduction is problem-dependent, savings will be greater for small γ and large cells where self-transitions occur (i.e., ξi is among the vertices of the cell enclosing z̃ai ).\nTo implement the “optimism in the face of uncertainty” principle, that is, to make the agent explore regions of the state space where the model predictions are uncertain, we employ the heuristic modification of the Bellman operator which was suggested in [15] and shown to perform well. Instead of Eq. (7’), the update rule becomes\n[Qak+1]i = (1− [C a]i)[1− γw a ii] −1\n\n[Ra]i + γmax a′\n\n\n\nN ∑\nj=1,j 6=i\nwaij [Q a′ k ]j\n\n\n\n\n+\n+ [Ca]iVMAX (7”)\nwhere VMAX := RMAX/(1 − γ). Eq. (7”) can be seen as a generalization of the binary uncertainty in the original RMAX paper to continuous uncertainty; whereas in RMAX a state was either “known” (sufficiently explored), in which case the unmodified update was used, or “unknown” (not sufficiently explored), in which case the value VMAX was assigned, here the shift from exploration to exploitation is more gradual.\nFinally we can take advantage of the fact that the planning function will be called many times during the process of learning. Since the discretization Γh is kept fixed, we can reuse the final Q-values obtained in one call to plan as initial values for the next call to plan. Since updates to the model often affect only states in some local neighborhood (in particular in later stages), the number of necessary iterations in each call to planning will be further reduced.\nA summary of our model-based planning function is shown below.\n– Input: • Model Mt, initial [Qa0]i, i = 1, . . . , N , a = 1, . . . , |A|\n– Static inputs: • Grid Γh with nodes ξ1, . . . , ξN , discount factor γ, reward function r(x, a) evaluated in nodes giving [Ra]i\n– Initialize: • Compute z̃ai = f̃(ξi, a) and [C\na]i from Mt (see Eq. (6)) • Compute weights waij for each z̃ a i (see Eq. (3))\n– Loop: • Repeat update Eq. (7”) until |Qak+1 −Q a k|∞ < tol, ∀a, or the maximum\nnumber of iterations is reached."
    }, {
      "heading" : "4 Experiments",
      "text" : "We now examine the online learning performance of GP-RMAX in various wellknown RL benchmark domains."
    }, {
      "heading" : "4.1 Description of domains",
      "text" : "In particular, we choose the following domains (where a large number of comparative results is available in the literature):\nMountain car: In mountain car, the goal is to drive an underpowered car from the bottom of a valley to the top of one hill. The car is not powerful enough to climb the hill directly, instead it has to build up the necessary momentum by reversing throttle and going up the hill on the opposite side first. The problem is 2-dimensional, state variable x1 ∈ [−1.2, 0.5] describes the position of the car, x2 ∈ [−0.07, 0.07] its velocity. Possible actions are a ∈ {−1, 0,+1}. Learning is episodic: every step gives a reward of −1 until the top of the hill at x1 ≥ 0.5 is reached. Our experimental setup (dynamics and domain specific constants) is the same as in [19], with the following exceptions: maximal episode length is 500 steps, discount factor γ = 0.99 and every episode starts with the agent being at the bottom of the valley with zero velocity, xstart = (−π/6, 0).\nInverted pendulum: The next task is to swing up and stabilize a single-link inverted pendulum. As in mountain car, the motor does not provide enough torque to push the pendulum up in a single rotation. Instead, the pendulum needs to be swung back and forth to gather energy, before being pushed up and balanced. This creates a more difficult, nonlinear control problem. The state space is 2-dimensional, θ ∈ [−π, π] being the angle, θ̇ ∈ [−10, 10] the angular velocity. Control force is discretized to a ∈ {−5,−2.5, 0,+2.5,+5} and held constant for 0.2sec. Reward is defined as r(x, a) := −0.1x21 − 0.01x 2 2 − 0.01a\n2. The remaining experimental setup (equations of motion and domain specific constants) is the same as in [6]. The task is made episodic by resetting the system every 500 steps to the initial state xstart = (0, 0). Discount factor γ = 0.99.\nBicycle: Next we consider the problem of balancing a bicycle that rides at a constant speed [8],[12]. The problem is 4-dimensional: state variables are the roll angle ω ∈ [−12π/180, 12π/180], roll rate ω̇ ∈ [−2π, 2π], angle of the handle bar α ∈ [−80π/180, 80π/180], and the angular velocity α̇ ∈ [−2π, 2π]. The action space is inherently 2-dimensional (displacement of rider from the vertical and turning the handlebar); in RL it is usually discretized into 5 actions. Our experimental setup so far is similar to [8]. To allow a more conclusive comparison of performance, instead of just being able to keep the bicycle from falling, we define a more discriminating reward r(x, a) = −x21, and r(x, a) = −10 for |x1| < 12π/180 (bicycle has fallen). Learning is episodic: every episode starts in one of two (symmetric) states close to the boundary from where recovery is impossible: xstart = (10π/180, 0, 0, 0) or xstart = (−10π/180, 0, 0, 0), and proceeds for 500 steps or until the bicycle has fallen. Discount factor γ = 0.98.\nAcrobot: Our final problem is the acrobot swing-up task [19]. The goal is to swing up the tip of the lower link of an underactuated two-link robot over a given height (length of first link). Since only the lower link is actuated, this is a rather challenging problem. The state space is 4-dimensional: θ1 ∈ [−π, π], θ̇1 ∈ [−4π, 4π], θ2 ∈ [−π, π], θ̇2 ∈ [−9π, 9π]. Possible actions are a ∈ {−1,+1}. Our experimental setup and implementation of state transition dynamics is similar to [19]. The objective of learning is to reach a goal state as quickly as possible, thus r(x, a) = −1 for every step. The initial state for every episode is xstart = (0, 0, 0, 0). An episode ends if either a goal state is reached or 500 steps have passed. The discount factor was set to γ = 1, as in [19]."
    }, {
      "heading" : "4.2 Results",
      "text" : "We now apply our algorithm GP-RMAX to each of the four problems. The granularity of the discretization Γh in the planner is chosen such that for the 2- dimensional problems, the loss in performance due to discretization is negligible. For the 4-dimensional problems, we ran offline trials with the true transition function to find the best compromise of granularity and computational efficiency. As result, we use a 100 × 100 grid for mountain car and inverted pendulum, a 20 × 20 × 20 × 20 grid for the bicycle balancing task, and a 25 × 25 × 25 × 25 grid for the acrobot. The maximum number of value iterations was set to 500, tolerance was < 10−2. In practice, running the full planning step took between 0.1-10 seconds for the small problems, and less than 5 min for the large problems (where often more than 50% of the CPU time was spent on computing the GP predictions in all the nodes of the grid). Using the planning module offline with the true transition function, we computed the best possible performance for each domain in advance. We obtained: mountain car (103 steps), inverted pendulum (-18.41 total cost), bicycle balancing (-3.49 total cost), and acrobot (64 steps).4\nFor the GP-based model-learner, we set the maximum size of the subset to 1000, and ICD tolerance to 10−2. The hyperparameters of the covariance were not manually tuned, but found from the data by likelihood optimiziation.\nSince it would be computationally too expensive to update the model and perform the full planning step after every single observation, we set the planning frequency K to 50 steps. To gauge if optimal behavior is reached and further learning becomes unnessecary, we monitor the change in the model predictions and uncertainties between successive updates and stop if both fall below a threshold (test points in a fixed coarse grid).\nWe consider the following variations of the base algorithm: (1)GP-RMAXexp, which actively explores by adjusting the Bellman updates in Eq. (7”) according to the uncertainties produced by the GP prediction; (2) GP-RMAXgrid, which does the same but uses binary uncertainty by overlaying a uniform grid on top of the state-action space and keeping track which cells are visited; and (3) GPRMAXnoexp, which does not actively explore (see Eq. (7’)). For comparison,\n4 Note that 64 steps is not the optimal solution, [2] demonstrated swing-up with 61 steps.\nwe repeat the experiments using the standard online model-free RL algorithm Sarsa(λ) with tile coding [19], where we consider two different setup of the tilings (one finer and one coarser).\nFigure 3 shows the result of online learning with GP-RMAX and Sarsa. In short, the graphs show us two things in particular: (1) GP-RMAX learns very quickly; and (2) GP-RMAX learns a behavior that is very close to optimal. In comparison, Sarsa(λ) has a much higher sample complexity and does not always learn the optimal behavior (exception is the acrobot). While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.\nExamining the plots in more detail, we find that, while GP-RMAXgrid is somewhat less sample efficient (explores more), GP-RMAXexp and GPRMAXnoexp perform nearly the same. Initially, this appears to be in contrast with the whole point of RMAX, which is efficient exploration guided by the uncertainty of the predictions. Here, we believe that this behavior can be explained by the good generalization capabilities of GPs. Figure 4 illustrates model learning and certainty propagation with GPs in the mountain car domain (predicting acceleration as function of state). The state of the model-learner is shown for two snapshots: after 40 transitions and after 120 transitions. The top row shows the value function that results from applying value iteration with the update modified for uncertainty, see Eq. (7”). The bottom row shows the observed samples and the associated certainty of the predictions. As expected, certainty is high in regions where data was observed. However, due to the generalization of GPs and data-dependent hyperparameter selection, certainty is also high in unexplored regions; and in particular it is constant along the y-coordinate. To understand this, we have to look at the state transition function of the mountain car: acceleration of the car indeed only depends on the position, but not on velocity. This shows that certainty estimates of GPs are supervised and take the properties of the target function into account, whereas prior RMAX treatments of uncertainty are unsupervised and only consider the density of samples to decide if a state is “known”. For comparison, we also show what GP-RMAX with grid-based uncertainty would produce in the same situation."
    }, {
      "heading" : "5 Summary",
      "text" : "We presented an implementation of model-based online reinforcement learning similar to RMAX for continuous domains by combining GP-based model learning and value iteration on a grid. Doing so, our algorithm separates the problem function approximation in the model-learner from the problem function approximation/interpolation in the planner. If the transition function is easier to learn, i.e., requires only few samples relative to the representation of the optimal value\nfunction, then large savings in sample-complexity can be gained. Related modelfree methods, such as fitted Q-iteration, can not take advantage of this situation. The fundamental limitation of our approach is that it relies on solving the Bellman equation globally over the state space. Even with more advanced discretization methods, such as adaptive grids, or sparse grids, the curse of dimensionality limits the applicability to problems with low or moderate dimensionality. Other, more minor limitations, concern the simplifying assumptions we made: deterministic state transitions and known reward function. However, these are not conceptual limitations but rather simplifying assumptions made for the present paper; they could be easily addressed in future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work has taken place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by grants from the National Science Foundation\n(IIS-0917122), ONR (N00014-09-1-0658), DARPA (FA8650-08-C-7812), and the Federal Highway Administration (DTFH61-07-H-00030)."
    } ],
    "references" : [ {
      "title" : "Adaptive-resolution reinforcement learning with efficient exploration",
      "author" : [ "A. Bernstein", "N. Shimkin" ],
      "venue" : "Machine Learning (published online: 5 May 2010). DOI:10.1007/s10994-010-5186-7,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Minimum-time control of the acrobot",
      "author" : [ "G. Boone" ],
      "venue" : "Proc. of IEEE International Conference on Robotics and Automation, 4:3281–3287,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "R-MAX, a general polynomial time algorithm for near-optimal reinforcement learning",
      "author" : [ "R. Brafman", "M. Tennenholtz" ],
      "venue" : "JMLR, 3:213–231,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Online least-squares policy iteration for reinforcement learning control",
      "author" : [ "L. Busoniu", "D. Ernst", "B. De Schutter", "R. Babuska" ],
      "venue" : "In American Control Conference (ACC-10),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multidimensional triangulation and interpolation for reinforcement learning",
      "author" : [ "S. Davies" ],
      "venue" : "In NIPS 9. Morgan,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Gaussian process dynamic programming",
      "author" : [ "M.P. Deisenroth", "C.E. Rasmussen", "J. Peters" ],
      "venue" : "Neurocomputing, 72(7-9):1508–1524,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Bayes meets Bellman: The Gaussian process approach to temporal difference learning",
      "author" : [ "Y. Engel", "S. Mannor", "R. Meir" ],
      "venue" : "In Proc. of ICML 20, pages 154–161,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Tree-based batch mode reinforcement learning",
      "author" : [ "D. Ernst", "P. Geurts", "L. Wehenkel" ],
      "venue" : "JMLR, 6:503–556,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "An adaptive grid scheme for the discrete Hamilton-Jacobi-Bellman equation",
      "author" : [ "L. Grüne" ],
      "venue" : "Numerische Mathematik, 75:319–337,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Model-based exploration in continuous state spaces",
      "author" : [ "N.K. Jong", "P. Stone" ],
      "venue" : "In The 7th Symposium on Abstraction, Reformulation and Approximation,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning robocup-keepaway with kernels",
      "author" : [ "T. Jung", "D. Polani" ],
      "venue" : "JMLR: Workshop and Conference Proceedings (Gaussian Processes in Practice), 1:33–57,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Least-squares policy iteration",
      "author" : [ "M.G. Lagoudakis", "R. Parr" ],
      "venue" : "JMLR, 4:1107–1149,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Online exploration in least-squares policy iteration",
      "author" : [ "L. Li", "M.L. Littman", "C.R. Mansley" ],
      "venue" : "In Proc. of 8th AAMAS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Variable resolution discretization in optimal control",
      "author" : [ "R. Munos", "A. Moore" ],
      "venue" : "Machine Learning, 49:291–323,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Multi-resolution exploration in continuous spaces",
      "author" : [ "A. Nouri", "M.L. Littman" ],
      "venue" : "In NIPS 21,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Approximation methods for gaussian process regression",
      "author" : [ "J. Quiñonero-Candela", "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : "In Leon Bottou, Olivier Chapelle, Dennis DeCoste, and Jason Weston, editors, Large Scale Learning Machines, pages 203– 223. MIT Press,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : "MIT Press,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Neural fitted q-iteration",
      "author" : [ "M. Riedmiller" ],
      "venue" : "In Proc. of 16th ECML,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R. Sutton", "A. Barto" ],
      "venue" : "MIT Press,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].",
      "startOffset" : 151,
      "endOffset" : 159
    }, {
      "referenceID" : 9,
      "context" : "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].",
      "startOffset" : 151,
      "endOffset" : 159
    }, {
      "referenceID" : 4,
      "context" : "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].",
      "startOffset" : 151,
      "endOffset" : 159
    }, {
      "referenceID" : 17,
      "context" : "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.",
      "startOffset" : 57,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.",
      "startOffset" : 57,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.",
      "startOffset" : 57,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.",
      "startOffset" : 108,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.",
      "startOffset" : 108,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.",
      "startOffset" : 108,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.",
      "startOffset" : 108,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "Conceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "Conceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner.",
      "startOffset" : 168,
      "endOffset" : 173
    }, {
      "referenceID" : 0,
      "context" : "Conceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner.",
      "startOffset" : 168,
      "endOffset" : 173
    }, {
      "referenceID" : 0,
      "context" : "For example, unlike the recent ARL [1], for which PAC-style performance bounds could be derived (because of its grid-based implementation of model-learning), a GP is much better able to handle generalization and as a consequence can achieve much lower sample complexity.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : ", [9,14], for our purpose here, a uniform grid is sufficient as proof of concept.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 13,
      "context" : ", [9,14], for our purpose here, a uniform grid is sufficient as proof of concept.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 18,
      "context" : "Consider the reinforcement learning problem for MDPs with continuous state space, finite action space, discounted reward criterion and deterministic dynamics [19].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 8,
      "context" : ", see [9,14] for details).",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 13,
      "context" : ", see [9,14] for details).",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 9,
      "context" : "Instead of trying to estimate f directly (which corresponds to absolute transitions), we try to estimate the relative change xt+1−xt as in [10].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 16,
      "context" : "The details of working with GPs can be found in [17]; using GPs to learn a model for RL was previously also studied in [6] (for offline RL and without uncertainty-guided exploration).",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "The details of working with GPs can be found in [17]; using GPs to learn a model for RL was previously also studied in [6] (for offline RL and without uncertainty-guided exploration).",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 15,
      "context" : "For the lack of space we can only sketch our particular implementation, see [16] for more detailed information.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 16,
      "context" : "Once trained, for any testpoint x, GP ij provides a distribution over target values, N (μij(x), σ ij(x)), with mean μij(x) and variance σ 2 ij(x) (exact formulas for μ and σ can be found in [17]).",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 0,
      "context" : "For any state x and action a, model Mt can be evaluated to “produce” the transition f̃(x, a) along with normalized scalar uncertainty c(x, a) ∈ [0, 1], where 0 means maximally certain and 1 maximally uncertain (see Section 3.",
      "startOffset" : 144,
      "endOffset" : 150
    }, {
      "referenceID" : 8,
      "context" : "To reduce the number of iterations necessary, we adapt Grüne’s increasing coordinate algorithm [9] to the case of Q-functions: instead of Eq.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "In [9] it was proved that Eq.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 14,
      "context" : "To implement the “optimism in the face of uncertainty” principle, that is, to make the agent explore regions of the state space where the model predictions are uncertain, we employ the heuristic modification of the Bellman operator which was suggested in [15] and shown to perform well.",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 18,
      "context" : "Our experimental setup (dynamics and domain specific constants) is the same as in [19], with the following exceptions: maximal episode length is 500 steps, discount factor γ = 0.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "The remaining experimental setup (equations of motion and domain specific constants) is the same as in [6].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "Bicycle: Next we consider the problem of balancing a bicycle that rides at a constant speed [8],[12].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : "Bicycle: Next we consider the problem of balancing a bicycle that rides at a constant speed [8],[12].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "Our experimental setup so far is similar to [8].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "Acrobot: Our final problem is the acrobot swing-up task [19].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 18,
      "context" : "Our experimental setup and implementation of state transition dynamics is similar to [19].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "The discount factor was set to γ = 1, as in [19].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "4 Note that 64 steps is not the optimal solution, [2] demonstrated swing-up with 61 steps.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 18,
      "context" : "we repeat the experiments using the standard online model-free RL algorithm Sarsa(λ) with tile coding [19], where we consider two different setup of the tilings (one finer and one coarser).",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.",
      "startOffset" : 98,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.",
      "startOffset" : 98,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.",
      "startOffset" : 98,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.",
      "startOffset" : 147,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.",
      "startOffset" : 147,
      "endOffset" : 159
    }, {
      "referenceID" : 12,
      "context" : "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.",
      "startOffset" : 147,
      "endOffset" : 159
    }, {
      "referenceID" : 10,
      "context" : "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.",
      "startOffset" : 147,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.",
      "startOffset" : 190,
      "endOffset" : 195
    }, {
      "referenceID" : 5,
      "context" : "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.",
      "startOffset" : 190,
      "endOffset" : 195
    }, {
      "referenceID" : 18,
      "context" : "This in turn allows the optimal value function to be learned from very few sample transitions: panel (b) shows that after only 120 transitions (still in the middle of the very first episode) the approximated value function already resembles the true one [19].",
      "startOffset" : 254,
      "endOffset" : 258
    } ],
    "year" : 2012,
    "abstractText" : "We present an implementation of model-based online reinforcement learning (RL) for continuous domains with deterministic transitions that is specifically designed to achieve low sample complexity. To achieve low sample complexity, since the environment is unknown, an agent must intelligently balance exploration and exploitation, and must be able to rapidly generalize from observations. While in the past a number of related sample efficient RL algorithms have been proposed, to allow theoretical analysis, mainly model-learners with weak generalization capabilities were considered. Here, we separate function approximation in the model learner (which does require samples) from the interpolation in the planner (which does not require samples). For model-learning we apply Gaussian processes regression (GP) which is able to automatically adjust itself to the complexity of the problem (via Bayesian hyperparameter selection) and, in practice, often able to learn a highly accurate model from very little data. In addition, a GP provides a natural way to determine the uncertainty of its predictions, which allows us to implement the “optimism in the face of uncertainty” principle used to efficiently control exploration. Our method is evaluated on four common benchmark domains.",
    "creator" : "LaTeX with hyperref package"
  }
}