{
  "name" : "1503.08316.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Variance Reduced Stochastic Newton Method",
    "authors" : [ "Aurelien Lucchi", "Brian McWilliams", "Thomas Hofmann" ],
    "emails" : [ "@inf.ethz.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 3.\n08 31\n6v 4\n[ cs\n.L G\n] 9\nJ un\n2 01"
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of optimizing a function expressed as an expectation over a set of datadependent functions. Stochastic gradient descent (SGD) has become the method of choice for such tasks as it only requires computing stochastic gradients over a small subset of datapoints [2, 18]. The simplicity of SGD is both its greatest strength and weakness. Due to the effects of evaluating noisy approximation of the true gradient, SGD achieves a convergence rate which is only sub-linear in the number of steps. In an effort to deal with this randomness, two primary directions of focus have been developed. The first line of work focuses on choosing the appropriate SGD step-size [1, 10, 14]. If a decaying step-size is chosen, the variance is forced to zero asymptotically guaranteeing convergence. However, small steps also slow down progress and limit the rate of convergence in practise. The stepsize must be chosen carefully, which can require extensive experimentation possibly negating the computational speedup of SGD. Another approach is to use an improved, lower-variance estimate of the gradient. If this estimator is chosen correctly – such that its variance goes to zero asymptotically – convergence can be guaranteed with a constant learning rate. This scheme is used in [5, 16] where the improved estimate of the gradient combines stochastic gradients computed at the current stage with others used at an earlier stage. A similar approach proposed in [8, 9] combines stochastic gradients with gradients periodically re-computed at a pivot point.\nWith variance reduction, first-order methods can obtain a linear convergence rate. In contrast, second-order methods have been shown to obtain super-linear convergence. However, this requires the computation and inversion of the Hessian matrix which is impractical for large-scale datasets. Approximate variants known as quasi-Newton methods [6] have thus been developed, such as the popular BFGS or its limited memory version known as LBFGS [11]. Quasi-Newton methods such as BFGS do not require computing the Hessian matrix but instead construct a quadratic model of the objective function by successive measurements of the gradient. This also yields super-linear convergence when the quadratic model is accurate. Stochastic variants of BFGS have been proposed (oBFGS [17]), for which stochastic gradients replace their deterministic counterparts. A regularized version known as RES [12] achieves a sublinear convergence rate with a decreasing step-size by\nenforcing a bound on the eigenvalues of the approximate Hessian matrix. SQN [3], another related method also requires a decreasing step size to achieve sub-linear convergence. Although stochastic second order methods have not be shown to achieve super-linear convergence, they empirically outperform SGD for problems with a large condition number [12].\nA clear drawback to stochastic second order methods is that similarly to their first-order counterparts, they suffer from high variance in the approximation of the gradient. Additionally, this problem can be exaggerated due to the estimate of the Hessian magnifying the effect of this noise. Overall, this can lead to such algorithms taking large steps in poor descent directions.\nIn this paper, we propose and analyze a stochastic variant of BFGS that uses a multi-stage scheme similar to [8, 9] to progressively reduce the variance of the stochastic gradients. We call this method Variance-reduced Stochastic Newton (VITE). Under standard conditions on Ĵ , we show that that variance reduction on the gradient estimate alone is sufficient for fast convergence. For smooth and strongly convex functions, VITE reaches the optimum at a geometric rate with a constant step-size. To our knowledge VITE is the first stochastic Quasi-Newton method with these properties.\nIn the following section, we briefly review the BFGS algorithm and its stochastic variants. We then introduce the VITE algorithm and analyze its convergence properties. Finally, we present experimental results on real-world datasets demonstrating its superior performance over a range of competitors."
    }, {
      "heading" : "2 Stochastic second order optimization",
      "text" : ""
    }, {
      "heading" : "2.1 Problem setting",
      "text" : "Given a dataset D = {(x1, y1), . . . , (xn, yn)} consisting of feature vectors xi ∈ Rd and targets yi ∈ [0, C], we consider the problem of minimizing the expected loss f(w) = E[fi(w)]. Each function fi(w) takes the form fi(w) = ℓ(h(w,xi), yi), where ℓ is a loss function and h is a prediction model parametrized by w ∈ Rd. The expectation is over the set of samples and we denote w∗ = argminw f(w).\nThis optimization problem can be solved exactly for convex functions using gradient descent, where the gradient of the loss function is expressed as ∇wf(w) = E[∇wfi(w)]. When the size of the dataset n is large, the computation of the gradient is impractical and one has to resort to stochastic gradients. Similar to gradient descent, stochastic gradient descent updates the parameter vector wt by stepping in the opposite direction of the stochastic gradient ∇wfi(wt) by an amount specified by a step size ηt as follows:\nwt+1 = wt − ηt∇wfi(wt). (1)\nIn general, a stochastic gradient can also be computed as an average over a sample of datapoints as f̂(wt) = r−1 ∑r\ni=1 fi(wt). Given that the stochastic gradients are unbiased estimates of the gradient, Robbins and Monro [15] proved convergence of SGD to w∗ assuming a decreasing stepsize sequence. A common choice for the step size is [18, 12]\na) ηt = η0 t\nor b) ηt = η0T0 T0 + t\n(2)\nwhere η0 is a constant initial step size and T0 controls the speed of decrease.\nAlthough the cost per iteration of SGD is low, it suffers from slow convergence for certain illconditioned problems [12]. An alternative is to use a second order method such as Newton’s method that estimates the curvature of the objective function and can achieve quadratic convergence. In the following, we review Newton’s method and its approximations known as quasi-Newton methods."
    }, {
      "heading" : "2.2 Newton’s method and BFGS",
      "text" : "Newton’s method is an iterative method that minimizes the Taylor expansion of f(w) around wt:\nf(w) =f(wt) + (w −wt) ⊤∇wf(wt) +\n1 2 (w −wt) ⊤H(w−wt), (3)\nwhere H is the Hessian of the function f(w) and quantifies its curvature. Minimizing Eq. 3 leads to the following update rule:\nwt+1 = wt − ηtH −1 t · ∇f(wt), (4)\nwhere ηt is the step size chosen by backtracking line search.\nGiven that computing and inverting the Hessian matrix is an expensive operation, approximate variants of Newton’s method have emerged, where H−1t is replaced by an approximate version H̃ −1 t selected to be positive definite and as close to H−1t as possible. The most popular member of this class of quasi-Newton methods is BFGS [13] that incrementally updates an estimate of the inverse Hessian, denoted Jt = H̃ −1 t . This estimate is computed by solving a weighted Frobenius norm minimization subject to the secant condition:\nwt+1 −wt = Jt+1(∇f(wt+1)−∇f(wt)). (5)\nThe solution can be obtained in closed form leading to the following explicit expression:\nJt+1 =\n(\nI − sy⊤\ny⊤s\n)\nJt\n(\nI − ys⊤\ny⊤s\n)\n+ ss⊤\ny⊤s , (6)\nwhere s = wt+1 − wt and y = ∇f(wt+1) − ∇f(wt). Eq. 6 is known to be positive definitive assuming that J0 is initialized to be a positive definite matrix."
    }, {
      "heading" : "2.3 Stochastic BFGS",
      "text" : "A stochastic version of BFGS (oBFGS) was proposed in [17] in which stochastic gradients are used for both the determination of the descent direction and the approximation of the inverse Hessian. The oBFGS approach described in Algorithm 1 uses the following update equation:\nwt+1 = wt − ηtĴt · ∇f̂(wt), (7)\nwhere the matrix Ĵt and the vector ∇f̂(wt) are stochastic estimates computed as follows. Let A ⊂ {1 . . . n} and B ⊂ {1 . . . n} be sets containing two independent samples of datapoints. The variables y and ∇f(w) defined in Eq. 6 are replaced by sampled variables computed as\nŷ = 1\n|A|\n∑\nk∈A\n∇fk(wt+1)−∇fk(wt) and ∇f̂(wt) = ∇fB(wt) = 1\n|B|\n∑\nk∈B\n∇fk(wt). (8)\nThe estimate of the inverse Hessian then becomes\nĴt+1 =\n(\nI − sŷ⊤\nŷ⊤s\n)\nĴt\n(\nI − ŷs⊤\nŷ⊤s\n)\n+ ss⊤\nŷ⊤s (9)\nUnlike Newton’s method, oBFGS uses a fixed step size sequence instead of a line search. A common choice is to use a step size similar to the one used for SGD in Eq. 2.\nA regularized version of oBFGS (RES) was recently proposed in [12]. RES differs from oBFGS in the use of a regularizer to enforce a bound on the eigenvalues of Ĵt such that\nγI Ĵt ρI =\n(\nγ + 1\nδ\n)\nI, (10)\nwhere γ and δ are given positive constants and the notation A B means that B − A is a positive semi-definite matrix. Note that (10) also implies an upper and lower bound on E[Ĵt] [12]. The update of RES is modified to incorporate an identity bias term γI as follows:\nwt+1 = wt − ηt(Ĵt + γI) · ∇f̂(wt). (11)\nThe convergence proof derived in [12] shows that lower and upper bounds on the Hessian eigenvalues of the sample functions are sufficient to guarantee convergence to the optimum. However, the analysis shows that RES will converge to the optimum at a rate O(1/t) and requires a decreasing step-size. Similar results were derived in [3] for the SQN algorithm.\nAlgorithm 1 oBFGS 1: INPUTS : 2: D : Training set of n examples. 3: w0 : Arbitrary initial values, e.g., 0. 4: {ηt} : Step size sequence 5: OUTPUT : wt 6: Ĵ0 ← αI 7: for t = 0 . . . T do 8: Randomly pick two sets A and B 9: s ← wt+1 −wt 10: ŷ ← ∑\nk∈B ∇fk(wt+1)−∇fk(wt)\n11: ∇f̂(wt) ← ∑ k∈A ∇fk(wt)\n12: wt+1 ← wt − ηtĴt+1 · ∇f̂(wt) 13: Ĵt+1 ← ( I − sŷ ⊤\nŷ⊤s\n)\nĴt\n(\nI − ŷs ⊤\nŷ⊤s\n)\n+ ss ⊤\nŷ⊤s\n14: end for\n3 The VITE algorithm\nReducing the size of the sets A and B used to estimate the inverse Hessian approximation and the stochastic gradient is desirable for reasons of computational efficiency. However, doing so also increases the variance of the update step. Here we propose a new method called VITE that explicitly reduces this variance.\nIn order to simplify the analysis of VITE, we do not explicitly consider the randomness in the matrix Ĵt. Instead, we assume that it is positive definite (which holds under weak conditions due to the BFGS update step) and that its variance can be kept under control, for example by using the regularization of the RES method.\nTo motivate VITE we first consider the standard oLBFGS step, (7) estimated with the sets A and B. The first and second moments simplify as\nE [Ĵt∇fB(wt)] = ĴtEB[∇fB(wt)] (12)\nand\nE\n∣ ∣ ∣ ∣ ∣ ∣ Ĵt∇fB(wt) ∣ ∣ ∣ ∣ ∣ ∣ 2 ≤ ∣ ∣ ∣ ∣ ∣ ∣ Ĵt ∣ ∣ ∣ ∣ ∣ ∣ 2 EB ||∇fB(wt)|| 2 , (13)\nrespectively. For |A| large enough, in order to reduce the variance of the estimate Ĵt · ∇fB(wt), it is only required to reduce the variance of ∇fB(wt) independently. We proceed using a technique similar to the one proposed in [8, 9].\nVITE differs from oBFGS and other stochastic Quasi-Newton methods in the use of a multi-stage scheme as shown in Algorithm 2. In the outer loop a variable w̃ is introduced. We periodically evaluate the gradient of the function with respect to w̃. This pivot point is inserted in the update equation to reduce the variance. Each inner loop runs for a a random number of steps tj ∈ [1,m] whose distribution follows a geometric law with parameter β = ∑m\nt=1(1 − µγη) m−t. Stochastic\ngradients at wt and w̃ are computed and the inverse Hessian approximation is updated in each iteration of the inner loop. Ĵt can be updated using the same update as RES although we found in practice that using Eq. 9 did not affect the results significantly. The descent direction ∇fB(w) is then replaced by\nvt = ∇fB(wt)−∇fB(w̃) + ν̃.\nVITE then makes updates of the form\nwt+1 = wt − ηĴt · vt. (14)\nClearly, ν̃ = E[∇fB(w̃)] and E[vt] = E[∇fB(wt)] so in expectation the descent is in the same direction as Eq. (12). Following the analysis of [8], the variance of vt goes to zero when both w̃ and wt converge to the same parameter w∗. Therefore, convergence can be guaranteed with a constant step-size. The complexity of this approach depends on the number of epochs S and a constant m limiting the number of stochastic gradients computed in a single epoch, as well as other parameters that will be introduced in more detail in Section 4.\nAlgorithm 2 VITE 1: INPUTS : 2: D : Training set of n examples w̃0 : Arbitrary initial values, e.g., 0 3: η : Constant step size m: Arbitrary constant 4: OUTPUT : wt 5: Ĵ0 ← αI 6: for s = 0 . . . S do 7: w̃ = w̃s−1 8: ν̃ = 1\nn ∑n i=1 ∇fi(w̃)\n9: w0 = w̃\n10: Let tj ← t with probability (1−µρη)m−t\nβ for t = 1, . . . ,m\n11: for t = 0 . . . tj − 1 do 12: Randomly pick independent sets A,B ⊂ {1 . . . n}"
    }, {
      "heading" : "13: vt = ∇fB(wt)−∇fB(w̃) + ν̃",
      "text" : "14: wt+1 ← wt − ηĴt · vt 15: Update Ĵt+1 16: end for 17: w̃s = wtj . 18: end for"
    }, {
      "heading" : "4 Analysis",
      "text" : "In this section we present a convergence proof for the VITE algorithm that builds upon and generalizes previous analyses of variance reduced first order methods [8, 9]. Specifically, we show how variance reduction on the stochastic gradient direction is sufficient to establish geometric convergence rates, even when performing linear transformations with a matrix Ĵt. Since we do not exploit the specific form of the stochastic evolution equations for Ĵt, this analysis will not allow us to argue in favor of the specific choice of Eq. (9), yet it shows that variance reduction on the gradient estimate is sufficient for fast convergence as long as Ĵt is sufficiently well behaved. Our analysis relies on the following standard assumptions:\nA1 Each function fi is differentiable and has a Lipschitz continuous gradient with constant L > 0, i.e. ∀w,v ∈ Rn,\nfi(w) ≤ fi(v) + (w − v) ⊤∇fi(v) +\nL 2 ||w − v||2 (15)\nA2 f is µ-strongly convex, i.e. ∀w,v ∈ Rn,\nf(w) ≥ f(v) + (w − v)⊤∇f(v) + µ\n2 ||w − v||\n2 (16)\nwhich also implies ||∇f(w)|| 2 ≥ 2µ(f(w)− f(w∗)) ∀w ∈ Rn (17)\nfor the minimizer w∗ of f .\nAssumptions A1 and A2 also implies that the eigenvalues of the Hessian are bounded as follows:\nµI Ht LI. (18)\nFinally we make the assumption that the inverse Hessian approximation is always well-behaved.\nA3 There exist positive constants γ and ρ such that, ∀w ∈ Rn,\nγI Ĵt ρI. (19)\nAssumption A3 is equivalent to assuming that Ĵt is bounded in expectation (see: e.g. [12]) but allows us to remove this complication, simplifying notation in the analysis which follows. We now introduce two lemmas required for the proof of convergence.\nLemma 1. The following identity holds:\nEf(w̃s+1) = 1\nβ\nm−1 ∑\nt=0\nτtEf(wt)\nwhere τt := (1− γηµ)m−t−1 and the weight vectors wt belong to epoch s.\nThis result follows directly from Lemma 3 in [9].\nLemma 2.\nE‖vt‖ 2 ≤ 4L(f(wt)− f(w ∗) + f(w̃)− f(w∗))\nThe proof is given in [8, 9] and reproduced for convenience in the Appendix. We are now ready to state our main result.\nTheorem 1. Let Assumptions A1-A3 be satisfied. Define the rescaled strong convexity µ′ := γµ ≤ µ and Lipschitz L′ := ρL ≥ L constants respectively. Choose 0 < η ≤ µ ′\n2L′2 and let m be sufficiently\nlarge so that α = (1−ηµ ′)m\nβη(µ′−2L′2η) + 2L\n′2η\nµ′−2L′2η < 1.\nThen the suboptimality of w̃s is bounded in expectation as follows:\nE(f(w̃s)− f(w ∗) ≤ αsE[f(w0)− f(w ∗)]. (20)\nRemark 1. Observe that γ and ρ are bounds on the inverse Hessian approximation. If Ĵt is a good approximation to H , then by plugging in γ = L and ρ = µ, the upper bound on the learning rate reduces to η ≤ 12µL .\nProof of Theorem 1. Our starting point is the basic inequality\nf(wt+1) = f(wt − ηĴt · vt)\n≤ f(wt)− η〈∇f(wt), Ĵt · vt〉+ L 2 η2 ∣ ∣ ∣ ∣ ∣ ∣ Ĵtvt ∣ ∣ ∣ ∣ ∣ ∣ 2 . (21)\nWe first use the properties of vt and Ĵt to reduce the dependence of (21) on Ĵt to its largest and smallest eigenvalues given by (19). For the purpose of the analysis, we define Ft to be the sigmaalgebra measuring wt. By conditioning on Ft, and by A3, the remaining randomness is in the choice of the index set B in round t, which is tied to the stochasticity of vt. Taking expectations with respect to B gives us\nEB\n∣ ∣ ∣ ∣ ∣ ∣ Ĵtvt ∣ ∣ ∣ ∣ ∣ ∣ 2 ≤ ‖Ĵt‖ 2 EB‖vt‖ 2 ≤ ρ2EB‖vt‖ 2 (22)\nand\nEB〈∇f(wt), Ĵt · vt〉 = 〈∇f(wt), Ĵt · ∇f(wt)〉 ≥ γ ||∇f(wt)|| 2 (23)\nwhere (23) comes from the definition EBvt = ∇f(wt). Therefore, taking the expectation of the inequality (21) and dropping the notational dependence on B results in\nEf(wt+1) ≤ Ef(wt)− γηE ||∇f(wt)|| 2 +\nL 2 η2ρ2E ||vt|| 2 . (24)\nTo simplify the remainder of the proof we make the following substitution\nµ′ := γµ ≤ µ and L′ := ρL ≥ L.\nConsidering a fixed epoch s, we can further bound Ef(wt+1) using Lemma 2 and Eq. 17. By taking the expectation over Ft, adding and subtracting f(w∗), we get\nE[f(wt+1)− f(w ∗)] ≤E[f(wt)− f(w ∗)] + 2η2L′ 2( f(w̃s)− f(w ∗) )\n(25)\n+ 2 ( η2L′ 2 − ηµ′ ) E[f(wt)− f(w ∗)]\n=2η2L′ 2( f(w̃s)− f(w ∗) ) + ( 2η2L′ 2 − 2ηµ′ + 1 ) E[f(wt)− f(w ∗)].\nWriting ∆f(wt) := f(wt)− f(w∗), we then have\n(ηµ′ − 2η2L′ 2 )E∆f(wt) ≤ 2η 2L′ 2 ∆f(w̃s) + (1− ηµ ′)E∆f(wt)− E∆f(wt+1) (26)\nNow we sum all these inequalities at iterations t = 0, . . . ,m− 1 performed in epoch s with weights τt = (1− ηµ ′)m−t−1. Applying Lemma 1 to the last summand to recover f(w̃s+1) we arrive at\nβE∆f(w̃s+1) ≤ 2βη2L′\n2\nηµ′ − 2η2L′2 E∆f(w̃s) +\nm−1 ∑\nt=0\nτt (1 − ηµ′)E∆f(wt)− E∆f(wt+1)\nηµ′ − 2η2L′2 .\nWe now need to bound the remaining sum (∗) in the numerator, which can be accomplished by re-grouping summands\n(∗) =(1− ηµ′)mE△f(w̃s)− (1− ηµ ′)E△f(w̃s+1)\nBy ignoring the negative term in (∗), we get the final bound\nE∆f(w̃s+1) ≤ αE∆f(w̃s),\nwhere\nα =\n(\n(1− ηµ′)m\nβ(ηµ′ − 2η2L′2) +\n2η2L′ 2\nηµ′ − 2η2L′2\n)\nTheorem 1 implies that VITE has a local geometric convergence rate with a constant learning rate. In order to satisfy E(f(w̃s)− f(w∗)) ≤ ǫ, the number of stages s needs to satisfy\ns ≥ − logα−1 log E(f(w̃0)− f(w\n∗))\nǫ .\nSince each stage requiresn+m(2|A|+2|B|) component gradient evaluations, the overall complexity is O((n+ 2m(|A|+ |B|)) log(1/ǫ))."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "This section presents experimental results that compare the performance of VITE to SGD, SVRG [8] which incorporates variance reduction and RES [12] which incorporates second order information. We consider two commonly occurring problems in machine learning, namely least-square regression and regularized logistic regression.\nLinear Least Squares Regression. We apply least-square regression on the binary version of the COV dataset [4] that contains n = 581, 012 datapoints, each described by d = 54 input features. Logistic Regression. We apply logistic regression on the ADULT and IJCNN1 datasets obtained from the LIBSVM website 1. The ADULT dataset contains n = 32, 561 datapoints, each described by d = 123 input features. The IJCNN1 dataset contains n = 49, 990 datapoints, each described by d = 22 input features. We added an ℓ2-regularizer with parameter λ = 10−5 to ensure the objective is strongly convex.\nThe complexity of VITE depends on three quantities: the approximate Hessian Ĵ , the pair of stochastic gradients (∇fB(w),∇fB(w̃)) and ν̃, respectively computed over the sets A, B and D. Similarly to [12], we consider different choices for |A| and |B| and pick the best value in a limited interval {1, . . . , 0.05n}. These results are also reported for the RES method that also depends on both |A| and |B|. For SGD, we use |B| = 1 as we found this value to be the best performer on all datasets. Computing the average gradient, ν̃ over the full dataset for SVRG and VITE is impractical. We therefore estimate ν̃ over a small subset C ⊂ D. Although this introduces some bias, it did not seem to practically affect convergence for sufficiently large |C|. In our experiments, we selected |C| = 0.1n samples uniformly at random. Each experiment was averaged over 5 runs with different\n1http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets\ninitializations of w0 and a random selection of the samples in A, B and C. Given that the complexity per iteration of each method is different, we compare them as a function of the number of gradient evaluations.\nFig. 1 shows the empirical convergence properties of VITE against RES for least-square regression and logistic regression. The horizontal axis corresponds to the number of gradient evaluations while the vertical axis corresponds to the objective function value. The vertical bars in each plot show the variance over 5 runs. We show plots for different values of |B| and the best corresponding A. For small |B|, the variance of the stochastic gradients clearly hurts RES while the variance corrections of VITE lead to fast convergence. As we increase |B|, thus reducing the variance of the stochastic gradients, the convergence rate of RES and VITE becomes similar. However, VITE with small |B| is much faster to converge to a lower objective value. This clearly demonstrates how using small batches for the computation of the gradients while reducing their variance leads to a fast convergence rate. We also investigated the effect of |A| on the convergence of RES and VITE (see Appendix). In short, we find that a good-enough curvature estimate can be obtained for |A| = O(10−5n). Increasing this value incurs a penalty in terms of number of gradient evaluations required and so overall performance degrades.\nFinally, we compared VITE against SGD, RES and SVRG [8, 9]. A critical factor in the performance of SGD is the selection of the step-size. We use the step-size given in Eq. 2b and pick the parameters T0 and η0 by performing cross-validation over T0 = {1, 10, 102, . . . , 104} and η0 = {10\n−1, . . . , 10−5}. Although it is a quasi-Newton method, RES also requires a decaying stepsize and so the same selection process was performed. For SVRG and VITE, we used a constant step size chosen in the same interval as η0. For SVRG and VITE we used the same size subset, C to compute ν̃. Fig. 2 shows the objective value of each method in log scale. Although RES and SVRG are superior to SGD, neither clearly outperforms the other. On the other hand, we observe that VITE consistently converges faster than both RES and SVRG. This demonstrates that the combination of second order information and variance reduction is beneficial for fast convergence."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have shown that stochastic variants of BFGS can be made more robust to the effects of noisy stochastic gradients using variance reduction. We introduced VITE and showed that it obtains a geometric convergence rate for smooth convex functions – to our knowledge the first stochastic Quasi-Newton algorithm with this property. We have shown experimentally that VITE outperforms both variance reduced SGD and stochastic BFGS. The theoretical analysis we present is quite general and additionally only requires that the bound on the eigenvalues of the inverse Hessian matrix in (19) holds. Therefore, the variance reduced framework we propose can be extended to other quasi-Newton methods, including the widely used L-BFGS and ADAGRAD [7] algorithms. Finally, an important open question is how to bridge the gap between the theoretical and empirical results. Specifically, whether it is possible to obtain better convergence rates for stochastic BFGS algorithms which match the improvement we have demonstrated over SVRG."
    }, {
      "heading" : "7 Appendix",
      "text" : ""
    }, {
      "heading" : "7.1 Proof of Lemma 2",
      "text" : "E ||vt|| 2 = E ||∇fi(wt)−∇fi(w̃) +∇f(w̃)|| 2\n≤ 2E ||∇fi(wt)−∇fi(w ∗)||\n2\n+ 2E ||(∇fi(w̃)−∇fi(w ∗))−∇f(w̃)||\n2\n= 2E ||∇fi(wt)−∇fi(w ∗)||2\n+ 2E ||(∇fi(w̃)−∇fi(w ∗))− (∇f(w̃)−∇f(w∗))||\n2\n≤ 2E ||∇fi(wt)−∇fi(w ∗)||\n2\n+ 2E ||∇fi(w̃)−∇fi(w ∗)||\n2\n≤ 4L(f(wt)− f(w ∗) + f(w̃)− f(w∗)) (27)\nThe second inequality uses E ||ξ − Eξ||2 = E ||ξ||2 − ||Eξ||2 ≤ E ||ξ||2 for any random vector ξ.\nThe last inequality uses the following inequality derived from the fact that fi is a Lipschitz function:\nE ||∇fi(w ∗)−∇fi(wt)|| 2 ≤ 2L(f(wt)− f(w ∗)).\n7.2 Selection of the parameter |A|.\nFigure 3 shows the effect of the set A, used to estimate the inverse Hessian, on the convergence of RES and VITE. We show results for |A| = {0.00001, 0.0001} × n. Firstly we see that better performance is obtained for both methods for the smaller value of |A|. By increasing |A|, the penalty paid in terms of gradient evaluations outweighs the gain in terms of better curvature estimates and so convergence is slower. A similar observation was made in [12]. However, we also observe that VITE always outperforms RES for all combinations of |A|."
    } ],
    "references" : [ {
      "title" : "et al",
      "author" : [ "F. Bach", "E. Moulines" ],
      "venue" : "Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pages 451–459",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "COMPSTAT, pages 177–186. Springer",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A stochastic quasi-newton method for large-scale optimization",
      "author" : [ "R.H. Byrd", "S. Hansen", "J. Nocedal", "Y. Singer" ],
      "venue" : "arXiv preprint arXiv:1401.7020",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A parallel mixture of svms for very large scale problems",
      "author" : [ "R. Collobert", "S. Bengio", "Y. Bengio" ],
      "venue" : "Neural computation, 14(5):1105–1114",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "A. Defazio", "F. Bach", "S. Lacoste-Julien" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1646–1654",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Jr and J",
      "author" : [ "J.E. Dennis" ],
      "venue" : "J. Moré. Quasi-newton methods, motivation and theory. SIAM review, 19(1):46–89",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "The Journal of Machine Learning Research, 12:2121–2159",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 315–323",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Semi-stochastic gradient descent methods",
      "author" : [ "J. Konečnỳ", "P. Richtárik" ],
      "venue" : "arXiv preprint arXiv:1312.1666",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A simpler approach to obtaining an o (1/t) convergence rate for the projected stochastic subgradient method",
      "author" : [ "S. Lacoste-Julien", "M. Schmidt", "F. Bach" ],
      "venue" : "arXiv preprint arXiv:1212.2002",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the limited memory bfgs method for large scale optimization",
      "author" : [ "D.C. Liu", "J. Nocedal" ],
      "venue" : "Mathematical programming, 45(1-3):503–528",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Res: Regularized stochastic bfgs algorithm",
      "author" : [ "A. Mokhtari", "A. Ribeiro" ],
      "venue" : "arXiv preprint arXiv:1401.7625",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Numerical optimization",
      "author" : [ "J. Nocedal", "S. Wright" ],
      "venue" : "volume 2. Springer New York",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization",
      "author" : [ "A. Rakhlin", "O. Shamir", "K. Sridharan" ],
      "venue" : "arXiv preprint arXiv:1109.5647",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : "The annals of mathematical statistics, pages 400–407",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1951
    }, {
      "title" : "A stochastic gradient method with an exponential convergence rate for finite training sets",
      "author" : [ "N.L. Roux", "M. Schmidt", "F.R. Bach" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2663–2671",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A stochastic quasi-newton method for online convex optimization",
      "author" : [ "N. Schraudolph", "J. Yu", "S. Günter" ],
      "venue" : "Intl. Conf. Artificial Intelligence and Statistics (AIstats)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter" ],
      "venue" : "Mathematical programming, 127(1):3–30",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Stochastic gradient descent (SGD) has become the method of choice for such tasks as it only requires computing stochastic gradients over a small subset of datapoints [2, 18].",
      "startOffset" : 166,
      "endOffset" : 173
    }, {
      "referenceID" : 17,
      "context" : "Stochastic gradient descent (SGD) has become the method of choice for such tasks as it only requires computing stochastic gradients over a small subset of datapoints [2, 18].",
      "startOffset" : 166,
      "endOffset" : 173
    }, {
      "referenceID" : 0,
      "context" : "The first line of work focuses on choosing the appropriate SGD step-size [1, 10, 14].",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "The first line of work focuses on choosing the appropriate SGD step-size [1, 10, 14].",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 13,
      "context" : "The first line of work focuses on choosing the appropriate SGD step-size [1, 10, 14].",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "This scheme is used in [5, 16] where the improved estimate of the gradient combines stochastic gradients computed at the current stage with others used at an earlier stage.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : "This scheme is used in [5, 16] where the improved estimate of the gradient combines stochastic gradients computed at the current stage with others used at an earlier stage.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "A similar approach proposed in [8, 9] combines stochastic gradients with gradients periodically re-computed at a pivot point.",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "A similar approach proposed in [8, 9] combines stochastic gradients with gradients periodically re-computed at a pivot point.",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "Approximate variants known as quasi-Newton methods [6] have thus been developed, such as the popular BFGS or its limited memory version known as LBFGS [11].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "Approximate variants known as quasi-Newton methods [6] have thus been developed, such as the popular BFGS or its limited memory version known as LBFGS [11].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : "Stochastic variants of BFGS have been proposed (oBFGS [17]), for which stochastic gradients replace their deterministic counterparts.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "A regularized version known as RES [12] achieves a sublinear convergence rate with a decreasing step-size by",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : "SQN [3], another related method also requires a decreasing step size to achieve sub-linear convergence.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "Although stochastic second order methods have not be shown to achieve super-linear convergence, they empirically outperform SGD for problems with a large condition number [12].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 7,
      "context" : "In this paper, we propose and analyze a stochastic variant of BFGS that uses a multi-stage scheme similar to [8, 9] to progressively reduce the variance of the stochastic gradients.",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 8,
      "context" : "In this paper, we propose and analyze a stochastic variant of BFGS that uses a multi-stage scheme similar to [8, 9] to progressively reduce the variance of the stochastic gradients.",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "Given that the stochastic gradients are unbiased estimates of the gradient, Robbins and Monro [15] proved convergence of SGD to w assuming a decreasing stepsize sequence.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "A common choice for the step size is [18, 12] a) ηt = η0 t or b) ηt = η0T0 T0 + t (2) where η0 is a constant initial step size and T0 controls the speed of decrease.",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "A common choice for the step size is [18, 12] a) ηt = η0 t or b) ηt = η0T0 T0 + t (2) where η0 is a constant initial step size and T0 controls the speed of decrease.",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "Although the cost per iteration of SGD is low, it suffers from slow convergence for certain illconditioned problems [12].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "The most popular member of this class of quasi-Newton methods is BFGS [13] that incrementally updates an estimate of the inverse Hessian, denoted Jt = H̃ −1 t .",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "3 Stochastic BFGS A stochastic version of BFGS (oBFGS) was proposed in [17] in which stochastic gradients are used for both the determination of the descent direction and the approximation of the inverse Hessian.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "A regularized version of oBFGS (RES) was recently proposed in [12].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "Note that (10) also implies an upper and lower bound on E[Ĵt] [12].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "(11) The convergence proof derived in [12] shows that lower and upper bounds on the Hessian eigenvalues of the sample functions are sufficient to guarantee convergence to the optimum.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "Similar results were derived in [3] for the SQN algorithm.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "We proceed using a technique similar to the one proposed in [8, 9].",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "We proceed using a technique similar to the one proposed in [8, 9].",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "Following the analysis of [8], the variance of vt goes to zero when both w̃ and wt converge to the same parameter w.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "4 Analysis In this section we present a convergence proof for the VITE algorithm that builds upon and generalizes previous analyses of variance reduced first order methods [8, 9].",
      "startOffset" : 172,
      "endOffset" : 178
    }, {
      "referenceID" : 8,
      "context" : "4 Analysis In this section we present a convergence proof for the VITE algorithm that builds upon and generalizes previous analyses of variance reduced first order methods [8, 9].",
      "startOffset" : 172,
      "endOffset" : 178
    }, {
      "referenceID" : 11,
      "context" : "[12]) but allows us to remove this complication, simplifying notation in the analysis which follows.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "This result follows directly from Lemma 3 in [9].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "E‖vt‖ 2 ≤ 4L(f(wt)− f(w ) + f(w̃)− f(w)) The proof is given in [8, 9] and reproduced for convenience in the Appendix.",
      "startOffset" : 63,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "E‖vt‖ 2 ≤ 4L(f(wt)− f(w ) + f(w̃)− f(w)) The proof is given in [8, 9] and reproduced for convenience in the Appendix.",
      "startOffset" : 63,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "5 Experimental Results This section presents experimental results that compare the performance of VITE to SGD, SVRG [8] which incorporates variance reduction and RES [12] which incorporates second order information.",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 11,
      "context" : "5 Experimental Results This section presents experimental results that compare the performance of VITE to SGD, SVRG [8] which incorporates variance reduction and RES [12] which incorporates second order information.",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 3,
      "context" : "We apply least-square regression on the binary version of the COV dataset [4] that contains n = 581, 012 datapoints, each described by d = 54 input features.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "Similarly to [12], we consider different choices for |A| and |B| and pick the best value in a limited interval {1, .",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "Finally, we compared VITE against SGD, RES and SVRG [8, 9].",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "Finally, we compared VITE against SGD, RES and SVRG [8, 9].",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "Therefore, the variance reduced framework we propose can be extended to other quasi-Newton methods, including the widely used L-BFGS and ADAGRAD [7] algorithms.",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "A similar observation was made in [12].",
      "startOffset" : 34,
      "endOffset" : 38
    } ],
    "year" : 2015,
    "abstractText" : "Quasi-Newton methods are widely used in practise for convex loss minimization problems. These methods exhibit good empirical performance on a wide variety of tasks and enjoy super-linear convergence to the optimal solution. For largescale learning problems, stochastic Quasi-Newton methods have been recently proposed. However, these typically only achieve sub-linear convergence rates and have not been shown to consistently perform well in practice since noisy Hessian approximations can exacerbate the effect of high-variance stochastic gradient estimates. In this work we propose VITE, a novel stochastic Quasi-Newton algorithm that uses an existing first-order technique to reduce this variance. Without exploiting the specific form of the approximate Hessian, we show that VITE reaches the optimum at a geometric rate with a constant step-size when dealing with smooth strongly convex functions. Empirically, we demonstrate improvements over existing stochastic Quasi-Newton and variance reduced stochastic gradient methods.",
    "creator" : "LaTeX with hyperref package"
  }
}