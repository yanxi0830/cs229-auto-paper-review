{
  "name" : "1604.01350.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bounded Optimal Exploration in MDP",
    "authors" : [ "Kenji Kawaguchi" ],
    "emails" : [ "kawaguch@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Introduction The formulation of sequential decision making as a Markov decision process (MDP) has been successfully applied to a number of real-world problems. MDPs provide the ability to design adaptable agents that can operate effectively in uncertain environments. In many situations, the environment we wish to model has unknown aspects, and thus the agent needs to learn an MDP by interacting with the environment. In other words, the agent has to explore the unknown aspects of the environment to learn the MDP. A considerable amount of theoretical work on MDPs has focused on efficient exploration, and a number of principled methods have been derived with the aim of learning an MDP to obtain a nearoptimal policy. For example, Kearns and Singh (2002) and Strehl and Littman (2008a) considered discrete state spaces, whereas Bernstein and Shimkin (2010) and Pazis and Parr (2013) examined continuous state spaces.\nIn practice, however, heuristics are still commonly used (Li 2012). The focus of theoretical work (learning a nearoptimal policy within a polynomial yet long time) has apparently diverged from practical needs (learning a satisfactory policy within a reasonable time). In this paper, we modify the prevalent theoretical approach to develop theoretically driven methods that come close to practical needs.\nCopyright c© 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nPreliminaries\nAn MDP (Puterman 2004) can be represented as a tuple (S,A,R, P, γ), where S is a set of states, A is a set of actions, P is the transition probability function, R is a reward function, and γ is a discount factor. The value of policy π at state s, V π(s), is the cumulative (discounted) expected reward, which is given by: V π(s) =\nE [ ∞∑ i=0 γiR (si, π(si), si+1) | s0 = s, π ] , where the expectation is over the sequence of states si+1 ∼ P (S|si, π(si)) for all i ≥ 0. Using Bellman’s equation, the value of the optimal policy or the optimal value, V ∗(s), can be written as V ∗(s) = maxa ∑ s′ P (s\n′|s,a))[R(s, a, s′) + γV ∗(s′)]. In many situations, the transition function P and/or the reward function R are initially unknown. Under such conditions, we often want a policy of an algorithm at time t, At, to yield a value V At(st) that is close to the optimal value V ∗(st) after some exploration. Here, st denotes the current state at time t. More precisely, we may want the following: for all > 0 and for all δ = (0, 1), V At(st) ≥ V ∗(st) − , with probability at least 1 − δ when t ≥ τ , where τ is the exploration time. The algorithm with a policyAt is said to be “probably approximately correct” for MDPs (PAC-MDP) (Strehl 2007) if this condition holds with τ being at most polynomial in the relevant quantities of MDPs. The notion of PAC-MDP has a strong theoretical basis and is widely applicable, avoiding the need for additional assumptions, such as reachability in state space (Jaksch, Ortner, and Auer 2010), access to a reset action (Fiechter 1994), and access to a parallel sampling oracle (Kearns and Singh 1999).\nHowever, the PAC-MDP approach often results in an algorithm over-exploring the state space, causing a low reward per unit time for a long period of time. Accordingly, past studies that proposed PAC-MDP algorithms have rarely presented a corresponding experimental result, or have done so by tuning the free parameters, which renders the relevant algorithm no longer PAC-MDP (Strehl, Li, and Littman 2006; Kolter and Ng 2009; Sorg, Singh, and Lewis 2010). This problem was noted in (Kolter and Ng 2009; Brunskill 2012; Kawaguchi and Araya 2013). Furthermore, in many problems, it may not even be possible to guarantee V At close to V ∗ within the agent’s lifetime. Li (2012) noted that, despite the strong theoretical basis of the PAC-MDP approach,\nar X\niv :1\n60 4.\n01 35\n0v 1\n[ cs\n.A I]\n5 A\npr 2\n01 6\nheuristic-based methods remain popular in practice. This would appear to be a result of the above issues. In summary, there seems to be a dissonance between a strong theoretical approach and practical needs.\nBounded Optimal Learning The practical limitations of the PAC-MDP approach lie in their focus on correctness without accommodating the time constraints that occur naturally in practice. To overcome the limitation, we first define the notion of reachability in model learning, and then relax the PAC-MDP objective based on it. For brevity, we focus on the transition model."
    }, {
      "heading" : "Reachability in Model Learning",
      "text" : "For each state-action pair (s, a), let M(s,a) be a set of all transition models and P̂t(·|s, a) ∈ M(s,a) be the current model at time t (i.e., P̂t(·|s, a) : S → [0,∞)). Define S′(s,a) to be a set of possible future samples as S ′ (s,a) = {s′|P (s′|s, a) > 0}. Let f(s,a) : M(s,a) × S′(s,a) → M(s,a) represent the model update rule; f(s,a) maps a model (in M(s,a)) and a new sample (in S′(s,a)) to a corresponding new model (in M(s,a)). We can then write L = (M,f) to represent a learning method of an algorithm, where M = ∪(s,a)∈(S,A)M(s,a) and f = {f(s,a)}(s,a)∈(S,A).\nThe set of h-reachable models, ML,t,h,(s,a), is recursively defined as ML,t,h,(s,a) ={ P̂ ′ ∈M(s,a)|P̂ ′ = f(s,a)(P̂ , s′) for some P̂ ∈\nML,t,h−1,(s,a) and s′ ∈ S′(s,a) } with the boundary\nconditionMt,0,(s,a) = {P̂t(·|s, a)}. Intuitively, the set of h-reachable models,ML,t,h,(s,a) ⊆ M(s,a), contains the transition models that can be obtained if the agent updates the current model at time t using any combination of h additional samples s′1, s ′ 2, . . . , s ′ h ∼ P (S|s, a). Note that the set of h-reachable models is defined separately for each state-action pair. For example, ML,t,h,(s1,a1) contains only those models that are reachable using the h additional samples drawn from P (S|s1, a1).\nWe define the h-reachable optimal value V d∗L,t,h(s) with respect to a distance function d as\nV d∗L,t,h(s) = max a ∑ s′ P̂ d∗L,t,h(s ′|s, a)[R(s, a, s′) + γV d∗L,t,h(s′)],\nwhere\nP̂ d∗L,t,h(·|s, a) = argmin P̂∈ML,t,h,(s,a) d(P̂ (·|s, a), P (·|s, a)).\nIntuitively, the h-reachable optimal value, V d∗L,t,h(s), is the optimal value estimated with the “best” model in the set of h-reachable models (here, the term “best” is in terms of the distance function d(·, ·))."
    }, {
      "heading" : "PAC in Reachable MDP",
      "text" : "Using the concept of reachability in model learning, we define the notion of “probably approximately correct” in an\nh-reachable MDP (PAC-RMDP(h)). Let P(x1, x2, . . . , xn) be a polynomial in x1, x2, . . . , xn and |MDP| be the complexity of an MDP (Li 2012).\nDefinition 1. (PAC-RMDP(h)) An algorithm with a policy At and a learning method L is PAC-RMDP(h) with respect to a distance function d if for all > 0 and for all δ = (0, 1),\n1) there exists τ = O(P(1/ , 1/δ, 1/(1 − γ), |MDP|, h)) such that for all t ≥ τ ,\nV At(st) ≥ V d∗L,t,h(st)−\nwith probability at least 1− δ, and 2) there exists h∗( , δ) = O(P(1/ , 1/δ, 1/(1 − γ), |MDP|))\nsuch that for all t ≥ 0,\n|V ∗(st)− V d∗L,t,h∗( ,δ)(st)|≤ .\nwith probability at least 1− δ. The first condition ensures that the agent efficiently learns the h-reachable models. The second condition guarantees that the learning method L and the distance function d are not arbitrarily poor.\nIn the following, we relate PAC-RMDP(h) to PAC-MDP and near-Bayes optimality. The proofs are given in the appendix at the end of this paper.\nProposition 1. (PAC-MDP) If an algorithm is PACRMDP(h∗( , δ)), then it is PAC-MDP, where h∗( , δ) is given in Definition 1.\nProposition 2. (Near-Bayes optimality) Consider modelbased Bayesian reinforcement learning (Strens 2000). Let H be a planning horizon in the belief space b. Assume that the Bayesian optimal value function, V ∗b,H , converges to the H-reachable optimal function such that, for all > 0, |V d∗L,t,H(st) − V ∗b,H(st, bt)|≤ for all but polynomial time steps. Then, a PAC-RMDP(H) algorithm with a policy At obtains an expected cumulative reward V At(st) ≥ V ∗b,H(st, bt)−2 for all but polynomial time steps with probability at least 1− δ. Note that V At(st) is the actual expected cumulative reward with the expectation over the true dynamics P , whereas V ∗b,H(st, bt) is the believed expected cumulative reward with the expectation over the current belief bt and its belief evolution. In addition, whereas the PAC-RMDP(H) condition guarantees convergence to an H-reachable optimal value function, Bayesian optimality does not1. In this sense, Proposition 2 suggests that the theoretical guarantee of PACRMDP(H) would be stronger than that of near-Bayes optimality with an H step lookahead.\nSummarizing the above, PAC-RMDP(h∗( , δ)) implies PAC-MDP, and PAC-RMDP(H) is related to near-Bayes optimality. Moreover, as h decreases in the range (0, h∗) or (0, H), the theoretical guarantee of PAC-RMDP(h) becomes\n1A Bayesian estimation with random samples converges to the true value under certain assumptions. However, for exploration, the selection of actions can cause the Bayesian optimal agent to ignore some state-action pairs, removing the guarantee of the convergence. This effect was well illustrated by Li (2009, Example 9).\nAlgorithm 1 Discrete PAC-RMDP"
    }, {
      "heading" : "Parameter: h ≥ 0",
      "text" : "for time step t = 1, 2, 3, . . . do Action: Take action based on Ṽ A(st) in Equation (1) Observation: Save the sufficient statistics Estimate: Update the model P̂t,0\nweaker than previous theoretical objectives. This accommodates the practical need to improve the trade-off between the theoretical guarantee (i.e., optimal behavior after a long period of exploration) and practical performance (i.e., satisfactory behavior after a reasonable period of exploration) via the concept of reachability. We discuss the relationship to bounded rationality (Simon 1982) and bounded optimality (Russell and Subramanian 1995) as well as the corresponding notions of regret and average loss in the appendix.\nDiscrete Domain To illustrate the proposed concept, we first consider a simple case involving finite state and action spaces with an unknown transition function P . Without loss of generality, we assume that the reward function R is known."
    }, {
      "heading" : "Algorithm",
      "text" : "Let Ṽ A(s) be the internal value function used by the algorithm to choose an action. Let V A(s) be the actual value function according to true dynamics P . To derive the algorithm, we use the principle of optimism in the face of uncertainty, such that Ṽ A(s) ≥ V d∗L,t,h(s) for all s ∈ S. This can be achieved using the following internal value function:\nṼ A(s) = max a,\nP̃∈ML,t,h,(s,a)\n∑ s′ P̃ (s′|s, a)[R(s, a, s′) + γṼ A(s′)] (1)\nThe pseudocode is shown in Algorithm 1. In the following, we consider the special case in which we use the sample mean estimator (which determines L). That is, we use P̂t(s\n′|s, a) = nt(s, a, s′)/nt(s, a), where nt(s, a) is the number of samples for the state-action pair (s, a), and nt(s, a, s′) is the number of samples for the transition from s to s′ given an action a. In this case, the maximum over the model in Equation (1) is achieved when all future h observations are transitions to the state with the best value. Thus, Ṽ A can be computed by Ṽ A(s) = maxa ∑ s′∈S nt(s,a,s ′) nt(s,a)+h [R(s, a, s′)+ γṼ A(s′)] + maxs′ h\nnt(s,a)+h [R(s, a, s′) + γṼ A(s′)]."
    }, {
      "heading" : "Analysis",
      "text" : "We first show that Algorithm 1 is PAC-RMDP(h) for all h ≥ 0 (Theorem 1), maintains an anytime error bound and average loss bound (Corollary 1 and the following discussion), and is related with previous algorithms (Remarks 1 and 2). We then analyze its explicit exploration runtime (Definition 3). We assume that Algorithm 1 is used with the sample mean estimator, which determines L. We fix the distance function as d(P̂ (·|s, a), P (·|s, a)) = ‖P̂ (·|s, a) − P (·|s, a)‖1. The proofs are given in the appendix.\nTheorem 1. (PAC-RMDP) LetAt be a policy of Algorithm 1. Let z = max(h, ln(2\n|S||S||A|/δ) (1−γ) ). Then, for all > 0, for all δ =\n(0, 1), and for all h ≥ 0, 1) for all but at most O (\nz|S||A| 2(1−γ)2 ln |S||A| δ\n) time steps,\nV At(st) ≥ V d∗L,t,h(st)− , with probability at least 1− δ, and 2) there exist h∗( , δ) = O(P(1/ , 1/δ, 1/(1− γ), |MDP|)) such\nthat |V ∗(st)−V d∗L,t,h∗( ,δ)(st)|≤ with probability at least 1− δ.\nDefinition 2. (Anytime error) The anytime error t,h ∈ R is the smallest value such that V At(st) ≥ V d∗L,t,h(st)− t,h. Corollary 1. (Anytime error bound) With probability at least 1 − δ, if h ≤ ln(2\n|S||S||A|/δ) (1−γ) , t,h =\nO\n( 3 √ |S||A|\nt(1− γ)3 ln |S||A| δ ln 2|S||S||A| δ\n) ; otherwise,\nt,h = O (√ h|S||A| t(1−γ)2 ln |S||A| δ ) .\nThe anytime T -step average loss is equal to 1 T ∑T t=1(1 − γT+1−t) t,h. Moreover, in this simple problem, we can relate Algorithm 1 to a particular PAC-MDP algorithm and a near-Bayes optimal algorithm.\nRemark 1. (Relation to MBIE) Let m = O( |S| 2(1−γ)4 +\n1 2(1−γ)4 ln |S||A| (1−γ)δ ). Let h ∗(s, a) = n(s,a)z(s,a) 1−z(s,a) , where z(s, a) = 2 √\n2[ln(2|S| − 2)− ln(δ/(2|S||A|m))]/n(s, a). Then, Algorithm 1 with the input parameter h = h∗(s, a) behaves identically to a PAC-MDP algorithm, Model Based Interval Estimation (MBIE) (Strehl and Littman 2008a), the sample complexity of which is O( |S||A| 3(1−γ)6 (|S| + ln |S||A| (1−γ)δ ) ln 1 δ ln 1 (1−γ) )).\nRemark 2. (Relation to BOLT) Let h = H , whereH is a planning horizon in the belief space b. Assume that Algorithm 1 is used with an independent Dirichlet model for each (s, a), which determines L. Then, Algorithm 1 behaves identically to a near-Bayes optimal algorithm, Bayesian Optimistic Local Transitions (BOLT) (ArayaLópez, Thomas, and Buffet 2012), the sample complexity of which is O(H\n2|S||A| 2(1−γ)2 ln |S||A| δ ).\nAs expected, the sample complexity for PAC-RMDP(h) (Theorem 1) is smaller than that for PAC-MDP (Remark 1) (at least when h ≤ |S|(1 − γ)−3), but larger than that for near-Bayes optimality (Remark 2) (at least when h ≥ H). Note that BOLT is not necessarily PAC-RMDP(h), because misleading priors can violate both conditions in Definition 1.\nFurther Discussion An important observation is that, when h ≤ |S|\n(1−γ) ln |S||A| δ\n, the sample complexity of Algorithm 1 is dominated by the number of samples required to refine the model, rather than the explicit exploration of unknown aspects of the world. Recall that the internal value function Ṽ A is designed to force the agent to explore, whereas the use of the currently estimated value function V d∗L,t,0(s) results in exploitation. The difference between Ṽ A and V ∗L,t,0(s) decreases at a rate of O(h/nt(s, a)), whereas the error between V A and V d∗L,t,0(s) decreases at a rate of O(1/ √ nt(s, a)). Thus, Algorithm 1 would stop the explicit exploration much sooner (when Ṽ A and V d∗L,t,0(s) become\nclose), and begin exploiting the model, while still refining it, so that V d∗L,t,0(s) tends to V A. In contrast, PAC-MDP algorithms are forced to explore until the error between V A and V ∗ becomes sufficiently small, where the error decreases at a rate of O(1/ √ nt(s, a)). This provides some intuition to explain why a PAC-RMDP(h) algorithm with small h may avoid over-exploration, and yet, in some cases, learn the true dynamics to a reasonable degree, as shown in the experimental examples.\nIn the following, we formalize the above discussion.\nDefinition 3. (Explicit exploration runtime) The explicit exploration runtime is the smallest integer τ such that for all t ≥ τ , |Ṽ At(st)− V d∗L,t,0(st)|≤ .\nCorollary 2. (Explicit exploration bound) With probability at least 1 − δ, the explicit exploration runtime of Algorithm 1 is O( h|S||A|\n(1−γ) Pr[AK ] ln |S||A| δ ) = O( h|S||A| 2(1−γ)2 ln |S||A| δ\n), where AK is the escape event defined in the proof of Theorem 1.\nIf we assume Pr[AK ] to stay larger than a fixed constant, or to be very small (≤ (1−γ)\n3Rmax ) (so that Pr[AK ] does not ap-\npear in Corollary 2 as shown in the corresponding case analysis for Theorem 1), the explicit exploration runtime can be reduced toO(h|S||A|\n(1−γ) ln |S||A| δ\n). Intuitively, this happens when the given MDP does not have low yet not-too low probability and high-consequence transition that is initially unknown. Naturally, such a MDP is difficult to learn, as reflected in Corollary 2."
    }, {
      "heading" : "Experimental Example",
      "text" : "We compare the proposed algorithm with MBIE (Strehl and Littman 2008a), variance-based exploration (VBE) (Sorg, Singh, and Lewis 2010), Bayesian Exploration Bonus (BEB) (Kolter and Ng 2009), and BOLT (Araya-López, Thomas, and Buffet 2012). These algorithms were designed to be PAC-MDP or near-Bayes optimal, but have been used with parameter settings that render them neither PAC-MDP nor near-Bayes optimal. In contrast to the experiments in previous research, we present results with set to several theoretically meaningful values2 as well as one theoretically nonmeaningful value to illustrate its property3. Because our algorithm is deterministic with no sampling and no assumptions on the input distribution, we do not compare it with algorithms that use sampling, or rely heavily on knowledge of the input distribution.\n2MBIE is PAC-MDP with the parameters δ and . VBE is PACMDP in the assumed (prior) input distribution with the parameter δ. BEB and BOLT are near-Bayes optimal algorithms whose parameters β and η are fully specified by their analyses, namely β = 2H2 and η = H . Following Araya-López, Thomas, and Buffet (2012), we set β and η using the ′-approximated horizon H ≈ dlogγ( ′(1−γ))e = 148. We use the sample mean estimator for the PAC-MDP and PAC-RMDP(h) algorithms, and an independent Dirichlet model for the near-Bayes optimal algorithms.\n3We can interpolate their qualitative behaviors with values of other than those presented here. This is because the principle behind our results is that small values of causes over-exploration due to the focus on the near-optimality.\nWe consider a five-state chain problem (Strens 2000), which is a standard toy problem in the literature. In this problem, the optimal policy is to move toward the state farthest from the initial state, but the reward structure explicitly encourages an exploitation agent, or even an -greedy agent, to remain in the initial state. We use a discount factor of γ = 0.95 and a convergence criterion for the value iteration of ′ = 0.01.\nFigure 1 shows the numerical results in terms of the average reward per time step (average over 1000 runs). As can be seen from this figure, the proposed algorithm worked better. MBIE and VBE work reasonably if we discard the theoretical guarantee. As the maximum reward is Rmax = 1, the upper bound on the value function is ∑∞ i=1 γ\niRmax = 1 1−γRmax = 20. Thus, -closeness does not yield any useful information when ≥ 20. A similar problem was noted by Kolter and Ng (2009) and Araya-López, Thomas, and Buffet (2012).\nIn the appendix, we present the results for a problem with low-probability high-consequence transitions, in which PAC-RMDP(8) produced the best result.\nContinuous Domain In this section, we consider the problem of a continuous state space and discrete action space. The transition function is possibly nonlinear, but can be linearly parameterized as: s(i)t+1 = θ T (i)Φ(i)(st, at) + ζ (i) t , where the state st ∈ S ⊆ RnS is represented by nS state parameters (s(i) ∈ R with i ∈ {1, . . . , ns}), and at ∈ A is the action at time t. We assume that the basis functions Φ(i) : S × A → Rni are known, but the weights θ ∈ Rni are unknown. ζ(i)t ∈ R is the noise term and given by ζ(i)t ∼ N (0, σ2(i)). In other words, P (s\n(i) t+1|st, at) = N (θT(i)Φ(i)(st, at), σ2(i)). For brevity, we focus on unknown transition dynamics, but our method is directly applicable to unknown reward functions if the reward is represented in the above form. This problem is a slightly generalized version of those considered by Abbeel and Ng (2005), Strehl and Littman (2008b), and Li et al. (2011)."
    }, {
      "heading" : "Algorithm",
      "text" : "We first define the variables used in our algorithm, and then explain how the algorithm works. Let θ̂(i) be the vector of the model parameters for the ith state component. Let Xt,i ∈ Rt×ni consist of t input vectors ΦT(i)(s, a) ∈ R1×ni at time t. We then denote the eigenvalue decomposition of the input matrix as XTt,iXt,i = Ut,iDt,i(λ(1), . . . , λ(n))UTt,i, where Dt,i(λ(1), . . . , λ(n)) ∈ Rni×ni represents a diagonal matrix. For simplicity of notation, we arrange the eigenvectors and eigenvalues such that the diagonal elements of Dt,i(λ(1), . . . , λ(n)) are λ(1), . . . , λ(j) ≥ 1 and λ(j+1), . . . , λ(n) < 1 for some 0 ≤ j ≤ n. We now define the main variables used in our algorithm: zt,i := (XTt,iXt,i) −1, gt,i := Ut,iDt,i( 1\nλ(1) , . . . , 1 λ(j) , 0, . . . , 0)UTt,i, and\nwt,i := Ut,iDt,i(0, . . . , 0, 1(j+1), . . . , 1(n))U T t,i. Let ∆(i) ≥ sups,a|(θ(i) − θ̂(i))TΦ(i)(s, a)| be the upper bound on the model error. Define ς(M) = √ 2 ln(π2M2nsh/(6δ)) where M is the number of calls for Ih (i.e., the number of computing r̃ in Algorithm 2).\nWith the above variables, we define the h-reachable model interval Ih as Ih(Φ(i)(s, a), Xt,i)/[h(∆ (i) + ς(M)σ(i))] =\n|ΦT(i)(s, a)gt,iΦ(i)(s, a)|+‖ΦT(i)(s, a)zt,i‖‖wt,iΦ(i)(s, a)‖. The h-reachable model interval is a function that maps a new state-action pair considered in the planning phase, Φ(i)(s, a), and the agent’s experience, Xt,i, to the upper bound of the error in the model prediction. We define the column vector consisting of nS h-reachable intervals as Ih(s, a,Xt) = [Ih(Φ(1)(s, a), Xt,1), . . . , Ih(Φ(nS)(s, a), Xt,nS )]\nT . We also leverage the continuity of the internal value function Ṽ to avoid an expensive computation (to translate the error in the model to the error in value). Assumption 1. (Continuity) There exists L ∈ R such that, for all s, s′ ∈ S, |Ṽ ∗(s)− Ṽ ∗(s′)|≤ L‖s− s′‖.\nWe set the degree of optimism for a state-action pair to be proportional to the uncertainty of the associated model. Using the h-reachable model interval, this can be achieved by simply adding a reward bonus that is proportional to the interval. The pseudocode for this is shown in Algorithm 2."
    }, {
      "heading" : "Analysis",
      "text" : "Following previous work (Strehl and Littman 2008b; Li et al. 2011), we assume access to an exact planning algorithm. This assumption would be relaxed by using a planning method that provides an error bound. We assume that Algorithm 2 is used with least-squares estimation, which determines L. We fix the distance function as d(P̂ (·|s, a), P (·|s, a)) = |Es′∼P̂ (·|s,a)[s\n′] − Es′∼P (·|s,a)[s′]| (since the unknown aspect is the mean, this choice makes sense). In the following, we use n̄ to represent the average value of {n(1), . . . , n(nS)}. The proofs are given in the appendix. Lemma 3. (Sample complexity of PAC-MDP) For our problem setting, the PAC-MDP algorithm proposed by Strehl and Littman (2008b) and Li et al. (2011) has sample complexity\nÕ ( n2S n̄ 2\n5(1−γ)10\n) .\nAlgorithm 2 Linear PAC-RMDP\nParameter: h, δ Optional: ∆(i), L Initialize: θ̂, ∆(i), and L for time step t = 1, 2, 3, ... . . . do\nAction: take an action based on p̂(s′|s, a)← N (θ̂TΦ(s, a), σ2I) r̃(s, a, s′)← R(s, a, s′) + L‖Ih(s, a,Xt−1)‖ Observation: Save the input-output pair (st+1,Φt(st, at)) Estimate:Estimate θ̂(i), ∆(i)(if not given), andL(if not given)\nTheorem 2. (PAC-RMDP) Let At be the policy of Algorithm 2. Let z = max(h2 ln m\n2nsh δ , L 2nS n̄ ln 2m 3 ln nS δ\n). Then, for all > 0, for all δ = (0, 1), and for all h ≥ 0,\n1) for all but at most m′ = O ( zL2nS n̄ ln\n2m 3(1−γ)2 ln 2 nS δ\n) time steps\n(with m ≤ m′), V At(st) ≥ V d∗L,t,h(st)− , with probability at least 1− δ, and\n2) there exists h∗( , δ) = O(P(1/ , 1/δ, 1/(1−γ), |MDP|)) such that |V ∗(st)−V d∗L,t,h∗( ,δ)(st)|≤ with probability at least 1− δ.\nCorollary 3. (Anytime error bound) With probability at least 1 − δ, if h2 ln m\n2nsh δ ≤ L 2nS n̄ ln 2m 3 ln nS δ ,\nt,h = O  5√L4n2Sn̄2 ln2 m t(1− γ) ln 3 nS δ ; otherwise, t,h = O ( h2L2nS n̄ ln 2m t(1−γ) ln 2 nS δ ) .\nThe anytime T -step average loss is equal to 1 T ∑T t=1(1 − γT+1−t) t,h. Corollary 4. (Explicit exploration runtime) With probability at least 1 − δ, the explicit exploration runtime of Algorithm 2 is O ( h2L2nS n̄ lnm\n2 Pr[Ak] ln2 nS δ ln m 2nsh δ\n) =\nO ( h2L2nS n̄ lnm\n3(1−γ) ln 2 nS δ ln m 2nsh δ ) , where AK is the escape\nevent defined in the proof of Theorem 2."
    }, {
      "heading" : "Experimental Examples",
      "text" : "We consider two examples: the mountain car problem (Sutton and Barto 1998), which is a standard toy problem in the literature, and the HIV problem (Ernst et al. 2006), which originates from a real-world problem. For both examples, we compare the proposed algorithm with a directly related PAC-MDP algorithm (Strehl and Littman 2008b; Li et al. 2011). For the PAC-MDP algorithm, we present the results with set to several theoretically meaningful values and one theoretically non-meaningful value to illustrate its property4. We used δ = 0.9 for the PAC-MDP and PAC-RMDP algorithms5. The -greedy algorithm is executed with = 0.1. In the planning phase, L is estimated as L← maxs,s′∈Ω|Ṽ A(s)− Ṽ A(s′)|/‖s− s′‖, where Ω is the set of states that are visited in the planning phase (i.e., fitted\n4See footnote 3 on the consideration of different values of . 5We considered δ = [0.5, 0.8, 0.9, 0.95], but there was no\nchange in any qualitative behavior of interest in our discussion.\nvalue iteration and a greedy roll-out method). For both problems, more detailed descriptions of the experimental settings are available in the appendix.\nMountain Car In the mountain car problem, the reward is negative everywhere except at the goal. To reach the goal, the agent must first travel far away, and must explore the world to learn this mechanism. Each episode consists of 2000 steps, and we conduct simulations for 100 episodes.\nThe numerical results are shown in Figure 2. As in the discrete case, we can see that the PAC-RMDP(h) algorithm worked well. The best performance, in terms of the total reward, was achieved by PAC-RMDP(10). Since this problem required a number of consecutive explorations, the random exploration employed by the -greedy algorithm did not allow the agent to reach the goal. As a result of exploration and the randomness in the environment, the PAC-MDP algorithm reached the goal several times, but kept exploring the environment to ensure near-optimality. From Figure 2, we can see that the PAC-MDP algorithm quickly converges to good behavior if we discard the theoretical guarantee (the difference between the values in the optimal value function had an upper bound of 120, and the total reward had an upper bound of 2000. Hence, > 2000 does not yield a useful theoretical guarantee).\nSimulated HIV Treatment This problem is described by a set of six ordinary differential equations (Ernst et al. 2006). An action corresponds to whether the agent administers two treatments (RTIs and PIs) to patients (thus, there are four actions). Two types of exploration are required: one to learn the effect of using treatments on viruses, and another to learn the effect of not using treatments on immune systems. Learning the former is necessary to reduce the population of viruses, but the latter is required to prevent the overuse of treatments, which weakens the immune system. Each episode consists of 1000 steps (i.e., days), and we conduct simulations for 30 episodes.\nAs shown in Figure 3, the PAC-MDP algorithm worked reasonably well with = 3010. However, the best total reward did not exceed 3010, and so the PAC-MDP guarantee with = 3010 does not seem to be useful. The -greedy algorithm did not work well, as this example required sequential exploration at certain periods to learn the effects of treatments.\nConclusion In this paper, we have proposed the PAC-RMDP framework to bridge the gap between theoretical objectives and practical needs. Although the PAC-RMDP(h) algorithms worked well in our experimental examples with small h, it is possible to devise a problem in which the PAC-RMDP algorithm should be used with large h. In extreme cases, the algorithm would reduce to PAC-MDP. Thus, the adjustable theoretical guarantee of PAC-RMDP(h) via the concept of reachability seems to be a reasonable objective.\nWhereas the development of algorithms with traditional objectives (PAC-MDP or regret bounds) requires the consideration of confidence intervals, PAC-RMDP(h) concerns\na set of h-reachable models. For a flexible model, the derivation of the confidence interval would be a difficult task, but a set of h-reachable models can simply be computed (or approximated) via lookahead using the model update rule. Thus, future work includes the derivation of a PAC-RMDP algorithm with a more flexible and/or structured model.\nAcknowledgment The author would like to thank Prof. Michael Littman, Prof. Leslie Kaelbling and Prof. Tomás Lozano-Pérez for their thoughtful comments and suggestions. We gratefully acknowledge support from NSF grant 1420927, from ONR grant N00014-14-1-0486, and from ARO grant W911NF1410433. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors.\nAppendix A\nA1. Proofs of Propositions 1 and 2 In this section, we present the proofs of Propositions 1 and 2. Proposition 1. (PAC-MDP) PAC-RMDP(h∗( , δ)) implies PAC-MDP, where h∗( , δ) is given in Definition 1.\nProof. For any PAC-RMDP(h∗( , δ)) algorithm, Definition 1 implies that V A(st) ≥ V ∗h∗(st) − ≥ V ∗(st) − 2 with probability at least 1− 2δ for all but polynomial time steps. This satisfies the condition of the PAC-MDP.\nProposition 2. (Near-Bayes optimality) Consider the model-based Bayesian reinforcement learning (Strens 2000). Let H be a planning horizon in the belief space b. Assume that the Bayesian optimal value function, V ∗b,H , converges to the H-reachable optimal function such that, for all > 0, |V ∗L,t,H(st) − V ∗b,H(st, bt)|≤ for all but polynomial time steps. Then, a PAC-RMDP(H) algorithm with a policy At obtains an expected cumulative reward V At(st) ≥ V ∗b,H(st, bt)− 2 for all but polynomial time steps with probability at least 1− δ.\nProof. It directly follows Definition 1 and the assumption. For all but polynomial time steps, with probability at least 1 − δ, V A(st) ≥ V ∗L,t,H(st)− ≥ V ∗b,H(st, bt)− 2 .\nA2. Relationship to Bounded Rationality and Bounded Optimality As the concept of PAC-RMDP considers the inherent limitations of a decision maker, it shares properties with the concepts of bounded rationality (Simon 1982) and bounded optimality (Russell and Subramanian 1995).\nBounded rationality and bounded optimality focus on limitations in the planning phase (e.g., computational resources). In contrast, PAC-RMDP considers limitations in the learning phase (e.g., the agent’s lifetime). As in the case of bounded rationality, the performance guarantee of a PAC-RMDP(h) algorithm can be arbitrary, depending on the choice of h. On the contrary, bounded optimality solves the problem of arbitrariness, seemingly at the cost of applicability. It requires a strong notion of optimality, similar to instance optimality; roughly, we must find the optimal algorithm given the available computational resources. Automated optimization over the set of algorithms is a difficult task. Zilberstein (2008) claims that bounded optimality is difficult to achieve, resulting in very few successful examples, and is not, in practice, as promising as other bounded rational methods. However, in future research, it would be interesting to compare PAC-RMDP with a possible relaxation of PAC-MDP based on a concept similar to bounded optimality.\nA3. Corresponding Notions of Regret and Average Loss In the definition of PAC-RMDP(h), our focus is on learning useful models, enabling us to obtain high rewards in a short period of time. Instead, one may wish to guarantee the worst total reward in a given time horizon T . There are several ways to achieve this goal. One solution is to minimize the expected T -step regret bound rA(T ), given by\nrA(T ) ≥ V ∗(s0, T )− V A(s0, T ). (1)\nIn this case, V ∗(s0, T ) = E [∑T i=0 γ iR (s∗i , π ∗(si), s ∗ i+1) ] , where the sequence of states s∗0, s∗1, . . . , s∗T with s∗0 = s0 is generated\nwhen the agent follows the optimal policy π∗ from s0, and V A(s0, T ) = E [∑T i=0 γ iR (si,Ai(si), si+1) ] , where the sequence of states s0, s1, . . . , sT is generated when the agent follows policyAi. Since one mistake in the early stages may make it impossible to return to the optimal state sequence s∗i , all the regret approaches in the literature rely on some reachability assumptions in the state space; for example, Jaksch, Ortner, and Auer (2010) assumed that every state was reachable from every other state within a certain (average) number of steps.\nAnother approach is to minimize the expected T -step average loss bound rA(T ), which obviates the need for any reachability assumptions in the state space:\n`A(T ) ≥ 1 T T∑ t=0 [ V ∗(st, T )− V A(st, T ) ] , (2)\nwhere st is the state visited by algorithm A at time t. The value functions inside the sum are defined as V ∗(st, T ) = E [∑T−t\ni=0 γ iR (s∗t+i, π ∗(st+i), s ∗ t+i+1) ] with s∗t = st and V A(s0, T ) = E [∑T−t i=0 γ iR (s∗t+i,At(st+i), st+i+1) ] . By averaging the\nT -step regrets (i.e., losses) of the T initial states s0, s1, . . . , sT visited by A, the average loss mitigates the effects of irreversible mistakes in the early stages that may dominate the regret.\nThe expected h-reachable regret bound rAh (T ) and average loss bound `Ah (T ) are defined as rAh (T ) ≥ V ∗L,t,h(s0, T )−V A(s0, T ) and `Ah (T ) ≥ 1T ∑T t=1 [ V ∗L,t,h(st, T )− V A(st, T ) ] . That is, they are the same as the standard expected regret and average loss, respectively, with the exception that the optimal value function V ∗ has been replaced by the h-reachable optimal value function V ∗L,t,h(st).\nWhile the definition of PAC-RMDP(h) focuses on exploration, the proposed PAC-RMDP(h) algorithms maintain anytime expected h-reachable average loss bounds and anytime error bounds, and thus the performances of our algorithms are expected to improve with time, rather than after some number of exploration steps.\nA4. Proofs of Theoretical Results for Algorithm 1 We first verify the main properties of Algorithm 1 and then analyze a practically relevant property of the algorithm in the subsection of Further Discussion. We assume that Algorithm 1 is used with the sample mean estimator, which determines L.\nMain Properties To compare the results with those of past studies, we assume that Rmax ≤ c for some fixed constant c. The effect of this assumption can be seen in the proof of Theorem 1. Algorithm 1 requires no input parameter related to and δ. This is because the required degree of optimism can be determined independently of the unknown aspect of the world. This means that Theorem 1 holds at any time during an execution for a pair of corresponding and δ.\nLemma 1. (Optimism) For all s ∈ S and for all t, h ≥ 0, the internal value Ṽ At(s) used by Algorithm 1 is at least the h-reachable optimal value V ∗L,t,h(s); Ṽ At(s) ≥ V ∗L,t,h(s).\nProof. The claim follows directly from the construction of Algorithm 1. It can be verified by induction on each step of the value iteration or the roll-out in a planning algorithm.\nTheorem 1. (PAC-RMDP) Let At be a policy of Algorithm 1. Let z = max(h, ln(2 |S||S||A|/δ) (1−γ) ). Then, for all > 0, for all δ = (0, 1), and for all h ≥ 0,\n1) for all but at most O (\nz|S||A| 2(1−γ)2 ln |S||A| δ ) time steps, V At(st) ≥ V ∗L,t,h(st)− , with probability at least 1− δ, and\n2) there exists h∗( , δ) = O(P(1/ , 1/δ, 1/(1− γ), |MDP|)) such that |V ∗(st)− V ∗L,t,h∗( ,δ)(st)|≤ with probability at least 1− δ.\nProof. Let K be a set of state-action pairs where the agent has at least m samples (this corresponds to the set of known state-action pairs described by Kearns and Singh (2002)). With the boundary condition V A(s, 0) = 0, define the mixed value function V A(s,H) with a finite horizon H ′ = 1\n1−γ ln 6Rmax (1−γ) as\nV A(s,H ′) =\n{∑ s′ P (s\n′|s,A(s))[R(s,A(s), s′) + γV A(s′, H ′ − 1)] if (s,A(s)) ∈ K maxP̃∈ML,t,h,(s,a) ∑ s′ P̃ (s ′|s,A(s))[R(s,A(s), s′) + γV A(s′, H ′ − 1)] otherwise\nLet AK be the escape event in which a pair (s, a) /∈ K is generated for the first time when starting at state st, following policy At, and transitioning based on the true dynamics P for H ′ steps. Then, for all t, h ≥ 0, with probability at least 1− δ/2,\nV At(st) ≥ V At(st, H ′)− Rmax 1− γ Pr(Ak)− 6\n≥ Ṽ At(st)− Rmax 1− γ Pr(Ak)− 3 − Rmax 1− γ\n( h\nm +\n√ 2 ln(2|S|+1|S||A|/δ)\nm\n)\n≥ V ∗L,t,h(st)− Rmax 1− γ Pr(Ak)− 3 − Rmax 1− γ\n( h\nm +\n√ 2 ln(2|S|+1|S||A|/δ)\nm\n) .\nThe first inequality follows from the fact that V At(st) and V At\n(st) are only different when event AK occurs, and their difference is bounded above by Rmax\n1−γ (this is the upper bound on the value Ṽ (st)). Furthermore, the finite horizon approximation adds an error of 1/6 . A more detailed argument only involves algebraic manipulations that mirror the proofs given by Strehl and Littman (2008a, Lemma 3) and Kearns and Singh (2002, Lemma 2).\nThe second inequality follows from the fact that V A is different from Ṽ A only for the state-action pairs (s, a) ∈ K, for which Ṽ At(st) deviates from V\nAt (st) by at most Rmax1−γ ( h m\n+ √\n2 ln(2|S|+1|S||A|/δ)/m) with probability at least 1−δ/2. This is because |Ṽ At(st)− V AtL,t,0(st)|≤ Rmax 1−γ h m with certainty, and |V At L,t,0(st) − V At(st)|≤ Rmax 1−γ √ 2 ln(2|S|+1|S||A|/δ)/m with probability at least 1− δ/2 (the later is due to the result of Weissman et al. (2003, Theorem 2.1) and the union bound for state-action pairs). The third inequality follows from Lemma 1. Therefore, if h ≤ √ 2m ln(2|S|+1|S||A|/δ),\nV At(st) ≥ V ∗L,t,h(st)− Rmax 1− γ Pr(Ak)− 3 − 2Rmax 1− γ\n√ 2 ln(2|S|+1|S||A|/δ)\nm .\nIf h > √ 2m ln(2|S|+1|S||A|/δ),\nV At(st) ≥ V ∗L,t,h(st)− Rmax 1− γ Pr(Ak)− 3 − 2Rmax 1− γ h m .\nLet us consider the case where h ≤ √ 2m ln(2|S|+1|S||A|/δ). We fix m = 72R 2 max ln(2\n|S|+1|S||A|/δ) 2(1−γ)2 to give 3 in the last term\non the right-hand side. If Pr(AK) ≤ (1−γ)3Rmax for all t, V At(st) ≥ V ∗L,t,h(st)− with probability at least 1− δ/2. For the case where Pr(AK) > (1−γ) 3Rmax for some t, we define an independent random event A′K such that Pr(A ′ K) = (1−γ) 3Rmax\n< Pr(AK). According to the Chernoff bound, for all k ≥ 4, with probability at least 1− δ/2, the event AK will occur at least k times after\n2k Pr(A′K) ln 2δ time steps. Thus, by applying the union bound on |S| and |A|, we have a probability of at least 1 − δ/2 of event AK occurring at least m times for all state-action pairs after O ( m|S||A| Pr(A′k) ln |S||A|δ ) = O ( mRmax|S||A| (1−γ) ln |S||A| δ ) time steps.\nLet us carefully consider what this means. Whenever AK occurs, the sample is used to minimize the error between V A and Ṽ A by the definition of AK . Since Ṽ (s) ≥ V ∗L,t,h(s) holds at any time, whenever AK occurs, the sample is used to reduce the error in V At(st) ≥ Ṽ At(st) − (error) ≥ V ∗L,t,h(st) − (error) (note that if Ṽ (s) ≥ V ∗L,t,h(s) holds randomly, this event must occur concurrently with AK to reduce the error on the right-hand side). Thus, after this number of time steps, Pr(AK) goes to zero with probability at least 1− δ/2. Hence, from the union bound, the above inequality becomes V A(st) ≥ V ∗L,t,h(st)− 23 with probability at least 1− δ.\nFor the case where h > √ 2m ln(2|S|+1|S||A|/δ), we fix m = hRmax6 (1−γ) . The rest of the proof follows that for the case of smaller values of h. Therefore, we have proved the first part of the statement. Finally, we consider the second part of the statement. Let P̂t,h(·|s, a) be the future model obtained by updating the current model P̂(·|s, a) with h random future samples (h samples drawn from P (S|s, a) for each (s, a) ∈ (S,A)). Using a result given by Weissman et al. (2003, Theorem 2.1), we know that for all s ∈ S, with probability at least 1− δ,\nmax s,a ‖P̂t,h(·|s, a)− P (·|s, a)‖1≤\n√ 2 ln(2|S|+1|S||A|/δ)\nnt,min + h ,\nwhere nt,min = mins,a nt(s, a). Now, if we use the distance function d(P̂ (·|s, a), P (·|s, a)) = ‖P̂ (·|s, a) − P (·|s, a)‖1 to define the h-reachable optimal function,\n|V ∗(st)− V d∗L,t,h∗( ,δ)(st)| ≤ Rmax 1− γ max s,a ‖P d∗L,t,h(·|s, a)− P (·|s, a)‖1\n= Rmax 1− γ max s,a\nmin P̂∈ML,t,h,(s,a)\n‖P̂ (·|s, a)− P (·|s, a)‖1\n≤ Rmax 1− γ\n√ 2 ln(2|S|+1|S||A|/δ)\nnt,min + h ,\nThe last inequality follows that the models reachable with h random samples, P̂t,h(·|s, a) , are contained in a set of hreachable models and the best h-reachable model, P d∗L,t,h(·|s, a), explicitly minimize the norm, resulting in that P d∗L,t,h(·|s, a) is at least as good as P̂t,h(·|s, a) in terms of the norm. The right-hand side of the above inequality becomes less than or equal to when h← h∗( , δ) = 2R 2 max ln(2\n|S|+1|S||A|/δ) 2(1−γ)2 . Thus, we have the second part of the statement.\nCorollary 1. (Anytime error bound) With probability at least 1− δ, if h ≤ ln(2 |S||S||A|/δ) (1−γ) ,\nt,h = O\n( 3 √ |S||A|\nt(1− γ)3 ln |S||A| δ ln 2|S||S||A| δ\n) ,\nand otherwise,\nt,h = O (√ h|S||A| t(1− γ)2 ln |S||A| δ ) .\nProof. From Theorem 1, if t = c z|S||A| 2(1−γ)2 ln |S||A| δ with c being some fixed constant, V A(st) ≥ V ∗L,t,h(st) − with probability at least 1 − δ. Since this holds for all t ≥ 0 with corresponding and δ, it implies that 2 ≤ A z|S||A|\nt(1−γ)2 ln |S||A| δ with probability at least 1 − δ.\nSubstituting z = max(h, ln(2 |S||S||A|/δ) (1−γ) ) yields the statement.\nThe anytime T -step average loss is equal to 1 T ∑T t=1(1−γ\nT+1−t) t,h,δ. Since the errors considered in Theorem 1 and Corollary 3 are for an infinite horizon, the factor (1 − γT+1−t) translates the infinite horizon error to the T -step finite horizon error (this can be seen when we modify the proof of Theorem 1 by replacing 1\n1−γ with 1−γT+1−t 1−γ ).\nCorollary 2. (Explicit exploration runtime) With probability at least 1 − δ, the explicit exploration runtime of Algorithm 1 is O( h|S||A|\n(1−γ) Pr[AK ] ln |S||A| δ ) = O( h|S||A| 2(1−γ)2 ln |S||A| δ ), where AK is the escape event defined in the proof of Theorem 1.\nProof. The proof directly follows that of Theorem 1 with z. Compared to the sample complexity of Algorithm 1, z is replaced by h based on the proof of Theorem 1.\nA5. Additional Experimental Example for Discrete Domain Figure 1 shows the results in the main paper along with 10% and 90% values.Aside from the proposed algorithm, only BOLT gathered better rewards than a greedy algorithm while maintaining the claimed theoretical guarantee.\nIn this example, our proposed algorithm worked well and maintained its theoretical guarantee. One might consider the theoretical guarantee of PAC-RMDP, especially PAC-RMDP(1), to be too weak. Two things should be noted. First, the 1- reachable value function is not the value function that can be obtained with just one additional sample, but requires an additional sample for all |S||A| state-action pairs. Second, in contrast to Bayesian optimality, the 1-reachable value function is not the value function believed to be obtained with |S||A| additional samples, but is possibly reachable in terms of the unknown true world dynamics with the new samples.\nHowever, it is certainly possible to devise a problem such that PAC-RMDP(1) is not guaranteed to conduct sufficient exploration. As an example, we consider a modified version of the five-state chain problem, where the probability of successfully moving away from the initial state is very small (= 0.05), thus requiring more extensive exploration. We modified the transition model as follows: Let a1 be the optimal action that moves the agent away from the initial state. For i = {2, 3, 4, 5}, Pr(si, a1, smin(i+1,5)) = 0.99 and Pr(si, a1, s1) = 0.01. For i = 1, Pr(si, a1, si+1)) = 0.05 and Pr(si, a1, s1) = 0.95. For action a2 and any si, Pr(si, a2, s1) = 1. The numerical results for this example are shown in Figure 2. As expected, the PAC-RMDP(1) algorithm often became stuck in the initial state.\nA6. Proofs of Theoretical Results for Algorithm 2 We assume that Algorithm 2 is used with the least square estimation, which determines L. Because the true world dynamics are assumed to have the parametric form P (s′|s, a) = N (θTΦ(s, a), σ2I) with a known σ, their unknown aspect is attributed to the weight vector θ. Therefore, we discuss h-reachability in terms of θ̂ instead of P̂ . For each ith component, Let θ̂∗(i),h,(s,a) be the best h-reachable model parameter corresponding to the best h-reachable models, P̂ ∗L,t,h (we drop the index L, t and d for brevity); using the set θ̂∗(i),h,(s,a) for every (s, a) pair results in the h-reachable value function V d∗L,t,h. Note that θ̂(i) is the current model parameter. In the following, we make a relatively strict assumption to simplify the analysis: when they are not provided as inputs, the estimated values of L and ∆(i) are correct in that they satisfy Assumption 2 and ∆(i) ≥ sups,a|(θ(i)− θ̂(i))TΦ(i)(s, a)|. This assumption can be relaxed by allowing the correctness to be violated with a constant probability. In such a case, we must force the random event to occur concurrently with the escape event, as discussed in the proof of Theorem 1 (the easiest way to do so is to take a union bound over the time steps until convergence). Furthermore, if we can specify the inputs L and ∆(i), there is no need for this assumption. Lemma 2. (Correctness of the h-reachable model interval) For the entire execution of Algorithm 2, for all state components 1 ≤ i ≤ ns, for all t, h ≥ 0, and for all (s, a) ∈ (S,A), the following inequality holds with probability at least 1− δ/2:∣∣∣[θ̂(i) − θ̂∗(i),h,(s,a)]TΦ(i)(s, a)∣∣∣ ≤ Ih(Φ(i)(s, a), Xt). Proof. Let s∗1 ∈ S′(s,a) be the future possible observation from which the current model parameter θ̂(i) is updated to θ̂∗(i),1,(s,a). Then,∣∣∣[θ̂∗(i),1,(s,a) − θ̂(i)]TΦ(i)(s, a)∣∣∣ = ∣∣∣ΦT(i)(s, a)(XTt Xt)−1Φ(i)(s, a)[s∗1 − θ̂∗T(i),1,(s,a)Φ(i)(s, a)]∣∣∣\n≤ ∣∣∣∣ΦT(i)(s, a)Dt( 1λ(1) , . . . , 1λ(n) )UtTΦ(i)(s, a)(∆(i) + ς(M)σ(i)) ∣∣∣∣ . The first line follows directly from a result given by Cook (1977, Equation (5)). The second line is due to the following: with probability\nat least 1− 1 2 e−ς 2(M)/2,\ns∗1 − θ̂∗T(i),1,(s,a)Φ(i)(s, a) ≤ θT(i)Φ(i)(s, a)− θ̂∗T(i),1,(s,a)Φ(i) + ς(M)σ(i) ≤ |θT(i)Φ(i)(s, a)− θ̂∗T(i),1,(s,a)Φ(i)(s, a)|+ς(M)σ(i) ≤ |θT(i)Φ(i)(s, a)− θ̂T(i)Φ(i)(s, a)|+ς(M)σ(i) ≤ ∆(i) + ς(M)σ(i)\nwhere the first inequality follows that Pr(st+1 > θT(i)Φ(i)(s, a) + ς(M)σ(i)) < 1 2 e−ς 2(M)/2 and the third inequality follows the choice of the distance function d (i.e., the mean prediction with the best h reachable model is at least as good as that of the best h− 1 model). We then separate the above into two terms with large and small eigenvalues: with probability at least 1− 1\n2 e−ς 2(M)/2,∣∣∣[θ̂∗(i),1(s,a) − θ̂(i)]TΦ(i)(s, a)∣∣∣ ≤ |ΦT(i)(s, a)UtDt( 1λ(1) , . . . , 1λ(j) , 0, . . . , 0)UtTΦ(i)(s, a) ( ∆(i) + ς(M)σ(i) ) + ΦT(i)(s, a)UtDt(0, . . . , 0, 1\nλ(j+1) , . . . ,\n1\nλ(n) )Ut\nTΦ(i)(s, a)(∆ (i) + ς(M)σ(i))|.\nWith wt, we can rewrite part of the second term as UD(0, . . . , 0, 1λ(j+1) , . . . , 1 λ(n) )UT = UD( 1 λ(1) , . . . , 1 λ(n) )UTwt. Then, with gt and\nzt, with probability at least 1− 12e −ς2(M)/2,∣∣∣[θ̂∗(i),1,(s,a) − θ̂(i)]TΦ(i)(s, a)∣∣∣ ≤ (∆(i) + ς(M)σ(i)) ∣∣∣ΦT(i)(s, a)gtΦ(i)(s, a) + ΦT(i)(s, a)ztwtΦ(i)(s, a)∣∣∣ .\nThus, by applying the union bound for h, with probability at least 1− h 2 e−ς 2(M)/2,∣∣∣[θ̂∗(i),h,(s,a) − θ̂(i)]TΦ(i)(s, a)∣∣∣ ≤ h ∣∣∣[θ̂∗(i),1,(s,a) − θ̂(i)]TΦ(i)(s, a)∣∣∣ ≤ h(∆(i) + ς(M)σ(i))\n∣∣∣ΦT(i)(s, a)gtΦ(i)(s, a) + ΦT(i)(s, a)ztwtΦ(i)(s, a)∣∣∣ ≤ Ih(Φ(i)(s, a), Xt).\nFor ns components, the above inequality holds with probability at least 1 − nsh2 e −ς2(M)/2 (union bound). For all M ≥ 1, the above inequality holds with probability at least 1 − nsh 2 ∑∞ M=1 e −ς2(M)/2 (union bound). Substituting ς(M) = √\n2 ln(π2M2nsh/(6δ)), we obtain the statement.\nIn Lemma 3 and Theorem 2, following previous work (Strehl and Littman 2008b; Li et al. 2011), we assume that an exact planning algorithm is accessible. This assumption will be relaxed by using a planning method that provides an error bound. We also assume that Rmax ≤ c1, ∆(i) ≤ c2, and ‖θ‖≤ c3 for some fixed constants c1, c2, and c3. Removing this assumption results in these quantities appearing in the sample complexity, but produces no exponential dependence (thus, the sample complexity\nremains polynomial). We assume that M = O(the number of samples), meaning that a planing algorithm calls Ih every iteration at most for a constant number of times. In the following, we use n̄ to represent the average value of {n(1), ..., n(nS)}. Before analyzing the proposed algorithm, we re-derive the sample complexity of an existing PAC-MDP algorithm (Strehl and Littman 2008b; Li et al. 2011) for our problem setting. Lemma 3. (Sample complexity of PAC-MDP) With an appropriate parameter setting, the PAC-MDP algorithm proposed by Strehl and Littman (2008b) and Li et al. (2011) has the following sample complexity:\nÕ\n( n2Sn̄ 2\n5(1− γ)10\n) .\nProof. The proof follows directly from Theorems 1 and 3 in the previous work of Li et al. (2011). The only difference is that we need to take a union bound of different components Φ(i) with varying domains, codomains and dimensions n(s).\nTheorem 2. (PAC-RMDP) Let At be a policy of Algorithm 2. Let z = max(h2 ln m 2nsh δ , L 2nS n̄ ln 2m 3 ln nS δ\n). Then, for all > 0, for all δ = (0, 1), and for all h ≥ 0,\n1) for all but at most m′ = O ( zL2nS n̄ ln\n2m 3(1−γ)2 ln 2 nS δ ) time steps (with m ≤ m′), V At(st) ≥ V ∗L,t,h(st)− with probability at least 1− δ,\nand 2) there exists h∗( , δ) = O(P(1/ , 1/δ, 1/(1− γ), |MDP|)) such that |V ∗(st)− V ∗L,t,h∗( ,δ)(st)|≤ with probability at least 1− δ.\nProof. Let Ṽ A be the internal value function used in Algorithm 2. We prove the statement by following the structure of the proof of Theorem 1. DefineK,m,AK , V , andH in the same manner as in the proof of Theorem 1, and let the vector consisting of nS estimation error intervals be ER(s, a) = (|(θ(1) − θ̂(1))TΦ(1)(s, a)|, . . . , |(θ(ns) − θ̂(ns))\nTΦ(ns)(s, a)|. By following the proof of Theorem 1, with probability at least 1− δ/2 (due to Lemma 2),\nV A(st) ≥ Ṽ A(st)− Rmax 1− γ Pr(Ak)− 3 − L ( max s,a ‖Ih(s, a,Xm′)‖+ max s,a ‖ER(s, a)‖ ) ≥ V ∗L,t,h(st)−\nc1 1− γ Pr(Ak)− 3 − L ( max s,a ‖Ih(s, a,Xm′)‖+ max s,a ‖ER(s, a)‖ ) .\nIn the second line, we used the assumption Rmax ≤ c1. In the first line, maxs,a L‖Ih(s, a,Xt)‖ is the difference between Ṽ A(st) and V ∗L,t,0(st), and maxs,a L‖ER(s, a)‖ is the difference between V ∗L,t,0(st) and V A. The second line follows from the fact that Ṽ A ≥ V ∗L,t,h(st) because of the correctness of Ih shown in Lemma 2 and the assignment of the most optimistic value within the interval Ih (based on Assumptions 1 and 2). We now impose an upper bound on ‖Ih(s, a,Xt)‖ and ‖ER(s, a)‖. Following a proof given by Li et al. (2011, Theorem 1) with the assumption ∆(i) ≤ c2 and ‖θ‖≤ c3, with probability at least 1− δ4nS ,∣∣∣(θ(i) − θ̂(i))TΦ(i)(s, a)∣∣∣ ≤ ‖q̄‖∆E(θ̂) + ‖ū‖≤ 2c3√n(i) lnm\nm1/4\n( 24c2 ln\n8nS δ\n)1/4 + (2c3 √ lnm+ 5) √ n(i)√\nm\n≤ O\n( (n(i) lnm) 1/2(ln(nS/δ)) 1/4\nm1/4\n) ,\nwhere ‖q̄‖, ‖ū‖ and ∆E(θ̂) are as defined by Li et al. (2011). Since ΦT(i)zt(st+1 − θ̂Tt+1Φ(i)) = θ̂t+1 − θ̂t, there exist θ̂ and θ̂′ such that∥∥∥ΦT(i)(s, a)zt(∆(i) + ς(M)σ(i))∥∥∥ ≤ ‖θ̂ − θ̂′‖≤ ‖θ̂‖+‖θ̂′‖≤ 2c3, where we use the assumption ‖θ‖≤ c3. Then, following the proofs of Lemmas 11, 12, and 13 given by Auer (2002),\nIh(Φ(i)(s, a), Xt)\nh ≤ (∆(i) + ς(M)σ(i)) ∑ j:λj≥1 Φ2j λj + ‖θ̂ − θ̂′‖ √ ∑ j:λj<1 Φ2j\n≤ 20(c2 +\n√ 2 ln(π2M2nsh/(6δ))σ(i))n ln(m)\nm + 2c3 √ 20n(i) m\n≤ O (√\nn(i)√ m\nlnm √ ln(m2nsh/(6δ)) ) .\nIf h ≤ O( m 1/2(lnnS/δ) 1/4\n(lnm)1/2(ln(m2nsh/(6δ)))1/2 ), with probability at least 1− ns δ4ns − δ/2,\nV A(st) ≥ V ∗L,t,h(st)− c1 Pr(Ak)\n1− γ − 3 −O\n( Ln\n1/2 S n̄ 1/2(lnm)1/2(ln(nS/δ)) 1/4\nm1/4\n) .\nIf h > O( m 1/2(lnnS/δ) 1/4\n(lnm)1/2(ln(m2nsh/(6δ)))1/2 ), with probability at least 1− ns δ4ns − δ/2,\nV A(st) ≥ V ∗L,t,h(st)− c1 Pr(Ak)\n1− γ − 3 −O\n( Lhn\n1/2 S n̄ 1/2\n√ m\nlnm √ ln(m2nsh/(6δ)) ) .\nTo have /3 in the last term, we fix m = O(L 4n2S n̄ 2 ln4m 4 ln nS δ ) for the former case, and m = O(L 2h2nS n̄ ln 2m ln(m2nsh/(6δ)) 2 ) for the latter case. Then, the rest of the first part of the statement follows from the proof of Theorem 1. That is, we can show that by applying the Chernoff bound, the escape event happens no more than the sample complexity in the statement with probability 1 − δ/2 unless the term c1 Pr(Ak)\n1−γ is negligible. Taking union bound on the failure probability, we obtain the sample complexity in the statement with probability at leat 1− δ.\nFinally, we consider the second part of the statement, following the proof in Theorem 1. Let θ̂(i),h,(s,a) be the future model parameter obtained by updating the current model θ̂(i) with h random future samples (h samples drawn from P (S|s, a) for each (s, a) ∈ (S,A)).\nBased on the first part of the proof, |(θ(i)− θ̂(i),h,(s,a))TΦ(i)(s, a)|≤ O ( (n(i) ln(nmin+h)) 1/2(ln(nS/δ)) 1/4\n(nmin+h) 1/4\n) with probability at least 1− δ.\nSince |(θ(i) − θ̂∗(i),h,(s,a))TΦ(i)(s, a)|≤ |(θ(i) − θ̂(i),h,(s,a))TΦ(i)(s, a)| (this directly follows the definition of θ̂∗(i),h,(s,a) and the choice of the distance function d), this implies that h∗( , δ) = O(L 4n2S n̄ 2 ln2m 4 ln nS δ ) is sufficient.\nCorollary 3. (Anytime error bound) With probability at least 1− δ, if h2 ln m 2nsh δ ≤ L 2nS n̄ ln 2m 3 ln nS δ ,\nt,h = O  5√L4n2Sn̄2 ln2 m t(1− γ) ln 3 nS δ  , and otherwise,\nt,h = O\n( h2L2nSn̄ ln 2 m\nt(1− γ) ln 2 nS δ\n) .\nProof. The proof follows directly from Theorem 2 and the proof of Corollary 1.\nAs in the discrete case, the anytime T -step average loss can be computed by summing the anytime errors as 1 T ∑T t=1(1 −\nγT+1−t) t,h,δ. In addition, we can derive the explicit exploration runtime. Corollary 6. (Explicit exploration runtime) With probability at least 1− δ, the explicit exploration runtime of Algorithm 2 is\nO\n( h2L2nSn̄ lnm\n2 Pr[Ak] ln2 nS δ ln m2nsh δ\n) = O ( h2L2nSn̄ lnm\n3(1− γ) ln 2 nS δ ln m2nsh δ\n) ,\nwhere AK is the escape event defined in the proof of Theorem 2.\nProof. The proof follows that of Theorem 2. Compared to the sample complexity of Algorithm 2, z is replaced by h based on the proof of Theorem 2.\nA7. Experimental Settings for Continuous Domain For each problem used in the main paper, we present more detailed descriptions of the experimental settings.\nMountain Car In the mountain car problem, the reward is negative everywhere except at the goal. To reach the goal, the agent must first travel far away, and must explore the world to learn this mechanism. To require a greater degree of exploration, we modify the original problem as follows: The agent obtains a reward equal to -0.9 around the initial position (position = [-0.6, 0.4]), and -1.0 everywhere else but at the goal. At the start of each episode, the agent is always at the bottom of the valley (position = -0.5) with zero velocity. Moreover, a small amount of Gaussian noise with standard deviation 0.001 is added to the velocity. Our model uses 10 grids of residual basis functions over the control signal and velocity as features. For the planning phase, we use a fitted value iteration with a 30×30 grid of residual basis functions. We set ∆(i) and the corresponding parameter in the PAC-MDP algorithm to be 0.14, because the velocity is bounded in [−0.07, 0.07]. Each episode consists of 2000 steps, and we conduct simulations for 100 episodes.\nSimulated HIV treatment This problem is described by a set of six ordinary differential equations (Ernst et al. 2006). An action corresponds to whether the agent administers two treatments (RTIs and PIs) to patients (thus, there are four actions). Two types of exploration are required: one to learn the effect of using treatments on viruses, and another to learn the effect of not using treatments on immune systems. Learning the former is necessary to reduce the population of viruses, but the latter is required to prevent the overuse of treatments, which weakens the immune system. We select the initial state to be unhealthy, following Ernst et al. (2006) and Pazis and Parr (2013). As in previous work, we assume that noise-free data can be obtained every five days. Unlike past studies, we assume that noisy data can be obtained a day after each instance of noise-free data is collected, with the noise term being ζ′ ∼ N (0, 0.1). We add another noise term to represent the model error with ζ ∼ N (0, 0.01) for each dynamic state. For the model, we use the six states and the multiple of any two of these six states as features (i.e., the number of features is 6 + ( 6 2 ) ). For planning, we use a greedy roll-out method, as described by Adams et al. (2004, Section 5). We set ∆(i) and the corresponding parameter in the PAC-MDP algorithm to be the average error among all the predictions and observations. Each episode consists of 1000 days, and we conduct simulations for 30 episodes.\nReferences [2005] Abbeel, P., and Ng, A. Y. 2005. Exploration and apprenticeship learning in reinforcement learning. In Proceedings of the\n22nd international conference on Machine learning (ICML). [2004] Adams, B.; Banks, H.; Kwon, H.-D.; and Tran, H. T. 2004. Dynamic multidrug therapies for HIV: Optimal and STI control approaches. Mathematical Biosciences and Engineering 1(2):223–241.\n[2012] Araya-López, M.; Thomas, V.; and Buffet, O. 2012. Near-optimal BRL using optimistic local transitions. In Proceedings of the 29th International Conference on Machine Learning (ICML).\n[2002] Auer, P. 2002. Using confidence bounds for exploitation-exploration trade-offs. The Journal of Machine Learning Research (JMLR) 3:397–422.\n[2010] Bernstein, A., and Shimkin, N. 2010. Adaptive-resolution reinforcement learning with polynomial exploration in deterministic domains. Machine learning 81(3):359–397.\n[2012] Brunskill, E. 2012. Bayes-optimal reinforcement learning for discrete uncertainty domains. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS).\n[1977] Cook, R. D. 1977. Detection of influential observation in linear regression. Technometrics 15–18. [2006] Ernst, D.; Stan, G.-B.; Goncalves, J.; and Wehenkel, L. 2006. Clinical data based optimal STI strategies for HIV: a reinforcement learning approach. In Proceedings of the 45th IEEE Conference on Decision and Control.\n[1994] Fiechter, C.-N. 1994. Efficient reinforcement learning. In Proceedings of the seventh annual ACM conference on Computational learning theory (COLT).\n[2010] Jaksch, T.; Ortner, R.; and Auer, P. 2010. Near-optimal regret bounds for reinforcement learning. The Journal of Machine Learning Research (JMLR) 11:1563–1600.\n[2013] Kawaguchi, K., and Araya, M. 2013. A greedy approximation of Bayesian reinforcement learning with probably optimistic transition model. In Proceedings of AAMAS 2013 workshop on adaptive learning agents, 53–60.\n[1999] Kearns, M., and Singh, S. 1999. Finite-sample convergence rates for Q-learning and indirect algorithms. In Proceedings of Advances in neural information processing systems (NIPS).\n[2002] Kearns, M., and Singh, S. 2002. Near-optimal reinforcement learning in polynomial time. Machine Learning 49(2-3):209– 232.\n[2009] Kolter, J. Z., and Ng, A. Y. 2009. Near-Bayesian exploration in polynomial time. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML).\n[2011] Li, L.; Littman, M. L.; Walsh, T. J.; and Strehl, A. L. 2011. Knows what it knows: a framework for self-aware learning. Machine learning 82(3):399–443.\n[2009] Li, L. 2009. A unifying framework for computational reinforcement learning theory. Ph.D. Dissertation, Rutgers, The State University of New Jersey.\n[2012] Li, L. 2012. Sample complexity bounds of exploration. In Reinforcement Learning. Springer. 175–204. [2013] Pazis, J., and Parr, R. 2013. PAC Optimal Exploration in Continuous Space Markov Decision Processes. In Proceedings\nof the 27th AAAI conference on Artificial Intelligence (AAAI). [2004] Puterman, M. L. 2004. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons. [1995] Russell, S. J., and Subramanian, D. 1995. Provably bounded-optimal agents. Journal of Artificial Intelligence Research\n(JAIR) 575–609. [1982] Simon, H. A. 1982. Models of bounded rationality, volumes 1 and 2. MIT press. [2010] Sorg, J.; Singh, S.; and Lewis, R. L. 2010. Variance-based rewards for approximate Bayesian reinforcement learning. In\nProceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI). [2008a] Strehl, A. L., and Littman, M. L. 2008a. An analysis of model-based interval estimation for Markov decision processes.\nJournal of Computer and System Sciences 74(8):1309–1331. [2008b] Strehl, A. L., and Littman, M. L. 2008b. Online linear regression and its application to model-based reinforcement learning. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 1417–1424.\n[2006] Strehl, A. L.; Li, L.; and Littman, M. L. 2006. Incremental model-based learners with formal learning-time guarantees. In Proceedings of the 22th Conference on Uncertainty in Artificial Intelligence (UAI).\n[2007] Strehl, A. L. 2007. Probably approximately correct (PAC) exploration in reinforcement learning. Ph.D. Dissertation, Rutgers University.\n[2000] Strens, M. 2000. A Bayesian framework for reinforcement learning. In Proceedings of the 16th International Conference on Machine Learning (ICML).\n[1998] Sutton, R. S., and Barto, A. G. 1998. Reinforcement learning: An introduction. MIT press Cambridge.\n[2003] Weissman, T.; Ordentlich, E.; Seroussi, G.; Verdu, S.; and Weinberger, M. J. 2003. Inequalities for the L1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep.\n[2008] Zilberstein, S. 2008. Metareasoning and bounded rationality. In Proceedings of the AAAI workshop on Metareasoning: Thinking about Thinking."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Within the framework of probably approximately correct Markov decision processes (PAC-MDP), much theoretical work has focused on methods to attain near optimality after a relatively long period of learning and exploration. However, practical concerns require the attainment of satisfactory behavior within a short period of time. In this paper, we relax the PAC-MDP conditions to reconcile theoretically driven exploration methods and practical needs. We propose simple algorithms for discrete and continuous state spaces, and illustrate the benefits of our proposed relaxation via theoretical analyses and numerical examples. Our algorithms also maintain anytime error bounds and average loss bounds. Our approach accommodates both Bayesian and nonBayesian methods.",
    "creator" : "LaTeX with hyperref package"
  }
}