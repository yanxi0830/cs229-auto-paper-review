{
  "name" : "1610.09420.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "lxu66@gatech.edu", "mdav@gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n09 42\n0v 1\n[ st\nat .M\nL ]\n2 8"
    }, {
      "heading" : "1 Introduction",
      "text" : "Suppose that X ∈ Rn1×n2 is a rank-r matrix with r much smaller than n1 and n2. We observe X through a linear operator A : Rn1×n2 → Rm,\ny = A(X), y ∈ Rm. In recent years there has been a significant amount of progress in our understanding of how to recover X from observations of this form even when the number of observations m is much less than the number of entries in X . (See [8] for an overview of this literature.) When A is a set of weighted linear combinations of the entries of X , this problem is often referred to as the matrix sensing problem. In the special case where A samples a subset of entries of X , it is known as the matrix completion problem. There are a number of ways to establish recovery guarantee in these settings. Perhaps the most popular approach for theoretical analysis in recent years has focused on the use of nuclear norm minimization as a convex surrogate for the (nonconvex) rank constraint [1, 3, 4, 5, 6, 7, 15, 19, 21, 22]. An alternative, however is to aim to directly solve the problem under an exact low-rank constraint. This leads a non-convex optimization problem, but has several computational advantages over most approaches to minimizing the nuclear norm and is widely used in large-scale applications (such as recommendation systems) [16]. In general, popular algorithms for solving the rank-constrained models – e.g., alternating minimization and alternating gradient descent – do not have as strong of convergence or recovery error guarantees due to the non-convexity of the rank constraint. However, there has been significant progress on this front in recent years [11, 10, 12, 13, 14, 23, 25], with many of these algorithms now having guarantees comparable to those for nuclear norm minimization.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nNearly all of this existing work assumes that the underlying low-rank matrix X remains fixed throughout the measurement process. In many practical applications, this is a tremendous limitation. For example, users’ preferences for various items may change (sometimes quite dramatically) over time. Modelling such drift of user’s preference has been proposed in the context of both music and movies as a way to achieve higher accuracy in recommendation systems [9, 17]. Another example in signal processing is dynamic non-negative matrix factorization for the blind signal separation problem [18]. In these and many other applications, explicitly modelling the dynamic structure in the data has led to superior empirical performance. However, our theoretical understanding of dynamic low-rank matrix recovery is still very limited.\nIn this paper we provide the first theoretical results on the dynamic low-rank matrix recovery problem. We determine the sense in which dynamic constraints can help to recover the underlying timevarying low-rank matrix in a particular dynamic model and quantify this impact through recovery error bounds. To describe our approach, we consider a simple example where we have two rank-r matrices X1 and X2. Suppose that we have a set of observations for each of X1 and X2, given by\nyi = Ai ( X i ) , i = 1, 2.\nThe naïve approach is to use y1 to recover X1 and y2 to recover X2 separately. In this case the number of observations required to guarantee successful recovery is roughlymi ≥ Cirmax(n1, n2) for i = 1, 2 respectively, where C1, C2 are fixed positive constants (see [4]). However, if we know that X2 is close to X1 in some sense (for example, if X2 is a small perturbation of X1), then the above approach is suboptimal both in terms of recovery accuracy and sample complexity, since in this setting y1 actually contains information about X2 (and similarly, y2 contains information about X1). There are a variety of possible approaches to incorporating this additional information. The approach we will take is inspired by the LOWESS (locally weighted scatterplot smoothing) approach from non-parametric regression. In the case of this simple example, if we look just at the problem of estimating X2, our approach reduces to solving a problem of the form\nmin X2\n‖A2(X2)− y2‖22 + λ‖A1(X2)− y1‖22 s.t. rank ( X2 ) ≤ r,\nwhere λ is a parameter that determines how strictly we are enforcing the dynamic constraint (if X1 is very close to X2 we can set λ to be larger, but if X1 is far from X2 we will set it to be comparatively small). This approach generalizes naturally to the locally weighted matrix smoothing (LOWEMS) program described in Section 2. Note that it has a (simple) convex objective function, but a nonconvex rank constraint. Our analysis in Section 3 shows that the proposed program outperforms the above naïve recovery strategy both in terms of recovery accuracy and sample complexity.\nWe should emphasize that the proposed LOWEMS program is non-convex due to the exact lowrank constraint. Inspired by previous work on matrix factorization, we propose using an efficient alternating minimization algorithm (described in more detail in Section 4). We explicitly enforce the low-rank constraint by optimizing over a rank-r factorization and alternately minimize with respect to one of the factors while holding the other one fixed. This approach is popular in practice since it is typically less computationally complex than nuclear norm minimization based algorithms. In addition, thanks to recent work on global convergence guarantees for alternating minimization for low-rank matrix recovery [10, 13, 25], one can reasonably expect similar convergence guarantees to hold for alternating minimization in the context of LOWEMS, although we leave the pursuit of such guarantees for future work.\nTo empirically verify our analysis, we perform both synthetic and real world experiments, described in Section 5. The synthetic experimental results demonstrate that LOWEMS outperforms the naïve approach in practice both in terms of recovery accuracy and sample complexity. We also demonstrate the effectiveness of LOWEMS in the context of recommendation systems.\nBefore proceeding, we briefly state some of the notation that we will use throughout. For a vector x ∈ Rn, we let ‖x‖p denote the standard ℓp norm. Given a matrixX ∈ Rn1×n2 , we use Xi: to denote the ith row of X and X:j to denote the j th column of X . We let ‖X‖F denote the the Frobenius norm, ‖X‖2 the operator norm, ‖X‖∗ the nuclear norm, and ‖X‖∞ = maxi,j |Xij | the elementwise infinity norm. Given a pair of matrices X,Y ∈ Rn1×n2 , we let 〈X,Y 〉 = ∑i,j XijYij = Tr ( Y TX )\ndenote the standard inner product. Finally, we let nmax and nmin denote max{n1, n2} and min{n1, n2} respectively."
    }, {
      "heading" : "2 Problem formulation",
      "text" : "The underlying assumption throughout this paper is that our low-rank matrix is changing over time during the measurement process. For simplicity we will model this through the following discrete dynamic process: at time t, we have a low-rank matrix Xt ∈ Rn1×n2 with rank r, which we assume is related to the matrix at previous time-steps via\nXt = f(X1, . . . , Xt−1) + ǫt,\nwhere ǫt represents noise. We assume that we observe each Xt through a linear operator At : Rn1×n2 → Rmt , yt = At(Xt) + zt, yt, zt ∈ Rmt , (1) where zt is measurement noise. In our problem we will suppose that we observe up to d time steps, and our goal is to recover {Xt}dt=1 jointly from {yt}dt=1. The above model is sufficiently flexible to incorporate a wide variety of dynamics, but we will make several simplifications. First, we note that we can impose the low-rank constraint explicitly by factorizing Xt as Xt = U t (V t)T , U t ∈ Rn1×r, V t ∈ Rn2×r. In general both U t and V t may be changing over time. However, in some applications, it is reasonable to assume that only one set of factors is changing. For example, in a recommendation system where our matrix represent user preferences, if the rows correspond to items and the columns correspond to users, then U t contains the latent properties of the items and V t models the latent preferences of the users. In this context it is reasonable to assume that only V t changes over time [9, 17], and that there is a fixed matrix U (which we may assume to be orthonormal) such that we can write Xt = UV t for all t. Similar arguments can be made in a variety of other applications, including personalized learning systems, blind signal separation, and more.\nSecond, we will assume a Markov property on f , so that Xt (or equivalently, V t) only depends on the previousXt−1 (orV t−1). Furthermore, although other dynamic models could be accommodated, for the sake of simplicity in our analysis we consider the simple model on V t where\nV t = V t−1 + ǫt, t = 2, . . . , d. (2)\nWe will also assume that both ǫt and the measurement noise zt are i.i.d. zero-mean Gaussian noise.\nTo simplify our discussion, we will assume that our goal is to recover the matrix at the most recent time-step, i.e., we wish to estimate Xd from {yt}dt=1. Our general approach can be stated as follows. Let C(r) = {X ∈ Rn1×n2 : rank(X) ≤ r}. The LOWEMS estimator is given by the following optimization program:\nX̂d = arg min X∈C(r) L (X) = arg min X∈C(r)\n1\n2\nd ∑\nt=1\nwt ∥ ∥At (X)− yt ∥ ∥ 2\n2 , (3)\nwhere {wt}dt=1 are non-negative weights, and we assume ∑d\nt=1 wt = 1 to avoid ambiguity. In the following section we provide bounds on the performance of the LOWEMS estimator for two common choices of operators At."
    }, {
      "heading" : "3 Recovery error bounds",
      "text" : "Given the estimator X̂d from (3), we define the recovery error to be ∆d := X̂d −Xd. Our goal in this section will be to provide bounds on ‖X̂d−Xd‖F under two common observation models. Our analysis builds on the following (deterministic) inequality.\nProposition 3.1. Both the estimator X̂d by (3) and (9) satisfies\nd ∑\nt=1\nwt ∥ ∥At ( ∆d )∥ ∥\n2 2 ≤ 2\n√ 2r\n∥ ∥ ∥ ∥ ∥ d ∑\nt=1\nwtAt∗ ( ht − zt )\n∥ ∥ ∥ ∥ ∥\n2\n∥ ∥∆d ∥ ∥\nF , (4)\nwhere ht = At ( Xd −Xt ) and At∗ is the adjoint operator of At.\nThis is a deterministic result that holds for any set of {At}. The remaining work is to lower bound the LHS of (4), and upper bound the RHS of (4) for concrete choices of {At}. In the following sections we derive such bounds in the settings of both Gaussian matrix sensing and matrix completion. For simplicity and without loss of generality, we will assume m1 = . . . = md =: m0, so that the total number of observations is simply m = dm0."
    }, {
      "heading" : "3.1 Matrix sensing setting",
      "text" : "For the matrix sensing problem, we will consider the case where all operators At correspond to Gaussian measurement ensembles, defined as follows.\nDefinition 3.2. [4] A linear operator A : Rn1×n2 → Rm is a Gaussian measurement ensemble if we can express each entry of A (X) as [A (X)]i = 〈Ai, X〉 for a matrix Ai whose entries are i.i.d. according to N (0, 1/m), and where the matrices A1, . . . , Am are independent from each other.\nAlso, we define the matrix restricted isometry property (RIP) for a linear map A. Definition 3.3. [4] For each integer r = 1, . . . , nmin, the isometry constant δr of A is the smallest quantity such that\n(1− δr) ‖X‖2F ≤ ‖A (X)‖ 2 2 ≤ (1 + δr) ‖X‖ 2 F\nholds for all matrices X of rank at most r.\nAn important result (that we use in the proof of Theorem 3.4) is that Gaussian measurement ensembles satisfy the matrix RIP with high probability provided m ≥ Crnmax. See, for example, [4] for details.\nTo obtain an error bound in the matrix sensing case we lower bound the LHS of (4) using the matrix RIP and upper bound the stochastic error (the RHS of (4)) using a covering argument. The following is our main result in the context of matrix setting.\nTheorem 3.4. Suppose that we are given measurements as in (1) where all At’s are Gaussian measurement ensembles. Assume that Xt evolves according to (2) and has rank r. Further assume that the measurement noise zt is i.i.d. N (\n0, σ21 ) for 1 ≤ t ≤ d and that the perturbation noise ǫt is i.i.d. N (\n0, σ22 ) for 2 ≤ t ≤ d. If\nm0 ≥ D1 max { nmaxr d ∑\nt=1\nw2t , nmax\n}\n, (5)\nwhere D1 is a fixed positive constant, then the estimator X̂d from (3) satisfies\n∥ ∥∆d ∥ ∥\n2 F ≤ C0\n(\nd ∑\nt=1\nw2t σ 2 1 +\nd−1 ∑\nt=1\n(d− t)w2t σ22\n)\nnmaxr (6)\nwith probability at least P1 = 1− dC1 exp (−c1n2), where C0, C1, c1 are positive constants.\nIf we choose the weights as wd = 1 and wt = 0 for 1 ≤ t ≤ d − 1, the bound in Theorem 3.4 reduces to a bound matching classical (static) matrix recovery results (see, for example, [4] Theorem 2.4). Also note that in this case Theorem 3.4 implies exact recovery when the sample complexity is O(rn/d). In order to help interpret this result for other choices of the weights, we note that for a given set of parameters, we can determine the optimal weights that will minimize this bound. Towards this end, we define κ := σ22/σ 2 1 and set pt = (d− t), 1 ≤ t ≤ d. Then one can calculate the optimal weights by solving the following quadratic program:\n{w∗t }dt=1 = arg min∑ t wt=1; wt≥0\nd ∑\nt=1\nw2t + d−1 ∑\nt=1\nptκw 2 t . (7)\nUsing the method of Lagrange multipliers one can show that (7) has the analytical solution:\nw∗j = 1\n∑d i=1 1 1+piκ\n1\n1 + pjκ , 1 ≤ j ≤ d. (8)\nA simple special case occurs when σ2 = 0. In this case all V t’s are the same, and the optimal weights go to wt = 1d for all t. In contrast, when σ2 grows large the weights eventually converge to wd = 1 andwt = 0 for all t 6= d. This results in essentially using only yd to recoverXd and ignoring the rest of the measurements. Combining these, we note that when the σ2 is small, we can gain by a factor of approximately d over the naïve strategy that ignores dynamics and tries to recover Xd using only yd. Notice also that the minimum sample complexity is proportional to ∑d\nt=1 w 2 t when\nr/d is relatively large. Thus, when σ2 is small, the required number of measurements can be reduced by a factor of d compared to what would be required to recover Xd using only yd."
    }, {
      "heading" : "3.2 Matrix completion setting",
      "text" : "For the matrix completion problem, we consider the following simple uniform sampling ensemble: Definition 3.5. A linear operator A : Rn1×n2 → Rm is a uniform sampling ensemble (with replacement) if all sensing matrices Ai are i.i.d. uniformly distributed on the set\nX = { ej (n1) e T k (n2) , 1 ≤ j ≤ n1, 1 ≤ k ≤ n2 ) ,\nwhere ej (n) are the canonical basis vectors in Rn. We let p = m0/(n1n2) denote the fraction of sampled entries.\nFor this observation architecture, our analysis is complicated by the fact that it does not satisfy the matrix RIP. (A quick problematic example is a rank-1 matrix with only one non-zero entry.) To handle this we follow the typical approach and restrict our focus to matrices that satisfy certain incoherence properties. Definition 3.6. (Subspace incoherence [10]) Let U ∈ Rn×r be the orthonormal basis for an rdimensional subspace U , then the incoherence of U is defined as µ(U) := maxi∈[n] √ n√ r ∥ ∥eTi U ∥ ∥ 2 , where ei denotes the ith standard basis vector. We also simply denote µ(span(U)) as µ(U). Definition 3.7. (Matrix incoherence [13]) A rank-r matrix X ∈ Rn1×n2 with SVD X = UΣV T is incoherent with parameter µ if\n‖U:i‖2 ≤ µ √ r√\nn1 for any i ∈ [n1] and ‖V:j‖2 ≤\nµ √ r√\nn2 for any j ∈ [n2],\ni.e., the subspaces spanned by the columns of U and V are both µ-incoherent.\nThe incoherence assumption guarantees that X is far from sparse, which make it possible to recover X from incomplete measurements since a measurement contains roughly the same amount of information for all dimensions.\nTo proceed we also assume that the matrix Xd has “bounded spikiness” in that the maximum entry of Xd is bounded by a, i.e., ∥ ∥Xd ∥ ∥\n∞ ≤ a. To exploit the spikiness constraint below we replace the optimization constraints C (r) in (3) with C (r, a) :== {X ∈ Rn1×n2 : rank (X) ≤ r, ‖X‖∞ ≤ a}:\nX̂d = arg min X∈C(r,a) L (X) = arg min X∈C(r,a)\n1\n2\nd ∑\nt=1\nwt ∥ ∥At (X)− yt ∥ ∥ 2\n2 . (9)\nNote that Proposition 3.1 still holds for (9).\nTo obtain an error bound in the matrix completion case, we lower bound the LHS of 4 using a restricted convexity argument (see, for example, [20]) and upper bound the RHS using matrix Bernstein inequality. The result of this approach is the following theorem. Theorem 3.8. Suppose that we are given measurements as in (1) where all At’s are uniform sampling ensembles. Assume that Xt evolves according to (2), has rank r, and is incoherent with parameter µ0 and ∥ ∥Xd ∥ ∥\n∞ ≤ a. Further assume that the perturbation noise and the measurement noise satisfy the same assumptions in Theorem 3.4. If m0 ≥ D2nmin log2(n1 + n2)φ′(w), (10) where φ′(w) = maxt w 2 t ((d−t)µ20rσ22/n1+σ21) ∑\nd t=1 w 2 t ((d−t)σ22+σ21) , then the estimator X̂d from (9) satisfies\n∥ ∥∆d ∥ ∥\n2 F ≤ max\n\n\n\nB1 := C2a 2n1n2\n√\n∑d t=1 w 2 t log(n1 + n2)\nm0 , B2\n\n\n\n, (11)\nwith probability at least P1 = 1− 5/(n1 + n2)− 5dnmax exp(−nmin), where\nB2 = C3rn\n2 1n 2 2 log(n1 + n2)\nnminm0\n((\nd ∑\nt=1\nw2t σ 2 1 +\nd−1 ∑\nt=1\n(d− t)w2t σ22\n)\n+\nd ∑\nt=1\nw2t a 2\n)\n, (12)\nand C2, C3, D2 are absolute positive constants.\nIf we choose the weights as wd = 1 and wt = 0 for 1 ≤ t ≤ d − 1, the bound in Theorem 3.8 reduces to a result comparable to classical (static) matrix completion results (see, for example, [15] Theorem 7). Moreover, from the B2 term in (11), we obtain the same dependence on m as that of (6), i.e., 1/m. However, there are also a few key differences between Theorem 3.4 and our results for matrix completion. In general the bound is loose in several aspects compared to the matrix sensing bound. For example, when m0 is small, B1 actually dominates, in which case the dependence on m is actually 1/ √ m instead of 1/m. When m0 is sufficiently large, then B2 dominates, in which case we can consider two cases. The first case corresponds to when a is relatively large compared to σ1, σ2 – i.e., the low-rank matrix is spiky. In this case the term containing a2 in B2 dominates, and the optimal weights are equal weights of 1/d. This occurs because the term involving a dominates and there is little improvement to be obtained by exploiting temporal dynamics. In the second case, when a is relatively small compared to σ1, σ2 (which is usually the case in practice), the bound can be simplified to\n‖∆‖2F ≤ c3rn\n2 1n 2 2 log(n1 + n2)\nnminm0\n((\nd ∑\nt=1\nw2t σ 2 1 +\nd−1 ∑\nt=1\n(d− t)w2t σ22\n))\n.\nThe above bound is much more similar to the bound in (6) from Theorem 3.4. In fact, we can also obtain the optimal weights by solving the same quadratic program as (7).\nWhen n1 ≈ n2, the sample complexity is Θ(nmin log2(n1 + n2)φ′(w)). In this case Theorem 3.8 also implies a similar sample complexity reduction as we observed in the matrix sensing setting. However, the precise relations between sample complexity and weights wt’s are different in these two cases (deriving from the fact that the proof uses matrix Bernstein inequalities in the matrix completion setting rather than concentration inequalities of Chi-squared variables as in the matrix sensing setting)."
    }, {
      "heading" : "4 An algorithm based on alternating minimization",
      "text" : "As noted in Section 2, any rank-r matrix can be factorized as X = UV T where U is n1 × r and V is n2 × r, therefore the LOWEMS estimator in (3) can be reformulated as\nX̂d = arg min X∈C(r) L (X) = arg min X=UV T\nd ∑\nt=1\n1 2 wt ∥ ∥At ( UV T ) − yt ∥ ∥ 2 2 . (13)\nThe above program can be solved by alternating minimization (see [17]), which alternatively minimizes the objective function over U (or V ) while holding V (or U ) fixed until a stopping criterion is reached. Since the objective function is quadratic, each step in this procedure reduces to conventional weighted least squares, which can be solved via efficient numerical procedures. Theoretical guarantees for global convergence of alternating minimization for the static matrix sensing/completion problem have recently been established in [10, 13, 25] by treating the alternating minimization as a noisy version of the power method. Extending these results to establish convergence guarantees for (13) would involve analyzing a weighted power method. We leave this analysis for future work, but expect that similar convergence guarantees should be possible in this setting."
    }, {
      "heading" : "5 Simulations and experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Synthetic simulations",
      "text" : "Our synthetic simulations consider both matrix sensing and matrix completion, but with an emphasis on matrix completion. We set n1 = 100, n2 = 50, d = 4 and r = 5. We consider two baselines:\nbaseline one is only using yd to recover Xd and simply ignoring y1, . . . yd−1; baseline two is using {yt}dt=1 with equal weights. Note that both of these can be viewed as special cases of LOWEMS with weights (0, . . . , 0, 1) and ( 1d , 1 d , . . . , 1 d) respectively. Recalling the formula for the optimal choice of weights in (8), it is easy to show that baseline one is equivalent to the case where κ = (σ22)/(σ 2 1) → ∞ and the baseline two equivalent to the case where κ → 0. This also makes intuitive sense since κ → ∞ means the perturbation is arbitrarily large between time steps, while κ → 0 reduces to the static setting.\n1). Recovery error. In this simulation, we set m0 = 4000 and set the measurement noise level σ1 to 0.05. We vary the perturbation noise level σ2. For every pair of (σ1, σ2) we perform 10 trials, and show the average relative recovery error ∥ ∥∆d ∥ ∥ 2\nF / ∥ ∥Xd ∥ ∥ 2 F . Figure 1 illustrates how LOWEMS\nreduces the recovery error compared to our baselines. As one can see, when σ2 is small, the optimal κ, i.e., σ22/σ 2 1 , generates nearly equal weights (baseline two), reducing recovery error approximately by a factor of 4 over baseline one, which is roughly equal to d as expected. As σ2 grows, the recovery error of baseline two will increase dramatically due to the perturbation noise. However in this case the optimal κ of LOWEMS grows with it, leading to a more uneven weighting and to somewhat diminished performance gains. We also note that, as expected, LOWEMS converges to baseline one when σ2 is large.\n2). Sample complexity. In the interest of conciseness we only provide results here for the matrix completion setting (matrix sensing yields broadly similar results). In this simulation we vary the fraction of observed entries p to empirically find the minimum sample complexity required to guarantee successful recovery (defined as a relative error ≤ 0.04). We compare the sample complexity of the proposed LOWEMS to baseline one and baseline two under different perturbation noise level σ2. For fixed σ2, the relative recovery error is the averaged over 10 trials. Figure 2 illustrates how LOWEMS reduces the sample complexity required to guarantee successful recovery. When the perturbation noise is weaker than the measurement noise, the sample complexity can be reduced approximately by a factor of d compared to baseline one. When the perturbation noise is much stronger than measurement noise, the recovery error of baseline two will increase due to the perturbation noise and hence the sample complexity increase rapidly. However in this case proposed LOWEMS still achieves relatively small sample complexity and its sample complexity converges to baseline one when σ2 is relatively large."
    }, {
      "heading" : "5.2 Real world experiments",
      "text" : "We next test the LOWEMS approach in the context of a recommendation system using the (truncated) Netflix dataset. We eliminate those movies with few ratings, and those users rating few movies, and generate a truncated dataset with 3199 users, 1042 movies, 2462840 ratings, and hence the fraction of visible entries in the rating matrix is ≈ 0.74. All the ratings are distributed over a period of 2191 days. For the sake of robustness, we additionally impose a Frobenius norm penalty on the factor matrices U and V in (13). We keep the latest (in time) 10% of the ratings as a testing set. The remaining ratings are split into a validation set and a training set for the purpose of cross valida-\ntion. We divide the remaining ratings into d ∈ {1, 3, 6, 8} bins respectively with same time period according to their timestamps. We use 5-fold cross validation, and we keep 1/5 of the ratings from the dth bin as a validation set. The number of latent factors r is set to 10. The Frobenius norm regularization parameter γ is set to 1. We also note that in practice one likely has no prior information on σ1, σ2 and hence κ. However, we use model selection techniques like cross validation to select the best κ incorporating the unknown prior information on measurement/perturbation noise. We use root mean squared error (RMSE) to measure prediction accuracy. Since alternating minimization uses a random initialization, we generate 10 test RMSE’s (using a boxplot) for the same testing set. Figure 3(a) shows that the proposed LOWEMS estimator improves the testing RMSE significantly with appropriate κ. Additionally, the performance improvement increases as d gets larger.\nTo further investigate how the parameter κ affects accuracy, we also show the validation RMSE compared to κ in Figure 3(b). When κ ≈ 1, LOWEMS achieves the best RMSE on the validation data. This further demonstrates that imposing an appropriate dynamic constraint should improve recovery accuracy in practice."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we consider the low-rank matrix recovery problem in a novel setting, where one of the factor matrices changes over time. We propose the locally weighted matrix smoothing (LOWEMS) framework, and have established error bounds for LOWEMS in both the matrix sensing and matrix completion cases. Our analysis quantifies how the proposed estimator improves recovery accuracy and reduces sample complexity compared to static recovery methods. Finally, we provide both synthetic and real world experimental results to verify our analysis and demonstrate superior empirical performance when exploiting dynamic constraints in a recommendation system."
    }, {
      "heading" : "A Proof of Proposition 3.1",
      "text" : "Proof. Let x := vec (X) ∈ Rn1n2 and L̃ (x) := L (X). Since the objective function is continuous in X and the set C (r) is compact, L (X) achieves a minimizer at some point X̂d ∈ C (r). Since X̂d is a minimizer of the constrained problem, then for all matrices X ∈ C (r) we have the following inequality\nL̃ ( x̂d ) − L̃ (x) ≤ 0. (14) By the second-order Taylor’s theorem, we expand L̃ (x) around xd = vec ( Xd )\nL̃ (x) = L̃ ( xd ) + 〈 ∇L̃ ( xd ) , x− xd 〉 + 1\n2\n〈\n∇2L̃ (x̄) ( x− xd ) , x− xd 〉 , (15)\nwhere x̄ = αxd + (1− α) x for some α ∈ [0, 1]. Plugging (15) with x = x̂d into (14) we obtain 〈\n∇L̃ ( xd ) , x̂d − xd 〉 + 1\n2\n〈\n∇2L̃ (x̄) ( x̂d − xd ) , x̂d − xd 〉 ≤ 0. (16)\nThrough some algebraic manipulation we have the following expression for the gradient of L̃ (x):\n∇L̃ (x) = vec ( d ∑\nt=1\nwtAt∗ [ At (X)− yt ]\n)\n. (17)\nBased on the above gradient it follows that\n∇2L̃ (x) b = vec ( d ∑\nt=1\nwtAt∗ [ At (B) ]\n)\n, (18)\nwhere b = vec (B).\nNow based on (17) and (18), the absolute value of first term in (16) can be bounded as\n∣ ∣ ∣〈∇L̃ ( xd ) , x̂d − xd〉 ∣ ∣ ∣ =\n∣ ∣ ∣ ∣ ∣ 〈 d ∑\nt=1\nwtAt∗ [ At ( Xd ) − yt ] ,∆d\n〉∣\n∣ ∣ ∣ ∣\n≤ ∥ ∥ ∥\n∥ ∥\nd ∑\nt=1\nwtAt∗ [ At ( Xd ) − yt ]\n∥ ∥ ∥ ∥ ∥\n2\n∥ ∥∆d ∥ ∥\n∗\n≤ ∥ ∥ ∥\n∥ ∥\nd ∑\nt=1\nwtAt∗ ( ht − zt )\n∥ ∥ ∥ ∥ ∥\n2\n√ 2r ∥ ∥∆d ∥ ∥\nF\n(19)\nThe first inequality above used the trace dual norm inequality, while the second inequality follows from a basic inequality for rank-2r matrices. Similarly the second term in (16) is\n1\n2\n〈\n∇2L̃ (x̄) ( x̂d − xd ) , x̂d − xd 〉 = 1\n2\n〈\nd ∑\nt=1\nwtAt∗At ( ∆d ) ,∆d\n〉\n= 1\n2\nd ∑\nt=1\nwt 〈 At ( ∆d ) ,At ( ∆d )〉 .\n(20)\nThe result follows from combining (19) and (20). Note that the above proof holds if we replace C (r, ) with C (r, a), which completes our proof."
    }, {
      "heading" : "B Proof of Theorem 3.4",
      "text" : "Proof. The proof consists of lower bounding the LHS of (4) and upper bounding the RHS of (4).\nWe use the following lemma to lower bound ∑d t=1 wt ∥ ∥At ( ∆d )∥ ∥ 2 2 .\nLemma B.1. Suppose the linear operator At : Rn1×n2 → Rm0 is random Gaussian ensemble for all 1 ≤ t ≤ d. If m0 > Dnmaxr ∑d t=1 w 2 t , the composite operator {√ wtAt }d t=1 satisfies the rank-2r matrix RIP with constant δ2r ≤ δ with probability exceeding 1 − C exp (−cm0), where D,C and c (which depends on σ) are absolute positive constants.\nProof. See Appendix C.\nNext lemma gives us an upper bound for the stochastic error ∥ ∥\n∥ ∑d t=1 wtAt∗ (ht − zt)\n∥ ∥ ∥\n2 .\nLemma B.2. Under the assumptions of Theorem 3.4, when m0 ≥ Dnmax, we have ∥\n∥ ∥ ∥ ∥\nd ∑\nt=1\nwtAt∗ ( ht − zt )\n∥ ∥ ∥ ∥ ∥\n2\n≤ C1\n√ √ √ √nmax(1 + δ1) ( d ∑\nt=1\nw2t σ 2 1 +\nd−1 ∑\nt=1\n(d− t)w2t 2rn2 m0 σ22\n)\nwith probability exceeding 1 − dC exp(−cn2), where D,C1, C, c are positive constants and δ1 is the rank-1 matrix RIP parameter for all At’s.\nProof. See Appendix D.\nTheorem 3.4 follows by combining Lemma B.1, Lemma B.2 and Definition 3.3."
    }, {
      "heading" : "C Proof of Lemma B.1",
      "text" : "Proof. First we introduce the following theorem providing a double-sided tail bound on the sum of independent sub-exponential random variables.\nTheorem C.1. For independent Xi sub-exponential with parameters (σi, bi), with mean µi,\nP\n(∣\n∣ ∣ ∣ ∣\nn ∑\ni=1\n(Xi − µi) ∣ ∣ ∣ ∣\n∣\n≥ nt ) ≤ 2 exp ( − nt 2\n2 (σ2 + bt)\n)\n,\nwhere σ2 = ∑ i σ 2 i and b = maxi bi.\nWe now lower bound ∑d t=1 wt ∥ ∥At ( ∆d )∥ ∥ 2 2 . Since all At’s are Gaussian random measurement ensembles, then a particular measurement 〈\nAti,∆ d 〉2 is distributed as m−10 ∥ ∥∆d ∥ ∥ 2 F χ2 (1). Therefore\n∑d t=1 wt\n∥ ∥At ( ∆d )∥ ∥ 2\n2 =\n∑ t,i wt 〈 Ati, ( ∆d )〉2 is a weighted sum of i.i.d. χ2 (1) random variables. Since χ2 (1) is sub-exponential with parameters (4, 4), Theorem C.1 implies a double-sided tail bound for ∑d\nt=1 wt ∥ ∥At ( ∆d )∥ ∥ 2 2 : for any given ∆d ∈ Rn1×n2 and any fixed 0 < s < 1\nP\n(∣\n∣ ∣ ∣ ∣\nd ∑\nt=1\nwt ∥ ∥At ( ∆d )∥ ∥\n2 2 − ∥ ∥∆d ∥ ∥ 2 F\n∣ ∣ ∣ ∣ ∣ ≤ s ∥ ∥∆d ∥ ∥ 2 F ) ≤ 2 exp ( − m0s 2\n8 ∑d t=1 w 2 t + 8wmaxs\n)\n,\nwhere wmax = max{w1, . . . , wd}. The probability can be further simplified if s is very small (≤ 1/d). Rank of ∆d is at most 2r since X̂d, Xd are rank-r matrices. By Theorem 2.3 in [4] (one may see the proof if necessary) if m0 > Dnmaxr ∑d t=1 w 2 t , the composite operator {√ wtAt }d t=1 satisfies the rank-2r matrix RIP with constant δ2r ≤ δ with probability exceeding 1−C exp (−cm0), where C and c (depends on δ) are absolute positive constants."
    }, {
      "heading" : "D Proof of Lemma B.2",
      "text" : "Proof. Let W = ∑d t=1 wtAt∗ (ht − zt) and n = nmax for short. Following the basic framework of the proof of Lemma 1.1 in [4], we use ǫ-nets method to bound the stochastic error ‖W‖2. The operator norm of W is\n‖W‖2 = sup ‖u‖=‖v‖=1 〈u,Wv〉 ,\nConsider a 1/4-net N1/4 of the unite sphere Sn−1 with ∣ ∣N1/4 ∣ ∣ ≤ 12n (see (III.1) in [4]). For any v, u ∈ Sn−1\n〈u,Wv〉 = 〈u− u0,Wv〉+ 〈u0,W (v − v0)〉+ 〈u0,Wv0〉 ≤ ‖W‖2 ‖u− u0‖2 + ‖W‖2 ‖v − v0‖2 + 〈u0,Wv0〉 ,\nfor some v0, w0 ∈ N1/4 obeying ‖u− u0‖2 ≤ 1/4 and ‖v − v0‖ ≤ 1/4. So the operator norm of W is\n‖W‖2 ≤ 2 sup u0,v0∈N1/4 〈u0,Wv0〉 .\nFor fixed u0, v0\n〈u0,Wv0〉 = Tr ( uT0 Wv0 ) = Tr ( v0u T 0 W ) = 〈 u0v T 0 ,W 〉 =\nd ∑\nt=1\nwt 〈 At ( u0v T 0 ) , ht − zt 〉 .\nLet Z = ∑d t=1 wt 〈 At ( u0v T 0 ) , zt 〉 and H = ∑d t=1 wt 〈 At ( u0v T 0 ) , ht 〉 . Since for all 1 ≤ t ≤ d, entries of zt are i.i.d. N (\n0, σ21 ) , therefore Z ∼ N ( 0, σ2Z ) , where the variance σ2Z is\nσ2Z =\nd ∑\nt=1\nw2t ∥ ∥At ( u0v T 0 )∥ ∥\n2 2 σ21 ≤\nd ∑\nt=1\nw2t (1 + δ1) ∥ ∥u0v T 0 ∥ ∥\n2 F σ21 =\nd ∑\nt=1\nw2t (1 + δ1)σ 2 1 . (21)\nThe first inequality uses the matrix RIP for rank-1 matrices. For a fixed t, At satisfies the rank-1 matrix RIP with constant δ1, with probability at least 1−C2 exp(−c2m0) provided that m0 ≥ D2n by Theorem 2.3 in [4], where C2, c2 and D2 are fixed positive constants. Then by a union bound, for all 1 ≤ t ≤ d, At satisfies the rank-1 matrix RIP property with parameter σ1, with probability at least 1− dC2 exp(−c2m0) provided that m0 ≥ D2n. We now simplify H as\nH = d ∑\nt=1\nwt 〈 At ( u0v T 0 ) , ht 〉\n= d−1 ∑\nt=1\nwt\n〈\nAt ( u0v T 0 )\n, d ∑\ns=t+1\nAt [ U (ǫs)T ]\n〉\n=\nd ∑\ns=2\ns−1 ∑\nt=1\n〈\nwtAt ( u0v T 0 )\n,At [ U (ǫs) T ]〉\n=\nd ∑\ns=2\ns−1 ∑\nt=1\n〈\nwtAt∗At ( u0v T 0 ) , U (ǫs) T 〉\n=\nd ∑\ns=2\ns−1 ∑\nt=1\nm0 ∑\ni=1\n〈\nwt [ At ( u0v T 0 )] i Ati, U (ǫ\ns) T 〉\n=\nd ∑\ns=2\n〈 s−1 ∑\nt=1\nwt ∥ ∥At ( u0v T 0 )∥ ∥ 2 UTAt, (ǫs) T\n〉\n,\nwhere At ∈ Rn1×n2 contains i.i.d. N (0, 1/m0) entries. The last equality uses the property that sum of independent Gaussian variables is also Gaussian, and the variance is the sum of individual variances. Since for all 2 ≤ s ≤ d, entries of ǫs are i.i.d. N (\n0, σ22 ) , therefore H ∼ N ( 0, σ2H ) ,\nwhere the variance σ2H is\nσ2H = d ∑\ns=2\n∥ ∥ ∥ ∥ ∥ s−1 ∑\nt=1\nwt ∥ ∥At ( u0v T 0 )∥ ∥ 2 UTAt\n∥ ∥ ∥ ∥ ∥\n2\nF\nσ22 (ξ1) ≤ d ∑\ns=2\n∥ ∥ ∥ ∥ ∥ s−1 ∑\nt=1\nwt √ 1 + δ1U TAt\n∥ ∥ ∥ ∥ ∥\n2\nF\nσ22\n(ξ2) =\nd ∑\ns=2\ns−1 ∑\nt=1\nw2t (1 + δ1) ∥ ∥UTBs ∥ ∥\n2 F σ22\n= d ∑\ns=2\ns−1 ∑\nt=1\nw2t (1 + δ1) 1\nm0 χ2s (rn2)σ 2 2\n(ξ3) ≤ d ∑\ns=2\ns−1 ∑\nt=1\nw2t (1 + δ1) 1\nm0 3m0σ\n2 2\n=\nd−1 ∑\nt=1\n(d− t)w2t (1 + δ1)σ22 .\n(22)\nInequality (ξ1) holds with probability exceeding 1 − dC2 exp(−c2m0) provided that m0 ≥ Dn based on the matrix RIP for rank-1 matrices as used while bounding σ2Z . Equality (ξ2) uses the property that sum of independent Gaussian variables is also Gaussian and entries of Bs are i.i.d. N (0, 1/m0). Inequality (ξ3) holds with probability at least 1− dC3 exp(−c3m0) by the concentration property of correlated Chi-squared variables.\nSince the measurement noise Z and dynamic perturbation H are independent, then 〈u0,Wv0〉 ∼ N (\n0, σ2Z + σ 2 H\n)\n. Then by a standard tail bound for Gaussian random variables we have\nP (|〈u0,Wv0〉| > λ) ≤ 2 exp ( − λ 2\n2 (σ2H + σ 2 Z)\n)\n.\nTherefore by an standard union bound we bound the stochastic error\nP\n(\n‖W‖2 ≥ C0 √ n (σ2H + σ 2 Z)\n)\n≤ 2 ∣ ∣N1/4 ∣ ∣ 2 exp\n(\n−C 2 0n\n8\n)\n≤ 2 exp (−cn) , (23)\nwhere c = C 2 0 8 − 2 log 12. To ensure c > 0, we require C0 > 4 √ log 12. Combining (21), (22), and (23), if m0 ≥ Dn we have\n‖W‖2 ≤ C0\n√ √ √ √n ( (1 + δ1) d ∑\nt=1\nw2t\n(\nσ21 + (d− t) 5rn2 m0 σ22\n)\n)\nwith probability exceeding 1 − [dC2 exp(−c2m0) + dC3 exp(−c3m0) + 2 exp(−cn)] ≥ 1 − dC exp(−cn2)."
    }, {
      "heading" : "E Proof of Theorem 3.8",
      "text" : "Proof. The proof follows the same framework of the proof of Theorem 7 in [15].\nBefore we lower bound ∑d t=1 wt ∥ ∥At ( ∆d )∥ ∥ 2 2 , we consider the following constraint set for a given 0 < r ≤ n:\nE (r) =\n\n\n\nX ∈ C(r) : ‖X‖∞ = 1, ‖X‖ 2 F ≥ n1n2\n√\n64 ∑d t=1 w 2 t log(n1 + n2)\nlog(6/5)m0\n\n\n\n.\nDefine the following random matrix\nΣR =\nd ∑\nt=1\nm0 ∑\ni=1\nwtγ t iA t i,\nwhere γti is Rademacher variable. The following lemma bounds the restricted strong convexity (see [20]) of the operator {√ wtAt }d\nt=1 .\nLemma E.1. Suppose all At’s are fixed uniform sampling ensembles. For all X ∈ E (r) d\n∑\nt=1\nwt ∥ ∥At (X) ∥ ∥ 2 2 ≥ p 2 ‖X‖2F − 44rn1n2 m0 (E(‖ΣR‖))2 (24)\nwith probability at least 1− 2(n1+n2) .\nProof. See Appendix F.\nNote that ∥ ∥∆d ∥ ∥ ∞ ≤ ∥ ∥ ∥X̂d ∥ ∥ ∥\n∞ +\n∥ ∥Xd ∥ ∥ ∞ ≤ 2 ∥ ∥Xd ∥ ∥\n∞. To proceed, we consider the following two cases.\nCase I. ∆ d\n2‖Xd‖ ∞\n/∈ E(2r).\nFollowing the definition of E(2r) we have\n∥ ∥∆d ∥ ∥\n2 F ≤ c2 ∥ ∥Xd ∥ ∥ 2 ∞ n1n2\n√\n∑d t=1 w 2 t log(n1 + n2)\nm0 ,\nwhere C2 = 4 √ 64 log(6/5) . This yields the first part of inequality (11) in Theorem 3.8.\nCase II. ∆ d\n2‖Xd‖ ∞\n∈ E(2r).\nSince ∆ d\n2‖Xd‖ ∞ ∈ E(2r), applying Lemma E.1 yields d\n∑\nt=1\nwt ∥ ∥At ( ∆d )∥ ∥\n2 2 ≥ p\n2\n∥ ∥∆d ∥ ∥\n2 F − 362rn1n2\nm0 (E(‖ΣR‖))2\n∥ ∥Xd ∥ ∥\n2 ∞ . (25)\nCombining (25) and (4) yields\np\n2\n∥ ∥∆d ∥ ∥\n2 F ≤ 2\n√ 2r\n∥ ∥ ∥ ∥ ∥ d ∑\nt=1\nwtAt∗ ( ht − zt )\n∥ ∥ ∥ ∥ ∥\n2\n∥ ∥∆d ∥ ∥\nF + 362rn1n2 m0 (E(‖ΣR‖))2 ∥ ∥Xd ∥ ∥ 2 ∞\n≤ 8r p\n∥ ∥ ∥ ∥ ∥ d ∑\nt=1\nwtAt∗ ( ht − zt )\n∥ ∥ ∥ ∥ ∥\n2\n2\n+ p\n4\n∥ ∥∆d ∥ ∥\n2 F + 362rn1n2 m0 (E(‖ΣR‖))2 ∥ ∥Xd ∥ ∥ 2 ∞ .\nThe above inequality can be further simplified as\n∥ ∥∆d ∥ ∥\n2 F ≤ 32rn\n2 1n 2 2\nm20\n∥ ∥ ∥ ∥ ∥ d ∑\nt=1\nwtAt∗ ( ht − zt )\n∥ ∥ ∥ ∥ ∥\n2\n2\n+ 1448rn21n 2 2\nm20 (E(‖ΣR‖))2\n∥ ∥Xd ∥ ∥\n2 ∞ . (26)\nNext we bound E(‖ΣR‖) in the following lemma. Lemma E.2. Suppose all At’s are fixed uniform sampling ensembles. For m0 ≥ Dnmin log (n1 + n2)φ(w), where φ(w) = w2 max∑d\nt=1 w 2 t\n, there exists an absolute positive constant C\nsuch that\nE(‖ΣR‖) ≤ C\n√\n2e log (n1 + n2) ∑d t=1 w 2 tm0\nnmin . (27)\nThe proof is not provided since it is almost the same as that of Lemma 6 in [15] with some minor modifications. Note that our results are a bit stronger compared to Lemma 6 in [15], since we are dealing with bounded variables.\nNow we upper bound the stochastic error ‖J‖22 := ∥ ∥ ∥ ∑d t=1 wtAt∗ (ht − zt) ∥ ∥ ∥ 2\n2 . First, we rewrite J\nas\nJ =\nd ∑\nt=1\nwtAt∗At  U ( d ∑\ns=t+1\nǫs\n)T\n+ Zt\n\n ,\nwhere each entry of the random matrix Zt ∈ Rn1×n2 is i.i.d. Gaussian distributed with variance σ21 . Set Y t = U ( ∑d s=t+1 ǫ s )T and F t = Y t + Zt. Note that F t may be correlated for different 1 ≤ t ≤ d, though for a given t the entries of F t are independent. We now introduce an n1 × n2 random matrix Gt that has exactly one non-zero entry:\nGt = wtn1n2F t ijEij , with probability\n1\nn1n2 ,\nwhereEij is the canonical basis of matrices with dimensionn1×n2. We also introduce the following random matrix Ht, which is the average of m0 independent copies of Gt:\nHt = 1\nm0\nm0 ∑\ni=1\nGti where each G t i is an independent copy of G t.\nThen J can be decomposed as sum of independent random matrices: J = m0n1n2 ∑d t=1 H t. It is immediate that\nEGt = EHt = wtF t, EJ = m0 n1n2\nd ∑\nt=1\nwtF t.\nBefore we proceed we introduce a lemma describing the spectral norm deviation of a sum of uncentered random matrices from its mean value.\nLemma E.3. (Corollary 6.1.2 in [24]) Consider a finite sequence {Sk} of independent random matrices with common dimension n1×n2. Assume that each matrix has uniformly bounded deviation from its mean: ‖Sk − ESk‖ ≤ L for each index k. Consider the sum\nZ = ∑\nk\nSk.\nLet ρ(Z) denotes the matrix variance statistic of the sum:\nρ(Z) = max {∥ ∥E[(Z − EZ)(Z − EZ)T ] ∥ ∥ , ∥ ∥E[(Z − EZ)T (Z − EZ)] ∥ ∥ }\n= max\n{∥\n∥ ∥ ∥ ∥ ∑\nk\nE[(Sk − ESk)(Sk − ESk)T ] ∥ ∥ ∥ ∥\n∥\n,\n∥ ∥ ∥ ∥ ∥ ∑\nk\nE[(Sk − ESk)T (Sk − ESk)] ∥ ∥ ∥ ∥\n∥\n}\n.\nThen for all s ≥ 0,\nP (‖Z − EZ‖ ≥ s) ≤ (n1 + n2) exp ( −s2/2 ρ(Z) + Ls/3 ) .\nWe are going to apply the above uncentered Bernstein inequality to the sum of dm0 independent random matrices\n∑d t=1 H t = 1m0 ∑d t=1 ∑m0 k=1 G t k. Before doing so, we note that for given t and k,\n∥ ∥Gtk − EGtk ∥ ∥ ≤ ∥ ∥Gtk ∥ ∥+ ∥ ∥EGtk ∥ ∥ ≤ ∥ ∥Gtk ∥ ∥+ E ∥ ∥Gtk ∥ ∥ ≤ 2 ∥ ∥Gtk ∥ ∥ .\nThe first inequality uses the triangle inequality; the second is Jensen’s inequality.\nTo control ρ( ∑d t=1 H t), first note that\n0 ∑\nt\n∑\nk\nE [ Gtk − EGtk)(Gtk − EGtk)T ] = ∑\nt\n∑\nk\nE [ (Gtk(G t k) T ] − (EGtk)(EGtk)T\n∑\nt\n∑\nk\nE [ Gtk(G t k) T ]\n= m0 ∑\nt\nE [ Gt(Gt)T ] .\nThe third relation holds because (EGtk)(EG t k) T is positive semidefinite; the last relation uses the fact that for a fixed t, Gtk are random matrices following identical distributions independently for all 1 ≤ k ≤ m0. Now we can control ρ( ∑d t=1 H t) in the following\nρ\n(\nd ∑\nt=1\nHt\n)\n≤ 1 m0 max\n{∥\n∥ ∥ ∥ ∥ ∑\nt\nE [ (Gt(Gt)T ]\n∥ ∥ ∥ ∥ ∥ , ∥ ∥ ∥ ∥ ∥ ∑\nt\nE [ (Gt)TGt ]\n∥ ∥ ∥ ∥ ∥ } .\nSet ρ0 := max {∥ ∥ ∥ ∑d t=1 E(G t(Gt)T ) ∥ ∥ ∥ , ∥ ∥ ∥ ∑d t=1 E((G t)TGt) ∥ ∥ ∥ } . Then the remaining work is to uniformly upper bound ‖Gtk‖ for all 1 ≤ t ≤ d and 1 ≤ k ≤ m0 and upper bound ρ0. First we turn to the uniform bound on the spectral norm of the random matrix Gtk for all 1 ≤ t ≤ d and 1 ≤ k ≤ m0. We have for all 1 ≤ t ≤ d and 1 ≤ k ≤ m0\n∥ ∥Gtk ∥ ∥ ≤ max i,j,t wt ∥ ∥n1n2F t ijEij ∥ ∥ = n1n2 max i,j,t wt|F tij |.\nSince µ(U) ≤ µ0, the variance of each entry of the random matrix F t can be bounded as Var(F tij) ≤ µ2 0 r\nn1 σ22(d − t) + σ21 . Let σ2max = maxt w2t\n(\nµ2 0 r\nn1 σ22(d− t) + σ21\n)\n. Then by the tail probability\nof Gaussian random variables and the standard union bound (over i, j), for all 1 ≤ t ≤ d and 1 ≤ k ≤ m0 we have\nP\n(\n∥ ∥Gtk ∥ ∥ ≤ n1n2 √ 2 log(d(n1 + n2)n1n2)σ2max =: L ) ≥ 1− 2/(n1 + n2).\nSecond we turn to the computation of E(Gt(Gt)T ). We have\nE(Gt(Gt)T ) = w2t n 2 1n 2 2\nn1 ∑\ni=1\nn2 ∑\nj=1\n(F tij) 2EijE T ij\n1\nn1n2 = w2t n1n2\nn1 ∑\ni=1\nn2 ∑\nj=1\n(F tij) 2Eii.\nSimilarly E((Gt)TGt) = w2t n1n2 ∑n1 i=1 ∑n2 j=1(F t ij) 2Ejj . Then\nρ = n1n2max\n\n\n\nmax i\nd ∑\nt=1\nn2 ∑\nj=1\nw2t (F t ij) 2,max j\nd ∑\nt=1\nn1 ∑\ni=1\nw2t (F t ij) 2\n\n\n\n.\nLet ai = ∑d\nt=1 ∑n2 j=1 w 2 t (F t ij) 2 and bj = ∑d t=1 ∑n1 i=1 w 2 t (F t ij) 2. We first bound maxi ai. Note that\nai = ∑d t=1 w 2 t ∑n2 j=1(Y t ij + Z t ij) 2 ≤ 2∑dt=1 w2t ∑n2 j=1[(Y t ij) 2 + (Ztij) 2]. Note that for 1 ≤ i ≤ n1 and 1 ≤ t ≤ d, ∑n2j=1(Ztij)2 ∼ σ21χ2(n2) and are independent. So by the tail bound of Chi-squared variable and the standard union bound (over i and t) we have\nP\n\nmax i\nd ∑\nt=1\nw2t\nn2 ∑\nj=1\n(Ztij) 2 ≤ 5n2\nd ∑\nt=1\nw2t σ 2 1\n\n ≥ 1− dn1 exp(−n2). (28)\nSimilarly we have\nP\n(\nmax j\nd ∑\nt=1\nw2t\nn2 ∑\ni=1\n(Ztij) 2 ≤ 5n1\nd ∑\nt=1\nw2t σ 2 1\n)\n≥ 1− dn2 exp(−n1). (29)\nFor ∑n2 j=1(Y t ij) 2, note that Y tij is Gaussian distributed and the variance is not greater than µ2 0 r n1 (d − t)σ22 for all i, j, t, since µ(U) ≤ µ0. For a fixed i, for all 1 ≤ j ≤ n2, Y tij are independent Gaussian random variables. So given i and t, applying the tail bound of Chi-squared random variables yields\nP\n\n\nn2 ∑\nj=1\n(Y tij) 2 ≤ 5n2(d− t)\nµ20r\nn1 σ22\n\n ≥ 1− exp(−n2).\nBy the standard union bound (over i and t) we have\nP\n\nmax i\nd ∑\nt=1\nw2t\nn2 ∑\nj=1\n(Y tij) 2 ≤ 5n2\nµ20r\nn1\nd ∑\nt=1\n(d− t)w2t σ22\n\n ≥ 1− dn1 exp(−n2). (30)\nNow we turn to ∑n1 i=1(Y t ij) 2, which follows a Chi-squared distribution (d− t)σ22χ2(r), since n1 ∑\ni=1\n(Y tij) 2 = (Y t:j) TY t:j = ǭ t j:U TU ( ǭtj: )T = ǭtj: ( ǭtj: )T\nwhere ǭt = ∑d s=t+1 ǫ s. The last equality uses the fact that U is orthonormal. Then by the tail bound of Chi-squared random variables and the standard union bound (over j and t) we have\nP\n(\nmax j\nd ∑\nt=1\nw2t\nn1 ∑\ni=1\n(Y tij) 2 ≤ 5n1\nd ∑\nt=1\n(d− t)w2t σ22\n)\n≥ 1− dn2 exp(−n1). (31)\nCombining (28) and (30) yields\nP\n(\nmax i\nai ≤ 10n2 d ∑\nt=1\nw2t\n(\nσ21 + µ20r\nn1 (d− t)σ22\n)\n)\n≥ 1− 2dn1 exp(−n2). (32)\nSimilarly combining (29) and (31) yields\nP\n(\nmax j\nbj ≤ 10n1 d ∑\nt=1\nw2t ( σ21 + (d− t)σ22 )\n)\n≥ 1− 2dn2 exp(−n1). (33)\nNote that 1 ≤ µ0 ≤ √ n1/ √ r, so µ 2 0 r\nn1 ≤ 1. Now we are ready to bound ρ0 by combining (32) and\n(33):\nP\n( ρ0 ≤ 10nmaxn1n2 ( d ∑\nt=1\nw2t σ 2 1 +\nd ∑\nt=1\nw2t (d− t)σ22\n)\n=: ν\n)\n≥ 1−4dnmax exp(−nmin). (34)\nNow by Lemma E.3, we have\nP\n(∥\n∥ ∥ ∥ ∥\nd ∑\nt=1\nHt − d ∑\nt=1\nwtF t\n∥ ∥ ∥ ∥ ∥ ≥ s ) ≤ (n1 + n2) exp ( −m0s2/2 ν + 2Ls/3 ) .\nIf we let s = √\n8 log(n1+n2)ν m0 and substitute this into the above matrix Bernstein inequality we obtain\nP\n\n\n∥ ∥ ∥ ∥ ∥ d ∑\nt=1\nHt − d ∑\nt=1\nwtF t\n∥ ∥ ∥ ∥ ∥ ≥ √ 8 log(n1 + n2)ν\nm0\n\n ≤ 1/(n1 + n2).\nA hidden condition when the above inequality holds is that ν dominates the denominator of the exponential term. The remaining work is to have sufficiently large m0 to guarantee that ν dominates the denominator of the exponential, which follows\nν ≥ 2/3L √ 8 log(n1 + n2)ν\nm0 .\nThe above inequality immediately implies that\nm0 ≥ 32\n45 nmin log(d(n1 + n2)n1n2) log(n1 + n2)\nmaxt w 2 t\n(\n(d− t)µ 2 0 r\nn1 σ22 + σ 2 1\n)\n∑d t=1 w 2 t ((d− t)σ22 + σ21)\n.\nNote that n1 + n2 > ni, i = 1, 2, and n1 + n2 > d, then the above sample complexity can be simplified as\nm0 ≥ 128\n45 nmin log\n2(n1 + n2) maxt w\n2 t\n(\n(d− t)µ 2 0 r\nn1 σ22 + σ 2 1\n)\n∑d t=1 w 2 t ((d− t)σ22 + σ21)\n. (35)\nThe remaining work is to bound ∥ ∥\n∥ ∑d t=1 wtF\nt ∥ ∥\n∥ . First we note that each entry of F t is Gaussian and\nthe variance is not greater than σ21 +(d− t)σ22 . Then, according to results on bounds for the spectral norm of i.i.d. Gaussian ensemble, we have\nP\n\n\n∥ ∥ ∥ ∥ ∥ d ∑\nt=1\nwtF t\n∥ ∥ ∥ ∥ ∥ ≤ 2\n√ √ √ √ d ∑\nt=1\nw2t (σ 2 1 + (d− t)σ22) √ nmax\n\n ≥ 1− C1 exp(−c2nmax), (36)\nwhere C1, c2 are absolute positive constants. Note that C1 exp(−c2nmax) ≪ dnmax exp(−nmin). Now we are ready to bound ‖J‖22. With probability at least 1 − 3n1+n2 − 5dnmax exp(−nmin) we have\n‖J‖22 ≤ p2   ∥ ∥ ∥ ∥\n∥\nd ∑\nt=1\nwtF t\n∥ ∥ ∥ ∥ ∥ +\n√\n8 log(n1 + n2)ν\nm0\n\n\n2\n≤ 320p2max{n1n2 log(n1 + n2)/m0, 1}nmax d ∑\nt=1\nw2t ((d− t)σ22 + σ21)\n= 320p2 d ∑\nt=1\nw2t ((d− t)w22 + σ21)n1n2 log(n1 + n2)nmax/m0\n= 320m0 log(n1 + n2)\n∑d t=1 w 2 t ((d− t)σ22 + σ21) nmin .\n(37)\nThe first equality uses the fact that m0 < n1n2 log(n1 + n2).\nCombining (26),(27) and (37) yields the second part of inequality (11) in Theorem 3.8."
    }, {
      "heading" : "F Proof of Lemma E.1",
      "text" : "Proof. The proof is almost the same as the proof of Lemma 12 in [15] with some minor modifications.\nSet F = 44rn1n2m0 (E(‖ΣR‖)) 2. We will show that the probability of the following bad event is small:\nB = { ∃X ∈ E(r) such that ∣ ∣ ∣\n∣ ∣\nd ∑\nt=1\nwt ∥ ∥At (X) ∥ ∥ 2 2 − p ‖X‖2F\n∣ ∣ ∣ ∣ ∣ > p 2 ‖X‖2F + F } .\nNote that B contains the complement of the event in Lemma E.1.\nWe use a peeling argument to bound the probability of B. Let ν = √ 64 ∑d t=1 w 2 t log(n1+n2)\nlog(6/5)m0 and\nα = 6/5. For l ∈ N let\nSl =\n{\nX ∈ E(r) : ναl−1 ≤ 1 n1n2 ‖X‖2F ≤ ναl } .\nThen if event B holds for some X ∈ E(r), it must be that X belongs to some Sl and ∣\n∣ ∣ ∣ ∣\nd ∑\nt=1\nwt ∥ ∥At (X) ∥ ∥ 2 2 − p ‖X‖2F\n∣ ∣ ∣ ∣ ∣ > p 2 ‖X‖2F + F > 5 12 αlνm0 + F . (38)\nFor T > ν consider the set\nE(r, T ) = { X ∈ E(r) : ‖X‖2F ≤ n1n2T }\nand the event\nBl = { ∃X ∈ E(r, αlν) such that ∣ ∣ ∣ ∣\n∣\nd ∑\nt=1\nwt ∥ ∥At (X) ∥ ∥ 2 2 − p ‖X‖2F\n∣ ∣ ∣ ∣ ∣ > 5 12 αlνm0 + F } . (39)\nNote that X ∈ Sl implies that X ∈ E(r, αlν). Then (38) implies that Bl holds and B ⊂ ∪Bl. Thus, it is sufficient to bound the probability of the simpler event Bl and then apply the union bound. Such a bound is given by the following lemma. Its proof is given in Appendix G. Let\nHT = sup X∈E(r,T )\n∣ ∣ ∣ ∣ ∣ d ∑\nt=1\nwt ∥ ∥At (X) ∥ ∥ 2 2 − p ‖X‖2F\n∣ ∣ ∣ ∣ ∣ .\nLemma F.1. Suppose all At’s are fixed uniform sampling ensembles. Then\nP\n(\nHT > 5\n12 αlνm0 + F\n) ≤ exp ( −c5m0T 2 ∑d\nt=1 w 2 t\n)\n,\nwhere c5 = 1/128.\nThe above lemma implies that P(Bl) ≤ exp(−c5m0α2lν2). By a union bound, we have\nP(B) ≤ ∞ ∑\nl=1\nP(Bl) ≤ ∞ ∑\nl=1\nexp\n(\n−c5m0α2lν2 ∑d\nt=1 w 2 t\n) ≤ ∞ ∑\nl=1\nexp\n(\n−(2c5m0αν2)l ∑d\nt=1 w 2 t\n)\n,\nwhere the last inequality uses the bound ex ≥ x. Then, substituting v = √ 64 ∑d t=1 w 2 t log(n1+n2)\nlog(6/5)m0\ninto the above summation we obtain\nP(B) ≤ 2/(n1 + n2). This completes the proof."
    }, {
      "heading" : "G Proof of Lemma F.1",
      "text" : "Proof. The proof is almost the same as the proof of Lemma 14 in [15] with some minor modifications.\nBy Massart’s concentration inequality (see, e.g., [2], Theorem 14.2), we have\nP\n(\nHT ≥ E(HT ) + 1\n9\n5\n12 T\n) ≤ exp ( −c5m0T 2 ∑d\nt=1 w 2 t\n)\n, (40)\nwhere c5 = 1/128. Next we bound the expectation E(HT ). Using a symmetrization argument we obtain\nE(HT ) ≤ 2E (\nsup X∈E(r,T )\n∣ ∣ ∣ ∣ ∣ d ∑\nt=1\nwtγ t i\nm0 ∑\ni=1\n〈 Ati, X 〉2\n∣ ∣ ∣ ∣ ∣ ) ,\nwhere γti is a Rademacher variable (independent on both i and t). The assumption ‖X‖∞ = 1 implies that |〈Ati, X〉| ≤ 1. Then the contraction inequality yields\nE(HT ) ≤ 8E (\nsup X∈E(r,T )\n∣ ∣ ∣ ∣ ∣ d ∑\nt=1\nwtγ t i\nm0 ∑\ni=1\n〈 Ati, X 〉\n∣ ∣ ∣ ∣ ∣ ) = 8E ( sup X∈E(r,T ) |〈ΣR, X〉| ) ,\nwhere ΣR = ∑d\nt=1 ∑m0 i=1 wtγ t iA t i . Since X ∈ E(r, T ), we have\n‖X‖∗ ≤ √ r ‖X‖F ≤ √ rn1n2T .\nThen by the trace duality inequality, we obtain\nE(HT ) ≤ 8 √ rn1n2TE ‖ΣR‖2 . Finally using\n1\n9\n5\n12 T + 8\n√ rn1n2TE ‖ΣR‖2 ≤ 1\n9\n5\n12 T +\n8\n9\n5\n12 T + 44rn1n2 (E ‖ΣR‖2)\n2\ncombined with (40) we complete the proof."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Low-rank matrix factorizations arise in a wide variety of applications – including recommendation systems, topic models, and source separation, to name just a few. In these and many other applications, it has been widely noted that by incorporating temporal information and allowing for the possibility of time-varying models, significant improvements are possible in practice. However, despite the reported superior empirical performance of these dynamic models over their static counterparts, there is limited theoretical justification for introducing these more complex models. In this paper we aim to address this gap by studying the problem of recovering a dynamically evolving low-rank matrix from incomplete observations. First, we propose the locally weighted matrix smoothing (LOWEMS) framework as one possible approach to dynamic matrix recovery. We then establish error bounds for LOWEMS in both the matrix sensing and matrix completion observation models. Our results quantify the potential benefits of exploiting dynamic constraints both in terms of recovery accuracy and sample complexity. To illustrate these benefits we provide both synthetic and real-world experimental results.",
    "creator" : "LaTeX with hyperref package"
  }
}