{
  "name" : "1503.06567.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On some provably correct cases of variational inference for topic models",
    "authors" : [ "Pranjal Awasthi", "Andrej Risteski" ],
    "emails" : [ "pawashti@cs.princeton.edu.", "risteski@cs.princeton.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 3.\n06 56\n7v 2\n[ cs\n.L G\n] 2\n2 A\nMore specifically, we show that variational inference provably learns the optimal parameters of a topic model under natural assumptions on the topic-word matrix and the topic priors. The properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora et al., 2012b). The assumptions on the topic priors are related to the well known Dirichlet prior, introduced to the area of topic modeling by (Blei et al., 2003).\nIt is well known that initialization plays a crucial role in how well variational based algorithms perform in practice. The initializations that we use are fairly natural. One of them is similar to what is currently used in LDA-c, the most popular implementation of variational inference for topic models. The other one is an overlapping clustering algorithm, inspired by a work by (Arora et al., 2014) on dictionary learning, which is very simple and efficient.\nWhile our primary goal is to provide insights into when variational inference might work in practice, the multiplicative, rather than the additive nature of the variational inference updates forces us to use fairly non-standard proof arguments, which we believe will be of general interest. Our proofs rely on viewing the updates as an operation which, at each timestep, sets the new parameter estimates to be noisy convex combinations of the ground truth values, and a bounded error term which depends on the previous estimate. The weight on the ground truth values will be large, compared to the error term, which will cause the error term to eventually reach zero. The large weight on the ground truth values will be a byproduct of our model assumptions, which will imply a “local” notion of anchor words for each document - words which only appear in one topic in a given document, as well as a “local” notion of anchor documents for each word - documents where that word appears as part of a single topic.\n∗Princeton University, Computer Science Department. Email: pawashti@cs.princeton.edu. Supported by NSF grant CCF1302518. †Princeton University, Computer Science Department. Email: risteski@cs.princeton.edu. Partially supported by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Sanjeev Arora’s Simons Investigator Award, and a Simons Collaboration Grant.\nContents"
    }, {
      "heading" : "1 Introduction 3",
      "text" : ""
    }, {
      "heading" : "2 Latent variable models and EM 3",
      "text" : ""
    }, {
      "heading" : "3 Topic models 4",
      "text" : ""
    }, {
      "heading" : "4 Variational relaxation for learning topic models 5",
      "text" : "4.1 Simplified updates in the long document limit . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4.2 Alternating KL minimization and thresholded updates . . . . . . . . . . . . . . . . . . . . . . 6"
    }, {
      "heading" : "5 Initializations 7",
      "text" : ""
    }, {
      "heading" : "6 Case study 1: Sparse topic priors, support initialization 8",
      "text" : ""
    }, {
      "heading" : "7 Case study 2: Dominating topics, seeded initialization 9",
      "text" : ""
    }, {
      "heading" : "8 On common words 11",
      "text" : ""
    }, {
      "heading" : "9 Discussion and open problems 11",
      "text" : ""
    }, {
      "heading" : "A Notation throughout supplementary material 12",
      "text" : ""
    }, {
      "heading" : "B Case study 1: Sparse topic priors, support initialization 12",
      "text" : "B.1 Provable convergence of tEM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 B.1.1 Determining largest topic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 B.1.2 Lower bounds on the γtd,i and β t i,j variables . . . . . . . . . . . . . . . . . . . . . . . . 13\nB.1.3 Upper bound on the βti,j values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 B.1.4 Upper bounds on the γ values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 B.1.5 Phase II: Alternating minimization - upper and lower bound evolution . . . . . . . . . 16\nB.2 Iterative tEM updates, incomplete tEM updates . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.3 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nB.3.1 Constructing a no-false-positives test . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.3.2 Finding the topic supports from identifying pairs . . . . . . . . . . . . . . . . . . . . . 21 B.3.3 Finding the identifying pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.3.4 Finding the document supports . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23"
    }, {
      "heading" : "C Case study 2: Dominating topics, seeded initialization 24",
      "text" : "C.1 Estimates on the dominating topic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.2 Phase I: Determining the anchor words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nC.2.1 Lower bounds on the βti,j values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 C.2.2 Decreasing βti′,j values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nC.3 Discriminative words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C.3.1 Bounds on the βti,j values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C.3.2 Decreasing βti′,j values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 C.4 Determining dominant topic and parameter range . . . . . . . . . . . . . . . . . . . . . . . . . 32 C.5 Getting the supports correct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 C.6 Alternating minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33"
    }, {
      "heading" : "D Justification of prior assumptions 33",
      "text" : "D.1 Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 D.2 Weak topic correlations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 D.3 Dominant topic equidistribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 D.4 Independent topic inclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37"
    }, {
      "heading" : "E On common words 37",
      "text" : "E.1 Phase I with common words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 E.2 Phase II of analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 E.3 Generalizing Case Study 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43"
    }, {
      "heading" : "F Estimates on number of documents 43",
      "text" : ""
    }, {
      "heading" : "1 Introduction",
      "text" : "Over the last few years, heuristics for non-convex optimization have emerged as one of the most fascinating phenomena for theoretical study in machine learning. Methods like alternating minimization, EM, variational inference and the like enjoy immense popularity among ML practitioners, and with good reason: they’re vastly more efficient than alternate available methods like convex relaxations, and are usually easily modified to suite different applications.\nTheoretical understanding however is sparse and we know of very few instances where these methods come with formal guarantees. Among more classical results in this direction are the analyses of Lloyd’s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of Gaussians (Kumar and Kannan, 2010), (Dasgupta and Schulman, 2000), (Dasgupta and Schulman, 2007). The recent work of (Balakrishnan et al., 2014) also characterizes global convergence properties of the EM algorithm for more general settings. Another line of recent work has focused on a different heuristic called alternating minimization in the context of dictionary learning. (Agarwal et al., 2013), (Arora et al., 2015) prove that with appropriate initialization, alternating minimization can provably recover the ground truth. (Netrapalli et al., 2013) have proven similar results in the context of phase retreival.\nAnother popular heuristic which has so far eluded such attempts is known as variational inference (Jordan et al., 1999). We provide the first characterization of global convergence of variational inference based algorithms for topic models (Blei et al., 2003). We show that under natural assumptions on the topicword matrix and the topic priors, along with natural initialization, variational inference converges to the parameters of the underlying ground truth model. To prove our result we need to overcome a number of technical hurdles which are unique to the nature of variational inference. Firstly, the difficulty in analyzing alternating minimization methods for dictionary learning is alleviated by the fact that one can come up with closed form expressions for the updates of the dictionary matrix. We do not have this luxury. Second, the “norm” in which variational inference naturally operates is KL divergence, which can be difficult to work with. We stress that the focus of this work is not to identify new instances of topic modeling that were previously not known to be efficiently solvable, but rather providing understanding about the behaviour of variational inference, the defacto method for learning and inference in the context of topic models."
    }, {
      "heading" : "2 Latent variable models and EM",
      "text" : "We briefly review expectation-maximization (EM) and variational methods. We will be dealing with latent variable models, where the observations Xi are generated according to a distribution\nP (Xi|θ) = P (Zi|θ)P (Xi|Zi, θ)\nwhere θ are parameters of the models, and Zi are termed as hidden variables. Given the observations Xi, a common task in this context is to find the maximum likelihood value of the parameter θ:\nargmaxθ ∑\ni\nlog(P (Xi|θ))\nThe expectation-maximization (EM) algorithm is an iterative method to achieve this, dating all the way back to (Dempster et al., 1977) and (Sundberg, 1974) in the 70s. In the above framework it can be formulated as the following procedure, maintaining estimates θt, P̃ t(Z) of the model parameters and the posterior distribution over the hidden variables:\n• E-step: Compute the distribution P̃ t(Z) = P (Z|X, θt−1)\n• M-step: Set θt to be argmaxθ ∑\ni\nEP̃ t−1 [logP (Xi, Zi|θ)]\nIt’s implicitly assumed however, that both steps can be performed efficiently. Sadly, that is not the case in many scenarios. A common approach then is to relax the above formulation to a tractable form. This is achieved by choosing an appropriate family of distributions F , and perform the following updates:\n• Variational E-step: Compute the distribution P̃ t(Z) = minP t∈FKL(P t(Z)||P (Z|X, θt−1)\n• Variational M-step: Set θt to be argmaxθ ∑ iEP̃ t−1 [logP (Xi, Zi|θ)]\nBy picking the family F appropriately, it’s often possible to make both steps above run in polynomial time. None of the above two families of approximations, however, usually come with any guarantees. With EM, the problem is ensuring that one does not get stuck in a local optimum. With variational EM, additionally, we are faced with the issue of in principle not even exploring the entire space of solutions."
    }, {
      "heading" : "3 Topic models",
      "text" : "We will focus on a particular latent variable model, which is very often studied - topic models (Blei et al., 2003). The generative model here is as follows: there is a prior distribution over topics α. Then, each document is generated by the following process:\n• Sample a proportion of topics γ1, γ2, . . . , γk according to α.\n• For each position in the document, pick a topic according to a multinomial distribution with parameters γ1, . . . , γk.\n• Conditioned on topic i being picked at that position, pick a word j from a multinomial with parameters (βi,1, βi,2, . . . , βi,k)\nIn this paper we will be interested in topic priors which result in sparse documents and where the correlation of the distributions for different topics is small. These types of properties are very commonly assumed, and are satisfied by the Dirichlet prior, one of the most popular priors in topic modeling. (Originally introduced by (Blei et al., 2003).)\nThe body of work on topic models is vast (Blei and Lafferty, 2009). Prior theoretical work relevant in the context of this paper includes the sequence of works by (Arora et al., 2012b),(Arora et al., 2013), as well as (Anandkumar et al., 2013), (Ding et al., 2013), (Ding et al., 2014) and (Bansal et al., 2014). (Arora et al., 2012b) and (Arora et al., 2013) assume that the topic-word matrix contains “anchor words”. This means that each topic has a word which appears in that topic, and no other. (Anandkumar et al., 2013) on the other hand work with a certain expansion assumption on the word-topic graph, which says that if one takes a subset S of topics, the number of words in the support of these topics should be at least |S|+ smax, where smax is the maximum support size of any topic. Neither paper needs any assumption on the topic priors, and can handle (almost) arbitrarily short documents.\nThe assumptions we make on the word-topic matrix will be related to the ones in the above works, but our documents will need to be long, so that the empirical counts of the words are close to their expected counts. Our priors will also be more structured. This is expected since we are trying to analyze an existing heuristic rather than develop a new algorithmic strategy. The case where the documents are short seems significantly more difficult. Namely, in that case there are two issues to consider. One is proving the variational approximation to the posterior distribution over topics is not too bad. The second is proving that the updates do actually reach the global optimum. Assuming long documents allows us to focus on the second issue alone, which is already challenging. On a high level, the instances we consider will have the following structure:\n• The topics will satisfy a weighted expansion property: for any set S of topics of constant size, for any topic i in this set, the probability mass on words which belong to i, and no other topic in S will be large. (Similar to the expansion in (Anandkumar et al., 2013), but only over constant sized subsets.)\n• The number of topics per document will be small. Further, the probability of including a given topic in a document is almost independent of any other topics that might be included in the document already. Similar properties are satisfied by the Dirichlet prior, one of the most popular priors in topic modeling. (Originally introduced by (Blei et al., 2003).) The documents will also have a “dominating topic”, similarly as in (Bansal et al., 2014).\n• For each word j, and a topic i it appears in, there will be a decent proportion of documents that contain topic i and no other topic containing j. These can be viewed as “local anchor documents” for that word-pair topic.\nWe state below, informally, our main result. See Sections 6 and 7 for more details.\nTheorem. Under the above mentioned assumptions, popular variants of variational inference for topic models, with suitable initializations, provably recover the ground truth model in polynomial time."
    }, {
      "heading" : "4 Variational relaxation for learning topic models",
      "text" : "In this section we briefly review the variational relaxation for topic models following closely the description in (Blei et al., 2003). Throughout the paper, we will denote by N the total number of words and K the number of topics. We will assume that we are working with a sample set of D documents. We will also denote by f̃d,j the fractional count of word j in document d (i.e. f̃d,j = Count(j)/Nd, where Count(j) is the number of times word j appears in the document, and Nd is the number of words in the document).\nFor topic models variational updates are way to approximate the computationally intractable E-step (Sontag and Roy, 2000) as described in Section 2. Recall the model parameters for topic models are the topic prior parameters α and the topic-word matrix β. The observableX is the list of words in the document. The latent variables are the topic assignments Zj at each position j in the document and the topic proportions γ. The variational E-step hence becomes P̃ t(Z, γ) = minP t∈FKL(P t(Z, γ)||P (Z, γ|X,αt, βt) for some family F of distributions. The family F one usually considered is P t(γ, Z) = q(γ)ΠNdj=1q ′ j(Zj), i.e. a mean field family. In (Blei et al., 2003) it’s shown that for Dirichlet priors α the optimal distributions q, q′j are a Dirichlet distribution for q, with some parameter γ̃, and multinomials for q′j , with some parameters φj . The variational EM updates are shown to have the following form.\n• In the E-step, one runs to convergence the following updates on the φ and γ̃ parameters:\nφd,j,i ∝ βti,wd,jeEq [log(γd)|γ̃d]\n˜γd,i = α t d,i +\nNd ∑\nj=1\nφd,j,i\n• In the M-step, one updates the β and parameters as follows:\nβt+1i,j ∝ D ∑\nd=1\nNd ∑\nj′=1\nφtd,j,iwd,j,j′\nwhere φtd,j,i is the converged value of φd,j,i; wd,j is the word in document d, position j; wd,j,j′ is an indicator variable which is 1 if the word in position j′ in document d is word j.\nThe α Dirichlet parameters do not have a closed form expression and are updated via gradient descent."
    }, {
      "heading" : "4.1 Simplified updates in the long document limit",
      "text" : "From the above updates it is difficult to give assign an intuitive meaning to the γ̃ and φ parameters. (Indeed, it’s not even clear what one would like them to be ideally at the global optimum.) We will be however working in the large document limit - and this will simplify the updates. In particular, in the E-step, in the large document limit, the first term in the update equation for γ̃ has a vanishing contribution. In this case, we can simplify the E-update as:\nφd,j,i ∝ βti,jγd,i\nγd,i ∝ Nd ∑\nj=1\nφd,j,i\nNotice, importantly, in the second update we now use variables γd,i instead of γ̃d,i, which are normalized such that K ∑\ni=1\nγd,i = 1. These correspond to the max-likelihood topic proportions, given our current estimates β t i,j\nfor the model parameters. The M-step will remain as is - but we will focus on the β only, and ignore the α updates - as the α estimates disappeared from the E updates:\nβt+1i,j ∝ D ∑\nd=1\nf̃d,jγ t d,i\nwhere γtd,i is the converged value of γd,i. In this case, the intuitive meaning of the β t and γt variables is clear: they are estimates of the the model parameters, and the max-likelihood topic proportions, given an estimate of the model parameters, respectively.\nThe way we derived them, these updates appear to be an approximate form of the variational updates in (Blei et al., 2003). However it is possible to also view them in a more principled manner. These updates approximate the posterior distribution P (Z, γ|X,αt, βt) by first approximating this posterior by P (Z|X, γ∗, αt, βt), where γ∗ is the max-likelihood value for γ, given our current estimates of α, β, and then setting P (Z|X, γ∗, αt, βt) to be a product distribution.\nIt is intuitively clear that in the large document limit, this approximation should not be much worse than the one in (Blei et al., 2003), as the posterior concentrates around the maximum likelihood value. (And in fact, our proofs will work for finite, but long documents.) Finally, we will rewrite the above equations in\na slightly more convenient form. Denoting fd,j =\nK ∑\ni=1\nγd,iβ t i,j , the E-step can be written as: iterate until\nconvergence\nγd,i = γd,i\nN ∑\nj=1\nf̃d,j fd,j βti,j\nThe M-step becomes:\nβt+1i,j = β t i,j\n∑D d=1 f̃d,j ftd,j γtd,i ∑D\nd=1 γ t d,i\nwhere f td,j =\nK ∑\ni=1\nγtd,iβ t i,j and γ t d,i is the converged value of γd,i."
    }, {
      "heading" : "4.2 Alternating KL minimization and thresholded updates",
      "text" : "We will further modify the E and M-step update equations we derived above. In a slightly modified form, these updates were used in a paper by (Lee and Seung, 2000) in the context of non-negative matrix factorization. There the authors proved that under these updates ∑D\nd=1 KL(f t d,j||f̃d,j) is non-decreasing. One can\neasily modify their arguments to show that the same property is preserved if the E-step is replaced by a step γtd = minγtd∈∆K KL(f̃d||fd), where ∆K is the K-dimensional simplex - i.e. minimizing the KL divergence between the counts and the ”predicted counts” with respect to the γ variables. (In fact, iterating the γ\nupdates above is a way to solve this convex minimization problem via a version of gradient descent which makes multiplicative updates, rather than additive updates.)\nThus the updates are performing alternating minimization using the KL divergence as the distance measure (with the difference that for the β variables one essentially just performs a single gradient step). In this paper, we will make a modification of the M-step which is very natural. Intuitively, the update for βti,j goes over all appearances of the word j and adds the “fractional assignment” of the word j to topic i under our current estimates of the variables β, γ. In the modified version we will only average over those documents d, where γtd,i > γ t d,i′ , ∀i′ 6= i.\nThe intuitive reason behind this modification is the following. The EM updates we are studying work with the KL divergence, which puts more weight on the larger entries. Thus, for the documents in Di, the estimates for γtd,i should be better than they might be in the documents D \\Di. (Of course, since the terms f td,j involve all the variables γ t d,i, it is not a priori clear that this modification will gain us much, but we will prove that it in fact does.) Formally, we discuss the following three modifications of variational inference (we call them tEM, for thresholded EM):\nAlgorithm 1 KL-tEM\n• (E-step) Solve the following convex program for each document d:\nmin γt d,i\n∑\nj\nf̃d,j log( f̃d,j f td,j )\ns.t. (1): γtd,i ≥ 0, ∑ i γ t d,i = 1 and γ t d,i = 0 if i does not belong to document d (M-step) Let Di to be the set of documents d, s.t. γ t d,i > γ t d,i′ , ∀i′ 6= i.\nSet βt+1i,j = β t i,j\n∑\nd∈Di\nf̃d,j ft d,j γtd,i ∑\nd∈Di γtd,i\nAlgorithm 2 Iterative tEM\n• (E-step) Initialize γd,i uniformly among the topics in the support of document d. Repeat\nγd,i = γd,i\nN ∑\nj=1\nf̃d,j fd,j βti,j (4.1)\nuntil convergence. (M-step) Same as above.\nAlgorithm 3 Incomplete tEM\n• (E-step) Initialize γd,i with the values gotten in the previous iteration, just perform one step of 4.1. (M-step) Same as before."
    }, {
      "heading" : "5 Initializations",
      "text" : "We will consider two different strategies for initialization. First, we will consider the case where we initialize with the topic-word matrix, and the document priors having the correct support. The analysis of tEM in this case will be the cleanest. While the main focus of the paper is tEM, we’ll show that this initialization can actually be done for our case efficiently.\nSecond, we will consider an initialization that is inspired by what the current LDA-c implementation uses. Concretely, we’ll assume that the user has some way of finding, for each topic i, a seed document in which the proportion of topic i is at least Cl. Then, when initializing, one treats this document as if it were\npure: namely one sets β0i,j to be the fractional count of word j in this document. We do not attempt to design an algorithm to find these documents."
    }, {
      "heading" : "6 Case study 1: Sparse topic priors, support initialization",
      "text" : "We start with a simple case. As mentioned, all of our results only hold in the long documents regime: we will assume for each document d, the number of sampled words is large enough, so that one can approximate the expected frequencies of the words, i.e., one can find values γ∗d,i, such that f̃d,j = (1 ± ǫ) ∑K i=1 γ ∗ d,iβ ∗ i,j . We’ll split the rest of the assumptions into those that apply to the topic-word matrix, and the topic priors. Let’s first consider the assumptions on the topic-word matrix. We will impose conditions that ensure the topics don’t overlap too much. Namely, we assume:\n• Words are discriminative: Each word appears in o(K) topics. • Almost disjoint supports : ∀i, i′, if the intersection of the supports of i and i′ is S, ∑j∈S β∗i,j ≤ o(1) · ∑\nj β ∗ i,j .\nWe also need assumptions on the topic priors. The documents will be sparse, and all topics will be roughly equally likely to appear. There will be virtually no dependence between the topics: conditioning on the size or presence of a certain topic will not influence much the probability of another topic being included. These are analogues of distributions that have been analyzed for dictionary learning (Arora et al., 2015). Formally:\n• Sparse and gapped documents : Each of the documents in our samples has at most T = O(1) topics. Furthermore, for each document d, the largest topic i0 = argmaxiγ ∗ d,i is such that for any other topic\ni′, γ∗d,i′ − γ∗d,i0 > ρ for some (arbitrarily small) constant ρ. • Dominant topic equidistribution: The probability that topic i is such that γ∗d,i > γ∗d,i′ , ∀i′ 6= i is Θ(1/K). • Weak topic correlations and independent topic inclusion: For all sets S with o(K) topics, it must be the case that: E[γ∗d,i|γ∗d,i is dominating] = (1 ± o(1))E[γ∗d,i|γ∗d,i is dominating, γ∗d,i′ = 0, i′ ∈ S]. Furthermore, for any set S of topics, s.t. |S| ≤ T − 1, Pr[γ∗d,i > 0|γ∗d,i′∀i′ ∈ S] = Θ( 1K )\nThese assumptions are a less smooth version of properties of the Dirichlet prior. Namely, it’s a folklore result that Dirichlet draws are sparse with high probability, for a certain reasonable range of parameters. This was formally proven by (Telgarsky, 2013) - though sparsity there means a small number of large coordinates. It’s also well known that Dirichlet essentially cannot enforce any correlation between different topics. 1\nThe above assumptions can be viewed as a local notion of separability of the model, in the following sense. First, consider a particular document d. For each topic i that participates in that document, consider the words j, which only appear in the support of topic i in the document. In some sense, these words are local anchor words for that document: these words appear only in one topic of that document. Because of the ”almost disjoint supports” property, there will be a decent mass on these words in each document. Similarly, consider a particular non-zero element β∗i,j of the topic-word matrix. Let’s call Dl the set of documents where β∗i′,j = 0 for all other topics i\n′ 6= i appearing in that document. These documents are like local anchor documents for that word-topic pair: in those documents, the word appears as part of only topic i. It turns out the above properties imply there is a decent number of these for any word-topic pair.\nFinally, a technical condition: we will also assume that all nonzero γ∗d,i, β ∗ i,j are at least 1 poly(N) . Intuitively, this means if a topic is present, it needs to be reasonably large, and similarly for words in topics. Such assumptions also appear in the context of dictionary learning (Arora et al., 2015).\nWe will prove the following\nTheorem 1. Given an instance of topic modelling satisfying the properties specified above, where the number of documents is Ω(K log 2 N\nǫ2 ), if we initialize the supports of the β t i,j and γ t d,i variables correctly, after\nO (log(1/ǫ′) + logN) KL-tEM, iterative-tEM updates or incomplete-tEM updates, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1 + ǫ′, for any ǫ′ s.t. 1 + ǫ′ ≤ 1(1−ǫ)7 .\n1We show analogues of the weak topic correlations property and equidistribution in the supplementary material for com-\npleteness sake.\nTheorem 2. If the number of documents is Ω(K4 log2 K), there is a polynomial-time procedure which with probability 1− Ω( 1K ) correctly identifies the supports of the β∗i,j and γ∗d,i variables.\nProvable convergence of tEM: The correctness of the tEM updates is proven in 3 steps:\n• Identifying dominating topic: First, we prove that if γtd,i is the largest one among all topics in the document, topic i is actually the largest topic.\n• Phase I: Getting constant multiplicative factor estimates : After initialization, after O(logN) rounds, we will get to variables βti,j , γ t d,i which are within a constant multiplicative factor from β ∗ i,j , γ ∗ d,i.\n• Phase II (Alternating minimization - lower and upper bound evolution): Once the β and γ estimates are within a constant factor of their true values, we show that the lone words and documents have a boosting effect: they cause the multiplicative upper and lower bounds to improve at each round.\nThe updates we are studying are multiplicative, not additive in nature, and the objective they are optimizing is non-convex, so the standard techniques do not work. The intuition behind our proof in Phase II can be described as follows. Consider one update for one of the variables, say βti,j . We show that βt+1i,j ≈ αβ∗i,j + (1 − α)Ctβ∗i,j for some constant Ct at time step t. α is something fairly large (one should think of it as 1 − o(1)), and comes from the existence of the local anchor documents. A similar equation holds for the γ variables, in which case the “good” term comes from the local anchor words. Furthermore, we show that the error in the ≈ decreases over time, as does the value of Ct, so that eventually we can reach β∗i,j . The analysis bears a resemblance to the state evolution and density evolution methods in error decoding algorithm analysis - in the sense that we maintain a quantity about the evolving system, and analyze how it evolves under the specified iterations. The quantities we maintain are quite simple - upper and lower multiplicative bounds on our estimates at any round t.\nInitialization: Recall the goal of this phase is to recover the supports - i.e. to find out which topics are present in a document, and identify the support of each topic. We will find the topic supports first. This uses an idea inspired by (Arora et al., 2014) in the setting of dictionary learning. Roughly, we devise a test, which will take as input two documents d, d′, and will try to determine if the two documents have a topic in common or not. The test will have no false positives, i.e., will never say YES, if the documents don’t have a topic in common, but might say NO even if they do. We then ensure that with high probability, for each topic we find a pair of documents intersecting in that topic, such that the test says YES. 2"
    }, {
      "heading" : "7 Case study 2: Dominating topics, seeded initialization",
      "text" : "Next, we’ll consider an initialization which is essentially what the current implementation of LDA-c uses. Namely, we will call the following initialization a seeded initialization:\n• For each topic i, the user supplies a document d, in which γ∗d,i ≥ Cl.\n• We treat the document as if it only contains topic i and initialize with β0i,j = f∗d,j.\nWe show how to modify the previous analysis to show that with a few more assumptions, this strategy works as well. Firstly, we will have to assume anchor words, that make up a decent fraction of the mass of each topic. Second, we also assume that the words have a bounded dynamic range, i.e. the values of a word in two different topics are within a constant B from each other. The documents are still gapped, but the gap now must be larger. Finally, in roughly 1/B fraction of the documents where topic i is dominant, that topic has proportion 1 − δ, for some small (but still constant) δ. A similar assumption (a small fraction of almost pure documents) appeared in a recent paper by (Bansal et al., 2014). Formally, we have:\n• Small dynamic range and large fraction of anchors : For each discriminative words, if β∗i,j 6= 0 and β∗i′,j 6= 0, β∗i,j ≤ Bβ∗i′,j . Furthermore, each topic i has anchor words, such that their total weight is at least p.\n2The detailed initialization algorithm is included in the supplementary material.\n• Gapped documents : In each document, the largest topic has proportion at least Cl, and all the other topics are at most Cs, s.t.\nCl − Cs ≥ 1\np\n( √\n2\n(\np log( 1\nCl ) + (1− p) log(BCl)\n)\n+ √ log(1 + ǫ)\n)\n+ ǫ\n• Small fraction of 1 − δ dominant documents : Among all the documents where topic i is dominating, in a 8/B fraction of them, γ∗d,i ≥ 1− δ, where\nδ := min\n(\nC2l 2B3 − 1 p\n( √\n2\n(\np log( 1\nCl ) + (1− p) log(BCl)\n)\n+ √ log(1 + ǫ)\n)\n− ǫ, 1− √\nCl\n)\nThe dependency between the parameters B, p, Cl is a little difficult to parse, but if one thinks of Cl as 1− η for η small, and p ≥ 1− ηlogB , since log( 1Cl ) ≈ 1 + η, roughly we want that Cl −Cs ≫ 2 p √ η. (In other words, the weight we require to have on the anchors depends only logarithmically on the range B.) In the documents where the dominant topic has proportion 1− δ, a similar reasoning as above gives that we want is approximately γ∗d,i ≥ 1−\n1− 2η 2B3 + 2 p √ η. The precise statement is as follows:\nTheorem 3. Given an instance of topic modelling satisfying the properties specified above, where the number of documents is Ω(K log 2 N\nǫ2 ), if we initialize with seeded initialization, after O (log(1/ǫ ′) + logN) of KL-\ntEM updates, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1 + ǫ′, if 1 + ǫ′ ≥ 1(1−ǫ)7 .\nThe proof is carried out in a few phases:\n• Phase I: Anchor identification: We show that as long as we can identify the dominating topic in each of the documents, anchor words will make progress: after O(logN) number of rounds, the values for the topic-word estimates will be almost zero for the topics for which word w is not an anchor. For topic for which a word is an anchor we’ll have a good estimate.\n• Phase II: Discriminative word identification: After the anchor words are properly identified in the previous phase, if β∗i,j = 0, β t i,j will keep dropping and quickly reach almost zero. The values corresponding\nto β∗i,j 6= 0 will be decently estimated.\n• Phase III: Alternating minimization: After Phase I and II above, we are back to the scenario of the previous section: namely, there is improvement in each next round.\nDuring Phase I and II the intuition is the following: due to our initialization, even in the beginning, each topic is ”correlated” with the correct values. In a γ update, we are minimizing KL(f̃d||fd) with respect to the γd variables, so we need a way to argue that whenever the β estimates are not too bad, minimizing this quantity provides an estimate about how far the optimal γd variables are from γ ∗ d . We show the following useful claim:\nLemma 4. If, for all topics i, KL(β∗i ||βti ) ≤ Rβ, and minγd∈∆KKL(f̃d,j||fd,j) ≤ Rf , after running a KL divergence minimization step with respect to the γd variables, we get that ||γ∗d−γd||1 ≤ 1p ( √ 1 2Rβ+ 1 2 √ Rf )+ǫ.\nThis lemma critically uses the existence of anchor words - namely we show ||β∗v||1 ≥ p||v||1. Intuitively, if one thinks of v as γ∗ − γt, ||β∗v||1 will be large if ||v||1 is large. Hence, if ||β∗ − βt||1 is not too large, whenever ||f∗ − f t||1 is small, so is ||γ∗ − γt||1. We will be able to maintain Rβ and Rf small enough throughout the iterations, so that we can identify the largest topic in each of the documents."
    }, {
      "heading" : "8 On common words",
      "text" : "We briefly remark on common words : words such that β∗i,j ≤ κβ∗i′,j, ∀i, i′, κ ≤ B. In this case, the proofs above, as they are, will not work, 3 since common words do not have any lone documents. However, if 1 − 1κ100 fraction of the documents where topic i is dominant contains topic i with proportion 1− 1κ100 and furthermore, in each topic, the weight on these words is no more than 1κ100 , then our proofs still work with either initialization4 The idea for the argument is simple: when the dominating topic is very large, we show that f∗d,j ftd,j is very highly correlated with β∗i,j βti,j , so these documents behave like anchor documents. Namely, one can show:\nTheorem 5. If we additionally have common words satisfying the properties specified above, after O(log(1/ǫ′)+ logN) KL-tEM updates in Case Study 2, or any of the tEM variants in Case Study 1, and we use the same initializations as before, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1 + ǫ′, if 1 + ǫ′ ≥ 1(1−ǫ)7 ."
    }, {
      "heading" : "9 Discussion and open problems",
      "text" : "In this work we provide the first characterization of sufficient conditions when variational inference leads to optimal parameter estimates for topic models. Our proofs also suggest possible hard cases for variational inference, namely instances with large dynamic range compared to the proportion of anchor words and/or correlated topic priors. It’s not hard to hand-craft such instances where support initialization performs very badly, even with only anchor and common words. We made no effort to explore the optimal relationship between the dynamic range and the proportion of anchor words, as it’s not clear what are the “worst case” instances for this trade-off.\nSeeded initialization, on the other hand, empirically works much better. We found that when Cl ≥ 0.6, and when the proportion of anchor words is as low as 0.2, variational inference recovers the ground truth, even on instances with fairly large dynamic range. Our current proof methods are too weak to capture this observation. (In fact, even the largest topic is sometimes misidentified in the initial stages, so one cannot even run tEM, only the vanilla variational inference updates.) Analyzing the dynamics of variational inference in this regime seems like a challenging problem which would require significantly new ideas."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank Sanjeev Arora for helpful discussions in various stages of this work.\n3We stress we want to analyze whether variational inference will work or not. Handling common words algorithmically is easy: they can be detected and ”filtered out” initially. Then we can perform the variational inference updates over the rest of the words only. This is in fact often done in practice. 4See supplementary material.\nSupplementary material"
    }, {
      "heading" : "A Notation throughout supplementary material",
      "text" : "We will use ≃,.,& to denote that the corresponding (in)equality holds up to constants. We will use ⇔ to denote equivalence. We will say that an event happens with high probability, if it happens with probability 1− 1Kc or 1− 1Nc for some constant c."
    }, {
      "heading" : "B Case study 1: Sparse topic priors, support initialization",
      "text" : "B.1 Provable convergence of tEM\nAs a reminder, the theorem we want to prove is:\nTheorem 1. Given an instance of topic modelling satisfying the Case Study 1 properties specified above, where the number of documents is Ω(K log 2 N\nǫ′2 ), if we initialize the supports of the β t i,j and γ t d,i variables\ncorrectly, after O(log(1/ǫ′)+ logN) KL-tEM, iterative-tEM updates or incomplete-tEM updates, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1+ ǫ′, for any ǫ′ s.t. 1 + ǫ′ ≤ 1(1−ǫ)7 .\nThe general outline of the proof will be the following.\n• Identifying dominating topic: For the modified tEM updates, we need to make sure that the topic with maximal γtd,i is the dominant.\n• Phase I: Getting constant multiplicative factor estimates : First, we’ll show that after initialization, after O(logN) number of rounds, we will get to variables βti,j , γ t d,i which are within a constant multiplicative\nfactor from β∗i,j , γ ∗ d,i.\n– Lower bounds on the β and γ variables : We’ll show that determining the supports of the documents and the topic-word matrix, as well as being able to identify the documents in which topic i is large is enough to ensure that all the βti,j and γ t i,j variables are lower bounded by\n1 C0β β∗i,j and\n1 C0γ γ∗d,i respectively for some constants C 0 β ≥ 1, C0γ ≥ 1.\n– Improving upper bounds on the βti,j values : We show that, if the above two properties are satisfied, we can get a multiplicative upper bound of the βti,j values, which strictly improves at each step until it reaches a constant. This improvement is very fast: we only need a logarithmic number of steps. After this happens, we show that the γ variables corresponding to these β estimates must be within a constant of the ground truth as well.\n• Phase II (Alternating minimization - lower and upper bound evolution): Once the β and γ estimates are within a constant factor of their true values, we show that the lone words and documents have a boosting effect: they cause the multiplicative upper and lower bounds to improve at each round.\nA word about incorporating the ”correct supports” assumption in our algorithms. For the β variables this is obvious: we just set βti,j = 0 if β ∗ i,j = 0. For the γ variables it’s also fairly straightforward. In KL-tEM we mean simply that in the convex program above, we constrain γtd,i = 0 if γ ∗ d,i = 0.\nIn the iterative version, this just means that before starting the γ iterations, we set the initial value to 0 if γ∗d,i = 0, and uniform among the rest of the variables. Same for the incomplete version.\nIn the interest of brevity, whenever we say ”the supports are correct”, the above is what we will mean. Recall, we use t to count the iterations for β variables. Put another way, γtd,i is the value we get for γd,i after the β variables were updated to βti,j . (Which of course, implies, β t+1 i,j will be the values we get for the β variables after the γ variables are updated to γtd,i.) The proofs are for each of the variants of tEM are similar. For starters, we show everything for KL-tEM, and then just mention how to modify the arguments to get the results for the other variants in section B.2.\nB.1.1 Determining largest topic\nFirst, we show that the ”thresholding” operation works. Namely, we show that if γtd,i > γ t d,i′ , ∀i 6= i′, then γ∗d,i is the largest topic in the document (there is a unique one by the ”slightly gapped documents” property). Furthermore, we can say that 12γ ∗ d,i ≤ γtd,i ≤ 2γ∗d,i.\nLemma 6. Fix a document d. Let the supports of the γ and β variables be correct. Then, after a γ iteration, if γtd,i > γ t d,i′ , i 6= i′, γ∗d,i is the largest topic in the document. Furthermore, 12γ∗d,i ≤ γtd,i ≤ 2γ∗d,i.\nProof. Since there are a constant number of topics in the document, the largest topic has proportion Ω(1). Consider the KL-tEM convex optimization problem. The KKT conditions are easily seen to imply5:\nN ∑\nj=1\nf̃d,j f td,j βti,j = 1 (B.1)\nFor each topic i, since we are considering a constrained optimization problem, it has to be the case that it either satisfies B.1, γtd,i = 0 or γ t d,i = 1.\nLet’s assume first that i satisfies B.1. Then,\nγtd,i =\nN ∑\nj=1\nf̃d,j f td,j βti,jγ t d,i ≤ ∑\nj:β∗i,j 6=0\nf̃d,j\nLet’s call the words j, which only appear in the support of topic i in the document lone for that topic, and let’s denote that set as Li.\nIf Li are the lone words for topic i, ∑\nj /∈Li,β∗i,j 6=0 f̃d,j = To(1) = o(1), so\nγtd,i ≤ ∑\nj∈Li\n(1 + ǫ)β∗i,jγ ∗ d,i + o(1) ≤ (1 + ǫ)γ∗d,i + o(1) ≤ γ∗d,i + o(1)\nOn the other hand, γtd,i ≥ ∑ j∈Li β∗i,jγ ∗ d,i ≥ (1− ǫ)(1− o(1))γ∗d,i ≥ (1− o(1))γ∗d,i, so γtd,i ≥ γ∗d,i − o(1).\nSince there is a constant gap of ρ between the largest topic and the next largest one, the maximum γtd,i is indeed the largest topic in the document. Furthermore, since (1− o(1))γ∗d,i ≤ γtd,i ≤ (1 + o(1))γ∗d,i, clearly 1 2γ\n∗ d,i ≤ γtd,i ≤ 2γ∗d,i follows as well. On the other hand, we claim no topic which is in the support of a document d can actually have γtd,i = 0.\nIf this happens, it’s easy to see that ∑N j=1 f̃d,j log( f̃d,j ft d,j ) = ∞: one only needs to look at a summand corresponding to a lone word j for topic i. Just by virtue of the way lone words are defined, γtd,i = 0 would imply f td,j = 0. It’s clear that one can get a finite value for ∑\nj f̃d,j log( f̃d,j ftd,j ) on the other hand, by just\nsetting γtd,i = γ ∗ d,i, so γ t d,i = 0 cannot happen at an optimum.\nB.1.2 Lower bounds on the γtd,i and β t i,j variables\nNext, we show that subject to the thresholding being correct, at any point in time t, all the estimates γtd,i and βti,j are appropriately lower bounded.\nThe proof is similar for both the β and γ variables, and both for the KL-tEM and iterative tEM updates, but as mentioned before, we focus on the KL-tEM first.\nLemma 7. Fix a particular document d. Suppose that the supports of the γ and β variables are correct. Then, γtd,i ≥ (1 − o(1))γ∗d,i.\n5One gets these trivially, turning the constraint that ∑K\ni=1 γ t d,i = 1 into a Lagrange multiplier\nProof. Multiplying both sides of B.1 by γtd,i, we get\nγtd,i =\nN ∑\nj=1\nf̃d,j f td,j βti,jγ t d,i\nAs above, let’s split the above sum in two parts: lone words, and non-lone. Then clearly,\nγtd,i ≥ ∑\nj∈Li\n(1− ǫ)β∗i,jγ∗d,i\nFor notational convenience, let’s denote α̃ = ∑ j∈Li β∗i,j . Let’s estimate α̃. By the assumption on the size\nof the intersection of topics, ∑\nj /∈Li\nβ∗i,j ≤ Tr = o(1)\n. Hence, α̃ ≥ (1 − ǫ)(1− o(1)) = 1− o(1). So, the claim of the lemma holds.\nThe lower bound on the βti,j values proceeds similarly, but here we will crucially make use of the fact that for the large topics, we have both upper and lower bounds on the γtd,i values.\nLemma 8. Suppose that the supports of the γ and β variables are correct. Additionally, if i is a large topic in d, let 12γ ∗ d,i ≤ γtd,i ≤ 2γ∗d,i. Then, βt+1i,j ≥ 12 (1 − o(1))β∗i,j.\nProof. Let’s call lone the documents where β∗i′,j = 0 for all other topics i ′ 6= i appearing in that document for the topic-word pair (i, j). Let Dl be the set of lone documents. Then, certainly it’s true that\nβt+1i,j ≥ βti,j\n∑\nd∈Dl f̃d,j ftd,j γtd,i ∑D\nd=1 γ t d,i\nHowever, for a lone document, f td,j = γ t d,i · βti,j (it’s easy to check all the other terms in the summation for f td,j vanish, because either γ t d,i′ = 0 or β t i′,j = 0). Hence,\nβt+1i,j ≥ ∑ d∈Dl (1− ǫ) γ\n∗ d,iβ ∗ i,j\nγtd,i·β t i,j βti,jγ t d,i\n∑D d=1 γ t d,i\n= (1 − ǫ)β∗i,j ∑ d∈Dl γ∗d,i\n∑D d=1 γ t d,i\nHowever, since the update is happening only over documents where topic i is large, γtd,i ≤ 2γ∗d,i. So, we can conclude\nβt+1i,j ≥ (1− ǫ)β∗i,j 1\n2\n∑\nd∈Dl γ∗d,i\n∑D d=1 γ ∗ d,i\nLet’s call α = ∑ d∈Dl γ∗d,i ∑\nD d=1 γ ∗ d,i\n, and let’s analyze it’s value.\nBy Lemma 51 and Lemma 50,\n∑\nd∈Dl\nγ∗d,i ≥ (1− ǫ)|Dl|E[γ∗d,i|γ∗d,i is dominating, γ∗d,i′ = 0, ∀i′ 6= i s.t. j appears in topic i′]\nD ∑\nd=1\nγ∗d,i ≤ (1 + ǫ)|D|E[γ∗d,i|γ∗d,i is dominating]\nBy the weak topic correlations assumption, then,\n∑\nd∈Dl γ∗d,i\n∑D d=1 γ ∗ d,i\n≥ (1 − o(1)) |Dl||D| .\nFurthermore, by the independent topic inclusion property, each of the o(K) topics other than i that word j belongs to appears in a document with probability Θ(1/K), so the probability that a document which\ncontains topic i contains one of them is o(1), i.e. |Dl||D| . By Lemma 52, furthermore, |Dl| |D| ≥ 1 − o(1) when ǫ = o(1). Hence, α ≥ 1− o(1). Altogether, we get that βt+1i,j ≥ 12 (1 − o(1))β∗i,j as claimed.\nB.1.3 Upper bound on the βti,j values\nHaving established a lower bound on the βti,j variables throughout all iterations, together with the lower bounds on the γtd,i variables and the good estimates for the large topics, we will be able to prove the upper bound of the multiplicative error of βti,j keeps improving, until β t i,j ≤ Cββ∗i,j , for some constant Cβ .\nLemma 9. Let the β variables have the correct support, and βti,j ≥ 1Cmβ ∗ i,j, γ t d,i ≥ 1Cm γ ∗ d,i whenever β ∗ i,j 6= 0, γ∗d,i 6= 0. Let βti,j = Ctββ∗i,j , where Ctβ ≥ 4Cm, and Cm is a constant. Then, in the next iteration, βt+1i,j ≤ Ct+1β β ∗ i,j, where C t+1 β ≤ Ctβ 2 .\nProof. Without loss of generality, let’s assume Cm ≥ 2. (Since certainly, if the statement of the lemma holds with a smaller constant, it holds with Cm = 2.)\nWe proceed similarly as in the prior analyses. We will split the sum into the portion corresponding to the lone and non-lone documents.\nLet’s analyze the terms f̃d,j ftd,j γtd,i corresponding to the non-lone documents. Now, f td,j ≥ 1C2m f ∗ d,j, so f̃d,j ftd,j ≤ (1 + ǫ)C2m. Also, γtd,i ≤ 2γ∗d,i, since topic i is the dominant in document d.\nSince Cm ≥ 2, f̃d,jft d,j γtd,i ≤ (1 + ǫ)C3mγ∗d,i. Also, note that\n∑D d=1 γ t d,i ≥ 1Cm ∑D d=1 γ ∗ d,i, again, since i is the dominant topic.\nAs usual, let’s denote the set of lone documents Dl:\nβt+1i,j ≤ (1 + ǫ)Cm ∑ d∈Dl β∗i,jγ ∗ d,i + ∑ d∈D\\Dl C3mγ ∗ d,iβ t i,j\n∑D d=1 γ ∗ d,i\nAs in the prior proofs, let’s denote by α := ∑ d∈Dl γ∗d,i\n∑D d=1 γ ∗ d,i\n.\nAs in Lemma 8, α ≥ 1 − o(1), so βt+1i,j ≤ (1 + ǫ)Cm(αβ∗i,j + (1 − α)C3mβti,j), which in turn implies that βt+1i,j β∗i,j ≤ (1+ ǫ)Cm(α+(1−α)C3mCtβ). In order to ensure that βt+1i,j β∗i,j < Ctβ 2 , it would be sufficient to prove that\n(1 + ǫ)Cm(α+ (1− α)(C3mCtβ) < Ctβ 2\nwhich is equivalent to α > C3mC t β −\nCtβ 2(1+ǫ)Cm\nC3mC t β − 1\n.\nLet’s look at the right hand side. As, by assumption, Ctβ ≥ 4Cm, it follows that\nC3mC t β − Ctβ 2(1+ǫ)Cm\nC3mC t β − 1\n≤ C3mC t β −\nCtβ 2(1+ǫ)Cm\nC3mC t β − Ctβ 4Cm\nHence, the right hand side is upper bounded by\nC3m − 12(1+ǫ)Cm C3m − 14Cm = 1− 2 1+ǫ−1 4Cm\nC3m − 14Cm But, since Cm is bounded by a constant, and α = 1− o(1), the claim follows.\nB.1.4 Upper bounds on the γ values\nFinally, we show that if we ever reach a point where the β values are both upper and lower bounded by a constant, the γ values one gets after the γ step are appropriately upper bounded by a constant. More precisely:\nLemma 10. Fix a particular document d. Let’s assume the supports for the β and γ variables are correct. Furthermore, let 1Cm ≤ βti,j β∗i,j ≤ Cm for some constant Cm. Then, γtd,i ≤ (1 + o(1))γ∗d,i.\nProof. As in the proof of Lemma 7, let’s look at the KKT conditions for γtd,i into a part corresponding to lone words Li and non-lone words. Multiplying B.1 by γ t d,i as before,\nγtd,i = ∑\nj∈Li\nf̃d,j + γ t d,i\n∑\nj /∈Li\nf̃d,j f td,j βti,j\nAgain, let α̃ = ∑ j∈Li β∗i,j .\nBy Lemma 7, certainly γtd,i ≥ 1Cm γ ∗ d,i. Hence, f̃d,j f td,j ≤ (1 + ǫ)C2m. So we have, γtd,i ≤ (1 + ǫ)(α̃γ∗d,i + C3m(1− α̃)γtd,i). In other words, this implies γtd,i ≤ (1+ǫ)α̃1−(1+ǫ)C3m(1−α̃)γ ∗ d,i. Since α̃ = 1− o(1), it’s easy to check that α̃1−C3m(1−α̃) ≤ 1 + o(1), which is enough for what we need.\nSo, as a corollary, we finally get:\nCorollary 11. For some t0 = O(log( 1 β∗min ))) = O(logN) , it will be the case that for all t ≥ t0,\n1\nC0β ≤ β∗i,j βti,j ≤\nC0β for some constant C 0 β and\n1\nC0γ ≤ γ∗d,i γtd,i ≤ C0γ for some constant C0γ .\nThis concludes Phase I of the analysis.\nB.1.5 Phase II: Alternating minimization - upper and lower bound evolution\nTaking Corollary 11 into consideration, we finally show that, if the β and γ values are correct up to a constant multiplicative factor, and we have the correct support, we can improve the multiplicative error in each iteration, thus achieving convergence to the correct values.\nThis portion bears resemblance to techniques like state evolution and density evolution in the literature for iterative methods for decoding error correcting codes. In those techniques, one keeps track of a certain quantity of the system that’s evolving in each iteration. In density evolution, this is the probability density function of the messages that are being passed, in state evolution, it is a certain average and variance of the variables we are estimating.\nIn our case, we keep track of the ”multiplicative accuracy” of our estimates γtd,i, β t i,j . In particular, we\nwill keep track of quantities Ctγ and C t β , such that at iteration t, 1 Ctβ ≤ βi,j∗ βti,j ≤ Ctβ and 1Ctγ ≤ γd,i∗ γtd,i ≤ Ctγ after the corresponding γ iteration.\nWe will show that improvement in the quantities Ctβ causes a large enough improvement in the C t γ updates, so that after an alternating step of β and γ updates, Ct+1β ≤ (Ctβ)1/2. First, we show that when the β variables are estimated up to a constant multiplicative factor, the constant for the γ values after they’ve been iterated to convergence is slightly better than the constant for the β values. More precisely:\nLemma 12. Let’s assume that our current iterates βti,j satisfy 1 Ctβ ≤ βi,j∗βti,j ≤ C t β for C t β ≥ 1(1−ǫ)7 . Then, after iterating the γ updates to convergence, we will get values γtd,i that satisfy (C t β)\n1/3 ≤ γd,i∗ γtd,i ≤ (Ctβ)1/3.\nProof. As usual, we will split the KKT conditions for γt+1,t ′\nd,i into two parts: one for the lone, and one for the non-lone words. Let’s call the set of lone words Li, as previously. Then. we have\nγtd,i = ∑\nj∈Li\nf̃d,j + γ t d,i\n∑\nj /∈Li\nf̃d,j f td,j βti,j\nAgain, let α̃ = ∑ j∈Li β∗i,j = o(1), as we proved before. Let’s denote as Ctγ = maxi(max( γ∗d,i γtd,i , γtd,i γ∗ d,i )). We claim that it has to hold that Ctγ ≤ (Ctβ)1/3. Assume the contrary, and let i0 = argmaxi(max( γ∗d,i γtd,i , γtd,i γ∗d,i )). Let’s first assume that γtd,i0 γ∗d,i0 = Ctγ . By the definition of Ctγ ,\nγtd,i0 = ∑\nj∈Li0\nf̃d,j + γ t d,i0\n∑\nj /∈Li0\nf̃d,j f td,j βti0,j ≤ (1 + ǫ)(α̃γ∗d,i0 + (1− α̃)(Ctβ)2(Ctγ)2γ∗d,i0)\nWe claim that (1 + ǫ)(α̃+ (1− α̃)(Ctβ)2(Ctγ)2) ≤ (Ctγ)1/3 (B.2) which will be a contradiction to the definition of Ctγ . After a little rewriting, B.2 translates to α̃ ≥ 1− (Ctγ ) 1/3 1+ǫ −1\n(CtβC t γ)\n2−1 . By our assumption on Ctγ , C t β ≤ C3γ , so the\nright hand side above is upper bounded by 1− (Ctγ ) 1/3 1+ǫ −1\n(Ctγ) 8−1 .\nBut, Lemma 10 implies that certainly Ctγ ≤ C0γ , where C0γ is some absolute constant. The function\nf(c) = c1/3 1+ǫ − 1 c8 − 1\ncan be easily seen to be monotonically decreasing on the interval of interest, and hence is lower bounded by (C0γ ) 1/3\n1+ǫ −1\n(C0γ) 8−1 , which is in terms some absolute constant smaller than one. Since α̃ = 1−o(1). the claim we want\nis clearly true.\nThe case where γ∗d,i0 γtd,i0 = Ctγ is similar. In this case,\nγtd,i0 = ∑\nj∈Li0\nf̃d,j + γ t d,i0\n∑\nj /∈Li0\nf̃d, j\nf td,j βti0,j ≥ (1 − ǫ)(α̃γ∗d,i0 + (1− α̃)\n1\n(Ctβ) 2(Ctγ)\n2 γ∗d,i0)\nWe then claim that\n(1− ǫ)(α̃+ (1− α̃) 1 (Ctβ) 2(Ctγ) 2 ) ≥ 1 (Ctγ) 1/3\n(B.3)\nAgain, B.3 rewrites to:\nα̃ ≥ 1 (1−ǫ)(Ctγ) 1/3 − 1(Ctβ)2(Ctγ)2\n1− 1(Ct β )2(Ctγ) 2\n= 1− 1− 1 (1−ǫ)(Ctγ) 1/3\n1− 1(Ct β Ctγ) 2\nAgain, the right hand side above is upper bounded by 1 − 1− 1 (1−ǫ)(Ctγ ) 1/3\n1− 1 (Ctγ )\n8 . But Cγ ∈ [1, C0γ ], and the\nfunction 1− 1 (1−ǫ)c1/3\n1− 1 c8\nis monotonically increasing, so lower bounded by\n1− 1 (1−ǫ)( 1\n(1−ǫ)7 )1/3\n1− 1 ( 1 (1−ǫ)7 )8\n= 1− (1 − ǫ)4/3 1− (1 − ǫ)56 ≥ 1 42\nHence, 1− 1− 1 (1−ǫ)(Ctγ ) 1/3\n1− 1 (Ctγ )\n32 is upper bounded by 4142 . Again, our bound on α̃ gives us what we want.\nLemma 13. Let’s assume that our current iterates βti,j satisfy 1 Ctβ ≤ βi,j∗ βti,j ≤ Ctβ , Ctβ ≥ 1(1−ǫ)7 , and after the corresponding γ update, we get 1Ctγ ≤ γd,i∗γtd,i ≤ C t γ , where C t β ≥ (Ctγ)3. Then, after one β step, we will get new values βt+1i,j that satisfy 1\nCt+1β ≤ βi,j∗ βt+1i,j ≤ Ct+1β where Ct+1β = (Ctβ)1/2.\nProof. The proof proceeds in complete analogy with Lemmas 8 and 9. Again, let’s tackle the lower and upper bound separately. The upper bound condition is:\nα > (CtβC t γ)\n2 − (C t β) 1/2\n(1+ǫ)Ctγ\n(CtγC t β) 2 − 1\nUsing Ctβ ≥ (Ctγ)3, we can upper bound the expression on the right by 1 − (Ctβ) 1/6 1+ǫ − 1 (Ctβ) 8/3 − 1 . The function\nf(c) = x1/6 1+ǫ −1\nx8/3−1 is monotonically decreasing on the interval [1, C0β] of interest, so because α = 1− o(1), we get\nwhat we want. Similarly, for the lower bound, we want that\nα >\nCtγ (Ctβ) 1/2(1−ǫ) − 1 (CtγC t β) 2\n1− 1(Ct β Ctγ) 2\nYet again, using Ctβ ≥ (Ctγ)3, we get that the right hand side is upper bounded by\n1− 1− 1 (1−ǫ)C 1/6 β\n1− 1 C3β\nHowever, the function f(c) = 1− 1 (1−ǫ)c1/6\n1− 1 c8/3\nis monotonically increasing on the interval [1, C0β], so lower bounded\nby 1− 1 (1−ǫ)( 1 (1−ǫ)7 )1/6\n1− 1 ( 1 (1−ǫ)7 )8/3\n= 1−(1−ǫ) 1/6 1−(1−ǫ)21 ≥ 1126 . Hence, 1 − 1− 1 (1−ǫ)C 1/6 β\n1− 1 C3 β\nis upper bounded by 125126 , so using the fact\nthat α = 1− o(1), we get what we want.\nPutting lemmas 12 and 13 together, we get:\nLemma 14. Suppose it holds that 1Ct ≤ βi,j∗ βti,j ≤ Ct, Ct ≥ 1(1−ǫ)7 . Then, after one KL minimization step with respect to the γ variables and one β iteration, we get new values βt+1i,j that satisfy 1 Ct+1 ≤ βi,j∗\nβt+1i,j ≤ Ct+1,\nwhere Ct+1 = √ Ct\nProof. By Lemma 12, after the γ iterations, we get γtd,i values that satisfy the condition 1 (C′)t ≤ γd,i∗ γt d,i ≤ (C′)t, where (C′)t = (Ct)1/3.\nThen, by Lemma 13, after the γ iteration, we will get 1Ct+1 ≤ βi,j∗ βt+1i,j ≤ Ct+1, such that Ct+1 = (Ct)1/2,\nwhich is what we need.\nHence, as a corollary, we get immediately:\nCorollary 15. Lemma 14 above implies that Phase III requires O(log( 1log(1+ǫ′) )) = O(log( 1 ǫ′ )) iterations to estimate each of the topic-word matrix and document proportion entries to within a multiplicative factor of 1 + ǫ′.\nThis finished the proof of Theorem 1 for the KL-tEM version of the updates. In the next section, we will remark on why the proofs are almost identical in the iterative and incomplete tEM version of the updates.\nB.2 Iterative tEM updates, incomplete tEM updates\nWe show how to modify the proofs to show that the iterative tEM and incomplete tEM updates work as well. We’ll just sketch the arguments as they are almost identical as above.\nIn those updates, when we are performing a γ update, we initialize with γtd,i = 0 whenever topic i does not belong to document d, and γtd,i uniform among all the other topics. Then, the way to modify Lemmas 7, 10, 12 is simple. Instead of arguing by contradiction about what happens at the KKT conditions, one will assume that at iteration t′ (t′ to indicate these are the separate iterations for the γ variables that converge to the values γtd,i) it holds that\n1 Ct′γ γ∗d,i ≤ γt ′ d,i ≤ Ct ′ γ γ ∗ d,i. Then, as\nlong as Ct ′ γ is too big, compared to C t β , one can show that C t′ γ is decreasing (to C t′+1 γ = (C t γ) 1/2, say), using exactly the same argument we had before. Furthermore, the number of such iterations needed will clearly be logarithmic.\nBut the same argument as above proves the incomplete tEM updates work as well. Namely, even if we perform only one update of the γ variables, they are guaranteed to improve.\nB.3 Initialization\nFor completeness, we also give here a fairly easy, efficient initialization algorithm. Recall, the goal of this phase is to recover the supports - i.e. to find out which topics are present in a document, and identify the support of each topic. To reiterate the theorem statement: Theorem 2. If the number of documents is Ω(K4 log2 K), there is a polynomial-time procedure which with probability 1− Ω( 1K ) correctly identifies the supports of the β∗i,j and γ∗d,i variables.\nWe will find the topic supports first. Roughly speaking, we will devise a test, which will take as input two documents d, d′, and will try to determine if the two documents have a topic in common or not. The test will have no false positives, i.e. will never say NO, if the documents do have a topic in common, but might say NO even if they do. We will then, ensure that with high probability, for each topic we find a pair of documents intersecting in that topic, such that the test says YES.\nWe will also be able to identify which pairs intersect in exactly one topic, and from this we will be able to find all the topic supports. Having done all of this, finding the topics in each document will be easy as well. Roughly speaking, if a document doesn’t contain a given topic, it will not contain all of the discriminative words in that document.\nWe give the algorithm formally as pseudocode Algorithm 4. Now, let’s proceed to analyze the above algorithm, proceeding in a few parts.\nB.3.1 Constructing a no-false-positives test\nFirst, we describe how one determines the supports of the topics. Let’s define Test(d, d′) = YES, if ∑\nj min{f∗d,j, f∗d′,j} ≥ 12T , and NO otherwise. Then, we claim the following. Lemma 16. If d, d′ both contain a topic i0, s.t. γ\n∗ d,i0 ≥ 1/T , γ∗d′,i0 ≥ 1/T then Test(d, d′) = YES. If d, d′ do not contain a topic i0 in common, then Test(d, d ′) = NO.\nProof. Let’s prove the first claim. ∑\nj\nmin{f̃d,j, f̃d′,j} ≥ ∑\nj\n(1− ǫ)min{β∗i0,jγ∗d,i0 , β∗i0,jγ∗d′,i0} ≥\n∑\nj\n(1 − ǫ)1/Tβ∗i0,j ≥ 1/2T\nAlgorithm 4 Initialization\nrepeatK4 log2 K times Sample a pair of documents (d, d′). ⊲Test if (d, d′) intersect with no false positives: if ∑\nj min{f∗d,j, f∗d′,j} ≥ 12T then Sd,d′ := {j, s.t.f∗d,j, f∗d′,j > 0} ⊲”Weed-out” words that are not in the support of the intersection of (d,d’) for all documents d′′ 6= {d, d′} do\nif ∑ j min{f∗d,j, f∗d′′,j} ≥ 12T and ∑ j min{f∗d′,j , f∗d′′,j} ≥ 12T then Sd,d′ = Sd,d′ ∩ j, s.t f∗d′′,j > 0 end if\nend for\nend if\nuntil ⊲Determine which Sa,b correspond to documents intersecting in one topic only) if Set Sa,b appears less than D/K\n2.5 times, where D is the total number of documents then Remove Sa,b.\nend if if Set Sa,b can be written as the union of two other sets Sc,d, Se,f , where neither is contained inside the other then\nRemove Sa,b. end if if Set Sa,b is strictly contained inside Sd,d′ for some Sd,d′ then Remove Sd,d′. end if Remove duplicates. The remaining lists Sa,b are declared to be topic supports.\nNow, let’s prove the second claim. Let’s suppose d, d′ contain no topic in common. Let’s fix a topic i0 that belongs to document d. By the ”small discriminative words intersection”, we\nhave the following property: ∑\nj∈i0,j∈i′\nβ∗i,j = o(1)\nfor any other topic i′ 6= i0. Denoting by Toutside the words belonging to topic i0, and no topic in document d\n′, and Tinside the words belonging to at least one other topic in d′, we have\n∑\nj∈Tinside\nβ∗i,j ≤ T · o(1) = o(1)\nFor the words j ∈ Toutside, min{f∗d,j, f∗d′,j} = 0 By the above,\n∑\nj\nmin{f̃d,j, f̃d′,j} ≤ (1 + ǫ)T 2o(1) = o(1)\nThus, the test will say NO, as we wanted.\nB.3.2 Finding the topic supports from identifying pairs\nLet’s call d, d′ an identifying pair of documents for topic i, if d, d′ intersect in topic i only, and furthermore the test says YES on that pair.\nFrom this identifying pair, we show how to find the support of the topic i in the intersection. What we’d like to do is just declare the words j, s.t. f∗d,j, f ∗ d′,j are both non-zero as the support of topic i. Unfortunately, this doesn’t quite work. The reason is that one might find words j, s.t. they belong to one topic i′ in d, and another topic i′′ in d′′. Fortunately, this is easy to remedy. As per the pseudo-code above, let’s call the following operation WEEDOUT (d, d′):\n• Set S = {j, s.t.f∗d,j > 0, f∗d′,j > 0}.\n• For all d′′, s.t. Test(d, d′′) = Y ES, Test(d′, d′′) = Y ES:\n• Set S = S ∪ {j, s.t.f∗d′′,j > 0}\n• Return S.\nLemma 17. With probability 1−Ω( 1K ), for any pair of documents d, d′ intersecting in one topic, WEEDOUT (d, d′) is the support of S.\nProof. For this, we prove two things. First, it’s clear that S is initialized in the first line in a way that ensures that it contains all words in the support of topic i. Furthermore, it’s clear that at no point in time we will remove a word j from S that is in the support of topic i. Indeed - if Test(d, d′′) = Y ES and Test(d′, d′′) = Y ES, then by Lemma 16 document d′′ must contain topic i. In this case, f∗d′′,j > 0, and we won’t exclude j from S.\nSo, we only need to show that the words that are not in the support of topic i will get removed. Let d, d′ intersect in a topic i. Let a word j be outside the support of a given topic i. Because of the independent topic inclusion property, the probability that a document d′′ contains topic i, and no other topic containing j is Ω(1/K).\nSince the number of documents is Ω(K4 log2 K), by Chernoff, the probability that there is a document d′′, s.t. Test(d, d′′) = Y ES, Test(d′, d′′) = Y ES, but f∗d′′,j = 0, is 1 − Ω( 1eK2 log2 K ). Union bounding over all words j, as well as pairs of documents d, d′, we get that for any documents d, d′ intersection in a topic i, we get the claim we want.\nB.3.3 Finding the identifying pairs\nFinally, we show how to actually find the identifying pairs. The main issue we need to handle are documents that do intersect, and the TEST returns yes, but they intersect in more than one topic. There’s two ingredients to ensuring this is true in the above algorithm.\n• First, we delete all sets in the list of sets Sa,b that show up less than D2/K2.5 number of times.\n• Second, we remove sets that can be written as the union of two other sets Sc,d, Se,f , where neither of the two is contained inside the other.\n• After this, we delete the non-maximal sets in the list.\nThe following lemma holds:\nLemma 18. Each topic has Ω(D2/K2) identifying pairs with probability 1− Ω( 1K ).\nProof. Let Ii be the event that there are at least Ω(D2/k2) identifying pairs for topic i. Let Ni be a random variable denoting the number of documents which have topic i as a dominating topic. Furthermore, let Mi be the event that there are at least N 2 i 2 − K √\nN2i identifying pairs among the Ni ones that have i as a dominating topic. By the dominant topic equidistribution property, probability that a document d has a topic i as a dominating topic is at least C/K for some constant C. Then, clearly,\nPr[∩Ki=1Ii] ≥ Pr [ ∩Ki=1 ( Ni ≥ 1 2 C D K )] Pr [ ∩Ki=1Mi| ∩Ki=1 ( Ni ≥ 1 2 C D K )]\nLet’s estimate Pr [ ∩Ki=1 ( Ni ≥ 12C DK )]\nfirst. The probabilities that different documents have i0 as the dominating topic are clearly independent, so by Chernoff, if Ni is the number of documents where i is the dominating topic,\nPr[Ni ≥ (1− ǫ)C D K ] ≥ 1− e− ǫ 2 3 C D K\nSince D = Ω(K2), plugging in ǫ = 12 , Pr[Ni < 1 2C D K ] ≥ 1 − e−Ω(K). Union bounding over all topics, we get that with probability Pr [\n∩Ki=1 ( Ni ≥ 12C DK )] ≥ 1− 1K . Now, let’s consider Pr [\n∩Ki=1Mi| ∩Ki=1 ( Ni ≥ 12C DK )] . The event ∩Ki=1 ( Ni ≥ 12C DK )\ncan be written as the disjoint union of events {D = ∪Ki=1Di, ∀i 6= j,Di ∩Dj = ∅} where D is the set of all documents, Di is the set of documents that have i as the dominating topic, and |Di| ≥ 12C DK , ∀i. (i.e. all the partitions of D into K sets of sufficiently large size). Evidently, if we prove a lower bound on Pr [\n∩Ki=1Mi|E ] for any such event E, it will imply a lower bound on\nPr [ ∩Ki=1Mi| ∩Ki=1 ( Ni ≥ 12C DK )] . For any such event, consider two documents d, d′ ∈ {Di}, i.e. having i as the dominating topic. Let Id,d′ be an indicator variable denoting the event that d, d′ do not intersect in an additional topic. Pr[Id,d′ = 1] = 1− o(1), by the independent topic inclusion property and the events Id,d′ are easily seen to be pairwise independent. Furthermore, Var[Id,d′ ] = o(1). By Chebyshev’s inequality,\nPr\n\n\n∑\nd,d′∈Di\nId,d′ ≥ 1\n2 D2i − c\n√\nD2i\n\n ≥ 1− 1 c2\nIf Ni = Ω(K logK), plugging in c = K, we get that Pr\n\n\n∑\nd,d′∈Di\nId,d′ = Ω(D2i )\n\n ≥ 1 − Ω( 1 K2 ). Hence,\nPr [ ∩Ki=1Mi|E ] ≥ 1− 1K , by a union bound, which implies Pr [ ∩Ki=1Mi| ∩Ki=1 ( Ni ≥ 12C DK )] ≥ 1− 1K . Putting all of the above together, if D = Ω(K2 logK), with probability 1 − Ω( 1K ), all topics have Ω(D2/K2) identifying pairs, which is what we want.\nThe lemma implies that with probability 1−Ω( 1K ), we will not eliminate the sets Sa,b corresponding to topic supports.\nWe introduce the following concept of a ”configuration”. A set of words C will be called a ”configuration” if it can be constructed as the intersection of the discriminative words in some set of topics, i.e.\nDefinition. A set of words C is called a configuration if there exists a set I = {I1, . . . , I|I|} of topics, s.t.\nC = ∩|I|i=1WIi Let’s call the minimal size of a set I that can produce C the generator size of C.\nNow, we claim the following fact:\nLemma 19. If a configuration C has generator size ≥ 3, then with probability 1 − Ω( 1K ), it cannot appear as one of the sets Sa,b after step 2 in the WEEDOUT procedure.\nProof. Since C has generator size at least 3, if two sets d, d′ intersect in less than two topics, then step 1 in WEEDOUT cannot produce Sa,b which is equal to C. Hence, prior to step 2, C can only appear as Sd,d′ for d, d′ that intersect in at least 3 topics.\nLet Id,d′ be an indicator variable denoting the fact that the pair of documents d, d′ intersects in at least 3 topics. We have Pr[Id,d′ = 1] ≤ 1/K3 + 1/K4 + . . . 1/KT = O(1/K3) by the independent topic inclusion property.\nIf I3 is a variable denoting the total number of documents that intersect in at least 3 topics, again by Chebyshev as in Lemma 18 we get:\nPr[I3 ≥ Θ(D/K3)− cΘ( √ D/K3/2)] ≥ 1− 1\nc2\nAgain, by putting c = √ K, since the number of documents is K4 log2 K, with probability 1 − 1K , all\nconfigurations with generator size ≥ 3 cannot appear as one of the sets Sa,b, as we wanted.\nThis means that after the WEEDOUT step, with probability 1 − Ω( 1K ), we will just have sets Sa,b corresponding to configurations generated by two topics or less. The options for these are severely limited: they have to be either a topic support, the union of two topic supports, or the intersection of two topic supports. We can handle this case fairly easily, as proven in the following lemma:\nLemma 20. After the end of step 3, with probability 1 − Ω( 1K ), the only remaining Sa,b are those corresponding to topic supports.\nProof. First, when we check if some Sd,d′ is the union of two other sets and delete it if yes, I claim we will delete the sets equal to configurations that correspond to unions of two topic supports (and nothing else). This is not that difficult to see: certainly the sets that do correspond to configurations of this type will get deleted.\nOn the other hand, if it’s the case that Sa,b corresponds to a single topic support, we won’t be able to write it as the union of two sets Sd,d′, Sd′′,d′′′ , unless one is contained inside the other - this is ensured by the existence of discriminative words.\nHence, after the first two passes, we will only be left with sets that are either topic supports, or intersections of two topic supports. Then, removing the non-maximal is easily seen to remove the sets that are intersections, again due to the existence of discriminative words.\nB.3.4 Finding the document supports\nNow, given the supports of each topic, for each document, we want to determine the topics which are non-zero in it. The algorithm is given in 5:\nLemma 21. If a topic i0 is such that γ ∗ d,i0 > 0, it will be declared as ”IN”. If a topic i0 is such that γ ∗ d,i0 = 0, it will be declared as out.\nAlgorithm 5 Finding document supports\nInitialize R = ∅. for each i do\nCompute Score(i) = ∑\nj∈Support(i)\\R f̃d,j end for Find i∗ such that Score(i∗) is maximum. while Score(i∗) > 0 do\nOutput i∗ to be in the support of d. R = R ∪ support(i∗) Recompute Score for every other topic. Find i∗ with maximum score.\nend while\nProof. Consider a topic i. At any iteration of the while cycle, consider ∑ j∈Support(i)\\R f̃d,j. Clearly, f̃d,j ≥ (1− ǫ)γ∗d,iβ∗i,j . Also ∑ j∈R β ∗ i,j = To(1). Hence,\n∑\nj∈Support(i)\\R\nf̃d,j ≥ (1 − ǫ)γ∗d,i(1− To(1)) ≥ 1\n2 γ∗d,i\nSo, topic i will be added eventually. On the other hand, let’s assume the document doesn’t contain a given topic i0. Let’s call B the set of words j which are in the support of i0, and belong to at least one of the topics in document d. Then, ∑\nj∈i0 f̃d,j =\n∑\nj∈B f̃d,j. Let i ∗ be the topic which is present in the document but not added yet and has\nmaximum value of γ∗d,i. Then ∑\nj∈B\nf̃d,j ≤ (1 + ǫ) ∑\ni∈d\n∑\nj∈B\nγ∗d,iβ ∗ i,j ≤\n(1 + ǫ)γ∗d,i∗ ∑\ni∈d\n∑\nj∈B\nβ∗i,j ≤\n(1 + ǫ)Tγ∗d,i∗o(1) ≤ γ∗d,i∗ ] · o(1) Hence, topic i∗ will always get preference over i0. Once all the topics which are present in the document\nhave been added, it is clear that no more topic will be added since score will be 0.\nThis finally finishes the proof of Theorem 2."
    }, {
      "heading" : "C Case study 2: Dominating topics, seeded initialization",
      "text" : "As a reminder, seeded initialization does the following:\n• For each topic i, the user supplies a document d, in which γ∗d,i ≥ Cl.\n• We initialize with β0i,j = f∗d,j. The theorem we want to show is:\nTheorem 3. Given an instance of topic modelling satisfying the Case Study 2 properties specified above, where the number of documents is Ω(K log 2 N\nǫ′2 ), if we initialize with seeded initialization, after O(log(1/ǫ ′) + logN)\nof KL-tEM updates, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1+ǫ′.\nThe proof will be in a few phases again:\n• Phase I: Anchor identification: First, we will show that as long as we can identify the dominating topic in each of the documents, the anchor words will make progress, in the sense that after O(logN) number of rounds, the values for the topic-word estimates will be almost zero for the topics for which the word is not an anchor, and lower bounded for the one for which it is.\n• Phase II: Discriminative word identification: Next, we show that as long as we can identify the dominating topics in each of the documents, and the anchor words were properly identified in the previous phase, the values of the topic-word matrix for words which do not belong to a certain topic will keep dropping until they reach almost zero, while being lower bounded for the words that do.\n• For Phase I and II above, we will need to show that the dominating topic can be identified at any step. Here we’ll leverage the fact that the dominating topic is sufficiently large, as well as the fact that the anchor words have quite a large weight.\n• Phase III: Alternating minimization: Finally, we show that after Phase I and II above, we are back to the scenario of the previous section: namely, there is a ”boosting” type of improvement in each next round.\nC.1 Estimates on the dominating topic\nBefore diving into the specifics of the phases above, we will show what the conditions we need are to be able to identify the dominating topic in each of the documents. For notational convenience, let ∆m be the m-dimensional simplex: x ∈ ∆m iff ∀i ∈ [m], 0 ≤ xi ≤ 1 and ∑ i xi = 1.\nFirst, during a γ update, we are minimizing KL(f̃d||fd) with respect to the γd variables, so we need some way or arguing that whenever the β estimates are not too bad, minimizing this quantity also quantifies how far the γd variables are from γ ∗ d .\nFormally, we’ll show the following:\nLemma 22. If, for all i, KL(β∗i ||βti ) ≤ Rβ, and minγd∈∆K KL(f̃d||fd) ≤ Rf , after running a KL divergence minimization step with respect to the γd variables, we get that ||γ∗d − γd||1 ≤ 1p ( √ 1 2Rβ + √ 1 2Rf ) + ǫ.\nWe will start with the following simple helper claim:\nLemma 23. If the word-topic matrix β is such that in each topic the anchor words have total probability at least p, then ||β∗v||1 ≥ p||v||1.\nProof.\n||β∗v||1 = ∑\nj\n| ∑\ni\nβ∗i,jvi| ≥ ∑\ni\n∑\nj∈Wi\n|β∗i,jvi| ≥ ∑\ni\np|vi| ≥ p||v||1\nLemma 24. If, for all i, KL(β∗i ||βti ) ≤ Rβ, and minγd∈∆K KL(f̃d||fd) ≤ Rf , after running a KL divergence minimization step with respect to the γd variables, we get that ||γ∗d − γd||1 ≤ 1p ( √ 1 2Rβ + √ 1 2Rf ) + ǫ.\nProof. First, observe that minγd∈∆K KL(f̃d||fd) ≤ Rf , at the the optimal γd, we have that ||f̃d−fd||21 ≤ 12Rf , i.e. ||f̃d − fd|| ≤ √ 1 2Rf , by Pinsker’s inequality.\nWe will show that if ||γ∗d − γd||1 is large, so must be ||f̃d − fd||1, and hence KL(f̃d||fd) - which will contradict the above upper bound.\nLet’s consider β∗ as N by K matrix, and γ∗ and f∗ as K-dimensional vectors. Let β∗γ∗ just denote matrix-vector multiplication - so f∗ = β∗γ∗. For any other vector γ̂, let’s denote f̂ = βtγ̃. Then:\n||f̃ − f̂ ||1 = ||f̃ − βtγ̃||1 = ||f̃ − (β∗ + (βt − β∗))γ̃||1 ≥\n||f̃ − β∗γ̃||1 − ||(βt − β∗)γ̃||1 (C.1) Hence, ||f̃ − β∗γ̃||1 ≤ ||(βt − β∗)γ̃||1 + ||f̃ − f̂ ||1. However, However,\n||(βt − β∗)γ||1 ≤ max i\n∑\nj\n|βti,j − β∗i,j | ≤ max i\n√\n1 2 KL(β∗i ||βti ) ≤\n√\n1 2 Rβ (C.2)\nThe first inequality is a property of induced matrix norms, the second is via Pinsker’s inequality. So, by C.1 and C.2, ||f̃−β∗γ̃||1 ≤ √ 1 2Rβ+ √ 1 2Rf . But now, finally, Lemma 23 implies that ||γ∗d−γd||1 ≤\n1 p ( √ 1 2Rf + √ 1 2Rβ) + ǫ.\nLemma 25. Suppose that for the dominating topic i in a document d, γ∗d,i ≥ Cl, and for all other topics i′, γ∗d,i′ ≤ Cs, s.t. Cl − Cs > 1p ( √ 1 2Rf + √ 1 2Rβ) + ǫ. Then, the above test identifies the largest topic. Furthermore, 12γ ∗ d,i ≤ γtd,i ≤ 32γ∗d,i\nProof. By Lemma 24, and the relationship between l1 and total variation distance between distributions, we have that |γtd,i − γ∗d,i| ≤ 12 ( 1 p ( √ 1 2Rf + √ 1 2Rβ ) + ǫ ) .\nFor the dominating topic i, γtd,i ≥ Cl − 12 ( 1 p\n( √\n1 2Rf +\n√\n1 2Rβ\n) + ǫ ) . On the other hand, for any other\ntopic i′, γtd,i′ ≤ Cs + 12 ( 1 p\n(√\n1 2Rf +\n√\n1 2Rβ\n) + ǫ ) . Since Cl −Cs ≥ 1p (√ 1 2Rf + √ 1 2Rβ ) + ǫ, γtd,i > γ t d,i′ , so\nthe test works.\nOn the other hand, since γtd,i ≥ γ∗d,i − ( 1 p\n( √\n1 2Rf +\n√\n1 2Rβ\n) + ǫ )\n≥ γ∗d,i − 12γ∗d,i = 12γ∗d,i. Similarly,\nγtd,i ≤ γ∗d,i + 1p ( √ 1 2Rf + √ 1 2Rβ ) + ǫ ≤ γ∗d,i + 12γ∗d,i = 32γ∗d,i.\nC.2 Phase I: Determining the anchor words\nWe proceed as outlined. In this section we show that in the first phase of the algorithm, the anchor words will be identified - by this we mean that we will be able to show that if a word j is an anchor for topic i, βti,j will be within a factor of roughly 2 from β ∗ i,j , and β t i′,j will be almost 0 for any other topic i\n′. We will assume throughout this and the next section that we can identify what the dominating topic is, and that we have an estimate of the proportion of the dominating topic to within a factor of 2. (We won’t restate this assumption in all the lemmas in favor of readability.)\nWe will return to this issue after we’ve proven the claims of Phases I and II modulo this claim. The outline is the following. We show that at any point in time, by virtue of the initialization, βti,j is pretty well lower bounded (more precisely it’s at least constant times β∗i,j). This enables us to show that βti′,j will halve at each iteration - so in some polynomial number of iterations will be basically 0.\nC.2.1 Lower bounds on the βti,j values\nWe proceed as outlined above. We show here that the βti,j variables are lower bounded at any point in time. More precisely, we show the following lemma:\nLemma 26. Let j be an anchor word for topic i, and let i′ 6= i. Suppose that βti′,j ≤ βti,j. Then, βt+1i,j ≥ (1− ǫ)Clβ∗i,j holds.\nProof. We’ll prove a lower bound on each of the terms f̃d,j ftd,j βti,j . Since the update on the β variables is a convex combination of terms of this type, this will imply a lower bound on βt+1i,j .\nFor this, we upper bound f td,j. We have:\nf td,j = β t i,jγ t d,i +\n∑\ni′ 6=i\nβti′,jγ t d,i′\nThis means that f td,j is a convex combination of terms, each of which is at most β t i,j . Hence, f t d,j ≤ βti,j\nholds. But then f̃d,j ftd,j βti,j ≥ f̃d,j ≥ (1 − ǫ)β∗i,jγ∗d,i ≥ (1 − ǫ)Clβ∗i,j . This implies βt+1i,j ≥ (1 − ǫ)Clβ∗i,j , as we wanted.\nC.2.2 Decreasing βti′,j values\nWe’ll bootstrap to the above result. Namely, we’ll prove that whenever βti,j ≥ 1/Cββ∗i,j for some constant Cβ , the βti′,j values decrease multiplicatively at each round. Prior to doing that, the following lemma is useful. It will state that whenever the values of the variables βti′,j are somewhat small, we can get some reasonable lower bound on the values γtd,i we get after a step of KL minimization with respect to the γ variables. Lemma 27. Let j be an anchor for topic i, and let i′ 6= i. Let βti′,j ≤ bβti,j. Then, for any document d, when performing KL divergence minimization with respect to the variables γd, for the optimum value γ t d,i, it holds that γtd,i ≥ (1 − ǫ) p1−bγ∗d,i − b1−b .\nProof. The KKT conditions B.1 imply that if we denote Ai the set of anchors in topic i, ∑\nj∈Ai f̃d,j ftd,j βti,j ≤ 1. By the assumption of the lemma,\nf td,j ≤ bti,jγtd,i + bbti,j(1− γtd,i)\nSince f̃d,j ≥ (1− ǫ)β∗i,jγ∗d,i, this implies f̃d,j ft d,j βti,j ≥ (1− ǫ)β∗i,j γ∗d,i γt d,i (1−b)+b , i.e. ∑ j∈Ai (1− ǫ)β∗i,j γ∗d,i γt d,i (1−b)+b ≤ 1. Rearranging the terms, we get\nγtd,i ≥ (1 − ǫ) ∑\nj∈Ai\nβ∗i,j γ∗d,i 1− b − b 1− b ≥ (1 − ǫ)pγ ∗ d,i − b 1− b\nas we needed.\nWith this in place, we show that the value βti′,j when j is an anchor for topic i 6= i′, decreases by a factor of 2 after the update for the β variables.\nThis requires one more new idea. Intuitively, if we view the update as setting βt+1i′,j to β t i,j multiplied by\na convex combination of terms f∗d,j ft d,j , a large number of them will be zero, just because f∗d,j = 0 unless topic i belongs to document d. By the topic equidistribution property then, the probability that this happens is only O(1/K), so if the weight in the convex combination on these terms is reasonable, we will multiply βti,j by something less than 1, which is what we need.\nLemma 27 says that if γ∗d,i is reasonably large, we will estimate it somewhat decently. If γ ∗ d,i is small,\nthen f∗d,j would be small anyway. So we proceed according to this idea.\nLemma 28. Let j be an anchor for topic i. Let βti′,j ≤ bβti,j for i′ 6= i, and let βti,j ≥ 1/Cββ∗i,j for some constant Cβ. Then, β t+1 i′,j ≤ b/2β∗i,j\nProof. We will split the β update as\nβt+1i′,j = β t i′,j(\n∑\nd∈D1 f̃d,j ftd,j γtd,i′ ∑\nd γ t d,i′\n+\n∑\nd∈D2 f̃d,j ftd,j γtd,i′ ∑\nd γ t d,i′\n+\n∑\nd∈D3 f̃d,j ftd,j γtd,i′ ∑\nd γ t d,i′\n)\nfor some appropriately chosen partition of the documents into three groups D1, D2, D3. Let D1 be documents which do not contain topic i at all, D2 documents which do contain topic i, and γ∗d,i ≥ 2bp , and D3 documents which do contain topic i and γ∗d,i < 2bp . The first part will just vanish because word j is an anchord word for topic i, and topic i does not appear in it, so f∗d,j = 0 for all documents d ∈ D1. The second summand we will upper bound as follows. First, we upper bound\nf̃d,j ftd,j . We have that\nf td,j ≥ βti,jγtd,i ≥ 1/Cββ∗i,jγtd,i. However, we can use Lemma 27 to lower bound γtd,i. We have that γtd,i ≥ (1− ǫ)( p1−bγ∗d,i − b1−b ) ≥ (1− ǫ) p 2(1−b)γ ∗ d,i. This alltogether implies f̃d,j ftd,j ≤ 11−ǫ 2(1−b)Cβ p . Hence,\nβti′,j\n∑\nd∈D2 f̃d,j ftd,j γtd,i′ ∑\nd γ t d,i′\n≤ 1 1− ǫ 2Cβ p\n(1 − b)βti′,j ∑ d∈D2 γtd,i′ ∑\nd γ t d,i′\nFurthermore, ∑ d γ t d,i′ ≥ 12 |D|Cl. On the other hand, I claim ∑ d∈D2 γtd,i′ = O(K/|D|). Recall that D is the set of documents where topic i′ is the dominating topic - so by definition they contain topic i. On the other hand, if a document is in D2 then it contains topic i as well. However, by the independent topic inclusion property, the probability that a document with dominating topic i′ contains topic i as well is O(1/K). Hence,\nβti′,j\n∑\nd∈D2 f̃d,j ftd,j γtd,i′ ∑\nd γ t d,i′\n= O( 1\nK )bβti,j\nFor the third summand we provide a trivial bound for the terms f̃d,j ftd,j βti′,jγ t d,i′ :\nf̃d,j f td,j βti′,jγ t d,i′ ≤ (1 + ǫ)β∗i,jγ∗d,i ≤ (1 + ǫ)β∗i,j 2b p\nSince again, ∑ d γ t d,i′ ≥ 12 |D|Cl, and again, the number of document in D3 is at most O(1/K) for the same reasons as before, we have that\nβti′,j\n∑\nd∈D3 f∗d,j ftd,j\nγtd,i′ ∑\nd γ t d,i′\n≤ O(1/K)bβ∗i,j = O(1/K)bβti,j\nsince βti,j ≥ 1Cβ β ∗ i,j .\nFrom the above three bounds, we get that βt+1i′,j ≤ O(1/K)bβti,j ≤ b\n2 βti,j .\nNow, we just have to put together the previous two claims: namely we need to show that the conditions for the decay of the non-anchor topic values, and the lower bound on the anchor-topic values are actually preserved during the iterations. We will hence show the following:\nLemma 29. Suppose we initialize with seeded initialization. Then, after t rounds, if j is an anchor word for topic i, βti,j ≥ (1− ǫ)Clβ∗i,j, and βti′,j ≤ 2−tCsβ∗i,j.\nProof. We prove this by induction. Let’s cover the base case first. In the seed document corresponding to topic i, γ∗d,i ≥ Cl, so at initialization β0i,j ≥ Clβ∗i,j . On the other hand, if topic i appears in the seed document for topic i′, then after initialization β0i′,j ≤ Csβ∗i,j < β0i,j . Hence, at initialization, the claim is true.\nOn to the induction step. If the claim were true at time step t, since βti′,j ≤ 2−tCsβ∗i,j , by Lemma 26, βt+1i,j ≥ Clβ∗i,j - so the lower bound still holds at time t + 1. On the other hand, since βti,j ≥ Clβ∗i,j , by Lemma 28, at time t+ 1, βti′,j ≤ 2−(t+1)Csβ∗i,j .\nHence, the claim we want follows.\nFinally, we show the easy lemma that after the values βti′,j have decreased to (almost) 0, β t i,j ≥ 12β∗i,j .\nLemma 30. Let word j be an anchor word for topic i. Suppose βti′,j ≤ 2−tCsβ∗i,j and\nt > 10 max(log(N), log( 1\nγ∗min ), log(\n1\nβ∗min ))\nThen 4β∗i,j ≥ βt+1i,j ≥ 14β∗i,j.\nProof. Let us do the lower bound first. It’s easy to see ∑ i′ β t i′,jγd,i′ ≤ 2βti,jγtd,i. Hence,\nf̃d,j f td,j βti,jγ t d,i = f̃d,j ∑ i′ β t i′,jγ t d,i′ βti,jγ t d,i ≥\n1 2 f̃d,j βti,jγ t d,i βti,jγ t d,i ≥ (1− ǫ) 1 2 β∗i,jγ ∗ d,i\nHence, after the update,\nβt+1i,j ≥ (1− ǫ) 1\n2 β∗i,j\n∑\nd γ ∗ d,i\n∑\nd γ t d,i\n≥ 1 4 β∗i,j\nsince γtd,i ≤ 2γ∗d,i. The upper bound is similar. Since ∑\ni′ β t i′,jγd,i′ ≥ βti,jγtd,i,\nf̃d,j f td,j βti,jγ t d,i ≤ f̃d,j ≤ (1 + ǫ)β∗i,jγ∗d,i\nHence,\nβt+1i,j ≤ (1 + ǫ)β∗i,j ∑ d γ ∗ d,i ∑\nd γ t d,i\n≤ 2β∗i,j\nsince γtd,i ≥ 12γ∗d,i. This certainly implies the claim we want.\nFurthermore, the following simple application of Lemma 27 is immediate and useful:\nLemma 31. Let t > 10max(logN, log 1γ∗min , log 1β∗min ). Then, γtd,i ≥ p2γ∗d,i.\nC.3 Discriminative words\nWe established in the previous section that after logarithmic number of steps, the anchor words will be correctly identified, and estimated within a factor of 2. We show that this is enough to cause the support of the discriminative words to be correctly identified too, as well as estimate them to within a constant factor where they are non-zero.\nSame as before, we will assume in this section that we can identify the dominating topic. We will crucially rely on the fact that the discriminative words will not have a very large dynamic range comparatively to their total probability mass in a topic. The high level outline will be similar to the case for the anchor words. We will prove that if a discriminative word j is in the support of topic i, then βti,j will always be reasonably lower bounded, and this will cause the values βti′,j to keep decaying for the topics i ′ that the word j does not belong to. The reason we will need the bound on the dynamic range, and the proportion of the dominating topic, and the size of the dominating topic, is to ensure that the β’s are always properly lower bounded.\nC.3.1 Bounds on the βti,j values\nFirst, we show that because the discriminative words have a small range, the values βti,j whenever β ∗ i,j is non-zero are always maintained to be within some multiplicative constant (which depends on the range of the β∗i,j).\nAs a preliminary, notice that having identified the anchor words correctly the γ values are appropriately lower bounded after running the γ update. Namely, by Lemma 31, γtd,i ≥ p/2γ∗d,i\nWith this in hand, we show that the βti,j values are well upper bounded whenever β ∗ i,j is non-zero.\nLemma 32. At any point in time t, βti,j ≤ (1 + ǫ)2BCl β ∗ i,j.\nProof. Since f̃d,j ftd,j βti,jγ t d,i ≤ f̃d,j we have:\nβt+1i,j ≤ ∑ d f̃d,j ∑\nd γ t d,i\n≤ 2 · ∑ d f̃d,j ∑\nd γ ∗ d,i\nOn the other hand, we claim that f̃d,j ≤ (1+ ǫ)Bβ∗i,j . Indeed, f̃d,j ≤ (1+ ǫ) ∑ i γ ∗ d,iβ ∗ i,j , and for any other\ntopic i′, β∗i′,j ≤ Bβ∗i,j . Hence,\n2 · ∑ d f̃d,j ∑\nd γ ∗ d,i\n≤ 2(1 + ǫ)DBβ∗i,j ∑\nd γ ∗ d,i\nHowever, since γ∗d,i ≥ Cl, the previous expression is at most\n2(1 + ǫ)DBβ∗i,j DCl = 2(1 + ǫ)B Cl β∗i,j\nSo, we get the claim we wanted.\nThe lower bound on the βti,j values is a bit more involved. To show a lower bound on the β t i,j values is maintained, we will make use of both the fact that the discriminative words have a small range, and that we have some small, but reasonable proportion of documents where γ∗d,i ≥ 1− δ. More precisely, we show:\nLemma 33. Let βti,j ≤ 2(1+ǫ)BCl β ∗ i,j for all topics i that word j belongs to, and let β t i,j ≥ ClB β∗i,j. Then, βt+1i,j ≥ ClB β∗i,j as well.\nProof. Let’s call Dδ the documents where γ ∗ d,i ≥ 1− δ. We can certainly lower bound\nβt+1i,j ≥ ∑ d∈Dδ f̃d,j ftd,j γtd,iβ t i,j ∑\nd∈D γ t d,i\nFirst, let’s focus on f̃d,j ftd,j βti,j . Then,\nf̃d,j ≥ (1 − ǫ)(1− δ)β∗i,j (C.3)\nFurthermore, since ∑ d∈Dδ γtd,i ≥ 12 ∑ d∈Dδ γ∗d,i and ∑ d γ t d,i ≤ 2 ∑ d γ ∗ d,i, we have that\n∑\nd∈Dδ γtd,i\n∑\nd γ t d,i\n≥ 1 4 8 B (1 − δ) = 2 B (1− δ) (C.4)\nFinally, we claim that βti,j ftd,j ≥ 12 . Massaging this inequality a bit, we get it’s equivalent to:\nβti,j f td,j ≥ 1 2 ⇔\nf ti,j ≤ 2βti,j ⇔\nγtd,iβ t i,j +\n∑\ni′\nγtd,i′β t i′,j ≤ 2βti,j\nThe left hand side can be upper bounded by\nγtd,iβ t i,j +\n∑\ni′\nγtd,i′ 2(1 + ǫ)B3\nC2l βti,j ≤\nγtd,iβ t i,j + (1 − γtd,i)\n2(1 + ǫ)B3\nC2l βti,j\nby the assumptions of the lemma. So, it is sufficient to show that γtd,iβ t i,j + (1 − γtd,i)2(1+ǫ)B 3\nC2l βti,j ≤ 2βti,j , however this is equivalent after\nsome rearrangement to γtd,i ≥ 1− 12(1+ǫ)B3 C2 l −1 .\nIt’s certainly sufficient for this that γtd,i ≥ 1− 1B3 C2 l\n= 1− C 2 l\nB3 , but since since γ ∗ d,i ≥ 1− δ, by the definition\nof δ and Lemmas 24, 35, 36, this certainly holds. Together with C.4 and C.3, we get that\nβt+1i,j ≥ (1− ǫ) 2 B (1− δ)2 1 2 β∗i,j ≥ (1 − ǫ) (1− δ)2 B β∗i,j\nBut, by our assumptions, (1− ǫ)(1− δ)2 ≥ Cl, so the claim follows.\nC.3.2 Decreasing βti′,j values\nFinally, we show that if the discriminative word j does not belong in topic i′, the value for βti′,j will keep dropping. More precisely, the following is true:\nLemma 34. Let word j and topic i be such that β∗i′,j = 0 and let β t i′,j ≤ b. Furthermore, let for all the topics i that j belongs to hold: βti,j ≥ 1/Cββ∗i,j for some constant Cβ. Finally, let γtd,i ≥ 1Cγ γ ∗ d,i for some constant Cγ . Then, β t+1 i′,j ≤ b/2.\nProof. We proceed similarly as the analogous claim for anchor words. We split the update as\nβt+1i′,j = β t i′,j(\n∑\nd∈D1 f̃d,j ftd,j γtd,i′ ∑\nd γ t d,i′\n+\n∑\nd∈D2 f̃d,j ftd,j γtd,i′ ∑\nd γ t d,i′\n)\nfor some appropriate partitioning of the documents D1, D2. Namely, let D1 be documents which do not contain any topic to which word j belongs, the D2 documents which contain at least one topic word j belongs to.\nFor all the documents in D1, f ∗ d,j = 0, and we will provide a good bound for the terms f̃d,j ftd,j in D2, this way, we’ll ensure βti,j gets multiplied by a quantity which is o(1) to get β t+1 i,j , which is of course enough for what we want. Bounding the terms in D2 is even simpler than before. We have:\nf td,j = ∑\ni\nβti,jγ t d,i ≥\n1\nCβCγ\n∑\ni\nβ∗i,jγ ∗ d,i =\n1\nCβCγ f∗d,j\nHence, f∗d,j ftd,j ≤ CβCγ . Then we have:\n∑ d f̃d,j ftd,j γtd,i ∑\nd γ t d,i\n≤ (1 + ǫ) ∑ d f∗d,j ftd,j γtd,i ∑\nd γ t d,i\n≤\n4(1 + ǫ)\n∑\nd f∗d,j ftd,j\nγ∗d,i ∑\nd γ ∗ d,i\n≤ 4(1 + ǫ) ∑ d∈D2 CβCγγ ∗ d,i ∑\nd γ ∗ d,i\nBut now, by the ”weak topic correlation” property, ∑ d∈D2 γ∗d,i ∑\nd γ ∗ d,i = o(1). Indeed, D consists of the documents\nwhere i′ is the dominating topic. In order for the document to belong to D2, at least one of the topics word j belongs to must belong in the document as well. Since the word j only belongs to o(K) of the topics, and each document contains only a constant number of topics, by the small topic correlation property, the claim we want follows.\nBut then, clearly, 4 ∑ d∈D2 CβCγγ ∗ d,i ∑\nd γ ∗ d,i = o(1) as well.\nHence, βt+1i′,j = o(1)β t i′,j ≤ 12βti′,j , which is what we need.\nC.4 Determining dominant topic and parameter range\nTo complete the proofs of the claims for Phase I and II, we need to show that at any point in time we correctly identify the dominant topic. Furthermore, in order to maintain the lower bounds on the estimates for the discriminative words, we will need to make sure that γtd,i is large as well in the documents where γ∗d,i ≥ 1− δ.\nLet’s proceed to the problem of detecting the largest topic first. By Lemma 25 all we need to do is bound Rf and Rβ at any point in time during this phase. To do this, let’s show the following lemma:\nLemma 35. Suppose for the anchor words βti,j ≥ C1β∗i,j, for the discriminative words βti,j ≥ C2β∗i,j . Let pi be the proportion of anchor words in topic i. Then, KL(β∗i ||βti ) ≤ pi log( 1C1 ) + (1 − pi) log( 1 C2 ).\nProof. This is quite simple. Since log is an increasing function,\nKL(β∗i ||βti ) = ∑\nj\nβ∗i,j log( β∗i,j βti,j ) ≤ pi log( 1 C1 ) + (1 − pi) log( 1 C2 )\nLemma 36. Suppose for the anchor words βti,j ≥ C1β∗i,j, for the discriminative words βti,j ≥ C2β∗i,j. Let pi be the proportion of anchor words in topic i. Then, minγ∈∆K KL(f̃d||fd) ≤ log(1+ǫ)+ ( p log( 1C1 ) + (1− p) log( 1 C2 ) ) .\nProof. Also simple. The value of KL(f̃d||fd) one gets by plugging in γd = γ∗ is exactly what is stated in the lemma.\nWe’ll just use the above two lemmas combined from our estimates from before. We know, for all the anchor words, that βti,j ≥ Clβ∗i,j , and that for the discriminative words, βti,j ≥ ClB β∗i,j . Hence, by Lemma 35, at any point in time KL(β∗i ||βti) ≤ p log( 1Cl ) + (1− p) log( B Cl ). So, by Lemma 25, it’s enough that\nCl − Cs ≥ 1\np\n( √\n2\n(\np log( 1\nCl ) + (1− p) log(BCl)\n)\n+ √ log(1 + ǫ)\n)\n+ ǫ\n.\nSince 1p\n√\n2 ( p log( 1Cl ) + (1− p) log(BCl) ) ≤ 1p √ 2 ( log( 1Cl ) + (1− p) logB ) , to get a sense of the pa-\nrameters one can achieve, for detecting the dominant topic, (ignoring ǫ contributions), it’s sufficient that Cl − Cs ≥ 2p √\nmax(log( 1Cl ), (1− p) logB) If one thinks of Cl as 1−η and p ≥ 1− ηlogB , since log( 1Cl ) ≈ η roughly we want that Cl−Cs ≫ 2 p √ η. (One takeaway message here is that the weight we require to have on the anchors depends only logarithmically on the range B.)\nLet’s finally figure out what the topic proportions must be in the ”heavy” documents. In these, we want\nγ∗d,i ≥ 1− C2l 2B3 + 1 p\n( √\n2 ( p log( 1Cl ) + (1− p) log(BCl) ) − √ log(1 + ǫ)\n)\n+ ǫ. A similar approximation to the\nabove gives that we roughly want γ∗d,i ≥ 1− 1−2η2B3 + 2p √ η.\nC.5 Getting the supports correct\nAt the end of the previous section, we argued that after O(logN) rounds, we will identify the anchor words correctly, and the supports of the discriminative words as well. Furthremore, we will also have estimated the values of the non-zero discriminative word probabilities, as well the anchor word probabilities up to a multiplicative constant. Then, I claim that from this point onward at each of the γ steps, the γt values we get will have the correct support. Namely, the following is true:\nLemma 37. Suppose for the anchor words and discriminative words j, if β∗i,j = 0, it’s true that β t i,j = o( 1 n ). Furthermore, suppose that if β∗i,j 6= 0, 1Cβ β ∗ i,j ≤ βti,j ≤ Cββ∗i,j for some constant Cβ.\nThen, when performing KL minimization with respect to the γ variables, whenever γ∗d,i = 0 we have\nγtd,i = 0.\nProof. Let γ∗d,i = 0. If γ t d,i 6= 0, then the KKT conditions imply:\nN ∑\nj=1\nf̃d,j f td,j βti,j = 1 (C.5)\nThe only terms that are non-zero in the above summation are due to words j that belong to at least one topic i′ in the document. Let I be the set of words that belong to topic i as well.\nBy Lemma 31, we know that γtd,i ≥ p/2γ∗d,i Since also βti,j ≥ 1Cβ β ∗ i,j , f t d,i ≥ p2Cβ f ∗ d,j. Since β t i,j = o( 1 n ) for\nwords j not in the support of topic I, ∑\nj /∈I\nf̃d,j f td,j βti,j = o(1).\nOn the other hand, for words in I, f̃d,j ftd,j βti,j ≤ (1+ ǫ) 2C2β p β ∗ i,j , so ∑ j∈I f̃d,j ftd,j βti,j = o(1), by the small support\nintersection property. However, this contradicts C.5, so we get what we want.\nThis means that after this phase, we will always correctly identify the supports of the γ variables as well.\nC.6 Alternating minimization\nNow, finishing the proof of Theorem 3 is trivial. Namely, because of Lemmas 37, 29, and the analogue of 29, we are basically back to the case where we have the correct supports for both the β and γ variables. The only thing left to deal with is the fact that the β variables are not quite zero.\nLet j be an anchor word for topic i. Let ǫ′′ = 1− (1 − ǫ′)1/7. Similarly as in Lemma 31, for\nt > 10max(logN, log( 1\nǫ′′γ∗min ), log(\n1\nǫ′′β∗min ))\nit holds that f∗d,j ftd,j ≥ (1− ǫ′)1/7 β ∗ i,jγ ∗ d,i βtd,iγ t d,i . The same inequality is true if j is a lone word for topic i in document d. After the above event, the same proof from Case Study 1 implies that after O(log( 1ǫ′ )) iterations we’ll get 1\n1 + ǫ′ β∗i,j ≤ βti,j ≤ (1 + ǫ′)β∗i,j\nand 1\n1 + ǫ′ γ∗i,j ≤ γti,j ≤ (1 + ǫ′)γ∗i,j\nThis finishes the proof of Theorem 3."
    }, {
      "heading" : "D Justification of prior assumptions",
      "text" : "In this section we provide a brief motivation for our choice of properties on the topic model instances we are looking at. Nothing in the other sections crucially depends on this section, so it can be freely skipped upon first reading.\nMost of our properties on the topic priors are inspired from what happens with the Dirichlet prior - specifically, variants of all of the ”weak correlations” between topics hold for Dirichlet. Essentially the only difference between our assumptions and Dirichlet is the lack of smoothness. (Dirichlet is sparse, but only in the sense that it leads to a few ”large” topics, but the other topics may be non-negligible as well.)\nTo the best of our knowledge, the lemmas proven here were not derived elsewhere, so we include them for completeness.\nFor all of the claims below, we will be concerned with the following scenario: ~γ = (γ1, γ2, . . . , γK) will be a vector of variables, and ~α = (α1, α2, . . . , αk) a vector of parameters. We\nwill let ~γ be distributed as ~γ := Dir(α1, α2, . . . , αk), where αi = Ci/K c, for some constants Ci and c > 1.\nD.1 Sparsity\nTo characterize the sparsity of the topic proportions in a document, we will need the following lemma from (Telgarsky, 2013):\nLemma 38. (Telgarsky, 2013) For a Dirichlet distribution with parameters (C1/k c, C2/k c, . . . , Ck/k c), the probability that there are more than c0 ln k coordinates in the Dirichlet draw that are ≥ 1/kc0 is at most 1/kc0.\nIt’s clear how this is related to our assumption: if one considers the coordinates ≥ 1kc0 as ”large”, we assume, in a similar way, that there are only a few ”large” coordinates. The difference is that we want the rest of the coordinates to be exactly zero.\nD.2 Weak topic correlations\nWe will prove that the Dirichlet distribution satisfies something akin to the weak topic correlations property. We prove that when conditioning on some small (o(K)) set of topics being small, the marginal distributions for the rest of the topic proportions are very close to the original ones. This implies our ”weak topic correlations” property.\nThe following is true:\nLemma 39. Let ~γ = (γ1, γ2, . . . , γK) be distributed as specified above. Let S be a set of topics of size o(K), and let’s denote by γS the vector of variables corresponding to the topics in the set S, and γS̄ the rest of the coordinates. Furthermore, let’s denote by γ̃S̄ the distribution of γS̄ conditioned on all the coordinates of γS being at most 1/K\nc1 for c1 > 1. Then, for any i ∈ S̄ and γ = 1− δ, any δ = Ω(1), PγS̄(γi = γ) = (1 ± o(1))Pγ̃S̄ (γi = γ).\nProof. It’s a folklore fact that if ~Y = Dir(~α), then\n(Y1, Y2, . . . , Yi−1, Yi+1, . . . , YK |Yi = yi) = (1 − yi)Dir(α1, α2, . . . , αi−1, αi+1, . . . , αK)\nApplying this inductively, we get that γ̃S̄ = (1 − ∑ j∈S γi)Dir(~αS̄). Let’s denote s := ∑\nj∈S γi, and s̃ = ∑\ni∈S αi. Then, since γi ≤ 1/Kc1 for i ∈ S, s = o(1). Similarly, s̃ = o(1). For notational convenience, let’s call α̃0 = ∑ i/∈S αi, and α0 = ∑ i αi = α̃0 + s̃. The marginal distribution of variable Yi where ~Y = Dir(~α) is Beta(αi, α0 − αi). Hence,\nPγS̄ (γi = γ) = 1\nB(αi, α̃0 + s̃− αi) γαi−1(1 − γ)α̃0+s̃−αi−1\nand\nPγ̃S̄ (γi = γ) = 1\nB(αi, α̃0 − αi) (\nγ 1− s ) αi−1 (1− γ 1− s ) α̃0−αi−1\nThe following holds:\nγαi−1(1− γ)α̃0+s̃−αi−1 ( γ1−s ) αi−1(1− γ1−s )α̃0−αi−1 =\n(1 − s)αi−1 ( (1 − s)(1− γ) 1− s− γ\n)−αi−1\n(1 − γ)s̃ =\n( 1 + s\n1− s− γ\n)−αi−1\n(1− γ)s̃\nNow, I claim the above expression is 1± o(1). We’ll just prove this for each of the terms individually. Since 1 + s1−s−γ ≥ 1 and −1 − αi ≤ −1, it follows that (1 + s1−s−γ ) −αi−1 ≤ 1. On the other hand, by Bernoulli’s inequality, (1 + s1−s−γ )−αi−1 ≥ 1− (αi + 1) s1−s−γ ≥ 1− o(1), since γ = 1− δ, for some constant δ, by our assumptions. For the second term, since 1 − γ ≤ 1 and s̃ ≥ 0, (1 − γ)s̃ ≤ 1. On the other hand, again by Bernoulli’s inequality, (1 − γ)s̃ ≥ 1− γs̃ = 1− o(1), as we needed. Comparing B(αi, α̃0 + s̃−αi) and B(αi, α̃0 −αi) is not so much more difficult. By definition, B(αi, α0− αi) = ∫ 1 0 xαi−1(1− x)α0−αi−1 dx, so\nB(αi, α0 + s̃− αi) B(αi, α0 − αi) =\n∫ 1\n0 xαi−1(1 − x)α̃0+s̃−αi−1 dx ∫ 1\n0 x αi−1(1 − x)α̃0−αi−1 dx\nWe’ll just bound each of the ratios xαi−1(1 − x)α̃0+s̃−αi−1 xαi−1(1 − x)α̃0−αi−1\nNamely, this is just (1 − x)s̃. Same as above, 1 − o(1) ≤ (1 − γ)s̃ ≤ 1. Hence, these are within a constant from each other.\nD.3 Dominant topic equidistribution\nNow, we pass to proving a smooth version of the dominant topic equidistribution property. Namely, for a threshold x0 = o(1), we can consider a topic ”large” whenever it’s bigger than x0. We will show that for any topics Yi, Yj , the probabilities that Yi > x0 and Yj > x0 are within a constant from each other.\nMathematically formalizing the above statement, we will prove the following lemma:\nLemma 40. Let ~γ = (γ1, γ2, . . . , γK) be distributed as specified above. Then, P(Yi>x0) P(Yj>x0) = O(1), for any i, j if x0 = o(1).\nProof. As before, the marginal distribution of Yi is Beta(αi, α0 − αi). The Beta distribution pdf is just P(x) = x αi−1(1−x)α0−αi−1\nB(αi,α0−αi) , where B(αi, α0 − αi) =\n∫ 1\n0 xαi−1(1− x)α0−αi−1 dx.\nHence, the ratio we care about can be written as\n( ∫ 1 x0 xαi−1(1− x)α0−αi−1 dx)/B(αi, α0 − αi) ( ∫ 1\nx0 xαj−1(1− x)α0−αj−1 dx)/B(αj , α0 − αj)\nTo get a bound on this ratio, it’s sufficient to bound the normalization constants B(αi, α0 − αi) and B(αj , α0−αj), as well as the ratio ∫ 1 x0 xαi−1(1−x)α0−αi−1 dx ∫\n1 x0\nxαj−1(1−x)α0−αj−1 dx . Let’s prove first that B(αi, α0−αi) ≃ B(αj , α0−\nαj)\nBy definition, B(αi, α0 − αi) = ∫ 1 0 xαi−1(1 − x)α0−αi−1 dx. The way we’ll analyze this quantity is that\nwe’ll divide the integral in two parts, one from 0 to 12 and one from 1 2 to 1.\nSince α0 = O(1), it follows that α0 − αi − 1 & −1 and α0 − αi − 1 . 1. Hence, (1 − x)α0−αi−1 = Θ(1). It follows that\n∫ 1 2\n0\nxαi−1(1− x)α0−αi−1 dx ≃ ∫ 1 2\n0\nxαi−1 dx =\n≃ (1/2) αi αi ≃ 1 αi\nwhere the last equality follows since 12 ≤ (1/2)αi ≤ 1. The second portion is not much more difficult. Since 12 ≤ 12\nαi−1 ≤ 1, it follows ∫ 1\n1 2\nxαi−1(1− x)α0−αi−1 dx ≃ ∫ 1\n1 2\n(1− x)α0−αi−1 dx =\n≃ (1/2) α0−αi α0 − αi ≃ 1 α0\nwhere the last two equalities come about since −1 . α0 − αi . 1. But the above two estimates proved that for any i, B(αi, α0 − αi) ≃ 1αi , as we needed. So, we proceed onto bounding\n∫ 1\nx0 xαi−1(1− x)α0−αi−1 dx\n∫ 1\nx0 xαj−1(1− x)α0−αj−1 dx\nWe’ll proceed in a similar fashion as before. We’ll pick some point xT , and if x < xT , we will show that xαj−1(1−x)α0−αj−1 is within a constant factor from xαi−1(1−x)α0−αi−1. On the other hand, we will show that part of the integral where x > xT is dominated by the part where x < xT , which will imply the claim we need.\nLet’s rewrite the ratio above a little:\nxαj−1(1− x)α0−αj−1 xαi−1(1− x)α0−αi−1 =\n( x\n1− x ) αj−αi = e(αj−αi) ln(\nx 1−x )\nProceeding as outlined, I claim that for sufficiently large constants C1, C2, s.t. if x ≤ 1 − 1 1+C1e 1 αi 1 C2 ,\nthen x αj−1(1−x)α0−αj−1\nxαi−1(1−x)α0−αi−1 = O(1). Let’s call xT = 1− 1\n1+C1e 1 αi 1 C2\n.\nThe claim is then, that if xT ≥ x ≥ x0, that (αj − αi) ln( x1−x ) = O(1). First let’s assume, αj − αi ≥ 0. Then, if ln( x1−x) < 0 ⇔ x < 12 , the condition is of course satisfied. So let’s assume x ≥ 12 . When\n1 2 ≤ x ≤ xT , we get that x1−x ≤ C1ee\n1 αj 1 C2\n. Hence, ln( x1−x ) ≤ lnC1 + 1αj 1 C2 . It follows that if C1, C2 are\nsufficiently large,\n( x 1− x ) αj−αi ≤ eln( x1−x )αj = O(1)\nOn the other hand, if αj − αi ≤ 0, when x ≥ 12 , (αj − αi) ln( x1−x ) ≤ 0, so we are fine. However, since |αj − αi| ≤ αi, it’s easy to check when x ≥ e −c1/αi\n1+e−c1/αi > x0, that (αj − αi) ln( x1−x ) = O(1).\nFinally, we want to claim that the portion of the integral from xT to 1 is dominated by the portion from x0 to xT .\nWe can show that the latter portion is O(e−K), and the first is Ω(1). Let’s lower bound the first portion. We lower bound\n∫ xT x0 xαi−1(1 − x)α0−αi−1 dx by xαi−1T ∫ xT x0 (1 − x)α0−αi−1 dx. For the first factor in the above expression, we use Bernoulli’s inequality to prove it’s Ω(1). For the second, the integral will evaluate to\n(1 − x0)α0−αi − (1− xT )α0−αi α0 − αi\nLet’s lower bound the first term in the numerator. If α0 − αi ≥ 1, another application of Bernoulli’s inequality gives: (1 − x0)α0−αi ≥ 1 − (α0 − αi)x0 ≥ 1 − o(1). If, on the other hand, 0 ≤ α0 − αi ≤ 1, (1− x0)α0−αi ≥ 1− x0 ≥ 1− o(1).\nThen, I claim that (1 − xT )α0−αi = e−Ω(K). Indeed, for some constant C3, (\n1\n1 + C1e 1 αj 1 C2\n)α0−αi\n≤ (\n1\nC3e 1 αj 1 C2\n)α0−αi\n=\n= e− ln(C3e 1 αj 1 C2 )(α0−αi)\nHowever, since α0 = Ω(Kαj) and α0−αi = Ω(α0), the above expression is upper bounded by e−Ω(K), which is what we were claiming. Hence, xαi−1T ∫ xT x0\n(1− x)α0−αi−1 dx = Ω(1). Let’s upper bound the latter portion. This expression is upper bounded by\nxαi−1T\n∫ 1\nxT\n(1− x)α0−αi−1 dx = xαi−1T\n(\n1\n1+C1e 1 αj 1 C2\n)α0−αi\nα0 − αi\nNow, we will separately bound each of xαi−1T and\n\n\n1\n1+C1e\n1 αj 1 C2\n\n\nα0−αi\nα0−αi .\nThe first term can be written as 1 x 1−αi T . Now, since 1 − αi ≥ 0, we can use Bernoulli’s inequality to\nlower bound x1−αiT by 1 − 1 1+C1e 1 αj 1 C2 (1 − αi). Since 1 1+C1e 1 αj 1 C2\n= O(1/e 1 αj ), and 1 − αi ≤ 1/2, let’s say,\n1− 1 1+C1e 1 αj 1 C2 (1− αi) = Ω(1), i.e. xαi−1T = O(1).\nFor the second term, we already proved above that (1−xT )α0−αi = e−Ω(K), This implies that ∫ 1 xT xαi−1(1−\nx)α0−αi−1 dx = O(e−K), which finishes the proof.\nD.4 Independent topic inclusion\nFinally, there’s a very simple proxy for ”independent topic inclusion”. Again, as above, γ̃S̄ = (1 − ∑\nj∈S γi)Dir(~αS̄).\nBut, if we consider ”inclusion” the probability that a given topic is ”noticeable” (i.e. ≥ 1nc0 , say), we can use the above Lemma 40 to show that the probability that any topic is ”large” (but still o(1)) is within a constant for all the topics in S̄."
    }, {
      "heading" : "E On common words",
      "text" : "In this section, we show how one would modify the proofs from the previous section to handle common words as well. We stress that common words are easy to handle if one were allowed to filter them out, but we want to analyze under which conditions the variational inference updates could handle them on their own.\nThe difference in contrast to the previous sections is it’s not clear how to argue progress for the common words: common words do not have lone documents. However, if we can’t argue progress for the common words, then we can’t argue progress for the γ variables, so the entire argument seems to fail.\nFormally, we consider the following scenario:\n• On top of the assumptions we have either in Case Study 1 or Case Study 2, we assume that there are words which show up in all topics, but their probabilities are within a constant κ from each other, B ≥ κ ≥ 2. We will call these common words. (The κ ≥ 2 is without loss of generality. If the claim holds for a smaller κ, then it certainly holds for κ = 2. The only difference is that the estimates to follow could be strengthened, but we assume κ ≥ 2 to get cleaner bounds.)\n• For each topic i, if C is the set of common words, ∑j∈C β∗i,j ≤ 1κ100 , i.e. there isn’t too much mass on these words.\n• Conditioned on topic i being dominant, there is a probability of 1− 1κ100 that the proportion of topic i is at least 1− 1κ100 .\nThen, recall the theorem we want to prove is:\nTheorem 41. If we additionally have common words satisfying the properties specified above, after O(log(1/ǫ′)+ logN) of KL-tEM updates in Case Study 2, or any of the tEM variants in Case Study 1, and we use the same initializations as before, we recover the topic-word matrix and topic proportions to multiplicative accuracy 1 + ǫ′, if 1 + ǫ′ ≥ 1(1−ǫ)7 .\nOur analysis here is fairly loose, since the result is anyway a little weak. (e.g. 1 − 1κ100 is not really the best value for the proportion of the dominating topic, or the proportion of such documents required.) At any rate, it will be clear from the proofs that the dependency of the dominating topic on κ has to be of the form 1 − 1κc , so it’s not clear one would gain too much from the tightest possible analysis. The reason we are including this section is to show cases where our proof methods start breaking down.\nWe will do the proof for Case Study 1 first, after which Case Study 2 will easily follow.\nE.1 Phase I with common words\nThe outline is the same as before. We prove the lower bounds on the γ and β variables first. Namely, we prove:\nLemma 42. Suppose that the supports of β and γ are correct. Then, γtd,i ≥ 12γ∗d,i.\nProof. Similarly as before, multiplying both sides of B.1 by γtd,i, we get that\nγtd,i ≥ ∑\nLi\nf∗d,j f td,j βti,jγ t d,i ≥ (1− o(1))(1 − 1 κ100 )γ∗d,i ≥ 1 2 γ∗d,i\nwhere the second inequality follows since 1− 1κ100 fraction of the words in topic i is discriminative.\nLemma 43. Suppose that the supports of the γ and β variables are correct. Additionally, if i is a large topic in d, let 12γ ∗ d,i ≤ γtd,i ≤ 3γ∗d,i. Then, for a discriminative word j for topic i, βt+1i,j ≥ 13β∗i,j.\nProof. Again, similarly as in Lemma 8,\nβt+1i,j ≥ ∑ d∈Dl (1− ǫ) γ\n∗ d,iβ ∗ i,j\nγtd,i·β t i,j βti,jγ t d,i\n∑D d=1 γ t d,i\n=\n(1− ǫ)β∗i,j ∑D d∈Dl γ∗d,i\n∑D d=1 γ t d,i\nIn the documents where topic i is the largest, γtd,i ≤ 3γ∗d,i. So, we can conclude\nβt+1i,j ≥ (1− ǫ)β∗i,j 1\n3\n∑D d∈Dl γ∗d,i ∑D\nd=1 γ ∗ d,i\nSince ∑D d∈Dl γ∗d,i\n∑D d=1 γ ∗ d,i\n≥ (1− o(1)), as before, we get what we want.\nLemma 44. Let the β variables have the correct support. Let j be a discriminative word for topic i, and let βti,j ≥ 1Cmβ ∗ i,j, γ t d,i ≥ 1Cm γ ∗ d,i whenever β ∗ i,j 6= 0, γ∗d,i 6= 0. Let βti,j = Ctββ∗i,j , where Ctβ ≥ 4Cm, and Cm is a constant. Then, in the next iteration, βt+1i,j ≤ Ct+1β β∗i,j, where Ct+1β ≤ Ctβ 2 .\nProof. The proof is exactly the same as Lemma 9.\nNow, we finally get to the upper bound of the γ values.\nLemma 45. Fix a particular document d. Let’s assume the supports for the β and γ variables are correct. Furthermore, let 1Cm ≤ βti,j β∗i,j ≤ Cm for some constant Cm. Then, γtd,i ≤ 2γ∗d,i.\nProof. Again, multiplying B.1 by γtd,i, we get\nγtd,i = ∑\nj∈Li\nf̃d,j + γ t d,i\n∑\nj /∈Li\nf̃d,j f td,j βti,j + γ t d,i ∑\nj∈C\nf̃d,j f td,j βti,j\nIf α̃ = ∑ j∈Li β∗i,j , since γ t d,i ≥ 1Cm γ ∗ d,i,\nf̃d,j f td,j ≤ (1 + ǫ)C2m\nIf we denote Γ = ∑ j∈C β ∗ i,j , then\nγtd,i ≤ (1 + ǫ)(α̃γ∗d,i + C3m(1− Γ− α̃)γtd,i + Γκ4γtd,i)\nEquivalently, γtd,i ≤ (1+ǫ)α̃1−(1+ǫ)C3m(1−Γ−α̃)−(1+ǫ)Γκ4 γ ∗ d,i Then, we claim that (1+ǫ)α̃1−(1+ǫ)C3m(1−Γ−α̃)−(1+ǫ)Γκ4 ≤ 1 + 1κ50 . Indeed, Γκ4 ≤ κ−96, and C3m(1 − Γ − α̃) ≤\nC3m(1 − α̃) = o(1). Hence,\n(1 + ǫ)α̃ 1− (1 + ǫ)C3m(1− Γ− α̃)− (1 + ǫ)Γκ4 ≤ (1 + ǫ)α̃ 1− o(1)− κ−96 ≤ (1 + ǫ)α̃ 1− κ−95\nFinally, we claim that (1+ǫ)α̃1−κ−95 ≤ 1 + κ−50. Indeed, this is equivalent to\nα̃ ≤ (1 + ǫ)(1 + κ−50)(1− κ−95) ≤ (1 + ǫ)(1 + κ−50)\nBut, since we assume κ ≥ 2, the claim we need follows easily.\nE.2 Phase II of analysis\nFinally, we deal with the alternating minimization portion of the argument. How will we deal with the lack of anchor documents? The almost obvious way: if a document has topic i with proportion 1 − 1κ100 , it will behave for all purposes like an anchor document, because the dynamic range of word β∗i,j is limited, and the contribution from the other topics is not that significant.\nIntuitively, we’ll show that f∗d,j ftd,j ≈ β ∗ i,j βti,j , so that these documents provide a ”push” for the value of βti,j in\nthe correct direction.\nLemma 46. Let’s assume that our current iterates βti,j satisfy 1 Ctβ ≤ βi,j∗ βti,j ≤ Ctβ for Ctβ ≥ 1(1−ǫ)20 . Then, after iterating the γ updates to convergence, we will get values γtd,i that satisfy (C t β)\n1/10 ≤ γd,i∗ γtd,i ≤ (Ctβ)1/10.\nProof. As before, we have that\nγtd,i = ∑\nj∈Li\nf̃d,j + γ t d,i\n∑\nj /∈Li\nf̃d,j f td,j βti,j\nLet’s denote as Ctγ = maxi(max( γ∗d,i γtd,i , γtd,i γ∗ d,i )), and let, as before, assume that γtd,i0 γ∗ d,i0 = Ctγ . By the definition of Ctγ ,\nγtd,i0 = ∑\nj∈Li0\nf̃d,j + γ t d,i0\n∑\nj /∈Li0\nf̃d,j f td,j βti0,j ≤\n(1 + ǫ)(α̃γ∗d,i0 + (1− α̃)(Ctβ)2(Ctγ)2γ∗d,i0) We claim that (1 + ǫ)(α̃+ (1 − α̃)(Ctβ)2(Ctγ)2) ≤ (Ctγ)1/10 (E.1) which will be a contradiction to the definition of Ctγ . After a little rewriting, E.1 translates to α̃ ≥ 1 − (Ctγ ) 1/10 1+ǫ −1\n(Ct β Ctγ)\n2−1 . By our assumption on C t γ , C t β ≤ C10γ , so\nthe right hand side above is upper bounded by 1− (Ctγ ) 1/10 1+ǫ −1\n(Ctγ) 8−1 .\nBut, Lemma 45 implies that certainly Ctγ ≤ C0γ . The function\nf(c) = c1/10 1+ǫ − 1 c8 − 1\ncan be easily seen to be monotonically decreasing on the interval of interest, and hence is lower bounded by (C0γ ) 1/10\n1+ǫ −1\n(C0γ) 8−1 . Since α̃ = (1 − o(1))(1 − 1κ100 ) and C0γ ≤ 3, the claim we want is clearly true.\nThe case where γ∗d,i0 γtd,i0 = Ctγ is not much more difficult. An analogous calculation as in Lemma 12 gives\nthat to get a contradiction to the definition of Ctγ , the condition required is that 1− 1− 1 (1−ǫ)(Ctγ ) 1/10\n1− 1 (Ctγ )\n8 . As before,\nif f(c) = 1− 1 (1−ǫ)c1/10\n1−c8 , it s easy to check that f(c) is monotonically increasing in the interval of interest, so lower bounded by\n1− 1 (1−ǫ)( 1\n(1−ǫ)20 )1/10\n1− 1 (( 11−ǫ ) 20)8\n=\n1− (1 − ǫ) 1− (1− ǫ)160 ≥ 1 160\nBut, α̃ ≥ (1− 1κ100 )(1− o(1)) ≥ 1− 1160 , so we get what we want.\nNext, we show the following lemma.\nLemma 47. Suppose at time step t, 1Ctγ γ∗d,i ≤ γtd,i ≤ Ctγγ∗d,i and 1Ctβ β ∗ i,j ≤ βti,j ≤ Ctββ∗i,j , such that Ctγ ≤ (Ctβ) 1/10 for Ctβ ≥ 1(1−ǫ)20 . Then, at time step t+ 1, 1/C t+1 β β ∗ i,j ≤ βti,j ≤ Ct+1β β∗i,j, where Ct+1β = (Ctβ)3/4\nProof. Let’s assume a document d has a dominating topic of proportion at least 1− 1/κ100. Then, we claim that\nf∗d,j ftd,j ≥ 1 (Ctβ) 1/4 β∗i,j βti,j . We will do a sequence of rearrangements to get this condition to\na simpler form:\nf∗d,j f td,j ≥ 1 (Ctβ) 1/4 β∗i,j βti,j ⇔\nf∗d,j β∗i,j ≥ 1 (Ctβ) 1/4 f td,j βti,j ⇔\nγ∗d,i + ∑\ni′\nγ∗d,i′ β∗i′,j β∗i,j > 1 (Ctβ) 1/4 (γtd,i + ∑\ni′\nγtd,i′ βti′,j βti,j )\nLet’s upper bound the right hand side by some simpler quantities. We have:\n1\n(Ctβ) 1/4\n(γtd,i + ∑\ni′\nγtd,i′ βti′,j βti,j ) ≤\n1 (Ctβ) 1/4 Ctγ(γ ∗ d,i + ∑\ni′\nγ∗d,i′ βti′,j βti,j ) ≤\n1\n(Ctβ) 1/4\nCtγ(γ ∗ d,i + (C t β)\n2 ∑\ni′\nγ∗d,i′ β∗i′,j β∗i,j )\nHence, it is sufficient to prove\nγ∗d,i + ∑\ni′\nγ∗d,i′ β∗i′,j β∗i,j ≥ 1 (Ctβ) 1/4 Ctγ(γ ∗ d,i + (C t β) 2 ∑\ni′\nγ∗d,i′ β∗i′,j β∗i,j ) ⇔\nγ∗d,i(1 − Ctγ\n(Ctβ) 1/4\n) ≥ ∑\ni′\nγ∗d,i′( Ctγ\n(Ctβ) 1/4\n(Ctβ) 2 − 1) β∗i′,j β∗i,j\nAgain, we can upper bound the right hand side by\n∑\ni′\nγ∗d,i′( Ctγ\n(Ctβ) 1/4\n(Ctβ) 2 − 1)κ =\n(1− γ∗d,i)( Ctγ\n(Ctβ) 1/4\n(Ctβ) 2 − 1)κ\nSo, it is sufficient to prove:\n(1− γ∗d,i)( Ctγ\n(Ctβ) 1/4\n(Ctβ) 2 − 1)κ ≤ γ∗d,i(1− Ctγ (Ctβ) 1/4 ) ⇔\nγ∗d,i(1− Ctγ\n(Ctβ) 1/4\n+ ( Ctγ\n(Ctβ) 1/4\n(Ctβ) 2 − 1)κ) ≥ ( Ctγ (Ctβ) 1/4 (Ctβ) 2 − 1)κ ⇔\nγ∗d,i ≥ 1− 1− C\nt γ\n(Ct β )1/4\n1− C t γ\n(Ct β )1/4\n+ ( Ctγ\n(Ct β )1/4\n(Ctβ) 2 − 1)κ\nIt’s easy to check that the expression on the right hand side as a function of Ctγ is decreasing. Hence, the RHS is upper bounded by\n1− 1− 1 (Ctβ) 3/20\n1− 1 (Ctβ) 3/20 + κ((C t β) 37/20 − 1)\nNow, let’s analyze this expression. If we let f(x) = 1− 1− 1 x3/20\n1− 1 x3/20\n+κ(x37/20−1) , I claim f(x) is an increasing\nfunction of x. Indeed, we can calculate it’s derivative fairly easily:\nf ′(x) = − 3 20x − 2320 (1− 1 x3/20 + κ(x37/20 − 1))− (1− 1 x3/20 )(− 320x− 23 20 + 3720κx 17 20 )\n(1− 1 x3/20\n+ κ(x37/20 − 1))2 =\n− 3 20x − 2320 κ(x 37 20 − 1)− 3720κx 17 20 (1− x− 320 )\n(1− 1 x3/20\n+ κ(x37/20 − 1))2 = κ 20 (40x 14/20 − (3x−23/40 + 37x17/20)) (1 − x3/20 + 1κ ( 1x37/20 − 1))2\nBy the AM-GM inequality, 3x−23/40 + 37x17/20 ≥ 40((x17/20)37(x−23/20)3)1/40 = 40x14/20, so f ′(x) is positive, so the RHS, as a function of Ctβ , is (x) is increasing.\nSo, it is sufficient to satisfy the inequality when Ctβ = C 0 β . One can check however that by Lemma 43 and 44 this is true. Proceeding to the lower bound, a similar calculation as before gives that the necessary condition for progress is:\nγ∗d,i ≥ 1− 1− (C\nt β) 1/4 Ctγ\n1− (Cβ)1/4Ctγ + 1 κ (\n(Ctβ) 1/4\nCtγ 1 (Ctβ) 2 − 1)\nAgain, the right hand side expression is decreasing in Cγ , so it is certainly upper bounded by\n1− 1− (Ctβ)3/20\n1− (Ctβ)3/20 + 1κ ( 1(Ctβ)37/20 − 1)\nNow, the claim is that this expression is increasing in Ctβ . Again, denoting f(x) = 1− 1−x 3/20\n1−x3/20+ 1κ ( 1\nx37/20 −1)\nf ′(x) = −− 3 20x −17/20(1− x3/20 + 1κ ( 1x37/20 − 1))− (1− x3/20)(− 3 20x −17/20 − 1κ 3720x−57/20) (1− x3/20 + 1κ ( 1x37/20 − 1))2 =\n−− 3 20x −17/20 1 κ( 1 x37/20 − 1) + (1− x3/20) 1κ 3720x−57/20 (1− x3/20 + 1κ ( 1x37/20 − 1))2 = 1 20κ (−40x−54/20 + (3x−17/40 + 37x−57/20)) (1− x3/20 + 1κ ( 1x37/20 − 1))2\nBy the AM-GM inequality, 3x−17/40 + 37x−57/20 ≥ 40((x−17/20)37(x−57/20)37)1/40 = 40x−54/20, so f ′(x) is negative, so the RHS, as a function of Ctβ , is decreasing. So it suffices to check the inequality when Ctβ = (1− ǫ)20. In this case, we want to check that\n1− 1 κ100\n≥ 1− 1− 1(1−ǫ)3\n1− 1(1−ǫ)3 + 1κ((1 − ǫ)37 − 1)\nSince 1− 1− 1 (1−ǫ)3\n1− 1 (1−ǫ)3 + 1κ ((1−ǫ) 37−1) ≤ 1− 3κ37+3κ , and κ ≥ 2, this is easily seen to be true. Now, we’ll split the β update into two parts: documents where topic i is at least 1− 1/κ100, and the rest\nof them. In the first group, as we showed above, f∗d,j ftd,j ≥ 1 (Ctβ) 1/2 . In the second group, we can certainly claim that f∗d,j ftd,j ≥ 1 CtγC t β from the inductive hypothesis. If we denote the set of documents where topic i is at least 1− 1/κ100 as D1, we get that\nβt+1i,j = β t i,j\n∑\nd f∗d,j ftd,j\nγtd,i ∑D\ni=1 γ t d,i\n≥\n∑\nd∈D1 1\n(Ctβ) 1/2Ctγ\nβ∗i,jγ ∗ d,i +\n∑\nd∈D\\D1 1\n(Ct β )2(Ctγ)\n2 β ∗ i,jγ ∗ d,i\n(Ctγ) ∑ d∈D γ ∗ d,i\nIf we denote µ = ∑ d∈D1 γ∗d,i ∑\nd∈D γ ∗ d,i , then\nβt+1i,j ≥ µ β∗i,j\n(Ctβ) 1/4(Ctγ)\n2 + (1− µ) β∗i,j (Ctβ) 2(Ctγ) 3\nSo, to prove βt+1i,j ≥ 1C3/4β β∗i,j , it’s sufficient to show\nµ β∗i,j\n(Ctβ) 1/4(Ctγ)\n2 + (1 − µ) β∗i,j (Ctβ) 2(Ctγ) 3 ≥ 1\nC 3/4 β\n⇔\nµ >\n1 (Ctβ) 1/2 − 1(Ctβ)2(Ctγ)3 1\n(Ctβ) 1/4(Ctγ) 2 − 1(Ctβ)2(Ctγ)3\nGiven that Ctγ ≤ (Ctβ)1/10, it’s sufficient to show\nµ >\n1 (Ctβ) 1/2 − 1(Ctβ)23/10 1 (Ctβ) 9/20 − 1(Ctβ)23/10 = 1− 1 (Ctβ) 9/20 − 1(Ctβ)1/2 1 (Ctβ) 9/20 − 1(Ctβ)23/10\nCompletely analogously as before, 1 − 1 (Ct β )9/20 − 1 (Ct β )1/2\n1\n(Ct β )9/20\n− 1 (Ct\nβ )23/10\nis a decreasing function of Ctβ , so it’s sufficient\nto check that µ > 1− 1 (Ct β )9/20 − 1 (Ct β )1/2\n1\n(Ct β )9/20\n− 1 (Ct\nβ )23/10\nwhen Ctβ = ( 1 1−ǫ ) 20, which is easily checked to be true.\nIn the same way, one can prove that βt+1i,j ≤ (Ctβ)3/4β∗i,j Putting lemmas 46 and 47 together, we get that the analogue of Lemma 14:\nLemma 48. Suppose it holds that 1Ct ≤ βi,j∗ βti,j ≤ Ct, Ct ≥ 1(1−ǫ)20 . Then, after one KL minimization step with respect to the γ variables and one β iteration, we get new values βt+1i,j that satisfy 1 Ct+1 ≤ βi,j∗\nβt+1i,j ≤ Ct+1,\nwhere Ct+1 = (Ct)3/4\nAs a corollary,\nCorollary 49. Phase III requires O(log( 1log(1+ǫ) )) = O(log( 1 ǫ )) iterations to estimate each of the topic-word matrix and document proportion entries to within a multiplicative factor of 1(1−ǫ)7\nThis finished the proof of Theorem 41 for Case Study 1.\nE.3 Generalizing Case Study 2\nFinally, the proof for Case Study 2 is quite simple. Because the dynamic range κ ≤ B for the common words, Lemmas 35 and 36 still hold, and hence we again determine the dominant topic correctly. Because of this, it’s also easy to see that the lower bounds and upper bounds on the βti,j values for the common words are maintained to be a constant, since the proof of Lemmas 32 and 33 holds for the common words verbatim. This means that the anchor words and discriminative words will be correctly determined just as before. But after that point, the analysis of Case Study 2 is exactly the same as the one for Case Study 2 — which we already covered in the above section. This finishes the proof of Theorem 41."
    }, {
      "heading" : "F Estimates on number of documents",
      "text" : "Finally, we state a few helper lemmas to estimate how many documents will be needed. The properties we need are that the empirical marginals of a dominating topic in the documents where it’s dominating are close to the actual ones, and similarly that the empirical marginals of the dominating topic, conditioned on the set of topics that a discriminative word belongs to not being present are close to the actual ones.\nThe former statement is the following:\nLemma 50. Let Ei = E[γ ∗ d,i|γ∗d,i is dominating]. If the total number of documents is D = Ω(K log 2 K ǫ2 ), and Di is the number of documents where i is the dominant topic, then with high probability, for all topics i,\n(1− ǫ)Ei ≤ 1\nDi\n∑\nd∈Di\nγ∗d,i ≤ (1 + ǫ)Ei\nProof. Since documents are generated independently, Pr[ 1Di ∑ d∈Di γ∗d,i > (1+ ǫ)Ei] ≤ e−\nǫ2DiEi 3 by Chernoff.\nSince there are at most T topics per document, Ei ≥ 1T , so Pr[ 1Di ∑ d∈Di γ∗d,i > (1 + ǫ)Ei] ≤ e−\nǫ2Di 3T\nAn analogous statement holds for Pr[ 1Di ∑ d∈Di γ∗d,i < (1 − ǫ)Ei]\nThen, if Di = log2 K ǫ2 , by union bounding, we get that with high probability, for all topics, (1 − ǫ)Ei ≤ 1 Di ∑ d∈Di γ∗d,i ≤ (1 + ǫ)Ei\nHowever, the probability of a topic being dominating is Ci/K for some constant Ci. So, by another Chernoff bound,\nPr[Di < (1 − ǫ)CiD/K] ≤ e− ǫ2CiD 3K (F.1)\nSo, if we take D = Kǫ2 log 2 K, with high probability, for all topics, Di = Θ(D/K). Putting everything together, we get that if D = K log 2 K\nǫ2 , with high probability,\n(1− ǫ)Ei ≤ 1\nDi\n∑\nd∈Di\nγ∗d,i ≤ (1 + ǫ)Ei\nNext, we calculate how many documents are needed to match the marginals of the dominating topics, conditioned on a small subset (of size o(K)) of the topics not being included in a document. More formally,\nLemma 51. For the discriminative word j, let jS be the set of topics it belongs to. For a topic i ∈ jS, let Let Ei,jS = E[γ ∗ d,i|γ∗d,i is dominating, γ∗d,i′ = 0, ∀i′ ∈ jS]. Let Di,jS be the number of documents where i is dominating, and γ∗d,i′ = 0, ∀i′ ∈ jS. If the number of documents D ≥ K log2 Nǫ2 , then with high probability, for all topics i and discriminative words j, (1− ǫ)Ei,jS ≤ 1Di,jS ∑ d∈Di,jS γ∗d,i ≤ (1 + ǫ)Ei,jS\nProof. Since Ei,jS = (1±o(1))Ei, by the weak topic correlation property, an analogous proof as above shows that if we get that if Di,jS =\nlog2 K ǫ2 , with high probability, (1− ǫ)EiS ≤ 1DiS ∑ d∈DiS γ∗d,i ≤ (1 + ǫ)EiS .\nBut by the independent topic inclusion property, the probability of generating a document D with i being the dominating topic, s.t. no topics in jS appear in it is Θ(1/K). So, again by Chernoff,\nPr[Di,jS < (1− ǫ)CiD/K] ≤ e− ǫ2CiD 3K (F.2)\nIf we take D = Kǫ2 log 2 N , Pr[Di,jS < (1− ǫ)CiD/K] ≤ e− log 2 N . However, since the total number of i, jS pairs is at most N2, union bounding, we get that with high probability, for all pairs i, jS,\n(1− ǫ)Ei,jS ≤ 1\nDi,jS\n∑\nd∈Di,jS\nγ∗d,i ≤ (1 + ǫ)Ei,jS\nFinally, the following short lemma to estimate the number of documents in which a word j belongs only to the dominating topic is implicit in the proof above:\nLemma 52. Let Di,jS be the number of documents where i is dominating, and γ ∗ d,i′ = 0, ∀i′ ∈ jS. If the number of documents D ≥ K log2 Nǫ2 , then with high probability, for all topics i and discriminative words j, Di,jS ≥ Di(1− ǫ)(1− o(1))"
    } ],
    "references" : [ {
      "title" : "Learning sparsely used overcomplete dictionaries via alternating minimization",
      "author" : [ "A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli" ],
      "venue" : "In Proceedings of The 27th Conference on Learning Theory (COLT),",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2013
    }, {
      "title" : "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation",
      "author" : [ "A. Anandkumar", "S. Kakade", "D. Foster", "Y. Liu", "D. Hsu" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning latent bayesian networks and topic models under expansion constraints",
      "author" : [ "A. Anandkumar", "D. Hsu", "A. Javanmard", "S. Kakade" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2013
    }, {
      "title" : "Computing a nonnegative matrix factorization–provably",
      "author" : [ "S. Arora", "R. Ge", "R. Kanna", "A. Moitra" ],
      "venue" : "In Proceedings of the forty-fourth annual ACM symposium on Theory of Computing,",
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning topic models – going beyond svd",
      "author" : [ "S. Arora", "R. Ge", "A. Moitra" ],
      "venue" : "In Proceedings of the 53rd Annual IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "A practical algorithm for topic modeling with provable guarantees",
      "author" : [ "S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Arora et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2013
    }, {
      "title" : "New algorithms for learning incoherent and overcomplete dictionaries",
      "author" : [ "S. Arora", "R. Ge", "A. Moitra" ],
      "venue" : "In Proceedings of The 27th Conference on Learning Theory (COLT),",
      "citeRegEx" : "Arora et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2014
    }, {
      "title" : "Simple, efficient, and neural algorithms for sparse coding",
      "author" : [ "S. Arora", "R. Ge", "T. Ma", "A. Moitra" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory (COLT),",
      "citeRegEx" : "Arora et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2015
    }, {
      "title" : "Statistical guarantees for the em algorithm: From population to sample-based analysis",
      "author" : [ "S. Balakrishnan", "M.J. Wainwright", "B. Yu" ],
      "venue" : "arXiv preprint arXiv:1408.2156,",
      "citeRegEx" : "Balakrishnan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Balakrishnan et al\\.",
      "year" : 2014
    }, {
      "title" : "A provable svd-based algorithm for learning topics in dominant admixture corpus",
      "author" : [ "T. Bansal", "C. Bhattacharyya", "R. Kannan" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Bansal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2014
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D. Blei", "A. Ng", "M. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "A two-round variant of em for gaussian mixtures",
      "author" : [ "S. Dasgupta", "L. Schulman" ],
      "venue" : "In Proceedings of Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Dasgupta and Schulman.,? \\Q2000\\E",
      "shortCiteRegEx" : "Dasgupta and Schulman.",
      "year" : 2000
    }, {
      "title" : "A probabilistic analysis of em for mixtures of separated, spherical gaussians",
      "author" : [ "S. Dasgupta", "L. Schulman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Dasgupta and Schulman.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dasgupta and Schulman.",
      "year" : 2007
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm",
      "author" : [ "A. Dempster", "N. Laird", "D. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "Dempster et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "Topic discovery through data dependent and random projections",
      "author" : [ "W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama" ],
      "venue" : "arXiv preprint arXiv:1303.3664,",
      "citeRegEx" : "Ding et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient distributed topic modeling with provable guarantees",
      "author" : [ "W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama" ],
      "venue" : "In Proceedings ot the 17th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Ding et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2014
    }, {
      "title" : "Stochastic variational inference",
      "author" : [ "M. Hoffman", "D. Blei", "J. Paisley", "C. Wan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2013
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "M. Jordan", "Z. Ghahramani", "T. Jaakkola", "L. Saul" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Jordan et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jordan et al\\.",
      "year" : 1999
    }, {
      "title" : "Clustering with spectral norm and the k-means algorithm",
      "author" : [ "A. Kumar", "R. Kannan" ],
      "venue" : "In Proceedings of Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Kumar and Kannan.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kumar and Kannan.",
      "year" : 2010
    }, {
      "title" : "Algorithms for non-negative matrix factorization",
      "author" : [ "D. Lee", "S. Seung" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Lee and Seung.,? \\Q2000\\E",
      "shortCiteRegEx" : "Lee and Seung.",
      "year" : 2000
    }, {
      "title" : "Phase retrieval using alternating minimization",
      "author" : [ "P. Netrapalli", "P. Jain", "S. Sanghavi" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Netrapalli et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Netrapalli et al\\.",
      "year" : 2013
    }, {
      "title" : "Complexity of inference in latent dirichlet allocation",
      "author" : [ "D. Sontag", "D. Roy" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Sontag and Roy.,? \\Q2000\\E",
      "shortCiteRegEx" : "Sontag and Roy.",
      "year" : 2000
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm",
      "author" : [ "R. Sundberg" ],
      "venue" : "Scandinavian Journal of Statistics,",
      "citeRegEx" : "Sundberg.,? \\Q1974\\E",
      "shortCiteRegEx" : "Sundberg.",
      "year" : 1974
    }, {
      "title" : "Dirichlet draws are sparse with high probability",
      "author" : [ "M. Telgarsky" ],
      "venue" : null,
      "citeRegEx" : "Telgarsky.,? \\Q2013\\E",
      "shortCiteRegEx" : "Telgarsky.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "The properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora et al.",
      "startOffset" : 130,
      "endOffset" : 155
    }, {
      "referenceID" : 10,
      "context" : "The assumptions on the topic priors are related to the well known Dirichlet prior, introduced to the area of topic modeling by (Blei et al., 2003).",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "The other one is an overlapping clustering algorithm, inspired by a work by (Arora et al., 2014) on dictionary learning, which is very simple and efficient.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "Among more classical results in this direction are the analyses of Lloyd’s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of Gaussians (Kumar and Kannan, 2010), (Dasgupta and Schulman, 2000), (Dasgupta and Schulman, 2007).",
      "startOffset" : 174,
      "endOffset" : 198
    }, {
      "referenceID" : 11,
      "context" : "Among more classical results in this direction are the analyses of Lloyd’s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of Gaussians (Kumar and Kannan, 2010), (Dasgupta and Schulman, 2000), (Dasgupta and Schulman, 2007).",
      "startOffset" : 200,
      "endOffset" : 229
    }, {
      "referenceID" : 12,
      "context" : "Among more classical results in this direction are the analyses of Lloyd’s algorithm for K-means, which is very closely related to the EM algorithm for mixtures of Gaussians (Kumar and Kannan, 2010), (Dasgupta and Schulman, 2000), (Dasgupta and Schulman, 2007).",
      "startOffset" : 231,
      "endOffset" : 260
    }, {
      "referenceID" : 8,
      "context" : "The recent work of (Balakrishnan et al., 2014) also characterizes global convergence properties of the EM algorithm for more general settings.",
      "startOffset" : 19,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "(Agarwal et al., 2013), (Arora et al.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : ", 2013), (Arora et al., 2015) prove that with appropriate initialization, alternating minimization can provably recover the ground truth.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 20,
      "context" : "(Netrapalli et al., 2013) have proven similar results in the context of phase retreival.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "Another popular heuristic which has so far eluded such attempts is known as variational inference (Jordan et al., 1999).",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "We provide the first characterization of global convergence of variational inference based algorithms for topic models (Blei et al., 2003).",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "The expectation-maximization (EM) algorithm is an iterative method to achieve this, dating all the way back to (Dempster et al., 1977) and (Sundberg, 1974) in the 70s.",
      "startOffset" : 111,
      "endOffset" : 134
    }, {
      "referenceID" : 22,
      "context" : ", 1977) and (Sundberg, 1974) in the 70s.",
      "startOffset" : 12,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "3 Topic models We will focus on a particular latent variable model, which is very often studied - topic models (Blei et al., 2003).",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "(Originally introduced by (Blei et al., 2003).",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : ", 2012b),(Arora et al., 2013), as well as (Anandkumar et al.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : ", 2013), as well as (Anandkumar et al., 2013), (Ding et al.",
      "startOffset" : 20,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : ", 2013), (Ding et al., 2013), (Ding et al.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 15,
      "context" : ", 2013), (Ding et al., 2014) and (Bansal et al.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : ", 2014) and (Bansal et al., 2014).",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : ", 2012b) and (Arora et al., 2013) assume that the topic-word matrix contains “anchor words”.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "(Anandkumar et al., 2013) on the other hand work with a certain expansion assumption on the word-topic graph, which says that if one takes a subset S of topics, the number of words in the support of these topics should be at least |S|+ smax, where smax is the maximum support size of any topic.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "(Similar to the expansion in (Anandkumar et al., 2013), but only over constant sized subsets.",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "(Originally introduced by (Blei et al., 2003).",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : ") The documents will also have a “dominating topic”, similarly as in (Bansal et al., 2014).",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "4 Variational relaxation for learning topic models In this section we briefly review the variational relaxation for topic models following closely the description in (Blei et al., 2003).",
      "startOffset" : 166,
      "endOffset" : 185
    }, {
      "referenceID" : 21,
      "context" : "For topic models variational updates are way to approximate the computationally intractable E-step (Sontag and Roy, 2000) as described in Section 2.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "In (Blei et al., 2003) it’s shown that for Dirichlet priors α the optimal distributions q, q j are a Dirichlet distribution for q, with some parameter γ̃, and multinomials for q j , with some parameters φj .",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "The way we derived them, these updates appear to be an approximate form of the variational updates in (Blei et al., 2003).",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "It is intuitively clear that in the large document limit, this approximation should not be much worse than the one in (Blei et al., 2003), as the posterior concentrates around the maximum likelihood value.",
      "startOffset" : 118,
      "endOffset" : 137
    }, {
      "referenceID" : 19,
      "context" : "In a slightly modified form, these updates were used in a paper by (Lee and Seung, 2000) in the context of non-negative matrix factorization.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "These are analogues of distributions that have been analyzed for dictionary learning (Arora et al., 2015).",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 23,
      "context" : "This was formally proven by (Telgarsky, 2013) - though sparsity there means a small number of large coordinates.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "Such assumptions also appear in the context of dictionary learning (Arora et al., 2015).",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "This uses an idea inspired by (Arora et al., 2014) in the setting of dictionary learning.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "A similar assumption (a small fraction of almost pure documents) appeared in a recent paper by (Bansal et al., 2014).",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 23,
      "context" : "1 Sparsity To characterize the sparsity of the topic proportions in a document, we will need the following lemma from (Telgarsky, 2013): Lemma 38.",
      "startOffset" : 118,
      "endOffset" : 135
    }, {
      "referenceID" : 23,
      "context" : "(Telgarsky, 2013) For a Dirichlet distribution with parameters (C1/k , C2/k , .",
      "startOffset" : 0,
      "endOffset" : 17
    } ],
    "year" : 2015,
    "abstractText" : "Variational inference is a very efficient and popular heuristic used in various forms in the context of latent variable models. It’s closely related to Expectation Maximization (EM), and is applied when exact EM is computationally infeasible. Despite being immensely popular, current theoretical understanding of the effectiveness of variaitonal inference based algorithms is very limited. In this work we provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models. More specifically, we show that variational inference provably learns the optimal parameters of a topic model under natural assumptions on the topic-word matrix and the topic priors. The properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora et al., 2012b). The assumptions on the topic priors are related to the well known Dirichlet prior, introduced to the area of topic modeling by (Blei et al., 2003). It is well known that initialization plays a crucial role in how well variational based algorithms perform in practice. The initializations that we use are fairly natural. One of them is similar to what is currently used in LDA-c, the most popular implementation of variational inference for topic models. The other one is an overlapping clustering algorithm, inspired by a work by (Arora et al., 2014) on dictionary learning, which is very simple and efficient. While our primary goal is to provide insights into when variational inference might work in practice, the multiplicative, rather than the additive nature of the variational inference updates forces us to use fairly non-standard proof arguments, which we believe will be of general interest. Our proofs rely on viewing the updates as an operation which, at each timestep, sets the new parameter estimates to be noisy convex combinations of the ground truth values, and a bounded error term which depends on the previous estimate. The weight on the ground truth values will be large, compared to the error term, which will cause the error term to eventually reach zero. The large weight on the ground truth values will be a byproduct of our model assumptions, which will imply a “local” notion of anchor words for each document words which only appear in one topic in a given document, as well as a “local” notion of anchor documents for each word documents where that word appears as part of a single topic. Princeton University, Computer Science Department. Email: pawashti@cs.princeton.edu. Supported by NSF grant CCF1302518. Princeton University, Computer Science Department. Email: risteski@cs.princeton.edu. Partially supported by NSF grants CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Sanjeev Arora’s Simons Investigator Award, and a Simons Collaboration Grant.",
    "creator" : "LaTeX with hyperref package"
  }
}