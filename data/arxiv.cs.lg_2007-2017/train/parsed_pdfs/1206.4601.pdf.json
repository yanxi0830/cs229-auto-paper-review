{
  "name" : "1206.4601.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Convex Multitask Learning with Flexible Task Clusters",
    "authors" : [ "Leon Wenliang Zhong", "James T. Kwok" ],
    "emails" : [ "WZHONG@CSE.UST.HK", "JAMESK@CSE.UST.HK" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Many real-world problems involve the learning of a number of tasks. Instead of learning them individually, it is now well-known that better generalization performance can be obtained by harnessing the intrinsic task relationships and allowing tasks to borrow strength from each other. In recent years, a number of techniques have been developed under this multitask learning (MTL) framework.\nTraditional MTL methods assume that all the tasks are related (Evgeniou & Pontil, 2004; Evgeniou et al., 2005).\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nHowever, when this assumption does not hold, the performance can be even worse than single-task learning. If it is known that the tasks are clustered, a simple remedy is to constrain task sharing to be just within the same cluster (Argyriou et al., 2008; Evgeniou et al., 2005). This can be further extended to the case where task relationships are represented in the form of a network (Kato et al., 2007). However, in practice, such an explicit knowledge of task clusters/network is rarely available.\nRecently, by adopting different modeling assumptions, a number of approaches have been proposed that identify task relationships simultaneously with parameter learning. For example, some assume that the task parameters share a common prior in a Bayesian model (Yu et al., 2005; Zhang & Schneider, 2010; Zhang & Yeung, 2010); that the data follows a dirty model (Jalali et al., 2010); that most of the tasks lie in a low-dimensional subspace (Ando & Zhang, 2005; Chen et al., 2010), or that outlier tasks are present (Chen et al., 2011a). In this paper, we will mainly be interested in techniques that assume the tasks are clustered (Argyriou et al., 2008; Evgeniou et al., 2005), and then infer the clustering structure automatically during learning (Jacob et al., 2008; Kang et al., 2011). Interestingly, it is recently shown that this clustered MTL approach is equivalent to alternating structure optimization (Ando & Zhang, 2005) that assumes the tasks share a low-dimensional structure (Zhou et al., 2011).\nHowever, all the existing methods model the task relationships at the task level, and the features are assumed to always observe the same set of task clusters or covariance structure (Figure 1(a)). This may be restrictive in some real-world applications. For example, in recommender systems, each customer corresponds to a task and each feature a movie attribute. Suppose that we have a relatively coherent group of customers, such as Jackie Chan fans who are interested in action comedy movies (Figure 1(b)). On the “language” attribute, however, some of them may prefer English, some prefer standard Chinese (Putonghua/Mandarin), some prefer Cantonese or even a combination of these. Hence, the clustering structure as seen by this feature is very different from those of the oth-\ners. Another example is when the features are obtained by some feature extraction algorithm (such as PCA) and so have different discrimination abilities. While the less discriminating features may be used in a similar manner by all the tasks, highly discriminating features may be very class-specific and are used differently by different tasks (Figure 1(c)). Hence, again, these features may observe different task relationships. This phenomenon will also be demonstrated in the experiments in Section 3.\nIn this paper, we extend clustered MTL such that the task cluster structure can vary from feature to feature. This is thus more fine-grained than existing MTL methods that only capture task-level (but not feature-level) relationships. Moreover, a key difference with (Jacob et al., 2008) is that we do not require the number of clusters to be pre-specified. Indeed, depending on the complexity of the tasks and usefulness of each feature, different numbers of clusters can be formed for different features.\nComputationally, the optimization problem is often challenging in clustered MTL algorithms. For example, in (Kang et al., 2011), it leads to a mixed integer program, which has to be relaxed as a nonlinear optimization problem and then solved by gradient descent. This suffers from the local minimum problem and potentially slow convergence. On the other hand, the proposed approach directly leads to a (strongly) convex optimization problem, which can then be efficiently solved by accelerated proximal methods (Nesterov, 2007) after some transformations.\nNotation: Vector/matrix transpose is denoted by the superscript ′, ‖A‖F = √ trace(A′A) is the Frobenius norm of matrix A, Ai· is its ith row and A·j its jth column."
    }, {
      "heading" : "2. The Model",
      "text" : "Suppose that there are T tasks. The tth task has nt training samples {(x(t)1 , y (t) 1 ), . . . , (x (t) nt , y (t) nt )}, with input x (t) i ∈ RD and output y(t)i ∈ R. We stack the inputs and outputs together to form matrices X(t) = [x(t)1 , . . . ,x (t) nt ] ′ and y(t) = [y (t) 1 , . . . , y (t) nt ] ′, respectively. A linear model is used to learn each task. Let the weight associated with task t be wt. The predictions on the nt samples are stored in the vector X(t)wt."
    }, {
      "heading" : "2.1. Simultaneous Clustering of Task Parameters",
      "text" : "We decompose each wt into ut+vt, where ut tries to capture the shared clustering structure among task parameters, and vt captures variations specific to each task. Learning of wt’s is performed jointly with the clustering of ut’s via the following regularized risk minimization problem:\nminU,V T∑ t=1 ‖y(t) −X(t)(ut + vt)‖2 + λ1‖U‖clus\n+λ2‖U‖2F + λ3‖V‖2F , (1)\nwhere U = [u1, . . . ,uT ] and V = [v1, . . . ,vT ], and λ1, λ2, λ3 are regularization parameters. The first term in (1) is the empirical (squared) loss on the training data, and ‖U‖clus is the sum of pairwise differences for elements in each row of U,\n‖U‖clus = D∑ d=1 ∑ i<j |Udi − Udj |. (2)\nFor each feature d and each (ui,uj) pair, the pairwise penalty in ‖U‖clus encourages Udi, Udj to be close together, leading to feature-specific task clusters. It can also be shown that ‖U‖clus is a convex relaxation of k-means clustering on each feature. Note that this is different from the fused lasso regularizer (Tibshirani et al., 2005), which is used for clustering features in single-task learning while ‖U‖clus is for clustering tasks in MTL. It is also different from the graph-guided fused lasso (GFlasso) (Chen et al., 2011b), which does not decompose wt as ut + vt, and subsequently cannot cluster the tasks due to the use of smoothing. The regularizer ‖V‖2F = ∑T t=1 ‖vt‖2 penalizes the deviations of each wt from ut, and ‖U‖2F is the usual ridge regularizer penalizing U’s complexity. Since ‖U‖2F , ‖V‖2F are strongly convex and the other terms in (1) are convex, (1) is a strongly convex optimization problem.\nSome MTL papers also decompose wt as ut + vt, but the formulations and goals are different from ours. In (Evgeniou et al., 2005), ut is the (single) cluster center of all the tasks; in (Ando & Zhang, 2005; Chen et al., 2010; 2011a),\nut comes from a low-dimensional linear subspace, which is extended to a nonlinear manifold in (Agarwal et al., 2010); in (Jalali et al., 2010), ut is the component that uses features shared by other tasks.\nMoreover, model (1) encompasses a number of interesting special cases: (i) λ1 → ∞:1 For each d, all Udt’s become the same. Thus, wt reduces to ū + vt for some “mean weight” ū, and (1) reduces to the model in (Evgeniou et al., 2005). (ii) λ1 = 0: The following Proposition shows that (1) reduces to independent ridge regression on each task. Proposition 1. When λ1 = 0, model (1) reduces to minwt ‖y(t) −X(t)wt‖2 + λ2λ3λ2+λ3 ‖wt‖ 2, t = 1, . . . , T .\n(iii) λ2 6= 0, λ3 = 0: Since ut is penalized while vt is not, ut will become zero at optimality, irrespective of the value of λ1. Thus, (1) reduces to independent least squares regression on each task: minwt ‖y(t) − X(t)wt‖2. Obviously, this is the same as setting λ1 = λ2 = λ3 = 0."
    }, {
      "heading" : "2.2. Properties",
      "text" : "Denote the optimal solution in (1) by (U∗,V∗), and let W∗ ≡ U∗ + V∗. The following Proposition shows that if tasks i and j have similar weights on feature d, the corresponding U∗ entries are clustered together. On the other hand, for an outlier task t, its ut component is separated from the main group. Proposition 2. If |W ∗di −W ∗dj | < λ1 λ3 , then U∗di = U ∗ dj . If |W ∗di −W ∗dj | > (T − 1) λ1 λ3 , then U∗di 6= U∗dj .\nFor simplicity, all T tasks are assumed to have the same number of training instances n. Assume that the data for task t is generated as y(t) = X(t)w̆t + , where ∼ N (0, σ2I) is the i.i.d. Gaussian noise, and ‖X(t)·i ‖ ≤ √ 2n. The following Theorem shows that, with high probability, W∗ is close to the ground truth W̆ = [w̆1, . . . , w̆T ] w.r.t. the elementwise `∞-error ‖W̆ − W∗‖∞,∞ = maxd=1,...,D maxt=1,...,T |W̆dt − Wdt|. Moreover, when all the tasks are identical, the shared clustering component U∗ is close to W̆; and V∗, the deviation from the cluster center, goes to zero.\nTheorem 1. 1. ‖W̆ − W∗‖∞,∞ ≤ c1 √ Λ̃maxσ2\nn + C\nholds with probability at least 1−2 exp(−c2 log(DT )) for any c1 > √ (1 + c2) log(DT ) and c2 > 0. Here, Λ̃max = maxt Λ̃ (t) max with Λ̃ (t) max being an upper bound on the eigen-\nvalue of Σ(t) = (\n1 nX (t)′X(t) + λ2λ32n(λ2+λ3)I )−1 , and C =\nmaxt\n( λ2λ3\n2n(λ2+λ3) ∥∥∥Σ(t)W̆·t∥∥∥ ∞ + (T−1)λ1λ32n(λ2+λ3) ∥∥Σ(t)∥∥∞,1).\nIf n → ∞, 1nX (t)′X(t) → C(t) where C(t) is a posi-\ntive definite matrix, and λ1, λ2, λ3 = O( √ n), then\n1Indeed, λ1 is only required to be sufficiently large. The precise statement is in Proposition 2.\nΣ(t) → [C(t)]−1 and ‖W∗ − W̆‖∞,∞ ≤ O (\n1√ n\n) with\narbitrary high probability.\n2. When all tasks are identical (i.e., w̆1 = · · · = w̆T and C(1) = · · · = C(T )), λ3λ2 → ∞ and λ1 →\n∞, we have ‖W∗ − W̆‖∞,∞ ≤ ĉ1 √ Λ̂maxσ2\nnT + Ĉ\nand ‖V‖∞,∞ → 0 hold with probability at least 1 − 2 exp(−ĉ2 logD) for any ĉ1 > √ 2(1 + ĉ2) log(D) and ĉ2 > 0. Here, Λ̂max is an upper bound on the eigen-\nvalue of Σ̂ =  1nT X (1)\n... X(T )\n ′ X (1)\n... X(T )\n+ λ22nI  −1 , and\nĈ = λ22n ∥∥∥Σ̂W̆·1∥∥∥ ∞ + o(1). If n → ∞, λ1, λ3 = O(n2) and λ2 = O( √ n), then ‖W∗−W̆‖∞,∞ ≤ O ( 1√ nT ) with arbitrary high probability.\nMoreover, the following Corollary shows that the underlying clustering structure can be exactly recovered when n is sufficiently large.\nCorollary 1. Suppose that for any feature d, W̆di = W̆dj if i, j are in the same cluster; and |W̆di − W̆dj | ≥ ρ otherwise. Assume that 12 ∥∥∥Σ(t)W̆·t∥∥∥ ∞ ≤\nC1 and T−12 ∥∥Σ(t)∥∥∞,1 ≤ C2. Then for n ≥[\n2T ρ ( c1 √ Λ̃maxσ2 + k2k3C1 k2+k3 + k1k3C2k2+k3 )]2 , where λ1 =\nk1 √ n, λ2 = k2 √ n, λ3 = k3 √ n and k1k3 = ρ T , we have U∗di = U ∗ dj if i, j are in the same cluster; and U∗di 6= U∗dj otherwise, with probability at least 1 − 2 exp(−c2 log(DT )) for any c1 > √ (1 + c2) log(DT ) and c2 > 0."
    }, {
      "heading" : "2.3. Optimization via Accelerated Proximal Method",
      "text" : "In recent years, accelerated proximal methods (Nesterov, 2007) have been popularly used by the machine learning community (Bach et al., 2011) for convex problems of the form minθ f(θ)+r(θ), where f(θ) is convex and smooth, and r(θ) is convex but nonsmooth. The convergence rate is optimal for the class of first-order methods. Together with their algorithmic and implementation simplicities, they can be used on large smooth/nonsmooth convex problems.\nIn this paper, we use the well-known method of FISTA (Fast Iterative Shrinkage-Thresholding Algorithm) (Beck & Teboulle, 2009). Extending to other accelerated proximal methods is straightforward. Each FISTA iteration performs the following proximal step\nmin θ f(θ̃k) + (θ − θ̃k)′∇f(θ̃k) + Lk 2 ‖θ − θ̃k‖2F + r(θ), (3) where θ̃k is the current iterate, and Lk is a scalar often\ndetermined by line search. Since (3) is required in every FISTA iteration, it needs to be solved very efficiently.\nFor problem (1), let Θ = [U′,V′]′. Define\nf(Θ) = T∑ t=1 ‖y(t) −X(t)(ut + vt)‖2, (4) r(Θ) = λ1‖U‖clus + λ2‖U‖2F + λ3‖V‖2F .\nStep (3) can be rewritten as minΘ ‖Θ − Θ̂‖2F + 2Lk r(Θ), where Θ̂ = [Û′, V̂′]′ = Θ̃k − 1Lk∇f(Θ̃k) (Chen et al., 2011a). Expressing back in terms of U and V, (3) becomes\nminU,V ‖U− Û‖2F + λ̂1‖U‖clus + λ̂2‖U‖2F +‖V − V̂‖2F + λ̂3‖V‖2F , (5)\nwhere λ̂i = 2λiLk (i = 1, 2, 3) and\nÛ = Ũk− 1\nLk ∂Uf(Θ̃k), V̂ = Ṽk−\n1\nLk ∂Vf(Θ̃k). (6)\nAs f(Θ) in (4) is simply the squared loss, the tth columns of both ∂Uf(Θ̃k) and ∂Vf(Θ̃k) can be easily obtained as 2(X(t))′(X(t)[Θ̃k]·t − y(t)). Since f in the proximal step is only required to be convex and smooth, many other commonly used loss functions can be used in (1) instead.\nAs U and V are now decoupled, they can be optimized independently as will be shown in the sequel. The whole algorithm for solving (1) is shown in Algorithm 1.\nAlgorithm 1 Algorithm for solving (1). 1: Initialize: Ũ1, Ṽ1, τ1 ← 1. 2: for k = 1, 2, . . . , N − 1 do 3: Compute Û and V̂ in (6). 4: Uk ← arg minU ‖U − Û‖2F + λ̂1‖U‖clus +\nλ̂2‖U‖2F using the algorithm in (Zhong & Kwok, 2011). 5: Vk ← [ v̂ij\n1+λ̂3\n] .\n6: τk+1 ← 1+ √ 1+4τ2k 2 .\n7:\n[ Ũk+1\nṼk+1 ] ← [ Uk Vk ] + τk−1τk+1 ([ Uk Vk ] − [ Uk−1 Vk−1 ]) .\n8: end for 9: Output UN ."
    }, {
      "heading" : "2.3.1. COMPUTING V",
      "text" : "For fixed U, the subproblem in (5) related to V is minV ‖V − V̂‖2F + λ̂3‖V‖2F . On setting the gradient of the objective w.r.t. V to zero, we obtain V = [ v̂ij\n1+λ̂3\n] ."
    }, {
      "heading" : "2.3.2. COMPUTING U",
      "text" : "For fixed V, the subproblem in (5) related to U is minU ‖U−Û‖2F + λ̂1‖U‖clus+ λ̂2‖U‖2F . Because of the\nO(T 2) number of terms in ‖U‖2F , this is more challenging than the computing of V in Section 2.3.1. However, as the rows of U are independent, U can be optimized row by row. For the dth row, we have\nmin u ‖u− û‖2 + λ̂1 ∑ i<j |ui − uj |+ λ̂2‖u‖2, (7)\nwhere û = Ûd· = [û1, . . . , ûT ]′. It can be shown that (7) can be rewritten as the optimization problem considered in (Zhong & Kwok, 2011), and hence can be solved efficiently using the algorithm proposed there."
    }, {
      "heading" : "2.3.3. TIME COMPLEXITY",
      "text" : "Computing the gradients ∂Uf(Θ̃k) and ∂Vf(Θ̃k) takes O(nDT ) time. Computing Vk takes O(DT ) time. Computing one row of Uk using the algorithm in (Zhong & Kwok, 2011) takes O(T log T ) time, and thus O(DT log T ) time for the whole Uk. Hence, the total complexity for Algorithm 1 is onlyO(TDn+DT log T ). Moreover, FISTA converges as O(1/N2) (Beck & Teboulle, 2009), where N is the number of iterations. This is much faster than traditional gradient methods, which converges as O(1/ √ N). It is also faster than GFlasso (Chen et al., 2011b), which solves a similar problem as (1), but converges as O(1/N) and has a per-iteration complexity of O(T 2).\nThough (7) is similar to the optimization problems of the pairwise fused lasso in (Petry et al., 2011; She, 2010), using the optimization procedures there are much more expensive. Specifically, the procedure in (Petry et al., 2011) takes O(T 6) time, as it involves a QP with ( T 2 ) additional optimization variables; while (She, 2010) relies on annealing, which is even more complicated and expensive."
    }, {
      "heading" : "2.4. Adaptive Clustering",
      "text" : "As in the adaptive lasso (Zou, 2006), weights can be added to each term of ‖U‖clus as ∑D d=1 ∑ ĩ<j̃ αd,̃ij̃ |Udĩ − Udj̃ |, where αd,̃ij̃ is the weight associated with the ith and jth largest entries (Udĩ and Udj̃ , respectively) on the dth row of U. To set the weights αd,̃ij̃ , we first run model (1) with the unweighted ‖U‖clus to obtain W, and then set αd,̃ij̃ =\n1 |Wdĩ−Wdj̃ | . Hence, when Wdĩ,Wdj̃ are similar, Udĩ, Udj̃ will be strongly encouraged to be clustered together, and vice verse. Moreover, the optimization procedure in Algorithm 1 can still be used."
    }, {
      "heading" : "3. Experiments",
      "text" : "In this section, we perform experiments on a number of synthetic and real-world data sets. All the data sets are standardized such that the features have zero mean and unit variance for each task. The output of each task is also standardized to have mean zero."
    }, {
      "heading" : "3.1. Synthetic Data Sets",
      "text" : "In this experiment, the input has dimensionality D = 30 and is generated from the multivariate normal distribution x ∼ N (0, I). We use T = 10 tasks, with the output of the tth task generated as yt ∼ x′w̆t + N (0, 400). All tasks have 30 training samples and 100 test samples. The task parameters are designed in the following manner to mimic various real-world scenarios:\n(C1) All tasks are independent: w̆t ∼ N (0, 25I) for all t.\n(C2) All tasks are from the same cluster: w̆t = wm + N (0, I) for all t.\n(C3) All tasks are from the same cluster as in C2, but with corrupted features as are often encountered in real-world data sets. We first generate w̆t ∼ wm + N (0, I) for all t. Then, for each feature, we randomly pick one task and replace its weight by a random number from 10 +N (0, 100).\n(C4) A main task cluster plus a few outlier tasks: w̆t ∼ {\nwm +N (0, I) t = 1, 2, 3, 4, 5, 6, 7, 8, 10 · 1 +N (0, 100I) t = 9, 10.\n(C5) Tasks in overlapping groups: We have two groups with weights w(1),w(2). For each feature d, several tasks (1-9) are randomly assigned to group 1, and the rest to group 2. Suppose that task t belongs to group g, we then generate [w̆t]i ∼ [w(g)]i +N (0, 1).\n(C6) This is used to simulate the recommender systems example in Section 1. All but the last two features are generated from a common cluster, as [w̆t]i ∼ [wm]i +N (0, 1). For the last two features, we generate [w̆t]i ∼ 10 +N (0, 100) for each task t.\nThe proposed model will be called FlexTClus (Flexible Task-Clustered MTL). It is compared with a variety of single-task and state-of-the-art MTL algorithms, including: 1) Independent ridge regression on each task; 2) Pooling all the training data together to learn a single model:\nThis assumes that all the tasks are identical; 3) Regularized MTL: This assumes that all the tasks come from a single cluster (Evgeniou & Pontil, 2004); 4) The dirty model in (Jalali et al., 2010); 5) Low-rank-based robust MTL (Chen et al., 2011a); 6) Sparse-LowRank MTL (Chen et al., 2010), which learns sparse and low-rank patterns from the tasks; 7) Clustered MTL (Jacob et al., 2008)2 and 8) Multitask relationship learning (MTRL) (Zhang & Yeung, 2010).\nRegularization parameters for all the methods are tuned by a validation set of size 100. To reduce statistical variability, results are averaged over 10 repetitions. In each repetition, wm is generated from N (0, 25I); whereas in C5, w(1) ∼ N (0, 25I) and w(2) ∼ N (0, 100I). The normalized mean squared error (NMSE), which is defined as the MSE divided by the variance of the ground truth, is used for performance evaluation.\nResults are shown in Table 1. We have the following observations.\n• C1: Since the tasks are independent, so as expected, ridge gives good result, while pooling is the worst. Recall that FlexTClus can be reduced to ridge regression with a suitable choice of regularization parameters. Hence, both versions of FlexTClus are as good as ridge. Similarly, regularized MTL can also be reduced to ridge regression by using a very strong regularizer on the task mean parameter. As for clustered MTL, since the true number of clusters is given (which is equal to the number of tasks in this case), it reduces to ridge regression and so the result is also good. On the other hand, the remaining MTL methods suffer from negative transfer.\n• In C2, all tasks are from the same group, and hence regularized MTL and FlexTClus (which can be reduced to regularized MTL) perform best. This is followed by pooling, while the other MTL methods\n2The clustered MTL algorithm of (Jacob et al., 2008) requires the number of task clusters as input. This is set to be the ground truth in the experiment. Hence, results obtained for this method can be overly optimistic.\nlag further behind and suffer from negative transfer. When noisy features are added (C3), pooling suffers tremendously, while FlexTClus still retains its superior performance.\n• C4 is a common MTL setup. As expected, almost all MTL methods perform well.\n• C5 and C6 are the most challenging. FlexTClus (and its adaptive variant) is the only method that can capture the complicated feature-wise task relationships.\nFigure 2 compares the ground truth clustering structures of the task parameters with those obtained by adaptive FlexTClus. As can be seen, FlexTClus can well capture the underlying structure."
    }, {
      "heading" : "3.2. Examination Score Prediction",
      "text" : "In this section, experiment is performed on the school data set (Bakker & Heskes, 2003). As in (Chen et al., 2011a), we use 10%, 20% and 30% of the data for training, another 45% for testing, and the remaining for validation. To reduce statistical variability, results are averaged over 5 repetitions.\nResults are shown in Table 2. Note that though the school data has been popularly used as a MTL benchmark, it has been pointed out previously that all the tasks are indeed the same (Bakker & Heskes, 2003; Evgeniou et al., 2005).\nHence, the trend in Table 2 is similar to that of C2 in Table 1. As can be seen, both versions of FlexTClus are very competitive in this single-cluster case, and are better than the other MTL methods. Figure 3 shows the task clustering structure obtained by adaptive FlexTClus. Clearly, it indicates that there is only one underlying task cluster."
    }, {
      "heading" : "3.3. Handwritten Digit Recognition",
      "text" : "In this section, we perform experiments on two popular handwritten digits data sets, USPS and MNIST. As in (Kang et al., 2011), PCA is used to reduce the feature dimensionality to 64 for USPS and 87 for MNIST. For each digit, we randomly choose 10, 30, 50 samples for training, 500 samples for validation and another 500 samples for testing. The 10-class classification problem is decomposed into 10 one-vs-rest binary problems, each of which is treated as a task.\nResults averaged over 5 repetitions are shown in Table 3. We do not compare with pooling, which assumes that all the tasks are identical and is clearly invalid in this one-vsrest setting. As can be seen, FlexTClus and its adaptive version are consistently among the best, while many other MTL methods suffer from negative transfer and are only comparable or even worse than ridge regression. Fig. 4 shows the task clustering structures obtained. As expected, many trailing PCA features are not useful for discrimination and the corresponding weights are zero. In contrast,\nthe leading PCA features are more discriminative and are used by the different tasks in different manners, leading to more varied cluster structures."
    }, {
      "heading" : "3.4. Rating of Products",
      "text" : "In this section, we use the computer survey data in (Argyriou et al., 2008). This contains the ratings of 201 students on 20 different personal computers, each described by 13 attributes. After removing the invalid ratings and students with more than 8 zero ratings, we are left with 172 students (tasks). For each task, we randomly split the 20 instances into training, validation and test sets of sizes 8,8, and 4, respectively.\nTable 4 shows the root mean squared error (RMSE) averaged over 10 random splits. Again, FlexTClus and its adaptive variant outperform the other models. Figure 5 shows the task clustering structure obtained in a typical run. Note that the first 12 features are about the PC’s performance (such as memory and CPU speed). As can be seen, there is one main cluster, indicating that most students in this survey have similar preference on these attributes. On the other hand, the last feature is price, and the result indicates that there are lots of varied opinions on this attribute."
    }, {
      "heading" : "4. Conclusion and Future Work",
      "text" : "While existing MTL methods can only model task relationships at the task level, we introduced in this paper\na novel MTL formulation that captures task relationships at the feature-level. Depending on the myriad relationships among tasks and features, the proposed method can cluster tasks in a flexible feature-by-feature manner, without even the need of pre-specifying the number of clusters. Moreover, the proposed formulation is (strongly) convex, and can be solved by accelerated proximal methods with an efficient and scalable proximal step. Experiments on a number of synthetic and real-world data sets show that the proposed method is accurate. The obtained feature-specific task clustering structure also agrees with the known/plausible clustering structure of the tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grant 614311)."
    } ],
    "references" : [ {
      "title" : "Learning multiple tasks using manifold regularization",
      "author" : [ "A. Agarwal", "H. Daumé III", "S. Gerber" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2010
    }, {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "R.K. Ando", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ando and Zhang,? \\Q2005\\E",
      "shortCiteRegEx" : "Ando and Zhang",
      "year" : 2005
    }, {
      "title" : "Convex multitask feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Argyriou et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Argyriou et al\\.",
      "year" : 2008
    }, {
      "title" : "Convex optimization with sparsity-inducing norms",
      "author" : [ "F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski" ],
      "venue" : "In Optimization for Machine Learning,",
      "citeRegEx" : "Bach et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2011
    }, {
      "title" : "Task clustering and gating for Bayesian multitask learning",
      "author" : [ "B. Bakker", "T. Heskes" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bakker and Heskes,? \\Q2003\\E",
      "shortCiteRegEx" : "Bakker and Heskes",
      "year" : 2003
    }, {
      "title" : "A fast iterative shrinkagethresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "Beck and Teboulle,? \\Q2009\\E",
      "shortCiteRegEx" : "Beck and Teboulle",
      "year" : 2009
    }, {
      "title" : "Learning incoherent sparse and low-rank patterns from multiple tasks",
      "author" : [ "J. Chen", "J. Liu", "J. Ye" ],
      "venue" : "In Proceedings of the 16th International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Chen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2010
    }, {
      "title" : "Integrating low-rank and group-sparse structures for robust multi-task tearning",
      "author" : [ "J. Chen", "J. Zhou", "J. Ye" ],
      "venue" : "In Proceedings of the 17th International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Chen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2011
    }, {
      "title" : "Smoothing proximal gradient method for general structured sparse learning",
      "author" : [ "X. Chen", "Q. Lin", "S. Kim", "J.G. Carbonell", "E.P. Xing" ],
      "venue" : "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Chen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2011
    }, {
      "title" : "Regularized multi-task learning",
      "author" : [ "T. Evgeniou", "M. Pontil" ],
      "venue" : "In Proceedings of the 10th International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Evgeniou and Pontil,? \\Q2004\\E",
      "shortCiteRegEx" : "Evgeniou and Pontil",
      "year" : 2004
    }, {
      "title" : "Learning multiple tasks with kernel methods",
      "author" : [ "T. Evgeniou", "C.A. Micchelli", "M. Pontil" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Evgeniou et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Evgeniou et al\\.",
      "year" : 2005
    }, {
      "title" : "Clustered multi-task learning: A convex formulation",
      "author" : [ "L. Jacob", "F. Bach", "J. Vert" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Jacob et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Jacob et al\\.",
      "year" : 2008
    }, {
      "title" : "A dirty model for multi-task learning",
      "author" : [ "A. Jalali", "P. Ravikumar", "S. Sanghavi", "C. Ruan" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Jalali et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jalali et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning with whom to share in multi-task feature learning",
      "author" : [ "Z. Kang", "K. Grauman", "K. Sha" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "Kang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2011
    }, {
      "title" : "Multitask learning via conic programming",
      "author" : [ "T. Kato", "H. Kashima", "M. Sugiyama", "K. Asai" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kato et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kato et al\\.",
      "year" : 2007
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Technical Report 76, Catholic University of Louvain,",
      "citeRegEx" : "Nesterov,? \\Q2007\\E",
      "shortCiteRegEx" : "Nesterov",
      "year" : 2007
    }, {
      "title" : "Pairwise fused lasso",
      "author" : [ "S. Petry", "C. Flexeder", "G. Tutz" ],
      "venue" : "Technical Report 102,",
      "citeRegEx" : "Petry et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Petry et al\\.",
      "year" : 2011
    }, {
      "title" : "Sparse regression with exact clustering",
      "author" : [ "Y. She" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "She,? \\Q2010\\E",
      "shortCiteRegEx" : "She",
      "year" : 2010
    }, {
      "title" : "Sparsity and smoothness via the fused lasso",
      "author" : [ "R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight" ],
      "venue" : "Journal of the Royal Statistical Society: Series B,",
      "citeRegEx" : "Tibshirani et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Tibshirani et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning Gaussian processes from multiple tasks",
      "author" : [ "K. Yu", "V. Tresp", "A. Schwaighofer" ],
      "venue" : "In Proceedings of the 22nd International Conference on Machine Learning,",
      "citeRegEx" : "Yu et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning multiple tasks with a sparse matrix-normal penalty",
      "author" : [ "Y. Zhang", "J. Schneider" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Zhang and Schneider,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang and Schneider",
      "year" : 2010
    }, {
      "title" : "A convex formulation for learning task relationships in multi-task learning",
      "author" : [ "Y. Zhang", "Yeung", "D.-Y" ],
      "venue" : "In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2010
    }, {
      "title" : "Efficient sparse modeling with automatic feature grouping",
      "author" : [ "L.W. Zhong", "J.T. Kwok" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "Zhong and Kwok,? \\Q2011\\E",
      "shortCiteRegEx" : "Zhong and Kwok",
      "year" : 2011
    }, {
      "title" : "Clustered multi-task learning via alternating structure optimization",
      "author" : [ "J. Zhou", "J. Chen", "J. Ye" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Zhou et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2011
    }, {
      "title" : "The adaptive lasso and its oracle properties",
      "author" : [ "H. Zou" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Zou,? \\Q2006\\E",
      "shortCiteRegEx" : "Zou",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Traditional MTL methods assume that all the tasks are related (Evgeniou & Pontil, 2004; Evgeniou et al., 2005).",
      "startOffset" : 62,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "If it is known that the tasks are clustered, a simple remedy is to constrain task sharing to be just within the same cluster (Argyriou et al., 2008; Evgeniou et al., 2005).",
      "startOffset" : 125,
      "endOffset" : 171
    }, {
      "referenceID" : 10,
      "context" : "If it is known that the tasks are clustered, a simple remedy is to constrain task sharing to be just within the same cluster (Argyriou et al., 2008; Evgeniou et al., 2005).",
      "startOffset" : 125,
      "endOffset" : 171
    }, {
      "referenceID" : 14,
      "context" : "This can be further extended to the case where task relationships are represented in the form of a network (Kato et al., 2007).",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 19,
      "context" : "For example, some assume that the task parameters share a common prior in a Bayesian model (Yu et al., 2005; Zhang & Schneider, 2010; Zhang & Yeung, 2010); that the data follows a dirty model (Jalali et al.",
      "startOffset" : 91,
      "endOffset" : 154
    }, {
      "referenceID" : 12,
      "context" : ", 2005; Zhang & Schneider, 2010; Zhang & Yeung, 2010); that the data follows a dirty model (Jalali et al., 2010); that most of the tasks lie in a low-dimensional subspace (Ando & Zhang, 2005; Chen et al.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : ", 2010); that most of the tasks lie in a low-dimensional subspace (Ando & Zhang, 2005; Chen et al., 2010), or that outlier tasks are present (Chen et al.",
      "startOffset" : 66,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "In this paper, we will mainly be interested in techniques that assume the tasks are clustered (Argyriou et al., 2008; Evgeniou et al., 2005), and then infer the clustering structure automatically during learning (Jacob et al.",
      "startOffset" : 94,
      "endOffset" : 140
    }, {
      "referenceID" : 10,
      "context" : "In this paper, we will mainly be interested in techniques that assume the tasks are clustered (Argyriou et al., 2008; Evgeniou et al., 2005), and then infer the clustering structure automatically during learning (Jacob et al.",
      "startOffset" : 94,
      "endOffset" : 140
    }, {
      "referenceID" : 11,
      "context" : ", 2005), and then infer the clustering structure automatically during learning (Jacob et al., 2008; Kang et al., 2011).",
      "startOffset" : 79,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : ", 2005), and then infer the clustering structure automatically during learning (Jacob et al., 2008; Kang et al., 2011).",
      "startOffset" : 79,
      "endOffset" : 118
    }, {
      "referenceID" : 23,
      "context" : "Interestingly, it is recently shown that this clustered MTL approach is equivalent to alternating structure optimization (Ando & Zhang, 2005) that assumes the tasks share a low-dimensional structure (Zhou et al., 2011).",
      "startOffset" : 199,
      "endOffset" : 218
    }, {
      "referenceID" : 11,
      "context" : "Moreover, a key difference with (Jacob et al., 2008) is that we do not require the number of clusters to be pre-specified.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "For example, in (Kang et al., 2011), it leads to a mixed integer program, which has to be relaxed as a nonlinear optimization problem and then solved by gradient descent.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 15,
      "context" : "On the other hand, the proposed approach directly leads to a (strongly) convex optimization problem, which can then be efficiently solved by accelerated proximal methods (Nesterov, 2007) after some transformations.",
      "startOffset" : 170,
      "endOffset" : 186
    }, {
      "referenceID" : 18,
      "context" : "Note that this is different from the fused lasso regularizer (Tibshirani et al., 2005), which is used for clustering features in single-task learning while ‖U‖clus is for clustering tasks in MTL.",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "In (Evgeniou et al., 2005), ut is the (single) cluster center of all the tasks; in (Ando & Zhang, 2005; Chen et al.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : ", 2005), ut is the (single) cluster center of all the tasks; in (Ando & Zhang, 2005; Chen et al., 2010; 2011a),",
      "startOffset" : 64,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "ut comes from a low-dimensional linear subspace, which is extended to a nonlinear manifold in (Agarwal et al., 2010); in (Jalali et al.",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : ", 2010); in (Jalali et al., 2010), ut is the component that uses features shared by other tasks.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "Thus, wt reduces to ū + vt for some “mean weight” ū, and (1) reduces to the model in (Evgeniou et al., 2005).",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "In recent years, accelerated proximal methods (Nesterov, 2007) have been popularly used by the machine learning community (Bach et al.",
      "startOffset" : 46,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "In recent years, accelerated proximal methods (Nesterov, 2007) have been popularly used by the machine learning community (Bach et al., 2011) for convex problems of the form minθ f(θ)+r(θ), where f(θ) is convex and smooth, and r(θ) is convex but nonsmooth.",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "Though (7) is similar to the optimization problems of the pairwise fused lasso in (Petry et al., 2011; She, 2010), using the optimization procedures there are much more expensive.",
      "startOffset" : 82,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : "Though (7) is similar to the optimization problems of the pairwise fused lasso in (Petry et al., 2011; She, 2010), using the optimization procedures there are much more expensive.",
      "startOffset" : 82,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "Specifically, the procedure in (Petry et al., 2011) takes O(T ) time, as it involves a QP with ( T",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 17,
      "context" : "2 ) additional optimization variables; while (She, 2010) relies on annealing, which is even more complicated and expensive.",
      "startOffset" : 45,
      "endOffset" : 56
    }, {
      "referenceID" : 24,
      "context" : "As in the adaptive lasso (Zou, 2006), weights can be added to each term of ‖U‖clus as ∑D d=1 ∑ ĩ<j̃ αd,̃ij̃ |Udĩ − Udj̃ |, where αd,̃ij̃ is the weight associated with the ith and jth largest entries (Udĩ and Udj̃ , respectively) on the dth row of U.",
      "startOffset" : 25,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "It is compared with a variety of single-task and state-of-the-art MTL algorithms, including: 1) Independent ridge regression on each task; 2) Pooling all the training data together to learn a single model: This assumes that all the tasks are identical; 3) Regularized MTL: This assumes that all the tasks come from a single cluster (Evgeniou & Pontil, 2004); 4) The dirty model in (Jalali et al., 2010); 5) Low-rank-based robust MTL (Chen et al.",
      "startOffset" : 381,
      "endOffset" : 402
    }, {
      "referenceID" : 6,
      "context" : ", 2011a); 6) Sparse-LowRank MTL (Chen et al., 2010), which learns sparse and low-rank patterns from the tasks; 7) Clustered MTL (Jacob et al.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : ", 2010), which learns sparse and low-rank patterns from the tasks; 7) Clustered MTL (Jacob et al., 2008)2 and 8) Multitask relationship learning (MTRL) (Zhang & Yeung, 2010).",
      "startOffset" : 84,
      "endOffset" : 104
    }, {
      "referenceID" : 11,
      "context" : "The clustered MTL algorithm of (Jacob et al., 2008) requires the number of task clusters as input.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "Note that though the school data has been popularly used as a MTL benchmark, it has been pointed out previously that all the tasks are indeed the same (Bakker & Heskes, 2003; Evgeniou et al., 2005).",
      "startOffset" : 151,
      "endOffset" : 197
    }, {
      "referenceID" : 13,
      "context" : "As in (Kang et al., 2011), PCA is used to reduce the feature dimensionality to 64 for USPS and 87 for MNIST.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "In this section, we use the computer survey data in (Argyriou et al., 2008).",
      "startOffset" : 52,
      "endOffset" : 75
    } ],
    "year" : 2012,
    "abstractText" : "Traditionally, multitask learning (MTL) assumes that all the tasks are related. This can lead to negative transfer when tasks are indeed incoherent. Recently, a number of approaches have been proposed that alleviate this problem by discovering the underlying task clusters or relationships. However, they are limited to modeling these relationships at the task level, which may be restrictive in some applications. In this paper, we propose a novel MTL formulation that captures task relationships at the feature-level. Depending on the interactions among tasks and features, the proposed method construct different task clusters for different features, without even the need of pre-specifying the number of clusters. Computationally, the proposed formulation is strongly convex, and can be efficiently solved by accelerated proximal methods. Experiments are performed on a number of synthetic and real-world data sets. Under various degrees of task relationships, the accuracy of the proposed method is consistently among the best. Moreover, the feature-specific task clusters obtained agree with the known/plausible task structures of the data.",
    "creator" : "TeX"
  }
}