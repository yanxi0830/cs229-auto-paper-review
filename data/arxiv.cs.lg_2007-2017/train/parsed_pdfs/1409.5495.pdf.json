{
  "name" : "1409.5495.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Feature Group Sequencing for Anytime Linear Prediction",
    "authors" : [ "Hanzhang Hu", "Alexander Grubb", "J. Andrew Bagnell" ],
    "emails" : [ "hanzhang@andrew.cmu.edu", "agrubb@cmu.edu", "dbagnell@ri.cmu.edu", "hebert@ri.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose a regularized linear learning algorithm to sequence groups of features, where each group incurs test-time cost or computation. Specifically, we develop a simple extension to Orthogonal Matching Pursuit (OMP) that respects the structure of groups of features with variable costs, and we prove that it achieves nearoptimal anytime linear prediction at each budget threshold where a new group is selected. Our algorithm and analysis extends to generalized linear models with multi-dimensional responses. We demonstrate the scalability of the resulting approach on large real-world data-sets with many feature groups associated with test-time computational costs. Our method improves over Group Lasso and Group OMP in the anytime performance of linear predictions, measured in timeliness[7], an anytime prediction performance metric, while providing rigorous performance guarantees."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the common machine learning setting where each group of features has an associated cost, e.g., object recognition tasks in computer vision may require various groups of features, such as SIFT, HOG, and context information; inferences in computational biology may be based on several consecutive gene segments. Furthermore, many prediction problems have unknown computational budgets until test-time, e.g., an obstacle detector on-board a moving vehicle may have varying deadlines depending on the speed of the vehicle; an algorithm for document analysis may produce different levels of analysis depending on the desired turn-around time of its user. We view these prediction problems with unknown test-time budgets as anytime predictions [4], which trade execution time for quality of results. In our work, we propose an algorithm to address these anytime linear predictions using groups of features: our algorithm produces a list of linear predictors with increasing computational cost, such that each predictor performs nearly as well as the optimal predictor of a similar cost.\nIn the context of linear prediction, the core of producing the near-optimal linear predictors is to choose the most cost efficient feature groups, which is closely related to feature selection problem for linear regression. Given a feature matrix X ∈ Rn×D, a response vector Y ∈ Rn, and a partition of the D dimensions as feature groups, G1,G2, ...,GJ , then the feature group selection problem, in the context of linear prediction, is to balance between choosing an inexpensive set of groups, I ⊂ {1, ..., J}, and minimizing the risk of linear regression using the selected features dimensions, minwRD ‖Y − ∑ j∈I XGjwGj‖22. Several methods have addressed the feature group selection problem, aiming to discover the true models. One popular approach is to extend Lasso [14] to the group setting, e.g., Group Lasso[18] balances the trade-off by solving\nar X\niv :1\n40 9.\n54 95\nv1 [\ncs .L\nG ]\n1 9\nSe p\nminw∈RD ‖Y −Xw‖22 + λ ∑J j=1 ‖wGj‖2. Group Lasso can be further extended to incorporate feature group costs in the group regularization constants[2].\nAnother class of approaches comes from the idea of ”forward greedy selection”, which evaluates the performance gain of a candidate feature group and greedily selects the one with the best marginal gain. Selecting based on the exact gains in performance, also known as Forward Regression (FR) [13], often performs well. However, since the algorithm computes O(J2) number of models, it is often prohibitively expensive to train with large number of candidates. One computational feasible method to approximate this exact greedy selection is Orthogonal Matching Pursuit (OMP) [12], which chooses candidates that induce the steepest change in objective value. [19] proves that OMP discovers, in the absence of group structures, the true linear models, if they do exist. This result is then extended by [10] and [11] to handle feature groups. This feature selection literature, however, focuses mostly on model recovery rather than the trade-off between feature computational costs and regression results. Hence our work addresses a closely related but different problem and achieves provable anytime prediction guarantees regardless of the true models.\nOn the other hand, multiple works have addressed the problem of prediction and feature selection under a limited test-time computational budget, or anytime prediction. [17] and [16] propose methods to learn cost-sensitive non-linear transformation of features for linear classification. [5] approaches the trade-off between anytime prediction quality and feature computation with gradient boosting. [7] treats feature selection procedure as a Markov Decision Process and learns a policy of applying intermediate learners and computing features through reinforcement learning. Cascading classifiers like [2] and [9] approach anytime prediction by filtering out the dominating negative classes using a sequence classifiers of increasing complexity and feature costs. However, cascades may suffer in general anytime prediction settings because they can never recover from early mistakes of false negatives.\nContributions. In this work, we propose Cost-Sensitive Group Orthogonal Matching Pursuit (CSG-OMP), which extends Group Orthogonal Matching Pursuit (G-OMP) [10], for anytime linear predictions. Our theoretical and experimental analyses show the following:\n• Without assumptions on the true models, our approach provably produces a near-optimal feature group sequence: linear predictions using the feature groups in the front of the sequence are close, measured in prediction risks, to the optimal linear predictions of similar feature costs.\n• On real-world data-sets, the anytime prediction performance of CS-G-OMP is close to that of Forward Regression, which is a more accurate ”forward greedy selection method” by design, but is prohibitively expensive with large number of feature groups."
    }, {
      "heading" : "2 Method",
      "text" : "This section introduces our Cost-Sensitive Group Orthogonal Matching Pursuit algorithm for linear prediction with a single response dimension. In Appendix B we extend this algorithm to generalized linear models and multi-dimensional outputs."
    }, {
      "heading" : "2.1 Feature Group Sequencing for Linear Regression",
      "text" : "Let the data be {(xi, yi) : i = 1, ..., n}, where xi ∈ RD is the feature of sample i and yi ∈ R is its response. Let X ∈ Rn×D, Y ∈ Rn be the data matrix and response vector. We assume here that the responses have mean zero, i.e., ∑n i=1 yi = 0. Let G1, ...,GJ be a group structure (partition) of features with cost c(G1), ..., c(GJ), and number of dimensions DG1 , ..., DGJ . Let S be a union of a collection of feature groups, i.e., S ∈ F , {∪j∈ΛGj : Λ ⊆ {1, ..., J}}. We define xiS to be the feature of ith sample restricted to dimensions in S, and we define the risk of S with regularization constant λ as:\nR(S) , min w∈R|S|\n1\n2n n∑ i=1 ‖wTxiS − yi‖22 + λ 2 ‖w‖22. (1)\nBy convention, we define wTxi∅ as 0. Hence we have R(∅) = 1 2n ∑n i=1 ‖yi‖22. We define w(S) ∈ R|S| as the feature coefficient vectorw that achieves the minimum in Equation 1; w(S) can be solved\nanalytically, with w(S) = 1n ( 1 nX T SXS + λI) −1(XTS Y ). For convenience, we define w(S)Gj as the weight assigned to group Gj by w(S), if Gj ⊆ S; and zero if otherwise. We define the problem of selecting the most efficient features for linear prediction with a given budget B as the following concave maximization problem: maxS∈F R(∅)−R(S) subject to ∑ j=1..J:Gj⊆S c(Gj) ≤ B. The objective function F (S) , R(∅)−R(S) is also known as the explained variance, and our theoretical analysis provides a bound on our achieved explained variance by a constant factor of the optimal value. Note that F (S) ≥ 0 for all S, since R(∅) is equivalent to setting w = 0 in solving the minimization in Equation 1, and thus, R(∅) upper-bounds R(S) for all S."
    }, {
      "heading" : "2.2 Cost Sensitive Group Orthogonal Matching Pursuit",
      "text" : "input : The data matrix X = [f1, ..., fD] ∈ Rn×D, with group structure G1, ...GJ , such that for each group g = Gj , XTg Xg = IDg . The cost c(g) of each group g = Gj . The response of all samples Y ∈ {0, 1}n. We assume the responses are shifted so that the mean is 0, i.e., 1 n ∑n i=1 yi = 0. Regularization constant λ. output: A sequence G = g1, g2, ..., gJ of feature groups, where we define Gj as the first j number of feature groups, g1, g2, ..., gj . A sequence of associated coefficients w(G1), w(G2), ..., w(GJ). G0 = ∅; for j = 1, 2, ... do\n// Learn linear model w = w(Gj−1) ; // w(S) = 1n ( 1 nX T SXS + λI)\n−1(XTS Y ) // Selection step (*)\ngj = argmax g=G1,...,GJ ,g /∈Gj−1\n‖XTg (Y−XGj−1w)‖ 2 2\nc(g) ;\nend Algorithm 1: Cost Sensitive Group Orthogonal Matching Pursuit (G-OMP) for Linear Model\nAlgorithm 1 describes our proposed algorithm, Cost-Sensitive Group Orthogonal Matching Pursuit (CS-G-OMP), for linear regression with one-dimensional outputs. The core of any OMP based algorithm is to select features of the best gradient of the objective. More specifically, at the jth selection step (∗) in Algorithm 1, assuming we have chosen a sequence Gj−1 of j − 1 feature groups g1, g2, ..., gj−1, we first compute for each group g the gradient of of the explained variance F (Gj−1) with respect to the coefficients wg of the group:\nbGj−1g , X T g (Y −XGj−1w(Gj−1))− λw(Gj−1)g. (2)\nSince w(Gj−1)g = 0 for g 6⊆ Gj−1, we shorten the gradient as XTg (Y − XGj−1w(Gj−1)) in Algorithm 1. After computing the gradients, we select the feature group gj with the largest ratio of gradient L2-norm square to cost.\nNote that we use L2-norm square of the gradients of the groups, ‖bg‖22, as a measurement of the effectiveness of a feature group. We show experimentally and theoretically that replacing L2-norm with L∞ results in worse performance. We call the L∞ alternative as Single, since it evaluates a feature group based on its single best feature dimension. It is also crucial that we assume each feature group g to be whitened, i.e., XTg Xg = IDg , a property that we call Group Whitened. Theoretical analyses of [10] and [11] also rely on this assumption but did not explain the impact of not group whitening the data, which we will do in experiments and analysis. As mentioned in [11], if the data is not group whitened before training, one equivalent method of group whitening is to replace ‖XTg (Y −XGj−1w)‖22 in step (∗) with\nbGj−1g T (XTg Xg) −1bGj−1g = (Y −XGj−1w)TXg(XTg Xg)−1XTg (Y −XGj−1w). (3)\nThis replacement frees us from group whitening each individual sample, reducing training complexity by O(nD2) operations."
    }, {
      "heading" : "3 Theoretical Analysis",
      "text" : ""
    }, {
      "heading" : "3.1 Main Result",
      "text" : "Let G = g1, g2, ..., gJ be the sequence of feature groups that our CS-G-OMP selects. Let S be any sequence of feature groups. For any B such that B = ∑L j=1 c(gj) for some L, let G〈B〉 be the first L feature groups of G, g1, g2, ..., gL. We denote S〈K〉 = s1, s2, ... to be any competing sequences of cost K, that is, ∑ j=1..J:Gj⊆S〈K〉 c(Gj) = K. For any two selected feature group sequence G and S, we define bGS as the gradient of the objective F (G) with respect to coefficient of group S, wS ; components of bGS can be computed using b G g defined in Equation 2. We define matrix C to be C , 1nX TX + λI , which is the regularized covariance matrix of features. Let the minimum eigenvalue of C be λmin(C) = 1nλmin(X TX) + λ > 0. Then our main results below bounds F (G〈B〉), the explained variance achieved using proposed features, with a constant factor of F (S〈K〉), the explained variance of any competing sequence with budget K.\nTheorem 3.1. Let B = ∑L i=1 c(gi), for some L > 0. Let X be the group whitened feature matrix. There exists a constant γ = λmin(C)1+λ , such that for any sequence S and total cost K, F (G〈B〉) > (1− e−γ B K )F (S〈K〉).\nWe first discuss some implications of the theorem. Constant γ measures the strength of the bound on explained variance, F , and it depends on the regularization constant λ and the correlation among the features, measured by λmin(C). If all feature groups are uncorrelated, then C = (1+λ)I so that γ = 1. If features have linear dependencies, then γ = λ1+λ , since λmin(C) = 1 nλmin(X TX)+λ = λ, where XTX is a singular matrix. In general, the less feature groups are correlated, the higher is λmin(C), and the better is the bound.\nGiven the next Lemma 3.2, which mimics a classical result in submodular maximization literature such as Equation 8 of [8], the proof of the theorem is a standard technique in approximate submodular maximization literature, which we defer to the appendix. We prove Lemma 3.2 using the next two lemmas: Lemma 3.3 and Lemma 3.4, whose proofs are based on Taylor expansion of a strongly smooth and strongly convex loss R. We justify the smoothness and convexity assumptions and prove the lemmas in the appendix. We also show in the appendix that if we do not group whiten the feature, the constant (1 + λ) in γ of the theorem can inflate up to (Dgj + λ), resulting a worse bound, especially for data-sets of large feature groups.\nLemma 3.2 (main). There exists constant γ = λmin(C)1+λ > 0 such that for all S and total cost K, and all j = 1, 2, ..., J , F (S〈K〉)− F (Gj−1) ≤ Kγ [ F (Gj)−F (Gj−1) c(gj) ]. Lemma 3.3 (Using Smoothness). Let S and G be some fixed sequences. Let G⊕S be the sequence G followed by the sequence S. Then F (S)− F (G) ≤ 12b G G⊕SC −1 G⊕Sb G G⊕S .\nLemma 3.4 (Using Convexity). For j = 1, 2, ..., J , F (Gj)− F (Gj−1) ≥ 12(1+λ)b Gj−1 gj T b Gj−1 gj .\nProof. (of Lemma 3.2, using Lemma 3.3 and Lemma 3.4) Using Lemma 3.3, on S〈K〉 and Gj−1, we have:\nF (S〈K〉)− F (Gj−1) ≤ 1\n2 b Gj−1 Gj−1⊕S〈K〉 T CGGj−1⊕S〈K〉b Gj−1 Gj−1⊕S〈K〉 (4)\nNote that the gradient of R(Gj−1) with respect to wGj−1 , b Gj−1 Gj−1 , equals 0, because R(Gj−1) is computed by minimizing with respect to wGj−1 . So b Gj−1 g = 0 for all g ∈ Gj−1. Then, using block matrix inverse formula, we have:\nF (S〈K〉)− F (Gj−1) ≤ 1\n2 b Gj−1 S〈K〉 T CGS〈K〉b Gj−1 S〈K〉\n(5)\nwhere CGS〈K〉 = CS〈K〉 − CS〈K〉GC −1 S〈K〉\nCGS〈K〉 . Using spectral techniques in Lemmas 2.5 and 2.6 in [3], we can show that:\nF (S〈K〉)− F (Gj−1) ≤ 1\n2 b Gj−1 S〈K〉 T CGS〈K〉b Gj−1 S〈K〉 ≤ 1 2λmin(C) b Gj−1 S〈K〉 T b Gj−1 S〈K〉 . (6)\nExpanding S〈K〉 into individual groups si, we continue the right most term above:\n= 1\n2λmin(C) ∑ si∈S〈K〉 bGj−1si T bGj−1si ≤\n1\n2λmin(C) ∑ si∈S〈K〉 c(si) max g b Gj−1 g T b Gj−1 g c(g) (7)\n= 1\n2λmin(C) ∑ si∈S〈K〉 c(si) b Gj−1 gj T b Gj−1 gj c(gj) ≤ (1 + λ) λmin(C) ∑ si∈S〈K〉 c(si) F (Gj)− F (Gj−1) c(gj) . (8)\nThe last equality follows from the greedy selection step of Algorithm 1, and the last inequality is given by Lemma 3.4. The theorem then follows.\nsectionExperiment and Result"
    }, {
      "heading" : "3.2 Data-sets and Parameter Set-up",
      "text" : "We experiment CS-G-OMP for anytime linear prediction on two real-world data-sets, each of which has a significant number of feature groups with associated costs.\n• Yahoo! Learning to Rank Challenge [1] contains 883k web documents, each of which has a relevance score in {0, 1, 2, 3, 4}. Each of the 501 document features has an associated computational cost in {1, 5, 20, 50, 100, 150, 200}, and the total feature cost is around 17K. The original data-set has no feature group structures, so we generated random group structures by grouping features of the same cost into groups of a given size s.1\n• Agriculture is a proprietary data-set that contains 510k data samples, 328 features, and 57 feature groups. Each sample has a binary label in {1, 2}. Each feature group has an associated cost measured in its average computation time.2"
    }, {
      "heading" : "3.3 Evaluation Metric",
      "text" : "Following the practice of [7], we use the area under the maximization objective F (explained variance) vs. cost curve, normalized by the total area, as the timeliness measurement of the anytime\n1We experiment on group sizes s ∈ {5, 10, 15, 20}. We choose regularizer λ = 10−5 based on validation. We use s = 10 for qualitative results such as plots and illustrations, but we report quantitative results for all group size s. For our quantitative results, we report the average test performance. The initial risk is R(∅) = 0.85.\n2 There are 6 groups of size 32, and the other groups have sizes between 1 and 6. The cost of each group is its expected computation time in seconds, ranging between 0.0005 and 0.0088; the total feature cost is 0.111. We choose regularizer λ = 10−7. The data-set is split into five 100k sets, and the remaining 10k are used for validation. We report the cross validation results on the five 100K sets as the test results. The initial risk is R(∅) = 0.091.\nperformance of an algorithm. In our data-sets, the performance of linear predictors plateaus much before all features are used, e.g., Figure 1a demonstrates this effect in YAHOO! LTR, where the last one percent of total improvement is bought by half of total feature cost. Hence the majority of the timeliness measurement is from the plateau performance of linear predictors, and the difference between timeliness of different anytime algorithms diminishes due to the plateau effect. Furthermore, the difference vanishes as we include additional redundant high cost features. To account for this effect, we stop the curve when it reaches the plateau: we define an α-stopping cost for parameter α in [0, 1] as the cost at which our CS-G-OMP achieves α of the final objective value in training, and we ignore the objective vs. cost curve after the α-stopping cost. We call the timeliness measure on the shortened curve as α-timeliness; 1-timeliness equals the normalized area under the full curve, and 0-timeliness is zero. If a curve does not pick a group at α-stopping cost, we linearly interpolate the objective value at the stopping cost for computing timeliness. We say an objective vs. cost curve has reached its final plateau, if at least 95% of total objective has been achieved and the next 1% requires more than 20% feature costs (if it does not exist, we use α = 1). Following this rule, we choose we have α = 0.97 for AGRICULTURAL and α = 0.99 for YAHOO! LTR.\nTo approximate the best possible anytime performance, we follow the approach of [7] to create an Oracle objective vs. cost curve: we reorder the feature groups in descending order of their marginal benefit per unit cost. We have to do this approximation, because an exhaustive search over permutation of feature groups is intractable. Note that Oracle is dependent of the curve from which it is created, so we choose the best performing curve to generate the Oracle. We specify which curves are used in Section 3.6. We extend Group Lasso[18] to be our baseline algorithm: we scale the regularization constant of each group with the cost of the group, to get the minimization problem, minw∈RD ‖Y −Xw‖22 + λ ∑J j=1 c(Gj)‖wGj‖2, where we use various value of regularization constant λ to obtain lasso paths. We call this baseline algorithm Sparse in figures and tables3."
    }, {
      "heading" : "3.4 Feature Cost",
      "text" : "Our proposed CS-G-OMP differs from Group Orthogonal Matching Pursuit (G-OMP) [10] in that G-OMP does not consider feature costs when evaluating features. We show that this difference is crucial for anytime linear prediction. In Figure 1b, we compare the objective vs. costs curves of CS-G-OMP and G-OMP that are stopped at 0.97-stopping cost on YAHOO! LTR. Expectedly, CS-G-OMP achieves a better overall prediction at every budget, qualitatively demonstrating the importance of incorporating feature costs. Table 1 and Table 2 quantify this effect, showing that CS-G-OMP achieves a better timeliness measure than regular G-OMP."
    }, {
      "heading" : "3.5 Group Whitening",
      "text" : "We provide experimental evidence that Group whitening, i.e., XTg Xg = IDg for each group g, is a key assumption of both this work and previous feature group selection literature [10] and [11]. In Figure 2, we compare anytime prediction performances using group whitened data against those using the common normalization scheme where each feature dimension is individually normalized to have zero mean and unit variance. The objective vs. cost curve qualitatively shows that group whitening consistently results in the better predictions. This behavior is expected from data-sets whose feature groups contain correlated features, e.g., group whitening effectively prevents selection step (∗) from overestimating the predictive power of feature groups of repeated good features.\n2[7] defines timeliness as the area under the average precision vs. time curve 3We use an off-the-shelf software to solve the optimization: SPAMS (SPArse Modeling Software [6]).\nTable 1 and Table 2 demonstrate quantitatively the consistent better timeliness performance of CSG-OMP over that of CS-G-OMP-no-whiten."
    }, {
      "heading" : "3.6 Variants on Selection Criterion",
      "text" : "This section discuss some alternative approaches to our proposed CS-G-OMP method. The first variant, Single (CS-G-OMP-single), formulated in Section 2.2, intuitively chooses feature groups of the best single feature dimension per group cost, and our experiments show that this modification degrades prediction performance. The second variant extends from Forward Regression, and iteratively adds features that achieves the best marginal objective increase per cost. We call this FR extension as CS-G-FR. FR usually performs better than OMP at the cost of computing and evaluating a model for each candidate feature group at each iteration. We will will examine the exact difference in runtime complexity. Since we expect CS-G-FR to perform the best, we use its curve to compute the Oracle curve, as an approximate to the best achievable performance.\nIn Figure 3, we evaluate CS-G-FR, CS-G-OMP and CS-G-OMP-single based on the objective in Theorem 3.1, i.e., explained variance vs. feature cost curves. CS-G-FR, as expected, outperforms all other methods including the baseline method, Sparse. The performance advantage of CS-G-OMP over CS-G-OMP-Single is much clearer in AGRICULTURAL than that in YAHOO! LTR, because the former data-set has a natural group structure which may contain correlated features in each group, whereas the latter data-set has a randomly generated group structure whose features have been filtered by feature selection before the publication of the data-set [1]. The baseline algorithm, Sparse, is outperformed by CS-G-FR and CS-G-OMP. We speculate that scaling group regularization constants by group costs did not enforce group-Lasso to choose the most cost-efficient features early. The test-time timeliness measures of each of the methods are recorded in Table 1 and Table 2, and quantitatively confirm analysis above. Since AGRICULTURAL and YAHOO! LTR are originally a classification and a ranking data-set, respectively, we also report in Figure 3 the performance using classification accuracy and NDCG@5, which demonstrate the same qualitatively results as using explained variants.\nAs mentioned earlier, CS-G-FR is able to consistently choose more cost-efficient features at the cost of a longer training time. In the context of linear regression, assuming the group sizes are bounded by a constant, when we are to select the number K feature group, we can compute a new model of K groups in O(K2N) using matrix inversion lemma [15]), evaluate it in O(KN), and compute the gradients w.r.t to weights of unselected groups in O(N(J − K)). So CS-G-OMP\nrequires O(K2N +JN) at step K = 1, 2, 3, ..., J and CS-G-FR requires O((J −K)K2N), so that the total training complexities for CS-G-OMP and CS-G-FR are O(J3N) and O(J4N) 4. We also show this training complexity gap empirically in Figure 4, which plots the curves of training time vs. number of feature groups selected. When all feature groups are selected, CS-G-OMP achieves a 8x speed-up in AGRICULTURAL over CS-G-FR. In YAHOO! LTR, CS-G-OMP achieves a speed-up factor between 10 and 20, and the smaller the group-sizes, the more speed-up due to the increase in the number of groups."
    }, {
      "heading" : "A Proof Details of Theorem and Lemmas",
      "text" : "A.1 Functional Boosting View of Feature Selection\nThis section describes a functional boosting view of selecting features for generalized linear models. We prove a more general version of Lemma 3.3 and Lemma 3.4. These more general results in turn lead to extension of Theorem 3.1 to generalized linear models.\nWe view each feature f as a function hf that maps sample x to xf . We define fS : RD → R to be the best linear predictor using features in S, i.e., fS(x) , w(S)TxS . For each feature dimension d ∈ D, the coefficient of d is in w(S) is w(S)d = fS(ed), where ed is the dth dimensional unit vector. So ‖w(S)‖22 = ∑D d=1 ‖fS(ed)‖22. Given a generalized linear model with link function ∇Φ, the\npredictor is E[y|x] = ∇Φ(wTx) for some w, and the calibrated loss is r(w) = ∑n i=1(Φ(w\nTxi) − yiw Txi). Replacing fS(xi) = w(S)Txi, we have\nr(w(S)) = n∑ i=1 (Φ(fS(xi))− yifS(xi)). (9)\nNote that the risk function in Equation 1 can be rewritten as the following to resemble Equation 9:\nR(S) = R[fS ] = 1\nn n∑ i=1 (Φ(fS(xi))− yTi fS(xi)) + λ 2 D∑ d=1 ‖fS(ed)‖22 +A, (10)\nwhere φ(x) = 12x 2 for linear predictions, and constant A = 12n ∑n i=1 y 2 i . Next we define the inner product between two functions f, h : RD → R over the training set to be:\n〈f, h〉 , 1 n n∑ i=1 f(xi)h(xi) + λ 2 D∑ d=1 f(ed)h(ed). (11)\nWith this definition of inner product, we can compute the derivative ofR:\n∇R[f ] = n∑ i=1 (∇Φ(f(xi))− yi)δxi + D∑ d=1 f(ed)δed , (12)\nwhere ∇φ(x) = x for linear predictions, and δx is an indicator function for x. Then the gradient of objective F (S) w.r.t coefficient wf of a feature dimension d can be written as:\nbSd = − 1\nn n∑ i=1 (∇Φp(w(S)Txi)− yi)xid − λw(S)d = −〈∇R[fS ], hd〉. (13)\nIn addition, the regularized covariance matrix of features C satisfies,\nCij = 1\nn XTi Xj + λI(i = j) = 〈hi, hj〉, (14)\nfor all i, j = 1, 2, ..., D. So in this functional boosting view, Algorithm 1 greedily chooses group g that maximizes, with a slight abuse of notation of 〈 , 〉, ‖〈hg,∇R[fS ]〉‖22/c(g), i.e., the ratio between similarity of a feature group and the functional gradient, measured in sum of square of inner products, and the cost of the group\nA.2 Proof of Lemma 3.3 and Lemma 3.4\nThe more general version of Lemma 3.3 and Lemma 3.4 assumes that the objective functional R is m-strongly smooth and M -strongly convex using our proposed inner product rule. M -strong convexity is a reasonable assumption, because the regularization term ‖w‖22 = ∑D d=1 ‖fS(ed)‖22 ensures that all loss functional R with a convex Φ strongly convex. In the linear prediction case both m and M equals 1.\nThe following two lemmas are the more general versions of Lemma 3.3 and Lemma 3.4.\nLemma A.1. Let R be an m-strongly smooth functional with respect to our definition of inner products. Let S and G be some fixed sequences. Then\nF (S)− F (G) ≤ 1 2m bGG⊕SC −1 G⊕Sb G G⊕S (15)\nProof. First we optimize over the weights in S.\nF (S)− F (G) = R[fG]−R[fS ] = R[fG]−R[ ∑ s∈S αTs hs]\n≤ R[fG]− min w:wTi ∈R dsi ,si∈S R[ ∑ si∈S wTsihsi ]\nAdding dimensions in G will not increase the risk, we have:\n≤ R[fG]− min w:wi∈Rdsi ,si∈G⊕S\nR[ ∑\nsi∈G⊕S wsihsi ]\nSince fG = ∑ gi∈G αihgi , we have:\n≤ R[fG]−min w R[fG + ∑ si∈G⊕S wTi hsi ]\nExpanding using strong smoothness around fG, we have:\n≤ R[fG]−min w\n(R[fG] + 〈∇R[fG], ∑\nsi∈G⊕S wTi hsi〉+\nm 2 ‖ ∑ si∈G⊕S wTi hsi‖2)\n= max w −〈∇R[fG], ∑ si∈G⊕S wTi hsi〉 − m 2 ‖ ∑ si∈G⊕S wTi hsi‖2\n= max w\nbGG⊕S T w − m\n2 wTCG⊕Sw\nSolving w directly we have:\nF (S)− F (G) ≤ 1 2m bGG⊕S T C−1G⊕Sb G G⊕S\nLemma A.2. Let R be a M-strongly convex functional with respect to our definition of inner products. Then\nF (Gj)− F (Gj−1) ≥ 1\n2M(1 + λ) bGj−1gj T bGj−1gj (16)\nProof. After the greedy algorithm chooses some group gj at step j, we form fGj = ∑ αi αTi hgi , such that\nR[fG] = min αi∈Rdgi R[ ∑ gi∈Gj αTi hgi ] ≤ min β∈Rdgj R[fGj−1 + βhgj ]\nSetting β = arg min β∈Rdgj R[fGj−1 + βhgj ], using the strongly convex condition at fGj−1 , we have:\nF (Gj)− F (Gj−1) = R[fGj−1 ]−R[fGj ] ≥ R[fGj−1 ]−R[fGj−1 + βhgj ]\n≥ R[fGj−1 ]− (R[fGj−1 ] + 〈∇R[fGj−1 ], βhgj 〉+ M\n2 ‖βhgj‖2)\n= −〈∇R[fGj−1 ], βhgj 〉 − M\n2 ‖βhgj‖2\n= bGj−1gj T β − M\n2 βTCgjβ\n≥ 1 2M bGj−1gj T C−1gj b Gj−1 gj = 1\n2M(1 + λ) bGj−1gj T bGj−1gj\nThe last equality holds because each group is whitened, so that Cgj = (1 + λ)I .\nNote that the (1 + λ) constant is a result of group whitening, without which the constant can be as large as (Dgj + λ) for the worst case where all the Dgj number of features are the same.\nA.3 Proof of Main Theorem\nGiven Lemma A.1 and Lemma A.2, the proof of Lemma 3.2 holds with the same analysis with a more general constant γ = mλmin(C)M(1+λ) . The following prove our main theorem 3.1.\nProof. (of Theorem 3.1, given Lemma 3.2) Define ∆j = F (S〈K〉) − F (Gj−1). Then we have ∆j −∆j+1 = F (Gj)− F (Gj−1). By Lemma 3.2, we have:\n∆j = F (S〈K〉)− F (Gj−1) ≤ K γ [ F (Gj)− F (Gj−1) c(gj) ] = K γ [ ∆j −∆j+1 c(gj) ]\nRearranging we get ∆j+1 ≤ ∆j(1− γc(gj)K ). Unroll we get:\n∆L+1 ≤ ∆1 L∏ j=1 (1− γc(gj) K ) ≤ ∆1( 1 L L∑ j=1 (1− γc(gj) K ))L = ∆1(1− Bγ LK )L < ∆1e −γ BK\nBy definition of ∆1 and ∆L+1, we have:\nF (S〈K〉)− F (G〈B〉) < F (S〈K〉)e−γ B K\nThe theorem follows, and linear prediction is the special case that m = M = 1."
    }, {
      "heading" : "B Appendix B: Generalized Linear Model with Multi-Dimensional Response",
      "text" : "Here we describe the CS-G-OMP extension to generalized linear models with multi-dimensional outputs. Assuming we have P dimensional responses, we define best feature coefficient matrix on feature set S as W (S) ∈ RP×DS . We assume the model is E[y|x] = ∇φ(Wx), where φ : RP → R is in the risk definition in Equation 10. Similar to the Algorithm 1, Algorithm 2 takes a greedy approach at each step. Specifically, it computes the functional gradient, r, with respect to current linear predictor fS , which is defined as fS(x) = Wx. Then it chooses the group g that maximizes the per unit cost sum of squares of inner products between feature functions hf and gradient dimension rp, for p = 1, ..., P .\ninput : The data matrix X = [f1, ..., fD] ∈ Rn×D, with group structures, such that for each group g, XTg Xg = IDg . The cost c(g) of each group g. The response matrix Y ∈ {0, 1}n×P . The link function∇Φ. Regularization constant λ. output: A sequence ((Gj ,Wj))j , where Gj = (g1, g2, ..., gj) is the sequence of first j selected feature groups, g1, g2, ..., gj , and Wj ∈ RP×(Dg1+Dg2+...+Dgj ) is its associated coefficient matrices. G0 = ∅; W0 = 0; Define (xiG, y\ni) as the ith data sample restricted to feature selected in G, for all i; for j = 1, 2, ... do\nDefine r : RP → RP such that for x ∈ RP r(x) = ∑n i=1 1 n (∇Φ(Wj−1x iGj−1)− yi)1(x = xi) + λ ∑ d∈Gj−1 Wj−1ed1(x = ed); // Selection step (*) gj = arg maxg( ∑n i=1 ‖xTi,gr(xi)‖2F + ∑ d∈g ‖eTd r(ed)‖22)/c(g); // Append selected group Gj = Gj−1 ⊕ (gj); // Solve for the best model with selected feature Use a GLM algorithm to solve: Wj = arg minW 1 n ∑n i=1(Φ(Wx i Gj )− yTi WxiGj ) + λ 2 ‖W‖ 2 F ;\nend Algorithm 2: Cost Sensitive Group Orthogonal Matching Pursuit (G-OMP) for Generalized Linear Models\nB.1 Proof Extension\nIn this section, we extend the proof of Lemma 3.3 and 3.4 to multi-dimensional responses. For proof of Lemma 3.3 and Lemma 3.4, we use the same arguments, except that\n1. 〈∇R[fG], ∑ si∈G⊕S wTi hsi〉\nis replaced with\n〈∇R[fG], ∑\nsi∈G⊕S Wihsi〉 = P∑ p=1 〈∇R[fG]p, ∑ si∈G⊕S (Wi)phsi〉,\nwhereR[fG]p and (Wi)p refer to their pth row, respectively; 2.\n‖ ∑\nsi∈G⊕S wTi hsi‖2\nis replaced with\n‖ ∑\nsi∈G⊕S Wihsi‖2F = P∑ p=1 ‖ ∑ si∈G⊕S (Wi)phsi‖2.\nThen the bounds for Lemma 3.3 and 3.4 are replaced with ∑P p=1 1 2mb G G⊕S,pC −1 G⊕Sb\nG G⊕S,p, and∑P\np=1 1 2M(1+λ)b Gj−1 gj ,p T b Gj−1 gj ,p , respectively. With the above extension, the arguments for Lemma 3.2 follows, and thus, we use the same technique in Section A.3 to prove the theorem for generalized linear models with multi-dimensional outputs."
    } ],
    "references" : [ {
      "title" : "Proceedings of the Yahoo! Learning to Rank Challenge, held at ICML 2010, Haifa, Israel, June 25, 2010, volume 14 of JMLR Proceedings",
      "author" : [ "Olivier Chapelle", "Yi Chang", "Tie-Yan Liu", "editors" ],
      "venue" : "JMLR.org,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Classifier cascade for minimizing feature evaluation cost",
      "author" : [ "Minmin Chen", "Zhixiang Eddie Xu", "Kilian Q. Weinberger", "Olivier Chapelle", "Dor Kedem" ],
      "venue" : "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection",
      "author" : [ "Abhimanyu Das", "David Kempe" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Anytime algorithm development",
      "author" : [ "Joshua Grass", "Shlomo Zilberstein" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1996
    }, {
      "title" : "Speedboost: Anytime prediction with uniform nearoptimality",
      "author" : [ "Alexander Grubb", "J. Andrew Bagnell" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Proximal methods for sparse hierarchical dictionary learning",
      "author" : [ "Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski", "Francis Bach" ],
      "venue" : "In ICML,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Timely object recognition",
      "author" : [ "Sergey Karayev", "Tobias Baumgartner", "Mario Fritz", "Trevor Darrell" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Submodular function maximization",
      "author" : [ "Andreas Krause", "Daniel Golovin" ],
      "venue" : "In Tractability: Practical Approaches to Hard Problems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Joint cascade optimization using a product of boosted classifiers",
      "author" : [ "Leonidas Lefakis", "Francois Fleuret" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Grouped orthogonal matching pursuit for variable selection and prediction",
      "author" : [ "Aurelie C. Lozano", "Grzegorz Swirszcz", "Naoki Abe" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Group orthogonal matching pursuit for logistic regression",
      "author" : [ "Aurelie C. Lozano", "Grzegorz Swirszcz", "Naoki Abe" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Matching pursuit with time-frequency dictionaries",
      "author" : [ "Stphane Mallat", "Zhifeng Zhang" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1993
    }, {
      "title" : "Subset selection in regression. Monographs on statistics and applied probability",
      "author" : [ "Alan J. Miller" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1994
    }, {
      "title" : "Inverting Modified Matrices",
      "author" : [ "Max A. Woodbury" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1950
    }, {
      "title" : "Anytime representation learning",
      "author" : [ "Zhixiang Xu", "Matt Kusner", "Gao Huang", "Kilian Q. Weinberger" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML- 13),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "The greedy miser: Learning under test-time budgets",
      "author" : [ "Zhixiang Xu", "Kilian Q. Weinberger", "Olivier Chapelle" ],
      "venue" : "In In ICML,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "Ming Yuan", "Yi Lin" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Our method improves over Group Lasso and Group OMP in the anytime performance of linear predictions, measured in timeliness[7], an anytime prediction performance metric, while providing rigorous performance guarantees.",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "We view these prediction problems with unknown test-time budgets as anytime predictions [4], which trade execution time for quality of results.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 13,
      "context" : "One popular approach is to extend Lasso [14] to the group setting, e.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : ", Group Lasso[18] balances the trade-off by solving",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 1,
      "context" : "Group Lasso can be further extended to incorporate feature group costs in the group regularization constants[2].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "Selecting based on the exact gains in performance, also known as Forward Regression (FR) [13], often performs well.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : "One computational feasible method to approximate this exact greedy selection is Orthogonal Matching Pursuit (OMP) [12], which chooses candidates that induce the steepest change in objective value.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 9,
      "context" : "This result is then extended by [10] and [11] to handle feature groups.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : "This result is then extended by [10] and [11] to handle feature groups.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 16,
      "context" : "[17] and [16] propose methods to learn cost-sensitive non-linear transformation of features for linear classification.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[17] and [16] propose methods to learn cost-sensitive non-linear transformation of features for linear classification.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "[5] approaches the trade-off between anytime prediction quality and feature computation with gradient boosting.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] treats feature selection procedure as a Markov Decision Process and learns a policy of applying intermediate learners and computing features through reinforcement learning.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Cascading classifiers like [2] and [9] approach anytime prediction by filtering out the dominating negative classes using a sequence classifiers of increasing complexity and feature costs.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "Cascading classifiers like [2] and [9] approach anytime prediction by filtering out the dominating negative classes using a sequence classifiers of increasing complexity and feature costs.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "In this work, we propose Cost-Sensitive Group Orthogonal Matching Pursuit (CSG-OMP), which extends Group Orthogonal Matching Pursuit (G-OMP) [10], for anytime linear predictions.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 9,
      "context" : "Theoretical analyses of [10] and [11] also rely on this assumption but did not explain the impact of not group whitening the data, which we will do in experiments and analysis.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "Theoretical analyses of [10] and [11] also rely on this assumption but did not explain the impact of not group whitening the data, which we will do in experiments and analysis.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "As mentioned in [11], if the data is not group whitened before training, one equivalent method of group whitening is to replace ‖X g (Y −XGj−1w)‖2 in step (∗) with bj−1 g T (X g Xg) −1bGj−1 g = (Y −XGj−1w)Xg(X g Xg)X g (Y −XGj−1w).",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "2, which mimics a classical result in submodular maximization literature such as Equation 8 of [8], the proof of the theorem is a standard technique in approximate submodular maximization literature, which we defer to the appendix.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "6 in [3], we can show that:",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "• Yahoo! Learning to Rank Challenge [1] contains 883k web documents, each of which has a relevance score in {0, 1, 2, 3, 4}.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "3 Evaluation Metric Following the practice of [7], we use the area under the maximization objective F (explained variance) vs.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "To account for this effect, we stop the curve when it reaches the plateau: we define an α-stopping cost for parameter α in [0, 1] as the cost at which our CS-G-OMP achieves α of the final objective value in training, and we ignore the objective vs.",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "To approximate the best possible anytime performance, we follow the approach of [7] to create an Oracle objective vs.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "We extend Group Lasso[18] to be our baseline algorithm: we scale the regularization constant of each group with the cost of the group, to get the minimization problem, minw∈RD ‖Y −Xw‖2 + λ ∑J j=1 c(Gj)‖wGj‖2, where we use various value of regularization constant λ to obtain lasso paths.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "4 Feature Cost Our proposed CS-G-OMP differs from Group Orthogonal Matching Pursuit (G-OMP) [10] in that G-OMP does not consider feature costs when evaluating features.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 9,
      "context" : ", X g Xg = IDg for each group g, is a key assumption of both this work and previous feature group selection literature [10] and [11].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : ", X g Xg = IDg for each group g, is a key assumption of both this work and previous feature group selection literature [10] and [11].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "[7] defines timeliness as the area under the average precision vs.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "time curve We use an off-the-shelf software to solve the optimization: SPAMS (SPArse Modeling Software [6]).",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "The performance advantage of CS-G-OMP over CS-G-OMP-Single is much clearer in AGRICULTURAL than that in YAHOO! LTR, because the former data-set has a natural group structure which may contain correlated features in each group, whereas the latter data-set has a randomly generated group structure whose features have been filtered by feature selection before the publication of the data-set [1].",
      "startOffset" : 390,
      "endOffset" : 393
    }, {
      "referenceID" : 14,
      "context" : "In the context of linear regression, assuming the group sizes are bounded by a constant, when we are to select the number K feature group, we can compute a new model of K groups in O(KN) using matrix inversion lemma [15]), evaluate it in O(KN), and compute the gradients w.",
      "startOffset" : 216,
      "endOffset" : 220
    } ],
    "year" : 2017,
    "abstractText" : "We propose a regularized linear learning algorithm to sequence groups of features, where each group incurs test-time cost or computation. Specifically, we develop a simple extension to Orthogonal Matching Pursuit (OMP) that respects the structure of groups of features with variable costs, and we prove that it achieves nearoptimal anytime linear prediction at each budget threshold where a new group is selected. Our algorithm and analysis extends to generalized linear models with multi-dimensional responses. We demonstrate the scalability of the resulting approach on large real-world data-sets with many feature groups associated with test-time computational costs. Our method improves over Group Lasso and Group OMP in the anytime performance of linear predictions, measured in timeliness[7], an anytime prediction performance metric, while providing rigorous performance guarantees.",
    "creator" : "LaTeX with hyperref package"
  }
}