{
  "name" : "1406.3840.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimal Resource Allocation with Semi-Bandit Feedback",
    "authors" : [ "Tor Lattimore", "Csaba Szepesvári" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 6.\n38 40\nv1 [\ncs .L\nG ]\n1 5\nJu n\nWe study a sequential resource allocation problem involving a fixed number of recurring jobs. At each time-step the manager should distribute available resources among the jobs in order to maximise the expected number of completed jobs. Allocating more resources to a given job increases the probability that it completes, but with a cut-off. Specifically, we assume a linear model where the probability increases linearly until it equals one, after which allocating additional resources is wasteful. We assume the difficulty of each job is unknown and present the first algorithm for this problem and prove upper and lower bounds on its regret. Despite its apparent simplicity, the problem has a rich structure: we show that an appropriate optimistic algorithm can improve its learning speed dramatically beyond the results one normally expects for similar problems as the problem becomes resource-laden."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Assume that there are K jobs and at each time-step t a learner must distribute the available resources with Mk,t ≥ 0 going to job k, subject to a budget constraint,\nK ∑\nk=1\nMk,t ≤ 1.\nThe probability that the kth job completes in time-step t is min {1,Mk,t/νk}, where the unknown cut-off parameter νk ∈ (0,∞] determines the difficulty of job k. After every time-step the resources are replenished and all jobs are restarted regardless of whether or not they completed successfully in the previous time-step. The goal of the learner\n∗On sabbatical leave from the Department of Computing Science, University of Alberta, Canada\nis to maximise the expected number of jobs that successfully complete up to some known time horizon n.\nDespite the simple model, the problem is surprisingly rich. Given its information structure, the problem belongs to the class of stochastic partial monitoring problems, which was first studied by Agrawal et al. [1989]1, where in each time step the learner receives noisy information about a hidden “parameter” while trying to maximise the sum of rewards and both the information received and the rewards depend in a known fashion on the actions and the hidden parameter. While partial monitoring by now is relatively well understood, either in the stochastic or the adversarial framework when the action set is finite [Bartók et al., 2011, Foster and Rakhlin, 2012, Bartók, 2013], the case of continuous action sets has received only limited attention [Broder and Rusmevichientong, 2012, and references therein]. To illustrate the difficulty of the problem, notice that over-assigning resources to a given job means that the job completes with certainty and provides little information about the job’s difficulty. On the other hand, if resources are under-assigned, then the information received allows one to learn about the payoff associated with all possible arms, which is reminiscent of bandit problems where the arms have “correlated payoffs” (e.g., Filippi et al. 2010, Russo and Roy 2013 and the references therein). Finally, allocating less resources yields high-variance estimates.\nOur motivation to study this particular framework comes from the problem of cache allocation. In particular, data collected offline from existing and experimental allocation strategies showed a relatively good fit to the above parametric model. In this problem each job is a computer process, which is successful in a given time-step if there were no cache misses (cache misses are very expensive). Besides this specific resource allocation problem, we also envision other applications, such as load balancing in networked environments, or any other computing applications where some precious resource (bandwidth, radio spectrum, CPU, etc.) is to be subdivided amongst competing processes. In fact, we anticipate numerous extensions and\n1The name was invented later by (perhaps) [Rustichini, 1999].\nadaptations for specific applications, such as in the case of bandits (see, Bubeck and Cesa-Bianchi [2012] for an overview of this rich literature). Finally, let us point out that although our problem is superficially similar to the socalled budgeted bandit problems (or, budget limited bandit problems), there are some major differences: in budgeted bandits, the information structure is still that of bandit problems and the resources are not replenished. Either learning stops when the budget is exhausted (e.g., Tran-Thanh et al. 2012, Ding et al. 2013, Badanidiyuru et al. 2013)2, or performance is measured against the total resources consumed in an ongoing fashion (e.g., György et al. 2007).\nThe main contribution besides the introduction of a new problem is a new optimistic algorithm for this problem that is shown to suffer poly-logarithmic regret with respect to optimal omniscient algorithm that knows the parameters (νk)k in advance. The structure of the bound depends significantly on the problem dynamics, ranging from a (relatively) easy full-information-like setting, corresponding to a resource-laden regime, to a bandit-like setting, corresponding to the resource-scant setting. Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too.\nProblems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache. However, they all assume the model is fully known and no learning is required. Bitirgen et al. [2008] used ANNs to predict individual program performance as a function of resources. Finally, Ipek et al. [2008] used reinforcement learning to allocate DRAM to multi-processors."
    }, {
      "heading" : "2 PRELIMINARIES",
      "text" : "In each time-step t the learner chooses Mk,t ≥ 0 subject to the constraint,\n∑K k=1 Mk,t ≤ 1. Then all jobs are exe-\ncuted and Xk,t ∈ {0, 1} indicates the success or failure of job k in time-step t and is sampled from a Bernoulli distribution with parameter β(Mk,t/νk) := min {1,Mk,t/νk}. The goal is to maximise the expected number of jobs that successfully complete, ∑Kk=1 β(Mk,t/νk). We define the gaps ∆j,k = ν −1 j − ν−1k . We assume throughout for conve-\n2Besides Badanidiyuru et al. [2013], all works consider finite action spaces and unstructured reward functions.\nnience, and without loss of generality, that ν1 < ν2 < · · · < νK . It can be shown that the optimal allocation distributes the resources to jobs in increasing order of difficulty.\nM∗k = min\n{ 1− k−1 ∑\ni=1\nM∗i , νk\n}\n.\nWe let ℓ be the number of jobs that are fully allocated under the optimal policy: ℓ = max {i : M∗i = νi}. The overflow is denoted by S∗ = M∗ℓ+1, which we assume to vanish if ℓ = K . The expected reward (number of completed jobs) when following the optimal allocation is\nK ∑\nk=1\nM∗k νk = ℓ+ S∗ νℓ+1 ,\nwhere we define νK+1 = ∞ in the case that ℓ = K . The (expected n-step cumulative) regret of a given allocation algorithm is the difference between the expected number of jobs that complete under the optimal policy and those that complete given the algorithm,\nRn = E\n[\nn ∑\nt=1\nrt\n]\n, rt =\nK ∑\nk=1\nβ(M∗k /νk)− K ∑\nk=1\nβ(Mk,t/νk)\n=\n( ℓ+ S∗\nνℓ+1\n) − K ∑\nk=1\nβ(Mk,t/νk)."
    }, {
      "heading" : "3 OVERVIEW OF ALGORITHM",
      "text" : "We take inspiration from the optimal policy for known νk, which is to fully allocate the jobs with the smallest νk (easiest jobs) and allocate the remainder/overflow to the next easiest job. At each time-step t we replace the unknown νk by a high-probability lower bound νk,t−1 ≤ νk. This corresponds to the optimistic strategy, which assumes that each job is as easy as reasonably possible. The construction of a confidence interval about νk is surprisingly delicate. There are two main challenges. First, the function β(Mk,t/νk) is non-differentiable at Mk,t = νk, and for Mk,t ≥ νk the job will always complete and little information is gained. This is addressed by always using a lower estimate of νk in the algorithm. The second challenge is that Mk,t will vary with time, so the samples Xk,t are not identically distributed. This would normally be unproblematic, since martingale inequalities can be applied, but the specific structure of this problem means that a standard sample average estimator is a little weak in the sense that its estimation accuracy can be dramatically improved. In particular, we will propose an estimator that is able to take advantage of the fact that the variance of Xk,t decreases to zero as Mk,t approaches νk from below.\nAs far as the estimates are concerned, rather than estimate the parameters νk, it turns out that learning the reciprocal\nν−1k is both more approachable and ultimately more useful for proving regret bounds. Fix k and let Mk,1, . . . ,Mk,t ≤ νk be a sequence of allocations with Mk,s ≤ νk and Xk,s ∼ Bernoulli (Mk,s/νk). Then a natural (unbiased) estimator of ν−1k is given by\n1\nν̂k,t :=\n1\nt\nt ∑\ns=1\nXk,s Mk,s .\nThe estimator has some interesting properties. First, the random variable Xk,s/Mk,s ∈ [0, 1/Mk,s] has a large range for small Mk,s, which makes it difficult to control the error ν̂−1k,t − ν−1k via the usual Azuma/Bernstein inequalities. Secondly, if Mk,s is close to νk, then the range of Xk,s/Mk,s is small, which makes estimation easier. Additionally, the variance is greatly decreased for Mk,s close to νk. This suggests that samples for which Mk,s is large are more useful than those where Mk,s is small, which motivates the use of the weighted estimator,\n1\nν̂k,t :=\n∑t s=1 wsXk,s ∑t s=1 wsMk,s ,\nwhere ws will be chosen in a data-dependent way, but with the important characteristic that ws is large for Mk,s close to νk. The pseudo-code of the main algorithm is shown on Algorithm Listing 1. It accepts as input the horizon n, the number of jobs, and a set {νk,0}Kk=1 for which 0 < νk,0 ≤ νk for each k. In Section 7 we present a simple (and efficient) algorithm that relaxes the need for the lower bounds νk,0.\nRemark 1. Later (in Lemma 6) we will show that with high probability 1 ≤ wk,s ≤ O(s). By definition the confidence bounds νk,t and ν̄k,t are non-decreasing/increasing respectively. These results are sufficient to guarantee that the new algorithm is numerically stable. It is also worth noting that the running time of Algorithm 1 is O(1) per time step, since all sums can be computed incrementally."
    }, {
      "heading" : "4 UPPER BOUNDS ON THE REGRET",
      "text" : "The regret of Algorithm 1 depends in a subtle way on the parameters νk. There are four natural cases, which will appear in our main result.\nCase 1: Insufficient budget for any jobs. In this case ℓ = 0 and the optimal algorithm allocates all available resources to the easiest task, which means M∗1 = 1. Knowing that ℓ = 0, the problem can be reduced to a K-armed Bernoulli bandit by restricting the action space to Mk,t = 1 for all k. Then a bandit algorithm such as UCB1 [Auer et al., 2002] will achieve logarithmic (problem dependent) regret with some dependence on the gaps ∆1,k = 1ν1 − 1 νk . In particular, the regret looks like Rn ∈ O ( ∑K k=2 log n ∆1,k ) .\nAlgorithm 1 Optimistic Allocation Algorithm\n1: input: n,K , {νk,0}Kk=1 2: δ ← (nK)−2 and ν̄k,0 = ∞ for each k 3: for t ∈ 1, . . . , n do 4: /* Optimistically choose Mk,t using νk,t−1 */ 5: (∀k ∈ 1, . . . ,K) initialise Mk,t ← 0 6: for i ∈ 1, . . . ,K do 7: k ← argmin\nk:Mk,t=0 νk,t−1\n8: Mk,t ← min { νk,t−1, 1− ∑K j=1 Mj,t }\n9: end for 10: (∀k ∈ 1, . . . ,K) observe Xk,t 11: (∀k ∈ 1, . . . ,K) compute weighted estimates:\nwk,t ← 1\n1− Mk,tν̄k,t−1\n1\nν̂k,t ←\n∑t s=1 wk,sXk,s ∑t s=1 wk,sMk,s\n12: (∀k ∈ 1, . . . ,K) update confidence intervals:\nRk,t ← max s≤t wk,s V̂ 2 k,t ←\n∑\ns≤t\nwk,sMk,s νk,t−1\nε̃k,t ← f(Rk,t, V̂\n2 k,t, δ)\n∑t s=1 wk,sMk,s\n1\nνk,t ← min\n{\n1\nνk,t−1 ,\n1\nν̂k,t + ε̃k,t\n}\n1\nν̄k,t ← max\n{\n1\nν̄k,t−1 ,\n1\nν̂k,t − ε̃k,t\n}\n13: end for\n14: function f (R,V 2, δ) 15: δ0 ← δ3(R+1)2(V 2+1)2 16: return R+13 log 2 δ0\n+ √\n2(V 2 + 1) log 2δ0 + ( R+1 3 )2 log2 2δ0\n17: end function\nCase 2: Sufficient budget for all jobs. In this case ℓ = K and the optimal policy assigns Mk,t = νk for all k, which enjoys a reward of K at each time-step. Now Algorithm 1 will choose Mk,t = νk,t−1 for all time-steps and by Theorem 4 stated below we will have νk,t−1/νk ∈ O(1 − 1t logn). Consequently, the regret may be bounded by Rn ∈ O ( log2 n ) with no dependence on the gaps.\nCase 3: Sufficient budget for all but one job. Now the algorithm must learn which jobs should be fully allocated. This introduces a weak dependence on the gaps ∆ℓ,k for k > ℓ, but choosing the overflow job is trivial. Again we expect the regret to be O(log2 n), but with an additional modest dependence on the gaps.\nCase 4: General case. In the completely general case even\nthe choice of the overflow job is non-trivial. Ultimately it turns out that in this setting the problem decomposes into two sub-problems. Choosing the jobs to fully allocate, and choosing the overflow job. The first component is fast, since we can make use of the faster learning when fully allocating. Choosing the overflow reduces to the bandit problem as described in case 1.\nOur main result is the following theorem bounding the regret of our algorithm.\nTheorem 2. Let δ be as in the algorithm, ηk = min {1, νk} /νk,0, δ̃k = δ48η4\nk n6 , ck,1 = 27 log 2δ̃k , ck,2 =\n6 log 2 δ̃k , uk,j = ck,1 νk,0∆j,k . Then Algorithm 1 suffers regret at most\nRn ≤ 1 + ℓ ∑\nk=1\nck,1ηk(1 + log n)\n+ 1{ℓ < K} [ K ∑\nk=ℓ+2\nck,2 νk,0∆ℓ+1,k +\nℓ+1 ∑\nk=1\nck,1ηk(1 + logn)\n+\nK ∑\nk=ℓ+2\nck,1ηk(1 + log uℓ+1,k) +\nK ∑\nk=ℓ+1\nck,1ηk(1 + log uℓ,k)\n]\n.\nIf we assume ηk ∈ O(1) for each k (reasonable as discussed in Section 7), then the regret bound looks like\nRn ∈ O ( ℓ log2 n+ K ∑\nk=ℓ+1\n(\nlog 1\nνk∆ℓ,k\n)\nlogn (1)\n+\nK ∑\nk=ℓ+2\n(\nlog 1\nνk∆ℓ+1,k\n)\nlogn+\nK ∑\nk=ℓ+1\nlogn\n∆ℓ+1,k\n)\n,\nwhere the first term is due to the gap between νk,t and νk, the second due to discovering which jobs should be fully allocated, while the third and fourth terms are due to mistakes when choosing the overflow job.\nThe proof is broken into two components. In the first part we tackle the convergence of ν̂t,k to νk and analyse the width of the confidence intervals, which are data-dependent and shrink substantially faster when Mk,t is chosen close to νk. In the second component we decompose the regret in terms of the width of the confidence intervals. While we avoided large constants in the algorithm itself, in the proof we focus on legibility. Optimising the constants would complicate an already long result."
    }, {
      "heading" : "5 ESTIMATION",
      "text" : "We consider a single job with parameter ν and analyse the estimator and confidence intervals used by Algorithm 1. We start by showing that the confidence intervals contain the truth with high-probability and then analyse the rate at which the intervals shrink as more more data is observed.\nSomewhat surprisingly the rate has a strong dependence on the data with larger allocations leading to faster convergence.\nLet {Ft}∞t=0 be a filtration and let M1, . . . ,Mn be a sequence of positive random variables such that Mt is Ft−1measurable. Define Xt to be sampled from a Bernoulli distribution with parameter β(Mt/ν) for some ν ∈ [ν0,∞] and assume that Xt is Ft-measurable. Our goal is to construct a sequence of confidence intervals {[νt, ν̄t]}nt=1 such that ν ∈ [νt, ν̄t] with high probability and ν̄t − νt → 0 as fast as possible. We assume a known lower bound ν0 ≤ ν and define ν̄0 = ∞. Recall that the estimator used by Algorithm 1 is defined by\nws = 1\n1− Mtν̄t−1 ,\n1 ν̂t =\n∑t s=1 wsXs ∑t s=1 wsMs .\nFix a number 0 < δ < 1 and define ε̃t = f(Rt, V̂ 2 t , δ)/ ∑t s=1 wsMs, where the function f is defined in Algorithm 1, Rt = maxs≤t ws and V̂ 2t = ∑t\ns=1 wsMs νt−1 . The lower and upper confidence bounds on\nν−1 are defined by,\n1\nνt =min\n{\n1 νt−1 , 1 ν̂t + ε̃t\n}\n, 1\nν̄t =max\n{\n1 ν̄t−1 , 1 ν̂t − ε̃t\n}\n.\nWe define εt = ν −1 t − ν̄−1t to be the (decreasing) width of the confidence interval. Note that both νt and ν̄t depend on δ, although this dependence is not shown to minimise clutter.\nTheorem 3. If Ms is chosen such that Ms ≤ νs−1 for all s then P {∃s ≤ t s.t. ν 6∈ [νs, ν̄s]} ≤ tδ holds for any 0 < δ < 1.\nProof of Theorem 3. Let Ft be the event Ft = {ν ∈ [νt, ν̄t]}. Note that since [νt, ν̄t] ⊂ [νt−1, ν̄t−1] ⊂ · · · ⊂ [ν0, ν̄0], Ft ⊂ Ft−1 ⊂ · · · ⊂ F0. Hence, Ft = ∩s≤tFs and it suffices to prove that P {F ct } ≤ tδ.3\nDefine Ys = wsXs − wsMsν and St = ∑t s=1 Ys and V 2 t = ∑t s=1 Var[Ys|Fs−1]. We proceed by induction. Assume P {\nF ct−1 } ≤ (t − 1)δ, which is trivial for t = 1. Now, on Ft−1,\nV 2t (a) =\nt ∑\ns=1\nVar[Ys|Fs−1] (b) =\nt ∑\ns=1\nw2sMs ν\n(\n1− Ms ν\n)\n(c) =\nt ∑\ns=1\nwsMs ν\n(\n1− Msν 1− Msν̄s−1\n)\n(d) ≤ t ∑\ns=1\nwsMs ν\n(e) ≤ V̂ 2t ,\nwhere (a) is the definition of V 2t , (b) follows since ws is Fs−1-measurable, (c) follows by substituting the definition of ws, (d) and (e) are true since given Ft−1 we\n3For an event E, we use Ec to denote its complement.\nknow that νs−1 ≤ ν ≤ ν̄s−1. Therefore f(Rt, V 2t , δ) ≤ f(Rt, V̂ 2 t , δ), which follows since f is monotone increasing in its second argument. Therefore,\nP\n{∣\n∣ ∣ ∣\n1 ν̂t − 1 ν\n∣ ∣ ∣ ∣ ≥ ε̃t ∧ Ft−1 }\n= P\n{∣\n∣ ∣ ∣ ∣\n∑t s=1 wsXs ∑t s=1 wsMs − 1 ν\n∣ ∣ ∣ ∣ ∣ ≥ f(Rt, V̂ 2 t , δ)\n∑t s=1 wsMs\n∧ Ft−1 }\n≤ P {∣ ∣ ∣\n∣ ∣\nt ∑\ns=1\nwsXs − t ∑\ns=1\nwsMs ν\n∣ ∣ ∣ ∣ ∣ ≥ f(Rt, V 2t , δ) ∧ Ft−1 }\n= P { |St| ≥ f(Rt, V 2t , δ) ∧ Ft−1 } . (2)\nBy the union bound we have\nP\n{ |St| ≥ f(Rt, V̂ 2t , δ) ∨ F ct−1 }\n≤ P { |St| ≥ f(Rt, V 2t , δ) ∧ Ft−1 } + P { F ct−1 }\n(a) ≤ δ + P {\nF ct−1 } ≤ δ + (t− 1)δ = tδ ,\nwhere (a) follows from Theorem 13 in the Appendix. Therefore P {\n|St| ≤ f(Rt, V 2t , δ) ∧ Ft−1 } ≥ 1 − tδ and so with probability at least 1− tδ we have that Ft−1 and\n∣ ∣ ∣ ∣ 1 ν̂t − 1 ν ∣ ∣ ∣ ∣ ≤ f(Rt, V̂ 2 t , δ)\n∑t s=1 wsMs\n= ε̃t,\nin which case\n1\nνt = min\n{\n1 νt−1 , 1 ν̂t + ε̃t\n}\n≥ 1 ν ,\nand similarly 1ν̄t ≤ 1 ν , which implies Ft. Therefore P {F ct } ≤ tδ as required.\nWe now analyse the width εt ≡ ν−1t − ν̄−1t of the confidence interval obtained after t samples are observed. We say that a job is fully allocated at time-step s if Ms = νs−1. The first theorem shows that the width εt drops with order O(1/T (t)), where T (t) =\n∑t s=1 1{Ms = νs−1} is the\nnumber of fully allocated time-steps. The second theorem shows that for any α > 0, the width εt drops with order O( √ 1/(αUα(t))), where Uα(t) = ∑t\ns=1 1{Ms ≥ α}. The dramatic difference in speeds is due to the low variance Var[Xt|Ft−1] when Mt is chosen close to ν. For the next results define η = min {1, ν} /ν0 and δ̃ = δ48η4n6 . Theorem 4. εt ≤ c1\nν0(T (t) + 1) where c1 = 27 log 2δ̃ .\nTheorem 5. εt ≤ √ c2 αν0Uα(t) where c2 = 6 log 2δ̃ .\nThe proofs are based on the following lemma that collects some simple observations:\nLemma 6. The following hold for any t ≥ 1:\n1. wtMt ≤ 1εt−1 , with equality if Mt = νt−1. 2. 1 ≤ Rt ≤ 1ν0εt−1 . 3. εt ≥ 1tmin{1,ν} . 4. 1− νtν ≤ νtεt.\nProof. Using the definition of ws and the fact that Ms is always chosen to be smaller or equal to νs−1, we get\nws ≡ ( 1− Ms ν̄s−1 )−1 (a) ≤ ( 1− νs−1 ν̄s−1 )−1 =\n1\nεs−1νs−1 .\nThe first claim follows since the inequality (a) can be replaced by equality if Ms = νs−1. The second follows from the definition of Rt and the facts that (εs)s is nonincreasing and (νs)s is non-decreasing. For the third claim we recall that Rt = maxs≤t ws and Ms ≤ ν. Therefore,\nεt (a) ≥ min { εt−1, Rt\n∑t s=1 wsMs\n}\n(b) ≥ min { εt−1, 1\ntmin {1, ν}\n}\n,\nwhere (a) follows from the definition of εt and naive bounding of the function f , (b) follows since Rt ≥ ws for all s ≤ t and because Ms ≤ min {1, ν} for all s. Trivial induction and the fact that ε0 = ν −1 0 ≥ ν−1 completes the proof of the third claim. For the final claim we use the facts that ν−1t ≤ ν−1 + εt. Therefore, 1− νtνt = νt ( 1 νt − 1ν )\n≤ νtεt.\nLemma 7. εt ≤ 6Rt log\n2 δ̃\n∑t s=1 wsMs\n+\n√ √ √ √\n8 log 2 δ̃\nν0 ∑t s=1 wsMs .\nProof. Let δt = δ/(3(Rt + 1)2(V̂ 2t + 1) 2) < 1. By the definition of εt,\nεt ≤ 2f(Rt, V̂\n2 t , δ)\n∑t s=1 wsMs\n(a) ≤ 4(Rt+1) 3 log 2 δt\n+ 2 √\n2(V̂ 2t + 1) log 2 δt\n∑t s=1 wsMs\n(b) ≤ 6Rt log\n2 δt\n+ √\n8 ν0 ∑t s=1 wsMs log 2 δt\n∑t s=1 wsMs\n= 6Rt log\n2 δt\n∑t s=1 wsMs\n+\n√\n8 log 2δt ν0 ∑t s=1 wsMs ,\nwhere in (a) we used the definition of f , in (b) we substituted the definition of V̂ 2t and used the facts that Rt ≥ 1 and ν0 ≤ νt−1 and we also used a naive bound. The proof is completed by proving 2/δt ≤ 2/δ̃. Indeed, by Lemma 6,\n1 ≤ Rt ≤ 1εt−1ν0 ≤ 1 εtν0 . We also have V̂ 2t ≤ tR2t . Thus,\n2 δt =\n6(Rt + 1) 2(V̂ 2t + 1) 2\nδ ≤ 6 δ\n(\n16t2\n(εtν0) 4\n)\n(a) ≤ 2 δ̃ ,\nwhere in (a) we used Lemma 6(3).\nProof of Theorem 4. By Lemma 7,\nεt ≤ 6Rt log\n2 δ̃\n∑t s=1 wsMs\n+\n√\n8\nν0 ∑t s=1 wsMs log\n2 δ̃ . (3)\nWe proceed by induction. Assume that εs−1 ≤ c1 ν0(T (s−1)+1) , which is trivial for s = 1. By Lemma 6(1),\nt ∑\ns=1\nwsMs ≥ T (t) ∑\ns=1\nsν0 c1 = ν0T (t)(T (t) + 1) 2c1 . (4)\nTherefore, √\n8\nν0 ∑t s=1 wsMs log\n2\nδ̃\n(a) ≤ 1 ν0T (t)\n√\n4c1 log 2\nδ̃ . (5)\nNow we work on the first term in (3). If εt−1 ≤ c1ν0(T (t)+1) , then we are done, since εs is non-increasing. Otherwise, we use Lemma 6(2) to obtain,\n6Rt ∑t\ns=1 wsMs log\n2 δ̃ ≤ 6 ν0εt−1 ∑t s=1 wsMs log 2 δ̃\n(a) ≤ 3 ν0T (t) log 2 δ̃ , (6)\nwhere in (a) we used (4) and the lower bound on εt−1. Substituting (5) and (6) into (3) we have\nεt ≤ 1\nν0T (t)\n√\n4c1 log 2\nδ̃ +\n3\nν0T (t) log\n2 δ̃ .\nChoosing c1 = 27 log 2δ̃ leads to\nεt ≤ 1\nν0T (t)\n√\n4 · 27 log2 2 δ̃ +\n3\nν0T (t) log\n2\nδ̃\n≤ 27 ν0(T (t) + 1) log 2 δ̃ = c1 ν0(T (t) + 1) ,\nwhich completes the induction and proof.\nProof of Theorem 5. Firstly, by Lemma 7,\nεt ≤ 6Rt\n∑t s=1 wsMs\nlog 2\nδ̃ +\n√\n8\nν0 ∑t s=1 wsMs log\n2 δ̃ .\nThe second term is easily bounded by using the fact that ws ≥ 1 and the definition of Uα(t),\n√\n8\nν0 ∑t s=1 wsMs log\n2 δ̃ ≤\n√\n8\nν0Uα(t)α log\n2 δ̃ .\nFor the first term we assume εt−1 ≥ √\nc2 ν0Uα(t)α , since\notherwise we can apply monotonicity of εt. Therefore\n6Rt ∑t\ns=1 wsMs log\n2 δ̃ ≤ 6 ν0εt−1 ∑t s=1 wsMs log 2 δ̃\n≤ √\nUα(t)αν0 c2\n· 6 log 2 δ̃\nν0Uα(t)α ≤ 6\n√\n1\nc2αν0Uα(t) log\n2 δ̃ .\nNow choose c2 = 6 log 2δ̃ to complete the result."
    }, {
      "heading" : "6 PROOF OF THEOREM 2",
      "text" : "We are now ready to use the results of Section 5 to bound the regret of Algorithm 1. The first step is to decompose the regret into two cases depending on whether or not the confidence intervals contain the truth. The probability that they do not is low, so this contributes negligibly to the regret. When the confidence intervals are valid we break the problem into two components. The first is the selection of the processes to fully allocation, which leads to the O(log2 n) part of the bound. The second component involves analysing the selection of the overflow process, where the approach is reminiscent of the analysis for the UCB algorithm for stochastic bandits [Auer et al., 2002].\nLet Fk,t denote the event when none of the confidence intervals underlying job k fail up to time t:\nFk,t = {∀s ≤ t : ν ∈ [νk,s, ν̄k,s]} .\nThe algorithm uses δ = (nK)−2, which is sufficient by a union bound and Theorem 3 to ensure that,\nP {Gc} ≤ 1 nK\n, where G = K ⋂\nk=1\nFk,n . (7)\nThe regret can be decomposed into two cases depending on whether G holds:\nRn = E\nn ∑\nt=1\nrt (a) = E1{Gc}\nn ∑\nt=1\nrt + E1{G} n ∑\nt=1\nrt (8)\n(b) ≤ E1{Gc}nK + E1{G} n ∑\nt=1\nrt (c) ≤ 1 + E1{G} n ∑\nt=1\nrt,\nwhere (a) follows from the definition of expectation, (b) is true by bounding rt ≤ K for all t, and (c) follows from (7). For the remainder we assume G holds and use Theorems 4 and 5 combined with the definition of the algorithm to control the second term in (8). The first step is to decompose the regret in round t:\nrt = ℓ ∗ +\nS∗\nνℓ+1 −\nK ∑\nk=1\nβ\n(\nMk,t νk\n)\n.\nBy the assumption that G holds we know for all t ≤ n and k that ν̄−1k,t ≤ ν−1k ≤ ν−1k,t . Therefore Mk,t ≤ νk,t−1 ≤ νk, which means that β(Mk,t/νk) = Mk,t/νk. Define πt(i) ∈ {1, . . . ,K} such that νπt(i),t−1 ≤ νπt(i+1),t−1. Also let\nAt = {k : Mk,t = νk,t−1} , A≤jt = At ∩ {πi(t) : 1 ≤ i ≤ j} ,\nTk(t) =\nt ∑\ns=1\n1{k ∈ At} and Bt = πt(ℓ+ 1).\nInformally, At is the set of jobs that are fully allocated at time-step t, A≤jt is a subset of At containing the j jobs believed to be easiest, Tk(t) is the number of times job k has been fully allocated at time-step t, and Bt is the (ℓ + 1)th easiest job at time-step t (this is only defined if ℓ < K and will only be used in that case).\nLemma 8. For all t, |At| ≥ ℓ and if |At| = ℓ, then MBt,t ≥ S∗.\nProof. |At| = max { j : ∑j i=1 νπt(i),t−1 ≤ 1 } . But νk,t−1 ≤ νk for all k and t, so ∑ℓ\ni=1 νπt(i),t−1 ≤ ∑ℓ\nk=1 νk,t−1 ≤ ∑ℓ k=1 νk ≤ 1. Therefore |At| ≥ ℓ. If |At| = ℓ, then Bt /∈ At is the overflow job and so MBt,t = 1 − ∑ k∈At νk,t−1 ≥ 1 − ∑\nk∈A∗ νk,t−1 ≥ 1−∑k∈A∗ νk ≡ S∗\nWe now decompose the regret, while still assuming that G holds:\nn ∑\nt=1\nrt =\nn ∑\nt=1\n(\nℓ+ S∗\nνℓ+1 −\nK ∑\nk=1\nMk,t νk\n)\n≤ n ∑\nt=1\n∑\nk∈A≤ℓt\n(\n1− Mk,t νk\n)\n(9)\n+ 1{ℓ < K} n ∑\nt=1\n(\nS∗\nνℓ+1 − MBt,t νBt\n)\n. (10)\nLet us bound the first sum:\nn ∑\nt=1\n∑\nk∈A≤ℓt\n(\n1− Mk,t νk\n)\n=\nn ∑\nt=1\nK ∑\nk=1\n1\n{ k ∈ A≤ℓt }\n(\n1− νk,t−1 νk\n)\n(a) ≤ n ∑\nt=1\nK ∑\nk=1\n1\n{ k ∈ A≤ℓt } νk,t−1εk,t−1\n(b) ≤ n ∑\nt=1\nK ∑\nk=1\n1\n{ k ∈ A≤ℓt } ck,1νk,t−1\nνk,0Tk(t) , (11)\nwhere (a) follows by Lemma 6 and (b) by Theorem 4.\nLemma 9. If k > j, then\nn ∑\nt=1\n1\n{ k ∈ A≤jt } ≤ ck,1 νk,0∆j,k =: uj,k.\nProof. Assume k ∈ A≤jt . Therefore νk,t−1 ≤ νj . But if uj,k < ∑t s=1 1 { k ∈ A≤js } ≤ Tk(t− 1) + 1, then\n1 νk,t−1 ≤ 1 νk + εk,t−1 = 1 νj + εk,t−1 −∆j,k\n(a) ≤ 1 νj + ck,1 νk,0(Tk(t− 1) + 1) −∆j,k < 1 νj ,\nwhere (a) follows from Theorem 4. Therefore k ∈ A≤jt implies that ∑t s=1 1 { k ∈ A≤js }\n≤ uj,k and so ∑n\nt=1 1\n{ k ∈ A≤jt } ≤ uj,k as required.\nContinuing (11) by applying Lemma 9 with j = ℓ:\nn ∑\nt=1\nK ∑\nk=1\n1\n{ k ∈ A≤ℓt } ck,1νk,t−1\nνk,0Tk(t)\n=\nn ∑\nt=1\n∑\nk∈A∗ 1\n{ k ∈ A≤ℓt } ck,1νk,t−1\nνk,0Tk(t)\n+\nn ∑\nt=1\n∑\nk/∈A∗ 1\n{ k ∈ A≤ℓt } ck,1νk,t−1\nνk,0Tk(t) (12)\n(a) ≤ ∑\nk∈A∗\nn ∑\nt=1\nck,1ηk t + ∑\nk/∈A∗\nuℓ,k ∑\nt=1\nck,1ηk t\n≤ ℓ ∑\nk=1\nck,1ηk(1 + logn) +\nK ∑\nk=ℓ+1\nck,1ηk(1 + log uℓ,k),\nwhere (a) follows by Lemma 9 and the fact that k ∈ A≤ℓt implies that νk,t−1νk,0 ≤ ηk. Now if ℓ = K , then the second term in (9) is zero and the proof is completed by substituting the above result into (9) and then into (8). So now we assume ℓ > K and bound the second term in (9) as follows:\nn ∑\nt=1\n( S∗\nνℓ+1 − MBt,t νBt\n) ≤ n ∑\nt=1\n1{Bt ∈ At} ( 1− νBt,t−1 νBt )\n+\nn ∑\nt=1\n1{Bt /∈ At} ( S∗\nνℓ+1 − S\n∗\nνBt\n)\n, (13)\nwhere we used Lemma 8 and S∗ ≤ 1 and that if Bt ∈ At,\nthen MBt,t = νBt,t−1. Bounding each term separately:\nn ∑\nt=1\n1{Bt ∈ At} ( 1− νBt,t−1 νBt )\n(a) ≤ K ∑\nk=1\nn ∑\nt=1\n1\n{ k ∈ A≤ℓ+1t }\n(\n1− νk,t−1 νk\n)\n(b) ≤ K ∑\nk=1\nn ∑\nt=1\n1\n{ k ∈ A≤ℓ+1t } νk,t−1εk,t−1 (14)\n(c) ≤ K ∑\nk=1\nn ∑\nt=1\n1\n{ k ∈ A≤ℓ+1t } ck,1νk,t−1\nνk,0Tk(t)\n(d) ≤ ℓ+1 ∑\nk=1\nck,1ηk(1 + logn) +\nK ∑\nk=ℓ+2\nck,1ηk(1 + log uℓ+1,k),\nwhere (a) follows since Bt ∈ At implies that Bt ∈ A≤ℓ+1t , (b) follows from Lemma 6(4), (c) by Theorem 4, and (d) follows from Lemma 9 and the same analysis as (12). For the second term we need the following lemma, which uses Theorem 5 and a reasoning analogues to that of Auer et al. [2002] to bound the regret of the UCB algorithm for stochastic bandits:\nLemma 10. Let Uk(t) = ∑t s=1 1{Mk,s ≥ S∗} and k > ℓ+ 1. If Uk(t) ≥ ck,2S∗νk,0∆2ℓ+1,k =: vk, then k 6= Bt.\nProof. If νk,t−1 > νℓ+1, then k 6= Bt. Furthermore, if Uk(t) > vk, then\n1 νk,t−1 ≤ 1 νk + εk,t−1 = 1 νℓ+1 −∆ℓ+1,k + εk,t−1\n(a) ≤ 1 νℓ+1 −∆ℓ+1,k + √ ck,2 νk,0S∗Uk(t) < 1 νℓ+1 ,\nwhere (a) follows from Theorem 5.\nTherefore n ∑\nt=1\n1{Bt /∈ At} ( S∗\nνℓ+1 − S\n∗\nνBt\n)\n(a) ≤ S∗ K ∑\nk=1\nn ∑\nt=1\n1{k = Bt /∈ At}∆ℓ+1,k\n(b) ≤ S∗ K ∑\nk=ℓ+2\nn ∑\nt=1\n1{k = Bt /∈ At}∆ℓ+1,k\n(c) ≤ S∗ K ∑\nk=ℓ+2\nn ∑\nt=1\n1{k = Bt ∧Mk,t ≥ S∗}∆ℓ+1,k\n(d) ≤ K ∑\nk=ℓ+2\nS∗∆ℓ+1,kvk (e) =\nK ∑\nk=ℓ+2\nck,2 νk,0∆ℓ+1,k , (15)\nwhere (a) follows from the definition of ∆ℓ+1,k and the fact that if Bt /∈ At, then |At| = ℓ, (b) follows since ∆ℓ+1,k is\nnegative for k ≤ ℓ+1, (c) by Lemma 8, (d) by Lemma 10, and (e) by the definition of vk. Substituting (14) and (15) into (13) we have\nn ∑\nt=1\n( S∗\nνℓ+1 − MBt,t νBt\n) ≤ ℓ+1 ∑\nk=1\nck,1ηk(1 + logn)\n+\nK ∑\nk=ℓ+2\nck,1ηk(1 + log uℓ+1,k) +\nK ∑\nk=ℓ+2\nck,2 νk,0∆ℓ+1,k .\nWe then substitute this along with (12) into (9) and then (8) to obtain\nRn ≤ 1 + ℓ ∑\nk=1\nck,1ηk(1 + log n)\n+ 1{ℓ < K} [ K ∑\nk=ℓ+2\nck,2 νk,0∆ℓ+1,k\n+ ℓ+1 ∑\nk=1\nck,1ηk(1 + logn)\n+\nK ∑\nk=ℓ+2\nck,1ηk(1 + log uℓ+1,k) +\nK ∑\nk=ℓ+1\nck,1ηk(1 + log uℓ,k)\n]\n."
    }, {
      "heading" : "7 INITIALISATION",
      "text" : "Previously we assumed a known lower bound νk,0 ≤ νk for each k. In this section we show that these bounds are easily obtained using a halving trick. In particular, the following algorithm computes a lower bound ν0 ≤ ν for a single job with unknown parameter ν.\nAlgorithm 2 Initialisation of ν0 1: for t ∈ 1, . . . ,∞ do 2: Allocate Mt = 2−t and observe Xt 3: if Xt = 0 then return ν0 ← 2−t. 4: end for\nA naive way to eliminate the need for the lower bounds (νk,0)k is simply to run Algorithm 2 for each job sequentially. Then the following proposition shows that η ∈ O(1) is reasonable, which justifies the claim made in (1) that the ηk terms appearing in Theorem 2 are O(1).\nProposition 11. If η = min{1,ν}ν0 , then Eη ≤ 4.\nProof. Let pt be the probability that the algorithm ends after time-step t, which is\npt = (1− β(2−t/ν)) t−1 ∏\ns=1\nβ(2−s/ν).\nTherefore\nEη = E\n[\nmin {1, ν} ν0\n] = ∞ ∑\nt=1\npt · min {1, ν}\nMt\n= min {1, ν} ∞ ∑\nt=1\n2t(1 − β(2−t/ν)) t−1 ∏\ns=1\nβ(2−s/ν)\n≤ 4,\nwhere the final inequality follows from an arduous, but straight-forward, computation.\nThe problem with the naive method is that the expected running time of Algorithm 2 is O(log 1ν ), which may be arbitrary large for small ν and lead to a high regret during the initialisation period. Fortunately, the situation when ν is small is easy to handle, since the amount of resources required to complete such a job is also small. The trick is to run K offset instances of Algorithm 2 alongside a modified version of Algorithm 1. First we describe the parallel implementations of Algorithm 2. For job k, start Algorithm 2 in time-step k, which means that the total amount of resources used by the parallel copies of Algorithm 2 in time-step t is bounded by K ∑\nk=1\n1{t ≥ k} 2k−t−1\n≤ min { 1, 2K−t } . (16)\nJob Mk,1 Mk,2 Mk,3 Mk,4 1 1/2 1/4 1/8 1/16 2 0 1/2 1/4 1/8\n3 0 0 1/2 1/4 K∑\nk=1\nMk,t 1/2 3/4 7/8 7/16\nAlgorithm 1 is implemented starting from time-step 1, but only allocates resources to jobs for which the initialisation process has completed. Estimates are computed using only the samples for which Algorithm 1 chose the allocation, which ensures that they are based on allocations with Mk,t ≤ νk. Note that the analysis of the modified algorithm does not depend on the order in which the parallel processes are initialised. The regret incurred by the modified algorithm is given in order notation in (1). The proof is omitted, but relies on two observations. First, that the expected number of time-steps that a job is not (at least) fully allocated while it is being initialised is 2. The second is that the resources available to Algorithm 1 at time-step t converges exponentially fast to 1 by (16)."
    }, {
      "heading" : "8 MINIMAX LOWER BOUNDS",
      "text" : "Despite the continuous action space, the techniques used when proving minimax lower bounds for standard stochastic bandits [Auer et al., 1995] can be adapted to our setting.\nTheorem 12. Given fixed n and 8n ≥ K ≥ 2 and an arbitrary algorithm, there exists an allocation problem for which the expected regret satisfies Rn ≥ √ nK\n16 √ 2 .\nProof. Let 1 ≥ ε > 0 be a constant to be chosen later. We consider a set of K allocation problems where in problem\nk, νj = 2 for all j 6= k and νk = 21+ε . The optimal action in problem k is to assign all available resources to job k when the expected reward is 1+ε2 . The interaction between the algorithm and a problem k defines a measure Pk on the set of outcomes (successes, allocations). We write Ek for expectations with respect to measure Pk. We have\nEk\n[\nn ∑\nt=1\nMk,t\n] − E0 [ n ∑\nt=1\nMk,t\n]\n≤ n √ 1\n2 KL(P0,Pk) ,\n(17)\nwhere KL(P0,Pk) is the Kullback-Leibler divergence (or relative entropy) between P0 and Pk. The divergence is bounded by\nKL(P0,Pk) (a) ≤ E0 [ n ∑\nt=1\nε2M2k,t 4\n(\n1 Mk,t 2 + 1 1− Mk,t2\n)]\n(b) = E0\n[\nn ∑\nt=1\nε2Mk,t 2−Mk,t\n]\n(c) ≤ ε2E0 [ n ∑\nt=1\nMk,t\n]\n, (18)\nwhere (a) follows from the telescoping property of the KL divergence and by bounding the KL divergence by the χsquared distance, (b) is trivial and (c) follows since Mk,t ≤ 1. The n-step expected regret given environment k is\nRn(k) = n(1 + ε)\n2 − Ek\nn ∑\nt=1\nK ∑\nj=1\nMj,t νj\n(b) ≥ ε 2\n(\nn− Ek n ∑\nt=1\nMk,t\n)\n(19)\nwhere (b) follows by recalling that νj = 2 unless j = k, when νj = 2/(1 + ε). Therefore,\nsup k\nRn(k) (a) ≥ 1 K\nK ∑\nk=1\nRn(k)\n(b) ≥ 1 K\nK ∑\nk=1\nε\n2\n(\nn− Ek n ∑\nt=1\nMk,t\n)\n(c) ≥ 1 K\nK ∑\nk=1\nε\n2\n n− E0 n ∑\nt=1\nMk,t − nε\n√ √ √ √ 1\n2 E0\nn ∑\nt=1\nMk,t\n\n\n(d) ≥ ε 2K\n nK − n− nε K ∑\nk=1\n√ √ √ √ 1\n2 E0\nn ∑\nt=1\nMk,t\n\n\n(e) ≥ ε 2K\n\nnK − n− nε\n√ √ √ √ K\n2\nK ∑\nk=1\nE0\nn ∑\nt=1\nMk,t\n\n\n(f) ≥ ε 2K\n(\nnK − n− nε √ nK\n2\n)\n(g) ≥ εn 4 − ε 2n 3 2 2 √ 2K 1 2 ,\nwhere (a) follows since the max is greater than the average, (b) follows from (19), (c) is obtained by combining (17)\nand (18), (d) follows from the fact that ∑K k=1 Mk,t ≤ 1, (e) is true by Jensen’s inequality and (f/g) are straightforward. Choosing ε = √\nK/(8n) leads to supk Rn(k) ≥√ nK/(16 √ 2) as required."
    }, {
      "heading" : "9 EXPERIMENTS",
      "text" : "Data points were generated using the modified algorithm described in Section 7 and by taking the mean of 300 samples. With this many samples the standard error is relatively low (and omitted for readability). We should note that the variance in the regret of the modified algorithm is reasonably large, because the regret depends linearly on the random ηk. For known lower bounds the variance is extremely low. To illustrate the behaviour of the algorithm we performed four experiments on synthetic data with K = 2, which are plotted below as TL (top left), TR, BL, BR (bottom right) respectively. In TL we fixed n = 104, ν1 = 2 and plotted the regret as a function of ν2 ∈ [2, 10]. The experiment shows the usual bandit-like dependence on the gap 1/∆1,2. In TR we fixed ν1 = 4/10, ν2 = 6/10 and plotted Rn/ log\n2 n as a function of n. The experiment lies within case 2 described in Section 4 and shows that the algorithm suffers regret Rn ≈ 45 log2 n as predicted by Theorem 2. In BL we fixed n = 105, ν1 = 4/10 and plotted the regret as a function of ν2 ∈ [4/10, 1]. The results show the algorithm suffering O(log2 n) regret for both processes until the critical point when ν2 > 6/10 when the second process can no longer be fully allocated, which is quickly learned and the algorithm suffers O(log2 n) regret for only one process. In BR we fixed ν1 = 4/10 and ν2 = 6/10 and plotted the regret as a function of n for two algorithms. The first algorithm (solid blue) is the modified version of Algorithm 1 as described in Section 7. The second (dotted red) is the same, but uses the unweighted estimator wk,t = 1 for all k and t. The result shows that both algorithms suffer sub-linear regret, but that the weighted estimator is a significant improvement over the unweighted one.\n2 3 4 5 6\n140\n160\n180\nν2\nR n\n0 1e6\n20\n40\nn\nR n lo g 2 n\n0.4 0.6 0.8 1\n2e3\n5e3\nν2\nR n\n0 1e5 0\n1e4\nn\nR n"
    }, {
      "heading" : "10 CONCLUSIONS",
      "text" : "We introduced the linear stochastic resource allocation problem and a new optimistic algorithm for this setting. Our main result shows that the new algorithm enjoys a (squared) logarithmic problem-dependent regret. We also presented a minimax lower bound of Ω( √ nK), which is\nconsistent with the problem-dependent upper bound. The simulations confirm the theory and highlight the practical behaviour of the new algorithm. There are many open questions and possibilities for future research. Most important is whether the log2 n can be reduced to logn. Problemdependent lower bounds would be interesting. The algorithm is not anytime (although a doubling trick presumably works in theory). Developing and analysing algorithms when the horizon it not known, and have high-probability bounds are both of interest. We also wonder if Thompson sampling can be efficiently implemented for some reasonable prior, and if it enjoys the same practical and theoretical guarantees in this domain as it does for bandits. Other interesting extensions are when resources are not replenished, or the state of the jobs follow a Markov process. Finally, we want to emphasise that we have made just the first steps towards developing this new and interesting setting. We hope to see significant activity extending and modifying the model/algorithm for specific problems.\nAcknowledgements This work was supported by the Alberta Innovates Technology Futures, NSERC, by EU Framework 7 Project No. 248828 (ADVANCE), and by Israeli Science Foundation grant ISF- 1567/10. Part of this work was done while Csaba Szepesvári was visiting Technion."
    }, {
      "heading" : "A TECHNICAL INEQUALITIES",
      "text" : "The proof of the following theorem is given in the supplementary material.\nTheorem 13. Let δ ∈ (0, 1) and X1, . . . , Xn be a sequence of random variables adapted to filtration {Ft} with E[Xt|Ft−1] = 0. Let Rt be Ft−1-measurable such that |Xt| ≤ Rt almost surely, R = maxt≤n Rt. Define S =\n∑n t=1 Xt, V 2 = ∑n t=1 Var[Xt|Ft−1], and\nδr,v = δ\n3(r + 1)2(v + 1)2 ,\nf(r, v) = r + 1\n3 log\n2\nδr,v\n+\n√\n2(v + 1) log 2\nδr,v +\n(\nr + 1\n3\n)2\nlog2 2\nδr,v .\nThen P { |S| ≥ f(R, V 2) } ≤ δ.\nProof of Theorem 13. Note that f(r, v) is strictly monotone increasing in both r and v. We now use a peeling\nargument. We have,\nP { |Sn| ≥ f(R, V 2) }\n(a) =\n∞ ∑\nr=1\n∞ ∑\nv=1\nP { |Sn| ≥ f(R, V 2), ⌈ V 2 ⌉ = v, ⌈R⌉ = r }\n(b) ≤ ∞ ∑\nr=1\n∞ ∑\nv=1\nP { |Sn| ≥ f(r − 1, v − 1), ⌈ V 2 ⌉ = v, ⌈R⌉ = r }\n(c) ≤ ∞ ∑\nr=1\n∞ ∑\nv=1\n2 exp\n(\n− f(r − 1, v − 1) 2\n2v + 2rf(r−1,v−1)3\n)\n(d) ≤ ∞ ∑\nr=1\n∞ ∑\nv=1\nδr−1,v−1 (e) =\nδ\n3\n∞ ∑\nr=1\n∞ ∑\nv=1\n1\nv2r2\n(f) ≤ δ ,\nwhere (a) follows from the positivity of R and V , (b) by the monotonicity of f , (c) by Theorem 14 stated below (a martingale version Bernstein’s inequality), (d) by Lemma 15, (e) by the definition of δr,v, (f) is trivial.\nTheorem 14 (Theorem 3.15 of McDiarmid 1998, see also Freedman 1975 and Bernstein 1946). Let X1, . . . , Xn be a sequence of random variables adapted to the filtration {Ft} with E[Xt|Ft−1] = 0. Further, let Rt be Ft−1measurable such that Xt ≤ Rt almost surely, R = maxt≤n Rt and V 2 = ∑n t=1 Var[Xt|Ft−1]. Then for any ε, r, v > 0,\nP\n{\nn ∑\nt=1\nXt ≥ ε, V 2 ≤ v,R ≤ r } ≤ exp ( − ε 2\n2v + 2εr3\n)\n.\nWe note that although this inequality is usually stated for deterministic Rt, the extension is trivial: Just define Yt = Xt 1{Rt ≤ r} and apply the standard inequality to Yt. The result then follows since on R ≤ r, Yt = Xt for all t and thus\n∑n t=1 Xt = ∑n t=1 Yt.\nLemma 15. If ε ≥ r3 log 2δ + √ 2v log 2δ + r2 9 log 2 2 δ , then 2 exp (\n− ε2 2v+ 2εr\n3\n)\n≤ δ."
    }, {
      "heading" : "B TABLE OF NOTATION",
      "text" : "K number of jobs\nn time horizon\nνk parameter characterising difficulty of job k β(p) function β(p) := min {1, p} Mk,t resources assigned to job k in time-step t\nXk,t outcome of job k in time-step t\nνk,t lower bound on νk at time-step t\nν̄k,t upper bound on νk at time-step t\nδ bound on probability that some confidence intervals fails\nπt(i) ith easiest job at time-step t sorted by νk,t−1 ℓ number of fully allocated jobs under optimal\nallocation\nS∗ optimal amount of resources assigned to overflow process A∗ contains the ℓ easiest jobs (sorted by νk)\nAt set of jobs with Mk,t = νk,t−1 at time-step t\nBt equal to πt(ℓ+ 1) ηk min{1,νk}\nνk,0"
    } ],
    "references" : [ {
      "title" : "Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space",
      "author" : [ "Rajeev Agrawal", "Demosthenis Teneketzis", "Venkatachalam Anantharam" ],
      "venue" : "IEEE Transaction on Automatic Control,",
      "citeRegEx" : "Agrawal et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 1989
    }, {
      "title" : "Gambling in a rigged casino: The adversarial multi-armed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "Auer et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 1995
    }, {
      "title" : "Finitetime analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicoló Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Bandits with knapsacks",
      "author" : [ "Ashwinkumar Badanidiyuru", "Robert Kleinberg", "Aleksandrs Slivkins" ],
      "venue" : "In FOCS, pages 207–216,",
      "citeRegEx" : "Badanidiyuru et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Badanidiyuru et al\\.",
      "year" : 2013
    }, {
      "title" : "A near-optimal algorithm for finite partialmonitoring games against adversarial opponents",
      "author" : [ "Gábor Bartók" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Bartók.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bartók.",
      "year" : 2013
    }, {
      "title" : "Minimax regret of finite partial-monitoring games in stochastic environments",
      "author" : [ "Gábor Bartók", "Dávid Pál", "Csaba Szepesvári" ],
      "venue" : "COLT",
      "citeRegEx" : "Bartók et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bartók et al\\.",
      "year" : 2011
    }, {
      "title" : "The Theory of Probabilities (Russian)",
      "author" : [ "Sergei Bernstein" ],
      "venue" : "Moscow,",
      "citeRegEx" : "Bernstein.,? \\Q1946\\E",
      "shortCiteRegEx" : "Bernstein.",
      "year" : 1946
    }, {
      "title" : "Dynamic pricing under a general parametric choice model",
      "author" : [ "Josef Broder", "Paat Rusmevichientong" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Broder and Rusmevichientong.,? \\Q2012\\E",
      "shortCiteRegEx" : "Broder and Rusmevichientong.",
      "year" : 2012
    }, {
      "title" : "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Foundations and Trends in Machine Learning",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi" ],
      "venue" : "Now Publishers Incorporated,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Multi-armed bandit with budget constraint and variable costs",
      "author" : [ "Wenkui Ding", "Tao Qin", "Xu-Dong Zhang", "Tie-Yan Liu" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Ding et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2013
    }, {
      "title" : "Parametric bandits: The generalized linear case",
      "author" : [ "Sarah Filippi", "Olivier Cappé", "Aurélien Garivier", "Csaba Szepesvári" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Filippi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Filippi et al\\.",
      "year" : 2010
    }, {
      "title" : "No internal regret via neighborhood watch",
      "author" : [ "Dean P. Foster", "Alexander Rakhlin" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track (AISTATS),",
      "citeRegEx" : "Foster and Rakhlin.,? \\Q2012\\E",
      "shortCiteRegEx" : "Foster and Rakhlin.",
      "year" : 2012
    }, {
      "title" : "On tail probabilities for martingales",
      "author" : [ "David A. Freedman" ],
      "venue" : "The Annals of Probability, 3(1):100–118,",
      "citeRegEx" : "Freedman.,? \\Q1975\\E",
      "shortCiteRegEx" : "Freedman.",
      "year" : 1975
    }, {
      "title" : "Continuous time associative bandit problems",
      "author" : [ "András György", "Levente Kocsis", "Ivett Szabó", "Csaba Szepesvári" ],
      "venue" : "In IJCAI-07,",
      "citeRegEx" : "György et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "György et al\\.",
      "year" : 2007
    }, {
      "title" : "Self-optimizing memory controllers: A reinforcement learning approach",
      "author" : [ "Engin Ipek", "Onur Mutlu", "José F. Martı́nez", "Rich Caruana" ],
      "venue" : "SIGARCH Comput. Archit. News,",
      "citeRegEx" : "Ipek et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ipek et al\\.",
      "year" : 2008
    }, {
      "title" : "Organizing the last line of defense before hitting the memory wall for cmps",
      "author" : [ "Chun Liu", "Anand Sivasubramaniam", "Mahmut Kandemir" ],
      "venue" : "In Software, IEE Proceedings-,",
      "citeRegEx" : "Liu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2004
    }, {
      "title" : "Concentration. In Probabilistic methods for algorithmic discrete mathematics, pages 195–248",
      "author" : [ "Colin McDiarmid" ],
      "venue" : null,
      "citeRegEx" : "McDiarmid.,? \\Q1998\\E",
      "shortCiteRegEx" : "McDiarmid.",
      "year" : 1998
    }, {
      "title" : "Eluder dimension and the sample complexity of optimistic exploration",
      "author" : [ "Daniel Russo", "Benjamin Van Roy" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Russo and Roy.,? \\Q2013\\E",
      "shortCiteRegEx" : "Russo and Roy.",
      "year" : 2013
    }, {
      "title" : "Minimizing regret: The general case",
      "author" : [ "Aldo Rustichini" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "Rustichini.,? \\Q1999\\E",
      "shortCiteRegEx" : "Rustichini.",
      "year" : 1999
    }, {
      "title" : "A new memory monitoring scheme for memory-aware scheduling and partitioning",
      "author" : [ "G Edward Suh", "Srinivas Devadas", "Larry Rudolph" ],
      "venue" : "In High-Performance Computer Architecture,",
      "citeRegEx" : "Suh et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Suh et al\\.",
      "year" : 2002
    }, {
      "title" : "Knapsack based optimal policies for budget-limited multi-armed bandits",
      "author" : [ "Long Tran-Thanh", "Archie C. Chapman", "Alex Rogers", "Nicholas R. Jennings" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Tran.Thanh et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tran.Thanh et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "In fact, we anticipate numerous extensions and The name was invented later by (perhaps) [Rustichini, 1999].",
      "startOffset" : 88,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "Given its information structure, the problem belongs to the class of stochastic partial monitoring problems, which was first studied by Agrawal et al. [1989]1, where in each time step the learner receives noisy information about a hidden “parameter” while trying to maximise the sum of rewards and both the information received and the rewards depend in a known fashion on the actions and the hidden parameter.",
      "startOffset" : 136,
      "endOffset" : 158
    }, {
      "referenceID" : 0,
      "context" : "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989].",
      "startOffset" : 252,
      "endOffset" : 274
    }, {
      "referenceID" : 6,
      "context" : "adaptations for specific applications, such as in the case of bandits (see, Bubeck and Cesa-Bianchi [2012] for an overview of this rich literature).",
      "startOffset" : 76,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data.",
      "startOffset" : 253,
      "endOffset" : 589
    }, {
      "referenceID" : 0,
      "context" : "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache.",
      "startOffset" : 253,
      "endOffset" : 721
    }, {
      "referenceID" : 0,
      "context" : "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache. However, they all assume the model is fully known and no learning is required. Bitirgen et al. [2008] used ANNs to predict individual program performance as a function of resources.",
      "startOffset" : 253,
      "endOffset" : 948
    }, {
      "referenceID" : 0,
      "context" : "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache. However, they all assume the model is fully known and no learning is required. Bitirgen et al. [2008] used ANNs to predict individual program performance as a function of resources. Finally, Ipek et al. [2008] used reinforcement learning to allocate DRAM to multi-processors.",
      "startOffset" : 253,
      "endOffset" : 1056
    }, {
      "referenceID" : 0,
      "context" : "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache. However, they all assume the model is fully known and no learning is required. Bitirgen et al. [2008] used ANNs to predict individual program performance as a function of resources. Finally, Ipek et al. [2008] used reinforcement learning to allocate DRAM to multi-processors. 2 PRELIMINARIES In each time-step t the learner chooses Mk,t ≥ 0 subject to the constraint, ∑K k=1 Mk,t ≤ 1. Then all jobs are executed and Xk,t ∈ {0, 1} indicates the success or failure of job k in time-step t and is sampled from a Bernoulli distribution with parameter β(Mk,t/νk) := min {1,Mk,t/νk}. The goal is to maximise the expected number of jobs that successfully complete, ∑Kk=1 β(Mk,t/νk). We define the gaps ∆j,k = ν −1 j − ν−1 k . We assume throughout for conveBesides Badanidiyuru et al. [2013], all works consider finite action spaces and unstructured reward functions.",
      "startOffset" : 253,
      "endOffset" : 1630
    }, {
      "referenceID" : 2,
      "context" : "Then a bandit algorithm such as UCB1 [Auer et al., 2002] will achieve logarithmic (problem dependent) regret with some dependence on the gaps ∆1,k = 1 ν1 − 1 νk .",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "The second component involves analysing the selection of the overflow process, where the approach is reminiscent of the analysis for the UCB algorithm for stochastic bandits [Auer et al., 2002].",
      "startOffset" : 174,
      "endOffset" : 193
    }, {
      "referenceID" : 1,
      "context" : "For the second term we need the following lemma, which uses Theorem 5 and a reasoning analogues to that of Auer et al. [2002] to bound the regret of the UCB algorithm for stochastic bandits: Lemma 10.",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "8 MINIMAX LOWER BOUNDS Despite the continuous action space, the techniques used when proving minimax lower bounds for standard stochastic bandits [Auer et al., 1995] can be adapted to our setting.",
      "startOffset" : 146,
      "endOffset" : 165
    } ],
    "year" : 2014,
    "abstractText" : "We study a sequential resource allocation problem involving a fixed number of recurring jobs. At each time-step the manager should distribute available resources among the jobs in order to maximise the expected number of completed jobs. Allocating more resources to a given job increases the probability that it completes, but with a cut-off. Specifically, we assume a linear model where the probability increases linearly until it equals one, after which allocating additional resources is wasteful. We assume the difficulty of each job is unknown and present the first algorithm for this problem and prove upper and lower bounds on its regret. Despite its apparent simplicity, the problem has a rich structure: we show that an appropriate optimistic algorithm can improve its learning speed dramatically beyond the results one normally expects for similar problems as the problem becomes resource-laden.",
    "creator" : "Creator"
  }
}