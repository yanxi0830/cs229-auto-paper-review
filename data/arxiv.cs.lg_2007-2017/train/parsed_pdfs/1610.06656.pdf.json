{
  "name" : "1610.06656.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Shanshan Wu" ],
    "emails" : [ "shanshan@utexas.edu", "srinadh@ttic.edu", "sanghavi@mail.utexas.edu", "dimakis@austin.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n06 65\n6v 2\n[ st\nat .M\nL ]"
    }, {
      "heading" : "1 Introduction",
      "text" : "Given two large matrices A and B we study the problem of finding a low rank approximation of their product ATB, using only one pass over the matrix elements. This problem has many applications in machine learning and statistics. For example, if A = B, then this general problem reduces to Principal Component Analysis (PCA). Another example is a low rank approximation of a co-occurrence matrix from large logs, e.g., A may be a user-by-query matrix and B may be a user-by-ad matrix, so ATB computes the joint counts for each query-ad pair. The matrices A and B can also be two large bag-of-word matrices. For this case, each entry of ATB is the number of times a pair of words co-occurred together. As a fourth example, ATB can be a cross-covariance matrix between two sets of variables, e.g., A and B may be genotype and phenotype data collected on the same set of observations. A low rank approximation of the product matrix is useful for Canonical Correlation Analysis (CCA) [8]. For all these examples, ATB captures pairwise variable interactions and a low rank approximation is a way to efficiently represent the significant pairwise interactions in sub-quadratic space.\nLet A and B be matrices of size d × n (d ≫ n) assumed too large to fit in main memory. To obtain a rank-r approximation of ATB, a naive way is to compute ATB first, and then perform truncated singular value decomposition (SVD) of ATB. This algorithm needs O(n2d) time and O(n2) memory to compute the product, followed by an SVD of the n×n matrix. An alternative option is to directly run power method on ATB without explicitly computing the product. Such an algorithm will need to access the data matrices A and B multiple times and the disk IO overhead for loading the matrices to memory multiple times will be the major performance bottleneck.\nFor this reason, a number of recent papers introduce randomized algorithms that require only a few passes over the data, approximately linear memory, and also provide spectral norm guarantees. The key step in these algorithms is to compute a smaller representation of data. This can be achieved by two different methods: (1) dimensionality reduction, i.e., matrix sketching [29, 11, 26, 12]; (2) random sampling [14, 3]. The recent results of Cohen et al. [12] provide the strongest spectral norm guarantee of the former. They show that a sketch size of O(r̃/ǫ2) suffices for the sketched matrices ÃT B̃ to achieve a spectral error of ǫ, where r̃ is the maximum stable rank of A and B. Note that ÃT B̃ is not the desired rank-r approximation of ATB. On the other hand, [3] is a recent sampling method with very good performance guarantees. The authors consider entrywise sampling based on column norms, followed by a matrix completion step to compute low rank approximations. There is also a lot of interesting work on streaming PCA, but none can be directly applied to the general case when A is different from B (see Figure 4(c)). Please refer to Appendix D for more discussions on related work.\nDespite the significant volume of prior work, there is no method that computes a rank-r approximation of ATB when the entries of A and B are streaming in a single pass 1. Bhojanapalli et al. [3] consider a two-pass algorithm which computes column norms in the first pass and uses them to sample in a second pass over the matrix elements. In this paper, we combine ideas from the sketching and sampling literature to obtain the first algorithm that requires only a single pass over the data.\nContributions:\n• We propose a one-pass algorithm SMP-PCA (which stands for Streaming Matrix Product PCA) that computes a rank-r approximation of ATB in time O((nnz(A) + nnz(B))ρ\n2r3r̃ η2 + nr 6ρ4r̃3 η4 ). Here\nnnz(·) is the number of non-zero entries, ρ is the condition number, r̃ is the maximum stable rank, and η measures the spectral norm error. Existing two-pass algorithms such as [3] typically have longer runtime than our algorithm (see Figure 3(a)). We also compare our algorithm with the simple idea that first sketches A and B separately and then performs SVD on the product of their sketches. We show that our algorithm always achieves better accuracy and can perform arbitrarily better if the column vectors of A and B come from a cone (see Figures 2, 4(b), 3(b)).\n• The central idea of our algorithm is a novel rescaled JL embedding that combines information from matrix sketches and vector norms. This allows us to get better estimates of dot products of high dimensional vectors compared to previous sketching approaches. We explain the benefit compared to a naive JL embedding in Figure 2 and the related discussion; we believe it may be of more general interest beyond low rank matrix approximations.\n• We prove that our algorithm recovers a low rank approximation of ATB up to an error that depends on ‖ATB − (ATB)r‖ and ‖ATB‖, decaying with increasing sketch size and number of samples (Theorem 3.1). The first term is a consequence of low rank approximation and vanishes if ATB is exactly rank-r. The second term results from matrix sketching and subsampling; the bounds have similar dependencies as in [12].\n• We implement SMP-PCA in Apache Spark and perform several distributed experiments on synthetic and real datasets. Our distributed implementation uses several design innovations described in Section 4 and Appendix C.5 and it is the only Spark implementation that we are aware of that can handle matrices that are large in both dimensions. Our experiments show that we improve by approximately a factor of 2× in running time compared to the previous state of the art and scale gracefully as the cluster size increases. The source code is available online [36].\n1One straightforward idea is to sketch each matrix individually and perform SVD on the product of the sketches. We compare against that scheme and show that we can perform arbitrarily better using our rescaled JL embedding.\n• In addition to better performance, our algorithm offers another advantage: It is possible to compute low-rank approximations to ATB even when the entries of the two matrices arrive in some arbitrary order (as would be the case in streaming logs). We can therefore discover significant correlations even when the original datasets cannot be stored, for example due to storage or privacy limitations."
    }, {
      "heading" : "2 Problem setting and algorithms",
      "text" : "Consider the following problem: given two matrices A ∈ Rd×n1 and B ∈ Rd×n2 that are stored in disk, find a rank-r approximation of their product ATB. In particular, we are interested in the setting where both A, B and ATB are too large to fit into memory. This is common for modern large scale machine learning applications. For this setting, we develop a single-pass algorithm SMP-PCA that computes the rank-r approximation without explicitly forming the entire matrix ATB.\nNotations. Throughout the paper, we use A(i, j) or Aij to denote (i, j) entry for any matrix A. Let Ai and Aj be the i-th column vector and j-th row vector. We use ‖A‖F for Frobenius norm, and ‖A‖ for spectral (or operator) norm. The optimal rank-r approximation of matrix A is Ar, which can be found by SVD. Given a set Ω ⊂ [n1]× [n2] and a matrix A ∈ Rn1×n2 , we define PΩ(A) ∈ Rn1×n2 as the projection of A on Ω, i.e., PΩ(A)(i, j) = A(i, j) if (i, j) ∈ Ω and 0 otherwise."
    }, {
      "heading" : "2.1 SMP-PCA",
      "text" : "Our algorithm SMP-PCA (Streaming Matrix Product PCA) takes four parameters as input: the desired rank r, number of samples m, sketch size k, and the number of iterations T . Performance guarantee involving these parameters is provided in Theorem 3.1. As illustrated in Figure 1, our algorithm has three main steps: 1) compute sketches and side information in one pass over A and B; 2) given partial information of A and B, estimate important entries of ATB; 3) compute low rank approximation given estimates of a few entries of ATB. Now we explain each step in detail.\ncompute its entry as M̃ (i, j) = ‖Ai‖ · ‖Bj‖ · Ã T i B̃j\n‖Ãi‖·‖B̃j‖ . Performing matrix completion on PΩ(M̃ ) gives the\ndesired rank-r approximation.\nStep 1: Compute sketches and side information in one pass over A and B. In this step we compute sketches Ã := ΠA and B̃ := ΠB, where Π ∈ Rk×d is a random matrix with entries being i.i.d. N (0, 1/k)\nAlgorithm 1 SMP-PCA: Streaming Matrix Product PCA\n1: Input: A ∈ Rd×n1 , B ∈ Rd×n2 , desired rank: r, sketch size: k, number of samples: m, number of iterations: T 2: Construct a random matrix Π ∈ Rk×d, where Π(i, j) ∼ N (0, 1/k), ∀(i, j) ∈ [k]× [d]. Perform a single pass over A and B to obtain: Ã = ΠA, B̃ = ΠB, and ‖Ai‖, ‖Bj‖, ∀(i, j) ∈ [n1]× [n2]. 3: Sample each entry (i, j) ∈ [n1] × [n2] independently with probability q̂ij = min{1, qij}, where qij is defined in Eq.(1); maintain a set Ω ⊂ [n1]× [n2] which stores all the sampled pairs (i, j). 4: Define M̃ ∈ Rn1×n2 , where M̃(i, j) is given in Eq. (2). Calculate PΩ(M̃ ) ∈ Rn1×n2 , where PΩ(M̃ ) = M̃(i, j) if (i, j) ∈ Ω and zero otherwise.\n5: Run WAltMin(PΩ(M̃ ), Ω, r, q̂, T ), see Appendix A for more details. 6: Output: Û ∈ Rn1×r and V̂ ∈ Rn2×r.\nrandom variables. It is known that Π satisfies an ”oblivious Johnson-Lindenstrauss (JL) guarantee” [29][34] and it helps preserving the top row spaces of A and B [11]. Note that any sketching matrix Π that is an oblivious subspace embedding can be considered here, e.g., sparse JL transform and randomized Hadamard transform (see [12] for more discussion).\nBesides Ã and B̃, we also compute the L2 norms for all column vectors, i.e., ‖Ai‖ and ‖Bj‖, for all (i, j) ∈ [n1] × [n2]. We use this additional information to design better estimates of ATB in the next step, and also to determine important entries of ÃT B̃ to sample. Note that this is the only step that needs one pass over data.\nStep 2: Estimate important entries of ATB by rescaled JL embedding. In this step we use partial information obtained from the previous step to compute a few important entries of ÃT B̃. We first determine what entries of ÃT B̃ to sample, and then propose a novel rescaled JL embedding for estimating those entries.\nWe sample entry (i, j) of ATB independently with probability q̂ij = min{1, qij}, where\nqij = m · ( ‖Ai‖2\n2n2‖A‖2F + ‖Bj‖2 2n1‖B‖2F ). (1)\nLet Ω ⊂ [n1] × [n2] be the set of sampled entries (i, j). Since E( ∑\ni,j qij) = m, the expected number of sampled entries is roughly m. The special form of qij ensures that we can draw m samples in O(n1 + m log(n2)) time; we show how to do this in Appendix C.5.\nNote that qij intuitively captures important entries of ATB by giving higher weight to heavy rows and columns. We show in Section 3 that this sampling actually generates good approximation to the matrix ATB.\nThe biased sampling distribution of Eq. (1) is first proposed by Bhojanapalli et al. [3]. However, their algorithm [3] needs a second pass to compute the sampled entries, while we propose a novel way of estimating dot products, using information obtained in the first step.\nDefine M̃ ∈ Rn1×n2 as M̃(i, j) = ‖Ai‖ · ‖Bj‖ ·\nÃTi B̃j\n‖Ãi‖ · ‖B̃j‖ . (2)\nNote that we will not compute and store M̃ , instead, we only calculate M̃(i, j) for (i, j) ∈ Ω. This matrix is denoted as PΩ(M̃), where PΩ(M̃ )(i, j) = M̃(i, j) if (i, j) ∈ Ω and 0 otherwise.\nWe now explain the intuition of Eq. (2), and why M̃ is a better estimator than ÃT B̃. To estimate the (i, j) entry of ATB, a straightforward way is to use ÃTi B̃j = ‖Ãi‖ · ‖B̃j‖ · cos θ̃ij , where θ̃ij is the angle between vectors Ãi and B̃j . Since we already know the actual column norms, a potentially better estimator\nwould be ‖Ai‖ · ‖Bj‖ · cos θ̃ij . This removes the uncertainty that comes from distorted column norms2. Figure 2(a) compares the two estimators ÃTi B̃j (JL embedding) and M̃(i, j) (rescaled JL embedding) for dot products. We plot simulation results on pairs of unit-norm vectors with different angles. The vectors have dimension 1,000 and the sketching matrix has dimension 10-by-1,000. Clearly rescaling by the actual norms help reduce the estimation uncertainty. This phenomenon is more prominent when the true dot products are close to ±1, which makes sense because cos θ has a small slope when cos θ approaches ±1, and hence the uncertainty from angles may produce smaller distortion compared to that from norms. In the extreme case when cos θ = ±1, rescaled JL embedding can perfectly recover the true dot product.\nIn the lower part of Figure 2(b) we illustrate how to construct unit-norm vectors from a cone with angle θ. Given a fixed unit-norm vector x, and a random Gaussian vector t with expected norm tan(θ/2), we construct new vector y by randomly picking one from the two possible choices x+ t and −(x+ t), and then renormalize it. Suppose the columns of A and B are unit vectors randomly drawn from a cone with angle θ, we plot the ratio of spectral norm errors ‖ATB − ÃT B̃‖/‖ATB − M̃‖ in Figure 2(b). We observe that M̃ always outperforms ÃT B̃ and can be much better when θ approaches zero, which agrees with the trend indicated in Figure 2(a).\nStep 3: Compute low rank approximation given estimates of few entries of ATB. Finally we compute the low rank approximation of ATB from the samples using alternating least squares:\nmin U,V ∈Rn×r\n∑\n(i,j)∈Ω\nwij(e T i UV T ej − M̃(i, j))2, (3)\n2We also tried using the cosine rule for computing the dot product, and another sketching method specifically designed for preserving angles [4], but empirically those methods perform worse than our current estimator.\nwhere wij = 1/q̂ij denotes the weights, and ei, ej are standard base vectors. This is a popular technique for low rank recovery and matrix completion (see [3] and the references therein). After T iterations, we will get a rank-r approximation of M̃ presented in the convenient factored form. This subroutine is quite standard, so we defer the details to Appendix A."
    }, {
      "heading" : "3 Analysis",
      "text" : "Now we present the main theoretical result. Theorem 3.1 characterizes the interaction between the sketch size k, the sampling complexity m, the number of iterations T , and the spectral error ‖(ATB)r − ÂTBr‖, where ÂTBr is the output of SMP-PCA, and (ATB)r is the optimal rank-r approximation of ATB. Note that the following theorem assumes that A and B have the same size. For the general case of n1 6= n2, Theorem 3.1 is still valid by setting n = max{n1, n2}.\nTheorem 3.1. Given matrices A ∈ Rd×n and B ∈ Rd×n, let (ATB)r be the optimal rank-r approximation of ATB. Define r̃ = max{‖A‖ 2 F\n‖A‖2 , ‖B‖2 F ‖B‖2 } as the maximum stable rank, and ρ = σ∗1 σ∗r as the condition number\nof (ATB)r, where σ∗i is the i-th singular values of A TB.\nLet ÂTBr be the output of Algorithm SMP-PCA. If the input parameters k, m, and T satisfy\nk ≥ C1‖A‖ 2‖B‖2ρ2r3 ‖ATB‖2F · max{r̃, 2 log(n)}+ log (3/γ) η2 , (4)\nm ≥ C2r̃ 2 γ · (‖A‖2F + ‖B‖2F ‖ATB‖F )2 · nr 3ρ2 log(n)T 2 η2 , (5)\nT ≥ log(‖A‖F + ‖B‖F ζ ), (6)\nwhere C1 and C2 are some global constants independent of A and B. Then with probability at least 1− γ, we have\n‖(ATB)r − ÂTBr‖ ≤ η‖ATB − (ATB)r‖F + ζ + ησ∗r . (7)\nRemark 1. Compared to the two-pass algorithm proposed by [3], we notice that Eq. (7) contains an additional error term ησ∗r . This extra term captures the cost incurred when we are approximating entries of ATB by Eq. (2) instead of using the actual values. The exact tradeoff between η and k is given by Eq. (4). On one hand, we want to have a small k so that the sketched matrices can fit into memory. On the other hand, the parameter k controls how much information is lost during sketching, and a larger k gives a more accurate estimation of the inner products.\nRemark 2. The dependence on ‖A‖ 2 F +‖B‖2 F\n‖ATB‖F captures one difficult situation for our algorithm. If\n‖ATB‖F is much smaller than ‖A‖F or ‖B‖F , which could happen, e.g., when many column vectors of A are orthogonal to those of B, then SMP-PCA requires many samples to work. This is reasonable. Imagine that ATB is close to an identity matrix, then it may be hard to tell it from an all-zero matrix without enough samples. Nevertheless, removing this dependence is an interesting direction for future research.\nRemark 3. For the special case of A = B, SMP-PCA computes a rank-r approximation of matrix ATA in a single pass. Theorem 3.1 provides an error bound in spectral norm for the residual matrix (ATA)r − ÂTAr. Most results in the online PCA literature use Frobenius norm as performance measure. Recently, [22] provides an online PCA algorithm with spectral norm guarantee. They achieves a spectral norm bound of ǫσ∗1+σ ∗ r+1, which is stronger than ours. However, their algorithm requires a target dimension\nof O(r log n/ǫ2), i.e., the output is a matrix of size n-by-O(r log n/ǫ2), while the output of SMP-PCA is simply n-by-r.\nRemark 4. We defer our proofs to Appendix C. The proof proceeds in three steps. In Appendix C.2, we show that the sampled matrix provides a good approximation of the actual matrix ATB. In Appendix C.3, we show that there is a geometric decrease in the distance between the computed subspaces Û , V̂ and the optimal ones U∗, V ∗ at each iteration of WAltMin algorithm. The spectral norm bound in Theorem 3.1 is then proved using results from the previous two steps.\nComputation Complexity. We now analyze the computation complexity of SMP-PCA. In Step 1, we compute the sketched matrices of A and B, which requires O(nnz(A)k + nnz(B)k) flops. Here nnz(·) denotes the number of non-zero entries. The main job of Step 2 is to sample a set Ω and calculate the corresponding inner products, which takes O(m log(n) +mk) flops. Here we define n as max{n1, n2} for simplicity. According to Eq. (4), we have log(n) = O(k), so Step 2 takes O(mk) flops. In Step 3, we run alternating least squares on the sampled matrix, which can be completed in O((mr2 + nr3)T ) flops. Since Eq. (5) indicates nr = O(m), the computation complexity of Step 3 is O(mr2T ). Therefore, SMP-PCA has a total computation complexity O(nnz(A)k + nnz(B)k +mk +mr2T )."
    }, {
      "heading" : "4 Numerical Experiments",
      "text" : "Spark implementation. We implement our SMP-PCA in Apache Spark 1.6.2 [37]. For the purpose of comparison, we also implement a two-pass algorithm LELA [3] in Spark3. The matrices A and B are stored as a resilient distributed dataset (RDD) in disk (by setting its StorageLevel as DISK_ONLY). We implement the two passes of LELA using the treeAggregate method. For SMP-PCA, we choose the subsampled randomized Hadamard transform (SRHT) [32] as the sketching matrix 4. The biased sampling procedure is performed using binary search (see Appendix C.5 for how to sample m elements in O(m log n) time). After obtaining the sampled matrix, we run ALS (alternating least squares) to get the desired low-rank matrices. More details can be found in [36].\nDescription of datasets. We test our algorithm on synthetic datasets and three real datasets: SIFT10K [20], NIPS-BW [23], and URL-reputation [24]. For synthetic data, we generate matrices A and B as GD, where G has entries independently drawn from standard Gaussian distribution, and D is a diagonal matrix with Dii = 1/i. SIFT10K is a dataset of 10,000 images. Each image is represented by 128 features. We set A as the image-by-feature matrix. The task here is to compute a low rank approximation of ATA, which is a standard PCA task. The NIPS-BW dataset contains bag-of-words features extracted from 1,500 NIPS papers. We divide the papers into two subsets, and let A and B be the corresponding word-by-paper matrices, so ATB computes the counts of co-occurred words between two sets of papers. The original URL-reputation dataset has 2.4 million URLs. Each URL is represented by 3.2 million features, and is indicated as malicious or benign. This dataset has been used previously for CCA [25]. Here we extract two subsets of features, and let A and B be the corresponding URL-by-feature matrices. The goal is to compute a low rank approximation of ATB, the cross-covariance matrix between two subsets of features.\nSample complexity. In Figure 4(a) we present simulation results on a small synthetic data with n = d = 5, 000 and r = 5. We observe that a phase transition occurs when the sample complexity m = Θ(nr log n). This agrees with the experimental results shown in previous papers, see, e.g., [9, 3]. For the rest experiments presented in this section, unless otherwise specified, we set r = 5, T = 10, and sampling complexity m as 4nr log n.\n3To our best knowledge, this the first distributed implementation of LELA. 4Compared to Gaussian sketch, SRHT reduces the runtime from O(ndk) to O(nd log d) and space cost from O(dk) to O(d),\nwhile maintains the same quality of the output.\nComparison of SMP-PCA and LELA. We now compare the statistical performance of SMP-PCA and LELA [3] on three real datasets and one synthetic dataset. As shown in Figure 3(b) and Table 1, LELA always achieves a smaller spectral norm error than SMP-PCA, which makes sense because LELA takes two passes and hence has more chances exploring the data. Besides, we observe that as the sketch size increases, the error of SMP-PCA keeps decreasing and gets closer to that of LELA.\nIn Figure 3(a) we compare the runtime of SMP-PCA and LELA using a 150GB synthetic dataset on m3.2xlarge Amazon EC2 instances5 . The matrices A and B have dimension n = d = 100, 000. The sketch dimension is set as k = 2, 000. We observe that the speedup achieved by SMP-PCA is more prominent for small clusters (e.g., 56 mins versus 34 mins on a cluster of size two). This is possibly due to the increasing spark overheads at larger clusters, see [17] for more related discussion.\nComparison of SMP-PCA and SVD(ÃT B̃). In Figure 4(b) we repeat the experiment in Section 2 by generating column vectors of A and B from a cone with angle θ. Here SVD(ÃT B̃) refers to computing\n5Each machine has 8 cores, 30GB memory, and 2×80GB SSD.\nSVD on the sketched matrices6. We plot the ratio of the spectral norm error of SVD(ÃT B̃) over that of SMP-PCA, as a function of θ. Note that this is different from Figure 2(b), as now we take the effect of random sampling and SVD into account. However, the trend in both figures are the same: SMP-PCA always outperforms SVD(ÃT B̃) and can be arbitrarily better as θ goes to zero.\nIn Figure 3(b) we compare SMP-PCA and SVD(ÃT B̃) on two real datasets SIFK10K and NIPS-BW. The y-axis represents spectral norm error, defined as ||ATB − ÂTBr||/||ATB||, where ÂTBr is the rank-r approximation found by a specific algorithm. We observe that SMP-PCA outperforms SVD(ÃT B̃) by a factor of 1.8 for SIFT10K and 1.1 for NIPS-BW.\nNow we explain why SMP-PCA produces a more accurate result than SVD(ÃT B̃). The reasons are twofold. First, our rescaled JL embedding M̃ is a better estimator for ATB than ÃT B̃ (Figure 2). Second, the noise due to sampling is relatively small compared to the benefit obtained from M̃ , and hence the final result computed using PΩ(M̃ ) still outperforms SVD(ÃT B̃).\nComparison of SMP-PCA and ATr Br. Let Ar and Br be the optimal rank-r approximation of A and B, we show that even if one could use existing methods (e.g., algorithms for streaming PCA) to estimate Ar and Br, their product ATr Br can be a very poor low rank approximation of A\nTB. This is demonstrated in Figure 4(c), where we intentionally make the top r left singular vectors of A orthogonal to those of B."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We develop a novel one-pass algorithm SMP-PCA that directly computes a low rank approximation of a matrix product, using ideas of matrix sketching and entrywise sampling. As a subroutine of our algorithm, we propose rescaled JL for estimating entries of ATB, which has smaller error compared to the standard estimator ÃT B̃. This we believe can be extended to other applications. Moreover, SMP-PCA allows the non-zero entries of A and B to be presented in any arbitrary order, and hence can be used for steaming applications. We design a distributed implementation for SMP-PCA. Our experimental results show that\n6This can be done by standard power iteration based method, without explicitly forming the product matrix ÃT B̃, whose size is too big to fit into memory according to our assumption.\nSMP-PCA can perform arbitrarily better than SVD(ÃT B̃), and is significantly faster compared to algorithms that require two or more passes over the data.\nAcknowledgements We thank the anonymous reviewers for their valuable comments. This research has been supported by NSF Grants CCF 1344179, 1344364, 1407278, 1422549, 1302435, 1564000, and ARO YIP W911NF-14-1-0258."
    }, {
      "heading" : "A Weighted alternating minimization",
      "text" : "Algorithm 2 provides a detailed explanation of WAltMin, which follows a standard procedure for matrix completion. We use RΩ(A) = w. ∗ PΩ(A) to denote the Hadamard product between w and PΩ(A): RΩ(A)(i, j) = w(i, j) ∗ PΩ(A)(i, j) for (i, j) ∈ Ω and 0 otherwise, where w ∈ Rn1×n2 = 1/q̂ij is the weight matrix. Similarly we define the matrix R1/2Ω (A) as R 1/2 Ω (A)(i, j) = √ w(i, j) ∗ PΩ(A)(i, j) for (i, j) ∈ Ω and 0 otherwise. The algorithm contains two parts: initialization (Step 2-6) and weighted alternating minimization (Step 7-10). In the first part, we compute SVD of the weighted sampled matrix RΩ(M̃) and then set row i of U (0) to be zero if its norm is larger than a threshold (Step 6). More details of this trim step can be found in [3]. In the second part, the goal is to solve the following non-convex problem by alternating minimization:\nmin U,V\n∑\n(i,j)∈Ω\nwij(e T i UV T ej − M̃(i, j))2, (8)\nwhere ei, ej are standard base vectors. After running T iterations, the algorithm outputs a rank-r approximation of M̃ presented in the convenient factored form.\nAlgorithm 2 WAltMin [3]\n1: Input: PΩ(M̃) ∈ Rn1×n2 , Ω, r, q̂, and T 2: wij = 1/q̂ij when q̂ij > 0, 0 else, ∀i, j 3: Divide Ω in 2T + 1 equal uniformly random subsets, i.e., Ω = {Ω0, . . . ,Ω2T } 4: RΩ0(M̃ ) = w. ∗ PΩ0(M̃) 5: U (0)Σ(0)(V (0))T = SVD(RΩ0(M̃ ), r) 6: Trim U (0) and let Û (0) be the output 7: for t = 0 to T − 1 do 8: V̂ (t+1) = argminV ‖R1/2Ω2t+1(M̃ − Û\n(t)V T )‖2F 9: Û (t+1) = argminU ‖R1/2Ω2t+2(M̃ − U(V̂\n(t+1))T )‖2F 10: end for 11: Output: Û (T ) ∈ Rn1×r and V̂ (T ) ∈ Rn2×r."
    }, {
      "heading" : "B Technical Lemmas",
      "text" : "We will frequently use the following concentration inequality in the proof.\nLemma B.1. (Matrix Bernstein’s Inequality [33]). Consider p independent random matrices X1, ....,Xp in Rn×n, where each matrix has bounded deviation from its mean:\n||Xi − E[Xi]|| ≤ L, ∀i."
    }, {
      "heading" : "Let the norm of the covariance matrix be",
      "text" : "σ2 = max {∣∣∣∣∣ ∣∣∣∣∣E [ p∑\ni=1\n(Xi − E[Xi])(Xi − E[Xi])T ]∣∣∣∣∣ ∣∣∣∣∣ , ∣∣∣∣∣ ∣∣∣∣∣E [ p∑\ni=1\n(Xi − E[Xi])T (Xi − E[Xi]) ]∣∣∣∣∣ ∣∣∣∣∣ }"
    }, {
      "heading" : "Then the following holds for all t ≥ 0:",
      "text" : "P [∣∣∣∣∣ ∣∣∣∣∣ p∑\ni=1\n(Xi − E[Xi]) ∣∣∣∣∣ ∣∣∣∣∣ ] ≤ 2n exp( −t 2/2 σ2 + Lt/3 ).\nA formal definition of JL transform is given below [29][34].\nDefinition B.2. A random matrix Π ∈ Rk×d forms a JL transform with parameters ǫ, δ, f or JLT(ǫ, δ, f ) for short, if with probability at least 1− δ, for any f -element subset V ⊂ Rd, for all v, v′ ∈ V it holds that |〈Πv,Πv′〉 − 〈v, v′〉| ≤ ǫ||v|| · ||v′||.\nThe following lemma [34] characterizes the tradeoff between the reduced dimension k and the error level ǫ.\nLemma B.3. Let 0 < ǫ, δ < 1, and Π ∈ Rk×d be a random matrix where the entries Π(i, j) are i.i.d. N (0, 1/k) random variables. If k = Ω(log(f/δ)ǫ−2), then Π is a JLT(ǫ, δ, f ).\nWe now present two lemmas that connect Ã ∈ Rk×n and B̃ ∈ Rd×n with A ∈ Rd×n and B ∈ Rd×n.\nLemma B.4. Let 0 < ǫ, δ < 1, if k = Ω( log(2n/δ) ǫ2 ), then with probability at least 1− δ,\n(1− ǫ)||A||2F ≤ ||Ã||2F ≤ (1 + ǫ)||A||2F , (1− ǫ)||B||2F ≤ ||B̃||2F ≤ (1 + ǫ)||B||2F ,\n||ÃT B̃ −ATB||F ≤ ǫ||A||F ||B||F . Proof. This is again a standard result of JL transformation, e.g., see Definition 2.3 and Theorem 2.1 of [34] and Lemma 6 of [29] .\nLemma B.5. Let 0 < ǫ, δ < 1, if k = Θ( r̃+log(1/δ)ǫ2 ), where r̃ = max{ ||A||2 F ||A||2 , ||B||2 F ||B||2 } is the maximum stable rank, then with probability at least 1− δ,\n||ÃT B̃ −ATB|| ≤ ǫ||A||||B||.\nProof. This follows from a recent paper [12].\nUsing the above two lemmas, we can prove the following two lemmas that relate M̃ with ATB, for M̃ defined in Algorithm 1. A more compact definition of M̃ is DAÃT B̃DB , where DA and DB are diagonal matrices with (DA)ii = ||Ai||/||Ãi|| and (DB)jj = ||Bj ||/||B̃j ||.\nLemma B.6. Let 0 < ǫ < 1/14, 0 < δ < 1, if k = Ω( log(2n/δ)ǫ2 ), then with probability at least 1− δ,\n|M̃ij −ATi Bj | ≤ ǫ||Ai|| · ||Bj ||, ||M̃ −ATB||F ≤ ǫ||A||F ||B||F .\nProof. Let 0 < ǫ < 1/2, 0 < δ < 1, according to the Definition B.2 and Lemma B.3, we have that if k = Ω( log(2n/δ)\nǫ2 ), then with probability at least 1− δ, and for all i, j\n1− ǫ ≤ (DA)ii ≤ 1 + ǫ, 1− ǫ ≤ (DB)jj ≤ 1 + ǫ, |ÃTi B̃j −ATi Bj | ≤ ǫ||Ai||||Bj ||. (9)\nWe can now bound |M̃ij −ATi Bj| as\n|M̃ij −ATi Bj | ξ1 = |ÃTi B̃j(DA)ii(DB)jj −ATi Bj| ξ2 ≤ max{|ÃTi B̃j(1 + ǫ)2 −ATi Bj |, |ÃTi B̃j(1− ǫ)2 −ATi Bj |} ξ3 ≤ max{(1 + ǫ)2ǫ||Ai||||Bj ||+ ((1 + ǫ)2 − 1)|ATi Bj |, (1− ǫ)2ǫ||Ai||||Bj ||+ (1− (1− ǫ)2)|ATi Bj |} ξ4 ≤ 7ǫ||Ai||||Bj ||, (10)\nwhere ξ1 follows from the definition of M̃ij , ξ2 follows from the bound in Eq.(9), ξ3 follows from triangle inequality and Eq.(9), and ξ4 follows from |ATi Bj | ≤ ||Ai||||Bj ||. Now rescaling ǫ as ǫ/7 gives the desired bound in Lemma B.6.\nHence, ||M̃ −ATB||F = √∑ ij |M̃ij −ATi Bj |2 ≤ √∑ ij ǫ 2||Ai||2||Bj ||2 = ǫ||A||F ||B||F .\nLemma B.7. Let 0 < ǫ < 1/14, 0 < δ < 1, if k = Ω( r̃+log(n/δ)ǫ2 ), then with probability at least 1− δ,\n||M̃ −ATB|| ≤ ǫ||A||||B||.\nProof. We can bound the spectral norm of the difference matrix as follows:\n||M̃ −ATB|| ξ1= ||DAÃT B̃DB −DAATBDB +DAATBDB −DAATB +DAATB −ATB|| ≤ ||DA||||ÃT B̃ −ATB||||DB ||+ ||DA||||ATB||||DB − I||+ ||DA − I||||ATB|| ξ3 ≤ (1 + ǫ)2ǫ||A||||B|| + (1 + ǫ)ǫ||A||||B|| + ǫ||A||||B|| ≤ 7ǫ||A||||B||, (11)\nwhere ξ1 follows from the definition of M̃ij , and ξ2 follows from Lemma B.5 and bound in Eq.(9). Rescaling ǫ as ǫ/7 gives the desired bound in Lemma B.7.\nWe will frequently use the term with high probability. Here is a formal definition.\nDefinition B.8. We say that an event E occurs with high probability (w.h.p.) in n if the probability that its complement Ē happens is polynomially small, i.e., Pr(Ē) = O( 1nα ) for some constant α > 0.\nThe following two lemmas define a ”nice” Π and when this happens with high probability.\nDefinition B.9. The random Gaussian matrix Π is ”nice” with parameter ǫ if for all (i, j) such that qij ≤ 1 (i.e., qij = q̂ij), the sketched values M̃ij satisfies the following two inequalities:\n|M̃ij | q̂ij ≤ (1 + ǫ) n m (||A||2F + ||B||2F ),\n∑\n{j:q̂ij=qij}\nM̃2ij q̂ij ≤ (1 + ǫ)2n m (||A||2F + ||B||2F )2.\nLemma B.10. If k = Ω( log(n) ǫ2 ), and 0 < ǫ < 1/14, then the random Gaussian matrix Π ∈ Rk×d is ”nice” w.h.p. in n.\nProof. According to Lemma B.6, if k = Ω( log(n)ǫ2 ), then w.h.p. in n, for all (i, j) we have |M̃ij −ATi Bj | ≤ ǫ||Ai|| · ||Bj ||. In other words, the following holds with probability at least 1− δ:\n|M̃ij | ≤ |ATi Bj |+ ǫ||Ai|| · ||Bj || ≤ (1 + ǫ)||Ai|| · ||Bj ||, ∀(i, j)\nThe above inequality is sufficient for Π to be ”nice”:\nM̃ij q̂ij ≤ (1 + ǫ) ||Ai|| · ||Bj || q̂ij ≤ (1 + ǫ) (||Ai|| 2 + ||Bj ||2)/2\nm · ( ||Ai||2 2n||A||2\nF\n+ ||Bj ||2\n2n||B||2 F\n) ≤ (1 + ǫ) n m (||A||2F + ||B||2F )\n∑ {j:q̂ij=qij} M̃2ij q̂ij ≤ ∑ {j:q̂ij=qij} (1 + ǫ)2||Ai||2||Bj ||2 q̂ij\n≤ (1 + ǫ) ∑\n{j:q̂ij=qij}\n||Ai||4 + ||Bj ||4\nm · ( ||Ai||2 2n||A||2\nF\n+ ||Bj ||2\n2n||B||2 F\n)\n≤ (1 + ǫ)2n m (||A||2F + ||B||2F )2.\nTherefore, we conclude that if k = Ω( log(n)ǫ2 ), then Π is ”nice” w.h.p. in n."
    }, {
      "heading" : "C Proofs",
      "text" : ""
    }, {
      "heading" : "C.1 Proof overview",
      "text" : "We now present the key steps in proving Theorem 3.1. The framework is similar to that of LELA [3]. Our proof proceeds in three steps. In the first step, we show that the sampled matrix provides a good approximation of the actual matrix ATB. The result is summarized in Lemma C.1. Here RΩ(M̃ ) denotes the sampled matrix weighted by the inverse of sampling probability (see Line 4 of Algorithm 2). Detailed proof can be found in Appendix C.2. For consistency, we will use Ci (i = 1, 2, ...) to denote global constant that can vary from step to step.\nLemma C.1. (Initialization) Let m and k satisfy the following conditions for sufficiently large constants C1 and C2:\nm ≥ C1 ( ||A||2F + ||B||2F\n||ATB||F\n)2 n\nδ2 log(n),\nk ≥ C2 r̃ + log(n) δ2 · ||A|| 2||B||2 ||ATB||2F ,\nthen the following holds w.h.p. in n:\n||RΩ(M̃ )−ATB|| ≤ δ||ATB||F .\nIn the second step, we show that at each iteration of WAltMin algorithm, there is a geometric decrease in the distance between the computed subspaces Û , V̂ and the optimal ones U∗, V ∗. The result is shown in Lemma C.2. Appendix C.3 provides the detailed proof. Here for any two orthonormal matrices X and Y , we define their distance as the principal angle based distance, i.e., dist(X,Y ) = ||XT⊥Y ||, where X⊥ denotes the subspace orthogonal to X.\nLemma C.2. (WAltMin Descent) Let k, m, and T satisfy the conditions stated in Theorem 3.1. Also, consider the case when ||ATB − (ATB)r||F ≤ 1576ρr1.5 ||(ATB)r||F . Let Û (t) and V̂ (t+1) be the t-th and (t+1)-th step iterates of the WAltMin procedure. Let U (t) and V (t+1) be the corresponding orthonormal matrices. Let ||(U (t))i|| ≤ 8√rρ||Ai||/||A||F and dist(U (t), U∗) ≤ 1/2. Denote ATB as M , then the following holds with probability at least 1− γ/T :\ndist(V t+1, V ∗) ≤ 1 2 dist(U t, U∗) + η||M −Mr||F /σ∗r + η,\n||(V (t+1))j || ≤ 8√rρ||Bj ||/||B||F .\nIn the third step, we prove the spectral norm bound in Theorem 3.1 using results from the above two lemmas. Comparing Lemma C.1 and C.2 with their counterparts of LELA (see Lemma C.2 and C.3 in [3]), we notice that Lemma C.1 has the same bound as that of LELA, but the bound in Lemma C.2 contains an extra term η. This term eventually leads to an additive error term ησ∗r in Eq.(7). Detailed proof is in Appendix C.4."
    }, {
      "heading" : "C.2 Proof of Lemma C.1",
      "text" : "We first prove the following lemma, which shows that RΩ(M̃) is close to M̃ . For simplicity of presentation, we define CAB := (||A||2 F +||B||2 F )2\n||ATB||2 F\n.\nLemma C.3. Suppose Π is fixed and is ”nice”. Let m ≥ C1 · CAB nδ2 log(n) for sufficiently large global constant C1, then w.h.p. in n, the following is true:\n||RΩ(M̃)− M̃ || ≤ δ||ATB||F .\nProof. This lemma can be proved in the same way as the proof of Lemma C.2 in [3]. The key idea is to use the matrix Bernstein inequality. Let Xij = (δij − q̂ij)wijM̃ijeieTj , where δij is a {0, 1} random variable indicating whether the value at (i, j) has been sampled. Since Π is fixed, {Xij}ni,j=1 are independent zero mean random matrices. Furthermore, ∑ i,j{Xij}ni,j=1 = RΩ(M̃ )− M̃ .\nSince Π is ”nice” with parameter 0 < ǫ < 1/14, we can bound the 1st and 2nd moment of Xij as follows:\n||Xij || = max{|(1− q̂ij)wijM̃ij |, |q̂ijwijM̃ij|} ≤ |M̃ij | q̂ij ξ1 ≤ (1 + ǫ) n m (||A||2F + ||B||2F );\nσ2 = max{ ∣∣∣∣∣∣ ∣∣∣∣∣∣ E  ∑\nij\nXijX T ij   ∣∣∣∣∣∣ ∣∣∣∣∣∣ , ∣∣∣∣∣∣ ∣∣∣∣∣∣ E  ∑\nij\nXTijXij   ∣∣∣∣∣∣ ∣∣∣∣∣∣ } ξ2= max i ∣∣∣∣∣∣ ∑\nj\nq̂ij(1− q̂ij)w2ijM̃2ij ∣∣∣∣∣∣\n= max i |( 1 q̂ij − 1)M̃2ij | ξ3 ≤\n∑\n{j:q̂ij=qij}\nM̃2ij q̂ij ξ1 ≤ (1 + ǫ)2n m (||A||2F + ||B||2F )2,\nwhere ξ1 follows from Lemma B.10, ξ2 follows from a direct calculation, and ξ3 follows from the fact that q̂ij ≤ 1. Now we can use matrix Bernstein inequality (see Lemma B.1) with t = δ||ATB||F to show that if m ≥ (1 + ǫ)C1CAB nδ2 log(n), then the desired inequality holds w.h.p. in n, where C1 is some global constant independent of A and B. Note that since 0 < ǫ < 1/14, (1 + ǫ) < 2. Rescaling C1 gives the desired result.\nNow we are ready to prove Lemma C.1, which is a counterpart of Lemma C.2 in [3].\nProof. We first show that ||RΩ(M̃ )− M̃ || ≤ δ||ATB||F holds w.h.p. in n over the randomness of Π. Note that in Lemma C.3, we have shown that it is true for a fixed and ”nice” Π, now we want to show that it also holds w.h.p. in n even for a random chosen Π.\nLet G be the event that we desire, i.e., G = {||RΩ(ÃT B̃) − ÃT B̃|| ≤ δ||ATB||F }. Let Ḡ be the complimentary event. By conditioning on Π, we can bound the probability of Ḡ as\nPr(Ḡ) = Pr(Ḡ|Π is ”nice”)Pr(Π is ”nice”) + Pr(Ḡ|Π is not ”nice”)Pr(Π is not ”nice”) ≤ Pr(Ḡ|Π is ”nice”) + Pr(Π is not ”nice”).\nAccording to Lemma C.3 and Lemma B.10, if m ≥ C1 · CAB nδ2 log(n), and k ≥ C2 log(n) ǫ2 , then both events {G|Π is ”nice”} and Pr(Π is ”nice”) happen w.h.p. in n. Therefore, the the probability of Ḡ is polynomially small in n, i.e., the desired event G happens w.h.p. in n.\nNext we show that ||M̃ − ATB|| ≤ δ||ATB||F holds w.h.p. in n. According to Lemma B.7, if k = Θ( r̃+log(n)ǫ2 ), then w.h.p. in n, we have ||M̃ −ATB|| ≤ ǫ||A||||B||. Now let ǫ := δ ||ATB||F ||A||||B|| , we have that if k = Θ( r̃+log(n) δ2 · ||A||2||B||2 ||ATB||2\nF ), then ||M̃ −ATB|| ≤ δ||ATB||F holds w.h.p. in n. By triangle inequality, we have ||RΩ(M̃ ) − ATB|| ≤ ||RΩ(M̃ ) − M̃ || + ||M̃ − ATB||. We have shown that w.h.p. in n, both terms are less than δ||ATB||F . By rescaling δ as δ/2, we have that the desired inequality ||RΩ(ÃT B̃)−ATB|| ≤ δ||ATB||F holds w.h.p. in n, when m and k are chosen according to the statement of Lemma C.1.\nBecause the bound of Lemma C.1 has the same form as that of Lemma C.2 in [3], the corollary of Lemma C.2 also holds for RΩ(M̃), which is stated here without proof: if ||ATB − (ATB)r||F ≤\n1 576κr1.5 ||(ATB)r||F , then w.h.p. in n we have\n||(Û (0))i|| ≤ 8√r||Ai||/||A||F and dist(Û (0), U∗) ≤ 1/2,\nwhere Û (0) is the initial iterate produced by the WAltMin algorithm (see Step 6 of Algorithm 2). This corollary will be used in the proof of Lemma C.2.\nSimilar to the original proof in [3], we can now consider two cases separately: (1) ||ATB−(ATB)r||F ≥ 1\n576ρr1.5 ||(ATB)r||F ; (2) ||ATB − (ATB)r||F ≤ 1576ρr1.5 ||(ATB)r||F . The first case is simple: use Lemma C.1 and Wely’s inequality [31] already implies the desired bound in Theorem 3.1. To see why, note that Lemma C.1 and Wely’s inequality imply that\n||(ATB)r − (RΩ(M̃ )r|| ξ1 ≤ ||ATB − (ATB)r||+ ||ATB −RΩ(M̃)||+ ||RΩ(M̃)− (RΩ(M̃))r|| ξ2 ≤ ||ATB − (ATB)r||+ δ||ATB||F + ||RΩ(M̃)−ATB||+ ||ATB − (ATB)r|| ξ3 ≤ 2||ATB − (ATB)r||+ 2δ||ATB||F , (12)\nwhere Mr denotes the best rank-r approximation of M , ξ1 follows triangle inequality, ξ2 follows from Lemma C.1 and Wely’s inequality, and ξ3 follows from Lemma C.1. If ||ATB−(ATB)r||F ≥ 1576ρr1.5 ||(ATB)r||F , then ||ATB||F = ||(ATB)r||F + ||ATB − (ATB)r||F ≤ O(ρr1.5)||ATB − (ATB)r||F . Setting δ = O(η/(ρr1.5)) in Eq.(12) gives the desired error bound in Theorem 3.1. Therefore, in the following analysis we only need to consider the second case."
    }, {
      "heading" : "C.3 Proof of Lemma C.2",
      "text" : "We first prove the following lemma, which is a counterpart ofLemma C.5 in [3]. For simplicity of presentation, we use M to denote ATB in the following proof.\nLemma C.4. If m ≥ C1nr log(n)T/(γδ2) and k ≥ C2(r̃+log(n))/ǫ2 for sufficiently large global constants C1 and C2, then the following holds with probability at least 1− γ/T :\n||(U (t))TRΩ(M̃ −Mr)− (U (t))T (M −Mr)|| ≤ δ||M −Mr||F + δǫ||A||F ||B||F + ǫ||A||||B||.\nProof. For a fixed Π, we have that if m ≥ C1nr log(n)T/(γδ2), then following holds with probability at least 1− γ/T : ||(U (t))TRΩ(M̃ −Mr)− (U (t))T (M̃ −Mr)|| ≤ δ||M̃ −Mr||F . (13) The proof of Eq.(13) is exactly the same as the proof of Lemma C.5/B.6/B.2 in [3], so we omit its details here. The key idea is to define a set of zero-mean random matrices Xij such that ∑ ij Xij = (U\n(t))TRΩ(M̃− Mr) − (U (t))T (M̃ −Mr), and then use second moment-based matrix Chebyshev inequality to obtain the desired bound.\nAccording to Lemma B.6 and Lemma B.7, if k = Θ((r̃ + log(n))/ǫ2), then w.h.p. in n, the following holds:\n||M̃ −ATB||F ≤ ǫ||A||F ||B||F , ||M̃ −ATB|| ≤ ǫ||A||||B||. (14) Using triangle inequality, we have that if m and k satisfy the conditions of Lemma C.4, then the follow-\ning holds with probability at least 1− γ/T :\n||(U (t))TRΩ(M̃ −Mr)− (U (t))T (M −Mr)|| ≤ ||(U (t))TRΩ(M̃ −Mr)− (U (t))T (M̃ −Mr)||+ ||(U (t))T (M − M̃)|| ξ1 ≤ δ||M̃ −Mr||F + ||M − M̃ || ≤ δ||M −Mr||F + δ||M − M̃ ||F + ||M − M̃ || ξ2 ≤ δ||M −Mr||F + δǫ||A||F ||B||F + ǫ||A||||B||,\nwhere ξ1 follows from Eq.(13), and ξ2 follows from Eq.(14).\nNow we are ready to prove Lemma C.2. For simplicity, we focus on the rank-1 case here. Rankr proof follows a similar line of reasoning and can be obtained by combining the current proof with the rank-r analysis in the original proof of LELA [3]. Note that compared to Lemma C.5 in [3], Lemma C.4 contains two extra terms δǫ||A||F ||B||F +ǫ||A||||B||. Therefore, we need to be careful for steps that involve Lemma C.4.\nIn the rank-1 case, we use ût and v̂t+1 to denote the t-th and (t+1)-th step iterates (which are column vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized vectors.\nProof. This proof contains two parts. In the first part, we will prove that the distance dist(vt+1, v∗) decreases geometrically over time. In the second part, we show that the j-th entry of vt+1 satisfies |vt+1j | ≤ c1||Bj ||/||B||F , for some constant c1.\nBounding dist(vt+1, v∗): In Lemma C.4, set ǫ = ||A TB||\n2||A||||B||η and δ = η 2r̃ , where 0 < η < 1, then we have δǫ||A||F ||B||F ≤\n||A||F ||B||F ||A||||B|| · η2 2r̃ ||ATB|| ≤ η||ATB||/2, and ǫ||A||||B|| ≤ η||ATB||/2. Therefore, with probability at least 1− γ/T , the following holds:\n||(ut)TRΩ(M̃ −M1)− (ut)T (M −M1)|| ≤ η||M −M1||F /r̃ + ησ∗1 . (15)\nHence, we have ||(ut)TRΩ(M̃ −M1)|| ≤ dist(ut, u∗)||M −M1||+ η||M −M1||F /r̃ + ησ∗1 . Using the explicit formula for WAltMin update (see Eq.(46) and Eq.(47) in [3]), we can bound 〈v̂t+1, v∗〉 and 〈v̂t+1, v∗⊥〉 as follows.\n||ût||〈v̂t+1, v∗〉/σ∗1 ≥ 〈ut, u∗〉 − δ1 1− δ1 √ 1− 〈ut, u∗〉2 − 1 1− δ1 (η ||M −M1||F r̃σ∗1 + η).\n||ût||〈v̂t+1, v∗⊥〉/σ∗1 ≤ δ1 1− δ1 √ 1− 〈ut, u∗〉2 + 1 1− δ1 (dist(ut, u∗) ||M −M1|| σ∗1 + η ||M −M1||F r̃σ∗1 + η).\nAs discussed in the end of Appendix C.2, we only need to consider the case when ||ATB−(ATB)r||F ≤ 1\n576ρr1.5 ||(ATB)r||F , where ρ = σ∗1/σ∗r . In the rank-1 case, this condition reduces to ||M −M1||F ≤ σ ∗ 576 . For sufficiently small constants δ1 and η (e.g., δ1 ≤ 120 , η ≤ 120 ), and use the fact that 〈ut, u∗〉 ≥ 〈u0, u∗〉 and dist(u0, u∗) ≤ 1/2, we can further bound 〈v̂t+1, v∗〉 and 〈v̂t+1, v∗⊥〉 as\n||ût||〈v̂t+1, v∗〉/σ∗1 ≥ 〈u0, u∗〉 − 1\n10\n√ 1− 〈u0, u∗〉2 − 1\n10 ≥\n√ 3\n2 − 2 10 ≥ 1 2 . (16)\n||ût||〈v̂t+1, v∗⊥〉/σ∗1 ≤ δ1\n1− δ1 dist(ut, u∗) +\n1\n576(1 − δ1) dist(ut, u∗) +\n1\n1− δ1 (η ||M −M1||F r̃σ∗1 + η)\nξ1 ≤ 1\n4 dist(ut, u∗) + 2(η||M −M1||F /σ∗1 + η), (17)\nwhere ξ1 uses the fact that r̃ ≥ 1 and the assumption that δ1 is sufficiently small. Now we are ready to bound dist(vt+1, v∗) as\ndist(vt+1, v∗) = √ 1− 〈vt+1, v∗〉2 = 〈v̂ t+1, v∗⊥〉√ 〈v̂t+1, v∗⊥〉2 + 〈v̂t+1, v∗〉2 ≤ 〈v̂ t+1, v∗⊥〉 〈v̂t+1, v∗〉\nξ1 ≤ 1\n2 dist(ut, u∗) + 4(η||M −Mr||F /σ∗1 + η), (18)\nwhere ξ1 follows from substituting Eqs. (16) and (17). Rescaling η as η/4 gives the desired bound of Lemma C.2 for the rank-1 case. Rank-r proof can be obtained by following a similar framework.\nBounding vt+1j : In this step, we need to prove that the j-th entry of vt+1 satisfies |vt+1j | ≤ c1 ||Bj || ||B||F for all j, under the assumption that ut satisfies the norm bound |uti| ≤ c1 ||Ai||||A||F for all i. The proof follows very closely to the second part of proving Lemma C.3 in [3], except that an extra multiplicative term (1 + ǫ) will show up when bounding ∑\ni δijwiju t iM̃ij using Bernstein inequality. More\nspecifically, let Xi = (δij − q̂ij)wijutiM̃ij . Note that if q̂ij = 1, then δij = 1, Xi = 0, so we only need to consider the case when q̂ij < 1, i.e., q̂ij = qij , where qij is defined in Eq.(1).\nSuppose Π is fixed and its dimension satisfies k = Ω( log(n) ǫ2\n), then according to Lemma B.6, we have that w.h.p. in n,\n|M̃ij | ≤ |Mij |+ ǫ||Ai|| · ||Bj || ≤ (1 + ǫ)||Ai|| · ||Bj||, ∀(i, j). (19)\nHence, we have M̃2ij q̂ij ξ1 ≤ (1 + ǫ) 2||Ai||2||Bj ||2\nm · ( ||Ai||2 2n||A||2\nF\n+ ||Bj ||2\n2n||B||2 F\n) ≤ 2n(1 + ǫ)\n2\nm · ||Bj ||2||A||2F , (20)\n(uti) 2\nq̂ij\nξ2 ≤ c 2 1||Ai||2/||A||2F\nm · ( ||Ai||2 2n||A||2\nF\n+ ||Bj ||2\n2n||B||2 F\n) ≤ 2nc\n2 1\nm , (21)\nwhere ξ1 follows from substituting Eqs.(19) and (1), and ξ2 follows from the assumption that |uti| ≤ c1||Ai||/||A||F .\nWe can now bound the first and second moments of Xi as\n|Xi| ≤ |wijutiM̃ij | ≤ √ (uti) 2\nq̂ij √ M̃2ij q̂ij ξ1 ≤ 2nc1(1 + ǫ) m ||Bj ||||A||F .\n∑\ni\nV ar(Xi) = ∑\ni\nq̂ij(1− q̂ij)w2ij(uti)2M̃2ij ≤ ∑\ni\n(uti) 2\nq̂ij (1 + ǫ)2||Ai||2||Bj ||2\nξ2 ≤ 2nc 2 1(1 + ǫ) 2\nm ||Bj ||2||A||2F ,\nwhere ξ1 and ξ2 follows from substituting Eqs.(20) and (21). The rest proof involves applying Bernstein’s inequality to derive a high-probability bound on ∑ i Xi, which is almost the same as the second part of proving Lemma C.3 in [3], so we omit the details here. The only difference is that, because of the extra multiplicative term (1 + ǫ) in the bound of the first and second moments, the lower bound on the sample complexity m should also be multiplied by an extra (1+ ǫ)2 term. By restricting 0 < ǫ < 1/2, this extra multiplicative term can be ignored as long as the original lower bound of m contains a large enough constant."
    }, {
      "heading" : "C.4 Proof of Theorem 3.1",
      "text" : "We now prove our main theorem for rank-1 case here. Rank-r proof follows a similar line of reasoning and can be obtained by combining the current proof with the rank-r analysis in the original proof of LELA [3]. Similar to the previous section, we use ût and v̂t+1 to denote the t-th and (t+1)-th step iterates (which are column vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized vectors.\nThe closed form solution for WAltMin update at t+ 1 iteration is\n||ût||v̂t+1j = σ∗1v∗j ∑ i δijwiju t iu ∗ i∑\ni δijwij(u t i) 2 +\n∑ i δijwiju\nt i(M̃ −M1)ij∑\ni δijwij(u t i) 2\n.\nWriting in matrix form, we get\n||ût||v̂t+1j = σ∗1〈u∗, ut〉v∗ − σ∗1B−1(〈u∗, ut〉B −C)v∗ +B−1y, (22)\nwhere B and C are diagonal matrices with Bjj = ∑ i δijwij(u t i) 2 and Cjj = ∑ i δijwiju t iu ∗ i , and y is the\nvector RΩ(M̃ −M1)Tut with entries yj = ∑ i δijwiju t i(M̃ −M1)ij .\nEach term of Eq.(22) can be bounded as follows.\n||(〈u∗, ut〉B − C)v∗|| ≤ dist(ut, u∗), ||B−1|| ≤ 2, (23)\n||y|| = ||RΩ(M̃ −M1)Tut|| ξ1 ≤ dist(ut, u∗)||M −M1||+ η||M −M1||F /r̃ + ησ∗1 , (24)\nwhere ξ1 follows directly from Lemma C.4. The proof of Eq.(23) is exactly the same as the proof of Lemma B.3 and B.4 in [3].\nAccording to Lemma C.2, since the distance is decreasing geometrically, after O(log(1ζ )) iterations we get\ndist(ut, u∗) ≤ ζ + 2η||M −M1||F /σ∗1 + 2η. (25)\nNow we are ready to prove the spectral norm bound in Theorem 3.1:\n||M1 − ût(v̂t+1)T || ≤ ||M1 − ut(ut)TM1||+ ||ut(ut)TM1 − ût(v̂t+1)T || ≤ ||(I − ut(ut)T )M1||+ ||ut[(ut)TM1 − ||ût||(v̂t+1)T ]|| ξ1 ≤ σ∗1dist(ut, u∗) + ||σ1〈ut, u∗〉v∗ − ||ût||(v̂t+1)T || ξ2 ≤ σ∗1dist(ut, u∗) + ||σ∗1B−1(〈u∗, ut〉B − C)v∗||+ ||B−1y|| ξ3 ≤ σ∗1dist(ut, u∗) + 2σ∗1dist(ut, u∗) + 2dist(ut, u∗)||M −M1||+ 2η||M −M1||F /r̃ + 2ησ∗1 ξ4 ≤ 5(ζσ∗1 + 2η||M −M1||F + 2ησ∗1) + 2η||M −M1||F + 2ησ∗1 = 5ζσ∗1 + 12η||M −M1||F + 12ησ∗1 (26)\nwhere ξ1 follows from the definition of dist(ut, u∗), the fact that ||ut|| = 1, and (ut)TM1 = σ1〈ut, u∗〉v∗, ξ2 follows from substituting Eq.(22), ξ3 follows from Eqs.(23) and (24), and ξ4 follows from the Eq.(25), and fact that ||M −M1|| ≤ σ∗1 , r̃ ≥ 1. Rescaling ζ to ζ/(5σ∗1) (this will influence the number of iterations) and also rescaling η to η/12 gives us the desired spectral norm error bound in Eq.(7). This completes our proof of the rank-1 case. Rank-r proof follows a similar line of reasoning and can be obtained by combining the current proof with the rank-r analysis in the original proof of LELA [3]."
    }, {
      "heading" : "C.5 Sampling",
      "text" : "We describe a way to sample m elements in O(m log(n)) time using distribution qij defined in Eq. (1). Naively one can compute all the n2 entries of min{qij , 1} and toss a coin for each entry, which takes O(n2) time. Instead of this binomial sampling we can switch to row wise multinomial sampling. For this, first estimate the expected number of samples per row mi = m( ||Ai|| 2\n2||A||2 F\n+ 12n). Now sample m1 entries from row\n1 according to the multinomial distribution,\nq̃1j = m m1 · ( ||A1|| 2 2n||A||2F + ||Bj ||2 2n||B||2F ) =\n||A1||2\n2n||A||2 F\n+ ||Bj ||\n2\n2n||B||2 F\n||Ai||2 2||A||2 F + 12n\n.\nNote that ∑\nj q̃1j = 1. To sample from this distribution, we can generate a random number in the interval [0, 1], and then locate the corresponding column index by binary searching over the cumulative distribution function (CDF) of q̃1j . This takes O(n) time for setting up the distribution and O(m1 log(n)) time to sample. For subsequent row i, we only need O(mi log(n)) time to sample mi entries. This is because for binary search to work, only O(mi log(n)) entries of the CDF vector needs to be computed and checked. Note that the specific form of q̃ij ensures that its CDF entries can be updated in an efficient way (since we only need to update the linear shift and scale). Hence, sampling m elements takes a total O(m log(n)) time. Furthermore, the error in this model is bounded up to a factor of 2 of the error achieved by the Binomial model [7] [21]. For more details please see our Spark implementation."
    }, {
      "heading" : "D Related work",
      "text" : ""
    }, {
      "heading" : "Approximate matrix multiplication:",
      "text" : "In the seminal work of [14], Drineas et al. give a randomized algorithm which samples few rows of A and B and computes the approximate product. The distribution depends on the row norms of the matrices and\nthe algorithm achieves an additive error proportional to ||A||F ||B||F . Later Sarlos [29] propose a sketching based algorithm, which computes sketched matrices and then outputs their product. The analysis for this algorithm is then improved by [10]. All of these results compare the error ||ATB − ÃT B̃||F in Frobenius norm.\nFor spectral norm bound of the form ||ATB − C||2 ≤ ǫ||A||2||B||2, the authors in [29, 11] show that the sketch size needs to satisfy O(r/ǫ2), where r = rank(A) + rank(B). This dependence on rank is later improved to stable rank in [26], but at the cost of a weaker dependence on ǫ. Recently, Cohen et al. [12] further improve the dependence on ǫ and give a bound of O(r̃/ǫ2), where r̃ is the maximum stable rank. Note that the sketching based algorithm does not output a low rank matrix. As shown in Figure 2, rescaling by the actual column norms provide a better estimator than just using the sketched matrices. Furthermore, we show that taking SVD on the sketched matrices gives higher error rate than our algorithm (see Figure 3(b)).\nLow rank approximation: [16] introduced the problem of computing low rank approximation of a given matrix using only few passes over the data. They gave an algorithm that samples few rows and columns of the matrix and computes its SVD for low rank approximation. They show that this algorithm achieves additive error guarantees in Frobenius norm. [15, 29, 19, 13] have later developed algorithms using various sketching techniques like Gaussian projection, random Hadamard transform and volume sampling that achieve relative error in Frobenius norm.[35, 28, 18, 6] improved the analysis of these algorithms and provided error guarantees in spectral norm. More recently [11] presented an algorithm based on subspace embedding that computes the sketches in the input sparsity time.\nAnother class of methods use entrywise sampling instead of sketching to compute low rank approximation. [1] considered an uniform entrywise sampling algorithm followed by SVD to compute low rank approximation. This gives an additive approximation error. More recently [3] considered biased entrywise sampling using leverage scores, followed by matrix completion to compute low rank approximation. While this algorithm achieves relative error approximation, it takes two passes over the data.\nThere is also lot of interesting work on computing PCA over streaming data under some statistical assumptions, e.g., [2, 27, 5, 30]. In contrast, our model does not put any assumptions on the input matrix. Besides, our goal here is to get a low rank matrix and not just the subspace."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In this paper we present a new algorithm for computing a low rank approximation of the product<lb>AB by taking only a single pass of the two matrices A and B. The straightforward way to do this is to<lb>(a) first sketch A and B individually, and then (b) find the top components using PCA on the sketch. Our<lb>algorithm in contrast retains additional summary information about A,B (e.g. row and column norms<lb>etc.) and uses this additional information to obtain an improved approximation from the sketches. Our<lb>main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in<lb>addition we also provide results from an Apache Spark implementation that shows better computational<lb>and statistical performance on real-world and synthetic evaluation datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}