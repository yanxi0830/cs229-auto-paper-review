{
  "name" : "1501.05396.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Multi-Modal Learning for Audio-Visual Speech Recognition",
    "authors" : [ "Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Human speech perception is not only about hearing but also about seeing: our brain integrates the waveforms representing the speech information as well as the lips poses and motions, often called visemes, which carry important visual information about what is being said. This has been demonstrated by the so called McGurk effect [MM76], which shows that a voicing of ba and a mouthing of ga is perceived as being da. In the presence of noise and multiple speakers (cocktail party effect), humans rely on lip reading in order to enhance speech recognition [CHLN08]. The visual information is also important in a clean speech scenario as it helps in disambiguating voices with similar acoustics [Sum92].\nIn Audio-Visual Automatic Speech Recognition (AV-ASR), both audio recordings and videos of the person talking are available at training time. It is challenging to build models that integrates both visual and audio information, and that enhance the recognition performance of the overall system. While most previous works in AV-ASR focused on enhancing the performance in the noisy case [PNLM04, NKK+11], where the visual information can be crucial, we focus in this paper on showing that the visual information is indeed helpful even in the clean speech scenario.\n∗This work was done while Youssef Mroueh was an intern in the Speech and Algorithms Group at IBM T.J Watson Research Center.\nar X\niv :1\n50 1.\n05 39\n6v 1\n[ cs\n.C L\n] 2\nMultimodal learning consists of fusing and relating information coming from different sources, hence AV-ASR is an important multimodal problem. Finding correlations between different modalities, and modeling their interactions, has been addressed in various learning frameworks and has been applied to AV-ASR [Gea09, LS06, MHD96, PKPM07, PKPM09, PKPM06]. Deep Neural Networks (DNN) have shown impressive performance in both audio and visual classification tasks, which is why we restrict ourselves to the deep multimodal learning framework [YGS89, NKK+11, SS, SCMN, AALB13].\nIn this paper, we propose methods in deep learning to fuse modalities, and validate them on the IBM AV-ASR Large Vocabulary Studio Dataset (Section 2). First we consider the training of two networks on the audio and the visual modality separately. Then, considering the last layer of each network as a better feature space, and concatenating them, we train a classifier on that joint representation, and obtain gains in Phone Error Rates (PER), with respect to an audio-only trained network. We then propose a new bilinear network that accounts for correlations between modalities and allows for joint training of the two networks, we show that a committee of such bilinear networks, fused at the level of posteriors, achieves a better PER in a clean speech scenario.\nThe paper is organized as follows. In Section 2 we present the IBM AV-ASR large vocabulary studio dataset, our feature extraction pipeline for the audio and the visual channels. Next, in Section 3, we present results for the fusion of networks separately trained on each modality. In Section 4 we introduce the bilinear DNN that allows for a joint training and captures correlations between the two modalities, and derive its back-propagation algorithm in Section 5. Finally we present posterior combination of bimodal and bilinear bimodal DNNs in Section 6."
    }, {
      "heading" : "2 Audio-Visual Data Set & Feature Extraction",
      "text" : "In this Section we present the IBM AV-ASR Large Vocabulary Studio dataset, and our feature extraction pipeline."
    }, {
      "heading" : "2.1 IBM AV-ASR Large Vocabulary Studio Dataset",
      "text" : "The IBM AV-ASR Large Vocabulary Studio Dataset consists of 40 hours of audio-visual recordings from 262 speakers. These were carried out in clean, studio conditions. The audio is sampled at 16 KHz along with the video frame rate of 30 frames per second at 704 × 480 resolution. The vocabulary size in these recordings is 10, 400 words. This data set was divided into a test set of 2 hours of audio+video from 22 speakers, with the rest used for training."
    }, {
      "heading" : "2.2 Feature Extraction",
      "text" : "For the audio channel we extract 24 MFCC coefficients at 100 frames per second. Nine consecutive frames of MFCC coefficients are stacked and projected to 40 dimensions using an LDA matrix. Input to the audio neural network is formed by concatenating ±4 LDA frames to the central frame of interest, resulting in an audio feature vector of dimension 360.\nFor the visual channel we start by detecting the face in the image using the openCV implementation of the Viola-Jones algorithm. We then do a mouth carving by an openCV mouth detection model. Both these utilize the ENCARA2 model as described in [MDHL11]. In order to get an invariant representation to small distortions and scales we then extract level 1 and level 2 scattering coefficients [BM13] on the 64× 64 mouth region of interest and then reduce their dimension to 60 using LDA (Linear discriminant Analysis). In order to match the audio frame rate we replicate\nvideo frames according to audio and video time stamps. We also add ±4 context frames to the central frame of interest, and obtain finally a visual feature vector of dimension 540."
    }, {
      "heading" : "2.3 Context-dependent Phoneme Targets",
      "text" : "Each audio+video frame is labeled with one of 1328 targets that represent context dependent phonemes. 42 phones in phonetic context of ±2 are clustered using decision trees down to 1328 classes. We measure classification error rate at the level of these 1328 classes, this is referred to as phone error rate (PER)."
    }, {
      "heading" : "3 Uni-modal DNNs & Feature Fusion",
      "text" : "In the supervised multimodal scenario, we are given a training set S of N labeled examples, and C classes:\nS = {(x1i , x2i , yi), i = 1 . . . N}, yi ∈ Y = {1 . . . C},\nwhere x1i , x 2 i correspond to the first and the second modality feature vectors, respectively. We note ti = eyi the classification targets, where {ey}y∈Y is the canonical basis in RC . Let ρ(y|x1, x2) be the posterior probability of being in class y given the two modalities x1 and x2. In a classification task, we would like to find the model that maximizes the cross-entropy E :\nE = 1 N N∑ i=1 C∑ y=1 tyi log ρ(y|x 1 i , x 2 i ). (1)\nThe first multimodal modeling approach we study is to train two separate networks DNNa and DNNv on the audio and the visual features, respectively. The networks are optimized under the cross-entropy objective (1) using the stochastic gradient descent. We formed a joint Audio-Visual feature representation by concatenating the outputs of final hidden layers of these two networks, as shown in Figure 1. This feature space is then kept fixed while a deep or a shallow (softmax only) network is trained in this fused space up to the targets. To keep the feature space dimension manageable, we configure the individual audio and video networks to have a low dimensional final hidden layer.\nWe consider forDNNa andDNNv the following architecture dim/1024/1024/1024/1024/1024/200/1328, where dim = 360 for DNNa and dim = 540 for DNNv. The fused feature space dimension is 400.\nWhile DNNa achieves a PER of 41.25%, DNNv alone achieves a PER of 69.36%, showing that the visual information alone carries some information but that is not enough in itself to get a low error rate. A deep network built in the fused feature space results in a PER of 35.77% while a softmax layer only in this feature space yields PER of 35.83%. This substantial PER gain from joint audio-visual representation, even in clean audio conditions, demonstrates the value of visual information for the phoneme classification task. Interestingly, the deep and the shallow fusion are roughly on par in terms of PER. Results are summarized in the following table:"
    }, {
      "heading" : "4 Bilinear Deep Neural Network",
      "text" : "In the previous section the training was done separately on the two modalities, in this section we address the joint training problem, and introduce the bilinear bimodal DNN.\nFor a DNN, we note by σ the non linearity function (sigmoid in this paper), v` the input of a unit, and h` the output of a unit in a layer `. For a layer ` we note the dimension of an input v` by K`. As shown in Figure 1, we consider two DNNs, one for each modality that we fuse at the level of the decision function. For simplicity of the exposure, we assume the same number of layers L (L1 = L2 = L). For the intermediate layers, we have the standard separate networks:\nhj` = σ(W j,> ` v j ` + b j `), v j ` = h j `−1, h j 0 = x j ,\nW j` ∈ R Kj`×K j `+1 , bj` ∈ R Kj`+1 , ` = 1 . . . L− 1, j ∈ {1, 2}.\nThe fusion happens at the last hidden layer, where the posteriors capture the correlation between the intermediate non-linear features of the two modalities produced by the DNN layers, through a bilinear term. Let v1L = h 1 L−1, v 2 L = h 2 L−1, the posteriors have the following form:\nρ ( y|x1, x2 ) = exp\n( v1,>L W yv2L + V > y ( v1L v2L ) + by ) Z , (2)\nwhere Z = ∑\ny′∈Y exp ( v1,>L W y′v2L + V > y′ ( v1L v2L ) + by′ ) , W y ∈ RK1L×K2L , Vy = [V 1y , V 2y ] ∈ R(K 1 L+K 2 L), by ∈\nR and y ∈ {1 . . . C}."
    }, {
      "heading" : "4.1 Factored Bilinear Softmax",
      "text" : "As the number of classes increases, the bilinear model becomes cumbersome computationally, and we need large training sets to get better estimates of the parameters. In order to decrease the computational complexity of the model, we propose the use of a factorization of the bilinear term,\nthat is similar to the one in [MZHP], but is motivated in our case by Canonical Correlation Analysis (CCA) [HSST04]:\nW y = U1diag(wy)U 2,>, y = 1 . . .C, (3)\nwhere U1 ∈ RK1L×F , U2 ∈ RK2L×F , wy ∈ RF , and diag(wy) is a diagonal matrix with wy on its diagonal . For numerical stability we consider ||U j ||F ≤ λ, j ∈ {1, 2}, where λ is a regularization parameter. We note by F the dimension of the fused space, which is typically smaller than K1L and K2L . Considering the factorization in (3) and maximizing the cross-entropy in the bilinear model (2), we have :\nlog ρ(y|x1, x2) = Tr(U1,>v1Lv 2,> L U 2diag(wy)) + 〈 V1y, v 1 L 〉 + 〈 V2y, v 2 L 〉 + by − log(Z).\nFor fixed weights wy, learning (U 1, U2) corresponds to a class specific weighted CCA-like learning where we are looking for projections that maximize alignment between the intermediate features of the two modalities, in a discriminative way. Deep CCA of [AALB13] shares similarities with this model. On the other hand, for fixed (U1, U2), we can rewrite the log-posteriors in the following way:\nlog(ρ(y|x1, x2)) = 〈 wy, U 1,>v1L U2,>v2L 〉 + 〈 V 1y , v 1 L 〉 + 〈 V 2y , v 2 L 〉 + by − log(Z)\nwhere is the element-wise vector product. Hence, for fixed (U1, U2), we are learning a linear hyperplane in the fused space of dimension F . The projection on U1 and U2 defines a CCA-like lower dimensional spaces, where the two modalities are maximally correlated. The fused space is then defined as the element-wise vector product between two co-occurring vectors in the CCA-like lower dimensional spaces. Hence, we can think of the last layer of the bilinear, bimodal DNN as being an ordinary softmax, having the following input (v1L, v 2 L, U 1,>v1L U2,>v2L) ∈ RK 1 L+K 2 L+F . Therefore the decision function is learned based on the individual contributions of the modalities v1L and v 2 L, as well as the joint representation produced by the fused space U1,>v1L U2,>v2L."
    }, {
      "heading" : "4.2 Factored Bilinear Softmax With Sharing",
      "text" : "When the classes we would like to predict are organized as the leaves of a tree structure of depth two, we can further reduce the computational complexity by sharing weights between leaves having the same parent node. This is the case in AV-ASR as the 1328 contextual phoneme states are organized as leaves of a tree, where the parent nodes correspond to 42 different phoneme categories. In that case we share the bilinear term across leaves having the same parents. By doing so in the case of AV-ASR, we are only taking into account the correlations between the audio and the visual channel at the phoneme level, rather than on a fine grained grid of contextual states. We can think of this sharing as a pooling operation at the phoneme level. More formally, assume that the label set Y is partitioned into G non overlapping groups {Yg}g=1...G, we assume that:\nW y = W g = U1diag(wg)U 2,>, ∀ y ∈ Yg, g = 1 . . .G.\nHence we reduce the number of weights to learn for the joint representation from C ×F to G×F ."
    }, {
      "heading" : "5 Back-propagation with the Factored Bilinear DNN with Sharing",
      "text" : "In this section we give the back-propagation algorithm and the update rules for the bilinear DNN with sharing (bi2-DDN-wS ). Recall that our classes have a tree structure with leaves y, and parent\nnodes g; a training example is therefore labeled by its leave label y (States) as well its parent node g (Phonemes), (x1, x2, y, g), y ∈ {1 . . . C}, and g ∈ {1 . . . G}. We use the notation g(y) to note the group to which y belongs, and we set Rootg(y) = 1, Rootg = 0, g = 1 . . . G, g 6= g(y). For the bilinear softmax with sharing, we keep track of the errors at the level of the labels (States), as well as the groups level (Phonemes):\nδkL = tk − ρ(k|v1L, v2L), k = 1 . . . C, δL ∈ RC×1. δgG = Rootg − ∑ k∈Yg ρ(k|v1L, v2L), g = 1 . . . G, δG ∈ RG×1.\nLet W = [w1, . . . , wG] ∈ RF×G, the gradients of the parameters of the bilinear softmax are given by:\n∂E ∂W\n= ( U1,>v1L U2,>v2L ) δ>G.\n∂E ∂U1 = v1Lv 2,> L U 2diag(WδG), ∂E ∂U2 = v2Lv 1,> L U 1diag(WδG).\n∂E ∂V 1 = v1Lδ > L , ∂E ∂V 2 = v2Lδ > L , ∂E ∂b = δL.\nFor the layer right before the Bilinear softmax, we have a double projection to the first modality network (audio stream) and to the second modality network (visual stream). We need to compute:\nW g = U1diag(wg)U 2,>, g = 1 . . .G.\nm2→1g = W gv2L M 2→1 L = [m 2→1 1 , . . . ,m 2→1 G ] ∈ RK 1 L×G. m1→2g = W g,>v1L M 1→2 L = [m 1→2 1 , . . . ,m 1→2 G ] ∈ RK 2 L×G.\nLet V j = [V j1 . . . V j C ], j ∈ {1, 2}, hence the errors we propagate to each network have the following form:\nδ1L−1 = ∂E ∂v1L = M2→1L δG + V 1δL. (4) δ2L−1 = ∂E ∂v2L = M1→2L δG + V 2δL. (5)\nNote that the errors now have an additional term, M2→1L δG, and M 1→2 L δG, respectively. We can think of those terms as messages passed between networks through the bilinear term. In that way, one network influences the weights of the other one. For the rest of the updates, it follows standard back-propagation in both networks; we give it here for completeness. Let u1` = W 1,> ` v 1 ` + b1` , u 2 ` = W 2,> ` v 2 ` + b 2 ` , then finally we have:\n∂E ∂W j` = vj` (diag(σ ′(uj`)δ j ` ) >, ∂E ∂bj` = diag(σ′(uj`))δ j ` ,\nδj`−1 = W j ` δ j ` , j ∈ {1, 2}, ` = L− 1 . . . ..1, where δ 1 L−1, and δ 2 L−1 are given in equations (4) and (5). For each variable θ, we have an update rule θ ← θ+ η ∂E∂θ , where η is the learning rate. For U 1 and U2, we need to keep control of the Frobenius norm by following the gradient step with a projection to the Frobenius ball : U j ← U j min(1, λ||Uj ||F ), j ∈ {1, 2}.\nRemark 1. For the bilinear softmax without sharing the update rules are similar (δG is replaced by δL)."
    }, {
      "heading" : "6 Combining Posteriors from Bimodal and Bilinear Bimodal Net-",
      "text" : "works\nWe experiment with various factored bi2-DNN-wS architectures, initialized at random on the IBM AV-ASR Large Vocabulary Studio Dataset. We use the following notation for the architecture of the bilinear network: [archa|archv|F ], where archa and archv are the architectures of the audio and the visual network respectively, and F is the dimension of the fused space. We consider architectures by increasing complexity Arch = [360, 500, 500, 200, 1328|540, 500, 500, 200, 1328|F = 200], Arch1 = [360, 600, 600, 400, 100, 1328|540, 600, 600, 400, 100, 1328|F = 100], and Arch2 = [360, 500, 500, 500, 500, 500, 200, 1328|540, 500, 500, 500, 500, 500, 200, 1328|F = 200]. In all our experiments we set λ = 2. Recall that the bimodal DNN using the separate training paradigm introduced in Section 3 achieves 35.83% PER. As shown in Table 2, each architecture alone does not improve on the bimodal DNN, but averaging the posteriors of the three architectures we obtain a small gain. A gain of 1.8% absolute is obtained by averaging the posteriors of the bimodal and the bilinear bimodal networks, showing that the bilinear networks have uncorrelated errors with the bimodal network."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper we have studied deep multimodal learning for the task of phonetic classification from audio and visual modalities. We demonstrate that even in clean acoustic conditions using visual channel in addition to speech results in signifiantly improved classification performance. A bilinear bimodal DNN is introduced which leverages correlation between the audio and visual modalities, and leads to further error rate reduction."
    } ],
    "references" : [ {
      "title" : "In International Conference on Machine Learning (ICML)",
      "author" : [ "Galen Andrew", "Raman Arora", "Karen Livescu", "Jeff Bilmes. Deep canonical correlation analysis" ],
      "venue" : "Atlanta, Georgia,",
      "citeRegEx" : "AALB13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "35(8):1872–1886",
      "author" : [ "Joan Bruna", "Stephane Mallat. Invariant scattering convolution networks. IEEE Trans. Pattern Anal. Mach. Intell." ],
      "venue" : "August",
      "citeRegEx" : "BM13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The challenge of multispeaker lip-reading",
      "author" : [ "S. Cox", "R. Harvey", "Y. Lan", "J. Newman" ],
      "venue" : "International Conference on Auditory-Visual Speech Processing",
      "citeRegEx" : "CHLN08",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Information theoretic feature extraction for audio-visual speech recognition",
      "author" : [ "Mihai Gurban" ],
      "venue" : "IEEE Transactions on signal processing,",
      "citeRegEx" : "Gea09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Canonical correlation analysis: An overview with application to learning methods",
      "author" : [ "David R. Hardoon", "Sndor Szedmk", "John Shawe-Taylor" ],
      "venue" : "Neural Computation, 16(12):2639–2664,",
      "citeRegEx" : "HSST04",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "In Proceedings of the HCSNet Workshop on Use of Vision in Human-computer Interaction - Volume 56",
      "author" : [ "Patrick Lucey", "Sridha Sridharan. Patch-based representation of visual speech" ],
      "venue" : "VisHCI ’06, pages 79–85, Darlinghurst, Australia, Australia,",
      "citeRegEx" : "LS06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Machine Vision and Applications",
      "author" : [ "Modesto Castrillon Mcastrillon", "Oscar Deniz", "Daniel Hernandez", "Javier Lorenzo. A comparison of face", "facial feature detectors based on the violajones general object detection framework" ],
      "venue" : "22(3):481–494,",
      "citeRegEx" : "MDHL11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "and Paul Duchnowski",
      "author" : [ "Uwe Meier", "Wolfgang Hrst" ],
      "venue" : "Adaptive bimodal sensor fusion for automatic speechreading,",
      "citeRegEx" : "MHD96",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Hearing lips and seeing voices",
      "author" : [ "H. McGurk", "J. MacDonald" ],
      "venue" : "Nature, 264:746–748",
      "citeRegEx" : "MM76",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "In International Conference on Machine Learning (ICML)",
      "author" : [ "Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y. Ng. Multimodal deep learning" ],
      "venue" : "Bellevue, USA, June",
      "citeRegEx" : "NKK+11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "In INTERSPEECH 2006 - ICSLP",
      "author" : [ "Vassilis Pitsikalis", "Athanassios Katsamanis", "George Papandreou", "Petros Maragos. Adaptive multimodal fusion by uncertainty compensation" ],
      "venue" : "Ninth International Conference on Spoken Language Processing, Pittsburgh, PA, USA, September 17-21, 2006,",
      "citeRegEx" : "PKPM06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "In IEEE 9th Workshop on Multimedia Signal Processing",
      "author" : [ "George Papandreou", "Athanassios Katsamanis", "Vassilis Pitsikalis", "Petros Maragos. Multimodal fusion", "learning with uncertain features applied to audiovisual speech recognition" ],
      "venue" : "MMSP 2007, Chania, Crete, Greece, October 1-3, 2007, pages 264–267,",
      "citeRegEx" : "PKPM07",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Speech & Language Processing",
      "author" : [ "George Papandreou", "Athanassios Katsamanis", "Vassilis Pitsikalis", "Petros Maragos. Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition. IEEE Transactions on Audio" ],
      "venue" : "17(3):423–435,",
      "citeRegEx" : "PKPM09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Audio-visual automatic speech recognition: An overview",
      "author" : [ "G. Potamianos", "C. Neti", "J. Luettin", "I. Matthews" ],
      "venue" : "Issues in Visual and Audio-Visual Speech Processing. MIT Press",
      "citeRegEx" : "PNLM04",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Lipreading and audio-visual speech perception",
      "author" : [ "Q. Summerfield" ],
      "venue" : "Trans. R. Soc., London",
      "citeRegEx" : "Sum92",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Integration of acoustic and visual speech signals using neural networks",
      "author" : [ "Ben P. Yuhas", "Moise H. Goldstein", "Terrence J. Sejnowski" ],
      "venue" : "IEEE Communications Magazine,",
      "citeRegEx" : "YGS89",
      "shortCiteRegEx" : null,
      "year" : 1989
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "This has been demonstrated by the so called McGurk effect [MM76], which shows that a voicing of ba and a mouthing of ga is perceived as being da.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "In the presence of noise and multiple speakers (cocktail party effect), humans rely on lip reading in order to enhance speech recognition [CHLN08].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : "The visual information is also important in a clean speech scenario as it helps in disambiguating voices with similar acoustics [Sum92].",
      "startOffset" : 128,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "Both these utilize the ENCARA2 model as described in [MDHL11].",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "In order to get an invariant representation to small distortions and scales we then extract level 1 and level 2 scattering coefficients [BM13] on the 64× 64 mouth region of interest and then reduce their dimension to 60 using LDA (Linear discriminant Analysis).",
      "startOffset" : 136,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "that is similar to the one in [MZHP], but is motivated in our case by Canonical Correlation Analysis (CCA) [HSST04]: W y = Udiag(wy)U 2,>, y = 1 .",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "Deep CCA of [AALB13] shares similarities with this model.",
      "startOffset" : 12,
      "endOffset" : 20
    } ],
    "year" : 2015,
    "abstractText" : "In this paper, we present methods in deep multimodal learning for fusing speech and visual modalities for Audio-Visual Automatic Speech Recognition (AV-ASR). First, we study an approach where uni-modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built. While the audio network alone achieves a phone error rate (PER) of 41% under clean condition on the IBM large vocabulary audio-visual studio dataset, this fusion model achieves a PER of 35.83% demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio. Second, we present a new deep network architecture that uses a bilinear softmax layer to account for class specific correlations between modalities. We show that combining the posteriors from the bilinear networks with those from the fused model mentioned above results in a further significant phone error rate reduction, yielding a final PER of 34.03%.",
    "creator" : "LaTeX with hyperref package"
  }
}