{
  "name" : "1608.06235.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adaptive Probabilistic Trajectory Optimization via Efficient Approximate Inference",
    "authors" : [ "Yunpeng Pan", "Xinyan Yan", "Evangelos Theodorou", "Byron Boots" ],
    "emails" : [ "ypan37@gatech.edu,", "xyan43@gatech.edu,", "evangelos.theodorou@gatech.edu,", "bboots@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Over the last decade, reinforcement learning (RL) has started to be successfully applied to robotics and autonomous systems. While model-free RL has demonstrated promising results [1], it typically requires human expert demonstrations or relies on lots of direct interactions with physical systems. Model-based RL was developed to address the issue of sample inefficiency by learning dynamics models explicitly from data, which can provide better generalization [2, 1]. However, these deterministic model-based methods suffer from error in the learned models which compounds when making long-range predictions. Recent probabilistic model-based RL methods overcome this issue, achieving state-of-the-art performance [3–5]. These methods represent dynamics models using nonparametric Gaussian processes (GPs) and take into account model uncertainty for control policy learning. Despite these successes, inference in nonparametric probabilistic models can be computationally demanding, making it difficult to adapt to rapid changes in the environment or dynamics.\nIn contrast to probabilistic model-based RL methods, where learning through interaction occurs at the scale of trajectories/rollouts, Model Predictive Control (MPC) is more reactive, performing re-optimization every few timesteps. However, MPC also requires accurate models of the system dynamics and environment. Our goal in this work is to develop a method that combines the benefits of MPC with probabilistic model-based RL to perform learning control at small time scales with little prior knowledge of the dynamics. More precisely, we propose a method that relies on local trajectory optimization such as DDP [6], which is scalable and has been shown to work well on challenging learning control tasks in robotics [7–9]. We model the dynamics probabilistically and perform optimization in belief space. In previous GP-related RL methods, approximate inference is the major computational bottleneck [3–5]. Therefore, we develop two novel scalable approximate inference algorithms based on Sparse Spectrum Gaussian Processes (SSGPs) [10]. To cope with\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 8.\n06 23\n5v 1\n[ cs\n.R O\n] 2\n2 A\nug 2\nchanges in the task or with varying dynamics we apply online re-optimization in the spirit of MPC. This combination leads to a general data-driven framework for online trajectory optimization."
    }, {
      "heading" : "2 Trajectory Optimization",
      "text" : "We consider a general unknown dynamical system described by the following differential equation\ndx = F(x,u)dt+ Cdω, x(t0) = x0, dω ∼ N (0,Σω), (1)\nwhere x ∈ Rn is the state, u ∈ Rm is the control and ω ∈ Rp is standard Brownian noise. The goal of optimal control and reinforcement learning is to find the control policy π(x(t), t) that minimizes the expected cost\nJπ(x(t0)) = Ex [ h ( x(T ) ) + ∫ T t0 L ( x(t), π(x(t)), t ) dt ] , (2)\nwhere h(x(T )) is the terminal cost, and L(x(t), π(x(t)), t) is the instantaneous cost rate. The control policy u(t) = π(x(t), t) is a function that maps states and time to controls. The cost Jπ(x(t0)) is defined as the expectation of the total cost accumulated from t0 to T . Ex denotes the expectation operator with respect to x. We assume that the states are fully observable. For the rest of our analysis, we discretize the time as k = 1, 2, ...,H with time step ∆t = TH−1 and denote xk = x(tk). We use this subscript rule for other time-varying variables as well. The discretized system dynamics can be written as xk+1 = xk + ∆xk where ∆xk = ∆tF(xk,uk). To simplify notation we define f(xk,uk) = ∆tF(xk,uk). Throughout the paper we consider the quadratic instantaneous cost function L(xk,uk) = (xk − xgoalk )TQ(xk − x goal k ) + u T kRuk, where Q and R are weighting matrices. The problem formulation (2) is a standard finite horizon control or RL problem. However, in this paper we will perform this optimization at every time step in a receding horizon fashion.\nDifferential Dynamic Programming (DDP) DDP is a model-based trajectory optimization method for solving optimal control problems defined in (2). The main idea is that a complex nonlinear control problem can be simplified using local approximations such that the original problem becomes a linear-quadratic problem in the neighborhood of a trajectory. In DDP, and related methods such as iLQG [11], a local model is constructed based on i) a first or second-order linear approximation of the dynamics model; ii) a second-order local approximation of the value function along a nominal trajectory. The optimal control law and value function can be computed in a backward pass. The control law is used to generate a new state-control trajectory in a forward pass, this trajectory becomes the new nominal trajectory for the next iteration. DDP uses the aforementioned backward-forward pass to optimize the trajectory iteratively until convergence to an optimal solution. DDP-related methods have been widely used for solving control problem in robotics tasks [7, 8]."
    }, {
      "heading" : "3 Probabilistic Model Learning and Inference",
      "text" : "DDP requires an accurate dynamics model, however a good analytic model is not always available. Therefore, various methods have been used within trajectory optimization frameworks to learn a model. In iLQG-LD [12] and Minimax DDP[13], the dynamics are learned using Locally Weighted Projection Regression (LWPR) [14] and Receptive Field Weighted Regression (RFWR) [15]. However, both methods require a large amount of training data collected from lots of interactions with the physical system. Recently, probabilistic methods such as PDDP [4] and AGP-iLQR [16] have demonstrated impressive data efficiency with Gaussian process (GP) dynamics models. However, GP inference in PDDP [4] is too computationally expensive for online optimization and incremental updates. AGP-iLQR [16] employs subset of regression (SOR) approximations that feature faster GP inference and incremental model adaptation, but neglects the predictive uncertainty when performing multi-step predictions using the learned forward dynamics. This leads to biased long-range predictions. In order to perform efficient and robust trajectory optimization, we introduce a learning and inference scheme based on Sparse Spectrum Gaussian Processes (SSGPs) [10, 17]."
    }, {
      "heading" : "3.1 Model learning via sparse spectrum Gaussian processes",
      "text" : "Learning a continuous mapping from state-control pairs x̃ = (x,u) ∈ Rn+m to state transitions ∆x ∈ Rn can be viewed as probabilistic inference. Given a sequence of N state-control pairs X̃ = {(xi,ui)}Ni=1 and the corresponding state transition ∆X = {∆xi}Ni=1, Gaussian processes (GP)\ncan be leveraged to learn the dynamics model [3–5]. The posterior distribution of the state transition at a test state-control pair can be computed in close-form by conditioning on the observations, because they are jointly Gaussian. Although GP regression is a powerful regression technique, it exhibits significant practical limitations for learning and inference on large datasets due to its O(N3) computation and O(N2) space complexity, which is a direct consequence of having to store and invert a N ×N matrix. This computational inefficiency is a bottleneck for applying GP-based RL in real-time.\nSparse Spectrum GP Regression (SSGPR) SSGP [10] is a recent approach that provides a principled approximation of GPR by employing a random Fourier feature approximation of the kernel function [18]. Based on Bochner’s theorem [19], any shift-invariant kernel functions can be represented as the Fourier transform of a unique measure k(x̃i − x̃j) = ∫ Rn+m e iωT(x̃i−x̃j)p(ω)dω = Eω[φω(x̃i) Tφω(x̃j)]. We can, therefore, unbiasedly approximate any shift-invariant function by\ndrawing r random samples from the distribution p(ω), k(x̃i, x̃j) ≈ ∑r i=1 φωi(x̃i) Tφωi(x̃j) = φ(x̃i) Tφ(x̃j), where φ(x̃) is a feature mapping which maps a state-control pair to feature space.\nWe consider the popular Squared Exponential (SE) covariance function with Automatic Relevance Determination (ARD) distance measure as it has been applied successfully in learning dynamics and optimal control [17, 16], k(x̃i, x̃j) = σ2f exp(− 12 (x̃i − x̃j)\nTP−1(x̃i − x̃j)), where P = diag({l2i } n+m i=1 ). The hyper-parameters consist of the signal variance σ 2 f , the noise variance σ 2 n and the length scales l = {li}n+mi=1 . The feature mapping for this SE kernel can be derived from φω(x̃) = σf√ r [ cos(ωTx̃) sin(ωTx̃) ]T, and ω ∼ N (0,P−1). Assuming the prior distribution of weights of the features w ∼ N (0,Σp), the posterior distribution of ∆x can be derived as in standard Bayesian linear regression\n∆x|X̃,∆X, x̃ ∼ N (wTφ, σ2n(1 + φ TA−1φ)), (3)\nwhere w = A−1Φ∆X, A = ΦΦT+σ2nΣ −1 p , φ = φ(x̃), Φ = {φ(x̃i)}ri=1. Thus the computational complexity becomes O(Nr2 + r3), which is significantly more efficient than GPR with O(N3) time complexity when the number of random features r is much smaller than the number of training samples N . The hyper-parameters can be learned by maximizing the log-likelihood of the training outputs given the inputs using numerical methods such as conjugate gradient [20].\nOn-line model adaptation To update the weights w incrementally given a new sample, we do not store or invert A explicitly. Instead, we keep track of its upper triangular Cholesky factor A = RTR [17]. Given a new sample, a rank-1 update is applied to the Cholesky factor R, which requires O(r2) time. To cope with time-varying systems and to make the method more adaptive, we employ a forgetting factor λ ∈ (0, 1), such that the impact of the previous samples decays exponentially in time [21]. With this weighting criterion, we update A and Φ∆X with a new sample (x̃,∆x) as A← λA + (1− λ)φ(x̃)φ(x̃)T, Φ∆X← λΦ∆X + (1− λ)φ(x̃)∆x Here λ ∈ (0, 1) is the forgetting factor [21], and A and Φ∆X are normalized by the number of offline training points M after offline batch training A ← 1MA, Φ∆X ← 1 MΦ∆X. This convex combination blends the previous samples and the current one."
    }, {
      "heading" : "3.2 Approximate Bayesian inference",
      "text" : "When performing long-term prediction using the SSGP models, the input state-control pair x̃ becomes uncertain. Here we define the joint distribution over state-control pair at one time step as p(x̃) = p(x,u). Thus the distribution over state transition becomes p(∆x) = ∫ ∫ p(f(x̃)|x̃)p(x̃)dfdx̃. This predictive distribution cannot be computed analytically because the nonlinear mapping of an input Gaussian distribution leads to a non-Gaussian predictive distribution. Therefore we need to resort to approximate methods. Deterministic approximate inference methods approximate the posterior with a Gaussian [22, 23, 3], e.g., ∆x = f(x̃) ∼ N (µf ,Σf ). However, these methods scale quadratically with the number of training samples, making them unsuitable for online learning with moderate amounts of data (e.g., 500 or more data points).\nIn this section we present two approximate inference methods based on SSGPs. Our methods offer similar prediction performance compared with related methods in GPs. However, our methods scale\nquadratically with the number of random features, which is usually much less than the number of training samples. In this paper, we assume the conditional independency between different dimensions of ∆x, given x̃, so in the following derivation we use f to denote one of the dimensions. This assumption can be relaxed by using vector-valued kernels."
    }, {
      "heading" : "3.2.1 Exact moment matching (SSGP-EMM)",
      "text" : "Given an input joint distribution N (µ̃, Σ̃), we may compute the exact posterior mean and variance. Applying the law of iterated expectation, the predictive mean µf is evaluated as\nµf = E[f(x̃)|µ̃, Σ̃] = Ex̃[Ef [f(x̃)]] = Ex̃[wTφ(x̃)] = wTq, qi = σf√ r { ec(ωi) i ≤ r es(ωī) i > r\nwhere es(x̃) = exp(− x̃ T ˜Σx̃ 2 ) sin(x̃Tµ̃), ec(x̃) = exp(− x̃ T ˜Σx̃ 2\n) cos(x̃Tµ̃), and ī = mod(i, r) are defined for notation simplicity. Next we compute the predictive variance using the law of total variance\nΣf = Var[f(x̃)|µ̃, Σ̃] = Ex̃[Varf [f(x̃)]] + Varx̃[Ef [f(x̃)]]\n= Ex̃[Varf [f(x̃)]] + Ex̃[Ef [f(x̃)] 2 ]− Ex̃[Ef [f(x̃)]]2\n= Ex̃[σ 2 n(1 + φ(x̃) T A −1 φ(x̃))] + Ex̃[(w T φ(x̃)) 2 ]− µ2f\n= σ 2 n + Tr\n( ( σ\n2 nA −1 + ww T)︸ ︷︷ ︸ S ∫ φ(x̃)φ(x̃) T p(x̃)dx̃︸ ︷︷ ︸\nT\n) − µ2f\nTij = σ2f\n2r ·  +ec(θ1) + ec(θ2) i ≤ r, j ≤ r +es(θ1)− es(θ2) i ≤ r, j > r +es(θ1) + es(θ2) i > r, j ≤ r −ec(θ1) + ec(θ2) i > r, j > r\nθ1 =ωī + ωj̄ θ2 = ωī − ωj̄\nThe covariance between input and prediction can be computed as: Σx̃,f = Cov[x̃, f(x̃)|µ̃, Σ̃] ≈ Covx̃[x̃,Ef [f(x̃)]] = Covx̃[x̃,wTφ(x̃)]\n= Ex̃[(x̃− µ̃)(wTφ(x̃)−wTq)]\n= Ex̃[w T φ(x̃)x̃−wTqx̃−wTφ(x̃)µ̃+ wTqµ̃] = ( 2r∑\ni=1 wi Ex̃[φi(x̃)x̃]︸ ︷︷ ︸ Pi\n) −wTqµ̃−wTEx̃[φ(x̃)]µ̃+ wTqµ̃\n= Pw −wTqµ̃\nPi = σf√ r · { ec(ωi)µ̃− es(ωi)Σ̃ωi i ≤ r es(ωī)µ̃+ ec(ωī)Σ̃ωī i > r\nNext we compute the covariance of two output dimensions (off-diagonal entries in the predictive covariance matrix) f i, f j with uncertain input as follows:\nCov[f i , f j |µ̃, Σ̃] = Ex̃[E[fi]E[fj ]]− Ex̃[E[fi]]Ex̃[E[fj ]]\n= Ex̃[(w iT φ i )(w jT φ j )]− µifµ j f\n= w iT Ex̃[φ i φ jT ]w j − µifµ j f = w iT (∫ φ i φ jT p(x̃)dx̃ ) ︸ ︷︷ ︸\nTij\nw j − µifµ j f\nT ij st =\nσ2f 2r ·  +ec(θ1) + ec(θ2) s ≤ r, t ≤ r +es(θ1)− es(θ2) s ≤ r, t > r +es(θ1) + es(θ2) s > r, t ≤ r −ec(θ1) + ec(θ2) s > r, t > r\nθ1 = ω i s̄ + ω j t̄ θ2 = ω i s̄ − ω j t̄\n(4)\nwhere superscript i denotes the corresponding value related to the ith output, e.g. wi, φi, and µif are the coefficients, features mapping, and predictive mean of the ith output, respectively. See the supplementary material for a detailed derivation of SSGP-EMM."
    }, {
      "heading" : "3.2.2 Linearization (SSGP-Lin)",
      "text" : "Another approach to approximate the predictive distribution under uncertain input is through linearizing of the posterior SSGP mean function. First we derive the partial derivative of the predictive mean Ef [f(x̃)] to the input x̃, and predictive mean’s first-order Taylor expansion around the input mean\n∂Ef [f(x̃)] ∂x̃ = ∂ ( wTφ(x̃) ) ∂x̃ = ∂φ(x̃) ∂x̃ w = D(x̃)w (5)\nwhere Di(x̃) = σf√ r · { −ωi sin(ωTi x̃) i ≤ r +ωī cos(ω T ī x̃) i > r , is the ith column of D(x̃).\nEf [f(x̃)] ≈ Ef [f(µ̃)] + ∂Ef [f(x̃)]\n∂x̃ ∣∣∣∣T x̃=µ̃ ( x̃− µ̃ ) = w T φ(µ̃) + (D(µ̃)w) T (x̃− µ̃) = (D(µ̃)w)T︸ ︷︷ ︸\na(µ̃)T\nx̃ + w T φ(µ̃)− (D(µ̃)w)Tµ̃︸ ︷︷ ︸\nb(µ̃)\nThe predictive mean µf is obtained by evaluating the function at input mean µ̃. More precisely\nµf = E[f(x̃)|µ̃, Σ̃] = Ef [Ex̃[f(x̃)]] ≈ Ef [f(µ̃)|µ̃] = wTφ(µ̃) (6)\nBased on the linearized model, the predictive variance Σf with uncertain input is evaluated using the law of total variance\nΣf = Var[f(x̃)|µ̃, Σ̃] = Varx̃[Ef [f(x̃)]] + Ex̃[Varf [f(x̃)]] ≈ Varx̃[aTx̃ + b] + Varf [f(µ̃)]\n= Varx̃[a Tx̃] + Varf [f(µ̃)] = a TΣ̃ a + σ2n\n( 1 + φ(µ̃)TA−1φ(µ̃) ) (7) The covariance between input and prediction Σx̃,f , and the covariance between two prediction can be computed as\nΣx̃,f = Cov[x̃, f(x̃)|µ̃, Σ̃] ≈ Covx̃[x̃,Ef [f(x̃)]]\n= Covx̃[x̃,a Tx̃ + b]Ex̃[(x̃− µ̃)(aTx̃− aTµ̃)] = Σ̃a\nCov[f i, f j |µ̃, Σ̃] = Ex̃ [( Ef [f i]− Ex̃[Ef [f i]] )( Ef [f j ]− Ex̃[Ef [f j ]] )]\n= Ex̃[a iT(x̃− µ̃) ajT(x̃− µ̃)] = aiTΣ̃aj\nDifferent from the exact moment matching used for SSGP inference, the approach presented here is an approximation of the posterior moments. See Table 1 for a comparison between our methods and exact moment matching approach for GPs (GP-EMM) [22, 23, 3].\nBelief space representation Given the above expressions, we can compute the predictive distribution µk+1,Σk+1 as follows\nµk+1 = µk + µfk Σk+1 = Σk + Σfk + Σx̃k,fk + Σfk,x̃k . (8)\nNote that µfk ,Σk and Σx̃k,fk are nonlinear functions of µk and Σk. We define the belief as\nthe predictive distribution vk = [µk vec(Σk)]T over state xk, where vec(Σk) is the vectorization of Σk. Therefore eq (8) can be written in a compact form\nvk+1 = F(vk,uk), (9)\nwhere F is defined by (8). The above equation corresponds to the belief space representation of the unknown dynamics in eq (1) in discrete-time."
    }, {
      "heading" : "4 Online Probabilistic Trajectory Optimization",
      "text" : "In order to incorporate dynamics model uncertainty explicitly, we perform trajectory optimization in belief space. In our proposed framework, at each iteration we create a local model along a nominal trajectory through the belief space (v̄k, ūk) including i) a linear approximation of the belief dynamics model; ii) a second-order local approximation of the value function. We define the belief and control nominal trajectory (v̄1:H , ū1:H ) and deviations from this trajectory δvk = vk − v̄k, δuk = uk − ūk. The linear approximation of the belief dynamics along the nominal trajectory is\nδvk+1 =  ∂µk+1∂µk ∂µk+1∂Σk ∂Σk+1 ∂µk Σk+1 ∂Σk  δvk + [ ∂µk+1∂uk∂Σk+1 ∂uk ] δuk, (10)\nAll partial derivatives are computed analytically. Based on the dynamic programming principle, the value function is the solution to the Bellman equation\nV (vk, k) = min uk\n( L(vk,uk) + V ( f(vk,uk), k + 1 )︸ ︷︷ ︸ Q(vk,uk) ) . (11)\nThe Q function can be approximated as a quadratic model along the nominal trajectory [6]. The local optimal control law is computed by minimizing the approximated Q function\nδûk = arg min δuk\n[ Qk(vk + δvk,uk + δuk) ] = −(Quuk )−1Quk − (Quuk )−1Quxk δvk. (12)\nwhere superscripts of Q indicate partial derivatives. The new control is obtained as ûk = ūk + δûk, and the quadratic approximation of the value function is propagated backward in time iteratively. See the supplementary material for details regarding the backward propagation.\nWe use the learned control policy to generate a locally optimal trajectory by propagating the belief dynamics forward in time using the proposed approximate inference methods, i.e., SSGP-EMM (3.2.1) or SSGP-Lin (3.2.2). Note that the belief dynamcis model is determinsitc. And the instantaneous cost L(vk,uk) = E[L(xk,uk)|µk,Σk]. In our implementation we apply line search to guarantee convergence to a locally optimal control policy [11]. Control constraints are taken into account using the method in [8]. The summary of algorithm is included in the supplementary material."
    }, {
      "heading" : "4.1 Online optimization: a receding horizon scheme",
      "text" : "The trajectory optimization approach with learned dynamics can be used for episodic RL. However, this requires interactions with the physical system over a long time scale (trajectory), and is not reactive to task or model variations that occur in short time scales (e.g., at every time step). Here we propose an online approach in the spirit of model predictive control (MPC).\nGiven a solution to a single H-step trajectory optimization problem starting at x1, we only apply the first element of the control sequence u1 and proceed to solve another H-step trajectory optimization problem starting at x2. Different from the offline approach, we initialize with the previous optimized trajectory. The H-step nominal control sequence for warm-start becomes u2,u3, ...,uH ,uH . The online optimization would converge much faster than the offline case as long as the new target is not far from the previous one because of the warm-start. In addition, at x2 we collect the training pair {(x1,u1),∆x1} and update the learned SSGP model as introduced in section 3.1. Algorithm 1 Adaptive Probabilistic Trajectory Optimization (1-3: offline learning, 4-8: online learning) 1: Initialization: Collect offline data. 2: Model learning: Train GP hyperparameters. Sample random features and compute their weights (sec.3.1). 3: Trajectory optimization: Perform belief space DDP based on the learned model (algorithm 1 in supple-\nmentary material). 4: repeat 5: Policy execution: Apply one-step control û1 to the system and move one step forward. Record data. 6: Model adaptation: Incorporate data and update random features’ weights w (sec.3.1). 7: Trajectory optimization: Perform re-optimization with the updated model (algorithm 1 in supplementary material). Initialize with the previously optimized policy/trajectory. 8: until Task terminated\nThis online scheme is particularly suitable for applications with task or model variations. In contrast, most state of the art RL methods could not be efficiently applied in those cases. An summary of the algorithm can be found in algorithm 1."
    }, {
      "heading" : "4.2 Relation to prior work",
      "text" : "Our method shares some similarities with methods such as iLQG-LD[12], AGP-iLQR [16], PDDP[4] and Minimax DDP[13] . All of these methods are based on DDP or iLQR and learned dynamics models. Our method differs in both model and controller learning. GP models are more robust to modeling error than LWPR (iLQG-LD [12]) or RFWR (Minimax DDP [13]). But the major obstacle for applying GPs is the high computational demand for performing inference. Our method integrates scalable approximate inference (see section 3.2) into trajectory optimization. In contrast, related methods either employs computation-intensive inference approach (PDDP [4]), or drops uncertainty in the dynamics model (AGP-iLQR [16]), which becomes less robust to modeling errors. Our method also performs fast online model adaptation and re-optimization. These features are essential when we are dealing with 1) condition variations, such as time-varying tasks, dynamics or environment; 2) infinite horizon problems, such as stabilization."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Approximate inference performance",
      "text" : "We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12]. Note that SoR-GP and LWPR do\nnot take into account input uncertainty when performing regressions. We consider two multi-step prediction tasks using the dynamics models of a quadrotor (16 state dimensions, 4 control dimensions) and a Puma-560 manipulator (12 state dimensions, 6 control dimensions).\nAccuracy of multi-step prediction In the following, we evaluate the performance in terms of prediction accuracy. We collected training sets of 1000 and 2000 data points for the quadrotor and puma task, respectively. For model learning we used 100/50 random features for our methods and 100/50 reference points for SoR-GP. Based on the learned models, we used a set of 10 initial states and control sequences to perform rollouts (200 steps for quadrotor and 100 steps for Puma) and compute the cost expectations at each step. Fig.1(a)(b) shows the cost prediction errors, i.e.(L(x̃k)− E[L(x̃k)|µ̃k, Σ̃k])2. It can be seen that SSGP-EMM is very close to GP-EMM and SSGP-EMM performs slightly better than SSGP-Lin in all cases. Since SoR-GP and LWPR do not take into account input uncertainty when performing regression, our methods outperform them consistently.\nComputational efficiency In terms of the computational demand, we tested the CPU time for one-step prediction using SSGP-EMM and SSGP-Lin and full GP-EMM. We used sets of 800 random data points of 1,10,20,30,40,50,60,70,80,90 and 100 dimensions to learn SSGP and GP models. The results are shown in fig.1c,1d. Both SSGP-EMM and SSGP-Lin show significant less computational demand than GP-EMM. In contrast, as shown in the last subsection and fig.1, their prediction performances differences are not substantial. See section 3 for a comparison in terms of computational complexity. Our methods are more scalable than GP-EMM, which is the major computational bottleneck for probabilistic model-based RL approaches [3, 4]."
    }, {
      "heading" : "5.2 Trajectory optimization performance",
      "text" : "We evaluate the performance of our methods using three model predictive control (MPC) tasks.\nPUMA-560 task: moving target and model parameter changes The task is to steer the endeffector to the desired position and orientation. The desired state is time-varying over 800 time steps as shown in fig.2b. We collected 1000 data points offline and sampled 50 random features for both of our methods. Similarly for AGP-iLQR we used 50 reference points. In order to show the effect of online adaptation, we increased the mass of the end-effector by 500% at the beginning of online learning (it is fixed during learning). Fig.2(a) shows the cost reduction results averaged over 3 independent trials. Our method based on SSGP-EMM slightly outperforms SSGP-lin based method. Although iLQG-LD and AGP-iLQR also feature online model adaptation and DDP-based optimization, their controls are computed based on predictions that are more biased than our methods (see comparisons in section 5.1). As a result our methods show superior predictive control performance.\nQuadrotor task: time-varying tasks and dynamics The objective is to start at (-1, 1, 0.5) and track a moving target as shown in fig.2d for 400 steps. The mass of the quadrotor is decreasing at a rate of 0.02 kg/step. The controls are thrust forces of the 4 rotors and we consider the control constraint umin = 0.5,umax = 3. We collected 3000 data points offline, and sampled 100 and 400 features for online learning. The forgetting factor for online learning λ = 0.992. SSGP-Lin was used for approximate inference. Results are shown in fig. 2c. The effect of online model adaptation\nis significant after 100 steps due to the time-varying dynamics. Not surprisingly, increasing the number of features results in better performance. The receding-horizon DDP (RH-DDP) [24] with full knowledge of the dynamics model was used as a baseline.\nAutonomous driving during extreme conditions: steady-state stabilization In this example, we study the control of a wheeled vehicle during extreme operation conditions (powerslide). The task is to stabilize the vehicle to a specified steady-state using purely longitudinal control. The desired steady-state consists of velocity V , side slip angle β, and yaw rate VR where R is the path radius. This problem has been studied in [25] where the authors developed a LQR control scheme based on analytic linearization of the dynamics model. However, this method is restrictive due to the assumption of full knowledge of the complex dynamics model. We applied our method to this task under unknown dynamics with 2500 offline data points, which were sampled from the empirical vehicle model in [25]. We used 50, 150, and 400 random features and SSGP-EMM for approximate inference in our experiments. Results and comparisons with the solution in [25] are shown in fig.3. As can be seen, in the case of 400 random features, our solution is very close to the analytic LQR solution and the system is stabilized after 30 time steps. With only 50 features, our method is sill able to stabilize the system after 300 time steps. Our method is suitable for infinite horizon control tasks due to its feature of efficient learning and online optimization."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper presents a trajectory optimization framework for solving optimal control problems under uncertain dynamics. Similar to RL, our method can efficiently learn from experience and adapt to new situations. Different from most RL algorithms, our method updates control policy and dynamics\nmodel in an online incremental fashion under tasks and dynamics variations. In order to perform robust and fast planning, we have introduced two scalable approximate inference methods. Both methods have demonstrated superior performance in terms of computational efficiency and longterm prediction accuracy compared to well-known methods. Our adaptive probabilistic trajectory optimization framework combines the benefits of efficient inference and model predictive control (MPC), and is applicable to a wide range of learning and control problems. Future work will include 1) partially observable learning and 2) transfer learning using samples from other models."
    } ],
    "references" : [ {
      "title" : "A survey on policy search for robotics",
      "author" : [ "M.P. Deisenroth", "G. Neumann", "J. Peters" ],
      "venue" : "Foundations and Trends in Robotics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "A comparison of direct and model-based reinforcement learning",
      "author" : [ "C.G. Atkeson", "J.C. Santamaria" ],
      "venue" : "In In International Conference on Robotics and Automation. Citeseer,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1997
    }, {
      "title" : "Gaussian processes for data-efficient learning in robotics and control",
      "author" : [ "M. Deisenroth", "D. Fox", "C. Rasmussen" ],
      "venue" : "IEEE Transsactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Probabilistic differential dynamic programming",
      "author" : [ "Y. Pan", "E. Theodorou" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Model-based contextual policy search for data-efficient generalization of robot skills",
      "author" : [ "A. Kupcsik", "M.P. Deisenroth", "J. Peters", "AP Loh", "P. Vadakkepat", "G. Neumann" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Differential dynamic programming",
      "author" : [ "D. Jacobson", "D. Mayne" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1970
    }, {
      "title" : "An application of reinforcement learning to aerobatic helicopter",
      "author" : [ "P. Abbeel", "A. Coates", "M. Quigley", "A. Y Ng" ],
      "venue" : "flight. NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Control-limited differential dynamic programming",
      "author" : [ "Y. Tassa", "N. Mansard", "E. Todorov" ],
      "venue" : "ICRA",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Learning neural network policies with guided policy search under unknown dynamics",
      "author" : [ "S. Levine", "P. Abbeel" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Sparse spectrum gaussian process regression",
      "author" : [ "M. Lázaro-Gredilla", "J. Quiñonero-Candela", "C.E. Rasmussen", "A.R. Figueiras-Vidal" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "A generalized iterative lqg method for locally-optimal feedback control of constrained nonlinear stochastic systems",
      "author" : [ "E. Todorov", "W. Li" ],
      "venue" : "In American Control Conference,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "Adaptive optimal feedback control with learned internal dynamics models",
      "author" : [ "D. Mitrovic", "S. Klanke", "S. Vijayakumar" ],
      "venue" : "In From Motor Learning to Interaction Learning in Robots,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Minimax differential dynamic programming: An application to robust biped walking",
      "author" : [ "J. Morimoto", "CG Atkeson" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Incremental online learning in high dimensions",
      "author" : [ "Sethu Vijayakumar", "Aaron D’souza", "Stefan Schaal" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "Constructive incremental learning from only local information",
      "author" : [ "Stefan Schaal", "Christopher G Atkeson" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1998
    }, {
      "title" : "Approximate real-time optimal control based on sparse gaussian process models",
      "author" : [ "J. Boedecker", "JT. Springenberg", "J. Wulfing", "M. Riedmiller" ],
      "venue" : "ADPRL",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Real-time model learning using incremental sparse spectrum gaussian process regression",
      "author" : [ "A. Gijsberts", "G. Metta" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Fourier analysis on groups",
      "author" : [ "W. Rudin" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1962
    }, {
      "title" : "Gaussian processes for machine learning",
      "author" : [ "C.K.I Williams", "C.E. Rasmussen" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "Propagation of uncertainty in bayesian kernel models-application to multiple-step ahead forecasting",
      "author" : [ "J. Quinonero Candela", "A. Girard", "J. Larsen", "C.E. Rasmussen" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2003
    }, {
      "title" : "Gaussian process priors with uncertain inputs application to multiple-step ahead time series forecasting",
      "author" : [ "A. Girard", "C.E. Rasmussen", "J. Quinonero-Candela", "R. Murray-Smith" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2003
    }, {
      "title" : "Receding horizon differential dynamic programming",
      "author" : [ "Y. Tassa", "T. Erez", "W.D. Smart" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2007
    }, {
      "title" : "Steady-state cornering equilibria and stabilisation for a vehicle during extreme operating conditions",
      "author" : [ "E. Velenis", "E. Frazzoli", "P. Tsiotras" ],
      "venue" : "International Journal of Vehicle Autonomous Systems,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "While model-free RL has demonstrated promising results [1], it typically requires human expert demonstrations or relies on lots of direct interactions with physical systems.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "Model-based RL was developed to address the issue of sample inefficiency by learning dynamics models explicitly from data, which can provide better generalization [2, 1].",
      "startOffset" : 163,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "Model-based RL was developed to address the issue of sample inefficiency by learning dynamics models explicitly from data, which can provide better generalization [2, 1].",
      "startOffset" : 163,
      "endOffset" : 169
    }, {
      "referenceID" : 2,
      "context" : "Recent probabilistic model-based RL methods overcome this issue, achieving state-of-the-art performance [3–5].",
      "startOffset" : 104,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "Recent probabilistic model-based RL methods overcome this issue, achieving state-of-the-art performance [3–5].",
      "startOffset" : 104,
      "endOffset" : 109
    }, {
      "referenceID" : 4,
      "context" : "Recent probabilistic model-based RL methods overcome this issue, achieving state-of-the-art performance [3–5].",
      "startOffset" : 104,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "More precisely, we propose a method that relies on local trajectory optimization such as DDP [6], which is scalable and has been shown to work well on challenging learning control tasks in robotics [7–9].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "More precisely, we propose a method that relies on local trajectory optimization such as DDP [6], which is scalable and has been shown to work well on challenging learning control tasks in robotics [7–9].",
      "startOffset" : 198,
      "endOffset" : 203
    }, {
      "referenceID" : 7,
      "context" : "More precisely, we propose a method that relies on local trajectory optimization such as DDP [6], which is scalable and has been shown to work well on challenging learning control tasks in robotics [7–9].",
      "startOffset" : 198,
      "endOffset" : 203
    }, {
      "referenceID" : 8,
      "context" : "More precisely, we propose a method that relies on local trajectory optimization such as DDP [6], which is scalable and has been shown to work well on challenging learning control tasks in robotics [7–9].",
      "startOffset" : 198,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "In previous GP-related RL methods, approximate inference is the major computational bottleneck [3–5].",
      "startOffset" : 95,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "In previous GP-related RL methods, approximate inference is the major computational bottleneck [3–5].",
      "startOffset" : 95,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "In previous GP-related RL methods, approximate inference is the major computational bottleneck [3–5].",
      "startOffset" : 95,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "Therefore, we develop two novel scalable approximate inference algorithms based on Sparse Spectrum Gaussian Processes (SSGPs) [10].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "In DDP, and related methods such as iLQG [11], a local model is constructed based on i) a first or second-order linear approximation of the dynamics model; ii) a second-order local approximation of the value function along a nominal trajectory.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "DDP-related methods have been widely used for solving control problem in robotics tasks [7, 8].",
      "startOffset" : 88,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : "DDP-related methods have been widely used for solving control problem in robotics tasks [7, 8].",
      "startOffset" : 88,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "In iLQG-LD [12] and Minimax DDP[13], the dynamics are learned using Locally Weighted Projection Regression (LWPR) [14] and Receptive Field Weighted Regression (RFWR) [15].",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 12,
      "context" : "In iLQG-LD [12] and Minimax DDP[13], the dynamics are learned using Locally Weighted Projection Regression (LWPR) [14] and Receptive Field Weighted Regression (RFWR) [15].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 13,
      "context" : "In iLQG-LD [12] and Minimax DDP[13], the dynamics are learned using Locally Weighted Projection Regression (LWPR) [14] and Receptive Field Weighted Regression (RFWR) [15].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "In iLQG-LD [12] and Minimax DDP[13], the dynamics are learned using Locally Weighted Projection Regression (LWPR) [14] and Receptive Field Weighted Regression (RFWR) [15].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 3,
      "context" : "Recently, probabilistic methods such as PDDP [4] and AGP-iLQR [16] have demonstrated impressive data efficiency with Gaussian process (GP) dynamics models.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "Recently, probabilistic methods such as PDDP [4] and AGP-iLQR [16] have demonstrated impressive data efficiency with Gaussian process (GP) dynamics models.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "However, GP inference in PDDP [4] is too computationally expensive for online optimization and incremental updates.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "AGP-iLQR [16] employs subset of regression (SOR) approximations that feature faster GP inference and incremental model adaptation, but neglects the predictive uncertainty when performing multi-step predictions using the learned forward dynamics.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 9,
      "context" : "In order to perform efficient and robust trajectory optimization, we introduce a learning and inference scheme based on Sparse Spectrum Gaussian Processes (SSGPs) [10, 17].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 16,
      "context" : "In order to perform efficient and robust trajectory optimization, we introduce a learning and inference scheme based on Sparse Spectrum Gaussian Processes (SSGPs) [10, 17].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 2,
      "context" : "can be leveraged to learn the dynamics model [3–5].",
      "startOffset" : 45,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "can be leveraged to learn the dynamics model [3–5].",
      "startOffset" : 45,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "can be leveraged to learn the dynamics model [3–5].",
      "startOffset" : 45,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "Sparse Spectrum GP Regression (SSGPR) SSGP [10] is a recent approach that provides a principled approximation of GPR by employing a random Fourier feature approximation of the kernel function [18].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "Sparse Spectrum GP Regression (SSGPR) SSGP [10] is a recent approach that provides a principled approximation of GPR by employing a random Fourier feature approximation of the kernel function [18].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 18,
      "context" : "Based on Bochner’s theorem [19], any shift-invariant kernel functions can be represented as the Fourier transform of a unique measure k(x̃i − x̃j) = ∫ Rn+m e iωT(x̃i−x̃j)p(ω)dω = Eω[φω(x̃i) φω(x̃j)].",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "We consider the popular Squared Exponential (SE) covariance function with Automatic Relevance Determination (ARD) distance measure as it has been applied successfully in learning dynamics and optimal control [17, 16], k(x̃i, x̃j) = σ f exp(− 1 2 (x̃i − x̃j) P(x̃i − x̃j)), where P = diag({l i } n+m i=1 ).",
      "startOffset" : 208,
      "endOffset" : 216
    }, {
      "referenceID" : 15,
      "context" : "We consider the popular Squared Exponential (SE) covariance function with Automatic Relevance Determination (ARD) distance measure as it has been applied successfully in learning dynamics and optimal control [17, 16], k(x̃i, x̃j) = σ f exp(− 1 2 (x̃i − x̃j) P(x̃i − x̃j)), where P = diag({l i } n+m i=1 ).",
      "startOffset" : 208,
      "endOffset" : 216
    }, {
      "referenceID" : 19,
      "context" : "The hyper-parameters can be learned by maximizing the log-likelihood of the training outputs given the inputs using numerical methods such as conjugate gradient [20].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 16,
      "context" : "Instead, we keep track of its upper triangular Cholesky factor A = RR [17].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 20,
      "context" : "Deterministic approximate inference methods approximate the posterior with a Gaussian [22, 23, 3], e.",
      "startOffset" : 86,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "Deterministic approximate inference methods approximate the posterior with a Gaussian [22, 23, 3], e.",
      "startOffset" : 86,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "Deterministic approximate inference methods approximate the posterior with a Gaussian [22, 23, 3], e.",
      "startOffset" : 86,
      "endOffset" : 97
    }, {
      "referenceID" : 20,
      "context" : "See Table 1 for a comparison between our methods and exact moment matching approach for GPs (GP-EMM) [22, 23, 3].",
      "startOffset" : 101,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "See Table 1 for a comparison between our methods and exact moment matching approach for GPs (GP-EMM) [22, 23, 3].",
      "startOffset" : 101,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "See Table 1 for a comparison between our methods and exact moment matching approach for GPs (GP-EMM) [22, 23, 3].",
      "startOffset" : 101,
      "endOffset" : 112
    }, {
      "referenceID" : 20,
      "context" : "Computational complexity GP-EMM [22, 23, 3] O ( Nn(n+m) )",
      "startOffset" : 32,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : "Computational complexity GP-EMM [22, 23, 3] O ( Nn(n+m) )",
      "startOffset" : 32,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "Computational complexity GP-EMM [22, 23, 3] O ( Nn(n+m) )",
      "startOffset" : 32,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "The Q function can be approximated as a quadratic model along the nominal trajectory [6].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 10,
      "context" : "In our implementation we apply line search to guarantee convergence to a locally optimal control policy [11].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "Control constraints are taken into account using the method in [8].",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "2 Relation to prior work Our method shares some similarities with methods such as iLQG-LD[12], AGP-iLQR [16], PDDP[4] and Minimax DDP[13] .",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 15,
      "context" : "2 Relation to prior work Our method shares some similarities with methods such as iLQG-LD[12], AGP-iLQR [16], PDDP[4] and Minimax DDP[13] .",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : "2 Relation to prior work Our method shares some similarities with methods such as iLQG-LD[12], AGP-iLQR [16], PDDP[4] and Minimax DDP[13] .",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "2 Relation to prior work Our method shares some similarities with methods such as iLQG-LD[12], AGP-iLQR [16], PDDP[4] and Minimax DDP[13] .",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "GP models are more robust to modeling error than LWPR (iLQG-LD [12]) or RFWR (Minimax DDP [13]).",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "GP models are more robust to modeling error than LWPR (iLQG-LD [12]) or RFWR (Minimax DDP [13]).",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "In contrast, related methods either employs computation-intensive inference approach (PDDP [4]), or drops uncertainty in the dynamics model (AGP-iLQR [16]), which becomes less robust to modeling errors.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "In contrast, related methods either employs computation-intensive inference approach (PDDP [4]), or drops uncertainty in the dynamics model (AGP-iLQR [16]), which becomes less robust to modeling errors.",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 20,
      "context" : "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 21,
      "context" : "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 19,
      "context" : "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 15,
      "context" : "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 13,
      "context" : "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 11,
      "context" : "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].",
      "startOffset" : 278,
      "endOffset" : 282
    }, {
      "referenceID" : 2,
      "context" : "Our methods are more scalable than GP-EMM, which is the major computational bottleneck for probabilistic model-based RL approaches [3, 4].",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "Our methods are more scalable than GP-EMM, which is the major computational bottleneck for probabilistic model-based RL approaches [3, 4].",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 22,
      "context" : "The receding-horizon DDP (RH-DDP) [24] with full knowledge of the dynamics model was used as a baseline.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 23,
      "context" : "This problem has been studied in [25] where the authors developed a LQR control scheme based on analytic linearization of the dynamics model.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 23,
      "context" : "We applied our method to this task under unknown dynamics with 2500 offline data points, which were sampled from the empirical vehicle model in [25].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 23,
      "context" : "Results and comparisons with the solution in [25] are shown in fig.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 23,
      "context" : "Blue lines are the analytic LQR solution in [25]",
      "startOffset" : 44,
      "endOffset" : 48
    } ],
    "year" : 2017,
    "abstractText" : "Robotic systems must be able to quickly and robustly make decisions when operating in uncertain and dynamic environments. While Reinforcement Learning (RL) can be used to compute optimal policies with little prior knowledge about the environment, it suffers from slow convergence. An alternative approach is Model Predictive Control (MPC), which optimizes policies quickly, but also requires accurate models of the system dynamics and environment. In this paper we propose a new approach, adaptive probabilistic trajectory optimization, that combines the benefits of RL and MPC. Our method uses scalable approximate inference to learn and updates probabilistic models in an online incremental fashion while also computing optimal control policies via successive local approximations. We present two variations of our algorithm based on the Sparse Spectrum Gaussian Process (SSGP) model, and we test our algorithm on three learning tasks, demonstrating the effectiveness and efficiency of our approach.",
    "creator" : "LaTeX with hyperref package"
  }
}