{
  "name" : "1705.07041.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Posterior sampling for reinforcement learning: worst-case regret bounds",
    "authors" : [ "Shipra Agrawal" ],
    "emails" : [ "sa3305@columbia.edu", "rqj2000@columbia.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n07 04\n1v 1\n[ cs\n.L G\n] 1\n9 M\nay 2\neter. Our main result is a high probability regret upper bound of Õ(D √ SAT ) for any communicating MDP with S states, A actions and diameter D, when T ≥ S5A. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon T . This result improves over the best previously known upper bound of Õ(DS √ AT ) achieved by any algorithm in this setting, and matches the dependence on S in the established lower bound of Ω( √ DSAT ) for this problem. Our techniques involve proving some novel results about the anticoncentration of Dirichlet distribution, which may be of independent interest."
    }, {
      "heading" : "1 Introduction",
      "text" : "Reinforcement Learning (RL) refers to the problem of learning and planning in sequential decision making systems when the underlying system dynamics are unknown, and may need to be learned by trying out different options and observing their outcomes. A typical model for the sequential decision making problem is a Markov Decision Process (MDP), which proceeds in discrete time steps. At each time step, the system is in some state s, and the decision maker may take any available action a to obtain a (possibly stochastic) reward. The system then transitions to the next state according to a fixed state transition distribution. The reward and the next state depend on the current state s and the action a, but are independent of all the previous states and actions. In the reinforcement learning problem, the underlying state transition distributions and/or reward distributions are unknown, and need to be learned using the observed rewards and state transitions, while aiming to maximize the cumulative reward. This requires the algorithm to manage the tradeoff between exploration vs. exploitation, i.e., exploring different actions in different states in order to learn the model more accurately vs. taking actions that currently seem to be reward maximizing.\nExploration-exploitation tradeoff has been studied extensively in the context of stochastic multiarmed bandit (MAB) problems, which are essentially MDPs with a single state. The performance of MAB algorithms is typically measured through regret, which compares the total reward obtained by the algorithm to the total expected reward of an optimal action. Optimal regret bounds have been established for many variations of MAB (see Bubeck et al. [2012] for a survey), with a large majority of results obtained using the Upper Confidence Bound (UCB) algorithm, or more generally, the optimism in the face of uncertainty principle. Under this principle, the learning algorithm maintains tight over-estimates (or optimistic estimates) of the expected rewards for individual actions, and at any given step, picks the action with the highest optimistic estimate. More recently, posterior sampling, aka Thompson Sampling [Thompson, 1933], has emerged as another popular algorithm design principle in MAB, owing its popularity to a simple and extendible algorithmic structure, an attractive empirical performance [Chapelle and Li, 2011, Kaufmann et al., 2012], as well as prov-\nably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value.\nWe consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s′ using an appropriate policy, for each pair s, s′. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of Õ(DS √ AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D. Jaksch et al. [2010] also established a worst-case lower bound of Ω( √ DSAT ) on the regret of any algorithm for this problem.\nOur main contribution is a posterior sampling based algorithm with a high probability worst-case regret upper bound of Õ(D √ SAT + DS7/4A3/4T 1/4), which is Õ(D √ SAT ) when T ≥ S5A. This improves the previously best known upper bound for this problem by a factor of √ S, and matches the dependence on S in the lower bound, for large enough T .\nOur algorithm uses an ‘optimistic version’ of the posterior sampling heuristic, while utilizing several ideas from the algorithm design structure in Jaksch et al. [2010], such as an epoch based execution and the extended MDP construction. The algorithm proceeds in epochs, where in the beginning of\nevery epoch, it generates ψ = Õ(S) sample transition probability vectors from a posterior distribution for every state and action, and solves an extended MDP with ψA actions and S states formed using these samples. The optimal policy computed for this extended MDP is used throughout the epoch. Posterior Sampling for Reinforcement Learning (PSRL) approach has been used previously in Osband et al. [2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1\nWe should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of Õ( √ HSAT ) when T ≥ H3S3A. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm’s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper.\nAmong other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al. [2003], Asmuth et al. [2009], Dann and Brunskill [2015]. There, the aim is to bound the performance of the policy\n1Worst-case regret is a strictly stronger notion of regret in case the reward distribution function is known and only the transition probability distribution is unknown, as we will assume here for the most part. In case of unknown reward distribution, extending our worst-case regret bounds would require an assumption of bounded rewards, where as the Bayesian regret bounds in the above-mentioned literature allow more general (known) priors on the reward distributions with possibly unbounded support. Bayesian regret bounds in those more general settings are incomparable to the worst-case regret bounds presented here.\nlearned at the end of the learning horizon, and not the performance during learning as quantified by regret. Strehl and Littman [2005], Strehl and Littman [2008] provide an optimistic algorithm for bounding regret in a discounted reward setting, but the definition of regret is slightly different in that it measures the difference between the rewards of an optimal policy and the rewards of the learning algorithm along the trajectory taken by the learning algorithm."
    }, {
      "heading" : "2 Preliminaries and Problem Definition",
      "text" : ""
    }, {
      "heading" : "2.1 Markov Decision Process (MDP)",
      "text" : "We consider a Markov Decision Process M defined by tuple {S,A, P, r, s1}, where S is a finite state-space of size S, A is a finite action-space of size A, P : S ×A → ∆S is the transition model, r : S × A → [0, 1] is the reward function, and s1 is the starting state. When an action a ∈ A is taken in a state s ∈ S, a reward rs,a is generated and the system transitions to the next state s′ ∈ S with probability Ps,a(s ′), where ∑ s′∈S Ps,a(s ′) = 1.\nWe consider ‘communicating’ MDPs with finite ‘diameter’. Below we define communicating MDPs, and recall some useful known results for such MDPs.\nDefinition 1 (Policy). A deterministic policy π : S → A is a mapping from state space to action space.\nDefinition 2 (Diameter D(M)). Diameter D(M) of an MDP M is defined as the minimum time required to go from one state to another in the MDP using some deterministic policy:\nD(M) = max s6=s′,s,s′∈S min π:S→A T πs→s′ ,\nwhere T πs→s′ is the expected number of steps it takes to reach state s ′ when starting from state s and using policy π.\nDefinition 3 (Communicating MDP). An MDP M is communicating if and only if it has a finite diameter. That is, for any two states s 6= s′, there exists a policy π such that the expected number of steps to reach s′ from s, T πs→s′ , is at mostD, for some finiteD ≥ 0. Definition 4 (Gain of a policy). The gain of a policy π, from starting state s1 = s, is defined as the infinite horizon undiscounted average reward, given by\nλπ(s) = E[ lim T→∞\n1\nT\nT ∑\ni=1\nrst,π(st)|s1 = s].\nwhere st is the state reached at time t. Lemma 2.1 (Optimal gain for communicating MDPs). For a communicating MDP M with diameter D:\n(a) (Puterman [2014] Theorem 8.1.2, Theorem 8.3.2) The optimal (maximum) gain λ∗ is state independent and is achieved by a deterministic stationary policy π∗, i.e., there exists a deterministic policy π∗ such that\nλ∗ := max s′∈S max π\nλπ(s′) = λπ ∗ (s), ∀s ∈ S.\nHere, π∗ is referred to as an optimal policy for MDP M. (b) (Tewari and Bartlett [2008], Theorem 4) The optimal gain λ∗ satisfies the following equa-\ntions, λ∗ = min\nh∈RS max s,a rs,a + P T s,ah− hs = maxa rs,a + P T s,ah ∗ − h∗s, ∀s (1)\nwhere h∗, referred to as the bias vector of MDP M, satisfies: max\ns h∗s −min s h∗s ≤ D.\nGiven the above definitions and results, we can now define the reinforcement learning problem studied in this paper."
    }, {
      "heading" : "2.2 The reinforcement learning problem",
      "text" : "The reinforcement learning problem proceeds in rounds t = 1, . . . , T . The learning agent starts from a state s1 at round t = 1. In the beginning of every round t, the agent takes an action at ∈ A and observes the reward rst,at as well as the next state st+1 ∼ Pst,at , where r and P are the reward function and the transition model, respectively, for a communicating MDP M with diameterD. The learning agent knows the state-space S, the action space A, as well as the rewards rs,a, ∀s ∈ S, a ∈ A, for the underlying MDP, but not the transition model P or the diameterD. (The assumption of known and deterministic rewards has been made here only for simplicity of exposition, since the unknown transition model is the main source of difficulty in this problem. Our algorithm and results can be extended to bounded stochastic rewards with unknown distributions using standard Thompson Sampling for MAB, e.g., using the techniques in Agrawal and Goyal [2013b].)\nThe agent can use the past observations to learn the underlyingMDPmodel and decide future actions. The goal is to maximize the total reward ∑T\nt=1 rst,at , or equivalently, minimize the total regret over a time horizon T , defined as R(T,M) := Tλ∗ −∑Tt=1 rst,at (2) where λ∗ is the optimal gain of MDP M. We present an algorithm for the learning agent with a near-optimal upper bound on the regret R(T,M) for any communicating MDP M with diameter D, thus bounding the worst-case regret over this class of MDPs."
    }, {
      "heading" : "3 Algorithm Description",
      "text" : "Our algorithm combines the ideas of Posterior sampling (aka Thompson Sampling) with the extended MDP construction used in Jaksch et al. [2010]. Below we describe the main components of our algorithm.\nSome notations: N ts,a denotes the total number of times the algorithm visited state s and played action a until before time t, and N ts,a(i) denotes the number of time steps among these N t s,a steps where the next state was i, i.e., a transition from state s to i was observed. We index the states from 1 to S, so that\n∑S i=1 N t s,a(i) = N t s,a for any t. We use the symbol 1 to denote the vector of all 1s,\nand 1i to denote the vector with 1 at the i th coordinate and 0 elsewhere.\nDoubling epochs: Our algorithm uses the epoch based execution framework of Jaksch et al. [2010]. An epoch is a group of consecutive rounds. The rounds t = 1, . . . , T are broken into consecutive epochs as follows: the kth epoch begins at the round τk immediately after the end of (k− 1)th epoch and ends at the first round τ such that for some state-action pair s, a, N τs,a ≥ 2N τks,a. The algorithm computes a new policy π̃k at the beginning of every epoch k, and uses that policy through all the rounds in that epoch. It is easy to observe that irrespective of how the policy π̃k is computed, the number of epochs in T rounds is bounded by SA log(T ).\nPosterior Sampling: We use posterior sampling to compute the policy π̃k in the beginning of every epoch. Dirichlet distribution is a convenient choice maintaining posteriors for the transition probability vectors Ps,a for every s ∈ S, a ∈ A, as they satisfy the following useful property: given a prior Dirichlet(α1, . . . , αS) on Ps,a, after observing a transition from state s to i (with underlying probability Ps,a(i)), the posterior distribution is given by Dirichlet(α1, . . . , αi+1, . . . , αS). By this property, for any s ∈ S, a ∈ A, on starting from prior Dirichlet(1) for Ps,a, the posterior at time t is Dirichlet({N ts,a(i) + 1}i=1,...,S). Our algorithm uses a modified, optimistic version of this approach. At the beginning of every epoch k, for every s ∈ S, a ∈ A such thatNs,a ≥ η, it generates multiple samples forPs,a from a ‘boosted’ posterior. Specifically, it generates ψ = O(S log(SA/ρ)) independent sample probability vectors Q1,ks,a, . . . , Q ψ,k s,a as Qj,ks,a ∼ Dirichlet(Mτks,a), whereMts,a denotes the vector [M t s,a(i)]i=1,...,S , with\nM ts,a(i) := 1 κ (N t s,a(i) + ω), for i = 1, . . . , S. (3)\nHere, κ = O(log(T/ρ)), ω = O(log(T/ρ)), η = √\nTS A + 12ωS 2, and ρ ∈ (0, 1) is a parameter of the algorithm. In the regret analysis, we derive sufficiently large constants that can be used in the definition of ψ, κ, ω to guarantee the bounds. However, no attempt has been made to optimize those constants, and it is likely that much smaller constants suffice.\nFor every remaining s, a, i.e., those with small Ns,a (Ns,a < η) the algorithm use a simple optimistic sampling described in Algorithm 1. This special sampling for s, a with small Ns,a has been introduced to handle a technical difficulty in analyzing the anti-concentration of Dirichlet posteriors when the parameters are very small. We suspect that with an improved analysis, this may not be required.\nExtended MDP: The policy π̃k to be used in epoch k is computed as the optimal policy of an extended MDP M̃k defined by the sampled transition probability vectors, using the construction of Jaksch et al. [2010]. Given sampled vectors Qj,ks,a, j = 1, . . . , ψ, for every state-action pair s, a, we define extended MDP M̃k by extending the original action space as follows: for every s, a, create ψ actions for every action a ∈ A, denoting by aj the action corresponding to action a and sample j; then, in MDP M̃k, on taking action aj in state s, reward is rs,a but transitions to the next state follows the transition probability vectorQj,ks,a.\nNote that the algorithm uses the optimal policy π̃k of extended MDP M̃k to take actions in the action space A which is technically different from the action space of MDP M̃k, where the policy π̃k is defined. We slightly abuse the notation to say that the algorithm takes action at = π̃(st) to mean that the algorithm takes action at = a ∈ A when π̃k(st) = aj for some j. Our algorithm is summarized as Algorithm 1."
    }, {
      "heading" : "4 Regret Bounds",
      "text" : "We prove the following bound on the regret of Algorithm 1 for the reinforcement learning problem. Theorem 1. For any communicating MDP M with S states, A actions, and diameter D, with probability 1− ρ. the regret of Algorithm 1 in time T ≥ CDA log2(T/ρ) is bounded as:\nR(T,M) ≤ Õ ( D √ SAT +DS7/4A3/4T 1/4 +DS5/2A )\nwhere C is an absolute constant. For T ≥ S5A, this implies a regret bound of\nR(T,M) ≤ Õ ( D √ SAT ) .\nHere Õ hides logarithmic factors in S,A, T, ρ and absolute constants.\nThe rest of this section is devoted to proving the above theorem. Here, we provide a sketch of the proof and discuss some of the key lemmas, all missing details are provided in the supplementary material."
    }, {
      "heading" : "4.1 Proof of Theorem 1",
      "text" : "As defined in Section 2, regret R(T,M) is given by R(T,M) = Tλ∗ − ∑Tt=1 rst,at , where λ∗ is the optimal gain of MDP M, at is the action taken and st is the state reached by the algorithm at time t. Algorithm 1 proceeds in epochs k = 1, 2, . . . ,K , where K ≤ SA log(T ). To bound its regret in time T , we first analyze the regret in each epoch k, namely,\nRk := (τk+1 − τk)λ∗ − ∑τk+1−1\nt=τk rst,at ,\nand boundRk by roughly D ∑\ns,a\nN τk+1 s,a −N τks,a √\nN τks,a\nwhere, by definition, for every s, a, (N τk+1 s,a − N τks,a) is the number of times this state-action pair is visited in epoch k. The proof of this bound has two main components:\nAlgorithm 1 A posterior sampling based algorithm for the reinforcement learning problem\nInputs: State space S, Action space A, starting state s1, reward function r, time horizon T , parameters ρ ∈ (0, 1], ψ = O(S log(SA/ρ)), ω = O(log(T/ρ)), κ = O(log(T/ρ)), η = √\nTS A + 12ωS 2.\nInitialize: τ1 := 1,Mτ1s,a = ω1.\nfor all epochs k = 1, 2, . . . , do\nSample transition probability vectors: For each s, a, generate ψ independent sample probability vectors Qj,ks,a, j = 1, . . . , ψ, as follows: • (Posterior sampling): For s, a such that N τks,a ≥ η, use samples from the Dirichlet\ndistribution: Qj,ks,a ∼ Dirichlet(Mτks,a),\n• (Simple optimistic sampling): For remaining s, a, with N τks,a < η, use the following simple optimistic sampling: let\nP−s,a = P̂s,a −∆,\nwhere P̂s,a(i) = N\nτk s,a(i) N τk s,a , and∆i = min\n{√\n3P̂s,a(i) log(4S)\nN τk s,a\n+ 3 log(4S) N τk s,a , P̂s,a(i)\n}\n, and\nlet z be a random vector picked uniformly at random from {11, . . . ,1S}; set\nQj,ks,a = P − s,a + (1− ∑S i=1 P − s,a(i))z.\nCompute policy π̃k: as the optimal gain policy for extended MDP M̃k constructed using sample set {Qj,ks,a, j = 1, . . . , ψ, s ∈ S, a ∈ A}. Execute policy π̃k: for all time steps t = τk, τk + 1, . . . , until break epoch do Play action at = π̃k(st). Observe the transition to the next state st+1. Set N t+1s,a (i),M t+1 s,a (i) for all a ∈ A, s, i ∈ S as defined (refer to Equation (3)).\nIf N t+1st,at ≥ 2N τkst,at , then set τk+1 = t+ 1 and break epoch. end for\nend for\n(a) Optimism: The policy π̃k used by the algorithm in epoch k is computed as an optimal gain policy of the extendedMDP M̃k. The first part of the proof is to show that with high probability, the extended MDP M̃k is (i) a communicating MDP with diameter at most 2D, and (ii) optimistic, i.e., has optimal gain at least (close to) λ∗. Part (i) is stated as Lemma 4.1, with a proof\nprovided in the supplementary material. Now, let λ̃k be the optimal gain of the extended MDP M̃k. In Lemma 4.2, which forms one of the main novel technical components of our proof, we show that with probability 1− ρ,\nλ̃k ≥ λ∗ − Õ(D √ SA T ).\nWe first show that above holds if for every s, a, there exists a sample transition probability vector whose projection on a fixed unknown vector (h∗) is optimistic. Then, in Lemma 4.3 we prove this optimism by deriving a fundamental new result on the anti-concentration of any fixed projection of a Dirichlet random vector (Proposition A.1 in the supplementary material). Substituting this upper bound on λ∗, we have the following bound onRkwith probability 1− ρ:\nRk ≤ ∑τk+1−1\nt=τk\n( λ̃k − rst,at + Õ(D √ SA T ) ) . (4)\n(b) Deviation bounds: Optimism guarantees that with high probability, the optimal gain λ̃k for\nMDP M̃k is at least λ∗. And, by definition of π̃k, λ̃k is the gain of the chosen policy π̃k\nfor MDP M̃k. However, the algorithm executes this policy on the true MDP M. The only difference between the two is the transition model: on taking an action aj := π̃k(s) in state s in MDP M̃k, the next state follows the sampled distribution P̃s,a := Q j,k s,a, (5) where as on taking the corresponding action a in MDPM, the next state follows the distribution Ps,a. The next step is to bound the difference between λ̃k and the average reward obtained by the algorithm by bounding the deviation (P̃s,a − Ps,a). This line of argument bears similarities to the analysis of UCRL2 in Jaksch et al. [2010], but with tighter deviation bounds that we are able to guarantee due to the use of posterior sampling instead of deterministic optimistic bias\nused in UCRL2. Now, since at = π̃k(st), using the relation between the gain λ̃k , the bias vector h̃, and reward vector of optimal policy π̃k for communicating MDP M̃k (refer to Lemma 2.1) ∑τk+1−1\nt=τk\n( λ̃− rst,at ) = ∑τk+1−1 t=τk (P̃st,at − 1st)T h̃\n= ∑τk+1−1 t=τk (P̃st,at − Pst,at + Pst,at − 1st)T h̃ (6)\nwhere with high probability, h̃ ∈ RS , the bias vector of MDP M̃k satisfies maxs h̃s −mins h̃s ≤ D(M̃k) ≤ 2D (refer to Lemma 4.1).\nNext, we bound the deviation (P̃s,a − Ps,a)T h̃ for all s, a, to bound the first term in above. Note that h̃ is random and can be arbitrarily correlated with P̃ , therefore, we need to bound maxh∈[0,2D]S(P̃s,a − Ps,a)Th. (For the above term, w.l.o.g. we can assume h̃ ∈ [0, 2D]S). For s, a such thatN τks,a > η, P̃s,a = Q j,k s,a is a sample from the Dirichlet posterior. In Lemma 4.4, we show that with high probability,\nmax h∈[0,2D]S\n(P̃ ks,a − Ps,a)Th ≤ Õ( D √\nN τks,a +\nDS N τks,a ). (7)\nThis bound is an improvement by a √ S factor over the corresponding deviation bound obtainable for the optimistic estimates of Ps,a in UCRL2. The derivation of this bound utilizes and extends the stochastic optimism technique from Osband et al. [2014]. For s, a with N τks,a ≤ η, P̃s,a = Qj,ks,a is a sample from the simple optimistic sampling, where we can only show the following weaker bound, but since this is used only while N τks,a is small, the total contribution of this deviation will be small:\nmax h∈[0,2D]S\n(P̃ ks,a − Ps,a)Th ≤ Õ ( D\n√\nS\nN τks,a +\nDS N τks,a\n)\n. (8)\nFinally, to bound the second term in (6), we observe that E[1Tst+1 h̃|π̃k, h̃, st] = PTst,at h̃ and use Azuma-Hoeffding inequality to obtain with probability (1 − ρSA):\n∑τk+1−1 t=τk (Pst,at − 1st)T h̃ ≤ O( √ (τk+1 − τk) log(SA/ρ)). (9) Combining the above observations (equations (4), (6), (7), (8), (9)), we obtain the following bound onRk within logarithmic factors: D(τk+1−τk) √ SA\nT +D\n∑\ns,a\nN τk+1 s,a −N τks,a √\nN τks,a\n( 1(N τk+1s,a > η) + √ S1(N τk+1s,a ≤ η) ) +D √ τk+1 − τk.\n(10)\nWe can finish the proof by observing that (by definition of an epoch) the number of visits of any state-action pair can at most double in an epoch, N τk+1s,a −N τks,a ≤ N τks,a, and therefore, substituting this observation in (10), we can bound (within logarithmic factors) the total regretR(T ) = ∑Kk=1 Rk as: K ∑\nk=1\n(\nD(τk+1 − τk) √ SA T +D\n∑\ns,a:N τk s,a>η\n√\nN τks,a +D ∑\ns,a:N τk s,a<η\n√ SN τks,a +D √ τk+1 − τk\n)\n≤ D √ SAT +D log(K)( ∑\ns,a\n√ N τKs,a ) +D log(K)(SA √ Sη) +D √ KT\nwhere we used N τk+1 s,a ≤ 2N τks,a and ∑ k(τk+1 − τk) = T . Now, we use that K ≤ SA log(T ), and SA √ Sη = O(S7/4A3/4T 1/4 + S5/2A log(T/ρ)) (using η = √\nTS A + 12ωS 2). Also, since ∑\ns,a N τK s,a ≤ T , by simple worst scenario analysis,\n∑\ns,a\n√ N τKs,a ≤ √ SAT , and we obtain,\nR(T,M) ≤ Õ(D √ SAT +DS7/4A3/4T 1/4 +DS5/2A)."
    }, {
      "heading" : "4.2 Main lemmas",
      "text" : "Following lemma form the main technical components of our proof. All the missing proofs are provided in the supplementary material.\nLemma 4.1. Assume T ≥ CDA log2(T/ρ) for a large enough constant C. Then, with probability 1− ρ, for every epoch k, the diameter of MDP M̃k is bounded by 2D. Lemma 4.2. With probability 1 − ρ, for every epoch k, the optimal gain λ̃k of the extended MDP M̃k satisfies:\nλ̃k ≥ λ∗ −O ( D log2(T/ρ) √ SA T ) ,\nwhere λ∗ the optimal gain of MDP M andD is the diameter.\nProof. Let h∗ be the bias vector for an optimal policy π∗ of MDP M (refer to Lemma 2.1 in the preliminaries section). Since h∗ is a fixed (though unknown) vector with |hi − hj | ≤ D, we can apply Lemma 4.3 to obtain that with probability 1− ρ, for all s, a, there exists a sample vectorQj,ks,a for some j ∈ {1, . . . , ψ} such that\n(Qj,ks,a) Th∗ ≥ PTs,ah∗ − δ\nwhere δ = O ( D log2(T/ρ) √\nSA T\n)\n. Now, consider the policy π forMDP M̃k which for any s, takes action aj , with a = π∗(s) and j being a sample satisfying above inequality. LetQπ be the transition matrix for this policy, whose rows are formed by the vectors Qj,ks,π∗(s), and Pπ∗ be the transition matrix whose rows are formed by the vectors Ps,π∗(s). Above implies Qπh ∗ ≥ Pπ∗h∗ − δ1. We use this inequality along with the known relations between the gain and the bias of optimal policy in communicating MDPs to obtain that the gain λ̃(π) of policy in π for MDP M̃k satisfies λ̃(π) ≥ λ∗ − δ (details provided in the supplementary material), which proves the lemma statement since by optimality λ̃k ≥ λ̃(π).\nLemma 4.3. (Optimistic Sampling) Fix any vector h ∈ RS such that |hi − hi′ | ≤ D for any i, i′, and any epoch k. Then, for every s, a, with probability 1− ρSA there exists at least one j such that\n(Qj,ks,a) Th ≥ PTs,ah−O\n( D log2(T/ρ) √\nSA T\n)\n.\nLemma 4.4. (Deviation bound) With probability 1− ρ, for all epochs k, sample j, all s, a\nmax h∈[0,2D]S\n(Qj,ks,a − Ps,a)Th ≤\n\n    \n    \nO\n(\nD\n√\nlog(SAT/ρ)\nN τks,a +D\nS log(SAT/ρ)\nN τks,a\n)\n, N τks,a > η\nO\n(\nD\n√\nS log(SAT/ρ)\nN τks,a +D\nS log(S)\nN τks,a\n)\n, N τks,a ≤ η"
    }, {
      "heading" : "5 Conclusions",
      "text" : "We presented an algorithm inspired by posterior sampling that achieves near-optimal worst-case regret bounds for the reinforcement learning problem with communicating MDPs in a non-episodic, undiscounted average reward setting. Our algorithm may be viewed as a more efficient randomized version of the UCRL2 algorithm of Jaksch et al. [2010], with randomization via posterior sampling forming the key to the √ S factor improvement in the regret bound provided by our algorithm. Our analysis demonstrates that posterior sampling provides the right amount of uncertainty in the samples, so that an optimistic policy can be obtained without excess over-estimation."
    }, {
      "heading" : "A Anti-concentration of Dirichlet distribution",
      "text" : "We prove the following general result on anti-concentration of Dirichlet distributions, which will be used to prove optimism.\nPropositionA.1. Consider a random vector p̃ generated from Dirichlet distribution with parameters (mp̄1, . . . ,mp̄S), wheremp̄i ≥ 6. Then, for any fixed h ∈ [0, D]S , with probability Ω(1/S)− Sρ,\n(p̃− p̄)Th ≥ 1 8\n√ √ √ √ ∑\ni<S\nγ̄ic̄2i m − 2SD log(2/ρ) m\nwhere\nγ̄i := p̄i(p̄i+1 + . . .+ p̄S)\n(p̄i + . . .+ p̄S) , c̄i = (hi − H̄i+1), H̄i+1 =\n1 ∑S\nj=i+1 p̄j\nS ∑\nj=i+1\nhj p̄j .\nWe use an equivalent representation of a Dirichlet vector in terms of independent Beta random variables.\nFact 1. Fix an ordering of indices 1, . . . , S, and define ỹi := p̃i\np̃i+···+p̃S , ȳi := p̄i p̄i+···+p̄S . Then, for\nany h ∈ RS ,\n(p̃− p̄)Th = ∑\ni\n(ỹi − ȳi)(hi − H̃i+1)(p̄i + · · ·+ p̄S) = ∑\ni\n(ỹi − ȳi)(hi − H̄i+1)(p̃i + · · ·+ p̃S)\nwhere H̃i+1 = 1∑\nS j=i+1 p̃j\n∑S j=i+1 hj p̃j , H̄i+1 = 1∑ S j=i+1 p̄j ∑S j=i+1 hj p̄j .\nFact 2. For i = 1, . . . , S, ỹi := p̃i\np̃i+···+p̃S are independent Beta random variables distributed as\nBeta(mp̄i,m(p̄i+1 + · · ·+ p̄S)), with mean\nE[ỹi] = mp̄i\nm(p̄i + · · ·+ p̄S) = ȳi,\nand variance\nσ̄2i := E[(ỹi − ȳi)2] = p̄i(p̄i+1 + · · ·+ p̄S)\n(p̄i + · · ·+ p̄S)2(m(p̄i + · · ·+ p̄S) + 1) .\nLemma A.2 (Corollary of Lemma E.2). Let ỹi, ȳi, σ̄i be defined as in Fact 2. Ifmp̄i,m(p̄i+1+ · · ·+ p̄S) ≥ 6, then, for any positive constant C ≤ 12 ,\nP (ỹi ≥ ȳi + Cσ̄i + C\nm(p̄i + ...+ p̄S) ) ≥ 0.15 =: η.\nProof. Apply Lemma E.2 with a = mp̄i, b = m(p̄i+1 + · · ·+ p̄S). Lemma A.3. (Application of Berry-Esseen theorem) LetG ⊆ {1, . . . , S} be a set of indices, zi ∈ R be fixed. Let\nXG := ∑\ni∈G\n(ỹi − ȳi)zi.\nLet F be the cumulative distribution function of\nXG σG , where, σ2G = ∑\ni∈G\nz2i σ̄ 2 i ,\nσ̄i being the standard deviation of ỹi (refer to Fact 2). Let Φ be the cumulative distribution function of standard normal distribution. Then, for all ǫ > 0:\nsup x\n|F (x)− Φ(x)| ≤ ǫ\nas long as √\n|G| ≥ RC ǫ , where R := max i,j∈G ziσ̄i zj σ̄j\nfor some C ≤ 3 + 6mp̄i .\nProof. Yi = (ỹi − ȳi)zi. Then, Yi, i ∈ G are independent variables, with E[Yi] = 0, σ2i := E[Y 2 i ] = E[(ỹi − ȳi)2(zi)2]\n= z2i σ̄ 2 i\nρi := E[|Yi|3] ≤ E[|Yi|4]3/4\n= E[|ỹ − ȳ|4]3/4z3i ≤ κE[|ỹ − ȳ|2]3/2z3i = κσ̄3i z 3 i\nwhere the first inequality is by using Jensen’s inequality and κ is the Kurtosis of Beta distribution. Next, we use that ỹ is Beta distributed, and Kurtosis of Beta(νµ, ν(1 − µ)) Distribution is\nκ = 3 + 6\n(3 + ν)\n( (1− 2µ)2(1 + ν) µ(1− µ)(2 + ν) − 1 ) ≤ 3 + 6 (3 + ν)µ .\nHere, α = m(p̄i + · · ·+ p̄S)ȳi, β = m(p̄i + · · ·+ p̄S)(1 − ȳi), so that\nκ ≤ 3 + 6 3 +m(p̄i + · · ·+ p̄S) 1 ȳi ≤ 3 + 6 mp̄i .\nNow, we use Berry-Esseen theorem (Fact 6), with\nψ1 = 1 √\n∑\ni∈G σ 2 i\nmax i∈G ρi σ2i\n≤ κ√ |G| maxi∈G ziσ̄i mini∈G ziσ̄i\nto obtain the lemma statement.\nLemma A.4. Assumingmp̄i ≥ 6, ∀i, for any fixed zi, i = 1, . . . , S,\nPr\n\n\n∑\ni\n(ỹi − ȳi)zi ≥ 1\n4\n√\n∑\ni\nσ̄2i z 2 i\n\n ≥ Ω(1/S).\nProof. Define constant δ := (1−Φ)( 1 2 ) 2 and k(δ) := C2 δ4 , where C ≤ 4. Consider the the group of indices with the k(δ) largest values of |ziσ̄i|, call it groupG(1), and then divide the remaining into smallest possible collection G of groups such that |ziσ̄i|/|zjσ̄j | ≤ 1δ for all i, j in any given groupG. Define an ordering ≺ on groups by ordering them by maximum value\nof |ziσ̄i| in the group. That is G ≻ G′ if maxi∈G z2i σ̄2i ≥ maxj∈G′ z2j σ̄2j Note that by construction, for G ≻ G′, we havemaxi∈G z2i σ̄2i ≥ 1δ2 maxj∈G′ z2j σ̄2j .\nRecall from Lemma A.3, for every group G ∈ G of size √ |G| > Cδǫ , we have that its cdf is within ǫ of normal distribution cdf, giving that Pr(XG ≥ 12σG) ≥ 2δ − ǫ. Using this result for ǫ = δ, we get that for every group of size at least k(δ), we have\nPr(XG ≥ 1\n2 σG) ≥ δ. (11)\nWe will look at three types of the groups we created above:\n• Top big groups: those among the top log1/δ(S) groups that have cardinality at least k(δ)\n• Top small groups: those among the top log1/δ(S) groups that have cardinality smaller than k(δ)\n• Bottom groups: those not among the top log1/δ(S) groups\nHere, top groups refers to the those ranked higher according to the ordering≻. For the first group type above, apply (11) to obtain,\nfor all big groups among top log1/δ(S), XG ≥ 12σG\nwith probability at least δlog1/δ(S) = 1\nS . (12)\nNext, we analyze the remaining indices (among top small groups and bottom groups). Consider the groupG(1) we set aside. Using Lemma A.2 k(δ) times, we have:\nPr\n\n\n∑\ni∈G(1)\n(ỹi − ȳi)zi ≥ 0.5 √ ∑\ni∈G(1)\nz2i σ̄ 2 i\n\n ≥ ηk(δ)\nwhere η ≥ 0.15. Now, if it is the case where the top group is of small size, we apply the above anticoncentration of beta for each element in the group, so that for all indices i in this group, (ỹi − ȳi)zi ≥ 0.5ziσ̄i, with probability ηk(δ). To conclude, so far, we have with probability at least 1S η 2k(δ)\n∑\ni∈G(1),i∈top big groups\n(ỹi − ȳi)zi ≥ 0.5 √\n∑\ni∈G(1),i∈top big groups\nz2i σ̄ 2 i .\nFor every other small group G, the group’s total variance is at most k(δ)maxi∈G z 2 i σ̄ 2 i ≤ k(δ)δ2jz2(1)σ̄ 2 (1), where j is the rank of the group in ordering ≻ and (1) is the index of the smallest variance in G(1). So, the sum of the standard deviation for top log1/δ(S) small groups is at most\nk(δ) ∑\nG:top small groups\nmax i∈G\nz2i σ̄ 2 i ≤ k(δ)\nlog1/δ(S) ∑\nj=1\nδ2jz(1)σ̄(1) ≤ k(δ)δ2\n1− δ2 z 2 (1)σ̄ 2 (1)\nas it is a geometric series with δ multiplier. For the remaining bottom group, each element’s variance is at most 1S2 z 2 (1)σ̄ 2 (1), therefore\n∑\ni:top small groups, bottom groups\nz2i σ̄ 2 i ≤ (\nk(δ)δ2\n1− δ2 + 1 S )z2(1)σ̄ 2 (1) ≤ k(δ) 25 z2(1)σ̄ 2 (1) ≤ 1 25 ∑\ni∈G(1)\nz2i σ̄ 2 i .\nBy Cantelli’s Inequality (Fact 5), with probability at least 12 ,\n∑\ni:top small groups, bottom groups\n(ỹi − ȳi)zi ≥ − √\n∑\ni∈top small groups, bottom groups\nz2i σ̄ 2 i ≥ −\n1\n5\n√\n∑\ni∈G(1)\nz2i σ̄ 2 i .\nHence combining our results above,\n∑\ni\n(ỹi − ȳi)zi ≥ 1\n2\n√\n∑\ni∈G(1),top big groups\nz2i σ̄ 2 i −\n1\n5\n√\n∑\ni∈G(1)\nz2i σ̄ 2 i\n≥ 3 10\n√\n∑\ni∈G(1),top big groups\nz2i σ̄ 2 i +\n1\n25\n√\n∑\ni∈G(1)\nz2i σ̄ 2 i −\n1\n25\n√\n∑\ni∈G(1)\nz2i σ̄ 2 i\n≥ 13 50\n√\n∑\ni∈G(1),top big groups\nz2i σ̄ 2 i +\n1\n25\n√\n∑\ni∈G(1)\nz2i σ̄ 2 i\n≥ 1 4\n√\n∑\ni\nz2i σ̄ 2 i\nwith probability η2k(δ) 12S = Ω(1/S).\nProof. (Proof of Proposition A.1) Use Fact 1 to express (p̃− p̄)Th as: (p̃− p̄)Th = ∑\ni\n(ỹi − ȳi)(hi − H̃i+1)(p̄i + · · ·+ p̄S)\nUsing Lemma E.3 and Corollary E.7,\n|H̃i − H̄i| ≤ D √\n2 log(2/ρ)\nm(p̄i + . . .+ p̄S)\nwith probability 1− ρ for any i. and similarly using Lemma E.3 and Corollary E.7,\n|ỹi − ȳi| ≤ √ 2 log (2/ρ)\nm(p̄i + ...+ p̄S) .\nTherefore, with probability 1− Sρ, (p̃− p̄)Th− ∑\ni\n(ỹi − ȳi)(hi − H̄i+1)(p̄i + · · ·+ p̄S)\n= ∑\ni\n(ỹi − ȳi)(H̃i+1 − H̄i+1)(p̄i + · · ·+ p̄S)\n≥ − ∑\ni\n√\n2 log(2/ρ)\nm(p̄i + ...+ p̄S) D\n√\n2 log(2/ρ)\nm(p̄i + ...+ p̄S) (p̄i + · · ·+ p̄S)\n≥ −2SD log(2/ρ) m . (13)\nThen, applying Lemma A.4 (given mp̄i ≥ 6) for zi = (hi − H̄i+1)(p̄i + · · · + p̄S), i = 1, . . . , S, with probability Ω(1/S),\n(p̃− p̄)Th ≥ 1 4\n√\n∑\ni\nz2i σ̄ 2 i −\n2SD log(2/ρ)\nm .\nNow, we observe\n∑\ni\nz2i σ̄ 2 i = (hi − H̄i+1)2(p̄i + · · ·+ p̄S)2σ̄2i =\nc̄2i p̄i(p̄i + . . . , p̄S)\nm(p̄i + . . .+ p̄S) + 1 ,\nto obtain\n(p̃− p̄)Th ≥ 1 8\n√\n∑\ni\nγ̄ic̄2i m − 2SD log(2/ρ) m\nwhere\nγ̄i = p̄i(p̄i+1 + . . .+ p̄S)\n(p̄i + . . .+ p̄S) ."
    }, {
      "heading" : "B Optimism",
      "text" : "In this section, we prove the following lemmas.\nLemma 4.2. With probability 1 − ρ, for every epoch k, the optimal gain λ̃k of the extended MDP M̃k satisfies:\nλ̃k ≥ λ∗ −O ( D log2(T/ρ) √ SA T ) ,\nwhere λ∗ the optimal gain of MDP M andD is the diameter.\nProof. Let h∗ be the bias vector for an optimal policy π∗ of MDP M (refer to Lemma 2.1 in the preliminaries section). Since h∗ is a fixed (though unknown) vector with |hi − hj | ≤ D, we can apply Lemma 4.3 to obtain that with probability 1− ρ, for all s, a, there exists a sample vectorQj,ks,a for some j ∈ {1, . . . , ψ} such that\n(Qj,ks,a) Th∗ ≥ PTs,ah∗ − δ\nwhere δ = O ( D log2(T/ρ) √\nSA T\n)\n. Now, consider the policy π for MDP M̃k which for any s, takes action aj , where a = π∗(s), and j is a sample satisfying above inequality. Note that π is essentially π∗ but with a different transition probability model. Let Qπ be the transition matrix for this policy, whose rows are formed by the vectors Qj,ks,π∗(s), and Pπ∗ be the transition matrix whose rows are formed by the vectors Ps,π∗(s). Above implies\nQπh ∗ ≥ Pπ∗h∗ − δ1.\nLet Q∗π denote the limiting matrix for Markov chain with transition matrix Qπ. Observe that Qπ is aperiodic, recurrent and irreducible : it is aperiodic and irreducible because each entry of Qπ being a sample from Dirichlet distribution is non-zero, and it is positive recurrent because in a finite irreducible Markov chain, all states are positive and recurrent. This implies that Q∗π is of the form 1q∗T where q∗ is the stationary distribution of Qπ, and 1 is the vector of all 1s (refer to (A.6) in Puterman [2014]). Also, Q∗πQπ = Qπ, and Q ∗ π1 = 1.\nTherefore, the gain of policy π\nλ̃(π)1 = (rTπ q ∗)1 = Q∗πrπ\nwhere rπ is the S dimensional vector [rs,π(s)]s=1,...,S . Now,\nλ̃(π)1− λ∗1 = Q∗πrπ − λ∗1 = Q∗πrπ − λ∗(Q∗π1) . . . (using Q∗π1 = 1) = Q∗π(rπ − λ∗1) = Q∗π(I − Pπ∗)h∗ . . . (using (1)) = Q∗π(Qπ − Pπ∗)h∗ . . . (using Q∗πQπ = Q∗π) ≥ −δ1 . . . (using (Qπ − Pπ∗)h∗ ≥ −δ1, Q∗π1 = 1).\nThen, by optimality,\nλ̃k ≥ λ̃(π) ≥ λ∗ − δ.\nLemma 4.3. (Optimistic Sampling) Fix any vector h ∈ RS such that |hi − hi′ | ≤ D for any i, i′, and any epoch k. Then, for every s, a, with probability 1− ρSA there exists at least one j such that\n(Qj,ks,a) Th ≥ PTs,ah−O\n( D log2(T/ρ) √\nSA T\n)\n.\nProof. For s, a with N τks,a ≥ η, Qj,ks,a were generated using posterior sampling from Dirichlet distribution Dirichlet(M τks,a(i), i = 1, . . . , S). We use Proposition B.3 for optimism of a Dirichlet posterior sample. Let’s verify the conditions applying for this proposition. We have N τks,a ≥ η = √\nTS A + 12ωS 2 ≥ 12ωS2. and ω = 720 log(n/ρ).\nTherefore, applying Proposition B.3, with probability Ω(1/S), the jth sample Qj,ks,a satisfies the following kind of optimism:\n(Qj,ks,a) Th ≥ PTs,ah−O(\nDS log2(n/ρ)\nN τks,a ).\nSubstituting N τks,a ≥ η = √ TS A + 12ωS 2 we get that every j satisfies the stated condition with probability Ω(1/S).\nFor s, a with N τks,a ≤ η, we used simple optimistic sampling. In Lemma B.1 we show for such s, a the condition (Qj,ks,a)\nTh ≥ PTs,ah is satisfied by any j with probability 1/2S. Therefore, given that the number of samples is ψ = CS log(SA/ρ) for some large enough constant C, for every s, a, with probability 1 − ρSA , there exists at least one sample Qj,ks,a satisfying the required condition.\nNotations We fix some notations for the rest of the section. Fix an epoch k, state and action pair s, a, sample j. In below, we denote n = N τks,a, ni = N τk s,a(i), pi = Ps,a(i), p̂i := ni n , p̄i = ni+ω n+ωS , p̃i = Q j,k s,a(i), for i ∈ S.\nB.1 Optimism for n ≤ η (Simple Optimistic Sampling)\nWhen n < η, simple optimistic sampling is used, so that any sample vector p̃ was generated as follows: we let p− = [p̂ − ( √\n3p̂i log(4S) n + 3 log(4S) n )1] +, and let z be a random vector picked\nuniformly at random from {11, . . . ,1S}, and set\np̃ = p− + (1−∑j p−j )z.\nWe prove the following lemmas for this sample vector.\nLemma B.1. For any fixed h ∈ [0, D]S , we have\np̃Th ≥ pTh,\nwith probability at least Ω(1/S).\nProof. Define δi := p̂i − pi (and hence ∑ i δi = 0). By multiplicative Chernoff bounds (Fact 4), with probability 1 − 12S , |δi| ≤ √ 3p̂i log(4S) n + 3 log(4S) n . Also define ∆i := p̂i − p−i =\nmin\n{\n√\n3p̂i log(4S) n + 3 log(4S) n , p̂i\n}\n. Note that∆i ≥ δi and ∑ i ∆i = ∑ i(p̂i − p−i ) = 1− ∑ i p − i .\nWith probability 1/S, z = 1i is picked such that hi = D, and (by union bound over all i) with probability 1− S 12S = 12 , |δi| ≤ √ 3 log(4S) n + 3 log(4S) n for every i. So with probability 1/2S:\n∑ i p̃ihi = ∑ i p−i hi +D(1− ∑ j p−j ) = ∑ i p−i hi +D ∑ j ∆j\n= ∑\ni\n(p̂i −∆i)hi +D∆i = ∑\ni\np̂ihi + (D − hi)∆i\n≥ ∑\ni\np̂ihi + (D − hi)δi = ∑\ni\n(p̂i − δi)hi +Dδi\n= ∑\ni\npihi +D ∑\ni\nδi = ∑\ni\npihi.\nUsing the same technique as above, we can also prove the following “pessimism” for these samples, which will be used later, in bounding the diameter in Section D.\nLemma B.2 (Pessimism). When n < η, we have for any fixed h ∈ [0, D]S\np̃Th ≤ pTh, with probability at least Ω(1/S).\nProof. Define δi,∆i as before. With probability 1/S, z = 1i is picked such that hi = 0, and again with probability 1− S 12S = 12 , |δi| ≤ √ 3 log(4S) n + 3 log(4S) n for every i. So with probability 1/2S:\n∑\ni\np̃ihi = ∑\ni\np−i hi\n= ∑\ni\n(p̂i −∆i)hi\n≤ ∑\ni\n(p̂i − δi)hi\n= ∑\ni\npihi.\nB.2 Optimism for n > η (Dirichlet posterior sampling)\nWhen n > η, Dirichlet posterior sampling is used so that p̃ is a random vector distributed as Dirichlet(mp̄1, . . . ,mp̄S), where m = n+ωS κ , p̄ = ni+ω n+ωS . We prove an optimism property for this sample vector. Following notations will be useful.\nγi := pi(pi+1 + . . .+ pS)\n(pi + . . .+ pS) , ci := (hi −Hi+1), Hi+1 =\n1 ∑S\nj=i+1 pj\nS ∑\nj=i+1\nhjpj\nγ̄i := p̄i(p̄i+1 + . . .+ p̄S)\n(p̄i + . . .+ p̄S) , c̄i := (hi − H̄i+1), H̄i+1 =\n1 ∑S\nj=i+1 p̄j\nS ∑\nj=i+1\nhj p̄j\nwhere the states are indexed from 1 to S such that p̄1 ≤ · · · ≤ p̄S . Proposition B.3. Assuming ω = 720 log(n/ρ) ≥ 613 log(2/ρ), n > 12ωS2, κ = 120 log(n/ρ) = ω 6 , then with probability Ω(1/S)− 8Sρ,\np̃Th ≥ pTh−O(DS log 2(n/ρ)\nn ).\nProof. The proof of this proposition involves showing that with probaility Ω(1/S) − 8Sρ, the random quantity p̃Th exceeds its mean p̄Th enough to overcome the possible deviation of empirical estimate p̄Th from the true value pTh. This involves a Dirichlet anti-concentration bound (Proposition A.1 and Lemma B.4) to lower bound p̃Th, and a concentration bound on empirical estimates p̂ (Lemma C.3) to lower bound p̄Th which by definition is close to p̂Th.\nIn Lemma B.4, we show that with probability Ω(1/S)− 7Sρ,\n(p̃− p̄)Th ≥ 0.188 √ √ √ √ ∑\ni<S\nγic2i m −O(DSω log(n/ρ) n ).\nNote thatm = n+ωSκ and so n κ < m < 25n 24κ since n > 12ωS 2. Then we have that\n(p̃− p̄)Th ≥ 0.184 √ κ ∑\ni\nγic2i n −O(DSω log(n/ρ) n ).\nWe can also calculate\n|(p̄− p̂)Th| = | S ∑\ni=1\nhi( np̂i + ω n+ ωS − np̂i n )| = | ∑\ni\nhi( ω(1 − Sp̂i) n+ ωS )| ≤ ωDS n+ ωS ≤ ωDS n .\nFinally, from Lemma C.3 bounding the deviation of empirical estimates, we have that with probability 1− ρ,\n|(p̂− p)Th| ≤ 2 √ √ √ √log(n/ρ) ∑\ni<S\nγic2i n + 2D log(n/ρ) n .\nHence putting everything together we have that with probability Ω(1/S)− 8Sρ, (p̃− p)Th = (p̃− p̄)Th+ (p̄− p̂)Th+ (p̂− p)Th\n≥ (p̃− p̄)Th− |(p̄− p̂)Th| − |(p̂− p)Th|\n≥ 0.184 √ κ ∑\ni\nγic2i n\n− 2 √ √ √ √log(n/ρ) ∑\ni<S\nγic2i n −O(DSω log(n/ρ) n )\n≥ −O(DS log 2(n/ρ)\nn )\nwhere the last inequality follows with ω = 720 log(n/ρ) and κ = 120 log(n/ρ).\nLemma B.4. Assume that h ∈ [0, D]S , and ω ≥ 613 log(2/ρ), n > 12ωS2, κ = ω6 , and an ordering of i such that p̄1 ≤ · · · ≤ p̄S . Then, with probability Ω(1/S)− 7Sρ,\n(p̃− p̄)Th ≥ 0.188 √ ∑\ni\nγic2i m −O(DSω log(n/ρ) n ).\nProof. The proof is obtained by a modification to the proof of Proposition A.1, which proves a similar bound but in terms of γ̄i’s and c̄i’s.\nIn the proof of that proposition, we obtain (refer to Equation (13)), with probability 1−Sρ (assuming mp̄i ≥ 6),\n(p̃− p̄)Th ≥ ∑\ni\n(ỹi − ȳi)(hi − H̄i+1)(p̄i + · · ·+ p̄S)− 2DS log(2/ρ)\nm\n≥ ∑\ni\n(ỹi − ȳi)(hi − H̄i+1)(p̄i + · · ·+ p̄S)−O( DSω log(n/ρ)\nn )\nwhere ỹi := p̃i\np̃i+···+p̃S , ȳi := p̄i p̄i+···+p̄S , H̃i+1 = 1∑\nS j=i+1 p̃j\n∑S j=i+1 hj p̃j , H̄i+1 =\n1∑ S j=i+1 p̄j ∑S j=i+1 hj p̄j . Now, breaking up the term in the summation and using Lemma B.7 to bound |Hi+1 − H̄i+1|(p̄i + · · · + p̄S) (since we have by assumption that ω ≥ 613 log(2/ρ) and n > 12ωS2) and Lemma E.4 and Corollary E.7 to bound |ỹi − ȳi|, we get that for every i, with probability 1− 4Sρ,\n(p̃− p̄)Th− ∑\ni\n(ỹi − ȳi)(hi −Hi+1)(p̄i + · · ·+ p̄S) +O( DSω log(n/ρ)\nm )\n≥ ∑\ni\n(ỹi − ȳi)(H̄i+1 −Hi+1)(p̄i + · · ·+ p̄S)\n≥ − ∑\ni\n√\n2 log(2/ρ)\nm(p̄i + · · ·+ p̄S)\n(\n3D\n√\nlog(n/ρ) (p̄i + · · ·+ p̄S)\nn + 4\n(ωS + log(n/ρ))D\nn\n)\n(∗) ≥ −6DS √\nlog(2/ρ) log(n/ρ)√ mn\n− 4(ωS + log(n/ρ))D √ 2 log(2/ρ)\nn √ m\n∑\ni\n1 √\n(p̄i + · · ·+ p̄S) .\nRecall that m = n+ωSκ , so that for n > Sω, n ≥ mκ2 = mω12 ≥ m log(2/ρ), and the first term of (∗) is at least:\n−6DS √ log(2/ρ) log(n/ρ) √\nm2 log(2/ρ) = −6DS\n√\nlog(n/ρ) m = −O(DSω log(n/ρ) n ).\nThen using Lemma B.5 andm = (n+ Sω)/κ > 6n/ω > 72S2, the second term in (∗) is at least:\n−8S(ωS + log(n/ρ))D √ 2 log(2/ρ)\nn √ 72S2\n= −O(DSω log(n/ρ) n ).\nThen, applying Lemma A.4 (given mp̄i ≥ 6) for zi = (hi − Hi+1)(p̄i + · · · + p̄S), i = 1, . . . , S, with probability Ω(1/S),\n∑\ni\n(ỹi − ȳi)zi ≥ 1\n4\n√\n∑\ni\nσ̄2i z 2 i .\nWe substitute this in the above, with the observation\n∑\ni\nz2i σ̄ 2 i =\n∑\ni\n(hi −Hi+1)2(p̄i + · · ·+ p̄S)2σ̄2i = ∑\ni\nc2i p̄i(p̄i + . . . , p̄S)\nm(p̄i + . . .+ p̄S) + 1 ≥\n∑\ni\n6\n7\nγ̄ic 2 i\nm .\nSo far we have that with probability Ω(1/S)− 4Sρ,\n(p̃− p̄)Th ≥ √ 6\n4 √ 7\n√\n∑\ni\nγ̄ic2i m −O(DSω log(n/ρ) n ). (14)\nFinally, we use Lemma B.6 with k = 14 (this requires ω ≥ 613 log(2/ρ)) to lower bound γ̄i by 1 1.51γi −O(ωSn ) to get with probability Ω(1/S)− 7Sρ,\n(p̃− p̄)Th ≥ 0.188 √ ∑\ni\nγic2i m −O(DSω log(n/ρ) n ).\nLemma B.5. Let x ∈ Rn such that 0 ≤ x1 ≤ · · · ≤ xn ≤ 1 and ∑ i xi = 1. Then\nn ∑\ni=1\n1√ xi + · · ·xn ≤ 2n.\nProof. Define f(y) := 1√ xy+···+xn for all y = 1, · · · , n. We prove that x∗ := ( 1n , 1n , · · · , 1n ) achieves the maximum value. Consider any solution x′. Assume there exists some index pair i, j with i < j and some ǫ > 0 such that x′i 6= x′j and increasing x′i by ǫ and decreasing x′j by ǫ preserves the ordering of the indices. This strictly increases the objective, because f(k) strictly increases for all i < k ≤ j and remains unchanged otherwise, and hence x′ is not an optimal solution. The only case where no such index pair (i, j) exists is when every xi is equal- this is precisely the solution x∗. Since ∑\ni f(i) is a continuous functions over a compact set, it has a maximum, which therefore must be attained at x∗.\nThis means n ∑\ni=1\n1√ xi + · · ·xn\n≤ n ∑\ni=1\n1 √\nx∗i + · · ·+ x∗n =\nn ∑\ni=1\n√\nn i ≤ √n ∫ n\ni=0\n1√ i di = 2n.\nLemma B.6. Let A = 3 log( 2ρ ) and ω ≥ 2524k2A. Also let n > 12ωS2. Then for any group G of indices, with probability 1− ρ,\n(1− 1 k ) ∑\ni∈G\np̄i − 2ωS\nn ≤\n∑\ni∈G\npi ≤ (1 + 1 k ) ∑\ni∈G\np̄i + 2ωS\nn .\nIf in the definition of γ̄i, we use an ordering of i such that p̄S ≥ 1S (e.g., if max p̄i is the last in the ordering), then for all i, with probability 1− 3ρ,\nγi ≤ (1 + 1k ) 2\n1− 1k − 16 γ̄i +\n2(1 + 1k + 1 6 )\n1− 1k − 16 ωS n .\nProof. By multiplicative Chernoff-Hoeffding bounds (Fact 4), with probability 1− ρ,\n| ∑\ni\npi − ∑\ni\np̂i| ≤ √ A ∑\ni p̂i n + A n\nwhere A = 3 log( 2ρ ) so that using | ∑ i p̄i − ∑ i p̂i| ≤ ωSn ,\n| ∑\ni\npi − ∑\ni\np̄i| ≤ √ ∑ i p̄iA\nn +\n√ AωS\nn +\nA n + ωS n ≤\n√\n∑\ni p̄iA\nn +\n2ωS\nn .\nNow, for n > 12ωS2, np̄i = n np̂i+ω n+ωS ≥ nωn+ωS ≥ 24ω25 ≥ k2A.\n| ∑\ni\npi − ∑\ni\np̄i| ≤ ∑\ni\np̄i\n√\nA n ∑ i p̄i + 2ωS n ≤ ∑\ni\np̄i\n√\nA\nk2A +\n2ωS n ≤ 1 k ∑\ni\np̄i + 2ωS\nn\nso that ∑\ni\npi ≤ (1 + 1 k ) ∑\ni\np̄i + 2ωS n , ∑\ni\npi ≥ (1− 1 k ) ∑\ni\np̄i − 2ωS\nn .\nFor the second statement of the lemma, using what we just proved, we have that with probability 1− 3ρ,\nγi = pi(pi+1 + · · ·+ pS) pi + · · ·+ pS ≤ (1 + 1 k )\n2p̄i(p̄i+1 + · · ·+ p̄S) + 2(1+ 1 k )ωS(p̄i+···+p̄S) n + 4ω2S2 n2\n(1− 1k )(p̄i + · · ·+ p̄S)− 2ωSn .\nNow, if indices i are ordered such that p̄S ≥ 1S , then p̄i+ · · ·+ p̄S ≥ 1S for all i. Also, if n > 12ωS2, we have the following bound on the denominator in above: (1 − 1k )(p̄i + · · · + p̄S) − 2ωSn ≥ (1 − 1k − 16 )(p̄i + · · ·+ p̄S), so that from above\nγi ≤ (1 + 1k ) 2\n1− 1k − 16 γ̄i +\n2(1 + 1k + 1 6 )\n1− 1k − 16 ωS n .\nLemma B.7. For any fixed h ∈ RS , and i, let Ĥi = 1∑S j=i p̂j ∑S j=i hj p̂j ,Hi = 1∑ S j=i pj ∑S j=i hjpj , H̄i = 1∑\nS j=i p̄j\n∑S j=i hj p̄j . Then if n ≥ 96, with probability 1− ρ,\n|(H̄i −Hi)(p̄i + . . .+ p̄S)| ≤ 2D √ log(n/ρ) (pi + · · ·+ pS)\nn + 3\n(ωS + log(n/ρ))D\nn .\nMoreover, if we also assume that ω ≥ 30 log(2/ρ) and n > 12ωS2, then with probability 1− 2ρ,\n|(H̄i −Hi)(p̄i + . . .+ p̄S)| ≤ 3D √ log(n/ρ) (p̄i + · · ·+ p̄S)\nn + 4\n(ωS + log(n/ρ))D\nn .\nProof. For every t, k ≥ i, define\nZt,k =\n(\nhk1(st = k)− hk pk\npi + · · ·+ pS · 1(st ∈ {i, . . . , S})\n)\n1(st−1 = s, at−1 = a),\nZt = ∑\nk≥i\nZt,k.\nThen, ∑τ\nt=1 Zt n = ∑\nk≥i\nhkp̂k − ∑\nk≥i\nhk pk\npi + · · ·+ pS · (p̂i + . . .+ p̂S) = (Ĥi −Hi)(p̂i + . . .+ p̂S)\nwhere we used Fact 1 for the last equality. Now, E[Zt|st−1, at−1] = ∑ k≥i E[Zt,k|st−1, at−1] = 0. Also, we observe that for any t, Zt,k and Zt,j for any k 6= j are negatively correlated given the current state and action: E[Zt,kZt,j|st−1, at−1] = hkhjE[1(st = k)1(st = j)− 1(st = j) pk\npi + · · ·+ pS · 1(st ∈ {i, . . . , S})\n−1(st = k) pj\npi + · · ·+ pS · 1(st ∈ {i, . . . , S})\n+ pjpk\n(pi + · · ·+ pS)2 · 1(st ∈ {i, . . . , S})]\n= hkhjE[− 2pjpk\npi + · · ·+ pS + pkpj (pi + · · ·+ pS)2 · 1(st ∈ {i, . . . , S})]\n= hkhjE[− pjpi\npi + · · ·+ pS ]\n≤ 0. And,\nE[ τ ∑\nt=1\nZ2t,k|st−1 = s, at−1 = a] = h2k t ∑\nτ=1\n1(st−1 = s, at−1 = a)\n(\npk − p2k\n(pi + · · ·+ pS)2 (pi + · · ·+ pS)\n)\n= h2k\nτ ∑\nt=1\n1(st−1 = s, at−1 = a) pk(\n∑\nj≥i,j 6=k pj)\npi + · · ·+ pS\n= nh2k pk(\n∑\nj≥i,j 6=k pj)\npi + · · ·+ pS ≤ nD2pk.\nTherefore, τ ∑\nt=1\nE[Z2t |st−1, at−1] ≤ τ ∑\nt=1\n∑\nk≥i\nE[Z2t,k|st−1, at−1] ≤ nD2(pi + · · ·+ pS).\nThen, applying Bernstein’s inequality (refer to Corollary E.1) to bound |∑τt=1 Zt|, we get the following bound on 1n ∑τ t=1 Zt = (Ĥi −Hi)(p̂i + . . .+ p̂S) with probability 1− ρ:\n|(Ĥi −Hi)(p̂i + . . .+ p̂S)| = | 1\nn\nτ ∑\nt=1\nZt| ≤ 2D √ log(n/ρ) (pi + · · ·+ pS)\nn + 3D\nlog(n/ρ)\nn .\nAlso,\n|Ĥi − H̄i| = | ∑\nk\np̂k p̂i + · · ·+ p̂S hk − p̄k p̄i + · · ·+ p̄S hk| ≤\nωSD\nn(p̂i + · · ·+ p̂S) ,\nCombining,\n|(H̄i −Hi)(p̂i + . . .+ p̂S)| ≤ 2D √ log(n/ρ) (pi + · · ·+ pS)\nn + 3D\nlog(n/ρ)\nn +\nωSD\nn .\nReplacing p̂i by p̄i,\n|(H̄i −Hi)(p̄i + . . .+ p̄S)| ≤ 2D √ log(n/ρ) (pi + · · ·+ pS)\nn + 3\n(ωS + log(n/ρ))D\nn\nwith probability 1− ρ. Now, if we also have that ω ≥ 30 log(2/ρ) and n > 12ωS2, using lemma B.6 with k = 3 to replace pi by p̄i, with probability 1− 2ρ,\n|(H̄i −Hi)(p̄i + . . .+ p̄S)| ≤ 3D √ log(n/ρ) (p̄i + · · ·+ p̄S)\nn + 4\n(ωS + log(n/ρ))D\nn ."
    }, {
      "heading" : "C Deviation bounds",
      "text" : "Lemma 4.4. (Deviation bound) With probability 1− ρ, for all epochs k, sample j, all s, a\nmax h∈[0,2D]S\n(Qj,ks,a − Ps,a)Th ≤\n\n   \n    \nO\n(\nD\n√\nlog(SAT/ρ)\nN τks,a +D\nS log(SAT/ρ)\nN τks,a\n)\n, N τks,a > η\nO\n(\nD\n√\nS log(SAT/ρ)\nN τks,a +D\nS log(S)\nN τks,a\n)\n, N τks,a ≤ η\nProof. For n > η, express the above as\nmax h∈[0,2D]S (Qj,ks,a − Ps,a)Th ≤ max h∈[0,2D]S (Qj,ks,a − P̄s,a)Th+ (P̄s,a − P̂s,a)Th+ (P̂s,a − Ps,a)Th\nwhere P̄s,a = M\nτk s,a(i)\nM τk s,a\n= N\nτk s,a(i)+ω N τk s,a+ωS is the mean of Dirichlet(Mτks,a) distribution used to sampleQ j,k,\nand P̂s,a = N\nτk s,a(i) N τk s,a . Now,\nmax h∈[0,2D]S\n(P̄s,a − P̂s,a)Th ≤ 2ωSD\nN τks,a .\nAnd, to bound the first and the last terms in above, we use Lemma C.1 and Lemma C.2 with union bound for all S,A, ψ, k, to get the lemma statement for n > η.\nFor n < η, we use Lemma C.4 with a union bound for S,A, ψ, k, we get the lemma statement.\nC.1 Dirichlet concentration\nA similar result as the lemma below for concentration of Dirichlet random vectors was proven in Osband and Van Roy [2016]. We include (an expanded version of) the proof for completeness.\nLemma C.1 (Osband and Van Roy [2016]). Let p̃ ∼ Dirichlet(mp̄). Let\nZ := max v∈[0,D]S\n(p̃− p̄)T v.\nThen, Z ≤ D √\n2 log(2/ρ) m , with probability 1− ρ.\nProof. Define disjoint events Ev, v ∈ [0, D]S in the sample space of Z as Ev = {Z : Z = max\nw∈[0,D]S (p̃− p̄)Tw = (p̃− p̄)T v}.\nLet f(v) be the probability of event Ev. (Here, ties are broken in arbitrary but fixed manner to assign each Z to one of the Ev so that Ev are disjoint and f(v) integrate to 1). Now, define a random variable Y distributed as follows: Y = Yv − E[Yv] with probability f(v), where Yvs are Beta variables distributed as Yv ∼ Beta(m 1D p̄T v,m(1 − 1D p̄T v)). We show that Y is stochastically optimistic compared to Z .\nWe couple Y and Z as follows: when Z ∈ Ev, which is with probability f(v), we set Y is Yv. By definition, under this event, Z = (p̃ − p̄)T v. By Dirichlet-Beta optimism (Lemma E.5), for any v, DYv is stochastically optimistic compared to p̃\nT v. Now, since they have the same mean, from equivalence condition for stochastic optimism (Condition 3 in Lemma 3 of Osband et al. [2014])\nE[DYv − p̃T v|p̃T v] = 0 for all values of v, p̃T v. Since we coupled Y and Z so that Y is Yv − E[Yv] when Z ∈ Ev, we can derive that for any v, and z ∈ Ev,\nE[DY − Z|Z = z : z ∈ Ev] = E[DYv −DE[Yv]− Z |Z = z : z ∈ Ev] = E[DYv −DE[Yv]− (p̃− p̄)T v | (p̃− p̄)T v] = E[DYv − p̃T v | p̃T v] = 0.\nThis is true for all z, since every z ∈ Ev for some v, thus proving DY so Z.\nLet X be distributed as Gaussian with mean 0 and variance 1m . By Gaussian-Beta stochastic optimismX so Yv − E[Yv], which implies for any convex increasing u(·),\nE[u(Y )] =\n∫\nv\nE[u(Yv − E[Yv ])f(v) ≤ ∫\nv\nE[u(X)]f(v) = E[u(X)]\nso thatX so Y , and X so Y so 1\nD Z.\nTherefore, we can use Corollary E.7 to bound Z byD √\n2 log(2/ρ) m with probability 1− ρ.\nC.2 Concentration of average of independent multinoulli trials\nBelow we study concentration properties of vector p̂ defined as the average of n independent multinoulli trials with parameter p ∈ ∆S , i.e., p̂ = ∑nj=1 xj , where xjs are iid random vectors, with xij = 1 with probability pi.\nLemma C.2. Let p̂ be the average of n independent multinoulli trials with parameter p. Let\nZ := max v∈[0,D]S\n(p̂− p)T v.\nThen, Z ≤ D √\n2 log(1/ρ) n , with probability 1− ρ.\nProof. Define disjoint events Ev, v ∈ [0, D]S in the sample space of Z as Ev = {Z : Z = max\nw∈[0,D]S (p̂− p)Tw = (p̂− p)T v}.\nLet f(v) be the probability of event Ev. (Here, ties are broken in arbitrary but fixed manner to assign each Z to one of the Ev so that Ev are disjoint and f(v) integrate to 1). Now, define a random variable Y distributed as follows: Y = Yv − E[Yv] with probability f(v), where Yvs are independent Binomial variables distributed as Yv ∼ 1nBinomial(n, 1DpT v). We show that Y is stochastically optimistic compared to Z .\nWe couple Y and Z as follows: when Z ∈ Ev, which is with probability f(v), we set Y is Yv. By definition, under this event, Z = (p̂− p)T v. By Multinomial-Binomial optimism (Lemma E.8 and Corollary E.9), for any v, DYv is stochastically optimistic compared to p̂\nT v. Now, since they have the same mean, from equivalence condition for stochastic optimism (Condition 3 in Lemma 3 of Osband et al. [2014]) E[DYv − p̂T v|p̂T v] = 0 for all values of v, p̃T v. Since we coupled Y and Z so that Y is Yv − E[Yv] when Z ∈ Ev, we can derive that for any v, and z ∈ Ev,\nE[DY − Z|Z = z : z ∈ Ev] = E[DYv −DE[Yv]− Z |Z = z : z ∈ Ev] = E[DYv −DE[Yv]− (p̂− p)T v | (p̂− p)T v] = E[DYv − p̂T v | p̂T v] = 0.\nThis is true for all z, since every z ∈ Ev for some v, thus proving DY so Z.\nNext, we boundZ using the stochastic optimism. First, let us express the distribution of Y in a more convenient way. Let µv = 1 Dp T v, µ = ∫ v f(v)µv . Define\nX = n ∑\nj=1\nXj\nwhereXjs are iid random variables, distributed as follows: Xj takes value 1− µv with probability f(v)µv and −µv w.p. f(v)(1 − µv), for v ∈ [0, D]d. Therefore, E[Xj ] = ∫\nv(1 − µv)f(v)µv − µvf(v)(1 − µv) = 0, andXj ∈ [−1, 1]. We show that X and Y have the same distribution. Since each Yv is Binomial(n, µv), we can write it as Yv = ∑n j=1 Y j v where Y j v are independent Bernoulli(µv) random variables. Define a random variable ṽ which is v with probability f(v). Then, since Y is Yv − µv w.p. f(v),\nY ∼ ∫\nv\n(Yv−µv)1(ṽ = v) = 1\nn\n∑\nj\n∫\nv\n(1−µv)1(ṽ = v, Y jv = 1)−µv1(ṽ = v, Y jv = 0) ∼ 1\nn\n∑\nj\nXj.\nTherefore,\nX ∼ Y so 1\nD Z\nwhere X = 1n ∑n j=1 X j , is the sum of n mean 0, bounded [−1, 1], iid random variables. By Hoeffding’s lemma, for any s ∈ R\nE[esX j ] ≤ e s 2 2 , so that, E[esnX ] ≤ ens 2 2 .\nUsing stochastic optimism E[u(Z/d)] ≤ E[u(Y )] = E[u(X)] for all convex increasing u(·), therefore for s > 0,\nP (n Z D > nt) ≤ E[e\nsn ZD ]\nesnt ≤ E[e\nsnX ]\nesnt ≤ ens\n2\n2 −snt.\nChoosing s = t = √\n2 log(1/ρ) n ,\nP ( Z\nD >\n√\nlog(1/ρ)\nn ) ≤ e−t\n2\n2 < ρ.\nLemma C.3. Let p̂ ∈ ∆S be the average n independent multinoulli trials with parameter p ∈ ∆S . Then, for any fixed h ∈ [0, D]S and n ≥ 96, with probability 1− ρ,\n|(p̂− p)Th| ≤ 2 √ √ √ √log(n/ρ) ∑\ni<S\nγic2i n + 3D log(2/ρ) n ,\nwhere γi = pi(pi+1+···+pS) (pi+···+pS) , ci = hi −Hi+1, Hi+1 = 1∑S\nj=i+1 pj\n∑S j=i+1 hjpj .\nProof. For every t, i, define\nZt,i =\n(\nci1(st = i)− ci pi\npi + · · ·+ pS · 1(st ∈ {i, . . . , S})\n)\n1(st−1 = s, at−1 = a),\nZt = ∑\ni\nZt,i.\nThen,\n∑τ t=1 Zt n = ∑\ni\ncip̂i− ∑\ni\ncipi pi + · · ·+ pS\n·(p̂i+. . .+p̂S) = S−1 ∑\ni=1\n(ŷi−yi)(p̂i+. . .+p̂S)ci = (p̂−p)Th\nwhere we used Fact 1 for the last equality. Now, E[Zt|st−1, at−1] = ∑ iE[Zt,i|st−1, at−1] = 0. Also, we observe that for any t, Zt,i and Zt,j for any i 6= j are independent given the current state and action: (assume j > i w.l.o.g.)\nE[Zt,iZt,j|st−1, at−1] = cicjE[1(st = i)1(st = j)− 1(st = j) pi\npi + · · ·+ pS · 1(st ∈ {i, . . . , S})\n−1(st = i) pj\npj + · · ·+ pS · 1(st ∈ {j, . . . , S})\n+ pjpi\n(pj + · · ·+ pS)(pi + · · ·+ pS) · 1(st ∈ {j, . . . , S})]\n= cicjE[−1(st = j) pi\npi + · · ·+ pS +\npjpi (pj + · · ·+ pS)(pi + · · ·+ pS) · 1(st ∈ {j, . . . , S})]\n= cicjE[− pjpi\npi + · · ·+ pS + pjpi (pi + · · ·+ pS) ]\n= 0.\nTherefore, τ\n∑\nt=1\nE[Z2t |st−1, at−1] = τ ∑\nt=1\n∑\ni\nc2iE[Z 2 t,i|st−1, at−1] =\n∑\ni\nc2inγi,\nwhere the last equality is obtained using the following derivation:\nE[\nτ ∑\nt=1\nZ2t,i|st−1 = s, at−1 = a] = τ ∑\nt=1\n1(st−1 = s, at−1 = a)\n(\npi − p2i\n(pi + · · ·+ pS)2 (pi + · · ·+ pS)\n)\n= τ ∑\nt=1\n1(st−1 = s, at−1 = a) pi(pi+1 + · · ·+ pS)\npi + · · ·+ pS\n= n pi(pi+1 + · · ·+ pS)\npi + · · ·+ pS = nγi.\nThen, applying Bernstein’s inequality (refer to Corollary E.1) to bound |∑τt=1 Zt|, we get the desired bound on (p− p̂)Th = 1n ∑τ t=1 Zt.\nC.3 Concentration of simple optimistic samples\nLemma C.4. Let p̃ = p− + (1 − ∑Si=1 p−i )z where z be a random vector picked uniformly at random from {11, . . . ,1S}, and p− = p̂−∆,∆i = min { √ 3p̂i log(4S) n + 3 log(4S) n , p̂i } , then with probability at least 1− ρ, for anyD, we have\nmax h∈[0,D]S\n(p̃Th− pTh) ≤ O(D √ S log(nS/ρ)\nn +\nDS log(2S)\nn ).\nProof. By definition of p̃ and using Lemma C.2, with probability 1− ρ,\nmax h∈[0,D]S\n(p̃Th− pTh) ≤ (p̂Th− pTh) +D ∑\ni\n√\n3p̂i log(4S)\nn + ∑\ni\n3D log(4S)\nn\n≤ 2D √ 2 log(1/ρ)\nn +D\n√\nS 3 log(4S)\nn +\nDS log(4S)\nn\n= O(D\n√\nS log(4S/ρ)\nn +\nDS log(4S)\nn )."
    }, {
      "heading" : "D Diameter of the extended MDP M̃k",
      "text" : "Lemma 4.1. Assume T ≥ CDA log2(T/ρ) for a large enough constant C. Then, with probability 1− ρ, for every epoch k, the diameter of MDP M̃k is bounded by 2D.\nProof. Using Lemma D.2, along with Lemma D.1 for h = Es, we obtain that the diameter of M̃k is bounded by D/(1 − δ) for δ = O(D √\nlog(1/ρ) η +D log(T/ρ) η )), where η =\n√\nTS A . Therefore, if\nT ≥ CDA log2(T/ρ), then η ≥ CDS log(T/ρ) ≥ CD2 log(1/ρ), making δ ≤ 1/2 for some large enough constant C.\nLemma D.1. For every k, and any fixed h ∈ [0, D]S , with probability 1 − ρ, there exists a sample vector Qj,ks,a such that\nQj,ks,a · h ≤ Ps,a · h+O(D √ log(1/ρ)\nη +DS\nlog(T/ρ)\nη )).\nProof. First consider s, a with N τks,a ≥ η. For such s, a posterior sampling is used, and by Lemmas C.1 and C.2,\nQj,ks,a · h ≤ Ps,a · h+O(D √ log(1/ρ)\nN τks,a +D\nωS\nN τks,a ) ≤ Ps,a · h+O(D\n√\nlog(1/ρ)\nη +DS\nlog(T/ρ)\nη ).\nFor s, a with N τks,a ≤ η, we use a simple optimistic sampling. In Lemma B.2, we prove that under such samplingQj,ks,a ·h ≤ Ps,a ·h with probability 1/2S for every sample j. Then, since the number of samples is Θ(S log(1/ρ)), we get that it holds for some j with probability 1− ρ. Lemma D.2. Let Es ∈ RS+ be the vector of the minimum expected times to reach s from s′ ∈ S in true MDP M, i.e., Ess′ = minπ T πs′→s. Note that Ess = 0. For any episode k, if for every s, a there exists some j such that Qj,ks,a ·Es ≤ Ps,a · Es + δ, (15) for some δ ∈ [0, 1), then the diameter of extended MDP M̃k is at most D1−δ , whereD is the diameter of MDP M.\nProof. Fix a k. For brevity, we omit the superscript k in below.\nFix any two states s1 6= s2. We prove the lemma statement by constructing a policy π̃ for M̃ such that the expected time to reach s2 from s1 is at most D 1−δ . Let π be the policy for MDPM for which the expected time to reach s2 from s1 is at most D (since M has diameterD, such a policy exists). Let E be the |S| − 1 dimensional vector of expected times to reach s2 from every state, except s2 itself, using π (E is the sub-vector formed by removing sth2 coordinate of vector E\ns2 where Es was defined in the lemma statement. Note that Es2s2 = 0). By first step analysis, E is a solution of:\nE = 1+ P †πE,\nwhere P †π is defined as the (S − 1)× (S − 1) transition matrix for policy π, with the (s, s′)th entry being the transition probability Ps,π(s)(s\n′) for all s, s′ 6= s2. Also, by choice of π, E satisfies Es1 ≤ D.\nNow, we define π̃ using π as follows: For any state s 6= s2, let a = π(s) and jth sample satisfies the property (15) for s, a, Es2 , then we define π̃(s) := aj . Let Qπ̃ be the transition matrix (dimension S × S) for this policy. Qπ̃ defines a Markov chain. Next, we modify this Markov chain to construct an absorbing Markov chain with a single absorbing state s2. Let Q † π̃ be the submatrix (S − 1)× (S − 1) submatrix ofQπ̃ obtained by removing the row and column corresponding to the state s2. Then Q ′ is defined as (an appropriate reordering of) the following matrix:\nQ′π̃ =\n[\nQ†π̃ q 0 1\n]\nwhere q is an (S − 1)-length vector such that the rows of Q′π̃ sum to 1. Since the probabilities in Qπ̃ were drawn from Dirichlet distribution, they are all strictly greater than 0 and less than 1. Therefore each row-sum of Q†π̃ is strictly less than 1, so that the vector q has no zero entries and the Markov chain is indeed an absorbing chain with single absorbing state s2. Then we notice that (I − Q†π̃)−1 is precisely the fundamental matrix of this absorbing Markov chain and hence exists and is non-negative (see Grinstead and Snell [2012], Theorem 11.4). Let Ẽ be defined as the S − 1 dimensional vector of expected time to reach s2 from s\n′ 6= s2 in MDP M̃k using π̃. Then, it is same as the expected time to reach the absorbing state s2 from s ′ 6= s2 in the Markov chain Q′π̃, given by\nẼ = (I − Q̄†π̃)−11.\nThen using (15) (since Es2s2 = 0, the inequality holds for P †, Q†),\nE = 1+ P †πE ≥ 1+Q†π̃E − δ1 ⇒ (I −Q†π̃)E ≥ (1 − δ)1. (16)\nMultiplying the non-negative matrix (I −Q†π̃)−1 on both sides of this inequality, it follows that\nE ≥ (1− δ)(I −Q†π̃)−11 = (1− δ)Ẽ\nso that Ẽs1 ≤ 1(1−δ)Es1 ≤ D1−δ , proving that the expected time to reach s2 from s1 using policy π̃ in MDP M̃k is at most D1−δ ."
    }, {
      "heading" : "E Useful deviation inequalities",
      "text" : "Fact 3 (Bernstein’s Inequality, from Seldin et al. [2012] Lem 11/Cor 12). Let Z1, Z2, ..., Zn be a bounded martingale difference sequence so that |Zi| ≤ K and E[Zi|Fi−1] = 0. Define Mn = ∑n\ni=1 Zi and Vn = ∑n i=1 E[(Zi) 2|Fi−1]. For any c > 1 and δ ∈ (0, 1), with probability greater\nthan 1− δ, if √\nln 2νδ (e− 2)Vn ≤ 1 K\nthen\n|Mn| ≤ (1 + c) √ (e− 2)Vn ln 2ν\nδ ,\notherwise,\n|Mn| ≤ 2K ln 2ν\nδ ,\nwhere\nν = ⌈ ln (\n√\n(e−2)n\nln 2δ )\nln c ⌉+ 1.\nCorollary E.1 (to Bernstein’s Inequality above). Let Zi for i = 1, · · · , n, Mn, and Vn as above. For n ≥ 96 and δ ∈ (0, 1), with probability greater than 1− δ,\n|Mn| ≤ 2 √ Vn ln n\nδ + 3K ln\nn δ .\nProof. Applying Bernstein’s Inequality above with c = 1 + 4n , with probability greater than 1− δ,\n|Mn| ≤ (1 + c) √ (e− 2)Vn ln 2ν\nδ + 2K ln\n2ν\nδ\n≤ (1 + c)\n√\n(e− 2)Vn ln n\n4 3\nδ + 2K ln\nn 4 3\nδ\n≤ (1 + c) √\n(e− 2)4 3 Vn ln n δ + 3K ln n δ\n≤ 2 √ Vn ln n\nδ + 3K ln\nn\nδ\nwhere\nν = ⌈ ln (\n√\n(e−2)n\nln 2δ )\nln c ⌉+ 1 = ⌈n 2 ln (\n√\n(e− 2)n ln 2δ )⌉+ 1 ≤ n 2 ln (\n√\n(e− 2)n ln 2 ) + 2 ≤ 1 2 n 4 3 .\nFact 4 (Multiplicative Chernoff Bound, Kleinberg et al. [2008] Lemma 4.9). Consider n i.i.d. random variablesX1, · · · , Xn on [0, 1]. Let µ be their mean and let X be their average. Then for any α > 0 the following holds:\nP (|X − µ| < r(α,X) < 3r(α, µ)) > 1− eΩ(α), where r(α, x) = √\nαx n + α n .\nMore explicitly, we have that with probability 1− ρ,\n|X − µ| < √ 3 log(2/ρ)X\nn +\n3 log(2/ρ)\nn .\nFact 5 (Cantelli’s Inequality). Let X be a real-valued random variable with expectation µ and variance σ2. Then P (X − µ ≥ λ) ≤ σ2σ2+λ2 for λ > 0 and P (X − µ ≥ λ) ≥ 1− σ 2 σ2+λ2 for λ < 0.\nFact 6 (Berry-Esseen Theorem). Let X1, X2, ..., Xn be independent random variables with E[Xi] = 0, E[X 2 i ] = σ 2 i > 0, and E[|Xi|3] = ρi < ∞. Let\nSn = X1 +X2 + ...+Xn √\nσ21 + ...+ σ 2 n\nand denote Fn the cumulative distribution function of Sn andΦ the cumulative distribution function of the standard normal distribution. Then for all n, there exists an absolute constant C1 such that\nsupx∈R|Fn(x)− Φ(x)| ≤ C1ψ1\nwhere ψ1 = ( n ∑\ni=1\nσ2i ) −1/2 max1≤i≤n ρi σ2i . The best upper bound on C1 known is C1 ≤ 0.56 (see\nShevtsova [2010]).\nFact 7 (Abramowitz and Stegun [1964] 26.5.21). Consider the regularized incomplete Beta function Iz(a, b) (cdf) for the Beta random variable with parameters (a, b). For any z such that (a + b − 1)(1− z) ≥ 0.8, Iz(a, b) = Φ(y) + ǫ, with |ǫ| < 0.005 if a+ b > 6. Here Φ is the standard normal CDF with\ny = 3[w1(1− 19b )− w2(1 − 19a )]\n[ w2 1 b + w2 2 a ] 1/2\n,\nwhere w1 = (bz) 1/3 and w2 = [a(1− z)]1/3.\nThe following lemma uses the above fact to lower bound the probability of a Beta random variable to exceed its mean by a quantity close to its standard deviation.\nLemma E.2 (Anti-concentration for Beta Random Variables). Let Fa,b denote the cdf of a Beta random variable with parameter (a, b), with a ≥ 6, b ≥ 6. Let z = aa+b +C √ ab (a+b)2(a+b+1) + C a+b , with C ≤ 0.5. Then, 1− F(a,b)(z) ≥ 1− Φ(1)− 0.005 ≥ 0.15.\nProof. Let x = C √\nab (a+b+1) + C. Then, z = a+x a+b ,w1 = (b(a + x)/(a + b)) 1/3 and w2 =\n[a(b − x)/(a + b))]1/3. Also, z ≤ 2C √\nab a+b . Also, (a+ b − 1)(1 − z) ≥ (a + b − 1)(1 − aa+b −\nC √\nab (a+b)2(a+b+1) − Ca+b ) = (a+ b− 1)( ba+b − Ca+b\n√\nab a+b+1 − Ca+b ) ≥ a+b−1a+b (b−C\n√\nab a+b+1 −\nC a+b ) ≥ 1112 (b−C √ b− C12 ) ≥ 0.8. Hence we can apply Fact 7 relating Beta with Normal. We bound the numerator and denominator in the expression of y, to show that the relation Iz(a, b) ≤ Φ(y) + ǫ holds for some y ≤ 1.\nnumerator(y) = 3[w1(1− 1\n9b )− w2(1−\n1\n9a )]\n= 3( ab\na+ b )\n1 3 [(1 +\nx a ) 1 3 (1− 1 9b )− (1 − x b ) 1 3 (1− 1 9a )]\n≤ 3( ab a+ b ) 1 3 [(1 + x 3a )(1− 1 9b )− (1− x 3b − 2x\n2\n9b2 )(1− 1 9a )]\n= 3( ab\na+ b )\n1 3 [( b− a 9ab ) + ( x(a+ b) 3ab )− ( 2x 27ab )] + 3( ab a+ b ) 1 3 [ 2x2 9b2 (1− 1 9a )]\n≤ 3( ab a+ b ) 1 3 [( b− a 9ab ) + ( x(a+ b) 3ab )] + 3( ab a+ b ) 1 3 [ 2x2 9b2 (1 − 1 9a )]\n= ( ab\na+ b )\n1 3 (\na+ b\nab )[( b− a 3(a+ b) ) + x+ 2x2 3b2 (1− 1 9a )]\n≤ ( ab a+ b ) 1 3 ( a+ b ab )[( b− a 3(a+ b) ) + 2x2 3b2 (1− 1 9a ) + C + C( ab a+ b ) 1 2 ]\n≤ ( b− a 3 √ ab(a+ b) +\n4C2 √ ab b2 √ a+ b + C √ a+ b√ ab + C)( ab a+ b ) 5 6 ( a+ b ab )\n≤ ( 1 3 √ 6 + 1 6 √ 6 + 1 2 √ 3 + 1 2 )( ab a+ b ) 5 6 ( a+ b ab ).\nIn above, we used that C ≤ 12 and a, b ≥ 6. Similarly,\ndenominator(y) = [ w21 b + w22 a ]1/2\n= ( ab a+ b )[ (1 + xa ) 2 3 b + (1− xb ) 2 3 a ] 1 2\n≥ ( ab a+ b\n) 1 3 [ (1 + 2x3a − x\n2\n9a2 )\nb + (1− 2x3b ) a − x 2 9a2 ] 1 2\n= ( ab\na+ b )\n1 3 [ a(1 + 2x3a − x\n2 9a2 ) + b(1− 2x3b − x 2 9b2 )\nab ] 1 2\n= ( ab\na+ b )\n1 3 (\na+ b ab (1− x\n2\n9ab ))\n1 2\n≥ ( ab a+ b ) 1 3 ( a+ b ab (1− 4C\n2\n9(a+ b) ))\n1 2\n≥ ( ab a+ b ) 1 3 ( a+ b ab ( 107 108 )) 1 2 .\nHence we have that y ≤ 1 3 √ 6 + 1 6 √ 6 + 1 2 √ 3 + 1\n2√ 107 108 ≤ 1, so that Iz(a, b) ≤ φ(1) + ǫ for ǫ ≤ 0.005. The lemma statement follows by observing that 1 − F(a,b)(z) = 1 − Iz(a, b) ≥ 1 − φ(1) − ǫ ≥ 1− 0.845− 0.005 ≥ 0.15. Definition 5. For any X and Y real-valued random variables,X is stochastically optimistic for Y if for any u : R → R convex and increasing E[u(X)] ≥ E[u(Y )]. Lemma E.3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1). Let Y = PTV for V ∈ [0, 1]S fixed and P ∼ Dirichlet(α) with α ∈ RS+ and ∑S\ni=1 αi ≥ 2. Let X ∼ N(µ, σ2) with µ =\n∑S i=1 αiVi∑ S i=1 αi , σ2 = ( ∑S i=1 αi) −1, thenX is stochastically optimistic for Y .\nLemma E.4 (Gaussian vs Beta optimism, Osband et al. [2014] Lemma 6). Let Ỹ ∼ Beta(α, β) for any α, β > 0 and X ∼ N( αα+β , 1α+β ). Then X is stochastically optimistic for Ỹ whenever α+ β ≥ 2. Lemma E.5 (Dirichlet vs Beta optimism, Osband et al. [2014] Lemma 5). Let y = pT v for some random variable p ∼ Dirichlet(α) and constants v ∈ Rd and α ∈ N d. Without loss of generality, assume v1 ≤ v2 ≤ · · · ≤ vd. Let α̃ = ∑d i=1 αi(vi−v1)/(vd−v1) and β̃ = ∑d i=1 αi(vd−vi)/(vd− v1). Then, there exists a random variable p̃ ∼ Beta(α̃, β̃) such that, for ỹ = p̃vd + (1 − p̃)v1, E[ỹ|y] = E[y]. Lemma E.6. If E[X ] = E[Y ] and X is stochastically optimistic for Y , then −X is stochastically optimistic for −Y .\nProof. By Lemma 3.3 in Osband et al. [2014], X stochastically optimistic for Y is equivalent to having X =D Y + A + W with A ≥ 0 and E[W |Y + A] = 0 for all values y + a. Taking expectation of both sides, we get that E[X ] = E[Y ] + E[A] + E[W ] and since E[X ] = E[Y ] = 0 and E[W ] = E[E[W |Y + A]] = 0 we get that E[A] = 0. Since A ≥ 0, A = 0. Also note that E[W |Y = y] = 0 for all y. Now we can show that −X is stochastically optimistic for −Y as follows: From above, −X =D −(Y + A +W ) = −Y + (−W ). Then for all y′, E[−W | − Y = y′] = −E[W |Y = −y′] = 0 by definition ofW . Therefore,−X is stochastically optimistic for−Y . Corollary E.7. Let Y be any distribution with mean µ such that X ∼ N(µ, σ2) is stochastically optimistic for Y . Then with probability 1− ρ,\n|Y − µ| ≤ √ 2σ2 log(2/ρ).\nProof. For any s > 0, and t, and applying Markov’s inequality,\nP (Y − µ > t) = P (Y > µ+ t) = P (esY > es(µ+t)) ≤ E[e sY ]\nes(µ+t) .\nBy Definition 5, taking u(a) = esa, which is a convex and increasing function, E[esY ] ≤ E[esX ], and hence\nP (Y − µ > t) ≤ E[e sX ]\nes(µ+t) =\neµs+ 1 2 σ2s2\nes(µ+t) = e\n1 2 σ2s2−st.\nSince the above holds for all s > 0, using s = tσ2 , P (Y − µ > t) ≤ e − t\n2\n2σ2 .\nSimilarly, for the lower tail bound, we have for any s > 0,\nP (Y − µ < −t) = P (−Y > −µ+ t) = P (es(−Y ) > es(−µ+t)) ≤ E[e s(−Y )]\nes(−µ+t) .\nBy Lemma E.6, −X is stochastically optimistic for−Y , so E[es(−Y )] ≤ E[es(−X)], and hence\nP (Y − µ < −t) ≤ E[e s(−X)]\nes(−µ+t) =\ne−µs+ 1 2 σ2s2\nes(−µ+t) = e\n1 2 σ2s2−st.\nAgain letting s = tσ2 , P (Y − µ < −t) ≤ e − t\n2\n2σ2 .\nThen, for t = √ 2σ2 log(2/ρ), we have that\nP (|Y − µ| ≤ √ 2σ2 log(2/ρ)) ≥ 1− ρ.\nLemma E.8 (Binomial, Multinomial). Let Ŷ = p̂T v where p̂ ∈ ∆S be distributed as multinomial average with parameter n, p and fixed v ∈ Rd, where 0 ≤ vi ≤ D. Then, there exists a random variable distributed as q̂ ∼ 1nBinomial(n, pTh D ) such that, E[q̂|Ŷ ] = 1D Ŷ .\nProof. LetXji , j = 1, . . . , n denote the outcomes of the trials used to define p̂i, that is,\np̂i :=\nn ∑\nj=1\nXji /n\nwhereXji , j = 1, . . . , n are distributed as X j i ∼ Multivariate(p, 1).\nFor every i, define n i.i.d. variables Y ji , j = 1, . . . , n, where Y j i ∼ Bernoulli(vi/D), and is independent ofXji . Define q̂ as:\nq̂ = 1\nn\n∑\ni\nn ∑\nj=1\nXji Y j i /n\nLet X = {Xi,j , i = 1, . . . , S, j = 1, . . . , n}. Then,\nE[q̂|p̂T v, n] = E[E[q̂|X , p̂T v, n]|p̂T v, n] = E[E[q̂|X , n]|p̂T v, n]\n= 1\nn E[E[\n∑\ni,j\nXji Y j i |X , n]|p̂T v, n]\n= 1 n E[ ∑\ni,j\nXji E[Y j i ]|p̂T v, n]\n= 1 n E[ ∑\ni,j\nXji vi D |p̂T v, n]\n= p̂T v/D.\nAlso, nq̂ is a binomial random variableBinomial(n, 1Dp T v) since it is formed by sum of outcomes of n trials ∑n\nj=1 Z j , where each trail Zj =\n∑ i X j i Y j i is an independent Bernoulli trial: takes value\n1 with probability ∑\ni pivi/D.\nCorollary E.9. For X = Dq̂, Y = p̂T v (with q̂ and p̂T v as defined in the previous lemma), X is stochastically optimistic for Y .\nProof. We have E[X − Y |Y ] = E[Dq̂ − p̂T v|p̂T v] = 0. Then stochastic optimism follows from applying the optimism equivalence condition from Lemma 3 (Condition 3) of Osband et al. [2014]."
    } ],
    "references" : [ {
      "title" : "Bayesian optimal control of smoothly parameterized systems: The lazy posterior sampling algorithm",
      "author" : [ "Yasin Abbasi-Yadkori", "Csaba Szepesvari" ],
      "venue" : "arXiv preprint arXiv:1406.3926,",
      "citeRegEx" : "Abbasi.Yadkori and Szepesvari.,? \\Q2014\\E",
      "shortCiteRegEx" : "Abbasi.Yadkori and Szepesvari.",
      "year" : 2014
    }, {
      "title" : "Handbook of mathematical functions: with formulas, graphs, and mathematical tables, volume 55",
      "author" : [ "Milton Abramowitz", "Irene A Stegun" ],
      "venue" : "Courier Corporation,",
      "citeRegEx" : "Abramowitz and Stegun.,? \\Q1964\\E",
      "shortCiteRegEx" : "Abramowitz and Stegun.",
      "year" : 1964
    }, {
      "title" : "Analysis of Thompson Sampling for the Multi-armed Bandit Problem",
      "author" : [ "Shipra Agrawal", "Navin Goyal" ],
      "venue" : "In Proceedings of the 25th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Agrawal and Goyal.,? \\Q2012\\E",
      "shortCiteRegEx" : "Agrawal and Goyal.",
      "year" : 2012
    }, {
      "title" : "Thompson sampling for contextual bandits with linear payoffs",
      "author" : [ "Shipra Agrawal", "Navin Goyal" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Agrawal and Goyal.,? \\Q2013\\E",
      "shortCiteRegEx" : "Agrawal and Goyal.",
      "year" : 2013
    }, {
      "title" : "Further Optimal Regret Bounds for Thompson Sampling",
      "author" : [ "Shipra Agrawal", "Navin Goyal" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Agrawal and Goyal.,? \\Q2013\\E",
      "shortCiteRegEx" : "Agrawal and Goyal.",
      "year" : 2013
    }, {
      "title" : "A Bayesian sampling approach to exploration in reinforcement learning",
      "author" : [ "John Asmuth", "Lihong Li", "Michael L Littman", "Ali Nouri", "David Wingate" ],
      "venue" : "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Asmuth et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Asmuth et al\\.",
      "year" : 2009
    }, {
      "title" : "Minimax regret bounds for reinforcement learning",
      "author" : [ "Mohammad Gheshlaghi Azar", "Ian Osband", "Rémi Munos" ],
      "venue" : "arXiv preprint arXiv:1703.05449,",
      "citeRegEx" : "Azar et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Azar et al\\.",
      "year" : 2017
    }, {
      "title" : "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs",
      "author" : [ "Peter L Bartlett", "Ambuj Tewari" ],
      "venue" : "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Bartlett and Tewari.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bartlett and Tewari.",
      "year" : 2009
    }, {
      "title" : "R-max-a general polynomial time algorithm for nearoptimal reinforcement learning",
      "author" : [ "Ronen I Brafman", "Moshe Tennenholtz" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Brafman and Tennenholtz.,? \\Q2002\\E",
      "shortCiteRegEx" : "Brafman and Tennenholtz.",
      "year" : 2002
    }, {
      "title" : "Prior-free and prior-dependent regret bounds for Thompson sampling",
      "author" : [ "Sébastien Bubeck", "Che-Yu Liu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bubeck and Liu.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bubeck and Liu.",
      "year" : 2013
    }, {
      "title" : "Optimal adaptive policies for Markov decision processes",
      "author" : [ "Apostolos N Burnetas", "Michael N Katehakis" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Burnetas and Katehakis.,? \\Q1997\\E",
      "shortCiteRegEx" : "Burnetas and Katehakis.",
      "year" : 1997
    }, {
      "title" : "An empirical evaluation of Thompson sampling",
      "author" : [ "Olivier Chapelle", "Lihong Li" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Chapelle and Li.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chapelle and Li.",
      "year" : 2011
    }, {
      "title" : "Sample complexity of episodic fixed-horizon reinforcement learning",
      "author" : [ "Christoph Dann", "Emma Brunskill" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dann and Brunskill.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dann and Brunskill.",
      "year" : 2015
    }, {
      "title" : "Introduction to probability",
      "author" : [ "Charles Miller Grinstead", "James Laurie Snell" ],
      "venue" : "American Mathematical Soc.,",
      "citeRegEx" : "Grinstead and Snell.,? \\Q2012\\E",
      "shortCiteRegEx" : "Grinstead and Snell.",
      "year" : 2012
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "Thomas Jaksch", "Ronald Ortner", "Peter Auer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Jaksch et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jaksch et al\\.",
      "year" : 2010
    }, {
      "title" : "On the sample complexity of reinforcement learning",
      "author" : [ "ShamMachandranath Kakade" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Kakade,? \\Q2003\\E",
      "shortCiteRegEx" : "Kakade",
      "year" : 2003
    }, {
      "title" : "Thompson Sampling: An Optimal Finite Time Analysis",
      "author" : [ "Emilie Kaufmann", "Nathaniel Korda", "Rémi Munos" ],
      "venue" : "In International Conference on Algorithmic Learning Theory (ALT),",
      "citeRegEx" : "Kaufmann et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kaufmann et al\\.",
      "year" : 2012
    }, {
      "title" : "Finite-sample convergence rates for Q-learning and indirect algorithms",
      "author" : [ "Michael J Kearns", "Satinder P Singh" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Kearns and Singh.,? \\Q1999\\E",
      "shortCiteRegEx" : "Kearns and Singh.",
      "year" : 1999
    }, {
      "title" : "Multi-armed bandits in metric spaces",
      "author" : [ "Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal" ],
      "venue" : "In Proceedings of the fortieth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2008
    }, {
      "title" : "Why is posterior sampling better than optimism for reinforcement learning",
      "author" : [ "Ian Osband", "Benjamin Van Roy" ],
      "venue" : "arXiv preprint arXiv:1607.00215,",
      "citeRegEx" : "Osband and Roy.,? \\Q2016\\E",
      "shortCiteRegEx" : "Osband and Roy.",
      "year" : 2016
    }, {
      "title" : "More) efficient reinforcement learning via posterior sampling",
      "author" : [ "Ian Osband", "Dan Russo", "Benjamin Van Roy" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Osband et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2013
    }, {
      "title" : "Generalization and exploration via randomized value functions",
      "author" : [ "Ian Osband", "Benjamin Van Roy", "Zheng Wen" ],
      "venue" : "arXiv preprint arXiv:1402.0635,",
      "citeRegEx" : "Osband et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2014
    }, {
      "title" : "Markov decision processes: discrete stochastic dynamic programming",
      "author" : [ "Martin L Puterman" ],
      "venue" : null,
      "citeRegEx" : "Puterman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Puterman.",
      "year" : 2014
    }, {
      "title" : "Learning to Optimize Via Posterior Sampling",
      "author" : [ "Daniel Russo", "Benjamin Van Roy" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Russo and Roy.,? \\Q2014\\E",
      "shortCiteRegEx" : "Russo and Roy.",
      "year" : 2014
    }, {
      "title" : "An Information-Theoretic Analysis of Thompson Sampling",
      "author" : [ "Daniel Russo", "Benjamin Van Roy" ],
      "venue" : "Journal of Machine Learning Research (to appear),",
      "citeRegEx" : "Russo and Roy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russo and Roy.",
      "year" : 2015
    }, {
      "title" : "PAC-Bayesian inequalities for martingales",
      "author" : [ "Yevgeny Seldin", "François Laviolette", "Nicolo Cesa-Bianchi", "John Shawe-Taylor", "Peter Auer" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Seldin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Seldin et al\\.",
      "year" : 2012
    }, {
      "title" : "An improvement of convergence rate estimates in the Lyapunov theorem",
      "author" : [ "I.G. Shevtsova" ],
      "venue" : null,
      "citeRegEx" : "Shevtsova.,? \\Q2010\\E",
      "shortCiteRegEx" : "Shevtsova.",
      "year" : 2010
    }, {
      "title" : "A theoretical analysis of model-based interval estimation",
      "author" : [ "Alexander L Strehl", "Michael L Littman" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "Strehl and Littman.,? \\Q2005\\E",
      "shortCiteRegEx" : "Strehl and Littman.",
      "year" : 2005
    }, {
      "title" : "An analysis of model-based interval estimation for Markov decision processes",
      "author" : [ "Alexander L Strehl", "Michael L Littman" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Strehl and Littman.,? \\Q2008\\E",
      "shortCiteRegEx" : "Strehl and Littman.",
      "year" : 2008
    }, {
      "title" : "Optimistic linear programming gives logarithmic regret for irreducible MDPs",
      "author" : [ "Ambuj Tewari", "Peter L Bartlett" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Tewari and Bartlett.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tewari and Bartlett.",
      "year" : 2008
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "William R Thompson" ],
      "venue" : null,
      "citeRegEx" : "Thompson.,? \\Q1933\\E",
      "shortCiteRegEx" : "Thompson.",
      "year" : 1933
    }, {
      "title" : "In Section D, we prove Lemma 4.1 bounding the diameter of extended MDP with high probability. And, in Section E we list some known results (or easy corollaries of known results) that are utilized in our proofs. A Anti-concentration of Dirichlet distribution We prove the following general result on anti-concentration",
      "author" : [ "Osband" ],
      "venue" : null,
      "citeRegEx" : "Osband,? \\Q2014\\E",
      "shortCiteRegEx" : "Osband",
      "year" : 2014
    }, {
      "title" : "Gaussian vs Dirichlet optimism, from Osband et al",
      "author" : [ "Lemma E" ],
      "venue" : "Let Y = PV for V ∈ [0,",
      "citeRegEx" : "E.3,? \\Q2014\\E",
      "shortCiteRegEx" : "E.3",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "More recently, posterior sampling, aka Thompson Sampling [Thompson, 1933], has emerged as another popular algorithm design principle in MAB, owing its popularity to a simple and extendible algorithmic structure, an attractive empirical performance [Chapelle and Li, 2011, Kaufmann et al.",
      "startOffset" : 57,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010].",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of Õ(DS √ AT ) for this problem.",
      "startOffset" : 93,
      "endOffset" : 1278
    }, {
      "referenceID" : 1,
      "context" : "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of Õ(DS √ AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D.",
      "startOffset" : 93,
      "endOffset" : 1460
    }, {
      "referenceID" : 1,
      "context" : "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of Õ(DS √ AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D. Jaksch et al. [2010] also established a worst-case lower bound of Ω( √ DSAT ) on the regret of any algorithm for this problem.",
      "startOffset" : 93,
      "endOffset" : 1531
    }, {
      "referenceID" : 1,
      "context" : "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of Õ(DS √ AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D. Jaksch et al. [2010] also established a worst-case lower bound of Ω( √ DSAT ) on the regret of any algorithm for this problem. Our main contribution is a posterior sampling based algorithm with a high probability worst-case regret upper bound of Õ(D √ SAT + DSAT ), which is Õ(D √ SAT ) when T ≥ SA. This improves the previously best known upper bound for this problem by a factor of √ S, and matches the dependence on S in the lower bound, for large enough T . Our algorithm uses an ‘optimistic version’ of the posterior sampling heuristic, while utilizing several ideas from the algorithm design structure in Jaksch et al. [2010], such as an epoch based execution and the extended MDP construction.",
      "startOffset" : 93,
      "endOffset" : 2142
    }, {
      "referenceID" : 1,
      "context" : "ably optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal and Goyal, 2012, 2013b,a, Russo and Van Roy, 2015, 2014, Bubeck and Liu, 2013]. In this approach, the algorithm maintains a Bayesian posterior distribution for the expected reward of every action; then at any given step, it generates an independent sample from each of these posteriors and takes the action with the highest sample value. We consider the reinforcement learning problem with finite states S and finite actions A in a similar regret based framework, where the total reward of the reinforcement learning algorithm is compared to the total expected reward achieved by a single benchmark policy over a time horizon T . In our setting, the benchmark policy is the infinite-horizon undiscounted average reward optimal policy for the underlying MDP, under the assumption that the MDP is communicating with (unknown) finite diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any other state s using an appropriate policy, for each pair s, s. A finite diameter is understood to be necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al., 2010]. The UCRL2 algorithm of Jaksch et al. [2010], which is based on the optimism principle, achieved the best previously known upper bound of Õ(DS √ AT ) for this problem. A similar bound was achieved by Bartlett and Tewari [2009], though assuming the knowledge of the diameter D. Jaksch et al. [2010] also established a worst-case lower bound of Ω( √ DSAT ) on the regret of any algorithm for this problem. Our main contribution is a posterior sampling based algorithm with a high probability worst-case regret upper bound of Õ(D √ SAT + DSAT ), which is Õ(D √ SAT ) when T ≥ SA. This improves the previously best known upper bound for this problem by a factor of √ S, and matches the dependence on S in the lower bound, for large enough T . Our algorithm uses an ‘optimistic version’ of the posterior sampling heuristic, while utilizing several ideas from the algorithm design structure in Jaksch et al. [2010], such as an epoch based execution and the extended MDP construction. The algorithm proceeds in epochs, where in the beginning of every epoch, it generates ψ = Õ(S) sample transition probability vectors from a posterior distribution for every state and action, and solves an extended MDP with ψA actions and S states formed using these samples. The optimal policy computed for this extended MDP is used throughout the epoch. Posterior Sampling for Reinforcement Learning (PSRL) approach has been used previously in Osband et al. [2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework.",
      "startOffset" : 93,
      "endOffset" : 2677
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework.",
      "startOffset" : 8,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework.",
      "startOffset" : 8,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of Õ( √ HSAT ) when T ≥ HSA.",
      "startOffset" : 8,
      "endOffset" : 458
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of Õ( √ HSAT ) when T ≥ HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H .",
      "startOffset" : 8,
      "endOffset" : 636
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of Õ( √ HSAT ) when T ≥ HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm’s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper.",
      "startOffset" : 8,
      "endOffset" : 1289
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of Õ( √ HSAT ) when T ≥ HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm’s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants.",
      "startOffset" : 8,
      "endOffset" : 1463
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of Õ( √ HSAT ) when T ≥ HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm’s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants.",
      "startOffset" : 8,
      "endOffset" : 1494
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of Õ( √ HSAT ) when T ≥ HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm’s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al.",
      "startOffset" : 8,
      "endOffset" : 1680
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of Õ( √ HSAT ) when T ≥ HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm’s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al.",
      "startOffset" : 8,
      "endOffset" : 1712
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of Õ( √ HSAT ) when T ≥ HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm’s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al. [2003], Asmuth et al.",
      "startOffset" : 8,
      "endOffset" : 1734
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of Õ( √ HSAT ) when T ≥ HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm’s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al. [2003], Asmuth et al. [2009], Dann and Brunskill [2015].",
      "startOffset" : 8,
      "endOffset" : 1756
    }, {
      "referenceID" : 0,
      "context" : "[2013], Abbasi-Yadkori and Szepesvari [2014], Osband and Van Roy [2016], but in a Bayesian regret framework. Bayesian regret is defined as the expected regret over a known prior on the transition probability matrix. Here, we consider the stronger notion of worst-case regret, aka minimax regret, which requires bounding the maximum regret for any instance of the problem. 1 We should also compare our result with the very recent result of Azar et al. [2017], which provides an optimistic version of value-iteration algorithm with a minimax regret bound of Õ( √ HSAT ) when T ≥ HSA. However, the setting considered in Azar et al. [2017] is that of an episodic MDP, where the learning agent interacts with the system in episodes of fixed and known length H . The initial state of each episode can be arbitrary, but importantly, the sequence of these initial states is shared by the algorithm and any benchmark policy. In contrast, in the non-episodic setting considered in this paper, the state trajectory of the benchmark policy over T time steps can be completely different from the algorithm’s trajectory. To the best of our understanding, the shared sequence of initial states and the fixed known lengthH of episodes seem to form crucial components of the analysis in Azar et al. [2017], making it difficult to extend their analysis to the non-episodic communicating MDP setting considered in this paper. Among other related work, Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] present optimistic linear programming approaches that achieve logarithmic regret bounds with problem dependent constants. Strong PAC bounds have been provided in Kearns and Singh [1999], Brafman and Tennenholtz [2002], Kakade et al. [2003], Asmuth et al. [2009], Dann and Brunskill [2015]. There, the aim is to bound the performance of the policy Worst-case regret is a strictly stronger notion of regret in case the reward distribution function is known and only the transition probability distribution is unknown, as we will assume here for the most part.",
      "startOffset" : 8,
      "endOffset" : 1783
    }, {
      "referenceID" : 27,
      "context" : "Strehl and Littman [2005], Strehl and Littman [2008] provide an optimistic algorithm for bounding regret in a discounted reward setting, but the definition of regret is slightly different in that it measures the difference between the rewards of an optimal policy and the rewards of the learning algorithm along the trajectory taken by the learning algorithm.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 27,
      "context" : "Strehl and Littman [2005], Strehl and Littman [2008] provide an optimistic algorithm for bounding regret in a discounted reward setting, but the definition of regret is slightly different in that it measures the difference between the rewards of an optimal policy and the rewards of the learning algorithm along the trajectory taken by the learning algorithm.",
      "startOffset" : 0,
      "endOffset" : 53
    }, {
      "referenceID" : 22,
      "context" : "For a communicating MDP M with diameter D: (a) (Puterman [2014] Theorem 8.",
      "startOffset" : 48,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "For a communicating MDP M with diameter D: (a) (Puterman [2014] Theorem 8.1.2, Theorem 8.3.2) The optimal (maximum) gain λ is state independent and is achieved by a deterministic stationary policy π, i.e., there exists a deterministic policy π such that λ := max s′∈S max π λ(s) = λ ∗ (s), ∀s ∈ S. Here, π is referred to as an optimal policy for MDP M. (b) (Tewari and Bartlett [2008], Theorem 4) The optimal gain λ satisfies the following equations, λ = min h∈RS max s,a rs,a + P T s,ah− hs = max a rs,a + P T s,ah ∗ − h∗s, ∀s (1) where h, referred to as the bias vector of MDP M, satisfies: max s hs −min s hs ≤ D.",
      "startOffset" : 48,
      "endOffset" : 385
    }, {
      "referenceID" : 2,
      "context" : ", using the techniques in Agrawal and Goyal [2013b].) The agent can use the past observations to learn the underlyingMDPmodel and decide future actions.",
      "startOffset" : 26,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : ", using the techniques in Agrawal and Goyal [2013b].) The agent can use the past observations to learn the underlyingMDPmodel and decide future actions. The goal is to maximize the total reward ∑T t=1 rst,at , or equivalently, minimize the total regret over a time horizon T , defined as R(T,M) := Tλ −∑Tt=1 rst,at (2) where λ is the optimal gain of MDP M. We present an algorithm for the learning agent with a near-optimal upper bound on the regret R(T,M) for any communicating MDP M with diameter D, thus bounding the worst-case regret over this class of MDPs. 3 Algorithm Description Our algorithm combines the ideas of Posterior sampling (aka Thompson Sampling) with the extended MDP construction used in Jaksch et al. [2010]. Below we describe the main components of our algorithm.",
      "startOffset" : 26,
      "endOffset" : 730
    }, {
      "referenceID" : 2,
      "context" : ", using the techniques in Agrawal and Goyal [2013b].) The agent can use the past observations to learn the underlyingMDPmodel and decide future actions. The goal is to maximize the total reward ∑T t=1 rst,at , or equivalently, minimize the total regret over a time horizon T , defined as R(T,M) := Tλ −∑Tt=1 rst,at (2) where λ is the optimal gain of MDP M. We present an algorithm for the learning agent with a near-optimal upper bound on the regret R(T,M) for any communicating MDP M with diameter D, thus bounding the worst-case regret over this class of MDPs. 3 Algorithm Description Our algorithm combines the ideas of Posterior sampling (aka Thompson Sampling) with the extended MDP construction used in Jaksch et al. [2010]. Below we describe the main components of our algorithm. Some notations: N t s,a denotes the total number of times the algorithm visited state s and played action a until before time t, and N t s,a(i) denotes the number of time steps among these N t s,a steps where the next state was i, i.e., a transition from state s to i was observed. We index the states from 1 to S, so that ∑S i=1 N t s,a(i) = N t s,a for any t. We use the symbol 1 to denote the vector of all 1s, and 1i to denote the vector with 1 at the i th coordinate and 0 elsewhere. Doubling epochs: Our algorithm uses the epoch based execution framework of Jaksch et al. [2010]. An epoch is a group of consecutive rounds.",
      "startOffset" : 26,
      "endOffset" : 1372
    }, {
      "referenceID" : 14,
      "context" : "Extended MDP: The policy π̃k to be used in epoch k is computed as the optimal policy of an extended MDP M̃k defined by the sampled transition probability vectors, using the construction of Jaksch et al. [2010]. Given sampled vectors Q s,a, j = 1, .",
      "startOffset" : 189,
      "endOffset" : 210
    }, {
      "referenceID" : 14,
      "context" : "This line of argument bears similarities to the analysis of UCRL2 in Jaksch et al. [2010], but with tighter deviation bounds that we are able to guarantee due to the use of posterior sampling instead of deterministic optimistic bias used in UCRL2.",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : "The derivation of this bound utilizes and extends the stochastic optimism technique from Osband et al. [2014]. For s, a with N τk s,a ≤ η, P̃s,a = Q s,a is a sample from the simple optimistic sampling, where we can only show the following weaker bound, but since this is used only while N τk s,a is small, the total contribution of this deviation will be small: max h∈[0,2D]S (P̃ k s,a − Ps,a)h ≤ Õ (",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 14,
      "context" : "Our algorithm may be viewed as a more efficient randomized version of the UCRL2 algorithm of Jaksch et al. [2010], with randomization via posterior sampling forming the key to the √ S factor improvement in the regret bound provided by our algorithm.",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 20,
      "context" : "Here, we utilize the stochastic optimism technique from Osband et al. [2014]. In Section D, we prove Lemma 4.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : "6) in Puterman [2014]).",
      "startOffset" : 6,
      "endOffset" : 22
    }, {
      "referenceID" : 31,
      "context" : "1 Dirichlet concentration A similar result as the lemma below for concentration of Dirichlet random vectors was proven in Osband and Van Roy [2016]. We include (an expanded version of) the proof for completeness.",
      "startOffset" : 122,
      "endOffset" : 148
    }, {
      "referenceID" : 31,
      "context" : "1 Dirichlet concentration A similar result as the lemma below for concentration of Dirichlet random vectors was proven in Osband and Van Roy [2016]. We include (an expanded version of) the proof for completeness. Lemma C.1 (Osband and Van Roy [2016]).",
      "startOffset" : 122,
      "endOffset" : 250
    }, {
      "referenceID" : 20,
      "context" : "Now, since they have the same mean, from equivalence condition for stochastic optimism (Condition 3 in Lemma 3 of Osband et al. [2014]) E[DYv − p̃ v|p̃ v] = 0 for all values of v, p̃ v.",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 20,
      "context" : "Now, since they have the same mean, from equivalence condition for stochastic optimism (Condition 3 in Lemma 3 of Osband et al. [2014]) E[DYv − p̂ v|p̂ v] = 0 for all values of v, p̃ v.",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 13,
      "context" : "Then we notice that (I − Q†π̃) is precisely the fundamental matrix of this absorbing Markov chain and hence exists and is non-negative (see Grinstead and Snell [2012], Theorem 11.",
      "startOffset" : 140,
      "endOffset" : 167
    }, {
      "referenceID" : 25,
      "context" : "E Useful deviation inequalities Fact 3 (Bernstein’s Inequality, from Seldin et al. [2012] Lem 11/Cor 12).",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "Fact 4 (Multiplicative Chernoff Bound, Kleinberg et al. [2008] Lemma 4.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 25,
      "context" : "56 (see Shevtsova [2010]).",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "Fact 7 (Abramowitz and Stegun [1964] 26.",
      "startOffset" : 8,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1).",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1). Let Y = PV for V ∈ [0, 1] fixed and P ∼ Dirichlet(α) with α ∈ R + and ∑S i=1 αi ≥ 2. Let X ∼ N(μ, σ) with μ = ∑S i=1 αiVi ∑ S i=1 αi , σ = ( ∑S i=1 αi) , thenX is stochastically optimistic for Y . Lemma E.4 (Gaussian vs Beta optimism, Osband et al. [2014] Lemma 6).",
      "startOffset" : 40,
      "endOffset" : 327
    }, {
      "referenceID" : 20,
      "context" : "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1). Let Y = PV for V ∈ [0, 1] fixed and P ∼ Dirichlet(α) with α ∈ R + and ∑S i=1 αi ≥ 2. Let X ∼ N(μ, σ) with μ = ∑S i=1 αiVi ∑ S i=1 αi , σ = ( ∑S i=1 αi) , thenX is stochastically optimistic for Y . Lemma E.4 (Gaussian vs Beta optimism, Osband et al. [2014] Lemma 6). Let Ỹ ∼ Beta(α, β) for any α, β > 0 and X ∼ N( α α+β , 1 α+β ). Then X is stochastically optimistic for Ỹ whenever α+ β ≥ 2. Lemma E.5 (Dirichlet vs Beta optimism, Osband et al. [2014] Lemma 5).",
      "startOffset" : 40,
      "endOffset" : 522
    }, {
      "referenceID" : 20,
      "context" : "3 (Gaussian vs Dirichlet optimism, from Osband et al. [2014] Lemma 1). Let Y = PV for V ∈ [0, 1] fixed and P ∼ Dirichlet(α) with α ∈ R + and ∑S i=1 αi ≥ 2. Let X ∼ N(μ, σ) with μ = ∑S i=1 αiVi ∑ S i=1 αi , σ = ( ∑S i=1 αi) , thenX is stochastically optimistic for Y . Lemma E.4 (Gaussian vs Beta optimism, Osband et al. [2014] Lemma 6). Let Ỹ ∼ Beta(α, β) for any α, β > 0 and X ∼ N( α α+β , 1 α+β ). Then X is stochastically optimistic for Ỹ whenever α+ β ≥ 2. Lemma E.5 (Dirichlet vs Beta optimism, Osband et al. [2014] Lemma 5). Let y = p v for some random variable p ∼ Dirichlet(α) and constants v ∈ Rd and α ∈ N . Without loss of generality, assume v1 ≤ v2 ≤ · · · ≤ vd. Let α̃ = ∑d i=1 αi(vi−v1)/(vd−v1) and β̃ = ∑d i=1 αi(vd−vi)/(vd− v1). Then, there exists a random variable p̃ ∼ Beta(α̃, β̃) such that, for ỹ = p̃vd + (1 − p̃)v1, E[ỹ|y] = E[y]. Lemma E.6. If E[X ] = E[Y ] and X is stochastically optimistic for Y , then −X is stochastically optimistic for −Y . Proof. By Lemma 3.3 in Osband et al. [2014], X stochastically optimistic for Y is equivalent to having X =D Y + A + W with A ≥ 0 and E[W |Y + A] = 0 for all values y + a.",
      "startOffset" : 40,
      "endOffset" : 1015
    }, {
      "referenceID" : 20,
      "context" : "Then stochastic optimism follows from applying the optimism equivalence condition from Lemma 3 (Condition 3) of Osband et al. [2014].",
      "startOffset" : 112,
      "endOffset" : 133
    } ],
    "year" : 2017,
    "abstractText" : "We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of Õ(D √ SAT ) for any communicating MDP with S states, A actions and diameter D, when T ≥ SA. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon T . This result improves over the best previously known upper bound of Õ(DS √ AT ) achieved by any algorithm in this setting, and matches the dependence on S in the established lower bound of Ω( √ DSAT ) for this problem. Our techniques involve proving some novel results about the anticoncentration of Dirichlet distribution, which may be of independent interest.",
    "creator" : "LaTeX with hyperref package"
  }
}