{
  "name" : "1206.4599.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Unified Robust Classification Model",
    "authors" : [ "Akiko Takeda", "Hiroyuki Mitsugi", "Takafumi Kanamori" ],
    "emails" : [ "takeda@ae.keio.ac.jp,", "kiyurohi7@z2.keio.jp", "kanamori@is.nagoya-u.ac.jp" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "There are a wide variety of machine learning algorithms for binary classification. Support vector machine (SVM) is one of the most successful classification algorithms in modern machine learning (Schölkopf & Smola, 2002). The minimax probability machine (MPM) (Lanckriet et al., 2002) and Fisher discriminant analysis (FDA) (Fukunaga, 1990) also address the binary classification problem. Their problem settings assume that only the mean and covariance matrix of each class are known. The optimal hyperplane of MPM is determined by minimizing the worst-case (maximum) probability of misclassification of unseen test samples over all possible class-conditional distributions. FDA is to find a direction which maximizes\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nthe projected class means while minimizing the class variance in this direction.\nThe purpose of this paper is to provide a unified framework for learning algorithms, including SVM, MPM, and FDA, from the viewpoint of robust optimization (Ben-Tal et al., 2009). Robust optimization is an approach that handles optimization problems defined by uncertain inputs. A simple example of robust optimization is\nmax w∈W min x∈U\nx > w, (1)\nwhere w is the parameter to be optimized under the constraint w ∈ W and x is an uncertain input in the problem. The uncertainty set U represents the uncertainty of the input. (1) determines the decision making parameter w which maximizes the benefit x>w for the worst-case setup among x ∈ U . For binary classification, we regard the means x+ and x− of the data points of each class as uncertain inputs and prepare uncertainty sets U+ and U− of those uncertain inputs. We assume that x of (1) exists in the Minkowski difference U of U+ and U−, i.e.,\nU = U+ U− := {x+ − x− ∣∣ x+ ∈ U+, x− ∈ U−},\nand define W by {w ∣∣ ‖w‖2 = 1}, where ‖ · ‖ is the Euclidean norm. Then we transform (1) into\nmax w:‖w‖2=1 min x+∈U+x−∈U−\n(x+ − x−)>w. (2)\nWe call it robust classification model (RCM)1. This problem always seems to be non-convex because ofW. However, it reduces to a convex problem that includes a constraint ‖w‖2 ≤ 1 instead of ‖w‖2 = 1 when U+ and U− do not intersect.\n1 Here we used the terminology of “robust” for the model (2) from the notion of “robust optimization”, not from the notion of “robust statistics”. The aim of the RCM is in providing a unified framework to existing learning methods, not in providing a learning method with better tolerance to outliers.\nIn this paper, we show that RCM (2) reduces to the learning methods mentioned above, depending on a prescribed uncertainty set U . For example, we show that MPM is a special case of (2) with an ellipsoidal uncertainty set U . When U+ and U− are defined as reduced convex hulls (Bennett & Bredensteiner, 2000), (2) reduces to ν-SVM (Schölkopf et al., 2000) if U+ ∩ U− = ∅ and reduces to Eν-SVM (Perez-Cruz et al., 2003), otherwise. The difference between these learning methods turns out only to be in the definition of U of (2). The first contribution of handling the unified model (2) is to obtain new learning methods. For example, we can obtain non-convex variants of MPM and FDA by mimicking Perez-Cruz et al.’s extension (Perez-Cruz et al., 2003) from convex ν-SVM to nonconvex Eν-SVM.\nThe second contribution is to provide theoretical results to above learning methods at once by dealing with the unified model (2). Indeed, we provide statistical interpretation for (2) on the basis of the conventional statistical learning theory. We show that (2) with some corresponding uncertainty set is a good approximation for the worst-case minimization of expected loss functions under uncertain probabilities.\nWe also provide a generalized local optimum search algorithm, that is applicable to non-convex variants of learning models. We prove theoretical results on the local optimum search algorithm.\nThe paper is organized as follows. In Section 2, we elucidate the unified model, RCM (2), for classification problems. In Section 3, we show RCM’s connection with existing learning algorithms and obtain nonconvex variants for MPM and FDA in the same way as non-convex Eν-SVM. In Section 4, we give a statistical interpretation of RCM in terms of minimizing the upper and lower bounds of the worst-case expected loss. In Section 5, we describe a local optimum search algorithm for non-convex RCM. We summarize our contributions and future work in Section 6."
    }, {
      "heading" : "2. Unified Robust Classification Model",
      "text" : ""
    }, {
      "heading" : "2.1. Problem Settings",
      "text" : "We shall start by introducing the problem setting and the notations. The observed training samples are denoted as (xi, yi) ∈ Rd×{+1,−1}, i ∈M := {1, ...,m}. Let M+ be the set of indices of training samples with the label +1; likewise for M−. Let |M+| = m+ and |M−| = m−, where | · | shows the size of the set. The goal of the classification task is to obtain a classi-\nfier that minimizes the prediction error rate for unseen test samples. For the sake of simplicity, we shall focus on linear classifiers, i.e., x>w + b where w (∈ Rd) is a vector and b (∈ R) is a bias parameter. Most of the discussions in this paper can be directly applied to kernel classifiers (Schölkopf & Smola, 2002). Concretely, the change from x ∈ X to the kernel function k(·,x) makes statements of Sections 2-4 hold for kernel classifiers, while the algorithm in Section 5 needs small modification.\nWe shall assume that the training samples are not reliable because of noise or measurement errors. To make a classification model less sensitive to noise in the training samples, we shall focus on representative points of each class, denoted by x+ and x−. These points are not necessarily individual samples, but may be means of the data points of each class. Since the training samples are not reliable, it is reasonable to assume that x+ and x− will involve some uncertainty. The largest possible sets of x+ and x− are denoted by U+ and U−, respectively, and these sets are defined on the basis of training samples. Throughout this paper, we will assume that both U+ and U− are convex and compact and that they have interior points. Then, their Minkowski difference U is convex and has a nonempty interior.\nThe way of constructing the uncertainty set U± is a very important issue in practice. If we set U too large in (2), the optimal decision is very robust to uncertain data x but too conservative. Moreover, if we define U with complicated functions, we cannot easily solve (2). Many robust optimization studies have used polyhedral sets and ellipsoidal sets as U for the sake of computational tractability. We show examples of U+ and U− in Section 3. We might possibly deal with more complicated problem setting beyond convex U+ and U− by using kernelization techniques."
    }, {
      "heading" : "2.2. Properties of RCM",
      "text" : "To geometrically interpret RCM (2), Figure 1 shows the ellipsoidal uncertainty sets U+, U− and their Minkowski difference. We can separate the problem (2) into two cases, i.e., whether U+ and U− have an intersection or not, which is equivalent to whether U includes 0 or not. As shown in Theorem 2.2, there is a large difference in computational effort between the two cases. Before giving an intuitive geometric interpretation of RCM in Theorem 2.2, we introduce Lemma 2.1 that further separates the case 0 ∈ U into two cases: U includes 0 in its interior, int(U), or on its boundary, bd(U). In the geometric sense, 0 6∈ U holds when U+ and U− are disjoint. 0 ∈ U implies that U+\nand U− are joint. In particular, 0 ∈ bd(U) implies that U+ and U− touch externally. Lemma 2.1. The optimal value of RCM (2) is positive if and only if 0 6∈ U . It is zero if and only if 0 ∈ bd(U), and it is negative if and only if 0 ∈ int(U).\nWe can prove “if” parts by using the supporting hyperplane theorem to three cases (0 6∈ U , 0 ∈ bd(U) and 0 ∈ int(U)). By taking the contrapositive of all the “if” parts, we also can prove “only if” parts.\nLet Uη be a parametrized uncertainty set for RCM (2) such that Uη1 ⊂ Uη2 holds for η1 ≤ η2. Then the following inequality holds:\nmax w:‖w‖2=1 min x∈Uη1\nx > w ≥ max w:‖w‖2=1 min x∈Uη2 x > w.\nThis indicates that the optimal value of (2) is nonincreasing with respect to the inclusion relation of uncertainty sets. Figure 1 (right) plots the non-increasing optimal value of RCM (2) with respect to η. An uncertainty set Uη2 might exist such that the optimal value of (2) becomes zero.\nThe following theorem shows that when 0 6∈ U , the equality constraint ‖w‖2 = 1 in (2) can be replaced by ‖w‖2 ≤ 1 without changing the solution. Moreover, ‖w‖2 = 1 can be replaced by ‖w‖2 ≥ 1 when 0 ∈ U . Figure 1 (left and middle) illustrates Theorem 2.2. Theorem 2.2. For an uncertainty set such that 0 6∈ U , RCM (2) is equivalent to\nmax w:‖w‖2≤1 min x∈U\nx > w. (3)\nMoreover, the problem is equivalent to\nmin x±∈U± ‖x+ − x−‖, or equivalently, min x∈U ‖x‖. (4)\nAn optimal w of (3) can be obtained from x∗/‖x∗‖ by using the optimal x∗ ∈ U of (4). For an uncertainty set such that 0 ∈ int(U), RCM (2) is equivalent to\nmax w:‖w‖2≥1 min x∈U\nx > w. (5)\nMoreover, the problem is equivalent to minx∈Uc ‖x‖, where Uc is the closure of the complement of the convex set U . An optimal w of (5) can be obtained from −x∗/‖x∗‖ by using the optimal x∗ ∈ Uc.\nProof. Assume 0 6∈ U . By applying the discussion on the minimum norm duality (Luenberger, 1969) to (3), we can confirm the equivalence of (3) and minx∈U ‖x‖, and the optimal solution w∗ = x∗/‖x∗‖. On the other hand, in the case of 0 ∈ int(U), the equivalence of (5) and minx∈Uc ‖x‖ is proved from Proposition 3.1 of (Briec, 1997) under the assumption that a convex U has a nonempty interior. Hence, it is enough to show that there exists an optimal solution w∗ of (3) (or (5)) such that ‖w∗‖ = 1, because the difference between (2) and (3) (or (5)) is only the norm constraint of w.\nLemma 2.1 ensures that the optimal value of (3) is positive, because\nmax w:‖w‖2≤1 min x∈U\nx > w ≥ max w:‖w‖2=1 min x∈U x > w > 0.\nSince the optimal solution w∗ of (3) satisfies 0 < ‖w∗‖ ≤ 1, the following inequalities hold:\n0 < min x∈U\nx > w ∗ ≤ min x∈U x > w ∗/‖w∗‖ ≤ min x∈U x > w ∗.\nThe last inequality comes from the optimality of w∗. These inequalities imply that w∗/‖w∗‖ is also an optimal solution of (3) and that ‖w∗‖ = 1. For the case of 0 ∈ int(U), we can similarly show that the optimal\nvalue of (5) is negative and that an optimal solution w ∗ of (5) exists such that ‖w∗‖ = 1.\nFor 0 ∈ int(U), RCM (2) is essentially a non-convex problem, and we need to use non-convex optimization methods to solve it. Section 5 describes an optimization algorithm for non-convex problems of (2)."
    }, {
      "heading" : "3. Equivalence to Existing Classifiers",
      "text" : "We will show that RCM can be reduced to support vector machine (SVM), minimax probability machine (MPM), or Fisher discriminant analysis (FDA) depending on the prescribed uncertainty set U . In Table 1, “×” means that the corresponding cases never happen. “ √ ” means that there are no corresponding existing models as far as we know. The models indicated by √ are the target in this paper.\nWe denote an optimal solution of (2) as w∗ and define the bias term b such that the decision boundary passes through the mid-point of x∗+ and x ∗ −, i.e., b = −(x∗+ + x ∗ −) > w\n∗/2. Here, x∗+ ∈ U+ and x∗− ∈ U− stand for the optimal solutions of the inner-minimization in (2) for w = w∗."
    }, {
      "heading" : "3.1. Hard-Margin SVM, ν-SVM and Eν-SVM",
      "text" : "Whenever a data set is linearly separable, there are many hyperplanes that correctly classify all training samples. Vapnik-Chervonenkis theory indicates that a large margin classifier has a small generalization error. The problem can be transformed into a quadratic programming problem and the classification method is called hard-margin support vector classification machine (HM-SVM). Here, we define the uncertainty set (convex hull, CH) as follows:\nU± = conv{xi ∣∣ i ∈M±}, (6)\nwhere conv means convex hull. By using the Wolfe duality, the equivalence of HM-SVM and RCM (4) is obvious for U+ ∩ U− = ∅. HM-SVM has been extended to cope with nonseparable data. C-SVM (Cortes & Vapnik, 1995) and ν-SVM (Schölkopf et al., 2000) are typical examples\nof “soft-margin” SVMs. There is a correspondence between C-SVM and ν-SVM. That is, the classifier estimated by C-SVM with C ∈ (0,∞) can be obtained from ν-SVM with a parameter ν ∈ (νmin, νmax] ⊂ [0, 1], and vice versa. Crisp and Burges (2000) showed νmax = 2min{m+,m−}/m and gave a geometric interpretation for νmin. For ν ∈ (νmax, 1], the optimization problem of ν-SVM is unbounded, and for ν ∈ [0, νmin), ν-SVM provides a trivial solution (w = 0 and b = 0). Perez-Cruz et al. (2003) devised extended ν-SVM (EνSVM) as a way of avoiding such a trivial solution:\nmin w,b,ξ,ρ − νρ + 1 m\nm∑\ni=1\nξi (7)\ns.t. yi(x > i w + b) ≥ ρ− ξi, ξi ≥ 0, i ∈M, ‖w‖2 = 1.\nBy forcing the norm of w to be unity, a non-trivial and meaningful solution is obtained for any ν ∈ [0, νmin), but this comes at the expense of convexity. It furthermore provides the same solution as ν-SVM for other values of ν. In that sense, Eν-SVM can be regarded as an extension of ν-SVM. It was experimentally found in (Perez-Cruz et al., 2003) that Eν-SVM often has better generalization performance than ν-SVM.\nIn order to connect (E)ν-SVM with RCM, we define Uν± as\n8\n<\n:\nX\ni∈M±\nλixi ˛ ˛\nX\ni∈M±\nλi = 1, 0 ≤ λi ≤ 2\nνm , i ∈ M±\n9\n=\n;\n. (8)\nThe set (8) is essentially equal to a reduced convex hull (RCH) (Bennett & Bredensteiner, 2000) or soft convex hull (Crisp & Burges, 2000). For linearly nonseparable data set, Uν+ and Uν− intersect with small ν. Crisp and Burges (2000) showed that νmin is the largest ν such that two RCHs, Uν+ and Uν−, intersect. The model that finds νmin corresponds to the case of 0 ∈ bd(Uν) in the “RCH” of Table 1. Barbero et al. (2012) transformed ν-SVM and Eν-SVM (7) into RCM (2) with Uν± in order to give them a geometric interpretation. Using the results, we can relate ν-SVM, Eν-SVM, and RCM (2) as shown in Table 1."
    }, {
      "heading" : "3.2. Minimax Probability Machine and Its Extension",
      "text" : "The minimax probability machine (MPM) only uses the mean and covariance matrix of each class for classification tasks (Lanckriet et al., 2002). Suppose that x+ (or x−) is a d-dimensional random vector with mean x̄+ (or x̄−) and covariance Σ+ (or Σ−). We assume that x̄+ 6= x̄− and that Σ± are positive definite. The MPM minimizes the misclassification probabili-\nties under the worst-case setting as follows:\nmax α,w,b α s.t. inf x±∼(x̄±,Σ±)\nPr{x>±w + b ≥ 0} ≥ α, (9)\nwhere x+ ∼ (x̄+,Σ+) refers to the class of distributions that have mean x̄+ and covariance Σ+, but are otherwise arbitrary; likewise for x−. In practice, the mean vectors and covariance matrices of each class are estimated from the training samples.\nLanckriet et al. (2002) represented problem (9) as a convex optimization problem known as a second-order cone program (SOCP) and show the dual form:\nmin κ\nκ s.t. 0 ∈ Uκ := Uκ+ Uκ−, (10)\nwhere Uκ± = {x̄± + Σ1/2± u ∣∣ ‖u‖ ≤ κ}. (11)\nα of (9) corresponds to κ of (10) as κ = √\nα/(1− α). Therefore, MPM (9) is the problem to find the smallest positive κ (denoted by κmax) such that the two ellipsoids intersect, i.e., 0 ∈ bd(Uκmax). The idea of MPM is combined with the idea of the margin maximization in (Nath & Bhattacharyya, 2007). Given acceptable false positive and negative rates, η+ and η−, the linear classifier can be estimated by\nmin w,b\n1 2 ‖w‖2 s.t. sup\nx±∼(x̄±,Σ±)\nPr{x>±w + b < 0} ≤ η±. (12)\nIn this paper, we call this model the “margin maximized MPM” (MM-MPM). In the same way as in MPM, (12) can be transformed into an SOCP.\nRobust optimization techniques for ellipsoidal uncertainty (Ben-Tal et al., 2009) transform RCM (2) with U± = Uκ±± into\nmin ‖w‖2=1\nκ+‖Σ 1/2 + w‖ + κ−‖Σ 1/2 − w‖ − (x̄+ − x̄−) > w. (13)\nWe define κmax+ and κ max − as constants such that U κ+ + and Uκ−− touch. For κ± ∈ [0, κmax± ), U κ+ + ∩ U κ− − = ∅ holds, and RCM (13) is equivalent to MM-MPM (12) with κ± = √ (1− η±)/η±. We can confirm this by comparing the dual form of MM-MPM and the dual of (13), that is equivalent to (4). Furthermore, (13) with κ± = κmax coincides with MPM (9) (see Table 1)."
    }, {
      "heading" : "3.3. Fisher Discriminant Analysis and Its Extension",
      "text" : "In Fisher discriminant analysis (FDA) as in MPM (9), a discriminant hyperplane is computed from the means and covariances of random vectors x+ and x−. The hyperplane is determined from the optimal solution w ∗ to the following problem (Fukunaga, 1990):\nmax w (x̄+ − x̄−)>w ‖(Σ+ + Σ−)1/2w‖ . (14)\nThe problem finds a direction which maximizes the projected class means while minimizing the class variance in this direction.\nLikewise for MPM, FDA has a probabilistic interpretation under the worst-case scenario. Using the ellipsoidal uncertainty set defined by\nUζ = {x = (x̄+ − x̄−) + (Σ+ + Σ−)1/2u ∣∣ ‖u‖ ≤ ζ},\n(15) FDA (14) can be represented as\nmin ζ\nζ s.t. 0 ∈ Uζ . (16)\nFDA can be extended to RCM (2) with the uncertainty set Uζ for a prescribed parameter ζ > 0. Let ζmax be the optimal value of (16). Then, along the same lines as the MPM in Section 3.2, we find that RCM (2) with U = Uζmax is equivalent to FDA. Indeed, RCM (2) with Uζ is transformed into\nmin ‖w‖2=1\nζ‖(Σ+ + Σ−)1/2w‖ − (x̄+ − x̄−)>w.\nEspecially for ζ ∈ [0, ζmax), the norm constraint is replaced with the convex constraint ‖w‖2 ≤ 1 without changing the optimal solution. Here, MM-FDA refers to this estimator. In replacing the Euclidean norm ‖w‖ with the L1-norm ‖w‖1, MM-FDA is equivalent to a sparse feature selection model based on FDA (FSFD) (Bhattacharyya, 2004)."
    }, {
      "heading" : "4. Statistical Interpretation for RCM",
      "text" : "We can give a statistical interpretation for RCM on the basis of statistical learning theory. Let us start by introducing a loss function ` : R→ R that defines the loss of the decision function x>w + b regarding the sample (x, y) as `(y(x>w + b)).\nA goal of the classification task is to obtain an accurate classifier. For this purpose, it is reasonable to minimize the expected loss, E[`(y(x>w + b))], with respect to w and b. Let us define p(x|y) as the conditional probability density of x, given the binary label y, and π+ and π− as the marginal probabilities of the positive and negative labels, respectively. E[`(y(x>w + b))] is computed by\nπ+ ∫\n`(x>w + b)p(x|+ 1)dx +π− ∫ `(−(x>w + b))p(x| − 1)dx.\nSince the true probability distribution is unknown, we cannot minimize the expected loss directly.\nNow let us consider the ambiguity of the probability distribution p(x|y). Let P+ and P− be sets of probability densities. Each set of probabilities expresses the uncertainty of the conditional probabilities p(x| + 1) and p(x| − 1), respectively. We can use the min-max decision rule for the uncertainty of p(x|y) as follows:\nmin w:‖w‖2=1 max p(x|±1)∈P± min b∈R\nE[`(y(x>w + b))]. (17)\nThe worst-case minimization problem is difficult to solve. Therefore, we propose to solve RCM (2), since we can prove that RCM (2) is a good approximation for minimizing the worst-case expected loss.\nTo relate (17) and RCM, we firstly give an equivalent formulation for RCM. Here, we define x+ and x− as the mean of the input vector x under the conditional probabilities p(x|+ 1) and p(x| − 1), respectively, i.e., x± = ∫ xp(x| ± 1)dx. Here, we assume that all probability distributions in P± have the mean vector. Let U+ and U− be\nU± = { ∫ xp(x| ± 1)dx ∣∣∣∣ p(x| ± 1) ∈ P± } . (18)\nSuppose that the uncertainty sets of probability densities, P±, are both convex; i.e., a mixture of two probability densities also lies in the uncertainty set. Then U+ and U− are convex sets. Theorem 4.1. Suppose that `(z) is a non-increasing function. An optimal solution of the RCM with the uncertainty sets U+ and U− in (18) is also optimal to\nmin w:‖w‖2=1 max x±∈U± min b∈R J`(w, b;x+,x−), (19)\nwhere\nJ`(w, b;x+,x−) = π+`(x > +w + b) + π−`(−x>−w − b).\nProof. For a fixed w and x± ∈ U±, minimizing J`(w, b;x+,x−) respect to b is equivalent to\nmin b′\nπ+`((x+ − x−)>w − b′) + π−`(b′).\nSince the objective function above is non-increasing in (x+ − x−)>w, there exists a non-increasing function φ(z) such that\nφ((x+ − x−)>w) = min b J`(w, b;x+,x−).\nHence, one has\nmin w:‖w‖2=1 max x±∈U± min b J`(w, b;x+,x−)\n= φ( max w:‖w‖2=1 min x±∈U±\n(x+ − x−)>w).\nAs a result, the optimal solution of the RCM is also optimal for problem (19).\nTheorem 4.2. We assume that i) `(z) is convex, decreasing, and second-order differentiable, and that ii) 0 ≤ `′′(z) ≤ L ∈ R holds for all z. Suppose that x± ∈ U± is in the ball with the radius c, i.e., ‖x±‖ ≤ c. Then, for the optimal value J∗ of (19), one has\nmin w:\n‖w‖2=1\nmax p(x|±1)∈P± min b∈R\nE[`(y(x>w+b))] ∈ [J∗, J∗+Lc 2\n2 ].\nProof. The convexity of `(z) leads to a lower bound, J`(w, b;x+,x−), and the Taylor expansions of `(z) around z = x>+w + b and z = x > −w + b yield an upper bound, J`(w, b;x+,x−) + Lc2 2 , of E[`(y(x >\nw + b))]. Even when the min-max operation is applied, J∗ and J∗ + Lc 2\n2 remain bounds for the worst-case expected loss (17).\nThe theorem implies that problem (19) minimizes the bounds of (17). Noticing that the optimal solution of problem (19) is available by solving RCM as shown in Theorem 4.1, Theorem 4.2 implies that RCM minimizes the upper and lower bounds of the worst-case expected loss (17) at the same time.\nThere are various ways to estimate the bias term b for RCM. The simplest way is to use b∗ = −(x∗+ + x ∗ −) > w\n∗/2. Another promising method is to construct an appropriate statistical model for the projected samples (x>i w\n∗, yi), i ∈M . The projected samples, x>i w∗, i ∈ M , are scattered in one-dimensional space, from which we can estimate b on the basis of the statistical model."
    }, {
      "heading" : "5. Solution Method for RCM",
      "text" : "The RCM has a significantly larger range of parameter κ or ζ than an existing convex model such as MPM, MM-MPM, FDA or FS-FD (see Table 1). Therefore, the RCM enhances a possibility of improving these existing classification models. Indeed, Perez-Cruz et al. (2003) experimentally showed that the generalization performance of Eν-SVM is often better than that of original ν-SVM. In this section, we propose a solution method that is generalized from the local algorithms of (Perez-Cruz et al., 2003; Takeda & Sugiyama, 2008)."
    }, {
      "heading" : "5.1. Two-stage Optimization Strategy",
      "text" : "Suppose that we solve RCM (2) with the uncertainty set Uη with one parameter η and that Uη1 ⊂ int(Uη2) holds for η1 < η2. Let us define ηmax such that the optimal value of (2) with U = Uηmax is zero. First, we need to compute ηmax in order to confirm that the given problem (2) is essentially convex or not.\nAlgorithm 5.1.\nThe parameter ηmax is obtained as the optimal solution of the convex problem:\nmin η\nη s.t. 0 ∈ Uη. (20)\nWhen Uη± are ellipsoidal sets of (11) (or (15)), the problem reduces to MPM (10) (or FDA (16)). When Uη± are RCHs, the problem reduces to a linear programming problem and gives us νmin.\nIf the input parameter η is equal to ηmax, we have already obtained an optimal solution from (20). If η < ηmax, we next solve the convex problem (4) by using a standard optimization software."
    }, {
      "heading" : "5.2. Local Optimization Algorithm for Non-convex RCM",
      "text" : "For η > ηmax, RCM (2) is essentially equivalent to (5) that includes a non-convex constraint, ‖w‖2 ≥ 1. We next need to solve (2) as a non-convex problem.\nIn the area of global optimization, non-convex RCM (5) (precisely, a problem constructed by taking dual for the inner-minimization in (5)) is known as a reverse convex program (RCP), or canonical d.c. programming. This differs from a conventional convex program only by the presence of a reverse convex constraint (‖w‖2 ≥ 1 in the current case). When all functions are linear except for the reverse convex constraint, the RCP problem is especially called linear reverse convex program (LRCP). Eν-SVM is an LRCP, for which Perez-Cruz et al. (2003) proposed a local optimum search algorithm and Takeda and Sugiyama (2008) proposed a global optimum search algorithm.\nHere, we show a local optimum search algorithm (Algorithm 5.1) that is generalized from the local algorithms (Perez-Cruz et al., 2003; Takeda & Sugiyama,\n2008) of Eν-SVM for non-convex RCM. It is essentially the same as the local algorithm, Algorithm 7, in (Takeda & Sugiyama, 2008) when U of g(w) is an RCH (8) and = 0.\nRCM (2) requires maximizing g(w) = minx∈U x > w subject to a non-convex constraint, w>w = 1. Instead of solving the non-convex problem directly, we can iteratively solve the relaxation problems (21) (in Algorithm 5.1). Since g(w) is concave, (21) can be solved by using convex minimization techniques.\nThe non-convex constraint of (2) is linearized at w̃t in the algorithm, and the linear constraint w̃>t w = 1 is updated every iteration. Note that the negativity of the optimal value of (21) is guaranteed because of 0 ∈ int(U). As the algorithm proceeds, the solutions w̃t improve, i.e.,\ng(w̃t) ≤ g(ŵ∗t ) < g(ŵ∗t /‖ŵ∗t ‖) = g(w̃t+1) < 0, (22)\nbecause w̃>t ŵ ∗ t = 1 together with w̃ > t w̃t = 1 implies ‖ŵ∗t ‖ > 1. Note that w̃t is a feasible solution for (21). Hence, if (21) has no better solutions than w̃t, w̃t is returned as an optimal solution ŵ ∗ t of (21). The algorithm terminates after that.\nThe computation of g(w) may be difficult for general uncertainty sets. However, we do not need an explicit formula for g(w) in (21). If U is a convex set, we can obtain a dual formulation (max-problem) for g(w) and replace the max-min problem (21) with a simple max-problem, that is, a one-level convex problem. Indeed, when the uncertainty set is an RCH (8) of data points, we can take the dual for g(w) = minx∈U x > w and change (21) into a linearized Eν-SVM (7) whose constraint is w̃>t w = 1 instead of ‖w‖2 = 1. When the algorithm is applied to the RCM having ellipsoidal uncertainty, we analytically obtain the optimal value g(w) for any w. Indeed, for ellipsoidal uncertainty (11), g(w) is equal to the one derived by multiplying the objective function of (13) by -1.\nTheorem 5.2. For any > 0, Algorithm 5.1 terminates in a finite number of iterations.\nProof. Let the negative value gopt be the optimal value of RCM (2). Suppose ‖ŵ∗t ‖ > 1 for all t = 1, 2, . . .. Otherwise, the algorithm terminates. By evaluating g(w̃t+1)− g(w̃t), we have\n∞∑\nt=0\n(g(w̃t+1)− g(w̃t)) ≥ ∞∑\nt=0\n( 1\n‖ŵ∗t ‖ − 1\n) gopt > 0.\nThe above inequality and the boundedness of g(w̃t) lead to limt→∞\n1 ‖ bw∗t ‖ − 1 = 0. Therefore, γt exists such that ‖ŵ∗t ‖ = 1 + γt, 0 < γt = o(1), that leads\nto ‖w̃t − ŵ∗t ‖ = √\n2γt + γ2t . Since γt → 0 holds, the stopping rule ‖w̃t−ŵ∗t ‖ ≤ with positive is satisfied in a finite number of iterations.\nWe can show that Algorithm 5.1 with = 0 terminates within a finite number of iterations when the uncertainty set of RCM (2) is represented by a convex polyhedron by mimicking the proof of Theorem 8 for Eν-SVM in (Takeda & Sugiyama, 2008).\nHere, suppose that g(w̃∗) is differentiable, i.e., g(w) has a unique subgradient at w̃∗ as ∂g(w̃∗) = arg minx∈U x > w̃\n∗ = {∇g(w̃∗)}. For example, g(w) is differentiable under ellipsoidal uncertainty (11) (or (15)). Then Theorem 5.3 shows a sufficient condition for the local optimality of the solution w̃∗ if it is obtained by Algorithm 5.1 with = 0.\nTheorem 5.3. Suppose that g(w̃∗) is differentiable. Algorithm 5.1 that terminates with = 0 provides a local solution w̃∗ to RCM (2) when the maximum eigenvalue of ∇2g(w̃∗) is less than g(w̃∗).\nProof. Note that w̃∗ is the optimal solution of (21) at the final iteration. Therefore, w̃∗ satisfies the firstand second-order necessary conditions:\n∇g(w̃∗) + ηw̃∗ = 0, (23) d >∇2g(w̃∗)d ≤ 0, ∀d such that w̃∗>d = 0, (24)\nwhere η is a Lagrange multiplier. We can show η = −g(w̃∗) > 0 by noticing that ∇g(w̃∗) is a minimizer to minx∈U x > w̃\n∗ = g(w̃∗) and using (23). When the maximum eigenvalue of ∇2g(w̃∗) is less than −η, w̃∗ satisfying (23) and (24) also satisfies the second-order sufficient conditions for the local optimality of (2):\n∇g(w) + 2ζw = 0, d>(∇2g(w) + 2ζI)d < 0, ∀d 6= 0 such that w>d = 0,\nwhere I is the identity matrix and ζ(≥ 0) is a multiplier. This shows that w̃∗ is a local solution of (2).\nTheorem 5.3 may be extendable to the nondifferentiable case of g(w̃∗), though more assumptions are necessary (see Theorems 3.2.16, 3.2.20 and 3.2.21 in (Polak, 1997))."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We developed the robust classification model (RCM), a model which includes SVM, MPM and FDA for specific uncertainty sets. The choice of uncertainty set is significant in this model. This model enables extensions and improvements to SVM to be applied to MPM and FDA, and vice versa.\nThe unified model will be of help in clarifying relationships among existing models and in finding new classifiers and new algorithms. That is, we might be able to devise a new classifier by finding a reasonable uncertainty set for RCM. It will be important to see how the learning algorithm, uncertainty set, and prediction accuracy relate to each other."
    } ],
    "references" : [ {
      "title" : "Geometric intuition and algorithms for Eν-svm",
      "author" : [ "Á. Barbero", "A. Takeda", "J. López" ],
      "venue" : null,
      "citeRegEx" : "Barbero et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Barbero et al\\.",
      "year" : 2012
    }, {
      "title" : "Robust Optimization",
      "author" : [ "A. Ben-Tal", "L. El-Ghaoui", "A. Nemirovski" ],
      "venue" : null,
      "citeRegEx" : "Ben.Tal et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ben.Tal et al\\.",
      "year" : 2009
    }, {
      "title" : "Duality and geometry in SVM classifiers",
      "author" : [ "K.P. Bennett", "E.J. Bredensteiner" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Bennett and Bredensteiner,? \\Q2000\\E",
      "shortCiteRegEx" : "Bennett and Bredensteiner",
      "year" : 2000
    }, {
      "title" : "Second order cone programming formulations for feature selection",
      "author" : [ "C. Bhattacharyya" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bhattacharyya,? \\Q2004\\E",
      "shortCiteRegEx" : "Bhattacharyya",
      "year" : 2004
    }, {
      "title" : "Minimum distance to the complement of a convex set: Duality result",
      "author" : [ "W. Briec" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "Briec,? \\Q1997\\E",
      "shortCiteRegEx" : "Briec",
      "year" : 1997
    }, {
      "title" : "A geometric interpretation of ν-SVM classifiers",
      "author" : [ "D.J. Crisp", "C.J.C. Burges" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Crisp and Burges,? \\Q2000\\E",
      "shortCiteRegEx" : "Crisp and Burges",
      "year" : 2000
    }, {
      "title" : "Introduction to statistical pattern recognition",
      "author" : [ "K. Fukunaga" ],
      "venue" : null,
      "citeRegEx" : "Fukunaga,? \\Q1990\\E",
      "shortCiteRegEx" : "Fukunaga",
      "year" : 1990
    }, {
      "title" : "A robust minimax approach to classification",
      "author" : [ "G.R.G. Lanckriet", "Ghaoui", "L. El", "C. Bhattacharyya", "M.I. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Lanckriet et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Lanckriet et al\\.",
      "year" : 2002
    }, {
      "title" : "Optimization by Vector Space Methods",
      "author" : [ "D.G. Luenberger" ],
      "venue" : null,
      "citeRegEx" : "Luenberger,? \\Q1969\\E",
      "shortCiteRegEx" : "Luenberger",
      "year" : 1969
    }, {
      "title" : "Maximum margin classifiers with specified false positive and false negative error rates",
      "author" : [ "J.S. Nath", "C. Bhattacharyya" ],
      "venue" : "In Proceedings of the seventh SIAM International Conference on Data mining,",
      "citeRegEx" : "Nath and Bhattacharyya,? \\Q2007\\E",
      "shortCiteRegEx" : "Nath and Bhattacharyya",
      "year" : 2007
    }, {
      "title" : "Extension of the ν-SVM range for classification",
      "author" : [ "F. Perez-Cruz", "J. Weston", "D.J.L. Hermann", "B. Schölkopf" ],
      "venue" : "In Advances in Learning Theory: Methods, Models and Applications",
      "citeRegEx" : "Perez.Cruz et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Perez.Cruz et al\\.",
      "year" : 2003
    }, {
      "title" : "Optimization: Algorithms and Consistent Approximations",
      "author" : [ "E. Polak" ],
      "venue" : null,
      "citeRegEx" : "Polak,? \\Q1997\\E",
      "shortCiteRegEx" : "Polak",
      "year" : 1997
    }, {
      "title" : "Learning with Kernels",
      "author" : [ "B. Schölkopf", "A.J. Smola" ],
      "venue" : null,
      "citeRegEx" : "Schölkopf and Smola,? \\Q2002\\E",
      "shortCiteRegEx" : "Schölkopf and Smola",
      "year" : 2002
    }, {
      "title" : "New support vector algorithms",
      "author" : [ "B. Schölkopf", "A. Smola", "R. Williamson", "P. Bartlett" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schölkopf et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 2000
    }, {
      "title" : "ν-support vector machine as conditional value-at-risk minimization",
      "author" : [ "A. Takeda", "M. Sugiyama" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Takeda and Sugiyama,? \\Q2008\\E",
      "shortCiteRegEx" : "Takeda and Sugiyama",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The minimax probability machine (MPM) (Lanckriet et al., 2002) and Fisher discriminant analysis (FDA) (Fukunaga, 1990) also address the binary classification problem.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : ", 2002) and Fisher discriminant analysis (FDA) (Fukunaga, 1990) also address the binary classification problem.",
      "startOffset" : 47,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "The purpose of this paper is to provide a unified framework for learning algorithms, including SVM, MPM, and FDA, from the viewpoint of robust optimization (Ben-Tal et al., 2009).",
      "startOffset" : 156,
      "endOffset" : 178
    }, {
      "referenceID" : 13,
      "context" : "When U+ and U− are defined as reduced convex hulls (Bennett & Bredensteiner, 2000), (2) reduces to ν-SVM (Schölkopf et al., 2000) if U+ ∩ U− = ∅ and reduces to Eν-SVM (Perez-Cruz et al.",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : ", 2000) if U+ ∩ U− = ∅ and reduces to Eν-SVM (Perez-Cruz et al., 2003), otherwise.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "’s extension (Perez-Cruz et al., 2003) from convex ν-SVM to nonconvex Eν-SVM.",
      "startOffset" : 13,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "By applying the discussion on the minimum norm duality (Luenberger, 1969) to (3), we can confirm the equivalence of (3) and minx∈U ‖x‖, and the optimal solution w = x∗/‖x∗‖.",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "1 of (Briec, 1997) under the assumption that a convex U has a nonempty interior.",
      "startOffset" : 5,
      "endOffset" : 18
    }, {
      "referenceID" : 13,
      "context" : "C-SVM (Cortes & Vapnik, 1995) and ν-SVM (Schölkopf et al., 2000) are typical examples of “soft-margin” SVMs.",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Crisp and Burges (2000) showed νmax = 2min{m+,m−}/m and gave a geometric interpretation for νmin.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "Crisp and Burges (2000) showed νmax = 2min{m+,m−}/m and gave a geometric interpretation for νmin. For ν ∈ (νmax, 1], the optimization problem of ν-SVM is unbounded, and for ν ∈ [0, νmin), ν-SVM provides a trivial solution (w = 0 and b = 0). Perez-Cruz et al. (2003) devised extended ν-SVM (EνSVM) as a way of avoiding such a trivial solution:",
      "startOffset" : 0,
      "endOffset" : 266
    }, {
      "referenceID" : 10,
      "context" : "It was experimentally found in (Perez-Cruz et al., 2003) that Eν-SVM often has better generalization performance than ν-SVM.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "Crisp and Burges (2000) showed that νmin is the largest ν such that two RCHs, Uν + and Uν −, intersect.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "Barbero et al. (2012) transformed ν-SVM and Eν-SVM (7) into RCM (2) with Uν ± in order to give them a geometric interpretation.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "The minimax probability machine (MPM) only uses the mean and covariance matrix of each class for classification tasks (Lanckriet et al., 2002).",
      "startOffset" : 118,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "Robust optimization techniques for ellipsoidal uncertainty (Ben-Tal et al., 2009) transform RCM (2) with U± = U± ± into",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "w ∗ to the following problem (Fukunaga, 1990): max w (x̄+ − x̄−)w ‖(Σ+ + Σ−)1/2w‖ .",
      "startOffset" : 29,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "In replacing the Euclidean norm ‖w‖ with the L1-norm ‖w‖1, MM-FDA is equivalent to a sparse feature selection model based on FDA (FSFD) (Bhattacharyya, 2004).",
      "startOffset" : 136,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : "In this section, we propose a solution method that is generalized from the local algorithms of (Perez-Cruz et al., 2003; Takeda & Sugiyama, 2008).",
      "startOffset" : 95,
      "endOffset" : 145
    }, {
      "referenceID" : 10,
      "context" : "Indeed, Perez-Cruz et al. (2003) experimentally showed that the generalization performance of Eν-SVM is often better than that of original ν-SVM.",
      "startOffset" : 8,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "Eν-SVM is an LRCP, for which Perez-Cruz et al. (2003) proposed a local optimum search algorithm and Takeda and Sugiyama (2008) proposed a global optimum search algorithm.",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "Eν-SVM is an LRCP, for which Perez-Cruz et al. (2003) proposed a local optimum search algorithm and Takeda and Sugiyama (2008) proposed a global optimum search algorithm.",
      "startOffset" : 29,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "1) that is generalized from the local algorithms (Perez-Cruz et al., 2003; Takeda & Sugiyama, 2008) of Eν-SVM for non-convex RCM.",
      "startOffset" : 49,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "21 in (Polak, 1997)).",
      "startOffset" : 6,
      "endOffset" : 19
    } ],
    "year" : 0,
    "abstractText" : "A wide variety of machine learning algorithms such as support vector machine (SVM), minimax probability machine (MPM), and Fisher discriminant analysis (FDA), exist for binary classification. The purpose of this paper is to provide a unified classification model that includes the above models through a robust optimization approach. This unified model has several benefits. One is that the extensions and improvements intended for SVM become applicable to MPM and FDA, and vice versa. Another benefit is to provide theoretical results to above learning methods at once by dealing with the unified model. We give a statistical interpretation of the unified classification model and propose a non-convex optimization algorithm that can be applied to nonconvex variants of existing learning methods.",
    "creator" : "LaTeX with hyperref package"
  }
}