{
  "name" : "1206.4623.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On the Size of the Online Kernel Sparsification Dictionary",
    "authors" : [ "Yi Sun", "Faustino Gomez", "Jürgen Schmidhuber" ],
    "emails" : [ "yi@idsia.ch", "tino@idsia.ch", "juergen@idsia.ch" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Kernel least squares (KLS) is a simple non-parametric regression method widely used in machine learning (e.g., see Schölkopf and Smola, 2002). Standard KLS requires storing and computing the (pseudo) inverse of the Gram matrix, and thus its complexity scales at least quadratically in the number of data points, rendering the method intractable for large data sets. In order to reduce the computational cost and avoid overfitting, it is common to replace the Gram matrix with a low rank approximation formed by projecting all samples1 onto the span of a chosen subset or dictionary of samples. Using such an approximated Gram matrix can greatly reduce the computational cost of KLS, sometimes to linear in the number of data points. Generally speaking, there are two approaches to constructing the dictionary. The first is the Nyström method\n1With a little abuse of notation, we use samples to refer to elements in some reproducing kernel Hilbert space (RKHS), i.e., samples are the images of the feature map of the data points.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\n(Williams and Seeger, 2000), where a randomly selected subset is used. The second, which is the concern of this paper, is called Online Kernel Sparsification (OKS; Engel et al. 2004), where the dictionary is built up incrementally by incorporating new samples that cannot be represented well (in the least squares sense) using the current dictionary.\nSince being proposed, OKS has found numerous applications in regression (Duy and Peters, 2010), classification (Slavakis et al., 2008) and reinforcement learning (Engel, 2005; Xu, 2006). Despite this empirical success, however, the theoretical understanding of OKS is still lacking. Most of the theoretical analysis has been done by Engel et al. (2004), who showed that the constructed dictionary is guaranteed to represent major fraction of the leading eigenvectors of the Gram matrix (Theorem 3.3, Engel et al. 2004). It was also proven that the dictionary stays finite if the set of possible samples is compact, and thus admits a finite covering number (Theorem 3.1, Engel et al. 2004). Yet, an important question remains open:\nHow does the size of the dictionary scale with the number of samples if the set of possible samples does not admit a finite covering number, or if the covering number is too large compared to the size of the data set?\nAnswering this question allows us to: (1) estimate the computational complexity of OKS, and therefore the associated KLS method, more accurately, and (2) characterize the generalization capability of the KLS regression function obtained, as the usual risk bounds are controlled by the quotient between the size of the dictionary and the number of samples (e.g., see Györfi et al., 2004).\nIn this paper, we address this question theoretically. Our analysis proceeds in two steps:\n1. We provide a novel formula expressing the expected Gram determinant over a set of i.i.d. samples in terms of the eigenvalues of the covariance operator. We then prove that the expected Gram\ndeterminant diminishes with the cardinality of the set faster than any exponential function.\n2. We observe that the Gram determinant over the OKS dictionary is lower bounded by some exponential function in the size of the dictionary. However, since step 1 concludes that the chance of a finding a big Gram matrix with large determinant is exceedingly small, the size of the dictionary must also stay small with high probability. Specifically, we show that the size of the dictionary will always grow sub-linearly in the number of data points, which implies consistency of KLS regressors constructed from the dictionary.\nThe rest of the paper is organized as follows: Section 2 describes the first step of our analysis, establishing a number of theoretical properties concerning the Gram determinant, including its expectation, decay, and moments. In section 3, we proceed to step 2, and analyze the growth of the size of the dictionary in OKS using the results from section 2. Section 4 briefly discusses the results and directions for future research."
    }, {
      "heading" : "2. The Determinant of a Gram matrix",
      "text" : "Let H be a separable Hilbert space endowed with inner product 〈·, ·〉, and P be a distribution over H. Assume Eφ∼P ‖φ‖2 < ∞, and let C = Eφ∼P [φ⊗ φ] be the (non-centered) covariance operator, where ⊗ denotes the tensor product. Let λ1 ≥ λ2 ≥ · · · be the eigenvalues of C sorted in descending order, then ∑ λi <∞ (Theorem 2.1, Blanchard et al. 2007).\nGiven i.i.d. samples φ1, . . . , φk ∼ P , let Gk (φ1:k) be2 the k×k Gram matrix with (i, j)-th entry 〈φi, φj〉, and let detGk be the determinant of Gk. Clearly, detGk is a random variable from Hk to R. Moreover, detGk has bounded expectation since from Hadamard’s inequality\n0 ≤ E [detGk] ≤ E [∏k\ni=1 ‖φi‖2\n] = ( E ‖φ‖2 )k .\nLet λ̃1 ≥ λ̃2 ≥ · · · ≥ λ̃k denote the eigenvalues of k−1Gk (and thus those of the empirical covariance op-\nerator C̃k = k−1 ∑k i=1 φi ⊗ φi) sorted in descending order. We assume the following condition. Assumption 1 limk→∞ ∑∞ i=1\n∣∣∣λ̃i − λi∣∣∣ = 0, a.s., where we take λ̃i = 0 for i > k.\nThe validity of this assumption will be discussed later in Section 4.1.\n2We use φ1:k as a short hand for φ1, . . . , φk."
    }, {
      "heading" : "2.1. A Formula for the Expectation of the Gram Determinant",
      "text" : "Before presenting our first main result (Theorem 1), we introduce some additional notation. The elementary symmetric polynomial3 of order k over n variables is defined as\nνn,k (λ1:n) = k! ∑\n1≤i1<i2<···<ik≤n\nλi1λi2 · · ·λik ,\nwhere the summation runs over all k-subsets of {1, . . . , n}. We denote the infinite extension of νn,k as\nνk (λ1, λ2, . . . ) = k! ∑\n1≤i1<i2<···<ik\nλi1λi2 · · ·λik ,\nwhenever the infinite sum exists. For simplicity, νk and νn,k denote both the function and their respective values with default argument (λ1, λ2, . . . ), and we only write down the arguments when they differ from (λ1, λ2, . . . ). Some of the useful properties of νn,k and νk are summarized in the following Lemma.\nLemma 1 We have\na) νn,k ≥ νn−1,k ≥ 0, and limn→∞ νn,k = νk.\nb) νn,k = kλnνn−1,k−1 + νn−1,k,\nc) ν2k ≥ νk−1νk+1 (Newton’s inequality),\nd) ν 1 k k ≥ ν 1 k+1 k+1 (Maclaurin’s inequality).\nProof. We only prove the limit in a) exists. The other properties can be derived easily using the limit argument and the properties of elementary symmetric polynomials (e.g., see Niculescu, 2000). In particular, c) is a direct consequence of Newton’s inequality, and d) is a rephrase of Maclaurin’s inequality.\nNote that νn,k is a non-decreasing sequence of n. Moreover,\nνn,k = k! ∑\n1≤i1<i2<···<ik≤n\nλi1 · · ·λik < k! ( n∑ i=1 λi )k is bounded because ∑ λi < ∞. Therefore the limit exists.\nNote that property b) enables us to compute νn,k in O (nk) time using dynamic programming. More precisely, this is done by initializing i) ν1,1 = λ1, ii) νi,1 = νi−1,1 + λi for i = 1, . . . , n, and iii) νi,i = iλiνi−1,i−1 for i = 1, . . . , k, and then applying the recursion in b).\nThe following theorem gives an explicit representation of the expectation of detGk in terms of the eigenvalues of C.\n3Note that the standard definition does not have the k! term.\nTheorem 1 E [detGk (φ1:k)] = νk\nThat is, the expectation of the determinant of a Gram matrix built from k samples is equal to the k−th order elementary symmetric polynomial over the eigenvalues of the covariance operator.\nProof. 4 Let φ1, . . . , φn ∼ P , and Gn = Gn (φ1:n) be the corresponding Gram matrix. Denote λ̃1, . . . , λ̃n the eigenvalues of n−1Gn, so that nλ̃i are the eigenvalues of Gn. The characteristic polynomial of Gn is given by f (λ) = det (Gn − λI). By definition,\nf (−λ) = n∏ i=1 (λ+ nλi)\n= n∑ k=0 nk  ∑ 1≤i1<···<ik≤n λi1 · · ·λik  · λn−k =\nn∑ k=0 nk νn,k\n( λ̃1:n ) k! · λn−k.\nAlternatively, we can express f (−λ) using the determinants of the principal submatrices (see for example Meyer, 2001, pp.494), which are Gram matrices by themselves:\nf (−λ) = n∑ k=0 ∑ I∈[n]k detGk (φI) · λn−k,\nwhere [n]k is the family of k-subsets in {1, . . . , n}, and φI denotes {φi}i∈I . Divide the coefficients before λn−k by binomial coefficient ( n k ) to get the identity:\n( n\nk )−1 ∑ I∈[n]k detGk (φI) = (n− k)!nk n! νn,k ( λ̃1:n ) .\nThe l.h.s. is a U-statistic (Serfling, 1980) with kernel detGk. Since E [detGk] <∞, the law of large numbers for U-statistics (Hoeffding, 1961) asserts that\nE [detGk] = lim n→∞\n( n\nk )−1 ∑ I∈[n]k detGk (φI) , a.s.\nNow consider the r.h.s. For the first term\nlim n→∞\n(n− k)!nk\nn! = lim n→∞\nn n− 1 · · · n n− k + 1 = 1.\n4An alternative proof may be derived using the generator function of E [detGk] (Martin, 2007). Unfortunately, the result is only briefly alluded to in the slides, and no detailed documentation has been made available up to now.\nFor the second term, we have\nνn,k ( λ̃1:n ) − νk\n= n∑ i=1 ( νn,k ( λ1:i−1, λ̃i:n ) − νn,k ( λ1:i, λ̃i+1:n )) + (νn,k (λ1, . . . , λn)− νk) .\nNote that∣∣∣νn,k (λ1:i−1, λ̃i:n)− νn,k (λ1:i, λ̃i+1:n)∣∣∣ = ∣∣∣k (λ̃i − λi) νn−1,k−1 (λ1:i−1, λ̃i+1:n)∣∣∣\n≤ k ∣∣∣λ̃i − λi∣∣∣ νn,k−1 (λ1:i−1, λi, λ̃i+1:n)\n≤ k ∣∣∣λ̃i − λi∣∣∣ ν∗n,k−1,\nwhere\nν∗n,k−1 = νn,k−1\n( max { λ̃1, λ1 } , . . . ,max { λ̃n, λn }) is bounded as ∑ max { λ̃i, λi\n} <∞. Therefore∣∣∣νn,k (λ̃1:n)− νk∣∣∣ ≤ kν∗n,k−1 n∑ i=1\n∣∣∣λ̃i − λi∣∣∣+ |νn,k (λ1, . . . , λn)− νk| → 0, a.s.\nThe first summand vanishes because of Assumption 1, and the second one diminishes because of Lemma 1 a). As a result,\nlim n→∞\n(n− k)!nk\nn! νn,k\n( λ̃1:n ) = νk.\n2.2. The Decaying Speed of E [detGk]\nIt is not immediately obvious how νk = E [detGk] behaves with increasing k. Here we provide a direct link between the speed with which νk approaches zero and the tail behavior of {λi}. The analysis is based on the following lemma.\nLemma 2 Let λ(0) = ∑ λj, and λ (k) = ∑ j>k λj. Then\nlog νk+s − log νk ≤ s log λ(k) + log ( k + s\nk\n) .\nProof. Note that\nνk+s = (k + s)! k!s! k! ∑\n1≤i1<···<ik\nλi1 · · ·λik · s! ∑\nik<j1<···js\nλj1 · · ·λjs\n=\n( k + s\nk\n) k! ∑\n1≤i1<···<ik\nλi1 · · ·λik · νs (λik+1, λik+2, . . . )\nSince λi is decreasing and ik ≥ k, we have for all ik\nνs (λik+1, λik+2, . . . ) ≤ νs (λk+1, λk+2, . . . ) ≤ (∑\nj>k λj\n)s ,\nwhere the last inequality is from Lemma 1 d). Therefore, νk+s ≤ ( k + s\nk\n)( λ(k) )s · νk.\nTaking the logarithm gives the desired result.\nAn immediate consequence is that νk converges to 0 faster than any exponential function.\nCorollary 1 For any α > 0, limk→∞ α −kνk = 0.\nProof. Assume k is fixed and s is large. From Stirling’s formula\nlog\n( k + s\nk\n) = k log ( 1 + s\nk\n) + s log ( 1 + k\ns ) +O (log s) < s+O (log s) ,\nwhere we use log (1 + x) < x for all x > −1.\nBy Lemma 2,\nlog νk+s − (k + s) logα ≤ s [ 1− logα+ log λ(k) ] +O (log s) .\nSince ∑ λi < ∞, we can pick a k∗ such that\nlog (∑\nj>k∗ λj\n) < −2 + logα, then\nlim k→∞ log νk − k logα = lim s→∞ (log νk∗+s − (k∗ + s) logα)\n< lim s→∞\n(−s+O (log s)) = −∞,\nand thus limk→∞ α −kνk = 0.\nRemark 1 We can also bound νk in terms of λ (k)\nusing Lemma 2. For exponential decay, i.e., λi ∼ O ( σ−i ) , we take s = 1, then\nlog νk < − k2\n2 log σ + log k! +O (k) .\nThe bound is tight since for λi = σ −i, direct computation gives\nνk = k! ∞∑ i1=1 λi1 · · · ∞∑ ik=ik−1+1 λik = k!σ−k∏k i=1 (σ i − 1) .\nTaking the logarithm and applying some algebra we get\nlog νk = − k2\n2 log σ + log k! +O (k) .\nFor polynomial decay, i.e., λi ∼ O ( i−(1+p) ) ,∑\ni≥k i −(1+p) ∼ k\n−p p , we set s = k, then\nlog ν2k − log νk ≤ k log λ(k) + log ( 2k\nk\n) .\nUsing Stirling’s formula,\nlog (2k)!− 2 log k! = k log 4 +O (log k) .\nTherefore,\nlog ν2k νk ≤ −pk log k + k log 4 p +O (log k) ,\nwhich characterizes the convergence of νk."
    }, {
      "heading" : "2.3. Bounding the Moments of the Gram Determinant",
      "text" : "In this section we prove a simple result concerning the moment E [(detGk)m], with the additional assumption thatH is the reproducing kernel Hilbert space (RKHS) associated with some bounded Mercer kernel ` (x, x′). Note that for any m ≥ 1, `(m) (x, x′) = (` (x, x′))m is still a bounded Mercer kernel. Let H(m) be the RKHS associated with `(m) and denote λ\n(m) 1 ≥ λ (m) 2 ≥ · · · the\neigenvalues of the corresponding covariance operator in H(m). We have the following bound. Theorem 2 E [(detGk)m] ≤ νk ( λ (m) 1 , λ (m) 2 , . . . ) for m = 2, 3, . . . .\nProof. Let A ◦B be the Hadamard product of A and B. We use the well-known fact: If A, B are positive semi-definite, then\ndet (A ◦B) ≥ det (A) det (B) .\nRepeating the process in the proof of Theorem 1, and applying\ndetG (m) k ≥ (detGk) m\ngives the result.\nRemark 2 Theorem 2 allows us to estimate empirically the bound of E [(detGk)m] without enumerating all subsets of size k. Moreover, for RBF and polynomial kernels, `(m) stays RBF and polynomial, respectively. However, it remains unknown how λ (m) i behaves in the general case.\nOn the Size of the Online Kernel Sparsification Dictionary"
    }, {
      "heading" : "3. Analyzing Online Kernel Sparsification",
      "text" : "In OKS, the dictionary D is initially empty. When a new sample φ arrives5, it is added to the dictionary if\ndetGD∪{φ}\ndetGD > α,\nwhere GD and GD∪{φ} are the respective Gram matrices of D and D ∪ {φ}, and α > 0 is a user-defined constant controlling the approximation error. Note that our notation is equivalent to the form originally proposed by Engel et al. (2004) as\ndetGD∪{φ}\ndetGD = 〈φ, φ〉 − g>G−Dg = min ψ∈spanD ‖φ− ψ‖2\nwhere g = [ 〈φ, φ1〉 , . . . , 〈 φ, φ|D| 〉]> for D ={\nφ1, . . . , φ|D| } , and G−D is the inverse of GD. The new φ can be added in O(|D|2) time if G−D is updated incrementally, for a total computational complexity O(|D|2 n) for n samples.\nOur analysis is based on the key observation that\ndetGD > α |D|.\nSince we have shown in the previous section that α−kE [detGk]→ 0, the chance of finding a subset with the property that detGD > α\n|D| will diminish as |D| grows, making a large dictionary unlikely.\nMore specifically, let φ1, . . . , φn be n i.i.d. samples from P , and let Dn be the dictionary constructed from φ1:n. Denote [n]k to be the family of all k-subsets of {1, . . . , n}. For A ∈ [n]k, let\nρk (φA) = I [ detGk (φA) > α k ] ,\nwhere I [·] is the indicator function. Define\nk∗n = argmax k  ∑ A∈[n]k ρk (φA) > 0  . Then clearly |Dn| < k∗n, and we may study k∗n instead of |D|. Intuitively, k∗n characterizes the dimensionality of the linear space spanned by φ1:n, because for any subset larger than k∗n there will be some φ which can be represented within error α by the linear combination of φ1:n.\nTo characterize k∗n we study P [k∗n ≥ k]. The following lemma shows that this probability is equal to the probability of the existence of k-subsets A with ρk(A) = 1.\n5In practice φ are often features in some RKHS induced by a kernel, and we store samples in the original domain. However we assume D is made of features for conceptual simplicity.\nLemma 3 P [k∗n ≥ k] = P [∑ A∈[n]k ρk (φA) > 0 ] .\nProof. By definition\nP  ∑ A∈[n]k ρk (φA) > 0  ≤ P  ⋃ k′≥k  ∑ A∈[n]k′ ρk′ (φA) > 0  \n= P [k∗n ≥ k] .\nTherefore the equality is not trivial.\nFrom Theorem 5 in Cover and Thomas (1988),( detGk+1 (φ1:k+1)\nαk+1\n) 1 k+1\n≤ 1 k + 1 ∑ A∈[k+1]k ( detGk (φA) αk ) 1 k .\nTherefore, ∑ A∈[n]k\nρk (φA) = 0 implies∑ A∈[n]k′ ρk′ (φA) = 0 for all k ′ ≥ k, and thus\nP  ∑ A∈[n]k ρk (φA) = 0  ≤ P  ⋂ k′≥k  ∑ A∈[n]k′ ρk′ (φA) = 0   .\nTaking the complement on both sides,\nP  ∑ A∈[n]k ρk (φA) > 0  ≥ P  ⋃ k′≥k  ∑ A∈[n]k′ ρk′ (φA) > 0  \n= P [k∗n ≥ k] .\nWe may now proceed to bound k∗n, using basic tools from probability theory.\nTheorem 3 P [|Dn| ≥ k] ≤ P [k∗n ≥ k] < α−k ( n k ) νk.\nProof. Note that\nE  ∑ A⊂[n]k ρk (φA)  = (n k ) E [ρk] = ( n k ) P [ detGk > α k ] .\nFrom Markov’s inequality,\nP [ detGk > α k ] <\nE [detGk] αk .\nIt then follows\nP [k∗n ≥ k] = P  ∑ A∈[n]k ρk (φA) ≥ 1  ≤ E\n ∑ A⊂[n]k ρk (φA)  < (n k ) E [detGk] αk .\nHere we use the fact that ρk is {0, 1}-valued, and apply Markov’s inequality again.\nNote that the proof only uses Markov’s inequality, which usually provides bounds that are by no means tight. The possibility of strengthening the bound is discussed in the next section. However, even with this simple analysis, some interesting results for the size of D can be obtained. The first is the following corollary.\nCorollary 2 For any ε ∈ (0, 1],\nlim n→∞ P [ k∗n n ≥ ε ] = 0.\nProof. For simplicity assume εn is an integer. Let k = nε, then (\nn\nk ) νk αk = ( ε−1k k ) νk αk .\nUsing Stirling’s formula,\nlog\n( ε−1k\nk\n) = log ( ε−1k ) !− log k!− log (( ε−1 − 1 ) k ) !\n= γk +O (log k) ,\nwhere\nγ = 1\nε log\n1 ε − ( 1 ε − 1 ) log ( 1 ε − 1 ) .\nTherefore, following Corollary 1 and Theorem 3,\nlim n→∞ P [ k∗n n ≥ ε ] < lim n→∞,k=εn ( n k ) E [detGk] αk\n= lim k→∞\n( ε−1k\nk ) νk αk = 0.\nRemark 3 By definition, Corollary 2 indicates that n−1k∗n → 0 in probability, or the size of the dictionary grows only sub-linearly with the number of samples. Assuming finite variance of the response variable, it immediately follows that the ordinary linear regressor constructed using features obtained from OKS is consistent, as the generalization error is controlled by n−1 |D| (e.g., see Györfi et al., 2004).\nThe next corollary provides a bound given a finite number of samples.\nCorollary 3 For arbitrary δ > 0 and\nn < αk\ne\n( δ\nνk\n) 1 k\n,\nwe have P [|Dn| > k] < δ.\nRemark 4 It is possible to give a bound in k rather than n. However, such a bound requires the inversion of νk and complicates the notation.\nProof. Assume n = ε−1k. Rewrite Theorem 3 as P [|Dε−1k| ≥ k] < α−k ( ε−1k\nk\n) νk.\nUsing the simple relation ( ε−1k k ) < ( ε−1e )k , we have\nlogα−k ( ε−1k\nk\n) νk < k (1− logα)− k log ε+ log νk.\nLetting the r.h.s. equal log δ, it follows that\nlog e\nα +\n1 k log νk δ = log ε, and ε = e α (νk δ ) 1 k .\nUsing Corollary 3, an upper bound on the dictionary size can be derived using {λi}, and the impact of α on the dictionary size can be analyzed.\nFrom the previous discussion, if λi ≤ σ−i, then\nνk ≤ k! (σ) −k∏k i=1 (σ i − 1) =\nk! (σ) −k\nσ k(k+1) 2 ∏k i=1 (1− σ−i) ,\nand some elementary manipulation gives\nk∑ i=1 log ( 1− σ−i ) > − σ (σ − 1)2 .\nTherefore,\n1 k log νk > − k 2 log σ + log k − 3 2 − log σ − 1 k\nσ\n(σ − 1)2 .\nPlugging this into Corollary 3, n < αβ δ 1 k σ k 2 , where β is some constant depending on σ, which implies k ∼ O (log (n)). Similarly, for polynomial decay n−(1+p), we have for large k\n1 k log ν2k νk < −p log k + log 4 p ,\nand then n > αδ 1 k k1+p. Therefore, the dictionary size grows approximately at the rate of n 1\n1+p . Note that the order of magnitude of these bounds coincides with the number of eigenvalues above certain threshold (Bach and Jordan, 2002, Table 3)."
    }, {
      "heading" : "4. Discussion",
      "text" : "This paper presented a rigorous theoretical analysis of how the dictionary in online kernel sparsification\nscales with respect to the number of samples, based on properties of the Gram matrix determinant. This work should lead to a better understanding of OKS, both in terms of its computational complexity, and the generalization capabilities associated with kernel least squares regressors. Three additional points are discussed below concerning a) the validity of Assumption 1, b) how our results relate to the Nyström method, and c) how the analysis can be potentially developed further."
    }, {
      "heading" : "4.1. On Assumption 1",
      "text" : "Under the mild condition Eφ∼P ‖φ‖2 < ∞, it can be seen that 〈·, ·〉 is a Mercer kernel in the sense of Definition 2.15 in Braun (2005), and subsequently by Theorem 3.26 therein, it follows that the δ2 distance\n(Koltchinskii and Giné 2000, pp. 116) between { λ̃i } and {λi} vanishes almost surely.\nHowever, the convergence of the spectrum in δ2 metric is insufficient for Theorem 1 to hold, and the stronger L1 convergence of the eigenspectrum is needed. It is possible to drop Assumption 1 altogether and base the discussion on limn→∞ νn,k instead, where the limit always exists and equals to E [detGk]. Otherwise, following the analysis by Gretton et al. (2009), we may provide sufficient conditions6 to Assumption 1 using the following extension of the Hoffman–Wielandt inequality (Theorem 3, Bhatia and Elsner 1994)∑\ni ∣∣∣λ̃i − λi∣∣∣ ≤ ∥∥∥C̃k − C∥∥∥ 1 ,\nwhere ‖·‖1 denotes the trace norm. Using Proposition 12 in Harchaoui et al. (2008), the convergence of∥∥∥C̃k − C∥∥∥\n1 to zero can be established provided that i)\nH is a separable RKHS (e.g., an RKHS induced by a continuous kernel over a separable metric space; Steinwart et al. 2006) induced by some bounded kernel, and\nii) the eigenspectrum of C satisfies ∑ i λ 1 2 i <∞."
    }, {
      "heading" : "4.2. Comparison with Nyström Method",
      "text" : "A similar approach to OKS for reducing the computational cost of kernel methods is the Nyström method (Williams and Seeger, 2000), where the dictionary consists of a subset of samples chosen at random. One distinction of the two methods, following from the analysis before, is that the dictionary from OKS satisfies detGD > α\n|D|, while the randomly selected subset D̃, satisfies detGD̃ α| D̃| for larger D. Therefore,\ndetGn detGD\ndetGn detGD̃ .\n6We thank the anonymous reviewers for pointing this out.\nFrom an information theoretic point of view, log detGndetGD̃ can be interpreted as the conditional entropy (Cover and Thomas, 1988), which indicates that D̃ captures less information about the data sets.\nThe theoretical study of the Nyström method by Drineas and Mahoney (2005) suggests that O ( α−4k ) samples are needed to approximate the first k eigenvectors well, which is linear in k, irrespective of the sample size. A recent study (Jin et al., 2012) shows that assuming bounded kernel, the spectral norm of the approximation error between the true and the approximated Gram matrix scales at a rate\nof O ( n |D|− 1 2 ) , and in the case of λi ∼ i−p, an\nO ( n |D|1−p ) rate may be obtained. In contrast, the\nresults in this paper are over the dictionary size |D|, and the approximation error is controlled by α. In particular, assuming bounded kernel, the (i, j)-th entry of the difference between the true and approximated Gram matrix using OKS is bounded by\n|〈φi, φj〉 − 〈ΠDφi,ΠDφj〉| < 2 sup ‖φ‖ √ α,\nwhere ΠD denotes the projection operator into the space spanned by D and the inequality follows from the Cauchy-Schwartz inequality. Using the fact that ‖A‖2 ≤ √ ‖A‖1 ‖A‖∞ for arbitrary matrix A, where ‖·‖2, ‖·‖1 and ‖·‖∞ respectively denote the spectral norm, maximum absolute column sum norm and maximum absolute row sum norm, we conclude that the spectral norm of the approximation error is controlled by O (n √ α), which is a non-probabilistic bound and does not explicitly depend on the dictionary size."
    }, {
      "heading" : "4.3. On Strengthening the Bound",
      "text" : "The proof of Theorem 2 uses Markov’s inequality to bound both P [ detGk > α k ] , and the probability of∑\nA∈[n]k ρk (A) 6= 0. In practice, this bound is hardly satisfying. One possibility is to strengthen the bound by incorporating information from higher order moments (Philips and Nelson, 1995), i.e.,\nP [ detGk > α k ] ≤ inf m∈{1,2,...} E [detGmk ] αkm\n≤ inf m∈{1,2,...}\nν ( λ (m) 1 , λ (m) 2 , . . . ) αkm .\nHowever, analyzing λ (m) i is difficult in general, and remains an open research question.\nIt is also possible to improve the second step, using concentration inequalities for configuration functions (Boucheron et al., 1999). Let ψ1, . . . , ψk be a subsequence of φ1:n. We say ψ1:k is α-compatible, if for\nj = 1, . . . , k,\ndetG{ψ1,...,ψj}\ndetG{ψ1,...,ψj−1} > α.\nNote that the dictionary constructed by OKS is αcompatible, and the property of α-compatibility is hereditary, i.e., ψ1:k being α-compatible implies that all sub-sequences are also α-compatible. To see this, let ψi1 , . . . , ψis be a sub-sequence of ψ1:k, then\ndetG{ψi1 ,...,ψis} detG{ψi1 ,...,ψis−1} = min ψ∈span{ψi1 ,...,ψis−1} ‖ψis − ψ‖ 2\n≥ min ψ∈span{ψ1,...,ψis−1} ‖ψis − ψ‖ 2\n= detG{ψ1,...,ψis}\ndetG{ψ1,...,ψis−1} > α.\nAs a result, let Zn denote the length of the longest subsequence in φ1:n that is α-compatible, then |Dn| < Zn. By Theorem 2 in Boucheron et al. (1999), Zn concentrates sharply around E [Zn]. Therefore, it is unlikely that |Dn| exceeds E [Zn] by much. However, providing tight bounds for E [Zn] is difficult and requires further study."
    } ],
    "references" : [ {
      "title" : "Kernel independent component analysis",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "JMLR, 3:1–48,",
      "citeRegEx" : "Bach and Jordan.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bach and Jordan.",
      "year" : 2002
    }, {
      "title" : "The Hoffman-Wielandt inequality in infinite dimensions",
      "author" : [ "R. Bhatia", "L. Elsner" ],
      "venue" : "Proc. Indian Acad. Sci. (Math. Sci),",
      "citeRegEx" : "Bhatia and Elsner.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bhatia and Elsner.",
      "year" : 1994
    }, {
      "title" : "Statistical properties of kernel principal component analysis",
      "author" : [ "G. Blanchard", "O. Bousquet", "L. Zwald" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Blanchard et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Blanchard et al\\.",
      "year" : 2007
    }, {
      "title" : "A sharp concentration inequality with applications",
      "author" : [ "S. Boucheron", "G. Lugosi", "P. Massart" ],
      "venue" : "Technical Report 376,",
      "citeRegEx" : "Boucheron et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Boucheron et al\\.",
      "year" : 1999
    }, {
      "title" : "Spectral properties of the kernel matrix and their relation to kernel methods in machine learning",
      "author" : [ "M.L. Braun" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Braun.,? \\Q2005\\E",
      "shortCiteRegEx" : "Braun.",
      "year" : 2005
    }, {
      "title" : "Determinant inequalities via information theory",
      "author" : [ "T.M. Cover", "J.A. Thomas" ],
      "venue" : "SIAM J. Matrix Anal. Appl.,",
      "citeRegEx" : "Cover and Thomas.,? \\Q1988\\E",
      "shortCiteRegEx" : "Cover and Thomas.",
      "year" : 1988
    }, {
      "title" : "On the Nyström method for approximating a Gram matrix for improved kernel-based learning",
      "author" : [ "P. Drineas", "M.W. Mahoney" ],
      "venue" : null,
      "citeRegEx" : "Drineas and Mahoney.,? \\Q2005\\E",
      "shortCiteRegEx" : "Drineas and Mahoney.",
      "year" : 2005
    }, {
      "title" : "Incremental sparsification for real-time online model learning",
      "author" : [ "N. Duy", "J. Peters" ],
      "venue" : "In AISTAT’10,",
      "citeRegEx" : "Duy and Peters.,? \\Q2010\\E",
      "shortCiteRegEx" : "Duy and Peters.",
      "year" : 2010
    }, {
      "title" : "Algorithms and representations for reinforcement learning",
      "author" : [ "Y. Engel" ],
      "venue" : "PhD thesis, Hebrew University,",
      "citeRegEx" : "Engel.,? \\Q2005\\E",
      "shortCiteRegEx" : "Engel.",
      "year" : 2005
    }, {
      "title" : "The kernel recursive least-squares algorithm",
      "author" : [ "Y. Engel", "S. Mannor", "R. Meir" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Engel et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Engel et al\\.",
      "year" : 2004
    }, {
      "title" : "A fast, consistent kernel twosample test",
      "author" : [ "A. Gretton", "K. Fukumizu", "Z. Harchaoui", "B.K. Sriperumbudur" ],
      "venue" : "In NIPS’09,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2009
    }, {
      "title" : "A distribution-free theory of nonparametric regression",
      "author" : [ "L. Györfi", "M. Kohler", "A. Krzyzak", "H. Walk" ],
      "venue" : null,
      "citeRegEx" : "Györfi et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Györfi et al\\.",
      "year" : 2004
    }, {
      "title" : "Testing for homogeneity with kernel Fisher discriminant analysis",
      "author" : [ "Z. Harchaoui", "F.R. Bach", "É. Moulines" ],
      "venue" : "In NIPS’08,",
      "citeRegEx" : "Harchaoui et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Harchaoui et al\\.",
      "year" : 2008
    }, {
      "title" : "The strong law of large numbers for U-statistics",
      "author" : [ "W. Hoeffding" ],
      "venue" : "Technical Report 302, Department of statistics,",
      "citeRegEx" : "Hoeffding.,? \\Q1961\\E",
      "shortCiteRegEx" : "Hoeffding.",
      "year" : 1961
    }, {
      "title" : "Improved bound for the Nyström method and its application to kernel classification",
      "author" : [ "R. Jin", "T.-B. Yang", "M. Mahdavi", "Y.-F. Li", "Z.H. Zhou" ],
      "venue" : "Technical Report arXiv:1111.2262v3,",
      "citeRegEx" : "Jin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2012
    }, {
      "title" : "Random matrix approximation of spectra of integral operators",
      "author" : [ "V. Koltchinskii", "E. Giné" ],
      "venue" : null,
      "citeRegEx" : "Koltchinskii and Giné.,? \\Q2000\\E",
      "shortCiteRegEx" : "Koltchinskii and Giné.",
      "year" : 2000
    }, {
      "title" : "The expected determinant of the random Gram matrix and its application to information retrieval",
      "author" : [ "J. Martin" ],
      "venue" : "URL http://dydan. rutgers.edu/Seminars/Slides/martin2.pdf",
      "citeRegEx" : "Martin.,? \\Q2007\\E",
      "shortCiteRegEx" : "Martin.",
      "year" : 2007
    }, {
      "title" : "Matrix analysis and applied linear algebra",
      "author" : [ "C.D. Meyer" ],
      "venue" : "SIAM: Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "Meyer.,? \\Q2001\\E",
      "shortCiteRegEx" : "Meyer.",
      "year" : 2001
    }, {
      "title" : "A new look at Newton’s inequalities",
      "author" : [ "C.P. Niculescu" ],
      "venue" : "Journal of Inequalities in Pure and Applied Mathematics,",
      "citeRegEx" : "Niculescu.,? \\Q2000\\E",
      "shortCiteRegEx" : "Niculescu.",
      "year" : 2000
    }, {
      "title" : "The moment bound is tighter than Chernoff’s bound for positive tail probabilities",
      "author" : [ "T.K. Philips", "R. Nelson" ],
      "venue" : "The American Statistician,",
      "citeRegEx" : "Philips and Nelson.,? \\Q1995\\E",
      "shortCiteRegEx" : "Philips and Nelson.",
      "year" : 1995
    }, {
      "title" : "Learning with kernels: support vector machines, regularization, optimization, and beyond",
      "author" : [ "B. Schölkopf", "A.J. Smola" ],
      "venue" : null,
      "citeRegEx" : "Schölkopf and Smola.,? \\Q2002\\E",
      "shortCiteRegEx" : "Schölkopf and Smola.",
      "year" : 2002
    }, {
      "title" : "Approximation theorems of mathematical statistics",
      "author" : [ "R.J. Serfling" ],
      "venue" : null,
      "citeRegEx" : "Serfling.,? \\Q1980\\E",
      "shortCiteRegEx" : "Serfling.",
      "year" : 1980
    }, {
      "title" : "Online kernel-based classification using adaptive projection algorithms",
      "author" : [ "K. Slavakis", "S. Theodoridis", "I. Yamada" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "Slavakis et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Slavakis et al\\.",
      "year" : 2008
    }, {
      "title" : "An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels",
      "author" : [ "I. Steinwart", "D. Hush", "C. Scovel" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Steinwart et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Steinwart et al\\.",
      "year" : 2006
    }, {
      "title" : "Using the Nyström method to speed up kernel machines",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "In NIPS’00,",
      "citeRegEx" : "Williams and Seeger.,? \\Q2000\\E",
      "shortCiteRegEx" : "Williams and Seeger.",
      "year" : 2000
    }, {
      "title" : "A sparse kernel-based least-squares temporal difference algorithm for reinforcement learning",
      "author" : [ "X. Xu" ],
      "venue" : "In Advances in Natural Computation,",
      "citeRegEx" : "Xu.,? \\Q2006\\E",
      "shortCiteRegEx" : "Xu.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "(Williams and Seeger, 2000), where a randomly selected subset is used.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 9,
      "context" : "The second, which is the concern of this paper, is called Online Kernel Sparsification (OKS; Engel et al. 2004), where the dictionary is built up incrementally by incorporating new samples that cannot be represented well (in the least squares sense) using the current dictionary.",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "Since being proposed, OKS has found numerous applications in regression (Duy and Peters, 2010), classification (Slavakis et al.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 22,
      "context" : "Since being proposed, OKS has found numerous applications in regression (Duy and Peters, 2010), classification (Slavakis et al., 2008) and reinforcement learning (Engel, 2005; Xu, 2006).",
      "startOffset" : 111,
      "endOffset" : 134
    }, {
      "referenceID" : 8,
      "context" : ", 2008) and reinforcement learning (Engel, 2005; Xu, 2006).",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 25,
      "context" : ", 2008) and reinforcement learning (Engel, 2005; Xu, 2006).",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "Since being proposed, OKS has found numerous applications in regression (Duy and Peters, 2010), classification (Slavakis et al., 2008) and reinforcement learning (Engel, 2005; Xu, 2006). Despite this empirical success, however, the theoretical understanding of OKS is still lacking. Most of the theoretical analysis has been done by Engel et al. (2004), who showed that the constructed dictionary is guaranteed to represent major fraction of the leading eigenvectors of the Gram matrix (Theorem 3.",
      "startOffset" : 73,
      "endOffset" : 353
    }, {
      "referenceID" : 21,
      "context" : "is a U-statistic (Serfling, 1980) with kernel detGk.",
      "startOffset" : 17,
      "endOffset" : 33
    }, {
      "referenceID" : 13,
      "context" : "Since E [detGk] <∞, the law of large numbers for U-statistics (Hoeffding, 1961) asserts that",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 16,
      "context" : "An alternative proof may be derived using the generator function of E [detGk] (Martin, 2007).",
      "startOffset" : 78,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "Note that our notation is equivalent to the form originally proposed by Engel et al. (2004) as",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "From Theorem 5 in Cover and Thomas (1988), ( detGk+1 (φ1:k+1) αk+1 ) 1 k+1 ≤ 1 k + 1 ∑",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "15 in Braun (2005), and subsequently by Theorem 3.",
      "startOffset" : 6,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "Otherwise, following the analysis by Gretton et al. (2009), we may provide sufficient conditions to Assumption 1 using the following extension of the Hoffman–Wielandt inequality (Theorem 3, Bhatia and Elsner 1994) ∑",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 23,
      "context" : "(2008), the convergence of ∥∥∥C̃k − C∥∥∥ 1 to zero can be established provided that i) H is a separable RKHS (e.g., an RKHS induced by a continuous kernel over a separable metric space; Steinwart et al. 2006) induced by some bounded kernel, and ii) the eigenspectrum of C satisfies ∑ i λ 1 2 i <∞.",
      "startOffset" : 109,
      "endOffset" : 208
    }, {
      "referenceID" : 12,
      "context" : "Using Proposition 12 in Harchaoui et al. (2008), the convergence of ∥∥∥C̃k − C∥∥∥ 1 to zero can be established provided that i) H is a separable RKHS (e.",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 24,
      "context" : "A similar approach to OKS for reducing the computational cost of kernel methods is the Nyström method (Williams and Seeger, 2000), where the dictionary consists of a subset of samples chosen at random.",
      "startOffset" : 102,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "From an information theoretic point of view, log detGn detGD̃ can be interpreted as the conditional entropy (Cover and Thomas, 1988), which indicates that D̃ captures less information about the data sets.",
      "startOffset" : 108,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : "A recent study (Jin et al., 2012) shows that assuming bounded kernel, the spectral norm of the approximation error between the true and the approximated Gram matrix scales at a rate of O ( n |D| 1 2 ) , and in the case of λi ∼ i−p, an",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : "The theoretical study of the Nyström method by Drineas and Mahoney (2005) suggests that O ( α−4k ) samples are needed to approximate the first k eigenvectors well, which is linear in k, irrespective of the sample size.",
      "startOffset" : 47,
      "endOffset" : 74
    }, {
      "referenceID" : 19,
      "context" : "One possibility is to strengthen the bound by incorporating information from higher order moments (Philips and Nelson, 1995), i.",
      "startOffset" : 98,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "It is also possible to improve the second step, using concentration inequalities for configuration functions (Boucheron et al., 1999).",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "By Theorem 2 in Boucheron et al. (1999), Zn concentrates sharply around E [Zn].",
      "startOffset" : 16,
      "endOffset" : 40
    } ],
    "year" : 2012,
    "abstractText" : "We analyze the size of the dictionary constructed from online kernel sparsification, using a novel formula that expresses the expected determinant of the kernel Gram matrix in terms of the eigenvalues of the covariance operator. Using this formula, we are able to connect the cardinality of the dictionary with the eigen-decay of the covariance operator. In particular, we show that under certain technical conditions, the size of the dictionary will always grow sublinearly in the number of data points, and, as a consequence, the kernel linear regressor constructed from the resulting dictionary is consistent.",
    "creator" : "LaTeX with hyperref package"
  }
}