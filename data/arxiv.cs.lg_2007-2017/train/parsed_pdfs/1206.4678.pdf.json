{
  "name" : "1206.4678.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Linear Regression with Limited Observation",
    "authors" : [ "Elad Hazan", "Tomer Koren" ],
    "emails" : [ "ehazan@ie.technion.ac.il", "tomerk@cs.technion.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art."
    }, {
      "heading" : "1. Introduction",
      "text" : "In regression analysis the statistician attempts to learn from examples the underlying variables affecting a given phenomenon. For example, in medical diagnosis a certain combination of conditions reflects whether a patient is afflicted with a certain disease.\nIn certain common regression cases various limitations are placed on the information available from the examples. In the medical example, not all parameters of a certain patient can be measured due to cost, time and patient reluctance.\nIn this paper we study the problem of regression in which only a small subset of the attributes per example can be observed. In this setting, we have access to all attributes and we are required to choose which of them to observe. Recently, Cesa-Bianchi et al. (2010)\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nstudied this problem and asked the following interesting question: can we efficiently learn the optimal regressor in the attribute efficient setting with the same total number of attributes as in the unrestricted regression setting? In other words, the question amounts to whether the information limitation hinders our ability to learn efficiently at all. Ideally, one would hope that instead of observing all attributes of every example, one could compensate for fewer attributes by analyzing more examples, but retain the same overall sample and computational complexity.\nIndeed, we answer this question on the affirmative for the main variants of regression: Ridge and Lasso. For support-vector regression we make significant advancement, reducing the parameter dependence by an exponential factor. Our results are summarized in the table below 1, which gives bounds for the number of examples needed to attain an error of ε, such that at most k attributes 2 are viewable per example. We denote by d the dimension of the attribute space.\nOur bounds imply that for reaching a certain accuracy, our algorithms need the same number of attributes as their full information counterparts. In particular, when k = Ω(d) our bounds coincide with those of full information regression, up to constants (cf. Kakade et al. 2008).\nWe complement these upper bounds and prove that Ω( dε2 ) attributes are in fact necessary to learn an ε-\n1The previous bounds are due to (Cesa-Bianchi et al., 2010). For SVR, the bound is obtained by additionally incorporating the methods of (Cesa-Bianchi et al., 2011).\n2For SVR, the number of attributes viewed per example is a random variable whose expectation is k.\naccurate Ridge regressor. For Lasso regression, CesaBianchi et al. (2010) proved that Ω(dε ) attributes are necessary, and asked what is the correct dependence on the problem dimension. Our bounds imply that the number of attributes necessary for regression learning grows linearly with the problem dimensions.\nThe algorithms themselves are very simple to implement, and run in linear time. As we show in later sections, these theoretical improvements are clearly visible in experiments on standard datasets."
    }, {
      "heading" : "1.1. Related work",
      "text" : "The setting of learning with limited attribute observation (LAO) was first put forth in (Ben-David & Dichterman, 1998), who coined the term “learning with restricted focus of attention”. Cesa-Biachi et al. (2010) were the first to discuss linear prediction in the LAO setting, and gave an efficient algorithm (as well as lower bounds) for linear regression, which is the primary focus of this paper."
    }, {
      "heading" : "2. Setting and Result Statement",
      "text" : ""
    }, {
      "heading" : "2.1. Linear regression",
      "text" : "In the linear regression problem, each instance is a pair (x, y) of an attributes vector x ∈ Rd and a target variable y ∈ R. We assume the standard framework of statistical learning (Haussler, 1992), in which the pairs (x, y) follow a joint probability distribution D over Rd×R. The goal of the learner is to find a vector w for which the linear rule ŷ ← w>x provides a good prediction of the target y. To measure the performance of the prediction, we use a convex loss function `(ŷ, y) : R2 → R. The most common choice is the squared loss `(ŷ, y) = 12 (ŷ−y)\n2, which stands for the popular leastsquares regression. Hence, in terms of the distribution D, the learner would like to find a regressor w ∈ Rd with low expected loss, defined as\nLD(w) = E(x, y)∼D[`(w >x, y)] . (1)\nThe standard paradigm for learning such regressor is seeking a vector w ∈ Rd that minimizes a trade-off between the expected loss and an additional regularization term, which is usually a norm of w. An equivalent form of this optimization problem is obtained by replacing the regularization term with a proper constraint, giving rise to the problem\nmin w∈Rd\nLD(w) s.t. ‖w‖p 6 B , (2)\nwhere B > 0 is a regularization parameter and p > 1. The main variants of regression differ on the type of\n`p norm constraint as well as the loss functions in the above definition:\n• Ridge regression: p = 2 and squared loss,\n`(ŷ, y) = 12 (ŷ − y) 2 .\n• Lasso regression: p = 1 and squared loss.\n• Support-vector regression: p = 2 and the δinsensitive absolute loss (Vapnik, 1995),\n`(ŷ, y) = |ŷ − y|δ := max{0, |ŷ − y| − δ} .\nSince the distribution D is unknown, we learn by relying on a training set S = {(xt, yt)}mt=1 of examples, that are assumed to be sampled independently from D. We use the notation `t(w) := `(w\n>xt, yt) to refer to the loss function induced by the instance (xt, yt).\nWe distinguish between two learning scenarios. In the full information setup, the learner has unrestricted access to the entire data set. In the limited attribute observation (LAO) setting, for any given example pair (x, y), the learner can observe y, but only k attributes of x (where k > 1 is a parameter of the problem). The learner can actively choose which attributes to observe."
    }, {
      "heading" : "2.2. Limitations on LAO regression",
      "text" : "Cesa-Biachi et al. (2010) proved the following sample complexity lower bound on any LAO Lasso regression algorithm.\nTheorem 2.1. Let 0 < ε < 116 , k > 1 and d > 4k. For any regression algorithm accessing at most k attributes per training example, there exist a distribution D over {x : ‖x‖∞ 6 1} × {±1} and a regressor w? with ‖w?‖1 6 1 such that the algorithm must see (in expectation) at least Ω( dkε ) examples in order to learn a linear regressor w with LD(w)− LD(w?) < ε.\nWe complement this lower bound, by providing a stronger lower bound on the sample complexity of any Ridge regression algorithm, using informationtheoretic arguments. Theorem 2.2. Let ε = Ω(1/ √ d). For any regression algorithm accessing at most k attributes per training example, there exist a distribution D over {x : ‖x‖2 6 1} × {±1} and a regressor w? with ‖w?‖2 6 1 such that the algorithm must see (in expectation) at least Ω( dkε2 ) examples in order to learn a linear regressor w, ‖w‖2 6 1 with LD(w)− LD(w?) 6 ε.\nOur algorithm for LAO Ridge regression (see section 3) imply this lower bound to be tight up to constants.\nNote, however, that the bound applies only to a particular regime of the problem parameters 3."
    }, {
      "heading" : "2.3. Our algorithmic results",
      "text" : "We give efficient regression algorithms that attain the following risk bounds. For our Ridge regression algorithm, we prove the risk bound\nE [LD(w̄)] 6 min ‖w‖26B LD(w) +O\n( B2 √ d\nkm\n) ,\nwhile for our Lasso regression algorithm we establish the bound\nE [LD(w̄)] 6 min ‖w‖16B LD(w) +O\n( B2 √ d log d\nkm\n) .\nHere we use w̄ to denote the output of each algorithm on a training set of m examples, and the expectations are taken with respect to the randomization of the algorithms. For Support-vector regression we obtain a risk bound that depends on the desired accuracy ε. Our bound implies that\nm = O\n( d\nk\n) · exp ( O ( log2 B\nε\n)) .\nexamples are needed (in expectation) for obtaining an ε-accurate regressor."
    }, {
      "heading" : "3. Algorithms for LAO least-squares regression",
      "text" : "In this section we present and analyze our algorithms for Ridge and Lasso regression in the LAO setting. The loss function under consideration here is the squared loss, that is, `t(w) = 1 2 (w\n>xt− yt)2. For convenience, we show algorithms that use k+1 attributes of each instance, for k > 1 4.\nOur algorithms are iterative and maintain a regressor wt along the iterations. The update of the regressor at iteration t is based on gradient information, and specifically on gt := ∇`t(wt) that equals (w>t xt− yt) · xt for the squared loss. In the LAO setting, however, we do not have the access to this information, thus we build upon unbiased estimators of the gradients.\n3Indeed, there are (full-information) algorithms that are known to converge in O(1/ε) rate – see e.g. (Hazan et al., 2007).\n4We note that by our approach it is impossible to learn using a single attribute of each example (i.e., with k = 0), and we are not aware of any algorithm that is able to do so. See (Cesa-Bianchi et al., 2011) for a related impossibility result.\nAlgorithm 1 AERR Parameters: B, η > 0 Input: training set S = {(xt, yt)}t∈[m] and k > 0 Output: regressor w̄ with ‖w̄‖2 6 B 1: Initialize w1 6= 0, ‖w1‖2 6 B arbitrarily 2: for t = 1 to m do 3: for r = 1 to k do 4: Pick it,r ∈ [d] uniformly and observe xt[it,r] 5: x̃t,r ← dxt[it,r] · eit,r 6: end for 7: x̃t ← 1k ∑k r=1 x̃t,r\n8: Choose jt ∈ [d] with probability wt[j]2/‖wt‖22, and observe xt[jt]\n9: φ̃t ← ‖wt‖22 xt[jt]/wt[jt]− yt 10: g̃t ← φ̃t · x̃t 11: vt ← wt − ηg̃t 12: wt+1 ← vt ·B/max{‖vt‖2, B} 13: end for 14: w̄← 1m ∑m t=1 wt"
    }, {
      "heading" : "3.1. Ridge regression",
      "text" : "Recall that in Ridge regression, we are interested in the linear regressor that is the solution to the optimization problem (2) with p = 2, given explicitly as\nmin w∈Rd\nLD(w) s.t. ‖w‖2 6 B , (3)\nOur algorithm for the LAO setting is based on a randomized Online Gradient Descent (OGD) strategy (Zinkevich, 2003). More specifically, at each iteration t we use a randomized estimator g̃t of the gradient gt to update the regressor wt via an additive rule. Our gradient estimators make use of an importance-sampling method inspired by (Clarkson et al., 2010).\nThe pseudo-code of our Attribute Efficient Ridge Regression (AERR) algorithm is given in Algorithm 1. In the following theorem, we show that the regressor learned by our algorithm is competitive with the optimal linear regressor having 2-norm bounded by B.\nTheorem 3.1. Assume the distribution D is such that ‖x‖2 6 1 and |y| 6 B with probability 1. Let w̄ be the output of AERR, when run with η = √ k/2dm. Then, ‖w̄‖2 6 B and for any w? ∈ Rd with ‖w?‖2 6 B,\nE [LD(w̄)] 6 LD(w?) + 4B 2\n√ 2d\nkm ."
    }, {
      "heading" : "3.1.1. Analysis",
      "text" : "Theorem 3.1 is a consequence of the following two lemmas. The first lemma is obtained as a result of a standard regret bound for the OGD algorithm (see Zinkevich 2003), applied to the vectors g̃1, . . . , g̃m.\nLemma 3.2. For any ‖w?‖2 6 B we have m∑ t=1 g̃>t (wt −w?) 6 2B2 η + η 2 m∑ t=1 ‖g̃t‖22 . (4)\nThe second lemma shows that the vector g̃t is an unbiased estimator of the gradient gt := ∇`t(wt) at iteration t, and establishes a “variance” bound for this estimator. To simplify notations, here and in the rest of the paper we use Et[·] to denote the conditional expectation with respect to all randomness up to time t.\nLemma 3.3. The vector g̃t is an unbiased estimator of the gradient gt := ∇`t(wt), that is Et[g̃t] = gt. In addition, for all t we have Et[‖g̃t‖22] 6 8B2d/k.\nFor a proof of the lemma, see (Hazan & Koren, 2011). We now turn to prove Theorem 3.1.\nProof (of Theorem 3.1). First note that as ‖wt‖2 6 B, we clearly have ‖w̄‖2 6 B. Taking the expectation of (4) with respect to the randomization of the algorithm, and letting G2 := maxtEt[‖g̃t‖22], we obtain\nE [ m∑ t=1 g>t (wt −w?) ] 6 2B2 η + η 2 G2m .\nOn the other hand, the convexity of `t gives `t(wt)− `t(w?) 6 g>t (wt − w?). Together with the above this implies that for η = 2B/G √ m,\nE\n[ 1\nm m∑ t=1 `t(wt)\n] 6 1\nm m∑ t=1 `t(w?) + 2 BG√ m .\nTaking the expectation of both sides with respect to the random choice of the training set, and using G 6 2B √ 2d/k (according to Lemma 3.3), we get\nE\n[ 1\nm m∑ t=1 LD(wt)\n] 6 LD(w?) + 4B 2 √ 2d\nkm .\nFinally, recalling the convexity of LD and using Jensen’s inequality, the Theorem follows."
    }, {
      "heading" : "3.2. Lasso regression",
      "text" : "We now turn to describe our algorithm for Lasso regression in the LAO setting, in which we would like to solve the problem\nmin w∈Rd\nLD(w) s.t. ‖w‖1 6 B . (5)\nThe algorithm we provide for this problem is based on a stochastic variant of the EG algorithm (Kivinen & Warmuth, 1997), that employs multiplicative updates\nAlgorithm 2 AELR Parameters: B, η > 0 Input: training set S = {(xt, yt)}t∈[m] and k > 0 Output: regressor w̄ with ‖w̄‖1 6 B 1: Initialize z+1 ← 1d , z − 1 ← 1d\n2: for t = 1 to m do 3: wt ← (z+t − z−t ) ·B/(‖z+t ‖1 + ‖z−t ‖1) 4: for r = 1 to k do 5: Pick it,r ∈ [d] uniformly and observe xt[it,r] 6: x̃t,r ← dxt[it,r] · eit,r 7: end for 8: x̃t ← 1k ∑k r=1 x̃t,r 9: Choose jt ∈ [d] with probability |w[j]|/‖w‖1, and observe xt[jt]\n10: φ̃t ← ‖wt‖1 sign(wt[jt])xt[jt]− yt 11: g̃t ← φ̃t · x̃t 12: for i = 1 to d do 13: ḡt[i]← clip(g̃t[i], 1/η) 14: z+t+1[i]← z + t [i] · exp(−η ḡt[i]) 15: z−t+1[i]← z − t [i] · exp(+η ḡt[i]) 16: end for 17: end for 18: w̄← 1m ∑m t=1 wt\nbased on an estimation of the gradients ∇`t. The multiplicative nature of the algorithm, however, makes it highly sensitive to the magnitude of the updates. To make the updates more robust, we “clip” the entries of the gradient estimator so as to prevent them from getting too large. Formally, this is accomplished via the following “clip” operation:\nclip(x, c) := max{min{x, c},−c}\nfor x ∈ R and c > 0. This clipping has an even stronger effect in the more general setting we consider in Section 4.\nWe give our Attribute Efficient Lasso Regression (AELR) algorithm in Algorithm 2, and establish a corresponding risk bound in the following theorem.\nTheorem 3.4. Assume the distribution D is such that ‖x‖∞ 6 1 and |y| 6 B with probability 1. Let w̄ be the output of AELR, when run with η = 14B2 √ 2k log 2d 5md , Then, ‖w̄‖1 6 B and for any w? ∈ Rd with ‖w?‖1 6 B we have\nE [LD(w̄)] 6 LD(w?) + 4B 2\n√ 10d log 2d\nkm ,\nprovided that m > log 2d."
    }, {
      "heading" : "3.2.1. Analysis",
      "text" : "In the rest of the section, for a vector v we let v2 denote the vector for which v2[i] = (v[i])2 for all i.\nIn order to prove Theorem 3.4, we first consider the augmented vectors z′t := (z + t , z − t ) ∈ R2d and ḡ′t := (ḡt,−ḡt) ∈ R2d, and let pt := z′t/‖z′t‖1. For these vectors, we have the following.\nLemma 3.5. m∑ t=1 p>t ḡ ′ t 6 min i∈[2d] m∑ t=1 ḡ′t[i] + log 2d η + η m∑ t=1 p>t (ḡ ′ t) 2\nThe lemma is a consequence of a second-order regret bound for the Multiplicative-Weights algorithm, essentially due to (Clarkson et al., 2010). By means of this lemma, we establish a risk bound with respect to the “clipped” linear functions ḡ>t w. Lemma 3.6. Assume that ‖Et[g̃2t ]‖∞ 6 G2 for all t, for some G > 0. Then, for any ‖w?‖1 6 B, we have\nE [ m∑ t=1 ḡ>t wt ] 6 E [ m∑ t=1 ḡ>t w? ] +B ( log 2d η + ηG2m ) Our next step is to relate the risk generated by the linear functions g̃>t w, to that generated by the “clipped” functions ḡ>t w. Lemma 3.7. Assume that ‖Et[g̃2t ]‖∞ 6 G2 for all t, for some G > 0. Then, for 0 < η 6 1/2G we have\nE [ m∑ t=1 g̃>t wt ] 6 E [ m∑ t=1 ḡ>t wt ] + 4BηG2m .\nThe final component of the proof is a “variance” bound, similar to that of Lemma 3.3.\nLemma 3.8. The vector g̃t is an unbiased estimator of the gradient gt := ∇`t(wt), that is Et[g̃t] = gt. In addition, for all t we have ‖Et[g̃t]2‖∞ 6 8B2d/k.\nFor the complete proofs, refer to (Hazan & Koren, 2011). We are now ready to prove Theorem 3.4.\nProof (of Theorem 3.4). Since ‖wt‖1 6 B for all t, we obtain ‖w̄‖2 6 B. Next, note that as Et[g̃t] = gt, we have E[ ∑m t=1 g̃ > t wt] = E[ ∑m t=1 g > t wt]. Putting Lemmas 3.6 and 3.7 together, we get for η 6 1/2G that\nE [ T∑ t=1 g>t (wt −w?) ] 6 B ( log 2d η + 5ηG2m ) .\nProceeding as in the proof of Theorem 3.1, and choos-\ning η = 1G √ log 2d 5m , we obtain the bound\nE [LD(w̄)] 6 LD(w?) + 2BG\n√ 5 log 2d\nm .\nNote that for this choice of η we indeed have η 6 1/2G, as we originally assumed that m > log 2d. Finally, putting G = 2B √ 2d/k as implied by Lemma 3.8, we obtain the bound in the statement of the theorem."
    }, {
      "heading" : "4. Support-vector regression",
      "text" : "In this section we show how our approach can be extended to deal with loss functions other than the squared loss, of the form\n`(w>x, y) = f(w>x− y) , (6)\n(with f real and convex) and most importantly, with the δ-insensitive absolute loss function of SVR, for which f(x) = |x|δ := max{|x| − δ, 0} for some fixed 0 6 δ 6 B (recall that in our results we assume the labels yt have |yt| 6 B). For concreteness, we consider only the 2-norm variant of the problem (as in the standard formulation of SVR)—the results we obtain can be easily adjusted to the 1-norm setting. We overload notation, and keep using the shorthand `t(w) := `(w\n>xt, yt) for referring the loss function induced by the instance (xt, yt).\nIt should be highlighted that our techniques can be adapted to deal with many other common loss functions, including “classification” losses (i.e., of the form `(w>x, y) = f(y · w>x)). Due to its importance and popularity, we chose to describe our method in the context of SVR.\nUnfortunately, there are strong indications that SVR learning (more generally, learning with non-smooth loss function) in the LAO setting is impossible via our approach of unbiased gradient estimations (see CesaBianchi et al. 2011 and the references therein). For that reason, we make two modifications to the learning setting: first, we shall henceforth relax the budget constraint to allow k observed attributes per instance in expectation; and second, we shall aim for biased gradient estimators, instead of unbiased as before.\nTo obtain such biased estimators, we uniformly εapproximate the function f by an analytic function fε and learn with the approximate loss function `εt (w) = fε(w\n>xt − yt) instead. Clearly, any εsuboptimal regressor of the approximate problem is an 2ε-suboptimal regressor of the original problem. For learning the approximate problem we use a novel technique, inspired by (Cesa-Bianchi et al., 2011), for estimating gradients of analytic loss functions. Our estimators for∇`εt can then be viewed as biased estimators of ∇`t (we note, however, that the resulting bias might be quite large).\nProcedure 3 GenEst Parameters: {an}∞n=0 — Taylor coefficients of f ′ Input: regressor w, instance (x, y) Output: φ̂ with E[φ̂] = f ′(w>x− y) 1: Let N = d4B2e. 2: Choose n > 0 with probability Pr[n] = (12 ) n+1\n3: if n 6 2 log2N then 4: for r = 1, . . . , n do 5: Choose j ∈ [d] with probability w[j]2/‖w‖22, and observe x[j] 6: θ̃r ← ‖w‖22 x[j]/w[j]− y 7: end for 8: else 9: for r = 1, . . . , n do\n10: Choose j1, . . . , jN ∈ [d] w.p. w[j]2/‖w‖22, (independently), and observe x[j1], . . . ,x[jN ]\n11: θ̃r ← 1N ∑N s=1‖w‖22 x[js]/w[js]− y 12: end for 13: end if 14: φ̂← 2n+1an · θ̃1θ̃2 · · · θ̃n"
    }, {
      "heading" : "4.1. Estimators for analytic loss functions",
      "text" : "Let f : R → R be a real, analytic function (on the entire real line). The derivative f ′ is thus also analytic and can be expressed as f ′(x) = ∑∞ n=0 anx\nn where {an} are the Taylor expansion coefficients of f ′.\nIn Procedure 3 we give an unbiased estimator of f ′(w>x − y) in the LAO setting, defined in terms of the coefficients {an} of f ′. For this estimator, we have the following (proof is omitted).\nLemma 4.1. The estimator φ̂ is an unbiased estimator of f ′(w>x − y). Also, assuming ‖x‖2 6 1, ‖w‖2 6 B and |y| 6 B, the second-moment E[φ̂2] is upper bounded by exp(O(log2B)), provided that the Taylor series of f ′(x) converges absolutely for |x| 6 1. Finally, the expected number of attributes of x used by this estimator is no more than 3."
    }, {
      "heading" : "4.2. Approximating SVR",
      "text" : "In order to approximate the δ-insensitive absolute loss function, we define\nfε(x) = ε\n2 ρ ( x− δ ε ) + ε 2 ρ ( x+ δ ε ) − δ\nwhere ρ is expressed in terms of the error function erf,\nρ(x) = x erf(x) + 1√ π e−x 2 ,\nand consider the approximate loss functions `εt (w) = fε(w >xt − yt). Indeed, we have the following.\nAlgorithm 4 AESVR Parameters: B, δ, η > 0 and accuracy ε > 0 Input: training set S = {(xt, yt)}t∈[m] and k > 0 Output: regressor w̄ with ‖w̄‖2 6 B 1: Let a2n = 0 for n > 0, and\na2n+1 = 2√ π · (−1) n n!(2n+ 1) , n > 0 (7)\n2: Execute algorithm 1 with lines 8–9 replaced by: x′t ← xt/ε y+t ← (yt + δ)/ε, y−t ← (yt − δ)/ε φ̃t ← 12 [GenEst(wt,x ′ t, y + t ) + GenEst(wt,x ′ t, y − t )] 3: Return the output w̄ of the algorithm\nClaim 4.2. For any ε > 0, fε is convex, analytic on the entire real line and\nsup x∈R |fε(x)− |x|δ| 6 ε .\nThe claim follows easily from the identity |x|δ = 12 |x− δ| + 12 |x + δ| − δ. In addition, for using Procedure 3 we need the following simple observation, that follows immediately from the series expansion of erf(x).\nClaim 4.3. ρ′(x) = ∑∞ n=0 a2n+1x\n2n+1, with the coefficients {a2n+1}n>0 given in (7).\nWe now give the main result of this section, which is a sample complexity bound for the Attribute Efficient SVR (AESVR) algorithm, given in Algorithm 4.\nTheorem 4.4. Assume the distribution D is such that ‖x‖2 6 1 and |y| 6 B with probability 1. Then, for any w? ∈ Rd with ‖w?‖2 6 B, we have E [LD(w̄)] 6 LD(w?) + ε where w̄ is the output of AESVR (with η properly tuned) on a training set of size\nm = O\n( d\nk\n) · exp ( O ( log2 B\nε\n)) . (8)\nThe algorithm queries at most k+ 6 attributes of each instance in expectation.\nProof. First, note that for the approximate loss functions `εt we have\n∇`εt (wt) = 12 [ ρ′(w>t x ′ t − y+t ) + ρ′(w>t x′t − y−t ) ] · xt .\nHence, Lemma 4.1 and Claim 4.3 above imply that g̃t in Algorithm 4 is an unbiased estimator of ∇`εt (wt). Furthermore, since ‖x′t‖2 6 1ε and |y ± t | 6 2Bε , according to the same lemma we have Et[φ̃ 2 t ] = exp(O(log2 Bε )). Repeating the proof of Lemma 3.3,\nwe then have\nEt[‖g̃t‖22] = Et[φ̃2t ] ·Et[‖x̃t‖22] = exp(O(log 2 B ε )) ·\nd k .\nReplacing G2 in the proof of theorem 3.1 with the above bound, we get for the output of Algorithm 4,\nE [LD(w̄)] 6 LD(w?) + exp(O(log 2 B ε ))\n√ d\nkm ,\nwhich imply that for obtaining an ε-accurate regressor w̄ of the approximate problem, it is enough to take m as given in (8). However, claim 4.2 now gives that w̄ itself is an 2ε-accurate regressor of the original problem, and the proof is complete."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section we give experimental evidence that support our theoretical bounds, and demonstrate the superior performance of our algorithms compared to the state of the art. Naturally, we chose to compare our AERR and AELR algorithms 5 with the AER algorithm of (Cesa-Bianchi et al., 2010). We note that AER is in fact a hybrid algorithm that combines 1- norm and 2-norm regularizations, thus we use it for benchmarking in both the Ridge and Lasso settings.\nWe essentially repeated the experiments of (CesaBianchi et al., 2010) and used the popular MNIST digit recognition dataset (LeCun et al., 1998). Each instance in this dataset is a 28 × 28 image of a handwritten digit 0− 9. We focused on the “3 vs. 5” task, on a subset of the dataset that consists of the “3” digits (labeled −1) and the “5” digits (labeled +1). We applied the regression algorithms to this task by regressing to the labels.\nIn all our experiments, we randomly split the data to training and test sets, and used 10-fold cross-validation for tuning the parameters of each algorithm. Then, we ran each algorithm on increasingly longer prefixes of the dataset and tracked the obtained squared-error on the test set. For faithfully comparing partial- and full-information algorithms, we also recorded the total number of attributes used by each algorithm.\nIn our first experiment, we executed AELR, AER and (offline) Lasso on the “3 vs. 5” task. We allowed both AELR and AER to use only k = 4 pixels of each training image, while giving Lasso unrestricted access to the entire set of attributes (total of 784) of each instance. The results, averaged over 10 runs on\n5The AESVR algorithm is presented mainly for theoretical considerations, and was not implemented in the experiments.\nrandom train/test splits, are presented in Figure 1. Note that the x-axis represents the cumulative number of attributes used for training. The graph ends at roughly 48500 attributes, which is the total number of attributes allowed for the partial-information algorithms. Lasso, however, completes this budget after seeing merely 62 examples.\nAs we see from the results, AELR keeps its test error significantly lower than that of AER along the entire execution, almost bridging the gap with the fullinformation Lasso. Note that the latter has the clear advantage of being an offline algorithm, while both AELR and AER are online in nature. Indeed, when we compared AELR with an online Lasso solver, our algorithm obtained test error almost 10 times better.\nIn the second experiment, we evaluated AERR, AER and Ridge regression on the same task, but now allowing the partial-information algorithms to use as much as k = 56 pixels (which amounts to 2 rows) of each instance. The results of this experiment are given in Figure 2. We see that even if we allow the algorithms to view a considerable number of attributes, the gap between AERR and AER is large."
    }, {
      "heading" : "6. Conclusions and Open Questions",
      "text" : "We have considered the fundamental problem of statistical regression analysis, and in particular Lasso and Ridge regression, in a setting where the observation upon each training instance is limited to a few attributes, and gave algorithms that improve over the state of the art by a leading order term with respect to the sample complexity. This resolves an open question of (Cesa-Bianchi et al., 2010). The algorithms are efficient, and give a clear experimental advantage in\npreviously-considered benchmarks.\nFor the challenging case of regression with general convex loss functions, we describe exponential improvement in sample complexity, which apply in particular to support-vector regression.\nIt is interesting to resolve the sample complexity gap of 1ε which still remains for Lasso regression, and to improve upon the pseudo-polynomial factor in ε for support-vector regression. In addition, establishing analogous bounds for our algorithms that hold with high probability (other than in expectation) appears to be non-trivial, and is left for future work.\nAnother possible direction for future research is adapting our results to the setting of learning with (randomly) missing data, that was recently investigated— see e.g. (Rostamizadeh et al., 2011; Loh & Wainwright, 2011). The sample complexity bounds our algorithms obtain in this setting are slightly worse than those presented in the current paper, and it is interesting to check if one can do better."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Shai Shalev-Shwartz for several useful discussions, and the anonymous referees for their detailed comments."
    } ],
    "references" : [ {
      "title" : "Learning with restricted focus of attention",
      "author" : [ "S. Ben-David", "E. Dichterman" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Ben.David and Dichterman,? \\Q1998\\E",
      "shortCiteRegEx" : "Ben.David and Dichterman",
      "year" : 1998
    }, {
      "title" : "Online learning of noisy data",
      "author" : [ "N. Cesa-Bianchi", "S. Shalev-Shwartz", "O. Shamir" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Sublinear optimization for machine learning",
      "author" : [ "K.L. Clarkson", "E. Hazan", "D.P. Woodruff" ],
      "venue" : "IEEE 51st Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Clarkson et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Clarkson et al\\.",
      "year" : 2010
    }, {
      "title" : "Decision theoretic generalizations of the PAC model for neural net and other learning applications",
      "author" : [ "D. Haussler" ],
      "venue" : "Information and computation,",
      "citeRegEx" : "Haussler,? \\Q1992\\E",
      "shortCiteRegEx" : "Haussler",
      "year" : 1992
    }, {
      "title" : "Optimal algorithms for ridge and lasso regression with partially observed attributes",
      "author" : [ "E. Hazan", "T. Koren" ],
      "venue" : "Arxiv preprint arXiv:1108.4559,",
      "citeRegEx" : "Hazan and Koren,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan and Koren",
      "year" : 2011
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "E. Hazan", "A. Agarwal", "S. Kale" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2007
    }, {
      "title" : "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization",
      "author" : [ "S.M. Kakade", "K. Sridharan", "A. Tewari" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kakade et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2008
    }, {
      "title" : "Exponentiated gradient versus gradient descent for linear predictors",
      "author" : [ "J. Kivinen", "M.K. Warmuth" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Kivinen and Warmuth,? \\Q1997\\E",
      "shortCiteRegEx" : "Kivinen and Warmuth",
      "year" : 1997
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity",
      "author" : [ "P.L. Loh", "M.J. Wainwright" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Loh and Wainwright,? \\Q2011\\E",
      "shortCiteRegEx" : "Loh and Wainwright",
      "year" : 2011
    }, {
      "title" : "Learning with missing features",
      "author" : [ "A. Rostamizadeh", "A. Agarwal", "P. Bartlett" ],
      "venue" : "In The 27th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Rostamizadeh et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rostamizadeh et al\\.",
      "year" : 2011
    }, {
      "title" : "The nature of statistical learning theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "Vapnik,? \\Q1995\\E",
      "shortCiteRegEx" : "Vapnik",
      "year" : 1995
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "In Proceedings of the 20th international conference on Machine learning,",
      "citeRegEx" : "Zinkevich,? \\Q2003\\E",
      "shortCiteRegEx" : "Zinkevich",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "By that, we resolve an open problem recently posed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art.",
      "startOffset" : 54,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "Recently, Cesa-Bianchi et al. (2010)",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "For SVR, the bound is obtained by additionally incorporating the methods of (Cesa-Bianchi et al., 2011).",
      "startOffset" : 76,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "We assume the standard framework of statistical learning (Haussler, 1992), in which the pairs (x, y) follow a joint probability distribution D over R×R.",
      "startOffset" : 57,
      "endOffset" : 73
    }, {
      "referenceID" : 11,
      "context" : "• Support-vector regression: p = 2 and the δinsensitive absolute loss (Vapnik, 1995),",
      "startOffset" : 70,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "(Hazan et al., 2007).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "See (Cesa-Bianchi et al., 2011) for a related impossibility result.",
      "startOffset" : 4,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "Our algorithm for the LAO setting is based on a randomized Online Gradient Descent (OGD) strategy (Zinkevich, 2003).",
      "startOffset" : 98,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : "Our gradient estimators make use of an importance-sampling method inspired by (Clarkson et al., 2010).",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "The lemma is a consequence of a second-order regret bound for the Multiplicative-Weights algorithm, essentially due to (Clarkson et al., 2010).",
      "startOffset" : 119,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "For learning the approximate problem we use a novel technique, inspired by (Cesa-Bianchi et al., 2011), for estimating gradients of analytic loss functions.",
      "startOffset" : 75,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : ", 2010) and used the popular MNIST digit recognition dataset (LeCun et al., 1998).",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "(Rostamizadeh et al., 2011; Loh & Wainwright, 2011).",
      "startOffset" : 0,
      "endOffset" : 51
    } ],
    "year" : 2012,
    "abstractText" : "We consider the most common variants of linear regression, including Ridge, Lasso and Support-vector regression, in a setting where the learner is allowed to observe only a fixed number of attributes of each example at training time. We present simple and efficient algorithms for these problems: for Lasso and Ridge regression they need the same total number of attributes (up to constants) as do full-information algorithms, for reaching a certain accuracy. For Support-vector regression, we require exponentially less attributes compared to the state of the art. By that, we resolve an open problem recently posed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art.",
    "creator" : "LaTeX with hyperref package"
  }
}