{
  "name" : "1605.08257.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Low-rank tensor completion: a Riemannian manifold preconditioning approach",
    "authors" : [ "Hiroyuki Kasai", "Bamdev Mishra" ],
    "emails" : [ "kasai@is.uec.ac.jp", "bamdevm@amazon.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "This paper addresses the problem of low-rank tensor completion when the rank is a priori known or estimated. We focus on 3-order tensors in the paper, but the developments can be generalized to higher order tensors in a straightforward way. Given a tensor X n1×n2×n3 , whose entries X ?i1,i2,i3 are only known for some indices (i1, i2, i3) ∈ Ω, where Ω is a subset of the complete set of indices {(i1, i2, i3) : id ∈ {1, . . . , nd}, d ∈ {1, 2, 3}}, the fixed-rank tensor completion problem is formulated as\nmin X∈Rn1×n2×n3\n1\n|Ω|‖PΩ(X )−PΩ(X ?)‖2F\nsubject to rank(X ) = r, (1)\nwhere the operator PΩ(X )i1,i2,i3 = Xi1,i2,i3 if (i1, i2, i3) ∈ Ω and PΩ(X )i1,i2,i3 = 0 otherwise and (with a slight abuse of notation) ‖ · ‖F is the Frobenius norm. |Ω| is the number of known entries. rank(X ) (= r = (r1, r2, r3)), called the multilinear rank of X , is the set of the ranks of for each of mode-d unfolding matrices. rd nd enforces a low-rank structure. The mode is a matrix obtained by concatenating the mode-d fibers along columns, and mode-d unfolding of a D-order tensor X is Xd ∈ Rnd×nd+1···nDn1···nd−1 for d = {1, . . . , D}.\nProblem (1) has many variants, and one of those is extending the nuclear norm regularization approach from the matrix case [6] to the tensor case. This results in a summation of nuclear norm regularization terms, each one corresponds to each of the unfolding matrices of X . While this generalization leads to good results [15, 25, 24], its applicability to large-scale instances is not trivial, especially due to the necessity of high-dimensional singular value decomposition computations. A different approach exploits Tucker decomposition [12, Section 4] of a low-rank tensor X to develop large-scale algorithms for (1), e.g., in [8, 13].\n1This paper extends the earlier work [11] to include a stochastic gradient descent algorithm for low-rank tensor completion.\nar X\niv :1\n60 5.\n08 25\n7v 1\n[ cs\n.L G\n] 2\n6 M\nay 2\n01 6\nThe present paper exploits both the symmetry present in Tucker decomposition and the least-squares structure of the cost function of (1) to develop competitive algorithms. The multilinear rank constraint forms a smooth manifold [13]. To this end, we use the concept of manifold preconditioning. While preconditioning in unconstrained optimization is well studied [20, Chapter 5], preconditioning on constraints with symmetries, owing to non-uniqueness of Tucker decomposition [12], is not straightforward. We build upon the recent work [18] that suggests to use preconditioning with a tailored metric (inner product) in the Riemannian optimization framework on quotient manifolds [1, 7, 18]. The differences with respect to the work of Kressner et al. [13], which also exploits the manifold structure, are twofold. (i) Kressner et al. [13] exploit the search space as an embedded submanifold of the Euclidean space, whereas we view it as a product of simpler search spaces with symmetries. Consequently, certain computations have straightforward interpretation. (ii) Kressner et al. [13] work with the standard Euclidean metric, whereas we use a metric that is tuned to the least-squares cost function, thereby inducing a preconditioning effect. This novel idea of using a tuned metric leads to a superior performance of our algorithms. They also connect to state-of-the-art algorithms proposed in [19, 27, 17, 4].\nThe paper is organized as follows. Section 2 discusses the two fundamental structures of symmetry and least-squares associated with (1) and proposes a novel metric that captures the relevant second order information of the problem. The optimization-related ingredients on the Tucker manifold are developed in Section 3. The cost function specific ingredients are developed in Section 4. The final formulas are listed in Table 1, which allow to develop preconditioned conjugate gradient descent algorithm in the batch setup and stochastic gradient descent algorithm in the online setup. In Section 5, numerical comparisons with state-of-the-art algorithms on various synthetic and real-world benchmarks suggest a superior performance of our proposed algorithms. Our proposed algorithms are implemented in the Matlab toolbox Manopt [5]. The concrete proofs of propositions, development of optimization-related ingredients, and additional numerical experiments are shown in Sections A and B, respectively, of the supplementary material file. The Matlab codes for first and second order implementations, e.g., gradient descent and trust-region methods, are available at https://bamdevmishra.com/codes/ tensorcompletion/."
    }, {
      "heading" : "2 Exploiting the problem structure",
      "text" : "Construction of efficient algorithms depends on properly exploiting the problem structure. To this end, we focus on two fundamental structures in (1): symmetry in the constraints and the least-squares structure of the cost function. Finally, a novel metric is proposed.\nThe symmetry structure in Tucker decomposition. The Tucker decomposition of a tensor X ∈ Rn1×n2×n3 of rank r (=(r1, r2, r3)) is\nX = G×1U1×2U2×3U3, (2) where Ud ∈ St(rd, nd) for d ∈ {1, 2, 3} belongs to the Stiefel manifold of matrices of size nd × rd with orthogonal columns and G ∈ Rr1×r2×r3 [12]. Here, W×dV ∈ Rn1×···nd−1×m×nd+1×···nD computes the d-mode product of a tensor W ∈ Rn1×···×nD and a matrix V ∈ Rm×nd . Tucker decomposition (2) is not unique as X remains unchanged under the transformation\n(U1,U2,U3,G) 7→ (U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 ) (3) for all Od ∈ O(rd), which is the set of orthogonal matrices of size of rd × rd. The classical remedy to remove this indeterminacy is to have additional structures on G like sparsity or restricted orthogonal rotations [12, Section 4.3]. In contrast, we encode the transformation (3) in an abstract search space of equivalence classes, defined as,\n[(U1,U2,U3,G)] := {(U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 ) : Od ∈ O(rd)}. (4) The set of equivalence classes is the quotient manifold [14]\nM/∼ := M/(O(r1)×O(r2)×O(r3)), (5)\nwhereM is called the total space (computational space) that is the product space\nM := St(r1, n1)× St(r2, n2)× St(r3, n3)× Rr1×r2×r3 . (6)\nDue to the invariance (3), the local minima of (1) inM are not isolated, but they become isolated onM/∼. Consequently, the problem (1) is an optimization problem on a quotient manifold for which systematic procedures are proposed in [1, 7]. A requirement is to endow endowM/∼with a Riemannian structure, which conceptually translates (1) into an unconstrained optimization problem over the search spaceM/∼. We callM/∼, defined in (5), the Tucker manifold as it results from Tucker decomposition.\nThe least-squares structure of the cost function. In unconstrained optimization, the Newton method is interpreted as a scaled steepest descent method, where the search space is endowed with a metric (inner product) induced by the Hessian of the cost function [20]. This induced metric (or its approximation) resolves convergence issues of first order optimization algorithms. Analogously, finding a good inner product for (1) is of profound consequence. Specifically for the case of quadratic optimization with rank constraint (matrix case), Mishra and Sepulchre [18] propose a family of Riemannian metrics from the Hessian of the cost function. Applying this approach directly for the particular cost function of (1) is computationally costly. To circumvent the issue, we consider a simplified cost function by assuming that Ω contains the full set of indices, i.e., we focus on ‖X − X ?‖2F to propose a metric candidate. Applying the metric tuning approach of [18] to the simplified cost function leads to a family of Riemannian metrics. A good trade-off between computational cost and simplicity is by considering only the block diagonal elements of the Hessian of ‖X −X ?‖2F . It should be noted that the cost function ‖X − X ?‖2F is convex and quadratic in X . Consequently, it is also convex and quadratic in the arguments (U1,U2,U3,G) individually. Equivalently, the block diagonal approximation of the Hessian of ‖X −X ?‖2F in (U1,U2,U3,G) is\n((G1GT1 )⊗ In1 , (G2GT2 )⊗ In2 , (G3GT3 )⊗ In3 , Ir1r2r3), (7)\nwhere Gd is the mode-d unfolding of G and is assumed to be full rank. ⊗ is the Kronecker product. The terms GdGTd for d ∈ {1, 2, 3} are positive definite when r1 ≤ r2r3, r2 ≤ r1r3, and r3 ≤ r1r2, which is a reasonable assumption.\nA novel Riemannian metric. An element x in the total space M has the matrix representation (U1,U2,U3,G). Consequently, the tangent space TxM is the Cartesian product of the tangent spaces of the individual manifolds of (6), i.e., TxM has the matrix characterization [7]\nTxM = {(ZU1 ,ZU2 ,ZU3 ,ZG) ∈ Rn1×r1 × Rn2×r2 × Rn3×r3 × Rr1×r2×r3 : UTd ZUd + Z T Ud Ud = 0, for d ∈ {1, 2, 3}}. (8)\nFrom the earlier discussion on symmetry and least-squares structure, we propose the novel metric or inner product gx : TxM× TxM→ R\ngx(ξx, ηx) = 〈ξU1 , ηU1(G1GT1 )〉+ 〈ξU2 , ηU2(G2GT2 )〉+ 〈ξU3 , ηU3(G3GT3 )〉+ 〈ξG , ηG〉, (9)\nwhere ξx, ηx ∈ TxM are tangent vectors with matrix characterizations, shown in (8), (ξU1 , ξU2 , ξU3 , ξG) and (ηU1 , ηU2 , ηU3 , ηG), respectively and 〈·, ·〉 is the Euclidean inner product. It should be emphasized that the proposed metric (9) is induced from (7).\nProposition 1. Let (ξU1 , ξU2 , ξU3 , ξG) and (ηU1 , ηU2 , ηU3 , ηG) be tangent vectors to the quotient manifold (5) at (U1,U2,U3,G), and (ξU1O1 , ξU2O2 , ξU3O3 , ξG×1OT1 ×2OT2 ×3OT3 ) and (ηU1O1 , ηU2O2 , ηU3O3 , ηG×1OT1 ×2OT2 ×3OT3 ) be tangent vectors to the quotient manifold (5) at (U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 ). The metric (9) is invariant along the equivalence class (4), i.e.,\ng(U1,U2,U3,G)((ξU1 , ξU2 , ξU3 , ξG), (ηU1 , ηU2 , ηU3 , ηG))\n= g(U1O1,U2O2,U3O3,G×1OT1 ×2OT2 ×3OT3 ) ((ξU1O1 , ξU2O2 , ξU3O3 , ξG×1OT1 ×2OT2 ×3OT3 ),\n(ηU1O1 , ηU2O2 , ηU3O3 , ηG×1OT1 ×2OT2 ×3OT3 ))."
    }, {
      "heading" : "3 Notions of manifold optimization",
      "text" : "Each point on a quotient manifold represents an entire equivalence class of matrices in the total space. Abstract geometric objects on the quotient manifold M/ ∼ call for matrix representatives in the total space M. Similarly, algorithms are run in the total space M, but under appropriate compatibility between the Riemannian structure ofM and the Riemannian structure of the quotient manifoldM/∼, they define algorithms on the quotient manifold. The key is endowingM/∼ with a Riemannian structure. Once this is the case, a constraint optimization problem, for example (1), is conceptually transformed into an unconstrained optimization over the Riemannian quotient manifold (5). Below we briefly show the development of various geometric objects that are required to optimize a smooth cost function on the quotient manifold (5) with first order methods, e.g., conjugate gradients.\nQuotient manifold representation and horizontal lifts. Figure 1 illustrates a schematic view of optimization with equivalence classes, where the points x and y inM belong to the same equivalence class (shown in solid blue color) and they represent a single point [x] := {y ∈ M : y ∼ x} on the quotient manifoldM/∼. The abstract tangent space T[x](M/∼) at [x] ∈M/∼ has the matrix representation in TxM, but restricted to the directions that do not induce a displacement along the equivalence class [x]. This is realized by decomposing TxM into two complementary subspaces, the vertical and horizontal subspaces. The vertical space Vx is the tangent space of the equivalence class [x]. On the other hand, the horizontal space Hx is the orthogonal subspace to Vx in the sense of the metric (9). Equivalently, TxM = Vx ⊕ Hx. The horizontal subspace Hx provides a valid matrix representation to the abstract tangent space T[x](M/∼). An abstract tangent vector ξ[x] ∈ T[x](M/∼) at [x] has a unique element ξx ∈ Hx that is called its horizontal lift.\nA Riemannian metric gx : TxM × TxM → R at x ∈ M defines a Riemannian metric g[x] : T[x](M/∼) × T[x](M/∼) → R, i.e., g[x](ξ[x], η[x]) := gx(ξx, ηx) on the quotient manifoldM/∼, if gx(ξx, ηx) does not depend on a specific representation along the equivalence class [x]. Here, ξ[x] and η[x] are tangent vectors in T[x](M/∼), and ξx and ηx are their horizontal lifts in Hx at x, respectively. Equivalently, the definition of the Riemannian metric is well posed when gx(ξx, ζx) = gx(ξy, ζy) for all x, y ∈ [x], where ξx, ζx ∈ Hx and ξy, ζy ∈ Hy are the horizontal lifts of ξ[x], ζ[x] ∈ T[x](M/∼) along the same equivalence class [x]. This holds true for the proposed metric (9) as shown in Proposition 1. From [1], endowed with the Riemannian metric (9), the quotient manifoldM/∼ is a Riemannian submersion ofM. The submersion principle allows to work out concrete matrix representations of abstract object onM/∼, e.g., the gradient of a smooth cost function [1].\nStarting from an arbitrary matrix (with appropriate dimensions), two linear projections are needed: the first projection Ψx is onto the tangent space TxM, while the second projection Πx is onto the horizontal subspaceHx. The computation cost of these is O(n1r21 + n2r22 + n3r23).\nThe tangent space TxM projection is obtained by extracting the component normal to TxM in the ambient space. The normal spaceNxM has the matrix characterization {(U1SU1(G1GT1 )−1,U2SU2(G2GT2 )−1,\nU3SU3(G3G T 3 ) −1, 0) : SUd ∈ Rrd×rd ,STUd = SUd , for d ∈ {1, 2, 3}}. Symmetric matrices SUd for all d ∈ {1, 2, 3} parameterize the normal space. Finally, the operator Ψx : Rn1×r1 × Rn2×r2 × Rn3×r3 × Rr1×r2×r3 → TxM : (YU1 ,YU2 ,YU3 ,YG) 7→ Ψx(YU1 ,YU2 ,YU3 ,YG) is given as follows. Proposition 2. The quotient manifold (5) endowed with the metric (9) admits the tangent space projector defined as\nΨx(YU1 ,YU2 ,YU3 ,YG) = (YU1 − U1SU1(G1GT1 )−1,YU2 − U2SU2(G2GT2 )−1, YU3 − U3SU3(G3GT3 )−1,YG),\n(10)\nwhere SUd is the solution to the Lyapunov equation SUdGdG T d +GdG T d SUd = GdG T d (Y T UdUd+U T d YUd)GdG T d for d ∈ {1, 2, 3}. The Lyapunov equations in Proposition 2 are solved efficiently with the Matlab’s lyap routine. The horizontal space projection of a tangent vector is obtained by removing the component along the vertical space. The vertical space Vx has the matrix characterization {(U1Ω1,U2Ω2,U3Ω3,−(G×1Ω1+ G×2Ω2+G×3Ω3)) : Ωd ∈ Rrd×rd ,ΩTd = −Ωd for d ∈ {1, 2, 3}}. Skew symmetric matrices Ωd for all d ∈ {1, 2, 3} parameterize the vertical space. Finally, the horizontal projection operator Πx : TxM :→ Hx : ηx 7→ Πx(ηx) is given as follows. Proposition 3. The quotient manifold (5) endowed with the metric (9) admits the horizontal projector defined as\nΠx(ηx) = (ηU1 − U1Ω1, ηU2 − U2Ω2, ηU3 − U3Ω3, ηG − (−(G×1Ω1 + G×2Ω2 + G×3Ω3))), where ηx = (ηU1 , ηU2 , ηU3 , ηG) ∈ TxM and Ωd is a skew-symmetric matrix of size rd × rd that is the solution to the coupled Lyapunov equations G1GT1 Ω1 + Ω1G1GT1 −G1(Ir3 ⊗Ω2)GT1 −G1(Ω3 ⊗ Ir2)GT1 = Skew(UT1 ηU1G1G T 1 ) + Skew(G1ηTG1), G2GT2 Ω2 + Ω2G2GT2 −G2(Ir3 ⊗Ω1)GT2 −G2(Ω3 ⊗ Ir1)GT2 = Skew(UT2 ηU2G2G T 2 ) + Skew(G2ηTG2),\nG3GT3 Ω3 + Ω3G3GT3 −G3(Ir2 ⊗Ω1)GT3 −G3(Ω2 ⊗ Ir1)GT3 = Skew(UT3 ηU3G3G T 3 ) + Skew(G3ηTG3),\n(11)\nwhere Skew(·) extracts the skew-symmetric part of a square matrix, i.e., Skew(D) = (D− DT )/2. The coupled Lyapunov equations (11) are solved efficiently with the Matlab’s pcg routine that is combined with a specific symmetric preconditioner resulting from the Gauss-Seidel approximation of (11). For the variable Ω1, the preconditioner is of the form G1GT1 Ω1 + Ω1G1GT1 . Similarly, for the variables Ω2 and Ω3.\nRetraction. A retraction is a mapping that maps vectors in the horizontal space to points on the search spaceM and satisfies the local rigidity condition [1]. It provides a natural way to move on the manifold along a search direction. Because the total spaceM has the product nature, we can choose a retraction by combining retractions on the individual manifolds, i.e., Rx(ξx) = (uf(U1 + ξU1), uf(U2 + ξU2),uf(U3 + ξU3),G + ξG), where ξx ∈ Hx and uf(·) extracts the orthogonal factor of a full column rank matrix, i.e., uf(A) = A(ATA)−1/2. The retractionRx defines a retractionR[x](ξ[x]) := [Rx(ξx)] on the quotient manifoldM/ ∼, as the equivalence class [Rx(ξx)] does not depend on specific matrix representations of [x] and ξ[x], where ξx is the horizontal lift of the abstract tangent vector ξ[x] ∈ T[x](M/ ∼).\nVector transport. A vector transport on a manifoldM is a smooth mapping that transports a tangent vector ξx ∈ TxM at x ∈M to a vector in the tangent space at a pointRx(ηx). It is defined by the symbol Tηxξx. It generalizes the classical concept of translation of vectors in the Euclidean space to manifolds [1, Section 8.1.4]. The horizontal lift of the abstract vector transport Tη[x]ξ[x] onM/∼ has the matrix characterization ΠRx(ηx)(Tηxξx) = ΠRx(ηx)(ΨRx(ηx)(ξx)), where ξx and ηx are the horizontal lifts in Hx of ξ[x] and η[x] that belong to T[x](M/∼). Ψx(·) and Πx(·) are projectors defined in Propositions 2 and 3. The computational cost of transporting a vector solely depends on the projection and retraction operations."
    }, {
      "heading" : "4 Riemannian algorithms for (1)",
      "text" : "We propose two Riemannian preconditioned algorithms for the tensor completion problem (1) that are based on the developments in Section 3. The preconditioning effect follows from the specific choice of the metric (9). In the batch setting, we use the off-the-shelf conjugate gradient implementation of Manopt for any smooth cost function [5]. A complete description of the Riemannian nonlinear conjugate gradient method is in [1, Chapter 8]. In the online setting, we use the stochastic gradient descent implementation [2]. For fixed rank, theoretical convergence of the Riemannian algorithms are to a stationary point, and the convergence analysis follows from [22, 21, 2]. However, as simulations show, convergence to global minima is observed in many challenging instances.\nIn addition to the manifold-related ingredients in Section 3, the ingredients needed are the cost function specific ones. To this end, we show the computation of the Riemannian gradient as well as a way to compute an initial guess for the step-size, which is used in the conjugate gradient method. The concrete formulas are shown in Table 1.\nRiemannian gradient computation. Let f(X ) = ‖PΩ(X )−PΩ(X ?)‖2F /|Ω| be the mean square error function of (1), and S = 2(PΩ(G×1U1×2U2×3U3)−PΩ(X ?))/|Ω| be an auxiliary sparse tensor variable that is interpreted as the Euclidean gradient of f in Rn1×n2×n3 . The partial derivatives of f with respect to (U1,U2,U3,G) are computed in terms of the unfolding matrices Sd. Due to the specific scaled metric (9), the partial derivatives are further scaled by ((G1GT1 )−1, (G2GT2 )−1, (G3GT3 )−1,I), denoted as egradxf (after scaling). Finally, from the Riemannian submersion theory [1, Section 3.6.2], the horizontal lift of grad[x]f is equal to gradxf = Ψ(egradxf). The total numerical cost of computing the Riemannian gradient depends on computing the partial derivatives, which is O(|Ω|r1r2r3).\nProposition 4. The cost function (1) at (U1,U2,U3,G) under the quotient manifold (5) endowed with\nthe Riemannian metric (9) admits the horizontal lift of the Riemannian gradient\n(S1(U3 ⊗ U2)GT1 (G1GT1 )−1 − U1BU1(G1GT1 )−1, S2(U3 ⊗ U1)GT2 (G2GT2 )−1 − U2BU2(G2GT2 )−1, S3(U2 ⊗ U1)GT3 (G3GT3 )−1 − U3BU3(G3GT3 )−1,S ×1 UT1 ×2 UT2 ×3 UT3 ),\n(12)\nwhere BUd for d ∈ {1, 2, 3} are the solutions to the Lyapunov equations BU1G1G T 1 + G1GT1 BU1 = 2Sym(G1G T 1 UT1 (S1(U3 ⊗ U2)GT2 ), BU2G2G T 2 + G2GT2 BU2 = 2Sym(G2G T 2 UT2 (S2(U3 ⊗ U1)GT2 ),\nBU3G3G T 3 + G3GT3 BU3 = 2Sym(G3G T 3 UT3 (S3(U2 ⊗ U1)GT3 ),\nwhere Sym(·) extracts the symmetric part of a matrix.\nInitial guess for the step-size. Following [17, 26, 13], the least-squares structure of the cost function in (1) is exploited to compute a linearized step-size guess efficiently along a search direction by considering a polynomial approximation of degree 2 over the manifold. Given a search direction ξx ∈ Hx, the step-size guess is arg mins∈R+ ‖PΩ(G×1U1×2U2×3U3 + sG×1ξU1×2U2×3U3 + sG×1U1×2ξU2×3U3+sG×1U1×2U2×3ξU3+sξG×1U1×2U2×3U3)−PΩ(X ?)‖2F , which has a closedform expression and the numerical cost of computing it is O(|Ω|r1r2r3).\nStochastic gradient descent in online setting. In the online setting, we update (U1,U2,U3,G) every time a frontal slice, i.e., a matrix ∈ Rn1×n2 , is randomly sampled from X ?i1,i2,i3 . Equivalently, we assume that the tensor grows along the third dimension. More concretely, we calculate the rank-one Riemannian gradient (12) for the input slice. (U1,U2,U3,G) are updated by taking a step along the negative Riemannian gradient direction. Subsequently, we retract using Rx. A popular formula for the step-size γk at k-th update is γk = γ0/(1 + γ0λk), where γ0 is the initial step-size and λ is a fixed reduction factor. Following [3], we select γ0 in the pre-training phase using a small sample size of a training set. λ is fixed to 10−7.\nComputational cost. The total computational cost per iteration of our proposed conjugate gradient implementation is O(|Ω|r1r2r3), where |Ω| is the number of known entries. It should be stressed that the computational cost of our conjugate gradient implementation is equal to that of [13]. In the online setting, each stochastic gradient descent update costs O(|Ωslice|r1r2 + n1r21 + n2r22 + Tr23 + r1r2r3), where |Ωslice| is the number of known entries of the current frontal slice of the incomplete tensor X ?i1,i2,i3 , and T is the number of slices that we have seen along n3 direction."
    }, {
      "heading" : "5 Numerical comparisons",
      "text" : "In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms. In the online setting, we compare our proposed stochastic gradient descent algorithm with CANDECOMP/PARAFAC based TeCPSGD [16] and OLSTEC [10]. All simulations are performed in Matlab on a 2.6 GHz Intel Core i7 machine with 16 GB RAM. For specific operations with unfoldings of S, we use the mex interfaces for Matlab that are provided by the authors of geomCG. For large-scale instances, our algorithm is only compared with geomCG as others cannot handle them. Cases S and R are for batch instances, whereas Case O is for online instances. Since the dimension of the space of a tensor ∈ Rn1×n2×n3 of rank r = (r1, r2, r3) is dim(M/∼) =∑3 d=1(ndrd − r2d) + r1r2r3, we randomly and uniformly select known entries based on a multiple of the dimension, called the over-sampling (OS) ratio, to create the train set Ω. Algorithms are initialized randomly, as suggested in [13], and are stopped when either the mean square error (MSE) on the train set Ω is below 10−12 or the number of iterations exceeds 250. We also evaluate the mean square error on a test set Γ, which is different from Ω. Five runs are performed in each scenario and the plots show all of them. The time plots are shown with standard deviations. It should be noted that we show most numerical\ncomparisons on the test set Γ as it allows to compare with nuclear norm minimization algorithms, which optimize a different (training) cost function. Additional plots are provided as supplementary material.\nCase S1: comparison with the Euclidean metric. We first show the benefit of the proposed metric (9) over the conventional choice of the Euclidean metric that exploits the product structure of M and symmetry (3). This is defined by combining the individual natural metrics for St(rd, nd) and Rr1×r2×r3 . For simulations, we randomly generate a tensor of size 200 × 200 × 200 and rank r = (10, 10, 10). OS is 10. For simplicity, we compare gradient descent algorithms with Armijo backtracking linesearch for both the metric choices. Figure 2(a) shows that the algorithm with the metric (9) gives a superior performance in train error than that of the conventional metric choice.\nCase S2: small-scale instances. Small-scale tensors of size 100 × 100 × 100, 150 × 150 × 150, and 200 × 200 × 200 and rank r = (10, 10, 10) are considered. OS is {10, 20, 30}. Figure 2(b) shows that our proposed algorithm has faster convergence than others. In Figure 2(c), the lowest test errors are obtained by our proposed algorithm and geomCG.\nCase S3: large-scale instances. We consider large-scale tensors of size 3000 × 3000 × 3000, 5000 × 5000 × 5000, and 10000 × 10000 × 10000 and ranks r = (5, 5, 5) and (10, 10, 10). OS is 10. Our proposed algorithm outperforms geomCG in Figure 2(d).\nCase S4: influence of low sampling. We look into problem instances from scarcely sampled data, e.g., OS is 4. The test requires completing a tensor of size 10000×10000×10000 and rank r = (5, 5, 5).\nFigure 2(e) shows the superior performance of the proposed algorithm against geomCG. Whereas the test error increases for geomCG, it decreases for the proposed algorithm.\nCase S5: influence of ill-conditioning and low sampling. We consider the problem instance of Case S4 with OS = 5. Additionally, for generating the instance, we impose a diagonal core G with exponentially decaying positive values of condition numbers (CN) 5, 50, and 100. Figure 2(f) shows that the proposed algorithm outperforms geomCG for all the considered CN values.\nCase S6: influence of noise. We evaluate the convergence properties of algorithms under the presence of noise by adding scaled Gaussian noise PΩ(E) to PΩ(X ?) as in [13]. The different noise levels are = {10−4, 10−6, 10−8, 10−10, 10−12}. In order to evaluate for = 10−12, the stopping threshold on the MSE of the train set is lowered to 10−24. The tensor size and rank are same as in Case S4 and OS is 10. Figure 2(g) shows that the test error for each is almost identical to the 2‖PΩ(X ?)‖2F [13], but our proposed algorithm converges faster than geomCG.\nCase S7: rectangular instances. We consider instances where the dimensions and ranks along certain modes are different than others. Two cases are considered. Case (7.a) considers tensors size 20000× 7000× 7000, 30000× 6000× 6000, and 40000× 5000× 5000 with rank r = (5, 5, 5). Case (7.b) considers a tensor of size 10000× 10000× 10000 with ranks (7, 6, 6), (10, 5, 5), and (15, 4, 4). In all the cases, the proposed algorithm converges faster than geomCG as shown in Figure 2(h).\nCase R1: hyperspectral image. We consider the hyperspectral image “Ribeira” [9] discussed in [23, 13]. The tensor size is 1017 × 1340 × 33, where each slice corresponds to a particular image measured at a different wavelength. As suggested in [23, 13], we resize it to 203 × 268 × 33. We perform five random samplings of the pixels based on the OS values 11 and 22, corresponding to the rank r=(15, 15, 6) adopted in [13]. This set is further randomly split into 80/10/10–train/validation/test partitions. The algorithms are stopped when the MSE on the validation set starts to increase. While OS = 22 corresponds to the observation ratio of 10% studied in [13], OS = 11 considers a challenging scenario with the observation ratio of 5%. Figures 2(i) shows the good performance of our algorithm. Table 2 compiles the results.\nCase R2: MovieLens-10M2. This dataset contains 10000054 ratings corresponding to 71567 users and 10681 movies. We split the time into 7-days wide bins results, and finally, get a tensor of size 71567 × 10681 × 731. The fraction of known entries is less than 0.002%. The completion task on this dataset reveals periodicity of the latent genres. We perform five random 80/10/10–train/validation/test partitions. The maximum iteration threshold is set to 500. In Table 2, our proposed algorithm consistently gives lower test errors than geomCG across different ranks.\nCase O: online instances. We compare the proposed stochastic gradient descent algorithm with its batch counterpart gradient descent algorithm and with TeCPSGD [16] and OLSTEC [10]. As the implementations of TeCPSGD and OLSTEC are computationally more intensive than ours, our plots\n2http://grouplens.org/datasets/movielens/.\nonly show test MSE against the number of outer iterations, i.e., the number of the passes through the data.\nFigure 3(a) shows comparisons on a synthetic instance of tensor size 100 × 100 × 10000 with rank r = (5, 5, 5). γ0 is selected from the step-size list {8, 9, 10, 11, 12} in the pre-training phase. 10% entries are randomly observed. The pre-training uses 10% frontal slices of all the slices. The maximum number of outer loops is set to 100. Figure 3(a) shows five different runs, where the online algorithm has the same asymptotic convergence behavior as the batch counterpart on a test dataset Γ. Figure 3(b) shows comparisons on the Airport Hall surveillance video sequence dataset3 of size 176×144 with 1000 frames. γ0 is selected from {30, 40, 50, 60, 70} and 10% frontal slices are selected for pre-training. 2% of the entries are observed. In Figure 3(b), both the proposed online and batch algorithms achieve lower test errors than TeCPSGD and OLSTEC."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have proposed preconditioned batch (conjugate gradient) and online (stochastic gradient descent) algorithms for the tensor completion problem. The algorithms stem from the Riemannian preconditioning approach that exploits the fundamental structures of symmetry (due to non-uniqueness of Tucker decomposition) and least-squares of the cost function. A novel Riemannian metric (inner product) is proposed that enables to use the versatile Riemannian optimization framework. Numerical comparisons suggest that our proposed algorithms have a superior performance on different benchmarks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Rodolphe Sepulchre, Paul Van Dooren, and Nicolas Boumal for useful discussions on the paper. This paper presents research results of the Belgian Network DYSCO (Dynamical Systems, Control, and Optimization), funded by the Interuniversity Attraction Poles Programme, initiated by the Belgian State, Science Policy Office. The scientific responsibility rests with its authors. Hiroyuki Kasai is (partly) supported by the Ministry of Internal Affairs and Communications, Japan, as the SCOPE Project (150201002). This work was initiated while Bamdev Mishra was with the Department of Electrical Engineering and Computer Science, University of Liège, 4000 Liège, Belgium and was visiting the Department of Engineering (Control Group), University of Cambridge, Cambridge, UK. He was supported as a research fellow (aspirant) of the Belgian National Fund for Scientific Research (FNRS).\n3http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html"
    }, {
      "heading" : "A Proof and derivation of manifold-related ingredients",
      "text" : "The concrete computations of the optimization-related ingredients presented in the paper are discussed below.\nThe total space isM := St(r1, n1)×St(r2, n2)×St(r3, n3)×Rr1×r2×r3 . Each element x ∈M has the matrix representation (U1,U2,U3,G). Invariance of Tucker decomposition under the transformation (U1,U2,U3,G) 7→ (U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 ) for all Od ∈ O(rd), the set of orthogonal matrices of size of rd × rd results in equivalence classes of the form [x] = [(U1,U2,U3,G)] := {(U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 ) : Od ∈ O(rd)}.\nA.1 Tangent space characterization and the Riemannian metric\nThe tangent space, TxM, at x given by (U1,U2,U3,G) in the total spaceM is the product space of the tangent spaces of the individual manifolds. From [1], the tangent space has the matrix characterization\nTxM = {(ZU1 ,ZU2 ,ZU3 ,ZG) ∈ Rn1×r1 × Rn2×r2 × Rn3×r3 × Rr1×r2×r3 : UTd ZUd + Z T Ud Ud = 0, for d ∈ {1, 2, 3}}. (A.1)\nThe proposed metric gx : TxM× TxM→ R is\ngx(ξx, ηx) = 〈ξU1 , ηU1(G1GT1 )〉+ 〈ξU2 , ηU2(G2GT2 )〉+ 〈ξU3 , ηU3(G3GT3 )〉+ 〈ξG , ηG〉, (A.2)\nwhere ξx, ηx ∈ TxM are tangent vectors with matrix characterizations (ξU1 , ξU2 , ξU3 , ξG) and (ηU1 , ηU2 , ηU3 , ηG), respectively and 〈·, ·〉 is the Euclidean inner product.\nA.2 Characterization of the normal space\nGiven a vector in Rn1×r1 × Rn2×r2 × Rn3×r3 × Rr1×r2×r3 , its projection onto the tangent space TxM is obtained by extracting the component normal, in the metric sense, to the tangent space. This section describes the characterization of the normal space, NxM.\nLet ζx = (ζU1 , ζU2 , ζU3 , ζG) ∈ NxM, and ηx = (ηU1 , ηU2 , ηU3 , ηG) ∈ TxM. Since ζx is orthogonal to ηx, i.e., gx(ζx, ηx) = 0, the conditions\nTrace(GdGTd ζ T UdηUd) = 0, for d ∈ {1, 2, 3} (A.3)\nmust hold for all ηx in the tangent space. Additionally from [1], ηUd has the characterization\nηUd = UdΩ + Ud⊥K, (A.4)\nwhere Ω is any skew-symmetric matrix, K is a any matrix of size (nd − rd)× rd, and Ud⊥ is any nd × (nd − rd) that is orthogonal complement of Ud. Let ζ̃Ud = ζUdGdGTd and let ζ̃Ud is defined as\nζ̃Ud = UdA + Ud⊥B (A.5)\nwithout loss of generality, where A ∈ Rrd×rd and B ∈ R(nd−rd)×rd are to be characterized from (A.3) and (A.4). A few standard computations show that A has to be symmetric and B = 0. Consequently, ζ̃Ud = UdSUd , where SUd = S T Ud . Equivalently, ζUd = UdSUd(GdG T d ) −1 for a symmetric matrix SUd . Finally, the normal space NxM has the characterization\nNxM = {(U1SU1(G1GT1 )−1,U2SU2(G2GT2 )−1,U3SU3(G3GT3 )−1, 0) : SUd ∈ Rrd×rd ,STUd = SUd , for d ∈ {1, 2, 3}}.\n(A.6)\nA.3 Characterization of the vertical space\nThe horizontal space projector of a tangent vector is obtained by removing the component along the vertical direction. This section shows the matrix characterization of the vertical space Vx. Vx is the defined as the linearization of the equivalence class [(U1,U2,U3,G)] at x = [(U1,U2,U3,G)]. Equivalently, Vx is the linearization of (U1O1,U2O2,U3O3,G×1OT1 ×2OT2 ×3OT3 ) along Od ∈ O(rd) at the identity element for d ∈ {1, 2, 3}. From the characterization of linearization of an orthogonal matrix [1], we have the characterization for the vertical space as\nVx = {(U1Ω1,U2Ω2,U3Ω3,−(G×1Ω1 + G×2Ω2 + G×3Ω3)) : Ωd ∈ Rrd×rd ,ΩTd = −Ωd for d ∈ {1, 2, 3}}.\n(A.7)\nA.4 Characterization of the horizontal space\nThe characterization of the horizontal space Hx is derived from its orthogonal relationship with the vertical space Vx.\nLet ξx = (ξU1 , ξU2 , ξU3 , ξG) ∈ Hx, and ζx = (ζU1 , ζU2 , ζU3 , ζG) ∈ Vx. Since ξx must be orthogonal to ζx, which is equivalent to gx(ξx, ζx) = 0 in (A.2), the characterization for ξx is derived from (A.2) and (A.7).\ngx(ξx, ζx) = 〈ξU1 , ζU1(G1GT1 )〉+ 〈ξU2 , ζU2(G2GT2 )〉+ 〈ξU3 , ζU3(G3GT3 )〉+ 〈ξG , ζG〉 = 〈ξU, (U1Ω1)(G1GT1 )〉+ 〈ξU2 , (U2Ω2)(G2GT2 )〉+ 〈ξU3 , (U3Ω3)(G3GT3 )〉\n+〈ξG ,−(G×1Ω1 + G×2Ω2 + G×3Ω3)〉 (Switch to unfoldings of G.)\n= Trace((G1GT1 )ξ T U1(U1Ω1)) + Trace((G2G T 2 )ξ T U2(U2Ω2)) + Trace((G3G T 3 )ξ T U3(U3Ω3))\n+Trace(ξG1(−Ω1G1)T ) + Trace(ξG2(−Ω2G2)T ) + Trace(ξG3(−Ω3G3)T ) = Trace [{ (G1GT1 )ξ T U1U1 + ξG1G T 1 } Ω1 ] + Trace [{ (G2GT2 )ξ T U2U2 + ξG2G T 2 } Ω2 ]\n+Trace [{\n(G3GT3 )ξ T U3U3 + ξG3G T 3 } Ω3 ] ,\nwhere ξGd is the mode-d unfolding of ξG . Since gx(ξx, ζx) above should be zero for all skew-matrices Ωd, ξx = (ξU1 , ξU2 , ξU3 , ξG) ∈ Hx must satisfy\n(GdGTd )ξTUdUd + ξGdG T d is symmetric for d ∈ {1, 2, 3}. (A.8)\nA.5 Proof of Proposition 1\nWe first introduce the following lemma:\nLemma 1. Let (U1,U2,U3,G) ∈ St(r1, n1) × St(r2, n2) × St(r3, n3) × Rr1×r2×r3 and ξ[(U1,U2,U3,G)] be a tangent vector to the quotient manifold at [(U1,U2,U3,G)]. The horizontal lifts of ξ[(U1,U2,U3,G)] at (U1,U2,U3,G) and (U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 ) are related for Od ∈ O(rd) as follows,\n(ξU1O1 , ξU2O2 , ξU3O3 , ξG×1OT1 ×2OT2 ×3OT3 ) = (ξU1O1, ξU2O2, ξU3O3, ξG×1OT1×2OT2×3OT3 ). (A.9)\nProof. Let f : (St(r1, n1)× St(r2, n2)× St(r3, n3)× Rr1×r2×r3/(O(r1)×O(r2)×O(r3)))→ R be an arbitrary smooth function, and define\nf̄ := f ◦ π : (St(r1, n1)× St(r2, n2)× St(r3, n3)× Rr1×r2×r3/(O(r1)×O(r2)×O(r3)))→ R,\nwhere π is the mapping π :M→M/ ∼ defined by x 7→ [x]. Consider the mapping\nh : (U1,U2,U3,G) 7→ (U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 ),\nwhere Od ∈ O(rd). Since π(h(U1,U2,U3,G)) = π(U1,U2,U3,G) for all (U1,U2,U3,G), we have\nf̄(h(U1,U2,U3,G)) = f̄(U1,U2,U3,G).\nBy taking the differential of both sides,\nDf̄(h(U1,U2,U3,G))[Dh(U1,U2,U3,G)[(ξU1 , ξU2 , ξU3 , ξG)]] = Df̄(U1,U2,U3,G)[(ξU1 , ξU2 , ξU3 , ξG)]. (A.10)\nBy noting the definition of (ξU1 , ξU2 , ξU3 , ξG), i.e., Dπ(U1,U2,U3,G)[ξU1 , ξU2 , ξU3 , ξG ] = ξ[(ξU1 ,ξU2 ,ξU3 ,ξG)], the right side of (A.10) is\nDf̄(U1,U2,U3,G)[(ξU1 , ξU2 , ξU3 , ξG)] = Df(π(U1,U2,U3,G))[Dπ(U1,U2,U3,G)[(ξU1 , ξU2 , ξU3 , ξG)] = Df(π(U1,U2,U3,G))[ξ[(U1,U2,U3,G)]],\nwhere the chain rule is applied to the first equality. Moreover, from the directional derivatives of the mapping h, the bracket of the left side of (A.10) is obtained as\nDh(U1,U2,U3,G)[(ξU1 , ξU2 , ξU3 , ξG)] = (ξU1O1, ξU2O2, ξU3O3, ξG×1OT1×2OT2×3OT3 ).\nTherefore, (A.10) yields\nDf̄(U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 )[(ξU1O1, ξU2O2, ξU3O3, ξG×1OT1×2OT2×3OT3 )] = Df(π(U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 ))[ξ[(U1,U2,U3,G)]],\n(A.11) where we address the equivalence class π(U1,U2,U3,G) = π(U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 ). The left side of (A.11) is further transformed by the chain rule as\nDf̄(U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 )[(ξU1O1, ξU2O2, ξU3O3, ξG×1OT1×2OT2×3OT3 )] = Df(π(U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 ))[Dπ(U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 )]\n[(ξU1O1, ξU2O2, ξU3O3, ξG×1OT1×2OT2×3OT3 )]. (A.12)\nBy comparing the right sides of (A.11) and (A.12), since this equality holds for any smooth function f , it implies that\nDπ(U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 )[(ξU1O1, ξU2O2, ξU3O3, ξG×1OT1×2OT2×3OT3 )] = ξ[(U1,U2,U3,G)].\n(A.13) Finally, we check whether (ξU1O1, ξU2O2, ξU3O3, ξG×1OT1×2OT2×3OT3 ) is an element of H(U1O1,U2O2,U3O3,G×1OT1 ×2OT2 ×3OT3 ). Addressing that the mode-1 unfolding of G×1O T 1×2OT2×3OT3 is OT1 G1(OT3 ⊗ OT2 )T , plugging (ξU1O1, ξU2O2, ξU3O3, ξG×1OT1×2OT2×3OT3 ) into (GdGTd )ξTUdUd + ξGdG T d in (A.8) yields\n(OT1 G1(OT3 ⊗OT2 )T ) ( OT1 G1(OT3 ⊗OT2 )T )T (ξU1O1)T (U1O1)\n+(OT1 )T ξG1(O T 3 ⊗OT2 )(OT1 G1(OT3 ⊗OT2 )T )T\n= OT1 G1GT1 O1 + OT1 ξG1G T 1 O1 = OT1 ((G1GT1 )ξTU1U1 + ξG1G T 1 )O1.\n(A.14)\nSince (ξU1 , ξU2 , ξU3 , ξG) is a symmetric matrix, the obtained result is also symmetric. Therefore, (ξU1O1, ξU2O2, ξU3O3, ξG×1OT1×2OT2×3OT3 ) is a horizontal vector at (U1O1,U2O2,U3O3,G×1OT1×2OT2×3OT3 ). This implies that (ξU1O1, ξU2O2, ξU3O3, ξG×1OT1×2OT2×3OT3 ) is the horizontal lift of ξ at (U1O1,U2O2, U3O3,G×1OT1×2OT2×3OT3 ), and the proof is completed.\nNow, the proof of Proposition 1 is given below using the result (A.9) in Lemma 1.\nProof. Plugging ξ ′ U1 = ξU1O1 , η ′ U1 = ηU1O1 , and G ′ 1 = OT1 G1(OT3 ⊗ OT2 ) into the first term of (A.2) yields\n〈ξU1O1 , ηU1O1(G ′ 1G ′T 1 ))〉 = Trace(ξTU1O1ηU1O1(G ′ 1G ′T 1 ))\n(A.9) = Trace((ξU1O1)T ηU1O1(G ′ 1G ′T 1 )) = Trace [ (ξU1O1)T (ηU1O1)(O T 1 G1(OT3 ⊗OT2 )T ) ( OT1 G1(OT3 ⊗OT2 )T )T ] = Trace [ (ξU1O1)T (ηU1O1)O T 1 G1(OT3 ⊗OT2 )T (OT3 ⊗OT2 )GT1 O1\n] = Trace [ OT1 ξTU1ηU1O1O T 1 G1GT1 O1\n] = Trace [ ξTU1ηU1G1G T 1\n] = 〈ξU1 , ηU1(G1GT1 )〉.\nSince the same equalities against the each term in the metric (A.2) corresponding to U2, U3 and G hold, we finally obtain the invariant property that the proposition claims;\ng(U1,U2,U3,G)((ξU1 , ξU2 , ξU3 , ξG), (ηU1 , ηU2 , ηU3 , ηG))\n= g(U1O1,U2O2,U3O3,G×1OT1 ×2OT2 ×3OT3 ) ((ξU1O1 , ξU2O2 , ξU3O3 , ξG×1OT1 ×2OT2 ×3OT3 ),\n(ηU1O1 , ηU2O2 , ηU3O3 , ηG×1OT1 ×2OT2 ×3OT3 )).\nA.6 Proof of Proposition 2 (derivation of the tangent space projector)\nProof. The tangent space TxM projector is obtained by extracting the component normal to TxM in the ambient space. The normal space NxM has the matrix characterization shown in (A.6). The operator Ψx : Rn1×r1×Rn2×r2×Rn3×r3×Rr1×r2×r3 → TxM : (YU1 ,YU2 ,YU3 ,YG) 7→ Ψx(YU1 ,YU2 ,YU3 ,YG) has the expression\nΨx(YU1 ,YU2 ,YU3 ,YG) = (YU1 − U1SU1(G1GT1 )−1,YU2 − U2SU2(G2GT2 )−1, YU3 − U3SU3(G3GT3 )−1,YG).\n(A.15)\nFrom the definition of the tangent space in (A.1), Ud should satisfy\nηTUdUd + U T d ηUd = (YUd − UdSUd(GdGTd )−1)TUd + UTd (YUd − UdSUd(GdGTd )−1)\n= YTUdUd − (GdG T d ) −1STUdU T d Ud + U T d YUd − UTd UdSUd(GdGTd )−1 = YTUdUd − (GdG T d ) −1SUd + U T d YUd − SUd(GdGTd )−1 = 0.\nMultiplying (GdGTd ) from the right and left sides results in\n(GdGTd ) −1SUd + SUd(GdG T d ) −1 = YTUdUd + U T d YUd\nSUdGdG T d + GdG T d SUd = GdG T d (Y T UdUd + U T d YUd)GdG T d .\nFinally, we obtain the Lyapunov equation as\nSUdGdG T d + GdG T d SUd = GdG T d (Y T UdUd + U T d YUd)GdG T d for d ∈ {1, 2, 3}, (A.16)\nthat are solved efficiently with the Matlab’s lyap routine.\nA.7 Proof of Proposition 3 (derivation of the horizontal space projector)\nProof. We consider the projection of a tangent vector ηx = (ηU1 , ηU2 , ηU3 , ηG) ∈ TxM into a vector ξx = (ξU1 , ξU2 , ξU3 , ξG) ∈ Hx. This is achieved by subtracting the component in the vertical space Vx in (A.7) as ηU1 = ηU1 − U1Ω1︸ ︷︷ ︸ =ξU1∈Hx + U1Ω1︸ ︷︷ ︸ ∈Vx ,\nηU2 = ηU2 − U2Ω2 + U2Ω2, ηU3 = ηU3 − U3Ω3 + U3Ω3, ηG = ηG − (−(G×1Ω1 + G×2Ω2 + G×3Ω3)) + (−(G×1Ω1 + G×2Ω2 + G×3Ω3)).\nAs a result, the horizontal operator Πx : TxM→Hx : ηx 7→ Πx(ηx) has the expression Πx(ηx) = (ηU1 − U1Ω1, ηU2 − U2Ω2, ηU3 − U3Ω3, ηG − (−(G×1Ω1 + G×2Ω2 + G×3Ω3))),\n(A.17) where ηx = (ηU1 , ηU2 , ηU3 , ηG) ∈ TxM and Ωd is a skew-symmetric matrix of size rd × rd. The skew-matrices Ωd for d = {1, 2, 3} that are identified based on the conditions (A.8).\nIt should be noted that the tensor G×1Ω1 +G×2Ω2 +G×3Ω3 in (A.7) has the following equivalent unfoldings.\nG×1Ω1 + G×2Ω2 + G×3Ω3 mode −1⇐===⇒ Ω1G1 + G1(Ir3 ⊗Ω2)T + G1(Ω3 ⊗ Ir2)T mode −2⇐===⇒ G2(Ir3 ⊗Ω1)T + Ω2G2 + G2(Ω3 ⊗ Ir1)T mode −3⇐===⇒ G3(Ir2 ⊗Ω1)T + G3(Ω2 ⊗ Ir1)T + Ω3G3.\nPlugging ξU1 = ηU1 −U1Ω1 and ξG1 = ηG1 + Ω1G1 + G1(Ir3 ⊗Ω2)T + G1(Ω3⊗ Ir2)T into (A.8) and using the relation (A⊗ B)T = AT ⊗ BT results in\n(G1GT1 )ξTU1U+ξG1G T 1 = (G1GT1 )(ηU1 − U1Ω1)TU1 + { ηG1 + (Ω1G1 + G1(Ir3 ⊗Ω2)T + G1(Ω3 ⊗ Ir2)T ) } GT1\n= (G1GT1 )ηTU1U1 − (G1G T 1 )(U1Ω1)TU1 + ηG1G T 1 + Ω1G1GT1\n+G1(Ir3 ⊗Ω2)TGT1 + G1(Ω3 ⊗ Ir2)TGT1 = (G1GT1 )ηTU1U1 + (G1G T 1 )Ω1 + ηG1G T 1 + Ω1G1GT1\n−G1(Ir3 ⊗Ω2)GT1 −G1(Ω3 ⊗ Ir2)GT1 , which should be a symmetric matrix due to (A.8), i.e., (G1GT1 )ξTU1U+ξG1G T 1 = ((G1GT1 )ξTU1U+ξG1G T 1 ) T .\nSubsequently,\n(G1GT1 )ηTU1U1 + (G1G T 1 )Ω1 + ηG1G T 1 + Ω1G1GT1 −G1(Ir3 ⊗Ω2)GT1 −G1(Ω3 ⊗ Ir2)GT1\n= UT1 ηU1(G1G T 1 )−Ω1G1GT1 + G1ηTG1 −G1G T 1 Ω1 + G1(Ir3 ⊗Ω2)GT1 + G1(Ω3 ⊗ Ir2)GT1 ,\nwhich is equivalent to\nG1GT1 Ω1 + Ω1G1GT1 −G1(Ir3 ⊗Ω2)GT1 −G1(Ω3 ⊗ Ir2)GT1 = Skew(UT1 ηU1G1GT1 ) + Skew(G1ηTG1).\nHere Skew(·) extracts the skew-symmetric part of a square matrix, i.e., Skew(D) = (D− DT )/2. Finally, we obtain the coupled Lyapunov equations G1GT1 Ω1 + Ω1G1GT1 −G1(Ir3 ⊗Ω2)GT1 −G1(Ω3 ⊗ Ir2)GT1 = Skew(UT1 ηU1G1G T 1 ) + Skew(G1ηTG1), G2GT2 Ω2 + Ω2G2GT2 −G2(Ir3 ⊗Ω1)GT2 −G2(Ω3 ⊗ Ir1)GT2 = Skew(UT2 ηU2G2G T 2 ) + Skew(G2ηTG2),\nG3GT3 Ω3 + Ω3G3GT3 −G3(Ir2 ⊗Ω1)GT3 −G3(Ω2 ⊗ Ir1)GT3 = Skew(UT3 ηU3G3G T 3 ) + Skew(G3ηTG3),\n(A.18)\nthat are solved efficiently with the Matlab’s pcg routine that is combined with a specific preconditioner resulting from the Gauss-Seidel approximation of (A.18).\nA.8 Proof of Proposition 4 (derivation of the Riemannian gradient formula)\nProof. Let f(X ) = ‖PΩ(X )−PΩ(X ?)‖2F /|Ω| and S = 2(PΩ(G×1U1×2U2×3U3)−PΩ(X ?))/|Ω| be an auxiliary sparse tensor variable that is interpreted as the Euclidean gradient of f in Rn1×n2×n3 .\nThe partial derivatives of f(U1,U2,U3,G) are ∂f1(U1,U2,U3,G1) ∂U1 = 2 |Ω|(PΩ(U1G1(U3 ⊗ U2) T )−PΩ(X?1))(U3 ⊗ U2)GT1 = S1(U3 ⊗ U2)GT1 , ∂f2(U1,U2,U3,G2) ∂U2 = 2 |Ω|(PΩ(U2G2(U3 ⊗ U1) T )−PΩ(X?2))(U3 ⊗ U1)GT2 = S2(U2 ⊗ U1)GT2 , ∂f3(U1,U2,U3,G3) ∂U3 = 2 |Ω|(PΩ(U3G3(U2 ⊗ U1) T )−PΩ(X?3))(U2 ⊗ U1)GT3 = S3(U2 ⊗ U1)GT3 , ∂f(U1,U2,U3,G) ∂G = 2 |Ω|(PΩ(G×1U1×2U2×3U3)−PΩ(X ?))×1 UT1 ×2 UT2 ×3 UT3\n= S ×1 UT1 ×2 UT2 ×3 UT3 , where X?d is mode-d unfolding of X ? and S1 = 2 |Ω|(PΩ(U1G1(U3 ⊗ U2) T )−PΩ(X?1)) S2 = 2 |Ω|(PΩ(U2G2(U3 ⊗ U1) T )−PΩ(X?2)) S3 = 2 |Ω|(PΩ(U3G3(U2 ⊗ U1) T )−PΩ(X?3))\nS = 2|Ω|(PΩ(G×1U1×2U2×3U3)−PΩ(X ?)).\nDue to the specific scaled metric (A.2), the partial derivatives of f are further scaled by ((G1GT1 )−1, (G2GT2 )−1, (G3GT3 )−1,I), denoted as egradxf (after scaling), i.e.,\negradxf = (S1(U3 ⊗ U2)GT1 (G1GT1 )−1,S2(U3 ⊗ U1)GT2 (G2GT2 )−1,S3(U2 ⊗ U1)GT3 (G3GT3 )−1, S ×1 UT1 ×2 UT2 ×3 UT3 ).\nConsequently, from the relationship that horizontal lift of grad[x]f is equal to gradxf = Ψ(egradxf), we obtain that, using (A.15),\nthe horizontal lift of grad[x]f = (S1(U3 ⊗ U2)GT1 (G1GT1 )−1 − U1BU1(G1GT1 )−1, S2(U3 ⊗ U1)GT2 (G2GT2 )−1 − U2BU2(G2GT2 )−1, S3(U2 ⊗ U1)GT3 (G3GT3 )−1 − U3BU3(G3GT3 )−1, S ×1 UT1 ×2 UT2 ×3 UT3 ),\nFrom the requirements in (A.16) for a vector to be in the tangent space, we have the following relationship for mode-1.\nBU1G1G T 1 + G1G T 1 BU1 = G1G T 1 (Y T U1U1 + U T 1 YU1)G1G T 1 ,\nwhere YU1 = (S1(U3 ⊗ U2)GT1 (G1GT1 )−1. Subsequently,\nG1GT1 (YTU1U1 + U T 1 YU1)G1G T 1 = G1GT1 { ((S1(U3 ⊗ U2)GT1 (G1GT1 )−1)TU1\n+ UT1 (S1(U3 ⊗ U2)GT1 (G1GT1 )−1 }\nG1GT1 = ((S1(U3 ⊗ U2)GT1 )TU1G1GT1 + G1GT1 UT1 (S1(U3 ⊗ U2)GT1 = (G1GT1 UT1 (S1(U3 ⊗ U2)GT1 )T + G1GT1 UT1 (S1(U3 ⊗ U2)GT1 = 2Sym(G1GT1 UT1 (S1(U3 ⊗ U2)GT1 ).\nFinally, BUd for d ∈ {1, 2, 3} are obtained by solving the Lyapunov equations BU1G1G T 1 + G1GT1 BU1 = 2Sym(G1G T 1 UT1 (S1(U3 ⊗ U2)GT2 ), BU2G2G T 2 + G2GT2 BU2 = 2Sym(G2G T 2 UT2 (S2(U3 ⊗ U1)GT2 ),\nBU3G3G T 3 + G3GT3 BU3 = 2Sym(G3G T 3 UT3 (S3(U2 ⊗ U1)GT3 ),\nwhere Sym(·) extracts the symmetric part of a square matrix, i.e., Sym(D) = (D + DT )/2. The above Lyapunov equations are solved efficiently with the Matlab’s lyap routine."
    }, {
      "heading" : "B Additional numerical comparisons",
      "text" : "In addition to the representative numerical comparisons in the paper, we show additional numerical experiments spanning synthetic and real-world datasets.\nExperiments on synthetic datasets: Case S1: comparison with the Euclidean metric. We first show the benefit of the proposed metric (A.2) over the conventional choice of the Euclidean metric that exploits the product structure ofM and symmetry. We compare steepest descent algorithms with Armijo backtracking linesearch for both the metric choices. Figure A.1 shows that the algorithm with the metric (A.2) gives a superior performance in test error than that of the conventional metric choice.\nCase S2: small-scale instances. We consider tensors of size 100×100×100, 150×150×150, and 200×200×200 and ranks (5, 5, 5), (10, 10, 10), and (15, 15, 15). OS is {10, 20, 30}. Figures A.2(a)-(c) and Figures A.3(a)-(c) show the convergence behavior of different algorithms on a train set Ω and on a test set Γ, where Figures A.3(b) is identical to the figure in the manuscript paper. Figures A.2(d)-(f) and A.3(d)-(f) show the mean square error on Ω and Γ on each algorithm. Furthermore, Figure A.2(g)-(i) and Figure A.3(g)-(i) show the mean square error on Ω and Γ when OS is 10 in all the five runs. From Figures A.2 and Figures A.3, our proposed algorithm is consistently competitive or faster than geomCG, HalRTC, and TOpt. In addition, the mean square errors on a train set Ω and a test set Γ are consistently competitive or lower than those of geomCG and HalRTC, especially for lower sampling ratios, e.g, for OS 10.\nCase S3: large-scale instances. We consider large-scale tensors of size 3000 × 3000 × 3000, 5000× 5000× 5000, and 10000× 10000× 10000 and ranks r=(5, 5, 5) and (10, 10, 10). OS is 10. We compare our proposed algorithm to geomCG. Figure A.4 and Figure A.5 show the convergence behavior of the algorithms. The proposed algorithm outperforms geomCG in all the cases.\nCase S4: influence of low sampling. We look into problem instances which result from scarcely sampled data. The test requires completing a tensor of size 10000× 10000× 10000 and rank r=(5, 5, 5). Figure A.6 and Figure A.7 show the convergence behavior when OS is {8, 6, 5}. The case of OS = 5 is particularly interesting. In this case, while the mean square errors on Ω and Γ increase for geomCG, the proposed algorithm stably decreases the error in all the five runs.\nCase S5: influence of ill-conditioning and low sampling. We consider the problem instance of Case S4 with OS = 5. Additionally, for generating the instance, we impose a diagonal core G with exponentially decaying positive values of condition numbers (CN) 5, 50, and 100. Figure A.8 shows that the proposed algorithm outperforms geomCG for all the considered CN values on a train set Ω.\nCase S6: influence of noise. We evaluate the convergence properties of algorithms under the presence of noise The tensor size and rank are same as in Case S4 and OS is 10. Figure A.9 shows that the train error on a train set Ω for each is almost identical to the 2‖PΩ(X ?)‖2F , but our proposed algorithm converges faster than geomCG.\nCase S7: rectangular instances. We consider instances where dimensions and ranks along certain modes are different than others. Two cases are considered. Case (7.a) considers tensors size 20000 × 7000×7000, 30000×6000×6000, and 40000×5000×5000 and rank r = (5, 5, 5). Case (7.b) considers a tensor of size 10000×10000×10000 with ranks r = (7, 6, 6), (10, 5, 5), and (15, 4, 4). Figures A.10(a)(c) and Figures A.11(a)-(c) show that the convergence behavior of our proposed algorithm is superior to\nthat of geomCG on Ω and Γ, respectively. Our proposed algorithm also outperforms geomCG for the asymmetric rank cases as shown in Figure A.10(d)-(f) and Figure A.11(d)-(f).\nCase S8: medium-scale instances. We additionally consider medium-scale tensors of size 500 × 500 × 500, 1000 × 1000 × 1000, and 1500 × 1500 × 1500 and ranks r = (5, 5, 5), (10, 10, 10), and (15, 15, 15). OS is {10, 20, 30, 40}. Our proposed algorithm and geomCG are only compared as the other algorithms cannot handle these scales efficiently. Figures A.12(a)-(c) and A.13(a)-(c) show the convergence behavior on Ω and Γ, respectively. Figures A.12(d)-(f) and Figures A.13(d)-(f) also show the mean square error on Ω and Γ of rank r = (15, 15, 15) in all the five runs. The proposed algorithm performs better than geomCG in all the cases.\nExperiments on real-world datasets: Case R1: hyperspectral image. We also show the performance of our algorithm on the hyperspectral image “Ribeira”. We show the mean square error on Ω and Γ when OS is {11, 22} in Figure A.14 and Figure A.15, where Figure A.15(a) is identical to the figure in the manuscript paper. Our proposed algorithm gives lower test errors than those obtained by the other algorithms. We also show the image recovery results. Figures A.16 and A.17 show the reconstructed images when OS is {11, 22}, respectively. From these figures, we find that the proposed algorithm shows a good performance, especially for the lower sampling ratio.\nCase R2: MovieLens-10M. Figure A.18 and Figure A.19 show the convergence plots for all the five runs of ranks r = (4, 4, 4), (6, 6, 6), (8, 8, 8) and (10, 10, 10) on Ω and Γ, respectively. These figures show the superior performance of our proposed algorithm.\nExperiments for online algorithms: Case O: online instances. Figure A.20 and A.21 show the convergence plots for all the five runs on tensors of ranks 100 × 100 × 5000, and 100 × 100 × 10000 with rank r = (5, 5, 5) on Ω and Γ, respectively. These figures show that the proposed stochastic gradient descent algorithm gives similar or faster convergence than the proposed batch gradient descent algorithm.\nFigure A.22 and A.23 show the convergence speed comparisons in the train error and the test error of the proposed online and batch algorithms with TeCPSGD and OLSTEC with rank r = (5, 5, 5) on the real-world video sequence Airport Hall dataset. These figures show that the proposed stochastic gradient descent algorithm gives similar or faster convergence than the proposed batch algorithm. In addition, Table B shows that the final train and test MSEs show the superior performance of the proposed algorithms.\n0 200 400 600\n10 −20\n10 −10\n10 0\nTime in seconds\nM e a n\ns q\nu a re\ne rr\no r\no n\nΩ\nProposed (ε=0) geomCG (ε=0) Proposed (ε=1e−12) geomCG (ε=1e−12) Proposed (ε=1e−10) geomCG (ε=1e−10) Proposed (ε=1e−08) geomCG (ε=1e−08) Proposed (ε=1e−06) geomCG (ε=1e−06) Proposed (ε=0.0001) geomCG (ε=0.0001)\nFigure A.9: Case S6: noisy data on Ω (train error)."
    } ],
    "references" : [ {
      "title" : "Optimization Algorithms on Matrix Manifolds",
      "author" : [ "Absil", "P.-A", "R. Mahony", "R. Sepulchre" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Stochastic gradient descent on Riemannian manifolds",
      "author" : [ "S. Bonnabel" ],
      "venue" : "IEEE Trans. Autom. Control,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Stochastic gradient descent tricks",
      "author" : [ "L. Bottou" ],
      "venue" : "Neural Networks: Tricks of the Trade (2nd ed.),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Low-rank matrix completion via preconditioned optimization on the Grassmann manifold",
      "author" : [ "N. Boumal", "Absil", "P.-A" ],
      "venue" : "Linear Algebra Appl.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Manopt: a Matlab toolbox for optimization on manifolds",
      "author" : [ "N. Boumal", "B. Mishra", "Absil", "P.-A", "R. Sepulchre" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E.J. Candès", "B. Recht" ],
      "venue" : "Found. Comput. Math.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "The geometry of algorithms with orthogonality constraints",
      "author" : [ "A. Edelman", "T.A. Arias", "S.T. Smith" ],
      "venue" : "SIAM J. Matrix Anal. Appl.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1998
    }, {
      "title" : "Tucker factorization with missing data with application to low-n-rank tensor completion",
      "author" : [ "M. Filipović", "A. Jukić" ],
      "venue" : "Multidim. Syst. Sign. P.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Information limits on neural identification of colored surfaces in natural scenes",
      "author" : [ "D.H. Foster", "S.M.C. Nascimento", "K. Amano" ],
      "venue" : "Visual Neurosci.,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "Online low-rank tensor subspace tracking from incomplete data by CP decomposition using recursive least squares",
      "author" : [ "H. Kasai" ],
      "venue" : "In IEEE ICASSP,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Riemannian preconditioning for tensor completion",
      "author" : [ "Kasai", "Hiroyuki", "Mishra", "Bamdev" ],
      "venue" : "Technical report, arXiv preprint arXiv:1506.02159,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "T.G. Kolda", "B.W. Bader" ],
      "venue" : "SIAM Rev.,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Low-rank tensor completion by Riemannian optimization",
      "author" : [ "D. Kressner", "M. Steinlechner", "B. Vandereycken" ],
      "venue" : "BIT Numer. Math.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Introduction to smooth manifolds, volume 218 of Graduate Texts in Mathematics",
      "author" : [ "J.M. Lee" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2003
    }, {
      "title" : "Tensor completion for estimating missing values in visual data",
      "author" : [ "J. Liu", "P. Musialski", "P. Wonka", "J. Ye" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Subspace learning and imputation for streaming big data matrices and tensors",
      "author" : [ "M. Mardani", "G. Mateos", "G.B. Giannakis" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "R3MC: A Riemannian three-factor algorithm for low-rank matrix completion",
      "author" : [ "B. Mishra", "R. Sepulchre" ],
      "venue" : "In IEEE CDC, pp",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Scaled gradients on Grassmann manifolds for matrix completion",
      "author" : [ "T. Ngo", "Y. Saad" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Numerical Optimization, volume Second Edition",
      "author" : [ "J. Nocedal", "S.J. Wright" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "Optimization methods on Riemannian manifolds and their application to shape space",
      "author" : [ "W. Ring", "B. Wirth" ],
      "venue" : "SIAM J. Optim.,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "A new, globally convergent Riemannian conjugate gradient method",
      "author" : [ "H. Sato", "T. Iwai" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Tensor versus matrix completion: A comparison with application to spectral data",
      "author" : [ "M. Signoretto", "Plas", "R.V. d", "B.D. Moor", "J.A.K. Suykens" ],
      "venue" : "IEEE Signal Process. Lett.,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Learning with tensors: a framework based on convex optimization and spectral regularization",
      "author" : [ "M. Signoretto", "Q.T. Dinh", "L.D. Lathauwer", "J.A.K. Suykens" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Estimation of low-rank tensors via convex optimization",
      "author" : [ "R. Tomioka", "K. Hayashi", "H. Kashima" ],
      "venue" : "Technical report, arXiv preprint arXiv:1010.0789,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Low-rank matrix completion by Riemannian optimization",
      "author" : [ "B. Vandereycken" ],
      "venue" : "SIAM J. Optim.,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Problem (1) has many variants, and one of those is extending the nuclear norm regularization approach from the matrix case [6] to the tensor case.",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 14,
      "context" : "While this generalization leads to good results [15, 25, 24], its applicability to large-scale instances is not trivial, especially due to the necessity of high-dimensional singular value decomposition computations.",
      "startOffset" : 48,
      "endOffset" : 60
    }, {
      "referenceID" : 23,
      "context" : "While this generalization leads to good results [15, 25, 24], its applicability to large-scale instances is not trivial, especially due to the necessity of high-dimensional singular value decomposition computations.",
      "startOffset" : 48,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "While this generalization leads to good results [15, 25, 24], its applicability to large-scale instances is not trivial, especially due to the necessity of high-dimensional singular value decomposition computations.",
      "startOffset" : 48,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : ", in [8, 13].",
      "startOffset" : 5,
      "endOffset" : 12
    }, {
      "referenceID" : 12,
      "context" : ", in [8, 13].",
      "startOffset" : 5,
      "endOffset" : 12
    }, {
      "referenceID" : 10,
      "context" : "This paper extends the earlier work [11] to include a stochastic gradient descent algorithm for low-rank tensor completion.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "The multilinear rank constraint forms a smooth manifold [13].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "While preconditioning in unconstrained optimization is well studied [20, Chapter 5], preconditioning on constraints with symmetries, owing to non-uniqueness of Tucker decomposition [12], is not straightforward.",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 0,
      "context" : "We build upon the recent work [18] that suggests to use preconditioning with a tailored metric (inner product) in the Riemannian optimization framework on quotient manifolds [1, 7, 18].",
      "startOffset" : 174,
      "endOffset" : 184
    }, {
      "referenceID" : 6,
      "context" : "We build upon the recent work [18] that suggests to use preconditioning with a tailored metric (inner product) in the Riemannian optimization framework on quotient manifolds [1, 7, 18].",
      "startOffset" : 174,
      "endOffset" : 184
    }, {
      "referenceID" : 12,
      "context" : "[13], which also exploits the manifold structure, are twofold.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] exploit the search space as an embedded submanifold of the Euclidean space, whereas we view it as a product of simpler search spaces with symmetries.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] work with the standard Euclidean metric, whereas we use a metric that is tuned to the least-squares cost function, thereby inducing a preconditioning effect.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "They also connect to state-of-the-art algorithms proposed in [19, 27, 17, 4].",
      "startOffset" : 61,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "They also connect to state-of-the-art algorithms proposed in [19, 27, 17, 4].",
      "startOffset" : 61,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "They also connect to state-of-the-art algorithms proposed in [19, 27, 17, 4].",
      "startOffset" : 61,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "Our proposed algorithms are implemented in the Matlab toolbox Manopt [5].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "The Tucker decomposition of a tensor X ∈ Rn1×n2×n3 of rank r (=(r1, r2, r3)) is X = G×1U1×2U2×3U3, (2) where Ud ∈ St(rd, nd) for d ∈ {1, 2, 3} belongs to the Stiefel manifold of matrices of size nd × rd with orthogonal columns and G ∈ Rr1×r2×r3 [12].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 13,
      "context" : "(4) The set of equivalence classes is the quotient manifold [14] M/∼ := M/(O(r1)×O(r2)×O(r3)), (5)",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Consequently, the problem (1) is an optimization problem on a quotient manifold for which systematic procedures are proposed in [1, 7].",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : "Consequently, the problem (1) is an optimization problem on a quotient manifold for which systematic procedures are proposed in [1, 7].",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 18,
      "context" : "In unconstrained optimization, the Newton method is interpreted as a scaled steepest descent method, where the search space is endowed with a metric (inner product) induced by the Hessian of the cost function [20].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 6,
      "context" : ", TxM has the matrix characterization [7] TxM = {(ZU1 ,ZU2 ,ZU3 ,ZG) ∈ Rn1×r1 × Rn2×r2 × Rn3×r3 × Rr1×r2×r3 : Ud ZUd + Z T Ud Ud = 0, for d ∈ {1, 2, 3}}.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "From [1], endowed with the Riemannian metric (9), the quotient manifoldM/∼ is a Riemannian submersion ofM.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : ", the gradient of a smooth cost function [1].",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "A retraction is a mapping that maps vectors in the horizontal space to points on the search spaceM and satisfies the local rigidity condition [1].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "In the batch setting, we use the off-the-shelf conjugate gradient implementation of Manopt for any smooth cost function [5].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "In the online setting, we use the stochastic gradient descent implementation [2].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 20,
      "context" : "For fixed rank, theoretical convergence of the Riemannian algorithms are to a stationary point, and the convergence analysis follows from [22, 21, 2].",
      "startOffset" : 138,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : "For fixed rank, theoretical convergence of the Riemannian algorithms are to a stationary point, and the convergence analysis follows from [22, 21, 2].",
      "startOffset" : 138,
      "endOffset" : 149
    }, {
      "referenceID" : 1,
      "context" : "For fixed rank, theoretical convergence of the Riemannian algorithms are to a stationary point, and the convergence analysis follows from [22, 21, 2].",
      "startOffset" : 138,
      "endOffset" : 149
    }, {
      "referenceID" : 16,
      "context" : "Following [17, 26, 13], the least-squares structure of the cost function in (1) is exploited to compute a linearized step-size guess efficiently along a search direction by considering a polynomial approximation of degree 2 over the manifold.",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 24,
      "context" : "Following [17, 26, 13], the least-squares structure of the cost function in (1) is exploited to compute a linearized step-size guess efficiently along a search direction by considering a polynomial approximation of degree 2 over the manifold.",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "Following [17, 26, 13], the least-squares structure of the cost function in (1) is exploited to compute a linearized step-size guess efficiently along a search direction by considering a polynomial approximation of degree 2 over the manifold.",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "Following [3], we select γ0 in the pre-training phase using a small sample size of a training set.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 12,
      "context" : "It should be stressed that the computational cost of our conjugate gradient implementation is equal to that of [13].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 12,
      "context" : "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 14,
      "context" : "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.",
      "startOffset" : 276,
      "endOffset" : 280
    }, {
      "referenceID" : 23,
      "context" : "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.",
      "startOffset" : 289,
      "endOffset" : 293
    }, {
      "referenceID" : 22,
      "context" : "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.",
      "startOffset" : 304,
      "endOffset" : 308
    }, {
      "referenceID" : 15,
      "context" : "In the online setting, we compare our proposed stochastic gradient descent algorithm with CANDECOMP/PARAFAC based TeCPSGD [16] and OLSTEC [10].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 9,
      "context" : "In the online setting, we compare our proposed stochastic gradient descent algorithm with CANDECOMP/PARAFAC based TeCPSGD [16] and OLSTEC [10].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 12,
      "context" : "Algorithms are initialized randomly, as suggested in [13], and are stopped when either the mean square error (MSE) on the train set Ω is below 10−12 or the number of iterations exceeds 250.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "We evaluate the convergence properties of algorithms under the presence of noise by adding scaled Gaussian noise PΩ(E) to PΩ(X ?) as in [13].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "Figure 2(g) shows that the test error for each is almost identical to the ‖PΩ(X )‖F [13], but our proposed algorithm converges faster than geomCG.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "We consider the hyperspectral image “Ribeira” [9] discussed in [23, 13].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "We consider the hyperspectral image “Ribeira” [9] discussed in [23, 13].",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "We consider the hyperspectral image “Ribeira” [9] discussed in [23, 13].",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 21,
      "context" : "As suggested in [23, 13], we resize it to 203 × 268 × 33.",
      "startOffset" : 16,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "As suggested in [23, 13], we resize it to 203 × 268 × 33.",
      "startOffset" : 16,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "We perform five random samplings of the pixels based on the OS values 11 and 22, corresponding to the rank r=(15, 15, 6) adopted in [13].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "While OS = 22 corresponds to the observation ratio of 10% studied in [13], OS = 11 considers a challenging scenario with the observation ratio of 5%.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : "We compare the proposed stochastic gradient descent algorithm with its batch counterpart gradient descent algorithm and with TeCPSGD [16] and OLSTEC [10].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "We compare the proposed stochastic gradient descent algorithm with its batch counterpart gradient descent algorithm and with TeCPSGD [16] and OLSTEC [10].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "References [1] Absil, P.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] Bonnabel, S.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Bottou, L.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Boumal, N.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Boumal, N.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Candès, E.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Edelman, A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Filipović, M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Foster, D.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Kasai, H.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Kasai, Hiroyuki and Mishra, Bamdev.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Kolda, T.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Kressner, D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Lee, J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Liu, J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Mardani, M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] Mishra, B.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[19] Ngo, T.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[20] Nocedal, J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[21] Ring, W.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[22] Sato, H.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[23] Signoretto, M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[24] Signoretto, M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25] Tomioka, R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[26] Vandereycken, B.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "From [1], the tangent space has the matrix characterization TxM = {(ZU1 ,ZU2 ,ZU3 ,ZG) ∈ Rn1×r1 × Rn2×r2 × Rn3×r3 × Rr1×r2×r3 : Ud ZUd + Z T Ud Ud = 0, for d ∈ {1, 2, 3}}.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "Additionally from [1], ηUd has the characterization ηUd = UdΩ + Ud⊥K, (A.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "From the characterization of linearization of an orthogonal matrix [1], we have the characterization for the vertical space as Vx = {(U1Ω1,U2Ω2,U3Ω3,−(G×1Ω1 + G×2Ω2 + G×3Ω3)) : Ωd ∈ Rrd×rd ,Ωd = −Ωd for d ∈ {1, 2, 3}}.",
      "startOffset" : 67,
      "endOffset" : 70
    } ],
    "year" : 2016,
    "abstractText" : "We propose a novel Riemannian manifold preconditioning approach for the tensor completion problem with rank constraint. A novel Riemannian metric or inner product is proposed that exploits the least-squares structure of the cost function and takes into account the structured symmetry that exists in Tucker decomposition. The specific metric allows to use the versatile framework of Riemannian optimization on quotient manifolds to develop preconditioned nonlinear conjugate gradient and stochastic gradient descent algorithms for batch and online setups, respectively. Concrete matrix representations of various optimization-related ingredients are listed. Numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets1.",
    "creator" : "LaTeX with hyperref package"
  }
}