{
  "name" : "1704.06850.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Testing from One Sample: Is the casino really using a riffle shuffle?",
    "authors" : [ "Constantinos Daskalakis", "Nishanth Dikkala", "Nick Gravin" ],
    "emails" : [ "costis@mit.edu", "nishanthd@csail.mit.edu", "ngravin@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In the first part of the paper, we propose a measure of difference between two Markov chains, which captures the scaling behavior of the total variation distance between words sampled from the Markov chains as the length of these words grows. We provide efficient and sample nearoptimal testers for identity testing under our proposed measure of difference. In the second part of the paper, we study Markov chains whose state space is exponential in their description, providing testers for testing identity of card shuffles. We apply our results to testing the validity of the Gilbert, Shannon, and Reeds model for the riffle shuffle.\n∗Supported by a Microsoft Research Faculty Fellowship, and NSF Award CCF-1551875, CCF-1617730 and CCF1650733.\n†Supported by NSF Award CCF-1551875, CCF-1617730 and CCF-1650733. ‡Supported by NSF Award CCF-1551875, CCF-1617730 and CCF-1650733.\nar X\niv :1\n70 4.\n06 85\n0v 1\n[ cs\n.L G\n] 2\n2 A\npr 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "We formulate theories about the laws that govern physical phenomena by making observations and testing them against our hypotheses. A common scenario is when our observations can be reasonably modeled as i.i.d. samples from a distribution that we are trying to understand. This is the setting tackled by most classical work in Statistics. Of course, having access to i.i.d. samples from a distribution is rare and quite commonly a mere approximation of reality. We typically only have access to approximate samples from a stationary distribution, sampled by a Markov chain whose transition matrix/kernel is unknown to us and which can only be observed for some finite time horizon. In fact, to the best of our knowledge, the underlying Markov chain may not even be rapidly mixing, so there is no guarantee that we will ever see samples that are approximately distributed according to the stationary distribution.\nThese issues are exacerbated in high-dimensional settings, e.g., when observing the configurations of a deck of cards or a weather system, where it may also be completely impractical to work with the high-dimensional stationary distribution itself. Moreover, there are several ways to sample a stationary distribution, so it may be more important to know how it comes to be sampled. For all these considerations, it may be both more interesting and more practical to understand the “mechanics” of the process that generates our observations, namely the transition matrix/kernel of the Markov chain whose evolution we get to observe.\nMotivated by these considerations, in this paper we study the problem of testing the identity of Markov chains. We are given access to a single trajectory X0, X1, . . . , Xt, . . . of some unknown Markov chain M over some state space [n], and we want to test the identity of M to some given Markov chain M′. Importantly, we do not get to control the distribution of the starting state X0, and we can only observe a single trajectory of M, i.e. we cannot restart the Markov chain. What could we hope to achieve?\nIf there is any difference in the transition matrices ofM andM′, one would think that we would ultimately be able to identify it by observing a sufficiently long trajectory. This is certainly true if the transition matrices of the two chains differ at a state that belongs to the strongly connected component of M absorbing the observed trajectory (Xt)t. For instance, suppose that M is a chain on states {1, 2, . . . , 7} whose transition matrix is the random walk matrix on a graph that is the disjoint union of a square on nodes {1, . . . , 4} and a triangle on nodes {5, 6, 7}, while M′’s transition matrix is the random walk matrix on a graph that is the disjoint union of a clique on nodes {1, . . . , 4} and a triangle on nodes {5, 6, 7}. If our observed trajectory ofM lies in the strong connected component defined by states {1, . . . , 4}, we will easily identify its difference to M′. On the other hand, if our observed trajectory of M lies on the strong component defined by states {5, 6, 7}, we will not be able to identify that we are not observing a trajectory of M′, no matter how long the trajectory is.\nFor some notion of difference, Dist (M,M′), between Markov chains, we would like to quantify how long a trajectory X0, . . . , X` from an unknown chain, M, we need to observe to be able to distinguish, with probability at least 1− δ:\nM =M′ versus Dist ( M,M′ ) > , (1)\nfor some given parameters δ ∈ (0, 1) and > 0. Let us call this problem single-sample goodness-offit (or identity) testing for Markov chains. We will study it taking δ = 1/3, with the understanding that this probability can be boosted to any small constant at the cost of a O(log(1/δ)-multiplicative factor in the length ` of the observed trajectory.\nWhat notion of difference between Markov chains is the right one to use to study the aforedescribed goodness-of-fit testing problem is not clear. One of our contributions is to identify a meaningful measure of difference in Section 2. What are the desiderata for such a measure? Let us discuss this:\n1. First, as our simple example above illustrates, under a worst-case starting state X0, we may not be able to identify that M 6=M′ from a single trajectory. So, we would like to identify a notion of difference that takes a value Dist (M,M′) = 0, whenever chains M and M′ are indistinguishable from a single trajectory.\n2. Whenever M and M′ are distinguishable from a single trajectory, whose starting state we do not get to control, i.e. from any starting state, we would like that our difference measure quantifies how different the chains are. Clearly, our notion of difference could not just be a combinatorial property of the connectivity of the state space of M and M′, since the combinatorial structure won’t reflect the magnitude of the differences in the chains.\nA natural notion of difference between two chains M and M′ is the total variation distance between the trajectories (a.k.a. words) WtM def = X0X1 · · ·Xt and WtM′ def = Y0Y1 · · ·Yt sampled from the two chains in some t steps and starting at some state X0 = s0 = Y0. Indeed, this distance captures how different our t-step observations from the two chains are. As the starting state s0 is out of our control, we should rather use the\nmin s0 dTV\n( WtM ,W t M′ | X0 = Y0 = s0 ) . (2)\nIn particular, taking the min makes sure that Property 1 above is satisfied. While there is sometimes a natural t to use in (2) as we will see in Section 4, where we analyze card shuffles, it is mostly unclear how to set t. Setting t to infinity will make the above quantity take 0/1 values, which is un-informative about how different the two chains are. Setting t too small would only capture differences in the proximity of the starting state s0. Hence, setting t to some reasonably large but finite value makes more sense, but how to choose it?\nTo avoid the conundrum, we propose a notion of difference between Markov chainsM andM′, which captures the scaling behavior, as t → ∞, of (2). Interestingly, we argue in Section 2 that this scaling behavior is tightly captured by spectral properties of the following matrix:\n[P,Q]√ def = [√ Pij ·Qij ] ij∈[n×n] ,\nwhere P and Q are the transition matrices of the two chains, i.e. Pij and Qij denote the probabilities of transitioning from state i to state j in the two chains. In Eq. (4), we show a recursive decomposition that allows us to exactly express the square Hellinger similarity, 1−d2 Hel ( WtM ,W t M′ ) of `-length words sampled from the two chains in terms of the `-th power of the above matrix, and the distribution of the starting states X0 and Y0 in the two words. Given the relationship between Hellinger and total variation distance (see Eq. (3)), the `-the power of [P,Q]√ also captures the total variation distance between words sampled from the two chains.\nTo identify a word-length independent measure of difference between the two chains, we argue that the scaling behavior of the Hellinger distance/total variation distance is captured by the largest eigenvalue λ1 = ρ([P,Q]√) of matrix [P,Q]√. We show that always λ1 ≤ 1 (Claim 1), and that λ1 = 1\nif and only if the two chains have an identical connected component (Claim 1), hence we would be unable to identify the difference between the two chains from a single trajectory and a worst-case starting state, as per our discussion above. Furthermore, the slowest (with respect to the choice of the starting state) that the square Hellinger similarity of the two chains can drop as a function of the length ` is λ`1, up to factors that do not depend on `; this follows from (4) and (6). That is, the slowest that the square Hellinger distance of the two chains can increase is 1 − O(λ`1). Given these, and the intimate relationship between Hellinger and total variation distance (Eq. (3)), we propose the use of Dist (M,M′) = 1−ρ([P,Q]√) as a scale-free and meaningful measure of difference between Markov chains. As per our discussion, this notion of distance satisfies Desiderata 1 and 2 outlined above. For symmetric Markov chains, our notion of difference is even more tightly related to the scaling behavior of trajectories sampled from the two chains even for a starting state sampled from the stationary distribution, as per Claim 2. Figure 1 illustrates how Dist (M,M′) behaves for different pairs of Markov chains M and M′.\nOur Results. Using our proposed measure of difference between Markov chains we provide algorithms for goodness-of-fit testing of Markov chains, namely Problem (1), targeting two interesting regimes. The first targets applications where the state space is polynomial in the representation of the target Markov chain M′. The second targets settings where the state space is exponential in the representation of the target chain, but the chain has sparsity and structure. Importantly, this is applicable to testing card shuffles. Our results in the two settings are the following.\nIn Section 3, we study Problem (1) under Dist (M,M′) = 1 − ρ([P,Q]√), where P and Q are the transition matrices of chains M and M′. We study this problem when M and M′ are both symmetric, and provide near-optimal upper and lower bounds for the minimum length ` of a trajectory from the unknown chain M that is needed to determine the correct answer with probability at least 2/3. In particular, Theorems 3.1 and 3.2 combined show that the length of the required trajectory from M to answer Problem (1) is n/ , where n is the size of the state space, up to logarithmic factors and an additive term that does not depend on orM. Our upper bound is established via an information-efficient reduction from single-sample identity testing for Markov chains with n states to the classical problem of identity testing of distributions over O(n2) elements, from i.i.d. samples. The naive way to obtain such a reduction is to look at every MixTM′th step of the trajectory of M, where MixTM′ is the mixing time of chain M′, pretending that these transitions are i.i.d. samples from the distribution { 1nPij}ij∈[n2]. This incurs an unnecessary blow-up of a factor of MixTM′ in the required length of the observed trajectory, which we show how to avoid, exchanging it with an additive term that is quasi-linear in the hitting time.\nIn Section 4, we take on the challenge of testing Markov chains whose nominal state-space is exponentially large in their representation, such as shuffles. Often, shuffles have symmetries that allow studying their transitions in a state space of manageable size. For example, the random choices that the riffle shuffle makes in the course of one step do not depend on the permutation of the cards at the beginning of the step, and can be described succinctly. Moreover, viewed appropriately the transitions out of any state are typically sparse. In the riffle shuffle, starting from any permutation there are n+ 1 places to cut the deck. And, starting from a cut, every little step of the riffle has two options (whether to drop a card from the left or the right stack). So breaking down one step of the riffle shuffle into a sequence of simple steps makes the transitions very sparse.\nIn Definition 2 we provide a model of sparse Markov chains, capturing succinct representations of Markov chains resulting from “breaking up into trivial steps and projecting into a smaller state\nspace” of Markov chains with exponentially large state-spaces such as different variants of the riffle shuffle and other shuffles [BD92]. Roughly speaking a sparse Markov chain in our model performs transitions according to a sequence of transition matrices P1, . . . , Pn, over and over again, ad infinitum. We discuss how this model captures the essential mechanics of the riffle shuffle after Definition 2.\nKeeping the riffle shuffle as our running application, we develop tools that allow us to perform goodness-of-fit testing of sparse Markov chains according to our model. What difference measure between chains should we use? Given two sparse chains P = (Pi)i∈[n] and Q = (Qi)i∈[n], conforming to our model, we could still define our difference measure between them using spectral properties of matrix [P1, Q1]√ · · · [Pt, Qt]√ · · · [Pn, Qn]√. However, we find it more natural in this case to use as difference measure the total variation distance between words sampled in one round of sampling transitions from the sequence of matrices P1, . . . , Pn and Q1, . . . , Qn.\n1 This total variation distance captures the divergence of the two chains in one iteration through their transitions matrices; in our running application to riffle shuffle, they correspond to the divergence of two riffle shuffles of different parameters in one iteration of the shuffle. With this notion of difference we provide efficient testers, and sample complexity lower bounds; see Theorems 4.1 and 4.4. Our upper bounds imply, in particular, that we can test goodness-of-fit of a given riffle shuffle model, such as the GilbertShannon-Reeds model, against all competing riffle shuffle models at distance ≥ , from O(n3/2/ 2) shuffles. Our tester, applying to testing any sparse Markov chain model, is based on a modified and pruned χ2-style statistic, inspired by [ADK15], which tracks the number of times a particular transition between two states occured in the observed trajectory of the Markov chain.\nRelated Work. Testing goodness-of-fit for distributions has a long history in Statistics; for some old and more recent references see, e.g., [Pea00, Fis35, RS81, Agr12]. In this literature the emphasis has been on the asymptotic analysis of tests, pinning down their error exponents as the number of samples tends to infinity [Agr12, TAW10]. In the last two decades or so, distribution testing has also piqued the interest of theoretical computer scientists [BFF+01, Pan08, LRR13, VV14, CDVV14, ADK15, CDGR16, DK16, DDS+13, CRS14], where the emphasis, in contrast, has been on minimizing the number of samples required to test hypotheses with a strong control for both type I and type II errors. A few recent works have identified tight upper and lower bounds on the sample complexity of various testing problems [Pan08, VV14, ADK15, DK16]. All of the papers in this vast body of literature assume access to i.i.d. samples from the underlying distribution.\nSome work in Statistics has considered the problem of testing with dependent samples. For instance, [Bar51, M+82, GM+83, MMPV02] and the references therein study goodness-of-fit testing for Markov chains. [TA83] and more recently [BPR16] study the problem of testing the stationary distribution of Markov chains. [Kaz78] studies the problem of detection between two Markov chains. All these works focus on the asymptotic regime where the length of the observed trajectories tends to infinity, as opposed to the non-asymptotic regime that we study here. In the computer science literature [BFR+13] considered the problem of testing the property whether a Markov chain is fast mixing or not. They defined a notion of closeness between two random walks started at different states of the same chain, which is similar in spirit to the distance notion we define in this work.\nThere is a large body of statistical literature on estimating properties of Markov chains such as mixing time. The question of estimation is related to but different than the goodness-of-fit testing\n1We argue, in Section 4, that the spectral and total variation distance approaches to define difference in this model are tightly related, as they are in the symmetric case.\nthat we perform here. A particularly important parameter is the mixing time of a Markov chain, as it is useful in designing MCMC algorithms. [HKS15] and the references therein study the problem of mixing time estimation in Markov chains.\nOrganization We start with a description of the notational conventions we use and a formal definition of our distance notion between Markov chains in Section 2. In Section 3, we study the problem of testing identity of symmetric Markov chains. We present our tester along with a sample complexity lower bound for this problem in this Section. In Section 4, we study the identity testing question on riffle shuffles. We present our formulation of this question as a problem of testing identity for sparse Markov chains and present upper and lower bounds for this problem. Finally, in Section 5, we conclude with some open questions from the framework introduced in this paper."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "A discrete-time Markov chain is a stochastic process {Xt}t∈{0,1,...} over a state space S which satisfies the Markov property: the probability of being in state s at time t+ 1 depends only on the state at previous time t. In this paper, we only consider Markov chains with a finite state space such that |S| = n. Such Markov chains can be completely specified by a n×n transition matrix (kernel) that contains probabilities of transitioning from state i to state j in the ith row and jth column. The transition matrix has non-negative entries and is a stochastic matrix. We use capital letters P,Q,M to represent Markov chains as well as their respective transition matrices. The stationary distribution π of a Markov chain P is a distribution over the state space S (written as a column vector) such that it satisfies π> ◦ P = π>. An important parameter in the study of Markov chains is the distribution of the starting state s0 which we denote by ~p (for the Markov chain P ). It may or not may not be the stationary distribution. In many cases it will be the distribution with all probability mass at a single state. Two popular notions of distance between distributions will be used heavily in this paper. We state their formal definitions below.\nDefinition 1. The total variation and Hellinger distances between distributions p, q over [k]: dTV (p, q) def = 12 ∑ i∈[k] |pi − qi|; d2Hel (p, q) def = 12 ∑ i∈[k] (√ pi − √ qi )2 = 1− ∑ i∈[k] √ pi · qi;\n√ 2 · d\nHel (p, q) ≥ dTV (p, q) ≥ d 2 Hel (p, q) . (3)\nWe now state without proof a well-known result which relates the total variation distance between two distributions to the best achievable distinguishability between the two. This will be useful in understanding our proposed notion of distance between two chains.\nLemma 2.1. Given two distributions p, q and a sample x drawn from either p or q, no algorithm can distinguish whether x was drawn from p or q with probability more than 1+d TV (p,q)\n2 ."
    }, {
      "heading" : "2.1 Notations",
      "text" : "We list the general notational conventions used in this paper. We denote vectors by small letters such as ~v and matrices by capital letters such as A,B,P ,Q. The ith entry of vector ~v is denoted by vi or v[i] and the (ij)th entry of matrix A (ith row, jth column) is denoted by Aij or A[ij]; ~ei denotes the\nstandard basis vector with 1 in its ith coordinate and 0 elsewhere; ~1 denotes the vector of all ones. The “entrywise” L1 and L2 norms of a matrix A are respectfully denoted as ‖A‖1 and ‖A‖2; ρ (A) denotes the spectral radius of matrix A, i.e., the maximum absolute eigenvalue of A. The eigenvalues of A are denoted by λ1, . . . , λi, . . . , λn and the respective right eigenvectors by ~v1, . . . , ~vi, . . . , ~vn (left eigenvectors by ~u1, . . . , ~un); for symmetric matrix A we assume that λ1 ≥ · · · ≥ λi ≥ · · · ≥ λn.\nBefore formulating the precise question we study, we need a notion of distance between Markov chains to work with."
    }, {
      "heading" : "2.2 Distance between Markov Chains",
      "text" : "We begin by considering the following simple question: how close is the behavior of two given Markov chains P and Q? A natural notion of distance would tell us how easy it is to distinguish which Markov chain P or Q a word w = s0 → s1 · · · → s` of certain length ` was generated from. The answer to this question is precisely captured by the total variation distance dTV ( W` P ,W` Q\n) between word distributions W`\nP , W` Q for words of length ` generated by Markov chains P , and\nrespectively Q (see Lemma 2.1). As a proxy for the total variation distance dTV\n( W`\nP ,W` Q\n) , it\nis more convenient to use square of the Hellinger distance d2 Hel\n( W`\nP ,W` Q\n) or the closely related\nBhattacharya coefficient2, which is useful for studying divergence of non-stationary and continuous Markov chains as was observed in [Kaz78]. Similar to [Kaz78], we can establish nice recurrence relations for the Bhattacharya coefficient of two word distributions, which is captured by the matrix\n[P,Q]√ def = [√ Pij ·Qij ] i,j∈[n×n].\n1− d2 Hel\n( W`\nP ,W` Q ) = [~p, ~q]>√ ◦ ( [P,Q]√ )` ◦ ~1, (4)\n1− d2 Hel\n( W`\nP ,W` Q\n) = ∑ w=s0...s` √ Pr P [w] Pr Q [w] =  ∑ w=s0...s` s`=s √ Pr P [w] Pr Q [w]  >\ns∈[n]\n◦ ~1\n= ∑ r∈[n] √ Pr P [r → s] Pr Q [r → s] ∑\nw=s0...s`−1 s`−1=r\n√ Pr P [w] Pr Q [w]  >\ns∈[n]\n◦ ~1\n=  ∑ w=s0...s`−1 s`−1=r √ Pr P [w] Pr Q [w]  >\nr∈[n]\n◦  ... · · · √ Prs ·Qrs · · ·\n...\n r,s∈[n×n] ◦ ~1\n=  ∑ w=s0...s`−1 s`−1=r √ Pr P [w] Pr Q [w]  >\nr∈[n]\n◦ [P,Q]√ ◦ ~1 = [~p, ~q] > √ ◦ ( [P,Q]√ )` ◦ ~1,\n2Hellinger distance is tightly related to the Bhattacharya coefficient between two distributions which is defined as BC(p, q) = ∑ i∈[k] √ pi · qi. It captures similarity of two distributions and lies in [0, 1].\nwhere ~p and ~q are vectors of initial distributions of s0 over [n], and [~p, ~q]√ def = [√ ps · qs ] s∈[n]. An important observation is that the distance between W` P and W` Q\nabove depends on the initial distribution of the first state in w, and also the length ` of the word.\nAssumption on the starting state. We study two reasonable models for the choice of the starting state: (i) a worst-case model where both P and Q begin from the same state i, which is chosen in adversarial manner to make P and Q look as much alike as possible; (ii) an average-case model, where the initial distributions ~p = ~q for P and Q either are given to us, or are related to P and Q in some natural way3. Given the assumption on the starting state we want to answer the question of what ` to pick, so that W`\nP and W` Q are far apart in squared Hellinger distance (say\n≥ 0.50). For the worst-case and average-case starting state models we respectfully get\nmin `>0\n` : ∀i ∈ [n] 0.5 ≥ 1− d2 Hel\n( W`\nP ,W` Q ) = ~e>i ◦ ( [P,Q]√ )` ◦ ~1. (5)\nmin `>0\n` : 0.5 ≥ 1− d2 Hel\n( W`\nP ,W` Q ) = [~p, ~q]>√ ◦ ( [P,Q]√ )` ◦ ~1\nDue to the relation between Hellinger and total variation distances, the inequality (5) holds for 1 − dTV ( W` P ,W` Q ) but for a slightly different than 0.5 constant. We call minimal ` that satisfies\ndTV\n( W`\nP ,W` Q ) ≥ 23 either for all starting states i ∈ [n], or for fixed ~p, ~q the minimal distinguishing\nlength. We note that (5) gives us an estimate on ` up to a constant factor. We note that when ` is large, the behavior of RHS of (5) is governed by the largest eigenvalue λ1 = ρ ( [P,Q]√ ) of [P,Q]√. By Perron-Frobenius theorem, we have that the largest eigenvalue of [P,Q]√ is non-negative and the corresponding left eigenvector ~u1 : ~u > 1 ◦ [P,Q]√ = λ1 · ~u>1 has non-negative coordinates. In particular, if we choose initial distributions ~p = ~q proportional to ~u1, then\n~p> ◦ (\n[P,Q]√ )` ◦ ~1 = λ`1 · 〈~p, ~1〉 = λ`1. (6)\nClaim 1. It is always true that λ1 = ρ ( [P,Q]√ ) ≤ 1. Moreover, λ1 = 1 iff P and Q have an identical connected component4.\nProof. Note that P+Q2 is a stochastic matrix that entry-wise dominates matrix [P,Q]√ with nonnegative entries. Therefore, λ1 · 〈~u1, ~1〉 = ~u>1 ◦ [P,Q]√ ◦ ~1 ≤ ~u>1 ◦ [ P+Q 2 ] ◦ ~1 = ~u>1 ◦ ~1 = 〈~u1, ~1〉, where ~1 is vector with all 1 entries. We get λ1 ≤ 1, since 〈~u1, ~1〉 > 0. For the case of equality, if P and Q have the same connected component C, then matrix [P,Q]√ has the same transition probabilities as Markov chains P and Q restricted to the vertices of C. We note that C is a stochastic matrix and, therefore, its largest positive eigenvalue is one. Hence,\nρ (\n[P,Q]√\n) ≥ ρ (C) = 1.\n3For example ~p and ~q could be respective stationary distributions of P and Q. However, we still want the assumption that ~p = ~q, as otherwise there might be another strategy to distinguish P and Q than observing a long stream of samples w by making the decision right away based on one initial sample from ~p. Example 1d illustrates how two Markov chains can produce very similar distributions of words W`\nP ,W` Q starting from any state for some\nlarge `, and yet have vastly different stationary distributions. 4or essential communicating class in the terminology of Book [LPW09]\nIf ρ (\n[P,Q]√ ) = 1, we apply Perron-Frobinius theorem to [P,Q]√ to get that the largest eigen-\nvalue λ1 = ρ ( [P,Q]√ ) = 1 has corresponding (left) eigenvector ~u1 with non-negative entries.\nSimilar to the proof of Claim 1 we observe that ~u>1 ◦ ( P+Q 2 − [P,Q]√ ) ◦ ~1 = 0, and all entries of the matrix in this expression are non-negative. This implies that Pij = Qij for every strictly positive coordinates i of the eigenvector ~u1 and any j ∈ [n]. Since ~u>1 ◦ [P,Q]√ = ~u>1 , we also have Pij = Qij = 0 for any positive coordinate i and zero coordinate j of eigenvector ~u1. Therefore, the set of vertices corresponding to positive coordinates of ~u1 form a component (which might have more than one connected component of P and Q) such that P = Q on these vertices.\nWe consider the quantity ε def = 1− ρ ( [P,Q]√ ) as a proxy for the closeness of Markov chains P and Q. In particular in (5) if ~p = ~q is proportional to ~u1, then ` · ln(1 − ε) ≤ ln 0.5 =⇒ ` ≥ ln 22ε . This shows that in worst-case we need to observe a trajectory of length at least Ω(1/ε) before we can satisfactorily distinguish the two chains. Note however that, in general, ` might need to be larger than O(1ε ) as is illustrated in Example 1c. In the remainder of this section and the following section we study an interesting special case of symmetric Markov chains that avoids such irregular behavior and dependency on the starting state.\nSymmetric Markov Chains. The stationary distribution for any symmetric Markov chain is the uniform distribution over all states. In this case the starting distributions in the average-case part of equation (5) are ~p = ~q = 1n\n~1. In this setting of symmetric Markov chains, we can provide sharp bounds on the minimal distinguishing length `.\nClaim 2. The necessary and sufficient distinguishing length `, which allows to distinguish P vs. Q with high probability, is Θ̃ (\n1 ε\n) (up to a log n factor), where ε = 1−ρ ( [P,Q]√ ) under both worst-case\nand average-case assumptions for the starting state.\nProof. We first consider the average-case model for the starting state. Note that [P,Q]√ is a symmetric matrix. Let ~v1, . . . , ~vn be normalized orthogonal eigenvectors of [P,Q]√, corresponding to real λ1 ≥ · · · ≥ λn eigenvalues. Then for RHS of (5) we have\n1 n ~1> ◦\n( [P,Q]√ )` ◦ ~1 = 1\nn ~1> ◦ ( n∑ i=1 λi · ~vi ◦ ~v>i )` ◦ ~1 = n∑ i=1 λ`i · 1 n 〈~1, ~vi〉2 = (∗) (7)\nNow we can write an upper and lower bound on (∗) in terms of λ`1 (assuming that ` is even):\nλ`1 n = λ`1 n ‖~v1‖22 ≤ λ ` 1 · 1 n ‖~v1‖21 ≤ (∗) ≤ n∑ i=1 λ`i · 1 n ‖~vi‖21 ≤ n∑ i=1 λ`i · ‖~vi‖ 2 2 = n∑ i=1 λ`i ≤ n · λ`1,\nwhere in the second inequality we used Perron-Frobenius theorem stating that all coordinates of ~v1 are non negative. Consequently, these bounds imply that ` = Θ ( 1 ε ) up to a log n factor, if\nρ (\n[P,Q]√ ) = λ1 = 1− ε. I.e., ` = Θ̃ ( 1 ε ) .\nFor the worst-case assumption on the starting state, it is sufficient to show an upper bound ` = O (\nlogn ε\n) . In this case (7) becomes\n~e>i ◦ ( [P,Q]√ )` ◦ ~1 = n∑ i=1 λ`i · 〈~ei, ~vi〉 · 〈~1, ~vi〉 ≤ n∑ i=1 |λi|` · ‖~vi‖∞ · ‖~vi‖1 ≤ n∑ i=1 |λi|` · √ n ≤ n1.5 · λ`1,\nsince ‖~vi‖1 ≤ √ n ‖~vi‖2 = √ n, and ‖~vi‖∞ ≤ ‖~vi‖2 = 1.\nWe note that, if one could pick the starting state instead of working with average-case or worstcase assumptions of Claim 2, then then ` can be much smaller (see Example 1b). Claim 2 gives a strong evidence that Dist (P,Q) def = 1 − ρ ( [P,Q]√ ) is a meaningful and important parameter that captures closeness between P and Q. In the following section we will use it as analytical proxy for the distance between Markov Chains5.\nFixed word length. In some applications the length ` of the observed word might be given a priori. One such example corresponds to card riffle shuffle, where the random choices in the process can be described (see Section 4 for more detail) as a Markov chain over O(n2) states (n = 52 for the card deck), where the process terminates after ` = n steps. In this case we can expect a few i.i.d. samples of length-` words. For such examples and more generally for the Markov Chains with a specified number of steps T it is natural to define Dist (P,Q) def = dTV ( WT P ,WT Q ) . Note that now the distance Dist (P,Q) satisfies triangle inequality. Moreover, due to the relation between Hellinger and total variation distances we can estimate 1− Dist 2(P,Q) 2 ≥ 1− d 2 Hel ( WT P ,WT Q ) , where the RHS term admits a nice analytical expression similar to (4).\n5In general this notion of distance should be used with care. One thing about parameter Dist (P,Q) = 1 − ρ (\n[P,Q]√\n) , is that it is not a metric. In particular, Dist (P,Q) violates the triangle inequality (Dist (M1,M2) =\nDist (M2,M3) = 0, but Dist (M1,M3) > 0 for some M1,M2,M3) as is illustrated by Example 1a. We note that this problem can only appear for reducible chains, as is shown in Claim 1. Also it is not always possible to extend the sharp bounds on ` of Claim 2 from symmetric Markov chains to non-symmetric Markov chains, even if both MC have the uniform distribution as their stationary distribution (see Example 1e)"
    }, {
      "heading" : "3 Identity Testing of Symmetric Markov Chains",
      "text" : "As we have formalized a notion of distance between symmetric Markov Chains in the previous section we get a well defined framework from property testing literature [BFF+01, Pan08, LRR13] for testing properties of distributions generated by Markov Chains. Arguably, the next fundamental\nquestion, after deciding one out of two given distributions (we call it A-B testing), is identity testing problem. In this problem the goal is to test from a stream of samples whether the real distribution (which unlike the case of A-B testing is completely unknown to us) coincides with a given hypothesis distribution. In this section, we present our results for identity testing of symmetric Markov chains. We first present a polynomial time algorithm that provides an efficient reduction to the identity testing problem with i.i.d. samples. The algorithm improves on the performance of a naive reduction, which waits for a period of time to get an independent sample and as such suffers a multiplicative loss of mixing time MixTQ of Markov Chain Q. Our algorithm suffers only an additive loss of Õ (HitTQ · log (HitTQ)) in sampling complexity and allows us to reduce the problem to testing identity with respect to squared Hellinger distance of a distribution supported on a domain of size n2 and with access to i.i.d. samples. In the next subsection we provide a nearly matching lower bound for the identity testing problem. We begin by giving a formal statement of the identity testing problem below:\nInput: ε; explicit symmetric Markov chain Q; m consecutive samples s1 · · · sm from a symmetric Markov Chain P . Output: P = Q, or P 6= Q if 1− ρ (\n[P,Q]√\n) > ε.\nOur approach. We consider a mapping K~k from infinite words W ∞ M of an irreducible Markov chain M to ∏n i=1[n]\nki , where ~k = (k1, · · · , kn) is a vector of n non negative integers, as follows. For each infinite word w = s1s2 · · · and each state i ∈ [n] we look at the first ki visits to state i (i.e., at times t = t1, . . . , tki with st = i) and write down the corresponding transitions in the infinite word w, i.e., st+1. We note that every state is visited almost surely in w, since M is an irreducible finite-state Markov chain. Therefore, mapping K~k defines a probability distribution on ∏n i=1[n]\nki . We note that this distribution is independent across all different states and/or independent for a particular state i because of the Markov property of Markov chains. Furthermore, a specific draw for a copy of a state i is distributed according to the i-th row of the transition matrix M .\nIn Lemma 3.1 we show that for a big enough number of samples m = Õ (HitTQ log (HitTQ) + nε ) 6 and ki = O(E[# visits to i]) = O( m n ) the mapping K~k is well defined for a finite m number of samples for all but a small fraction of the words in Wm M\n. This effectively allows us to generate a large number of independent samples from a discrete distribution corresponding to matrix P : pick uniformly at random a state i ∈ [n] and then observe transition from i according to transition probabilities of P . Indeed, to this end, we first simulate m′ = Θ ( m\nlog2(n/ε)\n) i.i.d. samples from\n[n]. Let ~k be the histogram of these m′ samples (note that maxi ki ≤ O(m′ log n/n) with high probability). We apply K~k mapping to our stream of m consecutive samples of Markov chain P , which is well defined with high probability. Apart from some small probability events (maxi ki is too large, or K~k is not defined) we obtain the desired m ′ i.i.d. samples.\nLemma 3.1. If m = Õ (log (HitTQ) HitTQ), then Pr[∃ state i : |{t : i = st ∈ w}| < m8e·n ] ≤ ε2 n .\nProof. To simplify notations we denote by ∆ def = 2HitTQ. By union bound over all states i it is enough to show that Pr[|{t : i = st ∈ w}| < m8e·n ] ≤ ε2 n2 for each fixed state i. We can make sure that in the first m2 steps state i is visited at least once with probability at least 1 − ε2 n3 . Once\n6in this paper, Õ always hides poly log(n/ε) factors.\nwe visited state i, instead of hitting time for state i we can analyze the return time Returni for i. Note that for symmetric Markov chains 1n\n~1 (uniform distribution) is a stationary distribution. Therefore, every state appears at average once in every n steps in an infinite word from W∞\nQ . In\nother terms, the expectation of Returni for each state i is exactly n. By definition of hitting time we have that in ∆2 steps the probability of reaching a particular state i from any other state j is greater than 1− 1/e (or any other given constant). It implies that Pr[Returni ≥ ∆2 · C] ≤ e\n−C for any C ∈ N. Indeed, one can show this by induction on parameter C. Notice that if the random walk did not return to i after C − 1 steps it has stopped at some state j 6= i. Then for any choice of j by definition of the hitting time the random walk will return to i with probability at least 1/e in the next ∆2 steps. It is not hard to get a similar bound Pr[Returni ≥ ∆ · C] ≤ e\n−C for any C ≥ 1, C ∈ R. To simplify notations we use X to denote the random variable Returni and X1, . . . , X` to denote ` i.i.d. samples of X. We have\nX ≥ 0 and ∀C ∈ R≥1,Pr [X ≥ ∆ · C] ≤ e−C and E [X] = n. (8)\nWe only need to show that Pr[X1 + · · ·+X` > m/2] ≤ ε 2 n2 for ` = m8e·n . To this end, we use a standard technique for large deviations and apply Markov’s inequality to the moment generating function of X1 + · · ·+X`,\nPr [X1 + · · ·+X` > m/2] = Pr [ eθ·(X1+···+X`) > eθ·m/2 ] ≤ E[e θ·(X1+···+X`)]\neθ·m/2 =\nE[eθX ]`\neθ·m/2 (9)\nWe note that given restrictions (8) on X maximum of E[eθX ] for any fixed θ > 0 is attained at\nX∗ ∼\n{ ∆ · x x ∈ [C0,∞) with probability density function e−x\n0 with remaining probability 1− e−C0 ,\nwhere constant C0 > 1 is such that E[X ∗] = n. Indeed, distribution X∗ maximizes (9) due to simple variational inequality: · eθ·a + · eθ·b < · eθ·(a−c) + · eθ·(b+c) for any b ≥ a ≥ c > 0, and probability mass > 0. This inequality allows us to increase E[eθ·X ] and not change E[X] by tweaking density function f(x) of X f ′(a − c) = f(a − c) + , f ′(a) = f(a) − , f ′(b) = f(b) − , f ′(b + c) = f ′(b + c) + , (f ′(x) = f(x) for all other x), for some c ≤ a. The only time we cannot apply this incremental change is when X = X∗.\nWe have E [X∗] = ∆(C0 + 1)e −C0 = n. (10)\nWe set θ def = 12∆ log ∆ in (9). Now we are ready to estimate E[e θ·X ]. To simplify notations we denote γ def = 12 log ∆ .\nE [ eθ·X ] = 1− e−C0 + ∫ ∞ C0 eθ·∆·x · e−x dx = 1− e−C0 + ∫ ∞ C0 e −x·(1− 1 2 log ∆ ) dx\n= 1− e−C0 + e −C0(1−γ)\n1− γ = 1 + e−C0\n( eC0γ 1− γ − 1 ) . (11)\nWe notice that γC0 < 1, since from (10) we can conclude that eC0 C0+1 = ∆n =⇒ C0 < 2 log ∆ = 1/γ. The last implication can be obtained as follows: for C0 > 2.52, we have C0− C02 ≤ C0− ln(1+C0) =\nln(∆n ). Now, we can estimate e γC0 ≤ 1 + e · γC0 in (11). Furthermore, since γ < 1/2 we have the term e C0γ\n1−γ − 1 in (11) to be at most 2eγ(C0 + 1). With this estimate we continue (11)\nE [ eθ·X ] ≤ 1 + e−C02eγ(C0 + 1) = 1 +\ne · n ∆ log ∆ . (12)\nWe apply estimate (12) and formula θ = 12∆ log ∆ to (9) to obtain\nPr [X1 + · · ·+X` > m/2] ≤\n( 1 + e·n∆ log ∆ )` em/4∆ log ∆ ≤ e m/8∆ log ∆ em/4∆ log ∆ = e −m 8∆ log ∆ < ε2 n2 ,\nwhere in the second inequality we used the fact (\n1 + e·n∆ log ∆\n)∆ log ∆ e·n\n< e, and to get the last inequality\nwe used m = Ω̃ (∆ log ∆) (where in Ω̃ the hidden dependency is only on log ε and log n).\n1 ~k ← Histogram (Θ (\nm log2(n/ε)\n) i.i.d. Uniform [n] samples);\n2 for t← 1 to m− 1 do 3 if |Samples[st]| < ~k[st] then Add (st → st+1) to Samples[st]; 4 end 5 if ∃i, s.t., |Samples[i]| < ~k[i] then 6 return Reject; 7 else 8 Samples← Samples[1] ∪ · · · ∪ Samples[n]; 9 return IdentityTestIID (ε, {qij = 1n ·Qij}i,j∈[n], Samples);\n10 end\nAlgorithm 1: Independent Edges Sampler.\nWe use as a black-box the following recently proposed identity test under the Hellinger distance7.\nLemma 3.2. Given a discrete distribution q supported on [n] and access to i.i.d. samples from a discrete distribution p on the same support, there is an algorithm which can distinguish whether\np = q or d Hel\n(p, q) ≥ ε with probability ≥ 2/3 using O (√\nn ε2\n) samples.\nAs a corollary of the lemma, we get a test that can distinguish whether P = Q, or d2 Hel ( 1 nP, 1 nQ ) ≥\nε using m = O ( n ε2 ) i.i.d samples from 1nP , which can be viewed as a distribution on a support of size n2. Lemma 3.3 shows that the required distance condition for the i.i.d. sampler is implied by our input guarantee. Lemma 3.3. 12 ∑\ni,j∈[n]\n(√ Pij n − √ Qij n )2 = d2 Hel ( 1 nP, 1 nQ ) ≥ ε.\nProof. We note that, as P and Q are symmetric matrices, so is [P,Q]√. Thus we have 1− ε = ρ (\n[P,Q]√ ) = max ‖~v‖2=1 ~v> ◦ [P,Q]√ ◦ ~v. (13)\n7this result uses a test similar to [ADK15] and is based on private communication with an author on that paper\nIf we use a particular ~v = 1√ n ~1 in (13), then we get the following inequality.\n1− ε ≥ 1√ n ~1> ◦ [P,Q]√ ◦ 1√ n ~1 = 1 n ∑ i,j √ Pij ·Qij = 1− d2Hel ( 1 n P, 1 n Q ) ,\nwhich implies d2 Hel ( 1 nP, 1 nQ ) ≥ ε.\nNext we get a bound on sampling complexity of Algorithm 1.\nTheorem 3.1. Algorithm 1 provides correct output with probability at least 2/3, with a single sample stream of length m = Õ ( HitTQ · log (HitTQ) + nε ) from P .\nProof. In the case P = Q, the probability that Algorithm 1 proceeds to IID tester, i.e., it does not reject P , because of small number of visits to a state, is at least Pr[∀i : |{t ∈ w : st = i}| > m8e·n ] · Pr[∀i : m 8e·n > ki] ≥ ( 1− ε2n ) · ( 1− ε2n ) ≥ 1 − 2ε2n . In the previous estimate, we used Lemma 3.1 to bound Pr[∀i : |{t ∈ w : st = i}| > m8e·n ], the fact that Pr[ m 8e·n ≤ ki] ≤ ε2 n2 (follows from a Chernoff bound), and a union bound. IID tester then correctly accepts P = Q with probability at least 4/5. Hence, the error probability is at most 1/5 + 2ε 2\nn < 1/3. For the case P 6= Q, Lemma 3.3 says that if 1 − ρ (\n[P,Q]√\n) > ε, then distributions passed\ndown to the IID tester {p : pij = 1nPij} and {q : qij = 1 nQij} are at least ε far in Hellingersquared distance. Classic results on identity testing with independent samples give sharp bounds of Θ( n\nε2 ) on sampling complexity with respect to total variation distance for distributions with\nsupport size n2. This estimate can be improved to work for Hellinger distance (Lemma 3.2). In our case this implies a O ( n ε ) sampling complexity for the IID tester. Furthermore, random mapping K~k : W ∞ P → p (where ~k is a histogram of m′ = Θ ( m log2(n/ε) ) i.i.d. uniform samples from [n]) produces m′ i.i.d. samples from p. Hence, if Algorithm 1 has sufficient samples from P to define the mapping K~k, it would be able to distinguish p and q with probability at least 2/3. On the other hand, if Algorithm 1 gets finite number of samples which are not sufficient to define the mapping K~k, then it correctly rejects P before even running the IID tester.\nThus in both cases the probability of error is at most 1/3."
    }, {
      "heading" : "3.1 Lower Bound.",
      "text" : "In this section we provide nearly matching lower bound to our result in Theorem 3.1.\nTheorem 3.2. There is an instance of Identity testing problem for symmetric Markov chain Q that requires a word of length at least Ω(nε ) to check identity of Q with 99% confidence.\nProof. We use Le Cam’s two point method and construct a class of Markov chains P s.t. (i) every P ∈ P is at least ε far from Q for a given constant ε. That is 1 − ρ ( [P,Q]√ ) ≥ ε for any P ∈ P; (ii) there is a constant c > 0, s.t. it is impossible to distinguish a word of length m generated by a randomly chosen Markov chain P̄ ∼ P, from a word of length m produced by Q with probability equal to or greater than 99100 for m ≤ cn ε . To prove (ii) we show that the total variation distance between the m-word distributions obtained from the two processes, Q and P̄ , is small when m < cnε for some constant c. We denote distribution of length m words obtained from Q by Wm\nQ , and from\nMC P̄ ∼ P by WmP . We represent symmetric MC as undirected weighted graphs G = (V,E). We allow graph to have multi-edges (this is helpful to provide an intuitive understanding of the lower bound construction and is not essential). We can ultimately remove all multi-edges and give a construction with only simple edges by doubling the number of states.\nMarkov Chain Q: complete double graph on n vertices with uniform weights, i.e.,\n∀ i 6= j (ij)1, (ij)2 ∈ E Q(ij)1 = Q(ij)2 = 1\n2(n− 1) .\nFamily P: for any pair of vertices i 6= j there are two bidirectional edges (ij)1, (ij)2 with weights randomly (and independently for each pair of (i, j)) chosen to be either\nP(ij)1 , P(ij)2 = 1± √ 8ε\n2(n− 1) , or P(ij)1 , P(ij)2 =\n1∓ √ 8ε 2(n− 1) .\nTo make this instance a simple graph with at most one bidirectional edge between any pair of vertices we apply a standard graph theoretic transformation: we make a copy i′ for each vertex i; for each pair of double edges e1 = (ij)1, e2 = (ij)2 construct 4 edges (ij), (ij\n′), (i′j), (i′j′) with weights w(ij) = w(i′j′) = w(e1) and w(ij\n′) = w(i′j) = w(e2). As all Markov chains Q and P ∈ P are symmetric with respect to the starting state, we can assume without loss of generality that word w starts from the state i = 1. First, we observe that for the simple graph 2n-state representation\nLemma 3.4. Every Markov chain P ∈ P is at least ε-far from Q.\nProof. For any P ∈ P, it can be seen that\n[P,Q]√ ◦ ~1 =\n(√ 1 + √ 8ε+ √ 1− √ 8ε\n2\n) · ~1.\nBy Perron-Frobenius theorem ~1 is the unique eigenvector corresponding to the largest absolute value eigenvalue. Hence, ρ (\n[P,Q]√\n) = √ 1+ √ 8ε+ √ 1− √\n8ε 2 which by Taylor series expansion implies 1− ρ (\n[P,Q]√ ) ≥ ε+ 52ε 2 + o(ε2) ≥ ε for any ε < 18 .\nWe say that a given word w = s1 . . . sm from a Markov chain P represented as a multi-edge graph on n states has a (ij) collision, if any state transition between states i and j (in any direction along any of the edges (ij)1, (ij)2) occurs more than once in w. We now state and prove the following claims about the Markov chain family P.\nLemma 3.5. Consider a word w of length m drawn from Q. The expected number of collisions in w is at most O ( m2\nn2\n) = O ( 1 ε2 ) .\nProof of Lemma 3.5: Let Iw(t1, t2, (ij)) indicate the event that in the multi-edge interpretation of the Markov chain P , the transition along (ij) edge occurs at times t1 < t2 in w. First, we observe that Pr[st1 = s|st1−1 = x] ≤ 1n−1 and Pr[st2 = s|st1−1 = x] ≤ 1 n−1 for all x and both s = i or s = j.\nThus for any t2 ≥ t1 + 2 by a union bound for all four possible cases of st1 , st1+1, st2 , st2+1 ∈ {i, j} we have\nE [Iw(t1, t2, (ij))] ≤ 4\n(n− 1)4 .\nSimilarly, for the case t2 = t1 + 1 we can obtain\nE [Iw(t1, t2, (ij))] ≤ 2\n(n− 1)3 .\nLet X denote the random variable which is equal to the total number of collisions in the word w. Then,\nE [X] = ∑\nt2≥t1+2 ∑ i 6=j E [Iw(t1, t2, (ij))] + m−1∑ t1=1 ∑ i 6=j E [Iw(t1, t1 + 1, (ij))]\n≤ 4 (n− 1)4 · m 2 2 · n(n− 1) 2 +\n2 (n− 1)3 ·m · n(n− 1) 2 = O\n( m2\nn2\n)\nWe also consider 3-way collisions which are collisions where there was at least 3 different transition between a pair of states i and j in the word w.\nLemma 3.6. Consider a word w of length m drawn from Q. The probability of w having a 3-way collision is at most O(m 3\nn4 ) = o(1).\nProof of Lemma 3.6: Similar to the proof of Lemma 3.5 we can give a sharp upper bound on the expected number of 3-way collisions with the most significant term being 8m 3\n6(n−1)6 · n(n−1) 2 , i.e., the expected number of 3-way collisions is O ( m3\nn4\n) . By Markov inequality we obtain the required\nbound on the probability of a 3-way collision.\nNow consider a typical word w generated by Q. As we know from Lemma 3.6 it has no 3- way collisions and by Markov inequality and Lemma 3.5 has at most O( 1\nε2 ) collisions with high\nprobability. As we show next a typical word w has similar probabilities under Q or P̄ ∼ P models.\nLemma 3.7. For m = O(nε ) at least 1 2 fraction of words w = s1 · · · sm generated by Q satisfy\n1 2 ·PrQ [w] < PrP̄∼P [w] < 2 ·PrQ [w]\nProof of Lemma 3.7: For each feasible word w in Q, i.e., w such that PrQ[w] > 0\nPr Q [w] =\n( 1\n2(n− 1) )m−1 Pr P̄∼P [w] = ∏ j>i ∑ P̄(ij)1= 1± √ 8ε 2(n−1) P̄ |{(ij)1∈w}| (ij)1 · P̄ |{(ij)2∈w}|(ij)2\nFirst, if w has only one transition along edge (ij), then the corresponding term in PrP̄∼P [w]∑ P̄(ij)1 P̄ |{(ij)1∈w}| (ij)1 · P̄ |{(ij)2∈w}|(ij)2 = 1 2 ( 1 + √ 8ε 2(n− 1) + 1− √ 8ε 2(n− 1) ) = 1 2(n− 1) .\nFrom Lemma 3.6, we know that probability of a 3-way collision in w is o(1) under Q model. We observe that for a 2-way collision (ij) (a collision which is not a 3-way collision), the corresponding term in PrP̄∼P [w] for the case of transition along two different edges (ij)1 and (ij)2 is∑\nP̄(ij)1\nP̄ |{(ij)1∈w}| (ij)1 · P̄ |{(ij)2∈w}|(ij)2 = 1 + √ 8ε 2(n− 1) · 1− √ 8ε 2(n− 1) = (1− 8ε) 4(n− 1)2 .\nWe call this type of collision type I collision. For the other case (type II collisions) of transition along the same edges the respective probability is (1+8ε) 4(n−1)2 . By Lemma 3.5 and by Markov inequality the total number of collisions is O( 1 ε2\n) with probability 34 . We can also make sure that out of these collisions number of type I and type II collisions is roughly the same. More precisely, the difference between numbers of type I and type II collisions is at most O(1ε ) with probability of at least 3 4 . Indeed, the choice of edge collision type in w is uniform between type I and type II, and is independent across all collision edges. Now, for small enough m we can make sure that at least 1 2 fraction of words w has number of collisions at most c1 ε2\nand the difference between number of type I and II collisions is at most c2ε , for some small constants c1, c2 > 0. Thus the corresponding density functions can be related as follows.\n2 > (1 + 8ε) c2 ε > PrP̄∼P [w] PrQ[w] > ( 1− 64ε2 ) c1 2ε2 · (1− 8ε) c2 ε > 1/2\nLemma 3.7 shows that dTV\n( Wm\nQ ,WmP ) ≤ 34 , which implies that no algorithm can successfully\ndistinguish Q from the family P with probability greater than 34 for some m = Ω( n ε )."
    }, {
      "heading" : "4 Card Shuffling",
      "text" : "A commonly used technique to shuffle decks of n = 52 cards is the riffle shuffle: first, the dealer cuts the deck into two piles. Then, the piles are “riffled” together: the shuffler successively drops cards from the bottom of each pile to form a new pile. There are two variable aspects in this procedure. First, the numbers of cards in each pile after the initial cut can vary. Second, each time the dealer drops a card she needs to choose the pile from which the card is dropped.\nThe most well studied mathematical model for riffle shuffle is due to Gilbert, Shannon, and Reeds (GSR-model for short): first, the deck is cut into two packs according to a (n, 0.5)-binomial random variable where n is the number of cards in the deck; next, cards are dropped one by one from the bottom of one or the other pile with probability proportional to the relative sizes of the piles (i.e., if the left pile contains a cards and the right pile b cards, the next card drops from the left pile with probability aa+b). A well known result in this model is due to Bayer and Diaconis [BD92] who gave a sharp mathematical analysis of the mixing time of the riffle shuffle Markov chain showing that “seven shuffles are necessary and sufficient to approximately randomize 52 cards,” which actually convinced Las Vegas casinos to increase the number of shuffles in their shuffling procedures.\nThere have been some statistical studies validating the accuracy of the GSR model for riffle shuffles in practice. For example Diaconis and Reeds (see [Dia88]) did empirical analysis of about\na hundred riffle shuffles performed by each of them. They looked at a few different statistics including the count of consecutive cards dropped from each of the piles. In other work (see [Dia02] open problem 5), Chakraborty and Diaconis pointed out that some shuffling machines performing riffle shuffles do not conform to the GSR model. Instead they proposed another model to capture this observed phenomenon (we call it CD-model), where, in contrast to the GSR model, cards are dropped with probability aa+b from the pile containing b cards and b a+b from the pile containing a cards. Despite a large interest in card shuffling and all the existing work mentioned above, there has not been much theoretical statistical analysis on the question of testing whether a particular shuffling model is accurate. In this section we propose a new theoretical framework for the statistical analysis of riffle shuffles. We aim to address the following question:\nHow many trials are needed to test that shuffles are performed according to a specified probabilistic model?\nIt is natural to parametrize a riffle shuffle by (i) the distribution of possible cuts, and (ii) the probability pab of choosing the next card to be dropped from the left pile for each profile (a, b) of the number of cards left in the two piles. In this description each riffle shuffle can be represented as a random walk on a 2-dimensional grid (a, b) ∈ Z2 that starts at a position chosen from a specified distribution on the diagonal a + b = n and decreases the sum of the coordinates a + b by one at each move. In fact, we get an almost one-to-one correspondence between riffle shuffles and the aforementioned random walks on the n × n grid (the only shuffle that corresponds to more than one path on the grid is the identical permutation). Therefore, by knowing the initial permutation of cards at the beginning of the shuffle, and by scanning the permutation of cards obtained after one riffle shuffle we can reconstruct (except for the unimportant case of void shuffle) the random walk of n steps taken by our grid Markov Chain. We note that this grid representation might not be enough to accurately model the behavior of a dealer. For example, shufflers often tend to drop chunks of consecutive cards from left or right pile regardless of the pile sizes. To address this issue one might want to introduce an extra parameter – which pile the last card was taken from – to our grid parameterization, which can be done by doubling the number of states in the Markov chain. To capture this and other potential extensions of the Markov chain model for a riffle shuffle we introduce the following general theoretical framework.\nDefinition 2 (Sparse MC). Consider a Markov chain defined by transition matrices P = {Pt}n1t=1 over n2 states that proceeds in n1-step rounds, as follows. Starting from some state X0 = s0 it follows a transition according to P1, then P2, etc, then Pn1 to arrive at some state X1. Starting from X1 it repeats transitions according to P1, . . . , Pn1 , in sequence, and so on, ad infinitum. The transition matrices are assumed sparse having O(n3) non-zero entries.\nTo relate the above definition to the riffle shuffle, we should think of the state space as the set {(a, b)|0 ≤ a + b ≤ n}, where n = 52. So in particular, there are n2 = O(n2) states. There are n1 = n+ 1 transition matrices. P1 takes us from an uncut deck of cards, corresponding to the state (0, 0), to a cut deck corresponding to a state in set {(a, b)|a + b = n}. Then each other transition matrix Pt, t > 1, maps a state in {(a, b) : a+ b = n− t+ 2} to states in {(a, b) : a+ b = n− t+ 1}. All transition matrices have O(n) non-zero entries. Note that this way of modeling the riffle shuffle, while forgetting the specific ordering of cards, maintains the essential information that we need to test a riffle shuffle model, and in particular saves exponentially in the size of the state space.\nTesting sparse Markov chains. We develop tools for goodness-of-fit testing of sparse Markov chain models.\nSimplification: To avoid carrying around several parameters, we will henceforth take n1 = n3 = n and n2 = O(n\n2), which is what we would need for the riffle shuffle. Our results, namely the use and analysis of our edge tester, extend to the general case. We will also assume that Pn1 is the trivial matrix taking all states into a fixed state s0, which is also what we would need for the riffle shuffle, namely s0 = (0, 0). Again, our results easily extend to the general case.\nWith these simplifications in place, we can break an observed trajectory from a sparse Markov chain model into “samples.” One sample is a word w = s0 · · · sn, whose transitions st−1 → st are performed according to transition matrix Pt.\nAs described in Section 2.2, a natural measure of distance between two Markov chains Q = {Qt}nt=1 and P = {Pt}nt=1 is the total variation distance between words of certain length sampled from these chains. As we have a natural length n to use here, we can take our distance between chains to be Dist (P,Q) def = dTV ( Wn P ,Wn Q ) .\nWe note that Dist 2(P,Q) 2 =\nd2 TV ( Wn P ,Wn Q ) 2 ≤ d 2 Hel ( Wn P ,Wn Q ) and for Hellinger-squared distance we\ncan derive a formula similar to (4): 1−d2 Hel\n( Wn\nP ,Wn Q ) = ~e>1 ◦[P1, Q1]√◦· · ·◦[Pt, Qt]√◦· · · [Pn, Qn]√◦~1.\nIn particular, we could alternatively define our distance using the spectral approach we took in Section 3. To this end we can define a large matrix Q∗ for Markov chain Q that acts on n(n+ 1) states (n + 1 distinct copies nt of n states for each t ∈ {0, · · · , n}), such that Q∗ behaves exactly like matrix Qt on states nt−1 transitioning them to sates nt, and for t = n states nt are transitioned to the initial state s1 with t = 0. We similarly define large matrix P ∗ for the Markov chain P .\nThen it turns out that the spectral radius ρ (\n[Q∗, P ∗]√\n)n+1 = 1− d2\nHel\n( Wn\nP ,Wn Q\n) .\nWith these definitions we are interested in the following testing problem:\nInput: Q = {Qt}nt=1, s.t. each Qt is sparse, i.e., it has only O(n) non zero entries in total; m samples of w = s0 · · · sn from a sparse Markov chain P = {Pt}nt=1. Output: P = Q, or P 6= Q if Dist (P,Q) ≥ ε."
    }, {
      "heading" : "4.1 Upper Bound",
      "text" : "For the upper bound, one might consider an appropriately defined statistic on the number of visits to a particular state si,t to distinguish between the two cases. Such statistics however can be mathematically difficult to deal with and we obtain worse bounds on the moments. In this paper, instead, we consider a different statistic. We look at all one-step transitions that can have positive probability in the Markov chain Q or P , we call these one-step transitions as edges. We denote a generic edge from a state si,t−1 to a state sj,t by e; the set of all possible edges by E; the set of transitions in E between states at time t−1 and time t by E(t) for each t ∈ [n]. For each edge e, let qe and pe be the probabilities that there was a transition along edge e in one sample s1 · · · sn from Q, or from P , respectively. Our statistic is defined on these edges. From a high level perspective, it consists of two steps:\nPruning. We remove all rare edges, i.e., edges that are traversed with probability less than O( ε 2\nn2 )\nin Q. We show that the Markov chain obtained post pruning and renormalization is still close to\nthe original chain (Lemma 4.5). This step is necessary as rare instances of such transitions along rare edges could potentially shift the value of the statistic by a lot and we want to avoid that. Let E∗ be the resulting pruned set of edges. We reject all samples from P that go along any removed edge e /∈ E∗. We return P 6= Q if there are too many rejected samples. Otherwise, we continue to the next step. In Lemma 4.6 we show that returning P 6= Q in this step doesn’t affect the success probability by too much.\nχ2-statistic on edges. For each non rare edge e ∈ E∗ we count the number of transitions ne along e. We define χ2 edge statistic Ze def = (ne−qe·m) 2−ne qe·m . Our main statistic is\nZ def = ∑ e∈E∗ Ze = ∑ e∈E∗ (ne − qe ·m)2 − ne qe ·m .\nWe accept or reject P = Q depending on Z being smaller or larger than a certain threshold. This test is similar in spirit to that of [ADK15] but requires much more involved analysis. Indeed, in our stetting it is not clear which statistic to use: one can attempt to count frequencies of state visits in the MC, or employ other state dependent statistics. After many trials and errors we figured out that doing the analysis across separate edges was the best approach. Indeed, since we are dealing with non i.i.d. but dependent samples, obtaining a non-trivial variance bound for our edge statistic proves to be a challenging task. Similar to the classical i.i.d. setting, poissonisation helps to ease the analysis in our setting too. However, we rely on it in more subtle way: the effects of poissonisation at the top layer of n states percolate nicely through to the bottom layers of the chain as shown in Lemma 4.1. We show that Var[Z] = O ( kn3 ) in Lemmas 4.3 and 4.4. Another challenge for us was to relate the new definition of distance between two non stationary Markov chains with the parameters in the description of their kernels, as e.g. in our Lemma 4.2. Our χ2 test yields the following guarantee on the number of samples\nTheorem 4.1. There is an algorithm that can tell whether P = Q, or P 6= Q, when Dist (P,Q) ≥ ε, with probability at least 23 using O( n3/2 ε2 ) samples.\nNote that, while we state the above theorem in terms of the number of samples, we really mean that we observe a single trajectory from the sparse Markov chain which we have partitioned into segments of length n. In particular, the length of the required trajectory for the above statements is a factor of n larger than the stated number of samples. Additionally note that to properly compare to our results from Section 3 we should note that the number of states here is O(n2). All details are provided in Section 4.2. We also have a complementary lower bound which is presented in Section 4.3."
    }, {
      "heading" : "4.2 More Details on the Tester for Sparse Markov Chains",
      "text" : "We consider all one-step transitions that can have positive probability in the Markov chain Q or P , we call these one-step transitions as edges. We denote a generic edge from a state si,t−1 to a state sj,t by e; the set of all possible edges by E; the set of transitions in E between states at time t − 1 and time t by E(t) for each t ∈ [n]. For each edge e, let qe and pe be the probabilities that there was a transition along edge e in one sample s1 · · · sn from Q, or from P , respectively. Our test, from a high level perspective, consists of two steps:\nPruning. We remove all rare edges, i.e., edges that are traversed with probability less than O( ε 2\nn2 )\nin Q. Let E∗ be the resulting pruned set of edges. We reject all samples from P that go along any removed edge e /∈ E∗. We return P 6= Q if there are too many rejected samples. Otherwise, we continue to the next step.\nχ2-statistic on edges. For each non rare edge e ∈ E∗ we count the number of transitions ne along e. We define χ2 edge statistic Ze def = (ne−qe·m) 2−ne qe·m . Our main statistic is\nZ def = ∑ e∈E∗ Ze = ∑ e∈E∗ (ne − qe ·m)2 − ne qe ·m .\nWe accept or reject P = Q depending on Z being smaller or larger than a certain threshold.\nIn the remainder of this section we mostly focus on the latter step. Specifically, we analyze the χ2 edge statistic in the case when qe ≥ Ω( ε 2\nn2 ) for all e ∈ E and pe = 0 for all e /∈ E. At the end of the\nsection we explain why after the pruning step these conditions are satisfied.\nPoisson Sampling. Throughout the analysis of χ2 statistic, we use the standard Poissonization approach. Instead of drawing exactly m samples from P , we first draw m′ ∼ Poisson (m), and then draw m′ samples from P . The benefit of this is that the number of times different elements in the support of the cut distribution occur in the sample become independent, giving simpler analysis. Moreover, the number of transitions observed along the edge e ∈ E(t), ne, for a fixed t will be distributed as Poisson (m · pe), independently for all e ∈ E(t) (see Lemma 4.1). As Poisson (m) is tightly concentrated around m, this additional flexibility comes only at a sub-constant cost in the sample complexity with an inversely exponential in m, additive increase in the error probability. We note that as in equations (1− 2) from [ADK15] the expectation and variance of our χ2 statistic are as follows.\nE [Ze] = m · (pe − qe)2\nqe and Var [Ze] = 2 p2e q2e +m · pe(pe − qe) 2 q2e .\nLemma 4.1. The number of transitions ne along an edge e is distributed as Poisson (m · pe) and all ne are independent for e ∈ E(t), for any t ∈ [n].\nProof. The proof proceeds by induction on t and is based on the following two standard observations on Poisson random variables:\n(Observation I) for any discrete distribution D, k ∼ Poisson (λ) i.i.d. samples from D form a collection of jointly independent Poisson random variables for the occurrences of each element in the support of D, i.e., distribution ∏ i Poisson (λ ·Prx∼D[x = i]);\n(Observation II) the sum of two independent Poisson random variables with distributions Poisson (λ1) and Poisson (λ2) is a Poisson random variable with the distribution Poisson (λ1 + λ2).\nFor t = 1 as we start with Poisson (m) samples, Observation I gives us the desired result. For the induction step (from t = k to t = k + 1), we observe that counts of visits to each particular state i at time t are independent Poisson random variables by Observation II. Now, Observation I applied to the states at time t yields the desired result.\nThe following procedure correctly distinguishes between the case P = Q, or Dist (P,Q) ≥ ε, in the regime when qe ≥ Ω( ε 2\nn2 ) for all e ∈ E(Q) and pe = 0 for all e /∈ E.\nInput: ε; an explicit k-sparse Markov Chain Q = {Qt}nt=1; (Poisson) m samples from a Markov Chain P = {Pt}nt=1, where ne denotes the number of transitions along the edge e.\nOutput: Accept if P = Q, or reject if Dist (P,Q) ≥ ε 1 E ← {e : qe > 0}; 2 Z ←\n∑ e∈E (ne−qe·m)2−ne qe·m ;\n3 if Z ≤ 2 √ kn3/2 then 4 return Accept; 5 else 6 return Reject; 7 end\nAlgorithm 2: χ2 Edge Test\nTheorem 4.2. Algorithm 2 is correct with probability at least 4/5, if m ≥ Cn3/2 ε2 for some C = O(1), qe ≥ ε 2\nkn2 for all e ∈ E, and pe = 0 for all e /∈ E.\nProof. To get the desired result we analyze expectation and variance of Z. First, we relate the expected value of Z with the distance Dist (P,Q) between P and Q.\nLemma 4.2. E[Z] ≥ m4 ·Dist 2 (P,Q) .\nProof. We recall that\nd2 Hel (P,Q) = 1− ~e>1 ◦ [P1, Q1]√ ◦ · · · ◦ [Pt, Qt]√ ◦ · · · [Pn, Qn]√ ◦ ~1 = ~e>1 ◦ ( P1 +Q1\n2\n) ◦ · · · ◦ ( Pn +Qn\n2\n) ◦ ~1− ~e>1 ◦ [P1, Q1]√ ◦ · · · [Pn, Qn]√ ◦ ~1\n= n∑ t=1 ~e>1 ◦ [P1, Q1]√ ◦ · · · ◦ ( Pt +Qt 2 − [Pt, Qt]√ ) ◦ ( Pt+1 +Qt+1 2 ) · · · ( Pn +Qn 2 ) ◦ ~1\n= 1\n2 n∑ t=1 ~e>1 ◦ [P1, Q1]√ ◦ · · · ◦ [Pt−1, Qt−1]√ ◦ [Pt, Qt](√-√)2 ◦ ~1, (14)\nwhere in the last line [Pt, Qt](√-√)2 =\n((√ Pt(ij)− √ Qt(ij) )2) ij . Indeed, the first equality holds\ntrue as Pt ◦~1 = Qt ◦~1 = ~1 for any t ∈ [n]; the second equality is a telescopic sum; the last equality is simply the formula for the complete square. Let qt and pt be the respective distribution vectors over the states si,t, i ∈ [n] in Q and P Markov chains. We also define distributions p0 = q0 = ~e1.\nBy applying Cauchy-Schwarz inequality to the corresponding Bhattacharya coefficient of P and\nQ at a fixed state i and time t we obtain ~e>1 ◦ [P1, Q1]√ ◦ · · · ◦ [Pt, Qt]√ ◦ ~ei = HSi ( Wt P ,Wt Q ) = ∑ w=s0...st s.t. st=i √ Pr P [w] Pr Q [w]\n≤ √√√√ ∑\nw=s1...st s.t. st=i\nPr P\n[w] ∑\nw=s1...st s.t. st=i\nPr Q\n[w] = √ pt(i) · qt(i).\nWe plug in this estimate to (14) and obtain\n1 4 ·Dist2 (P,Q) = 1 4 d2 TV (P,Q) ≤ d2 Hel (P,Q) ≤ 1 2 n∑ t=1 [pt−1, qt−1] > √ ◦ [Pt, Qt](√-√)2 ◦ ~1,\nWe examine each term of the summation in the right hand side of the last equation\n1 2 [pt−1, qt−1] > √ ◦ [Pt, Qt](√-√)2 ◦ ~1 = ∑ i\n√ pt−1(i) · qt−1(i)\n2 · ∑ j:(ij)∈E(t) (√ Pt(ij)− √ Qt(ij) )2 , (15)\nWe show that corresponding terms in E[Z] = ∑ i,t ∑ e:(ij)∈E(t) E[Ze] give an upper bound on (15) for each fixed state i.\n1\nm ∑ e:(ij)∈E(t) E [Ze] = ∑ e:(ij)∈E(t) (pe − qe)2 qe = ∑ j:(ij)∈E(t) (pt−1(i)Pt(ij)− qt−1(i)Qt(ij))2 qt−1(i)Qt(ij)\n= p2t−1(i)\nqt−1(i)  ∑ j:(ij)∈E(t) P 2t (ij) Qt(ij) − 2pt−1(i) + qt−1(i) = p2t−1(i)\nqt−1(i) ∑ j:(ij)∈E(t) (Pt(ij)−Qt(ij))2 Qt(ij) + (pt−1(i)− qt−1(i))2 qt−1(i)\n≥ p2t−1(i)\nqt−1(i) ∑ j:(ij)∈E(t) (√ Pt(ij)− √ Qt(ij) )2 + (pt−1(i)− qt−1(i))2 qt−1(i) , (16)\nwhere the third and forth equalities hold true as ∑ j:(ij)∈E(t) Pt(ij) = ∑ j:(ij)∈E(t)Qt(ij) = 1, and to get the last inequality one can simply use identity (a−b) 2 b = ( √ a − √ b)2 (√ a+ √ b√\nb\n)2 . Now, we\nclaim that the expression in RHS of (16) is at least RHS of (15) for a given i, i.e., we need to show that(\np2t−1(i) qt−1(i) − √ pt−1(i)qt−1(i) 2 ) ∑ j:(ij)∈E(t) (√ Pt(ij)− √ Qt(ij) )2 + (pt−1(i)− qt−1(i))2 qt−1(i) ≥ 0. (17)\nThe inequality is obviously true, if p2t−1(i) qt−1(i) ≥ √ pt−1(i)qt−1(i)\n2 . Otherwise, without loss of generality, we can substitute the term ∑\nj:(ij)∈E(t)\n(√ Pt(ij)− √ Qt(ij) )2\nwith an upper bound of 2. Furthermore, by denoting x = √\npt−1(i) qt−1(i) the inequality (17) can be rewritten as qt−1(i) · ( 2x4 − x+ (x2 − 1)2 ) ≥ 0, and, indeed, one can verify that this fourth degree polynomial is always positive.\nNow we estimate the variance of random variable Z Lemma 4.3. E[Z] ≥ ( 12 √\n2k C +\n2 √ 2+ √ k√\nC\n) · √\nVar[Z], when m ≥ C·n3/2 ε2 and Dist (P,Q) ≥ ε.\nProof. We recall that Z = ∑\ne∈E Ze and by Lemma 4.1 all Ze for a fixed t and e ∈ E(t) are independent. For any random variables X1, . . . , Xn it is true that\nVar [X1 + · · ·+Xn] ≤ (√ Var [X1] + · · ·+ √ Var [Xn] )2 .\nWe use this estimate for Xt = ∑ e∈E(t) Ze to obtain\n√ Var [Z] ≤ n∑ t=1 √ ∑ e∈E(t) Var [Ze] = n∑ t=1 √√√√ ∑ e∈E(t) 2 p2e q2e +m · pe(pe − qe) 2 q2e\nWe further simplify the above expression by using the fact that √ x+ y ≤ √ x+ √ y.\n√ Var [Z] ≤ n∑ t=1 √√√√ ∑ e∈E(t) 2 p2e q2e + √ m n∑ t=1 √√√√ ∑ e∈E(t) pe(pe − qe)2 q2e\n(18)\nOn the other hand, by Lemma 4.2\nE [Z] ≥ 1 2 E [Z] + m 8 Dist2 (P,Q) = m ·\n( 1\n2 ∑ e∈E (pe − qe)2 qe + ε2 8\n) . (19)\nIn the following we use (19) to give separate upper bounds on each of the two summation terms in the RHS of (18).\nFirst term of (18). To estimate the first term, we split E into two sets\nE1 def = {e ∈ E : pe ≤ 2qe} and E2 def = {e ∈ E : pe > 2qe}\nWe define accordingly the sets E1(t) and E2(t) for each t ∈ [n]. Again we have\nn∑ t=1 √√√√ ∑ e∈E(t) 2 p2e q2e ≤ n∑ t=1 √√√√ ∑ e∈E1(t) 2 p2e q2e + n∑ t=1 √√√√ ∑ e∈E2(t) 2 p2e q2e .\nEstimate for E1. We have 2 p2e q2e ≤ 8 for any edge e ∈ E1. Therefore, by the sparsity condition∑\ne∈E1(t) 2 p2e q2e ≤ 8n · k, where k = O(1). Thus,\nn∑ t=1 √√√√ ∑ e∈E1(t) 2 p2e q2e ≤ √ 8k · n3/2 ≤ √ 8k C ·m · ε2 ≤ 4 √ 8k C ·E [Z] (20)\nEstimate for E2. We have (pe−qe)2 qe ≥ p 2 e 4qe for any e ∈ E2. Therefore,\n√ 32k\nC ·E [Z] ≥\n√ 32km C · n∑ t=1  1 16 ∑ e∈E2(t) 2 p2e qe + ε2 8n  ≥ √32km C √ 4ε2 16 · 8n n∑ t=1 √√√√ ∑ e∈E2(t) 2 p2e qe\n≥ √ 32km\nC\n√ ε2 32n · √ ε2 kn2 n∑ t=1 √√√√ ∑ e∈E2(t) 2 p2e q2e ≥ n∑ t=1 √√√√ ∑ e∈E2(t) 2 p2e q2e , (21)\nwhere the first inequality holds by (19); the second inequality is simply AM-GM inequality; and to get the third inequality we used the bound qe ≥ ε 2\nkn2 .\nSecond term of (18). We split E into another two sets E3, E4 (similarly define E3(t) and E4(t)):\nE3 def = {e ∈ E : pe ≤ 2 √ nqe} and E4 def = {e ∈ E : pe > 2 √ nqe}\nAgain n∑ t=1 √√√√ ∑ e∈E(t) pe(pe − qe)2 q2e ≤ n∑ t=1 √√√√ ∑ e∈E3(t) pe(pe − qe)2 q2e + n∑ t=1 √√√√ ∑ e∈E4(t) pe(pe − qe)2 q2e\nEstimate for E3. We have\n2\n√ 2\nC ·E [Z] ≥\n√ 2\nC m · n∑ t=1  ∑ e∈E3(t) (pe − qe)2 qe + ε2 4n  ≥√ 2 C m √ ε2 n n∑ t=1 √√√√ ∑ e∈E3(t) (pe − qe)2 qe\n≥ m √ 2ε2 Cn · √ 1 2n1/2 n∑ t=1 √√√√ ∑ e∈E3(t) pe(pe − qe)2 q2e = √ m n∑ t=1 √√√√ ∑ e∈E3(t) pe(pe − qe)2 q2e , (22)\nwhere the first inequality holds by (19), the second inequality is AM-GM inequality, to get the third inequality we use definition of E3 that pe qe ≤ 2 √ n.\nEstimate for E4. We have pe − qe ≥ (\n1− 1 2 √ n\n) pe for any e ∈ E4. Therefore,\n√ k √ 2C (\n1− 1 2 √ n\n) ·E [Z] ≥ m√k√ 2C ∑ e∈E4 pe(pe − qe) qe ≥ √ m\n√ km · 2ε2\n2C · kn3/2 ∑ e∈E4 p 1/2 e (pe − qe) qe\n≥ √ m n∑ t=1 √√√√ ∑ e∈E4(t) p 1/2 e (pe − qe) qe , (23)\nwhere the first inequality follows from(19), to get the second inequality we estimate √ pe ≥√ 2n1/2qe ≥ √ 2ε2\nkn3/2 , to get the last inequality we simply use that\n√ x+ √ y ≥ √ x+ y.\nFinally, combining estimates (20),(21),(22),(23) we obtain the desired bound on the variance.\nLemma 4.4. If P = Q, then Var[Z] ≤ √ kn3/2.\nProof. Using similar estimate as in the proof of Lemma 4.3 we get\n√ Var [Z] ≤ n∑ t=1 √ ∑ e∈E(t) Var [Ze] = n∑ t=1 √√√√ ∑ e∈E(t) 2 q2e q2e ≤ n · √ kn.\nTo conclude the proof of Theorem 4.2 it remains to notice that in case P = Q the error probability (Algorithm 2 returns “reject”) is at most\nPr [ Z > 2 √ kn3/2 ] ≤ Pr [ Z > 2 √ Var [Z] ] ≤ 1\n5 ,\nwhere the first inequality follows from Lemma 4.4, the last is Cantelli’s inequality, since E[Z] = 0. On the other hand, when Dist (P,Q) ≥ ε, then E[Z] ≥ m · ε24 . If Algorithm 2 makes an error by returning “accept”, then Z ≤ 2 √ kn3/2 ≤ 2 √ k C E[Z]. Thus Z −E[Z] ≤ ( 2 √ k\nC − 1) E[Z]. Now if C is such that 1− 2 √ k C ≥ 2 · ( 12 √ 2k C + 2 √ 2+ √ k√\nC\n) (any constant C ≥ max (√ k(42 √ 2 + 4), (8 √ 2 + 4 √ k)2 )\nwould work), then Z −E[Z] ≤ −2 √ Var[Z] by Lemma 4.3. Furthermore, by Cantelli’s inequality\nPr [ Z ≤ 2 √ kn3/2 ] ≤ Pr [ Z −E [Z] ≤ −2 √ Var [Z] ] ≤ 1\n5 ,\ni.e., probability of Algorithm 2’s error in the case P 6= Q is at most 15 .\nAnalysis of pruning step. Here we slightly modify Markov Chains Q and P so that probability of traversing any edge in Q is Ω( ε 2\nn2 ) and that P only uses these edges. To this end, we sequentially\nremove edges from Q that have too low probability of traversal in Q. Effectively, in the pruning process we are sampling from Q, but rejecting all the samples that use a “rare” edge. We choose the threshold for the “rare” edges in such a way that we would reject at most ε2 fraction of samples from Q. Recall that the Markov chain obtained by pruning Q is denoted as Q∗. After pruning of Q, we do the corresponding empirical rejection sampling for P (see Algorithm 3) which is equivalent to obtaining samples from the modified Markov chain P ∗. If there are too many rejected samples, we conclude that Dist (P, P ∗) > Dist (Q,Q∗) with high probability, and thus P 6= Q. Otherwise we proceed to Algorithm 2 for the modified Q and pruned samples from P .\nLet Q∗ be a modified Markov Chain Q that sample w ∼ Q and reject any w with e /∈ E∗.\nLemma 4.5. Dist (Q,Q∗) ≤ 2ε2\nProof. By definition of the set of pruned edges E∗, by removing each new edge we lose at most ε 2\nkn2\nfraction of samples. As there are only at most kn2 edges in Q, the probability of avoiding removed edges is at least (1− ε2\nkn2 )kn 2 ≥ 1− ε2. Thus at most ε2 fraction of words Wn Q are rejected in Wn Q∗ , which implies that 2ε2 ≤ dTV ( Wn Q ,Wn Q∗ ) = Dist (Q,Q∗).\nWe need to argue about probability of error in Algorithm 3. First, in the case P = Q Algorithm 3 could incorrectly return “reject”. Our next Lemma 4.6 provides the necessary bound. On the other hand, when P 6= Q and Algorithm 3 does not return “reject”, we want Algorithm 2 to be able to distinguish P ∗ and Q∗. To this end, Lemma 4.6 shows that Dist (P ∗, Q∗) = Ω(ε) with high probability.\nInput: ε; an explicit k-sparse Markov Chain Q = {Qt}nt=1; m samples from a Markov Chain P = {Pt}nt=1 Output: m · (1− 2ε2) i.i.d. samples from pruned P ∗ and pruned Q∗ s.t., each qe = Ω( ε 2\nn2 );\nreject P if there are not enough samples.\n1 Let E∗ = {e : qe > 0}; 2 while ∃ e ∈ E∗ : qe < ε 2\nk·n2 do 3 E∗ ← E∗ \\ e; /* e = (ij), e ∈ E(t) */ 4 Delete (e,Q); /* set Qt(ij) = 0, re-normalize row i in Qt */ 5 foreach edge e ∈ E∗ do Recompute(qe); 6 end 7 for w = s1 · · · sn ∼ P do 8 if ∀ t (stst+1) ∈ E∗ then Add w to PrunedSamples; 9 else increase RejectCount;\n10 end 11 if RejectCount > 2mε2 then 12 return Reject; 13 else 14 return PrunedSamples; 15 end\nAlgorithm 3: Pruning Test. In the pruned Markov Chain Q∗ all edges with qe > 0 satisfy qe ≥ ε 2 kn2 ; samples from Markov Chain P ∗ can only go along these edges.\nLemma 4.6. (1) If P = Q, then Algorithm 3’s error of rejecting P is at most 0.1. (2) If Dist (P,Q) ≥ ε, then either Dist (P ∗, Q∗) = Ω(ε), or Algorithm 3 correctly rejects P with probability at least 0.9.\nProof. Let X` be Bernoulli random variables for ` ∈ [m] denoting (X` = 1) whether `-th sample from P was rejected, or accepted to P ∗ (X` = 0). Then the number of rejected samples X =∑m\n`=1X`. Variables X` are i.i.d. Let x = Pr[X = 1]. 1. If P = Q, then probability of rejecting a sample from P is not more than ε2, i.e., x ≤ ε2.\nE[X] = m · x,Var[X] = m · (x− x2). Then\nPr [ X ≥ 2mε2 ] ≤ Pr [ X ≥ E [X] + 3 √ Var [X] ] ≤ 1\n10 ,\nwhere the first inequality holds because 2mε2 > m · ε2 + 3 √ mε2 > E[X] + 3 √ Var[X]; the second is Cantelli’s inequality. 2. We first observe that Dist (P ∗, Q∗) ≥ ε − Dist (Q,Q∗) − Dist (P, P ∗). We want to argue that if Dist (P, P ∗) ≥ 6ε2, then Algorithm 3 rejects P with probability at least 9/10. Indeed, then probability x of rejecting a sample from P must be at least 12Dist (P, P\n∗) ≥ 3ε2, then E[X] ≥ 3mε2 and\nPr [ X ≤ 2mε2 ] ≤ Pr [ X −E [X] ≤ −E[X]\n3\n] ≤ Pr [ X −E [X] ≤ −3 √ Var [X] ] ≤ 1\n10 .\nThus we get Dist (P ∗, Q∗) ≥ ε−Dist (Q,Q∗)−Dist (P, P ∗) ≥ ε− 2ε2− 6ε2 ≥ ε2 if ε is small enough constant.\nTheorem 4.3. Together, Algorithm 3 and Algorithm 2 tell whether P = Q, or P 6= Q, when Dist (P,Q) ≥ ε, with probability at least 23 using O( n3/2 ε2 ) samples.\nProof. In case P = Q, Algorithm 3 produces P ∗ = Q∗ and m(1 − 2ε2) samples from P ∗ with probability at least 0.9. Furthermore, Algorithm 2 converts these m(1− 2ε2) samples into Poisson m′ = Ω(m) samples and accepts P = Q with 4/5 probability. Overall, we have 4/5 · 0.9 > 2/3 probability of correctly accepting P = Q.\nWhen Dist (P,Q) ≥ ε, Algorithm 3 either correctly rejects P ∗, or produces P ∗ and Q∗ s.t. Dist (P ∗, Q∗) ≥ ε2 with probability at least 0.9. In the latter case Algorithm 2 correctly rejects P ∗ with probability at least 4/5. Overall, we get the probability of correctly rejecting P to be at least 4/5 · 0.9 > 2/3."
    }, {
      "heading" : "4.3 Lower Bound",
      "text" : "In this section we will show that any algorithm that tests identity of a sparse Markov chain representing card riffle shuffling requires at least Ω ( n ε2 ) independent trials, where each trial is a nlength word generated by a sparse Markov chain. We recall Definition 2 of a sparse Markov chain adapting it slightly for the convenience of lower bound presentation. Sparse Markov chains P = {Pt}Tt=1, Q = {Qt}Tt=1: each independent run consists of T = O(n) time steps; has O(n) states for each time t; Markov chain starts from a single state at t = 0 ; there are only O(1) possible transitions Pt(ij) 6= 0 from each state i to other states for all but the very first time step t > 0.\nTheorem 4.4. There is an instance of Identity testing problem for a sparse Markov chain Q that requires at least m ≥ Ω( n\nε2 ) i.i.d. samples to check identity of Q with 99% confidence8.\nProof. The high-level proof idea is similar to that of Theorem 3.2, but particular details and proofs are more involved. At an abstract level, we construct a sparse Markov chain Q, with respect to which we are interested in testing identity, and a class of sparse Markov chains P such that\n1. Every P ∈ P is at least ε far from Q, i.e., dTV ( WT P ,WT Q ) ≥ ε for any P ∈ P.\n2. There is a constant c > 0, such that it is impossible to distinguish m i.i.d. samples of T -length words generated by a random Markov chain P̄ ∼ P from the samples produced by Q with probability equal to or greater than 99100 , for m ≤ cn ε2 .\nWe denote the joint distribution of m i.i.d. samples from Q by Q⊗m and that from P̄ by P̄⊗m. To prove the last point we show that dTV ( Q⊗m, P̄⊗m ) is small for some m = Ω ( n ε2 ) . We now describe our construction. To simplify presentation, we use multi-edges in the description of sparse Markov chains. We convert this instance into simple edge graph by the same duplicating trick we employed in the lower bound construction of Theorem 3.2.\nMarkov Chain Q: T = 2n + 1 time steps; single state at t = 0, states [2n] at each 1 ≤ t ≤ T . All states are divided into two categories: Frequently visited (F ) def = {2i − 1, i ∈ [n]} and\n8We assume ε = ω(n−1/6) in the requirement Dist (P,Q) ≥ ε, when P 6= Q.\nRare (R) def = {2i, i ∈ [n]}. Table below describes weighted multi-graph Q = {Qt}Tt=1: x ∈ F, y ∈ R, x, y ∈ [2n] denote respective generic frequent and rare states. We adopt notational convention: 0 def = 2n, 2n+ 1 def = 1.\nF → F F → R R→ F R→ R\nt=1: (1→ x) ∈ E ∅ ∅ ∅ Q1(1, x) = 1 n t=2k: (x→ x) ∈ E (x→ x± 1) ∈ E (y → y ± 1) ∈ E ∅\nQt(x, x) = 1− 2n Qt(x, x± 1) = 1 n Qt(y, y ± 1) = 1 2\nt=2k+1: (x→ x) ∈ E ∅ ∅ (y → y)1,2 ∈ E Qt(x, x) = 1 Qt(e1) = Qt(e2) = 1 2\nFamily P: every P ∈ P has the same set of states and edges as Q. Similar to the construction in Section 3.1, we only change weights of the multi-edge pairs (independently and uniformly at random for each pair (y → y)1,2 and time t):\nPt(e1,2) = 1± 4ε\n2 or Pt(e1,2) = 1∓ 4ε 2 .\nThe idea of this construction is that a typical trajectory of Markov chain Q or P ∈ P stays in frequent states almost all the time and very rarely visits one of the rare states. However, a typical trajectory has constant probability of visiting a rare state.\nLemma 4.7. Any P ∈ P is at least ε-far from Q, i.e., Dist (P,Q) = dTV ( WT P ,WT Q ) ≥ ε.\nProof. Consider a P ∈ P. First, we argue that probability of any word w with only a single visit to a rare state (at even t = 2k) satisfies |PrQ[w]−PrP [w]| = 4ε · PrQ[w]. Indeed, let visit to the rare state 2i in w happen at the time t = 2k, then{\nPrQ[w] = PrQ[s1 . . . s2k] · 12 ·PrQ[s2k+1 . . . s2n+1|s2k+1] PrP [w] = PrP [s1 . . . s2k] · 12(1± 4ε) ·PrP [s2k+1 . . . s2n+1|s2k+1].\nTherefore, as Q and P have the same transitional probabilities of st → st+1 for t 6= 2k, we get\n|PrQ [w]−PrP [w]| = 4ε ·PrQ [w] .\nNow, the probability of visiting a rare state exactly once (at even t) in Q is precisely\n∑ w with\n1 rare visit\nPrQ [w] = n · 2 n · ( 1− 2 n )n−1 > 2 · e−2 > 1 4 ,\nwhere we used inequality (1− 1n) n−1 > e−1 to get the estimate in the right hand side. Hence,\ndTV\n( WT\nP ,WT Q\n) ≥ ∑ w with\n1 rare visit\n|PrQ [w]−PrP [w]| = ∑ w with\n1 rare visit\n4ε ·PrQ [w] ≥ 4ε · 1\n4 = ε.\nWe define collisions and 3-way collisions similar to the proof of Theorem 3.2. Namely, a collision is a transition that occurs in two samples (3-way collision – in at least three samples) of Q⊗m or P⊗m from the same state and at the same time. Moreover, as P ∼ P and Q are the same except for the double edges between rare states, we are only interested in the collisions along multi-edges between rare states.\nLemma 4.8. The expected number of collisions in samples Q⊗m is O ( m2\nn2\n) = O ( 1 ε4 ) .\nProof. Let Iw(m1,m2, y, t) indicate the event that either of the transitions along edges e1,2 = y → y at time t, occurred in samples m1 and m2. Note that at each fixed time all of the n frequent states are equally likely to occur in a sample from Q. Therefore, the probability of visiting given rare state y at time t in a single run is at most O(1/n2). This implies PrQ[Iw(m1,m2, y, t) = 1] = O (\n1 n2 · 1 n2\n) = O(1/n4). Let X denote the total number of collisions.\nE [X] = ∑\nm1 6=m2 ∑ i 6=j E [Iw(m1,m2, y, t)] = O ( m2n2 1 n4 ) = O ( m2 n2 )\nLemma 4.9. The probability of a 3-way collision in samples Q⊗m is o(1).\nProof. Similar to the proof of Lemma 4.8 we can give an upper bound on the expected number of 3-way collisions being O ( n2m3 · 1 n6 · ) = O ( m3 n4 ) . Markov’s inequality concludes the proof.\nNow consider a typical set of m words generated by Q (or equivalently drawn from Q⊗m). As we know from Lemma 4.9 it has no 3-way collisions and by Markov’s inequality and Lemma 4.8 has at most O( 1\nε2 ) collisions with probability greater than 9/10 (for sufficiently small c > 0 and\nm = cn ε2\n). As we show next a typical set of m words drawn from Q has similar probability under the P̄ ∼ P and Q models.\nLemma 4.10. At least 1/2 of the sets S = {w1, . . . , wm} of samples from Q⊗m satisfy\n1 2 ·PrQ⊗m [S] < PrP̄⊗m [S] < 2 ·PrQ⊗m [S]\nProof. We consider the ratio of the respective probabilities (∗) def= PrP̄⊗m [S]PrQ⊗m [S] . As in Lemma 3.7, both probabilities in the numerator and denominator can be expressed in terms of simple statistics for the set S, specifically the number of single step transitions between pairs of states. Also similar to the Lemma 3.7, the corresponding multiplicative terms for P̄⊗m and Q⊗m are identical except for the collisions along multi-edges between two rare states (at least two transitions between a pair of rare states). Moreover, we also differentiate type I and type II collisions between rare states: (type I) transitions between rare states y → y at time t were made along different edges e1 and e2; (type II) two transitions were made along the same edge either e1, or e2. Per type I and II collisions the corresponding terms in (∗) are respectively (1− 4ε)(1 + 4ε) = 1− 16ε2 and 1 + 16ε2.\nWe further continue following the proof of the Lemma 3.7, and make sure that with high probability there are only X = O( 1\nε4 ) collisions and no 3-way collisions in S. We can make sure\nthat the difference between numbers of type I and type II collisions is at most O( √ X) = O( 1\nε2 )\nwith probability at least 3/4, as the choice of collision type in w under Q model is uniform between\ntype I and type II and independent across different collisions. For small enough m = Ω( n ε2\n) we can make sure that at least 12 fraction of words w under Q model have the number of collisions at most c1 ε4 and also have the difference between number of type I and type II collisions at most c2 ε2\n, for some small constants c1, c2 > 0. In this case we get the following bounds on (∗).\n2 > ( 1 + 16ε2 ) c2 ε2 > PrP̄⊗m [S] PrQ⊗m [S] > ( 1− 256ε4 ) c1 2ε4 · ( 1− 16ε2 ) c2 ε2 > 1/2\nLemma 4.10 shows that dTV ( Q⊗m, P̄⊗m ) ≤ 34 for m ≤ cn ε2\nfor some constant c, which implies that no algorithm can successfully distinguish Q from the family P with probability greater than 3 4 for some m = Ω( n ε2 )."
    }, {
      "heading" : "5 Open Questions",
      "text" : "In this paper, we proposed a new framework for studying property testing questions on Markov chains. There seem to be multiple avenues for future research and abundant number of open problems arising from this framework. We first list some questions which may be of interest here.\n1. What is the optimal sample complexity for identity testing on symmetric Markov chains? In this paper, we show an upper bound of Õ ( HitTQ · log (HitTQ) + nε ) samples (Theorem 3.1).\nWe conjecture that Θ ( n ε ) (same as our lower bound) is the right sample complexity for this problem and an explicit dependence on the hitting time of chain Q may not be necessary. It is implicitly captured to an extent by the guarantee we get from the parameter ε.\n2. What is the optimal sample complexity for identity testing on the sparse Markov chains defined in Section 4? In this paper, we show an upper bound of Õ ( n3/2\n2\n) (Theorem 4.2).\nWe conjecture that Θ ( n ε2 ) (same as our lower bound) is the right sample complexity for this problem.\n3. As there is a natural operation of taking a convex combination of Markov chains, it is natural to ask how our spectral definition of distance 1− ρ (\n[P,Q]√\n) between two symmetric chains\nchanges if we substitute either P or Q with a convex combination of P and Q. How does the distance now relate to the original value?\n4. How is the distance ε = 1 − ρ (\n[P,Q]√\n) between two Markov chains P and Q related to\nthe distance between Markov chains P k and Qk,i.e., states in Markov chains P and Q being observed only at intervals of size k?\n5. Given ε2 ≥ ε1, and access to words from each of two chains, can we distinguish whether the two chains are ≤ ε1-close or ≥ ε2-far? This problem, known as closeness testing in literature, is another interesting direction using our framework."
    } ],
    "references" : [ {
      "title" : "Optimal testing for properties of distributions",
      "author" : [ "Jayadev Acharya", "Constantinos Daskalakis", "Gautam Kamath" ],
      "venue" : "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Acharya et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Acharya et al\\.",
      "year" : 2015
    }, {
      "title" : "The frequency goodness of fit test for probability chains",
      "author" : [ "Maurice S Bartlett" ],
      "venue" : "In Mathematical Proceedings of the Cambridge Philosophical Society,",
      "citeRegEx" : "Bartlett.,? \\Q1951\\E",
      "shortCiteRegEx" : "Bartlett.",
      "year" : 1951
    }, {
      "title" : "Trailing the dovetail shuffle to its lair",
      "author" : [ "Dave Bayer", "Persi Diaconis" ],
      "venue" : "Ann. Appl. Probab., 2(2):294–313,",
      "citeRegEx" : "Bayer and Diaconis.,? \\Q1992\\E",
      "shortCiteRegEx" : "Bayer and Diaconis.",
      "year" : 1992
    }, {
      "title" : "Testing random variables for independence and identity",
      "author" : [ "Tugkan Batu", "Eldar Fischer", "Lance Fortnow", "Ravi Kumar", "Ronitt Rubinfeld", "Patrick White" ],
      "venue" : "In Proceedings of the 42nd Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Batu et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Batu et al\\.",
      "year" : 2001
    }, {
      "title" : "Testing closeness of discrete distributions",
      "author" : [ "Tuğkan Batu", "Lance Fortnow", "Ronitt Rubinfeld", "Warren D Smith", "Patrick White" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Batu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Batu et al\\.",
      "year" : 2013
    }, {
      "title" : "Hypothesis testing for markovian models with random time observations",
      "author" : [ "Flavia Barsotti", "Anne Philippe", "Paul Rochet" ],
      "venue" : "Journal of Statistical Planning and Inference,",
      "citeRegEx" : "Barsotti et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Barsotti et al\\.",
      "year" : 2016
    }, {
      "title" : "Testing shape restrictions of discrete distributions",
      "author" : [ "Clément L. Canonne", "Ilias Diakonikolas", "Themis Gouleakis", "Ronitt Rubinfeld" ],
      "venue" : "In Proceedings of the 33rd Symposium on Theoretical Aspects of Computer Science,",
      "citeRegEx" : "Canonne et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Canonne et al\\.",
      "year" : 2016
    }, {
      "title" : "Optimal algorithms for testing closeness of discrete distributions",
      "author" : [ "Siu-On Chan", "Ilias Diakonikolas", "Gregory Valiant", "Paul Valiant" ],
      "venue" : "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Chan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2014
    }, {
      "title" : "Testing equivalence between distributions using conditional samples",
      "author" : [ "Clément L. Canonne", "Dana Ron", "Rocco A. Servedio" ],
      "venue" : "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Canonne et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Canonne et al\\.",
      "year" : 2014
    }, {
      "title" : "Testing k -modal distributions: Optimal algorithms via reductions",
      "author" : [ "Constantinos Daskalakis", "Ilias Diakonikolas", "Rocco A. Servedio", "Gregory Valiant", "Paul Valiant" ],
      "venue" : "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Daskalakis et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Daskalakis et al\\.",
      "year" : 2013
    }, {
      "title" : "Group representations in probability and statistics",
      "author" : [ "Persi Diaconis" ],
      "venue" : "Lecture notes. Hayward, Calif. Institute of Mathematical Statistics,",
      "citeRegEx" : "Diaconis.,? \\Q1988\\E",
      "shortCiteRegEx" : "Diaconis.",
      "year" : 1988
    }, {
      "title" : "Mathematical Developments from the Analysis of Riffle Shuffling",
      "author" : [ "P. Diaconis" ],
      "venue" : "Technical report (Stanford University. Dept. of Statistics)",
      "citeRegEx" : "Diaconis.,? \\Q2002\\E",
      "shortCiteRegEx" : "Diaconis.",
      "year" : 2002
    }, {
      "title" : "A new approach for testing properties of discrete distributions",
      "author" : [ "Ilias Diakonikolas", "Daniel M. Kane" ],
      "venue" : "In Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Diakonikolas and Kane.,? \\Q2016\\E",
      "shortCiteRegEx" : "Diakonikolas and Kane.",
      "year" : 2016
    }, {
      "title" : "The effect of dependence on chi-squared and empiric distribution tests of fit",
      "author" : [ "Leon J Gleser", "David S Moore" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Gleser and Moore,? \\Q1983\\E",
      "shortCiteRegEx" : "Gleser and Moore",
      "year" : 1983
    }, {
      "title" : "Mixing time estimation in reversible markov chains from a single sample path",
      "author" : [ "Daniel J Hsu", "Aryeh Kontorovich", "Csaba Szepesvári" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Hsu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2015
    }, {
      "title" : "The bhattacharyya distance and detection between markov chains",
      "author" : [ "Dimitri Kazakos" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "Kazakos.,? \\Q1978\\E",
      "shortCiteRegEx" : "Kazakos.",
      "year" : 1978
    }, {
      "title" : "Markov chains and mixing times. Providence, R.I",
      "author" : [ "David Asher Levin", "Yuval Peres", "Elizabeth Lee Wilmer" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "Levin et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Levin et al\\.",
      "year" : 2009
    }, {
      "title" : "Testing properties of collections of distributions",
      "author" : [ "Reut Levi", "Dana Ron", "Ronitt Rubinfeld" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "Levi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Levi et al\\.",
      "year" : 2013
    }, {
      "title" : "The effect of dependence on chi squared tests of fit",
      "author" : [ "David S Moore" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Moore,? \\Q1982\\E",
      "shortCiteRegEx" : "Moore",
      "year" : 1982
    }, {
      "title" : "On size increase for goodness of fit tests when observations are positively dependent",
      "author" : [ "I Molina", "D Morales", "L Pardo", "I Vajda" ],
      "venue" : "Statistics & Risk Modeling,",
      "citeRegEx" : "Molina et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Molina et al\\.",
      "year" : 2002
    }, {
      "title" : "A coincidence-based test for uniformity given very sparsely sampled discrete data",
      "author" : [ "Liam Paninski" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Paninski.,? \\Q2008\\E",
      "shortCiteRegEx" : "Paninski.",
      "year" : 2008
    }, {
      "title" : "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling",
      "author" : [ "Karl Pearson" ],
      "venue" : "Philosophical Magazine Series",
      "citeRegEx" : "Pearson.,? \\Q1900\\E",
      "shortCiteRegEx" : "Pearson.",
      "year" : 1900
    }, {
      "title" : "The analysis of categorical data from complex sample surveys: Chi-squared tests for goodness of fit and independence in two-way tables",
      "author" : [ "Jon N.K. Rao", "Alastair J. Scott" ],
      "venue" : "Journal of the Americal Statistical Association,",
      "citeRegEx" : "Rao and Scott.,? \\Q1981\\E",
      "shortCiteRegEx" : "Rao and Scott.",
      "year" : 1981
    }, {
      "title" : "Serial dependence of observations leading to contingency tables, and corrections to chi-squared statistics",
      "author" : [ "Simon Tavare", "Patricia M.E. Altham" ],
      "venue" : null,
      "citeRegEx" : "Tavare and Altham.,? \\Q1983\\E",
      "shortCiteRegEx" : "Tavare and Altham.",
      "year" : 1983
    }, {
      "title" : "Error exponents for composite hypothesis testing of Markov forest distributions",
      "author" : [ "Vincent Y.F. Tan", "Animashree Anandkumar", "Alan S. Willsky" ],
      "venue" : "In Proceedings of the 2010 IEEE International Symposium on Information Theory, ISIT",
      "citeRegEx" : "Tan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2010
    }, {
      "title" : "An automatic inequality prover and instance optimal identity testing",
      "author" : [ "Gregory Valiant", "Paul Valiant" ],
      "venue" : "In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Valiant and Valiant.,? \\Q2014\\E",
      "shortCiteRegEx" : "Valiant and Valiant.",
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Classical distribution testing assumes access to i.i.d. samples from the distributions that are being tested. We initiate the study of Markov chain testing, assuming access to a single sample from the Markov Chains that are being tested. In particular, we get to observe a single trajectory X0, . . . , Xt, . . . of an unknown Markov Chain M, for which we do not even get to control the distribution of the starting state X0. Our goal is to test whether M is identical to a model Markov Chain M′. In the first part of the paper, we propose a measure of difference between two Markov chains, which captures the scaling behavior of the total variation distance between words sampled from the Markov chains as the length of these words grows. We provide efficient and sample nearoptimal testers for identity testing under our proposed measure of difference. In the second part of the paper, we study Markov chains whose state space is exponential in their description, providing testers for testing identity of card shuffles. We apply our results to testing the validity of the Gilbert, Shannon, and Reeds model for the riffle shuffle. Supported by a Microsoft Research Faculty Fellowship, and NSF Award CCF-1551875, CCF-1617730 and CCF1650733. Supported by NSF Award CCF-1551875, CCF-1617730 and CCF-1650733. Supported by NSF Award CCF-1551875, CCF-1617730 and CCF-1650733. ar X iv :1 70 4. 06 85 0v 1 [ cs .L G ] 2 2 A pr 2 01 7",
    "creator" : "LaTeX with hyperref package"
  }
}