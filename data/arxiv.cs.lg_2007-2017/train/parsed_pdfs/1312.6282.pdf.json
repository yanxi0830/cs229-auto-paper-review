{
  "name" : "1312.6282.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning",
    "authors" : [ "François Denis", "Mattias Gybels", "Amaury Habrard" ],
    "emails" : [ "FRANCOIS.DENIS@LIF.UNIV-MRS.FR", "MATTIAS.GYBELS@LIF.UNIV-MRS.FR", "AMAURY.HABRARD@UNIV-ST-ETIENNE.FR" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Many applications in natural language processing, text analysis or computational biology require learning probabilistic models over finite variable-size strings such as probabilistic automata, Hidden Markov Models (HMM), or more generally, weighted automata. Weighted automata\nexactly model the class of rational series, and their algebraic properties have been widely studied in that context (Droste et al., 2009). In particular, they admit algebraic representations that can be characterized by a set of finitedimensional linear operators whose rank corresponds to the minimum number of states needed to define the automaton. From a machine learning perspective, the objective is then to infer good estimates of these linear operators from finite samples. In this paper, we consider the problem of learning the linear representation of a weighted automaton, from a finite sample, composed of variable-size strings i.i.d. from an unknown target distribution.\nRecently, the seminal papers of Hsu et al. (2009) for learning HMM and Bailly et al. (2009) for weighted automata, have defined a new category of approaches - the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle & Mohri, 2012). Extensions to probabilistic models for treestructured data (Bailly et al., 2010; Parikh et al., 2011; Cohen et al., 2012), transductions (Balle et al., 2011) or other graphical models (Anandkumar et al., 2012c;b;a; Luque et al., 2012) have also attracted a lot of interest.\nSpectral methods suppose that the main parameters of a model can be expressed as the spectrum of a linear operator and estimated from the spectral decomposition of a matrix that sums up the observations. Given a rational series r, the values taken by r can be arranged in a matrix Hr whose rows and columns are indexed by strings, such that the linear operators defining r can be recovered directly from the right singular vectors of Hr. This matrix is called the Hankel matrix of r.\nIn a learning context, given a learning sample S drawn from a target distribution p, an empirical estimate HS of Hp is built and then, a rational series p̃ is inferred from the right singular vectors of HS . However, the size of HS increases drastically with the size of S and state of the art approaches\nar X\niv :1\n31 2.\n62 82\nv1 [\ncs .L\nG ]\n2 1\nD ec\nconsider smaller matrices HU,VS indexed by limited subset of strings U and V . It can be shown that the above learning scheme, or slight variants of it, are consistent as soon as the matrix HU,VS has full rank (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||HU,VS −HU,Vp ||2 between the empirical Hankel matrix and its mean (Hsu et al., 2009; Bailly, 2011).\nOn the one hand, limiting the size of the Hankel matrix avoids prohibitive calculations. Moreover, most existing concentration bounds on sum of random matrices depend on their size and suggest that ||HU,VS − HU,Vp ||2 may become significantly looser with the size of U and V , compromising the accuracy of the inferred model.\nOn the other hand, limiting the size of the Hankel matrix implies a drastic loss of information: only the strings of S compatible with U and V will be considered. In order to limit the loss of information when dealing with restricted sets U and V , a general trend is to work with other functions than the target p, such as the prefix function p(u) = ∑ v∈Σ∗ p(uv) or the factor function p̂ =∑\nv,w∈Σ∗ p(vuw) (Balle et al., 2013; Luque et al., 2012). These functions are rational, they have the same rank as p, a representation of p can easily be derived from representations of p or p̂ and they allow a better use of the information contained in the learning sample.\nA first contribution is to provide a dimension free concentration inequality for ||HU,VS −HU,Vp ||2, by using recent results on tail inequalities for sum of random matrices showing that restricting the dimension of H is not mandatory.\nHowever, these results cannot be directly applied as such to the prefix and factor series, since the norm of the corresponding random matrices are unbounded. A second contribution of the paper is then to define two classes of parametrized functions, pη and p̂η , that constitute continuous intermediates between p and p (resp. p and p̂), and to provide analogous dimension-free concentration bounds for these two classes.\nThese bounds are evaluated on a benchmark made of 11 problems extracted from the PAutomaC challenge (Verwer et al., 2012). These experiments show that the bounds derived from our theoretical results are quite tight - compared to the exact values- and that they significantly improve existing bounds, even on matrices of fixed dimensions.\nThese results have two practical consequences for spectral learning: (i) the concentration of the empirical Hankel matrix around its mean does not highly depend on its dimension and the only reason not to use all the information contained in the sample should only rely on computing resources limitations. In that perspective, using random techniques to perform singular values decomposition on huge\nHankel matrices should be considered (Halko et al., 2011); (ii) by constrast, the concentration is weaker for the prefix and factor functions, and smoothed variants should be used, with an appropriate parameter.\nThe paper is organized as follows. Section 2 introduces the main notations, definitions and concepts. Section 3 presents a first dimension free-concentration inequality for the standard Hankel matrices. Then, we introduce the prefix and the factor variants and provide analogous concentration results. Section 4 describes some experiments before the conclusion presented in Section 5."
    }, {
      "heading" : "2. Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1. Singular Values, Eigenvalues and Matrix Norms",
      "text" : "Let M ∈ Rm×n be a m × n real matrix. The singular values of M are the square roots of the eigenvalues of the matrix MTM , where MT denotes the transpose of M : σmax(M) and σmin(M) denote the largest and smallest singular value of M , respectively.\nIn this paper, we mainly use the spectral norms || · ||k induced by the corresponding vector norms on Rn and defined by ||M ||k = maxx 6=0 ||Mx||k||x||k :\n• ||M ||1 = Max1≤j≤n ∑m i=1 |M [i, j]|,\n• ||M ||∞ = Max1≤i≤m ∑n j=1 |M [i, j]|,\n• ||M ||2 = σmax(M).\nWe have: ||M ||2 ≤ √ ||M ||1||M ||∞.\nThese norms can be extended, under certain conditions, to infinite matrices and the previous inequalities remain true when the corresponding norms are defined."
    }, {
      "heading" : "2.2. Rational stochastic languages and Hankel matrices",
      "text" : "Let Σ be a finite alphabet. The set of all finite strings over Σ is denoted by Σ∗, the empty string is denoted by , the length of string w is denoted by |w| and Σn (resp. Σ≤n) denotes the set of all strings of length n (resp. ≤ n). For any string w, let Pref(w)={u ∈ Σ∗|∃v ∈ Σ∗ w = uv}.\nA series is a mapping r : Σ∗ 7→ R. A series r is convergent if the sequence r(Σ≤n) = ∑ w∈Σ≤n r(w) is convergent; its limit is denoted by r(Σ∗). A stochastic language p is a probability distribution over Σ∗, i.e. a series taking non negative values and converging to 1.\nLet n ≥ 1 and M be a morphism defined from Σ∗ to M(n), the set of n× n matrices with real coefficients. For all u ∈ Σ∗, let us denote M(u) by Mu and Σx∈ΣMx by MΣ. A series r over Σ is rational if there exists an integer n ≥ 1, two vectors I, T ∈ Rn and a morphism M :\nΣ∗ 7→ M(n) such that for all u ∈ Σ∗, r(u) = ITMuT . The triplet 〈I,M, T 〉 is called an n-dimensional linear representation of r. The vector I can be interpreted as a vector of initial weights, T as a vector of terminal weights and the morphism M as a set of matrix parameters associated with the letters of Σ. A rational stochastic language is thus a stochastic language admitting a linear representation.\nLet U, V ⊆ Σ∗, the Hankel matrix HU,Vr , associated with a series r, is the matrix indexed by U × V and defined by HU,Vr [u, v] = r(uv), for any (u, v) ∈ U × V . If U = V = Σ∗,HU,Vr , simply denoted byHr, is a bi-infinite matrix. In the following, we always assume that ∈ U and that U and V are ordered in quasi-lexicographic order: strings are first ordered by increasing length and then, according to the lexicographic order. It can be shown that a series r is rational if and only if the rank of the matrix Hr is finite. The rank of Hr is equal to the minimal dimension of a linear representation of r.\nLet r be a non negative convergent rational series and let 〈I,M, T 〉 be a minimal d-dimensional linear representation of r. Then, the sum Id+MΣ+. . .+MnΣ +. . . is convergent and r(Σ∗) = IT (Id −MΣ)−1T where Id is the identity matrix of size d.\nSeveral convergent rational series can be naturally associated with a stochastic language p:\n• p, defined by p(u) = ∑ v∈Σ∗ p(uv), the series associ-\nated with the prefixes of the language,\n• p̂, defined by p̂(u) = ∑ v,w∈Σ∗ p(vuw), the series as-\nsociated with the factors of the language.\nIt can be noticed that p(u) = p(uΣ∗), the probability that a string begins with u, but that in general, p̂(u) ≥ p(Σ∗uΣ∗), the probability that a string contains u as a substring.\nIf 〈I,M, T 〉 is a minimal d-dimensional linear representation of p, then 〈I,M, (Id − MΣ)−1T 〉 (resp. 〈[IT (Id − MΣ)\n−1]T ,M, (Id −MΣ)−1T 〉) is a minimal linear representation of p (resp. of p̂). Any linear representation of these variants of p can be reconstructed from the others.\nFor any integer k ≥ 1, let S(k)p = ∑\nu1u2...uk∈Σ∗ p(u1u2 . . . uk) = I\nT (Id −MΣ)−kT.\nClearly, p(Σ∗) =S(1)p = 1, p(Σ∗) =S (2) p and p̂(Σ∗) =S (3) p .\nLet U, V ⊆ Σ∗. For any string w ∈ Σ∗, let us define the matrices HU,Vw , H U,V w and Ĥ U,V w by\n• HU,Vw [u, v] = 1uv=w,\n• HU,Vw [u, v] = 1uv∈Pref(w) and\n• ĤU,Vw [u, v] = ∑ x,y∈Σ∗ 1xuvy=w\nfor any (u, v) ∈ U × V . For any sample of strings S, let HU,VS = 1 |S| ∑ w∈S H U,V w , H U,V S = 1 |S| ∑ w∈S H U,V w and\nĤU,VS = 1 |S| ∑ w∈S Ĥ U,V w .\nFor example, let S = {a, ab}, U = V = { , a, b}. We have\nH U,V S =  0 0.5 00.5 0 0.5 0 0 0  , HU,VS =  1 1 01 0 0.5 0 0 0  , ĤU,V S = 2.5 1 0.51 0 0.5 0.5 0 0  ."
    }, {
      "heading" : "2.3. Spectral Algorithm for Learning Rational Stochastic Languages",
      "text" : "Rational series admit a canonical linear representation determined by their Hankel matrix. Let r be a rational series of rank d and U ⊂ Σ∗ such that the matrix HU×Σ∗r (denoted by H in the following) has rank d.\n• For any string s, let Ts be the constant matrix whose rows and columns are indexed by Σ∗ and defined by Ts[u, v] = 1 if v = us and 0 otherwise.\n• Let E be a vector indexed by Σ∗ whose coordinates are all zero except the first one equals to 1: E[u] = 1u= and let P be the vector indexed by Σ∗ defined by P [u] = r(u).\n• Let H = LDRT be a reduced singular value decomposition of H: R (resp. L) is a matrix whose columns form a set of orthonormal vectors - the right (resp. left) singular vectors of H - and D is a d × d diagonal matrix, composed of the singular values of H .\nThen, 〈RTE, (RTTxR)x∈Σ, RTP 〉 is a linear representation of r (Bailly et al., 2009; Hsu et al., 2009; Bailly, 2011; Balle et al., 2012).\nProposition 1. 〈RTE, (RTTxR)x∈Σ, RTP 〉 is a linear representation of r\nProof. From the definition of Ts, it can easily be shown that the mapping s 7→ Ts is a morphism: Ts1Ts2 [u, v] = ∑ w∈Σ∗ Ts1 [u,w]Ts2 [w, v] = 1 iff v = us1s2 and 0 otherwise. If X is a matrix whose rows are indexed by Σ∗, we have TsX[u, v] = ∑ w Ts[u,w]X[w, v] = X[us, v]: ie the rows of TSX are included in the set of rows of X . Then, it follows from the definition of E that ETTs is equal to the first row of Ts (indexed by ) with all coordinates equal to zero except the one indexed by s which equal 1.\nNow, from the reduced singular value decomposition of H = LDRT at rank d, R is a matrix of dimension∞× d\nwhose columns form a set of orthonormal vectors - the right singular vectors of H - such that RTR = Id and RRTHT =HT (RRT is the orthogonal projection on the subspace spanned by the rows of H). One can easily deduce, by a recurrence over n, that for every string u = x1 . . . xn, (RTTx1R) ◦ . . . ◦ (RTTxnR)RTHT = RTTuHT . Indeed, the inequality is trivially true for n = 0 since T = Id. Then, we have that RTTxRRTTuHT = RTTxTuH T = RTTxuH T since the columns of TuHT are rows of H and T is a morphism.\nIf PT is the first row of H then: ETR(RTTx1R)◦ . . . ◦(RTTxnR)RTP =ETTuP = r(u). Thus, 〈RTE, (RTTxR)x∈Σ, RTP 〉 is a linear representation of r of dimension d. Note here that r is only needed in the right singular vectors R and in the vector P .\nThe basic spectral algorithm for learning rational stochastic languages aims at identifying the canonical linear representation of the target p determined by its Hankel matrix Hp.\nLet S be a sample independently drawn according to p:\n• Choose sets U, V ⊆ Σ∗ and build the Hankel matrix HU×VS ,\n• choose a rank d and compute a reduced SVD of HU×VS truncated at rank d,\n• build the canonical linear representation 〈RTSE, (RTSTxRS)x∈Σ, RTSPS〉 from the right singular vectors RS and the empirical distribution pS defined from S.\nAlternative learning strategies consist in learning p or p̂, using the same algorithm, and then to compute an estimate of p. In all cases, the accuracy of the learned representation mainly depends on the estimation of R. The Stewart formula (Stewart, 1990) bounds the principle angle θ between the spaces spanned by the right singular vectors of R and RS :\n|sin(θ)| ≤ ||HU×VS −HU×Vr ||2\nσmin(H U×V r )\n.\nAccording to this formula, the concentration of the Hankel matrix around its mean is critical and the question of limiting the sizes of U and V naturally arises. Note that the Stewart inequality does not give any clear indication on the impact or on the interest of limiting these sets. Indeed, Weyl’s inequalities can be used to show that both the numerator and the denominator of the right part of the inequality increase with U and V ."
    }, {
      "heading" : "3. Concentration Bounds for Hankel Matrices",
      "text" : "Let p be a rational stochastic language over Σ∗, let ξ be a random variable distributed according to p, let U, V ⊆ Σ∗ and let Z(ξ) ∈ R|U |×|V | be a random matrix. For instance, Z(ξ) may be equal to HU,Vξ , H U,V ξ or Ĥ U,V ξ .\nConcentration bounds for sum of random matrices can be used to estimate the spectral distance between the empirical matrix ZS computed on the sample S and its mean (see (Hsu et al., 2011) for references). However, most of classical inequalities depend on the dimensions of the matrices. For example, it can be proved that with probability at least 1− δ (Kakade, 2010):\n||ZS − EZ||2 ≤ 6M√ N\n(√ log d+ √ log 1\nδ\n) (1)\nwhere N is the size of S, d is the minimal dimension of the matrix Z and ||Z||2 ≤ M almost surely. If Z = HU,Vξ , then M = 1; if Z = H U,V\nξ , M = Ω(D 1/2) in the worst\ncase; if Z = ĤU,Vξ , ||Z||2 is generally unbounded.\nThese concentration bounds get worse with both sizes of the matrices. Coming back to the discussion at the end of Section 2, they suggest to limit the size of the sets U and V , and therefore, to design strategies to choose optimal sets.\nWe then use recent results (Tropp, 2012; Hsu et al., 2011) to obtain dimension-free concentration bounds for Hankel matrices. Let ξ1, . . . , ξN be some random variables and for each i = 1, . . . , N , and let Xi = Xi(ξ1, . . . , ξi) be a random matrix function of ξ1, . . . , ξi. The notation Ei[·] is a shortcut for E[·|ξ1, . . . , ξi−1]. Theorem 1. (Matrix Bernstein Bound)(Hsu et al., 2011). If there exists b > 0, σ > 0, k > 0 s.t. for all i = 1, . . . , N , Ei[Xi] = 0, ||Xi||2 ≤ b, || 1N ∑N i=1 Ei(X2i )||2 ≤\nσ2 and E [ tr (\n1 N ∑N i=1 Ei(X2i ) )] ≤ σ2k almost surely,\nthen for all t > 0,\nPr [ λmax ( 1\nN N∑ i=1 Xi\n) > √ 2σ2t\nN +\nbt\n3N ] ≤ k · t et − t− 1 .\nWe use this theorem in the particular case where the random variables ξi are i.i.d. and each matrixXi depends only on ξi.\nThis theorem is valid for symmetric matrices, but it can be extended to general real-valued matrices thanks to the principle of dilation.\nLet Z be a random matrix, the dilation of Z is the symmetric random matrix X defined by\nX = [ 0 Z ZT 0 ] . Then X2 = [ ZZT 0 0 ZTZ ]\nand ||X||2 = ||Z||2, tr(X2) = tr(ZZT ) + tr(ZTZ) and ||X2||2 ≤Max(||ZZT ||2, ||ZTZ||2).\nWe can then reformulate the result that we use as follows.\nTheorem 2. Let ξ1, . . . , ξN be i.i.d. random variables, and for i = 1, . . . , N , let Zi = Z(ξi) be i.i.d. matrices and Xi the dilation of Zi. If there exists b > 0, σ > 0, and k > 0 such that E[X1] = 0, ||X1||2 ≤ b, ||E(X21 )||2 ≤ σ2 and tr(E(X21 )) ≤ σ2k almost surely, then for all t > 0,\nPr [ || 1 N N∑ i=1 Xi||2 > √ 2σ2t N + bt 3N ] ≤ k·t(et−t−1)−1.\nWe will then make use of this theorem to derive our new concentration bounds. Section 3.1 deals with the standard case, Section 3.2 with the prefix case and Section 3.3 with the factor case.\n3.1. Concentration Bound for the Hankel Matrix HU,Vp Let p be a rational stochastic language over Σ∗, let S be a sample independently drawn according to p, and let U, V ⊆ Σ∗. In this section, we compute a bound on ||HU,VS −HU,Vp ||2 which is independent from the sizes of U and V and holds in particular when U = V = Σ∗.\nLet ξ be a random variable distributed according to p, let Z(ξ) = HU,Vξ − HU,Vp be the random matrix defined by Zu,v = 1ξ=uv − p(uv) and let X be the dilation of Z.\nClearly, E(X) = 0. In order to apply Theorem 2, it is necessary to compute the parameters b, σ and k. We first prove a technical lemma that will provide a bound on E(X2). Lemma 1. For any u, u′ ∈ U , v, v′ ∈ V ,\n|E(ZuvZu′v)| ≤ p(u′v) and |E(ZuvZuv′)| ≤ p(uv′).\nProof.\nE(ZuvZu′v) = E(1ξ=uv1ξ=u′v)− p(uv)p(u′v) = ∑ w∈Σ∗ p(w)1w=uv1w=u′v − p(uv)p(u′v)\n= p(u′v)[1u=u′ − p(uv)]\nand |E(ZuvZu′v)| ≤ p(u′v).\nThe second inequality is proved in a similar way.\nNext lemma provides parameters b, σ and k needed to apply Theorem 2.\nLemma 2. ||X||2 ≤ 2, E(Tr(X2)) ≤ 2S(2)p and ||E(X2)||2 ≤ S(2)p .\nProof. 1. ∀u ∈ U , ∑ v∈V |Zu,v| = ∑ v∈V |1ξ=uv − p(uv)| ≤ 1 + p(uΣ∗) ≤ 2. Therefore, ||Z||∞ ≤ 2. In a similar way, it can be shown that ||Z||1 ≤ 2. Hence,\n||X||2 = ||Z||2 ≤ √ ||Z||∞||Z||1 ≤ 2.\n2. For all (u, u′) ∈ U2 : ZZT [u, u′] = ∑ v∈V Zu,vZu′,v .\nTherefore,\nE(Tr(ZZT )) = E( ∑ u∈U ZZT [u, u])\n= E( ∑\nu∈U,v∈V Zu,vZu,v)\n≤ ∑\nu∈U,v∈V E(Zu,vZu,v)\n≤ S(2)p .\nIn a similar way, it can be proved that E(Tr(ZTZ)) ≤ S(2)p and therefore, E(Tr(X2)) ≤ 2S(2)p .\n3. For any u ∈ U ,∑ u′∈U |E(ZZT [u, u′])| ≤ ∑ u′∈U,v∈V |E(ZuvZu′v)|\n≤ ∑\nu′∈U,v∈V p(u′v)\n≤ S(2)p .\nHence, ||ZZT ||∞ ≤ S(2)p . It can be proved, in a similar way, that ||ZTZ||∞ ≤ S(2)p , ||ZZT ||1 ≤ S(2)p and ||ZTZ||1 ≤ S(2)p . Therefore, ||X2||2 ≤ S(2)p .\nWe can now prove the main theorem of this section: Theorem 3. Let p be a rational stochastic language and let S be a sample of N strings drawn i.i.d. from p. For all t > 0,\nPr ||HU,VS −HU,Vp ||2 > √ 2S (2) p t\nN +\n2t\n3N  ≤ 2t(et−t−1)−1. Proof. Let ξ1, . . . , ξN be N independent copies of ξ, let Zi = Z(ξi) and let Xi be the dilation of Zi for i = 1, . . . , N . Lemma 2 shows that the 4 conditions of Theorem 2 are fulfilled with b = 2, σ2 = S(2)p and k = 2.\nThis bound is independent from U and V . It can be noticed that the proof also provides a dimension dependent bound by replacing S(2)p with ∑ (u,v)∈U×V p(uv), which may result in a significative improvement if U or V are small.\n3.2. Bound for the prefix Hankel Matrix HU,Vp\nThe random matrix Z(ξ) = H U,V ξ − H U,V p is defined by Zu,v = 1uv∈Pref(ξ) − p(uv). It can easily be shown that ||Z||2 may be unbounded if U or V are unbounded: ||Z||2 = Ω(|ξ|1/2). Hence, Theorem 2 cannot be directly applied, which suggests that the concentration of Z around its mean could be far weaker than the concentration of Z.\nFor any η ∈ [0, 1], we define a smoothed variant of p by\npη(u) = ∑ x∈Σ∗ η|x|p(ux) = ∑ n≥0 ηnp(uΣn).\nNote that p1 = p, p0 = p and that p(u) ≤ pη(u) ≤ p(u) for any string u. Therefore, the functions pη are natural intermediates between p and p. Moreover, when p is rational, each pη is also rational.\nProposition 2. Let p be a rational stochastic language and let 〈I, (Mx)x∈Σ, T 〉 be a minimal linear representation of p. Let T η = (Id − ηMΣ)−1T . Then, pη is rational and 〈I, (Mx)x∈Σ, T η〉 is a linear representation of pη . Proof. For any string u, pη(u) = ∑ n≥0 I TMuη nMnΣT =\nITMu( ∑ n≥0 η nMnΣ)T = I TMuT η .\nNote that T can be computed from T η when η and MΣ are known and therefore, it is a consistent learning strategy to learn pη from the data, for some η, and next, to derive p.\nFor any 0 ≤ η ≤ 1, let Zη(ξ) be the random matrix defined by\nZη[u, v] = ∑ x∈Σ∗ η|x|1ξ=uvx − pη(uv)\n= ∑ x∈Σ∗ η|x|(1ξ=uvx − p(uvx)).\nfor any (u, v) ∈ U × V . It is clear that E(Zη) = 0 and we show below that ||Zη||2 is bounded if η < 1.\nThe moments S(k)pη can naturally be associated with pη . For any 0 ≤ η ≤ 1 and any k ≥ 1, let\nS (k) pη\n= ∑\nu1u2...uk∈Σ∗ pη(u1u2 . . . uk).\nWe have S(k)pη = I T (Id−MΣ)−k(Id− ηMΣ)−1T and it is clear that S(k)p0 = S (k) p and S (k) p1 = S (k+1) p . Lemma 3. ||Zη||2 ≤ 1\n1− η + S (1) pη .\nProof. Indeed, let u ∈ U .∑ v∈V |Zη[u, v]| ≤ ∑ v,x∈Σ∗ η|x|1ξ=uvx + ∑ v,x∈Σ∗ η|x|p(uvx)\n≤ (1 + η + . . .+ η|ξ|−|u|) + S(1)pη ≤ 1 1− η + S (1) pη .\nHence, ||Zη||∞ ≤ 11−η +S (1) pη . Similarly, ||Zη||1 ≤ 11−η + S (1) pη , which completes the proof.\nWhen U and V are bounded, let l be the maximal length of a string in U ∪ V . It can easily be shown that ||Zη||2 ≤ l + 1 + S\n(1) pη and therefore, in that case,\n||Zη||2 ≤Min(l + 1, 1\n1− η ) + S (1) pη\n(2)\nwhich holds even if η = 1.\nLemma 4. |E(Zη[u, v]Zη[u′, v])| ≤ pη(u′v), for any u, u′, v ∈ Σ∗.\nProof. We have E((1ξ=w − p(w))(1ξ=w′ − p(w′))) = E(1ξ=w1ξ=w′)− p(w)p(w′). Therefore,\nE(Zη[u, v]Zη[u′, v])\n= ∑ x,x′ η|xx ′|[E(1ξ=uvx1ξ=u′vx′)− p(u′vx′)p(uvx)]\n= ∑ x,x′,w η|xx ′|p(w)1w=u′vx′ [1w=uvx − p(uvx)]\n= ∑ x,x′ η|xx ′|p(u′vx′)[1u′vx′=uvx − p(uvx)]\n= ∑ x′ η|x ′|p(u′vx′)[ ∑ x η|x|(1u′vx′=uvx − p(uvx))]\nand |E(Zη[u, v]Zη[u′, v])| ≤ ∑ x′ η|x ′|p(u′vx′) = pη(u ′v)\nsince −1 ≤ −pη(uv) ≤ ∑ x η|x|(1u′vx′=uvx − p(uvx)) ≤ 1 i.e. | ∑ x η|x|(1u′vx′=uvx − p(uvx))| ≤ 1.\nLemma 5.\n||E(Zη Z T η )||2 ≤ S (2) pη and Tr(E(Zη Z T η )) ≤ S (2) pη .\n||E(ZTη Zη)||2 ≤ S (2) pη and Tr(E(ZTη Zη)) ≤ S (2) pη .\nProof. Indeed,\n||E(ZηZ T η )||∞ ≤Maxu ∑ u′,v |E(Zη[u, v]Zη[u′, v])|\n≤ ∑ u′,v,x′ η|x ′|p(u′vx′) ≤ S(2)pη .\nIn the same way,\nTr(E(ZηZ T η )) = ∑ u,v E(Zη[u, v]Zη[u, v]) ≤ S(2)pη .\nSimilar computations provide all the inequalities.\nTherefore, we can apply the Theorem 2 with b = 11−η + S (1) pη , σ2 = S (2) pη and k = 2.\nTheorem 4. Let p be a rational stochastic language, let S be a sample of N strings drawn i.i.d. from p and let 0 ≤ η < 1. For all t > 0,\nPr ||HU,Vη,S −HU,Vpη ||2 > √ 2S (2) pη t N + t 3N [ 1 1− η + S (1) pη ] ≤ 2t(et − t− 1)−1.\nRemark that when η = 0 we find back the concentration bound of Theorem 3, and that Inequality 2 provides a bound when η = 1."
    }, {
      "heading" : "3.3. Bound for the factor Hankel Matrix Hp̂U,V",
      "text" : "The random matrix Ẑ(ξ) = ĤU,Vξ −Hp̂U,V is defined by\nẐu,v = ∑\nx,y∈Σ∗ 1ξ=xuvy − p̂(uv).\n||Ẑ||2 is generally unbounded. Moreover, unlike the prefix case, ||Ẑ||2 can be unbounded even if U and V are finite. Hence, the Theorem 2 cannot be directly applied either.\nWe can also define smoothed variants of p̂ by\np̂η(u) = ∑\nx,y∈Σ∗ η|xy|p(xuy) = ∑ m,n≥0 ηm+np(ΣmuΣn)\nwhich have properties similar to functions pη:\n• p ≤ p̂η ≤ p̂, p̂1 = p̂ and p̂0 = p,\n• if 〈I, (Mx)x∈Σ, T 〉 be a minimal linear representation of p then 〈Îη, (Mx)x∈Σ, T η〉, where Îη = (Id − ηMTΣ ) −1I , is a linear representation of p̂η .\nHowever, proofs of the previous Section cannot be directly extended to p̂η because p is bounded by 1, a property which is often used in the proofs, while p̂ is not. Next lemma provides a tool which allows to bypass this difficulty.\nLemma 6. Let 0 < η ≤ 1. For any integer n, (n+ 1)ηn ≤ Kη where\nKη =\n{ 1 if η ≤ e−1\n(−eη ln η)−1 otherwise.\nProof. Let f(x) = (x + 1)ηx. We have f ′(x) = ηx(1 + (x + 1) ln η) and f takes its maximum for xM = −1 − 1/ ln η, which is positive if and only if η > 1/e. We have f(xM ) = (−eη ln η)−1.\nLemma 7. Let w, u ∈ Σ∗. Then,∑ x,y∈Σ∗ η|xy|1w=xuy ≤ Kη and p̂(u) ≤ Kηp(Σ∗uΣ∗).\nProof. Indeed, if w = xuy, then |xy| = |w| − |u| and u appears at most |w| − |u|+ 1 times as a factor of w.\np̂(u) = ∑\nx,y∈Σ∗ η|xy|p(xuy)\n= ∑\nw∈Σ∗uΣ∗ p(w) ∑ x,y∈Σ∗ η|xy|1w=xuvy\n≤ Kηp(Σ∗uΣ∗).\nFor η ∈ [0, 1], let Ẑη(ξ) be the random matrix defined by\nẐη[u, v] = ∑\nx,y∈Σ∗ η|xy|1ξ=xuvy − p̂η(uv)\n= ∑\nx,y∈Σ∗ η|xy|(1ξ=xuvy − p(xuvy)).\nand, for any k ≥ 0, let\nS (k) p̂η\n= ∑\nu1u2...uk∈Σ∗ p̂η(u1u2 . . . uk).\nIt can easily be shown that E(Ẑη) = 0, S(k)p̂η = I T (Id − ηMΣ) −1(Id −MΣ)−k(Id − ηMΣ)−1T , S(k)p̂0 = S (k) p and S (k) p̂1 = S (k+2) p .\nIt can be shown that ||Ẑη||2 is bounded if η < 1. Lemma 8.\n||Ẑη||2 ≤ (1− η)−2 + S(1)p̂η .\nProof. Indeed, for all u,∑ v∈V |Ẑη[u, v]| ≤ ∑ v,x,y∈Σ∗ η|xy|1ξ=xuvy + p̂η(uv)\n≤ (1 + η + . . .+ η|ξ|−|u|)2 + S(1)p̂η ≤ 1 (1− η)2 + S (1) pη .\nHence, ||Ẑη||∞ ≤ 1(1−η)2 + S (1) p̂η . Similarly, ||Zη||1 ≤ 1\n(1−η)2 + S (1) p̂η , which completes the proof. Lemma 9. For any u, u′, v ∈ Σ∗, |E(Ẑη[u, v]Ẑη[u′, v])| ≤ Kη ∑ x′y′ η |x′y′|p(x′u′vy′).\nProof. We have\nE(Ẑη[u, v]Ẑη[u′, v]) =∑ x,x′,y,y′ η|xx ′yy′|[E(1ξ=xuvy1ξ=x′u′vy′)\n− p(x′u′vy′)p(xuvy)].\nWe remark that\nE(1ξ=xuvy1ξ=x′u′vy′)− p(x′u′vy′)p(xuvy)\n= ∑ w p(w)1w=x′u′vy′(1w=xuvy − p(xuvy)),\nand therefore, E(Ẑη[u, v]Ẑη[u′, v]) =∑ x′,y′,w η|x ′y′|p(w)1w=x′u′vy′( ∑ x,y η|xy|(1w=xuvy − p(xuvy))). Moreover, | ∑ xy η |xy|(1w=xuvy − p(xuvy))| ≤ Kη .\nLemma 10.\n||E(ẐẐT )||2 ≤ KηS(2)p̂η and Tr(E(ẐẐ T )) ≤ KηS(2)p̂η .\nProof. We have ||E(ẐẐT )||∞ ≤ Supu ∑ u′,v |E(Ẑη[u, v]Ẑη[u′, v])|\nThen from previous lemma:∑ u′,v |E(Ẑη[u, v]Ẑη[u′, v])| ≤ KηS (2) p̂η\nfor any u ∈ Σ∗. Finally,\nTr(E(ẐẐT )) = ∑ u,v E(Ẑη[u, v]Ẑη[u, v]) ≤ KηS(2)p̂η .\nSimilar proof gives\nLemma 11.\n||E(ẐT Ẑ)||2 ≤ KηS(2)p̂η and Tr(E(Ẑ T Ẑ)) ≤ KηS(2)p̂η .\nEventually, we can apply the Theorem 2 with b = (1 − η)−2 + S\n(1) p̂η , σ2 = KηS (2) p̂η and k = 2.\nTheorem 5. Let p be a rational stochastic language, let S be a sample of N strings drawn i.i.d. from p and let 0 ≤ η < 1. For all t > 0,\nPr ||ĤU,Vη,S −HU,Vp̂η ||2 > √ 2KηS (2) p̂η t\nN +\nt\n3N\n[ 1\n(1− η)2 + S (1) p̂η ] ≤ 2t(et − t− 1)−1.\nRemark that when η = 0 we find back the concentration bound of Theorem 3. We provide experimental evaluation of the proposed bounds in the next Section."
    }, {
      "heading" : "4. Experiments",
      "text" : "The proposed bounds are evaluated on the benchmark of PAutomaC (Verwer et al., 2012) which provides samples of strings generated from several probabilistic automata, designed to evaluate probabilistic automata learning. Eleven problems have been selected from that benchmark for which sparsity of the Hankel matrices makes the use of standard SVD algorithms available from NumPy or SciPy possible. Table 1 provides some information about the selected problems.\nFigure 1 shows the typical behavior of S(1)pη and S (1) p̂η , similar for all the problems.\nFor each problem, the exact value of ||HU,VS − HU,Vp ||2 is computed for sets U and V of the form Σ≤l, trying to maximize l according to our computing resources. It is compared to the bounds provided by Theorem 3 and Equation (1), with δ = 0.05 (Table 2). The optimized bound\n(”opt.”), refers to the case where σ2 has been calculated over U × V rather than Σ∗×Σ∗ (see the remark at the end of Section 3.1). Tables 3 and 4 show analog comparisons for the prefix and the factor cases with different values of η. Similar results have been obtained for all the problems of PautomaC. We can remark that our dimension-free bounds are significantly more accurate than the one provided by\nEquation (1). Notice that in the prefix case, the dimensionfree bound has a better behavior in the limit case η = 1 than the bound from Eq. (1). This is due to the fact that in our bound, the term that bounds ||Z||2 appears in the 1N term while it appears in the 1√\nN term in the other one.\nImplication for learning These results show that the concentration of the empirical Hankel matrix around its mean does not highly depend on its dimension and they suggest that as far as computational resources permit it, the size of the matrices should not be artificially restricted in spectral algorithms for learning HMMs or rational stochastic languages.\nTo illustrate this claim, we have performed additional experiments by considering matrices with 3,000 columns and a variable number of rows, from 70 to 3,000.\nFor each problem and each set of rows and columns, we have computed the r first right singular vectors R of HU,V (resp. RS of H U,V S ), where r is the rank of the target, and the distance between the linear spaces spanned by R and RS . Most classical distances are based on the principal angles θ1 ≥ θ2 ≥ . . . ≥ θr between the spaces span(R) and span(RS). The largest principal angle θ1 is a harsh measure since, even if the two spaces coincide along the last r− 1 principal angles, the distance between the two spaces can be large. We have considered the following measure\nd(span(R), span(RS)) = 1− 1\nr r∑ i=1 cos θi (3)\nwhich is equal to 0 if the spaces coincide and 1 if they are completely orthogonal, and which takes into account all the principal angles.\nThe table 5 shows the sum ∑r i=1 cos θi for each problem. The table 6 displays the same information but each measure is normalised by using formula 3.\nThese tables show that for all problems but two, the spaces spanned by the right singular vectors are the closest for the maximal size Hankel matrix. They also show that these spaces remain quite distant for 6 problems over 11. For 4 problems, the spaces are already close to each other even for small matrices - but it can be noticed that widening the matrix do not deteriorate the results."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We have provided dimension-free concentration inequalities for Hankel matrices in the context of spectral learning of rational stochastic languages. These bounds cover 3 cases, each one corresponding to a specific way to exploit the strings under observation, paying attention to the strings themselves, to their prefixes or to their factors. For the last two cases, we introduced parametrized variants which allow a trade-off between the rate of the concentration and the exploitation of the information contained in data.\nA consequence of these results is that there is no a priori good reason, aside from computing resources limitations, to restrict the size of the Hankel matrices. This suggests\nan immediate future work consisting in investigating recent random techniques (Halko et al., 2011) to compute singular values decomposition on Hankel matrices in order to be able to deal with huge matrices. Then, a second aspect is to evaluate the impact of these methods on the quality of the models, including an empirical evaluation of the behavior of the standard approach and its prefix and factor extensions, along with the influence of the parameter η.\nAnother research direction would be to link up the prefix and factor cases to concentration bounds for sum of random tensors and to generalize the results to the case where a fixed number ≥ 1 of factors is considered for each string.\nAcknowledments This work was supported by the French National Agency for Research (Lampada - ANR-09-EMER-007)."
    } ],
    "references" : [ {
      "title" : "A spectral algorithm for latent dirichlet allocation",
      "author" : [ "A. Anandkumar", "D.P. Foster", "D. Hsu", "S. Kakade", "Liu", "Y.-K" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning mixtures of tree graphical models",
      "author" : [ "A. Anandkumar", "D. Hsu", "F. Huang", "S. Kakade" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "A method of moments for mixture models and hidden markov models",
      "author" : [ "A. Anandkumar", "D. Hsu", "S.M. Kakade" ],
      "venue" : "Proceedings of COLT - Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "Méthodes spectrales pour l’inférence grammaticale probabiliste de langages stochastiques rationnels",
      "author" : [ "R. Bailly" ],
      "venue" : "PhD thesis, Aix-Marseille Université,",
      "citeRegEx" : "Bailly,? \\Q2011\\E",
      "shortCiteRegEx" : "Bailly",
      "year" : 2011
    }, {
      "title" : "Grammatical inference as a principal component analysis problem",
      "author" : [ "R. Bailly", "F. Denis", "L. Ralaivola" ],
      "venue" : "In Proceedings of ICML, pp",
      "citeRegEx" : "Bailly et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bailly et al\\.",
      "year" : 2009
    }, {
      "title" : "A spectral approach for probabilistic grammatical inference on trees",
      "author" : [ "R. Bailly", "A. Habrard", "F. Denis" ],
      "venue" : "In Proceedings of ALT,",
      "citeRegEx" : "Bailly et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bailly et al\\.",
      "year" : 2010
    }, {
      "title" : "Spectral learning of general weighted automata via constrained matrix completion",
      "author" : [ "B. Balle", "M. Mohri" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Balle and Mohri,? \\Q2012\\E",
      "shortCiteRegEx" : "Balle and Mohri",
      "year" : 2012
    }, {
      "title" : "A spectral learning algorithm for finite state transducers",
      "author" : [ "B. Balle", "A. Quattoni", "X. Carreras" ],
      "venue" : "In Proceedings of ECML/PKDD",
      "citeRegEx" : "Balle et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Balle et al\\.",
      "year" : 2011
    }, {
      "title" : "Local loss optimization in operator models: A new insight into spectral learning",
      "author" : [ "B. Balle", "A. Quattoni", "X. Carreras" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "Balle et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Balle et al\\.",
      "year" : 2012
    }, {
      "title" : "Spectral learning for non-deterministic dependency parsing",
      "author" : [ "F.M. Luque", "A. Quattoni", "B. Balle", "X. Carreras" ],
      "venue" : "In Proceedings of EACL,",
      "citeRegEx" : "Luque et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Luque et al\\.",
      "year" : 2012
    }, {
      "title" : "A spectral algorithm for latent tree graphical models",
      "author" : [ "A.P. Parikh", "L. Song", "E.P. Xing" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "Parikh et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2011
    }, {
      "title" : "Reduced-rank hidden Markov models",
      "author" : [ "S. Siddiqi", "B. Boots", "G.J. Gordon" ],
      "venue" : "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010),",
      "citeRegEx" : "Siddiqi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Siddiqi et al\\.",
      "year" : 2010
    }, {
      "title" : "Hilbert space embeddings of hidden markov models",
      "author" : [ "L. Song", "B. Boots", "S.M. Siddiqi", "G.J. Gordon", "A.J. Smola" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "Song et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2010
    }, {
      "title" : "Perturbation theory for the singular value decomposition",
      "author" : [ "G.W. Stewart" ],
      "venue" : "In SVD and Signal Processing II: Algorithms, Analysis and Applications,",
      "citeRegEx" : "Stewart,? \\Q1990\\E",
      "shortCiteRegEx" : "Stewart",
      "year" : 1990
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "J.A. Tropp" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Tropp,? \\Q2012\\E",
      "shortCiteRegEx" : "Tropp",
      "year" : 2012
    }, {
      "title" : "Results of the PAutomaC probabilistic automaton learning competition",
      "author" : [ "S. Verwer", "R. Eyraud", "C. de la Higuera" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "Verwer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Verwer et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "(2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle & Mohri, 2012).",
      "startOffset" : 180,
      "endOffset" : 262
    }, {
      "referenceID" : 12,
      "context" : "(2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle & Mohri, 2012).",
      "startOffset" : 180,
      "endOffset" : 262
    }, {
      "referenceID" : 8,
      "context" : "(2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle & Mohri, 2012).",
      "startOffset" : 180,
      "endOffset" : 262
    }, {
      "referenceID" : 5,
      "context" : "Extensions to probabilistic models for treestructured data (Bailly et al., 2010; Parikh et al., 2011; Cohen et al., 2012), transductions (Balle et al.",
      "startOffset" : 59,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "Extensions to probabilistic models for treestructured data (Bailly et al., 2010; Parikh et al., 2011; Cohen et al., 2012), transductions (Balle et al.",
      "startOffset" : 59,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : ", 2012), transductions (Balle et al., 2011) or other graphical models (Anandkumar et al.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : ", 2011) or other graphical models (Anandkumar et al., 2012c;b;a; Luque et al., 2012) have also attracted a lot of interest.",
      "startOffset" : 34,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "(2009) for learning HMM and Bailly et al. (2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "It can be shown that the above learning scheme, or slight variants of it, are consistent as soon as the matrix H S has full rank (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||H S −H p ||2 between the empirical Hankel matrix and its mean (Hsu et al.",
      "startOffset" : 129,
      "endOffset" : 181
    }, {
      "referenceID" : 8,
      "context" : "It can be shown that the above learning scheme, or slight variants of it, are consistent as soon as the matrix H S has full rank (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||H S −H p ||2 between the empirical Hankel matrix and its mean (Hsu et al.",
      "startOffset" : 129,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : ", 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||H S −H p ||2 between the empirical Hankel matrix and its mean (Hsu et al., 2009; Bailly, 2011).",
      "startOffset" : 169,
      "endOffset" : 201
    }, {
      "referenceID" : 9,
      "context" : "In order to limit the loss of information when dealing with restricted sets U and V , a general trend is to work with other functions than the target p, such as the prefix function p(u) = ∑ v∈Σ∗ p(uv) or the factor function p̂ = ∑ v,w∈Σ∗ p(vuw) (Balle et al., 2013; Luque et al., 2012).",
      "startOffset" : 245,
      "endOffset" : 285
    }, {
      "referenceID" : 15,
      "context" : "These bounds are evaluated on a benchmark made of 11 problems extracted from the PAutomaC challenge (Verwer et al., 2012).",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "Then, 〈RE, (RTxR)x∈Σ, RP 〉 is a linear representation of r (Bailly et al., 2009; Hsu et al., 2009; Bailly, 2011; Balle et al., 2012).",
      "startOffset" : 59,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "Then, 〈RE, (RTxR)x∈Σ, RP 〉 is a linear representation of r (Bailly et al., 2009; Hsu et al., 2009; Bailly, 2011; Balle et al., 2012).",
      "startOffset" : 59,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "Then, 〈RE, (RTxR)x∈Σ, RP 〉 is a linear representation of r (Bailly et al., 2009; Hsu et al., 2009; Bailly, 2011; Balle et al., 2012).",
      "startOffset" : 59,
      "endOffset" : 132
    }, {
      "referenceID" : 13,
      "context" : "The Stewart formula (Stewart, 1990) bounds the principle angle θ between the spaces spanned by the right singular vectors of R and RS : |sin(θ)| ≤ ||HU×V S −HU×V r ||2 σmin(H U×V r ) .",
      "startOffset" : 20,
      "endOffset" : 35
    }, {
      "referenceID" : 14,
      "context" : "We then use recent results (Tropp, 2012; Hsu et al., 2011) to obtain dimension-free concentration bounds for Hankel matrices.",
      "startOffset" : 27,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : "Experiments The proposed bounds are evaluated on the benchmark of PAutomaC (Verwer et al., 2012) which provides samples of strings generated from several probabilistic automata, designed to evaluate probabilistic automata learning.",
      "startOffset" : 75,
      "endOffset" : 96
    } ],
    "year" : 2013,
    "abstractText" : "Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution. These methods rely on a singular value decomposition of a matrix HS , called the Hankel matrix, that records the frequencies of (some of) the observed strings. The accuracy of the learned distribution depends both on the quantity of information embedded in HS and on the distance betweenHS and its mean Hr. Existing concentration bounds seem to indicate that the concentration over Hr gets looser with the size of Hr, suggesting to make a tradeoff between the quantity of used information and the size of Hr. We propose new dimensionfree concentration bounds for several variants of Hankel matrices. Experiments demonstrate that these bounds are tight and that they significantly improve existing bounds. These results suggest that the concentration rate of the Hankel matrix around its mean does not constitute an argument for limiting its size.",
    "creator" : "LaTeX with hyperref package"
  }
}