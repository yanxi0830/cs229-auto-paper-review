{
  "name" : "1504.06305.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Regularization-free estimation in trace regression with symmetric positive semidefinite matrices",
    "authors" : [ "Martin Slawski", "Ping Li", "Matthias Hein" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 4.\n06 30\n5v 1\n[ st\nat .M\nL ]\n2 3"
    }, {
      "heading" : "1 Introduction",
      "text" : "Trace regression models of the form\nyi = tr(X ⊤ i Σ ∗) + εi, i = 1, . . . , n, (1)\nwhere Σ∗ ∈ Rm1×m2 is the parameter of interest to be estimated given measurement matrices Xi ∈ Rm1×m2 and observations yi contaminated by errors εi, i = 1, . . . , n, have attracted considerable interest in high-dimensional statistical inference, machine learning and signal processing over the past few years. Research in these areas has focused on a setting with few measurements n ≪ m1 · m2 and Σ∗ being (at least approximately) of low rank r ≪ min{m1,m2}. Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9]. A common thread in these works is the use of the nuclear norm of a matrix as a convex surrogate for its rank [22] in regularized estimation amenable to modern optimization techniques. This approach can be seen as natural generalization of ℓ1-norm (aka lasso) regularization for the standard linear regression model [28] that arises as a special case of model (1) in which both Σ∗ and the measurement matrices {Xi}ni=1 are diagonal. It is inarguable that in general regularization is essential if n < m1 · m2. However, the situation is less clear if Σ∗ is known to satisfy additional constraints that can be incorporated in estimation. Specifically, in the present paper we consider the case in which m1 = m2 = m and Σ∗ is known to be symmetric positive semidefinite (spd), written as Σ∗ ∈ Sm+ with Sm+\ndenoting the positive semidefinite cone in the space of symmetric real-valued m ×m matrices Sm. The set Sm+ deserves specific interest as it includes covariance matrices and Gram matrices in kernel-based learning methods [24]. It is rather common for these matrices to be of low rank (at least approximately), given the widespread use of principal components analysis and low-rank kernel approximations [33]. In the present paper, we focus on the usefulness of the spd constraint for estimation. We argue that if Σ∗ is spd and the measurement matrices {Xi}ni=1 obey certain conditions, constrained least squares estimation\nmin Σ∈Sm\n+\n1\n2n\nn∑\ni=1\n(yi − tr(X⊤i Σ))2 (2)\nmay perform similarly well in prediction and parameter estimation as approaches employing nuclear norm regularization with proper choice of the regularization parameter, including the interesting regime n < δm, where δm = dim(S\nm) = m(m + 1)/2. Note that the objective in (2) only consists of a data fitting term and is hence convenient to work with in practice since one does not need to choose any parameter. Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for high-dimensional linear regression with non-negative parameters [20, 25]. In these papers it is shown that for certain design matrices, non-negative least squares can achieve comparable performance to ℓ1-norm regularized estimation with regard to prediction, estimation and support recovery, thereby generalizing prior work [4, 13, 31] on sparse recovery of a non-negative vector in a noiseless setting.\nRelated work. Model (1) with Σ∗ ∈ Sm+ has been studied in several recent papers. A good deal of these papers consider the setup of compressed sensing according to which the matrices {Xi}ni=1 can be chosen by the user, with the goal to minimize the number of observations required to (approximately) recover Σ∗.\nIn [32], the problem of exactly recovering Σ∗ being low-rank from noiseless observations (εi = 0, i = 1, . . . , n) by solving a linear feasibility problem over the positive semidefinite cone is considered, which is equivalent to the proposed least squares problem (1) in a noiseless setting. Apart from the fact that we primarily study a noisy setting, we shall argue below that in the setup of compressed sensing the measurement matrices studied in [32] constitute an unfavourable choice relative to those recommended in the present paper.\nIn [10], recovery from rank-one measurements is considered, i.e., for {xi}ni=1 ⊂ Rm\nyi = x ⊤ i Σ ∗xi + εi = tr(X ⊤ i Σ ∗) + εi, with Xi = xix ⊤ i , i = 1, . . . , n. (3)\nAs opposed to [10], where estimation based on nuclear norm regularization is proposed, the present work is devoted to regularization-free estimation. While rank-one measurements as in (3) are also in the center of interest herein, our framework is not limited to this specific case.\nIn [5], rank-one measurements are considered for general Σ∗ ∈ Rm1×m2 . Specializing to Σ∗ ∈ Sm+ , the authors discuss an application of (3) to covariance matrix estimation given only one-dimensional projections {x⊤i zi}ni=1 of the data points, where the {zi}ni=1 are i.i.d. from a distribution with zero mean and covariance matrix Σ∗. In fact, when using observations yi = (x ⊤ i zi) 2, one obtains\n(x⊤i zi) 2 = x⊤i ziz ⊤ i xi = x ⊤ i Σ ∗xi + εi, with εi = x ⊤ i {ziz⊤i − Σ∗}xi, i = 1, . . . , n. (4)\nOn the other hand, in [5], no specific attention is given to the spd constraint: the convex program proposed therein, which can be seen as a modification of the approach in [10], applies to general symmetric matrices and does not enforce positive semidefiniteness.\nSpecializing model (3) further to the case in which also Σ∗ = σ∗(σ∗)⊤ has rank one,\none obtains the quadratic model\nyi = |x⊤i σ∗|2 + εi (5)\nwhich (with complex-valued σ∗) is relevant to the problem of phase retrieval [17] that has received some attention recently. The approach of [9] treats (5) as an instance of (1) and uses nuclear norm regularization to enforce rank-one solutions. In followup work [6], the authors show a refined recovery result stating that imposing an spd constraint − without regularization − suffices. A similar result has been proven independently by [12]. However, the results in both [6] and [12] only concern model (5). In [18], Σ∗ is assumed to be a complex Hermitian positive semidefinite matrix of unit trace, which is the setting in quantum state tomography. While the setting as well as the measurement matrices under consideration are different from ours, a notable point of contact to our work can be seen in the fact that the negative von Neumann entropy1, which is the proposed regularizer in [18], does not promote low rankedness, but constitutes one possible way of enforcing positive definiteness. At the same time, adaptivity of the approach to low rankedness is established in [18].\nOutline and contributions of the paper. In Section 2, we study statistical properties of constrained least squares estimation (2) in small sample (n < δm) and low-rank settings. Specifically, we introduce certain geometric conditions associated with the measurements {Xi}ni=1 that allow us to derive non-asymptotic upper bounds on the prediction and estimation error indicating that (2) can achieve competitive performance while being regularization-free. On the other hand, we show that without extra conditions on the measurements {Xi}ni=1, the performance of (2) can be as poor as that of unconstrained least squares. Section 3 contains numerical results based on synthetic and real world data that support or complement our theoretical results. Our findings are briefly summarized in Section 4. The appendix contains the proofs.\nNotation. We here gather notation and terminology used throughout the paper. For an integer d ≥ 1, let Md denote the Euclidean vector space of real d × d matrices with inner product 〈M,M ′〉 := tr(M⊤M ′), M,M ′ ∈ Md. The set of real symmetric d× d matrices Sd is a subspace of Md of dimension δd := d(d+1)/2. Each element M of Sd has an eigen-decomposition M = UΛU⊤ = ∑d j=1 λj(M)uju ⊤ j , where λ1(M) = λmax(M) ≥ λ2(M) ≥ . . . ≥ λd(M) = λmin(M) is the sequence of real eigenvalues with corresponding orthonormal eigenvectors {uj}dj=1, Λ = diag(λ1(M), . . . , λd(M)), and U = [u1 . . . ud]. For q ∈ [1,∞], Sd can be endowed with a norm given by the mapping M 7→ ‖M‖q := (∑d j=1 |λj(M)|q )1/q called the Schatten-q-norm. In particular, for q = 1 we speak of the nuclear norm, while q = 2 yields the Frobenius norm ‖·‖F . We set ‖M‖∞ := max1≤j≤d |λj(M)|, the spectral norm of M . We denote by S1(d) = {M ∈ Sd : ‖M‖q = 1} the Schatten-1-norm unit sphere and set S+1 (d) = S1(d) ∩ Sd+, where Sd+ = {M ∈ Sd : v⊤Mv ≥ 0 ∀v ∈ Rd} is the positive semidefinite cone in Sd. The symbols , ,≻,≺ are understood with respect to the semidefinite ordering, e.g. M M ′ means that M ′ − M ∈ Sd+. For v ∈ Rd and q ∈ [1,∞], ‖v‖q denotes the usual q-norm. For set A,B and a real number α, αA := {αa, a ∈ A}, A−B = {a− b, a ∈ A, b ∈ B}, and for A,B ⊂ Rd, dist(A,B) := mina∈A,b∈B‖a− b‖2.\nIt is convenient to re-write model (1) as\ny = X (Σ∗) + ε,\nwhere y = (yi) n i=1, ε = (εi) n i=1 and X : Mm → Rn is a linear map defined by (X (M))i = tr(X⊤i M), i = 1, . . . , n, referred to as sampling operator. Its adjoint X ∗ : Rn → Mm is given by the map v 7→ ∑n i=1 viXi.\n1The von Neumann entropy of a positive definite Hermitian matrix is given by the entropy of its eigenvalues"
    }, {
      "heading" : "2 Analysis",
      "text" : "Preliminaries. Throughout this section, we consider a special instance of model (1) in which\nyi = tr(XiΣ ∗) + εi, where Σ ∗ ∈ Sm+ , Xi ∈ Sm, and εi i.i.d.∼ N(0, σ2), i = 1, . . . , n.\n(6) The assumption that the errors {εi}ni=1 follow a Gaussian distribution is made for convenience as it simplifies the stochastic part of our analysis, which could be extended to cover error distributions with sub-Gaussian tails.\nNote that without loss of generality, we may assume that the {Xi}ni=1 are symmetric. In fact, any M ∈ Mm can be decomposed as\nM = M sym +M skew, where M sym = M +M⊤\n2 and M skew = M −M⊤ 2\ndenote the Euclidean projections of M onto Sm and its orthogonal complement (the subspace of skew-symmetric matrices), respectively. Accordingly, since Σ∗ ∈ Sm, we have tr(MΣ∗) = tr(M symΣ∗).\nIn the sequel, we study the statistical performance of the constrained least squares estimator\nΣ̂ ∈ argmin Σ∈Sm\n+\n1\n2n ‖y −X (Σ)‖22 (7)\nunder model (6) with respect to prediction and estimation. More specifically, under certain conditions on X , we shall derive bounds on\n(a) 1\nn ‖X (Σ∗)−X (Σ̂)‖22, and (b) ‖Σ̂− Σ∗‖1, (8)\nwhere (a) will be referred to as “prediction error” below.\nThe most basic method for estimating Σ∗ is ordinary least squares (ols) estimation\nΣ̂ols ∈ argmin Σ∈Sm\n1\n2n ‖y −X (Σ)‖22, (9)\nwhich is computationally much simpler than (7). While obtaining (7) requires techniques from convex programming, it is straightforward to compute (9) by solving a linear system of equations in δm = m(m + 1)/2 variables. On the other hand, the prediction error of ols scales as OP(dim(range(X ))/n), where dim(range(X )) can be as large as min{n, δm}, in which case the prediction error vanishes asymptotically only if δm/n → 0 as n → ∞. Moreover, the estimation error ‖Σ̂ols−Σ∗‖1 is unbounded unless n ≥ δm. Research conducted over the past few years has consequently focused on methods that deal successfully with the situation n < δm if the target Σ\n∗ possesses additional structure, notably low-rankedness. Indeed, if Σ∗ has rank r ≪ m, the intrinsic dimension of the problem becomes (roughly) mr ≪ δm. Rank-constrained estimation or regularized estimation with the matrix rank as regularizer yield computationally intractable optimization problems in general. In a large body of work, nuclear norm regularization, which can be seen as a convex surrogate of rank regularization, is considered as a computationally convenient alternative for which a series of adaptivity properties to underlying low-rankedness has been established, e.g. [7, 19, 21, 22, 23]. Complementing (9) with nuclear norm regularization gives rise to the estimator\nΣ̂1 ∈ argmin Σ∈Sm\n1\n2n ‖y −X (Σ)‖22 + λ‖Σ‖1, (10)\nwhere λ > 0 is a regularization parameter. In case an spd constraint is imposed (10) becomes\nΣ̂1+ ∈ argmin Σ∈Sm\n+\n1\n2n ‖y −X (Σ)‖22 + λ tr(Σ). (11)\nOur analysis aims at elucidating potential advantages of the spd constraint in the constrained least squares problem (7) from a statistical point of view. It turns out that depending on properties of X , the behaviour of Σ̂ can range from a performance similar to the least squares estimator Σ̂ols on the one hand to a performance similar to the nuclear norm regularized estimator Σ̂1+ with properly chosen/tuned λ on the\nother hand. The latter case appears to be remarkable inasmuch as Σ̂ may enjoy similar adaptivity properties as nuclear norm regularized estimators even though Σ̂ is obtained from a pure data fitting problem without any explicit form of regularization."
    }, {
      "heading" : "2.1 Negative results",
      "text" : "We first discuss examples of X for which the spd-constrained estimator Σ̂ does not improve (substantially) over the unconstrained estimator Σ̂ols. At the same time, these examples provide some clues on conditions that need to be imposed on X to achieve substantially better performance.\nExample 1: equivalence of constrained and unconstrained least squares Let m be even and consider measurement matrices of the form\nXi =\n[ X̃i 0\n0 −X̃i\n]\nfor matrices X̃i ∈ Sm/2, i = 1, . . . , n. For Σ ∈ Sm arbitrary, we can partition\nΣ = [ Σ11 Σ12 Σ12 Σ22, ]\nwhere Σ11 is the top m/2×m/2 block of Σ etc. We have\ntr(XiΣ) = tr(X̃i(Σ11 − Σ22)), i = 1, . . . , n.\nHence Σ enters the least squares objective (2) via the difference of the top and bottom m/2×m/2 blocks. Since for any dimension d\n{Σ− Σ′ : Σ ∈ Sd+, Σ′ ∈ Sd+} = Sd = {Σ− Σ′ : Σ ∈ Sd, Σ′ ∈ Sd},\nthe spd constraint becomes vacuous and can be dropped from (7).\nExample 2: Orthonormal design The following statement indicates that for orthonormal design, the prediction error of Σ̂ cannot be expected to improve over that of Σ̂ols by substantially more than a constant factor 1/2.\nProposition 1. Let Σ∗ = 0 so that y = ε, let n = δm and let {Xi}1≤i≤δm be an orthonormal basis of Sm. Then, ‖X (Σ̂)‖22/n → σ 2 2 in probability as m,n → ∞.\nBy contrast, it is desired that ‖X (Σ̂)‖22/n = oP(1) as m,n → ∞.\nExample 3: Random Gaussian design Consider the Gaussian orthogonal ensemble (GOE) of random matrices\nGOE(m) = {X = (xjk)1≤j,k≤m, {xjj}mj=1 i.i.d.∼ N(0, 1),\n{xjk = xkj}1≤j<k≤m i.i.d.∼ N(0, 1/2)}.\nRandom Gaussian measurements are common in compressed sensing-type settings, see e.g. [7, 21]. It is hence of interest to study measurementsXi i.i.d.∼ GOE(m), i = 1, . . . , n, in the context of the constrained least squares problem (7). The following statement, which follows from results in [2], points to a serious limitation associated with the use of such measurements.\nProposition 2. Consider measurements Xi i.i.d.∼ GOE(m), i = 1, . . . , n. Then, for any ε > 0, if n ≤ (1− ε)δm/2, with probability at least 1− 32 exp(−ε2δm), there exists ∆ ∈ Sm+ , ∆ 6= 0 such that X (∆) = 0.\nProposition 2 has the following implications.\n• If the number of measurements drops below one half of the ambient dimension δm, estimating Σ\n∗ based on (7) becomes ill-posed; the estimation error ‖Σ̂−Σ∗‖1 is unbounded, irrespective of the rank of Σ∗.\n• Geometrically, the consequence of Proposition 2 is that the convex cone CX = {z ∈ Rn : z = X (∆), ∆ ∈ Sm+ } contains 0. Unless 0 is contained in the boundary of CX (we conjecture that this event has measure zero), this means that CX = Rn, i.e., the spd constraint becomes vacuous.\nRemarks.\n1. In [32], the following noiseless analog to the constrained least squares problem (7) is considered:\nfind Σ ∈ Sm+ such thatX (Σ) = y = X (Σ∗), (12)\nwhere Xi ∼ GOE(m), i = 1, . . . , n. The authors prove that for all ξ ∈ (0, 1), there exists α ∈ (0, 1) so that if n ≥ αδm, Σ∗ is the unique solution of the feasibility problem (12) as long as rank(Σ∗) ≤ ξm. While this implies that the spd constraint allows undersampling (i.e., n < δm), it is not clear to what extent undersampling is possible, i.e., how small α could possibly be. Proposition 2 yields that α cannot be smaller than 1/2.\n2. It is of interest to relate Proposition 2 to corresponding results on the vector case (equivalent to having diagonal {Xi}ni=1 and diagonal Σ∗) in [13]. Compared to Proposition 2, the corresponding result in [13] applies to a much wider class of random measurement matrices including all random matrices with i.i.d. entries from a symmetric distribution around zero. It is thus natural to ask whether Proposition 2 holds more generally for all Wigner matrices [27].\n3. The fact that the threshold 12δm for the number measurements in Proposition 2 equals (up to the scaling factor σ2) the asymptotic prediction error of Example 2 is not a coincidence; this is part of a wider phenomenon as pointed out in [2]. In the framework of [2], 12δm is the “statistical dimension” of S m + ."
    }, {
      "heading" : "2.2 Slow rate bound on the prediction error",
      "text" : "We now turn to the first positive result on the spd-constrained least squares estimator Σ̂ under an additional condition on the sampling operator X . Specifically, the prediction error will be bounded as\n1 n ‖X (Σ∗)−X (Σ̂)‖22 = O(λ0‖Σ∗‖1 + λ20), where λ0 = 1 n ‖X ∗(ε)‖∞ (13)\nwith λ0 typically being of the order O( √ m/n) (up to logarithmic factors). The rate in (13) can be a significant improvement of what is achieved by Σ̂ols if ‖Σ∗‖1 = tr(Σ∗) is small. If λ0 = o(‖Σ∗‖1) that rate coincides with those of the nuclear norm regularized estimators (10), (11) with regularization parameter λ ≥ λ0, cf. Theorem 1 in [23]. For nuclear norm regularized estimators, the rate O(λ0‖Σ∗‖1) is achieved for any choice of X and is hence slow in the sense that the squared prediction error only decays at the rate n−1/2 instead of n−1. Therefore, we refer to (13) as “slow rate bound”.\nCondition on X . In order to arrive at a suitable condition to be imposed on X so that (13) can be achieved, it makes sense to re-consider Example 3 to identify possible obstacles. Proposition 2 states that as long as n is bounded away from δm/2 from above, there is a non-trivial ∆ ∈ Sm+ such that X (∆) = 0. Equivalently,\ndist(PX , 0) = min ∆∈S+\n1 (m)\n‖X (∆)‖2 = 0, where\nPX := {z ∈ Rn : z = X (∆), ∆ ∈ S+1 (m)}, and S+1 (m) := {∆ ∈ Sm+ : tr(∆) = 1}.\nIn this situation, it is in general not possible to derive a non-trivial upper bound on the prediction error as dist(PX , 0) = 0 may imply that CX = Rn in which case ‖X (Σ∗) − X (Σ̂)‖22 = ‖ε‖22. To rule this out, the condition dist(PX , 0) > 0 appears to be a natural requirement. More strongly, one may ask for the following:\nThere exists a constant τ > 0 such that τ20 (X ) := min ∆∈S+\n1 (m)\n1 n ‖X (∆)‖22 ≥ τ2. (14)\nThis condition is sufficient to obtain a slow rate bound in the vector case, cf. Theorem 1 in [25]. However, the condition required for the slow rate bound in Theorem 1 below is somewhat stronger than (14).\nCondition 1. There exist constants R∗ > 1 and τ∗ > 0 such that τ 2(X , R∗) ≥ τ2∗ , where for R ∈ R\nτ2(X , R) = dist2(RPX ,PX )/n = min A∈RS+\n1 (m)\nB∈S+ 1 (m)\n1 n ‖X (A)−X (B)‖22.\nIt follows from\nτ2(X , R) = min A∈RS+\n1 (m)B∈S+ 1 (m)\n1 n ‖X (A)−X (B)‖22\n≤ min A∈S+\n1 (m)\n1 n ‖X (R · A)−X (A)‖22\n= (R − 1)2 min A∈S+\n1 (m)\n1 n ‖X (A)‖22 = (R − 1)2τ20 (X )\n(15)\nthat Condition 1 is in fact stronger than (14). Below, we provide a sufficient condition on X that implies Condition 1. Proposition 3. Suppose that there exists a ∈ Rn, ‖a‖2 ≤ 1, and constants 0 < φmin ≤ φmax such that\nλmin(n −1/2X ∗(a)) ≥ φmin, and λmax(n−1/2X ∗(a)) ≤ φmax.\nThen for any ζ > 1, X satisfies Condition 1 with R∗ = ζ φmaxφmin and τ 2 ∗ = (ζ − 1)2φ2max.\nThe condition of Proposition 3 can be phrased as having a positive definite matrix in the unit ball of the range of X ∗, which, after scaling by 1/√n, has its smallest eigenvalues bounded away from zero and condition number bounded from above. As a simple example, suppose that X1 = √ nI. Invoking Proposition 3 with a = (1, 0, . . . , 0)⊤ and ζ = 2, we find that Condition 1 is satisfied with R∗ = 2 and τ 2 ∗ = 1. A more interesting example is random design where the {Xi}ni=1 are (sample) covariance matrices, where the underlying random vectors satisfy appropriate tail or moment conditions.\nCorollary 1. Let πm be a probability distribution on R m with second moment matrix Γ := Ez∼πm [zz ⊤] satisfying λmin(Γ) > 0. Consider the random matrix ensemble\nM(πm, q) = { 1\nq\nq∑\nk=1\nzkz ⊤ k , {zk}qk=1 i.i.d.∼ πm } . (16)\nSuppose that {Xi}ni=1 i.i.d.∼ M(πm, q) and let Γ̂n := 1n ∑n i=1 Xi and 0 < ǫn < λmin(Γ). Under the event {‖Γ− Γ̂n‖∞ ≤ ǫn}, X satisfies Condition 1 with\nR∗ = 2(λmax(Γ) + ǫn)\nλmin(Γ)− ǫn and τ2∗ = (λmax(Γ) + ǫn) 2.\nIt is instructive to spell out Corollary 1 with πm as the standard Gaussian distribution on Rm. The matrix Γ̂n equals the sample covariance matrix computed from N = n · q samples. It is well-known (see e.g. [11]) that for m,N large, λmax(Γ̂n) and λmin(Γ̂n) concentrate sharply around (1+ ηn) 2 and (1− ηn)2, respectively, where ηn = √ m/N . Hence, for any γ > 0, there exists Cγ > 1 so that if N ≥ Cγm, it holds that R∗ ≤ 2+γ. Similar though weaker concentration results for ‖Γ− Γ̂n‖∞ are available for the broad class of distributions πm having finite fourth moments [30]. When specialized to q = 1, Corollary 1 yields a statement about X made up from random rank-one measurements Xi = zz\n⊤, i = 1, . . . , n, cf. (3). The preceding discussion indicates that Condition 1 tends to be satisfied in this case.\nMain result of this subsection. We are now in position to state the following theorem.\nTheorem 1. Suppose that model (6) holds with X satisfying Condition 1 with constants R∗ and τ 2 ∗ . We then have\n1 n ‖X (Σ∗)−X (Σ̂)‖22 ≤ max\n{ 2(1 +R∗)λ0‖Σ∗‖1, 2λ0‖Σ∗‖1 + 8 ( λ0\nR∗ τ∗\n)2}\nwhere, for any µ ≥ 0, with probability at least 1− (2m)−µ\nλ0 ≤ σ √ (1 + µ)2 log(2m)\nV 2n n , where V 2n = ∥∥∥∥∥ 1 n n∑\ni=1\nX2i ∥∥∥∥∥ ∞ .\nRemarks.\n1. Under the scalings R∗ = O(1) and τ 2 ∗ = Ω(1), the bound of Theorem 1 is of the\norder O(λ0‖Σ∗‖1 + λ20) as announced in (13) at the beginning of this section.\n2. For given X , the quantity τ2(X , R) can be evaluated by solving a least squares problem with spd constraints. Hence it is feasible to check in practice whether Condition 1 holds. In fact, the bound of Theorem 1 can be replaced with\nmin R>1 max\n{ 2(1 +R)λ0‖Σ∗‖1, 2λ0‖Σ∗‖1 + 8 ( λ0 R\nτ(X , R)\n)2} .\n3. For later reference, it is of interest to evaluate the term V 2n for M(πm, q) with πm as the standard Gaussian distribution. It is proved in Appendix F that with high probability, it holds that\nV 2n ≤ ( 1 + q−1/2 + √ m/(nq) )2 ( 1 + √ m/q + √ 4(m/q) logn )2 = O(m log n)\nas long as m = O(nq)."
    }, {
      "heading" : "2.3 Bound on the estimation error",
      "text" : "In the previous subsection, we did not make any assumptions about Σ∗ apart from Σ∗ ∈ Sm+ . Henceforth, we suppose that Σ∗ is of low rank 1 ≤ r ≪ m and study\nthe performance of the constrained least squares estimator (7) for prediction and estimation in such setting.\nPreliminaries. Let Σ∗ = UΛU⊤ be the eigenvalue decomposition of Σ∗, where\nU =\n[ U‖ U⊥\nm× r m× (m− r)\n] [ Λr 0r×(m−r)\n0(m−r)×r 0(m−r)×(m−r)\n]\nwhere Λr is diagonal with positive diagonal entries. Consider the linear subspace\nT ⊥ = {M ∈ Sm : M = U⊥AU⊤⊥ , A ∈ Sm−r}.\nFrom U⊤⊥Σ ∗U⊥ = 0, it follows that Σ ∗ is contained in the orthogonal complement\nT = {M ∈ Sm : M = U‖B +B⊤U⊤‖ , B ∈ Rr×m},\nwhich has dimension mr − r(r − 1)/2 ≪ δm if r ≪ m. The image of T under X is denoted by T = {z ∈ Rn : z = X (M), M ∈ T}.\nConditions on X . We now introduce the key quantities the bound in this subsection depends on. Separability constant.\nτ2(T) = 1\nn dist2 (T ,PX ) , PX := {z ∈ Rn : z = X (∆), ∆ ∈ T⊥ ∩ S+1 (m)}\n= min Θ∈T, Λ∈S+\n1 (m)∩T⊥\n1 n ‖X (Θ)−X (Λ)‖22\nRestricted eigenvalue.\nφ2(T) = min 06=∆∈T ‖X (∆)‖22/n ‖∆‖21 .\nAs indicated by the following statement concerning the noiseless case, for bounding ‖Σ̂− Σ∗‖, it is inevitable to have lower bounds on the above two quantities.\nProposition 4. Consider the trace regression model (1) with εi = 0, i = 1, . . . , n. Then\nargmin Σ∈Sm\n+\n1\n2n ‖X (Σ∗)−X (Σ)‖22 = {Σ∗} for all Σ∗ ∈ T ∩ Sm+\nif and only if it holds that τ2(T) > 0 and φ2(T) > 0.\nCorrelation constant. Moreover, we make use of the following the quantity. It is not yet clear to us whether control of this quantity is intrinsically required, or whether its appearance in our bound is for merely technical reasons.\nµ(T) = max\n{ 1\nn 〈X (∆),X (∆′)〉 : ‖∆‖1 ≤ 1,∆ ∈ T, ∆′ ∈ S+1 (m) ∩ T⊥\n} .\nWe are now in position to provide a bound on ‖Σ̂− Σ∗‖1.\nTheorem 2. Suppose that model (6) holds with Σ∗ as considered throughout this subsection and let λ0 be defined as in Theorem 1. We then have\n‖Σ̂− Σ∗‖1 ≤ max { 8λ0\nµ(T)\nτ2(T)φ2(T)\n( 3\n2 +\nµ(T)\nφ2(T)\n) + 4λ0 ( 1\nφ2(T) +\n1\nτ2(T)\n) ,\n8λ0 φ2(T)\n( 1 + µ(T)\nφ2(T)\n) ,\n8λ0 τ2(T)\n} .\nRemark. Given the above bound on ‖Σ̂ − Σ∗‖1, it is possible to obtain an improved bound on the prediction error scaling with λ20 in place of λ0, cf. (31) in Appendix E.\nThe quality of the bound of Theorem 2 depends on how the quantities τ2(T), φ2(T) and µ(T) scale with n, m and r, which is highly design-dependent. Accordingly, the estimation error in nuclear norm can be non-finite in the worst case and O(λ0r) in the best case.\n• The quantity τ2(T) is specific to the geometry of the constrained least squares problem (7) and hence of critical importance. For instance, it follows from Proposition 2 that for standard Gaussian measurements, τ2(T) = 0 with high probability once n < 12δm. The situation can be much better for random spd\nmeasurements (16) as exemplified for measurements Xi = ziz ⊤ i with zi i.i.d.∼ N(0, I) in the subsequent section. Specifically, it turns out that τ2(T) = Ω(1/r) as long as n = Ω(m · r).\n• It is not restrictive to assume that the quantity φ2(T) is positive. Indeed, without that assumption, even an oracle estimator based on knowledge of the subspace T would fail. Reasonable sampling operators X have rank min{n, δm} so that the nullspace of X only has a trivial intersection with the subspace T as long as n ≥ dim(T) = mr − r(r − 1)/2.\n• For fixed T, computing µ(T) entails solving a biconvex (albeit non-convex) optimization problem in the variables ∆ ∈ T and ∆′ ∈ S+1 (m) ∩ T⊥. Alternating optimization (also known as block coordinate descent) is a practical approach to such optimization problems for which a globally optimal solution is out of reach. In this manner we explore the scaling of µ(T) numerically as done for τ2(T). We find that µ(T) = O(δm/n) so that µ(T) = O(1) apart from the regime n/δm → 0, without ruling out the possibility of undersampling, i.e. n < δm."
    }, {
      "heading" : "3 Numerical results",
      "text" : "In this section, we provide a series of empirical results regarding properties of the estimator Σ̂. In particular, its performance relative to regularization-based methods is explored. We also present an application to spiked covariance estimation for the CBCL face image data set and stock prices from NASDAQ.\n3.1 Scaling of the constant τ 2(T)\nFor X and T given, it is possible to evaluate τ2(T) by solving a convex optimization problem. This is different from other conditions employed in the literature such as restricted strong convexity [21], 1-RIP [10] or restricted uniform boundedness [5] that involve a non-convex optimization problem even for fixed T.\nWe here consider sampling operators with random i.i.d. measurements Xi = ziz ⊤ i , where zi ∼ N(0, I) is a standard Gaussian random vector in Rm (equivalently, Xi follows a Wishart distribution) , i = 1, . . . , n. We expect τ2(T) to behave similarly for random rank-one measurements of the same form as long as the underlying probability distribution has finite fourth moments, and thus for (a broad subclass of) the ensemble M(πm, q) (16).\nIn order to explore the scaling of τ2(T) with n, m and r, we fix m ∈ {30, 50, 70, 100}. For each choice of m, we vary n = αδm, where a grid of 20 values ranging from 0.16 to 1.1 is considered α. For r, we consider the grid {1, 2, . . . ,m/5}. For each combination\nof m, n, and r, we use 50 replications. Within each replication, the subspace T is generated randomly from the eigenspace associated with the non-zero eigenvalues of a random matrix G⊤G, where the entries of the m× r matrix G are i.i.d. N(0, 1). The results point to the existence of a phase transition as it is typical for problems related to that under study [2]. Specifically, it turns out that the scaling of τ2(T) can be well described by the relation\nτ2(T) ≈ φm,n max{1/r − θm,n, 0}, (17)\nwhere φm,n, θm,n > 0 depend on m and n. In order to arrive at model (17), we first obtain the 5%-quantile as summary statistic of the 50 replications associated with each triple (n,m, r). At this point, note that the use of the mean as a summary statistic is not appropriate as it may mask the fact that the majority of the observations are zero. For each pair of (n,m), we then identify all values of r for which the corresponding 5%-quantile drops below 10−6, which serves as effective zero here. For the remaining values, we fit model (17) using nonlinear least squares (working on a log scale). Figure 1 shows that model (17) provides a rather accurate description of the given data. Concerning φm,n and θm,n, the scalings φm,n = φ0n/m and θm,n = θ0m/n for constants φ0, θ0 > 0 appear to be reasonable. This gives rise to the requirement n > θ0(mr) for exact recovery to be possible in the noiseless case (cf. Proposition 4) and yields that τ2(T) = Ω(1/r) as long as n = Ω(mr),"
    }, {
      "heading" : "3.2 Comparison with regularization-based approaches",
      "text" : "In this subsection, we empirically evaluate ‖Σ̂−Σ∗‖1 relative to regularization-based methods proposed in the literature.\nSetup. We consider Wishart measurement matrices as in the previous subsection. Again, we expect a similar behaviour for (most) other random designs from ensemble M(πm, q). We fix m = 50 and let n ∈ {0.24, 0.26, . . . , 0.36, 0.4, . . . , 0.56} · m2 and r ∈ {1, 2, . . . , 10} vary. For each configuration of n and r, we consider 50 replications. In each of these replications, we generate data\nyi = tr(XiΣ ∗) + σεi, σ = 0.1, i = 1, . . . , n, (18)\nwhere Σ∗ is generated as the sum of rWishart matrices and the {εi}ni=1 are i.i.d.N(0, 1).\nRegularization-based approaches. We compare Σ̂ to the corresponding nuclear norm regularized estimator in (11). Regarding the choice of the regularization parameter λ, we consider the grid λ∗·{0.01, 0.05, 0.1, 0.3, 0.5, 1, 2, 4, 8, 16}, where λ∗ = σ √ m/n as recommended in [21] and pick λ so that the prediction error on a separate validation data set of size n generated from (18) is minimized. Note that in general, neither σ is known nor an extra validation data set is available. Our goal here is to ensure that the regularization parameter is properly tuned. In addition, we consider an oracular choice of λ where λ is picked from the above grid such that the performance measure of interest (the distance to the target in the nuclear norm) is minimized. We also compare to the constrained nuclear norm minimization approach of Chen et al. [10]\ngiven by min Σ tr(Σ) subject to Σ 0, and ‖y −X (Σ)‖1 ≤ λ. (19) For the parameter λ, we consider the grid nσ √\n2/π ·{0.2, 0.3, . . . , 1, 1.25}. This specific choice is motivated by the observation that E[‖y − X (Σ∗)‖1] = E[‖ε‖1] = nσ √ 2/π. Apart from that, tuning of λ is performed as for the nuclear norm regularized estimator. In addition, we have assessed the performance of the approach in [5], which does not impose an spd constraint but adds one more constraint to the formulation (19). That additional constraint significantly complicates optimization of the problem and yields a second tuning parameter. Therefore, instead of doing a grid search over a 2Dgrid, we use fixed values as specified in [5] given the knowledge of σ. The results are similar or worse than those of (19) (note in particular that positive semidefiniteness is not taken advantage of in the approach of [5]) and are hence not reported here.\nDiscussion of the results. We can conclude from Figure 2 that in most cases, the performance of the constrained least squares estimator does not differ much from that of the regularization-based methods with careful parameter tuning, which are not too far from the oracle. However, for larger values of r, the constrained least squares estimator seems to require slightly more measurements to achieve competitive performance."
    }, {
      "heading" : "3.3 Real data examples",
      "text" : "We conclude this section by presenting an application to recovery of spiked covariance matrices, a notion due to [16]. Background. A spiked covariance matrix is of the form Σ∗ = ∑r\nj=1 λjuju ⊤ j + σ 2I,\nwhere r ≪ m and λj ≫ σ2 > 0, j = 1, . . . , r. Note that for data {zi}ni=1 following the factor model\nzi = r∑\nj=1\nαijfj + σξi, ξi ∼ N(0, I), (20)\nfor orthogonal factors {fj}rj=1 and random coefficients αij ∼ N(0, λj) independent from ξi, the population covariance matrix E[ziz ⊤ i ], i = 1, . . . , n, is of the form given above. Model (20) is one possible way to motivate principal components analysis (PCA); this connection explains the relevance and the popularity of spiked covariance models.\nExtension to the spiked case. So far, we have assumed that the target Σ∗ is of low rank, but it is straightforward to extend the proposed approach to the case in which Σ∗ is spiked as long as σ2 is known or an estimate is available. A constrained least squares estimator of Σ∗ takes the form Σ̂ + σ2I, where\nΣ̂ ∈ argmin Σ∈Sm\n+\n1\n2n ‖y −X (Σ + σ2I)‖22. (21)\nData sets. (1) The CBCL facial image data set [1] consist of N = 2429 images of 19 × 19 pixels (i.e., m = 361). We take Σ∗ as the sample covariance matrix of this data set. It turns out that Σ∗ can be well approximated by Σr, r = 50, where Σr is the best rank r approximation to Σ∗ obtained from computing its eigendecomposition and setting to zero all but the top r eigenvalues. (2) We construct a second data set from the daily end prices of m = 252 stocks from the technology sector in NASDAQ, starting from the beginning of the year 2000 to the end of the year 2014 (in total N = 3773 days, retrieved from finance.yahoo.com). We take Σ∗ as the resulting sampling correlation matrix and choose r = 100.\nExperimental setup. As in all preceding measurements, we consider n random Wishart measurements for the operator X , where n = C(mr), where C ranges from\n0.25 to 12. Since ‖Σr−Σ∗‖F /‖Σ∗‖F ≈ 10−3 for both data sets, we work with σ2 = 0 in (21) for simplicity. To make the problem of recovering Σ∗ more difficult, we introduce additional noise to the problem by using observations\nyi = tr(XiSi), i = 1, . . . , n, (22)\nwhere Si is an approximation to Σ ∗ obtained from the sample covariance respectively sample correlation matrix of βN data points randomly sampled with replacement from the entire data set, i = 1, . . . , n, where β ranges from 0.4 to 1/N (Si is computed from a single data point). For each choice of n and β, 20 replications are considered. The reported results are averages over these replications.\nResults. For the CBCL data set, it can be seen from Figure 3 and Table 1, that Σ̂ accurately approximates Σ∗ (within a factor of three of the best rank-r approximation Σr) once the number of measurements crosses 2mr. Performance degrades once additional noise is introduced to the problem by using measurements (22) that are taken from a perturbed version of Σ∗. Even under significant perturbations (β = 0.08), reasonable reconstruction of Σ∗ remains possible, albeit the number of required measurements increases accordingly. In the extreme case β = 1/N , the error is still decreasing with n, but millions of samples seems to be required to achieve reasonable reconstruction error (for computational reasons, we stop at n = 12mr ≈ 216, 000). The general picture is similar for the NASDAQ data set, but the difference between using measurements based on the full sample correlation matrix on the one hand and approximations based on random subsampling (22) on the other hand are more pronounced. For β = 1, the reduction in error with increasing n progresses visibly faster as for the first data set, and a smaller error relative to Σr close to 1 is achieved."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we have investigated trace regression in the situation that the underlying matrix is symmetric positive semidefinite. We have shown that under certain restrictions on the design, the constrained least squares estimator enjoys excellent statistical properties similar to methods employing nuclear norm regularization. This may come as a surprise, as regularization is widely regarded as necessary in small sample settings. On the application side, we have pointed out the usefulness of our findings for recovering spiked covariance matrices from quadratic measurements."
    }, {
      "heading" : "Acknowledgement",
      "text" : "The work of Martin Slawski and Ping Li is partially supported by NSF-DMS-1444124, NSF-III-1360971, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137."
    }, {
      "heading" : "A Proof of Proposition 1",
      "text" : "By rotational invariance of the Gaussian distribution of ε, it suffices to consider the canonical orthonormal basis of Sm given by\nX1 = e1e ⊤ 1 , X2 = 1√ 2 (e1e ⊤ 2 + e2e ⊤ 1 ), . . . , Xm = 1√ 2 (e1e ⊤ m + eme ⊤ 1 ), Xm+1 = e2e ⊤ 2 , Xm+2 = 1√ 2 (e2e ⊤ 3 + e3e ⊤ 2 ), . . . , Xδm−1 = 1√ 2 (em−1e ⊤ m + eme ⊤ m−1), Xδm = eme ⊤ m,\nwhere {ej}mj=1 denote the canonical basis vectors of Rm. Equivalently, the corresponding map X : Sm → Rδm equals the symmetric vectorization operator\nΣ = (σjk) 7→ (σ11, √ 2σ12, . . . , √ 2σ1m, σ22, √ 2σ23, . . . , √ 2σ(m−1)m, σmm) ⊤ (23)\nAccordingly, denote by {εjk}1≤j≤k≤m the error terms corresponding to the entries {σjk}1≤j≤k≤m. The minimization problem (7) can hence be expressed as\nmin Σ∈Sm\n+\n1\n2n    m∑\nj=1\n(εjj − σjj)2 + ∑\nj<k\n(εjk − √ 2σjk) 2   \n= min Σ∈Sm\n+\n1\n2n    m∑\nj=1\n(εjj − σjj)2 + 2 ∑\nj<k\n( εjk√ 2 − σjk )2   \n= min Σ∈Sm\n+\n‖E − Σ‖2F , (24)\nwhere the matrix E = X ∗(ε) has entries Ejj = εjj , j = 1, . . . ,m, and Ejk = εjk/ √ 2, j, k = 1, . . . ,m, j 6= k. Now observe that the minimizer Σ̂ of (24) coincides with the Euclidean projection of E on Sm+ . It is well-known [3] that the projection of a symmetric matrix on the positive semidefinite cone is obtained by setting all its negative eigenvalues to zero, i.e., in terms of the eigendecomposition of E = ∑p j=1 λj(E)u ⊤ j u ⊤ j , we have\nΣ̂ =\nm∑\nj=1\nmax{λj(E), 0}uju⊤j .\nAt this point, we note that E is a Wigner matrix, whose empirical distribution of its eigenvalues follows Wigner’s semicircle law as m → ∞ (cf. [27]), which is symmetric\naround zero. Consequently, we have\n‖X (Σ̂)‖22 = ‖Σ̂‖2F = m∑\nj=1\n{λj(E), 0}2 → 1\n2 ‖E‖2F →\nσ2\n2 δm in probability as m → ∞."
    }, {
      "heading" : "B Proof of Proposition 2",
      "text" : "The proof of Proposition 2 follows from results in [2].\nDefinition B.1. Let C ⊆ Rd be a convex cone. The statistical dimension of C is defined as δ(C) = E[‖ΠCg‖22], where ΠC denotes the Euclidean projection onto C and the entries of g are i.i.d. N(0, 1).\nTheorem B.1. [2] Let f : Rd → R∪{−∞,+∞} be a proper convex function. Suppose that A ∈ Rn×d has i.i.d. N(0, 1) entries, and let z0 = Ax0 for a fixed x0 ∈ Rd. Consider the convex optimization problem\nminimize f(x) subject to Ax = z0. (25)\nand let D(f, x0) = ⋃\nt>0{v ∈ Rd : f(x0 + tv) ≤ f(x0)} denote the descent cone of f at x0. Then, for any ε > 0, if n ≤ (1 − ε)δ(D(f, x0)), with probability at least 1− 32 exp(−ε2δm), x0 fails to be the unique solution of (25).\nProof. (Proposition 2). Denote by svec : Sm → Rδm the symmetric vectorization map (cf. (23)), which is an isometry with respect to the Euclidean inner product on Sm and Rδm , and by svec−1 : Rδm → Sm its inverse. We can then apply Theorem B.1 to the setting of Proposition 2 by using\nd = δm, x = svec(Σ), x0 = 0, f(x) = ιSm + (svec−1(x)), A =\n  svec(X1)\n... svec(Xn)\n  ,\nwhere ιSm + is the convex indicator function of Sm+ which takes the value 0 if its argument is contained in Sm+ and +∞ otherwise. Observe that D(f, 0) = Sm+ . It is shown in [2], Proposition 3.2, that the statistical dimension δ(Sm+ ) = δm/2. This concludes the proof."
    }, {
      "heading" : "C Proof of Proposition 3",
      "text" : "Proposition 3 follows from the dual problem of the convex optimization problem associated with τ2(X , R). Below, it will be shown that the Lagrangian dual of the optimization problem\nmin A,B\n1\nn1/2 ‖X (A)−X (B)‖2\nsubject to A 0, B 0, tr(A) = R, tr(B) = 1. (26)\nis given by\nmax θ,δ,a\nθ · R− δ\nsubject to X ∗(a)√ n θI, X ∗(a)√ n δI, ‖a‖2 ≤ 1. (27)\nThe assertion of Proposition 3 follows immediately from (27) by identifying θ = λmin(n\n−1/2X ∗(a)) and δ = λmax(n−1/2X ∗(a)). In the remainder of the proof, duality of (26) and (27) is established. Using the shortcut X̃ = X/√n, the Lagrangian of the dual problem (27) is given by\nL(θ, δ, a;A,B, κ) = θ ·R− δ + 〈 X̃ ∗(a)− θI, A 〉 − 〈 X̃ ∗(a)− δI, B 〉 − κ(‖a‖22 − 1).\nTaking derivatives w.r.t. θ, δ, r and the setting the result equal to zero, we obtain from the KKT conditions that a primal-dual optimal pair (θ̂, δ̂, â, Â, B̂, κ̂) obeys\ntr(Â) = R, tr(B̂) = 1, X̃ (Â)− X̃ (B̂)− κ̂2â = 0. (28)\nTaking the inner product of the rightmost equation with â, we obtain\n〈 â, X̃ (Â)− X̃ (B̂) 〉 − κ̂2‖â‖22 = 0.\n⇔ 〈 X̃ ∗(â), Â− B̂ 〉 − κ̂2‖â‖22 = 0.\n⇔ θ̂ tr(Â)− δ̂ tr(B̂)− κ̂2‖â‖22 = 0. ⇔ θ̂R− δ̂ = κ̂2‖â‖22,\nwhere the second equivalence is by complementary slackness. Consider first the case θ̂R− δ̂ > 0. This entails κ̂ > 0 and thus ‖â‖22 = 1, so that 2κ̂ = θ̂R− δ̂. Substituting this result into the rightmost equation in (28) and taking norms, we obtain\nθ̂R− δ̂ = ‖X̃ (Â)− X̃ (B̂)‖2 = 1√ n ‖X (Â)−X (B̂)‖2. (29)\nFor the second case, note that θ̂R− δ̂ cannot be negative as a = 0 is feasible for (27). Thus, θ̂R − δ̂ = 0 implies that â = 0 and in turn also (29)."
    }, {
      "heading" : "D Proof of Corollary 1",
      "text" : "The corollary follows from Proposition 3 by choosing a = 1/ √ n so that n−1/2X ∗(a) = 1 n ∑n i=1 Xi, and using that ‖Γ − Γ̂n‖∞ ≤ ǫn implies that |λj(Γ) − λj(Γ̂n)| ≤ ǫn, j = 1, . . . ,m ([15], §4.3). The specific values of R∗ and τ2∗ are obtained by choosing ζ = 2 in Proposition 3."
    }, {
      "heading" : "E Proof of Theorem 1",
      "text" : "The following lemma is a crucial ingredient in the proof. In the sequel, let ∆̂ = Σ̂−Σ∗. Let the eigendecomposition of ∆̂ be given by\n∆̂ =\nm∑\nj=1\nλj(∆̂)uju ⊤ j =\nm∑\nj=1 max{0, λj(∆̂)}uju⊤j ︸ ︷︷ ︸\n=:∆̂+\n+\nm∑\nj=1 min{0, λj(∆̂)}uju⊤j ︸ ︷︷ ︸\n=:∆̂−\n= ∆̂+ + ∆̂−\n(30)\nLemma E.1. Consider the decomposition (30). We have ‖∆̂−‖1 ≤ ‖Σ∗‖1.\nProof. Write ∆̂+ = U+Λ+U ⊤ + and ∆̂ − = U−Λ−U ⊤ − for the eigendecompositions of ∆̂ +\nand ∆̂−, respectively. Since Σ̂ 0, we must have tr(Σ̂U−U⊤− ) ≥ 0 and thus\n0 ≤ tr(Σ̂U−U⊤− ) = tr(U⊤− Σ̂U−) = tr(U⊤− (Σ ∗ + ∆̂)U−)\n= tr(U⊤− (Σ ∗ + U+Λ+U ⊤ + + U−Λ−U ⊤ − )U−) = tr(Σ∗U−U ⊤ − ) + tr(Λ−),\nwhere for the last identity, we have used that U⊤+U− = 0. It follows that\n‖∆̂−‖1 = ‖Λ−‖1 = − tr(Λ−) ≤ tr(Σ∗U−U⊤− ) ≤ ‖Σ∗‖1‖U−U⊤− ‖∞ = ‖Σ∗‖1.\nEquipped with Lemma E.1, we turn to the proof of Theorem 1.\nProof. (Theorem 1) By definition of Σ̂, we have ‖y −X (Σ̂)‖22 ≤ ‖y −X (Σ∗)‖22. Using (6) and the definition of ∆̂, we obtain after re-arranging terms that\n1 n ‖X (∆̂)‖22 ≤ 2 n\n〈 ε,X (∆̂) 〉 = 2\nn\n〈 X ∗(ε), ∆̂ 〉\n⇒ 1 n ‖X (∆̂)‖22 ≤ 2‖X ∗(ε)/n‖∞‖∆̂‖1 = 2λ0(‖∆̂+‖1 + ‖∆̂−‖1), (31)\nwhere we have used Hölder’s inequality, the decomposition of ∆̂ as in Lemma E.1 and λ0 = ‖X ∗(ε)/n‖∞. We now upper bound the l.h.s. of (31) by invoking Condition 1 and Lemma E.1, which yields ‖∆̂−‖1 ≤ ‖Σ∗‖1. If ‖∆̂+‖1 ≤ R∗‖∆̂−‖1, we have\n1 n ‖X (Σ̂)−X (Σ∗)‖22 = 1 n ‖X (∆̂)‖22 ≤ 2(R∗ + 1)λ0‖Σ∗‖1,\nwhich is the first part in the maximum of the bound to be established. In the opposite case, suppose first that ‖∆̂−‖1 > 0 (the case ‖∆̂−‖1 = 0 is discussed at the end of this proof) and we have ‖∆̂+‖1/‖∆̂−‖1 = R̂ > R∗ > 1. Consequently,\n1 n ‖X (∆̂)‖22 = 1 n ‖X (∆̂+)−X (−∆̂−)‖22\n= ‖∆̂−‖21 1\nn ∥∥∥∥∥X ( ∆̂+\n‖∆̂−‖1\n) −X ( −∆̂−\n‖∆̂−‖1\n)∥∥∥∥∥ 2\n2\n≥ ‖∆̂−‖21 min A∈R̂S+\n1 (m)\nB∈S+ 1 (m)\n1 n ‖X (A) −X (B)‖22\n= τ2(X , R̂)‖∆̂−‖21 = τ2(X , R̂) ‖∆̂+‖21 R̂2\nInserting this into (31), we obtain the following upper bound on ‖∆̂+‖1.\nτ2(X , R̂) R̂2 ‖∆+‖21 ≤ 2λ0 R̂ + 1 R̂ ‖∆̂+‖1\n⇒ ‖∆̂+‖1 ≤ 2λ0 R̂(R̂+ 1)\nτ2(X , R̂) ≤ 4λ0\nR̂2\nτ2(X , R̂) ≤ 4λ0 R2∗ τ2∗ ,\nwhere the last inequality follows from the observation that for any R ≥ R∗\nτ2(X , R) ≥ (R/R∗)2τ2(X , R∗),\nwhich can be easily seen from the dual problem (27) associated with τ2(X , R). Substituting the above bound on ‖∆̂+‖1 into (31) and using the bound ‖∆̂−‖1 ≤ ‖Σ∗‖1 yields the second part in the maximum of the desired bound. To finish the proof, we still need to address the case ‖∆̂−‖1 = 0. Recalling the definition of the quantity τ20 (X ) in (14), we bound\n1 n ‖X̂(∆̂)‖22 = 1 n ‖X̂(∆̂+)‖22 ≥ τ20 (X )‖∆̂+‖21.\nInserting this into (31), we obtain from (15)\n‖∆̂+‖1 ≤ 2λ0 τ20 (X ) ≤ 2λ0(R∗ − 1) 2 τ2∗ .\nBack-substitution into (31) yields a bound that is implied by that of Theorem 1. This concludes the proof.\nBound on λ0. The bound on λ0 is an application of Theorem 4.6.1 in [29]. Theorem E.1. [29] Consider a sequence {Xi}ni=1 of fixed matrices in Sm and let {εi}ni=1 i.i.d.∼ N(0, σ2). Then for all t ≥ 0\nP (∥∥∥∥∥ n∑\ni=1\nεiXi ∥∥∥∥∥ ∞ ≥ t ) ≤ 2m exp(−t2/(2σ2V 2)), V 2 := ∥∥∥∥∥ n∑ i=1 X2i ∥∥∥∥∥ ∞ .\nChoosing t = σV √ (1 + µ)2 log(2m) yields the desired bound."
    }, {
      "heading" : "F Proof of Theorem 1, Remark 3",
      "text" : "The bound hinges on the following concentration result for the extreme eigenvalues of the sample covariance of a Gaussian sample.\nTheorem F.1. [11] Let z1, . . . , zN be an i.i.d. sample from N(0, Im) and let ΓN = 1 N ∑N i=1 ziz ⊤ i . We then have for any δ > 0\nP ( λmax ( 1\nN ΓN\n) > ( 1 + δ + √ m\nN\n)2) ≤ exp(−Nδ2/2).\nIn the proof, we also make use of the following fact.\nLemma F.1. Let {Xi}ni=1 ⊂ Sm+ . Then∥∥∥∥∥ n∑\ni=1\nX2i ∥∥∥∥∥ ∞ ≤ max 1≤i≤n ‖Xi‖∞ ∥∥∥∥∥ n∑ i=1 Xi ∥∥∥∥∥ ∞ .\nProof. First note that for any v ∈ Rm and any M ∈ Sm+ , we have that\nv⊤M2v =\nm∑\nj=1\nλ2j(M)(u ⊤ j v) 2 ≤ λmax(M) m∑\nj=1\nλj(M)(u ⊤ j v) 2 = ‖M‖∞v⊤Xv,\nwhere {uj}mj=1 are the eigenvectors of X . Accordingly, we have ∥∥∥∥∥ n∑\ni=1\nX2i ∥∥∥∥∥ ∞ = max ‖v‖2=1 v⊤ n∑ i=1 X2i v ≤ max 1≤i≤n ‖Xi‖∞ max ‖v‖2=1 v⊤ n∑ i=1 Xiv\n= max 1≤i≤n\n‖Xi‖∞ ∥∥∥∥∥ n∑\ni=1\nXi ∥∥∥∥∥ ∞ .\nWe now establish the bound to be shown. Each measurement matrix can be expanded as\nXi = 1\nq\nq∑\nk=1\nzikz ⊤ ik, {zik}qk=1 i.i.d.∼ N(0, Im), i = 1, . . . , n.\nAccordingly, we have\n∥∥∥∥∥ 1 n n∑\ni=1\nX2i ∥∥∥∥∥ ∞ = ∥∥∥∥∥∥ 1 n n∑ i=1 { 1 q q∑ k=1 zikz ⊤ ik }2∥∥∥∥∥∥ ∞\n≤ max 1≤i≤n {∥∥∥∥∥ { 1 q q∑\nk=1\nzikz ⊤ ik }∥∥∥∥∥ ∞ }∥∥∥∥∥ 1 nq n∑ i=1 q∑ k=1 zikz ⊤ ik ∥∥∥∥∥ ∞\n≤ max 1≤i≤n\n{ λmax ( 1\nq\nq∑\nk=1\nzikz ⊤ ik )} λmax(Γnq)\nwhere Γnq follows the distribution of ΓN in Theorem F.1 with N = nq. For the first\nterm, applying Theorem F.1 with N = q and δ = √ 4m log(n)/q and using the union bound, we obtain that\nP ( λmax ( 1\nq\nq∑\nk=1\nzikz ⊤ ik\n) > (√ q + √ m+ √ 4m logn\n√ q\n)2) ≤ exp(−(2m− 1) logn).\nApplying Theorem F.1 to ΓN with δ = 1/ √ q, we obtain that\nP ( λmax(Γnq) > ( 1 +\n1√ q +\n√ m\nnq\n)2) ≤ exp(−n/2).\nCombining the two previous bounds yields the assertion."
    }, {
      "heading" : "G Proof of Proposition 4",
      "text" : "In the sequel, we write ΠT and ΠT⊥ for the orthogonal projections on T and T ⊥, respectively. Note first that since the {εi}ni=1 are zero, any minimizer Σ̂ satisfies\nX (Σ̂) = X (Σ∗) ⇐⇒ X (∆̂) = 0 ⇐⇒ X (∆̂T) + X (∆̂T⊥) = 0 (32)\nwhere ∆̂T = ΠT∆̂ and ∆̂T⊥ = ΠT⊥∆̂, where we recall that ∆̂ = Σ̂ − Σ∗. Note that since Σ∗ = ΠTΣ ∗, for Σ̂ to be feasible, it is necessary that ∆̂T⊥ 0.\nSuppose first that τ2(T) = 0. Then there exist Θ ∈ T and Λ ∈ S+1 (m) ∩ T⊥ such that X (Θ) + X (Λ) = 0. Hence, for any Σ∗ ∈ T with Σ∗ + Θ 0, the choices ∆̂T = Θ and ∆̂T⊥ = Λ ensure that Σ̂ is feasible and that (32) is satisfied. Since Λ is contained in the Schatten 1-norm sphere of radius 1, it is necessarily non-zero and thus Σ̂ 6= Σ∗. If φ2(T) = 0, there exists 0 6= Θ ∈ T such that X (Θ) = 0. Consequently, for any Σ∗ ∈ T ∩ Sm+ with Σ̂ = Σ∗ +Θ 0, (32) is satisfied with Σ̂ 6= Σ∗.\nConversely, if τ2(T) > 0, (32) cannot be satisfied for ∆̂T⊥ 0, ∆̂T⊥ 6= 0. Otherwise, we could divide by tr(∆̂T⊥), which would yield\nX (∆̂T / tr(∆̂T⊥)︸ ︷︷ ︸ ∈T ) + X (∆̂T⊥ / tr(∆̂T⊥)︸ ︷︷ ︸\n∈S+ 1 (m)∩T⊥\n) = 0,\nwhich would imply τ2(T) = 0. Therefore, we must have ∆̂T⊥ = 0 and X (∆̂T) = 0, which implies ∆̂T = 0 as long as φ 2(T) > 0."
    }, {
      "heading" : "H Proof of Theorem 2",
      "text" : "Let ∆̂ = Σ̂ − Σ∗, ∆̂T = ΠT∆̂ and ∆̂T⊥ = ΠT⊥∆̂ 0 as in the preceding proof. We start with the following analog to (31)\n1 n ‖X (∆̂)‖22 = 1 n ‖X (∆̂T + ∆̂T⊥)‖22 ≤ 2λ0(‖∆̂T‖1 + ‖∆̂T⊥‖1) (33)\nSuppose that ∆̂T⊥ 6= 0. We then have\n‖∆̂T⊥‖21    1 n ∥∥∥∥∥X ( ∆̂T\n‖∆̂T⊥‖1\n) + X ( ∆̂T⊥\n‖∆̂T⊥‖1\n)∥∥∥∥∥ 2\n2\n   ≤ 2λ0(‖∆̂T‖1 + ‖∆̂T⊥‖1)\nSince ∆̂T/‖∆̂T⊥‖1 ∈ T and ∆̂T⊥/‖∆̂T⊥‖1 = ∆̂T⊥/ tr(∆̂T⊥) ∈ S+1 (m), we obtain that the term inside the curly brackets is lower bounded by τ2(T) and thus\n‖∆̂T⊥‖1 ≤ 2λ0 τ2(T)\n( 1 +\n‖∆̂T‖1 ‖∆̂T⊥‖1\n) (34)\nOn the other hand, expanding the quadratic term in (33), we obtain that\n1 n ‖X (∆̂T)‖22 − 2 n\n〈 X (∆̂T),X (∆̂T⊥ ) 〉 ≤ 1\nn ‖X (∆̂)‖22 ≤ 2λ0(‖∆̂T‖1 + ‖∆̂T⊥‖1)\n⇒ 1 n ‖X (∆̂T)‖22 ≤ 2λ0(‖∆̂T‖1 + ‖∆̂T⊥‖1) + 2µ(T)‖∆̂T‖1‖∆̂T⊥‖1 ⇒ φ2(T)‖∆̂T‖21 ≤ 2λ0(‖∆̂T‖1 + ‖∆̂T⊥‖1) + 2µ(T)‖∆̂T‖1‖∆̂T⊥‖1\n⇒ ‖∆̂T‖1 ≤ 2λ0\n( 1 + ‖∆̂T⊥‖1 / ‖∆̂T‖1 ) + 2µ(T)‖∆̂T⊥‖1\nφ2(T) (35)\nWe now distinguish several cases.\nCase 1: ‖∆̂T‖1 ≤ ‖∆̂T⊥‖1. It then immediately follows from (34) that\n‖∆̂‖1 ≤ 8λ0 τ2(T) =: T3. (36)\nCase 2a: ‖∆̂T‖1 > ‖∆̂T⊥‖1 and ‖∆̂T⊥‖1 ≤ 4λ0/φ2(T). From (35), we first get\n‖∆̂T‖1 ≤ 4λ0 + 2µ(T)‖∆̂T⊥‖1\nφ2(T) (37)\nand thus\n‖∆̂‖1 ≤ 8λ0 φ2(T)\n( 1 + µ(T)\nφ2(T)\n) =: T2 (38)\nCase 2b: ‖∆̂T‖1 > ‖∆̂T⊥‖1 and ‖∆̂T⊥‖1 > 4λ0/φ2(T). Plugging (37) into (34), we obtain that\n‖∆̂T⊥‖1 ≤ 4λ0 τ2(T) + 4λ0µ(T) τ2(T)φ2(T) .\nSubstituting this bound back into (37) yields\n‖∆̂T‖1 ≤ 4λ0 φ2(T) + 8λ0µ(T) τ2(T)φ2(T) +\n8λ0µ 2(T)\nφ4(T)τ2(T) .\nCollecting terms, we obtain altogether\n‖∆̂‖1 ≤ 8λ0 µ(T)\nτ2(T)φ2(T)\n( 3\n2 +\nµ(T)\nφ2(T)\n) + 4λ0 ( 1\nφ2(T) +\n1\nτ2(T)\n) =: T1. (39)\nCombining (36), (38) and (39) yields the assertion."
    } ],
    "references" : [ {
      "title" : "Living on the edge: phase transitions in convex programs with random data",
      "author" : [ "D. Amelunxen", "M. Lotz", "M. McCoy", "J. Tropp" ],
      "venue" : "Information and Inference,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "On the uniqueness of nonnegative sparse solutions to underdetermined systems of equations",
      "author" : [ "A. Bruckstein", "M. Elad", "M. Zibulevsky" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "ROP: Matrix recovery via rank-one projections",
      "author" : [ "T. Cai", "A. Zhang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Solving quadratic equations via PhaseLift when there are about as many equations as unknowns",
      "author" : [ "E. Candes", "X. Li" ],
      "venue" : "Foundation of Computational Mathematics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Tight oracle bounds for low-rank matrix recovery from a minimal number of noisy measurements",
      "author" : [ "E. Candes", "Y. Plan" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E. Candes", "B. Recht" ],
      "venue" : "Foundation of Computational Mathematics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "PhaseLift: exact and stable signal recovery from magnitude measurements via convex programming",
      "author" : [ "E. Candes", "T. Strohmer", "V. Voroninski" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Handbook of the Geometry of Banach Spaces, volume 1, chapter Local operator theory, random matrices and Banach spaces, pages 317–366",
      "author" : [ "K. Davidson", "S. Szarek" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2001
    }, {
      "title" : "Counting the faces of randomly-projected hypercubes and orthants, with applications",
      "author" : [ "D. Donoho", "J. Tanner" ],
      "venue" : "Discrete and Computational Geometry,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Quantum State Tomography via Compressed Sensing",
      "author" : [ "D. Gross", "Y.-K. Liu", "S. Flammia", "S. Becker", "J. Eisert" ],
      "venue" : "Physical Review Letters,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Matrix Analysis",
      "author" : [ "R. Horn", "C. Johnson" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1985
    }, {
      "title" : "On the distribution of the largest eigenvalue in principal components analysis",
      "author" : [ "I. Johnstone" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    }, {
      "title" : "The phase retrieval problem",
      "author" : [ "M. Klibanov", "P. Sacks", "A. Tikhonarov" ],
      "venue" : "Inverse Problems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1995
    }, {
      "title" : "Von Neumann entropy penalization and low-rank matrix estimation",
      "author" : [ "V. Koltchinskii" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion",
      "author" : [ "V. Koltchinskii", "K. Lounici", "A. Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Sign-constrained least squares estimation for high-dimensional regression",
      "author" : [ "N. Meinshausen" ],
      "venue" : "The Electronic Journal of Statistics,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Estimation of (near) low-rank matrices with noise and high-dimensional scaling",
      "author" : [ "S. Negahban", "M. Wainwright" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear normminimization",
      "author" : [ "B. Recht", "M. Fazel", "P. Parillo" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Estimation of high-dimensional low-rank matrices",
      "author" : [ "A. Rohde", "A. Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Learning with kernels",
      "author" : [ "B. Schölkopf", "A. Smola" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2002
    }, {
      "title" : "Non-negative least squares for high-dimensional linear models: consistency and sparse recovery without regularization",
      "author" : [ "M. Slawski", "M. Hein" ],
      "venue" : "The Electronic Journal of Statistics,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Maximum margin matrix factorization",
      "author" : [ "N. Srebro", "J. Rennie", "T. Jaakola" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2005
    }, {
      "title" : "Topics in Random Matrix Theory",
      "author" : [ "T. Tao" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Regression shrinkage and variable selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society Series B,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1996
    }, {
      "title" : "User-friendly tools for random matrices: An introduction",
      "author" : [ "J. Tropp" ],
      "venue" : "http://users.cms.caltech.edu/~jtropp/",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2014
    }, {
      "title" : "How close is the sample covariance matrix to the actual covariance matrix ",
      "author" : [ "R. Vershynin" ],
      "venue" : "Journal of Theoretical Probability,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2012
    }, {
      "title" : "Conditions for a Unique Non-negative Solution to an Underdetermined System",
      "author" : [ "M. Wang", "A. Tang" ],
      "venue" : "In Allerton Conference on Communication, Control, and Computing,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "A unique ’nonnegative’ solution to an underdetermined system: from vectors to matrices",
      "author" : [ "M. Wang", "W. Xu", "A. Tang" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2011
    }, {
      "title" : "Using the Nyström method to speed up kernel machines",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 10,
      "context" : "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : "Such setting is relevant, among others, to problems such as matrix completion [8, 26], compressed sensing [7, 21], quantum state tomography [14] and phase retrieval [9].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 18,
      "context" : "A common thread in these works is the use of the nuclear norm of a matrix as a convex surrogate for its rank [22] in regularized estimation amenable to modern optimization techniques.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 24,
      "context" : "This approach can be seen as natural generalization of l1-norm (aka lasso) regularization for the standard linear regression model [28] that arises as a special case of model (1) in which both Σ and the measurement matrices {Xi}i=1 are diagonal.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 20,
      "context" : "The set S+ deserves specific interest as it includes covariance matrices and Gram matrices in kernel-based learning methods [24].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 29,
      "context" : "It is rather common for these matrices to be of low rank (at least approximately), given the widespread use of principal components analysis and low-rank kernel approximations [33].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 16,
      "context" : "Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for high-dimensional linear regression with non-negative parameters [20, 25].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 21,
      "context" : "Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for high-dimensional linear regression with non-negative parameters [20, 25].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "In these papers it is shown that for certain design matrices, non-negative least squares can achieve comparable performance to l1-norm regularized estimation with regard to prediction, estimation and support recovery, thereby generalizing prior work [4, 13, 31] on sparse recovery of a non-negative vector in a noiseless setting.",
      "startOffset" : 250,
      "endOffset" : 261
    }, {
      "referenceID" : 9,
      "context" : "In these papers it is shown that for certain design matrices, non-negative least squares can achieve comparable performance to l1-norm regularized estimation with regard to prediction, estimation and support recovery, thereby generalizing prior work [4, 13, 31] on sparse recovery of a non-negative vector in a noiseless setting.",
      "startOffset" : 250,
      "endOffset" : 261
    }, {
      "referenceID" : 27,
      "context" : "In these papers it is shown that for certain design matrices, non-negative least squares can achieve comparable performance to l1-norm regularized estimation with regard to prediction, estimation and support recovery, thereby generalizing prior work [4, 13, 31] on sparse recovery of a non-negative vector in a noiseless setting.",
      "startOffset" : 250,
      "endOffset" : 261
    }, {
      "referenceID" : 28,
      "context" : "In [32], the problem of exactly recovering Σ being low-rank from noiseless observations (εi = 0, i = 1, .",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 28,
      "context" : "Apart from the fact that we primarily study a noisy setting, we shall argue below that in the setup of compressed sensing the measurement matrices studied in [32] constitute an unfavourable choice relative to those recommended in the present paper.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 3,
      "context" : "In [5], rank-one measurements are considered for general Σ ∈ R12 .",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "On the other hand, in [5], no specific attention is given to the spd constraint: the convex program proposed therein, which can be seen as a modification of the approach in [10], applies to general symmetric matrices and does not enforce positive semidefiniteness.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "yi = |xi σ| + εi (5) which (with complex-valued σ) is relevant to the problem of phase retrieval [17] that has received some attention recently.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 7,
      "context" : "The approach of [9] treats (5) as an instance of (1) and uses nuclear norm regularization to enforce rank-one solutions.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "In followup work [6], the authors show a refined recovery result stating that imposing an spd constraint − without regularization − suffices.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "However, the results in both [6] and [12] only concern model (5).",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "In [18], Σ is assumed to be a complex Hermitian positive semidefinite matrix of unit trace, which is the setting in quantum state tomography.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "While the setting as well as the measurement matrices under consideration are different from ours, a notable point of contact to our work can be seen in the fact that the negative von Neumann entropy, which is the proposed regularizer in [18], does not promote low rankedness, but constitutes one possible way of enforcing positive definiteness.",
      "startOffset" : 238,
      "endOffset" : 242
    }, {
      "referenceID" : 14,
      "context" : "At the same time, adaptivity of the approach to low rankedness is established in [18].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "[7, 19, 21, 22, 23].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 15,
      "context" : "[7, 19, 21, 22, 23].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 17,
      "context" : "[7, 19, 21, 22, 23].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 18,
      "context" : "[7, 19, 21, 22, 23].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 19,
      "context" : "[7, 19, 21, 22, 23].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "[7, 21].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "[7, 21].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "The following statement, which follows from results in [2], points to a serious limitation associated with the use of such measurements.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 28,
      "context" : "In [32], the following noiseless analog to the constrained least squares problem (7) is considered: find Σ ∈ S+ such thatX (Σ) = y = X (Σ), (12) where Xi ∼ GOE(m), i = 1, .",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "It is of interest to relate Proposition 2 to corresponding results on the vector case (equivalent to having diagonal {Xi}i=1 and diagonal Σ) in [13].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 9,
      "context" : "Compared to Proposition 2, the corresponding result in [13] applies to a much wider class of random measurement matrices including all random matrices with i.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 23,
      "context" : "It is thus natural to ask whether Proposition 2 holds more generally for all Wigner matrices [27].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "The fact that the threshold 12δm for the number measurements in Proposition 2 equals (up to the scaling factor σ) the asymptotic prediction error of Example 2 is not a coincidence; this is part of a wider phenomenon as pointed out in [2].",
      "startOffset" : 234,
      "endOffset" : 237
    }, {
      "referenceID" : 0,
      "context" : "In the framework of [2], 12δm is the “statistical dimension” of S m + .",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "Theorem 1 in [23].",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 21,
      "context" : "Theorem 1 in [25].",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "[11]) that for m,N large, λmax(Γ̂n) and λmin(Γ̂n) concentrate sharply around (1+ ηn) 2 and (1− ηn), respectively, where ηn = √ m/N .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "Similar though weaker concentration results for ‖Γ− Γ̂n‖∞ are available for the broad class of distributions πm having finite fourth moments [30].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "This is different from other conditions employed in the literature such as restricted strong convexity [21], 1-RIP [10] or restricted uniform boundedness [5] that involve a non-convex optimization problem even for fixed T.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "This is different from other conditions employed in the literature such as restricted strong convexity [21], 1-RIP [10] or restricted uniform boundedness [5] that involve a non-convex optimization problem even for fixed T.",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "The results point to the existence of a phase transition as it is typical for problems related to that under study [2].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "5, 1, 2, 4, 8, 16}, where λ∗ = σ √ m/n as recommended in [21] and pick λ so that the prediction error on a separate validation data set of size n generated from (18) is minimized.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "In addition, we have assessed the performance of the approach in [5], which does not impose an spd constraint but adds one more constraint to the formulation (19).",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "Therefore, instead of doing a grid search over a 2Dgrid, we use fixed values as specified in [5] given the knowledge of σ.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "The results are similar or worse than those of (19) (note in particular that positive semidefiniteness is not taken advantage of in the approach of [5]) and are hence not reported here.",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 12,
      "context" : "We conclude this section by presenting an application to recovery of spiked covariance matrices, a notion due to [16].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "It is well-known [3] that the projection of a symmetric matrix on the positive semidefinite cone is obtained by setting all its negative eigenvalues to zero, i.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 23,
      "context" : "[27]), which is symmetric",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "The proof of Proposition 2 follows from results in [2].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "[2] Let f : R → R∪{−∞,+∞} be a proper convex function.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "It is shown in [2], Proposition 3.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 11,
      "context" : ",m ([15], §4.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 25,
      "context" : "1 in [29].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 25,
      "context" : "[29] Consider a sequence {Xi}i=1 of fixed matrices in S and let {εi}i=1 i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[11] Let z1, .",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2015,
    "abstractText" : "Over the past few years, trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularizationbased approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In the present paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of the regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity.",
    "creator" : "LaTeX with hyperref package"
  }
}