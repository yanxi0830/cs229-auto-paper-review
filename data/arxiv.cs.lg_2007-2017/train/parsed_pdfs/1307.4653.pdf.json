{
  "name" : "1307.4653.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "bernardino.paredes.09@ucl.ac.uk", "m.pontil@cs.ucl.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 7.\n46 53\nv1 [\ncs .L"
    }, {
      "heading" : "1 Introduction",
      "text" : "During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein. This methodology, which is also referred to as tensor completion, has been applied to various fields, ranging from collaborative filtering [14], to computer vision [16], to medical imaging [8], among others. In this paper, we propose a new method to tensor completion, which is based on a convex regularizer which encourages low rank tensors and develop an algorithm for solving the associated regularization problem.\nArguably the most widely used convex approach to tensor completion is based upon the extension of trace norm regularization [23] to that context. This involves computing the average of the trace norm of each matricization of the tensor [15]. A key insight behind using trace norm regularization for matrix completion is that this norm provides a tight convex relaxation of the rank of a matrix defined on the spectral unit ball [7]. Unfortunately, the extension of this methodology to the more general\ntensor setting presents some difficulties. In particular, we shall prove in this paper that the tensor trace norm is not a tight convex relaxation of the tensor rank.\nThe above negative result stems from the fact that the spectral norm, used to compute the convex relaxation for the trace norm, is not an invariant property of the matricization of a tensor. This observation leads us to take a different route and study afresh the convex relaxation of tensor rank on the Euclidean ball. We show that this relaxation is tighter than the tensor trace norm, and we describe a technique to solve the associated regularization problem. This method builds upon the alternating direction method of multipliers and a subgradient method to compute the proximity operator of the proposed regularizer. Furthermore, we present numerical experiments on one synthetic dataset and two real-life datasets, which indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable.\nThe paper is organized in the following manner. In Section 2, we describe the tensor completion framework. In Section 3, we highlight some limitations of the tensor trace norm regularizer and present an alternative convex relaxation for the tensor rank. In Section 4, we describe a method to solve the associated regularization problem. In Section 5, we report on our numerical experience with the proposed method. Finally, in Section 6, we summarize the main contributions of this paper and discuss future directions of research."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "In this section, we begin by introducing some notation and then proceed to describe the learning problem. We denote by N the set of natural numbers and, for every k ∈ N, we define [k] = {1, . . . , k}. Let N ∈ N and let1 p1, . . . , pN ≥ 2. An N-order tensor W ∈ Rp1×···×pN , is a collection of real numbers (Wi1,...,iN : in ∈ [pn], n ∈ [N ]). Boldface Euler scripts, e.g. W , will be used to denote tensors of order higher than two. Vectors are 1-order tensors and will be denoted by lower case letters, e.g. x or a; matrices are 2-order tensors and will be denoted by upper case letters, e.g. W . If x ∈ Rd then for every r ≤ s ≤ d, we define xr:s := (xi : r ≤ i ≤ s). We also use the notation pmin = min{p1, . . . , pN} and pmax = max{p1, . . . , pN}. A mode-n fiber of a tensor W is a vector composed of the elements of W obtained by fixing all indices but one, corresponding to the n-th mode. This notion is a higher order analogue of columns (mode-1 fibers) and rows (mode-2 fibers) for matrices. The mode-n matricization (or unfolding) of W , denoted by W(n), is a matrix obtained by arranging the mode-n fibers of W so that each of them is a column of W(n) ∈ Rpn×Jn , where Jn := ∏\nk 6=n pk. Note that the order of the columns is not important as long as it is consistent.\nWe are now ready to describe the learning problem. We choose a linear operator I : Rp1×···×pN → Rm, representing a set of linear measurements obtained from a target tensor W0 as y = I(W0)+ξ, where ξ is some disturbance noise. In this paper, we mainly focus on tensor completion, in which case the operator I measures elements of the tensor. That is, we have I(W0) = (W0i1(j),...,iN (j) : j ∈ [m]), where, for every j ∈ [m] and n ∈ [N ], the index in(j) is a prescribed integer in the set [pn]. Our aim\n1For simplicity we assume that pn ≥ 2 for every n ∈ [N ], otherwise we simply reduce the order of the tensor without loss of information.\nis to recover the tensor W0 from the data (I, y). To this end, we solve the regularization problem\nmin { ‖y − I(W)‖22 + γR(W) : W ∈ Rp1×···×pN }\n(1)\nwhere γ is a positive parameter which may be chosen by cross validation. The role of the regularizer R is to encourage tensors W which have a simple structure in the sense that they involve a small number of “degrees of freedom”. A natural choice is to consider the average of the rank of the tensor’s matricizations. Specifically, we consider the combinatorial regularizer\nR(W) = 1\nN\nN ∑\nn=1\nrank(W(n)). (2)\nFinding a convex relaxation of this regularizer has been the subject of recent works [8, 16, 22]. They all agree to use the trace norm for tensors as a convex proxy of R. This is defined as the average of the trace norm of each matricization of W , that is,\n‖W‖tr = 1\nN\nN ∑\nn=1\n‖W(n)‖tr (3)\nwhere ‖W(n)‖tr is the trace (or nuclear) norm of matrix W(n), namely the ℓ1-norm of the vector of singular values of matrix W(n) (see, e.g. [13]). Note that in the particular case of 2-order tensors, functions (2) and (3) coincide with the usual notion of rank and trace norm of a matrix, respectively.\nA rational behind the regularizer (3) is that the trace norm is the tightest convex lower bound to the rank of a matrix on the spectral unit ball, see [7, Thm. 1]. This lower bound is given by the convex envelope of the function\nΨ(W ) =\n{\nrank(W ), if ‖W‖∞ ≤ 1 +∞, otherwise (4)\nwhere ‖ · ‖∞ is the spectral norm, namely the largest singular value of W . The convex envelope can be derived by computing the double conjugate of Ψ. This is defined as\nΨ∗∗(W ) = sup { 〈W,S〉 −Ψ∗(W ) : S ∈ Rp1×p2 }\n(5)\nwhere Ψ∗ is the conjugate of Ψ, namely Ψ∗(S) = sup {〈W,S〉 −Ψ(W ) : W ∈ Rp1×p2}. Note that Ψ is a spectral function, that is, Ψ(W ) = ψ(σ(W )) where ψ : Rd+ → R denotes the associated symmetric gauge function. Using von Neumann’s trace theorem (see e.g. [13]) it is easily seen that Ψ∗(S) is also a spectral function. That is, Ψ∗(S) = ψ∗(σ(S)), where\nψ∗(σ) = sup { 〈σ, w〉 − ψ(w) : w ∈ Rd+ } , with d := min(p1, p2).\nWe refer to [7] for a detailed discussion of these ideas. We will use this equivalence between spectral and gauge functions repeatedly in the paper."
    }, {
      "heading" : "3 Alternative Convex Relaxation",
      "text" : "In this section, we show that the tensor trace norm is not a tight convex relaxation of the tensor rank R in equation (2). We then propose an alternative convex relaxation for this function.\nNote that due to the composite nature of the function R, computing its convex envelope is a challenging task and one needs to resort to approximations. In [21], the authors note that the tensor trace norm ‖ · ‖tr in equation (3) is a convex lower bound to R on the set\nG∞ := { W ∈ Rp1×···×pN : ∥ ∥W(n) ∥ ∥ ∞ ≤ 1, ∀n ∈ [N ] } .\nThe key insight behind this observation is summarized in Lemma 4, which we report in Appendix A. However, the authors of [21] leave open the question of whether the tensor trace norm is the convex envelope of R on the set G∞. In the following, we will prove that this question has a negative answer by showing that there exists a convex function Ω 6= ‖ · ‖tr which underestimates the function R on G∞ and such that for some tensor W ∈ G∞ it holds that Ω(W) > ‖W‖tr. To describe our observation we introduce the set\nG2 := { W ∈ Rp1×...×pN : ‖W‖2 ≤ 1 }\nwhere ‖ · ‖2 is the Euclidean norm for tensors, that is,\n‖W‖22 := p1 ∑\ni1=1\n· · · pN ∑\niN=1\n(Wi1,...,iN )2.\nWe will choose\nΩ(W) = Ωα(W) := 1\nN\nN ∑\nn=1\nω∗∗α ( σ ( W(n) ))\n(6)\nwhere ω∗∗α is the convex envelope of the cardinality of a vector on the ℓ2-ball of radius α and we will choose α = √ pmin. Note, by Lemma 4 stated in Appendix A, that, for every α > 0, function Ωα is a convex lower bound of function R on the set αG2. Below, for every vector s ∈ Rd we denote by s↓ the vector obtained by reordering the components of s so that they are non increasing in absolute value, that is, |s↓1| ≥ · · · ≥ |s↓d|.\nLemma 1. Let ω∗∗α be the convex envelope of the cardinality function on the ℓ2-ball of radius α. Then, for every x ∈ Rd such that ‖x‖2 = α, it holds that ω∗∗α (x) = card (x).\nProof. First, we note that the conjugate of the function card on the ℓ2 ball of radius α is given by the formula\nω∗α (s) = sup ‖y‖2≤α {〈s, y〉 − card (y)} = max r∈{0,...,d} {α‖s↓1:r‖2 − r}. (7)\nHence, by the definition of the double conjugate, we have, for every s ∈ Rd that\nω∗∗α (x) ≥ 〈s, x〉 − max r∈{0,...,d} {α‖s↓1:r‖2 − r}.\nIn particular, if s = kx for some k > 0 this inequality becomes\nω∗∗α (x) ≥ k‖x‖22 − max r∈{0,...,d} (αk‖x↓1:r‖2 − r).\nIf k is large enough, the maximum is attained at r = card(x). Consequently,\nω∗∗α (x) ≥ kα2 − kα2 + card(x) = card(x). By the definition of the convex envelope, it also holds that ω∗∗α (x) ≤ card(x). The result follows.\nThe next lemma provides, together with Lemma 1, a sufficient condition for the existence of a tensor W ∈ G∞ at which the proposed regularizer is strictly larger than the tensor trace norm. Lemma 2. If N ≥ 3 and p1, . . . , pN are not all equal to each other, then there exists W ∈ Rp1×···×pN such that: (a) ‖W‖2 = √pmin, (b) W ∈ G∞, (c) min\nn∈[N ] rank(W(n)) < max n∈[N ] rank(W(n)).\nProof. Without loss of generality we assume that p1 ≤ · · · ≤ pN . By hypothesis p1 < pN . First we consider the special case p1 = · · · = pN−1, and pN = p1 + 1. (8) We define a class of tensors W by choosing a singular value decomposition for their mode-N matricization,\nWi1,i2,...,iN = pN ∑\nk=1\nσku k iN vki1,...,iN−1 (9)\nwhere σ1 = · · · = σpN = √ p1/(p1 + 1), the vectors uk ∈ RpN , ∀k ∈ [pN ] are orthonormal and the vectors vk ∈ Rp1p2···pN−1, ∀k ∈ [pN ] are orthonormal as well. Moreover, we choose vk as\nvki1,...,iN−1 =\n\n\n\n1 if i1 = · · · = iN−1 = k, k < pN 1√ p1\nif i2 = · · · = iN−1 = module(i1, p1) + 1, k = pN 0 otherwise.\n(10)\nBy construction the matrix W(N) has rank equal to pN and Frobenius norm equal to √ p1. Thus properties (a) and (c) hold true. It remains to show that W satisfies property (b). To this end, we will show, for every n ∈ [N ] and every x ∈ Rpn , that\n‖W⊤(n)x‖2 ≤ ‖x‖2. The case n = N is immediate. If n = 1 we have\n‖W⊤(1)x‖22 = ∑\ni2,...,iN\n(\n∑\nk\nσk ∑\ni1\nukiNv k i1,...,iN−1 xi1\n)2\n= ∑\ni2,...,iN\n∑\nk,ℓ\n∑\ni1,j1\nxi1xj1σkσℓu k iN uℓiNv k i1,i2,...,iN−1 vℓj1,i2,...,iN−1\n= ∑\nk\nσ2k ∑\ni1,j1\nxi1xj1\n\n\n∑\ni2,...,iN−1\nvki1,i2,...,iN−1v k j1,i2,...,iN−1\n\n\n= ∑\nk\nσ2kx 2 k + σ2pN p1 ∑\nk\nx2k = ‖x‖22\nwhere we used ∑ iN ukiNu ℓ iN = δk,ℓ in the third equality, equation (10) and a direct computation in the fourth equality, and the definition of σk in the last equality. All other cases, namely n = 2, . . . , N − 1, are conceptually identical, so we only discuss the case n = 2. We have\n‖W⊤(2)x‖22 = ∑\ni1,i3,...,iN\n(\n∑\nk\nσk ∑\ni2\nukiNv k i2,...,iN−1 xi2\n)2\n= ∑\ni1,i3,...,iN\n∑\nk,ℓ\n∑\ni2,j2\nxi2xj2σkσℓu k iN uℓiNv k i1,i2,...,iN−1 vℓi1,j2,...,iN−1\n= ∑\nk\nσ2k ∑\ni2,j2\n(\nxi2xj2 ∑\ni1,i3,...,iN=1\nvki1,i2,...,iN−1v k i1,j2,...,iN−1\n)\n= ∑\nk\nσ2kx 2 k + σ2pN p1 ∑\nk\nx2k = ‖x‖22\nwhere again we used ∑ iN ukiNu ℓ iN\n= δk,ℓ in the third equality, equation (10) and a direct computation in the fourth equality, and the definition of σk in the last equality. Finally, if assumption (8) is not true we set Wi1,...,iN = 0 if in ≥ p1 + 1, for some n ≤ N − 1 or iN > p1 + 1. We then proceed as in the case p1 = · · · = pN−1 and pN = p1 + 1.\nWe are now ready to present the main result of this section.\nProposition 3. Let p1, . . . , pN ∈ N, let ‖ · ‖tr be the tensor trace norm in equation (3) and let Ωα be the function in equation (6) for α = √ pmin. If pmin < pmax, then there are infinitely many tensors W ∈ G∞ such that Ωα(W) > ‖W‖tr. Moreover, for every W ∈ G2, it holds that Ω1(W) ≥ ‖W‖tr.\nProof. By construction Ωα(W) ≤ R(W) for every W ∈ αG2. Since G∞ ⊂ αG2 then Ωα is a convex lower bound for the tensor rank R on the set G∞ as well. The first claim now follows by Lemmas 1 and 2. Indeed, all tensors obtained following the process described in Lemma 2 have the property that\n‖W‖tr = 1\nN\nN ∑\nn=1\n‖σ(W(n))‖1\n= 1\nN\n(\npmin(N − 1) + √ p2min + pmin\n)\n< 1\nN (pmin(N − 1) + pmin + 1) = Ω(W) = R(W).\nFurthermore there are infinitely many such tensors which satisfy this claim since the left singular vectors can be arbitrarily chosen in equation (9). To prove the second claim, we note that since ω∗∗1 is the convex envelope of the cardinality card on the Euclidean unit ball, then it holds that ω∗∗1 (σ) ≥ ‖σ‖1 for every vector σ such that ‖σ‖2 ≤ 1. Consequently,\nΩ1(W) = 1\nN\nN ∑\nn=1\nω∗∗1 ( σ ( W(n) )) ≥ 1 N\nN ∑\nn=1\n‖σ(W(n))‖1 = ‖W‖tr.\nThe above result stems from the fact that the spectral norm is not an invariant property of the matricization of a tensor, whereas the Euclidean (Frobenius) norm is. This observation leads us to further study the function Ωα."
    }, {
      "heading" : "4 Optimization Method",
      "text" : "In this section, we explain how to solve the regularization problem associated with the proposed regularizer (6). For this purpose, we first recall the alternating direction method of multipliers (ADMM) [3], which was conveniently applied to tensor trace norm regularization in [8, 21]."
    }, {
      "heading" : "4.1 Alternating Direction Method of Multipliers (ADMM)",
      "text" : "To explain ADMM we consider a more general problem comprising both tensor trace norm regularization and the regularizer we propose,\nmin W\n{\nE (W) + γ N ∑\nn=1\nΨ ( W(n) )\n}\n(11)\nwhere E(W) is an error term such as ‖y−I(W)‖22 and Ψ is a convex spectral function. It is defined, for every matrix A, as\nΨ(A) = ψ(σ(A))\nwhere ψ is a gauge function, namely a function which is symmetric and invariant under permutations. In particular, if ψ is the ℓ1 norm then problem (11) corresponds to tensor trace norm regularization, whereas if ψ = ω∗∗α it implements the proposed regularizer.\nProblem (11) poses some difficulties because the terms under the summation are interdependent, that is, the different matricizations of W have the same elements rearranged in a different way. In order to overcome this difficulty, the authors of [8, 21] proposed to use ADMM as a natural way to decouple the regularization term appearing in problem (11). This strategy is based on the introduction of N auxiliary tensors, B1, . . . ,BN ∈ Rp1×···×pN , so that problem (11) can be reformulated as2\nmin W ,B1,...,BN\n{\n1 γ E (W) +\nN ∑\nn=1\nΨ ( Bn(n) ) : Bn = W, n ∈ [N ] }\n(12)\nThe corresponding augmented Lagrangian (see e.g. [3, 4]) is given by\nL (W ,B,A) = 1 γ E (W) +\nN ∑\nn=1\n(\nΨ ( Bn(n) ) − 〈An,W −Bn〉+ β\n2 ‖W − Bn‖22\n)\n, (13)\n2The somewhat cumbersome notationB n(n) denotes the mode-n matricization of tensor Bn, that is, Bn(n) = (Bn)(n).\nwhere 〈·, ·〉 denotes the scalar product between tensors, β is a positive parameter and A1, . . .AN ∈ R p1×···×pN are the set of Lagrange multipliers associated with the constraints in problem (12).\nADMM is based on the following iterative scheme\nW [i+1] ← argmin\nW\nL ( W ,B[i],A[i] )\n(14)\nB [i+1] n ← argmin\nBn\nL ( W [i+1],B,A[i] )\n(15)\nA [i+1] n ← A[i]n −\n( βW [i+1] − B[i+1]n ) . (16)\nStep (16) is straightforward, whereas step (14) is described in [8]. Here we focus on the step (15) since this is the only problem which involves function Ψ. We restate it with more explanatory notations as\nargmin Bn(n)\n{\nΨ ( Bn(n) ) − 〈 An(n),W(n) −Bn(n) 〉 + β\n2\n∥ ∥W(n) −Bn(n) ∥ ∥ 2\n2\n}\n.\nBy completing the square in the right hand side, the solution of this problem is given by\nB̂n(n) = prox 1 β Ψ (X) := argmin\nBn(n)\n{\n1 β Ψ ( Bn(n) ) + 1 2 ∥ ∥Bn(n) −X ∥ ∥ 2 2\n}\nwhere X = W(n) − 1βAn(n). By using properties of proximity operators (see e.g. [1, Prop. 3.1]) we know that if ψ is a gauge function then\nprox 1 β Ψ (X) = UXdiag\n(\nprox 1 β ψ (σ(X))\n)\nV ⊤X\nwhere UX and VX are the orthogonal matrices formed by the left and right singular vectors of X , respectively.\nIf we choose ψ = ‖·‖1 the associated proximity operator is the well-known soft thresholding operator, that is, prox 1\nβ ‖·‖1 (σ) = v, where the vector v has components\nvi = sign (σi)\n(\n|σi| − 1\nβ\n)\n.\nOn the other hand, if we choose ψ = ω∗∗α , we need to compute prox 1 β ω∗∗α\n. In the next section, we describe a method to accomplish this task."
    }, {
      "heading" : "4.2 Computation of the Proximity Operator",
      "text" : "In order to compute the proximity operator of the function 1 β ω∗∗α we will use several properties of proximity calculus. First, we use the formula (see e.g. [6]) proxg∗ (x) = x−proxg (x) for g∗ = 1βω∗∗α . Next we use a property of conjugate functions from [20, 12], which states that g(·) = 1\nβ ω∗α(β·).\nFinally, by the scaling property of proximity operators [6], we have that proxg (x) = 1 β proxβω∗α (βx).\nAlgorithm 1 Computation of proxβω∗α(y)\nInput: y ∈ Rd, α, β > 0. Output: ŵ ∈ Rd. Initialization: initial step τ0 = 12 , initial and best found solution w\n0 = ŵ = PS(y) ∈ Rd. for t = 1, 2, . . . do τ ← τ0√\nt\nFind k such that k ∈ argmax { α‖wt−11:r ‖2 − r : 0 ≤ r ≤ d } w̃1:k ← wt−11:k − τ ( wt−11:k (\n1 + αβ‖wt−11:k ‖2\n) − y1:k )\nw̃k+1:d ← wt−1k+1:d − τ ( wt−1k+1:d − yk+1:d ) wt ← P̃S (w̃) If h(wt) < h(ŵ) then ŵ ← wt If “Stopping Condition = True” then terminate.\nend for\nIt remains to compute the proximity operator of a multiple of the function ω∗α in equation (7), that is, for any β > 0, y ∈ S, we wish to compute\nproxβω∗α (y) = argmin w\n{h (w) : w ∈ S}\nwhere we have defined S := {w ∈ Rd : w1 ≥ · · · ≥ wd ≥ 0} and\nh (w) = 1\n2 ‖w − y‖22 + β d max r=0 {α ‖w1:r‖2 − r} .\nIn order to solve this problem we employ the projected subgradient method, see e.g. [5]. It consists in applying two steps at each iteration. First, it advances along a negative subgradient of the current solution; second, it projects the resultant point onto the feasible set S. In fact, according to [5], it is sufficient to compute an approximate projection, a step which we describe in Appendix B. To compute a subgradient of h at w, we first find any integer k such that k ∈ dargmax r=0\n{α ‖w1:r‖2 − r}. Then, we calculate a subgradient g of the function h at w by the formula\ngi =\n{ (\n1 + αβ‖w1:k‖2\n)\nwi − yi, if i ≤ k, wi − yi, otherwise.\nNow we have all the ingredients to apply the projected subgradient method, which is summarized in Algorithm 1. In our implementation we stop the algorithm when an update of ŵ is not made for more than 103 iterations."
    }, {
      "heading" : "5 Experiments",
      "text" : "We have conducted a set of experiments to assess whether there is any advantage of using the proposed regularizer over the tensor trace norm for tensor completion. First, we have designed a synthetic\nexperiment to evaluate the performance of both approaches under controlled conditions. Then, we have tried both methods on two tensor completion real data problems. In all cases, we have used a validation procedure to tune the hyper-parameter γ, present in both approaches, among the values {10j : j = −7,−6, . . . , 0}. In our proposed approach there is one further hyper-parameter, α, to be specified. It should take the value of the Frobenius norm of any matricization of the underlying tensor. Since this is unknown, we propose to use the estimate\nα̂ =\n√ √ √\n√‖w‖22 + (mean(w)2 + var(w)) ( N ∏\ni=1\npi −m ) ,\nwhere m if the number of known entries and w ∈ Rm contains their values. This estimator assumes that each value in w is sampled from N (mean(w), var(w)), where mean(w) and var(w) are the average and the variance of the elements in w."
    }, {
      "heading" : "5.1 Synthetic Dataset",
      "text" : "We have generated a 3-order tensor W0 ∈ R40×20×10 by the following procedure. First we generated a tensor W with ranks (12, 6, 3) using Tucker decomposition (see e.g. [15])\nWi1,i2,i3 = 12 ∑\nj1=1\n6 ∑\nj2=1\n3 ∑\nj3=1\nCj1,j2,j3M (1)i1,j1M (2) i2,j2 M (3) i3,j3 , (i1, i2, i3) ∈ [40]× [20]× [10]\nwhere each entry of the Tucker decomposition components is sampled from the standard Gaussian distribution N (0, 1). We then created the ground truth tensor W0 by the equation\nW0i1,i2,i3 = Wi1,i2,i3 −mean(W)\nstd(W) + ξi1,i2,i3\nwhere mean(W) and std(W) are the mean and standard deviation of the elements of W and the ξi1,i2,i3 are i.i.d. Gaussian random variables with zero mean and variance σ\n2. We have randomly sampled 10% of the elements of the tensor to compose the training set, 45% for the validation set, and the remaining 45% for the test set. After repeating this process 20 times, we report the average results in Figure 1 (Left). Having conducted a paired t-test for each value of σ2, we conclude that the visible differences in the performances are highly significant, obtaining always p-values less than 0.01 for σ2 ≤ 10−2. Furthermore, we have conducted an experiment to test the running time of both approaches. We have generated tensors W0 ∈ Rp×p×p for different values of p ∈ {20, 40, . . . , 200}, following the same procedure as outlined above. The results are reported in Figure 1 (Right). For low values of p, the ratio between the running time of our approach and that of trace norm regularization is quite high. For example in the lowest value tried for p in this experiment, p = 20, this ratio is 22.661. However, as the volume of the tensor increases, the ratio quickly decreases. For example, for p = 200, the running time ratio is 1.9113. These outcomes are expected since when p is low, the most demanding routine in our method is the one described in Algorithm 1, where each iteration is of order O (p) and O (p2) in the\nbest and worst case, respectively. However, as p increases the singular value decomposition routine, which is common to both methods, becomes the most demanding because it has a time complexity O (p3) [9]. Therefore, we can conclude that even though our approach is slower than the trace norm based method, this difference becomes much smaller as the size of the tensor increases."
    }, {
      "heading" : "5.2 School Dataset",
      "text" : "The first real dataset we have tried is the Inner London Education Authority (ILEA) dataset3 . It is composed of examination marks ranging from 0 to 70, of 15362 students which are described by a set of attributes such as school and ethnic group. Most of these attributes are categorical, thereby we can think of exam mark prediction as a tensor completion problem where each of the modes corresponds to a categorical attribute. In particular, we have used the following attributes: school (139), gender (2), VR-band (3), ethnic (11), and year (3), leading to a 5-order tensor W ∈ R139×2×3×11×3. We have selected randomly 5% of the instances to make the test set and another 5% of the instances for the validation set. From the remaining instances, we have randomly chosen m of them for several values of m. This procedure has been repeated 20 times and the average performance is presented in Figure 2 (Left). There is a distinguishable improvement of our approach with respect to tensor trace norm regularization. To check whether this gap is significant, we have conducted a set of paired t-tests for each value of m. In all cases we obtained a p-value below 0.01."
    }, {
      "heading" : "5.3 Video Completion",
      "text" : "In the second real-data experiment we have performed a video completion test. Any video can be treated as a 4-order tensor: “width” × “height” × “RGB” × “video length”, so we can use tensor\n3Available at http://www.bristol.ac.uk/cmm/learning/support/datasets/ilea567.zip.\ncompletion algorithms to rebuild a video from a few inputs, a procedure that can be useful for compression purposes. In our case, we have used the Ocean video, available at [16]. This video sequence can be treated as a tensor W ∈ R160×112×3×32. We have randomly sampled m tensors elements as training data, 5% of them as validation data, and the remaining ones composed the test set. After repeating this procedure 10 times, we present the average results in Figure 2 (Right). The proposed approach is noticeably better than the tensor trace norm in this experiment. This apparent outcome is strongly supported by the paired t-tests which we run for each value of m, obtaining always p-values below 0.01, and for the cases m > 5× 104, we obtained p-values below 10−6."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we proposed a convex relaxation for the average of the rank of the matricizations of a tensor. We compared this relaxation to a commonly used convex relaxation used in the context of tensor completion, which is based on the trace norm. We proved that this second relaxation is not tight and argued that the proposed convex regularizer may be advantageous. Empirical comparisons indicate that our method consistently improves in terms of estimation error over tensor trace norm regularization, while being computationally comparable on the range of problems we considered. In the future it would be interesting to study methods to speed up the computation of the proximity operator of our regularizer and investigate its utility in tensor learning problem beyond tensor completion such as multilinear multitask learning [19]."
    }, {
      "heading" : "A A Useful Lemma",
      "text" : "Lemma 4. Let C1, . . . , CN be convex subsets of a Euclidean space and let D = ⋂N n=1 Cn 6= ∅. Let g : ∏N\nn=1 Cn → R and let h : D → R be the function defined, for every x ∈ D, as h(x) = g(x, . . . , x). Then, for every x ∈ D, it holds that\nh∗∗(x) ≥ g∗∗(x1, . . . , xN) ∣ ∣ xn=x, ∀n∈[N ] .\nProof. Since the restriction of g on DN ⊆ ∏Nn=1 Cn equals to h, the convex envelope of g when evaluated on the smaller set DN cannot be larger than the convex envelope of h on D.\nUsing this result it is immediately possible to derive a convex lower bound for the function R in equation (2). Since the convex envelope of the rank function on the unit ball of the spectral norm is the trace norm, using Lemma 4 with Cn = {W : ‖W(n)‖∞ ≤ 1} and\ng(W1, . . . ,WN) = 1\nN\nN ∑\nn=1\nrank((Wn)(n)),\nwe conclude that the convex envelope of the function R on the set G∞ is bounded from below by 1 N ∑N\nn=1 ‖W(n)‖tr. Likewise the convex envelope of R on the set αG2 is lower bounded by the function Ωα in equation (6)."
    }, {
      "heading" : "B Computation of an Approximated Projection",
      "text" : "Here, we address the issue of computing an approximate Euclidean projection onto the set\nS = {v ∈ Rd : v1 ≥ · · · ≥ vd ≥ 0}.\nThat is, for every v, we shall find a point P̃S(v) ∈ S such that ∥\n∥ ∥ P̃S (v)− z\n∥ ∥ ∥\n2 ≤ ‖v − z‖2 , ∀z ∈ S. (17)\nAs noted in [5], in order to build P̃S such that this property holds true, it is useful to express the set of interest as the smallest one in a series of nested sets. In our problem, we can express S as\nS = Sd ⊆ Sd−1 ⊆ . . . ⊆ S1,\nwhere Si := { v ∈ Rd : v1 ≥ v2 ≥ . . . ≥ vi, v ≥ 0 }\n. This property allows us to sequentially compute an approximate projection on the set S using the formula\nP̃S (v) = PSd ( PSd−1 · · · (PS1 (v)) )\n(18)\nwhere, for every close convex set C, we let PC be the associated projection operator. Indeed, following [5], we can argue by induction on i that P̃S (v) verifies condition (17). The base case is ‖PS1 (v)− z‖2 = ‖v − z‖2, which is obvious. Now, if for a given 1 ≤ i ≤ d− 1 it holds that\n‖PSi (· · ·PS1 (v))− z‖2 ≤ ‖v − z‖2\nthen\n∥ ∥PSi+1 (PSi (· · ·PS1 (v)))− z ∥ ∥ 2 ≤ ‖PSi (· · ·PS1 (v))− z‖2 ≤ ‖v − z‖2,\nsince z is also contained in Si+1. Note that to evaluate the right hand side of equation (18) we do not require full knowledge of PSi , we only need to compute PSi+1(v) for v ∈ Si. The next proposition describes a recursive formula to achieve this step.\nAlgorithm 2 Computing an approximated projection onto the set S = {v ∈ Rd : v1 ≥ · · · ≥ vd ≥ 0}. Input: y ∈ Rd+. Output: v ∈ S. Initialization: v ← y. for i = 1, 2, . . . , d do\nwhile vi < vi+1 do j ← argmax{ℓ : ℓ ∈ [i], vi = vi−ℓ+1} if vi + vi+1−vi j+1 then\nv1:i+1 ← [ v1:i−j, ( vi + vi+1−vi j+1 ) 1 j+1 ]\nelse v1:i+1 ← [ v1:i−j , vi−j1 j, vi+1 − (vi−j − vi) j ]\nend if end while\nend for\nProposition 5. For any v ∈ Si, we express its first i elements as v1:i = [ v1:i−j , vi1 j ]\n, where the last j ∈ [i] is the largest integer such that vi−j+1 = vi−j+2 = · · · = vi. It holds that\nPSi+1(v) =\n\n  \n  \nv if vi ≥ vi+1 [\nv1:i−j, ( vi + vi+1−vi j+1 ) 1 j+1, vi+2:d ] if vi < vi+1 and vi−j ≥ vi+ vi+1−vij+1 PSi+1 ([ v1:i−j, vi−j1 j , vi+1− (vi−j − vi) j, vi+2:d ]) otherwise,\nwhere 1d ∈ Rd denotes the vector containing 1 in all its elements.\nProof. The first case is straightforward. In the following we prove the remaining two. In both cases it will be useful to recall that the projection operator PC on any convex set C is characterized as\nx = PC (y) ⇐⇒ 〈y − x, z − x〉 ≤ 0, ∀z ∈ C. (19)\nTo prove the second case, we use property (19) and apply simple algebraic transformations to obtain, for all z ∈ Si+1, that\n〈 v − PSi+1 (v) , z − PSi+1 (v) 〉 = vi+1 − vi j + 1 ( jzi+1 − ‖zi−j+1:i‖1 ) ≤ 0.\nFinally we prove the third case. We want to show that if x = PSi+1(v) then\nx = PSi+1 ([ v1:i−j , vi−j1 j, vi+1 − (vi−j − vi) j, vi+2:d ]) .\nBy using property (19), the last equation is equivalent to the statement that if\n〈v − x, z − x〉 ≤ 0, ∀z ∈ Si+1 then (20) 〈[\nv1:i−j , vi−j1 j, vi+1 − (vi−j − vi) j, vi+2:d\n] − x, z − x 〉 ≤ 0, ∀z ∈ Si+1. (21)\nA way to show that it holds true is to prove that the term in the left hand side of (21) is upper bounded by the corresponding term in (20). That is, for every z ∈ Si+1, we want to show that\n〈[\nv1:i−j, vi−j1 j , vi+1 − (vi−j − vi) j, vi+2:d\n] − v, z − x 〉 ≤ 0.\nA direct computation yields the equivalent inequality\n(vi−j − vi) ( jxi+1 − ‖xi−j+1:i‖1 + ‖zi−j+1:i‖1 − jzi+1 ) ≤ 0. (22)\nSince x = PSi+1 (v), vi−j+1 = vi−j+2 = · · · = vi and vi+1 > vi, then xi−j+1 = xi−j+2 = · · · = xi+1. Consequently, the left hand side of inequality (22) is equivalent to\n(vi−j − vi) ( ‖zi−j+1:i‖1 − jzi+1 ) ≤ 0.\nNote that the first factor is negative and the second is positive because z and v are in Si+1. The result follows.\nAlgorithm 2 summarizes our method to compute the approximated projection operator onto the set S, based on Proposition 5."
    } ],
    "references" : [ {
      "title" : "Efficient first order methods for linear composite regularizers",
      "author" : [ "A. Argyriou", "C.A. Micchelli", "M. Pontil", "L. Shen", "Y. Xu" ],
      "venue" : "arXiv:1104.1436",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Matrix Analysis",
      "author" : [ "R. Bhatia" ],
      "venue" : "Springer Verlag",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Parallel and Distributed Computation: Numerical Methods",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Prentice-Hall",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning, 3(1):1–122",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Subgradient methods",
      "author" : [ "S. Boyd", "L. Xiao", "A. Mutapcic" ],
      "venue" : "Stanford University",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Proximal splitting methods in signal processing",
      "author" : [ "P.L. Combettes", "J.-C. Pesquet" ],
      "venue" : "Fixed- Point Algorithms for Inverse Problems in Science and Engineering (H. H. Bauschke et al. Eds), pages 185–212, Springer",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A rank minimization heuristic with application to minimum order system approximation",
      "author" : [ "M. Fazel", "H. Hindi", "S. Boyd" ],
      "venue" : "Proc. American Control Conference, Vol. 6, pages 4734–4739",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Tensor completion and low-n-rank tensor recovery via convex optimization",
      "author" : [ "S. Gandy", "B. Recht", "I. Yamada" ],
      "venue" : "Inverse Problems, 27(2)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Matrix Computations",
      "author" : [ "G.H. Golub", "C.F. Van Loan" ],
      "venue" : "3rd Edition. Johns Hopkins University Press",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Large-scale image classification with trace-norm regularization",
      "author" : [ "Z. Harchaoui", "M. Douze", "M. Paulin", "M. Dudik", "J. Malick" ],
      "venue" : "IEEE Conference on Computer Vision & Pattern Recognition (CVPR), pages 3386–3393",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Convex Analysis and Minimization Algorithms",
      "author" : [ "J-B. Hiriart-Urruty", "C. Lemaréchal" ],
      "venue" : "Part I. Springer",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Convex Analysis and Minimization Algorithms",
      "author" : [ "J-B. Hiriart-Urruty", "C. Lemaréchal" ],
      "venue" : "Part II. Springer",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Topics in Matrix Analysis",
      "author" : [ "R.A. Horn", "C.R. Johnson" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Multiverse recommendation: ndimensional tensor factorization for context-aware collaborative filtering",
      "author" : [ "A. Karatzoglou", "X. Amatriain", "L. Baltrunas", "N. Oliver" ],
      "venue" : "Proc. 4th ACM Conference on Recommender Systems, pages 79–86",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "T.G. Kolda", "B.W. Bade" ],
      "venue" : "SIAM Review, 51(3):455– 500",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Tensor completion for estimating missing values in visual data",
      "author" : [ "J. Liu", "P. Musialski", "P. Wonka", "J. Ye" ],
      "venue" : "Proc. 12th International Conference on Computer Vision (ICCV), pages 2114–2121",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Gradient methods for minimizing composite objective functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "ECORE Discussion Paper, 2007/96",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A simpler approach to matrix completion",
      "author" : [ "B. Recht" ],
      "venue" : "Journal of Machine Learning Research, 12:3413–3430",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multilinear multitask learning",
      "author" : [ "B. Romera-Paredes", "H. Aung", "N. Bianchi-Berthouze", "M. Pontil" ],
      "venue" : "Proc. 30th International Conference on Machine Learning (ICML), pages 1444–1452",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Minimization Methods for Non-differentiable Functions",
      "author" : [ "N.Z. Shor" ],
      "venue" : "Springer",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "R",
      "author" : [ "M. Signoretto" ],
      "venue" : "Van de Plas, B. De Moor, J.A.K. Suykens. Tensor versus matrix completion: a comparison with application to spectral data. IEEE Signal Processing Letters, 18(7):403–406",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Maximum margin matrix factorization",
      "author" : [ "N. Srebro", "J. Rennie", "T. Jaakkola" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 17, pages 1329–1336",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Estimation of low-rank tensors via convex optimization",
      "author" : [ "R. Tomioka", "K. Hayashi", "H. Kashima", "J.S.T. Presto" ],
      "venue" : "arXiv:1010.0789",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convex tensor decomposition via structured Schatten norm regularization",
      "author" : [ "R. Tomioka", "T. Suzuki" ],
      "venue" : "arXiv:1303.6370",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Statistical performance of convex tensor decomposition",
      "author" : [ "R. Tomioka", "T. Suzuki", "K. Hayashi", "H. Kashima" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 24, pages 972–980",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.",
      "startOffset" : 182,
      "endOffset" : 209
    }, {
      "referenceID" : 15,
      "context" : "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.",
      "startOffset" : 182,
      "endOffset" : 209
    }, {
      "referenceID" : 20,
      "context" : "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.",
      "startOffset" : 182,
      "endOffset" : 209
    }, {
      "referenceID" : 22,
      "context" : "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.",
      "startOffset" : 182,
      "endOffset" : 209
    }, {
      "referenceID" : 23,
      "context" : "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.",
      "startOffset" : 182,
      "endOffset" : 209
    }, {
      "referenceID" : 24,
      "context" : "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.",
      "startOffset" : 182,
      "endOffset" : 209
    }, {
      "referenceID" : 13,
      "context" : "This methodology, which is also referred to as tensor completion, has been applied to various fields, ranging from collaborative filtering [14], to computer vision [16], to medical imaging [8], among others.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "This methodology, which is also referred to as tensor completion, has been applied to various fields, ranging from collaborative filtering [14], to computer vision [16], to medical imaging [8], among others.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 7,
      "context" : "This methodology, which is also referred to as tensor completion, has been applied to various fields, ranging from collaborative filtering [14], to computer vision [16], to medical imaging [8], among others.",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 21,
      "context" : "Arguably the most widely used convex approach to tensor completion is based upon the extension of trace norm regularization [23] to that context.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 14,
      "context" : "This involves computing the average of the trace norm of each matricization of the tensor [15].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "A key insight behind using trace norm regularization for matrix completion is that this norm provides a tight convex relaxation of the rank of a matrix defined on the spectral unit ball [7].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "(2) Finding a convex relaxation of this regularizer has been the subject of recent works [8, 16, 22].",
      "startOffset" : 89,
      "endOffset" : 100
    }, {
      "referenceID" : 15,
      "context" : "(2) Finding a convex relaxation of this regularizer has been the subject of recent works [8, 16, 22].",
      "startOffset" : 89,
      "endOffset" : 100
    }, {
      "referenceID" : 20,
      "context" : "(2) Finding a convex relaxation of this regularizer has been the subject of recent works [8, 16, 22].",
      "startOffset" : 89,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "[13]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13]) it is easily seen that Ψ∗(S) is also a spectral function.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "We refer to [7] for a detailed discussion of these ideas.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "For this purpose, we first recall the alternating direction method of multipliers (ADMM) [3], which was conveniently applied to tensor trace norm regularization in [8, 21].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "For this purpose, we first recall the alternating direction method of multipliers (ADMM) [3], which was conveniently applied to tensor trace norm regularization in [8, 21].",
      "startOffset" : 164,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "In order to overcome this difficulty, the authors of [8, 21] proposed to use ADMM as a natural way to decouple the regularization term appearing in problem (11).",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "[3, 4]) is given by L (W ,B,A) = 1 γ E (W) + N ∑",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "[3, 4]) is given by L (W ,B,A) = 1 γ E (W) + N ∑",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "(16) Step (16) is straightforward, whereas step (14) is described in [8].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "[6]) proxg∗ (x) = x−proxg (x) for g∗ = 1 βω α .",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 19,
      "context" : "Next we use a property of conjugate functions from [20, 12], which states that g(·) = 1 β ω∗ α(β·).",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "Next we use a property of conjugate functions from [20, 12], which states that g(·) = 1 β ω∗ α(β·).",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "Finally, by the scaling property of proximity operators [6], we have that proxg (x) = 1 β proxβω∗ α (βx).",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "[5].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "In fact, according to [5], it is sufficient to compute an approximate projection, a step which we describe in Appendix B.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 14,
      "context" : "[15]) Wi1,i2,i3 = 12 ∑",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "j3=1 Cj1,j2,j3M (1) i1,j1M (2) i2,j2 M (3) i3,j3 , (i1, i2, i3) ∈ [40]× [20]× [10] where each entry of the Tucker decomposition components is sampled from the standard Gaussian distribution N (0, 1).",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "j3=1 Cj1,j2,j3M (1) i1,j1M (2) i2,j2 M (3) i3,j3 , (i1, i2, i3) ∈ [40]× [20]× [10] where each entry of the Tucker decomposition components is sampled from the standard Gaussian distribution N (0, 1).",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "However, as p increases the singular value decomposition routine, which is common to both methods, becomes the most demanding because it has a time complexity O (p) [9].",
      "startOffset" : 165,
      "endOffset" : 168
    } ],
    "year" : 2013,
    "abstractText" : "We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable.",
    "creator" : "LaTeX with hyperref package"
  }
}