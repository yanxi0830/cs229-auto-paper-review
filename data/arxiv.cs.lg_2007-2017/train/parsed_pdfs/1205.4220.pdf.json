{
  "name" : "1205.4220.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Ali H. Sayed" ],
    "emails" : [ "sayed@ee.ucla.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 5.\n42 20\nv2 [\ncs .M\nA ]\n5 M\nay 2\n01 3\nDIFFUSION ADAPTATION OVER NETWORKS∗\nAli H. Sayed\nElectrical Engineering Department University of California at Los Angeles\nAdaptive networks are well-suited to perform decentralized information processing and optimization tasks and to model various types of self-organized and complex behavior encountered in nature. Adaptive networks consist of a collection of agents with processing and learning abilities. The agents are linked together through a connection topology, and they cooperate with each other through local interactions to solve distributed optimization, estimation, and inference problems in real-time. The continuous diffusion of information across the network enables agents to adapt their performance in relation to streaming data and network conditions; it also results in improved adaptation and learning performance relative to non-cooperative agents. This article provides an overview of diffusion strategies for adaptation and learning over networks. The article is divided into several sections:\n1. Motivation.\n2. Mean-Square-Error Estimation.\n3. Distributed Optimization via Diffusion Strategies.\n4. Adaptive Diffusion Strategies.\n5. Performance of Steepest-Descent Diffusion Strategies.\n6. Performance of Adaptive Diffusion Strategies.\n7. Comparing the Performance of Cooperative Strategies.\n8. Selecting the Combination Weights.\n9. Diffusion with Noisy Information Exchanges.\n10. Extensions and Further Considerations.\n11. Appendix A: Properties of Kronecker Products.\n12. Appendix B: Graph Laplacian and Network Connectivity.\n13. Appendix C: Stochastic Matrices.\n14. Appendix D: Block Maximum Norm.\n15. Appendix E: Comparison with Consensus Strategies.\n16. References.\n∗The cite information for this article is as follows: A. H. Sayed, “Diffusion adaptation over networks,” in E-Reference Signal Processing, R. Chellapa and S. Theodoridis, editors, Elsevier, 2013. The work was supported in part by NSF grants EECS-060126, EECS-0725441, CCF-0942936, and CCF-1011918. The author is with the Electrical Engineering Department, University of California, Los Angeles, CA 90095, USA. Email: sayed@ee.ucla.edu."
    }, {
      "heading" : "1 Motivation",
      "text" : "Consider a collection of N agents interested in estimating the same parameter vector, wo, of size M ×1. The vector is the minimizer of some global cost function, denoted by Jglob(w), which the agents seek to optimize, say,\nwo = argmin w Jglob(w) (1)\nWe are interested in situations where the individual agents have access to partial information about the global cost function. In this case, cooperation among the agents becomes beneficial. For example, by cooperating with their neighbors, and by having these neighbors cooperate with their neighbors, procedures can be devised that would enable all agents in the network to converge towards the global optimum wo through local interactions. The objective of decentralized processing is to allow spatially distributed agents to achieve a global objective by relying solely on local information and on in-network processing. Through a continuous process of cooperation and information sharing with neighbors, agents in a network can be made to approach the global performance level despite the localized nature of their interactions."
    }, {
      "heading" : "1.1 Networks and Neighborhoods",
      "text" : "In this article we focus mainly on connected networks, although many of the results hold even if the network graph is separated into disjoint subgraphs. In a connected network, if we pick any two arbitrary nodes, then there will exist at least one path connecting them: the nodes may be connected directly by an edge if they are neighbors, or they may be connected by a path that passes through other intermediate nodes. Figure 1 provides a graphical representation of a connected network with N = 10 nodes. Nodes that are able to share information with each other are connected by edges. The sharing of information over these edges can be unidirectional or bi-directional. The neighborhood of any particular node is defined as the set of nodes that are connected to it by edges; we include in this set the node itself. The figure illustrates the neighborhood of node 3, which consists of the following subset of nodes: N3 = {1, 2, 3, 5}. For each node, the size of its neighborhood defines its degree. For example, node 3 in the figure has degree |N3| = 4, while node 8 has degree |N8| = 5. Nodes that are well connected have higher degrees. Note that we are denoting the neighborhood of an arbitrary node k by Nk and its size by |Nk|. We shall also use the notation nk to refer to |Nk|.\nThe neighborhood of any node k therefore consists of all nodes with which node k can exchange information. We assume a symmetric situation in relation to neighbors so that if node k is a neighbor of node ℓ, then node ℓ is also a neighbor of node k. This does not necessarily mean that the flow of information between these two nodes is symmetrical. For instance, in future sections, we shall assign pairs of nonnegative weights to each edge connecting two neighboring nodes — see Fig. 2. In particular, we will assign the coefficient aℓk to denote the weight used by node k to scale the data it receives from node ℓ; this scaling can be interpreted as a measure of trustworthiness or reliability that node k assigns to its interaction with node ℓ. Note that we are using two subscripts, ℓk, with the first subscript denoting the source node (where information originates from) and the second subscript denoting the sink node (where information moves to) so that:\naℓk ≡ aℓ → k (information flowing from node ℓ to node k) (2)\nIn this way, the alternative coefficient akℓ will denote the weight used to scale the data sent from node k to ℓ:\nakℓ ≡ ak → ℓ (information flowing from node k to node ℓ) (3)\nThe weights {akℓ, aℓk} can be different, and one or both of them can be zero, so that the exchange of information over the edge connecting the neighboring nodes (k, ℓ) need not be symmetric. When one of the\nweights is zero, say, akℓ = 0, then this situation means that even though nodes (k, ℓ) are neighbors, node ℓ is either not receiving data from node k or the data emanating from node k is being annihilated before reaching node ℓ. Likewise, when akk > 0, then node k scales its own data, whereas akk = 0 corresponds to the situation when node k does not use its own data. Usually, in graphical representations like those in Fig. 1, edges are drawn between neighboring nodes that can share information. And, it is understood that the actual sharing of information is controlled by the values of the scaling weights that are assigned to the edge; these values can turn off communication in one or both directions and they can also scale one direction more heavily than the reverse direction, and so forth."
    }, {
      "heading" : "1.2 Cooperation Among Agents",
      "text" : "Now, depending on the application under consideration, the solution vector wo from (1) may admit different interpretations. For example, the entries of wo may represent the location coordinates of a nutrition source that the agents are trying to find, or the location of an accident involving a dangerous chemical leak. The nodes may also be interested in locating a predator and tracking its movements over time. In these localization applications, the vector wo is usually two or three-dimensional. In other applications, the entries of wo may represent the parameters of some model that the network wishes to learn, such as identifying the model parameters of a biological process or the occupied frequency bands in a shared communications medium. There are also situations where different agents in the network may be interested in estimating different entries of wo, or even different parameter vectors wo altogether, say, {wok} for node k, albeit with some relation among the different vectors so that cooperation among the nodes can still be rewarding. In this article, however, we focus exclusively on the special (yet frequent and important) case where all agents are interested in estimating the same parameter vector wo.\nSince the agents have a common objective, it is natural to expect cooperation among them to be beneficial in general. One important question is therefore how to develop cooperation strategies that can lead to better performance than when each agent attempts to solve the optimization problem individually. Another important question is how to develop strategies that endow networks with the ability to adapt and learn in\nreal-time in response to changes in the statistical properties of the data. This article provides an overview of results in the area of diffusion adaptation with illustrative examples. Diffusion strategies are powerful methods that enable adaptive learning and cooperation over networks. There have been other useful works in the literature on the use of alternative consensus strategies to develop distributed optimization solutions over networks. Nevertheless, we explain in App. E why diffusion strategies outperform consensus strategies in terms of their mean-square-error stability and performance. For this reason, we focus in the body of the chapter on presenting the theoretical foundations for diffusion strategies and their performance."
    }, {
      "heading" : "1.3 Notation",
      "text" : "In our treatment, we need to distinguish between random variables and deterministic quantities. For this reason, we use boldface letters to represent random variables and normal font to represent deterministic (non-random) quantities. For example, the boldface letter d denotes a random quantity, while the normal font letter d denotes an observation or realization for it. We also need to distinguish between matrices and vectors. For this purpose, we use CAPITAL letters to refer to matrices and small letters to refer to both vectors and scalars; Greek letters always refer to scalars. For example, we write R to denote a covariance matrix and w to denote a vector of parameters. We also write σ2v to refer to the variance of a random variable. To distinguish between a vector d (small letter) and a scalar d (also a small letter), we use parentheses to index scalar quantities and subscripts to index vector quantities. Thus, we write d(i) to refer to the value of a scalar quantity d at time i, and di to refer to the value of a vector quantity d at time i. Thus, d(i) denotes a scalar while di denotes a vector. All vectors in our presentation are column vectors, with the exception of the regression vector (denoted by the letter u), which will be taken to be a row vector for convenience of presentation. The symbol T denotes transposition, and the symbol ∗ denotes complex conjugation for scalars and complex-conjugate transposition for matrices. The notation col{a, b} denotes a column vector\nwith entries a and b stacked on top of each other, and the notation diag{a, b} denotes a diagonal matrix with entries a and b. Likewise, the notation vec(A) vectorizes its matrix argument and stacks the columns of A on top of each other. The notation ‖x‖ denotes the Euclidean norm of its vector argument, while ‖x‖b,∞ denotes the block maximum norm of a block vector (defined in App. D). Similarly, the notation ‖x‖2Σ denotes the weighted square value, x\n∗Σx. Moreover, ‖A‖b,∞ denotes the block maximum norm of a matrix (also defined in App. D), and ρ(A) denotes the spectral radius of the matrix (i.e., the largest absolute magnitude among its eigenvalues). Finally, IM denotes the identity matrix of size M ×M ; sometimes, for simplicity of notation, we drop the subscript M from IM when the size of the identity matrix is obvious from the context. Table 1 provides a summary of the symbols used in the article for ease of reference."
    }, {
      "heading" : "2 Mean-Square-Error Estimation",
      "text" : "Readers interested in the development of the distributed optimization strategies and their adaptive versions can move directly to Sec. 3. The purpose of the current section is to motivate the virtues of distributed in-network processing, and to provide illustrative examples in the context of mean-square-error estimation. Advanced readers can skip this section on a first reading.\nWe start our development by associating with each agent k an individual cost (or utility) function, Jk(w). Although the algorithms presented in this article apply to more general situations, we nevertheless assume in our presentation that the cost functions Jk(w) are strictly convex so that each one of them has a unique minimizer. We further assume that, for all costs Jk(w), the minimum occurs at the same value wo. Obviously, the choice of Jk(w) is limitless and is largely dependent on the application. It is sufficient for our purposes to illustrate the main concepts underlying diffusion adaptation by focusing on the case of mean-square-error (MSE) or quadratic cost functions. In the sequel, we provide several examples to illustrate how such quadratic cost functions arise in applications and how cooperative processing over networks can be beneficial. At the same time, we note that most of the arguments in this article can be extended beyond MSE optimization to more general cost functions and to situations where the minimizers of the individual costs Jk(w) need not agree with each other — as already shown in [1–3]; see also Sec. 10.4 for a brief summary.\nIn non-cooperative solutions, each agent would operate individually on its own cost function Jk(w) and\noptimize it to determines wo, without any interaction with the other nodes. However, the analysis and derivations in future sections will reveal that nodes can benefit from cooperation between them in terms of better performance (such as converging faster to wo or tracking a changing wo more effectively) — see, e.g., Theorems 6.3–6.5 and Sec. 7.3."
    }, {
      "heading" : "2.1 Application: Autoregressive Modeling",
      "text" : "Our first example relates to identifying the parameters of an auto-regressive (AR) model from noisy data. Thus, consider a situation where agents are spread over some geographical region and each agent k is observing realizations {dk(i)} of an AR zero-mean random process {dk(i)}, which satisfies a model of the form:\ndk(i) = M∑\nm=1\nβmdk(i −m) + vk(i) (4)\nThe scalars {βm} represent the model parameters that the agents wish to identify, and vk(i) represents an additive zero-mean white noise process with power:\nσ2v,k ∆ = E |vk(i)| 2 (5)\nIt is customary to assume that the noise process is temporally white and spatially independent so that noise terms across different nodes are independent of each other and, at the same node, successive noise samples are also independent of each other with a time-independent variance σ2v,k:\n{ Evk(i)v ∗ k(j) = 0, for all i 6= j (temporal whiteness)\nEvk(i)v ∗ m(j) = 0, for all i, j whenever k 6= m (spatial whiteness)\n(6)\nThe noise process vk(i) is further assumed to be independent of past signals {dℓ(i −m),m ≥ 1} across all nodes ℓ. Observe that we are allowing the noise power profile, σ2v,k, to vary with k. In this way, the quality of the measurements is allowed to vary across the network with some nodes collecting noisier data than other nodes. Figure 3 illustrates an example of a network consisting of N = 10 nodes spread over a region in space. The figure shows the neighborhood of node 2, which consists of nodes {1, 2, 3}.\nLinear Model To illustrate the difference between cooperative and non-cooperative estimation strategies, let us first explain that the data can be represented in terms of a linear model. To do so, we collect the model parameters {βm} into an M × 1 column vector wo:\nwo ∆ = col {β1, β2, . . . , βM} (7)\nand the past data into a 1×M row regression vector uk,i:\nuk,i ∆ = [ dk(i− 1) dk(i − 2) . . . dk(i−M) ] (8)\nThen, we can rewrite the measurement equation (4) at each node k in the equivalent linear model form:\ndk(i) = uk,iw o + vk(i) (9)\nLinear relations of the form (9) are common in applications and arise in many other contexts (as further illustrated by the next three examples in this section).\nWe assume the stochastic processes {dk(i),uk,i} in (9) have zero means and are jointly wide-sense stationary. We denote their second-order moments by:\nσ2d,k ∆ = E |dk(i)| 2 (scalar) (10)\nRu,k ∆ = Eu∗k,iuk,i (M ×M) (11)\nrdu,k ∆ = Edk(i)u ∗ k,i (M × 1) (12)\nAs was the case with the noise power profile, we are allowing the moments {σ2d,k, Ru,k, rdu,k} to depend on the node index k so that these moments can vary with the spatial dimension as well. The covariance matrix Ru,k is, by definition, always non-negative definite. However, for convenience of presentation, we shall assume that Ru,k is actually positive-definite (and, hence, invertible):\nRu,k > 0 (13)\nNon-Cooperative Mean-Square-Error Solution One immediate result that follows from the linear model (9) is that the unknown parameter vector wo can be recovered exactly by each individual node from knowledge of the local moments {rdu,k, Ru,k} alone. To see this, note that if we multiply both sides of (9) by u∗k,i and take expectations we obtain\nEu∗k,idk(i)︸ ︷︷ ︸ rdu,k\n= ( Eu∗k,iuk,i ) ︸ ︷︷ ︸\nRu,k\nwo + Eu∗k,ivk(i)︸ ︷︷ ︸ =0\n(14)\nso that\nrdu,k = Ru,k w o ⇐⇒ wo = R−1u,k rdu,k (15)\nIt is seen from (15) that wo is the solution to a linear system of equations and that this solution can be computed by every node directly from its moments {Ru,k, rdu,k}. It is useful to re-interpret construction\n(15) as the solution to a minimum mean-square-error (MMSE) estimation problem [4, 5]. Indeed, it can be verified that the quantity R−1u,k rdu,k that appears in (15) is the unique solution to the following MMSE problem:\nmin w\nE |dk(i)− uk,iw| 2\n(16)\nTo verify this claim, we denote the cost function that appears in (16) by\nJk(w) ∆ = E |dk(i)− uk,iw| 2 (17)\nand expand it to find that\nJk(w) = σ 2 d,k − w ∗rdu,k − r ∗ du,kw + w ∗Ru,kw (18)\nThe cost function Jk(w) is quadratic in w and it has a unique minimizer since Ru,k > 0. Differentiating Jk(w) with respect to w we find its gradient vector:\n∇wJ(w) = (Ru,kw − rdu,k) ∗ (19)\nIt is seen that this gradient vector is annihilated at the same value w = wo given by (15). Therefore, we can equivalently state that if each node k solves the MMSE problem (16), then the solution vector coincides with the desired parameter vector wo. This observation explains why it is often justified to consider mean-squareerror cost functions when dealing with estimation problems that involve data that satisfy linear models similar to (9).\nBesides wo, the solution of the MMSE problem (16) also conveys information about the noise level in the data. Note that by substituting wo from (15) into expression (16) for Jk(w), the resulting minimum mean-square-error value that is attained by node k is found to be:\nMSEk ∆ = Jk(w o)\n= E |dk(i)− uk,iw o|2\n(9) = E |vk(i)| 2 = σ2v,k (20)\n∆ = Jk,min\nWe shall use the notation Jk(w o) and Jk,min interchangeably to denote the minimum cost value of Jk(w). The above result states that, when each agent k recovers wo from knowledge of its moments {Ru,k, rdu,k} using expression (15), then the agent attains an MSE performance level that is equal to the noise power at its location, σ2v,k. An alternative useful expression for the minimum cost can be obtained by substituting expression (15) for wo into (18) and simplifying the expression to find that\nMSEk = σ 2 d,k − r ∗ du,kR −1 u,krdu,k (21)\nThis second expression is in terms of the data moments {σ2d,k, rdu,k, Ru,k}.\nNon-Cooperative Adaptive Solution The optimal MMSE implementation (15) for determining wo requires knowledge of the statistical information {rdu,k, Ru,k}. This information is usually not available beforehand. Instead, the agents are more likely to have access to successive time-indexed observations {dk(i), uk,i} of the random processes {dk(i),uk,i} for i ≥ 0. In this case, it becomes necessary to devise a scheme that would allow each node to use its measurements to approximate wo. It turns out that an adaptive solution is possible. In this alternative implementation,\neach node k feeds its observations {dk(i), uk,i} into an adaptive filter and evaluates successive estimates for wo. As time passes by, the estimates would get closer to wo.\nThe adaptive solution operates as follows. Let wk,i denote an estimate for w o that is computed by node k at time i based on all the observations {dk(j), uk,j , j ≤ i} it has collected up to that time instant. There are many adaptive algorithms that can be used to compute wk,i; some filters are more accurate than others (usually, at the cost of additional complexity) [4–7]. It is sufficient for our purposes to consider one simple (yet effective) filter structure, while noting that most of the discussion in this article can be extended to other structures. One of the simplest choices for an adaptive structure is the least-mean-squares (LMS) filter, where the data are processed by each node k as follows:\nek(i) = dk(i)− uk,iwk,i−1 (22)\nwk,i = wk,i−1 + µku ∗ k,iek(i), i ≥ 0 (23)\nStarting from some initial condition, say, wk,−1 = 0, the filter iterates over i ≥ 0. At each time instant, i, the filter uses the local data {dk(i), uk,i} at node k to compute a local estimation error, ek(i), via (22). The error is then used to update the existing estimate from wk,i−1 to wk,i via (23). The factor µk that appears in (23) is a constant positive step-size parameter; usually chosen to be sufficiently small to ensure mean-square stability and convergence, as discussed further ahead in the article. The step-size parameter can be selected to vary with time as well; one popular choice is to replace µk in (23) with the following construction:\nµk(i) ∆ = µk ǫ+ ‖uk,i‖2\n(24)\nwhere ǫ > 0 is a small positive parameter and µk > 0. The resulting filter implementation is known as normalized LMS [5] since the step-size is normalized by the squared norm of the regression vector. Other choices include step-size sequences {µ(i) ≥ 0} that satisfy both conditions:\n∞∑\ni=0\nµ(i) = ∞, ∞∑\ni=0\nµ2(i) < ∞ (25)\nSuch sequences converge slowly towards zero. One example is the choice µk(i) = µk i+1 . However, by virtue of the fact that such step-sizes die out as i → ∞, then these choices end up turning off adaptation. As such, step-size sequences satisfying (25) are not generally suitable for applications that require continuous learning, especially under non-stationary environments. For this reason, in this article, we shall focus exclusively on the constant step-size case (23) in order to ensure continuous adaptation and learning.\nEquations (22)–(23) are written in terms of the observed quantities {dk(i), uk,i}; these are deterministic values since they correspond to observations of the random processes {dk(i),uk,i}. Often, when we are interested in highlighting the random nature of the quantities involved in the adaptation step, especially when we study the mean-square performance of adaptive filters, it becomes more useful to rewrite the recursions using the boldface notation to highlight the fact that the quantities that appear in (22)–(23) are actually realizations of random variables. Thus, we also write:\nek(i) = dk(i)− uk,iwk,i−1 (26)\nwk,i = wk,i−1 + µku ∗ k,iek(i), i ≥ 0 (27)\nwhere {ek(i),wk,i} will be random variables as well. The performance of adaptive implementations of this kind are well-understood for both cases of stationary wo and changing wo [4–7]. For example, in the stationary case, if the adaptive implementation (26)–(27) were to succeed in having its estimator wk,i tend to w\no with probability one as i → ∞, then we would expect the error signal ek(i) in (26) to tend towards the noise signal vk(i) (by virtue of the linear model (9)). This means that, under this ideal scenario, the variance of the error signal ek(i) would be expected to tend towards the noise variance, σ2v,k, as i → ∞. Recall from (20) that the noise variance is the least cost that the MSE solution can attain. Therefore, such limiting behavior by the adaptive filter would be desirable.\nHowever, it is well-known that there is always some loss in mean-square-error performance when adaptation is employed due to the effect of gradient noise, which is caused by the algorithm’s reliance on observations (or realizations) {dk(i), uk,i} rather than on the actual moments {rdu,k, Ru,k}. In particular, it is known that for LMS filters, the variance of ek(i) in steady-state will be larger than σ 2 v,k by a small amount, and the size of the offset is proportional to the step-size parameter µk (so that smaller step-sizes lead to better mean-square-error (MSE) performance albeit at the expense of slower convergence). It is easy to see why the variance of ek(i) will be larger than σ 2 v,k from the definition of the error signal in (26). Introduce the weight-error vector\nw̃k,i ∆ = wo −wk,i (28)\nand the so-called a-priori error signal\nea,k(i) ∆ = uk,iw̃k,i−1 (29)\nThis second error measures the difference between the uncorrupted term uk,iw o and its estimator prior to adaptation, uk,iwk,i−1. It then follows from the data model (9) and from the defining expression (26) for ek(i) that\nek(i) = dk(i)− uk,iwk,i−1\n= uk,iw o + vk(i)− uk,iwk,i−1\n= ea,k(i) + vk(i) (30)\nSince the noise component, vk(i), is assumed to be zero-mean and independent of all other random variables, we conclude that\nE |ek(i)| 2 = E |ea,k(i)| 2 + σ2v,k (31)\nThis relation holds for any time instant i; it shows that the variance of the output error, ek(i), is larger than σ2v,k by an amount that is equal to the variance of the a-priori error, ea,k(i). We define the filter mean-square-error (MSE) and excess-mean-square-error (EMSE) as the following steady-state measures:\nMSEk ∆ = lim\ni→∞ E |ek(i)|\n2 (32)\nEMSEk ∆ = lim\ni→∞ E |ea,k(i)|\n2 (33)\nso that, for the adaptive implementation (compare with (20)):\nMSEk = EMSEk + σ 2 v,k (34)\nTherefore, the EMSE term quantifies the size of the offset in the MSE performance of the adaptive filter. We also define the filter mean-square-deviation (MSD) as the steady-state measure:\nMSDk ∆ = lim\ni→∞ E‖w̃k,i‖\n2 (35)\nwhich measures how far wk,i is from w o in the mean-square-error sense in steady-state. It is known that the MSD and EMSE of LMS filters of the form (26)–(27) can be approximated for sufficiently small-step sizes by the following expressions [4–7]:\nEMSEk ≈ µkσ 2 v,kTr(Ru,k)/2 (36)\nMSDk ≈ µkσ 2 v,kM/2 (37)\nIt is seen that the smaller the step-size parameter, the better the performance of the adaptive solution.\nCooperative Adaptation through Diffusion Observe from (36)–(37) that even if all nodes employ the same step-size, µk = µ, and even if the regression data are spatially uniform so that Ru,k = Ru for all k, the mean-square-error performance across the nodes still varies in accordance with the variation of the noise power profile, σ2v,k, across the network. Nodes with larger noise power will perform worse than nodes with smaller noise power. However, since all nodes are observing data arising from the same underlying model wo, it is natural to expect cooperation among the nodes to be beneficial. By cooperation we mean that neighboring nodes can share information (such as measurements or estimates) with each other as permitted by the network topology. Starting in the next section, we will motivate and describe algorithms that enable nodes to carry out adaptation and learning in a cooperative manner to enhance performance.\nSpecifically, we are going to see that one way to achieve cooperation is by developing adaptive algorithms that enable the nodes to optimize the following global cost function in a distributed manner:\nmin w\nN∑\nk=1\nE |dk(i)− uk,iw| 2 (38)\nwhere the global cost is the aggregate objective:\nJglob(w) ∆ =\nN∑\nk=1\nE |dk(i)− uk,iw| 2 =\nN∑\nk=1\nJk(w) (39)\nComparing (38) with (16), we see that we are now adding the individual costs, Jk(w), from across all nodes. Note that since the desired wo satisfies (15) at every node k, then it also satisfies\n( M∑\nk=1\nRu,k\n) wo = N∑\nk=1\nrdu,k (40)\nBut it can be verified that the optimal solution to (38) is given by the same wo that satisfies (40). Therefore, solving the global optimization problem (38) also leads to the desired wo. In future sections, we will show how cooperative and distributed adaptive schemes for solving (38), such as (153) or (154) further ahead, lead to improved performance in estimating wo (in terms of smaller mean-square-deviation and faster convergence rate) than the non-cooperative mode (26)–(27), where each agent runs its own individual adaptive filter — see, e.g., Theorems 6.3–6.5 and Sec. 7.3."
    }, {
      "heading" : "2.2 Application: Tapped-Delay-Line Models",
      "text" : "Our second example to motivate MSE cost functions, Jk(w), and linear models relates to identifying the parameters of a moving-average (MA) model from noisy data. MA models are also known as finite-impulseresponse (FIR) or tapped-delay-line models. Thus, consider a situation where agents are interested in estimating the parameters of an FIR model, such as the taps of a communications channel or the parameters of some (approximate) model of interest in finance or biology. Assume the agents are able to independently probe the unknown model and observe its response to excitations in the presence of additive noise; this situation is illustrated in Fig. 4, with the probing operation highlighted for one of the nodes (node 4).\nThe schematics inside the enlarged diagram in Fig. 4 is meant to convey that each node k probes the model with an input sequence {uk(i)} and measures the resulting response sequence, {dk(i)}, in the presence\nof additive noise. The system dynamics for each agent k is assumed to be described by a MA model of the form:\ndk(i) =\nM−1∑\nm=0\nβmuk(i−m) + vk(i) (41)\nIn this model, the term vk(i) again represents an additive zero-mean noise process that is assumed to be temporally white and spatially independent; it is also assumed to be independent of the input process, {uℓ(j)}, for all i, j and ℓ. The scalars {βm} represent the model parameters that the agents seek to identify. If we again collect the model parameters into an M × 1 column vector wo:\nwo ∆ = col {β0, β1, . . . , βM−1} (42)\nand the input data into a 1×M row regression vector:\nuk,i ∆ = [ uk(i) uk(i − 1) . . . uk(i−M + 1) ] (43)\nthen, we can again express the measurement equation (41) at each node k in the same linear model as (9), namely,\ndk(i) = uk,iw o + vk(i) (44)\nAs was the case with model (9), we can likewise verify that, in view of (44), the desired parameter vector wo satisfies the same normal equations (15), i.e.,\nrdu,k = Ru,k w o ⇐⇒ wo = R−1u,k rdu,k (45)\nwhere the moments {rdu,k, Ru,k} continue to be defined by expressions (11)–(12) with uk,i now defined by (43). Therefore, each node k can determine wo on its own by solving the same MMSE estimation problem\n(16). This solution method requires knowledge of the moments {rdu,k, Ru,k} and, according to (20), each agent k would then attain an MSE level that is equal to the noise power level at its location.\nAlternatively, when the statistical information {rdu,k, Ru,k} is not available, each agent k can estimate wo iteratively by feeding data {dk(i),uk,i} into the adaptive implementation (26)–(27). In this way, each agent k will achieve the same performance level shown earlier in (36)–(37), with the limiting performance being again dependent on the local noise power level, σ2v,k. Therefore, nodes with larger noise power will perform worse than nodes with smaller noise power. However, since all nodes are observing data arising from the same underlying model wo, it is natural to expect cooperation among the nodes to be beneficial. As we are going to see, starting from the next section, one way to achieve cooperation and improve performance is by developing algorithms that optimize the same global cost function (38) in an adaptive and distributed manner, such as algorithms (153) and (154) further ahead."
    }, {
      "heading" : "2.3 Application: Target Localization",
      "text" : "Our third example relates to the problem of locating a destination of interest (such as the location of a nutrition source or a chemical leak) or locating and tracking an object of interest (such as a predator or a projectile). In several such localization applications, the agents in the network are allowed to move towards the target or away from it, in which case we would end up with a mobile adaptive network [8]. Biological networks behave in this manner such as networks representing fish schools, bird formations, bee swarms, bacteria motility, and diffusing particles [8–12]. The agents may move towards the target (e.g., when it is a nutrition source) or away from the target (e.g., when it is a predator). In other applications, the agents may remain static and are simply interested in locating a target or tracking it (such as tracking a projectile).\nTo motivate mean-square-error estimation in the context of localization problems, we start with the situation corresponding to a static target and static nodes. Thus, assume that the unknown location of the target in the cartesian plane is represented by the 2× 1 vector wo = col{xo, yo}. The agents are spread over the same region of space and are interested in locating the target. The location of every agent k is denoted by the 2× 1 vector pk = col{xk, yk} in terms of its x and y coordinates – see Fig. 5. We assume the agents are aware of their location vectors. The distance between agent k and the target is denoted by rok and is equal to:\nrok = ‖w o − pk‖ (46)\nThe 1× 2 unit-norm direction vector pointing from agent k towards the target is denoted by uok and is given by:\nuok = (wo − pk)T\n‖wo − pk‖ (47)\nObserve from (46) and (47) that rok can be expressed in the following useful inner-product form:\nrok = u o k(w o − pk) (48)\nIn practice, agents have noisy observations of both their distance and direction vector towards the target. We denote the noisy distance measurement collected by node k at time i by:\nrk(i) = r o k + vk(i) (49)\nwhere vk(i) denotes noise and is assumed to be zero-mean, and temporally white and spatially independent with variance\nσ2v,k ∆ = E |vk(i)| 2 (50)\nWe also denote the noisy direction vector that is measured by node k at time i by uk,i. This vector is a perturbed version of uok. We assume that uk,i continues to start from the location of the node at pk, but\nthat its tip is perturbed slightly either to the left or to the right relative to the tip of uok — see Fig. 6. The perturbation to the tip of uok is modeled as being the result of two effects: a small deviation that occurs along the direction that is perpendicular to uok, and a smaller deviation that occurs along the direction of uok. Since we are assuming that the tip of uk,i is only slightly perturbed relative to the tip of u o k, then it is reasonable to expect the amount of perturbation along the parallel direction to be small compared to the amount of perturbation along the perpendicular direction.\nThus, we write\nuk,i = u o k + αk(i) u o⊥ k + βk(i) u o k (1× 2) (51)\nwhere uo⊥k denotes a unit-norm row vector that lies in the same plane and whose direction is perpendicular to uok. The variables αk(i) and βk(i) denote zero-mean independent random noises that are temporally white and spatially independent with variances:\nσ2α,k ∆ = E |αk(i)| 2, σ2β,k ∆ = E |βk(i)| 2 (52)\nWe assume the contribution of βk(i) is small compared to the contributions of the other noise sources, αk(i) and vk(i), so that\nσ2β,k ≪ σ 2 α,k, σ 2 β,k ≪ σ 2 v,k (53)\nThe random noises {vk(i),αk(i),βk(i)} are further assumed to be independent of each other. Using (48) we find that the noisy measurements {rk(i),uk,i} are related to the unknown wo via:\nrk(i) = uk,i(w o − pk) + zk(i) (54)\nwhere the modified noise term zk(i) is defined in terms of the noises in rk(i) and uk,i as follows:\nzk(i) ∆ = vk(i) − αk(i) u o⊥ k · (w o − pk) − βk(i) · u o k · (w o − pk)\n= vk(i) − βk(i) · r o k ≈ vk(i) (55)\nsince, by construction, uo⊥k · (w o − pk) = 0 (56)\nand the contribution by βk(i) is assumed to be sufficiently small. If we now introduce the adjusted signal:\ndk(i) ∆ = rk(i) + uk,i pk (57)\nthen we arrive again from (54) and (55) at the following linear model for the available measurement variables {dk(i),uk,i} in terms of the target location wo:\ndk(i) ≈ uk,iw o + vk(i) (58)\nThere is one important difference in relation to the earlier linear models (9) and (44), namely, the variables {dk(i),uk,i} in (58) do not have zero means any longer. It is nevertheless straightforward to determine the first and second-order moments of the variables {dk(i),uk,i}. First, note from (49), (51), and (57) that\nEuk,i = u o k, Edk(i) = r o k + u o kpk (59)\nEven in this case of non-zero means, and in view of (58), the desired parameter vector wo can still be shown to satisfy the same normal equations (15), i.e.,\nrdu,k = Ru,kw o ⇐⇒ wo = R−1u,k rdu,k (60)\nwhere the moments {rdu,k, Ru,k} continue to be defined as\nRu,k ∆ = Eu∗k,iuk,i, rdu,k ∆ = Eu∗k,idk(i) (61)\nTo verify that (60) holds, we simply multiply both sides of (58) by u∗k,i from the left, compute the expectations of both sides, and use the fact that vk(i) has zero mean and is assumed to be independent of {uℓ,j} for\nall times j and nodes ℓ. However, the difference in relation to the earlier normal equations (15) is that the matrix Ru,k is not the actual covariance matrix of uk,i any longer. When uk,i is not zero mean, its covariance matrix is instead defined as:\ncovu,k ∆ = E (uk,i − u o k) ∗(uk,i − u o k)\n= Eu∗k,iuk,i − u o∗ k u o k (62)\nso that Ru,k = covu,k + u o∗ k u o k (63)\nWe conclude from this relation that Ru,k is positive-definite (and, hence, invertible) so that expression (60) is justified. This is because the covariance matrix, covu,k, is itself positive-definite. Indeed, some algebra applied to the difference uk,i − u o k from (51) shows that\ncovu,k = [ ( uo⊥k )∗ (uok) ∗ ]\n[ σ2α,k\nσ2β,k ] [ uo⊥k uok ] (64)\nwhere the matrix [ uo⊥k uok ] (65)\nis full rank since the rows {uok, u o⊥ k } are linearly independent vectors.\nTherefore, each node k can determine wo on its own by solving the same minimum mean-square-error estimation problem (16). This solution method requires knowledge of the moments {rdu,k, Ru,k} and, according to (20), each agent k would then attain an MSE level that is equal to the noise power level, σ2v,k, at its location.\nAlternatively, when the statistical information {rdu,k, Ru,k} is not available beforehand, each agent k can estimate wo iteratively by feeding data {dk(i),uk,i} into the adaptive implementation (26)–(27). In this case, each agent k will achieve the performance level shown earlier in (36)–(37), with the limiting performance being again dependent on the local noise power level, σ2v,k. Therefore, nodes with larger noise power will perform worse than nodes with smaller noise power. However, since all nodes are observing distances and direction vectors towards the same target location wo, it is natural to expect cooperation among the nodes to be beneficial. As we are going to see, starting from the next section, one way to achieve cooperation and improve performance is by developing algorithms that solve the same global cost function (38) in an adaptive and distributed manner, by using algorithms such as (153) and (154) further ahead.\nRole of Adaptation The localization application helps highlight one of the main advantages of adaptation, namely, the ability of adaptive implementations to learn and track changing statistical conditions. For example, in the context of mobile networks, where nodes can move closer or further away from a target, the location vector for each agent k becomes time-dependent, say, pk,i = col{xk(i), yk(i)}. In this case, the actual distance and direction vector between agent k and the target also vary with time and become:\nrok(i) = ‖w o − pk,i‖, u o k,i =\n(wo − pk,i) T\n‖wo − pk,i‖ (66)\nThe noisy distance measurement to the target is then:\nrk(i) = r o k(i) + vk(i) (67)\nwhere the variance of vk(i) now depends on time as well:\nσ2v,k(i) ∆ = E |vk(i)| 2 (68)\nIn the context of mobile networks, it is reasonable to assume that the variance of vk(i) varies both with time and with the distance to the target: the closer the node is to the target, the less noisy the measurement of the distance is expected to be. Similar remarks hold for the variances of the noises αk(i) and βk(i) that perturb the measurement of the direction vector, say,\nσ2α,k(i) ∆ = E |αk(i)| 2, σ2β,k(i) ∆ = E |βk(i)| 2 (69)\nwhere now\nuk,i = u o k,i + αk(i) u o⊥ k,i + βk(i) u o k,i (70)\nThe same arguments that led to (58) can be repeated to lead to the same model, except that now the means of the variables {dk(i),uk,i} become time-dependent as well:\nEuk,i = u o k,i, Edk(i) = r o k(i) + u o k,i pk,i (71)\nNevertheless, adaptive solutions (whether cooperative or non-cooperative), are able to track such timevariations because these solutions work directly with the observations {dk(i), uk,i} and the successive observations will reflect the changing statistical profile of the data. In general, adaptive solutions are able to track changes in the underlying signal statistics rather well [4, 5], as long as the rate of non-stationarity is slow enough for the filter to be able to follow the changes."
    }, {
      "heading" : "2.4 Application: Collaborative Spectral Sensing",
      "text" : "Our fourth and last example to illustrate the role of mean-square-error estimation and cooperation relates to spectrum sensing for cognitive radio applications. Cognitive radio systems involve two types of users: primary users and secondary users. To avoid causing harmful interference to incumbent primary users, unlicensed cognitive radio devices need to detect unused frequency bands even at low signal-to-noise (SNR) conditions [13–16]. One way to carry out spectral sensing is for each secondary user to estimate the aggregated power spectrum that is transmitted by all active primary users, and to locate unused frequency bands within the estimated spectrum. This step can be performed by the secondary users with or without cooperation.\nThus, consider a communications environment consisting of Q primary users and N secondary users. Let Sq(e\njω) denote the power spectrum of the signal transmitted by primary user q. To facilitate estimation of the spectral profile by the secondary users, we assume that each Sq(e\njω) can be represented as a linear combination of basis functions, {fm(ejω)}, say, B of them [17]:\nSq(e jω) =\nB∑\nm=1\nβqmfm(e jω), q = 1, 2, . . . , Q (72)\nIn this representation, the scalars {βqm} denote the coefficients of the basis expansion for user q. The variable ω ∈ [−π, π] denotes the normalized angular frequency measured in radians/sample. The power spectrum is often symmetric about the vertical axis, ω = 0, and therefore it is sufficient to focus on the interval ω ∈ [0, π]. There are many ways by which the basis functions, {fm(ejω)}, can be selected. The following is one possible construction for illustration purposes. We divide the interval [0, π] into B identical intervals and denote their center frequencies by {ωm}. We then place a Gaussian pulse at each location ωm and control its width through the selection of its standard deviation, σm, i.e.,\nfm(e jω) ∆ = e\n− (ω−ωm) 2\nσ2m (73)\nFigure 7 illustrates this construction. The parameters {ωm, σm} are selected by the designer and are assumed to be known. For a sufficiently large number, B, of basis functions, the representation (72) can approximate well a large class of power spectra.\nWe collect the combination coefficients {βqm} for primary user q into a column vector wq:\nwq ∆ = col {βq1, βq2, βq3, . . . , βqB} (B × 1) (74)\nand collect the basis functions into a row vector:\nfω ∆ = [ f1(e jω) f2(e jω) . . . fB(e jω) ]\n(1×B) (75)\nThen, the power spectrum (72) can be expressed in the alternative inner-product form:\nSq(e jω) = fωwq (76)\nLet pqk denote the path loss coefficient from primary user q to secondary user k. When the transmitted spectrum Sq(e\njω) travels from primary user q to secondary user k, the spectrum that is sensed by node k is pqkSq(e\njω). We assume in this example that the path loss factors {pqk} are known and that they have been determined during a prior training stage involving each of the primary users with each of the secondary users. The training is usually repeated at regular intervals of time to accommodate the fact that the path loss coefficients can vary (albeit slowly) over time. Figure 8 depicts a cognitive radio system with 2 primary users and 10 secondary users. One of the secondary users (user 5) is highlighted and the path loss coefficients from the primary users to its location are indicated; similar path loss coefficients can be assigned to all other combinations involving primary and secondary users.\nEach user k senses the aggregate effect of the power spectra that are transmitted by all active primary users. Therefore, adding the effect of all primary users, we find that the power spectrum that arrives at secondary user k is given by:\nSk(e jω) =\nQ∑\nq=1\npqkSq(e jω) + σ2k\n=\nQ∑\nq=1\npqkfωwq + σ 2 k\n∆ = uk,ωw o + σ2k (77)\nwhere σ2k denotes the receiver noise power at node k, and where we introduced the following vector quantities:\nwo ∆ = col{w1, w2, . . . , wQ} (BQ × 1) (78)\nuk,ω ∆ = [ p1kfω p2kfω . . . pQkfω ] (1×BQ) (79)\nThe vector wo is the collection of all combination coefficients for all Q primary users. The vector uk,ω contains the path loss coefficients from all primary users to user k. Now, at every time instant i, user k observers its received power spectrum, Sk(e\njω), over a discrete grid of frequencies, {ωr}, in the interval [0, π] in the presence of additive measurement noise. We denote these measurements by:\ndk,r(i) = uk,ωrw o + σ2k + vk,r(i)\nr = 1, 2, . . . , R (80)\nThe term vk,r(i) denotes sampling noise and is assumed to have zero mean and variance σ 2 v,k; it is also assumed to be temporally white and spatially independent; and is also independent of all other random variables. Since the row vectors uk,ω in (79) are defined in terms of the path loss coefficients {pqk}, and since these coefficients are estimated and subject to noisy distortions, we model the uk,ωr as zero-mean random variables in (80) and use the boldface notation for them.\nObserve that in this application, each node k collects R measurements at every time instant i and not only a single measurement, as was the case with the three examples discussed in the previous sections (AR modeling, MA modeling, and localization). The implication of this fact is that we now deal with an estimation problem that involves vector measurements instead of scalar measurements at each node. The solution structure continues to be the same. We collect the R measurements at node k at time i into vectors and introduce the R× 1 quantities:\ndk,i ∆ =   dk,1(i)− σ2k dk,2(i)− σ2k\n... dk,R(i)− σ2k\n  , vk,i ∆ =   vk,1(i) vk,2(i)\n... vk,R(i)\n  (81)\nand the regression matrix:\nUk,i ∆ =   uk,ω1 uk,ω2\n... uk,ωR\n  (R×QB) (82)\nThe time subscript in Uk,i is used to model the fact that the path loss coefficients can change over time due to the possibility of node mobility. With the above notation, expression (80) is equivalent to the linear model:\ndk,i = Uk,iw o + vk,i (83)\nCompared to the earlier examples (9), (44), and (58), the main difference now is that each agent k collects a vector of measurements, dk,i, as opposed to the scalar dk(i), and its regression data are represented by the matrix quantity, Uk,i, as opposed to the row vector uk,i. Nevertheless, the estimation approach will continue to be the same. In cognitive network applications, the secondary users are interested in estimating the aggregate power spectrum of the primary users in order for the secondary users to identify vacant frequency bands that can be used by them. In the context of model (83), this amounts to determining the parameter vector wo since knowledge of its entries allows each secondary user to reconstruct the aggregate power spectrum defined by:\nSA(e jω) ∆ =\nQ∑\nq=1\nSq(e jω) = (1TQ ⊗ fω)w o (84)\nwhere the notation ⊗ denotes the Kronecker product operation, and 1Q denotes a Q×1 vector whose entries are all equal to one.\nAs before, we can again verify that, in view of (83), the desired parameter vector wo satisfies the same normal equations:\nRdU,k = RU,kw o ⇐⇒ wo = R−1U,k RdU,k (85)\nwhere the moments {RdU,k, RU,k} are now defined by\nRdU,k ∆ = EU∗k,idk,i (QB × 1) (86)\nRU,k ∆ = EU∗k,iUk,i (QB ×QB) (87)\nTherefore, each secondary user k can determine wo on its own by solving the following minimum meansquare-error estimation problem:\nmin w\nE ‖dk,i −Uk,iw‖ 2\n(88)\nThis solution method requires knowledge of the moments {RdU,k, RU,k} and, in an argument similar to the one that led to (20), it can be verified that each agent k would attain an MSE performance level that is equal to the noise power level, σ2v,k, at its location.\nAlternatively, when the statistical information {RdU,k, RU,k} is not available, each secondary user k can estimate wo iteratively by feeding data {dk,i,Uk,i} into an adaptive implementation similar to (26)–(27), such as the following vector LMS recursion:\nek,i = dk,i −Uk,iwk,i−1 (89)\nwk,i = wk,i−1 + µkU ∗ k,iek,i (90)\nIn this case, each secondary user k will achieve the same performance levels shown earlier in (36)–(37) with Ru,k replaced by RU,k. The performance will again be dependent on the local noise level, σ 2 v,k. As a result, secondary users with larger noise power will perform worse than secondary users with smaller noise power. However, since all secondary users are observing data arising from the same underlying model wo, it is natural to expect cooperation among the users to be beneficial. As we are going to see, starting from the\nnext section, one way to achieve cooperation and improve performance is by developing algorithms that solve the following global cost function in an adaptive and distributed manner:\nmin w\nN∑\nk=1\nE ‖dk,i −Uk,iw‖ 2\n(91)"
    }, {
      "heading" : "3 Distributed Optimization via Diffusion Strategies",
      "text" : "The examples in the previous section were meant to illustrate how MSE cost functions and linear models are useful design tools and how they arise frequently in applications. We now return to problem (1) and study the distributed optimization of global cost functions such as (39), where Jglob(w) is assumed to consist of the sum of individual components. Specifically, we are now interested in solving optimization problems of the type:\nmin w\nN∑\nk=1\nJk(w) (92)\nwhere each Jk(w) is assumed to be differentiable and convex over w. Although the algorithms presented in this article apply to more general situations, we shall nevertheless focus on mean-square-error cost functions of the form:\nJk(w) ∆ = E |dk(i)− uk,iw| 2 (93)\nwhere w is an M × 1 column vector, and the random processes {dk(i),uk,i} are assumed to be jointly wide-sense stationary with zero-mean and second-order moments:\nσ2d,k ∆ = E |dk(i)| 2 (94)\nRu,k ∆ = Eu∗k,iuk,i > 0 (M ×M) (95)\nrdu,k ∆ = Edk(i)u ∗ k,i (M × 1) (96)\nIt is clear that each Jk(w) is quadratic in w since, after expansion, we get\nJk(w) = σ 2 d,k − w ∗rdu,k − r ∗ du,k w + w ∗Ru,k w (97)\nA completion-of-squares argument shows that Jk(w) can be expressed as the sum of two squared terms, i.e.,\nJk(w) = ( σ2d,k − r ∗ du,kR −1 u,krdu,k ) + (w − wo)∗Ru,k(w − w o) (98)\nor, more compactly,\nJk(w) = Jk,min + ‖w − w o‖2Ru,k (99)\nwhere wo denotes the minimizer of Jk(w) and is given by\nwo ∆ = R−1u,k rdu,k (100)\nand Jk,min denotes the minimum value of Jk(w) when evaluated at w = w o:\nJk,min ∆ = σ2d,k − r ∗ du,kR −1 u,krdu,k = Jk(w o) (101)\nObserve that this value is necessarily non-negative since it can be viewed as the Schur complement of the following covariance matrix:\nE ([ d∗k(i) u∗k,i ] [ dk(i) uk,i ]) = [ σ2d,k r ∗ du,k rdu,k Ru,k ] (102)\nand covariance matrices are nonnegative-definite. The choice of the quadratic form (93) or (97) for Jk(w) is useful for many applications, as was already illustrated in the previous section for examples involving AR modeling, MA modeling, localization, and spectral sensing. Other choices for Jk(w) are of course possible and these choices can even be different for different nodes. It is sufficient in this article to illustrate the main concepts underlying diffusion adaptation by focusing on the useful case of MSE cost functions of the form (97); still, most of the derivations and arguments in the coming sections can be extended beyond MSE optimization to more general cost functions — as already shown in [1–3]; see also Sec. 10.4.\nThe positive-definiteness of the covariance matrices {Ru,k} ensures that each Jk(w) in (97) is strictly convex, as well as Jglob(w) from (39). Moreover, all these cost functions have a unique minimum at the same wo, which satisfies the normal equations:\nRu,k w o = rdu,k, for every k = 1, 2, . . . , N (103)\nTherefore, given knowledge of {rdu,k, Ru,k}, each node can determine wo on its own by solving (103). One then wonders about the need to seek distributed cooperative and adaptive solutions. There are a couple of reasons:\n(a) First, even for MSE cost functions, it is often the case that the required moments {rdu,k, Ru,k} are not known beforehand. In this case, the optimal wo cannot be determined from the solution of the normal equations (103). The alternative methods that we shall describe will lead to adaptive techniques that enable each node k to estimate wo directly from data realizations.\n(b) Second, since adaptive strategies rely on instantaneous data, these strategies possess powerful tracking abilities. Even when the moments vary with time due to non-stationary behavior (such as wo changing with time), these changes will be reflected in the observed data and will in turn influence the behavior of the adaptive construction. This is one of the key advantages of adaptive strategies: they enable learning and tracking in real-time.\n(c) Third, cooperation among nodes is generally beneficial. When nodes act individually, their performance is limited by the noise power level at their location. In this way, some nodes can perform significantly better than other nodes. On the other hand, when nodes cooperate with their neighbors and share information during the adaptation process, we will see that performance can be improved across the network."
    }, {
      "heading" : "3.1 Relating the Global Cost to Neighborhood Costs",
      "text" : "Let us therefore consider the optimization of the following global cost function:\nJglob(w) =\nN∑\nk=1\nJk(w) (104)\nwhere Jk(w) is given by (93) or (97). Our strategy to optimize J glob(w) in a distributed manner is based on two steps, following the developments in [1, 2, 18]. First, using a completion-of-squares argument (or, equivalently, a second-order Taylor series expansion), we approximate the global cost function (104) by an alternative local cost that is amenable to distributed optimization. Then, each node will optimize the alternative cost via a steepest-descent method.\nTo motivate the distributed diffusion-based approach, we start by introducing a set of nonnegative coefficients {ckℓ} that satisfy two conditions:\nfor k = 1, 2, . . . , N :\nckℓ ≥ 0, N∑\nℓ=1\nckℓ = 1, and ckℓ = 0 if ℓ /∈ Nk (105)\nwhere Nk denotes the neighborhood of node k. Condition (105) means that for every node k, the sum of the coefficients {ckℓ} that relate it to its neighbors is one. The coefficients {ckℓ} are free parameters that are chosen by the designer; obviously, as shown later in Theorem 6.8, their selection will have a bearing on the performance of the resulting algorithms. If we collect the entries {ckℓ} into an N ×N matrix C, so that the k−th row of C is formed of {ckℓ, ℓ = 1, 2, . . . , N}, then condition (105) translates into saying that each of row of C adds up to one, i.e.,\nC1 = 1 (106)\nwhere the notation 1 denotes an N × 1 column vector with all its entries equal to one:\n1 ∆ = col{1, 1, . . . , 1} (107)\nWe say that C is a right stochastic matrix. Using the coefficients {ckℓ} so defined, we associate with each node ℓ, a local cost function of the following form:\nJ locℓ (w) ∆ =\n∑\nk∈Nℓ\nckℓ Jk(w) (108)\nThis cost consists of a weighted combination of the individual costs of the neighbors of node ℓ (including ℓ itself) — see Fig. 9. Since the {ckℓ} are all nonnegative and each Jk(w) is strictly convex, then J locℓ (w) is also strictly convex and its minimizer occurs at the same w = wo. Using the alternative representation (99) for the individual Jk(w), we can re-express the local cost J loc ℓ (w) as\nJ locℓ (w) = ∑\nk∈Nℓ\nckℓ Jk,min + ∑\nk∈Nℓ\nckℓ ‖w − w o‖2Ru,k (109)\nor, equivalently,\nJ locℓ (w) = J loc ℓ,min + ‖w − w o‖2Rℓ (110)\nwhere J locℓ,min corresponds to the minimum value of J loc ℓ (w) at the minimizer w = w o:\nJ locℓ,min ∆ =\n∑\nk∈Nℓ\nckℓ Jk,min (111)\nand Rℓ is a positive-definite weighting matrix defined by:\nRℓ ∆ =\n∑\nk∈Nℓ\nckℓRu,k (112)\nThat is, Rℓ is a weighted combination of the covariance matrices in the neighborhood of node ℓ. Equality (110) amounts to a (second-order) Taylor series expansion of J locℓ (w) around w = w\no. Note that the righthand side consists of two terms: the minimum cost and a weighted quadratic term in the difference (w−wo).\nNow note that we can express Jglob(w) from (104) as follows:\nJglob(w) (105) =\nN∑\nk=1\n( N∑\nℓ=1\nckℓ ) Jk(w)\n=\nN∑\nℓ=1\n( N∑\nk=1\nckℓ Jk(w)\n)\n(108) =\nN∑\nℓ=1\nJ locℓ (w)\n= J lock (w) +\nN∑\nℓ 6=k\nJ locℓ (w) (113)\nSubstituting (110) into the second term on the right-hand side of the above expression gives:\nJglob(w) = J lock (w) + ∑\nℓ 6=k\n‖w − wo‖2Rℓ + ∑\nℓ 6=k\nJ locℓ,min (114)\nThe last term in the above expression does not depend on w. Therefore, minimizing Jglob(w) over w is equivalent to minimizing the following alternative global cost:\nJglob ′ (w) = J lock (w) + ∑\nℓ 6=k\n‖w − wo‖2Rℓ (115)\nExpression (115) relates the optimization of the original global cost function, Jglob(w) or its equivalent Jglob ′\n(w), to the newly-introduced local cost function J lock (w). The relation is through the second term on the right-hand side of (115), which corresponds to a sum of quadratic factors involving the minimizer wo; this term tells us how the local cost J lock (w) can be corrected to the global cost J\nglob′(w). Obviously, the minimizer wo that appears in the correction term is not known since the nodes wish to determine its value. Likewise, not all the weighting matrices Rℓ are available to node k; only those matrices that originate from its neighbors can be assumed to be available. Still, expression (115) suggests a useful way to replace J lock by another local cost that is closer to Jglob ′\n(w). This alternative cost will be shown to lead to a powerful distributed solution to optimize Jglob(w) through localized interactions.\nOur first step is to limit the summation on the right-hand side of (115) to the neighbors of node k (since every node k can only have access to information from its neighbors). We thus introduce the modified cost function at node k:\nJglob ′ k (w) ∆ = J lock (w) +\n∑\nℓ∈Nk\\{k}\n‖w − wo‖2Rℓ (116)\nThe cost functions J lock (w) and J glob′\nk (w) are both associated with node k; the difference between them is that the expression for the latter is closer to the global cost function (115) that we want to optimize.\nThe weighting matrices {Rℓ} that appear in (116) may or may not be available because the secondorder moments {Ru,ℓ} may or may not be known beforehand. If these moments are known, then we can proceed with the analysis by assuming knowledge of the {Rℓ}. However, the more interesting case is when these moments are not known. This is generally the case in practice, especially in the context of adaptive solutions and problems involving non-stationary data. Often, nodes can only observe realizations {uℓ,i} of the regression data {uℓ,i} arising from distributions whose covariance matrices are the unknown {Ru,ℓ}. One way to address the difficulty is to replace each of the weighted norms ‖w − wo‖2Rℓ in (116) by a scaled multiple of the un-weighted norm, say,\n‖w − wo‖2Rℓ ≈ bℓk · ‖w − w o‖2 (117)\nwhere bℓk is some nonnegative coefficient; we are even allowing its value to change with the node index k. The above substitution amounts to having each node k approximate the {Rℓ} from its neighbors by multiples of the identity matrix\nRℓ ≈ bℓk IM (118)\nApproximation (117) is reasonable in view of the fact that all vector norms are equivalent [19–21]; this norm property ensures that we can bound the weighted norm ‖w − wo‖2Rℓ by some constants multiplying the un-weighted norm ‖w − wo‖2, say, as:\nr1‖w − w o‖2 ≤ ‖w − wo‖2Rℓ ≤ r2‖w − w o‖2 (119)\nfor some positive constants (r1, r2). Using the fact that the {Rℓ} are Hermitian positive-definite matrices, and calling upon the Rayleigh-Ritz characterization of eigenvalues [19, 20], we can be more specific and replace the above inequalities by\nλmin(Rℓ) · ‖w − w o‖2 ≤ ‖w − wo‖2Rℓ ≤ λmax(Rℓ) · ‖w − w o‖2 (120)\nWe note that approximations similar to (118) are common in stochastic approximation theory and they mark the difference between using a Newton’s iterative method or a stochastic gradient method [5, 22]; the former uses Hessian matrices as approximations for Rℓ and the latter uses multiples of the identity matrix. Furthermore, as the derivation will reveal, we do not need to worry at this stage about how to select the scalars {bℓk}; they will end up being embedded into another set of coefficients {aℓk} that will be set by the designer or adjusted by the algorithm — see (132) further ahead.\nThus, we replace (116) by\nJglob ′′\nk (w) = J loc k (w) +\n∑\nℓ∈Nk\\{k}\nbℓk ‖w − w o‖2 (121)\nThe argument so far has suggested how to modify J lock (w) from (108) and replace it by the cost (121) that is closer in form to the global cost function (115). If we replace J lock (w) by its definition (108), we can rewrite (121) as\nJglob ′′ k (w) = ∑\nℓ∈Nk\ncℓk Jℓ(w) + ∑\nℓ∈Nk\\{k}\nbℓk ‖w − w o‖2 (122)\nWith the exception of the variable wo, this approximate cost at node k relies solely on information that is available to node k from its neighborhood. We will soon explain how to handle the fact that wo is not known beforehand to node k."
    }, {
      "heading" : "3.2 Steepest-Descent Iterations",
      "text" : "Node k can apply a steepest-descent iteration to minimize Jglob ′′\nk (w). Let wk,i denote the estimate for the minimizer wo that is evaluated by node k at time i. Starting from an initial condition wk,−1, node k can compute successive estimates iteratively as follows:\nwk,i = wk,i−1 − µk [ ∇wJ glob′′ k (wk,i−1) ]∗ , i ≥ 0 (123)\nwhere µk is a small positive step-size parameter, and the notation ∇wJ(a) denotes the gradient vector of the function J(w) relative to w and evaluated at w = a. The step-size parameter µk can be selected to vary with time as well. One choice that is common in the optimization literature [5,22,52] is to replace µk in (123) by step-size sequences {µ(i) ≥ 0} that satisfy the two conditions (25). However, such step-size sequences are not suitable for applications that require continuous learning because they turn off adaptation as i → ∞; the steepest-descent iteration (123) would stop updating since µk(i) would be tending towards zero. For this reason, we shall focus mainly on the constant step-size case described by (123) since we are interested in developing distributed algorithms that will endow networks with continuous adaptation abilities.\nReturning to (123) and computing the gradient vector of (122) we get:\nwk,i = wk,i−1 − µk ∑\nℓ∈Nk\ncℓk [∇wJℓ(wk,i−1)] ∗ − µk\n∑\nℓ∈Nk\\{k}\nbℓk (wk,i−1 − w o) (124)\nUsing the expression for Jℓ(w) from (97) we arrive at\nwk,i = wk,i−1 + µk ∑\nℓ∈Nk\ncℓk (rdu,ℓ −Ru,ℓ wk,i−1) + µk ∑\nℓ∈Nk\\{k}\nbℓk (w o − wk,i−1) (125)\nThis iteration indicates that the update from wk,i−1 to wk,i involves adding two correction terms to wk,i−1. Among many other forms, we can implement the update in two successive steps by adding one correction term at a time, say, as follows:\nψk,i = wk,i−1 + µk ∑\nℓ∈Nk\ncℓk (rdu,ℓ −Ru,ℓ wk,i−1) (126)\nwk,i = ψk,i + µk ∑\nℓ∈Nk\\{k}\nbℓk (w o − wk,i−1) (127)\nStep (126) updates wk,i−1 to an intermediate value ψk,i by using local gradient vectors from the neighborhood of node k. Step (127) further updates ψk,i to wk,i. However, this second step is not realizable since w\no is not known and the nodes are actually trying to estimate it. Two issues stand out from examining (127):\n(a) First, iteration (127) requires knowledge of the minimizer wo. Neither node k nor its neighbors know the value of the minimizer; each of these nodes is actually performing steps similar to (126) and (127)\nto estimate the minimizer. However, each node ℓ has a readily available approximation for wo, which is its local intermediate estimate ψℓ,i. Therefore, we replace w\no in (127) by ψℓ,i. This step helps diffuse information throughout the network. This is because each neighbor of node k determines its estimate ψℓ,i by processing information from its own neighbors, which process information from their neighbors, and so forth.\n(b) Second, the intermediate value ψk,i at node k is generally a better estimate for w o than wk,i−1 since it\nis obtained by incorporating information from the neighbors through the first step (126). Therefore, we further replace wk,i−1 in (127) by ψk,i. This step is reminiscent of incremental-type approaches to optimization, which have been widely studied in the literature [23–26].\nWith the substitutions described in items (a) and (b) above, we replace the second step (127) by\nwk,i = ψk,i + µk ∑\nℓ∈Nk\\{k}\nbℓk (ψℓ,i − ψk,i)\n=  1− µk ∑\nℓ∈Nk\\{k}\nbℓk  ψk,i + µk ∑\nℓ∈Nk\\{k}\nbℓk ψℓ,i (128)\nIntroduce the weighting coefficients:\nakk ∆ = 1− µk\n∑\nℓ∈Nk\\{k}\nbℓk (129)\naℓk ∆ = µkbℓk, ℓ ∈ Nk\\{k} (130) aℓk ∆ = 0, ℓ /∈ Nk (131)\nand observe that, for sufficiently small step-sizes µk, these coefficients are nonnegative and, moreover, they satisfy the conditions:\nfor k = 1, 2, . . . , N :\naℓk ≥ 0, N∑\nℓ=1\naℓk = 1, and aℓk = 0 if ℓ /∈ Nk (132)\nCondition (132) means that for every node k, the sum of the coefficients {aℓk} that relate it to its neighbors is one. Just like the {cℓk}, from now on, we will treat the coefficients {aℓk} as free weighting parameters that are chosen by the designer according to (132); their selection will also have a bearing on the performance of the resulting algorithms — see Theorem 6.8. If we collect the entries {aℓk} into an N ×N matrix A, such that the k−th column of A consists of {aℓk, ℓ = 1, 2, . . . , N}, then condition (132) translates into saying that each column of A adds up to one:\nAT1 = 1 (133)\nWe say that A is a left stochastic matrix."
    }, {
      "heading" : "3.3 Adapt-then-Combine (ATC) Diffusion Strategy",
      "text" : "Using the coefficients {aℓk} so defined, we replace (126) and (128) by the following recursions for i ≥ 0:\n(ATC strategy)\nψk,i = wk,i−1 + µk ∑\nℓ∈Nk\ncℓk (rdu,ℓ −Ru,ℓ wk,i−1)\nwk,i = ∑\nℓ∈Nk\naℓk ψℓ,i (134)\nfor some nonnegative coefficients {cℓk, aℓk} that satisfy conditions (106) and (133), namely,\nC1 = 1, AT1 = 1 (135)\nor, equivalently, for k = 1, 2, . . . , N :\ncℓk ≥ 0, N∑\nk=1\ncℓk = 1, cℓk = 0 if ℓ /∈ Nk\naℓk ≥ 0, N∑\nℓ=1\naℓk = 1, aℓk = 0 if ℓ /∈ Nk\n(136)\nTo run algorithm (134), we only need to select the coefficients {aℓk, cℓk} that satisfy (135) or (136); there is no need to worry about the intermediate coefficients {bℓk} any longer since they have been blended into the {aℓk}. The scalars {cℓk, aℓk} that appear in (134) correspond to weighting coefficients over the edge linking node k to its neighbors ℓ ∈ Nk. Note that two sets of coefficients are used to scale the data that are being received by node k: one set of coefficients, {cℓk}, is used in the first step of (134) to scale the moment data {rdu,ℓ, Ru,ℓ}, and a second set of coefficients, {aℓk}, is used in the second step of (134) to scale the estimates {ψℓ,i}. Figure 10 explains what the entries on the columns and rows of the combination matrices {A,C} stand for using an example with N = 6 and the matrix C for illustration. When the combination matrix is right-stochastic (as is the case with C), each of its rows would add up to one. On the other hand, when the matrix is left-stochastic (as is the case with A), each of its columns would add up to one.\nAt every time instant i, the ATC strategy (134) performs two steps. The first step is an information exchange step where node k receives from its neighbors their moments {Ru,ℓ, rdu,ℓ}. Node k combines this information and uses it to update its existing estimate wk,i−1 to an intermediate value ψk,i. All other nodes in the network are performing a similar step and updating their existing estimates {wℓ,i−1} into intermediate estimates {ψℓ,i} by using information from their neighbors. The second step in (134) is an aggregation or\nconsultation step where node k combines the intermediate estimates of its neighbors to obtain its update estimate wk,i. Again, all other nodes in the network are simultaneously performing a similar step. The reason for the name Adapt-then-Combine (ATC) strategy is that the first step in (134) will be shown to lead to an adaptive step, while the second step in (134) corresponds to a combination step. Hence, strategy (134) involves adaptation followed by combination or ATC for short. The reason for the qualification “diffusion” is that the combination step in (134) allows information to diffuse through the network in real time. This is because each of the estimates ψℓ,i is influenced by data beyond the immediate neighborhood of node k.\nIn the special case when C = I, so that no information exchange is performed but only the aggregation step, the ATC strategy (134) reduces to:\n(ATC strategy without information exchange)\nψk,i = wk,i−1 + µk (rdu,k −Ru,k wk,i−1)\nwk,i = ∑\nℓ∈Nk\naℓk ψℓ,i (137)\nwhere the first step relies solely on the information {Ru,k, rdu,k} that is available locally at node k. Observe in passing that the term that appears in the information exchange step of (134) is related to the gradient vectors of the local costs {Jℓ(w)} evaluated at wk,i−1, i.e., it holds that\nrdu,ℓ −Ru,ℓ wk,i−1 = − [∇wJℓ(wk,i−1)] ∗\n(138)\nso that the ATC strategy (134) can also be written in the following equivalent form:\n(ATC strategy)\nψk,i = wk,i−1 − µk ∑\nℓ∈Nk\ncℓk [∇wJℓ(wk,i−1)] ∗\nwk,i = ∑\nℓ∈Nk\naℓk ψℓ,i (139)\nThe significance of this general form is that it is applicable to optimization problems involving more general local costs Jℓ(w) that are not necessarily quadratic in w, as detailed in [1–3] — see also Sec. 10.4. The top part of Fig. 11 illustrates the two steps involved in the ATC procedure for a situation where node k has three other neighbors labeled {1, 2, ℓ}. In the first step, node k evaluates the gradient vectors of its neighbors at wk,i−1, and subsequently aggregates the estimates {ψ1,i, ψ2,i, ψℓ,i} from its neighbors. The dotted arrows represent flow of information towards node k from its neighbors. The solid arrows represent flow of information from node k to its neighbors. The CTA diffusion strategy is discussed next."
    }, {
      "heading" : "3.4 Combine-then-Adapt (CTA) Diffusion Strategy",
      "text" : "Similarly, if we return to (125) and add the second correction term first, then (126)–(127) are replaced by:\nψk,i−1 = wk,i−1 + µk ∑\nℓ∈Nk\\{k}\nbℓk (w o − wk,i−1) (140)\nwk,i = ψk,i−1 + µk ∑\nℓ∈Nk\ncℓk (rdu,ℓ −Ru,ℓ wk,i−1) (141)\nFollowing similar reasoning to what we did before in the ATC case, we replace wo in step (140) by wℓ,i−1 and replace wk,i−1 in (141) by ψk,i−1. We then introduce the same coefficients {aℓk} and arrive at the following\ncombine-then-adapt (CTA) strategy:\n(CTA strategy)\nψk,i−1 = ∑\nℓ∈Nk\naℓk wℓ,i−1\nwk,i = ψk,i−1 + µk ∑\nℓ∈Nk\ncℓk (rdu,ℓ −Ru,ℓ ψk,i−1) (142)\nwhere the nonnegative coefficients {cℓk, aℓk} satisfy the same conditions (106) and (133), namely,\nC1 = 1, AT1 = 1 (143)\nor, equivalently, for k = 1, 2, . . . , N :\ncℓk ≥ 0, N∑\nk=1\ncℓk = 1, cℓk = 0 if ℓ /∈ Nk\naℓk ≥ 0, N∑\nℓ=1\naℓk = 1, aℓk = 0 if ℓ /∈ Nk\n(144)\nAt every time instant i, the CTA strategy (142) also consists of two steps. The first step is an aggregation step where node k combines the existing estimates of its neighbors to obtain the intermediate estimate ψk,i−1.\nAll other nodes in the network are simultaneously performing a similar step and aggregating the estimates of their neighbors. The second step in (142) is an information exchange step where node k receives from its neighbors their moments {Rdu,ℓ, rdu,ℓ} and uses this information to update its intermediate estimate to wk,i. Again, all other nodes in the network are simultaneously performing a similar information exchange step. The reason for the name Combine-then-Adapt (CTA) strategy is that the first step in (142) involves a combination step, while the second step will be shown to lead to an adaptive step. Hence, strategy (142) involves combination followed by adaptation or CTA for short. The reason for the qualification “diffusion” is that the combination step of (142) allows information to diffuse through the network in real time.\nIn the special case when C = I, so that no information exchange is performed but only the aggregation step, the CTA strategy (142) reduces to:\n(CTA strategy without information exchange)\nψk,i−1 = ∑\nℓ∈Nk\naℓk wℓ,i−1\nwk,i = ψk,i−1 + µk (rdu,k −Ru,k ψk,i−1) (145)\nwhere the second step relies solely on the information {Ru,k, rdu,k} that is available locally at node k. Again, the CTA strategy (142) can be rewritten in terms of the gradient vectors of the local costs {Jℓ(w)} as follows:\n(CTA strategy)\nψk,i−1 = ∑\nℓ∈Nk\naℓk wℓ,i−1\nwk,i = ψk,i−1 − µk ∑\nℓ∈Nk\ncℓk [∇wJℓ(ψk,i−1)] ∗ (146)\nThe bottom part of Fig. 11 illustrates the two steps involved in the CTA procedure for a situation where node k has three other neighbors labeled {1, 2, ℓ}. In the first step, node k aggregates the estimates {w1,i−1, w2,i−1, wℓ,i−1} from its neighbors, and subsequently performs information exchange by evaluating the gradient vectors of its neighbors at ψk,i−1."
    }, {
      "heading" : "3.5 Useful Properties of Diffusion Strategies",
      "text" : "Note that the structure of the ATC and CTA diffusion strategies (134) and (142) are fundamentally the same: the difference between the implementations lies in which variable we choose to correspond to the updated weight estimate wk,i. In the ATC case, we choose the result of the combination step to be wk,i, whereas in the CTA case we choose the result of the adaptation step to be wk,i.\nFor ease of reference, Table 2 lists the steepest-descent diffusion algorithms derived in the previous sections. The derivation of the ATC and CTA strategies (134) and (142) followed the approach proposed in [18,27]. CTA estimation schemes were first proposed in the works [35–39], and later extended in [18,27,32,33]. The earlier versions of CTA in [35–37] used the choice C = I. This form of the algorithm with C = I, and with the additional constraint that the step-sizes µk should be time-dependent and decay towards zero as time progresses, was later applied by [40,41] to solve distributed optimization problems that require all nodes to reach consensus or agreement. Likewise, special cases of the ATC estimation scheme (134), involving an information exchange step followed by an aggregation step, first appeared in the work [28] on diffusion leastsquares schemes and subsequently in the works [18,29–33] on distributed mean-square-error and state-space estimation methods. A special case of the ATC strategy (134) corresponding to the choice C = I with decaying step-sizes was adopted in [34] to ensure convergence towards a consensus state. Diffusion strategies of the form (134) and (142) (or, equivalently, (139) and (146)) are general in several respects:\n(1) These strategies do not only diffuse the local weight estimates, but they can also diffuse the local gradient vectors. In other words, two sets of combination coefficients {aℓk, cℓk} are used.\n(5) Even the combination weights {aℓk, cℓk} can be adapted, as we shall discuss later in Sec. 8.3. In this way, diffusion strategies allow multiple layers of adaptation: the nodes perform adaptive processing, the combination weights can be adapted, and even the topology can be adapted especially for mobile networks [8]."
    }, {
      "heading" : "4 Adaptive Diffusion Strategies",
      "text" : "The distributed ATC and CTA steepest-descent strategies (134) and (142) for determining the wo that solves (92)–(93) require knowledge of the statistical information {Ru,k, rdu,k}. These moments are needed in order to be able to evaluate the gradient vectors that appear in (134) and (142), namely, the terms:\n− [∇wJℓ(wk,i−1)] ∗ = (rdu,ℓ −Ru,ℓ wk,i−1) (147) − [∇wJℓ(ψk,i−1)] ∗ = (rdu,ℓ −Ru,ℓ ψk,i−1) (148)\nfor all ℓ ∈ Nk. However, the moments {Ru,ℓ, rdu,ℓ} are often not available beforehand, which means that the true gradient vectors are generally not available. Instead, the agents have access to observations {dk(i), uk,i} of the random processes {dk(i),uk,i}. There are many ways by which the true gradient vectors can be approximated by using these observations. Recall that, by definition,\nRu,ℓ ∆ = Eu∗ℓ,iuℓ,i, rdu,ℓ ∆ = Edℓ(i)u ∗ ℓ,i (149)\nOne common stochastic approximation method is to drop the expectation operator from the definitions of {Ru,ℓ, rdu,ℓ} and to use the following instantaneous approximations instead [4–7]:\nRu,ℓ ≈ u ∗ ℓ,iuℓ,i, rdu,ℓ ≈ dℓ(i)u ∗ ℓ,i (150)\nIn this case, the approximate gradient vectors become:\n(rdu,ℓ −Ru,ℓ wk,i−1) ≈ u ∗ ℓ,i [dℓ(i)− uℓ,i wk,i−1] (151) (rdu,ℓ −Ru,ℓ ψk,i−1) ≈ u ∗ ℓ,i [dℓ(i)− uℓ,i ψk,i−1] (152)\nSubstituting into the ATC and CTA steepest-descent strategies (134) and (142), we arrive at the following adaptive implementations of the diffusion strategies for i ≥ 0:\n(adaptive ATC strategy)\nψk,i = wk,i−1 + µk ∑\nℓ∈Nk\ncℓk u ∗ ℓ,i [dℓ(i)− uℓ,iwk,i−1]\nwk,i = ∑\nℓ∈Nk\naℓk ψℓ,i (153)\nand\n(adaptive CTA strategy)\nψk,i−1 = ∑\nℓ∈Nk\naℓk wℓ,i−1\nwk,i = ψk,i−1 + µk ∑\nℓ∈Nk\ncℓk u ∗ ℓ,i [dℓ(i)− uℓ,i ψk,i−1]\n(154)\nwhere the coefficients {aℓk, cℓk} are chosen to satisfy:\nfor k = 1, 2, . . . , N :\ncℓk ≥ 0, N∑\nk=1\ncℓk = 1, cℓk = 0 if ℓ /∈ Nk\naℓk ≥ 0, N∑\nℓ=1\naℓk = 1, aℓk = 0 if ℓ /∈ Nk\n(155)\nThe adaptive implementations usually start from the initial conditions wℓ,−1 = 0 for all ℓ, or from some other convenient initial values. Clearly, in view of the approximations (151)–(152), the successive iterates {wk,i, ψk,i, ψk,i−1} that are generated by the above adaptive implementations are different from the iterates that result from the steepest-descent implementations (134) and (142). Nevertheless, we shall continue to use the same notation for these variables for ease of reference. One key advantage of the adaptive implementations (153)–(154) is that they enable the agents to react to changes in the underlying statistical information {rdu,ℓ, Ru,ℓ} and to changes in wo. This is because these changes end up being reflected in the data realizations {dk(i), uk,i}. Therefore, adaptive implementations have an innate tracking and learning ability that is of paramount significance in practice.\nWe say that the stochastic gradient approximations (151)–(152) introduce gradient noise into each step of the recursive updates (153)–(154). This is because the updates (153)–(154) can be interpreted as corresponding to the following forms:\n(adaptive ATC strategy)\nψk,i = wk,i−1 − µk ∑\nℓ∈Nk\ncℓk ̂[∇wJℓ(wk,i−1)] ∗\nwk,i = ∑\nℓ∈Nk\naℓk ψℓ,i (156)\nand\n(adaptive CTA strategy)\nψk,i−1 = ∑\nℓ∈Nk\naℓk wℓ,i−1\nwk,i = ψk,i−1 − µk ∑\nℓ∈Nk\ncℓk ̂[∇wJℓ(ψk,i−1)] ∗ (157)\nwhere the true gradient vectors, {∇wJℓ(·)}, have been replaced by approximations, {∇̂wJℓ(·)} — compare with (139) and (146). The significance of the alternative forms (156)–(157) is that they are applicable to optimization problems involving more general local costs Jℓ(w) that are not necessarily quadratic, as detailed in [2, 3]; see also Sec. 10.4. In the next section, we examine how gradient noise affects the performance of the diffusion strategies and how close the successive estimates {wk,i} get to the desired optimal solution w\no. Table 3 lists several of the adaptive diffusion algorithms derived in this section.\nThe operation of the adaptive diffusion strategies is similar to the operation of the steepest-descent diffusion strategies of the previous section. Thus, note that at every time instant i, the ATC strategy (153) performs two steps; as illustrated in Fig. 12. The first step is an information exchange step where node k receives from its neighbors their information {dℓ(i), uℓ,i}. Node k combines this information and uses it to update its existing estimate wk,i−1 to an intermediate value ψk,i. All other nodes in the network are performing a similar step and updating their existing estimates {wℓ,i−1} into intermediate estimates {ψℓ,i} by using information from their neighbors. The second step in (153) is an aggregation or consultation step where node k combines the intermediate estimates {ψℓ,i} of its neighbors to obtain its update estimate wk,i. Again, all other nodes in the network are simultaneously performing a similar step. In the special case when C = I, so that no information exchange is performed but only the aggregation step, the ATC strategy (153) reduces to:\n(adaptive ATC strategy without information exchange)\nψk,i = wk,i−1 + µk u ∗ k,i [dk(i)− uk,iwk,i−1] wk,i = ∑\nℓ∈Nk\naℓk ψℓ,i (158)\nLikewise, at every time instant i, the CTA strategy (154) also consists of two steps – see Fig. 13. The first step is an aggregation step where node k combines the existing estimates of its neighbors to obtain the intermediate estimate ψk,i−1. All other nodes in the network are simultaneously performing a similar step and aggregating the estimates of their neighbors. The second step in (154) is an information exchange step where node k receives from its neighbors their information {dℓ(i), uℓ,i} and uses this information to update its intermediate estimate to wk,i. Again, all other nodes in the network are simultaneously performing a similar information exchange step. In the special case when C = I, so that no information exchange is performed but only the aggregation step, the CTA strategy (154) reduces to:\n(adaptive CTA strategy without information exchange)\nψk,i−1 = ∑\nℓ∈Nk\naℓk wℓ,i−1\nwk,i = ψk,i−1 + µk u ∗ k,i [dk(i)− uk,i ψk,i−1]\n(159)\nWe further note that the adaptive ATC and CTA strategies (153)–(154) reduce to the non-cooperative adaptive solution (22)–(23), where each node k runs its own individual LMS filter, when the coefficients {aℓk, cℓk} are selected as\naℓk = δℓk = cℓk (non-cooperative case) (160)\nwhere δℓk denotes the Kronecker delta function:\nδℓk ∆ = { 1, ℓ = k 0, otherwise\n(161)\nIn terms of the combination matrices A and C, this situation corresponds to setting\nA = IN = C (non-cooperative case) (162)"
    }, {
      "heading" : "5 Performance of Steepest-Descent Diffusion Strategies",
      "text" : "Before studying in some detail the mean-square performance of the adaptive diffusion implementations (153)– (154), and the influence of gradient noise, we examine first the convergence behavior of the steepest-descent diffusion strategies (134) and (142), which employ the true gradient vectors. Doing so, will help introduce the necessary notation and highlight some features of the analysis in preparation for the more challenging treatment of the adaptive strategies in Sec. 6."
    }, {
      "heading" : "5.1 General Diffusion Model",
      "text" : "Rather than study the performance of the ATC and CTA steepest-descent strategies (134) and (142) separately, it is useful to introduce a more general description that includes the ATC and CTA recursions as special cases. Thus, consider a distributed steepest-descent diffusion implementation of the following general form for i ≥ 0:\nφk,i−1 = ∑\nℓ∈Nk\na1,ℓk wℓ,i−1 (163)\nψk,i = φk,i−1 + µk ∑\nℓ∈Nk\ncℓk [rdu,ℓ −Ru,ℓ φk,i−1] (164)\nwk,i = ∑\nℓ∈Nk\na2,ℓk ψℓ,i (165)\nwhere the scalars {a1,ℓk, cℓk, a2,ℓk} denote three sets of non-negative real coefficients corresponding to the (ℓ, k) entries of N×N combination matrices {A1, C,A2}, respectively. These matrices are assumed to satisfy the conditions:\nAT1 1 = 1, C1 = 1, A T 2 1 = 1 (166)\nso that {A1, A2} are left stochastic and C is right-stochastic, i.e.,\nfor k = 1, 2, . . . , N :\ncℓk ≥ 0, N∑\nk=1\ncℓk = 1, cℓk = 0 if ℓ /∈ Nk\na1,ℓk ≥ 0, N∑\nℓ=1\na1,ℓk = 1, a1,ℓk = 0 if ℓ /∈ Nk\na2,ℓk ≥ 0, N∑\nℓ=1\na2,ℓk = 1, a2,ℓk = 0 if ℓ /∈ Nk\n(167)\nDifferent choices for {A1, C,A2} correspond to different cooperation modes. For example, the choice A1 = IN and A2 = A corresponds to the ATC implementation (134), while the choice A1 = A and A2 = IN corresponds to the CTA implementation (142). Likewise, the choice C = IN corresponds to the case in which the nodes only share weight estimates and the distributed diffusion recursions (163)–(165) become\nφk,i−1 = ∑\nℓ∈Nk\na1,ℓk wℓ,i−1 (168)\nψk,i = φk,i−1 + µk (rdu,k −Ru,k φk,i−1) (169) wk,i = ∑\nℓ∈Nk\na2,ℓk ψℓ,i (170)\nFurthermore, the choice A1 = A2 = C = IN corresponds to the non-cooperative mode of operation, in which case the recursions reduce to the classical (stand-alone) steepest-descent recursion [4–7], where each node minimizes individually its own quadratic cost Jk(w), defined earlier in (97):\nwk,i = wk,i−1 + µk [rdu,k −Ru,k wk,i−1] , i ≥ 0 (171)"
    }, {
      "heading" : "5.2 Error Recursions",
      "text" : "Our objective is to examine whether, and how fast, the weight estimates {wk,i} from the distributed implementation (163)–(165) converge towards the solution wo of (92)–(93). To do so, we introduce the M × 1 error vectors:\nφ̃k,i ∆ = wo − φk,i (172)\nψ̃k,i ∆ = wo − ψk,i (173)\nw̃k,i ∆ = wo − wk,i (174)\nEach of these error vectors measures the residual relative to the desired minimizer wo. Now recall from (100) that\nrdu,k = Ru,k w o (175)\nThen, subtracting wo from both sides of the relations in (163)–(165) we get\nφ̃k,i−1 = ∑\nℓ∈Nk\na1,ℓk w̃ℓ,i−1 (176)\nψ̃k,i = ( IM − µk ∑\nℓ∈Nk\ncℓk Ru,ℓ ) φ̃k,i−1 (177)\nw̃k,i = ∑\nℓ∈Nk\na2,ℓk ψ̃ℓ,i (178)\nWe can describe these relations more compactly by collecting the information from across the network into block vectors and matrices. We collect the error vectors from across all nodes into the following N × 1 block vectors, whose individual entries are of size M × 1 each:\nψ̃i ∆ =   ψ̃1,i ψ̃2,i ...\nψ̃N,i\n  , φ̃i ∆ =   φ̃1,i φ̃2,i ...\nφ̃N,i\n  , w̃i ∆ =   w̃1,i w̃2,i ...\nw̃N,i\n  (179)\nThe block quantities {ψ̃i, φ̃i, w̃i} represent the state of the errors across the network at time i. Likewise, we introduce the following N ×N block diagonal matrices, whose individual entries are of size M ×M each:\nM ∆ = diag{ µ1IM , µ2IM , . . . , µNIM } (180)\nR ∆ = diag\n{ ∑\nℓ∈N1\ncℓ1 Ru,ℓ, ∑\nℓ∈N2\ncℓ2 Ru,ℓ, . . . , ∑\nℓ∈NN\ncℓN Ru,ℓ\n} (181)\nEach block diagonal entry of R, say, the k-th entry, contains the combination of the covariance matrices in the neighborhood of node k. We can simplify the notation by denoting these neighborhood combinations as follows:\nRk ∆ =\n∑\nℓ∈Nk\ncℓk Ru,ℓ (182)\nso that R becomes\nR ∆ = diag { R1, R2, . . . , RN } (when C 6= I) (183)\nIn the special case when C = IN , the matrix R reduces to\nRu = diag{Ru,1, Ru,2, . . . , Ru,N} (when C = I) (184)\nwith the individual covariance matrices appearing on its diagonal; we denote R by Ru in this special case. We further introduce the Kronecker products\nA1 ∆ = A1 ⊗ IM , A2 ∆ = A2 ⊗ IM (185)\nThe matrix A1 is an N × N block matrix whose (ℓ, k) block is equal to a1,ℓkIM . Likewise, for A2. In other words, the Kronecker transformation defined above simply replaces the matrices {A1, A2} by block matrices {A1,A2} where each entry {a1,ℓk, a2,ℓk} in the original matrices is replaced by the diagonal matrices {a1,ℓkIM , a2,ℓkIM}. For ease of reference, Table 5 lists the various symbols that have been defined so far, and others that will be defined in the sequel.\nso that the network weight error vector, w̃i, ends up evolving according to the following dynamics:\nw̃i = A T 2 (INM −MR)A T 1 w̃i−1, i ≥ 0 (diffusion strategy) (189)\nFor comparison purposes, if each node in the network minimizes its own cost function, Jk(w), separately from the other nodes and uses the non-cooperative steepest-descent strategy (171), then the weight error vector across all N nodes would evolve according to the following alternative dynamics:\nw̃i = (INM −MRu) w̃i−1, i ≥ 0 (non-cooperative strategy) (190)\nwhere the matrices A1 and A2 do not appear, and R is replaced by Ru from (184). This recursion is a special case of (189) when A1 = A2 = C = IN ."
    }, {
      "heading" : "5.3 Convergence Behavior",
      "text" : "Note from (189) that the evolution of the weight error vector involves block vectors and block matrices; this will be characteristic of the distributed implementations that we consider in this article. To examine the stability and convergence properties of recursions that involve such block quantities, it becomes useful to rely on a certain block vector norm. In App. D, we describe a so-called block maximum block and establish some of its useful properties. The results of the appendix will be used extensively in our exposition. It is therefore advisable for the reader to review the properties stated in the appendix at this stage.\nUsing the result of Lemma D.6, we can establish the following useful statement about the convergence of the steepest-descent diffusion strategy (163)–(165). The result establishes that all nodes end up converging to the optimal solution wo if the nodes employ positive step-sizes µk that are small enough; the lemma provides a sufficient bound on the {µk}.\nTheorem 5.1. (Convergence to Optimal Solution) Consider the problem of optimizing the global cost (92) with the individual cost functions given by (93). Pick a right stochastic matrix C and left stochastic matrices A1 and A2 satisfying (166) or (167); these matrices define the network topology and how information is shared over neighborhoods. Assume each node in the network runs the (distributed) steepest-descent diffusion algorithm (163)–(165). Then, all estimates {wk,i} across the network converge to the optimal solution wo if the positive step-size parameters {µk} satisfy\nµk < 2\nλmax(Rk) (191)\nwhere the neighborhood covariance matrix Rk is defined by (182).\nProof. The weight error vector w̃i converges to zero if, and only if, the coefficient matrix AT2 (INM −MR)A T 1 in (189) is a stable matrix (meaning that all its eigenvalues lie strictly inside the unit disc). From property (605) established in App. D, we know that AT2 (INM −MR)A T 1 is stable if the block diagonal matrix (INM −MR) is stable. It is now straightforward to verify that condition (191) ensures the stability of (INM −MR). It follows that\nw̃i −→ 0 as i −→ ∞ (192)\nObserve that the stability condition (191) does not depend on the specific combination matrices A1 and A2. Thus, as long as these matrices are chosen to be left-stochastic, the weight-error vectors will converge to zero under condition (191) no matter what {A1, A2} are. Only the combination matrix C influences the condition on the step-size through the neighborhood covariance matrices {Rk}. Observe further that the statement of the lemma does not require the network to be connected. Moreover, when C = I, in which case the nodes only share weight estimates and do not share the neighborhood moments {rdu,ℓ, Ru,ℓ}, as in (168)–(170), condition (191) becomes\nµk < 2\nλmax(Ru,k) (cooperation with C = I) (193)\nin terms of the actual covariance matrices {Ru,k}. Results (191) and (193) are reminiscent of a classical result for stand-alone steepest-descent algorithms, as in the non-cooperative case (171), where it is known that the estimate by each individual node in this case will converge to wo if, and only if, its positive step-size satisfies\nµk < 2\nλmax(Ru,k) (non-cooperative case (171) with A1 = A2 = C = IN ) (194)\nThis is the same condition as (193) for the case C = I. The following statement provides a bi-directional statement that ensures convergence of the (distributed) steepest-descent diffusion strategy (163)–(165) for any choice of left-stochastic combination matrices A1 and A2.\nTheorem 5.2. (Convergence for Arbitrary Combination Matrices) Consider the problem of optimizing the global cost (92) with the individual cost functions given by (93). Pick a right stochastic matrix C satisfying (166). Then, the estimates {wk,i} generated by (163)–(165) converge to w\no, for all choices of left-stochastic matrices A1 and A2 satisfying (166) if, and only if,\nµk < 2\nλmax(Rk) (195)\nProof. The result follows from property (b) of Corollary D.1, which is established in App. D.\nMore importantly, we can verify that under fairly general conditions, employing the steepest-descent diffusion strategy (163)–(165) enhances the convergence rate of the error vector towards zero relative to the non-cooperative strategy (171). The next three results establish this fact when C is a doubly stochastic matrix, i.e., it has non-negative entries and satisfies\nC1 = 1, CT1 = 1 (196)\nwith both its rows and columns adding up to one. Compared to the earlier right-stochastic condition on C in (105), we are now requiring ∑\nℓ∈Nk\nckℓ = 1, ∑\nℓ∈Nk\ncℓk = 1 (197)\nFor example, these conditions are satisfied when C is right stochastic and symmetric. They are also satisfied for C = I, when only weight estimates are shared as in (168)–(170); this latter case covers the ATC and CTA diffusion strategies (137) and (145), which do not involve information exchange.\nTheorem 5.3. (Convergence Rate is Enhanced: Uniform Step-Sizes) Consider the problem of optimizing the global cost (92) with the individual cost functions given by (93). Pick a doubly stochastic matrix C satisfying (196) and left stochastic matrices A1 and A2 satisfying (166). Consider two modes of operation. In one mode, each node in the network runs the (distributed) steepest-descent diffusion algorithm (163)–(165). In the second mode, each node operates individually and runs the non-cooperative steepest-descent algorithm (171). In both cases, the positive step-sizes used by all nodes are assumed to be the same, say, µk = µ for all k, and the value of µ is chosen to satisfy the required stability conditions (191) and (194), which are met by selecting\nµ < min 1≤k≤N\n{ 2\nλmax(Ru,k)\n} (198)"
    }, {
      "heading" : "It then holds that the magnitude of the error vector, ‖w̃i‖, in the diffusion case decays to zero more rapidly than in the non-cooperative case. In other words, diffusion cooperation enhances convergence rate.",
      "text" : "Proof. Let us first establish that any positive step-size µ satisfying (198) will satisfy both stability conditions (191) and (194). It is obvious that (194) is satisfied. We verify that (191) is also satisfied when C is doubly stochastic. In this case, each neighborhood covariance matrix, Rk, becomes a convex combination of individual covariance matrices {Ru,ℓ}, i.e.,\nRk = ∑\nℓ∈Nk\ncℓkRu,ℓ\nwhere now ∑\nℓ∈Nk\ncℓk = 1 (when C is doubly stochastic)\nTo proceed, we recall that the spectral norm (maximum singular value) of any matrix X is a convex function of X [56]. Moreover, for Hermitian matrices X, their spectral norms coincide with their spectral radii (largest eigenvalue magnitude). Then, Jensen’s inequality [56] states that for any convex function f(·) it holds that\nf\n( ∑\nm\nθmXm ) ≤ ∑\nm\nθmf(Xm)\nfor Hermitian matrices Xm and nonnegative scalars θm that satisfy\n∑\nm\nθm = 1\nChoosing f(·) as the spectral radius function, and applying it to the definition of Rk above, we get\nρ(Rk) = ρ\n  ∑\nℓ∈Nk\ncℓkRu,ℓ\n \n≤ ∑\nℓ∈Nk\ncℓk · ρ(Ru,ℓ)\n≤ ∑\nℓ∈Nk\ncℓk · [ max\n1≤ℓ≤N ρ(Ru,ℓ)\n]\n= max 1≤ℓ≤N ρ(Ru,ℓ)\nIn other words, λmax(Rk) ≤ max\n1≤k≤N {λmax(Ru,k)}\nIt then follows from (198) that\nµ < 2\nλmax(Rk) , for all k = 1, 2, . . . , N\nso that (191) is satisfied as well. Let us now examine the convergence rate. To begin with, we note that the matrix (INM −MR) that appears in the weight-error recursion (189) is block diagonal:\n(INM −MR) = diag{(IM − µR1), (IM − µR2), . . . , (IM − µRN )}\nand each individual block entry, (IM −µRk), is a stable matrix since µ satisfies (191). Moreover, each of these entries can be written as\nIM − µRk = ∑\nℓ∈Nk\ncℓk(IM − µRu,ℓ)\nwhich expresses (IM − µRk) as a convex combination of stable terms (IM − µRu,ℓ). Applying Jensen’s inequality again we get\nρ\n  ∑\nℓ∈Nk\ncℓk(IM − µRu,ℓ)   ≤ ∑\nℓ∈Nk\ncℓk ρ(IM − µRu,ℓ)\nNow, we know from (189) that the rate of decay of w̃i to zero in the diffusion case is determined by the spectral radius of the coefficient matrix AT2 (INM −MR)A T 1 . Likewise, we know from (190) that the rate of decay of w̃i to zero in the non-cooperative case is determined by the spectral radius of the coefficient matrix (INM −MRu). Then, note that\nρ ( AT2 (INM −MR)A T 1 ) (605) ≤ ρ(INM −MR)\n= max 1≤k≤N ρ (IM − µRk)\n= max 1≤k≤N ρ\n  ∑\nℓ∈Nk\ncℓk(IM − µRu,ℓ)\n\n\n≤ max 1≤k≤N\n∑\nℓ∈Nk\ncℓk ρ(IM − µRu,ℓ)\n≤ max 1≤k≤N\n∑\nℓ∈Nk\ncℓk ( max\n1≤ℓ≤N ρ(IM − µRu,ℓ)\n)\n= max 1≤k≤N    ( max 1≤ℓ≤N ρ(IM − µRu,ℓ) ) · ∑\nℓ∈Nk\ncℓk   \n= max 1≤k≤N\n( max\n1≤ℓ≤N ρ(IM − µRu,ℓ)\n)\n= max 1≤ℓ≤N ρ(IM − µRu,ℓ)\n= ρ(INM −MRu)\nTherefore, the spectral radius of AT2 (INM −MR)A T 1 is at most as large as the largest individual spectral radius in the non-cooperative case.\nThe argument can be modified to handle different step-sizes across the nodes if we assume uniform covariance data across the network, as stated below.\nTheorem 5.4. (Convergence Rate is Enhanced: Uniform Covariance Data) Consider the same setting of Theorem 5.3. Assume the covariance data are uniform across all nodes, say, Ru,k = Ru is independent of k. Assume further that the nodes in both modes of operation employ steps-sizes µk that are chosen to satisfy the required stability conditions (191) and (194), which in this case are met by:\nµk < 2\nλmax(Ru) , k = 1, 2, . . . , N (199)"
    }, {
      "heading" : "It then holds that the magnitude of the error vector, ‖w̃i‖, in the diffusion case decays to zero more rapidly than in the non-cooperative case. In other words, diffusion enhances convergence rate.",
      "text" : "Proof. Since Ru,ℓ = Ru for all ℓ and C is doubly stochastic, we get Rk = Ru and INM −MR = INM −MRu. Then,\nρ ( AT2 (INM −MR)A T 1 ) (605) ≤ ρ(INM −MR)\n= ρ(INM −MRu)\nThe next statement considers the case of ATC and CTA strategies (137) and (145) without information exchange, which correspond to the case C = IN . The result establishes that these strategies always enhance the convergence rate over the non-cooperative case, without the need to assume uniform step-sizes or uniform covariance data.\nTheorem 5.5. (Convergence Rate is Enhanced when C = I) Consider the problem of optimizing the global cost (92) with the individual cost functions given by (93). Pick left stochastic matrices A1 and A2 satisfying (166) and set C = IN . This situation covers the ATC and CTA strategies (137) and (145), which\ndo not involve information exchange. Consider two modes of operation. In one mode, each node in the network runs the (distributed) steepest-descent diffusion algorithm (168)–(170). In the second mode, each node operates individually and runs the non-cooperative steepest-descent algorithm (171). In both cases, the positive step-sizes are chosen to satisfy the required stability conditions (193) and (194), which in this case are met by\nµk < 2\nλmax(Ru,k) , k = 1, 2, . . . , N (200)"
    }, {
      "heading" : "It then holds that the magnitude of the error vector, ‖w̃i‖, in the diffusion case decays to zero more rapidly than in the non-cooperative case. In other words, diffusion cooperation enhances convergence rate.",
      "text" : "Proof. When C = IN , we get Rk = Ru,k and, therefore, R = Ru and INM −MR = INM −MRu. Then,\nρ ( AT2 (INM −MR)A T 1 ) (605) ≤ ρ(INM −MR)\n= ρ(INM −MRu)\nThe results of the previous theorems highlight the following important facts about the role of the combination matrices {A1, A2, C} in the convergence behavior of the diffusion strategy (163)–(165):\n(a) The matrix C influences the stability of the network through its influence on the bound in (191). This is because the matrices {Rk} depend on the entries of C. The matrices {A1, A2} do not influence network stability.\n(b) The matrices {A1, A2, C} influence the rate of convergence of the network since they influence the spectral radius of the matrix AT2 (INM −MR)A T 1 , which controls the dynamics of the weight error\nvector in (189)."
    }, {
      "heading" : "6 Performance of Adaptive Diffusion Strategies",
      "text" : "We now move on to examine the behavior of the adaptive diffusion implementations (153)–(154), and the influence of both gradient noise and measurement noise on convergence and steady-state performance. Due to the random nature of the perturbations, it becomes necessary to evaluate the behavior of the algorithms on average, using mean-square convergence analysis. For this reason, we shall study the convergence of the weight estimates both in the mean and mean-square sense. To do so, we will again consider a general diffusion structure that includes the ATC and CTA strategies (153)–(154) as special cases. We shall further resort to the boldface notation to refer to the measurements and weight estimates in order to highlight the fact that they are now being treated as random variables. In this way, the update equations becomes stochastic updates. Thus, consider the following general adaptive diffusion strategy for i ≥ 0:\nφk,i−1 = ∑\nℓ∈Nk\na1,ℓk wℓ,i−1 (201)\nψk,i = φk,i−1 + µk ∑\nℓ∈Nk\ncℓk u ∗ ℓ,i [ dℓ(i)− uℓ,i φk,i−1 ] (202)\nwk,i = ∑\nℓ∈Nk\na2,ℓk ψℓ,i (203)\nAs before, the scalars {a1,ℓk, cℓk, a2,ℓk} are non-negative real coefficients corresponding to the (ℓ, k) entries of N × N combination matrices {A1, C,A2}, respectively. These matrices are assumed to satisfy the same conditions (166) or (167). Again, different choices for {A1, C,A2} correspond to different cooperation modes. For example, the choice A1 = IN and A2 = A corresponds to the adaptive ATC implementation (153), while the choice A1 = A and A2 = IN corresponds to the adaptive CTA implementation (154). Likewise, the\nchoice C = IN corresponds to the case in which the nodes only share weight estimates and the distributed diffusion recursions (201)–(203) become\nφk,i−1 = ∑\nℓ∈Nk\na1,ℓk wℓ,i−1 (204)\nψk,i = φk,i−1 + µku ∗ k,i [ dk(i)− uk,i φk,i−1 ] (205) wk,i = ∑\nℓ∈Nk\na2,ℓk ψℓ,i (206)\nFurthermore, the choice A1 = A2 = C = IN corresponds to the non-cooperative mode of operation, where each node runs the classical (stand-alone) least-mean-squares (LMS) filter independently of the other nodes: [4–7]:\nwk,i = wk,i−1 + µkuk,i [dk(i)− uk,i wk,i−1] , i ≥ 0 (207)"
    }, {
      "heading" : "6.1 Data Model",
      "text" : "When we studied the performance of the steepest-descent diffusion strategy (163)–(165) we exploited result (175), which indicated how the moments {rdu,k, Ru,k} that appeared in the recursions related to the optimal solution wo. Likewise, in order to be able to analyze the performance of the adaptive diffusion strategy (201)–(203), we need to know how the data {dk(i),uk,i} across the network relate to wo. Motivated by the several examples presented earlier in Sec. 2, we shall assume that the data satisfy a linear model of the form:\ndk(i) = uk,iw o + vk(i) (208)\nwhere vk(i) is measurement noise with variance σ 2 v,k:\nσ2v,k ∆ = E |vk(i)| 2 (209)\nand where the stochastic processes {dk(i),uk,i} are assumed to be jointly wide-sense stationary with moments:\nσ2d,k ∆ = E |dk(i)| 2 (scalar) (210)\nRu,k ∆ = Eu∗k,iuk,i > 0 (M ×M) (211)\nrdu,k ∆ = Edk(i)u ∗ k,i (M × 1) (212)\nAll variables are assumed to be zero-mean. Furthermore, the noise process {vk(i)} is assumed to be temporally white and spatially independent, as described earlier by (6), namely,\n{ Evk(i)v ∗ k(j) = 0, for all i 6= j (temporal whiteness)\nEvk(i)v ∗ m(j) = 0, for all i, j whenever k 6= m (spatial whiteness)\n(213)\nThe noise process vk(i) is further assumed to be independent of the regression data um,j for all k,m and i, j so that:\nEvk(i)u ∗ m,j = 0, for all k,m, i, j (214)\nWe shall also assume that the regression data are temporally white and spatially independent so that:\nEu∗k,iuℓ,j = Ru,kδkℓδij (215)\nAlthough we are going to derive performance measures for the network under this independence assumption on the regression data, it turns out that the resulting expressions continue to match well with simulation results for sufficiently small step-sizes, even when the independence assumption does not hold (in a manner similar to the behavior of stand-alone adaptive filters) [4, 5]."
    }, {
      "heading" : "6.2 Performance Measures",
      "text" : "Our objective is to analyze whether, and how fast, the weight estimates {wk,i} from the adaptive diffusion implementation (201)–(203) converge towards wo. To do so, we again introduce the M × 1 weight error vectors:\nφ̃k,i ∆ = wo − φk,i (216)\nψ̃k,i ∆ = wo −ψk,i (217)\nw̃k,i ∆ = wo −wk,i (218)\nEach of these error vectors measures the residual relative to the desired wo in (208). We further introduce two scalar error measures:\nek(i) ∆ = dk(i)− uk,iwk,i−1 (output error) (219)\nea,k(i) ∆ = uk,iw̃k,i−1 (a-priori error) (220)\nThe first error measures how well the term uk,iwk,i−1 approximates the measured data, dk(i); in view of (208), this error can be interpreted as an estimator for the noise term vk(i). If node k is able to estimate w o well, then ek(i) would get close to vk(i). Therefore, under ideal conditions, we would expect the variance of ek(i) to tend towards the variance of vk(i). However, as remarked earlier in (31), there is generally an offset term for adaptive implementations that is captured by the variance of the a-priori error, ea,k(i). This second error measures how well uk,iwk,i−1 approximates the uncorrupted term uk,iw\no. Using the data model (208), we can relate {ek(i), ea,k(i)} as\nek(i) = ea,k + vk(i) (221)\nSince the noise component, vk(i), is assumed to be zero-mean and independent of all other random variables, we recover (31):\nE |ek(i)| 2 = E |ea,k(i)| 2 + σ2v,k (222)\nThis relation confirms that the variance of the output error, ek(i), is always at least as large as σ 2 v,k and away from it by an amount that is equal to the variance of the a-priori error, ea,k(i). Accordingly, in order to quantify the performance of any particular node in the network, we define the mean-square-error (MSE) and excess-mean-square-error (EMSE) for node k as the following steady-state measures:\nMSEk ∆ = lim\ni→∞ E |ek(i)|\n2 (223)\nEMSEk ∆ = lim\ni→∞ E |ea,k(i)|\n2 (224)\nThen, it holds that\nMSEk = EMSEk + σ 2 v,k (225)\nTherefore, the EMSE term quantifies the size of the offset in the MSE performance of each node. We also define the mean-square-deviation (MSD) of each node as the steady-state measure:\nMSDk ∆ = lim\ni→∞ E‖w̃k,i‖\n2 (226)\nwhich measures how far wk,i is from w o in the mean-square-error sense.\nWe indicated earlier in (36)–(37) how the MSD and EMSE of stand-alone LMS filters in the noncooperative case depend on {µk, σ2v, Ru,k}. In this section, we examine how cooperation among the nodes\ninfluences their performance. Since cooperation couples the operation of the nodes, with data originating from one node influencing the behavior of its neighbors and their neighbors, the study of the network performance requires more effort than in the non-cooperative case. Nevertheless, when all is said and done, we will arrive at expressions that approximate well the network performance and reveal some interesting conclusions."
    }, {
      "heading" : "6.3 Error Recursions",
      "text" : "Using the data model (208) and subtracting wo from both sides of the relations in (201)–(203) we get\nφ̃k,i−1 = ∑\nℓ∈Nk\na1,ℓk w̃ℓ,i−1 (227)\nψ̃k,i = ( IM − µk ∑\nℓ∈Nk\ncℓk u ∗ ℓ,iuℓ,i ) φ̃k,i−1 − µk ∑\nℓ∈Nk\ncℓk u ∗ ℓ,ivℓ(i) (228)\nw̃k,i = ∑\nℓ∈Nk\na2,ℓk ψ̃ℓ,i (229)\nComparing the second recursion with the corresponding recursion in the steepest-descent case (176)–(178), we see that two new effects arise: the effect of gradient noise, which replaces the covariance matrices Ru,ℓ by the instantaneous approximation u∗ℓ,iuℓ,i, and the effect of measurement noise, vℓ(i).\nWe again describe the above relations more compactly by collecting the information from across the network in block vectors and matrices. We collect the error vectors from across all nodes into the following N × 1 block vectors, whose individual entries are of size M × 1 each:\nψ̃i ∆ =   ψ̃1,i ψ̃2,i ...\nψ̃N,i\n  , φ̃i ∆ =   φ̃1,i φ̃2,i ...\nφ̃N,i\n  , w̃i ∆ =   w̃1,i w̃2,i ...\nw̃N,i\n  (230)\nThe block quantities {ψ̃i, φ̃i, w̃i} represent the state of the errors across the network at time i. Likewise, we introduce the following N ×N block diagonal matrices, whose individual entries are of size M ×M each:\nM ∆ = diag{ µ1IM , µ2IM , . . . , µNIM } (231)\nRi ∆ = diag\n{ ∑\nℓ∈N1\ncℓ1 u ∗ ℓ,iuℓ,i,\n∑\nℓ∈N2\ncℓ2 u ∗ ℓ,iuℓ,i, . . . ,\n∑\nℓ∈NN\ncℓN u ∗ ℓ,iuℓ,i\n} (232)\nEach block diagonal entry of Ri, say, the k-th entry, contains a combination of rank-one regression terms collected from the neighborhood of node k. In this way, the matrix Ri is now stochastic and dependent on time, in contrast to the matrix R in the steepest-descent case in (181), which was a constant matrix. Nevertheless, it holds that\nERi = R (233)\nso that, on average, Ri agrees with R. We can simplify the notation by denoting the neighborhood combinations as follows:\nRk,i ∆ =\n∑\nℓ∈Nk\ncℓk u ∗ ℓ,iuℓ,i (234)\nso that Ri becomes\nRi ∆ = diag { R1,i, R2,i, . . . ,RN,i } (when C 6= I) (235)\nAgain, compared with the matrix Rk defined in (182), we find that Rk,i is now both stochastic and timedependent. Nevertheless, it again holds that\nERk,i = Rk (236)\nIn the special case when C = I, the matrix Ri reduces to\nRu,i ∆ = diag{u∗1,iu1,i, u ∗ 2,iu2,i, . . . , u ∗ N,iuN,i} (when C = I) (237)\nwith\nERu,i = Ru (238)\nwhere Ru was defined earlier in (184). We further introduce the following N × 1 block column vector, whose entries are of size M × 1 each:\nsi ∆ = col{ u∗1,iv1(i), u ∗ 2,iv2(i), . . . , u ∗ N,ivN (i) } (239)\nObviously, given that the regression data and measurement noise are zero-mean and independent of each other, we have\nEsi = 0 (240)\nand the covariance matrix of si is N ×N block diagonal with blocks of size M ×M :\nS ∆ = Esis ∗ i = diag{σ 2 v,1Ru,1, σ 2 v,2Ru,2, . . . , σ 2 v,NRu,N} (241)\nReturning to (227)–(229), we conclude that the following relations hold for the block quantities:\nφ̃i−1 = A T 1 w̃i−1 (242)\nψ̃i = (INM −MRi) φ̃i−1 − MC Tsi (243) w̃i = A T 2 ψ̃i (244)\nwhere\nC ∆ = C ⊗ IM (245)\nso that the network weight error vector, w̃i, ends up evolving according to the following stochastic recursion:\nw̃i = A T 2 (INM −MRi)A T 1 w̃i−1 − A T 2 MC Tsi, i ≥ 0 (diffusion strategy) (246)\nFor comparison purposes, if each node operates individually and uses the non-cooperative LMS recursion (207), then the weight error vector across all N nodes would evolve according to the following stochastic recursion:\nw̃i = (INM −MRu,i) w̃i−1 − Msi, i ≥ 0 (non-cooperative strategy) (247)\nwhere the matrices A1 and A2 do not appear, and Ri is replaced by Ru,i from (237)."
    }, {
      "heading" : "6.4 Convergence in the Mean",
      "text" : "Taking expectations of both sides of (246) we find that:\nE w̃i = A T 2 (INM −MR)A T 1 · E w̃i−1, i ≥ 0 (diffusion strategy) (248)\nwhere we used the fact that w̃i−1 and Ri are independent of each other in view of our earlier assumptions on the regression data and noise in Sec. 6.1. Comparing with the error recursion (189) in the steepestdescent case, we find that both recursions are identical with w̃i replaced by E w̃i. Therefore, the convergence statements from the steepest-descent case can be extended to the adaptive case to provide conditions on the step-size to ensure stability in the mean, i.e., to ensure\nE w̃i −→ 0 as i −→ ∞ (249)\nWhen (249) is guaranteed, we would say that the adaptive diffusion solution is asymptotically unbiased. The following statements restate the results of Theorems 5.1–5.5 in the context of mean error analysis.\nTheorem 6.1. (Convergence in the Mean) Consider the problem of optimizing the global cost (92) with the individual cost functions given by (93). Pick a right stochastic matrix C and left stochastic matrices A1 and A2 satisfying (166) or (167). Assume each node in the network measures data that satisfy the conditions described in Sec. 6.1, and runs the adaptive diffusion algorithm (201)–(203). Then, all estimators {wk,i} across the network converge in the mean to the optimal solution wo if the positive step-size parameters {µk} satisfy\nµk < 2\nλmax(Rk) (250)\nwhere the neighborhood covariance matrix Rk is defined by (182). In other words, Ewk,i → wo for all nodes k as i → ∞.\nObserve again that the mean stability condition (250) does not depend on the specific combination matrices A1 and A2 that are being used. Only the combination matrix C influences the condition on the step-size through the neighborhood covariance matrices {Rk}. Observe further that the statement of the lemma does not require the network to be connected. Moreover, when C = IN , in which case the nodes only share weight estimators and do not share neighborhood data {dℓ(i),uℓ,i} as in (204)–(206), condition (250) becomes\nµk < 2\nλmax(Ru,k) (adaptive cooperation with C = IN ) (251)\nResults (250) and (251) are reminiscent of a classical result for the stand-alone LMS algorithm, as in the non-cooperative case (207), where it is known that the estimator by each individual node in this case would converge in the mean to wo if, and only if, its step-size satisfies\nµk < 2\nλmax(Ru,k) (non-cooperative adaptation) (252)\nThe following statement provides a bi-directional result that ensures the mean convergence of the adaptive diffusion strategy for any choice of left-stochastic combination matrices A1 and A2.\nTheorem 6.2. (Mean Convergence for Arbitrary Combination Matrices) Consider the problem of optimizing the global cost (92) with the individual cost functions given by (93). Pick a right stochastic matrix C satisfying (166). Assume each node in the network measures data that satisfy the conditions described in Sec. 6.1. Then, the estimators {wk,i} generated by the adaptive diffusion strategy (201)–(203), converge in the mean to wo, for all choices of left stochastic matrices A1 and A2 satisfying (166) if, and only if,\nµk < 2\nλmax(Rk) (253)\nAs was the case with steepest-descent diffusion strategies, the adaptive diffusion strategy (201)–(203) also enhances the convergence rate of the mean of the error vector towards zero relative to the non-cooperative strategy (207). The next results restate Theorems 5.3–5.5; they assume C is a doubly stochastic matrix.\nTheorem 6.3. (Mean Convergence Rate is Enhanced: Uniform Step-Sizes) Consider the problem of optimizing the global cost (92) with the individual cost functions given by (93). Pick a doubly stochastic matrix C satisfying (196) and left stochastic matrices A1 and A2 satisfying (166). Assume each node in the network measures data that satisfy the conditions described in Sec. 6.1. Consider two modes of operation. In one mode, each node in the network runs the adaptive diffusion algorithm (201)–(203). In the second mode, each node operates individually and runs the non-cooperative LMS algorithm (207). In both cases, the positive step-sizes used by all nodes are assumed to be the same, say, µk = µ for all k, and the value of µ is chosen to satisfy the required mean stability conditions (250) and (252), which are met by selecting\nµ < min 1≤k≤N\n{ 2\nλmax(Ru,k)\n} (254)"
    }, {
      "heading" : "It then holds that the magnitude of the mean error vector, ‖E w̃i‖ in the diffusion case decays to zero more rapidly than in the non-cooperative case. In other words, diffusion enhances convergence rate.",
      "text" : "Theorem 6.4. (Mean Convergence Rate is Enhanced: Uniform Covariance Data) Consider the same setting of Theorem 6.3. Assume the covariance data are uniform across all nodes, say, Ru,k = Ru is independent of k. Assume further that the nodes in both modes of operation employ steps-sizes µk that are chosen to satisfy the required stability conditions (250) and (252), which in this case are met by:\nµk < 2\nλmax(Ru) , k = 1, 2, . . . , N (255)"
    }, {
      "heading" : "It then holds that the magnitude of the mean error vector, ‖E w̃i‖, in the diffusion case also decays to zero more rapidly than in the non-cooperative case. In other words, diffusion enhances convergence rate.",
      "text" : "The next statement considers the case of ATC and CTA strategies (204)–(206) without information exchange, which correspond to the choice C = IN . The result establishes that these strategies always enhance the convergence rate over the non-cooperative case, without the need to assume uniform step-sizes or uniform covariance data.\nTheorem 6.5. (Mean Convergence Rate is Enhanced when C = I) Consider the problem of optimizing the global cost (92) with the individual cost functions given by (93). Pick left stochastic matrices A1 and A2 satisfying (166) and set C = IN . This situation covers the ATC and CTA strategies (204)–(206) that do not involve information exchange. Assume each node in the network measures data that satisfy the conditions described in Sec. 6.1. Consider two modes of operation. In one mode, each node in the network\nruns the adaptive diffusion algorithm (204)–(206). In the second mode, each node operates individually and runs the non-cooperative LMS algorithm (207). In both cases, the positive step-sizes are chosen to satisfy the required stability conditions (251) and (252), which in this case are met by\nµk < 2\nλmax(Ru,k) , k = 1, 2, . . . , N (256)"
    }, {
      "heading" : "It then holds that the magnitude of the mean error vector, ‖E w̃i‖, in the diffusion case decays to zero more rapidly than in the non-cooperative case. In other words, diffusion cooperation enhances convergence rate.",
      "text" : "The results of the previous theorems again highlight the following important facts about the role of the combination matrices {A1, A2, C} in the convergence behavior of the adaptive diffusion strategy (201)–(203):\n(a) The matrix C influences the mean stability of the network through its influence on the bound in (250). This is because the matrices {Rk} depend on the entries of C. The matrices {A1, A2} do not influence network mean stability.\n(b) The matrices {A1, A2, C} influence the rate of convergence of the mean weight-error vector over the network since they influence the spectral radius of the matrix AT2 (INM −MR)A T 1 , which controls the\ndynamics of the weight error vector in (248)."
    }, {
      "heading" : "6.5 Mean-Square Stability",
      "text" : "It is not sufficient to ensure the stability of the weight-error vector in the mean sense. The error vectors, w̃k,i, may be converging on average to zero but they may have large fluctuations around the zero value. We therefore need to examine how small the error vectors get. To do so, we perform a mean-square-error analysis. The purpose of the analysis is to evaluate how the variances E‖w̃k,i‖\n2 evolve with time and what their steady-state values are, for each node k.\nIn this section, we are particularly interested in evaluating the evolution of two mean-square-errors, namely,\nE‖w̃k,i‖ 2 and E |ea,k(i)| 2 (257)\nThe steady-state values of these quantities determine the MSD and EMSE performance levels at node k and, therefore, convey critical information about the performance of the network. Under the independence assumption on the regression data from Sec. 6.1, it can be verified that the EMSE variance can be written as:\nE |ea,k(i)| 2 ∆= E |uk,iw̃k,i−1| 2\n= E w̃∗k,i−1u ∗ k,iuk,iw̃k,i−1 = E [ E (w̃∗k,i−1u ∗ k,iuk,iw̃k,i−1|w̃k,i) ] = E w̃∗k,i−1 [ Eu∗k,iuk,i ] w̃k,i−1 = E w̃∗k,i−1Ru,kw̃k,i−1 = E‖w̃k,i−1‖ 2 Ru,k\n(258)\nin terms of a weighted square measure with weighting matrix Ru,k. Here we are using the notation ‖x‖2Σ to denote the weighted square quantity x∗Σx, for any column vector x and matrix Σ. Thus, we can evaluate mean-square-errors of the form (257) by evaluating the means of weighted square quantities of the following form:\nE‖w̃k,i‖ 2 Σk\n(259)\nfor an arbitrary Hermitian nonnegative-definite weighting matrix Σk that we are free to choose. By setting Σk to different values (say, Σk = I or Σk = Ru,k), we can extract various types of information about\nthe nodes and the network, as the discussion will reveal. The approach we follow is based on the energy conservation framework of [4, 5, 57].\nSo, let Σ denote an arbitrary N × N block Hermitian nonnegative-definite matrix that we are free to choose, with M ×M block entries {Σℓk}. Let σ denote the (NM)2 × 1 vector that is obtained by stacking the columns of Σ on top of each other, written as\nσ ∆ = vec(Σ) (260)\nIn the sequel, it will become more convenient to work with the vector representation σ than with the matrix Σ itself.\nWe start from the weight-error vector recursion (246) and re-write it more compactly as:\nw̃i = Biw̃i−1 − Gsi, i ≥ 0 (261)\nwhere the coefficient matrices Bi and G are short-hand representations for\nBi ∆ = AT2 (INM −MRi)A T 1 (262)\nand\nG ∆ = AT2 MC T (263)\nNote that Bi is stochastic and time-variant, while G is constant. We denote the mean of Bi by\nB ∆ = EBi = A T 2 (INM −MR)A T 1 (264)\nwhere R is defined by (181). Now equating weighted square measures on both sides of (261) we get\n‖w̃i‖ 2 Σ = ‖Biw̃i−1 − Gsi‖ 2 Σ (265)\nExpanding the right-hand side we find that\n‖w̃i‖ 2 Σ = w̃ ∗ i−1B ∗ iΣBiw̃i−1 + s ∗ iG TΣGsi −\nw̃ ∗ i−1B ∗ iΣGsi − s ∗ iG TΣBiw̃i−1 (266)\nUnder expectation, the last two terms on the right-hand side evaluate to zero so that\nE‖w̃i‖ 2 Σ = E\n( w̃\n∗ i−1B ∗ iΣBiw̃i−1\n) + E ( s∗iG TΣGsi )\n(267)\nLet us evaluate each of the expectations on the right-hand side. The last expectation is given by\nE ( s∗iG TΣGsi )\n= Tr ( GTΣG Esis ∗ i )\n(241) = Tr ( GTΣGS )\n= Tr ( ΣGSGT ) (268)\nwhere S is defined by (241) and where we used the fact that Tr(AB) = Tr(BA) for any two matrices A and B of compatible dimensions. With regards to the first expectation on the right-hand side of (267), we have\nE ( w̃\n∗ i−1B ∗ iΣBiw̃i−1\n) = E [ E ( w̃\n∗ i−1B ∗ iΣBiw̃i−1|w̃i−1\n)]\n= E w̃∗i−1 [E (B ∗ iΣBi)] w̃i−1 ∆ = E w̃∗i−1Σ ′w̃i−1 = E‖w̃i−1‖ 2 Σ′ (269)\nwhere we introduced the nonnegative-definite weighting matrix\nΣ′ ∆ = EB∗iΣBi\n(262) = EA1 (INM −RiM)A2ΣA T 2 (INM −MRi)A T 1\n= A1A2ΣA T 2 A T 1 −A1A2ΣA T 2 MRA T 1 −A1RMA2ΣA T 2 A T 1 + O(M 2) (270)\nwhere R is defined by (181) and the term O(M2) denotes the following factor, which depends on the square of the step-sizes, {µ2k}:\nO(M2) = E ( A1RiMA2ΣA T 2 MRiA T 1 ) (271)\nThe evaluation of the above expectation depends on higher-order moments of the regression data. While we can continue with the analysis by taking this factor into account, as was done in [4, 5, 18, 57], it is sufficient for the exposition in this article to focus on the case of sufficiently small step-sizes where terms involving higher powers of the step-sizes can be ignored. Therefore, we continue our discussion by letting\nΣ′ ∆ = A1A2ΣA T 2 A T 1 −A1A2ΣA T 2 MRA T 1 −A1RMA2ΣA T 2 A T 1 (272)\nThe weighting matrix Σ′ is fully defined in terms of the step-size matrix, M, the network topology through the matrices {A1,A2, C}, and the regression statistical profile through R. Expression (272) tells us how to construct Σ′ from Σ. The expression can be transformed into a more compact and revealing form if we instead relate the vector forms σ′ = vec(Σ′) and σ = vec(Σ). Using the following equalities for arbitrary matrices {U,W,Σ} of compatible dimensions [5]:\nvec(UΣW ) = (WT ⊗ U)σ (273)\nTr(ΣW ) = [ vec(WT ) ]T σ (274)\nand applying the vec operation to both sides of (272) we get\nσ′ = (A1A2 ⊗A1A2)σ − ( A1R TMA2 ⊗A1A2 ) σ − (A1A2 ⊗A1RMA2)σ\nThat is,\nσ′ ∆ = Fσ (275)\nwhere we are introducing the coefficient matrix of size (NM)2 × (NM)2:\nF ∆ = (A1A2 ⊗A1A2) − ( A1R TMA2 ⊗A1A2 ) − (A1A2 ⊗A1RMA2) (276)\nA reasonable approximate expression for F for sufficiently small step-sizes is\nF ≈ BT ⊗ B∗ (277)\nIndeed, if we replace B from (264) into (277) and expand terms, we obtain the same factors that appear in (276) plus an additional term that depends on the square of the step-sizes, {µ2k}, whose effect can be ignored for sufficiently small step-sizes.\nIn this way, using in addition property (274), we find that relation (267) becomes:\nE‖w̃i‖ 2 Σ = E‖w̃i−1‖ 2 Σ′ +\n[ vec ( GSTGT )]T σ (278)\nThe last term is dependent on the network topology through the matrix G, which is defined in terms of {A2, C,M}, and the noise and regression data statistical profile through S. It is convenient to introduce the alternative notation ‖x‖2σ to refer to the weighted square quantity ‖x‖ 2 Σ, where σ = vec(Σ). We shall use these two notations interchangeably. The convenience of the vector notation is that it allows us to exploit the simpler linear relation (275) between σ′ and σ to rewrite (278) as shown in (279) below, with the same weight vector σ appearing on both sides.\nTheorem 6.6. (Variance Relation) Consider the data model of Sec. 6.1 and the independence statistical conditions imposed on the noise and regression data, including (208)–(215). Assume further sufficiently small step-sizes are used so that terms that depend on higher-powers of the step-sizes can be ignored. Pick left stochastic matrices A1 and A2 and a right stochastic matrix C satisfying (166). Under these conditions, the weight-error vector w̃i = col{w̃k,i}Nk=1 associated with a network running the adaptive diffusion strategy (201)–(203) satisfies the following variance relation\nE‖w̃i‖ 2 σ = E‖w̃i−1‖ 2 Fσ +\n[ vec ( YT )]T σ (279)\nfor any Hermitian nonnegative-definite matrix Σ with σ = vec(Σ), and where {S,G,F} are defined by (241), (263), and (277), and\nY ∆ = GSGT (280)\nNote that relation (279) is not an actual recursion; this is because the weighting matrices {σ,Fσ} on both sides of the equality are different. The relation can be transformed into a true recursion by expanding it into a convenient state-space model; this argument was pursued in [4, 5, 18, 57] and is not necessary for the exposition here, except to say that stability of the matrix F ensures the mean-square stability of the filter — this fact is also established further ahead through relation (327). By mean-square stability we mean that each term E‖w̃k,i‖2 remains bounded over time and converges to a steady-state MSDk value. Moreover, the spectral radius of F controls the rate of convergence of E‖w̃i‖2 towards its steady-state value.\nTheorem 6.7. (Mean-Square Stability) Consider the same setting of Theorem 6.6. The adaptive diffusion strategy (201)–(203) is mean-square stable if, and only if, the matrix F defined by (276), or its approximation (277), is stable (i.e., all its eigenvalues lie strictly inside the unit disc). This condition is satisfied by sufficiently small positive step-sizes {µk} that also satisfy:\nµk < 2\nλmax(Rk) (281)\nwhere the neighborhood covariance matrix Rk is defined by (182). Moreover, the convergence rate of the algorithm is determined by the value [ρ(B)]2 (the square of the spectral radius of B).\nProof. Recall that, for two arbitrary matrices A and B of compatible dimensions, the eigenvalues of the Kronecker product A⊗B is formed of all product combinations λi(A)λj(B) of the eigenvalues of A and B [19]. Therefore, using expression (277), we have that ρ(F) = [ρ(B)]2. It follows that F is stable if, and only if, B is stable. We already noted earlier in Theorem 6.1 that condition (281) ensures the stability of B. Therefore, step-sizes that ensure stability in the mean and are sufficiently small will also ensure mean-square stability.\nRemark. More generally, had we not ignored the second-order term (271), the expression for F would have been the following. Starting from the definition Σ′ = EB∗iΣBi, we would get\nσ′ = ( EB\nT i ⊗ B ∗ i\n) σ\nso that\nF ∆ = E ( B\nT i ⊗B ∗ i\n) (for general step-sizes) (282)\n= (A1 ⊗A1) · { I − ( RTM⊗ I ) − (I ⊗RM) + E ( R T i M⊗RiM )} · (A2 ⊗A2)\nMean-square stability of the filter would then require the step-sizes {µk} to be chosen such that they ensure the stability of this matrix F (in addition to condition (281) to ensure mean stability)."
    }, {
      "heading" : "6.6 Network Mean-Square Performance",
      "text" : "We can now use the variance relation (279) to evaluate the network performance, as well as the performance of the individual nodes, in steady-state. Since the dynamics is mean-square stable for sufficiently small step-sizes, we take the limit of (279) as i → ∞ and write:\nlim i→∞\nE‖w̃i‖ 2 σ = lim\ni→∞ E‖w̃i−1‖\n2 Fσ +\n[ vec ( YT )]T σ (283)\nGrouping terms leads to the following result.\nCorollary 6.1. (Steady-State Variance Relation) Consider the same setting of Theorem 6.6. The weight-error vector, w̃i = col{w̃k,i}Nk=1, of the adaptive diffusion strategy (201)–(203) satisfies the following relation in steady-state:\nlim i→∞\nE‖w̃i‖ 2 (I−F)σ =\n[ vec ( YT )]T σ (284)\nfor any Hermitian nonnegative-definite matrix Σ with σ = vec(Σ), and where {F ,Y} are defined by (277) and (280).\nExpression (284) is a very useful relation; it allows us to evaluate the network MSD and EMSE through proper selection of the weighting vector σ (or, equivalently, the weighting matrix Σ). For example, the network MSD is defined as the average value:\nMSDnetwork ∆ = lim\ni→∞\n1\nN\nN∑\nk=1\nE‖w̃k,i‖ 2 (285)\nwhich amounts to averaging the MSDs of the individual nodes. Therefore,\nMSDnetwork = lim i→∞\n1\nN E‖w̃i‖ 2 = lim i→∞ E‖w̃i‖ 2 1/N (286)\nThis means that in order to recover the network MSD from relation (284), we should select the weighting vector σ such that\n(I −F)σ = 1\nN vec (INM )\nSolving for σ and substituting back into (284) we arrive at the following expression for the network MSD:\nMSDnetwork = 1 N · [ vec ( YT )]T · (I −F)−1 · vec (INM ) (287)\nLikewise, the network EMSE is defined as the average value\nEMSEnetwork ∆ = lim\ni→∞\n1\nN\nN∑\nk=1\nE |ea,k(i)| 2\n= lim i→∞\n1\nN\nN∑\nk=1\nE‖w̃k,i‖ 2 Ru,k\n(288)\nwhich amounts to averaging the EMSEs of the individual nodes. Therefore,\nEMSEnetwork = lim i→∞\n1\nN E‖w̃i‖ 2 diag{Ru,1,Ru,2,...,Ru,N} = lim i→∞\n1\nN E‖w̃i‖\n2 Ru (289)\nwhere Ru is the matrix defined earlier by (184), and which we repeat below for ease of reference:\nRu = diag{Ru,1, Ru,2, . . . , Ru,N} (290)\nThis means that in order to recover the network EMSE from relation (284), we should select the weighting vector σ such that\n(I −F)σ = 1\nN vec (Ru) (291)\nSolving for σ and substituting into (284) we arrive at the following expression for the network EMSE:\nEMSEnetwork = 1 N · [ vec ( YT )]T · (I − F)−1 · vec (Ru) (292)"
    }, {
      "heading" : "6.7 Mean-Square Performance of Individual Nodes",
      "text" : "We can also assess the mean-square performance of the individual nodes in the network from (284). For instance, the MSD of any particular node k is defined by\nMSDk ∆ = lim\ni→∞ E‖w̃k,i‖\n2 (293)\nIntroduce the N ×N block diagonal matrix with blocks of size M ×M , where all blocks on the diagonal are zero except for an identity matrix on the diagonal block of index k, i.e.,\nJk ∆ = diag{ 0M , . . . , 0M , IM , 0M , . . . , 0M } (294)\nThen, we can express the node MSD as follows:\nMSDk ∆ = lim\ni→∞ E‖w̃i‖\n2 Jk\n(295)\nThe same argument that was used to obtain the network MSD then leads to\nMSDk = [ vec ( YT )]T · (I −F)−1 · vec (Jk) (296)\nLikewise, the EMSE of node k is defined by\nEMSEk ∆ = lim\ni→∞ E |ea,k(i)|\n2\n= lim i→∞\nE‖w̃k,i‖ 2 Ru,k\n(297)\nIntroduce the N ×N block diagonal matrix with blocks of size M ×M , where all blocks on the diagonal are zero except for the diagonal block of index k whose value is Ru,k, i.e.,\nTk ∆ = diag{ 0M , . . . , 0M , Ru,k, 0M , . . . , 0M } (298)\nThen, we can express the node EMSE as follows:\nEMSEk ∆ = lim\ni→∞ E‖w̃i‖\n2 Tk (299)\nThe same argument that was used to obtain the network EMSE then leads to\nEMSEk = [ vec ( YT )]T · (I −F)−1 · vec (Tk) (300)\nWe summarize the results in the following statement.\nTheorem 6.8. (Network Mean-Square Performance) Consider the same setting of Theorem 6.6. Introduce the 1× (NM)2 row vector hT defined by\nhT ∆ = [ vec ( YT )]T · (I −F)−1 (301)\nwhere {F ,Y} are defined by (277) and (280). Then the network MSD and EMSE and the individual node performance measures are given by\nMSDnetwork = hT · vec (INM ) /N (302) EMSEnetwork = hT · vec (Ru) /N (303)\nMSDk = h T · vec (Jk) (304) EMSEk = h T · vec (Tk) (305)\nwhere {Jk, Tk} are defined by (294) and (298).\nWe can obviously recover from the above expressions the performance of the nodes in the non-cooperative implementation (207), where each node performs its adaptation individually, by setting A1 = A2 = C = IN .\nWe can express the network MSD, and its EMSE if desired, in an alternative useful form involving a series representation.\nCorollary 6.2. (Series Representation for Network MSD) Consider the same setting of Theorem 6.6. The network MSD can be expressed in the following alternative series expansion form:\nMSDnetwork = 1\nN\n∞∑\nj=0\nTr ( BjYB∗j ) (306)\nwhere\nY = GSGT (307) G = AT2 MC T (308) B = AT2 (I −MR)A T 1 (309)\nProof. Since F is stable when the filter is mean-square stable, we can expand (I − F)−1 as\n(I − F)−1 = I + F + F2 + . . .\n(277) = I + ( BT ⊗ B∗ ) + ( BT ⊗B∗ )2 + . . .\nSubstituting into (287) and using property (274), we obtain the desired result."
    }, {
      "heading" : "6.8 Uniform Data Profile",
      "text" : "We can simplify expressions (307)–(309) for {Y,G,B} in the case when the regression covariance matrices are uniform across the network and all nodes employ the same step-size, i.e., when\nRu,k = Ru, for all k (uniform covariance profile) (310)\nµk = µ, for all k (uniform step-sizes) (311)\nand when the combination matrix C is doubly stochastic, so that\nC1 = 1, CT1 = 1 (312)\nWe refer to conditions (310)–(312) as corresponding to a uniform data profile environment. The noise variances, {σ2v,k}, do not need to be uniform so that the signal-to-noise ratio (SNR) across the network can still vary from node to node. The simplified expressions derived in the sequel will be useful in Sec. 7 when we compare the performance of various cooperation strategies.\nThus, under conditions (310)–(312), expressions (180), (181), and (263) for {M,R,G} simplify to\nM = µINM (313)\nR = IN ⊗Ru (314)\nG = µAT2 C T (315)\nSubstituting these values into expression (309) for B we get\nB = AT2 (I −MR)A T 1\n= (AT2 ⊗ I) · (I − µ(I ⊗Ru)) · (A T 1 ⊗ I) = (AT2 ⊗ I)(A T 1 ⊗ I) − µ(A T 2 ⊗ I)(I ⊗Ru)(A T 1 ⊗ I) = (AT2 A T 1 ⊗ I) − µ(A T 2 A T 1 ⊗Ru) = AT2 A T 1 ⊗ (I − µRu) (316)\nwhere we used the useful Kronecker product identities:\n(X + Y )⊗ Z = (X ⊗ Z) + (Y ⊗ Z) (317)\n(X ⊗ Y )(W ⊗ Z) = (XW ⊗ Y Z) (318)\nfor any matrices {X,Y, Z,W} of compatible dimensions. Likewise, introduce the N × N diagonal matrix with noise variances:\nRv ∆ = diag{σ2v,1, σ 2 v,2, . . . , σ 2 v,N} (319)\nThen, expression (241) for S becomes\nS = diag{σ2v,1Ru, σ 2 v,2Ru, . . . , σ 2 v,NRu}\n= Rv ⊗Ru (320)\nIt then follows that we can simplify expression (307) for Y as:\nY = µ2AT2 C TSCA2\n= µ2 · (AT2 ⊗ I) · (C T ⊗ I)⊗ (Rv ⊗Ru) · (C ⊗ I) · (A2 ⊗ I) = µ2(AT2 C TRvCA2 ⊗Ru) (321)\nCorollary 6.3. (Network MSD for Uniform Data Profile) Consider the same setting of Theorem 6.6 with the additional requirement that conditions (310)–(312) for a uniform data profile hold. The network MSD is still given by the same series representation (306) where now\nY = µ2(AT2 C TRvCA2 ⊗Ru) (322) B = AT2 A T 1 ⊗ (I − µRu) (323)\nUsing these expressions, we can decouple the network MSD expression (306) into two separate factors: one is dependent on the step-size and data covariance {µ,Ru}, and the other is dependent on the combination matrices and noise profile {A1, A2, C,Rv}:\nMSDnetwork = µ2\nN\n∞∑\nj=0\nTr ([( AT2 A T 1 )j ( AT2 C TRvCA2 ) (A1A2) j ] ⊗ [ (I − µRu) jRu(I − µRu) j ])\n(324)\nProof. Using (306) and the given expressions (322)–(323) for {Y,B}, we get\nMSDnetwork = µ2\nN\n∞∑\nj=0\nTr ([( AT2 A T 1 )j ⊗ (I − µRu) j ] (AT2 C TRvCA2 ⊗Ru) [ (A1A2) j ⊗ (I − µRu) j ])\nResult (324) follows from property (317)."
    }, {
      "heading" : "6.9 Transient Mean-Square Performance",
      "text" : "Before comparing the mean-square performance of various cooperation strategies, we pause to comment that the variance relation (279) can also be used to characterize the transient behavior of the network, and not just its steady-state performance. To see this, iterating (279) starting from i = 0, we find that\nE‖w̃i‖ 2 σ = E‖w̃−1‖ 2 Fi+1σ +\n[ vec ( YT )]T ·   i∑\nj=0\nF jσ   (325)\nwhere w̃−1 ∆ = wo −w−1 (326)\nin terms of the initial condition, w−1. If this initial condition happens to be w−1 = 0, then w̃−1 = w o. Comparing expression (325) at time instants i and i− 1 we can relate E‖w̃i‖2σ and E‖w̃i−1‖ 2 σ as follows:\nE‖w̃i‖ 2 σ = E‖w̃i−1‖ 2 σ +\n[ vec ( YT )]T · F iσ − E‖w̃−1‖ 2 (I−F)Fiσ (327)\nThis recursion relates the same weighted square measures of the error vectors {w̃i, w̃i−1}. It therefore describes how these weighted square measures evolve over time. It is clear from this relation that, for meansquare stability, the matrix F needs to be stable so that the terms involving F i do not grow unbounded.\nThe learning curve of the network is the curve that describes the evolution of the network EMSE over time. At any time i, the network EMSE is denoted by ζ(i) and measured as:\nζ(i) ∆ =\n1\nN\nN∑\nk=1\nE |ea,k(i)| 2 (328)\n= 1\nN\nN∑\nk=1\nE‖w̃k,i‖ 2 Ru,k\nThe above expression indicates that ζ(i) is obtained by averaging the EMSE of the individual nodes at time i. Therefore,\nζ(i) = 1\nN E‖w̃i‖ 2 diag{Ru,1,Ru,2,...,Ru,N}\n= 1\nN E‖w̃i‖\n2 Ru (329)\nwhere Ru is the matrix defined by (290). This means that in order to evaluate the evolution of the network EMSE from relation (327), we simply select the weighting vector σ such that\nσ = 1\nN vec (Ru) (330)\nSubstituting into (327) we arrive at the learning curve for the network.\nCorollary 6.4. (Network Learning Curve) Consider the same setting of Theorem 6.6. Let ζ(i) denote the network EMSE at time i, as defined by (328). Then, the learning curve of the network corresponds to the evolution of ζ(i) with time and is described by the following recursion over i ≥ 0:\nζ(i) = ζ(i− 1) + 1\nN\n[ vec ( YT )]T · F i · vec (Ru) − 1\nN E‖w̃−1‖\n2 (I−F)Fivec(Ru)\n(331)\nwhere {F ,Y,Ru} are defined by (277), (280), and (290)."
    }, {
      "heading" : "7 Comparing the Performance of Cooperative Strategies",
      "text" : "Using the expressions just derived for the MSD of the network, we can compare the performance of various cooperative and non-cooperative strategies. Table 6 further ahead summarizes the results derived in this section and the conditions under which they hold."
    }, {
      "heading" : "7.1 Comparing ATC and CTA Strategies",
      "text" : "We first compare the performance of the adaptive ATC and CTA diffusion strategies (153) and (154) when they employ a doubly stochastic combination matrix A. That is, let us consider the two scenarios:\nC, A1 = A, A2 = IN (adaptive CTA strategy) (332)\nC, A1 = IN , A2 = A (adaptive ATC strategy) (333)\nwhere A is now assumed to be doubly stochastic, i.e.,\nA1 = 1, AT1 = 1 (334)\nwith its rows and columns adding up to one. For example, these conditions are satisfied when A is left stochastic and symmetric. Then, expressions (307) and (309) give:\nBcta = (I −MR)A T , Ycta = MC TSCM (335) Batc = A T (I −MR), Yatc = A TMCTSCMA (336)\nwhere A = A⊗ IM (337)\nFollowing [18], introduce the auxiliary nonnegative-definite matrix\nHj ∆ = [ (I −MR)AT ]j ·MCTSCM · [ (I −MR)AT ]∗j (338)\nThen, it is immediate to verify from (306) that\nMSDnetworkcta = 1\nN\n∞∑\nj=0\nTr(Hj) (339)\nMSDnetworkatc = 1\nN\n∞∑\nj=0\nTr(AT Hj A) (340)\nso that\nMSDnetworkcta −MSD network atc =\n1\nN\n∞∑\nj=0\nTr ( Hj −A THjA )\n(341)\nNow, since A is doubly stochastic, it also holds that the enlarged matrix A is doubly stochastic. Moreover, for any doubly stochastic matrix A and any nonnegative-definite matrix H of compatible dimensions, it holds that (see part (f) of Theorem C.3):\nTr(ATHA) ≤ Tr(H) (342)\nApplying result (342) to (341) we conclude that\nMSDnetworkatc ≤ MSD network cta (doubly stochastic A) (343)\nso that the adaptive ATC strategy (153) outperforms the adaptive CTA strategy (154) for doubly stochastic combination matrices A."
    }, {
      "heading" : "7.2 Comparing Strategies with and without Information Exchange",
      "text" : "We now examine the effect of information exchange (C 6= I) on the performance of the adaptive ATC and CTA diffusion strategies (153)–(154) under conditions (310)–(312) for uniform data profile.\nCTA Strategies We start with the adaptive CTA strategy (154), and consider two scenarios with and without information exchange. These scenarios correspond to the following selections in the general description (201)–(203):\nC 6= I, A1 = A, A2 = IN (adaptive CTA with information exchange ) (344)\nC = I, A1 = A, A2 = IN (adaptive CTA without information exchange) (345)\nThen, expressions (322) and (323) give:\nBcta,C6=I = A T ⊗ (I − µRu), Ycta,C6=I = µ 2(CTRvC ⊗Ru) (346) Bcta,C=I = A T ⊗ (I − µRu), Ycta,C=I = µ 2(Rv ⊗Ru) (347)\nwhere the matrix Rv is defined by (319). Note that Bcta,C6=I = Bcta,C=I, so we denote them simply by B in the derivation that follows. Then, from expression (306) for the network MSD we get:\nMSDnetworkcta,C=I −MSD network cta,C 6=I =\nµ2\nN\n∞∑\nj=0\nTr ( Bj [ (Rv − C TRvC)⊗Ru ] B∗j ) (348)\nIt follows that the difference in performance between both CTA implementations depends on how the matrices Rv and C TRvC compare to each other:\n(1) When Rv − CTRvC ≥ 0, we obtain\nMSDnetworkcta,C=I ≥ MSD network cta,C 6=I (when C TRvC ≤ Rv) (349)\nso that a CTA implementation with information exchange performs better than a CTA implementation without information exchange. Note that the condition on {Rv, C} corresponds to requiring\nCTRvC ≤ Rv (350)\nwhich can be interpreted to mean that the cooperation matrix C should be such that it does not amplify the effect of measurement noise. For example, this situation occurs when the noise profile is uniform across the network, in which case Rv = σ 2 vIM . This is because it would then hold that\nRv − C TRvC = σ 2 v(I − C TC) ≥ 0 (351)\nin view of the fact that (I − CTC) ≥ 0 since C is doubly stochastic (cf. property (e) in Lemma C.3).\n(2) When Rv − CTRvC ≤ 0, we obtain\nMSDnetworkcta,C=I ≤ MSD network cta,C 6=I (when C TRvC ≥ Rv) (352)\nso that a CTA implementation without information exchange performs better than a CTA implementation with information exchange. In this case, the condition on {Rv, C} indicates that the combination matrix C ends up amplifying the effect of noise.\nATC Strategies We can repeat the argument for the adaptive ATC strategy (153), and consider two scenarios with and without information exchange. These scenarios correspond to the following selections in the general description (201)–(203):\nC 6= I, A1 = IN , A2 = A (adaptive ATC with information exchange ) (353)\nC = I, A1 = IN , A2 = A (adaptive ATC without information exchange) (354)\nThen, expressions (322) and (323) give:\nBatc,C6=I = A T ⊗ (I − µRu), Yatc,C6=I = µ 2(ATCTRvCA⊗Ru) (355) Batc,C=I = A T ⊗ (I − µRu), Yatc,C=I = µ 2(ATRvA⊗Ru) (356)\nNote again that Batc,C6=I = Batc,C=I, so we denote them simply by B. Then,\nMSDnetworkatc,C=I −MSD network atc,C 6=I =\nµ2\nN\n∞∑\nj=0\nTr ( Bj [ AT (Rv − C TRvC)A ⊗Ru ] B∗j ) (357)\nIt again follows that the difference in performance between both ATC implementations depends on how the matrices Rv and C TRvC compare to each other and we obtain:\nMSDnetworkatc,C=I ≥ MSD network atc,C 6=I (when C TRvC ≤ Rv) (358)\nand\nMSDnetworkatc,C=I ≤ MSD network atc,C 6=I (when C TRvC ≥ Rv) (359)"
    }, {
      "heading" : "7.3 Comparing Diffusion Strategies with the Non-Cooperative Strategy",
      "text" : "We now compare the performance of the adaptive CTA strategy (154) to the non-cooperative LMS strategy (207) assuming conditions (310)–(312) for uniform data profile. These scenarios correspond to the following selections in the general description (201)–(203):\nC, A1 = A, A2 = I (adaptive CTA) (360)\nC = I, A1 = I, A2 = I (non-cooperative LMS) (361)\nwhere A is further assumed to be doubly stochastic (along with C) so that\nA1 = 1, AT1 = 1 (362)\nThen, expressions (322) and (323) give:\nBcta = A T ⊗ (I − µRu), Ycta = µ 2(CTRvC ⊗Ru) (363) Blms = I ⊗ (I − µRu), Ylms = µ 2(Rv ⊗Ru) (364)\nNow recall that C = C ⊗ IM (365)\nso that, using the Kronecker product property (317),\nYcta = µ 2(CTRvC ⊗Ru)\n= µ2(CT ⊗ IM )(Rv ⊗Ru)(C ⊗ IM ) = µ2CT (Rv ⊗Ru)C = CTYlms C (366)\nThen,\nMSDnetworklms −MSD network cta =\n1\nN\n∞∑\nj=0\nTr ( BjlmsYlmsB ∗j lms ) − 1\nN\n∞∑\nj=0\nTr ( BjctaC TYlmsCB ∗j cta )\n= 1\nN\n∞∑\nj=0\nTr ( B∗jlmsB j lmsYlms ) − 1\nN\n∞∑\nj=0\nTr ( CB∗jctaB j ctaC TYlms )\n= 1\nN\n∞∑\nj=0\nTr [(\nB∗jlmsB j lms − CB ∗j ctaB j ctaC T ) Ylms ] (367)\nLet us examine the difference:\nB∗jlmsB j lms − CB ∗j ctaB j ctaC T = ( I ⊗ (I − µRu) 2j ) − ( CAj ⊗ (I − µRu) j ) ( AjTCT ⊗ (I − µRu) j )\n(317) = ( I ⊗ (I − µRu) 2j ) − ( CAjAjTCT ⊗ (I − µRu) 2j )\n= (I − CAjAjTCT )⊗ (I − µRu) 2j (368)\nNow, due to the even power, it always holds that (I − µRu)2j ≥ 0. Moreover, since Aj and C are doubly stochastic, it follows that CAjAjTCT is also doubly stochastic. Therefore, the matrix (I − CAjAjTCT ) is nonnegative-definite as well (cf. property (e) of Lemma C.3). It follows that\nB∗jlmsB j lms − CB ∗j ctaB j ctaC T ≥ 0 (369)\nBut since Ylms ≥ 0, we conclude from (367) that\nMSDnetworklms ≥ MSD network cta (370)\nThis is because for any two Hermitian nonnegative-definite matrices A and B of compatible dimensions, it holds that Tr(AB) ≥ 0; indeed if we factor B = XX∗ with X full rank, then Tr(AB) = Tr(X∗AX) ≥ 0. We conclude from this analysis that adaptive CTA diffusion performs better than non-cooperative LMS under uniform data profile conditions and doubly stochastic A. If we refer to the earlier result (343), we conclude that the following relation holds:\nMSDnetworkatc ≤ MSD network cta ≤ MSD network lms (371)\nTable 6 lists the comparison results derived in this section and lists the conditions under which the conclusions hold."
    }, {
      "heading" : "8 Selecting the Combination Weights",
      "text" : "The adaptive diffusion strategy (201)–(203) employs combination weights {a1,ℓk, a2,ℓk, cℓk} or, equivalently, combination matrices {A1, A2, C}, where A1 and A2 are left-stochastic matrices and C is a right-stochastic matrix. There are several ways by which these matrices can be selected. In this section, we describe constructions that result in left-stochastic or doubly-stochastic combination matrices, A. When a rightstochastic combination matrix is needed, such as C, then it can be obtained by transposition of the leftstochastic constructions shown below."
    }, {
      "heading" : "8.1 Constant Combination Weights",
      "text" : "Table 7 lists a couple of common choices for selecting constant combination weights for a network with N nodes. Several of these constructions appeared originally in the literature on graph theory. In the table, the symbol nk denotes the degree of node k, which refers to the size of its neighborhood. Likewise, the symbol nmax refers to the maximum degree across the network, i.e.,\nnmax = max 1≤k≤N {nk} (372)\nThe Laplacian rule, which appears in the second line of the table, relies on the use of the Laplacian matrix L of the network and a positive scalar γ. The Laplacian matrix is defined by (574) in App. B, namely, it is a symmetric matrix whose entries are constructed as follows [64–66]:\n[L]kℓ =    nk − 1, if k = ℓ −1, if k 6= ℓ and nodes k and ℓ are neighbors 0, otherwise\n(373)\nThe Laplacian rule can be reduced to other forms through the selection of the positive parameter γ. One choice is γ = 1/nmax, while another choice is γ = 1/N and leads to the maximum-degree rule. Obviously, it always holds that nmax ≤ N so that 1/nmax ≥ 1/N . Therefore, the choice γ = 1/nmax ends up assigning larger weights to neighbors than the choice γ = 1/N . The averaging rule in the first row of the table is one of the simplest combination rules whereby nodes simply average data from their neighbors.\nIn the constructions in Table 7, the values of the weights {aℓk} are largely dependent on the degree of the nodes. In this way, the number of connections that each node has influences the combination weights with its neighbors. While such selections may be appropriate in some applications, they can nevertheless degrade the performance of adaptation over networks [54]. This is because such weighting schemes ignore the noise profile across the network. And since some nodes can be noisier than others, it is not sufficient to rely solely on the amount of connectivity that nodes have to determine the combination weights to their neighbors. It is important to take into account the amount of noise that is present at the nodes as well. Therefore, designing combination rules that are aware of the variation in noise profile across the network is an important task.\nIt is also important to devise strategies that are able to adapt these combination weights in response to variations in network topology and data statistical profile. For this reason, following [58, 62], we describe in the next subsection one adaptive procedure for adjusting the combination weights. This procedure allows the network to assign more or less relevance to nodes according to the quality of their data."
    }, {
      "heading" : "8.2 Optimizing the Combination Weights",
      "text" : "Ideally, we would like to select N ×N combination matrices {A1, A2, C} in order to minimize the network MSD given by (302) or (306). In [18], the selection of the combination weights was formulated as the following optimization problem:\nmin {A1,A2,C}\nMSDnetwork given by (302) or (306)\nover left and right-stochastic matrices with nonnegative entries:\nAT1 1 = 1, a1,ℓk = 0 if ℓ /∈ Nk AT2 1 = 1, a2,ℓk = 0 if ℓ /∈ Nk C1 = 1, cℓk = 0 if ℓ /∈ Nk\n(374)\nWe can pursue a numerical solution to (374) in order to search for optimal combination matrices, as was done in [18]. Here, however, we are interested in an adaptive solution that becomes part of the learning process so that the network can adapt the weights on the fly in response to network conditions. We illustrate an approximate approach from [58,62] that leads to one adaptive solution that performs reasonably well in practice.\nWe illustrate the construction by considering the ATC strategy (158) without information exchange where A1 = IN , A2 = A, and C = I. In this case, recursions (204)–(206) take the form:\nψk,i = wk,i−1 + µku ∗ k,i [dk(i)− uk,iwk,i−1] (375) wk,i = ∑\nℓ∈Nk\naℓkψℓ,i (376)\nand, from (306), the corresponding network MSD performance is:\nMSDnetworkatc = 1\nN\n∞∑\nj=0\nTr ( BjatcYatcB ∗j atc ) (377)\nwhere\nBatc = A T (I −MRu) (378) Yatc = A TMSMA (379)\nRu = diag{Ru,1, Ru,2, . . . , Ru,N} (380)\nS = diag{σ2v,1Ru,1, σ 2 v,2Ru,2, . . . , σ 2 v,NRu,N} (381) M = diag{µ1IM , µ2IM , . . . , µNIM} (382)\nA = A⊗ IM (383)\nMinimizing the MSD expression (377) over left-stochastic matrices A is generally non-trivial. We pursue an approximate solution.\nTo begin with, for compactness of notation, let r denote the spectral radius of the N ×N block matrix I −MRu:\nr ∆ = ρ(I −MRu) (384)\nWe already know, in view of the mean and mean-square stability of the network, that |r| < 1. Now, consider the series that appears in (377) and whose trace we wish to minimize over A. Note that its block maximum norm can be bounded as follows:\n∥∥∥∥∥∥ ∞∑\nj=0\nBjatcYatcB ∗j atc ∥∥∥∥∥∥ b,∞ ≤ ∞∑ j=0 ∥∥∥Bjatc ∥∥∥ b,∞ · ‖Yatc‖b,∞ · ∥∥∥B∗jatc ∥∥∥ b,∞\n(a) ≤ N ·   ∞∑\nj=0\n∥∥∥Bjatc ∥∥∥ 2\nb,∞ · ‖Yatc‖b,∞\n \n≤ N ·\n  ∞∑\nj=0\n‖Batc‖ 2j b,∞ · ‖Yatc‖b,∞\n \n(b) ≤ N ·   ∞∑\nj=0\nr2j · ‖Yatc‖b,∞\n \n= N\n1− r2 · ‖Yatc‖b,∞ (385)\nwhere for step (b) we use result (602) to conclude that\n‖Batc‖b,∞ = ‖A T (I −MRu)‖b,∞\n≤ ‖AT ‖b,∞ · ‖I −MRu‖b,∞\n= ‖I −MRu‖b,∞ (602) = r (386)\nTo justify step (a), we use result (584) to relate the norms of Bjatc and its complex conjugate, [ Bjatc ]∗ , as\n‖B∗jatc‖b,∞ ≤ N · ‖B j atc‖b,∞ (387)\nExpression (385) then shows that the norm of the series appearing in (377) is bounded by a scaled multiple of the norm of Yatc, and the scaling constant is independent of A. Using property (586) we conclude that there exists a positive constant c, also independent of A, such that\nTr\n  ∞∑\nj=0\nBjatcYatcB ∗j atc   ≤ c · Tr(Yatc) (388)\nTherefore, instead of attempting to minimize the trace of the series, the above result motivates us to minimize an upper bound to the trace. Thus, we consider the alternative problem of minimizing the first term of the series (377), namely,\nmin A Tr(Yatc) subject to AT1 = 1, aℓk ≥ 0, aℓk = 0 if ℓ /∈ Nk (389)\nUsing (379), the trace of Yatc can be expressed in terms of the combination coefficients as follows:\nTr(Yatc) = N∑\nk=1\nN∑\nℓ=1\nµ2ℓ a 2 ℓk σ 2 v,ℓ Tr(Ru,ℓ) (390)\nso that problem (389) can be decoupled into N separate optimization problems of the form:\nmin {aℓk}Nℓ=1\nN∑\nℓ=1\nµ2ℓ a 2 ℓk σ 2 v,ℓ Tr(Ru,ℓ), k = 1, . . . , N\nsubject to\naℓk ≥ 0, N∑\nℓ=1\naℓk = 1, aℓk = 0 if ℓ /∈ Nk\n(391)\nWith each node ℓ, we associate the following nonnegative noise-data-dependent measure:\nγ2ℓ ∆ = µ2ℓ · σ 2 v,ℓ · Tr(Ru,ℓ) (392)\nThis measure amounts to scaling the noise variance at node ℓ by µ2ℓ and by the power of the regression data (measured through the trace of its covariance matrix). We shall refer to γ2ℓ as the noise-data variance product (or variance product, for simplicity) at node ℓ. Then, the solution of (391) is given by:\naℓk =    γ−2 ℓ∑ m∈Nk γ−2m , if ℓ ∈ Nk\n0, otherwise (relative-variance rule) (393)\nWe refer to this combination rule as the relative-variance combination rule [58]; it leads to a left-stochastic matrix A. In this construction, node k combines the intermediate estimates {ψℓ,i} from its neighbors in (376) in proportion to the inverses of their variance products, {γ−2m }. The result is physically meaningful. Nodes with smaller variance products will generally be given larger weights. In comparison, the following relativedegree-variance rule was proposed in [18] (a typo appears in Table III in [18], where the noise variances appear written in the table instead of their inverses):\naℓk =    nℓσ −2 v,ℓ∑ m∈Nk nmσ −2 v,m , if ℓ ∈ Nk\n0, otherwise (relative degree-variance rule) (394)\nThis second form also leads to a left-stochastic combination matrix A. However, rule (394) does not take into account the covariance matrices of the regression data across the network. Observe that in the special case when step-sizes, the regression covariance matrices, and the noise variances are uniform across the network, i.e., µk = µ, Ru,k = Ru, and σ 2 v,k = σ 2 v for all k, expression (393) reduces to the simple averaging rule (first line of Table 7). In contrast, expression (394) reduces the relative degree rule (last line of Table 7)."
    }, {
      "heading" : "8.3 Adaptive Combination Weights",
      "text" : "To evaluate the combination weights (393), the nodes need to know the variance products, {γ2m}, of their neighbors. According to (392), the factors {γ2m} are defined in terms of the noise variances, {σ 2 v,m}, and the regression covariance matrices, {Tr(Ru,m)}, and these quantities are not known beforehand. The nodes only have access to realizations of {dm(i),um,i}. We now describe one procedure that allows every node k to learn the variance products of its neighbors in an adaptive manner. Note that if a particular node ℓ happens to belong to two neighborhoods, say, the neighborhood of node k1 and the neighborhood of node k2, then\neach of k1 and k2 need to evaluate the variance product, γ 2 ℓ , of node ℓ. The procedure described below allows each node in the network to estimate the variance products of its neighbors in a recursive manner. To motivate the algorithm, we refer to the ATC recursion (375)–(376) and use the data model (208) to write for node ℓ: ψℓ,i = wℓ,i−1 + µℓu ∗ ℓ,i [uℓ,iw̃ℓ,i−1 + vℓ(i)] (395)\nso that, in view of our earlier assumptions on the regression data and noise in Sec. 6.1, we obtain in the limit as i → ∞:\nlim i→∞\nE ∥∥ψℓ,i −wℓ,i−1 ∥∥2 = µ2ℓ · ( lim i→∞ E‖w̃i−1‖ 2 E (u∗ℓ,i‖uℓ,i‖2uℓ,i) ) + µ2ℓ · σ 2 v,ℓ · Tr(Ru,ℓ) (396)\nWe can evaluate the limit on the right-hand side by using the steady-state result (284). Indeed, we select the vector σ in (284) to satisfy\n(I −F)σ = vec [ E ( u∗ℓ,i‖uℓ,i‖ 2uℓ,i )]\n(397)\nThen, from (284),\nlim i→∞ E‖w̃i−1‖ 2 E (u∗ℓ,i‖uℓ,i‖2uℓ,i)\n= [ vec ( YT )]T · (I −F)−1 · vec [ E ( u∗ℓ,i‖uℓ,i‖ 2uℓ,i )]\n(398)\nNow recall from expression (379) for Y that for the ATC algorithm under consideration we have\nY = ATMSMA (399)\nso that the entries of Y depend on combinations of the squared step-sizes, {µ2m, m = 1, 2, . . . , N}. This fact implies that the first term on the right-hand side of (396) depends on products of the form {µ2ℓµ 2 m}; these fourth-order factors can be ignored in comparison to the second-order factor µ2ℓ for small step-sizes so that\nlim i→∞\nE ∥∥ψℓ,i −wℓ,i−1 ∥∥2 ≈ µ2ℓ · σ2v,ℓ · Tr(Ru,ℓ)\n= γ2ℓ (400)\nin terms of the desired variance product, γ2ℓ . Using the following instantaneous approximation at node k (where wℓ,i−1 is replaced by wk,i−1):\nE‖ψℓ,i −wℓ,i−1‖ 2 ≈ ‖ψℓ,i − wk,i−1‖ 2 (401)\nwe can motivate an algorithm that enables node k to estimate the variance product, γ2ℓ , of its neighbor ℓ. Thus, let γ2ℓk(i) denote an estimate for γ 2 ℓ that is computed by node k at time i. Then, one way to evaluate γ2ℓk(i) is through the recursion:\nγ2ℓk(i) = (1− νk) · γ 2 ℓk(i− 1) + νk · ‖ψℓ,i − wk,i−1‖ 2 (402)\nwhere 0 < νk ≪ 1 is a positive coefficient smaller than one. Note that under expectation, expression (402) becomes\nEγ2ℓk(i) = (1− νk) · Eγ 2 ℓk(i− 1) + νk · E‖ψℓ,i −wk,i−1‖ 2 (403)\nso that in steady-state, as i → ∞,\nlim i→∞ Eγ2ℓk(i) ≈ (1− νk) · lim i→∞ Eγ2ℓk(i − 1) + νk · γ 2 ℓ (404)\nHence, we obtain lim i→∞ Eγ2ℓk(i) ≈ γ 2 ℓ (405)\nThat is, the estimator γ2ℓk(i) converges on average close to the desired variance product γ 2 ℓ . In this way, we can replace the optimal weights (393) by the adaptive construction:\naℓk(i) =   \nγ−2 ℓk\n(i) ∑\nm∈Nk γ−2 mk\n(i) , if ℓ ∈ Nk\n0, otherwise (406)\nEquations (402) and (406) provide one adaptive construction for the combination weights {aℓk}."
    }, {
      "heading" : "9 Diffusion with Noisy Information Exchanges",
      "text" : "The adaptive diffusion strategy (201)–(203) relies on the fusion of local information collected from neighborhoods through the use of combination matrices {A1, A2, C}. In the previous section, we described several constructions for selecting such combination matrices. We also motivated and developed an adaptive scheme for the ATC mode of operation (375)–(376) that computes combination weights in a manner that is aware of the variation of the variance-product profile across the network. Nevertheless, in addition to the measurement noises {vk(i)} at the individual nodes, we also need to consider the effect of perturbations that are introduced during the exchange of information among neighboring nodes. Noise over the communication links can be due to various factors including thermal noise and imperfect channel information. Studying the degradation in mean-square performance that results from these noisy exchanges can be pursued by straightforward extension of the mean-square analysis of Sec. 6, as we proceed to illustrate. Subsequently, we shall use the results to show how the combination weights can also be adapted in the presence of noisy exchange links."
    }, {
      "heading" : "9.1 Noise Sources over Exchange Links",
      "text" : "To model noisy links, we introduce an additive noise component into each of the steps of the diffusion strategy (201)–(203) during the operations of information exchange among the nodes. The notation becomes a bit cumbersome because we need to account for both the source and destination of the information that is being exchanged. For example, the same signal dℓ(i) that is generated by node ℓ will be broadcast to all the neighbors of node ℓ. When this is done, a different noise will interfere with the exchange of dℓ(i) over each of the edges that link node ℓ to its neighbors. Thus, we will need to use a notation of the form dℓk(i), with two subscripts ℓ and k, to indicate that this is the noisy version of dℓ(i) that is received by node k from node ℓ. The subscript ℓk indicates that ℓ is the source and k is the sink, i.e., information is moving from ℓ to k. For the reverse situation where information flows from node k to ℓ, we would use instead the subscript kℓ.\nWith this notation in mind, we model the noisy data received by node k from its neighbor ℓ as follows:\nwℓk,i−1 = wℓ,i−1 + v (w) ℓk,i−1 (407)\nψℓk,i = ψℓ,i + v (ψ) ℓk,i (408) uℓk,i = uℓ,i + v (u) ℓk,i (409) dℓk(i) = dℓ(i) + v (d) ℓk (i) (410)\nwhere v (w) ℓk,i−1 (M × 1), v (ψ) ℓk,i (M × 1), and v (u) ℓk,i (1 × M) are vector noise signals, and v (d) ℓk (i) is a scalar noise signal. These are the noise signals that perturb exchanges over the edge linking source ℓ to sink k (i.e., for data sent from node ℓ to node k). The superscripts {(w), (ψ), (u), (d)} in each case refer to the variable that these noises perturb. Figure 14 illustrates the various noise sources that perturb the exchange of information from node ℓ to node k. The figure also shows the measurement noises {vℓ(i),vk(i)} that exist locally at the nodes in view of the data model (208).\nWe assume that the following noise signals, which influence the data received by node k,\n{ vk(i), v (d) ℓk (i), v (w) ℓk,i−1, v (ψ) ℓk,i, v (u) ℓk,i } (411)\nare temporally white and spatially independent random processes with zero mean and variances or covariances given by\n{ σ2v,k, σ 2 v,ℓk, R (w) v,ℓk, R (ψ) v,ℓk, R (u) v,ℓk } (412)\nObviously, the quantities { σ2v,ℓk, R (w) v,ℓk, R (ψ) v,ℓk, R (u) v,ℓk } (413)\nare all zero if ℓ /∈ Nk or when ℓ = k. We further assume that the noise processes (411) are independent of each other and of the regression data um,j for all k, ℓ,m and i, j."
    }, {
      "heading" : "9.2 Error Recursion",
      "text" : "Using the perturbed data (407)–(410), the adaptive diffusion strategy (201)–(203) becomes\nφk,i−1 = ∑\nℓ∈Nk\na1,ℓk wℓk,i−1 (414)\nψk,i = φk,i−1 + µk ∑\nℓ∈Nk\ncℓk u ∗ ℓk,i [dℓk(i)− uℓk,iφk,i−1] (415)\nwk,i = ∑\nℓ∈Nk\na2,ℓk ψℓk,i (416)\nObserve that the perturbed quantities {wℓk,i−1,uℓk,i,dℓk(i),ψℓk,i}, with subscripts ℓk, appear in (414)– (416) in place of the original quantities {wℓ,i−1,uℓ,i,dℓ(i),ψℓ,i} that appear in (201)–(203). This is because these quantities are now subject to exchange noises. As before, we are still interested in examining the evolution of the weight-error vectors:\nw̃k,i ∆ = wo −wk,i, k = 1, 2, . . . , N (417)\nFor this purpose, we again introduce the following N × 1 block vector, whose entries are of size M × 1 each:\nw̃i ∆ =   w̃1,i w̃2,i ...\nw̃N,i\n  (418)\nand proceed to determine a recursion for its evolution over time. The arguments are largely similar to what we already did before in Sec. 6.3 and, therefore, we shall emphasize the differences that arise. The main deviation is that we now need to account for the presence of the new noise signals; they will contribute additional terms to the recursion for w̃i — see (442) further ahead. We may note that some studies on the effect of imperfect data exchanges on the performance of adaptive diffusion algorithms are considered in [59–61]. However, these earlier investigations were limited to particular cases in which only noise in the exchange of wℓ,i−1 was considered (as in (407)), in addition to setting C = I (in which case there is no exchange of {dℓ(i),uℓ,i}), and by focusing on the CTA case for which A2 = I. Here, we consider instead the general case that accounts for the additional sources of imperfections shown in (408)–(410), in addition to the general diffusion strategy (201)–(203) with combination matrices {A1, A2, C}.\nTo begin with, we introduce the aggregate M × 1 zero-mean noise signals:\nv (w) k,i−1 ∆ =\n∑\nℓ∈Nk\na1,ℓk v (w) ℓk,i−1, v (ψ) k,i ∆ =\n∑\nℓ∈Nk\na2,ℓk v (ψ) ℓk,i (419)\nThese noises represent the aggregate effect on node k of all exchange noises from the neighbors of node k while exchanging the estimates {wℓ,i−1,ψℓ,i} during the two combination steps (201) and (203). The M×M covariance matrices of these noises are given by\nR (w) v,k ∆ =\n∑\nℓ∈Nk\na21,ℓk R (w) v,ℓk, R (ψ) v,k ∆ =\n∑\nℓ∈Nk\na22,ℓk R (ψ) v,ℓk (420)\nThese expressions aggregate the exchange noise covariances in the neighborhood of node k; the covariances are scaled by the squared coefficients {a21,ℓk, a 2 2,ℓk}. We collect these noise signals, and their covariances, from across the network into N × 1 block vectors and N ×N block diagonal matrices as follows:\nv (w) i−1 ∆ = col { v (w) 1,i−1, v (w) 2,i−1, . . . , v (w) N,i−1 } (421)\nv (ψ) i ∆ = col { v (ψ) 1,i , v (ψ) 2,i , . . . , v (ψ) N,i } (422)\nR(w)v ∆ = diag\n{ R\n(w) v,1 , R (w) v,2 , . . . , R (w) v,N\n} (423)\nR(ψ)v ∆ = diag\n{ R\n(ψ) v,1 , R (ψ) v,2 , . . . , R (ψ) v,N\n} (424)\nWe further introduce the following scalar zero-mean noise signal:\nvℓk(i) ∆ = vℓ(i) + v (d) ℓk (i) − v (u) ℓk,i w o (425)\nwhose variance is\nσ2ℓk = σ 2 v,ℓ + σ 2 v,ℓk + w o∗R (u) v,ℓkw o (426)\nIn the absence of exchange noises for the data {dℓ(i),uℓ,i}, the signal vℓk(i) would coincide with the measurement noise vℓ(i). Expression (425) is simply a reflection of the aggregate effect of the noises in exchanging {dℓ(i),uℓ,i} on node k. Indeed, starting from the data model (208) and using (409)–(410), we can easily verify that the noisy data {dℓk(i),uℓk,i} are related via:\ndℓk(i) = uℓk,iw o + vℓk(i) (427)\nWe also define (compare with (234)–(235) and note that we are now using the perturbed regression vectors {uℓk,i}):\nR′k,i ∆ =\n∑\nℓ∈Nk\ncℓku ∗ ℓk,iuℓk,i (428)\nR ′ i ∆ = diag { R′1,i, R ′ 2,i, . . . , R ′ N,i } (429)\nIt holds that\nER′k,i = R ′ k (430)\nwhere\nR′k ∆ =\n∑\nℓ∈Nk\ncℓk [ Ru,ℓ + R (u) v,ℓk ] (431)\nWhen there is no noise during the exchange of the regression data, i.e., when R (u) v,ℓk = 0, the expressions for {R′k,i, R ′ i, R ′ k} reduce to expressions (234)–(235) and (182) for {Rk,i, Ri, Rk}.\nLikewise, we introduce (compare with (239)):\nzk,i ∆ =\n∑\nℓ∈Nk\ncℓku ∗ ℓk,ivℓk(i) (432)\nzi ∆ = col {z1,i, z2,i, . . . , zN,i} (433)\nCompared with the earlier definition for si in (239) when there is no noise over the exchange links, we see that we now need to account for the various noisy versions of the same regression vector uℓ,i and the same signal dℓ(i). For instance, the vectors uℓk,i and uℓm,i would denote two noisy versions received by nodes k and m for the same regression vector uℓ,i transmitted from node ℓ. Likewise, the scalars dℓk(i) and dℓm(i) would denote two noisy versions received by nodes k and m for the same scalar dℓ(i) transmitted from node ℓ. As a result, the quantity zi is not zero mean any longer (in contrast to si, which had zero mean). Indeed, note that\nEzk,i = ∑\nℓ∈Nk\ncℓk Eu ∗ ℓk,ivℓk(i)\n= ∑\nℓ∈Nk\ncℓk E ([ uℓ,i + v (u) ℓk,i ]∗ · [ vℓ(i) + v (d) ℓk (i) − v (u) ℓk,i w o ])\n= −\n( ∑\nℓ∈Nk\ncℓk R (u) v,ℓk\n) wo (434)\nIt follows that\nEzi = −   ∑ ℓ∈N1 cℓ1 R (u) v,ℓ1 ∑ ℓ∈N2 cℓ2 R (u) v,ℓ2 ... ∑\nℓ∈NN\ncℓN R (u) v,ℓN\n  wo (435)\nAlthough we can continue our analysis by studying this general case in which the vectors zi do not have zero-mean (see [62,63]), we shall nevertheless limit our discussion in the sequel to the case in which there is no noise during the exchange of the regression data, i.e., we henceforth assume that:\nv (u) ℓk,i = 0, R (u) v,ℓk = 0, uℓk,i = uℓ,i (assumption from this point onwards) (436)\nWe maintain all other noise sources, which occur during the exchange of the weight estimates {wℓ,i−1,ψℓ,i} and the data {dℓ(i)}. Under condition (436), we obtain\nEzi = 0 (437)\nσ2ℓk = σ 2 v,ℓ + σ 2 v,ℓk (438) R′k = ∑\nℓ∈Nk\ncℓk Ru,ℓ (182) = Rk (439)\nThen, the covariance matrix of each term zk,i is given by\nRz,k ∆ =\n∑\nℓ∈Nk\nc2ℓk σ 2 ℓk Ru,ℓ (440)\nand the covariance matrix of zi is N ×N block diagonal with blocks of size M ×M :\nZ ∆ = Eziz ∗ i = diag{Rz,1, Rz,2, . . . , Rz,N} (441)\nNow repeating the argument that led to (246) we arrive at the following recursion for the weight-error vector:\nw̃i = A T 2 ( INM −MR ′ i ) AT1 w̃i−1 −A T 2 Mzi −A T 2 ( INM −MR ′ i ) v (w) i−1 − v (ψ) i (noisy links) (442)\nFor comparison purposes, we repeat recursion (246) here (recall that this recursion corresponds to the case when the exchanges over the links are not subject to noise):\nw̃i = A T 2 (INM −MRi)A T 1 w̃i−1 − A T 2 MC Tsi (perfect links) (443)\nComparing (442) and (443) we find that:\n(1) The covariance matrix Ri in (443) is replaced by R ′ i. Recall from (429) that R ′ i contains the influence\nof the noises that arise during the exchange of the regression data, i.e., the {R (u) v,ℓk}. But since we are now assuming that R (u) v,ℓk = 0, then R ′ i = Ri.\n(2) The term CTsi in (443) is replaced by zi. Recall from (432) that zi contains the influence of the noises\nthat arise during the exchange of the measurement data and the regression data, i.e., the {σ2v,ℓk, R (u) v,ℓk}.\n(3) Two new driving terms appear involving v (w) i−1 and v (ψ) i . These terms reflect the influence of the noises\nduring the exchange of the weight estimates {wℓ,i−1,ψℓ,i}.\n(4) Observe further that:\n(4a) The term involving v (w) i−1 accounts for noise introduced at the information-exchange step (414)\nbefore adaptation.\n(4b) The term involving zi accounts for noise introduced during the adaptation step (415).\n(4c) The term involving v (ψ) i accounts for noise introduced at the information-exchange step (416)\nafter adaptation.\nTherefore, since we are not considering noise during the exchange of the regression data, the weight-error recursion (442) simplifies to:\nw̃i = A T 2 (INM −MRi)A T 1 w̃i−1 −A T 2 Mzi −A T 2 (INM −MRi)v (w) i−1 − v (ψ) i (noisy links) (444)\nwhere we used the fact that R′i = Ri under these conditions."
    }, {
      "heading" : "9.3 Convergence in the Mean",
      "text" : "Taking expectations of both sides of (444) we find that the mean error vector evolves according to the following recursion:\nE w̃i = A T 2 (INM −MR)A T 1 · E w̃i−1, i ≥ 0 (445)\nwith R defined by (181). This is the same recursion encountered earlier in (248) during perfect data exchanges. Note that had we considered noises during the exchange of the regression data, then the vector zi in (444) would not be zero mean and the matrix R ′ i will have to be used instead of Ri. In that case, the recursion for E w̃i will be different from (445); i.e., the presence of noise during the exchange of regression data alters the dynamics of the mean error vector in an important way — see [62, 63] for details on how to extend the arguments to this general case with a driving non-zero bias term. We can now extend Theorem 6.1 to the current scenario.\nTheorem 9.1. (Convergence in the Mean) Consider the problem of optimizing the global cost (92) with the individual cost functions given by (93). Pick a right stochastic matrix C and left stochastic matrices A1 and A2 satisfying (166). Assume each node in the network runs the perturbed adaptive diffusion algorithm (414)–(416). Assume further that the exchange of the variables {wℓ,i−1,ψℓ,i,dℓ(i)} is subject to additive noises as in (407), (408), and (410). We assume that the regressors are exchanged unperturbed. Then, all estimators {wk,i} across the network will still converge in the mean to the optimal solution wo if the step-size parameters {µk} satisfy\nµk < 2\nλmax(Rk) (446)\nwhere the neighborhood covariance matrix Rk is defined by (182). That is, Ewk,i → wo as i → ∞."
    }, {
      "heading" : "9.4 Mean-Square Convergence",
      "text" : "Recall from (264) that we introduced the matrix:\nB ∆ = AT2 (INM −MR)A T 1 (447)\nWe further introduce the N ×N block matrix with blocks of size M ×M each:\nH ∆ = AT2 (INM −MR) (448)\nThen, starting from (444) and repeating the argument that led to (279) we can establish the validity of the following variance relation:\nE‖w̃i‖ 2 σ = E‖w̃i−1‖ 2 Fσ +\n[ vec ( AT2 MZ TMA2 ) + vec (( HR(w)Tv H ∗ )T) + vec ( R(ψ)Tv )]T σ (449)\nfor an arbitrary nonnegative-definite weighting matrix Σ with σ = vec(Σ), and where F is the same matrix defined earlier either by (276) or (277). We can therefore extend the statement of Theorem 6.7 to the present scenario.\nTheorem 9.2. (Mean-Square Stability) Consider the same setting of Theorem 9.1. Assume sufficiently small step-sizes to justify ignoring terms that depend on higher powers of the step-sizes. The perturbed adaptive diffusion algorithm (414)–(416) is mean-square stable if, and only if, the matrix F defined by (276), or its approximation (277), is stable (i.e., all its eigenvalues are strictly inside the unit disc). This condition is satisfied by sufficiently small step-sizes {µk} that are also smaller than:\nµk < 2\nλmax(Rk) (450)\nwhere the neighborhood covariance matrix Rk is defined by (182). Moreover, the convergence rate of the algorithm is determined by [ρ(B)]2.\nWe conclude from the previous two theorems that the conditions for the mean and mean-square convergence of the adaptive diffusion strategy are not affected by the presence of noises over the exchange links (under the assumption that the regression data are exchanged without perturbation; otherwise, the convergence conditions would be affected). The mean-square performance, on the other hand, is affected as follows. Introduce the N ×N block matrix:\nYimperfect ∆ = AT2 MZMA2 + HR (w) v H ∗ + R(ψ)v (imperfect exchanges) (451)\nwhich should be compared with the corresponding quantity defined by (280) for the perfect exchanges case, namely,\nYperfect = A T 2 MC TSCMA2 (perfect exchanges) (452)\nWhen perfect exchanges occur, the matrix Z reduces to CTSC. We can relate Yimperfect and Yperfect as follows. Let\nR(du) ∆ = diag\n{ ∑\nℓ∈N1\nc2ℓ1σ 2 v,ℓ1Ru,ℓ,\n∑\nℓ∈N2\nc2ℓ2σ 2 v,ℓ2Ru,ℓ, . . .\n∑\nℓ∈NN\nc2ℓNσ 2 v,ℓNRu,ℓ\n} (453)\nThen, using (438) and (441), it is straightforward to verify that\nZ = CTSC + R(du) (454)\nand it follows that:\nYimperfect = Yperfect + A T 2 MR (du)MA2 + HR (w) v H ∗ + R(ψ)v ∆ = Yperfect + ∆Y (455)\nExpression (455) reflects the influence of the noises {R (w) v , R (ψ) v , σ2v,ℓk}. Substituting the definition (451) into (449), and taking the limit as i → ∞, we obtain from the latter expression that:\nlim i→∞\nE‖w̃i‖ 2 (I−F)σ =\n[ vec ( YTimperfect )]T σ (456)\nwhich has the same form as (284); therefore, we can proceed analogously to obtain:\nMSDnetworkimperfect = 1 N · [ vec ( YTimperfect )]T · (I −F)−1 · vec (INM ) (457)\nand\nEMSEnetworkimperfect = 1 N · [ vec ( YTimperfect )]T · (I −F)−1 · vec (Ru) (458)\nUsing (455), we see that the network MSD and EMSE deteriorate as follows:\nMSDnetworkimperfect = MSD network perfect +\n1 N · [ vec ( ∆YT )]T · (I −F)−1 · vec (INM ) (459)\nEMSEnetworkimperfect = EMSE network perfect +\n1 N · [ vec ( ∆YT )]T · (I −F)−1 · vec (Ru) (460)"
    }, {
      "heading" : "9.5 Adaptive Combination Weights",
      "text" : "We can repeat the discussion from Secs. 8.2 and 8.3 to devise one adaptive scheme to adjust the combination coefficients in the noisy exchange case. We illustrate the construction by considering the ATC strategy corresponding to A1 = IN , A2 = A,C = IN , so that only weight estimates are exchanged and the update recursions are of the form:\nψk,i = wk,i−1 + µku ∗ k,i[dk(i)− uk,iwk,i−1] (461) wk,i = ∑\nℓ∈Nk\naℓk ψℓk,i (462)\nwhere from (408):\nψℓk,i = ψℓ,i + v (ψ) ℓk,i (463)\nIn this case, the network MSD performance (457) becomes\nMSDnetworkatc,C=I,imperfect = 1\nN\n∞∑\nj=0\nTr ( Bjatc,C=IYatc,imperfectB ∗j atc,C=I ) (464)\nwhere, since now Z = S and R (w) v = 0, we have\nBatc,C=I = A T (I −MRu) (465) Yatc,imperfect = A TMSMA + R(ψ)v (466)\nR(ψ)v = diag { R (ψ) v,1 , R (ψ) v,2 , . . . , R (ψ) v,N } (467)\nR (ψ) v,k =\n∑\nℓ∈Nk\na2ℓk R (ψ) v,ℓk (468)\nRu = diag{Ru,1, Ru,2, . . . , Ru,N} (469)\nS = diag{σ2v,1Ru,1, σ 2 v,2Ru,2, . . . , σ 2 v,NRu,N} (470) M = diag{µ1IM , µ2IM , . . . , µNIM} (471)\nA = A⊗ IM (472)\nTo proceed, as was the case with (389), we consider the following simplified optimization problem:\nmin A Tr(Yatc,imperfect) subject to AT1 = 1, aℓk ≥ 0, aℓk = 0 if ℓ /∈ Nk (473)\nUsing (466), the trace of Yatc,imperfect can be expressed in terms of the combination coefficients as follows:\nTr(Yatc,imperfect) = N∑\nk=1\nN∑\nℓ=1\na2ℓk [ µ2ℓσ 2 v,ℓ Tr(Ru,ℓ) + Tr ( R (ψ) v,ℓk )] (474)\nso that problem (473) can be decoupled into N separate optimization problems of the form:\nmin {aℓk}Nℓ=1\nN∑\nℓ=1\na2ℓk [ µ2ℓσ 2 v,ℓ Tr(Ru,ℓ) + Tr ( R (ψ) v,ℓk )] , k = 1, . . . , N\nsubject to\naℓk ≥ 0, N∑\nℓ=1\naℓk = 1, aℓk = 0 if ℓ /∈ Nk\n(475)\nWith each node ℓ, we associate the following nonnegative variance products:\nγ2ℓk ∆ = µ2ℓ · σ 2 v,ℓ · Tr(Ru,ℓ) + Tr\n( R\n(ψ) v,ℓk ) , k ∈ Nℓ (476)\nThis measure now incorporates information about the exchange noise covariances {R (ψ) v,ℓk}. Then, the solution of (475) is given by:\naℓk =    γ−2 ℓk∑ m∈Nk γ−2 mk , if ℓ ∈ Nk\n0, otherwise (relative-variance rule) (477)\nWe continue to refer to this combination rule as the relative-variance combination rule [58]; it leads to a left-stochastic matrix A. To evaluate the combination weights (477), the nodes need to know the variance products, {γ2mk}, of their neighbors. As before, we can motivate one adaptive construction as follows.\nWe refer to the ATC recursion (461)–(463) and use the data model (208) to write for node ℓ:\nψℓk,i = wℓ,i−1 + µℓu ∗ ℓ,i [uℓ,iw̃ℓ,i−1 + vℓ(i)] + v (ψ) ℓk,i (478)\nso that, in view of our earlier assumptions on the regression data and noise in Secs. 6.1 and 9.1, we obtain in the limit as i → ∞:\nlim i→∞\nE ∥∥ψℓk,i −wℓ,i−1 ∥∥2 = µ2ℓ · ( lim i→∞ E‖w̃i−1‖ 2 E (u∗ℓ,i‖uℓ,i‖2uℓ,i) ) + µ2ℓ ·σ 2 v,ℓ ·Tr(Ru,ℓ) + Tr ( R (ψ) v,ℓk ) (479)\nIn a manner similar to what was done before for (396), we can evaluate the limit on the right-hand side by using the corresponding steady-state result (456). We select the vector σ in (456) to satisfy:\n(I −F)σ = vec [ E ( u∗ℓ,i‖uℓ,i‖ 2uℓ,i )]\n(480)\nThen, from (456),\nlim i→∞ E‖w̃i−1‖ 2 E (u∗ℓ,i‖uℓ,i‖2uℓ,i)\n= [ vec ( YTatc,imperfect )]T · (I −F)−1 · vec [ E ( u∗ℓ,i‖uℓ,i‖ 2uℓ,i )]\n(481)\nNow recall from expression (466) that the entries of Yatc,imperfect depend on combinations of the squared\nstep-sizes, {µ2m, m = 1, 2, . . . , N}, and on terms involving { Tr ( R (ψ) v,m )} . This fact implies that the first term on the right-hand side of (479) depends on products of the form {µ2ℓµ 2 m}; these fourth-order factors can be ignored in comparison to the second-order factor µ2ℓ for small step-sizes. Moreover, the same first term\non the right-hand side of (479) depends on products of the form { µ2ℓTr ( R (ψ) v,m )} , which can be ignored in comparison to the last term, Tr ( R\n(ψ) v,ℓk\n) , in (479), which does not appear multiplied by a squared step-size.\nTherefore, we can approximate:\nlim i→∞\nE ∥∥ψℓk,i −wℓ,i−1 ∥∥2 ≈ µ2ℓ · σ2v,ℓ · Tr(Ru,ℓ) + Tr ( R (ψ) v,ℓk )\n= γ2ℓk (482)\nin terms of the desired variance product, γ2ℓk. Using the following instantaneous approximation at node k (where wℓ,i−1 is replaced by wk,i−1):\nE‖ψℓk,i −wℓ,i−1‖ 2 ≈ ‖ψℓk,i − wk,i−1‖ 2 (483)\nwe can motivate an algorithm that enables node k to estimate the variance products γ2ℓk. Thus, let γ̂ 2 ℓk(i) denote an estimate for γ2ℓk that is computed by node k at time i. Then, one way to evaluate γ̂ 2 ℓk(i) is through the recursion:\nγ̂2ℓk(i) = (1 − νk) · γ̂ 2 ℓk(i − 1) + νk · ‖ψℓk,i − wk,i−1‖ 2 (484)\nwhere νk is a positive coefficient smaller than one. Indeed, it can be verified that\nlim i→∞\nE γ̂ 2 ℓk(i) ≈ γ 2 ℓk (485)\nso that the estimator γ̂2ℓk(i) converges on average close to the desired variance product γ 2 ℓk. In this way, we can replace the weights (477) by the adaptive construction:\naℓk(i) =   \nγ̂−2 ℓk\n(i) ∑\nm∈Nk γ̂−2 mk\n(i) , if ℓ ∈ Nk\n0, otherwise (486)\nEquations (484) and (486) provide one adaptive construction for the combination weights {aℓk}."
    }, {
      "heading" : "10 Extensions and Further Considerations",
      "text" : "Several extensions and variations of diffusion strategies are possible. Among those variations we mention strategies that endow nodes with temporal processing abilities, in addition to their spatial cooperation abilities. We can also apply diffusion strategies to solve recursive least-squares and state-space estimation problems in a distributed manner. In this section, we highlight select contributions in these and related areas."
    }, {
      "heading" : "10.1 Adaptive Diffusion Strategies with Smoothing Mechanisms",
      "text" : "In the ATC and CTA adaptive diffusion strategies (153)–(154), each node in the network shares information locally with its neighbors through a process of spatial cooperation or combination. In this section, we describe briefly an extension that adds a temporal dimension to the processing at the nodes. For example, in the ATC implementation (153), rather than have each node k rely solely on current data, {dℓ(i), uℓ,i, ℓ ∈ Nk}, and on current weight estimates received from the neighbors, {ψℓ,i, ℓ ∈ Nk}, node k can be allowed to store and process present and past weight estimates, say, P of them as in {ψℓ,j, j = i, i− 1, . . . , i− P + 1}. In this way, previous weight estimates can be smoothed and used more effectively to help enhance the mean-square-deviation performance especially in the presence of noise over the communication links.\nTo motivate diffusion strategies with smoothing mechanisms, we continue to assume that the random data {dk(i),uk,i} satisfy the modeling assumptions of Sec. 6.1. The global cost (92) continues to be the same but the individual cost functions (93) are now replaced by:\nJk(w) = P−1∑\nj=0\nqkj E |dk(i− j)− uk,i−j w| 2 (487)\nso that\nJglob(w) = N∑\nk=1\n  P−1∑\nj=0\nqkj E |dk(i− j)− uk,i−j w| 2   (488)\nwhere each coefficient qkj is a non-negative scalar representing the weight that node k assigns to data from time instant i− j. The coefficients {qkj} are assumed to satisfy the normalization condition:\nqko > 0, P−1∑\nj=0\nqkj = 1, k = 1, 2, . . . , N (489)\nWhen the random processes dk(i) and uk,i are jointly wide-sense stationary, as was assumed in Sec. 6.1, the optimal solution wo that minimizes (488) is still given by the same normal equations (40). We can extend the arguments of Secs. 3 and 4 to (488) and arrive at the following version of a diffusion strategy incorporating temporal processing (or smoothing) of the intermediate weight estimates [73, 74]:\nφk,i = wk,i−1 + µk ∑\nℓ∈Nk\ncℓk qℓo u ∗ ℓ,i [dℓ(i)−uℓ,iwk,i−1] (adaptation) (490)\nψk,i =\nP−1∑\nj=0\nfkj φk,i−j (temporal processing or smoothing) (491)\nwk,i = ∑\nℓ∈Nk\naℓk ψℓ,i (spatial processing) (492)\nwhere the nonnegative coefficients {cℓk, aℓk, fkj , qlo} satisfy:\nfor k = 1, 2, . . . , N :\ncℓk ≥ 0, N∑\nk=1\ncℓk = 1, cℓk = 0 if ℓ /∈ Nk (493)\naℓk ≥ 0, N∑\nℓ=1\naℓk = 1, aℓk = 0 if ℓ /∈ Nk (494)\nfkj ≥ 0, P−1∑\nj=0\nfkj = 1 (495)\n0 < qℓo ≤ 1 (496)\nSince only the coefficients {qℓo} are needed, we alternatively denote them by the simpler notation {qℓ} in the listing in Table 8. These are simply chosen as nonnegative coefficients:\n0 < qℓ ≤ 1, ℓ = 1, 2, . . . , N (497)\nNote that algorithm (490)-(492) involves three steps: (a) an adaptation step (A) represented by (490); (b) a temporal filtering or smoothing step (T) represented by (491), and a spatial cooperation step (S) represented by (492). These steps are illustrated in Fig. 15. We use the letters (A,T,S) to label these steps; and we use the sequence of letters (A,T,S) to designate the order of the steps. According to this convention, algorithm (490)- (492) is referred to as the ATS diffusion strategy since adaptation is followed by temporal processing, which is followed by spatial processing. In total, we can obtain six different combinations of diffusion algorithms by changing the order by which the temporal and spatial combination steps are performed in relation to the adaptation step. The resulting variations are summarized in Table 8. When we use only the most recent weight vector in the temporal filtering step (i.e., set ψk,i = φk,i), which corresponds to the case P = 1, the algorithms of Table 8 reduce to the ATC and CTA diffusion algorithms (153) and (154). Specifically, the variants TSA, STA, and SAT (where spatial processing S precedes adaptation A) reduce to CTA, while the variants TAS, ATS, and AST (where adaptation A precedes spatial processing S) reduce to ATC.\nThe mean-square performance analysis of the smoothed diffusion strategies can be pursued by extending the arguments of Sec. 6. This step is carried out in [73, 74] for doubly stochastic combination matrices A\nwhen the filtering coefficients {fkj} do not change with k. For instance, it is shown in [74] that whether temporal processing is performed before or after adaptation, the strategy that performs adaptation before spatial cooperation is always better. Specifically, the six diffusion variants can be divided into two groups with the respective network MSDs satisfying the following relations:\nGroup #1 : MSDnetworkTSA = MSD network STA ≥ MSD network TAS (498)\nGroup #2 : MSDnetworkSAT ≥ MSD network ATS = MSD network AST (499)\nNote that within groups 1 and 2, the order of the A and T operations is the same: in group 1, T precedes A and in group 2, A precedes T. Moreover, within each group, the order of the A and S operations determines performance; the strategy that performs A before S is better. Note further that when P = 1, so that temporal processing is not performed, then TAS reduces to ATC and TSA reduces to CTA. This conclusion is consistent with our earlier result (343) that ATC outperforms CTA.\nIn related work, reference [75] started from the CTA algorithm (159) without information exchange and added a useful projection step to it between the combination step and the adaptation step; i.e., the work considered an algorithm with an STA structure (with spatial combination occurring first, followed by a projection step, and then adaptation). The projection step uses the current weight estimate, φk,i, at node k and projects it onto hyperslabs defined by the current and past raw data. Specifically, the algorithm\nfrom [75] has the following form:\nφk,i−1 = ∑\nℓ∈Nk\naℓk wℓ,i−1 (500)\nψk,i−1 = P ′ k,i[φk,i−1] (501)\nwk,i = ψk,i−1 − µk   ψk,i−1 − P−1∑\nj=0\nfkj · Pk,i−j [φk,i−1]    (502)\nwhere the notation ψ = Pk,i[φ] refers to the act of projecting the vector φ onto the hyperslab Pk,i that consists of all M × 1 vectors z satisfying (similarly for the projection P ′\nk,i):\nPk,i ∆ = { z such that |dk(i)− uk,iz| ≤ ǫk } (503) P ′\nk,i ∆ = { z such that |dk(i)− uk,iz| ≤ ǫ ′ k } (504)\nwhere {ǫk, ǫ′k} are positive (tolerance) parameters chosen by the designer to satisfy ǫ ′ k > ǫk. For generic values {d, u, ǫ}, where d is a scalar and u is a row vector, the projection operator is described analytically by the following expression [76]:\nP [φ] = φ +    u∗ ‖u‖2 [d− ǫ− uφ] , if d− ǫ > uφ 0, if |d− uφ| ≤ ǫ\nu∗\n‖u‖2 [d+ ǫ− uφ] , if d+ ǫ < uφ\n(505)\nThe projections that appear in (501)–(502) can be regarded as another example of a temporal processing step. Observe from the middle plot in Fig. 15 that the temporal step that we are considering in the algorithms listed in Table 8 is based on each node k using its current and past weight estimates, such as {φk,i, φk,i−1, . . . , φk,i−P+1}, rather than only φk,i and current and past raw data {dk(i), dk(i− 1), . . . , dk(i− P + 1), uk,i, uk,i−1, . . . , uk,i−P+1}. For this reason, the temporal processing steps in Table 8 tend to exploit information from across the network more broadly and the resulting mean-square error performance is generally improved relative to (500)–(502)."
    }, {
      "heading" : "10.2 Diffusion Recursive Least-Squares",
      "text" : "Diffusion strategies can also be applied to recursive least-squares problems to enable distributed solutions of least-squares designs [28,29]; see also [72]. Thus, consider again a set of N nodes that are spatially distributed over some domain. The objective of the network is to collectively estimate some unknown column vector of length M , denoted by wo, using a least-squares criterion. At every time instant i, each node k collects a scalar measurement, dk(i), which is assumed to be related to the unknown vector w o via the linear model:\ndk(i) = uk,iw o + vk(i) (506)\nIn the above relation, the vector uk,i denotes a row regression vector of length M , and vk(i) denotes measurement noise. A snapshot of the data in the network at time i can be captured by collecting the measurements and noise samples, {dk(i), vk(i)}, from across all nodes into column vectors yi and vi of sizes N × 1 each, and the regressors {uk,i} into a matrix Hi of size N ×M :\nyi =   d1(i) d2(i) ...\ndN (i)\n  (N × 1), vi =   v1(i) v2(i) ...\nvN (i)\n  (N × 1), Hi =   u1,i u2,i ...\nuN,i\n  (N ×M) (507)\nLikewise, the history of the data across the network up to time i can be collected into vector quantities as follows:\nYi =  \nyi yi−1 ... y0\n  , Vi =  \nvi vi−1 ... v0\n  , Hi =  \nHi Hi−1 ... H0\n  (508)\nThen, one way to estimate wo is by formulating a global least-squares optimization problem of the form:\nmin w\n‖w‖2Πi + ‖Yi − Hiw‖ 2 Wi (509)\nwhere Πi > 0 represents a Hermitian regularization matrix and Wi ≥ 0 represents a Hermitian weighting matrix. Common choices for Πi and Wi are\nWi = diag{IN , λIN , . . . , λ iIN} (510)\nΠi = λ i+1δ−1 (511)\nwhere δ > 0 is usually a large number and 0 ≪ λ ≤ 1 is a forgetting factor whose value is generally very close to one. In this case, the global cost function (509) can be written in the equivalent form:\nmin w\nλi+1‖w‖2 + i∑\nj=0\nλi−j\n( N∑\nk=1\n|dk(j)− uk,jw| 2 ) (512)\nwhich is an exponentially weighted least-squares problem. We see that, for every time instant j, the squared errors, |dk(j)−uk,jw|2, are summed across the network and scaled by the exponential weighting factor λi−j . The index i denotes current time and the index j denotes a time instant in the past. In this way, data occurring in the remote past are scaled more heavily than data occurring closer to present time. The global solution of (509) is given by [5]:\nwi = [Πi +HiWiHi] −1 H∗iWiYi (513)\nand the notation wi, with a subscript i, is meant to indicate that the estimate wi is based on all data collected from across the network up to time i. Therefore, the wi that is computed via (513) amounts to a global construction.\nIn [28, 29] a diffusion strategy was developed that allows nodes to approach the global solution wi by relying solely on local interactions. Let wk,i denote a local estimate for w\no that is computed by node k at time i. The diffusion recursive-least-squares (RLS) algorithm takes the following form. For every node k, we start with the initial conditions wk,−1 = 0 and Pk,−1 = δIM , where Pk,−1 is an M ×M matrix. Then, for every time instant i, each node k performs an incremental step followed by a diffusion step as follows:\nDiffusion RLS.\nStep 1 (incremental update)\nψk,i ← wk,i−1 Pk,i ← λ−1Pk,i−1 for every neighboring node ℓ ∈ Nk, update :\nψk,i ← ψk,i + cℓkPk,iu\n∗ ℓ,i\n1 + cℓkuℓ,iPk,iu∗ℓ,i [dℓ,i − uℓ,iψk,i]\nPk,i ← Pk,i − cℓkPk,iu\n∗ ℓ,iuℓ,iPk,i\n1 + cℓkuℓ,iPk,iu∗ℓ,i end\nStep 2 (diffusion update)\nwk,i = ∑\nℓ∈Nk\naℓkψℓ,i\n(514)\nwhere the symbol ← denotes a sequential assignment, and where the scalars {aℓk, cℓk} are nonnegative coefficients satisfying:\nfor k = 1, 2, . . . , N :\ncℓk ≥ 0, N∑\nk=1\ncℓk = 1, cℓk = 0 if ℓ /∈ Nk (515)\naℓk ≥ 0, N∑\nℓ=1\naℓk = 1, aℓk = 0 if ℓ /∈ Nk (516)\nThe above algorithm requires that at every instant i, nodes communicate to their neighbors their measurements {dℓ(i), uℓ,i} for the incremental update, and the intermediate estimates {ψℓ,i} for the diffusion update. During the incremental update, node k cycles through its neighbors and incorporates their data contributions represented by {dℓ(i), uℓ,i} into {ψk,i, Pk,i}. Every other node in the network is performing similar steps. At the end of the incremental step, neighboring nodes share their intermediate estimates {ψℓ,i} to undergo diffusion. Thus, at the end of both steps, each node k would have updated the quantities {wk,i−1, Pk,i−1} to {wk,i, Pk,i}. The quantities Pk,i are matrices of size M × M each. Observe that the diffusion RLS implementation (514) does not require the nodes to share their matrices {Pℓ,i}, which would amount to a substantial burden in terms of communications resources since each of these matrices has M2 entries. Only the quantities {dℓ(i), uℓ,i, ψℓ,i} are shared. The mean-square performance and convergence of the diffusion RLS strategy are studied in some detail in [29].\nThe incremental step of the diffusion RLS strategy (514) corresponds to performing a number of |Nk| successive least-squares updates starting from the initial conditions {wk,i−1, Pk,i−1} and ending with the values {ψk,i, Pk,i} that move on to the diffusion update step. It can be verified from the properties of recursive least-squares solutions [4, 5] that these variables satisfy the following equations at the end of the incremental stage (step 1):\nP−1k,i = λP −1 k,i−1 +\n∑\nℓ∈Nk\ncℓku ∗ ℓ,iuℓ,i (517)\nP−1k,i ψk,i = λP −1 k,i−1wk,i−1 +\n∑\nℓ∈Nk\ncℓku ∗ ℓ,idℓ(i) (518)\nIntroduce the auxiliary M × 1 variable:\nqk,i ∆ = P−1k,i ψk,i (519)\nThen, the above expressions lead to the following alternative form of the diffusion RLS strategy (514).\nAlternative form of diffusion RLS.\nwk,i−1 = ∑\nℓ∈Nk\naℓkψℓ,i−1\nP−1k,i = λP −1 k,i−1 +\n∑\nℓ∈Nk\ncℓku ∗ ℓ,iuℓ,i\nqk,i = λP −1 k,i−1wk,i−1 +\n∑\nℓ∈Nk\ncℓku ∗ ℓ,idℓ(i)\nψk,i = Pk,iqk,i\n(520)\nUnder some approximations, and for the special choices A = C and λ = 1, the diffusion RLS strategy (520) can be reduced to a form given in [79] and which is described by the following equations:\nP−1k,i = ∑\nℓ∈Nk\ncℓk [ P−1ℓ,i−1 + u ∗ ℓ,iuℓ,i ] (521)\nqk,i = ∑\nℓ∈Nk\ncℓk [ qℓ,i−1 + u ∗ ℓ,idℓ(i) ] (522)\nψk,i = Pk,iqk,i (523)\nAlgorithm (521)–(523) is motivated in [79] by using consensus-type arguments. Observe that the algorithm requires the nodes to share the variables {dℓ(i), uℓ,i, qℓ,i−1, Pℓ,i−1}, which corresponds to more communications overburden than required by diffusion RLS; the latter only requires that nodes share {dℓ(i), uℓ,i, ψℓ,i−1}. In order to illustrate how a special case of diffusion RLS (520) can be related to this scheme, let us set\nA = C and λ = 1 (524)\nThen, equations (520) give:\nSpecial form of diffusion RLS when A = C and λ = 1.\nwk,i−1 = ∑\nℓ∈Nk\ncℓkψℓ,i−1\nP−1k,i = P −1 k,i−1 +\n∑\nℓ∈Nk\ncℓku ∗ ℓ,iuℓ,i\nqk,i = P −1 k,i−1wk,i−1 +\n∑\nℓ∈Nk\ncℓku ∗ ℓ,idℓ(i)\nψk,i = Pk,iqk,i\n(525)\nComparing these equations with (521)–(523), we find that algorithm (521)–(523) of [79] would relate to the diffusion RLS algorithm (520) when the following approximations are justified:\n∑\nℓ∈Nk\ncℓkP −1 ℓ,i−1 ≈ P −1 k,i−1 (526)\n∑\nℓ∈Nk\ncℓkqℓ,i−1 = ∑\nℓ∈Nk\ncℓkP −1 ℓ,i−1ψℓ,i−1\n≈ ∑\nℓ∈Nk\ncℓkP −1 k,i−1ψℓ,i−1\n= P−1k,i−1\n∑\nℓ∈Nk\ncℓkψℓ,i−1 (527)\n= P−1k,i−1wk,i−1 (528)\nIt was indicated in [29] that the diffusion RLS implementation (514) or (520) leads to enhanced performance in comparison to the consensus-based update (521)–(523)."
    }, {
      "heading" : "10.3 Diffusion Kalman Filtering",
      "text" : "Diffusion strategies can also be applied to the solution of distributed state-space filtering and smoothing problems [30, 31, 33]. Here, we describe briefly the diffusion version of the Kalman filter; other variants and smoothing filters can be found in [33]. We assume that some system of interest is evolving according to linear state-space dynamics, and that every node in the network collects measurements that are linearly related to the unobserved state vector. The objective is for every node to track the state of the system over time based solely on local observations and on neighborhood interactions.\nThus, consider a network consisting of N nodes observing the state vector, xi, of size n × 1 of a linear state-space model. At every time i, every node k collects a measurement vector yk,i of size p × 1, which is related to the state vector as follows:\nxi+1 = Fixi +Gini (529)\nyk,i = Hk,ixi + vk,i, k = 1, 2, . . . , N (530)\nThe signals ni and vk,i denote state and measurement noises of sizes n× 1 and p× 1, respectively, and they are assumed to be zero-mean, uncorrelated and white, with covariance matrices denoted by\nE [ ni vk,i ] [ nj vk,j ]∗ ∆ = [ Qi 0 0 Rk,i ] δij (531)\nThe initial state vector, xo, is assumed to be zero-mean with covariance matrix\nExox ∗ o = Πo > 0 (532)\nand is uncorrelated with ni and vk,i, for all i and k. We further assume that Rk,i > 0. The parameter matrices {Fi, Gi, Hk,i, Qi, Rk,i,Πo} are assumed to be known by node k.\nLet x̂k,i|j denote a local estimator for xi that is computed by node k at time i based solely on local observations and on neighborhood data up to time j. The following diffusion strategy was proposed in [30, 31, 33] to evaluate approximate predicted and filtered versions of these local estimators in a distributed manner for data satisfying model (529)–(532). For every node k, we start with x̂k,0|−1 = 0 and Pk,0|−1 = Πo, where Pk,0|−1 is an M × M matrix. At every time instant i, every node k performs an incremental step\nfollowed by a diffusion step:\nTime and measurement-form of the diffusion Kalman filter.\nStep 1 (incremental update)\nψk,i ← x̂k,i|i−1 Pk,i ← Pk,i|i−1 for every neighboring node ℓ ∈ Nk, update : Re ← Rℓ,i +Hℓ,iPk,iH∗ℓ,i ψk,i ← ψk,i + Pk,iH ∗ ℓ,iR −1 e [ yℓ,i −Hℓ,iψk,i ]\nPk,i ← Pk,i − Pk,iH∗ℓ,iR −1 e Hℓ,iPk,i\nend\nStep 2 (diffusion update)\nx̂k,i|i = ∑\nℓ∈Nk\naℓkψℓ,i\nPk,i|i = Pk,i x̂k,i+1|i = Fix̂k,i|i Pk,i+1|i = FiPk,i|iF ∗ i +GiQiG ∗ i .\n(533)\nwhere the symbol← denotes a sequential assignment, and where the scalars {aℓk} are nonnegative coefficients satisfying:\nfor k = 1, 2, . . . , N :\naℓk ≥ 0, N∑\nℓ=1\naℓk = 1, aℓk = 0 if ℓ /∈ Nk (534)\nThe above algorithm requires that at every instant i, nodes communicate to their neighbors their measurement matrices Hℓ,i, the noise covariance matrices Rℓ,i, and the measurements yℓ,i for the incremental update, and the intermediate estimators ψℓ,i for the diffusion update. During the incremental update, node k cycles through its neighbors and incorporates their data contributions represented by {yℓ,i, Hℓ,i, Rℓ,i} into {ψk,i, Pk,i}. Every other node in the network is performing similar steps. At the end of the incremental step, neighboring nodes share their updated intermediate estimators {ψℓ,i} to undergo diffusion. Thus, at the end of both steps, each node k would have updated the quantities {x̂k,i|i−1, Pk,i|i−1} to {x̂k,i+1|i, Pk,i+1|i}. The quantities Pk,i|i−1 are n × n matrices. It is important to note that even though the notation Pk,i|i and Pk,i|i−1 has been retained for these variables, as in the standard Kalman filtering notation [5,77], these matrices do not represent any longer the covariances of the state estimation errors, x̃k,i|i−1 = xi − x̂k,i|i−1, but can be related to them [33].\nAn alternative representation of the diffusion Kalman filter may be obtained in information form by further assuming that Pk,i|i−1 > 0 for all k and i; a sufficient condition for this fact to hold is to requires the matrices {Fi} to be invertible [77]. Thus, consider again data satisfying model (529)–(532). For every node k, we start with x̂k,0|−1 = 0 and P −1 k,0|−1 = Π −1 o . At every time instant i, every node k performs an incremental step followed by a diffusion step:\nInformation form of the diffusion Kalman filter.\nStep 1 (incremental update)\nSk,i = ∑\nℓ∈Nk\nH∗ℓ,iR −1 ℓ,i Hℓ,i\nqk,i = ∑\nℓ∈Nk\nH∗ℓ,iR −1 ℓ,i yℓ,i\nP−1k,i|i = P −1 k,i|i−1 + Sk,i\nψk,i = x̂k,i|i−1 + Pk,i|i [ qk,i − Sk,ix̂k,i|i−1 ]\nStep 2: (diffusion update)\nx̂k,i|i = ∑\nℓ∈Nk\naℓkψℓ,i\nx̂k,i+1|i = Fix̂k,i|i Pk,i+1|i = FiPk,i|iF ∗ i +GiQiG ∗ i\n(535)\nThe incremental update in (535) is similar to the update used in the distributed Kalman filter derived in [48]. An important difference in the algorithms is in the diffusion step. Reference [48] starts from a continuous-time consensus implementation and discretizes it to arrive at the following update relation:\nx̂k,i|i = ψk,i + ǫ ∑\nℓ∈Nk\n(ψℓ,i −ψk,i) (536)\nwhich, in order to facilitate comparison with (535), can be equivalently rewritten as:\nx̂k,i|i = (1 + ǫ− nkǫ) · ψk,i + ∑\nℓ∈Nk\\{k}\nǫ · ψℓ,i (537)\nwhere nk denotes the degree of node k (i.e., the size of its neighborhood, Nk). In comparison, the diffusion step in (535) can be written as:\nx̂k,i|i = akk ·ψk,i + ∑\nℓ∈Nk\\{k}\naℓk ·ψℓ,i (538)\nObserve that the weights used in (537) are (1 + ǫ − nkǫ) for the node’s estimator, ψk,i, and ǫ for all other estimators, {ψℓ,i}, arriving from the neighbors of node k. In contrast, the diffusion step (538) employs a convex combination of the estimators {ψℓ,i} with generally different weights {aℓk} for different neighbors; this choice is motivated by the desire to employ combination coefficients that enhance the fusion of information at node k, as suggested by the discussion in App. D of [33]. It was verified in [33] that the diffusion implementation (538) leads to enhanced performance in comparison to the consensus-based update (537). Moreover, the weights {aℓk} in (538) can also be adjusted over time in order to further enhance performance, as discussed in [78]. The mean-square performance and convergence of the diffusion Kalman filtering implementations are studied in some detail in [33], along with other diffusion strategies for smoothing problems including fixed-point and fixed-lag smoothing."
    }, {
      "heading" : "10.4 Diffusion Distributed Optimization",
      "text" : "The ATC and CTA steepest-descent diffusion strategies (134) and (142) derived earlier in Sec. 3 provide distributed mechanisms for the solution of global optimization problems of the form:\nmin w\nN∑\nk=1\nJk(w) (539)\nwhere the individual costs, Jk(w), were assumed to be quadratic in w, namely,\nJk(w) = σ 2 d,k − w ∗rdu,k − r ∗ du,k w + w ∗Ru,k w (540)\nfor given parameters {σ2d,k, rdu,k, Ru,k}. Nevertheless, we remarked in that section that similar diffusion strategies can be applied to more general cases involving individual cost functions, Jk(w), that are not necessarily quadratic in w [1–3]. We restate below, for ease of reference, the general ATC and CTA diffusion strategies (139) and (146) that can be used for the distributed solution of global optimization problems of the form (539) for more general convex functions Jk(w):\n(ATC strategy)\nψk,i = wk,i−1 − µk ∑\nℓ∈Nk\ncℓk [∇wJℓ(wk,i−1)] ∗\nwk,i = ∑\nℓ∈Nk\naℓk ψℓ,i (541)\nand\n(CTA strategy)\nψk,i−1 = ∑\nℓ∈Nk\naℓk wℓ,i−1\nwk,i = ψk,i−1 − µk ∑\nℓ∈Nk\ncℓk [∇wJℓ(ψk,i−1)] ∗ (542)\nfor positive step-sizes {µk} and for nonnegative coefficients {cℓk, aℓk} that satisfy:\nfor k = 1, 2, . . . , N :\ncℓk ≥ 0, N∑\nk=1\ncℓk = 1, cℓk = 0 if ℓ /∈ Nk\naℓk ≥ 0, N∑\nℓ=1\naℓk = 1, aℓk = 0 if ℓ /∈ Nk\n(543)\nThat is, the matrix A = [aℓk] is left-stochastic while the matrix C = [cℓk] is right-stochastic:\nC1 = 1, AT1 = 1 (544)\nWe can again regard the above ATC and CTA strategies as special cases of the following general diffusion scheme:\nφk,i−1 = ∑\nℓ∈Nk\na1,ℓk wℓ,i−1 (545)\nψk,i = φk,i−1 − µk ∑\nℓ∈Nk\ncℓk [∇wJℓ(φk,i−1)] ∗ (546)\nwk,i = ∑\nℓ∈Nk\na2,ℓk ψℓ,i (547)\nwhere the coefficients {a1,ℓk, a2,ℓk, cℓk} are nonnegative coefficients corresponding to the (l, k)-th entries of combination matrices {A1, A2, C} that satisfy:\nAT1 1 = 1, A T 2 1 = 1, C1 = 1 (548)\nThe convergence behavior of these diffusion strategies can be examined under both conditions of noiseless updates (when the gradient vectors are available) and noisy updates (when the gradient vectors are subject to gradient noise). The following properties can be proven for the diffusion strategies (545)–(547) [1–3]. The statements that follow assume, for convenience of presentation, that all data are real-valued; the conditions would need to be adjusted for complex-valued data.\nNoiseless Updates Let\nJglob(w) = N∑\nk=1\nJk(w) (549)\ndenote the global cost function that we wish to minimize. Assume Jglob(w) is strictly convex so that its minimizer wo is unique. Assume further that each individual cost function Jk(w) is convex and has a minimizer at the same wo. This case is common in practice; situations abound where nodes in a network need to work cooperatively to attain a common objective (such as tracking a target, locating the source of chemical leak, estimating a physical model, or identifying a statistical distribution). The case where the {Jk(w)} have different individual minimizers is studied in [1,3], where it is shown that the same diffusion strategies of this section are still applicable and nodes would converge instead to a Pareto-optimal solution.\nTheorem 10.1. (Convergence to Optimal Solution: Noise-Free Case) Consider the problem of minimizing the strictly convex global cost (549), with the individual cost functions {Jk(w)} assumed to be convex with each having a minimizer at the same wo. Assume that all data are real-valued and suppose the Hessian matrices of the individual costs are bounded from below and from above as follows:\nλℓ,minIM ≤ ∇ 2 wJℓ(w) ≤ λℓ,maxIM , ℓ = 1, 2, . . . , N (550)\nfor some positive constants {λℓ,min, λℓ,max}. Let\nσk,min ∆ =\n∑\nℓ∈Nk\ncℓkλℓ,min, σk,max ∆ =\n∑\nℓ∈Nk\ncℓkλℓ,max (551)\nAssume further that σk,min > 0 and that the positive step-sizes are chosen such that:\nµk ≤ 2\nσk,max , k = 1, . . . , N (552)\nThen, it holds that wk,i → wo as i → ∞. That is, the weight estimates generated by (545)–(547) at all nodes will tend towards the desired global minimizer.\nWe note that in works on distributed sub-gradient methods (e.g., [40, 80]), the norms of the sub-gradients are usually required to be uniformly bounded. Such a requirement is restrictive in the unconstrained optimization of differentiable functions. Condition (550) is more relaxed since it allows the gradient vector ∇wJℓ(w) to have unbounded norm. This extension is important because requiring bounded gradient norms, as opposed to bounded Hessian matrices, would exclude the possibility of using quadratic costs for the Jℓ(w) (since the gradient vectors would then be unbounded). And, as we saw in the body of the chapter, quadratic costs play a critical role in adaptation and learning over networks.\nUpdates with Gradient Noise It is often the case that we do not have access to the exact gradient vectors to use in (546), but to noisy versions of them, say,\n̂∇wJℓ(φk,i−1) ∆ = ∇wJℓ(φk,i−1) + vℓ(φ̃k,i−1) (553)\nwhere the random vector variable vℓ(·) refers to gradient noise; its value is generally dependent on the weight-error vector realization,\nφ̃k,i−1 ∆ = wo − φk,i−1 (554)\nat which the gradient vector is being evaluated. In the presence of gradient noise, the weight estimates at the various nodes become random quantities and we denote them by the boldface notation {wk,i}. We assume that, conditioned on the past history of the weight estimators at all nodes, namely,\nFi−1 ∆ = {wm,j, m = 1, 2, . . . , N, j < i} (555)\nthe gradient noise has zero mean and its variance is upper bounded as follows:\nE { vℓ(φ̃k,i−1) | Fi−1 } = 0 (556)\nE { ‖vℓ(φ̃k,i−1)‖ 2 | Fi−1 } ≤ α‖φ̃k,i−1‖ 2 + σ2v (557)\nfor some α > 0 and σ2v ≥ 0. Condition (557) allows the variance of the gradient noise to be time-variant, so long as it does not grow faster than E‖φ̃k,i−1‖ 2. This condition on the noise is more general than the “uniform-bounded assumption” that appears in [40], which required instead:\nE { ‖vℓ(φ̃k,i−1)‖ 2 } ≤ σ2v , E { ‖vℓ(φ̃k,i−1)‖ 2 | Fi−1 } ≤ σ2v (558)\nThese two requirements are special cases of (557) for α = 0. Furthermore, condition (557) is similar to condition (4.3) in [81], which requires the noise variance to satisfy:\nE { ‖vℓ(φ̃k,i−1)‖ 2 | Fi−1 } ≤ α [ ‖∇wJℓ(φk,i−1)‖ 2 + 1 ]\n(559)\nThis requirement can be verified to be a combination of the “relative random noise” and the “absolute random noise” conditions defined in [22] — see [2].\nNow, introduce the column vector:\nzi ∆ =\nN∑\nℓ=1\ncol {cℓ1vℓ(w o), cℓ2vℓ(w o), . . . , cℓNvℓ(w o)} (560)\nand let Z ∆ = Eziz ∗ i (561)\nLet further w̃i ∆ = col {w̃i,1, w̃i,2, . . . , w̃i,N} (562)\nwhere w̃k,i ∆ = wo −wk,i (563)\nThen, the following result can be established [2]; it characterizes the network mean-square deviation in steady-state, which is defined as\nMSDnetwork ∆ = lim\ni→∞\n( 1\nN\nN∑\nk=1\nE‖w̃k,i‖ 2 ) (564)\nTheorem 10.2. (Mean-Square Stability: Noisy Case) Consider the problem of minimizing the strictly convex global cost (549), with the individual cost functions {Jk(w)} assumed to be convex with each having a minimizer at the same wo. Assume all data are real-valued and suppose the Hessian matrices of the individual costs are bounded from below and from above as stated in (550). Assume further that the diffusion strategy (545)–(547) employs noisy gradient vectors, where the noise terms are zero mean and satisfy conditions (557) and (561). We select the positive step-sizes to be sufficiently small and to satisfy:\nµk < min\n{ 2σk,max\nσ2k,max + α‖C‖ 2 1\n, 2σk,min\nσ2k,min + α‖C‖ 2 1\n} (565)\nfor k = 1, 2, . . . , N . Then, the diffusion strategy (545)–(547) is mean-square stable and the mean-squaredeviation of the network is given by:\nMSDnetwork ≈ 1\nN\n[ vec ( AT2 MZ TMA2 )]T · (I −F)−1 · vec(INM ) (566)\nwhere\nA2 = A2 ⊗ IM (567)\nM = diag{µ1IM , µ2IM , . . . , µNIM} (568)\nF ≈ BT ⊗ B∗ (569) B = AT2 (I −MR)A T 1 (570) R = N∑\nℓ=1\ndiag { cℓ1∇ 2 wJℓ(w o), cℓ2∇ 2 wJℓ(w o), . . . , cℓN∇ 2 wJℓ(w o) }\n(571)"
    }, {
      "heading" : "Acknowledgements",
      "text" : "The development of the theory and applications of diffusion adaptation over networks has benefited greatly from the insights and contributions of several UCLA PhD students, and several visiting graduate students to the UCLA Adaptive Systems Laboratory (http://www.ee.ucla.edu/asl). The assistance and contributions of all students are hereby gratefully acknowledged, including Cassio G. Lopes, Federico S. Cattivelli, ShengYuan Tu, Jianshu Chen, Xiaochuan Zhao, Zaid Towfic, Chung-Kai Yu, Noriyuki Takahashi, Jae-Woo Lee, Alexander Bertrand, and Paolo Di Lorenzo. The author is also particularly thankful to S.-Y. Tu, J. Chen, X. Zhao, Z. Towfic, and C.-K. Yu for their assistance in reviewing an earlier draft of this article."
    }, {
      "heading" : "A Appendix: Properties of Kronecker Products",
      "text" : "For ease of reference, we collect in this appendix some useful properties of Kronecker products. All matrices are assumed to be of compatible dimensions; all inverses are assumed to exist whenever necessary. Let E = [eij ] n i,j=1 and B = [bij ] m i,j=1 be n × n and m × m matrices, respectively. Their Kronecker product is denoted by E ⊗B and is defined as the nm× nm matrix whose entries are given by [20]:\nE ⊗B =   e11B e12B . . . e1nB e21B e22B . . . e2nB ...\n... en1B en2B . . . ennB\n  (572)\nIn other words, each entry of E is replaced by a scaled multiple of B. Let {λi(E), i = 1, . . . , n} and {λj(B), j = 1, . . . ,m} denote the eigenvalues of E and B, respectively. Then, the eigenvalues of E ⊗B will consist of all nm product combinations {λi(E)λj(B)}. Table 9 lists some well-known properties of Kronecker products."
    }, {
      "heading" : "B Appendix: Graph Laplacian and Network Connectivity",
      "text" : "Consider a network consisting of N nodes and L edges connecting the nodes to each other. In the constructions below, we only need to consider the edges that connect distinct nodes to each other; these edges do not contain any self-loops that may exist in the graph and which connect nodes to themselves directly. In other words, when we refer to the L edges of a graph, we are excluding self-loops from this set; but we are still allowing loops of at least length 2 (i.e., loops generated by paths covering at least 2 edges).\nThe neighborhood of any node k is denoted by Nk and it consists of all nodes that node k can share information with; these are the nodes that are connected to k through edges, in addition to node k itself. The degree of node k, which we denote by nk, is defined as the positive integer that is equal to the size of its neighborhood:\nnk ∆ = |Nk| (573)\nSince k ∈ Nk, we always have nk ≥ 1. We further associate with the network an N ×N Laplacian matrix,\ndenoted by L. The matrix L is symmetric and its entries are defined as follows [64–66]:\n[L]kℓ =    nk − 1, if k = ℓ −1, if k 6= ℓ and nodes k and ℓ are neighbors 0, otherwise\n(574)\nNote that the term nk − 1 measures the number of edges that are incident on node k, and the locations of the −1′s on row k indicate the nodes that are connected to node k. We also associate with the graph an N × L incidence matrix, denoted by I. The entries of I are defined as follows. Every column of I represents one edge in the graph. Each edge connects two nodes and its column will display two nonzero entries at the rows corresponding to these nodes: one entry will be +1 and the other entry will be −1. For directed graphs, the choice of which entry is positive or negative can be used to identify the nodes from which edges emanate (source nodes) and the nodes at which edges arrive (sink nodes). Since we are dealing with undirected graphs, we shall simply assign positive values to lower indexed nodes and negative values to higher indexed nodes:\n[I]ke =    +1, if node k is the lower-indexed node connected to edge e −1, if node k is the higher-indexed node connected to edge e 0, otherwise\n(575)\nFigure 16 shows the example of a network with N = 6 nodes and L = 8 edges. Its Laplacian and incidence matrices are also shown and these have sizes 6 × 6 and 6 × 8, respectively. Consider, for example, column 6 in the incidence matrix. This column corresponds to edge 6, which links nodes 3 and 5. Therefore, at location I36 we have a +1 and at location I56 we have −1. The other columns of I are constructed in a similar manner.\nObserve that the Laplacian and incidence matrices of a graph are related as follows:\nL = I IT (576)\nThe Laplacian matrix conveys useful information about the topology of the graph. The following is a classical result from graph theory [64–67].\nLemma B.1. (Laplacian and Network Connectivity) Let\nθ1 ≥ θ2 ≥ . . . ≥ θN (577)\ndenote the ordered eigenvalues of L. Then the following properties hold:\n(a) L is symmetric nonnegative-definite so that θi ≥ 0.\n(b) The rows of L add up to zero so that L1 = 0. This means that 1 is a right eigenvector of L corresponding to the eigenvalue zero.\n(c) The smallest eigenvalue is always zero, θN = 0. The second smallest eigenvalue, θN−1, is called the algebraic connectivity of the graph.\n(d) The number of times that zero is an eigenvalue of L (i.e., its multiplicity) is equal to the number of connected subgraphs.\n(e) The algebraic connectivity of a connected graph is nonzero, i.e., θN−1 6= 0. In other words, a graph is connected if, and only if, its algebraic connectivity is nonzero.\nProof. Property (a) follows from the identity L = I IT . Property (b) follows from the definition of L. Note that for each row of L, the entries on the row add up to zero. Property (c) follows from properties (a) and (b) since L1 = 0 implies that zero is an eigenvalue of L. For part (d), assume the network consists of two separate connected subgraphs. Then, the Laplacian matrix would have a block diagonal structure, say, of the form L = diag{L1,L2}, where L1 and L2 are the Laplacian matrices of the smaller subgraphs. The smallest eigenvalue of each of these Laplacian matrices would in turn be zero and unique by property (e). More generally, if the graph consists of m connected subgraphs, then the multiplicity of zero as an eigenvalue of L must be m. To establish property (e), first observe that if the algebraic connectivity is nonzero then it is obvious that the graph must be connected. Otherwise, if the graph were disconnected, then its Laplacian matrix would be block diagonal and the algebraic multiplicity of zero as an eigenvalue of L would be larger than one so that θN−1 would be zero, which is a contradiction. For the converse statement, assume the graph is connected and let x denote an arbitrary eigenvector of L corresponding to the eigenvalue at zero, i.e., Lx = 0. We already know that L1 = 1 from property (b). Let us verify that x must be proportional to the vector 1 so that the algebraic multiplicity of the eigenvalue at zero is one. Thus note that xTLx = 0. If we denote the individual entries of x by xk, then this identity implies that for each node k:\n∑\nℓ∈Nk\n(xk − xℓ) 2 = 0\nIt follows that the entries of x within each neighborhood have equal values. But since the graph is connected, we conclude that all entries of x must be equal. It follows that the eigenvector x is proportional to the vector 1, as desired."
    }, {
      "heading" : "C Appendix: Stochastic Matrices",
      "text" : "Consider N × N matrices A with nonnegative entries, {aℓk ≥ 0}. The matrix A = [aℓk] is said to be right-stochastic if it satisfies\nA1 = 1 (right-stochastic) (578)\nin which case each row of A adds up to one. The matrix A is said to be left-stochastic if it satisfies\nAT1 = 1 (left-stochastic) (579)\nin which case each column of A adds up to one. And the matrix is said to be doubly stochastic if both conditions hold so that both its columns and rows add up to one:\nA1 = 1, AT1 = 1 (doubly-stochastic) (580)\nStochastic matrices arise frequently in the study of adaptation over networks. This appendix lists some of their properties.\nLemma C.1. (Spectral Norm of Stochastic Matrices) Let A be an N × N right or left or doubly stochastic matrix. Then, ρ(A) = 1 and, therefore, all eigenvalues of A lie inside the unit disc, i.e., |λ(A)| ≤ 1.\nProof. We prove the result for right stochastic matrices; a similar argument applies to left or doubly stochastic matrices. Let A be a right-stochastic matrix. Then, A1 = 1, so that λ = 1 is one of the eigenvalues of A. Moreover, for any matrix A, it holds that ρ(A) ≤ ‖A‖∞, where ‖ · ‖∞ denotes the maximum absolute row sum of its matrix argument. But since all rows of A add up to one, we have ‖A‖∞ = 1. Therefore, ρ(A) ≤ 1. And since we already know that A has an eigenvalue at λ = 1, we conclude that ρ(A) = 1.\nThe above result asserts that the spectral radius of a stochastic matrix is unity and that A has an eigenvalue at λ = 1. The result, however, does not rule out the possibility of multiple eigenvalues at λ = 1, or even other eigenvalues with magnitude equal to one. Assume, in addition, that the stochastic matrix A is regular. This means that there exists an integer power jo such that all entries of A jo are strictly positive, i.e.,\nfor all (ℓ, k), it holds that [ Ajo ] ℓk > 0, for some jo > 0 (581)\nThen a result in matrix theory known as the Perron-Frobenius Theorem [20] leads to the following stronger characterization of the eigen-structure of A.\nLemma C.2. (Spectral Norm of Regular Stochastic Matrices) Let A be an N ×N right stochastic and regular matrix. Then:\n(a) ρ(A) = 1.\n(b) All other eigenvalues of A are strictly inside the unit circle (and, hence, have magnitude strictly less than one).\n(c) The eigenvalue at λ = 1 is simple, i.e., it has multiplicity one. Moreover, with proper sign scaling, all entries of the corresponding eigenvector are positive. For a right-stochastic A, this eigenvector is the vector 1 since A1 = 1.\n(d) All other eigenvectors associated with the other eigenvalues will have at least one negative or complex entry.\nProof. Part (a) follows from Lemma C.1. Parts (b)-(d) follow from the Perron-Frobenius Theorem when A is regular [20].\nLemma C.3. (Useful Properties of Doubly Stochastic Matrices) Let A be an N×N doubly stochastic matrix. Then the following properties hold:\n(a) ρ(A) = 1.\n(b) AAT and ATA are doubly stochastic as well.\n(c) ρ(AAT ) = ρ(ATA) = 1.\n(d) The eigenvalues of AAT or ATA are real and lie inside the interval [0, 1].\n(e) I −AAT ≥ 0 and I − ATA ≥ 0.\n(f) Tr(ATHA) ≤ Tr(H), for any N ×N nonnegative-definite Hermitian matrix H.\nProof. Part (a) follows from Lemma C.1. For part (b), note that AAT is symmetric and AAT1 = A1 = 1. Therefore, AAT is doubly stochastic. Likewise for ATA. Part (c) follows from part (a) since AAT and ATA are themselves doubly stochastic matrices. For part (d), note that AAT is symmetric and nonnegative-definite. Therefore, its eigenvalues are real and nonnegative. But since ρ(AAT ) = 1, we must have λ(AAT ) ∈ [0, 1]. Likewise for the matrix ATA. Part (e) follows from part (d). For part (f), since AAT ≥ 0 and its eigenvalues lie within [0, 1], the matrix AAT admits an eigen-decomposition of the form:\nAAT = UΛUT\nwhere U is orthogonal (i.e., U−1 = UT ) and Λ is diagonal with entries in the range [0, 1]. It then follows that\nTr(ATHA) = Tr(AATH)\n= Tr(UΛUTH) = Tr(ΛUTHU)\n(∗) ≤ Tr(UTHU) = Tr(UUTH)\n= Tr(H)\nwhere step (∗) is because UTHU = U−1HU and, by similarity, the matrix U−1HU has the same eigenvalues as H . Therefore, UTHU ≥ 0. This means that the diagonal entries of UTHU are nonnegative. Multiplying UTHU by Λ ends up scaling the nonnegative diagonal entries to smaller values so that (∗) is justified."
    }, {
      "heading" : "D Appendix: Block Maximum Norm",
      "text" : "Let x = col{x1, x2, . . . , xN} denote an N × 1 block column vector whose individual entries are of size M × 1 each. Following [52, 54, 55], the block maximum norm of x is denoted by ‖x‖b,∞ and is defined as\n‖x‖b,∞ ∆ = max\n1≤k≤N ‖xk‖ (582)\nwhere ‖·‖ denotes the Euclidean norm of its vector argument. Correspondingly, the induced block maximum norm of an arbitrary N×N block matrix A, whose individual block entries are of size M×M each, is defined as\n‖A‖b,∞ ∆ = max\nx 6=0 ‖Ax‖b,∞ ‖x‖b,∞\n(583)\nThe block maximum norm inherits the unitary invariance property of the Euclidean norm, as the following result indicates [54].\nLemma D.1. (Unitary Invariance) Let U = diag{U1, U2, . . . , UN} be an N × N block diagonal matrix with M ×M unitary blocks {Uk}. Then, the following properties hold:\n(a) ‖Ux‖b,∞ = ‖x‖b,∞\n(b) ‖UAU∗‖b,∞ = ‖A‖b,∞\nfor all block vectors x and block matrices A of appropriate dimensions.\nThe next result provides useful bounds for the block maximum norm of a block matrix.\nLemma D.2. (Useful Bounds) Let A be an arbitrary N ×N block matrix with blocks Aℓk of size M ×M each. Then, the following results hold:\n(a) The norms of A and its complex conjugate are related as follows:\n‖A∗‖b,∞ ≤ N · ‖A‖b,∞ (584)\n(b) The norm of A is bounded as follows:\nmax 1≤ℓ,k≤N ‖Aℓk‖ ≤ ‖A‖b,∞ ≤ N ·\n( max\n1≤ℓ,k≤N ‖Aℓk‖\n) (585)\nwhere ‖ · ‖ denotes the 2−induced norm (or maximum singular value) of its matrix argument.\n(c) If A is Hermitian and nonnegative-definite (A ≥ 0), then there exist finite positive constants c1 and c2 such that\nc1 · Tr(A) ≤ ‖A‖b,∞ ≤ c2 · Tr(A) (586)\nProof. Part (a) follows directly from part (b) by noting that\n‖A∗‖b,∞ ≤ N · ( max\n1≤ℓ,k≤N ‖A∗ℓk‖\n)\n= N · ( max\n1≤ℓ,k≤N ‖Aℓk‖\n)\n≤ N · ‖A‖b,∞\nwhere the equality in the second step is because ‖A∗ℓk‖ = ‖Aℓk‖; i.e., complex conjugation does not alter the 2−induced norm of a matrix.\nTo establish part (b), we consider arbitrary N × 1 block vectors x with entries x = col{x1, x2, . . . , xN} and where each xk is M × 1. Then, note that\n‖Ax‖b,∞ = max 1≤ℓ≤N ∥∥∥∥∥ N∑\nk=1\nAℓkxk ∥∥∥∥∥\n≤ max 1≤ℓ≤N\n( N∑\nk=1\n‖Aℓk‖ · ‖xk‖\n)\n≤ ( max\n1≤ℓ≤N\nN∑\nk=1\n‖Aℓk‖ ) · max 1≤k≤N ‖xk‖\n≤ ( max\n1≤ℓ≤N\nN∑\nk=1\nmax 1≤k≤N ‖Aℓk‖\n) · ‖x‖b,∞\n≤ N · ( max\n1≤ℓ,k≤N ‖Aℓk‖\n) · ‖x‖b,∞\nso that\n‖A‖b,∞ ∆ = max\nx 6=0 ‖Ax‖b,∞ ‖x‖b,∞ ≤ N ·\n( max\n1≤ℓ,k≤N ‖Aℓk‖\n)\nwhich establishes the upper bound in (585). To establish the lower bound, we assume without loss of generality that max1≤ℓ,k≤N ‖Aℓk‖ is attained at ℓ = 1 and k = 1. Let σ1 denote the largest singular value of A11 and let {v1, u1} denote the corresponding M × 1 right and left singular vectors. That is,\n‖A11‖ = σ1, A11v1 = σ1u1 (587)\nwhere v1 and u1 have unit norms. We now construct an N × 1 block vector x o as follows:\nxo ∆ = col{v1, 0M , 0M , . . . , 0M} (588)\nThen, obviously, ‖xo‖b,∞ = 1 (589)\nand Axo = col{A11v1, A21v1, . . . , AN1v1} (590)\nIt follows that\n‖Axo‖b,∞ = max { ‖A11v1‖, ‖A21v1‖, . . . , ‖AN1v1‖ }\n≥ ‖A11v1‖\n= ‖σ1u1‖\n= σ1\n= ‖A11‖\n= max 1≤ℓ,k≤N ‖Aℓk‖ (591)\nTherefore, by the definition of the block maximum norm,\n‖A‖b,∞ ∆ = max\nx 6=0 ( ‖Ax‖b,∞ ‖x‖b,∞ )\n≥ ‖Axo‖b,∞ ‖xo‖b,∞ = ‖Axo‖b,∞\n≥ max 1≤ℓ,k≤N ‖Aℓk‖ (592)\nwhich establishes the lower bound in (585). To establish part (c), we start by recalling that all norms on finite-dimensional vector spaces are equivalent [20,21]. This means that if ‖ · ‖a and ‖ · ‖d denote two different matrix norms, then there exist positive constants c1 and c2 such that for any matrix X,\nc1 · ‖X‖a ≤ ‖X‖d ≤ c2 · ‖X‖a (593)\nNow, let ‖A‖∗ denote the nuclear norm of the square matrix A. It is defined as the sum of its singular values:\n‖A‖∗ ∆ =\n∑\nm\nσm(A) (594)\nSince A is Hermitian and nonnegative-definite, its eigenvalues coincide with its singular values and, therefore,\n‖A‖∗ = ∑\nm\nλm(A) = Tr(A)\nNow applying result (593) to the two norms ‖A‖b,∞ and ‖A‖∗ we conclude that\nc1 · Tr(A) ≤ ‖A‖b,∞ ≤ c2 · Tr(A) (595)\nas desired.\nThe next result relates the block maximum norm of an extended matrix to the ∞−norm (i.e., maximum absolute row sum) of the originating matrix. Specifically, let A be an N ×N matrix with bounded entries and introduce the block matrix\nA ∆ = A⊗ IM (596)\nThe extended matrix A has blocks of size M ×M each.\nLemma D.3. (Relation to Maximum Absolute Row Sum) Let A and A be related as in (596). Then, the following properties hold:\n(a) ‖A‖b,∞ = ‖A‖∞, where the notation ‖ · ‖∞ denotes the maximum absolute row sum of its argument.\n(b) ‖A∗‖b,∞ ≤ N · ‖A‖b,∞.\nProof. The results are obvious for a zero matrix A. So assume A is nonzero. Let x = col{x1, x2, . . . , xN} denote an arbitrary N × 1 block vector whose individual entries {xk} are vectors of size M × 1 each. Then,\n‖Ax‖b,∞ = max 1≤k≤N ∥∥∥∥∥ N∑\nℓ=1\nakℓxℓ ∥∥∥∥∥\n≤ max 1≤k≤N\n( N∑\nℓ=1\n|akℓ| · ‖xℓ‖\n)\n≤ ( max\n1≤k≤N\nN∑\nℓ=1\n|akℓ| ) · max 1≤ℓ≤N ‖xℓ‖\n= ‖A‖∞ · ‖x‖b,∞ (597)\nso that\n‖A‖b,∞ ∆ = max\nx 6=0 ‖Ax‖b,∞ ‖x‖b,∞ ≤ ‖A‖∞ (598)\nThe argument so far establishes that ‖A‖b,∞ ≤ ‖A‖∞. Now, let ko denote the row index that corresponds to the maximum absolute row sum of A, i.e.,\n‖A‖∞ = N∑\nℓ=1\n|akoℓ|\nWe construct an N × 1 block vector z = col{z1, z2, . . . , zN}, whose M × 1 entries {zℓ} are chosen as follows:\nzℓ = sign(akoℓ) · e1\nwhere e1 is the M × 1 basis vector: e1 = col{1, 0, 0, . . . , 0}\nand the sign function is defined as\nsign(a) =\n{ 1, if a ≥ 0\n−1, otherwise\nThen, note that z 6= 0 for any nonzero matrix A, and\n‖z‖b,∞ = max 1≤ℓ≤N ‖zℓ‖ = 1\nMoreover,\n‖A‖b,∞ ∆ = max\nx 6=0 ‖Ax‖b,∞ ‖x‖b,∞\n≥ ‖Az‖b,∞ ‖z‖b,∞ = ‖Az‖b,∞\n= max 1≤k≤N ∥∥∥∥∥ N∑\nℓ=1\nakℓzℓ ∥∥∥∥∥\n≥ ∥∥∥∥∥ N∑\nℓ=1\nakoℓzℓ ∥∥∥∥∥\n= ∥∥∥∥∥ N∑\nℓ=1\nakoℓ · sign(akoℓ)e1 ∥∥∥∥∥\n= N∑\nℓ=1\n|akoℓ| · ‖e1‖\n=\nN∑\nℓ=1\n|akoℓ|\n= ‖A‖∞ (599)\nCombining this result with (598) we conclude that ‖A‖b,∞ = ‖A‖∞, which establishes part (a). Part (b) follows from the statement of part (a) in Lemma D.2.\nThe next result establishes a useful property for the block maximum norm of right or left stochastic matrices; such matrices arise as combination matrices for distributed processing over networks as in (166) and (185).\nLemma D.4. (Right and Left Stochastic Matrices) Let C be an N ×N right stochastic matrix, i.e., its entries are nonnegative and it satisfies C1 = 1. Let A be an N ×N left stochastic matrix, i.e., its entries are nonnegative and it satisfies AT1 = 1. Introduce the block matrices\nAT ∆ = AT ⊗ IM , C ∆ = C ⊗ IM (600)\nThe matrices A and C have blocks of size M ×M each. It holds that\n‖AT ‖b,∞ = 1, ‖C‖b,∞ = 1 (601)\nProof. Since AT and C are right stochastic matrices, it holds that ‖AT ‖∞ = 1 and ‖C‖∞ = 1. The desired result then follows from part (a) of Lemma D.3.\nThe next two results establish useful properties for the block maximum norm of a block diagonal matrix transformed by stochastic matrices; such transformations arise as coefficient matrices that control the evolution of weight error vectors over networks, as in (189).\nLemma D.5. (Block Diagonal Hermitian Matrices) Consider an N × N block diagonal Hermitian matrix D = diag{D1, D2, . . . , DN}, where each Dk is M ×M Hermitian. It holds that\nρ(D) = max 1≤k≤N ρ(Dk) = ‖D‖b,∞ (602)\nwhere ρ(·) denotes the spectral radius (largest eigenvalue magnitude) of its argument. That is, the spectral radius of D agrees with the block maximum norm of D, which in turn agrees with the largest spectral radius of its block components.\nProof. We already know that the spectral radius of any matrix X satisfies ρ(X ) ≤ ‖X‖, for any induced matrix norm [19,20]. Applying this result to D we readily get that ρ(D) ≤ ‖D‖b,∞. We now establish the reverse inequality, namely, ‖D‖b,∞ ≤ ρ(D). Thus, pick an arbitrary N × 1 block vector x with entries {x1, x2, . . . , xN}, where each xk is M × 1. From definition (583) we have\n‖D‖b,∞ ∆ = max\nx 6=0 ‖Dx‖b,∞ ‖x‖b,∞\n= max x 6=0\n( 1\n‖x‖b,∞ · max 1≤k≤N ‖Dkxk‖\n)\n≤ max x 6=0\n( 1\n‖x‖b,∞ · max 1≤k≤N (‖Dk‖ · ‖xk‖)\n)\n= max x 6=0 max 1≤k≤N\n( ‖Dk‖ · ‖xk‖\n‖x‖b,∞\n)\n≤ max 1≤k≤N ‖Dk‖\n= max 1≤k≤N ρ(Dk) (603)\nwhere the notation ‖Dk‖ denotes the 2−induced norm of Dk (i.e., its largest singular value). But since Dk is assumed to be Hermitian, its 2−induced norm agrees with its spectral radius, which explains the last equality.\nLemma D.6. (Block Diagonal Matrix Transformed by Left Stochastic Matrices) Consider an N ×N block diagonal Hermitian matrix D = diag{D1, D2, . . . , DN}, where each Dk is M ×M Hermitian. Let A1 and A2 be N×N left stochastic matrices, i.e., their entries are nonnegative and they satisfy AT1 1 = 1 and AT2 1 = 1. Introduce the block matrices\nAT1 = A T 1 ⊗ IM , A T 2 ∆ = AT2 ⊗ IM (604)\nThe matrices A1 and A2 have blocks of size M ×M each. Then it holds that\nρ ( AT2 · D · A T 1 ) ≤ ρ(D) (605)\nProof. Since the spectral radius of any matrix never exceeds any induced norm of the same matrix, we have that\nρ ( AT2 · D · A T 1 ) ≤ ∥∥∥ AT2 · D · AT1 ∥∥∥ b,∞\n≤ ∥∥∥AT2 ∥∥∥ b,∞ · ‖D‖ b,∞ · ∥∥∥AT1 ∥∥∥ b,∞\n(601) = ‖D‖\nb,∞\n(602) = ρ(D) (606)\nIn view of the result of Lemma D.5, we also conclude from (605) that\nρ ( AT2 · D · A T 1 ) ≤ max\n1≤k≤N ρ(Dk) (607)\nIt is worth noting that there are choices for the matrices {A1,A2,D} that would result in strict inequality in (605). Indeed, consider the special case:\nD = [ 2 0 0 1 ] , AT1 = [ 1 3 2 3\n2 3 1 3\n] , AT2 = [ 1 3 2 3\n2 3 1 3\n]\nThis case corresponds to N = 2 and M = 1 (scalar blocks). Then,\nAT2 DA T 1 =\n[ 2 3 2 3\n2 3 1\n]\nand it is easy to verify that ρ(D) = 2, ρ(AT2 DA T 1 ) ≈ 1.52\nThe following conclusions follow as corollaries to the statement of Lemma D.6, where by a stable matrix X we mean one whose eigenvalues lie strictly inside the unit circle.\nCorollary D.1. (Stability Properties) Under the same setting of Lemma D.6, the following conclusions hold:\n(a) The matrix AT2 DA T 1 is stable whenever D is stable.\n(b) The matrix AT2 DA T 1 is stable for all possible choices of left stochastic matrices A1 and A2 if, and only\nif, D is stable.\nProof. Since D is block diagonal, part (a) follows immediately from (605) by noting that ρ(D) < 1 whenever D is stable. [This statement fixes the argument that appeared in App. I of [18] and Lemma 2 of [33]. Since the matrix X in App. I of [18] and the matrix M in Lemma 2 of [33] are block diagonal, the ‖ · ‖b,∞ norm should replace the ‖ · ‖ρ norm used there, as in the proof that led to (606) and as already done in [54].] For part (b), assume first that D is stable, then AT2 DA T 1 will also be stable by part (a) for any left-stochastic matrices A1 and A2. To prove the converse, assume that AT2 DA T 1 is stable for any choice of left stochastic matrices A1 and A2. Then, A T 2 DA T 1 is stable for the particular choice A1 = I = A2 and it follows that D must be stable."
    }, {
      "heading" : "E Appendix: Comparison with Consensus Strategies",
      "text" : "Consider a connected network consisting of N nodes. Each node has a state or measurement value xk, possibly a vector of size M × 1. All nodes in the network are interested in evaluating the average value of their states, which we denote by\nwo ∆ =\n1\nN\nN∑\nk=1\nxk (608)\nA centralized solution to this problem would require each node to transmit its measurement xk to a fusion center. The central processor would then compute wo using (608) and transmit it back to all nodes. This centralized mode of operation suffers from at least two limitations. First, it requires communications and power resources to transmit the data back and forth between the nodes and the central processor; this problem is compounded if the fusion center is stationed at a remote location. Second, the architecture has a critical point of failure represented by the central processor; if it fails, then operations would need to be halted.\nConsensus Recursion The consensus strategy provides an elegant distributed solution to the same problem, whereby nodes interact locally with their neighbors and are able to converge to wo through these interactions. Thus, consider an\narbitrary node k and assign nonnegative weights {aℓk} to the edges linking k to its neighbors ℓ ∈ Nk. For each node k, the weights {aℓk} are assumed to add up to one so that\nfor k = 1, 2, . . . , N :\naℓk ≥ 0, N∑\nℓ=1\naℓk = 1, aℓk = 0 if ℓ /∈ Nk (609)\nThe resulting combination matrix is denoted by A and its k−th column consists of the entries {aℓk, ℓ = 1, 2, . . . , N}. In view of (609), the combination matrix A is seen to satisfy AT1 = 1. That is, A is leftstochastic. The consensus strategy can be described as follows. Each node k operates repeatedly on the data from its neighbors and updates its state iteratively according to the rule:\nwk,n = ∑\nℓ∈Nk\naℓk wℓ,n−1, n > 0 (610)\nwhere wℓ,n−1 denotes the state of node ℓ at iteration n − 1, and wk,n denotes the updated state of node k after iteration n. The initial conditions are\nwk,o = xk, k = 1, 2, . . . , N (611)\nIf we collect the states of all nodes at iteration n into a column vector, say,\nzn ∆ = col{w1,n, w2,n, . . . , wN,n} (612)\nThen, the consensus iteration (610) can be equivalently rewritten in vector form as follows:\nzn = A T zn−1, n > 0 (613)\nwhere AT = AT ⊗ IM (614)\nThe initial condition is\nzo ∆ = col{x1, x2, . . . , xN}\n(615)\nError Recursion\nNote that we can express the average value, wo, from (608) in the form\nwo = 1\nN · (1T ⊗ IM ) · zo (616)\nwhere 1 is the vector of size M × 1 and whose entries are all equal to one. Let\nw̃k,n = w o − wk,n (617)\ndenote the weight error vector for node k at iteration n; it measures how far the iterated state is from the desired average value wo. We collect all error vectors across the network into an N × 1 block column vector whose entries are of size M × 1 each:\nw̃n ∆ =   w̃1,n w̃2,n ...\nw̃N,n\n  (618)\nThen,\nw̃n = (1⊗ IM )w o − zn (619)\nConvergence Conditions The following result is a classical result on consensus strategies [42–44]. It provides conditions under which the state of all nodes will converge to the desired average, wo, so that w̃n will tend to zero.\nTheorem E.1. (Convergence to Consensus) For any initial states {xk}, the successive iterates wk,n generated by the consensus iteration (610) converge to the network average value wo as n → ∞ if, and only if, the following three conditions are met:\nAT1 = 1 (620)\nA1 = 1 (621)\nρ ( AT − 1\nN 11\nT ) < 1 (622)\nThat is, the combination matrix A needs to be doubly stochastic, and the matrix AT − 1N 11 T needs to be stable.\nProof. (Sufficiency). Assume first that the three conditions stated in the theorem hold. Since A is doubly stochastic, then so is any power of A, say, An for any n ≥ 0, so that\n[An]T 1 = 1, An1 = 1 (623)\nUsing this fact, it is straightforward to verify by induction the validity of the following equality:\n( AT − 1\nN 11\nT )n = [An]T − 1\nN 11\nT (624)\nLikewise, using the Kronecker product identities\n(E +B)⊗C = (E ⊗ C) + (B ⊗ C) (625)\n(E ⊗B)(C ⊗D) = (EC ⊗BD) (626)\n(E ⊗B)n = En ⊗Bn (627)\nfor matrices {E,B,C,D} of compatible dimensions, we observe that\n(An)T − 1\nN · (1⊗ IM ) · (1\nT ⊗ IM ) = [ (An)T ⊗ IM ] − 1\nN · (11T ⊗ IM )\n= [ (An)T − 1\nN · 11T\n] ⊗ IM\n(624) = ( AT − 1\nN 11\nT )n ⊗ IM\n= [( AT − 1\nN 11\nT ) ⊗ IM ]n (628)\nIterating (613) we find that zn = [A n]T zo (629)\nand, hence, from (616) and (619),\nw̃n = − [ (An)T − 1\nN · (1⊗ IM ) · (1\nT ⊗ IM ) ] · zo\n(628) = − [( AT − 1\nN 11\nT ) ⊗ IM ]n · zo (630)\nNow recall that, for two arbitrary matrices C and D of compatible dimensions, the eigenvalues of the Kronecker product C ⊗D is formed of all product combinations λi(C)λj(D) of the eigenvalues of C and D [19]. We conclude from this property, and from the fact that AT − 1\nN 11 T is stable, that the coefficient matrix ( AT − 1\nN · 11T\n) ⊗ IM\nis also stable. Therefore, w̃n → 0 as n → ∞ (631) (Necessity). In order for zn in (629) to converge to (1⊗ IM )w o, for any initial state zo, it must hold that\nlim n→∞\n(An)T · zo = 1\nN · (1⊗ IM ) · (1\nT ⊗ IM ) · zo (632)\nfor any zo. This implies that we must have\nlim n→∞\n(An)T = 1\nN · (11T ⊗ IM ) (633)\nor, equivalently,\nlim n→∞\n(An)T = 1\nN 11\nT (634)\nThis in turn implies that we must have\nlim n→∞\nAT · (An)T = AT · 1\nN 11\nT (635)\nBut since lim\nn→∞ AT · (An)T = lim n→∞\n( An+1 )T = lim\nn→∞ (An)T (636)\nwe conclude from (634) and (635) that it must hold that\n1 N 11 T = 1 N AT · 11T (637)\nThat is, 1\nN\n( AT1 − 1 ) · 1T = 0 (638)\nfrom which we conclude that we must have AT1 = 1. Similarly, we can show that A1 = 1 by studying the limit of (An)T AT . Therefore, A must be a doubly stochastic matrix. Now using the fact that A is doubly stochastic, we know that (624) holds. It follows that in order for condition (634) to be satisfied, we must have\nρ ( AT − 1\nN 11\nT ) < 1 (639)\nRate of Convergence From (630) we conclude that the rate of convergence of the error vectors {w̃k,n} to zero is determined by the spectrum of the matrix\nAT − 1\nN 11\nT (640)\nNow since A is a doubly stochastic matrix, we know that it has an eigenvalue at λ = 1. Let us denote the eigenvalues of A by λk(A) and let us order them in terms of their magnitudes as follows:\n0 ≤ |λM (A)| ≤ . . . ≤ |λ3(A)| ≤ |λ2(A)| ≤ 1 (641)\nwhere λ1(A) = 1. Then, the eigenvalues of the coefficient matrix (A T − 1N 11 T ) are equal to\n{ λM (A), , . . . , λ3(A), λ2(A), 0 } (642)\nIt follows that the magnitude of λ2(A) becomes the spectral radius of A T − 1N 11 T . Then condition (639) ensures that |λ2(A)| < 1. We therefore arrive at the following conclusion.\nCorollary E.1. (Rate of Convergence of Consensus) Under conditions (620)–(622), the rate of convergence of the successive iterates {wk,n} towards the network average wo in the consensus strategy (610) is determined by the second largest eigenvalue magnitude of A, i.e., by |λ2(A)| as defined in (641).\nIt is worth noting that doubly stochastic matrices A that are also regular satisfy conditions (620)–(622). This is because, as we already know from Lemma C.2, the eigenvalues of such matrices satisfy |λm(A)| < 1, for m = 2, 3, . . . , N , so that condition (622) is automatically satisfied.\nCorollary E.2. (Convergence for Regular Combination Matrices) Any doubly-stochastic and regular matrix A satisfies the three conditions (620)–(622) and, therefore, ensures the convergence of the consensus iterates {wk,n} generated by (610) towards w o as n → ∞.\nA regular combination matrix A would result when the two conditions listed below are satisfied by the graph connecting the nodes over which the consensus iteration is applied.\nCorollary E.3. (Sufficient Condition for Regularity) Assume the combination matrix A is doubly stochastic and that the graph over which the consensus iteration (610) is applied satisfies the following two conditions:\n(a) The graph is connected. This means that there exists a path connecting any two arbitrary nodes in the network. In terms of the Laplacian matrix that is associated with the graph (see Lemma B.1), this means that the second smallest eigenvalue of the Laplacian is nonzero.\n(b) aℓk = 0 if, and only if, ℓ /∈ Nk. That is, the combination weights are strictly positive between any two neighbors, including akk > 0.\nThen, the corresponding matrix A will be regular and, therefore, the consensus iterates {wk,n} generated by (610) will converge towards wo as n → ∞.\nProof. We first establish that conditions (a) and (b) imply that A is a regular matrix, namely, that there should exist an integer jo > 0 such that [\nAjo ]\nℓk > 0 (643)\nfor all (ℓ, k). To begin with, by the rules of matrix multiplication, the (ℓ, k) entry of the i−th power of A is given by:\n[ Ai ]\nℓk =\nN∑\nm1=1\nN∑\nm2=1\n. . . N∑\nmi−1=1\naℓm1am1m2 . . . ami−1k (644)\nThe summand in (644) is nonzero if, and only if, there is some sequence of indices (ℓ,m1, . . . ,mi−1, k) that forms a path from node ℓ to node k. Since the network is assumed to be connected, there exists a minimum (and finite) integer value iℓk such that a path exists from node ℓ to node k using iℓk edges and that\n[ Aiℓk ]\nℓk > 0\nIn addition, by induction, if [ Aiℓk ] ℓk > 0, then\n[ Aiℓk+1 ]\nℓk =\nN∑\nm=1\n[ Aiℓk ]\nℓm amk\n≥ [ Aiℓk ]\nℓk akk\n> 0\nLet jo = max\n1≤k,ℓ≤N {iℓk}\nThen, property (643) holds for all (ℓ, k). And we conclude from (581) that A is a regular matrix. It then follows from Corollary E.2 that the consensus iterates {wk,n} converge to the average network value w o.\nComparison with Diffusion Strategies Observe that in comparison to diffusion strategies, such as the ATC strategy (153), the consensus iteration (610) employs the same quantities wk,· on both sides of the iteration. In other words, the consensus construction keeps iterating on the same set of vectors until they converge to the average value wo. Moreover, the index n in the consensus algorithm is an iteration index. In contrast, diffusion strategies employ different quantities on both sides of the combination step in (153), namely, wk,i and {ψℓ,i}; the latter variables have been processed through an information exchange step and are updated (or filtered) versions of the wℓ,i−1. In addition, each step of the diffusion strategy (153) can incorporate new data, {dℓ(i), uℓ,i}, that are collected by the nodes at every time instant. Moreover, the index i in the diffusion implementation is a time index (and not an iteration index); this is because diffusion strategies are inherently adaptive and perform online learning. Data keeps streaming in and diffusion incorporates the new data into the update equations at every time instant. As a result, diffusion strategies are able to respond to data in an adaptive manner, and they are also able to solve general optimization problems: the vector wo in adaptive diffusion iterations is the minimizer of a global cost function (cf. (92)), while the vector wo in consensus iterations is the average value of the initial states of the nodes (cf. (608)).\nMoreover, it turns out that diffusion strategies influence the evolution of the network dynamics in an interesting and advantageous manner in comparison to consensus strategies. We illustrate this point by means of an example. Consider initially the ATC strategy (158) without information exchange, whose update equation we repeat below for ease of reference:\nψk,i = wk,i−1 + µku ∗ k,i [dk(i)− uk,iwk,i−1] (645) wk,i = ∑\nℓ∈Nk\naℓk ψℓ,i (ATC diffusion ) (646)\nThese recursions were derived in the body of the article as an effective distributed solution for optimizing (92)–(93). Note that they involve two steps, where the weight estimator wk,i−1 is first updated to the intermediate estimator ψk,i, before the intermediate estimators from across the neighborhood are combined to obtain wk,i. Both steps of ATC diffusion (645)–(646) can be combined into a single update as follows:\nwk,i = ∑\nℓ∈Nk\naℓk [ wℓ,i−1 + µℓu ∗ ℓ,i (dℓ(i)− uℓ,iwℓ,i−1) ] (ATC diffusion) (647)\nLikewise, consider the CTA strategy (159) without information exchange, whose update equation we also repeat below:\nψk,i−1 = ∑\nℓ∈Nk\naℓk wℓ,i−1 (CTA diffusion ) (648)\nwk,i = ψk,i−1 + µku ∗ k,i [ dk(i)− uk,iψk,i−1 ] (649)\nAgain, the CTA strategy involves two steps: the weight estimators {wℓ,i−1} from the neighborhood of node k are first combined to yield the intermediate estimator ψk,i−1, which is subsequently updated to wk,i. Both steps of CTA diffusion can also be combined into a single update as follows:\nwk,i = ∑\nℓ∈Nk\naℓk wℓ,i−1 + µku ∗ k,i [ dk(i)− uk,i ∑\nℓ∈Nk\naℓkwℓ,i−1\n] (CTA diffusion) (650)\nNow, motivated by the consensus iteration (610), and based on a procedure for distributed optimization suggested in [52] (see expression (7.1) in that reference), some works in the literature (e.g., [45, 53, 82–88]) considered distributed strategies that correspond to the following form for the optimization problem under consideration (see, e.g., expression (1.20) in [53] and expression (9) in [87]):\nwk,i = ∑\nℓ∈Nk\naℓk wℓ,i−1 + µku ∗ k,i [dk(i)− uk,iwk,i−1] (consensus strategy) (651)\nThis strategy can be derived by following the same argument we employed earlier in Secs. 3.2 and 4 to arrive at the diffusion strategies, namely, we replace wo in (127) by wℓ,i−1 and then apply the instantaneous approximations (150). Note that the same variable wk,· appears on both sides of the equality in (651). Thus, compared with the ATC diffusion strategy (647), the update from wk,i−1 to wk,i in the consensus implementation (651) is only influenced by data {dk(i),uk,i} from node k. In contrast, the ATC diffusion structure (645)–(646) helps incorporate the influence of the data {dℓ(i),uℓ,i} from across the neighborhood of node k into the update of wk,i, since these data are reflected in the intermediate estimators {ψℓ,i}. Likewise, the contrast with the CTA diffusion strategy (650) is clear, where the right-most term in (650) relies on a combination of all estimators from across the neighborhood of node k, and not only on wk,i−1 as in the consensus strategy (651). These facts have desirable implications on the evolution of the weight-error vectors across diffusion networks. Some simple algebra, similar to what we did in Sec. 6, will show that the mean of the extended error vector for the consensus strategy (651) evolves according to the recursion:\nE w̃i = ( AT −MRu ) · E w̃i−1, i ≥ 0 (consensus strategy) (652)\nwhere Ru is the block diagonal covariance matrix defined by (184) and w̃i is the aggregate error vector defined by (230). We can compare the above mean error dynamics with the ones that correspond to the ATC and CTA diffusion strategies (645)–(646) and (648)–(650); their error dynamics follow as special cases from (248) by setting A1 = I = C and A2 = A for ATC and A2 = I = C and A1 = A for CTA:\nE w̃i = A T (INM −MRu) · E w̃i−1, i ≥ 0 (ATC diffusion) (653)\nand\nE w̃i = (INM −MRu)A T · E w̃i−1, i ≥ 0 (CTA diffusion) (654)\nWe observe that the coefficient matrices that control the evolution of E w̃i are different in all three cases. In particular,\nconsensus strategy (652) is stable in the mean ⇐⇒ ρ ( AT −MRu ) < 1 (655)\nATC diffusion (653) is stable in the mean ⇐⇒ ρ [ AT (INM −MRu) ] < 1 (656) CTA diffusion (654) is stable in the mean ⇐⇒ ρ [ (INM −MRu)A T ] < 1 (657)\nIt follows that the mean stability of the consensus network is sensitive to the choice of the combination matrix A. This is not the case for the diffusion strategies. This is because from property (605) established in App. D, we know that the matrices AT (INM −MRu) and (INM −MRu)AT are stable if (INM −MRu) is stable. Therefore, we can select the step-sizes to satisfy µk < 2/λmax(Ru,k) for the ATC or CTA diffusion strategies and ensure their mean stability regardless of the combination matrix A. This also means that the diffusion networks will be mean stable whenever the individual nodes are mean stable, regardless of the topology defined by A. In contrast, for consensus networks, the network can exhibit unstable mean behavior\neven if all its individual nodes are stable in the mean. For further details and other results on the meansquare performance of diffusion networks in relation to consensus networks, the reader is referred to [89,90].\nAcknowledgement. The development of the theory and applications of diffusion adaptation over networks has benefited greatly from the insights and contributions of several UCLA Ph.D. students, and several visiting graduate students to the UCLA Adaptive Systems Laboratory (http://www.ee.ucla.edu/asl). The assistance and contributions of all students are hereby gratefully acknowledged, including Cassio G. Lopes, Federico S. Cattivelli, Sheng-Yuan Tu, Jianshu Chen, Xiaochuan Zhao, Zaid Towfic, Chung-Kai Yu, Noriyuki Takahashi, Jae-Woo Lee, Alexander Bertrand, and Paolo Di Lorenzo. The author is also particularly thankful to S.-Y. Tu, J. Chen, X. Zhao, Z. Towfic, and C.-K. Yu for their assistance in reviewing an earlier draft of this chapter."
    } ],
    "references" : [ {
      "title" : "On the limiting behavior of distributed optimization strategies",
      "author" : [ "J. Chen", "A.H. Sayed" ],
      "venue" : "Proc. 50th Annual Allerton Conference on Communication, Control, and Computing, pp. 1–8, Allerton, IL, October 2012.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Diffusion adaptation strategies for distributed optimization and learning over networks",
      "author" : [ "J. Chen", "A.H. Sayed" ],
      "venue" : "IEEE Trans. Signal Processing, vol. 60, no. 8, pp. 4289–4305, August 2012.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Distributed Pareto optimization via diffusion strategies",
      "author" : [ "J. Chen", "A.H. Sayed" ],
      "venue" : "IEEE J. Selected Topics in Signal Processing, vol. 7, no. 2, pp. 205–220, April 2013.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Adaptive Filter Theory",
      "author" : [ "S. Haykin" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "Mobile adaptive networks",
      "author" : [ "S.-Y. Tu", "A.H. Sayed" ],
      "venue" : "IEEE J. Sel. Topics. Signal Process., vol. 5, no. 4, pp. 649–664, Aug. 2011.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Modeling bird flight formations using diffusion adaptation",
      "author" : [ "F. Cattivelli", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 59, no. 5, pp. 2038–2051, May 2011.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Modeling bee swarming behavior through diffusion adaptation with asymmetric information sharing",
      "author" : [ "J. Li", "A.H. Sayed" ],
      "venue" : "EURASIP Journal on Advances in Signal Processing, 2012:18, doi:10.1186/1687-6180-2012-18, 2012.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Bio-inspired cooperative optimization with application to bacteria motility",
      "author" : [ "J. Chen", "A.H. Sayed" ],
      "venue" : "Proc. ICASSP, Prague, Czech Republic, pp. 5788–5791, May 2011.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Diffusion adaptation over networks of particles subject to Brownian fluctuations",
      "author" : [ "A.H. Sayed", "F.A. Sayed" ],
      "venue" : "Proc. Asilomar Conference on Signals, Systems, and Computers, pp. 685–690, Pacific Grove, CA, November 2011.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Cognitive radio: Making software radios more personal,”IEEE",
      "author" : [ "J. Mitola", "G.Q. Maguire" ],
      "venue" : "Personal Commun.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1999
    }, {
      "title" : "Cognitive radio: Brain-empowered wireless communications",
      "author" : [ "S. Haykin" ],
      "venue" : "IEEE J. Sel. Areas Commun., vol. 23, no. 2, pp. 201-220, Feb. 2005.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Optimal spectral feature detection for spectrum sensing at very low SNR",
      "author" : [ "Z. Quan", "W. Zhang", "S.J. Shellhammer", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Communications, vol. 59, no. 1, pp. 201-212, January 2011.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Cooperative sensing via sequential detection",
      "author" : [ "Q. Zou", "S. Zheng", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 58, no. 12, pp. 6266-6283, December 2010.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Bio-inspired swarming for dynamic radio access based on diffusion adaptation",
      "author" : [ "P. Di Lorenzo", "S. Barbarossa", "A.H. Sayed" ],
      "venue" : "Proc. EUSIPCO, pp. 402-406, Barcelona, Spain, August 2011.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Diffusion LMS strategies for distributed estimation",
      "author" : [ "F.S. Cattivelli", "A.H. Sayed" ],
      "venue" : "IEEE Trans. Signal Process., vol. 58, no. 3, pp. 1035–1048, March 2010.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Introductory Functional Analysis with Applications",
      "author" : [ "E. Kreyszig" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1989
    }, {
      "title" : "Introduction to Optimization, Optimization",
      "author" : [ "B. Poljak" ],
      "venue" : "Software, NY,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1987
    }, {
      "title" : "A new class of incremental gradient methods for least squares problems",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "SIAM J. Optim., vol. 7, no. 4, pp. 913–926, 1997.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Incremental subgradient methods for nondifferentiable optimization",
      "author" : [ "A. Nedic", "D.P. Bertsekas" ],
      "venue" : "SIAM J. Optim., vol. 12, no. 1, pp. 109–138, 2001.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Quantized incremental algorithms for distributed optimization",
      "author" : [ "M.G. Rabbat", "R.D. Nowak" ],
      "venue" : "IEEE J. Sel. Areas Commun., vol. 23, no. 4, pp. 798–808, 2005.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Incremental adaptive strategies over distributed networks",
      "author" : [ "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "IEEE Trans. Signal Process., vol. 55, no. 8, pp. 4064–4077, Aug. 2007.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Diffusion LMS algorithms with information exchange",
      "author" : [ "F.S. Cattivelli", "A.H. Sayed" ],
      "venue" : "Proc. Asilomar Conf. Signals, Syst. Comput., Pacific Grove, CA, pp. 251–255, Nov. 2008.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A diffusion RLS scheme for distributed estimation over adaptive networks",
      "author" : [ "F.S. Cattivelli", "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "Proc. IEEE Workshop on Signal Process. Advances Wireless Comm. (SPAWC), Helsinki, Finland, pp. 1–5, June 2007.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Diffusion recursive least-squares for distributed estimation over adaptive networks",
      "author" : [ "F.S. Cattivelli", "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "IEEE Trans. Signal Process., vol. 56, no. 5, pp. 1865–1877, May 2008.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1865
    }, {
      "title" : "Diffusion strategies for distributed Kalman filtering: Formulation and performance analysis",
      "author" : [ "F.S. Cattivelli", "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "Proc. IAPR Workshop on Cognitive Inf. Process.(CIP), Santorini, Greece, pp. 36–41, June 2008.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Diffusion mechanisms for fixed-point distributed Kalman smoothing",
      "author" : [ "F.S. Cattivelli", "A.H. Sayed" ],
      "venue" : "Proc. EUSIPCO, Lausanne, Switzerland, pp. 1–4, Aug. 2008.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Distributed adaptive learning mechanisms",
      "author" : [ "A.H. Sayed", "F. Cattivelli" ],
      "venue" : "Handbook on Array Processing and Sensor Networks, S. Haykin and K. J. Ray Liu, Eds., pp. 695–722, Wiley, NJ, 2009.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Diffusion strategies for distributed Kalman filtering and smoothing",
      "author" : [ "F. Cattivelli", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 55, no. 9, pp. 2069–2084, Sep. 2010.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Decentralized parameter estimation by consensus based stochastic approximation",
      "author" : [ "S.S. Stankovic", "M.S. Stankovic", "D.S. Stipanovic" ],
      "venue" : "IEEE Trans. on Autom. Control, vol. 56, no. 3, pp. 531–543, Mar. 2011.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Distributed processing over adaptive networks",
      "author" : [ "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "Proc. Adaptive Sensor Array Processing Workshop, MIT Lincoln Laboratory, MA, pp.1–5, June 2006.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Adaptive processing over distributed networks",
      "author" : [ "A.H. Sayed", "C.G. Lopes" ],
      "venue" : "IEICE Trans. Fund. of Electron., Commun. and Comput. Sci., vol. E90-A, no. 8, pp. 1504–1510, 2007.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Diffusion least-mean-squares over adaptive networks",
      "author" : [ "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "Proc. IEEE ICASSP, Honolulu, Hawaii, vol. 3, pp. 917-920, April 2007.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Steady-state performance of adaptive diffusion least-mean squares",
      "author" : [ "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "Proc. IEEE Workshop on Statistical Signal Processing (SSP), pp. 136-140, Madison, WI, August 2007.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis",
      "author" : [ "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "IEEE Trans. Signal Process., vol. 56, no. 7, pp. 3122–3136, July 2008.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Distributed stochastic subgradient projection algorithms for convex optimization",
      "author" : [ "S.S. Ram", "A. Nedic", "V.V. Veeravalli" ],
      "venue" : "J. Optim. Theory Appl., vol. 147, no. 3, pp. 516–545, 2010.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convergence of a distributed parameter estimator for sensor networks with local averaging of the estimates",
      "author" : [ "P. Bianchi", "G. Fort", "W. Hachem", "J. Jakubowicz" ],
      "venue" : "Proc. IEEE ICASSP, Prague, Czech, pp. 3764–3767, May 2011.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Reaching a consensus",
      "author" : [ "M.H. DeGroot" ],
      "venue" : "Journal of the American Statistical Association, vol. 69, no. 345, pp. 118–121, 1974.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "A necessary and sufficient condition for reaching a consensus using DeGroot’s method",
      "author" : [ "R.L. Berger" ],
      "venue" : "Journal of the American Statistical Association, vol. 76, no. 374, pp. 415-418, Jun. 1981. 113",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Convergence and asymptotic agreement in distributed decision problems",
      "author" : [ "J. Tsitsiklis", "M. Athans" ],
      "venue" : "IEEE Trans. Autom. Control, vol. 29, no. 1, pp. 42–50, Jan. 1984.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Coordination of groups of mobile autonomous agents using nearest neighbor rules",
      "author" : [ "A. Jadbabaie", "J. Lin", "A.S. Morse" ],
      "venue" : "IEEE Trans. Autom. Control, vol. 48, no. 6, pp. 988–1001, Jun. 2003.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Consensus problems in networks of agents with switching topology and time-delays",
      "author" : [ "R. Olfati-Saber", "R.M. Murray" ],
      "venue" : "IEEE Trans. Autom. Control, vol. 49, pp. 1520–1533, Sep. 2004.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Distributed Kalman filter with embedded consensus filters",
      "author" : [ "R. Olfati-Saber" ],
      "venue" : "Proc. 44th IEEE Conf. Decision Control, pp. 8179–8184, Sevilla, Spain, Dec. 2005.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Distributed Kalman filtering for sensor networks",
      "author" : [ "R. Olfati-Saber" ],
      "venue" : "Proc. 46th IEEE Conf. Decision Control, pp. 5492–5498, New Orleans, LA, Dec. 2007.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Fast linear iterations for distributed averaging",
      "author" : [ "L. Xiao", "S. Boyd" ],
      "venue" : "Syst. Control Lett., vol. 53, no. 1, pp. 65–78, Sep. 2004.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A scheme for robust distributed sensor fusion based on average consensus",
      "author" : [ "L. Xiao", "S. Boyd", "S. Lall" ],
      "venue" : "Proc. IPSN, 2005, pp. 63–70, Los Angeles, CA, April 2005.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Distributing the Kalman filter for large-scale systems",
      "author" : [ "U.A. Khan", "J.M.F. Moura" ],
      "venue" : "IEEE Trans. Signal Processing, vol. 56, no. 10, pp. 4919–4935, Oct. 2008.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Parallel and Distributed Computation: Numerical Methods, 1st edition",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Athena Scientific, Singapore,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 1997
    }, {
      "title" : "Cooperative distributed multi-agent optimization",
      "author" : [ "A. Nedic", "A. Ozdaglar" ],
      "venue" : "Convex Optimization in Signal Processing and Communications, Y. Eldar and D. Palomar (Eds.), Cambridge University Press, pp. 340-386, 2010.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Diffusion least-mean-squares with adaptive combiners: Formulation and performance analysis",
      "author" : [ "N. Takahashi", "I. Yamada", "A.H. Sayed" ],
      "venue" : "IEEE Trans. on Signal Processing, vol. 9, pp. 4795-4810, Sep. 2010.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Parallel algorithms for variational inequalities over the cartesian product of the intersections of the fixed point sets of nonexpansive mappings",
      "author" : [ "N. Takahashi", "I. Yamada" ],
      "venue" : "J. Approx. Theory, vol. 153, no. 2, pp. 139–160, Aug. 2008.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Transient analysis of data-normalized adaptive filters,”IEEE",
      "author" : [ "T.Y. Al-Naffouri", "A.H. Sayed" ],
      "venue" : "Transactions on Signal Processing,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2003
    }, {
      "title" : "Optimal combination rules for adaptation and learning over networks",
      "author" : [ "S-Y. Tu", "A.H. Sayed" ],
      "venue" : "Proc. IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), San Juan, Puerto Rico, pp. 317–320, December 2011.",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Diffusion LMS algorithms for sensor networks over non-ideal inter-sensor wireless channels",
      "author" : [ "R. Abdolee", "B. Champagne" ],
      "venue" : "Proc. IEEE Int. Conf. Dist. Comput. Sensor Systems (DCOSS), pp. 1–6, Barcelona, Spain, June 2011.",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Steady state analysis of diffusion LMS adaptive networks with noisy links",
      "author" : [ "A. Khalili", "M.A. Tinati", "A. Rastegarnia", "J.A. Chambers" ],
      "venue" : "IEEE Trans. Signal Processing, vol. 60, no. 2, pp. 974–979, Feb. 2012.",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Adaptive networks with noisy links",
      "author" : [ "S-Y. Tu", "A.H. Sayed" ],
      "venue" : "Proc. IEEE Globecom, pp. 1–5, Houston, TX, December 2011.",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Diffusion adaptation over networks under imperfect information exchange and non-stationary data",
      "author" : [ "X. Zhao", "S-Y. Tu", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 60, no. 7, pp. 3460–3475, July 2012.",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Combination weights for diffusion strategies with imperfect information exchange",
      "author" : [ "X. Zhao", "A.H. Sayed" ],
      "venue" : "Proc. IEEE ICC, pp. 1–5, Ottawa, Canada, June 2012.",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Graphs, Algorithms and Optimization",
      "author" : [ "W. Kocay", "D.L. Kreher" ],
      "venue" : null,
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2005
    }, {
      "title" : "Algebraic connectivity of graphs",
      "author" : [ "M. Fiedler" ],
      "venue" : "Czech. Math. J., vol. 23, pp. 298–305, 1973. 114",
      "citeRegEx" : "67",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Convergence in multiagent coordination, consensus, and flocking",
      "author" : [ "V.D. Blondel", "J.M. Hendrickx", "A. Olshevsky", "J.N. Tsitsiklis" ],
      "venue" : "Proc. Joint 44th IEEE Conf. on Decision and Control and European Control Conf. (CDC-ECC), pp. 2996-3000, Seville, Spain, Dec. 2005.",
      "citeRegEx" : "68",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Locally constructed algorithms for distributed computations in ad-hoc networks",
      "author" : [ "D.S. Scherber", "H.C. Papadopoulos" ],
      "venue" : "Proc. Information Processing in Sensor Networks (IPSN), pp. 11-19, Berkeley, CA, April 2004.",
      "citeRegEx" : "69",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Equations of state calculations by fast computing machines",
      "author" : [ "N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller" ],
      "venue" : "Journal of Chemical Physics, vol. 21, no. 6, pp. 1087-1092, 1953.",
      "citeRegEx" : "70",
      "shortCiteRegEx" : null,
      "year" : 1953
    }, {
      "title" : "Monte Carlo sampling methods using Markov chains and their applications",
      "author" : [ "W.K. Hastings" ],
      "venue" : "Biometrika, vol. 57, no. 1, pp. 97-109, 1970.",
      "citeRegEx" : "71",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Distributed recursive least-squares strategies over adaptive networks",
      "author" : [ "A.H. Sayed", "C. Lopes" ],
      "venue" : "Proc. 40th Asilomar Conference on Signals, Systems and Computers, Pacific Grove, CA, pp. 233-237, October-November, 2006.",
      "citeRegEx" : "72",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Spatio-temporal diffusion mechanisms for adaptation over networks",
      "author" : [ "J-W. Lee", "S-E. Kim", "W-J. Song", "A.H. Sayed" ],
      "venue" : "Proc. EUSIPCO, pp. 1040-1044, Barcelona, Spain, August-September 2011.",
      "citeRegEx" : "73",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Spatio-temporal diffusion strategies for estimation and detection over networks",
      "author" : [ "J-W. Lee", "S-E. Kim", "W-J. Song", "A.H. Sayed" ],
      "venue" : "IEEE Trans. Signal Processing, vol. 60, no. 8, pp. 4017–4034, August 2012.",
      "citeRegEx" : "74",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Adaptive robust distributed learning in diffusion sensor networks",
      "author" : [ "S. Chouvardas", "K. Slavakis", "S. Theodoridis" ],
      "venue" : "IEEE Trans. on Signal Processing, vol. 59, no. 10, pp. 4692–4707, Oct. 2011.",
      "citeRegEx" : "75",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Adaptive algorithm for sparse system identification using projections onto weighted l1 balls",
      "author" : [ "K. Slavakis", "Y. Kopsinis", "S. Theodoridis" ],
      "venue" : "Proc. IEEE ICASSP, pp. 3742–3745, Dallas, TX, March 2010.",
      "citeRegEx" : "76",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Diffusion distributed Kalman filtering with adaptive weights",
      "author" : [ "F. Cattivelli", "A.H. Sayed" ],
      "venue" : "Proc. Asilomar Conference on Signals, Systems and Computers, pp. 908–912, Pacific Grove, CA, November 2009.",
      "citeRegEx" : "78",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A space-time diffusion scheme peer-to-peer least-squares-estimation",
      "author" : [ "L. Xiao", "S. Boyd", "S. Lall" ],
      "venue" : "Proc. Information Processing in Sensor Networks (IPSN), pp. 168–176, Nashville, TN, April 2006.",
      "citeRegEx" : "79",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Distributed subgradient methods for multi-agent optimization",
      "author" : [ "A. Nedic", "A. Ozdaglar" ],
      "venue" : "IEEE Trans. Autom. Control, vol. 54, no. 1, pp. 48–61, Jan. 2009.",
      "citeRegEx" : "80",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Gradient convergence in gradient methods with errors",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "SIAM J. Optim., vol. 10, no. 3, pp. 627–642, 2000.",
      "citeRegEx" : "81",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Bio-inspired sensor network design",
      "author" : [ "S. Barbarossa", "G. Scutari" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 24, no. 3, pp. 26–35, May 2007.",
      "citeRegEx" : "82",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Kalman-consensus filter: Optimality, stability, and performance",
      "author" : [ "R. Olfati-Saber" ],
      "venue" : "Proc. IEEE CDC, pp. 7036– 7042, Shangai, China, 2009.",
      "citeRegEx" : "83",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Distributed LMS for consensus-based in-network adaptive processing",
      "author" : [ "I.D. Schizas", "G. Mateos", "G.B. Giannakis" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 57, no. 6, pp. 2365–2382, June 2009.",
      "citeRegEx" : "84",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Performance analysis of the consensus-based distributed LMS algorithm",
      "author" : [ "G. Mateos", "I.D. Schizas", "G.B. Giannakis" ],
      "venue" : "EURASIP J. Adv. Signal Process., pp. 1–19, 2009.",
      "citeRegEx" : "85",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Distributed consensus algorithms in sensor networks: Link failures and channel noise",
      "author" : [ "S. Kar", "J.M.F. Moura" ],
      "venue" : "IEEE Trans. Signal Process., vol. 57, no. 1, pp. 355–369, Jan. 2009.",
      "citeRegEx" : "86",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Convergence rate analysis of distributed gossip (linear parameter) estimation: Fundamental limits and tradeoffs",
      "author" : [ "S. Kar", "J.M.F. Moura" ],
      "venue" : "IEEE Journal on Selected Topics on Signal Processing, vol. 5, no. 4, pp. 674– 690, August 2011.",
      "citeRegEx" : "87",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Gossip algorithms for distributed signal processing",
      "author" : [ "A.G. Dimakis", "S. Kar", "J.M.F. Moura", "M.G. Rabbat", "A. Scaglione" ],
      "venue" : "Proc. IEEE, vol. 98, no. 11, pp. 1847–1864, November 2010.",
      "citeRegEx" : "88",
      "shortCiteRegEx" : null,
      "year" : 1847
    }, {
      "title" : "Diffusion networks outperform consensus networks",
      "author" : [ "S-Y. Tu", "A.H. Sayed" ],
      "venue" : "Proc. IEEE Statistical Signal Processing Workshop, pp. 313–316, Ann Arbor, Michigan, August 2012.",
      "citeRegEx" : "89",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Diffusion strategies outperform consensus strategies for distributed estimation over adaptive networks",
      "author" : [ "S.-Y. Tu", "A.H. Sayed" ],
      "venue" : "IEEE Trans. Signal Processing, vol. 60, no. 12, pp. 6217–6234, Dec. 2012. 115",
      "citeRegEx" : "90",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "At the same time, we note that most of the arguments in this article can be extended beyond MSE optimization to more general cost functions and to situations where the minimizers of the individual costs Jk(w) need not agree with each other — as already shown in [1–3]; see also Sec.",
      "startOffset" : 262,
      "endOffset" : 267
    }, {
      "referenceID" : 1,
      "context" : "At the same time, we note that most of the arguments in this article can be extended beyond MSE optimization to more general cost functions and to situations where the minimizers of the individual costs Jk(w) need not agree with each other — as already shown in [1–3]; see also Sec.",
      "startOffset" : 262,
      "endOffset" : 267
    }, {
      "referenceID" : 2,
      "context" : "At the same time, we note that most of the arguments in this article can be extended beyond MSE optimization to more general cost functions and to situations where the minimizers of the individual costs Jk(w) need not agree with each other — as already shown in [1–3]; see also Sec.",
      "startOffset" : 262,
      "endOffset" : 267
    }, {
      "referenceID" : 3,
      "context" : "There are many adaptive algorithms that can be used to compute wk,i; some filters are more accurate than others (usually, at the cost of additional complexity) [4–7].",
      "startOffset" : 160,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "The performance of adaptive implementations of this kind are well-understood for both cases of stationary w and changing w [4–7].",
      "startOffset" : 123,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "It is known that the MSD and EMSE of LMS filters of the form (26)–(27) can be approximated for sufficiently small-step sizes by the following expressions [4–7]: EMSEk ≈ μkσ 2 v,kTr(Ru,k)/2 (36) MSDk ≈ μkσ 2 v,kM/2 (37)",
      "startOffset" : 154,
      "endOffset" : 159
    }, {
      "referenceID" : 4,
      "context" : "In several such localization applications, the agents in the network are allowed to move towards the target or away from it, in which case we would end up with a mobile adaptive network [8].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 4,
      "context" : "Biological networks behave in this manner such as networks representing fish schools, bird formations, bee swarms, bacteria motility, and diffusing particles [8–12].",
      "startOffset" : 158,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "Biological networks behave in this manner such as networks representing fish schools, bird formations, bee swarms, bacteria motility, and diffusing particles [8–12].",
      "startOffset" : 158,
      "endOffset" : 164
    }, {
      "referenceID" : 6,
      "context" : "Biological networks behave in this manner such as networks representing fish schools, bird formations, bee swarms, bacteria motility, and diffusing particles [8–12].",
      "startOffset" : 158,
      "endOffset" : 164
    }, {
      "referenceID" : 7,
      "context" : "Biological networks behave in this manner such as networks representing fish schools, bird formations, bee swarms, bacteria motility, and diffusing particles [8–12].",
      "startOffset" : 158,
      "endOffset" : 164
    }, {
      "referenceID" : 8,
      "context" : "Biological networks behave in this manner such as networks representing fish schools, bird formations, bee swarms, bacteria motility, and diffusing particles [8–12].",
      "startOffset" : 158,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "To avoid causing harmful interference to incumbent primary users, unlicensed cognitive radio devices need to detect unused frequency bands even at low signal-to-noise (SNR) conditions [13–16].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 10,
      "context" : "To avoid causing harmful interference to incumbent primary users, unlicensed cognitive radio devices need to detect unused frequency bands even at low signal-to-noise (SNR) conditions [13–16].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 11,
      "context" : "To avoid causing harmful interference to incumbent primary users, unlicensed cognitive radio devices need to detect unused frequency bands even at low signal-to-noise (SNR) conditions [13–16].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 12,
      "context" : "To avoid causing harmful interference to incumbent primary users, unlicensed cognitive radio devices need to detect unused frequency bands even at low signal-to-noise (SNR) conditions [13–16].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 13,
      "context" : "To facilitate estimation of the spectral profile by the secondary users, we assume that each Sq(e ) can be represented as a linear combination of basis functions, {fm(e)}, say, B of them [17]: Sq(e ) = B ∑",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "It is sufficient in this article to illustrate the main concepts underlying diffusion adaptation by focusing on the useful case of MSE cost functions of the form (97); still, most of the derivations and arguments in the coming sections can be extended beyond MSE optimization to more general cost functions — as already shown in [1–3]; see also Sec.",
      "startOffset" : 329,
      "endOffset" : 334
    }, {
      "referenceID" : 1,
      "context" : "It is sufficient in this article to illustrate the main concepts underlying diffusion adaptation by focusing on the useful case of MSE cost functions of the form (97); still, most of the derivations and arguments in the coming sections can be extended beyond MSE optimization to more general cost functions — as already shown in [1–3]; see also Sec.",
      "startOffset" : 329,
      "endOffset" : 334
    }, {
      "referenceID" : 2,
      "context" : "It is sufficient in this article to illustrate the main concepts underlying diffusion adaptation by focusing on the useful case of MSE cost functions of the form (97); still, most of the derivations and arguments in the coming sections can be extended beyond MSE optimization to more general cost functions — as already shown in [1–3]; see also Sec.",
      "startOffset" : 329,
      "endOffset" : 334
    }, {
      "referenceID" : 0,
      "context" : "Our strategy to optimize J (w) in a distributed manner is based on two steps, following the developments in [1, 2, 18].",
      "startOffset" : 108,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "Our strategy to optimize J (w) in a distributed manner is based on two steps, following the developments in [1, 2, 18].",
      "startOffset" : 108,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "Our strategy to optimize J (w) in a distributed manner is based on two steps, following the developments in [1, 2, 18].",
      "startOffset" : 108,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "The above substitution amounts to having each node k approximate the {Rl} from its neighbors by multiples of the identity matrix Rl ≈ blk IM (118) Approximation (117) is reasonable in view of the fact that all vector norms are equivalent [19–21]; this norm property ensures that we can bound the weighted norm ‖w − wo‖2Rl by some constants multiplying the un-weighted norm ‖w − w‖, say, as: r1‖w − w ‖ ≤ ‖w − wo‖2Rl ≤ r2‖w − w ‖ (119) for some positive constants (r1, r2).",
      "startOffset" : 238,
      "endOffset" : 245
    }, {
      "referenceID" : 16,
      "context" : "Using the fact that the {Rl} are Hermitian positive-definite matrices, and calling upon the Rayleigh-Ritz characterization of eigenvalues [19, 20], we can be more specific and replace the above inequalities by λmin(Rl) · ‖w − w ‖ ≤ ‖w − wo‖2Rl ≤ λmax(Rl) · ‖w − w ‖ (120) We note that approximations similar to (118) are common in stochastic approximation theory and they mark the difference between using a Newton’s iterative method or a stochastic gradient method [5, 22]; the former uses Hessian matrices as approximations for Rl and the latter uses multiples of the identity matrix.",
      "startOffset" : 466,
      "endOffset" : 473
    }, {
      "referenceID" : 16,
      "context" : "One choice that is common in the optimization literature [5,22,52] is to replace μk in (123) by step-size sequences {μ(i) ≥ 0} that satisfy the two conditions (25).",
      "startOffset" : 57,
      "endOffset" : 66
    }, {
      "referenceID" : 46,
      "context" : "One choice that is common in the optimization literature [5,22,52] is to replace μk in (123) by step-size sequences {μ(i) ≥ 0} that satisfy the two conditions (25).",
      "startOffset" : 57,
      "endOffset" : 66
    }, {
      "referenceID" : 17,
      "context" : "This step is reminiscent of incremental-type approaches to optimization, which have been widely studied in the literature [23–26].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : "This step is reminiscent of incremental-type approaches to optimization, which have been widely studied in the literature [23–26].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : "This step is reminiscent of incremental-type approaches to optimization, which have been widely studied in the literature [23–26].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 20,
      "context" : "This step is reminiscent of incremental-type approaches to optimization, which have been widely studied in the literature [23–26].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "The significance of this general form is that it is applicable to optimization problems involving more general local costs Jl(w) that are not necessarily quadratic in w, as detailed in [1–3] — see also Sec.",
      "startOffset" : 185,
      "endOffset" : 190
    }, {
      "referenceID" : 1,
      "context" : "The significance of this general form is that it is applicable to optimization problems involving more general local costs Jl(w) that are not necessarily quadratic in w, as detailed in [1–3] — see also Sec.",
      "startOffset" : 185,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : "The significance of this general form is that it is applicable to optimization problems involving more general local costs Jl(w) that are not necessarily quadratic in w, as detailed in [1–3] — see also Sec.",
      "startOffset" : 185,
      "endOffset" : 190
    }, {
      "referenceID" : 14,
      "context" : "The derivation of the ATC and CTA strategies (134) and (142) followed the approach proposed in [18,27].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "The derivation of the ATC and CTA strategies (134) and (142) followed the approach proposed in [18,27].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 29,
      "context" : "CTA estimation schemes were first proposed in the works [35–39], and later extended in [18,27,32,33].",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 30,
      "context" : "CTA estimation schemes were first proposed in the works [35–39], and later extended in [18,27,32,33].",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 31,
      "context" : "CTA estimation schemes were first proposed in the works [35–39], and later extended in [18,27,32,33].",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 32,
      "context" : "CTA estimation schemes were first proposed in the works [35–39], and later extended in [18,27,32,33].",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 33,
      "context" : "CTA estimation schemes were first proposed in the works [35–39], and later extended in [18,27,32,33].",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "CTA estimation schemes were first proposed in the works [35–39], and later extended in [18,27,32,33].",
      "startOffset" : 87,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "CTA estimation schemes were first proposed in the works [35–39], and later extended in [18,27,32,33].",
      "startOffset" : 87,
      "endOffset" : 100
    }, {
      "referenceID" : 26,
      "context" : "CTA estimation schemes were first proposed in the works [35–39], and later extended in [18,27,32,33].",
      "startOffset" : 87,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : "CTA estimation schemes were first proposed in the works [35–39], and later extended in [18,27,32,33].",
      "startOffset" : 87,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : "The earlier versions of CTA in [35–37] used the choice C = I.",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 30,
      "context" : "The earlier versions of CTA in [35–37] used the choice C = I.",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 31,
      "context" : "The earlier versions of CTA in [35–37] used the choice C = I.",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 34,
      "context" : "This form of the algorithm with C = I, and with the additional constraint that the step-sizes μk should be time-dependent and decay towards zero as time progresses, was later applied by [40,41] to solve distributed optimization problems that require all nodes to reach consensus or agreement.",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 35,
      "context" : "This form of the algorithm with C = I, and with the additional constraint that the step-sizes μk should be time-dependent and decay towards zero as time progresses, was later applied by [40,41] to solve distributed optimization problems that require all nodes to reach consensus or agreement.",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 22,
      "context" : "Likewise, special cases of the ATC estimation scheme (134), involving an information exchange step followed by an aggregation step, first appeared in the work [28] on diffusion leastsquares schemes and subsequently in the works [18,29–33] on distributed mean-square-error and state-space estimation methods.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 14,
      "context" : "Likewise, special cases of the ATC estimation scheme (134), involving an information exchange step followed by an aggregation step, first appeared in the work [28] on diffusion leastsquares schemes and subsequently in the works [18,29–33] on distributed mean-square-error and state-space estimation methods.",
      "startOffset" : 228,
      "endOffset" : 238
    }, {
      "referenceID" : 23,
      "context" : "Likewise, special cases of the ATC estimation scheme (134), involving an information exchange step followed by an aggregation step, first appeared in the work [28] on diffusion leastsquares schemes and subsequently in the works [18,29–33] on distributed mean-square-error and state-space estimation methods.",
      "startOffset" : 228,
      "endOffset" : 238
    }, {
      "referenceID" : 24,
      "context" : "Likewise, special cases of the ATC estimation scheme (134), involving an information exchange step followed by an aggregation step, first appeared in the work [28] on diffusion leastsquares schemes and subsequently in the works [18,29–33] on distributed mean-square-error and state-space estimation methods.",
      "startOffset" : 228,
      "endOffset" : 238
    }, {
      "referenceID" : 25,
      "context" : "Likewise, special cases of the ATC estimation scheme (134), involving an information exchange step followed by an aggregation step, first appeared in the work [28] on diffusion leastsquares schemes and subsequently in the works [18,29–33] on distributed mean-square-error and state-space estimation methods.",
      "startOffset" : 228,
      "endOffset" : 238
    }, {
      "referenceID" : 26,
      "context" : "Likewise, special cases of the ATC estimation scheme (134), involving an information exchange step followed by an aggregation step, first appeared in the work [28] on diffusion leastsquares schemes and subsequently in the works [18,29–33] on distributed mean-square-error and state-space estimation methods.",
      "startOffset" : 228,
      "endOffset" : 238
    }, {
      "referenceID" : 27,
      "context" : "Likewise, special cases of the ATC estimation scheme (134), involving an information exchange step followed by an aggregation step, first appeared in the work [28] on diffusion leastsquares schemes and subsequently in the works [18,29–33] on distributed mean-square-error and state-space estimation methods.",
      "startOffset" : 228,
      "endOffset" : 238
    }, {
      "referenceID" : 28,
      "context" : "A special case of the ATC strategy (134) corresponding to the choice C = I with decaying step-sizes was adopted in [34] to ensure convergence towards a consensus state.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 34,
      "context" : "E and [40, 42–44].",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 36,
      "context" : "E and [40, 42–44].",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 37,
      "context" : "E and [40, 42–44].",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 38,
      "context" : "E and [40, 42–44].",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 34,
      "context" : "E and [40, 45–51]).",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 39,
      "context" : "E and [40, 45–51]).",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 40,
      "context" : "E and [40, 45–51]).",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 41,
      "context" : "E and [40, 45–51]).",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 42,
      "context" : "E and [40, 45–51]).",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 43,
      "context" : "E and [40, 45–51]).",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 44,
      "context" : "E and [40, 45–51]).",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 45,
      "context" : "E and [40, 45–51]).",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : ", [8–10]).",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : ", [8–10]).",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 6,
      "context" : ", [8–10]).",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 16,
      "context" : ", [22, 40, 52, 53]).",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 34,
      "context" : ", [22, 40, 52, 53]).",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 46,
      "context" : ", [22, 40, 52, 53]).",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 47,
      "context" : ", [22, 40, 52, 53]).",
      "startOffset" : 2,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Sections 5 and 6 highlight the convergence properties of the diffusion strategies — see also [1–3] for results pertaining to more general cost functions.",
      "startOffset" : 93,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "Sections 5 and 6 highlight the convergence properties of the diffusion strategies — see also [1–3] for results pertaining to more general cost functions.",
      "startOffset" : 93,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "Sections 5 and 6 highlight the convergence properties of the diffusion strategies — see also [1–3] for results pertaining to more general cost functions.",
      "startOffset" : 93,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "In this way, diffusion strategies allow multiple layers of adaptation: the nodes perform adaptive processing, the combination weights can be adapted, and even the topology can be adapted especially for mobile networks [8].",
      "startOffset" : 218,
      "endOffset" : 221
    }, {
      "referenceID" : 3,
      "context" : "Recall that, by definition, Ru,l ∆ = Eul,iul,i, rdu,l ∆ = Edl(i)u ∗ l,i (149) One common stochastic approximation method is to drop the expectation operator from the definitions of {Ru,l, rdu,l} and to use the following instantaneous approximations instead [4–7]: Ru,l ≈ u ∗ l,iul,i, rdu,l ≈ dl(i)u ∗ l,i (150) In this case, the approximate gradient vectors become: (rdu,l −Ru,l wk,i−1) ≈ u ∗ l,i [dl(i)− ul,i wk,i−1] (151) (rdu,l −Ru,l ψk,i−1) ≈ u ∗ l,i [dl(i)− ul,i ψk,i−1] (152) Substituting into the ATC and CTA steepest-descent strategies (134) and (142), we arrive at the following adaptive implementations of the diffusion strategies for i ≥ 0:",
      "startOffset" : 257,
      "endOffset" : 262
    }, {
      "referenceID" : 1,
      "context" : "The significance of the alternative forms (156)–(157) is that they are applicable to optimization problems involving more general local costs Jl(w) that are not necessarily quadratic, as detailed in [2, 3]; see also Sec.",
      "startOffset" : 199,
      "endOffset" : 205
    }, {
      "referenceID" : 2,
      "context" : "The significance of the alternative forms (156)–(157) is that they are applicable to optimization problems involving more general local costs Jl(w) that are not necessarily quadratic, as detailed in [2, 3]; see also Sec.",
      "startOffset" : 199,
      "endOffset" : 205
    }, {
      "referenceID" : 3,
      "context" : "Furthermore, the choice A1 = A2 = C = IN corresponds to the non-cooperative mode of operation, in which case the recursions reduce to the classical (stand-alone) steepest-descent recursion [4–7], where each node minimizes individually its own quadratic cost Jk(w), defined earlier in (97): wk,i = wk,i−1 + μk [rdu,k −Ru,k wk,i−1] , i ≥ 0 (171)",
      "startOffset" : 189,
      "endOffset" : 194
    }, {
      "referenceID" : 3,
      "context" : "Furthermore, the choice A1 = A2 = C = IN corresponds to the non-cooperative mode of operation, where each node runs the classical (stand-alone) least-mean-squares (LMS) filter independently of the other nodes: [4–7]: wk,i = wk,i−1 + μkuk,i [dk(i)− uk,i wk,i−1] , i ≥ 0 (207)",
      "startOffset" : 210,
      "endOffset" : 215
    }, {
      "referenceID" : 50,
      "context" : "The approach we follow is based on the energy conservation framework of [4, 5, 57].",
      "startOffset" : 72,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : "While we can continue with the analysis by taking this factor into account, as was done in [4, 5, 18, 57], it is sufficient for the exposition in this article to focus on the case of sufficiently small step-sizes where terms involving higher powers of the step-sizes can be ignored.",
      "startOffset" : 91,
      "endOffset" : 105
    }, {
      "referenceID" : 50,
      "context" : "While we can continue with the analysis by taking this factor into account, as was done in [4, 5, 18, 57], it is sufficient for the exposition in this article to focus on the case of sufficiently small step-sizes where terms involving higher powers of the step-sizes can be ignored.",
      "startOffset" : 91,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "The relation can be transformed into a true recursion by expanding it into a convenient state-space model; this argument was pursued in [4, 5, 18, 57] and is not necessary for the exposition here, except to say that stability of the matrix F ensures the mean-square stability of the filter — this fact is also established further ahead through relation (327).",
      "startOffset" : 136,
      "endOffset" : 150
    }, {
      "referenceID" : 50,
      "context" : "The relation can be transformed into a true recursion by expanding it into a convenient state-space model; this argument was pursued in [4, 5, 18, 57] and is not necessary for the exposition here, except to say that stability of the matrix F ensures the mean-square stability of the filter — this fact is also established further ahead through relation (327).",
      "startOffset" : 136,
      "endOffset" : 150
    }, {
      "referenceID" : 14,
      "context" : "Then, expressions (307) and (309) give: Bcta = (I −MR)A T , Ycta = MC SCM (335) Batc = A T (I −MR), Yatc = A MCSCMA (336) where A = A⊗ IM (337) Following [18], introduce the auxiliary nonnegative-definite matrix Hj ∆ = [ (I −MR)A ]j ·MCSCM · [ (I −MR)A ]∗j (338)",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 57,
      "context" : "B, namely, it is a symmetric matrix whose entries are constructed as follows [64–66]:",
      "startOffset" : 77,
      "endOffset" : 84
    }, {
      "referenceID" : 59,
      "context" : "Averaging rule [68]:",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 43,
      "context" : "Laplacian rule [49,69]: A = IN − γL, γ > 0 symmetric and doubly-stochastic",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 60,
      "context" : "Laplacian rule [49,69]: A = IN − γL, γ > 0 symmetric and doubly-stochastic",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 44,
      "context" : "Laplacian rule using γ = 1/N(maximum-degree rule [50]) :",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 43,
      "context" : "Metropolis rule [49,70,71]:",
      "startOffset" : 16,
      "endOffset" : 26
    }, {
      "referenceID" : 61,
      "context" : "Metropolis rule [49,70,71]:",
      "startOffset" : 16,
      "endOffset" : 26
    }, {
      "referenceID" : 62,
      "context" : "Metropolis rule [49,70,71]:",
      "startOffset" : 16,
      "endOffset" : 26
    }, {
      "referenceID" : 23,
      "context" : "Relative-degree rule [29]:",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 48,
      "context" : "While such selections may be appropriate in some applications, they can nevertheless degrade the performance of adaptation over networks [54].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 51,
      "context" : "For this reason, following [58, 62], we describe in the next subsection one adaptive procedure for adjusting the combination weights.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 55,
      "context" : "For this reason, following [58, 62], we describe in the next subsection one adaptive procedure for adjusting the combination weights.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 14,
      "context" : "In [18], the selection of the combination weights was formulated as the following optimization problem:",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "We can pursue a numerical solution to (374) in order to search for optimal combination matrices, as was done in [18].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 51,
      "context" : "We illustrate an approximate approach from [58,62] that leads to one adaptive solution that performs reasonably well in practice.",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 55,
      "context" : "We illustrate an approximate approach from [58,62] that leads to one adaptive solution that performs reasonably well in practice.",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 51,
      "context" : "We refer to this combination rule as the relative-variance combination rule [58]; it leads to a left-stochastic matrix A.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "In comparison, the following relativedegree-variance rule was proposed in [18] (a typo appears in Table III in [18], where the noise variances appear written in the table instead of their inverses):",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "In comparison, the following relativedegree-variance rule was proposed in [18] (a typo appears in Table III in [18], where the noise variances appear written in the table instead of their inverses):",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 52,
      "context" : "We may note that some studies on the effect of imperfect data exchanges on the performance of adaptive diffusion algorithms are considered in [59–61].",
      "startOffset" : 142,
      "endOffset" : 149
    }, {
      "referenceID" : 53,
      "context" : "We may note that some studies on the effect of imperfect data exchanges on the performance of adaptive diffusion algorithms are considered in [59–61].",
      "startOffset" : 142,
      "endOffset" : 149
    }, {
      "referenceID" : 54,
      "context" : "We may note that some studies on the effect of imperfect data exchanges on the performance of adaptive diffusion algorithms are considered in [59–61].",
      "startOffset" : 142,
      "endOffset" : 149
    }, {
      "referenceID" : 55,
      "context" : "Although we can continue our analysis by studying this general case in which the vectors zi do not have zero-mean (see [62,63]), we shall nevertheless limit our discussion in the sequel to the case in which there is no noise during the exchange of the regression data, i.",
      "startOffset" : 119,
      "endOffset" : 126
    }, {
      "referenceID" : 56,
      "context" : "Although we can continue our analysis by studying this general case in which the vectors zi do not have zero-mean (see [62,63]), we shall nevertheless limit our discussion in the sequel to the case in which there is no noise during the exchange of the regression data, i.",
      "startOffset" : 119,
      "endOffset" : 126
    }, {
      "referenceID" : 55,
      "context" : ", the presence of noise during the exchange of regression data alters the dynamics of the mean error vector in an important way — see [62, 63] for details on how to extend the arguments to this general case with a driving non-zero bias term.",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 56,
      "context" : ", the presence of noise during the exchange of regression data alters the dynamics of the mean error vector in an important way — see [62, 63] for details on how to extend the arguments to this general case with a driving non-zero bias term.",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 51,
      "context" : "We continue to refer to this combination rule as the relative-variance combination rule [58]; it leads to a left-stochastic matrix A.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 64,
      "context" : "3 and 4 to (488) and arrive at the following version of a diffusion strategy incorporating temporal processing (or smoothing) of the intermediate weight estimates [73, 74]:",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 65,
      "context" : "3 and 4 to (488) and arrive at the following version of a diffusion strategy incorporating temporal processing (or smoothing) of the intermediate weight estimates [73, 74]:",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 64,
      "context" : "This step is carried out in [73, 74] for doubly stochastic combination matrices A",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 65,
      "context" : "This step is carried out in [73, 74] for doubly stochastic combination matrices A",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 65,
      "context" : "For instance, it is shown in [74] that whether temporal processing is performed before or after adaptation, the strategy that performs adaptation before spatial cooperation is always better.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 66,
      "context" : "In related work, reference [75] started from the CTA algorithm (159) without information exchange and added a useful projection step to it between the combination step and the adaptation step; i.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 66,
      "context" : "from [75] has the following form: φk,i−1 = ∑ l∈Nk alk wl,i−1 (500)",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 67,
      "context" : "For generic values {d, u, ǫ}, where d is a scalar and u is a row vector, the projection operator is described analytically by the following expression [76]:",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 22,
      "context" : "2 Diffusion Recursive Least-Squares Diffusion strategies can also be applied to recursive least-squares problems to enable distributed solutions of least-squares designs [28,29]; see also [72].",
      "startOffset" : 170,
      "endOffset" : 177
    }, {
      "referenceID" : 23,
      "context" : "2 Diffusion Recursive Least-Squares Diffusion strategies can also be applied to recursive least-squares problems to enable distributed solutions of least-squares designs [28,29]; see also [72].",
      "startOffset" : 170,
      "endOffset" : 177
    }, {
      "referenceID" : 63,
      "context" : "2 Diffusion Recursive Least-Squares Diffusion strategies can also be applied to recursive least-squares problems to enable distributed solutions of least-squares designs [28,29]; see also [72].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 22,
      "context" : "In [28, 29] a diffusion strategy was developed that allows nodes to approach the global solution wi by relying solely on local interactions.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 23,
      "context" : "In [28, 29] a diffusion strategy was developed that allows nodes to approach the global solution wi by relying solely on local interactions.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 23,
      "context" : "The mean-square performance and convergence of the diffusion RLS strategy are studied in some detail in [29].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 69,
      "context" : "Under some approximations, and for the special choices A = C and λ = 1, the diffusion RLS strategy (520) can be reduced to a form given in [79] and which is described by the following equations: P k,i = ∑",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 69,
      "context" : "ψk,i = Pk,iqk,i (523) Algorithm (521)–(523) is motivated in [79] by using consensus-type arguments.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 69,
      "context" : "Comparing these equations with (521)–(523), we find that algorithm (521)–(523) of [79] would relate to the diffusion RLS algorithm (520) when the following approximations are justified: ∑",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : "l∈Nk clkψl,i−1 (527) = P k,i−1wk,i−1 (528) It was indicated in [29] that the diffusion RLS implementation (514) or (520) leads to enhanced performance in comparison to the consensus-based update (521)–(523).",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 24,
      "context" : "3 Diffusion Kalman Filtering Diffusion strategies can also be applied to the solution of distributed state-space filtering and smoothing problems [30, 31, 33].",
      "startOffset" : 146,
      "endOffset" : 158
    }, {
      "referenceID" : 25,
      "context" : "3 Diffusion Kalman Filtering Diffusion strategies can also be applied to the solution of distributed state-space filtering and smoothing problems [30, 31, 33].",
      "startOffset" : 146,
      "endOffset" : 158
    }, {
      "referenceID" : 27,
      "context" : "3 Diffusion Kalman Filtering Diffusion strategies can also be applied to the solution of distributed state-space filtering and smoothing problems [30, 31, 33].",
      "startOffset" : 146,
      "endOffset" : 158
    }, {
      "referenceID" : 27,
      "context" : "Here, we describe briefly the diffusion version of the Kalman filter; other variants and smoothing filters can be found in [33].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 24,
      "context" : "The following diffusion strategy was proposed in [30, 31, 33] to evaluate approximate predicted and filtered versions of these local estimators in a distributed manner for data satisfying model (529)–(532).",
      "startOffset" : 49,
      "endOffset" : 61
    }, {
      "referenceID" : 25,
      "context" : "The following diffusion strategy was proposed in [30, 31, 33] to evaluate approximate predicted and filtered versions of these local estimators in a distributed manner for data satisfying model (529)–(532).",
      "startOffset" : 49,
      "endOffset" : 61
    }, {
      "referenceID" : 27,
      "context" : "The following diffusion strategy was proposed in [30, 31, 33] to evaluate approximate predicted and filtered versions of these local estimators in a distributed manner for data satisfying model (529)–(532).",
      "startOffset" : 49,
      "endOffset" : 61
    }, {
      "referenceID" : 27,
      "context" : "It is important to note that even though the notation Pk,i|i and Pk,i|i−1 has been retained for these variables, as in the standard Kalman filtering notation [5,77], these matrices do not represent any longer the covariances of the state estimation errors, x̃k,i|i−1 = xi − x̂k,i|i−1, but can be related to them [33].",
      "startOffset" : 312,
      "endOffset" : 316
    }, {
      "referenceID" : 42,
      "context" : "The incremental update in (535) is similar to the update used in the distributed Kalman filter derived in [48].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 42,
      "context" : "Reference [48] starts from a continuous-time consensus implementation and discretizes it to arrive at the following update relation: x̂k,i|i = ψk,i + ǫ ∑ l∈Nk (ψl,i −ψk,i) (536)",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 27,
      "context" : "D of [33].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 27,
      "context" : "It was verified in [33] that the diffusion implementation (538) leads to enhanced performance in comparison to the consensus-based update (537).",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 68,
      "context" : "Moreover, the weights {alk} in (538) can also be adjusted over time in order to further enhance performance, as discussed in [78].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 27,
      "context" : "The mean-square performance and convergence of the diffusion Kalman filtering implementations are studied in some detail in [33], along with other diffusion strategies for smoothing problems including fixed-point and fixed-lag smoothing.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "Nevertheless, we remarked in that section that similar diffusion strategies can be applied to more general cases involving individual cost functions, Jk(w), that are not necessarily quadratic in w [1–3].",
      "startOffset" : 197,
      "endOffset" : 202
    }, {
      "referenceID" : 1,
      "context" : "Nevertheless, we remarked in that section that similar diffusion strategies can be applied to more general cases involving individual cost functions, Jk(w), that are not necessarily quadratic in w [1–3].",
      "startOffset" : 197,
      "endOffset" : 202
    }, {
      "referenceID" : 2,
      "context" : "Nevertheless, we remarked in that section that similar diffusion strategies can be applied to more general cases involving individual cost functions, Jk(w), that are not necessarily quadratic in w [1–3].",
      "startOffset" : 197,
      "endOffset" : 202
    }, {
      "referenceID" : 0,
      "context" : "The following properties can be proven for the diffusion strategies (545)–(547) [1–3].",
      "startOffset" : 80,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "The following properties can be proven for the diffusion strategies (545)–(547) [1–3].",
      "startOffset" : 80,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "The following properties can be proven for the diffusion strategies (545)–(547) [1–3].",
      "startOffset" : 80,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "The case where the {Jk(w)} have different individual minimizers is studied in [1,3], where it is shown that the same diffusion strategies of this section are still applicable and nodes would converge instead to a Pareto-optimal solution.",
      "startOffset" : 78,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "The case where the {Jk(w)} have different individual minimizers is studied in [1,3], where it is shown that the same diffusion strategies of this section are still applicable and nodes would converge instead to a Pareto-optimal solution.",
      "startOffset" : 78,
      "endOffset" : 83
    }, {
      "referenceID" : 34,
      "context" : ", [40, 80]), the norms of the sub-gradients are usually required to be uniformly bounded.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 70,
      "context" : ", [40, 80]), the norms of the sub-gradients are usually required to be uniformly bounded.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 34,
      "context" : "This condition on the noise is more general than the “uniform-bounded assumption” that appears in [40], which required instead:",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 71,
      "context" : "3) in [81], which requires the noise variance to satisfy:",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 16,
      "context" : "This requirement can be verified to be a combination of the “relative random noise” and the “absolute random noise” conditions defined in [22] — see [2].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "This requirement can be verified to be a combination of the “relative random noise” and the “absolute random noise” conditions defined in [22] — see [2].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : ", w̃i,N} (562) where w̃k,i ∆ = w −wk,i (563) Then, the following result can be established [2]; it characterizes the network mean-square deviation in steady-state, which is defined as",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 57,
      "context" : "The matrix L is symmetric and its entries are defined as follows [64–66]:",
      "startOffset" : 65,
      "endOffset" : 72
    }, {
      "referenceID" : 57,
      "context" : "The following is a classical result from graph theory [64–67].",
      "startOffset" : 54,
      "endOffset" : 61
    }, {
      "referenceID" : 58,
      "context" : "The following is a classical result from graph theory [64–67].",
      "startOffset" : 54,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "(d) The eigenvalues of AA or AA are real and lie inside the interval [0, 1].",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "But since ρ(AA ) = 1, we must have λ(AA ) ∈ [0, 1].",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "For part (f), since AA ≥ 0 and its eigenvalues lie within [0, 1], the matrix AA admits an eigen-decomposition of the form: AA = UΛU where U is orthogonal (i.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : ", U = U ) and Λ is diagonal with entries in the range [0, 1].",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 46,
      "context" : "Following [52, 54, 55], the block maximum norm of x is denoted by ‖x‖b,∞ and is defined as",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 48,
      "context" : "Following [52, 54, 55], the block maximum norm of x is denoted by ‖x‖b,∞ and is defined as",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 49,
      "context" : "Following [52, 54, 55], the block maximum norm of x is denoted by ‖x‖b,∞ and is defined as",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 48,
      "context" : "The block maximum norm inherits the unitary invariance property of the Euclidean norm, as the following result indicates [54].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 15,
      "context" : "To establish part (c), we start by recalling that all norms on finite-dimensional vector spaces are equivalent [20,21].",
      "startOffset" : 111,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 4,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 4,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "D = [ 2 0 0 1 ] , A1 = [ 1 3 2 3 2 3 1 3 ] , A2 = [ 1 3 2 3 2 3 1 3 ]",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "A2 DA T 1 = [ 2 3 2 3 2 3 1 ]",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "A2 DA T 1 = [ 2 3 2 3 2 3 1 ]",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "A2 DA T 1 = [ 2 3 2 3 2 3 1 ]",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "A2 DA T 1 = [ 2 3 2 3 2 3 1 ]",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "A2 DA T 1 = [ 2 3 2 3 2 3 1 ]",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "A2 DA T 1 = [ 2 3 2 3 2 3 1 ]",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "A2 DA T 1 = [ 2 3 2 3 2 3 1 ]",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "I of [18] and Lemma 2 of [33].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 27,
      "context" : "I of [18] and Lemma 2 of [33].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "I of [18] and the matrix M in Lemma 2 of [33] are block diagonal, the ‖ · ‖b,∞ norm should replace the ‖ · ‖ρ norm used there, as in the proof that led to (606) and as already done in [54].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 27,
      "context" : "I of [18] and the matrix M in Lemma 2 of [33] are block diagonal, the ‖ · ‖b,∞ norm should replace the ‖ · ‖ρ norm used there, as in the proof that led to (606) and as already done in [54].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 48,
      "context" : "I of [18] and the matrix M in Lemma 2 of [33] are block diagonal, the ‖ · ‖b,∞ norm should replace the ‖ · ‖ρ norm used there, as in the proof that led to (606) and as already done in [54].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 36,
      "context" : "Convergence Conditions The following result is a classical result on consensus strategies [42–44].",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 37,
      "context" : "Convergence Conditions The following result is a classical result on consensus strategies [42–44].",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 38,
      "context" : "Convergence Conditions The following result is a classical result on consensus strategies [42–44].",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 46,
      "context" : "Now, motivated by the consensus iteration (610), and based on a procedure for distributed optimization suggested in [52] (see expression (7.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 39,
      "context" : ", [45, 53, 82–88]) considered distributed strategies that correspond to the following form for the optimization problem under consideration (see, e.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 47,
      "context" : ", [45, 53, 82–88]) considered distributed strategies that correspond to the following form for the optimization problem under consideration (see, e.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 72,
      "context" : ", [45, 53, 82–88]) considered distributed strategies that correspond to the following form for the optimization problem under consideration (see, e.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 73,
      "context" : ", [45, 53, 82–88]) considered distributed strategies that correspond to the following form for the optimization problem under consideration (see, e.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 74,
      "context" : ", [45, 53, 82–88]) considered distributed strategies that correspond to the following form for the optimization problem under consideration (see, e.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 75,
      "context" : ", [45, 53, 82–88]) considered distributed strategies that correspond to the following form for the optimization problem under consideration (see, e.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 76,
      "context" : ", [45, 53, 82–88]) considered distributed strategies that correspond to the following form for the optimization problem under consideration (see, e.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 77,
      "context" : ", [45, 53, 82–88]) considered distributed strategies that correspond to the following form for the optimization problem under consideration (see, e.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 78,
      "context" : ", [45, 53, 82–88]) considered distributed strategies that correspond to the following form for the optimization problem under consideration (see, e.",
      "startOffset" : 2,
      "endOffset" : 17
    }, {
      "referenceID" : 47,
      "context" : "20) in [53] and expression (9) in [87]):",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 77,
      "context" : "20) in [53] and expression (9) in [87]):",
      "startOffset" : 34,
      "endOffset" : 38
    } ],
    "year" : 2013,
    "abstractText" : "Adaptive networks are well-suited to perform decentralized information processing and optimization tasks and to model various types of self-organized and complex behavior encountered in nature. Adaptive networks consist of a collection of agents with processing and learning abilities. The agents are linked together through a connection topology, and they cooperate with each other through local interactions to solve distributed optimization, estimation, and inference problems in real-time. The continuous diffusion of information across the network enables agents to adapt their performance in relation to streaming data and network conditions; it also results in improved adaptation and learning performance relative to non-cooperative agents. This article provides an overview of diffusion strategies for adaptation and learning over networks. The article is divided into several sections: 1. Motivation. 2. Mean-Square-Error Estimation. 3. Distributed Optimization via Diffusion Strategies. 4. Adaptive Diffusion Strategies. 5. Performance of Steepest-Descent Diffusion Strategies. 6. Performance of Adaptive Diffusion Strategies. 7. Comparing the Performance of Cooperative Strategies. 8. Selecting the Combination Weights. 9. Diffusion with Noisy Information Exchanges. 10. Extensions and Further Considerations. 11. Appendix A: Properties of Kronecker Products. 12. Appendix B: Graph Laplacian and Network Connectivity. 13. Appendix C: Stochastic Matrices. 14. Appendix D: Block Maximum Norm. 15. Appendix E: Comparison with Consensus Strategies. 16. References.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}