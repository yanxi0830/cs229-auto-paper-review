{
  "name" : "1401.6421.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Riffled Independence for Efficient Inference with Partial Rankings",
    "authors" : [ "Jonathan Huang", "James H. Clark", "Ashish Kapoor", "Carlos Guestrin" ],
    "emails" : [ "jhuang11@stanford.edu", "akapoor@microsoft.com", "guestrin@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "such as preference analysis and political elections. Modeling such distributions presents several computational challenges, however, due to the factorial size of the set of rankings over an item set. Some of these challenges are quite familiar to the artificial intelligence community, such as how to compactly represent a distribution over a combinatorially large space, and how to efficiently perform probabilistic inference with these representations. With respect to ranking, however, there is the additional challenge of what we refer to as human task complexity — users are rarely willing to provide a full ranking over a long list of candidates, instead often preferring to provide partial ranking information.\nSimultaneously addressing all of these challenges — i.e., designing a compactly representable model which is amenable to efficient inference and can be learned using partial ranking data — is a difficult task, but is necessary if we would like to scale to problems with nontrivial size. In this paper, we show that the recently proposed riffled independence assumptions cleanly and efficiently address each of the above challenges. In particular, we establish a tight mathematical connection between the concepts of riffled independence and of partial rankings. This correspondence not only allows us to then develop efficient and exact algorithms for performing inference tasks using riffled independence based representations with partial rankings, but somewhat surprisingly, also shows that efficient inference is not possible for riffle independent models (in a certain sense) with observations which do not take the form of partial rankings. Finally, using our inference algorithm, we introduce the first method for learning riffled independence based models from partially ranked data."
    }, {
      "heading" : "1. Probabilistic Modeling of Ranking Data: Three Challenges",
      "text" : "Rankings arise in a number of machine learning application settings such as preference analysis for movies and books (Lebanon & Mao, 2008) and political election analysis (Gormley & Murphy, 2007; Huang & Guestrin, 2010). In many of these problems, it is of great interest to build statistical models over ranking data in order to make predictions, form recommendations, discover latent trends and structure and to construct human-comprehensible data summaries.\nc©2012 AI Access Foundation. All rights reserved.\nModeling distributions over rankings is a difficult problem, however, due to the fact that as the number of items being ranked increases, the number of possible rankings increases factorially. This combinatorial explosion forces us to confront three central challenges when dealing with rankings. First, we need to deal with storage complexity — how can we compactly represent a distribution over the space of rankings?1 Then there is algorithmic complexity — how can we efficiently answer probabilistic inference queries given a distribution?\nFinally, we must contend with what we refer to as human task complexity, which is a challenge stemming from the fact that it can be difficult to accurately elicit a full ranking over a large list of candidates from a human user; choosing from a list of n! options is no easy task and users typically prefer to provide partial information. Take the American Psychological Association (APA) elections, for example, which allow their voters to rank order candidates from favorite to least favorite. In the 1980 election, there were five candidates, and therefore 5! = 120 ways to rank those five candidates. Despite the small candidate list, most voters in the election preferred to only specify their top-k favorite candidates rather than writing down full rankings on their ballots (see Figure 1). For example, roughly a third of voters simply wrote down their single favorite candidate in this 1980 election.\nThese three intertwined challenges of storage, algorithmic, and human task complexity are the central issues of probabilistic modeling for rankings, and models that do not efficiently handle all three sources of complexity have limited applicability. In this paper, we examine a flexible and intuitive class of models for rankings based on a generalization of probabilistic independence called riffled independence, proposed in our recent work (Huang & Guestrin, 2009, 2010). While our previous papers have focused primarily on representational (storage complexity) issues, we now concentrate on inference and incomplete observations (i.e., partial rankings), showing that in addition to storage complexity, riffle independence based models can efficiently address issues of algorithmic and human task complexity.\nIn fact the two issues of algorithmic and human task complexity are intricately linked for riffle independent models. By considering partial rankings, we give users more flexibility to provide as much or as little information as they care to give. In the context of partial ranking data, the most relevant inference queries also take the form of partial rankings. For example, we might want to predict a voter’s second choice candidate given information about his first choice. One of our main contributions in this paper is to show that inference for such partial ranking queries can be performed particularly efficiently for riffle independent models.\nThe main contributions of our work are as follows:2\n• We reveal a natural and fundamental connection between riffle independent models and partial rankings. In particular, we show that the collection of partial rankings over an item set form a complete characterization of the space of observations upon\n1. Note that it is common to wonder why one would care to represent a distribution over all rankings if the number of sample rankings is never nearly as large. This problem that the number of samples is always much smaller than n! however, means that most rankings are never observed, limiting our ability to estimate the probability of an arbitrary ranking. The only way to overcome the paucity of samples is to exploit representational structure, which is very much in alignment with solving the storage complexity issue. 2. This paper is an extended presentation of our paper (Huang, Kapoor, & Guestrin, 2011) which appeared in the 2011 Conference on Uncertainty in Artificial Intelligence (UAI) as well as results from the first author’s dissertation (Huang, 2011).\nwhich one can efficiently condition a riffle independent model. As a result, we show that when ranked items satisfy the riffled independence relationship, conditioning on partial rankings can be done efficiently, with running time O(n·|H|), where |H| denotes the number of model parameters. • We prove that, in a sense (which we formalize), it is impossible to efficiently condition\nriffle independent models on observations that do not take the form of partial rankings. • We propose the first algorithm that is capable of efficiently estimating the structure\nand parameters of riffle independent models from heterogeneous collections of partially ranked data. • We show results on real voting and preference data evidencing the effectiveness of our\nmethods."
    }, {
      "heading" : "2. Riffled Independence For Rankings",
      "text" : "A ranking, σ, of items in an item set Ω is a one-to-one mapping between Ω and a rank set R = {1, . . . , n} and is denoted using vertical bar notation as σ−1(1)|σ−1(2)| . . . |σ−1(n). We say that σ ranks item i1 before (or over) item i2 if the rank of i1 is less than the rank of i2. For example, Ω might be {Corn, Peas,Apples,Oranges} and the ranking Corn|Peas|Apples|Oranges encodes a preference of Corn over Peas which is in turn preferred over Apples and so on. The collection of all possible rankings of item set Ω is denoted by SΩ (or just Sn when Ω is implicit).\nSince there are n! rankings of n items, it is intractable to estimate or even explicitly represent arbitrary distributions on Sn without making structural assumptions about the underlying distribution. While there are many possible simplifying assumptions that one can make, we focus on an approach that we have proposed in recent papers (Huang & Guestrin, 2009, 2010) in which the ranks of items are assumed to satisfy an intuitive generalized notion of probabilistic independence known as riffled independence. In this paper, we argue that riffled independence assumptions are particularly effective in settings where one would like to make queries taking the form of partial rankings. In the remainder of this section, we review riffled independence.\nThe riffled independence assumption posits that rankings over the item set Ω are generated by independently generating rankings of smaller disjoint item subsets (say, A and B) which partition Ω, and piecing together a full ranking by interleaving (or riffle shuffling) these smaller rankings together. For example, to rank our item set of foods, one might first rank the vegetables and fruits separately, then interleave the two subset rankings to form a full ranking. To formally define riffled independence, we use the notions of relative rankings and interleavings.\nDefinition 1 (Relative ranking map). Given a ranking σ ∈ SΩ and any subset A ⊂ Ω, the relative ranking of items in A, φA(σ), is a ranking, π ∈ SA, such that π(i) < π(j) if and only if σ(i) < σ(j).\nDefinition 2 (Interleaving map). Given a ranking σ ∈ SΩ and a partition of Ω into disjoint sets A and B, the interleaving of A and B in σ (denoted, τAB(σ)) is a (binary) mapping from the rank set R = {1, . . . , n} to {A,B} indicating whether a rank in σ is occupied by A or B. As with rankings, we denote the interleaving of a ranking by its vertical bar notation: [τAB(σ)](1)|[τAB(σ)](2)| . . . |[τAB(σ)](n).\nExample 3. Consider a partitioning of an item set Ω into vegetables A = {Corn, Peas} and fruits B = {Apples,Oranges}, as well as a full ranking over these four items: σ = Corn|Oranges|Peas|Apples. In this case, the relative ranking of vegetables in σ is φA(σ) = Corn|Peas and the relative ranking of fruits in σ is φB(σ) = Oranges|Apples. The interleaving of vegetables and fruits in σ is τAB(σ) = A|B|A|B.\nDefinition 4 (Riffled Independence). Let h be a distribution over SΩ and consider a subset of items A ⊂ Ω and its complement B. The sets A and B are said to be riffle independent if h decomposes (or factors) as:\nh(σ) = mAB(τAB(σ)) · fA(φA(σ)) · gB(φB(σ)),\nfor distributions mAB, fA and gB, defined over interleavings and relative rankings of A and B respectively. In other words, A and B are riffle independent if the relative rankings of A and B, as well as their interleaving are mutually independent. We refer to mAB as the interleaving distribution and fA and gB as the relative ranking distributions.\nRiffled independence has been found to approximately hold in a number of real datasets (Huang & Guestrin, 2012). When such relationships can be identified in data, then instead of exhaustively representing all n! ranking probabilities, one can represent just the factors mAB, fA and gB, which are distributions over smaller sets."
    }, {
      "heading" : "2.1 Hierarchical Riffle Independent Models",
      "text" : "The relative ranking factors fA and gB are themselves distributions over rankings. To further reduce the parameter space, it is natural to consider hierarchical decompositions of item sets into nested collections of partitions (like hierarchical clustering). For example, Figure 2.1 shows a hierarchical decomposition where vegetables are riffle independent of fruits among the “healthy” foods, and these healthy foods are, in turn, riffle independent of the subset of desserts: {Doughnuts,M&Ms}.\nFor simplicity, we restrict consideration to binary hierarchies, defined as tuples of the form H = (HA, HB), where HA and HB are either (1) null, in which case H is called a leaf, or (2) hierarchies over item sets A and B respectively. In this second case, A and B are assumed to form a nontrivial partitioning of the item set.\nDefinition 5. We say that a distribution h factors riffle independently with respect to a hierarchy H = (HA, HB) if item sets A and B are riffle independent with respect to h, and both fA and gB factor riffle independently with respect to subhierarchies HA and HB, respectively.\nLike Bayesian networks, these hierarchies represent families of distributions obeying a certain set of (riffled) independence constraints and can be parameterized locally. To draw from such a model, one generates full rankings recursively starting by drawing rankings of the leaf sets, then working up the tree, sequentially interleaving rankings until reaching the root. The parameters of these hierarchical models are simply the interleaving and relative ranking distributions at the internal nodes and leaves of the hierarchy, respectively.\nIn general, the number of total parameters required to represent a hierarchical riffle independent model can (as with Bayesian networks) still scale exponentially in the number of items. For example, the number of interleavings of p items with n − p items is ( n p ) . It is often the case however, that much fewer parameters are necessary. For example, thin models (Huang & Guestrin, 2012), in which the number of items factored out of the model at each stage of the hierarchy is never more than a small constant k, can always be represented with a (degree k) polynomial number of parameters. We will use |H| to refer to the number of parameters necessary for representing a distribution which factors according to hierarchy H.\nBy decomposing distributions over rankings into small pieces (like Bayesian networks have done for other distributions), these hierarchical models allow for better interpretability, efficient probabilistic representation, low sample complexity, efficient MAP optimization, and, as we show in this paper, efficient inference.\nExample 6. In Figure 3(a), we reproduce the hierarchical structure that was learned using a fully ranked subset of the APA data consisting of 5000 training examples in Huang and Guestrin (2012). There were five candidates in the election: (1) William Bevan, (2) Ira Iscoe, (3) Charles Kiesler, (4) Max Siegle, and (5) Logan Wright (Marden, 1995). Strikingly, the structure that is learned using an algorithm (maximum likelihood) which knows nothing about the underlying politics of the APA, has leaf nodes which correspond exactly to the political coalitions that dominated the APA in the 1980 election — the research psychologists\n(candidates 1 and 3), the clinical psychologists (candidates 4 and 5), and the community psychologists (candidate 2).\nIn Figure 3(b), we plot the corresponding parameter distributions that are learned via maximum likelihood. There are three relative ranking distributions, each corresponding to a political party, as well as two interleaving distributions (one for the interleaving of research and clinical psychologists, and one for the interleaving of the community psychologist and all remaining candidates). Since each parameter distribution is constrained to sum to 1, there are a total of 11 free parameters."
    }, {
      "heading" : "2.2 Model Estimation",
      "text" : "In this paper we estimate riffle independent models based on the methods introduced in our earlier work. Given the hierarchial structure of a model, the maximum likelihood parameter estimates of a hierarchical riffle independent model are straightforward to compute via frequency estimates. But how to estimate the correct structure of a model is a more challenging problem. The key insight lies in noticing that if two subsets A and B are riffle independent, then for any i ∈ A and j, k ∈ B, the independence relation σ(i) ⊥ (σ(j) < σ(k)) must hold. Our structure learning algorithms operate by hunting for these ‘tripletwise’ independence relations within the data. We defer interested readers to the details in (Huang & Guestrin, 2012).\nNote that in our earlier work, we assumed that our algorithms have access to a dataset consisting of i.i.d. full rankings provided by users. In the current work, we will relax our assumptions by allowing for users to provide partially ranked data. One assumption throughout, however, is that each user has a full ranking in mind over the items. In particular, our current work does not address the incomplete ranking problem, in which users might not have seen all of the items (we discuss possible extensions to the incomplete ranking setting in Section 9."
    }, {
      "heading" : "3. Decomposable Observations",
      "text" : "Given a prior distribution, h, over rankings and an observation O, Bayes rule tells us that the posterior distribution, h(σ|O), is proportional to L(O|σ) · h(σ), where L(O|σ) is the likelihood function. This operation of conditioning h on an observation O is typically computationally intractable since it requires multiplying two n! dimensional functions, unless one can exploit structural decompositions of the problem. In this section, we describe a decomposition for a certain class of likelihood functions over the space of rankings in which the observations are ‘factored’ into simpler parts. When an observation O is decomposable in this way, we show that one can efficiently condition a riffle independent prior distribution on O. For simplicity in this paper, we focus primarily on subset observations whose likelihood functions encode membership with some subset of rankings in Sn.\nDefinition 7 (Subset observations). A subset observation O is a binary observation whose likelihood is proportional to the indicator function of some subset of Sn — i.e.,\nL(O|σ) = {\n1 if σ ∈ O 0 otherwise .\nAs a running example, we will consider the class of first place observations throughout the chapter (we will consider far more general observation models in later sections). The first place observation O =“Corn is ranked first”, for example, is associated with the collection of rankings placing the item Corn in first place (O = {σ : σ(Corn) = 1}). We are interested in computing the posterior h(σ|σ ∈ O). Thus in the first place scenario, we are given a voter’s top choice and we would like to infer his preferences over the remaining candidates.\nGiven a partitioning of the item set Ω into two subsets A and B, it is sometimes possible to decompose (or factor) a subset observation involving items in Ω into smaller subset observations involving A, B and the interleavings of A and B independently. Such decompositions can often be exploited for efficient inference.\nExample 8.\n• Consider the first place observation\nO = “Corn is ranked first”,\nwhich can be decomposed into two independent observations — an observation on the relative ranking of Vegetables, and an observation on the interleaving of Vegetables and Fruits:\n– OA = “Corn is ranked first among Vegetables”, – OA,B = “First place is occupied by a Vegetable”.\nTo condition on O in this case, one updates the relative ranking distribution over Vegetables (A) by zeroing out rankings of vegetables which do not place Corn in first place, and updates the interleaving distribution by zeroing out interleavings which do not place a Vegetable in first place, then normalizes the resulting distributions. • An example of a nondecomposable observation is the observation\nO = “Corn is in third place”.\nTo see that O does not decompose (with respect to Vegetables and Fruits), it is enough to notice that the interleaving of Vegetables and Fruits is not independent of the relative ranking of Vegetables. If, for example, an element σ ∈ O interleaves A (Vegetables) and B (Fruits) as τAB(σ) = A|B|A|B, then since σ(Corn) = 3, the relative ranking of Vegetables is constrained to be φA(σ) = Peas|Corn. Since the interleavings and relative rankings are not independent, we see that O cannot be decomposable.\nFormally, we use riffle independent factorizations to define decomposability with respect to a hierarchy H of the item set.\nDefinition 9 (Decomposability). Given a hierarchy H over the item set, a subset observation O decomposes with respect to H if its likelihood function L(O|σ) factors riffle independently with respect to H.\nWhen subset observations and the prior decompose according to the same hierarchy, we can show (as in Example 8) that the posterior also decomposes.\nProposition 10. Let H be a hierarchy over the item set. Given a prior distribution h and a subset observation O which both decompose with respect to H, the posterior distribution h(σ|O) also factors riffle independently with respect to H.\nProof. Denote the likelihood function corresponding to O by L (in this proof, it does not matter that O is assumed to be a subset observation — the result holds for arbitrary likelihoods).\nWe use induction on the size of the item set n = |Ω|. The base case n = 1 is trivially true. Next consider the general case where n > 1. The posterior distribution, by Bayes rule, can be written h(σ|O) ∝ L(σ) · h(σ). There are now two cases. If H is a leaf node, then the posterior h′ trivially factors according to H, and we are done. Otherwise, L and h both factor, by assumption, according to H = (HA, HB) in the following way:\nL(σ) = mL(τAB(σ)) ·fL(φA(σ)) ·gL(φB(σ)), and h(σ) = mh(τAB(σ)) ·fh(φA(σ)) ·gh(φB(σ)).\nMultiplying and grouping terms, we see that the posterior factors as:\nh(σ|O) = [mL ·mh](τAB(σ)) · [fL · fh](φA(σ)) · [gL · gh](φB(σ)).\nTo show that h(σ|O) factors with respect to H, we need to demonstrate (by Definition 5) that the distributions [fL ·fh] and [gL ·gh] (after normalizing) factor with respect to HA and\nHB, respectively. Since fL and fh both factor according to the hierarchy HA by assumption and |A| < n since H is not a leaf, we can invoke the inductive hypothesis to show that the posterior distribution, which is proportional to fL · fh must also factor according to HA. Similarly, the distribution proportional to gL · gh must factor according to HB."
    }, {
      "heading" : "4. Complete Decomposability",
      "text" : "The condition of Proposition 10, that the prior and observation must decompose with respect to exactly the same hierarchy, is a sufficient one for efficient inference, but it might at first glance seem so restrictive as to render the proposition useless in practice. To overcome this limitation of “hierarchy specific” decomposability, we explore a special family of observations (which we call completely decomposable) for which the property of decomposability does not depend specifically on a particular hierarchy, implying in particular that for these observations, efficient inference is always possible (provided that efficient representation of the prior distribution is also possible).\nTo illustrate how an observation can decompose with respect to multiple hierarchies over the item set, consider again the first place observation O =“Corn is ranked first”. We argued in Example 8 that O is a decomposable observation. Notice however that decomposability for this particular observation does not depend on how the items are partitioned by the hierarchy. Specifically, if instead of Vegetables and Fruits, the sets A = {Corn,Apples} and B = {Peas,Oranges} are riffle independent, a similar decomposition of O would continue to hold, with O decomposing as an observation on the relative ranking of items in A (“Corn is first among items in A”), and an observation on the interleaving of A and B (“First place is occupied by some element of A”).\nTo formally capture this notion that an observation can decompose with respect to arbitrary underlying hierarchies, we define complete decomposability :\nDefinition 11 (Complete decomposability). We say that a subset observation O is completely decomposable if it decomposes with respect to every possible hierarchy over the item set Ω. We denote the collection of all possible completely decomposable (subset) observations as C. See Figure 4 for an illustration of the set C.\nConceptually, completely decomposable observations correspond to indicator functions that are “as riffle independent as possible”. Complete decomposability is a guarantee for an observation O that one can always exploit any available factorized structure of the prior distribution in order to efficiently condition on O.\nProposition 12. Let H be any binary hierarchy over the item set. Given a prior h which factorizes with respect to H, and a completely decomposable observation O, the posterior h(σ|O) also decomposes with respect to H.\nProof. Proposition 12 follows as a simple corollary to Proposition 10.\nExample 13. The simplest example of a completely decomposable observation is the uniform observation Ounif = SΩ, which includes all possible rankings and corresponds to a uniform indicator function δunif over rankings. Given any hierarchy H, δunif can be shown to decompose riffle independently with respect to H, where each factor is also uniform, and hence Ounif is completely decomposable.\nThe uniform observation is of course not particularly interesting in the context of Bayesian inference, but on the other hand, given the stringent conditions in Definition 11, it is not obvious that nontrivial completely decomposable observations can even exist. Nonetheless, there do exist nontrivial examples (such as the first place observations), and in the next section, we exhibit a rich and general class of completely decomposable observations."
    }, {
      "heading" : "5. Complete Decomposability of Partial Ranking Observations",
      "text" : "In this section we discuss the mathematical problem of fully characterizing the class of completely decomposable observations. Our main contribution in this section is to show that completely decomposable observations correspond precisely to partial rankings of the item set.\nPartial rankings. We begin our discussion by introducing partial rankings, which allow for items to be tied with respect to a ranking σ by ‘dropping’ verticals from the vertical bar representation of σ.\nDefinition 14 (Partial ranking observation). Let Ω1, Ω2,. . . , Ωr be an ordered collection of subsets which partition Ω (i.e., ∪iΩi = Ω and Ωi ∩ Ωj = ∅ if i 6= j). The partial ranking observation3 corresponding to this partition is the collection of rankings which rank items\n3. As remarked by Ailon (2007), we note that “The term partial ranking used here should not be confused with two other standard objects: (1) Partial order, namely, a reflexive, transitive anti-symmetric binary\nin Ωi before items in Ωj if i < j. We denote this partial ranking as Ω1|Ω2| . . . |Ωr and say that it has type γ = (|Ω1|, |Ω2|, . . . , |Ωr|). We denote the collection of all partial rankings (over n items) as P.\nEach partial ranking as defined above can be viewed as a coset of the subgroup Sγ = Sγ1 × Sγ2 × · · · × Sγr . Given the type γ and any full ranking π ∈ SΩ, there is only one partial ranking of type γ containing π, thus we will therefore equivalently denote the partial ranking Ω1|Ω2| . . . |Ωr as Sγπ, where π is any element of Ω1|Ω2| . . . |Ωr. Note that this coset notation allows for multiple rankings σ to refer to the same partial ranking Sγσ.\nThe space of partial rankings as defined above captures a rich and natural class of observations. In particular, partial rankings encompass a number of commonly occurring special cases, which have traditionally been modeled in isolation, but in our work (as well as recent works such as Lebanon & Lafferty, 2003; Lebanon & Mao, 2008) can be used in a unified setting.\nExample 15. Partial ranking observations include:\n• (First place, or Top-1 observations): First place observations correspond to partial rankings of type γ = (1, n − 1). The observation that “Corn is ranked first” can be written as Corn|Peas,Apples,Oranges. • (Top-k observations): Top-k observations are partial rankings with type γ = (1, . . . , 1, n− k). These generalize the first place observations by specifying the items mapping to the first k ranks, leaving all n− k remaining items implicitly ranked behind. For example, the observation that “Corn is ranked first and Peas is ranked second” can be written as Corn|Peas|Apples,Oranges. • (Desired/less desired dichotomy): Partial rankings of type γ = (k, n−k) correspond to a subset of k items being preferred or desired over the remaining subset of n−k items. For example, partial rankings of type (k, n−k) might arise in approval voting in which voters mark the subset of “approved” candidates, implicitly indicating disapproval of the remaining n− k candidates. • (Ratings): Finally, partial rankings can come in the form of rating data where, for example, restaurants are rated as, ?, ??, or ? ? ?. A corresponding partial ranking would thus “tie” restaurants that are rated with the same number of stars, while ranking restaurants with more stars above restaurants with fewer stars. • (Trivial observations): Partial rankings of type γ = (n) refer to trivial observations whose likelihood functions are uniform on the entire space of rankings, SΩ. The trivial observation for rankings of the item set Ω = {Corn, Peas,Apples}, for example, can simply be written simply as Corn, Peas,Apples.\nTo show how partial ranking observations decompose, we will exhibit an explicit factorization with respect to a hierarchy H over items. For simplicity, we begin by considering the single layer case, in which the items are partitioned into two leaf sets A and B. Our factorization depends on the following notions of consistency of relative rankings and interleavings with a partial ranking.\nrelation; and (2) A ranking of a subset of Ω [which we discuss in Section 9 as incomplete rankings]. In search engines, for example, although only the top-k elements of Ω are returned, the remaining n − k are implicitly assumed to be ranked behind [and therefore, search engines return partial rankings].”\nDefinition 16 (Restriction consistency). Given a partial ranking Sγπ = Ω1|Ω2| . . . |Ωr and any subset A ⊂ Ω, we define the restriction of Sγπ to A as the partial ranking on items in A obtained by intersecting each Ωi with A. Hence the restriction of Sγπ to A is:\n[Sγπ]A = Ω1 ∩A|Ω2 ∩A| . . . |Ωr ∩A.\nGiven a ranking, σA of items in A, we say that σA is consistent with the partial ranking Sγπ if σA is a member of the restriction of Sγπ to A, [Sγπ]A.\nDefinition 17 (Interleaving consistency). Given an interleaving τAB of two sets A,B which partition Ω, we say that τAB is consistent with a partial ranking Sγπ = Ω1| . . . |Ωr (with type γ) if the first γ1 entries of τAB contain the same number of As and Bs as Ω1, and the second γ2 entries of τAB contain the same number of As and Bs as Ω2, and so on. Given a partial ranking Sγπ, we denote the collection of consistent interleavings as [Sγπ]AB.\nFor example, consider the partial ranking\nSγπ = Corn,Apples|Peas,Oranges,\nwhich places a single vegetable and a single fruit in the first two ranks, and a single vegetable and a single fruit in the last two ranks. Alternatively, Sγπ partially specifies an interleaving AB|AB. The full interleavings A|B|B|A and B|A|B|A are consistent with Sγπ (by dropping vertical lines) while A|A|B|B is not consistent (since it places two vegetables in the first two ranks).\nUsing the notions of consistency with a partial ranking, we show that partial ranking observations are decomposable with respect to any binary partitioning (i.e., single layer hierarchy) of the item set.\nProposition 18 (Single layer hierarchy). For any partial ranking observation Sγπ and any binary partitioning of the item set (A,B), the indicator function of Sγπ, δSγπ, factors riffle independently as:\nδSγπ(σ) = mAB(τAB(σ)) · fA(φA(σ)) · gB(φB(σ)), (5.1)\nwhere the factors mAB, fA and gB are the indicator functions for consistent interleavings and relative rankings, [Sγπ]AB, [Sγπ]A and [Sγπ]B, respectively.\nThe single layer decomposition of Proposition 18 can be turned into a recursive decomposition for partial ranking observations over arbitrary binary hierarchies, which establishes our main result. In particular, given a partial ranking Sγπ and a prior distribution which factorizes according to a hierarchy H, we first condition the topmost interleaving distribution by zeroing out all parameters corresponding to interleavings which are not consistent with Sγπ, and normalizing the distribution. We then need to condition the subhierarchies HA and HB on relative rankings of A and B which are consistent with Sγπ, respectively. Since these consistent sets, [Sγπ]A and [Sγπ]B, are partial rankings themselves, the same algorithm for conditioning on a partial ranking can be applied recursively to each of the subhierarchies HA and HB. To be precise, we show that:\nTheorem 19. Every partial ranking is completely decomposable (P ⊂ C).\nprcondition (Prior hprior, Hierarchy H, Observation Sγπ = Ω1|Ω2| . . . |Ωr) if isLeaf(H) then\nforall σ do hpost(σ)← { hprior(σ) if σ ∈ Sγπ\n0 otherwise ;\nNormalize (hpost) ; return (hpost);\nelse forall τ do\nmpost(τ)← { mprior(τ) if τ ∈ [Sγπ]AB\n0 otherwise ;\nNormalize (mpost) ; f(φA)←prcondition (fprior, HA, [Sγπ]A) ; g(φB)←prcondition (gprior, HB , [Sγπ]B) ; return (mpost, fpost, gpost);\nAlgorithm 1: Pseudocode for prcondition, an algorithm for recursively conditioning a hierarchical riffle independent prior distribution on partial ranking observations. See Definitions 16 and 17 for [Sγσ]A, [Sγσ]B , and [Sγσ]AB . The runtime of prcondition is O(n · |H|), where |H| is the number of model parameters. Input: All parameter distributions of the prior hprior represented in explicit tabular form, and an observation Sγπ in the form of a partial ranking. Output: All parameter distributions of the posterior hpost represented in explicit tabular form.\nSince the proof of Theorem 19 is fairly straight forward given the form of the factorization (Equation 5.1), it is deferred to the Appendix. As a consequence of Theorem 19 and Proposition 12, conditioning on partial ranking observations can be performed efficiently. See Algorithm 1 for details on our recursive conditioning algorithm.\nWhat is the running time complexity of conditioning on a partial ranking? The recursion of Algorithm 1 operates on each parameter distribution once, setting the probabilities of the interleavings or relative rankings in each such distribution to either zero or not, then normalizing. To decide whether to zero out a probability or not, one must check a partial ranking for consistency against either an interleaving or relative ranking, which requires at most O(n) time. Therefore, in total, Algorithm 1 requires O(n · |H|) time, where |H| is the total number of model parameters. Notice that the complexity of conditioning depends linearly on the complexity of the prior — whenever the prior distribution can be compactly represented, efficient inference for partial ranking observations is also possible. As we have stated in Section 2, |H| can in general scale exponentially in n, but for thin chain models, in which the number of items factored out of the model at each stage is never more than a small constant k, verifying interleaving or relative ranking consistency can be performed in constant time, implying that the conditioning operation is linear in the number of model parameters, and guaranteed to be polynomial in n.\nExample 20. In this example, we consider conditioning the APA distribution from Example 6 on the observation O that “Candidate 3 is ranked in first place,” which can also be represented as the partial ranking O = 3|1, 2, 4, 5. Recall that candidate 3 was Charles Kiesler, who was a research psychologist.\nIn Figure 5(a) we show again the structure and parameters of the prior distribution for the APA election data, highlighting in particular the interleavings and relative rankings which\nare consistent with O. For example, of the possible interleavings of research psychologists (D) with clinical psychologists (E), the interleavings that are consistent with O are those which rank a research psychologist first among the research and clinical psychologists. There are therefore only three consistent interleavings: D|D|E|E, D|E|D|E, and D|E|E|D.\nConditioning on O sets all relative rankings and interleavings which are not consistent with O to zero and normalizes each resulting parameter distribution. The resulting riffle independent representation of the posterior distribution is shown in Figure 5(b)."
    }, {
      "heading" : "5.1 An Impossibility Result",
      "text" : "It is interesting to consider what completely decomposable observations exist beyond partial rankings. One of our main contributions is to show that there are no such observations.\nTheorem 21 (Converse of Theorem 19). Every completely decomposable observation takes the form of a partial ranking (C ⊂ P).\nTogether, Theorems 19 and 21 form a significant insight into the nature of rankings, showing that the notions of partial rankings and riffled independence are deeply connected. In fact, our result shows that it is even possible to define partial rankings via complete decomposability!\nAs a practical matter, Theorem 21 shows that there is no algorithm based on simple multiplicative updates to the parameters which can exactly condition on observations which do not take the form of partial rankings. The computational complexity of conditioning on observations which are not partial rankings remains open. We conjecture that approximate inference approaches may be necessary for efficiently handling more complex observations."
    }, {
      "heading" : "5.2 Proof of the Impossiblity Result (Theorem 21)",
      "text" : "We now turn to proving Theorem 21. Since this proof is significantly longer and less obvious than the proof for its converse (Theorem 19), we sketch the main ideas that drive the proof here and refer interested readers to details in the Appendix.\nRecall that the definition of the linear span of a set of vectors in a vector space is the intersection of all linear subspaces containing that set of vectors. To prove Theorem 21, we introduce analogous concepts of the span of a set of rankings.\nDefinition 22 (rspan and pspan). Let X ⊂ Sn be any collection of rankings. We define pspan(X) to be the intersection of all partial rankings containing X. Similarly, we define rspan(X) to be the intersection of all completely decomposable observations containing X. More formally,\npspan(X) = ⋂\nSγσ:X⊂Sγσ Sγσ, and rspan(X) = ⋂ O:X⊂O, O∈C O.\nFor example, if X = {Corn|Peas|Apples,Apples|Peas|Corn}, it can be checked that the only partial ranking of all three items containing both items of X is the entire set itself. Thus pspan(X) = Corn, Peas,Apples.\nOur proof strategy is to establish two claims: (1) that the pspan of any set is always a partial ranking, and (2) that in fact, the rspan and pspan of a set X are exactly the same sets. Since claim (1) is a fact about partial rankings and does not involve riffled independence, we defer all related proofs to the Appendix. Thus we have:\nLemma 23. For any X ⊂ Sn, pspan(X) is a partial ranking.\nProof. See Appendix.\nThe following discussion will instead sketch a proof of claim (2). We first show, however, that Theorem 21 must hold if it is indeed true that claims (1) and (2) hold.\nProof. (of Theorem 21): Given some O ∈ C, we want to show that O ∈ P. By claim (2), rspan(O) = pspan(O). Since O is an element of C, however, we also have that O = rspan(O), and thus that O = pspan(O). Finally Lemma 23 (claim (2)) guarantees that pspan(O) is a partial ranking, and so we conclude that O ∈ P.\nWe now proceed to establish the claim that rspan(X) = pspan(X). The following proposition lists several basic properties of the rspan that we will use in several of the proofs. They all follow directly from definition so we do not write out the proofs.\nProposition 24.\nI. (Monotonicity) For any X, X ⊂ rspan(X). II. (Subset preservation) For any X,X ′ such that X ⊂ X ′, rspan(X) ⊂ rspan(X ′). III. (Idempotence) For any X, rspan(rspan(X)) = rspan(X).\nOne inclusion of our proof that rspan(X) = pspan(X) follows directly from the fact that P ⊂ C (Theorem 19):\nformPspan(X) X0 ← X; t← 0; while ∃Sγπ, Sγ′π′ ∈ Xt which disagree on the relative ordering of items a1, a2 do\nXt ← ∅ ; foreach Sγσ ∈ Xt do\nAdd any partial ranking obtained by deleting a vertical bar from Sγσ between items a1 and a2 to Xt;\nt← t+ 1; return (any element of Xt) ;\nAlgorithm 2: Pseudocode for computing pspan(X). formPspan(X) takes a set of partial rankings (or full rankings) X as input and outputs a partial ranking. This algorithm iteratively deletes vertical bars from elements of X until they are in agreement. Note that it is not necessary to keep track of t, but we do so here to ease notation in the proofs. Nor is this algorithm the most direct way of computing pspan(X), but again, it simplifies the proof of our main theorem.\nLemma 25. For any subset of orderings, X, rspan(X) ⊂ pspan(X).\nProof. Fix a subset X ⊂ Sn and let π be any element of rspan(X). We would like to show π to be an element of pspan(X). Consider any partial ranking Sγσ ∈ P which covers X (i.e., σ′ ∈ Sγσ for all σ′ ∈ X). We want to see that π ∈ Sγσ. By Theorem 19, P ∈ C, and therefore, Sγσ ∈ C. Since π ∈ rspan(X), and σ′ ∈ Sγσ for all σ′ ∈ X, we conclude, by definition of rspan, that π ∈ Sγσ. Since this holds for any partial ranking covering X, π ∈ pspan(X).\nWhat remains is the task of establishing the reverse inclusion:\nProposition 26. For any subset of orderings, X, rspan(X) ⊃ pspan(X).\nTo prove Proposition 26, we consider the problem of computing the partial ranking span (pspan) of a given set of rankings X. In Algorithm 2, we show a simple procedure based on iteratively finding rankings in X which disagree on the pairwise ranking of two items, and replacing those rankings by a partial ranking in which a vertical bar between those two elements have been removed. We show that this algorithm provably outputs the correct result.\nProposition 27. Given a set of rankings X as input, Algorithm 2 outputs pspan(X).\nProof. See Appendix.\nAs a final step before being able to prove Proposition 26, we prove the following two technical lemmas which relate the computation of the pspan in Algorithm 2 to riffled independence, and really form the heart of our argument. In particular, for a completely decomposable observation O ∈ C, Lemma 28 below shows how a ranking contained in O can “force” other rankings to also be contained in O.\nLemma 28. Let O ∈ C and suppose there exist π1, π2 ∈ O which disagree on the relative ranking of items i, j ∈ Ω. Then the ranking obtained by swapping the relative ranking of items i, j within any π3 ∈ O must also be contained in O.\nProof. Let h be the indicator distribution corresponding to the observation O. We will show that swapping the relative ranking of items i, j in π3 will result in a ranking which is assigned nonzero probability by h, thus showing that this new ranking is contained in O.\nLet A = {i, j} and B = Ω\\A. Since O ∈ C, h must factor riffle independently according to the partition (A,B). Thus,\nh(π1) = m(τAB(π1)) · f(φA(π1)) · g(φB(π1)) > 0, and h(π2) = m(τAB(π2)) · f(φA(π2)) · g(φB(π2)) > 0.\nSince π1 and π2 disagree on the relative ranking of items in A, this factorization implies in particular that both f(φA = i|j) > 0 and f(φA = j|i) > 0. Since h(π3) > 0, it must also be that each of m(τAB(π3)), f(φA(π3)), and g(φB(π3)) have positive probability. We can therefore swap the relative ranking of A, φA, to obtain a new ranking which has positive probability since all of the terms in the decomposition of this new ranking have positive probability.\nLemma 29 below provides conditions under which removing a vertical bar from one of the rankings in X will not change the support of a completely riffle independent distribution. To illustrate with an example, consider a completely decomposable observation O which contains the partial ranking Sγπ = Corn, Peas|Apples,Oranges as a subset. What Lemma 29 guarantees is that, if, in addition, there exists any element π̃ in O which disagrees with Sγπ on the relative ordering of, say, Peas and Oranges, then in fact the partial ranking Sγ′π\n′ ⊂= Corn, Peas,Apples,Oranges (with the bar removed from Sγπ) must also be a subset of O. Formally,\nLemma 29. Let Sγπ = Ω1| . . . |Ωi|Ωi+1| . . . |Ωk be a partial ranking on item set Ω, and Sγ′π\n′ = Ω1| . . . |Ωi ∪ Ωi+1| . . . |Ωk, the partial ranking in which the sets Ωi and Ωi+1 are merged. Let a1 ∈ ∪ij=1Ωj and a2 ∈ ∪kj=i+1Ωj. If O is any element of C such that Sγπ ⊂ O and there additionally exists a ranking π̃ ∈ O which disagrees with Sγπ on the relative ordering of a1, a2, then Sγ′π′ ⊂ O.\nProof. The key strategy in our proof of Lemma 29 is to argue that large subsets of rankings must be contained in a completely decomposable observation O by decomposing rankings into transpositions and invoking the technical lemma from above (Lemma 28) repeatedly. See the Appendix for details.\nWe now can use Lemma 29 to show that the reverse inclusion of Proposition 26 also holds, establishing that the two sets rspan(X) and pspan(X) are in fact equal and thereby proving the desired result, that C ⊂ P.\nProof. (of Proposition 26) At each iteration t, Algorithm 2 produces a set of partial rankings, Xt. We denote the union of all partial rankings at time t as X̃t ≡ ⋃ Sγσ∈Xt Sγσ. Note that X̃0 = X and X̃T = pspan(X). The idea of our proof will be to show that at each iteration t, the following set inclusion holds: rspan(X̃t) ⊂ rspan(X̃t−1). If indeed this holds, then\nafter the final iteration T , we will have shown that:\npspan(X) = X̃T , (Proposition 27)\n⊂ rspan(X̃T ), (Monotonicity, Proposition 24) ⊂ rspan(X̃0), (since rspan(X̃t) ⊂ rspan(X̃t−1), shown below), ⊂ rspan(X) (X̃0 = X, see Algorithm 2)\nwhich would prove the Proposition. It remains now to show that rspan(X̃t) ⊂ rspan(X̃t−1). We claim that X̃t ⊂ rspan(X̃t−1). Let σ ∈ X̃t. If σ ∈ X̃t−1, then since X̃t−1 ⊂ rspan(X̃t−1), we have σ ∈ rspan(X̃t−1) and the proof is done. Otherwise, σ ∈ X̃t\\X̃t−1. In this second case, we use the fact that at iteration t, the vertical bar between Ωi and Ωi+1 was deleted from the partial ranking Sγπ = Ω1| . . . |Ωi|Ωi+1| . . . |Ωk (which is a subset of X̃t−1) to form the partial ranking Sγ′π\n′ = Ω1| . . . |Ωi ∪ Ωi+1| . . . |Ωk. (which is a subset of X̃t). Furthermore, in order for the vertical bar to have been deleted by the algorithm, there must have existed some partial ranking (and therefore some full ranking ω′) that disagreed with Sγπ on the relative ordering of items a1, a2 on opposite sides of the bar. Since σ ∈ X̃t\\X̃t−1 we can assume that σ ∈ Sγ′π′.\nWe now would like to apply Lemma 29. Note that for any O ∈ C such that X̃t−1 ⊂ O, we also have Sγπ ⊂ O, since Sγπ ⊂ X̃t−1. An application of Lemma 29 then shows that Sγ′π\n′ ⊂ O and therefore that σ ∈ O. We have shown in fact that σ ∈ O holds for any observation O ∈ C such that X̃t−1 ⊂ O, and therefore taking the intersection of supports over all O ∈ C, we see that X̃t ⊂ rspan(X̃t−1). Taking the rspan of both sides yields:\nrspan(X̃t) ⊂ rspan(rspan(X̃t−1)), (Subset preservation, Proposition 24) ⊂ rspan(X̃t−1). (Idempotence, Proposition 24)"
    }, {
      "heading" : "5.3 Going Beyond Subset Observations",
      "text" : "Though we have stated all of our results so far for subset observations, we now comment on what our theory would look like if we had considered general likelihood functions. In order to avoid confusion, we here refer to a more general class of functions that we call completely decomposable functions, instead of the completely decomposable subset observations of Definition 11.\nDefinition 30. A function h : Sn → R is called a completely decomposable function if it factors riffle independently with respect to every hierarchy over the item set Ω. We denote the collection of all possible completely decomposable functions as C̃.\nAs we discuss, C and C̃ are very nearly the same. It is quite simple to restate Theorem 19 with respect to the general case of completely decomposable functions:\nTheorem. Every partial ranking indicator function is a completely decomposable function.\nUnfortunately, the proof of its converse (Theorem 21) does not easily generalize, and instead can only be used to show that the support ({σ ∈ Sn : h(σ) > 0}) of every completely decomposable function is a partial ranking. It is natural, however, to suspect that a full converse does indeed exist — that every completely decomposable function is proportional to the indicator function of some partial ranking. In fact, this suspected converse only almost holds. We have:\nTheorem. If h is any completely decomposable function supported on a partial ranking Sγπ = Ω1| . . . |Ωr where |Ωi| 6= 2 for all i = 1, . . . , r, then h is proportional to the indicator function on Sγπ.\nProof. See Appendix.\nExample 31. For completely decomposable functions, it is not possible to do away with the assumption that |Ωi| 6= 2 for all i. As an example, the function defined below as:\nh(σ) =  2/3 if σ = Corn|Peas|Apples 1/3 if σ = Peas|Corn|Apples 0 otherwise ,\nis supported on the partial ranking Sγπ = Corn, Peas|Apples (where |Ω1| = 2), and is not proportional to any indicator function (i.e., it is not uniform on rankings which are not assigned positive probability).\nHowever, it is still possible to show that h is a completely decomposable function. To prove so, it is necessary to establish only three things: that {Corn, Peas} and {Apples} are riffle independent, that {Corn,Apples} and {Peas} are riffle independent, and that {Peas,Apples} and {Corn} are riffle independent. For example, with respect to the partitioning into sets A = {Corn,Apples} and B = {Peas}, we see that\nh(σ) = m(τAB(σ)) · f(φA(σ)) · g(φB(σ)),\nwhere:\nm(τAB) =  2/3 if τAB = A|B|A 1/3 if τAB = B|A|A 0 if τAB = A|A|B , f(σA) = { 1 if σ{AC} = A|C 0 otherwise , g(σB) = 1.\nTherefore, when |Ωi| = 2, it is possible to have completely decomposable functions which are not uniform on their supports."
    }, {
      "heading" : "5.4 Conditioning on Noisy Observations",
      "text" : "We conclude this section with a remark on handling noise in observations. While we have assumed in this paper that observed partial rankings are always consistent with a user’s underlying full ranking, there are situations in which one may wish to model a noisier setting, where the partial rankings may be misreported with some small probability. A natural model that accounts for noise, for example, might be:\nL(O|σ) = { 1− ε if σ ∈ O ε\n|O|−1 otherwise . (5.2)\nIf a prior distribution factorizes with respect to a hierarchy H, then conditioning on the noisy likelihood of Equation 5.2 results in a posterior distribution which can be written as a weighted mixture of the prior distribution and the posterior that would have resulted from conditioning on a noise-free observation. While each component of this posterior distribution factorizes with respect to H, the mixture itself does not factor in general (and should not factor according to our theory). As a result, iteratively conditioning on multiple partial rankings according to the noisy likelihood function above would quickly lead to an unmanageable number of mixture components. We therefore believe that approximate inference methods for conditioning on multiple noisy partial ranking observations is a fruitful area for further research."
    }, {
      "heading" : "6. Model Estimation from Partially Ranked Data",
      "text" : "In many ranking based applications, datasets are predominantly composed of partial rankings rather than full rankings due to the fact that for humans, partial rankings are typically easier and faster to specify. In addition, many datasets are heterogeneous, containing partial ranking of different types. For example, in the American Psychological Assoication as well as the Irish House of Parliament elections, voters are allowed to specify their top-k candidate choices for any value of k (see Figures 7(a) and 7(b)). In this section we use the efficient inference algorithm proposed in Section 5 for estimating a riffle independent model from partially ranked data. Because estimating a model using partially ranked data is typically considered to be more difficult than estimating one using only full rankings, a common practice (e.g., see Huang & Guestrin, 2010) has been to simply ignore the partial rankings in a dataset. The ability of a method to incorporate all of the available data however, can lead to significantly improved model accuracy as well as wider applicability of that method. In this section, we propose the first efficient method for estimating the structure and parameters of a hierarchical riffle independent model from heterogeneous datasets consisting of arbitrary partial ranking types. Central to our approach is the idea that given someone’s partial preferences, we can use the efficient algorithms developed in the previous section to infer his full preferences and consequently apply previously proposed algorithms which are designed to work with full rankings."
    }, {
      "heading" : "6.1 Censoring Interpretations of Partial Rankings",
      "text" : "The model estimation problem for full rankings is stated as follows. Given i.i.d. training examples σ(1), . . . , σ(m) (consisting of full rankings) drawn from a hierarchical riffle independent distribution h, recover the structure and parameters of h.\nIn the partial ranking setting, we again assume i.i.d. draws, but that each training example σ(i) undergoes a censoring process producing a partial ranking consistent with σ(i). For example, censoring might only allow for the ranking of the top-k items of σ(i) to be observed. While we allow for arbitrary types of partial rankings to arise via censoring, we make a common assumption that the partial ranking type resulting from censoring σ(i) does not depend on σ(i) itself."
    }, {
      "heading" : "6.2 Algorithm",
      "text" : "We treat the model estimation from partial rankings problem as a missing data problem. As with many such problems, if we could determine the full ranking corresponding to each observation in the data, then we could apply algorithms which work in the completely observed data setting. Since full rankings are not given, we utilize an Expectation-Maximization (EM) approach in which we use inference to compute a posterior distribution over full rankings given the observed partial ranking. In our case, we then apply the algorithms from Huang and Guestrin (2010, 2012) which were designed to estimate the hierarchical structure of a model and its parameters from a dataset of full rankings.\nGiven an initial model h and a collection of training examples {O(1),O(2), . . . ,O(m)} consisting of partial rankings, our EM-based approach alternates between the following two steps until convergence is achieved.\n• (E-step): For each observation, O(i) = Sγ(i)π(i), in the training examples, we use inference to compute a posterior distribution over the full ranking σ that could have generated O(i) via censoring, h(σ|O(i) = Sγ(i)π(i)). Since the observations take the form of partial rankings and are hence completely decomposable, we use the efficient algorithms in Section 5 to perform the E-step. • (M-step): In the M-step, one maximizes the expected log-likelihood of the training\ndata with respect to the model. When the hierarchical structure of the model has been provided, or is known beforehand, our M-step can be performed using standard methods for optimizing parameters. When the structure is unknown, we use a structural EM approach, which is analogous to methods from the graphical models literature for structure learning from incomplete data (Friedman, 1997, 1998).\nUnfortunately, the (riffled independence) structure learning algorithm of Huang and Guestrin (2010) is unable to directly use the posterior distributions computed from the E-step. Instead, observing that sampling from riffle independent models can be done efficiently and exactly (as opposed to, for example, MCMC methods), we simply sample full rankings from the posterior distributions computed in the E-step and pass these full rankings into the structure learning algorithm of Huang and Guestrin (2010). The number of samples that are necessary, instead of scaling factorially, scales according to the number of samples required to detect riffled independence (which under mild assumptions is polynomial in n, Huang & Guestrin, 2010)."
    }, {
      "heading" : "7. Related Work",
      "text" : "Rankings and permutations have recently become an active area of research in machine learning due in part to the hinge role that they play in information retrieval and preference elicitation. Algorithms such as the RankSVM (Joachims, 2002) and RankBoost (Freund, Iyer, Schapire, & Singer, 2003), for example, have been successful in the large scale ranking problems that appear in web search. The main aims of our work differ from these web scale settings however — instead of seeking a single ‘optimal’ ranking with respect to some objective function, we seek an understanding of a large collection of rankings via density estimation. In the following, we outline two major lines of research which have influenced our work."
    }, {
      "heading" : "7.1 Additive and Multiplicative Decompositions",
      "text" : "Our paper builds in particular upon a thread of recent work on tractable models for permutation data based on function decompositions. Kondor, Howard, and Jebara (2007) and Huang, Guestrin, and Guibas (2008, 2009) considered additive decompositions of a distribution into a weighted sum of Fourier basis functions. These papers show that low-frequency Fourier assumptions can often be effective for coping with the representational complexity of working with distributions over permutations. They show in particular that conditioning prior distributions on the ‘low frequency’ likelihood functions that often arise in multiobject tracking problems can be performed especially efficiently.\nUnfortunately, low frequency assumptions are not as applicable for distributions defined over rankings, and to address ranking problems specifically, Huang and Guestrin (2009, 2010) introduced the concept of riffled independence as a useful generalization of probabilistic independence for rankings. Using multiplicative decompositions based on riffled independence, we showed that it is possible to learn the hierarchical structure of a model given a fully ranked dataset. While our previous papers on the topic of riffled independence focused more on problems related to efficiently representing distributions, the main focus of our current paper lies in efficient reasoning/inference and tackling human task complexity by considering partial rankings.\nIt is interesting to note that while it is natural and efficient to condition a Fourier based representation on low-frequency observations (involving a very small number of items) such as O =“Alice is in third place”, a multiplicative decomposition based on riffled independence would not be able to efficiently condition on the same observation. On the other hand, multiplicative decompositions allow us to condition on top-k observations efficiently (independently of the size of k), whereas top-k observations would be difficult to handle in a Fourier theoretic setting (except for very small k)."
    }, {
      "heading" : "7.2 Mallows Models",
      "text" : "Our work also fits into a larger body of research about the well known Mallows distribution over rankings, parameterized by:\nh(σ;φ, σ0) ∝ φ−dτ (σ,σ0), (7.1)\nwhere the function dτ refers to the Kendall’s tau distance metric on rankings. A Mallows distribution (Equation 7.1) can always be shown to be a special case of a hierarchical riffle independent model in which items are sequentially factored out of the model one by one (Huang, 2011) (see Figure 6).\nMallows models (as well as other similar distance based models) have the advantage that they can compactly represent distributions for very large n, and admit conjugate prior distributions (Meila, Phadnis, Patterson, & Bilmes, 2007). Estimating parameters has been a popular problem for statisticians — recovering the optimal σ0 from data is known as the consensus ranking or rank aggregation problem and is known to be NP -hard (Bartholdi, Tovey, & Trick, 1989). Many authors have focused on approximation algorithms instead.\nLike Gaussian distributions, Mallows models tend to lack flexibility, and so Lebanon and Mao (2008) propose a nonparametric model of ranked (and partially ranked) data based on placing weighted Mallows kernels on top of training examples, which, as they show, can\nrealize a far richer class of distributions, and can be learned efficiently. However, they do not address the inference problem, and it is not immediately clear in many Mallows models papers whether one can efficiently perform inference operations like marginalization and conditioning in such models. Riffle independent models, on the other hand, encompass a class of distributions which is both rich as well as interpretable, and additionally, we have identified precise conditions under which efficient conditioning is possible (the conditions being that the observations take the form of partial rankings).\nThere are several recent works to model partial rankings using Mallows based models. Busse, Orbanz, and Buhmann (2007) learned finite mixtures of Mallows models from topk data (also using an EM approach). Lebanon and Mao (2008), as we have mentioned, developed a nonparametric model based on Mallows models which can handle arbitrary types of partial rankings. In both settings, a central problem is to marginalize a Mallows model over all full rankings which are consistent with a particular partial ranking. To do so efficiently, both papers rely on the fact (first shown in Fligner & Verducci, 1986) that this marginalization step can be performed in closed form. This closed form equation of Fligner and Verducci (1986), however, can be seen as a very special case of our setting since Mallows models can always be shown to factor riffle independently according to a chain structure. Specifically, to compute the sum over rankings which are consistent with a partial ranking Sγσ, it is necessary to condition on Sγσ, and to compute the normalization constant of the resulting function. The conditioning step can be performed using the methods that we have described in this paper, and the normalization constant can be computed by multiplying the normalization constant of each factor of the hierarchical decomposition. Thus, instead of resorting to the more complicated mathematics of inversion combinatorics, our theory of complete decomposability offers a simple conceptual way to understand why Mallows models can be conditioned efficiently on partial ranking observations.\nFinally in recent related work, Lu and Boutilier (2011) considered an even more general class of observations based on DAG (directed acyclic graph) based observations in which probabilities of rankings which are not consistent with a DAG of relative ranking relations are set to zero. Lu and Boutilier show in particular that the conditioning problem for their DAG-based class of observations is #P -hard. They additionally propose an efficient rejection sampling method for performing probabilistic inference within the general class of DAG observations and prove that the sampling method is exact for the class of partial rankings that we have discussed in this paper."
    }, {
      "heading" : "8. Experiments",
      "text" : "In this section, we demonstrate our method for learning hierarchical riffle independent models from partial rankings on simulated data as well as real datasets taken from different domains. In all experiments, we initialize distributions to be uniform, and do not use random restarts."
    }, {
      "heading" : "8.1 Datasets",
      "text" : "In addition to roughly 5000 full rankings, the APA dataset has over 10,000 top-k rankings of 5 candidates. In previous work, we had used only the full rankings of the APA data (Huang & Guestrin, 2010, 2012), but now we are able to use the entire dataset. Figure 7(a) plots, for each k ∈ {1, . . . , 5}, the number of ballots in the APA data of length k.\nLikewise, the Meath dataset (Gormley & Murphy, 2007) which was taken from the 2002 Irish Parliament election has over 60,000 top-k rankings of 14 candidates. As with the APA data, we had used only the full rankings of the Meath data in previous work, but here we use the entire dataset. Figure 7(b) plots, for each k ∈ {1, . . . , 14}, the number of ballots in the Meath data of length k. In particular, note that the vast majority of ballots in the dataset consist of partial rather than full rankings, with over half of the electorate preferring to list only their favorite three or four candidates. We can run inference (Algorithm 1) on over 5000 top-k examples for the Meath data in 10 seconds on a dual 3.0 GHz Pentium machine with an unoptimized Python implementation. Using ‘brute force’ inference, we estimate that the same job would require roughly one hundred years.\nWe extracted a third dataset from a database of searchtrails collected by White and Drucker (2007), in which browsing sessions of roughly 2000 users were logged during 2008- 2009. In many cases, users are unlikely to read articles about the same news story twice, and so it is often possible to think of the order in which a user reads through a collection of articles as a top-k ranking over articles concerning a particular story/topic. The ability to model visit orderings would allow us to make long term predictions about user browsing behavior, or even recommend ‘curriculums’ over articles for users. We ran our algorithms on roughly 300 visit orderings for the eight most popular posts from www.huffingtonpost.com concerning ‘Sarah Palin’, a popular subject during the 2008 U.S. presidential election. Since no user visited every article, there are no full rankings in the data and thus there does not even exist the option of learning using only the subset of full rankings."
    }, {
      "heading" : "8.2 APA Structure Learning Results",
      "text" : "Due to the unordinarily large number of full rankings in the APA data, the gains made by additionally using partially ranked data are insignificant. To better illustrate the benefits of partial rankings, we subsampled a dataset of 300 rankings (including both full and partial rankings) and present results with this smaller dataset. Performing structure learning using only the full rankings of these 300 training examples (consisting of roughly 100 examples), one obtains the structure in Figure 8(a), which can be seen to not match the ‘correct’ structure of Figure 3(a) which was learned using 5000 full rankings. Figures 8(b) and 8(c)\nplot the results of our EM algorithm with the former displaying the resulting structure after just a single EM iteration and the latter the result after structural convergence, which occurs by the third iteration, showing that our method can learn the ‘correct’ structure given just 300 training examples.\nWe compared our EM algorithm against two alternative baseline approaches that we refer to in our plots as FlatEM and Uniform Fill-in. The FlatEM algorithm is the same as the EM algorithm above except for two details: (1) it performs conditioning exhaustively instead of exploiting the factorized model structure, and (2) it performs the M-step without sampling. The Uniform Fill-in approach treats every top-k ranking in the training set as a uniform collection of votes for all of the full rankings consistent with that top-k ranking, and is accomplished by using just one iteration of our EM algorithm.\nIn Figure 9(a) we plot test set loglikelihoods corresponding to each approach, with EM and FlatEM having almost identical results and both performing much better than the Uniform Fill-in approach. On the other hand, Figure 9(b), which compares running times of the three approaches, shows that FlatEM can be far more costly (for most datasets, it cannot even be run in a reasonable amount of time).\nTo verify that partial rankings do indeed make a difference in the APA data, we plot the results of estimating a model from the subsets of APA training data consisting of top-k rankings with length larger than some fixed k. Figures 9(c) and 9(d) show the log-likelihood and running times for k = 0, 1, 2, 3 with k = 0 being the entire training set and k = 3 being the subset of training data consisting only of full rankings. As our results show, including partial rankings does indeed help on average for improving test log-likelihood (with diminishing returns)."
    }, {
      "heading" : "8.3 Structure Discovery with EM with Larger n.",
      "text" : "Our experiments have led to several observations about using EM for learning with partial rankings. First, we observe that typical runs converge to a fixed structure quickly, with no more than three EM iterations. Figure 10 shows the progress of EM on the Sarah Palin data, whose structure converges by the third iteration. As expected, the log-likelihood increases at each iteration, and we remark that the structure becomes more interpretable — for example, the leaf set {0, 2, 3} corresponds to the three posts about Palin’s wardrobe before the election, while the posts from the leaf set {1, 4, 6} were related to verbal gaffes made by Palin during the campaign. Notice that this structure is discovered purely using data about visit orders and that no text information was used in our experiments.\nSecondly, the number of EM iterations required to reach convergence in log-likelihood depends on the types of partial rankings observed. We ran our algorithm on subsets of the Meath dataset, each time training on m = 2000 rankings all with length larger than\nsome fixed k. Figure 11(a) shows the number of iterations required for convergence as a function of k (with 20 bootstrap trials for each k). We observe fastest convergence for datasets consisting of almost-full rankings and slowest convergence for those consisting of almost-empty rankings, with almost 25 iterations necessary if one trains using rankings of all types. Finally we remark that the model obtained after the first iteration of EM is interesting and can be thought of as the result of pretending that each voter is completely ambivalent regarding the n− k unspecified candidates."
    }, {
      "heading" : "8.4 The Value of Partial Rankings",
      "text" : "We now verify again with larger n that using partial rankings in addition to full rankings allows us to achieve better density estimates. We first learned models from synthetic data drawn from a hierarchy, training using 343 full rankings plus varying numbers of partial ranking examples (ranging between 0-64,000). We repeat each setting with 20 bootstrap trials, and for evaluation, we compute the log-likelihood of a testset with 5000 examples. For speed, we learn a structure H only once and fix H to learn parameters for each trial. Figure 11(b), which plots the test log-likelihood as a function of the number of partial rankings made available to the training set, shows that we are indeed able to learn more accurate distributions as more and more data in the form of partial rankings are made available."
    }, {
      "heading" : "8.5 Comparing to a Nonparametric Model",
      "text" : "Comparing the performance of riffle independent models to other approaches was not possible in previous work since we had not been able to handle partial rankings. Using the methods developed in our current paper, however, we compare riffle independent models with the state-of-the-art nonparametric estimator of Lebanon and Mao (2008) (to which we hereby refer as the LM08 estimator) on the same data (setting their regularization parameter to be C =1,2,5, or 10 via a validation set). Figure 11(b) shows (naturally) that when the data are drawn synthetically from a riffle independent model, then our EM method significantly outperforms the LM08 estimator. We remark that in theory, the LM08 is guaranteed to catch up in performance (under appropriate conditions) given enough training examples.\nFor the Meath data, which is only approximately riffle independent, we trained on subsets of size 5,000 and 25,000 (testing on remaining data). For each subset, we evaluated our EM algorithm for learning a riffle independent model against the LM08 estimator when (1) using only full ranking data, and (2) using all data. As before, both methods do better when partial rankings are made available.\nFor the smaller training set, the riffle independent model performs as well or better than the LM’08 estimator. For the larger training set of 25,000, we see that the nonparametric method starts to perform slightly better on average, the advantage of a nonparametric model being that it is guaranteed to be consistent, converging to the correct model given enough data. The advantage of riffle independent models, however, is that they are simple, interpretable, and can highlight global structures hidden within the data."
    }, {
      "heading" : "9. Future Directions",
      "text" : "There remain several possible extensions to the current work. We list a few such open questions and extensions in the following."
    }, {
      "heading" : "9.1 Inference with Incomplete Rankings",
      "text" : "We have shown in this paper that one can exploit riffled independence structure to condition on an observation if and only if it takes the form of a partial ranking. While the space of partial rankings is both rich and useful in many settings, it does not cover an important class of observations: that of incomplete rankings, which are defined to be a ranking (or partial ranking) of a subset of the itemset Ω. For example, Theorem 21 shows that the conditioning problem for pairwise observations of the form “Apples are preferred over Bananas” is nondecomposable. Note that top-k rankings are considered to be “complete” rankings since they implicitly rank all other items in the last n− k positions.\nHow then, can we tractably condition on incomplete rankings? One possible approach is to convert to a Fourier representation using the methods from (Huang & Guestrin, 2012), then conditioning on a pairwise ranking observation using the Fourier domain conditioning algorithm proposed in (Huang et al., 2008). This Fourier domain approach would be useful if one were particularly interested in low-order marginal probabilities of the posterior distributions.\nWhen the Fourier approach is not viable, another option may be to assume that the posterior distribution takes on a particular riffle independent structure (in the same way that mean field methods from the graphical models literature would assume a factorized posterior). The research question of interest is: which hierarchical structure should be used for the purposes of approximating the posterior?"
    }, {
      "heading" : "9.2 Reexamining Data Independence Assumptions",
      "text" : "In this paper, we have assumed throughout that training examples are independent and identically distributed. However in practice these are not always safe assumptions as a number of factors can impact the validity of both. For example, in an internet survey in which a user must perform a series of preference ranking tasks in sequence, a concern is that the user’s prior ranking tasks may bias the results of his future rankings.\nAnother source of bias lies in the reference ranking that may be displayed, in which the user is asked to rearrange items by ‘dragging and dropping’. On the one hand, showing everyone the same reference ranking may bias the resulting data. But on the other hand, showing every user a different reference ranking may mean that the training examples are not exactly identically distributed.\nYet another form of bias lies in the partial ranking types that are reported in data. To formulate our EM algorithm, we have assumed that a user’s preferences does not influence whether he chooses to, say, report a full ranking instead of a top-3 ranking. In practice, however, partial ranking types and user preferences are often correlated. In the Irish elections, for example, where there is typically only one Sinn Fein candidate, those who rank Sinn Fein first are typically more likely to have only reported their top-1 choice.\nUnderstanding, identifying, and finally, learning in spite of the different types of biases that may occur in eliciting preference data remains a fundamental problem in ranking."
    }, {
      "heading" : "9.3 Probabilistic Modeling of Strategic Voting",
      "text" : "It is interesting to consider the differences between the actual vote distributions considered in this paper against the approximate riffle independent distributions. Take the APA dataset, for example, in which the optimal approximation by a riffle independent hierarchy reflects the underlying political coalitions within the organization. Upon comparison between the approximation and the empirical distribution, however, some marked differences arise. For example, the riffle independent approximation underestimates the number of votes obtained by candidate 3 (a research psychologist) who ultimately won the election.\nOne possible explanation for the discrepancy may lie in the idea that voters tend to vote strategically in APA elections, placing stronger candidates of opposing political coalitions lower in the ranking, rather than revealing their true preferences. An interesting line of future work lies in detecting and studying the presence of such strategic voting in election datasets. Open questions include (1) verifying mathematically whether strategic voting does indeed exist in, say, the APA election data, and (2) if so, why the strategic voting effect is not strong enough to overwhelm our riffled independence structure learning algorithms, and (3) how strategic voting can manifest itself in partial ranking votes."
    }, {
      "heading" : "10. Conclusion",
      "text" : "In probabilistic reasoning problems, it is often the case that certain data types suggest certain distribution representations. For example, sparse dependency structure in the data often suggests a Markov random field (or other graphical model) representation (Friedman, 1997, 1998). For low-order permutation observations (depending on only a few items at a time), recent work (Huang et al., 2009; Kondor, 2008) has shown that a Fourier domain representation is appropriate. For preference ranking scenarios, one must contend with ‘human task complexity’ — the difficulty involved for a human to rank a long list of items and often leads to partially, instead of fully ranked data. In this paper, we have shown that when data takes the form of partial rankings, then hierarchical riffle independent models are a natural representation.\nAs with conjugate priors, we showed that a riffle independent model is guaranteed to retain its factorization structure after conditioning on a partial ranking (which can be performed in efficiently). Most surprisingly, our work shows that observations which do not take the form of partial rankings are not amenable to simple multiplicative update based conditioning algorithms. Finally, we showed that it is possible to learn hierarchical riffle independent models from partially ranked data, significantly extending the applicability of previous work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This project was formulated and largely conducted during an internship by Jonathan Huang at Microsoft Research. Additional work was supported in part by ONR under MURI N000140710747, and ARO under MURI W911NF0810242. Carlos Guestrin was funded\nin part by NSF Career IIS-064422. We thank Eric Horvitz, Ryen White, Dan Liebling, and Yi Mao for discussions."
    }, {
      "heading" : "Appendix A. Proofs",
      "text" : "In this appendix, we provide supplementary proofs of some of the theoretical results in this paper.\nA.1 Proof of Theorem 19\nTo prove Theorem 19 (as well as later results), we will refer to rank sets.\nDefinition 32. Given a partial ranking of type γ, we denote the rank set occupied by Ωi by R γ i . Note that R γ i depends only on γ and can be written as R γ 1 = {1, . . . , γ1}, Rγ2 = {γ1 + 1, . . . , γ1 + γ2}, . . . , R γ r = { ∑r−1 i=1 γi + 1, . . . , n}.\nAnd we will refer to the following basic fact regarding rank sets:\nProposition 33. σ ∈ Sγπ = Ω1| . . . |Ωr if and only if for each i, σ(Ωi) = Rγi .\nProof. (of Theorem 19) We use induction on the size of the itemset. The cases n = 1, 2 are trivial since every distribution on S1 or S2 factors riffle independently. We now consider the more general case of n > 2.\nFix a partial ranking Sγπ = Ω1|Ω2| . . . |Ωr of type γ and a binary partition of the item set into subsets A and B. We will show that the indicator function δSγπ factors as:\nδSγπ(σ) = m(τAB(σ)) · f(φA(σ)) · g(φB(σ)), (A.1)\nwhere factors m, f and g are the indicator functions for the set of consistent interleavings, [Sγσ]AB, and the sets of consistent relative rankings, [Sγσ]A and [Sγσ]B, respectively. If Equation A.1 is true, then we will have shown that δSγπ must decompose with respect to the top layer of H. To show that δSγπ decomposes hierarchically, we must also show that the relative ranking factors fA and gB decompose with respect to HA and HB, the subhierarchies over the item sets A and B. To establish this second step (assuming that Equation A.1 holds), note that fA and gB are indicator functions for the restricted partial rankings, [Sγσ]A and [Sγσ]B, which themselves are partial rankings over smaller item sets A and B. The inductive hypothesis (and the fact that A and B are assumed to be strictly smaller sets than Ω) then shows that the functions fA and gB both factor according to their respective subhierarchies.\nWe now turn to establishing Equation A.1. It suffices to prove that the following two statements are equivalent:\nI. The ranking σ is consistent with the partial ranking Sγπ (i.e., σ ∈ Sγπ). II. The following three conditions hold:\n(a) The interleaving τAB(σ) is consistent with Sγπ (i.e., τAB(σ) ∈ [Sγπ]AB), and (b) The relative ranking φA(σ) is consistent with Sγπ (i.e., φA(σ) ∈ [Sγπ]A), and (c) The relative ranking φB(σ) is consistent with Sγπ (i.e., φB(σ) ∈ [Sγπ]B).\n• (I ⇒ II): We first show that σ ∈ Sγπ implies conditions (a), (b) and (c).\n(a) If σ ∈ Sγπ, then for each i,\n|j ∈ Rγi : τAB(j) = A| = |j ∈ R γ i : σ −1(j) ∈ A|, (by Definition 2) = |k ∈ Ωi : k ∈ A|, (by Proposition 33) = |Ωi ∩A|.\nThe same argument (replacing A with B) shows that for each i, we have |j ∈ Rγi : τAB(j) = B| = |Ωi ∩B|. These two conditions (by Definition 17) show that τAB is consistent with Sγπ. (b) If σ ∈ Sγπ, then (by Definition 14) σ ranks items in Ωi before items in Ωj for any i < j. Intersecting each Ωi with A, we also see that σ ranks any item in Ωi ∩ A before any item in Ωj ∩A for all i, j. By Definition 2, φA(σ) also ranks any item in Ωi∩A before any item in Ωj ∩A for all i, j. And finally by Definition 16 again, we see that φA(σ) is consistent with the partial ranking Sγπ. (c) (Same argument as (b)).\n• (II ⇒ I): We now assume conditions (a), (b), and (c) to hold, and show that σ ∈ Sγπ. By Proposition 33 it is sufficient to show that if an item k ∈ Ωi, then σ(k) ∈ Rγi . To prove this claim, we show by induction on i that if an item k ∈ Ωi∩A, then σ(k) ∈ Rγi (and similarly if k ∈ Ωi ∩B, then σ(k) ∈ Rγi ). Base case. In the base case (i = 1), we assume that k ∈ Ω1∩A, and the goal is to show that σ(k) ∈ R1. By condition (a), we have that τAB(σ) ∈ [Sγπ]AB. By Definition 17, this means that: |Ω1 ∩ A| = {j ∈ R1 : [τAB(σ)](j) = A} = {j ∈ R1 : σ−1(j) ∈ A}. In words, there are m = |Ω1∩A| items from A which lie in rank set R1 = {1, . . . , γ1}. To show that an item k ∈ A maps to a rank in R1, we now must show that in the relative ranking of elements in A, k is among the first m. By condition (b), φA(σ) ∈ [Sγπ]A, implying that the item subset Ω1 ∩ A occupies the first m positions in the relative ranking of A. Since k ∈ Ω1 ∩ A, item k is among the first m items ranked by φA(σ) and therefore σ(k) ∈ R1. A similar argument shows that k ∈ Ω1 ∩ B implise that σ(k) ∈ R1. Inductive case. We now show that if k ∈ Ωi ∩ A, then σ(k) ∈ Ri. By condition (b), φA(σ) ∈ [Sγπ]A, implying that the item subset Ωi∩A (and hence, item k) occupies the first m = |Ωi ∩ A| positions in the relative ranking of A beyond the items ∪i−1j=1(Ωj ∩ A). By the inductive hypothesis and mutual exclusivity, these items, together with ∪i−1j=1(Ωj ∩ B) occupy ranks ∪ i−1 j=1Rj , and therefore σ(k) ∈ R` for some ` ≥ i. On the\nother hand, condition (a) assures us that |Ωi ∩ A| = {j ∈ Ri : σ−1(j) ∈ A} — or in other words, that the ranks in Ri are occupied by exactly m items of A. Therefore, σ(k) ∈ Ri. Again, a similar argument shows that k ∈ Ωi ∩B implies that σ(k) ∈ Ri.\nA.2 The pspan of a Set is Always a Partial Ranking\nTo reason about the pspan of a set of rankings, we first introduce some basic concepts regarding the combinatorics of partial rankings. The collection of partial rankings over Ω\nforms a partially ordered set (poset) where Sγ′π′ ≺ Sγπ if Sγπ can be obtained from Sγ′π′ by dropping vertical lines. For example, on S3, we have that 1|2|3 ≺ 12|3. The Hasse diagram is the graph in which each node corresponds to a partial ranking and a node x is connected to node y via an edge if x ≺ y and there exists no partial ranking z such that x ≺ z ≺ y (see Lebanon & Mao, 2008). At the top of the Hasse diagram is the partial ranking 1, 2, . . . , n (i.e., all of SΩ) and at the bottom of the Hasse diagram lie the full rankings. See Figure 13 for an example of the partial ranking lattice on S3.\nLemma 34. [Lebanon & Mao, 2008] Given any two partial rankings Sγπ, Sγ′π′, there exists a unique supremum of Sγπ and Sγ′π′ (a node Sγsupπsup such that Sγπ ≺ Sγsupπsup and Sγ′π′ ≺ Sγsupπsup, and any other such node is greater than Sγsupπsup). Similarly, there exists a unique infimum of Sγπ and Sγ′π′.\nLemma 35. Given two partial rankings Sγπ, Sγ′π′, the relation Sγ′π′ ⊂ Sγπ holds if and only Sγπ lies above Sγ′π′ in the Hasse diagram.\nProof. If Sγπ lies above Sγ′π′ in the Hasse diagram, then Sγ′π′ ⊂ Sγπ is trivial since Sγπ can be obtained by dropping vertical bars of Sγ′π′. Now given that Sγπ does not lie above Sγ′π\n′, we would like to show that Sγ′π′ 6⊂ Sγπ. Let Sγinfπinf be the unique infimum of Sγπ and Sγ′π′ as guaranteed by Lemma 34. By the definition of the Hasse diagram, both Sγπ and Sγπ can be obtained by ‘dropping’ verticals from the vertical bar representation of Sγinfπinf . Since Sγπ does not lie above Sγ′π\n′, there must be a vertical bar that was dropped by Sγ′π′ which was not dropped by Sγπ (if there does not exist such a bar, then Sγ′π\n′ ⊂ Sγπ), and hence there must exist a pair of items i, j separated by a single vertical bar in Sγπ but unseparated in Sγ′π′. Therefore there exists σ ∈ Sγ′π′ such that σ(j) < σ(i) even though there exists no such σ ∈ Sγπ. We conclude that Sγ′π′ 6⊂ Sγπ.\nLemma 36 (Lemma 23 in main body). For any X ⊂ Sn, pspan(X) is a partial ranking.\nProof. Consider any subset X ⊂ Sn. A partial ranking containing every element in X must be an upper bound of every element of X in the Hasse diagram by Lemma 35. By Lemma 34, there must exist a unique least upper bound (supremum) of X, Sγsupπsup, such that for any common upper bound Sγπ of X, Sγπ must also be an ancestor of Sγsupπsup and hence Sγsupπsup ⊂ Sγπ. We therefore see that any partial ranking containing X must be a superset of Sγsupπsup. On the other hand, Sγsupπsup is itself a partial ranking containing X. Since pspan(X) is the intersection of partial rankings containing X, we have pspan(X) = Sγsupπsup and therefore that pspan(X) must be a partial ranking.\nA.3 Proofs for the Claim that rspan(X) = pspan(X)\nTo simplify the notation in some of the remaining proofs, we introduce the following definition.\nDefinition 37 (Ties). Given a partial ranking Sγπ = Ω1| . . . |Ωr, we say that items a1 and a2 are tied (written a1 ∼ a2) with respect to Sγσ if a1, a2 ∈ Ωi for some i.\nThe following basic properties of the tie relation are straightforward.\nProposition 38.\n123\n1|2|3 1|3|2 2|1|3 3|1|2 2|3|1 3|2|1"
    }, {
      "heading" : "1|23 12|3 13|2 2|13 3|12 23|1",
      "text" : "Proposition 39. Given a set of rankings X as input, Algorithm 2 outputs pspan(X).\nProof. We prove three things, which together prove the proposition: (1) that the algorithm terminates, (2) that at each stage the elements of X are contained in pspan(X), and (3) that upon termination, pspan(X) is contained in each element of X.\n1. First we note that the algorithm must terminate in finitely many iterations of the while loop since at each stage at least one vertical bar is removed from a partial ranking, and when all of the vertical bars have been removed from the elements of X, there are no disagreements on relative ordering.\n2. We now show that at any stage in the algorithm, every element of Xt is a subset of the pspan(X). At initialization, of course, if Sγπ ∈ X0, then it is simply a singleton set consisting of an element of X, and therefore Sγπ ⊂ pspan(X). Suppose now that Sγπ ⊂ pspan(X) for every Sγπ ∈ Xt. If Sγπ is replaced by Sγ̃ π̃ in Xt+1, then we want to show that Sγ̃ π̃ ⊂ pspan(X) as well. From Algorithm 2, for some j, if Sγπ = Ω1| . . . |Ωj |Ωj+1| . . . |Ωr, Sγ̃ π̃ can be written as Ω1| . . . |Ωj ∪ Ωj+1| . . . |Ωr, where the vertical bar between Ωj and Ωj+1 is deleted due to the existence of some partial ranking in Xt, Sγ′π′ ∈ Xt which disagrees with Sγπ on the relative ordering of items a1, a2 on opposite sides of the bar. Since Sγπ and Sγ′π′ are both subsets of pspan(X) by assumption, we know that a1 ∼ a2 with respect to pspan(X) (Proposition 38, II). Suppose now that a1 ∈ Ωi and a2 ∈ Ωi′ . Then for any x ∈ Ωi and y ∈ Ωi′ , we have x ∼ a1 and y ∼ a2 with respect to pspan(X) by (III) of Proposition 38. Moreover, by (I, transitivity), we see that x ∼ y with respect to pspan(X). for any two elements of Ωi and Ωi′ . By (IV) of Proposition 38, all the items lying in Ωi,Ωi+1, . . . ,Ωi′ are thus tied with respect to pspan(X) and therefore removing any bar between items a1 and a2 (producing, for example, Sγ̃ π̃) results in a partial ranking which is a subset of pspan(X).\n3. Finally, upon termination, if some ranking σ ∈ X is not contained in some element Sγπ ∈ Xt, then there would exist two items a1, a2 whose relative ranking σ and Sγπ disagree upon, which is a contradiction. Therefore, every element Sγπ ∈ Xt contains every element of X and thus pspan(X) ⊂ Sγπ for every Sγπ ∈ Xt.\nLemma 40. Let Sγπ = Ω1| . . . |Ωi|Ωi+1| . . . |Ωk be a partial ranking on item set Ω, and Sγ′π\n′ = Ω1| . . . |Ωi ∪ Ωi+1| . . . |Ωk, the partial ranking in which the sets Ωi and Ωi+1 are merged. Let a1 ∈ ∪ij=1Ωj and a2 ∈ ∪kj=i+1Ωj. If O is any element of C such that Sγπ ⊂ O and there additionally exists a ranking π̃ ∈ O which disagrees with Sγπ on the relative ordering of a1, a2, then Sγ′π′ ⊂ O.\nProof. We will fix a completely decomposable O and again work with h, the indicator distribution corresponding to O. Let σ ∈ Sγ′π′. To prove the lemma, we need to establish that h(σ) > 0. Let σ0 be any element of Sγπ such that σ0(k) = σ(k) for all k ∈ Ω\\(Ωi∪Ωi+1). Since Sγπ ⊂ supp(h) by assumption, we have that h(σ0) > 0.\nSince σ0 and σ match on all items except for those in Ωi ∪Ωi+1, there exists a sequence of rankings σ0, σ1, σ2, . . . , σm = σ such that adjacent rankings in this sequence differ only by a pairwise exchange of items b1, b2 ∈ Ωi ∪ Ωi+1. We will now show that at each step along this sequence, h(σt) > 0 implies that h(σt+1) > 0, which will prove that h(σ) > 0. Suppose now that h(σt) > 0 and that σt and σt+1 differ only by the relative ranking of items b1, b2 ∈ Ωi ∪Ωi+1 (without loss of generality, we will assume that σt(b2) < σt(b1) and σt+1(b1) < σ\nt+1(b2)). The idea of the following paragraph is to use the previous lemma (Lemma 28) to prove that σt+1 has positive probability and to do so, it will be necessary to argue that there exists some ranking σ′ such that h(σ′) > 0 and σ′(b1) < σ′(b2) (i.e., σ′ disagrees with σt on the relative ranking of b1, b2). Let ω be any element of Sγπ. If a1 ∈ Ωi, rearrange ω such that a1 is ranked first among elements of Ωi. If a2 ∈ Ωi+1, further rearrange ω such that a2 is ranked last among elements of Ωi+1. Note that ω is still an element of Sγπ after the possible rearrangements and therefore h(ω) > 0. We can assume that ω(b2) < ω(b1) since otherwise we will have shown what we wanted to show. Thus the relative ordering of a1, a2, b1, b2 within ω is a1|b2|b1|a2. Note that we treat the case where the items a1, a2, b1, b2 are distinct, but the same argument follows in the cases when a1 = b2 or a2 = b1.\nNow since π̃ disagrees with Sγπ on the relative ordering of a1, a2 by assumption (and hence disagrees with ω), we apply Lemma 28 to conclude that swapping the relative ordering of a1, a2 within ω (obtaining a2|b2|b1|a1) results in a ranking, ω′, such that h(ω′) > 0. Finally, observe that ω and ω′ must now disagree on the relative ranking of a2, b2, and invoking Lemma 28 again shows that we can swap the relative ordering of a2, b2 within ω (obtaining a1|a2|b1|b2) to result in a ranking σ′ such that h(σ′) > 0. This element σ′ ranks b1 before b2, which is what we wanted to show.\nWe have shown that there exist rankings which disagree on the relative ordering of b1 and b2 with positive probability under h. Again applying Lemma 28 shows that we can swap the relative ordering of items b1, b2 within σt to obtain σt+1 such that h(σt+1) > 0, which concludes the proof.\nA.4 Uniformity of C̃ Functions Over a Partial Ranking\nWe have thus far shown that any element of C̃ must be supported on some partial ranking. In the following, we show that (up to a certain class of exceptions), such an element must assign uniform probability to all members of this partial ranking.\nTheorem 41. If h is any completely decomposable function supported on a partial ranking Sγπ = Ω1| . . . |Ωr where |Ωi| 6= 2 for all i = 1, . . . , r, then h is uniform on Sγπ (i.e., h(σ) = 1∏\ni |Ωi| for all σ ∈ Sγπ).\nTo establish Theorem 41, we must establish two supporting results: (1) Lemma 42 which factors h into r smaller completely decomposable functions, each of which is nonzero everywhere on its domain, and (2) Theorem 43 which establishes uniformity for any completely decomposable function which is nonzero everywhere on its domain.\nLemma 42. Any completely decomposable function, h, supported on the partial ranking Sγπ = Ω1| . . . |Ωr, must factor as: h(σ) = ∏r i=1 h(σ(Ωi)), where each factor distribution h(σ(Ωi)) is itself a completely decomposable function on SΩi .\nProof. Since h is completely decomposable, we have that σ(Ωi) is riffle independent of σ(Ω\\Ωi) for each i. Since h is supported on the partial ranking Sγπ = Ω)1| . . . |Ωr, however, the interleaving of Ωi with its complement is deterministic and therefore we conclude in fact that σ(Ωi) is fully independent of σ(Ω\\Ωi). Since σ(Ωi) ⊥ σ(Ω\\Ωi) for each i, we have the factorization: h(σ) = ∏r i=1 h(σ(Ωi)).\nWe now turn to establishing that each factor h(σ(Ωi)) is itself a completely decomposable observation. Fix i = 1 (without loss of generality) and consider any partition of the set Ω1 into subsets A∪B. We would like to see that the sets A and B are riffle independent of each other with respect to h(σ(Ω1)). Since h is assumed to be completely decomposable, we know that A is riffle independent of its complement, B∪(Ω\\Ω1). In other words, if B̃ = B∪(Ω\\Ω1), then the variables φA(σ), τAB̃, φB̃ (the relative ranking of A, the interleaving of A with all remaining items, and the relative ranking of all remaining items, respectively) are mutually independent. We then observe that (1) the interleaving of A and B, τAB, is a deterministic function of the interleaving of τAB̃ and (2) the relative ranking of B, φB, is a deterministic function of φB̃, thus proving that φA, τAB and φB are mutually independent and hence that A and B are riffle independent.\nTheorem 43. Let h a completely decomposable function such that h(σ) > 0 for all σ ∈ Sn for n > 2. Then for any two rankings σ1, σ2 which differ by a single transposition, we have h(σ1) = h(σ2).\nOur proof strategy for Theorem 43 will involve examining the ratio between the two probabilities h(σ1) and h(σ2). We then define an operation transforming σ1 and σ2 into new rankings σ′1 and σ′2 such that the ratio between the rankings is preserved (i.e., h(σ1)/h(σ2) = h(σ′1)/h(σ ′ 2)). By performing a sequence of such ratio-preserving operations, we show that:\nh(σ1) h(σ2) = h(σ2) h(σ1) ,\nfrom which Theorem 43 easily follows.\nWe will use two types of operations which transform a ranking into a new ranking: (1) changing the interleaving of two sets A and B within a ranking σ, and (2), changing the relative ranking of a set A within a ranking σ. More precisely, given a ranking σ and a partitioning of the item set into subsets A and B, we can uniquely index σ as a triplet (τ, πA, πB), where τ = τA,B(σ), πA = φA(σ), and πB = φB(σ). The two operations are defined as follows:\n1. Changing the interleaving of A,B within σ to τ ′: yields the new ranking σ′ which is indexed by (τ ′, πA, πB).\n2. Changing the relative ranking of A (or B) within σ to π′A (or π ′ B): yields the new\nranking σ′ which is indexed by (τ, π′A, πB) [or (τ, πA, π ′ B)].\nIf we use the above operations to obtain from σ′1 and σ′2, we are interested in conditions under which this transformation is ratio-preserving (i.e., h(σ1)/h(σ2) = h(σ′1)/h(σ′2)). The following lemma provides sufficient conditions for ratio-preservation.\nLemma 44. Let h be any completely decomposable function and consider σ1, σ2 ∈ Sn such that h(σ2) > 0. Then for any partitioning of the item set into subsets A and B, we have:\n1. If σ1 and σ2 match on the interleaving of A and B (i.e., τA,B(σ1) = τAB(σ2)), then h(σ1) h(σ2) = h(σ′1) h(σ′2) , where σ′1 and σ ′ 2 are formed by changing the interleaving of the sets A\nand B within σ1 and σ2 to be any new interleaving τ ′.\n2. If σ1 and σ2 match on the relative ranking of A (or B) (i.e., φA(σ1) = φA(σ2) (or φB(σ1) = φB(σ2))), then\nh(σ1) h(σ2) = h(σ′1) h(σ′2) , where σ′1 and σ ′ 2 are formed by changing the\nrelative ranking of set A (or B) within σ1 and σ2 to be any new relative ranking π′A (or π′B).\nProof. Since the proofs of parts 1 and 2 are nearly identical, we just prove part 1 here. Since h ∈ C, the sets A and B are riffle independent by assumption, and hence we have the factorizations:\nh(σ1) h(σ2) = m(τ1) · f(πA1 ) · g(πB2 ) m(τ2) · f(πA2 ) · g(πB2 ) .\nIf σ1 and σ2 match on the interleaving of the sets A and B, then we have that τ = τ1 = τ2, and thus the interleaving terms, m(τ1) and m(τ2) are the same in both the numerator and denominator.\nOn the other hand, if we examine the ratio between h(σ′1) and h(σ′2), we also see that the interleaving terms must cancel:\nh(σ1) h(σ2) = m(τ ′1) · f(πA1 ) · g(πB2 ) m(τ ′2) · f(πA2 ) · g(πB2 ) .\nWe therefore have that: h(σ1)\nh(σ2) = f(πA1 ) · g(piB1 ) f(πA2 ) · g(πB2 ) = h(σ′1) h(σ′2) .\nHaving now established Lemma 44, we turn to establishing three short claims (using the lemma) that will allow us to prove finally prove Theorem 43. It is interesting to note that we require n > 2 (strictly) in claim III below in which we swap the order of i and j in numerator and denominator. The third item k in our proof below can be thought of as playing the role of a dummy variable analogous to the temporary storage variables that one might use in implementing a swap function. The necessity of this third item is precisely why our result does not hold in the special case that n = 2.\nProposition 45. Let h : Sn → R be a completely decomposable function with n > 2 with h(σ) > 0 for all σ ∈ Sn. We have the following equivalences (where in each of the below ratios, entries which have not been explicitly written out are assumed to match identically in both the numerator and denominator).\nI. h(i|j| . . . |k| . . . ) h(j|i| . . . |k| . . . ) = h(i|j|k| . . . ) h(j|i|k| . . . ) .\nII. h(. . . |i| . . . |j| . . . ) h(. . . |j| . . . |i| . . . ) = h(i|j| . . . ) h(j|i| . . . ) .\nIII. h(i|j|k| . . . ) h(j|i|k| . . . ) = h(j|i|k| . . . ) h(i|j|k| . . . ) .\nProof.\nI. Equality holds in I since σ1 and σ2 match on the interleaving of the sets A = {k} with B = Ω\\{k}. Thus we can change the interleaving of A and B in both σ1 and σ2 so that item k is inserted in rank 3 while preserving the ratio.\nII. Equality holds in II since σ1 and σ2 match on the interleaving of the sets A = {i, j} with B = Ω\\{i, j}. Thus we can change the interleaving of A and B in both σ1 and σ2 so that items i and j occupy the first two ranks while preserving the ratio between h(σ1) and h(σ2).\nIII. In the following we use σ1 and σ2 to refer to the arguments in the numerator and denominator, respectively, of the preceding line.\nh(i|j|k| . . . ) h(j|i|k| . . . ) = h(i|k|j| . . . ) h(k|i|j| . . . ) , (since σ1, σ2 match on the relative ranking of {j, k})\n= h(j|i|k| . . . ) h(j|k|i| . . . ) , (since σ1, σ2 match on the interleaving of {j} with Ω\\{j}) = h(i|j|k| . . . ) h(i|k|j| . . . ) , (since σ1, σ2 match on the relative ranking of {i, j}) = h(k|j|i| . . . ) h(k|i|j| . . . ) , (since σ1, σ2 match on the relative ranking of {i, k}) = h(j|i|k| . . . ) h(i|j|k| . . . ) , (since σ1, σ2 match on the interleaving of {k} with Ω\\{k}).\nProof. (of Theorem 43) We want to show that if two rankings differ by a single transposition, then they are assigned equal probability under h. Suppose then that σ2 is obtained from σ1 by swapping the ranks of items i and j. Additionally, let k be any item besides i and j (such an item must exist since n > 2). In the following, we use Proposition 45 to show that h(σ1)/h(σ2) = h(σ2)/h(σ1). As before, entries which have not been explicitly written out are assumed to match identically in both the numerator and denominator.\nh(σ1) h(σ2) = h(. . . |i| . . . |j| . . . ) h(. . . |j| . . . |i| . . . ) = h(i|j| . . . ) h(j|i| . . . ) , (by Prop. 45, Part II)\n= h(i|j| . . . |k| . . . ) h(j|i| . . . |k| . . . ) = h(i|j|k| . . . ) h(j|i|k| . . . ) , (by Prop. 45, Part I) = h(j|i|k| . . . ) h(i|j|k| . . . ) , (by Prop. 45, Part III) = h(j|i| . . . |k| . . . ) h(i|j| . . . |k| . . . ) , (by Prop. 45, Part I) = h(j|i| . . . ) h(i|j| . . . ) = h(. . . |j| . . . |i| . . . ) h(. . . |i| . . . |j| . . . ) , (by Prop. 45, Part II)\n= h(σ2)\nh(σ1) .\nSince we have assumed h(σ1) and h(σ2) > 0, we must conclude that h(σ1) = h(σ2).\nFinally, we assemble all of our supporting results to prove Theorem 41.\nProof. (of Theorem 41) By Lemma 42, a completely decomposable function h must factor as:\nh(σ) = r∏ i=1 h(σ(Ωi)), (A.2)\nwhere each factor distribution h(σ(Ωi)) is itself a completely decomposable function on SΩi . By assumption, |Ωi| 6= 2. If |Ωi| = 1, then its corresponding factor h(σ(Ωi)) must trivially be uniform. Otherwise, we have that |Ωi| > 2. In this latter case, we apply Theorem 43 to h(σ(Ωi)) to show that it must assign equal probability to any two rankings that differ by a single transposition. However, given any rankings σ1, σ2 ∈ SΩi , we can obtain a sequence of transpositions that transforms σ1 into σ2, and therefore, Theorem 43 in fact implies that the factor h(σ(Ωi)) is constant on all inputs. Having proved that each factor in Equation A.2 is constant, we conclude that h must be constant on its support."
    } ],
    "references" : [ {
      "title" : "Aggregation of partial rankings, p-ratings and top-m lists",
      "author" : [ "N. Ailon" ],
      "venue" : "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, SODA ’07,",
      "citeRegEx" : "Ailon,? \\Q2007\\E",
      "shortCiteRegEx" : "Ailon",
      "year" : 2007
    }, {
      "title" : "Voting schemes for which it can be difficult to tell who won",
      "author" : [ "J.J. Bartholdi", "C.A. Tovey", "M. Trick" ],
      "venue" : "Social Choice and Welfare,",
      "citeRegEx" : "Bartholdi et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Bartholdi et al\\.",
      "year" : 1989
    }, {
      "title" : "Cluster analysis of heterogeneous rank data",
      "author" : [ "L.M. Busse", "P. Orbanz", "J. Buhmann" ],
      "venue" : "In The 24th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Busse et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Busse et al\\.",
      "year" : 2007
    }, {
      "title" : "Distance based ranking models",
      "author" : [ "M.A. Fligner", "J.S. Verducci" ],
      "venue" : "Journal of the Royal Statistical Society,",
      "citeRegEx" : "Fligner and Verducci,? \\Q1986\\E",
      "shortCiteRegEx" : "Fligner and Verducci",
      "year" : 1986
    }, {
      "title" : "An efficient boosting algorithm for combining preferences",
      "author" : [ "Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Freund et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning belief networks in the presence of missing values and hidden variables",
      "author" : [ "N. Friedman" ],
      "venue" : "In Proceedings of the Fourteenth International Conference on Machine Learning,",
      "citeRegEx" : "Friedman,? \\Q1997\\E",
      "shortCiteRegEx" : "Friedman",
      "year" : 1997
    }, {
      "title" : "The bayesian structural em algorithm",
      "author" : [ "N. Friedman" ],
      "venue" : "In The 14th Conference on Uncertainty in Artificial Intelligence, UAI ’98,",
      "citeRegEx" : "Friedman,? \\Q1998\\E",
      "shortCiteRegEx" : "Friedman",
      "year" : 1998
    }, {
      "title" : "A latent space model for rank data",
      "author" : [ "C. Gormley", "B. Murphy" ],
      "venue" : "In Proceedings of the 2006 conference on Statistical network analysis,",
      "citeRegEx" : "Gormley and Murphy,? \\Q2007\\E",
      "shortCiteRegEx" : "Gormley and Murphy",
      "year" : 2007
    }, {
      "title" : "Efficient probabilistic inference with partial ranking queries",
      "author" : [ "J. Huang", "A. Kapoor", "C. Guestrin" ],
      "venue" : "In The 27th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Huang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2011
    }, {
      "title" : "Probabilistic Reasoning and Learning on Permutations: Exploiting Structural Decompositions of the Symmetric Group",
      "author" : [ "J. Huang" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Huang,? \\Q2011\\E",
      "shortCiteRegEx" : "Huang",
      "year" : 2011
    }, {
      "title" : "Riffled independence for ranked data",
      "author" : [ "J. Huang", "C. Guestrin" ],
      "venue" : "Advances in Neural Information Processing Systems 22,",
      "citeRegEx" : "Huang and Guestrin,? \\Q2009\\E",
      "shortCiteRegEx" : "Huang and Guestrin",
      "year" : 2009
    }, {
      "title" : "Learning hierarchical riffle independent groupings from rankings",
      "author" : [ "J. Huang", "C. Guestrin" ],
      "venue" : "In Proceedings of the 27th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Huang and Guestrin,? \\Q2010\\E",
      "shortCiteRegEx" : "Huang and Guestrin",
      "year" : 2010
    }, {
      "title" : "Uncovering the riffled independence structure of ranked data",
      "author" : [ "J. Huang", "C. Guestrin" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "Huang and Guestrin,? \\Q2012\\E",
      "shortCiteRegEx" : "Huang and Guestrin",
      "year" : 2012
    }, {
      "title" : "Efficient inference for distributions on permutations",
      "author" : [ "J. Huang", "C. Guestrin", "L. Guibas" ],
      "venue" : "Advances in Neural Information Processing Systems 20,",
      "citeRegEx" : "Huang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2008
    }, {
      "title" : "Fourier theoretic probabilistic inference over permutations",
      "author" : [ "J. Huang", "C. Guestrin", "L.J. Guibas" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Huang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2009
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "T. Joachims" ],
      "venue" : "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Joachims,? \\Q2002\\E",
      "shortCiteRegEx" : "Joachims",
      "year" : 2002
    }, {
      "title" : "Multi-object tracking with representations of the symmetric group",
      "author" : [ "R. Kondor", "A. Howard", "T. Jebara" ],
      "venue" : "Proceedings of the Eleventh",
      "citeRegEx" : "Kondor et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kondor et al\\.",
      "year" : 2007
    }, {
      "title" : "Group theoretical methods in machine learning",
      "author" : [ "R. Kondor" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Kondor,? \\Q2008\\E",
      "shortCiteRegEx" : "Kondor",
      "year" : 2008
    }, {
      "title" : "Conditional models on the ranking poset",
      "author" : [ "G. Lebanon", "J. Lafferty" ],
      "venue" : "Advances in Neural Information Processing Systems 15,",
      "citeRegEx" : "Lebanon and Lafferty,? \\Q2003\\E",
      "shortCiteRegEx" : "Lebanon and Lafferty",
      "year" : 2003
    }, {
      "title" : "Non-parametric modeling of partially ranked data",
      "author" : [ "G. Lebanon", "Y. Mao" ],
      "venue" : "Advances in Neural Information Processing Systems 20,",
      "citeRegEx" : "Lebanon and Mao,? \\Q2008\\E",
      "shortCiteRegEx" : "Lebanon and Mao",
      "year" : 2008
    }, {
      "title" : "Learning mallows models with pairwise preferences",
      "author" : [ "T. Lu", "C. Boutilier" ],
      "venue" : "In The 28th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Lu and Boutilier,? \\Q2011\\E",
      "shortCiteRegEx" : "Lu and Boutilier",
      "year" : 2011
    }, {
      "title" : "Analyzing and Modeling Rank Data",
      "author" : [ "J.I. Marden" ],
      "venue" : null,
      "citeRegEx" : "Marden,? \\Q1995\\E",
      "shortCiteRegEx" : "Marden",
      "year" : 1995
    }, {
      "title" : "Consensus ranking under the exponential model",
      "author" : [ "M. Meila", "K. Phadnis", "A. Patterson", "J. Bilmes" ],
      "venue" : "Tech. rep. 515,",
      "citeRegEx" : "Meila et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Meila et al\\.",
      "year" : 2007
    }, {
      "title" : "Investigating behavioral variability in web search",
      "author" : [ "R. White", "S. Drucker" ],
      "venue" : "In Proceedings of the 16th international conference on World Wide Web, WWW ’07,",
      "citeRegEx" : "White and Drucker,? \\Q2007\\E",
      "shortCiteRegEx" : "White and Drucker",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "This paper is an extended presentation of our paper (Huang, Kapoor, & Guestrin, 2011) which appeared in the 2011 Conference on Uncertainty in Artificial Intelligence (UAI) as well as results from the first author’s dissertation (Huang, 2011).",
      "startOffset" : 228,
      "endOffset" : 241
    }, {
      "referenceID" : 21,
      "context" : "There were five candidates in the election: (1) William Bevan, (2) Ira Iscoe, (3) Charles Kiesler, (4) Max Siegle, and (5) Logan Wright (Marden, 1995).",
      "startOffset" : 136,
      "endOffset" : 150
    }, {
      "referenceID" : 9,
      "context" : "In Figure 3(a), we reproduce the hierarchical structure that was learned using a fully ranked subset of the APA data consisting of 5000 training examples in Huang and Guestrin (2012). There were five candidates in the election: (1) William Bevan, (2) Ira Iscoe, (3) Charles Kiesler, (4) Max Siegle, and (5) Logan Wright (Marden, 1995).",
      "startOffset" : 157,
      "endOffset" : 183
    }, {
      "referenceID" : 21,
      "context" : "Candidates are enumerated as: (1) William Bevan, (2) Ira Iscoe, (3) Charles Kiesler, (4) Max Siegle, and (5) Logan Wright (Marden, 1995).",
      "startOffset" : 122,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "As remarked by Ailon (2007), we note that “The term partial ranking used here should not be confused with two other standard objects: (1) Partial order, namely, a reflexive, transitive anti-symmetric binary",
      "startOffset" : 15,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "When the structure is unknown, we use a structural EM approach, which is analogous to methods from the graphical models literature for structure learning from incomplete data (Friedman, 1997, 1998). Unfortunately, the (riffled independence) structure learning algorithm of Huang and Guestrin (2010) is unable to directly use the posterior distributions computed from the E-step.",
      "startOffset" : 176,
      "endOffset" : 299
    }, {
      "referenceID" : 5,
      "context" : "When the structure is unknown, we use a structural EM approach, which is analogous to methods from the graphical models literature for structure learning from incomplete data (Friedman, 1997, 1998). Unfortunately, the (riffled independence) structure learning algorithm of Huang and Guestrin (2010) is unable to directly use the posterior distributions computed from the E-step. Instead, observing that sampling from riffle independent models can be done efficiently and exactly (as opposed to, for example, MCMC methods), we simply sample full rankings from the posterior distributions computed in the E-step and pass these full rankings into the structure learning algorithm of Huang and Guestrin (2010). The number of samples that are necessary, instead of scaling factorially, scales according to the number of samples required to detect riffled independence (which under mild assumptions is polynomial in n, Huang & Guestrin, 2010).",
      "startOffset" : 176,
      "endOffset" : 706
    }, {
      "referenceID" : 15,
      "context" : "Algorithms such as the RankSVM (Joachims, 2002) and RankBoost (Freund, Iyer, Schapire, & Singer, 2003), for example, have been successful in the large scale ranking problems that appear in web search.",
      "startOffset" : 31,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "Kondor, Howard, and Jebara (2007) and Huang, Guestrin, and Guibas (2008, 2009) considered additive decompositions of a distribution into a weighted sum of Fourier basis functions.",
      "startOffset" : 0,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "1) can always be shown to be a special case of a hierarchical riffle independent model in which items are sequentially factored out of the model one by one (Huang, 2011) (see Figure 6).",
      "startOffset" : 156,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "1) can always be shown to be a special case of a hierarchical riffle independent model in which items are sequentially factored out of the model one by one (Huang, 2011) (see Figure 6). Mallows models (as well as other similar distance based models) have the advantage that they can compactly represent distributions for very large n, and admit conjugate prior distributions (Meila, Phadnis, Patterson, & Bilmes, 2007). Estimating parameters has been a popular problem for statisticians — recovering the optimal σ0 from data is known as the consensus ranking or rank aggregation problem and is known to be NP -hard (Bartholdi, Tovey, & Trick, 1989). Many authors have focused on approximation algorithms instead. Like Gaussian distributions, Mallows models tend to lack flexibility, and so Lebanon and Mao (2008) propose a nonparametric model of ranked (and partially ranked) data based on placing weighted Mallows kernels on top of training examples, which, as they show, can",
      "startOffset" : 157,
      "endOffset" : 813
    }, {
      "referenceID" : 18,
      "context" : "Lebanon and Mao (2008), as we have mentioned, developed a nonparametric model based on Mallows models which can handle arbitrary types of partial rankings.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "This closed form equation of Fligner and Verducci (1986), however, can be seen as a very special case of our setting since Mallows models can always be shown to factor riffle independently according to a chain structure.",
      "startOffset" : 29,
      "endOffset" : 57
    }, {
      "referenceID" : 20,
      "context" : "Finally in recent related work, Lu and Boutilier (2011) considered an even more general class of observations based on DAG (directed acyclic graph) based observations in which probabilities of rankings which are not consistent with a DAG of relative ranking relations are set to zero.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "In previous work, we had used only the full rankings of the APA data (Huang & Guestrin, 2010, 2012), but now we are able to use the entire dataset. Figure 7(a) plots, for each k ∈ {1, . . . , 5}, the number of ballots in the APA data of length k. Likewise, the Meath dataset (Gormley & Murphy, 2007) which was taken from the 2002 Irish Parliament election has over 60,000 top-k rankings of 14 candidates. As with the APA data, we had used only the full rankings of the Meath data in previous work, but here we use the entire dataset. Figure 7(b) plots, for each k ∈ {1, . . . , 14}, the number of ballots in the Meath data of length k. In particular, note that the vast majority of ballots in the dataset consist of partial rather than full rankings, with over half of the electorate preferring to list only their favorite three or four candidates. We can run inference (Algorithm 1) on over 5000 top-k examples for the Meath data in 10 seconds on a dual 3.0 GHz Pentium machine with an unoptimized Python implementation. Using ‘brute force’ inference, we estimate that the same job would require roughly one hundred years. We extracted a third dataset from a database of searchtrails collected by White and Drucker (2007), in which browsing sessions of roughly 2000 users were logged during 20082009.",
      "startOffset" : 70,
      "endOffset" : 1223
    }, {
      "referenceID" : 19,
      "context" : "We compare our method against the work by Lebanon and Mao (2008) in two settings: (1) training on all available data and (2) training on the subset of full rankings.",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 19,
      "context" : "Using the methods developed in our current paper, however, we compare riffle independent models with the state-of-the-art nonparametric estimator of Lebanon and Mao (2008) (to which we hereby refer as the LM08 estimator) on the same data (setting their regularization parameter to be C =1,2,5, or 10 via a validation set).",
      "startOffset" : 149,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "How then, can we tractably condition on incomplete rankings? One possible approach is to convert to a Fourier representation using the methods from (Huang & Guestrin, 2012), then conditioning on a pairwise ranking observation using the Fourier domain conditioning algorithm proposed in (Huang et al., 2008).",
      "startOffset" : 286,
      "endOffset" : 306
    }, {
      "referenceID" : 14,
      "context" : "For low-order permutation observations (depending on only a few items at a time), recent work (Huang et al., 2009; Kondor, 2008) has shown that a Fourier domain representation is appropriate.",
      "startOffset" : 94,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : "For low-order permutation observations (depending on only a few items at a time), recent work (Huang et al., 2009; Kondor, 2008) has shown that a Fourier domain representation is appropriate.",
      "startOffset" : 94,
      "endOffset" : 128
    } ],
    "year" : 2012,
    "abstractText" : "Distributions over rankings are used to model data in a multitude of real world settings such as preference analysis and political elections. Modeling such distributions presents several computational challenges, however, due to the factorial size of the set of rankings over an item set. Some of these challenges are quite familiar to the artificial intelligence community, such as how to compactly represent a distribution over a combinatorially large space, and how to efficiently perform probabilistic inference with these representations. With respect to ranking, however, there is the additional challenge of what we refer to as human task complexity — users are rarely willing to provide a full ranking over a long list of candidates, instead often preferring to provide partial ranking information. Simultaneously addressing all of these challenges — i.e., designing a compactly representable model which is amenable to efficient inference and can be learned using partial ranking data — is a difficult task, but is necessary if we would like to scale to problems with nontrivial size. In this paper, we show that the recently proposed riffled independence assumptions cleanly and efficiently address each of the above challenges. In particular, we establish a tight mathematical connection between the concepts of riffled independence and of partial rankings. This correspondence not only allows us to then develop efficient and exact algorithms for performing inference tasks using riffled independence based representations with partial rankings, but somewhat surprisingly, also shows that efficient inference is not possible for riffle independent models (in a certain sense) with observations which do not take the form of partial rankings. Finally, using our inference algorithm, we introduce the first method for learning riffled independence based models from partially ranked data. 1. Probabilistic Modeling of Ranking Data: Three Challenges Rankings arise in a number of machine learning application settings such as preference analysis for movies and books (Lebanon & Mao, 2008) and political election analysis (Gormley & Murphy, 2007; Huang & Guestrin, 2010). In many of these problems, it is of great interest to build statistical models over ranking data in order to make predictions, form recommendations, discover latent trends and structure and to construct human-comprehensible data summaries. c ©2012 AI Access Foundation. All rights reserved. Huang, Kapoor & Guestrin Modeling distributions over rankings is a difficult problem, however, due to the fact that as the number of items being ranked increases, the number of possible rankings increases factorially. This combinatorial explosion forces us to confront three central challenges when dealing with rankings. First, we need to deal with storage complexity — how can we compactly represent a distribution over the space of rankings?1 Then there is algorithmic complexity — how can we efficiently answer probabilistic inference queries given a distribution? Finally, we must contend with what we refer to as human task complexity, which is a challenge stemming from the fact that it can be difficult to accurately elicit a full ranking over a large list of candidates from a human user; choosing from a list of n! options is no easy task and users typically prefer to provide partial information. Take the American Psychological Association (APA) elections, for example, which allow their voters to rank order candidates from favorite to least favorite. In the 1980 election, there were five candidates, and therefore 5! = 120 ways to rank those five candidates. Despite the small candidate list, most voters in the election preferred to only specify their top-k favorite candidates rather than writing down full rankings on their ballots (see Figure 1). For example, roughly a third of voters simply wrote down their single favorite candidate in this 1980 election. These three intertwined challenges of storage, algorithmic, and human task complexity are the central issues of probabilistic modeling for rankings, and models that do not efficiently handle all three sources of complexity have limited applicability. In this paper, we examine a flexible and intuitive class of models for rankings based on a generalization of probabilistic independence called riffled independence, proposed in our recent work (Huang & Guestrin, 2009, 2010). While our previous papers have focused primarily on representational (storage complexity) issues, we now concentrate on inference and incomplete observations (i.e., partial rankings), showing that in addition to storage complexity, riffle independence based models can efficiently address issues of algorithmic and human task complexity. In fact the two issues of algorithmic and human task complexity are intricately linked for riffle independent models. By considering partial rankings, we give users more flexibility to provide as much or as little information as they care to give. In the context of partial ranking data, the most relevant inference queries also take the form of partial rankings. For example, we might want to predict a voter’s second choice candidate given information about his first choice. One of our main contributions in this paper is to show that inference for such partial ranking queries can be performed particularly efficiently for riffle independent models. The main contributions of our work are as follows:2 • We reveal a natural and fundamental connection between riffle independent models and partial rankings. In particular, we show that the collection of partial rankings over an item set form a complete characterization of the space of observations upon 1. Note that it is common to wonder why one would care to represent a distribution over all rankings if the number of sample rankings is never nearly as large. This problem that the number of samples is always much smaller than n! however, means that most rankings are never observed, limiting our ability to estimate the probability of an arbitrary ranking. The only way to overcome the paucity of samples is to exploit representational structure, which is very much in alignment with solving the storage complexity issue. 2. This paper is an extended presentation of our paper (Huang, Kapoor, & Guestrin, 2011) which appeared in the 2011 Conference on Uncertainty in Artificial Intelligence (UAI) as well as results from the first author’s dissertation (Huang, 2011).",
    "creator" : "TeX"
  }
}