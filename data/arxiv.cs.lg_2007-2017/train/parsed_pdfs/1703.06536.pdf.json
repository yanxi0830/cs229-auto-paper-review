{
  "name" : "1703.06536.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Relationship Between Agnostic Selective Classification Active Learning and the Disagreement Coefficient",
    "authors" : [ "Roei Gelbhart", "Ran El-Yaniv" ],
    "emails" : [ "ROEIGE@CS.TECHNION.AC.IL", "RANI@CS.TECHNION.AC.IL" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We focus on the agnostic setting, for which there is a known algorithm called LESS that learns a PCS classifier and achieves a fast rejection rate (depending on Hanneke’s disagreement coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS for which we show a fast rate (depending on Hanneke’s disagreement coefficient) without any assumptions. Our rejection bound smoothly interpolates the realizable and agnostic settings. The main result of this paper is an equivalence between the following three entities: (i) the existence of a fast rejection rate for any PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic bound for Hanneke’s disagreement coefficient; and (iii) an exponential speedup for a new disagreement-based active learner called Active-ILESS. Keywords: Active learning, selective prediction, disagreement coefficient, selective sampling, selective classification, reject option, pointwise-competitive, selective classification, statistical learning theory, PAC learning, sample complexity, agnostic case"
    }, {
      "heading" : "1. Introduction",
      "text" : "Selective classification is a unique and extreme instance of the broader concept of confidencerated prediction [1, 2]. Given a training sample consisting of m labeled instances, the learning algorithm is required to output a selective classifier [3], defined to be a pair ( f ,g), where f is a prediction function, chosen from some hypothesis class F , and g : X → {0,1} is a selection function, serving as a qualifier for f as follows: for any x, if g(x) = 1, the classifier predicts f (x), and otherwise it abstains. The general performance of a selective classifier is quantified in terms of its coverage and risk, where coverage is the probabilistic mass of non-rejected instances, and risk is\nar X\niv :1\n70 3.\n06 53\n6v 1\n[ cs\n.L G\n] 1\n9 M\nthe normalized average loss of f restricted to non-rejected instances. Let f ∗ be any (unknown) true risk minimizer1 in F for the given problem. The selective classifier ( f ,g) is said to be pointwisecompetitive if, for each x with g(x) = 1, it must hold that f (x) = f ∗(x) for all f ∗ ∈ F [4]. Thus, pointwise-competitiveness w.h.p. over choices of the training sample, is a highly desirable property: it guarantees, for each non-rejected test point, the best possible classification obtainable using the best in-hindsight classifier from F . We don’t restrict g to be from any specific hypothesis class, however, because we use disagreement based selective prediction, the selection of F will limit the possibilities of g. The scenario of a predefined decision functions hypothesis class is investigated in [5].\nPointwise-competitive selective classification (PCS) was first considered in the realizable case [3], for which a simple consistent selective strategy (CSS) was shown to achieve a bounded and monotonically increasing (with m) coverage in various non-trivial settings. Note that in the realizable case, any PCS strategy attains zero risk (over the sub-domain it covers). These results were recently extended to the agnostic setting [4, 6] with a related but different algorithm called low-error selective strategy (LESS), for which a number of coverage bounds were shown. These bounds relied on the fact that the underlying probability distribution and the hypothesis class F will satisfy the so-called “(β1,β2)-Bernstein property” [7]. The coverage bounds in [4, 6] are dependent on the parameters β1,β2. This Bernstein property assumption (as presented in [7]), which allows for better concentration, can be problematic. First, it is defined with respect to a unique true risk minimizer f ∗, a property which is unlikely to hold in noisy agnostic settings. Moreover, for arbitrary F , even for the 0/1 loss function, it is not necessarily known whether the Bernstein property can hold at all.2 We removed the Bernstein assumption from our analysis. Assuming that a selective classifier is w.h.p. pointwise-competitive, our key goal is a small rejection rate. We will say that a learner has a fast R∗ rejection rate, if w.h.p. the rejection rate is bounded by\npolylog( 1 R( f ∗)+1/m ) ·R( f ∗)+ polylog(m,d,1/δ) m .\nSelective classification is very closely related to the field of active learning (AL). In active learning, the learner can actively influence the learning process by selecting the points to be labeled. The incentive for introducing this extra flexibility is to reduce labeling efforts. A key question in theoretical studies of AL is how many label requests are sufficient to learn a given (unknown) target concept to a specified accuracy, a quantity called label complexity. For an AL algorithm satisfying the “passive example complexity” property (consuming the same number of labeled/unlabeled examples as a passive algorithm for achieving the same error; see Definition 6.2), we will say it has R∗ exponential speedup, if w.h.p. the number of labels it requests is bounded by\npolylog( 1\nR( f ∗)+1/m ) ·R( f ∗)m+polylog(m,d,1/δ).\nThe connection between AL and confidence-rated prediction is quite intuitive. A (pointwisecompetitive) selective classifier P can be straightforwardly used as the querying component of an active learning algorithm. This reduction is most naturally demonstrated in the stream-based AL\n1. We assume that there exists an f ∗ in F . Otherwise, we can artificially define f ∗ to be any function whose risk is sufficiently close to inf f∈F (R( f )), for instance, not greater than a small multiplicative factor from this infimum. 2. It was mentioned in [4] that, under the Tsybakov noise condition [8], the desired property holds, but this is guaranteed only for cases in which the Bayes classifier is within F , which is a fairly strong assumption in itself.\nmodel: at each iteration, the active algorithm trains a selective classifier on the currently available labeled samples, and then decides to query a newly introduced (unlabeled) point x if P abstains on x.\nHanneke’s disagreement coefficient [9] (see Definition 2.1), is a well-known parameter of the hypothesis class and the marginal distribution; it is used in most of the known label complexity bounds [9, 10, 11]. The disagreement coefficient is the supremum of the relation between the disagreement mass of functions that are r-distanced from f ∗ to r, over r. PCS classification is based on using generalization bounds to estimate the empirical error of f ∗, and more specifically, its distance from the empirical error of the ERM. Whenever there is a unanimous agreement of all the functions that reside within a ball around the ERM, the classifier choses to classify. Thus, the abstain rate is dependent on the disagreement mass of the functions within the ball. The radius of the ball depends on the generalization bounds. The generalization bounds we use are of the form Õ(1/m) for the realizable case (we consider the realizable case here for simplicity). After observing m examples, we can bound the disagreement mass of a ball around the ERM, by multiplying the radius of the ball, which is Õ(1/m), with the disagreement coefficient. Thus, if for example, the disagreement coefficient is bounded by a constant, the abstain rate of some PCS algorithms can be bounded by Õ(1/m) for the realizable case. This gives a basic idea of the disagreement coefficient, which will be formally presented later on.\nNote that in principle, the disagreement coefficient can be replaced by another important quantity, namely, the version space compression set size, recently shown to be equivalent to it [12, 13]. Specifically, an O(polylog(m)log(1/δ)) version space compression set size minimal bound was shown in [12, Corollary 11], to be equivalent to an O(polylog(1/r)) disagreement coefficient.\nThe first contribution of this paper is a novel selective classifier, called ILESS, which utilizes a tighter generalization error bound than LESS and depends on R( f ∗) (and interpolates the agnostic and realizable cases). Most importantly, the new strategy can be analyzed completely without the Bernstein condition.\nWe derive an active learning algorithm, called Active-ILESS, corresponding to our selective classifier, ILESS. Active-ILESS is constructed to work in a stream-based AL model and its querying function is extremely conservative: for each unlabeled example, the algorithm requests its label if and only if the labeling of the optimal classifier (from the same class) on this point cannot be inferred from information already acquired. This querying strategy, which is often termed “disagreementbased,” has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18]. In [19], a computationally efficient algorithm for disagreement based AL.\nThe first formal relationship between PCS classification and AL was proposed in [20, 21], where the aforementioned CSS algorithm was shown to be equivalent to the well-known CAL AL algorithm of [22], in the sense that a fast coverage rate for CSS was proven to be equivalent to exponential label complexity speedup for CAL. This result applies to the realizable setting only. Our first contribution is a similar equivalence relation between pointwise-competitive selective classification and AL, which applies to the more challenging agnostic case and smoothly interpolates the realizable and agnostic settings.\nOur second and main contribution is to show a complete equivalence between (i) selective classification with a fast R∗ rejection rate, (ii) AL with R∗ exponential speedup (represented by ActiveILESS), and (iii) the existence of an f ∗ with a bounded disagreement coefficient. This is illustrated in Figure 1, where the blue errors indicate the equivalence relationships we prove in this paper, and\nthe red arrow indicates a previously known result (from [9, 10]) (and can also be deduced from the other arrows)."
    }, {
      "heading" : "2. Definitions",
      "text" : "Consider a domain X , and a binary label set Y = {±1}. A learning problem is specified via a hypothesis class F and an unknown probability distribution PX ,Y . Given a sequence of labeled training examples Sm = ((x1,y1),(x2,y2), ...,(xm,ym)), such that ∀i,(xi,yi) ∈ X ×Y , the empirical error of a hypothesis f over Sm is R̂( f ,Sm) , 1m ∑ m i=1 `( f (xi),yi), where ` : Y ×Y → R+ is a loss function. In this paper we will mainly focus on the zero-one loss function, `01(y,y′) , 1{y 6= y′}. The true (zero-one) error of f is R( f ) , EP [`01( f (x),y)]. An empirical risk minimizer hypothesis (henceforth an ERM) is\nf̂ (Sm), argmin f∈F R̂( f ,Sm), (1)\nand a true risk minimizer is f ∗ , argmin f∈F R( f ). 3\n3. We assume that f ∗ exists, and that it need not be unique, in which case f ∗ refers to any one of the minimizers.\nWe acquire the following definitions from [4]. For any hypothesis class F , hypothesis f ∈ F , distribution PX ,Y , sample Sm, and real number r > 0, define the true and empirical low-error sets,\nV ( f ,r), { f ′ ∈ F : R( f ′)≤ R( f )+ r }\n(2)\nand V̂ ( f ,r), { f ′ ∈ F : R̂( f ′,Sm)≤ R̂( f ,Sm)+ r } . (3)\nLet G⊆ F . The disagreement set [9] and agreement set [3] w.r.t. G are defined, respectively, as\nDIS(G), {x ∈ X : ∃ f1, f2 ∈ G s.t. f1(x) 6= f2(x)} (4)\nand AGR(G), {x ∈ X : ∀ f1, f2 ∈ G s.t. f1(x) = f2(x)} . (5)\nIn selective classification [3], the learning algorithm receives Sm and is required to output a selective classifier, defined to be a pair ( f ,g), where f ∈ F is a classifier, and g : X → {0,1} is a selection function, serving as a qualifier for f as follows. For any x ∈ X , ( f ,g)(x) = f (x) iff g(x) = 1. Otherwise, the classifier outputs “I don’t know”. For any selective classifier ( f ,g) we define its coverage to be\nΦ( f ,g), Pr X∼PX (g(X) = 1),\nand its complement, 1−Φ, is called the abstain rate. For any f ∈ F and r > 0, define the set B( f ,r) of all hypotheses that reside within a ball of radius r around f ,\nB( f ,r), {\nf ′ ∈ F : Pr X∼PX\n{ f ′(X) 6= f (X) } ≤ r } .\nFor any G⊆ F , and distribution PX , we denote by ∆G the volume of the disagreement set of G (see (4)), ∆G , Pr{DIS(G)}.\nDefinition 2.1 (Disagreement Coefficient) Let r0 ≥ 0. Then, Hanneke’s disagreement coefficient [9] of a classifier f ∈ F with respect to the target distribution PX is\nθ f (r0), sup r>r0 ∆B( f ,r) r , (6)\nand the general disagreement coefficient of the entire hypothesis class F is\nθ(r0), sup f∈F θ f (r0). (7)\nNotice that this definition of the disagreement coefficient is independent of PY |X . Another commonly used definition of the disagreement coefficient does depend on a true risk minimizer f ∗, as follows:\nθ′(r0) = sup r>r0 ∆B( f ∗,r) r . (8)\nClearly, it always holds that θ′ ≤ θ. The independence of θ of unknown quantities such as the underlying distribution (and f ∗), however, is a convenient property that sometimes allows for a direct estimation of θ, which only depends on the marginal distribution, PX . This is, for example, the case in active learning, where labels are expensive but information about the marginal distribution\n(provided by unlabeled examples) is cheap. Note also that the above definition of θ′ implicitly assumes a unique f ∗. Nevertheless, the definition can be extended to cases where f ∗ is not unique, in which case the infimum over all f ∗ can be considered (the analysis can be extended accordingly using limits). For more on the disagreement coefficient, and examples of probabilities distributions and hypothesis classes for which it is bounded, see [16]."
    }, {
      "heading" : "3. Convergence Bounds and LESS",
      "text" : "We use a uniform convergence bound from [18, 23]. Define convergence slacks σR−R̂(m,δ,d,R, R̂) and σR̂−R(m,δ,d,R, R̂), given in terms of the training sample, Sm, its size, m, the confidence parameter, δ, and the VC-dimension d of the class F . For any f ∈ F ,\nσR−R̂(m,δ,d,R, R̂), min{ 4d ln(16medδ )\nm +\n√ 4d ln(16medδ )\nm · R̂︸ ︷︷ ︸\nσ̂R−R̂(m,δ,d,R̂)\n,\n√ 4d ln(16medδ ) m ·R︸ ︷︷ ︸\nσ̄R−R̂(m,δ,d,R)\n} (9)\nand\nσR̂−R(m,δ,d,R, R̂), min{ 4d ln(16medδ )\nm +\n√ 4d ln(16medδ )\nm ·R︸ ︷︷ ︸\nσ̄R̂−R(m,δ,d,R)\n,\n√ 4d ln(16medδ ) m · R̂︸ ︷︷ ︸\nσ̂R̂−R(m,δ,d,R̂)\n}. (10)\nTo simplify the analysis, we further decompose the above slack terms to their empirical and nonempirical components. For (9), we thus have, respectively,\nσ̂R−R̂(m,δ,d, R̂), 4d ln(16medδ )\nm +\n√ 4d ln(16medδ )\nm · R̂ (11)\nand\nσ̂R̂−R(m,δ,d, R̂),\n√ 4d ln(16medδ )\nm · R̂. (12)\nSimilarly, the non-empirical part in these minimums are denoted by σ̄R−R̂ and σ̄R̂−R. With this notation, we can write, for example, σR−R̂ = min{σ̂R−R̂, σ̄R−R̂}. Our Lemma 1 is taken from [18, Lemma 1], which is based on [23, Theorem 7] 4.\nLemma 1 ([18]) Let F be a hypothesis class with VC-dimension d. For any 0 < δ < 1, with probability of at least 1−δ over the choice of Sm from P m, any hypothesis f ∈ F satisfies\nR( f )≤ R̂( f )+σR−R̂ ( m,δ,d,R( f ), R̂( f ) ) (13)\nR̂( f )≤ R( f )+σR̂−R ( m,δ,d,R( f ), R̂( f ) ) . (14)\nStrategy 1 is the LESS algorithm of [4]. LESS learns w.h.p. a pointwise-competitive selective classifier, ( f ,g), where f ∈F and g : X →{0,1} is its selection function which determines whether to abstain or to classify. A pointwise-competitive selective classifier must satisfy the following condition: for each x with g(x) = 1, it must hold that f (x) = f ∗(x) for all f ∗ ∈ F .\n4. In the original lemma from [18], there appears S(H ,n), the growth function. We plug in Sauer’s Lemma, S(H ,n)≤ ( emd ) d , into Lemma 1 from [18] to get our lemma.\nRemark 2 The original definition of pointwise-competitiveness from [4] requires a single f ∗. We widen the definition to cases for which there are more than one f ∗, and require that a pointwisecompetitive selective classifier will be equal to all f ∗, wherever g = 1. This extrapolation seems a bit strict. However, even if the requirement would have been relaxed to “any f ∗”, any pointwisecompetitive selective classifier would still have been forced to identify with all f ∗, as it is impossible to differentiate whether a set of functions are all f ∗, or one is better than the rest.\nThe main idea behind LESS is that, w.h.p. all f ∗ lie within a ball around an ERM hypothesis with error radius of 2σ(m,δ/4,d), where\nσ(m,δ,d), 2\n√ 2d ( ln 2med ) + ln 2δ\nm (15)\nis the slack term of a certain uniform convergence bound. Therefore, if all the functions in that ball agree over the labeling of any instance x, we know with high probability that all f ∗ label x the same way as the ERM. This property ensures that LESS is pointwise-competitive w.h.p.\nStrategy 1 Agnostic low-error selective strategy (LESS) Input: Sample set of size m, Sm,\nConfidence level δ Hypothesis class F with VC dimension d\nOutput: A selective classifier (h,g) 1: Set f̂ = ERM(F ,Sm), i.e., f̂ is any empirical risk minimizer from F 2: Set G = V̂ ( f̂ ,2σ(m,δ/4,d) ) 3: Construct g such that g(x) = 1⇐⇒ x ∈ {X \\DIS (G)} 4: f = f̂"
    }, {
      "heading" : "4. ILESS",
      "text" : "Strategy 2 Improved Low-Error Selective Strategy (ILESS) Input: Sample set of size m, Sm,\nConfidence level δ Hypothesis class F with VC dimension d\nOutput: A selective classifier (h,g) 1: Set f̂ = ERM(F ,Sm), i.e., f̂ is any empirical risk minimizer from F 2: Set σILESS = σ̂R−R̂ ( m,δ,d, R̂( f̂ ,Sm) ) + σ̄R̂−R ( m,δ,d, R̂( f̂ ,Sm)+ σ̂R−R̂(m,δ,d, R̂( f̂ ,Sm)) ) 3: Set G = V̂ ( f̂ ,σILESS\n) 4: Construct g such that g(x) = 1⇐⇒ x ∈ {X \\DIS (G)} 5: h = f̂\nIn this section we introduce an improved version of LESS, called ILESS, which utilizes a radius of the form polylog(m,1/δ,d) · ( 1m + √ R( f ∗) m ). Noting that the radius, 2σ(m,δ/4,d), used by LESS to define G = V̂ , is of the form polylog(m,1/δ,d)/ √\nm, we observe that in cases where R( f ∗)≈ Cm , this new radius behaves as polylog(m,1/δ,d)m . We later show that this radius allows ILESS to achieve a faster rejection decay rate than the one achieved by LESS.\nConsider the pseuodo-code of ILESS given in Strategy 2. We now analyze ILESS, and begin by showing in Lemma 3 that ILESS is pointwise-competitive w.h.p., i.e., for any x for which g(x) = 1, f (x) = f ∗(x) for all f ∗. The calculation of g appears to be very problematic, as for a specific x, a unanimous decision over an infinite number of functions must be ensured. This problem was shown to be reducible to finding an ERM under one constraint ([24, Lemma 6.1] a.k.a. the disbelief principle). This is a difficult problem, nonetheless, albeit one that could be estimated with heuristics.\nDefinition 4.1 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown probability distribution. Given a sample set Sm, drawn from PX ,Y , we denote by E the event where both inequalities (13) and (14) of Lemma 1 simultaneously hold. We know from the lemma that E occurs with probability of at least 1−δ.\nLemma 3 (ILESS is pointwise-competitive) Given that event E occurred (see Definition 4.1), for all f ∗ ∈ F , f ∗ resides within G (from Strategy 2), and therefore, ILESS is pointwise-competitive w.h.p.\nProof From (14) it follows that,\nR̂( f ∗,Sm) ≤ R( f ∗)+σR̂−R(m,δ,d,R( f ∗), R̂( f ∗,Sm))\n≤ R( f ∗)+ σ̄R̂−R(m,δ,d,R( f ∗)). (16)\nAdditionally, by the definition of f ∗, we know that it has the lowest true error, and using Inequality (13) from Lemma 1 we obtain,\nR( f ∗) ≤ R( f̂ ) ≤ R̂( f̂ ,Sm)+σR−R̂(m,δ,d,R( f̂ ), R̂( f̂ ,Sm)) ≤ R̂( f̂ ,Sm)+ σ̂R−R̂(m,δ,d, R̂( f̂ ,Sm)). (17)\nFinally, by applying (17) in (16), we have,\nR̂( f ∗,Sm)≤ R̂( f̂ ,Sm)+ σ̂R−R̂(m,δ,d, R̂( f̂ ,Sm)) + σ̄R̂−R ( m,δ,d, R̂( f̂ ,Sm)+ σ̂R−R̂(m,δ,d, R̂( f̂ ,Sm)) ) ,\nR̂( f ∗,Sm) ≤ R̂( f̂ ,Sm)+ σ̂R−R̂(m,δ,d, R̂( f̂ ,Sm))+ σ̄R̂−R ( m,δ,d, R̂( f̂ ,Sm)+ σ̂R−R̂(m,δ,d, R̂( f̂ ,Sm)) ) , which means that f ∗ ∈ G.\nLemma 4 below bounds the radius σILESS of ILESS. The lemma utilizes the notation\nA , 4d ln( 16me\ndδ ),\nwith which, by the definition of σILESS (see Strategy 2), we have, σILESS = σ̂R−R̂(m,δ,d, R̂( f̂ ,Sm))+ σ̄R̂−R ( m,δ,d, R̂( f̂ ,Sm)+ σ̂R−R̂(m,δ,d, R̂( f̂ ,Sm)) ) =\nA m + √ A m · R̂( f̂ ,Sm)+ A m + √ A m · [R̂( f̂ ,Sm)+ A m + √ A m · R̂( f̂ ,Sm)].\n(18)\nLemma 4 Given that event E (see Definition 4.1) occurred, the radius of ILESS satisfies\nσILESS = 6 A m +3 √ A m ·R( f ∗) = O(A m + √ A m ·R( f ∗)), (19)\nwhere A , 4d ln(16medδ ).\nProof Under our assumption, inequalities (13) and (14) hold for every f ∈ F . We thus have\nR̂( f̂ ,Sm)≤ R̂( f ∗,Sm)≤ R( f ∗)+ A m + √ A m ·R( f ∗). (20)\nReplacing the three occurrences of R̂( f ∗,Sm) in (18) with the R.H.S. of (20), and using the basic inequalities √ A+B≤ √ A+ √ B and √ AB≤ A/2+B/2, we get,\nσILESS ≤ A m + √√√√A m · ( R( f ∗)+ A m + √ A m ·R( f ∗) ) + A m +\n+ √√√√√A m · R( f ∗)+ A m + √ A m ·R( f ∗)+ A m + √√√√A m · ( R( f ∗)+ A m + √ A m ·R( f ∗) ) ≤ A\nm + √ A m · ( R( f ∗)+ A m + A 2m + 1 2 R( f ∗) ) + A m +\n+ √√√√A m · [ R( f ∗)+ A m + A 2m + 1 2 R( f ∗)+ A m + √ A m · ( R( f ∗)+ A m + A 2m + 1 2 R( f ∗) )]\n≤ 2A m + 3A 2m + 3 2 √ A m ·R( f ∗)+ √√√√A m · [ 5A 2m + 3 2 R( f ∗)+ √ A m · ( 3A 2m + 3 2 R( f ∗) )]\n≤ 7A 2m + 3 2 √ A m ·R( f ∗)+ √√√√A m · [ 5A 2m + 3 2 R( f ∗)+ 3A 2m + √ A m · 3 2 R( f ∗) ]\n≤ 7A 2m + 3 2 √ A m ·R( f ∗)+ √ A m · [ 5A 2m + 3 2 R( f ∗)+ 3A 2m + 3A 4m + 3 4 R( f ∗) ]\n≤ 7A 2m + 3 2 √ A m ·R( f ∗)+ √ 19 4 A m + √ A m · 9 4 R( f ∗)\n≤ 6 A m +3 √ A m ·R( f ∗). (21)\nIn comparison, the radius of LESS is of order O( √\nA m), which can be significantly larger when R( f ∗)\nis small. This potential radius advantage translates to a potential coverage advantage of ILESS, as stated in the following theorem.\nTheorem 5 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown probability distribution. Given that event E (see Definition 4.1) occurred, for all f ∗, the abstain rate is bounded by\n1−Φ(ILESS)≤ θ f ∗(R0) ·R0,\nwhere\nR0 , 2 ·R( f ∗)+11 · A m +6 · √ A m ·R( f ∗).\nThis immediately implies (by definition) that\n1−Φ(ILESS)≤ θ(R0) ·R0.\nRemark 6 Note that R0 = O(R( f ∗)+ Am) due to √ A m ·R( f ∗)≤ 1 2( A m +R( f ∗)).\nProof We start by showing that G, defined in Strategy 2, resides within a ball around any specific f ∗. To do so, we need to bound the true error of all functions in G.\nf ∈ G ⇒ R̂( f ,Sm)≤ R̂( f̂ ,Sm)+σILESS (22)\n⇒ R̂( f ,Sm)≤ R( f ∗)+ A m + √ A m ·R( f ∗)+6 A m +3 √ A m ·R( f ∗) (23)\n⇒ R̂( f ,Sm)≤ R( f ∗)+7 · A m +4 · √ A m ·R( f ∗), (24)\nwhere inequality (22) is by the definition of G, and inequality (23) follows from (20) and (21) (under event E). We then have,\nR( f ) ≤ R̂( f ,Sm)+ σ̂R−R̂(m,δ,d, R̂) (25)\n≤ R̂( f ,Sm)+ A m + √ A m · R̂( f ,Sm) (26)\n≤ R( f ∗)+8 · A m +4 · √ A m ·R( f ∗)+ √√√√A m · [ R( f ∗)+7 · A m +4 · √ A m ·R( f ∗) ] (27)\n≤ R( f ∗)+8 · A m +4 · √ A m ·R( f ∗)+ √ A m · [ 3R( f ∗)+9 · A m ] (28)\n≤ R( f ∗)+11 · A m +6 · √ A m ·R( f ∗), (29)\nwhere inequality (25) is (13) (which holds given E), inequality (26) follows directly from the definition of σ̂R−R̂, inequality (27) is obtained using (24), inequality (28) follows from √ AB≤ A/2+B/2,\nand (29) from √ A+B≤ √ A+ √ B.\nUsing (29), for all f ∈ G, and any f ∗ we have,\nPr X∼PX { f (X) 6= f ∗(X)} = Pr X ,Y∼PX ,Y { f (X) 6= f ∗(X)∧ f ∗(X) = Y}+ Pr X ,Y∼PX ,Y { f (X) 6= f ∗(X)∧ f ∗(X) 6= Y}\n≤ Pr X ,Y∼PX ,Y\n{ f (X) 6= f ∗(X)∧ f ∗(X) = Y}+R( f ∗)\n≤ Pr X ,Y∼PX ,Y\n{ f (X) 6= Y}+R( f ∗)\n= R( f )+R( f ∗)\n≤ 2 ·R( f ∗)+11 · A m +6 · √ A m ·R( f ∗). (30)\nIt follows that\nf ∈ B( f ∗,2 ·R( f ∗)+11 · A m +6 · √ A m ·R( f ∗)) = B( f ∗,R0),\nand, in particular, G⊆ B( f ∗,R0),\nso ∆G≤ ∆B( f ∗,R0).\nThe abstain rate of ILESS equals ∆G. We can now use the disagreement coefficient to bound the abstain rate from above,\n∆G≤ ∆B( f ∗,R0) = ∆B( f ∗,R0)\nR0 ·R0 ≤ θ(R0) ·R0, (31)\nwhich concludes the proof.\nAccording to Theorem 5, assuming the disagreement coefficient is θ(r) = O(polylog(1/r)) for r ≥ R( f ∗), the rejection mass of ILESS, defined as the probability that the classifier trained by ILESS will output “I don’t know” is bounded w.h.p. by\npolylog1( 1 R( f ∗)+1/m ) ·R( f ∗)+ polylog2(m,d,1/δ) m . (32)\nIn many cases, the disagreement coefficient, θ(r), is bounded by a constant, or by O(polylog(1/r)) for all r > 0 (see [16]). For example, it was shown in [12], that for linear separators under mixture of Gaussians, and for axis-aligned rectangles under product densities over Rk, θ(r) is bounded by O(polylog(1/r)) for all r > 0. For such cases, we know that (32) always holds, regardless of the size of R( f ∗). The disagreement coefficient is only dependent on the marginal PX , the hypothesis class F , and the identity of the true risk minimizers, f ∗ (which is not necessarily unique). This fact motivates the following definition of a rejection rate of a selective learning algorithm, which is only dependent on PX ,F and f ∗.\nDefinition 4.2 (Fast R∗ Rejection Rate) Given PX ,F and f ∗, if for any PY |X , for which f ∗ is a true risk minimizer, the rejection mass of a selective classifier learning algorithm is bounded by probability of at least 1−δ by (32), we say that the algorithm achieves a fast R∗ rejection rate, with polylog1 and polylog2 as its parameters.\nClearly, by Theorem 5, if θ(r) = O(polylog(1/r)) for all r > 0, then ILESS has a fast R∗ rejection rate. In the next section, we will show the other direction; that is, if there is a PCS learning algorithm that has a fast R∗ rejection rate, then θ(r) = O(polylog(1/r)) for all r > 0.\nAs long as the number of training examples that ILESS receives is not “too large” relative to 1/R( f ∗), i.e., m 1R( f ∗) , the rejection mass of ILESS is O( polylog(m,d,1/δ) m ). When m is large, and R( f ∗) becomes more dominant than 1m , our coverage bound is dominated by R( f ∗). This should not surprise us, as ILESS achieves pointwise-competitiveness w.h.p., and any strategy that achieves pointwise-competitiveness cannot ensure a better rejection mass than R( f ∗) without making more assumptions about the error or the distribution. This can be seen in the following example, in which θ(r)≤ 1 for all r > 0, but the rejection mass of any pointwise-competitive strategy is always at least R( f ∗).\nExample 1 Given any 0 < ε < 0.5, let X = [0,1], and F = { f1, f2} where\nf1(x) = { 1, x < ε 0, otherwise , f2(x) = { 1, x > 1− ε 0, otherwise.\nLet PX be the uniform distribution over [0,1]. Assume that Y will always be zero. f1 and f2 are both f ∗. Every pointwise-competitive classifier will have to output g(x) = 0 for every x in the disagreement set of f1 and f2. R( f ∗) = ε, and the rejection mass is 2ε(= 2R( f ∗))."
    }, {
      "heading" : "5. From Selective Classification to the Disagreement Coefficient",
      "text" : "We now turn to show a reduction from selective classification, to the disagreement coefficient.\nTheorem 7 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. Let PCS be an algorithm that returns a pointwise-competitive selective classifier w.h.p. If there exists an mmax s.t. for every m ≤ mmax, with probability of at least 1− δ, the abstain rate 1−Φ of PCS(Sm,δ,F ,d) is bounded above as follows:\n1−Φ(PCS)≤ polylog(m,d,1/δ) m . (33)\nThen for every f ∗ (every true risk minimizer), for every r ≥ 1/mmax,\nθ f ∗(r)≤ 8(polylog(1/r,d,1/r)+3).\nProof For any m ∈ {2,3, ...,mmax}, denote by Sm a random training sample drawn from PX ,Y . Let Z be a random variable representing a single random unlabeled example sampled from PX , and let f ∗ to be a specific true risk minimizer.\nGiven z ∈ DIS ( B( f ∗, 1m) ) , as used in [15, Lemma 47], we know that there exists a function hz ∈ F s.t. hz(z) 6= f ∗(z) and Pr(hz(X) 6= f ∗(X)) ≤ 1m . We denote by PX ,Y z a new probability distribution that is identical to PX ,Y over all x ∈ X with the exception of {x : hz(x) 6= f ∗(x)}, over which it is defined to be Y , hz(x). It is easy to see that hz is an f ∗ for such a distribution.\nDenote by e1 the probability event where (33) holds (for a specific m≤mmax). Denote by e2 the event where PCS has succeeded in returning a pointwise-competitive selective classifier ( fsm ,gsm) under Sm.\nDefine S′m to be a modified Sm. For every x s.t. hz(x) 6= f ∗(x), y changes to be y = hz(x). S′m is a random training sample drawn from PX ,Y z. Denote by e3z the event where PCS has succeeded in returning a pointwise-competitive selective classifier ( fs′m ,gs′m) under S ′ m. hz is only defined for\ncases in which z ∈ DIS ( B( f ∗, 1m) ) , and thus we define that e3z will also include cases for which\nz /∈ DIS ( B( f ∗, 1m) ) .\nUnder our assumptions, Pr(e1),Pr(e2)≥ 1−δ. For every z ∈ DIS(B( f ∗, 1m)), Pr(e3z|z)≥ 1−δ, and for every z /∈ DIS(B( f ∗, 1m)), Pr(e3z|z) = 1, which implies that Pr(e3z) ≥ 1− δ. We denote by hz(Sm) = f ∗(Sm) the event where hz(x) = f ∗(x) for all x ∈ Sm. The explanations for the following equations follow.\nPr{Z ∈ DIS (\nB( f ∗, 1 m )\n) ∧hz(Sm) = f ∗(Sm)} (34)\n= Pr{Z ∈ DIS (\nB( f ∗, 1 m )\n) ∧hz(Sm) = f ∗(Sm)∧ e1∧ e2∧ e3z} (35)\n+Pr{Z ∈ DIS (\nB( f ∗, 1 m )\n) ∧hz(Sm) = f ∗(Sm) | ¬(e1∧ e2∧ e3z)} ·Pr(¬(e1∧ e2∧ e3z))\n≤ Pr{Z ∈ DIS (\nB( f ∗, 1 m )\n) ∧hz(Sm) = f ∗(Sm)∧ e1∧ e2∧ e3z}+3δ (36)\n≤ Pr{gsm(Z) = 0∧ e1∧ e2∧ e3z}+3δ (37) ≤ Pr{gsm(Z) = 0∧ e1}+3δ (38) ≤ Pr{gsm(Z) = 0 | e1}+3δ (39) ≤ polylog(m,d,1/δ) m +3δ. (40)\nIn (34), it is convenient to view the random experiment as if we draw Z first, and then Sm. If Z ∈ DIS(B( f ∗, 1m)), then consider hz to be any function that holds hz(Z) 6= f\n∗(Z) and Pr(hz(X) 6= f ∗(X)) ≤ 1m . If Z /∈ DIS(B( f\n∗, 1m)), then the event described in (34) does not occur, and hz is undefined. In (35), we use conditional probability, and in (36) we apply the union bound. Inequality (37) is justified as follows. If hz(Sm) = f ∗(Sm), then the algorithm received the same input under PX ,Y z and PX ,Y . Given that e2 and e3z occurred, we know that the algorithm had successfully output a pointwise-competitive selective classifier for both probabilities, which means that whenever f ∗ and hz disagree, gsm has to output zero; otherwise, it will not be pointwise-competitive for one of the distributions. By the definition of hz, hz(Z) 6= f ∗(Z), which explains the inequality. (40) is driven from the definition of e1. Taking δ = 1m , we get,\nPr{Z ∈ DIS (\nB( f ∗, 1 m )\n) ∧hz(Sm) = f ∗(Sm)} ≤\npolylog(m,d,m)+3 m . (41)\nThe following inequalities are derived using elementary conditional probability. In Equation (42) we use an argument taken from the proof of [15, Lemma 47]. hz ∈ ( B( f ∗, 1m) ) and thus the probability\nthat f ∗ and hz will have the same labels over a sample of size m is at least (1− 1m) m.\nPr{Z ∈ DIS (\nB( f ∗, 1 m )\n) ∧hz(Sm) = f ∗(Sm)}\n= Pr{hz(Sm) = f ∗(Sm) | Z ∈ DIS (\nB( f ∗, 1 m )\n) } ·Pr{Z ∈ DIS ( B( f ∗,\n1 m )\n) }\n≥ (1− 1 m )m ·Pr{Z ∈ DIS\n( B( f ∗,\n1 m )\n) } (42)\n≥ 1 4 ·∆B( f ∗, 1 m ). (43)\nCombining (41) and (43), we get that for every m ∈ {2,3, ...,mmax},\n∆B( f ∗,1/m) 1/m ≤ 4(polylog(m,d,m)+3). (44)\nThe following inequalities follow from (44), and from the fact that ∆B( f ∗,x) and polylog1(x) are non-decreasing. For any r in [ 1mmax , 1 2 ],\n∆B( f ∗,r) r ≤ ∆B( f ∗, 1b1/rc)\n1 b1/rc\n· 1 r · b1/rc\n≤ 4(polylog(b1/rc,d,b1/rc)+3) · 1 r · (1/r−1) ≤ 4(polylog(b1/rc,d,b1/rc)+3) · 1 1− r ≤ 8(polylog(1/r,d,1/r)+3)\nand for r in [12 ,1],\n∆B( f ∗,r) r ≤ 1 1/2 = 2, (45)\nwhich concludes the proof.\nCorollary 8 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. If there exists an mmax s.t. for every m ≤ mmax, with probability of at least 1− δ, the abstain rate 1−Φ of ILESS(Sm,δ,F ,d) is bounded above as follows:\n1−Φ(ILESS)≤ polylog(m,d,1/δ) m . (46)\nThen for every f ∗ (every true risk minimizer), for every r ≥ 1/mmax,\nθ f ∗(r)≤ 8(polylog(1/r,d,1/r)+3).\nProof This is a direct result from Theorem 7, and from the fact that ILESS is PCS.\nGiven PX ,F and f ∗, if any PCS has a fast R∗ rejection rate, we can apply Theorem 7 with a deterministic PY |X distribution for which Y = f ∗(X), and get that R( f ∗) = 0. Thus, by definition,\n1−Φ(ILESS)≤ polylog1( 1 R( f ∗)+1/m ) ·0+ polylog2(m,d,1/δ) m . (47)\nWe can now apply Theorem 7 with mmax = ∞, and get that the disagreement coefficient is bounded by polylog(1/r) for all r > 0. Thus, completing a two sided equivalence from PCS with a fast R∗ rejection rate to a bounded disagreement coefficient for all r > 0."
    }, {
      "heading" : "6. Active-ILESS",
      "text" : "Strategy 3 Agnostic low-error active learning strategy (Active-ILESS) Input: ε and/or m depending on the desired termination condition (error or labeling budget, respectively)\nConfidence level δ Hypothesis class F with VC dimension d An unlabeled input sequence sampled i.i.d from PX ,Y : x1,x2,x3, . . .\nOutput: A classifier f̂ . Initialize: Set Ŝ = /0, G0 = F , t = 1. Perform for each example xt received:\n1: if xt ∈ AGR(Gt−1): don’t request label for xt and set yt = f (xt) using any f ∈ Gt−1 otherwise: request label yt . 2: Set Ŝ = Ŝ∪{(xt ,yt)}. 3: Set f̂ = f̂ (Ŝ) 4: if log2(t) ∈ N:\n• Set σActive = σ̂R−R̂ ( t 2 , δ 2t ,d, R̂( f̂ , Ŝ) ) + σ̄R̂−R ( t 2 , δ 2t ,d, R̂( f̂ , Ŝ)+ σ̂R−R̂( t 2 , δ 2t ,d, R̂( f̂ , Ŝ)) ) • If ε was given as input and σActive < ε, terminate and return f̂\n• Set Gt = V̂ ( f̂ ,σActive )\n• Set Ŝ = /0. otherwise:\n• Gt = Gt−1 5: If m was given as input and t = m, terminate and return f̂ 6: Set t = t +1\nIn this section we introduce, in Strategy 3, an agnostic active learning algorithm called Active-ILESS. Active-ILESS is very similar to Agnostic CAL [10], Algorithm 4.2 on page 36, and A2 [14]. Much like Agnostic CAL, Active-ILESS creates artificial labels (step 1). The two algorithms differ mainly in that Active-ILESS works in batches (inside each batch, the decision whether to query an example is made instantly and not at the end of the batch). This allows Active-ILESS to be a bit more conservative with its deltas. Moreover, while Agnostic CAL requires calculation of an ERM with many constraints (defined by the function LEARN in HSU’s thesis), Active-ILESS requires a calculation of the ERM with only one constraint, as seen from the disbelief principle [24], already discussed in Section 4.\nAlthough ILESS is not novel in and of itself, we use its similarity to Agnostic CAL to demonstrate a deep connection between active learning and selective classification.\nIn Section 7 we use Active-ILESS to show an equivalence between active learning (represented by Active-ILESS) and selective classification (represented by a variant of ILESS, “Batch-ILESS”). The introduction of these new variants facilitates a straightforward proof of the equivalence relationship. This equivalence implies a novel relationship between selective and active classification in the agnostic setting.\nWe begin by analyzing Active-ILESS and showing that much like ILESS, f ∗ ∈Gt in each iteration t. The low-error set G, maintained by ILESS, contains all the hypotheses that have an empirical error smaller than R̂( f̂ )+σILESS. In Lemma 1 we showed that this condition implies that f ∗ resides within the low-error set G of ILESS. A proof that f ∗ ∈ Gt , after each iteration of Active-ILESS, cannot follow the same argument due to the fact that Active-ILESS, shown in Strategy 3, labels by itself each example whose label is not requested from the teacher, and obviously, since we consider an agnostic setting, these self-labels can differ from the true labels.\nActive-ILESS, as seen in Strategy 3, receives as a termination condition either ε > 0 and/or m, and terminates when the radius of its low-error set, Gt , is smaller than ε, or when it has processed m examples.\nActive-ILESS changes its low-error set, Gt , only for t that are natural powers of 2. For each change, Active-ILESS begins to create fake labels for xt ∈ AGR(Gt−1) that may or may not be equal to the real label of xt (under the original distribution). In fact, this Gt defines a new distribution, PX ,Y (Gt), and this distribution changes for every t that is a natural power of 2. With respect to a run of Active-ILESS, and t = 2i, i∈N, we denote by PX ,Y (Gt), the new probability distribution implied by Gt , and the fake labels created by the algorithm. RPX ,Y (Gt)( f ) will be the true risk under the new distribution, while RPX ,Y ( f ) is the true risk of f under the original distribution.\nDefinition 6.1 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. Given a run of Active-ILESS, we denote by K the event where both inequalities (48) and (49) hold simultaneously for every f ∈ F , for all iterations of Active-ILESS where t = 2i, i ∈ N. R̂( f ), R̂( f , Ŝ) for Ŝ before it was initialized:\nRPX ,Y (Gt)( f )≤ R̂( f )+σR−R̂ ( t 2 , δ 2t ,d,R( f ), R̂( f ) ) (48)\nR̂( f )≤ RPX ,Y (Gt)( f )+σR̂−R ( t 2 , δ 2t ,d,R( f ), R̂( f ) ) (49)\nLemma 9 K occurs with probability of at least 1−δ.\nProof Gt changes only for iterations of the type 2i, i∈N. We know by Lemma 1 that the probability that inequalities (48) and (49) do not hold is smaller than δ/(2t). By the union bound, the probability that one of these inequalities does not hold after any iteration is smaller than\n∑ t=2i,i∈N\nδ 2t ≤ δ.\nLemma 10 If f ∗, a true risk minimizer under probability distribution PX ,Y , resides within Gt , then it is also a true risk minimizer under probability distribution PX ,Y (Gt).\nProof\nargmin f∈F RPX ,Y (Gt)( f ) = argmin f∈F RPX ,Y ( f )︸ ︷︷ ︸ A +RPX ,Y (Gt)( f )−RPX ,Y ( f )︸ ︷︷ ︸ B  . We know that f ∗ minimizes A, and we note that every function that resides within Gt minimizes B, because every difference in the labeling between PX ,Y and PX ,Y (Gt) was done according to the label given by the unanimous decision of functions in Gt . Hence, f ∗ minimizes A+B.\nThe proofs of the following four lemmas appear in the appendix. They all show basic good qualities of Active-ILESS.\nLemma 11 Given that event K (see Definition 6.1) occurred, each f ∗ of the original distribution PX ,Y resides within Gt for all t. This implies that RPX ,Y (Gt)( f\n∗)≤ R( f ∗), for all t, as every change in the labeling is done according to f ∗.\nLemma 12 Given that event K (see Definition 6.1) occurred, and under the assumption that Active-ILESS terminated with the ε condition, the hypothesis returned by Active-ILESS, f̂ , holds:\nRPX ,Y ( f̂ )≤ RPX ,Y ( f ∗)+ ε.\nLemma 13 Given that event K (see Definition 6.1) occurred, the final radius of Active-ILESS satisfies\nσActive = O( B m + √ B m ·R( f ∗)), (50)\nwhere B , 16d ln(16m 2e\ndδ ).\nLemma 14 Given that event K (see Definition 6.1) occurred, the total number of examples that Active-ILESS(ε) processed (without necessarily requesting labels) is\nO (\n1 ε ln( 1 ε )+ R( f ∗) ε2 ln( R( f ∗) ε2 )\n) ,\nwhere we hide factors of d, ln(1/δ) under the O.\nDefinition 6.2 An active learner that generates a hypothesis whose true error is smaller than ε w.h.p., has passive example complexity, if it observes up to O ( 1 ε ln( 1 ε )+ R( f ∗) ε2 ln( R( f ∗) ε2 ) ) examples (not necessarily labeled).\nBy Lemmas 12 and 14 we know that Active-ILESS has passive example complexity. The definition of a fast R∗ rejection rate for selective classification induces the following related\ndefinition for exponential speedup of active learning algorithms.\nDefinition 6.3 (R∗ Exponential Speedup) Given PX ,F and f ∗, we say that an active learner has R∗ exponential speedup, with polylog1 and polylog2 as its parameters, if for every PY |X for which f ∗ is a true risk minimizer, and for every m > 0, with probability of at least 1− δ, the number of labels requested by the active learner after observing m examples is not greater than\npolylog1( 1\nR( f ∗)+1/m ) ·R( f ∗)m+polylog2(m,d,1/δ).\nIn [10], Hsu introduced the agnostic CAL algorithm and showed (Theorem 4.3, page 41) that if the disagreement coefficient is bounded, then Agnostic CAL has R∗ exponential speedup (under our new definition). Any active algorithm that has passive example complexity and achieves R∗\nexponential speedup requires w.h.p. no more than O ( polylog(R( f ∗)\nε2 ) R( f ∗)2 ε2 +polylog( 1 ε ) ) labels to\nreach a true error smaller than ε. The proof is immediate by considering the cases R( f ∗)\nε ≥ 1 and R( f ∗)\nε < 1. The leading term of this bound is R( f ∗)2 ε2 , which is also the case for A 2 [14]."
    }, {
      "heading" : "7. A Reduction from Active-iLess to Batch-ILESS",
      "text" : "In Strategy 4 we define a selective classifier, called Batch-ILESS, which uses Active-ILESS as its engine. Given a labeled sample Sm, Batch-ILESS simulates the active algorithm, by applying it over a uniformly random ordering of Sm in a straightforward manner (i.e., it sequentially introduces to the active algorithm an unlabeled example and reveals the label only if the active algorithm requests it). Upon termination, after the active algorithm has consumed all examples, our batch algorithm receives f̂ from the active algorithm and utilizes its last low-error set Gt to define its selection function.\nLemma 11 implies that Batch-ILESS is pointwise-competitive. We note that Lemma 4, Theorem 5 and Theorem 8, which were proven for ILESS, can also be proven for Batch-ILESS. We chose to prove it for ILESS, as it is more simple than Batch-ILESS, and doesn’t require an active algorithm as its engine. We state these ideas formally, and give sketches for their proofs, in the Appendix in Lemma 21 and Theorem 22.\nStrategy 4 Batch Improved Low-Error Selective Strategy (Batch-ILESS) Input: Sample set of size m, Sm,\nConfidence level δ Hypothesis class F with VC dimension d\nOutput: A selective classifier (h,g) 1: Simulate Active-ILESS with a random ordering of Sm as its input stream; let Gt be the low-error set obtained by\nActive-ILESS in its last round, and let f̂ be its resulting classifier. 2: Construct g such that g(x) = 1⇐⇒ x ∈ {X \\DIS (Gt)} 3: h = f̂\nThe following theorem shows a deep connection between the speedup of Active-ILESS to the rejection mass of Batch-ILESS for specific PX ,Y . An immediate corollary of this theorem is that if Active-ILESS has R∗ exponential speedup (see Definition 6.3), then Batch-ILESS has a fast R∗ rejection rate (see Definition 4.2).\nTheorem 15 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. If after observing m examples, with probability of at least 1−δ, the number of labels\nrequested by Active-ILESS is not greater than\npolylog1( 1\nR( f ∗)+1/m ) ·R( f ∗)m+polylog2(m,d,1/δ), (51)\nthen the rejection mass of Batch-ILESS is bounded w.h.p. by\n8 ·polylog1( 1\nR( f ∗)+1/m ) ·R( f ∗)+\n2 (√ ln(2/δ)+ √ ln(2/δ)+2polylog2(2m,2/δ) )2\nm .\nProof Consider an application of Active-ILESS with δ= δ0 over m0 , 2dlog(m+1)e examples. Denote by Xi an indicator random variable for the labeling of its ith example, 1≤ i≤ m0. With probability of at least 1−δ0 over the choice of samples from PX ,Y ,\nm0 ∑ i=1 Xi ≤ polylog1( 1 R( f ∗)+1/m0 ) ·R( f ∗)m0 +polylog2(m0,1/δ0). (52)\nWe know by the definition of Active-ILESS (Strategy 3), that the last m0/2 examples had the exact same probability, ∆Gm0/2, of requiring a label, and that this is exactly the probability that Batch-ILESS will decide to abstain after receiving m examples, according to Strategy 4.\nWe now estimate ∆Gm0/2 using the following version of the Chernoff bound given by Canny [25]. For the sake of self-containment, Canny’s statement and proof of the bound are provided in Lemma 19 in the Appendix.\nThe statement of the lemma is as follows. Let X1,X2, . . . ,Xn be independent Bernoulli trials with Pr[Xi = 1] = p, let X , ∑ni=1 Xi, and µ = EX . Then, for every α > 0:\nPr(X < (1−α)µ)≤ exp(−µα2/2).\nApplying the Chernoff bound with the indicator variables of the last m0/2 examples, we have X = ∑m0m0/2 Xi, µ = p m0 2 , and set p , ∆Gm0/2. Select α such that\nexp(−pm0 2 α2/2) = δ2.\nSolving for α,\nα =\n√ 4ln(1/δ1)\nm0 p .\nWe conclude that with probability of at least 1−δ1,\nX ≥ (1−\n√ 4ln(1/δ1)\nm0 p ) · pm0 2\n⇔ 0≥ pm0 2 − √ pm0 · ln(1/δ1)−X . (53)\nSolving the quadratic equation (53) for √ pm0, we get that\n√ pm0 ≤\n√ ln(1/δ1)+ √ ln(1/δ1)+2X\n1 ⇒ p≤ ( √ ln(1/δ1)+ √ ln(1/δ1)+2X)2\nm0 . (54)\nCombining (52) and (54), from the union bound we get that with probability of at least 1−δ0−δ1,\n∆Gm0/2 ≤\n(√ ln(1/δ1)+ √ ln(1/δ1)+2polylog1( 1R( f ∗)+1/m0 ) ·R( f ∗)m0 +2polylog2(m0,1/δ0) )2\nm0 .\nIf we take δ0 = δ1 = δ/2, then, since m≤ m0 ≤ 2m, we can use √ a+b≤ √ a+ √\nb and (a+b)2 ≤ 2a2 +2b2, to obtain\n∆Gm0/2 ≤\n(√ ln(2/δ)+ √ ln(2/δ)+4polylog1( 1R( f ∗)+1/m) ·R( f ∗)m+2polylog2(2m,2/δ) )2 m\n≤\n(√ ln(2/δ)+ √ ln(2/δ)+2polylog2(2m,2/δ)+ √ 4polylog1( 1 R( f ∗)+1/m) ·R( f ∗)m )2 m\n≤ 2 (√ ln(2/δ)+ √ ln(2/δ)+2polylog2(2m,2/δ) )2 +2 (√ 4polylog1( 1 R( f ∗)+1/m) ·R( f ∗)m )2\nm = 2 (√ ln(2/δ)+ √ ln(2/δ)+2polylog2(2m,2/δ) )2\nm +8 ·polylog1( 1 R( f ∗)+1/m ) ·R( f ∗)\nCorollary 16 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. If after observing m examples, with probability of at least 1−δ, the number of labels requested by Active-ILESS is not greater than\npolylog1( 1\nR( f ∗)+1/m ) ·R( f ∗)m+polylog2(m,d,1/δ),\nthen for every r ≥ R( f ∗), θ f ∗(r)≤ 8 ( 2 (√ ln(2r)+ √ ln(2r)+2polylog2(2/r,2/r) )2 +8 ·polylog1(1/r)+2 ) =O(polylog(1/r)).\nProof The proof follows from Theorems 15 and 7. Applying Theorem 15, we know that for m≤ 1/R( f ∗), the rejection mass of Batch-ILESS is bounded w.h.p. by,\n2 (√ ln(2/δ)+ √ ln(2/δ)+2polylog2(2m,2/δ) )2 +8 ·polylog1( 1R( f ∗)+1/m)\nm .\nApplying Theorem 7 with mmax = 1/R( f ∗), we get that for every r ≥ R( f ∗), θ f ∗(r) ≤ 8 ( 2 (√ ln(2r)+ √ ln(2r)+2polylog2(2/r,2/r) )2 +8 ·polylog1( 1\nR( f ∗)+ r )+3 ) ≤ 8 ( 2 (√ ln(2r)+ √ ln(2r)+2polylog2(2/r,2/r) )2 +8 ·polylog1(1/r)+3 ) .\nNote that the Theorem 7 does not require mmax to be an integer."
    }, {
      "heading" : "8. From the Disagreement Coefficient to Active Learning",
      "text" : "In this section we show that when θ′(r) is bounded by polylog1(1/r) for all r > R( f ∗) for some specific PX ,Y , then the label complexity of Active-ILESS under the same PX ,Y is bounded by\npolylog2( 1\nR( f ∗)+1/m ) ·R( f ∗)m+polylog3(m,d,1/δ), (55)\nwhere the parameters of polylog2 and polylog3 are only dependent on polylog1(1/r). Thus, if θ′(r)≤ polylog1(1/r) for all r > 0, we get that Active-ILESS has R∗ exponential speedup. This direction has been shown before in [9, 10] for agnostic CAL and A2. For the sake of self-containment, we show it here for Active-ILESS. Due to the fact that Active-ILESS relies on ILESS, which we already have bounds for, the proof is straightforward.\nAs a preparation for the theorem, we present Lemma 17 (shown before in [16]), in which we introduce a small feature of the disagreement coefficient that will serve us later.\nLemma 17 Let F be a hypothesis class with a finite VC dimension d, and let PX ,Y be an unknown distribution. For every f ∈ F and 0 < r ≤ 1, θ f (r) · r is a non-decreasing function.\nProof Given 0 < r1 < r2, we will show that θ f (r1) · r1 ≤ θ f (r2) · r2. Assume by contradiction that\nθ f (r1) · r1 > θ f (r2) · r2,\ni.e.,\nsup r>r1 ∆B( f ,r1) r1 · r1 > sup r>r2 ∆B( f ,r2) r2 · r2.\nThis implies, that there exists r1 ≤ r̂ < r2 s.t.\n∆B( f , r̂) r̂ r1 > sup r>r2 ∆B( f ,r2) r2 r2 ≥ ∆B( f ,r2) r2 r2 = ∆B( f ,r2).\nThis contradicts the known monotonicity of ∆B( f ,x).\nTheorem 18 Let F be a hypothesis class with a finite VC dimension d, let PX ,Y be an unknown distribution, and f ∗ is a true risk minimizer of PX ,Y . If for all r > R( f ∗),\nθ′(r)≤ polylog1(1/r),\nthen the label complexity of Active-ILESS(m,δ/2) is bounded by\npolylog1\n( 1\n5R( f ∗)+14 Am\n) 2e ·mR( f ∗)+ log2(2/δ)+56e · log2 m ·A ·polylog1 ( 1\n5R( f ∗)+14 Am\n) ,\nwhich has the same form of Equation (55).\nProof Each run of Active-ILESS(m,δ/2) simulates log2 m runs of ILESS. We know by Lemma 9 that with probability of at least 1− δ/2, inequalities (48) and (49) hold for each run. Recall that we denoted by K the event where both inequalities hold through out all runs of ILESS, which is exactly the definition of event E per run (see Definition 4.1). Under event K , Lemma 11 implies that all f ∗ of the original distribution PX ,Y reside within Gt for all t. This also implies that all f ∗ of the original distribution remain the true risk minimizers under PX ,Y (Gt), for all t, as they always benefit from the creation of the artificial labels.\nBecause the marginal of the distribution does not change during the run of Active-ILESS, and because event E holds for each iteration of ILESS, we can apply Theorem 5 for all of the runs of ILESS. We thus get that for every run of ILESS, the rejection mass is bounded by\n1−Φ(ILESS)≤ θ(R0) ·R0,\nwhere\nR0 , 2 ·R( f ∗)+11 · A m +6 · √ A m ·R( f ∗).\nWe denoted by R( f ∗) the true error according to the original distribution, which might be larger than the true error implied by the fake label distributions that the algorithm induces. However, according to Lemma 17, enlarging R0 can only weaken the bound, and thus, there is no problem doing so. We additionally bound R0 using √ AB≤ A/2+B/2 to get\nR0 ≤ 5 ·R( f ∗)+14 · A m .\nGiven our bound on the disagreement coefficient, we conclude that\n1−Φ(ILESS)≤ polylog1( 1 5 ·R( f ∗)+14 · Am ) · (5 ·R( f ∗)+14 · A m ).\nEach activation of ILESS has delta equals δ4t , and thus, exactly as in Lemma 9, with probability of at least 1−δ/2, they all have a bounded rejection mass simultaneously. We assume that this event occurred. According to the definition of Gt in Strategy 3, the probability distribution of the artificial labeling done by Active-ILESS changes only when t is a natural power of 2. Thus, the probability of requesting label t > 2, denoted by Pt , is bounded by\nPt ≤ polylog1\n( 1\n5 ·R( f ∗)+14 · AT\n) ·5R( f ∗)+ 14A ·polylog1 ( 1 5·R( f ∗)+14· AT ) T , (56)\nwhere T = 2blog2(t−1)c−1. We now have a series of Poisson trials, X1,X2, . . . ,Xm, with Pr(Xt = 1) = Pt , and each Xi is an indicator variable for the labeling of the ith example. We use a version of the Chernoff bound [25] to bound the label complexity.5 The statement and a sketch of the proof of this bound are provided in Lemma 20 in the Appendix.\nFor independent Poisson variables X1,X2, . . . ,Xm, where Pr[Xi = 1] = pi, X , ∑ni=1 Xi, and µ = EX , for every α > 2e−1:\nPr(X > (1+α)µ)≤ 2−µα.\n5. We found this useful bound in [26] (Theorem 5.4).\nTo bound µ = EX from above, we use inequality (56) and plug it into the definition of µ.\nµ = P1 +P2 + m\n∑ i=3 Pt\n≤ 2+ log2 m−1\n∑ k=1 2kP2k+1\n≤ 2+m ·polylog1\n( 1\n5R( f ∗)+14 Am\n) ·R( f ∗)+ log2 m−1\n∑ k=1\n2k 14A ·polylog1\n( 1\n5R( f ∗)+14 Am ) 2k−1\n≤ 2+m ·polylog1\n( 1\n5R( f ∗)+14 Am\n) ·R( f ∗)+28log2 m ·A ·polylog1 ( 1\n5R( f ∗)+14 Am\n) .\n(57)\nWe need to choose an α that satisfies both 2−µα≤ δ/2, and α> 2e−1. Clearly, α= log2(2/δ)µ +2e−1 suffices. Hence, we get that with probability of at least 1−δ/2,\nX ≤ (1+ log2(2/δ) µ +2e−1)µ\n= log2(2/δ)+2eµ.\nInequality (57) holds with probability of at least 1− δ/2, and using the union bound, we get that with probability of at least 1−δ,\nX ≤ log2(2/δ)+2e ( 2+m ·polylog1 ( 1\n5R( f ∗)+14 Am\n) ·R( f ∗)+28log2 m ·A ·polylog1 ( 1\n5R( f ∗)+14 Am\n))\n= polylog1\n( 1\n5R( f ∗)+14 Am\n) 2e ·mR( f ∗)+ log2(2/δ)+56e · log2 m ·A ·polylog1 ( 1\n5R( f ∗)+14 Am ) (58)\nThe dominant factor of Equation (58), if we ignore the logarithmic factors, is mR( f ∗). Active-ILESS has passive example complexity (see Definition 6.2), which means that the total sample complexity is bounded by Õ(1ε + R( f ∗) ε2 ), where Õ(·) hides logarithmic factors. Plugging the sample complexity into m in (58), we get that the total label complexity is bounded by Õ(R( f ∗)2\nε2 ), in cases for which ILESS has a fast R∗ rejection rate. In [27, Theorem 3], Kääriäinen showed that for every active learning algorithm, under a specific (non-trivial) hypothesis class F , there exists a deterministic target function g, and a marginal distribution PX , s.t. the label complexity is Ω̃(R( f ∗)2\nε2 ) (where Ω̃(·) hides logarithmic factors)."
    }, {
      "heading" : "9. Concluding Remarks",
      "text" : "In this paper we focused on disagreement-based methods. Namely, we always required that f ∗ remain inside a low-error subset of hypotheses w.h.p., and made decisions based on disagreement considerations. We introduced a new selective classification algorithm, called ILESS, whose rejection\n“engine” utilizes sharp generalization bounds (which depend on R( f ∗)). Our analysis proves that ILESS has sometimes significantly better rejection guarantees relative to the best known pointwisecompetitive selective strategy of [4]. Moreover, the guarantees we provide for ILESS do not depend at all on the Bernstein assumption. For the general agnostic setting, we showed an equivalence relation between pointwise-competitive selective classification, active learning, and the disagreement coefficient (see Figure 1). This equivalence is formulated in terms of a fast R∗ rejection rate and R∗ exponential speedup (Definitions 4.2 and 6.3). Theorems 7 and 5 show that selective classification with a fast R∗ rejection rate is completely equivalent to having a disagreement coefficient bounded by polylog(1/r) for r > 0. In Section 6, in Strategy 3, we define Active-ILESS using ILESS implicitly as its engine (see State 4 in Strategy 3). We can replace ILESS with another pointwise-competitive selective algorithm, and thus construct a new active learner, that queries a label whenever the selective classifier abstains, and create a fake label according to the decision of the classifier whenever it decides to predict. Because the selective predictor is pointwise-competitive, we know that the underlying distribution induced by its fake labels is equivalent to a distribution defined by a deterministic labeling according to f ∗ and the same PX . The algorithm will terminate using the exact same termination condition as ActiveILESS (when σActive < ε), and thus the total sample complexity (labeled and unlabeled examples) will remain the same. The change will only be in the labeling criterion. Lemmas 10, 11, 12, 13, and 14 can all be generalized to such an algorithm.\nGoing in the other direction to create a selective classifier from a general active learner is more challenging. However, if the active learner follows the Active-ILESS paradigm, and in particular, uses a pointwise-competitive selective classifier to decide on label requests, then a new pointwisecompetitive selective classifier can be created in the same way that Batch-ILESS was created, and then we can obtain a restatement of Theorem 15 providing a reduction from an R∗ exponential speedup of the active algorithm, to a fast R∗ rejection rate of the selective classifier.\nDisagreement-based decision making in active and selective learning leads to “defensive” algorithms. For example, in the active learning case, this means that a defensive algorithm will ask for more labels than a more aggressive algorithm. In selective classification, this defensiveness provides the power to be pointwise-competitive, but will entail an increased rejection rate. It would be interesting to consider more aggressive algorithms that could, for example, take into consideration an estimation of PX in order to ignore examples that cause disagreement only between functions that are very similar to each other (in terms of the probability mass of their difference). Such algorithms can be seen in [28, 29, 30], for the realizable and the low error scenarios. We believe that there is still work to be done for the agnostic scenario.\nMany aggressive algorithms could be devised under assumptions about knowledge of PX (that could be acquired during the run of the algorithm, and is given in the transductive case), or in a Bayesian setting where a prior distribution on F exists. When researching this direction, one might also want to define a cost over unlabeled examples, and discuss the trade-off between labeled and unlabeled examples. The main open question inspired by our results would be to identify similar correspondence between aggressive selective classification algorithms and aggressive active learners.\nAnother aspect of selective classification and active learning, which was not addressed in this paper, is differentiating between more and less noisy areas of the distribution. A noisy area could be defined as an area for which even the best classifier in the class could not achieve a low-error. This motivates a new type of labeling for selective prediction, where one can abstain for two reasons: (i)\nlack of knowledge in a specific region of X , i.e., not enough examples were observed in that region, and the generalization bounds are not sufficiently tight. (ii) The region was well explored, but even the best classifier performs poorly, and thus the answer is unknown (the region is noisy). In our paper, an active learner will query for both scenarios; however, a more clever active learner might only query examples of the first type, as examples of the second type cannot reduce its error."
    }, {
      "heading" : "Appendix A.",
      "text" : "Proof of Lemma 11 We prove the claim by induction over t for which Gt is different from Gt−1. The base case of the induction is clear. We now show that functions that are true risk minimizers of PX ,Y (Gt−1) reside within Gt . According to Lemma 10, f ∗ is a true risk minimizer under PX ,Y (Gt−1) (given the induction hypothesis), and hence will also be within Gt . We refer by f ∗ to a true risk minimizer according to PX ,Y (Gt−1). Using inequality (49) and the definition of σ̄R̂−R,\nR̂( f ∗, Ŝ) ≤ RPX ,Y (Gt−1)( f ∗)+σR̂−R ( t 2 , δ 2t ,d,RPX ,Y (Gt−1)( f ∗), R̂( f ∗, Ŝ) )\n≤ RPX ,Y (Gt−1)( f ∗)+ σ̄R̂−R ( t 2 , δ 2t ,d,RPX ,Y (Gt−1)( f ∗) ) ,\n(59)\nand by inequality (48) and the definition of f̂ we get,\nRPX ,Y (Gt−1)( f ∗) ≤ RPX ,Y (Gt−1)( f̂ ) ≤ R̂( f̂ , Ŝ)+σR−R̂ (\nt 2 , δ 2t ,d,RPX ,Y (Gt−1)( f̂ ), R̂( f̂ , Ŝ) ) ≤ R̂( f̂ , Ŝ)+ σ̂R−R̂ ( t 2 , δ 2t ,d, R̂( f̂ , Ŝ) ) .\n(60)\nPlugging (60) into (59) we get,\nR̂( f ∗, Ŝ) ≤ R̂( f̂ , Ŝ)+ σ̂R−R̂ (\nt 2 , δ 2t ,d, R̂( f̂ , Ŝ) ) + σ̄R̂−R ( t 2 , δ 2t ,d, R̂( f̂ , Ŝ)+ σ̂R−R̂( t 2 , δ 2t ,d, R̂( f̂ , Ŝ)) ) ⇒ f ∗ ∈ Gt . (61)\nProof of Lemma 12 Let Gt−1 be the final low-error set of Active-ILESS, and let Ŝ be the final set of examples. The following inequalities are derived from Lemma 9 and inequalities (59) and (60).\nRPX ,Y (Gt−1)( f̂ ) ≤ R̂( f̂ , Ŝ)+ σ̂R−R̂ ( t 2 , δ 2t ,d, R̂( f̂ , Ŝ) ) ≤ R̂( f ∗, Ŝ)+ σ̂R−R̂ ( t 2 , δ 2t ,d, R̂( f̂ , Ŝ)\n) ≤ RPX ,Y (Gt−1)( f ∗)+ σ̄R̂−R ( t 2 , δ 2t ,d,RPX ,Y (Gt−1)( f ∗) ) + σ̂R−R̂ ( t 2 , δ 2t ,d, R̂( f̂ , Ŝ)\n) ≤ RPX ,Y (Gt−1)( f ∗)+ σ̄R̂−R ( t 2 , δ 2t ,d, R̂( f̂ , Ŝ)+ σ̂R−R̂ ( t 2 , δ 2t ,d, R̂( f̂ , Ŝ) )) + σ̂R−R̂ ( t 2 , δ 2t ,d, R̂( f̂ , Ŝ)\n) ≤ RPX ,Y (Gt−1)( f ∗)+ ε.\nBy Lemma 11 we know that f ∗ resides within Gt−1, which implies that any change in PX ,Y (Gt−1) in comparison to PX ,Y reduces the true error of f ∗. This also means that for every f ∈ F ,\nRPX ,Y ( f )−RPX ,Y (Gt−1)( f )≤ RPX ,Y ( f ∗)−RPX ,Y (Gt−1)( f ∗),\nwhich results in RPX ,Y ( f̂ )≤ RPX ,Y ( f ∗)+ ε.\nProof of Lemma 13 The proof is similar to the proof of Lemma 4. We consider the last modification of Gt as a run of ILESS, under PX ,Y (Gt−1), with m0 , 2blog2mc−1 examples and delta equal to δ4m0 .\nUnder event K , the conditions of Lemma 4 hold, and by Lemma 11, RPX ,Y (Gt−1)( f ∗) ≤ R( f ∗).\nWe simply apply Lemma 4 with these parameters to get A′ (A in Lemma 4).\nA′ = 4d ln( 16m0e\ndδ/4m0 ) = 4d ln( 64m20e dδ ).\nThe fact that m/4≤ m0 ≤ m/2 completes the proof.\nProof of Lemma 14 We know by Lemma 13 that there exist constants C1,C2 that depend only on ln(1δ) and d, and are independent of m, s.t.\nσActive ≤C1 lnm m +C2 √ lnm m ·R( f ∗).\nWe also know by the definition of Active-ILESS (Strategy 3), that it terminates when σActive is smaller than the given ε. We will find m large enough s.t.\nC1 lnm m ≤ ε/2, (62)\nC2 √ lnm m ·R( f ∗)≤ ε/2. (63)\nWe assume that ε≤ 1/e, as it is easy to find a proper m for ε > 1/e. Starting with Equation (62), we want to show that m = O(1ε ln( 1 ε )) satisfies it. Thus, we find k1 s.t.\nC1 ln(k1 1ε ln( 1 ε )) k1 1ε ln( 1 ε ) ≤ ε 2\n⇔ ln(k1 1ε · ln( 1 ε )) ln(1ε ) ≤ k1 2C1 .\nBounding the left-hand side of the equation for ε≤ 1/e gives us,\nln(k1 1ε · ln( 1 ε ))\nln(1ε ) ≤\nln(k1 1ε · 1 ε )\nln(1ε ) ≤ 2+ lnk1.\nWe need to find k1 that will satisfy\n2+ lnk1 ≤ k1\n2C1 .\nk1 = 16C21 will work for C1 ≥ 1; otherwise, we take k1 = 10. We use the same procedure to show that m = O(R( f\n∗) ε2 ln( R( f ∗) ε2 )) satisfies Equation (63). We\nrewrite the equation in the following way:\nlnm m ≤ ε\n2\n4C22R( f ∗) , ε0.\nWe assume that ε0 ≤ 1/e (m = 4 holds otherwise) and find k2 s.t.\nln(k2 1ε0 ln( 1 ε0 ))\nk2 1ε0 ln( 1 ε0 )\n≤ ε0.\nAs before, we reduce the problem to finding k2 that satisfies\n2+ ln(k2)≤ k2.\nk2 = 4 suffices. We thus get that m = O( 1ε20 ln( 1ε20\n)) = O(R( f ∗) ε2 ln( R( f ∗) ε2 )) satisfies Equation (63). This implies that there exists a function m(1/ε,R( f ∗)) = O (\n1 ε ln( 1 ε )+ R( f ∗) ε2 ln( R( f ∗) ε2\n) that bounds the to-\ntal number of labels processed by Active-ILESS.\nLemma 19 [25] Let X1,X2, ...,Xn be independent Bernoulli trials with Pr[Xi = 1] = p, let X , ∑ni=1 Xi, and let µ = EX. Then, for every α≥ 0:\nPr(X < (1−α)µ)≤ exp(−µα2/2).\nProof This proof is taken from the work of John Canny [25]. For t > 0, we have\nPr(X < (1−α)µ) = Pr(exp(−tX)> exp(−t(1−α)µ)). (64)\nWe use Markov’s inequality. For a nonnegative random variable X , and a > 0,\nPr(X ≤ a)≤ E(X) a .\nWe apply the inequality for the right-hand side of Equation (64), to get\nPr(X < (1−α)µ)≤ E(exp(−tX)) exp(−t(1−α)µ) . (65)\nX1,X2, ...,Xn are independent and thus\nE(exp(−tX)) = n\n∏ i=1 E(exp(−tXi)).\nFor each Xi E(exp(−tXi)) = pe−t +(1− p) = 1− p(1− e−t).\nWe use the fact that 1− x < exp(−x) for all x, with x = p(1− e−t), to get\nE(exp(−tXi))≤ exp(−p(1− e−t)),\nand conclude that\nE(exp(−tX)) = n\n∏ i=1\nE(exp(−tXi))≤ n\n∏ i=1\nexp ( −p(1− e−t) ) = exp ( n\n∑ i=1\np(e−t −1) ) = exp ( µ(e−t −1) ) . (66)\nGoing back to Equation (65), we have,\nPr(X < (1−α)µ)≤ exp(µ(e −t −1))\nexp(−t(1−α)µ) = exp\n( µ(e−t −1+ t− tα) ) . (67)\nWe choose t > 0 to make the right-hand side of the equation as small as possible. After derivation, we get that the best t is t = ln( 11−α), and plugging it into Equation (67) gives us,\nPr(X < (1−α)µ) ≤ exp (\nµ(1−α−1+ ln( 1 1−α )− ln( 1 1−α\n)α) )\n= exp (\nµ(−α+ ln( 1 1−α\n)(1−α)) )\n=\n( e−α\n(1−α)1−α\n)µ . (68)\nWe now simplify this bound to get the desired result. We know that (1−α)1−α = e(1−α)ln(1−α), and by Taylor expansion\nln(1−α) =−α− α 2 2 − α 3 3 ...,\nwhich multiplied by (1−α), gives us\n(1−α)ln(1−α) =−α+ α 2 2 +positive terms >−α+ α 2 2 . (69)\nPlugging (69) into Equation (68), we finally get,\nPr(X < (1−α)µ) ≤ (\ne−α\n(1−α)1−α )µ = ( e−α\ne(1−α)ln(1−α) )µ ≤ ( e−α\ne−α+ α2 2 )µ = e−µα 2/2 (70)\nLemma 20 [25] Let X1,X2, ...,Xn be independent Poisson trials with Pr[Xi = 1] = p, let X ,∑ni=1 Xi, and let µ = EX. Then, for every α≥ 2e−1:\nPr(X > (1+α)µ)≤ 2−µα.\nProof Sketch This sketch is taken from the work of John Canny [25]. It is almost identical to the proof of Lemma 19.\nWe start by showing that\nPr(X > (1+α)µ) ≤ (\neα\n(1+α)1+α\n)µ .\nFor every t > 0,\nPr(X > (1+α)µ) = Pr[exp(tX)> exp(t(1+α)µ)].\nAs we did in Lemma 19, we compute the Markov bound,\nPr(X > (1+α)µ) ≤ E(exp(tX)) exp(t(1+α)µ) ,\nand use the fact that Xi are independent, just like in (66), to get that\nE(exp(tX))≤ exp ( µ(et −1) ) .\nThus we get that\nPr(X > (1+α)µ) ≤ exp(µ(e t −1))\nexp(t(1+α)µ) = exp\n( µ(et −1− t−αt) ) .\nFrom deviation, we choose t = ln(1+α) to get\nPr(X > (1+α)µ) ≤ (\neα\n(1+α)1+α\n)µ .\nFor α≥ 2e−1:\nPr(X > (1+α)µ) ≤ (\neα\n(1+α)1+α\n)µ ≤ ( eα\n(2e)1+α\n)µ ≤ ( eα\n(2e)α\n)µ = 2−µα.\nLemma 21 Given that event K (see Definition 6.1) occurred, the radius of Batch-ILESS, as defined in Strategy 3, stage 4, satisfies\nσActive = O ( B m + √ B m ·R( f ∗) ) , (71)\nwhere B , 4d ln(8m 2e\ndδ ).\nProof Batch-ILESS simulates a run of Active-ILESS. Consider a run of Active-ILESSwith m0 examples and δ = δ0. The last iteration in which Gt has changed (relative to Gt−1) was iteration 2blog2 m0c , T . GT is calculated in exactly the same way as ILESS calculates its G under probability distribution PX ,Y (GT−1), when it is provided with T/2 examples, and δ02T as its delta. Assuming that event K occurred, we deduce that event E (see Definition 4.1) occurred as well. Therefore, Lemma 4 holds for the last iteration of Batch-ILESS.\nILESS operates in this run on T/2 labeled examples, and it holds that m0/4≤ T/2≤m0/2. The delta it uses in this run is δ02T > δ0 m0 , so by Lemma 4, we have\nσActive ≤ 6 B\nm0/4 +3\n√ B\nm0/4 ·RPX ,Y (Gt−1)( f ∗) = 24 B m0\n+6 √\nB m0 ·RPX ,Y (Gt−1)( f ∗).\nTo finish the proof, we need to show that R( f ∗)≥ RPX ,Y (Gt−1)( f ∗). From Lemma 11, we know that when K occurs, any f ∗ of the original distribution PX ,Y resides within Gt for all t. Thus, the true error of f ∗ can only decrease under the revised distribution Gt−1( f ∗).\nTheorem 22 Let F be a hypothesis class with VC-dimension d, and let PX ,Y be an unknown probability distribution. Assume that event K (see Definition 6.1) occurred. Then, for all f ∗, the abstain rate is bounded by\n1−Φ(Batch-ILESS)≤ θ f ∗(R0) ·R0,\nwhere\nR0 , 2 ·R( f ∗)+44 · B m +12 · √ B m ·R( f ∗).\nwhere B , 4d ln(8m 2e\ndδ ). This immediately implies (by definition) that\n1−Φ(Batch-ILESS)≤ θ(R0) ·R0.\nProof Sketch The proof is very similar to the proof of Lemma 21. We observe the last modification of GT , and notice that the change was made according to a run of ILESS, on the implied probability distribution PX ,Y (GT−1). Then we simply activate Theorem 5 with the relevant parameters plugged into it.\nNote that by Lemma 11, all f ∗ of the original distribution reside within Gt for all t, and thus, by Lemma 10, they are all true risk minimizers of PX ,Y (GT−1). This also implies that R( f ∗) ≥ RPX ,Y (Gt−1)( f\n∗) and thus can be used to bound Equation (30) of the original theorem that was proven for LESS. θ f is independent of PY |X for all f , and thus the change of the labels does not affect it."
    } ],
    "references" : [ {
      "title" : "On optimum recognition error and reject trade-off",
      "author" : [ "C. Chow" ],
      "venue" : "IEEE Trans. on Information Theory, vol. 16, pp. 41–36, 1970.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Algorithmic Learning in a Random World",
      "author" : [ "V. Vovk", "A. Gammerman", "G. Shafer" ],
      "venue" : "New York: Springer,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "On the foundations of noise-free selective classification",
      "author" : [ "R. El-Yaniv", "Y. Wiener" ],
      "venue" : "Journal of Machine Learning Research, vol. 11, pp. 1605–1641, 2010.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Agnostic pointwise-competitive selective classification",
      "author" : [ "Y. Wiener", "R. El-Yaniv" ],
      "venue" : "Journal of AI Research, vol. 52, pp. 171–201, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning with rejection",
      "author" : [ "C. Cortes", "G. DeSalvo", "M. Mohri" ],
      "venue" : "International Conference on Algorithmic Learning Theory, pp. 67–82, Springer, 2016.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Agnostic selective classification",
      "author" : [ "R. El-Yaniv", "Y. Wiener" ],
      "venue" : "Neural Information Processing Systems (NIPS), pp. 1665–1673, 2011.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Local complexities for empirical risk minimization",
      "author" : [ "P. Bartlett", "S. Mendelson", "P. Philips" ],
      "venue" : "COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers, 2004. 31",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Optimal aggregation of classifiers in statistical learning",
      "author" : [ "A. Tsybakov" ],
      "venue" : "The Annals of Mathematical Statistics, vol. 32, pp. 135–166, 2004.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A bound on the label complexity of agnostic active learning",
      "author" : [ "S. Hanneke" ],
      "venue" : "Proceedings of the 24th International Conference on Machine Learning (ICML), pp. 353–360, 2007.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Algorithms for Active Learning",
      "author" : [ "D. Hsu" ],
      "venue" : "PhD thesis, Department of Computer Science and Engineering, School of Engineering,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Active learning using smooth relative regret approximations with applications",
      "author" : [ "N. Ailon", "R. Begleiter", "E. Ezra" ],
      "venue" : "25th Annual Conference on Learning Theory (COLT), 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A compression technique for analyzing disagreement-based active learning",
      "author" : [ "Y. Wiener", "S. Hanneke", "R. El-Yaniv" ],
      "venue" : "Journal of Machine Learning Research, vol. 16, pp. 713– 745, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On the version space compression set size and its applications",
      "author" : [ "R. El-Yaniv", "Y. Wiener" ],
      "venue" : "Measures of Complexity, pp. 341–357, Springer International Publishing, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Agnostic active learning",
      "author" : [ "M.-F. Balcan", "A. Beygelzimer", "J. Langford" ],
      "venue" : "Proceedings of the 23rd International Conference on Machine Learning, pp. 65–72, ACM, 2006.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Activized learning: Transforming passive to active with improved label complexity",
      "author" : [ "S. Hanneke" ],
      "venue" : "The Journal of Machine Learning Research, vol. 13, no. 5, pp. 1469–1587, 2012.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Theory of disagreement-based active learning",
      "author" : [ "S. Hanneke" ],
      "venue" : "Foundations and Trends R  © in Machine Learning, vol. 7, no. 2-3, pp. 131–309, 2014.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Surrogate losses in passive and active learning",
      "author" : [ "S. Hanneke", "L. Yang" ],
      "venue" : "arXiv:1207.3772, 2012.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A general agnostic active learning algorithm",
      "author" : [ "S. Dasgupta", "D.J. Hsu", "C. Monteleoni" ],
      "venue" : "Advances in Neural Information Processing Systems 20, pp. 353–360, 2007.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Efficient and parsimonious agnostic active learning",
      "author" : [ "T.-K. Huang", "A. Agarwal", "D.J. Hsu", "J. Langford", "R.E. Schapire" ],
      "venue" : "Advances in Neural Information Processing Systems 28 (C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, eds.), pp. 2755–2763, Curran Associates, Inc., 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Active learning via perfect selective classification",
      "author" : [ "R. El-Yaniv", "Y. Wiener" ],
      "venue" : "Journal of Machine Learning Research, vol. 13, pp. 255–279, 2012.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Theoretical Foundations of Selective Prediction",
      "author" : [ "Y. Wiener" ],
      "venue" : "PhD thesis, the Technion — Israel Institute of Technology,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Improving generalization with active learning",
      "author" : [ "D. Cohn", "L. Atlas", "R. Ladner" ],
      "venue" : "Machine Learning, vol. 15, no. 2, pp. 201–221, 1994.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Introduction to statistical learning theory",
      "author" : [ "O. Bousquet", "S. Boucheron", "G. Lugosi" ],
      "venue" : "Advanced Lectures on Machine Learning, vol. 3176 of Lecture Notes in Computer Science, pp. 169–207, Springer, 2003. 32",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Agnostic selective classification",
      "author" : [ "R. El-Yaniv", "Y. Wiener" ],
      "venue" : "Neural Information Processing Systems (NIPS), 2011.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Theory of Active Learning.",
      "author" : [ "S. Hanneke" ],
      "venue" : "http://www.stevehanneke.com,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Active learning in the non-realizable case",
      "author" : [ "M. Kääriäinen" ],
      "venue" : "Lecture Notes in Computer Science, vol 4264. Springer, Berlin, Heidelberg, 2006.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Coarse sample complexity bounds for active learning",
      "author" : [ "S. Dasgupta" ],
      "venue" : "Advances in Neural Information Processing Systems 18, pp. 235–242, 2005.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Selective sampling using the query by committee algorithm",
      "author" : [ "Y. Freund", "H. Seung", "E. Shamir", "N. Tishby" ],
      "venue" : "Machine Learning, vol. 28, pp. 133–168, 1997.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Efficient active learning of halfspaces: an aggressive approach",
      "author" : [ "A. Gonen", "S. Sabato", "S. Shalev-Shwartz" ],
      "venue" : "Journal of Machine Learning Research, vol. 14, no. 1, pp. 2583–2615, 2013. 33",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Introduction Selective classification is a unique and extreme instance of the broader concept of confidencerated prediction [1, 2].",
      "startOffset" : 124,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "Introduction Selective classification is a unique and extreme instance of the broader concept of confidencerated prediction [1, 2].",
      "startOffset" : 124,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "Given a training sample consisting of m labeled instances, the learning algorithm is required to output a selective classifier [3], defined to be a pair ( f ,g), where f is a prediction function, chosen from some hypothesis class F , and g : X → {0,1} is a selection function, serving as a qualifier for f as follows: for any x, if g(x) = 1, the classifier predicts f (x), and otherwise it abstains.",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "The selective classifier ( f ,g) is said to be pointwisecompetitive if, for each x with g(x) = 1, it must hold that f (x) = f ∗(x) for all f ∗ ∈ F [4].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : "The scenario of a predefined decision functions hypothesis class is investigated in [5].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "Pointwise-competitive selective classification (PCS) was first considered in the realizable case [3], for which a simple consistent selective strategy (CSS) was shown to achieve a bounded and monotonically increasing (with m) coverage in various non-trivial settings.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "These results were recently extended to the agnostic setting [4, 6] with a related but different algorithm called low-error selective strategy (LESS), for which a number of coverage bounds were shown.",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "These results were recently extended to the agnostic setting [4, 6] with a related but different algorithm called low-error selective strategy (LESS), for which a number of coverage bounds were shown.",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "These bounds relied on the fact that the underlying probability distribution and the hypothesis class F will satisfy the so-called “(β1,β2)-Bernstein property” [7].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "The coverage bounds in [4, 6] are dependent on the parameters β1,β2.",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "The coverage bounds in [4, 6] are dependent on the parameters β1,β2.",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "This Bernstein property assumption (as presented in [7]), which allows for better concentration, can be problematic.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "It was mentioned in [4] that, under the Tsybakov noise condition [8], the desired property holds, but this is guaranteed only for cases in which the Bayes classifier is within F , which is a fairly strong assumption in itself.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "It was mentioned in [4] that, under the Tsybakov noise condition [8], the desired property holds, but this is guaranteed only for cases in which the Bayes classifier is within F , which is a fairly strong assumption in itself.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "Hanneke’s disagreement coefficient [9] (see Definition 2.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "1), is a well-known parameter of the hypothesis class and the marginal distribution; it is used in most of the known label complexity bounds [9, 10, 11].",
      "startOffset" : 141,
      "endOffset" : 152
    }, {
      "referenceID" : 9,
      "context" : "1), is a well-known parameter of the hypothesis class and the marginal distribution; it is used in most of the known label complexity bounds [9, 10, 11].",
      "startOffset" : 141,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : "1), is a well-known parameter of the hypothesis class and the marginal distribution; it is used in most of the known label complexity bounds [9, 10, 11].",
      "startOffset" : 141,
      "endOffset" : 152
    }, {
      "referenceID" : 11,
      "context" : "Note that in principle, the disagreement coefficient can be replaced by another important quantity, namely, the version space compression set size, recently shown to be equivalent to it [12, 13].",
      "startOffset" : 186,
      "endOffset" : 194
    }, {
      "referenceID" : 12,
      "context" : "Note that in principle, the disagreement coefficient can be replaced by another important quantity, namely, the version space compression set size, recently shown to be equivalent to it [12, 13].",
      "startOffset" : 186,
      "endOffset" : 194
    }, {
      "referenceID" : 13,
      "context" : "This querying strategy, which is often termed “disagreementbased,” has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 14,
      "context" : "This querying strategy, which is often termed “disagreementbased,” has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].",
      "startOffset" : 209,
      "endOffset" : 217
    }, {
      "referenceID" : 15,
      "context" : "This querying strategy, which is often termed “disagreementbased,” has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].",
      "startOffset" : 209,
      "endOffset" : 217
    }, {
      "referenceID" : 16,
      "context" : "This querying strategy, which is often termed “disagreementbased,” has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 17,
      "context" : "This querying strategy, which is often termed “disagreementbased,” has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].",
      "startOffset" : 268,
      "endOffset" : 272
    }, {
      "referenceID" : 18,
      "context" : "In [19], a computationally efficient algorithm for disagreement based AL.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "The first formal relationship between PCS classification and AL was proposed in [20, 21], where the aforementioned CSS algorithm was shown to be equivalent to the well-known CAL AL algorithm of [22], in the sense that a fast coverage rate for CSS was proven to be equivalent to exponential label complexity speedup for CAL.",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 20,
      "context" : "The first formal relationship between PCS classification and AL was proposed in [20, 21], where the aforementioned CSS algorithm was shown to be equivalent to the well-known CAL AL algorithm of [22], in the sense that a fast coverage rate for CSS was proven to be equivalent to exponential label complexity speedup for CAL.",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : "The first formal relationship between PCS classification and AL was proposed in [20, 21], where the aforementioned CSS algorithm was shown to be equivalent to the well-known CAL AL algorithm of [22], in the sense that a fast coverage rate for CSS was proven to be equivalent to exponential label complexity speedup for CAL.",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 8,
      "context" : "the red arrow indicates a previously known result (from [9, 10]) (and can also be deduced from the other arrows).",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "the red arrow indicates a previously known result (from [9, 10]) (and can also be deduced from the other arrows).",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "We acquire the following definitions from [4].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "The disagreement set [9] and agreement set [3] w.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "The disagreement set [9] and agreement set [3] w.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "(5) In selective classification [3], the learning algorithm receives Sm and is required to output a selective classifier, defined to be a pair ( f ,g), where f ∈ F is a classifier, and g : X → {0,1} is a selection function, serving as a qualifier for f as follows.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "Then, Hanneke’s disagreement coefficient [9] of a classifier f ∈ F with respect to the target distribution PX is",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "For more on the disagreement coefficient, and examples of probabilities distributions and hypothesis classes for which it is bounded, see [16].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 17,
      "context" : "Convergence Bounds and LESS We use a uniform convergence bound from [18, 23].",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 22,
      "context" : "Convergence Bounds and LESS We use a uniform convergence bound from [18, 23].",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "Lemma 1 ([18]) Let F be a hypothesis class with VC-dimension d.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "(14) Strategy 1 is the LESS algorithm of [4].",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "In the original lemma from [18], there appears S(H ,n), the growth function.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "We plug in Sauer’s Lemma, S(H ,n)≤ ( em d ) d , into Lemma 1 from [18] to get our lemma.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "Remark 2 The original definition of pointwise-competitiveness from [4] requires a single f ∗.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 15,
      "context" : "In many cases, the disagreement coefficient, θ(r), is bounded by a constant, or by O(polylog(1/r)) for all r > 0 (see [16]).",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "For example, it was shown in [12], that for linear separators under mixture of Gaussians, and for axis-aligned rectangles under product densities over Rk, θ(r) is bounded by O(polylog(1/r)) for all r > 0.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "5, let X = [0,1], and F = { f1, f2} where",
      "startOffset" : 11,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "Let PX be the uniform distribution over [0,1].",
      "startOffset" : 40,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "and for r in [2 ,1], ∆B( f ∗,r) r ≤ 1 1/2 = 2, (45)",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "and for r in [2 ,1], ∆B( f ∗,r) r ≤ 1 1/2 = 2, (45)",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "Active-ILESS is very similar to Agnostic CAL [10], Algorithm 4.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 13,
      "context" : "2 on page 36, and A2 [14].",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 23,
      "context" : "Moreover, while Agnostic CAL requires calculation of an ERM with many constraints (defined by the function LEARN in HSU’s thesis), Active-ILESS requires a calculation of the ERM with only one constraint, as seen from the disbelief principle [24], already discussed in Section 4.",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 9,
      "context" : "In [10], Hsu introduced the agnostic CAL algorithm and showed (Theorem 4.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "The leading term of this bound is R( f ∗)2 ε2 , which is also the case for A 2 [14].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "This direction has been shown before in [9, 10] for agnostic CAL and A2.",
      "startOffset" : 40,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "This direction has been shown before in [9, 10] for agnostic CAL and A2.",
      "startOffset" : 40,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "As a preparation for the theorem, we present Lemma 17 (shown before in [16]), in which we introduce a small feature of the disagreement coefficient that will serve us later.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 24,
      "context" : "We found this useful bound in [26] (Theorem 5.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "Our analysis proves that ILESS has sometimes significantly better rejection guarantees relative to the best known pointwisecompetitive selective strategy of [4].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 26,
      "context" : "Such algorithms can be seen in [28, 29, 30], for the realizable and the low error scenarios.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 27,
      "context" : "Such algorithms can be seen in [28, 29, 30], for the realizable and the low error scenarios.",
      "startOffset" : 31,
      "endOffset" : 43
    }, {
      "referenceID" : 28,
      "context" : "Such algorithms can be seen in [28, 29, 30], for the realizable and the low error scenarios.",
      "startOffset" : 31,
      "endOffset" : 43
    } ],
    "year" : 2017,
    "abstractText" : "A selective classifier ( f ,g) comprises a classification function f and a binary selection function g, which determines if the classifier abstains from prediction, or uses f to predict. The classifier is called pointwise-competitive if it classifies each point identically to the best classifier in hindsight (from the same class), whenever it does not abstain. The quality of such a classifier is quantified by its rejection mass, defined to be the probability mass of the points it rejects. A “fast” rejection rate is achieved if the rejection mass is bounded from above by Õ(1/m) where m is the number of labeled examples used to train the classifier (and Õ hides logarithmic factors). Pointwise-competitive selective (PCS) classifiers are intimately related to disagreement-based active learning and it is known that in the realizable case, a fast rejection rate of a known PCS algorithm (called Consistent Selective Strategy) is equivalent to an exponential speedup of the well-known CAL active algorithm. We focus on the agnostic setting, for which there is a known algorithm called LESS that learns a PCS classifier and achieves a fast rejection rate (depending on Hanneke’s disagreement coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS for which we show a fast rate (depending on Hanneke’s disagreement coefficient) without any assumptions. Our rejection bound smoothly interpolates the realizable and agnostic settings. The main result of this paper is an equivalence between the following three entities: (i) the existence of a fast rejection rate for any PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic bound for Hanneke’s disagreement coefficient; and (iii) an exponential speedup for a new disagreement-based active learner called Active-ILESS.",
    "creator" : "LaTeX with hyperref package"
  }
}