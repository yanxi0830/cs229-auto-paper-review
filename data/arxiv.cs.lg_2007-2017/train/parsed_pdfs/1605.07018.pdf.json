{
  "name" : "1605.07018.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning with Feedback Graphs Without the Graphs",
    "authors" : [ "Alon Cohen", "Tamir Hazan", "Tomer Koren" ],
    "emails" : [ "alon.cohen@technion.ac.il", "tamir.hazan@technion.ac.il", "tomerk@technion.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n07 01\n8v 1\n[ cs\n.L G\n] 2\n3 M\nay 2\n01 6\n? αT q regret over T rounds, provided that the\nindependence numbers of the hidden feedback graphs are at most α. We also extend our results to a more general feedback model, in which the learner does not necessarily observe her own loss, and show that, even in simple cases, concealing the feedback graphs might render a learnable problem unlearnable."
    }, {
      "heading" : "1 Introduction",
      "text" : "Online learning is a general framework for sequential decision-making under uncertainty. In its most basic form, it can be described as follows. A learner has to iteratively choose an action from a set of K available actions, and suffer a loss associated with that action. The losses of the actions on each round are assigned in advance by an arbitrary, possibly adversarial, environment. The learner’s goal is to minimize her regret over T rounds of the game, which is the difference between her cumulative loss and that of the best fixed action in hindsight.\nAfter making each decision, the learner receives some form of feedback about the losses. Traditionally, the literature considers two types of feedback: full feedback (Littlestone and Warmuth, 1994; Vovk, 1990; Cesa-Bianchi et al., 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al. (2002b), where the learner only observes the loss of the action she has actually taken.\nFull feedback and bandit feedback are special cases of a general framework introduced by Mannor and Shamir (2011), in which the feedback model is specified by a sequence G1, . . . , GT of feedback graphs, one for each round t of the game. Each feedback graph Gt is a directed graph whose nodes correspond to the learner’s K possible actions; a directed edge u Ñ v in this graph indicates that whenever the learner chooses action u on round t, in addition to observing the loss of action u, she also gets to observe the loss associated with the action v on that round.\nOnline learning with feedback graphs was further studied by several authors. Alon et al. (2013), and subsequently Kocák et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve rOp ? αT q regret, where α is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al. (2002b); Audibert and Bubeck (2009). The rOp ? αT q bound turns out to be tight for any feedback graph (when it is fixed throughout the game and known in advance), in light of a matching lower bound due to Mannor and Shamir (2011).\nHowever, all of the optimal algorithms mentioned above require the full structure of the feedback graph in order to operate. While some require the entire graph Gt for performing their updates only at the end of round t (e.g., Alon et al., 2013; Kocák et al., 2014; Alon et al., 2015),1 others actually need the description of Gt at the beginning of the round before making their decision (e.g., Alon et al., 2014). In fact, none of the algorithms previously proposed in the literature is able to provide non-trivial regret guarantees without the feedback graphs being disclosed.\nThe assumption that the entire observation system is revealed to the learner on each round, even if only after making her prediction, is rather unnatural. In principle, the learner need not be even aware of the fact that there is a graph underlying the feedback model; the feedback graph is merely a technical notion for us to specify a set of observations for each of the possible actions. Ideally, the only signal we would like the learner to receive following each round is the set of observations that corresponds to the action she has taken on that round (in addition to her own loss).\nAs a motivating example for situations where receiving the entire observation system is unrealistic, consider the following online pricing problem that faces any vendor selling goods over the internet. On each round, the seller has to announce a price for his product. Then, a buyer arrives and decides whether or not to purchase the product at this price based on his private value; the only feedback the seller receives is whether or not the buyer purchased the product at the announced price. However, when a purchase takes place, the seller also knows that the buyer would have bought the product at any price lower than the price that she announced. While this feedback structure can be thought of as a directed graph over the seller’s actions (i.e., prices), the graph itself is never fully revealed to the seller as its structure discloses the buyer’s private value."
    }, {
      "heading" : "1.1 Our contributions",
      "text" : "In this paper, we study online learning with feedback graphs in a setting where the feedback graphs are never revealed to the learner in their entirety. That is, in this setting the only feedback available to the learner at the end of round t is the out-neighborhood of her chosen action in the graph Gt, along with the loss associated with each of the actions in this neighborhood and the loss of the action that she chose. We address the following questions: how this lack of full disclosure affects the learner’s regret? Is it possible to achieve any non-trivial regret guarantee in this setting, i.e., one that improves on the trivial Op ? KT q bound? In particular, can we obtain bounds that scale with the independence numbers of the feedback graphs? Our main results show that not knowing the entire feedback graphs can have a significant impact on the learner’s achievable regret. First, we show that in a standard adversarial online learning setting, where we assume nothing about the process generating the losses and the feedback graphs (i.e., both are possibly chosen by an adversary), any strategy of the learner must suffer Ωp ? KT q regret in the worst case, even if the independence numbers of G1, . . . , GT are all bounded by some small absolute constant. Namely, by hiding the feedback graphs from the learner, the problem surprisingly becomes as hard as the K-armed bandit problem, even when the feedback available to the learner is “almost full”: each of the feedback graphs is “almost a clique.” In other words, the side observations received by the learner are effectively useless; she may as well ignore them and use a standard bandit algorithm such as Exp3 Auer et al. (2002b) to perform optimally.\nSecond, and in contrast to the adversarial setting, we show that in a stochastic setting where the losses of each action are known to be drawn i.i.d. from some unknown probability distribution, side observations can still be very useful. We show that the learner is able to\n1More precisely, these algorithms do not need the entire graph but rather the incoming neighborhood of each\nof the actions for which the associated loss has been observed.\nachieve an optimal regret bound of the form rOp ? αT q, even if the graphs G1, . . . , GT are chosen adversarially and are never fully revealed to the learner, as long as their independence numbers are all bounded by α. We give an efficient elimination-based algorithm achieving this bound, that does not require knowing the value of α in advance. This result is optimal up to logarithmic factors, even when the feedback graph is fixed throughout the game and known in advance, due to a lower bound of Mannor and Shamir (2011).\nFor our algorithm in the stochastic case, we also prove a distribution-dependent regret bound that scales logarithmically with T . The bound we prove is of the form OpřvPV 1p1{∆vq log T q, where ∆v is the gap of action v, and V\n1 is the subset of rOpαq actions with smallest gaps. This bound is a substantial improvement over standard regret bounds of stochastic multi-armed bandit algorithms such as UCB Auer et al. (2002a): whereas the regret of the latter algorithms is typically bounded by a sum ř vPV p1{∆vq taken over all K actions, the sum in our bound is taken only over the subset of rOpαq actions with the smallest gaps. Again, this result cannot be improved even when the feedback graph is fixed throughout the game, and has an optimal dependence on α as well as on the gaps ∆v, thus resolving an open question of Alon et al. (2014).\nFinally, we extend our results to a more general feedback model recently studied by Alon et al. (2015), in which the learner does not necessarily observe her own loss after making predictions (namely, each action may or may not have a self-loop in each feedback graph). Alon et al. (2015) gave a necessary and sufficient condition for attaining Θp ? T q regret in this more general model—a graph-theoretic condition they call strong observability. The extension of our results to their model bears some surprising consequences: for example, even in the strongly observable case with only two actions, not revealing the entire feedback graphs to the learner might make the problem unlearnable! Nevertheless, in the case of stochastic losses, our positive results do extend to the more general feedback model."
    }, {
      "heading" : "1.2 Additional related work",
      "text" : "Online learning with feedback graphs was previously considered in the stochastic setting by Caron et al. (2012), who gave results depending on the graph clique structure. Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014).\nMore recently, Wu et al. (2015) and Kocák et al. (2016) have investigated a noisy version of the feedback graph model, where feedback is specified by a weighted directed graph with edge weights indicating the quality (e.g., the noise level or variance) of the feedback received on adjacent vertices. Wu et al. (2015) provided finite-time problem-dependent lower bounds for this setting; Kocák et al. (2016) generalized the notion of independence number to the noisy case and gave new efficient algorithms in this setting."
    }, {
      "heading" : "2 Setup and Main Results",
      "text" : "We consider a general online learning model with graph-structured feedback, which can be described as a game between a learner and an environment that proceeds for T rounds. Before the game begins, the environment privately determines a sequence of loss functions ℓ1, ..., ℓT : V ÞÑ r0, 1s defined over a set V “ t1, ...,Ku of K actions, which we view as a sequence of loss vectors ℓ1, ..., ℓT P r0, 1sK . In addition, the environment fixes a sequence of directed graphs G1, . . . , GT over V as vertices.\nWe will consider two different cases, that we refer to as the adversarial setting and the stochastic setting:\n• In the adversarial setting, the loss vectors ℓ1, ..., ℓT and the feedback graphs G1, . . . , GT are chosen by the environment in an arbitrary way.\n• In the stochastic setting, the environment privately selects a loss distribution D over r0, 1sK and an arbitrary sequence G1, . . . , GT ; thereafter, the loss vectors ℓ1, . . . , ℓT are sampled i.i.d. from D. An important property of this setting is that the loss vectors are statistically independent from the feedback graphs.\nIteratively on rounds t “ 1, 2, ..., T , the learner randomly chooses an action vt P V and incurs the loss ℓtpvtq. At the end of each round t, the learner receives a feedback comprised of tpv, ℓtpvqq : pvt Ñ vq P Gtqu, that includes the loss ℓtpvtq incurred by the learner (i.e., we assume that pv Ñ vq P Gt for all t and v P V ). In words, the learner observes the losses associated with vt and the actions in the out-neighborhood of vt in the feedback graph Gt. The feedback graph Gt itself is never revealed in its entirety to the learner.\nThe goal of the learner throughout the T rounds of the game is to minimize her expected regret, which is defined as\nRT “ E « Tÿ\nt“1\nℓtpvtq ´ Tÿ\nt“1\nℓtpv‹q ff , (1)\nwhere v‹ “ minvPV Er řT\nt“1 ℓtpvqs is the best action in hindsight. Here, the expectations are taken over the random choices of the learner and, in the stochastic setting, also over the randomness of the losses.\nFor the stochastic setting we require additional notation. For each v P V , we denote by µpvq the mean of the loss of action v under D. We denote µ‹ “ µpv‹q, and let ∆v “ µpvq ´ µ‹ for all v P V . We refer to ∆v as the gap of action v, and assume for simplicity that v‹ is unique so that ∆v ą 0 for all v ‰ v‹.\nFor stating our results, we need a standard graph-theoretic definition. An independent set in a graph G “ pV,Eq (either directed or undirected) is a set of vertices that are not connected by any edges. Namely, S Ď V is independent if for any u, v P S, u ‰ v, it holds that pu Ñ vq R E. The independence number αpGq of G is the size of the largest independent set in G."
    }, {
      "heading" : "2.1 Main results",
      "text" : "We now state the main results of this paper. Our first result deals with the adversarial case and shows that when the feedback graphs are not revealed to the learner at the end of each round, her regret might be very large even when the independence numbers of the graphs are small—they are all bounded by a constant. Theorem 1. In the adversarial setting, any online learning algorithm must suffer at least Ωp ? KT q regret in the worst case, even when all feedback graphs G1, . . . , GT have independence numbers ď Op1q.\nThe lower bound in the theorem is tight: it can be matched by simply running a standard bandit algorithm (e.g., Exp3, Auer et al., 2002b), ignoring all observed feedback besides the loss of the action played.\nOur next result shows that in the stochastic case, the learner is still able to attain non-trivial regret despite the fact that the feedback graphs are never fully revealed to her.\nTheorem 2. In the stochastic setting, Algorithm 1 described in Section 4 attains an expected regret of at most rOp ? αT q, provided that the independence numbers of the graphs G1, . . . , GT are all bounded by α.\nThis regret bound is optimal up to logarithmic factors, since the lower bound of Ωp ? αT q found in Mannor and Shamir (2011) applies in our stochastic setting. In the stochastic setting we also give a distribution-dependent analysis of Algorithm 1 which depends on the gaps of the actions under the distribution D.\nTheorem 3. In the stochastic setting, Algorithm 1 described in Section 4 attains an expected regret of\nO\n˜ ÿ\nvPV 1\n1\n∆v log T\n¸ ,\nwhere V 1 is the set of rOpαq actions with the smallest gaps (excluding v‹), provided that the the independence numbers of the graphs G1, . . . , GT are all bounded by α.\nWe also extend our results to a more general class of feedback graphs, in which each vertex may or may not have a self-loop. For the statements of these additional results, see Section 5."
    }, {
      "heading" : "2.2 Discussion of the results",
      "text" : "Our results show that there is a large gap between the achievable regret rates in the adversarial and stochastic settings, in terms of the dependence on the properties of the feedback graphs.\nIn the adversarial case, the environment is free to simultaneously choose the sequences of loss values and feedback graphs in conjunction with each other; for example, they can be drawn from a joint distribution over sequences of loss values and sequences of directed graphs. The environment may use this freedom to manipulate the feedback observed by the learner and bias her observations in a malicious way. In the stochastic setting, on the other hand, the loss values are drawn from the underlying distribution only after the environment commits to some arbitrary sequence of graphs, so that the feedback graphs are probabilistically independent of the realizations of the losses.\nIn fact, as our arguments in Section 3 reveal, there exists a randomized construction of loss vectors and feedback graphs that inflicts Ωp ? KT q on any learner, in which the loss vectors are i.i.d. However, the stochastic process that generates the feedback graphs in that construction is correlated with the actual realizations of the i.i.d. losses. This is a crucial aspect of our construction, as implied by our upper bound in the stochastic case."
    }, {
      "heading" : "3 Lower Bound for Adversarial Losses",
      "text" : "In this section we deal with the adversarial setting and prove Theorem 1: we show an Ωp ? KT q lower bound on the performance of any online learning algorithm, where both the losses of the actions and the feedback graphs can be chosen arbitrarily.\nLet us sketch the idea behind the lower bound, and defer the formal details to Section 6. By Yao’s minimax principle, in order to prove a lower bound on the learner’s regret it is enough to demonstrate a randomized strategy for the environment that forces any deterministic learner to incur Ωp ? KT q regret. We construct our environment’s strategy as follows.\nFirst, before the game begins, the environment chooses an action v‹ uniformly at random from V . At each round, the loss of all actions v ‰ v‹ is distributed Bernoulli(1{2), while the loss of action v‹ is distributed Bernoulli(1{2´ ǫ) with ǫ “ p1{8q a K{T . All of the loss values in the construction are drawn independently of each other. The feedback graphs G1, . . . , GT are chosen i.i.d. from the following distribution. Any edge u Ñ v for v ‰ v‹ appears with probability 1 ´ 2ǫ independently from all other edges and the losses of the actions. Edges of the form u Ñ v‹ appear mutually independently given the loss of action v‹: if the loss of v‹ is 1, each edge appears with probability 1; if the loss of v‹ is 0, each edge appears with probability p1 ´ 2ǫq{p1 ` 2ǫq. See Figure 1 for a summary of the edge probabilities in this construction.\nThe idea behind the construction is as following. Suppose that the learner plays some action u ‰ v‹, the distributions of the observed losses of every other actions are identical, including that of v‹. In other words, her only option of finding v‹ is by sampling it directly and observing its loss. Hence, the construction is capable of simulating a K-armed bandit problem whose minimax regret is Ωp ? KT q.\nFor the construction above, we prove the following theorem. The proof itself is deferred to Section 6.4.\nTheorem 4. Assume that K ě 2 and T ě K2. Any deterministic learner must suffer an expected regret of at least p1{32q ? KT against the environment constructed above.\nTo show that Theorem 1 holds, we need to show that the learner suffers a large regret against an environment that selects feedback graphs with constant independence numbers. While the independence numbers of the graphs that we have constructed might, in principle, be large, we can show that with very high probability they are uniformly bounded by a constant.\nLemma 5. Suppose that |V | “ K ě 2 and T ě K2. Let G1, ..., GT be a sequence of graphs as constructed above. With probability at least 1´ ǫ{8, the independence numbers of all graphs are at most 9.\nTheorem 1 now follows by combining Theorem 4 and Lemma 5; for the technical details, see Section 6.5."
    }, {
      "heading" : "4 Algorithms for Stochastic Losses",
      "text" : "In this section we present and analyze our algorithm for the stochastic setting. The algorithm, given in Algorithm 1, is reminiscent of elimination-based algorithms for the stochastic multiarmed bandit problem (e.g., Even-Dar et al., 2002; Karnin et al., 2013). For this algorithm, we prove the following guarantee on the expected regret, which implies Theorem 2.\nTheorem 6. Assume that K ě 2. Suppose that Algorithm 1 is run on a sequence of feedback graphs with independence numbers ď α. Then the expected regret of the algorithm is at most rOp ? αT q.\nAlgorithm 1 works in phases r “ 1, 2, . . .. It maintains a subset of actions Vr, where initially V1 “ V . At each phase r, the algorithm estimates the mean losses of all actions in Vr to within ǫr accuracy, by invoking a procedure called AlphaSample nr times. It then filters out from Vr the actions that are known to be 2ǫr-suboptimal with sufficient confidence, and repeats this process, decreasing the accuracy parameter ǫr after each phase.\nThe key for achieving optimal regret lies in the the procedure AlphaSample, that appears as Algorithm 2. Each call to this procedure allows us to observe the losses of all actions in Vr\nAlgorithm 1\ninput Set V of K actions, number of rounds T initialize r Ð 1, V1 “ V , ǫ1 “ 1{4 while |Vr| ą 1 and T rounds have not elapsed do\nSet nr “ r2 logp2KT q{ǫ2rs Invoke AlphaSample(Vr) for nr times, and\ncompute empirical mean mrpvq of each action v P Vr using collected samples\nCompute m‹r “ minvPVr mrpvq Eliminate actions:\nVr`1 “ tv P Vr : mrpvq ď m‹r ` 2ǫru Set ǫr`1 “ ǫr{2, r Ð r ` 1\nend while Play the action left in Vr until T rounds have passed\nonce, while spending only rOpαq rounds in expectation. The exact details of AlphaSample are discussed in Section 4.2 below, and here we just state its guarantee.\nLemma 7. AlphaSample returns one sample of the loss of each action in Vr and terminates after at most 10α logK rounds of the game in expectation, provided that the independence numbers of all feedback graphs G1, . . . , GT are at most α.\nTo prove Theorem 6 we need one additional lemma. It shows that, at each phase, the elimination procedure of the algorithm succeeds with high probability. Namely, after phase r, the algorithm is left with actions that are at most 4ǫr-suboptimal.\nLemma 8. For all r, with probability at least 1´ 1{T we have µpvq ď µ‹` 4ǫr for all v P Vr`1.\nWe can now proceed with the proof of the theorem.\nProof of Theorem 6. Let us start by bounding the number of phases R the algorithm makes. Let the random variable Tr denote the number of game rounds elapsed during phase r. Since the algorithm runs for T rounds we must have that\nRÿ\nr“1\nTr ď T . (2)\nIn particular, since AlphaSample takes at least one round to complete, we have that Tr ě nr ě 2 logp2KT q4r`1 and we get the crude bound of\nR ď r̄ “ 1 2 log2\nˆ 3T 32 logp2KT q ` 1 ˙ . (3)\nWe turn to bound the expected regret of the algorithm. By Lemma 8 and the union bound, the total probability of failure of the mean estimations is at most r̄{T . Then the expected regret of the algorithm is at most the expected regret conditioned on the success of the estimation of the means plus pr̄{T q ¨ T “ r̄ “ Oplog T q by Eq. (3), and since the regret is bounded by T with probability 1. Thus it remains to bound the regret conditioned on the success of the mean estimations.\nFor convenience, define ǫ0 “ 1{2. On phase r, by Lemma 8 we have an instantaneous regret of at most 4ǫr´1 “ 8ǫr per round. If only one action is left in Vr then it must be v‹ and therefore after the final phase the algorithm suffers zero instantaneous expected regret. Overall,\nthe expected regret is at most,\nE\n« Rÿ\nr“1\nTr ¨ 8ǫr ff ď 8 gffeE « Rÿ\nr“1\nTr\nff ¨ gffeE « Rÿ\nr“1\nTrǫ2r\nff\nby the Cauchy-Schwartz inequality. Note that řR\nr“1 Tr ď T by Eq. (2). Additionally, by Lemma 7 each call to AlphaSample spends at most m “ 10α logK rounds in expectation and thus ErTrs ď mnr. Hence,\nE\n« Rÿ\nr“1\nTrǫ 2 r\nff ď r̄ÿ\nr“1\nmnrǫ 2 r ď mr̄p2 logp2KT q ` 1q .\nThe first inequality holds since the number of phases is at most r̄. The right-hand side is Opα logpKq log2pKT qq by Eq. (3) and the definition of m."
    }, {
      "heading" : "4.1 Gap-based analysis",
      "text" : "We can also provide a distribution-dependent analysis of Algorithm 1 that yields a logarithmic regret bound, albeit with an explicit dependence on the gaps ∆v.\nDenote by V pnq the set of n actions with smallest gaps, excluding v‹ and breaking ties arbitrarily.2 Our main result in this section is the following theorem, which gives Theorem 3. Recall that we assume v‹ is the unique optimal action, and so the gaps of all other actions are positive.\nTheorem 9. Suppose that K ě 2 and T ě K, and let τ “ r10α logKs. Suppose that Algorithm 1 is run on a sequence of feedback graphs with independence numbers ď α. Then the expected regret of Algorithm 1 is at most\nO\n˜ ÿ\nvPV pτq\n1\n∆v log T\n¸ .\nWe can explain the intuition behind the bound as follows. Each call to AlphaSample spends at most rOpαq rounds while producing samples of all K actions. Thus, in the worst case, after a quick pruning phase the algorithm is left with the “hardest” τ “ rOpαq actions and has to tell them apart; in this last phase, the additional observations provided by the feedback graphs might not help the algorithm at all (e.g., the remaining τ actions might form an independent set in all graphs). Let us turn to the proof of the theorem.\nProof of Theorem 9. As in the proof of Theorem 6, we have that the expected regret of the algorithm is at most the expected regret conditioned on the success of the mean estimations plus Oplog T q, and thus it remains to bound the regret conditioned on the success of the mean estimations.\nConditioned on the success of the algorithm, the regret of the algorithm is at most the regret of an algorithm that has finished running with Vr “ tv‹u. Thus we can assume that T is large enough for that to happen.\nIf τ ď K ´ 1, we begin by bounding the regret until the algorithm eliminates all actions besides the ones in V pτq. Let ∆̄ be the largest gap of an action from V pτq. Let r̄ “ tlog2p2{∆̄qu. Thus, it takes r̄ ` 1 phases in order for ǫr to be less than ∆̄{4. The regret up to round r̄ is bounded using the following lemma.\n2If n ą K ´ 1, we simply take V pnq to be the set of all actions besides v‹.\nLemma 10. Let m “ 10α logK. The expected regret of Algorithm 1 up to round r̄ is at most 128m\n∆̄ logp2KT q .\nWe proceed with the analysis of the expected regret after phase r̄. This is given by this next lemma.\nLemma 11. The expected regret of Algorithm 1 from round r̄ ` 1 until the end of the game is at most ÿ\nvPV pτq\n128 ∆v logp2KT q .\nIf τ ą K´1 then the regret of the algorithm is given by Lemma 11. Otherwise, the proof of the theorem is completed by noticing that the regret of the algorithm up to round r̄ is at most the regret from round r̄ ` 1 thereafter. Since ∆̄ ě ∆v for all v P V pτq we get that\nm ∆̄ ď m|V pτq|\nÿ\nvPV pτq\n1\n∆v ď\nÿ\nvPV pτq\n1\n∆v ,\nby definition of m and V pτq. This in total gives a regret bound of OpřvPV pτqp1{∆vq logpKT qq. Finally, we use our assumption that T ě K to simplify the bound.\nProof of Lemma 10. By Lemma 7, each call to AlphaSample spends at most m rounds in expectation. By Lemma 8, the instantaneous regret for each round on phase r is at most 4ǫr´1 “ 8ǫr. Then the expected regret up to round r̄ is at most\nr̄ÿ\nr“1\nm ¨ nr ¨ 8ǫr ď 32m logp2KT q r̄ÿ\nr“1\n1 ǫr ,\nand we have r̄ÿ\nr“1\n1 ǫr “\nr̄ÿ\nr“1\n2r`1 ď 2r̄`2 ď 4 ∆̄ .\nProof of Lemma 11. Let us denote r̄v “ tlog2p2{∆vqu, the number of phases until v is removed from Vr. Let w be the action with the minimum nonzero gap. We shall assume that the game is finished after r̄w phases.\nNote that after we have eliminated all actions not in V pτq, each call to AlphaSample is finished after at most |Vr| steps. Thus, the expected regret for the remaining phases is at most\nr̄wÿ\nr“r̄`1\n32 logp2KT q ǫr |Vr| “ 32 logp2KT q ÿ\nvPV pτq\nr̄vÿ\nr“r̄`1\n1 ǫr ,\nand, for all v P V pτq, r̄vÿ\nr“r̄`1\n1 ǫr ď\nr̄vÿ\nr“0\n2r`1 “ 2r̄v`2 ď 4 ∆v ."
    }, {
      "heading" : "4.2 Efficient sampling scheme",
      "text" : "In this section, we discuss the AlphaSample randomized sampling procedure. This procedure allows us to collect one sample of the loss for each action while spending only rOpαq rounds in expectation. AlphaSample is described in Algorithm 2.\nLet us now explain the intuition behind the procedure. At each round, the procedure samples the loss of an action uniformly at random from a subset of actions U . As each sample is uniform over U , the procedure observes the losses of Ωp|U |{αq actions in expectation. The actions that\nAlgorithm 2 AlphaSample\ninput Set of actions U Ď V initialize S Ð H while |U | ą 0 do\nPlay an action u P U uniformly at random, and let W puq be the set of actions observed Collect samples of losses of each w P W puq into S Update U Ð UzW puq\nend while return S\nhave been observed are then removed from U and the process continues recursively until U is empty. This phase is complete after an expected rOpαq rounds.\nThe main result regarding AlphaSample is the following theorem, from which Lemma 7 would follow immediately (see Section 6.2).\nTheorem 12. Algorithm 2 returns one sample of the loss of each action in U and terminates after at most 4α logpK{δq rounds with probability at least 1´δ, provided that all feedback graphs G1, . . . , GT have independence numbers ď α.\nTo analyze the number of rounds that the algorithm spends, we shall define the following random process. Consider an infinite sequence U1, U2, ... such that U1 “ U . For every r ą 0, if Ur is not empty we sample an action uniformly at random from Ur, and we let Ur`1 be Ur after removing the actions whose losses were observed. Otherwise, we let Ur`1 be the empty set.\nThe following lemma lower bounds the expected number of actions whose losses are observed at each iteration of the process.\nLemma 13. Let r ą 0. Let N be the number of actions seen when sampling uniformly at random from Ur. Then, ErN |Urs ě |Ur|{p2αq.\nThe main tool used in the proof of the lemma is the following version of Turán’s theorem (see, e.g., Alon and Spencer, 2008).\nTheorem 14 (Turán). Let G “ pV,Eq be an undirected graph and α be the independence number of G. Then,\nα ě |V | 1` 2|E|{|V | .\nProof of Lemma 13. Fix some feedback graph G “ pV,Eq with independence number ď α, and let doutpvq be the out-degree of vertex v. Note that the independence number of the subgraph over U can only decrease, namely it is also at most α. As such, we shall think of doutpvq as the out-degree of v in the subgraph.\nWe would like to apply Turán’s theorem to the subgraph, which is a directed graph. We do so by constructing an undirected version of the subgraph, namely one in which we ignore the orientation of the edges. Note that the number of edges in the undirected version can only decrease. Therefore,\nErN |Urs “ 1` 1 |Ur| ÿ\nvPUr\ndoutpvq “ 1` |E| |Ur| ě |Ur| 2α ,\nwhere the inequality follows from Turán’s theorem (Theorem 14).\nProof of Theorem 12. By the construction of the random process, the probability that Algorithm 2 spends more than t rounds of the game is exactly the probability that Ut`1 is not empty. To bound this probability we claim that for any r ą 0,\nEr|Ur`1|s ď K expp´r{p2αqq . (4)\nIndeed, fix some i ą 0. By Lemma 13 we have that Er|Ui`1||Uis “ |Ui| ´ ErN |Uis ď |Ui|p1 ´ 1{p2αqq. Taking expectation with respect to Ui and then applying this argument recursively, we get that Er|Ur`1|s ď |U1|p1´ 1{p2αqqr ď K expp´r{p2αqq.\nNow, let t1 “ t2α logpK{δqu ` 1. We will show that the probability that Ut1`1 is not empty is at most δ. By Markov’s inequality and Eq. (4),\nPr|Ut1`1| ą 0s ď Er|Ut1`1|s ď K expp´t1{p2αqq ă δ .\nTo conclude, with probability at least 1´ δ, the number of rounds that the algorithm spends is at most t1 ď 4α logpK{δq, since K ě 2 by assumption."
    }, {
      "heading" : "5 Beyond Bandit Feedback",
      "text" : "In this section we extend our results to a more general class of feedback graphs. In particular, we no longer assume that the learner automatically gets to observe the loss of the action that she chose. Instead, we allow the graphs to have self-loops, namely edges of the form v Ñ v. The absence of self-loops at individual actions allows for feedback models that are not necessarily more informative than the bandit model.\nRecently, Alon et al. (2015) have studied this more general feedback model and divided feedback graphs into three categories: unobservable graphs, for which the induced problem is not learnable; weakly observable graphs, for which rΘpT 2{3q regret is achievable; and strongly observable graphs, for which it is possible to attain rΘp ? T q regret. Their results assume that the feedback graphs are available to the learner, at least after making each prediction. Here, we revisit their results assuming that the graphs are never fully revealed to the learner.\nWe begin by recalling the definitions of observability of Alon et al. (2015). A vertex in a directed graph is observable if it has at least one incoming edge. A vertex is strongly observable if it has either a self-loop or edges incoming from all other vertices. A vertex is weakly observable if it is observable but not strongly observable. A graph is observable if all of its vertices are observable, and it is strongly observable if all of its vertices are strongly observable. A graph is weakly observable if it is observable but not strongly observable. Note that a graph with self-loops at all vertices is necessarily strongly observable.\nLet us now discuss our results; below, we only give the main ideas and sketch the proofs, deferring details to the full version of the paper."
    }, {
      "heading" : "5.1 Strongly observable graphs",
      "text" : "In the adversarial setting, we show that the problem might be unlearnbable even with strongly observable graphs; formally, we prove (see Section 6.6):\nTheorem 15. In the adversarial setting, any algorithm must suffer at least T {16 regret in the worst case, even when G1, . . . , GT are all strongly observable.\nProof sketch. Consider a problem over two actions, u and v. The environment chooses one of two distributions over the choice of the loss of action v, the edge u Ñ v and the self-loop v Ñ v, that are summarized in Figure 2. Each cell in the table is split into two, where the left half is for the first distribution, and the right half is for the second distribution. The two rightmost columns indicate the marginal distributions between the loss of action v and either the edge\nu Ñ v or the self-loop at v. Additionally, the action u always has a self-loop and its loss is constantly 1{2.\nThe key implication of the construction is that under both distributions, whether the learner plays action v or u, she does not observe the loss of v with probability 1{4, she observes a loss of 0 for action v with probability 3{8 and she observes a loss of 1 with probability 3{8. This is although in the first distribution the loss of v is distributed Bernoulli(3{8), and in the second distribution it is distributed Bernoulli(5{8). Therefore, the learner can never tell if u or v is the action with the smaller loss.\nThe result above is in contrast to the stochastic setting; not only that the problem is learnable but there is an algorithm that attains rOp ? αT q regret—the same regret bound that is obtained in the setting where the learner gets to observe the feedback graph fully at the end of each round. In particular, we have:\nTheorem 16. In the stochastic setting, there exists an online learning algorithm that attains rOp ? αT q regret, provided that G1, . . . , GT are all strongly observable.\nProof sketch. The algorithm is the same as Algorithm 1, where the only difference is in the implementation of Algorithm 2. Even if the graph is strongly observable, a subgraph of exactly one vertex might not be observable since this vertex might not have a self-loop. Therefore, we stop the while loop in Algorithm 2 when Vr has one action or less. If it is left with exactly one action, say v, we add a different arbitrary action from V and we sample uniformly at random from both actions until the loss of v is observed. The probability of observing v is at least 1{2, and therefore the modification only adds a constant number of rounds in expectation to the total runtime of Algorithm 2."
    }, {
      "heading" : "5.2 Observable graphs",
      "text" : "For the adversarial setting, the problem is unlearnable since it is already unlearnable for strongly observable graphs by Theorem 15. On the other hand, in the stochastic setting we have the following result.\nTheorem 17. In the stochastic setting, there exists an online learning algorithm that attains rOpK1{3T 2{3q regret, provided that G1, . . . , GT are all observable.\nThis regret bound is tight up to logarithmic factors for weakly observable G1, ..., GT , since the rΩpK1{3T 2{3q lower bound proved by Alon et al. (2015), in the easier setting where the graphs are revealed following each decision, applies in our stochastic setting.\nProof sketch. The algorithm is done in two phases. In the exploration phase, the learner estimates the means of the losses of all actions to ǫ accuracy. The learner simply plays actions uniformly at random. By an argument similar to that of the coupon collector problem, it suffices for the exploration phase to complete after rOpK{ǫ2q rounds. In the exploitation phase, the learner plays the best action found during the exploration phase, and suffers an expected instantaneous regret of at most ǫ per round. In total, the expected regret of the learner is rOpK{ǫ2 ` ǫT q. Setting ǫ “ rΘppK{T q1{3q gives an expected regret bound of rOpK1{3T 2{3q."
    }, {
      "heading" : "6 Additional proofs",
      "text" : ""
    }, {
      "heading" : "6.1 Proof of Lemma 8",
      "text" : "Proof. AlphaSample observed the loss of each action in Vr for nr times. Note that by assumption the losses of the actions are distributed independently from the feedback graphs. Therefore by Hoeffding’s inequality and the union bound we have w.p. at least 1´ T´1,\n@ v P Vr , |µpvq ´mrpvq| ď ǫr .\nDenote by ṽ an action such that mrpṽq “ m‹r. Note that by induction v‹ P Vr`1 since if v‹ P Vr then mrpv‹q ď µ‹ ` ǫr ď µpṽq ` ǫr ď m‹r ` 2ǫr . Therefore, for all v P Vr`1 we have\nµpvq ď mrpvq ` ǫr ď m‹r ` 3ǫr ď mrpv‹q ` 3ǫr ď µ‹ ` 4ǫr ."
    }, {
      "heading" : "6.2 Proof of Lemma 7",
      "text" : "Proof. Notice that Algorithm 2 takes at most K rounds with probability 1. Using Theorem 12 with δ “ α{K, we get that the expected number of rounds for the algorithm to complete is at most 4α logpK2{αq ` pα{Kq ¨K ď 10α logK, since K ě 2 by assumption and α ě 1."
    }, {
      "heading" : "6.3 Proof of Lemma 5",
      "text" : "To prove Lemma 5 we will need the following lemma.\nLemma 18. Let G “ pV,Eq be an undirected Erdös-Rényi graph, such that each edge appears independently with probability p. For any 0 ď δ ď ?1´ p, the independence number of G is at most 2 log1{p1´pqpK{δq ` 1 with probability at least 1´ δ. Proof. Denote τ “ 1{p1 ´ pq and set α “ r2 logτ K ` a 2 logτ p1{δqs. First, consider the concave quadratic x logτ K ´ xpx ´ 1q{2 ´ logτ δ. It is negative for any x ą logτ K ` 1{2 `b log2τ K ´ 2 logτ δ and in particular it is negative for x “ α` 1. Thus we have\npα` 1q logτ K ´ αpα ` 1q{2 ă logτ δ (5)\nNext, suppose that the independence number of the graph is more than α, and therefore there is an independent set of α ` 1 vertices. The probability that a subset S Ď V is an independent set is p1´ pqp |S| 2 q “ τ´p |S| 2 q. By a union bound,\nPrαpGq ą αs ď Kα`1τ´p α`1 2 q “ τ pα`1q logτ K´αpα`1q{2 ă τ logτ δ “ δ .\nwhere the second inequality is by Eq. (5).\nTo finish the proof of the lemma, we have that with probability at least 1´ δ, the independence number of the graph is at most\nα “ R 2 logτ K ` b 2 logτ 1 δ V ď 2 logτ K ` 2 logτ 1δ ` 1 “ 2 logτ Kδ ` 1\nsince δ ď τ´1{2 by assumption.\nWe now turn to prove Lemma 5.\nProof of Lemma 5. Let G be one of the graphs in the sequence, and consider an undirected version of G3. If we remove v‹ from the graph, we get an Erdös-Rényi subgraph with each edge appearing with probability 1´4ǫ2 and independently of other edges. The independence number of the original graph is at most that of the subgraph plus one. Thus, it remains to bound the independence number of the subgraph.\nDenote τ “ 1{p4ǫ2q “ 16T {K. We apply Lemma 18 with δ “ ǫ{p8T q and get that with probability at least 1´δ, the independence number of the subgraph is at most α “ 2 logτ p8KT {ǫq`1. Then by the choice of ǫ and since T ě K2,\nα “ 2 logτ 8KT\nǫ ` 1 “ 2 logτ p64K1{2T 3{2q ` 1\nď 2 logτ ˆ 16T\nK\n˙7{2 ` 1\n“ 2 ¨ 7 2 ` 1 “ 8 ,\nAppying this argument to each of the graphs G1, ..., GT , the claim holds by the union bound."
    }, {
      "heading" : "6.4 Proof of Theorem 4",
      "text" : "To prove the theorem, we shall need a few definitions. Let P,Q be a couple of distributions over the same space and sigma-algebra F . We define the total variation distance between P and Q as\nDTVpP , Qq“ sup EPF |PrEs ´QrEs|\nIf P and Q are discrete distributions, we define the KL divergence between P and Q as\nDKLpP }Qq“ ÿ\nx\nlog ˆ Prxs Qrxs ˙ Prxs\nassuming the support of P is contained in that of Q, and where the sum is taken over the support of P.\nWe can now turn to the proof of the theorem.\nProof of Theorem 4. Let us introduce the random variables Tv whose value is the number of times the learner plays action v. We also introduce the notations Pv and Ev indicating probability and expectation with respect to the marginal distributions under which v‹ “ v. Then,\n3An undirected graph in which there is an edge between u and v if pu, vq P E or pv, uq P E.\nwe have\nRT “ E « Tÿ\nt“1\nℓtpvtq ´ Tÿ\nt“1\nℓtpv‹q ff\n“ 1 K\nÿ\nvPV\nEv\n« Tÿ\nt“1\nℓtpvtq ´ Tÿ\nt“1\nℓtpvq ff\n“ 1 K\nÿ\nvPV\nǫ ¨ EvrT ´ Tvs\n“ ǫ ˜ T ´ 1\nK\nÿ\nvPV\nEvrTvs ¸ , (6)\nand in order to proceed we shall upper bound EvrTvs. Introduce a new distribution, in which the losses of the actions are independent Bernoulli(1{2) variables, and the feedback graphs are such that each directed edge appears with probability 1 ´ 2ǫ independently of the other edges and the losses of the actions. We will refer to this new law using P0 and E0. Let λt be the losses and edges observed at time t, and similarly λptq “ pλ1, ..., λtq are the losses and edges observed up until time t (inclusive). Then, since the sequence λpT q determines the actions of the learner over the entire game, and by Pinsker’s inequality,\nEvrTvs ´ E0rTvs ď T ¨DTV ´ PvrλpT qs , P0rλpT qs ¯\nď T c 1\n2 DKL\n` P0rλpT qs ››PvrλpT qs ˘ . (7)\nMoreover, by the chain rule of KL-divergence, DKL ` P0rλpT qs ››PvrλpT qs ˘ equals\nTÿ\nt“1\nÿ\nλpt´1q\nP0rλpt´1qsDKL ´ P0rλt|λpt´1qs ›››Pvrλt|λpt´1qs ¯ . (8)\nConsider a single term in the sum. Recall that λpt´1q determines the action vt chosen by the learner on round t. If vt ‰ v then, by our construction, the losses and edges of the graph observed by the learner are distributed exactly the same under Pv and P0, and the KL divergence is 0. If vt “ v then the losses of all other actions are distributed Bernoulli(1{2), and independently of the loss of action v and the observed edges. The latter is so under both Pv and P0. Moreover, the observed edges are distributed Bernoulli(1 ´ 2ǫ) independently of the loss of action v under both Pv and P0. Namely, the only element that is distributed differently under Pv and P0 is the loss of action v, and the latter is distributed independently from all other observed variables. Recall that the loss of action v is distributed as Bernoulli(1{2) under P0 and as Bernoulli(1{2 ´ ǫ) under Pv. Therefore, DKL ` P0rλt|λpt´1qs ››Pvrλt|λpt´1qs ˘ is upper-bounded by\nDKL\nˆ 1\n2\n›››› 1\n2 ´ ǫ\n˙ “ ´1\n2 logp1´ 4ǫ2q ď 4ǫ2 ,\nwhere the last inequality holds since ǫ ă 1{4 by assumption. Plugging the above back into Eq. (8),\nDKL ´ P0rλpT qs ›››PvrλpT qs ¯ ď Tÿ\nt“1\nP0rvt “ vs4ǫ2 “ 4ǫ2E0rTvs ,\nand the latter into Eq. (7), we get that EvrTvs ď E0rTvs ` Tǫ a 2E0rTvs.\nNow, K ě 2 by assumption, and therefore 1\nK\nÿ\nvPV\nEvrTvs ď 1\nK\nÿ\nvPV\nE0rTvs ` 1\nK\nÿ\nvPV\nTǫ a 2E0rTvs\nď 1 K\nÿ\nvPV\nE0rTvs ` Tǫ d 1\nK\nÿ\nvPV\n2E0rTvs\n“ T K\n` Tǫ c 2T\nK ď T 2 ` Tǫ\nc 2T\nK .\nLet us now return to Eq. (6). We can lower bound the regret as\nRT ě ǫ ˜ T ´ T\n2 ´ Tǫ\nc 2T\nK\n¸ “ ǫT ˜ 1\n2 ´ ǫ\nc 2T\nK\n¸ .\nBy our choice of ǫ, we have that ǫ a p2T q{K is at most 1{4, and so\nRT ě T\n8\nc K\nT\nˆ 1\n2 ´ 1 4\n˙ “ 1\n32\n? KT ,\nas claimed."
    }, {
      "heading" : "6.5 Proof of Theorem 1",
      "text" : "We shall construct an environment whose distribution over graphs is the same as the environment described in Section 3, conditioned on the event that the independence numbers of all graphs are bounded by 9.\nWe now claim that the regret against this environment is not too far off the regret against the original environment. This is because the expected regret against the original environment is at most the expected regret against this environment plus pǫ{8qT , by Lemma 5 and since the regret is at most T (with probability 1).\nBy Theorem 4, the regret against this environment is at least\nRT ´ ǫ\n8 T ě\n? KT\n32 ´ p1{8q\na K{T\n8 T “\n? KT\n64\nand thus the lower bound holds."
    }, {
      "heading" : "6.6 Proof of Theorem 15",
      "text" : "Proof. By Yao’s minimax principle, in order to prove a lower bound on the learner’s regret it is enough to demonstrate a randomized strategy for the environment that forces any deterministic learner to incur ΩpT q regret. We will construct our environment’s strategy as follows.\nConsider a learning problem over two actions, u and v. Before the game starts, the environment samples an index χ P t1, 2u uniformly at random. If χ “ 1 then the environment plays one distribution; if χ “ 2 then she plays another distribution. Under the first distribution, the loss of v is distributed Bernoulli(3{8); under the second distribution, it is distributed as Bernoulli(5{8). In both cases the action u always has a self-loop and its loss is constantly 1{2.\nThe feedback graphs G1, . . . , GT are chosen i.i.d. and are dependent on the loss of action v. Under the first distribution, if the loss of v is 1, both an edge u Ñ v and the self-loop v Ñ v appear with probability 1; if the loss of v is 0, with probability 2{5 only the edge u Ñ v appears, with probability 2{5 only the self-loop v Ñ v appears and with probability 1{5 both the edge and the self-loop appear. Under the second distribution, if the loss of v is 1, with probability\n2{5 only the edge u Ñ v appears, with probability 2{5 only the self-loop v Ñ v appears and with probability 1{5 both the edge and the self-loop appear; if the loss of v is 0, both an edge u Ñ v and the self-loop v Ñ v appear with probability 1. See Figure 2 for a summary of the edge probabilities in this construction. Note that in every case, the action v either have a self-loop or and incoming edge from u (or both). Therefore, the graphs G1, ...GT are all strongly observable.\nThe key implication of the construction above is as following. Under both distributions, if the learner plays action v, she does not observe the loss of v with probability 1{4, she observes a loss of 0 with probability 3{8 and she observes a loss of 1 with probability 3{8. If the learner plays action u, she does not observe the loss of v with probability 1{4, she observes a loss of 0 with probability 3{8 and she observes a loss of 1 with probability 3{8. Moreover, if the learner plays action v, she does not observe whether there is an incoming edge u Ñ v; if she plays action u she does not observe whether there is a self-loop v Ñ v.\nLet M denote the number of times the learner chooses to play action v. By the argument above she must play the exact same strategy under both distributions, and as a consequence the random variable M is independent of the choice of distribution χ. Hence, the expected regret of the learner against the environment constructed above is at least\n1 2 E\n„ 1\n8 pT ´Mq\nˇ̌ ˇ̌χ “ 1  ` 1\n2 E\n„ 1\n8 M\nˇ̌ ˇ̌χ “ 2  “ 1\n2 E\n„ 1\n8 pT ´Mq\n ` 1\n2 E\n„ 1\n8 M\n “ 1\n16 T\nas claimed."
    }, {
      "heading" : "Acknowledgements",
      "text" : "TK would like to thank Nicolò Cesa-Bianchi and Ofer Dekel for stimulating discussions in the early stages of this research."
    } ],
    "references" : [ {
      "title" : "The Probabilistic Method",
      "author" : [ "N. Alon", "J.H. Spencer" ],
      "venue" : null,
      "citeRegEx" : "Alon and Spencer.,? \\Q2008\\E",
      "shortCiteRegEx" : "Alon and Spencer.",
      "year" : 2008
    }, {
      "title" : "From bandits to experts: A tale of domination and independence",
      "author" : [ "N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Alon et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2013
    }, {
      "title" : "Nonstochastic multi-armed bandits with graph-structured feedback",
      "author" : [ "N. Alon", "N. Cesa-Bianchi", "C. Gentile", "S. Mannor", "Y. Mansour", "O. Shamir" ],
      "venue" : "CoRR, abs/1409.8428,",
      "citeRegEx" : "Alon et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2014
    }, {
      "title" : "Online learning with feedback graphs: Beyond bandits",
      "author" : [ "N. Alon", "N. Cesa-Bianchi", "O. Dekel", "T. Koren" ],
      "venue" : "In Proceedings of The 28th Conference on Learning",
      "citeRegEx" : "Alon et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2015
    }, {
      "title" : "Minimax policies for adversarial and stochastic bandits",
      "author" : [ "J. Audibert", "S. Bubeck" ],
      "venue" : "In Proceedings of the 22nd Conference on Learning Theory (COLT),",
      "citeRegEx" : "Audibert and Bubeck.,? \\Q2009\\E",
      "shortCiteRegEx" : "Audibert and Bubeck.",
      "year" : 2009
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Leveraging side observations in stochastic bandits",
      "author" : [ "S. Caron", "B. Kveton", "M. Lelarge", "S. Bhagat" ],
      "venue" : "In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, Catalina Island, CA,",
      "citeRegEx" : "Caron et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Caron et al\\.",
      "year" : 2012
    }, {
      "title" : "How to use expert advice",
      "author" : [ "N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 1997
    }, {
      "title" : "Pac bounds for multi-armed bandit and markov decision processes",
      "author" : [ "E. Even-Dar", "S. Mannor", "Y. Mansour" ],
      "venue" : "In Computational Learning Theory,",
      "citeRegEx" : "Even.Dar et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Even.Dar et al\\.",
      "year" : 2002
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "Journal of computer and system sciences,",
      "citeRegEx" : "Freund and Schapire.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1997
    }, {
      "title" : "Almost optimal exploration in multi-armed bandits",
      "author" : [ "Z. Karnin", "T. Koren", "O. Somekh" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Karnin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Karnin et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient learning by implicit exploration in bandit problems with side observations",
      "author" : [ "T. Kocák", "G. Neu", "M. Valko", "R. Munos" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kocák et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kocák et al\\.",
      "year" : 2014
    }, {
      "title" : "Online learning with noisy side observations",
      "author" : [ "T. Kocák", "G. Neu", "M. Valko" ],
      "venue" : "In Proceedings of the Nineteenth International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Kocák et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kocák et al\\.",
      "year" : 2016
    }, {
      "title" : "The weighted majority algorithm",
      "author" : [ "N. Littlestone", "M.K. Warmuth" ],
      "venue" : "Information and computation,",
      "citeRegEx" : "Littlestone and Warmuth.,? \\Q1994\\E",
      "shortCiteRegEx" : "Littlestone and Warmuth.",
      "year" : 1994
    }, {
      "title" : "From bandits to experts: On the value of side-observations",
      "author" : [ "S. Mannor", "O. Shamir" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mannor and Shamir.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mannor and Shamir.",
      "year" : 2011
    }, {
      "title" : "Aggregating strategies",
      "author" : [ "V.G. Vovk" ],
      "venue" : "In Proc. Third Workshop on Computational Learning Theory,",
      "citeRegEx" : "Vovk.,? \\Q1990\\E",
      "shortCiteRegEx" : "Vovk.",
      "year" : 1990
    }, {
      "title" : "Online learning with gaussian payoffs and side observations",
      "author" : [ "Y. Wu", "A. György", "C. Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Wu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is never fully revealed to the learner.",
      "startOffset" : 52,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : "Traditionally, the literature considers two types of feedback: full feedback (Littlestone and Warmuth, 1994; Vovk, 1990; Cesa-Bianchi et al., 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al.",
      "startOffset" : 77,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : "Traditionally, the literature considers two types of feedback: full feedback (Littlestone and Warmuth, 1994; Vovk, 1990; Cesa-Bianchi et al., 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al.",
      "startOffset" : 77,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "Traditionally, the literature considers two types of feedback: full feedback (Littlestone and Warmuth, 1994; Vovk, 1990; Cesa-Bianchi et al., 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al.",
      "startOffset" : 77,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : ", 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al. (2002b), where the learner only observes the loss of the action she has actually taken.",
      "startOffset" : 112,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : ", 1997), where the learner observes the losses associated with all of her possible actions, and bandit feedback Auer et al. (2002b), where the learner only observes the loss of the action she has actually taken. Full feedback and bandit feedback are special cases of a general framework introduced by Mannor and Shamir (2011), in which the feedback model is specified by a sequence G1, .",
      "startOffset" : 112,
      "endOffset" : 326
    }, {
      "referenceID" : 1,
      "context" : "Alon et al. (2013), and subsequently Kocák et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "Alon et al. (2013), and subsequently Kocák et al. (2014); Alon et al.",
      "startOffset" : 0,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "Alon et al. (2013), and subsequently Kocák et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? αT q regret, where α is a bound on the independence numbers of the graphs G1, .",
      "startOffset" : 0,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Alon et al. (2013), and subsequently Kocák et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? αT q regret, where α is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al.",
      "startOffset" : 0,
      "endOffset" : 379
    }, {
      "referenceID" : 1,
      "context" : "Alon et al. (2013), and subsequently Kocák et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? αT q regret, where α is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al. (2002b); Audibert and Bubeck (2009).",
      "startOffset" : 0,
      "endOffset" : 434
    }, {
      "referenceID" : 1,
      "context" : "Alon et al. (2013), and subsequently Kocák et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? αT q regret, where α is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al. (2002b); Audibert and Bubeck (2009). The r Op ? αT q bound turns out to be tight for any feedback graph (when it is fixed throughout the game and known in advance), in light of a matching lower bound due to Mannor and Shamir (2011).",
      "startOffset" : 0,
      "endOffset" : 462
    }, {
      "referenceID" : 1,
      "context" : "Alon et al. (2013), and subsequently Kocák et al. (2014); Alon et al. (2015), gave regret-minimization algorithms that achieve r Op ? αT q regret, where α is a bound on the independence numbers of the graphs G1, . . . , GT . Up to logarithmic factors, their results recover and interpolate between the classic bounds of Op ? T logKq with full feedback Freund and Schapire (1997) and Op ? KT q with bandit feedback Auer et al. (2002b); Audibert and Bubeck (2009). The r Op ? αT q bound turns out to be tight for any feedback graph (when it is fixed throughout the game and known in advance), in light of a matching lower bound due to Mannor and Shamir (2011).",
      "startOffset" : 0,
      "endOffset" : 658
    }, {
      "referenceID" : 12,
      "context" : "While some require the entire graph Gt for performing their updates only at the end of round t (e.g., Alon et al., 2013; Kocák et al., 2014; Alon et al., 2015), others actually need the description of Gt at the beginning of the round before making their decision (e.",
      "startOffset" : 95,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "While some require the entire graph Gt for performing their updates only at the end of round t (e.g., Alon et al., 2013; Kocák et al., 2014; Alon et al., 2015), others actually need the description of Gt at the beginning of the round before making their decision (e.",
      "startOffset" : 95,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "” In other words, the side observations received by the learner are effectively useless; she may as well ignore them and use a standard bandit algorithm such as Exp3 Auer et al. (2002b) to perform optimally.",
      "startOffset" : 166,
      "endOffset" : 186
    }, {
      "referenceID" : 10,
      "context" : "This result is optimal up to logarithmic factors, even when the feedback graph is fixed throughout the game and known in advance, due to a lower bound of Mannor and Shamir (2011). For our algorithm in the stochastic case, we also prove a distribution-dependent regret bound that scales logarithmically with T .",
      "startOffset" : 154,
      "endOffset" : 179
    }, {
      "referenceID" : 2,
      "context" : "This bound is a substantial improvement over standard regret bounds of stochastic multi-armed bandit algorithms such as UCB Auer et al. (2002a): whereas the regret of the latter algorithms is typically bounded by a sum ř vPV p1{∆vq taken over all K actions, the sum in our bound is taken only over the subset of r Opαq actions with the smallest gaps.",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "Again, this result cannot be improved even when the feedback graph is fixed throughout the game, and has an optimal dependence on α as well as on the gaps ∆v, thus resolving an open question of Alon et al. (2014). Finally, we extend our results to a more general feedback model recently studied by Alon et al.",
      "startOffset" : 194,
      "endOffset" : 213
    }, {
      "referenceID" : 1,
      "context" : "Again, this result cannot be improved even when the feedback graph is fixed throughout the game, and has an optimal dependence on α as well as on the gaps ∆v, thus resolving an open question of Alon et al. (2014). Finally, we extend our results to a more general feedback model recently studied by Alon et al. (2015), in which the learner does not necessarily observe her own loss after making predictions (namely, each action may or may not have a self-loop in each feedback graph).",
      "startOffset" : 194,
      "endOffset" : 317
    }, {
      "referenceID" : 1,
      "context" : "Again, this result cannot be improved even when the feedback graph is fixed throughout the game, and has an optimal dependence on α as well as on the gaps ∆v, thus resolving an open question of Alon et al. (2014). Finally, we extend our results to a more general feedback model recently studied by Alon et al. (2015), in which the learner does not necessarily observe her own loss after making predictions (namely, each action may or may not have a self-loop in each feedback graph). Alon et al. (2015) gave a necessary and sufficient condition for attaining Θp ? T q regret in this more general model—a graph-theoretic condition they call strong observability.",
      "startOffset" : 194,
      "endOffset" : 503
    }, {
      "referenceID" : 4,
      "context" : "Online learning with feedback graphs was previously considered in the stochastic setting by Caron et al. (2012), who gave results depending on the graph clique structure.",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Kocák et al.",
      "startOffset" : 296,
      "endOffset" : 348
    }, {
      "referenceID" : 1,
      "context" : "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Kocák et al. (2016) have investigated a noisy version of the feedback graph model, where feedback is specified by a weighted directed graph with edge weights indicating the quality (e.",
      "startOffset" : 296,
      "endOffset" : 372
    }, {
      "referenceID" : 1,
      "context" : "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Kocák et al. (2016) have investigated a noisy version of the feedback graph model, where feedback is specified by a weighted directed graph with edge weights indicating the quality (e.g., the noise level or variance) of the feedback received on adjacent vertices. Wu et al. (2015) provided finite-time problem-dependent lower bounds for this setting; Kocák et al.",
      "startOffset" : 296,
      "endOffset" : 633
    }, {
      "referenceID" : 1,
      "context" : "Their analysis, however, only applies when the feedback graph is fixed throughout the game, and can only bound the regret in terms of a quantity akin to the clique-partition number of this graph, which is always larger than its independence number (the gap between the two can be very large; see Alon et al., 2014). More recently, Wu et al. (2015) and Kocák et al. (2016) have investigated a noisy version of the feedback graph model, where feedback is specified by a weighted directed graph with edge weights indicating the quality (e.g., the noise level or variance) of the feedback received on adjacent vertices. Wu et al. (2015) provided finite-time problem-dependent lower bounds for this setting; Kocák et al. (2016) generalized the notion of independence number to the noisy case and gave new efficient algorithms in this setting.",
      "startOffset" : 296,
      "endOffset" : 723
    }, {
      "referenceID" : 15,
      "context" : "This regret bound is optimal up to logarithmic factors, since the lower bound of Ωp ? αT q found in Mannor and Shamir (2011) applies in our stochastic setting.",
      "startOffset" : 100,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "The algorithm, given in Algorithm 1, is reminiscent of elimination-based algorithms for the stochastic multiarmed bandit problem (e.g., Even-Dar et al., 2002; Karnin et al., 2013).",
      "startOffset" : 129,
      "endOffset" : 179
    }, {
      "referenceID" : 1,
      "context" : "Recently, Alon et al. (2015) have studied this more general feedback model and divided feedback graphs into three categories: unobservable graphs, for which the induced problem is not learnable; weakly observable graphs, for which r ΘpT 2{3q regret is achievable; and strongly observable graphs, for which it is possible to attain r Θp ? T q regret.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "Recently, Alon et al. (2015) have studied this more general feedback model and divided feedback graphs into three categories: unobservable graphs, for which the induced problem is not learnable; weakly observable graphs, for which r ΘpT 2{3q regret is achievable; and strongly observable graphs, for which it is possible to attain r Θp ? T q regret. Their results assume that the feedback graphs are available to the learner, at least after making each prediction. Here, we revisit their results assuming that the graphs are never fully revealed to the learner. We begin by recalling the definitions of observability of Alon et al. (2015). A vertex in a directed graph is observable if it has at least one incoming edge.",
      "startOffset" : 10,
      "endOffset" : 639
    }, {
      "referenceID" : 1,
      "context" : ", GT , since the r ΩpK1{3T 2{3q lower bound proved by Alon et al. (2015), in the easier setting where the graphs are revealed following each decision, applies in our stochastic setting.",
      "startOffset" : 54,
      "endOffset" : 73
    } ],
    "year" : 2016,
    "abstractText" : "We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is never fully revealed to the learner. We show a large gap between the adversarial and the stochastic cases. In the adversarial case, we prove that even for dense feedback graphs, the learner cannot improve upon a trivial regret bound obtained by ignoring any additional feedback besides her own loss. In contrast, in the stochastic case we give an algorithm that achieves r Θp ? αT q regret over T rounds, provided that the independence numbers of the hidden feedback graphs are at most α. We also extend our results to a more general feedback model, in which the learner does not necessarily observe her own loss, and show that, even in simple cases, concealing the feedback graphs might render a learnable problem unlearnable.",
    "creator" : "LaTeX with hyperref package"
  }
}