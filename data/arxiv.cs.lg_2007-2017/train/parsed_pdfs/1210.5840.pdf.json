{
  "name" : "1210.5840.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Supervised Learning with Similarity Functions",
    "authors" : [ "Purushottam Kar", "Prateek Jain" ],
    "emails" : [ "purushot@cse.iitk.ac.in", "prajain@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The goal of this paper is to develop an extended framework for supervised learning with similarity functions. Kernel learning algorithms [1] have become the mainstay of discriminative learning with an incredible amount of effort having been put in, both from the theoretician’s as well as the practitioner’s side. However, these algorithms typically require the similarity function to be a positive semi-definite (PSD) function, which can be a limiting factor for several applications. Reasons being: 1) the Mercer’s condition is a formal statement that is hard to verify, 2) several natural notions of similarity that arise in practical scenarios are not PSD, and 3) it is not clear as to why an artificial constraint like PSD-ness should limit the usability of a kernel.\nSeveral recent papers have demonstrated that indefinite similarity functions can indeed be successfully used for learning [2, 3, 4, 5]. However, most of the existing work focuses on classification tasks and provides specialized techniques for the same, albeit with little or no theoretical guarantees. A notable exception is the line of work by [6, 7, 8] that defines a goodness criterion for a similarity function and then provides an algorithm that can exploit this goodness criterion to obtain provably accurate classifiers. However, their definitions are yet again restricted to the problem of classification as they take a “margin” based view of the problem that requires positive points to be more similar to positive points than to negative points by at least a constant margin.\nIn this work, we instead take a “target-value” point of view and require that target values of similar points be similar. Using this view, we propose a generic goodness definition that also admits the\nar X\niv :1\n21 0.\n58 40\nv1 [\ncs .L\nG ]\n2 2\nO ct\n2 01\ngoodness definition of [6] for classification as a special case. Furthermore, our definition can be seen as imposing the existence of a smooth function over a generic space defined by similarity functions, rather than over a Hilbert space as required by typical goodness definitions of PSD kernels.\nWe then adapt the landmarking technique of [6] to provide an efficient algorithm that reduces learning tasks to corresponding learning problems over a linear space. The main technical challenge at this stage is to show that such reductions are able to provide good generalization error bounds for the learning tasks at hand. To this end, we consider three specific problems: a) regression, b) ordinal regression, and c) ranking. For each problem, we define appropriate surrogate loss functions, and show that our algorithm is able to, for each specific learning task, guarantee bounded generalization error with polynomial sample complexity. Moreover, by adapting a general framework given by [9], we show that these guarantees do not require the goodness definition to be overly restrictive by showing that our definitions admit all good PSD kernels as well.\nFor the problem of real-valued regression, we additionally provide a goodness definition that captures the intuition that usually, only a small number of landmarks are influential w.r.t. the learning task. However, to recover these landmarks, the uniform sampling technique would require sampling a large number of landmarks thus increasing the training/test time of the predictor. We address this issue by applying a sparse vector recovery algorithm given by [10] and show that the resulting sparse predictor still has bounded generalization error.\nWe also address an important issue faced by algorithms that use landmarking as a feature constructions step viz [6, 7, 8], namely that they typically assume separate landmark and training sets for ease of analysis. In practice however, one usually tries to overcome paucity of training data by reusing training data as landmark points as well. We use an argument outlined in [11] to theoretically justify such “double dipping” in our case. The details of the argument are given in Appendix B.\nWe perform several experiments on benchmark datasets that demonstrate significant performance gains for our methods over the baseline of kernel regression. Our sparse landmark selection technique provides significantly better predictors that are also more efficient at test time.\nRelated Work: Existing approaches to extend kernel learning algorithms to indefinite kernels can be classified into three broad categories: a) those that use indefinite kernels directly with existing kernel learning algorithms, resulting in non-convex formulations [2, 3]. b) those that convert a given indefinite kernel into a PSD one by either projecting onto the PSD-cone [4, 5] or performing other spectral operations [12]. The second approach is usually expensive due to the spectral operations involved apart from making the method inherently transductive. Moreover, any domain knowledge stored in the original kernel is lost due to these task oblivious operations and consequently, no generalization guarantees can be given. c) those that use notions of “task-kernel alignment” or equivalently, notions of “goodness” of a kernel, to give learning algorithms [6, 7, 8]. This approach enjoys several advantages over the other approaches listed above. These models are able to use the indefinite kernel directly with existing PSD kernel learning techniques; all the while retaining the ability to give generalization bounds that quantitatively parallel those of PSD kernel learning models. In this paper, we adopt the third approach for general supervised learning problem."
    }, {
      "heading" : "2 Problem formulation and Preliminaries",
      "text" : "The goal in similarity-based supervised learning is to closely approximate a target predictor y : X → Y over some domain X using a hypothesis f̂( · ;K) : X → Y that restricts its interaction with data points to computing similarity values given by K. Now, if the similarity function K is not discriminative enough for the given task then we cannot hope to construct a predictor out of it that enjoys good generalization properties. Hence, it is natural to define the “goodness” of a given similarity function with respect to the learning task at hand. Definition 1 (Good similarity function: preliminary). Given a learning task y : X → Y over some distribution D, a similarity function K : X × X → R is said to be ( 0, B)-good with respect to this task if there exists some bounded weighing function w : X → [−B,B] such that for at least a (1− 0) D-fraction of the domain, we have y(x) = E\nx′∼D Jw(x′)y(x′)K(x,x′)K .\nThe above definition is inspired by the definition of a “good” similarity function with respect to classification tasks given in [6]. However, their definition is tied to class labels and thus applies only\nAlgorithm 1 Supervised learning with Similarity functions Input: A target predictor y : X → Y over a distribution D, an ( 0, B)-good similarity function K, labeled\ntraining points sampled from D: T = { (xt1, y1), . . . , (x t n, yn) } , loss function `S : R× Y → R+.\nOutput: A predictor f̂ : X → R with bounded true loss over D 1: Sample d unlabeled landmarks from D: L = { xl1, . . . ,x l d } // Else subsample d landmarks from T (see Appendix B for details)\n2: ΨL : x 7→ 1/ √ d ( K(x,xl1), . . . ,K(x,x l d) ) ∈ Rd 3: ŵ = arg min w∈Rd:‖w‖2≤B ∑n i `S (〈 w,ΨL(x t i) 〉 , yi ) 4: return f̂ : x 7→ 〈ŵ,ΨL(x)〉\nto classification tasks. Similar to [6], the above definition calls a similarity function K “good” if the target value y(x) of a given point x can be approximated in terms of (a weighted combination of) the target values of the K-“neighbors” of x. Also, note that this definition automatically enforces a smoothness prior on the framework.\nHowever the above definition is too rigid. Moreover, it defines goodness in terms of violations, a non-convex loss function. To remedy this, we propose an alternative definition that incorporates an arbitrary (but in practice always convex) loss function.\nDefinition 2 (Good similarity function: final). Given a learning task y : X → Y over some distribution D, a similarity function K is said to be ( 0, B)-good with respect to a loss function `S : R× Y → R if there exists some bounded weighing function w : X → [−B,B] such that if we define a predictor as f(x) := E\nx′∼D Jw(x′)K(x,x′)K, then we have E x∼D J`S(f(x), y(x))K ≤ 0.\nNote that Definition 2 reduces to Definition 1 for `S(a, b) = 1{a6=b}. Moreover, for the case of binary classification where y ∈ {−1,+1}, if we take `S(a, b) = 1{ab≤Bγ}, then we recover the ( 0, γ)-goodness definition of a similarity function, given in Definition 3 of [6]. Also note that, assuming sup\nx∈X {|y(x)|} <∞ we can w.l.o.g. merge w(x′)y(x′) into a single term w(x′).\nHaving given this definition we must make sure that “good” similarity functions allow the construction of effective predictors (Utility property). Moreover, we must make sure that the definition does not exclude commonly used PSD kernels (Admissibility property). Below, we formally define these two properties and in later sections, show that for each of the learning tasks considered, our goodness definition satisfies these two properties."
    }, {
      "heading" : "2.1 Utility",
      "text" : "Definition 3 (Utility). A similarity functionK is said to be 0-useful w.r.t. a loss function `actual (·, ·) if the following holds: there exists a learning algorithm A that, for any 1, δ > 0, when given poly(1/ 1, log(1/δ)) “labeled” and “unlabeled” samples from the input distribution D, with probability at least 1 − δ , generates a hypothesis f̂(x;K) s.t. E\nx∼D\nr `actual ( f̂(x), y(x) )z ≤ 0 + 1.\nNote that f̂(x;K) is restricted to access the data solely through K.\nHere, the 0 term captures the misfit or the bias of the similarity function with respect to the learning problem. Notice that the above utility definition allows for learning from unlabeled data points and thus puts our approach in the semi-supervised learning framework.\nAll our utility guarantees proceed by first using unlabeled samples as landmarks to construct a landmarked space. Next, using the goodness definition, we show the existence of a good linear predictor in the landmarked space. This guarantee is obtained in two steps as outlined in Algorithm 1: first of all we choose d unlabeled landmark points and construct a map Ψ : X → Rd (see Step 1 of Algorithm 1) and show that there exists a linear predictor over Rd that closely approximates the predictor f used in Definition 2 (see Lemma 15 in Appendix A). In the second step, we learn a predictor (over the landmarked space) using ERM over a fresh labeled training set (see Step 3 of Algorithm 1). We then use individual task-specific arguments and Rademacher average-based generalization bounds [13] thus proving the utility of the similarity function."
    }, {
      "heading" : "2.2 Admissibility",
      "text" : "In order to show that our models are not too rigid, we would prove that they admit good PSD kernels. The notion of a good PSD kernel for us will be one that corresponds to a prevalent large margin technique for the given problem. In general, most notions correspond to the existence of a linear operator in the RKHS of the kernel that has small loss at large margin. More formally, Definition 4 (Good PSD Kernel). Given a learning task y : X → Y over some distribution D, a PSD kernelK : X ×X → R with associated RKHSHK and canonical feature map ΦK : X → HK is said to be ( 0, γ)-good with respect to a loss function `K : R× Y → R if there exists W∗ ∈ HK such that ‖W∗‖ = 1 and\nE x∼D\ns `K\n( 〈W∗,ΦK(x)〉\nγ , y(x)\n){ < 0\nWe will show, for all the learning tasks considered, that every ( 0, γ)-good PSD kernel, when treated as simply a similarity function with no consideration of its RKHS, is also ( + 1, B)-good for arbitrarily small 1 with B = h(γ, 1) for some function h. To prove these results we will adapt techniques introduced in [9] with certain modifications and task-dependent arguments."
    }, {
      "heading" : "3 Applications",
      "text" : "We will now instantiate the general learning model described above to real-valued regression, ordinal regression and ranking by providing utility and admissibility guarantees. Due to lack of space, we relegate all proofs as well as the discussion on ranking to the supplementary material (Appendix F)."
    }, {
      "heading" : "3.1 Real-valued Regression",
      "text" : "Real-valued regression is a quintessential learning problem [1] that has received a lot of attention in the learning literature. In the following we shall present algorithms for performing real-valued regression using non-PSD similarity measures. We consider the problem with `actual (a, b) = |a− b| as the true loss function. For the surrogates `S and `K , we choose the -insensitive loss function [1] defined as follows:\n` (a, b) = ` (a− b) = {\n0, if |a− b| < , |a− b| − , otherwise.\nThe above loss function automatically gives us notions of good kernels and similarity functions by appealing to Definitions 4 and 2 respectively. It is easy to transfer error bounds in terms of absolute error to those in terms of mean squared error (MSE), a commonly used performance measure for real-valued regression. See Appendix D for further discussion on the choice of the loss function.\nUsing the landmarking strategy described in Section 2.1, we can reduce the problem of real regression to that of a linear regression problem in the landmarked space. More specifically, the ERM step in Algorithm 1 becomes the following: arg min\nw∈Rd:‖w‖2≤B\n∑n i ` (〈w,ΨL(xi)〉 − yi).\nThere exist solvers (for instance [14]) to efficiently solve the above problem on linear spaces. Using proof techniques sketched in Section 2.1 along with specific arguments for the -insensitive loss, we can prove generalization guarantees and hence utility guarantees for the similarity function. Theorem 5. Every similarity function that is ( 0, B)-good for a regression problem with respect to the insensitive loss function ` (·, ·) is ( 0 + )-useful with respect to absolute loss as well as (B 0 +B )-useful with respect to mean squared error. Moreover, both the dimensionality of the\nlandmarked space as well as the labeled sample complexity can be bounded by O ( B2\n21 log 1δ\n) .\nWe are also able to prove the following (tight) admissibility result: Theorem 6. Every PSD kernel that is ( 0, γ)-good for a regression problem is, for any 1 > 0,( 0 + 1,O ( 1\n1γ2\n)) -good as a similarity function as well. Moreover, for any 1 < 1/2 and any\nγ < 1, there exists a regression instance and a corresponding kernel that is (0, γ)-good for the regression problem but only ( 1, B)-good as a similarity function for B = Ω (\n1 1γ2\n) ."
    }, {
      "heading" : "3.2 Sparse regression models",
      "text" : "An artifact of a random choice of landmarks is that very few of them might turn out to be “informative” with respect to the prediction problem at hand. For instance, in a network, there might exist hubs or authoritative nodes that yield rich information about the learning problem. If the relative abundance of such nodes is low then random selection would compel us to choose a large number of landmarks before enough “informative” ones have been collected.\nHowever this greatly increases training and testing times due to the increased costs of constructing the landmarked space. Thus, the ability to prune away irrelevant landmarks would speed up training and test routines. We note that this issue has been addressed before in literature [8, 12] by way of landmark selection heuristics. In contrast, we guarantee that our predictor will select a small number of landmarks while incurring bounded generalization error. However this requires a careful restructuring of the learning model to incorporate the “informativeness” of landmarks. Definition 7. A similarity function K is said to be ( 0, B, τ)-good for a real-valued regression problem y : X → R if for some bounded weight function w : X → [−B,B] and choice function R : X → {0, 1} with E\nx∼D JR(x)K = τ , the predictor f : x 7→ E x′∼D Jw(x′)K(x,x′)|R(x′)K has\nbounded -insensitive loss i.e. E x∼D J` (f(x), y(x))K < 0.\nThe role of the choice function is to single out informative landmarks, while τ specifies the relative density of informative landmarks. Note that the above definition is similar in spirit to the goodness definition presented in [15]. While the motivation behind [15] was to give an improved admissibility result for binary classification, we squarely focus on the utility guarantees; with the aim of accelerating our learning algorithms via landmark pruning.\nWe prove the utility guarantee in three steps as outlined in Appendix D. First, we use the usual landmarking step to project the problem onto a linear space. This step guarantees the following: Theorem 8. Given a similarity function that is ( 0, B, τ)-good for a regression problem, there exists a randomized map Ψ : X → Rd for d = O ( B2\nτ 21 log 1δ\n) such that with probability at least 1 − δ,\nthere exists a linear operator f̃ : x 7→ 〈w,x〉 over Rd such that ‖w‖1 ≤ B with -insensitive loss bounded by 0 + 1. Moreover, with the same confidence we have ‖w‖0 ≤ 3dτ 2 .\nOur proof follows that of [15], however we additionally prove sparsity of w as well. The number of landmarks required here is a Ω (1/τ) fraction greater than that required by Theorem 5. This formally captures the intuition presented earlier of a small fraction of dimensions (read landmarks) being actually relevant to the learning problem. So, in the second step, we use the Forward Greedy Selection algorithm given in [10] to learn a sparse predictor. The use of this learning algorithm necessitates the use of a different generalization bound in the final step to complete the utility guarantee given below. We refer the reader to Appendix D for the details of the algorithm and its utility analysis. Theorem 9. Every similarity function that is ( 0, B, τ)-good for a regression problem with respect to the insensitive loss function ` (·, ·) is ( 0 + )-useful with respect to absolute loss as well; with the dimensionality of the landmarked space being bounded by O ( B2\nτ 21 log 1δ\n) and the labeled sampled\ncomplexity being bounded by O ( B2\n21 log B 1δ\n) . Moreover, this utility can be achieved by an O (τ)-\nsparse predictor on the landmarked space.\nWe note that the improvements obtained here by using the sparse learning methods of [10] provide Ω (τ) increase in sparsity. We now prove admissibility results for this sparse learning model. We do this by showing that the dense model analyzed in Theorem 5 and that given in Definition 7 are interpretable in each other for an appropriate selection of parameters. The guarantees in Theorem 6 can then be invoked to conclude the admissibility proof. Theorem 10. Every ( 0, B)-good similarity function K is also ( 0, B, w̄ B ) -good where w̄ =\nE x∼D J|w(x)|K. Moreover, every ( 0, B, τ)-good similarity function K is also ( 0, B/τ)-good.\nUsing Theorem 6, we immediately have the following corollary: Corollary 11. Every PSD kernel that is ( 0, γ)-good for a regression problem is, for any 1 > 0,( 0 + 1,O ( 1\n1γ2\n) , 1 ) -good as a similarity function as well."
    }, {
      "heading" : "3.3 Ordinal Regression",
      "text" : "The problem of ordinal regression requires an accurate prediction of (discrete) labels coming from a finite ordered set [r] = {1, 2, . . . , r}. The problem is similar to both classification and regression, but has some distinct features due to which it has received independent attention [16, 17] in domains such as product ratings etc. The most popular performance measure for this problem is the absolute loss which is the absolute difference between the predicted and the true labels.\nA natural and rather tempting way to solve this problem is to relax the problem to real-valued regression and threshold the output of the learned real-valued predictor using predefined thresholds b1, . . . , br to get discrete labels. Although this approach has been prevalent in literature [17], as the discussion in the supplementary material shows, this leads to poor generalization guarantees in our model. More specifically, a goodness definition constructed around such a direct reduction is only able to ensure ( 0 + 1)-utility i.e. the absolute error rate is always greater than 1.\nOne of the reasons for this is the presence of the thresholding operation that makes it impossible to distinguish between instances that would not be affected by small perturbations to the underlying real-valued predictor and those that would. To remedy this, we enforce a (soft) margin with respect to thresholding that makes the formulation more robust to noise. More formally, we expect that if a point belongs to the label i, then in addition to being sandwiched between the thresholds bi and bi+1, it should be separated from these by a margin as well i.e. bi + γ ≤ f(x) ≤ bi+1 − γ. This is a direct generalization of the margin principle in classification where we expect w>x > b+γ for positively labeled points and w>x < b − γ for negatively labeled points. Of course, wherein classification requires a single threshold, we require several, depending upon the number of labels. For any x ∈ R, let [x]+ = max {x, 0}. Thus, if we define the γ-margin loss function to be [x]γ := [γ − x]+ (note that this is simply the well known hinge loss function scaled by a factor of γ), we can define our goodness criterion as follows: Definition 12. A similarity function K is said to be ( 0, B)-good for an ordinal regression problem y : X → [r] if for some bounded weight function w : X → [−B,B] and some (unknown but fixed) set of thresholds {bi}ri=1 with b1 = −∞, the predictor f : x 7→ Ex′∼D Jw(x ′)K(x,x′)K satisfies\nE x∼D\nr[ f(x)− by(x) ] γ + [ by(x)+1 − f(x) ] γ z < 0.\nWe now give utility guarantees for our learning model. We shall give guarantees on both the misclassification error as well as the absolute error of our learned predictor. We say that a set of points x1, . . . , xi . . . is ∆-spaced if min\ni 6=j {|xi − xj |} ≥ ∆. Define the function ψ∆(x) = x+∆−1∆ .\nTheorem 13. Let K be a similarity function that is ( 0, B)-good for an ordinal regression problem with respect to ∆-spaced thresholds and γ-margin loss. Let γ̄ = max {γ, 1}. Then K is ψ(∆/γ̄) ( 0 γ̄ ) -useful with respect to ordinal regression error (absolute loss). Moreover, K is ( 0 γ̄ ) - useful with respect to the zero-one mislabeling error as well.\nWe can bound, both dimensionality of the landmarked space as well as labeled sampled complexity, by O ( B2\n21 log 1δ\n) . Notice that for 0 < 1 and large enough d, n, we can ensure that the ordinal\nregression error rate is also bounded above by 1 since sup x∈[0,1],∆>0 (ψ∆ (x)) = 1. This is in contrast with the direct reduction to real valued regression which has ordinal regression error rate bounded below by 1. This indicates the advantage of the present model over a naive reduction to regression.\nWe can show that our definition of a good similarity function admits all good PSD kernels as well. The kernel goodness criterion we adopt corresponds to the large margin framework proposed by [16]. We refer the reader to Appendix E.3 for the definition and give the admissibility result below. Theorem 14. Every PSD kernel that is ( 0, γ)-good for an ordinal regression problem is also( γ1 0 + 1,O ( γ21 1γ2 )) -good as a similarity function with respect to the γ1-margin loss for any γ1, 1 > 0. Moreover, for any 1 < γ1/2, there exists an ordinal regression instance and a corresponding kernel that is (0, γ)-good for the ordinal regression problem but only ( 1, B)-good as a\nsimilarity function with respect to the γ1-margin loss function for B = Ω ( γ21 1γ2 ) .\nDue to lack of space we refer the reader to Appendix F for a discussion on ranking models that includes utility and admissibility guarantees with respect to the popular NDCG loss."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "In this section we present an empirical evaluation of our learning models for the problems of realvalued regression and ordinal regression on benchmark datasets taken from a variety of sources [18, 19, 20]. In all cases, we compare our algorithms against kernel regression (KR), a well known technique [21] for non-linear regression, whose predictor is of the form:\nf : x 7→ ∑\nxi∈T y(xi)K(x,xi)∑ xi∈T K(x,xi) .\nwhere T is the training set. We selected KR as the baseline as it is a popular regression method that does not require similarity functions to be PSD. For ordinal regression problems, we rounded off the result of the KR predictor to get a discrete label. We implemented all our algorithms as well as the\nbaseline KR method in Matlab. In all our experiments we report results across 5 random splits on the (indefinite) Sigmoid: K(x,y) = tanh(a 〈x,y〉 + r) and Manhattan: K(x,y) = −‖x− y‖1 kernels. Following standard practice, we fixed r = −1 and a = 1/dorig for the Sigmoid kernel where dorig is the dimensionality of the dataset.\nReal valued regression: For this experiment, we compare our methods (RegLand and RegLand-Sp) with the KR method. For RegLand, we constructed the landmarked space as specified in Algorithm 1 and learned a linear predictor using the LIBLINEAR package [14] that minimizes -insensitive loss. In the second algorithm (RegLand-Sp), we used the sparse learning algorithm of [10] on the landmarked space to learn the best predictor for a given sparsity level. Due to its simplicity and good convergence properties, we implemented the Fully Corrective version of the Forward Greedy Selection algorithm with squared loss as the surrogate.\nWe evaluated all methods using Mean Squared Error (MSE) on the test set. Figure 1a shows the MSE incurred by our methods along with reference values of accuracies obtained by KR as landmark sizes increase. The plots clearly show that our methods incur significantly lesser error than KR. Moreover, RegLand-Sp learns more accurate predictors using the same number of landmarks. For instance, when learning using the Sigmoid kernel on the CPUData dataset, at 20 landmarks, RegLand is able to guarantee an MSE of 0.016 whereas RegLand-Sp offers an MSE of less than 0.02 ; MLKR is only able to guarantee an MSE rate of 0.04 for this dataset. In Table 1a, we compare accuracies of the two algorithms when given 50 landmark points with those of KR for the Sigmoid and Manhattan kernels. We find that in all cases, RegLand-Sp gives superior accuracies than KR. Moreover, the Manhattan kernel seems to match or outperform the Sigmoid kernel on all the datasets.\nOrdinal Regression: Here, we compare our method with the baseline KR method on benchmark datasets. As mentioned in Section 3.3, our method uses the EXC formulation of [16] along with landmarking scheme given in Algorithm 1. We implemented a gradient descent-based solver (ORLand) to solve the primal formulation of EXC and used fixed equi-spaced thresholds instead of learning them as suggested by [16]. Of the six datasets considered here, the two Wine datasets are ordinal regression datasets where the quality of the wine is to be predicted on a scale from 1 to 10. The remaining four datasets are regression datasets whose labels were subjected to equi-frequency binning to obtain ordinal regression datasets [16]. We measured the average absolute error (AAE) for each method. Figure 1b compares ORLand with KR as the number of landmarks increases. Table 1b compares accuracies of ORLand for 50 landmark points with those of KR for Sigmoid and Manhattan kernels. In almost all cases, ORLand gives a much better performance than KR. The Sigmoid kernel seems to outperform the Manhattan kernel on a couple of datasets.\nWe refer the reader to Appendix G for additional experimental results."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work we considered the general problem of supervised learning using non-PSD similarity functions. We provided a goodness criterion for similarity functions w.r.t. various learning tasks. This allowed us to construct efficient learning algorithms with provable generalization error bounds. At the same time, we were able to show, for each learning task, that our criterion is not too restrictive in that it admits all good PSD kernels. We then focused on the problem of identifying influential landmarks with the aim of learning sparse predictors. We presented a model that formalized the intuition that typically only a small fraction of landmarks is influential for a given learning problem. We adapted existing sparse vector recovery algorithms within our model to learn provably sparse predictors with bounded generalization error. Finally, we empirically evaluated our learning algorithms on benchmark regression and ordinal regression tasks. In all cases, our learning methods, especially the sparse recovery algorithm, consistently outperformed the kernel regression baseline.\nAn interesting direction for future research would be learning good similarity functions á la metric learning or kernel learning. It would also be interesting to conduct large scale experiments on realworld data such as social networks that naturally capture the notion of similarity amongst nodes."
    }, {
      "heading" : "Acknowledgments",
      "text" : "P. K. is supported by a Microsoft Research India Ph.D. fellowship award. Part of this work was done while P. K. was an intern at Microsoft Research Labs India, Bangalore."
    }, {
      "heading" : "Appendix A Proofs of supplementary theorems",
      "text" : "In this section we give proofs for certain generic results that would be used in the utility and admissibility proofs. The first result, given as Lemma 15, allows us to analyze the landmarking step (Step 1 of Algorithm 1) and allows us to reduce the learning problem to that of learning a linear predictor over the landmarked space. The second result, given as Lemma 16, gives us a succinct re-statement of generalization error bounds proven in [13] that would be used in proving utility bounds. The third result, given as Lemma 17, is a technical result that helps us prove admissibility bounds for our goodness definitions. Lemma 15 (Landmarking approximation guarantee [8]). Given a similarity function K over a domainX and a bounded function of the form f(x) = E\nx′∼D Jw(x′)K(x,x′)K for some bounded weight\nfunction w : X → {−B,B}, for every , δ > 0 there exists a randomized map Ψ : X → Rd for d = d ( , δ) such that with probability at least 1 − δ, there exists a linear operator f̃ over Rd such that E\nx∼D r∣∣∣f̃ (Ψ (x))− f(x)∣∣∣z ≤ . Proof. This result essentially allows us to project the learning problem into a Euclidean space where one can show, for the various learning problems considered here, that existing large margin techniques are applicable to solve the original problem. The result appeared in [8] and is presented here for completeness.\nSample d landmark points L = {x1, . . . ,xd} from D and construct the map ΨL : x 7→ 1√ d\n(K(x,x1), . . . ,K(x,xd)) and consider the linear operator f̃ over Rd defined as follows (in the following, we shall always omit the subscript L for clarity):\nf̃ : x 7→ 1 d d∑ i=1 w(xi)K(x,xi) = 〈w̃,Ψ(x)〉\nfor w = 1√ d (w(x1), . . . , w(xd)) ∈ Rd. A standard Hoeffding-style argument shows that for d = O ( B2\n2 log 1 δ2\n) = O ( B2\n2 log 1 δ\n) , f̃ gives a point wise approximation to f , i.e. for all x ∈ X , with\nprobability greater than 1− δ2, we have ∣∣∣f̃(Ψ(x))− f(x)∣∣∣ < .\nNow call the event BAD-APPROX (x) := ∣∣∣f̃(Ψ(x))− f(x)∣∣∣ > . Thus we have for all x ∈ X ,\nP̃ f [BAD-APPROX (x)] = Ẽ f\nq 1BAD-APPROX(x) y < δ2 (here the probabilities are being taken over\nthe construction of f̃ i.e. the choice of the landmark points). Taking expectations over the entire domain, applying Fubini’s theorem to switch expectations and applying Markov’s inequality we get\nP̃ f\n[ P\nx∼D [BAD-APPROX (x)] > δ\n] < δ\nThus with confidence 1 − δ we have P x∼D [BAD-APPROX (x)] < δ and thus\nE x∼D r∣∣∣f̃(Ψ(x))− f(x)∣∣∣z < (1 − δ) + 2Bδ since sup x∈X ∣∣∣f̃(Ψ(x))∣∣∣ = sup x∈X |f(x)| = B. For\nδ < B we get Ex∼D r∣∣∣f̃(Ψ(x))− f(x)∣∣∣z < 2 . Lemma 16 (Risk bounds for linear predictors [13]). Consider a real-valued prediction problem y over a domain X = {x : ‖x‖2 ≤ CX} and a linear learning model F : {x 7→ 〈w,x〉 : ‖w‖2 ≤ CW } under some fixed loss function ` (·, ·) that is CL-Lipschitz in its second argument. For any f ∈ F , let Lf = E\nx∼D J`(f(x), y(x))K and L̂nf be the empirical loss on a set\nof n i.i.d. chosen points. Then we have, with probability greater than (1− δ),\nsup f∈F\n( Lf − L̂nf ) ≤ 3CLCXCW √ log(1/δ)\nn\nProof. There exist a few results that provide a unified analysis for the generalization properties of linear predictors [13, 22]. However we use the heavy hammer of Rademacher average based analysis since it provides sharper bounds than covering number based analyses.\nThe result follows from imposing a squared L2 regularization on the w vectors. Since the squared L2 function is 2-strongly convex with respect to the L2 norm, using [13, Theorem 1], we get a bound on the Rademacher complexity of the function class F as Rn (F) ≤ CXCW √ 1 n . Next, using the Lipschitz properties of the loss function, a result from [23] allows us to bound the excess error by\n2CLRn(F) + CLCXCW √ log(1/δ) 2n . The result then follows from simple manipulations.\nLemma 17 (Admissible weight functions for PSD kernels [9]). Consider a PSD kernel that is ( 0, γ)-good for a learning problem with respect to some convex loss function `K . Then there exists a vector W′ ∈ HK and a bounded weight function w : X → R such that E\nx∼D J`K (〈W′,ΦK(x)〉 , y(x))K ≤ 0 + 12Cγ2 for some arbitrary positive constant C and for all x ∈ X , we have E x′∼D Jw(x′)K(x,x′)K = 〈W′,ΦK(x)〉.\nProof. Note that the ( 0, γ)-goodness of K guarantees the existence of a weight vector W∗ ∈ HK with small loss at large margin. Thus W′ acts as a proxy for W∗ providing bounded loss at unit margin but with the additional property of being functionally equivalent to a bounded weighted average of the kernel values as required by the definition of a good similarity function. This will help us prove admissibility results for our similarity learning models.\nWe start by proving the theorem for a discrete distribution - the generalization to non-discrete distributions will follow by using variational optimization techniques as discussed in [9]. Consider a discrete learning problem with X = {x1, . . . ,xn}, corresponding distribution D = {p1, . . . , pn} and target y = {y1, . . . , yn} such that ∑ pi = 1. Set up the following regularized ERM problem (albeit on the entire domain):\nmin W∈HK\n1 2 ‖W‖2HK + C n∑ i=1 pi`K (〈W,ΦK(xi)〉 , yi)\nLet W′ be the weight vector corresponding to the optima of the above problem. By the Representer Theorem (for example [24]), we can choose W′ = ∑ αiΦK(xi) for some bounded αi (the exact\nbounds on αi are problem specific). By ( 0, γ)-goodness of K we have\n1 2 ‖W′‖2HK + C n∑ i=1 pi`K (〈W′,ΦK(xi)〉 , yi) ≤ 1 2 ∥∥∥∥ 1γW∗ ∥∥∥∥2 HK + C n∑ i=1 pi`K ( 〈W∗,ΦK(xi)〉 γ , yi ) = 1\n2γ2 + C · E x∼D\ns `K\n( 〈W∗,ΦK(x)〉\nγ , y(x) ){ ≤ 1\n2γ2 + C 0\nThus we have\nE x∼D\nJ`K (〈W′,ΦK(x)〉 , y(x))K ≤ 1\n2C ‖W′‖2HK + n∑ i=1 pi`K (〈W′,ΦK(xi)〉 , yi)\n≤ 0 + 1\n2Cγ2\nwhich proves the first part of the claim. For the second part, set up a weight function wi = αipi . Then, for any x ∈ X we have\nE x′∼D Jw(x′)K(x,x′)K = n∑ i=1 piwiK(x,xi) = n∑ i=1 pi αi pi K(x,xi)\n= n∑ i=1 αi 〈ΦK(x),ΦK(xi)〉 = 〈W′,ΦK(x)〉\nThe weight function is bounded since the αi are bounded and, this being a discrete learning problem, cannot have vanishing probability masses pi (actually, in the cases we shall consider, the αi will itself contain a pi term that will subsequently get cancelled). For non-discrete cases, variational techniques give us similar results."
    }, {
      "heading" : "Appendix B Justifying Double-dipping",
      "text" : "All our analyses (as well as the analyses presented in [6, 7, 8]) use some data as landmark points and then require a fresh batch of training points to learn a classifier on the landmarked space. In practice, however, it might be useful to reuse training data to act as landmark points as well. This is especially true of [7, 8] who require labeled landmarks. We give below, generalization bounds for similarity-based learning algorithms that indulge in such “double dipping”. The argument uses a technique outlined in [11] and falls within the Rademacher-average based uniform convergence guarantees used elsewhere in the paper. We present a generic argument that, in a manner similar to Lemma 16, can be specialized to the various learning problems considered in this paper.\nTo make the presentation easier we set up some notation. For any predictor f , let Lf = E\nx∼D J`(f(x), y(x))K and for any training set S of size n, let L̂Sf = 1n ∑ xi∈S `(f(xi), y(xi)). For any landmark set S = (x1, . . . ,xn), we let ΨS : x 7→ (K(x,x1), . . . ,K(x,xn)). For any weight vector w ∈ Rn, ‖w‖∞ ≤ B in the landmarked space, denote the predictor\nf(S,w) := 1 n 〈w,ΨS(x)〉 = x 7→ 1 n n∑ i=1 wiK(x,xi). Also let FS := { x 7→ 1n 〈w,ΨS(x)〉 } ={\nf(S,w) : w ∈ Rn, ‖w‖∞ ≤ B } .\nWe note that the embedding defined above is “stable” in the sense that changing a single landmark does not change the embedding too much with respect to bounded predictors. More formally, for any set of n points S = (x1, . . . ,xn), define g(S) := sup\nf∈FS\n{ Lf − L̂Sf } . Let Si be another set of n\npoints that (arbitrarily) differs from S just at the ith point and coincides with S on the rest. Then we have, for any fixed w of bounded L∞ norm (i.e. ‖w‖∞ ≤ B) and bounded similarity function (i.e.\nK(x,y) ≤ 1),\nsup x {∣∣f(S,w)(x)− f(Si,w)(x)∣∣} = sup x  ∣∣∣∣∣∣ 1n n∑ j=1 wjK(x,xj)− 1 n n∑ i=1 wjK(x,x ′ j) ∣∣∣∣∣∣ \n= sup x {∣∣∣∣ 1nwi (K(x,xi)−K(x,x′i)) ∣∣∣∣}\n≤ 2B n\nNote that, although [8] uses pairs of labeled points to define the embedding, the following argument can easily be extended to incorporate this since the embedding is identical to the embedding ΨS described above with respect to being “stable”. In fact this analysis holds for any stable embedding defined using training points.\nOur argument proceeds by showing that with high probability (over choice of the set S) we have\nsup w\n{ Lf(S,w) − L̂ S f(S,w) } ≤\nBy the definition of FS , the above requirement translates to showing that with high probability,\nsup f∈FS\n{ Lf − L̂Sf } ≤\nwhich highlights the fact that we are dealing with a problem of sample dependent hypothesis spaces1. Note that this exactly captures the double dipping procedure of reusing training points as landmark points. Such a result would be useful as follows: using Lemma 15 and task specific guarantees (outlined in detail in the subsequent sections), we have, with high probability, the existence of a good predictor in the landmarked space of a randomly chosen landmark set S i.e. with very high probability over choice of S, we have inf\nf∈FS {Lf} ≤ 0. Let this be achieved by the predictor f∗.\nUsing the uniform convergence guarantee above we get L̂Sf∗ ≤ 0 + (with some loss of confidence due to application of a union bound).\nNow consider the predictor f̂ := inf f∈FS\n{ L̂Sf } . Clearly L̂S f̂ ≤ L̂Sf∗ ≤ 0 + . Invoking the uniform\nconvergence bound yet again shows us that\nLf̂ ≤ L̂ S f̂\n+ sup f∈FS\n{ Lf − L̂Sf } ≤ 0 + 2\nNote that we incur some more loss of confidence due to another application of the union bound. This tells us that with high probability, a predictor learned by choosing a random landmark set and training on the landmark set itself would yield a good predictor.\nWe will proceed via a vanilla uniform convergence argument involving symmetrization and an application of the McDiarmid’s inequality (stated below). However, proving the stability prerequisite for the application of the McDiarmid’s inequality shall require use of the stability of both the predictor f(S,w) as well as the embedding ΨS . Let the loss function ` be CL-Lipschitz in its first argument. Theorem 18 (McDiarmid’s inequality [25]). Let X1, . . . , Xn be independent random variables taking values in some set X . Further, let f : Xn → R be a function of n variables that satisfies, for all i ∈ [n] and all x1, . . . , xn, x′i ∈ X ,\n|f (x1, . . . , xi, . . . , xn)− f (x1, . . . , x′i, . . . , xn)| ≤ ci then for all > 0, we have\nP [f − E JfK > ] ≤ exp ( −2 2∑n i=1 c 2 i ) 1We were not able to find any written manuscript detailing the argument of [11]. However the argument itself is fairly generic in allowing one to prove generalization bounds for sample dependent hypothesis spaces.\nWe shall invoke the McDiarmid’s inequality on the function g(S) := sup f∈FS\n{ Lf − L̂Sf } with S =\n(x1, . . . ,xn) being the random variables in question. To do so we first prove the stability of the function g(S) with respect to its variables and then bound the value of E\nS Jg(S)K. Theorem 19. For any S, Si, we have ∣∣g(S)− g(Si)∣∣ ≤ 6BCLn .\nProof. We have\ng(S) = sup f∈FS\n{ Lf − L̂Sf } = sup\nf∈FS\n{ Lf − L̂Sf − L̂S i f + L̂S i f } ≤ sup\nf∈FS\n{ Lf − L̂S i\nf } + sup f∈FS { L̂Sf − L̂S i f } ≤ sup\nf∈FS\n{ Lf − L̂S i\nf\n} +\n2BCL n\nwhere in the fourth step we have used the fact that the loss function is Lipschitz and the embedding function ΨS is bounded. We also have\nsup f∈FS\n{ Lf − L̂S i\nf\n} = sup\nw\n{ Lf(S,w) − L̂ Si\nf(S,w) } = sup\nw\n{ Lf(S,w) − Lf(Si,w) + Lf(Si,w) − L̂ Si f(Si,w) + L̂S i f(Si,w) − L̂S i f(S,w) } ≤ sup\nw\n{ Lf(Si,w) − L̂ Si f(Si,w) } + sup\nw\n{ Lf(S,w) − Lf(Si,w) } +sup\nw\n{ L̂S i\nf(Si,w) − L̂S\ni\nf(S,w) } ≤ sup\nw\n{ Lf(Si,w) − L̂ Si f(Si,w) } + 2BCL n + 2BCL n\n= sup f∈FSi\n{ Lf − L̂S i\nf\n} +\n4BCL n\n= g(Si) + 4BCL n\nwhere in the fourth step we have used the stability of the embedding function and that the loss function is CL-Lipschitz in its first argument so that for all x we have∣∣` (f(S,w)(x), y(x))− ` (f(Si,w)(x), y(x))∣∣ ≤ 2BCLn which holds in expectation over any (empirical) distribution as well. Putting the two inequalities together gives us g(S) ≤ g(Si) + 6BCL\nn .\nSimilarly we also have g(Si) ≤ g(S) + 6BCL n which gives us the result. We now have that the function g(S) is O ( 1\nn\n) -stable with respect to each of its inputs. We now\nmove on to bound its expectation. For any function class F we define its empirical Rademacher average as follows\nR̂n(F) := E σ t sup f∈F\n{ 1\nn ∑ xi∈S σif(xi) }∣∣∣∣∣S |\nAlso let F := {x 7→ 〈w,x〉 : ‖w‖2 ≤ B} and X := {x : ‖x‖2 ≤ 1}.\nTheorem 20. E S t sup f∈FS { Lf − L̂Sf }| ≤ 2BCL √ 1 n\nProof. We have\nE S t sup f∈FS { Lf − L̂Sf }| = E S t sup f∈FS { E S′ r L̂S ′ f z − L̂Sf }|\n≤ E S,S′ t sup f∈FS { L̂S ′ f − L̂Sf }|\n≤ E S,S′\nt sup\nf∈FS∪S′\n{ L̂S ′ f − L̂Sf }|\n= E S,S′,σ u v sup f∈FS∪S′  1n ∑ xi∈S,x′i∈S′ σi (`(f(x ′ i), y(x ′ i))− `(f(xi), y(xi)))  } ~\n≤ 2 E S,S′,σ\nt sup\nf∈FS∪S′\n{ 1\nn ∑ xi∈S σi`(f(xi), y(xi))\n}|\n= 2 E S,S′,σ t sup w\n{ 1\nn ∑ xi∈S σi`(f(S∪S′,w)(xi), y(xi))\n}|\n≤ 2 E S,S′,σ t sup f∈F\n{ 1\nn ∑ xi∈S σi`(f(xi), y(xi))\n}|\n= 2 E S\nr R̂n(` ◦ F) z ≤ 2CL E\nS\nr R̂n(F) z ≤ 2BCL\n√ 1\nn\nwhere in the third step we have used the fact that FS ⊇ FS′ if S ⊇ S′ (this is the monotonicity requirement in [11]). Note that this is essential to introduce symmetry so that Rademacher variables can be introduced in the next (symmetrization) step. In the seventh step, we have used the fact that for every S such that |S| = n and w ∈ Rn such that ‖w‖∞ ≤ B, there exists a function f ∈ F such that for all x, there exists a x′ ∈ X such that f(S,w)(x) = f(x′). In the last step we have used a result from [26] which allows calculation of Rademacher averages for composition classes and an intermediate result from the proof of Lemma 16 which gives us Rademacher averages for the function class F .\nThus, by an application of McDiarmid’s inequality we have, with probability (1− δ) over choice of the landmark (training) set,\nsup f∈FS\n{ Lf − L̂Sf } ≤ E t sup f∈FS { Lf − L̂Sf }| + 6BCL √ log 1/δ 2n ≤ 4BCL √ log 1/δ n\nwhich concludes our argument justifying double dipping."
    }, {
      "heading" : "Appendix C Regression with Similarity Functions",
      "text" : "In this section we give proofs of utility and admissibility results for our similarity based learning model for real-valued regression tasks.\nC.1 Proof of Theorem 5\nFirst of all, we use Lemma 15 to project onto a d dimensional space where there exists a linear predictor f̃ : x 7→ 〈w,x〉 such that E\nx∼D r∣∣∣f̃ (Ψ (x))− f(x)∣∣∣z ≤ 2 1. Note that ‖w‖2 ≤ B and sup x∈X {‖Ψ(x)‖} ≤ 1 by construction. We will now show that f̃ has bounded -insensitive loss.\nE x∼D\nr ` ( f̃ (Ψ (x)) , y(x) )z = E\nx∼D J` (f(x), y(x))K + E x∼D\nr ` ( f̃ (Ψ (x)) , y(x) ) − ` (f(x), y(x)) z\n≤ 0 + E x∼D\nr ` ( f̃ (Ψ (x)) , y(x) ) − ` (f(x), y(x)) z\n≤ 0 + E x∼D r∣∣∣f̃ (Ψ(x))− f(x)∣∣∣z ≤ 0 + 2 1\nwhere in the second step we have used the goodness properties of K, in the third step we used the fact that the -insensitive loss function is 1-Lipschitz in its first argument. Note that ‖w‖ ≈ E x∼D q w2(x) y with high probability and if E x∼D q w2(x) y B then we get a much better bound on the norm of w. The excess loss incurred due to this landmarking step is, with probability 1 − δ, at most 32B √ log(1/δ)\nd .\nNow consider the following regularized ERM problem on n i.i.d. sample points:\nŵ = arg min w:‖w‖2≤B\n1\nn n∑ i=1 ` (〈w,Ψ(xi)〉 , y(xi))\nThe final output of our learning algorithm shall be x 7→ 〈ŵ,Ψ(x)〉. Here we have CX = 1, CL = 1 since ` (·) is 1-Lipschitz and CW = B. Thus by Lemma 16, we get that the excess loss incurred due to this regularized ERM step is at most 3B √ log 1/δ n .\nSince the -insensitive loss is related to the absolute error by |x| ≤ ` (x) + we have the total error (with respect to absolute loss) being incurred by our predictor to be, with probability at least 1− 2δ, at most\n0 + 32B\n√ log(1/δ)\nd + 3B\n√ log 1/δ\nn +\nTaking d = O ( B2\n21 log 1δ\n) unlabeled landmarks and n = O ( B2\n21 log 1δ\n) labeled training points\ngives us our desired result.\nC.2 Proof of Theorem 6\nWe prove the two parts of the result separately. Part 1: Admissibility: Using Lemma 17 it is possible to obtain a vector W′ = n∑ i=1 (αi − α∗i )ΦK(xi) ∈ HK with small loss such that 0 ≤ αi, α∗i ≤ piC and αiα∗i = 0 (these inequalities are a consequence of applying the KKT conditions). This allows us to construct as weight function wi = αi−α∗i pi such that |wi| ≤ C and E x′∼D Jw(x′)K(x,x′)K = 〈W′,ΦK(x)〉 for all x ∈ X .\nThus we have E x∼D\nr ` ( E\nx′∼D Jw(x′)K(x,x′)K , y(x)\n)z = E\nx∼D J` (〈W′,ΦK(x)〉 , y(x))K ≤\n1 2Cγ2 + 0. Setting C = 1 2 1γ2 gives us our result.\nWe can use variational techniques to extend this to non-discrete distributions as well.\nPart 2: Tightness: The tight example that we provide is an adaptation of the example given for large margin classification in [9]. However, our analysis differs from that of [9], partly necessitated by our choice of loss function.\nConsider the following regression problem: X = {x1,x2,x3,x4} ⊂ R3, D = { 1 2 − , , , 1 2 − } , y = {+1,+1,−1,−1}\nx1 = ( γ, γ, √ 1− 2γ2 ) x2 = ( γ,−γ, √ 1− 2γ2\n) x3 = ( −γ, γ, √ 1− 2γ2\n) x4 = ( −γ,−γ, √ 1− 2γ2\n) Clearly the vector w = (1, 0, 0) yields a predictor y′ with no -insensitive loss for = 0 (i.e. E\nx∼D J`0 (y(x)− y′(x))K = 0) at margin γ. Thus the native inner product 〈·, ·〉 on R3 is a (0, γ)good kernel for this particular regression problem.\nNow consider any bounded weighing function on X , w = {w1, w2, w3, w4} and analyze the effectiveness of 〈·, ·〉 as a similarity function. The output ỹ of the resulting predictor on the different\npoints is given by ỹi = 4∑ j=1 pjwj 〈xi,xj〉.\nIn particular, consider the output on the heavy points x1 and x4 (note that the analysis in [9] considers the light points x2 and x3 instead). We have\nỹ1 =\n( 1 2 − ) w1 + ( 1− 2γ2 ) (w2 + w3) + ( 1 2 − ) w4 ( 1− 4γ2 ) = a+ ( 1 2 − ) (w1 + bw4)\nỹ4 =\n( 1 2 − ) w1 ( 1− 4γ2 ) + ( 1− 2γ2 ) (w2 + w3) + ( 1 2 − ) w4 = a+ ( 1 2 − ) (bw1 + w4)\nfor a = ( 1− 2γ2 ) (w2 + w3) , b = ( 1− 4γ2 ) . The main idea behind this choice is that the difference in the value of the predictor on these points is only due to the values of w1 and w4. Since the true values at these points are very different, this should force w1 and w4 to take large values unless a large error is incurred. To formalize this argument we lower bound the expected `0 (·) loss of this predictor by the loss incurred on these heavy points.\nE x∼D\nJ`0 (y(x)− ỹ(x))K ≥ ( 1 2 − ) (`0 (y(x1)− ỹ(x1)) + `0 (y(x4)− ỹ(x4)))\n=\n( 1 2 − ) (|1− ỹ(x1)|+ |−1− ỹ(x4)|)\n≥ ( 1 2 − ) (2− ỹ(x1) + ỹ(x4))\n=\n( 1 2 − )( 2− ( 1 2 − ) (1− b) (w4 − w1) )\n=\n( 1 2 − )( 2− ( 1 2 − )( 4γ2 ) (w4 − w1) )\nwhere in the second step we use the fact that `0 (x) = |x| and in the third step we used the fact that |a|+ |b| ≥ a− b. Thus, in order to have expected error at most 1, we require\nw4 − w1 ≥ 1\n4γ2\n( 2− 11\n2 −\n) 1\n1 2 −\n= 1\n4 1γ2\nfor the setting = 12 − 1. Thus we have |w1| + |w4| ≥ w4 − w1 ≥ 1 4 1γ2 which implies max (|w1| , |w4|) ≥ 18 1γ2 which proves the result."
    }, {
      "heading" : "Appendix D Sparse Regression with Similarity functions",
      "text" : "Our utility proof proceeds in three steps. In the first step we project our learning problem, via the landmarking step given in Step 1 of Algorithm 1, to a linear landmarked space and show that the\nAlgorithm 2 Sparse regression [10] Input: A β-smooth loss function `(·, ·), regularization parameter CW used in Equation 2, error tolerance Output: A sparse predictor ŵ with bounded loss\n1: k ← ⌈ 8C2W 2 ⌉ , w(0) = 0\n2: for t = 1 to k do 3: θ(t) ← ∇wR(w(t)) = E\nx∼D\nr ∂ ∂w ` (〈 w(t),x 〉 , y(x) )z 4: rt = arg max\nj∈d ∣∣∣θ(t)j ∣∣∣ 5: δt = 〈 θ(t),w(t) 〉 + CW ∥∥∥θ(t)∥∥∥ ∞\n6: ηt = min {\n1, δt 4C2 W β } 7: w(t+1) ← (1− ηt)w(t) + ηtsign ( −θ(t)rt ) CW e rt 8: if δt ≤ then 9: return w(t)\n10: end if 11: end for 12: return w(k)\nlandmarked space admits a sparse linear predictor with bounded -insensitive loss. This is formalized in Theorem 8 which we restate for convenience.\nTheorem 21 (Theorem 8 restated). Given a similarity function that is ( 0, B, τ)-good for a regression problem, there exists a randomized map Ψ : X → Rd for d = O ( B2\nτ 21 log 1δ\n) such that\nwith probability at least 1 − δ, there exists a linear operator f̃ : x 7→ 〈w,x〉 over Rd such that ‖w‖1 ≤ B with -insensitive loss bounded by 0 + 1. Moreover, with the same confidence we have ‖w‖0 ≤ 3dτ 2 .\nProof. The proof of this theorem essentially parallels that of [15, Theorem 8] but diverges later since the aim there is to preserve margin violations whereas we wish to preserve loss under the absolute loss function. Sample d landmark points L = {x1, . . . ,xd} from the distribution D and construct the map ΨL : x 7→ (K(x,x1), . . . ,K(x,xd)) and consider the linear operator f̃ : x 7→ 〈w,x〉\nwith wi = w(xi)R(xi)\ndinfo where dinfo = d∑ i=1 R(xi) is the number of informative landmarks. In the\nfollowing we will refer to f̃ and w interchangeably. This ensures that ∥∥∥f̃∥∥∥\n1 := ‖w‖1 ≤ B. Note\nthat we have chosen an L1 normalized weight vector instead of an L2 normalized one like we had in Lemma 15. This is due to a subsequent use of sparsity promoting regularizers whose analysis requires the existence of bounded L1 norm predictors.\nUsing the arguments given for Lemma 15 and Theorem 5, we can show that if dinfo = Ω ( B2\n21 log 1δ ) (i.e. if we have collected enough informative landmarks), then we are done. However, the Chernoff bound (lower tail) tells us that for d = Ω ( B2\nτ 21 log 1δ\n) , this will happen with probability 1 − δ.\nMoreover, the Chernoff bound (upper tail) tells us that, simultaneously we will also have dinf ≤ 3dτ2 . Together these prove the claim.\nNote that the number of informative landmarks required is, up to constant factors, the same as the number required in Theorem 5. However, we see that in order to get these many informative landmarks, we have to sample a much larger number number of landmarks. In the following, we shall see how to extract a sparse predictor in the landmarked space with good generalization properties. The following analysis shall assume the the existence of a good predictor on the landmarked space and hence all subsequent results shall be conditioned on the guarantees given by Theorem 8.\nD.1 Learning sparse predictors in the landmarked space\nWe use the Forward Greedy Selection algorithm presented in [10] to extract a sparse predictor in the landmarked space. The algorithm is presented in pseudo code form in Algorithm 2. The algorithm can be seen as a (modified) form of orthogonal matching pursuit wherein at each step we add a coordinate to the support of the weight vector. The coordinate is added in a greedy manner so as to provide maximum incremental benefit in terms of lowering the loss. Thus the sparsity of the resulting predictor is bounded by the number of steps for which this algorithm is allowed to run. The algorithm requires that it be used with a smooth loss function. A loss function ` : R×R→ R+ is said to be β-smooth if, for all y, a, b ∈ R, we have\n`(a, y)− `(b, y) ≤ ∂ ∂x `(x, y) ∣∣∣∣ x=b (a− b) + β(a− b) 2 2\nUnfortunately, this excludes the -insensitive loss. However it is possible to run the algorithm with a smooth surrogate whose loss can be transferred to -insensitive loss. Following [10], we choose the following loss function:\n˜̀ β(a, b) = inf\nv∈R\n[ β\n2 v2 + ` (a− v, b) ] One can, by a mildly tedious case-by-case analysis, arrive at an explicit form for this loss function\n˜̀ β(a, b) =  0 |a− b| ≤ β 2 (|a− b| − ) 2 < |a− b| < + 1β\n|a− b| − − 12β |a− b| ≥ + 1 β\nNote that this loss function is convex as well as differentiable (actually β-smooth) which will be crucial in the following analysis. Moreover, for any a, b we have\n0 ≤ ` (a, b)− ˜̀β(a, b) ≤ 1\n2β (1)\nAnalysis of Forward Greedy Selection: We need to setup some notation before we can describe the guarantees given for the predictor learned using the Forward Greedy Selection algorithm. Consider a domain X ⊂ Rd for some d > 0 and the class of functions F = {x 7→ 〈w,x〉 : ‖w‖1 ≤ CW }. For any distribution D on X and any predictor from F , define RD(w) := E\nx∼D J` (〈w,x〉 , y(x))K\nand R̃D(w) := E x∼D\nr ˜̀ β(〈w,x〉 , y(x)) z . Also let w̄ be the minimizer of the following program\nw̄ = arg min w:‖w‖1≤CW\nR̃D(w) (2)\nThen [10, Theorem 2.4], when specialized to our case, guarantees that Algorithm 2, when executed with ˜̀β(·, ·) as the loss function for β = 1 2 , produces a k-sparse predictor x̂, for k = ⌈ 8C2W 22 ⌉ , with ‖ŵ‖1 ≤ CW such that R̃D(ŵ)− R̃D(w̄) ≤ 2\nThus, if we can show the existence of a good predictor in our space with bounded L1 norm then this would upper bound the loss incurred by the minimizer of Equation 2 and using [10, Theorem 2.4] we would be done. Note that Theorem 8 does indeed give us such a guarantee which allows us to make the following argument: we are guaranteed of the existence of a predictor f̃ with L1 norm bounded by B that has -insensitive loss bounded by ( 0 + 1). Thus if we take CW = B in Equation 2 and use the left inequality of Equation 1, we get R̃D(w̄) ≤ 0+ 1. Thus we have R̃D(ŵ) ≤ 0+ 1+ 2. Using Equation 1 (right inequality) with β = 1 2 , we getRD(ŵ) ≤ 0 + 1 + 3 2/2.\nHowever it is not possible to give utility guarantees with bounded sample complexities using the above analysis, the reason being that Algorithm 2 requires us to calculate, for any given vector w, the vector ∇wR̃(w) = E\nx∼D\nr ∂ ∂w ˜̀ β(〈w,x〉 , y(x)) z which is infeasible to calculate for a distribution\nwith infinite support since it requires unbounded sample complexities. To remedy we shall, as\nsuggested by [10], take D not to be the true distribution over the entire domain X , but rather the empirical distributionDemp = 1n n∑ i=1 1{x=xi} for a given sample of training points x1, . . . ,xn. Note that the result in [10] holds for any distribution which allows us to proceed as before.\nNotice however, that we are yet again faced with the challenge of proving an upper bound on the loss incurred by the minimizer of Equation 2. This we do as follows: the predictor f̃ defined in Theorem 8 has expected -insensitive loss over the entire domain bounded by 0 + 1. Hence it will, with probability greater than (1− δ), have at most 0 + 1 +O ( B√ n ) loss on a random sample of n\npoints by an application of Hoeffding’s inequality. Thus we have R̃Demp(w̄) ≤ 0 + 1 + O ( B√ n ) with high probability.\nThe main difference in this analysis shall be that the guarantee on ŵ we get will be on its training loss rather than its true loss, i.e. we will have RDemp(ŵ) ≤ 0 + 1 + O ( B√ n ) + 2. However since Algorithm 2 guarantees ‖ŵ‖1 ≤ CW = B, we can still hope to bound its generalization error. More specifically, Lemma 22, given below, shows that with probability greater than (1− δ) over the choice of training points we will have, for all w ∈ Rd, RD(w)−RDemp(w) ≤ Õ ( B√ n ) where the\nÕ (·) notation hides certain log factors. Lemma 22 (Risk bounds for sparse linear predictors [13]). Consider a real-valued prediction problem y over a domain X = {x : ‖x‖∞ ≤ CX} ⊂ Rd and a linear learning model F : {x 7→ 〈w,x〉 : ‖w‖0 ≤ k, ‖w‖1 ≤ CW } under some fixed loss function ` (·, ·) that is CL-Lipschitz in its second argument. For any f ∈ F , let Lf = E\nx∼D J`(f(x), y(x))K and L̂nf be the empirical loss\non a set of n i.i.d. chosen points, then we have, with probability greater than (1− δ),\nsup f∈F\n( Lf − L̂nf ) ≤ 2CLCXCW √ 2 log(2d)\nn + CLCXCW\n√ log(1/δ)\n2n\nProof. The result for non-sparse vectors, that applies here as well, follows in a straightforward manner from [13, Theorem 1, Example 3.1(2)] and [23] which we reproduce for completeness. Since the L1 and L∞ norms are dual to each other, for any w ∈ (R+)\nd such that ‖w‖1 = B and any µ ∈ ∆d, where ∆d is the probability simplex in d dimensions, the Kullback-divergence function KL ( w B ∥∥µ) is 1B2 -strongly convex with respect to the L1 norm. We can remove the positivity constraints on the coordinates of w by using the standard method of introducing additional dimensions that encode negative components of the (signed) weight vector.\nUsing [13, Theorem 1], thus, we can bound the Rademacher complexity of the function class F as Rn (F) ≤ CXCW √ 2 log 2d n . Next, using the Lipschitz properties of the loss function, a result from\n[23] allows us to bound the excess error by 2CLRn(F) + CLCXCW √ log(1/δ) 2n . The result then follows.\nThus, by applying a union bound, with probability at least (1− 2δ), we will choose a training set such that f̃ , and consequently w̄, has bounded loss on that set as well as the uniform convergence guarantee of Lemma 22 will hold. Then we can bound the true loss of the predictor returned by Algorithm 2 as\nRD(ŵ) ≤ RDemp(ŵ) + Õ ( B√ n ) ≤ 0 + 1 + 2 + Õ ( B√ n ) where the first inequality uses the uniform convergence guarantee and the second inequality holds conditional on f̃ having bounded loss on a given training set. The final guarantee is formally given in Theorem 9.\nNote that using Lemma 16 here would at best guarantee a decay of O (√\nd n\n) . Transferring -\ninsensitive loss to absolute loss requires an addition of . Using all the results given above, we can now give a proof for Theorem 9 which we restate for convenience.\nTheorem 23 (Theorem 9 restated). Every similarity function that is ( 0, B, τ)-good for a regression problem with respect to the insensitive loss function ` (·, ·) is ( 0 + )-useful with respect to absolute loss as well; with the dimensionality of the landmarked space being bounded by O ( B2\nτ 21 log 1δ\n) and\nthe labeled sampled complexity being bounded by O ( B2\n21 log B 1δ\n) . Moreover, this utility can be\nachieved by an O (τ)-sparse predictor on the landmarked space.\nProof. Using Theorem 8, we first bound the excess loss due to landmarking by 32B √\nlog(1/τδ) d .\nNext we set up the (dummy) Ivanov regularized regression problem (given in Equation 2) with the training loss being the objective and regularization parameter CW = B. The training loss incurred by the minimizer of that problem winter is, with probability at least (1− δ), bounded by L̂ (winter) ≤ 0 + 32B √ log(1/δ) τd + B √ log(1/δ)\nn due to the guarantees of Theorem 8. Next, we run the Forward Greedy Selection algorithm of [10] (specialized to our case in Algorithm 2) and obtain another predictor ŵ with L1 norm bounded by B that has empirical error at most L̂ (ŵ) ≤ L̂ (winter) + √ 18B2\nk . Finally, using Lemma 22, we bound the true -insensitive loss incurred by ŵ by L̂ (ŵ) + 2B √\n2 log(2d) n +B\n√ log(1/δ)\n2n . Adding to convert this loss to absolute loss we get that with probability at most (1− 3δ), we will output a k-sparse predictor in a d-dimensional space with absolute regression loss at most\n0 + 32B\n√ log(1/δ)\nτd +\n√ 18B2\nk + 2B\n√ 2 log(2d)\nn + 2B\n√ log(1/δ)\n2n +\nWe note that Forward Greedy Selection gives O (\n1 k\n) error rates, which are much better, if the loss\nfunction being used is smooth. This can be achieved by using squared loss `sq (a, b) = (a− b)2 as the surrogate. However we note that assuming goodness of the similarity function in terms of squared loss would impose strictly stronger conditions on the learning problem. This is because E J`sq (a, b)K = sup (a− b) · E J|a− b|K and thus, under boundedness conditions, squared loss is bounded by a constant times the absolute loss but it is not possible to bound absolute loss (or - insensitive loss) as a constant multiple of the squared loss since there exist distributions such that E J|a− b|K = Ω ( 1 inf(|a−b|) · E J`sq (a, b)K ) and 1inf(|a−b|) can diverge.\nBelow we prove admissibility results for the sparse learning model.\nD.2 Proof of Theorem 10\nTo prove the first part, construct a new weight function w̃(x) = sign (w(x)) · w̄. Note that we have |w̃(x)| ≤ w̄ ≤ B. Also construct the choice function as follows: for any x, let P [R(x) = 1|x] = |w(x)| B . This gives us Ex∼D JR(x)K = w̄ B . Then for any x, we have\nE x′∼D Jw̃(x′)K(x,x′)|R(x′)K = E x′∼D\ns sign (w(x)) w̄K(x,x′)\n|w(x)| B { / P x∼D [R(x) = 1]\n= E x′∼D\nr w(x)K(x,x′) w̄\nB z / ( w̄ B ) = E\nx′∼D Jw(x′)K(x,x′)K\nSince f(x) = E x′∼D Jw(x′)K(x,x′)K has small -insensitive loss by ( 0, B)-goodness ofK, we have our result. To prove the second part, construct a new weight function w̃(x) = w(x)τ P [R(x) = 1|x].\nNote that we have |w̃(x)| ≤ Bτ . Then for any x, we have\nE x′∼D Jw̃(x′)K(x,x′)K = E x′∼D\ns w(x′)\nτ R(x′)K(x,x′)\n{\n= E x′∼D\ns w(x′)\nτ K(x,x′)|R(x′)\n{ P\nx′∼D [R(x′) = 1]\n= E x′∼D\nJw(x′)K(x,x′)|R(x′)K\nSince f(x) = E x′∼D Jw(x′)K(x,x′)|R(x′)K has small -insensitive loss by ( 0, B, τ)-goodness of K, we have our result.\nUsing the above result we get out admissibility guarantee. Corollary 24. Every PSD kernel that is ( 0, γ)-good for a regression problem is, for any 1 > 0,( 0 + 1,O ( 1\n1γ2\n) , 1 ) -good as a similarity function as well.\nThe above result is rather weak with respect to the sparsity parameter τ since we have made no assumptions on the distribution of the dual variables αi, α∗i in the proof of Theorem 6 which is why we are forced to use the (weak) inequality w̄B ≤ 1. Any stronger assumptions on the kernel goodness shall also strengthen this admissibility result."
    }, {
      "heading" : "Appendix E Ordinal Regression",
      "text" : "In this section we give missing utility and admissibility proofs for the similarity-based learning model for ordinal regression. But before we present the analysis of our model, we give below, an analysis of algorithms that choose to directly reduce the ordinal regression problem to real-valued regression. The analysis will serve as motivation that will help us define our goodness criteria.\nE.1 Reductions to real valued regression\nOne of the simplest learning algorithms for the problem of ordinal regression involves a reduction to real-valued regression [17, 16] where we modify our goal to that of learning a real valued function f which we then threshold using a set of thresholds {bi}ri=1 with b1 = −∞ to get discrete labels as shown below\nyf (x) = arg max i∈[r]\n{bi : f(x) ≥ bi}\nThese thresholds may themselves be learned or fixed apriori. A simple choice for these thresholds is bi = i− 1 for i > 1. It is easy to show (using a result in [17]) that for the fixed thresholds specified above, we have for all f : X → R,\n`ord (yf (x), y(x)) ≤ min {\n2 |f(x)− y(x)| , |f(x)− y(x)|+ 1 2 } ≤ min { 2` (f(x)− y(x)) + 2 , ` (f(x)− y(x)) + + 1\n2 } where in the last step we use the fact that |x| − ≤ ` (x) ≤ |x|. It is tempting to use this reduction along with guarantees given for real-valued regression to directly give generalization bounds for ordinal regression. To pursue this further, we need a notion of a good similarity function which we give below: Definition 25. A similarity function K is said to be ( 0, B)-good for an ordinal regression problem y : X → [r] if for some bounded weight function w : X → [−B,B], the following predictor, when subjected to fixed thresholds, has expected ordinal regression error at most 0\nf : x 7→ E x′∼D\nJw(x′)K(x,x′)K\ni.e. E x∼D J|yf (x)− y(x)|K < 0.\nFrom the definition of the thresholding scheme used to define yf from f , it is clear that |f(x)− y(x)| ≤ |yf (x)− y(x)| + 12 . Since we have ` (x) ≤ |x| for any ≥ 0, we have ` (f(x)− y(x)) ≤ |y(x)− yf (x)|+ 12 and thus we have Ex∼D J` (f(x), y(x))K < 0 + 1 2 .\nThus, starting with goodness guarantee of the similarity function with respect to ordinal regression, we obtain a guarantee of the goodness of the similarity function K with respect to real-valued regression that satisfies the requirements of Theorem 5. Thus we have the existence of a linear predictor over a low dimensional space with -insensitive error at most 0 + 12 + 1. We can now argue (using results from [17]) that this real-valued predictor, when subjected to the fixed thresholds, would yield a predictor with ordinal regression error at most\nmin { 2 ( 0 + 1\n2 + 1\n) + 2 , ( 0 + 1\n2 + 1\n) + + 1\n2\n} = 1 + 0 + 1 + .\nHowever, this is rather disappointing since this implies that the resulting predictor would, on an average, give out labels that are at least one step away from the true label. This forms the intuition behind introducing (soft) margins in the goodness formulation that gives us Definition 12. Below we give proofs for utility and admissibility guarantees for our model for similarity-based ordinal regression.\nE.2 Proof of Theorem 13\nWe use Lemma 15 to construct a landmarked space with a linear predictor f̃ : x 7→ 〈w,x〉 such that E\nx∼D r∣∣∣f̃ (Ψ (x))− f(x)∣∣∣z ≤ 2 1. As before, we have ‖w‖2 ≤ B and sup x∈X {‖Ψ(x)‖} ≤ 1. In\nthe following, we shall first show bounds on the mislabeling error i.e P x∼D [ŷ(x) 6= y(x)]. Next, we shall convert these bounds into ordinal regression loss by introducing a spacing parameter into the model.\nSince the γ-margin loss function is 1-Lipschitz, we get[ f̃(Ψ(x))− by(x) ] γ ≤ [ f(x)− by(x) ] γ\n+ 2 1[ by(x)+1 − f̃(Ψ(x)) ] γ ≤ [ by(x)+1 − f(x) ] γ + 2 1\nWhich gives us, upon taking expectations on both sides,\nE x∼D\ns[ f̃(Ψ(x))− by(x) ] γ + [ by(x)+1 − f̃(Ψ(x)) ] γ { ≤ 0 + 4 1\nLemma 15 guarantees the excess loss due to landmarking to be at most 64B √\nlog(1/δ) d . Moreover,\nsince the γ-margin loss is 1-Lipschitz, Lemma 16 allows us to bound excess loss due to training by 3B √\nlog(1/δ) n so that the learned predictor has γ-margin loss at most 0 + 1 for any 1 given large\nenough d and n. Now, from the definition of the γ-margin loss it is clear that if the loss is greater than γ then it indicates a mislabeling. Hence, the mislabeling error is bounded by 0+ 1γ .\nThis may be unsatisfactory if γ 1 - to remedy such situations we show that we can bound the 1-margin loss directly. Starting from E\nx∼D r∣∣∣f̃(Ψ(x))− f(x)∣∣∣z < 2 1, we can also deduce E\nx∼D\ns[ 1− f̃(Ψ(x)) + by(x) ] + + [ 1− by(x)+1 + f̃(Ψ(x)) ] + { ≤ 0 + 4 1\nWe can bound the excess training error for this loss function as well. Since the 1-margin loss directly bounds the mislabeling error, combining the two arguments we get the second part of the claim.\nHowever, the margin losses themselves do not present any bound on the ordinal regression error. This is because, if the thresholds are closely spaced together, then even an instance of gross ordinal regression loss could correspond to very small margin loss. To remedy this, we introduce a spacing parameter into the model. We say that a set of thresholds is ∆-spaced if min\ni∈[r] {|bi − bi+1|} ≥ ∆.\nSuch a condition can easily be incorporated into the model of [17] as a constraint in the optimization formulation.\nSuppose that a given instance has ordinal regression error `ord (ŷ(x), y(x)) = k. This can happen if the point was given a label k labels below (or above) its correct label. Also suppose that the γ-margin error in this case is [ŷ(x)− y(x)]γ = h. Without loss of generality, assume that the point x of label k + 1 was given the label 1 giving an ordinal regression loss of lord = k (a similar analysis would hold if the point of label 1 were to be given a label k + 1 by symmetry of the margin loss formulation with respect to left and right thresholds). In this case the value of the underlying regression function must lie between b1 and b2 and thus, the margin loss h satisfies\nh ≥ bk+1 + γ − b2 = γ + k∑ i=2 (bi+1 − bi) ≥ γ + (k − 1) ∆. Thus, if the margin loss is at most h, the ordinal regression error must satisfy `ord (ŷ(x), y(x)) ≤ [ŷ(x)−by(x)]γ+[by(x)+1−ŷ(x)]γ−γ\n∆ + 1. Let ψ∆(x) = x+∆−1∆ . Using the bounds on the γ-margin and 1-margin losses given above, we get the first part of the claim.\nIn particular, a constraint of ∆ = 1 put into an optimization framework ensures that the bounds on mislabeling loss and ordinal regression loss match since ψ1(x) = x for all x. In general, the cases where the above framework yields a non-trivial bound for the mislabeling error rate, i.e. `01 < 1 (which can always be ensured if 0 < 1 by taking large enough d and n), also correspond to those where the ordinal regression error rate is also bounded above by 1 since sup\nx∈[0,1],∆>0 (ψ∆ (x)) = 1.\nE.3 Admissibility Guarantees\nWe begin by giving the kernel goodness criterion which we adapt from existing literature on large margin approaches to ordinal regression. More specifically we use the framework described in [16] for which generalization guarantees are given in [17]. Definition 26. Call a PSD kernel K ( 0, γ)-good for an ordinal regression problem y : X → [r] if there exists W∗ ∈ HK , ‖W∗‖ = 1 and a fixed set of thresholds {bi}ri=1 such that\nE x∼D\nt[ by(x) + 1−\n〈W∗,ΦK(x)〉 γ ] + + [ 〈W∗,ΦK(x)〉 γ − by(x)+1 + 1 ] + | < 0\nThe above definition exactly corresponds to the EXC formulation put forward by [17] except for the fact that during actual optimization, a strict ordering on the thresholds is imposed explicitly. [17] present yet another model called IMC which does not impose any explicit orderings, rather the ordering emerges out of the minimization process itself. Our model can be easily extended to the IMC formulation as well. Theorem 27 (Theorem 14 restated). Every PSD kernel that is ( 0, γ)-good for an ordinal regression problem is also ( γ1 0 + 1,O ( γ21 1γ2 )) -good as a similarity function with respect to the γ1-margin loss for any γ1, 1 > 0. Moreover, for any 1 < γ1/2, there exists an ordinal regression instance and a corresponding kernel that is (0, γ)-good for the ordinal regression problem but only ( 1, B)-good\nas a similarity function with respect to the γ1-margin loss function for B = Ω ( γ21 1γ2 ) .\nProof. We prove the two parts of the result separately. Part 1: Admissibility: As before, using Lemma 17 it is possible to obtain a vector W′ = n∑ i=1 (αi − α∗i )ΦK(xi) ∈ HK such that 0 ≤ αi, α∗i ≤ piC (by applying the KKT conditions) and the following holds:\nE x∼D\nr[ by(x) + 1− 〈W′,ΦK(x)〉 ] + + [ 〈W′,ΦK(x)〉 − by(x)+1 + 1 ] + z < 1\n2Cγ2 + 0 (3)\nThis allows us to construct a weight function wi = αi−α∗i pi\nsuch that |wi| ≤ 2C (since we do not have any guarantee that αiα∗i = 0) and E\nx′∼D Jw(x′)K(x,x′)K = 〈W′,ΦK(x)〉 for all x ∈ X .\nDenoting f(x) := E x′∼D\nJw(x′)K(x,x′)K for convenience gives us\nE x∼D\nq[ f(x)− by(x) ] 1 + [ by(x)+1 − f(x) ] 1 y = E\nx∼D\nr[ 1− f(x) + by(x) ] + + [ 1− by(x)+1 + f(x) ] + z\n≤ 1 2Cγ2 + 0\nwhere in the first step we used [x]1 = [1− x]+. Now use the fact [x]1 = 1 γ [γx]γ to get the following:\nE x∼D\nr[ γ1f(x)− γ1by(x) ] γ1 + [ γ1by(x)+1 − γ1f(x) ] γ1 z ≤ γ1 2Cγ2 + γ1 0\nNote that it is not possible to perform the analysis on the loss function [·]γ directly since using it requires us to scale the threshold values by a factor of γ1 that makes the result in Equation 3 unusable. Hence we first perform the analysis for [·]1, utilize Equation 3 and then interpret the resulting inequality in terms of [·]γ1 .\nSetting 2C = γ1 1γ2 , using w ′(x) = γ1w(x) as weights, using b′j = γ1bj as the thresholds and noting that the new bound on the weights is |w′i| ≤ 2Cγ1 gives us the result. As before, using variational optimization techniques, this result can be extended to non-discrete distributions as well.\nIn particular, setting γ1 = γ gives us that any PSD kernel that is ( 0, γ)-good for an ordinal regression problem is also ( γ 0 + 1,\n1 1\n) -good as a similarity function with respect to the γ-margin\nloss.\nPart 2: Tightness: We adapt our running example (used for proving the lower bound for real regression) for the case of ordinal regression as well. Consider the points with value −1 as having label 1 and those having value +1 as having label 2. Clearly, w = (1, 0, 0) along with the thresholds b1 = −∞ and b2 = 0 establishes the native inner product as a (0, γ)-good PSD kernel. Now consider the heavy points yet again and some weight function and threshold b2 (b1 is always fixed at−∞) that is supposed to demonstrate the goodness of the inner product kernel as a similarity function. Clearly we have\nE x∼D\nr[ f(x)− by(x) ] γ1 + [ by(x)+1 − f(x) ] γ1 z ≥\n( 1 2 − )( [f(x1)− b2]γ1 + [b2 − f(x4)]γ1 )\n=\n( 1 2 − )( [γ1 − f(x1) + b2]+ + [γ1 − b2 + f(x4)]+ )\n≥ ( 1 2 − ) (2γ1 − f(x1) + f(x4))\n=\n( 1 2 − )( 2γ1 − ( 1 2 − ) (1− b) (w4 − w1) )\n=\n( 1 2 − )( 2γ1 − ( 1 2 − )( 4γ2 ) (w4 − w1) )\nwhere in the third step we have used the fact that [a]++[b]+ ≥ a+b. Thus, in order to have expected error at most 1, we must have\nw4 − w1 ≥ 1\n4γ2\n( 2γ1 −\n1 1 2 −\n) 1\n1 2 −\n= γ21\n4 1γ2\nby setting = 12 − 1 γ1 which then proves the result after applying an averaging argument."
    }, {
      "heading" : "Appendix F Ranking",
      "text" : "The problem of ranking stems from the need to sort a set of items based on their relevance. In the model considered here, each ranking instance is composed of m documents (pages) (p1, . . . , pm)\nfrom some universe P along with their relevance to some particular query q ∈ Q that are given as relevance scores from some set R ⊂ R. Thus we have X = Q × Pm with each instance x ∈ X being provided with a relevance vector r(x) = Rm. Let the ith query-document pair of a ranking instance x be denoted by zi ∈ Q × P . For any z = (p, q) ∈ P × Q, let r(z) ∈ R denote the true relevance of document p to query q.\nFor any relevance vector r ∈ Rm, let r̄ be the vector with elements of r sorted in descending order and πr be the permutation that this sorting induces. For any permutation π, π(i) shall denote the index given to the index i under π. Although the desired output of a ranking problem is a permutation, we shall follow the standard simplification [27] of requiring the output to be yet another relevance vector s with the permutation πs being considered as the actual output. This converts the ranking problem into a vector-valued regression problem.\nWe will take the true loss function `actual (·, ·) to be the popular NDCG loss function [28] defined below\n`NDCG (s, r) = − 1\n‖G(r)‖D m∑ i=1 G(r(i)) F (πs(i))\nwhere ‖r‖D = maxπ∈Sm m∑ i=1 r(i) F (π(i)) , G(r) = 2r − 1 is the growth function and F (t) = log(1 + t) is the decay function.\nFor the surrogate loss functions `K and `S , we shall use the squared loss function `sq (s, r) = ‖s− r‖22. We shall overload notation to use `sq (·, ·) upon reals as well. For any vector r ∈ Rm, let η(r) := G(r)\n‖G(r)‖D and let ri denote its ith coordinate.\nDue to the decomposable nature of the surrogate loss function, we shall require kernels and similarity functions to act over query-document pairs i.e. K : (P ×Q) × (P ×Q) → R. This also coincides with a common feature extraction methodology (see for example [27, 29]) where every query-document pair is processed to yield a feature vector. Consequently, all our goodness definitions shall loosely correspond to the ability of a kernel/similarity to accurately predict the true relevance scores for a given query-document pair. We shall assume ranking instances to be generated by the sampling of a query q ∼ DQ followed by m independent samples of documents from the (conditional) distribution DP|q . The distribution over ranking instances is then a product distribution D = DX = DQ × DP|q ×DP|q × . . .×DP|q︸ ︷︷ ︸\nm times\n. A key consequence of this generative\nmechanism is that the ith query-document pair of a random ranking instance, for any fixed i, is a random query-document instance selected from the distribution µ := DQ ×DP|q . Definition 28. A similarity function K is said to be ( 0, B)-good for a ranking problem y : X → Sm if for some bounded weight function w : P × Q → [−B,B], for any ranking instance x = (q, p1, p2, . . . , pm), if we define f : X → Rm as\nfi := E z∼µ Jw(z)K(zi, z)K\nwhere zi = (pi, q), then we have E x∼D J`sq (f(x), η(r(z)))K < 0.\nDefinition 29. A PSD kernel K is said to be ( 0, γ)-good for a ranking problem y : X → Sm if there exists W∗ ∈ HK , ‖W∗‖ = 1 such that if for any ranking instance x = (q, p1, p2, . . . , pm), if, for any W ∈ HK , when we define f ( · ;W) : X → Rm as\nfi(x;W) = 〈W,ΦK(zi)〉\nγ\nwhere fi is the ith coordinate of the output of f and zi = (pi, q), then we have E\nx∼D J`sq (f(x;W∗), η(r(z)))K < 0.\nThe choice of this surrogate is motivated by consistency considerations. We would ideally like a minimizer of the surrogate loss to have bounded actual loss as well. Using results from [27], it can be shown that the above defined surrogate is not only consistent, but that excess loss in terms of\nthis surrogate can be transferred to excess loss in terms of `NDCG (·, ·), a very desirable property. Although [27] shows this to be true for a whole family of surrogates, we chose `sq (·, ·) for its simplicity. All our utility arguments carry forward to other surrogates defined in [27] with minimal changes.\nWe move on to prove utility guarantees for the given similarity learning model.\nTheorem 30. Every similarity function that is ( 0, B)-good for a ranking problem form-documents with respect to squared loss is O (√ m\nlogm · √ 0\n) -useful with respect to NDCG loss.\nProof. As before, we use Lemma 15 to construct a landmarked space with a linear predictor f̃ : x 7→ 〈w,x〉 such that E\nz∼µ r∣∣∣f̃ (Ψ (z))− f(z)∣∣∣z ≤ 2 1. We have ‖w‖2 ≤ B and sup x∈X {‖Ψ(x)‖} ≤ 1.\nNow lets overload notation to denote by Ψ(x) the concatenation of the images of the m documentquery pairs in x under Ψ(·) and by f̃(Ψ(x)), the m-dimensional vector obtained by applying f̃ to each of the m components of Ψ(x).\nSince the squared loss function is 2B-Lipschitz in its first argument in the region of interest, we get\nE x∼D\nr `sq ( f̃(Ψ(x)), η(r(x)) )z = E\nx∼D t m∑ i=1 `sq ( f̃(Ψ(zi)), η(r(x))i )|\n= m∑ i=1 E x∼D r `sq ( f̃(Ψ(zi)), η(r(x))i )z =\nm∑ i=1 E x∼D J`sq (f(zi), η(r(x))i)K +\nm∑ i=1 E x∼D r `sq ( f̃(Ψ(zi)), η(r(x))i ) − `sq (f(zi), η(r(x))i) z\n≤ m∑ i=1 E x∼D J`sq (f(zi), η(r(x))i)K + 2B m∑ i=1 E x∼D r∣∣∣f̃(Ψ(zi))− f(zi)∣∣∣z =\nm∑ i=1 E x∼D J`sq (f(zi), η(r(x))i)K + 2B m∑ i=1 E z∼µ r∣∣∣f̃(Ψ(z))− f(z)∣∣∣z ≤\nm∑ i=1 E x∼D J`sq (f(zi), η(r(x))i)K + 4Bm 1\n= E x∼D t m∑ i=1 `sq (f(zi), η(r(x))i) | + 4Bm 1\n= E x∼D J`sq (f(x), η(r(x)))K + 4Bm 1 ≤ 0 + 4Bm 1\nwhere x = (q, p1, . . . , pm) and zi = (pi, q). In the first and the last but one step we have used decomposability of the squared loss, in the fourth step we have used Lipschitz properties of the squared loss, in the fifth step we have used properties of the generative mechanism assumed for ranking instances, in the sixth step we have used the guarantee given by Lemma 15. Throughout we have repeatedly used linearity of expectation. This bounds the excess error due to landmarking to d\ndimensions by 64B2m2 √\nlog(1/δ) d using Lemma 15. Similarly, Lemma 16 also allows us to bound the excess error due to training by 3B2 √\nlog(1/δ) n which puts our total squared loss at 0 + 1 for\nlarge enough d and n.\nWe now invoke [27, Theorem 10] that states that if the surrogate loss function `(·, ·) being used is a Bregman divergence generated by a function that is CS-strongly convex with respect to some norm\n‖·‖ then we can bound `NDCG (s, r) ≤ CF√CS · √ ` (s, r) where CF = 2 ∥∥∥∥( 1F (1) , . . . , 1F (m))>∥∥∥∥ ∗ , F is the decay function used in the definition of NDCG and ‖·‖∗ is the dual norm of ‖·‖. Note that we are using the “noiseless” version of the result where r(x) is a deterministic function of x.\nIn our case the squared loss is 2-strongly convex with respect to the L2 norm which is its own dual. Hence CS = 2 and CF = O\n(√ m\nlogm\n) , if f̂ : x 7→ 〈ŵ,Ψ(x)〉 is our final output, we get, for some\nconstant C,\nE x∼D\nr `NDCG ( f̂(x), r(x) )z ≤ C √ m logm · √ 0 + 4Bm 1 ≤ C √ m logm · √ 0 +C 2m√ logm · √ B 1\nwhich proves the claim. This affects the bounds given by Lemmata 15 and 16 since the dependence of the excess error on d and n will now be in terms of the inverse of their fourth roots instead of inverse of the square roots as was the case in regression and ordinal regression.\nWe note that the (rather heavy) dependence of the final utility guarantee (that is O (√ m 0 ) ) on m is because the decay function F (t) = log(1 + t) chosen here (which seems to be a standard in literature but with little theoretical justification) is a very slowly growing function (it might sound a bit incongruous to have an increasing function as our decay function - however since this function appears in the denominator in the definition of NDCG, it effectively induces a decay). Using decay functions that grow super-linearly (or rather those that induce super-linear decays), we can ensure O (√ 0 ) -usefulness since in those cases, CF = O (1).\nWe next prove admissibility bounds for the ranking problem. The learning setting as well as the proof is different for ranking (due to presence of multiple entities in a single ranking instance), hence we shall provide all the arguments for completeness. Theorem 31. Every PSD kernel that is ( 0, γ)-good for a ranking problem is also( 0 + 1,O ( m √ m\n1 √ 1γ3\n)) -good as a similarity function for any 1 > 0.\nProof. For notational convenience, we shall assume that the RKHS HK is finite dimensional so that we can talks in terms of finite dimensional matrices and vectors. As before, let f(z;W) = 〈W,ΦK(z)〉 and let W′ be the minimizer of the following program.\nmin W∈HK\n1 2 ‖W‖2HK + C Ex∼D J`sq (f(x;W), η(r(x)))K\n≡ min W∈HK\n1 2 ‖W‖2HK + C Ex∼D t m∑ i=1 `sq (f(zi;W), η(r(x))i) |\n≡ min W∈HK\n1 2 ‖W‖2HK + C m∑ i=1 E x∼D J`sq (f(zi;W), η(r(x))i)K\n≡ min W∈HK\n1 2 ‖W‖2HK +mC Ez∼µ J`sq (f(z;W), r̃(z))K + CD\nwhere for any z ∈ Q× P , r̃(z) gives us the expected normalized relevance of this document-query pair across ranking instances and CD is some constant independent of W and dependent solely on the underlying distributions. Using the goodness of the kernel K and the argument given in the proof of Lemma 17, it is possible to show that the vector W′ has squared loss at most 12Cγ2 + 0. Hence the only task remaining is to show that their exists a bounded weight function w such that for all z ∈ P ×Q, we have f(z;W) = 〈W′,ΦK(z)〉 = E\nz′∼µ Jw(z)K(z, z′)K which will prove the\nclaim.\nTo do so we assume that the (finite) set of document-query pairs is (z1, . . . , zk) with zi having probability µi and relevance ri = r̃(zi). Then the above program can equivalently be written as\nmin W∈HK\n1 2 ‖W‖2HK +mC k∑ i=1 µi`sq (〈W,ΦK(zi)〉 , ri)\n≡ min W∈HK\n1 2 ‖W‖2HK +mC ∥∥∥√PX>W −√Pr∥∥∥2 2\n≡ min W∈HK\n1 2 ‖W‖2HK +mC ∥∥∥X̃>W − r̃∥∥∥2 2\n≡ min α∈Rmn\n1 2 ‖Xα‖2HK +mC ∥∥∥X̃>Xα− r̃∥∥∥2 2\nwhere X = (ΦK(z1), . . . ,ΦK(zk)), r = (r1, . . . , rk) >, P is the k × k diagonal matrix with\nPii = µi, X̃ = X √ P and r̃ = √ Pr. The last step follows by the Representer Theorem which tells us that at the optima, W′ = Xα for some α ∈ Rk. Some simple linear algebra shows us that the minimizer α has the form\nα = ( X>X̃X̃>X + 1\n2mC X>X\n)−1 X>X̃ r̃\n= ( GPG+ G\n2mC\n)−1 GPr\n= ( PG+ I\n2mC\n)−1 G−1GPr\n= ( PG+ I\n2mC\n)−1 Pr\nwhere G = X>X is the Gram matrix given by the kernel K. In the third step we have assumed that G does not have vanishing eigenvalues which can always be ensured by adding a small positive constant to the diagonal. Thus we have(\nPG+ I\n2mC\n) α = Pr\nlooking at the ith element of both sides we have\nµi k∑ j=1 αjK(zi, zj) + αi m2C = µiri\nwhich gives us αi = 2mCµi (ri − 〈W′,ΦK(zi)〉). Now assume, without loss of generality, that the relevance scores are normalized, i.e. ri ≤ 1 for all i. Thus we have\n1 2 ‖W′‖2HK +mC ∥∥∥X̃>W′ − r̃∥∥∥2 2 ≤ 1 2 ‖0‖2HK +mC ∥∥∥X̃>0− r̃∥∥∥2 2\nwhich gives us 12 ‖W ′‖2HK ≤ mC ‖r̃‖ 2 2 ≤ mC k∑ i=1 µi = mC which gives us ‖W′‖ ≤ √ 2mC. Since the kernel is already a normalized kernel, ‖ΦK(zi)‖ ≤ 1 which gives us, by an application of Cauchy-Schwartz, |αi| ≤ 2mCµi(1 + √ m2C) ≤ 5µimC √ mC. If we now establish a weight function over the domain wi = αiµi , then |wi| ≤ 5mC √ mC and we can show that for all z, we have 〈W′,ΦK(z)〉 = E z′∼µ\nJw(z)K(z, z′)K. Setting C = 12 1γ2 finishes the proof."
    }, {
      "heading" : "Appendix G Supplementary Experimental Results",
      "text" : "Below we present additional experimental results for regression and ordinal regression problems.\nG.1 Regression Experiments\nWe present results on various benchmark datasets considered in Section 4 for Gaussian K(x,y) = exp ( −‖x−y‖ 2 2\n2σ2 ) and Euclidean: K(x,y) = −‖x− y‖22 kernels. Following standard practice, we\nfixed σ to be the average pairwise distance between data points in the training set.\nG.2 Ordinal Regression Experiments\nWe present results on various benchmark datasets considered in Section 4 for Gaussian K(x,y) = exp ( −‖x−y‖ 2 2\n2σ2 ) and Manhattan: K(x,y) = −‖x− y‖1 kernels."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "We address the problem of general supervised learning when data can only be ac-<lb>cessed through an (indefinite) similarity function between data points. Existing<lb>work on learning with indefinite kernels has concentrated solely on binary/multi-<lb>class classification problems. We propose a model that is generic enough to handle<lb>any supervised learning task and also subsumes the model previously proposed for<lb>classification. We give a “goodness” criterion for similarity functions w.r.t. a given<lb>supervised learning task and then adapt a well-known landmarking technique to<lb>provide efficient algorithms for supervised learning using “good” similarity func-<lb>tions. We demonstrate the effectiveness of our model on three important super-<lb>vised learning problems: a) real-valued regression, b) ordinal regression and c)<lb>ranking where we show that our method guarantees bounded generalization error.<lb>Furthermore, for the case of real-valued regression, we give a natural goodness<lb>definition that, when used in conjunction with a recent result in sparse vector re-<lb>covery, guarantees a sparse predictor with bounded generalization error. Finally,<lb>we report results of our learning algorithms on regression and ordinal regression<lb>tasks using non-PSD similarity functions and demonstrate the effectiveness of<lb>our algorithms, especially that of the sparse landmark selection algorithm that<lb>achieves significantly higher accuracies than the baseline methods while offering<lb>reduced computational costs.",
    "creator" : "LaTeX with hyperref package"
  }
}