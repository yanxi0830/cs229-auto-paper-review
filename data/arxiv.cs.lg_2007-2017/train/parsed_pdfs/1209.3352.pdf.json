{
  "name" : "1209.3352.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Thompson Sampling for Contextual Bandits with Linear Payoffs",
    "authors" : [ "Shipra Agrawal" ],
    "emails" : [ "shipra@microsoft.com", "navingo@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 9.\n33 52\nv1 [\ncs .L\nG ]\n1 5\nSe p\n20 12\nǫ\n√ T 1+ǫd) in time\nT for any 0 < ǫ < 1, where d is the dimension of each context vector and ǫ is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of Ω( √ Td) for this problem. This essentially solves the COLT open problem of Chapelle and Li [COLT 2012] regarding regret bounds for Thompson Sampling for contextual bandits problem.\nOur version of Thompson sampling uses Gaussian prior and Gaussian likelihood function. Our novel martingale-based analysis techniques also allow easy extensions to the use of more general distributions, satisfying certain general conditions.\n0"
    }, {
      "heading" : "1 Introduction",
      "text" : "Multi-armed bandit (MAB) problems model the exploration/exploitation trade-off inherent in many sequential decision problems. There are many versions of multi-armed bandit problems; a particularly useful version is the contextual multi-armed bandit problem. In this problem, on each of T rounds, a learner is presented with the choice of taking one of N actions, referred to as N arms. Before making the choice of which arm to play, the learner sees a d-dimensional feature vector bi, referred to as “context”, associated with each arm i. The learner uses these feature vectors along with the feature vectors and rewards of arms played by her in the past to make the choice of arm. Over time, the learner’s aim is gather enough information about how the feature vectors and rewards are related to each other, so that she can predict, with some certainty, which arm will give the best reward by looking at the feature vectors. The learner competes with a class of predictors, in which each predictor takes in the feature vectors and predicts which arm will give the best reward. If the learner can guarantee to do nearly as well as the predictions of the best predictor in hindsight (to have low regret), the learner is said to successfully compete with that class.\nIn the contextual bandits setting with linear payoff functions, the learner competes with the class of all “linear” predictors on the feature vectors. That is, a predictor is defined by N d-dimensional parameters {µ̃i}Ni=1, and the predictor ranks the arms according to bTi µ̃i. We consider the contextual bandit problem under the linear realizability assumption, that is, we assume that there are unknown underlying parameters {µi}Ni=1 such that the expected reward for each arm i, given context bi, is bTi µi. Under this realizability assumption, the linear predictor corresponding to {µi}Ni=1 is in fact the best predictor and the learner’s aim is to learn these underlying parameters. This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].\nIn this paper, we analyze Thompson Sampling (TS) algorithm for the contextual bandits problem with linear payoffs. Thompson Sampling is one the earliest heuristics for the multi-armed bandit problems. The first version of this Bayesian heuristic is around 80 years old, dating to Thompson (1933) [25]. Since then, it got rediscovered numerous times independently in the context of reinforcement learning, e.g., in [27, 20, 24]. It is a member of the family of randomized probability matching algorithms. The basic idea is to assume a simple prior distribution on the underlying parameters of the reward distribution of every arm, and at every time step, play an arm according to its posterior probability of being the best arm. The general structure of Thompson sampling for the contextual bandits problem involves the following elements:\n1. a set Θ of parameters µ̃;\n2. an assumed prior distribution P (µ̃) on these parameters; 3. past observation D consisting of (context b, reward r) for the past time steps; 4. an assumed likelihood function P (r|b, µ̃), which gives the probability of reward given a context\nb and a parameter µ̃;\n5. a posterior distribution P (µ̃|D) ∝ P (D|µ̃)P (µ̃), where P (D|µ̃) is the likelihood function.\nIn each round, TS plays an arm according to its posterior probability of maximizing the expected reward. A simple way to achieve that is to produce a sample of reward for each arm, using the posterior distributions, and play the arm that produces the largest sample. We emphasize that although TS algorithm is a Bayesian approach, the description of the algorithm and our analysis apply to the prior-free stochastic MAB model, and are directly comparable to the UCB family of\nalgorithms which are a frequentist approach to the same problem. One could interpret the Bayesian priors used by the TS algorithm as a way of capturing the current knowledge about the arms.\nRecently, TS has attracted considerable attention. Several studies (e.g., [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques. Chapelle and Li [7] demonstrate that for the basic stochastic MAB problem, empirically TS achieves regret comparable to the lower bound of [16]; and in applications like display advertising and news article recommendation modeled by the contextual bandits problem, it is competitive to or better than the other methods such as UCB. In their experiments, TS is also more robust to delayed or batched feedback than the other methods. TS has also been used in an industrial scale application for CTR prediction of search ads on search engines [12]. Kaufmann et al. [15] do a thorough comparison of TS with the best known versions of UCB, and show that TS has the lowest regret in the long run.\nDespite being easy to implement and being competitive to the state of the art methods, the theoretical understanding of TS algorithm is limited. [13, 18] provided weak guarantees, namely, a bound of o(T ) on expected regret in time T . More recently, some significant progress was made by [3, 15], who provided near-optimal problem-dependent bounds on the expected regret of TS for the basic (i.e. without contexts) version of the stochastic MAB problem. However, many questions regarding theoretical analysis of TS remained open, including near-optimal problem-independent regret bounds, high probability regret bounds, and regret bounds for the more general contextual bandits setting. Some of these questions were formally raised as a COLT 2012 open problem [8]. In this paper, we use novel and simple martingale-based analysis techniques to demonstrate that TS achieves high probability, near-optimal problem independent regret bounds for contextual bandits with linear payoffs. To our knowledge, ours are the first non-trivial regret bounds for TS for the contextual bandits problem. Additionally, our results are the first high probability regret bounds for TS, even in the case of basic MAB problem. This essentially solves the COLT 2012 open problem [8] for linear contextual bandits.\nThe contextual MAB problem does not seem easily amenable to the techniques used so far for analyzing the basic MAB problem by [3, 15]. In Section 2.3, we describe some of the challenges, and our martingale-based solution ideas to handle them. Our version of Thompson Sampling algorithm, described formally in Section 2.2, uses Gaussian prior and Gaussian likelihood functions. As we discuss towards the end of the paper in Section 4, our techniques are easily extensible to the use of other prior distributions, satisfying certain conditions."
    }, {
      "heading" : "1.1 Our Results",
      "text" : "The formal problem statement appears in Sec. 2.1.\nTheorem 1. For the contextual bandit problem with linear payoffs, with probability 1 − δ, the total regret in time T for Thompson Sampling is bounded by O ( d √ NT 1+ǫ\nǫ lnN lnT ln 1 δ\n)\n, for any\n0 < ǫ < 1. Here, ǫ is a parameter used by the Thompson Sampling algorithm.\nTheorem 2. When µ1 = µ2 · · · = µN = µ, i.e. there is a single underlying d-dimensional parameter µ, then with probability 1 − δ, the total regret in time T for Thompson Sampling is bounded by O ( d √ T 1+ǫ\nǫ lnN lnT ln 1 δ\n)\n.\nRemark 1. Here 0 < ǫ < 1 can be chosen to be any constant. If T is known, one could choose ǫ = 1lnT , to get Õ(d √ NT ) (and Õ(d √ T )) regret bound.\nRemark 2. Note that Theorem 2 has only a logarithmic dependence on the number of arms N , which makes it particularly useful when the number of arms N is very large, but there is a single underlying d-dimensional parameter µ, with d being much smaller than N . One could also recover the setting with different µis from the setting with a single µ, by letting µ be an Nd-dimensional vector formed by appending all the µis, and letting context bi(t) be an Nd-dimensional vector which is 0 in all but d positions corresponding to arm i. However, a direct application of Theorem 2 would then give a slightly weaker bound of Õ( 1√\nǫ Nd\n√ T 1+ǫ) compared to Theorem 1.\nWe will mainly describe the algorithm and regret analysis for the setting of single parameter µ = µ1 = · · ·µN , i.e. the proof of Theorem 2. The algorithm and analysis for the setting with different µis is similar. In Section 4, we describe the changes required for the latter setting in order to get the result of Theorem 1."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "The contextual bandit problem with linear payoffs is a widely studied problem in statistics and machine learning often under different names as mentioned by Chu et al. [9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5]. The name contextual bandits was coined in Langford and Zhang [17].\nChu et al. [9] show that for any algorithm the regret is Ω( √ Td) for d2 ≤ T for the N -armed contextual bandits problem with linear payoffs and single parameter. Auer [4] and Chu et al. [9] SupLinUCB, a complicated algorithm using UCB as a subroutine, for this problem. Chu et al. achieve a regret bound of O( √\nTd ln3(NT ln(T )/δ)) with probability at least 1− δ (Auer [4] proves similar results). Let us compare these results with ours. Our bounds have a factor of d compared to √ d in the bounds just mentioned. As can be observed in our regret analysis, the extra √ d factor in our bounds appears because we will use a concentration inequality that provides only a concentration of O( √\nd ln 1δ ) for the empirical estimate of the mean rewards around the actual mean.\nThe advantage of using this (weaker though more generally applicable) concentration inequality is that it allows for statistical dependence between the samples used in the estimates of the mean rewards, which could be due to the dependence between the past rewards and the future choice of arms, or because the contexts are generated by an adaptive adversary. By contrast, in the analysis of SupLinUCB, [4, 9] consider only oblivious adversary, and achieve statistical independence of samples by using a complicated master procedure SupLin on top of the basic UCB style algorithm. This allows them to use a stronger concentration of O( √\nln 1δ ) given by the Azuma-Hoeffding\ninequality. We do not use any such master algorithm. A closely related setting is that of linear stochastic bandits problem, e.g. [10, 1]. In linear stochastic bandits problem, every arm i is associated with a known fixed vector bi, and the expected reward of the arm, when played, is bTi µ for some common unknown underlying parameter µ. AbbasiYadkori et al. [1] analyze a UCB-style algorithm for that problem. When adapted to our setting, their regret bound is O(d log (T ) √ T + √\ndT log (T/δ)). Note that their regret bound does not depend on N , and thus can even be applied to infinite set of arms, for example, when the set\nof arms is specified as those corresponding to all vectors in a d-dimensional polytope. The lower bound for this setting was given by Dani et al. [10] as Ω(d √ T ). The state-of-the-art bounds for linear bandits problem in case of finite N are given by [6]. They provide an algorithm based on exponential weights, with regret of order √ dT logN for any finite set of N actions. However, their setting is slightly different from ours. They consider a non-stochastic (adversarial) bandit setting where the reward at time t for arm i is bTi µt with µt chosen by an adversary. The set of arms and the associated bi vectors are non-adaptive and fixed in advance.\nWhile the results in this paper do not claim to provide regret bounds for Thompson Sampling algorithm that match or better the best available bounds of this extensively studied problem, our bounds for this natural and efficient heuristic are close to the best bounds. Our bounds are essentially within a factor of √ d lnT of the best bounds for finite N (those for UCB1 by [9, 4],\nand for Exp2 algorithm by [6]), and within √ lnN factor of the best bounds that do not depend on N (by Abbasi-Yadkori et al. [1]). The main contribution of this paper is to provide tools for the analysis of Thompson Sampling algorithm for contextual bandits, which despite of being popular and empirically attractive, has eluded theoretical analysis. While significant recent progress was made in analyzing it for basic MAB [3, 15], it was not clear how to extend that to contextual bandits problem, for which no regret bounds were available. There were considerable difficulties in extending the existing techniques to this case, some of which were also pointed out in [8]. We believe the techniques used in this paper will provide useful insights into the workings of this Bayesian algorithm, and may be useful for further improvements and extensions."
    }, {
      "heading" : "2 Problem setting and algorithm description",
      "text" : ""
    }, {
      "heading" : "2.1 Problem setting",
      "text" : "There are N arms. At time t = 1, 2, . . ., a context vector bi(t) ∈ Rd, ||bi(t)|| ≤ 1, is revealed for every arm i. These context vectors are chosen by an adversary in an adaptive manner after observing the arms played and their rewards up to time t− 1, i.e. history Ht−1,\nHt−1 = {i(w), ri(w)(w), bi(w), i = 1, . . . , N,w = 1, . . . , t− 1}, where i(t) denotes the arm played at time t.\nGiven bi(t), reward for arm i at time t is generated from an (unknown) distribution with mean bi(t)\nTµi, where µi ∈ Rd, ||µi|| ≤ 1 are fixed but unknown parameters. Also, given history Ht−1, and bi(t), i = 1, . . . , N , reward for arms i, i\n′, i 6= i′ are independent of each other. E [\nri(t) {bi(t)}Ni=1,Ht−1 ] = E [ri(t) bi(t)] = bi(t) Tµi\nFurthermore, we assume that ηi,t = ri(t) − bi(t)Tµi is conditionally R-sub-Gaussian for constant R ≥ 0, i.e.,\n∀λ ∈ R,E[eληi,t |{bi(t)}Ni=1,Ht−1] ≤ exp ( λ2R2 2 ) .\nThis assumption is satisfied if ri(t) ∈ [bi(t)Tµi − R, bi(t)Tµi + R] (refer to Remark 1 in Appendix A.1 of [11]). Note that this assumption is weaker than assuming ri(t) is bounded.\nAn algorithm for the contextual bandit problem needs to choose, at every time t, an arm i(t) to play, using history Ht−1 and current contexts bi(t), i = 1, . . . , N . Let i∗(t) denote the optimal arm at time t, i.e. i∗(t) = argmaxi bi(t)Tµi. Then the regret at time t,\nregret(t) = bi∗(t)(t) Tµi∗(t) − bi(t)(t)Tµi(t).\nThe objective is to minimize the total regret R(T ) = ∑Tt=1 regret(t) in time T . The time horizon T is finite but possibly unknown.\nRemark 3. An alternative definition of regret that appears in literature is R(T ) = ∑Tt=1 ri∗(t)(t)− ri(t)(t). When the reward ri(t) at all time steps is bounded as |ri(t)| ≤ R for some constant R, for all i, then we can obtain the same results as in Theorem 1 and Theorem 2 for this definition of regret. The details are provided in Section 3.1."
    }, {
      "heading" : "2.2 Thompson Sampling algorithm",
      "text" : "Here, we describe the algorithm for the setting of single parameter µ = µ1 = · · ·µN . (For the case of N different parameters, see Section 4.) Since there is a single underlying parameter, TS will maintain a common prior distribution over this parameter.\nWe use Gaussian likelihood function and Gaussian prior in our version of Thompson Sampling algorithm. More precisely, we assume that the likelihood of reward ri(t) at time t, given context bi(t) and parameter µ, is given by the pdf of Gaussian distribution N (bi(t)Tµ, v2). Here, v = R √\n6 ǫd ln( 1 δ ), with ǫ ∈ (0, 1) which parameterizes our algorithm. Let\nB(t) = Id + ∑t−1 w=1 bi(w)(w)bi(w)(w) T , µ̂(t) = B(t)−1\n(\n∑t−1 w=1 bi(w)(w)ri(w)(w)\n)\n.\nThen, assuming that the prior for µ at time t is given by N (µ̂(t), v2B(t)−1), it easy to compute the posterior distribution Pr(µ̃|ri(t)) ∝ Pr(ri(t)|bi(t)T µ̃) Pr(µ̃) as N (µ̂(t+1), v2B(t+ 1)−1) (details of this computation are in Appendix A). Or, equivalently, the posterior distribution of the mean reward bi(t+1)\nT µ̂(t+1) for arm i is N (bi(t+1)T µ̂(t+1), v2bi(t+ 1)TB(t+1)−1bi(t+1)). In our Thompson Sampling algorithm, for each arm i, we will generate an independent sample θi(t) from the distribution N (bi(t)T µ̂(t), v2bi(t)TB(t)−1bi(t)) at time t. The arm with maximum value of θi(t) will be played.\nAlgorithm 1: Thompson Sampling for Contextual bandits\nSet B = Id, µ̂ = 0d, f = 0d. foreach t = 1, 2, . . . , do\nFor each arm i = 1, . . . , N , sample θi(t) independently from distribution N (bi(t)T µ̂, v2bi(t)TB−1bi(t)). Play arm i(t) := argmaxi θi(t) and observe reward rt. Update B = B + bi(t)(t)bi(t)(t) T , f = f + bi(t)(t)rt, µ̂ = B −1f .\nend\nRemark 4. Note that in the case of a single underlying parameter µ, one could alternatively first generate a single µ̃ from distribution N (µ̂(t+1), v2B(t+1)−1), and then generate θi(t) as bi(t)T µ̃. This alternative algorithm could be more efficient if N is large, but there is an efficient way to compute maxi bi(t)\nT µ̃. While in this alternative algorithm, the marginal distribution of each θi(t) remains N (bi(t+1)T µ̂(t+1), v2bi(t+1)TB(t+1)−1bi(t+1)), θi(t)s are not independent anymore. In our proof, we utilize the independence of θi(t)s (in Lemma 2), and it is not clear to us at this point whether the algorithm with dependent θis will have the same regret. For the case of N different parameters, this distinction is not important, as a separate µ̃i has to be generated for every i."
    }, {
      "heading" : "2.3 Challenges and solution outline",
      "text" : "The contextual version of the multi-armed bandit problem presents new challenges for the analysis of TS algorithm, and the techniques used so far for analyzing the basic multi-armed bandit problem by [3, 15] do not seem directly applicable. Let us describe some of these difficulties and our novel solution ideas to resolve them.\nIn the basic MAB problem there are N arms, each with mean reward µi, and the regret for playing a suboptimal arm i is µi∗ −µi, where i∗ is the arm with highest mean. Let us compare this to a 1-dimensional contextual MAB problem, where each arm i is associated with a parameter µi, but in addition, at every time t, it is associated with a context bi(t), so that mean reward is bi(t)µi, the best arm i∗(t) at time t is the arm with the highest mean at time t, and the regret for playing arm i is bi∗(t)(t)µi∗(t) − bi(t)µi.\nIn general, the basis of regret analysis for stochastic MAB is to prove that the variance of empirical estimates for all arms decreases fast enough, so that the regret incurred until the variance becomes small enough, is small. In the basic MAB, the variance of the empirical mean is inversely proportional to the number of plays ki(t) of arm i at time t. Thus, every time a suboptimal arm is played, we know that even though a regret of µi∗ − µi ≤ 1 in incurred, there is also an improvement of exactly 1 in the number of plays of that arm, and hence, corresponding decrease in the variance. The techniques for analyzing basic MAB rely on this observation to precisely quantify the exploration-exploitation tradeoff. On the other hand, the variance of empirical mean for contextual case is given by inverse of Bi(t) = ∑t u=1 bu(t)\n2. When a suboptimal arm i is played, if bi(t) is small, the regret bi∗(t)(t)µi∗(t) − bi(t)µi could be much higher than the improvement bi(t) in Bi(t).\nIn our solution, we overcome this difficulty by bounding the expected regret at every step by a function of the probability of playing the optimal arm at that step. So, a high expected regret would mean large expected number of plays of optimal arm, in turn implying that regret is small. More precisely, we prove that, for “most histories” Ft−1,\n1 ( √ 4 ln(NT ) v+ℓ(T )) E[regret(t)|Ft−1] . 1p Pr (i(t) = i∗(t) Ft−1) st,i∗(t) + st,i(t).\nThis inequality will form the basis for establishing our super-martingale process. Here filtration Ft−1 will be defined as the union of history Ht−1 and the contexts bi(t), i = 1, . . . , N at time t. And, p = 1\n2e √ πT ǫ\n, ℓ(T ) = R √ d ln(T 3\nδ ) + 1, st,i = √ bi(t)TB(t)−1bi(t).\nThe main idea behind proving the above inequality is to divide the arms into two groups at any given time :\n• unsaturated arms defined as those with ∆i(t) := bi∗(t)(t)Tµ − bi(t)Tµ ≤ ( √\n4 ln(NT ) v + ℓ(T ))st,i,\n• saturated arms defined as those with ∆i(t) > ( √ 4 ln(NT ) v + ℓ(T ))st,i.\nNote that st,i gives the standard deviation of the estimate bi(t) T µ̂(t) and of θi(t). Thus, intuitively saturated arms are arms with the property that the estimates of the means constructed so far in the direction of their contexts are good, making the deviations st,is small enough—significantly smaller than their current ∆i(t).\nIf an unsaturated arm is played at time t, then regret is at most ∆i(t)(t) ≤ ( √\n4 ln(NT ) v + ℓ(T ))st,i(t). For saturated arms, the regret can be large, but on the other hand, since their deviation st,i is small, the concentration of θi(t) and bi(t) T µ̂(t) will ensure that with reasonable probability, the algorithm is able to distinguish between them and the optimal arm. In particular, we prove\nthat the probability of playing a saturated arm is within p of the probability of playing the optimal arm. Further, using concentration bounds for θi(t) and µ̂(t), the regret at any time can be bounded by ( √\n4 ln(NT ) v + ℓ(T ))(st,i∗(t) + st,i(t)) to get the desired inequality. Then, using the Azuma-Hoeffding inequality for super-martingales, it will follow that with high\nprobability,\nR(T ) = ∑Tt=1 regret(t) . ( √\n4 ln(NT ) v + ℓ(T )) (\n1 p ∑T t=1 I(i(t) = i ∗(t))st,i∗(t) + ∑ t st,i(t)\n)\n≤ ( √ 4 ln(NT ) v + ℓ(T )) (\n1 p\n∑ t st,i(t) + ∑ t st,i(t)\n)\nThen, we will use the inequality ∑ t st,i(t) = O( √ Td) (derived along the lines of [4]), to get the desired regret bound."
    }, {
      "heading" : "3 Regret Analysis: Proof of Theorem 2",
      "text" : "Definition 1. Define ℓ(T ) = R √ d ln(T 3 δ ) + 1, v = R √ 6 ǫd ln( 1 δ ). And for all i, define st,i = √\nbi(t)TB(t)−1bi(t), ∆i(t) = bi∗(t)(t) Tµ − bi(t)Tµ. Also define filtration Ft as the union of history\nuntil time t, and the contexts at time t+ 1, i.e., Ft = {Ht, bi(t+ 1), i = 1, . . . , N}.\nDefinition 2. An arm i is called saturated at time t if ∆i(t) > ( √\n4 ln(NT ) v + ℓ(T ))st,i, and unsaturated otherwise. Let C(t) denote the set of saturated arms at time t. Note that the optimal arm at time t is always unsaturated at time t, i.e., i∗(t) /∈ C(t), and an arm may keep shifting from saturated to unsaturated and vice-versa over time.\nDefinition 3. Define E(t) and Ẽ(t) as the events that bi(t) T µ̂(t) and θi(t) are concentrated around their respective means. More precisely, define E(t) as the event that ∀i : |bi(t)T µ̂(t)− bi(t)Tµ| ≤ ℓ(T )st,i. Define Ẽi(t) as the event that\n|θi(t)− bi(t)T µ̂(t)| ≤ √ 4 ln(NT ) vst,i,\nand Ẽ(t) as the event that ∀i, Ẽi(t) holds.\nLemma 1. For all t, 0 < δ < 1, Pr(E(t)) ≥ 1 − δ T 2 . And, for all possible filtrations Ft−1, ∀i,Pr(Ẽi(t)|Ft−1) ≥ 1− 1NT 2 , and Pr(Ẽ(t)|Ft−1) ≥ 1− 1T 2 .\nProof. The complete proof of this lemma appears in Appendix B.2. The probability bound for E(t) will be proven using the concentration inequality given by Theorem 1 in [1]). The probability bound for Ẽi(t) will be proven using a concentration inequality for Gaussian random variables from [2] stated as Lemma 4 in Appendix B.1 .\nDefinition 4. Recall that regret(t) was defined as the regret at time t, regret(t) = bi∗(t)(t) Tµ − bi(t)(t) Tµ. Define regret′(t) = regret(t)− I(E(t)).\nNext, we establish a super-martingale process that will form the basis of proving our highprobability regret bound.\nDefinition 5. Let\nXt := 1 ( √ 4 ln(NT ) v+ℓ(T )) regret′(t)− 1pI(i(t) = i∗(t))st,i∗(t) − st,i(t) − 5pT 2 ,\nand Yt := ∑t w=1Xw, where p = 1 2e √ πT ǫ .\nLemma 2. (Yt; t ≥ 0) is a super-martingale process with respect to filtration Ft.\nProof. We need to prove that for all t ∈ [1, T ],\n1\n( √ 4 ln(NT ) v + ℓ(T )) E[regret′(t)|Ft−1] ≤ Pr (i(t) = i∗(t) Ft−1) p st,i∗(t) + E [ st,i(t) Ft−1 ] + 5 pT 2 .\nLet g(T ) = ( √ 4 ln(NT ) v + ℓ(T )). Note that whether E(t) is true or not is completely determined by Ft−1. If Ft−1 is such that E(t) does not hold, then regret′(t) = regret(t)− I(E(t)) ≤ 0, and the lemma holds trivially. So, we will prove the above lemma while assuming we are given an Ft−1 such that E(t) holds.\nLet Es(t) denote the event that some (by definition suboptimal) saturated arm i in C(t) exceeds all the suboptimal unsaturated arms at time t, i.e., Es(t) : ∃i ∈ C(t), such that ∀j /∈ C(t), j 6= i∗(t), θi(t) ≥ θj(t). We prove the following lower bound on the probability of playing the optimal arm,\nPr (i(t) = i∗(t) Ft−1) ≥ pPr ( Es(t) Ẽ(t),Ft−1 ) − 2 T 2 .\nAnd we prove that\n1 g(T )E[regret ′(t)|Ft−1] ≤ Pr ( Es(t) Ẽ(t),Ft−1 ) st,i∗(t) + E [ st,i(t) Ft−1 ] + 3 T 2 ,\nto get the desired inequality. For the lower bound on Pr (i(t) = i∗(t) Ft−1),\nPr (i(t) = i∗(t) Ft−1) ≥ Pr(i(t) = i∗(t), Es(t), Ẽ(t)|Ft−1) = Pr(i(t) = i∗(t) Es(t), Ẽ(t),Ft−1) · Pr(Es(t) Ẽ(t),Ft−1) · Pr(Ẽ(t) Ft−1)\n(1)\nNow, given Ẽ(t), and Ft−1 such that E(t) is true, using the definition of saturated arms, it holds that for all i ∈ C(t), θi(t) ≤ bi(t)Tµ+ g(T )st,i ≤ bi(t)Tµ+∆i(t) ≤ bi∗(t)(t)Tµ, and given Es(t), it holds that there exists j ∈ C(t) with maxi/∈C(t) θi(t) ≤ θj(t) ≤ bj(t)Tµ+∆j(t) ≤ bi∗(t)(t)Tµ, so that for all arms i, θi(t) ≤ bi∗(t)(t)Tµ. Therefore,\nPr(i(t) = i∗(t) Es(t), Ẽ(t),Ft−1) ≥ Pr(θi∗(t)(t) ≥ bi∗(t)(t)Tµ Es(t), Ẽ(t),Ft−1) = Pr(θi∗(t)(t) ≥ bi∗(t)(t)Tµ Ẽi∗(t)(t),Ft−1) ≥ Pr(θi∗(t)(t) ≥ bi∗(t)(t)Tµ Ft−1)− 1T 2\nThe equality in above holds because events Es(t) and Ẽi(t),∀i 6= i∗(t) do not concern the optimal arm, and given Ft−1 (and hence i∗(t), bi∗(t)(t), µ̂(t), and B(t)), θi∗(t)(t) is independent of these events. For the last inequality, we use that for any two events A,B, Pr(A) ≤ Pr(A|B) + Pr(B).\nIn Lemma 3, we prove a lower bound of p on the probability of θi∗(t)(t) to exceed the optimal mean reward bi∗(t)(t) Tµi∗(t) given Ft−1 such that E(t) holds. This will be proven using concentration provided by E(t) and anti-concentration of Gaussian random variable θi∗(t)(t). Using Lemma 3,\nPr(i(t) = i∗(t) Es(t), Ẽ(t),Ft−1) ≥ p− 1T 2 Substituting this along with Pr ( Ẽ(t) Ft−1 ) ≥ 1− 1T 2 in Equation (1), we get\nPr (i(t) = i∗(t) Ft−1) ≥ pPr ( Es(t) Ẽi(t),Ft−1 ) − 2 T 2 . (2)\nFor the regret upper bound, we observe that given Ẽ(t), and Ft−1 such that E(t) holds, if an arm i is played at time t, then ∆i(t) ≤ g(T )(st,i + st,i∗(t)). This holds because if an arm i is played at time t, then it must be true that θi(t) ≥ θi∗(t)(t). And, given Ẽ(t) and E(t),\nbi(t) Tµ ≥ θi(t)− g(T )st,i\n≥ θi∗(t)(t)− g(T )st,i ≥ bi∗(t)(t)Tµ− g(T )st,i∗(t) − g(T )st,i.\nAlso, by definition of unsaturated arms, for every unsaturated arm i, ∆i(t) ≤ g(T )st,i. Therefore, E [regret′(t) Ft−1] ≤ E [ ∑ i∈C(t) ∆i(t)I(i = i(t)) Ft−1 ] + E [ ∑ i/∈C(t),i 6=i∗(t) g(T )st,iI(i = i(t)) Ft−1 ]\n(∗) ≤ E [ ∑ i∈C(t) ∆i(t)I(i = i(t)) Ẽ(t),Ft−1 ] + 1 T 2 + g(T )E [ st,i(t)I(i(t) /∈ C(t)) Ft−1 ]\n≤ E [ ( g(T )st,i∗(t) + g(T )st,i(t) ) I(i(t) ∈ C(t)) Ẽ(t),Ft−1 ] + 1 T 2\n+g(T )E [ st,i(t)I(i(t) /∈ C(t)) Ft−1 ]\n(∗∗) ≤ g(T )st,i∗(t) · E [ I(i(t) ∈ C(t)) Ẽ(t),Ft−1 ] + 1 T 2 + g(T )E [ st,i(t) Ft−1 ] ( 1− 1 T 2 )−1\n≤ ( g(T )st,i∗(t) ) Pr ( Es Ẽ(t),Ft−1 ) + 1 T 2 + g(T )E [ st,i(t) Ft−1 ] (1 + 2 T 2 )\n≤ ( g(T )st,i∗(t) ) Pr ( Es Ẽ(t),Ft−1 ) + 1 T 2 + g(T )E [ st,i(t) Ft−1 ] + 2g(T ) T 2\n(3)\nFor the inequality marked (∗), we use that for any random variable A ≤ 1, event B, and F , E[A|F ] ≤ E[A|B,F ] + 1 − Pr(B|F ). We use this with A = ∑i∈C(t) ∆i(t)I(i = i(t)) ≤ ∑\ni∈C(t) ||bi∗(t)(t)|| · ||µi∗(t)||I(i = i(t)) ≤ 1, B = Ẽ(t),F = Ft−1. For the inequality marked (∗∗), we use that for any events A,B,F , Pr(A|F ) ≥ Pr(A|B,F ) Pr(B|F ). We use this with A = I(i(t) /∈ C(t)),B = Ẽ(t), F = Ft−1. For the last inequality, we use that st,i(t) ≤ ||bi(t)(t)|| ≤ 1.\nThe next lemma lower bounds the probability that the sample θi∗(t)(t) of the optimal arm at time t will exceed its mean reward.\nLemma 3. For any filtration Ft−1 such that E(t) is true, Pr ( θi∗(t)(t) ≥ bi∗(t)(t)Tµ Ft−1 )\n≥ 1 2e √ πT ǫ .\nProof. Given event E(t), |bi∗(t)(t)T µ̂(t) − bi∗(t)(t)Tµ| ≤ ℓ(T )st,i∗(t). And, since Gaussian random variable θi∗(t)(t) has mean bi∗(t)(t) T µ̂(t) and standard deviation vst,i∗(t), using anti-concentration inequality in Lemma 4, we will prove that with probability at least 1 2e √ πT ǫ , θi∗(t)(t) will exceed bi∗(t)(t) T µ̂(t) + ( ln Tǫ ) vst,i∗(t). Then, the proof will follow from observing that bi∗(t)(t) T µ̂(t) + (\nln Tǫ ) vst,i∗(t) ≥ bi∗(t)(t)T µ̂(t) + ℓ(T )st,i∗(t) ≥ bi∗(t)(t)Tµ. The details of the proof are in Appendix C.\nNow, we are ready to prove Theorem 2."
    }, {
      "heading" : "3.1 Proof of Theorem 2",
      "text" : "Note that for super-martingale Yt,\n|Yt − Yt−1| = |Xt| = | 1 ( √ 4 ln(NT ) v+ℓ(T )) regret′(t)− 1pI(i(t) = i∗(t))st,i∗(t) − st,i(t) − 5pT 2 | ≤ 7p .\nThe last inequality holds because for any i, st,i = √ bi(t)TB−1(t)bi(t) ≤ ||bi(t)||2 ≤ 1. Therefore, by Azuma-Hoeffding inequality,\nPr (\nYT − Y0 > 7p √\nT ln(2/δ) ) ≤ exp ( − ln(2/δ)T T ) ≤ δ/2.\nTherefore with probability 1− δ2 , ∑T\nt=1\n(\n1 ( √ 4 ln(NT ) v+ℓ(T )) regret′(t)− 1pI(i(t) = i∗(t))st,i∗(t) − st,i(t) − 5pT 2\n)\n≤ 7p √ T ln(2/δ).\nAlso, 1 p ∑T t=1 I(i(t) = i ∗(t))st,i∗(t) + ∑T t=1 st,i(t) + 5 pT = 1 p ∑ t:i(t)=i∗(t) st,i∗(t) + ∑T t=1 st,i(t) + 5 pT\n≤ 1p ∑T t=1 st,i(t) + ∑T t=1 st,i(t) + 5 pT = O( √ T ǫ √ Td lnT )\nFor the last inequality, we use that ∑T t=1 st,i(t) ≤ 5 √ dT lnT , which can be derived along the lines of Lemma 3 of [9] using Lemma 11 of [4]. Details are in Appendix B.3. Therefore, with probability 1− δ2 ,\n∑T t=1 regret ′(t) ≤ ( √\n4 ln(NT ) v + ℓ(T )) · ( O( √ T ǫ √ Td ln T ) + 7p √ T ln(2δ ) ) =\nO\n(\nd √ T 1+ǫ\nǫ lnN lnT ln 1 δ\n)\nAlso, because E(t) holds for all t with probability at least 1 − δ2 (refer to Lemma 1), regret′(t) = regret(t) for all t with probability at least 1− δ2 . Hence, with probability 1− δ,\nR(T ) = ∑Tt=1 regret(t) = O ( d √ T 1+ǫ ǫ lnN lnT ln 1 δ ) .\nTo obtain bounds for the other definition of regret in Remark 3, observe that the expected regret for this definition is the same as before,\nE[regret(t)] = E[ri∗(t)(t)−ri(t)(t)] = E[E[ri∗(t)(t)|i∗(t)]]−E[E[ri(t)(t)|i(t)]] = E[bi∗(t)(t)Tµ−bi(t)(t)Tµ].\nTherefore, Lemma 2 holds as it is, and Yt defined in Definition 5 is a super-martingale with respect to this new definition of regret(t) as well. Now, if |ri(t)| ≤ R for all i, then |regret′(t)| ≤ R and |Yt − Yt−1| ≤ 7p +R, and we can apply Azuma-Hoeffding inequality exactly as in this subsection to obtain regret bounds of the same order as Theorem 2 for the new definition."
    }, {
      "heading" : "4 Extensions",
      "text" : ""
    }, {
      "heading" : "4.1 N different parameters",
      "text" : "Theorem 1 considers the setting where each arm i is associated with a parameter µi, where possibly µi 6= µi′ for two different arms i and i′. In this case, Thompson Sampling would maintain a separate estimate of mean µ̂i(t), and Bi(t) for each arm i which would be updated only at the time instances when i is played. The statements of Lemma 1, and the super-martingale property established by Lemma 2 will hold as it is for the new definitions. The only difference will appear in the bound for ∑\nt st,i(t) used in the proof of Theorem 2. For the case of N different parameters, we will get a\nbound of O( √ NTd lnT ) on this quantity instead of O( √ Td lnT ), leading to the extra √ N factor in the bound in Theorem 1 compared to Theorem 2. The details of the algorithm for the case of N different parameters, and the changes in the analysis required for proving Theorem 1 are provided in Appendix D."
    }, {
      "heading" : "4.2 General distributions",
      "text" : "In the algorithm in this paper, θi(t) is generated from a Gaussian distribution. However, the analysis techniques in this paper are easily extendable to an algorithm that uses a posterior distribution other than the Gaussian distribution. The only distribution specific properties we have used in the analysis are the concentration and anti-concentration inequalities for Gaussian distributed random variables mentioned in Lemma 4. The concentration inequality was used to prove that Ẽ(t) happens with high probability in Lemma 1, and the anti-concentration inequality was used to lower bound the probability that Gaussian distributed random variable θi∗(t)(t) exceeds its mean by some factors of its standard deviation in Lemma 3. If any other distribution provides similar tail inequalities, these inequalities can be used as a black box in the analysis, and the regret bounds can be reproduced for that distribution."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We provided a theoretical analysis of Thompson Sampling for the contextual bandits problem with linear payoffs. Our results resolve many open questions regarding the theoretical guarantees for Thompson Sampling, and establish that even for the contextual version of the stochastic MAB problem, TS achieves regret bounds comparable to the state-of-the-art methods. We used novel martingale-based analysis techniques which are simpler than those in the past work on TS [3, 15], and amenable to extensions. In fact, the techniques introduced in this paper could also be used to provide a simpler proof for the optimal expected regret bounds for TS for the basic MAB problem studied in [3, 15]. The proof of this claim will appear elsewhere.\nSeveral questions remain open. A tighter analysis that can remove the dependence on ǫ is desirable. We believe that our techniques would adapt to provide such bounds for the expected regret. Other avenues to explore are contextual bandits with generalized linear models considered in [11], the setting with delayed and batched feedbacks, and the agnostic case of contextual bandits with linear payoffs. The agnostic case refers to the setting which does not make the realizability assumption that there exists a vector µi for each i for which E[ri(t)|bi(t)] = bi(t)Tµi. To our knowledge, no existing algorithm has been shown to have non-trivial regret bounds for the agnostic case."
    }, {
      "heading" : "A Posterior distribution computation",
      "text" : "Pr(µ̃|ri(t)) ∝ Pr(ri(t)|bi(t)T µ̃) Pr(µ̃) ∝ exp{− 1\n2v2 ((ri(t)− µ̃T bi(t))2 + (µ̃− µ̂(t))TB(t)(µ̃− µ̂(t))}\n∝ exp{− 1 2v2 (ri(t) 2 + µ̃T bi(t)bi(t) T µ̃+ µ̃TB(t)µ̃− 2µ̃T bi(t)ri(t)− 2µ̃TB(t)µ̂(t))} ∝ exp{− 1 2v2 (µ̃TB(t+ 1)µ̃ − 2µ̃TB(t+ 1)µ̂(t+ 1))} ∝ exp{− 1 2v2 (µ̃− µ̂(t+ 1))TB(t+ 1)(µ̃ − µ̂(t+ 1))} ∝ N (µ̂(t+ 1), v2B(t+ 1)−1)\nTherefore, the posterior distribution of µ at time t+ 1 is N (µ̂(t+ 1), v2B(t+ 1)−1),"
    }, {
      "heading" : "B Proof of Theorem 2",
      "text" : "B.1 Gaussian concentration\nFormula 7.1.13 from [2] can be used to derive the following concentration and anti-concentration inequalities for Gaussian distributed random variables.\nLemma 4. [2] For a Gaussian distributed random variable Z with mean m and variance σ2, for any z ≥ 1,\n1\n2 √ πz\ne−z 2/2 ≤ Pr(|Z −m| > zσ) ≤ 1√\nπz e−z 2/2.\nB.2 Proof of Lemma 1\nWe will use the following lemma (implied by Theorem 1 in [1]):\nLemma 5. [1] Let (F ′t ; t ≥ 0) be a filtration, (mt; t ≥ 1) be an Rd-valued stochastic process such that mt is (F ′t−1) measurable, (ηt; t ≥ 1) be a real-valued martingale difference process such that ηt is (F ′t) measurable and For t ≥ 0, define ξt = ∑t u=1muηu and Mt = Id + ∑t u=1 mum T u , where Id is the d-dimensional identity matrix. Assume ηt is conditionally R-sub-Gaussian. Then, for any δ′ > 0, t ≥ 0, with probability at least 1− δ′,\n||ξt||M−1t ≤ R √ d ln ( t+ 1 δ′ ) .\nWe use the above lemma with mt = bi(t)(t), ηt = ri(t) − bi(t)(t)Tµ, F ′t = (mu+1, ηu : u ≤ t). (Note that effectively, F ′t can be imagined to have all the information including the arms played until time t+ 1, except for the reward of the arm played at time t+ 1). By definition of F ′t , mt is F ′t−1 measurable, and ηt is F ′t measurable. And, ηt is a martingale difference process:\nE [ ηt|F ′t−1 ] = E[ri(t)|bi(t)(t), i(t)] − bi(t)(t)Tµ = 0. Also, this makes\nMt = Id + t ∑\nu=1\nmum T u = Id +\nt ∑\nu=1\nbi(u)(u)bi(u)(u) T ,\nξt =\nt ∑\nu=1\nmuηu =\nt ∑\nu=1\nbi(u)(u)(ri(u) − bi(u)(u)Tµ).\nNote that B(t) = Mt−1, and µ̂(t)− µ = M−1t−1(ξt−1 − µ), so that\n|bi(t)T µ̂(t)−bi(t)Tµ| = |bi(t)TM−1t−1(ξt−1−µ)| ≤ ||bi(t)||M−1t−1 ||ξt−1−µ||M−1t−1 = ||bi(t)||B(t)−1 ||ξt−1−µ||M−1t−1 . The inequality holds because M−1t−1 is a positive definite matrix. Using the above lemma, for any δ′ > 0, t ≥ 1, with probability at least 1− δ′,\n||ξt−1||M−1t−1 ≤ R √ d ln ( t δ′ ) .\nTherefore, ||ξt−1 − µ||M−1t−1 ≤ R √ d ln ( t δ′ ) + ||µ||M−1t−1 ≤ R √ d ln ( t δ′ ) + 1. Substituting δ′ = δ T 2 , we get that with probability 1− δ T 2 , for all i,\n|bi(t)T µ̂(t)− bi(t)Tµ| ≤ st,i · ( R √ d ln( T 3\nδ ) + 1\n)\n≤ ℓ(T )st,i\nThis proves the bound on the probability of E(t). To prove bound on probability of Ẽi(t), we use the fact that since θi(t) is distributed as N (bi(t)T µ̂(t), v2bi(t)TB(t)−1bi(t)), therefore, using concentration inequalities for Gaussian random variables,\nPr(|θi(t)− bi(t)T µ̂(t)| > z v √ bi(t)TB(t)−1bi(t)|Ft−1) ≤ 1√ πz e−z 2/2\nSubstituting z = √ 4 ln(NT ) , we get the desired bound.\nB.3 Bound on the sum of st,i(t)\nWe will use the following result, implied by the referred lemma in [4]\nLemma 6. [[4], Lemma 11]. Let A′ = A+xxT , where x ∈ Rd, A,A′ ∈ Rd×d, and all the eigenvalues λj , j = 1, . . . , d of A are greater than or equal to 1. Then, the eigenvalues λ ′ j, j = 1, . . . , d of A\n′ can be arranged so that λj ≤ λ′j for all j, and\nxTA−1x ≤ 10 d ∑\nj=1\nλ′j − λj λj\nLet λj,t denote the eigenvalues of B(t). Note that B(t + 1) = B(t) + bi(t)(t)bi(t)(t) T , and\nλj,t ≥ 1,∀j. Therefore, above implies\ns2t,i(t) ≤ 10 d ∑\nj=1\nλj,t+1 − λj,t λj,t .\nThis allows us to derive the following along the lines of Lemma 3 of [9].\nT ∑\nt=1\nst,i(t) ≤ 5 √ dT lnT ."
    }, {
      "heading" : "C Proof of Lemma 3",
      "text" : "Given event E(t), |bi∗(t)(t)T µ̂(t) − bi∗(t)(t)Tµ| ≤ ℓ(T )st,i∗(t). And, since Gaussian random variable θi∗(t)(t) has mean bi∗(t)(t)\nT µ̂(t) and standard deviation vst,i∗(t), using anti-concentration inequality in Lemma 4,\nPr ( θi∗(t)(t) ≥ bi∗(t)(t)Tµ Ft−1 ) = Pr\n(\nθi∗(t)(t)− bi∗(t)(t)T µ̂(t) vst,i∗(t) ≥ bi ∗(t)(t) Tµ− bi∗(t)(t)T µ̂(t) vst,i∗(t) Ft−1 )\n≥ 1 2 √ π e−Z 2 t\nwhere\n|Zt| = ∣ ∣ ∣ ∣\n∣ bi∗(t)(t) Tµ− bi∗(t)(t)T µ̂(t) vst,i∗(t)\n∣ ∣ ∣ ∣ ∣\n≤ ℓ(T )st,i∗(t)\nvst,i∗(t)\n≤ R √ d ln(T 3 δ ) + 1\nR √\n6 ǫd ln( 1 δ )\n≤ √ ǫ\n2 (lnT + 1)\nPr ( θi∗(t)(t) ≥ bi∗(t)(t)Tµ Ft−1 ) ≥ 1 2 √ π e− ǫ 2 (lnT+1) =\n1\n2e √ πT ǫ 2"
    }, {
      "heading" : "D N different parameters: Proof of Theorem 1",
      "text" : "Theorem 1 considers the setting where each arm i is associated with a parameter µi, where possibly µi 6= µi′ for two different arms i and i′. In this case, Thompson Sampling would maintain a separate estimate of mean µ̂i(t), and Bi(t) for each arm i which would be updated only at the time instances when i is played.\nBi(t) = Id + t−1 ∑\nu=1:i(u)=i\nbi(u)bi(u) T\nµ̂i(t) = Bi(t) −1\n\n\nt−1 ∑\nu=1:i(u)=i\nbi(u)ri(u)\n\n\nst,i = √ bi(t)TBi(t)−1bi(t)\nThe posterior distribution for each arm i at time t would beN (bi(t)T µ̂i(t), v2 bi(t)TBi(t)−1bi(t)). Algorithm 2: Thompson Sampling for Contextual bandits with N parameters\nSet Bi = Id, µ̂i = 0d, i = 1, . . . , N , fi = 0d. foreach t = 1, 2, . . . , do\nFor each arm i = 1, . . . , N , sample θi(t) independently from distribution N (bi(t)T µ̂i, v2 bi(t)TB−1i bi(t)). Play arm i(t) := argmaxi θi(t) and observe reward rt. Update Bi(t) = Bi(t) + bi(t)(t)bi(t)(t) T , fi(t) = fi(t) + bi(t)(t)rt, µ̂i(t) = B −1 i(t)fi(t).\nend\nIn the regret analysis, the events E(t) will now be defined with respect to concentration of all µ̂i(t) around their respective means. That is,\nE(t) : ∀i, bi(t)T µ̂i(t) ∈ [bi(t)Tµi − ℓ(T )st,i, bi(t)Tµi + ℓ(T )st,i]\nSimilarly, Ẽi(t) will be the event that\nθi(t) ∈ [bi(t)T µ̂i(t)− √ 4 ln(NT ) vst,i, bi(t) T µ̂i(t) + √ 4 ln(NT ) vst,i],\nand Ẽ(t) will be the event that ∀i, Ẽi(t) holds. It is easy to observe that the statements of Lemma 1 and the super-martingale property established by Lemma 2 will hold as it is for these new definitions. The only difference will appear in the bound for ∑\nt st,i(t) used in the proof of Theorem\n2. For the case of N different parameters, we will get a bound of O( √ NTd ln T ) on this quantity.\nLet ni(T ) be the number of times arm i is played by time T . Then using Lemma 6, for two consequent time steps t, t′ at which arm i is played\ns2t,i(t) ≤ 10 d ∑\nj=1\nλj,t′ − λj,t λj,t .\nThis allows us to derive the following lemma along the lines of Lemma 3 of [9].\nLemma 7. [[9], Lemma 3] For i = 1, . . . , N ,\nT ∑\nt=1:i(t)=i\nst,i(t) ≤ 5 √ dni(T ) ln(ni(T )).\nUsing above lemma,\nT ∑\nt=1\nst,i(t) =\nN ∑\ni=1\nT ∑\nt=1:i(t)=i\nst,i(t) ≤ N ∑\ni=1\n5 √ ni(T )d lnT ≤ 5 √ N √ ∑\ni\nni(T ) √ d lnT = 5 √ NTd ln T .\nTherefore, following the same lines as proof of Theorem 2, we will get a regret bound ofO(d √ T ǫ\nǫ √ NT lnN lnT ln 1δ )."
    } ],
    "references" : [ {
      "title" : "Improved Algorithms for Linear Stochastic Bandits",
      "author" : [ "Yasin Abbasi-Yadkori", "Dávid Pál", "Csaba Szepesvári" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables",
      "author" : [ "Milton Abramowitz", "Irene A. Stegun" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1964
    }, {
      "title" : "Analysis of Thompson Sampling for the Multi-armed Bandit Problem",
      "author" : [ "Shipra Agrawal", "Navin Goyal" ],
      "venue" : "In COLT,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Using Confidence Bounds for Exploitation-Exploration Trade-offs",
      "author" : [ "Peter Auer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "The Nonstochastic Multiarmed Bandit Problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Towards minimax policies for online linear optimization with bandit feedback",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi", "Sham M. Kakade" ],
      "venue" : "Proceedings of the 25th Conference on Learning Theory (COLT),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "An Empirical Evaluation of Thompson Sampling",
      "author" : [ "Olivier Chapelle", "Lihong Li" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Open Problem: Regret Bounds for Thompson Sampling",
      "author" : [ "Olivier Chapelle", "Lihong Li" ],
      "venue" : "In COLT,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Contextual Bandits with Linear Payoff Functions",
      "author" : [ "Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E. Schapire" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Stochastic Linear Optimization under Bandit Feedback",
      "author" : [ "Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade" ],
      "venue" : "In COLT,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Parametric Bandits: The Generalized Linear Case",
      "author" : [ "Sarah Filippi", "Olivier Cappé", "Aurélien Garivier", "Csaba Szepesvári" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Web-Scale Bayesian Click-Through rate Prediction for Sponsored Search Advertising in Microsoft’s Bing Search Engine",
      "author" : [ "Thore Graepel", "Joaquin Quiñonero Candela", "Thomas Borchert", "Ralf Herbrich" ],
      "venue" : "In ICML,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Solving Two-Armed Bernoulli Bandit Problems Using a Bayesian Learning Automaton",
      "author" : [ "O.-C. Granmo" ],
      "venue" : "International Journal of Intelligent Computing and Cybernetics (IJICC),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Associative Reinforcement Learning: Functions in k-DNF",
      "author" : [ "Leslie Pack Kaelbling" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1994
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1985
    }, {
      "title" : "The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information",
      "author" : [ "John Langford", "Tong Zhang" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Linearly Parametrized Bandits",
      "author" : [ "Pedro A. Ortega", "Daniel A. Braun" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "One-armed badit problem with covariates",
      "author" : [ "Jyotirmoy Sarkar" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1991
    }, {
      "title" : "A modern Bayesian look at the multi-armed bandit",
      "author" : [ "S. Scott" ],
      "venue" : "Applied Stochastic Models in Business and Industry,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Experienceefficient learning in associative bandit problems",
      "author" : [ "Alexander L. Strehl", "Chris Mesterharm", "Michael L. Littman", "Haym Hirsh" ],
      "venue" : "In ICML,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2006
    }, {
      "title" : "A Bayesian Framework for Reinforcement Learning",
      "author" : [ "Malcolm J.A. Strens" ],
      "venue" : "In ICML, pages 943–950,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2000
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "William R. Thompson" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1933
    }, {
      "title" : "A one-armed bandit problem with a concomitant variable",
      "author" : [ "Michael Woodroofe" ],
      "venue" : "Journal of the American Statistics Association,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1979
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].",
      "startOffset" : 103,
      "endOffset" : 116
    }, {
      "referenceID" : 10,
      "context" : "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].",
      "startOffset" : 103,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].",
      "startOffset" : 103,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].",
      "startOffset" : 103,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "The first version of this Bayesian heuristic is around 80 years old, dating to Thompson (1933) [25].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : ", in [27, 20, 24].",
      "startOffset" : 5,
      "endOffset" : 17
    }, {
      "referenceID" : 20,
      "context" : ", in [27, 20, 24].",
      "startOffset" : 5,
      "endOffset" : 17
    }, {
      "referenceID" : 12,
      "context" : ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 18,
      "context" : ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 18,
      "context" : ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "Chapelle and Li [7] demonstrate that for the basic stochastic MAB problem, empirically TS achieves regret comparable to the lower bound of [16]; and in applications like display advertising and news article recommendation modeled by the contextual bandits problem, it is competitive to or better than the other methods such as UCB.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : "Chapelle and Li [7] demonstrate that for the basic stochastic MAB problem, empirically TS achieves regret comparable to the lower bound of [16]; and in applications like display advertising and news article recommendation modeled by the contextual bandits problem, it is competitive to or better than the other methods such as UCB.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "TS has also been used in an industrial scale application for CTR prediction of search ads on search engines [12].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "[13, 18] provided weak guarantees, namely, a bound of o(T ) on expected regret in time T .",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 2,
      "context" : "More recently, some significant progress was made by [3, 15], who provided near-optimal problem-dependent bounds on the expected regret of TS for the basic (i.",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "Some of these questions were formally raised as a COLT 2012 open problem [8].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "This essentially solves the COLT 2012 open problem [8] for linear contextual bandits.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "The contextual MAB problem does not seem easily amenable to the techniques used so far for analyzing the basic MAB problem by [3, 15].",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 22,
      "context" : "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 13,
      "context" : "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 19,
      "context" : "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 15,
      "context" : "The name contextual bandits was coined in Langford and Zhang [17].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "[9] show that for any algorithm the regret is Ω( √ Td) for d2 ≤ T for the N -armed contextual bandits problem with linear payoffs and single parameter.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "Auer [4] and Chu et al.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 8,
      "context" : "[9] SupLinUCB, a complicated algorithm using UCB as a subroutine, for this problem.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "Td ln(NT ln(T )/δ)) with probability at least 1− δ (Auer [4] proves similar results).",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "By contrast, in the analysis of SupLinUCB, [4, 9] consider only oblivious adversary, and achieve statistical independence of samples by using a complicated master procedure SupLin on top of the basic UCB style algorithm.",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "By contrast, in the analysis of SupLinUCB, [4, 9] consider only oblivious adversary, and achieve statistical independence of samples by using a complicated master procedure SupLin on top of the basic UCB style algorithm.",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "[10, 1].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "[10, 1].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "[1] analyze a UCB-style algorithm for that problem.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] as Ω(d √ T ).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "The state-of-the-art bounds for linear bandits problem in case of finite N are given by [6].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "Our bounds are essentially within a factor of √ d lnT of the best bounds for finite N (those for UCB1 by [9, 4], and for Exp2 algorithm by [6]), and within √ lnN factor of the best bounds that do not depend on N (by Abbasi-Yadkori et al.",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "Our bounds are essentially within a factor of √ d lnT of the best bounds for finite N (those for UCB1 by [9, 4], and for Exp2 algorithm by [6]), and within √ lnN factor of the best bounds that do not depend on N (by Abbasi-Yadkori et al.",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "Our bounds are essentially within a factor of √ d lnT of the best bounds for finite N (those for UCB1 by [9, 4], and for Exp2 algorithm by [6]), and within √ lnN factor of the best bounds that do not depend on N (by Abbasi-Yadkori et al.",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "[1]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "While significant recent progress was made in analyzing it for basic MAB [3, 15], it was not clear how to extend that to contextual bandits problem, for which no regret bounds were available.",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "There were considerable difficulties in extending the existing techniques to this case, some of which were also pointed out in [8].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "1 of [11]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "3 Challenges and solution outline The contextual version of the multi-armed bandit problem presents new challenges for the analysis of TS algorithm, and the techniques used so far for analyzing the basic multi-armed bandit problem by [3, 15] do not seem directly applicable.",
      "startOffset" : 234,
      "endOffset" : 241
    }, {
      "referenceID" : 3,
      "context" : "t st,i(t) = O( √ Td) (derived along the lines of [4]), to get the desired regret bound.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "The probability bound for E(t) will be proven using the concentration inequality given by Theorem 1 in [1]).",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "The probability bound for Ẽi(t) will be proven using a concentration inequality for Gaussian random variables from [2] stated as Lemma 4 in Appendix B.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "Also, 1 p ∑T t=1 I(i(t) = i (t))st,i∗(t) + ∑T t=1 st,i(t) + 5 pT = 1 p ∑ t:i(t)=i(t) st,i∗(t) + ∑T t=1 st,i(t) + 5 pT ≤ 1 p ∑T t=1 st,i(t) + ∑T t=1 st,i(t) + 5 pT = O( √ T ǫ √ Td lnT ) For the last inequality, we use that ∑T t=1 st,i(t) ≤ 5 √ dT lnT , which can be derived along the lines of Lemma 3 of [9] using Lemma 11 of [4].",
      "startOffset" : 303,
      "endOffset" : 306
    }, {
      "referenceID" : 3,
      "context" : "Also, 1 p ∑T t=1 I(i(t) = i (t))st,i∗(t) + ∑T t=1 st,i(t) + 5 pT = 1 p ∑ t:i(t)=i(t) st,i∗(t) + ∑T t=1 st,i(t) + 5 pT ≤ 1 p ∑T t=1 st,i(t) + ∑T t=1 st,i(t) + 5 pT = O( √ T ǫ √ Td lnT ) For the last inequality, we use that ∑T t=1 st,i(t) ≤ 5 √ dT lnT , which can be derived along the lines of Lemma 3 of [9] using Lemma 11 of [4].",
      "startOffset" : 325,
      "endOffset" : 328
    }, {
      "referenceID" : 2,
      "context" : "We used novel martingale-based analysis techniques which are simpler than those in the past work on TS [3, 15], and amenable to extensions.",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "In fact, the techniques introduced in this paper could also be used to provide a simpler proof for the optimal expected regret bounds for TS for the basic MAB problem studied in [3, 15].",
      "startOffset" : 178,
      "endOffset" : 185
    }, {
      "referenceID" : 10,
      "context" : "Other avenues to explore are contextual bandits with generalized linear models considered in [11], the setting with delayed and batched feedbacks, and the agnostic case of contextual bandits with linear payoffs.",
      "startOffset" : 93,
      "endOffset" : 97
    } ],
    "year" : 2016,
    "abstractText" : "Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze Thompson Sampling algorithm for the contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is perhaps the most important and widely studied version of the contextual bandits problem. We prove a high probability regret bound of Õ( 1 √ ǫ √ T d) in time T for any 0 < ǫ < 1, where d is the dimension of each context vector and ǫ is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of Ω( √ Td) for this problem. This essentially solves the COLT open problem of Chapelle and Li [COLT 2012] regarding regret bounds for Thompson Sampling for contextual bandits problem. Our version of Thompson sampling uses Gaussian prior and Gaussian likelihood function. Our novel martingale-based analysis techniques also allow easy extensions to the use of more general distributions, satisfying certain general conditions.",
    "creator" : "LaTeX with hyperref package"
  }
}