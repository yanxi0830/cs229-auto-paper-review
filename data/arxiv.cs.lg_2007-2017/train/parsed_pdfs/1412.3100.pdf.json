{
  "name" : "1412.3100.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semi-Supervised Learning with Heterophily",
    "authors" : [ "Wolfgang Gatterbauer" ],
    "emails" : [ "gatt@cmu.edu" ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "Graph-based Semi-Supervised Learning (SSL) methods define a graph where the nodes are labeled and unlabeled examples in the dataset, and where (potentially weighted) edges reflect the similarity of examples [26]. Given this input, the goal of SSL is to infer the labels of the unlabeled data. Existing methods commonly assume “smoothness” of labels over the graph, i.e. a certain homophily or assortative mixing property in the network that results in “birds of a feather flock together.” However, the reverse is often true in actual data and is also called heterophily (“opposites attract”). Previous work has looked into ways to adapt the existing SSL frameworks to handle both similarity and dissimilarity (e.g., [7] and [21]). These methods are limited to expressing binary dependencies between nodes (e.g., more similar or more dissimilar). We are interested in more general constraints among classes and also how to learn them from data. For example, assume a social dating network with three different kinds of classes amongst its users. Class 1 prefers to date users of class 2 (and v.v.), whereas users\nof class 3 prefer to date among themselves (see Fig. 1a).1 We call these relations simply heterophily relations, as they naturally generalize notions of similarity/dissimilarity and assortative or disassortative mixing.2\nIn this paper, we show how existing SSL methods (e.g., those based on Local and Blobal Consistency (LGC) [25] or Harmonic Function methods (HF) [27]) can be generalized in a natural way as to allow to propagate heterophily from labeled to unlabeled data. This allows us to propagate label information through a graph in the presence of heterophily, thus, generalizing the commonly implied smoothness assumption between nodes of similar labels. In this regard, this work draws heavily upon and provides a generalization of our recent work on linearizing the update equations of belief propagation [6].\nOur key idea relies on well-known and intuitive facts from Markov chains and properties of symmetric doubly stochastic matrices. We illustrate with the help of Fig. 2c: when a stochastic vector x (which, e.g., represents the inferred label distribution of a certain node) is transformed (or “modulated”) with a doubly stochastic matrix H, then the re-\n1Notice that we imply undirected, i.e. symmetric relationships throughout this paper. 2In the physics community, homophily or assortative mixing is also referred to as “ferromagnetism,” while heterophily or disassortative mixing is referred to as “antiferromagnetism.”\nar X\niv :1\n41 2.\n31 00\nv1 [\ncs .L\nG ]\n9 D\nec 2\n01 4\nsulting vector x′ is still stochastic.3 This simple fact allows us to express a node’s label distribution based on the label distribution of its neighbors and, furthermore, to express arbitrary similarity or dissimilarity relationships between neighboring nodes. We call such a modulating H matrix interchangeably affinity, coupling, modulation, or heterophily matrix, as it captures the pair-wise preference or coupling strengths between nodes and their classes in a network. This, in turn, allows use to derive a generalized formulation of standard semi-supervised learning approaches and to derive them back by using the identify matrix Ik as affinity matrix. The original Linear Belief Propagation (LinBP) formulation is one of several possible linear label propagation approaches. We thus call our first contribution Semi-Supervised Learning with Heterophily (SSL-H).\nOur second key contribution is a simple way to learn the heterophily matrix H from the data. Here, we draw an interesting connection to the Locally Linear Embedding (LLE) [15] framework. While the problems in LLE and our setup are different (LLE tries to find an optimal linear embedding across neighbors to reduce the dimensionality of the data; we find an optimal “heterophily explanation” between the classes of neighbors in a network), the mathematical formalism is similar. We thus call our second contribution Linear Heterophily Estimation (LHE). Together, SSL-H and LHE allow us to both (i) learn heterophily and to (ii) label unlabeled data (see Fig. 2b).\nContributions. The two main contributions are thus: 1. Semi-Supervised Learning with Heterophily (SSL-H):\nWe generalize semi-supervised learning to general heterophily assumptions. The commonly used smooth-\n3A Markov process is described by a state transition matrix T where Ti,j = P[X(t+1) = i|X(t) = j] is the probability to end up in state i if starting at j in the previous step. In that formalism, T is column stochastic and the column-wise sums are equal to 1: ∑ i Ti,j = 1.\nness assumption is the important special case with the identify matrix as affinity matrix. 2. Linear Heterophily Estimation (LHE): We show how heterophily can be learned from existing partially labeled data even at the presence of few labels. This resolves the issue that the propagation matrix would otherwise have to be supplied by domain experts.\nOutline. Section 2 starts with reviewing existing work that our approaches builds upon. Section 3 gives our first contribution and show how to generalize semi-supervised learning to heterophily. Section 4 gives our second contribution and shows how heterophily can be learned from partially labeled data. Section 5 gives experiments, Section 6 reviews related work, before Section 7 concludes."
    }, {
      "heading" : "2. BACKGROUND",
      "text" : "Here we describe three highly related bodies of work that the methods described in this paper build upon."
    }, {
      "heading" : "2.1 Semi-Supervised Learning (SSL)",
      "text" : "Semi-supervised learning methods (SSL) derive their formalism usually by motivating a loss function consisting of (i) a fit term to existing labels, e.g. (fi − xi)2 where xi denotes the given label and fi the learned label, and (ii) a smoothness term or regularizer, e.g. (fi − fj)2. We focus our discussion here only on the most important aspects of SSL and refer to [26] and [4] for two excellent surveys on SSL. We follow here the exposition of [1] with two notable restrictions: (i) we assume a symmetric graph structure W = Wᵀ; (ii) we allow relabeling of already labeled information ((fi − xi)2 6= 0 possible). We focus on three methods, in particular:\n1. The harmonic function method (HF) [27] minimizes the loss function E(f) = ∑ i(fi−xi)2+ µ2 ∑ i,jWij(fi−\nfj) 2. Figure 3 shows E(f) by using the Laplacian ma-\ntrix: L = D−W. 2. The linear neighborhood propagation (LNP) [22] is a\nvariation in which the weights of the neighbors of a node i need to sum up to 1: ∑ iWij = 1. The update equation can thus be written slightly simpler by using α = µ\n1+µ .\n3. The local and global consistency method (LGC) [25] is similar to HF except for normalizing the labeling function by the square root of the degrees of each node:\nE(f) = ∑ i(fi − xi)2 + µ2 ∑ i,jWij(\nfi√ di − fj√ dj )2 The\nenergy function can thus be written by using the normalized Laplacian matrix: Ln = D − 1 2 LD− 1 2 , and the\nupdate equations by using L∗ = I− Ln. Multi-class classification with homophily. It is easy to extend existing label propagation algorithms to multiclass classification problems [22] by assigning with a vector to each node, where each entry represents the belief of a node in a particular labeling class. Suppose there are k classes, and the label set becomes L = {1, 2, . . . , k}. LetM be a set of n× k matrices with nonnegative real-valued entries. Any matrix F = [Fᵀ1,F ᵀ 1, . . . ,F ᵀ n]\nᵀ ∈M corresponds to a specific classification onX that labels i as yi = argmaxi≤kFij . Thus, F can be viewed as the n×k-dimensional label matrix or as a function that assign labels for each data point. Initially, we set F0 = T, where Tij = 1 if xi is labeled as j, and Tij = 0 otherwise, and for unlabeled points, Tuj = 0(1 ≤ j ≤ k).\nFigure 3 show the required changes to the loss function, update equations and closed-form solutions for HF, LNP and LGP, respectively (we will discuss FaBP and LinBP separately). Importantly, notice that labels for different classes do not interact with each other. In other words, they have no influence on each other."
    }, {
      "heading" : "2.2 Locally Linear Embedding (LLE)",
      "text" : "Locally linear embedding (LLE) [15] is a method to derive compact representations of high-dimensional data by building a linear relationship among neighboring points. It is originally an unsupervised learning algorithm. We will later use the formulation for our SSL scenario, namely estimating the heterophily matrix H instead of the embedding W from data. We follow here exactly the exposition in the original paper [11] while using our notation.\nThe LLE algorithm assumes the data consist of n realvalued vectors xi, each of dimensionality k. LLE then reconstructs each data point from its neighbors by constructing a locally linear combination. Reconstruction errors are measured by the loss function:\nE(W) = ∑\ni\n||xi − ∑\nj\nWijxj ||2 (1)\nwhich adds up the squared distances between all the data points and their reconstructions. The weights Wij summarize the contribution of the j-th data point to the i-th reconstruction. To compute the weights Wij , one minimizes the cost function subject to the constraint of all rows of the weight matrix summing to one: ∑ jWij = 1. The optimal weights Wij are found by solving a least-squares problem and can be computed in closed form.\nConsider a particular data point x with neighbors ηj and sum-to-one reconstruction weights w. The reconstruction error ||x−∑Kj=1 wjηj ||2 is minimized in three steps: First, evaluate inner products between neighbors to compute the neighborhood correlation matrix, Cjk = ηj ηk and its matrix inverse, C−1. Second, compute the Lagrange multiplier, λ = α/β, that enforces the sum-to-one constraint, where α = 1−∑jk C−1jk (xηk) and β = ∑ jk C −1 jk . Third, compute the reconstruction weights: wj = ∑ k C −1 jk (xηk + λ)\nIn the final step of LLE, each high-dimensional observation xi is mapped to a low-dimensional vector f i representing global internal coordinates on the manifold. This is done by choosing d < k-dimensional coordinates f i to minimize\nthe embedding cost function\nE(f) = ∑\ni\n||f i − ∑\nj\nWijf j ||2 (2)\nThis cost function, like the previous one, is based on locally linear reconstruction errors, but here we fix the weights Wij while optimizing the coordinates f i."
    }, {
      "heading" : "2.3 Linearized Belief Propagation (LinBP)",
      "text" : "Another widely used methods for semi-supervised reasoning in networked data is Belief Propagation (BP) [18]. Similar to the other SSL methods, BP helps to propagate the information from a few labeled nodes throughout the network by iteratively propagating information between neighboring nodes. In addition, it can handle the case of multiple classes influencing each other. For graphs with loops, however, BP has well-known convergence problems (see [18] for a detailed discussion from a practitioner’s point of view). While there is a lot of work on convergence of BP [5, 12], exact criteria for convergence are not known [13, Sec. 22], and practical use of BP is still non-trivial [18].\nMotivated by this, Koutra et al. [10] proposed to linearize belief propagation for the case of two classes and proposed fast belief propagation (FaBP) as a method to propagate existing knowledge of homophily or heterophily to unlabeled data. This framework allows to specify a homophily factor h (h > 0 for homophily or h < 0 for heterophily) and to then use this algorithm with exact convergence criteria for binary classification (e.g., yes/no or male/female).\nIn [6], we have recently solved the problem for the multiclass case and proposed Linearized Belief Propagation (LinBP) as an efficient linearization of belief propagation on pairwise Markov random fields.4 Our observations stem from the insight that the original update equations of belief propagation (here written compactly in matrix notation by using the symbol for the Hadamard product5)\nf i ∝ xi ⊙\nj∈N(i) mji\nmit ∝ H ( xi\n⊙\nj∈N(i)\\t mji\n)\n4Notice that in [6], we distinguished between two versions of linearized belief propagation: LinBP and LinBP∗. The formulation of LinBP that we consider in this paper corresponds to LinBP∗ while we do not discuss the original LinBP version. 5The Hadamard product (also called component-wise multiplication operator), is defined by: Z = X Y ⇔ Z(i, j) = X(i, j) · Y (i, j).\ncan be approximated by linearized equations\nf̂ i ← x̂i + 1 k · ∑\nj∈N(i) m̂ji (3)\nm̂it ← Ĥ ( f̂ i − 1\nk m̂ti\n) (4)\nby “centering” all variables around appropriate values. We call a vector or matrix x “centered around c” if all its entries are close to c and their average is exactly c. If a vector x is centered around c, then the residual vector around c is defined as x̂ = [x1− c, x2− c, . . .]. Accordingly, we denote a matrix X̂ as a residual matrix if each column and row vector corresponds to a residual vector.\nConcretely, we centered the k-dimensional message vectors m are around 1, and centered all the other k-dimensional vectors/matrices around 1/k. Thus, they become probability vectors and their entries have to sum up to 1. As a consequence, Ĥ ∈ Rk×k is the residual coupling matrix that makes explicit the relative attraction and repulsion: the sign of Ĥji tells us if the class j attracts or repels class i in a neighbor, and the magnitude of Ĥji indicates the strength. Subsequently, this centering allows us to rewrite belief propagation in terms of the residuals. Importantly, the derived messages remain centered for any iteration and thus no normalization is necessary.\nWe have also given an iterative calculation of the final beliefs. Starting with an arbitrary initialization of F̂ (e.g., all values zero), we repeatedly compute the right hand side of the equations and update the values of F until the process converges: We have shown that the solution for LinBP can be calculated by applying the following iterative update equation:\nF̂← X̂ + WF̂Ĥ (LinBP) (5) Thus, the final beliefs of each node can be computed via elegant matrix operations and optimized solvers. In addition, this formalism allows a closed form solution:\nProposition 1 (Closed-form [6]). The closed-form solution for Eq. 5 is given by:\nvec ( F̂ ) = (I− Ĥ⊗W)−1vec ( X̂ )\n(6)\nFurthermore, by analyzing Eq. 6, we could derive exact (sufficient and necessary) conditions for convergence of our update equations based on the spectral radius (ρ) of the adjacency matrix W and the affinity matrix H:\nLemma 2 (Convergence [6]). Necessary and sufficient criterium for convergence of LinBP is:\nLinBP converges⇔ ρ(Ĥ) < 1 ρ(W)\n(7)\nIn practice, we use the matrix H as a scaled version of an original unscaled but centered matrix: Ĥ = hĤ0."
    }, {
      "heading" : "3. GENERALIZING SSL FOR HETEROPHILY",
      "text" : "We illustrate here generalizing the update equations for the harmonic functions (HF) method [27] to Harmonic functions with heterophily (HFH).\nDerivation from regularization framework. In the following, we partially follow the exposition by Bengio et al. [1]. Assume n nodes with ` labeled nodes, and WLOG\nall labeled nodes have index i ≤ `. The harmonic function method (HF) [27] then minimizes the loss function\nE(f) = ∑̀\ni\n(fi − xi)2 + µ 2\nn∑\ni,j\nWij(fi − fj)2\nLet G be the diagonal matrix with Gi,i = 1 if i ≤ `, and Gi,i = 0, otherwise and write L = D −W for the graph Laplacian matrix. Thus, we can alternatively write:\nE(f) = ||Gf −Gx||2 + µfᵀLf We get as derivative:\n∂E(f)\n∂f = G(f − x) + µLf\nSetting the derivative to 0, we get:\n(G + µL)f = Gx\nf = (G + µL)−1Gx\nModulating the existing update equations. Intuitively, the previous equations can be seen as a weighted average of the neighbors’ current labels, where for labeled examples we overwrite the propagated values with the initial label.\nIn the following, we write slightly different update equations. Let’s call P := D−1 W the row-stochastic adjacency matrix and write the update equation as follows:\nf ′ ← P f However, we always use the explicit label if available, i.e. the labels of explicit beliefs constrained to be equal x:\nfi ← { f ′i if i implicit node\nxi if i explicit node\nAnother way to write this is by defining Px with:\nP xi,j ← { Pi,j if i implicit node\n0 if i explicit node\nand writing:\nf ← x + Px f In other words, we define a new matrix that is the result of deleting all column for the indices of explicit beliefs. An alternative way to write this is by defining a column vector e that has an entry 1 for all indices of explicit beliefs, and 0 otherwise. Then, ē = 1− e, and Px = 1 · ēᵀ P.\nNew assume we have k different classes, and we propagate each separately. However, the nodes are the same across all classes. Then the multi-class version is\nF← X + PxF The, we get the closed form after convergence:\nF = X + PxF\nF = (I−Px)−1X New, we can next introduce “heterophily propagation,”\ni.e. we modulate the vectors before propagating them:\nF← X + PxFH (HHF) (8)\nWe can now use the exact same mathematics we had derived in [6] in order to determine the closed-form as follows:\nvec ( F ) = (I−H⊗Px)−1vec ( X )\nNotice that the above update equations will always converge as long as the spectral radius of Px < 1. This is the case if each connected component has at least one explicit belief\nConstrained LinBP (LinBPx). We propose here a “constraint” variant of LinBP, where the labels of explicit nodes (sometimes depicted as f l) are constrained to the explicit beliefs (xl). We use the same trick as before by defining Wx with:\nW xi,j ← { Wi,j if i > l\n0 if i ≤ l\nto get:\nF̂← X̂ + WxF̂Ĥ vec ( F̂ ) = (I− Ĥ⊗Wx)−1vec ( X̂ )\nWe call the resulting formulation LinBPx, where the x notation means the constrained (or “fixed”) version of some propagation matrix.\nGeneralization. More generally, we propose here a general form of update equations and closed-form where the matrix M can be chosen from existing semi-supervised learning methods.\nF← X + MFH vec ( F ) = (I−H⊗M)−1vec ( X )\nWe have seen three examples: W for LinBP, Px for HFHx and Wx for LinBPx."
    }, {
      "heading" : "4. LEARNING HETEROPHILY FROM DATA",
      "text" : "In this section, we introduce our framework for learning heterophily from partially labeled data."
    }, {
      "heading" : "4.1 Baseline approach: MHE",
      "text" : "We first introduce a straight-forward baseline method that we call Myopic Heterophily Estimation (MHE). We call it “myopic” as it tries to infer the relative frequencies between different classes in the network by a straightforward frequency calculation, followed by a transformation into a symmetric doubly-stochastic matrix.\nGiven partially labeled n × k-matrix X′, with X ′(i, c) = 1, if node i has label c. Recall that some nodes have no label. Then the Matrix n × k-matrix N := WX′ contains the number of neighbors of a certain class for each node, i.e. N(i, c) determines the number of neighbors from i that are of class c. Next, the matrix H′ = X′ᵀN = X′ᵀWX has as entry H(c, d) the number of nodes of class d that are neighbors of nodes of class c. This matrix is symmetric, but not stochastic. One can make it row-stochastic by dividing each row by the sum of each row. Let’s call this matrix H′′. This matrix, however, is not symmetric anymore.\nWhat are proposing as baseline method is touse the symmetric, doubly stochastic matrix H∗ (i.e., it fulfills the conditions H = Hᵀ and H 1 = 1) that is closest to the matrix\nH′′ by using the Frobenius norm as distance:\nargmin H\n||H−H′′|| (MHE)\nsubject to H = Hᵀ\nH 1 = 1\nThis problem can be solved efficiently with Algorithm 1 in [24] that finds a Frobenius-norm optimum doubly stochastic approximation to a given matrix\nNotice that in the case of an incompletely labeled graph, it suffices to consider only the subgraph induced by the labeled nodes.\nExample 3. Consider the graph in Fig. 4a. Gray nodes are showing unlabeled nodes. Thus the graph we need to consider is only Fig. 4b. + and − show classes 1 and 2, respectively. Then H′=[ 0 22 2 ]. And H ′′= [ 0 1 1 2 1 2 ] . MHE leads to H∗= [ 1 4 3 4\n3 4 1 4\n] . In constrast, LHE leads to H= [ 1 3 2 3\n2 3 1 3\n] ."
    }, {
      "heading" : "4.2 Improved approach: LHE",
      "text" : "We next give a simple energy minimization framework for LinBP. This formalization will allows us in a latter step to solve the problem of deriving the optimal heterophily matrix.\nProposition 4 (LinBP loss function). The energy function minimized by LinBP is\nE(F) = ||F−X−WFH||2 (9)\nThe proof follows immediately from our proof of convergence of LinBP [6].\nWe would next like to compare the loss functions for LLE (Eq. 1), LinBP (Eq. 9), and HF (Fig. 3):\nE(W)= ∑\ni\n||xi − ∑\nj\nWijxj ||2 (LLE)\nE(F) = ||F−X−WFH||2 (LinBP) E(F) = ||F−X||2 + µ\n2\n∑\ni,j\nWij ∑\nk\n(Fik − Fjk)2 (HF)\nNotice several similarities and differences: (i) LLE tries to learn W, whereas LinBP and HF try to learn F. (ii) By ignoring explicit labels X (i.e., ignoring the fit term in HF and ignoring the vector in LinBP), and replacing H with the identity matrix I), all three equations become the same. We thus propose the following two formalizations:\n1. First, we propose to estimate the heterophily matrix H from partially labeled data via the following loss function\nE(H) = ||X−WXH||2 (LHE) (10)\nThis is justified for three reasons: (i) The difference between X and F is not important for learning of the heterophily matrix. These are two different loss functions and we can focus only on one of them; (ii) We can use the mathematical machinery developed for LLE for our own problem setup; and (iii) This formalism explains heterophily for both LinBP and HF, as we show next. 2. Second, we propose to generalize the regularization term of HF (and analogously for LNP and LGC) as follows:\nR(F,H) = 1\n2\n∑\ni,j\nWij ||Fi: − Fj:H||2 (HHF) (11)\nHere, we write Fi: as short notation for the i-th row vector of the label matrix F. This is justified for two reasons: (i) For H = I, we get back the original formulation of HF.6 (ii) As long as each entry Fi: is stochastic, the equations make sure that all updates and final labeling functions remain stochastic. Thus, we propose in this section to learn the heterophily matrix via the following simple convex optimization problem:\nargmin H\n||X−WXH|| (LHE) (12)\nsubject to H = Hᵀ\nH 1 = 1\nNotice that we can transform the simple constraint optimization problem from Eq. 12 into an even simple unconstrained optimization problem. A k × k-dimensional doubly stochastic matrix k(k−1)\n2 degrees of freedom, thus k(k+1)\n2 constraints which we need to impose as boundary conditions. In other words, we can pose the optimization problem simply over the degrees of freedom of the matrix. For example, for a k = 3, we have 3 degrees of freedom:\nH = [ h1 h2 1−h1−h2 h2 h3 1−h2−h3\n1−h1−h2 1−h2−h3 h1+2h2+h3−1\n]\nThus we have an unconstrained optimization problem\nargmin h\n||X−W X H(h)|| (LHE) (13)\nWe found that the latter formulation is faster in practice."
    }, {
      "heading" : "5. EXPERIMENTS",
      "text" : "We evaluate our method with regard to two questions: (1) How well does our Linear Heterophily Estimation (LHE) work as compared to the baseline method? (2) How well do our combined methods scale with the size of the network?"
    }, {
      "heading" : "5.1 Quality",
      "text" : "We chose to evaluate our technique on carefully controlled synthetic data only, as this allows us to change the ground truth and evaluate the accuracy of our techniques as result of\n6HHF stands for Heterophily Harmonic Functions.\nsystematic changes to problem parameters. We repeated the experiments with various synthetic data sets, and will focus on one particular choice of parameters to illustrate the main conclusions. Figure 5h summarizes our overall experimental setup.\nData generation: W,X,X′. We assume k = 3 classes and use the heterophily matrix from the introduction: H =[ 0.1 0.8 0.1 0.8 0.1 0.1 0.1 0.1 0.8 ] . We create a graph with n = 100 nodes and assign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1\n3 , 1 3 , 1 3 ]).\nThis leads to our n× k indicator ground truth labeling matrix X. We then add for each node exactly m\n2 ∈ {1, 2, . . . , 10}\nnumber of undirected edges to other nodes (leading to m average degree). The random choice of a neighbor reflects the homophily matrix H as follows: We first randomly choose the type of neighbor to connect to (i.e., a node i of class c1 is connected to another node j of class c2 with probability H(c1, c2)), and then randomly sample a node of that class. This leads to the binary, symmetric adjacency matrix W. We then remove a random fraction p ∈ {0.1, . . . , 0.95} of the labels from X, leading to the partially labeled data X′. We varying the parameters and repeat this experiment.\nHomophily estimation: H2,H1. We use Eq. 12 to estimate H2 on the partially labeled graph with edges W and labels X′. We calculate the Frobenius norm ||H −H2|| as our estimation error. We also estimate H1 on the completely labeled graph with X. This allows us to later analyze and interpret the relative contribution of (i) more accurate estimation of H and (ii) larger fraction of labeled information, to the labeling accuracy.\nLabel propagation: F′. We transform our estimated homophily matrices H2, H1, and the original matrix H into a centered matrix around 0: Ĥ = H(H − [ 1k ]k), whereby systematically varying H . We then use Eq. 5 to propagate the labeling information from X′ throughout the graph.\nQuality assessment. For each node in the hold out set, we calculate the label with maximum belief in F′ and then evaluate labeling accuracy as the percentage of correctly retrieved labels on the hold-out set only. Notice that accuracy of 1\n3 corresponds to random assignment of labels.\nDiscussion. Overall, our method works well. For example, for m = 2 (i.e., average degree of a node is 2) and deleting 50% of the labels (p = 0.5), we have an average labeling accuracy of 0.76 for H = 0.01 (Fig. 5a). This is surprising, given that each column of the homophily matrix itself has relative high entropy H([0.8, 0.1, 0.1]) ≈ 0.92 (as compared to the random vector H([ 1\n3 , 1 3 , 1 3 ]) ≈ 1.58). In-\ncreasing the connectivity of the graph, also increases the labeling accuracy (more each unlabeled node, more edges will lead to labeled neighbors). However, it does not increase the estimation accuracy for H: The average estimation error ||H−H2|| increases with the fraction p of removed labels, but is independent of the number of edges per node m (Fig. 5e and Fig. 5f).\nThe impact of the choice of the scaling factor H for the label propagation homophily matrix Ĥ is interesting. Below a certain threshold, accuracy is constant (e.g., 0.84 for H = 0.01), but then increases slightly up to the point of divergence (e.g., 0.86 for H = 0.3 in Fig. 5g for m = 4, p = 0.5). This result is consistent across different choices of parameters. For example, in the case of m = 4, the average boundary between convergence and divergence (Eq. 7)\nis max ≈ 0.315 (the graph in Fig.5g ends at h = 0.3). This suggests that higher choices of H (meaning the information is propagated further through the graph) allow to partially compensate for missing labels. For example, an unlabeled node may have no labeled neighbor, but labeling information is still passed through those neighbor nodes. All figures, except Fig. 5g, are drawn using h = 0.01).\nComparison baseline. We also compared our approach of estimating the matrix with LHE against MHE. Figure 6a shows the result for a graph with n = 100 nodes and m = 2 edges per node as a function of the hold-out fraction. For example for f = 0.9 (i.e. only 10% of nodes are labeled) NHE stops working (we do not have enough neighbors of all classes to estimate a matrix), while LHE combined with LinBP still 55% accuracy of determining the correct labels of the remaining 90 nodes."
    }, {
      "heading" : "5.2 Scalability",
      "text" : "For the scalability experiments, we implemented our learning framework Python 2.7. We use fmin slsqp in scipy.optimize to minimize our function using Sequential Least SQuares Programming. Figure 6b shows the results for graphs with either average m = 2 or m = 10 edges per node. The time for LHE for 1M nodes and 10M edges is 31sec and LinBP around 7sec for 10 iterations."
    }, {
      "heading" : "6. RELATED WORK",
      "text" : "Our work is related to multiple existing works on semisupervised learning. We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6]. In [27] a symmetric weight matrix W is learned\nbut they assume simple homophily and do not account for more complex heterophily relations. In [22] a multi-class classification problems is considered. Though, each of the classes is treated separately and they never interact (mixing/modularization) with other classes. The works [7] and [21] integrate dissimilarity into the label learning process: one can indicate that two neighboring nodes should have different labels. Our proposed approach allows for much richer interaction between different classes. Also, [23] formulate a local learning regularizer (LL-Reg): learned to predict the label of each data point based on the neighbors and their labels.\nTo the best of our knowledge, the type of linear mixing described in this paper, by using a heterophily matrix H, has\nonly been described before in our recent work on LinBP [6]. We are not aware of any linear learning framework for heterophily from existing data. We refer to [11], [26], [4], and [1] for excellent exposition and comparison of various methods."
    }, {
      "heading" : "7. CONCLUSIONS",
      "text" : "In this paper, we proposed a novel semi-supervised learning formulation that relies on two novel components: First, it allows to use not only similarity and dissimilarity, but any type of mutual coupling strengths between different classes of nodes (we abstract this with the doubly stochastic heterophily matrix). Second, we showed how to estimate the heterophily matrix based on partially labeled data. The learned heterophily can subsequently be used to label the remaining data. We also showed how our framework generalizes a number of existing frameworks and naturally extends them from homophily to heterophily. Finally we provide experiments that illustrate the effectiveness of our method, plus detailed insights about the implications of parameters of the problem on our ability to correctly label data.\nAcknowledgements. I like to thank Christos Faloutsos for stimulating discussions, helpful comments, and for getting me excited about Linear Algebra problems in the first place (thanks , !). I also thank Stephan Günnemann for comments on an earlier version of this paper, and Jeff Schneider and Fernando de la Torre for helpful pointers."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] Y. Bengio, O. Delalleau, and N. L. Roux. Label propagation\nand quadratic criterion. In O. Chapelle, B. Schölkopf, and A. Zien, editors, Semi-supervised learning, pp. 193–216. MIT Press, 2006. [2] B. Bollobás, C. Borgs, J. Chayes, and O. Riordan. Directed scale-free graphs. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’03, pp. 132–139, Philadelphia, PA, USA, 2003. Society for Industrial and Applied Mathematics. [3] L. Breiman. Statistical modeling: The two cultures. Statistical Science, 16(3):199–215, 2001. [4] O. Chapelle, B. Schölkopf, and A. Zien. Semi-supervised learning. MIT Press, Cambridge, Mass., 2006. [5] G. Elidan, I. McGraw, and D. Koller. Residual belief propagation: Informed scheduling for asynchronous message passing. In UAI, 2006. [6] W. Gatterbauer, S. Günnemann, D. Koutra, and C. Faloutsos. Linearized and single-pass belief propagation. PVLDB, 8(5), 2015. (full version: CoRR abs/1406.7288). [7] A. B. Goldberg, X. Zhu, and S. J. Wright. Dissimilarity in graph-based semi-supervised classification. In AISTATS, pp. 155–162, 2007. [8] A. Goldenberg, A. X. Zheng, S. E. Fienberg, and E. M. Airoldi. A survey of statistical network models. Foundations and Trends in Machine Learning, 2(2):129–233, 2009. [9] P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks, 5(2):109 – 137, 1983. [10] D. Koutra, T.-Y. Ke, U. Kang, D. H. Chau, H.-K. K. Pao, and C. Faloutsos. Unifying guilt-by-association approaches: Theorems and fast algorithms. In ECML/PKDD (2), pp. 245–260, 2011. [11] Q. Lu and L. Getoor. Link-based classification. In ICML, pp. 496–503, 2003. [12] J. M. Mooij and H. J. Kappen. Sufficient conditions for convergence of the sum-product algorithm. IEEE Transactions on Information Theory, 53(12):4422–4437, 2007. [13] K. P. Murphy. Machine learning: a probabilistic perspective. Adaptive computation and machine learning series. MIT Press, Cambridge, MA, 2012. [14] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,\nM. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.\n[15] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–6, Dec 2000. [16] M. H. Schneider and S. A. Zenios. A comparative study of algorithms for matrix balancing. Oper. Res., 38(3):439–455, May 1990. [17] P. Sen and L. Getoor. Link-based classification. Technical report, University of Maryland Technical Report CS-TR-4858, February 2007. [18] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and T. Eliassi-Rad. Collective classification in network data. AI Magazine, 29(3):93–106, 2008. [19] R. Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. The Annals of Mathematical Statistics, 35(2):876–879, 06 1964. [20] L. Stockmeyer. Planar 3-colorability is polynomial complete. SIGACT News, 5(3):19–25, July 1973. [21] W. Tong and R. Jin. Semi-supervised learning by mixed label propagation. In AAAI, pp. 651–656. AAAI Press, 2007. [22] F. Wang and C. Zhang. Label propagation through linear neighborhoods. IEEE Transactions on Knowledge and Data Engineering, 20(1):55–67, 2008. [23] M. Wu and B. Schölkopf. Transductive classification via local learning regularization. In AISTATS, pp. 628–635, 2007. [24] R. Zass and A. Shashua. Doubly stochastic normalization for spectral clustering. In B. Schölkopf, J. C. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006, pp. 1569–1576. MIT Press, 2006. [25] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Schölkopf. Learning with local and global consistency. In NIPS, 2003. [26] X. Zhu. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences, University of Wisconsin-Madison, 2005. [27] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In ICML, pp. 912–919, 2003.\nAPPENDIX"
    }, {
      "heading" : "A. NOMENCLATURE",
      "text" : "n number of nodes k number of classes W n× n weighted symmetric adjacency matrix P n× n row stochastic adjacency matrix Wx,Px “fixed” variants: removed columns for explicit beliefs xi given label of node i (if labeled) fi inferred label of node i X n× k given label distribution F n× k inferred label distribution L n× n Laplacian matrix Ln n× n normalized Laplacian matrix H k×k coupling matrix with Hi,j indicating the influence\nof class i of a sender on class j of the recipient\nX̂, F̂, Ĥ residual matrices centered around 1 k Ĥo unscaled, centered coupling matrix: Ĥ = HĤo H scaling factor Ik k-dimensional identity matrix Xᵀ transpose of matrix X vec ( X ) vectorization of matrix X X⊗Y Kronecker product between matrices X and Y ρ(X) spectral radius of a matrix X Xi: i-th row vector of X"
    } ],
    "references" : [ {
      "title" : "Label propagation and quadratic criterion",
      "author" : [ "Y. Bengio", "O. Delalleau", "N.L. Roux" ],
      "venue" : "O. Chapelle, B. Schölkopf, and A. Zien, editors, Semi-supervised learning, pp. 193–216. MIT Press,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Directed scale-free graphs",
      "author" : [ "B. Bollobás", "C. Borgs", "J. Chayes", "O. Riordan" ],
      "venue" : "Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’03, pp. 132–139, Philadelphia, PA, USA,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Statistical modeling: The two cultures",
      "author" : [ "L. Breiman" ],
      "venue" : "Statistical Science, 16(3):199–215,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Semi-supervised learning",
      "author" : [ "O. Chapelle", "B. Schölkopf", "A. Zien" ],
      "venue" : "MIT Press, Cambridge, Mass.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Residual belief propagation: Informed scheduling for asynchronous message passing",
      "author" : [ "G. Elidan", "I. McGraw", "D. Koller" ],
      "venue" : "UAI,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Linearized and single-pass belief propagation",
      "author" : [ "W. Gatterbauer", "S. Günnemann", "D. Koutra", "C. Faloutsos" ],
      "venue" : "PVLDB, 8(5),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Dissimilarity in graph-based semi-supervised classification",
      "author" : [ "A.B. Goldberg", "X. Zhu", "S.J. Wright" ],
      "venue" : "AISTATS, pp. 155–162,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A survey of statistical network models",
      "author" : [ "A. Goldenberg", "A.X. Zheng", "S.E. Fienberg", "E.M. Airoldi" ],
      "venue" : "Foundations and Trends in Machine Learning, 2(2):129–233,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Stochastic blockmodels: First steps",
      "author" : [ "P.W. Holland", "K.B. Laskey", "S. Leinhardt" ],
      "venue" : "Social Networks, 5(2):109 – 137,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Unifying guilt-by-association approaches: Theorems and fast algorithms",
      "author" : [ "D. Koutra", "T.-Y. Ke", "U. Kang", "D.H. Chau", "H.-K.K. Pao", "C. Faloutsos" ],
      "venue" : "ECML/PKDD (2), pp. 245–260,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Link-based classification",
      "author" : [ "Q. Lu", "L. Getoor" ],
      "venue" : "ICML, pp. 496–503,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Sufficient conditions for convergence of the sum-product algorithm",
      "author" : [ "J.M. Mooij", "H.J. Kappen" ],
      "venue" : "IEEE Transactions on Information Theory, 53(12):4422–4437,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Machine learning: a probabilistic perspective",
      "author" : [ "K.P. Murphy" ],
      "venue" : "Adaptive computation and machine learning series. MIT Press, Cambridge, MA,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Scikit-learn: Machine learning in Python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Nonlinear dimensionality reduction by locally linear embedding",
      "author" : [ "S.T. Roweis", "L.K. Saul" ],
      "venue" : "Science, 290(5500):2323–6, Dec",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A comparative study of algorithms for matrix balancing",
      "author" : [ "M.H. Schneider", "S.A. Zenios" ],
      "venue" : "Oper. Res., 38(3):439–455, May",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Link-based classification",
      "author" : [ "P. Sen", "L. Getoor" ],
      "venue" : "Technical report, University of Maryland Technical Report CS-TR-4858, February",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Collective classification in network data",
      "author" : [ "P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad" ],
      "venue" : "AI Magazine, 29(3):93–106,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A relationship between arbitrary positive matrices and doubly stochastic matrices",
      "author" : [ "R. Sinkhorn" ],
      "venue" : "The Annals of Mathematical Statistics, 35(2):876–879, 06",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "Planar 3-colorability is polynomial complete",
      "author" : [ "L. Stockmeyer" ],
      "venue" : "SIGACT News, 5(3):19–25, July",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Semi-supervised learning by mixed label propagation",
      "author" : [ "W. Tong", "R. Jin" ],
      "venue" : "AAAI, pp. 651–656. AAAI Press,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Label propagation through linear neighborhoods",
      "author" : [ "F. Wang", "C. Zhang" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 20(1):55–67,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Transductive classification via local learning regularization",
      "author" : [ "M. Wu", "B. Schölkopf" ],
      "venue" : "AISTATS, pp. 628–635,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Doubly stochastic normalization for spectral clustering",
      "author" : [ "R. Zass", "A. Shashua" ],
      "venue" : "B. Schölkopf, J. C. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006, pp. 1569–1576. MIT Press,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning with local and global consistency",
      "author" : [ "D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Schölkopf" ],
      "venue" : "NIPS,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Semi-supervised learning literature survey",
      "author" : [ "X. Zhu" ],
      "venue" : "Technical Report 1530, Computer Sciences, University of Wisconsin-Madison,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Semi-supervised learning using gaussian fields and harmonic functions",
      "author" : [ "X. Zhu", "Z. Ghahramani", "J.D. Lafferty" ],
      "venue" : "ICML, pp. 912–919,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "INTRODUCTION Graph-based Semi-Supervised Learning (SSL) methods define a graph where the nodes are labeled and unlabeled examples in the dataset, and where (potentially weighted) edges reflect the similarity of examples [26].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 6,
      "context" : ", [7] and [21]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 20,
      "context" : ", [7] and [21]).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : ", those based on Local and Blobal Consistency (LGC) [25] or Harmonic Function methods (HF) [27]) can be generalized in a natural way as to allow to propagate heterophily from labeled to unlabeled data.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : ", those based on Local and Blobal Consistency (LGC) [25] or Harmonic Function methods (HF) [27]) can be generalized in a natural way as to allow to propagate heterophily from labeled to unlabeled data.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "In this regard, this work draws heavily upon and provides a generalization of our recent work on linearizing the update equations of belief propagation [6].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "H→ ↓x x′↘ [ 1 0 0 0 1 0 0 0 1 ] [ 0.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "H→ ↓x x′↘ [ 1 0 0 0 1 0 0 0 1 ] [ 0.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "H→ ↓x x′↘ [ 1 0 0 0 1 0 0 0 1 ] [ 0.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "[ 1 0 0 ] [ 1 0 0 ] [ 0.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "[ 1 0 0 ] [ 1 0 0 ] [ 0.",
      "startOffset" : 10,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : "Here, we draw an interesting connection to the Locally Linear Embedding (LLE) [15] framework.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : "We focus our discussion here only on the most important aspects of SSL and refer to [26] and [4] for two excellent surveys on SSL.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "We focus our discussion here only on the most important aspects of SSL and refer to [26] and [4] for two excellent surveys on SSL.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "We follow here the exposition of [1] with two notable restrictions: (i) we assume a symmetric graph structure W = WT; (ii) we allow relabeling of already labeled information ((fi − xi) 6= 0 possible).",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 26,
      "context" : "The harmonic function method (HF) [27] minimizes the loss function E(f) = ∑ i(fi−xi)2+ μ2 ∑ i,jWij(fi− fj) .",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "The linear neighborhood propagation (LNP) [22] is a variation in which the weights of the neighbors of a node i need to sum up to 1: ∑ iWij = 1.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 24,
      "context" : "The local and global consistency method (LGC) [25] is similar to HF except for normalizing the labeling function by the square root of the degrees of each node: E(f) = ∑ i(fi − xi) + μ2 ∑ i,jWij( fi √ di − fj √ dj ) The",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 21,
      "context" : "It is easy to extend existing label propagation algorithms to multiclass classification problems [22] by assigning with a vector to each node, where each entry represents the belief of a node in a particular labeling class.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 26,
      "context" : "Update equation Closed form Loss function Previous methods for binary case: HF [27] f ← (I + μD)−1(x + μWf) f = (I + μL)−1x E(f) = ||f − x||2 + μfTLf LNP [22] f ← ᾱx + αWf f = ᾱ(I− αW)−1x ” ” ” ” ” ” LGC [25] f ← ᾱx + αL∗f f = ᾱ(I− αL∗)−1x E(f) = ||f − x||2 + μfTLnf FaBP [10] f ← x + 2hWf f = (I− 2hW)−1x E(f) = ||f − x− 2hWf ||2 (shown in this paper) Previous methods for multi-class case: HF F← (I + μD)−1(X + μWF) F = (I + μL)−1X E(F) = ||F−X||2 + μ 2 ∑ i,jWij ∑ k(Fik − Fjk) LNP F← ᾱX + αWF F = ᾱ(I− αW)−1X ” ” ” ” ” ” ” ” ” ” ” LGC F← ᾱX + αL∗F F = ᾱ(I− αL∗)−1X E(F) = ||F−X||2 + μ2 ∑ i,jWij ∑ k( Fik √ di − Fjk √ dj )2 LinBP [6] F← X + WFH vec ( F ) = (I−H⊗W)−1vec ( X ) E(F) = ||F−X−WFH||2 (shown in this paper)",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "Update equation Closed form Loss function Previous methods for binary case: HF [27] f ← (I + μD)−1(x + μWf) f = (I + μL)−1x E(f) = ||f − x||2 + μfTLf LNP [22] f ← ᾱx + αWf f = ᾱ(I− αW)−1x ” ” ” ” ” ” LGC [25] f ← ᾱx + αL∗f f = ᾱ(I− αL∗)−1x E(f) = ||f − x||2 + μfTLnf FaBP [10] f ← x + 2hWf f = (I− 2hW)−1x E(f) = ||f − x− 2hWf ||2 (shown in this paper) Previous methods for multi-class case: HF F← (I + μD)−1(X + μWF) F = (I + μL)−1X E(F) = ||F−X||2 + μ 2 ∑ i,jWij ∑ k(Fik − Fjk) LNP F← ᾱX + αWF F = ᾱ(I− αW)−1X ” ” ” ” ” ” ” ” ” ” ” LGC F← ᾱX + αL∗F F = ᾱ(I− αL∗)−1X E(F) = ||F−X||2 + μ2 ∑ i,jWij ∑ k( Fik √ di − Fjk √ dj )2 LinBP [6] F← X + WFH vec ( F ) = (I−H⊗W)−1vec ( X ) E(F) = ||F−X−WFH||2 (shown in this paper)",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 24,
      "context" : "Update equation Closed form Loss function Previous methods for binary case: HF [27] f ← (I + μD)−1(x + μWf) f = (I + μL)−1x E(f) = ||f − x||2 + μfTLf LNP [22] f ← ᾱx + αWf f = ᾱ(I− αW)−1x ” ” ” ” ” ” LGC [25] f ← ᾱx + αL∗f f = ᾱ(I− αL∗)−1x E(f) = ||f − x||2 + μfTLnf FaBP [10] f ← x + 2hWf f = (I− 2hW)−1x E(f) = ||f − x− 2hWf ||2 (shown in this paper) Previous methods for multi-class case: HF F← (I + μD)−1(X + μWF) F = (I + μL)−1X E(F) = ||F−X||2 + μ 2 ∑ i,jWij ∑ k(Fik − Fjk) LNP F← ᾱX + αWF F = ᾱ(I− αW)−1X ” ” ” ” ” ” ” ” ” ” ” LGC F← ᾱX + αL∗F F = ᾱ(I− αL∗)−1X E(F) = ||F−X||2 + μ2 ∑ i,jWij ∑ k( Fik √ di − Fjk √ dj )2 LinBP [6] F← X + WFH vec ( F ) = (I−H⊗W)−1vec ( X ) E(F) = ||F−X−WFH||2 (shown in this paper)",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 9,
      "context" : "Update equation Closed form Loss function Previous methods for binary case: HF [27] f ← (I + μD)−1(x + μWf) f = (I + μL)−1x E(f) = ||f − x||2 + μfTLf LNP [22] f ← ᾱx + αWf f = ᾱ(I− αW)−1x ” ” ” ” ” ” LGC [25] f ← ᾱx + αL∗f f = ᾱ(I− αL∗)−1x E(f) = ||f − x||2 + μfTLnf FaBP [10] f ← x + 2hWf f = (I− 2hW)−1x E(f) = ||f − x− 2hWf ||2 (shown in this paper) Previous methods for multi-class case: HF F← (I + μD)−1(X + μWF) F = (I + μL)−1X E(F) = ||F−X||2 + μ 2 ∑ i,jWij ∑ k(Fik − Fjk) LNP F← ᾱX + αWF F = ᾱ(I− αW)−1X ” ” ” ” ” ” ” ” ” ” ” LGC F← ᾱX + αL∗F F = ᾱ(I− αL∗)−1X E(F) = ||F−X||2 + μ2 ∑ i,jWij ∑ k( Fik √ di − Fjk √ dj )2 LinBP [6] F← X + WFH vec ( F ) = (I−H⊗W)−1vec ( X ) E(F) = ||F−X−WFH||2 (shown in this paper)",
      "startOffset" : 272,
      "endOffset" : 276
    }, {
      "referenceID" : 5,
      "context" : "Update equation Closed form Loss function Previous methods for binary case: HF [27] f ← (I + μD)−1(x + μWf) f = (I + μL)−1x E(f) = ||f − x||2 + μfTLf LNP [22] f ← ᾱx + αWf f = ᾱ(I− αW)−1x ” ” ” ” ” ” LGC [25] f ← ᾱx + αL∗f f = ᾱ(I− αL∗)−1x E(f) = ||f − x||2 + μfTLnf FaBP [10] f ← x + 2hWf f = (I− 2hW)−1x E(f) = ||f − x− 2hWf ||2 (shown in this paper) Previous methods for multi-class case: HF F← (I + μD)−1(X + μWF) F = (I + μL)−1X E(F) = ||F−X||2 + μ 2 ∑ i,jWij ∑ k(Fik − Fjk) LNP F← ᾱX + αWF F = ᾱ(I− αW)−1X ” ” ” ” ” ” ” ” ” ” ” LGC F← ᾱX + αL∗F F = ᾱ(I− αL∗)−1X E(F) = ||F−X||2 + μ2 ∑ i,jWij ∑ k( Fik √ di − Fjk √ dj )2 LinBP [6] F← X + WFH vec ( F ) = (I−H⊗W)−1vec ( X ) E(F) = ||F−X−WFH||2 (shown in this paper)",
      "startOffset" : 632,
      "endOffset" : 635
    }, {
      "referenceID" : 0,
      "context" : "Figure 3: HF: harmonic function method (according to [1]), LNP: linear neighborhood propagation, LGC: local and global consistency method, FaBP: fast belief propagation.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "2 Locally Linear Embedding (LLE) Locally linear embedding (LLE) [15] is a method to derive compact representations of high-dimensional data by building a linear relationship among neighboring points.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "We follow here exactly the exposition in the original paper [11] while using our notation.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "3 Linearized Belief Propagation (LinBP) Another widely used methods for semi-supervised reasoning in networked data is Belief Propagation (BP) [18].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 17,
      "context" : "For graphs with loops, however, BP has well-known convergence problems (see [18] for a detailed discussion from a practitioner’s point of view).",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "While there is a lot of work on convergence of BP [5, 12], exact criteria for convergence are not known [13, Sec.",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 11,
      "context" : "While there is a lot of work on convergence of BP [5, 12], exact criteria for convergence are not known [13, Sec.",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "22], and practical use of BP is still non-trivial [18].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "[10] proposed to linearize belief propagation for the case of two classes and proposed fast belief propagation (FaBP) as a method to propagate existing knowledge of homophily or heterophily to unlabeled data.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "In [6], we have recently solved the problem for the multiclass case and proposed Linearized Belief Propagation (LinBP) as an efficient linearization of belief propagation on pairwise Markov random fields.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "Notice that in [6], we distinguished between two versions of linearized belief propagation: LinBP and LinBP∗.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "Proposition 1 (Closed-form [6]).",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "Lemma 2 (Convergence [6]).",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 26,
      "context" : "GENERALIZING SSL FOR HETEROPHILY We illustrate here generalizing the update equations for the harmonic functions (HF) method [27] to Harmonic functions with heterophily (HFH).",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 26,
      "context" : "The harmonic function method (HF) [27] then minimizes the loss function",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "We can now use the exact same mathematics we had derived in [6] in order to determine the closed-form as follows:",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 23,
      "context" : "This problem can be solved efficiently with Algorithm 1 in [24] that finds a Frobenius-norm optimum doubly stochastic approximation to a given matrix Notice that in the case of an incompletely labeled graph, it suffices to consider only the subgraph induced by the labeled nodes.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "Then H′=[ 0 2 2 2 ].",
      "startOffset" : 8,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "Then H′=[ 0 2 2 2 ].",
      "startOffset" : 8,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "Then H′=[ 0 2 2 2 ].",
      "startOffset" : 8,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "And H ′′= [ 0 1 1 2 1 2 ] .",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "And H ′′= [ 0 1 1 2 1 2 ] .",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "And H ′′= [ 0 1 1 2 1 2 ] .",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "And H ′′= [ 0 1 1 2 1 2 ] .",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "And H ′′= [ 0 1 1 2 1 2 ] .",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "to H∗= [ 1 4 3 4 3 4 1 4 ] .",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "to H∗= [ 1 4 3 4 3 4 1 4 ] .",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "to H∗= [ 1 4 3 4 3 4 1 4 ] .",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "to H∗= [ 1 4 3 4 3 4 1 4 ] .",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "to H∗= [ 1 4 3 4 3 4 1 4 ] .",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "to H∗= [ 1 4 3 4 3 4 1 4 ] .",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "to H∗= [ 1 4 3 4 3 4 1 4 ] .",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "to H∗= [ 1 4 3 4 3 4 1 4 ] .",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : "The proof follows immediately from our proof of convergence of LinBP [6].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) ≈ 1.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) ≈ 1.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) ≈ 1.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) ≈ 1.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) ≈ 1.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) ≈ 1.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 26,
      "context" : "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 14,
      "context" : "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 9,
      "context" : "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 5,
      "context" : "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].",
      "startOffset" : 282,
      "endOffset" : 285
    }, {
      "referenceID" : 26,
      "context" : "In [27] a symmetric weight matrix W is learned 0 0.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "In [22] a multi-class classification problems is considered.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "The works [7] and [21] integrate dissimilarity into the label learning process: one can indicate that two neighboring nodes should have different labels.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 20,
      "context" : "The works [7] and [21] integrate dissimilarity into the label learning process: one can indicate that two neighboring nodes should have different labels.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 22,
      "context" : "Also, [23] formulate a local learning regularizer (LL-Reg): learned to predict the label of each data point based on the neighbors and their labels.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 5,
      "context" : "only been described before in our recent work on LinBP [6].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "We refer to [11], [26], [4], and [1] for excellent exposition and comparison of various methods.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 25,
      "context" : "We refer to [11], [26], [4], and [1] for excellent exposition and comparison of various methods.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "We refer to [11], [26], [4], and [1] for excellent exposition and comparison of various methods.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "We refer to [11], [26], [4], and [1] for excellent exposition and comparison of various methods.",
      "startOffset" : 33,
      "endOffset" : 36
    } ],
    "year" : 2017,
    "abstractText" : "We propose a novel linear semi-supervised learning formulation that is derived from a solid probabilistic framework: belief propagation. We show that our formulation generalizes a number of label propagation algorithms described in the literature by allowing them to propagate generalized assumptions about influences between classes of neighboring nodes. We call this formulation Semi-Supervised Learning with Heterophily (SSL-H). We also show how the affinity matrix can be learned from observed data with a simple convex optimization framework that is inspired by locally linear embedding. We call this approach Linear Heterophily Estimation (LHE). Experiments on synthetic data show that both approaches combined can learn heterophily of a graph with 1M nodes, 10M edges and few labels in under 1min, and give better labeling accuracies than a baseline method in the case of small fraction of explicitly labeled nodes.",
    "creator" : "LaTeX with hyperref package"
  }
}