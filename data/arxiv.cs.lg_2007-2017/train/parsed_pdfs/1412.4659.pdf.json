{
  "name" : "1412.4659.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Finding a sparse vector in a subspace: Linear sparsity using alternating directions",
    "authors" : [ "Qing Qu", "Ju Sun" ],
    "emails" : [ "jw2966}@columbia.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ n. In contrast, we exhibit a relatively simple\nnonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is Ω(1). To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning."
    }, {
      "heading" : "1 Introduction",
      "text" : "Suppose we are given a linear subspace S of a high-dimensional space Rp, which contains a sparse vector x0 6= 0. Given arbitrary basis of S, can we efficiently recover x0? Equivalently, provided a matrix A ∈ R(p−n)×p, can we efficiently find a nonzero sparse vector x such that Ax = 0? In the language of sparse approximation, can we solve\nmin x ‖x‖0 s.t. Ax = 0, x 6= 0 ? (1.1)\nVariants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony’s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].\nHowever, in contrast to the standard sparse regression problem (Ax = b, b 6= 0), for which convex relaxations perform nearly optimally for broad classes of designs A [CT05, Don06], the computational properties of problem (1.1) are not nearly as well understood. It has been known for several decades that the basic formulation\nmin x ‖x‖0 , s.t. x ∈ S \\ {0}, (1.2)\nis NP-hard [CP86]. However, it is only recently that efficient computational surrogates with nontrivial recovery guarantees have been discovered for some practical cases of interest. In the context of sparse dictionary learning, Spielman et al. [SWW12] introduced a relaxation which replaces the nonconvex problem (1.2) with a sequence of linear programs:\n`1/`∞ Relaxation: min x ‖x‖1 , s.t. xi = 1, x ∈ S, 1 ≤ i ≤ p, (1.3)\nar X\niv :1\n41 2.\n46 59\nv1 [\ncs .I\nT ]\n1 5\nD ec\n2 01\nand proved that when S is generated as a span of n random sparse vectors, with high probability the relaxation recovers these vectors, provided the probability of an entry being nonzero is at most θ ∈ O (1/√n). In a planted sparse model, in which S consists of a single sparse vector x0 embedded in a “generic” subspace, Hand and Demanent proved that (1.3) also correctly recovers x0, provided the fraction of nonzeros in x0 scales as θ ∈ O (1/√n) [HD13]. Unfortunately, the results of [SWW12, HD13] are essentially sharp: when θ substantially exceeds 1/ √ n, in both models the relaxation (1.3) provably breaks down. Moreover, the most natural semidefinite programming (SDP) relaxation of (1.1),\nmin X ‖X‖1 , s.t.\n〈 A>A,X 〉 = 0, trace[X] = 1, X 0. (1.4)\nalso breaks down at exactly the same threshold of θ ∼ 1/√n.1 One might naturally conjecture that this 1/ √ n threshold is simply an intrinsic price we must pay for having an efficient algorithm, even in these random models. Some evidence towards this conjecture might be borrowed from the superficial similarity of (1.2)-(1.4) and sparse PCA [ZHT06]. In sparse PCA, there is a substantial gap between what can be achieved with efficient algorithms and the information theoretic optimum [BR13]. Is this also the case for recovering a sparse vector in a subspace? Is θ ∈ O (1/√n) simply the best we can do with efficient, guaranteed algorithms?\nRemarkably, this is not the case. Recently, Barak et al. introduced a new rounding technique for sum-ofsquares relaxations, and showed that the sparse vector x0 in the planted sparse model can be recovered when p ≥ Ω ( n2 ) and θ = Ω(1) [BKS13]. It is perhaps surprising that this is possible at all with a polynomial time algorithm. Unfortunately, the runtime of this approach is a high-degree polynomial in p, and so for machine learning problems in which p is either a feature dimension or sample size, this algorithm is of theoretical interest only. However, it raises an interesting algorithmic question: Is there a practical algorithm that provably recovers a sparse vector with θ 1/√n portion of nonzeros from a generic subspace S?\nIn this paper, we address this problem, under the following hypotheses: we assume the planted sparse model, in which a target sparse vector x0 is embedded in an otherwise random n-dimensional subspace ofRp. We allow x0 to have up to θ0p nonzero entries, where θ0 ∈ (0, 1) is a constant. We provide a relatively simple algorithm which, with very high probability, exactly recovers x0, provided that p ≥ Ω ( n4 log n ) , where the comparison with existing results is shown in Table 1. Our algorithm is based on alternating directions, with two special twists. First, we introduce a special data driven initialization, which seems to be important for achieving θ = Ω(1). Second, our theoretical results require a second, linear programming based rounding phase, which is similar to [SWW12]. Our core algorithm has very simple iterations, of linear complexity in the size of the data, and hence should be scalable to moderate-to-large scale problems.\nIn addition to enjoying theoretical guarantees in a regime (θ = Ω(1)) that is out of the reach of previous practical algorithms, it performs well in simulations – succeeding with p ≥ Ω (n log n). It also performs well empirically on more challenging data models, such as the dictionary learning model, in which the subspace of interest contains not one, but n target sparse vectors. Breaking the O(1/ √ n) sparsity barrier with a practical algorithm is an important open problem in the nascent literature on algorithmic guarantees for dictionary learning [AGM13, ABGM14, AAN13, AAJ+13]. We are optimistic that the techniques introduced here will be applicable in this direction.2\n1This breakdown behavior is again in sharp contrast to the standard sparse approximation problem (withb 6= 0), inwhich it is possible to handle very large fractions of nonzeros (say, θ = Ω(1/ logn), or even θ = Ω(1)) using a very simple `1 relaxation [CT05, Don06]\n2In work currently in preparation [SQW14], we show that in the dictionary learning problem, efficient algorithms based on nonconvex"
    }, {
      "heading" : "2 Problem Formulation and Global Optimality",
      "text" : "We study the problem of recovering a sparse vector x0 6= 0 (up to scale), which is an element of a known subspace S ⊂ Rp of dimension n, provided an arbitrary orthonormal basis Y ∈ Rp×n for S. Our starting point is the nonconvex formulation (1.2). Both the objective and constraint are nonconvex, and hence not easy to optimize over. We relax (1.2) by replacing the `0 norm with the `1 norm. For the constraint x 6= 0, which is necessary to avoid a trivial solution, we force x to live on the unit sphere ‖x‖2 = 1, giving\nmin x ‖x‖1 , s.t. x ∈ S, ‖x‖2 = 1. (2.1)\nThis formulation is still nonconvex, and so we should not expect to obtain an efficient algorithm that can solve it globally for general inputs S. Nevertheless, the geometry of the sphere is benign enough that for well-structured inputs it actually will be possible to give algorithms that find the global optimum.\nThe formulation (2.1) can be contrasted with (1.3), in which effectively we optimize the `1 norm subject to the constraint ‖x‖∞ = 1. Because ‖·‖∞ is polyhedral, that formulation immediately yields a sequence of linear programs. This is very convenient for computation and analysis, but suffers from the aforementioned breakdown behavior around ‖x0‖0 ∼ p/ √ n. In contrast, the sphere ‖x‖2 = 1 is a more complicated geometric constraint, but will allow much larger numbers of nonzeros in x0. Indeed, if we consider the global optimizer of a reformulation of (2.1):\nmin q∈Rn\n‖Yq‖1 , s.t. ‖q‖2 = 1, (2.2)\nwhere Y is any orthonormal basis for S, we have strong recovery guarantees as follows:\nTheorem 2.1 (`1/`2 recovery, planted sparse model). There exists a constant θ0 > 0 such that if the subspace S follows the planted sparse model S = span (x0,g1, . . . ,gn−1) ⊂ Rp, (2.3) where gi ∼i.i.d. N (0, 1pI), and x0 ∼i.i.d. 1√θpBer(θ) are all mutually independent and 1/ √ n < θ < θ0, then optimum q? to (2.2), for any orthonormal basis Y of S, produces Yq? = ξx0 for some ξ 6= 0 with high probability, provided p ≥ Ω (n log n). 3\nHence, ifwe could find the global optimizer of (2.2), we would be able to recover x0 whose number of nonzero entries is quite large – even linear in the dimension p (θ = Ω(1)). On the other hand, it is not obvious that this should be possible: (2.2) is nonconvex. In the next section, we will describe a simple heuristic algorithm for (a near approximation of) the `1/`2 problem (2.2), which guarantees to find a stationary point. More surprisingly, we will then prove that for a class of random problem instances, this algorithm, plus an auxiliary rounding technique, actually recovers the global optimum – the target sparse vector x0. The proof requires a detailed probabilistic analysis, which is sketched in Section 4.2.\nBefore continuing, it is worth noting that the formulation (2.1) is in no way novel – see, e.g., the work of [ZP01] in blind source separation for precedent. However, our algorithms and subsequent analysis are novel."
    }, {
      "heading" : "3 Algorithm based on Alternating Direction Method (ADM)",
      "text" : "To develop an algorithm for solving (2.2), it is useful to consider a slight relaxation of (2.2), in which we introduce an auxiliary variable x ≈ Yq:\nmin q,x\n1 2 ‖Yq− x‖22 + λ ‖x‖1 , s.t. ‖q‖2 = 1, (3.1)\noptimization also produce global solutions, even when θ = Ω(1). 3Note that this version is much stronger and more practical than that appearing in the conference version [QSW14].\nHere, λ > 0 is a penalty parameter. It is not difficult to see that this problem is equivalent to minimizing the HuberM-estimator over Yq. This relaxation makes it possible to apply the alternating direction method to this problem. This method starts from some initial point q(0), alternates between optimizing with respect to x and optimizing with respect to q:\nx(k+1) = arg min x\n1\n2 ∥∥∥Yq(k) − x∥∥∥2 2 + λ ‖x‖1 , (3.2)\nq(k+1) = arg min q\n1\n2 ∥∥∥Yq− x(k+1)∥∥∥2 2 s.t. ‖q‖2 = 1. (3.3)\nBoth (3.2) and (3.3) have simple closed form solutions:\nx(k+1) = Sλ[Yq (k)], q(k+1) = Y>x(k+1)∥∥Y>x(k+1)∥∥ 2 , (3.4)\nwhere Sλ [x] = sign(x) max {|x| − λ, 0} is the soft-thresholding operator. The proposed ADM algorithm is summarized in Algorithm 1.\nAlgorithm 1 Nonconvex ADM for solving (3.1)\nInput: A matrix Y ∈ Rp×n with Y>Y = I, initialization q(0), threshold parameter λ > 0. Output: The recovered sparse vector x̂0 = Yq(k) 1: for k = 0, . . . , O ( n4 log n ) do\n2: x(k+1) = Sλ[Yq (k)], 3: q(k+1) = Y >x(k+1)\n‖Y>x(k+1)‖ 2 , 4: end for\nThe algorithm is simple to state and easy to implement. However, if our goal is to recover the sparsest vector x0, some additional tricks are needed.\nInitialization. Because the problem (2.2) is nonconvex, an arbitrary or random initialization is unlikely to produce a global minimizer.4 Therefore, good initializations are critical for the proposed ADM algorithm to succeed. For this purpose, we suggest to use every normalized row of Y as initializations for q, and solve a sequence of p nonconvex programs (2.2) by the ADM algorithm.\nTo get an intuition ofwhy our initializationworks, recall the planted sparsemodel: S = span(x0,g1, . . . ,gn−1). Write Z = [x0 | g1 | · · · | gn−1] ∈ Rp×n. Suppose we take a row zi of Z, in which x0(i) is nonzero, then x0(i) = Θ ( 1/ √ θp ) . Meanwhile, the entries of g1(i), . . .gn−1(i) are allN (0, 1/p), and so have size about 1/√p. Hence, when θ is not too large, x0(i) will be somewhat bigger than most of the other entries in zi. Put another way, zi is biased towards the first standard basis vector e1.\nNow, under our probabilistic assumptions, Z is very well conditioned: Z>Z ≈ I.5 Using, e.g., GramSchmidt, we can find a basis Y for S of the form\nY = ZR, (3.5)\nwhere R is upper triangular, and R is itself well-conditioned: R ≈ I. Since the i-th row of Z is biased in the direction of e1 and R is well-conditioned, the i-th row yi is also biased in the direction of e1. Moreover, we know that the global optimizer q? should satisfy Yq? = x0. Since Ze1 = x0, we have q? = R−1e1 ≈ e1. Here, the approximation comes from R ≈ I. Hence, for this particular choice of Y, described in (3.5), the i-th row is biased in the direction of the global optimizer.\n4More precisely, in our models, random initialization doeswork, but only when the subspace dimension n is extremely low compared to the ambient dimension p.\n5This is the common heuristic that “tall random matrices are well conditioned” [Ver10].\nWhat if we are handed some other basis Y = YU, where U is an orthogonal matrix? Suppose q? is a global optimizer to (2.2) with input matrix Y, then it is easy to check that, with input matrix Y, U>q? is also a global optimizer to (2.2), which implies that our initialization is invariant to any rotation of the basis. Hence, even if we are handed an arbitrary basis for S, the i-th row is still biased in the direction of the global optimizer.\nRounding. Let q denote the output of Algorithm 1. We will prove that with our particular initialization and an appropriate choice of λ, the solution of our ADM algorithm falls within a certain radius of the globally optimal solution q? to (2.2). To recover q?, or equivalently to recover the sparse vector x0 = ξYq? for some ξ 6= 0, we solve the linear program\nmin q ‖Yq‖1 s.t. 〈r,q〉 = 1 (3.6)\nwith r = q. We will prove that if q is close enough to q?, then (3.6) exactly recovers q?, and hence x0."
    }, {
      "heading" : "4 Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Main Results",
      "text" : "In this section, we describe our main theoretical result, which shows that with high probability, the algorithm described in the previous section succeeds.\nTheorem 4.1. Suppose that S satisfies the planted sparse model, and let the columns of Y be an arbitrary orthonormal basis for the subspace S. Let y1, . . . ,yp ∈ Rn denote the (transposes of) the rows of Y. Apply Algorithm 1 with λ = 1/ √ p, using initializations q(0) = y1, . . . ,yp, to produce outputs q1, . . . ,qp. Solve the linear program (3.6) with r = q1, . . . ,qp, to produce q̂1, . . . , q̂p. Set i? ∈ arg mini ‖Yq̂i‖0. Then\nYq̂i? = γx0, (4.1)\nfor some γ 6= 0 with overwhelming probability, provided\nexp (n/2) /2 ≥ p ≥ Cn4 log n, and 1√ n ≤ θ ≤ θ0. (4.2)\nHere, C and θ0 > 0 are universal constants.\nWe can see that the result in Theorem 4.1 is suboptimal compared to the global optimality result in Theorem 2.1 and Barak et al.’s result [BKS13] in sampling complexity. For successful recovery, we require p ≥ Ω ( n4 log n ) , while the global optimality and Barak et al. demand p ≥ Ω (n log n) and p ≥ Ω ( n2 ) , respectively. Aside from possible deficiencies in our current analysis, compared to Barak et al., we believe this is still the first practical and efficient method which is guaranteed to achieve θ ∼ O(1) rate. The lower bound on θ in Theorem 4.1 is mostly for convenience in the proof; in fact, the LP rounding stage of our algorithm already succeeds with high probability when θ ∈ O (1/√n)."
    }, {
      "heading" : "4.2 A Sketch of Analysis",
      "text" : "The proof of our main result requires rather detailed technical analysis of the iteration-by-iteration properties of Algorithm 1. In this section, as illustrated in Fig. 1, we briefly sketch the main ideas. Detailed proofs are deferred to the appendices.\nAs noted in Section 3, the ADM algorithm is invariant to change of basis. So, we can assume without loss of generality that we are working with the particular basis Y = ZR defined in that section. In order to further streamline the presentation, we are going to sketch the proof under the assumption that\nY = [x0 | g1 | · · · | gn−1], (4.3)\nrather than the orthogonalized version Y. When p is large Y is already nearly orthogonal, and hence Y is very close to Y. In fact, in our proof, we simply carry through the argument for Y, and then note that Y and Y are close enough that all steps of the proof still hold with Y replaced by Y. With that noted, let y1, . . . ,yp ∈ Rn denote the transposes of the rows of Y, and note that these are independent random vectors. From (3.4), we can see one step of the ADM algorithm takes the form:\nq(k+1) =\n1 p ∑p i=1 y iSλ [( q(k) )> yi ]\n∥∥∥ 1p∑pi=1 yiSλ [(q(k))> yi]∥∥∥ 2 . (4.4)\nThis is a very favorable form for analysis: if q(k) is viewed as fixed, the term in the numerator is a sum of p independent random vectors. To this end, we define a vector valued random process Q(q) on q ∈ Sn−1, via\nQ(q) = 1\np p∑ i=1 yiSλ[q >yi]. (4.5)\nWe study the behavior of the iteration (4.4) through the random process Q(q). We wish to show that with overwhelming probability in our choice of Y, q(k) converges to some small neighborhood of ±e1, so that the ADM algorithm plus the LP rounding (described in Section 3) successfully retrieves the sparse vector x0 = Ye1. Thus, we hope that in general, Q(q) is more concentrated on the first coordinate than\nq. Let us partition the vector q as q = [ q1 q2 ] , with q1 ∈ R and q2 ∈ Rn−1, and correspondingly partition\nQ(q) = [ Q1(q) Q2(q) ] , where\nQ1(q) = 1\np p∑ i=1 x0iSλ [ q>yi ] and Q2(q) = 1 p p∑ i=1 giSλ [ q>yi ] . (4.6)\nThe inner product of Q(q)/ ‖Q(q)‖2 and e1 is strictly larger than the inner product of q and e1 if and only if\n|Q1(q)| |q1| > ‖Q2(q)‖2 ‖q2‖2 . (4.7)\nIn the appendix, we show that with overwhelming probability, this inequality holds uniformly over a significant portion of the sphere, so the algorithm moves in the correct direction. To complete the proof of Theorem 4.1, we combine the following observations, provided exp (n/2) /2 ≥ p ≥ Ω ( n4 log n ) :\n1. Good initializers. With overwhelming probability, at least one of the initializers q(0) satisfies |q(0)1 | > 1\n4 √ θn .\n2. Uniform progress away from the equator. With overwhelming probability, for every q ∈ Sn−1 such that 1\n4 √ θn ≤ |q1| ≤ 3\n√ θ, the bound\n|Q1(q)| |q1| − ‖Q2(q)‖2‖q‖2 > C θ2np (4.8)\nholds, for some numerical constant C > 0. This implies that if at any iteration k of the algorithm, |q(k)1 | > 14√θn , the algorithm will eventually obtain a point q (k′), k′ > k, for which |q(k ′) 1 | > 3 √ θ, if sufficiently many iterations are allowed.\n3. No jumps away from the caps. With overwhelming probability,\n|Q1(q)|√ |Q1(q)2|+ ‖Q2(q)‖22 ≥ 2 √ θ (4.9)\nfor all q ∈ Sn−1 with |q1| > 3 √ θ.\n4. Location of stationary points. The above steps imply that, with overwhelming probability, Algorithm 1 fed with the proposed initialization scheme produces at least one stopping point q ∈ Sn−1 satisfying |q1| ≥ 2 √ θ. 5. Rounding succeeds when |r1| > 2 √ θ. With overwhelming probability, the linear programming based\nrounding (3.6) will produce ±x0, up to scale, whenever it is provided with an input r whose first coordinate has magnitude at least 2 √ θ.\nTaken together, these claims imply that from at least one of the initializers q(0), the ADM algorithm will produce an output q which is accurate enough for LP rounding to exactly return x0, up to scale. As x0 is the sparsest nonzero vector in the subspace S with overwhelming probability, it will be selected as Yqi? , and hence produced by the algorithm."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "In this section, we show the performance of the proposed ADM algorithm on both synthetic and real datasets. On the synthetic dataset, we show the phase transition of our algorithm on both the planted sparse vector and dictionary learning models; for the real dataset, we demonstrate how seeking sparse vectors can help discover interesting patterns."
    }, {
      "heading" : "5.1 Phase Transition on Synthetic Data",
      "text" : "For the planted sparse model, for each pair of (k, p), we generate the n dimensional subspace S ∈ Rp by a k sparse vector x0 with nonzero entries equal to 1 and a random Gaussian matrix G ∈ Rp×(n−1) with Gij ∼i.i.d. N (0, 1/p), so that one basis Y of the subspace S can be constructed by Y = GS ([x0,G]) U, where GS (·) denotes the Gram-Schmidt orthonormalization operator and U ∈ Rn×n is an arbitrary orthogonal matrix. We fix the relationship between n and p as p = 5n log n, and set the regularization parameter in (3.1) as λ = 1/√p. We use all the normalized rows of Y as initializations of q for the proposed ADM\nalgorithm, and run every program for 5000 iterations. We determine the recovery to be successful whenever ‖x0/ ‖x0‖2 −Yq‖2 ≤ ε for at least one of the p programs, where ε = 10−3. For each pair of (k, p), we repeat the simulation for five times.\nSecond, we consider the same dictionary learning model as in [SWW12]. Specifically, the observation is assumed to be Y = A0X0, where A0 is a square, invertible matrix, and X0 a n× p sparse matrix. Since A0 is invertible, the row space ofY is the same as that ofX0. For each pair of (k, n), we generateX0 = [x1, · · · ,xn]>, where each vector xi ∈ Rp is k-sparse with every nonzero entry following i.i.d. Gaussian distribution, and construct the observation by Y> = GS ( X>0 ) U>.We repeat the same experiment as for the planted sparse model presented above. The only difference is that here we determine the recovery to be successful as long as one sparse row of X0 is recovered by one of those p programs.\nFigure 2 shows the phase transition between the sparsity level k and p for both models. It seems clear for both problems our algorithm can work well into (even beyond) the linear sparsity regime whenever p ∼ n log n. Hence for the planted sparse model, to close the gap between our algorithm and practice is one future direction. Also, how to extend our analysis for dictionary learning is another interesting direction."
    }, {
      "heading" : "5.2 Exploratory Experiments on Faces",
      "text" : "It is well known in computer vision that appearance of convex objects only subject to illumination changes leads to image collection that can be well approximated by low-dimensional space in raw-pixel space [BJ03]. We will play with face subspaces here. First, we extract face images of one person (65 images) under different illumination conditions. Thenwe apply robust principal component analysis [CLMW11] to the data and get a low dimensional subspace of dimension 10, i.e., the basis Y ∈ R32256×10. We apply the ADM algorithm to find the sparsest element in such a subspace, by randomly selecting 10% rows as initializations for q. We judge the sparsity in a `1/`2 sense, that is, the sparsest vector x̂0 = Yq? should produce the smallest ‖Yq‖1 / ‖Yq‖2 among all results. Once some sparse vectors are found, we project the subspace onto orthogonal complement of the sparse vectors already found, and continue the seeking process in the projected subspace. Figure 3 shows the first four sparse vectors we get from the data. We can see they correspond well to different extreme illumination conditions.\nSecond, we manually select ten different persons’ faces under the normal lighting condition. Again, the dimension of the subspace is 10 and Y ∈ R32256×10. We repeat the same experiment as stated above. Figure 4 shows four sparse vectors we get from the data. Interestingly, the sparse vectors roughly correspond to differences of face images concentrated around facial parts that different people tend to differ from each other, e.g., eye bows, forehead hair, nose, etc.\nIn sum, our algorithm seems to find useful sparse vectors for potential applications, like peculiarity discovery in first setting, and locating differences in second setting. Nevertheless, the main goal of this experiment is to invite readers to think about similar pattern discovery problems that might be cast as the problem of seeking sparse vectors in a subspace. The experiment also demonstrates in a concrete way the practicality of our algorithm, both in handling data sets of realistic size and in producing meaningful results even beyond the (idealized) planted sparse model that we adopt for analysis."
    }, {
      "heading" : "6 Discussion",
      "text" : "The random models we assume for the subspace can be easily extended to other randommodels, particularly for dictionary learning. Moreover we believe the algorithm paradigm works far beyond the idealized models, as our preliminary experiments on face data have clearly shown. For the particular planted sparse model, the performance gap in terms of (p, n, θ) between the empirical simulation and our result is likely due to analysis itself. Advanced techniques to bound the empirical process, such as decoupling [DlPG99] techniques, can be deployed in place of our crude union bound to cover all iterates. On the application side, the potential of seeking sparse/structured element in a subspace seems largely unexplored, despite the cases we mentioned at the start. We hope this work can invite more application ideas.\nThis paper is part of a recent surge of research into provable and practical nonconvex approaches to estimating various types of low-dimensional structures, often in large-scale settings [CLS14, JNS13, Har13, NJS13, YCS13]. The dominant approach is to start with a clever, problem-specific initialization, and then perform a local analysis of the subsequent iterates. Our forthcoming work [SQW14] on dictionary learning takes a more geometric approach, and proves global recovery via efficient algorithms, with arbitrary initialization. The approach developed there may be applicable to the planted sparse model studied here, as well as to many other interesting nonconvex problems."
    }, {
      "heading" : "Acknowledgement",
      "text" : "JS thanks the Wei Family Private Foundation for their generous support. We thank Cun Mu, IEOR Department of Columbia University, for helpful discussion and input regarding this work. This work was partially supported by grants ONR N00014-13-1-0492, NSF 1343282, and funding from the Moore and Sloan Foundations."
    }, {
      "heading" : "A Technical Tools and Preliminaries",
      "text" : "Lemma A.1. Let ψ(x) and Ψ(x) to denote the probability density function (pdf) and the cumulative distribution function (cdf) for the standard normal distribution:\n(Standard Normal pdf) ψ(x) = 1√ 2π exp\n{ −x 2\n2\n} (A.1)\n(Standard Normal cdf) Ψ(x) = 1√ 2π ∫ x −∞ exp { − t 2 2 } dt, (A.2)\nSuppose a random variable X ∼ N (0, σ2), with the pdf fσ(x) = 1σψ ( x σ ) , then for any t2 > t1 we have∫ t2\nt1\nfσ(x)dx = Ψ ( t2 σ ) −Ψ ( t1 σ ) , (A.3)∫ t2\nt1\nxfσ(x)dx = −σ [ ψ ( t2 σ ) − ψ ( t1 σ )] , (A.4)∫ t2\nt1\nx2fσ(x)dx = σ 2 [ Ψ ( t2 σ ) −Ψ ( t1 σ )] − σ [ t2ψ ( t2 σ ) − t1ψ ( t1 σ )] . (A.5)\nLemma A.2 (Taylor Expansion of Standard Gaussian cdf and pdf ). Assume ψ(x) and Ψ(x) be defined as above. There exists some universal constant Cψ > 0 such that\n|ψ(x)− [ψ(x0)− x0ψ (x0) (x− x0)]| ≤ Cψ(x− x0)2, (A.6) |Ψ(x)− [Ψ(x0) + ψ(x0)(x− x0)]| ≤ Cψ(x− x0)2. (A.7)\nLemma A.3 (Matrix Induced Norms). For any matrix A ∈ Rp×n, the induced matrix norm from `p → `q is defined as\n‖A‖`p→`q . = sup ‖x‖p=1 ‖Ax‖q . (A.8)\nIn particular, we have\n‖A‖`2→`1 = sup ‖x‖2=1 p∑ k=1 ∣∣a>k x∣∣ , ‖A‖`2→`∞ = max 1≤k≤p ∥∥ak∥∥ 2 , (A.9) ‖AB‖`p→`r ≤ ‖A‖`q→`r ‖B‖`p→`q , (A.10)\nand A and B are any matrices of compatible size.\nLemma A.4 (Moments of the Gaussian Random Variable). If X ∼ N ( 0, σ2X ) , then it holds for all integerm ≥ 1 that\nE [|X|m] = σmX (m− 1)!! [√ 2\nπ 1m=2k+1 + 1m=2k\n] ≤ σmX (m− 1)!!, k = bm/2c. (A.11)\nLemma A.5 (Moments of the χ Random Variable). If X ∼ χ (n), i.e., X ≡d ‖x‖26 for x ∼ N (0, I). Then it holds for all integerm ≥ 1 that\nE [Xm] = 2m/2 Γ (m/2 + n/2)\nΓ (n/2) ≤ m!nm/2 (A.12)\nLemma A.6 (Moments of the χ2 Random Variable). If X ∼ χ2 (n), i.e., X ≡d ‖x‖22 for x ∼ N (0, I). Then it holds for all integerm ≥ 1 that\nE [Xm] = 2m Γ (m+ n/2)\nΓ (n/2) = m∏ k=1 (n+ 2k − 2) ≤ m! 2 (2n)m. (A.13)\nLemma A.7 (Moment-Control Bernstein’s Inequality for RandomVariables). LetX1, . . . , Xp be i.i.d. real-valued random variables. Suppose that there exist some positive number R and σ2X such that\nE [|Xk|m] ≤ m!\n2 σ2XR\nm−2, for all integersm ≥ 2. (A.14)\nLet S .= 1p ∑p k=1Xk, then for all t > 0, it holds that\nP [|S − E [S]| ≥ t] ≤ 2 exp ( − pt 2\n2σ2X + 2Rt\n) . (A.15)\nLemma A.8 (Moment-Control Bernstein’s Inequality for RandomVectors). Let x1, . . . ,xp ∈ Rd be i.i.d. random vectors. Suppose there exist some positive number R and σ2X such that\nE [‖xk‖m2 ] ≤ m!\n2 σ2XR\nm−2, for all integersm ≥ 2. (A.16)\nLet s = 1p ∑p k=1 sk, then for any t > 0, it holds that\nP [‖s− E [s]‖2 ≥ t] ≤ 2(d+ 1) exp ( − pt 2\n2σ2X + 2Rt\n) . (A.17)\nLemma A.9 (Hoeffding’s Inequality). Let X1, · · · , Xp be independent random variables such that Xk takes its values in [ak, bk] almost surely for all 1 ≤ k ≤ p. Let S = ∑p k=1 (Xk − EXk), then for every t > 0,\nP [S ≥ t] ≤ exp ( − 2t\n2∑p k=1(bk − ak)2\n) . (A.18)\nLemma A.10 (Gaussian Concentration Inequality). Let x ∼ N (0, Ip). Let f : Rp 7→ R be anL-Lipschitz function. Then we have for all t > 0 that\nP [f(X)− Ef(X) ≥ t] ≤ exp ( − t 2\n2L2\n) . (A.19)\n6The notation ≡d means equivalent in distribution.\nLemma A.11 (Bounding Maximum Norm of Gaussian Vector Sequence). Let x1, . . . ,xp be a sequence of (not necessarily independent) standard Gaussian vectors in Rn. Then, it holds that\nP [ max i∈[p] ‖xi‖2 > √ 2 log p+ 2 √ n ] ≤ exp ( −1 2 n ) . (A.20)\nProof. Since the function ‖·‖2 is 1-Lipschitz, by Gaussian concentration inequality, for any i ∈ [p], we have\nP [ ‖xi‖2 − √ E ‖xi‖22 > t ] ≤ P [‖xi‖2 − E ‖xi‖2 > t] ≤ exp ( − t 2\n2\n) (A.21)\nfor all t > 0. Since E ‖xi‖22 = n, by a simple union bound, we obtain\nP [ max i∈[p] ‖xi‖ > √ n+ t ] ≤ exp ( − t 2 2 + log p ) (A.22)\nfor all t > 0. Taking t = √ 2 log p+ √ n and simplifying the terms gives the claimed result.\nLemma A.12 (Covering Number of a Unit Ball). Let B = {x ∈ Rn | ‖x‖2 ≤ 1} be a unit ball. For any ε ∈ (0, 1), there exists some ε cover of B w.r.t. the normal Rn metric, denoted as Nε, such that\n|Nε| ≤ ( 1 + 2\nε\n)n ≤ ( 3\nε\n)n . (A.23)\nLemma A.13 (Spectrum of Gaussian Matrices, [Ver10]). Let A ∈ Rp×n (p > n) contain i.i.d. standard normal entries. Then for every t ≥ 0, with probability at least 1− 2 exp ( −t2/2 ) , one has\n√ p−√n− t ≤ σmin(A) ≤ σmax(A) ≤ √ p+ √ n+ t. (A.24)\nLemma A.14. For any ε ∈ (0, 1), there exists a constant C (ε) > 1, such that provided n1 > C (ε)n2, the random matrix Φ ∈ Rn1×n2 ∼i.i.d. N (0, 1) obeys\n(1− ε) √ 2\nπ n1 ‖x‖2 ≤ ‖Φx‖1 ≤ (1 + ε)\n√ 2\nπ n1 ‖x‖2 for all x ∈ Rn2 , (A.25)\nwith probability at least 1− 2 exp (−c (ε)n2) for some c (ε) > 0.\nGeometrically, this lemma roughly corresponds to thewell known almost spherical section theorem [FLM77, GG84], see also [GM03]. A slight variant of this version has been proved in [Don06], borrowing ideas from [Pis99].\nProof. By homogeneity, it is enough to consider all x with unit norms. For a fixed x0 with ‖x0‖2 = 1, Φx0 ∼ N (0, I). So E ‖Φx‖1 = √ 2 πn1. By concentration of measure for Gaussian vectors,\nP [|‖Φx‖1 − E [‖Φx‖1]| > t] ≤ 2 exp ( − t 2\n2n1\n) (A.26)\nfor any t > 0. For a fixed δ ∈ (0, 1), Sn2−1 can be covered by a δ-net Nδ with cardinality #Nδ ≤ (1 + 2/δ)n2 . Now consider the event\nE .= { (1− δ) √ 2\nπ n1 ≤ ‖Φx‖1 ≤ (1 + δ)\n√ 2\nπ n1 ∀ x ∈ N1\n} . (A.27)\nA simple application of union bound yields P [Ec] ≤ 2 exp ( −δ\n2n1 π + n2 log\n( 1 + 2\nδ\n)) . (A.28)\nChoosing δ small enough such that\n(1− 3δ) (1− δ)−1 ≥ 1− ε and (1 + δ) (1− δ)−1 ≤ 1 + ε, (A.29)\nthen conditioned on E , we can conclude that (1− ε) √ 2\nπ n1 ≤ ‖Φx‖1 ≤ (1 + ε)\n√ 2\nπ n1 ∀ x ∈ Sn2−1. (A.30)\nIndeed, suppose E holds. Then it can easily be seen that any z ∈ Sn2−1 can be written as\nz = ∞∑ k=0 λkxk, with |λk| ≤ δk,xk ∈ N1 for all k. (A.31)\nHence we have\n‖Φz‖1 = ∥∥∥∥∥Φ ∞∑ k=0 λkxk ∥∥∥∥∥ 1 ≤ ∞∑ k=0 δk ‖Φxk‖1 ≤ (1 + δ) (1− δ) −1 √ 2 π n1. (A.32)\nSimilarly, ‖Φz‖1 = ∥∥∥∥∥Φ ∞∑ k=0 λkxk ∥∥∥∥∥ 1 ≥ [ 1− δ − δ (1 + δ) (1− δ)−1 ]√ 2 π n1 = (1− 3δ) (1− δ)−1 √ 2 π n1. (A.33)\nHence, the choice of δ above leads to the claimed result. To make P [Ec] small, it is enough to choose C such that\nCδ2/π > log ( 1 + 2\nδ\n) . (A.34)\nSetting C = 2 log ( 1 + 2δ ) π/δ2 completes the proof.\nLemma A.15. Suppose n1 ≤ 12 exp (n2/2). Fix ε ∈ (0, 1). Then for any ξ such that ξ2 > 2 log (1 + 2ε). The random matrix Φ ∈ Rn1×n2 ∼i.i.d. N (0, 1) obeys\n‖Φx‖∞ ≤ 1 + ξ 1− ε √ n2 ‖x‖2 for all x ∈ Rn2 , (A.35)\nwith probability at least 1− exp ( −n2 ( ξ2/2− log (1 + 2ε) )) .\nProof. Again for a fixed x0 ∈ Sn2−1, Φx0 ≡d v ∼ N (0, I). For any fixed β > 0 to be decided later,\nE [β ‖v‖∞] = E [ β max i∈[n1] |vi| ] = E [ log max i∈[n1] exp (β |v1|) ] ≤ logE [ max i∈[n1] exp (β |v1|) ]\n(A.36)\n≤ logE [ n1∑ i=1 exp (β |v1|) ] = log n1E [exp (β |v1|)] ≤ log 2n1 exp ( β2/2 ) . (A.37)\nHence\nE [‖v‖∞] ≤ log 2n1 exp\n( β2/2 ) β . (A.38)\nTaking β = √ 2 log (2n1), we obtain\nE [‖v‖∞] ≤ √ 2 log (2n1). (A.39)\nBecause the mapping v 7→ ‖v‖∞ is 1-Lipschitz, by concentration of measure for Gaussian vectors, we obtain\nP [‖Φx‖∞ − E [‖Φx‖∞] > t] ≤ exp ( − t 2\n2\n) . (A.40)\nTaking t = ξ√n2, and consider an ε-net Nε that covers Sn2−1 with cardinality |Nε| ≤ (1 + 2/ε)n2 , we have the event\nE .= {‖Φx‖∞ ≤ (1 + ξ) √ n2 ∀ x ∈ Nε} (A.41)\nholds with probability at least 1− exp ( −ξ2n2/2 + n2 log (1 + 2ε) ) . Conditioned on E , we have\nsup ‖z‖2=1 ‖Φz‖∞ ≤ sup z′∈Nε ‖Φz′‖∞ + sup ‖e‖2≤ε ‖Φe‖∞ = sup z′∈Nε ‖Φz′‖∞ + ε sup ‖e‖2=1 ‖Φe‖∞ . (A.42)\nHence we have\nsup ‖z‖2=1\n‖Φz‖∞ ≤ 1 1− ε supz′∈Nε ‖Φz′‖∞ = 1 + ξ 1− ε √ n2, (A.43)\ncompleting the proof."
    }, {
      "heading" : "B The Random Basis vs. Its Orthonormalized Version",
      "text" : "We consider Y obeying the planted sparse model:\nY = [x0 | G] ∈ Rp×n (B.1)\nwith\nx0 ∼i.i.d. 1√ θp\nBer (θ) ,G ∼i.i.d. N ( 0, 1\np\n) . (B.2)\nOne “natural/canonical” orthonormal basis for the subspace spanned by columns of Y is\nY′ = [ x0 ‖x0‖2 | Px⊥0 G ( G>Px⊥0 G )−1/2] . (B.3)\nWe alsowriteG′ .= Px⊥0 G ( G>Px⊥0 G )−1/2 for convenience. In this section, wewant to show that the intuition Y′ well approximating Y7 can be made rigorous. These results are needed when we prove Theorem 2.1 for the global optimality of the natural `2 constrained formulation (2.1), as well as when we translate the results for Y to quantitative statements about Y′ in Appendix F.4.\nFor any realization of x0, let the support (index set of nonzero elements) of x0 be I. By Hoeffding’s inequality in Lemma A.9, we have the event\nE0 .= { 1\n2 θp ≤ |I| ≤ 2θp\n} (B.4)\nholds with probability at least 1− 2 exp ( −pθ2/2 ) . Moreover, we show the following:\n7When n and p are large, Y has nearly orthonormal columns.\nLemma B.1. The bound ∣∣∣∣1− 1‖x0‖2 ∣∣∣∣ ≤ 2√25 √ n log p θ3p (B.5)\nholds with probability at least 1− 2 exp ( −pθ2/2 ) − 2 exp (−2n log p).\nProof. Because E [ ‖x0‖22 ] = 1, by Hoeffding’s inequality in Lemma A.9, we have\nP [∣∣∣‖x0‖22 − E [‖x0‖22] > t∣∣∣] = P [∣∣∣‖x0‖22 − 1∣∣∣ > t] ≤ 2 exp (−2θ2pt2) (B.6)\nfor all t > 0, which implies\nP [|‖x0‖2 − 1| (‖x0‖2 + 1) > t] ≤ 2 exp ( −2θ2pt2 ) . (B.7)\nOn the intersection with E0, ‖x0‖2 + 1 ≤ √ 2 + 1 ≤ 5/2, and setting t = √ n log p θ3p , we obtain\nP [ |‖x0‖2 − 1| > 2\n5\n√ n log p\nθ3p\n] ≤ 2 exp (−2n log p) . (B.8)\nSo we obtain that with probability at least 1− 2 exp ( −pθ2/2 ) − 2 exp (−2n log p),∣∣∣∣1− 1‖x0‖2 ∣∣∣∣ = |1− ‖x0‖2|‖x0‖2 ≤ 2 √ 2 5 √ n log p θ3p , (B.9)\nas desired. Next, let M .= ( G>Px⊥0 G )−1/2 , then G′ = GM− x0x > 0 ‖x0‖22 GM, we show the following results hold:\nLemma B.2. Provided p ≥ Cn ≥ 2 for some large enough constant C, it holds that\n‖M‖ ≤ 2, ‖M− I‖ ≤ 4 √ n\np (B.10)\nwith probability at least 1− c′ exp (−c′′n) for some positive constants c′ and c′′.\nProof. First observe that\n‖M‖ = ( σmin ( G>Px⊥0 G ))−1/2 = ( σmin ( Px⊥0 G ))−1 . (B.11)\nNow suppose B is an orthonormal basis spanning x⊥0 . Then it is not hard to see the spectrum of Px⊥0 G is the same as that of B>G ∈ R(p−1)×(n−1); in particular,\nσmin ( Px⊥0 G ) = σmin ( B>G ) . (B.12)\nSince G ∼i.i.d. N ( 0, 1p ) , and B> has orthonormal rows, B>G ∼i.i.d. N ( 0, 1p ) , we can invoke the spectrum results for Gaussian matrices in Lemma A.13 and obtain that√ p− 1 p − 2 √ n− 1 p− 1 ≤ σmin ( B>G ) ≤ σmax ( B>G ) ≤ √ p− 1 p + 2 √ n− 1 p− 1 (B.13)\nwith probability at least 1− c1 exp (−c2n) for some c1, c2 > 0. Thus, when p ≥ C1n for some large constant C1, we have\n‖M‖ = (√ p− 1 p − 2 √ n− 1 p− 1 )−1 ≤ 2, (B.14)\n‖I−M‖ = max (|σmax (M)− 1| , |σmin (M)− 1|) ≤ 2 √ n− 1 p− 1 (√ p− 1 p − 2 √ n− 1 p− 1 )−1 ≤ 4 √ n p , (B.15)\nwith probability at least 1− c1 exp (−c2n).\nLemma B.3. There exists a constant C > 0, such that when p ≥ Cn, the following\n‖Y‖`2→`1 ≤ 3 √ p, (B.16)\n‖Y′I‖`2→`1 ≤ 7 √ 2θp, (B.17)\n‖YI −Y′I‖`2→`1 ≤ 10\nθ\n√ n log p, (B.18)\n‖G−G′‖`2→`1 ≤ 8 √ n, (B.19)\n‖Y −Y′‖`2→`1 ≤ 10\nθ\n√ n log p (B.20)\nhold simultaneously with probability at least 1− c′ exp (−c′′n)− 2 exp ( −pθ2/2 ) for some positive constants c′ and c′′.\nProof. First of all, we have∥∥∥∥∥ x0x>0‖x0‖22 GM ∥∥∥∥∥ `2→`1 ≤ 1 ‖x0‖22 ‖x0‖`2→`1 ∥∥x>0 GM∥∥`2→`2 = 2‖x0‖22 ‖x0‖1\n∥∥x>0 G∥∥2 , (B.21) where in the last inequalitywe have applied the fact ‖M‖ ≤ 2 fromLemma B.2. Now x>0 G is an i.i.d. Gaussian vectors with each entry distributed as N ( 0, ‖x0‖22 p ) , where ‖x0‖22 = |I| θp . So by measure concentration inequality for Gaussian vectors, we have\n∥∥x>0 G∥∥2 ≤ 2 ‖x0‖2√np (B.22) with probability at least 1− exp (−n/2). On the intersection with E0, this implies∥∥∥∥∥ x0x>0‖x0‖22 GM ∥∥∥∥∥ `2→`1 ≤ 4 √ |I| √ n p ≤ 4 √ 2θn, (B.23)\nwith probability at least 1− exp (−n/2)− 2 exp ( −pθ2/2 ) . Moreover, when intersected with E0, Lemma A.14 implies that when p ≥ Ω (n),\n‖G‖`2→`1 ≤ √ p, ‖GI‖`2→`1 ≤ √ 2θp (B.24)\nwith probability at least 1 − c1 exp (−c2n) − 2 exp ( −pθ2/2 ) , for some positive constants c1, c2. So when\np ≥ Ω (n), ‖G−G′‖`2→`1 ≤ ‖G−GM‖`2→`1 + ∥∥∥∥∥ x0x>0‖x0‖22 GM ∥∥∥∥∥ `2→`1 ≤ ‖G‖`2→`1 ‖I−M‖+ 4 √ 2θn ≤ 8√n, (B.25)\n‖Y‖`2→`1 ≤ ‖x0‖`2→`1 + ‖G‖`2→`1 ≤ ‖x0‖1 + √ p ≤ 2 √ θp+ √ p ≤ 3√p, (B.26)\n‖G′I‖`2→`1 ≤ ‖GIM‖`2→`1 + ∥∥∥∥∥ x0x>0‖x0‖22 GM ∥∥∥∥∥ `2→`1 ≤ ‖GI‖`2→`1 ‖M‖+ 4 √ 2θn ≤ 6 √ 2θp, (B.27)\n‖GI −G′I‖`2→`1 ≤ ‖GI (I−M)‖`2→`1 + ∥∥∥∥∥ x0x>0‖x0‖22 GM ∥∥∥∥∥ `2→`1 ≤ ‖GI‖`2→`1 ‖I−M‖+ 4 √ 2θn ≤ 8 √ 2θn,\n(B.28) ‖Y′I‖`2→`1 ≤ ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ `2→`1 + ‖G′I‖`2→`1 ≤ ‖x0‖1 ‖x0‖2 + 6 √ 2θp ≤ 7 √ 2θp (B.29)\nwith probability at least 1− c3 exp (−c4n)− 2 exp ( −pθ2/2 ) for some positive constants c3, c4, where we have used the above estimates and the results in Lemma B.2. Finally, by Lemma B.1, we obtain\n‖Y −Y′‖`2→`1 ≤ ∣∣∣∣1− 1‖x0‖2 ∣∣∣∣ ‖x0‖1 + ‖G−G′‖`2→`1 ≤ 10θ √n log p, (B.30) ‖YI −Y′I‖`2→`1 ≤ ∣∣∣∣1− 1‖x0‖2 ∣∣∣∣ ‖x0‖1 + ‖GI −G′I‖`2→`1 ≤ 10θ √n log p, (B.31)\nholding with probability at least 1− c5 exp (−c6n)− 2 exp ( −pθ2/2 ) for some positive constants c5, c6.\nLemma B.4. Provided Cn ≤ p ≤ exp (n/2) /2 for some constant C > 0, the following\n‖G′‖`2→`∞ ≤ 16 √ n\nθp , (B.32)\n‖G−G′‖`2→`∞ ≤ 32n√ θp\n(B.33)\nhold simultaneously with probability at least 1− c′ exp (−c′′n)− 2 exp ( −pθ2/2 ) for some positive constants c′ and c′′.\nProof. First of all, we have∥∥∥∥∥ x0x>0‖x0‖22 GM ∥∥∥∥∥ `2→`∞ ≤ 1 ‖x0‖22 ‖x0‖`2→`∞ ∥∥x>0 GM∥∥`2→`2 = 2‖x0‖22 ‖x0‖∞ ∥∥x>0 G∥∥2 , (B.34) where at the last inequality we have applied the fact ‖M‖ ≤ 2 from Lemma B.2. Similar to the proof to Lemma B.3, we have that\n∥∥x>0 G∥∥2 ≤ 2 ‖x0‖2√n/p with probability at least 1 − exp (−n/2). So on the intersection with E0, we obtain that∥∥∥∥∥ x0x>0‖x0‖22 GM ∥∥∥∥∥ `2→`∞ ≤ 4 ‖x0‖∞‖x0‖2 √ n p ≤ 4 √ 2n√ θp (B.35)\nholds with probability at least 1−exp (−n/2)−2 exp ( −pθ2/2 ) . Now taking ξ = 2 and ε = 1/2 in LemmaA.15, we have that\n‖G‖`2→`∞ ≤ 6 √ n\np (B.36)\nwith probability at least 1− exp (−n (2− log 2)). Combining with results in Lemma B.2, we obtain\n‖G′‖`2→`∞ ≤ ‖GM‖`2→`∞ + ∥∥∥∥∥ x0x>0‖x0‖22 GM ∥∥∥∥∥ `2→`∞\n≤ ‖G‖`2→`∞ ‖M‖+ 4 √\n2n√ θp ≤ 12\n√ n\np +\n4 √\n2n√ θp ≤ 16\n√ n\nθp , (B.37) ‖G−G′‖`2→`∞ ≤ ‖G‖`2→`∞ ‖I−M‖+ ∥∥∥∥∥ x0x>0‖x0‖22 GM ∥∥∥∥∥ `2→`∞ ≤ 24n p + 4 √ 2n√ θp ≤ 32n√ θp (B.38)\nwith probability at least 1− c7 exp (−c8n)− 2 exp ( −pθ2/2 ) for some positive constants c7, c8.\nC Proof of `1/`2 Global Optimality Proof. We will first analyze a canonical version, in which the input basis is Y′:\nmin q∈Rn\n‖Y′q‖1 , s.t. ‖q‖2 = 1. (C.1)\nLet q = [q1; q2]. For any fixed support I of x0, we have\n‖Y′q‖1 = ‖Y′Iq‖1 + ‖Y′Icq‖1 ≥ |q1| ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ 1 − ‖G′Iq2‖1 + ‖G′Icq2‖1\n≥ |q1| ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ 1 − ‖GIq2‖1 − ‖(GI −G′I) q2‖1 + ‖GIcq2‖1 − ‖(GIc −G′Ic) q2‖1\n≥ |q1| ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ 1 − ‖GIq2‖1 + ‖GIcq2‖1 − ‖G−G′‖`2→`1 ‖q2‖2 . (C.2)\nBy Lemma A.14 and intersecting with E0, we have that as long as p ≥ Ω (n), there exists constant c1 > 0 such that\n‖GIq2‖1 ≤ 2θp√ p ‖q2‖2 = 2θ √ p ‖q2‖2 for all q2 ∈ Rn−1, (C.3)\n‖GIcq2‖1 ≥ 1\n2 p− 2θp√ p ‖q2‖2 = 1 2 √ p (1− 2θ) ‖q2‖2 for all q2 ∈ Rn−1, (C.4)\nhold with probability at least 1− 2 exp (−c1n2)− 2 exp ( −pθ2/2 ) . Moreover, by Lemma B.3,\n‖G−G′‖`2→`1 ≤ 8 √ n (C.5)\nholds with probability at least 1− c2 exp (−c3n)− 2 exp ( −pθ2/2 ) when p ≥ Ω (n). So we obtain that\n‖Y′q‖1 ≥ |q1| ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ 1 + ‖q2‖2 ( 1 2 √ p (1− 2θ)− 2θ√p− 8√n ) (C.6)\nholds with probability at least 1− c4 exp (−c5n)− 2 exp ( −pθ2/2 ) for some positive c4 and c5. Assuming E0, we observe ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ 1 ≤ √ |I| ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ 2 ≤ √ 2θp. (C.7)\nSo in order to minimize the objective ‖Y′q‖1 at e1 or−e1, i.e., q1 = 1, subject to the constraint q21 + ‖q2‖ 2 2 = 1,\nit suffices to have √ 2θp < 1\n2\n√ p (1− 2θ)− 2θ√p− 8√n, (C.8)\nwhich is satisfied when θ is sufficiently small. Thus there exists a universal constant θ0 > 0, such that for all 1/ √ n ≤ θ ≤ θ0, when p ≥ Ω (n), ±e1 are the global minimizers of (2.2) with probability at least 1− c4 exp (−c5n)− 2 exp ( −pθ2/2 ) , if the input basis is Y′. As θ > 1/ √ n by assumption, from (B.4), to make the above probability high, it is enough to make p ≥ Ω (n log n). Any other input basis can be written as Y′R, for some orthogonal matrix R. The program now is written as\nmin q∈Rn\n‖Y′Rq‖1 , s.t. ‖q‖2 = 1, (C.9)\nwhich is equivalent to\nmin q∈Rn\n‖Y′Rq‖1 , s.t. ‖Rq‖2 = 1, (C.10)\nwhich is obviously equivalent to the canonical program we analyze above by a simple change of variable, i.e., q . = Rq, completing the proof."
    }, {
      "heading" : "D Proof of Main Result",
      "text" : "In this appendix, we prove our main result in Theorem 4.1. In particular, we will first show that when the Y′ defined in (B.3) is the input orthonormal basis, the “initialization + ADM + LP rounding” pipeline recovers x0 under the stated technical conditions. Then we will upgrade the recovery result to all orthonormal basis by observing that all three stages are “invariant” to the input orthonormal basis Y.\nKeep the notation in Section 3, let y1, · · · ,yp be the transpose of the rows of Y, and let y′1, · · · ,y′p be the transpose of the rows of Y′. For q ∈ Sn−1, set\nQ(q) = 1\np p∑ k=1 ykSλ [ q>yk ] , (D.1)\nQ′(q) = 1\np p∑ k=1 y′kSλ [ q>y′k ] . (D.2)\nFurther, we write Q (q) = [Q1 (q) ; Q2 (q)], where Q1 (q) is the first coordinate, and define similar notations for Q′ (q). In addition, for any k = 1, · · · , p, set\nX1k(Zk) = x0kSλ [ q>yk ] = x0kSλ [x0kq1 + Zk] , (D.3)\nX2k(Zk) = g kSλ\n[ q>yk ] = gkSλ [x0kq1 + Zk] , (D.4)\nwhere Zk = q>2 gk ∼ N (0, σ2) for σ = ‖q2‖2 / √ p, and x0k denotes the k-th coordinate of x0. Hence we obviously have\nQ1 = 1\np p∑ k=1 X1k , Q2 = 1 p p∑ k=1 X2k, . (D.5)\nNext we sketch the main technical pieces for establishing the recovery results for Y′ first. All detailed proofs are deferred to later sections of the appendix. We will assume exp (n/2) /2 ≥ p ≥ Cn4 log n for some large constant C for all the subsequent claims.\n1. Good initialization. Proposition E.1 in Appendix E shows that with overwhelming probability, at least one of our p initialization vectors suggested in Section 3, say q(0)i = y′i, obeys that∣∣∣∣〈 y′i‖y′i‖2 , e1\n〉∣∣∣∣ ≥ 14√θn. (D.6) 2. Uniform progress away from the equator. By Proposition F.1 in Appendix F, there exists some constant θ0 > 0, such that for any θ ∈ ( 1√ n , θ0 ) ,\nG′(q) = |Q′1(q)| |q1| − ‖Q ′ 2(q)‖2 ‖q‖2 ≥ 1 4000θ2np\n(D.7)\nholds uniformly for all q ∈ Sn−1 in the region 1 4 √ θn ≤ |q1| ≤ 3\n√ θ with overwhelming probability.\n3. No jumps away from the cap. Proposition G.1 in Appendix G shows that for any θ ∈ (\n1√ n , θ0\n) , with\noverwhelming probability,\nQ′1(q) ‖Q′(q)‖2 ≥ 2 √ θ (D.8)\nholds for all q with |q1| ≥ 3 √ θ.\n4. Location of the stationary/stopping point. The first point above ensures that with overwhelming probability at least one starting point q(0) will satisfy ∣∣∣q(0)1 ∣∣∣ ≥ 14√θn . As shown in Appendix H, the strictly positive gap of the second point ensures that one needs to run at most O ( n4 log n ) iterations\nto first encounter an iterate q(k) such that ∣∣∣q(k)1 ∣∣∣ ≥ 3√θ. The third point suggests extra iterations will not move away from the cap area, and hence the stationary point q of the ADM algorithm will satisfy |q1| ≥ 2 √ θ. If one enforces a hard stop after O ( n4 log n ) iterations, the stopping point will similarly\nstay in the region |q1| ≥ 2 √ θ.\n5. LP Rounding succeeds. We know that in the LP rounding stage, described in Section 3, will receive a vector r = q̄ with its first coordinate |r1| ≥ 2 √ θ. Proposition I.1 in Appendix I proves that with\noverwhelming probability, the LP rounding (3.6) (operated on Y′) will output a solution q? = e1.\nIn summary, our ADM algorithm in Algorithm 1 using a smart initialization, plus an LP rounding stage (3.6), will output q? = ±e1 with overwhelming probability, or Y′q? as a nontrivial scaled version of x0.\nFor the general case when the input is an arbitrary orthonormal basis Ŷ = Y′R for a certain orthogonal matrix R, the target solution is R>e1. The following technical pieces are perfectly parallel to the above for Y′.\n• Discussion at the end of Appendix E suggests with overwhelming probability, at least one row of Ŷ provides an initial point q(0) such that ∣∣〈q(0),R>e1〉∣∣ ≥ 14√θn . • Discussion followingProposition F.1 inAppendix F suggests that for allq such that 1 4 √ θn ≤ ∣∣〈q,R>e1〉∣∣ ≤\n3 √ θ, there is a strictly positive gap, indicating steadyprogress towards a pointq(k) such that ∣∣〈q(k),R>e1〉∣∣ ≥ 3 √ θ.\n• Discussion at the end of Appendix G indicates once q satisfying ∣∣〈q,R>e1〉∣∣, the next iterate will not\nmove far away from the target: 〈 Q′ ( q; Ŷ ) ,R>e1 〉 ∥∥∥Q′ (q; Ŷ)∥∥∥\n2\n≥ 2 √ θ. (D.9)\n• Repeating the argument in Appendix H for general input Ŷ shows it is enough to run the ADM algorithm O ( n4 log n ) iterations to cross the range 1 4 √ θn ≤ ∣∣〈q,R>e1〉∣∣ ≤ 3√θ. So the above three\npoints together dictates that with the proposed initialization, with overwhelming probability, we finally obtain a point q that satisfies ∣∣〈q,R>e1〉∣∣ ≥ 2√θ, if we run at least O (n4 log n) iterations. • Since the ADM returns q satisfying\n∣∣〈q,R>e1〉∣∣ ≥ 2√θ, discussion at the end of Appendix I dictates that we will obtain q? = R>e1 as the optimizer of the rounding program, exactly the target solution.\nWe complete the proof."
    }, {
      "heading" : "E Good Initialization",
      "text" : "Proposition E.1. Let y′k for k = 1, . . . , p be the transpose of the rows of the orthonormal bases Y′ defined in (B.3). If θ > 1/ √ n and exp (n/2) /2 ≥ p ≥ Cn2 for some constant C > 0, it holds that at least one of our p initialization vectors suggested in Section 3, say q(0)i = y′i, obeys∣∣∣∣〈 y′i‖y′i‖2 , e1 〉∣∣∣∣ ≥ 14√θn, (E.1)\nwith probability at least 1− c′ exp (−c′′n) for some positive constants c′ and c′′.\nProof. Since x0 is i.i.d. Bernoulli, with probability at least 1−(1− θ)p ≥ 1−exp (−θp), at least one component of x0 is nonzero. Without loss of generality (w.l.o.g.), assume the k-th component of x0 is nonzero. Then x0k =\n1√ θp , and\n|qi1| = 1√ θp\n‖x0‖2 ‖y′i‖2 ≥\n1√ θp ‖x0‖2 ( ‖x0/ ‖x0‖2‖`2→`∞ + ‖G′‖`2→`∞ ) = 1√ θp ‖x0‖∞ + ‖x0‖2 ‖G′‖`2→`∞ . (E.2)\nWe know that with probability at least 1− exp(−pθ2/2), it holds that\n‖x0‖2 = √ |I| × 1 θp ≤ √ 2θp× 1 θp = √ 2. (E.3)\nMoreover, using Lemma B.4 , and Lemma A.15 with ε = 1/16 and ξ = 1/2, we know when p ≥ C1n for some large C1 > 0, it holds that (note that ‖M‖ can be arbitrarily close to 1 for large C1 in Lemma B.2)\n‖G′‖`2→`∞ ≤ ‖G‖`2→`∞ ‖M‖+ 4 √\n2n√ θp ≤ 9 5\n√ n\np +\n4 √ 2n√ θp ≤ 2 √ n p (E.4)\nwith probability at least 1− c1 exp (−c2n) for some positive constants c1 and c2. Therefore with probability at least 1− exp (−θp)− exp ( −θ2p ) − c1 exp (−c2n), it holds that\n|qi1| ≥ 1\n1 + √ 2θp× 2 √\nn p\n= 1\n1 + 2 √ 2 √ θn . (E.5)\nUsing the fact that θ ≥ 1/√n, we obtain |qi1| ≥ 1(1+2√2)√θn . It is sufficient to set p ≥ C2n 2 for some large enough C2 > 0 to make the probability overwhelming, as desired.\nWe will next show that for an arbitrary orthonormal basis Ŷ .= Y′R the initialization still biases towards the target solution. To see this, suppose w.l.o.g. ( y′i )> is a row of Y′ with nonzero first coordinate. We have\nshown above that with high probability ∣∣∣〈 y′i‖y′i‖2 , e1〉∣∣∣ ≥ 14√θn if Y′ is the input orthonormal basis. For Y′, as x0 = Y ′e1 = Y′RR>e1, we know q? = R>e1 is the target solution corresponding to Ŷ. Observing that∣∣∣∣∣∣∣∣ 〈 R>e1, ( e>i Ŷ )>∥∥∥∥(e>i Ŷ)>∥∥∥∥ 2 〉∣∣∣∣∣∣∣∣ = ∣∣∣∣∣∣ 〈 R>e1, R> (Y′)> ei∥∥∥R> (Y′)> ei∥∥∥ 2 〉∣∣∣∣∣∣ = ∣∣∣∣∣∣ 〈 e1, (Y′)> ei∥∥∥(Y′)> ei∥∥∥ 2 〉∣∣∣∣∣∣ = ∣∣∣∣〈e1, y′i‖y′i‖2 〉∣∣∣∣ ≥ 14√nθ , (E.6)\ncorroborating our claim."
    }, {
      "heading" : "F Lower Bounding Finite Sample Gap G′(q)",
      "text" : "We will first work with the “canonical” orthonormal basis Y′. The task is to lower bound the gap for finite samples\nG′(q) = |Q′1(q)| |q1| − ‖Q ′ 2(q)‖2 ‖q2‖2 . (F.1)\nSince we can deterministically constrain |q1| and ‖q2‖2 (e.g., 14√nθ ≤ |q1| ≤ 3 √ θ and ‖q2‖2 ≥ 14 , where the choice of 14 is arbitrary here, as we can always take a sufficiently small θ), the challenge lies in lower bounding |Q′1 (q)| and upper bounding ‖Q′2 (q)‖2, which depends on the orthonormal basis Y′. It turns out to cook up a typical expectation-concentration style argument, the unnormalized basis Y is much easier to work with than Y′. Hence our proof will follow the observation that\n|Q′1 (q)| ≥ |E [Q1 (q)]| − |Q1 (q)− E [Q1 (q)]| − |Q′1 (q)−Q1 (q)| , (F.2) ‖Q′2 (q)‖ ≤ ‖E [Q2 (q)]‖2 + ‖Q2 (q)− E [Q2 (q)]‖2 + ‖Q′2 (q)−Q2 (q)‖2 . (F.3)\nIn particular, define the set Γ = {\nq ∈ Sn−1 : 1 4 √ nθ ≤ |q1| ≤ 3 √ θ, ‖q2‖2 ≥ 14\n} :\n• Appendix F.1 shows that the expected gap is lower bounded for all q ∈ Sn−1 with |q1| ≤ 3 √ θ:\nG (q) . = |E [Q1 (q)]| |q1| − ‖E [Q2 (q)]‖2‖q2‖2 ≥ 1 50 q21 θp . (F.4)\nAs |q1| ≥ 14√nθ , we have\ninf q∈Γ |E [Q1 (q)]| |q1| − ‖E [Q2 (q)]‖2‖q2‖2 ≥ 1 800 1 θ2np . (F.5)\n• Appendix F.2, as summarized in Proposition F.9, shows that whenever exp (n) ≥ p ≥ Ω ( n4 log n ) , it\nholds with overwhelmingly probability that\nsup q∈Γ |Q1 (q)− E [Q1 (q)]| |q1| + ‖Q2 (q)− E [Q2 (q)]‖2 ‖q2‖2\n≤ 4 √ θn\n16000θ5/2n3/2p +\n4\n16000θ2np =\n1\n2000θ2np . (F.6)\n• Appendix F.4 shows that whenever exp (n/2) /2 ≥ p ≥ Ω ( n4 log n ) , it holds with overwhelmingly\nprobability that\nsup q∈Γ |Q1 (q)−Q′1 (q)| |q1| + ‖Q2 (q)−Q′2 (q)‖2 ‖q2‖2\n≤ 4 √ θn\n16000θ5/2n3/2p +\n4\n16000θ2np =\n1\n2000θ2np . (F.7)\nObserving that\ninf q∈Γ G′(q) ≥ inf q∈Γ ( |E [Q1 (q)]| |q1| − ‖E [Q2 (q)]‖2‖q2‖2 ) − sup q∈Γ ( |Q1 (q)− E [Q1 (q)]| |q1| + ‖Q2 (q)− E [Q2 (q)]‖2 ‖q2‖2 ) − sup\nq∈Γ ( |Q1 (q)−Q′1 (q)| |q1| + ‖Q2 (q)−Q′2 (q)‖2 ‖q2‖2 ) , (F.8)\nwe obtain the following: Proposition F.1. There exists some constant θ0 > 0 such that, for all θ ∈ (\n1√ n , θ0\n) , when exp (n/2) /2 ≥ p ≥\nCn4 log n for some large constant C > 0, we have\ninf q∈Γ G′(q) ≥ 1 4000θ2np , , (F.9)\nwith probability at least 1− c′ exp (−c′′n) for some positive constants c′ and c′′.\nFor the general case when the input orthonormal basis is Ŷ = Y′R with target solution q? = R>e1, a straightforward extension of the definition for the gap would be:\nG′ ( q; Ŷ = Y′R ) . = ∣∣∣〈Q′ (q; Ŷ) ,R>e1〉∣∣∣ |〈q,R>e1〉| − ∥∥∥(I−R>e1e>1 R)Q′ (q; Ŷ)∥∥∥ 2∥∥(I−R>e1e>1 R)q∥∥2 . (F.10)\nSince Q′ ( q; Ŷ ) = 1p ∑p k=1 R >ykSλ ( q>R>yk ) , we have\nRQ′ ( q; Ŷ ) = 1\np p∑ k=1 RR>y′kSλ ( q>R>y′k ) = 1 p p∑ k=1 y′kSλ [ (Rq) > y′k ] = Q′ (Rq; Y′) . (F.11)\nHence we have\nG′ ( q; Ŷ = Y′R ) = |〈Q′ (Rq; Y′) , e1〉| |〈Rq, e1〉| − ∥∥(I− e1e>1 )Q′ (Rq; Y′)∥∥2∥∥(I− e1e>1 )Rq∥∥2 . (F.12)\nTherefore, from Proposition F.1 above, we conclude that under the same technical conditions as therein,\ninf q∈Sn−1: 1\n4 √ θn ≤|〈Rq,e1〉|≤3\n√ θ G′ ( q; Ŷ ) ≥ 1 4000θ2np (F.13)\nwith overwhelmingly probability.\nF.1 Lower Bounding the Expected Gap G(q) In this section, we provide a nontrivial lower bound for the gap\nG(q) = |E [Q1(q)]| |q1| − ‖E [Q2(q)]‖2‖q2‖2 . (F.14)\nMore specifically, we show that:\nProposition F.2. There exists some numerical constant θ0 > 0, such that for all θ ∈ (0, θ0), it holds that\nG(q) ≥ 1 50 q21 θp\n(F.15)\nfor all q ∈ Sn−1 with |q1| ≤ 3 √ θ.\nBecause the estimation for the gap G(q) involves dedicated estimation for E [Q1(q)] and E [Q2(q)], we sketch the main proof in Appendix F.1.1, and leave those detailed technical calculations in the subsequent subsections.\nF.1.1 Sketch of the Proof\nW.l.o.g., we only consider the situation that q1 > 0, because the case of q1 < 0 can be similarly shown by symmetry. By (D.5), (D.3) and (D.4), we have\nE [Q1(q)] = E [ x0Sλ [ x0q1 + q > 2 g ]] , (F.16)\nE [Q2(q)] = E [ gSλ [ x0q1 + q > 2 g ]] , (F.17)\nwhere q = [ q1,q > 2 ]>, g ∼ N (0, 1pI), and x0 ∼ 1√θpBer(θ). Let us decompose g = g‖ + g⊥, (F.18)\nwith g‖ = P‖g = q2q > 2\n‖q2‖22 g, and g⊥ = (I− P‖)g. Therefore, we have\nE [Q2(q)] = E [ g‖Sλ [ x0q1 + q > 2 g‖ ]] + E [ g⊥Sλ [ x0q1 + q > 2 g‖ ]] = E [ g‖Sλ [ x0q1 + q > 2 g ]] + E [g⊥]E [ Sλ [ x0q1 + q > 2 g ]]\n= q2 ‖q2‖22 E [ q>2 gSλ [ x0q1 + q > 2 g ]] , (F.19)\nwhere we used the facts that q>2 g = q>2 g‖, g⊥ and g‖ are uncorrelated Gaussian vectors and therefore independent, and E [g⊥] = 0. Let Z . = g>q2 ∼ N (0, σ2) with σ2 = ‖q2‖22 /p, by partial evaluation of the expectations with respect to x0, we get\nE [Q1(q)] =\n√ θ p E [ Sλ [ q1√ θp + Z ]] , (F.20)\nE [Q2(q)] = θq2 ‖q2‖22 E [ ZSλ [ q1√ θp + Z ]] + (1− θ)q2 ‖q2‖22 E [ZSλ [Z]] . (F.21)\nStraightforward integration based on Lemma A.1 gives a explicit form of the expectations as follows\nE [Q1(q)] =\n√ θ\np\n{[ αΨ ( −α σ ) + βΨ ( β σ )] + σ [ ψ ( −β σ ) − ψ ( −α σ )]} , (F.22)\nE [Q2(q)] = {\n2 (1− θ) p Ψ ( −λ σ ) + θ p [ Ψ ( −α σ ) + Ψ ( β σ )]} q2, (F.23)\nwhere the scalars α and β are defined as\nα = q1√ θp + λ, β = q1√ θp − λ, (F.24)\nand ψ (t) and Ψ (t) are pdf and cdf for standard normal distribution, respectively, as defined in Lemma A.1. Plugging (F.22) and (F.23) into (F.14), by some simplifications, we obtain\nG(q) = 1\nq1\n√ θ\np\n[ αΨ ( −α σ ) + βΨ ( β σ ) − 2q1√ θp Ψ ( −λ σ )] − θ p [ Ψ ( −α σ ) + Ψ ( β σ ) − 2Ψ ( −λ σ )]\n+ σ\nq1\n√ θ\np\n[ ψ ( β\nσ\n) − ψ ( −α σ )] . (F.25)\nWith λ = 1/√p and σ2 = ‖q2‖22 /p = (1− q21)/p, we have\n−α σ = − δ + 1√ 1− q21 , β σ = δ − 1√ 1− q21 , λ σ = 1√ 1− q21 , (F.26)\nwhere δ = q1/ √ θ for q1 ≤ 3 √ θ. To proceed, it is natural to consider estimating the gap G(q) by Taylor’s\nexpansion. More specifically, we approximate Ψ ( −ασ ) and ψ ( −ασ ) around −1− δ, and approximate Ψ ( β σ ) and ψ ( β σ ) around −1 + δ. Applying the estimates for the relevant quantities established in Lemma F.3, we obtain\nG(q) ≥ 1− θ p Φ1(δ)− 1 δp Φ2(δ) + 1− θ p ψ(−1)q21 + 1 p\n( σ √ p+ θ 2 − 1 ) η2(δ)q 2 1\n+ 1\n2δp\n[ 1 + δ2 − θδ2 − σ ( 1 + δ2 )√ p ] q21η1 (δ) + σ\nδ √ p η1 (δ)−\n5CT √ θq31\np (δ + 1)\n3 , (F.27)\nwhere we define\nΦ1(δ) = Ψ(−1− δ) + Ψ(−1 + δ)− 2Ψ(−1), Φ2(δ) = Ψ(−1 + δ)−Ψ(−1− δ), (F.28) η1(δ) = ψ(−1 + δ)− ψ(−1− δ), η2(δ) = ψ(−1 + δ) + ψ(−1− δ), (F.29)\nand CT is as defined in Lemma F.3. Since 1− σ√p ≥ 0, dropping those small positive terms q 2 1\np (1− θ)ψ(−1), θq21 2p η2(δ), and ( 1 + δ2 ) ( 1− σ√p ) q21η1 (δ) / (2δp), and using the fact that δ = q1/ √ θ, we obtain\nG(q) ≥ 1− θ p Φ1(δ)− 1 δp [Φ2(δ)− σ √ pη1(δ)]− q21 p\n(1− σ√p) η2(δ)− √ θ\n2p q31η1 (δ)− C1 √ θq31 p max ( q31 θ3/2 , 1 ) ≥ 1− θ\np Φ1(δ)−\n1\nδp [Φ2(δ)− η1(δ)]− q21 p η1 (δ) δ − q 2 1 pθ 2√ 2π θ − q 2 1 θp 3θ2 2 √ 2π − q 2 1 θp\n( C1θ 2 ) , (F.30)\nfor some constant C1 > 0, where we have used q1 ≤ 3 √ θ to simplify the bounds and the fact σ√p =√\n1− q21 ≥ 1 − q21 to simplify the expression. Substituting the estimates in Lemma F.5 and use the fact δ 7→ η1 (δ) /δ is bounded, we obtain\nG (p) ≥ 1 p\n( 1\n40 − 1 2 √ 2π θ\n) δ2 − q 2 1\nθp\n( c1θ + c2θ 2 )\n(F.31)\n≥ q 2 1\nθp\n( 1\n40 − 1√ 2π θ − c1θ − c2θ2\n) (F.32)\nfor some positive constants c1 and c2. We obtain the claimed result once θ0 is made sufficiently small.\nF.1.2 Auxiliary Results Used in the Proof Lemma F.3. Let δ .= q1/ √ θ. There exists some universal constant CT > 0 such that we have the follow polynomial\napproximations hold for all q1 ∈ ( 0, 12 ) :∣∣∣∣ψ (−ασ)− [ 1− 1 2 (1 + δ)2q21 ] ψ(−1− δ)\n∣∣∣∣ ≤ CT (1 + δ)2 q41 , (F.33)∣∣∣∣ψ(βσ ) − [ 1− 1 2 (δ − 1)2q21 ] ψ(δ − 1)\n∣∣∣∣ ≤ CT (δ − 1)2 q41 , (F.34)∣∣∣∣Ψ(−ασ)− [ Ψ(−1− δ)− 1 2 ψ(−1− δ)(1 + δ)q21\n]∣∣∣∣ ≤ CT (1 + δ)2 q41 , (F.35)∣∣∣∣Ψ(βσ ) − [ Ψ(δ − 1) + 1 2 ψ(δ − 1)(δ − 1)q21\n]∣∣∣∣ ≤ CT (δ − 1)2 q41 , (F.36)∣∣∣∣Ψ(−λσ ) − [ Ψ(−1)− 1 2 ψ(−1)q21\n]∣∣∣∣ ≤ CT q41 . (F.37) Proof. First observe that for any q1 ∈ ( 0, 12 ) it holds that\n0 ≤ 1√ 1− q21\n− (\n1 + q21 2\n) ≤ q41 . (F.38)\nHence we have\n−(1 + δ) ( 1 + 1\n2 q21 + q 4 1\n) ≤ −α\nσ ≤ −(1 + δ)\n( 1 + 1\n2 q21\n) , (F.39)\n(δ − 1) ( 1 + 1\n2 q21 ) ≤ β σ ≤ (δ − 1) ( 1 + 1 2 q21 + q 4 1 ) , when δ ≥ 1\n(δ − 1) ( 1 + 1\n2 q21 + q 4 1 ) ≤ β σ ≤ (δ − 1) ( 1 + 1 2 q21 ) , when δ ≤ 1. (F.40)\nSo we have\nψ ( −(1 + δ) ( 1 + 1\n2 q21 + q 4 1\n)) ≤ ψ ( −α σ ) ≤ ψ ( −(1 + δ) ( 1 + 1 2 q21 )) . (F.41)\nBy Taylor expansion of the left and right sides of the above two-side inequality around −1− δ using Lemma A.2, we obtain ∣∣∣∣ψ (−ασ)− ψ(−1− δ)− 12(1 + δ)2q21ψ(−1− δ)\n∣∣∣∣ ≤ CT (1 + δ)2 q41 , (F.42) for some numerical constant CT > 0 sufficiently large. In the same way, we can obtain other claimed results.\nLemma F.4. For any δ ∈ [0, 3], it holds that\nΦ2(δ)− η1(δ) ≥ η1 (3) 9 δ3 ≥ 1 20 δ3. (F.43)\nProof. Let us define\nh(δ) = Φ2(δ)− η1(δ)− Cδ3 (F.44)\nfor some C > 0 to be determined later. Then it is obvious that h(0) = 0. Direct calculation shows that\nd\ndδ Φ1(δ) = η1(δ),\nd\ndδ Φ2(δ) = η2(δ),\nd\ndδ η1(δ) = η2(δ)− δη1(δ). (F.45)\nThus, to show (F.43), it is sufficient to show that h′(δ) ≥ 0 for all δ ∈ [0, 3]. By differentiating h(δ) with respect to δ and use the results in (F.45), it is sufficient to have\nh′(δ) = δη1(δ)− 3Cδ2 ≥ 0⇐⇒ η1(δ) ≥ 3Cδ (F.46)\nfor all δ ∈ [0, 3]. We obtain the claimed result by observing that δ 7→ η1 (δ) /3δ is monotonically decreasing over δ ∈ [0, 3] as justified below.\nConsider the function\np (δ) . = η1 (δ)\n3δ =\n1 3 √ 2π exp\n( −δ 2 + 1\n2\n) eδ − e−δ\nδ . (F.47)\nTo show it is monotonically decreasing, it is enough to show p′ (δ) is always nonpositive for δ ∈ (0, 3), or equivalently\ng (δ) . = ( eδ + e−δ ) δ − ( δ2 + 1 ) ( eδ − e−δ ) ≤ 0 (F.48)\nfor all δ ∈ (0, 3), which can be easily verified by noticing that g (0) = 0 and g′ (δ) ≤ 0 for all δ ≥ 0.\nLemma F.5. For any δ ∈ [0, 3], we have\n(1− θ)Φ1(δ)− 1\nδ [Φ2(δ)− η1(δ)] ≥\n( 1\n40 − 1√ 2π θ\n) δ2. (F.49)\nProof. Let us define\ng(δ) = (1− θ)Φ1(δ)− 1\nδ [Φ2(δ)− η1(δ)]− c0 (θ) δ2, (F.50)\nwhere c0 (θ) > 0 is a function of θ. Thus, by the results in (F.45) and L’Hospital’s rule, we have\nlim δ→0\nΦ2(δ)\nδ = lim δ→0 η2 (δ) = 2ψ(−1), lim δ→0\nη1(δ)\nδ = lim δ→0\n[η2(δ)− δη1(δ)] = 2ψ(−1). (F.51)\nCombined that with the fact that Φ1(0) = 0, we conclude g (0) = 0. Hence, to show (F.49), it is sufficient to show that g′(δ) ≥ 0 for all δ ∈ [0, 3]. Direct calculation using the results in (F.45) shows that\ng′(δ) = 1\nδ2 [Φ2(δ)− η1(δ)]− θη1(δ)− 2c0 (θ) δ. (F.52)\nSince η1 (δ) /δ is monotonically decreasing as shown in Lemma F.4, we have that for all δ ∈ (0, 3)\nη1 (δ) ≤ δ lim δ→0\nη (δ) δ ≤ 2√ 2π δ. (F.53)\nUsing the above bound and the main result from Lemma F.4 again, we obtain\ng′(δ) ≥ 1 20 δ − 2√ 2π θδ − 2c0δ. (F.54)\nChoosing c0 (θ) = 140 − 1√2π θ completes the proof.\nF.2 Finite Sample Concentration In the following two subsections, we estimate the deviations around the expectations EQ1 and EQ2, i.e., |Q1 − EQ1| and ‖Q2 − EQ2‖2, and show that the total deviations fit into the gapG(q)wederived inAppendix F.1. Our analysis is based on the scalar and vector Bernstein’s inequalities with moment conditions. Finally, in Appendix F.3, we uniform the bound by applying the classical discretization argument.\nF.2.1 Concentration for Q1\nLemma F.6 (Bounding |Q1 − E [Q1(q)]|). For each q ∈ Sn−1, it holds for all t > 0 that\nP [|Q1(q)− E [Q1(q)]| ≥ t] ≤ 2 exp ( − θp 3t2\n8 + 4pt\n) . (F.55)\nProof. By (D.3) and (D.5), we know that\nQ1(q) = 1\np p∑ k=1 X1k , X 1 k = x0kSλ [x0kq1 + Zk] (F.56)\nwhere Zk ∼ N ( 0, ‖q2‖22 p ) . Thus, for anym ≥ 2, by Lemma A.4, we have\nE [∣∣X1k ∣∣m] ≤ θ( 1√θp )m E [∣∣∣∣ q1√θp + Zk ∣∣∣∣m] = θ ( 1√ θp )m m∑ l=0 ( m l )( q1√ θp )l E [ |Zk|m−l\n] = θ ( 1√ θp )m m∑ l=0 ( m l )( q1√ θp )l (m− l − 1)!! (‖q2‖2√ p\n)m−l ≤ m!\n2 θ ( 1√ θp )m( q1√ θp + ‖q2‖2√ p )m ≤ m!\n2 θ\n( 2\nθp\n)m = m!\n2\n4\nθp2\n( 2\nθp\n)m−2 (F.57)\nlet σ2X = 4/(θp2) and R = 2/(θp), apply Lemma A.7, we get P [|Q1(q)− E [Q1(q)]| ≥ t] ≤ 2 exp ( − θp 3t2\n8 + 4pt\n) . (F.58)\nas desired.\nF.2.2 Concentration for Q2\nLemma F.7 (Bounding ‖Q2 − E [Q2]‖2). For each q ∈ Sn−1, it holds for all t > 0 that\nP [‖Q2(q)− E [Q2(q)]‖2 > t] ≤ 2(n+ 1) exp ( − θp 3t2\n128n+ 16 √ θnpt\n) . (F.59)\nBefore proving Lemma F.7, we record the following useful results.\nLemma F.8. For any positive integer s, l > 0, we have\nE [∥∥gk∥∥s\n2 ∣∣q>2 gk∣∣l] ≤ (l + s)!2 ‖q2‖l2 (2 √ n) s(√ p )s+l (F.60)\nIn particular, when s = l, we have\nE [∥∥gk∥∥l\n2 ∣∣q>2 gk∣∣l] ≤ l!2 ‖q2‖l2 ( 4 √ n p )l (F.61)\nProof. Let P q ‖ 2\n= q2q > 2\n‖q2‖22 and Pq⊥2 = ( I− 1‖q2‖22 q2q > 2 ) denote the projection operators onto q2 and its orthogo-\nnal complement, respectively. By Lemma A.4, we have E [∥∥gk∥∥s\n2 ∣∣q>2 gk∣∣l] ≤ E [(∥∥∥Pq‖2gk∥∥∥2 + ∥∥∥Pq⊥2 gk∥∥∥2)s ∣∣q>2 gk∣∣l] =\ns∑ i=0 ( s i ) E [∥∥∥Pq⊥2 gk∥∥∥i2 ] E [∣∣q>2 gk∣∣l ∥∥∥Pq‖2gk∥∥∥s−i2 ]\n= s∑ i=0 ( s i ) E [∥∥∥Pq⊥2 gk∥∥∥i2 ] E [∣∣q>2 gk∣∣l+s−i] 1‖q2‖s−i2\n≤ ‖q2‖l2 s∑ i=0 ( s i ) E [∥∥∥Pq⊥2 gk∥∥∥i2 ]( 1√ p )l+s−i (l + s− i− 1)!!. (F.62)\nUsing Lemma A.5 and the fact that ∥∥∥Pq⊥2 gk∥∥∥22 ≤ ∥∥gk∥∥22, we obtain\nE [∥∥gk∥∥s\n2 ∣∣q>2 gk∣∣l] ≤ ‖q2‖l2 s∑ i=0 ( s i )(√ n√ p )i i! ( 1√ p )l+s−i (l + s− i− 1)!!\n≤ ‖q2‖l2 (\n1√ p\n)l (l + s)!\n2\n(√ n√ p + 1√ p )s ≤ (l + s)!\n2 ‖q2‖l2\n(2 √ n) s(√ p )s+l . (F.63)\nNow, we are ready to prove Lemma F.7,\nProof. By (D.5) and (D.4), note that\nQ2 = 1\np p∑ k=1 X2k, X 2 k = g kSλ [x0kq1 + Zk] (F.64)\nwhere Zk = q>2 gk. Thus, for anym ≥ 2, by Lemma F.8, we have\nE [∥∥X2k∥∥m2 ] ≤ θE [∥∥gk∥∥m2 ∣∣∣∣ q1√θp + q>2 gk ∣∣∣∣m]+ (1− θ)E [∥∥gk∥∥m2 ∣∣q>2 gk∣∣m] ≤ θ\nm∑ l=0 ( m l ) E [∣∣q>2 gk∣∣l ∥∥gk∥∥m2 ] ∣∣∣∣ q1√θp ∣∣∣∣m−l + (1− θ)E [∥∥gk∥∥m2 ∣∣q>2 gk∣∣m] ≤ θ ( 2 √ n√ p )m m∑ l=0 ( m l ) (m+ l)! 2 (‖q2‖2√ p )l ∣∣∣∣ q1√θp ∣∣∣∣m−l + (1− θ)m!2 ‖q2‖m2 ( 4 √ n p\n)m ≤ θm!\n2 ( 4 √ n√ p )m(‖q2‖2√ p + q1√ θp )m + (1− θ)m! 2 ‖q2‖m2 ( 4 √ n p )m (F.65)\n≤ m! 2 ( 8 √ n√ θp )m . (F.66)\nTaking σ2X = 64n/(θp2) and R = 8 √ n/( √ θp) and using vector Bernstein’s inequality in Lemma A.8, we obtain\nP [‖Q2(q)− E [Q2(q)]‖2 ≥ t] ≤ 2(n+ 1) exp ( − θp 3t2\n128n+ 16 √ θnpt\n) , (F.67)\nas desired.\nF.3 Union Bound Proposition F.9 (Uniformizing the Bounds). Suppose that θ > 1√\nn . Given any ξ > 0, there exists some constant\nC (ξ), such that whenever exp (n) ≥ p ≥ C (ξ)n4 log n, we have\n|Q1(q)− E [Q1(q)]| ≤ 2ξ\nθ5/2n3/2p , (F.68)\n‖Q2(q)− E [Q2(q)]‖2 ≤ 2ξ\nθ2np (F.69)\nhold uniformly for all q ∈ Sn−1, with probability at least 1− c′ exp (−c′′n) for some positive constants c′ and c′′.\nProof. We apply the standard covering argument. For any ε ∈ (0, 1), by Lemma A.12, the unit hemisphere of interest can be covered by an ε-net Nε of cardinality at most (3/ε)n. For any q ∈ Sn−1, it can be written as\nq = q′ + e (F.70)\nwhere q′ ∈ Nε and ‖e‖2 ≤ ε. Let yk = [ x0k,g k ]> be a row of Y, by (D.3) and (D.5), we have\n|Q1(q)− E [Q1(q)]|\n= ∣∣∣∣∣1p p∑ k=1 { x0kSλ [〈 yk,q′ + e 〉] − E [ x0kSλ [〈 yk,q′ + e 〉]]}∣∣∣∣∣ ≤ ∣∣∣∣∣1p p∑ k=1 x0kSλ [〈 yk,q′ + e 〉] − 1 p p∑ k=1 x0kSλ [〈 yk,q′ 〉]∣∣∣∣∣+ ∣∣∣∣∣1p p∑ k=1 x0kSλ [〈 yk,q′ 〉] − E [x0Sλ [〈y,q′〉]]\n∣∣∣∣∣ + |E [x0Sλ [〈y,q′〉]]− E [x0Sλ [〈y,q′ + e〉]]| . (F.71)\nUsing Cauchy-Schwarz inequality and the fact that Sλ [·] is a nonexpansive operator, we have\n|Q1(q)− E [Q1(q)]| ≤ |Q1(q′)− E [Q1(q′)]|+ ( 1\np p∑ k=1 |x0k| ∥∥yk∥∥ 2 + E [|x0| ‖y‖2] ) ‖e‖2 (F.72)\n≤ |Q1(q′)− E [Q1(q′)]|+ 2ε 1√ θp ( 1√ θp + max k∈[p] ∥∥gk∥∥ 2 ) . (F.73)\nBy Lemma A.11 and the assumption that p ≤ exp (n), we have that maxk∈[p] ∥∥gk∥∥ 2 ≤ 4 √ n/pwith probability at least 1 − exp (−n/2). Taking t = ξθ−5/2n−3/2p−1 in Lemma F.6 and applying a union bound, setting ε = ξθ−2n−2/10 and combining with the above estimate, we obtain that\n|Q1(q)− E [Q1(q)]| ≤ ξ θ5/2n3/2p + ξ 5 1 θ2n2 5 √ n√ θp ≤ 2ξ θ5/2n3/2p\n(F.74)\nholds for all q ∈ Sn−1, with probability at least 1 − exp (−n/2) − exp ( − c1(ξ)pθ4n3 + c2 (ξ)n log n ) for some numerical constants c1 (ξ) and c2 (ξ).\nSimilarly, by (D.3) and (D.5), we have ‖Q2(q)− E [Q2(q)]‖2 = ∥∥∥∥∥1p p∑ k=1 { gkSλ [〈 yk,q′ + e 〉] − E [ gkSλ [〈 yk,q′ + e 〉]]}∥∥∥∥∥ 2\n≤ ‖Q2(q′)− E [Q2(q′)]‖2 + ( 1\np p∑ k=1 ∥∥gk∥∥ 2 ∥∥yk∥∥ 2 + E [∥∥gk∥∥ 2 ∥∥yk∥∥ 2 ]) ‖e‖2\n≤ ‖Q2(q′)− E [Q2(q′)]‖2 + 2εmax k∈[p] ∥∥gk∥∥ 2 ( 1√ θp + max k∈[p] ∥∥gk∥∥ 2 ) . (F.75)\nApplying the above estimates for maxk∈[p] ∥∥gk∥∥ 2 , and taking t = ξθ−2n−1p−1 in Lemma F.7 and applying a union bound, then setting ε = ξθ−2n−2/40, we obtain that\n‖Q2(q)− E [Q2(q)]‖2 ≤ ξ\nθ2np +\nξ\n20θ2n2 4\n√ n\np ( 1√ θp + 4 √ n p ) ≤ 2ξ θ2np\n(F.76)\nholds for all q ∈ Sn−1, with probability at least 1− exp (−n/2)− exp ( − c3(ξ)pθ3n3 + c4 (ξ)n log n ) .\nOverall, it is enough to take p ≥ Cn4 log n for some large C to make the above events to hold with overwhelming probability, as desired.\nF.4 Q′(q) approximates Q(q) Proposition F.10. Suppose θ > 1√\nn . For any ξ > 0, there exists some constant C (ξ), such that whenever\nexp (n/2) /2 ≥ p ≥ C (ξ)n4 log n, the following bounds\nsup q∈Sn−1\n|Q′1(q)−Q1(q)| ≤ ξ\nθ5/2n3/2p (F.77)\nsup q∈Sn−1\n‖Q′2(q)−Q2(q)‖2 ≤ ξ\nθ2np , (F.78)\nhold for all q ∈ Sn−1, with probability at least 1− c′ exp (−c′′n) for some positive constants c′ and c′′. Proof. First, for any q ∈ Sn−1, from D.5, we know that\n|Q1(q)−Q′1(q)|\n= ∣∣∣∣∣1p p∑ k=1 x0kSλ [ q>yk ] − 1 p p∑ k=1 x0k ‖x0‖2 Sλ [ q>y′k ]∣∣∣∣∣ ≤ ∣∣∣∣∣1p p∑ k=1 x0kSλ [ q>yk ] − 1 p p∑ k=1 x0kSλ [ q>y′k ]∣∣∣∣∣+ ∣∣∣∣∣1p p∑ k=1 x0kSλ [ q>y′k ] − 1 p p∑ k=1 x0k ‖x0‖2 Sλ [ q>y′k\n]∣∣∣∣∣ ≤ 1\np p∑ k=1 |x0k| ∣∣Sλ [q>yk]− Sλ [q>y′k]∣∣+ 1 p p∑ k=1 |x0k| ∣∣∣∣1− 1‖x0‖2 ∣∣∣∣ ∣∣Sλ [q>y′k]∣∣ . (F.79) Let I = supp(x0). Conditioned on the support, using the facts that Sλ[·] is a nonexpansive operator, we obtain\nsup q∈Sn−1\n|Q1(q)−Q′1(q)| ≤ 1\np sup\nq∈Sn−1 ∑ k∈I |x0k| ∥∥q> (yk − y′k)∥∥ 2 + ∣∣∣∣1− 1‖x0‖2 ∣∣∣∣ 1p supq∈Sn−1∑k∈I |x0k| ∣∣q>y′k∣∣ =\n1√ θp3/2\n( ‖YI −Y′I‖`2→`1 + ∣∣∣∣1− 1‖x0‖2 ∣∣∣∣ ‖Y′I‖`2→`1) . (F.80)\nBy Lemma B.1 and Lemma B.3 in Appendix B, we have the following holds\nsup q∈Sn−1 |Q1(q)−Q′1(q)| ≤ 1√ θp3/2\n( 10\nθ\n√ n log p+ 2 √ 2\n5\n√ n log p\nθ3p × 7 √ 2θp ) ≤ 16 θ3/2p3/2 √ n log p, (F.81)\nwith probability at least 1 − c1 exp (−c2n) for some positive constants c1 and c2. Since the above holds uniformly for any support pattern I, we conclude that\nsup q∈Sn−1\n|Q1(q)−Q′1(q)| ≤ 16\nθ3/2p3/2\n√ n log p (F.82)\nwith probability at least 1− c1 exp (−c2n). Now it is sufficient to let p ≥ C (ξ)n4 log n for some C (ξ) > 0 to obtain the claimed result.\nSimilarly, by Lemma B.3 and Lemma B.4 in Appendix B, we have\nsup q∈Sn−1\n‖Q2(q)−Q′2(q)‖2\n= sup q∈Sn−1 ∥∥∥∥∥1p p∑ k=1 gkSλ [ q>yk ] − 1 p p∑ k=1 g′kSλ [ q>y′k ]∥∥∥∥∥ 2\n≤ sup q∈Sn−1 ∥∥∥∥∥1p p∑ k=1 gkSλ [ q>yk ] − 1 p p∑ k=1 g′kSλ [ q>yk ]∥∥∥∥∥ 2 + ∥∥∥∥∥1p p∑ k=1 g′kSλ [ q>yk ] − 1 p p∑ k=1 g′kSλ [ q>y′k ]∥∥∥∥∥ 2\n≤1 p sup q∈Sn−1 p∑ k=1 ∥∥gk − g′k∥∥ 2 ∣∣q>yk∣∣+ 1 p sup q∈Sn−1 p∑ k=1 ∥∥g′k∥∥ 2 ∣∣q> (yk − y′k)∣∣ ≤1 p (‖G−G′‖`2→`∞ ‖Y‖`2→`1 + ‖G′‖`2→`∞ ‖Y −Y′‖`2→`1)\n≤1 p ( 32n√ θp × 3√p+ 16 √ n θp × 10 θ √ n log p ) ≤ 192n √ log p θ3/2p3/2 (F.83)\nholds conditioned on any support pattern I, with probability at least 1 − c3 exp (−c4n) for some positive constants c3 and c4, which similarly implies the bound holds uniformly, regardless of the support, with the same probability. It is sufficiently to have exp (n/2) /2 ≥ p ≥ C2 (ξ)n4 log n to obtain the claimed result.\nG Large |q1| Iterates Staying in Safe Region for Rounding Proposition G.1. There exists a constant θ0 > 0, such that for any θ ∈ ( 1√ n , θ0 ) , whenever exp (n) ≥ p ≥ Cn4 log n for some large constant C > 0, we have\n|Q′1(q)| ‖Q′(q)‖2 ≥ 2 √ θ, (G.1)\nfor all q ∈ Sn−1 satisfying |q1| > 3 √ θ, with probability at least 1− c′ exp (−c′′n) for some positive constants c′ and c′′.\nProof. For notational simplicity, w.l.o.g. we will proceed to prove assuming q1 > 0. The proof for q1 < 0 is similar by symmetry. It is equivalent to show that\n‖Q′2 (q)‖2 |Q′1 (q)| <\n√ 1\n4θ − 1, (G.2)\nwhich is implied by\nL (q) .= ‖EQ2(q)‖2 + ‖Q ′ 2(q)− EQ2(q)‖2\nEQ1 (q)− |Q′1 (q)− EQ1 (q)| <\n√ 1\n4θ − 1 (G.3)\nfor any q ∈ Sn−1 satisfying q1 > 3 √ θ. Recall from (F.22) that\nEQ1(q) =\n√ θ\np\n{[ αΨ ( −α σ ) + βΨ ( β σ )] + σ [ ψ ( β σ ) − ψ ( −α σ )]} , (G.4)\nwhere\nα = 1√ p ( q1√ θ + 1 ) , β = 1√ p ( q1√ θ − 1 ) , σ = ‖q2‖2 / √ p. (G.5)\nNoticing the fact that\nψ\n( β\nσ\n) − ψ ( −α σ ) ≥ 0, (G.6)\nΨ\n( β\nσ\n) = Ψ ( 1√\n1− q21\n( q1√ θ − 1 )) ≥ Ψ (2) ≥ 19 20 for q1 > 3 √ θ, (G.7)\nwe have\nEQ1 (q) ≥ √ θ\np { q1√ θ [ Ψ ( −α σ ) + Ψ ( β σ )] + Ψ ( −α σ ) −Ψ ( β σ )} ≥ 2 √ θ p Ψ ( β σ ) ≥ 19 10 √ θ p . (G.8)\nMoreover, from (F.23), we have ‖EQ2 (q)‖2 = ‖q2‖2 { 2 (1− θ) p Ψ ( −λ σ ) + θ p [ Ψ ( −α σ ) + Ψ ( β σ )]} (G.9)\n≤ 2 (1− θ) p Ψ (−1) + θ p [Ψ (−1) + 1] ≤ 2 p Ψ (−1) + θ p ≤ 2 5p + θ p , (G.10)\nwhere we have used the fact that −λ/σ ≤ −1 and −α/σ ≤ −1. Moreover, from results in Proposition F.9 and Proposition F.10 in Appendix F , we know that\nsup q∈Sn−1 |Q′1(q)− EQ1(q)| ≤ sup q∈Sn−1 |Q′1(q)−Q1(q)|+ sup q∈Sn−1 |Q1(q)− EQ1(q)| ≤ 1 8000θ5/2n3/2p , (G.11)\nsup q∈Sn−1 ‖Q′(q)− EQ(q)‖2 ≤ sup q∈Sn−1 ‖Q′(q)−Q(q)‖2 + sup q∈Sn−1 ‖Q(q)− EQ(q)‖2 ≤ 1 8000θ2np (G.12)\nhold with probability at least 1− c′ exp (−c′′n) for some positive constants c′ and c′′ when p ≥ Ω ( n4 log n ) . Hence, with overwhelming probability, we have\nL (q) ≤ 2 5p + θ p + 1 8000θ2np\n19 10 √ θ p − 18000θ5/2n3/2p\n≤ 3 5\n18 √ θ\n10\n≤ 1 3 √ θ <\n√ 1\n4θ − 1, (G.13)\nwhenever θ is sufficiently small. This completes the proof.\nNow, keep the notation in Appendix F for general orthonormal basis. For any current iterate q ∈ Sn−1 that is close enough to the target solution, i.e., ∣∣〈q,R>e1〉∣∣ = |〈Rq, e1〉| ≥ 3√θ, we have∣∣∣〈Q′ (q; Ŷ) ,R>e1〉∣∣∣∥∥∥Q′ (q; Ŷ)∥∥∥ 2 = ∣∣∣〈RQ′ (q; Ŷ) , e1〉∣∣∣∥∥∥RQ′ (q; Ŷ)∥∥∥ 2 = |〈Q′ (Rq; Y′) , e1〉| ‖Q′ (Rq; Y′)‖2 , (G.14)\nwhere we have applied the identity proved in (F.11). Taking Rq ∈ Sn−1 as the object of interest, by Proposition G.1, we conclude that\n|〈Q′ (Rq; Y′) , e1〉| ‖Q′ (Rq; Y′)‖2 ≥ 2 √ θ (G.15)\nwith overwhelming probability."
    }, {
      "heading" : "H Bounding Iteration Complexity",
      "text" : "Proposition H.1. There is a constant θ0 > 0, such that for any θ ∈ (\n1√ n , θ0\n) , with probability at least 1−c′ exp (−c′′n)\n(c′ and c′′ are positive constants), the ADM algorithm in Algorithm 1, with any initialization q(0) ∈ Sn−1 satisfying∣∣∣q(0)1 ∣∣∣ ≥ 14√θn , will produce some iterate q with |q̄1| > 3√θ at least once in at most O(n4 log n) iterations, provided exp (n) ≥ p ≥ Cn4 log n for some large constant C. Proof. Recall from Proposition F.1 in Appendix F, the gap\nG′(q) = |Q′1(q)| |q1| − ‖Q ′ 2(q)‖2 ‖q‖2 ≥ 1 4000θ2np\n(H.1)\nholds uniformly over q ∈ Sn−1 satisfying 1 4 √ θp ≤ |q1| ≤ 3 √ θ with probability at least 1− c1 exp (−c2n) for\npositive constants c1 and c2, provided p ≥ Ω ( n4 log n ) . The gap G′(q) implies that∣∣∣Q̃′1 (q)∣∣∣ .= |Q′1(q)|‖Q′ (q2)‖2 ≥ |q1| ‖Q ′ 2(q)‖2 ‖q‖2 ‖Q′ (q)‖2 + |q1| 4000θ2np ‖Q′ (q)‖2\n(H.2)\n⇐⇒ ∣∣∣Q̃′1 (q)∣∣∣ ≥ |q1|‖q2‖2 √ 1− ∣∣∣Q̃′1 (q)∣∣∣2 + |q1|4000θ2np ‖Q′ (q)‖2 (H.3) =⇒ ∣∣∣Q̃′1 (q)∣∣∣2 ≥ |q1|2 ( 1 + ‖q2‖22\n40002θ4n2p2 ‖Q′ (q)‖22\n) . (H.4)\nNow we know that\nsup q∈Γ ‖Q′ (q)‖2 ≤ sup q∈Γ |Q′1 (q)|+ sup q∈Γ ‖Q′2 (q)‖2 (H.5)\n= sup q∈Γ ∣∣∣∣∣1p p∑ k=1 x0kSλ [ x0kq1 + q > 2 g k ]∣∣∣∣∣+ supq∈Γ ∥∥∥∥∥1p p∑ k=1 gkSλ [ x0kq1 + q > 2 g k ]∥∥∥∥∥\n2\n(H.6)\n≤ 1 p ( sup q∈Γ p∑ k=1 |x0k| ∣∣x0kq1 + q>2 gk∣∣+ sup q∈Γ p∑ k=1 ∥∥gk∥∥ 2 ∣∣x0kq1 + q>2 gk∣∣ )\n(H.7)\n≤ 2 (\n1√ θp + sup k∈[p]\n∥∥gk∥∥ 2 ) sup q∈Γ ( |q1|√ θp + ‖q2‖2 sup k∈[p] ∥∥gk∥∥ 2 ) (H.8)\n≤ 2 (\n1√ θp + sup k∈[p]\n∥∥gk∥∥ 2 )2 . (H.9)\nFrom Lemma A.11, we know that supk∈[p] ∥∥gk∥∥ 2 ≤ √2 log p/√p + 2√n/√p with probability at least 1 −\nexp (−n/2). Then provided p ≤ exp (n) and θ ≥ 1/√n, we obtain\nsup q∈Γ ‖Q′ (q)‖2 ≤\n50n\np . (H.10)\nSo we conclude that ∣∣∣Q̃′1 (q)∣∣∣ |q1| ≥ √ 1 + 1− 9θ 40002 × 502 × θ4n4 . (H.11)\nTherefore, starting with any q ∈ Sn−1 such that |q1| ≥ 14√θn , we will need at most\nT = 2 log\n( 3 √ θ/ 1\n4 √ θn ) log ( 1 + 1−9θ40002×502×θ4n4 ) = 2 log (12θ√n) log ( 1 + 1−9θ40002×502×θ4n4 ) ≤ 2 log (12θ√n) (log 2) 1−9θ40002×502×θ4n4 ≤ Cn4 log n (H.12)\nsteps to arrive at a q ∈ Sn−1 with |q̄1| ≥ 3 √ θ for the first time, where C > 0 is a numerical constant, and we assume θ0 < 1/9 and used the fact that log (1 + x) ≥ (log 2)x for x ∈ [0, 1] to simplify the final result."
    }, {
      "heading" : "I Rounding to the Desired Solution",
      "text" : "For convenience, we will assume the notations we used in Appendix B. Then the rounding scheme can be written as\nmin q ‖Y′Rq‖1 , s.t. 〈q,q〉 = 1, (I.1)\nfor some orthogonal matrix R. We will show the rounding procedure get us to the desired solution with overwhelming probability, regardless of the particular orthonormal basis used.\nProposition I.1. Suppose the input basis is Y′ defined in (B.3) and the ADM algorithm produces q̄ ∈ Sn−1 with q1 > 2 √ θ. Then there exists some constants C, θ0 > 0, such that when p ≥ Cn2 and θ ∈ ( 1√ n , θ0 ) , the rounding procedure with r = q returns the desired solution e1 with probability at least 1 − c exp (−c′n) for some numerical constants c, c′ > 0.\nProof. The rounding program (I.1) can be written as\ninf q ‖Y′q‖1 , s.t. q1q1 + 〈q2,q2〉 = 1. (I.2)\nConsider its relaxation\ninf q ‖Y′q‖1 , s.t. q1q1 + ‖q2‖2 ‖q2‖2 ≥ 1. (I.3)\nIt is obvious that the feasible set of (I.3) contains that of (I.2). So if e1 is the unique optimal solution (UOS) of (I.3), it is also the UOS of (I.2). Let I = supp(x0), and consider a modified problem\ninf q ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ 1 |q1| − ‖G′Iq2‖1 + ‖G′Icq2‖1 , s.t. q1q1 + ‖q2‖2 ‖q2‖2 ≥ 1. (I.4)\nThe objective value of (I.4) lower bounds the objective value of (I.3), and are equal when q = e1. So if q = e1 is the UOS to (I.4), it is also UOS to (I.3), and hence UOS to (I.2) by the argument above. Now\n−‖G′Iq2‖1 + ‖G′Icq2‖1 ≥ −‖GIq2‖1 + ‖GIcq2‖1 − ‖(G−G′) q2‖1 (I.5) ≥ −‖GIq2‖1 + ‖GIcq2‖1 − ‖G−G′‖`2→`1 ‖q2‖2 . (I.6)\nWhen p ≥ Ω ( n2 ) , by Lemma A.14 and Lemma B.3, we know that\n− ‖GIq2‖1 + ‖GIcq2‖1 − ‖G−G′‖`2→`1 ‖q2‖2\n≥ −6 5\n√ 2\nπ 2θ √ p ‖q2‖2 + 24 25\n√ 2\nπ (1− 2θ)√p ‖q2‖2 − 8 √ n ‖q2‖2 . = ζ ‖q2‖2 (I.7)\nholds with probability at least 1 − c1 exp (−c2n) for some positive constants c1 and c2. Thus, we make a further relaxation of problem (I.2) by\ninf q ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ 1 |q1|+ ζ ‖q2‖2 , s.t. q1q1 + ‖q2‖2 ‖q2‖2 ≥ 1, (I.8)\nwhose objective value lower bounds that of (I.4). By similar arguments, if e1 is UOS to (I.8), it is UOS to (I.2). At the optimal solution to (I.8), notice that it is necessary to have sign(q1) = sign(q1) and q1q1 +‖q2‖2 ‖q2‖2 = 1. So (I.8) is equivalent to\ninf q ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ 1 |q1|+ ζ ‖q2‖2 , s.t. q1q1 + ‖q2‖2 ‖q2‖2 = 1. (I.9)\nwhich is further equivalent to\ninf q1 ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ 1 |q1|+ ζ 1− |q1| |q1| ‖q2‖2 , s.t. |q1| ≤ 1 |q1| . (I.10)\nNotice that the problem in (I.10) is linear in |q1| with a compact feasible set, which indicates that the optimal solution only occur at the boundary points |q1| = 0 and |q1| = 1/ |q1|. Therefore, q = e1 is the UOS of (I.10) if and only if\n1\n|q1| ∥∥∥∥ x0‖x0‖2 ∥∥∥∥ 1 < ζ ‖q2‖2 . (I.11)\nSince ∥∥∥ x0‖x0‖2 ∥∥∥1 ≤ √2θp conditioned on E0, it is sufficient to have\n√ 2θp\n2 √ θ ≤ ζ = 24 25\n√ 2\nπ\n√ p ( 1− 9\n2 θ − 25 3\n√ n\np\n) . (I.12)\nTherefore there exists a constant θ0 > 0, such that whenever θ ≤ θ0, the rounding returns e1, completing the proof.\nWhen the input basis is Y′R for some R 6= I, if the ADM algorithm produces some q = R>q′, such that q′1 > 2 √ θ. It is not hard to see that now the rounding (I.1) is equivalent to\nmin q ‖Y′Rq‖1 , s.t. 〈q′,Rq〉 = 1. (I.13)\nRenaming Rq, it follows from the above argument that at optimum q? it holds that Rq? = e1 with overwhelming probability."
    } ],
    "references" : [ {
      "title" : "Learning sparsely used overcomplete dictionaries via alternating minimization",
      "author" : [ "Alekh Agarwal", "Animashree Anandkumar", "Prateek Jain", "Praneeth Netrapalli", "Rashish Tandon" ],
      "venue" : "arXiv preprint arXiv:1310.7991,",
      "citeRegEx" : "AAJ13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Exact recovery of sparsely used overcomplete dictionaries",
      "author" : [ "Alekh Agarwal", "Animashree Anandkumar", "Praneeth Netrapalli" ],
      "venue" : "arXiv preprint arXiv:1309.1952,",
      "citeRegEx" : "AAN13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "More algorithms for provable dictionary learning",
      "author" : [ "Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma" ],
      "venue" : "arXiv preprint arXiv:1401.0579,",
      "citeRegEx" : "ABGM14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "New algorithms for learning incoherent and overcomplete dictionaries",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ankur Moitra" ],
      "venue" : "arXiv preprint arXiv:1308.6273,",
      "citeRegEx" : "AGM13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "When are overcomplete topic models identifiable? uniqueness of tensor tucker decompositions with structured sparsity",
      "author" : [ "AnimaAnandkumar", "Daniel Hsu", "Majid Janzamin", "ShamMKakade" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1986–1994,",
      "citeRegEx" : "AHJK13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "IEEE Transactions on",
      "author" : [ "Ronen Basri", "David W Jacobs. Lambertian reflectance", "linear subspaces. Pattern Analysis", "Machine Intelligence" ],
      "venue" : "25(2):218–233,",
      "citeRegEx" : "BJ03",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Rounding sum-of-squares relaxations",
      "author" : [ "Boaz Barak", "Jonathan Kelner", "David Steurer" ],
      "venue" : "arXiv preprint arXiv:1312.6652,",
      "citeRegEx" : "BKS13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Applied and Computational Harmonic Analysis",
      "author" : [ "Gregory Beylkin", "Lucas Monzón. On approximation of functions by exponential sums" ],
      "venue" : "19(1):17–48,",
      "citeRegEx" : "BM05",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "In Conference on Learning Theory",
      "author" : [ "Quentin Berthet", "Philippe Rigollet. Complexity theoretic lower bounds for sparse principal component detection" ],
      "venue" : "pages 1046–1066,",
      "citeRegEx" : "BR13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Robust principal component analysis? Journal of the ACM",
      "author" : [ "Emmanuel Candès", "Xiaodong Li", "Yi Ma", "JohnWright" ],
      "venue" : "58(3), May",
      "citeRegEx" : "CLMW11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Phase retrieval via wirtinger flow: Theory and algorithms",
      "author" : [ "Emmanuel J. Candès", "Xiaodong Li", "Mahdi Soltanolkotabi" ],
      "venue" : "arXiv preprint arXiv:1407.1065,",
      "citeRegEx" : "CLS14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "SIAM Journal on Algebraic Discrete Methods",
      "author" : [ "Thomas F Coleman", "Alex Pothen. The null space problem i. complexity" ],
      "venue" : "7(4):527–537,",
      "citeRegEx" : "CP86",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "IEEE Transactions on",
      "author" : [ "Emmanuel J Candès", "Terence Tao. Decoding by linear programming. Information Theory" ],
      "venue" : "51(12):4203–4215,",
      "citeRegEx" : "CT05",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "In Computer Vision and Pattern Recognition (CVPR)",
      "author" : [ "Yuchao Dai", "Hongdong Li", "Mingyi He. A simple prior-free method for non-rigid structurefrom-motion factorization" ],
      "venue" : "2012 IEEE Conference on, pages 2018–2025. IEEE,",
      "citeRegEx" : "DLH12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Decoupling: from dependence to independence",
      "author" : [ "Victor De la Pena", "Evarist Giné" ],
      "venue" : "Springer,",
      "citeRegEx" : "DlPG99",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Communications on pure and applied mathematics",
      "author" : [ "David L Donoho. For most large underdetermined systems of linear equations the minimal `-norm solution is also the sparsest solution" ],
      "venue" : "59(6):797–829,",
      "citeRegEx" : "Don06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Acta Mathematica",
      "author" : [ "Tadeusz Figiel", "Joram Lindenstrauss", "Vitali D Milman. The dimension of almost spherical sections of convex bodies" ],
      "venue" : "139(1):53–94,",
      "citeRegEx" : "FLM77",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "volume 277",
      "author" : [ "Andrej Y Garnaev", "Efim D Gluskin. The widths of a euclidean ball. In Dokl. Akad. Nauk SSSR" ],
      "venue" : "pages 1048–1052,",
      "citeRegEx" : "GG84",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "pages 131–135",
      "author" : [ "E Gluskin", "VMilman. Note on the geometric-arithmetic mean inequality. In Geometric aspects of Functional analysis" ],
      "venue" : "Springer,",
      "citeRegEx" : "GM03",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "On the provable convergence of alternating minimization for matrix completion",
      "author" : [ "Moritz Hardt" ],
      "venue" : "arXiv preprint arXiv:1312.0925,",
      "citeRegEx" : "Har13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Recovering the sparsest element in a subspace",
      "author" : [ "Paul Hand", "Laurent Demanet" ],
      "venue" : "arXiv preprint arXiv:1310.1654,",
      "citeRegEx" : "HD13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "In Proceedings of The 30th International Conference on Machine Learning",
      "author" : [ "Jeffrey Ho", "Yuchen Xie", "Baba Vemuri. On a nonlinear generalization of sparse coding", "dictionary learning" ],
      "venue" : "pages 1480–1488,",
      "citeRegEx" : "HXV13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "In Proceedings of the 45th annual ACM symposium on Symposium on theory of computing",
      "author" : [ "Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi. Low-rank matrix completion using alternating minimization" ],
      "venue" : "pages 665–674. ACM,",
      "citeRegEx" : "JNS13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "In Advances in Neural Information Processing Systems",
      "author" : [ "Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi. Phase retrieval using alternating minimization" ],
      "venue" : "pages 2796–2804,",
      "citeRegEx" : "NJS13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "volume 94",
      "author" : [ "Gilles Pisier. The volume of convex bodies", "Banach space geometry" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "Pis99",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Finding a sparse vector in a subspace: Linear sparsity using alternating directions",
      "author" : [ "Qing Qu", "Ju Sun", "John Wright" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "QSW14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Complete dictionary learning over the sphere",
      "author" : [ "Ju Sun", "Qing Qu", "John Wright" ],
      "venue" : "preparation,",
      "citeRegEx" : "SQW14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Exact recovery of sparsely-used dictionaries",
      "author" : [ "Daniel A Spielman", "Huan Wang", "John Wright" ],
      "venue" : "Proceedings of the 25th Annual Conference on Learning Theory,",
      "citeRegEx" : "SWW12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "Roman Vershynin" ],
      "venue" : "arXiv preprint arXiv:1011.3027,",
      "citeRegEx" : "Ver10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Alternating minimization for mixed linear regression",
      "author" : [ "Xinyang Yi", "Constantine Caramanis", "Sujay Sanghavi" ],
      "venue" : "arXiv preprint arXiv:1310.3745,",
      "citeRegEx" : "YCS13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Applied Mathematics and Computation",
      "author" : [ "Yun-Bin Zhao", "Masao Fukushima. Rank-one solutions for homogeneous linear matrix equations over the positive semidefinite cone" ],
      "venue" : "219(10):5569– 5583,",
      "citeRegEx" : "ZF13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Journal of computational and graphical statistics",
      "author" : [ "Hui Zou", "Trevor Hastie", "Robert Tibshirani. Sparse principal component analysis" ],
      "venue" : "15(2):265–286,",
      "citeRegEx" : "ZHT06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Neural computation",
      "author" : [ "Michael Zibulevsky", "Barak A Pearlmutter. Blind source separation by sparse decomposition in a signal dictionary" ],
      "venue" : "13(4):863–882,",
      "citeRegEx" : "ZP01",
      "shortCiteRegEx" : null,
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony’s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 30,
      "context" : "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony’s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].",
      "startOffset" : 146,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony’s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].",
      "startOffset" : 185,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony’s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].",
      "startOffset" : 234,
      "endOffset" : 240
    }, {
      "referenceID" : 31,
      "context" : "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony’s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].",
      "startOffset" : 253,
      "endOffset" : 260
    }, {
      "referenceID" : 32,
      "context" : "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony’s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].",
      "startOffset" : 286,
      "endOffset" : 292
    }, {
      "referenceID" : 27,
      "context" : "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony’s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].",
      "startOffset" : 314,
      "endOffset" : 321
    }, {
      "referenceID" : 4,
      "context" : "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony’s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].",
      "startOffset" : 348,
      "endOffset" : 356
    }, {
      "referenceID" : 21,
      "context" : "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony’s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].",
      "startOffset" : 389,
      "endOffset" : 396
    }, {
      "referenceID" : 11,
      "context" : "2) is NP-hard [CP86].",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 27,
      "context" : "[SWW12] introduced a relaxation which replaces the nonconvex problem (1.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "Method Recovery Condition Total Complexity `1/`∞ Relaxation[HD13] θ ∈ O(1/√n) O(np) SDP Relaxation θ ∈ O(1/√n) O(p) SOS Relaxation [BKS13] p ≥ Ω(n), θ ∈ O(1) high order poly(p) This work p ≥ Ω(n log n), θ ∈ O(1) O(np log n)",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "Method Recovery Condition Total Complexity `1/`∞ Relaxation[HD13] θ ∈ O(1/√n) O(np) SDP Relaxation θ ∈ O(1/√n) O(p) SOS Relaxation [BKS13] p ≥ Ω(n), θ ∈ O(1) high order poly(p) This work p ≥ Ω(n log n), θ ∈ O(1) O(np log n)",
      "startOffset" : 131,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : "3) also correctly recovers x0, provided the fraction of nonzeros in x0 scales as θ ∈ O (1/√n) [HD13].",
      "startOffset" : 94,
      "endOffset" : 100
    }, {
      "referenceID" : 31,
      "context" : "4) and sparse PCA [ZHT06].",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "In sparse PCA, there is a substantial gap between what can be achieved with efficient algorithms and the information theoretic optimum [BR13].",
      "startOffset" : 135,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "introduced a new rounding technique for sum-ofsquares relaxations, and showed that the sparse vector x0 in the planted sparse model can be recovered when p ≥ Ω ( n ) and θ = Ω(1) [BKS13].",
      "startOffset" : 179,
      "endOffset" : 186
    }, {
      "referenceID" : 27,
      "context" : "Second, our theoretical results require a second, linear programming based rounding phase, which is similar to [SWW12].",
      "startOffset" : 111,
      "endOffset" : 118
    }, {
      "referenceID" : 26,
      "context" : "2 1This breakdown behavior is again in sharp contrast to the standard sparse approximation problem (withb 6= 0), inwhich it is possible to handle very large fractions of nonzeros (say, θ = Ω(1/ logn), or even θ = Ω(1)) using a very simple `1 relaxation [CT05, Don06] 2In work currently in preparation [SQW14], we show that in the dictionary learning problem, efficient algorithms based on nonconvex",
      "startOffset" : 301,
      "endOffset" : 308
    }, {
      "referenceID" : 32,
      "context" : ", the work of [ZP01] in blind source separation for precedent.",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 25,
      "context" : "3Note that this version is much stronger and more practical than that appearing in the conference version [QSW14].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 28,
      "context" : "5This is the common heuristic that “tall random matrices are well conditioned” [Ver10].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "’s result [BKS13] in sampling complexity.",
      "startOffset" : 10,
      "endOffset" : 17
    }, {
      "referenceID" : 27,
      "context" : "Second, we consider the same dictionary learning model as in [SWW12].",
      "startOffset" : 61,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "2 Exploratory Experiments on Faces It is well known in computer vision that appearance of convex objects only subject to illumination changes leads to image collection that can be well approximated by low-dimensional space in raw-pixel space [BJ03].",
      "startOffset" : 242,
      "endOffset" : 248
    }, {
      "referenceID" : 9,
      "context" : "Thenwe apply robust principal component analysis [CLMW11] to the data and get a low dimensional subspace of dimension 10, i.",
      "startOffset" : 49,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "Advanced techniques to bound the empirical process, such as decoupling [DlPG99] techniques, can be deployed in place of our crude union bound to cover all iterates.",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "Our forthcoming work [SQW14] on dictionary learning takes a more geometric approach, and proves global recovery via efficient algorithms, with arbitrary initialization.",
      "startOffset" : 21,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "References [AAJ13] Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon.",
      "startOffset" : 11,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "[AAN13] Alekh Agarwal, Animashree Anandkumar, and Praneeth Netrapalli.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 2,
      "context" : "[ABGM14] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 3,
      "context" : "[AGM13] Sanjeev Arora, Rong Ge, and Ankur Moitra.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 4,
      "context" : "[AHJK13] AnimaAnandkumar, Daniel Hsu, Majid Janzamin, and ShamMKakade.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "[BJ03] Ronen Basri and David W Jacobs.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "[BKS13] Boaz Barak, Jonathan Kelner, and David Steurer.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "[BM05] Gregory Beylkin and Lucas Monzón.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 8,
      "context" : "[BR13] Quentin Berthet and Philippe Rigollet.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : "[CLMW11] Emmanuel Candès, Xiaodong Li, Yi Ma, and JohnWright.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 10,
      "context" : "[CLS14] Emmanuel J.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "[CP86] Thomas F Coleman and Alex Pothen.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 12,
      "context" : "[CT05] Emmanuel J Candès and Terence Tao.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 13,
      "context" : "[DLH12] Yuchao Dai, Hongdong Li, and Mingyi He.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "[DlPG99] Victor De la Pena and Evarist Giné.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 15,
      "context" : "[Don06] David L Donoho.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "[FLM77] Tadeusz Figiel, Joram Lindenstrauss, and Vitali D Milman.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "[GG84] Andrej Y Garnaev and Efim D Gluskin.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 18,
      "context" : "[GM03] E Gluskin and VMilman.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 19,
      "context" : "[Har13] Moritz Hardt.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "[HD13] Paul Hand and Laurent Demanet.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 21,
      "context" : "[HXV13] Jeffrey Ho, Yuchen Xie, and Baba Vemuri.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 22,
      "context" : "[JNS13] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 23,
      "context" : "[NJS13] Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 24,
      "context" : "[Pis99] Gilles Pisier.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 25,
      "context" : "[QSW14] Qing Qu, Ju Sun, and John Wright.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : "[SQW14] Ju Sun, Qing Qu, and John Wright.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 27,
      "context" : "[SWW12] Daniel A Spielman, Huan Wang, and John Wright.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 28,
      "context" : "[Ver10] Roman Vershynin.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 29,
      "context" : "[YCS13] Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 30,
      "context" : "[ZF13] Yun-Bin Zhao and Masao Fukushima.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 31,
      "context" : "[ZHT06] Hui Zou, Trevor Hastie, and Robert Tibshirani.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 32,
      "context" : "[ZP01] Michael Zibulevsky and Barak A Pearlmutter.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 28,
      "context" : "13 (Spectrum of Gaussian Matrices, [Ver10]).",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "Geometrically, this lemma roughly corresponds to thewell known almost spherical section theorem [FLM77, GG84], see also [GM03].",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "A slight variant of this version has been proved in [Don06], borrowing ideas from [Pis99].",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 24,
      "context" : "A slight variant of this version has been proved in [Don06], borrowing ideas from [Pis99].",
      "startOffset" : 82,
      "endOffset" : 89
    } ],
    "year" : 2017,
    "abstractText" : "We consider the problem of recovering the sparsest vector in a subspace S ⊆ R with dim (S) = n < p. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing andmachine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds 1/ √ n. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is Ω(1). To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.",
    "creator" : "LaTeX with hyperref package"
  }
}