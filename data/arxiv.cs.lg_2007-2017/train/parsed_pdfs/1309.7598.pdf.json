{
  "name" : "1309.7598.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations",
    "authors" : [ "Tamir Hazan", "Subhransu Maji", "Tommi Jaakkola" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Inference in complex models drives much of the research in machine learning applications, from computer vision, natural language processing, to computational biology. Examples include scene understanding [3], parsing [12], or protein design [18]. The inference problem in such cases involves finding likely structures, whether objects, parsers, or molecular arrangements. Each structure corresponds to an assignment of values to random variables and the likelihood of an assignment is based on defining potential functions in a Gibbs distribution. Usually, it is feasible to find only the most likely or maximum a-posteriori (MAP) assignment (structure) rather than sampling from the full Gibbs distribution. Substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity [11] or by devising cutting-planes based methods on linear programming relaxations [18, 23]. However, MAP inference is limited when there are other likely assignments.\nOur work seeks to leverage MAP inference so as to sample efficiently from the full Gibbs distribution. Specifically, we aim to draw either approximate or unbiased samples from Gibbs distributions by introducing low dimensional perturbations in the potential functions and solving the corresponding MAP assignments. Connections between random MAP perturbations and Gibbs distributions have been explored before. Recently [16, 20] defined probability models that are based on low dimensional perturbations, and empirically tied them to Gibbs distributions. [6] augmented these results by providing bounds on the partition function in terms of random MAP perturbations.\nIn this work we build on these results to construct an efficient sampler for the Gibbs distribution, also deriving new lower bounds on the partition function. Our approach excels in regimes where there are several but not exponentially many prominent assignments. In such ragged energy landscapes classical methods for the Gibbs distribution such as Gibbs sampling and Markov chain Monte Carlo methods, remain computationally expensive [4, 24]."
    }, {
      "heading" : "2 Background",
      "text" : "Statistical inference problems involve reasoning about the states of discrete variables whose configurations (assignments of values) specify the discrete structures of interest. We assume that the\nar X\niv :1\n30 9.\n75 98\nv1 [\ncs .L\nG ]\n2 9\nSe p\n20 13\nmodels are parameterized by real valued potentials θ(x) = θ(x1, ..., xn) < ∞ defined over a discrete product space X = X1 × · · · ×Xn. The effective domain is implicitly defined through θ(x) via exclusions θ(x) = −∞ whenever x 6∈ dom(θ). The real valued potential functions are mapped to the probability scale via the Gibbs’ distribution:\np(x1, ..., xn) = 1\nZ exp(θ(x1, ..., xn)), where Z = ∑ x1,...,xn exp(θ(x1, ..., xn)). (1)\nThe normalization constantZ is called the partition function. The feasibility of using the distribution for prediction, including sampling from it, is inherently tied to the ability to evaluate the partition function, i.e., the ability to sum over the discrete structures being modeled. In general, such counting problems are often hard, in #P.\nA slightly easier problem is that of finding the most likely assignment of values to variables, also known as the maximum a-posterior (MAP) prediction.\n(MAP) arg max x1,...,yn θ(x1, ..., xn) (2)\nRecent advances in optimization theory have been translated to successful algorithms for solving such MAP problems in many cases of practical interest. Although the MAP prediction problem is still NP-hard in general, it is often simpler than sampling from the Gibbs distribution.\nOur approach is based on representations of the Gibbs distribution and the partition function using extreme value statistics of linearly perturbed potential functions. Let {γ(x)}x∈X be a collection of random variables with zero mean, and consider random potential functions of the form θ(x) +γ(x). Analytic expressions for the statistics of a randomized MAP predictor, x̂ ∈ argmaxx{θ(x) + γ(x)}, can be derived for general discrete sets, whenever independent and identically distributed (i.i.d.) random perturbations are applied for every assignment x ∈ X . Specifically, when the random perturbations follow the Gumbel distribution (cf. [13]), we obtain the following result. Theorem 1. [5] Let {γ(x)}x∈X be a collection of i.i.d. random variables, each following the Gumbel distribution with zero mean, whose cumulative distribution function is F (t) = exp(− exp(−(t+ c))), where c is the Euler constant. Then\nlogZ = Eγ [ max x∈X {θ(x) + γ(x)} ] .\n1 Z exp(θ(x̂)) = Pγ\n[ x̂ ∈ arg max\nx∈X {θ(x) + γ(x)}\n] .\nThe max-stability of the Gumbel distribution provides a straight forward approach to generate unbiased samples from the Gibbs distribution as well as to approximate the partition function by a sample mean of random MAP perturbation. Assume we sample j = 1, ...,m independent predictions maxx{θ(x) + γj(x)}, then every maximal argument is an unbiased sample from the Gibbs distribution. Moreover, the randomized MAP predictions maxx{θ(x)+γj(x)} are independent and follow the Gumbel distribution, whose variance is π2/6. Therefore Chebyshev’s inequality dictates, for every t,m\nPrγ [∣∣∣ 1 m m∑ j=1 max x {θ(x) + γj(x)} − logZ ∣∣∣ ≥ ] ≤ π 6m 2\n(3)\nIn general each x = (x1, ..., xn) represents an assignment to n variables. Theorem 1 suggests to introduce an independent perturbation γ(x) for each such n−dimensional assignment x ∈ X . The complexity of inference and learning in this setting would be exponential in n. In our work we propose to investigate low dimensional random perturbations as the main tool to efficiently (approximate) sampling from the Gibbs distribution."
    }, {
      "heading" : "3 Probable approximate samples from the Gibbs distribution",
      "text" : "Sampling from the Gibbs distribution is inherently tied to estimating the partition function. Markov properties that simplify the distribution also decompose the computation of the partition function.\nFor example, assume a graphical model with potential functions associated with subsets of variables α ⊂ {1, ..., n} so that θ(x) = ∑ α∈A θα(xα). Assume that the subsets are disjoint except for their common intersection β = ∩α∈A. This separation implies that the partition function can be computed in lower dimensional pieces\nZ = ∑ xβ ∏ α∈A ( ∑ xα\\xβ exp(θα(xα)) )\nAs a result, the computation is exponential only in the size of the subsets α ∈ A. Thus, we can also estimate the partition function with lower dimensional random MAP perturbations, Eγ [maxxα\\xβ{θα(xα) + γα(xα)}]. The random perturbation are now required only for each assignment of values to the variables within the subsets α ∈ A rather than the set of all variables. We approximate such partition functions with low dimensional perturbations and their averages. The overall computation is cast in a single MAP problem using an extended representation of potential functions by replicating variables.\nLemma 1. Let A be subsets of variables that are separated by their joint intersection β = ∩α∈Aα. We create multiple copies of xα, namely xα,jα for jα = 1, ...,mα, and define the extended potential function θ̂α(xα) = ∑mα jα=1\nθα(xα,jα)/mα. We also define the extended perturbation model γ̂α(xα) = ∑mα jα=1\nγα,jα(xα,jα)/mα, where each γα,jα(xα) is independent and distributed according to the Gumbel distribution with zero mean. Then, for every xβ , with probability at least 1− ∑ α∈A π2\n6mα 2∣∣∣max x\\xβ {∑ α∈A θ̂α(xα) + ∑ α∈A γ̂α(xα) } − ∑ α∈A log ( ∑ xα\\xβ exp(θα(xα)) )∣∣∣ ≤ |A|\nProof: Equation (3) implies that for every xβ with probability at most π2/6mα 2 holds∣∣∣ 1 mα mα∑ jα=1 max xα\\xβ {θα(xα) + γα,jα(xα)} − log ( ∑ xα\\xβ exp(θα(xα)) )∣∣∣ ≤ .\nTo compute the sampled average with a single max-operation we introduce the multiple copies x = (xα,jα)jα=1,...,mα thus ∑mα jα=1\nmaxxα\\xβ{θα(xα) + γα,jα(xα)} = maxxα,jα\\xβ ∑m j=1{θα(xα,jα) + γα,jα(xα,jα)}. By the union bound it holds for every α ∈ A\nsimultaneously with probability at least 1 − ∑ α∈A π 2/6mα 2. Since xβ is fixed for every α ∈ A the maximizations are done independently across subsets in x \\ xβ and∑ α∈A max x\\xβ mα∑ jα=1 { θα(xα,jα) + γα,jα(xα,jα) } = max x\\xβ mα∑ jα=1 {∑ α∈A θα(xα,jα) + ∑ α∈A γα,jα(xα,jα) } .\nThe proof then follows from the triangle inequality.\nWhenever the graphical model has no cycles we can iteratively apply the separation properties without increasing the computational complexity of perturbations. Thus we may randomly perturb the subsets of potentials in the graph. For notational simplicity we describe our approximate sampling scheme for pairwise interactions α = (i, j) although it holds for general graphical models without cycles:\nTheorem 2. Let θ(x) = ∑ i∈V θi(xi) + ∑ i,j∈E θi,j(xi, xj) be a graphical model without cycles, and let p(x) be the Gibbs distribution defined in Equation (1). Let θ̂(x) =∑mi ki=1 θ(x1,k1 , ..., xn,kn)/ ∏ imi, and γ̂i,j(xi, xj) = ∑mi,mj ki,kj=1\nγi,j,ki,kj (xi,ki , xj,kj )/mimj where each perturbation is independent and distributed according to the Gumbel distribution with zero mean. Then, for every edge (r, s) while mr = ms = 1 (i.e., they have no multiple copies) there holds with probability at least 1− ∑n i=1 π\n2c/6mi 2, where c = maxi |Xi|∣∣∣ log (Pγ[xr, xs ∈ arg max\nx̂\n{ θ̂(x) + ∑ i,j∈E γ̂i,j(xi, xj) }]) − log ( ∑ x\\xr,xs p(x) )∣∣∣ ≤ n\nProof: Theorem 1 implies that we sample (xr, xs) approximately from the Gibbs distribution marginal probabilities with a max-operation, if we approximate ∑ x\\{xr,xs} exp(θ(x)). Using graph separation (or equivalently the Markov property) it suffices to approximate the partial partition function over the disjoint subtrees Tr, Ts that originate from r, s respectively. Lemma 1 describes this case for a directed tree with a single parent. We use this by induction on the parents on these directed trees, noticing that graph separation guarantees: the statistics of Lemma 1 hold uniformly for every assignment of the parent’s non-descendants as well; the optimal assignments in Lemma 1 are chosen independently for every child for every assignment of the parent’s non-descendants label.\nOur approximated sampling procedure expands the graphical model, creating layers of the original graph, while connecting edges between vertices in the different layers if an edge exists in the original graph. We use graph separations (Markov properties) to guarantee that the number of added layers is polynomial in n, while we approach arbitrarily close to the Gibbs distribution. This construction preserves the structure of the original graph, in particular, whenever the original graph has no cycles, the expanded graph does not have cycles as well. In the experiments we show that this probability model approximates well the Gibbs distribution for graphical models with many cycles."
    }, {
      "heading" : "4 Unbiased sampling using sequential bounds on the partition function",
      "text" : "In the following we describe how to use random MAP perturbations to generate unbiased samples from the Gibbs distribution. Sampling from the Gibbs distribution is inherently tied to estimating the partition function. Assume we could have compute the partition function exactly, then we could have sample from the Gibbs distribution sequentially: for every dimension we sample xi with probability which is proportional to ∑ xi+1,...,xn\nexp(θ(x)). Unfortunately, approximations to the partition function, as described in Section 3, cannot provide a sequential procedure that would generate unbiased samples from the full Gibbs distribution. Instead, we construct a family of self-reducible upper bounds which imitate the partition function behavior, namely bound the summation over its exponentiations. These upper bounds extend the one in [6] when restricted to local perturbations. Lemma 2. Let {γi(xi)} be a collection of i.i.d. random variables, each following the Gumbel distribution with zero mean. Then for every j = 1, ..., n and every x1, ..., xj−1 holds∑ xj exp ( Eγ [ max xj+1,...,xn {θ(x) + n∑ i=j+1 γi(xi)} ]) ≤ exp ( Eγ [ max xj ,...,xn {θ(x) + n∑ i=j γi(xi)} ])\nIn particular, for j = n holds ∑ xn exp(θ(x)) = exp ( Eγn(xn) [ maxxj ,...,xn{θ(x) + γn(xn)} ]) .\nProof: The result is an application of the expectation-optimization interpretation of the partition function in Theorem 1. The left hand side equals to Eγj [ maxxj Eγj+1,...,γn [ maxxj+1,...,xn{θ(x)+∑n\ni=j γi(xi) }]]\n, while the right hand side is attained by alternating the maximization with respect to xj with the expectation of γj+1, ..., γn. The proof then follows by taking the exponent.\nWe use these upper bounds for every dimension i = 1, ..., n to sample from a probability distribution that follows a summation over exponential functions, with a discrepancy that is described by the upper bound. This is formalized below in Algorithm 1\nAlgorithm 1 Unbiased sampling from Gibbs distribution using randomized prediction Iterate over j = 1, ..., n, while keeping fixed x1, ..., xj−1. Set\n1. pj(xj) = exp ( Eγ [ maxxj+1,...,xn{θ(x)+ ∑n i=j+1 γi(xi)} ]) exp ( Eγ [ maxxj,...,xn{θ(x)+ ∑n i=j γi(xi)}\n]) . 2. pj(r) = 1− ∑ xj p(xj)\n3. Sample an element according to pj(·). If r is sampled then reject and restart with j = 1. Otherwise, fix the sampled element xj and continue the iterations.\nOutput: x1, ..., xn\nWhen we reject the discrepancy, the probability we accept a configuration x is the product of probabilities in all rounds. Since these upper bounds are self-reducible, i.e., for every dimension i we\nare using the same quantities that were computed in the previous dimensions 1, ..., i − 1, we are sampling an accepted configuration proportionally to exp(θ(x)), the full Gibbs distribution.\nTheorem 3. Let p(x) be the Gibbs distribution, defined in Equation (1) and let {γi(xi)} be a collection of i.i.d. random variables following the Gumbel distribution with zero mean. Then whenever Algorithm 1 accepts, it produces a configuration (x1, ..., xn) according to the Gibbs distribution\nP [ Algorithm 1 outputs x ∣∣ Algorithm 1 accepts] = p(x). Proof: The probability of sampling a configuration (x1, ..., xn) without rejecting is n∏ j=1 exp ( Eγ [ max xj+1,...,xn {θ(x) + ∑n i=j+1 γi(xi)} ]) exp ( Eγ [\nmax xj ,...,xn\n{θ(x) + ∑n i=j γi(xi)} ]) = exp(θ(x)) exp ( Eγ [\nmax x1,...,xn\n{θ(x) + ∑n i=1 γi(xi)} ]) . The probability of sampling without rejecting is thus the sum of this probability over all configuration, i.e., P [ Algorithm 1 accepts ] = Z / exp ( Eγ [ maxx1,...,xn{θ(x) + ∑n i=1 γi(xi)} ]) . Therefore conditioned on accepting a configuration, it is produced according to the Gibbs distribution. .\nAcceptance/rejection follows the geometric distribution, therefore the sampling procedure rejects k times with probability (1 − P [Algorithm 1 accepts])k. The running time of our Gibbs sampler is determined by the average number of rejections 1/P [Algorithm 1 accepts]. Interestingly, this average is the quality of the partition upper bound presented in [6]. To augment this result we investigate in the next section efficiently computable lower bounds to the partition function, that are based on random MAP perturbations. These lower bounds provide a way to efficiently determine the computational complexity for sampling from the Gibbs distribution for a given potential function."
    }, {
      "heading" : "5 Lower bounds on the partition function",
      "text" : "The realization of the partition function as expectation-optimization pair in Theorem 1 provides efficiently computable lower bounds on the partition function. Intuitively, these bounds correspond to moving expectations (or summations) inside the maximization operations. In the following we present two lower bounds that are derived along these lines, the first holds in expectation and the second holds in probability.\nCorollary 1. Consider a family of subsets α ∈ A and let xα be a set of variables {xi}i∈α restricted to the indexes in α. Assume that the random variables γα(xα) are i.i.d. according to the Gumbel distribution with zero mean, for every α, xα. Then\n∀α ∈ A logZ ≥ Eγ [\nmax x\n{ θ(x) + γα(xα) }] .\nIn particular, logZ ≥ Eγ [ maxx { θ(x) + 1|A| ∑ α∈A γα(xα) }] .\nProof: Let ᾱ = {1, ..., n} \\ α then Z = ∑ xα ∑ xᾱ exp(θ(x)) ≥ ∑ xα\nmaxxᾱ exp(θ(x)). The first result is derived by swapping the maximization with the exponent, and applying Theorem 1. The second result is attained while averaging these lower bounds logZ ≥ ∑ α∈A 1 |A|Eγ [maxx{θ(x) + γα(xα)}], and by moving the summation inside the maximization operation. The expected lower bound requires to invoke a MAP solver multiple times. Although this expectation may be estimated with a single MAP execution, the variance of this random MAP prediction is around √ n. We suggest to recursively use Lemma 1 to lower bound the partition function with a single MAP operation in probability.\nCorollary 2. Let θ(x) be a potential function over x = (x1, ..., xn). We create multiple copies of xi, namely xi,ki for ki = 1, ...,mi, and define the extended potential function θ̂(x) = ∑mi ki=1 θ(x1,k1 , ..., xn,kn)/ ∏ mi. We define the extended perturbation model γ̂i(xi) =∑mi\nki=1 γi,ki(xi,ki)/mi where each perturbation is independent and distributed according to the Gumbel distribution with zero mean. Then, with probability at least 1 − ∑n i=1 π 2|dom(θ)|/6mi 2\nholds logZ ≥ maxx̂{θ̂(x) + ∑n i=1 γ̂i(xi)} − n\nProof: We estimate the expectation-optimization value of the log-partition function iteratively for every dimension, while replacing each expectation with its sampled average, as described in Lemma 1. Our result holds for every potential function, thus the statistics in each recursion hold uniformly for every x with probability at least 1− π2|dom(θ)|/6mi 2. We then move the averages inside the maximization operation, thus lower bounding the n−approximation of the partition function. The probable lower bound that we provide does not assume graph separations thus the statistical guarantees are worse than the ones presented in the approximation scheme of Theorem 2. Also, since we are seeking for lower bound, we are able relax our optimization requirements and thus to use vertex based random perturbations γi(xi). This is an important difference that makes this lower bound widely applicable and very efficient."
    }, {
      "heading" : "6 Experiments",
      "text" : "We evaluated our approach on spin glass models θ(x) = ∑ i∈V θixi + ∑ (i,j)∈E θi,jxixj . where xi ∈ {−1, 1}. Each spin has a local field parameter θi, sampled uniformly from [−1, 1]. The spins interact in a grid shaped graphical model with couplings θi,j , sampled uniformly from [0, c]. Whenever the coupling parameters are positive the model is called attractive as adjacent variables give higher values to positively correlated configurations. Attractive models are computationally appealing as their MAP predictions can be computed efficiently by the graph-cut algorithm [2].\nWe begin by evaluating our lower bounds, presented in Section 5, on 10 × 10 spin glass models. Corollary 1 presents a lower bound that holds in expectation. We evaluated these lower bounds while perturbing the local potentials with γi(xi). Corollary 2 presents a lower bound that holds in probability and requires only a single MAP prediction on an expanded model. We evaluate the probable bound by expanding the model to 1000× 1000 grids, ignoring the discrepancy . For both the expected lower bound and the probable lower bound we used graph-cuts to compute the random MAP perturbations. We compared these bounds to the different forms of structured mean-field, taking the one that performed best: standard structured mean-field that we computed over the vertical chains [8, 1], and the negative tree re-weighted computed on the horizontal and vertical trees [14]. We also compared to the sum-product belief propagation algorithm, which was recently proven to produce lower bounds for attractive models [19, 17]. We computed the error in estimating the logarithm of the partition function, averaged over 10 spin glass models, see Figure 1. One can see that the probable bound is the tightest when considering the medium and high coupling domain, which is traditionally hard for all methods. As it holds in probability it might generate a solution which is not a lower bound. One can also verify that on average this does not happen. The expected lower bound is significantly worse for the low coupling regime, in which many configurations need to be taken into account. It is (surprisingly) effective for the high coupling regime, which is characterized by a few dominant configurations.\nSection 4 describes an algorithm that generates unbiased samples from the full Gibbs distribution. Focusing on spin glass models with strong local field potentials, it is well know that one cannot produce unbiased samples from the Gibbs distributions in polynomial time [4]. Theorem 3 connects\nthe computational complexity of our unbiased sampling procedure to the gap between the logarithm of the partition function and its upper bound in [6]. We use our probable lower bound to estimate this gap on large grids, for which we cannot compute the partition function exactly. Figure 1 suggests that the running time for this sampling procedure is sub-exponential.\nSampling from the Gibbs distribution in spin glass models with non-zero local field potentials is computationally hard [7, 4]. The approximate sampling technique in Theorem 3 suggests a method to overcome this difficulty by efficiently sampling from a distribution that approximates the Gibbs distribution on its marginal probabilities. Although our theory is only stated for graphs without cycles, it can be readily applied to general graphs, in the same way the (loopy) belief propagation algorithm is applied. For computational reasons we did not expand the graph. Also, we experiment both with pairwise perturbations, as the Theorem 2 suggests, and with local perturbations, which are guaranteed to preserve the potential function super-modularity. We computed the local marginal probability errors of our sampling procedure, while comparing to the standard methods of Gibbs sampling and Metropolis1. In our experiments we let them run for at most 1e8 iterations. We also compared to the sum-product belief propagation, although it does not generate samples from a well defined probability model, see Figure 1. The belief propagation algorithm performs the worse. Both Gibbs sampling and the Metropolis algorithm perform similarly (we omit the Metropolis performance for clarity). Although these algorithm directly sample from the Gibbs distribution, they typically require exponential running time to succeed on spin glass models. Although we omit from the plots for clarity, our approximate sampling marginal probabilities compares those of the tree re-weighted belief propagation [21]. Nevertheless, our sampling scheme also provide a probability notion, which lacks in the belief propagation type algorithms. Surprisingly, the approximate sampler that uses pairwise perturbations performs similarly to the approximate sampler that only use local perturbations. Although this is not explained by our current theory, it is an encouraging observation, since approximate sampler that uses random MAP predictions with local perturbations is orders of magnitude faster than Gibbs sampling and the Metropolis algorithm.\nLastly, we emphasize the importance of probabilistic reasoning over the current variational methods, such as tree re-weighted belief propagation [21] or max-marginal probabilities [9], that only generate probabilities over small subsets of variables. The task we consider is to obtain pixel accurate boundaries from rough boundaries provided by the user. For example in an image editing application the user may provide an input in the form of a rough polygon and the goal is to refine the boundaries using the information from the gradients in the image. A natural notion of error is the average deviation of the marked boundary from the true boundary of the image. Given a user boundary we set up a graphical model on the pixels using foreground/background models trained from regions well inside/outside the marked boundary. Exact binary labeling can be obtained using the graph-cuts algorithm. From this we can compute the expected error by sampling multiple solutions using random MAP predictors and averaging. On a dataset of 10 images which we carefully annotated to obtain pixel accurate boundaries we find that random MAP perturbations produce significantly more accurate estimates of boundary error compared to a single MAP solution. On average the error estimates obtained using random MAP perturbations is off by 1.04 pixels from the true error (obtained from ground truth) whereas the MAP which is off by 3.51 pixels. Such a measure can be used in an active annotation framework where the users can iteratively fix parts of the boundary that contain errors.\n1We used Talya Meltzer’s inference package.\nFigure 2 shows an example annotation, the MAP solution, the mean of 20 random MAP solutions, and boundary error estimates."
    }, {
      "heading" : "7 Related work",
      "text" : "The Gibbs distribution plays a key role in many areas of science, including computer science, statistics and physics. To learn more about its roles in machine learning, as well as its standard samplers, we refer the interested reader to the textbook [10, 22]. Our work is based on max-statistics of collections of random variables. For comprehensive introduction to extreme value statistics we refer the reader to [13].\nThe Gibbs distribution and its partition function can be realized from the statistics of random MAP perturbations with the Gumbel distribution (see Theorem 1), [13, 16, 20, 6]. Recently, [15, 16, 20] explore the different aspects of random MAP predictions with low dimensional perturbation. [15] describe sampling from the Gaussian distribution with random Gaussian perturbations. [16] show that random MAP predictors with low dimensional perturbations share similar statistics as the Gibbs distribution. [20] describe the Bayesian perspectives of these models and their efficient sampling procedures. In our work we formally relate random MAP perturbations and the Gibbs distribution. Specifically, we describe the case for which the marginal probabilities of random MAP perturbations, with the proper expansion, approximate those of the Gibbs distribution. We also show how to use the statistics of random MAP perturbations to generate unbiased samples from the Gibbs distribution. These probability models generate samples efficiently thorough optimization: they have statistical advantages over purely variational approaches such as tree re-weighted belief propagation [21] or max-marginals [9], and they are faster than standard Gibbs samplers and Markov chain Monte Carlo approaches when MAP prediction is efficient [4, 24]\nOur suggested samplers for the Gibbs distribution are based on low dimensional representation of the partition function, [6]. We augment their results in a few ways. In Lemma 2 we refine their upper bound, to a series of sequentially tighter bounds. Corollary 2 shows that the approximation scheme of [6] is in fact a lower bound that holds in probability. Lower bounds for the partition function have been extensively developed in the recent years within the context of variational methods. Structured mean-field methods are inner-bound methods where a simpler distribution is optimized as an approximation to the posterior in a KL-divergence sense [8, 1, 14]. The difficulty comes from non-convexity of the set of feasible distributions. Surprisingly, [19, 17] have shown that the sum-product belief propagation provides a lower bound to the partition function for super-modular potential functions. This result is based on the four function theorem which considers nonnegative functions over distributive lattices."
    }, {
      "heading" : "8 Discussion",
      "text" : "This work explores new approaches to sample from the Gibbs distribution. Sampling from the Gibbs distribution is key problem in machine learning. Traditional approaches, such as Gibbs sampling, fail in the “high-signal high-coupling” regime that results in ragged energy landscapes. Following [16, 20], we showed here that one can take advantage of efficient MAP solvers to generate approximate or unbiased samples from the Gibbs distribution, when we randomly perturb the potential function. Since MAP predictions are not affected by ragged energy landscapes, our approach excels in the “high-signal high-coupling” regime. As a by-product to our approach we constructed lower bounds to the partition functions, which are both tighter and faster than the previous approaches in the ”high-signal high-coupling” regime.\nOur approach is based on random MAP perturbations that estimate the partition functions with expectation. In practice we compute the empirical mean, and standard techniques in measure concentration, e.g. Chebyshev’s inequality, describe how the sampled mean relates to the expected value. However, our experiments show that in practice we get tighter concentration of measure. Exponentially small tails can be easily derived by the truncation method, but with bad constant.\nThe computational complexity of our approximate sampling procedure is determined by the perturbations dimension. Currently, our theory do not describe the success of the probability model that is based on the maximal argument of perturbed MAP program with local perturbations."
    } ],
    "references" : [ {
      "title" : "Optimization of structured mean field objectives",
      "author" : [ "Alexandre Bouchard-Côté", "Michael I Jordan" ],
      "venue" : "In AUAI,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Fast approximate energy minimization via graph cuts",
      "author" : [ "Y. Boykov", "O. Veksler", "R. Zabih" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Dynamic programming and graph algorithms in computer vision",
      "author" : [ "P.F. Felzenszwalb", "R. Zabih" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "The complexity of ferromagnetic ising with local fields",
      "author" : [ "L.A. Goldberg", "M. Jerrum" ],
      "venue" : "Combinatorics Probability and Computing,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Statistical theory of extreme values and some practical applications: a series of lectures, volume 33",
      "author" : [ "E.J. Gumbel", "J. Lieblein" ],
      "venue" : "US Govt. Print. Office,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1954
    }, {
      "title" : "On the partition function and random maximum a-posteriori perturbations",
      "author" : [ "T. Hazan", "T. Jaakkola" ],
      "venue" : "arXiv preprint arXiv:1206.6410,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Polynomial-time approximation algorithms for the ising model",
      "author" : [ "M. Jerrum", "A. Sinclair" ],
      "venue" : "SIAM Journal on computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1993
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1999
    }, {
      "title" : "Measuring uncertainty in graph cut solutions–efficiently computing min-marginal energies using dynamic graph cuts",
      "author" : [ "Pushmeet Kohli", "Philip HS Torr" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Probabilistic graphical models",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : "MIT press,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Convergent tree-reweighted message passing for energy minimization",
      "author" : [ "V. Kolmogorov" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Dual decomposition for parsing with non-projective head automata",
      "author" : [ "T. Koo", "A.M. Rush", "M. Collins", "T. Jaakkola", "D. Sontag" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Extreme value distributions: theory and applications",
      "author" : [ "S. Kotz", "S. Nadarajah" ],
      "venue" : "World Scientific Publishing Company,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2000
    }, {
      "title" : "Negative tree reweighted belief propagation",
      "author" : [ "Qiang Liu", "Alexander T Ihler" ],
      "venue" : "arXiv preprint arXiv:1203.3494,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Gaussian sampling by local perturbations",
      "author" : [ "G. Papandreou", "A. Yuille" ],
      "venue" : "In Proc. Int. Conf. on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models",
      "author" : [ "G. Papandreou", "A. Yuille" ],
      "venue" : "In Proc. IEEE Int. Conf. on Computer Vision (ICCV),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "The bethe partition function of log-supermodular graphical models",
      "author" : [ "Nicholas Ruozzi" ],
      "venue" : "arXiv preprint arXiv:1202.6035,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Tightening LP relaxations for MAP using message passing",
      "author" : [ "D. Sontag", "T. Meltzer", "A. Globerson", "T. Jaakkola", "Y. Weiss" ],
      "venue" : "In Conf. Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Loop series and Bethe variational bounds in attractive graphical models",
      "author" : [ "E.B. Sudderth", "M.J. Wainwright", "A.S. Willsky" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2008
    }, {
      "title" : "Randomized optimum models for structured prediction",
      "author" : [ "D. Tarlow", "R.P. Adams", "R.S. Zemel" ],
      "venue" : "In Proceedings of the Fifteenth Conference on Artificial Intelligence and Statistics: April,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "A new class of upper bounds on the log partition function",
      "author" : [ "M.J. Wainwright", "T.S. Jaakkola", "A.S. Willsky" ],
      "venue" : "Trans. on Information Theory,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2005
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Foundations and Trends R  © in Machine Learning,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf)",
      "author" : [ "T. Werner" ],
      "venue" : "In CVPR, pages",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Approximating partition functions of the two-state spin system",
      "author" : [ "J. Zhang", "H. Liang", "F. Bai" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Examples include scene understanding [3], parsing [12], or protein design [18].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 11,
      "context" : "Examples include scene understanding [3], parsing [12], or protein design [18].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "Examples include scene understanding [3], parsing [12], or protein design [18].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "Substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity [11] or by devising cutting-planes based methods on linear programming relaxations [18, 23].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 17,
      "context" : "Substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity [11] or by devising cutting-planes based methods on linear programming relaxations [18, 23].",
      "startOffset" : 244,
      "endOffset" : 252
    }, {
      "referenceID" : 22,
      "context" : "Substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity [11] or by devising cutting-planes based methods on linear programming relaxations [18, 23].",
      "startOffset" : 244,
      "endOffset" : 252
    }, {
      "referenceID" : 15,
      "context" : "Recently [16, 20] defined probability models that are based on low dimensional perturbations, and empirically tied them to Gibbs distributions.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 19,
      "context" : "Recently [16, 20] defined probability models that are based on low dimensional perturbations, and empirically tied them to Gibbs distributions.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "[6] augmented these results by providing bounds on the partition function in terms of random MAP perturbations.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "In such ragged energy landscapes classical methods for the Gibbs distribution such as Gibbs sampling and Markov chain Monte Carlo methods, remain computationally expensive [4, 24].",
      "startOffset" : 172,
      "endOffset" : 179
    }, {
      "referenceID" : 23,
      "context" : "In such ragged energy landscapes classical methods for the Gibbs distribution such as Gibbs sampling and Markov chain Monte Carlo methods, remain computationally expensive [4, 24].",
      "startOffset" : 172,
      "endOffset" : 179
    }, {
      "referenceID" : 12,
      "context" : "[13]), we obtain the following result.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[5] Let {γ(x)}x∈X be a collection of i.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "These upper bounds extend the one in [6] when restricted to local perturbations.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "Interestingly, this average is the quality of the partition upper bound presented in [6].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "Attractive models are computationally appealing as their MAP predictions can be computed efficiently by the graph-cut algorithm [2].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "We compared these bounds to the different forms of structured mean-field, taking the one that performed best: standard structured mean-field that we computed over the vertical chains [8, 1], and the negative tree re-weighted computed on the horizontal and vertical trees [14].",
      "startOffset" : 183,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "We compared these bounds to the different forms of structured mean-field, taking the one that performed best: standard structured mean-field that we computed over the vertical chains [8, 1], and the negative tree re-weighted computed on the horizontal and vertical trees [14].",
      "startOffset" : 183,
      "endOffset" : 189
    }, {
      "referenceID" : 13,
      "context" : "We compared these bounds to the different forms of structured mean-field, taking the one that performed best: standard structured mean-field that we computed over the vertical chains [8, 1], and the negative tree re-weighted computed on the horizontal and vertical trees [14].",
      "startOffset" : 271,
      "endOffset" : 275
    }, {
      "referenceID" : 18,
      "context" : "We also compared to the sum-product belief propagation algorithm, which was recently proven to produce lower bounds for attractive models [19, 17].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 16,
      "context" : "We also compared to the sum-product belief propagation algorithm, which was recently proven to produce lower bounds for attractive models [19, 17].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "Focusing on spin glass models with strong local field potentials, it is well know that one cannot produce unbiased samples from the Gibbs distributions in polynomial time [4].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 5,
      "context" : "the computational complexity of our unbiased sampling procedure to the gap between the logarithm of the partition function and its upper bound in [6].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "Sampling from the Gibbs distribution in spin glass models with non-zero local field potentials is computationally hard [7, 4].",
      "startOffset" : 119,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "Sampling from the Gibbs distribution in spin glass models with non-zero local field potentials is computationally hard [7, 4].",
      "startOffset" : 119,
      "endOffset" : 125
    }, {
      "referenceID" : 20,
      "context" : "Although we omit from the plots for clarity, our approximate sampling marginal probabilities compares those of the tree re-weighted belief propagation [21].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 20,
      "context" : "Lastly, we emphasize the importance of probabilistic reasoning over the current variational methods, such as tree re-weighted belief propagation [21] or max-marginal probabilities [9], that only generate probabilities over small subsets of variables.",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 8,
      "context" : "Lastly, we emphasize the importance of probabilistic reasoning over the current variational methods, such as tree re-weighted belief propagation [21] or max-marginal probabilities [9], that only generate probabilities over small subsets of variables.",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "To learn more about its roles in machine learning, as well as its standard samplers, we refer the interested reader to the textbook [10, 22].",
      "startOffset" : 132,
      "endOffset" : 140
    }, {
      "referenceID" : 21,
      "context" : "To learn more about its roles in machine learning, as well as its standard samplers, we refer the interested reader to the textbook [10, 22].",
      "startOffset" : 132,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "For comprehensive introduction to extreme value statistics we refer the reader to [13].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "The Gibbs distribution and its partition function can be realized from the statistics of random MAP perturbations with the Gumbel distribution (see Theorem 1), [13, 16, 20, 6].",
      "startOffset" : 160,
      "endOffset" : 175
    }, {
      "referenceID" : 15,
      "context" : "The Gibbs distribution and its partition function can be realized from the statistics of random MAP perturbations with the Gumbel distribution (see Theorem 1), [13, 16, 20, 6].",
      "startOffset" : 160,
      "endOffset" : 175
    }, {
      "referenceID" : 19,
      "context" : "The Gibbs distribution and its partition function can be realized from the statistics of random MAP perturbations with the Gumbel distribution (see Theorem 1), [13, 16, 20, 6].",
      "startOffset" : 160,
      "endOffset" : 175
    }, {
      "referenceID" : 5,
      "context" : "The Gibbs distribution and its partition function can be realized from the statistics of random MAP perturbations with the Gumbel distribution (see Theorem 1), [13, 16, 20, 6].",
      "startOffset" : 160,
      "endOffset" : 175
    }, {
      "referenceID" : 14,
      "context" : "Recently, [15, 16, 20] explore the different aspects of random MAP predictions with low dimensional perturbation.",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "Recently, [15, 16, 20] explore the different aspects of random MAP predictions with low dimensional perturbation.",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "Recently, [15, 16, 20] explore the different aspects of random MAP predictions with low dimensional perturbation.",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 14,
      "context" : "[15] describe sampling from the Gaussian distribution with random Gaussian perturbations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] show that random MAP predictors with low dimensional perturbations share similar statistics as the Gibbs distribution.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] describe the Bayesian perspectives of these models and their efficient sampling procedures.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "These probability models generate samples efficiently thorough optimization: they have statistical advantages over purely variational approaches such as tree re-weighted belief propagation [21] or max-marginals [9], and they are faster than standard Gibbs samplers and Markov chain Monte Carlo approaches when MAP prediction is efficient [4, 24]",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 8,
      "context" : "These probability models generate samples efficiently thorough optimization: they have statistical advantages over purely variational approaches such as tree re-weighted belief propagation [21] or max-marginals [9], and they are faster than standard Gibbs samplers and Markov chain Monte Carlo approaches when MAP prediction is efficient [4, 24]",
      "startOffset" : 211,
      "endOffset" : 214
    }, {
      "referenceID" : 3,
      "context" : "These probability models generate samples efficiently thorough optimization: they have statistical advantages over purely variational approaches such as tree re-weighted belief propagation [21] or max-marginals [9], and they are faster than standard Gibbs samplers and Markov chain Monte Carlo approaches when MAP prediction is efficient [4, 24]",
      "startOffset" : 338,
      "endOffset" : 345
    }, {
      "referenceID" : 23,
      "context" : "These probability models generate samples efficiently thorough optimization: they have statistical advantages over purely variational approaches such as tree re-weighted belief propagation [21] or max-marginals [9], and they are faster than standard Gibbs samplers and Markov chain Monte Carlo approaches when MAP prediction is efficient [4, 24]",
      "startOffset" : 338,
      "endOffset" : 345
    }, {
      "referenceID" : 5,
      "context" : "Our suggested samplers for the Gibbs distribution are based on low dimensional representation of the partition function, [6].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "Corollary 2 shows that the approximation scheme of [6] is in fact a lower bound that holds in probability.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "Structured mean-field methods are inner-bound methods where a simpler distribution is optimized as an approximation to the posterior in a KL-divergence sense [8, 1, 14].",
      "startOffset" : 158,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : "Structured mean-field methods are inner-bound methods where a simpler distribution is optimized as an approximation to the posterior in a KL-divergence sense [8, 1, 14].",
      "startOffset" : 158,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "Structured mean-field methods are inner-bound methods where a simpler distribution is optimized as an approximation to the posterior in a KL-divergence sense [8, 1, 14].",
      "startOffset" : 158,
      "endOffset" : 168
    }, {
      "referenceID" : 18,
      "context" : "Surprisingly, [19, 17] have shown that the sum-product belief propagation provides a lower bound to the partition function for super-modular potential functions.",
      "startOffset" : 14,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "Surprisingly, [19, 17] have shown that the sum-product belief propagation provides a lower bound to the partition function for super-modular potential functions.",
      "startOffset" : 14,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "Following [16, 20], we showed here that one can take advantage of efficient MAP solvers to generate approximate or unbiased samples from the Gibbs distribution, when we randomly perturb the potential function.",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 19,
      "context" : "Following [16, 20], we showed here that one can take advantage of efficient MAP solvers to generate approximate or unbiased samples from the Gibbs distribution, when we randomly perturb the potential function.",
      "startOffset" : 10,
      "endOffset" : 18
    } ],
    "year" : 2013,
    "abstractText" : "In this paper we describe how MAP inference can be used to sample efficiently from Gibbs distributions. Specifically, we provide means for drawing either approximate or unbiased samples from Gibbs’ distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical “high signal high coupling” regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds.",
    "creator" : "LaTeX with hyperref package"
  }
}