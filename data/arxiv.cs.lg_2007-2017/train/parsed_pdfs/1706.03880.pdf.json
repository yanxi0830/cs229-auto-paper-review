{
  "name" : "1706.03880.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MNL-Bandit: A Dynamic Learning Approach to Assortment Selection *",
    "authors" : [ "Shipra Agrawal", "Vashist Avadhanula", "Vineet Goyal", "Assaf Zeevi" ],
    "emails" : [ "sa3305@columbia.edu", "vavadhanula18@gsb.columbia.edu", "vg2277@columbia.edu", "assaf@gsb.columbia.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "SUBMITTED TO OPERATIONS RESEARCH Vol. 00, No. 0, Xxxxx 0000, pp. 000–000 issn 0030-364X |eissn 1526-5463 |00 |0000 |0001\nINFORMS doi 10.1287/xxxx.0000.0000\nc© 0000 INFORMS\nMNL-Bandit: A Dynamic Learning Approach to Assortment\nSelection *\nShipra Agrawal Industrial Engineering and Operations Research, Columbia University, New York, NY. sa3305@columbia.edu Vashist Avadhanula Decision Risk and Operations, Columbia Business School, New York, NY. vavadhanula18@gsb.columbia.edu Vineet Goyal Industrial Engineering and Operations Research, Columbia University, New York, NY. vg2277@columbia.edu\nAssaf Zeevi Decision Risk and Operations, Columbia Business School, New York, NY. assaf@gsb.columbia.edu\nWe consider a dynamic assortment selection problem, where in every round the retailer offers a subset (assortment) of N substitutable products to a consumer, who selects one of these products according to a multinomial logit (MNL) choice model. The retailer observes this choice and the objective is to dynamically learn the model parameters, while optimizing cumulative revenues over a selling horizon of length T . We refer to this exploration-exploitation formulation as the MNL-Bandit problem. Existing methods for this problem follow an explore-then-exploit approach, which estimate parameters to a desired accuracy and then, treating these estimates as if they are the correct parameter values, offers the optimal assortment based on these estimates. These approaches require certain a priori knowledge of “separability,” determined by the true parameters of the underlying MNL model, and this in turn is critical in determining the length of the exploration period. (Separability refers to the distinguishability of the true optimal assortment from the other sub-optimal alternatives.) In this paper, we give an efficient algorithm that simultaneously explores and exploits, achieving performance independent of the underlying parameters. The algorithm can be implemented in a fully online manner, without knowledge of the horizon length T . Furthermore, the algorithm is adaptive in the sense that its performance is near-optimal in both the “well separated” case, as well as the general parameter setting where this separation need not hold.\nKey words : Exploration-Exploitation, assortment optimization, upper confidence bound, multinomial logit\n∗ A preliminary version of the paper titled “A Near-Optimal Exploration-Exploitation Approach for Assortment\nSelection” appeared in the proceedings of ACM conference on Economics and Computation (EC) 2016\n1\nar X\niv :1\n70 6.\n03 88\n0v 1\n[ cs\n.L G\n] 1\n3 Ju\nn 20\n17"
    }, {
      "heading" : "1. Introduction",
      "text" : ""
    }, {
      "heading" : "1.1. Overview of the problem.",
      "text" : "Assortment optimization problems arise widely in many industries including retailing and online advertising where the seller needs to select a subset of items to offer from a universe of substitutable items such that the expected revenue is maximized. Choice models capture substitution effects among products by specifying the probability that a consumer selects a product given the offered set. Traditionally, the assortment decisions are made at the start of the selling period based on the estimated choice model from historical data; see Kök and Fisher (2007) for a detailed review.\nIn this work, we focus on the dynamic version of the problem where the retailer needs to simultaneously learn consumer preferences and maximize revenue. In many business applications such as fast fashion and online retail, new products can be introduced or removed from the offered assortments in a fairly frictionless manner and the selling horizon for a particular product can be short. Therefore, the traditional approach of first estimating the choice model and then using a static assortment based on the estimates, is not practical in such settings. Rather, it is essential to experiment with different assortments to learn consumer preferences, while simultaneously attempting to maximize immediate revenues. Suitable balancing of this exploration-exploitation tradeoff is the focal point of this paper.\nWe consider a stylized dynamic optimization problem that captures some salient features of this application domain, where our goal is to develop an exploration-exploitation policy that simultaneously learns from current observations and exploits this information gain for future decisions. In particular, we consider a constrained assortment selection problem under the Multinomial logit (MNL) model with N substitutable products and a “no purchase” option. Our goal is to offer a sequence of assortments, S1, . . . , ST , where T is the planning horizon, such that the cumulative expected revenues over said horizon is maximized, or alternatively, minimizing the gap between the performance of a proposed policy and that of an oracle that knows instance parameters a priori, a quantity referred to as the regret.\nRelated literature. The Multinomial Logit model (MNL), owing primarily to its tractability, is the most widely used choice model for assortment selection problems. (The model was introduced independently by Luce (1959) and Plackett (1975), see also Train (2009), McFadden (1978), Ben-Akiva and Lerman (1985) for further discussion and survey of other commonly used choice models.) If the consumer preferences (MNL parameters in our setting) are known a priori, then the problem of computing the optimal assortment, which we refer to as the static assortment optimization problem, is well studied. Talluri and van Ryzin (2004) consider the unconstrained assortment planning problem under the MNL model and present a greedy approach to obtain the optimal\nassortment. Recent works of Davis et al. (2013) and Désir and Goyal (2014) consider assortment\nplanning problems under MNL with various constraints. Other choice models such as Nested Logit\n(Williams (1977), Davis et al. (2011), Gallego and Topaloglu (2014) and Li et al. (2015)), Markov\nChain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013)\nand Gallego et al. (2014)) are also considered in the literature.\nMost closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong\net al. (2010) and Sauré and Zeevi (2013), where information on consumer preferences is not known\nand needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the\nsetting under which demand for products is independent of each other. Rusmevichientong et al.\n(2010) and Sauré and Zeevi (2013) consider the problem of minimizing regret under the MNL\nchoice model and present an “explore first and exploit later” approach. In particular, a selected set\nof assortments are explored until parameters can be estimated to a desired accuracy and then the\noptimal assortment corresponding to the estimated parameters is offered for the remaining selling\nhorizon. The exploration period depends on certain a priori knowledge about instance parameters.\nAssuming that the optimal and next-best assortment are “well separated,” they show an asymptotic\nO(N logT ) regret bound. However, their algorithm relies crucially on the a priori knowledge of\ncertain instance parameters which is not readily available in practice. Furthermore, their policies\nalso require a priori knowledge of the length of the planning horizon. In this work, we focus on\napproaches that simultaneously explore and exploit demand information and do not require any\nsuch a priori knowledge or assumptions; thereby, making our approach more universal in its scope.\nOur problem is closely related to the multi-armed bandit (MAB) paradigm. A naive mapping to\nthat setting would consider every assortment as an arm, and as such, would lead to exponentially\nmany arms. Popular extensions of MAB for large scale problems include the linear bandit (e.g., Auer\n(2003), Rusmevichientong and Tsitsiklis (2010)) and generalized linear bandit (Filippi et al. (2010))\nproblems. However, these do not apply directly to our problem, since the revenue corresponding\nto an assortment is nonlinear in problem parameters. Other works (see Chen et al. (2013)) have\nconsidered versions of MAB where one can play a subset of arms in each round and the expected\nreward is a function of rewards for the arms played. However, this approach assumes that the\nreward for each arm is generated independently of the other arms in the subset. This is not the case\ntypically in retail settings, and in particular, in the MNL choice model where purchase decisions\ndepend on the assortment of products offered in a time step. In this work, we use the structural\nproperties of the MNL model, along with techniques from MAB literature, to optimally explore\nand exploit in the presence of a large number of alternatives (assortments)."
    }, {
      "heading" : "1.2. Contributions",
      "text" : "Parameter independent online algorithm and regret bounds. We give an efficient online algorithm that judiciously balances the exploration and exploitation trade-off intrinsic to our problem and achieves a worst-case regret bound of O( √ NT logT ) under a mild assumption, namely that the no-purchase is the most “frequent” outcome. Our algorithm is online in the sense that it does not require any prior knowledge of the instance parameters or the time horizon, T. Moreover, the regret bound we present for this algorithm is non-asymptotic and parameter independent. The “big-oh” notation is used for brevity and only hides absolute constants. To the best of our knowledge, this is the first such policy to have a parameter independent regret bound for the MNL choice model. The assumption regarding no-purchase is quite natural and a norm in online retailing for example. Furthermore, we can establish a similar regret bound when this assumption is relaxed.\nWe also show that for “well separated” instances, the regret of our policy is bounded by O ( min ( N 2 logT/∆, √ NT )) where ∆ is the “separability” parameter. This is comparable to the bounds established in Sauré and Zeevi (2013) and Rusmevichientong et al. (2010), even though we do not require any prior information on ∆ unlike the aforementioned work. It is also interesting to note that the regret bounds hold true for a large class of constraints, e.g., we can handle matroid constraints such as assignment, partition and more general totally unimodular constraints (see Davis et al. (2013)). Our algorithm is predicated on upper confidence bound (UCB) type logic, originally developed to balance the aforementioned exploration-exploitation trade-off in the context of the multi-armed bandit (MAB) problem (cf. Lai and Robbins (1985) and Auer et al. (2002)). In this paper the UCB approach, also known as optimism in the face of uncertainty, is customized to the assortment optimization problem under the MNL model.\nLower bounds. We establish a non-asymptotic lower bound for the online assortment optimization problem under the MNL model. In particular, we show that for the cardinality constrained problem\nunder the MNL model, any algorithm must incur a regret of Ω( √ NT/K), where K is the bound on the number of products that can be offered in an assortment. This bound is derived via a reduction from a parametric multi-armed bandit problem, for which such lower bounds are constructed by means of information theoretic arguments. This result establishes that our online algorithm is nearly optimal, the upper bound being within a factor of √ K of the lower bound.\nComputational study. We present a computational study that highlights several salient features of our algorithm. In particular, we test the performance of our algorithm over instances with varying degrees of separability between optimal and sub-optimal solutions and observe that the performance is bounded irrespective of the “separability parameter”. In contrast, the approach of Sauré\nand Zeevi (2013) “breaks down” and results in linear regret for some values of the “separability parameter.” We also present results from a simulated study on a real world data set.\nOutline. In Section 2, we give the precise problem formulation. In Section 3, we present our algorithm for the MNL-Bandit problem, and in Section 4, we prove the worst-case regret bound of Õ( √ NT ) for our policy. We present the modified algorithm that works for a more general class of MNL parameters and establish Õ( √ BNT ) regret bounds in Section 5. In Section 6, we present the logarithmic regret bound for our policy for the case of “well separated” instances. In Section 7, we present our non-asymptotic lower bound on regret for any algorithm for MNL-Bandit. In Section 8, we present results from our computational study."
    }, {
      "heading" : "2. Problem formulation",
      "text" : "The basic assortment problem. In our problem, at every time instance t, the seller selects an assortment St ⊂ {1, . . . ,N} and observes the customer purchase decision Ct ∈ St ∪ {0}. As noted earlier, we assume consumer preferences are modeled using a multinomial logit (MNL) model. Under this model, the probability that a consumer purchases product i at time t when offered an assortment St = S ⊂ {1, . . . ,N} is given by,\npi(S) := P (Ct = i|St = S) =  vi v0 + ∑ j∈S vj , if i∈ S ∪{0}\n0, otherwise, (2.1)\nfor all t, where vi is the attraction parameter for product i in the MNL model. The random variables {Ct : t= 1,2, . . .} are conditionally independent, namely, Ct conditioned on the event {St = S} is independent of C1, . . . ,Ct−1. Without loss of generality, we can assume that v0 = 1. It is important to note that the parameters of the MNL model vi, are not known to the seller. From (2.1), the expected revenue when assortment S is offered and the MNL parameters are denoted by the vector v is given by\nR(S,v) =E [∑ i∈S ri1{Ct = i|St = S} ] = ∑ i∈S rivi 1 + ∑ j∈S vj . (2.2)\nWe consider several naturally arising constraints over the assortments that the retailer can offer. These include cardinality constraints (where there is an upper bound on the number of products that can be offered in the assortment), partition matroid constraints (where the products are partitioned into segments and the retailer can select at most a specified number of products from each segment) and joint display and assortment constraints (where the retailer needs to decide both the assortment as well as the display segment of each product in the assortment and there is an upper bound on the number of products in each display segment). More generally, we consider the set of totally unimodular (TU) constraints on the assortments. Let x(S)∈ {0,1}N be the incidence\nvector for assortment S ⊆ {1, . . . ,N}, i.e., xi(S) = 1 if product i∈ S and 0 otherwise. We consider constraints of the form\nS = {S ⊆ {1, . . . ,N} |A x(S)≤ b, 0≤ x≤ 1} , (2.3)\nwhere A is a totally unimodular matrix. The totally unimodular constraints model a rich class of practical assortment planning problems including the examples discussed above. We refer the reader to Davis et al. (2013) for a detailed discussion on assortment and pricing optimization problems that can be formulated under the TU constraints.\nAdmissible Policies. To define the set of policies that can be used by the seller, let U be a random variable, which encodes any additional sources of randomization and (U,U ,Pu) be the corresponding probability space. We define {πt, t= 1,2, . . .} to be a measurable mapping as follows:\nπ1 :U→S\nπt :U×St−1×{0, . . . ,N}t−1→S, for each t= 2,3, . . . where S is as defined in (2.3). Then the assortment selection for the seller at time t is given by\nSt =\n{ π1(U), t= 1\nπt(U,C1, . . . ,Ct−1, S1, . . . , St−1), t= 2,3, . . . , (2.4)\nFor further reference, let {Ht : t = 1,2, . . .} denote the filtration associated with the policy π = (π1, π2, . . . , πt, . . .). Specifically,\nH1 = σ(U)\nHt = σ(U,C1, . . . ,Ct−1, S1, . . . , St−1), for each t= 2,3, . . . . We denote by Pπ{.} and Eπ{.} the probability distribution and expectation value over path space induced by the policy π.\nThe online assortment optimization problem. The objective is to design a policy π = (π1, . . . , πT ) that selects a sequence of history dependent assortments (S1, S2, . . . , ST ) so as to maximize the cumulative expected revenue,\nEπ ( T∑ t=1 R(St,v) ) , (2.5)\nwhere R(S,v) is defined as in (2.2). Direct analysis of (2.5) is not tractable given that the parameters {vi, i = 1, . . . ,N} are not known to the seller a priori. Instead we propose to measure the performance of a policy π via its regret. The objective then is to design a policy that approximately minimizes the regret defined as\nRegπ(T,v) = T∑ t=1 R(S∗,v)−Eπ[R(St,v)], (2.6)\nwhere S∗ is the optimal assortment for (2.2), namely, S∗ = argmax S∈S R(S,v). This explorationexploitation problem, which we refer to as MNL-Bandit, is the focus of this paper."
    }, {
      "heading" : "3. The proposed policy",
      "text" : "In this section, we describe our proposed policy for the MNL-Bandit problem. The policy is designed using the characteristics of the MNL model based on the principle of optimism under uncertainty."
    }, {
      "heading" : "3.1. Challenges and overview",
      "text" : "A key difficulty in applying standard multi-armed bandit techniques to this problem is that the response observed on offering a product i is not independent of other products in assortment S. Therefore, the N products cannot be directly treated as N independent arms. As mentioned before, a naive extension of MAB algorithms for this problem would treat each of the feasible assortments as an arm, leading to a computationally inefficient algorithm with exponential regret. Our policy utilizes the specific properties of the dependence structure in MNL model to obtain an efficient algorithm with order √ NT regret.\nOur policy is based on a non-trivial extension of the UCB algorithm Auer et al. (2002). It uses the past observations to maintain increasingly accurate upper confidence bounds for the MNL parameters {vi, i = 1, . . . ,N}, and uses these to (implicitly) maintain an estimate of expected revenue R(S) for every feasible assortment S. In every round, our policy picks the assortment S with the highest optimistic revenue. There are two main challenges in implementing this scheme. First, the customer response to being offered an assortment S depends on the entire set S, and does not directly provide an unbiased sample of demand for a product i∈ S. In order to obtain unbiased estimates of vi for all i ∈ S, we offer a set S multiple times: specifically, it is offered repeatedly until a no-purchase occurs. We show that proceeding in this manner, the average number of times a product i is purchased provides an unbiased estimate of the parameter vi. The second difficulty is the computational complexity of maintaining and optimizing revenue estimates for each of the exponentially many assortments. To this end, we use the structure of the MNL model and define our revenue estimates such that the assortment with maximum estimated revenue can be efficiently found by solving a simple optimization problem. This optimization problem turns out to be a static assortment optimization problem with upper confidence bounds for vi’s as the MNL parameters, for which efficient solution methods are available."
    }, {
      "heading" : "3.2. Details of the policy",
      "text" : "We divide the time horizon into epochs, where in each epoch we offer an assortment repeatedly until a no purchase outcome occurs. Specifically, in each epoch `, we offer an assortment S` repeatedly. Let E` denote the set of consecutive time steps in epoch `. E` contains all time steps after the end of epoch `− 1, until a no-purchase happens in response to offering S`, including the time step at which no-purchase happens. The length of an epoch |E`| conditioned on S` is a geometric random\nvariable with success probability defined as the probability of no-purchase in S`. The total number of epochs L in time T is implicitly defined as the minimum number for which ∑L\n`=1 |E`| ≥ T . At the end of every epoch `, we update our estimates for the parameters of MNL, which are used in epoch `+ 1 to choose assortment S`+1. For any time step t ∈ E`, let ct denote the consumer’s response to S`, i.e., ct = i if the consumer purchased product i∈ S, and 0 if no-purchase happened. We define v̂i,` as the number of times a product i is purchased in epoch `.\nv̂i,` := ∑ t∈E` 1(ct = i) (3.1)\nFor every product i and epoch `≤ L, we keep track of the set of epochs before ` that offered an assortment containing product i, and the number of such epochs. We denote the set of epochs by Ti(`) be and the number of epochs by Ti(`). That is,\nTi(`) = {τ ≤ ` | i∈ Sτ} , Ti(`) = |Ti(`)|. (3.2)\nWe compute v̄i,` as the number of times product i was purchased per epoch,\nv̄i,` = 1\nTi(`) ∑ τ∈Ti(`) v̂i,τ . (3.3)\nWe show that for all i ∈ S`, v̂i,` and v̄i,` are unbiased estimators of the MNL parameter vi (see Lemma A.1 ) Using these estimates, we compute the upper confidence bounds, vUCBi,` for vi as,\nvUCBi,` := v̄i,` + √ v̄i,` 48 log (`+ 1)\nTi(`) +\n48 log (`+ 1)\nTi(`) . (3.4)\nWe establish that vUCBi,` is an upper confidence bound on the true parameter vi, i.e., v UCB i,` ≥ vi, for all i, ` with high probability (see Lemma 4.1). The role of the upper confidence bounds is akin to their role in hypothesis testing; they ensure that the likelihood of identifying the parameter value is sufficiently large. We then offer the optimistic assortment in the next epoch, based on the previous updates as follows,\nS`+1 := argmax S∈S\nmax { R(S, v̂) : v̂i ≤ vUCBi,` } (3.5)\nwhere R(S, v̂) is as defined in (2.2). We later show that the above optimization problem is equivalent to the following optimization problem (see Lemma A.3).\nS`+1 := argmax S∈S R̃`+1(S), (3.6)\nwhere R̃`+1(S) is defined as,\nR̃`+1(S) :=\n∑ i∈S riv UCB i,`\n1 + ∑ j∈S vUCBj,` . (3.7)\nAlgorithm 1 Exploration-Exploitation algorithm for MNL-Bandit\n1: Initialization: vUCBi,0 = 1 for all i= 1, . . . ,N . 2: t= 1 ; `= 1 keeps track of the time steps and total number of epochs respectively 3: while t < T do\n4: Compute S` = argmax S∈S R̃`(S) =\n∑ i∈S riv UCB i,`−1\n1+ ∑ j∈S vUCBj,`−1\n5: Offer assortment S`, observe the purchasing decision, ct of the consumer 6: if ct = 0 then\n7: compute v̂i,` = ∑\nt∈E` 1(ct = i), no. of consumers who preferred i in epoch `, for all i∈ S`.\n8: update Ti(`) = {τ ≤ ` | i∈ S`} , Ti(`) = |Ti(`)|, no. of epochs until ` that offered product\ni.\n9: update v̄i,` = 1\nTi(`) ∑ τ∈Ti(`) v̂i,τ , sample mean of the estimates\n10: update vUCBi,` =v̄i,` + √ v̄i,` 48 log (`+ 1)\nTi(`) +\n48 log (`+ 1)\nTi(`) ; `= `+ 1\n11: else 12: E` = E` ∪ t, time indices corresponding to epoch `. 13: end if 14: t= t+ 1 15: end while\nWe summarize the steps in our policy in Algorithm 1. Finally, we may remark on the computa-\ntional complexity of implementing (3.5). The optimization problem (3.5) is formulated as a static\nassortment optimization problem under the MNL model with TU constraints, with model parameters being vUCBi,` , i = 1, . . . ,N ((3.6)). There are efficient polynomial time algorithms to solve the static assortment optimization problem under MNL model with known parameters (see Davis et al.\n(2013), Rusmevichientong et al. (2010))."
    }, {
      "heading" : "4. Main results",
      "text" : "Assumption 4.1 We make the following assumptions.\n1. The MNL parameter corresponding to any product i∈ {1, . . . ,N} satisfies vi ≤ v0 = 1. 2. The family of assortments S is such that S ∈ S and Q⊆ S implies that Q∈ S.\nThe first assumption is equivalent to the ‘no purchase option’ being preferable to any other prod-\nuct. We note that this holds in many realistic settings, in particular, in online retailing and online\ndisplay-based advertising. The second assumption implies that removing a product from a feasible\nassortment preserves feasibility. This holds for most constraints arising in practice including cardinality, and matroid constraints more generally. We would like to note that the first assumption is made for ease in presenting the key results and is not central to deriving bounds on the regret. In section 5, we relax this assumption and derive regret bounds that hold for any parameter instance.\nOur main result is the following upper bound on the regret of the policy stated in Algorithm 1.\nTheorem 1 For any instance v = (v0, . . . , vN) of the MNL-Bandit problem with N products, ri ∈ [0,1] under Assumption 4.1, the regret of the policy given by Algorithm 1 at time T is bounded as,\nRegπ(T,v)≤C1 √ NT logT +C2N log 2 T,\nwhere C1 and C2 are absolute constants (independent of problem parameters)."
    }, {
      "heading" : "4.1. Proof Outline",
      "text" : "In this section, we provide an outline of different steps involved in proving Theorem 1.\nConfidence intervals. The first step in our regret analysis is to prove the following two properties of the estimates vUCBi,` computed as in (3.4) for each product i. Specifically, that vi is bounded by vUCBi,` with high probability, and that as a product is offered more and more times, the estimates vUCBi,` converge to the true value with high probability. Intuitively, these properties establish v UCB i,` as upper confidence bounds converging to actual parameters vi, akin to the upper confidence bounds used in the UCB algorithm for MAB in Auer et al. (2002). We provide the precise statements for the above mentioned properties in Lemma 4.1. These properties follow from an observation that is conceptually equivalent to the IIA (Independence of Irrelevant Alternatives) property of MNL. This observation shows that in each epoch τ , v̂i,τ (the number of purchases of product i) provides an independent unbiased estimates of vi. Intuitively, v̂i,τ is the ratio of probabilities of purchasing product i to preferring product 0 (no-purchase), which is independent of Sτ . This also explains why we choose to offer Sτ repeatedly until no-purchase occurs. Given these unbiased i.i.d. estimates from every epoch τ before `, we apply a multiplicative Chernoff-Hoeffding bound to prove concentration of v̄i,`.\nCorrectness of the optimistic assortment. The product demand estimates vUCBi,`−1 were used in (3.7) to define expected revenue estimates R̃`(S) for every set S. In the beginning of every epoch `, Algorithm 1 computes the optimistic assortment as S` = arg maxS R̃`(S), and then offers S` repeatedly until no-purchase happens. The next step in the regret analysis is to use above properties of vUCBi,` to prove similar, though slightly weaker, properties for the estimates R̃`(S). First, we show that estimated revenue is an upper confidence bound on the optimal revenue, i.e. R(S∗,v) is\nbounded by R̃`(S`) with high probability. The proof for these properties involves careful use of the structure of MNL model to show that the value of R̃`(S`) is equal to the highest expected revenue achievable by any feasible assortment, among all instances of the problem with parameters in the range [0, vUCBi ], i= 1, . . . , n. Since the actual parameters lie in this range with high probability, we have R̃`(S`) is at least R(S ∗,v) with high probability. Lemma 4.2 provides the precise statement.\nBounding the regret. The final part of our analysis is to bound the regret in each epoch. First, we use the above property to bound the loss due to offering the optimistic assortment S`. In particular, we show that the loss is bounded by the difference between optimistic estimated revenue, R̃`(S`), and actual expected revenue, R(S`). We then prove a Lipschitz property of the expected revenue function to bound the difference between optimistic estimate and expected revenues in terms of errors in individual product estimates |vUCBi,` − vi|. Finally, we leverage the structure of MNL model and the properties of vUCBi,` to bound the regret in each epoch. Lemma 4.3 provide the precise statements of above properties.\nGiven the above properties, the rest of the proof is relatively straightforward. In rest of this section, we make the above notions precise. Finally, in Appendix A.3, we utilize these properties to complete the proof of Theorem 1."
    }, {
      "heading" : "4.2. Upper confidence bounds",
      "text" : "In this section, we will show that the upper confidence bounds vUCBi,` converge to the true parameters vi from above. Specifically, we have the following result.\nLemma 4.1 For every `, we have:\n1. vUCBi,` ≥ vi with probability at least 1− 5` for all i= 1, . . . ,N . 2. There exists constants C1 and C2 such that\nvUCBi,` − vi ≤C1\n√ vi log (`+ 1)\nTi(`) +C2\nlog (`+ 1)\nTi(`)\nwith probability at least 1− 5 ` .\nWe first establish that the estimates v̂i,`, `≤ L are unbiased i.i.d estimates of the true parameter vi for all products. It is not immediately clear a priori if the estimates v̂i,`, `≤L are independent. In our setting, it is possible that the distribution of estimate v̂i,` depends on the offered assortment S`, which in turn depends on the history and therefore, previous estimates, {v̂i,τ , τ = 1, . . . , `− 1}. In Lemma A.1, we show that the moment generating of v̂i,` conditioned on S` only depends on the parameter vi and not on the offered assortment S`, there by establishing that estimates are\nindependent and identically distributed. Using the moment generating function, we show that v̂i,` is a geometric random variable with mean vi, i.e., E(v̂i,`) = vi. We will use this observation and extend the multiplicative Chernoff-Hoeffding bounds discussed in Mitzenmacher and Upfal (2005) and Babaioff et al. (2015) to geometric random variables and derive the result. The proof is provided in Appendix A.2"
    }, {
      "heading" : "4.3. Optimistic estimate and convergence rates",
      "text" : "In this section, we show that the estimated revenue converges to the optimal expected revenue from above. First, we show that the estimated revenue is an upper confidence bound on the optimal revenue. In particular, we have the following result.\nLemma 4.2 Suppose S∗ ∈ S is the assortment with highest expected revenue, and Algorithm 1 offers S` ∈ S in each epoch `. Then, for every epoch `, we have\nR̃`(S`)≥ R̃`(S∗)≥R(S∗,v) with probability at least 1− 5\n` .\nIn Lemma A.3, we show that the optimal expected revenue is monotone in the MNL parameters. It is important to note that we do not claim that the expected revenue is in general a monotone function, but only value of the expected revenue corresponding to optimal assortment increases with increase in MNL parameters. The result follows from Lemma 4.1, where we established that vUCBi,` > vi with high probability. We provide the detailed proof in Appendix A.2.\nThe following result provides the convergence rates of the estimate R̃`(S`) to the optimal expected revenue.\nLemma 4.3 There exists constants C1 and C2 such that for every `, we have\n(1 + ∑\nj∈S` vj)(R̃`(S`)−R(S`,v))≤C1\n√ vi log (`+1)\n|Ti(`)| +C2\nlog (`+1)\n|Ti(`)| ,\nwith probability at least 1− 5 `\nIn Lemma A.4, we show that the expected revenue function satisfies a certain kind of Lipschitz condition. Specifically, the difference between estimated, R̃`(S`), and expected revenues, R`(S`), is bounded by the sum of errors in parameter estimates for the products, |vUCBi,` −vi|. This observation in conjunction with the “optimistic estimates” property will let us bound the regret as an aggregated difference between estimated revenues and expected revenues of the offered assortments. Noting that we have already computed convergence rates between the parameter estimates earlier, we can extend them to show that the estimated revenues converge to the optimal revenue from above. We complete the proof in Appendix A.2."
    }, {
      "heading" : "5. Extensions",
      "text" : "In this section, we extend our approach (Algorithm 1) to the setting where the assumption that vi ≤ v0 for all i is relaxed. The essential ideas in the extension remain the same as our earlier approach, specifically optimism under uncertainty, and our policy is structurally similar to Algorithm 1. The modified policy requires a small but mandatory initial exploration period. However, unlike the works of Rusmevichientong et al. (2010) and Sauré and Zeevi (2013), the exploratory period does not depend on the specific instance parameters and is constant for all problem instances. Therefore, our algorithm is parameter independent and remains relevant for practical applications. Moreover, our approach continues to simultaneously explore and exploit after the initial exploratory phase. In particular, the initial exploratory phase is to ensure that the estimates converge to the true parameters from above particularly in cases when the attraction parameter, vi (frequency of purchase), is large for certain products. We describe our approach in Algorithm 2.\nWe can extend the analysis in Section 4 to bound the regret of Algorithm 2 as follows.\nTheorem 2 For any instance v = (v0, . . . , vN), of the MNL-Bandit problem with N products, ri ∈ [0,1] for all i= 1, . . . ,N , the regret of the policy corresponding to Algorithm 2 at time T is bounded as,\nRegπ(T,v)≤C1 √ BNT logT +C2N log 2 T +C3NB logT,\nwhere C1, C2 and C3 are absolute constants and B = max{maxi viv0 ,1}.\nProof outline. Note that the Algorithm 2 is very similar to Algorithm 1 except for the initial exploratory phase. Hence, to bound the regret, we first prove that the initial exploratory phase is indeed bounded and then follow the approach discussed in Section 4 to establish the correctness of confidence intervals, the optimistic assortment and finally deriving the convergence rates and regret bounds. Given the above properties, the rest of the proof is relatively straightforward. We make the above notions precise and provide the complete proof in Appendix B."
    }, {
      "heading" : "6. Parameter dependent regret bounds",
      "text" : "In this section, we derive an O(logT ) regret bound for Algorithm 1 that is parameter dependent. In Section 4 and 5, we established worst case regret bounds for Algorithm 1 that hold for all problem instances. Although our algorithm ensures that the exploration-exploitation tradeoff is balanced at all times, for problem instances that are “well separated”, our algorithm quickly converges to the optimal solution leading to better regret bounds. More specifically, we consider problem instances, where the optimal assortment and “second best” assortment are sufficiently “separated” and derive a O(logT ) regret bound that depends on the parameters of the instance. Note that, unlike the\nAlgorithm 2 Exploration-Exploitation algorithm for MNL-Bandit general parameters\n1: Initialization: vUCBi,0 = 1 for all i= 1, . . . ,N . 2: t= 1 ; `= 1 keeps track of the time steps and total number of epochs respectively 3: Ti(1) = 0 for all i= 1, . . . ,N . 4: while t < T do\n5: Compute S` = argmax S∈S R̃`(S) =\n∑ i∈S riv UCB i,`−1\n1+ ∑ j∈S vUCBj,`−1\n6: if Ti(`)< 96 log (`+ 1) for some i∈ S` then 7: Consider Ŝ ={i|Ti(`)< 48 log (`+ 1)}. 8: Chose S` ∈ S such that S` ⊂ Ŝ. 9: end if\n10: Offer assortment S`, observe the purchasing decision, ct of the consumer 11: if ct = 0 then\n12: compute v̂i,` = ∑\nt∈E` 1(ct = i), no. of consumers who preferred i in epoch `, for all i∈ S`.\n13: update Ti(`) = {τ ≤ ` | i∈ S`} , Ti(`) = |Ti(`)|, no. of epochs until ` that offered product i. 14: update v̄i,` = 1\nTi(`) ∑ τ∈Ti(`) v̂i,τ , sample mean of the estimates\n15: update vUCB2i,` =v̄i,` + max {√ v̄i,`, v̄i,` }√ 48 log (`+1) Ti(`) + 48 log (`+1) Ti(`) 16: `= `+ 1 17: else 18: E` = E` ∪ t, time indices corresponding to epoch `. 19: t= t+ 1 20: end if 21: end while\nparameter-independent bound derived in Section 4, the bound we derive only holds for sufficiently large T and is dependent on the separation between the revenues corresponding optimal and second best assortments. In particular, let ∆ denote the difference between the expected revenues of the optimal and second-best assortment, i.e.,\n∆(v) = min {S∈S|R(S) 6=R(S∗,v)}\n{R(S∗,v)−R(S)},\nWe have the following result.\nTheorem 3 For any instance, v = (v0, . . . , vN) of the MNL-Bandit problem with N products, ri ∈ [0,1] and v0 ≥ vi for i= 1, . . . ,N , there exists finite constants B1 and B2 such that the regret of the policy defined in Algorithm 1 at time T is bounded as,\nRegπ(T,v)≤B1 ( N 2 logT\n∆(v)\n) +B2."
    }, {
      "heading" : "6.1. Proof outline.",
      "text" : "In this setting, we analyze the regret by separately considering the epochs that satisfy certain desirable properties and the ones that do not. Specifically, we denote, epoch ` as a “good” epoch, if the parameters, vUCBi,` satisfy the following property,\n0≤ vUCBi,` − vi ≤C1\n√ vi log (`+ 1)\nTi(`) +C2\nlog (`+ 1)\nTi(`) ,\nand “bad” epoch, otherwise, where C1 and C2 are constants as defined in Lemma 4.1. Note that every epoch ` is a good epoch with high probability (1− 5 ` ) and we show that regret due to bad epochs is bounded by a constant (see Appendix C). Therefore, we focus on good epochs and show that there exists a constant τ , such that after each product has been offered in at least τ good epochs, Algorithm 1 finds the optimal assortment with high probability. Based on this result, we can then bound the total number of good epochs in which a sub-optimal assortment can be offered by our algorithm. Specifically, let\nτ = 4NC logT\n∆2(v) , (6.1)\nwhere C = max{C1,C2}. Then we have the following result.\nLemma 6.1 Let ` be a good epoch and S` be the assortment offered by Algorithm 1 in epoch ` and if every product in assortment S` is offered in at least τ good epochs, i.e. Ti(`)≥ τ for all i, then we have R(S`,v) =R(S ∗,v) .\nProof. From Lemma 4.3, we have,\nR(S∗,v)−R(S`,v)≤ 1\nV (S`) + 1 ∑ i∈S`\n( C1 √ vi log (`+ 1)\nTi(`) +C2\nlog (`+ 1)\nTi(`)\n)\n≤ ∆(v)\n∑ i∈S` √ vi\n2 √ |S`|(V (S`) + 1) ≤ ∆(v) 2 .\nThe result follows from the definition of ∆(v). The next step in the analysis is to show that Algorithm 1 will offer a small number of sub-optimal assortments in good epochs. We make this precise in the following result, the proof is a simple counting exercise using Lemma 6.1 and is completed in Appendix C.\nLemma 6.2 Algorithm 1 cannot offer sub-optimal assortment in more than N(N−1) 2 τ good epochs.\nThe proof for Theorem 3 follows from the above result. In particular, noting that the number of epochs in which sub-optimal assortment is offered is small, we re-use the regret analysis of Section 4 to bound the regret by O(N 2 logT ). We provide the rigorous proof in Appendix C for the sake of completeness. Noting that for the special case of cardinality constraints, we have |S`| ≤K for every epoch `, we can improve the regret bound to O(N 3/2K1/2 logT ). Specifically,\nCorollary 6.1 For any instance, v = (v0, . . . , vN) of the MNL-Bandit problem with N products and cardinality constraint K, ri ∈ [0,1] and v0 ≥ vi for i= 1, . . . ,N , there exists finite constants B1 and B2 such that the regret of the policy defined in Algorithm 1 at time T is bounded as,\nRegπ(T,v)≤B1 N\n3 2K 1 2 logT\n∆(v) +B2.\nIt should be noted that the bound obtained in Corollary 6.1 is similar in magnitude to the regret bounds obtained by the approaches of Rusmevichientong et al. (2010) and Sauré and Zeevi (2013) for the cardinality constrained problem. (In fact our algorithm also has improved regret bounds compared to the O(N 2 log2 T ) bound established by Rusmevichientong et al. (2010)). It is also important to note that our algorithm is independent of the parameter ∆(v) unlike the existing work which requires a conservative estimate of ∆(v) to implement a policy."
    }, {
      "heading" : "7. Lower bounds and near-optimality of the proposed policy",
      "text" : "In this section, we consider the special case of TU constraints, namely, cardinality constrained assortment optimization problem and establish that any policy must incur a regret of Ω( √ NT/K). More precisely, we prove the following result.\nTheorem 4 There exists a (randomized) instance of the MNL-Bandit problem with v0 ≥ vi , i = 1, . . . ,N , such that for any N , K <N , T ≥N , and any policy π that offers assortment Sπt , |Sπt | ≤K at time t, we have\nRegπ(T,v) :=Eπ ( T∑ t=1 R(S∗,v)−R(Sπt ,v) ) ≥C √ NT K\nwhere S∗ is (at-most) K-cardinality assortment with maximum expected revenue, and C is an absolute constant."
    }, {
      "heading" : "7.1. Proof overview",
      "text" : "We prove Theorem 4 by a reduction to a parametric multi-armed bandit (MAB) problem, for which a lower bound is known.\nDefinition 7.1 (MAB instance IMAB) Define IMAB as a (randomized) instance of MAB problem with N ≥ 2 Bernoulli arms, and following parameters (probability of reward 1)\nµi = { α, if i 6= j, α+ , if i= j,\nfor all i= 1, . . . ,N,\nwhere j is set uniformly at random from {1, . . . ,N}, α< 1 and = 1 100 √ Nα T .\nThroughout this section, we will use the terms algorithm and policy interchangeably. An algorithm A is referred to as online if it adaptively selects a history dependent At ∈ {1, . . . , n} at each time t on the lines of (2.4) for the MAB problem.\nLemma 7.1 For any N ≥ 2, α < 1, T and any online algorithm A that plays arm At at time t, the expected regret on instance IMAB is at least T\n6 . That is,\nRegA(T,µ) :=E [ T∑ t=1 (µj −µAt) ] ≥ T 6 ,\nwhere, the expectation is both over the randomization in generating the instance (value of j), as well as the random outcomes that result from pulled arms. The proof of Lemma 7.1 is a simple extension of the proof of the Ω( √ NT ) lower bound for the Bernoulli instance with parameters 1 2 and 1 2 + (for example, see Bubeck and Cesa-Bianchi (2012)), and has been provided in Appendix E for the sake of completeness. We use the above lower bound\nfor the MAB problem to prove that any algorithm must incur at least Ω( √ NT/K) regret on the following instance of the MNL-Bandit problem.\nDefinition 7.2 (MNL-Bandit instance IMNL) Define IMNL as the following (randomized) instance of MNL-Bandit problem with K-cardinality constraint, N̂ = NK products, parameters v0 =K and for i= 1, . . . , N̂ ,\nvi =\n{ α, if d i\nK e 6= j,\nα+ , if d i K e= j,\nwhere j is set uniformly at random from {1, . . . ,N}, α< 1, and = 1 100 √ Nα T .\nWe will show that any MNL-Bandit algorithm has to incur a regret of Ω (√\nNT K\n) on instance\nIMNL. The main idea in our reduction is to show that if there exists an algorithm AMNL for MNLBandit that achieves o( √\nNT K ) regret on instance IMNL, then we can use AMNL as a subroutine\nto construct an algorithm AMAB for the MAB problem that achieves strictly less than T6 regret on instance IMAB in time T , thus contradicting the lower bound of Lemma 7.1. This will prove Theorem 4 by contradiction.\nAlgorithm 3 Algorithm AMAB 1: Initialization: t= 0, `= 0.\n2: while t≤ T do 3: Update `= `+ 1; 4: Call AMNL, receive assortment S` ⊂ [N̂ ]; 5: Repeat until ‘exit loop’ 6: With probability 1 2 , send Feedback to AMNL ‘no product was purchased’, exit loop; 7: Update t= t+ 1; 8: Pull arm At = d iK e,where i∈ S` is chosen uniformly at random. 9: If reward is 1, send Feedback to AMNL ‘i was purchased’ and exit loop;\n10: end loop 11: end while"
    }, {
      "heading" : "7.2. Construction of the MAB algorithm using the MNL algorithm",
      "text" : "Algorithm 3 provides the exact construction of AMAB, which simulates AMNL algorithm as a “black-box”. Note that AMAB pulls arms at time steps t= 1, . . . , T . These arm pulls are interleaved by simulations of AMNL steps (Call AMNL , Feedback to AMNL ). When step ` of AMNL is simulated, it uses the feedback from 1, . . . , `−1 to suggest an assortment S`; and recalls a feedback from AMAB on which product (or no product) was purchased out of those offered in S`, where the probability of purchase of product i ∈ S` is be vi / (v0 + ∑ i∈S` vi). Before showing that the AMAB indeed provides the right feedback to AMNL in the `th step for each `, we introduce some notation.\nLet M` denote the number of times no arm is pulled or arm d iK e is pulled for some i ∈ S` by AMAB before exiting the loop. For every i ∈ S` ∪ 0, let ζi` denote the event that the feedback to AMNL from AMAB after step ` of AMNL is “product i is purchased”. We have,\nP(M` =m ∩ ζi`) = vi\n2K\n( 1\n2K ∑ i∈S` (1− vi)\n)m−1 for each i∈ S` ∪{0}.\nHence, the probability that AMAB ’s feedback to AMNL is “product i is purchased” is,\npS`(i) = ∞∑ m=1 P(M` =m ∩ ζi`) = vi v0 + ∑ q∈S` vq .\nThis establish that AMAB provides the appropriate feedback to AMNL ."
    }, {
      "heading" : "7.3. Proof of Theorem 4.",
      "text" : "We prove the result by establishing three key results. First, we upper bound the regret for the MAB algorithm, AMAB . Then, we prove a lower bound on the regret for the MNL algorithm,\nAMNL . Finally, we relate the regret of AMAB and AMNL and use the established lower and upper bounds to show a contradiction. For the rest of this proof, assume that L is the total number of calls to AMNL in AMAB . Let S∗ be the optimal assortment for IMNL. For any instantiation of IMNL, it is easy to see that the optimal assortment contains K items, all with parameter α+ , i.e., it contains all i such that d i K e = j. Therefore, V (S∗) =K(α+ ) =Kµj.\nUpper bound for the regret of the MAB algorithm. The first step in our analysis is to prove an upper bound on the regret of the MAB algorithm, AMAB on the instance IMAB. Let us label the loop following the `th call to AMNL in Algorithm 3 as `th loop. Note that the probability of exiting the loop is p=E[ 1 2 + 1 2 µAt ] = 1 2 + 1 2K V (S`). In every step of the loop until exited, an arm is pulled with probability 1/2. The optimal strategy would pull the best arm so that the total\nexpected optimal reward in the loop is ∑∞\nr=1(1− p)r−1 1 2 µj = µj 2p = 1 2Kp V (S∗). Algorithm AMAB pulls a random arm from S`, so total expected algorithm’s reward in the loop is ∑∞ r=1(1−p)r−1 1 2K V (S`) =\n1 2Kp V (S`). Subtracting the algorithm’s reward from the optimal reward, and substituting p, we obtain that the total expected regret of AMAB over the arm pulls in loop ` is\nV (S∗)−V (S`) (K +V (S`)) .\nNoting that V (S`)≥Kα, we have the following upper bound on the regret for the MAB algorithm.\nRegAMAB (T,µ)≤ 1\n(1 +α) E ( L∑ `=1 1 K (V (S∗)−V (S`)) ) , (7.1)\nwhere the expectation in equation (7.1) is over the random variables L and S`.\nLower bound for the regret of the MNL algorithm. Here, using simple algebraic manipulations we derive a lower bound on the regret of the MNL algorithm, AMNL on the instance IMNL. Specifically,\nRegAMNL (L,v) = E [ L∑ `=1 V (S∗) v0 +V (S∗) − V (S`) v0 +V (S`) ]\n≥ 1 K(1 +α) E [ L∑ `=1 ( V (S∗) 1 + 1+α −V (S`) )]\nTherefore, it follows that,\nRegAMNL (L,v)≥ 1\n(1 +α) E [ L∑ `=1 1 K (V (S∗)−V (S`))−\nv∗L\n(1 +α)2\n] , (7.2)\nwhere the expectation in equation (7.2) is over the random variables L and S`.\nRelating the regret of MNL algorithm and MAB algorithm. Finally, we relate the regret of MNL algorithm AMNL and MAB algorithm AMAB to derive a contradiction. The first step in relating the regret involves relating the length of the horizons of AMNL and AMAB , L and T respectively. Intuitvely, after any call to AMNL (“Call AMNL ” in Algorithm 3), many iterations of the following loop may be executed; in roughly 1/2 of those iterations, an arm is pulled and t is advanced (with probability 1/2, the loop is exited without advancing t). Therefore, T should be at least a constant fraction of L. Lemma E.3 in Appendix makes this intuition precise by showing that E(L)≤ 3T .\nNow we are ready to prove Theorem 4. From (7.1) and (7.2), we have RegAMAB (T,µ)≤E ( RegAMNL (L,v) + v∗L\n(1 +α)2\n) .\nFor the sake of contradiction, suppose that the regret of the AMNL , RegAMNL (L,v)≤ c √ N̂L K for a constant c to be prescribed below. Then, from Jensen’s inequality, it follows that,\nRegAMAB (T,µ) ≤ c √ N̂E(L) K + v∗E(L) (1 +α)2\nFrom lemma E.3, we have that E(L)≤ 3T . Therefore, we have, c √\nN̂E(L) K\n= c √ NE(L)≤ c √ 3NT =\nc T √\n3 α < T 12 on setting c < 1 12 √ α 3 . Also, using v∗ = α+ ≤ 2α, and setting α to be a small enough\nconstant, we can get that the second term above is also strictly less than T 12 . Combining these observations, we have RegAMAB (T,µ)< T 12 + T 12 = T 6 , thus arriving at a contradiction."
    }, {
      "heading" : "8. Computational study",
      "text" : "In this section, we present insights from numerical experiments that test the empirical performance of our policy and highlight some of its salient features. We study the performance of Algorithm 1 from the perspective of robustness with respect to the “separability parameter” of the underlying instance. In particular, we consider varying levels of separation between the optimal revenue and the second best revenue and perform a regret analysis numerically. We contrast the performance of Algorithm 1 with the approach in Sauré and Zeevi (2013) for different levels of separation between the optimal and sub-optimal revenues. We observe that when the separation between the optimal assortment and second best assortment is sufficiently small, the approach in Sauré and Zeevi (2013) breaks down, i.e., incurs linear regret, while the regret of Algorithm 1 only grows sub-linearly with respect to the selling horizon. We also present results from a simulated study on a real world data set."
    }, {
      "heading" : "8.1. Robustness of Algorithm 1.",
      "text" : "Here, we present a study that examines the robustness of Algorithm 1 with respect to the instance separability. We consider a parametric instance (see Example 8.1), where the separation between the revenues of the optimal assortment and next best assortment is specified by the parameter and compare the performance of Algorithm 1 for different values of .\nExperimental setup. We consider the parametric MNL setting with N = 10, K = 4, ri = 1 for all i and utility parameters v0 = 1 and for i= 1, . . . ,N ,\nvi = { 0.25 + , if i∈ {1,2,9,10} 0.25, else ,\n(8.1)\nwhere 0< < 0.25; recall specifies the difference between revenues corresponding to the optimal assortment and the next best assortment. Note that this problem has a unique optimal assortment, {1,2,9,10} with an expected revenue of 1 + 4 /2 + 4 and next best revenue of 1 + 3 /2 + 3 . We consider four different values for , = {0.05,0.1,0.15,0.25}, where higher value of corresponding to larger separation, and hence an “easier” problem instance.\nResults. Figure 1 summarizes the performance of Algorithm 1 for different values of . The results are based on running 100 independent simulations, the standard errors are within 2%. Note that the performance of Algorithm 1 is consistent across different values of ; with a regret that exhibits sub linear growth. Observe that as the value of increases the regret of Algorithm 1 decreases. While not immediately obvious from Figure 1, the regret behavior is fundamentally different in the case of “small” and “large” . To see this, in Figure 2, we focus on the regret for = 0.05 and\n= 0.25 and fit to logT and √ T respectively. (The parameters of these functions are obtained via\n(a)\n(b)\nrespectively simple linear regression of the regret vs logT and regret vs √ T ). It can be observed that regret is roughly logarithmic when = 0.25, and in contrast roughly behaves like √ T when\n= 0.05. This illustrates the theory developed in Section 6, where we showed that the regret grows logarithmically with time, if the optimal assortment and next best assortment are “well separated”, while the worst-case regret scales as √ T ."
    }, {
      "heading" : "8.2. Comparison with existing approaches.",
      "text" : "In this section, we present a computational study comparing the performance of our algorithm to that of Sauré and Zeevi (2013). (To the best of our knowledge, Sauré and Zeevi (2013) is currently the best existing approach for our problem setting.) To be implemented, their approach requires certain a priori information of a “separability parameter”; roughly speaking, measuring the degree to which the optimal and next-best assortments are distinct from a revenue standpoint. More specifically, their algorithm follows an explore-then-exploit approach, where every product is first required to be offered for a minimum duration of time that is determined by an estimate of said “separability parameter.” After this mandatory exploration phase, the parameters of the choice model are estimated based on the past observations and the optimal assortment corresponding to the estimated parameters is offered for the subsequent consumers. If the optimal assortment and the next best assortment are “well separated,” then the offered assortment is optimal with high probability, otherwise, the algorithm could potentially incur linear regret. Therefore, the knowledge of this “separability parameter” is crucial. For our comparison, we consider the exploration period suggested by Sauré and Zeevi (2013) and compare it with the performance of Algorithm 1 for different values of separation ( .) We will show that for any given exploration period, there is an instance where the approach in Sauré and Zeevi (2013) “breaks down” or in other words incurs\nlinear regret, while the regret of Algorithm 1 grows sub-linearly (as O( √ T ), more precisely) for all values of as asserted in Theorem 1.\nExperimental setup and results. We consider the parametric MNL setting as described in (8.1) and for each value of ∈ {0.05,0.1,0.15,0.25}. Since the implementation of the policy in Sauré and Zeevi (2013) requires knowledge of the selling horizon and minimum exploration period a priori, we consider the exploration period to be 20 logT as suggested in Sauré and Zeevi (2013) and the selling horizon as T = 106. Figure 3 compares the regret of Algorithm 1 with that of Sauré and Zeevi (2013). The results are based on running 100 independent simulations with standard error of 2%. We observe that the regret of the Sauré and Zeevi (2013) is better than the regret of Algorithm 1 when = 0.25 but is worse for other values of . This can be attributed to the fact that for the assumed exploration period, Their algorithm fails to identify the optimal assortment within the exploration phase with sufficient probability and hence incurs a linear regret for = 0.05,0.1 and 0.15. Specifically, among the 100 simulations we tested, the algorithm of Sauré and Zeevi (2013) identified the optimal assortment for only 7%,40%,61% and 97% cases, when = 0.05,0.1,0.15, and 0.25 respectively. This highlights the sensitivity to the “separability parameter” and the importance of having a reasonable estimate on the exploration period. Needless to say, such information is typically not available in practice. In contrast, the performance of Algorithm 1 is consistent across different values of , insofar as the regret grows in a sub-linear fashion in all cases."
    }, {
      "heading" : "8.3. Performance of Algorithm 1 on real data.",
      "text" : "Here, we present the results of a simulated study of Algorithm 1 on a real data set.\nData description. We consider the “UCI Car Evaluation Database” (see Lichman (2013)) which contains attributes based information of N = 1728 cars and consumer ratings for each car. The exact details of the attributes are provided in Table 1. Rating for each car is also available. In particular, every car is associated with one of the following four ratings, unacceptable, acceptable, good and very good.\nAssortment optimization framework. We assume that the consumer choice is modeled by\nthe MNL model, where the mean utility of a product is linear in the values of attributes. More\nspecifically, we convert the categorical attributes described in Table 1 to attributes with binary\nvalues by adding dummy attributes (for e.g. “price very high”, “price low” are considered as two\ndifferent attributes that can take values 1 or 0). Now every car is associated with an attribute vector mi ∈ {0,1}22, which is known a priori and the mean utility of product i is given by the inner product\nµi = θ ·mi i= 1, . . . ,N,\nwhere θ ∈R22 is some fixed, but initially unknown attribute weight vector. Under this model, the probability that a consumer purchases product i when offered an assortment S ⊂ {1, . . . ,N} is\ngiven by,\npi(S) =  eθ·mi 1 + ∑ j∈S e θ·mj , if i∈ S ∪{0}\n0, otherwise,\n(8.2)\nLet m = (m1, . . . ,mN). Our goal is to offer assortments S1, . . . , ST at times 1, . . . , T respectively such that the cumulative sales are maximized or alternatively, minimize the regret defined as\nRegπ(T,m) = T∑ t=1 (∑ i∈S∗ pi(S)− ∑ i∈St pi(St) ) , (8.3)\nwhere\nS∗ = arg max S ∑ i∈S\neθ·mi 1 + ∑\nj∈S e θ·mj\n.\nNote that regret defined in (8.3) is a special case formulation of the regret defined in (2.6) with ri = 1 and vi = e θ·mi for all i= 1, . . . ,N .\nExperimental setup and results. We first estimate a ground truth MNL model as follows. Using the available attribute level data and consumer rating for each car, we estimate a logistic model assuming every car’s rating is independent of the ratings of other cars to estimate the attribute weight vector θ. Specifically, under the logistic model, the probability that a consumer will purchase a car whose attributes are defined by the vector m ∈ {0,1}22 and the attribute weight vector θ is given by\npbuy(θ,m) ∆ = P (buy|θ) = e\nθ·m\n1 + eθ·m .\nFor the purpose of training the logistic model on the available data, we consider the consumer ratings of “acceptable,” “good,” and “very good” as success or intention to buy and the consumer rating of “unacceptable” as a failure or no intention to buy. We then use the maximum likelihood estimate θMLE for θ to run simulations and study the performance of Algorithm 1 for the realized θMLE. In particular, we compute θMLE that maximizes the following regularized log-likelihood\nθMLE = argmax θ N∑ i=1 log pbuy(θ,mi)−‖θ‖2.\nThe objective function in the preceding optimization problem is convex and therefore we can use any of the standard convex optimization techniques to obtain the estimate, θMLE. It is important to note that the logistic model is only employed to obtain an estimate for θ, θMLE. The estimate θMLE is assumed to be the ground truth MNL model and is used to simulate the feedback of consumer choices for our learning Algorithm 1. We measure the performance of Algorithm 1 in terms of regret as defined in (8.3) with θ= θMLE.\nFigure 4 plots the regret of Algorithm 1 on the “UCI Car Evaluation Database”, when the selling horizon is T = 107 and at each time index, retailer cann show at most k = 100 cars. The results are based on running 100 independent simulations and have a standard error of 2%. The regret of Algorithm 1, is seen to grow in a sublinear fashion with respect to the selling horizon. It should be noted that Algorithm 1 did not require any a priori knowledge on the parameters unlike the other existing approaches such as Sauré and Zeevi (2013) and therefore can be applied to a wide range of other settings."
    }, {
      "heading" : "Acknowledgments",
      "text" : "V. Goyal is supported in part by NSF Grants CMMI-1351838 (CAREER) and CMMI-1636046. A. Zeevi is supported in part by NSF Grants NetSE-0964170 and BSF-2010466."
    }, {
      "heading" : "A. Proof of Theorem 1",
      "text" : "In rest of this section, we provide a detailed proof of Theorem 1 following the outline discussed in Section 4.1. The proof is organized as follows. In Section A.1, we complete the proof of Lemma 4.1 and in Section A.2, we prove similar properties for estimates R̃`(S`). Finally, in Section A.3, we utilize these properties to complete the proof of Theorem 1 .\nA.1. Properties of estimates vUCBi,`\nFirst, we focus on properties of the estimates v̂i,` and v̄i,`, and then extend these properties to establish the necessary properties of vUCBi,` .\nUnbiased Estimates.\nLemma A.1 The moment generating function of estimate conditioned on S`, v̂i, is given by,\nEπ ( eθv̂i,` ∣∣∣S`)= 1 1− vi(eθ− 1) , for all θ≤ log 1 + vi vi , for all i= 1, · · · ,N.\nProof. From (2.1), we have that probability of no purchase event when assortment S` is offered\nis given by\np0(S`) = 1 1 + ∑\nj∈S` vj .\nLet n` be the total number of offerings in epoch ` before a no purchased occurred, i.e., n` = |E`|−1. Therefore, n` is a geometric random variable with probability of success p0(S`). And, given any fixed value of n`, v̂i,` is a binomial random variable with n` trials and probability of success given by\nqi(S`) = vi∑ j∈S` vj .\nIn the calculations below, for brevity we use p0 and qi respectively to denote p0(S`) and qi(S`). Hence, we have\nEπ ( eθv̂i,` ) =En` { Eπ ( eθv̂i,` ∣∣n`)} .\nSince the moment generating function for a binomial random variable with parameters n,p is (peθ + 1− p)n, we have\nEπ ( eθv̂i,` ∣∣n`)=En` {(qieθ + 1− qi)n`} . For any α, such that, α(1− p)< 1 n is a geometric random variable with parameter p, we have\nEπ(αn) = p\n1−α(1− p) .\nNote that for all θ < log 1+vi vi , we have (qie θ + (1− qi)) (1 − p0) = (1 − p0) + p0vi(eθ − 1) < 1. Therefore, we have Eπ ( eθv̂i,` ) = 1\n1− vi(eθ− 1) for all θ < log 1 + vi vi .\nFrom the moment generating function, we can establish that v̂i,` is a geometric random variable\nwith parameter 1 1+vi . Thereby also establishing that v̂i,` and v̄i,` are unbiased estimators of vi. More specifically, from lemma A.1, we have the following result.\nCorollary A.1 We have the following results.\n1. v̂i,`, `≤L are i.i.d geometrical random variables with parameter 11+vi , i .e. for any `, i\nPπ (v̂i,` =m) = (\nvi 1 + vi\n)m( 1\n1 + vi\n) ∀m= {0,1,2, · · · }.\n2. v̂i,`, `≤L are unbiased i.i.d estimates of vi, i .e. Eπ (v̂i,`) = vi ∀ `, i.\nConcentration Bounds. From Corollary A.1, it follows that v̂i,τ , τ ∈ Ti(`) are i.i.d geometric random variables with mean vi. We will use this observation and extend the multiplicative ChernoffHoeffding bounds discussed in Mitzenmacher and Upfal (2005) and Babaioff et al. (2015) to geometric random variables and derive the result.\nLemma A.2 If vi ≤ v0 for all i, for every epoch `, in Algorithm 1, we have the following concentration bounds.\n1. Pπ ( |v̄i,`− vi|> √ 48v̄i,` log (`+ 1)\nTi(`) +\n48 log (`+ 1)\nTi(`)\n) ≤ 5 ` .\n2. Pπ ( |v̄i,`− vi|> √ 24vi log (`+ 1)\nTi(`) +\n48 log (`+ 1)\nTi(`)\n) ≤ 5 ` .\n3. Pπ ( |v̄i,`− vi|> √ 24 log (`+ 1)\nn +\n48 log (`+ 1)\nn\n) ≤ 5 `\nNote that to apply standard Chernoff-Hoeffding inequality (see p.66 in Mitzenmacher and Upfal (2005)), we must have the individual sample values bounded by some constant, which is not the case with our estimates v̂i,τ . However, these estimates are geometric random variables and therefore\nhave extremely small tails. Hence, we can extend the Chernoff-Hoeffding bounds discussed in Mitzenmacher and Upfal (2005) and Babaioff et al. (2015) to geometric variables and prove the above result. Lemma 4.1 follows directly from Lemma A.2. The proof of Lemma A.2 is long and tedious and in the interest of continuity, we complete the proof in Appendix D. Following the proof of Lemma A.2, we obtain a very similar result that is useful to establish concentration bounds for the general parameter setting.\nLemma 4.1 follows from Corollary A.1 and Lemma A.2 and establishes the necessary properties\nof vUCBi.` and v UCB2 i.` as alluded in the proof outline.\nA.2. Properties of estimate R̃(S)\nIn this section, we show that the estimates R̃`(S`) are upper confidence bounds converging “quickly” to the expected revenue corresponding to the optimal assortment, R(S∗,v).\nOptimistic Estimates. First, we establish that R̃`(S) is an upper bound estimate of the optimal revenue,R(S∗,v). LetR(S,w) be as defined in (2.2), namely, the expected revenue when assortment S is offered and if the parameters of the MNL were given by the vector w. Then, from definition of R̃`(S) (refer to (3.7)), it follows that R̃`(S) =R(S,v UCB ` ).\nLemma A.3 Assume 0≤wi ≤ vUCBi for all i= 1, · · · , n. Suppose S is an optimal assortment when the MNL are parameters are given by w. Then, R(S,vUCB)≥R(S,w).\nProof. We prove the result by first showing that for any j ∈ S, we have R(S,wj) ≥ R(S,w), where wj is vector w with the jth component increased to vUCBj , i.e. w j i = wi for all i 6= j and wjj = v UCB j . We can use this result iteratively to argue that increasing each parameter of MNL to the highest possible value increases the value of R(S,w) to complete the proof.\nIt is easy to see that if rj < R(S) removing the product j from assortment S yields higher\nexpected revenue contradicting the optimality of S. Therefore, we have\nrj ≥R(S).\nMultiplying by (vUCBj −wj)( ∑ i∈S/j wi + 1) on both sides of the above inequality and re-arranging terms, we can show that R(S,wj)≥R(S,w).\nLet Ŝ,w∗ be maximizers of the optimization problem, max S∈S\nmax 0≤wi≤vUCBi,` R(S,w). Then applying\nlemma A.3 on assortment Ŝ and parameters v∗ and noting that vUCBi,` > vi with high probability, we have that\nR̃`(S`) = max S∈S R(S,vUCB` )≥max S∈S max 0≤wi≤vUCBi,` R(S,w)≥R(S∗,v).\nBounding Regret. Now we will establish the connection between the error on the expected revenues and the error on the estimates of MNL parameters. In particular, we have the following result.\nLemma A.4 If 0≤ vi ≤ vUCBi,` for all i∈ S`, then R̃`(S`)−R(S`,v)≤ ∑ j∈S` ( vUCBj,` − vj ) 1+ ∑\nj∈S` vj\n.\nProof. Since 1 + ∑\ni∈S` vUCBi,` ≥ 1 + ∑ i∈S` vi,`, we have\nR̃`(S`)−R(S`,v)≤ ∑ i∈S` riv UCB i,` 1+ ∑\nj∈S` vUCBj,`\n− ∑ i∈S` rivi\n1+ ∑\nj∈S` vUCBj,`\n,\n≤ ∑ i∈S` ( vUCBi,` − vi ) 1 + ∑ j∈S` vUCBj,` ≤ ∑ i∈S` ( vUCBi,` − vi ) 1 + ∑ j∈S` vj\nLemma 4.3 follows directly from the above result and Lemma 4.1, while Lemma 4.3 follows\ndirectly from the above result and Lemma 4.1.\nA.3. Putting it all together: Proof of Theorem 1\nIn this section, we formalize the intuition developed in the previous sections and complete the proof of Theorem 1.\nLet S∗ denote the optimal assortment, our objective is to minimize the regret defined in (2.6),\nwhich is same as\nRegπ(T,v) =Eπ { L∑ `=1 |E`| (R(S∗,v)−R(S`,v)) } , (A.1)\nNote that L, E` and S` are all random variables and the expectation in equation (A.1) is over these random variables. Let H` be the filtration (history) associated with the policy upto epoch `. In particular,\nH` = σ(U,C1, · · · ,Ct(`), S1, · · · , St(`)),\nwhere t(`) is the time index corresponding to the end of epoch `. The length of the `th epoch, |E`| conditioned on S` is a geometric random variable with success probability defined as the probability of no-purchase in S`, i.e.\np0(S`) = 1 1 + ∑\nj∈S` vj .\nLet V (S`) = ∑\nj∈S` vj, then we have Eπ\n( |E`| ∣∣∣ S`)= 1+V (S`). Noting that S` in our policy is determined by H`−1, we have Eπ ( |E`| ∣∣∣H`−1)= 1+V (S`). Therefore, by law of conditional expectations, we have\nRegπ(T,v) =Eπ { L∑ `=1 Eπ [ |E`| (R(S∗,v)−R(S`,v)) ∣∣∣H`−1]} ,\nand hence the regret can be reformulated as\nRegπ(T,v) =Eπ { L∑ `=1 (1 +V (S`)) (R(S ∗,v)−R(S`,v)) } (A.2)\nthe expectation in equation (A.2) is over the random variables L and S`. We now provide the proof sketch for Theorem 1 and the complete proof is provided in the full version of the paper. Proof of Theorem 1. For sake of brevity, let ∆R`=(1 + V (S`)) (R(S ∗,v)−R(S`,v)), for all `= 1, · · · ,L. Now the regret can be reformulated as\nRegπ(T,v) =Eπ { L∑ `=1 ∆R` } (A.3)\nLet Ti denote the total number of epochs that offered an assortment containing product i. Let A0 denote the complete set Ω and for all `= 1, . . . ,L, event A` is given by\nA` = { vUCBi,` < vi or v UCB i,` > vi +C1 √ vi Ti(`) log (`+ 1) +C2 log2 (`+ 1) Ti(`) for some i= 1, · · · ,N } .\nNoting that A` is a “low probability” event, we analyze the regret in two scenarios, one when A` is true and another when Ac` is true. Hence, we have\nEπ (∆R`) =E [ ∆R` · I(A`−1) + ∆R` · I(Ac`−1) ] Using the fact that R(S∗,v) and R(S`,v) are both bounded by one and V (S`)≤N , we have\nEπ (∆R`)≤ (N + 1)Pπ(A`−1) +Eπ [ ∆R` · I(Ac`−1) ] .\nWhenever I(Ac`−1) = 1, from Lemma A.3, we have R̃`(S∗)≥R(S∗,v) and by our algorithm design, we have R̃`(S`)≥ R̃`(S∗) for all `≥ 2. Therefore, it follows that\nEπ {∆R`} ≤ (N + 1)Pπ(A`−1) +Eπ {[ (1 +V (S`))(R̃`(S`)−R(S`,v)) ] · I(Ac`−1) } From Lemma 4.3, it follows that[\n(1 +V (S`))(R̃`(S`)−R(S`,v)) ] · I(Ac`−1)≤ ∑ i∈S` ( C1 √ vi logT Ti(`) + C2 logT Ti(`) )\nTherefore, we have\nEπ {∆R`} ≤ (N + 1)Pπ (A`−1) +C ∑ i∈S` Eπ\n(√ vi logT\nTi(`) +\nlogT Ti(`)\n) (A.4)\nwhere C = max{C1,C2}. Combining equations (A.2) and (A.4), we have\nRegπ(T,v)≤Eπ { L∑ `=1 [ (N + 1)Pπ (A`−1) +C ∑ i∈S` (√ vi logT Ti(`) + logT Ti(`) )]} .\nTherefore, from Lemma 4.1, we have\nRegπ(T,v)≤CEπ { L∑ `=1 N + 1 ` + ∑ i∈S` √ vi logT Ti(`) + ∑ i∈S` logT Ti(`) } ,\n(a) ≤ CN logT +CN log2 T +CEπ ( n∑ i=1 √ viTi logT ) (b)\n≤ CN logT +CN log2 T +C N∑ i=1 √ vi logTEπ(Ti)\n(A.5)\nInequality (a) follows from the observation that L ≤ T , Ti ≤ T , Ti∑\nTi(`)=1\n1√ Ti(`)\n≤ √ Ti and\nTi∑ Ti(`)=1 1 Ti(`) ≤ logTi, while Inequality (b) follows from Jensen’s inequality.\nFor any realization of L, E`, Ti, and S` in Algorithm 1, we have the following relation ∑L\n`=1 n` ≤ T . Hence, we have Eπ (∑L `=1 n` ) ≤ T. Let S denote the filtration corresponding to the offered assortments S1, · · · , SL, then by law of total expectation, we have,\nEπ ( L∑ `=1 n` ) =Eπ { L∑ `=1 ES (n`) } =Eπ { L∑ `=1 1 + ∑ i∈S` vi }\n=Eπ { L+\nn∑ i=1 viTi\n} =Eπ{L}+\nn∑ i=1 viEπ(Ti).\nTherefore, it follows that ∑ viEπ(Ti)≤ T. (A.6)\nTo obtain the worst case upper bound, we maximize the bound in equation (A.5) subject to the condition (A.6) and hence, we have Regπ(T,v) =O( √ NT logT +N log2 T )."
    }, {
      "heading" : "B. Proof of Theorem 2",
      "text" : "The proof for Theorem 2 is very similar to the proof of Theorem 1. Specifically, we first prove that the initial exploratory phase is indeed bounded and then follow the proof of Theorem 1 to establish the correctness of confidence intervals, optimistic assortment and finally deriving the convergence rates and regret bounds.\nBounding Exploratory Epochs. We would denote an epoch ` as an “exploratory epoch” if the assortment offered in the epoch contains a product that has been offered in less than 48 log (`+ 1) epochs. It is easy to see that the number of exploratory epochs is bounded by 48N logT , where T is the selling horizon under consideration. We then use the observation that the length of any epoch is a geometric random variable to bound the total expected duration of the exploration phase. Hence, we bound the expected regret due to explorations.\nLemma B.1 Let L be the total number of epochs in Algorithm 2 and let EL denote the set of “exploratory epochs”, i.e.\nEL = {` | ∃ i∈ S` such that Ti(`)< 48 log (`+ 1)} ,\nwhere Ti(`) is the number of epochs product i has been offered before epoch `. If E` denote the time indices corresponding to epoch ` and vi ≤Bv0 for all i= 1, . . . ,N , for some B ≥ 1, then we have that,\nEπ (∑ `∈EL |E`| ) < 48NB logT,\nwhere the expectation is over all possible outcomes of Algorithm 2.\nProof. Consider an `∈EL, note that |E`| is a geometric random variable with parameter 1V (S`)+1 . Since vi ≤ Bv0, for all i and we can assume without loss of generality v0 = 1, we have |E`| as a geometric random variable with parameter p, where p≥ 1\nB|S`|+1 . Therefore, we have the conditional\nexpectation of |E`| given that assortment S` is offered is bounded as,\nEπ (|E`| | S`)≤B|S`|+ 1.\nNote that after every product has been offered in at least 48 logT epochs, then we do not have any exploratory epochs. Therefore, we have that∑ `∈EL |S`| ≤ 48BN logT + 48≤ 96NB logT. The required result follows from the preceding two equations. .\nConfidence Intervals. We will now show a result analogous to Lemma 4.1, that establish the updates in Algorithm 2, vUCB2i,` , as upper confidence bounds converging to actual parameters vi. Specifically, we have the following result.\nLemma B.2 For every epoch `, if Ti(`)≥ 48 log (`+ 1) for all i∈ S`, then we have, 1. vUCB2i,` ≥ vi with probability at least 1− 5` for all i= 1, · · · ,N . 2. There exists constants C1 and C2 such that\nvUCB2i,` − vi ≤C1 max{ √ vi, vi}\n√ log (`+ 1)\nTi(`) +C2\nlog (`+ 1)\nTi(`) ,\nwith probability at least 1− 5 ` .\nThe proof is very similar to the proof of Lemma 4.1, where we first establish the following concentration inequality for the estimates v̂i,`, when Ti(`)≥ 48 log (`+ 1) from which the above result follows. The proof of Corollary B.1 is provided in Appendix D.\nCorollary B.1 If in epoch `, Ti(`)≥ 48 log (`+ 1) for all i ∈ S`, then we have the following concentration bounds\n1. Pπ ( |v̄i,`− vi| ≥max {√ v̄i,`, v̄i,` }√48 log (`+ 1) n + 48 log (`+ 1) n ) ≤ 5 ` .\n2. Pπ ( |v̄i,`− vi| ≥max{ √ vi, vi} √ 24 log (`+ 1)\nn +\n48 log (`+ 1)\nn\n) ≤ 5 ` .\n3. Pπ ( |v̄i,`− vi|> vi √ 12 log (`+ 1)\nn +\n√ 6 log (`+ 1)\nn +\n48 log (`+ 1)\nn\n) ≤ 5 `\nOptimistic Estimate and Convergence Rates: We will now establish two results analogous to Lemma 4.2 and 4.3, that show that the estimated revenue converges to the optimal expected revenue from above and also specify the convergence rate. In particular, we have the following two results.\nLemma B.3 Suppose S∗ ∈ S is the assortment with highest expected revenue, and Algorithm 2 offers S` ∈ S in each epoch `. Further, if Ti(`)≥ 48 log (`+ 1) for all i∈ S`, then we have,\nR̃`(S`)≥ R̃`(S∗)≥R(S∗,v) with probability at least 1− 5\n` .\nLemma B.4 For every epoch `, if Ti(`)≥ 48 log (`+ 1) for all i ∈ S`, then there exists constants C1 and C2 such that for every `, we have\n(1 + ∑\nj∈S` vj)(R̃`(S`)−R(S`,v))≤C1 max\n{√ vi, vi }√ log (`+1)\n|Ti(`)| +C2\nlog (`+1)\n|Ti(`)| ,\nwith probability at least 1− 5 ` .\nB.1. Putting it all together: Proof of Theorem 2\nProof of Theorem 2 is very similar to the proof of Theorem 1. We use the key results discussed above instead of similar results in Section 4 to complete the proof. Regret can decomposed as\nRegπ(T,v) =Eπ {∑ `∈EL |E`| (R(S∗,v)−R(S`,v)) + ∑ ` 6∈EL |E`| (R(S∗,v)−R(S`,v)) } ,\nFrom Lemma B.1 and R(S,v)≤ 1 for any S ∈ S, it follows that\nRegπ(T,v)≤ 192NB logT +Eπ {∑ ` 6∈EL |E`| (R(S∗,v)−R(S`,v)) } ,\nFor sake of brevity, let ∆R`=(1+V (S`)) (R(S ∗,v)−R(S`,v)), for all ` 6∈EL. Now the regret can\nbe bounded as,\nRegπ(T,v)≤ 192NB logT +Eπ {∑ 6̀∈EL ∆R` } (B.1)\nIn the interest of avoiding redundant analysis, we claim that the following inequality can be derived from the proof of Theorem 1 adapted to the general setting.\nEπ {∆R`} ≤C ∑ i∈S` Eπ\n( max{ √ vi, vi} √ logT\nTi(`) +\nlogT Ti(`)\n) (B.2)\nwhere C = max{C1,C2}. Combining equations (B.1) and (B.2), we have\nRegπ(T,v)≤ 192NB logT +CEπ {∑ ` 6∈EL ∑ i∈S` ( max{ √ vi, vi} √ logT Ti(`) + logT Ti(`) )} .\nDefine sets I = {i|vi ≥ 1} and D= {i|vi < 1}. Therefore, we have ,\nRegπ(T,v)≤ 192NB logT +CEπ {∑ 6̀∈EL ∑ i∈S` ( max{ √ vi, vi} √ logT Ti(`) + logT Ti(`) )} ,\n(a) ≤ 192NB logT +CN log2 T +CEπ (∑ i∈D √ viTi logT + ∑ i∈I vi √ Ti logT ) (b)\n≤ 192NB logT +CN log2 T +C ∑ i∈D √ viEπ(Ti) logT + ∑ i∈I vi √ Eπ(Ti) logT\n(B.3)\nInequality (a) follows from the observation that L ≤ T , Ti ≤ T , Ti∑\nTi(`)=1\n1√ Ti(`)\n≤ √ Ti and\nTi∑ Ti(`)=1 1 Ti(`) ≤ logTi, while Inequality (b) follows from Jensen’s inequality.\nFrom (A.6), we have that, ∑ viEπ(Ti)≤ T.\nTo obtain the worst case upper bound, we maximize the bound in equation (B.3) subject to the above constraint. Noting that the objective in (B.3) is concave, we use the KKT conditions to derive the worst case bound as Regπ(T,v) =O( √ BNT logT +N log2 T +BN logT )."
    }, {
      "heading" : "C. Parameter dependent bounds",
      "text" : "Proof of lemma 6.2 Assume for the sake of contradiction that Algorithm 1 has offered suboptimal assortments in more than N(N−1) 2 τ epochs.\nLet `1 be the epoch by which we have offered the first Nτ sub-optimal assortments and S1 be\nthe subset of products that have been offered in at least τ times by epoch `1, i.e.\nS1 = {i|Ti(`1)≥ τ},\nthen by the pigeon hole principle, we have at least one product that has been offered at least τ times (else the total number of offerings would not be more than Nτ), i.e.\n|S1| ≥ 1.\nSimilarly, let `2 be the epoch by which we have offered the next (N − 1)τ suboptimal assortments and S2 be the subset of products that have been offered in at least τ epochs by epoch `2, i.e.\nS2 = {i|Ti(`2)≥ τ}.\nFrom lemma 6.1, we have that if any assortment is a subset of S1 and is offered after the ` th 1 epoch, then it is an optimal assortment. Hence, among the next set of (N − 1)τ sub-optimal assortments, there must be at least one product that does not belong to S1 and therefore, by pigeon hole principle, we will have at least one more product that does not belong to S1 being offered in τ epochs, i.e.\n|S2| − |S1| ≥ 1.\nWe can similarly define subsets S3, · · · , SN and prove\n|Sk+1−Sk| ≥ 1 for all k≤N − 1,\nestablishing that after offering sub-optimal assortments in N(N−1) 2 τ good epochs, we have that, all the N products are offered in at least τ epochs contradicting our hypothesis. Proof of Theorem 3 Let V (S`) = ∑\nj∈S` vj, we have that\nRegπ(T,v) =Eπ { L∑ `=1 (1 +V (S`)) (R(S ∗,v)−R(S`,v)) }\nFor sake of brevity, let ∆R`=(1 +V (S`)) (R(S ∗,v)−R(S`,v)), for all `= 1, · · · ,L. Now the regret can be reformulated as\nRegπ(T,v) =Eπ { L∑ `=1 ∆R` } (C.1)\nLet Ti denote the total number of epochs that offered a sub-optimal assortment containing product i. From Lemma 6.2, we have that\n|Ti| ≤ N(N − 1)τ\n2 . (C.2)\nLet A0 denote the complete set Ω and for all `= 1, . . . ,L, event A` is given by\nA` = { vUCBi,` < vi or v UCB i,` > vi +C1 √ vi log (`+ 1)\nTi(`) +C2\nlog (`+ 1)\nTi(`)\n} .\nNoting that A` is a “low probability” event, we analyze the regret in two scenarios, one when A` is true and another when Ac` is true. Hence, we have\nEπ (∆R`) =Eπ [ ∆R` · I(A`−1) + ∆R` · I(Ac`−1) ]\nUsing the fact that R(S∗,v) and R(S`,v) are both bounded by one and V (S`)≤N , we have\nEπ (∆R`)≤ (N + 1)Pπ(A`−1) +Eπ [ ∆R` · I(Ac`−1) ] .\nFrom Lemma 6.2, we have that a sub-optimal assortment cannot be offered for more than N(N−1) 2 τ good epochs, i.e epochs for which I(Ac`−1) = 1. Let NOL denote the set of good epochs in which a sub-optimal assortment is offered. From Lemma 6.2, we have that\n|NOL| ≤ N(N − 1)τ\n2 .\nFor every ` ∈NOL, whenever I(Ac`−1) = 1, from Lemma A.3, we have R̃`(S∗) ≥ R(S∗,v) and by our algorithm design, we have R̃`(S`)≥ R̃`(S∗) for all `≥ 2. Therefore, it follows that\nEπ {∆R`} ≤ (N + 1)Pπ(A`−1) +Eπ {[ (1 +V (S`))(R̃`(S`)−R(S`,v)) ] · I(Ac`−1) } From Lemma 4.3, it follows that\n[ (1 +V (S`))(R̃`(S`)−R(S`,v)) ] · I(Ac`−1)≤ ∑ i∈S` ( C1 √ vi logT Ti(`) + C2 logT Ti(`) )\nTherefore, we have\nEπ {∆R`} ≤ (N + 1)P (A`−1) +CEπ (∑ i∈S` (√ vi logT Ti(`) + logT Ti(`) )) (C.3)\nwhere C = max{C1,C2}. Combining equations (C.1) and (C.3), we have\nRegπ(T,v)≤Eπ { ∑ `∈NOL [ (N + 1)P (A`−1) +C ∑ i∈S` (√ vi logT Ti(`) + logT Ti(`) )]} .\nFor the regret to be maximum, we need the sub-optimal assortments as early as possible. Therefore,\nfrom Lemma 4.1, we have\nRegπ(T,v)≤CEπ { ∑ `∈NOL N + 1 ` + ∑ i∈S` √ vi logT Ti(`) + ∑ i∈S` logT Ti(`) } ,\na\n≤CN +CN log2 T +CEπ ( n∑ i=1 √ viTi logT ) b ≤CN +CN log2 T +C\n( n∑ i=1 √ viEπ(Ti) logT ) c\n≤CN +CN log2 T + ( CN √ (N − 1)\n2 τ logT\n) (C.4)\nInequality (a) follows from the observation that L ≤ T , Ti ≤ T , Ti∑\nTi(`)=1\n1√ Ti(`)\n≤ √ Ti and\nTi∑ Ti(`)=1 1 Ti(`) ≤ logTi, while Inequalty (b) follows from Jensen’s inequality. Inequality (c) follows\nfrom maxmizing the bound in (C.4) subject to the constraint, ∑ viEπ(Ti) ≤ N(N−1)2 as we have done in the proof of Theorem 1. Finally, the proof follows from the definition of τ (See (6.1))."
    }, {
      "heading" : "D. Multiplicative Chernoff Bounds",
      "text" : "We will extend the Chernoff bounds as discussed in Mitzenmacher and Upfal (2005) 1 to geometric random variables and establish the following concentration inequality.\nTheorem 5 Consider n i.i.d geometric random variables X1, · · · ,Xn with parameter p, i.e. for any i\nPr(Xi =m) = (1− p)mp ∀m= {0,1,2, · · · },\nand let µ=E(Xi) = 1−pp . We have, 1.\nPr\n( 1\nn n∑ i=1 Xi > (1 + δ)µ\n) ≤  exp ( − nµδ 2 2(1+δ)(1+µ)2 ) if µ≤ 1, exp ( − nδ 2µ2\n6(1+µ)2\n( 3− 2δµ\n1+µ\n)) if µ≥ 1 and δ ∈ (0,1).\nand\n2.\nPr\n( 1\nn n∑ i=1 Xi < (1− δ)µ\n) ≤  exp ( − nδ 2µ 6(1+µ)2 ( 3− 2δµ 1+µ )) if µ≤ 1, exp ( − nδ 2µ2\n2(1+µ)2\n) if µ≥ 1.\nProof. We will first bound Pr (\n1 n ∑n i=1Xi > (1 + δ)µ ) and then follow a similar approach for\nbounding Pr (\n1 n ∑n i=1Xi < (1− δ)µ ) to complete the proof.\nBounding Pr (\n1 n ∑n i=1Xi > (1 + δ)µ ) :\nFor all i and for any 0< t< log 1+µ µ , we have,\nE(etXi) = 1\n1−µ(et− 1) .\nTherefore, from Markov Inequality, we have\nPr\n( 1\nn n∑ i=1 Xi > (1 + δ)µ\n) = Pr ( et ∑n i=1Xi > e(1+δ)nµt ) ,\n≤ e−(1+δ)nµt n∏ i=1 E(etXi),\n= e−(1+δ)nµt (\n1\n1−µ(et− 1)\n)n .\n1 (originally discussed in Angluin and Valiant (1977))\nTherefore, we have,\nPr\n( 1\nn n∑ i=1 Xi > (1 + δ)µ\n) ≤ min\n0<t<log 1+µµ\ne−(1+δ)nµt (\n1\n1−µ(et− 1)\n)n . (D.1)\nWe have,\nargmin 0<t<log 1+µµ\ne−(1+δ)nµt (\n1\n1−µ(et− 1)\n)n = argmin\n0<t<log 1+µµ\n− (1 + δ)nµt−n log (1−µ(et− 1)) , (D.2)\nNoting that the right hand side in the above equation is a convex function in t, we obtain the optimal t by solving for the zero of the derivative. Specifically, at optimal t, we have\net = (1 + δ)(1 +µ)\n1 +µ(1 + δ) .\nSubstituting the above expression in (D.1), we obtain the following bound.\nPr\n( 1\nn n∑ i=1 Xi > (1 + δ)µ\n) ≤ (\n1− δ (1 + δ)(1 +µ)\n)nµ(1+δ)( 1 + δµ\n1 +µ\n)n . (D.3)\nFirst consider the setting where µ∈ (0,1).\nCase 1a: If µ∈ (0,1): From Taylor series of log (1−x), we have that\nnµ(1 + δ) log ( 1− δ\n(1 + δ)(1 +µ)\n) ≤− nδµ\n1 +µ − nδ\n2µ\n2(1 + δ)(1 +µ)2 ,\nFrom Taylor series for log (1 +x), we have\nn log ( 1 + δµ\n1 +µ\n) ≤ nδµ\n(1 +µ) ,\nNote that if δ > 1, we can use the fact that log (1 + δx)≤ δ log (1 +x) to arrive at the preceding result. Substituting the preceding two equations in (D.3), we have\nPr\n( 1\nn n∑ i=1 Xi > (1 + δ)µ\n) ≤ exp ( − nµδ 2\n2(1 + δ)(1 +µ)2\n) , (D.4)\nCase 1b: If µ≥ 1 : From Taylor series of log (1−x), we have that\nnµ(1 + δ) log ( 1− δ\n(1 + δ)(1 +µ)\n) ≤− nδµ\n1 +µ ,\nIf δ < 1, from Taylor series for log (1 +x), we have\nn log ( 1 + δµ\n1 +µ\n) ≤ nδµ\n(1 +µ) − nδ\n2µ2\n6(1 +µ)2\n( 3− 2δµ\n1 +µ\n) .\nIf δ≥ 1, we have log (1 + δx)≤ δ log (1 +x) and from Taylor series for log (1 +x), it follows that,\nn log ( 1 + δµ\n1 +µ\n) ≤ nδµ\n(1 +µ) − nδµ\n2\n6(1 +µ)2\n( 3− 2µ\n1 +µ\n) .\nTherefore, substituting preceding results in (D.3), we have\nPr\n( 1\nn n∑ i=1 Xi > (1 + δ)µ\n) ≤  exp ( − nδ 2µ2 6(1+µ)2 ( 3− 2δµ 1+µ )) if µ≥ 1 and δ ∈ (0,1), exp ( − nδµ 2\n6(1+µ)2\n( 3− 2µ\n1+µ\n)) if µ≥ 1 and δ≥ 1\n(D.5)\nBounding Pr (\n1 n ∑n i=1Xi < (1− δ)µ ) :\nNow to bound the other one sided inequality, we use the fact that\nE(e−tXi) = 1\n1−µ(e−t− 1) ,\nand follow a similar approach. More specifically, from Markov Inequality, for any t > 0 and 0< δ < 1, we have\nPr\n( 1\nn n∑ i=1 Xi < (1− δ)µ\n) = Pr ( e−t ∑n i=1Xi > e−(1−δ)nµt ) ≤ e(1−δ)nµt\nn∏ i=1 E(e−tXi)\n= e(1−δ)nµt (\n1\n1−µ(e−t− 1) )n Therefore, we have\nPr\n( 1\nn n∑ i=1 Xi < (1− δ)µ\n) ≤min\nt>0 e−(1+δ)nµt\n( 1\n1−µ(e−t− 1)\n)n ,\nFollowing similar approach as in optimizing the previous bound (see (D.1)) to establish the following result.\nPr\n( 1\nn n∑ i=1 Xi < (1− δ)µ\n) ≤ ( 1 + δ\n(1− δ)(1 +µ)\n)nµ(1−δ)( 1− δµ\n1 +µ\n)n .\nNow we will use Taylor series for log (1 +x) and log (1−x) in a similar manner as described for the other bound to obtain the required result. In particular, since 1− δ≤ 1, we have for any x> 0 it follows that (1 + x 1−δ ) (1−δ) ≤ (1 +x) . Therefore, we have\nPr\n( 1\nn n∑ i=1 Xi < (1− δ)µ\n) ≤ ( 1 + δ\n1 +µ\n)nµ( 1− δµ\n1 +µ\n)n . (D.6)\nCase 2a. If µ ∈ (0,1): Note that since Xi ≥ 0 for all i, we have a zero probability event if δ > 1. Therefore, we assume δ < 1 and from Taylor series for log (1−x), we have\nn log ( 1− δµ\n1 +µ\n) ≤− nδµ\n1 +µ ,\nand from Taylor series for log (1 +x), we have\nnµ log ( 1 + δ\n1 +µ\n) ≤ nδµ\n(1 +µ) − nδ\n2µ\n6(1 +µ)2\n( 3− 2δµ\n1 +µ\n) .\nTherefore, substituting the preceding equations in (D.6), we have,\nPr\n( 1\nn n∑ i=1 Xi < (1− δ)µ\n) ≤ exp ( − nδ 2µ\n6(1 +µ)2\n( 3− 2δµ\n1 +µ\n)) . (D.7)\nCase 2b. If µ≥ 1: For similar reasons as discussed above, we assume δ < 1 and from Taylor series for log (1−x), we have\nn log ( 1− δµ\n1 +µ\n) ≤− nδµ\n1 +µ − nδ\n2µ2\n2(1 +µ)2 ,\nand from Taylor series for log (1 +x), we have\nn log ( 1 + δµ\n1 +µ\n) ≤ nδ\n(1 +µ) .\nTherefore, substituting the preceding equations in (D.6), we have,\nPr\n( 1\nn n∑ i=1 Xi < (1− δ)µ\n) ≤ exp ( − nδ 2µ2\n2(1 +µ)2\n) . (D.8)\nThe result follows from (D.4), (D.5), (D.7) and (D.8).\nNow, we will adapt a non-standard corollary from Babaioff et al. (2015) and Kleinberg et al.\n(2008) to our estimates to obtain sharper bounds.\nLemma D.1 Consider n i.i.d geometric random variables X1, · · · ,Xn with parameter p, i.e. for\nany i, P (Xi = m) = (1− p)mp ∀m = {0,1,2, · · · }. Let µ = Eπ(Xi) = 1−pp and X̄ = ∑n i=1Xi n . If n > 48 log (`+ 1), then we have,\n1. P (∣∣X̄ −µ∣∣>max{√X̄, X̄}√ 48 log (`+1)\nn + 48 log (`+1) n\n) ≤ 5\n`2 . for all n= 1,2, · · · .\n2. P (∣∣X̄ −µ∣∣≥max{√µ,µ}√24 log (`+ 1) n + 48 log (`+ 1) n ) ≤ 4 `2 for all n= 1,2, · · · .\nProof. We will analyze the cases µ< 1 and µ≥ 1 separately.\nCase-1: µ≤ 1. Let δ = (µ+ 1) √ 6 log (`+1)\nµn . First assume that δ ≤ 1 2 . Substituting the value of δ in\nTheorem 5, we obtain,\nP ( 2X̄ ≥ µ ) ≥ 1− 1\n`2 , P ( X̄ ≤ 3µ\n2\n) ≥ 1− 1\n`2 ,\nP (∣∣X̄ −µ∣∣< (µ+ 1)√6µ log (`+ 1) n ) ≥ 1− 2 `2 .\nFrom the above three results, we have,\nP (∣∣X̄ −µ∣∣<√48X̄ log (`+ 1) n ) ≥P (∣∣X̄ −µ∣∣<√24µ log (`+ 1) n ) ≥ 1− 3 `2 . (D.9)\nBy assumption, µ≤ 1. Therefore, we have P ( X̄ ≤ 3\n2\n) ≥ 1− 1\n`2 and,\nP ( X̄ ≤ √ 3X̄\n2\n) ≥ 1− 1\n`2 .\nTherefore, substituting above result in (D.9), we have\nP (∣∣X̄ −µ∣∣>max{√X̄,√2 3 X̄ }√ 48 log (`+ 1) n ) ≤ 4 `2 . (D.10)\nNow consider the scenario, when (µ+ 1) √ 6 log (`+1)\nµn > 1 2 . Then, we have,\nδ1 ∆ =\n12(µ+ 1)2 log (`+ 1)\nµn ≥ 1 2 ,\nwhich implies,\nexp ( − nµδ 2 1\n2(1 + δ1)(1 +µ)2\n) ≤ exp ( − nµδ1\n6(1 +µ)2\n) ,\nexp ( − nδ 2 1µ\n6(1 +µ)2\n( 3− 2δ1µ\n1 +µ\n)) ≤ exp ( − nµδ1\n6(1 +µ)2\n) .\nTherefore, substituting the value of δ1 in Theorem 5, we have P (∣∣X̄ −µ∣∣> 48 log (`+ 1)\nn\n) ≤ 2 `2 .\nHence, from the above result and (D.10), it follows that,\nP (∣∣X̄ −µ∣∣>max{√X̄,√2 3 X̄ }√ 48 log (`+ 1) n + 48 log (`+ 1) n ) ≤ 6 `2 . (D.11)\nCase 2: µ ≥ 1 Let δ= √ 12 log (`+1)\nn , then by our assumption, we have δ≤ 1 2 . From Theorem 5, it follows that,\nP (∣∣X̄ −µ∣∣<µ√12 log (`+ 1) n ) ≥ 1− 2 `2\nP ( 2X̄ ≥ µ ) ≥ 1− 1\n`2\nHence we have,\nP (∣∣X̄ −µ∣∣< X̄√48 log (`+ 1) n ) ≥P (∣∣X̄ −µ∣∣<µ√12 log (`+ 1) n ) ≥ 1− 3 `2 . (D.12)\nBy assumption µ≥ 1. Therefore, we have P ( X̄ ≥ 1\n2\n) ≥ 1− 1\n`2 and,\nP ( X̄ ≥ √ X̄\n2\n) ≥ 1− 1\n`2 . (D.13)\nTherefore, from (D.12) and (D.13), we have\nP (∣∣X̄ −µ∣∣>max{X̄,√X̄ 2 }√ 48 log (`+ 1) n ) ≤ 4 `2 . (D.14)\nThe result follows from (D.10) and (D.14).\nFrom the proof of Lemma D.1, the following result follows.\nCorollary D.1 Consider n i.i.d geometric random variables X1, · · · ,Xn with parameter p, i.e. for\nany i, P (Xi =m) = (1− p)mp ∀m= {0,1,2, · · · }. Let µ= Eπ(Xi) = 1−pp and X̄ = ∑n i=1Xi n . If µ≤ 1, then we have,\n1. P (∣∣X̄ −µ∣∣>√ 48X̄ log (`+1)\nn + 48 log (`+1) n\n) ≤ 5\n`2 . for all n= 1,2, · · · . 2. P (∣∣X̄ −µ∣∣≥√ 24µ log (`+1)\nn + 48 log (`+1) n\n) ≤ 4\n`2 for all n= 1,2, · · · .\nProof of Lemma A.2 Fix i and `, define the events,\nAi,` = { |v̄i,`− vi|> √ 48v̄i,` log (`+ 1)\n|Ti(`)| +\n48 log (`+ 1)\n|Ti(`)|\n} .\nLet v̄i,m = ∑m τ=1 v̂i,τ m . Then, we have,\nPπ (Ai,`)≤ Pπ { max m≤` ( |v̄i,m− vi| − √ 48v̄i,m log (`+ 1) m − 48 log (`+ 1) m ) > 0 }\n= Pπ ( ⋃̀ m=1 { |v̄i,m− vi| − √ 48v̄i,m log (`+ 1) m − 48 log (`+ 1) m > 0 })\n≤ ∑̀ m=1 Pπ\n( |v̄i,m− vi|> √ 48v̄i,m log (`+ 1)\nm − 48 log (`+ 1) m ) (a)\n≤ ∑̀ m=1 5 `2 ≤ 5 `\n(D.15)\nwhere inequality (a) in (D.15) follows from Corollary D.1. The inequalities in Lemma A.2 follows from definition of vUCBi,` , Corollary D.1 and (D.15).\nProof of Corollary B.1 is similar to the proof of Lemma A.2."
    }, {
      "heading" : "E. Proof of Lemma 7.1",
      "text" : "We follow the proof of Ω( √ NT ) lower bound for the Bernoulli instance with parameters 1\n2 . We\nfirst establish a bound on KL divergence, which will be useful for us later.\nLemma E.1 Let p and q denote two Bernoulli distributions with parameters α+ and α respectively. Then, the KL divergence between the distributions p and q is bounded by 4K 2,\nKL(p‖q)≤ 4 α 2.\nKL(p‖q) = α · log α α+ + (1−α) log 1−α 1−α−\n= α log 1−\n1−α 1 +\nα\n− log(1− 1−α )\n= α log ( 1−\n(1−α)(α+ )\n) − log ( 1−\n1−α ) using 1−x≤ e−x and bounding the Taylor series for − log 1−x by x+2∗x2 for x=\n1−α , we have\nKL(p‖q)≤ −α (1−α)(α+ ) + 1−α + 4 2\n= ( 2 α + 4) 2 ≤ 4 α 2\nQ.E.D.\nFix a guessing algorithm, which at time t sees the output of a coin at. Let P1, · · · , Pn denote the distributions for the view of the algorithm from time 1 to T , when the biased coin is hidden in the ith position. The following result establishes for any guessing algorithm, there are at least N 3 positions that a biased coin could be and will not be played by the guessing algorithm with probability at least 1 2 . Specifically,\nLemma E.2 Let A be any guessing algorithm operating as specified above and let t ≤ Nα 60 2 , for\n≤ 1 4 and N ≥ 12. Then, there exists J ⊂ {1, · · · ,N} with |J | ≥ N 3 such that\n∀j ∈ J, Pj(at = j)≤ 1\n2\nLet Ni to be the number of times the algorithm plays coin i up to time t. Let P0 be the\nhypothetical distribution for the view of the algorithm when none of the N coins are biased. We shall define the set J by considering the behavior of the algorithm if tosses it saw were according to the distribution P0. We define,\nJ1 =\n{ i ∣∣∣∣EP0(Ni)≤ 3tN } , J2 = { i ∣∣∣∣P0(at = i)≤ 3N } and J = J1 ∩J2. (E.1)\nSince ∑ iEP0(Ni) = t and ∑ iP0(at = i) = 1, a counting argument would give us |J1| ≥ 2N\n3 and\n|J2| ≥ 2n 3 and hence |J | ≥ N 3 . Consider any j ∈ J , we will now prove that if the biased coin is at position j, then the probability of algorithm guessing the biased coin will not be significantly different from the P0 scenario. By Pinsker’s inequality, we have\n|Pj(at = j)−P0(at = j)| ≤ 1\n2\n√ 2 log 2 ·KL(P0‖Pj), (E.2)\nwhere KL(P0‖Pj) is the KL divergence of probability distributions P0 and Pj over the algorithm. Using the chain rule for KL-divergence, we have\nKL(P0‖Pj) =EP0(Nj)KL(p||q),\nwhere p is a Bernoulli distribution with parameter α and q is a Bernoulli distribution with param-\neter α+ . From Lemma E.1 and (E.1), we have that Therefore,\nKL(P0‖Pj)≤ 4 2\nα ,\nTherefore,\nPj(at = j)≤P0(at = j) + 1\n2\n√ 2 log 2 ·KL(P0‖Pj)\n≤ 3 N + 1 2\n√ (2 log 2) 4 2\nα EP0(Nj)\n≤ 3 N\n+ √ 2 log 2\n√ 3t 2\nNα ≤ 1 2 .\n(E.3)\nThe second inequality follows from (E.1), while the last inequality follows from the fact that N > 12 and t≤ Nα 60 2 Q.E.D..\nProof of Lemma 7.1 . Let = √\nN 60αT . Suppose algorithm A plays coin at at time t for each\nt= 1, · · · , T . Since T ≤ Nα 60 2 , for all t∈ {1, · · · , T −1} there exists a set Jt ⊂ {1, · · · ,N} with |Jt| ≥ N3 such that\n∀ j ∈ Jt, Pj(j ∈ St)≤ 1\n2\nLet i∗ denote the position of the biased coin. Then,\nEπ (µat | i∗ ∈ Jt)≤ 1 2 · (α+ ) + 1 2 ·α= α+ 2\nEπ (µat | i∗ 6∈ Jt)≤ α+\nSince |Jt| ≥ N3 and i ∗ is chosen randomly, we have P (i∗ ∈ Jt)≥ 13 . Therefore, we have\nµat ≤ 1 3 · ( α+ 2 ) + 2 3 · (α+ ) = α+ 5 6\nWe have µ∗ = α+ and hence the Regret≥ T 6 .\nLemma E.3 Let L be the total number of calls to AMNL when AMAB is executed for T time steps. Then,\nE(L)≤ 3T.\nProof. Let η` be the random variable that denote the duration, assortment S` has been considered by AMAB, i.e. η` = 0, if we no arm is pulled when AMNL suggested assortment S` and η` ≥ 1, otherwise. We have L−1∑ `=1 η` ≤ T.\nTherefore, we have E (∑L−1\n`=1 η` ) ≤ T . Note that E(η`) ≥ 12 . Hence, we have E(L) ≤ 2T + 1 ≤ 3T."
    } ],
    "references" : [ {
      "title" : "Fast probabilistic algorithms for hamiltonian circuits and matchings",
      "author" : [ "D. Angluin", "L.G. Valiant" ],
      "venue" : null,
      "citeRegEx" : "Angluin and Valiant.,? \\Q1977\\E",
      "shortCiteRegEx" : "Angluin and Valiant.",
      "year" : 1977
    }, {
      "title" : "Using confidence bounds for exploitation-exploration trade-offs",
      "author" : [ "Auer", "Peter", "Nicolo Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Proceedings of the Ninth Annual ACM Symposium on Theory of Computing . STOC",
      "citeRegEx" : "Auer et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2003
    }, {
      "title" : "Dynamic pricing with limited supply",
      "author" : [ "M. Babaioff", "S. Dughmi", "R. Kleinberg", "A. Slivkins" ],
      "venue" : null,
      "citeRegEx" : "Babaioff et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Babaioff et al\\.",
      "year" : 2015
    }, {
      "title" : "Discrete choice analysis: theory and application to travel demand",
      "author" : [ "M. Ben-Akiva", "S. Lerman" ],
      "venue" : "Transactions on Economics and Computation",
      "citeRegEx" : "Ben.Akiva and Lerman.,? \\Q1985\\E",
      "shortCiteRegEx" : "Ben.Akiva and Lerman.",
      "year" : 1985
    }, {
      "title" : "A markov chain approximation to choice modeling",
      "author" : [ "press. Blanchet", "Jose", "Guillermo Gallego", "Vineet Goyal" ],
      "venue" : null,
      "citeRegEx" : "Blanchet et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Blanchet et al\\.",
      "year" : 2016
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : null,
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Assortment planning under the multinomial logit model with",
      "author" : [ "J. Davis", "G. Gallego", "H. Topaloglu" ],
      "venue" : null,
      "citeRegEx" : "Davis et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Davis et al\\.",
      "year" : 2013
    }, {
      "title" : "Near-optimal algorithms for capacity constrained assortment optimization",
      "author" : [ "A. Désir", "V. Goyal" ],
      "venue" : null,
      "citeRegEx" : "Désir and Goyal.,? \\Q2014\\E",
      "shortCiteRegEx" : "Désir and Goyal.",
      "year" : 2014
    }, {
      "title" : "Capacity constrained assortment optimization under the markov",
      "author" : [ "A. SSRN . Désir", "V. Goyal", "D. Segev", "C. Ye" ],
      "venue" : null,
      "citeRegEx" : "Désir et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Désir et al\\.",
      "year" : 2015
    }, {
      "title" : "A nonparametric approach to modeling choice with limited data",
      "author" : [ "V. Farias", "S. Jagabathula", "D. Shah" ],
      "venue" : null,
      "citeRegEx" : "Farias et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Farias et al\\.",
      "year" : 2013
    }, {
      "title" : "Parametric bandits: The generalized linear case",
      "author" : [ "S. Filippi", "O. Cappe", "A. Garivier", "C. Szepesvári" ],
      "venue" : null,
      "citeRegEx" : "Filippi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Filippi et al\\.",
      "year" : 2010
    }, {
      "title" : "Constrained assortment optimization for the nested logit model",
      "author" : [ "G. Gallego", "H. Topaloglu" ],
      "venue" : null,
      "citeRegEx" : "Gallego and Topaloglu.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gallego and Topaloglu.",
      "year" : 2014
    }, {
      "title" : "Multi-armed bandits in metric spaces",
      "author" : [ "R. Kleinberg", "A. Slivkins", "E. Upfal" ],
      "venue" : "Proceedings of the Fortieth",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2008
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "ology and application. Operations Research",
      "citeRegEx" : "Lai and Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "Individual choice behavior: A theoretical analysis",
      "author" : [ "M. Lichman" ],
      "venue" : "UCI machine learning repository. URL http://archive.ics.uci.edu/ml. Luce, R.D",
      "citeRegEx" : "Lichman,? \\Q2013\\E",
      "shortCiteRegEx" : "Lichman",
      "year" : 2013
    }, {
      "title" : "logit choice model and capacity constraint. Operations research",
      "author" : [ "P. Rusmevichientong", "J.N. Tsitsiklis" ],
      "venue" : "Linearly parameterized bandits. Math. Oper. Res",
      "citeRegEx" : "Rusmevichientong and Tsitsiklis.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rusmevichientong and Tsitsiklis.",
      "year" : 2010
    }, {
      "title" : "Optimal dynamic assortment planning with demand learning",
      "author" : [ "D. Sauré", "A. Zeevi" ],
      "venue" : null,
      "citeRegEx" : "Sauré and Zeevi.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sauré and Zeevi.",
      "year" : 2013
    }, {
      "title" : "Revenue management under a general discrete choice model of consumer",
      "author" : [ "K. Talluri", "G. van Ryzin" ],
      "venue" : "Service Operations Management",
      "citeRegEx" : "Talluri and Ryzin.,? \\Q2004\\E",
      "shortCiteRegEx" : "Talluri and Ryzin.",
      "year" : 2004
    }, {
      "title" : "Concentration Bounds. From Corollary A.1, it follows that v̂i,τ , τ ∈ Ti(`) are i.i.d geometric random variables with mean vi. We will use this observation and extend the multiplicative ChernoffHoeffding bounds discussed in Mitzenmacher and Upfal (2005) and Babaioff et al. (2015) to geometric random variables and derive the result",
      "author" : [ "Eπ (v̂i", "`) = vi" ],
      "venue" : null,
      "citeRegEx" : ".v̂i et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : ".v̂i et al\\.",
      "year" : 2015
    }, {
      "title" : "2015) to geometric variables and prove the above result. Lemma 4.1 follows directly from Lemma A.2. The proof of Lemma A.2 is long and tedious and in the interest of continuity, we complete the proof in Appendix D. Following the proof",
      "author" : [ "Mitzenmacher", "Upfal", "Babaioff" ],
      "venue" : null,
      "citeRegEx" : "Mitzenmacher et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mitzenmacher et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "(The model was introduced independently by Luce (1959) and Plackett (1975), see also Train (2009), McFadden (1978), Ben-Akiva and Lerman (1985) for further discussion and survey of other commonly used choice models.",
      "startOffset" : 116,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "(The model was introduced independently by Luce (1959) and Plackett (1975), see also Train (2009), McFadden (1978), Ben-Akiva and Lerman (1985) for further discussion and survey of other commonly used choice models.) If the consumer preferences (MNL parameters in our setting) are known a priori, then the problem of computing the optimal assortment, which we refer to as the static assortment optimization problem, is well studied. Talluri and van Ryzin (2004) consider the unconstrained assortment planning problem under the MNL model and present a greedy approach to obtain the optimal",
      "startOffset" : 116,
      "endOffset" : 462
    }, {
      "referenceID" : 5,
      "context" : "Recent works of Davis et al. (2013) and Désir and Goyal (2014) consider assortment planning problems under MNL with various constraints.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "Recent works of Davis et al. (2013) and Désir and Goyal (2014) consider assortment planning problems under MNL with various constraints.",
      "startOffset" : 16,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "Recent works of Davis et al. (2013) and Désir and Goyal (2014) consider assortment planning problems under MNL with various constraints. Other choice models such as Nested Logit (Williams (1977), Davis et al.",
      "startOffset" : 16,
      "endOffset" : 195
    }, {
      "referenceID" : 5,
      "context" : "Recent works of Davis et al. (2013) and Désir and Goyal (2014) consider assortment planning problems under MNL with various constraints. Other choice models such as Nested Logit (Williams (1977), Davis et al. (2011), Gallego and Topaloglu (2014) and Li et al.",
      "startOffset" : 16,
      "endOffset" : 216
    }, {
      "referenceID" : 5,
      "context" : "Recent works of Davis et al. (2013) and Désir and Goyal (2014) consider assortment planning problems under MNL with various constraints. Other choice models such as Nested Logit (Williams (1977), Davis et al. (2011), Gallego and Topaloglu (2014) and Li et al.",
      "startOffset" : 16,
      "endOffset" : 246
    }, {
      "referenceID" : 5,
      "context" : "Recent works of Davis et al. (2013) and Désir and Goyal (2014) consider assortment planning problems under MNL with various constraints. Other choice models such as Nested Logit (Williams (1977), Davis et al. (2011), Gallego and Topaloglu (2014) and Li et al. (2015)), Markov Chain (Blanchet et al.",
      "startOffset" : 16,
      "endOffset" : 267
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al.",
      "startOffset" : 23,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al.",
      "startOffset" : 23,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature.",
      "startOffset" : 23,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al.",
      "startOffset" : 23,
      "endOffset" : 258
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Sauré and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon.",
      "startOffset" : 23,
      "endOffset" : 290
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Sauré and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon.",
      "startOffset" : 23,
      "endOffset" : 317
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Sauré and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other.",
      "startOffset" : 23,
      "endOffset" : 461
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Sauré and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Sauré and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an “explore first and exploit later” approach.",
      "startOffset" : 23,
      "endOffset" : 575
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Sauré and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Sauré and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an “explore first and exploit later” approach.",
      "startOffset" : 23,
      "endOffset" : 602
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Sauré and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Sauré and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an “explore first and exploit later” approach. In particular, a selected set of assortments are explored until parameters can be estimated to a desired accuracy and then the optimal assortment corresponding to the estimated parameters is offered for the remaining selling horizon. The exploration period depends on certain a priori knowledge about instance parameters. Assuming that the optimal and next-best assortment are “well separated,” they show an asymptotic O(N logT ) regret bound. However, their algorithm relies crucially on the a priori knowledge of certain instance parameters which is not readily available in practice. Furthermore, their policies also require a priori knowledge of the length of the planning horizon. In this work, we focus on approaches that simultaneously explore and exploit demand information and do not require any such a priori knowledge or assumptions; thereby, making our approach more universal in its scope. Our problem is closely related to the multi-armed bandit (MAB) paradigm. A naive mapping to that setting would consider every assortment as an arm, and as such, would lead to exponentially many arms. Popular extensions of MAB for large scale problems include the linear bandit (e.g., Auer (2003), Rusmevichientong and Tsitsiklis (2010)) and generalized linear bandit (Filippi et al.",
      "startOffset" : 23,
      "endOffset" : 1929
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Sauré and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Sauré and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an “explore first and exploit later” approach. In particular, a selected set of assortments are explored until parameters can be estimated to a desired accuracy and then the optimal assortment corresponding to the estimated parameters is offered for the remaining selling horizon. The exploration period depends on certain a priori knowledge about instance parameters. Assuming that the optimal and next-best assortment are “well separated,” they show an asymptotic O(N logT ) regret bound. However, their algorithm relies crucially on the a priori knowledge of certain instance parameters which is not readily available in practice. Furthermore, their policies also require a priori knowledge of the length of the planning horizon. In this work, we focus on approaches that simultaneously explore and exploit demand information and do not require any such a priori knowledge or assumptions; thereby, making our approach more universal in its scope. Our problem is closely related to the multi-armed bandit (MAB) paradigm. A naive mapping to that setting would consider every assortment as an arm, and as such, would lead to exponentially many arms. Popular extensions of MAB for large scale problems include the linear bandit (e.g., Auer (2003), Rusmevichientong and Tsitsiklis (2010)) and generalized linear bandit (Filippi et al.",
      "startOffset" : 23,
      "endOffset" : 1969
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Sauré and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Sauré and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an “explore first and exploit later” approach. In particular, a selected set of assortments are explored until parameters can be estimated to a desired accuracy and then the optimal assortment corresponding to the estimated parameters is offered for the remaining selling horizon. The exploration period depends on certain a priori knowledge about instance parameters. Assuming that the optimal and next-best assortment are “well separated,” they show an asymptotic O(N logT ) regret bound. However, their algorithm relies crucially on the a priori knowledge of certain instance parameters which is not readily available in practice. Furthermore, their policies also require a priori knowledge of the length of the planning horizon. In this work, we focus on approaches that simultaneously explore and exploit demand information and do not require any such a priori knowledge or assumptions; thereby, making our approach more universal in its scope. Our problem is closely related to the multi-armed bandit (MAB) paradigm. A naive mapping to that setting would consider every assortment as an arm, and as such, would lead to exponentially many arms. Popular extensions of MAB for large scale problems include the linear bandit (e.g., Auer (2003), Rusmevichientong and Tsitsiklis (2010)) and generalized linear bandit (Filippi et al. (2010)) problems.",
      "startOffset" : 23,
      "endOffset" : 2023
    }, {
      "referenceID" : 4,
      "context" : "(2015)), Markov Chain (Blanchet et al. (2016) and Désir et al. (2015)) and more general models (Farias et al. (2013) and Gallego et al. (2014)) are also considered in the literature. Most closely related to our work are the papers of Caro and Gallien (2007), Rusmevichientong et al. (2010) and Sauré and Zeevi (2013), where information on consumer preferences is not known and needs to be learned over the course of the selling horizon. Caro and Gallien (2007) consider the setting under which demand for products is independent of each other. Rusmevichientong et al. (2010) and Sauré and Zeevi (2013) consider the problem of minimizing regret under the MNL choice model and present an “explore first and exploit later” approach. In particular, a selected set of assortments are explored until parameters can be estimated to a desired accuracy and then the optimal assortment corresponding to the estimated parameters is offered for the remaining selling horizon. The exploration period depends on certain a priori knowledge about instance parameters. Assuming that the optimal and next-best assortment are “well separated,” they show an asymptotic O(N logT ) regret bound. However, their algorithm relies crucially on the a priori knowledge of certain instance parameters which is not readily available in practice. Furthermore, their policies also require a priori knowledge of the length of the planning horizon. In this work, we focus on approaches that simultaneously explore and exploit demand information and do not require any such a priori knowledge or assumptions; thereby, making our approach more universal in its scope. Our problem is closely related to the multi-armed bandit (MAB) paradigm. A naive mapping to that setting would consider every assortment as an arm, and as such, would lead to exponentially many arms. Popular extensions of MAB for large scale problems include the linear bandit (e.g., Auer (2003), Rusmevichientong and Tsitsiklis (2010)) and generalized linear bandit (Filippi et al. (2010)) problems. However, these do not apply directly to our problem, since the revenue corresponding to an assortment is nonlinear in problem parameters. Other works (see Chen et al. (2013)) have considered versions of MAB where one can play a subset of arms in each round and the expected reward is a function of rewards for the arms played.",
      "startOffset" : 23,
      "endOffset" : 2208
    }, {
      "referenceID" : 13,
      "context" : "This is comparable to the bounds established in Sauré and Zeevi (2013) and Rusmevichientong et al.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "This is comparable to the bounds established in Sauré and Zeevi (2013) and Rusmevichientong et al. (2010), even though we do not require any prior information on ∆ unlike the aforementioned work.",
      "startOffset" : 48,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : ", we can handle matroid constraints such as assignment, partition and more general totally unimodular constraints (see Davis et al. (2013)).",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : ", we can handle matroid constraints such as assignment, partition and more general totally unimodular constraints (see Davis et al. (2013)). Our algorithm is predicated on upper confidence bound (UCB) type logic, originally developed to balance the aforementioned exploration-exploitation trade-off in the context of the multi-armed bandit (MAB) problem (cf. Lai and Robbins (1985) and Auer et al.",
      "startOffset" : 119,
      "endOffset" : 382
    }, {
      "referenceID" : 1,
      "context" : "Lai and Robbins (1985) and Auer et al. (2002)).",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "We refer the reader to Davis et al. (2013) for a detailed discussion on assortment and pricing optimization problems that can be formulated under the TU constraints.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "Our policy is based on a non-trivial extension of the UCB algorithm Auer et al. (2002). It uses the past observations to maintain increasingly accurate upper confidence bounds for the MNL parameters {vi, i = 1, .",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "There are efficient polynomial time algorithms to solve the static assortment optimization problem under MNL model with known parameters (see Davis et al. (2013), Rusmevichientong et al.",
      "startOffset" : 142,
      "endOffset" : 162
    }, {
      "referenceID" : 6,
      "context" : "There are efficient polynomial time algorithms to solve the static assortment optimization problem under MNL model with known parameters (see Davis et al. (2013), Rusmevichientong et al. (2010)).",
      "startOffset" : 142,
      "endOffset" : 194
    }, {
      "referenceID" : 1,
      "context" : "Intuitively, these properties establish v UCB i,` as upper confidence bounds converging to actual parameters vi, akin to the upper confidence bounds used in the UCB algorithm for MAB in Auer et al. (2002). We provide the precise statements for the above mentioned properties in Lemma 4.",
      "startOffset" : 186,
      "endOffset" : 205
    }, {
      "referenceID" : 2,
      "context" : "We will use this observation and extend the multiplicative Chernoff-Hoeffding bounds discussed in Mitzenmacher and Upfal (2005) and Babaioff et al. (2015) to geometric random variables and derive the result.",
      "startOffset" : 132,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : "(2010) and Sauré and Zeevi (2013), the exploratory period does not depend on the specific instance parameters and is constant for all problem instances.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : "(2010) and Sauré and Zeevi (2013) for the cardinality constrained problem.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : "(2010) and Sauré and Zeevi (2013) for the cardinality constrained problem. (In fact our algorithm also has improved regret bounds compared to the O(N 2 log T ) bound established by Rusmevichientong et al. (2010)).",
      "startOffset" : 11,
      "endOffset" : 212
    }, {
      "referenceID" : 5,
      "context" : "1 is a simple extension of the proof of the Ω( √ NT ) lower bound for the Bernoulli instance with parameters 1 2 and 1 2 + (for example, see Bubeck and Cesa-Bianchi (2012)), and has been provided in Appendix E for the sake of completeness.",
      "startOffset" : 141,
      "endOffset" : 172
    }, {
      "referenceID" : 16,
      "context" : "We contrast the performance of Algorithm 1 with the approach in Sauré and Zeevi (2013) for different levels of separation between the optimal and sub-optimal revenues.",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "We contrast the performance of Algorithm 1 with the approach in Sauré and Zeevi (2013) for different levels of separation between the optimal and sub-optimal revenues. We observe that when the separation between the optimal assortment and second best assortment is sufficiently small, the approach in Sauré and Zeevi (2013) breaks down, i.",
      "startOffset" : 64,
      "endOffset" : 324
    }, {
      "referenceID" : 16,
      "context" : "In this section, we present a computational study comparing the performance of our algorithm to that of Sauré and Zeevi (2013). (To the best of our knowledge, Sauré and Zeevi (2013) is currently the best existing approach for our problem setting.",
      "startOffset" : 104,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "In this section, we present a computational study comparing the performance of our algorithm to that of Sauré and Zeevi (2013). (To the best of our knowledge, Sauré and Zeevi (2013) is currently the best existing approach for our problem setting.",
      "startOffset" : 104,
      "endOffset" : 182
    }, {
      "referenceID" : 16,
      "context" : "In this section, we present a computational study comparing the performance of our algorithm to that of Sauré and Zeevi (2013). (To the best of our knowledge, Sauré and Zeevi (2013) is currently the best existing approach for our problem setting.) To be implemented, their approach requires certain a priori information of a “separability parameter”; roughly speaking, measuring the degree to which the optimal and next-best assortments are distinct from a revenue standpoint. More specifically, their algorithm follows an explore-then-exploit approach, where every product is first required to be offered for a minimum duration of time that is determined by an estimate of said “separability parameter.” After this mandatory exploration phase, the parameters of the choice model are estimated based on the past observations and the optimal assortment corresponding to the estimated parameters is offered for the subsequent consumers. If the optimal assortment and the next best assortment are “well separated,” then the offered assortment is optimal with high probability, otherwise, the algorithm could potentially incur linear regret. Therefore, the knowledge of this “separability parameter” is crucial. For our comparison, we consider the exploration period suggested by Sauré and Zeevi (2013) and compare it with the performance of Algorithm 1 for different values of separation ( .",
      "startOffset" : 104,
      "endOffset" : 1299
    }, {
      "referenceID" : 16,
      "context" : "In this section, we present a computational study comparing the performance of our algorithm to that of Sauré and Zeevi (2013). (To the best of our knowledge, Sauré and Zeevi (2013) is currently the best existing approach for our problem setting.) To be implemented, their approach requires certain a priori information of a “separability parameter”; roughly speaking, measuring the degree to which the optimal and next-best assortments are distinct from a revenue standpoint. More specifically, their algorithm follows an explore-then-exploit approach, where every product is first required to be offered for a minimum duration of time that is determined by an estimate of said “separability parameter.” After this mandatory exploration phase, the parameters of the choice model are estimated based on the past observations and the optimal assortment corresponding to the estimated parameters is offered for the subsequent consumers. If the optimal assortment and the next best assortment are “well separated,” then the offered assortment is optimal with high probability, otherwise, the algorithm could potentially incur linear regret. Therefore, the knowledge of this “separability parameter” is crucial. For our comparison, we consider the exploration period suggested by Sauré and Zeevi (2013) and compare it with the performance of Algorithm 1 for different values of separation ( .) We will show that for any given exploration period, there is an instance where the approach in Sauré and Zeevi (2013) “breaks down” or in other words incurs",
      "startOffset" : 104,
      "endOffset" : 1508
    }, {
      "referenceID" : 16,
      "context" : "Since the implementation of the policy in Sauré and Zeevi (2013) requires knowledge of the selling horizon and minimum exploration period a priori, we consider the exploration period to be 20 logT as suggested in Sauré and Zeevi (2013) and the selling horizon as T = 10.",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "Since the implementation of the policy in Sauré and Zeevi (2013) requires knowledge of the selling horizon and minimum exploration period a priori, we consider the exploration period to be 20 logT as suggested in Sauré and Zeevi (2013) and the selling horizon as T = 10.",
      "startOffset" : 42,
      "endOffset" : 236
    }, {
      "referenceID" : 16,
      "context" : "Since the implementation of the policy in Sauré and Zeevi (2013) requires knowledge of the selling horizon and minimum exploration period a priori, we consider the exploration period to be 20 logT as suggested in Sauré and Zeevi (2013) and the selling horizon as T = 10. Figure 3 compares the regret of Algorithm 1 with that of Sauré and Zeevi (2013). The results are based on running 100 independent simulations with standard error of 2%.",
      "startOffset" : 42,
      "endOffset" : 351
    }, {
      "referenceID" : 16,
      "context" : "Since the implementation of the policy in Sauré and Zeevi (2013) requires knowledge of the selling horizon and minimum exploration period a priori, we consider the exploration period to be 20 logT as suggested in Sauré and Zeevi (2013) and the selling horizon as T = 10. Figure 3 compares the regret of Algorithm 1 with that of Sauré and Zeevi (2013). The results are based on running 100 independent simulations with standard error of 2%. We observe that the regret of the Sauré and Zeevi (2013) is better than the regret of Algorithm 1 when = 0.",
      "startOffset" : 42,
      "endOffset" : 497
    }, {
      "referenceID" : 16,
      "context" : "Since the implementation of the policy in Sauré and Zeevi (2013) requires knowledge of the selling horizon and minimum exploration period a priori, we consider the exploration period to be 20 logT as suggested in Sauré and Zeevi (2013) and the selling horizon as T = 10. Figure 3 compares the regret of Algorithm 1 with that of Sauré and Zeevi (2013). The results are based on running 100 independent simulations with standard error of 2%. We observe that the regret of the Sauré and Zeevi (2013) is better than the regret of Algorithm 1 when = 0.25 but is worse for other values of . This can be attributed to the fact that for the assumed exploration period, Their algorithm fails to identify the optimal assortment within the exploration phase with sufficient probability and hence incurs a linear regret for = 0.05,0.1 and 0.15. Specifically, among the 100 simulations we tested, the algorithm of Sauré and Zeevi (2013) identified the optimal assortment for only 7%,40%,61% and 97% cases, when = 0.",
      "startOffset" : 42,
      "endOffset" : 924
    }, {
      "referenceID" : 14,
      "context" : "We consider the “UCI Car Evaluation Database” (see Lichman (2013)) which contains attributes based information of N = 1728 cars and consumer ratings for each car.",
      "startOffset" : 51,
      "endOffset" : 66
    }, {
      "referenceID" : 16,
      "context" : "Figure 3 Comparison with the algorithm of Sauré and Zeevi (2013). The graphs (a), (b), (c) and (d) compares the performance of Algorithm 1 to that of Sauré and Zeevi (2013) on problem instance (8.",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "Figure 3 Comparison with the algorithm of Sauré and Zeevi (2013). The graphs (a), (b), (c) and (d) compares the performance of Algorithm 1 to that of Sauré and Zeevi (2013) on problem instance (8.",
      "startOffset" : 42,
      "endOffset" : 173
    }, {
      "referenceID" : 16,
      "context" : "It should be noted that Algorithm 1 did not require any a priori knowledge on the parameters unlike the other existing approaches such as Sauré and Zeevi (2013) and therefore can be applied to a wide range of other settings.",
      "startOffset" : 138,
      "endOffset" : 161
    } ],
    "year" : 2017,
    "abstractText" : "We consider a dynamic assortment selection problem, where in every round the retailer offers a subset (assortment) of N substitutable products to a consumer, who selects one of these products according to a multinomial logit (MNL) choice model. The retailer observes this choice and the objective is to dynamically learn the model parameters, while optimizing cumulative revenues over a selling horizon of length T . We refer to this exploration-exploitation formulation as the MNL-Bandit problem. Existing methods for this problem follow an explore-then-exploit approach, which estimate parameters to a desired accuracy and then, treating these estimates as if they are the correct parameter values, offers the optimal assortment based on these estimates. These approaches require certain a priori knowledge of “separability,” determined by the true parameters of the underlying MNL model, and this in turn is critical in determining the length of the exploration period. (Separability refers to the distinguishability of the true optimal assortment from the other sub-optimal alternatives.) In this paper, we give an efficient algorithm that simultaneously explores and exploits, achieving performance independent of the underlying parameters. The algorithm can be implemented in a fully online manner, without knowledge of the horizon length T . Furthermore, the algorithm is adaptive in the sense that its performance is near-optimal in both the “well separated” case, as well as the general parameter setting where this separation need not hold.",
    "creator" : "TeX"
  }
}