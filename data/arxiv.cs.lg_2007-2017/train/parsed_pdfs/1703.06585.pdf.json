{
  "name" : "1703.06585.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning",
    "authors" : [ "Abhishek Das", "Satwik Kottur", "José M.F. Moura", "Stefan Lee", "Dhruv Batra" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative ‘image guessing’ game between two agents – Q-BOT and A-BOT– who communicate in natural language dialog so that Q-BOT can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end – from pixels to multi-agent multi-round dialog to game reward. We demonstrate two experimental results. First, as a ‘sanity check’ demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/size). Thus, we demonstrate the emergence of grounded language and communication among ‘visual’ dialog agents with no human supervision at all. Second, we conduct large-scale real-image experiments on the VisDial dataset [4], where we pretrain with supervised dialog data and show that the RL ‘fine-tuned’ agents significantly outperform SL agents. Interestingly, the RL Q-BOT learns to ask questions that A-BOT is good at, ultimately resulting in more informative dialog and a better team."
    }, {
      "heading" : "1. Introduction",
      "text" : "The focus of this paper is visually-grounded conversational artificial intelligence (AI). Specifically, we would like to develop agents that can ‘see’ (i.e., understand the contents of an image) and ‘communicate’ that understanding in natural language (i.e., hold a dialog involving questions and answers about that image). We believe the next generation of intelligent systems will need to posses this ability to hold a dialog about visual content for a variety of applications: e.g., helping visually impaired users understand their surroundings [2] or social media content [35] (‘Who is in the photo? Dave. What is he doing?’), enabling analysts to\n*The first two authors (AD, SK) contributed equally.\nsift through large quantities of surveillance data (‘Did anyone enter the vault in the last month? Yes, there are 103 recorded instances. Did any of them pick something up?’), and enabling users to interact naturally with intelligent assistants (either embodied as a robot or not) (‘Did I leave my phone on my desk? Yes, it’s here. Did I miss any calls?’). Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31– 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent. Two recent works [4, 5] have proposed studying this task of visually-grounded dialog. Perhaps somewhat counterintuitively, both these works treat dialog as a static supervised learning problem, rather than an interactive agent learning problem that it naturally is. Specifically, both\nar X\niv :1\n70 3.\n06 58\n5v 1\n[ cs\n.C V\n] 2\n0 M\nar 2\n01 7\nworks [4, 5] first collect a dataset of human-human dialog, i.e., a sequence of question-answer pairs about an image (q1, a1), . . . , (qT , aT ). Next, a machine (a deep neural network) is provided with the image I , the human dialog recorded till round t− 1, (q1, a1), . . . , (qt−1, at−1), the follow-up question qt, and is supervised to generate the human response at. Essentially, at each round t, the machine is artificially ‘injected’ into the conversation between two humans and asked to answer the question qt; but the machine’s answer ât is thrown away, because at the next round t+1, the machine is again provided with the ‘ground-truth’ human-human dialog that includes the human response at and not the machine response ât. Thus, the machine is never allowed to steer the conversation because that would take the dialog out of the dataset, making it non-evaluable. In this paper, we generalize the task of Visual Dialog beyond the necessary first stage of supervised learning – by posing it as a cooperative ‘image guessing’ game between two dialog agents. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end – from pixels to multi-agent multi-round dialog to the game reward. Our setup is illustrated in Fig. 1. We formulate a game between a questioner bot (Q-BOT) and an answerer bot (ABOT). Q-BOT is shown a 1-sentence description (a caption) of an unseen image, and is allowed to communicate in natural language (discrete symbols) with the answering bot (ABOT), who is shown the image. The objective of this fullycooperative game is for Q-BOT to build a mental model of the unseen image purely from the natural language dialog, and then retrieve that image from a lineup of images. Notice that this is a challenging game. Q-BOT must ground the words mentioned in the provided caption (‘Two zebra are walking around their pen at the zoo.’), estimate which images from the provided pool contain this content (there will typically be many such images since captions describe only the salient entities), and ask follow-up questions (‘Any people in the shot? Are there clouds in the sky? Are they facing each other?’) that help it identify the correct image. Analogously, A-BOT must build a mental model of what QBOT understands, and answer questions (‘No, there aren’t any. I can’t see the sky. They aren’t.’) in a precise enough way to allow discrimination between similar images from a pool (that A-BOT does not have access to) while being concise enough to not confuse the imperfect Q-BOT. At every round of dialog, Q-BOT listens to the answer provided by A-BOT, updates its beliefs, and makes a prediction about the visual representation of the unseen image (specifically, the fc7 vector of I), and receives a reward from the environment based on how close Q-BOT’s prediction is to the true fc7 representation of I . The goal of Q-BOT and A-BOT is to communicate to maximize this reward. One critical issue is that both the agents are imperfect and noisy – both ‘forget’ things in the past, sometimes repeat them-\nselves, may not stay consistent in their responses, A-BOT does not have access to an external knowledge-base so it cannot answer all questions, etc. Thus, to succeed at the task, they must learn to play to each other’s strengths. An important question to ask is – why force the two agents to communicate in discrete symbols (English words) as opposed to continuous vectors? The reason is twofold. First, discrete symbols and natural language is interpretable. By forcing the two agents to communicate and understand natural language, we ensure that humans can not only inspect the conversation logs between two agents, but more importantly, communicate with them. After the two bots are trained, we can pair a human questioner with A-BOT to accomplish the goals of visual dialog (aiding visually/situationally impaired users), and pair a human answerer with Q-BOT to play a visual 20-questions game. The second reason to communicate in discrete symbols is to prevent cheating – if Q-BOT and A-BOT are allowed to exchange continuous vectors, then the trivial solution is for A-BOT to ignore Q-BOT’s question and directly convey the fc7 vector for I , allowing Q-BOT to make a perfect prediction. In essence, discrete natural language is an interpretable lowdimensional “bottleneck” layer between these two agents. Contributions. We introduce a novel goal-driven training for visual question answering and dialog agents. Despite significant popular interest in VQA (over 200 works citing [1] since 2015), all previous approaches have been based on supervised learning, making this the first instance of goaldriven training for visual question answering / dialog. We demonstrate two experimental results. First, as a ‘sanity check’ demonstration of pure RL (from scratch), we show results on a diagnostic task where perception is perfect – a synthetic world with ‘images’ containing a single object defined by three attributes (shape/color/style). In this synthetic world, for Q-BOT to identify an image, it must learn about these attributes. The two bots communicate via an ungrounded vocabulary, i.e., symbols with no pre-specified human-interpretable meanings (‘X’, ‘Y’, ‘1’, ‘2’). When trained end-to-end with RL on this task, we find that the two bots invent their own communication protocol – Q-BOT starts using certain symbols to query for specific attributes (‘X’ for color), and A-BOT starts responding with specific symbols indicating the value of that attribute (‘1’ for red). Essentially, we demonstrate the automatic emergence of grounded language and communication among ‘visual’ dialog agents with no human supervision! Second, we conduct large-scale real-image experiments on the VisDial dataset [4]. With imperfect perception on real images, discovering a human-interpretable language and communication strategy from scratch is both tremendously difficult and an unnecessary re-invention of English. Thus, we pretrain with supervised dialog data in VisDial before ‘fine tuning’ with RL; this alleviates a number of challenges\nin making deep RL converge to something meaningful. We show that these RL fine-tuned bots significantly outperform the supervised bots. Most interestingly, while the supervised Q-BOT attempts to mimic how humans ask questions, the RL trained Q-BOT shifts strategies and asks questions that the A-BOT is better at answering, ultimately resulting in more informative dialog and a better team."
    }, {
      "heading" : "2. Related Work",
      "text" : "Vision and Language. A number of problems at the intersection of vision and language have recently gained prominence, e.g., image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23]. Most related to this paper are two recent works on visually-grounded dialog [4, 5]. Das et al. [4] proposed the task of Visual Dialog, collected the VisDial dataset by pairing two subjects on Amazon Mechanical Turk to chat about an image (with assigned roles of ‘Questioner’ and ‘Answerer’), and trained neural visual dialog answering models. De Vries et al. [5] extended the Referit game [13] to a ‘GuessWhat’ game, where one person asks questions about an image to guess which object has been ‘selected’, and the second person answers questions in ‘yes’/‘no’/NA (natural language answers are disallowed). One disadvantage of GuessWhat is that it requires bounding box annotations for objects; our image guessing game does not need any such annotations and thus an unlimited number of game plays may be simulated. Moreover, as described in Sec. 1, both these works unnaturally treat dialog as a static supervised learning problem. Although both datasets contain thousands of human dialogs, they still only represent an incredibly sparse sample of the vast space of visually-grounded questions and answers. Training robust, visually-grounded dialog agents via supervised techniques is still a challenging task. In our work, we take inspiration from the AlphaGo [26] approach of supervision from human-expert games and reinforcement learning from self-play. Similarly, we perform supervised pretraining on human dialog data and fine-tune in an end-to-end goal-driven manner with deep RL. 20 Questions and Lewis Signaling Game. Our proposed image-guessing game is naturally the visual analog of the popular 20-questions game. More formally, it is a generalization of the Lewis Signaling (LS) [16] game, widely studied in economics and game theory. LS is a cooperative game between two players – a sender and a receiver. In the classical setting, the world can be in a number of finite discrete states {1, 2, . . . , N}, which is known to the sender but not the receiver. The sender can send one of N discrete symbols/signals to the receiver, who upon receiving the signal must take one of N discrete actions. The game is perfectly cooperative, and one simple (though not unique) Nash Equilibrium is the ‘identity mapping’, where the sender encodes each world state with a bijective signal, and similarly the\nreceiver has a bijective mapping from a signal to an action. Our proposed ‘image guessing’ game is a generalization of LS with Q-BOT being the receiver and A-BOT the sender. However, in our proposed game, the receiver (Q-BOT) is not passive. It actively solicits information by asking questions. Moreover, the signaling process is not ‘single shot’, but proceeds over multiple rounds of conversation. Text-only or Classical Dialog. Li et al. [17] have proposed using RL for training dialog systems. However, they hand-define what a ‘good’ utterance/dialog looks like (nonrepetition, coherence, continuity, etc.). In contrast, taking a cue from adversarial learning [9, 18], we set up a cooperative game between two agents, such that we do not need to hand-define what a ‘good’ dialog looks like – a ‘good’ dialog is one that leads to a successful image-guessing play. Emergence of Language. There is a long history of work on language emergence in multi-agent systems [22]. The more recent resurgence has focused on deep RL [10,15,21]. The high-level ideas of these unpublished concurrent works are similar to our synthetic experiments. For our large-scale real-image results, we do not want our bots to invent their own uninterpretable language and use pretraining on VisDial [4] to achieve ‘alignment’ with English."
    }, {
      "heading" : "3. Cooperative Image Guessing Game:",
      "text" : "In Full Generality and a Specific Instantiation\nPlayers and Roles. The game involves two collaborative agents – a questioner bot (Q-BOT) and an answerer bot (ABOT) – with an information asymmetry. A-BOT sees an image I , Q-BOT does not. Q-BOT is primed with a 1-sentence description c of the unseen image and asks ‘questions’ (sequence of discrete symbols over a vocabulary V ), which ABOT answers with another sequence of symbols. The communication occurs for a fixed number of rounds. Game Objective in General. At each round, in addition to communicating, Q-BOT must provide a ‘description’ ŷ of the unknown image I based only on the dialog history and both players receive a reward from the environment inversely proportional to the error in this description under some metric `(ŷ, ygt). We note that this is a general setting where the ‘description’ ŷ can take on varying levels of specificity – from image embeddings (or fc7 vectors of I) to textual descriptions to pixel-level image generations. Specific Instantiation. In our experiments, we focus on the setting where Q-BOT is tasked with estimating a vector embedding of the image I . Given some feature extractor (i.e., a pretrained CNN model, say VGG-16), no human annotation is required to produce the target ‘description’ ŷgt (simply forward-prop the image through the CNN). Reward/error can be measured by simple Euclidean distance, and any image may be used as the visual grounding for a dialog. Thus, an unlimited number of ‘game plays’ may be simulated."
    }, {
      "heading" : "4. Reinforcement Learning for Dialog Agents",
      "text" : "In this section, we formalize the training of two visual dialog agents (Q-BOT and A-BOT) with Reinforcement Learning (RL) – describing formally the action, state, environment, reward, policy, and training procedure. We begin by noting that although there are two agents (Q-BOT, A-BOT), since the game is perfectly cooperative, we can without loss of generality view this as a single-agent RL setup where the single “meta-agent” comprises of two “constituent agents” communicating via a natural language bottleneck layer. Action. Both agents share a common action space consisting of all possible output sequences under a token vocabulary V . This action space is discrete and in principle, infinitely-large since arbitrary length sequences qt, at may be produced and the dialog may go on forever. In our synthetic experiment, the two agents are given different vocabularies to coax a certain behavior to emerge (details in Sec. 5). In our VisDial experiments, the two agents share a common vocabulary of English tokens. In addition, at each round of the dialog t, Q-BOT also predicts ŷt, its current guess about the visual representation of the unseen image. This component of Q-BOT’s action space is continuous. State. Since there is information asymmetry (A-BOT can see the image I , Q-BOT cannot), each agent has its own observed state. For a dialog grounded in image I with caption c, the state of Q-BOT at round t is the caption and dialog history so far sQt = [c, q1, a1, . . . , qt−1, at−1], and the state of A-BOT also includes the image sAt = [I, c, q1, a1, . . . , qt−1, at−1, qt]. Policy. We model Q-BOT and A-BOT operating under stochastic policies πQ(qt | sQt ; θQ) and πA(at | sAt ; θA), such that questions and answers may be sampled from these policies conditioned on the dialog/state history. These policies will be learned by two separate deep neural networks parameterized by θQ and θA. In addition, Q-BOT includes a feature regression network f(·) that produces an image representation prediction after listening to the answer at round t, i.e., ŷt = f(s Q t , qt, at; θf ) = f(s Q t+1; θf ). Thus, the goal of policy learning is to estimate the parameters θQ, θA, θf . Environment and Reward. The environment is the image I upon which the dialog is grounded. Since this is a purely cooperative setting, both agents receive the same reward. Let `(·, ·) be a distance metric on image representations (Euclidean distance in our experiments). At each round t, we define the reward for a state-action pair as:\nrt ( sQt︸︷︷︸ state , (qt, at, yt)︸ ︷︷ ︸ action ) = ` ( ŷt−1, y gt )︸ ︷︷ ︸\ndistance at t-1\n− ` ( ŷt, y gt )︸ ︷︷ ︸\ndistance at t\n(1)\ni.e., the change in distance to the true representation before and after a round of dialog. In this way, we consider a question-answer pair to be low quality (i.e., have a negative reward) if it leads the questioner to make a worse estimate of\nthe target image representation than if the dialog had ended. Note that the total reward summed over all time steps of a dialog is a function of only the initial and final states due to the cancellation of intermediate terms, i.e.,\nT∑ t=1 rt ( sQt , (qt, at, yt)) ) = ` ( ŷ0, y gt ) − ` ( ŷT , y gt )︸ ︷︷ ︸\noverall improvement due to dialog\n(2)\nThis is again intuitive – ‘How much do the feature predictions of Q-BOT improve due to the dialog?’ The details of policy learning are described in Sec. 4.2, but before that, let us describe the inner working of the two agents.\n4.1. Policy Networks for Q-BOT and A-BOT\nFig. 2 shows an overview of our policy networks for Q-BOT and A-BOT and their interaction within a single round of dialog. Both the agent policies are modeled via Hierarchical Recurrent Encoder-Decoder neural networks, which have recently been proposed for dialog modeling [4, 24, 25]. Q-BOT consists of the following four components:\n- Fact Encoder: Q-BOT asks a question qt: ‘Are there any animals?’ and receives an answer at: ‘Yes, there are two elephants.’. Q-BOT treats this concatenated (qt, at)-pair as a ‘fact’ it now knows about the unseen image. The fact encoder is an LSTM whose final hidden state FQt ∈ R512 is used as an embedding of (qt, at).\n- State/History Encoder is an LSTM that takes the encoded fact FQt at each time step to produce an encoding of the prior dialog including time t as SQt ∈ R512. Notice that this results in a two-level hierarchical encoding of the dialog (qt, at)→ FQt and (F Q 1 , . . . , F Q t )→ S Q t .\n- Question Decoder is an LSTM that takes the state/history encoding from the previous round SQt−1 and generates question qt by sequentially sampling words.\n- Feature Regression Network f(·) is a single fullyconnected layer that produces an image representation prediction ŷt from the current encoded state ŷt = f(S Q t ).\nEach of these components and their relation to each other are shown on the left side of Fig. 2. We collectively refer to the parameters of the three LSTM models as θQ and those of the feature regression network as θf . A-BOT has a similar structure to Q-BOT with slight differences since it also models the image I via a CNN:\n- Question Encoder: A-BOT receives a question qt from Q-BOT and encodes it via an LSTM QAt ∈ R512.\n- Fact Encoder: Similar to Q-BOT, A-BOT also encodes the (qt, at)-pairs via an LSTM to get FAt ∈ R512. The purpose of this encoder is for A-BOT to remember what it has already told Q-BOT and be able to understand references to entities already mentioned.\n- State/History Encoder is an LSTM that takes as input at each round t – the encoded question QAt , the image features from VGG [27] y, and the previous fact encoding FAt−1 – to produce a state encoding, i.e.( (y,QA1 , F A 0 ), . . . , (y,Q A t , F A t−1) ) → SAt . This allows\nthe model to contextualize the current question w.r.t. the history while looking at the image to seek an answer.\n- Answer Decoder is an LSTM that takes the state encoding SAt and generates at by sequentially sampling words.\nOur code will be publicly available. To recap, a dialog round at time t consists of 1) Q-BOT generating a question qt conditioned on its state encoding SQt−1, 2) A-BOT encoding qt, updating its state encoding SAt , and generating an answer at, 3) Q-BOT and A-BOT both encoding the completed exchange as FQt and F A t , and 4) Q-BOT updating its state to SQt based on F Q t and making an image representation prediction ŷt for the unseen image."
    }, {
      "heading" : "4.2. Joint Training with Policy Gradients",
      "text" : "In order to train these agents, we use the REINFORCE [34] algorithm that updates policy parameters (θQ, θA, θf ) in response to experienced rewards. In this section, we derive the expressions for the parameter gradients for our setup. Recall that our agents take actions – communication (qt, at) and feature prediction ŷt – and our objective is to maximize the expected reward under the agents’ policies, summed over the entire dialog:\nmin θA,θQ,θg J(θA, θQ, θg) where, (3)\nJ(θA, θQ, θg) = E πQ,πA [ T∑ t=1 rt ( sQt , (qt, at, yt) )] (4)\nWhile the above is a natural objective, we find that considering the entire dialog as a single RL episode does not differentiate between individual good or bad exchanges within it. Thus, we update our model based on per-round rewards,\nJ(θA, θQ, θg) = E πQ,πA\n[ rt ( sQt , (qt, at, yt) )] (5)\nFollowing the REINFORCE algorithm, we can write the gradient of this expectation as an expectation of a quantity related to the gradient. For θQ, we derive this explicitly:\n∇θQJ = ∇θQ [\nE πQ,πA\n[rt (·)] ] (rt inputs hidden to avoid clutter)\n= ∇θQ [∑ qt,at πQ ( qt|sQt−1 ) πA ( at|sAt ) rt (·) ] =\n∑ qt,at πQ ( qt|sQt−1 ) ∇θQ log πQ ( qt|sQt−1 ) πA ( at|sAt ) rt (·)\n= E πQ,πA\n[ rt (·) ∇θQ log πQ ( qt|sQt−1 )] (6)\nSimilarly, gradient w.r.t. θA, i.e., ∇θAJ can be derived as\n∇θAJ = E πQ,πA\n[ rt (·) ∇θA log πA ( at|sAt )] . (7)\nAs is standard practice, we estimate these expectations with sample averages. Specifically, we sample a question from Q-BOT (by sequentially sampling words from the question decoder LSTM till a stop token is produced), sample its answer from A-BOT, compute the scalar reward for this round, multiply that scalar reward to gradient of log-probability of this exchange, propagate backward to compute gradients w.r.t. all parameters θQ, θA. This update has an intuitive interpretation – if a particular (qt, at) is informative (i.e., leads to positive reward), its probabilities will be pushed up (positive gradient). Conversely, a poor exchange leading to negative reward will be pushed down (negative gradient).\nFinally, since the feature regression network f(·) forms a deterministic policy, its parameters θf receive ‘supervised’ gradient updates for differentiable `(·, ·)."
    }, {
      "heading" : "5. Emergence of Grounded Dialog",
      "text" : "To succeed at our image guessing game, Q-BOT and A-BOT need to accomplish a number of challenging sub-tasks – they must learn a common language (do you understand what I mean when I say ‘person’?) and develop mappings between symbols and image representations (what does ‘person’ look like?), i.e., A-BOT must learn to ground language in visual perception to answer questions and QBOT must learn to predict plausible image representations – all in an end-to-end manner from a distant reward function. Before diving in to the full task on real images, we conduct a ‘sanity check’ on a synthetic dataset with perfect perception to ask – is this even possible? Setup. As shown in Fig. 3, we consider a synthetic world with ‘images’ represented as a triplet of attributes – 4 shapes, 4 colors, 4 styles – for a total of 64 unique images. A-BOT has perfect perception and is given direct access to this representation for an image. Q-BOT is tasked with deducing two attributes of the image in a particular order – e.g., if the task is (shape, color), Q-BOT would need to output (square, purple) for a (purple, square, filled) image seen by A-BOT (see Fig. 3b). We form all 6 such tasks per image. Vocabulary. We conducted a series of pilot experiments and found the choice of the vocabulary size to be crucial for coaxing non-trivial ‘non-cheating’ behavior to emerge. For instance, we found that if the A-BOT vocabulary VA is large enough, say |VA| ≥ 64 (#images), the optimal policy learnt simply ignores what Q-BOT asks and A-BOT conveys the entire image in a single token (e.g. token 1 ≡ (red, square, filled)). As with human communication, an impoverished vocabulary that cannot possibly encode the richness of the visual sensor is necessary for non-trivial dialog to emerge. To ensure at least 2 rounds of dialog, we restrict each agent to only produce a single symbol utterance per round from ‘minimal’ vocabularies VA = {1, 2, 3, 4} for A-BOT and VQ = {X,Y, Z} for Q-BOT. Since |VA|#rounds < #images,\na non-trivial dialog is necessary to succeed at the task. Policy Learning. Since the action space is discrete and small, we instantiate Q-BOT and A-BOT as fully specified tables of Q-values (state, action, future reward estimate) and apply tabular Q-learning with Monte Carlo estimation over 10k episodes to learn the policies. Updates are done alternately where one bot is frozen while the other is updated. During training, we use -greedy policies [28], ensuring an action probability of 0.6 for the greedy action and split the remaining probability uniformly across other actions. At test time, we default to greedy, deterministic policy obtained from these -greedy policies. The task requires outputting the correct attribute value pair based on the task and image. Since there are a total of 4+ 4+ 4 = 12 unique values across the 3 attributes, Q-BOT’s final action selects one of 12×12=144 attribute-pairs. We use +1 and −1 as rewards for right and wrong predictions. Results. Fig. 3d shows the reward achieved by the agents’ policies vs. number of RL iterations (each with 10k episodes/dialogs). We can see that the two quickly learn the optimal policy. Fig. 3b,c show some example exchanges between the trained bots. We find that the two invent their own communication protocol – Q-BOT consistently uses specific symbols to query for specific attributes: X → color, Y → shape, Z → style. And A-BOT consistently responds with specific symbols to indicate the inquired attribute, e.g., if QBOT emits X (asks for color), A-BOT responds with: 1 → purple, 2 → green, 3 → blue, 4 → red. Similar mappings exist for responses to other attributes. Essentially, we find the automatic emergence of grounded language and a communication protocol among ‘visual’ dialog agents without any human supervision!"
    }, {
      "heading" : "6. Experiments",
      "text" : "Our synthetic experiments in the previous section establish that when faced with a cooperative task where information must be exchanged, two agents with perfect perception are capable of developing a complex communication protocol. In general, with imperfect perception on real images, discovering human-interpretable language and communication\nstrategy from scratch is both tremendously difficult and an unnecessary re-invention of English. We leverage the recently introduced VisDial dataset [4] that contains (as of the publicly released v0.5) human dialogs (10 rounds of question-answer pairs) on 68k images from the COCO dataset, for a total of 680k QA-pairs. Example dialogs from the VisDial dataset are shown in Tab. 1. Image Feature Regression. We consider a specific instantiation of the visual guessing game described in Sec. 3 – specifically at each round t, Q-BOT needs to regress to the vector embedding ŷt of image I corresponding to the fc7 (penultimate fully-connected layer) output from VGG16 [27]. The distance metric used in the reward computation is `2, i.e. rt(·) = ||ygt − ŷt−1|| 2 2 − ||ygt − ŷt|| 2 2. Training Strategies. We found two training strategies to be crucial to ensure/improve the convergence of the RL framework described in Sec. 4, to produce any meaningful dialog exchanges, and to ground the agents in natural language. 1) Supervised Pretraining. We first train both agents in a supervised manner on the train split of VisDial [4] v0.5 under an MLE objective. Thus, conditioned on human dialog history, Q-BOT is trained to generate the follow-up question by human1, A-BOT is trained to generate the response by human2, and the feature network f(·) is optimized to regress to y. The CNN in A-BOT is pretrained on ImageNet. This pretraining ensures that the agents can generally recognize some objects/scenes and emit English questions/answers. The space of possible (qt, at) is tremendously large and without pretraining most exchanges result in no information gain about the image. 2) Curriculum Learning. After supervised pretraining, we ‘smoothly’ transition the agents to RL training according to a curriculum. Specifically, we continue supervised training for the first K (say 9) rounds of dialog and transition to policy-gradient updates for the remaining 10 −K rounds. We start at K = 9 and gradually anneal to 0. This curriculum ensures that the agent team does not suddenly diverge off policy, if one incorrect q or a is generated. Models are pretrained for 15 epochs on VisDial, after which we transition to policy-gradient training by annealing K down by 1 every epoch. All LSTMs are 2-layered with 512- d hidden states. We use Adam [14] with a learning rate of 10−3, and clamp gradients to [−5, 5] to avoid explosion. All our code will be made publicly available. There is no explicit state-dependent baseline in our training as we initialize from supervised pretraining and have zero-centered reward, which ensures a good proportion of random samples are both positively and negatively reinforced. Model Ablations. We compare to a few natural ablations of our full model, denoted RL-full-QAf. First, we evaluate the purely supervised agents (denoted SL-pretrained), i.e., trained only on VisDial data (no RL). Comparison to these agents establishes how much RL helps over super-\nvised learning. Second, we fix one of Q-BOT or A-BOT to the supervised pretrained initialization and train the other agent (and the regression network f ) with RL; we label these as Frozen-Q or Frozen-A respectively. Comparing to these partially frozen agents tell us the importance of coordinated communication. Finally, we freeze the regression network f to the supervised pretrained initialization while training Q-BOT and A-BOT with RL. This measures improvements from language adaptation alone. We quantify performance of these agents along two dimensions – how well they perform on the image guessing task (i.e. image retrieval) and how closely they emulate human dialogs (i.e. performance on VisDial dataset [4]). Evaluation: Guessing Game. To assess how well the agents have learned to cooperate at the image guessing task, we setup an image retrieval experiment based on the test split of VisDial v0.5 (∼9.5k images), which were never seen by the agents in RL training. We present each image + an automatically generated caption [12] to the agents, and allow them to communicate over 10 rounds of dialog. After each round, Q-BOT predicts a feature representation ŷt. We sort the entire test set in ascending distance to this prediction and compute the rank of the source image. Fig. 4a shows the mean percentile rank of the source image for our method and the baselines across the rounds (shaded region indicates standard error). A percentile rank of 95% means that the source image is closer to the prediction than 95% of the images in the set. Tab. 1 shows example exchanges between two humans (from VisDial), the SL-pretrained and the RL-full-QAf agents. We make a few observations:\n• RL improves image identification. We see that RL-full-QAf significantly outperforms SL-pretrained and all other ablations (e.g., at round 10, improving percentile rank by over 3%), indicating that our training framework is indeed effective at training these agents for image guessing.\n• All agents ‘forget’; RL agents forget less. One interesting trend we note in Fig. 4a is that all methods significantly improve from round 0 (caption-based retrieval) to rounds 2 or 3, but beyond that all methods with the exception of RL-full-QAf get worse, even though they have strictly more information. As shown in Tab. 1, agents will often get stuck in infinite repeating loops but this is much rarer for RL agents. Moreover, even when RL agents repeat themselves, it is after longer gaps (2-5 rounds). We conjecture that the goal of helping a partner over multiple rounds encourages longer term memory retention.\n• RL leads to more informative dialog. SL A-BOT tends to produce ‘safe’ generic responses (‘I don’t know’, ‘I can’t see’) but RL A-BOT responses are\nmuch more detailed (‘It is hard to tell but I think it’s black’). These observations are consistent with recent literature in text-only dialog [17]. Our hypothesis for this improvement is that human responses are diverse and SL trained agents tend to ‘hedge their bets’ and achieve a reasonable log-likelihood by being non-\ncommittal. In contrast, such ‘safe’ responses do not help Q-BOT in picking the correct image, thus encouraging an informative RL A-BOT.\nEvaluation: Emulating Human Dialogs. To quantify how well the agents emulate human dialog, we evaluate A-BOT on the retrieval metrics proposed by Das et al. [4]. Specifi-\ncally, every question in VisDial is accompanied by 100 candidate responses. We use the log-likehood assigned by the A-BOT answer decoder to sort these candidates and report the results in Tab. 4b. We find that despite the RL A-BOT’s answer being more informative, the improvements on VisDial metrics are minor. We believe this is because while the answers are correct, they may not necessarily mimic human responses (which is what the answer retrieval metrics check for). In order to dig deeper, we train a variant of Frozen-Q with a multi-task objective – simultaneous (1) ground truth answer supervision and (2) image guessing reward, to keep A-BOT close to human-like responses. We use a weight of 1.0 for the SL loss and 10.0 for RL. This model, denoted Frozen-Q-multi, performs better than all other approaches on VisDial answering metrics, improving the best reported result on VisDial by 0.7 mean rank (relative improvement of 3%). Note that this gain is entirely ‘free’ since no additional annotations were required for RL. Human Study. We conducted a human interpretability study to measure (1) whether humans can easily understand the Q-BOT-A-BOT dialog, and (2) how imagediscriminative the interactions are. We show human subjects a pool of 16 images, the agent dialog (10 rounds), and ask humans to pick their top-5 guesses for the image the two agents are talking about. We find that mean rank of the ground-truth image for SL-pretrained agent dialog is 3.70 vs. 2.73 for RL-full-QAf dialog. In terms of MRR, the comparison is 0.518 vs. 0.622 respectively. Thus, under both metrics, humans find it easier to guess the unseen image based on RL-full-QAf dialog exchanges, which shows that agents trained within our framework (1) successfully develop image-discriminative language, and (2) this language is interpretable; they do not deviate off English."
    }, {
      "heading" : "7. Conclusions",
      "text" : "To summarize, we introduce a novel training framework for visually-grounded dialog agents by posing a cooperative ‘image guessing’ game between two agents. We use deep reinforcement learning to learn the policies of these agents end-to-end – from pixels to multi-agent multi-round dialog to game reward. We demonstrate the power of this framework in a completely ungrounded synthetic world, where the agents communicate via symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol without any human supervision. We go on to instantiate this game on the VisDial [4] dataset, where we pretrain with supervised dialog data. We find that the RL ‘fine-tuned’ agents not only significantly outperform SL agents, but learn to play to each other’s strengths, all the while remaining interpretable to outside humans observers.\nAcknowledgements. We thank Devi Parikh for helpful discussions. This work was funded in part by the following\nawards to DB – NSF CAREER award, ONR YIP award, ONR Grant N00014-14-1-0679, ARO YIP award, ICTAS Junior Faculty award, Google Faculty Research Award, Amazon Academic Research Award, AWS Cloud Credits for Research, and NVIDIA GPU donations. SK was supported by ONR Grant N00014-12-1-0903, and SL was partially supported by the Bradley Postdoctoral Fellowship. Views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor."
    } ],
    "references" : [ {
      "title" : "VQA: Visual Question Answering",
      "author" : [ "S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh" ],
      "venue" : "ICCV,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "VizWiz: Nearly Real-time Answers to Visual Questions",
      "author" : [ "J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White", "T. Yeh" ],
      "venue" : "UIST,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation",
      "author" : [ "X. Chen", "C.L. Zitnick" ],
      "venue" : "CVPR,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Visual Dialog",
      "author" : [ "A. Das", "S. Kottur", "K. Gupta", "A. Singh", "D. Yadav", "J.M. Moura", "D. Parikh", "D. Batra" ],
      "venue" : "CVPR,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "GuessWhat?! visual object discovery through multi-modal dialogue",
      "author" : [ "H. de Vries", "F. Strub", "S. Chandar", "O. Pietquin", "H. Larochelle", "A. Courville" ],
      "venue" : "In CVPR, 2017",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2017
    }, {
      "title" : "Long-term Recurrent Convolutional Networks for Visual Recognition and Description",
      "author" : [ "J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell" ],
      "venue" : "CVPR,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "From Captions to Visual Concepts and Back",
      "author" : [ "H. Fang", "S. Gupta", "F.N. Iandola", "R.K. Srivastava", "L. Deng", "P. Dollár", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig" ],
      "venue" : "CVPR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering",
      "author" : [ "H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu" ],
      "venue" : "NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Generative Adversarial Nets",
      "author" : [ "I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : "NIPS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Emergence of language with multiagent games: Learning to communicate with sequences of symbols",
      "author" : [ "S. Havrylov", "I. Titov" ],
      "venue" : "ICLR Workshop,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "DenseCap: Fully Convolutional Localization Networks for Dense Captioning",
      "author" : [ "J. Johnson", "A. Karpathy", "L. Fei-Fei" ],
      "venue" : "CVPR,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "A. Karpathy", "L. Fei-Fei" ],
      "venue" : "CVPR,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "ReferItGame: Referring to Objects in Photographs of Natural Scenes",
      "author" : [ "S. Kazemzadeh", "V. Ordonez", "M. Matten", "T.L. Berg" ],
      "venue" : "EMNLP,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "ICLR,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multi-agent cooperation and the emergence of (natural) language",
      "author" : [ "A. Lazaridou", "A. Peysakhovich", "M. Baroni" ],
      "venue" : "10  ICLR,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Convention: A philosophical study",
      "author" : [ "D. Lewis" ],
      "venue" : "John Wiley & Sons,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Deep Reinforcement Learning for Dialogue Generation",
      "author" : [ "J. Li", "W. Monroe", "A. Ritter", "M. Galley", "J. Gao", "D. Jurafsky" ],
      "venue" : "EMNLP,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Adversarial learning for neural dialogue generation",
      "author" : [ "J. Li", "W. Monroe", "T. Shi", "A. Ritter", "D. Jurafsky" ],
      "venue" : "arXiv preprint arXiv:1701.06547,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
      "author" : [ "M. Malinowski", "M. Fritz" ],
      "venue" : "NIPS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Ask your neurons: A neural-based approach to answering questions about images",
      "author" : [ "M. Malinowski", "M. Rohrbach", "M. Fritz" ],
      "venue" : "ICCV,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Emergence of grounded compositional language in multi-agent populations",
      "author" : [ "I. Mordatch", "P. Abbeel" ],
      "venue" : "arXiv preprint arXiv:1703.04908,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Evolution of Communication and Language in Embodied Agents",
      "author" : [ "S. Nolfi", "M. Mirolli" ],
      "venue" : "Springer Publishing Company, Incorporated, 1st edition,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Exploring Models and Data for Image Question Answering",
      "author" : [ "M. Ren", "R. Kiros", "R. Zemel" ],
      "venue" : "NIPS,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models",
      "author" : [ "I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau" ],
      "venue" : "AAAI,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues",
      "author" : [ "I.V. Serban", "A. Sordoni", "R. Lowe", "L. Charlin", "J. Pineau", "A. Courville", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1605.06069,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree search. Nature, 2016",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "ICLR,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT Press,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "MovieQA: Understanding Stories in Movies through Question-Answering",
      "author" : [ "M. Tapaswi", "Y. Zhu", "R. Stiefelhagen", "A. Torralba", "R. Urtasun", "S. Fidler" ],
      "venue" : "CVPR,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Joint Video and Text Parsing for Understanding Events and Answering Queries",
      "author" : [ "K. Tu", "M. Meng", "M.W. Lee", "T.E. Choe", "S.C. Zhu" ],
      "venue" : "IEEE MultiMedia,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sequence to Sequence - Video to Text",
      "author" : [ "S. Venugopalan", "M. Rohrbach", "J. Donahue", "R.J. Mooney", "T. Darrell", "K. Saenko" ],
      "venue" : "ICCV,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Translating Videos to Natural Language Using Deep Recurrent Neural Networks",
      "author" : [ "S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko" ],
      "venue" : "NAACL HLT,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan" ],
      "venue" : "CVPR,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "R.J. Williams" ],
      "venue" : "Machine learning, 8(3-4):229–256,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Using artificial intelligence to help blind people ‘see’ facebook",
      "author" : [ "S. Wu", "H. Pique", "J. Wieland" ],
      "venue" : "http://newsroom.fb.com/news/2016/04/using-artificialintelligence-to-help-blind-people-see-facebook/,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
      "author" : [ "K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio" ],
      "venue" : "ICML,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Second, we conduct large-scale real-image experiments on the VisDial dataset [4], where we pretrain with supervised dialog data and show that the RL ‘fine-tuned’ agents significantly outperform SL agents.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : ", helping visually impaired users understand their surroundings [2] or social media content [35] (‘Who is in the photo? Dave.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 34,
      "context" : ", helping visually impaired users understand their surroundings [2] or social media content [35] (‘Who is in the photo? Dave.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31– 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.",
      "startOffset" : 108,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31– 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.",
      "startOffset" : 108,
      "endOffset" : 127
    }, {
      "referenceID" : 32,
      "context" : "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31– 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.",
      "startOffset" : 108,
      "endOffset" : 127
    }, {
      "referenceID" : 35,
      "context" : "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31– 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.",
      "startOffset" : 108,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31– 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 19,
      "context" : "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31– 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 22,
      "context" : "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31– 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 28,
      "context" : "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31– 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 29,
      "context" : "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31– 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 3,
      "context" : "Two recent works [4, 5] have proposed studying this task of visually-grounded dialog.",
      "startOffset" : 17,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "Two recent works [4, 5] have proposed studying this task of visually-grounded dialog.",
      "startOffset" : 17,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "works [4, 5] first collect a dataset of human-human dialog, i.",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 4,
      "context" : "works [4, 5] first collect a dataset of human-human dialog, i.",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "Despite significant popular interest in VQA (over 200 works citing [1] since 2015), all previous approaches have been based on supervised learning, making this the first instance of goaldriven training for visual question answering / dialog.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "Second, we conduct large-scale real-image experiments on the VisDial dataset [4].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 32,
      "context" : ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 22,
      "context" : ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "Most related to this paper are two recent works on visually-grounded dialog [4, 5].",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "Most related to this paper are two recent works on visually-grounded dialog [4, 5].",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "[4] proposed the task of Visual Dialog, collected the VisDial dataset by pairing two subjects on Amazon Mechanical Turk to chat about an image (with assigned roles of ‘Questioner’ and ‘Answerer’), and trained neural visual dialog answering models.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] extended the Referit game [13] to a ‘GuessWhat’ game, where one person asks questions about an image to guess which object has been ‘selected’, and the second person answers questions in ‘yes’/‘no’/NA (natural language answers are disallowed).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "[5] extended the Referit game [13] to a ‘GuessWhat’ game, where one person asks questions about an image to guess which object has been ‘selected’, and the second person answers questions in ‘yes’/‘no’/NA (natural language answers are disallowed).",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : "In our work, we take inspiration from the AlphaGo [26] approach of supervision from human-expert games and reinforcement learning from self-play.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : "More formally, it is a generalization of the Lewis Signaling (LS) [16] game, widely studied in economics and game theory.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : "[17] have proposed using RL for training dialog systems.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "In contrast, taking a cue from adversarial learning [9, 18], we set up a cooperative game between two agents, such that we do not need to hand-define what a ‘good’ dialog looks like – a ‘good’ dialog is one that leads to a successful image-guessing play.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "In contrast, taking a cue from adversarial learning [9, 18], we set up a cooperative game between two agents, such that we do not need to hand-define what a ‘good’ dialog looks like – a ‘good’ dialog is one that leads to a successful image-guessing play.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "There is a long history of work on language emergence in multi-agent systems [22].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "The more recent resurgence has focused on deep RL [10,15,21].",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "The more recent resurgence has focused on deep RL [10,15,21].",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "The more recent resurgence has focused on deep RL [10,15,21].",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "For our large-scale real-image results, we do not want our bots to invent their own uninterpretable language and use pretraining on VisDial [4] to achieve ‘alignment’ with English.",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "Both the agent policies are modeled via Hierarchical Recurrent Encoder-Decoder neural networks, which have recently been proposed for dialog modeling [4, 24, 25].",
      "startOffset" : 150,
      "endOffset" : 161
    }, {
      "referenceID" : 23,
      "context" : "Both the agent policies are modeled via Hierarchical Recurrent Encoder-Decoder neural networks, which have recently been proposed for dialog modeling [4, 24, 25].",
      "startOffset" : 150,
      "endOffset" : 161
    }, {
      "referenceID" : 24,
      "context" : "Both the agent policies are modeled via Hierarchical Recurrent Encoder-Decoder neural networks, which have recently been proposed for dialog modeling [4, 24, 25].",
      "startOffset" : 150,
      "endOffset" : 161
    }, {
      "referenceID" : 26,
      "context" : "- State/History Encoder is an LSTM that takes as input at each round t – the encoded question Qt , the image features from VGG [27] y, and the previous fact encoding F t−1 – to produce a state encoding, i.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 33,
      "context" : "In order to train these agents, we use the REINFORCE [34] algorithm that updates policy parameters (θQ, θA, θf ) in response to experienced rewards.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 27,
      "context" : "During training, we use -greedy policies [28], ensuring an action probability of 0.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "Image + Caption Human-Human dialog [4] SL-pretrained Q-BOT-A-BOT dialog RL-full-QAf Q-BOT A-BOT dialog",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "We leverage the recently introduced VisDial dataset [4] that contains (as of the publicly released v0.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "3 – specifically at each round t, Q-BOT needs to regress to the vector embedding ŷt of image I corresponding to the fc7 (penultimate fully-connected layer) output from VGG16 [27].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : "We first train both agents in a supervised manner on the train split of VisDial [4] v0.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : "We use Adam [14] with a learning rate of 10−3, and clamp gradients to [−5, 5] to avoid explosion.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "performance on VisDial dataset [4]).",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "We present each image + an automatically generated caption [12] to the agents, and allow them to communicate over 10 rounds of dialog.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 16,
      "context" : "These observations are consistent with recent literature in text-only dialog [17].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "[4].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "We go on to instantiate this game on the VisDial [4] dataset, where we pretrain with supervised dialog data.",
      "startOffset" : 49,
      "endOffset" : 52
    } ],
    "year" : 2017,
    "abstractText" : "We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative ‘image guessing’ game between two agents – Q-BOT and A-BOT– who communicate in natural language dialog so that Q-BOT can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end – from pixels to multi-agent multi-round dialog to game reward. We demonstrate two experimental results. First, as a ‘sanity check’ demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/size). Thus, we demonstrate the emergence of grounded language and communication among ‘visual’ dialog agents with no human supervision at all. Second, we conduct large-scale real-image experiments on the VisDial dataset [4], where we pretrain with supervised dialog data and show that the RL ‘fine-tuned’ agents significantly outperform SL agents. Interestingly, the RL Q-BOT learns to ask questions that A-BOT is good at, ultimately resulting in more informative dialog and a better team.",
    "creator" : "LaTeX with hyperref package"
  }
}