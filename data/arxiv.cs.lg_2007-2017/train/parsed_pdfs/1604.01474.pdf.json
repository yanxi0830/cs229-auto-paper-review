{
  "name" : "1604.01474.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Self-Paced Multi-Task Learning",
    "authors" : [ "Changsheng Li", "Fan Wei", "Junchi Yan", "Weishan Dong", "Qingshan Liu", "Hongyuan Zha" ],
    "emails" : [ "dongweis}@cn.ibm.com.", "fanwei@stanford.edu.", "jcyan@sei.ecnu.edu.cn", "qsliu@nuist.edu.cn.", "zha@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nMulti-task learning (MTL) is a learning paradigm, where multiple tasks such as classification or regression tasks, are jointly learnt. One basic assumption in MTL is that there exists common or related information among the tasks; learning such information can result in better generalization performance than independently learning each individual task [1]. It is particularly desirable to share such information across the tasks, when there are many related tasks but the available training data per task is limited. Due to its empirical success, MTL has been applied to various domains, including disease modeling and prediction [2], web image and video search [3], relative attributes learning [4].\nMany multi-task learning methods have been proposed, which in general can be categorized into two classes [5], [6]. The first class assumes that all the tasks share a common yet low-rank feature representation. Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,∞ norm regularization [11] and trace norm regularization [12]. The other class of methods assumes that the model parameters used by the tasks are related to each other [13], [14], [15]. To combine the two classes of approaches, some works have attempted to simultaneously learn model parameter relation and feature representation in a unified framework.\nC. Li and W. Dong are with IBM Research-China, Beijing 100094, China. Email: {lcsheng, dongweis}@cn.ibm.com.\nF. Wei is with Department of Mathematics, Stanford University. E-mail: fanwei@stanford.edu.\nJ. Yan is with East China Normal University, Shanghai, China. E-mail: jcyan@sei.ecnu.edu.cn\nQ. Liu is with Nanjing University of Information Science and Technology, Nanjing 210014, China. Email: qsliu@nuist.edu.cn.\nH. Zha is with Georgia Institute of Technology, Atlanta, USA. E-mail: zha@cc.gatech.edu\n[16], [17], [18]. However, the assumption that all the tasks share some common information is fairly strong, recent works propose task grouping or task outlier detecting, which assume that there exists common information only within a subset of tasks, or there exists outlier task having no relation with the other tasks [19], [20], [21].\nHowever, the algorithms above treat all the tasks equally and all the instances per task equally during training. Such a learning process might not be optimal from the perspective of the human brain’s cognitive process. For example, a student often starts with easier concepts (e.g. recognizing objects in simple scenes where an object is clearly visible) and builds up to more complex ones (e.g. cluttered images with occlusions). In the regime of MTL, not only do there exist ‘easy’ to ‘hard’ training instances, but also ‘easy’ to ‘hard’ tasks. For example, recognizing a monkey from the dataset consisting of a monkey and a tiger is a relatively ‘easy’ task, while recognizing a baboon from the dataset consisting of baboons and orangutans is a relatively ‘hard’ task. In the first task, an image of monkey with plain background is a relatively ‘easy’ positive instance, while an image of monkey with complex background is a relatively ‘hard’ positive instance. Hence it can be advantageous to consider jointly the complexities of both instances and tasks in the multi-task model. Recently, SeqMT [22] uses a heuristic algorithm to rank all the tasks, and learns the tasks sequentially for multi-task classification tasks. SeqMT fixes the order of tasks, sequentially ordering from the previously solved tasks to the next-to-be-solved task, as it assumes that information is transferred only between subsequent tasks. However, in a dynamic setting, such a predetermined order is not flexible and comprehensive. Therefore, it is desired for the algorithm to automatically adjust the order of the tasks as the learning proceeds iteratively. In addition, SeqMT ignores the ‘easiness’ and ‘hardness’ properties of the instances during learning.\nRecently, self-paced learning (SPL) [23] proposes a paradigm advocating that learning should be first done for ‘simple’ or ‘easy’ instances, and then gradually move to ‘complex’ or ‘hard’ instances, inspired by the cognitive process of humans. By taking advantage of an instance-oriented selfpaced regularizer, SPL can dynamically adjust the learning order of the instances. This enables SPL to suit with the dynamic learning model well. Self-paced learning has been empirically demonstrated to be helpful for avoiding bad local minima, especially in the presence of heavy noise and outliers [23], [24]. By now, it has been successfully applied to many research areas, such as multi-view learning [25], matrix factorization [26], and multi-instance learning [27].\nIn this paper, we propose a novel MTL framework, called Self-Paced Multi-Task Learning (SPMTL), which aims at\nar X\niv :1\n60 4.\n01 47\n4v 1\n[ cs\n.L G\n] 6\nA pr\n2 01\n6\n2 building the connection between multi-task learning and selfpaced learning in a principled manner. The contributions are: • It is the first work, to our best knowledge, where a\nprincipled MTL model jointly takes into consideration the complexities of both the training instances and tasks. Our model can be interpreted as a self-paced MTL model to explore common information among the tasks. • We propose a new regularizer, which can set priorities for both the tasks and instances in each learning iteration, and use smooth weights for the tasks and instances priorities. To the best of our knowledge, this is also the first taskoriented self-paced regularizer in literature. • We design an effective learning algorithm for our model, and we explain theoretically its performance. Experimental results on the toy and real-world datasets demonstrate the effectiveness of the proposed approach."
    }, {
      "heading" : "II. SELF-PACED MULTI-TASK LEARNING",
      "text" : "Suppose we are given m learning tasks {Ti}mi=1. For the i-th task Ti, the training set Di consists of ni data points {(xij , yij)}nij=1 , where xij ∈ Rd is the feature representation of the j-th instance and yij is its corresponding output, such as yij ∈ R for regression and yij ∈ {−1, 1} for binary classification problem. The total number of the training instances is n = ∑m i=1 ni. The prediction model for the ith task is defined as f(pi,xij) = pTi xij . The objective of MTL is to derive optimal prediction models for all the tasks simultaneously. In practical applications, information is often shared in the same task group, and information in different task groups is also overlapped to some degree. Thus the MTL problem can be formulated by GO-MTL [21] as:\nmin U,V m∑ i=1 1 ni ni∑ j=1 L(yij ,vTi UTxij)+ α‖U‖2F +β‖V‖1, (1)\nwhere V = [v1, . . . ,vm] ∈ Rk×m. α ≥ 0 and β ≥ 0 are two trade-off parameters. L(yij ,vTi UTxij) is the empirical loss on the training data points (xij , yij), and Uvi acts as the learned prediction model pi. U is a d × k matrix with each column representing a basis. V is a k×m matrix whose columns contain the coefficients of the linear combination of the basis for the corresponding tasks.\nThe key idea of GO-MTL is that the weight vector of each task can be represented as a linear combination of a subset of k basis tasks (since vi is sparse, a subset of k basis tasks is used for representing the weight vector). Therefore, the tasks with the same basis can be seen as belonging to the same group, while the tasks whose basis are orthogonal are sure to belong to different groups. The partially overlapping of bases enables the algorithm to model those tasks which are not in the same group but still have something in common.\nIn the objective function (1), all the tasks and instances are treated equally for learning the model. However, as discussed earlier, there exists the distinction between ‘easy’ and ‘hard’ across both the tasks and instances in many real-world scenarios. Inspired by the cognitive process of human that often learns from the easy to the hard, it is beneficial to incorporate the easy-to-hard strategy into the process of MTL.\nIn addition, like many MTL formulations, problem (1) is not convex, thus having the risk of stuck in bad local minima during optimization, especially in the presence of noises and outliers. Previous works have demonstrated that adding prior information, such as the easy-to-hard strategy, can alleviate the problem of bad local minima [23]. In light of these points, we attempt to tailor the MTL process by simultaneously considering the complexities of both tasks and instances, which can be interpreted as a self-paced MTL framework. We propose a new objective function as follows:\nmin w,U,V m∑ i=1 1 ni ni∑ j=1 w (i) j L(yij ,v T i U Txij) + α‖U‖2F\n+β‖V‖1 + f(w, λ, γ) (2) s.t. w\n(i) j ∈ [0, 1],∀j = 1, . . . ni, i = 1, . . . ,m,\nwhere w = [w(1)1 , . . . , w (1) n1 , w (2) 1 , . . . , w (2) n2 , . . . , w (m) nm ] ∈ Rn denotes the importance weights imposed on all instances. Different from the objective function (1), the first term in (2) is a weighted loss term on all the instances. f(w, λ, γ) denotes the self-paced regularizer that determines the instances and tasks used for training. So far, many self-paced regularizers have been proposed for various tasks [23], [27], but almost all of them focus on imposing weights on the instance level. Thus, in order to simultaneously impose weights on the instance level and the task level, we propose a new self-paced regularizer as follows:\nf(w, λ, γ)=−λ m∑ i=1 ni∑ j=1 w (i) j +γ m∑ i=1 √√√√ 1 ni ni∑ j=1 (w (i) j ) 2\n= −λ m∑ i=1 ‖w(i)‖1 + γ m∑ i=1 1 √ ni ‖w(i)‖2, (3)\nwhere w(i) = [w(i)1 , . . . , w (i) ni ] ∈ [0, 1]ni , and thus w = [w(1), . . . ,w(m)]. λ and γ are two self-paced parameters imposed on the instance level and the task level, respectively. There are two terms in Eq. (3): The first term is the negative l1-norm inherited from the conventional SPL, which favors selecting the easy over hard instances. The second term is an adaptive l2,1-norm of a matrix, which favors selecting the easy over hard tasks. We use 1√ni in the second term to avoid the task imbalance, when one task has so many data points that dominates the norm. We know that minimizing the l2,1 norm of a matrix can make the matrix sparse in rows or columns [7]; thus minimizing the second term in (3) makes the w(i)’s corresponding to large empirical loss L (i.e., hard tasks) be close to or equal to zero vectors. In other words, this groupsparsity representation is expected to realize task selection from ‘easy’ ones to ‘hard’ ones. As the learning proceeds, ‘harder’ instances and tasks will be included in training by gradually increasing λ and reducing γ.\nPlugging (3) into (2), we obtain the final objective function:\n3 min w,U,V m∑ i=1 1 ni w(i)L̂(i) + α‖U‖2F + β‖V‖1\n−λ m∑ i=1 ‖w(i)‖1 + γ m∑ i=1 1 √ ni ‖w(i)‖2 (4)\ns.t. w(i) ∈ [0, 1]ni ,∀i = 1, . . . ,m,\nwhere the vector L̂(i) = [L(i)1 , . . . ,L (i) ni ] T . In this paper, we focus on the regression tasks, and define L(i)j = L(yij ,vTi UTxij) = (yij − vTi UTxij)2. Note our method can be naturally applied to classification tasks by using a classification loss function L(i)j as in [28]."
    }, {
      "heading" : "III. OPTIMIZATION",
      "text" : "We solve the optimization problem (4) in an alternating fashion by the following three main steps. i) Optimize w with fixed U and V: the optimal w can be obtained by decomposing the optimization function into individual problems for each task Ti:\nmin w(i)∈[0,1]ni\nLi := 1\nni w(i)L̂(i)−λ‖w(i)‖1+ γ √ ni ‖w(i)‖2. (5)\nFirst we assume L(i)1 ≤ L (i) 2 ≤ · · · ≤ L (i) ni . Let p(i) =∑\nk0<j<k1\n(λ− L (i) j\nni )2, and q(i) = ∑ k0<j<k1 (λ− L (i) j ni ). For each i\nand arbitrary k1 > k0, we define c∗(k0, k1), L(k0, k1), G∗i , S ∗ i for later computation: 1)\nc∗(k0, k1) =  √ k0ni/(γ2 − nip(i)), if γ 2 ni 6= p(i),( λ− L(i)k0+1/ni )−1 , if γ 2 ni = p(i) and γ 2 ni < q(i),\n0, if γ 2\nni =p(i)and γ\n2\nni ≥ q(i). 2) L(k0, k1) = ∑k0 j=1 L(i)j ni − λ(k0 + c∗(k0, k1)q(i)) +\nγ√ ni\n√ k0 + c∗(k0, k1)2p(i).\n3) G∗i be the smallest j such that L (i) j ≥ λni. 4) S∗i be the largest j such that L (i) j ≤ niλ− √ niγ.\nThe following theorem gives the global optimum of (5).\nTheorem 1. Let k1 = G∗i , and k0 be obtained by optimizing\nk0 = arg min S∗i ≤k0<k1 L(k0, k1) (6)\ns.t.  γ2 ni − p(i) ≥ 0, or positive if k0 > 0, (7) c∗(k0, k1)(λ−L(i)k0+1/ni) < 1, if k0 + 1<k1,(8) L(i)k0 ni + γ √ ni ( k0 + c ∗(k0, k1) 2 p(i) )−1/2\n≤ λ, if k0 + 1 < k1. (9)\nThen, the optimal w(i) is given by,\nw (i) j =  1, if j ≤ k0, 0, if j ≥ k1,\nc∗(k0, k1)(λ− L(i)j ni ), if k0 < j < k1.\n(10)\nThus it takes only linear time O(ni) to compute w(i).\nProof of Theorem 1 is in Appendix at the end of the paper. ii) Optimize U with fixed w and V: This step is to learn the k basis tasks. The optimal U can be obtained by solving:\nmin U m∑ i=1 1 ni w(i)L̂(i) + α‖U‖2F , (11)\nThe necessary optimality condition is that the derivative of (11) with respective to U is equal to zeros. Thus, we have\nm∑ i=1 ni∑ j=1 w (i) j ni xijx T ijUviv T i + αU = m∑ i=1 ni∑ j=1 w (i) j ni yijxijv T i ,\n⇒  m∑ i=1 ni∑ j=1 w (i) j ni (viv T i )⊗ (xijxTij) + αI  vec(U) =\nm∑ i=1 ni∑ j=1 w (i) j ni yijvec(xijvTi ), (12)\nwhere ⊗ denotes the Kronecker product and vec(·) is an operator that reshapes a matrix of size d× k into a vector of size dk×1. Here we have used a property of Kronecker product that vec(ABC) = (CT⊗A)vec(B). This is the standard form of system of linear equations that is full rank and thus has a unique solution. We can solve it using many efficient methods, such as LU decomposition. iii) Optimize V with fixed w and U: This step aims to learn the coefficient matrix. The optimization problem can be decomposed into individual problems for vi as:\nmin vi\n1\nni w(i)L̂(i) + β‖vi‖1. (13)\nThe above problem is a l1 regularized optimization problem with 1niw\n(i)L̂(i) differentiable with respect to vi. To solve this problem, a variety of efficient methods, such as optimal projected gradient, can be used."
    }, {
      "heading" : "IV. THEORETICAL ANALYSIS",
      "text" : "Here we relax the two penalty terms α‖U‖2F +β‖V‖1 in our formulation (2) to a weaker constraint: U,V are bounded by a constant C (it can also be changed to bounded rank). This relaxation renders our following theoretical analysis approximate while more mathematically tractable and consumable. In fact, this simplification shares the same motivation for our raw formulation as intuitively we want U,V to be simple.\nThe true values are yrij = v rT i U rTxij . We observe yij = yrij+eij with eij independent Gaussian distribution with mean 0. In each round, to solve for U,V is to solve\nmin U,V bounded m∑ i=1 1 ni ni∑ j=1 w (i) j (yij−v T i U Txij) 2. (14)\nLet the weighted error (or the ‘average’ error) be defined as\nER = √√√√ m∑ i=1 1 ni ni∑ j=1 w (i) j (y r ij − vTi UTxij)2. (15)\n4 Theorem 2. Let the matrix W be [W]ij = ( w (i) j /ni )1/2 . Then\nER ≤ 2 √√√√∑ ij w (i) j ni e2ij = 2‖W E‖2.\nwhere [E]ij = eij and is the entry-wise multiplication operator. Let [Σ]ij = Var(eij) and ∑ ni = N . For each fixed > 0, with probability at least 1 − 2Ne− 2/4, ER ≤ ‖W Σ‖2.\nProof. Let U∗,V∗ be the optimal solution. We have\nER = √√√√ m∑ i=1 1 ni ni∑ j=1 w (i) j (yij − v∗Ti U∗ Txij − eij)2\n≤ √√√√ m∑ i=1 1 ni ni∑ j=1 w (i) j (yij−v∗Ti U∗ Txij)2 + ‖W E‖2\n≤ √√√√ m∑ i=1 1 ni ni∑ j=1 w (i) j (yij − vrTi Ur Txij)2 + ‖W E‖2\n= √√√√ m∑ i=1 1 ni ni∑ j=1 w (i) j e 2 ij + ‖W E‖2 = 2‖W E‖2,\nThe last inequality holds because U∗,V∗ are the ones minimizing (14).\nTo prove the concentration inequality, we notice that\nPr 2 √√√√ m∑\ni=1 ni∑ j=1 w (i) j ni e2ij ≥ √√√√ m∑ i=1 ni∑ j=1 w (i) j ni Var(eij)  ≤Pr ( exists i, j : e2ij ≥ 2Var(eij)/4 ) .\nThis is because the event 2 √∑m i=1 ∑ni j=1 w (i) j ni e2ij ≥\n√∑m i=1 ∑ni j=1 w (i) j ni Var(eij) is contained in the event that there exists i, j with e2ij ≥ 2Var(eij)/4. Since eij is distributed as N(0, σij) where σ2ij = Var(eij), we have Pr(|eij | ≥ σij/2) ≤ 2 exp(− 2/4). Therefore by the union bound, Pr ( exists ij : e2ij ≥ 2Var(eij)/4 ) ≤∑\nij Pr ( e2ij ≥ 2Var(eij)/4 ) ≤ 2N exp(− 2/4).\nFrom the above analysis we can see that, if we fix the set of the values of w(i)j and can exchange the weights between w (i) j and w(i)j′ , then the upper bound for the ‘average’ error ER is minimized when the samples with higher noise variance have smaller weights. In other words, harder samples should be given smaller weights. And with probability exponentially small, the error is large."
    }, {
      "heading" : "V. EXPERIMENTS",
      "text" : "We conduct the experiments on one toy dataset and two realworld datasets to illustrate the effectiveness of our method. We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and\na recently proposed method, called Calibration [30].1 In addition, we also compare with single task learning (STL) where tasks are learned independently. Note that we do not compare with SeqMT [22] because it is hard to have a fair comparison since SeqMT is tailored to solve classification tasks, while our current model focuses on regression problems. For all the datasets, we randomly select the training instances from each task with different training ratios (10%, 20% and 30%) and use the rest of instances to form the testing set. We evaluate all the algorithms in terms of both root mean squared error (rMSE) and normalized mean squared error (nMSE), which are commonly used in multi-task learning problem [31], [32]. To have a fair comparison, we validate the regularization parameters of all the methods in the same search space on a subset of the training set, and use the optimal parameters to train the final models. The initial self-paced parameters λ and γ are set such that more than half of tasks are selected, and then they are iteratively increased and decreased, respectively. We repeat each case 10 times and report the average results."
    }, {
      "heading" : "A. Toy Example",
      "text" : "We first describe the synthetic data generation procedure. Let there be 3 groups and each group has 10 tasks. There are 100 instances in each task; each instance is represented by a 15-dimensional feature vectors. We generate parameter vectors for 4 latent tasks, i.e., U in the proposed formulation, in 20 dimensions, with each entry drawn i.i.d. from a standard normal distribution. Based on U, we generate the first 10 tasks by linearly combining only the first two latent tasks. In a similar manner, the next 10 tasks are generated by linearly combining the second and the third latent tasks. Last 10 task are generated by linear combination of the last two latent tasks. All the coefficients of linear combinations, i.e., V, are drawn i.i.d. from a standard normal distribution. The instance xij is sampled from a standard Gaussian distribution, and the response is yij = vTi U\nTxij+ξij . To create tasks with different difficulty levels, we add different noise to tasks and instances by setting ξij = σiθj , where σi’s are i.i.d. from a normal distribution N(0, 5), and θj is drawn i.i.d. from N(0, 1).\nWe first visualize w in (2) using 20% training data. Figure 1 (a), (b), and (c) show that in the earlier iterations, some ‘hard’ tasks are not selected for learning (‖wi‖1 = 0). In later iterations, more tasks are included (0 < ‖wi‖1 ≤ 20). Similarly, instances in one task are also selected from ‘easy’ ones to ‘hard’ ones, as shown in Figure 1 (d), (e), and (f).\nWe also report the statistical results on this data in Table I. From these results we conclude that: (1) All multi-task learning methods outperform single-task learning methods, which proves the effectiveness of multi-task learning. (2) Our proposed self-paced multi-task learning method outperforms all other MTL methods. In addition, as can be seen from the objective functions (1) and (2), GO-MTL is a special case to our method: when all the entries in w are 1, our method is reduced to GO-MTL. This shows that our method can improve the prediction performance of the model by jointly considering the complexities of the tasks and the instances.\n1The codes of the compared methods are obtained from the corresponding authors.\n5\nTABLE II PERFORMANCE COMPARISON AMONG DIFFERENT METHODS ON THE OHSUMED DATASET IN TERMS OF RMSE AND NMSE.\nMeasure Training ratio STL DG-MTL Calibration MSMTFL GO-MTL SPMTL\nrMSE 10% 1.0690 0.8197 0.6346 0.7528 0.7762 0.6298 20% 0.8400 0.6961 0.6341 0.7519 0.6372 0.6140 30% 0.7474 0.6527 0.6140 0.7512 0.6105 0.5946 nMSE 10% 3.9915 2.1414 1.0719 1.4592 1.7331 1.0844 20% 2.3802 1.4228 1.0804 1.4584 1.1433 1.0286 30% 1.7118 1.2022 1.0134 1.4578 1.1433 0.9661\nTABLE III PERFORMANCE COMPARISON AMONG DIFFERENT METHODS ON THE ISOLET DATASET IN TERMS OF RMSE AND NMSE.\nMeasure Training ratio STL DG-MTL Calibration MSMTFL GO-MTL SPMTL\nrMSE 10% 6.1931 5.8498 5.8494 6.3696 5.9806 5.2505 20% 5.6947 5.4044 5.6002 5.9407 5.2275 4.9706 30% 5.4524 5.2210 5.3867 5.6384 5.0173 4.8697 nMSE 10% 0.6836 0.6098 0.6088 0.7239 0.6361 0.4902 20% 0.5773 0.5199 0.5578 0.6284 0.4859 0.4393 30% 0.5275 0.4837 0.5149 0.5640 0.4464 0.4205"
    }, {
      "heading" : "B. Real-World Data Experiments",
      "text" : "In this section, we conduct the experiments on two realworld datasets: OHSUMED and Isolet. The first dataset is a bibliographical online searching document collection. It consists of 53 queries, which are our tasks. Each query comes with multiple returned documents with labels indicating how relevant the returned document is to the query: “definitely relevant”, “possibly relevant”, or “not relevant”. These documents and their labels are our instances. There are in total 7,546 instances. The response is the relevance level (0,1,2). The second dataset is collected from 150 individuals each pronounces the English alphabet twice. Thus there are 52 samples from each individual. Each English letter corresponds to a label (1-26) and we treat the labels as regression values as [29]. The individuals are grouped into 5 groups by speaking similarity. Thus, we naturally have 5 tasks with each task corresponding to a group. There are 1560, 1560, 1560, 1558, and 1559 instances in the 5 tasks respectively.\nTables II and III report the performance measured by rMSE and nMSE for the OHSUMED dataset and the Isolet dataset, respectively. Our SPMTL outperforms all the compared methods with smaller error rates on both real datasets. This again demonstrates that our method is effective by introducing selfpaced learning scheme into multi-task learning regime.\nWe also test the effectiveness of considering either or both instance order and task order in our method on the OHSUMED dataset. By setting γ = 0 in (4), we only consider the complexities of instances. We call it Self-Paced Instance Weight Learning (SPIWL). We conduct the experiments on the 30% training data with the results shown in Figure 2. SPIWL performs better than GO-MTL, which suggests including the instances from ‘easy’ to ‘hard’ improves the performance. SPMTL outperforms SPIWL, which shows that learning ‘easy’\n5\n10 15 20 25 30 0 5 10\n15\n20\n(a) iter=1\n5\n10 15 20 25 30 0 5 10\n15\n20\n(b) iter=15\n5\n10 15 20 25 30 0 5 10\n15\n20\n(c) iter=30\n5\n10\n15\n20 0\n0.2\n0.4\n0.6\n0.8\n1\n(d) iter=1\n5\n10\n15\n20 0\n0.2\n0.4\n0.6\n0.8\n1\n(e) iter=15\n5\n10\n15\n20 0\n0.2\n0.4\n0.6\n0.8\n1\n(f) iter=30\nFig. 1. The visualization of the learned w. The rows in (a), (b), and (c) denote the corresponding task weights, i.e., the summation of the weights of all the instances in one task. The rows in (d), (e), and (f) denote the corresponding instance weights in the i-th task, here i=18. Dark blue denotes that the values are close to zero.\n0.58\n0.59\n0.6\n0.61\n0.62\nrM S\nE\nGO−MTL SPIWL SPMTL\n(a) rMSE\n0.9\n0.95\n1\n1.05\n1.1\n1.15\nnM S\nE\nGO−MTL SPIWL SPMTL\n(b) nMSE\nFig. 2. Effectiveness verification of considering task order and instance order on the OHSUMED dataset.\ntasks first and gradually adding ‘hard’ tasks into training can also be helpful for prediction."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "We proposed a novel MTL algorithm, namely SPMTL. Unlike previous works that treat tasks and instances equally during learning, our model gradually include instances and tasks according to an easy-to-hard order. To achieve this goal, we propose a new task-oriented self-paced regularizer in our\n6 formulation. Experiments on both synthetic dataset and real datasets have verified the effectiveness of SPMTL."
    }, {
      "heading" : "VII. APPENDIX",
      "text" : "Proof of Theorem 1. Recall that L(i)1 ≤ L (i) 2 ≤ · · · ≤ L (i) ni . The optimal solution of (5) should be w(i)j ≥ w (i) j′ for j < j\n′, since otherwise we can swap the values for w(i)j and w (i) j′ to decrease the value in (5). Thus w(i)1 ≥ w (i) 2 ≥ · · · ≥ w (i) ni .\nSince w(i)j ∈ [0, 1], we know that in the optimal solution,\neither ∂Li(w\n(i) j )\n∂w (i) j\n= 0 or w(i)j = 0, 1. For a given j, we have\n∂Li\n∂w (i) j\n= L(i)j ni − λ+ γw (i) j√ ni ∑ s(w (i) s )2 . (16)\nIt is clear that w (i) j√∑ s(w (i) s )2 ∈ [0, 1]. If L (i) j ni ≥ λ, then ∂Li ∂w (i) j ≥ 0; thus Li is monotonely increasing in w (i) j . To minimize (5) we should let w(i)j = 0. Similarly, L(i)j ni − λ ≤ − γ√ni implies\n∂Li ∂w\n(i) j\n≤ 0; Li is decreasing in w(i)j . Thus w (i) j = 1. Thus\nk0 ≥ S∗i , k1 ≤ G∗i by the monotonicity of w (i) j ’s.\nWhen k1 > k0+1, the rest of the w (i) j ’s satisify\n∂Li(w (i) j )\n∂w (i) j\n=\n0. Equation (16) tells us that the rest of the w(i)j ’s are proportional to L(i)j ni − λ when fixing ∑ s(w (i) s )2. Thus for some constant c to be found later, w(i)j = c(λ − L (i) j /ni). Since recall w(i)j = 1 for j ≤ k0 and w (i) j = 0 for j ≥ k1, we\nhave ∑ s(w (i) s )2 = k0+ ∑ k0<j<k1\nc2(λ−L(i)j /ni)2. Plugging in back to (16), we have\n0 = L(i)j ni − λ+ γc(λ− L(i)j /ni) √ ni √ k0 + ∑ k0<j<k1 c2(λ− L(i)j /ni)2 .\nSince λ− L(i)j /ni 6= 0 as S∗i < j < G∗i , we have c2γ2/ni = k0 + ∑ k0<j<k1 c2(λ− L(i)j /ni) 2. (17)\nIn order to solve c, we need ∑ k0<j<k1\nc2(λ − L(i)j /ni)2< γ2/ni if k0 > 0, and ∑ k0<j<k1\nc2(λ−L(i)j /ni)2 ≤ γ2/ni if k0 = 0. Thus c = c∗(k0, k1) by solving the above equation if k0 6= 0 or γ2/ni 6= ∑ k0<j<k1\n(λ−L(i)j /ni)2. By the constraint 0 ≤ w(i)j ≤ 1 and the monotonicity of w (i) j , we just need w (i) k0+1\n= c∗(k0, k1)(λ− L(i)k0+1/ni) ≤ 1. By plugging in, the value in (5) is exactly L(k0, k1) defined before. Recall that we also need the constraints above for this set of solution to be valid, which are precisely the constraints (7), (8).\nWhen k0 = 0 and γ2/ni = ∑ k0<j<k1\n(λ − L(i)j /ni)2, we cannot obtain c from (17). In this case, we have\nL(k0, k1) = k0∑ j=1 L(i)j ni − λc ∑ k0<j<k1 (λ− L(i)j ni ) + γ2 ni c.\nL(k0, k1) is a linear function in c. To minimize it, we need the sign of ∂L∂c = −λ ∑ k0<j<k1 (λ − L (i) j ni ) + γ 2 ni . If the sign is non-negative, then L(k0, k1) is monotone increasing in c.\nThus we choose the minimum c, i.e., 0. If the sign is negative, then L(k0, k1) is monotone decreasing in c, and we pick the maximum c. Since w(i)j = c(λ − L(i)j ni\n) ≤ 1, and L(i)j ’s are monotone, we have c∗(k0, k1) = 1/(λ− L(i)k0+1/ni).\nThe optimal values for w(i) is determined by choosing the minimum of all the L(k0, k1) after plugging in c∗(k0, k1) under the constraints.\nIn the optimal solution w(i)j ’s have achieved the maximum value 1 for j ≤ k0 implying ∂Li\n∂w (i) j\n≤ 0. Plugging in\nthe values for w(i), for j ≤ k0 we have 0 ≥ L(i)j ni −\nλ + γ/ √ ni√\nk0+ ∑k1−1\nt=k0+1 c∗(k0,k1)\n2(λ−L(i)t /ni)2 . By monotonicity of\nL(i)j ’s, we just need to guarantee it holds for j = k0 + 1. Similarly, if w(i)j = 0 for j ≥ k1, then\n∂Li ∂w\n(i) j\n≥ 0. Thus\nfor j ≥ k1, we have ∂Li ∂w\n(i) j\n= L(i)j ni − λ+ β·0√∑\ns(w (i) s )2 ≥ 0. By\nmonotonicity of L(i)j ’s, we have L(i)k1 ni ≥ λ. Notice that G∗i is defined to be the smallest j such that L(i)j ni ≥ λ and k1 ≤ G∗i . We must have k1 = G∗i ."
    } ],
    "references" : [ {
      "title" : "Multitask learning",
      "author" : [ "R. Caruana" ],
      "venue" : "Machine learning, vol. 28, no. 1, pp. 41–75, 1997.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A multi-task learning formulation for predicting disease progression",
      "author" : [ "J. Zhou", "L. Yuan", "J. Liu", "J. Ye" ],
      "venue" : "KDD, 2011.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Boosted multi-task learning for face verification with applications to web image and video search",
      "author" : [ "X. Wang", "C. Zhang", "Z. Zhang" ],
      "venue" : "CVPR, 2009.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Predicting multiple attributes via relative multi-task learning",
      "author" : [ "L. Chen", "Q. Zhang", "B. Li" ],
      "venue" : "CVPR, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning with whom to share in multi-task feature learning",
      "author" : [ "Z. Kang", "K. Grauman", "F. Sha" ],
      "venue" : "ICML, 2011.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multiple task learning using iteratively reweighted least square",
      "author" : [ "J. Pu", "Y.-G. Jiang", "J. Wang", "X. Xue" ],
      "venue" : "IJCAI, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convex multi-task feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "Machine Learning, vol. 73, no. 3, pp. 243–272, 2008.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Tree-guided group lasso for multi-task regression with structured sparsity",
      "author" : [ "S. Kim", "E.P. Xing" ],
      "venue" : "2010.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Probabilistic multi-task feature selection",
      "author" : [ "Y. Zhang", "D.-Y. Yeung", "Q. Xu" ],
      "venue" : "NIPS, 2010.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multi-task feature learning via efficient l 2, 1-norm minimization",
      "author" : [ "J. Liu", "S. Ji", "J. Ye" ],
      "venue" : "UAI, 2009.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "An efficient projection for l1,∞ regularization",
      "author" : [ "A. Quattoni", "X. Carreras", "M. Collins", "T. Darrell" ],
      "venue" : "ICML, 2009.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Trace norm regularization: Reformulations, algorithms, and multi-task learning",
      "author" : [ "T.K. Pong", "P. Tseng", "S. Ji", "J. Ye" ],
      "venue" : "SIAM Journal on Optimization, vol. 20, no. 6, pp. 3465–3489, 2010.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning multiple related tasks using latent independent component analysis",
      "author" : [ "J. Zhang", "Z. Ghahramani", "Y. Yang" ],
      "venue" : "NIPS, 2005.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "R.K. Ando", "T. Zhang" ],
      "venue" : "JMLR, vol. 6, pp. 1817–1853, 2005.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1817
    }, {
      "title" : "A convex formulation for learning shared structures from multiple tasks",
      "author" : [ "J. Chen", "L. Tang", "J. Liu", "J. Ye" ],
      "venue" : "ICML, 2009.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multi-task learning with gaussian matrix generalized inverse gaussian model",
      "author" : [ "M. Yang", "Y. Li" ],
      "venue" : "ICML, 2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Bayesian max-margin multi-task learning with data augmentation",
      "author" : [ "C. Li", "J. Zhu", "J. Chen" ],
      "venue" : "ICML, 2014.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multi-task model and feature joint learning",
      "author" : [ "Y. Li", "X. Tian", "T. Liu", "D. Tao" ],
      "venue" : "IJCAI, 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A dirty model for multi-task learning",
      "author" : [ "A. Jalali", "S. Sanghavi", "C. Ruan", "P.K. Ravikumar" ],
      "venue" : "NIPS, 2010.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Integrating low-rank and group-sparse structures for robust multi-task learning",
      "author" : [ "J. Chen", "J. Zhou", "J. Ye" ],
      "venue" : "KDD, 2011.  7",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning task grouping and overlap in multi-task learning",
      "author" : [ "A. Kumar", "H. Daume III" ],
      "venue" : "ICML, 2012.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Curriculum learning of multiple tasks",
      "author" : [ "A. Pentina", "V. Sharmanska", "C.H. Lampert" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Self-paced learning for latent variable models",
      "author" : [ "M.P. Kumar", "B. Packer", "D. Koller" ],
      "venue" : "NIPS, 2010.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Selfpaced learning with diversity",
      "author" : [ "L. Jiang", "D. Meng", "S.-I. Yu", "Z. Lan", "S. Shan", "A. Hauptmann" ],
      "venue" : "NIPS, 2014.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multi-view self-paced learning for clustering",
      "author" : [ "C. Xu", "D. Tao", "C. Xu" ],
      "venue" : "IJCAI, 2015.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Self-paced learning for matrix factorization",
      "author" : [ "Q. Zhao", "D. Meng", "L. Jiang", "Q. Xie", "Z. Xu", "A.G. Hauptmann" ],
      "venue" : "AAAI, 2015.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A selfpaced multiple-instance learning framework for co-saliency detection",
      "author" : [ "D. Zhang", "D. Meng", "C. Li", "L. Jiang", "Q. Zhao", "J. Han" ],
      "venue" : "ICCV, 2015.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Regularized multi-task learning",
      "author" : [ "T. Evgeniou", "M. Pontil" ],
      "venue" : "KDD, 2004.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multi-stage multi-task feature learning",
      "author" : [ "P. Gong", "J. Ye", "C. Zhang" ],
      "venue" : "JMLR, vol. 14, no. 1, pp. 2979–3010, 2013.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient multi-task feature learning with calibration",
      "author" : [ "P. Gong", "J. Zhou", "W. Fan", "J. Ye" ],
      "venue" : "KDD, 2014.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A convex formulation for learning task relationships in multi-task learning",
      "author" : [ "Y. Zhang", "D.-Y. Yeung" ],
      "venue" : "UAI, 2010.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Factorial multi-task learning: a bayesian nonparametric approach",
      "author" : [ "S. Gupta", "D. Phung", "S. Venkatesh" ],
      "venue" : "ICML, 2013.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "One basic assumption in MTL is that there exists common or related information among the tasks; learning such information can result in better generalization performance than independently learning each individual task [1].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 1,
      "context" : "Due to its empirical success, MTL has been applied to various domains, including disease modeling and prediction [2], web image and video search [3], relative attributes learning [4].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "Due to its empirical success, MTL has been applied to various domains, including disease modeling and prediction [2], web image and video search [3], relative attributes learning [4].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "Due to its empirical success, MTL has been applied to various domains, including disease modeling and prediction [2], web image and video search [3], relative attributes learning [4].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 4,
      "context" : "Many multi-task learning methods have been proposed, which in general can be categorized into two classes [5], [6].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "Many multi-task learning methods have been proposed, which in general can be categorized into two classes [5], [6].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,∞ norm regularization [11] and trace norm regularization [12].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,∞ norm regularization [11] and trace norm regularization [12].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,∞ norm regularization [11] and trace norm regularization [12].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 9,
      "context" : "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,∞ norm regularization [11] and trace norm regularization [12].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 10,
      "context" : "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,∞ norm regularization [11] and trace norm regularization [12].",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 11,
      "context" : "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,∞ norm regularization [11] and trace norm regularization [12].",
      "startOffset" : 276,
      "endOffset" : 280
    }, {
      "referenceID" : 12,
      "context" : "The other class of methods assumes that the model parameters used by the tasks are related to each other [13], [14], [15].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "The other class of methods assumes that the model parameters used by the tasks are related to each other [13], [14], [15].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "The other class of methods assumes that the model parameters used by the tasks are related to each other [13], [14], [15].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "edu [16], [17], [18].",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 16,
      "context" : "edu [16], [17], [18].",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 17,
      "context" : "edu [16], [17], [18].",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 18,
      "context" : "However, the assumption that all the tasks share some common information is fairly strong, recent works propose task grouping or task outlier detecting, which assume that there exists common information only within a subset of tasks, or there exists outlier task having no relation with the other tasks [19], [20], [21].",
      "startOffset" : 303,
      "endOffset" : 307
    }, {
      "referenceID" : 19,
      "context" : "However, the assumption that all the tasks share some common information is fairly strong, recent works propose task grouping or task outlier detecting, which assume that there exists common information only within a subset of tasks, or there exists outlier task having no relation with the other tasks [19], [20], [21].",
      "startOffset" : 309,
      "endOffset" : 313
    }, {
      "referenceID" : 20,
      "context" : "However, the assumption that all the tasks share some common information is fairly strong, recent works propose task grouping or task outlier detecting, which assume that there exists common information only within a subset of tasks, or there exists outlier task having no relation with the other tasks [19], [20], [21].",
      "startOffset" : 315,
      "endOffset" : 319
    }, {
      "referenceID" : 21,
      "context" : "Recently, SeqMT [22] uses a heuristic algorithm to rank all the tasks, and learns the tasks sequentially for multi-task classification tasks.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 22,
      "context" : "Recently, self-paced learning (SPL) [23] proposes a paradigm advocating that learning should be first done for ‘simple’ or ‘easy’ instances, and then gradually move to ‘complex’ or ‘hard’ instances, inspired by the cognitive process of humans.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : "Self-paced learning has been empirically demonstrated to be helpful for avoiding bad local minima, especially in the presence of heavy noise and outliers [23], [24].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 23,
      "context" : "Self-paced learning has been empirically demonstrated to be helpful for avoiding bad local minima, especially in the presence of heavy noise and outliers [23], [24].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 24,
      "context" : "By now, it has been successfully applied to many research areas, such as multi-view learning [25], matrix factorization [26], and multi-instance learning [27].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 25,
      "context" : "By now, it has been successfully applied to many research areas, such as multi-view learning [25], matrix factorization [26], and multi-instance learning [27].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 26,
      "context" : "By now, it has been successfully applied to many research areas, such as multi-view learning [25], matrix factorization [26], and multi-instance learning [27].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 20,
      "context" : "Thus the MTL problem can be formulated by GO-MTL [21] as:",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 22,
      "context" : "Previous works have demonstrated that adding prior information, such as the easy-to-hard strategy, can alleviate the problem of bad local minima [23].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "w (i) j ∈ [0, 1],∀j = 1, .",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 22,
      "context" : "So far, many self-paced regularizers have been proposed for various tasks [23], [27], but almost all of them focus on imposing weights on the instance level.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 26,
      "context" : "So far, many self-paced regularizers have been proposed for various tasks [23], [27], but almost all of them focus on imposing weights on the instance level.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : ", w (i) ni ] ∈ [0, 1]i , and thus w = [w, .",
      "startOffset" : 15,
      "endOffset" : 21
    }, {
      "referenceID" : 6,
      "context" : "We know that minimizing the l2,1 norm of a matrix can make the matrix sparse in rows or columns [7]; thus minimizing the second term in (3) makes the w’s corresponding to large empirical loss L (i.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "w ∈ [0, 1]i ,∀i = 1, .",
      "startOffset" : 4,
      "endOffset" : 10
    }, {
      "referenceID" : 27,
      "context" : "Note our method can be naturally applied to classification tasks by using a classification loss function L j as in [28].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "min w(i)∈[0,1]ni Li := 1 ni wL̂−λ‖w‖1+ γ √ ni ‖w‖2.",
      "startOffset" : 9,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 28,
      "context" : "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 29,
      "context" : "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 21,
      "context" : "Note that we do not compare with SeqMT [22] because it is hard to have a fair comparison since SeqMT is tailored to solve classification tasks, while our current model focuses on regression problems.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 30,
      "context" : "We evaluate all the algorithms in terms of both root mean squared error (rMSE) and normalized mean squared error (nMSE), which are commonly used in multi-task learning problem [31], [32].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 31,
      "context" : "We evaluate all the algorithms in terms of both root mean squared error (rMSE) and normalized mean squared error (nMSE), which are commonly used in multi-task learning problem [31], [32].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 28,
      "context" : "Each English letter corresponds to a label (1-26) and we treat the labels as regression values as [29].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "Since w j ∈ [0, 1], we know that in the optimal solution, either ∂Li(w (i) j ) ∂w (i) j = 0 or w j = 0, 1.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "It is clear that w (i) j √∑ s(w (i) s )2 ∈ [0, 1].",
      "startOffset" : 43,
      "endOffset" : 49
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose a novel multi-task learning (MTL) framework, called Self-Paced Multi-Task Learning (SPMTL). Different from previous works treating all tasks and instances equally when training, SPMTL attempts to jointly learn the tasks by taking into consideration the complexities of both tasks and instances. This is inspired by the cognitive process of human brain that often learns from the easy to the hard. We construct a compact SPMTL formulation by proposing a new task-oriented regularizer that can jointly prioritize the tasks and the instances. Thus it can be interpreted as a selfpaced learner for MTL. A simple yet effective algorithm is designed for optimizing the proposed objective function. An error bound for a simplified formulation is also analyzed theoretically. Experimental results on toy and real-world datasets demonstrate the effectiveness of the proposed approach, compared to the stateof-the-art methods.",
    "creator" : "LaTeX with hyperref package"
  }
}