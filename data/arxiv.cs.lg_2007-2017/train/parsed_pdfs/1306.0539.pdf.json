{
  "name" : "1306.0539.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Performance Bounds of some Policy Search Dynamic Programming Algorithms",
    "authors" : [ "Bruno Scherrer" ],
    "emails" : [ "bruno.scherrer@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "—\nincrease of time complexity. We then describe an algorithm, Non-Stationary Direct Policy Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon situation or 2) a simplified version of the Non-Stationary PI with growing period of Scherrer and Lesner (2012). We provide an analysis of this algorithm, that shows in particular that it enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a time complexity similar to that of DPI."
    }, {
      "heading" : "1 Introduction",
      "text" : "The study of approximation in Dynamic Programming algorithms for infinite-horizon discounted Markov Decision Processes (MDP) has a rich history (Bertsekas and Tsitsiklis, 1996; Szepesvári, 2010). Some of the first important results, gathered by Bertsekas and Tsitsiklis (1996), provide bounds on the closeness to optimality of the computed policy as a function of the max-norm errors during iterations. If Value or Policy Iteration are run with some error k, it is known that the value vπk of the policies πk generated by the algorithm can get close to the optimal policy π∗ if the errors are small enough since we have\nlim sup k→∞\n‖vπ∗ − vπk‖∞ ≤ 2γ\n(1− γ)2 sup k ‖ k‖∞.\nUnfortunately, such results have a limited range since in practice, most implementations of Dynamic Programming algorithms involve function approximation (like classification or regression) that controls some ν-weighted Lp norm ‖ · ‖ν,p instead of the max-norm ‖ · ‖∞. Starting with the works of Munos (2003, 2007), this motivated a recent trend of research (Antos et al., 2008; Munos and Szepesvári, 2008; Farahmand et al., 2009, 2010; Lazaric et al., 2010, 2011; Scherrer et al., 2012) that provide generalizations of the above result of the form\nlim sup k→∞\n‖vπ∗ − vπk‖µ,p ≤ 2γC1/p\n(1− γ)2 sup k ‖ k‖ν,p, (1)\nwhere µ and ν are some distributions. The possibility to express the right hand side with respect to some ν-weighted Lp norm comes at the price of a constant C, called concentrability coefficient, that measures the stochastic smoothness of the MDP: the more the MDP dynamics may concentrate in some parts of\nar X\niv :1\n30 6.\n05 39\nv1 [\ncs .A\nI] 3\nJ un\n2 01\n3\nthe state space, the bigger C (see Munos and Szepesvári (2008) for a detailed discussion). Though efforts have been employed to improve these constants (Farahmand et al., 2010; Scherrer et al., 2012), the fact that it may be infinite (for instance when the MDP is deterministic) constitutes a severe limitation.\nInterestingly, one work (Kakade and Langford, 2002; Kakade, 2003)—anterior to those of Munos (2003, 2007)—proposed an approximate Dynamic Programming algorithm, Conservative Policy Iteration (CPI), with a performance bounds similar to Equation (1) but with a constant that is—as we will argue precisely in this paper (Remarks 1, 2 and 3)—better than all the others: it only involves the mismatch between some input measure of the algorithm and a baseline distribution corresponding roughly to the frequency visitation of the optimal policy, a natural piece of information that an expert of the domain may provide. The main motivation of this paper is to emphasize the importance of these concentrability constants regarding the significance of the performance bounds.\nIn Section 3, we will describe Direct Policy Iteration (DPI), a very simple Dynamic Programming algorithm proposed by Lagoudakis and Parr (2003); Fern et al. (2006); Lazaric et al. (2010) that is similar to CPI; for this algorithm, we will provide and extend the analysis developed by Lazaric et al. (2010). We will then consider CPI in Section 4, describe the theoretical properties originally given by Kakade and Langford (2002), as well as some new bounds that will ease the comparison with DPI. In particular, as a corollary of our analysis, we will obtain an original bound for CPI(α), a practical variation of CPI that uses a fixed stepsize. We will argue that the concentrability constant involved in the analysis of CPI is better than those of DPI. This improvement of quality unfortunately comes at some price: the number of iterations required by CPI can be exponentially bigger than that of DPI. This will motivate the introduction of another algorithm: we describe in Section 5 Non-Stationary Direct Policy Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon situation or 2) a simplification of the Non-Stationary PI with growing period algorithm of Scherrer and Lesner (2012). We will analyze this algorithm and prove in particular that it enjoys the best of both worlds: a guarantee similar to that of CPI and a fast rate like that of DPI. The next section begins by providing the background and the precise setting considered."
    }, {
      "heading" : "2 Background",
      "text" : "We consider an infinite-horizon discounted Markov Decision Process Puterman (1994); Bertsekas and Tsitsiklis (1996) (S,A, P, r, γ), where S is a possibly infinite state space, A is a finite action space, P (ds′|s, a), for all (s, a), is a probability kernel on S, r : S → [−Rmax, Rmax] is a reward function bounded by Rmax, and γ ∈ (0, 1) is a discount factor. A stationary deterministic policy π : S → A maps states to actions. We write Pπ(ds\n′|s) = P (ds′|s, π(s)) for the stochastic kernel associated to policy π. The value vπ of a policy π is a function mapping states to the expected discounted sum of rewards received when following π from any state: for all s ∈ S,\nvπ(s) = E [ ∞∑ t=0 γtr(st) ∣∣∣∣∣s0 = s, st+1 ∼ Pπ(·|st) ] .\nThe value vπ is clearly bounded by Vmax = Rmax/(1− γ). It is well-known that vπ can be characterized as the unique fixed point of the linear Bellman operator associated to a policy π: Tπ : v 7→ r + γPπv. Similarly, the Bellman optimality operator T : v 7→ maxπ Tπv has as unique fixed point the optimal value v∗ = maxπ vπ. A policy π is greedy w.r.t. a value function v if Tπv = Tv, the set of such greedy policies is written Gv. Finally, a policy π∗ is optimal, with value vπ∗ = v∗, iff π∗ ∈ Gv∗, or equivalently Tπ∗v∗ = v∗.\nIn this paper, we focus on algorithms that use an approximate greedy operator, G , that takes as input a distribution ν and a function v : S → R and returns a policy π that is ( , ν)-approximately greedy with respect to v in the sense that:\nν(Tv − Tπv) = ν(max π′ Tπ′v − Tπv) ≤ .\nIn practice, this can be achieved through a L1-regression of the so-called advantage function Kakade and Langford (2002); Kakade (2003) or through a sensitive classification problem Lazaric et al. (2010), in both case generating the learning problems through rollout trajectories induced by policy π. For all\nconsidered algorithms, we will provide bounds on the expected loss Es∼µ[vπ∗(s)− vπ(s)] = µ(vπ∗ − vπ) of using some generated policy π instead of the optimal policy π∗ for some distribution µ of interest as a function of the errors k at each iteration."
    }, {
      "heading" : "3 Direct Policy Iteration (DPI)",
      "text" : "We begin by describing Direct Policy Iteration (DPI) introduced by Lagoudakis and Parr (2003); Fern et al. (2006) and analyzed by Lazaric et al. (2010). The analysis of this algorithm relies on the following\nAlgorithm 1 DPI\ninput: an initial policy π0, a distribution ν for k = 0, 1, 2, . . . do πk+1 ← G k+1(ν, vπk) end for return: vπk\ncoefficients relating µ (the distribution of interest for measuring the loss) and ν (the parameter of the algorithm):\nDefinition 1. Let c(1), c(2), . . . be the smallest coefficients in [1,∞) ∪ {∞} such that for all i and all sets of policies π1, π2, . . . , πi, µPπ1Pπ2 . . . Pπi ≤ c(i)ν, and let C(1) and C(2) be the following coefficients in [1,∞) ∪ {∞}\nC(1) = (1− γ) ∞∑ i=0 γic(i), C(2) = (1− γ)2 ∞∑ i=0 ∞∑ j=i γjc(j) = (1− γ)2 ∞∑ i=0 (i+ 1)γic(i).\nRemark 1. A bound involving C(1) is in general better than one involving C(2), in the sense that we always have (i) C(1) ≤ 11−γC\n(2) = O(C(2)) while (ii) we may have C(1) <∞ and C(2) =∞. (i) holds because, using the fact that c(i) ≥ 1, we have\nC(1) = (1− γ) ∞∑ i=0 γic(i) ≤ (1− γ) ∞∑ i=0 ∞∑ j=0 γi+jc(i) = 1 1− γ C(2).\nNow (ii) can be obtained in any situation where c(i) = Θ( 1i2γi ), since the generic term of C (1) is γic(i) = Θ( 1i2 ) (the infinite sum converges) while that of C (2) is (i + 1)γic(i) = Θ( 1i ) (the infinite sum diverges to infinity).\nWith these coefficients in hand, we can prove the following performance bounds.\nTheorem 1. At each iteration k of DPI (Algorithm 1), the expected loss satisfies:\nµ(vπ∗ − vπk) ≤ C(2)\n(1− γ)2 max 1≤i≤k\ni + γ kVmax and µ(vπ∗ − vπk) ≤\nC(1)\n1− γ k∑ i=1 i + γ kVmax.\nProof. A proof of the first bound can be found in Lazaric et al. (2010). For completeness, we provide one in Appendix A, along with the proof of the (original) second bound, all the more that they share a significant part of the arguments.\nThough the second bound involves the sum of the errors instead of the max value, as noted in Remark 1 its coefficient C(1) is better than C(2). As stated in the following corollary, the above theorem implies that the asymptotic performance bound is approched after a small number (or order O(log 1 )) of iterations.\nCorollary 1. Write = max1≤i≤k i.\nIf k ≥ log Vmax\n1− γ , then µ(vπ∗ − vπk) ≤\n( C(2)\n(1− γ)2 + 1\n) .\nIf k =\n⌈ log Vmax\n1− γ\n⌉ , then µ(vπ∗ − vπk) ≤ ( kC(1)\n1− γ + 1\n) ≤ (( log Vmax + 1 ) C(1)\n(1− γ)2 + 1\n) ."
    }, {
      "heading" : "4 Conservative Policy Iteration (CPI)",
      "text" : "We now turn to the description of Conservative Policy Iteration (CPI) proposed by Kakade and Langford (2002). At iteration k, CPI (described in Algorithm 2) uses the distribution dπk,ν = (1 − γ)ν(I − γPπk)\n−1—the discounted occupancy measure induced by πk when starting from µ—for calling the approximate greedy operator and for deciding whether to stop or not. Furthermore, it uses an adaptive stepsize α to generate a stochastic mixture of all the policies that are returned by the successive calls to the approximate greedy operator, which explains the adjective “conservative”.\nAlgorithm 2 CPI\ninput: a distribution ν, an initial policy π0, ρ > 0 for k = 0, 1, . . . do π′k+1 ← G k+1(dπk,ν , vπk) Compute a ρ3 -accurate estimate Âk+1 of Ak+1 = dπk,ν(Tπ′k+1vπk − vπk) if Âk+1 ≤ 2ρ3 then\nreturn: πk end if αk+1 ← (1−γ)(Âk+1− ρ3 )\n4γVmax\nπk+1 ← (1− αk+1)πk + αk+1π′k+1 end for\nThe analysis here relies on the following coefficient:\nDefinition 2. Let Cπ∗ be the smallest coefficient in [1,∞) ∪ {∞} such that dπ∗,µ ≤ Cπ∗ν.\nRemark 2. Our motivation for revisiting CPI is related to the fact that the constant Cπ∗ that will appear soon in its analysis is better than C(1) (and thus also, by Remark 1, better that C(2)) of algorithms like DPI1 in the sense that (i) we always have Cπ∗ ≤ C(1) and (ii) we may have Cπ∗ < ∞ and C(1) = ∞; moreover, if for any MDP and distribution µ, there always exists a parameter ν such that Cπ∗ < ∞, there might not exist a ν such that C(2) <∞. (i) holds because\ndπ∗,µ = (1− γ)µ(I − γPπ∗)−1 = (1− γ) ∞∑ i=0 γiµ(Pπ∗) i ≤ (1− γ) ∞∑ i=0 γic(i)ν = C(1)ν\nand Cπ∗ is the smallest coefficient satisfying the above inequality. Now consider (ii). The positive part of the claim (“always exists”) is trivial: it is sufficient to take ν = dπ∗,µ and we have Cπ∗ = 1. The negative part (“there might not exist”) can be shown by considering an MDP defined on N, µ equal to the dirac measure δ({0}) on state 0 and an infinite number of actions a ∈ N that result in a deterministic transition from 0 to a. As in Definition 1, let c(1) ∈ [1,∞) ∪ {∞} be such that for all π, µPπ ≤ c(1)ν. Then for all actions a, we have δ({a}) ≤ c(1)ν. As a consequence, 1 = ∑ i∈N ν(i) ≥ 1 c(1) ∑ i∈N 1 and thus necessarily c(1) =∞. As a consequence C(2) =∞.\nThe constant Cπ∗ will be small when the parameter distribution ν is chosen so that it fits as much as possible dπ∗,µ that is the discounted expected long-term occupancy of the optimal policy π∗ starting from\n1Though we do not develop this here, it can be seen that concentrability coefficients that have been introduced for other approximate Dynamic Programming algorithms like Value Iteration (see Munos and Szepesvári (2008); Farahmand et al. (2010)) or Modified Policy Iteration (see Scherrer et al. (2012)) are equal to C(2).\nµ. A good domain expert should be able to provide such an estimate, and thus the condition Cπ∗ <∞ is rather mild. We have the following performance bound2.\nTheorem 2. CPI (Algorithm 2) has the following properties: (i) The expected values νvπk = E[vπk(s)|s ∼ ν] of policies πk starting from distribution ν are monotonically increasing: νvk+1 > νvk + ρ2\n72γVmax .\n(ii) The (random) iteration k∗ at which the algorithm stops is such that k∗ ≤ 72γVmax 2\nρ2 .\n(iii) The policy πk∗ that is returned satisfies\nµ(vπ∗ − vπk∗ ) ≤ Cπ∗\n(1− γ)2 ( k∗+1 + ρ).\nProof. The proof follows the lines of that Kakade and Langford (2002) and is provided in Appendix B for completeness.\nWe have the following immediate corollary that shows that CPI obtains a performance bounds similar to those of DPI (in Corollary 1)—at the exception that it involves a different (better) concentrability constant—after O( 1 2 ) iterations.\nCorollary 2. If CPI is run with parameter ρ = = max1≤i≤k i, then CPI stops after at most 72γVmax\n2\n2\niterations and returns a policy πk∗ that satisfies:\nµ(vπ∗ − vπk∗ ) ≤ 2Cπ∗\n(1− γ)2 .\nWe also provide a complementary original analysis of this algorithm that further highlights its connection with DPI.\nTheorem 3. At each iteration k < k∗ of CPI (Algorithm 2), the expected loss satisfies:\nµ(vπ∗ − vπk) ≤ C(1) (1− γ)2 k∑ i=1 αi i + e {(1−γ)∑ki=1 αi}Vmax.\nProof. The proof is a natural but tedious extension of the analysis of DPI to the situation where conservative steps are made, and is deferred to Appendix C.\nSince in the proof of Theorem 2 (in Appendix B), one shows that the learning steps of CPI satisfy αk ≥ (1−γ)ρ12γVmax , the right term e {(1−γ)∑ki=1 αi} above tends 0 exponentially fast, and we get the following corollary that shows that CPI has a performance bound with the coefficient C(1) of DPI in a number of iterations O( log 1 ).\nCorollary 3. Assume CPI is run with parameter ρ = = max1≤i≤k i. The smallest (random) iteration k† such that log Vmax 1−γ ≤ ∑k† i=1 αi ≤ log Vmax 1−γ + 1 is such that k † ≤ 12γVmax log Vmax (1−γ)2 and the policy πk† satisfies:\nµ(vπ∗ − vπk† ) ≤\nC(1) (∑k† i=1 αi ) (1− γ)2 + 1  ≤ (C(1) (log Vmax + 1) (1− γ)3 + 1 ) .\nIn practice, the choice for the learning step αk in Algorithm 2 is very conservative, which makes CPI (as it is) a very slow algorithm. Natural solutions to this problem, that have for instance been considered in a variation of CPI for search-based structure prediction problems (III et al., 2009; Daumé III et al., 2006), is to either use a line-search (to optimize the learning step αk ∈ (0, 1)) or even to use a fixed value\n2Note that there are two small differences between the algorithm and analysis described by Kakade and Langford (2002) and the ones we give here: 1) the stepsize α is a factor 1\nγ bigger in our description, and thus the number of iterations is\nslightly better (smaller by a factor γ); 2) our result is stated in terms of the error k that may not be known in advance and the input parameter ρ while Kakade and Langford (2002) assume a uniform bound on the errors ( k) is known and equal to the parameter ρ.\nα (e.g. α = 0.1) for all iterations. This latter solution, that one may call CPI(α), works indeed well in practice, and is significantly simpler to implement since one is relieved of the necessity to estimate Âk+1 through rollouts (see Kakade and Langford (2002) for the description of this process); indeed it becomes almost as simple as DPI except that one uses the distribution dπk,ν and conservative steps. Since the proof is based on a generalization of the analysis of DPI and thus does not use any of the specific properties of CPI, it turns out that the results we have just given (Corollary 3) can straightforwardly be specialized to the case of this algorithm.\nCorollary 4. Assume we run CPI(α) for some α ∈ (0, 1), that is CPI (Algorithm 2) with αk = α for all k. Write = max1≤i≤k i.\nIf k = ⌈ log Vmax α(1− γ) ⌉ , then µ(vπ∗ − vπk) ≤ α(k + 1)C(1) (1− γ)2 ≤ ( C(1) ( log Vmax + 1 ) (1− γ)3 + 1 ) .\nWe see here that the parameter α directly controls the rate of CPI(α)3. Furthermore, if one sets α to a sufficiently small value, one should recover the nice properties of Theorem 2-Corollary 2: monotonicity of the expected value and a performance bound with respect to the best constant Cπ∗ , though we do not formalize this here.\nIn summary, Corollary 2 and Remark 2 tell us that CPI has a performance guarantee that can be arbitrarily better than that of DPI, though the opposite is not true. This, however, comes at the cost of a significant exponential increase of time complexity since Corollary 2 states that there might be a number of iterations that scales in O( 1 2 ), while the guarantee of DPI (Corollary 1) only requires O ( log 1\n) iterations. When the analysis of CPI is relaxed so that the performance guarantee is expressed in terms of the (worse) coefficient C(1) of DPI (Corollary 3), we were able to slightly improve the rate—by a factor Õ( )—, though it is still exponentially slower than that of DPI. The algorithm that we present in the next section will be the best of both worlds: it will enjoy a performance guarantee involving the best constant Cπ∗ , but with a time complexity similar to that of DPI."
    }, {
      "heading" : "5 Non-stationary Direct Policy Iteration (NSDPI)",
      "text" : "We are now going to describe an algorithm that has a flavour similar to DPI – in the sense that at each step it does a full step towards a new policy – but also has a conservative flavour like CPI – in the sense that the policies will evolve more and more slowly. This algorithm is based on finite-horizon non-stationary policies. We will write σ = π1π2 . . . πk the k-horizon policy that makes the first action according to π1, then the second action according to π2, etc. Its value is vσ = Tπ1Tπ2 . . . Tπkr. We will write σ = ∅ the “empty” non-stationary policy. Note that v∅ = r and that any infinite-horizon policy that begins with σ = π1π2 . . . πk, which we will denote “σ . . . ” has a value vσ... ≥ vσ − γkVmax.\nThe last algorithm we consider, named here Non-Stationary Direct Policy Iteration (NSDPI) because it behaves as DPI but builds a non-stationary policy by iteratively concatenating the policies that are returned by the approximate greedy operator, is described in Algorithm 3.\nAlgorithm 3 NSDPI\ninput: a distribution ν initialization: σ0 = ∅ for k = 0, 1, 2, . . . do πk+1 ← G k+1(ν, vσk) σk+1 ← πk+1σk end for\nWe are going to state a performance bound for this algorithm with respect to the constant Cπ∗ , but also an alternative bound based on the following new concentrability coefficients.\n3The performance bound of CPI(1) (with α = 1) does not match the bound of DPI (Corollary 1), but is a factor 1 1−γ worse. This amplification is due to the fact that the approximate greedy operator uses the distribution dπk,ν ≥ (1 − γ)ν instead of ν (for DPI).\nDefinition 3. Let cπ∗(1), cπ∗(2), . . . be the smallest coefficients in [1,∞) ∪ {∞} such that for all i, µ(Pπ∗) i ≤ cπ∗(i)ν. and let C (1) π∗ be the following coefficient in [1,∞) ∪ {∞}:\nC(1)π∗ = (1− γ) ∞∑ i=0 γicπ(i).\nRemark 3. A bound involving Cπ∗ is in general better than one involving C (1) π∗ in the sense that (i) we always have Cπ∗ ≤ C (1) π∗ while (ii) we may have Cπ∗ < ∞ and C (1) π∗ = ∞. Similarly, a bound involving C (1) π∗ is in general better than one involving C (1) since (iii) we have C (1) π∗ ≤ C(1) while (iv) we may have C (1) π∗ <∞ and C(1) =∞. (i) holds because (very similarly to Remark 2-(i))\ndπ∗,µ = (1− γ)µ(I − γPπ∗)−1 = (1− γ) ∞∑ i=0 γiµ(Pπ∗) i ≤ (1− γ) ∞∑ i=0 γicπ∗(i)ν = C (1) π∗ ν\nand Cπ∗ is the smallest coefficient satisfying the above inequality. (ii) can easily be obtained by designing a problem with cπ∗(1) =∞ as in Remark 2-(ii). (iii) is a consequence of the fact that for all i, cπ∗(i) ≤ c(i). Finally, (iv) is trivially obtained by considering two different policies.\nWith this notations in hand, we are ready to state that NSDPI (Algorithm 3) enjoys two guarantees that have a fast rate like those of DPI (Theorem 1), one expressed in terms of the concentrability Cπ∗ that was introduced for CPI (in Definition 2), and the other in terms of the constant C (1) π∗ we have just introduced.\nTheorem 4. At each iteration k of NSDPI (Algorithm 3), the expected loss of running an infinitely-long policy that begins by σk satisfies\nµ(vπ∗ − vσk...) ≤ C\n(1) π∗\n1− γ max 1≤i≤k\ni + 2γ kVmax and µ(vπ∗ − vσk...) ≤ Cπ∗ 1− γ k∑ i=1 i + 2γ kVmax.\nProof. The proof of Theorem 4 is rather simple (much simpler than the previous ones), and is deferred to Appendix D.\nAs shown in the following immediate corollary of Theorem 4, these relations constitute guarantees that small errors i in the greedy operator quickly induce good policies.\nCorollary 5. Write = max1≤i≤k i.\nIf k ≥ log 2Vmax\n1− γ then µ(vπ∗ − vσk) ≤\n( C (1) π∗\n1− γ + 1\n) .\nIf k =\n⌈ log 2Vmax\n1− γ\n⌉ then µ(vπ∗ − vπk) ≤ ( kCπ∗ 1− γ + 1 ) ≤ (( log 2Vmax + 1 ) Cπ∗ (1− γ)2 + 1 ) .\nThe first bound has a better dependency with respect to 11−γ , but (as explained in Remark 3) is expressed in terms of the worse coefficient C (1) π∗ . The second guarantee is almost as good as that of CPI (Corollary 2) since it only contains an extra log 1 term, but it has the nice property that it holds quickly: in time log 1 instead of O( 1 2 ), that is exponentially faster.\nWe devised NSDPI as a DPI-like simplified variation of one of the non-stationary dynamic programming algorithm recently introduced by Scherrer and Lesner (2012), the Non-Stationary PI algorithm with growing period. The main difference with NSDPI is that one considers there the value v(σk)∞ of the policy that loops infinitely on σk instead of the value vσk of the only first k steps here. Following the intuition that when k is big, these two values will be close to each other, we ended up focusing on NSDPI because it is simpler. Remarkably, NSDPI turns out to be almost identical to an older algorithm, the Policy Search by Dynamic Programming (PSDP) algorithm Bagnell et al. (2003). It should\nhowever be noted that PSDP was introduced for a slightly different control problem where the horizon is finite, while we are considering here the infinite-horizon problem. Given a problem with horizon T , PSDP comes with a guarantee that is essentially identical to the first bound in Corollary 5, but requiring as many input distributions as there are time steps4. Our main contribution with respect to PSDP is that by considering the infinite-horizon case, we managed to require only one input parameter (the distribution ν that should match as much as possible the measure dπ∗,µ, recall Definition 1)—a much milder assumption—and provide the second performance bound with respect to Cπ∗ , that is better (cf. Remark 3)."
    }, {
      "heading" : "6 Discussion, Conclusion and Future Work",
      "text" : "In this article, we have described two algorithms of the literature, DPI and CPI, and introduced the NSDPI algorithm that borrows ideas from both DPI and CPI, while also having some very close algorithmic connections with PSDP Bagnell et al. (2003) and the Non-Stationary PI algorithm with growing period of Scherrer and Lesner (2012). Figure 1 synthesizes the theoretical guarantees we have discussed about these algorithms. For each such guarantee, we provide the dependency of the performance bound and the number of iterations with respect to , 11−γ and the concentrability coefficients (for CPI, we assume as Kakade and Langford (2002) that ρ = ). We highlight in red the bounds that are to our knowledge new5.\nOne of the most important message of our work is that what is usually hidden in the constants of the performance bounds does matter. The constants we discussed can be sorted from the worst to the best (cf. Remarks 1, 2 and 3) as follows: C(2), C(1), C (1) π∗ , Cπ∗ . To our knowledge, this is the first time that such an in-depth comparison of the bounds is done, and our hierarchy of constants has interesting implications that go beyond to the policy search algorithms we have been focusing on in this paper. As a matter of fact, several dynamic programming algorithms—AVI (Munos, 2007), API (Munos, 2003), λPI Scherrer (2013), AMPI (Scherrer et al., 2012)—come with guarantees involving the worst constant C(2), that can easily be made infinite. On the positive side, we have argued that the guarantee for CPI can be arbitrarily stronger than the other state-of-the-art algorithms. We have identified the NSDPI algorithm as having a similar nice property. Furthermore, we can observe that DPI and NSDPI both have the best time complexity guarantee. Thus, NSDPI turns out to have the overall best guarantees.\nAt the technical level, several of our bounds come in pair, and this is due to the fact that we have introduced a new proof technique in order to derive new bounds with various constants. This led to a new bound for DPI, that is better since it involves the C(1) constant instead of C(2). It also enabled us to derive new bounds for CPI (and its natural algorithmic variant CPI(α)) that is worse in terms of\n4 It is assumed by Bagnell et al. (2003) that a set of baseline distributions ν1, ν2, . . . , νT provide estimates of where the optimal policy is leading the system from some distribution µ at all time steps 1, 2, . . . , T ; then, the authors measure the mismatch through coefficients cπ∗ (i) such that µP i π∗ ≤ cπ∗ (i)νi. This kind of assumption is essentially identical to the concentrability assumption underlying the constant C (1) π∗ in Definition 3, the only difference being that we only refer to one measure ν. 5We do not highlight the second bound of NSDPI, acknowledging that it already appears in a very close form in Bagnell et al. (2003) for PSDP.\nguarantee but has a better time complexity (Õ( 1 ) instead of O( 1 )). We believe this new technique may be helpful in the future for the analysis of other algorithms. The main limitation of our analysis lies in the assumption, considered all along the paper, that all algorithms have at disposal an -approximate greedy operator. This is in general not realistic, since it may be hard to control directly the quality level . Furthermore, it may be unreasonable to compare all algorithms on this basis, since the underlying optimization problems may have slightly different complexities: for instance, methods like CPI look in a space of stochastic policies while DPI moves in a space of deterministic policies. Digging and understanding in more depth what is potentially hidden in the term —as we have done here for the concentrability constants—constitutes a very natural research direction.\nLast but not least, on the practical side, we have run preliminary numerical experiments that somewhat support our theoretical argument that algorithms with a better concentrability constant should be preferred. On simulations on relatively small problems, CPI+ (CPI with a crude line-search mechanism), CPI(α) and NSDPI were shown to always perform significantly better than DPI, NSDPI always displayed the least variability, and CPI(α) performed the best on average. We refer the reader to Appendix E for further details. Running and analyzing similar experiments on bigger domains constitutes interesting future work."
    }, {
      "heading" : "A Proof of Theorem 1",
      "text" : "Our proof is here even slightly more general than what is required: we provide the result for any reference policy π (and not only for the optimal policy π∗). Writing ek+1 = maxπ′ Tπ′vπk − Tπk+1vπk , we can first see that:\nvπ − vπk+1 = Tπvπ − Tπvπk + Tπvπk − Tπk+1vπk + Tπk+1vπk − Tπk+1vπk+1 ≤ γPπ(vπ − vπk) + ek+1 + γPπk+1(vπk − vπk+1). (2)\nUsing the fact that vπk+1 = (I − γPπk+1)−1r, one can notice that:\nvπk − vπk+1 = (I − γPπk+1)−1(vπk − γPπk+1vπk − r) = (I − γPπk+1)−1(Tπkvπk − Tπk+1vπk) ≤ (I − γPπk+1)−1ek+1.\nPutting this back in Equation (A) we get:\nvπ − vπk+1 = γPπ(vπ − vπk) + (I − γPπk+1)−1ek+1.\nBy induction on k we obtain:\nvπ − vπk = k−1∑ i=0 (γPπ) i(I − γPπk−i)−1ek−i + (γPπ)k(vπ − vπ0).\nMultiplying both sides by µ (and observing that ek ≥ 0) and using Definition 1 and the fact that νej ≤ j , we obtain:\nµ(vπ − vπk) ≤ k−1∑ i=0 µ(γPπ) i(I − γPπk−i)−1ek−i + γkVmax\n≤ k−1∑ i=0  ∞∑ j=0 γi+jc(i+ j) k−i + γkVmax =\nk−1∑ i=0 ∞∑ j=i γjc(j) k−i + γ kVmax (3)\n≤ k−1∑ i=0 ∞∑ j=i γjc(j) max 1≤l≤k l + γ kVmax,\nwhich provides the first bound. Starting back on Equation (A), we get\nµ(vπ − vπk) ≤ k−1∑ i=0 ∞∑ j=i γjc(j) k−i + γ kVmax\n≤ k−1∑ i=0 ∞∑ j=0 γjc(j) k−i + γ kVmax\n= ∞∑ j=0 γjc(j) k∑ i=1 i + γ kVmax,\nwhich proves the second bound."
    }, {
      "heading" : "B Proof of Theorem 2",
      "text" : "We include a concise and self-contained proof that essentially follows the steps in (Kakade and Langford, 2002).\n(i) We first show that the value ηk = νvπk is increasing along the iterations. Consider some iteration k of the algorithm. Using the facts that Tπk+1vπk = (1 − αk+1)vπk + αk+1Tπ′k+1vπk and Pπk+1 = (1 − αk+1)Pπk + αk+1Pπ′k+1 , we can see that:\nηk+1 − ηk = ν(vπk+1 − vπk) = ν[(I − γPπk+1)−1r − vπk ] = ν(I − γPπk)−1(I − γPπk)(I − γPπk+1)−1[r − (I − γPπk+1)vπk ] = ν(I − γPπk)−1(I − γPπk+1 + γPπk+1 − γPπk)(I − γPπk+1)−1(Tπk+1vπk − vπk)\n= 1\n1− γ dπk,ν\n[ I + γαk+1(Pπ′k+1 − Pπk)(I − γPπk+1) −1 ] αk+1(Tπ′k+1vπk − vπk)\n= αk+1 1\n1− γ dπk,ν(Tπ′k+1vπk − vπk) + α 2 k+1\nγ\n1− γ dπk,ν(Pπ′k+1 − Pπk)(I − γPπk+1) −1(Tπ′k+1vπk − vπk)\n≥ αk+1 1\n1− γ (Âk+1 −\nρ 3 )− 2α2k+1\nγ\n(1− γ)2 Vmax\nwhere we eventually used the fact that Tπ′k+1vπk − vπk ∈ (−Vmax, Vmax). Now, it can be seen that the setting αk+1 = (1−γ)(Âk+1− ρ3 )\n4γVmax of Algorithm 2 is the one that maximizes the above right hand side. With\nthis setting we get:\nηk+1 − ηk ≥ (Âk+1 − ρ3 ) 2\n8γVmax\n> ρ2\n72γVmax\nsince as long as the algorithm iterates, Âk+1 > 2ρ3 . (ii) The second point is a very simple consequence of (i): since ηk = νvπk ≤ Vmax, the number of iterations of the algorithm cannot exceed 72γVmax 2\nρ2 .\n(iii) We now prove the performance bound. Write e = maxπ′ Tπ′vπk∗ − vπk∗ . We have:\nvπ∗ − vπk∗ = Tπ∗vπ∗ − Tπ∗vπk∗ + Tπ∗vπk∗ − vπk∗ ≤ γPπ∗(vπ∗ − vπk∗ ) + e ≤ (I − γPπ∗)−1e.\nMultiplying by µ, using Definition 2 and the facts that e = maxπ′ Tπ′vπk∗ − Tπk∗ vπk∗ ≥ 0 and dπk∗ ,ν ≥ (1− γ)ν, we obtain\nµ(vπ∗ − vπk∗ ) ≤ 1\n1− γ dπ∗,µe\n≤ Cπ∗ 1− γ νe\n≤ Cπ∗ (1− γ)2 dπk∗ ,νe\n= Cπ∗\n(1− γ)2 dπk∗ ,ν(max π′ Tπ′vπk∗ − Tπ′k∗+1vπk∗ + Tπ′k∗+1vπk∗ − vπk∗ )\n≤ Cπ∗ (1− γ)2 ( k∗ +Ak+1).\nThe result follows by observing that the advantage satisfies Ak+1 ≤ Âk+1 + ρ3 ≤ ρ since Âk+1 ≤ 2ρ 3 when the algorithms stops."
    }, {
      "heading" : "C Proof of Theorem 3",
      "text" : "Using the facts that Tπk+1vπk = (1− αk+1)vπk + αk+1Tπk+1vπk and the notation ek+1 = maxπ′ Tπ′vπk − Tπ′k+1vπk , we have:\nvπ − vπk+1 = vπ − Tπk+1vπk + Tπk+1vπk − Tπk+1vπk+1 = vπ − (1− αk+1)vπk − αk+1Tπ′k+1vπk + γPπk+1(vπk − vπk+1)\n= (1− αk+1)(vπ − vπk) + αk+1(Tπvπ − Tπvπk) + αk+1(Tπvπk − Tπ′k+1vπk) + γPπk+1(vπk − vπk+1)\n≤ [(1− αk+1)I + αk+1γPπ] (vπ − vπk) + αk+1ek+1 + γPπk+1(vπk − vπk+1). (4)\nUsing the fact that vπk+1 = (I − γPπk+1)−1r, we can see that\nvπk − vπk+1 = (I − γPπk+1)−1(vπk − γPπk+1vπk − r) = (I − γPπk+1)−1(Tπkvπk − Tπk+1vπk) ≤ (I − γPπk+1)−1αk+1ek+1.\nPutting this back in Equation (C), we obtain:\nvπ − vπk+1 ≤ [(1− αk+1)I + αk+1γPπ] (vπ − vπk) + αk+1(I − γPπk+1)−1ek+1.\nDefine the matrix Qk = [(1− αk)I + αkγPπ], the set Ni,k = {j; k − i + 1 ≤ j ≤ k} (this set contains exactly i elements), the matrix Ri,k = ∏ j∈Ni,k Qj , and the coefficients βk = 1 − αk(1 − γ) and δk =∏k\ni=1 βk. We get by induction\nvπ − vπk ≤ k−1∑ i=0 Ri,kαk−i(I − γPπk−i)−1ek−i + δkVmax. (5)\nLet Pj(Ni,k) the set of subsets of Ni,k of size j. With this notation we have\nRi,k = i∑ j=0 ∑ I∈Pj(Ni,k) ζI,i,k(γPπ) j\nwhere for all subset I of Ni,k, we wrote\nζI,i,k = (∏ n∈I αn ) ∏ n∈Ni,k\\I (1− αn)  .\nTherefore, by multiplying Equation (C) by µ, using Definition 1, and the facts that ν ≤ (1− γ)dnu,πk+1 , we obtain:\nµ(vπ − vπk) ≤ 1\n1− γ k−1∑ i=0 i∑ j=0 ∞∑ l=0 ∑ I∈Pj(Ni,k) ζI,i,kγ j+lc(j + l)αk−i k−i + δkVmax.\n= 1\n1− γ k−1∑ i=0 i∑ j=0 ∞∑ l=j ∑ I∈Pj(Ni,k) ζI,i,kγ lc(l)αk−i k−i + δkVmax\n≤ 1 1− γ k−1∑ i=0 i∑ j=0 ∞∑ l=0 ∑ I∈Pj(Ni,k) ζI,i,kγ lc(l)αk−i k−i + δkVmax\n= 1\n1− γ ( ∞∑ l=0 γlc(l) ) k−1∑ i=0  i∑ j=0 ∑ I∈Pj(Ni,k) ζI,i,k αk−i k−i + δkVmax = 1\n1− γ ( ∞∑ l=0 γlc(l) ) k−1∑ i=0  ∏ j∈Ni,k (1− αj + αj) αk−i k−i + δkVmax = 1\n1− γ ( ∞∑ l=0 γlc(l) )( k−1∑ i=0 αk−i k−i ) + δkVmax.\nNow, using the fact that for x ∈ (0, 1), log(1− x) ≤ −x, we can observe that\nlog δk = log k∏ i=1 βi\n= k∑ i=1 log βi\n= k∑ i=1 log(1− αi(1− γ))\n≤ −(1− γ) k∑ i=1 αi\nAs a consequence, we get δk ≤ e−(1−γ) ∑k i=1 αi ."
    }, {
      "heading" : "D Proof of Theorem 4",
      "text" : "We begin by the first relation. For all k, we have\nvπ − vσk = Tπvπ − Tπvσk−1 + Tπvσk−1 − Tπkvσk−1 ≤ γPπ(vπ − vσk−1) + ek\nwhere we defined ek = maxπ′ Tπ′vσk−1 − Tπkvσk−1 . By induction, we deduce that\nvπ − vσk ≤ k−1∑ i=0 (γPπ) iek−i + γ kVmax.\nBy multiplying both sides of by µ, using Definition 3 and the fact that νjej ≤ , we get:\nµ(vπ − vσk) ≤ k−1∑ i=0 µ(γPπ) iek−i + γ kVmax (6)\n≤ k−1∑ i=0 γic(i) k−i + γ kVmax\n≤ ( k−1∑ i=0 γic(i) ) max 1≤i≤k i + γ kVmax.\nWe now prove the second relation. Starting back in Equation (D) and using Definition 2 (in particular\nthe fact that for all i, µ(γPπ) i ≤ 11−γ dπ∗,µ ≤ Cπ∗ 1−γ ν) and the fact that νej ≤ j , we get:\nµ(vπ − vσk) ≤ k−1∑ i=0 µ(γPπ) iek−i + γ kVmax ≤ Cπ∗ 1− γ k∑ i=1 i + γ kVmax\nand the result is obtained by using the fact that vσk... ≥ vσk − γkVmax."
    }, {
      "heading" : "E Experiments",
      "text" : "In this section, we present some experiments in order to illustrate the different algorithms discussed in the paper and to get some insight on their empirical behaviour. CPI as it is described in Algorithm 2 can be very slow (in one sample experiment on a 100 state problem, it made very slow progress and took several millions of iterations before it stopped) and we did not evaluate it further. Instead, we considered two variations: CPI+ that is identical to CPI except that it chooses the step α at each iteration by doing a line-search towards the policy output by the classifier6, and CPI(α) with α = 0.1, that makes “relatively but not too small” steps at each iteration. We begin by describing the domain and the approximation considered.\nDomain and Approximations In order to assess their quality, we consider finite problems where the exact value function can be computed. More precisely, we consider Garnet problems (Archibald et al., 1995), which are a class of randomly constructed finite MDPs. They do not correspond to any specific application, but are totally abstract while remaining representative of the kind of MDP that might be encountered in practice. In our experiments, a Garnet is parameterized by 4 parameters and is written G(nS , nA, b, p): nS is the number of states, nA is the number of actions, b is a branching factor specifying how many possible next states are possible for each state-action pair (b states are chosen uniformly at random and transition probabilities are set by sampling uniform random b−1 cut points between 0 and 1) and p is the number of features (for linear function approximation). The reward is state-dependent: for a given randomly generated Garnet problem, the reward for each state is uniformly sampled between 0 and 1. Features are chosen randomly: Φ is a nS×p feature matrix of which each component is randomly and uniformly sampled between 0 and 1. The discount factor γ is set to 0.99 in all experiments.\nAll the algorithms we have discussed in the paper need to repeatedly compute G (ν, v). In other words, they must be able to make calls to an approximate greedy operator applied to the value v of some policy for some distribution ν. To implement this operator, we compute a noisy estimate of the value v with a uniform white noise u(ι) of amplitude ι, then projects this estimate onto a Fourier basis of the value function space with F < n coefficients with respect to the ν-quadratic norm (projection that we write ΠF,ν), and then applies the (exact) greedy operator on this projected estimate. In a nutshell, one call to the approximate greedy operator G (ν, v) amounts to compute GΠF,ν(v + u(ι)).\n6We implemented a crude line-search mechanism, that looks on the set 2iα where α is the minimal step estimated by CPI to ensure improvement.\nSimulations We have run series of experiments, in which we callibrated the perturbations (noise, approximations) so that the algorithm are significantly perturbed but no too much (we do not want their behavior to become too erratic). After trial and error, we ended up considering the following setting. We used garnet problems G(ns, na, b, p) with the number of states ns ∈ {100, 200}, the number of actions na ∈ {2, 5}, the branching factor b ∈ {1, ns50 } (b = 1 corresponds to deterministic problems), the number of features to approximate the value p = ns10 , and the noise level ι = 0.05 (5%).\nFor each of these 23 = 8 parameter instances, we generated 30 (random) MDPs. For each such MDP, we ran DPI, CPI+, CPI(0.1) and NSDPI 30 times and estimated (for all iterations) the average and the standard deviation (on these 30 runs for a fixed MDP) of the performance (the error µ(vπ∗ − vπk)). Figures 2 to 5 display statistics of the average performance of the algorithms and of their standard deviation: statistics are here used because the average and the standard performance of an algorithm on an MDP are random variables since the MDP is itself random. For ease of comparison, all curves are displayed with the same x and y range. Figure 2 shows the statistics overall for the 23 = 8 parameter instances. Figure 3, 4 and 5 display statistics that are respectively conditional on the values of ns, na and b, which also gives some insight on the influence of these parameters.\nFrom these experiments and statistics, we made the following general observations: 1) In all experiments, CPI+ converged in very few iterations (most of the time less than 10, and always less than 20). 2) DPI is much more variable than the other algorithms and tends to provide the worst average performance. 3) If CPI+ and NSDPI have a similar average performance, the standard deviation of NSDPI is consistantly smaller than that of CPI and is thus more robust. 4) CPI(0.1) tend to provide the best average results, though its standard deviation is bigger than that of NSDPI. 5) All these relative behaviors tend to be amplified in the more difficult instances, that is when the state and actions spaces are big (ns = 200, na = 5) and the dynamics deterministic (b = 1).\nns = 100\nna = 2\nb = 1 (deterministic)"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on Policy Search algorithms, that compute an approximately optimal policy by following the standard Policy Iteration (PI) scheme via an -approximate greedy operator (Kakade and Langford, 2002; Lazaric et al., 2010). We describe existing and a few new performance bounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern et al., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI) (Kakade and Langford, 2002). By paying a particular attention to the concentrability constants involved in such guarantees, we notably argue that the guarantee of CPI is much better than that of DPI, but this comes at the cost of a relative—exponential in 1 — increase of time complexity. We then describe an algorithm, Non-Stationary Direct Policy Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon situation or 2) a simplified version of the Non-Stationary PI with growing period of Scherrer and Lesner (2012). We provide an analysis of this algorithm, that shows in particular that it enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a time complexity similar to that of DPI.",
    "creator" : "LaTeX with hyperref package"
  }
}