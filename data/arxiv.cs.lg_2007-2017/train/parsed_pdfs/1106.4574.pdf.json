{
  "name" : "1106.4574.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Better Mini-Batch Algorithms via Accelerated Gradient Methods",
    "authors" : [ "Andrew Cotter", "Ohad Shamir", "Karthik Sridharan" ],
    "emails" : [ "cotter@ttic.edu", "ohadsh@microsoft.com", "nati@ttic.edu", "karthik@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction\nWe consider a stochastic convex optimization problem of the form\nmin w∈W L(w),\nwhere L(w) = Ez [`(w, z)],\nand optimization is based on an empirical sample of instances z1, . . . , zm. We focus on objectives `(w, z) that are non-negative, convex and smooth in their first argument (i.e. have a Lipschitz-continuous gradient). The classical learning application is when z = (x, y) and `(w, (x, y)) is a prediction loss. In recent years, there has been much interest in developing efficient first-order stochastic optimization methods for these problems, such as stochastic mirror descent [2, 6] and stochastic dual averaging [9, 16]. These methods are characterized by incremental updates based on subgradients ∂`(w, zi) of individual instances, and enjoy the advantages of being highly scalable and simple to implement.\nAn important limitation of these methods is that they are inherently sequential, and so problematic to parallelize. A popular way to speed-up these algorithms, especially in a parallel setting, is via mini-batching, where the incremental update is performed on an average of the subgradients with respect to several instances at a time, rather than a single instance (i.e., 1b ∑b j=1 ∂`(w, zi+j)). The gradient computations for each minibatch can be parallelized, allowing these methods to perform faster in a distributed framework (see for\nar X\niv :1\n10 6.\n45 74\nv1 [\ncs .L\nG ]\nAlgorithm 1 Stochastic Gradient Descent with Mini-Batching (SGD)\nParameters: Step size η, mini-batch size b. Input: Sample z1, . . . , zm w1 = 0 for i = 1 to n = m/b do\nLet `i(wi) = 1 b ∑bi t=b(i−1)+1 `(wi, zt) w′i+1 := wi − η∇`i(wi)) wi+1 := PW(w ′ i+1)\nend for Return w̄ = 1n ∑n i=1 wi\ninstance [11]). Recently, [10] has shown that a mini-batching distributed framework is capable of attaining asymptotically optimal speed-up in general (see also [1]).\nA parallel development has been the popularization of accelerated gradient descent methods [7, 8, 15, 5]. In a deterministic optimization setting and for general smooth convex functions, these methods enjoy a rate of O(1/n2) (where n is the number of iterations) as opposed to O(1/n) using standard methods. However, in a stochastic setting (which is the relevant one for learning problems), the rate of both approaches have an O(1/ √ n) dominant term in general, so the benefit of using accelerated methods for learning problems is not obvious.\nAlgorithm 2 Accelerated Gradient Method (AG)\nParameters: Step sizes (γi, βi), mini-batch size b Input: Sample z1, . . . , zm w = 0 for i = 1 to n = m/b do\nLet `i(wi) := 1 b ∑bi t=b(i−1)+1 `(w, zt) wmdi := β −1 i wi + (1− β −1 i )w ag i w′i+1 := w md i − γi∇`i(wmdi ) wi+1 := PW(w ′ i+1) wagi+1 ← β −1 i wi+1 + (1− β −1 i )w ag i\nend for Return wagn\nIn this paper, we study the application of accelerated methods for mini-batch algorithms, and provide theoretical results, a novel algorithm, and empirical experiments. The main resulting message is that by using an appropriate accelerated method, we obtain significantly better stochastic optimization algorithms in terms of convergence speed. Moreover, in certain regimes acceleration is actually necessary in order to allow a significant speedups. The potential benefit of acceleration to mini-batching has been briefly noted in [4], but here we study this issue in much more depth. In particular, we make the following contributions:\n• We develop novel convergence bounds for the standard gradient method, which refines the result of [10, 4] by being dependent on L(w?) = infw∈W L(w), the expected loss of the best predictor in our class. For example, we show that in the regime where the desired suboptimality is comparable or larger than L(w?), including in the separable case L(w?) = 0, mini-batching does not lead to significant speed-ups with standard gradient methods.\n• We develop a novel variant of the stochastic accelerated gradient method [5], which is optimized for a mini-batch framework and implicitly adaptive to L(w?).\n• We provide an analysis of our accelerated algorithm, refining the analysis of [5] by being dependent on L(w?), and show how it always allows for significant speed-ups via mini-batching, in contrast to standard gradient methods. Moreover, its performance is uniformly superior, at least in terms of theoretical upper bounds.\n• We provide an empirical study, validating our theoretical observations and the efficacy of our new method.\n2 Preliminaries\nWe consider stochastic convex optimization problems over some convex domainW. Here, we takeW to be a convex subset of a Euclidean space, and use ‖w‖ to denote the standard Euclidean norm. In the Appendix, we state and prove the result in a more general setting, where W is a convex subset of a Banach space, and ‖w‖ can be an arbitrary norm.(subset of Euclidean case, see Appendix for the more general Banach space case), using an i.i.d. sample z1, . . . , zm ∈ Z drawn from some fixed distribution.\nThroughout this paper we assume that the instantaneous loss ` :W×Z 7→ R is convex in its first argument and non-negative. We further assume that the loss is H-smooth in its first argument for each z ∈ Z. That is for every z ∈ Z and w,w′ ∈ W,\n‖∇`(w, z)−∇`(w′, z)‖ ≤ H ‖w −w′‖\n(for more general Banach space case, the norm on the left hand side is the dual norm). Let us denote\nL(w) := Ez [`(w, z)]\nWe wish to minimize L(w) over convex domain W. We will provide guarantees on L(w) relative to L(w?) at some w? ∈ W, where the guarantees also depend on ‖w?‖. We could choose w? := arg minw∈W L(w), though our results hold for any w? ∈ W, and in some cases we might choose to compete with a low-norm w? that is not optimal in W.\nThe behavior of the accelerated gradient method also depends on the radius of W, defined as:\nD := sup w∈W\n‖w‖\nWe discuss two stochastic optimization approaches to deal with this problem: stochastic gradient descent (SGD), and accelerated gradient methods (AG). In a mini-batch setting, both approaches iteratively average sub-gradients with respect to several instances, and use this average to update the predictor. However, the update is done in different ways. In the Appendix, we also provide the form of the update in the more general mirror descent setting, where ‖w‖ is an arbitrary norm.\nThe stochastic gradient descent algorithm is summarized as Algorithm 1. In the pseudocode, PW refers to the projection on to the ball W (under the Euclidean distance). The accelerated gradient method (e.g., [5]) is summarized as Algorithm 2.\nIn terms of existing results, for the SGD algorithm we have [4, Section 5.1]\nE [L(w̄)]− L(w?) ≤ O\n(√ 1\nm +\nb\nm\n) ,\nwhereas for an accelerated gradient algorithm, we have [5]\nE [L(wagn )]− L(w?) ≤ O\n(√ 1\nm +\nb2\nm2\n) ,\nwhere in both cases the dependence on D,H and ‖w?‖ is suppressed. The above bounds suggest that, as long as b = o( √ m), both methods allow us to use a large mini-batch size b without significantly degrading the performance of either method. This allows the number of iterations n = m/b to be smaller, potentially resulting in faster convergence speed. However, these bounds do not show that accelerated methods have a significant advantage over the SGD algorithm, at least when b = o( √ m), since both have the same first-order term 1/ √ m. To understand the differences between these two methods better, we will need a more refined analysis, to which we now turn.\n3 Convergence Guarantees\nThe following theorems provide a refined convergence guarantee for the SGD algorithm and the AG algorithm, which improves on the analysis of [10, 4, 5] by being explicitly dependent on L(w?), the expected loss of the best predictor w? in W.\nTheorem 1. For any w? ∈ W, using Stochastic Gradient Descent with a step size of η = min  12H , √ b‖w∗‖2 L(w?)Hn\n1+ √ H‖w∗‖2 L(w?)bn , we have:\nE [L(w̄)]− L(w?) ≤\n√ 64H ‖w?‖2 L(w?)\nbn +\n4L(w?) + 4H ‖w?‖2\nn +\n8H ‖w?‖2\nbn\nNote that the radius D does not appear in the above bound, which depends only on ‖w?‖. This means that W could be unbounded, perhaps even the entire space, and a projection step for SGD is not really crucial. The step size, of course, still depends on ‖w?‖.\nTheorem 2. For any w? ∈ W, using Accelerated Gradient with step size parameters βi = i+12 , γi = γi p where\nγ = min\n{ 1\n4H ,\n√ b‖w∗‖2 348HL(w?)(n−1)2p+1 , ( b 1044H(n−1)2p ) p+1 2p+1 ( ‖w∗‖2 4H‖w∗‖2+ √ 4H‖w∗‖2L(w?) ) p 2p+1 } (1)\nand\np = min { max { log(b)\n2 log(n− 1) ,\nlog log(n)\n2 (log(b(n− 1))− log log(n))\n} , 1 } , (2)\nas long as n ≥ 783, we have:\nE [L(wagn )]− L(w?) ≤ 117\n√ H ‖w?‖2 L(w?)\nbn + 367H ‖w?‖4/3D 23√ bn + 546HD2\n√ log(n)\nbn +\n5H ‖w?‖2\nn2 ≤ 117 √ HD2L(w?)\nbn + 367HD2√ bn + 546HD2\n√ log(n)\nbn +\n5HD2\nn2\nUnlike for SGD, notice that the bound for the AG method above does depend on D, and a projection step is necessary for our analysis. However it is worth noting that D only appears in terms of order at least 1/n, and appears only mildly in the 1/( √ bn) term, suggesting some robustness to the radius D.\nWe emphasize that Theorem 2 gives more than a theoretical bound: it actually specifies a novel accelerated gradient strategy, where the step size γi scales polynomially in i, in a way dependent on the minibatch size b and L(w?). While L(w?) may not be known in advance, it does have the practical implication that choosing γi ∝ ip for some p < 1, as opposed to just choosing γi ∝ i as in [5]), might yield superior results.\nWe now provide a proof sketch of Theorems 1 and 2. A more general statement of the Theorems as well as a complete proof can be found in the Appendix.\nThe key observation used for analyzing the dependence on L(w?) is that for any non-negative H-smooth convex function f :W 7→ R, we have [13]:\n‖∇f(w)‖ ≤ √ 4Hf(w) (3)\nThis self-bounding property tells us that the norm of the gradient is small at a point if the loss is itself small at that point. This self-bounding property has been used in [14] in the online setting and in [13] in the stochastic setting to get better (faster) rates of convergence for non-negative smooth losses. The implication of this observation are that for any w ∈ W, ‖∇L(w)‖ ≤ √ 4HL(w) and ∀z ∈ Z, ‖`(w, z)‖ ≤ √ 4H`(w, z).\nProof sketch for Theorem 1. The proof for the stochastic gradient descent bound is mainly based on the proof techniques in [5] and its extension to the mini-batch case in [10]. Following the line of analysis in [5], one can show that\nE [ 1 n n∑ i=1 L(wi) ] − L(w?) ≤ ηn−1 n−1∑ i=1 E [ ‖∇L(wi)−∇`i(wi)‖2 ] + D 2 2η(n−1)\nIn the case of [5], E [‖∇L(wi)−∇`i(wi)‖] is bounded by the variance, and that leads to the final bound provided in [5] (by setting η appropriately). As noticed in [10], in the minibatch setting we have ∇`i(wi) = 1 b ∑bi t=b(i−1)+1 `(wi, zt) and so one can further show that\nE [ 1 n n∑ i=1 L(wi) ] − L(w?) ≤ ηb2(n−1) n−1∑ i=1 ib∑ t=\n(i−1)b+1\nE ‖∇L(wi)−∇`(wi, zt)‖2 + D 2\n2η(n−1) (4)\nIn [10], each of ‖∇L(wi)−∇`(wi, zt)‖ is bounded by σ0 and so setting η, the mini-batch bound provided there is obtained. In our analysis we further use the self-bounding property to (4) and get that\nE [ 1 n n∑ i=1 L(wi) ] − L(w?) ≤ 16Hηb(n−1) n−1∑ i=1 E [L(wi)] + D 2 2η(n−1)\nrearranging and setting η appropriately gives the final bound.\nProof sketch for Theorem 2. The proof of the accelerated method starts in a similar way as in [5]. For the γi’a and βi’s mentioned in the theorem, following similar lines of analysis as in [5] we get the preliminary bound\nE [L(wagn )]− L(w?) ≤ 2γ (n− 1)p+1 n−1∑ i=1 i2p E [∥∥∇L(wmdi )−∇`i(wmdi )∥∥2]+ D2γ(n− 1)p+1\nIn [5] the step size γi = γ(i+ 1)/2 and βi = (i+ 1)/2 which effectively amounts to p = 1 and further similar to the stochastic gradient descent analysis. Furthermore, each E [∥∥∇L(wmdi )−∇`i(wmdi )∥∥2] is assumed to be bounded by some constant, and thus leads to the final bound provided in [5] by setting γ appropriately. On the other hand, we first notice that due to the mini-batch setting, just like in the proof of stochastic gradient descent,\nE [L(wagn )]− L(w?) ≤ 2γ\nb2(n−1)p+1 n−1∑ i=1 i2p ib∑ t=\nb(i−1)+1\nE [∥∥∇L(wmdi )−∇`(wmdi , zt)∥∥2]+ D2γ(n−1)p+1\nUsing smoothness, the self bounding property some manipulations, we can further get the bound\nE [L(wagn )]− L(w?) ≤ 64Hγ b(n−1)1−p n−1∑ i=1 (E [L(wagi )]− L(w ?)) + 64HγL(w ?)(n−1)p b\n+ D 2 γ(n−1)p+1 + 32HD2 b(n−1)\nNotice that the above recursively bounds E [L(wagn )]−L(w?) in terms of ∑n−1 i=1 (E [L(w ag i )]− L(w?)). While unrolling the recursion all the way down to 2 does not help, we notice that for any w ∈ W, L(w)−L(w?) ≤ 12HD2 + 3L(w?). Hence we unroll the recursion to M steps and use this inequality for the remaining sum. Optimizing over number of steps up to which we unroll and also optimizing over the choice of γ, we get the bound,\nE [L(wagn )]− L(w?) ≤ √ 1648HD2L(w?) b(n−1) + 348(6HD2+2L(w?)) b(n−1) (b(n− 1)) p p+1 + 32HD 2 b(n−1)\n+ 4HD 2 (n−1)p+1 + 36HD2 b(n−1) log(n)\n(b(n−1)) p 2p+1\nUsing the p as given in the theorem statement, and few simple manipulations, gives the final bound.\n4 Optimizing with Mini-Batches\nTo compare our two theorems and understand their implications, it will be convenient to treat H and D as constants, and focus on the more interesting parameters of sample size m, minibatch size b, and optimal expected loss L(w?). Also, we will ignore the logarithmic factor in Theorem 2, since we will mostly be interested in significant (i.e. polynomial) differences between the two algorithms, and it is quite possible that this logarithmic factor is merely an artifact of our analysis. Using m = nb, we get that the bound for the SGD algorithm is\nE [L(w̄)]− L(w?) ≤ Õ\n(√ L(w?)\nbn +\n1\nn\n) = Õ (√ L(w?)\nm +\nb\nm\n) , (5)\nand the bound for the accelerated gradient method we propose is\nE [L(wagn )]− L(w?) ≤ Õ\n(√ L(w?)\nbn + 1√ bn + 1 n2\n) = Õ (√ L(w?)\nm +\n√ b m + b2 m2\n) . (6)\nTo understand the implication these bounds, we follow the approach described in [3, 12] to analyze large-scale learning algorithms. First, we fix a desired suboptimality parameter , which measures how close to L(w?) we want to get. Then, we assume that both algorithms are ran till the suboptimality of their outputs is at most . Our goal would be to understand the runtime each algorithm needs, till attaining suboptimality , as a function of L(w?), , b.\nTo measure this runtime, we need to discern two settings here: a parallel setting, where we assume that the mini-batch gradient computations are performed in parallel, and a serial setting, where the gradient computations are performed one after the other. In a parallel setting, we can take the number of iterations n as a rough measure of the runtime (note that in both algorithms, the runtime of a single iteration is comparable). In a serial setting, the relevant parameter is m, the number of data accesses.\nTo analyze the dependence on m and n, we upper bound (5) and (6) by , and invert them to get the bounds on m and n. Ignoring logarithmic factors, for the SGD algorithm we get\nn ≤ 1\n( L(w?)\n· 1 b + 1\n) m ≤ 1 ( L(w?) + b ) , (7)\nand for the AG algorithm we get\nn ≤ 1\n( L(w?)\n· 1 b + 1√ b + √\n) m ≤ 1 ( L(w?) + √ b+ b √ ) . (8)\nFirst, let us compare the performance of these two algorithms in the parallel setting, where the relevant parameter to measure runtime is n. Analyzing which of the terms in each bound dominates, we get that for the SGD algorithm, there are 2 regimes, while for the AG algorithm, there are 2-3 regimes depending on the relationship between L(w?) and . The following two tables summarize the situation (again, ignoring constants):\nSGD Algorithm\nRegime n b ≤ √ L(w?)m L(w\n?) 2b b ≥ √ L(w?)m 1\nAG Algorithm\nRegime n\n≤ L(w?)2 b ≤ L(w?)1/4m3/4 L(w ?) 2b\nb ≥ L(w?)1/4m3/4 1√\n≥ L(w?)2 b ≤ L(w?)m L(w ?) 2b\nL(w?)m ≤ b ≤ m2/3 1 √ b\nb ≥ m2/3 1√\nFrom the tables, we see that for both methods, there is an initial linear speedup as a function of the minibatch size b. However, in the AG algorithm, this linear speedup regime holds for much larger minibatch sizes1. Even beyond the linear speedup regime, the AG algorithm still maintains a √ b speedup, for the reasonable case where ≥ L(w?)2. Finally, in all regimes, the runtime bound of the AG algorithm is equal or significantly smaller than that of the SGD algorithm.\nWe now turn to discuss the serial setting, where the runtime is measured in terms of m. Inspecting (7) and (8), we see that a larger size of b actually requires m to increase for both algorithms. This is to be expected, since mini-batching does not lead to large gains in a serial setting. However, using mini-batching in a serial setting might still be beneficial for implementation reasons, resulting in constant-factor improvements in runtime (e.g. saving overhead and loop control, and via pipelining, concurrent memory accesses etc.). In that case, we can at least ask what is the largest mini-batch size that won’t degrade the runtime guarantee by more than a constant. Using our bounds, the mini-batch size b for the SGD algorithm can scale as much as L/ , vs. a larger value of L/ 3/2 for the AG algorithm.\nFinally, an interesting point is that the AG algorithm is sometimes actually necessary to obtain significant speed-ups via a mini-batch framework (according to our bounds). Based on the table above, this happens when the desired suboptimality is not much bigger then L(w?), i.e. = Ω(L(w?)). This includes the “separable” case, L(w?) = 0, and in general a regime where the “estimation error” and “approximation error” L(w?) are roughly the same—an arguably very relevant one in machine learning. For the SGD algorithm, the critical mini-batch value √ L(w?)m can be shown to equal L(w?)/ , which is O(1) in our case. So with SGD we get no non-constant parallel speedup. However, with AG, we still enjoy a speedup of at least Θ( √ b), all the way up to mini-batch size b = m2/3.\n5 Experiments\nWe implemented both the SGD algorithm (Algorithm 1) and the AG algorithm (Algorithm 2, using step-sizes of the form γi = γi p as suggested by Theorem 2) on two publicly-available binary classification problems,\n1Since it is easily verified that √ L(w?)m is generally smaller than both L(w?)1/4m3/4 and L(w?)m\nastro-physics and CCAT. We used the smoothed hinge loss `(w; x, y), defined as 0.5− yw>x if yw>x ≤ 0; 0 if yw>x > 1, and 0.5(1− yw>x)2 in between.\nWhile both datasets are relatively easy to classify, we also wished to understand the algorithms’ performance in the “separable” case L(w?) = 0, to see if the theory in Section 4 holds in practice. To this end, we created an additional version of each dataset, where L(w?) = 0, by training a classifier on the entire dataset and removing margin violations.\nIn all of our experiments, we used up to half of the data for training, and one-quarter each for validation and testing. The validation set was used to determine the step sizes η and γi. We justify this by noting that our goal is to compare the performance of the SGD and AG algorithms, independently of the difficulties in choosing their stepsizes. In the implementation, we neglected the projection step, as we found it does not significantly affect performance when the stepsizes are properly selected.\nIn our first set of experiments, we attempted to determine the relationship between the performance of the AG algorithm and the p parameter, which determines the rate of increase of the step sizes γi. Our experiments are summarized in Figure 5. Perhaps the most important conclusion to draw from these plots is that neither the “traditional” choice p = 1, nor the constant-step-size choice p = 0, give the best performance in all circumstances. Instead, there is a complicated data-dependent relationship between p, and the final classifier’s performance. Furthermore, there appears to be a weak trend towards higher p performing better for larger minibatch sizes b, which corresponds neatly with our theoretical predictions.\nIn our next experiment, we directly compared the performance of the SGD and AG methods. To do so, we varied the minibatch size b while holding the total amount of data used for training, m = nb, fixed. When L(w?) > 0 (top row of Figure 5), the total sample size m is high and the suboptimality is low (red and black plots), we see that for small minibatch size, both methods do not degrade as we increase b, corresponding to a linear parallel speedup. In fact, SGD is actually overall better, but as b increases, its performance degrades more quickly, eventually performing worse than AG. That is, even in the least favorable scenario for AG (high L(w?) and small , see the tables in Section 4), it does give benefits with large enough minibatch sizes. Also, we see that even here, once the suboptimality is roughly equal to L(w?), AG significantly outperforms SGD, even with small minibatches, agreeing with our the theory.\nTurning to the case L(w?) = 0 (bottom two rows of Figure 5), which is theoretically more favorable to AG, we see it is indeed mostly better, in terms of retaining linear parallel speedups for larger minibatch sizes, even for large data set sizes corresponding to small suboptimality values, and might even be advantageous with small minibatch sizes.\n6 Summary\nIn this paper, we presented novel contributions to the theory of first order stochastic convex optimization (Theorems 1 and 2, generalizing results of [4] and [5] to be sensitive to L (w?)), developed a novel step size strategy for the accelerated method that we used in order to obtain our results and we saw works well in practice, and provided a more refined analysis of the effects of minibatching which paints a different picture then previous analyses [4, 1] and highlights the benefit of accelerated methods.\nA remaining open practical and theoretical question is whether the bound of Theorem 2 is tight. Following [5], the bound is tight for b = 1 and b → ∞, i.e. the first and third terms are tight, but it is not clear whether the 1/( √ bn) dependence is indeed necessary. It would be interesting to understand whether with a more refined analysis, or perhaps different step-sizes, we can avoid this term, whether an altogether different algorithm is needed, or whether this term does represent the optimal behavior for any method based on b-aggregated stochastic gradient estimates.\nReferences\n[1] A. Agarwal and J. Duchi. Distributed delayed stochastic optimization. Technical report, arXiv, 2011.\n[2] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167 – 175, 2003.\n[3] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In NIPS, 2007.\n[4] O. Dekel, R. Gilad Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using mini-batches. Technical report, arXiv, 2010.\n[5] G. Lan. An optimal method for stochastic convex optimization. Technical report, Georgia Institute of Technology, 2009.\n[6] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.\n[7] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady AN SSSR, 269:543–547, 1983.\n[8] Y. Nesterov. Smooth minimization of non-smooth functions. Math. Program., 103(1):127–152, 2005.\n[9] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120(1):221–259, August 2009.\n[10] O. Shamir O. Dekel, R. Gilad-Bachrach and L. Xiao. Optimal distributed online prediction. In ICML, 2011.\n[11] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: primal estimated sub-gradient solver for SVM. Math. Program., 127(1):3–30, 2011.\n[12] S. Shalev-Shwartz and N. Srebro. SVM optimization: inverse dependence on training set size. In ICML, 2008.\n[13] N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and fast rates. In NIPS, 2010.\n[14] S.Shalev-Shwartz. Online Learning: Theory, Algorithms, and Applications. PhD thesis, Hebrew University of Jerusalem, 2007.\n[15] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. Submitted to SIAM Journal on Optimization, 2008.\n[16] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11:2543–2596, 2010.\nA Generalizing to Different Norms\nWe now turn to general norms and discuss the generic Mirror Descent and Accelerated Mirror Descent algorithms. In this more general case we let domain W be some closed convex set of a Banach space equipped with norm ‖·‖. We will use ‖·‖∗ to represent the dual norm of ‖·‖. Further the H-smoothness of the loss function in this general case is takes the form that for any z ∈ Z and any w,w′ ∈ W,\n‖∇`(w, z)−∇`(w′, z)‖∗ ≤ H ‖w −w ′‖\nThe key to generalizing the algorithms and result is to find a non-negative function R : W 7→ R that is strongly convex on the domain W w.r.t. to the norm ‖·‖, that is:\nDefinition 1. A function R :W 7→ R is said to be 1-strongly convex w.r.t. norm ‖·‖ if for any w,w′ ∈ W and any α ∈ [0, 1],\nR(αw + (1− α)w′) ≤ αR(w) + (1− α)R(w′)− α(1−α)2 ‖w −w ′‖2\nWe also denote more generally\nD := √\n2 sup w∈W R(w) .\nThe generalizations of the SGD and AG methods are summarized in Algorithms 3 and 4 respectively. The key difference between these and the Euclidean case is that the gradient descent step is replaced by a descent step involving gradient mappings of R and its conjugate R∗ and the projection step is replaced by Bregman projection (projection to set minimizing the Bregman divergence to the point).\nAlgorithm 3 Stochastic Mirror Descent with Mini-Batching (SMD)\nParameters: Step size η, mini-batch size b. Input: Sample z1, . . . , zm w1 = argmin\nw R(w)\nfor i = 1 to n = m/b do\nLet `i(wi) = 1 b ∑bi t=b(i−1)+1 `(wi, zt) w′i+1 := ∇R∗ (∇R (wi)− γi∇`i(wi)) wi+1 := argmin\nw∈W ∆R\n( w ∣∣w′i+1)\n} wi+1 = argmin\nw∈W {η〈∇`i(wi),w −wi〉+ ∆R (w|wi)}\nend for Return w̄ = 1n ∑n i=1 wi\nTheorem 3. Let R : W 7→ R be a non-negative strongly convex function on W w.r.t. norm ‖·‖. Let K = √ 2 supw:‖w‖≤1R(w). For any w ? ∈ W, using Stochastic Mirror Descent with a step size of\nη = min  12H , b32HK2 , √ 32bR(w?) L(w?)HK2n 16 ( 1 + √\n32HK2R(w?) L(w?)bn\n)  ,\nwe have that, E [L(w̄)]− L(w?) ≤ √ 128HK2R(w?) L(w?)\nbn +\n4L(w?) + 8HR(w?)\nn +\n16HK2R(w?)\nbn\nAlgorithm 4 Accelerated Mirror Descent Method (AMD)\nParameters: Step sizes (γi, βi), mini-batch size b Input: Sample z1, . . . , zm w1 = argmin\nw R(w)\nfor i = 1 to n = m/b do\nLet `i(wi) := 1 b ∑bi t=b(i−1)+1 `(w, zt) wmdi := β −1 i wi + (1− β −1 i )w ag i\nw′i+1 := ∇R∗ ( ∇R ( wmdi ) − γi∇`i(wmdi ) ) wi+1 := argmin\nw∈W ∆R\n( w ∣∣w′i+1)\n} wi+1 = argmin\nw∈W\n{ γi 〈 ∇`i(wmdi ),w −wmdi 〉 + ∆R ( w ∣∣wmdi )}\nwagi+1 ← β −1 i wi+1 + (1− β −1 i )w ag i\nend for Return wagn\nTheorem 4. Let R : W 7→ R be a non-negative strongly convex function on W w.r.t. norm ‖·‖. Also let K = √ 2 supw:‖w‖≤1R(w). For any w\n? ∈ W, using Accelerated Mirror Descent with step size parameters βi = i+1 2 , γi = γi p where\nγ = min\n{ 1\n4H ,\n√ bR(w?)\n174HK2L(w?)(n− 1)2p+1 ,\n( b\n1044HK2(n− 1)2p\n) p+1 2p+1 ( 6R(w?)\n3 2HD 2 + L(w?)\n) p 2p+1 } and\np = min { max { log(b)\n2 log(n− 1) ,\nlog log(n)\n2 (log(b(n− 1))− log log(n))\n} , 1 } ,\nas long as n ≥ max{783K2, 87K 2L(w?) HD2 }, we have that :\nE [L(wagn )]− L(w?) ≤ 164\n√ HK2R(w?)L(w?)\nb(n− 1) +\n580HK2(R(w?))2/3D 2 3\n√ b(n− 1)\n+ 545HK2D2\n√ log(n)\nb(n− 1) +\n8HR(w?)\n(n− 1)2\nB Complete Proofs\nWe provide complete proofs of Theorems 3 and 4, noting how Theorems 1 and 2 are specializations to the Euclidean case.\nB.1 Stochastic Mirror Descent\nProof of Theorem 3. Due to H-smoothness of convex function L we have that,\nL(wi+1) ≤ L(wi) + 〈∇L(wi),wi+1 −wi〉+ H\n2 ‖wi+1 −wi‖2\n= L(wi) + 〈∇L(wi)−∇`i(wi),wi+1 −wi〉+ H\n2 ‖wi+1 −wi‖2 + 〈∇`i(wi),wi+1 −wi〉\nby Holder’s inequality we get,\n≤ L(wi) + ‖∇L(wi)−∇`i(wi)‖∗‖wi+1 −wi‖+ H\n2 ‖wi+1 −wi‖2 + 〈∇`i(wi),wi+1 −wi〉\nsince for any α > 0, ab ≤ a 2 2α + αb2 2 ,\n≤ L(wi) + ‖∇L(wi)−∇`i(wi)‖2∗\n2(1/η −H) + (1/η −H) 2 ‖wi+1 −wi‖2 + H 2 ‖wi+1 −wi‖2 + 〈∇`i(wi),wi+1 −wi〉\n= L(wi) + ‖∇L(wi)−∇`i(wi)‖2∗ 2(1/η −H) + ‖wi+1 −wi‖2 2η + 〈∇`i(wi),wi+1 −wi〉\nWe now note that the update step can be written equivalently as\nwi+1 = argmin w∈W\n{η〈∇`i(wi),w −wi〉+ ∆R(w,wi)} .\nIt can be shown that (see for instance Lemma 1 of [5])\nη〈∇`i(wi),wi+1 −wi〉 ≤ η〈∇`i(wi),w? −wi〉+ ∆R(w?,wi)−∆R(w?,wi+1)−∆R(wi,wi+1)\nPlugging this we get that,\nL(wi+1) ≤ L(wi) + ‖∇L(wi)−∇`i(wi)‖2∗ 2(1/η −H) + ‖wi −wi+1‖2 2η + 〈∇`i(wi),w? −wi〉\n+ 1\nη (∆R(w\n?,wi)−∆R(w?,wi+1)−∆R(wi,wi+1))\n= L(wi) + ‖∇L(wi)−∇`i(wi)‖2∗ 2(1/η −H) + ‖wi −wi+1‖2 2η + 〈∇`i(wi)−∇L(wi),w? −wi〉+ 〈∇L(wi),w? −wi〉\n+ 1\nη (∆R(w\n?,wi)−∆R(w?,wi+1)−∆R(wi,wi+1))\n≥ L(wi) + ‖∇L(wi)−∇`i(wi)‖2∗ 2(1/η −H) + ‖wi −wi+1‖2 2η + 〈∇`i(wi)−∇L(wi),w? −wi〉 − 〈∇L(wi),wi −w?〉\n+ 1\nη (∆R(w\n?,wi)−∆R(w?,wi+1)−∆R(wi,wi+1))\nby strong convexity, ∆R(wi,wi+1) ≥ ‖wi−wi+1‖2 and so,\n≤ L(wi) + ‖∇L(wi)−∇`i(wi)‖2∗\n2(1/η −H) + 〈∇`i(wi)−∇L(wi),w? −wi〉 − 〈∇L(wi),wi −w?〉\n+ 1\n2η (∆R(w\n?,wi)−∆R(w?,wi+1))\nsince η ≤ 12H ,\n≤ L(wi) + η‖∇L(wi)−∇`i(wi)‖2∗ + 〈∇`i(wi)−∇L(wi),w? −wi〉 − 〈∇L(wi),wi −w?〉\n+ 1\nη (∆R(w\n?,wi)−∆R(w?,wi+1))\nby convexity, L(wi)− 〈∇L(wi),wi −w?〉 ≤ L(w?) and so\n≤ L(w?) + η‖∇L(wi)−∇`i(wi)‖2∗ + 〈∇`i(wi)−∇L(wi),w? −wi〉\n+ 1\nη (∆R(w\n?,wi)−∆R(w?,wi+1))\nHence we conclude that :\n1\nn− 1 n−1∑ i=1 L(wi+1)− L(w?) ≤ η (n− 1) n−1∑ i=1 ‖∇L(wi)−∇`i(wi)‖2∗ + 1 n− 1 n−1∑ i=1 〈∇`i(wi)−∇L(wi),w? −wi〉\n+ 1\nn− 1 n−1∑ i=1 ∆R(w ?,wi)−∆R(w?,wi+1) η\n= η\n(n− 1) n−1∑ i=1 ‖∇L(wi)−∇`i(wi)‖2∗ + 1 n− 1 n−1∑ i=1 〈∇`i(wi)−∇L(wi),w? −wi〉\n+ ∆R (w ?|w1)−∆R (w?|wn−1) η(n− 1)\n≤ η (n− 1) n−1∑ i=1 ‖∇L(wi)−∇`i(wi)‖2∗ + 1 n− 1 n−1∑ i=1 〈∇`i(wi)−∇L(wi),w? −wi〉\n+ R(w?)\nη(n− 1)\n≤ η (n− 1) n−1∑ i=1 ‖∇L(wi)−∇`i(wi)‖2∗ + 1 n− 1 n−1∑ i=1 〈∇`i(wi)−∇L(wi),w? −wi〉\n+ R(w?)\nη(n− 1)\nTaking expectation with respect to sample on both sides and noticing that E [〈∇`i(wi)−∇L(wi),w? −wi〉] = 0, we get that,\nE\n[ 1\nn− 1 n−1∑ i=1 L(wi+1)− L(w?)\n] ≤ η\n(n− 1) n−1∑ i=1 E [ ‖∇L(wi)−∇`i(wi)‖2∗ ] + R(w?) η(n− 1)\nNow note that\n∇L(wi)−∇`i(wi) = 1\nb bi∑ t=(i−1)b+1 (∇L(wi)−∇`(wi, zt))\nand that (∇L(wi)− `(wi, zt)) is a mean zero vector drawn i.i.d. Also note that wi only depends on the first (i− 1)b examples and so when we consider expectation w.r.t. z(i−1)b+1, . . . , zib alone, wi is fixed. Hence by Corollary B.2 we have that,\nE [ ‖∇L(wi)−∇`i(wi)‖2∗ ] ≤ K 2\nb2 E  ∥∥∥∥∥∥ bi∑ t=(i−1)b+1 (∇L(wi)−∇`(wi, zt)) ∥∥∥∥∥∥ 2\n∗  = K2\nb2 bi∑ t=(i−1)b+1 E [ ‖(∇L(wi)−∇`(wi, zt))‖2∗ ] Plugging this back we get that\nE\n[ 1\nn− 1 n−1∑ i=1 L(wi+1)− L(w?)\n] ≤ K 2η\nb2(n− 1) n−1∑ i=1 bi∑ t=(i−1)b+1 E [ ‖(∇L(wi)−∇`(wi, zt))‖2∗ ] + R(w?) η(n− 1)\n≤ 2K 2η\nb2(n− 1) n−1∑ i=1 ib∑ t=(i−1)b+1 E [ ‖∇L(wi)‖2 + ‖∇`(wi, zt)‖2∗ ] + R(w?) η(n− 1)\nfor any non-negative H-smooth convex function f , we have the self-bounding property that ‖∇f(w)‖∗ ≤√ 4Hf(w). Using this,\n≤ 8HK 2η\nb2(n− 1) n−1∑ i=1 ib∑ t=(i−1)b+1 E [L(wi) + `(wi, zt)] + R(w?) η(n− 1)\n= 16ηHK2\nb E\n[ 1\nn− 1 n−1∑ i=1 L(wi)\n] + R(w?)\nη(n− 1)\nAdding 1n−1L(w1) on both sides and removing L(wn) on the left we conclude that\nE\n[ 1\nn− 1 n−1∑ i=1 L(wi)\n] − L(w?) ≤ 16ηHK 2\nb E\n[ 1\nn− 1 n−1∑ i=1 L(wi)\n] + R(w?)\nη(n− 1) + L(w1) n− 1\nHence we conclude that\nE\n[ 1\nn n∑ i=1 L(wi)\n] − L(w?) ≤ 1( 1− 16ηHK2b ) (16ηHK2 b L(w?) + L(w1) n + R(w?) ηn )\n=\n( 1\n1− 16ηHK2b − 1\n) L(w?) +\n1\n1− 16ηHK2b\n( L(w1)\nn + R(w?) ηn\n)\n=\n( 1\n1− 16ηHK2b − 1\n) L(w?) + ( 1\n1− 16ηHK2b\n) L(w1)\nn\n+\n( 1\n1− 16ηHK2b\n) b\n16ηHK2 16HK2R(w?) bn\nWriting α = 1 1− 16ηHK2b\n− 1, so that η = b16HK2 ( 1− 1α+1 ) we get,\nE\n[ 1\nn n∑ i=1 L(wi)\n] − L(w?) ≤ αL(w?) + (α+ 1)L(w1)\nn +\n16H(α+ 1)2\nα\nR(w?)\nbn\n≤ αL(w?) + (α+ 1)L(w1) n +\n( α+ 1\nα\n) 32HR(w?)\nbn\nNow we shall always pick η ≤ b32HK2 so that α ≤ 1 and so\nE\n[ 1\nn n∑ i=1 L(wi)\n] − L(w?) ≤ αL(w?) + 32HK 2R(w?)\nα bn +\n2L(w1)\nn +\n16HK2R(w?)\nbn\nPicking\nη = min  12H , b32HK2 , √ 32bR(w?) L(w?)HK2n 16 ( 1 + √\n32HK2R(w?) L(w?)bn\n)  ,\nor equivalently α = min { 1, √\n32HK2R(w?) L(w?)bn\n} we get,\nE\n[ 1\nn n∑ i=1 L(wi)\n] − L(w?) ≤ √ 128HK2R(w?) L(w?)\nbn +\n2L(w1)\nn +\n16HK2R(w?)\nbn\nFinally note that by smoothness,\nL(w1) ≤ L(w?) + 〈∇L(w1)−∇L(w?),w1 −w?〉+ 〈∇L(w?),w1 −w?〉 ≤ L(w?) + ‖∇L(w1)−∇L(w?)‖∗ ‖w1 −w ?‖+ ‖∇L(w?)‖∗ ‖w1 −w ?‖\n≤ L(w?) +H ‖w1 −w?‖2 + √ 4HL(w?) ‖w1 −w?‖\nSince R is 1-strongly convex and w1 = argmin w R(w),\n≤ L(w?) + 2HR(w?) + √ 8HL(w?)R(w?) ≤ 2L(w?) + 4HR(w?)\nHence we conclude that\nE\n[ 1\nn n∑ i=1 L(wi)\n] − L(w?) ≤ √ 128HK2R(w?) L(w?)\nbn +\n4L(w?) + 8HR(w?)\nn +\n16HK2R(w?)\nbn\nUsing Jensen’s inequality concludes the proof.\nProof of Theorem 1. For Euclidean case R(w) = 12 ‖w‖ 2 2 and K = √ supw:‖w‖2≤1 ‖w‖ 2 = 1. Plugging these in the previous theorem concludes the proof.\nB.2 Accelerated Mirror Descent\nLemma B.1. For the accelerated update rule, if the step sizes βi ∈ [1,∞) and γi ∈ (0,∞) are chosen such that β1 = 1 and for all i ∈ [n]\n0 < γi+1(βi+1 − 1) ≤ βiγi and 2Hγi ≤ βi\nthen we have that\nE [L(wagn )]− L(w?) ≤ γ1(β1 − 1) γn(βn − 1) L(wag1 ) + 32H bγn(βn − 1) n−1∑ i=1 γ2i E [L(w ag i )] +\nD2\n2γn(βn − 1) +\n16H2D2\nbγn(βn − 1) n−1∑ i=1 γ2i β2i\nProof. First note that for any i,\nwagi+1 −w md i = β −1 i wi+1 + (1− β −1 i )w ag i −w md i\n= β−1i wi+1 + (1− β −1 i )w ag i − β −1 i wi − (1− β −1 i )w ag i = β−1i (wi+1 −wi) (9)\nNow by smoothness we have that\nL(wagi+1) ≤ L(w md i ) + 〈 ∇L(wmdi ),w ag i+1 −w md i 〉 + H\n2 ‖wagi+1 −w md i ‖2\n= L(wmdi ) + 〈 ∇L(wmdi ),w ag i+1 −w md i 〉 + H\n2β2i ‖wi+1 −wi‖2\n= L(wmdi ) + 〈 ∇L(wmdi ),w ag i+1 −w md i 〉 + 1\n2βiγi ‖wi+1 −wi‖2 − βi/γi −H 2β2i ‖wi+1 −wi‖2\nsince wagi+1 = β −1 i wi+1 + (1− β −1 i )w ag i ,\n= L(wmdi ) + 〈 ∇L(wmdi ), β−1i wi+1 + (1− β −1 i )w ag i −w md i 〉 + ‖wi −wi+1‖2\n2βiγi − βi/γi −H 2β2i ‖wi+1 −wi‖2\n= L(wmdi ) + (1− β−1i ) 〈 ∇L(wmdi ),w ag i −w md i 〉 +\n〈 ∇L(wmdi ),wi+1 −wmdi 〉 βi + ‖wi −wi+1‖2 2βiγi\n− βi/γi −H 2β2i ‖wi+1 −wi‖2\n= (1− β−1i ) ( L(wmdi ) + 〈 ∇L(wmdi ),w ag i −w md i 〉) + L(wmdi ) +\n〈 ∇L(wmdi ),wi+1 −wmdi 〉 βi + ‖wi −wi+1‖2 2βiγi\n− βi/γi −H 2β2i ‖wi+1 −wi‖2\n= (1− β−1i )L(w ag i ) +\nL(wmdi ) + 〈 ∇L(wmdi ),wi+1 −wmdi 〉 βi + ‖wi −wi+1‖2 2βiγi − βi/γi −H 2β2i ‖wi+1 −wi‖2\n= (1− β−1i )L(w ag i )− βi/γi −H 2β2i ‖wi+1 −wi‖2 + ‖wi −wi+1‖2 2βiγi\n+ L(wmdi ) +\n〈 ∇`i(wmdi ),wi+1 −wmdi 〉 + 〈 ∇L(wmdi )−∇`i(wmdi ),wi+1 −wmdi 〉 βi\n= (1− β−1i )L(w ag i )− βi/γi −H 2β2i ‖wi+1 −wi‖2 + ‖wi −wi+1‖2 2βiγi +\n〈 ∇L(wmdi )−∇`i(wmdi ),wi+1 −wi 〉 βi\n+ L(wmdi ) +\n〈 ∇`i(wmdi ),wi+1 −wmdi 〉 + 〈 ∇L(wmdi )−∇`i(wmdi ),wi −wmdi 〉 βi\nby Holder’s inequality,\n≤ (1− β−1i )L(w ag i )− βi/γi −H 2β2i ‖wi+1 −wi‖2 + ‖wi −wi+1‖2 2βiγi + ∥∥∇L(wmdi )−∇`i(wmdi )∥∥∗ ‖wi+1 −wi‖ βi\n+ L(wmdi ) +\n〈 ∇`i(wmdi ),wi+1 −wmdi 〉 + 〈 ∇L(wmdi )−∇`i(wmdi ),wi −wmdi 〉 βi\nsince for any a, b and α > 0, ab ≤ a 2 2α + αb2 2\n≤ (1− β−1i )L(w ag i ) + ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗ 2(βi/γi −H) + ‖wi −wi+1‖2 2βiγi\n+ L(wmdi ) +\n〈 ∇`i(wmdi ),wi+1 −wmdi 〉 + 〈 ∇L(wmdi )−∇`i(wmdi ),wi −wmdi 〉 βi\nWe now note that the update step 2 of accelerated gradient can be written equivalently as\nwi+1 = argmin w∈W\n{ γi 〈 ∇`i(wmdi ),w −wmdi 〉 + ∆R ( w ∣∣wmdi )} .\nIt can be shown that (see for instance Lemma 1 of [5]) γi 〈 ∇`i(wi),wi+1 −wmdi 〉 ≤ γi 〈 ∇`i(wmdi ),w? −wmdi 〉 + ∆R (w ?|wi)−∆R (w?|wi+1)−∆R (wi|wi+1)\nPlugging this we get that,\nL(wagi+1) ≤ (1− β −1 i )L(w ag i ) + ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗ 2(βi/γi −H) + ‖wi −wi+1‖2 2βiγi + 〈 ∇`i(wmdi ),w? −wmdi 〉 βi\n+ L(wmdi ) +\n〈 ∇L(wmdi )−∇`i(wmdi ),wi −wmdi 〉 βi + ∆R (w ?|wi)−∆R (w?|wi+1)−∆R (wi|wi+1) γiβi\nby strong-convexity of R, ∆R (wi|wi+1) ≥ 12 ‖wi −wi+1‖ 2 and so,\n= (1− β−1i )L(w ag i ) + ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗ 2(βi/γi −H) + 〈 ∇`i(wmdi ),w? −wmdi 〉 βi\n+ L(wmdi ) +\n〈 ∇L(wmdi )−∇`i(wmdi ),wi −wmdi 〉 βi + ∆R (w ?|wi)−∆R (w?|wi+1) γiβi\n= (1− β−1i )L(w ag i ) + ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗ 2(βi/γi −H) + 〈 ∇`i(wmdi )−∇L(wmdi ),w? −wmdi 〉 βi\n+\n〈 ∇L(wmdi )−∇`i(wmdi ),wi −wmdi 〉 βi + ∆R (w ?|wi)−∆R (w?|wi+1) γiβi\n+ L(wmdi ) +\n〈 ∇L(wmdi ),w? −wmdi 〉 βi\nby convexity, L(w?) ≥ L(wmdi ) + 〈 ∇L(wmdi ),w? −wmdi 〉 , hence\n≤ (1− β−1i )L(w ag i ) + ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗ 2(βi/γi −H) + 〈 ∇`i(wmdi )−∇L(wmdi ),w? −wmdi 〉 βi\n+\n〈 ∇L(wmdi )−∇`i(wmdi ),wi −wmdi 〉 βi + ∆R (w ?|wi)−∆R (w?|wi+1) γiβi + L(w?) βi\n= (1− β−1i )L(w ag i ) + ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗ 2(βi/γi −H) + 〈 ∇L(wmdi )−∇`i(wmdi ),wi −w? 〉 βi\n+ ∆R (w ?|wi)−∆R (w?|wi+1) γiβi + β−1i L(w ?)\n= L(w?) + (1− β−1i ) (L(w ag i )− L(w ?)) + ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗ 2(βi/γi −H) + 〈 ∇L(wmdi )−∇`i(wmdi ),wi −w? 〉 βi\n+ ∆R (w ?|wi)−∆R (w?|wi+1) γiβi\nThus we conclude that\nL(wagi+1)− L(w ?) ≤ (1− β−1i ) (L(w ag i )− L(w ?)) + ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗ 2(βi/γi −H) + 〈 ∇L(wmdi )−∇`i(wmdi ),wi −w? 〉 βi\n+ ∆R (w ?|wi)−∆R (w?|wi+1) βiγi\nMultiplying throughout by βiγi we get\nγiβi ( L(wagi+1)− L(w ?) ) ≤ γi(βi − 1) (L(wagi )− L(w ?)) + γiβi ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗ 2(βi/γi −H)\n+ ∆R (w ?|wi)−∆R (w?|wi+1) + γi 〈 ∇L(wmdi )−∇`i(wmdi ),wi −w? 〉\nOwing to the condition that γi+1(βi+1 − 1) ≤ γiβi we have that γi+1(βi+1 − 1) ( L(wagi+1)− L(w ?) ) ≤ γi(βi − 1) (L(wagi )− L(w ?)) + γiβi ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗ 2(βi/γi −H)\n+ ∆R (w ?|wi)−∆R (w?|wi+1) + γi 〈 ∇L(wmdi )−∇`i(wmdi ),wi −w? 〉 Using the above inequality repeatedly we conclude that\nγn(βn − 1) (L(wagn )− L(w?)) ≤ γ1(β1 − 1) (L(w ag 1 )− L(w?)) + n−1∑ i=1\nγiβi ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗\n2(βi/γi −H)\n+ ∆R (w ?|w1)−∆R (w?|wn) + n−1∑ i=1 γi 〈 ∇L(wmdi )−∇`i(wmdi ),wi −w? 〉 ≤ γ1(β1 − 1) (L(wag1 )− L(w?)) +\nn−1∑ i=1\nγiβi ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗\n2(βi/γi −H)\n+R(w?) + n−1∑ i=1 γi 〈 ∇L(wmdi )−∇`i(wmdi ),wi −w? 〉 = γ1(β1 − 1) (L(wag1 )− L(w?)) +\nn−1∑ i=1\nγiβi ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗\n2(βi/γi −H)\n+R(w?) + n−1∑ i=1 γi 〈 ∇L(wmdi )−∇`i(wmdi ),wi −w? 〉 since 2Hγi ≤ βi,\n≤ γ1(β1 − 1) (L(wag1 )− L(w?)) + n−1∑ i=1 γ2i ∥∥∇L(wmdi )−∇`i(wmdi )∥∥2∗ +R(w?)\n+ n−1∑ i=1 γi 〈 ∇L(wmdi )−∇`i(wmdi ),wi −w? 〉 ≤ γ1(β1 − 1)L(wag1 ) +\nn−1∑ i=1 2γ2i ‖∇L(w ag i )−∇`i(w ag i )‖ 2 ∗ +R(w ?)\n+ n−1∑ i=1 γi 〈 ∇L(wmdi )−∇`i(wmdi ),wi −w? 〉 +\nn−1∑ i=1 2γ2i ∥∥∇L(wmdi )−∇`i(wmdi )−∇L(wagi ) +∇`i(wagi )∥∥2∗\nTaking expectation we get that\nγn(βn − 1) (E [L(wagn )]− L(w?)) ≤ γ1(β1 − 1)L(w ag 1 ) + n−1∑ i=1 2γ2i E [ ‖∇L(wagi )−∇`i(w ag i )‖ 2 ∗ ] +R(w?)\n+ n−1∑ i=1 2γ2i E [∥∥∇L(wmdi )−∇`i(wmdi )−∇L(wagi ) +∇`i(wagi )∥∥2∗]\n(10)\nNow note that\n∇L(wagi )−∇`i(w ag i ) =\n1\nb bi∑ t=(i−1)b+1 (∇L(wagi )− `(w ag i , zt)) and\n∇L(wagi )−∇`i(w ag i )−∇L(w md i )+∇`i(wmdi ) =\n1\nb bi∑ t=(i−1)b+1 ( ∇L(wagi )− `(w ag i , zt)−∇L(w md i ) + `(w md i , zt) ) Further (∇L(wi)− `(wi, zt)) and ( ∇L(wagi )− `(w ag i , zt)−∇L(wmdi ) + `(wmdi , zt) ) are mean zero vectors drawn i.i.d. Also note that wagi only depends on the first (i − 1)b examples and so when we consider expectation w.r.t. z(i−1)b+1, . . . , zib, wi is fixed. Hence by Corollary B.2 we have that,\nE [ ‖∇L(wagi )−∇`i(w ag i )‖ 2 ∗ ] = K2\nb2 E  ∥∥∥∥∥∥ bi∑ t=(i−1)b+1 (∇L(wagi )−∇`(w ag i , zt)) ∥∥∥∥∥∥ 2\n∗  ≤ K 2\nb2 bi∑ t=(i−1)b+1 E [ ‖(∇L(wagi )−∇`(w ag i , zt))‖ 2 ∗ ] and similarly\nE [∥∥∇L(wagi )−∇`i(wagi )−∇L(wmdi ) +∇`i(wmdi )∥∥2∗]\n≤ K 2\nb2 bi∑ t=(i+1)b+1 E [∥∥∇L(wagi )−∇`(wagi , zt)−∇L(wmdi ) +∇`(wmdi , zt)∥∥2∗]\nPlugging these back in Equation 10 we get : γn(βn − 1) (E [L(wagn )]− L(w?)) ≤ γ1(β1 − 1)L(wag1 ) + n−1∑ i=1 2K2γ2i b2 bi∑ t=(i−1)b+1 E [ ‖(∇L(wagi )−∇`(w ag i , zt))‖ 2 ∗ ] +R(w?)\n+ n−1∑ i=1 2K2γ2i b2 bi∑ t=(i+1)b+1 E [∥∥∥∇L(wagi )−∇`(wagi , zt)−∇L(wmdi ) +∇`(wmdi , zt)∥∥∥2∗ ]\n≤ γ1(β1 − 1)L(wag1 ) + n−1∑ i=1 4K2γ2i b2 bi∑ t=(i−1)b+1 E [ ‖∇L(wagi )‖ 2 ∗ + ‖∇`(w ag i , zt)‖ 2 ∗ ] +R(w?)\n+ n−1∑ i=1 4K2γ2i b2 bi∑ t=(i+1)b+1 E [∥∥∥∇L(wagi )−∇L(wmdi )∥∥∥2∗ + ∥∥∥∇`(wmdi , zt)−∇`(wagi , zt)∥∥∥2∗ ]\nfor any non-negative H-smooth convex function f , we have the self-bounding property that ‖∇f(w)‖ ≤ √\n4Hf(w). Using this,\n≤ γ1(β1 − 1)L(wag1 ) + n−1∑ i=1 16HK2γ2i b2 bi∑ t=(i−1)b+1 E [L(wagi ) + `(w ag i , zt)] +R(w ?)\n+ n−1∑ i=1 4K2γ2i b2 bi∑ t=(i+1)b+1 E [∥∥∥∇L(wagi )−∇L(wmdi )∥∥∥2∗ + ∥∥∥∇`(wmdi , zt)−∇`(wagi , zt)∥∥∥2∗ ]\n= γ1(β1 − 1)L(wag1 ) + n−1∑ i=1 32HK2γ2i b E [L(wagi )] +R(w ?)\n+ n−1∑ i=1 4K2γ2i b2 bi∑ t=(i+1)b+1 E [∥∥∥∇L(wagi )−∇L(wmdi )∥∥∥2∗ + ∥∥∥∇`(wmdi , zt)−∇`(wagi , zt)∥∥∥2∗ ]\nby H-smoothness of L and ` we have that ∥∥∇L(wagi )−∇L(wmdi )∥∥∗ ≤ H ∥∥wagi −wmdi ∥∥. Similarly we also have that∥∥∇`(wagi , zt)−∇`(wmdi , zt)∥∥∗ ≤ H ∥∥wagi −wmdi ∥∥. Hence,\n≤ γ1(β1 − 1)L(wag1 ) + n−1∑ i=1 32HK2γ2i b E [L(wagi )] +R(w ?)\n+ n−1∑ i=1 8H2K2γ2i b E [∥∥∥wagi −wmdi ∥∥∥2]\nHowever, wmdi ← β−1i wi + (1 − β −1 i )w ag i . Hence ∥∥wagi −wmdi ∥∥2 ≤ ‖wi−wagi ‖2β2i ≤ 2‖wi−w1‖2+2‖w1−wagi ‖2β2i ≤ 4D2β2i . Hence,\n≤ γ1(β1 − 1)L(wag1 ) + n−1∑ i=1 32HK2γ2i b E [L(wagi )] +R(w ?) + 32H2K2D2 b n−1∑ i=1 γ2i β2i\nDividing throughout by γn(βn − 1) concludes the proof.\nProof of Theorem 4. First note that the for any i,\n2Hγi = 2Hγi p ≤ i\np 2 ≤ βi\nAlso note that since p ∈ [0, 1],\nγi+1(βi+1 − 1) = γ i(i+ 1)p 2 ≤ γ i p(i+ 1) 2 = γiβi\nThus we have verified that the step sizes satisfy the conditions required by previous lemma. From the previous lemma we have that\nE [L(wagn )]− L(w?) ≤ γ1(β1 − 1) γn(βn − 1) L(wag1 ) + 32HK2 bγn(βn − 1) n−1∑ i=1 γ2i E [L(w ag i )] + R(w?) γn(βn − 1) + 32H2K2D2 bγn(βn − 1) n−1∑ i=1 γ2i β2i\n= 64HK2γ\nbnp(n− 1) n−1∑ i=1 i2p E [L(wagi )] + 2R(w?) γnp(n− 1) + 256H2K2D2γ bnp(n− 1) n−1∑ i=1 i2p (i+ 1)2\n≤ 64HK 2γ(n− 1)2p\nbnp(n− 1)\nn−1∑ i=1 E [L(wagi )] + 2R(w?) γ(n− 1)p+1 + 256H2K2D2γ b(n− 1)p+1 n−1∑ i=1 1 i2(1−p)\n≤ 64HK 2γ b(n− 1)1−p n−1∑ i=1 E [L(wagi )] + 2R(w?) γ(n− 1)p+1 + 256H2K2D2γ b(n− 1)p+1 n−1∑ i=1 1 i2(1−p)\n≤ 64HK 2γ b(n− 1)1−p n−1∑ i=1 E [L(wagi )] + 2R(w?) γ(n− 1)p+1 + 256H2K2D2γ b(n− 1)\n≤ 64HK 2γ b(n− 1)1−p n−1∑ i=1 (E [L(wagi )]− L(w ?)) + 64HK2γL(w?)(n− 1)p b + 2R(w?) γ(n− 1)p+1\n+ 256H2K2D2γ\nb(n− 1)\nsince γ ≤ 1/4H,\n≤ 64HK 2γ b(n− 1)1−p n−1∑ i=1 (E [L(wagi )]− L(w ?)) + 64HK2γL(w?)(n− 1)p b + 2R(w?) γ(n− 1)p+1\n+ 64HK2D2\nb(n− 1)\nThus we have shown that\nE [L(wagn )]− L(w?) ≤ 64HK2γ b(n− 1)1−p n−1∑ i=1 (E [L(wagi )]− L(w ?)) + 64HK2γL(w?)(n− 1)p b + 2R(w?) γ(n− 1)p+1\n+ 64HK2D2\nb(n− 1)\nNow if we use the notation ai = E [L(wagi )]− L(w?), A(i) = 64HK2γ b(i−1)1−p and\nB(i) = 64HK2γL(w?)(i− 1)p\nb +\n2R(w?)\nγ(i− 1)p+1 +\n64HK2D2\nb(i− 1)\nNote that for any i by smoothness, ai ≤ L0 := 32HD 2 + L(w?) Also notice that\nn∑ i=n−M−1 A(i) = 64HK2γ b n∑ i=n−M−1\n1 (i− 1)1−p ≤ 64HK\n2γnp\nb\nHence as long as\nγ ≤ b 64HK2np\n, (11)∑n i=n−M−1A(i) ≤ 1. We shall ensure that the γ we choose will satisfy the above condition. Now applying lemma B.3 we get that for any M ,\nan ≤ eA(n) ( a0(n−M) +\nn∑ i=n−M−1 B(i)\n) +B(n) (12)\nNow notice that\nn∑ i=n−M−1 B(i) = 64HK2γL(w?) b n∑ i=n−M−1\n1\n(i− 1)p +\n2R(w?)\nγ\nn∑ i=n−M−1\n1\n(i− 1)p+1 +\n64HK2D2\nb\nn∑ i=n−M−1 1 (i− 1)\n≤ 64HK 2γL(w?)(n−M − 2)p\nb +\n2R(w?)\nγ(n−M − 2)p+1 +\n64HK2D2\nb(n−M − 2) +\n64HK2γL(w?)(n− 1)p+1\nb\n+ 2R(w?)\nγ(n−M − 2)p +\n64HK2D2 log n\nb\nPlugging this back in Equation 12 we conclude that\nan ≤ 64eHK2γ b(n− 1)1−p ( L0(n−M) + 64HK2γL(w?)(n−M − 2)p b +\n2R(w?)\nγ(n−M − 2)p+1\n+ 64HK2D2\nb(n−M − 2) +\n64HK2γL(w?)(n− 1)p+1\nb +\n2R(w?)\nγ(n−M − 2)p +\n64HK2D2 log n\nb ) + 64HK2γL(w?)(n− 1)p\nb +\n2R(w?)\nγ(n− 1)p+1 +\n64HK2D2\nb(n− 1)\n≤ 64eHK 2γ b(n− 1)1−p ( L0(n−M − 2) + 64HK2D2 b(n−M − 2) +\n4R(w?)\nγ(n−M − 2)p +\n64HK2D2 log(n)\nb\n+ 256HK2γL(w?)(n− 1)p+1\nb\n) + 64HK2γL(w?)(n− 1)p\nb +\n4R(w?)\nγ(n− 1)p+1 +\n64HK2D2\nb(n− 1)\nsince γ ≤ b64HK2np and 64HK2D2 b(n−M−2) ≤ 4R(w?) γ(n−M−2)p ,\n≤ 64eHK 2γ b(n− 1)1−p ( L0(n−M − 2) +\n6R(w?)\nγ(n−M − 2)p +\n64HK2D2 log n\nb\n+ 256HK2γL(w?)(n− 1)p+1\nb\n) + 64HK2γL(w?)(n− 1)p\nb +\n4R(w?)\nγ(n− 1)p+1 +\n64HK2D2\nb(n− 1)\nWe now optimize over the choice of M above by using\n(n−M − 2) = ( 6R(w?)\nγL0\n) 1 p+1\nOfcourse for the choice of M to be valid we need that n−M − 2 ≤ n which gives our second condition on γ which is\nγ ≥ 6R(w ?)\nnp+1L0 (13)\nPlugging in this M we get,\nan ≤ 64eHK2γ\nb(n− 1)1−p\n( 2L p p+1\n0\n( 6R(w?)\nγ\n) 1 p+1\n+ 128HK2γL(w?)(n− 1)p+1\nb +\n64HK2D2 log n\nb\n)\n+ 64HK2γL(w?)(n− 1)p\nb +\n2R(w?)\nγ(n− 1)p+1 +\n64HK2D2\nb(n− 1)\n= 128eHK2γ\np p+1L\np p+1 0 (6R(w ?)) 1 p+1\nb(n− 1)1−p +\n2e(64HK2γ)2L(w?)(n− 1)2p\nb2 +\n2R(w?)\nγ(n− 1)p+1\n+ 2e(64HK2)2D2γ log n\nb2(n− 1)1−p +\n64HK2γL(w?)(n− 1)p\nb +\n64HK2D2\nb(n− 1)\nhowever by condition in Equation 11, γ ≤ b64HK2np , hence\n≤ 348HK 2γ\np p+1L\np p+1 0 (6R(w ?)) 1 p+1\nb(n− 1)1−p +\n2e(64HK2)2D2γ log n\nb2(n− 1)1−p\n+ 348HK2γL(w?)(n− 1)p\nb +\n2R(w?)\nγ(n− 1)p+1 +\n64HK2D2\nb(n− 1) (14)\nWe shall try to now optimize the above bound w.r.t. γ, To this end set\nγ = min\n{ 1\n4H ,\n√ bR(w?)\n174HK2L(w?)(n− 1)2p+1 ,\n( b\n1044HK2(n− 1)2p\n) p+1 2p+1 ( 6R(w?)\nL0\n) p 2p+1 } (15)\nWe first need to verify that this choice of γ satisfies the conditions in Equation 11 and 13. To this end, note that as for the condition in Equation 11,\nγ ≤ (\nb\n1044HK2(n− 1)2p\n) p+1 2p+1 ( 6R(w?)\nL0\n) p 2p+1\nand hence it can be easily verified that for n ≥ 3, γ ≤ b64HK2np . On the other hand to verify the condition in Equation 13, we need to show that\nγ = min\n{ 1\n4H ,\n√ bR(w?)\n174HK2L(w?)(n− 1)2p+1 ,\n( b\n1044HK2(n− 1)2p\n) p+1 2p+1 ( 6R(w?)\nL0\n) p 2p+1 }\n≥ 6R(w ?)\nnp+1 (L0)\nIt can be verified that this condition is satisfied as long as, n ≥ max { 3, 87K2L(w?)\nb ,\n783K2\nb } So in effect as long as n ≥ 3 and sample size nb ≥ max{783K2, 87K\n2L(w?) HD2 } the conditions are satisfied. Now\nplugging in this choice of γ into the bound in Equation 14, we get\nan ≤\n√ 2784HK2R(w?)L(w?)\nb(n− 1) +\n2\n3\n( 6264HK2R(w?)L p p+1\n0\nb(n− 1)\n) p+1 2p+1\n+ 64HK2D2\nb(n− 1)\n+ 8HR(w?)\n(n− 1)p+1 +D2 log(n)\n( 64HK2\nb(n− 1)\n) 3p+1 2p+1 ( 6R(w?)\nL0\n) p 2p+1\n≤\n√ 2784HK2R(w?)L(w?)\nb(n− 1) +\n2\n3\n( 6264HK2R(w?)L p p+1\n0\nb(n− 1)\n) p+1 2p+1\n+ 64HK2D2\nb(n− 1)\n+ 8HR(w?)\n(n− 1)p+1 +D2 log(n)\n( 64HK2\nb(n− 1)\n) 3p+1 2p+1 ( 6R(w?)\nL0\n) p 2p+1\n≤\n√ 2784HK2R(w?)L(w?)\nb(n− 1) +\n2\n3\n( 6264HK2R(w?)L p p+1\n0\nb(n− 1)\n) p+1 2p+1\n+ 64HK2D2\nb(n− 1)\n+ 8HR(w?)\n(n− 1)p+1 +\n(( 96K2 ) p p+1 D2\nR(w?)\n) p+1 2p+1 (\n64HK2R(w?)\nb(n− 1)\n) log(n)\n(b(n− 1)) p 2p+1\n≤\n√ 2784HK2R(w?)L(w?)\nb(n− 1) +\n4176HK2R(w?)\nb(n− 1)\n( L0\n6264HK2R(w?)\n) p 2p+1\n(b(n− 1)) p 2p+1 + 64HK2D2\nb(n− 1)\n+ 8HR(w?)\n(n− 1)p+1 +\n( 64HK2D2\nb(n− 1)\n) log(n)\n(b(n− 1)) p 2p+1\n( 384HK2R(w?)\nL0\n) p 2p+1\n≤\n√ 2784HK2R(w?)L(w?)\nb(n− 1) +\n4176HK2R(w?)\nb(n− 1)\n( L0\n6264HK2R(w?)\n) p 2p+1\n(b(n− 1)) p 2p+1 + 64HK2D2\nb(n− 1)\n+ 8HR(w?)\n(n− 1)p+1 +\n( 64HK2D2\nb(n− 1)\n) log(n)\n(b(n− 1)) p 2p+1\nPicking\np = min { max { log(b)\n2 log(n− 1) ,\nlog log(n)\n2 (log(b(n− 1))− log log(n))\n} , 1 }\nwe get the bound,\nan ≤\n√ 2784HK2R(w?)L(w?)\nb(n− 1) +\n( 4176HK2R(w?)√\nb(n− 1) +\n4176HK2R(w?) √ log(n)\nb(n− 1)\n)( L0\n6264HK2R(w?)\n) 1 3\n+ 120HK2D2\nb(n− 1) +\n8HR(w?)\n(n− 1)2 + 8HR(w?)√ b(n− 1) + 64HK2D2\n√ log(n)\nb(n− 1)\n≤\n√ 2784HK2R(w?)L(w?)\nb(n− 1) +\n( 4176HK2R(w?)√\nb(n− 1) +\n4176HK2R(w?) √ log(n)\nb(n− 1)\n)( L0\n6264HK2R(w?)\n) 1 3\n+ 120HK2D2\nb(n− 1) +\n8HR(w?)\n(n− 1)2 + 8HR(w?)√ b(n− 1) + 64HK2D2\n√ log(n)\nb(n− 1)\n≤\n√ 2784HK2R(w?)L(w?)\nb(n− 1) +\n454(HK2R(w?))2/3L 1 3 0√\nb(n− 1) +\n454(HK2R(w?))2/3L 1 3 0 √ log(n)\nb(n− 1)\n+ 120HK2D2\nb(n− 1) +\n8HR(w?)\n(n− 1)2 + 8HR(w?)√ b(n− 1) + 64HK2D2\n√ log(n)\nb(n− 1)\nRecall that L0 = 3 2HD 2 + L(w?). Now note that if L(w?) ≤ HK2D2/2 then L0 ≤ 2HK2D2, on the other hand if L(w?) > HK2D2/2 then (HK2R(w?))2/3L 1 3 0 ≤ √ 4HK2R(w?)L(w?). Hence we can conclude that,\nan ≤\n√ 2784HK2R(w?)L(w?)\nb(n− 1) +\n454HK2(R(w?))2/3(2D2) 1 3\n√ b(n− 1)\n+ 454HK2(R(w?))2/3(2D2)\n1 3 √ log(n)\nb(n− 1)\n+ 120HK2D2\nb(n− 1) +\n8HR(w?)\n(n− 1)2 + 8HR(w?)√ b(n− 1) + 64HK2D2\n√ log(n)\nb(n− 1) +\n908 √ HK2R(w?)L(w?)√\nb(n− 1) + 908 √ HK2R(w?)L(w?) log(n)\nb(n− 1)\nSince n > 783K2 and R(w?) ≤ D2/2 we can conclude that\nan ≤ 164\n√ HK2R(w?)L(w?)\nb(n− 1) +\n580HK2(R(w?))2/3D 2 3\n√ b(n− 1)\n+ 545HK2D2\n√ log(n)\nb(n− 1) +\n8HR(w?)\n(n− 1)2\nThis concludes the proof.\nProof of Theorem 2. For Euclidean case R(w) = 12 ‖w‖ 2 2 and K = √ supw:‖w‖2≤1 ‖w‖ 2 = 1. Plugging these in the previous theorem (along with appropriate step size) we get\nE [L(wagn )]− L(w?) ≤ 116\n√ H ‖w?‖2 L(w?)\nb(n− 1) + 366H ‖w?‖4/3D 23√ b(n− 1) + 545HD2\n√ log(n)\nb(n− 1) +\n4H ‖w?‖2\n(n− 1)2\nThe second inequality is a direct consequence of the fact that ‖w?‖ ≤ D.\nB.3 Some Technical Lemmas Lemma B.2. Denote K := √\n2 supw:‖w‖≤1R(w), then for any x1, . . . ,xb mean zero vectors drawn iid from\nany fixed distribution,\nE ∥∥∥∥∥1b b∑ t=1 xt ∥∥∥∥∥ 2\n∗\n ≤ K2 b2 b∑ t=1 E [ ‖xt‖2∗ ]\nProof. We start by noting that∥∥∥∥∥1b i∑ t=1 xt ∥∥∥∥∥ 2\n∗\n= ( sup\nw:‖w‖≤1\n〈 w, 1\nb i∑ t=1 xt\n〉)2\n= ( inf α 1 α sup\nw:‖w‖≤1\n〈 w, α\nb i∑ t=1 xt\n〉)2\n≤ ( inf α { 1 α sup w:‖w‖≤1 R(w) + 1 α R∗ ( α b i∑ t=1 xt )})2\n= ( inf α { K2 2α + 1 α R∗ ( α b i∑ t=1 xt )})2 (16)\nwhere the step before last was due to Fenchel-Young inequality and R∗ is simply the convex conjugate of R. Now For any i ∈ [b] define Si = R∗ ( α b ∑i t=1 xt ) . We claim that\nE [Si] ≤ E [Si−1] + α2 2b2 E [ ‖xi‖2∗ ] To see this note that since R is 1-strongly convex w.r.t. ‖·‖, by duality R∗ is 1-strongly smooth w.r.t. ‖·‖∗ and so for any i ∈ [b],\nR∗\n( 1\nb i∑ t=1 xt\n) ≤ R∗ ( 1\nb i−1∑ t=1 xt\n) + 1\n2b\n〈 ∇R∗ ( 1\nb i−1∑ t=1 xt\n) ,xi 〉 + α2\n2b2 ‖xi‖2∗\ntaking expectation w.r.t. xi and noting that E [xi] = 0 by assumption we see that\nExb [Si] ≤ Si−1 + α2\n2b2 Exi\n[ ‖xi‖2∗ ] Taking expectation we get as claimed that :\nE [Si] ≤ E [Si−1] + α2 2b2 E [ ‖xi‖2∗ ] Now using this above recursively (and noting that S0 = 0 ) we conclude that\nE [Si] ≤ α2\n2b2 i∑ t=1 E [ ‖xt‖2∗ ] Plugging this back in Equation 16 we get\nE ∥∥∥∥∥1b b∑ t=1 xt ∥∥∥∥∥ 2\n∗\n ≤ (inf α { K2 2α + α 2b2 i∑ t=1 E [ ‖xt‖2∗ ]})2\n= ( inf α { K2 2α + α 2b2 i∑ t=1 E [ ‖xi‖2∗ ]})2 = K2 b2 i∑ t=1 E [ ‖xt‖2∗ ]\nLemma B.3. Consider a sequence of non-negative number a1, . . . , an ∈ [0, a0] that satisfy\nan ≤ A(n) n−1∑ i=1 ai +B(n)\nwhere A is decreasing in n. For such a sequence, for any m ∈ [n], as long as A(i) ≤ 1/2 for any i ≥ n−m−1 and ∑n i=n−m−1A(i) ≤ 1 then\nan ≤ eA(n) ( a0(n−m) +\nn∑ i=n−m−1 B(i)\n) +B(n)\nProof. We shall unroll this recursion. Note that\nan ≤ A(n) n−1∑ i=1 ai +B(n)\n= A(n) ( n−2∑ i=1 ai + an−1 ) +B(n)\n≤ A(n) ( n−2∑ i=1 ai +A(n− 1) n−2∑ i=1 ai +B(n− 1) ) +B(n)\n= A(n)(1 +A(n− 1)) n−2∑ i=1 ai +B(n) +A(n)B(n− 1)\n≤ A(n)(1 +A(n− 1)) ( n−3∑ i=1 ai +A(n− 2) n−3∑ i=1 ai +B(n− 2) ) + +B(n) +A(n)B(n− 1)\n= A(n)(1 +A(n− 1))(1 +A(n− 2)) n−3∑ i=1 ai +B(n) +A(n)B(n− 1) +A(n)(1 +A(n− 1))B(n− 2)\nContinuing so upto m steps we get\nan ≤ A(n) ( m−1∏ i=1 (1 +A(n− i)) ) n−m∑ i=1 ai +B(n) +A(n) m−1∑ i=1 i−1∏ j=1 (1 +A(n− j)) B(n− i)  (17)\nWe would now like to bound in general the term ∏m−1 i=1 (1 +A(n− i)). To this extant note that,\nm−1∏ i=1 (1 +A(n− i)) = exp ( m−1∑ i=1 log(1 +A(n− i)) )\nNow assume A(i) ≤ 1/2 for all i ≥ n−m− 1 so that log(1 +A(n− i)) ≤ A(n− i). We get\nm−1∏ i=1 (1 +A(n− i)) ≤ exp ( m−1∑ i=1 A(n− i) )\nNow if ∑n i=n−m−1A(i) ≤ 1 then we can conclude that\nm−1∏ i=1 (1 +A(n− i)) ≤ e\nPlugging this in Equation B.3 we get\nan ≤ eA(n) ( n−m∑ i=1 ai + m−1∑ i=1 B(n− i) ) +B(n)\n= eA(n) ( n−m∑ i=1 ai + n∑ i=n−m−1 B(i) ) +B(n)\nNow if for each i ≤ n, ai ≤ a0 then we see that\nan ≤ eA(n) ( a0(n−m) +\nn∑ i=n−m−1 B(i)\n) +B(n)\nHence we conclude that as long as ∑n i=n−m−1A(i) ≤ 1\nan ≤ eA(n) ( a0(n−m) +\nn∑ i=n−m−1 B(i)\n) +B(n)"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "<lb>Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization<lb>problems. We study how such algorithms can be improved using accelerated gradient methods. We<lb>provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to<lb>obtain a significant speed-up and propose a novel accelerated gradient algorithm, which deals with this<lb>deficiency, enjoys a uniformly superior guarantee and works well in practice.",
    "creator" : "LaTeX with hyperref package"
  }
}