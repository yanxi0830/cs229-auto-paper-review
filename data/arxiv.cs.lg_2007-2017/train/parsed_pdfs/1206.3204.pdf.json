{
  "name" : "1206.3204.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improved Spectral-Norm Bounds for Clustering",
    "authors" : [ "Pranjal Awasthi" ],
    "emails" : [ "pawasthi@cs.cmu.edu", "osheffet@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 6.\n32 04\nv1 [\ncs .L\nG ]\n1 4\nJu n\n20 12\nIn this paper we improve upon the work of Kumar and Kannan [KK10] along several axes. First, we weaken the center separation bound by a factor of √ k, and secondly we weaken the proximity condition by a factor of k (in other words, the revised separation condition is independent of k). Using these weaker bounds we still achieve the same guarantees when all points satisfy the proximity condition. Under the same weaker bounds, we achieve even better guarantees when only (1−ǫ)-fraction of the points satisfy the condition. Specifically, we correctly cluster all but a (ǫ + O(1/c4))-fraction of the points, compared to O(k2ǫ)-fraction of [KK10], which is meaningful even in the particular setting when ǫ is a constant and k = ω(1). Most importantly, we greatly simplify the analysis of Kumar and Kannan. In fact, in the bulk of our analysis we ignore the proximity condition and use only center separation, along with the simple triangle and Markov inequalities. Yet these basic tools suffice to produce a clustering which (i) is correct on all but a constant fraction of the points, (ii) has k-means cost comparable to the k-means cost of the target clustering, and (iii) has centers very close to the target centers.\nOur improved separation condition allows us to match the results of the Planted Partition Model of McSherry [McS01], improve upon the results of Ostrovsky et al [ORSS06], and improve separation results for mixture of Gaussian models in a particular setting."
    }, {
      "heading" : "1 Introduction",
      "text" : "In the long-studied field of clustering, there has been substantial work [Das99, DS07, SK01, VW02, AM05, CR08b, KSV08, DHKM07, BV08] studying the problem of clustering data from mixture of distributions under the assumption that the means of the distributions are sufficiently far apart. Each of these works focuses on one particular type (or family) of distribution, and devise an algorithm that successfully clusters datasets that come from that particular type. Typically, they show that w.h.p. such datasets have certain nice properties, then use these properties in the construction of the clustering algorithm.\nThe recent work of Kumar and Kannan [KK10] takes the opposite approach. First, they define a separation condition, deterministic and thus not tied to any distribution, and show that any set of data points satisfying this condition can be successfully clustered. Having established that, they show that many previously studied clustering problems indeed satisfy (w.h.p) this separation condition. These clustering problems include Gaussian mixture-models, the Planted Partition model of McSherry [McS01] and the work of Ostrovsky et al [ORSS06]. In this aspect they aim to unify the existing body of work on clustering under separation assumptions, proving that one algorithm applies in multiple scenarios.1\nHowever, the attempt to unify multiple clustering works is only successful in part. First, Kumar and Kannan’s analysis is “wasteful” w.r.t the number of clusters k. Clearly, motivated by an underlying assumption that k is constant, their separation bound has linear dependence in k and their classification guarantee has quadratic dependence on k. As a result, Kumar and Kannan overshoot best known bounds for the Planted Partition Model and for mixture of Gaussians by a factor of √ k. Similarly, the application to datasets considered by Ostrovsky et al only holds for constant k. Secondly, the analysis in Kumar-Kannan is far from simple – it relies on most points being “good”, and requires multiple iterations of Lloyd steps before converging to good centers. Our work addresses these issues.\nTo formally define the separation condition of [KK10], we require some notation. Our input consists of n points in Rd. We view our dataset as a n × d matrix, A, where each datapoint corresponds to a row Ai in this matrix. We assume the existence of a target partition, T1, T2, . . . , Tk, where each cluster’s center is µr =\n1 nr\n∑\ni∈Tr Ai, where nr = |Tr|. Thus, the target clustering is represented by a n×d matrix of cluster centers, C, where Ci = µr iff i ∈ Tr. Therefore, the k-means cost of this partition is the squared Frobenius norm ‖A − C‖2F , but the focus of this paper is on the spectral (L2) norm of the matrix A − C. Indeed, the deterministic equivalent of the maximal variance in any direction is, by definition, 1n‖A− C‖2 = max{v: ‖v‖=1} 1n‖(A −C)v‖2.\nDefinition. Fix i ∈ Tr. We say a datapoint Ai satisfies the Kumar-Kannan proximity condition if for any s 6= r, when projecting Ai onto the line connecting µr and µs, the projection of Ai is closer to µr than to µs by an additive factor of Ω (\nk( 1√nr + 1√ ns )‖A− C‖\n)\n.\nKumar and Kannan proved that if all but at most ǫ-fraction of the data points satisfy the proximity condition, they can find a clustering which is correct on all but an O(k2ǫ)-fraction of the points. In particular, when ǫ = 0, their algorithm clusters all points correctly. Observe, the Kumar-Kannan proximity condition gives that the distance ‖µr −µs‖ is also bigger than the above\n1We comment that, implicitly, Achlioptas and McSherry [AM05] follow a similar approach, yet they focus only on mixtures of Gaussians and log-concave distributions. Another deterministic condition for clustering was considered by [CO10], which generalized the Planted Partition Model of [McS01].\nmentioned bound. The opposite also holds – one can show that if ‖µr − µs‖ is greater than this bound then only few of the points do not satisfy the proximity condition."
    }, {
      "heading" : "1.1 Our Contribution",
      "text" : "Our Separation Condition. In this work, the bulk of our analysis is based on the following quantitatively weaker version of the proximity condition, which we call center separation. Formally, we define ∆r =\n1√ nr\nmin{ √ k‖A − C‖, ‖A − C‖F } and we assume throughout the paper that for a\nlarge constant2 c we have that the means of any two clusters Tr and Ts satisfy\n‖µr − µs‖ ≥ c(∆r +∆s) (1)\nObserve that this is a simpler version of the Kumar-Kannan proximity condition, scaled down by a factor of √ k. Even though we show that (1) gives that only a few points do not satisfy the proximity condition, our analysis (for the most part) does not partition the dataset into good and bad points, based on satisfying or non-satisfying the proximity condition. Instead, our analysis relies on basic tools, such as the Markov inequality and the triangle inequality. In that sense one can view our work as “aligning” Kumar and Kannan’s work with the rest of clustering-under-center-separation literature – we show that the bulk of Kannan and Kumar’s analysis can be simplified to rely merely on center-separation.\nOur results. We improve upon the results of [KK10] along several axes. In addition to the weaker condition of Equation (1), we also weaken the Kumar-Kannan proximity condition by a factor of k, and still retrieve the target clustering, if all points satisfy the (k-weaker) proximity condition. Secondly, if at most ǫn points do not satisfy the k-weaker proximity condition, we show that we can correctly classify all but a (ǫ+O(1/c4))-fraction of the points, improving over the bound of [KK10] of O(k2ǫ). Note that our bound is meaningful even if ǫ is a constant whereas k = ω(1). Furthermore, we prove that the k-means cost of the clustering we output is a (1 +O(1/c))-approximation of the k-means cost of the target clustering.\nOnce we have improved on the main theorem of Kumar and Kannan, we derive immediate improvements on its applications. In Section 3.1 we show our analysis subsumes the work of Ostrovsky et al [ORSS06], and applies also to non-constant k. Using the fact that Equation (1) “shaves off” a√ k factor from the separation condition of Kumar and Kannan, we obtain a separation condition of Ω(σmax √ k) for learning a mixture of Gaussians, and we also match the separation results of the Planted Partition model of McSherry [McS01]. These results are described in Section 5. From an approximation-algorithms perspective, it is clear why the case of k = ω(1) is of interest, considering the ubiquity of k-partition problems in TCS (e.g., k-Median, Max k-coverage, Knapsack for k items, maximizing social welfare in k-items auction – all trivially simple for constant k). In addition, we comment that in our setting only the case where k = ω(1) is of interest, since otherwise one can approximate the k-means cost using the PTAS of Kumar et al [KSS04], which\n2We comment that throughout the paper, and much like Kumar and Kannan, we think of c as a large constant (c = 100 will do). However, our results also hold when c = ω(1), allowing for a (1 + o(1))-approximation. We also comment that we think of d ≫ k, so one should expect ‖A−C‖2F ≥ k‖A−C‖2 to hold, thus the reader should think of ∆r as dependent on √ k‖A−C‖. Still, including the degenerate case, where ‖A−C‖2F < k‖A−C‖, simplifies our analysis in Section 3. One final comment is that (much like all the work in this field) we assume k is given, as part of the input, and not unknown.\ndoesn’t even require any separation assumptions. From a practical point of view, there is a variety of applications where k is quite large. This includes problems such as clustering images by who is in them, clustering protein sequences by families of organisms, and problems such as deduplication where multiple databases are combined and entries corresponding to the same true entity are to be clustered together [CR02, MBHC95]. The challenges that arise from treating k as a non-constant are detailed in the proofs overview (Section 1.4).\nTo formally detail our results, we first define some notations and discuss a few preliminary facts."
    }, {
      "heading" : "1.2 Notations and Preliminaries",
      "text" : "The Frobenius norm of a n ×m matrix M , denoted as ‖M‖F is defined as ‖M‖F = √ ∑ i,j M 2 i,j . The spectral norm of M is defined as ‖M‖ = maxx:‖x‖=1 ‖Mx‖. It is a well known fact that if the rank of M is t, then ‖M‖2F ≤ t‖M‖2. The Singular Value Decomposition (SVD) of M is a decomposition of M as M = UΣV T , where U is a n × n unitary matrix, V is a m × m unitary matrix, Σ is a n×m diagonal matrix whose entries are nonnegative real numbers, and its diagonal entries satisfy σ1 ≥ σ2 ≥ . . . ≥ σmin{m,n}. The diagonal entries in Σ are called the singular values of M , and the columns of U and V , denoted ui and vi resp., are called the left- and right-singular vectors. As a convention, when referring to singular vectors, we mean the right-singular vectors. Observe that the Singular Value Decomposition allows us to writeM = ∑rank(Σ)\ni=1 σiuiv T i . Projecting\nM onto its top t singular vectors means taking M̂ = ∑t i=1 σiuiv T i . It is a known fact that for any t, the t-dimensional subspace which best fits the rows of M , is obtained by projecting M onto the subspace spanned by the top t singular vectors (corresponding to the top t singular values). Another way to phrase this result is by saying that M̂ = argminN :rank(N)=t{‖M − N‖F }. For a proof, see [KV09]. The same matrix, M̂ , also minimizes the spectral norm of this difference, meaning M̂ = argminN :rank(N)=t{‖M −N‖} (see [GVL96] for proof).\nAs previously defined, ‖A− C‖ denotes the spectral norm of A−C. The target clustering, T , is composed of k clusters T1, T2, . . . , Tk. Observe that we use µ as an operator, where for every set X, we have µ(X) = 1|X| ∑ i∈X Ai. We abbreviate, and denote µr = µ(Tr). From this point on, we denote the projection of A onto the subspace spanned by its top k-singular vectors as Â, and for any vector v, we denote v̂ as the projection of v onto this subspace. Throughout the paper, we abuse notation and use i to iterate over the rows of A, whereas r and s are used to iterate over clusters (or submatrices). So Ai represents the ith row of A whereas Ar represents the submatrix [Ai]{i∈Tr}.\nBasic Facts. The analysis of our main theorem makes use of the following facts, from [McS01, KV09, KK10]. We advise the reader to go over the proofs, which are short, elegant, and provided in Appendix A. The first fact bounds the cost of assigning the points of Â to their original centers.\nFact 1.1 (Lemma 9 from [McS01]). ‖Â−C‖2F ≤ 8min{k‖A−C‖2, ‖A−C‖2F } ( = 8nr∆ 2 r for every r ) .\nNext, we show that we can match each target center µr to a unique, relatively close, center νr that we get in Part I of the algorithm.\nFact 1.2 (Claim 1 in Section 3.2 of [KV09]). For every µr there exists a center νs s.t. ‖µr − νs‖ ≤ 6∆r, so we can match each µr to a unique νr.\nFinally, we exhibit the following fact, which is detailed in the analysis of [KK10].\nFact 1.3. Fix a target cluster Tr and let Sr be a set of points created by removing ρoutnr points from Tr and adding ρin(s)nr points from each cluster s 6= r, s.t. every added point x satisfies ‖x− µs‖ ≥ 23‖x− µr‖. Assume ρout < 14 and ρin def = ∑ s 6=r ρin(s) < 1 4 . Then\n‖µ(Sr)− µr‖ ≤ 1√ nr\n\n √ ρout + 3 2 ∑\ns 6=r\n√\nρin(s)\n  ‖A− C‖ ≤ (√\nρout nr\n+ 32 √ k √\nρin nr\n)\n‖A− C‖"
    }, {
      "heading" : "1.3 Formal Description of the Algorithm and Our Theorems",
      "text" : "Having established notation, we now present our algorithm, in Figure 1. Our algorithm’s goal is three fold: (a) to find a partition that identifies with the target clustering on the majority of the points, (b) to have the k-means cost of this partition comparable with the target, and (c) output k centers which are close to the true centers. It is partitioned into 3 parts. Each part requires stronger assumptions, allowing us to prove stronger guarantees.\n• Assuming only the center separation of (1), then Part I gives a clustering which (a) is correct on at least 1−O(c−2) fraction of the points from each target cluster (Theorem 3.1), and (b) has k-means cost smaller than (1 +O(1/c))‖A − C‖2F (Theorem 3.2). • Assuming also that ∆r = √ k√ nr ‖A − C‖, i.e. assuming the non-degenerate case where ‖A −\nC‖2F ≥ k‖A−C‖2, then Part II finds centers that are O(1/c) ‖A−C‖√ nr close to the true centers (Theorem 4.1). As a result (see Section 4.1), if (1− ǫ)n points satisfy the proximity condition (weakened by a k factor,), then we misclassify no more than (ǫ+O(c−4))n points.\n• Assuming all points satisfy the proximity condition (weakened by a k-factor), Part III finds exactly the target partition (Theorem 4.8)."
    }, {
      "heading" : "1.4 Organization and Proofs Overview",
      "text" : "Organization. Related work is detailed in Section 2. The analysis of Part I of our algorithms is in Section 3. Part I is enough for us to give a “one-line” proof in Section 3.1 showing how the work of Ostrovsky et al falls into our framework. The analysis of Part II of the algorithm is in Section 4. The improved guarantees we get by applying the algorithm to the Planted Partition model and to the Gaussian mixture model are discussed in Section 5. We conclude with an open problem in Section 6.\nProof outline for Section 3. The first part of our analysis is an immediate application of Facts 1.1 and 1.2. Our assumption dictates that the distance between any two centers is big (≥ c(∆r +∆s)). Part I of the algorithm assigns each projected point Âi to the nearest νr instead of the true center µr and Fact 1.2 assures that the distance ‖µr − νr‖ is small (< 6∆r). Consider a misclassified point Ai, where ‖Ai − µr‖ < ‖Ai − µs‖ yet ‖Âi − νs‖ < ‖Âi − νr‖. The triangle inequality assures that Âi has a fairly big distance to its true center (> ( c 2−12)∆r). We deduce that each misclassified point contributes Ω(c2∆2r) to the k-means cost of assigning all projected points to their true centers. Fact 1.1 bounds this cost by ‖Â−C‖2F ≤ 8nr∆2r , so the Markov inequality proves only a few points are misclassified. Additional application of the triangle inequality for misclassified points gives that the distance between the original point Ai and a true center µr is comparable to the distance ‖Ai − µs‖, and so assigning Ai to the cluster s only increases the k-means cost by a small factor.\nProof outline for Section 4. In the second part of our analysis we compare between the true clustering T and some proposed clustering S, looking both at the number of misclassified points and at the distances between the matching centers ‖µr − θr‖. As Kumar and Kannan show, the two measurements are related: Fact 1.3 shows how the distances between the means depend on the number of misclassified points, and the main lemma (Lemma 4.5) essentially shows the opposite direction. These two relations are how Kumar and Kannan show that Lloyd steps converge to good centers, yielding clusters with few misclassified points. They repeatedly apply (their version of) the main lemma, showing that with each step the distances to the true means decrease and so fewer of the good points are misclassified.\nTo improve on Kumar and Kannan analysis, we improve on the two above-mentioned relations. Lemma 4.5 is a simplification of a lemma from Kumar and Kannan, where instead of projecting into a k-dimensional space, we project only into a 4-dimensional space, thus reducing dependency on k. However, the dependency of Fact 1.3 on k is tight3. So in Part II of the algorithm we devise sub-clusters Sr s.t. ρin(s) = ρout/k\n2. The crux in devising Sr lies in Proposition 4.4 – we show that any misclassified projected point i ∈ Ts ∩ Sr is essentially misclassified by µ̂r. And since (see [AM05]) ‖µr − µ̂r‖ ≤ 1√k∆r (compared to the bound ‖µr − νr‖ ≤ 6∆r), we are able to give a good bound on ρin(s).\nRecall that we rely only on center separation rather than a large batch of points satisfying the Kumar-Kannan separation, and so we do not apply iterative Lloyd steps (unless all points are good). Instead, we apply the main lemma only once, w.r.t to the misclassified points in Ts∩Sr, and deduce that the distances ‖µr − θr‖ are small. In other words, Part II is a single step that retrieve\n3In fact, Fact 1.3 is exactly why the case of k = ω(1) is hard – because the L1 and L2 norms of the vector ( 1√\nk , 1√ k , . . . , 1√ k ) are not comparable for non-constant k.\ncenters whose distances to the original centers are √ k-times better than the centers retrieved by Kumar and Kannan in numerous Lloyd iterations."
    }, {
      "heading" : "2 Related Work",
      "text" : "The work of [Das99] was the first to give theoretical guarantees for the problem of learning a mixture of Gaussians under separation conditions. He showed that one can learn a mixture of k spherical Gaussians provided that the separation between the cluster means is Ω̃( √ n(σr +σs)) and the mixing weights are not too small. Here σ2r denotes the maximum variance of cluster r along any direction. This separation was improved to Ω̃((σr + σs)n\n1/4) by [DS07]. Arora and Kannan [SK01] extended these results to the case of general Gaussians. For the case of spherical Gaussians, [VW02] showed that one can learn under a much weaker separation of Ω̃((σr + σs)k\n1/4). This was extended to arbitrary Gaussians by [AM05] and to various other distributions by [KSV08], although requiring a larger separation. In particular, the work of [AM05] requires a separation of Ω((σr + σs)( 1√\nmin(wr ,ws) + √ k log(kmin{2k, n}))) whereas [KSV08] require a separation of Ω̃( k3/2 w2min (σr+σs)).\nHere wr’s refer to the mixing weights. [CR08a, CR08b] gave algorithms for clustering mixtures of product distributions and mixtures of heavy tailed distributions. [BV08] gave an algorithm for clustering the mixture of 2 Gaussians assuming only that the two Gaussians are separated by a hyperplane. They also give results for learning a mixture of k > 2 Gaussians. The work of [KMV10] gave an algorithm for learning a mixture of 2 Gaussians, with provably minimal assumptions. This was extended in [MV10] to the case when k > 2 although the algorithm runs in time exponential in k. Similar results were obtained in the work of [BS10] who can also learn more general distribution families. The work of [CO10] studied a deterministic separation condition required for efficient clustering. The precise condition presented in [CO10] is technical but essentially assumes that the underlying graph over the set of points has a “low rank structure” and presents an algorithm to recover this structure which is then enough to cluster well. In addition, previous works (e.g. [Sch00, BBG09]) addressed the problem of clustering from the viewpoint of minimizing the number of mislabeled points.\nThere has been an extensive line of work on approximation algorithms for the k-means problem ([OR00, BHPI02, dlVKKR03, ES04, HPM04, KMN+02]). The current best guarantee is a (9 + ǫ)-approximation algorithm of [KMN+02] (with a much simpler analysis in [GT08]) if polynomial dependence on k and the dimension d is desired.4 Another popular algorithm for k-means is the Lloyd’s heuristics ([Llo82]). This heuristics, combined with a careful seeding of centers, has been shown to have good performance if the data is well separated (see [ORSS06]), or to provide O(log(k))-approximation in general [AV07]. The separation-based results of [ORSS06] were improved by [ABS10]."
    }, {
      "heading" : "3 Part I of the Algorithm",
      "text" : "In this section, we look only at Part I of our algorithm. Our approximation algorithm defines a clustering Z, where Zr = {i : ‖Âi − νr‖ ≤ ‖Âi − νs‖ for every s}. Our goal in this section is to show that Z is correct on all but a small constant fraction of the points, and furthermore, the k-means cost of Z is no more than (1 +O(1/c)) times the k-means cost of the target clustering.\n4For constant k, [KSS04] give a PTAS for the k-means problem.\nTheorem 3.1. There exists a matching (given by Fact 1.2) between the target clustering T and the clustering Z = {Zr}r where Zr = {i : ‖Âi − νr‖ ≤ ‖Âi − νs‖ for every s} that satisfies the following properties:\n• For every cluster Ts0 in the target clustering, no more than O(1/c2)|Ts0 | points are misclassified.\n• For every cluster Zr0 in the clustering that the algorithm outputs, we add no more than O(1/c2)|Tr0 | points from other clusters.\n• At most O(1/c2)|Tr2 | points are misclassified overall, where Tr2 is the second largest cluster.\nProof. Let us denote Ts→r as the set of points Âi that are assigned to Ts in the target clustering, yet are closer to νr than to any other ν ′ r. From triangle inequality we have that ‖Âi − µs‖ ≥ ‖Âi − νs‖ − ‖µs − νs‖. We know from Fact 1.2 that ‖µs − νs‖ ≤ 6∆s. Also, since Âi is closer to νr than to νs, the triangle inequality gives that 2‖Âi − νs‖ ≥ ‖νr − νs|. So,\n‖Âi − µs‖ ≥ 1\n2 ‖νr − νs‖ − 6∆s ≥\n1 2 ‖µr − µs‖ − 12(∆r +∆s) ≥ c 4 (∆r +∆s)\nThus, we can look at ‖Â− C‖2F , and using Fact 1.1 we immediately have that for every fixed r′\n∑\nr\n∑ s 6=r |Ts→r|\nc2 16 (∆r +∆s) 2 ≤ ∑\nr\n∑ i∈Tr ‖Âi − µr‖2 = ‖Â− C‖2F ≤ 8nr′∆2r′\nThe proof of the theorem follows from fixing some r0 or some s0 and deducing:\n∆2s0\n∑ r 6=s0 |Ts0→r| ≤ ∑ r 6=s0 |Ts0→r|(∆r +∆s0)2 ≤ ∑ r ∑ s 6=r |Ts→r|(∆r +∆s)2 ≤\n128\nc2 ns0∆\n2 s0\n∆2r0\n∑ s 6=r0 |Ts→r0 | ≤ ∑ s 6=r0 |Ts→r0 |(∆r0 +∆s)2 ≤ ∑ r ∑ s 6=r |Ts→r|(∆r +∆s)2 ≤\n128\nc2 nr0∆\n2 r0\nObserve that for every r 6= s we have that ∆r +∆s ≥ ∆r2 (where r2 is the cluster with the second largest number of points), so we have that\n∆2r2\n∑\nr\n∑ s 6=r |Ts→r| ≤ ∑ r ∑ s 6=r |Ts→r|(∆r +∆s)2 ≤\n128\nc2 nr2∆\n2 r2\nWe now show that the k-means cost of Z is close to the k-means cost of T . Observe that the k-means cost of Z is computed w.r.t the best center of each cluster (i.e., µ(Zr)), and not w.r.t the centers νr.\nTheorem 3.2. The k-means cost of Z is at most (1 +O(1/c))‖A − C‖2F .\nProof. Given Z, it is clear that the centers that minimize its k-means cost are µ(Zr) = 1|Zr| ∑\ni∈Zr Ai. Recall that the majority of points in each Zr belong to a unique Tr, and so, throughout this section, we assume that all points in Zr were assigned to µr, and not to µ(Zr). (Clearly, this can only increase the cost.) We show that by assigning the points of Zr to µr, our cost is at most (1+O(1/c))‖A−C‖2F , and so Theorem 3.2 follows. In fact, we show something stronger. We show\nthat by assigning all the points in Zr to µr, each point Ai pays no more than (1+O(1/c))‖Ai−Ci‖2. This is clearly true for all the points in Zr∩Tr. We show this also holds for the misclassified points.\nBecause i ∈ Ts→r, it holds that ‖Âi − νr‖ ≤ ‖Âi − νs‖. Observe that for every s we have that ‖Ai−νs‖2 = ‖Ai− Âi‖2+‖Âi−νs‖2, because Âi−νs is the projection of Ai−νs onto the subspace spanned by the top k-singular vectors of A. Therefore, it is also true that ‖Ai − νr‖ ≤ ‖Ai − νs‖. Because of Fact 1.2, we have that ‖µr − νr‖ ≤ 6∆r and ‖µs − νs‖ ≤ 6∆s, so we apply the triangle inequality and get\n‖Ai − µr‖ ≤ ‖Ai − µs‖+ ‖µr − νr‖+ ‖µs − νs‖ ≤ ‖Ai − µs‖ ( 1 + 6(∆r +∆s)\n‖Ai − µs‖\n)\nSo all we need to do is to lower bound ‖Ai − µs‖. As noted, ‖Ai − νs‖ ≥ ‖Âi − νs‖. Thus\n‖Ai − µs‖ ≥ ‖Ai − νs‖ − 6∆r ≥ ‖Âi − νs‖ − 6∆r ≥ 1\n2 ‖νs − νr‖ − 6∆r ≥\n1 4 c(∆r +∆s)\nand we have the bound ‖Ai − µr‖ ≤ ( 1 + 24c ) ‖Ai − µs‖, so ‖Ai − µr‖2 ≤ ( 1 + 49c ) ‖Ai − µs‖2."
    }, {
      "heading" : "3.1 Application: The ORSS-Separation",
      "text" : "One straight-forward application of Theorem 3.2 is for the datasets considered by Ostrovsky et al [ORSS06], where the optimal k-means cost is an ǫ-fraction of the optimal (k − 1)-means cost. Ostrovsky et al proved that for such datasets a variant of the Lloyd method converges to a good solution in polynomial time. Kumar and Kannan have shown that datasets satisfying the ORSSseparation, also have the property that most points satisfy their proximity-condition. Their analysis is not immediate, and gives a (1 + O( √ kǫ))-approximation. Here, we provide a “one-line” proof that Part I of Algorithm ∼Cluster yields a (1 +O(√ǫ))-approximation, for any k. Suppose we have a dataset satisfying the ORSS-separation condition, so any (k − 1)-partition of the dataset have cost ≥ 1ǫ‖A − C‖2F . For any r and any s 6= r, by assigning all the points in Tr to the center µs, we get some (k − 1)-partition whose cost is exactly ‖A−C‖2F + nr‖µr − µs‖2, so ‖µr − µs‖ ≥ √ 1 ǫ −1\n√ nr\n‖A− C‖F . Setting c = O(1/ √ ǫ), Theorem 3.2 is immediate."
    }, {
      "heading" : "4 Part II of the Algorithm",
      "text" : "In this section, our goal is to show that Part II of our algorithm gives centers that are very close to the target clusters. We should note that from this point on, we assume we are in the non-degenerate case, where ‖A− C‖2F ≥ k‖A− C‖2. Therefore, ∆r = √ k√ nr ‖A− C‖.\nRecall, in Part II we define the sets Sr = {i : ‖Âi − νr‖ ≤ 13‖Âi − νs‖, ∀s 6= r}. Observe, these set do not define a partition of the dataset! There are some points that are not assigned to any Sr. However, we only use the centers of Sr. We prove the following theorem.\nTheorem 4.1. Denote Sr = {i : ‖Âi − νr‖ ≤ 13‖Âi − νs‖, ∀s 6= r}. Then for every r it holds that ‖µ(Sr)− µr‖ = O(1/c) 1√nr ‖A−C‖ = O( 1 c √ k ∆r).\nThe proof of Theorem 4.1 is an immediate application of Fact 1.3 combined with the following two lemmas, that bound the number of misclassified points. Observe that for every point that\nbelongs to Ts yet is assigned to Sr (for s 6= r) is also assigned to Zr in the clustering Z discussed in the previous section. Therefore, any misclassified point i ∈ Ts ∩ Sr satisfies that ‖Ai − µr‖ ≤ (1 +O(c−1))‖Ai − µs‖ as the proof of Theorem 3.2 shows. So all conditions of Fact 1.3 hold. Lemma 4.2. Assume that for every r we have that ‖µr − νr‖ ≤ 6∆r. Then at most 512c2 nr points of Tr do not belong to Sr.\nLemma 4.3. Redefine Ts→r as the set Ts ∩ Sr. Assume that for every r we have that ‖µr − νr‖ ≤ 6∆r. Then for every r and every s 6= r we have that |Ts→r| = ( 482\nc4k2\n)\nnr.\nProof of Lemma 4.2. First, we claim that if i is such that ‖Âi − µr‖ ≤ c8∆r, then it must be the case that i ∈ Sr.\nThis is a simple consequence of the triangle inequality, bounding ‖Âi − νr‖ ≤ ‖Âi − µr‖ + ‖µr − νr‖ ≤ ((c/8) + 6)∆r. Yet, for every s 6= r, the triangle inequality gives that ‖Âi − νs‖ ≥ ‖µr − µs‖ − ‖Âi − µr‖ − ‖µs − νs‖ ≥ (c − c8 − 6)(∆r + ∆s). Assuming c > 48, we have that ‖Âi − νs‖ ≥ 3‖Âi − νr‖.\nAll that’s left is to show that the number of i ∈ Tr s.t. ‖Âi − µr‖ > c8∆r is small. This again follows from the Markov inequality: Since ‖Â−C‖2F ≤ 8k‖A−C‖2, then the number of such points is at most 8k‖A−C‖ 2\n(c2/64)k‖A−C‖2nr.\nWe now turn to proving Lemma 4.3. The general outline of the proof of Lemma 4.3 resembles to the outline of the proof of Lemma 4.2. Proposition 4.4 exhibit some property that every point in Ts→r must satisfy, and then we show that only few of the points in Ts satisfy this property. Recall that µ̂r indicates the projection of µr onto the subspace spanned by the top k-singular vectors of A. Proposition 4.4. Fix i ∈ Ts s.t. ‖Âi− µ̂s‖ ≤ 2‖Âi− µ̂r‖. Then ‖Âi−νs‖ < 3‖Âi−νr‖, so i /∈ Sr. Proof. First, for every r we have that ‖µ̂r − νr‖ ≤ ‖µr − νr‖ ≤ 6∆r, as µ̂r − νr is a projection of µr − νr.\nLet us fiddle with the triangle inequality, in order to obtain a lower bound on ‖Âi − νr‖. We have that 3‖Âi− µ̂r‖ ≥ ‖µ̂r− µ̂s‖ ≥ ‖µr−µs‖− ( ‖µr−νr‖+‖νr− µ̂r‖ ) − ( ‖µs−νs‖+‖νs− µ̂s‖ )\n≥ (c− 12)(∆r +∆s), thus ‖Âi − νr‖ ≥ ( c−12 3 − 6 ) (∆r +∆s).\nAssume for the sake of contradiction that ‖Âi − νs‖ ≥ 3‖Âi − νr‖, and let us show this yields an upper bound on ‖Âi − νr‖, which contradicts our lower bound. We have that\n6∆s ≥ ‖Âi − νs‖ − ‖Âi − µ̂s‖ ≥ 3‖Âi − νr‖ − 2‖Âi − µ̂r‖ ≥ ‖Âi − νr‖ − 2 · 6∆r It follows that 12(∆r +∆s) ≥ ‖Âi − νr‖ ≥ ( c−12 3 − 6 ) (∆r +∆s). Contradiction (c > 60).\nProposition 4.4, shows that in order to bound |Ts→r| it suffices to bound the number of points in Ts satisfying ‖Âi − µ̂s‖ ≥ 2‖Âi − µ̂r‖. The major tool in providing this bound is the following technical lemma. This lemma is a variation on the work of [KK10], on which we improve on the dependency on k and simplify the proof.\nLemma 4.5 (Main Lemma). Fix α, β > 0. Fix r 6= s and let ζr and ζs be two points s.t. ‖µr − ζr‖ ≤ α∆r and ‖µs − ζs‖ ≤ α∆s. We denote Ãi as the projection of Ai onto the line connecting ζr and ζs. Define X = { i ∈ Ts : ‖Ãi − ζs‖ − ‖Ãi − ζr‖ ≥ β‖ζs − ζr‖ }\n. Then |X| ≤ 256α 2\nβ2 1 c4k\n( min {nr, ns} ) .\nProof. Let V be the subspace spanned by the following 4 vectors: {µr, µs, ζr, ζs}. Denote PV as the projection onto V. We denote vi = PV(Ai), and observe that PV(µr) = µr, and the same goes for µs, ζr and ζs. Observe also that, as a projection, ‖PV(A−C)‖ ≤ ‖A−C‖ (alternatively, ‖PV‖ = 1).\nWe now make a simple observation. Let Āi denote the projection of Ai onto the line connecting µr and µs. Now, the inequality ‖Ai−µs‖ < ‖Ai−µr‖ holds iff the inequality ‖Āi−µs‖ ≤ ‖Āi−µr‖ holds (because ‖Ai − µr‖2 = ‖Ai − Āi‖2 + ‖Āi − µr‖2). Furthermore, such relation holds for any point whose projection on the line connecting µr and µs is identical to Āi. In particular, if W is any subspace containing µr and µs, then the projection of Ai onto W is closer to µr than to µs iff Ai is closer to µr than to µs. Thus, since ‖Ai − µs‖ ≤ ‖Ai − µr‖ then ‖vi − µs‖ ≤ ‖vi − µr‖. Furthermore, as ζr and ζs also belong to V, then the projection of Ai onto the line connecting ζs and ζr is identical to the projection of vi onto the same line (meaning, Ãi = ṽi). So vi also satisfies the inequality: ‖ṽi− ζs‖−‖ṽi− ζr‖ ≥ β‖ζs− ζr‖, and, of course, ‖vi− ζr‖2 = ‖vi− ṽi‖2+‖ṽi− ζr‖2.\nThe proof follows from upper- and lower-bounding the term ‖vi − ζs‖2 −‖vi − ζr‖2. We’ve just shown a lower bound, as we have that\n‖vi − ζs‖2 − ‖vi − ζr‖2 = (‖ṽi − ζs‖ − ‖ṽi − ζr‖) (‖ṽi − ζs‖+ ‖ṽi − ζr‖) ≥ β‖ζs − ζr‖2\nThe triangle inequality gives that ‖vi − ζs‖ ≤ ‖vi − µs‖ + α(∆r + ∆s), and that ‖vi − ζr‖ ≥ ‖vi − µr‖ − α(∆r +∆s), so we have the upper bound of\n‖vi − ζs‖2 − ‖vi − ζr‖2 ≤ (‖vi − µs‖+ α(∆r +∆s))2 − (‖vi − µr‖ − α(∆r +∆s))2\n≤ (‖vi − µr‖+ α(∆r +∆s))2 − (‖vi − µr‖ − α(∆r +∆s))2 ≤ 4α(∆r +∆s)‖vi − µr‖\nComparing the upper and the lower bound, we have that for any i ∈ X the distance ‖vi−µr‖ ≥ β 4α (c−α)2(∆r+∆s)2 ∆r+∆s . As X ⊂ Ts, the Markov inequality concludes the proof\n|X| ( c2\n8\nβ\nα\n√ k‖A− C‖ )2 1\nmin{nr, ns} ≤\n∑ i∈Ts ‖vi − µs‖2 ≤ ‖PV(A− C)‖2F ≤ 4‖A− C‖2\nProof of Lemma 4.3. Every i ∈ Ts→r must satisfy that ‖Âi−µ̂s‖ ≥ 2‖Âi−µ̂r‖ (Proposition 4.4). Therefore, we must have that ‖Ãi − µ̂s‖ ≥ 2‖Ãi − µ̂r‖, where we denote Ãi as the projection of A onto the line connecting µ̂r with µ̂s (simply because ‖Âi − µ̂s‖2 = ‖Âi − Ãi‖2 + ‖Ãi − µ̂s‖2.) Therefore, ‖µ̂r − µ̂s‖ ≤ 32‖Ãi − µ̂s‖, so ‖Ãi − µ̂s‖ − ‖Ãi − µ̂r‖ > 13‖µ̂r − µ̂s‖.\nThus, every i ∈ Ts→r satisfies the conditions of Lemma 4.5 with ζr = µ̂r, ζs = µ̂s, and β = 1/3. We deduce the |Ts→r| ≤ α2 256·9c4k min{nr, ns}, where α is the bound s.t. for every r, ‖µr − µ̂r‖ ≤ α\n√ k√ nr ‖A−C‖. Since α ≤ 1√ k , we conclude the proof.\nThe fact that α is small was proven by Achlioptas and McSherry (Theorem 1 of [AM05]). Denote ur as the indicator vector of Tr. Since rank(C) ≤ k, we get\n‖µr − µ̂r‖ = 1\nnr ‖(A − Â)Tur‖ ≤\n1\nnr ‖ur‖ ‖A− Â‖ ≤ 1√ nr ‖A− C‖\nAs an interesting corollary, Theorem 4.1 dictates that for every r we have that ‖µr − θr‖ = O(1/c)‖µr − µ̂r‖."
    }, {
      "heading" : "4.1 The Proximity Condition – Part III of the Algorithm",
      "text" : "Part II of our algorithm returns centers θ1, . . . , θk which are O( 1 c √ nr )‖A − C‖ close to the true centers. Suppose we use these centers to cluster the points: Θs = {i : ∀s′, ‖Ai− θs‖ ≤ ‖Ai− θs′‖}. It is evident that this clustering correctly classifies the majority of the points. It correctly classifies any point i ∈ Ts with ‖Ai − µr‖ − ‖Ai − µs‖ = Ω( 1c√nr )‖A− C‖ for every r 6= s, and the analysis of Theorem 3.1 shows that at most O(c−2)-fraction of the points do not satisfy this condition. In order to have a direct comparison with the Kumar-Kannan analysis, we now bound the number of misclassified points w.r.t the fraction of points satisfying the Kumar-Kannan proximity condition.\nDefinition 4.6. Denote gapr,s = ( 1√ nr + 1√ns )‖A − C‖. Call a point i ∈ Ts γ-good, if for every r 6= s we have that the projection of Ai onto the line connecting µr and µs, denoted Āi, satisfies that ‖Āi − µr‖ − ‖Āi − µs‖ ≥ γ gapr,s; otherwise we say the point is γ-bad.\nCorollary 4.7. If the number of γ-bad points is ǫn, then (a) the clustering {Θ1, . . . ,Θk} misclassifies no more than (\nǫ+ O(1) γ2c4\n) n points, and (b) ǫ < O (\n(c− γ√ k )−2\n) , assuming γ < c √ k.\nProof. Clearly, all ǫn bad points may be misclassified. In addition, for every r and s 6= r, Lemma 4.5 (setting ζr = θr, ζs = θs, α = 1/c √ k and β = Ω(γ/(c √ k))) proves that no more than O(γ−2c−2k−1)ns good points can be misclassified. Summing ∑ s 6=r 1 kns ≤ n, we conclude (a).\nThe proof of (b) is similar to the proof of Theorem 3.1. We look at the k-means cost of ‖Â−C‖2F . We show that all γ-bad points contribute a large amount to this cost.\nTake Ai to be a γ-bad point from Ts. Projecting it down to the line connecting µr and µs, we denote the projection as Āi. Clearly, ‖µr − µs‖ = ‖µr − Āi‖ + ‖Āi − µs‖ ≥ c √ kgapr,s whereas\n‖µr − Āi‖ − ‖Āi − µs‖ ≤ γgapr,s. It follows that ‖Âi − µs‖ ≥ ‖Āi − µs‖ ≥ 12 (c √ k − γ)gapr,s ≥\nc √ k−γ\n2 √ ns ‖A− C‖. Again, the Markov inequality gives that\n#{bad points from Ts} (c √ k − γ)2 4ns ‖A−C‖2 ≤ ‖Â− C‖2F ≤ 8k‖A− C‖2\nso from each cluster, only a fraction of 32 ( √\nk c √ k−γ\n)2 of the points can be bad.\nObserve that Corollary 4.7 allows for multiple scaled versions of the proximity condition, based on the magnitude of γ. In particular, setting γ = 1 we get a proximity condition whose bound is independent of k, and still our clustering misclassifies only a small fraction of the points – at most O(c−2) fraction of all points might be misclassified because they are 1-bad, and no more than a O(c−4)-fraction of 1-good points may be misclassified. In addition, if there are no 1-bad points we show the following theorem. The proof (omitted) merely follows the Kumar-Kannan proof, plugging in the better bounds, provided by Lemma 4.5.\nTheorem 4.8. Assume all data points are 1-good. That is, for every point Ai that belongs to the target cluster Tc(i) and every s 6= c(i), by projecting Ai onto the line connecting µc(i) with µs we have that the projected point Āi satisfies ‖Āi−µc(i)‖−‖Āi−µs‖ = Ω ( ( 1√nc(i) + 1√ ns ) ) ‖A−C‖, whereas ‖µc(i) − µs‖ = Ω (√ k( 1√nc(i) + 1√ ns ) )\n‖A − C‖. Then the Lloyd method, starting with θ1, . . . , θk, converges to the true centers."
    }, {
      "heading" : "5 Applications",
      "text" : "Clustering a mixture of Gaussians For a mixture of k Gaussians, we quote the suitable results without proof, as the proof is identical to the proof in [KK10]. We are given a mixture of k Gaussians, F1, . . . , Fk, where the standard deviation of each distribution in any direction is at most σr, and the weight of each distribution is wr. We denote σmax = maxr{σr} and wmin = minr{wr}.\nTheorem 5.1. Suppose we are given a set of n ≫ dwmin samples from a mixture of k Gaussians, such that for every r 6= s it holds that ‖µr−µs‖ ≥ cσmax √\nk wmin\npoly log (\nd wmin\n)\n. Then w.h.p. these\npoints satisfy the proximity condition.\nFor Gaussians, the best known separation bound is Achlioptas and McSherry’s bound [AM05]\nof Ω(σmax(w −1/2 min + √\nk log(k ·min{n, 2k}) )). As we assume k is large, this separation condition is Ω̃(σmax(w −1/2 min + √ k)) = Ω̃(σmax/ √ wmin). Therefore, the separation bound of Theorem 5.1 is√\nk times worse than the best known bound. However, applying Kumar and Kannan’s boosting technique (Section 7 in [KK10]), that replaces the polynomial dependency in wmin with a logarithmic one, we get:\nTheorem 5.2. Suppose we are given a set of n ≫ dwmin samples from a mixture of k Gaussians, such that for every r 6= s it holds that\n‖µr − µs‖ ≥ cσmax √ k poly log ( d\nwmin\n)\nThen there exists an algorithm that w.h.p. correctly classifies all points.\nTherefore, if for any r and r′, both σr ≈ σr′ and wr ≈ wr′ , then both [AM05] and Theorem 5.2 give roughly the same bound. If for any r and r′ we have that σr ≈ σr′ , yet wmin ≪ 1k , then Theorem 5.2 provides a better bound. If for any r and r′ we have that wr ≈ wr′ , yet the directional standard deviations of the distributions vary, then the bound of [AM05], in which the distance between any two cluster centers depends only the parameters of these two distributions, is the better bound. If both the standard deviations and the weights vary significantly between the different distributions, then better bound is determined on a case by case basis.\nMcSherry’s Planted Partition Model. In the Planted Partition Model [McS01, AK94, AKS98] our instance is a random n-vertex graph generated by using an implicit partition of the n points into k clusters. There exists an unknown k × k matrix of probabilities P , and for every pair of vertices u, v there exists an edge connecting u and v w.p. Prs (assuming u belongs to cluster r and v to cluster s). The goal here is to recover the partition of the points (thus – recover P ). Viewing this graph as a n× n matrix, each row is taken from a special distribution Fr over {0, 1}n – where each coordinate j is an independent Bernoulli r.v. with mean Pr,C(j), denoting C(j) as the cluster j belongs to. Thus, the mean of this distribution, µr, is a vector with its j-coordinate set to Pr,C(j). Denote wmin = minr{nrn } and σmax = maxr,s √ Prs. The result of [McS01] is that if for every r 6= s\n‖µr − µs‖ = Ω ( σmax √ k ( 1\nwmin + log(n/δ)\n))\n(2)\nthen it is possible to retrieve the partition of the vertices w.p. at least 1− δ.\nKumar and Kannan were not able to match the distance bounds of McSherry, and required centers to be √ k factor greater then the bound of (2). Here we match the bound of McSherry exactly. Following the proof in Kumar-Kannan (with few changes), we prove:\nTheorem 5.3. Assuming that σmax ≥ 3 log(n)n and that the planted partition model satisfies equation 2 for every r 6= s, then w.p. at least 1− δ, every point satisfies the proximity condition.\nProof. We follow the proof of Kumar-Kannan, making the suitable changes. McSherry (Theorem 10 of [McS01]) showed that w.h.p. ‖A − C‖ ≤ 4σmax √ n. So our goal is to show that, w.h.p., all\npoints are √ k-good. I.e., denoting u as a unit-length vector connecting µr and µs, we show that w.h.p. that for every i ∈ Tr we have\n|(Ai − µr) · u| = O( √ kσmax ( 1\nwmin + log(n/δ)\n)\n)\nObserve u = µs−µr‖µs−µr‖ , and due to the special structure of the means in this model, we have that\n(µr − µs)j = Prt − Pst where j ∈ Tt. It follows that\n‖µr − µs‖2 = k ∑\nt=1\nnt(Prt − Pst)2\nWe therefore have\n|(Ai − µr) · u| ≤ 1\n‖µr − µs‖\n\n\nk ∑\nt=1\n|Prt − Pst|\n∣ ∣ ∣ ∣ ∣ ∣ ∑ j∈Tt Aij − Prt ∣ ∣ ∣ ∣ ∣ ∣  \nObserve, Aij are i.i.d 0-1 random variables with mean Prt, so we expect their sum to deviate from its expectation by no more than a few standard deviations. Indeed, Kumar and Kannan prove that w.h.p. it holds that for every t we have\n∣ ∣ ∣ ∣ ∣ ∣ ∑ j∈Tt Aij − Prt ∣ ∣ ∣ ∣ ∣ ∣ ≤ B√ntσmax ( 1 wmin + log(n/δ) )\nwhere B is some sufficiently large constant. This allows us to deduce that\n|(Ai − µr) · u| ≤ Bσmax ( 1\nwmin + log(n/δ)\n) ∑k t=1 √ nt|Prt − Pst|\n√\n∑k t=1 nt(Prt − Pst)2\n≤ B √ kσmax ( 1\nwmin + log(n/δ)\n)\nwhere the last inequality is simply the power-mean inequality."
    }, {
      "heading" : "6 An Open Problem",
      "text" : "Our work presents an algorithm which successfully clusters a dataset, provided that the distance between any two cluster centers meets a certain lower bound. We would like to point out one\nparticular direction to improve this bound. Note that our center separation bound depends on ‖A−C‖, a property of the entire dataset. It would be nice to handle the case where the separation condition between µr and µs depends solely on Tr and Ts. That is, if we define ∆̃r = √ k√ nr ‖Ar−Cr‖, is it possible to successfully separate clusters s.t ‖µr−µs‖ ≥ c(∆̃r+∆̃s)? We comment that most of our analysis (and particularly Lemma 4.5) builds only on the ratio between ‖µr−νr‖ and ‖µr−µs‖ – we assume the first is no greater than α∆r and that the latter is no less than c(∆r+∆s). In fact, one can revise the proofs of Theorems 3.1 and 3.2 so that they will hold based on this assumption alone (without using the properties of the SVD). The problem therefore boils down to finding initial centers {νr} that are sufficiently close to the true centers {µr}, under the assumption that ∀r 6= s, ‖µr − µs‖ ≥ c(∆̃r + ∆̃s). But this is an intricate task, mainly because such separation condition does not imply that {µ1, µ2, . . . , µk} are the centers minimizing the k-means cost! (Nor do {µ̂1, µ̂2, . . . , µ̂k, } minimize the k-means cost of Â.) Consider the case, for example, where cluster r has very few points (say nr = √ n) and very small variance, and cluster s is very big (say ns = n/5), and is essentially composed of two sub-components with distance 1\n2 √ ns ‖As − Cs‖\nbetween the centers of the two sub-components. The k-means cost of placing two centers within Cs is smaller than placing one center at µs and one center at µr. This relates to the question of designing a t-approximation algorithm for k-means, guaranteeing that each cluster’s cost cannot increase by more than a factor of t."
    }, {
      "heading" : "A Some Basic Lemmas",
      "text" : "Fact A.1 (Lemma 9 from [McS01]). ‖Â−C‖2F ≤ 8min{k‖A−C‖2, ‖A−C‖2F } ( = 8nr∆ 2 r for every r ) .\nProof.\n‖Â− C‖2F ≤ 2k‖Â − C‖2 ≤ 2k ( ‖Â−A‖+ ‖A−C‖ )2 ≤ 2k (2‖A − C‖)2\nwhere the first inequality holds because rank(Â−C) ≤ 2k, and the last inequality follows from the fact that Â = argminN :rank(N)=k{‖A−N‖}. For the same reason, ‖Â− C‖F ≤ ‖A− Â‖F + ‖A− C‖F ≤ 2‖A− C‖F .\nFact A.2 (Claim 1 in Section 3.2 of [KV09]). For every µr there exists a center νs s.t. ‖µr−νs‖ ≤ 6∆r, so we can match each µr to a unique νr.\nProof. Observe that by taking Â − Ĉ, we project A − C to a k-dimensional subspace, so we have that ‖Â− Ĉ‖2F ≤ k‖Â− Ĉ‖2 ≤ k‖A− C‖2. Similarly, ‖Â− Ĉ‖2F ≤ ‖A−C‖2F .\nAssume for the sake of contradiction that ∃r s.t. ‖µr − νs‖ > 6∆r for all s. Since ‖Â− Ĉ‖2F ≤ nr∆ 2 r, then our 10-approximation algorithm yields a clustering of cost ≤ 10nr∆2r. In contrast, as\neach Âi is assigned to some νc(i), the contribution of only the points in Tr to the k-means cost of the clustering is more than\n∑\ni∈Tr\n∥ ∥ ∥(µr − νc(i))− (Âi − µr) ∥ ∥ ∥ 2 > nr 2 (6∆r) 2 − ∑\ni∈Tr ‖Âi − µr‖2 ≥ 18nr∆2r − ‖Â− C‖2F ≥ 10nr∆2r\nwhere the first inequality follows from the fact that (a− b)2 ≥ 12a2 − b2.\nNow, in order to prove Fact 1.3 (also cited below as Fact A.4), we need the following Fact.\nFact A.3 (Lemma 5.2 and Corollary 5.3 from [KK10]). Fix any cluster Tr and a subset X ⊂ Tr. Then\n|X| ‖µ(X) − µr‖ = (|Tr| − |X|) ‖µ(Tr \\X) − µr‖ ≤ √ |X| ‖Ar − Cr‖\nProof. Let uX be the indicator vector of X. Then\n‖ |X| (µ(X) − µr) ‖ = ‖(Ar − Cr)T uX‖ ≤ ‖(Ar − Cr)T ‖ ‖uX‖ = ‖Ar − Cr‖ √ |X|\nand the fact that |X| ‖µ(X)− µr‖ = |Tr \\X| ‖µ(Tr \\X)− µr‖ is simply because µr = |X||Tr |µ(X) + |Tr\\X| |Tr | µ(Tr \\X).\nFact A.4. Fix a target cluster Tr and let Sr be a set of points created by removing ρoutnr points from Tr and adding ρin(s)nr points from each cluster s 6= r, s.t. every added point x satisfies ‖x− µs‖ ≥ 23‖x− µr‖. Assume ρout < 14 and ρin def = ∑ s 6=r ρin(s) < 1 4 . Then\n‖µ(Sr)− µr‖ ≤ 1√ nr\n\n √ ρout + 3 2 ∑\ns 6=r\n√\nρin(s)\n  ‖A− C‖ ≤ (√\nρout nr + 32\n√ k √\nρin nr\n)\n‖A− C‖\nProof. We break ‖µ(Sr)− µr‖ into its components and deduce\n‖µ(Sr)− µr‖ ≤ (1− ρout)nr\nnr ‖µ(Sr ∩ Tr)− µr‖+\n∑\ns 6=r\nρin(s)nr nr ‖µ(Sr ∩ Ts)− µr‖\n≤ (1− ρout)nr nr ‖µ(Sr ∩ Tr)− µr‖+ 32 ∑\ns 6=r\nρin(s)nr nr ‖µ(Sr ∩ Ts)− µs‖\nPlugging in Fact A.3 we have ‖µ(Sr)−µr‖ ≤ 1nr (√ ρoutnr + 3 2 ∑ s 6=r √ ρin(s)nr )\n‖A−C‖. The last inequality comes from maximizing the sum of square-roots by taking each ρin(s) = ρin/k."
    } ],
    "references" : [ {
      "title" : "Stability yields a PTAS for k-median and k-means clustering",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Awasthi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2010
    }, {
      "title" : "A spectral technique for coloring random 3-colorable graphs",
      "author" : [ "Noga Alon", "Nabil Kahale" ],
      "venue" : "In SIAM Journal on Computing,",
      "citeRegEx" : "Alon and Kahale.,? \\Q1994\\E",
      "shortCiteRegEx" : "Alon and Kahale.",
      "year" : 1994
    }, {
      "title" : "Finding a large hidden clique in a random graph",
      "author" : [ "Noga Alon", "Michael Krivelevich", "Benny Sudakov" ],
      "venue" : null,
      "citeRegEx" : "Alon et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 1998
    }, {
      "title" : "On spectral learning of mixtures of distributions",
      "author" : [ "Dimitris Achlioptas", "Frank McSherry" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Achlioptas and McSherry.,? \\Q2005\\E",
      "shortCiteRegEx" : "Achlioptas and McSherry.",
      "year" : 2005
    }, {
      "title" : "k-means++: The advantages of careful seeding",
      "author" : [ "D. Arthur", "S. Vassilvitskii" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Arthur and Vassilvitskii.,? \\Q2007\\E",
      "shortCiteRegEx" : "Arthur and Vassilvitskii.",
      "year" : 2007
    }, {
      "title" : "Approximate clustering without the approximation",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2009
    }, {
      "title" : "Approximate clustering via coresets",
      "author" : [ "Mihai Bādoiu", "Sariel Har-Peled", "Piotr Indyk" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Bādoiu et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bādoiu et al\\.",
      "year" : 2002
    }, {
      "title" : "Polynomial learning of distribution families",
      "author" : [ "Mikhail Belkin", "Kaushik Sinha" ],
      "venue" : "Computing Research Repository,",
      "citeRegEx" : "Belkin and Sinha.,? \\Q2010\\E",
      "shortCiteRegEx" : "Belkin and Sinha.",
      "year" : 2010
    }, {
      "title" : "Isotropic pca and affine-invariant clustering",
      "author" : [ "S. Charles Brubaker", "Santosh Vempala" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Brubaker and Vempala.,? \\Q2008\\E",
      "shortCiteRegEx" : "Brubaker and Vempala.",
      "year" : 2008
    }, {
      "title" : "Graph partitioning via adaptive spectral techniques",
      "author" : [ "Amin Coja-Oghlan" ],
      "venue" : "Comb. Probab. Comput.,",
      "citeRegEx" : "Coja.Oghlan.,? \\Q2010\\E",
      "shortCiteRegEx" : "Coja.Oghlan.",
      "year" : 2010
    }, {
      "title" : "Learning to match and cluster large highdimensional data sets for data integration",
      "author" : [ "William W. Cohen", "Jacob Richman" ],
      "venue" : "In KDD,",
      "citeRegEx" : "Cohen and Richman.,? \\Q2002\\E",
      "shortCiteRegEx" : "Cohen and Richman.",
      "year" : 2002
    }, {
      "title" : "Beyond gaussians: Spectral methods for learning mixtures of heavy-tailed distributions",
      "author" : [ "Kamalika Chaudhuri", "Satish Rao" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Chaudhuri and Rao.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chaudhuri and Rao.",
      "year" : 2008
    }, {
      "title" : "Learning mixtures of product distributions using correlations and independence",
      "author" : [ "Kamalika Chaudhuri", "Satish Rao" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Chaudhuri and Rao.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chaudhuri and Rao.",
      "year" : 2008
    }, {
      "title" : "Learning mixtures of gaussians",
      "author" : [ "Sanjoy Dasgupta" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Dasgupta.,? \\Q1999\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 1999
    }, {
      "title" : "Spectral clustering with limited independence",
      "author" : [ "Anirban Dasgupta", "John Hopcroft", "Ravi Kannan", "Pradipta Mitra" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Dasgupta et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2007
    }, {
      "title" : "Approximation schemes for clustering problems",
      "author" : [ "W. Fernandez de la Vega", "Marek Karpinski", "Claire Kenyon", "Yuval Rabani" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Vega et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Vega et al\\.",
      "year" : 2003
    }, {
      "title" : "A probabilistic analysis of em for mixtures of separated, spherical gaussians",
      "author" : [ "Sanjoy Dasgupta", "Leonard Schulman" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Dasgupta and Schulman.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dasgupta and Schulman.",
      "year" : 2007
    }, {
      "title" : "Deterministic clustering with data",
      "author" : [ "Michelle Effros", "Leonard J. Schulman" ],
      "venue" : "nets. ECCC,",
      "citeRegEx" : "Effros and Schulman.,? \\Q2004\\E",
      "shortCiteRegEx" : "Effros and Schulman.",
      "year" : 2004
    }, {
      "title" : "Simpler analyses of local search algorithms for facility location",
      "author" : [ "Anupam Gupta", "Kanat Tangwongsan" ],
      "venue" : "CoRR, abs/0809.2554,",
      "citeRegEx" : "Gupta and Tangwongsan.,? \\Q2008\\E",
      "shortCiteRegEx" : "Gupta and Tangwongsan.",
      "year" : 2008
    }, {
      "title" : "Matrix computations (3rd ed.)",
      "author" : [ "Gene H. Golub", "Charles F. Van Loan" ],
      "venue" : null,
      "citeRegEx" : "Golub and Loan.,? \\Q1996\\E",
      "shortCiteRegEx" : "Golub and Loan.",
      "year" : 1996
    }, {
      "title" : "On coresets for k-means and k-median clustering",
      "author" : [ "Sariel Har-Peled", "Soham Mazumdar" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Har.Peled and Mazumdar.,? \\Q2004\\E",
      "shortCiteRegEx" : "Har.Peled and Mazumdar.",
      "year" : 2004
    }, {
      "title" : "Clustering with spectral norm and the k-means algorithm",
      "author" : [ "A. Kumar", "R. Kannan" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Kumar and Kannan.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kumar and Kannan.",
      "year" : 2010
    }, {
      "title" : "A local search approximation algorithm for k-means clustering",
      "author" : [ "Tapas Kanungo", "David M. Mount", "Nathan S. Netanyahu", "Christine D. Piatko", "Ruth Silverman", "Angela Y. Wu" ],
      "venue" : "In Proc. 18th Symp. Comp. Geom.,",
      "citeRegEx" : "Kanungo et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kanungo et al\\.",
      "year" : 2002
    }, {
      "title" : "Efficiently learning mixtures of two gaussians",
      "author" : [ "Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant" ],
      "venue" : "In STOC’10,",
      "citeRegEx" : "Kalai et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kalai et al\\.",
      "year" : 2010
    }, {
      "title" : "A simple linear time (1 + ǫ)approximation algorithm for k-means clustering in any dimensions",
      "author" : [ "Amit Kumar", "Yogish Sabharwal", "Sandeep Sen" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Kumar et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2004
    }, {
      "title" : "The spectral method for general mixture models",
      "author" : [ "Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Kannan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kannan et al\\.",
      "year" : 2008
    }, {
      "title" : "Spectral algorithms. Found",
      "author" : [ "Ravindran Kannan", "Santosh Vempala" ],
      "venue" : "Trends Theor. Comput. Sci.,",
      "citeRegEx" : "Kannan and Vempala.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kannan and Vempala.",
      "year" : 2009
    }, {
      "title" : "Least squares quantization in pcm",
      "author" : [ "Stuart P. Lloyd" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Lloyd.,? \\Q1982\\E",
      "shortCiteRegEx" : "Lloyd.",
      "year" : 1982
    }, {
      "title" : "Scop: a structural classification of proteins database for the investigation of sequences and structures",
      "author" : [ "A GMurzin", "S E Brenner", "T Hubbard", "C Chothia" ],
      "venue" : "Journal of Molecular Biology,",
      "citeRegEx" : "GMurzin et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "GMurzin et al\\.",
      "year" : 1995
    }, {
      "title" : "Spectral partitioning of random graphs",
      "author" : [ "F. McSherry" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "McSherry.,? \\Q2001\\E",
      "shortCiteRegEx" : "McSherry.",
      "year" : 2001
    }, {
      "title" : "Settling the polynomial learnability of mixtures of gaussians",
      "author" : [ "Ankur Moitra", "Gregory Valiant" ],
      "venue" : "In FOCS’10,",
      "citeRegEx" : "Moitra and Valiant.,? \\Q2010\\E",
      "shortCiteRegEx" : "Moitra and Valiant.",
      "year" : 2010
    }, {
      "title" : "Polynomial time approximation schemes for geometric k-clustering",
      "author" : [ "R. Ostrovsky", "Y. Rabani" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Ostrovsky and Rabani.,? \\Q2000\\E",
      "shortCiteRegEx" : "Ostrovsky and Rabani.",
      "year" : 2000
    }, {
      "title" : "The effectiveness of lloyd-type methods for the k-means problem",
      "author" : [ "Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Ostrovsky et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ostrovsky et al\\.",
      "year" : 2006
    }, {
      "title" : "Clustering for edge-cost minimization (extended abstract)",
      "author" : [ "Leonard J. Schulman" ],
      "venue" : "In STOC, pages 547–555,",
      "citeRegEx" : "Schulman.,? \\Q2000\\E",
      "shortCiteRegEx" : "Schulman.",
      "year" : 2000
    }, {
      "title" : "Learning mixtures of arbitrary gaussians",
      "author" : [ "Arora Sanjeev", "Ravi Kannan" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Sanjeev and Kannan.,? \\Q2001\\E",
      "shortCiteRegEx" : "Sanjeev and Kannan.",
      "year" : 2001
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Aiming to unify known results about clustering mixtures of distributions under separation conditions, Kumar and Kannan [KK10] introduced a deterministic condition for clustering datasets. They showed that this single deterministic condition encompasses many previously studied clustering assumptions. More specifically, their proximity condition requires that in the target k-clustering, the projection of a point x onto the line joining its cluster center μ and some other center μ, is a large additive factor closer to μ than to μ. This additive factor can be roughly described as k times the spectral norm of the matrix representing the differences between the given (known) dataset and the means of the (unknown) target clustering. Clearly, the proximity condition implies center separation – the distance between any two centers must be as large as the above mentioned bound. In this paper we improve upon the work of Kumar and Kannan [KK10] along several axes. First, we weaken the center separation bound by a factor of √ k, and secondly we weaken the proximity condition by a factor of k (in other words, the revised separation condition is independent of k). Using these weaker bounds we still achieve the same guarantees when all points satisfy the proximity condition. Under the same weaker bounds, we achieve even better guarantees when only (1−ǫ)-fraction of the points satisfy the condition. Specifically, we correctly cluster all but a (ǫ + O(1/c))-fraction of the points, compared to O(kǫ)-fraction of [KK10], which is meaningful even in the particular setting when ǫ is a constant and k = ω(1). Most importantly, we greatly simplify the analysis of Kumar and Kannan. In fact, in the bulk of our analysis we ignore the proximity condition and use only center separation, along with the simple triangle and Markov inequalities. Yet these basic tools suffice to produce a clustering which (i) is correct on all but a constant fraction of the points, (ii) has k-means cost comparable to the k-means cost of the target clustering, and (iii) has centers very close to the target centers. Our improved separation condition allows us to match the results of the Planted Partition Model of McSherry [McS01], improve upon the results of Ostrovsky et al [ORSS06], and improve separation results for mixture of Gaussian models in a particular setting.",
    "creator" : "LaTeX with hyperref package"
  }
}