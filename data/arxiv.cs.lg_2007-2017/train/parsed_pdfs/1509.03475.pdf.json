{
  "name" : "1509.03475.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hessian-Free Optimization For Learning Deep Multidimensional Recurrent Neural Networks",
    "authors" : [ "Minhyung Cho Chandra", "Shekhar Dhir Jaehyung Lee" ],
    "emails" : [ "mhyung.cho@gmail.com", "shekhardhir@gmail.com", "jaehyung.lee@kaist.ac.kr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 9.\n03 47\n5v 1\n[ cs\n.L G\n] 1\n1 Se\nMultidimensional recurrent neural network (MDRNN) has shown a remarkable performance in speech and handwriting recognition. The performance of MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by Hessian-free (HF) optimization. Considering that connectionist temporal classification (CTC) is utilized as an objective of learning MDRNN for sequence labelling, the non-convexity of CTC poses a problem to apply HF to the network. As a solution to this, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. MDRNN up to the depth of 15 layers is successfully trained using HF, resulting in improved performance for sequence labelling."
    }, {
      "heading" : "1 Introduction",
      "text" : "Multidimensional recurrent neural network (MDRNN) is an efficient architecture to build multidimensional context into recurrent neural networks [1]. End-to-end training of MDRNN in conjunction with connectionist temporal classification (CTC) has shown the state-of-the-art performance in on/off-line handwriting recognition [2, 3] and speech recognition [4].\nIn previous approaches, the performance of MDRNN has been demonstrated with the networks having up to depth of 5 layers, which are relatively limited compared to the recent progress on feedforward networks [5]. The effectiveness of deeper MDRNNs beyond 5 layers has been unknown.\nTraining a deeper architecture has always been a challenging topic in machine learning. Notable breakthrough was made where deep feedforward neural networks were initialized using layer-wise pre-training [6]. Recently, there has been approaches to add supervision to intermediate layers to train deep networks [5, 7]. To our knowledge, no such pre-training or bootstrapping method has been developed for MDRNN which potentially utilizes LSTM cells [8] as its hidden unit.\nAlternatively, HF optimization is an appealing approach to train deep neural networks due to its ability to overcome pathological curvature of the objective function [9]. Furthermore it can be applied to any connectionist model as long as its objective function is differentiable. The recent success of HF to deep feedforward and recurrent neural networks [9, 10] encourages the use of HF to MDRNN.\nIn this paper, we claim that MDRNN can benefit from deeper architecture, and applying second order optimization like HF enables its successful learning. First we offer details to develop HF optimization for MDRNN. Then, to apply HF optimization for sequence labelling tasks, we address the problem of non-convexity of CTC, and formulate a convex approximation. Also, its relationship with the EM algorithm and the Fisher information matrix is discussed. Experimental results of offline handwriting recognition and phoneme recognition show that MDRNN with HF performs better as the depth of the network increases up to fifteen."
    }, {
      "heading" : "2 Multidimensional recurrent neural networks",
      "text" : "MDRNN is a generalization of RNN to process multidimensional data by replacing the single recurrent connection with as many connections as dimensions of the data [1]. The network can access the contextual information from 2N directions, allowing to make a collective decision based on rich context information. To enhance its ability of exploiting context information, long short-term memory (LSTM) [8] cells are usually utilized as hidden units. In addition, stacking MDRNNs to construct deeper networks further improves the performance as the depth increases, reporting the state-of-the-art performance in phoneme recognition [4]. For sequence labelling, CTC is applied as a loss function of MDRNN. The important advantage of using CTC is that any pre-segmented sequences are not required, and the entire transcription of the input sample is sufficient."
    }, {
      "heading" : "2.1 Learning MDRNN",
      "text" : "A d-dimensional MDRNN with M inputs and K outputs is regarded as a mapping from an input sequence x ∈ RM×T1×···×Td to an output sequence a ∈ (RK)T of length T , where input data for M input neurons is given by vectorization of a d-dimensional data and T1, . . . , Td is the length of the sequence in each dimension. All learnable weights and biases are concatenated to obtain a parameter vector θ ∈ RN . In the learning phase with fixed training data, MDRNN is formalized as a mapping N : RN → (RK)T from the parameters θ to the output sequence a, i.e. a = N (θ). The scalar loss function is defined over the output sequence as L : (RK)T → R. Learning MDRNN is viewed as an optimization of the objective function L(N (θ)) = L ◦ N (θ) with respect to θ."
    }, {
      "heading" : "2.2 Notation",
      "text" : "The Jacobian JF of a function F : Rm → Rn is the n ×m matrix where each element is a partial derivative of an element of output with respect to an element of input. The Hessian HF of a scalar function F : Rm → R is the m × m matrix of second-order partial derivatives of the output with respect to its inputs. Throughout the paper, a symbol ⊤ is used for denoting the transpose of a vector or matrix. For variables, a sequence of vector is denoted by boldface a, a vector at time t in a is denoted by at, and the k-th element of at is denoted by atk."
    }, {
      "heading" : "3 Hessian-free optimization for MDRNN",
      "text" : "In this section, we discuss two main points to develop HF optimization for MDRNN. One is obtaining a local quadratic approximation for MDRNN, and the other is an efficient calculation of the matrix-vector product used at each iteration of the conjugate gradient (CG) method.\nHF minimizes an objective by constructing a local quadratic approximation to the objective function and minimizing the approximate function instead of the original one. The loss function L(θ) needs to be approximated at each point θn of the n-th iteration as follows:\nQn(θ) = L(θn) +∇θL| ⊤ θn δn +\n1 2 δ⊤n Gδn, (1)\nwhere δn = θ − θn is the search direction, i.e. parameters of the optimization, and G is a local approximation to the curvature of L(θ) at θn, which is typically obtained by the generalized GaussNewton (GGN) matrix as an approximation of the Hessian.\nHF uses the CG method in a subroutine to minimize the quadratic objective above for utilizing the complete curvature information and achieving computational efficiency. CG requires the computation of Gv for an arbitrary vector v, but not the explicit evaluation of G. For neural networks, an efficient way to compute Gv was proposed by [11], extending the work of [12]. In section 3.2, we provide the details for the efficient computation of Gv for MDRNN."
    }, {
      "heading" : "3.1 Quadratic approximation of loss function",
      "text" : "The Hessian matrix, HL◦N , of the objective L (N (θ)) is written as\nHL◦N = J ⊤ NHLJN +\nKT ∑\ni=1\n[JL]iH[N ]i , (2)\nwhere JN ∈ RKT×N , HL ∈ RKT×KT , and [q]i denotes the i-th component of the vector q. An indefinite Hessian matrix is problematic for 2nd-order optimization because it defines an unbounded local quadratic approximation [13]. For nonlinear systems, the Hessian is not necessarily positive semidefinite, thus the GGN matrix is used as an approximation of the Hessian [11, 9]. The GGN matrix is obtained by ignoring the second term in Eq. (2), as given by\nGL◦N = J ⊤ NHLJN . (3)\nThe sufficient condition for the GGN approximation to be exact is that the network makes a perfect prediction for every given sample, that is, JL = 0, or [N ]i stays in the linear region for all i, that is, H[N ]i = 0.\nGL◦N has less rank than KT and is positive semidefinite as long as HL is. Thus, L is chosen to be a convex function so that HL is positive semidefinite. In principle, it is best to define L and N in a way that L performs as much of the computation as possible, with the positive semidefiniteness of HL as a minimum requirement [13]. In practice, a nonlinear output layer along with its matching loss function [11], such as the softmax function with cross-entropy loss, is widely used.\nConsidering that MDRNN is normally applied to model sequential data such as speech or handwriting, complex loss functions need to be adopted, like the one provided by CTC. A detailed discussion of approximating the Hessian for CTC is provided in section 4."
    }, {
      "heading" : "3.2 Computation of matrix-vector product for MDRNN",
      "text" : "The product of an arbitrary vector v by the GGN matrix, Gv = J⊤NHLJN v, amounts to the sequential multiplication of v by three matrices. First, the product JN v is a Jacobian times vector and is therefore equal to the directional derivative of N (θ) along the direction of v. Thus, JN v can be written using a differential operator JN v = Rv(N (θ)) [12], and the properties of the operator can be utilized for efficient computation. Because MDRNN is a composition of differentiable components, the computation of Rv(N (θ)) throughout the whole network can be accomplished by repeatedly applying the sum, product, and chain rules starting from the input layer. The detailed derivation of R operator to LSTM, normally used as a hidden unit in MDRNN, is provided in appendix A.\nNext, the multiplication of JN v by HL can be done by direct computation. At first sight, the dimension of HL could be seen problematic since the dimension of the output vector used by the loss function L can be as high as KT , especially if CTC is adopted as an objective for MDRNN. If the loss function can be expressed as the sum of individual loss functions with restricted domain in time, the computation can be reduced significantly. For example, with the commonly used crossentropy loss function, KT × KT matrix HL can be turned into a block diagonal matrix with T blocks of K ×K Hessian matrix. Let HL,t be the t-th block in HL. Then, the GGN matrix can be written as\nGL◦N = ∑\nt\nJ⊤NtHL,tJNt , (4)\nwhere JNt is the Jacobian of the network at time t.\nFinally, the multiplication of a vector u = HLJN v by the matrix J⊤N is calculated using the backpropagation through time algorithm by propagating u instead of the error at the output layer."
    }, {
      "heading" : "4 Convex approximation of CTC for application to HF optimization",
      "text" : "Connectioninst temporal classification (CTC) [14] provides an objective function of learning MDRNN for sequence labelling. In this section, we derive a convex approximation of CTC inspired by the GGN approximation according to the following steps. First, the non-convex part from the original objective is separated out by reformulating the softmax part. Next, the remaining convex part is approximated without altering its Hessian, making it well matched to the non-convex part. Finally, the convex approximation is obtained by reuniting the convex and non-convex part."
    }, {
      "heading" : "4.1 Connectionist temporal classification",
      "text" : "CTC is formulated as the mapping from an output sequence of the recurrent network, a ∈ (RK)T , to a scalar loss. The output activations at time t are normalized using the softmax function\nytk = exp(atk) ∑\nk′ exp(a t k′)\n, (5)\nwhere ytk is the probability of label k given a at time t.\nThe conditional probability of the path π is calculated by the multiplication of the label probabilities at each timestep, as given by\np(π|a) = T ∏\nt=1\nytπt , (6)\nwhere πt is the label observed at time t along the path π. The path π of length T is mapped to a label sequence of length M ≤ T by an operator B which removes the repeated labels and then the blanks. Several mutually exclusive paths can map to the same label sequence. Let S be a set containing every possible sequence mapped by B, that is, S = {s|s ∈ B(π) for some π}, and let |S| denote the cardinality of the set.\nThe conditional probability of a label sequence l is given by\np(l|a) = ∑\nπ∈B−1(l)\np(π|a), (7)\nwhich is the sum of probabilities of all the paths mapped to a label sequence l by B.\nThe cross-entropy loss assigns a negative log probability to the correct answer. Given a target sequence z, the loss function of CTC for the sample is written as\nL(a) = − log p(z|a). (8)\nFrom the description above, CTC is composed of the sum of the product of softmax components. The function − log(ytk), corresponding to the softmax with cross-entropy loss, is convex [11]. Therefore, ytk is log-concave. Whereas log-concavity is closed under multiplication, the sum of log-concave functions is not log-concave in general [15]. As a result, the CTC objective is not convex in general because it contains the sum of softmax components in Eq. (7)."
    }, {
      "heading" : "4.2 Reformulation of CTC objective function",
      "text" : "We reformulate the CTC objective Eq. (8) to separate terms which are responsible for the nonconvexity of the function. By reformulation, the softmax function is defined over the categorical label sequences.\nBy substituting Eq. (5) into Eq. (6), it follows that\np(π|a) = exp(bπ) ∑\nπ′∈all exp(bπ′) , (9)\nwhere bπ = ∑ t a t πt . By substituting Eq. (9) into Eq. (7) and setting l = z, p(z|a) can be re-written as\np(z|a) =\n∑\nπ∈B−1(z) exp(bπ) ∑\nπ∈all exp(bπ) =\nexp(fz) ∑\nz′∈S exp(fz′) , (10)\nwhere S is the set of every possible label sequence and fz = log ( ∑ π∈B−1(z) exp(bπ) ) is the logsum-exp function1, which is proportional to the probability of observing the label sequence z among all the other label sequences.\nWith the reformulation above, the CTC objective can be regarded as the cross-entropy loss with the softmax output which is defined over all the possible label sequences. Because the cross-entropy\n1f(x1, . . . , xn) = log(e x1 + · · ·+ exn) is the log-sum-exp function defined on Rn\nloss function matches the softmax output layer [11], the CTC objective is convex except the part which computes fz for each of the label sequences. At this point, an obvious candidate for the convex approximation of CTC is the GGN matrix separating the convex part and non-convex part.\nLet the non-convex part be Nc and the convex part be Lc. The mapping Nc : (RK)T → R|S| is defined by\nNc(a) = F = [fz1 , . . . , fz|S| ] ⊤, (11)\nwhere fz is given above, and |S| is the number of all the possible label sequences. For given F as above, the mapping Lc : R|S| → R is defined by\nLc(F ) = − log exp(fz) ∑\nz′∈S exp(fz′) = −fz + log\n(\n∑\nz′∈S\nexp(fz′)\n)\n, (12)\nwhere z is the label sequence corresponding to a.\nThe final reformulation for the loss function of CTC is given by L(a) = Lc ◦ Nc(a). (13)"
    }, {
      "heading" : "4.3 Convex approximation of CTC loss function",
      "text" : "The GGN approximation of Eq. (13) immediately gives a convex approximation of the Hessian for CTC as GLc◦Nc = J ⊤ Nc\nHLcJNc . Although HLc has the form of a diagonal matrix plus a rank-1 matrix, i.e. diag(Y ) − Y Y ⊤, the dimension of HLc is |S| × |S| where |S| becomes exponentially large as the length of the sequence increases. It makes the practical calculation of HLc difficult.\nOn the other hand, removing the linear team −fz from Lc(F ) in Eq. (12) does not alter its Hessian. The resulting formula is Lp(F ) = log ( ∑ z′∈S exp(fz′) )\n. The GGN matrices of L = Lc ◦ Nc and M = Lp ◦ Nc are exactly the same, i.e. GLc◦Nc = GLp◦Nc . Therefore the Hessian matrices of them are approximations of each other. The condition that the two Hessian matrices, HL and HM, converges to the same matrix is discussed later.\nInterestingly, M is given as a compact formula M(a) = Lp ◦Nc(a) = ∑ t log ∑ k exp(a t k), where atk is the output unit k at time t. Its Hessian HM can be directly computed, resulting in a block diagonal matrix. Each block is restricted in time, and the t-th block is given by\nHM,t = diag(Y t)− Y tY t ⊤ , (14)\nwhere Y t = [yt1, . . . , y t K ] ⊤ and ytk is given in Eq. (5). Because the Hessian of each block is positive semidefinite, HM is positive semidefinite. A convex approximation of the Hessian of MDRNN using the CTC objective can be obtained by substituting HM for HL in Eq. (3). Note that the resulting matrix is block diagonal, and Eq. (4) can be utilized for efficient computation.\nSummary of our derivation is as follows:\n1. HL = HLc◦Nc is not positive semidefinite.\n2. GLc◦Nc = GLp◦Nc is positive semidefinite but is not computationally tractable.\n3. HLp◦Nc is positive semidefinite and computationally tractable."
    }, {
      "heading" : "4.4 Sufficient condition for the proposed approximation to be exact",
      "text" : "From Eq. (2), the condition HLc◦Nc = HLp◦Nc holds if and only if ∑KT\ni=1[JLc ]iH[Nc]i = ∑KT\ni=1[JLp ]iH[Nc]i . Since JLc 6= JLp in general, we consider only the case of H[Nc]i = 0 for all i, which corresponds to the case that Nc is a linear mapping.\n[Nc]i contains a log-sum-exp function mapping from paths to a label sequence. Let z be the label sequence corresponding to [Nc]i, then [Nc]i = fz(. . . , bπ, . . . ) for π ∈ B−1(z). If the probability of one path π′ is large enough to ignore all the other paths, that is, exp(bπ′) ≫ exp(bπ) for π ∈ {B−1(z)\\π′}, it follows that fz(. . . , bπ′ , . . . ) = bπ′ . This is a linear mapping, which results in H[Nc]i = 0.\nIn conclusion, the condition HLc◦Nc = HLp◦Nc holds if one dominant path π ∈ B −1(z) exists such that fz(. . . , bπ, . . . ) = bπ for every label sequence z."
    }, {
      "heading" : "4.5 Derivation of the proposed approximation from the Fisher information matrix",
      "text" : "The identity of the GGN and the Fisher information matrix [16] has been shown for the network using the softmax with cross-entropy loss [17, 18]. Thus, it follows that the GGN matrix of Eq. (13) is identical to the Fisher information matrix. Now we show that the Fisher information matrix is equivalent to the proposed matrix in Eq. (14) under the condition in section 4.4. The Fisher information matrix of MDRNN using CTC is written as\nF = Ex\n[\nJ⊤NEl∼p(l|a)\n[\n(\n∂ log p(l|a) ∂a\n)⊤ ( ∂ log p(l|a)\n∂a\n)\n]\nJN\n]\n, (15)\nwhere a = a(x, θ) is the KT -dimensional output of the network N . CTC assumes output probabilities at each timestep to be independent of those at other timesteps [1], therefore its Fisher information matrix is given as the sum of every timestep. It follows that\nF = Ex\n[\n∑\nt\nJ⊤NtEl∼p(l|a)\n[\n(\n∂ log p(l|a) ∂at\n)⊤ ( ∂ log p(l|a)\n∂at\n)\n]\nJNt\n]\n. (16)\nUnder the condition in section 4.4, the Fisher information matrix is given by\nF = Ex\n[\n∑\nt\nJ⊤Nt(diag(Y t)− Y tY t ⊤ )JNt\n]\n, (17)\nwhich is the same form as Eq. (4) and (14) combined. See appendix B for the detailed derivation."
    }, {
      "heading" : "4.6 EM interpretation of the proposed approximation",
      "text" : "The goal of the Expectation-Maximization (EM) algorithm is to find the maximum likelihood solution for models having latent variables [19]. Given an input sequence x, and its corresponding target label sequence z, the log likelihood of z is given by log p(z|x, θ) = log ∑\nπ∈B−1(z) p(π|x, θ), where θ represents the model parameters. For each observation x, we have a corresponding latent variable q which is a 1-of-k binary vector where k is the number of all the paths mapped to z. The log likelihood can be written in terms of q as log p(z, q|x, θ) = ∑\nπ∈B−1(z) qπ|x,z log p(π|x, θ).\nEM algorithm starts with an initial parameter θ̂, and repeats the following process until convergence.\nExpectation step calculates: γπ|x,z = p(π|x,θ̂)\n∑ π∈B−1(z) p(π|x,θ̂)\n.\nMaximization step updates: θ̂ = argmaxθQ(θ), where Q(θ) = ∑ π∈B−1(z) γπ|x,z log p(π|x, θ).\nIn the context of CTC and RNN, p(π|x, θ) is given as p(π|a(x, θ)) as in Eq. (6), where a(x, θ) is the KT -dimensional output of the neural network. Taking the second-order derivative of log p(π|a) with respect to at gives diag(Y t)−Y tY t⊤ , with Y t as in Eq. (14). Because this term is independent of π and ∑\nπ∈B−1(z) γπ|x,z = 1, the Hessian of Q with respect to a t is given by\nHQ,t = diag(Y t)− Y tY t ⊤ , (18)\nwhich is the same as the convex approximation in Eq. (14)."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we present the experimental results on two different tasks of sequence labelling, offline handwriting recognition and phoneme recognition. The performance of Hessian-free optimization for MDRNN with the proposed matrix is compared with the one of stochastic gradient descent (SGD) optimization on the same settings."
    }, {
      "heading" : "5.1 Database and preprocessing",
      "text" : "IFN/ENIT Database [20] is a database of handwritten Arabic words, which consists of 32,492 images written by 411 writers. The entire dataset has 5 subsets (a, b, c, d, e). The 25,955 images corresponding to the subsets (b − e) are used for training. The validation set consists of 3,269 images\ncorresponding to the first half of the sorted list in alphabetical order (ae07 001.tif − ai54 028.tif) in set a. Rest of the images in set a, which amounts to 3,268, are used for test. The intensity of pixels is centered and scaled using the mean and standard deviation calculated from the training set.\nTIMIT corpus [21] is a benchmark database for evaluating speech recognition performance. The standard training, validation, and core dataset are used for performance evaluation. Each set contains 3,696 sentences, 400 sentences, and 192 sentences respectively. Mel spectrum with 26 coefficients is used as a feature vector with a pre-emphasis filter, 25 ms window size, and 10 ms shift size. Each input feature of the training set is normalized to have zero mean and unit variance. Similarly, the features of core and validation sets are centered and scaled using the mean and standard deviation of the training set."
    }, {
      "heading" : "5.2 Experimental setup",
      "text" : "For handwriting recognition, the basic architecture was adopted from the one proposed in [3]. Deeper networks were constructed by replacing the top layer by more layers. The number of LSTM cells in the augmented layer was chosen to make the total number of weights between different networks similar to each other. Detailed architectures are described in Table 1 with results.\nFor phoneme recognition, deep bidirectional LSTM and CTC in [4] was adopted as the basic architecture. Additionally, the memory cell block [8], in which the cells share the gates, was applied for efficient information sharing. Each LSTM block was constrained to have 10 memory cells.\nWe have found that using a large value of bias for input/output gates is beneficial for training deep MDRNN. A possible explanation is that the activation of neurons is exponentially decayed by input/output gates during the propagation. Thus, setting large bias values for those gates may help sending information through many layers at the beginning of the learning. For this reason, biases of input and output gates were initialized to 2, whereas the ones for forget gates and memory cells were initialized to 0. All the other weight parameters of MDRNN were initialized randomly from a uniform distribution in the range of [−0.1, 0.1].\nLabel error rate was used as the metric for performance evaluation along with the average loss of CTC in Eq. (8). It is defined by the edit distance which sums the total number of insertions, deletions, and substitutions required to match two given sequences. The final performance in Table 1 and 2 was evaluated using the weight parameters which gave the best label error rate on the validation set. To map output probabilities to a label sequence, best path decoding [1] was used for Arabic handwriting, and beam search decoding [4, 22] with the beam width of 100 was used for phoneme recognition. For phoneme recognition, 61 phoneme labels were used during training and decoding, and then mapped to 39 classes for calculating the phoneme error rate (PER) in Table 2 [4, 23] .\nFor phoneme recognition, the regularization method suggested in [24] was used. We applied Gaussian weight noise of standard deviation σ = {0.03, 0.04, 0.05} along with L2 regularization of strength 0.001. Table 2 presents the best result from different values of σ. The network was first trained without noise, then it was initialized to the weights that gave the lowest CTC loss on the validation set. After that, the network was retrained with Gaussian weight noise [4]."
    }, {
      "heading" : "5.2.1 Parameters",
      "text" : "For HF optimization, we followed the basic setup described in [9], but different parameters were utilized. Tikhonov damping were used along with Levenberg-Marquardt heuristics. The value of the damping parameter λ was initialized to 0.1, and adjusted according to the reduction ratio ρ (multiplied by 0.9 if ρ > 0.75, divided by 0.9 if ρ < 0.25, and unchanged otherwise). The initial search direction for each run of CG was set to the CG direction found by the previous HF iteration decayed by 0.7. To ensure that CG follows the descent direction, we continued to perform minimum 5 and maximum 30 more CG iterations after it found the first descent direction. We terminated CG at iteration i before reaching the maximum iteration if the following condition is satisfied: (φ(xi)− φ(xi−5))/φ(xi) < 0.005 where φ is the quadratic objective of CG without offset. The training data was divided into 100 and 50 mini-batches for handwriting and phoneme recognition experiments respectively, and used for both of the gradient and matrix-vector product calculation. The learning was stopped if any of two criteria did not improve for 20 epochs in handwriting recognition and for 10 epochs in phoneme recognition, respectively.\nFor SGD optimization, the learning rate ǫ was chosen from {10−4, 10−5, 10−6}, and the momentum µ from {0.9, 0.95, 0.99}. For handwriting recognition, the best performance from all the possible combinations of parameters is presented in Table 1. For phoneme recognition, the best parameters out of 9 candidates for each network were selected after initialization (training without weight noise) based on the CTC loss. Then the networks were trained with weight noise. Additionally, the backpropagated error in LSTM layer was clipped to stay in the range [−1, 1] for stable learning [25]. The learning was stopped after 1000 epochs had been processed. Note that in order to guarantee the convergence, we selected a conservative criteria compared to the reference where the network converged after 85 epochs in handwriting recognition [3] and after 55-150 epochs in phoneme recognition [4]."
    }, {
      "heading" : "5.3 Results",
      "text" : "Table 1 presents the label error rate on the test set for handwriting recognition. In all cases, the networks trained using HF optimization outperformed the ones using SGD. The advantage of using HF is more pronounced as the depth increases. The improvements from deeper architecture can be seen with the error rate dropping from 6.1% to 4.5% as the depth increases from 3 to 13.\nTable 2 shows the phoneme error rate (PER) on the core set for phoneme recognition. The improved performance according to the depth can be observed for both optimization methods. The best PER for HF is 18.54% at 15 layers , and the one for SGD is 18.46% at 10 layers, which are comparable to the one in [4] where the reported results are PER 18.6% from the network with 3 layers having 3.8 million weights and PER 18.4% from the network with 5 layers having 6.8 million weights. The benefit from deeper network is obvious in terms of the number of weight parameters, although this is not meant to be the definitive performance comparison due to different preprocessing. The advantage of HF is not prominent for the experiments using TIMIT database. One explanation is that the networks tend to overfit to relatively small number of the training data samples, which removes the advantage of using advanced optimization techniques."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Hessian-free optimization as an approach for successful learning of deep MDRNN, in conjunction with CTC, has been presented. To apply HF to CTC, a convex approximation of its objective function has been explored. Improvements in performance are seen as the depth of the network increases for both HF and SGD. HF shows significantly better performance for handwriting recognition compared to SGD, and comparable performance for speech recognition."
    }, {
      "heading" : "A R operator to LSTM",
      "text" : "We follow the version of LSTM in [4]. The forward pass of LSTM is to calculate the following functions:\nit = σ(Wxixt +Whiht−1 +Wcict−1 + bi),\nft = σ(Wxfxt +Whfht−1 +Wcf ct−1 + bf ),\nct = ft · ct−1 + it · tanh(Wxcxt +Whcht−1 + bc),\not = σ(Wxoxt +Whoht−1Wcoct + bo),\nht = ot · tanh(ct),\nwhere · denotes the element-wise vector product, σ is the logistic sigmoid function, x, h, and c are the input, hidden, and cell activation vector respectively, and i, o, and f are the input, output, and forget gates respectively. All the gates and cells have the same size as the hidden vector h.\nApplying R operator to the above equations gives\nRv(it) = σ ′(Wxixt +Whiht−1 +Wcict−1 + bi)\n· (Vxixt + Vhiht−1 + Vcict−1 + Vi +WhiRv(ht−1) +WciRv(ct−1)),\nRv(ft) = σ ′(Wxfxt +Whfht−1 +Wcfct−1 + bf )\n· (Vxfxt + Vhfht−1 + Vcfct−1 + Vf +WhfRv(ht−1) +WcfRv(ct−1)),\nRv(ct) = Rv(ft) · ct−1 + ft · Rv(ct−1) +Rv(it) · tanh(Wxcxt +Whcht−1 + bc)\n+ it · tanh ′(Wxcxt +Whcht−1 + bc) · (Vxcxt + Vhcht−1 + Vc +WhcRv(ht−1)),\nRv(ot) = σ ′(Wxoxt +Whoht−1 +Wcoct + bo),\n· (Vxoxt + Vhoht−1 + Vcoct + Vo +WhoRv(ht−1) +WcoRv(ct)),\nRv(ht) = Rv(ot) · tanh(ct) + ot · tanh ′(ct) · Rv(ct),\nwhere Vij and Vi are taken from v at the same point of Wij and bi in θ, respectively. Note that θ and v have the same dimension."
    }, {
      "heading" : "B Detailed derivation of the proposed approximation from the Fisher information matrix",
      "text" : "The derivative of the negative log probability of Eq. (7) is given by\n− ∂ log p(l|a)\n∂atk = ytk −\n1\np(l|a)\n∑\ns∈lab(l,k)\nαt(s)βt(s). (19)\nwhere αt(s) and βt(s) denote forward and backward variables respectively, and lab(l, k) = {u|lu = k} is the set of positions where label k occurs in l [1, 3]. For compact notation, let Y t denote a column matrix containing ytk as its k-th element, and let V\nt denote a column matrix containing vtk = 1 p(l|a) ∑ s∈lab(l,k) αt(s)βt(s) as its k-th element.\nThe Fisher information matrix [16] is defined by\nF = Ex\n[\nEl∼p(l|x)\n[\n(\n∂ log p(l|x, θ) ∂θ\n)⊤ ( ∂ log p(l|x, θ)\n∂θ\n)\n]]\n. (20)\nThe Fisher information matrix of MDRNN using CTC is written as\nF = Ex\n[\nEl∼p(l|x)\n[\n(\n∂ log p(l|a) ∂a JN\n)⊤ ( ∂ log p(l|a)\n∂a JN\n)\n]]\n(21)\n= Ex\n[\nJ⊤NEl∼p(l|a)\n[\n(\n∂ log p(l|a) ∂a\n)⊤ ( ∂ log p(l|a)\n∂a\n)\n]\nJN\n]\n, (22)\nwhere a = a(x, θ) is the KT -dimensional output of the network N . The last step follows from that JN is independent of l.\nCTC assumes output probabilities at each timestep to be independent of those at other timesteps [1], therefore its Fisher information matrix is given as the sum of every timestep. It follows that\nF = Ex\n[\n∑\nt\nJ⊤NtEl∼p(l|a)\n[\n(\n∂ log p(l|a) ∂at\n)⊤ ( ∂ log p(l|a)\n∂at\n)\n]\nJNt\n]\n(23)\n= Ex\n[\n∑\nt\nJ⊤NtEl∼p(l|a)\n[\n( Y t − V t ) ( Y t − V t )⊤\n]\nJNt\n]\n(24)\n= Ex\n[\n∑\nt\nJ⊤Nt\n(\nY tY t ⊤ − Y tEl [ V t ]⊤ − El [ V t ] Y t ⊤ + El\n[ V tV t ⊤ ])\nJNt\n]\n, (25)\nwhere Y t and V t are defined above.\nEl[v t k] is given by\nEl[v t k] = El∼p(l|a)\n\n\n1\np(l|a)\n∑\ns∈lab(l,k)\nαt(s)βt(s)\n\n (26)\n= ∑\nl\n∑\ns∈lab(l,k)\nαt(s)βt(s) (27)\n= ytk. (28)\nEl[v t iv t j ] is given by\nEl[v t iv t j ] = El∼p(l|a)\n\n\n1\np(l|a)2 ∑\ns∈lab(l,i)\nαt(s)βt(s) ∑\ns∈lab(l,j)\nαt(s)βt(s)\n\n . (29)\nUnfortunately Eq. (29) cannot be analytically calculated in general. We apply the sufficient condition for the proposed approximation to be exact in section 4.4. By the assumption of one dominant path in a label sequence, El[vtiv t j ] = 0 for i 6= j. If the dominant path visits i at time t, ∑\ns∈lab(l,i) αt(s)βt(s) = p(l|a). Otherwise ∑\ns∈lab(l,i) αt(s)βt(s) = 0. Under this condition, Eq. (29) can be written as\nEl[v t iv t j ] = δij\n∑\nl\n∑\ns∈lab(l,i)\nαt(s)βt(s) (30)\n= δijy t i , (31)\nwhere δij is Kronecker delta. Substituting El[V t] = Y t and El[V tV t ⊤ ] = diag(Y t) into Eq. (25) gives\nF = Ex\n[\n∑\nt\nJ⊤Nt(diag(Y t)− Y tY t ⊤ )JNt\n]\n, (32)\nwhich is the same form as Eq. (4) and (14) combined."
    } ],
    "references" : [ {
      "title" : "Supervised sequence labelling with recurrent neural networks, volume 385",
      "author" : [ "Alex Graves" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Unconstrained on-line handwriting recognition with recurrent neural networks",
      "author" : [ "Alex Graves", "Marcus Liwicki", "Horst Bunke", "Jürgen Schmidhuber", "Santiago Fernández" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Offline handwriting recognition with multidimensional recurrent neural networks",
      "author" : [ "Alex Graves", "Jürgen Schmidhuber" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alex Graves", "Abdel-ranhman Mohamed", "Geoffrey Hinton" ],
      "venue" : "In Proceedings of ICASSP,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1412.6550,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "Geoffrey E Hinton", "Ruslan R Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Deep learning via Hessian-free optimization",
      "author" : [ "James Martens" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Learning recurrent neural networks with Hessian-free optimization",
      "author" : [ "James Martens", "Ilya Sutskever" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Fast curvature matrix-vector products for second-order gradient descent",
      "author" : [ "Nicol N Schraudolph" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "Fast exact multiplication by the hessian",
      "author" : [ "Barak A Pearlmutter" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1994
    }, {
      "title" : "Training deep and recurrent networks with Hessian-free optimization",
      "author" : [ "James Martens", "Ilya Sutskever" ],
      "venue" : "In Neural Networks: Tricks of the Trade,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber" ],
      "venue" : "In Proceedings of the 23rd International Conference on Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen Boyd", "Lieven Vandenberghe", "editors" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "Shun-Ichi Amari" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1998
    }, {
      "title" : "Revisiting natural gradient for deep networks",
      "author" : [ "Razvan Pascanu", "Yoshua Bengio" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Adaptive natural gradient learning algorithms for various stochastic models",
      "author" : [ "Hyeyoung Park", "S-I Amari", "Kenji Fukumizu" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2000
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "Christopher M. Bishop", "editor" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "IFN/ENIT-database of handwritten arabic words",
      "author" : [ "Mario Pechwitz", "S Snoussi Maddouri", "Volker Märgner", "Noureddine Ellouze", "Hamid Amiri" ],
      "venue" : "In Proceedings of CIFED,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "Sequence transduction with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "In ICML Representation Learning Workshop,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Speaker-independent phone recognition using hidden markov models",
      "author" : [ "Kai-Fu Lee", "Hsiao-Wuen Hon" ],
      "venue" : "IEEE Transactions on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1989
    }, {
      "title" : "Practical variational inference for neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Multidimensional recurrent neural network (MDRNN) is an efficient architecture to build multidimensional context into recurrent neural networks [1].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : "End-to-end training of MDRNN in conjunction with connectionist temporal classification (CTC) has shown the state-of-the-art performance in on/off-line handwriting recognition [2, 3] and speech recognition [4].",
      "startOffset" : 175,
      "endOffset" : 181
    }, {
      "referenceID" : 2,
      "context" : "End-to-end training of MDRNN in conjunction with connectionist temporal classification (CTC) has shown the state-of-the-art performance in on/off-line handwriting recognition [2, 3] and speech recognition [4].",
      "startOffset" : 175,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "End-to-end training of MDRNN in conjunction with connectionist temporal classification (CTC) has shown the state-of-the-art performance in on/off-line handwriting recognition [2, 3] and speech recognition [4].",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 4,
      "context" : "In previous approaches, the performance of MDRNN has been demonstrated with the networks having up to depth of 5 layers, which are relatively limited compared to the recent progress on feedforward networks [5].",
      "startOffset" : 206,
      "endOffset" : 209
    }, {
      "referenceID" : 5,
      "context" : "Notable breakthrough was made where deep feedforward neural networks were initialized using layer-wise pre-training [6].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "Recently, there has been approaches to add supervision to intermediate layers to train deep networks [5, 7].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "To our knowledge, no such pre-training or bootstrapping method has been developed for MDRNN which potentially utilizes LSTM cells [8] as its hidden unit.",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "Alternatively, HF optimization is an appealing approach to train deep neural networks due to its ability to overcome pathological curvature of the objective function [9].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 7,
      "context" : "The recent success of HF to deep feedforward and recurrent neural networks [9, 10] encourages the use of HF to MDRNN.",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "The recent success of HF to deep feedforward and recurrent neural networks [9, 10] encourages the use of HF to MDRNN.",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "2 Multidimensional recurrent neural networks MDRNN is a generalization of RNN to process multidimensional data by replacing the single recurrent connection with as many connections as dimensions of the data [1].",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 6,
      "context" : "To enhance its ability of exploiting context information, long short-term memory (LSTM) [8] cells are usually utilized as hidden units.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "In addition, stacking MDRNNs to construct deeper networks further improves the performance as the depth increases, reporting the state-of-the-art performance in phoneme recognition [4].",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 9,
      "context" : "For neural networks, an efficient way to compute Gv was proposed by [11], extending the work of [12].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : "For neural networks, an efficient way to compute Gv was proposed by [11], extending the work of [12].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "An indefinite Hessian matrix is problematic for 2nd-order optimization because it defines an unbounded local quadratic approximation [13].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "For nonlinear systems, the Hessian is not necessarily positive semidefinite, thus the GGN matrix is used as an approximation of the Hessian [11, 9].",
      "startOffset" : 140,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "For nonlinear systems, the Hessian is not necessarily positive semidefinite, thus the GGN matrix is used as an approximation of the Hessian [11, 9].",
      "startOffset" : 140,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "In principle, it is best to define L and N in a way that L performs as much of the computation as possible, with the positive semidefiniteness of HL as a minimum requirement [13].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "In practice, a nonlinear output layer along with its matching loss function [11], such as the softmax function with cross-entropy loss, is widely used.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "Thus, JN v can be written using a differential operator JN v = Rv(N (θ)) [12], and the properties of the operator can be utilized for efficient computation.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "Connectioninst temporal classification (CTC) [14] provides an objective function of learning MDRNN for sequence labelling.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "The function − log(y k), corresponding to the softmax with cross-entropy loss, is convex [11].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "Whereas log-concavity is closed under multiplication, the sum of log-concave functions is not log-concave in general [15].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "loss function matches the softmax output layer [11], the CTC objective is convex except the part which computes fz for each of the label sequences.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 14,
      "context" : "5 Derivation of the proposed approximation from the Fisher information matrix The identity of the GGN and the Fisher information matrix [16] has been shown for the network using the softmax with cross-entropy loss [17, 18].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "5 Derivation of the proposed approximation from the Fisher information matrix The identity of the GGN and the Fisher information matrix [16] has been shown for the network using the softmax with cross-entropy loss [17, 18].",
      "startOffset" : 214,
      "endOffset" : 222
    }, {
      "referenceID" : 16,
      "context" : "5 Derivation of the proposed approximation from the Fisher information matrix The identity of the GGN and the Fisher information matrix [16] has been shown for the network using the softmax with cross-entropy loss [17, 18].",
      "startOffset" : 214,
      "endOffset" : 222
    }, {
      "referenceID" : 0,
      "context" : "CTC assumes output probabilities at each timestep to be independent of those at other timesteps [1], therefore its Fisher information matrix is given as the sum of every timestep.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 17,
      "context" : "6 EM interpretation of the proposed approximation The goal of the Expectation-Maximization (EM) algorithm is to find the maximum likelihood solution for models having latent variables [19].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 18,
      "context" : "1 Database and preprocessing IFN/ENIT Database [20] is a database of handwritten Arabic words, which consists of 32,492 images written by 411 writers.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "2 Experimental setup For handwriting recognition, the basic architecture was adopted from the one proposed in [3].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : "For phoneme recognition, deep bidirectional LSTM and CTC in [4] was adopted as the basic architecture.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "Additionally, the memory cell block [8], in which the cells share the gates, was applied for efficient information sharing.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "To map output probabilities to a label sequence, best path decoding [1] was used for Arabic handwriting, and beam search decoding [4, 22] with the beam width of 100 was used for phoneme recognition.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "To map output probabilities to a label sequence, best path decoding [1] was used for Arabic handwriting, and beam search decoding [4, 22] with the beam width of 100 was used for phoneme recognition.",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 19,
      "context" : "To map output probabilities to a label sequence, best path decoding [1] was used for Arabic handwriting, and beam search decoding [4, 22] with the beam width of 100 was used for phoneme recognition.",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "For phoneme recognition, 61 phoneme labels were used during training and decoding, and then mapped to 39 classes for calculating the phoneme error rate (PER) in Table 2 [4, 23] .",
      "startOffset" : 169,
      "endOffset" : 176
    }, {
      "referenceID" : 20,
      "context" : "For phoneme recognition, 61 phoneme labels were used during training and decoding, and then mapped to 39 classes for calculating the phoneme error rate (PER) in Table 2 [4, 23] .",
      "startOffset" : 169,
      "endOffset" : 176
    }, {
      "referenceID" : 21,
      "context" : "For phoneme recognition, the regularization method suggested in [24] was used.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "After that, the network was retrained with Gaussian weight noise [4].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "1 Parameters For HF optimization, we followed the basic setup described in [9], but different parameters were utilized.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Note that in order to guarantee the convergence, we selected a conservative criteria compared to the reference where the network converged after 85 epochs in handwriting recognition [3] and after 55-150 epochs in phoneme recognition [4].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 3,
      "context" : "Note that in order to guarantee the convergence, we selected a conservative criteria compared to the reference where the network converged after 85 epochs in handwriting recognition [3] and after 55-150 epochs in phoneme recognition [4].",
      "startOffset" : 233,
      "endOffset" : 236
    }, {
      "referenceID" : 3,
      "context" : "46% at 10 layers, which are comparable to the one in [4] where the reported results are PER 18.",
      "startOffset" : 53,
      "endOffset" : 56
    } ],
    "year" : 2017,
    "abstractText" : "Multidimensional recurrent neural network (MDRNN) has shown a remarkable performance in speech and handwriting recognition. The performance of MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by Hessian-free (HF) optimization. Considering that connectionist temporal classification (CTC) is utilized as an objective of learning MDRNN for sequence labelling, the non-convexity of CTC poses a problem to apply HF to the network. As a solution to this, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. MDRNN up to the depth of 15 layers is successfully trained using HF, resulting in improved performance for sequence labelling.",
    "creator" : "LaTeX with hyperref package"
  }
}