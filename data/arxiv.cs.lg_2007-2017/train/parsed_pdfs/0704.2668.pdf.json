{
  "name" : "0704.2668.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Supervised Feature Selection via Dependence Estimation",
    "authors" : [ "Le Song", "Alex Smola", "Arthur Gretton", "Justin Bedo" ],
    "emails" : [ "lesong@it.usyd.edu.au", "alex.smola@gmail.com", "arthur.gretton@tuebingen.mpg.de", "borgwardt@dbs.ifi.lmu.de", "bedo@ieee.org" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In supervised learning problems, we are typically given m data points x ∈ X and their labels y ∈ Y. The task is to find a functional dependence between x and y, f : x 7−→ y, subject to certain optimality conditions. Representative tasks include binary classification, multi-class classification, regression and ranking. We often want to reduce the dimension of the data (the number of features) before the actual learning (Guyon & Elisseeff, 2003); a larger number of features can be associated with higher data collection cost, more difficulty in model interpretation, higher computational cost for the classifier, and decreased generalisation\nAppearing in Proceedings of the 24 th International Conference on Machine Learning, Corvallis, OR, 2007. Copyright 2007 by the author(s)/owner(s).\nability. It is therefore important to select an informative feature subset.\nThe problem of supervised feature selection can be cast as a combinatorial optimisation problem. We have a full set of features, denoted S (whose elements correspond to the dimensions of the data). We use these features to predict a particular outcome, for instance the presence of cancer: clearly, only a subset T of features will be relevant. Suppose the relevance of T to the outcome is quantified by Q(T ), and is computed by restricting the data to the dimensions in T . Feature selection can then be formulated as\nT 0 = arg max T ⊆S Q(T ) subject to | T | ≤ t, (1)\nwhere | · | computes the cardinality of a set and t upper bounds the number of selected features. Two important aspects of problem (1) are the choice of the criterion Q(T ) and the selection algorithm.\nFeature Selection Criterion. The choice of Q(T ) should respect the underlying supervised learning tasks — estimate dependence function f from training data and guarantee f predicts well on test data. Therefore, good criteria should satisfy two conditions:\nI: Q(T ) is capable of detecting any desired (nonlinear as well as linear) functional dependence between the data and labels. II: Q(T ) is concentrated with respect to the underlying measure. This guarantees with high probability that the detected functional dependence is preserved in the test data.\nar X\niv :0\n70 4.\n26 68\nv1 [\ncs .L\nG ]\n2 0\nWhile many feature selection criteria have been explored, few take these two conditions explicitly into account. Examples include the leave-one-out error bound of SVM (Weston et al., 2000) and the mutual information (Koller & Sahami, 1996). Although the latter has good theoretical justification, it requires density estimation, which is problematic for high dimensional and continuous variables. We sidestep these problems by employing a mutual-information like quantity — the Hilbert Schmidt Independence Criterion (HSIC) (Gretton et al., 2005). HSIC uses kernels for measuring dependence and does not require density estimation. HSIC also has good uniform convergence guarantees. As we show in section 2, HSIC satisfies conditions I and II, required for Q(T ).\nFeature Selection Algorithm. Finding a global optimum for (1) is in general NP-hard (Weston et al., 2003). Many algorithms transform (1) into a continuous problem by introducing weights on the dimensions (Weston et al., 2000, 2003). These methods perform well for linearly separable problems. For nonlinear problems, however, the optimisation usually becomes non-convex and a local optimum does not necessarily provide good features. Greedy approaches – forward selection and backward elimination – are often used to tackle problem (1) directly. Forward selection tries to increase Q(T ) as much as possible for each inclusion of features, and backward elimination tries to achieve this for each deletion of features (Guyon et al., 2002). Although forward selection is computationally more efficient, backward elimination provides better features in general since the features are assessed within the context of all others.\nBAHSIC. In principle, HSIC can be employed using either the forwards or backwards strategy, or a mix of strategies. However, in this paper, we will focus on a backward elimination algorithm. Our experiments show that backward elimination outperforms forward selection for HSIC. Backward elimination using HSIC (BAHSIC) is a filter method for feature selection. It selects features independent of a particular classifier. Such decoupling not only facilitates subsequent feature interpretation but also speeds up the computation over wrapper and embedded methods.\nFurthermore, BAHSIC is directly applicable to binary, multiclass, and regression problems. Most other feature selection methods are only formulated either for binary classification or regression. The multi-class extension of these methods is usually accomplished using a one-versus-the-rest strategy. Still fewer methods handle classification and regression cases at the same time. BAHSIC, on the other hand, accommodates all\nthese cases in a principled way: by choosing different kernels, BAHSIC also subsumes many existing methods as special cases. The versatility of BAHSIC originates from the generality of HSIC. Therefore, we begin our exposition with an introduction of HSIC."
    }, {
      "heading" : "2 Measures of Dependence",
      "text" : "We define X and Y broadly as two domains from which we draw samples (x, y): these may be real valued, vector valued, class labels, strings, graphs, and so on. We define a (possibly nonlinear) mapping φ(x) ∈ F from each x ∈ X to a feature space F , such that the inner product between the features is given by a kernel function k(x, x′) := 〈φ(x), φ(x′)〉: F is called a reproducing kernel Hilbert space (RKHS). Likewise, let G be a second RKHS on Y with kernel l(·, ·) and feature map ψ(y). We may now define a cross-covariance operator between these feature maps, in accordance with Baker (1973); Fukumizu et al. (2004): this is a linear operator Cxy : G 7−→ F such that\nCxy = Exy[(φ(x)− µx)⊗ (ψ(y)− µy)], (2)\nwhere ⊗ is the tensor product. The square of the Hilbert-Schmidt norm of the cross-covariance operator (HSIC), ‖ Cxy ‖2HS, is then used as our feature selection criterion Q(T ). Gretton et al. (2005) show that HSIC can be expressed in terms of kernels as\nHSIC(F ,G,Pr xy ) = ‖ Cxy ‖2HS (3)\n= Exx′yy′ [k(x, x′)l(y, y′)] + Exx′ [k(x, x′)] Eyy′ [l(y, y′)] − 2 Exy[Ex′ [k(x, x′)] Ey′ [l(y, y′)]],\nwhere Exx′yy′ is the expectation over both (x, y) ∼ Prxy and an additional pair of variables (x′, y′) ∼ Prxy drawn independently according to the same law. Previous work used HSIC to measure independence between two sets of random variables (Gretton et al., 2005). Here we use it to select a subset T from the first full set of random variables S. We now describe further properties of HSIC which support its use as a feature selection criterion.\nProperty (I) Gretton et al. (2005, Theorem 4) show that whenever F ,G are RKHSs with universal kernels k, l on respective compact domains X and Y in the sense of Steinwart (2002), then HSIC(F ,G,Prxy) = 0 if and only if x and y are independent. In terms of feature selection, a universal kernel such as the Gaussian RBF kernel or the Laplace kernel permits HSIC to detect any dependence between X and Y. HSIC is zero if and only if features and labels are independent.\nIn fact, non-universal kernels can also be used for HSIC, although they may not guarantee that all de-\npendencies are detected. Different kernels incorporate distinctive prior knowledge into the dependence estimation, and they focus HSIC on dependence of a certain type. For instance, a linear kernel requires HSIC to seek only second order dependence. Clearly HSIC is capable of finding and exploiting dependence of a much more general nature by kernels on graphs, strings, or other discrete domains.\nProperty (II) Given a sample Z = {(x1, y1), . . . , (xm, ym)} of size m drawn from Prxy, we derive an unbiased estimate of HSIC,\nHSIC(F ,G, Z) (4)\n= 1m(m−3) [tr(KL) + 1>K11> L1 (m−1)(m−2) − 2 m−2 1 >K L 1],\nwhere K and L are computed as Kij = (1 − δij)k(xi, xj) and Lij = (1 − δij)l(yi, yj). Note that the diagonal entries of K and L are set to zero. The following theorem, a formal statement that the empirical HSIC is unbiased, is proved in the appendix.\nTheorem 1 (HSIC is Unbiased) Let EZ denote the expectation taken over m independent observations (xi, yi) drawn from Prxy. Then\nHSIC(F ,G,Pr xy ) = EZ [HSIC(F ,G, Z)] . (5)\nThis property is by contrast with the mutual information, which can require sophisticated bias correction strategies (e.g. Nemenman et al., 2002).\nU-Statistics. The estimator in (4) can be alternatively formulated using U-statistics,\nHSIC(F ,G, Z) = (m)−14 m∑\n(i,j,q,r)∈im4\nh(i, j, q, r), (6)\nwhere (m)n = m!(m−n)! is the Pochhammer coefficient and where imr denotes the set of all r-tuples drawn without replacement from {1, . . . ,m}. The kernel h of the U-statistic is defined by\n1 4! (i,j,q,r)∑ (s,t,u,v) (Kst Lst + Kst Luv −2 Kst Lsu) , (7)\nwhere the sum in (7) represents all ordered quadruples (s, t, u, v) selected without replacement from (i, j, q, r).\nWe now show that HSIC(F ,G, Z) is concentrated. Furthermore, its convergence in probability to HSIC(F ,G,Prxy) occurs with rate 1/ √ m which is a slight improvement over the convergence of the biased estimator by Gretton et al. (2005).\nTheorem 2 (HSIC is Concentrated) Assume k, l are bounded almost everywhere by 1, and are nonnegative. Then for m > 1 and all δ > 0, with probability at least 1− δ for all Prxy\n|HSIC(F ,G, Z)−HSIC(F ,G,Pr xy\n)| ≤ 8 √ log(2/δ)/m\nBy virtue of (6) we see immediately that HSIC is a U-statistic of order 4, where each term is bounded in [−2, 2]. Applying Hoeffing’s bound as in Gretton et al. (2005) proves the result.\nThese two theorems imply the empirical HSIC closely reflects its population counterpart. This means the same features should consistently be selected to achieve high dependence if the data are repeatedly drawn from the same distribution.\nAsymptotic Normality. It follows from Serfling (1980) that under the assumptions E(h2) < ∞ and that the data and labels are not independent, the empirical HSIC converges in distribution to a Gaussian random variable with mean HSIC(F ,G,Prxy) and variance\nσ2HSIC = 16 m\n( R−HSIC2 ) , where (8)\nR = 1m m∑ i=1 ( (m− 1)−13 ∑ (j,q,r)∈im3 \\{i} h(i, j, q, r) )2 ,\nand imr \\{i} denotes the set of all r-tuples drawn without replacement from {1, . . . ,m} \\ {i}. The asymptotic normality allows us to formulate statistics for a significance test. This is useful because it may provide an assessment of the dependence between the selected features and the labels.\nSimple Computation. Note that HSIC(F ,G, Z) is simple to compute, since only the kernel matrices K and L are needed, and no density estimation is involved. For feature selection, L is fixed through the whole process. It can be precomputed and stored for speedup if needed. Note also that HSIC(F ,G, Z) does not need any explicit regularisation parameter. This is encapsulated in the choice of the kernels."
    }, {
      "heading" : "3 Feature Selection via HSIC",
      "text" : "Having defined our feature selection criterion, we now describe an algorithm that conducts feature selection on the basis of this dependence measure. Using HSIC, we can perform both backward (BAHSIC) and forward (FOHSIC) selection of the features. In particular, when we use a linear kernel on the data (there is no such requirement for the labels), forward selection\nand backward selection are equivalent: the objective function decomposes into individual coordinates, and thus feature selection can be done without recursion in one go. Although forward selection is computationally more efficient, backward elimination in general yields better features, since the quality of the features is assessed within the context of all other features. Hence we present the backward elimination version of our algorithm here (a forward greedy selection version can be derived similarly).\nBAHSIC appends the features from S to the end of a list S† so that the elements towards the end of S† have higher relevance to the learning task. The feature selection problem in (1) can be solved by simply taking the last t elements from S†. Our algorithm produces S† recursively, eliminating the least relevant features from S and adding them to the end of S† at each iteration. For convenience, we also denote HSIC as HSIC(σ,S), where S are the features used in computing the data kernel matrix K, and σ is the parameter for the data kernel (for instance, this might be the size of a Gaussian kernel k(x, x′) = exp(−σ ‖x− x′‖2) ).\nAlgorithm 1 BAHSIC Input: The full set of features S Output: An ordered set of features S†\n1: S† ← ∅ 2: repeat 3: σ ← Ξ 4: I ← arg maxI ∑ j∈I HSIC(σ,S \\{j}), I ⊂ S 5: S ← S \\I 6: S† ← S† ∪I 7: until S = ∅\nStep 3 of the algorithm denotes a policy for adapting the kernel parameters, e.g. by optimising over the possible parameter choices. In our experiments, we typically normalize each feature separately to zero mean and unit variance, and adapt the parameter for a Gaussian kernel by setting σ to 1/(2d), where d = | S | − 1. If we have prior knowledge about the type of nonlinearity, we can use a kernel with fixed parameters for BAHSIC. In this case, step 3 can be omitted.\nStep 4 of the algorithm is concerned with the selection of a set I of features to eliminate. While one could choose a single element of S, this would be inefficient when there are a large number of irrelevant features. On the other hand, removing too many features at once risks the loss of relevant features. In our experiments, we found a good compromise between speed and feature quality was to remove 10% of the current\nfeatures at each iteration."
    }, {
      "heading" : "4 Connections to Other Approaches",
      "text" : "We now explore connections to other feature selectors. For binary classification, an alternative criterion for selecting features is to check whether the distributions Pr(x|y = 1) and Pr(x|y = −1) differ. For this purpose one could use Maximum Mean Discrepancy (MMD) (Borgwardt et al., 2006). Likewise, one could use Kernel Target Alignment (KTA) (Cristianini et al., 2003) to test directly whether there exists any correlation between data and labels. KTA has been used for feature selection. Formally it is defined as tr K L /‖K ‖‖L ‖. For computational convenience the normalisation is often omitted in practise (Neumann et al., 2005), which leaves us with tr K L. We discuss this unnormalised variant below.\nLet us consider the output kernel l(y, y′) = ρ(y)ρ(y′), where ρ(1) = m−1+ and ρ(−1) = −m−1− , and m+ and m− are the numbers of positive and negative samples, respectively. With this kernel choice, we show that MMD and KTA are closely related to HSIC. The following theorem is proved in the appendix.\nTheorem 3 (Connection to MMD and KTA) Assume the kernel k(x, x′) for the data is bounded and the kernel for the labels is l(y, y′) = ρ(y)ρ(y′). Then∣∣HSIC− (m− 1)−2MMD∣∣ = O(m−1)∣∣HSIC− (m− 1)−2KTA∣∣ = O(m−1). This means selecting features that maximise HSIC also maximises MMD and KTA. Note that in general (multiclass, regression, or generic binary classification) this connection does not hold."
    }, {
      "heading" : "5 Variants of BAHSIC",
      "text" : "New variants can be readily derived from BAHSIC by combining the two building blocks of BAHSIC: a kernel on the data and another one on the labels. Here we provide three examples using a Gaussian kernel on the data, while varying the kernel on the labels. This provides us with feature selectors for three problems:\nBinary classification (BIN) We set m−1+ as the label for positive class members, and m−1− for negative class members. We then apply a linear kernel.\nMulticlass classification (MUL) We apply a linear kernel on the labels using the label vectors below, as described for a 3-class example. Here mi is the number\nof samples in class i and 1mi denotes a vector of all ones with length mi.\nY =  1m1 m1\n1m1 m2−m 1m1 m3−m\n1m2 m1−m\n1m2 m2\n1m2 m3−m\n1m3 m1−m 1m3 m2−m\n1m3 m3  m×3 . (9)\nRegression (REG) A Gaussian RBF kernel is also used on the labels. For convenience the kernel width σ is fixed as the median distance between points in the sample (Schölkopf & Smola, 2002).\nFor the above variants a further speedup of BAHSIC is possible by updating the entries of the kernel matrix incrementally, since we are using an RBF kernel. We use the fact that ‖x − x′‖2 = ∑ j ‖xj − x′j‖2. Hence ‖x − x′‖2 needs to be computed only once. Subsequent updates are effected by subtracting ‖xj − x′j‖2 (subscript here indices dimension).\nWe will use BIN, MUL and REG as the particular instances of BAHSIC in our experiments. We will refer to them commonly as BAHSIC since the exact meaning will be clear depending on the datasets encountered. Furthermore, we also instantiate FOHSIC using the same kernels as BIN, MUL and REG, and we adopt the same convention when we refer to it in our experiments."
    }, {
      "heading" : "6 Experimental Results",
      "text" : "We conducted three sets of experiments. The characteristics of the datasets and the aims of the experiments are: (i) artificial datasets illustrating the properties of BAHSIC; (ii) real datasets that compare BAHSIC with other methods; and (iii) a brain computer interface dataset showing that BAHSIC selects meaningful features."
    }, {
      "heading" : "6.1 Artificial datasets",
      "text" : "We constructed 3 artificial datasets, as illustrated in Figure 1, to illustrate the difference between BAHSIC variants with linear and nonlinear kernels. Each dataset has 22 dimensions — only the first two dimensions are related to the prediction task and the rest are just Gaussian noise. These datasets are (i) Binary XOR data: samples belonging to the same class have multimodal distributions; (ii) Multiclass data: there are 4 classes but 3 of them are collinear; (iii) Nonlinear regression data: labels are related to the first two dimension of the data by y = x1 exp(−x21−x22)+ , where denotes additive Gaussian noise. We compare BAHSIC to FOHSIC, Pearson’s correlation, mutual information (Zaffalon & Hutter, 2002), and RELIEF (RELIEF works only for binary problems). We aim to show that when nonlinear dependencies exist in the\ndata, BAHSIC with nonlinear kernels is very competent in finding them.\nWe instantiate the artificial datasets over a range of sample sizes (from 40 to 400), and plot the median rank, produced by various methods, for the first two dimensions of the data. All numbers in Figure 1 are averaged over 10 runs. In all cases, BAHSIC shows good performance. More specifically, we observe:\nBinary XOR Both BAHSIC and RELIEF correctly select the first two dimensions of the data even for small sample sizes; while FOHSIC, Pearson’s correlation, and mutual information fail. This is because the latter three evaluate the goodness of each feature independently. Hence they are unable to capture nonlinear interaction between features.\nMulticlass Data BAHSIC, FOHSIC and mutual information select the correct features irrespective of the size of the sample. Pearson’s correlation only works for large sample size. The collinearity of 3 classes provides linear correlation between the data and the labels, but due to the interference of the fourth class such corre-\nlation is picked up by Pearson’s correlation only for a large sample size.\nNonlinear Regression Data The performance of Pearson’s correlation and mutual information is slightly better than random. BAHSIC and FOHSIC quickly converge to the correct answer as the sample size increases.\nIn fact, we observe that as the sample size increases, BAHSIC is able to rank the relevant features (the first two dimensions) almost correctly in the first iteration (results not shown). While this does not prove BAHSIC with nonlinear kernels is always better than that with a linear kernel, it illustrates the competence of BAHSIC in detecting nonlinear features. This is obviously useful in a real-world situations. The second advantage of BAHSIC is that it is readily applicable to both classification and regression problems, by simply choosing a different kernel on the labels."
    }, {
      "heading" : "6.2 Real world datasets",
      "text" : "Algorithms In this experiment, we show that the performance of BAHSIC can be comparable to other state-of-the-art feature selectors, namely SVM Recursive Feature Elimination (RFE) (Guyon et al., 2002), RELIEF (Kira & Rendell, 1992), L0-norm SVM ( L0) (Weston et al., 2003), and R2W2 (Weston et al., 2000). We used the implementation of these algorithms as given in the Spider machine learning toolbox, since those were the only publicly available implementations.1 Furthermore, we also include filter methods, namely FOHSIC, Pearson’s correlation (PC), and mutual information (MI), in our comparisons.\nDatasets We used various real world datasets taken from the UCI repository,2 the Statlib repository,3 the LibSVM website,4 and the NIPS feature selection challenge5 for comparison. Due to scalability issues in Spider, we produced a balanced random sample of size less than 2000 for datasets with more than 2000 samples.\nExperimental Protocol We report the performance of an SVM using a Gaussian kernel on a feature subset of size 5 and 10-fold cross-validation. These 5 features were selected per fold using different methods. Since we are comparing the selected features, we used the same SVM for all methods: a Gaussian kernel with σ set as the median distance between points in the sample (Schölkopf & Smola, 2002) and regular-\n1http://www.kyb.tuebingen.mpg.de/bs/people/spider 2http://www.ics.uci.edu/ mlearn/MLSummary.html 3http://lib.stat.cmu.edu/datasets/ 4http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/ 5http://clopinet.com/isabelle/Projects/NIPS2003/\nization parameter C = 100. On classification datasets, we measured the performance using the error rate, and on regression datasets we used the percentage of variance not-explained (also known as 1−r2). The results for binary datasets are summarized in the first part of Table 1. Those for multiclass and regression datasets are reported respectively in the second and the third parts of Table 1.\nTo provide a concise summary of the performance of various methods on binary datasets, we measured how the methods compare with the best performing one in each dataset in Table 1. We recorded the best absolute performance of all feature selectors as the baseline, and computed the distance of each algorithm to the best possible result. In this context it makes sense to penalize catastrophic failures more than small deviations. In other words, we would like to have a method which is at least almost always very close to the best performing one. Taking the `2 distance achieves this effect, by penalizing larger differences more heavily. It is also our goal to choose an algorithm that performs homogeneously well across all datasets. The `2 distance scores are listed for the binary datasets in Table 1. In general, the smaller the `2 distance, the better the method. In this respect, BAHSIC and FOHSIC have the best performance. We did not produce the `2 distance for multiclass and regression datasets, since the limited number of such datasets did not allow us to draw statistically significant conclusions."
    }, {
      "heading" : "6.3 Brain-computer interface dataset",
      "text" : "In this experiment, we show that BAHSIC selects features that are meaningful in practise: we use BAHSIC to select a frequency band for a brain-computer interface (BCI) data set from the Berlin BCI group (Dornhege et al., 2004). The data contains EEG signals (118 channels, sampled at 100 Hz) from five healthy subjects (‘aa’, ‘al’, ‘av’, ‘aw’ and ‘ay’) recorded during two types of motor imaginations. The task is to classify the imagination for individual trials.\nOur experiment proceeded in 3 steps: (i) A Fast Fourier transformation (FFT) was performed on each channel and the power spectrum was computed. (ii) The power spectra from all channels were averaged to obtain a single spectrum for each trial. (iii) BAH-\nTable 1: Classification error (%) or percentage of variance not-explained (%). The best result, and those results not significantly worse than it, are highlighted in bold (one-sided Welch t-test with 95% confidence level). 100.0±0.0∗: program is not finished in a week or crashed. -: not applicable.\nData BAHSIC FOHSIC PC MI RFE RELIEF L0 R2W2 covertype 26.3±1.5 37.9±1.7 40.3±1.3 26.7±1.1 33.0±1.9 42.7±0.7 43.4±0.7 44.2±1.7 ionosphere 12.3±1.7 12.8±1.6 12.3±1.5 13.1±1.7 20.2±2.2 11.7±2.0 35.9±0.4 13.7±2.7\nsonar 27.9±3.1 25.0±2.3 25.5±2.4 26.9±1.9 21.6±3.4 24.0±2.4 36.5±3.3 32.3±1.8 heart 14.8±2.4 14.4±2.4 16.7±2.4 15.2±2.5 21.9±3.0 21.9±3.4 30.7±2.8 19.3±2.6 breastcancer 3.8±0.4 3.8±0.4 4.0±0.4 3.5±0.5 3.4±0.6 3.1±0.3 32.7±2.3 3.4±0.4 australian 14.3±1.3 14.3±1.3 14.5±1.3 14.5±1.3 14.8±1.2 14.5±1.3 35.9±1.0 14.5±1.3\nsplice 22.6±1.1 22.6±1.1 22.8±0.9 21.9±1.0 20.7±1.0 22.3±1.0 45.2±1.2 24.0±1.0 svmguide3 20.8±0.6 20.9±0.6 21.2±0.6 20.4±0.7 21.0±0.7 21.6±0.4 23.3±0.3 23.9±0.2\nadult 24.8±0.2 24.4±0.6 18.3±1.1 21.6±1.1 21.3±0.9 24.4±0.2 24.7±0.1 100.0±0.0∗ cleveland 19.0±2.1 20.5±1.9 21.9±1.7 19.5±2.2 20.9±2.1 22.4±2.5 25.2±0.6 21.5±1.3\nderm 0.3±0.3 0.3±0.3 0.3±0.3 0.3±0.3 0.3±0.3 0.3±0.3 24.3±2.6 0.3±0.3 hepatitis 13.8±3.5 15.0±2.5 15.0±4.1 15.0±4.1 15.0±2.5 17.5±2.0 16.3±1.9 17.5±2.0\nmusk 29.9±2.5 29.6±1.8 26.9±2.0 31.9±2.0 34.7±2.5 27.7±1.6 42.6±2.2 36.4±2.4 optdigits 0.5±0.2 0.5±0.2 0.5±0.2 3.4±0.6 3.0±1.6 0.9±0.3 12.5±1.7 0.8±0.3\nspecft 20.0±2.8 20.0±2.8 18.8±3.4 18.8±3.4 37.5±6.7 26.3±3.5 36.3±4.4 31.3±3.4 wdbc 5.3±0.6 5.3±0.6 5.3±0.7 6.7±0.5 7.7±1.8 7.2±1.0 16.7±2.7 6.8±1.2 wine 1.7±1.1 1.7±1.1 1.7±1.1 1.7±1.1 3.4±1.4 4.2±1.9 25.1±7.2 1.7±1.1\ngerman 29.2±1.9 29.2±1.8 26.2±1.5 26.2±1.7 27.2±2.4 33.2±1.1 32.0±0.0 24.8±1.4 gisette 12.4±1.0 13.0±0.9 16.0±0.7 50.0±0.0 42.8±1.3 16.7±0.6 42.7±0.7 100.0±0.0∗ arcene 22.0±5.1 19.0±3.1 31.0±3.5 45.0±2.7 34.0±4.5 30.0±3.9 46.0±6.2 32.0±5.5 madelon 37.9±0.8 38.0±0.7 38.4±0.6 51.6±1.0 41.5±0.8 38.6±0.7 51.3±1.1 100.0±0.0∗ `2 11.2 14.8 19.7 48.6 42.2 25.9 85.0 138.3 satimage 15.8±1.0 17.9±0.8 52.6±1.7 22.7±0.9 18.7±1.3 - 22.1±1.8 - segment 28.6±1.3 33.9±0.9 22.9±0.5 27.1±1.3 24.5±0.8 - 68.7±7.1 - vehicle 36.4±1.5 48.7±2.2 42.8±1.4 45.8±2.5 35.7±1.3 - 40.7±1.4 -\nsvmguide2 22.8±2.7 22.2±2.8 26.4±2.5 27.4±1.6 35.6±1.3 - 34.5±1.7 - vowel 44.7±2.0 44.7±2.0 48.1±2.0 45.4±2.2 51.9±2.0 - 85.6±1.0 - usps 43.4±1.3 43.4±1.3 73.7±2.2 67.8±1.8 55.8±2.6 - 67.0±2.2 -\nhousing 18.5±2.6 18.9±3.6 25.3±2.5 18.9±2.7 - - - - bodyfat 3.5±2.5 3.5±2.5 3.4±2.5 3.4±2.5 - - - - abalone 55.1±2.7 55.9±2.9 54.2±3.3 56.5±2.6 - - - -\nFigure 2: HSIC, encoded by the colour value for different frequency bands (axes correspond to upper and lower cutoff frequencies). The figures, left to right, top to bottom correspond to subjects ‘aa’, ‘al’, ‘av’, ‘aw’ and ‘ay’.\nSIC was used to select the top 5 discriminative frequency components based on the power spectrum. The 5 selected frequencies and their 4 nearest neighbours were used to reconstruct the temporal signals (with all other Fourier coefficients eliminated). The result was then passed to a normal CSP method (Dornhege et al., 2004) for feature extraction, and then classified using a linear SVM.\nWe compared automatic filtering using BAHSIC to other filtering approaches: normal CSP method with manual filtering (8-40 Hz), the CSSP method (Lemm et al., 2005), and the CSSSP method (Dornhege et al., 2006). All results presented in Table 2 are obtained using 50× 2-fold cross-validation. Our method is very competitive and obtains the first and second place for 4 of the 5 subjects. While the CSSP and the CSSSP methods are specialised embedded methods (w.r.t. the CSP method) for frequency selection on BCI data, our\nmethod is entirely generic: BAHSIC decouples feature selection from CSP.\nIn Figure 2, we use HSIC to visualise the responsiveness of different frequency bands to motor imagination. The horizontal and the vertical axes in each subfigure represent the lower and upper bounds for a frequency band, respectively. HSIC is computed for each of these bands. Dornhege et al. (2006) report that the µ rhythm (approx. 12 Hz) of EEG is most responsive to motor imagination, and that the β rhythm (approx. 22 Hz) is also responsive. We expect that HSIC will create a strong peak at the µ rhythm and a weaker peak at the β rhythm, and the absence of other responsive frequency components will create block patterns. Both predictions are confirmed in Figure 2. Furthermore, the large area of the red region for subject ‘al’ indicates good responsiveness of his µ rhythm. This also corresponds well with the lowest classification er-\nror obtained for him in Table 2."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This paper proposes a backward elimination procedure for feature selection using the Hilbert-Schmidt Independence Criterion (HSIC). The idea behind the resulting algorithm, BAHSIC, is to choose the feature subset that maximises the dependence between the data and labels. With this interpretation, BAHSIC provides a unified feature selection framework for any form of supervised learning. The absence of bias and good convergence properties of the empirical HSIC estimate provide a strong theoretical jutification for using HSIC in this context. Although BAHSIC is a filter method, it still demonstrates good performance compared with more specialised methods in both artificial and real world data. It is also very competitive in terms of runtime performance.6\nAcknowledgments NICTA is funded through the Australian Government’s Baking Australia’s Ability initiative, in part through the ARC.This research was supported by the Pascal Network (IST-2002-506778).\nAppendix\nProof [Theorem 1] Recall that Kii = Lii = 0. We prove the claim by constructing unbiased estimators for each term in (3). Note that we have three types of expectations, namely Exy Ex′y′ , a partially decoupled expectation Exy Ex′ Ey′ , and Ex Ey Ex′ Ey′ , which takes all four expectations independently.\nIf we want to replace the expectations by empirical averages, we need to take care to avoid using the same discrete indices more than once for independent random variables. In other words, when taking expectations over r independent random variables, we need rtuples of indices where each index occurs exactly once. The sets imr satisfy this property. Their cardinalities are given by the Pochhammer symbols (m)r. Jointly drawn random variables, on the other hand, share the same index. We have\nExy Ex′y′ [k(x, x′)l(y, y′)] = EZ [ (m)−12 ∑ (i,j)∈im2 Kij Lij ]\n= EZ [ (m)−12 tr K L ] .\nIn the case of the expectation over three independent terms Exy Ex′ Ey′ we obtain\nEZ [ (m)−13 ∑ (i,j,q)∈im3 Kij Liq ] = EZ [ (m)−13 1 >K L 1− tr K L ] .\n6Code is freely available as part of the Elefant package at http://elefant.developer.nicta.com.au.\nFor four independent random variables Ex Ey Ex′ Ey′ , EZ [ (m)−14 ∑ (i,j,q,r)∈im4 Kij Lqr ]\n= EZ [ (m)−14 ( 1>K 1 1> L 1−4 1>K L 1 +2 tr K L )] .\nTo obtain an expression for HSIC we only need to take linear combinations using (3). Collecting terms related to tr K L, 1>K L 1, and 1>K 1 1> L 1 yields\nHSIC(F ,G,Pr xy ) = 1m(m−3) EZ [ tr K L +1 >K11> L1 (m−1)(m−2) − 2 m−2 1 >K L 1 ] .\nThis is the expected value of HSIC[F ,G, Z].\nProof [Theorem 3] We first relate a biased estimator of HSIC to the biased estimator of MMD. The former is given by\n1 (m−1)2 tr KHLH where H = I−m −1 1 1>\nand the bias is bounded by O(m−1), as shown by Gretton et al. (2005). An estimator of MMD with bias O(m−1) is\nMMD[F , Z] = 1 m2+ m+∑ i,j k(xi,xj) + 1 m2− m−∑ i,j k(xi,xj)\n− 2 m+m− m+∑ i m−∑ j k(xi,xj) = tr K L .\nIf we choose l(y, y′) = ρ(y)ρ(y′) with ρ(1) = m−1+ and ρ(−1) = m−1− , we can see L 1 = 0. In this case tr K H L H = tr K L, which shows that the biased estimators of MMD and HSIC are identical up to a constant factor. Since the bias of tr K H L H is O(m−1), this implies the same bias for the MMD estimate.\nTo see the same result for Kernel Target Alignment, note that for equal class size the normalisations with regard to m+ and m− become irrelevant, which yields the corresponding MMD term."
    } ],
    "references" : [ {
      "title" : "Joint measures and cross-covariance operators",
      "author" : [ "C. Baker" ],
      "venue" : "Transactions of the American Mathematical Society,",
      "citeRegEx" : "Baker,? \\Q1973\\E",
      "shortCiteRegEx" : "Baker",
      "year" : 1973
    }, {
      "title" : "Integrating structured biological data by kernel maximum mean discrepancy",
      "author" : [ "K.M. Borgwardt", "A. Gretton", "M.J. Rasch", "H.P. Kriegel", "B. Schölkopf", "A.J. Smola" ],
      "venue" : "Bioinformatics (ISMB),",
      "citeRegEx" : "Borgwardt et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Borgwardt et al\\.",
      "year" : 2006
    }, {
      "title" : "On optimizing kernel alignment",
      "author" : [ "N. Cristianini", "J. Kandola", "A. Elisseeff", "J. Shawe-Taylor" ],
      "venue" : "Tech. rep., UC Davis Department of Statistics",
      "citeRegEx" : "Cristianini et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Cristianini et al\\.",
      "year" : 2003
    }, {
      "title" : "Boosting bit rates in non-invasive EEG singletrial classifications by feature combination and multiclass paradigms",
      "author" : [ "G. Dornhege", "B. Blankertz", "G. Curio", "K. Müller" ],
      "venue" : "IEEE Trans. Biomed. Eng.,",
      "citeRegEx" : "Dornhege et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Dornhege et al\\.",
      "year" : 2004
    }, {
      "title" : "Optimizing spatio-temporal filters for improving BCI",
      "author" : [ "G. Dornhege", "B. Blankertz", "M. Krauledat", "F. Losch", "G. Curio", "K. Müller" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Dornhege et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dornhege et al\\.",
      "year" : 2006
    }, {
      "title" : "Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces",
      "author" : [ "K. Fukumizu", "F.R. Bach", "M.I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "Fukumizu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Fukumizu et al\\.",
      "year" : 2004
    }, {
      "title" : "Measuring statistical dependence with HilbertSchmidt norms",
      "author" : [ "A. Gretton", "O. Bousquet", "A. Smola", "B. Schölkopf" ],
      "venue" : "In ALT,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2005
    }, {
      "title" : "An introduction to variable and feature selection",
      "author" : [ "I. Guyon", "A. Elisseeff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Guyon and Elisseeff,? \\Q2003\\E",
      "shortCiteRegEx" : "Guyon and Elisseeff",
      "year" : 2003
    }, {
      "title" : "Gene selection for cancer classification using support vector machines",
      "author" : [ "I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Guyon et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Guyon et al\\.",
      "year" : 2002
    }, {
      "title" : "A practical approach to feature selection",
      "author" : [ "K. Kira", "L. Rendell" ],
      "venue" : "In Proc. 9th Intl. Workshop on Machine Learning,",
      "citeRegEx" : "Kira and Rendell,? \\Q1992\\E",
      "shortCiteRegEx" : "Kira and Rendell",
      "year" : 1992
    }, {
      "title" : "Toward optimal feature selection",
      "author" : [ "D. Koller", "M. Sahami" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Koller and Sahami,? \\Q1996\\E",
      "shortCiteRegEx" : "Koller and Sahami",
      "year" : 1996
    }, {
      "title" : "Spatio-spectral filters for improving the classification of single trial EEG",
      "author" : [ "S. Lemm", "B. Blankertz", "G. Curio", "Mülller", "K.-R" ],
      "venue" : "IEEE Trans. Biomed. Eng.,",
      "citeRegEx" : "Lemm et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Lemm et al\\.",
      "year" : 2005
    }, {
      "title" : "Entropy and inference, revisited",
      "author" : [ "I. Nemenman", "F. Shafee", "W. Bialek" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Nemenman et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Nemenman et al\\.",
      "year" : 2002
    }, {
      "title" : "Combined SVM-based feature selection and classification",
      "author" : [ "J. Neumann", "C. Schnörr", "G. Steidl" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Neumann et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Neumann et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning with Kernels",
      "author" : [ "B. Schölkopf", "A. Smola" ],
      "venue" : null,
      "citeRegEx" : "Schölkopf and Smola,? \\Q2002\\E",
      "shortCiteRegEx" : "Schölkopf and Smola",
      "year" : 2002
    }, {
      "title" : "Approximation Theorems of Mathematical Statistics",
      "author" : [ "R. Serfling" ],
      "venue" : null,
      "citeRegEx" : "Serfling,? \\Q1980\\E",
      "shortCiteRegEx" : "Serfling",
      "year" : 1980
    }, {
      "title" : "On the influence of the kernel on the consistency of svms",
      "author" : [ "I. Steinwart" ],
      "venue" : null,
      "citeRegEx" : "Steinwart,? \\Q2002\\E",
      "shortCiteRegEx" : "Steinwart",
      "year" : 2002
    }, {
      "title" : "Use of zero-norm with linear models and kernel methods",
      "author" : [ "J. Weston", "A. Elisseeff", "B. Schölkopf", "M. Tipping" ],
      "venue" : null,
      "citeRegEx" : "Weston et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2003
    }, {
      "title" : "Feature selection for SVMs",
      "author" : [ "J. Weston", "S. Mukherjee", "O. Chapelle", "M. Pontil", "T. Poggio", "V. Vapnik" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Weston et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2000
    }, {
      "title" : "Robust feature selection using distributions of mutual information",
      "author" : [ "M. Zaffalon", "M. Hutter" ],
      "venue" : "In UAI",
      "citeRegEx" : "Zaffalon and Hutter,? \\Q2002\\E",
      "shortCiteRegEx" : "Zaffalon and Hutter",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Examples include the leave-one-out error bound of SVM (Weston et al., 2000) and the mutual information (Koller & Sahami, 1996).",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "We sidestep these problems by employing a mutual-information like quantity — the Hilbert Schmidt Independence Criterion (HSIC) (Gretton et al., 2005).",
      "startOffset" : 127,
      "endOffset" : 149
    }, {
      "referenceID" : 17,
      "context" : "Finding a global optimum for (1) is in general NP-hard (Weston et al., 2003).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "Forward selection tries to increase Q(T ) as much as possible for each inclusion of features, and backward elimination tries to achieve this for each deletion of features (Guyon et al., 2002).",
      "startOffset" : 171,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "We may now define a cross-covariance operator between these feature maps, in accordance with Baker (1973); Fukumizu et al.",
      "startOffset" : 93,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "We may now define a cross-covariance operator between these feature maps, in accordance with Baker (1973); Fukumizu et al. (2004): this is a linear operator Cxy : G 7−→ F such that",
      "startOffset" : 93,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "Gretton et al. (2005) show that HSIC can be expressed in terms of kernels as",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "Previous work used HSIC to measure independence between two sets of random variables (Gretton et al., 2005).",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "Property (I) Gretton et al. (2005, Theorem 4) show that whenever F ,G are RKHSs with universal kernels k, l on respective compact domains X and Y in the sense of Steinwart (2002), then HSIC(F ,G,Prxy) = 0 if and only if x and y are independent.",
      "startOffset" : 13,
      "endOffset" : 179
    }, {
      "referenceID" : 6,
      "context" : "Furthermore, its convergence in probability to HSIC(F ,G,Prxy) occurs with rate 1/ √ m which is a slight improvement over the convergence of the biased estimator by Gretton et al. (2005). Theorem 2 (HSIC is Concentrated) Assume k, l are bounded almost everywhere by 1, and are nonnegative.",
      "startOffset" : 165,
      "endOffset" : 187
    }, {
      "referenceID" : 6,
      "context" : "Applying Hoeffing’s bound as in Gretton et al. (2005) proves the result.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : "It follows from Serfling (1980) that under the assumptions E(h) < ∞ and that the data and labels are not independent, the empirical HSIC converges in distribution to a Gaussian random variable with mean HSIC(F ,G,Prxy) and variance",
      "startOffset" : 16,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "For this purpose one could use Maximum Mean Discrepancy (MMD) (Borgwardt et al., 2006).",
      "startOffset" : 62,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "Likewise, one could use Kernel Target Alignment (KTA) (Cristianini et al., 2003) to test directly whether there exists any correlation between data and labels.",
      "startOffset" : 54,
      "endOffset" : 80
    }, {
      "referenceID" : 13,
      "context" : "For computational convenience the normalisation is often omitted in practise (Neumann et al., 2005), which leaves us with tr K L.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "Algorithms In this experiment, we show that the performance of BAHSIC can be comparable to other state-of-the-art feature selectors, namely SVM Recursive Feature Elimination (RFE) (Guyon et al., 2002), RELIEF (Kira & Rendell, 1992), L0-norm SVM ( L0) (Weston et al.",
      "startOffset" : 180,
      "endOffset" : 200
    }, {
      "referenceID" : 17,
      "context" : ", 2002), RELIEF (Kira & Rendell, 1992), L0-norm SVM ( L0) (Weston et al., 2003), and R2W2 (Weston et al.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : ", 2003), and R2W2 (Weston et al., 2000).",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "In this experiment, we show that BAHSIC selects features that are meaningful in practise: we use BAHSIC to select a frequency band for a brain-computer interface (BCI) data set from the Berlin BCI group (Dornhege et al., 2004).",
      "startOffset" : 203,
      "endOffset" : 226
    }, {
      "referenceID" : 3,
      "context" : "The result was then passed to a normal CSP method (Dornhege et al., 2004) for feature extraction, and then classified using a linear SVM.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 11,
      "context" : "We compared automatic filtering using BAHSIC to other filtering approaches: normal CSP method with manual filtering (8-40 Hz), the CSSP method (Lemm et al., 2005), and the CSSSP method (Dornhege et al.",
      "startOffset" : 143,
      "endOffset" : 162
    }, {
      "referenceID" : 4,
      "context" : ", 2005), and the CSSSP method (Dornhege et al., 2006).",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "Dornhege et al. (2006) report that the μ rhythm (approx.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "and the bias is bounded by O(m−1), as shown by Gretton et al. (2005). An estimator of MMD with bias O(m−1) is",
      "startOffset" : 47,
      "endOffset" : 69
    } ],
    "year" : 2013,
    "abstractText" : "We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.",
    "creator" : "TeX"
  }
}