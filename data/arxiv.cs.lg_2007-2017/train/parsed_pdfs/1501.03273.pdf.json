{
  "name" : "1501.03273.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Classification with Low Rank and Missing Data",
    "authors" : [ "Elad Hazan", "Roi Livni", "Yishay Mansour" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The importance of handling correctly missing data is a fundamental and classical challenge in machine learning. There are many reasons why data might be missing. For example, consider the medical domain, some data might be missing because certain procedures were not performed on a given patient, other data might be missing because the patient choose not to disclose them, and even some data might be missing due to malfunction of certain equipment. While it is definitely much better to have always complete and accurate data, this utopian desire is not the reality many times. For this reason we need to utilize the available data even if some of it is missing.\nAnother, very different motivation for missing data are recommendations. For example, a movie recommendations dataset might have users opinions on certain movies, which is the case, for example, in the Netflix motion picture dataset. Clearly, no user has seen or reviewed all movies, or even close to it. In this respect recommendation data is an extreme case: the vast majority is usually missing (i.e., it is sparse to the extreme).\nMany times we can solve the missing data problem since the data resides on a lower dimension manifold. In the above examples, if there are prototypical users (or patients) and any user is a mixture of the prototypical users, then this implicitly suggests that the data is low rank. Another way to formalize this assumption is to consider the data in a matrix form, say, the users are rows and movies are columns, then our assumption is that the true complete matrix has a low rank.\nOur starting point is to consider the low rank assumption, but to avoid any explicit matrix completion, and instead directly dive in to the classification problem. At the end of the introduction we show that matrix completion is neither sufficient and/or necessary.\nWe consider perhaps the most fundamental data analysis technique of the machine learning toolkit: linear (and kernel) classification, as applied to data where some (or even most) of the attributes in an example might be missing. Our main result is an efficient algorithm for linear and kernel classification that performs as well as the best classifier that has access to all data, under low rank assumption with natural non-degeneracy conditions.\nWe stress that our result is worst case, we do not assume that the missing data follows any probabilistic rule other than the underlying matrix having low rank. This is a clear contrast to most existing matrix completion algorithms. We also cast our results in a distributional setting, showing that the classification error that we achieve is close to the best classification using the subspace of the examples (and with no missing data). Notably, many variants of the problem ∗Princeton University and Microsoft Research †Hebrew University and Microsoft Research ‡Tel Aviv University and Microsoft Research\nar X\niv :1\n50 1.\n03 27\n3v 1\n[ cs\n.L G\n] 1\n4 Ja\nn 20\n15\nof finding a hidden subspace are computationally hard (see e.g. Berthet & Rigollet (2013)), yet as we show, learning a linear classifier on a hidden subspace is non-properly learnable.\nAt a high level, we assume that a sample is a triplet (x,o, y), where x ∈ Rd is the complete example, o ⊂ {1, . . . , d} is the set of observable attributes and y ∈ Y is the label. The learner observes only (xo, y), where xo omits any attribute not in o. Our goal is given a sample S = {(x(i)o , y(i))}mi=1 to output a classifier hS such that w.h.p.:\nE [`(hS(xo), y)] ≤ min w∈Rd ‖w‖≤1 E [`(w · x, y)] + ,\nwhere ` is the loss function. Namely, we like our classifier hS to compete with the best linear classifier for the completely observable data.\nOur main result is achieving this task (under mild regularity conditions) using a computationally efficient algorithm for any convex Lipschitz-bounded loss function. Our basic result requires a sample size which is quasi-polynomial, but we complement it with a kernel construction which can guarantee efficient learning under appropriate large margin assumptions. Our kernel depends only on the intersection of observable values of two inputs, and is efficiently computable. (We give a more detailed overview of our main results in Section 2.)\nPreliminary experimental evidence indicates our theoretical contributions lead to promising classification performance both on synthetic data and on publicly-available recommendation data. This will be detailed in the full version of this paper.\nPrevious work. Classification with missing data is a well studied subject in statistics with numerous books and papers devoted to its study, (see, e.g., Little & Rubin (2002)). The statistical treatment of missing data is broad, and to a fairly large extent assumes parametric models both for the data generating process as well as the process that creates the missing data. One of the most popular models for the missing data process is Missing Completely at Random (MCAR) where the missing attributes are selected independently from the values.\nWe outline a few of the main approaches handling missing data in the statistics literature. The simplest method is simply to discard records with missing data, even this assumes independence between the examples with missing values and their labels. In order to estimate simple statistics, such as the expected value of an attribute, one can use importance sampling methods, where the probability of an attribute being missing can depend on it value (e.g., using the Horvitz-Thompson estimator Horvitz & Thompson (1952)). A large body of techniques is devoted to imputation procedures which complete the missing data. This can be done by replacing a missing attribute by its mean (mean imputation), or using a regression based on the observed value (regression imputation), or sampling the other examples to complete the missing value (hot deck). 1 The imputation methodologies share a similar goal as matrix completion, namely reduce the problem to one with complete data, however their methodologies and motivating scenarios are very different. Finally, one can build a complete Bayesian model for both the observed and unobserved data and use it to perform inference. As with almost any Bayesian methodology, its success depends largely on selecting the right model and prior, this is even ignoring the computational issues which make inference in many of those models computationally intractable.\nIn the machine learning community, missing data was considered in the framework of limited attribute observability Ben-David & Dichterman (1998) and its many refinements Dekel et al. (2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)). In our case the learner has no control which attributes are observable in an example and the domain is fixed. The latter case is captured in the work of Chechik et al. (2008), who rescale inner-products according to the amount of missing data. Their method, however, does not entail theoretical gaurantees on reconstruction in the worst case, and gives rise to non-convex programs.\nA natural and intuitive methodology to follow is to treat the labels (both known and unknown) as an additional column in the data matrix and complete the data using a matrix completion algorithm, thereby obtaining the classification. Indeed, this exactly was proposed by Goldberg et al. (2010). Although this is a natural approach, we show that\n1We remark that our model implicitly includes mean-imputation or 0-imputation method and therefore will always outperform them.\ncompletion is neither necessary nor sufficient for classification. Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic models with restricted distributions Srebro (2004); Candes & Recht (2009); Lee et al. (2010); Salakhutdinov & Srebro (2010); Shamir & Shalev-Shwartz (2011). The only nonprobabilistic matrix completion algorithm in the online learning setting we are aware of is Hazan et al. (2012), which we were not able to use for our purposes.\nIs matrix completion sufficient and/or necessary? We demonstrate that classification with missing data is provably different from that of matrix completion. We start by considering a learner that tries to complete the missing entries in an unsupervised manner and then performs classification on the completed data, this approach is close akin to imputation techniques, generative models and any other two step – unsupervised/supervised algorithm. Our example shows that even under realizable assumptions, such an algorithm may fail. We then proceed to analyze the approach previously mentioned – to treat the labels as an additional column.\nTo see that unsupervised completion is insufficient for prediction, consider the example in Figure 1: the original data is represented by filled red and green dots and it is linearly separable. Each data point will have one of its two coordinates missing (this can even be done at random. In the figure the arrow from each instance points to the observed attribute. However, the rank-one completion of projection onto the pink hyperplane is possible, and admits no separation. The problem is clearly that the mapping to a low dimension is independent from the labels, and therefore we should not expect that properties that depend on the labels, such as linear separability, will be maintained.\nNext, consider a learner that treats the labels as an additional column. Goldberg et al. (2010) Considered the following problem:\nminimize Z rank(Z) subject to: Zi,j = xi,j , (i, j) ∈ Ω , . (G)\nwhere Ω is the set of observed attributes (or observed labels for the corresponding columns). Now assume that we always see one of the following examples: [1, ∗, 1, ∗], [∗, − 1, ∗, − 1], or [1, ,−1, 1, − 1]. The observed labels are respectively 1,−1 and 1. A typical data matrix with one test point might be of the form:\nM =  1 ∗ 1 ∗ 1 ∗ −1 ∗ −1 −1 1 −1 1 −1 1 1 ∗ 1 ∗ ∗  (1) First note that there is no 1-rank completion of this matrix. On the other hand, we will show that there is more than one 2-rank completion each lead to a different classification of the test point. The first possible completion is to complete odd columns to a constant one vector, and even column vectors to a constant −1 vector. Then complete the labeling whichever way you choose. Clearly there is no hope for this completion to lead to any meaningful result as the label vector is independent of the data columns. On the other hand we may complete the first and last rows to a constant 1 vector, and the second row to a constant −1 vector. All possible completions lead to an optimal solution w.r.t Problem G but have different outcome w.r.t classification. We stress that this is not a sample complexity issue. Even if we observe abundant amount of data, the completion task is still ill-posed.\nFinally, matrix completion is also not necessary for prediction. Consider movie recommendation dataset with two separate populations, French and Chinese, where each population reviews a different set of movies. Even if each population has a low rank, performing successful matrix completion, in this case, is impossible (and intuitively it does not make sense in such a setting). However, linear classification in this case is possible via a single linear classifier, for example by setting all non-observed entries to zero. For a numerical example, return to the matrix M in Eq. 1. Note that we observe only three instances hence the classification task is easy but doesn’t lead to reconstruction of the missing entries."
    }, {
      "heading" : "2 Problem Setup and Main Result",
      "text" : "We begin by presenting the general setting: A vector with missing entries can be modeled as a tuple x × o, where x ∈ Rd and o ∈ 2d is a subset of indices. The vector x represents the full data and the set o represents the observed attributes. Given such a tuple, let us denote by xo a vector in (R ∪ {∗})d such that\n(xo)i = { xi i ∈ o ∗ else\nThe task of learning a linear classifier with missing data is to return a target function over xo that competes with best linear classifier over x. Specifically, a sequence of triplets {(x(i).o(i).yi)}mi=1 is drawn iid according to some distribution D. An algorithm is provided with the sample S = {(xioi , yi)} m i=1 and should return a target function fS over missing data such that w.h.p:\nE [`(fS(xo), y))] ≤ min w∈Bd(1) E [`(w · x, y))] + , (2)\nwhere ` is the loss function and Bd(r) denotes the Euclidean ball in dimension d of radius √ r. For brevity, we will say that a target function fS is -good if Eq. 2 holds. Without any assumptions on the distribution D, the task is ill-posed. One can construct examples where the learner over missing data doesn’t have enough information to compete with the best linear classifier. Such is the case when, e.g., yi is some attribute that is constantly concealed and independent of all other features. Therefore, certain assumptions on the distribution must be made.\nOne reasonable assumption is to assume that the marginal distribution D over x is supported on a small dimensional linear subspaceE and that for every set of observations, we can linearly reconstruct the vector x from the vector Pox, where Po : Rd → R|o| is the projection on the observed attributes. In other words, we demand that the mapping Po|E : E → PoE, which is the restriction of Po to E, is full-rank. As the learner doesn’t have access to the subspace E, the learning task is still far from trivial.\nWe give a precise definition of the last assumption in Assumption 1. Though our results hold under the low rank assumption the convergence rates we give depend on a certain regularity parameter. Roughly, we parametrize the ”distance” of Po|E from singularity, and our results will quantitively depend on this distance. Again, we defer all rigorous definitions to Section 3.2.\nOur first result is a an upper bound on the sample complexity of the problem. We then proceed to a more general statement that entails an efficient kernel-based algorithm."
    }, {
      "heading" : "2.1 Main Result",
      "text" : "Theorem 1 (Main Result). Assume that ` is a L-Lipschitz convex loss function Let D be a λ-regular distribution (see Definition 1) Let γ( ) ≥ log 2L/(λ )λ and\nΓ( ) = dγ( )+1 − d d− 1 .\nThere exists an algorithm (independent ofD) that receives a sample S = {(xioi , yi)} m i=1 of sizem ∈ Ω\n( L2Γ( )2 log 1/δ\n2 ) and returns a target function fS that is -good with probability at least (1− δ). The algorithm runs in time poly(|S|).\nTheorem 1 gives an upper bound on the computational and sample complexity of learning a linear classifier with missing data under the low rank assumption. As the sample complexity is quasipolynomial, this has limited practical value in many situations. However, as the next theorem states, fS can actually be computed by applying a kernel trick. Thus, under further large margin assumptions we can significantly improve performance.\nTheorem 2. For every γ ≥ 0, there exists an embedding over missing data\nφγ : xo → RΓ such that Γ = ∑γ k=1 d k = d γ+1−d d−1 , and the scalar product between two samples φγ(x 1 o1) and φγ(x 2 o2) can be efficiently computed, specifically it is given by the formula:\nkγ(x 1 o1 ,x 2 o2) := |o(1) ∩ o(2)|γ − 1 |o(1) ∩ o(2)| − 1\n× ∑\ni∈o(1)∩o(2) x\n(1) i · x (2) i .\nIn addition, let ` be an L-Lipschitz loss function and S = {xioi} m i=1 a sample drawn iid according to a distribution D. We make the assumption that ‖Pox‖ ≤ 1 a.s. The followings hold: 1. At each iteration of Alg. 1 we can efficiently compute v>t φγ(x t ot) for any new example x t ot . Specifically it is\ngiven by the formula\nv>t φγ(x t ot) := t∑ i=1 α (t) i k(x i oi ,x t ot).\nHence Alg. 1 runs in poly(T ) time and sequentially produces target functions ft(xo) = v>t φγ(xo) that can be computed at test time in poly(T ) time.\n2. Run Alg. 1 with ηt = Ct , ρ = 1 and T . Let v̄ = 1 T ∑m t=1 vt, then with probability (1− δ):\n1 2 ‖v̄‖2 + C m m∑ i=1 `(v̄>φγ(x i oi), yi) ≤ min 1 2 ‖v‖2 + C m m∑ i=1 [ `(v>φγ(x i oi), yi) ] + Õ ( (CL)2Γ ln 1/δ T ) . (3)\n3. For any > 0, if D is a λ-regular distribution and γ ≥ log 2L/(λ )λ then for some v ∗ ∈ BΓ(Γ)\nE [`(v∗ · φγ(xo, y)] ≤ min w∈Bd(1) E [`(w · φγ(xo), y)] + .\nTo summarize, Theorem 2 states that we can embed the sample points with missing attributes in a high dimensional, finite, Hilbert space of dimension Γ, such that:\n• The scalar product between embeded points can be computed efficiently. Hence, due to the conventional representer argument, the task of empirical risk minimization is tractable.\n• Following the conventional analysis of kernel methods: Under large margin assumptions in the ambient space, we can compute a predictor with scalable sample complexity and computational efficiency. • Finally, the best linear predictor over embedded sample points in a √\nΓ–ball is comparable to the best linear predictor over fully observed data.\nTaken together, we can learn a predictor with sample complexity Ω(Γ2( )/ 2 log 1δ ) and Theorem 1 holds. For completeness we present the method together with an efficient algorithm that optimizes the RHS of Eq. 3 via an SGD method. The optimization analysis is derived in a straightforward manner from the work of Shalev-Shwartz et al. (2011). Other optimization algorithms exist in the literature, and we chose this optimization method as it allows us to also derive regret bounds which are formally stronger (see Section 2.2). We stress that the main novelty of this paper is not in any specific optimization algorithm, but the introduction of a new kernel and our guarantees rely solely on it.\nFinally, note that φ1 induces the same scalar product as a 0-imputation. In that respect, by considering different γ = 1, 2, . . . and using a holdout set we can guarantee that our method will outperform the 0-imputation method. By normalizing or adding a bias term we can in fact compete with mean-imputation or any other first order imputation."
    }, {
      "heading" : "2.2 Regret minimization for joint subspace learning and classification",
      "text" : "A significant technical contribution of this manuscript is the agnostic learning of a subspace coupled with a linear classifier. A subspace is represented by a projection matrix Q ∈ Rd×d, which satisfies Q2 = Q. Denote the following class of target functions\nF0 = {fw,Q : w ∈ Bd, Q ∈Md×d, Q2 = Q}\nwhere fw,Q(xo) is the linear predictor defined by w over subspace defined by the matrix Q, as formally defined in definition 2.\nGiven the aforementioned efficient kernel mapping φγ , we consider the following kernel-gradient-based online algorithm for classification called KARMA (Kernelized Algorithm for Risk-minimization with Missing Attributes).\nAlgorithm 1 KARMA: Kernelized Algorithm for Risk-minimization with Missing Attributes 1: Input: parameters γ > 1, {ηt > 0}, 0 < ρ < 1, B > 0 2: for t = 1 to T do 3: Observe example (xtot , yt), suffer loss `(v > t φγ(x t ot), yt)\n4: Update\nα (t) i =  (1− ηtρ) · α(t−1)i i < t −ηt`′(v>t φγ(xtot)) i = t 0 else\nvt+1 = t∑ i=1 α (t) i φγ(x i oi)\n5: end for\nOur main result for the fully adversarial online setting is given next, and proved in the Appendix. Notice that the subspace E∗ and associated projection matrix Q∗ are chosen by an adversary and unknown to the algorithm.\nTheorem 3. For any γ > 1, λ > 0, X > 0, ρ > 0, B > 0, L-Lipschitz convex loss function `, and λ-regular sequence {(xt,ot, yt)} w.r.t subspace E∗ and associated projection matrix Q∗ such that ‖xt‖∞ < X , Run Algorithm 1 with {ηt = 1ρt}, sequentially outputs {vt ∈ R t} such that\n∑ t `(v>t φγ(x t ot), yt)− min‖w‖≤1 ∑ t `(fw,Q∗(x t ot), yt) ≤ 2L2X2Γ ρ (1 + log T ) + ρ 2 T ·B + e −λγ λ LT\nIn particular, taking ρ = LX √\nΓ√ BT , γ = 1λ log T we obtain∑ t `(v>t φγ(x t ot), yt)− min‖w‖≤1 ∑ t `(fw,Q∗(x t ot), yt) = O(XL √ ΓBT )"
    }, {
      "heading" : "3 Preliminaries and Notations",
      "text" : ""
    }, {
      "heading" : "3.1 Notations",
      "text" : "As discussed, we consider a model where a distribution D is fixed over Rd × O × Y , where O = 2d consists of all subsets of {1, . . . , d}. We will generally denote elements of Rd by x,w,v,u and elements of O by o. We denote by Bd the unit ball of Rd, and by Bd(r) the ball of radius √ r.\nGiven a subset o we denote by Po : Rd → R|o| the projection onto the indices in o, i.e., if i1 ≤ i2 ≤ · · · ≤ ik are the elements of o in increasing order then (Pox)j = xij . Given a matrix A and a set of indices o, we let\nAo,o = PoAP > o ."
    }, {
      "heading" : "3.2 Model Assumptions",
      "text" : "Definition 1 (λ-regularity). We say that D is λ-regular with associated subpsace E if the following happens with probability 1 (w.r.t the joint random variables (x,o)):\n1. ‖Pox‖ ≤ 1.\n2. x ∈ E.\n3. ker(PoPE) = ker(PE)\n4. If λo > 0 is a strictly positive singular value of the matrix PoPE then λo ≥ λ.\nAssumption 1 (Low Rank Assumption). We say that D satisfies the low rank assumption with asscoicated subspace E if it is λ-regular with associated subspace E for some λ > 0.\nNote that in our setting we assume that ‖Pox‖ ≤ 1 a.s. If ‖x‖ ≤ 1 then ‖Pox‖ ≤ 1 hence our assumption is weaker then assuming x is contained in a fixed sized ball. Further, the assumption can be verified on a sample set with missing attributes.\nNote also that we’ve normalized both w and xo. To achieve guarantees that scale with ‖w‖, note that we can replace the loss function `(w · x, y) with `(ρ · w · x, y) for any constant ρ. This will replace L–Lipschitness with ρ · L–Lipschitzness in all results."
    }, {
      "heading" : "4 Learning under low rank assumption and λ-regularity.",
      "text" : "Definition 2 (The class F0). We define the following class of target functions\nF0 = {fw,Q : w ∈ Bd(1), Q ∈Md×d, Q2 = Q}\nwhere fw,Q(xo) = (Pow) ·Q†o,o · (Pox).\n(Here M† denotes the pseudo inverse of M .)\nThe following Lemma states that, under the low rank assumption, the problem of linear learning with missing data is reduced to the problem of learning the class F0, in the sense that the hypothesis class F0 is not less-expressive.\nLemma 1. Let D be a distribution that satisfies the low rank assumption. For every w∗ ∈ Rd there is f∗w,Q ∈ F0 such that a.s:\nf∗w,Q(xo) = w ∗ · x.\nIn particular Q = PE and w = P>Ew ∗ , where PE is the projection matrix on the subspace E."
    }, {
      "heading" : "4.1 Approximating F0 under regularity",
      "text" : "We next define a surrogate class of target functions that approximates F0\nDefinition 3 (The classes Fγ). For every γ we define the following class\nFγ = {fγw,Q : w ∈ Bd(1), Q ∈ R d×d, Q2 = Q}\nwhere,\nfγw,Q(xo) = (Pow) · t−1∑ j=0 (Qo,o) j · (Pox)\nLemma 2. Let (x,o) be a sample drawn according to a λ-regular distribution D with associated subspace E. Let Q = PE and ‖w‖ ≤ 1 then a.s:\n‖fγw,I−Q(xo)− fw,Q(xo)‖ ≤ (1− λ)γ\nλ .\nCorollary 1. Let ` be a L-Lipschitz function. Under λ-regularity, for every γ ≥ logL/λ λ the class F γ contains an\n-good target function."
    }, {
      "heading" : "4.2 Improper learning of Fγ and a kernel trick",
      "text" : "Let G be the set of all finite, non empty, sequences of length at most γ over d. For each s ∈ G denote |s|– the length of the sequence and send the last element of the sequence. Given a set of observations o we write s ⊆ o if all elements of the sequence s belong to o. We let\nΓ = γ∑ j=1 dj = |G| = d γ+1 − d d− 1\nand we index the coordinates of RΓ by the elements of G:\nDefinition 4. We let φγ : (Rd ×O)→ RΓ be the embedding:\n(φγ(xo))s = { xsend s ⊆ o 0 else\nLemma 3. For every Q and w we have:\nfγw,Q(xo) = ∑ s1∈o ws1xs1 + ∑ {s:s⊆o, 2≤|s|≤t} ws1 ·Qs1,s2 ·Qs2,s3 · · ·Qs|s|−1,send · xsend\nCorollary 2. For every fγw,Q ∈ Fγ there is v ∈ BΓ(Γ), such that:\nfγw,Q(xo) = v · φγ(xo).\nAs a corllary, for every loss function ` and distribution D we have that:\nmin v∈BΓ(Γ) E [`(v · φ(xo), y)] ≤ min fγw,Q∈Fγ\nE [ `(fγw,Q(xo), y) ] Due to Corollary 2, learning Fγ can be improperly done via learning a linear classifier over the embedded sample set {φγ(xo)}mi=1. While the ambient space RΓ may be very large, the computational complexity of the next optimization scheme is actually dependent on the scalar product between the embedded samples. For that we give the following result that shows that the scalar product can be computed efficiently:\nTheorem 4. φγ(x (1) o1 ) · φγ(x (2) o2 ) =\n|o1 ∩ o2|γ − 1 |o1 ∩ o2| − 1 ∑ k∈o1∩o2 x (1) k x (2) k .\n(We use the convention that 1 γ−1 1−1 = limx→1 xγ−1 x−1 = γ)"
    }, {
      "heading" : "5 Discussion and future work",
      "text" : "We have described the first theoretically-sound method to cope with low rank missing data, giving rise to a classification algorithm that attains competitive error to that of the optimal linear classifier that has access to all data. Our non-proper agnostic framework for learning a hidden low-rank subspace comes with provable guarantees, whereas heuristics based on separate data reconstruction and classification are shown to fail for certain scenarios.\nOur technique is directly applicable to classification with low rank missing data and polynomial kernels via kernel (polynomial) composition. General kernels can be handled by polynomial approximation, but it is interesting to think about a more direct approach.\nIt is possible to derive all our results for a less stringent condition than λ-regularity: instead of bounding the smallest eigenvalue of the hidden subspace, it is possible to bound only the ratio of largest-to-smallest eigenvalue. This results in better bounds in a straightforward plug-and-play into our analysis, but was ommitted for simplicity."
    }, {
      "heading" : "A Proofs of theorems and lemmas from main text",
      "text" : ""
    }, {
      "heading" : "A.1 Technical Claims",
      "text" : "Claim 1. Let Q ∈Md×d be a square projection matrix and P ∈Mk×d a matrix. Recall that:\nIm(A) = {v : ∃u Au = v}, and ker(A) = {v : Av = 0}.\nAnd that rank(A) is the size of the largest collection of linearly independent columns of A. The following statements are equivalent:\n1. ker(PQ) = ker(Q).\n2. rank(PQ) = rank(QP>) = rank(PQP>) = rank(Q).\n3. Im(QP>) = Im(Q)."
    }, {
      "heading" : "Proof.",
      "text" : "1⇒ 2 Clearly rank(PQ) ≤ rank(Q). If rank(PQ) < rank(Q) we must have some collection of linearly independent columns of Q that are linearly dependent in PQ this implies that there is v such that PQv = 0 but Qv 6= 0. Hence ker(PQ) 6= ker(Q) and thus a contradiction, we conclude that rank(PQ) = rank(Q). That rank(PQ) = rank(QP>) = rank(PQP>) follows from the fact that rank(A) = rank(A>) = rank(AA>) and using the fact that Q2 = Q since Q is a projection matrix.\n2⇒ 3 We have that Im(QP>) ⊆ Im(Q). The two subspaces, Im(QP>) and Im(Q), are in fact the linear span of the columns of QP> and Q respectively.\nSince rank(QP>) = rank(Q) we conclude that the dimension of the two subspaces is equal. It follows that Im(QP>) = Im(Q).\n3⇒ 1 Since Im(QP>) = Im(Q) we also have rank(QP>) = rank(Q) and as a corollary rank(PQ) = rank(Q). Now by the rank-nullity Theorem, for every A ∈Mk×d, dim(ker(A)) = d− rank(A). Hence dim(ker(PQ)) = dim(ker(Q)). Since ker(PQ) ⊆ ker(Q) we must have . ker(PQ) = ker(Q).\nClaim 2. Let o ∈ 2d be drawn according to a distribution D that satisfies the low rank assumption. If Q = PE then:\nIm(Qo,o) = Im(PoQ)\nProof. ker(PoQ) = ker(Q) holds by assumption (assumption 3 in Definition 1). Im(Q) = Im(QP>o ) then follows from item 3. In particular Im(PoQ) = Im(PoQPo>) = Im(Qo,o)."
    }, {
      "heading" : "A.2 proof of Lemma 1",
      "text" : "By definition, if Pox ∈ Im(Qo,o) then Qo,o (Qo,o)† Pox = Pox. We claim that due to the low rank assumption, Pox ∈ Im(Qo,o).\nIndeed, recall that Q = PE and x ∈ E hence Qx = x and Pox ∈ Im(PoQ). By Claim 2 we have Im(Qo,o) = Im(PoQ), hence Pox ∈ (ImQo,o).\nNext, we have that\nPoQP > o (Qo,o) † Pox = Qo,o (Qo,o) † Pox = Pox\nAlternatively\nPo(QP > o Q † o,oPox− x) = 0. (4)\nAgain, since Qx = x we have that:\nPoQ(P > o Q † o,oPox− x) = 0. (5)\nThe low rank assumption implies that PoQv = 0 if and only if Qv = 0. Apply this to v = P>o Q † oPox− x and get:\nQP>o Q † o,oPox = Qx = x.\nFinally we have that\nfw,Q(xo) = (PoQ >w∗) ·Q†o,oPox = w∗ ·QP>o Q†o,oPox = w∗ · x."
    }, {
      "heading" : "A.3 proof of Lemma 2",
      "text" : "Let I denote the identity matrix in Md×d. First note that (Io,o − Qo,o) = (I − Q)o,o and that Io,o is the identity matrix in R|o|×|o|.\nLet v1, . . . ,vk be the normalized and orthogonal eigen-vectors of Qo,o with strictly positive eigenvalues λ1 ≥ . . . , λk. By λ-regularity we have that λk ≥ λ and since the spectral norm of Qo,o is smaller than the spectral norm of Q we have that λ1 ≤ 1.\nNote that for every vj we have Q†o,ovj = 1 λj vj . Next, recall that Q = PE and x ∈ E hence Qx = x and Pox ∈ Im(PoQ). By Claim 2 we have Pox ∈ Im(Qo,o). Since Im(Qo,o) = span(v1, . . . ,vk), we may write Pox = ∑ αivi. Since ‖Pox‖ ≤ 1 and {v1, . . . ,vk} is an orthonormal system we have ∑ α2i ≤ 1.\nHence\n‖ γ−1∑ j=0 (Io,o −Qo,o)j −Q†o,o Pox‖ = ‖∑ i αi γ−1∑ j=0 (1− λ)ji − 1 λi vi‖ ≤ max i ∣∣∣∣∣∣ γ−1∑ j=0 (1− λi)j − 1 λi ∣∣∣∣∣∣ ≤ max i ∣∣∣∣1− (1− λi)γλi − 1λi ∣∣∣∣ ≤ (1− λ)γλ .\nFinally since ‖Pow‖ ≤ 1 we get that\n‖fγw,I−Q(xo)− fw,Q(xo)‖ ≤ (1− λ)γ\nλ"
    }, {
      "heading" : "A.4 Proof of Lemma 3",
      "text" : "Let o1 ≤ o2,≤ . . . ≤ o|o| be the elements of o ordered in increasing order. First by definition we have that:\nfγw,Q(xo) = γ−1∑ j=0 |o|∑ n,k=1 won((Qo,o) j)n,kxok = ∑ i∈o wixi + γ−1∑ j=1 |o|∑ n,k=1 won((Qo,o) j)n,kxok (6)\nWe also have by definition that for j ≥ 1:\n((Qo,o) j)n,k = |o|∑ s=1 ((Qo,o) j−1)n,s((Qo,o))s,k = |o|∑ s=1 ((Qo,o) j−1)n,sQos,ok\nBy induction we can show that:\n((Qo,o) j)n,k = ∑ s1∈o Qon,s1 ∑ s2∈o Qs1,s2 ∑ · · ·  ∑ sj−1∈o Qsj−2,sj−1Qsj−1,ok  · · ·  .\nReordering the elements we get for j ≥ 1:\n((Qo,o) j)n,k = ∑ {s:|s|=j+1,s1=on,sj+1=ok} Qs1,s2 ·Qs2,s3 · · ·Qsj ,sj+1 (7)\nThe result now follows from Eq. 6 and Eq. 7 by a change of indexes."
    }, {
      "heading" : "A.5 Proof of Corollary 2",
      "text" : "Choose\nvs = { ws1 |s| = 1 ws1 ·Qs1,s2 ·Qs2,s3 · · ·Qs|s|−1,send |s| > 1\nIt is clear from Lemma 3 that fγw,Q(xo) = v · φγ(xo). We only need to show that ‖v‖ ≤ √\nΓ‖w‖. Note that since Q2 = Q we have max(|Qi,j |) < 1. Hence |vs| ≤ |ws1 | and:\n‖v‖2 = ∑ s∈G v2s ≤ ∑ s∈G w2s1 ≤ Γ‖w‖ 2"
    }, {
      "heading" : "A.6 Proof of Theorem 4",
      "text" : "By definition of φγ we have:\nφγ(x (1) o1 ) · φγ(x (2) o2 ) = ∑ s⊆o1∩o2 x(1)send · x (2) send = γ∑ l=1 ∑ k∈o1∩o2 ∑ s⊆o1∩o2,send=k,|s|=l x (1) k · x (2) k\n= 1∑ l=1 ∑ |s|=l−1,s⊂o1∩o2 ∑ k∈o1∩o2 x (1) k · x (2) k\n= 1∑ l=1 |s : |{|s| = l − 1, s ⊂ o1 ∩ o2}| ∑ k∈o1∩o2 x (1) k · x (2) k = γ∑ l=1 |o1 ∩ o2|l−1 · ∑ k∈o1∩o2 x (1) k · x (2) k =\n1− |o1 ∩ o2|γ 1− |o1 ∩ o2| ∑\nk∈o1∩o2\nx (1) k x (2) k"
    }, {
      "heading" : "A.7 Proof of Theorem 2",
      "text" : "We take φγ as in Definition 4. That φγ(x1o1) · φγ(x2o2) = 1−|o(1)∩o(2)|γ 1−|o(1)∩o(2)| ∑ i∈o(1)∩o(2) x (1) i · x (2) i is shown in Theorem 4. The analysis of sub-gradient descent methods to optimize problems of this form i.e:\n‖w‖2 + C m m∑ i=1 (`(w>φ(xi), yi).\nwas studied in Shalev-Shwartz et al. (2011) and the detailed analysis can be found there (with generalization to mercer kernels and general losses). We mention that since ` is L-Lipschitz and ‖φγ(xo)‖ ≤ √ Γ a bound R on the gradient of∇`(v>φγ(xo), y) = `′(v>φγ(xo), y)φγ(xo) is given by L √\nΓ. This establishes items 1 and 2. Next we let ` be an L-Lipschitz loss function and D a λ-regular distribution and we assume that γ ≥ log 2L/(λ )λ . Due to Corollary 2, for some v∗ ∈ BΓ(Γ)\nE [`(v∗ · φγ(xo), y)] ≤ min fγw,I−Q∈Fγ\nE [ `(fγw,I−Q(xo), y) ] Applying Lemma 2 and L-Lipschitness, for every f∗w,Q ∈ F0 we have:\nE [`(v∗ · φγ(xo), y)] ≤ E [ `(f∗w,Q(xo, y)) ] + L (1− λ)γ\nλ .\nThe result follows from choice of γ and the inequality −λ > log(1− λ):\n(1− λ) log 2L/(λ ) λ\nλ ≤ (1− λ)\nlog(λ )/(2L) log(1−λ) λ = 2L ."
    }, {
      "heading" : "A.8 Proof of Theorem 1",
      "text" : "Fix a sample S = {xioi} m i=1 and γ ≥ log 2L/λ λ . Let\nL(v) = E(`(v>φγ(xo), y) L̂(v) = 1\nm m∑ i=1 `(v>φγ(x i oi),\nthe expected and empirical losses of the vector v. Further denote by\nFc(v) = 1\n2C ‖v‖2 + L(v) F̂c(v) =\n1\n2C ‖v‖2 + L̂(v)\nLet C(m) ∈ O (√\nm log 1/δ ) . Run Alg. 1 with T = m and let v̄ = 1T ∑T i=1 vt. By Theorem 2, item 2 we get:\nF̂C(m)(v̄) ≤ min F̂C(m)(v) +O ( C(m)L2Γ( )\nm\n)\nNote that ‖φγ(xo)‖ ≤ √ dγ−1 d−1 ‖Pox‖ ≤ √ Γ( ). We now apply Corollary 4. in Sridharan et al. (2009) with\nB = √ Γ( ) to obtain the following bound (with probability 1− δ) for every w:\nL(v̄) ≤ L(w) +O\n(√ L2Γ( )‖w‖2 log(1/δ)\nm ) In particular for every ‖w‖ ≤ √ Γ( ) we have\nL(v̄) ≤ L(w) +O\n(√ L2Γ( )2 log(1/δ)\nm\n) .\nFrom Theorem 2, item 3 we have that for some ‖w‖ ≤ √ Γ( ):\nL(w) ≤ min ‖w‖≤1\nE(`(w>x, y) + .\nThe result now follows from the choice of m."
    }, {
      "heading" : "A.9 Proof of Theorem 3",
      "text" : "Before proving the theorem, we formally define the sequences for which the algorithm applies: a λ-regular sequence is one such that the uniform distribution over the sequence elements is λ-regular with associated subspace E.\nProof of Theorem 3. Let E∗ denote the adversarially chosen subspace andQ∗ The projection associated with it. Since the sequence {(xt,ot, yt) is λ-regular w.r.t. subspace E∗, we have by Lemma 2,\n∀‖w‖ ≤ 1 . ‖fγw,I−Q∗(xo)− fw,Q∗(xo)‖ ≤ (1− λ)γ λ ≤ 1 λ e−λγ\nThus, taking fγw∗,I−Q∗ ∈ Fγ we have\nminw∈Bd ∑ t `(fw,Q∗(x t ot), yt)− ∑ t `(f γ w∗,I−Q∗(x t ot), yt)\n= ∑ t `(f ∗ w,Q(x t ot), yt)− ∑ t `(f γ w∗,I−Q∗(x t ot), yt)\n≤ ∑ t L‖f∗w,Q(xtot)− f γ,∗ w,Q(x t ot)‖ ` is L-Lipschitz\n≤ TL 1λe −λγ Lemma 2\nHence it suffices to show that ∑ t `(v > t φγ(x t ot), yt)− ∑ t `(f γ w∗,I−Q∗(x t ot), yt)\n≤ ∑ t `(v > t φγ(x t ot), yt)−minfw,Q∈Fγ ∑ t `(f γ w,Q(x t ot), yt) = O( √ T )\nCorollary 2 asserts that fγw,Q(xo) = v · φγ(xo)\nThus, the theorem statement can be further reduced to∑ t `(v>t φγ(x t ot), yt)− minv∗∈BΓ(Γ) ∑ t `(v>∗ φγ(x t ot), yt) = O( √ T ) (8)\nWe proceed to prove equation Eq. 8 above. Algorithm 1 applies the following update rule\nvt+1 = t∑ i=1 α (t) i φγ(x i oi)\nwhere wt+1 can be re-written as:\nvt+1 = (1− ηtρ)vt − ηt`′(v>t φγ(xtot))φγ(x t ot)\n= vt − ηt∇˜̀t(vt) (9)\nwhere ˜̀ t(v) = `(v >φγ(x t ot)) + ρ\n2 ‖v‖2\nThe above implies a bound on the norm of the gradients of ˜̀t, as given by the following lemma:\nLemma 4. For all iterations t ∈ [T ] we have\n‖vt‖ ≤ LX √ Γ , ‖∇˜̀t(vt)‖ ≤ 2LX √ Γ\nEquation Eq. 9 implies that KARMA applies the online gradient descent algorithm on the functions ˜̀ which are ρ-strongly-convex. Hence, the bound of Theorem 3.3 in Hazan (2014), with appropriate learning rates ηt and with α = ρ, G = 2LX\n√ Γ) by lemma 4, gives∑\nt\n˜̀ t(vt)−min\nv∗ ∑ t ˜̀ t(v ∗) ≤ 2L 2X2Γ ρ (1 + log T )\nThis directly implies our theorem since (recall that ‖v∗‖ ≤ B by assumption):∑ t `(v > t φγ(x t ot), yt)−min‖w‖≤1 ∑ t `(fw,Q∗(x t ot), yt)\n= ∑ t ˜̀ t(vt)−minv∗ ∑ t ˜̀ t(v ∗) + ρ2 ( ∑ t ‖v∗‖2 − ‖vt‖2)\n≤ 2L 2X2Γ ρ (1 + log T ) + ρ 2T ·B\nProof of Lemma 4. First, notice that the norms of the gradients of the loss functions ` can be bounded by\n‖∇`(v>t φγ(xtot), yt)‖ = |` ′(v>t φγ(x t ot), yt)| · ‖φγ(xtot)‖ ≤ LX\n√ Γ\nwhere the last inequality follows from the Lipschitz property of ` and the fact that φγ(xtot) is a vector in RΓ, with coordinates from the vector xt, and the bound ‖xt‖∞ ≤ X .\nNext, we prove by induction that ‖vt‖ ≤ LX √\nΓ. For t = 0 we have v1 = 0. Equation Eq. 9 implies that vt+1 is a convex combination of two vectors:\n‖vt+1‖ = ‖(1− ηtC)vt − ηt`′(v>t φγ(xtot))φγ(x t ot)‖ ≤ max { C‖vt‖ , ‖∇`(v>t φγ(xtot))‖ } ≤ max { CLX √ Γ , ‖∇`(v>t φγ(xtot))‖ } induction hypothesis\n≤ max { CLX √ Γ , LX √ Γ }\nabove bound on ∇`\n≤ LX √ Γ C < 1\nWe can now conclude with the lemma, by definition of ˜̀t\n‖∇˜̀t(vt)‖ ≤ ‖∇`(v>t φγ(xtot))‖+ C\n2 ‖vt‖ ≤ LX\n√ Γ + C 2 LX √ Γ ≤ 2LX √ Γ"
    } ],
    "references" : [ {
      "title" : "Learning with restricted focus of attention",
      "author" : [ "S. Ben-David", "E. Dichterman" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Ben.David and Dichterman,? \\Q1998\\E",
      "shortCiteRegEx" : "Ben.David and Dichterman",
      "year" : 1998
    }, {
      "title" : "Complexity theoretic lower bounds for sparse principal component detection",
      "author" : [ "Q. Berthet", "P. Rigollet" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Berthet and Rigollet,? \\Q2013\\E",
      "shortCiteRegEx" : "Berthet and Rigollet",
      "year" : 2013
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E. Candes", "B. Recht" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Candes and Recht,? \\Q2009\\E",
      "shortCiteRegEx" : "Candes and Recht",
      "year" : 2009
    }, {
      "title" : "Efficient learning with partially observed attributes",
      "author" : [ "N. Cesa-Bianchi", "S. Shalev-Shwartz", "O. Shamir" ],
      "venue" : "In Proceedings of the 27th international conference on Machine learning,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Online learning of noisy data",
      "author" : [ "N. Cesa-Bianchi", "S. Shalev-Shwartz", "O. Shamir" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Max-margin classification of data with absent features",
      "author" : [ "Chechik", "Gal", "Heitz", "Geremy", "Elidan", "Abbeel", "Pieter", "Koller", "Daphne" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Chechik et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chechik et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning to classify with missing and corrupted features",
      "author" : [ "Dekel", "Ofer", "Shamir", "Ohad", "Xiao", "Lin" ],
      "venue" : "Mach. Learn.,",
      "citeRegEx" : "Dekel et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2010
    }, {
      "title" : "Discrete chebyshev classifiers",
      "author" : [ "Eban", "Elad", "Mezuman", "Globerson", "Amir" ],
      "venue" : null,
      "citeRegEx" : "Eban et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Eban et al\\.",
      "year" : 2014
    }, {
      "title" : "Nightmare at test time: robust learning by feature deletion",
      "author" : [ "Globerson", "Amir", "Roweis", "Sam" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "Globerson et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Globerson et al\\.",
      "year" : 2006
    }, {
      "title" : "Transduction with matrix completion: Three birds with one stone",
      "author" : [ "Goldberg", "Andrew B", "Zhu", "Xiaojin", "Recht", "Ben", "Xu", "Jun-Ming", "Nowak", "Robert D" ],
      "venue" : "In Proceedings of the 24th Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Goldberg et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Goldberg et al\\.",
      "year" : 2010
    }, {
      "title" : "Introduction to Online Convex Optimization",
      "author" : [ "Hazan", "Elad" ],
      "venue" : "URL http://ocobook.cs.princeton. edu/",
      "citeRegEx" : "Hazan and Elad.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hazan and Elad.",
      "year" : 2014
    }, {
      "title" : "Linear regression with limited observation",
      "author" : [ "Hazan", "Elad", "Koren", "Tomer" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2012
    }, {
      "title" : "Near-optimal algorithms for online matrix prediction",
      "author" : [ "Hazan", "Elad", "Kale", "Satyen", "Shalev-Shwartz", "Shai" ],
      "venue" : "In COLT, pp",
      "citeRegEx" : "Hazan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2012
    }, {
      "title" : "A generalization of sampling without replacement from a finite universe",
      "author" : [ "D.G. Horvitz", "D.J. Thompson" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Horvitz and Thompson,? \\Q1952\\E",
      "shortCiteRegEx" : "Horvitz and Thompson",
      "year" : 1952
    }, {
      "title" : "Practical large-scale optimization for max-norm regularization",
      "author" : [ "J. Lee", "B. Recht", "R. Salakhutdinov", "N. Srebro", "J.A. Tropp" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Lee et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2010
    }, {
      "title" : "Statistical Analysis with Missing Data, 2nd Edition",
      "author" : [ "Little", "Roderick J. A", "Rubin", "Donald B" ],
      "venue" : null,
      "citeRegEx" : "Little et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Little et al\\.",
      "year" : 2002
    }, {
      "title" : "Collaborative filtering in a non-uniform world: Learning with the weighted trace norm",
      "author" : [ "R. Salakhutdinov", "N. Srebro" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Salakhutdinov and Srebro,? \\Q2010\\E",
      "shortCiteRegEx" : "Salakhutdinov and Srebro",
      "year" : 2010
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan", "Cotter", "Andrew" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Collaborative filtering with the trace norm: Learning, bounding, and transducing",
      "author" : [ "O. Shamir", "S. Shalev-Shwartz" ],
      "venue" : "JMLR - Proceedings Track,",
      "citeRegEx" : "Shamir and Shalev.Shwartz,? \\Q2011\\E",
      "shortCiteRegEx" : "Shamir and Shalev.Shwartz",
      "year" : 2011
    }, {
      "title" : "Learning with Matrix Factorizations",
      "author" : [ "Srebro", "Nathan" ],
      "venue" : "PhD thesis, Massachusetts Institute of Technology,",
      "citeRegEx" : "Srebro and Nathan.,? \\Q2004\\E",
      "shortCiteRegEx" : "Srebro and Nathan.",
      "year" : 2004
    }, {
      "title" : "Fast rates for regularized objectives",
      "author" : [ "Sridharan", "Karthik", "Shalev-Shwartz", "Shai", "Srebro", "Nathan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sridharan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sridharan et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "In the machine learning community, missing data was considered in the framework of limited attribute observability Ben-David & Dichterman (1998) and its many refinements Dekel et al. (2010); Cesa-Bianchi et al.",
      "startOffset" : 170,
      "endOffset" : 190
    }, {
      "referenceID" : 3,
      "context" : "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data.",
      "startOffset" : 8,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)).",
      "startOffset" : 8,
      "endOffset" : 369
    }, {
      "referenceID" : 3,
      "context" : "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)).",
      "startOffset" : 8,
      "endOffset" : 396
    }, {
      "referenceID" : 3,
      "context" : "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)). In our case the learner has no control which attributes are observable in an example and the domain is fixed. The latter case is captured in the work of Chechik et al. (2008), who rescale inner-products according to the amount of missing data.",
      "startOffset" : 8,
      "endOffset" : 573
    }, {
      "referenceID" : 3,
      "context" : "(2010); Cesa-Bianchi et al. (2010, 2011); Hazan & Koren (2012). However, to the best of our knowledge, the low-rank property is not captured by previous work, nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which attributes to observe or on missing attributes at test or train time (see also Eban et al. (2014); Globerson & Roweis (2006)). In our case the learner has no control which attributes are observable in an example and the domain is fixed. The latter case is captured in the work of Chechik et al. (2008), who rescale inner-products according to the amount of missing data. Their method, however, does not entail theoretical gaurantees on reconstruction in the worst case, and gives rise to non-convex programs. A natural and intuitive methodology to follow is to treat the labels (both known and unknown) as an additional column in the data matrix and complete the data using a matrix completion algorithm, thereby obtaining the classification. Indeed, this exactly was proposed by Goldberg et al. (2010). Although this is a natural approach, we show that 1We remark that our model implicitly includes mean-imputation or 0-imputation method and therefore will always outperform them.",
      "startOffset" : 8,
      "endOffset" : 1074
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic models with restricted distributions Srebro (2004); Candes & Recht (2009); Lee et al. (2010); Salakhutdinov & Srebro (2010); Shamir & Shalev-Shwartz (2011).",
      "startOffset" : 180,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic models with restricted distributions Srebro (2004); Candes & Recht (2009); Lee et al. (2010); Salakhutdinov & Srebro (2010); Shamir & Shalev-Shwartz (2011).",
      "startOffset" : 180,
      "endOffset" : 229
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic models with restricted distributions Srebro (2004); Candes & Recht (2009); Lee et al. (2010); Salakhutdinov & Srebro (2010); Shamir & Shalev-Shwartz (2011). The only nonprobabilistic matrix completion algorithm in the online learning setting we are aware of is Hazan et al.",
      "startOffset" : 180,
      "endOffset" : 261
    }, {
      "referenceID" : 11,
      "context" : "The only nonprobabilistic matrix completion algorithm in the online learning setting we are aware of is Hazan et al. (2012), which we were not able to use for our purposes.",
      "startOffset" : 104,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "Goldberg et al. (2010) Considered the following problem: minimize Z rank(Z) subject to: Zi,j = xi,j , (i, j) ∈ Ω , .",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "The optimization analysis is derived in a straightforward manner from the work of Shalev-Shwartz et al. (2011). Other optimization algorithms exist in the literature, and we chose this optimization method as it allows us to also derive regret bounds which are formally stronger (see Section 2.",
      "startOffset" : 82,
      "endOffset" : 111
    } ],
    "year" : 2015,
    "abstractText" : "We consider classification and regression tasks where we have missing data and assume that the (clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper formulation we give an efficient agnostic algorithm that classifies as good as the best linear classifier coupled with the best low-dimensional subspace in which the data resides. A direct implication is that our algorithm can linearly (and non-linearly through kernels) classify provably as well as the best classifier that has access to the full data.",
    "creator" : "LaTeX with hyperref package"
  }
}