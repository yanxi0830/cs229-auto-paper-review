{
  "name" : "1501.03796.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Fast Convergence of Incremental PCA",
    "authors" : [ "Akshay Balsubramani" ],
    "emails" : [ "abalsubr@cs.ucsd.edu", "dasgupta@cs.ucsd.edu", "yfreund@cs.ucsd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 1.\n03 79\n6v 1\n[ cs\n.L G\n] 1\n5 Ja\nn 20"
    }, {
      "heading" : "1 Introduction",
      "text" : "Principal component analysis (PCA) is a popular form of dimensionality reduction that projects a data set on the top eigenvector(s) of its covariance matrix. The default method for computing these eigenvectors uses O(d2) space for data in Rd, which can be prohibitive in practice. It is therefore of interest to study incremental schemes that take one data point at a time, updating their estimates of the desired eigenvectors with each new point. For computing one eigenvector, such methods use O(d) space.\nFor the case of the top eigenvector, this problem has long been studied, and two elegant solutions were obtained by Krasulina [7] and Oja [9]. Their methods are closely related. At time n− 1, they have some estimate Vn−1 ∈ Rd of the top eigenvector. Upon seeing the next data point, Xn, they update this estimate as follows:\nVn = Vn−1 + γn ( XnX T n − V Tn−1XnX T n Vn−1\n‖Vn−1‖2 Id\n) Vn−1 (Krasulina)\nVn = Vn−1 + γnXnX\nT n Vn−1\n‖Vn−1 + γnXnXTn Vn−1‖ (Oja)\nHere γn is a “learning rate” that is typically proportional to 1/n.\nSuppose the points X1, X2, . . . are drawn i.i.d. from a distribution on Rd with mean zero and covariance matrix A. The original papers proved that these estimators converge almost surely to the top eigenvector of A (call it v∗) under mild conditions:\n• ∑n γn = ∞ while ∑ n γ 2 n < ∞. • If λ1, λ2 denote the top two eigenvalues of A, then λ1 > λ2. • E‖Xn‖k < ∞ for some suitable k (for instance, k = 8 works).\nThere are also other incremental estimators for which convergence has not been established; see, for instance, [12] and [16].\nIn this paper, we analyze the rate of convergence of the Krasulina and Oja estimators. They can be treated in a common framework, as stochastic approximation algorithms for maximizing the\nRayleigh quotient\nG(v) = vTAv\nvT v .\nThe maximum value of this function is λ1, and is achieved at v∗ (or any nonzero multiple thereof). The gradient is\n∇G(v) = 2‖v‖2 ( A− v TAv vT v Id ) v.\nSince EXnXTn = A, we see that Krasulina’s method is stochastic gradient descent. The Oja procedure is closely related: as pointed out in [10], the two are identical to within second-order terms.\nRecently, there has been a lot of work on rates of convergence for stochastic gradient descent (for instance, [11]), but this has typically been limited to convex cost functions. These results do not apply to the non-convex Rayleigh quotient, except at the very end, when the system is near convergence. Most of our analysis focuses on the buildup to this finale.\nWe measure the quality of the solution Vn at time n using the potential function\nΨn = 1− (Vn · v∗)2 ‖Vn‖2 ,\nwhere v∗ is taken to have unit norm. This quantity lies in the range [0, 1], and we are interested in the rate at which it approaches zero. The result, in brief, is that E[Ψn] = O(1/n), under conditions that are similar to those above, but stronger. In particular, we require that γn be proportional to 1/n and that ‖Xn‖ be bounded."
    }, {
      "heading" : "1.1 The algorithm",
      "text" : "We analyze the following procedure.\n1. Set starting time. Set the clock to time no. 2. Initialization. Initialize Vno uniformly at random from the unit sphere in R d. 3. For time n = no + 1, no + 2, . . .:\n(a) Receive the next data point, Xn. (b) Update step. Perform either the Krasulina or Oja update, with γn = c/n.\nThe first step is similar to using a learning rate of the form γn = c/(n + no), as is often done in stochastic gradient descent implementations [1]. We have adopted it because the initial sequence of updates is highly noisy: during this phase Vn moves around wildly, and cannot be shown to make progress. It becomes better behaved when the step size γn becomes smaller, that is to say when n gets larger than some suitable no. By setting the start time to no, we can simply fast-forward the analysis to this moment."
    }, {
      "heading" : "1.2 Initialization",
      "text" : "One possible initialization is to set Vno to the first data point that arrives, or to the average of a few data points. This seems sensible enough, but can fail dramatically in some situations.\nHere is an example. Suppose X can take on just 2d possible values: ±e1,±σe2, . . . ,±σed, where the ei are coordinate directions and 0 < σ < 1 is a small constant. Suppose further that the distribution of X is specified by a single positive number p < 1:\nPr(X = e1) = Pr(X = −e1) = p\n2\nPr(X = σei) = Pr(X = −σei) = 1− p\n2(d− 1) for i > 1\nThen X has mean zero and covariance diag(p, σ2(1− p)/(d− 1), . . . , σ2(1− p)/(d− 1)). We will assume that p and σ are chosen so that p > σ2(1 − p)/(d− 1); in our notation, the top eigenvalues are then λ1 = p and λ2 = σ2(1 − p)/(d− 1), and the target vector is v∗ = e1.\nIf Vn is ever orthogonal to some ei, it will remain so forever. This is because both the Krasulina and Oja updates have the following properties:\nVn−1 ·Xn = 0 =⇒ Vn = Vn−1 Vn−1 ·Xn 6= 0 =⇒ Vn ∈ span(Vn−1, Xn).\nIf Vno is initialized to a random data point, then with probability 1 − p, it will be assigned to some ei with i > 1, and will converge to a multiple of that same ei rather than to e1. Likewise, if it is initialized to the average of ≤ 1/p data points, then with constant probability it will be orthogonal to e1 and remain so always.\nSetting Vno to a random unit vector avoids this problem. However, there are doubtless cases, for instance when the data has intrinsic dimension ≪ d, in which a better initializer is possible."
    }, {
      "heading" : "1.3 The setting of the learning rate",
      "text" : "In order to get a sense of what rates of convergence we might expect, let’s return to the example of a random vector X with 2d possible values. In the Oja update Vn = Vn−1 + γnXnXTn Vn−1, we can ignore normalization if we are merely interested in the progress of the potential function Ψn. Since the Xn correspond to coordinate directions, each update changes just one coordinate of V :\nXn = ±e1 =⇒ Vn,1 = Vn−1,1(1 + γn) Xn = ±σei =⇒ Vn,i = Vn−1,i(1 + σ2γn)\nRecall that we initialize Vno to a random vector from the unit sphere. For simplicity, let’s just suppose that no = 0 and that this initial value is the all-ones vector (again, we don’t have to worry about normalization). On each iteration the first coordinate is updated with probability exactly p = λ1, and thus\nE[Vn,1] = (1 + λ1γ1)(1 + λ1γ2) · · · (1 + λ1γn) ∼ exp(λ1(γ1 + · · ·+ γn)) ∼ ncλ1\nsince γn = c/n. Likewise, for i > 1,\nE[Vn,i] = (1 + λ2γ1)(1 + λ2γ2) · · · (1 + λ2γn) ∼ ncλ2 . If all goes according to expectation, then at time n,\nΨn = 1− V 2n,1 ‖Vn‖2 ∼ 1− n 2cλ1 n2cλ1 + (d− 1)n2cλ2 ∼ d− 1 n2c(λ1−λ2) .\n(This is all very rough, but can be made precise by obtaining concentration bounds for lnVn,i.) From this, we can see that it is not possible to achieve a O(1/n) rate unless c ≥ 1/(2(λ1 − λ2)). Therefore, we will assume this when stating our final results, although most of our analysis is in terms of general γn. An interesting practical question, to which we do not have an answer, is how one would empirically set c without prior knowledge of the eigenvalue gap."
    }, {
      "heading" : "1.4 Nested sample spaces",
      "text" : "For n ≥ no, let Fn denote the sigma-field of all outcomes up to and including time n: Fn = σ(Vno , Xno+1, . . . , Xn). We start by showing that\nE[Ψn|Fn−1] ≤ Ψn−1(1− 2γn(λ1 − λ2)(1−Ψn−1)) +O(γ2n). Initially Ψn is likely to be close to 1. For instance, if the initial Vno is picked uniformly at random from the surface of the unit sphere in Rd, then we’d expect Ψno ≈ 1 − 1/d. This means that the initial rate of decrease is very small, because of the (1 −Ψn−1) term. To deal with this, we divide the analysis into epochs: the first takes Ψn from 1− 1/d to 1− 2/d, the second from 1−2/d to 1−4/d, and so on until Ψn finally drops below 1/2. We use martingale large deviation bounds to bound the length of each epoch, and also to argue that Ψn does not regress. In particular, we establish a sequence of times nj such that (with high probability)\nsup n≥nj\nΨn ≤ 1− 2j\nd . (1)\nThe analysis of each epoch uses martingale arguments, but at the same time, assumes that Ψn remains bounded above. Combining the two requires a careful specification of the sample space at each step. Let Ω denote the sample space of all realizations (vno , xno+1, xno+2, . . .), and P the probability distribution on these sequences. For any δ > 0, we define a nested sequence of spaces Ω ⊃ Ω′no ⊃ Ω′no+1 ⊃ · · · such that each Ω′n is Fn−1-measurable, has probability P (Ω′n) ≥ 1 − δ, and moreover consists exclusively of realizations ω ∈ Ω that satisfy the constraints (1) up to and including time n − 1. We can then build martingale arguments by restricting attention to Ω′n when computing the conditional expectations of quantities at time n."
    }, {
      "heading" : "1.5 Main result",
      "text" : "We make the following assumptions:\n(A1) The Xn ∈ Rd are i.i.d. with mean zero and covariance A. (A2) There is a constant B such that ‖Xn‖2 ≤ B. (A3) The eigenvalues λ1 ≥ λ2 ≥ · · · ≥ λd of A satisfy λ1 > λ2. (A4) The step sizes are of the form γn = c/n.\nUnder these conditions, we get the following rate of convergence for the Krasulina update.\nTheorem 1.1. There are absolute constants Ao, A1 > 0 and 1 < a < 4 for which the following holds. Pick any 0 < δ < 1, and any co > 2. Set the step sizes to γn = c/n, where c = co/(2(λ1 − λ2)), and set the starting time to no ≥ (AoB2c2d2/δ4) ln(1/δ). Then there is a nested sequence of subsets of the sample space Ω ⊃ Ω′no ⊃ Ω′no+1 ⊃ · · · such that for any n ≥ no, we have:\nP (Ω′n) ≥ 1− δ and\nEn [ (Vn · v∗)2 ‖Vn‖2 ] ≥ 1− ( c2B2eco/no 2(co − 2) ) 1 n+ 1 −A1 ( d δ2 )a ( no + 1 n+ 1 )co/2 ,\nwhere En denotes expectation restricted to Ω′n.\nSince co > 2, this bound is of the form En[Ψn] = O(1/n).\nThe result above also holds for the Oja update up to absolute constants.\nWe also remark that a small modification to the final step in the proof of the above yields a rate of En [Ψn] = O(n\n−co/2) for co < 2, with an identical definition of En [Ψn]. The details are in the proof, in Appendix D.2."
    }, {
      "heading" : "1.6 Related work",
      "text" : "There is an extensive line of work analyzing PCA from the statistical perspective, in which the convergence of various estimators is characterized under certain conditions, including generative models of the data [5] and various assumptions on the covariance matrix spectrum [14, 4] and eigenvalue spacing [17]. Such works do provide finite-sample guarantees, but they apply only to the batch case and/or are computationally intensive, rather than considering an efficient incremental algorithm.\nAmong incremental algorithms, the work of Warmuth and Kuzmin [15] describes and analyzes worst-case online PCA, using an experts-setting algorithm with a super-quadratic per-iteration cost. More efficient general-purpose incremental PCA algorithms have lacked finite-sample analyses [2]. There have been recent attempts to remedy this situation by relaxing the nonconvexity inherent in the problem [3] or making generative assumptions [8]. The present paper directly analyzes the oldest known incremental PCA algorithms under relatively mild assumptions."
    }, {
      "heading" : "2 Outline of proof",
      "text" : "We now sketch the proof of Theorem 1.1; almost all the details are relegated to the appendix.\nRecall that for n ≥ no, we take Fn to be the sigma-field of all outcomes up to and including time n, that is, Fn = σ(Vno , Xno+1, . . . , Xn).\nAn additional piece of notation: we will use û to denote u/‖u‖, the unit vector in the direction of u ∈ Rd. Thus, for instance, the Rayleigh quotient can be written G(v) = v̂TAv̂."
    }, {
      "heading" : "2.1 Expected per-step change in potential",
      "text" : "We first bound the expected improvement in Ψn in each step of the Krasulina or Oja algorithms. Theorem 2.1. For any n > no, we can write Ψn ≤ Ψn−1 + βn − Zn, where\nβn =\n{ γ2nB\n2/4 (Krasulina) 5γ2nB 2 + 2γ3nB 3 (Oja)\nand where Zn is a Fn-measurable random variable with the following properties:\n• E[Zn|Fn−1] = 2γn(V̂n−1 · v∗)2(λ1 −G(Vn−1)) ≥ 2γn(λ1 − λ2)Ψn−1(1−Ψn−1) ≥ 0.\n• |Zn| ≤ 4γnB.\nThe theorem follows from Lemmas A.4 and A.5 in the appendix. Its characterization of the two estimators is almost identical, and for simplicity we will henceforth deal only with Krasulina’s estimator. All the subsequent results hold also for Oja’s method, up to constants."
    }, {
      "heading" : "2.2 A large deviation bound for Ψn",
      "text" : "We know from Theorem 2.1 that Ψn ≤ Ψn−1 + βn − Zn, where βn is non-stochastic and Zn is a quantity of positive expected value. Thus, in expectation, and modulo a small additive term, Ψn decreases monotonically. However, the amount of decrease at the nth time step can be arbitrarily small when Ψn is close to 1. Thus, we need to show that Ψn is eventually bounded away from 1, i.e. there exists some ǫo > 0 and some time no such that for any n ≥ no, we have Ψn ≤ 1− ǫo. Recall from the algorithm specification that we advance the clock so as to skip the pre-no phase. Given this, what can we expect ǫo to be? If the initial estimate Vno is a random unit vector, then E[Ψno ] = 1− 1/d and, roughly speaking, Pr(Ψno > 1− ǫ/d) = O( √ ǫ). If no is sufficiently large, then Ψn may subsequently increase a little bit, but not by very much. In this section, we establish the following bound.\nTheorem 2.2. Suppose the initial estimate Vno is chosen uniformly at random from the surface of the unit sphere in Rd. Assume also that the step sizes are of the form γn = c/n, for some constant c > 0. Then for any 0 < ǫ < 1, if no ≥ 2B2c2d2/ǫ2, we have\nPr ( sup n≥no Ψn ≥ 1− ǫ d ) ≤ √ 2eǫ.\nTo prove this, we start with a simple recurrence for the moment-generating function of Ψn. Lemma 2.3. Consider a filtration (Fn) and random variables Yn, Zn ∈ Fn such that there are two sequences of nonnegative constants, (βn) and (ζn), for which:\n• Yn ≤ Yn−1 + βn − Zn.\n• Each Zn takes values in an interval of length ζn.\nThen for any t > 0, we have E[etYn |Fn−1] ≤ exp(t(Yn−1 − E[Zn|Fn−1] + βn + tζ2n/8)).\nThis relation shows how to define a supermartingale based on etYn , from which we can derive a large deviation bound on Yn. Lemma 2.4. Assume the conditions of Lemma 2.3, and also that E[Zn|Fn−1] ≥ 0. Then, for any integer m and any ∆, t > 0,\nPr ( sup n≥m Yn ≥ ∆ ) ≤ E[etYm ] exp ( − t ( ∆− ∑\nℓ>m\n(βℓ + tζ 2 ℓ /8)\n)) .\nIn order to apply this to the sequence (Ψn), we need to first calculate the moment-generating function of its starting value Ψno . Lemma 2.5. Suppose a vector V is picked uniformly at random from the surface of the unit sphere in Rd, where d ≥ 3. Define Y = 1− (V 21 )/‖V ‖2. Then, for any t > 0,\nEetY ≤ et √\nd− 1 2t .\nPutting these pieces together yields Theorem 2.2."
    }, {
      "heading" : "2.3 Intermediate epochs of improvement",
      "text" : "We have seen that, for suitable ǫ and no, it is likely that Ψn ≤ 1 − ǫ/d for all n ≥ no. We now define a series of epochs in which 1−Ψn successively doubles, until Ψn finally drops below 1/2. To do this, we specify intermediate goals (no, ǫo), (n1, ǫ1), (n2, ǫ2), . . . , (nJ , ǫJ), where no < n1 < · · · < nJ and ǫo < ǫ1 < · · · < ǫJ = 1/2, with the intention that:\nFor all 0 ≤ j ≤ J , we have sup n≥nj Ψn ≤ 1− ǫj . (2)\nOf course, this can only hold with a certain probability.\nLet Ω denote the sample space of all realizations (vno , xno+1, xno+2, . . .), and P the probability distribution on these sequences. We will show that, for a certain choice of {(nj , ǫj)}, all J + 1 constraints (2) can be met by excluding just a small portion of Ω.\nWe consider a specific realization ω ∈ Ω to be good if it satisfies (2). Call this set Ω′: Ω′ = {ω ∈ Ω : sup\nn≥nj\nΨn(ω) ≤ 1− ǫj for all 0 ≤ j ≤ J}.\nFor technical reasons, we also need to look at realizations that are good up to time n−1. Specifically, for each n, define\nΩ′n = {ω ∈ Ω : sup nj≤ℓ<n Ψℓ(ω) ≤ 1− ǫj for all 0 ≤ j ≤ J}.\nCrucially, this is Fn−1-measurable. Also note that Ω′ = ⋂\nn>no Ω′n.\nWe can talk about expectations under the distribution P restricted to subsets of Ω. In particular, let Pn be the restriction of P to Ω′n; that is, for any A ⊂ Ω, we have Pn(A) = P (A∩Ω′n)/P (Ω′n). As for expectations with respect to Pn, for any function f : Ω → R, we define\nEnf = 1\nP (Ω′n)\n∫\nΩ′n\nf(ω)P (dω).\nHere is the main result of this section. Theorem 2.6. Assume that γn = c/n, where c = co/(2(λ1 − λ2)) and co > 0. Pick any 0 < δ < 1 and select a schedule (no, ǫo), . . . , (nJ , ǫJ) that satisfies the conditions\nǫo = δ2 8ed , and 3 2ǫj ≤ ǫj+1 ≤ 2ǫj for 0 ≤ j < J , and ǫJ−1 ≤ 14 (nj+1 + 1) ≥ e5/co(nj + 1) for 0 ≤ j < J (3)\nas well as no ≥ (20c2B2/ǫ2o) ln(4/δ). Then Pr(Ω′) ≥ 1− δ.\nThe first step towards proving this theorem is bounding the moment-generating function of Ψn in terms of that of Ψn−1. Lemma 2.7. Suppose n > nj . Suppose also that γn = c/n, where c = co/(2(λ1 − λ2)). Then for any t > 0,\nEn[e tΨn ] ≤ En [ exp ( tΨn−1 ( 1− coǫj\nn\n))] exp\n( c2B2t(1 + 32t)\n4n2\n) .\nWe would like to use this result to bound En[Ψn] in terms of Em[Ψm] for m < n. The shift in sample spaces is easily handled using the following observation. Lemma 2.8. If g : R → R is nondecreasing, then En[g(Ψn−1)] ≤ En−1[g(Ψn−1)] for any n > no.\nA repeated application of Lemmas 2.7 and 2.8 yields the following. Lemma 2.9. Suppose that conditions (3) hold. Then for 0 ≤ j < J and any t > 0,\nEnj+1 [e tΨnj+1 ] ≤ exp ( t(1− ǫj+1)− tǫj + tc2B2(1 + 32t)\n4\n( 1\nnj − 1 nj+1\n)) .\nNow that we have bounds on the moment-generating functions of intermediate Ψn, we can apply martingale deviation bounds, as in Lemma 2.4, to obtain the following, from which Theorem 2.6 ensues. Lemma 2.10. Assume conditions (3) hold. Pick any 0 < δ < 1, and set no ≥ (20c2B2/ǫ2o) ln(4/δ). Then\nJ∑\nj=1\nPnj ( sup n≥nj Ψn > 1− ǫj ) ≤ δ 2 ."
    }, {
      "heading" : "2.4 The final epoch",
      "text" : "Recall the definition of the intermediate goals (nj , ǫj) in (2), (3). The final epoch is the period n ≥ nJ , at which point Ψn ≤ 1/2. The following consequence of Lemmas A.4 and 2.8 captures the rate at which Ψ decreases during this phase.\nLemma 2.11. For all n > nJ ,\nEn[Ψn] ≤ (1− αn)En−1[Ψn−1] + βn, where αn = (λ1 − λ2)γn and βn = (B2/4)γ2n.\nBy solving this recurrence relation, and piecing together the various epochs, we get the overall convergence result of Theorem 1.1.\nNote that Lemma 2.11 closely resembles the recurrence relation followed by the squaredL2 distance from the optimum of stochastic gradient descent (SGD) on a strongly convex function [11]. As Ψn → 0, the incremental PCA algorithms we study have convergence rates of the same form as SGD in this scenario."
    }, {
      "heading" : "3 Experiments",
      "text" : "When performing PCA in practice with massive d and a large/growing dataset, an incremental method like that of Krasulina or Oja remains practically viable, even as quadratic-time and -memory algorithms become increasingly impractical. Arora et al. [2] have a more complete discussion of the empirical necessity of incremental PCA algorithms, including a version of Oja’s method which is shown to be extremely competitive in practice.\nSince the efficiency benefits of these types of algorithms are well understood, we now instead focus on the effect of the learning rate on the performance of Oja’s algorithm (results for Krasulina’s are extremely similar). We use the CMU PIE faces [13], consisting of 11554 images of size 32 × 32, as a prototypical example of a dataset with most of its variance captured by a few PCs, as shown in\nFig. 1. We set n0 = 0.\nWe expect from Theorem 1.1 and the discussion in the introduction that varying c (the constant in the learning rate) will influence the overall rate of convergence. In particular, if c is low, then halving it can be expected to halve the exponent of n, and the slope of the log-log convergence graph (ref. the remark after Thm. 1.1). This is exactly what occurs in practice, as illustrated in Fig. 2. The dotted line in that figure is a convergence rate of 1/n, drawn as a guide.\n0 5 10 15 20 25 30 0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\n5000\nComponent Number\nE ig\nen va\nlu e\nPIE Dataset Covariance Spectrum\n10 0\n10 1\n10 2\n10 3\n10 4\n10 5\n10 −6\n10 −5\n10 −4\n10 −3\n10 −2\n10 −1\n10 0\nIteration Number\nR ec\non st\nru ct\nio n\nE rr\nor\nOja Subspace Rule Dependence on c\nc=6 c=3 c=1.5 c=1 c=0.666 c=0.444 c=0.296\nFigures 1 and 2."
    }, {
      "heading" : "4 Open problems",
      "text" : "Several fundamental questions remain unanswered. First, the convergence rates of the two incremental schemes depend on the multiplier c in the learning rate γn. If it is too low, convergence will be slower than O(1/n). If it is too high, the constant in the rate of convergence will be large. Is there a simple and practical scheme for setting c?\nSecond, what can be said about incrementally estimating the top p eigenvectors, for p > 1? Both methods we consider extend easily to this case [10]; the estimate at time n is a d × p matrix Vn whose columns correspond to the eigenvectors, with the invariant V Tn Vn = Ip always maintained. In Oja’s algorithm, for instance, when a new data point Xn ∈ Rd arrives, the following update is performed:\nWn = Vn−1 + γnXnX T n Vn−1\nVn = orth(Wn)\nwhere the second step orthonormalizes the columns, for instance by Gram-Schmidt. It would be interesting to characterize the rate of convergence of this scheme.\nFinally, our analysis applies to a modified procedure in which the starting time no is artificially set to a large constant. This seems unnecessary in practice, and it would be useful to extend the analysis to the case where no = 0."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors are grateful to the National Science Foundation for support under grant IIS-1162581."
    }, {
      "heading" : "A Expected per-step change in potential",
      "text" : ""
    }, {
      "heading" : "A.1 The change in potential of Krasulina’s update",
      "text" : "Write Krasulina’s update equation as\nVn = Vn−1 + γnξn ξn = ( XnX T n − V̂ Tn−1XnXTn V̂n−1Id ) Vn−1\nWe start with some basic observations.\nLemma A.1. For all n > no,\n(a) ξn is orthogonal to Vn−1.\n(b) ‖ξn‖2 ≤ B2‖Vn−1‖2/4.\n(c) E[ξn|Fn−1] = AVn−1 −G(Vn−1)Vn−1.\n(d) ‖Vn‖ ≥ ‖Vn−1‖.\nProof. For (a), let X⊥n denote the component of Xn orthogonal to Vn−1. Then\nξn = (Vn−1·Xn)Xn−(V̂n−1·Xn)2Vn−1 = (Vn−1·Xn)(Xn−(V̂n−1·Xn)V̂n−1) = (Vn−1·Xn)X⊥n .\nFor (b), note from the previous formulation that ‖ξn‖2 = (Vn−1·Xn)2‖X⊥n ‖2 ≤ ‖Vn−1‖2‖Xn‖4/4. Part (c) follows directly from E[XnXTn |Fn−1] = A. For (d), we use ‖Vn‖2 = ‖Vn−1 + γnξn‖2 = ‖Vn−1‖2 + γ2n‖ξn‖2 ≥ ‖Vn−1‖2.\nWe now check that (Vn · v∗)2 grows in expectation with each iteration. Lemma A.2. For any n > no, we have\n(a) (Vn · v∗)2 ≥ (Vn−1 · v∗)2 + 2γn(Vn−1 · v∗)(ξn · v∗).\n(b) E[ξn · v∗|Fn−1] = (Vn−1 · v∗)(λ1 −G(Vn−1)).\nProof. Part (a) follows directly from the update rule:\n(Vn · v∗)2 = ((Vn−1 · v∗) + γn(ξn · v∗))2 ≥ (Vn−1 · v∗)2 + 2γn(Vn−1 · v∗)(ξn · v∗). Part (b) follows by substituting the expression for E[ξn|Fn−1] from Lemma A.1(c): E[ξn · v∗|Fn−1] = (V Tn−1Av∗)−G(Vn−1)(Vn−1 · v∗) = λ1(Vn−1 · v∗)−G(Vn−1)(Vn−1 · v∗).\nIn order to use Lemma A.2 to bound the change in potential Ψn, we need to relate Ψn to the quantity λ1 −G(Vn). Lemma A.3. For any n ≥ no, we have λ1 −G(Vn) ≥ (λ1 − λ2)Ψn.\nProof. It is easiest to think of Vn in the eigenbasis of A: the component of Vn in direction v∗ is Vn · v∗, and the orthogonal component is V ⊥n = Vn − (Vn · v∗)v∗. Then\nG(Vn) = V Tn AVn ‖Vn‖2 = (Vn · v∗)2 ‖Vn‖2 λ1 + (V ⊥n ) TAV ⊥n ‖Vn‖2 ≤ λ1(Vn · v ∗)2 + λ2‖V ⊥n ‖2 ‖Vn‖2 .\nTherefore,\nλ1−G(Vn) ≥ λ1− λ1(Vn · v∗)2 + λ2(‖Vn‖2 − (Vn · v∗)2)\n‖Vn‖2 = (λ1−λ2)\n( 1− (Vn · v ∗)2 ‖Vn‖2 ) = (λ1−λ2)Ψn.\nWe can now explicitly bound the expected change in Ψn in each iteration. Lemma A.4. For any n > no, we can write Ψn ≤ Ψn−1 + βn − Zn, where βn = γ2nB2/4 and where Zn = 2γn(Vn−1 · v∗)(ξn · v∗)/‖Vn−1‖2 is a Fn-measurable random variable with the following properties:\n• E[Zn|Fn−1] = 2γn(V̂n−1 · v∗)2(λ1 −G(Vn−1)) ≥ 2γn(λ1 − λ2)Ψn−1(1−Ψn−1) ≥ 0.\n• |Zn| ≤ 4γnB.\nProof. Using Lemmas A.1 and A.2(a),\nΨn = ‖Vn‖2 − (Vn · v∗)2 ‖Vn‖2 ≤ ‖Vn−1‖ 2 + γ2n‖ξn‖2 − (Vn · v∗)2 ‖Vn−1‖2\n≤ 1 + 1 4 γ2nB 2 − (Vn · v ∗)2 ‖Vn−1‖2\n≤ 1 + 1 4 γ2nB 2 − (Vn−1 · v ∗)2 + 2γn(Vn−1 · v∗)(ξn · v∗) ‖Vn−1‖2 = Ψn−1 + 1\n4 γ2nB\n2 − 2γn (Vn−1 · v∗)(ξn · v∗)\n‖Vn−1‖2 ,\nwhich is Ψn−1+βn−Zn. The conditional expectation ofZn can be determined from Lemma A.2(b):\nE[Zn|Fn−1] = 2γn(Vn−1 · v∗)\n‖Vn−1‖2 E[ξn · v∗|Fn−1] = 2γn(V̂n−1 · v∗)2(λ1 −G(Vn−1))\nand this can be lower-bounded using Lemma A.3.\nFinally, we need to determine the range of possible values of Zn. By expanding ξn, we get\nZn = 2γn(V̂n−1 · v∗) ( (Xn · v∗)(Xn · V̂n−1)− (V̂n−1 · v∗)(Xn · V̂n−1)2 ) .\nSince ‖Xn‖2 ≤ B, we see that Zn must lie in the range ±4γnB."
    }, {
      "heading" : "A.2 The change in potential of the Oja update",
      "text" : "Recall the Oja update:\nVn = Vn−1 + γnXnX\nT n Vn−1\n‖Vn−1 + γnXnXTn Vn−1‖ .\nSince our bounds are on the potential function Ψn, which is insensitive to the length of Vn, we can skip the normalization, and instead just consider the update rule\nVn = Vn−1 + γnXnX T n Vn−1.\nThe final bounds, as well as many of the intermediate results, are almost exactly the same as for Krasulina’s estimator. Here is the analogue of Lemma A.4. Lemma A.5. For any n > no, we can write Ψn ≤ Ψn−1 − Zn + βn, where Zn is the same as in Lemma A.4 and βn = 5γ2nB 2 + 2γ3nB 3.\nProof. This is a series of calculations. First,\n(Vn · v∗)2 = ((Vn−1 · v∗) + γn(V Tn−1XnXTn v∗))2\n≥ (Vn−1 · v∗)2 + 2γn(Vn−1 · v∗)(V Tn−1XnXTn v∗). Similarly,\n‖Vn‖2 = ‖Vn−1 + γnXnXTn Vn−1‖2\n= ‖Vn−1‖2 + γ2n‖XnXTn Vn−1‖2 + 2γn(Vn−1 ·Xn)2 ≤ ‖Vn−1‖2(1 + γ2nB2 + 2γn(V̂n−1 ·Xn)2)\nwhere we have used ‖Xn‖2 ≤ B. Combining these, (Vn · v∗)2 ‖Vn‖2 ≥ (Vn−1 · v ∗)2 + 2γn(Vn−1 · v∗)(V Tn−1XnXTn v∗)\n‖Vn−1‖2(1 + γ2nB2 + 2γn(V̂n−1 ·Xn)2)\n= (V̂n−1 · v∗)2 + 2γn(V̂n−1 · v∗)(V̂ Tn−1XnXTn v∗)\n1 + γ2nB 2 + 2γn(V̂n−1 ·Xn)2\n≥ ( (V̂n−1 · v∗)2 + 2γn(V̂n−1 · v∗)(V̂ Tn−1XnXTn v∗) )( 1− γ2nB2 − 2γn(V̂n−1 ·Xn)2 ) ≥ (V̂n−1 · v∗)2 + 2γn(V̂n−1 · v∗) ( V̂ Tn−1XnX T n v ∗ − (V̂n−1 · v∗)(V̂n−1 ·Xn)2 ) − 5γ2nB2 − 2γ3nB3\nwhere the final step involves some extra algebra that we have omitted. The lemma now follows by invoking Ψn = 1− (V̂n · v∗)2.\nB A large deviation bound for Ψ n"
    }, {
      "heading" : "B.1 Proof of Lemma 2.3",
      "text" : "For any t > 0,\nE [ etYn |Fn−1 ] ≤ E [ et(Yn−1+βn−Zn)|Fn−1 ]\n= et(Yn−1+βn)E [ e−tZn |Fn−1 ] = et(Yn−1+βn) E [ e−tE[Zn|Fn−1]e−t(Zn−E[Zn|Fn−1])|Fn−1 ] ≤ et(Yn−1+βn−E[Zn|Fn−1]) E [ e−t(Zn−E[Zn|Fn−1])|Fn−1 ] .\nWe bound the last expected value using Hoeffding’s lemma: E[etW ] ≤ et2(b−a)2/8 for any random variable W of mean zero and range [a, b]."
    }, {
      "heading" : "B.2 Proof of Lemma 2.4",
      "text" : "By Lemma 2.3,\nE [ etYn |Fn−1 ] ≤ exp ( tYn−1 + tβn +\nt2ζ2n 8\n) .\nNow let’s define an appropriate martingale. Let τn = ∑ ℓ>n(βℓ+tζ 2 ℓ /8), and let Mn = exp(t(Yn+ τn)). Thus Mn ∈ Fn, and\nE[Mn|Fn−1] = E[etYn |Fn−1] exp(tτn) ≤ exp ( tYn−1 + tβn +\nt2ζ2n 8 + tτn\n) = Mn−1.\nThus (Mn) is a positive-valued supermartingale adapted to (Fn). A version of Doob’s martingale inequality—see, for instance, page 274 of [6]—then says that for any m, we have Pr(supn≥m Mn ≥ δ) ≤ (EMm)/δ. Using this, we see that for any ∆ > 0,\nPr ( sup n≥m Yn ≥ ∆ ) ≤ Pr ( sup n≥m Yn + τn ≥ ∆ ) = Pr ( sup n≥m Mn ≥ et∆ )\n≤ EMm et∆ = exp(−t(∆− τm))EetYm"
    }, {
      "heading" : "B.3 Proof of Lemma 2.5",
      "text" : "It is well known that V can be chosen by picking d values Z = (Z1, . . . , Zd) independently from the standard normal distribution and then setting V = Z/‖Z‖. Therefore,\nY = Z22 + · · ·+ Z2d\nZ21 + (Z 2 2 + · · ·+ Z2d)\n= W1\nW1 +W2 ,\nwhere W1 is drawn from a chi-squared distribution with d− 1 degrees of freedom and W2 is drawn independently from a chi-squared distribution with one degree of freedom. This characterization implies that Y follows the Beta((d− 1)/2, 1/2) distribution: specifically, for any 0 < y < 1,\nPr(Y = y) = Γ(d2 )\nΓ(d−12 )Γ( 1 2 )\ny(d−3)/2(1− y)−1/2.\nThe moment-generating function of this distribution is\nEetY = Γ(d2 )\nΓ(d−12 )Γ( 1 2 )\n∫ 1\n0\netyy(d−3)/2(1− y)−1/2dy.\nThere isn’t a closed form for this, but an upper bound on the integral can be obtained. Assuming d ≥ 3,\n∫ 1\n0\netyy(d−3)/2(1 − y)−1/2dy ≤ ∫ 1\n0\nety(1− y)−1/2dy\n= et√ t\n∫ t\n0\ne−zz−1/2dz\n≤ e t\n√ t\n∫ ∞\n0\ne−zz−1/2dz = et√ t Γ(1/2),\nwhere the second step uses a change of variable z = t(1 − y), and the fourth uses the definition of the gamma function. To finish up, we use the inequality Γ(z+1/2) ≤ √z Γ(z) (Lemma B.1) to get\nEetY ≤ Γ( d 2 )\nΓ(d−12 )\net√ t\n≤ et √\nd− 1 2t .\nThe following inequality is doubtless standard; we give a short proof here because we are unable to find a reference.\nLemma B.1. For any z > 0,\nΓ ( z + 1\n2\n) ≤ √ z Γ(z).\nProof. Suppose a random variable T > 0 is drawn according to the density Pr(T = t) ∝ tz−1e−t. Let’s compute ET and E √ T :\nET =\n∫∞ 0\ntze−tdt∫∞ 0 tz−1e−tdt = Γ(z + 1) Γ(z) = z\nE √ T =\n∫∞ 0 t\nz−1/2e−tdt∫∞ 0 t z−1e−tdt = Γ(z + 1/2) Γ(z) ,\nwhere we have used the standard fact Γ(z + 1) = zΓ(z). By concavity of the square root function, we know that E √ T ≤ √ ET . This yields the lemma."
    }, {
      "heading" : "B.4 Proof of Theorem 2.2",
      "text" : "From Lemma A.4(a), we have Ψn ≤ Ψn−1+βn−Zn, where βn = γ2nB2/4, and E[Zn|Fn−1] ≥ 0, and Zn lies in an interval of length ζn = 8γnB. We can thus directly apply the first deviation bound of Lemma 2.4.\nSince ∑\nℓ>n\nγ2n = c 2 ∑\nℓ>n\n1 ℓ2 ≤ c2\n∫ ∞\nn\ndx x2 =\nc2 n ,\nwe see that for any t > 0,\n∑\nℓ>no\n( βℓ +\ntζ2ℓ 8\n) = ∑\nℓ>no\n( B2\n4 γ2ℓ + 8B 2tγ2ℓ\n) ≤ B 2c2\n4no (1 + 32t).\nTo make this ≤ ǫ/d, it suffices to take no ≥ B2c2d(1 + 32t)/(4ǫ), whereupon Lemma 2.4 yields\nPr ( sup n≥no Ψn ≥ 1− ǫ d ) ≤ E[exp(tΨno)]e−t(1−(ǫ/d)−(ǫ/d))\n≤ et √ d\n2t e−t(1−(2ǫ/d)) = e2ǫt/d\n√ d\n2t .\nwhere the last step uses Lemma 2.5. The result follows by taking t = d/(4ǫ).\nC Intermediate epochs of improvement"
    }, {
      "heading" : "C.1 Proof of Lemma 2.7",
      "text" : "Lemma A.4 establishes an inequality Ψn ≤ Ψn−1 − Zn + βn as well as a lower bound on E[Zn|Fn−1], where Zn is a random variable that lies in an interval of length ζn = 8γnB. From Lemma 2.3, we then have\nE[etΨn |Fn−1] ≤ exp ( t(Ψn−1 − E[Zn|Fn−1] + βn + tζ2n/8) )\n≤ exp ( t(Ψn−1 − 2γn(λ1 − λ2)Ψn−1(1−Ψn−1) + γ2nB2(1 + 32t)/4) ) = exp ( t(Ψn−1 − coΨn−1(1 −Ψn−1)/n+ c2B2(1 + 32t)/4n2) )\nFor any ω ∈ Ω′n, we have Ψn−1(ω) ≤ 1− ǫj . Taking expectations over Ω′n, we get the lemma."
    }, {
      "heading" : "C.2 Proof of Lemma 2.8",
      "text" : "Let j be the largest index such that nj < n. Then\nΨn−1(ω) has value { ≤ 1− ǫj for ω ∈ Ω′n > 1− ǫj for ω ∈ Ω′n−1 \\Ω′n\nThus the expected value of g(Ψn−1) over Ω′n is at most the expected value over Ω ′ n−1."
    }, {
      "heading" : "C.3 Proof of Lemma 2.9",
      "text" : "We begin with the following Lemma.\nLemma C.1. For any n > nj and any t > 0,\nEn[e tΨn ] ≤ exp ( t(1 − ǫj) ( nj + 1\nn+ 1\n)coǫj + tc2B2(1 + 32t)\n4\n( 1\nnj − 1 n\n)) .\nProof. Define αn = 1 − (coǫj/n) and ξn(t) = c2B2t(1 + 32t)/4n2. By Lemmas 2.7 and 2.8, for n > nj ,\nEn[e tΨn ] ≤ En[etαnΨn−1 ] exp(ξn(t)) ≤ En−1[e(tαn)Ψn−1 ] exp(ξn(t)).\nBy applying these inequalities repeatedly, for n shrinking to nj +1 (and t shrinking as well), we get\nEn[e tΨn ] ≤ Enj+1\n[ exp ( tΨnjαnαn−1 · · ·αnj+1 )] exp(ξn(t)) exp(ξn−1(tαn)) · · · exp(ξnj+1(tαn · · ·αnj+2))\n≤ Enj+1 [ exp ( tΨnjαnαn−1 · · ·αnj+1 )] exp(ξn(t)) exp(ξn−1(t)) · · · exp(ξnj+1(t))\n= Enj+1\n[ exp ( tΨnj ( 1− coǫj\nn\n)( 1− coǫj\nn− 1\n) · · · ( 1− coǫj\nnj + 1\n))] ×\nexp\n( c2B2t(1 + 32t)\n4\n( 1\nn2 +\n1 (n− 1)2 + · · ·+ 1 (nj + 1)2\n))\n≤ exp ( t(1− ǫj) exp ( −coǫj ( 1\nnj + 1 + · · ·+ 1 n\n))) ×\nexp\n( c2B2t(1 + 32t)\n4\n( 1\nn2 +\n1 (n− 1)2 + · · ·+ 1 (nj + 1)2\n))\nsince Ψnj (ω) ≤ 1− ǫj for all ω ∈ Ω′nj+1. We then use the summations\n1 nj + 1 + · · ·+ 1 n ≥\n∫ n+1\nnj+1\ndx\nx = ln\nn+ 1\nnj + 1\n1 (nj + 1)2 + · · ·+ 1 n2 ≤\n∫ n\nnj\ndx x2 = 1 nj − 1 n\nto get the lemma.\nTo prove Lemma 2.9, we note that under conditions (3),\n(1− ǫj) ( nj + 1\nnj+1 + 1\n)coǫj ≤ e−ǫj (e−5/co)coǫj = e−6ǫj ≤ 1− 3ǫj ≤ 1− ǫj+1 − ǫj .\nWe have used the fact that e−2x ≤ 1−x for 0 ≤ x ≤ 3/4. The rest follows by applying Lemma C.1 with n = nj+1."
    }, {
      "heading" : "C.4 Proof of Lemma 2.10",
      "text" : "Pick any 0 < j ≤ J . We will mimic the reasoning of Theorem 2.2, being careful to define martingales only on the restricted space Ω′nj and with starting time nj . Then\nPnj ( sup n≥nj Ψn > 1− ǫj ) ≤ Enj [etΨnj ] exp ( −t(1− ǫj) + tc2B2(1 + 32t) 4nj )\n≤ exp ( −tǫj−1 + tc2B2(1 + 32t)\n4nj−1\n) ,\nwhere the second step invokes Lemma 2.9.\nTo finish, we pick t = (2/ǫo) ln(4/δ). The lower bound on no is also a lower bound on nj−1, and implies that tc2B2(1 + 32t)/4nj−1 ≤ tǫo/2, whereupon\nPnj ( sup n≥nj Ψn > 1− ǫj ) ≤ exp ( − tǫj−1 2 ) = ( δ 4 )ǫj−1/ǫo ≤ δ 2j+1 .\nSumming over j then yields the lemma."
    }, {
      "heading" : "D The final epoch",
      "text" : ""
    }, {
      "heading" : "D.1 Proof of Lemma 2.11",
      "text" : "By Lemma A.4,\nE[Ψn|Fn−1] ≤ Ψn−1(1− 2γn(1−Ψn−1)(λ1 − λ2)) + βn.\nFor realizations ω ∈ Ω′n, we have Ψn−1(ω) ≤ 1/2 and thus the right-hand side of the above expression is at most (1 − αn)Ψn−1 + βn. Using the fact that Ω′n is Fn−1-measurable, and taking expectations over Ω′n,\nEn[Ψn] ≤ (1− αn)En[Ψn−1] + βn ≤ (1− αn)En−1[Ψn−1] + βn,\nas claimed. The last step uses Lemma 2.8."
    }, {
      "heading" : "D.2 Proof of Theorem 1.1",
      "text" : "Define epochs (nj , ǫj) that satisfy the conditions of Theorem 2.6, with ǫJ = 1/2, and with ǫj+1 = 2ǫj whenever possible. Then J = log2 1/(2ǫo) and\nnJ + 1 = (no + 1) exp\n( 5J\nco\n) = (no + 1) ( 1\n2ǫo\n)5/(co ln 2) = (no + 1) ( 4ed\nδ2\n)5/(co ln 2) .\nBy Theorem 2.6, with probability > 1 − δ, we have Ψn ≤ 1/2 for all n ≥ nJ . More precisely, P (Ω′n) ≥ 1− δ for all n > no. By Lemma 2.11, for n > nJ ,\nEn[Ψn] ≤ ( 1− a\nn\n) En−1[Ψn−1] + b\nn2 ,\nfor a = co/2 and b = c2B2/4. By the a > 1 case of Lemma D.1,\nEn[Ψn] ≤ ( nJ + 1\nn+ 1\n)a EnJ [ΨnJ ] + b\na− 1\n( 1 +\n1\nnJ + 1\n)a+1 1\nn+ 1\n≤ 1 2\n( no + 1\nn+ 1\n)a ( 4ed\nδ2\n)5/(2 ln 2) + b a− 1 exp ( a+ 1\nnJ + 1\n) 1\nn+ 1 .\nwhich upon further simplification yields the bound of Theorem 1.1 for a > 1.\n(Note that the a < 1 case of Lemma D.1 yields a rate of En [Ψn] = O(n−a).)\nLemma D.1. Consider a nonnegative sequence (ut : t ≥ to), such that for some constants a, b > 0 and for all t > to ≥ 0,\nut ≤ ( 1− a\nt\n) ut−1 + b\nt2 .\nThen, writing the zeta function ζ(s) = ∑∞\ni=1 i −s,\nut ≤    ( to+1 t+1 )a uto + b a−1 ( 1 + 1to+1 )a+1 1 t+1 , a > 1\n( to+1 t+1 )a uto + 4bζ(a− 2) 1(t+1)a , a < 1\nProof. Recursively applying the given recurrence for ut yields\nut ≤ ( t∏\ni=to+1\n( 1− a\ni\n)) uto + t∑\ni=to+1\nb i2\n  t∏\nj=i+1\n( 1− a\nj\n)  .\nTo bound the product term, we use\nt∏\ni=to+1\n( 1− a\ni\n) ≤ exp ( −a ∑\ni=to\n1\ni\n) ≤ exp ( −a ∫ t+1\nto+1\ndx\nx\n) = ( to + 1\nt+ 1\n)a .\nTherefore,\nut ≤ ( to + 1\nt+ 1\n)a uto + t∑\ni=to+1\nb i2\n( i+ 1\nt+ 1\n)a\n≤ ( to + 1\nt+ 1\n)a uto +\nb\n(t+ 1)a\n( to + 2\nto + 1\n)2 t∑\ni=to+1\n(i+ 1)a−2.\nWe finish by bounding the summation of (i+ 1)a−2 by a definite integral, to get:\nt∑\ni=to+1\n(i+ 1)a−2 ≤    1 a−1 (t+ 2) a−1 , a > 1\nζ(a− 2) , a < 1 ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "<lb>We consider a situation in which we see samples Xn ∈ R drawn i.i.d. from some<lb>distribution with mean zero and unknown covariance A. We wish to compute the<lb>top eigenvector of A in an incremental fashion with an algorithm that maintains<lb>an estimate of the top eigenvector in O(d) space, and incrementally adjusts the<lb>estimate with each new data point that arrives. Two classical such schemes are<lb>due to Krasulina (1969) and Oja (1983). We give finite-sample convergence rates<lb>for both.",
    "creator" : "LaTeX with hyperref package"
  }
}