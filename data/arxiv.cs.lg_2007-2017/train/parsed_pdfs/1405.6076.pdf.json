{
  "name" : "1405.6076.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Linear Optimization via Smoothing",
    "authors" : [ "Jacob Abernethy", "Abhinav Sinha", "Ambuj Tewari", "ABERNETHY LEE", "SINHA TEWARI" ],
    "emails" : [ "JABERNET@UMICH.EDU", "CHANSOOL@UMICH.EDU", "ABSI@UMICH.EDU", "TEWARIA@UMICH.EDU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n60 76\nv1 [\ncs .L\nG ]\n2 3"
    }, {
      "heading" : "1. Introduction",
      "text" : "In this paper, we study online learning (other names include adversarial learning or no-regret learning) in which the learner iteratively plays actions based on the data received up to the previous iteration. The data sequence is chosen by an adversary and the learner’s goal is to minimize the worst-case regret. The key to developing optimal algorithms is regularization, interpreted as hedging against an adversarial future input and avoiding overfitting to the observed data. In this paper, we focus on regularization techniques for online linear optimization problems where the learner’s action is evaluated on a linear reward function.\nFollow the Regularized Leader (FTRL) is an algorithm that uses explicit regularization via penalty function, which directly changes the optimization objective. At every iteration, FTRL selects an action by optimizing argmaxw f(w,Θ) − R(w) where f is the true objective, Θ is the observed data, and R is a strongly convex penalty function such as the well-known ℓ2-regularizer ‖ · ‖2. The regret analysis of FTRL reduces to the analysis of the second-order behavior of the penalty function (Shalev-Shwartz, 2012), which is well-studied due to the powerful convex analysis tools. In fact, regularization via penalty methods for online learning in general are very well understood. Srebro et al. (2011) proved that Mirror Descent, a regularization via penalty method, achieves a nearly optimal regret guarantee for a general class of online learning problems, and McMahan (2011) showed that FTRL is equivalent to Mirror Descent under some assumptions.\nFollow the Perturbed Leader (FTPL), on the other hand, uses implicit regularization via perturbations. At every iteration, FTPL selects an action by optimizing argmaxw f(w,Θ + u) where\nΘ is the observed data and u is some random noise vector, often referred to as a “perturbation” of the input. Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014). Convex analysis techniques, which led to our current thorough understanding of FTRL, have not been applied to FTPL, partly because the decision rule of FTPL does not explicitly contain a convex function.\nIn this paper, we present a new analysis framework that makes it possible to analyze FTPL in the same way that FTRL has been analyzed, particularly with regards to second-order properties of convex functions. We show that both FTPL and FTRL naturally arise as smoothing operations of a non-smooth potential function and the regret analysis boils down to controlling the smoothing parameters as defined in Section 3. This new unified analysis framework not only recovers the known optimal regret bounds, but also gives a new type of generic regret bounds.\nPrior to our work, Rakhlin et al. (2012) showed that both FTPL and FTRL naturally arise as admissible relaxations of the minimax value of the game between the learner and adversary. In short, adding a random perturbation and adding a regularization penalty function are both optimal ways to simulate the worst-case future input sequence. We establish a stronger connection between FTRL and FTPL; both algorithms are derived from smoothing operations and they are equivalent up to the smoothing parameters. This equivalence is in fact a very strong result, considering the fact that Harsanyi (1973) showed that there is no general bijection between FTPL and FTRL.\nThis paper also aligns itself with the previous work that studied the connection between explicit regularization via penalty and implicit regularization via perturbations. Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton et al., 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the Fisher information. These results are derived from Taylor approximations, but our FTPL-FTRL connection is derived from the convex conjugate duality.\nAn interesting feature of our analysis framework is that we can directly apply existing techniques from the optimization literature, and conversely, our new findings in online linear optimization may apply to optimization theory. In Section 4.3, a straightforward application of the results on Gaussian smoothing by Nesterov (2011) and Duchi et al. (2012) gives a generic regret bound for an arbitrary online linear optimization problem. In Section 4.1 and 4.2, we improve this bound for the special cases that correspond to canonical online linear optimization problems, and these results may be of interest to the optimization community."
    }, {
      "heading" : "2. Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1. Convex Analysis",
      "text" : "Let f be a differentiable, closed, and proper convex function whose domain is domf ⊆ RN . We say that f is L-Lipschitz with respect to a norm ‖ · ‖ when f satisfies |f(x)− f(y)| ≤ L‖x− y‖ for all x, y ∈ dom(f).\nThe Bregman divergence, denoted Df (y, x), is the gap between f(y) and the linear approximation of f(y) around x. Formally, Df (y, x) = f(y) − f(x) − 〈∇f(x), y − x〉. We say that f is β-strongly convex with respect to a norm ‖ · ‖ if we have Df (y, x) ≥ β2 ‖y − x‖2 for all x, y ∈ domf . Similarly, f is said to be β-strongly smooth with respect to a norm ‖ · ‖ if we have\nDf (y, x) ≤ β2 ‖y−x‖2 for all x, y ∈ domf . The Bregman divergence measures how fast the gradient changes, or equivalently, how large the second derivative is. In fact, we can bound the Bregman divergence by analyzing the local behavior of Hessian, as the following adaptation of Abernethy et al. (2013, Lemma 4.6) shows.\nLemma 1 Let f be a twice-differentiable convex function with domf ⊆ RN . Let x ∈ domf , such that vT∇2f(x+αv)v ∈ [a, b] (a ≤ b) for all α ∈ [0, 1]. Then, a‖v‖2/2 ≤ Df (x+v, x) ≤ b‖v‖2/2.\nThe Fenchel conjugate of f is f⋆(θ) = supw∈dom(f){〈w, θ〉 − f(w)}, and it is a dual mapping that satisfies f = (f⋆)⋆ and ∇f⋆ ∈ dom(f). By the strong convexity-strong smoothness duality, f is β-strongly smooth with respect to a norm ‖ · ‖ if and only if f⋆ is 1\nβ -strongly smooth with respect\nto the dual norm ‖ · ‖⋆. For more details and proofs, readers are referred to an excellent survey by Shalev-Shwartz (2012)."
    }, {
      "heading" : "2.2. Online Linear Optimization",
      "text" : "Let X and Y be convex and closed subsets of RN . The online linear optimization is defined to be the following iterative process:\nOn round t = 1, . . . , T , • the learner plays wt ∈ X . • the adversary reveals θt ∈ Y . • the learner receives a reward1 〈wt, θt〉.\nWe say X is the decision set and Y is the reward set. Let Θt = ∑t\ns=1 θs be the cumulative reward. The learner’s goal is to minimize the (external) regret, defined as:\nRegret = max w∈X 〈w,ΘT 〉 ︸ ︷︷ ︸\nbaseline potential\n− T∑\nt=1\n〈wt, θt〉. (1)\nThe baseline potential function Φ(Θ) := maxw∈X 〈w,Θ〉 is the comparator term against which we define the regret, and it coincides with the support function of X . For a bounded compact set X , the support function of X is sublinear2 and Lipschitz continuous with respect to any norm ‖ · ‖ with the Lipschitz constant supx∈X ‖x‖. For more details and proofs, readers are referred to Rockafellar (1997, Section 13) or Molchanov (2005, Appendix F)."
    }, {
      "heading" : "3. Online Linear Optimization Algorithms via Smoothing",
      "text" : ""
    }, {
      "heading" : "3.1. Gradient-Based Prediction Algorithm",
      "text" : "Follow-the-Leader style algorithms solve an optimization objective every round and play an action of the form wt = argmaxw∈X f(w,Θt−1) given a fixed Θt−1. For example, Follow the Regularized Leader maximizes f(w,Θ) = 〈w,Θ〉 − R(w) where R is a strongly convex regularizer, and Follow the Perturbed Leader maximizes f = 〈w,Θ + u〉 where u is a random noise. A surprising\n1. Our somewhat less conventional choice of maximizing the reward instead of minimizing the loss was made so that we directly analyze the convex function max(·) without cumbersome sign changes. 2. A function f is sublinear if it is positive homogeneous (i.e., f(ax) = af(x) for all a > 0) and subadditive (i.e., f(x) + f(y) ≥ f(x+ y)).\nfact about these algorithms is that there are many scenarios in which the action wt is exactly the gradient of some scalar potential function Φt evaluated at Θt−1. This perspective gives rise to what we call the Gradient-based Prediction Algorithm (GBPA), presented below. Note that Cesa-Bianchi and Lugosi (2006, Theorem 11.6) presented a similar algorithm, but our formulation eliminates all dual mappings.\nAlgorithm 1: Gradient-Based Prediction Algorithm (GBPA)\nInput: X ,Y ⊆ RN Initialize Θ0 = 0 for t = 1 to T do\nThe learner chooses differentiable Φt : RN → R whose gradient satisfies Image(∇Φt) ⊆ X The learner plays wt = ∇Φt(Θt−1) The adversary reveals θt ∈ Y and the learner gets a reward of 〈wt, θt〉 Update Θt = Θt−1 + θt\nend\nLemma 2 (GBPA Regret) Let Φ be the baseline potential function for an online linear optimization problem. The regret of the GBPA can be written as:\nRegret = Φ(ΘT )− ΦT (ΘT ) ︸ ︷︷ ︸\nunderestimation penalty\n+ T∑\nt=1\n(\n(Φt(Θt−1)− Φt−1(Θt−1)) ︸ ︷︷ ︸\noverestimation penalty\n+DΦt(Θt,Θt−1) ︸ ︷︷ ︸\ndivergence penalty\n)\n, (2)\nwhere Φ0 ≡ Φ.\nProof See Appendix A.1.\nIn the existing FTPL analysis, the counterpart of the divergence penalty is 〈wt+1−wt, θt〉, which is controlled by analyzing the probability that the noise would cause the two random variables wt+1 and wt to differ. In our framework, wt is the gradient of a function Φt of Θ, which means that if Φt is twice-differentiable, we can take the derivative of wt with respect to Θ. This derivative is the Hessian matrix of Φt, which essentially controls 〈wt − wt−1〉 with the help of Lemma 1. Since we focus on the curvature property of functions as opposed to random vectors, our FTPL analysis involves less probabilistic analysis than Devroye et al. (2013) or van Erven et al. (2014) does.\nWe point out a couple of important facts about Lemma 2: (a) If Φ1 ≡ · · · ≡ ΦT , then the overestimation penalty sums up to Φ1(0)−Φ(0) = ΦT (0)−Φ(0). (b) If Φt is β-strongly smooth with respect to ‖ · ‖, the divergence penalty at t is at most β2 ‖θt‖2."
    }, {
      "heading" : "3.2. Smoothability of the Baseline Potential",
      "text" : "Equation 2 shows that the regret of the GBPA can be broken into two parts. One source of regret is the Bregman divergence of Φt; since θt is not known until playing wt, the GBPA always ascends along the gradient that is one step behind. The adversary can exploit this and play θt to induce a large gap between Φt(xt) and the linear approximation of Φt(Θt) around Θt−1. Of course, the learner can reduce this gap by choosing a smooth Φt whose gradient changes slowly. The learner,\nhowever, cannot achieve low regret by choosing an arbitrarily smooth Φt, because the other source of regret is the difference between Φt and Φ. In short, the GBPA achieves low regret if the potential function Φt gives a favorable tradeoff between the two sources of regret. This tradeoff is captured by the following definition of smoothability.\nDefinition 3 (Beck and Teboulle, 2012, Definition 2.1) Let Φ be a closed proper convex function. A collection of functions {Φ̂η : η ∈ R} is said to be an η-smoothing of a smoothable function Φ with smoothing parameters (α, β, ‖ · ‖), if for every η > 0\n(i) There exists α1 (underestimation bound) and α2 (overestimation bound) such that\nsup Θ∈dom(Φ) Φ(Θ)− Φ̂η(Θ) ≤ α1η and sup Θ∈dom(Φ) Φ̂η(Θ)− Φ(Θ) ≤ α2η\nwith α1 + α2 = α. (ii) Φ̂η is β η\n-strongly smooth with respect to ‖ · ‖. We say α is the deviation parameter, and β is the smoothness parameter.\nA straightforward application of Lemma 2 gives the following statement:\nCorollary 4 Let Φ be the baseline potential for an online linear optimization problem. Suppose {Φ̂η} is an η-smoothing of Φ with parameters (α, β, ‖ · ‖). Then, the GBPA run with Φ1 ≡ · · · ≡ ΦT ≡ Φ̂η has regret at most\nRegret ≤ αη + β 2η\nT∑\nt=1\n‖θt‖2\nIn online linear optimization, we often consider the settings where the marginal reward vectors θ1, . . . , θt are constrained by a norm, i.e., ‖θt‖ ≤ r for all t. In such settings, the regret grows in O( √ rαβT ) for the optimal choice of α. The product αβ, therefore, is at the core of the GBPA regret analysis."
    }, {
      "heading" : "3.3. Algorithms",
      "text" : "Follow the Leader (FTL) Consider the GBPA run with a fixed potential function Φt ≡ Φ for t = 1, . . . , T , i.e., the learner chooses the baseline potential function every iteration. At iteration t, this algorithm plays ∇Φt(Θt−1) = argmaxw〈w,Θt−1〉, which is equivalent to FTL (Cesa-Bianchi and Lugosi, 2006, Section 3.2). FTL suffers zero regret from the over- or underestimation penalty, but the divergence penalty grows linearly in T in the worst case, resulting in an Ω(T ) regret.\nFollow the Regularized Leader (FTRL) Consider the GBPA run with a regularized potential:\n∀t,Φt(Θ) = R⋆(Θ) = max w∈X {〈w,Θ〉 − R(w)} (3)\nwhere R : X → R is a β-strongly convex function. At time t, this algorithm plays ∇Φt(Θt−1) = argmaxw{〈w,Θt−1〉−R(w)}, which is equivalent to FTRL. By the strong convexity-strong smoothness duality, Φt is 1β -strongly smooth with respect to the dual norm ‖ · ‖⋆. In Section 5, we give an alternative interpretation of FTRL as a deterministic smoothing technique called inf-conv smoothing.\nFollow the Perturbed Leader (FTPL) Consider the GBPA run with a stochastically smoothed potential:\n∀t,Φt(Θ) = Φ̃(Θ; η,D) def= Eu∼D[Φ(Θ + ηu)] = Eu∼D [\nmax w∈X\n{〈w,Θ + ηu〉} ]\n(4)\nwhere D is a smoothing distribution with the support RN and η > 0 is a scaling parameter. This technique of stochastic smoothing has been well-studied in the optimization literature for gradientfree optimization algorithms (Glasserman, 1991; Yousean et al., 2010) and accelerated gradient methods for non-smooth optimizations (Duchi et al., 2012). If the max expression inside the expectation has a unique maximizer with probability one, we can swap the expectation and gradient (Bertsekas, 1973, Proposition 2.2) to obtain\n∇Φt(Θt−1) = Eu∼D [\nargmax w∈X\n{〈w,Θt−1 + ηu〉} ] . (5)\nEach argmax expression is equivalent to the decision rule of FTPL (Hannan, 1957; Kalai and Vempala, 2005); the GBPA on a stochastically smoothed potential can thus be seen as playing the expected action of FTPL. Since the learner gets a linear reward in online linear optimization, the regret of the GBPA on a stochastically smoothed potential is equal to the expected regret of FTPL.\nFTPL-FTRL Duality Our potential-based formulation of FTRL and FTPL reveals that a strongly convex regularizer defines a smooth potential function via duality, while adding perturbations is a direct smoothing operation on the baseline potential function. By the strong convexity-strong smoothness duality, if the stochastically smoothed potential function is (1/β)-strongly smooth with respect to ‖ · ‖⋆, then its Fenchel conjugate implicitly defines a regularizer that is β-strongly convex with respect to ‖ · ‖.\nThis connection via duality is a bijection in the special case where the decision set is onedimensional. Previously it had been observed3 that the Hedge Algorithm (Freund and Schapire, 1997), which can be cast as FTRL with an entropic regularization R(w) = ∑i wi logwi, is equivalent to FTPL with Gumbel-distributed noise. Hofbauer and Sandholm (2002, Section 2) gave a generalization of this fact to a much larger class of perturbations, although they focused on repeated game playing where the learner’s decision set X is the probability simplex. The inverse mapping from FTPL to FTRL, however, does not appear to have been previously published.\nTheorem 5 Consider the one-dimensional online linear optimization problem with X = Y = [0, 1]. Let R : X → R be a strongly convex regularizer. Its Fenchel conjugate R⋆ defines a valid CDF of a continuous distribution D such that Equation 3 and Equation 4 are equal. Conversely, let FD be a CDF of a continuous distribution D with a finite expectation. If we define R to be such that R(w)−R(0) = − ∫ w\n0 F −1 D (1− z)dz, then Equation 3 and Equation 4 are equal.\nProof In Appendix B.1.\n3. Adam Kalai first described this result in personal communication and Warmuth (2009) expanded it into a short note available online. However, the result appears to be folklore in the area of probabilistic choice models, and it is mentioned briefly in Hofbauer and Sandholm (2002)."
    }, {
      "heading" : "4. Online Linear Optimization via Gaussian Smoothing",
      "text" : "Gaussian smoothing is a standard technique for smoothing a function. In computer vision applications, for example, image pixels are viewed as a function of the (x, y)-coordinates, and Gaussian smoothing is used to blur noises in the image. We first present basic results on Gaussian smoothing from the optimization literature.\nDefinition 6 (Gaussian smoothing) Let Φ : RN → R be a function. Then, we define its Gaussian smoothing, with a scaling parameter η > 0 and a covariance matrix Σ, as\nΦ̃(Θ; η,N (0,Σ)) = Eu∼N (0,Σ)Φ(Θ + ηu) = (2π)− N 2 det(Σ)− 1 2\n∫\nRN\nΦ(Θ + ηu)e− 1 2 uTΣ−1u du\nIn this section, when the smoothing parameters are clear from the context, we use a shorthand notation Φ̃. An extremely useful property of Gaussian smoothing is that Φ̃ is always twice-differentiable, even when Φ is not. The trick is to introduce a new variable Θ̃ = Θ + ηu. After substitutions, the variable Θ only appears in the exponent, which can be safely differentiated.\nLemma 7 (Nesterov 2011, Lemma 2, and Bhatnagar 2007, Section 3) Let Φ : RN → R be a function. For any positive η, Φ̃(· ; η,N (0,Σ)) is twice-differentiable and\n∇Φ̃(Θ; η,N (0,Σ)) = 1 η Eu[Φ(Θ + ηu)Σ −1u] (6)\n∇2Φ̃(Θ; η,N (0,Σ)) = 1 η2 Eu\n[ Φ(Θ + ηu) ( (Σ−1u)(Σ−1u)T − Σ−1 )]\n(7)\nIf Φ(Θ+ ηu) is differentiable almost everywhere, then we can directly differentiate Equation 6 by swapping the expectation and gradient (Bertsekas, 1973, Proposition 2.2) and obtain an alternative expression for Hessian:\n∇2Φ̃(Θ; η,N (0,Σ)) = 1 η Eu[∇Φ ( Θ+ ηu)(Σ−1u)T ]. (8)"
    }, {
      "heading" : "4.1. Experts Setting (ℓ1-ℓ∞ case)",
      "text" : "The experts setting is where X = ∆N def= {w ∈ RN : ∑iwi = 1, wi ≥ 0 ∀i}, and Y = {θ ∈ R N : ‖θ‖∞ ≤ 1}. The baseline potential function is Φ(Θ) = maxw∈X 〈w,Θ〉 = Θi∗(Θ), where we define i∗(z) := min{i : i ∈ argmaxj zj}. Our regret bound in Theorem 8 is data-dependent, and it is stronger than the previously known O( √ T logN) regret bounds of the algorithms that use similar perturbations. In the game theoretic analysis of Gaussian perturbations by Rakhlin et al. (2012), the algorithm uses the scaling parameter ηt = √ T − t, which requires the knowledge of T and does not adapt to data. Devroye et al. (2013) proposed the Prediction by Random Walk (PRW) algorithm, which flips a fair coin every round and decides whether to add 1 to each coordinate. Due to the discrete nature of the algorithm, the analysis must assume the worst case where ‖θt‖⋆ = 1 for all t.\nTheorem 8 Let Φ be the baseline potential for the experts setting. The GBPA run with the Gaussian smoothing of Φ, i.e., Φt(·) = Φ̃(·; ηt,N (0, I)) for all t has regret at most\nRegret ≤ √ 2 logN ( ηT + ∑T\nt=1 1 ηt ‖θt‖2∞\n)\n. (9)\nIf the algorithm selects ηt = √ ∑T t=1 ‖θt‖2∞ for all t (with the help of hindsight), we have\nRegret ≤ 2 √ 2 ∑T\nt=1 ‖θt‖2∞ logN.\nIf the algorithm selects ηt adaptively according to ηt = √ 2(1 + ∑t−1 s=1 ‖θs‖2∞), we have\nRegret ≤ 4 √ (1 + ∑T\nt=1 ‖θt‖2∞) logN.\nProof In order to apply Lemma 2, we need to upper bound (i) the overestimation and underestimation penalty, and (ii) the Bregman divergence. To bound (i), first note that due to convexity of Φ, the smoothed potential Φ̃ is also convex and upper bounds the baseline potential. Hence, the underestimation penalty is at most 0, and when ηt is fixed for all t, it is straightforward to bound the overestimation penalty:\nΦT (0)− Φ(0) ≤ Eu∼N (0,I)[Φ(ηTu)] ≤ ηT √ 2 logN. (10)\nThe first inequality is the triangle inequality. The second inequality is a well-known result and we included the proof in Appendix C.1 for completeness. For the adaptive ηt, we apply Lemma 10, which we prove at the end of this section, to get the same bound.\nIt now remains to bound the Bregman divergence. This is achieved in Lemma 9 where we upper bound ∑\ni,j |∇2ijΦ|, which is an upper bound on maxθ:‖θ‖∞=1 θT (∇2Φ)θ. The final step is to apply Lemma 1.\nThe proof of Theorem 8 shows that for the experts setting, the Gaussian smoothing is an ηsmoothing with parameters (O( √ logN), O( √ logN), ‖ · ‖). This is in contrast to the Hedge Algorithm (Freund and Schapire, 1997), which is an η-smoothing with parameters (logN, 1, ‖ · ‖) (See Section 5 for details). Interestingly, the two algorithms obtain the same optimal regret (up to constant factors) although they have different smoothing parameters.\nLemma 9 Let Φ be the baseline potential for the experts setting. Let the Hessian matrix of the Gaussian-smoothed baseline potential be denoted H , i.e., H = ∇2Φ̃(Θ; η,N (0, I)). Then,\n∑\ni,j\n|Hij| ≤ 2 √ 2 logN\nη .\nProof With probability one, Φ(Θ + ηu) is differentiable and from Lemma 7, we can write\nH = 1\nη E[∇Φ(Θ+ ηu)uT ] = E[ei∗(ηu+Θ)uT ],\nwhere ei ∈ RN is the i-th standard basis vector.\nFirst, we note that all off-diagonals of H are negative and all diagonal entries in H are positive. This is because the Hessian matrix is the covariance matrix between the probability that i-th coordinate is the maximum and the extra random Gaussian noise added to the j-th coordinate; for any positive number α, uj = α and uj = −α have the same probability, but the indicator for i = i∗ has a higher probability to be 1 when ui is positive (hence Hii > 0) and uj is negative for i 6= j (hence Hij < 0 for i 6= j).\nSecond, the entries of H sum up to 0, as\n∑\ni,j\nHij = 1\nη E\n[ ∑\nj uj ∑ i 1{i = i∗(Θ + u)} ] = 1\nη E\n[ ∑\nj uj\n]\n= 0.\nCombining the two observations, we have\n∑\ni,j\n|Hij| = ∑\ni,j:Hij>0\nHij + ∑\ni,j:Hij<0\n−Hij = 2 ∑\ni,j:Hij>0\nHij = 2Tr(H)\n. Finally, the trace is bounded as follows:\nTr(H) = 1\nη E\n[∑\ni\nui1{i = i∗(Θ + u)} ] ≤ 1 η E [ (max k uk) ∑\ni\n1{i = i∗(Θ + u)} ]\n= 1\nη E[max k uk] ≤\n1\nη\n√\n2 logN,\nwhere the final inequality is shown in Appendix C.1. Multiplying both sides by 2 completes the proof.\nTime-Varying Scaling Parameters When the scaling parameter ηt changes every iteration, the overestimation penalty becomes a sum of T terms. The following lemma shows that using the sublinearity of the baseline potential, we can collapse them into one.\nLemma 10 Let Φ : RN → R be a sublinear function, and D be a continuous distribution with the support RN . Let Φt(Θ) = Φ̃(Θ; ηt,D) for t = 0, . . . , T and choose ηt to be a non-decreasing sequence of non-negative numbers (η0 = 0 so that Φ0 = Φ). Then, the overestimation penalty in Equation 2 has the following upper bound:\nT∑\nt=1\nΦt(Θt−1)− Φt−1(Θt−1) ≤ ηTEu∼D[Φ(u)].\nProof See Appendix C.2"
    }, {
      "heading" : "4.2. Online Linear Optimization over Euclidean Balls (ℓ2-ℓ2 case)",
      "text" : "The Euclidean balls setting is where X = Y = {x ∈ RN : ‖x‖2 ≤ 1}. The baseline potential function is Φ(Θ) = maxw∈X 〈w,Θ〉 = ‖Θ‖2. We show that the GBPA with Gaussian smoothing achieves a minimax optimal regret (Abernethy et al., 2008) up to a constant factor.\nTheorem 11 Let Φ be the baseline potential for the Euclidean balls setting. The GBPA run with Φt(·) = Φ̃(·; η,N (0, I)) for all t has regret at most\nRegret ≤ ηT √ N + 1\n2 √ N ∑T t=1 1 ηt ‖θt‖22. (11)\nIf the algorithm selects ηt = √ ∑T s=1 ‖θs‖22/(2N) for all t (with the help of hindsight), we have\nRegret ≤ √ 2 ∑T\nt=1 ‖θt‖22.\nIf the algorithm selects ηt adaptively according to ηt = √ (1 + ∑t−1 s=1 ‖θs‖22))/N , we have\nRegret ≤ 2 √ 1 + ∑T\nt=1 ‖θt‖22\nProof The proof is mostly similar to that of Theorem 8. In order to apply Lemma 2, we need to upper bound (i) the overestimation and underestimation penalty, and (ii) the Bregman divergence.\nThe Gaussian smoothing always overestimates a convex function, so it suffices to bound the overestimation penalty. Furthermore, it suffices to consider the fixed ηt case due to Lemma 1. The overestimation penalty can be upper-bounded as follows:\nΦT (0)− Φ(0) = Eu∼N (0,I)‖Θ+ ηTu‖2 − ‖Θ‖2 ≤ ηTEu∼N (0,I)‖u‖2 ≤ ηT √ Eu∼N (0,I)‖u‖22 = ηT √ N\nThe first inequality is from the triangle inequality, and the second inequality is from the concavity of the square root.\nFor the divergence penalty, note that the upper bound on maxv:‖θ‖2=1 θ T (∇2Φ̃)θ is exactly the maximum eigenvalue of the Hessian, which we bound in Lemma 12. The final step is to apply Lemma 1.\nLemma 12 Let Φ be the baseline potential for the Euclidean balls setting. Then, for all Θ ∈ RN and η > 0, the Hessian matrix of the Gaussian smoothed potential satisfies\n∇2Φ̃(Θ; η,N (0, I)) 1 η √ N I.\nProof The Hessian of the Euclidean norm ∇2Φ(Θ) = ‖Θ‖−12 I−‖Θ‖−32 ΘΘT diverges near Θ = 0. Expectedly, the maximum curvature is at origin even after Gaussian smoothing (See Appendix C.3). So, it suffices to prove\n∇2Φ(0) = Eu∼N (0,I)[‖u‖2(uuT − I)] √ 1 N I,\nwhere the Hessian expression is from Equation 8. By symmetry, all off-diagonal elements of the Hessian are 0. Let Y = ‖u‖2, which is Chisquared with N degrees of freedom. So,\nTr(E[‖u‖2(uuT − I)]) = E[Tr(‖u‖2(uuT − I))] = E[‖u‖32 −N‖u‖2] = E[Y 3 2 ]−NE[Y 12 ]\nUsing the Chi-Squared moment formula (Harvey, 1965, p. 20), the above becomes:\n2 3 2Γ(32 + N 2 )\nΓ(N2 ) − N2\n1 2Γ(12 + N 2 )\nΓ(N2 ) =\n√ 2Γ(12 + N 2 )\nΓ(N2 ) . (12)\nFrom the log-convexity of the Gamma function,\nlog Γ ( 1 2 + N 2 ) ≤ 12 ( log Γ ( N 2 ) + log Γ ( N 2 + 1 )) = log Γ ( N 2\n) √\nN 2 .\nExponentiating both sides, we obtain\nΓ ( 1 2 + N 2 ) ≤ Γ ( N 2\n) √\nN 2 ,\nwhich we apply to Equation 12 and get Tr(∇2Φ(0)) ≤ √ N . To complete the proof, note that by symmetry, each entry must have the same expected value, and hence it is bounded by √ 1/N ."
    }, {
      "heading" : "4.3. General Bound",
      "text" : "In this section, we will use a generic property of Gaussian smoothing to derive a regret bound that holds for any arbitrary online linear optimization problem.\nLemma 13 (Duchi et al., 2012, Lemma E.2) Let Φ be a real-valued convex function on a closed domain which is a subset of RN . Suppose Φ is L-Lipschitz with respect to ‖ · ‖2, and let Φ̂η be the Gaussian smoothing of Φ with the scaling parameter η and identity covariance. Then, {Φ̂η} is an η-smoothing of Φ with parameters (L √ N,L, ‖ · ‖2).\nConsider an instance of online linear optimization with decision set X and reward set Y . The baseline potential function Φ is ‖X‖2-Lipschitz with respect to ‖ ·‖2, where ‖X‖2 = supx∈X ‖x‖2. From Lemma 13 and Corollary 4, it follows that\nRegret ≤ η √ N‖X‖2 + ‖X‖2 2 T∑\nt=1\n‖θt‖22,\nwhich is O(N 1 4 ‖X‖2‖Y‖2 √ T ) after tuning η. This regret bound, however, often gives a suboptimal dependence on the dimension N . For example, it gives O(N 3 4T 1 2 ) regret bound for the experts setting where ‖X‖2 = 1 and ‖Y‖2 = √ N , and O(N 1 4T 1\n2 ) regret bound for the Euclidean balls setting where ‖X‖2 = ‖Y‖2 = 1."
    }, {
      "heading" : "4.4. Online Convex Optimization",
      "text" : "In online convex optimization, the learner receives a sequence of convex functions ft whose domain is X and its subgradients are in the set Y (Zinkevich, 2003). After the learner plays wt ∈ X , the reward function ft is revealed. The learner gains ft(wt) and observes ∇ft(wt), a subgradient of ft at wt.\nA simple linearization argument shows that our regret bounds for online linear optimization generalize to online convex optimization. Let w∗ be the optimal fixed point in hindsight. The\ntrue regret is upper bounded by the linearized regret, as ft(w∗) − ft(wt) ≤ 〈w∗ − wt,∇ft(wt)〉 for any subgradient ∇ft(·), and our analysis bounds the linearized regret. Unlike in the online linear optimization settings, however, the regret bound is valid only for the GBPA with smoothed potentials, which plays the expected action of FTPL."
    }, {
      "heading" : "5. Online Linear Optimization via Inf-conv Smoothing",
      "text" : "Beck and Teboulle (2012) proposed inf-conv smoothing, which is an infimal convolution with a strongly smooth function. In this section, we will show that FTRL is equivalent to the GBPA run with the inf-conv smoothing of the baseline potential function.\nLet (X , ‖ · ‖) be a normed vector space, and (X ⋆, ‖ · ‖⋆) be its dual. Let Φ : X ⋆ → R be a closed proper convex function, and let S be a β-strongly smooth function on X ⋆ with respect to ‖ · ‖⋆. Then, the inf-conv smoothing of Φ with S is defined as:\nΦic(Θ; η,S) def= inf Θ∗∈X ⋆\n{ Φ(Θ∗) + ηS ( Θ−Θ∗\nη\n)}\n= max w∈X\n{ 〈w,Θ〉 − Φ⋆(w)− ηS⋆(w) } . (13)\nThe first expression with infimum is precisely the infimal convolution of Φ(·) and ηS( · η ), and the second expression with supremum is an equivalent dual formulation. The inf-conv smoothing Φic(Θ; η,S) is finite, and it is an η-smoothing of Φ (Definition 3) with smoothing parameters\n(\nmax Θ∈X ⋆ max w∈∂Φ(Θ)\nS⋆(w), β, ‖ · ‖ ) . (14)\nwhere ∂Φ(Θ) is a set of subgradients of Φ at Θ.\nConnection to FTRL Consider an online linear optimization problem with decision set X ⊆ RN . Then, the dual space X ⋆ is simply RN . Let R be a β-strongly convex function on X with respect to a norm ‖·‖. By the strong convexity-strong smoothness duality, R⋆ is 1\nβ -strongly smooth. Consider\nthe inf-conv smoothing of the baseline potential function Φ with R⋆, denoted Φic(Θ; η,R⋆). We will that the GBPA run with Φic(Θ; η,R⋆) is equivalent to FTRL with R as the regularizer.\nFirst, note that the baseline potential is the convex conjugate of the null regularizer, i.e., Φ⋆(w) = 0 for all w ∈ X . The dual formulation of inf-conv smoothing (Equation 13) thus becomes\nΦic(Θ; η,S) = max w∈X\n{ 〈w,Θ〉 − ηR(w) } ,\nwhich is identical to Equation 3 except that the above expression has an extra parameter η that controls the degree of smoothing. To simplify the deviation parameter in Equation 14, note that the subgradients of Φ always lie in X because of duality. Hence, the two supremum expressions collapse to one supremum: maxw∈X S⋆(w). Plugging the smoothing parameters into Corollary 4 gives the well-known FTRL regret bound as in Theorem 2.11 or 2.21 of Shalev-Shwartz (2012):\nRegret ≤ ηmax w∈X S⋆(w) + β 2η\nT∑\nt=1\n‖θt‖2."
    }, {
      "heading" : "Acknowledgments",
      "text" : "CL and AT gratefully acknowledge the support of NSF under grant IIS-1319810. We thank the anonymous reviewers for their helpful suggestions. We would also like to thank Andre Wibisono for several very useful discussions and his help improving the manuscript. Finally we thank Elad Hazan for early support in developing the ideas herein."
    }, {
      "heading" : "Appendix A. Gradient-Based Prediction Algorithm",
      "text" : "A.1. Proof of Lemma 2\nProof We note that since Φ0(0) = 0,\nΦT (ΘT ) =\nT∑\nt=1\nΦt(Θt)− Φt−1(Θt−1)\n= T∑\nt=1\n(( Φt(Θt)− Φt(Θt−1) ) + ( Φt(Θt−1)− Φt−1(Θt−1) ))\nThe first difference can be rewritten as:\nΦt(Θt)− Φt(Θt−1) = 〈∇Φt(Θt−1),Θt)〉+DΦt(Θt,Θt−1)\nBy combining the above two,\nRegret = Φ(ΘT )− T∑\nt=1\n〈∇Φt(Θt−1),Θt〉\n= Φ(ΘT )− ΦT (ΘT ) + T∑\nt=1\nDΦt(Θt,Θt−1) + Φt(Θt−1)− Φt−1(Θt−1)\nwhich completes the proof."
    }, {
      "heading" : "Appendix B. FTPL-FTRL Duality",
      "text" : "B.1. Proof of Theorem 5\nProof Consider a one-dimensional online linear optimization prediction problem where the player chooses an action wt from X = [0, 1] and the adversary chooses a reward θt from Y = [0, 1]. This can be interpreted as a two-expert setting; the player’s action wt ∈ X is the probability of following the first expert and θt is the net excess reward of the first expert over the second. The baseline potential for this setting is Φ(Θ) = maxw∈[0,1]wΘ.\nLet us consider an instance of FTPL with a continuous distribution D whose cumulative density function (cdf) is FD . Let Φ̃ be the smoothed potential function (Equation 4) with distribution D. Its derivative is\nΦ̃′(Θ) = E[argmax w∈K w(Θ + u)] = P[u > −Θ] (15)\nbecause the maximizer is unique with probability 1. Notice, crucially, that the derivative Φ̃′(Θ) is exactly the expected solution of our FTPL instance. Moreover, by differentiating it again, we see that the second derivative of Φ̃ at Θ is exactly the pdf of D evaluated at (−Θ).\nWe can now precisely define the mapping from FTPL to FTRL. Our goal is to find a convex regularization function R such that P(u > −Θ) = argmaxw∈X (wΘ − R(w)). Since this is a\none-dimensional convex optimization problem, we can differentiate for the solution. The characterization of R is:\nR(w)−R(0) = − ∫ w\n0 F−1D (1− z)dz. (16)\nNote that the cdf FD(·) is indeed invertible since it is a strictly increasing function. The inverse mapping is just as straightforward. Given a regularization function R well-defined over [0, 1], we can always construct its Fenchel conjugate R⋆(Θ) = supw∈X 〈w,Θ〉 − R(w). The derivative of R⋆ is an increasing convex function, whose infimum is 0 at Θ = −∞ and supremum is 1 at Θ = +∞. Hence, R⋆ defines a cdf, and an easy calculation shows that this perturbation distribution exactly reproduces FTRL corresponding to R."
    }, {
      "heading" : "Appendix C. Gaussian smoothing",
      "text" : "C.1. Proof of Equation 10\nLet X1, . . . ,XN be independent standard Gaussian random variables, and let Z = maxi=1,...,N Xi. For any real number a, we have\nexp(aE[Z]) ≤ E exp(aZ) = E max i=1,...,N\nexp(aXi) ≤ N∑\ni=1\nE[exp(aXi)] = N exp(a 2/2).\nThe first inequality is from the convexity of the exponential function, and the last equality is by the definition of the moment generating function of Gaussian random variables. Taking the natural logarithm of both sides and dividing by a gives\nE[Z] ≤ logN a + a 2 .\nIn particular, by choosing a = √ 2 logN , we have E[Z] ≤ √2 logN.\nC.2. Proof of Lemma 10\nProof By the subadditivity (triangle inequality) of Φ,\nΦ̃(Θ; η,N (0, I)) − Φ̃(Θ; η′,N (0, I)) = Eu∼N (0,I)[Φ(Θ + ηu)−Φ(Θ + η′u)] (17) ≤ Eu∼N (0,I)[Φ((η − η′)u)] (18)\nand the statement follows from the positive homogeneity of Φ.\nC.3. Proof that the origin is the worst case (Lemma 12)\nProof Let Φ(Θ) = ‖Θ‖2 and η be a positive number. By continuity of eigenvectors, it suffices to show that the maximum eigenvalue of the Hessian matrix of the Gaussian smoothed potential Φ̃(Θ; η,N (0, I)) is decreasing in ‖Θ‖ for ‖Θ‖ > 0.\nBy Lemma 7, the gradient can be written as follows:\n∇Φ̃(Θ; η,N (0, I)) = 1 η Eu∼N (0,I)[u‖Θ+ ηu‖] (19)\nLet ui be the i-th coordinate of the vector u. Since the standard normal distribution is spherically symmetric, we can rotate the random variable u such that its first coordinate u1 is along the direction of Θ. After rotation, the gradient can be written as\n1 η Eu∼N (0,I)\n\nu √ √ √ √(‖Θ‖+ ηu1)2 + N∑\nk=2\nη2u2k\n\n\nwhich is clearly independent of the coordinates of Θ. The pdf of standard Gaussian distribution has the same value at (u1, u2, . . . , un) and its sign-flipped pair (u1,−u2, . . . ,−un). Hence, in expectation, the two vectors cancel out every coordinate but the first, which is along the direction of Θ. Therefore, there exists a function α such that Eu∼N (0,I)[u‖Θ+ ηu‖] = α(‖Θ‖)Θ.\nNow, we will show that α is decreasing in ‖Θ‖. Due to symmetry, it suffices to consider Θ = te1 for t ∈ R+, without loss of generality. For any t > 0,\nα(t) = E[u1\n√\n(t+ ηu1)2 + u2rest)]/t\n= Eurest [Eu1 [u1 √\n(t+ ηu1)2 + b2|urest = b]]/t = Eurest [Ea=η|u1|[a (√ (t+ a)2 + b2 − √ (t− a)2 +B ) |urest = b]]/t\nLet g(t) = (√ (t+ a)2 +B − √ (t− a)2 +B ) /t. Take the first derivative with respect to t,\nand we have:\ng′(t) = 1\nt2\n( √\n(t− a)2 + b2 − t(t− a)√ (t+ a)2 + b2 − √ (t+ a)2 + b2 + t(t− a) √ (t+ a)2 + b2\n)\n= 1\nt2\n(\na2 + b2 − at √ (t− a)2 + b2 − a 2 + b2 + at √ (t+ a)2 + b2\n)\n( (a2 + b2)− at )2( (t+ a)2 + b2 ) − ( (a2 + b2) + at )2( (t− a)2 + b2 ) = −4ab2t3 < 0\nbecause t, η, u′, B are all positive. So, g(t) < 0, which proves that α is decreasing in Θ. The final setp is to write the gradient as ∇(Φ̃; η,N (0, I))(Θ) = α(‖Θ‖)Θ and differentiate it:\n∇2fη(Θ) = α′(‖Θ‖) ‖Θ‖ ΘΘ T + α(‖Θ‖)I\nThe Hessian has two distinct eigenvalues α(‖Θ‖) and α(‖Θ‖) + α′(‖Θ‖)‖Θ‖, which correspond to the eigenspace orthogonal to Θ and parallel to Θ, respectively. Since α′ is negative, α is always the maximum eigenvalue and it decreases in ‖Θ‖."
    } ],
    "references" : [ {
      "title" : "Optimal stragies and minimax lower bounds for online convex games",
      "author" : [ "Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari" ],
      "venue" : "In Proceedings of Conference on Learning Theory (COLT),",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient market making via convex optimization, and a connection to online learning",
      "author" : [ "Jacob Abernethy", "Yiling Chen", "Jennifer Wortman Vaughan" ],
      "venue" : "ACM Transactions on Economics and Computation,",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2013
    }, {
      "title" : "Smoothing and first order methods: A unified framework",
      "author" : [ "Amir Beck", "Marc Teboulle" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Beck and Teboulle.,? \\Q2012\\E",
      "shortCiteRegEx" : "Beck and Teboulle.",
      "year" : 2012
    }, {
      "title" : "Stochastic optimization problems with nondifferentiable cost functionals",
      "author" : [ "Dimitri P. Bertsekas" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "Bertsekas.,? \\Q1973\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 1973
    }, {
      "title" : "Adaptive newton-based multivariate smoothed functional algorithms for simulation optimization",
      "author" : [ "Shalabh Bhatnagar" ],
      "venue" : "ACM Transactions on Modeling and Computer Simulation,",
      "citeRegEx" : "Bhatnagar.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bhatnagar.",
      "year" : 2007
    }, {
      "title" : "Training with noise is equivalent to tikhonov regularization",
      "author" : [ "Chris M. Bishop" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Bishop.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1995
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "Prediction by random-walk perturbation",
      "author" : [ "Luc Devroye", "Gábor Lugosi", "Gergely Neu" ],
      "venue" : "In Proceedings of Conference on Learning Theory (COLT),",
      "citeRegEx" : "Devroye et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Devroye et al\\.",
      "year" : 2013
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "In Proceedings of Conference on Learning Theory (COLT),",
      "citeRegEx" : "Duchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Randomized smoothing for stochastic optimization",
      "author" : [ "John Duchi", "Peter L. Bartlett", "Martin J. Wainwright" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2012
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Freund and Schapire.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1997
    }, {
      "title" : "Gradient Estimation Via Perturbation Analysis. Kluwer international series in engineering and computer science: Discrete event dynamic systems",
      "author" : [ "Paul Glasserman" ],
      "venue" : null,
      "citeRegEx" : "Glasserman.,? \\Q1991\\E",
      "shortCiteRegEx" : "Glasserman.",
      "year" : 1991
    }, {
      "title" : "Approximation to bayes risk in repeated play",
      "author" : [ "James Hannan" ],
      "venue" : "Contributions to the Theory of Games,",
      "citeRegEx" : "Hannan.,? \\Q1957\\E",
      "shortCiteRegEx" : "Hannan.",
      "year" : 1957
    }, {
      "title" : "Oddness of the number of equilibrium points: a new proof",
      "author" : [ "John C. Harsanyi" ],
      "venue" : "International Journal of Game Theory,",
      "citeRegEx" : "Harsanyi.,? \\Q1973\\E",
      "shortCiteRegEx" : "Harsanyi.",
      "year" : 1973
    }, {
      "title" : "Fractional moments of a quadratic form in noncentral normal random variables",
      "author" : [ "James R. Harvey" ],
      "venue" : null,
      "citeRegEx" : "Harvey.,? \\Q1965\\E",
      "shortCiteRegEx" : "Harvey.",
      "year" : 1965
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "ArXiv preprint,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "On the global convergence of stochastic fictitious play",
      "author" : [ "Josef Hofbauer", "William H. Sandholm" ],
      "venue" : null,
      "citeRegEx" : "Hofbauer and Sandholm.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hofbauer and Sandholm.",
      "year" : 2002
    }, {
      "title" : "Efficient algorithms for online decision problems",
      "author" : [ "Adam T. Kalai", "Santosh Vempala" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Kalai and Vempala.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kalai and Vempala.",
      "year" : 2005
    }, {
      "title" : "Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization",
      "author" : [ "H. Brendan McMahan" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "McMahan.,? \\Q2011\\E",
      "shortCiteRegEx" : "McMahan.",
      "year" : 2011
    }, {
      "title" : "Theory of random sets. Probability and its applications",
      "author" : [ "Ilya S. Molchanov" ],
      "venue" : null,
      "citeRegEx" : "Molchanov.,? \\Q2005\\E",
      "shortCiteRegEx" : "Molchanov.",
      "year" : 2005
    }, {
      "title" : "Random gradient-free minimization of convex functions",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "ECORE Discussion Paper,",
      "citeRegEx" : "Nesterov.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2011
    }, {
      "title" : "Relax and randomize : From value to algorithms",
      "author" : [ "Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan" ],
      "venue" : "In Proceedings of Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Rakhlin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rakhlin et al\\.",
      "year" : 2012
    }, {
      "title" : "Convex Analysis. Convex Analysis",
      "author" : [ "R.T. Rockafellar" ],
      "venue" : null,
      "citeRegEx" : "Rockafellar.,? \\Q1997\\E",
      "shortCiteRegEx" : "Rockafellar.",
      "year" : 1997
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "Shai Shalev-Shwartz" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Shalev.Shwartz.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shalev.Shwartz.",
      "year" : 2012
    }, {
      "title" : "On the universality of online mirror descent",
      "author" : [ "Nati Srebro", "Karthik Sridharan", "Ambuj Tewari" ],
      "venue" : "In Proceedings of Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Srebro et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2011
    }, {
      "title" : "Follow the leader with dropout perturbations",
      "author" : [ "Tim van Erven", "Wojciech Kotlowski", "Manfred K. Warmuth" ],
      "venue" : "In Proceedings of Conference on Learning Theory (COLT),",
      "citeRegEx" : "Erven et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Erven et al\\.",
      "year" : 2014
    }, {
      "title" : "Dropout training as adaptive regularization",
      "author" : [ "Stefan Wager", "Sida Wang", "Percy Liang" ],
      "venue" : "In Proceedings of Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Wager et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wager et al\\.",
      "year" : 2013
    }, {
      "title" : "Convex nondifferentiable stochastic optimization: A local randomized smoothing technique",
      "author" : [ "Farzad Yousean", "Angelia Nedić", "Uday V. Shanbhag" ],
      "venue" : "In Proceedings of American Control Conference (ACC),",
      "citeRegEx" : "Yousean et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Yousean et al\\.",
      "year" : 2010
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "Martin Zinkevich" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Zinkevich.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zinkevich.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "The regret analysis of FTRL reduces to the analysis of the second-order behavior of the penalty function (Shalev-Shwartz, 2012), which is well-studied due to the powerful convex analysis tools.",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 22,
      "context" : "The regret analysis of FTRL reduces to the analysis of the second-order behavior of the penalty function (Shalev-Shwartz, 2012), which is well-studied due to the powerful convex analysis tools. In fact, regularization via penalty methods for online learning in general are very well understood. Srebro et al. (2011) proved that Mirror Descent, a regularization via penalty method, achieves a nearly optimal regret guarantee for a general class of online learning problems, and McMahan (2011) showed that FTRL is equivalent to Mirror Descent under some assumptions.",
      "startOffset" : 106,
      "endOffset" : 316
    }, {
      "referenceID" : 18,
      "context" : "(2011) proved that Mirror Descent, a regularization via penalty method, achieves a nearly optimal regret guarantee for a general class of online learning problems, and McMahan (2011) showed that FTRL is equivalent to Mirror Descent under some assumptions.",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 17,
      "context" : "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014).",
      "startOffset" : 145,
      "endOffset" : 216
    }, {
      "referenceID" : 7,
      "context" : "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014).",
      "startOffset" : 145,
      "endOffset" : 216
    }, {
      "referenceID" : 15,
      "context" : "(2013) showed that for online learning, dropout training (Hinton et al., 2012) is similar to AdagGrad (Duchi et al.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : ", 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the Fisher information.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014). Convex analysis techniques, which led to our current thorough understanding of FTRL, have not been applied to FTPL, partly because the decision rule of FTPL does not explicitly contain a convex function. In this paper, we present a new analysis framework that makes it possible to analyze FTPL in the same way that FTRL has been analyzed, particularly with regards to second-order properties of convex functions. We show that both FTPL and FTRL naturally arise as smoothing operations of a non-smooth potential function and the regret analysis boils down to controlling the smoothing parameters as defined in Section 3. This new unified analysis framework not only recovers the known optimal regret bounds, but also gives a new type of generic regret bounds. Prior to our work, Rakhlin et al. (2012) showed that both FTPL and FTRL naturally arise as admissible relaxations of the minimax value of the game between the learner and adversary.",
      "startOffset" : 171,
      "endOffset" : 1018
    }, {
      "referenceID" : 6,
      "context" : "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014). Convex analysis techniques, which led to our current thorough understanding of FTRL, have not been applied to FTPL, partly because the decision rule of FTPL does not explicitly contain a convex function. In this paper, we present a new analysis framework that makes it possible to analyze FTPL in the same way that FTRL has been analyzed, particularly with regards to second-order properties of convex functions. We show that both FTPL and FTRL naturally arise as smoothing operations of a non-smooth potential function and the regret analysis boils down to controlling the smoothing parameters as defined in Section 3. This new unified analysis framework not only recovers the known optimal regret bounds, but also gives a new type of generic regret bounds. Prior to our work, Rakhlin et al. (2012) showed that both FTPL and FTRL naturally arise as admissible relaxations of the minimax value of the game between the learner and adversary. In short, adding a random perturbation and adding a regularization penalty function are both optimal ways to simulate the worst-case future input sequence. We establish a stronger connection between FTRL and FTPL; both algorithms are derived from smoothing operations and they are equivalent up to the smoothing parameters. This equivalence is in fact a very strong result, considering the fact that Harsanyi (1973) showed that there is no general bijection between FTPL and FTRL.",
      "startOffset" : 171,
      "endOffset" : 1575
    }, {
      "referenceID" : 5,
      "context" : "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton et al.",
      "startOffset" : 0,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton et al., 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the Fisher information. These results are derived from Taylor approximations, but our FTPL-FTRL connection is derived from the convex conjugate duality. An interesting feature of our analysis framework is that we can directly apply existing techniques from the optimization literature, and conversely, our new findings in online linear optimization may apply to optimization theory. In Section 4.3, a straightforward application of the results on Gaussian smoothing by Nesterov (2011) and Duchi et al.",
      "startOffset" : 0,
      "endOffset" : 805
    }, {
      "referenceID" : 5,
      "context" : "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton et al., 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the Fisher information. These results are derived from Taylor approximations, but our FTPL-FTRL connection is derived from the convex conjugate duality. An interesting feature of our analysis framework is that we can directly apply existing techniques from the optimization literature, and conversely, our new findings in online linear optimization may apply to optimization theory. In Section 4.3, a straightforward application of the results on Gaussian smoothing by Nesterov (2011) and Duchi et al. (2012) gives a generic regret bound for an arbitrary online linear optimization problem.",
      "startOffset" : 0,
      "endOffset" : 829
    }, {
      "referenceID" : 0,
      "context" : "In fact, we can bound the Bregman divergence by analyzing the local behavior of Hessian, as the following adaptation of Abernethy et al. (2013, Lemma 4.6) shows. Lemma 1 Let f be a twice-differentiable convex function with domf ⊆ R . Let x ∈ domf , such that vT∇2f(x+αv)v ∈ [a, b] (a ≤ b) for all α ∈ [0, 1]. Then, a‖v‖2/2 ≤ Df (x+v, x) ≤ b‖v‖2/2. The Fenchel conjugate of f is f(θ) = supw∈dom(f){〈w, θ〉 − f(w)}, and it is a dual mapping that satisfies f = (f) and ∇f⋆ ∈ dom(f). By the strong convexity-strong smoothness duality, f is β-strongly smooth with respect to a norm ‖ · ‖ if and only if f is 1 β -strongly smooth with respect to the dual norm ‖ · ‖⋆. For more details and proofs, readers are referred to an excellent survey by Shalev-Shwartz (2012).",
      "startOffset" : 120,
      "endOffset" : 759
    }, {
      "referenceID" : 7,
      "context" : "Since we focus on the curvature property of functions as opposed to random vectors, our FTPL analysis involves less probabilistic analysis than Devroye et al. (2013) or van Erven et al.",
      "startOffset" : 144,
      "endOffset" : 166
    }, {
      "referenceID" : 7,
      "context" : "Since we focus on the curvature property of functions as opposed to random vectors, our FTPL analysis involves less probabilistic analysis than Devroye et al. (2013) or van Erven et al. (2014) does.",
      "startOffset" : 144,
      "endOffset" : 193
    }, {
      "referenceID" : 11,
      "context" : "This technique of stochastic smoothing has been well-studied in the optimization literature for gradientfree optimization algorithms (Glasserman, 1991; Yousean et al., 2010) and accelerated gradient methods for non-smooth optimizations (Duchi et al.",
      "startOffset" : 133,
      "endOffset" : 173
    }, {
      "referenceID" : 27,
      "context" : "This technique of stochastic smoothing has been well-studied in the optimization literature for gradientfree optimization algorithms (Glasserman, 1991; Yousean et al., 2010) and accelerated gradient methods for non-smooth optimizations (Duchi et al.",
      "startOffset" : 133,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : ", 2010) and accelerated gradient methods for non-smooth optimizations (Duchi et al., 2012).",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "Each argmax expression is equivalent to the decision rule of FTPL (Hannan, 1957; Kalai and Vempala, 2005); the GBPA on a stochastically smoothed potential can thus be seen as playing the expected action of FTPL.",
      "startOffset" : 66,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "Each argmax expression is equivalent to the decision rule of FTPL (Hannan, 1957; Kalai and Vempala, 2005); the GBPA on a stochastically smoothed potential can thus be seen as playing the expected action of FTPL.",
      "startOffset" : 66,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "Previously it had been observed3 that the Hedge Algorithm (Freund and Schapire, 1997), which can be cast as FTRL with an entropic regularization R(w) = ∑i wi logwi, is equivalent to FTPL with Gumbel-distributed noise.",
      "startOffset" : 58,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "However, the result appears to be folklore in the area of probabilistic choice models, and it is mentioned briefly in Hofbauer and Sandholm (2002).",
      "startOffset" : 118,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "In the game theoretic analysis of Gaussian perturbations by Rakhlin et al. (2012), the algorithm uses the scaling parameter ηt = √ T − t, which requires the knowledge of T and does not adapt to data.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "Devroye et al. (2013) proposed the Prediction by Random Walk (PRW) algorithm, which flips a fair coin every round and decides whether to add 1 to each coordinate.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "This is in contrast to the Hedge Algorithm (Freund and Schapire, 1997), which is an η-smoothing with parameters (logN, 1, ‖ · ‖) (See Section 5 for details).",
      "startOffset" : 43,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "We show that the GBPA with Gaussian smoothing achieves a minimax optimal regret (Abernethy et al., 2008) up to a constant factor.",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 28,
      "context" : "Online Convex Optimization In online convex optimization, the learner receives a sequence of convex functions ft whose domain is X and its subgradients are in the set Y (Zinkevich, 2003).",
      "startOffset" : 169,
      "endOffset" : 186
    }, {
      "referenceID" : 2,
      "context" : "Online Linear Optimization via Inf-conv Smoothing Beck and Teboulle (2012) proposed inf-conv smoothing, which is an infimal convolution with a strongly smooth function.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "21 of Shalev-Shwartz (2012):",
      "startOffset" : 6,
      "endOffset" : 28
    } ],
    "year" : 2014,
    "abstractText" : "We present a new optimization-theoretic approach to analyzing Follow-the-Leader style algorithms, particularly in the setting where perturbations are used as a tool for regularization. We show that adding a strongly convex penalty function to the decision rule and adding stochastic perturbations to data correspond to deterministic and stochastic smoothing operations, respectively. We establish an equivalence between “Follow the Regularized Leader” and “Follow the Perturbed Leader” up to the smoothness properties. This intuition leads to a new generic analysis framework that recovers and improves the previous known regret bounds of the class of algorithms commonly known as Follow the Perturbed Leader.",
    "creator" : "LaTeX with hyperref package"
  }
}