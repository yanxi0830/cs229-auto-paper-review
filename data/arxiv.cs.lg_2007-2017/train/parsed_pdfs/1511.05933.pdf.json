{
  "name" : "1511.05933.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Sayantan Dasgupta" ],
    "emails" : [ "sayantad@uci.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "K-means has been used over a wide range of Data Mining Applications for years. It is the most common algorithm for extracting clusters from datasets containing real attributes. In recent times it has also been used for feature extraction purpose, which is further used for classification using Neural Networks in Coates et al. (2011). K-means is an NP hard problem even for K = 2 (Aloise et al. (2009)), and only heuristic solutions exist. Perhaps the most common of such heuristics is the Lloyd’s algorithm, which uses an EM style approach; it first assigns the points to different clusters according to their distance from the means of the clusters, and then updates the cluster means by averaging out the points in each cluster. The algorithm uses a random initialization, and comes with no guarantee to reach the global minima. The K-means++ in Arthur & Vassilvitskii (2007) addresses the problem by using an initial seeding of the means, and converges within O (logK) of the global optima. However, for most practical purposes, a bound of O (logK) might be too high. Moreover, the algorithm uses K passes through the entire dataset to seed the means; this makes it very expensive for large amount of data. K-means|| in Bahmani et al. (2012) suggests a method to oversample the means during every pass to reduce the number of passes in the seeding phase. The oversampling strategy also improves accuracy in case there are outliers present in the dataset, by reducing susceptibility of the seeding means to outliers. Nevertheless, both the algorithms are essentially heuristic in nature, and the extracted means as well as the final costs vary a lot over different runs. There are other variants of K-means algorithm, such as Ailon et al. (2009) and Shindler et al. (2011). Most of these algorithms build on K-means++ and aim to reduce the computational burden, rather than improving accuracy. Hence, we limit our discussion on K-means++ and K-means||. There have been recent developments on clustering algorithms based on Method of Moments (MoM), also referred to as Spectral Methods in the literature. Unlike traditional clustering algorithms which rely on EM or similar algorithms to maximize the likelihood of the data, MoM tries to learn the parameters from the higher order moments of the data. The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic\nar X\niv :1\n51 1.\n05 93\n3v 1\n[ cs\n.L G\n] 1\n8 N\nov 2\n01 5\nAlgorithm 1 K-means++\nInput: A set of points X ⊆ RD and K Output: A set C ⊆ RD containing K centres Choose the first centre c1 uniformly at random from X , C ← c1 for k = 2 to K do\nSample a point x from X with the probability px = D(x)2∑ x∈X D(x)\n2 , where D(x) = minc∈C ||x− c|| C ← C ∪ x\nend for return C\nModeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012), for Mixtures of Gaussian in Hsu & Kakade (2013) and for Spectral Experts of Linear Regression in Tejasvi Chaganty & Liang (2013).\nHere we propose a probabilistic K-means algorithm based on Method of Moments. We show the derivation of the probabilistic K-means algorithm, and show its performance in comparison with other existing algorithms for various datasets."
    }, {
      "heading" : "2 PROBLEM FORMULATION",
      "text" : "Given a dataset X ⊆ RD , K-means algorithm tries to find a set C ⊆ RD of K centres (K > 0) which optimizes the following cost function:\nφ(C) = ∑ x∈X min c∈C ||x− c||2 (1)\nThe algorithm is usually started with a random initialization, but fails to converge to global minima of the objective. K-means++ tries to alleviate the problem using an initial mean seeding, as described in Algorithm 1. K-means|| (described as Algorithm 2) introduces a method to reduce the number of passes during the seeding phase, by oversampling seeding points during each pass. In our algorithm, we propose a probabilistic method to extract the means of the cluster based on the covariance and third order central moment of the data, which we outline in this section. It should be noted that our approach is different than the computation of spherical Gaussians using Hsu & Kakade (2013); we do not need to estimate any variance term in order to compute the cluster means unlike Hsu & Kakade (2013); we compute the means solely through tensor factorization. Further, we do not try to find a non-parametric version of K-means unlike Kulis & Jordan (2011); although our model is probabilistic, we assume a fixed value of K."
    }, {
      "heading" : "2.1 METHOD OF MOMENTS",
      "text" : "Here we formulate a generative model for K-means, where we select a cluster h randomly based on the probability P [h = k] = πk, k ∈ 1 . . .K, and choose a data point in the vicinity of the mean of the cluster.\nh ∼ Discrete(π) ε ∼ Eh, where E[Eh] = µh (2)\nx = µX + ε\nwhere, µX = E[X], and can be estimated as the mean of the entire dataset. Eh is the p.d.f of hth cluster of the centred data, i.e., Ek = p[X − µX |h = k], and µh ∈ RD is the mean of the hth cluster of the centred data, i.e. µk = E[Ek] = E[X − µX |h = k],∀k ∈ 1 . . .K . We assume that p[X − µX |h] are conditionally independent given h. We do not assume p[X − µX |h] to follow any particular distribution; rather, we derive our equations using a generic form.\nAlgorithm 2 K-means||\nInput: A set of points X ⊆ RD, number of clusters K, oversampling factor l Output: A set C ⊆ RD containing K centres C ←sample a point c uniformly at random from X ψ(X)← ∑ x∈X ||x− c|| 2 for O (logψ(X)) times do C ′ ←sample each point x from X with the probability px = l·D(x)\n2∑ x∈X D(x)\n2 , where D(x) = minc∈C ||x− c|| Assign C ← C ∪ C ′\nend for For x ∈ C, set wx to be the number of points in X closer to x than any other point in C return K cluster means from weighted clustering of C with the weights wx\nFollowing the generative model in Equation 2, the expectation of the centred data takes the form,\nM1 = E[X − µX ] = Eh[E[X − µh|h]] = K∑ k=1 πkµk = 0 (3)\nIf xi, xj are any two instances in X , then the covariance matrix M2 can be expressed as,\nM2\n= E[(X − µX)(X − µX)>] = Eh[E[(xi − µX)(xj − µX)>|h]]\n= K∑ k=1 (∫ xi ∫ xj (xi − µX)(xj − µX)>P [xi − µX , xj − µX |h = k]dxidxj ) P [h = k] (4)\n= K∑ k=1 (∫ xi (xi − µX)P [xi − µX |h = k] ∫ xj (xj − µX)>[xj − µX |h = k]dxidxj ) P [h = k]\n= K∑ k=1 πkµkµ > k\nSimilarly, for any three elements xi, xj , xl ∈ X , the third order central moment can be expressed as,\nM3 = E[(X − µX)⊗ (X − µX)⊗ (X − µX)] = Eh[E[(xi − µX)⊗ (xj − µX)⊗ (xl − µX)|h]]\n= K∑ k=1 πkµk ⊗ µk ⊗ µk (5)\nThe aim of our algorithm is to retrieve the cluster means {µk}Kk=1 from the covariance M2 and third order momentM3. We first compute a matrixW of rankK to whitenM2, such thatW>M2W = I . This is similar to whitening in Independent Component Analysis (ICA), and is usually done through singular value decomposition. Let us note that, upon whitening, M2 takes the form:\nW>M2W = W > ( K∑ k=1 πkµkµ > k ) W = K∑ k=1 (√ πkW >µk ) (√ πkW >µk )> = K∑ k=1 µ̃kµ̃ > k = I\n(6)\nAlgorithm 3 Method of Moments for K-means\nInput: X ⊆ RD, Number of Clusters K Output: {µk}Kk=1 and {πk}Kk=1\n1. Compute X̃ = X − µX~1>N 2. Compute the Singular Value Decomposition of X̃> as X̃> = UΣV . 3. Compute the whitening matrix as W = UΣ−1. 4. Compute M̃3 ∈ RK×K×K as a Tucker product of X̃W as M̃3 = IN×N×N (X̃W, X̃W, X̃W ) 5. Compute eigenvalues {λk}Kk=1 and eigenvectors {vk}Kk=1 of M̃3 through power iteration followed by deflation (Algorithm 4)\n6. Estimate µk = λk ( W> )† vk, where ( W> )† = W (W>W )\n−1 , and πk = λ−2k , ∀k ∈ 1, 2 . . .K\nreturn {µk}Kk=1 and {πk}Kk=1\nHence µ̃k = √ πkW\n>µk are orthonormal vectors. Multiplying M3 by W across all three dimensions, we get\nM̃3 = M3(W,W,W ) = K∑ k=1 πk(W >µk)⊗ (W>µk)⊗ (W>µk) = K∑ k=1 1 √ πk µ̃k ⊗ µ̃k ⊗ µ̃k (7)\nHence upon the eigen-decomposition of M̃3, the robust eigenvectors will be µ̃k, whereas the eigenvalues will be 1/\n√ πk. The means {µk}Kk=1 can be retrieved from {µ̃k}Kk=1 as, µk = 1√πk\n( W> )† µ̃k.\nThe final cluster means of the data (non-centered) can be further computed as µX + µk,∀k = 1 . . .K.\nThe size ofM2 andM3 areD2 andD3 respectively. These quantities can grow and outstrip memory as D increase. In practice, we do not need to compute any of them. The eigenvalue decomposition M2 is equivalent to the singular value decomposition of X̃>, where X̃ = X − µX~1>N , with the eigenvectors of M2 corresponding to the left singular vectors of X̃>, and the eigenvalues of M2 being the square of the singular values of X̃>. Therefore, the Whitening Matrix can also be computed through a singular value decomposition of X̃>.\nSimilarly, M̃3 can be computed without explicitly computing M3 as a Tucker product of X̃W ,\nM̃3 = IN×N×N (X̃W, X̃W, X̃W ) (8)\nThe overall algorithm of extracting the means is described in 3. We adopt the tensor factorization from Anandkumar et al. (2014), which computes the eigen-vectors of the tensor M̃3 through power iteration followed by deflation, as described as Algorithm 4. The term T (·, θ, θ) ∈ RK is the product of the tensor T ∈ RK×K×K with the vector θ ∈ RK along any two axes. The space complexity of the overall algorithm is O(K3 + DK), whereas time complexity is O((N + D)K3 + NDK) excluding the tensor factorization step. Since the space complexity is independent of the number of instances in X , the algorithm has the capability to scale to a large amount of data. Method of Moments is proven to be globally convergent ( please see Anandkumar et al. (2014) for proof).\nIf the estimated value of M̃3 from training sample, denoted as ˆ̃M3, varies from the actual M̃3 of the population in a way such that ∥∥∥ ˆ̃M3 − M̃3∥∥∥ < for some > 0, then the complexity of the tensor\nfactorization (Algorithm 4) is O(k5+δ(log(k) + loglog(1/ )), where δ is a small positive number, with 0 < δ 1 (Please refer to (Anandkumar et al., 2014) for a detailed derivation). In practice, the tensor factorization is less time intensive than K-Means iterations through the large datasets."
    }, {
      "heading" : "2.2 UNIQUENESS OF THE MEANS",
      "text" : "The extracted means µk = 1√πk ( W> )† µ̃k consist of three terms, W , πk and µ̃k; πk and µ̃k’s are the eigenvalues and eigenvectors of the tensor M̃3 = M3(W,W,W ), and hence they also depend\nAlgorithm 4 Power Iteration for Eigen-decomposition of a Third Order Symmetric Tensor\nInput: Symmetric Tensor T ∈ RK×K×K , Number of Iterations L,N and tolerance Output: The set of eigenvalues {λk}Kk=1 and eigenvectors {vk}Kk=1 of T for k = 1 to K do\nfor τ = 1 to L do Initialize θ(τ)0 at random from a unit sphere in RK for t = 1 to N do\nθ (τ) t ←\nT ( ·,θ(τ)t−1,θ (τ) t−1 ) ∥∥∥T(·,θ(τ)t−1,θ(τ)t−1)∥∥∥\nend for end for Assign τ∗ = argmaxτ∈LT ( θ (τ) N , θ (τ) N , θ (τ) N ) Assign θ0 = θ (τ∗) N , t = 1 repeat θt ← T (·,θt−1,θt−1)‖T (·,θt−1,θt−1)‖ until ‖θt − θt−1‖ < Assign vk ← θt, λk ← T (.,θt,θt)‖T (.,θt,θt)‖ Deflate the tensor as T ← T − λk(vk ⊗ vk ⊗ vk)\nend for\non W . The whitening matrix W is unique only upto a factor of Q ∈ RK×K , such that Q>Q = I . Let us imagine the whitening matrix W whitens M2, then any matrix W ′ = WQ also whitens M2, since\nW ′>M2W ′ = Q> ( W>M2W ) Q = Q>Q = I (9)\nNow, let us assume that we obtain the matrix W ′ as the whitening matrix, then similar to Equation 6,\nW ′>M2W ′ = Q>W> ( K∑ k=1 πkµkµ > k ) WQ = K∑ k=1 (√ πkQ >W>µk ) (√ πkQ >W>µk )> =\nK∑ k=1 µ̃′kµ̃ ′> k = I\n(10)\nHence the new orthonormal vectors are µ̃′k, which vary from µ̃k by a factor of Q >. Furthermore,\nM̃ ′3 = M3(WQ,WQ,WQ) = K∑ k=1 πk(Q >W>µk)⊗ (Q>W>µk)⊗ (Q>W>µk)\n= K∑ k=1 1 √ πk µ̃′k ⊗ µ̃′k ⊗ µ̃′k\n(11)\nHence, the eigenvalues of M̃ ′3 remain the same as those of M̃3, whereas the eigenvectors change to µ̃′k = Q >µ̃k. {µk}Kk=1 can be computed back from {µ̃′k}Kk=1 as,\nµk = 1 √ πk\n( W ′> )† µ̃′k =\n1 √ πk W ′(W ′>W ′)−1µ̃′k (12)\nTherefore,\nµk = 1 √ πk WQ(Q>W>WQ)−1Q>µ̃k = 1 √ πk WQQ−1(W>W )−1Q−>Q>µ̃k\n= 1 √ πk W (W>W )−1µ̃k = 1 √ πk\n( W> )† µ̃k\n(13)\nHence, {µk}Kk=1 are invariant of Q, and are unique as long as the eigenvector decomposition of the tensor M̃3 is robust enough to produce unique eigenvectors. Any matrix W that can whiten M2 is sufficient for extracting the means."
    }, {
      "heading" : "3 EXPERIMENTAL RESULTS",
      "text" : "We demonstrate the performance of our algorithm on a synthetic dataset first, and then on a few real datasets. We choose real datasets with known cluster labels; this enables us to evaluate the clustering performance in terms of Normalized mutual information (NMI) along with the final cost. NMI is a more accurate indicator of how well an algorithm can identify the clusters than the final cost Kulis & Jordan (2011), and the median NMI for all the datasets are listed in Table 1. We show the variation of final cost, NMI and execution times for multiple executions for every dataset.\nWe we carry our experiments out on Unix Platform on a single machine with Intel i5 Processor (2.4GHz) and 8GB memory. For K-means||, we use r = 5 and l = 2K, since these settings usually give the best results as described in Bahmani et al. (2012). For each method of initialization, we run the K-means iterations until φt−1(C)− φt(C) < 10−3φt−1(C), where φt(C) is the cost defined in Equation 1 at iteration t.\nMoM produces very competitive NMI compared to the rest of the methods. Its results are distinctly better than the rest for MNIST and Yale-B datasets. Even when K-Means++ or K-Means|| produces the best NMI, the NMI of MoM is very close to the best. MoM also takes the less time to complete than K-Means++ or K-Means|| (Table 2). Although K-Means takes less time than MoM, the NMI of K-Means is always lower than that of MoM.\n3.1 Synthetic GMM Dataset: We generate 5000 points from 10 component Gaussian Mixture with dimension D = 20 and with σ = 1 along each dimension. The variation in final costs, NMI and the number of iterations through the data by each algorithm is shown in Figure 1. The Method of Moments yields the highest NMI in a very competitive execution time.\n3.2 ISOLET Dataset: We perform our experiments on dataset from UCI Repository, consisting of features for spoken word recognition from 26 groups of speakers; around 7800 data instances with each instance containing 617 features. Both the Method of Moments and K-means++ achieve\nthe lowest cost and highest NMI, but Method of Moments takes less time K-means++ (Figure 2)\n3.3 Human Activities and Postural Transitions (HAPT) Dataset: We perform our experiments on the training set of Smartphone-Based Recognition of Human Activities and Postural Transitions Dataset from UCI Repository used in Reyes-Ortiz et al. (2015). The training set consists of 7767 instances with each instance containing 561 attributes corresponding to 12 distinct activities and postures. Similar to the case of ISOLET, both MoM and K-means++ achieve the lowest cost and highest NMI, but MoM less time. The performance metrics are shown in Figure 3.\n3.4 MNIST Dataset: MNIST dataset consists of handwritten digits of 28× 28 pixels resolution. We extract the means out of the training set consisting of 50,000 instances. The variation in the performance metrics of each algorithm is shown in Figure 4 for 20 executions. Method of Moments gives slightly better NMI than other algorithms, while it takes significantly less time than K-Means++ or K-Means||.\n3.5 Yale-B Dataset: We perform our experiments on Yale-B face recognition dataset consisting of 2414 images of faces of 38 subjects, with each image of the size 192×168 pixels. MoM distinctly outperforms the other algorithms, both in terms of NMI and execution time. The performance metrics are shown in Figure 5.\n3.6 CIFAR Dataset: Each of CIFAR-10 and CIFAR-100 datasets consist of 50,000 coloured images each with each instance having 3072 pixel values; the images belonging to 10 and 100 classes respectively. The variation in the final costs, NMI and the number of iterations through the data for CIFAR-10 is shown in Figure 6, while that for CIFAR-100 is shown in Figure 7.\n3.7 Caltech-101 Dataset: We perform our experiments on Caltech-101 image recognition dataset consisting of 5000 coloured images of 102 different objects. We convert each image to the of size 32×32 pixels. K-means|| gives the best NMI on Caltech dataset, but takes a much higher execution time. MoM performs slightly worse but takes around one-third the time of K-Means|| (Performance Metrics in Figure 8)"
    }, {
      "heading" : "4 CONCLUSION AND FURTHER EXTENSION",
      "text" : "Here we present a probabilistic K-means algorithm based on factorization of higher order moments of the data. The existing algorithms are heuristic in nature, whereas here we introduce a probabilistic interpretation of K-means algorithm based on latent variables. Our model is very similar to the Latent Dirichlet Allocation using Method of Moments presented in Anandkumar et al. (2012), where the pair-wise and triple-wise probabilities replace the covariance and third order moment. Latent variable models can achieve accurate clustering of any dataset, and are used in a wide range of applications such as statistical mixture models and Latent Dirichlet Allocation. However, these algorithms assume that the clusters are generated from a particular probability distribution. Kmeans, on the other hand, imposes no such assumption, and is a much more versatile algorithm for clustering real datasets. Here, we show how the latent variable model can be incorporated in K-means without assuming the clusters to follow any particular distribution.\nWe establish the competitive performance of our model through various experiments. Method of Moments requires O (1) passes through the entire dataset to compute the second and third order moments to extract the initial means, and is therefore very suitable for Large Scale datasets. In practice, it will require only one or two passes to extract the moments. Also, Method of Moments takes less number of K-means or LLoyd’s iterations to converge after the mean initialization, and is therefore usually computationally inexpensive than K-Means++ or K-Means||. The only limitation of our algorithm is the requirement D > K. This can be easily overcome by synthesizing new features to increase the dimension of the training dataset. However, we keep such experiments out of the scope of this paper.\n0\n0.2\n0.4\n0.6\n0.8\n1\nK−Means K−Means++K−Means|| MoM\n(a) NMI\n1\n1.2\n1.4\n1.6\n1.8\n2\nx 105\nK−Means K−Means++K−Means|| MoM\n(b) Final Cost\n0\n1\n2\n3\n4\n5\nK−Means K−Means++K−Means|| MoM\n(c) Time (sec)\nFigure 1: Performance metrics of K-means, Kmeans++, K-means|| and MoM followed by K-means respectively on Synthetic GMM Dataset for K=10 over 20 executions.\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nK−Means K−Means++K−Means|| MoM\n(a) NMI\n4.5\n5\n5.5\n6\n6.5x 10 5\nK−Means K−Means++K−Means|| MoM\n(b) Final Cost\n20\n40\n60\n80\n100\n120\n140\nK−Means K−Means++K−Means|| MoM\n(c) Time (sec)\nFigure 2: Performance metrics of K-means, Kmeans++, K-means|| and MoM followed by K-means respectively on ISOLET Dataset for K=26 over 20 executions\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nK−Means K−Means++K−Means|| MoM\n(a) NMI\n1.5\n2\n2.5\n3\n3.5\n4 x 105\nK−Means K−Means++K−Means|| MoM\n(b) Final Cost\n10\n20\n30\n40\n50\nK−Means K−Means++K−Means|| MoM\n(c) Time (sec)\nFigure 3: Performance metrics of K-means, Kmeans++, K-means|| and MoM followed by K-means respectively on HAPT dataset for K=12 over 20 executions\n0.42\n0.44\n0.46\n0.48\n0.5\n0.52\nK−Means K−Means++K−Means|| MoM\n(a) NMI\n1.53\n1.535\n1.54\n1.545\n1.55\n1.555\n1.56\n1.565\nx 1011\nK−MeansK−Means++K−Means|| MoM\n(b) Final Cost\n100\n200\n300\n400\n500\nK−Means K−Means++K−Means|| MoM\n(c) Time (sec)\nFigure 4: Performance metrics of K-means, Kmeans++, K-means|| and MoM followed by K-means respectively on MNIST dataset for K=10 over 20 executions"
    } ],
    "references" : [ {
      "title" : "Streaming k-means approximation",
      "author" : [ "Ailon", "Nir", "Jaiswal", "Ragesh", "Monteleoni", "Claire" ],
      "venue" : "In Advances in Neural Information Processing Systems, pp",
      "citeRegEx" : "Ailon et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ailon et al\\.",
      "year" : 2009
    }, {
      "title" : "Np-hardness of euclidean sum-of-squares clustering",
      "author" : [ "Aloise", "Daniel", "Deshpande", "Amit", "Hansen", "Pierre", "Popat", "Preyas" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Aloise et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Aloise et al\\.",
      "year" : 2009
    }, {
      "title" : "A spectral algorithm for latent dirichlet allocation",
      "author" : [ "Anandkumar", "Anima", "Liu", "Yi-kai", "Hsu", "Daniel J", "Foster", "Dean P", "Kakade", "Sham M" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2012
    }, {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "Anandkumar", "Animashree", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M", "Telgarsky", "Matus" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2014
    }, {
      "title" : "k-means++: The advantages of careful seeding",
      "author" : [ "Arthur", "David", "Vassilvitskii", "Sergei" ],
      "venue" : "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,",
      "citeRegEx" : "Arthur et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Arthur et al\\.",
      "year" : 2007
    }, {
      "title" : "Scalable k-means++",
      "author" : [ "Bahmani", "Bahman", "Moseley", "Benjamin", "Vattani", "Andrea", "Kumar", "Ravi", "Vassilvitskii", "Sergei" ],
      "venue" : "Proceedings of the VLDB Endowment,",
      "citeRegEx" : "Bahmani et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bahmani et al\\.",
      "year" : 2012
    }, {
      "title" : "An analysis of single-layer networks in unsupervised feature learning",
      "author" : [ "Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak" ],
      "venue" : "In International conference on artificial intelligence and statistics,",
      "citeRegEx" : "Coates et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Coates et al\\.",
      "year" : 2011
    }, {
      "title" : "Spectral learning of latentvariable pcfgs: Algorithms and sample complexity",
      "author" : [ "Cohen", "Shay B", "Stratos", "Karl", "Collins", "Michael", "Foster", "Dean P", "Ungar", "Lyle" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Cohen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2014
    }, {
      "title" : "Spectral dependency parsing with latent variables. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, pp. 205–213",
      "author" : [ "Dhillon", "Paramveer S", "Rodu", "Jordan", "Collins", "Michael", "Foster", "Dean P", "Ungar", "Lyle H" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "Dhillon et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dhillon et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning mixtures of spherical gaussians: Moment methods and spectral decompositions",
      "author" : [ "Hsu", "Daniel", "Kakade", "Sham M" ],
      "venue" : "In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science,",
      "citeRegEx" : "Hsu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2013
    }, {
      "title" : "A spectral algorithm for learning hidden markov models",
      "author" : [ "Hsu", "Daniel", "Kakade", "Sham M", "Zhang", "Tong" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Hsu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2012
    }, {
      "title" : "Shifted power method for computing tensor eigenpairs",
      "author" : [ "Kolda", "Tamara G", "Mayo", "Jackson R" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "Kolda et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kolda et al\\.",
      "year" : 2011
    }, {
      "title" : "Revisiting k-means: New algorithms via bayesian nonparametrics",
      "author" : [ "Kulis", "Brian", "Jordan", "Michael I" ],
      "venue" : "arXiv preprint arXiv:1111.0352,",
      "citeRegEx" : "Kulis et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kulis et al\\.",
      "year" : 2011
    }, {
      "title" : "Transition-aware human activity recognition using smartphones",
      "author" : [ "Reyes-Ortiz", "Jorge-L", "Oneto", "Luca", "Samà", "Albert", "Parra", "Xavier", "Anguita", "Davide" ],
      "venue" : null,
      "citeRegEx" : "Reyes.Ortiz et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Reyes.Ortiz et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast and accurate k-means for large datasets. In Advances in neural information processing",
      "author" : [ "Shindler", "Michael", "Wong", "Alex", "Meyerson", "Adam W" ],
      "venue" : null,
      "citeRegEx" : "Shindler et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shindler et al\\.",
      "year" : 2011
    }, {
      "title" : "Hilbert space embeddings of hidden markov models",
      "author" : [ "Song", "Le", "Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J", "Smola", "Alex J" ],
      "venue" : "In Proceedings of the 27th international conference on machine learning",
      "citeRegEx" : "Song et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2010
    }, {
      "title" : "Spectral experts for estimating mixtures of linear regressions",
      "author" : [ "Tejasvi Chaganty", "Arun", "Liang", "Percy" ],
      "venue" : "In Proceedings of The 30th International Conference on Machine Learning,",
      "citeRegEx" : "Chaganty et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chaganty et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "In recent times it has also been used for feature extraction purpose, which is further used for classification using Neural Networks in Coates et al. (2011). K-means is an NP hard problem even for K = 2 (Aloise et al.",
      "startOffset" : 136,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "K-means is an NP hard problem even for K = 2 (Aloise et al. (2009)), and only heuristic solutions exist.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "K-means is an NP hard problem even for K = 2 (Aloise et al. (2009)), and only heuristic solutions exist. Perhaps the most common of such heuristics is the Lloyd’s algorithm, which uses an EM style approach; it first assigns the points to different clusters according to their distance from the means of the clusters, and then updates the cluster means by averaging out the points in each cluster. The algorithm uses a random initialization, and comes with no guarantee to reach the global minima. The K-means++ in Arthur & Vassilvitskii (2007) addresses the problem by using an initial seeding of the means, and converges within O (logK) of the global optima.",
      "startOffset" : 46,
      "endOffset" : 544
    }, {
      "referenceID" : 0,
      "context" : "K-means is an NP hard problem even for K = 2 (Aloise et al. (2009)), and only heuristic solutions exist. Perhaps the most common of such heuristics is the Lloyd’s algorithm, which uses an EM style approach; it first assigns the points to different clusters according to their distance from the means of the clusters, and then updates the cluster means by averaging out the points in each cluster. The algorithm uses a random initialization, and comes with no guarantee to reach the global minima. The K-means++ in Arthur & Vassilvitskii (2007) addresses the problem by using an initial seeding of the means, and converges within O (logK) of the global optima. However, for most practical purposes, a bound of O (logK) might be too high. Moreover, the algorithm uses K passes through the entire dataset to seed the means; this makes it very expensive for large amount of data. K-means|| in Bahmani et al. (2012) suggests a method to oversample the means during every pass to reduce the number of passes in the seeding phase.",
      "startOffset" : 46,
      "endOffset" : 911
    }, {
      "referenceID" : 0,
      "context" : "There are other variants of K-means algorithm, such as Ailon et al. (2009) and Shindler et al.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "There are other variants of K-means algorithm, such as Ailon et al. (2009) and Shindler et al. (2011). Most of these algorithms build on K-means++ and aim to reduce the computational burden, rather than improving accuracy.",
      "startOffset" : 55,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "There are other variants of K-means algorithm, such as Ailon et al. (2009) and Shindler et al. (2011). Most of these algorithms build on K-means++ and aim to reduce the computational burden, rather than improving accuracy. Hence, we limit our discussion on K-means++ and K-means||. There have been recent developments on clustering algorithms based on Method of Moments (MoM), also referred to as Spectral Methods in the literature. Unlike traditional clustering algorithms which rely on EM or similar algorithms to maximize the likelihood of the data, MoM tries to learn the parameters from the higher order moments of the data. The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al.",
      "startOffset" : 55,
      "endOffset" : 733
    }, {
      "referenceID" : 0,
      "context" : "There are other variants of K-means algorithm, such as Ailon et al. (2009) and Shindler et al. (2011). Most of these algorithms build on K-means++ and aim to reduce the computational burden, rather than improving accuracy. Hence, we limit our discussion on K-means++ and K-means||. There have been recent developments on clustering algorithms based on Method of Moments (MoM), also referred to as Spectral Methods in the literature. Unlike traditional clustering algorithms which rely on EM or similar algorithms to maximize the likelihood of the data, MoM tries to learn the parameters from the higher order moments of the data. The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al.",
      "startOffset" : 55,
      "endOffset" : 814
    }, {
      "referenceID" : 0,
      "context" : "There are other variants of K-means algorithm, such as Ailon et al. (2009) and Shindler et al. (2011). Most of these algorithms build on K-means++ and aim to reduce the computational burden, rather than improving accuracy. Hence, we limit our discussion on K-means++ and K-means||. There have been recent developments on clustering algorithms based on Method of Moments (MoM), also referred to as Spectral Methods in the literature. Unlike traditional clustering algorithms which rely on EM or similar algorithms to maximize the likelihood of the data, MoM tries to learn the parameters from the higher order moments of the data. The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic",
      "startOffset" : 55,
      "endOffset" : 837
    }, {
      "referenceID" : 2,
      "context" : "Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al.",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al.",
      "startOffset" : 12,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012), for Mixtures of Gaussian in Hsu & Kakade (2013) and for Spectral Experts of Linear Regression in Tejasvi Chaganty & Liang (2013).",
      "startOffset" : 12,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012), for Mixtures of Gaussian in Hsu & Kakade (2013) and for Spectral Experts of Linear Regression in Tejasvi Chaganty & Liang (2013).",
      "startOffset" : 12,
      "endOffset" : 186
    }, {
      "referenceID" : 2,
      "context" : "Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012), for Mixtures of Gaussian in Hsu & Kakade (2013) and for Spectral Experts of Linear Regression in Tejasvi Chaganty & Liang (2013). Here we propose a probabilistic K-means algorithm based on Method of Moments.",
      "startOffset" : 12,
      "endOffset" : 267
    }, {
      "referenceID" : 3,
      "context" : "If the estimated value of M̃3 from training sample, denoted as ˆ̃ M3, varies from the actual M̃3 of the population in a way such that ∥∥∥ ˆ̃ M3 − M̃3∥∥∥ < for some > 0, then the complexity of the tensor factorization (Algorithm 4) is O(k(log(k) + loglog(1/ )), where δ is a small positive number, with 0 < δ 1 (Please refer to (Anandkumar et al., 2014) for a detailed derivation).",
      "startOffset" : 327,
      "endOffset" : 352
    }, {
      "referenceID" : 2,
      "context" : "We adopt the tensor factorization from Anandkumar et al. (2014), which computes the eigen-vectors of the tensor M̃3 through power iteration followed by deflation, as described as Algorithm 4.",
      "startOffset" : 39,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "We adopt the tensor factorization from Anandkumar et al. (2014), which computes the eigen-vectors of the tensor M̃3 through power iteration followed by deflation, as described as Algorithm 4. The term T (·, θ, θ) ∈ R is the product of the tensor T ∈ RK×K×K with the vector θ ∈ R along any two axes. The space complexity of the overall algorithm is O(K + DK), whereas time complexity is O((N + D)K + NDK) excluding the tensor factorization step. Since the space complexity is independent of the number of instances in X , the algorithm has the capability to scale to a large amount of data. Method of Moments is proven to be globally convergent ( please see Anandkumar et al. (2014) for proof).",
      "startOffset" : 39,
      "endOffset" : 682
    }, {
      "referenceID" : 5,
      "context" : "For K-means||, we use r = 5 and l = 2K, since these settings usually give the best results as described in Bahmani et al. (2012). For each method of initialization, we run the K-means iterations until φt−1(C)− φ(C) < 10−3φt−1(C), where φ(C) is the cost defined in Equation 1 at iteration t.",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : "3 Human Activities and Postural Transitions (HAPT) Dataset: We perform our experiments on the training set of Smartphone-Based Recognition of Human Activities and Postural Transitions Dataset from UCI Repository used in Reyes-Ortiz et al. (2015). The training set consists of 7767 instances with each instance containing 561 attributes corresponding to 12 distinct activities and postures.",
      "startOffset" : 220,
      "endOffset" : 246
    }, {
      "referenceID" : 2,
      "context" : "Our model is very similar to the Latent Dirichlet Allocation using Method of Moments presented in Anandkumar et al. (2012), where the pair-wise and triple-wise probabilities replace the covariance and third order moment.",
      "startOffset" : 98,
      "endOffset" : 123
    } ],
    "year" : 2017,
    "abstractText" : "K-means is one of the most widely used algorithms for clustering in Data Mining applications, which attempts to minimize the sum of square of Euclidean distance of the points in the clusters from the respective means of the clusters. The simplicity and scalability of K-means makes it very appealing. However, K-means suffers from local minima problem, and comes with no guarantee to converge to the optimal cost. K-means++ tries to address the problem by seeding the means using a distance based sampling scheme. However, seeding the means in K-means++ needsO (K) passes through the entire dataset, which could be very costly in large amount of dataset. Here we propose a method of seeding initial means based on higher order moments of the data, which takes O (1) passes through the entire dataset to extract the initial set of means. Our method yields competitive performance with respect to all the existing K-means algorithms, whilst avoiding the expensive mean selection steps of K-means++ and other heuristics. We demonstrate the performance of our algorithm in comparison with the existing algorithms on various benchmark datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}