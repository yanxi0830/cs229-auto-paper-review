{
  "name" : "1701.06725.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Contextual Bandit Approach for Stream-Based Active Learning",
    "authors" : [ "Linqi Song", "Jie Xu" ],
    "emails" : [ "songlinqi@ucla.edu", "jiexu@miami.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nContextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5]. In a typical setting, a task arrives to the system with certain contextual information (e.g. incoming user’s age, gender, search and purchase history etc. in online content recommendation), then the system pulls an arm from a possibly very large arm space (e.g. recommend a piece of online content from a large content pool). A reward is later realized depending on the context value and the selected arm. The objective of a learner (or a learning algorithm) is to make arm selection decisions based on the history of context-armreward realizations to minimize the learning regret (i.e. the gap of achievable reward compared with certain benchmarks). A common assumption made in the literature is that the reward of each task is observed by the learner at no cost,\nthereby allowing the learner to fully and freely utilize this information. While this assumption holds true in some application scenarios, it hardly captures the reality in many others in which observing the ground truth reward requires substantial manpower, time, energy and/or other resources. For instance, to calculate the reward in stream mining systems, human experts are needed to manually annotate the ground truth labels of the mining tasks. Therefore, in addition to carefully deciding which arm to pull, the learner also has to actively and judiciously acquire the ground truth rewards from an annotator by assessing the benefits and costs of obtaining them in these application scenarios [6][7]. Figure 1 illustrates the considered active learning scenario.\nIn this paper, we design a learning algorithm, called Contextual Bandits with Active Learning (CB-AL), that accomplishes the aforementioned task. We prove that CB-AL is orderoptimal in terms of the learning regret, which matches that of conventional contextual bandits in cost-free scenarios. The key to achieving the optimal regret order by our proposed algorithm is that the query about the ground truth reward is sent to the annotator together with some prior information about this reward. Although the learner does not directly observe the reward realization by selecting an arm, it is learning the distribution of the reward as it learns the optimal arm to pull, and this statistical information can be utilized to reduce the cost of acquiring the ground truth reward by an annotator. This is in stark contrast with conventional active learning literature where the cost of acquiring the ground truth is constant [8][9][10].\nOur algorithm is able to effectively deal with large context and arm spaces. To this end, our algorithm divides time into epochs and the context/arm spaces are adaptively partitioned across epochs. The partitions become finer and finer as the\nar X\niv :1\n70 1.\n06 72\n5v 1\n[ cs\n.L G\n] 2\n4 Ja\nn 20\n17\nepoch grows. Within each epoch, our algorithm first explores various arm clusters (defined for each arm subspace) to learn their reward estimates for each context cluster (defined for each context subspace) and removes suboptimal arm clusters during the course of learning. When the remaining arm clusters are learned to be optimal or near-optimal, the algorithm enters an exploitation phase in which the arm cluster removal operation stops and acquiring the ground truth rewards is no longer needed for the remaining time slots in the current epoch, thereby maximizing the reward and minimizing the query cost concurrently. To optimize the overall long term performance and minimize the long-term learning regret, our algorithm carefully designs control functions that determine what arm clusters are optimal, near-optimal and suboptimal. The remainder of this paper is organized as follows. Section II formulates the problem and defines the learning regret. Section III describes our algorithm whose regret performance is analyzed in Section IV. Section V provides illustrative numerical results followed by conclusions in Section VI."
    }, {
      "heading" : "II. PROBLEM FORMULATION",
      "text" : ""
    }, {
      "heading" : "A. System Model",
      "text" : "We consider a discrete time system where time is divided into slots t = 1, 2, .... The arm space is a bounded space K with covering dimension dK . The context space is a bounded space X with covering dimension dX . For any context x ∈ X , the reward of selecting arm k ∈ K is r(x, k) ∈ [0, 1], which is sampled according to some underlying but unknown distribution f(x, k). The expected value of r(x, k) is denoted as µ(x, k), which is unknown too. We assume that the reward value space is [0, 1] for the ease of exposition but this assumption can be relaxed to account for any bounded interval. In the conventional contextual bandits setting, the following events occur in sequence in each time slot t: (1) A context xt ∈ X arrives; (2) An arm kt ∈ K is selected; (3) The (groundtruth) reward r(xt, kt) is generated according to f(xt, kt) and is observed by the learner at no cost as feedback, which provides information for future arm selections. In our considered setting, r(xt, kt) is not observed for free. Instead, there is a cost associated with requesting the ground truth reward. Therefore, there is a need to actively and judiciously decide when to request the ground truth reward to balance the learning efficiency and the cost minimization. Thus, in addition to deciding which arm kt to choose, the learner also has to decide whether or not to query the ground truth reward at a cost, denoted by qt ∈ {0, 1}, where qt = 1 stands for requesting and qt = 0 stands for not requesting. We consider that the query cost is not fixed, but a function of the prior information about the ground truth, which is updated as learning goes on. The intuition is that if the prior information is more informative, then the query cost should be smaller. In particular, we define the prior information about the reward r(xt, kt) as a tuple (at, bt, δt), which represents that the expected reward µ(xt, kt) is in the region [at, bt] with probability at least 1− δt. The query cost is then defined as a\nconvex increasing function of the confidence interval bt − at and the significance level δt of the following form:\nct = c[(bt − at)β1 + ηδβ2t ] (1)\nwhere c > 0, β1 ≥ 1, β2 ≥ 1, η > 0 are constant parameters. Therefore, a larger confidence interval bt − at and a smaller confidence level 1−δt result in a higher query cost. We choose this form of query cost because it captures the reality to a large extent and also is amenable to our subsequent analysis. Let r̂t denote the observed reward in time slot t, which is r̂t = r(xt, kt) if qt = 1, and r̂t = ∅ if qt = 0. Given the context arrival process, the selected arm sequence and the observed reward sequence, the history by time slot t is defined as\nht−1 = {(x1, k1, r̂1), ..., (xt−1, kt−1, r̂t−1))}, ∀t > 1 (2)\nand h0 = ∅ for t = 1. The set of all possible histories is denoted by H. An algorithm π is a mapping π : H × X → K×{0, 1}, which selects an arm and decides whether or not to query given the history and the current context. For the ease of exposition, we separately write πtK = πK(h\nt−1, xt) and πtq = πq(h\nt−1, xt) for the arm selection component and the query decision component of the algorithm, respectively."
    }, {
      "heading" : "B. Learning Regret",
      "text" : "We use the total expected payoff (i.e. the reward minus the query cost) to describe the performance of an algorithm π. The total expected payoff up to time slot T is thus\nUπ(T ) = E T∑\nt=1\n[r(xt, kt)− ctqt] (3)\nwhere the expectation is taken over the context arrival process and the reward distributions. We compare an algorithm with the static-best oracle policy π∗ which knows the reward distributions a priori. Therefore, in each time slot t, the oracle policy selects the arm k∗t = argmaxk µ(xt, kt) that maximizes the expected reward. Clearly, since the oracle knows the reward distributions, there is no need for it to query the ground truth to learn about them. Therefore, q∗t = 0, ∀t. The learning regret of an algorithm π is defined as\nRπ(T ) = Uπ∗(T )− Uπ(T ) (4)\nAs a widely-adopted assumption in contextual bandits literature [1][3], the reward function is assumed to satisfy a Lipschitz condition with respect to both the context and the arm. This assumption is formalized as follows.\nAssumption 1. For any two contexts x, x′ ∈ X and two arms k, k′ ∈ K, the expected rewards satisfy\n|µ(x, k)− µ(x′, k)| ≤ LX‖x− x′‖ (5) |µ(x, k)− µ(x, k′)| ≤ LK‖k − k′‖ (6)\nwhere LX , LK are the Lipschitz constants for the context space and the arm space, respectively."
    }, {
      "heading" : "III. THE ALGORITHM",
      "text" : "In this section, we describe the proposed contextual bandits\nalgorithm with active learning (CB-AL)."
    }, {
      "heading" : "A. Useful Notions",
      "text" : "First, we introduce some useful notions for the algorithm. Context/Arm Space Partition. Time slots are grouped into epochs. The i-th epoch lasts for Ti = 2i time slots. At the beginning of each epoch, the context space and the arm space are partitioned into small subspaces. A context (arm) space is called a context (arm) cluster. The context/arm space partition is kept unchanged throughout the entire epoch. Formally, the partition of the context space for epoch i is denoted by PX(i) = {X1,X2, ...,XMi} consisting of Mi subspaces. Similarly, the partition of the arm space for epoch i is denoted by PK(i) = {K1,K2, ...,KNi} consisting of Ni subspaces. The radius of a context cluster Xm is half of the maximum distance between any two context points in the cluster, i.e.\nρXm = 0.5 sup x,x′∈Xm\n‖x− x′‖ (7)\nThe radius of an arm cluster is defined similarly. The context/arm space partitioning is performed such that the context/arm clusters satisfy ∀m = {1, ...,Mi}, ρXm,i = T−αi , ρX,i and ∀n = {1, ..., Ni}, ρKn,i = T−αi , ρK,i, where α ∈ (0, 1). Active Arm Cluster. At the beginning of each epoch, all arm clusters according to the arm space partitioning are set to be active. We denote the set of active arm clusters with respect to context cluster Xm in epoch i by Am(i). The active arm cluster set will be updated as time goes by according to the learning outcome. Some arm clusters will be learned to be suboptimal and hence will be de-activated (i.e. be removed from Xm) and will not be selected by the algorithm in the remaining time slots of the current epoch. Round. A round sm(i) is defined for each context cluster Xm in each epoch i, which consists a number of |Am(i)| time slots. Thus, in each round sm(i), each active arm cluster in Am(i) is selected once. Therefore, even in the same epoch, the length of a round sm(i) may change due to the updating of the active arm cluster set Am(i). Control Functions. There are two important control functions in our algorithm. The first control function, denoted by Di(i, sm(i)), is used to de-active arm clusters, with respect to each context cluster Xm, that are learned to be suboptimal depending on the epoch index i and the round index sm(i). In particular, Di(i, sm(i)) has the form\nD1(i, sm(i)) = ǫ(i) + [2D(sm(i)) + 2LXρX,i + 2LKρK,i]\nwhere ǫ(i) = LT−αi is a small positive value for epoch i,\nand D(sm(i)) = √ ln(2T 1+γi )/2sm(i). Here L = L(c) > 4LX + 4LK and γ ∈ (0, 1) are constants. The second control function, denoted by D2(i, sm(i)), is used to determine when to stop querying the ground truth reward. When the stopping condition is satisfied, the algorithm stops de-activating arm clusters and enters a pure exploitation\nphase for the remaining time slots of the current epoch for the context cluster Xm. In particular, D2(i, sm(i)) has the form\nD2(i, sm(i)) = 2ǫ(i)− [2D(sm(i)) + 2LXρX,i + 2LKρK,i]\nSample Mean Reward. The sample mean reward of an arm cluster Kn with respect to a context cluster Xm by round sm(i) in epoch i is denoted by r̄m,n(sm(i)). The sample mean reward of the empirical best arm cluster is r̄∗m(sm(i)) = maxKn∈Am(i) r̄m,n(sm(i))."
    }, {
      "heading" : "B. The Algorithm",
      "text" : "Now, we describe the proposed CB-AL algorithm, whose pseudo-code is provided in Algorithm 1. Figure 2 provides an illustration of the algorithm. The algorithm operates in epochs. At the beginning of each epoch, the context/arm space partitions are determined. As aforementioned, the radii of the context and arm spaces become smaller as the epoch grows and therefore, the partitions of the spaces become finer and finer. All arm clusters for any context cluster are set to be active at the beginning of each epoch. In each time slot t, a context xt arrives and the algorithm finds the context cluster Xm ∈ Pm(i) that it belongs to. Depending on which phase the algorithm is in (with respect to the context cluster Xm), different operations are carried out as follows: Exploration. The goal of the algorithm in the exploration phase is to explore various active arm clusters to learn their performance for Xm. Arm clusters that are learned to be suboptimal will be de-activated over time, thereby improving the learning efficiency and system performance. In each round sm(i), the algorithm selects an active arm cluster Kn ∈ Am(i) that has not been selected in the current round for Xm. If all active arm clusters have been selected in the current round, then the current round sm(i) ends and a new round begins. Then the algorithm arbitrarily selects any arm kt in the selected arm cluster Kn. In the exploration phase, a query is always sent, namely qt = 1, together with prior information (at, bt, δt). The prior information is computed as follows: for round sm(i) > 1, the prior information is\nat = r̄m,n − 2LXρX,i − 2LKρK,i − 2D(sm(i)− 1) (8) bt = r̄m,n + 2LXρX,i + 2LKρK,i + 2D(sm(i)− 1) (9) δt = T −(1+γ) i (10)\nAlgorithm 1 Contextual Bandits with Active Learning 1: for epoch i = 0, 1, 2, ... do 2: Initialization: Create context and arm space partitioning PX(i) and PK(i). Set Am(i) = PK(i), ∀m. Set Stopm =\n0, ∀m. Set sm(i) = 1, ∀m. Set r̄m,n = 0, ∀m,n. 3: for time slot t = 2i to 2i+1 − 1 do 4: Observe the context xt and find Xm such that xt ∈ Xm. 5: switch Stopm do 6: case 0 ⊲ Exploration 7: Select Kn ∈ Am(i) that has not been selected in round sm(i) and select any kt ∈ Kn. 8: Choose qt = 1 and send the prior information (at, bt, δt) to the annotator 9: (A query cost ct is incurred, and the reward r̂t = r(xt, kt) is observed.) 10: Update r̄m,n(sm(i)). 11: if round sm(i) has finished then 12: For any Kn ∈ Am(i) such that ∆m,n(sm(i)) ≥ D1(i, sm(i)), remove Kn from Am(i). 13: If for all Kn ∈ Am(i), ∆m,n(sm(i)) ≤ D2(i, sm(i)), then set Stopm = 1. 14: Update sm(i)← sm(i) + 1. 15: end if 16: case 1 ⊲ Exploitation 17: Select any Kn ∈ Am(i) and any kt ∈ Kn. 18: Choose qt = 0. 19: (The reward r(xt, kt) is generated, but cannot be observed.) 20: end for 21: end for\nFor sm(i) = 1, the prior information is at = 0, bt = 1, δ = 0. Once the ground truth reward r(xt, kt) is obtained from the annotator, the sample mean r̄m,n is updated as follows\nr̄m,n ← (r̄m,n · (sm(i)− 1) + r(xt, kt))/sm(i) (11) At the end of a round sm(i), the algorithm de-activates suboptimal arm clusters if necessary. Specifically, the algorithm first finds the empirically best arm cluster for Xm and calculates the sample mean reward difference between the empirically best arm cluster and any other active arm cluster, denoted by ∆m,n(sm(i)) , r̄∗m(sm(i))− r̄m,n(sm(i)), ∀Kn ∈ Am(sm(i)). Then it compares ∆m,n(sm(i)) with the current value of the control function D1(i, sm(i)). If the sample mean reward difference is greater than or equal to this value, then the corresponding arm cluster is suboptimal with high probability and hence is de-activated. Moreover, if the remaining active arm clusters have sufficiently similar sample mean reward estimates, then the algorithm stops the de-activation process in the remaining time slots of the current epoch and enters the exploitation phase. Specifically, if the reward difference for any active cluster is smaller or equal to D2(i, sm(i)), then the de-activation process stops. We denote by Sim the number of rounds taken when the stopping condition is satisfied. Exploitation. The goal of the algorithm in the exploitation phase is to exploit the best arms to maximize the reward. Since in the exploitation phase, the remaining active arm clusters are the optimal arm cluster or near-optimal arm cluster for the corresponding context cluster with high probability, the algorithm simply arbitrarily selects an active arm cluster Kn ∈ Am(i) and then arbitrarily selects an arm from Kn. Notably,\nthe algorithm no longer requests for the ground truth reward, i.e. qt = 0, for all time slots in the exploitation phase."
    }, {
      "heading" : "IV. REGRET ANALYSIS",
      "text" : "To analyze the regret, we first introduce some notions. • Cluster reward. We define the expected reward of selecting an arm cluster Kn for context cluster Xm as µ(m,n) = maxx∈Xm,k∈Kn µ(x, k). The reward of the optimal arm cluster with respect to the context cluster Xm is thus µ∗m = maxn µ(m,n). Furthermore, we define the reward difference as ∆m,n = µ∗m − µ(m,n). • ǫ-optimal arm cluster. We define the ǫ-optimal arm clusters with respect to the context cluster Xm as the arm clusters Kn that satisfy µ(m,n) ≥ µ∗m − ǫ. Similarly, the ǫ-suboptimal arm clusters are those that satisfy µ(m,n) < µ∗m − ǫ. • Normal event and abnormal event. A normal event Nm,n(sm(i)) is an event such that the reward of selecting arm cluster Kn for context cluster Xm in round sm(i) satisfies |r̄m,n(sm(i)) − E[rm,n(sm(i))]| ≤ D(sm(i)). A abnormal event N cm,n(sm(i)) is an event such that |r̄m,n(sm(i)) − E[rm,n(sm(i))]| > D(sm(i)). Further, we denote Ni,m,n as the event that no abnormal event N cm,n(sm(i)) occurs with respect to arm cluster Kn and context cluster Xm for the entire epoch i. To analyze the regret, we first provide the following lemmas.\nLemma 1. An abnormal event for arm cluster Kn in epoch i occurs with probability at most δ(i) = T−γi .\nProof. According to the definition of abnormal event and the Chernoff-Hoeffding bound, the probability that an abnormal\nevent for an arm cluster occurs in round sm(i) can be bounded by\nPr{[Nm,n(sm(i))]C} ≤ 2e−2[D(sm(i))] 2sm(i) ≤ 1\nT 1+γi . (12)\nHence, the probability that an abnormal event for an arm cluster Kn in epoch i occurs with at most\n∑ m Pr{[Ni,m,n]C} ≤ ∑ m Sim∑ sm(i)=1 Pr{[Nm,n(sm(i))]C} ≤∑ m Sim∑ sm(i)=1 1 T 1+γi ≤ 1 Tγi .\n(13)\nLemma 2. (a) With probability at least 1 −Niδ(i), an ǫ(i)optimal arm clusters are not de-activated for context cluster Xm in epoch i. (b) With probability at least 1 − Niδ(i), the active set Am(i) in the exploitation phase contains at most 2ǫ(i)-optimal arm clusters for context cluster Xm in epoch i. Proof. If the normal event occurs, for any deactivated arm clusters Kn, we have:\nr̄∗m(sm(i))− r̄m,n(sm(i)) = (µ∗m − µ(m,n)) + (r̄∗m(sm(i))− µ∗m) + (µ(m,n)− r̄m,n(sm(i))) ≤ ∆m,n + 2D(sm(i)) + 2LXρX,i + 2LKρK,i,\n(14)\nwhere the inequality follows from that r̄∗m(sm(i)) − µ∗m ≤ D(sm(i)) and µ(m,n) − r̄m,n(sm(i)) ≤ D(sm(i)) + 2LXρX,i + 2LKρK,i. Combining with the deactivating rule, we have ∆m,n > ǫ(i). If the normal event occurs, for any reserved active arm cluster Kn, we also have: r̄∗m(S i m)− r̄m,n(Sim)\n= (µ∗m − µ(m,n)) + (r̄∗m(Sim)− µ∗m) + (µ(m,n)− r̄m,n(Sim))\n≥ ∆m,n − 2(D(Sim) + LXρX,i + LKρK,i), (15)\nwhere the inequality follows from that µ∗m − r̄∗m(Sim) ≤ D(Sim) + 2LXρX,i + 2LKρK,i and r̄m,n(Sim) − µ(m,n) ≤ D(sm(i)). Combining with the stopping rule, we have ∆m,n ≤ 2ǫ(i). Since the normal event occurs with probability at least 1−Niδ(i), the results follow. Now we are ready to prove the regret of CB-AL.\nTheorem 1. The regret of the CB-AL algorithm can be upperbounded by R(T ) = O(T dX+dK+1 dX+dK+2 ).\nProof. To bound the regret, we first consider the regret caused in epoch i, denoted by Ri. This regret can be decomposed into four terms: the regret Rai caused by abnormal events, the regret Rni caused by 2ǫ(i)-optimal arm cluster selection and the inaccuracy of clusters, the regret Rsi caused by 2ǫ(i)suboptimal arm cluster selection when no abnormal events occur, and the query cost Rqi . We have\nRi ≤ Rai +Rni +Rsi +Rqi . (16)\nLet us denote by Ti the number of time slots in epoch i, denote by Ti,m the number of context arrivals in context cluster Xm in epoch i, and denote by Ti,m,n the number of query requests for arm cluster Kn in context cluster Xm in epoch i. We set α = 1dA+dX+2 and γ = dA+1 dA+dX+2\n. For the first term Rai in (16), when an abnormal event happens, the regret is at most Ti. According to Lemma 1 abnormal events happens with probability at most δ(i) for arm cluster Kn in epoch i. Therefore, the regret Rai in (16) can be expressed as:\nRai ≤ Ni∑\nn=1\nδ(i)Ti ≤ Niδ(i)Ti. (17)\nFor the second term Rni in (16), the regret of 2ǫ(i)-optimal arm cluster selection at each time slot is at most 2ǫ(i), and the regret of inaccuracy of clusters at each time slot is at most 2LXρX,i + 2LKρK,i. Therefore, the regret Rni can be expressed as:\nRni ≤ 2i+1−1∑ t=2i (2ǫ(i) + 2LXρX,i + 2LKρK,i) = 2(ǫ(i) + LXρX,i + LKρK,i)Ti.\n(18)\nFor the third term Rsi in (16), when the normal event occurs, according to Lemma 2, 2ǫ(i)-suboptimal arm cluster can only be selected in the exploration phases. Hence, the regret Rsi can be expressed as:\nRsi ≤ E ∑\n∆m,n>2ǫ(i) 2i+1−1∑ t=2i\n∆m,nI{xt ∈ Xm, πtK ∈ Kn, πtq = 1,Ni,m,n}. (19)\nAccording to the deactivating rule, for normal events, if the following is satisfied:\n∆m,n−2D(s)−2LXρX,i−2LKρK,i ≥ r̄∗m(s)−r̄m,n(s) ≥ D1(i, s), (20) then the arm cluster is deactivated. Hence, the rounds of exploring arm cluster Kn, Ti,m,n with ∆m,n > 2ǫ(i), can be bounded by\nTi,m,n ≤ 8 ln(2T 1+γi )\n[∆m,n − (ǫ(i) + 4LXρX,i + 4LKρK,i)]2 . (21)\nTherefore, the regret Rsi can be bounded by\nRsi ≤ E ∑\n∆m,n>2ǫ(i)\n∆m,nTi,m,n\n≤ E ∑ ∆m,n>2ǫ(i) ( 8 ln(2T 1+γi ) ∆m,n−(ǫ(i)+4LXρX,i+4LKρK,i)\n+ 8(ǫ(i)+4LXρX,i+4LKρK,i) ln(2T\n1+γ i )\n[∆m,n−(ǫ(i)+4LXρX,i+4LKρK,i)]2 )\n≤ 8MiNi ln(2T 1+γ i )\n2ǫ(i)−(ǫ(i)+4LXρX,i+4LKρK,i)\n+ 8MiNi(ǫ(i)+4LXρX,i+4LKρK,i) ln(2T\n1+γ i )\n[2ǫ(i)−(ǫ(i)+4LXρX,i+4LKρK,i)]2 ≤ C1MiNi ln(2T 1+γi )Tαi ,\n(22)\nwhere C1 = 16L(L−4LX−4LK)2 is a constant. For the fourth term Rqi in (16), we first consider the query cost Rq,1i when the abnormal event occurs. In this case, since\nthe maximum query cost per slot is 2c, the query cost can be bounded by\nRq,1i ≤ Ni2cδ(i)Ti. (23)\nNext, we consider the query cost Rq,2i in the case that only normal events occur. This can be bounded by\nRq,2i ≤ E ∑ m,n 2i+1−1∑ t=2i ctI{xt ∈ Xm, πtK ∈ Kn, πtq = 1} ≤ E ∑ m,n c+ E ∑ m,n Sim∑ s=2 [c(4LXρX,i + 4LKρK,i + 4D(s− 1))β1\n+ cηT −(1+γ)β2 i ]\n≤ cMiNi + cMiNi Sim∑ s=2 (8LXρX,i+8LKρK,i) β1+(8D(s−1))β1 2\n+ cηMiNi Sim∑ s=2 T −(1+γ)β2 i ]\n≤ cMiNi + cMiNi23β1−1(LXρX,i + LKρK,i)β1Sim\n+cMiNi2 3β1−1 Sim∑ s=2 [ln(2T γ+1 i )] β1/2 2β1/2(s−1)β1/2 + cηMiNiT −(1+γ)β2+1 i ,\n(24) where the third inequality is due to the Jensen’s inequality. If 1 ≤ β1 < 2, the third term on the right hand side of the last inequality in (24) can be bounded by cNi2 5β1/2−1 [ln(2T γ+1 i )] β1/2(Sim) 1−β1/2\n1−β1/2 , due to the divergent series ∑T t=1 t\n−y ≤ T (1−y)/(1 − y) for 0 < y < 1 [11]. If β1 ≥ 2, the third term on the right hand side of the last inequality in (24) can be bounded by cNi2 5β1/2−1[ln(2T γ+1i )] β1/2(lnSim)\nβ1/2, due to the series∑T−1 t=1 t\n−y ≤ lnT for y ≥ 1. We can also have the bound of Sim (for all m) due to the fact that when D1(i, S i m) ≤ D2(i, S i m), the stopping rule is satisfied. Hence, Sim can be bounded by the minimum s, such that D1(i, s) ≤ D2(i, s). This shows:\nSim ≤ 8T 2αi ln(2T γ+1 i )\n(L− 4LX − 4LK)2 . (25)\nThus, we can bound Rq,2i by\nRq,2i ≤ { C2MiNiT α(2−β1) i ln(2T γ+1 i ), if 1 ≤ β1 < 2\nC3MiNi[ln(2T γ+1 i )] β1 , if β1 ≥ 2, (26)\nwhere C2 = c(1 + η) + c23β1+5(LX+LK) β1\n(L−4LX−4LK)2 + c26−β1/2\n(2−β1)(L−4LX−4LK)2 is a constant, C3 = c(1 + η + 25β1/2−1) + c2 3β1+5(LX+LK) β1\n(L−4LX−4LA)2 is a constant. According to the definition of covering dimensions [12], the maximum number of arm clusters can be bounded by Ni ≤ CAρ−dAK,i in epoch i, and the maximum number of context clusters can be bounded by Mi ≤ CXρ−dXX,i in epoch i, where CA, CX are covering constants for the arm space and\nthe context space. Hence, the regret can be bounded by\nR(T ) ≤ E log2T∑ i=0 Ri ≤ log2T∑ i=0 (δ(i)Ni + 2cδ(i)Ni + 2ǫ(i) + 2LXρX,i + 2LKρK,i)Ti +E log2T∑ i=0 O(1) ln(2T 1+γi )T α i ≤ O(1)T dX+dA+1\ndX+dA+2 ln(T ). (27)\nTherefore, the result of Theorem 1 follows.\nWe further show a lower bound for the CB-AL algorithm. Since the proposed algorithm incurs the query cost when it requests a ground truth, the lower bound of the regret cannot be lower than that of the conventional contextual MAB setting where no query cost is incurred [1].\nTheorem 2. The regret of the CB-AL algorithm can be lowerbounded by R(T ) = Ω(T dX+dK+1 dX+dK+2 ).\nTheorem 1 and Theorem 2 together show that our algorithm is order-optimal and achieves the same order as conventional contextual bandits algorithms in cost-free scenarios [1][2][3]."
    }, {
      "heading" : "V. NUMERICAL RESULTS",
      "text" : "We conduct illustrative experiments using synthetic data with 2-dimensional contexts and 2-dimensional arms. In our first experiment, we compare the performance of our proposed CB-AL algorithm with the Contextual Bandit algorithm and the Contextual Bandit Active Learning without considering prior information (CB-AL without prior information). The result is shown in Fig. 3. As we can see, the proposed CB-AL algorithm performs better, in terms of payoff, than the conventional bandit algorithm (by 16%) and the CBAL without prior information (by 13%) by the end of the experiment duration. In our second experiment, we show the payoffs achieved by our proposed CB-AL algorithm when the query cost varies (by changing cost parameters c in e.q. (1)). We show the result in Fig 4. We can see that as c increases from 0.1 to 1, the achieved payoff decreases by 15% for T = 10000 and by 8% for T = 20000."
    }, {
      "heading" : "VI. CONCLUSIONS",
      "text" : "In this paper, we developed a contextual bandits learning algorithm with active learning capability. The active learning cost is reduced by providing prior information about the reward realization to the annotator. The algorithm maintains and updates partitions of the context and arm spaces, and operates between exploration and exploitation phases. Through precise control of the partitioning process and when to request the ground truth of the reward, the algorithm gracefully balances the accuracy of learning and the cost incurred by active learning. We prove that the regret of the proposed algorithm achieves the same order as that of conventional contextual bandits algorithms in cost-free scenarios."
    } ],
    "references" : [ {
      "title" : "and M",
      "author" : [ "T. Lu", "D. Pál" ],
      "venue" : "Pál, “Contextual multi-armed bandits,” in Artificial Intelligence and Statistics Conference (AISTATS)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The epoch-greedy algorithm for multiarmed bandits with side information,",
      "author" : [ "J. Langford", "T. Zhang" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Contextual bandits with similarity information.",
      "author" : [ "A. Slivkins" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "and M",
      "author" : [ "L. Song", "W. Hsu", "J. Xu" ],
      "venue" : "van der Schaar, “Using contextual learning to improve diagnostic accuracy: Application in breast cancer screening,” IEEE Journal of Biomedical and Health Informatics, vol. 20, no. 3, pp. 902–914",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "and M",
      "author" : [ "J. Xu", "T. Xing" ],
      "venue" : "van der Schaar, “Personalized course sequence recommendations,” IEEE Transactions on Signal Processing, vol. 64, no. 20, pp. 5340–5352",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Active learning literature survey,",
      "author" : [ "B. Settles" ],
      "venue" : "University of Wisconsin, Madison,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "and M",
      "author" : [ "D.A. Cohn", "Z. Ghahramani" ],
      "venue" : "I. Jordan, “Active learning with statistical models,” Journal of Artificial Intelligence Research, vol. 4, no. 1, pp. 129–145",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Statistical active learning algorithms,",
      "author" : [ "M.-F.F. Balcan", "V. Feldman" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Nigamy, “Employing em and pool-based active learning for text classification,",
      "author" : [ "K.A.K. McCallumzy" ],
      "venue" : "in Proc. International Conference on Machine Learning (ICML),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Analysis of a greedy active learning strategy.",
      "author" : [ "S. Dasgupta" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "An approximate formula for a partial sum of the divergent p-series,",
      "author" : [ "E. Chlebus" ],
      "venue" : "Applied Mathematics Letters, vol. 22,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Lectures on analysis on metric spaces",
      "author" : [ "J. Heinonen" ],
      "venue" : "Springer Science & Business Media",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].",
      "startOffset" : 282,
      "endOffset" : 285
    }, {
      "referenceID" : 4,
      "context" : "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].",
      "startOffset" : 313,
      "endOffset" : 316
    }, {
      "referenceID" : 5,
      "context" : "Therefore, in addition to carefully deciding which arm to pull, the learner also has to actively and judiciously acquire the ground truth rewards from an annotator by assessing the benefits and costs of obtaining them in these application scenarios [6][7].",
      "startOffset" : 249,
      "endOffset" : 252
    }, {
      "referenceID" : 6,
      "context" : "Therefore, in addition to carefully deciding which arm to pull, the learner also has to actively and judiciously acquire the ground truth rewards from an annotator by assessing the benefits and costs of obtaining them in these application scenarios [6][7].",
      "startOffset" : 252,
      "endOffset" : 255
    }, {
      "referenceID" : 7,
      "context" : "This is in stark contrast with conventional active learning literature where the cost of acquiring the ground truth is constant [8][9][10].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 8,
      "context" : "This is in stark contrast with conventional active learning literature where the cost of acquiring the ground truth is constant [8][9][10].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : "This is in stark contrast with conventional active learning literature where the cost of acquiring the ground truth is constant [8][9][10].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "For any context x ∈ X , the reward of selecting arm k ∈ K is r(x, k) ∈ [0, 1], which is sampled according to some underlying but unknown distribution f(x, k).",
      "startOffset" : 71,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "We assume that the reward value space is [0, 1] for the ease of exposition but this assumption can be relaxed to account for any bounded interval.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "As a widely-adopted assumption in contextual bandits literature [1][3], the reward function is assumed to satisfy a Lipschitz condition with respect to both the context and the arm.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "As a widely-adopted assumption in contextual bandits literature [1][3], the reward function is assumed to satisfy a Lipschitz condition with respect to both the context and the arm.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "If 1 ≤ β1 < 2, the third term on the right hand side of the last inequality in (24) can be bounded by cNi2 5β1/2−1 [ln(2T γ+1 i )] 1(S m) 1−β1/2 1−β1/2 , due to the divergent series ∑T t=1 t −y ≤ T (1−y)/(1 − y) for 0 < y < 1 [11].",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 11,
      "context" : "According to the definition of covering dimensions [12], the maximum number of arm clusters can be bounded by Ni ≤ CAρA K,i in epoch i, and the maximum number of context clusters can be bounded by Mi ≤ CXρX X,i in epoch i, where CA, CX are covering constants for the arm space and the context space.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Since the proposed algorithm incurs the query cost when it requests a ground truth, the lower bound of the regret cannot be lower than that of the conventional contextual MAB setting where no query cost is incurred [1].",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 0,
      "context" : "Theorem 1 and Theorem 2 together show that our algorithm is order-optimal and achieves the same order as conventional contextual bandits algorithms in cost-free scenarios [1][2][3].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 1,
      "context" : "Theorem 1 and Theorem 2 together show that our algorithm is order-optimal and achieves the same order as conventional contextual bandits algorithms in cost-free scenarios [1][2][3].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 2,
      "context" : "Theorem 1 and Theorem 2 together show that our algorithm is order-optimal and achieves the same order as conventional contextual bandits algorithms in cost-free scenarios [1][2][3].",
      "startOffset" : 177,
      "endOffset" : 180
    } ],
    "year" : 2017,
    "abstractText" : "Contextual bandit algorithms – a class of multiarmed bandit algorithms that exploit the contextual information – have been shown to be effective in solving sequential decision making problems under uncertainty. A common assumption adopted in the literature is that the realized (ground truth) reward by taking the selected action is observed by the learner at no cost, which, however, is not realistic in many practical scenarios. When observing the ground truth reward is costly, a key challenge for the learner is how to judiciously acquire the ground truth by assessing the benefits and costs in order to balance learning efficiency and learning cost. From the information theoretic perspective, a perhaps even more interesting question is how much efficiency might be lost due to this cost. In this paper, we design a novel contextual bandit-based learning algorithm and endow it with the active learning capability. The key feature of our algorithm is that in addition to sending a query to an annotator for the ground truth, prior information about the ground truth learned by the learner is sent together, thereby reducing the query cost. We prove that by carefully choosing the algorithm parameters, the learning regret of the proposed algorithm achieves the same order as that of conventional contextual bandit algorithms in cost-free scenarios, implying that, surprisingly, cost due to acquiring the ground truth does not increase the learning regret in the long-run. Our analysis shows that prior information about the ground truth plays a critical role in improving the system performance in scenarios where active learning is necessary.",
    "creator" : "LaTeX with hyperref package"
  }
}