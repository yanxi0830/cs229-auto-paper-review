{
  "name" : "1611.03071.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fair Learning in Markovian Environments",
    "authors" : [ "Shahin Jabbari", "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth" ],
    "emails" : [ "aaroth}@cis.upenn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take exponentially many rounds in the number of states to achieve non-trivial approximation to the optimal policy. Our main result is a polynomial time algorithm that is provably fair under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness."
    }, {
      "heading" : "1 Introduction",
      "text" : "The growing use of machine learning for automated decision-making has raised concerns about the potential for unfairness and discrimination in learning algorithms and models. In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].\nThis paper initiates the study of fair learning algorithms in Markovian environments, where an algorithm’s choices may influence the state of the world and future rewards. Previous work on fairness in machine learning has focused on myopic settings where such influence is absent, as in i.i.d. or no-regret models (see e.g. [6, 7, 12, 14]). Myopic notions of fairness may be undesirable in the contexts outlined above, where policies favoring certain actions will influence future state and rewards. For example, increased policing of a particular neighborhood may lead to more arrests without an actual increase in crime, and in turn may incubate increased tension and frayed relationships with police. As another example, an oft-cited rationale for affirmative action policies is that in the long-term they improve not just the prospects but the qualifications and abilities of individuals in historically disadvantaged groups. Such examples explicitly acknowledge that policies do not operate in a vacuum, but must account for their influence on the evolution of state.\nWe conduct our study in the standard model of reinforcement learning, in which an algorithm seeks to maximize its discounted sum of rewards in a Markovian decision process (MDP). A state s ∈ S encodes the current environment. From state s, a chosen action a ∈ A will lead to an observed transition to a new state s′ with probability p(s, a, s′) and an observed reward r(s′). We now define fairness of an algorithm with respect to a measure Q∗ of the long-run quality of an action a from a state s.\nDefinition 1. [Fairness, Informal] A reinforcement learning algorithm L is fair if for any input δ > 0, the following condition holds with probability at least 1 − δ (over the randomization of the\nar X\niv :1\n61 1.\n03 07\n1v 1\n[ cs\n.L G\n] 9\nN ov\n2 01\n6\nMDP and the algorithm): for all rounds t, all states s ∈ S and all actions a, a′ ∈ A\nQ∗(s, a) ≥ Q∗(s, a′)⇒ L(s, a, ht−1) ≥ L(s, a′, ht−1)\nwhere Q∗(s, a) denotes the (expected) infinite discounted reward when taking action a from state s and following the optimal discounted policy thereafter, and L(s, a, ht−1) is the probability that L chooses action a from state s given the observed history ht−1.\nIn other words, we ask that if from some state s, action a is at least as good action a′ (with respect to the long-term discounted reward achievable following these actions), an algorithm must choose a with at least the probability it chooses a′. This definition of fairness is adapted from Joseph et al. [14], who study fairness in the contextual bandit framework. Their definition and ours differ from previous work by requiring that an algorithm be fair throughout the learning process itself rather than merely outputting a fair policy or model after a period of (possibly unfair) learning. Joseph et al. [14] define fairness with respect to one-step rewards, which is appropriate for the contextual bandit setting where the learner’s actions do not affect future state. In our Markovian setting, we define fairness with respect to the expected long-term discounted reward of an action.\nUnfortunately, our first result shows an exponential separation in expected performance between the best unfair algorithm and any algorithm satisfying Definition 1. This motivates our study of a natural relaxation of (exact) fairness, for which we provide a polynomial time learning algorithm, thus establishing an exponential separation between exact and approximately fair learning in MDPs.\nThroughout, the reader should interpret the actions available to a learning algorithm in an MDP as corresponding to choices affecting individuals — such as the choice of which applicants to admit to college, or which applicants to grant loans to. The rewards associated with each action should be viewed as the short-term payoff of making the corresponding decision (the 4-year graduation rate of the admitted class, or the default rate on the loans given). The actions taken by the algorithm affect the underlying state of the system (the high school graduation rate of students in different populations, or home ownership rate of different populations) — which in turn can affect the actions and payoffs available to the algorithm in the future. Direct use of the fairness definition from Joseph et al. [14] in terms of myopic rewards would be in conflict with the optimal policy in an underlying MDP if it was necessary to make sub-optimal decisions in the short run (e.g. admit students with less high school preparation) in order to obtain high rewards in the long-term (e.g. by improving the overall applicant pool). Instead, we propose using an action’s potential long-term reward as its measured quality, and so (in the college admissions example) would permit “affirmative action” in the short run so long as it actually improves long-term reward."
    }, {
      "heading" : "1.1 Our Contributions",
      "text" : "The contributions of this paper can be divided into three parts. First, in Section 4, we propose several definitions of fairness for learning in MDPs. The first (which formalizes Definition 1) requires that with high probability an algorithm never favors a worse action over a better one, where the quality of an action is measured in terms of its potential long-term discounted reward. We then relax this definition in two ways. The first relaxation, which we call approximate-choice fairness, requires that an algorithm never chooses a worse action with probability substantially higher than better actions. The second relaxation of fairness, approximate-action fairness, requires that an algorithm never favor an action of substantially lower quality than that of a better action.\nSecond, in Section 5, we analyze the consequences of these definitions, providing a lower bound on the time required for a fair learning algorithm to achieve near-optimal performance.\nTheorem (Informal statement of Theorems 4, 5, and 6). All fair and approximate-choice fair learning algorithms must take a number of rounds exponential in n before achieving -optimal performance for constant . All fair and approximate-action fair learning algorithms require a number of rounds exponential in 1/ √ 1− γ, where γ is the discount factor.\nThird, we present an approximate-action fair algorithm (Fair-E3) in Section 6. We prove a polynomial upper bound on the time Fair-E3 requires to achieve near-optimal performance in Section 6.4.\nTheorem (Informal statement of Theorem 7). For any MDP M satisfying standard assumptions, Fair-E3 is an approximate-action fair algorithm achieving -optimal performance for in a number of rounds that is exponential in 1/(1− γ), and polynomial in all other parameters.\nNote that Theorem 6 shows that it is impossible for any approximate-action fair algorithm to have a running time that has a polynomial dependence on 1/(1− γ). More generally, in the same vein as the contextual bandit results in Joseph et al. [14], our results establish rigorous trade-offs between fairness and performance in reinforcement learning algorithms."
    }, {
      "heading" : "1.2 Our Techniques",
      "text" : "We now briefly describe the tools we use to prove the theorems described above. The lower bounds follow from constructing two similar MDPs and arguing that fair and approximate-choice fair algorithms must behave identically for both until receiving some observation distinguishing the two, which will take an exponential number of steps in the size of state space with constant probability.\nWe then study a different relaxation of fairness, called approximate-action fairness, which allows an algorithm to prefer slightly worse actions to slightly better ones. We adapt E3 [19], a wellknown algorithm for MDP learning, to satisfy approximate-action fairness. E3 transitions between exploration and exploitation: if some state has not been sufficiently explored, E3 takes the least explored action in that state. The pigeonhole principle then implies E3 will eventually explore some state often enough to learn accurate estimates of its reward and transition functions, after which E3 treats the state as “known”. To exploit, E3 treats the set of known states as if they are the only states of the MDP and follows a near-optimal policy if such a policy exists in the MDP restricted to the known states. Otherwise, E3 plans to escape to the set of “unknown” states in hopes of achieving better long-term rewards.\nDeveloping a fair version of E3 requires a number of nontrivial modifications to both the algorithm and its analysis. The first main one has to do with the definition of a known state. The original E3 uses a weak definition that is sufficient for one-step lookahead. Since fairness demands that an algorithm never prefer a much worse action, we require a much stronger definition of known state for Fair-E3 that permits accurate estimation of Q∗ values (long-term discounted rewards) rather than just next-state transitions.\nThe second major modification arises from the fact that when attempting to escape to unknown states, E3 may badly violate fairness, since it is ignoring rewards entirely in an attempt to reach rarely visited states. Fair-E3 tries to find an escape policy subject to our fairness constraint. A “win-win” analysis proves that whenever Fair-E3 cannot execute an “exploit” policy, then there must exist a fair escape policy, and so our algorithm is always either able to exploit or fairly escape when it encounters a known state."
    }, {
      "heading" : "2 Related Work",
      "text" : "There is an enormous literature on reinforcement learning, and the most relevant parts focus on constructing learning algorithms with provable performance guarantees. E3 [19] was the first learning algorithm with a polynomial learning rate, and a subsequent literature gave a sequence of improved bounds (see Szita and Szepesvári [28] and references within).\nSeparately, there is a growing literature studying the problem of fairness in machine learning (see e.g. [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]). One line of work aims to give algorithms for batch classification which achieve notions of group fairness, and in particular, statistical parity (see e.g. [1, 5, 7, 8, 18, 22]). While statistical parity is sometimes a desirable goal, as observed by Dwork et al. [6], it suffers from two problems. First, if different populations have different statistical properties, then it can be at odds with accuracy. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level – see Dwork et al. [6] for a catalog of ways in which statistical parity fails as a definition of fairness. In the context of classification and regression, Hardt et al. [12] give a definition aiming to capture the idea of equality of opportunity that overcomes many of these limitations of notions of group fairness. Their definition is a “group fairness” definition in that it imposes a constraint that binds on average outcomes, rather than on individuals. In contrast, we study a definition of fairness that binds at the individual level.\nDwork et al. [6] propose and explore the basic properties of a technical definition of individual fairness formalizing the idea that “similar individuals should be treated similarly”. Specifically, their work presupposes the existence and knowledge of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. The work of Joseph et al. [14] and the present work suggest a natural metric for the tasks of regret minimization in a bandit setting and long-term reward maximization in an MDP. The primary conceptual difference between these works and that of Dwork et al. [6] is that the latter operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. Our fairness constraint, conversely, is entirely aligned with the goal of the algorithm designer in that it is satisfied by the optimal policy; nevertheless, it affects the space of feasible learning algorithms, because it interferes with learning an optimal policy, which depends on the unknown reward functions."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "In this paper we study reinforcement learning in Markov Decision Processes (MDPs):\nDefinition 2. A Markov Decision Process (MDP) is a tuple M = (SM ,AM , PM , RM , T, γ) where SM = {s1, . . . , sn} is a set of n states, AM = {a1, . . . , ak} is a set of k actions, T is a horizon of a (possibly infinite) number of rounds of activity in M , and γ is a discount factor, where\n• PM : SM × AM × SM → [0, 1] is a transition probability distribution mapping (s, a, s′) 7→ p where p is the probability of arriving in state s′ after taking action a from state s.\n• For any state s, RM (s) denote the reward distribution at state s. We assume the support of the reward distribution is [0, 1] for all s. R̄M : SM → [0, 1] denote the mean of reward distribution at state s. We remark that R̄M ≤ 1 and Var(RM ) ≤ 1 for all states s. 1\n1The assumption that the rewards are bounded can be relaxed to the assumption that the support of the reward\nFor the remainder of this paper, any MDP M is defined with the SM ,AM , PM , RM , T, and γ as given above. A policy will dictate choices of actions from each state. A deterministic policy π ∈ Π: SM → AM in M maps from states to actions. A non-deterministic policy π : SM → 4(AM ) maps from states to probability distributions on actions. We make the following unichain assumption for all M . It is important to note that the unichain assumption does not imply that every policy will eventually visit every state, or even that there exists a single policy that will do so quickly; thus, the exploitation/exploration dilemma remains in unichain MDPs.\nAssumption 1. The stationary distribution of any policy in M is independent of its start state.\nA few ancillary concepts will be useful for reasoning about MDPs, most of which are standard (see, e.g. Kearns and Singh [19]). First, we quantify the value of states and actions based on the rewards, both present and future. In the discounted case, the contribution of the future rewards to the value of a state is scaled down by the discount factor γ.\nDefinition 3. The T -step expected reward of π from s, and the limit of this for infinite T , and its optimum over policies are defined as\nV πM (s, T ) = ∑ p P[p] T∑ i=1 γi−1R̄(si) and V π M (s) = lim T→∞ V πM (s, T ) and V ∗ M (s, T ) = max π∈Π V πM (s, T )\nwhere p = (s, s1, . . . , sT ) is a sequence of T states starting from s, P[p] denotes the probability of taking p following policy π and suming over all sequences p. Given action a, the T -step expected reward from s after taking action a and following π thereafter and its optimum as QπM (s, a, T ) = ∑ pa P[pa] T∑ i=1 γi−1R̄(si), Q π M (s, a) = lim T→∞ QπM (s, a, T ) and Q ∗ M (s, a, T ) = max π∈Π QπM (s, a, T )\nfor pa = (s, s1, . . . , sT ) a sequence of T states starting from s, P[pa] the probability of pa after taking action a from s and then following policy π, and summing over all sequences pa.\nAny policy achieving the V ∗M (s) values for all states s is an optimal policy: fix one and denote it as π∗ for the remainder of the paper. Finally, note that V ∗M (s) = V π∗ M (s) and Q ∗ M (s, a) = Q π∗ M (s, a).\nLet µπ denote the stationary distribution of π. We now define the -mixing time of a policy.\nDefinition 4. Fix > 0. The -mixing time of π is the smallest number of steps T π such that for any T ≥ T π and any state s, the distribution µ̂πT of π on states of M after following π for T steps from s satisfies Σni=1|µπ(si)− µ̂πT (si)| ≤ . Let T ∗ denote the -mixing time of π ∗. Lemma 1 relates the -mixing time of any π to the number of rounds until the values V πM of the visited states are close to the expected V π M (under the stationary distribution µπ). We defer the proof of Lemma 1 to Section B.1.\nLemma 1. Fix > 0. For any state s, following π for T ≥ T π steps from s satisfies\nEs∼µπ [V πM (s)]− E\n[ 1\nT T∑ t=1 V πM (st)\n] ≤\n1− γ ,\nwhere st is the state visited at time t when following π from s and the expectation in the second term is over the transition function and the randomization of π.\ndistribution is over a possibly unbounded region but the mean and the variance of the reward distribution is bounded in all states using standard techniques (see Kearns and Singh [19] for more details). Furthermore, assuming that the support of the reward distributions is between [0, 1] can be made without loss of generality up to scaling.\nOur results can be stated for a weaker notion of mixing time called the -reward mixing time, the smallest number of time steps T such that the average undiscounted rewards of the states visited by the policy in T steps is close to the average undiscounted rewards of the states visited under the stationary distribution of the policy. -reward mixing time is always linearly bounded by the -mixing time but can be much smaller than the -mixing time in certain cases (see Kearns and Singh [19] for a discussion).\nThe horizon time Hγ of an MDP captures the number of steps an approximately optimal policy must optimize over. In the discounted rewards setting, the expected discounted reward of any policy after Hγ = log ( (1− γ)) / log(γ) (see Kearns and Singh [19], Lemma 2) steps approaches the expected asymptotic discounted reward: Lemma 2 (Lemma 2, Kearns and Singh [19]). If T ≥ Hγ = log( (1−γ))log(γ) , then for all s ∈ S, V πM (s, T )− ≤ V πM (s) ≤ V πM (s, T ) + . A learning algorithm L is a non-stationary2 policy that at each round takes the entire history (the sequence of triples (state, action, reward) observed so far) and outputs a distribution over actions. Formally, at any fixed time t, define the history at time t to be the sequence of (state, action, reward) triples ht−1 = {(st′ , at′ , rt′)}t−1t′=1 observed before round t. Then L : SM × ht−1 → ∆(AM ). A stationary policy as defined above is an algorithm whose mapping is independent of the observed history, i.e. π(st) = π(st, ht−1).\nWe conclude this section by defining a performance measure for learning algorithms.\nDefinition 5. Let > 0 and δ ∈ (0, 1/2). L achieves -optimality in T steps if for any T ≥ T with probability at least 1− δ,\nEs∼µ∗V ∗M (s)− 1\nT E T∑ t=1 V ∗M (st) ≤ 2 1− γ , (1)\nfor st the state L reaches at time t, where the expectation is taken over the transitions and the randomization of L, for any MDP M .\nWe pause to justify this notion of optimality. First, rewards are discounted, so the contribution of the discounted sum of rewards of any algorithm or policy after Hγ steps is less than : it is therefore unreasonable to expect the discounted sum of rewards of any algorithm to compete favorably with the discounted sum of rewards of an optimal policy (especially if the -mixing time of an optimal policy is much more than Hγ ). So, we instead consider the average V ∗M values of the states visited by our algorithm and compare this value to the average V ∗M values of the states visited under the stationary distribution of an optimal policy. Lemma 3 further justifies this definition; it shows that the average V πM values of states visited under the stationary distribution of π (and in particular an optimal policy) is closely related to the average undiscounted rewards achieved under the stationary distribution of that policy.\nLemma 3 (Singh [26]). Let R̄M be the vector of mean rewards in states of M and V π M the vector of discounted rewards in states under π. Then µπ · R̄M = (1− γ)µπ ·VπM .\nOur goal will be to find some algorithm for which T , the number of transitions after which our policy has average one-step rewards within of an optimal policy as in Definition 5, is small (ideally polynomial in the parameters of M). In Sections 5 and 6, we will prove lower and upper bounds on T based on various notions of fairness and also compare these bounds with standard reinforcement learning bounds for algorithms which do not satisfy these definitions of fairness.\n2A non-stationary policy allows the action choice from a state to depend on the time of arrival at that state and the previously observed rewards: an algorithm choosing actions based upon its observations will be non-stationary."
    }, {
      "heading" : "4 Notions of Fairness",
      "text" : "We now turn to quantitative notions of fairness. We begin by considering Joseph et al. [14]’s definition for learning in a contextual bandit setting: namely, Definition 1 when the Q∗(s, a) values are replaced with the expected payoff of choosing a given arm a. Translated to our setting, Joseph et al. [14] define the quality of an action to be Q∗M (s, a, 1), the expected one-step reward for choosing a from s. This definition would require that, with high probability, an algorithm in state s never play a with higher probability than a′ if a has lower expected one-step reward than a′.\nThis naive translation, however, does not adequately capture the structural differences between the bandit and MDP settings: in the bandit setting, present rewards are independent of past choices, while in MDPs they are not. In particular, defining fairness in terms of one-step rewards would prohibit any policy sacrificing short-term rewards in favor of long-term rewards. This is undesirable, as it is precisely long-term rewards that matter in reinforcement learning, and optimizing for longterm rewards often necessitates short-term sacrifices (see Figure 1 for an illustration).\nIn an MDP, the relevant quality of an action encompasses not only its immediate reward but also the future actions and rewards the action enables. We will therefore define fairness using the long-term discounted rewards, namely Q∗M (s, a).\nDefinition 6 (Fairness). L is fair if for any input δ > 0, for all MDPs M , all rounds t, all states s ∈ SM and actions a, a′ ∈ AM\nQ∗M (s, a) ≥ Q∗M (s, a′)⇒ L(s, a, ht−1) ≥ L(s, a′, ht−1)\nwith probability at least 1− δ over histories ht−1.\nIn Section 5, we show this requirement can prove extremely restrictive (see Theorem 4). Intuitively, L must play uniformly at random until the algorithm has a high degree of confidence about the Q∗M values, in some cases taking exponential time to achieve near-optimality. This motivates relaxing our definition. The first relaxation we introduce requires that an algorithm not substantially favor a worse action over a better one.\nDefinition 7 (Approximate-choice Fairness). L is α-choice fair if for any inputs δ > 0 and α > 0: for all MDPs M , all rounds t, all states s ∈ SM and actions a and a′ ∈ AM\nQ∗M (s, a) ≥ Q∗M (s, a′)⇒ L(s, a, ht−1) ≥ L(s, a′, ht−1)− α,\nwith probability of at least 1− δ over histories ht−1. If L is α-choice fair for any input α > 0, we call L approximate-choice fair.\nA slight modification of the lower bound for exact fairness shows that algorithms satisfying approximate-choice fairness can also require exponential time to achieve near-optimality. Therefore, we propose an alternative relaxation.\nDefinition 8 (Approximate-action Fairness). L is α-action fair if for any input δ > 0, for all MDPs M , all rounds t, all states s ∈ SM and actions a and a′ ∈ AM\nQ∗M (s, a) > Q ∗ M (s, a ′) + α⇒ L(s, a, ht−1) ≥ L(s, a′ht−1)\nwith probability of at least 1− δ over histories ht−1. If L is α-action fair for any input α > 0, we call L approximate-action fair.\nNote that α-action fair algorithms can make arbitrary choices between two actions which are αclose in quality, but cannot place higher probability on a than a′ if a′’s long-term expected reward is at least α greater than a’s. As a result, approximate-choice fairness prevents equally good actions from being chosen at very different rates, while approximate-action fairness only prevents very poor actions from being chosen over very good ones. Amongst actions of nearly identical quality, the algorithm is permitted to break ties arbitrarily without violating approximate-action fairness. For example, this constraint is consistent with a college making admissions decisions based on diversity considerations when deciding between two applicants whose relevant qualities appear to be identical. Hence, approximate-action fairness should be viewed as a weaker constraint: rather than safeguarding against every violation of “fairness”, it instead restricts how egregious these violations can be. Approximate-action fairness is a particularly attractive relaxation because it allows us to give algorithms circumventing the exponential hardness we prove for fairness and approximate-choice fairness. Thus, after proving lower bounds on the performance of all of our definitions of fairness in Section 5, approximate-action fairness will form our primary focus.\nWe conclude this section by making several useful observations. We defer all formal statements and their proofs to Appendix A. There always exists an optimal policy which is fair, but it might require randomization (Observation 1); conversely, any optimal policy (deterministic or randomized) is approximate-action fair (Observation 2), as is the uniformly random policy (Observation 3).\nFinally, we consider a restriction of the actions in an MDP M to nearly-optimal actions (as measured by Q∗M values). Optimal policies need only select amongst these actions, and fair policies can only select amongst these actions.\nDefinition 9. The α-restricted MDP of M , denoted Mα, is identical to M except that in each state s, the set of available actions are restricted to {a : Q∗M (s, a) ≥ maxa′∈AM Q∗M (s, a′)− α | a ∈ AM}.\nMα has the following two properties: (i) any policy in Mα is α-action fair in M (Observation 4) and (ii) the optimal policy in Mα is also optimal in M (Observation 5). Observations 4 and 5 aid our design of an approximate-action fair algorithm: we construct Mα from estimates of the Q∗M values (see Section 6.3 for more details)."
    }, {
      "heading" : "5 Lower Bounds",
      "text" : "In this section, we demonstrate a stark separation between the performance of algorithms for reinforcement learning with and without fairness. First, we show that neither fair nor approximate-choice fair algorithms achieve near-optimality (see Definition 5) unless the number of time steps T is at least Ω(kn), exponential in the size of the state space. We then show that any approximate-action fair algorithm requires a number of time steps T that is at least Ω(k1/ √ 1−γ) to achieve nearoptimality. We start by proving a lower bound for fair algorithms.\nTheorem 4. For δ < 1/4, γ > 0.5, and < 1/8, no fair algorithm can be -optimal in T = O(kn) steps. 3\n3We have not optimized for the upper bound constants on the parameters in the statement of Theorem 4 as well as Theorems 5 and 6. Hence, these values are only chosen for convenience.\nStandard algorithms like E3 (absent a fairness constraint) learn an -optimal policy in a number of steps polynomial in n and 1/ ; Theorem 4 therefore shows a steep cost of imposing fairness on reinforcement learning algorithms. We sketch the high level idea of the proof of Theorem 4 and defer all the proofs to Section B.2. For intuition, first consider the special case when the number of actions k = 2. We introduce the MDPs witnessing the claim in Theorem 4 for this special case.\nDefinition 10. Let M(x) = (SM ,AM , PM , RM , T, γ, x) be an MDP where AM = {L,R}, • for all i ∈ [n], PM (si, L, s1) = PM (si, R, sj) = 1 where j = min{i+ 1, n} and is 0 otherwise. • for i ∈ [n− 1], RM (si) = 0.5, and RM (sn) = x.\nFigure 2 illustrates the MDP from Definition 10. Note that all the transitions and rewards in M are deterministic. However, the reward at state sn can be either 1 or 1/2. Any algorithm (fair or otherwise) cannot determine whether the Q∗M values of all the states are the same or not until it reaches sn (the final state in the chain). But until then, fairness requires that the algorithm play all the actions uniformly at random. Thus, any fair algorithm will need to take exponential time in the number of states to reach sn. While we specialized this exposition to the case of k = 2, one can easily modify it such that, from each state si, k − 1 of the actions from state si (deterministically) reach state s1 and only one action (deterministically) reaches states smin{i+1,n}. This gives a lower bound of kn time steps before any fair algorithm can stop playing uniformly at random (which is necessary for near-optimality). The same example, with a slightly modified analysis, also provides a lower bound of Ω((k/(1 + kα)n) time steps for approximate-choice fair algorithms as stated in Theorem 5.\nTheorem 5. For δ < 1/4, α < 1/4, γ > 0.5, and < 1/8, no α-choice fair algorithm can be -optimal for T = O ( ( k1+kα) n ) steps.\nTheorem 5 suggests that fairness and approximate-choice fairness are both extremely costly constraints on reinforcement learning algorithms, ruling out polynomial time solutions. Hence, we focus on approximate-action fair learning. First, we note that the time complexity of approximate-action fair algorithms will still suffer from a super-polynomial dependence on 1/(1− γ). Theorem 6. For δ < 1/4, α < 1/4, γ > 0.9 and < 1/32, no α-action fair algorithm can be\n-optimal for T = O ( k1/ √ 1−γ ) steps.\nThe MDPs used in proving Theorem 4 can also witness the claim of Theorem 6, by choosing n = blog(1/2α)/(1−γ)c. We think of the discount factor γ as a constant, and so in most interesting cases, 1/ √ 1− γ n, meaning that this lower bound is substantially less stringent compared to the lower bounds proven for fairness and approximate-choice fairness. Therefore, in the remainder of the paper, we focus on approximate-action fairness. We aim to obtain algorithms that have learning rates that are polynomial in every parameter except for 1/(1− γ), since we show that the learning rate of any approximate-action algorithm has a super-polynomial dependence on 1/(1−γ)."
    }, {
      "heading" : "6 A Provably Fair and Efficient Learning Algorithm",
      "text" : "We now present an approximate-action fair learning algorithm, Fair-E3, which achieves the performance guarantees stated below.\nTheorem 7. Given > 0, α > 0, δ ∈ (0, 1/2) and γ ∈ [0, 1) as inputs, Fair-E3 is an α-action fair algorithm which achieves -optimality after\nT = O (\nn5T ∗\nmin{α4, 4} 2 (1− γ)12 k\n1 1−γ+5 log (n δ ) log ( k δ ) log ( 1 δ ) log9 ( 1 (1− γ) )) (2)\nsteps.\nThe running time of Fair-E3 is polynomial in all MDP parameters except 1/(1−γ). Moreover, Theorem 6 implies that no approximate-action fair algorithm can have a running time that is polynomial in 1/(1 − γ). The remainder of this section is structured as follows. A high-level description of Fair-E3 can be found in Section 6.1; the analysis of its behavior begins in Sections 6.2 and 6.3; the final proof of Theorem 7 follows in Section 6.4. We discuss extensions and relaxations in Section 6.5.\n6.1 Informal Description of Fair-E3\nLike E3, Fair-E3 relies heavily on a notion of “known” states. For both algorithms, a state s is defined to be known only after all actions have been played from s a sufficient number of times. For E3, “sufficient” is defined as a quantity such that with high probability, it is possible to estimate the reward distribution and transition probabilities for each action. For Fair-E3, on the other hand, “sufficient” is defined such that with high probability, for every action a at state s, it is possible to estimate Q∗M (s, a), which is a more exacting condition (see Section 6.2). Similarly, when in an unknown state s, E3 engages in a process of “balanced exploration” by choosing the least-explored action from s, in an attempt to hasten state s becoming known; Fair-E3, constrained by fairness, cannot do this. Instead, it chooses a trajectory of actions uniformly at random from s. Fair-E3 uses these random trajectories to estimate the Q∗M values once the state becomes known. From a known state, E3 performs an offline computation to determine the policy to follow in a prespecified number of future time steps. Fair-E3 also engages in an offline planning computation in known states, but modified to ensure that it does not violate the approximate-action fairness (see Section 6.3). In particular, because of fairness, it is necessary to prove that whenever FairE3 cannot find an exploitation policy amongst the set of known states, there must exist a fair exploration policy which quickly escapes to unknown states.\n6.2 Known States in Fair-E3\nWe now give a formal definition of known states for Fair-E3. If a state s is known by Fair-E3, then we will have (i) highly accurate estimates of all transition probabilities from s, and (ii) highly accurate estimates of Q∗M (s, a) for all a.\nDefinition 11 (Q-known State). Let\nm1 = O\n( kH γ +3n ( 1\n(1− γ)α\n)2 log ( k\nδ\n)) and m2 = O (( n\nmin{ , α}\n)4 Hγ 8 log ( 1\nδ\n)) .\nA state s becomes Q-known after taking\nmQ := k ·max{m1,m2} (3)\nlength-Hγ random trajectories from s.\nWe will show that both conditions (i) and (ii) hold for a Q-known state; informally, m1 many random trajectories suffice to ensure that we have accurate estimates of all Q∗M (s, a) values, and m2 random trajectories suffice to ensure accurate estimates of the transition probabilities and rewards.\nTo make the first condition formal, we rely on Theorem 8 adapted from Kearns et al. [20], connecting the number of random walks taken from s to the accuracy of the empirical V ∗M estimates.\nTheorem 8 (Theorem 5.5, Kearns et al. [20]). For any state s and α > 0, after\nm = O ( kH γ +3 ( 1\n(1− γ)α\n)2 log ( |Π| δ ))\nrandom walks of length Hγ from s, with probability of at least 1− δ, we can compute estimates V̂ πM such that |V πM (s)− V̂ πM (s) | ≤ α, simultaneously for all π ∈ Π.\nTheorem 8 enables us to translate between the number of trajectories taken from a state and the uncertainty about its V πM values for all policies (including π\n∗ and hence V ∗M ). Note that since |Π| = kn, we re-write log (|Π|) = n log (k). To estimate Q∗M (s, a) values using the V ∗M (s) values we increase the number of necessary length-Hγ random trajectories by a factor of k.\nFor the second condition, we adapt the analysis of E3 in which Kearns and Singh [19] show that if each action in a state s is taken m2 times, then the transition probabilities and reward in state s can be estimated accurately (see Section 6.4).\n6.3 Planning in Fair-E3\nWe now formalize the planning steps in Fair-E3 from Q-known states. For the remainder of this section and also throughout our analysis in Section 6.4, we make Assumption 2 which we remove in Section 6.5 entirely.\nAssumption 2. T ∗ is known.\nWe describe the offline planning step of Fair-E3 and why it is guaranteed to satisfy approximate-action fairness. Consider the following MDPs with deterministic rewards constructed from M .\n• MΓ (the exploitation MDP) is the MDP in which the unknown states of M are condensed into a single absorbing state s0 with no reward. In the known states Γ, the transitions are kept intact and the rewards are deterministically set to their mean value.\n• M[n]\\Γ (the exploration MDP) is identical to MΓ except for the rewards. The rewards in the known states Γ are set to 0 and the reward in s0 is set to 1.\nSee the middle (right) panel of Figure 3 for an illustration of MΓ (M[n]\\Γ), and Appendix C for formal definitions. M[n]\\Γ effectively inverts MΓ: M[n]\\Γ apportions all rewards to the unknown states of M , while MΓ apportions all rewards to the known states of M .\nFair-E3 plans according to the following natural idea: when in a Q-known state, Fair-E3 constructs M̂Γ and M̂[n]\\Γ, based on the estimated transition and rewards observed so far. Then it computes the restricted MDPs M̂αΓ and M̂ α [n]\\Γ from M̂Γ and M̂[n]\\Γ. Next, it finds the optimal policies in M̂αΓ and M̂ α [n]\\Γ. If the optimal policy in M̂ α [n]\\Γ escapes to the absorbing state of MΓ with high enough probability (to be specified shortly) in 2T ∗ steps, then Fair-E 3 explores by following that policy. Otherwise, Fair-E3 exploits by following the optimal policy in M̂αΓ for T ∗ steps. Note that while following any of these policies, if Fair-E3 encounters an unknown state, it stops following the policy and proceeds by taking a length-Hγ random trajectory.\n6.4 Analysis of Fair-E3\nIn this section we formally analyze Fair-E3 and prove Theorem 7. All the omitted proofs from this section can be found in Appendix B.3. Intuitively, MΓ distills the useful knowledge of FairE3 about M : from known states, Fair-E3 has enough information to play (nearly) optimally (and therefore fairly); from unknown states, Fair-E3 does not. We begin by showing that there either exists an exploitation policy achieving high reward in MαΓ or there exists an exploration policy in MαΓ which quickly reaches the unknown states of M .\nLemma 9 (Exploit or Explore Lemma). For any state s ∈ Γ, β ∈ (0, 1) and any T > 0 at least one of the statements below holds:\n• there exists an exploitation policy π in MαΓ such that\n1 T max π̄∈Π E T∑ t=1 V π̄M ( π̄t(s), T ) − 1 T E T∑ t=1 V πMΓ ( πt(s), T ) ≤ β\nwhere the random variables πt(s) and π̄t(s) denote the states reached from s after following π and π̄ for t steps, respectively.\n• there exists an exploration policy π in MαΓ such that the probability that a walk of 2T -steps from s following π will terminate in s0 exceeds β/T .\nNote that the optimal policy in M is approximate-action fair by Observation 2. If the optimal policy only stays in the known states of M (represented by MΓ) then following the optimal policy in MΓ (and more specifically in M α Γ ) results in both optimal and approximate-action fair play. However, the optimal policy in M might escape to the unknown states of M quickly, and in this case, it may be that no policy in MαΓ can compete with the optimal policy. Ignoring fairness, one natural way of computing an “escape” policy is to compute the optimal policy in M[n]\\Γ (as is done by E3). However, this approach will violate approximate-action fairness. Instead, we compute an escape policy in Mα[n]\\Γ and show that if no near-optimal exploitation policy exists, then the optimal policy in Mα[n]\\Γ (which is fair by construction) quickly escapes to the unknown states of M .\nFinally, Fair-E3 only has access to the empirically estimated MDPs M̂αΓ and M̂ α [n]\\Γ. We show that the behavior of any policy in M̂αΓ (and M̂ α [n]\\Γ) is similar to the behavior of the same policy\nin MαΓ (and M α [n]\\Γ). To do so, we prove the stronger claim that the behavior of any policy in M̂Γ (and M̂[n]\\Γ) is similar to the behavior of the same policy in MΓ (and M[n]\\Γ).\nLemma 10. Let Γ be the set of known states and M̂Γ the approximation to MΓ. Then for any state s ∈ Γ, any action a and any policy π\n1. V πMΓ(s)−min{α/2, } ≤ V π M̂Γ (s) ≤ V πMΓ(s) + min{α/2, },\n2. QπMΓ (s, a)−min{α/2, } ≤ Q π M̂Γ (s, a) ≤ QπMΓ (s, a) + min{α/2, },\nwith probability at least 1− δ.\nNote that an analog of Lemma 10 can be also stated for M̂[n]\\Γ and M[n]\\Γ. Moreover, as mentioned earlier, when Fair-E3 computes the optimal policy in MαΓ and M α [n]\\Γ, it first checks to see that whether the optimal policy in Mα[n]\\Γ quickly reaches the absorbing state of MΓ with significant probability (as specified in Lemma 9). This can be easily checked by simulating the execution of the optimal policy of Mα[n]\\Γ for 2T ∗ steps from the known state s in M α Γ several times, counting the ratio of runs which end up in s0 and applying Chernoff bound. Note that this is the only place that Assumption 2 is used by Fair-E3.\nWhen there is no exploration policy then Lemma 10 implies that an exploitation policy in MαΓ exists. By setting T ≥ T ∗ in Lemma 9 and applying Lemmas 1 and 10 we can prove Corollary 11 regarding this exploitation policy.\nCorollary 11. For any state s ∈ Γ and T ≥ T ∗ if there exists an exploitation policy π in MαΓ then∣∣∣∣∣ 1T E T∑ t=1 V πM ( πt(s), T ) − Es∼µ∗V ∗M (s) ∣∣∣∣∣ ≤ 1− γ . We finally have all the background needed to prove Theorem 7.\nProof of Theorem 7. We divide the analysis into separate parts: the performance guarantee of Fair-E3 and its approximate-action fairness. We defer the analysis of the probability of failure of Fair-E3 to Appendix B.3.\nWe start with the performance guarantee. We first show that when Fair-E3 follows the exploitation policy the average V ∗M values of the visited states is very close to Es∼µ∗V ∗M (s). However, when following the exploration policy or when taking random trajectories the V ∗M values of the visited states might be potentially low. We then show that the number of these exploratory steps is bounded by the parameters of the MDP, and so they have only a small effect on overall performance\nObserve that in each T ∗ -step exploitation phase of Fair-E 3, the expectation of the average V ∗M values of the visited states is at least Es∼µ∗V ∗M (s)− /(1−γ)− /2 by Lemmas 1, 9 and Observation 5. By a Chernoff bound, the probability that the actual average V ∗M values of the visited states is less than Es∼µ∗V ∗M (s)− /(1− γ)− 3 /4 is smaller than δ/4 if the number of exploitation phases is at least (1/ 2 log(1/δ)).\nThe total number of exploratory steps of Fair-E3 is bounded by\nT1 = O ( nmQH γ + nmQ T ∗ log (n δ )) ,\nwhere mQ is defined in Equation 3 of Definition 11. The first term in T1 bounds the total number of steps needed in all the possible Hγ -length random trajectories taken by Fair-E3 before all the\nstates become Q-known in the worst case. There are n states and in each state mQ trajectories are sufficient for the state to become Q-known. The second term in T1 bounds the total number of steps needed in all the possible 2T ∗ -steps of following the exploration policy before all the states become Q-known in the worst case. Each exploration policy will end up in an unknown state with probability of at least /T ∗ according to Lemma 9. So after O(T ∗ log(\nn δ )/ ) times of following the\nexploration policy Fair-E3 reaches an unknown state. Finally, each of the unknown states might need to be visited mQ times before becoming Q-known where mQ is again defined in Equation 3.\nFinally, to make up for the potentially poor performance in exploration, the number of T ∗ -step exploitation phases needed is at least\nT2 = O\n( T1(1− γ) ) .\nTherefore, after T = T1 + T2 steps we have\nEs∼µ∗V ∗M (s)− 1 T E T∑ t=1 V ∗M (st) ≤ 2 1− γ ,\nas claimed in Equation 2. The running time of Fair-E3 is O(nT 3/ ): the additional nT 2/ factor comes from offline computation of the optimal policies in M̂αΓ and M̂ α [n]\\Γ.\nWe now prove Fair-E3 satisfies approximate-action fairness. Observe that when taking Hγ - step random trajectories, Fair-E3 never violates the fairness (and hence the approximate-action fairness) constraint because randomly taking actions is α-action fair for any α ≥ 0 by Observation 3.\nWith probability at least 1− δ, all the estimates of the Q∗M values in the known states Γ have error of at most α/2. Hence, Observation 4 implies that taking any policy in either of the restricted MDPs MαΓ or M α [n]\\Γ is α/2-action fair in M . However, Fair-E 3 computes a policy in the empirically estimated MDPs M̂αΓ and M̂ α [n]\\Γ instead of MΓ and M[n]\\Γ. Lemma 10 then shows that the Q ∗ M values of every state/action pair in M̂αΓ and M̂ α [n]\\Γ are within α/2 of the corresponding Q ∗ M values in MαΓ and M α [n]\\Γ. Hence, Fair-E 3 is α-action fair."
    }, {
      "heading" : "6.5 Relaxations and Discussion",
      "text" : "Throughout Sections 6.3 and 6.4 we assumed that T ∗ , the -mixing time of the optimal policy π ∗, was known (see Assumption 2). Although Fair-E3 uses the knowledge of T ∗ to decide whether to follow the exploration or exploitation policy, Lemma 9 continues to hold even without this assumption. Note that Fair-E3 is parameterized by T ∗ and for any input T ∗ runs in time poly(T ∗ ). Thus if T ∗ is unknown, we simply run Fair-E 3 for T ∗ = 1, 2, . . . sequentially and the running time and sample complexity will still be poly(T ∗ ). Similar to the analysis of Fair-E 3 when T ∗ is known we have to run the new algorithm for sufficiently many steps so that the possibly low V ∗M values of the visited states in the early stages are dominated by the near-optimal V ∗M values of the visited states for large enough guessed values of T ∗ .\nFinally, our work leaves open several interesting questions. For example, we give an algorithm that has an undesirable exponential dependence on 1/(1 − γ), but we also show that no approximate-action fair algorithm can run in time polynomial in 1/(1 − γ). Without fairness, near-optimality in learning can be achieved in time that is polynomial in all of the parameters of the underlying MDP. So, we can ask: does there exist a meaningful fairness notion that enables reinforcement learning in time polynomial in all parameters?"
    }, {
      "heading" : "A Observations on Optimality and Fairness",
      "text" : "Observation 1. For any MDP M , there exists an optimal policy π∗ such that π∗ is fair.\nProof. In time t, let state st denote the state from which π chooses an action. Let a ∗ = argmaxaQ ∗ M (st, a) and A∗(st) = {a ∈ A | Q∗M (st, a) = Q∗M (st, a∗)}. The policy of playing an action uniformly at random from A∗(st) in state st for all t, is fair and optimal.\nApproximate-action fairness, conversely, can be satisfied by any optimal policy, even a deterministic one.\nObservation 2. Let π∗ be an optimal policy in MDP M . Then π∗ is approximate-action fair.\nProof. Assume that π∗ is not approximate-action fair. Given state s, the action that π∗ takes from s is uniquely determined since π∗ is deterministic we may denote it by a∗. Then there exists a time step in which π∗ is in state s and chooses action a∗(s) such that there exists another action a with\nQ∗M (s, a) > Q ∗ M (s, a ∗(s)) + α,\na contradiction of the optimality of π∗.\nObservations 1 and 2 state that policies with optimal performance are fair; we now state that playing an action uniformly at random is also fair.\nObservation 3. An algorithm that, in every state, plays each action uniformly at random (regardless of the history) is fair.\nProof. Let L denote an algorithm that in every state plays uniformly at random between all available actions. Then L(s, ht−1)a = L(s, ht−1)a′ regardless of state s, (available) action a, or history ht−1. Q ∗ M (s, a) > Q ∗ M (s, a\n′) + α ⇒ L(s, ht−1)a ≥ L(s, ht−1)a′ then follows immediately, which guarantees both fairness and approximate-action fairness.\nObservation 4. Let M be an MDP and Mα the α-restricted MDP of M . Let π be a policy in Mα. Then π is α-action fair.\nProof. Assume π is not α-action fair. Then there must exist round t, state s, and action a such that Q∗M (s, a) > Q ∗ M (s, a\n′) + α and L(s, ht−1)a < L(s, ht−1)a′ . Therefore L(s, ht−1)a′ > 0, so Mα must include action a′ from state s. But this is a contradiction, as in state s Mα only includes actions a′ such that Q∗M (s, a ′) + α ≥ Q∗M (s, a). π is therefore α-action fair.\nObservation 5. Let M be an MDP and Mα the α-restricted MDP of M . Let π∗ be an optimal policy in Mα. Then π∗ is also optimal in M .\nProof. If π∗ is not optimal in M , then there exists a state s and action a such that Q∗M (s, a) > Ea∗(s)∼π∗(s)Q∗M (s, a∗(s)) where a∗(s) is drawn from π∗(s) and the expectation is taken over choices of a∗(s). This is a contradiction because action a is available from state s in Mα by Definition 9."
    }, {
      "heading" : "B Omitted Proofs",
      "text" : "B.1 Omitted Proofs for Section 3\nProof of Lemma 1. Let µ̂πT denote the distribution of π on states of M after following π for T steps starting from s. Then we know,\nEs∼µπV πM (s)− 1\nT E T∑ t=1 V πM (st) = n∑ i=1 (µπ(si)− µ̂πT (si))V πM (si) ≤ n∑ i=1 |µπ(si)− µ̂πT (si)|V πM (si) ≤ 1− γ .\nThe last inequality is due to the following observations: (i) V πM (si) ≤ 1/(1 − γ) as rewards are in [0, 1] and (ii) Σni=1 |µπ(si)− µ̂πT (si)| ≤ since T is at least the -mixing time of π.\nB.2 Omitted Proofs for Section 5\nWe first state the following useful Lemma about M .\nLemma 12. Let M be the MDP in Definition 10. Then for any i ∈ {1, . . . , n}, V ∗M (si) < 1+2γn−i+1 2(1−γ) .\nProof.\nV ∗M (si) = discounted reward before reaching state n+ discounted reward from staying at state n\n< [ n−i−1∑ t=1 γt 2 ] + γn−i+1 1− γ\n=\n[ 1\n2\n( 1\n1− γ − γ\nn−i\n1− γ\n)] + γn−i+1\n1− γ\n= 1− γn−i 2(1− γ) + γn−i+1 1− γ = 1 + γn−i(2γ − 1)\n1− γ\n< 1 + 2γn−i+1\n2(1− γ) ,\nvia two applications of the summation formula for geometric series.\nProof of Theorem 4. We prove Theorem 4 for the special case of k = 2 first. Consider coupling the run of a fair algorithm L on both M(0.5) and M(1). To achieve this, we can fix the randomness of L up front, and use the same randomness on both MDPs. The set of observations and hence the actions taken on both MDPs are identical, until L reaches state sn. Until then, with probability at least 1 − δ, L must play L and R with equal probability in order to satisfy fairness (since, for M(0.5), the only fair policy is to play both actions with equal probability at each time step).\nLet fγ = d 11− 3√γ e and T = 2 n−2fγ for n ≥ 100(fγ)2. First observe that the probability of reaching a fixed state si for any i ≥ n− fγ from a random walk of length T is upper bounded by the probability that the random walk takes i ≥ n− fγ consecutive steps to the right in the first T steps. This probability is at most p = 2n−2fγ (12)\nn−fγ = 2−fγ for any fixed i. Then the probability that the T -step random walk arrives in any state si for i ≥ n− fγ is also upper bounded by p.\nNext, we observe that the V ∗M (si) is a nondecreasing function of i, for both MDPs. Then the average V ∗M values of the visited states of any fair policy can be broken into two pieces: the average\nconditioned on the 1 − δ fairness and never reaching a state beyond sn−fγ , and the average when fairness might be violated (i.e. choices are not uniform random) or the uniform random walk of length T reaches a state beyond sn−fγ . So, we have that\n1 T E T∑ t=1 V ∗M (st) ≤ (1− p− δ)V ∗M (sn−fγ ) + (p+ δ) 1 1− γ\n≤ (1− p− δ) 1 + 2γ fγ+1\n2(1− γ) + (p+ δ)\n1\n1− γ .\nThe first inequality follows from the fact that V ∗M (si) ≤ 1/(1− γ) for all i, and the second from Lemma 12 along with V ∗M values being nondecreasing in i. Putting it all together,\nEs∼µ∗V ∗M (s)− 1 T E T∑ t=1 V ∗M (st) ≥ 1 1− γ − [ (1− p− δ) 1 + 2γ fγ+1 2(1− γ) + (p+ δ) 1 1− γ ] =\n1− p− δ 1− γ\n[ 1− 1 + 2γ fγ+1\n2\n] .\nSo -optimality requires 2\n1− γ ≥ 1− p− δ 1− γ\n[ 1− 1 + 2γ fγ+1\n2\n] . (4)\nHowever, if < 1/8 we get\n2\n1− γ < 1− 0.04− 1/4 1− γ\n[ 1− 1 + 2× e −3\n2 ] <\n1− 2−fγ − δ 1− γ\n[ 1− 1 + 2γ fγ+1\n2\n] ,\nwhere the third inequality follows when δ < 1/4 and γ > 0.5. This means < 1/8 makes - optimality impossible, as desired.\nThroughout we considered the special case of k = 2 and proved a lower bound of Ω(2n) time steps for any fair algorithm satisfying the -optimality condition. However, it is easy to see that MDP M in Definition 10 can be easily modified in a way that k − 1 of the actions from state si reach state s1 and only one action in each state si reaches states smin{i+1,n}. Hence, a lower bound of Ω(kn) time steps can be similarly proved.\nProof of Theorem 5. We mimic the argument used to prove Theorem 4 with the difference that, until visiting sn, L may not play R with probability more than 1/2 + α (as opposed to 1/2 in Theorem 6). Let fγ = d 11− 3√γ e and T = ( 2 1+2α)\nn−2fγ for n ≥ 100(fγ)2. By a similar process as in Theorem 6, the probability of reaching state si for any i ≥ n − fγ from a random walk of length T is bounded by p = ( 21+2α)\nfγ , and so the probability that the T -step random walk arrives in any state si for i ≥ n − fγ is bounded by p. Carrying out the same process used to prove Theorem 6 then once more implies that -optimality requires Equation 5 to hold when δ < 1/4, α < 1/4 and γ > 0.5. Hence, < 1/8 violates this condition as desired.\nFinally, throughout we considered the special case of k = 2. The same trick as in the proof of Theorem 4 can be used to prove the lower bound of Ω(( k1+kα)\nn) time steps for any fair algorithm satisfying the -optimality condition.\nProof of Theorem 6. We also prove Theorem 6 for the special case of k = 2 first. Again consider the MDP in Definition 10. We set the size of the state space in M to be n = blog(1/(2α))/(1−γ)c. Then given the parameter ranges, for any i, Q∗M (si, R) − Q∗M (si, L) > α in M(1). Therefore, any approximate-action fair algorithm should play actions R and L with equal probability.\nLet T = 2n/2 = Ω(21/ √\n1−γ). First observe that the probability of reaching a fixed state si for any i ≥ 3n/4 from a random walk of length T is upper bounded by the probability that the random walk takes i ≥ 3n/4 consecutive steps to the right in the first T steps. This probability is at most p = 2n/2(12)\n3n/4 = 2−n/4 for any fixed i. Then the probability that the T -step random walk arrives in any state si for i ≥ 3n/4 is also upper bounded by p.\nNext, we observe that the V ∗M (si) is a nondecreasing function of i, for both MDPs. Then the average V ∗M values of the visited states of any fair policy can be broken into two pieces: the average conditioned on the 1 − δ fairness and never reaching a state beyond s3n/4, and the average when fairness might be violated or the uniform random walk of length T reaches a state beyond s3n/4. So, we have that\n1 T E T∑ t=1 V ∗M (st) ≤ (1− p− δ)V ∗M (s3n/4) + (p+ δ) 1 1− γ\n≤ (1− p− δ) 1 + (2γ − 1)γ n/4\n2(1− γ) + (p+ δ)\n1\n1− γ .\nThe first inequality follows from the fact that V ∗M (si) ≤ 1/(1− γ) for all i, and the second from Lemma 12 along with V ∗M values being nondecreasing in i. Putting it all together,\nEs∼µ∗V ∗M (s)− 1 T E T∑ t=1 V ∗M (st) ≥ 1 1− γ −\n[ (1− p− δ) 1 + (2γ − 1)γ n/4\n2(1− γ) + (p+ δ)\n1\n1− γ\n]\n= 1− p− δ\n1− γ\n[ 1− 1 + (2γ − 1)γ n/4\n2\n]\n= 1− p− δ\n1− γ\n[ 1\n2 − (2γ − 1)γ n/4 2\n] .\nSo -optimality requires\n2 1− γ ≥ 1− p− δ 1− γ\n[ 1\n2 − (2γ − 1)γ n/4 2\n] . (5)\nHowever, if < 1/32 we get\n2\n1− γ < 1− 0.22− 1/4 1− γ\n[ 1\n2 − 0.4 · 0.99/4 ] <\n1− 2−n/4 − δ 1− γ\n[ 1\n2 − (2γ − 1)γ n/4 2\n] ,\nwhere the second inequality follows from δ < 1/4, α < 1/4, γ > 0.9, and n ≥ log(1/2α)1−γ − 1 ≥ 9. This means < 1/32 makes -optimality impossible, as desired.\nFinally, the same trick as in the proof of Theorem 4 can be used to prove the Ω(k1/ √\n1−γ) lower bound for k > 2 actions.\nB.3 Omitted Proofs for Section 6\nProof of Lemma 9. We first show that either\n• there exists an exploitation policy π in MΓ such that\n1 T max π̄∈Π E T∑ t=1 V π̄M ( π̄t(s), T ) − 1 T E T∑ t=1 V πMΓ ( πt(s), T ) ≤ β\nwhere the random variable πt(s) and π̄t(s) denote the states reached from s after following π and π̄ for t steps, respectively.\n• there exists an exploration policy π in MΓ such that the probability of that a walk of 2T -steps from s following π will terminate in s0 exceeds β/T .\nLet π be a policy in M satisfying\n1 T E T∑ t=1 V πM (π t(s), T ) = 1 T max π̄∈Π E T∑ t=1 V π ′ M (π̄ t(s), T ) := Ṽ .\nFor any state s′ let p(s′) denote all the T -paths in M that start in s′, q(s′) denote all the T -paths in M that start in s′ such that all the states in every T -path in q(s′) are in Γ and r(s′) all the T -paths in M that start in s′ such that at least one state in every T -paths in r(s′) is not in Γ. Suppose\n1 T E T∑ t=1 V πMΓ(π t(s)) < Ṽ − β.\nOtherwise, π already witnesses the claim. We show that a walk of 2T -steps from s following π will terminate in s0 with probability of at least β/T .\nFirst,\nE T∑ t=1 V πM (π t(s), T ) = E T∑ t=1 ∑ p(πt(s)) P[p(πt(s))]VM (p(πt(s)))\n= E T∑ t=1 ∑ q(πt(s)) P[q(πt(s))]VM (q(πt(s))) + E T∑ t=1 ∑ r(πt(s)) P[r(πt(s))]VM (r(πt(s)))\nsince p(πt(s)) = q(πt(s)) ∪ r(πt(s)), which is a disjoint union. Next, E T∑ t=1 ∑ q(πt(s)) P[q(πt(s))]VM (q(πt(s))) = E T∑ t=1 ∑ q(πt(s)) PπMΓ [q(π t(s))]VMΓ(q(π t(s))) ≤ E T∑ t=1 V πMΓ(π t(s), T ),\nwhere the equality is due to Definition 13 and the definition of q, and the inequality follows because V πMΓ(π\nt(s), T ) is the sum over all the T -paths in MΓ, not just those that avoid the absorbing state s0. Therefore by our original assumption on π,\nE T∑ t=1 ∑ q(πt(s)) P[q(πt(s))]VM (q(πt(s))) ≤ E T∑ t=1 V πMΓ(π t(s), T ) < TṼ − Tβ.\nThis implies\nE T∑ t=1 ∑ r(πt(s)) P[r(πt(s))]VM (r(πt(s))) = E T∑ t=1 V πM (π t(s), T )− E T∑ t=1 ∑ q(πt(s)) P[q(πt(s))]VM (q(πt(s)))\n= T Ṽ − E T∑ t=1 ∑ q(πt(s)) P[q(πt(s))]VM (q(πt(s))) ≥ Tβ,\nwhere the last step is the result of applying the previous inequality. However,\nE T∑ t=1 ∑ r(πt(s)) P[r(πt(s))]VM (r(πt(s))) ≤ TE T∑ t=1 ∑ r(πt(s)) P[r(πt(s))],\nbecause trivially VM (r(π t(s))) ≤ T for all πt(s). So Tβ ≤ TE ∑T t=1 ∑ r(πt(s)) P[r(πt(s))]. Finally, if we let Pπ2T denote the probability that a walk of 2T steps following π terminates in s0, i.e. the probability that π escapes to an unknown state within 2T steps then for each t ∈ [T ], E ∑ r(πt(s)) ≤ TPπ2T . It follows that Tβ ≤ T 2Pπ2T\nand rearranging yields Pπ2T ≥ β/T as desired. Next, note that the exploitation policy (if it exists) can be derived by computing the optimal policy in MΓ. Moreover, the exploration policy (if it exists) in the exploitation MDP MΓ can indeed be derived by computing the optimal policy in the exploration MDP M[n]\\Γ as observed by [19]. Finally, by Observation 5, any optimal policy in M̂αΓ (M̂ α [n]\\Γ) is an optimal policy in M̂Γ (M̂[n]\\Γ)\nTo prove Lemma 10, we need some useful background adapted from Kearns and Singh [19].\nDefinition 12 (Definition 7, Kearns and Singh [19]). Let M and M̂ be two MDPs with the same set of states and actions. We say M̂ is a β-approximation of M if\n• For any state s, R̄M (s)− β ≤ R̄M̂ (s) ≤ R̄M (s) + β.\n• For any states s and s′ and action a,\nPM (s, a, s ′)− β ≤ PM̂ (s, a, s ′) ≤ PM (s, a, s′) + β.\nLemma 13 (Lemma 5, Kearns and Singh [19]). Let M be an MDP and Γ the set of known states of M . For any s, s′ ∈ Γ and action a ∈ A, let P̂M (s, a, s′) denote the empirical probability transition estimates obtained from the visits to s. Moreover, for any state s ∈ Γ let ¯̂R(s) denote the empirical estimates of the average reward obtained from visits to s. Then with probability at least 1− δ,\n|P̂M (s, a, s′)− PM (s, a, s′)| = O ( min{ , α}2\nn2Hγ 4\n) , and | ¯̂RM (s)− R̄M (s)| = O ( min{ , α}2\nn2Hγ 4\n) .\nLemma 13 shows that M̂Γ and M̂[n]\\Γ are O(min{ , α}2/(n2H γ 4 ))-approximation MDPs for MΓ\nand M[n]\\Γ, respectively.\nLemma 14 (Lemma 4, Kearns and Singh [19]). Let M be an MDP and M̂ its O(min{ , α}2/(n2Hγ 4))- approximation. Then for any policy π ∈ Π and any state s and action a\nV πM (s)−min{ , α} ≤ V πM̂ (s) ≤ V π M (s) + min{ , α},\nand QπM (s, a)−min{α/4, } ≤ QπM̂ (s, a) ≤ Q π M (s, a) + min{α/4, }.\nProof of Lemma 10. By Definition 11 and Lemma 13, M̂Γ is aO(min{ , α}2/(n2Hγ 4))-approximation of MΓ. Then the statement directly follows by applying Lemma 14.\nRest of the Proof of Theorem 7. The only remaining part of the proof of Theorem 7 is the analysis of the probability of failure of Fair-E3. To do so, we break down the probability of failure of Fair-E3, by considering the following (exhaustive) list of possible failures:\n1. At some known state the algorithm has a poor approximation of the next step, causing M̂Γ to not be a O(min{ , α}2/(n2Hγ 4))-approximation of MΓ.\n2. At some known state the algorithm has a poor approximation of the Q∗M values for one of the actions.\n3. Following the exploration policy for 2T ∗ -step fails to yield enough visits to unknown states.\n4. At some known state, the approximation values of that state in M̂Γ is not an accurate estimate for the value of the state in MΓ.\nWe allocate δ/4 of our total probability of failure to each of these sources:\n1. Set δ′ = δ/(4n) in Lemma 10.\n2. Set δ′ = δ/(4nk) in Theorem 8.\n3. By Lemma 9, each attempted exploration is a Bernoulli trial with probability of success of at least /(4T ∗ ). In the worst case we might need to make every state known before exploiting, leading to the nmQ trajectories (mQ as Equation 3 in Definition 11) of length H γ . Therefore,\nthe probability of taking fewer than nmQ trajectories of length H γ would be bounded by δ/4 if the number of 2T ∗ -steps explorations is at least\nmexp = O\n( T ∗ nmQ\nlog(\nn δ )\n) . (6)\n4. Set δ′ = δ/(4mexp) (mexp as defined in Equation 6) in Lemma 10, as Fair-E 3 might make\n2T ∗ -steps explorations up to mexp times.\nC Omitted Details of Fair-E3\nWe first formally define the exploitation MDP MΓ and the exploration MDP M[n]\\Γ:\nDefinition 13 (Definition 9, Kearns and Singh [19]). Let M = (SM ,AM , PM , RM , T, γ) be an MDP with state space SM and let Γ ⊂ SM . We define the exploration MDP MΓ = (SMΓ ,AM , PMΓ , RMΓ , T, γ) on Γ where\n• SMΓ = Γ ∪ {s0}.\n• For any state s ∈ Γ, R̄MΓ(s) = R̄M (s), rewards in MΓ are deterministic, and R̄MΓ(s0) = 0.\n• For any action a, PMΓ(s0, a, s0) = 1. Hence, s0 is an absorbing state.\n• For any states s1, s2 ∈ Γ and any action a, PMΓ(s1, a, s2) = PM (s1, a, s2), i.e. transitions between states in Γ are preserved in MΓ.\n• For any state s1 ∈ Γ and any action a, PMΓ(s1, a, s0) = Σs2 /∈ΓPM (s1, a, s2). Therefore, all the transitions between a state in Γ and states not in Γ are directed to s0 in MΓ.\nDefinition 14 (Implicit, Kearns and Singh [19]). Given MDP M and set of known states Γ, the exploration MDP M[n]\\Γ on Γ is identical to the exploitation MDP MΓ except for its reward function. Specifically, rewards in M[n]\\Γ are deterministic as in MΓ, but for any state s ∈ Γ, R̄M[n]\\Γ(s) = 0, and R̄M[n]\\Γ(s0) = 1.\nWe next define the approximation MDPs M̂Γ and M̂[n]\\Γ which are defined over the same set of states and actions as in MΓ and M[n]\\Γ, respectively.\nLet M be an MDP and Γ the set of known states of M . For any s, s′ ∈ Γ and action a ∈ A, let P̂MΓ(s, a, s ′) denote the empirical probability transition estimates obtained from the visits to s. Moreover, for any state s ∈ Γ let ¯̂RMΓ(s) denote the empirical estimates of the average reward obtained from visits to s. Then M̂Γ is identical to MΓ except that:\n• in any known state s ∈ Γ, R̂M̂Γ(s) = ¯̂ RMΓ(s).\n• for any s, s′ ∈ Γ and action a ∈ A, PM̂Γ(s, a, s ′) = P̂MΓ(s, a, s ′).\nAlso M̂[n]\\Γ is identical to M[n]\\Γ except that:\n• for any s, s′ ∈ Γ and action a ∈ A, PM̂[n]\\Γ(s, a, s ′) = P̂M[n]\\Γ(s, a, s ′)."
    } ],
    "references" : [ {
      "title" : "Auditing black-box models by obscuring features",
      "author" : [ "Philip Adler", "Casey Falk", "Sorelle Friedler", "Gabriel Rybeck", "Carlos Scheidegger", "Brandon Smith", "Suresh Venkatasubramanian" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "The new science of sentencing",
      "author" : [ "Anna Barry-Jester", "Ben Casselman", "Dana Goldstein" ],
      "venue" : "The Marshall Project, August",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Artificial intolerance. MIT Technology Review, March 28 2016",
      "author" : [ "Nanette Byrnes" ],
      "venue" : "URL https: //www.technologyreview.com/s/600996/artificial-intolerance/",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Three naive Bayes approaches for discrimination-free classification",
      "author" : [ "Toon Calders", "Sicco Verwer" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Fairness through awareness",
      "author" : [ "Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel" ],
      "venue" : "In Proceedings of the 3rd Innovations in Theoretical Computer Science,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Certifying and removing disparate impact",
      "author" : [ "Michael Feldman", "Sorelle Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "A confidence-based approach for balancing fairness and accuracy",
      "author" : [ "Benjamin Fish", "Jeremy Kun", "Ádám Dániel Lelkes" ],
      "venue" : "In Proceedings of the 2016 SIAM International Conference on Data Mining,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "A methodology for direct and indirect discrimination prevention in data mining",
      "author" : [ "Sara Hajian", "Josep Domingo-Ferrer" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Martinez-Balleste. Rule protection for indirect discrimination prevention in data mining",
      "author" : [ "Sara Hajian", "Josep Domingo-Ferrer", "Antoni" ],
      "venue" : "In Modeling Decision for Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Equality of opportunity in supervised learning",
      "author" : [ "Moritz Hardt", "Eric Price", "Nathan Srebro" ],
      "venue" : "In Proceedings of the 30th Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Rawlsian fairness for machine learning",
      "author" : [ "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Seth Neel", "Aaron Roth" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Fairness in learning: Classic and contextual bandits",
      "author" : [ "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth" ],
      "venue" : "In Proceedings of the 30th Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Data preprocessing techniques for classification without discrimination",
      "author" : [ "Faisal Kamiran", "Toon Calders" ],
      "venue" : "Knowledge and Information Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Discrimination aware decision tree learning",
      "author" : [ "Faisal Kamiran", "Toon Calders", "Mykola Pechenizkiy" ],
      "venue" : "In Proceedings of the 10th IEEE International Conference on Data Mining,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Decision theory for discrimination-aware classification",
      "author" : [ "Faisal Kamiran", "Asim Karim", "Xiangliang Zhang" ],
      "venue" : "In Proceedings of the 12th IEEE International Conference on Data Mining,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Fairness-aware classifier with prejudice remover regularizer",
      "author" : [ "Toshihiro Kamishima", "Shotaro Akaho", "Hideki Asoh", "Jun Sakuma" ],
      "venue" : "In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "Michael Kearns", "Satinder Singh" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2002
    }, {
      "title" : "Approximate planning in large POMDPs via reusable trajectories",
      "author" : [ "Michael Kearns", "Yishay Mansour", "Andrew Ng" ],
      "venue" : "In Proceedings of the 13th Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2000
    }, {
      "title" : "Inherent trade-offs in the fair determination of risk",
      "author" : [ "Jon Kleinberg", "Sendhil Mullainathan", "Manish Raghavan" ],
      "venue" : "scores. CoRR,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "k-NN as an implementation of situation testing for discrimination discovery and prevention",
      "author" : [ "Binh Thanh Luong", "Salvatore Ruggieri", "Franco Turini" ],
      "venue" : "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Can an algorithm hire better than a human?  The New York",
      "author" : [ "Clair Miller" ],
      "venue" : "Times, June",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Discrimination-aware data mining",
      "author" : [ "Dino Pedreshi", "Salvatore Ruggieri", "Franco Turini" ],
      "venue" : "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2008
    }, {
      "title" : "Predictive policing using machine learning to detect patterns of crime",
      "author" : [ "Cynthia Rudin" ],
      "venue" : "URL http://www.wired.com/insights/2013/08/ predictive-policing-using-machine-learning-to-detect-patterns-of-crime/",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Personal Communication, June 2016",
      "author" : [ "Satinder Singh" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Discrimination in online ad delivery",
      "author" : [ "Latanya Sweeney" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Model-based reinforcement learning with nearly tight exploration complexity bounds",
      "author" : [ "Istv’an Szita", "Csaba Szepesvári" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 24,
      "context" : "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].",
      "startOffset" : 176,
      "endOffset" : 183
    }, {
      "referenceID" : 4,
      "context" : "[6, 7, 12, 14]).",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "[6, 7, 12, 14]).",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 9,
      "context" : "[6, 7, 12, 14]).",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 11,
      "context" : "[6, 7, 12, 14]).",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 11,
      "context" : "[14], who study fairness in the contextual bandit framework.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[14] define fairness with respect to one-step rewards, which is appropriate for the contextual bandit setting where the learner’s actions do not affect future state.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[14] in terms of myopic rewards would be in conflict with the optimal policy in an underlying MDP if it was necessary to make sub-optimal decisions in the short run (e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[14], our results establish rigorous trade-offs between fairness and performance in reinforcement learning algorithms.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "We adapt E3 [19], a wellknown algorithm for MDP learning, to satisfy approximate-action fairness.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 16,
      "context" : "E3 [19] was the first learning algorithm with a polynomial learning rate, and a subsequent literature gave a sequence of improved bounds (see Szita and Szepesvári [28] and references within).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 25,
      "context" : "E3 [19] was the first learning algorithm with a polynomial learning rate, and a subsequent literature gave a sequence of improved bounds (see Szita and Szepesvári [28] and references within).",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 9,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 19,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 21,
      "context" : "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).",
      "startOffset" : 0,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "[1, 5, 7, 8, 18, 22]).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "[1, 5, 7, 8, 18, 22]).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "[1, 5, 7, 8, 18, 22]).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "[1, 5, 7, 8, 18, 22]).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "[1, 5, 7, 8, 18, 22]).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : "[1, 5, 7, 8, 18, 22]).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "[6], it suffers from two problems.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[6] for a catalog of ways in which statistical parity fails as a definition of fairness.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[12] give a definition aiming to capture the idea of equality of opportunity that overcomes many of these limitations of notions of group fairness.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[6] propose and explore the basic properties of a technical definition of individual fairness formalizing the idea that “similar individuals should be treated similarly”.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "[14] and the present work suggest a natural metric for the tasks of regret minimization in a bandit setting and long-term reward maximization in an MDP.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[6] is that the latter operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "• PM : SM × AM × SM → [0, 1] is a transition probability distribution mapping (s, a, s′) 7→ p where p is the probability of arriving in state s′ after taking action a from state s.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "We assume the support of the reward distribution is [0, 1] for all s.",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "R̄M : SM → [0, 1] denote the mean of reward distribution at state s.",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 16,
      "context" : "Kearns and Singh [19]).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 16,
      "context" : "distribution is over a possibly unbounded region but the mean and the variance of the reward distribution is bounded in all states using standard techniques (see Kearns and Singh [19] for more details).",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, assuming that the support of the reward distributions is between [0, 1] can be made without loss of generality up to scaling.",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "-reward mixing time is always linearly bounded by the -mixing time but can be much smaller than the -mixing time in certain cases (see Kearns and Singh [19] for a discussion).",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 16,
      "context" : "In the discounted rewards setting, the expected discounted reward of any policy after H = log ( (1− γ)) / log(γ) (see Kearns and Singh [19], Lemma 2) steps approaches the expected asymptotic discounted reward: Lemma 2 (Lemma 2, Kearns and Singh [19]).",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 16,
      "context" : "In the discounted rewards setting, the expected discounted reward of any policy after H = log ( (1− γ)) / log(γ) (see Kearns and Singh [19], Lemma 2) steps approaches the expected asymptotic discounted reward: Lemma 2 (Lemma 2, Kearns and Singh [19]).",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 23,
      "context" : "Lemma 3 (Singh [26]).",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 11,
      "context" : "[14]’s definition for learning in a contextual bandit setting: namely, Definition 1 when the Q∗(s, a) values are replaced with the expected payoff of choosing a given arm a.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[14] define the quality of an action to be QM (s, a, 1), the expected one-step reward for choosing a from s.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[20], connecting the number of random walks taken from s to the accuracy of the empirical V ∗ M estimates.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[20]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "For the second condition, we adapt the analysis of E3 in which Kearns and Singh [19] show that if each action in a state s is taken m2 times, then the transition probabilities and reward in state s can be estimated accurately (see Section 6.",
      "startOffset" : 80,
      "endOffset" : 84
    } ],
    "year" : 2017,
    "abstractText" : "We initiate the study of fair learning in Markovian settings, where the actions of a learning algorithm may affect its environment and future rewards. Working in the model of reinforcement learning, we define a fairness constraint requiring that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take exponentially many rounds in the number of states to achieve non-trivial approximation to the optimal policy. Our main result is a polynomial time algorithm that is provably fair under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness.",
    "creator" : "LaTeX with hyperref package"
  }
}