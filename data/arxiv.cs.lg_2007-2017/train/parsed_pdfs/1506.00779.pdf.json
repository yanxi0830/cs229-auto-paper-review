{
  "name" : "1506.00779.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays",
    "authors" : [ "Junpei Komiyama", "Junya Honda", "Hiroshi Nakagawa" ],
    "emails" : [ "JUNPEI@KOMIYAMA.INFO", "HONDA@STAT.T.U-TOKYO.AC.JP", "NAKAGAWA@DL.ITC.U-TOKYO.AC.JP" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The multi-armed bandit (MAB) problem is one of the most well-known instances of sequential decision-making problems in uncertain environments, which can model many real-world scenarios. The problem involves conceptual entities called arms. At each round, the forecaster draws one of K arms and receives a corresponding reward. The aim of the forecaster is to maximize the cumulative reward over rounds, and the forecaster’s performance is usually measured by a regret, which is the gap between his or her cumulative reward and that of an optimal drawing policy.\nThroughout the rounds, the forecaster faces an “exploration vs. exploitation” dilemma. On one hand, the forecaster wants to exploit the information that he or she has gathered up to the previous round by selecting seemingly good arms. On the other hand, there is always a possibility that the other arms have been underestimated, which motivates him or her to explore seemingly bad arms in order to gather their information. To resolve this dilemma, the forecaster uses an algorithm to control the number of draws for each arm.\nIn the stochastic MAB problem, which is the most widely studied version of the MAB problem, it is assumed that each arm is associated with a distinct probability distribution. While there have been many theoretical studies on the infinite setting in which future rewards are geometrically discounted (e.g., the Gittins index (Gittins & Jones, 1974)), recent availability of massive data has led to a finite horizon setting in which every reward has the same importance. In this work, we focus on the latter setting.\nThere has been significant progress in this setting of the MAB problem. In particular, the upper confidence bound (UCB) algorithm (Auer et al., 2002) has been widely used and studied for its computational simplicity and customizability. Whereas the coefficient of the leading logarithmic term in UCB is larger than the theoretical lower bound given by Lai & Robbins (1985), algorithms have been proposed that achieve this bound, such as DMED (Honda & Takemura, 2010), Kinf, and KL-UCB (Cappé et al., 2013).\nMoreover, Thompson sampling (TS) (Thompson, 1933) has recently attracted attention for its excellent performance (Scott, 2010; Chapelle & Li, 2011) and it has been revealed to be applicable to even a wider class of problems (Agrawal & Goyal, 2013a; Russo & Roy, 2013; Osband et al., 2013; Kocák et al., 2014; Guha & Munagala, 2014). Thompson sampling is an old heuristic that has a spirit of Bayesian inference and selects an arm based on posterior samples of the expectation of each arm. It has been shown that TS has an optimal regret bound (Agrawal & Goyal,\nar X\niv :1\n50 6.\n00 77\n9v 1\n[ st\nat .M\nL ]\n2 J\nun 2\n01 5\n2012; Kaufmann et al., 2012; Agrawal & Goyal, 2013b)."
    }, {
      "heading" : "1.1. Multiple-play MAB problem",
      "text" : "The literature mentioned above has specifically dealt with the MAB problem in which a single arm is selected and drawn at each round. Let us call this problem single-play MAB (SP-MAB). While the SP-MAB problem is indisputably important as a canonical problem, in many practical situations multiple entities corresponding to arms are selected at each round. We call the MAB problem in which several arms can be selected multiple-play MAB (MP-MAB). Examples of the situations that can be modeled as an MP-MAB problem include the followings.\n• Example 1 (placement of online advertisements): a web site has several slots where advertisements can be placed. Based on each user’s query, there is a set of candidates of relevant advertisements from which web sites can select to display. The effectiveness of advertisements varies: some advertisements are more appealing to the user than others. With the standard model in online advertising, it is assumed that each advertisement is associated with a click-through-rate (CTR), which is the number of clicks per view. Since web sites receive revenue from clicks on advertisements, it is natural to maximize it, which can be considered as an instance of an MP-MAB problem in which advertisements and clicks correspond to arms and rewards, respectively.\n• Example 2 (channel selection in cognitive radio networks (Huang et al., 2008)): a cognitive radio is an adaptive scheme for allocating channels, such as wireless network spectrums. There are two kinds of users: primary and secondary. Unlike primary users, secondary users do not have primary access to a channel but can take advantage of the vacancies in primary access and opportunistically exploit instantaneous spectrum availability when primary users are idle. However, the availabilities of channels are not easily known. Usually, secondary users have access to multiple channels. They can enhance their communication efficiency by adaptively estimating the availability statistics of the channels, which can be considered as an MP-MAB problem in which channels and the permission of communication are arms and rewards, respectively.\nThere have been several studies on the MP-MAB problem. Anantharam et al. (1987) derived an asymptotic lower bound on the regret for this problem and proposed an algorithm to achieve this bound. Because their algorithm requires certain statistics that are difficult to compute, efficiently computable MP-MAB algorithms have also been extensively studied. Chen et al. (2013) extended a UCB-\nbased algorithm to a multiple-play case with combinatorial rewards and Gopalan et al. (2014) extended TS to a wide class of problems. Although both papers provide a logarithmic regret bound, the constant factors of these regret bounds do not match the lower bound. Therefore, it is unknown whether the optimal regret bound for the MP-MAB problem is achievable by using a computationally efficient algorithm.\nThe main difficulty in analyzing the MP-MAB problem lies in the fact that the regret depends on the combinatorial structure of arm draws. More specifically, an algorithm with the optimal bound on the number of draws of suboptimal arms does not always ensure the optimal regret bound unlike the SP-MAB problem.\nContribution: Our contributions are as follows. • TS-based algorithm for the MP-MAB problem and\nits optimal regret bound: the first and main contribution of this paper is an extension of TS to the multiple play case, which we call MP-TS. We prove that MP-TS for binary rewards achieves an optimal regret bound. To the best of our knowledge, this paper is the first to provide a computationally efficient algorithm in the MP-MAB problem with the optimal regret bound by Anantharam et al. (1987).\n• Novel analysis technique: to solve the difficulty in the combinatorial structure of the MP-MAB problem, we show that the independence of posterior samples among arms in TS is a key property for suppressing the number of simultaneous draws of several suboptimal arms, and the use of this property eventually leads to the optimal regret bound.\n• Experimental comparison among MP-MAB algorithms: we compare MP-TS with other algorithms, and confirm its efficiency. We also propose an empirical improvement of MP-TS (IMP-TS) motivated by analyses on the regret structure of the MP-MAB problem. We confirm that IMP-TS improves the performance of MP-TS without increasing computational complexity."
    }, {
      "heading" : "2. Problem Setup",
      "text" : "Let there be K arms. Each arm i ∈ [K] = {1, 2, . . . ,K} is associated with a probability distribution νi = Bernoulli(µi), µi ∈ (0, 1). At each round t = 1, 2, . . . , T , the forecaster selects a set of L < K arms I(t), then receives the rewards of the selected arms. The reward Xi(t) of each selected arm i is i.i.d. samples from νi. LetNi(t) be the number of draws of arm i before round t (i.e., Ni(t) = ∑t−1 t′=1 1{i ∈ I(t′)}, where 1{A} = 1 if event A holds and = 0 otherwise.), and µ̂i(t) be the empirical mean of the rewards of arm i at the beginning of round t. The forecaster is interested in maximizing the sum\nof rewards over drawn arms. For simplicity, we assume that all arms have distinct expected rewards (i.e., µi 6= µj for any i 6= j). We discuss the case in which µi = µj for some i and j in Appendix A.1, which is in Supplementary Material. Without loss of generality, we assume µ1 > µ2 > µ3 > · · · > µK . Of course, algorithms do not exploit this ordering. We define optimal arms as topL arms (i.e., arms [L]), and suboptimal arms as the others (i.e., arms [K] \\ [L]). The regret, which is the expected loss of the forecaster, is defined as\nReg(T ) = T∑ t=1 ∑ i∈[L] µi − ∑ i∈I(t) µi  . The expectation of regret E[Reg(T )] is used to measure the performance of an algorithm."
    }, {
      "heading" : "3. Regret Bounds",
      "text" : "In this section we introduce the known lower bounds of the regret for the SP-MAB and MP-MAB problems and discuss the relation between them."
    }, {
      "heading" : "3.1. Regret bound for SP-MAB problem",
      "text" : "The SP-MAB problem, which has been thoroughly studied in the fields of statistics and machine learning, is a special case of the MP-MAB problem with L = 1. The optimal regret bound in the SP-MAB problem was given by Lai & Robbins (1985). They proved that, for any strongly consistent algorithm (i.e., algorithms with subpolynomial regret for any set of arms), there exists a lower bound\nE[Ni(T + 1)] ≥ (\n1− o(1) d(µi, µ1)\n) log T, (1)\nwhere d(p, q) = p log (p/q)+(1−p) log ((1− p)/(1− q)) is the KL divergence between two Bernoulli distributions with expectation p and q. Note that when arm i is drawn, the regret increases by ∆i,1 and the regret is written as\nE[Reg(T )] = ∑ i 6=1 Ni(T + 1)∆i,1, (2)\nwhere ∆i,j = µj − µi. Therefore, inequality (1) directly leads to the regret lower bound\nE[Reg(T )] ≥ ∑ i 6=1 ( (1− o(1))∆i,1 d(µi, µ1) ) log T. (3)\nOne may think that applying the techniques of the SP-MAB problem would directly yield an optimal bound for a more general MP-MAB problem. However, this is not the case. In short, the difficulty in analyzing the regret on the MPMAB problem arises from the fact that the optimal bound\non the number of suboptimal arm draws does not directly lead to the optimal regret. From this point forward, we focus on the MP-MAB problem in which L is not restricted to one."
    }, {
      "heading" : "3.2. Extension to MP-MAB problem",
      "text" : "The regret lower bound in the MP-MAB problem, which is the generalization of inequality (3), was provided by Anantharam et al. (1987). They first proved that, for any strongly consistent algorithm and suboptimal arm i, the number of arm i draws is lower-bounded as\nE[Ni(T + 1)] ≥ (\n1− o(1) d(µi, µL)\n) log T. (4)\nUnlike in the SP-MAB problem, the regret in the MP-MAB problem is not uniquely determined by the number of suboptimal arm draws. As illustrated in Figure 1, the regret is dependent on the combinatorial structure of arm draws.\nRecall that a regret increase at each round is the gap of expected rewards between the optimal arms and that of the selected arms. When a suboptimal arm is selected, one optimal arm is excluded from I(t) instead of the suboptimal arm. Let the selected suboptimal arm and excluded optimal arm be i and j, respectively. Then, we lose expected reward µj − µi. Namely, the loss in the expected reward at each round is given by∑\nj∈[L] µj − ∑ i∈I(t) µi = ∑ j∈[L]\\I(t) µj − ∑ i∈I(t)\\[L] µi\n≥ ∑\ni∈I(t)\\[L]\n(µL − µi), (5)\nAlgorithm 1 Multiple-play Thompson sampling (MP-TS) for binary rewards\nInput: # of armsK, # of selection L for i = 1, 2, . . . ,K do Ai, Bi = 1, 1 end for t← 1. for t = 1, 2, . . . , T do\nfor i = 1, 2, . . . ,K do θi(t) ∼ Beta(Ai, Bi) end for I(t) = top-L arms ranked by θi(t). for i ∈ I(t) do\nif Xi(t) = 1 then Ai ← Ai + 1 else Bi ← Bi + 1\nend if end for\nend for\nwhere we used the fact µj ≥ µL for any optimal arm j. From this relation, the regret is expressed as\nReg(T ) ≥ T∑ t=1 ∑ i∈I(t)\\[L] (µL − µi)\n= ∑\ni∈[K]\\[L]\n(µL − µi)Ni(T + 1) (6)\nwhich, combined with (4), leads to the regret lower bound by Anantharam et al. (1987) that any strongly consistent algorithm satisfies\nE[Reg(T )] ≥ ∑\ni∈[K]\\[L]\n(1− o(1))∆i,L d(µi, µL) log T. (7)"
    }, {
      "heading" : "3.3. Necessary condition for an optimal algorithm",
      "text" : "In Sections 3.1 and 3.2, we saw that the derivations of the regret bounds are analogous between the SP-MAB and MP-MAB problems. However, there is a difference in the relation between the regret andNi(T ), the number of draws of suboptimal arms, is given as equation (2) in the SP-MAB problem, whereas it is given as inequality (6) in the MPMAB problem. This means that, an algorithm achieving the asymptotic lower bound (4) on Ni(T ) does not always achieve the asymptotic regret bound (7).\nWhen suboptimal arm i is selected, one of the optimal arms is pushed out instead of arm i, and the regret increases by the difference between the expected rewards of these two arms. The best scenario is that, arm L, which is the optimal arm with the smallest expected reward, is almost always\nthe arm pushed out instead of a suboptimal arm. For this scenario to occur, it is necessary to ensure that at most one suboptimal arm is drawn for almost all rounds because, if two suboptimal arms are selected, at least one arm in [L−1] is pushed out.\nIn the next section, we propose an extension of TS to the MP-MAB problem, and explain that it has a crucial property for suppressing this simultaneous draw of two suboptimal arms.\nRemark: Corollary 1 of Gopalan et al. (2014) shows the achievability of the bound in the RHS of (4) on the number of draws of suboptimal arms. Whereas this does not lead to the optimal regret bound as discussed above, they originally derived in Theorem 1 an O(log T ) bound on the number of each suboptimal action (that is, each combination of arms including suboptimal ones) for a more general setting of MP-MAB. Thus, we can directly use this bound to derive a better regret bound. However, to show the optimality in the sense of regret it is necessary to prove that there are at most o(log T ) rounds such that an arm in [L−1] is pushed out. Therefore, it still requires further discussion to derive the optimal regret bound of TS. Note also that the regret bound by Gopalan et al. (2014) is restricted to the case that the prior has a finite support and the true parameter is in the support, and thus their analysis requires some approximation scheme for dealing Bernoulli rewards."
    }, {
      "heading" : "4. Multiple-play Thompson Sampling Algorithm",
      "text" : "Algorithm 1 is our MP-TS algorithm. While TS for singleplay selects the top-1 arm based on a posterior sample θi(t), MP-TS selects the top-L arms ranked by the posterior sample θi(t). Like Kaufmann et al. (2012) and Agrawal & Goyal (2013b), we set the uniform prior on each arm.\nIn Section 3.3, we discussed that the necessary condition to achieve the optimal regret bound is to suppress the simultaneous draws of two or more suboptimal arms, which characterizes the difficulty of the MP-MAB problem.\nNote that it is easy to extend other asymptotically optimal SP-MAB algorithms, such as KL-UCB, to the MPMAB problem. Nevertheless, we were not able to prove the optimality of these algorithms for the MP-MAB problem though the achievability of the bound (4) on Ni(T ) is easily proved, and the simulation results in Section 7 also imply their achievability of the regret bound. This is because TS has quite a plausible property to suppress simultaneous draws as we discuss below.\nBefore the exact statement in the next section, we give an intuition for the natural extension of TS (or other asymptotically optimal SP-MAB algorithms) can have the opti-\nmal regret in the MP-MAB problem. Roughly speaking, a bandit algorithm with a logarithmic regret draws a suboptimal arm with probability O(1/t) at the t-th round, which amounts to O( ∑T t=1 1/t) = O(log T ) regret. Thus, two suboptimal arms are drawn at the same round with probability O(1/t2), which amounts to O( ∑T t=1 1/t\n2) = O(1) total simultaneous draws, provided that each suboptimal arm is selected independently.\nIn TS, the score θi(t) for the choice of arms is generated randomly at each round from the posterior independently between each arm, which enables us to bound simultaneous draws as the above intuition. On the other hand, in KL-UCB (or in other index policies), the UCB score for the choice of arms is deterministic given the past results of rewards, which means that the scores of suboptimal arms may behave quite similarly in the worst case on the past rewards."
    }, {
      "heading" : "5. Optimal Regret Bound",
      "text" : "In this section, we state the main theoretical result (Theorem 1). The analysis that leads to this theorem is discussed in Section 6.\nTheorem 1. (Regret upper bound of MP-TS) For any sufficiently small 1 > 0, 2 > 0, the regret of MP-TS is upperbounded as\nE[Reg(T )] ≤ ∑\ni∈[K]\\[L]\n( (1 + 1)∆i,L log T\nd(µi, µL) ) +Ca( 1, µ1, µ2, . . . , µK) +Cb(T, 2, µ1, µ2, . . . , µK),\nwhere, Ca = Ca( 1, µ1, µ2, . . . , µK) is a constant independent on T and is O( −21 ) when we regard {µi}Ki=1 as constants. The value Cb = Cb(T, 2, µ1, µ2, . . . , µK) is a function of T , which, by choosing proper 2, grows at a rate of O(log log T ) = o(log T ).\nBy letting 1 = O((log T )−1/3) we obtain\nE[Reg(T )] ≤ ∑\ni∈[K]\\[L]\n∆i,L log T\nd(µi, µL) +O((log T )2/3) (8)\nand we see that MP-TS achieves the asymptotic bound in (7).\nExpected regret and high-probability regret: Anantharam et al. (1987) originally derived a regret lower bound in a stronger form than (7) such that for any > 0, the regret of a strongly consistent algorithm is lower-bounded as\nlim T→∞ Pr Reg(T ) log T ≥ ∑\ni∈[K]\\[L]\n(1− )∆i,L d(µi, µL)\n = 1.\nCombining this with (8) we can easily see that MP-TS satisfies\nlim T→∞ Pr Reg(T ) log T ≤ ∑\ni∈[K]\\[L]\n(1 + )∆i,L d(µi, µL)  = 1, (9) that is, MP-TS is also asymptotically optimal in the sense of high probability. Since an algorithm satisfying (9) is not always optimal in the sense of expectation, our result, the expected optimal regret bound, is also stronger in this sense than the high-probability bound by Gopalan et al. (2014)."
    }, {
      "heading" : "6. Regret Analysis",
      "text" : "We first define some additional notation that are useful for our analysis in Section 6.1 then analyze the regret bound in Section 6.2. The proofs of all the lemmas, except for Lemma 2, are given in the Appendix."
    }, {
      "heading" : "6.1. Additional notation",
      "text" : "Let µ(−)L = µL − δ and µ (+) i = µi + δ for δ > 0 and i ∈ [K] \\ [L]. We assume δ to be sufficiently small such that µ(−)L ∈ (µL+1, µL) and µ (+) i ∈ (µi, µL). We also define N sufi (T ) = log T\nd(µ (+) i ,µ (−) L )\n. Intuitively, N sufi (T ) is the\nsufficient number of explorations to make sure that arm i is not as good as arm L.\nEvents: Now, let max(m)i∈S ai denote the m-th largest element of {ai}i∈S ∈ R|S|, that is, maxi∈S(m)ai = maxS′⊂S:|S′|=m mini∈S′ ai. We define θ∗(t) = max\n(L) i∈[K] θi(t) as theL-th largest posterior sample at round t (i.e., the minimum posterior sample among the selected arms), and θ∗∗\\i,j(t) = max (L−1) k∈[K]\\{i,j} θk(t) as the (L− 1)- th largest posterior sample at round t except for arms i and j. Moreover, let ν = µL−1+µL2 . Let us define the following events.\nAi(t) = {i ∈ I(t)}, B(t) = {θ∗(t) ≥ µ(−)L },\nCi(t) = ⋂\nj∈[K]\\([L−1]∪{i})\n{θ∗∗\\i,j(t) ≥ ν},\nDi(t) = {Ni(t) < N sufi (T )}.\nEvent Ai(t) states that arm i is sampled at round t, and Di(t) states that arm i has not been sampled sufficiently yet. The complements of B(t) and Ci(t) are related to the underestimation of optimal arms. Since the optimal arms are sampled sufficiently, Bc(t) or Cci (t) should not occur very frequently."
    }, {
      "heading" : "6.2. Proof of Theorem 1",
      "text" : "We first decompose the regret to the contribution of each arm. Recall that, the regret increase by drawing suboptimal\narm i is determined by the optimal arm excluded in the selection set I(t). Formally, for suboptimal arm i, let\n∆i(t) = { (maxj∈[L]\\I(t) µj)− µi if I(t) 6= [L], 0 otherwise,\n(10) and\nRegi(T ) = T∑ t=1 1{i ∈ I(t)}∆i(t).\nFrom inequality (5) the following inequality is easily derived Reg(T ) ≤ ∑\ni∈[K]\\[L]\nRegi(T ).\nWe next decompose Regi(T ) into several terms by using events A–D. After giving bounds for these terms, we finally give the total regret bound, which proves Theorem 1. Note that, in bounding the deviation of Bernoulli means and Beta posteriors in the Appendix, our analysis borrowed some techniques developed in the context of the SP-MAB problem, mostly from Agrawal & Goyal (2013b), and some from Honda & Takemura (2014).\nLemma 2. The regret by drawing suboptimal arm i > L is decomposed as:\nRegi(T ) ≤ T∑ t=1\n1{Bc(t)}︸ ︷︷ ︸ (A) +\nT∑ t=1\n1{Ai(t), Cci (t)}︸ ︷︷ ︸ (B)\n+ ∑\nj∈[K]\\([L−1]∪{i}) T∑ t=1\n1{Ai(t), Ci(t),Di(t),Aj(t)}︸ ︷︷ ︸ (C)\n+ T∑ t=1\n1{Ai(t),B(t),Dci (t)}︸ ︷︷ ︸ (D) +N sufi (T )∆i,L,\nwhere, for example, {A,B} abbreviates {A ∩ B}.\nRoughly speaking,\n• Term (A) corresponds to the case in which, some of the optimal arms are under-estimated. • Term (B) corresponds to the case in which, arm i is selected and some of the arms in [L − 1] are underestimated. • Term (C) corresponds to the case in which, arm i ∈ [K] \\ [L] and j ∈ [K] \\ ([L− 1] ∪ {i}) are simultaneously drawn. In particular, term (C) is unique in the MP-MAB problem that causes additional regret increase, and in analyzing this term we fully use the fact that the samples of the posterior distributions on the arms are independent of each other.\n• Term (D) corresponds to the case in which, arm i is selected after it is sufficiently explored.\nProof of Lemma 2. The contribution of suboptimal arm i to the regret is decomposed as follows. By using the fact ∆i(t) ≤ 1 and the following decomposition of an event\nAi(t) ⊂ Bc(t) ∪ {Ai(t), Cci (t)} ∪ {Ai(t),B(t), Ci(t)} ⊂ Bc(t) ∪ {Ai(t), Cci (t)}\n∪ {Ai(t),B(t),Dci (t)} ∪ {Ai(t), Ci(t),Di(t)},\nwe have\nRegi(T ) = T∑ t=1 1{Ai(t)}∆i(t)\n≤ T∑ t=1 1{Bc(t)}+ T∑ t=1 1{Ai(t), Cci (t)}\n+ T∑ t=1 1{Ai(t),B(t),Dci (t)}\n+ T∑ t=1 1{Ai(t), Ci(t),Di(t)}∆i(t). (11)\nRecall that ∆i(t) is defined as (10). At each round, when L and all suboptimal arms, except for i, are not selected, then I(t) = {1, 2, . . . , L− 1, i}; ∆i(t) = ∆i,L. Therefore,\nT∑ t=1 1{Ai(t), Ci(t),Di(t)}∆i(t)\n≤ T∑ t=1 1{Ai(t), Ci(t),Di(t)}∆i,L\n+ T∑ t=1 1{Ai(t), Ci(t),Di(t), ⋃ j∈[K]\\([L−1]∪{i}) Aj(t)}\n≤ T∑ t=1 1{Ai(t),Di(t)}∆i,L\n+ ∑\nj∈[K]\\([L−1]∪{i}) T∑ t=1 1{Ai(t), Ci(t),Di(t),Aj(t)}\n≤ N sufi (T )∆i,L\n+ ∑\nj∈[K]\\([L−1]∪{i}) T∑ t=1 1{Ai(t), Ci(t),Di(t),Aj(t)}.\n(12)\nSummarizing (11) and (12) completes the proof.\nThe following lemma bounds terms (A)–(D).\nLemma 3. (Bounds on individual terms) Let 2 > 0 be arbitrary. For sufficiently small δ and 2, the four terms\nare bounded in expectation as:\nE[(A)] = O\n( 1\n(µL − µ(−)L )2\n) = O ( 1\nδ2\n) , (13)\nE[(B)] = O(1), (14) E[(C)] ≤ ∑\nj∈[K]\\([L−1]∪{i})\n( 2 + 4T − 2∆ 2 L,L−1 8 ) log T\nd(µi, µL) +O(1),\nand (15)\nE[(D)] ≤ 2+ 1 d(µ\n(+) i , µi)\n= O\n( 1\nδ2\n) . (16)\nThe proof of Lemma 3 is in Appendix A.4. Lemma 3 states that terms (A), (B), and (D) are O(1/δ2). Moreover, the following lemma bounds term (C).\nLemma 4. (Asymptotic convergence of 2-dependent factor) By choosing an O((log log T )/ log T ) value of 2, we obtain E[(C)] = O(log log T ).\nThe proof of Lemma 4 is in Appendix A.5. Now it suffices to evaluate N sufi (T ) = log T\nd(µ (+) i ,µ (−) L )\nto complete the\nproof. From the convexity of KL divergence there exists a constant ci = ci(µi, µL) > 0 such that\nd(µ (+) i , µ (−) L ) = d(µi + δ, µL − δ) ≥ (1− ciδ)d(µi, µL)\nand therefore E[Reg(T )]≤ ∑\ni∈[K]\\[L]\nE[Regi(T )] ≤ ∑\ni∈[K]\\[L]\nE [ T∑ t=1 1{Ai(t)}∆i(t) ]\n≤ ∑\ni∈[K]\\[L]\n{ E [(A) + (B) + (C) + (D)] +N sufi (T )∆i,L } ≤ ∑\ni∈[K]\\[L]\n∆i,L log T\n(1− ciδ)d(µi, µL)︸ ︷︷ ︸ main term\n+O\n( 1\nδ2 ) ︸ ︷︷ ︸\nCa\n+O(log log T )︸ ︷︷ ︸ Cb .\nSince (1 − ciδ)−1 ≤ 1 + 2ciδ for ciδ ≤ 1/2, we complete the proof of Theorem 1 by letting 1 < 1/2 and δ = 1/maxi∈[K]\\[L] ci = Θ( 1)."
    }, {
      "heading" : "7. Experiment",
      "text" : "We ran a series of computer simulations1 to clarify the empirical properties MP-TS. The simulations involved the following three scenarios. In Scenarios 1 and 2, we used fixed arms similar to that of Garivier & Cappé (2011), and Scenario 3 is based on a click log dataset of advertisements on a commercial search engine. Algorithms: the simulations involved MP-TS, Exp3.M (Uchiya et al., 2010), CUCB (Chen et al., 2013), and\n1The source code of the simulations is available at https://github.com/jkomiyama/multiplaybanditlib.\nMP-KL-UCB. Exp3.M is a state-of-the-art adversarial bandit algorithm for the MP-MAB problem2. The learning rate γ of Exp3.M is set in accordance with Corollary 1 of Uchiya et al. (2010). Note that the CUCB algorithm in the MP-MAB problem at each round draws the top-L arms of the UCB indices µ̂i +√\n(3 log t)/(2Ni(t)). MP-KL-UCB is the algorithm that selects the top-L arms in accordance with the KL-UCB index supq∈[µ̂i(t),1] {q|Ni(t)d(µ̂i(t), q) ≤ log t}. Scenario 1 (5-armed bandits): the simulations include 5 Bernoulli arms with {µ1, . . . , µ5} = {0.7, 0.6, 0.5, 0.4, 0.3}, and L = 2. Scenario 2 (20-armed bandits): the simulations include 20 Bernoulli arms with µ1 = 0.15, µ2 = 0.12, µ3 = 0.10, µi = 0.05 for i ∈ {4, 5, . . . , 12}, µi = 0.03 for i ∈ {13, 14, . . . , 20}, and L = 3. Scenario 3 (many-armed bandits, online advertisement based CTRs): we conducted another set of experiments with arms whose expectations were based on the dataset provided for KDD Cup3 2012 track 2. The dataset involves a click log on soso.com (a large-scale search engine serviced by Tencent), which is composed of 149 million impressions (view of advertisements). We processed the data as follows. First, we excluded users of abnormally high click probability (i.e., users who had more than 1, 000 impressions and more than 0.1 click probability) from the log. We also excluded minor advertisements (ads) that had less than 5, 000 impressions. There are a wide variety of ads on a search engine (e.g., ”rental cars”, ”music”, etc.) and randomly picking ads from a search engine should yield a set of irrelevant ads. To address this issue, we selected popular queries that had more than 104 impressions and more than 50 ads that appeared on the query. As a result, 80 queries were obtained. The number of ads associated with each query ranged from 50 to 105, and the average clickthrough-rate (CTR, the probability that the ad is clicked) of an ad on each query ranged from 1.15% to 6.86%. After that, each ad was converted into a Bernoulli arm with its expectations corresponding to the CTR of the ad. At the beginning of each run, one of the queries was randomly selected, and the bandit simulation with the arms corresponding to the query andL = 3 is then conducted. This scenario was more difficult than the first two scenarios in the sense that 1) a larger number of arms were involved and 2) the reward gap among arms was very small.\nThe simulation results are shown in Figure 2. In all scenarios, the tendency is the same: our proposed MP-TS performs significantly better than the other algorithms. MPKL-UCB is not as good as MP-TS, but clearly better than CUCB and Exp3.M. While it is unclear whether the slope\n2Note that, Exp3.M is designed for the adversarial setting in which the rewards of arms are not necessarily stationary.\n3https://www.kddcup2012.org/\nof the regret of MP-KL-UCB converges to the asymptotic bound or not, the slope of the regret of TS quickly approaches the asymptotic lower bound."
    }, {
      "heading" : "7.1. Improvement of MP-TS based on the empirical means",
      "text" : "We now introduce an improved version of MP-TS (IMPTS). In the theoretical analysis of the MP-MAB problem, we observed that an extra loss arises when multiple suboptimal arms are drawn at the same round. Based on this observation, the new algorithm selects L−1 arms on the basis of empirical averages and selects the last arm on the basis of TS to avoid simultaneous draws of suboptimal arms. In other words, this algorithm is further aimed to minimize the regret by purely exploiting the knowledge in the top(L − 1) arms; thus, limiting the exploration to only one arm. One might fear that this increase in exploitation could devastate the balance between exploration and exploitation. Although we provide no regret bound for the improved version of the algorithm, we expect that this algorithm will also achieve the asymptotic bound for the following reason. When we restrict the exploration to one arm, the number of opportunities for an arm to be explored may decrease, say, from T to T/L. Still, T/L opportunities are sufficient since O(log(T/L)) = O(log T ). In fact, the algorithm proposed by Anantharam et al. (1987) achieves the asymptotic bound\neven though L − 1 arms are selected based on empirical means as in IMP-TS. Similarly, we define an improved version of MP-KL-UCB (IMP-KL-UCB) for selecting the first L − 1 arms on the basis of empirical averages. The before/after analysis of this improvement is shown in Figure 3. One sees that, (i) MP-TS still performs better than IMPKL-UCB, and (ii) IMP-TS reduces the regret throughout the rounds. In particular, when the number of the rounds is small (T ∼ 103–104), the advantage of IMP-TS is large."
    }, {
      "heading" : "8. Discussion",
      "text" : "We extended TS to the multiple-play setting and proved its optimality in terms of the regret. We considered the case in which the total reward is linear to the individual rewards of selected arms. The analysis in this paper fully uses the independent property of posterior samples and paves the way to obtain a tight analysis on the multiple-play regret that depends on the combinatorial structure of arm selection. We now point out two promising directions for future work.\n• Position-dependent factors for online advertising: it is well-known that the CTR of an ad is dependent on its position. Taking the position-dependent factor into consideration changes the MP-MAB problem from the L-set selection problem to the L-sequence selection problem in which the position of L arms matters. For the starting point, we consider an extension of MP-TS for the cascade model (Kempe & Mahdian, 2008; Aggarwal et al., 2008) that corrects position-dependent bias in Appendix A.2.\n• Non-Bernoulli distributions for general problems: for the ease of argument, we exclusively consider the binary rewards. The analysis by Korda et al. (2013) is useful in extending our result to the case of the 1-d exponential families of rewards. Moreover, extending our result to multi-parameter reward distributions (Burnetas & Katehakis, 1996; Honda & Takemura, 2014) is interesting."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We gratefully acknowledge the insightful advice from Issei Sato and Tao Qin. We thank the anonymous reviewers for their useful comments. This work was supported in part by JSPS KAKENHI Grant Number 26106506."
    }, {
      "heading" : "A. Appendix",
      "text" : "A.1. Cases of several arms having the same expectation\nUp to now, we have assumed that all arms have distinct expectations. Here, we consider cases in which some arms have the same expectations. Without loss of generality, we assume µ1 ≥ µ2 ≥, . . . ,≥ µK . Let us call arms with a larger expectation than µL “strictly optimal” arms, arms with the same expectation as µL “marginal” arms, and arms with a smaller expectation than µL “strictly suboptimal” arms. Each arm is either strictly optimal, marginal, or strictly suboptimal.\nCase 1: Assume that all strictly optimal arms are distinct, that there is only one marginal arm, and that there are several strictly suboptimal arms with the same expectation. In this case, the regret bound of Theorem 1 holds because our analysis deals with each suboptimal arm separately.\nCase 2: Assume that there is only one marginal arm, that all strictly suboptimal arms are distinct, and that there are several strictly optimal arms with the same expectation. The regret bound also holds in this case since there is a gap between each strictly suboptimal arm and each strictly optimal arm.\nCase 3: Assume that all strictly optimal arms and strictly suboptimal arms are distinct and that there are several marginal arms with the same expectation. Unfortunately, we were unable to perform a meaningful analysis in this case. Intuitively, as stated by Agrawal and Goyal (Agrawal & Goyal, 2012) for SP-MAB, adding an additional marginal arm appears to require some extra exploration, which slightly increases the regret. However, the regret structure is more complex than the SP-MAB because several marginal arms can be drawn simultaneously.\nIn summary, our Theorem 1 holds when the marginal arm is distinct. That is, µ1 ≥ µ2 ≥ · · · ≥ µL−1 > µL > µL+1 ≥ · · · ≥ µK .\nA.2. Cascade model and position-dependent MP-MAB problem\nIn the main paper, we assumed that the rewards of arms are independently and identically drawn from individual distributions. In this section, we relax this assumption and consider a wider class of the MP-MAB problem. Remember that, one of our primary applications is multiple advertisement placement in the online advertising problem (c.f., Example 1). In this section, we interchangeably use the terms an advertisement (ad) and an arm. It is known that the CTR of an ad depends on the environment where the ad is placed, especially on the position of the ad. Among several models that explain this dependency on the position, the model that explains human behavior and agrees\nAlgorithm 2 Bias-Corrected Multiple-play Thompson sampling (BC-MP-TS) for binary rewards\nInput: # of armsK, # of positionsL, discount factors {γl(i)} for i = 1, 2, . . . ,K do Ai, Ni = 1, 2 end for t← 1. for t = 1, 2 . . . , T do\nfor i = 1, 2, . . . ,K do Bi ← max (Ni −Ai, 1) θi(t) ∼ Beta(Ai, Bi) end for Select Il(t) (l = 1, . . . , L) in accordance with Section A.2.2. for l ∈ 1, 2, . . . , L do\nif Xi(t) = 1 then Ai ← Ai + 1 end if Ni ← Ni + ∏l l′=2 γl′(Il′−1(t))\nend for end for\nwell with real data (Craswell et al., 2008) is the cascade model (Kempe & Mahdian, 2008; Aggarwal et al., 2008), with which it is assumed that the user scans the ads from top to bottom. Following Gatti et al. (2012), we define the discount factor γl(i) for l ≥ 2 as the probability that a user observing ad i in position l − 1 will observe the ad in the next position. Namely, the MP-MAB problem with a discount factor is defined as a MP-MAB problem in which the arm at position l yields reward 1 with probability (∏l l′=2 γl′(Il′−1(t)) ) µIl(t), where Il(t) be the arm placed at the l-th position at round t. Note that, when we set γl(i) = 1 for any position l ∈ [L] and ad i, this model is reduced to the model we considered in the main paper. In the MP-MAB problem in the main paper, the order of the L arms does not matter. Whereas, under a position-dependent discount factor smaller than 1, the order of L arms matters: the problem is not the selection of an L-set of arms, but an L-sequence of arms."
    }, {
      "heading" : "A.2.1. THOMPSON SAMPLING FOR CASCADE MODEL",
      "text" : "In the cascade model, there is some probability that the arm at position l > 1 is not drawn. The probability that the arm at position l is drawn, ∏l l′=2 γl′(Il′−1(t)), can be considered as the effective number of the draws at position i. MP-TS (Algorithm 1) keeps Ai and Bi, which respectively correspond to the number of rewards 1 and 0. The number of draws on the arm i is Ni = Ai +Bi. When we consider the cascade model, we need to take the effective number of draw into consideration. We introduce Bias-corrected MP-\nTS (BC-MP-TS, Algorithm 2). The crux of BC-MP-TS is that, for each arm that is selected, Ni should be increased not by 1, but by the effective number of draw for each position. Note that, when γl(i) = 1, BC-MP-TS is essentially the same as MP-TS."
    }, {
      "heading" : "A.2.2. OPTIMAL ARM SELECTION AND THE REGRET",
      "text" : "In general discount factor γl(i), even if we have perfect information over the expectation of all arms {µi}Ki=1, the computation of the optimal sequence of L-arms at each round t (optimal arm selection) appears to be computationally intractable when K is large because we need to search all the possible allocation of K ads over L positions. Kempe & Mahdian (2008) proposed a polynomialtime approximation of the optimal arm selection. We can obtain the arm selection strategy for BC-MP-TS by using this approximation algorithm as an oracle and plugging {θi(t)}Li=1 as estimated expected rewards.\nAd-independent discount factor: when the discount factor is independent of the ad at that position (i.e., γl(i) = γl), the optimal arm selection is easy: just select µl (i.e., l-th best arm) on the l-th position. We define the arm selection strategy of BC-MP-TS as placing the arm of the l-th largest θi (i.e., Il(t) = max (l) i∈[K] θi) on the l-th position.\nRegret: naturally, the regret per round is defined as the difference between the expected reward of the optimal arm selection and that of an algorithm. Namely,\nReg(T ) = T∑ t=1 L∑ l=1\n( l∏\nl′=2\nγl′(Iopt(l ′ − 1))µIopt(l)\n− l∏\nl′=2 γl′(Il′−1(t))︸ ︷︷ ︸ effective number of draw at position l ×µIl(t)\n) ,\nwhere (Iopt(1), . . . , Iopt(L)) is the optimal arm selection. In the case of the ad-independent discount factor, we conjecture that the regret lower bound should be identical to the case of no-discount factor that we analysed in the main paper (i.e., inequality (7)). Although we do not prove any regret bound for this cascade model, the conjecture is supported by the fact that (i) by identifying the top-L arm we immediately obtain the optimal arm selection, (ii) algorithms should require log T/d(µi, µL) number of effective draws to convince that suboptimal arm i > L is not as good as arm L, and (iii) the best situation is that the simultaneous draw of several optimal arms rarely occurs: arm L is pushed out instead of arm i, and the regret increase per an effective draw is µL−µi. In the case of the general discount factor, the problem is subtler because a slight difference in {µi} can change the optimal arm selection."
    }, {
      "heading" : "A.2.3. EXPERIMENT OF CASCADE MODEL",
      "text" : "This simulation adapts the cascade model and involves a constant discount factor γl(i) = 0.7 for any position and arm. There are 9 Bernoulli arms with µ1 = 0.24, µ2 = 0.21, . . . , µ9 = 0.00 and L = 3. In this case the optimal arm selection strategy is to choose {I1(t), I2(t), I3(t)} = {µ1, µ2, µ3} (c.f., Section A.2.2). The regret of the algorithms is shown in 4. On one hand, MP-TS failed to have a small regret due to its ignorance to the discount factors. On the other hand, the slope of BC-MP-TS quickly approaches the conjectured Lower Bound, which is empirical evidence of the ability of BC-MP-TS to correct the position-dependent bias."
    }, {
      "heading" : "A.3. Key fact and lemmas",
      "text" : "Fact 5. (Chernoff bound for binary random variables)\nLetX1, . . . , Xn be i.i.d. binary random variables. Let X̂ = 1 n ∑n i=1Xi and µ = E[Xi]. Then, for any ∈ (0, 1− µ),\nPr(X̂ ≥ µ+ ) ≤ exp (−d(µ+ , µ)n).\nand, for any ∈ (0, µ),\nPr(X̂ ≤ µ− ) ≤ exp (−d(µ− , µ)n).\nFact 6. (Beta-Binomial equality) Let F betaα,β (y) be the cdf of the beta distribution with integer parameters α and β. Let FBn,p(·) be the cdf of the binomial distribution with parameters n, p. Then,\nF betaα,β (y) = 1− FBα+β−1,y(α− 1),\nFact 7. (Pinsker’s inequality for binary random variables) For p, q ∈ (0, 1), the KL divergence between two Bernoulli distributions is bounded as:\nd(p, q) ≥ 2(p− q)2.\nLemma 8. (Lemma 2 in Agrawal & Goyal (2013b)) Let k ∈ [K], n ≥ 0 and x < µk. Let µ̂k,n be the empirical average of n samples from Bernoulli(µk). Let pk,n(x) = 1− F betaµ̂k,nn+1,(1−µ̂k,n)n+1(y) be the probability that the posterior sample from the Beta distribution with its parameter µ̂k,nn+ 1, (1− µ̂k,n)n+ 1 exceeds x. Then, its average over runs is bounded as:\nE [ 1\npk,n(x) ] ≤ 1 + 3∆k(x) (n < 8/∆k(x)) 1 + Θ ( e−∆k(x) 2n/2 + 1(n+1)∆k(x)2 e −Dk(x)n\n+ 1 e∆k(x) 2n/4−1\n) (n ≥ 8/∆k(x)),\nwhere ∆k(x) = µk − x,Dk(x) = d(x, µk).\nIn the proof of Lemma 3 we use the following Lemmas 9, 10, and 11 several times. Lemma 9 is essentially the combination of the existing techniques of Agrawal & Goyal (2013b) and Honda & Takemura (2014). Lemmas 10 and 11 are also existing techniques that appear in several previous analyses in Bayesian bandits with Bernoulli arms.\nLemma 9. Let k ∈ [K], z < µk be arbitrary, S(t), T (t), and U(t) be events such that\n(i) if {θk(t) ≥ z}, S(t), and T (t) occurred then the arm k is drawn at round t,\n(ii) θk(t), S(t) and T (t) are mutually independent given {µ̂i(t)}Ki=1 and {Ni(t)}Ki=1.\n(iii) The event U(t) is deterministic given {µ̂i(t)}Ki=1 and {Ni(t)}Ki=1.\n(iv) Given {µ̂i(t)}Ki=1 and {Ni(t)}Ki=1 such that U(t) holds, T (t) occurs with probability at least q > 0."
    }, {
      "heading" : "Then",
      "text" : "E [ T∑ t=1 1{θk(t) < z,S(t),U(t)} ] = O ( 1 q(µk − z)2 ) .\nIn particular, by setting T (t) and U(t) the trivial events that always hold (q = 1), we obtain the following inequality:\nE [ T∑ t=1 1{θk(t) < z,S(t)} ] = O ( 1 (µk − z)2 ) . (17)\nProof. First we have T∑ t=1 1{θk(t) < z,S(t),U(t)}\n≤ T−1∑ n=0 T∑ t=1 1{θk(t) < z,S(t),U(t), Nk(t) = n}\n≤ T−1∑ n=0 T∑ m=1 1\n[ m ≤\nT∑ t=1 1{θk(t) < z,S(t),U(t), Nk(t) = n}\n] .\n(18)\nHere note that the event\nm ≤ T∑ t=1 1{θk(t) < z,S(t),U(t), Nk(t) = n} (19)\nimplies that the event\n{S(t),U(t), Nk(t) = n} (20)\noccurred for at least m rounds and {θk(t) < z} or T c(t) occurred for the first m rounds such that (20) occurred. Thus, by using the mutual independence of {θk(t) < z}, S(t), and T (t), we have\nPr [ m ≤\nT∑ t=1 1{θk(t) < z,S(t),U(t), Nk(t) = n} ∣∣∣∣∣µ̂k,n ]\n≤ (1− pk,n(z)q)m (21)\nand therefore\nE [ T∑ t=1 1{θk(t) < z,S(t),U(t)} ∣∣∣∣∣µ̂k,n ]\n≤ T−1∑ n=0 T∑ m=1 (1− pk,n(z)q)m (by (18) and (21))\n≤ T−1∑ n=0 1− pk,n(z)q pk,n(z)q ≤ 1 q T−1∑ n=0 ( 1 pk,n(z) − 1 ) , (22)\nwhere we used q ≤ 1 in the last transformation. By using Lemma 8, we obtain\nE [ T−1∑ n=0 ( 1 pk,n(z) − 1 )]\n≤ 24 ∆k(z)2 + T−1∑ n=d8/∆k(z)e O\n( e−∆k(z) 2n/2\n+ e−Dk(z)n\n(n+ 1)∆k(z)2 +\n1\ne∆k(z)2n/4 − 1\n) .\n(23)\nBy using the fact thatDk(z) = d(z, µk) = Ω(1/(µk−z)2) (from the Pinsker’s inequality), it is easy to verify that the RHS of (23) is O(1/(µk − z)2). By using these facts, we finally obtain\nE [ T∑ t=1 1{θk(t)<z,S(t),U(t)} ] ≤1 q E [ T−1∑ n=0 ( 1 pk,n(z) −1 )]\n= O\n( 1\nq(µk − z)2\n) ,\nwhich concludes the proof of the lemma.\nLemma 10. (Deviation of empirical averages, Agrawal & Goyal (2013b, Appendix B.1)) Let k ∈ [K] and z > µk be arbitrary. Then,\nE [ ∞∑ t=0 1{Ak(t), µ̂k(t) > z} ] < 1 +\n1\nd(z, µk) .\nLemma 11. (Deviation of Beta posteriors) Let k ∈ [K], x1, x2 ∈ [0, 1] be arbitrary values such that x1 > x2, and n ≥ 1. Then,\nPr(θk(t) ≥ x1|µ̂k(t) ≤ x2, Nk(t) = n) ≤ exp (−d(x2, x1)n).\nProof. Note that, this lemma is essentially the same as the first display in Agrawal & Goyal (2013b, Appendix B.2). While Agrawal & Goyal (2013b) provide a bound for Nk(t) > n, the bound in our lemma is for Nk(t) = n. For the sake of rigor, we write the proof here.\nPr(θj(t) ≥ x1|µ̂j(t) ≤ x2, Nj(t) = n)\n= Pr ( θ ∼ Beta(µ̂j(t)n+ 1, (1− µ̂j(t))n+ 1),\nθ ≥ x1 ∣∣∣∣µ̂j(t) ≤ x2)\n= 1− F betax2n+1,(1−x2)n+1(x1) = FBn+1,x1(x2n)\n(by the Beta-Binomial equality)\n≤ FBn,x1(x2n) ≤ exp (−d(x2, x1)n) (by the Chernoff bound)."
    }, {
      "heading" : "A.4. Proof of Lemma 3",
      "text" : ""
    }, {
      "heading" : "Evaluation of term (A):",
      "text" : "Proof. Here, we prove inequality (13). Recall that\n(A) = T∑ t=1 1{Bc(t)} = T∑ t=1 1{θ∗(t) < µ(−)L }.\nSince θ∗(t) is theL-th largest posterior sample among arms at round t, θ∗(t) < µ(−)L implies that, there exists at least one arm in [L] with its posterior sample smaller than µ(−)L . Namely,\n{θ∗(t) < µ(−)L } ⊂ ⋃ k∈[L] {θk(t) < µ(−)L },\nand therefore\n{θ∗(t) < µ(−)L } = ⋃ k∈[L] {θk(t) < µ(−)L , θ ∗(t) < µ (−) L }\n= ⋃ k∈[L] {θk(t) < µ(−)L ,max j∈[L] (L)θj(t) < µ (−) L }\n⊂ ⋃ k∈[L] {θk(t) < µ(−)L , max j∈[L]\\{k} (L)θj(t) < µ (−) L }.\nBy using the union bound, we obtain\n1{θ∗(t) < µ(−)L } ≤ ∑ k∈[L] 1{θk(t) < µ(−)L , max j∈[L]\\{k} (L)θj(t) < µ (−) L }.\nNote that the event max(L)j∈[L]\\{k} θj(t) < µ (−) L satisfies the condition for the event S(t) in (17) in Lemma 9 with z := µ\n(−) L . Therefore we obtain from Lemma 9 that\nE [ T∑ t=1 1{θ∗(t) < µ(−)L } ]\n= O\n( 1\n(µk − µ(−)L )2\n) = O ( 1\n(µL − µ(−)L )2\n) ,\nwhich concludes the proof of inequality (13)."
    }, {
      "heading" : "Evaluation of term (B):",
      "text" : "Proof. Here, we prove inequality (14). We have,\n(B) = T∑ t=1 1{Ai(t), Cci (t)}\n= T∑ t=1 1  ⋃ j∈[K]\\([L−1]∪{i}) {Ai(t), θ∗∗\\i,j(t) < ν}  =\nT∑ t=1 ∑ j∈[K]\\([L−1]∪{i}) 1 { Ai(t), θ∗∗\\i,j(t) < ν }\n= T∑ t=1 ∑ j∈[K]\\([L−1]∪{i}){\n1 {Ai(t), µ̂i(t)>µL}+1 { Ai(t), µ̂i(t)≤µL, θ∗∗\\i,j(t) < ν }} .\n(24)\nIn the following, we bound the first and the second terms in the inner sum of the last line of (24). From Lemma 10, the first term of (24) is bounded as:\nE [ T∑ t=1 1 {Ai(t), µ̂i(t) > µL} ] ≤ 1 + 1 d(µL, µi) = O(1).\nOn the other hand, the second term of (24) is transformed as: T∑ t=1 1 { Ai(t), µ̂i(t) ≤ µL, θ∗∗\\i,j(t) < ν\n} ≤ 1 d(µL, ν)\n+ T∑ t=1 1 { Ai(t), Ni(t)>\n1\nd(µL, ν) , µ̂i(t)≤µL, θ∗∗\\i,j(t)<ν } ≤ 1 d(µL, ν)\n+ T∑ t=1 1 { Ni(t) >\n1\nd(µL, ν) , µ̂i(t) ≤ µL, θ∗∗\\i,j(t) < ν\n} .\nSince θ∗∗\\i,j(t) is the (L − 1)-th largest posterior sample among arms except for arms i and j, θ∗∗\\i,j(t) < ν indicates that, the number of arms excluding i and j with posterior samples larger than or equal to ν is at most L− 2, and thus at least one arm among [L−1] has its posterior smaller than ν. Namely,\n{θ∗∗\\i,j(t) < ν} = { max l∈[K]\\{i,j} (L−1)θl(t) < ν}\n= ⋃\nk∈[L−1]\n{θk(t) < ν, max l∈[K]\\{i,j} (L−1)θl(t) < ν}\n⊂ ⋃\nk∈[L−1]\n{θk(t) < ν, max l∈[K]\\{i,j,k} (L−1)θl(t) < ν}.\nBy using this, we have\nT∑ t=1 1 { Ni(t) >\n1\nd(µL, ν) , µ̂i(t) ≤ µL, θ∗∗\\i,j(t) < ν\n}\n≤ T∑ t=1 ∑ k∈[L−1] 1 { Ni(t) >\n1\nd(µL, ν) , µ̂i(t) ≤ µL,\nθk(t) < ν, max l∈[K]\\{i,j,k}\n(L−1)θl(t) < ν } .\nHere, z := ν, S(t) := {max(L−1)l∈[K]\\{i,j,k} θl(t) < ν}, T (t) := {θi(t) ≤ ν}, and U(t) := {Ni(t) > 1/d(µL, ν), µ̂i(t) ≤ µL} satisfy the condition in Lemma 9. Under U(t), T (t) holds with probability at least\n1− exp ( −d(µL, ν)(\n1\nd(µL, ν) )\n) = 1− 1/e\nby Lemma 11. Therefore, by using Lemma 9 we obtain\nE [ T∑ t=1 1 { Ni(t) >\n1\nd(µL, ν) , µ̂i(t) ≤ µL,\nθk(t) < ν, max l∈[K]\\{i,j,k}\n(L−1)θl(t) < ν }]\n≤ O (\n1\n(1− 1/e)(µk − ν)2\n) = O(1). (25)\nFrom (25) and the union bound over k ∈ [L−1], the second term of (24) is O(1). In summary, term (B) is O(1) in expectation."
    }, {
      "heading" : "Evaluation of term (C):",
      "text" : "Proof. Here, we prove inequality (15). Recall that, (C) = ∑\nj∈[K]\\([L−1]∪{i}) T∑ t=1 1{Ai(t),Aj(t), Ci(t),Di(t)}.\nLet ν2 = (ν + µL)/2 = (µL−1 + 3µL)/4. Note that, we defined ν and ν2 such that µL−1 > ν > ν2 > µL, O(µL−1 − ν) = O(ν − ν2) = O(ν2 − µL) = O(µL−1 − µL) = O(1) as a function of T . Then,\nT∑ t=1 1{Ai(t),Aj(t), Ci(t),Di(t)}\n= T∑ t=1 1{Ai(t),Aj(t), Ci(t),Di(t), µ̂j(t) > ν2}\n+ T∑ t=1 1{Ai(t),Aj(t), Ci(t),Di(t), µ̂j(t) ≤ ν2}\n≤ T∑ t=1 1{Aj(t), µ̂j(t) > ν2}\n+ T∑ t=1 1{Ai(t),Aj(t), Ci(t),Di(t), µ̂j(t) ≤ ν2}.\n(26)\nBy using Lemma 10 with z := ν2, the first term in (26) is bounded as:\nE [ T∑ t=1 1{Aj(t), µ̂j(t) > ν2} ] ≤ 1 + 1 d(ν2, µj)\n= O\n( 1\n(ν2 − µj)2\n) = O ( 1\n(µL−1 − µL)2\n) = O(1).\n(27)\nWe now bound the second term in (26). Let C′i,j(t) = {θ∗∗\\i,j(t) ≥ ν} ⊃ Ci(t). Let Ej(t) = {Nj(t) ≥ 2 log T}. We have, T∑ t=1 1{Ai(t),Aj(t), Ci(t),Di(t), µ̂j(t) ≤ ν2}\n≤ T∑ t=1 1{Ai(t),Aj(t), C′i,j(t),Di(t), µ̂j(t) ≤ ν2}\n≤ 2 log T\n+ T∑ t=1 1{Ai(t),Aj(t), C′i,j(t),Di(t), µ̂j(t) ≤ ν2, Ej(t)}.\n≤ 2 log T + Nsufi (T )−1∑\nn=0 T∑ t=1\n1{Ai(t),Aj(t), C′i,j(t), Ni(t) = n, µ̂j(t) ≤ ν2, Ej(t)}.\nIn the following, we bound T∑ t=1 1{Ai(t),Aj(t), C′i,j(t), Ni(t) = n, µ̂j(t) ≤ ν2, Ej(t)}. (28) Note that, (28) is at most 1 since {Ai(t), Ni(t) = n} occurs at most once. Let τ be the first round (if exists) at which {C′i,j(t), θ∗∗\\i,j(t) ≤ θi(t),Ai(t), Ni(t) = n} is satisfied. It is necessary that {θj(τ) ≥ θ∗∗\\i,j(τ)} for (28) to be 1: this is because, (i) both θi(τ) and θj(τ) need to be larger than θ∗∗\\i,j(τ) for the simultaneous draw of arms i and j, (ii) and if θj(τ) < θ∗∗\\i,j(τ) then arm i is drawn and thus {Ni(t) = n} is never satisfied after t > τ . Here,\nPr{θj(τ) ≥ θ∗∗\\i,j(τ), θ ∗∗ \\i,j(τ) ≥ ν, µ̂j(τ) ≤ ν2} ≤ exp (−d(ν2, ν)Nj(τ)),\nby Lemma 11. Therefore, we have\nE [ T∑ t=1 1{Ai(t),Aj(t), Ci(t), Ni(t) = n, µ̂j(t) ≤ ν2} ] ≤ exp (−d(ν2, ν) 2 log T ) = T− 2d(ν2,ν). (29)\nIn summary, the second term in (26) is bounded as:\nE [ T∑ t=1 1{Ai(t),Aj(t), Ci(t),Di(t), µ̂j(t) ≤ ν2} ] ≤ 2 log T +N sufi (T )T− 2d(ν2,ν)\n≤ ( 2 + 4T− 2d(ν2,ν)\nd(µi, µL)\n) log T (by (1 + δ)2 < 4),\nand thus,\nE[(C)]\n≤ ∑\nj∈[K]\\([L−1]∪{i})\n(( 2 + 4T − 2d(ν2,ν) ) log T\nd(µi, µL)\n) +O(1)\n≤ ∑\nj∈[K]\\([L−1]∪{i})\n ( 2 + 4T − 2∆2L,L−1/8 ) log T\nd(µi, µL)\n+O(1),\nwhere we used the fact that d(ν2, ν) ≥ 2(ν − ν2)2 = 2 × ((µL−1 − µL)/4)2 in the last transformation."
    }, {
      "heading" : "Evaluation of term (D):",
      "text" : "Proof. Here, we prove inequality (16). We first divide term (D) into two subterms as:\nE[(D)] = E [ T∑ t=1 1{Ai(t),B(t), Ni(t) ≥ N sufi (T )} ]\n≤ E [ T∑ t=1 1{Ai(t),B(t), µ̂i(t) > µ(+)i , Ni(t) ≥ N suf i (T )} ]\n+ E [ T∑ t=1 1{Ai(t),B(t), µ̂i(t) ≤ µ(+)i , Ni(t) ≥ N suf i (T )} ] .\n(30)\nOn one hand, the first term in (30) is bounded as:\nE [ T∑ t=1 1{Ai(t),B(t), µ̂i(t) > µ(+)i , Ni(t) ≥ N suf i (T )} ]\n≤ E [ T∑ t=1 1{Ai(t), µ̂i(t) > µ(+)i } ] ≤ 1 + 1 d(µ\n(+) i , µi)\n(by Lemma 10). (31)\nOn the other hand, each component of the second term of\n(30) is bounded as E [ 1[Ai(t),B(t), µ̂i(t) ≤ µ(+)i , Ni(t) ≥ N suf i (T )] ] ≤ E [ 1[θi(t) ≥ µ(−)L , µ̂i(t) ≤ µ (+) i , Ni(t) ≥ N suf i (T )]\n] = E [ E [ 1[θi(t) ≥ µ(−)L , µ̂i(t) ≤ µ (+) i , Ni(t) ≥ N\nsuf i (T )]∣∣µ̂i(t), Ni(t)]]\n≤ E [ E [ 1[µ̂i(t) ≤ µ(+)i , Ni(t) ≥ N suf i (T )]\nPr[θi(t) ≥ µ(−)L |µ̂i(t), Ni(t)] ∣∣µ̂i(t), Ni(t)]]\n≤ E [ E [ exp(−d(µ(+)i , µ (−) L )N suf i (T )) ∣∣∣µ̂i(t), Ni(t)]] (by Lemma 11)\n= exp (−d(µ(+)i , µ (−) L )N suf i (T )) = T−1 (by the definition of N sufi (T )), (32)\nwhere we used the fact E[X] = E[E[X|Y ]] for any random variables X and Y . Putting (30)–(32) together we obtain\nE[(D)] ≤ 1 + 1 d(µ\n(+) i , µi)\n+ T∑ t=1 T−1, (33)\nfrom which the inequality (16) follows."
    }, {
      "heading" : "A.5. Proof of Lemma 4",
      "text" : "It suffices to prove that for any a, b > 0\ninf 2>0\n{ T−a 2\nb + 2\n} = O ( log log T\nlog T\n) .\nBy letting 2 = (log log T )/(a log T ), we have\ninf 2>0\n{ T−a 2\nb + 2 } = inf 2>0 { e−a 2 log T b + 2 } ≤ e − log log T\nb +\nlog log T\na log T\n= 1\nb log T +\nlog log T\na log T\n= O\n( log log T\nlog T ) and the proof is completed."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>We discuss a multiple-play multi-armed ban-<lb>dit (MAB) problem in which several arms are<lb>selected at each round. Recently, Thompson<lb>sampling (TS), a randomized algorithm with a<lb>Bayesian spirit, has attracted much attention for<lb>its empirically excellent performance, and it is<lb>revealed to have an optimal regret bound in the<lb>standard single-play MAB problem. In this pa-<lb>per, we propose the multiple-play Thompson<lb>sampling (MP-TS) algorithm, an extension of TS<lb>to the multiple-play MAB problem, and discuss<lb>its regret analysis. We prove that MP-TS for bi-<lb>nary rewards has the optimal regret upper bound<lb>that matches the regret lower bound provided<lb>by Anantharam et al. (1987). Therefore, MP-<lb>TS is the first computationally efficient algorithm<lb>with optimal regret. A set of computer simula-<lb>tions was also conducted, which compared MP-<lb>TS with state-of-the-art algorithms. We also pro-<lb>pose a modification of MP-TS, which is shown<lb>to have better empirical performance.",
    "creator" : "LaTeX with hyperref package"
  }
}