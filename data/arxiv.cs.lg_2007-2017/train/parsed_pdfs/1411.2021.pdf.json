{
  "name" : "1411.2021.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Richard Peng", "He Sun", "Luca Zanetti" ],
    "emails" : [ "(rpeng@cc.gatech.edu)", "(h.sun@bristol.ac.uk)", "(luca.zanetti@bristol.ac.uk)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n20 21\nv3 [\ncs .D\nS] 3\n1 Ja\nn 20\n17\nWe also give a nearly-linear time algorithm for partitioning well-clustered graphs based on computing a matrix exponential and approximate nearest neighbor data structures.\nKeywords: graph partitioning, spectral clustering, k-means, heat kernel\n∗A preliminary version of this paper appeared in the 28th Annual Conference on Learning Theory (COLT 2015).\n†Georgia Institute of Technology, Atlanta, USA. (rpeng@cc.gatech.edu) ‡University of Bristol, Bristol, UK. (h.sun@bristol.ac.uk) Questions, comments, or corrections to this doc-\nument may be directed to that email address. §University of Bristol, Bristol, UK. (luca.zanetti@bristol.ac.uk)\nContents"
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3"
    }, {
      "heading" : "2 Preliminaries 4",
      "text" : ""
    }, {
      "heading" : "3 Connection Between Eigenvectors and Indicator Vectors of Clusters 5",
      "text" : ""
    }, {
      "heading" : "4 Analysis of Spectral Clustering 10",
      "text" : "4.1 k-Means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.2 Analysis of the Spectral Embedding . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.3 Approximation Guarantees of Spectral Clustering . . . . . . . . . . . . . . . . . . 13 4.4 Proof of Theorem 4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15"
    }, {
      "heading" : "5 Partitioning Well-Clustered Graphs in Nearly-Linear Time 17",
      "text" : "5.1 The Seeding Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 5.2 The Grouping Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.3 Approximation Analysis of the Algorithm . . . . . . . . . . . . . . . . . . . . . . 24 5.4 Fast computation of the required embedding . . . . . . . . . . . . . . . . . . . . . 25 5.5 Proof of Theorem 1.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27"
    }, {
      "heading" : "1 Introduction",
      "text" : "Partitioning a graph into two or more pieces is one of the most fundamental problems in combinatorial optimization, and has comprehensive applications in various disciplines of computer science.\nOne of the most studied graph partitioning problems is the edge expansion problem, i.e., finding a cut with few crossing edges normalized by the size of the smaller side of the cut. Formally, let G = (V,E) be an undirected graph. For any set S, the conductance of set S is defined by\nφG(S) , |E(S, V \\ S)|\nvol(S) ,\nwhere vol(S) is the total weight of edges incident to vertices in S, and let the conductance of G be\nφ(G) , min S:vol(S)6vol(G)/2 φG(S).\nThe edge expansion problem asks for a set S ⊆ V of vol(S) 6 vol(V )/2 such that φG(S) = φ(G). This problem is known to be NP-hard [26], and the current best approximation algorithm achieves an approximation ratio of O (√ log n ) [5].\nThe k-way partitioning problem is a natural generalization of the edge expansion problem. We call subsets of vertices (i.e. clusters) A1, . . . , Ak a k-way partition of G if Ai ∩ Aj = ∅ for different i and j, and ⋃k i=1 Ai = V . The k-way partitioning problem asks for a k-way partition of G such that the conductance of any Ai in the partition is at most the k-way expansion constant, defined by\nρ(k) , min partition A1,...,Ak max 16i6k φG(Ai). (1.1)\nClusters of low conductance in networks appearing in practice usually capture the notion of community, and algorithms for finding these subsets have applications in various domains such as community detection and network analysis. In computer vision, most image segmentation procedures are based on region-based merge and split [10], which in turn rely on partitioning graphs into multiple subsets [36]. On a theoretical side, decomposing vertex/edge sets into multiple disjoint subsets is used in designing approximation algorithms for Unique Games [39], and efficient algorithms for graph problems [18, 23, 38].\nDespite widespread use of various graph partitioning schemes over the past decades, the quantitative relationship between the k-way expansion constant and the eigenvalues of the graph Laplacians were unknown until a sequence of very recent results [22, 24]. For instance, Lee et al. [22] proved the following higher-order Cheeger inequality:\nλk 2\n6 ρ(k) 6 O(k2) √\nλk, (1.2)\nwhere 0 = λ1 6 . . . 6 λn 6 2 are the eigevalues of the normalized Laplacian matrix L of G. Informally, the higher-order Cheeger inequality shows that a graph G has a k-way partition with low ρ(k) if and only if λk is small. Indeed, (1.2) implies that a large gap between λk+1 and ρ(k) guarantees (i) existence of a k-way partition {Si}ki=1 with bounded φG(Si) 6 ρ(k), and (ii) any (k + 1)-way partition of G contains a subset with significantly higher conductance ρ(k + 1) > λk+1/2 compared with ρ(k). Hence, a suitable lower bound on the gap Υ(k) for some k, defined by\nΥ(k) , λk+1 ρ(k) , (1.3)\nimplies the existence of a k-way partition for which every cluster has low conductance, and that G is a well-clustered graph.\nWe study well-clustered graphs which satisfy a gap assumption on Υ(k) in this paper. Our gap assumption on Υ(k) is slightly weaker than assuming gaps between the eigenvalues,\nbut nonetheless related via Cheeger-type inequalities. Our assumption is also well-grounded in practical studies: clustering algorithms have been studied before under this assumption in machine learning, e.g. [1]. Sharp drop-offs between two consecutive eigenvalues have also been observed to give good indicators for the number of clusters, e.g. [40] and Section D in [14]."
    }, {
      "heading" : "1.1 Our Results",
      "text" : "We give structural results that show close connections between the eigenvectors and the indicator vectors of the clusters. This characterization allows us to show that many variants of spectral clustering, that are based on the spectral embedding and that work “in practice”, can be rigorously analyzed “in theory”. Moreover, exploiting our gap assumption, we can approximate this spectral embedding using the heat kernel of the graph. Combining this with approximate nearest neighbor data structures, we give a nearly-linear time algorithm for the k-way partitioning problem.\nOur structural results can be summarized as follows. Let {fi}ki=1 be the eigenvectors corresponding to the k smallest eigenvalues of L, and {Si}ki=1 be a k-way partition of G achieving ρ(k) defined in (1.1). We define {gi}ki=1 to be the indicator vectors of the clusters {Si}ki=1, where gi(u) = 1 if u ∈ Si, and gi(u) = 0 otherwise. We further use {ḡi}ki=1 to express the normalized indicator vectors of the clusters {Si}ki=1, defined by\nḡi = D1/2gi ‖D1/2gi‖ .\nWe show that, under the condition of Υ(k) = Ω(k2), the span of {ḡi}ki=1 and the span of {fi}ki=1 are close to each other, which is stated formally in Theorem 1.1.\nTheorem 1.1 (The Structure Theorem). Let {Si}ki=1 be a k-way partition of G achieving ρ(k), and let Υ(k) = λk+1/ρ(k) = Ω(k\n2). Let {fi}ki=1 and {ḡi}ki=1 be defined as above. Then, the following statements hold:\n1. For every ḡi, there is a linear combination of {fi}ki=1, called f̂i, such that ‖gi − f̂i‖2 6 1/Υ(k).\n2. For every fi, there is a linear combination of {gi}ki=1, called ĝi, such that ‖fi − ĝi‖2 6 1.1k/Υ(k).\nThis theorem generalizes the result shown by Arora et al. ([2], Theorem 2.2), which proves the easier direction (the first statement, Theorem 1.1), and can be considered as a stronger version of the well-known Davis-Kahan theorem [12]. We remark that, despite that we use the higher-order Cheeger inequality (1.2) to motivate the definition of Υ(k), our proof of the structure theorem is self-contained. Specifically, it omits much of the machinery used in the proofs of higher-order and improved Cheeger inequalities [21, 22].\nThe structure theorem has several applications. For instance, we look at the well-known spectral embedding F : V [G] → Rk defined by\nF (u) , 1\nNormalizationFactor(u) · (f1(u), . . . , fk(u))⊺ , (1.4)\nwhere NormalizationFactor(u) ∈ R is a normalization factor for u ∈ V [G]. We use Theorem 1.1 to show that this well-known spectral embedding exhibits very nice geometric properties: (i) all points F (u) from the same cluster are close to each other, and (ii) most pairs of points F (u), F (v) from different clusters are far from each other; (iii) the bigger the value of Υ(k), the higher concentration the embedded points within the same cluster.\nBased on these facts, we analyze the performance of spectral clustering, aiming at answering the following longstanding open question: Why does spectral clustering perform well in practice?\nWe show that the partition {Ai}ki=1 produced by spectral clustering gives a good approximation of any “optimal” partition {Si}ki=1: every Ai has low conductance, and has large overlap with its corresponding Si. This algorithm has comprehensive applications, and has been the subject of extensive experimental studies for more than 20 years, e.g. [28, 40]. Prior to this work, similar results on spectral clustering mainly focus on graphs generated from the stochastic block model. Instead, our gap assumption captures more general classes of graphs by replacing the input model with a structural condition. Our result represents the first rigorous analysis of spectral clustering for the general family of graphs that exhibit a multi-cut structure but are not captured by the stochastic block model. Our result is as follows:\nTheorem 1.2 (Approximation Guarantee of Spectral Clustering). Let G be a graph satisfying the condition Υ(k) = λk+1/ρ(k) = Ω(k\n3), and k ∈ N. Let F : V [G] → Rk be the embedding defined in (1.4). Let {Ai}ki=1 be a k-way partition by any k-means algorithm running in Rk that achieves an approximation ratio APT. Then, the following statements hold: (i) vol(Ai△Si) = O ( APT · k3/Υ(k) ) vol(Si), and (ii) φG(Ai) = 1.1 · φG(Si) +O ( APT · k3/Υ(k) ) .\nWe further study fast algorithms for partitioning well-clustered graphs. Notice that, for moderately large values of k, e.g. k = ω(log n), directly applying k-means algorithms and Theorem 1.2 does not give a nearly-linear time algorithm, since (i) obtaining the spectral embedding (1.4) requires Ω(mk) time for computing k eigenvectors, and (ii) most k-means algorithms run in Ω(nk) time.\nTo overcome the first obstacle, we study the so-called heat kernel embedding xt : V [G] → Rn, an embedding from V to Rn defined by\nxt(u) , 1 NormalizationFactor(u) · ( e−t·λ1f1(u), · · · , e−t·λnfn(u) )\nfor some t ∈ R>0. The heat kernel of a graph is a well-studied mathematical concept and is related to, for example, the study of random walks [34]. We exploit the heat kernel embedding to approximate the squared-distance ‖F (u)−F (v)‖2 of the embedded points F (u) and F (v) via their heat-kernel distance ‖xt(u)−xt(v)‖2. Since the heat kernel distances between vertices can be approximated in nearly-linear time [29], this approach avoids the computation of eigenvectors for a large value of k. For the second obstacle, instead of applying k-means algorithms as a black-box, we apply approximate nearest-neighbor data structures. This can be viewed as an ad-hoc version of a k-means algorithm, and indicates that in many scenarios the standard Lloydtype heuristic widely used in k-means algorithms can eventually be avoided. Our result is as follows:\nTheorem 1.3 (Nearly-Linear Time Algorithm For Partitioning Graphs). Let G = (V,E) be a graph of n vertices and m edges, and k = ω(log n) be the number of clusters. Assume that Υ(k) = λk+1/ρ(k) = Ω̃(k\n5), and {Si}ki=1 is a k-way partition such that φG(Si) 6 ρ(k). Then there is an algorithm which runs in Õ(m) time and outputs a k-way partition {Ai}ki=1 such that (i) vol(Ai△Si) = Õ ( k4/Υ(k) ) vol(Si), and (ii) φG(Ai) = 1.1 · φG(Si) + Õ ( k4/Υ(k) ) . The Õ(·) and Ω̃(·) terms here hide a factor of poly log n. We remark that bounds of other expansion parameters of k-way partitioning can be derived from our analysis as well. For instance, it is easy to see that ρ(k) and the normalized cut [36] studied in machine learning, which is defined as the sum of the conductance of all returned clusters, differ by at most a factor of k, and the normalized cut value of a k-way partition from spectral clustering can be derived from our results."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "In the broadest sense, our algorithms are clustering routines. Clustering can be formulated in many ways, and the study of algorithms in many such formulations are areas of active work\n[7, 8, 17, 25]. Among these, our work is most closely related to spectral clustering, which is closely related to normalized or low conductance cuts [36]. The k-way expansion that we study is always within a factor of k of k-way normalized cuts.\nTheoretical studies of graph partitioning are often based on augmenting the fractional relaxation of these cut problems with additional constraints in the form of semidefinite programs or Lasserre hierarchy. The goal of our study is to obtain similar bounds using more practical tools such as k-means and heat-kernel embedding.\nOveis Gharan and Trevisan [32] formulate the notion of clusters with respect to the inner and outer conductance: a cluster S should have low outer conductance, and the conductance of the induced subgraph by S should be high. Under a gap assumption between λk+1 and λk, they present a polynomial-time algorithm which finds a k-way partition {Ai}ki=1 that satisfies the inner- and outer-conductance condition. In order to ensure that every Ai has high inner conductance, they assume that λk+1 > poly(k)λ 1/4 k , which is much stronger than ours. Moreover, their algorithm runs in polynomial-time, in contrast to our nearly-linear time algorithm. Dey et al. [13] studies the properties of the spectral embedding for graphs having a gap between λk and λk+1 and presents a k-way partition algorithm, which is based on k-center clustering and is similar in spirit to our work. Using combinatorial arguments, they are able to show that the clusters concentrate around k distant points in the spectral embedding. In contrast to our work, their result only holds for bounded-degree graphs, and cannot provide an approximate guarantee for individual clusters. Moreover, their algorithm runs in nearly-linear time only if k = O(poly log n).\nWe also explore the separation between λk and λk+1 from an algorithmic perspective, and show that this assumption interacts well with heat-kernel embeddings. The heat kernel has been used in previous algorithms on local partitioning [9], balanced separators [29]. It also plays a key role in current efficient approximation algorithms for finding low conductance cuts [30, 35]. However, most of these theoretical guarantees are through the matrix multiplicative weights update framework [3, 4]. Our algorithm instead directly uses the heat-kernel embedding to find low conductance cuts.\nThere is also a considerable amount of research on partitioning random graphs. For instance, in the Stochastic Block Model (SBM) [27], the input graph with k clusters is generated according to probabilities p and q with p > q: an edge between any two vertices within the same cluster is placed with probability p, and an edge between any two vertices from different clusters is placed with probability q. It is proven that spectral algorithms give the correct clustering for certain ranges of p and q [27, 33, 41]. However, the analysis of these algorithms cannot be easily generalized into our setting: we consider graphs where edges are not necessarily chosen independently with certain probabilities, but can be added in an “adversarial” way. For this reason, standard perturbation theorems used in the analysis of algorithms for SBMs, such as the Davis-Kahan theorem [12], cannot be always applied, and ad-hoc arguments specific for graphs, like our structure theorem (Theorem 1.1), become necessary."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Let G = (V,E) be an undirected and unweighted graph with n vertices and m edges. The set of neighbors of a vertex u is represented by N(u), and its degree is du = |N(u)|. For any set S ⊆ V , let vol(S) , ∑u∈S du. For any set S, T ⊆ V , we define E(S, T ) to be the set of edges between S and T , aka E(S, T ) , {{u, v}|u ∈ S and v ∈ T}. For simplicity, we write ∂S = E(S, V \\S) for any set S ⊆ V . For two sets X and Y , the symmetric difference of X and Y is defined as X△Y , (X \\ Y ) ∪ (Y \\X).\nWe work extensively with algebraic objects related to G. We use D to denote the n × n diagonal matrix with Duu = du for u ∈ V [G]. The Laplacian matrix of G is defined by L , D −A, where A is the adjacency matrix of G defined by Au,v = 1 if {u, v} ∈ E[G], and\nAu,v = 0 otherwise. The normalized Laplacian matrix of G is defined by L , D−1/2LD−1/2 = I − D−1/2AD−1/2. For this matrix, we denote its n eigenvalues with 0 = λ1 6 · · · 6 λn 6 2, with their corresponding orthonormal eigenvectors f1, . . . , fn. Note that if G is connected, the first eigenvector is f1 = D 1/2f , where f is any non-zero constant vector.\nFor a vector x ∈ Rn, the Euclidean norm of x is given by ‖x‖ = (∑n\ni=1 x 2 i\n)1/2 . For any\nf : V → R and h , D−1/2f , the Rayleigh quotient of f with respect to graph G is given by\nR(f) , f ⊺Lf ‖f‖2 = h⊺Lh ‖h‖D =\n∑ {u,v}∈E(G) (h(u)− h(v))2∑\nu duh(u) 2\n,\nwhere ‖h‖D , h⊺Dh. Based on the Rayleigh quotient, the conductance of a set Si can be expressed as φG(Si) = R(ḡi), and the gap Υ(k) can be written as\nΥ(k) = λk+1 ρ(k) = min 16i6k λk+1 φG(Si) = min 16i6k λk+1 R(ḡi) . (2.1)\nSince k is always fixed as part of the algorithm’s input, throughout the rest of the paper we always use Υ to express Υ(k) for simplicity. We will also use S1, . . . , Sk to express a k-way partition of G achieving ρ(k). Note that this partition may not be unique."
    }, {
      "heading" : "3 Connection Between Eigenvectors and Indicator Vectors of",
      "text" : "Clusters\nIn this section we study the relations between the multiple cuts of a graph and the eigenvectors of the graph’s normalized Laplacian matrix. Given clusters S1 . . . Sk, define the indicator vector of cluster Si by\ngi(u) = { 1 if u ∈ Si, 0 if u 6∈ Si,\n(3.1)\nand define the corresponding normalized indicator vector by\ngi = D1/2gi\n‖D1/2gi‖ . (3.2)\nA basic result in spectral graph theory states that G has k connected components if and only if the k smallest eigenvalues are 0, implying that the spaces spanned by f1, · · · , fk and ḡ1, · · · , ḡk are the same. Generalizing this result, we expect that these two spaces would be still similar if these k components of G are loosely connected, in the sense that (i) every eigenvector fi can be approximately expressed by a linear combination of {gi}ki=1, and (ii) every indicator vector ḡi can be approximately expressed by a linear combination of {fi}ki=1. This leads to our structure theorem, which is illustrated in fig. 1.\nTheorem 3.1 (The Structure Theorem, Formal Statement). Let Υ = Ω(k2), and 1 6 i 6 k. Then, the following statements hold:\n1. There is a linear combination of the eigenvectors f1, . . . , fk with coefficients α (i) j : f̂i =\nα (i) 1 f1 + · · ·+ α (i) k fk, such that ∥∥∥gi − f̂i ∥∥∥ 2 6 1/Υ.\n2. There is a linear combination of the vectors ḡ1, . . . , ḡk with coefficients β (i) j : ĝi = β (i) 1 ḡ1 +\n· · · + β(i)k ḡk, such that ‖fi − ĝi‖ 2 6 1.1k/Υ.\nPart 1 of Theorem 3.1 shows that the normalized indicator vectors ḡi of every cluster Si can be approximated by a linear combination of the first k eigenvectors, with respect to the value of Υ. The proof follows from the fact that if ḡi has small Rayleigh quotient, then the inner product between ḡi and the eigenvectors corresponding to larger eigenvalues must be small. This statement was also shown implicitly in Theorem 2.2 of [2].\nProof of Part 1 of Theorem 3.1. We write gi as a linear combination of the eigenvectors of L, i.e.\ngi = α (i) 1 f1 + · · ·+ α(i)n fn\nand let the vector f̂i be the projection of vector ḡi on the subspace spanned by {fi}ki=1, i.e.\nf̂i = α (i) 1 f1 + · · ·+ α (i) k fk.\nBy the definition of Rayleigh quotients, we have that\nR(gi) = ( α (i) 1 f1 + · · ·+ α(i)n fn )⊺ L ( α (i) 1 f1 + · · · + α(i)n fn )\n= ( α (i) 1 )2 λ1 + · · · + ( α(i)n )2 λn > ( α (i) 2 )2 λ2 + · · · + ( α (i) k )2 λk + ( 1− α′ − ( α (i) 1 )2) λk+1,\nwhere α′ , ( α (i) 2 )2 + · · ·+ ( α (i) k )2 . Therefore, we have that\n1− α′ − ( α (i) 1 )2 6 R(gi)/λk+1 6 1/Υ,\nand\n‖gi − f̂i‖2 = ( α (i) k+1 )2 + · · ·+ ( α(i)n )2 = 1− α′ − ( α (i) 1 )2 6 1/Υ,\nwhich finishes the proof.\nPart 2 of Theorem 3.1 is more interesting, and shows that the opposite direction holds as well, i.e., any fi (1 6 i 6 k) can be approximated by a linear combination of the normalized indicator vectors {gi}ki=1. To sketch the proof, note that if we could write every gi exactly as a linear combination of {fi}ki=1, then we could write every fi (1 6 i 6 k) as a linear combination of {gi}ki=1. This is because both of {fi}ki=1 and {gi}ki=1 are sets of linearly independent vectors of the same dimension and span {g1, . . . , gk} ⊆ span {f1, . . . , fk}. However, the gi’s are only close to a linear combination of the first k eigenvectors, as shown in Part 1. We will denote this combination as f̂i, and use the fact that the errors of approximation are small to show that these {f̂i}ki=1 are almost orthogonal between each other. This allows us to show that span {f̂1, . . . , f̂k} = span {f1, . . . , fk}, which implies Part 2.\nWe will use the following two classical results in our proof.\nTheorem 3.2 (Geršgorin Circle Theorem). Let A be an n × n matrix , and let Ri(A) =∑ j 6=i |Ai,j|, for 1 6 i 6 n. Then, all eigenvalues of A are in the union of Geršgorin Discs defined by n⋃\ni=1\n{z ∈ C : |z −Ai,i| 6 Ri(A)} .\nTheorem 3.3 (Corollary 6.3.4, [15]). Let A be an n × n real and symmetric matrix with eigenvalues λ1, . . . , λn, and E be an n×n matrix. If λ̂ is an eigenvalue of A+E, then there is some eigenvalue λi of A for which |λ̂− λi| 6 ‖E‖. Proof of Part 2 of Theorem 3.1. By Part 1, every gi is approximated by a vector f̂i defined by\nf̂i = α (i) 1 f1 + · · ·α (i) k fk.\nDefine a k by k matrix A such that Ai,j = α (j) i , i.e., the jth column of matrix A consists of\nvalues { α (j) i }k i=1 representing f̂j. We express the jth column of A by a vector α (j), defined as\nα(j) = ( α (j) 1 , · · · , α (j) k )⊺ .\nWe will show that the vectors { α(j) }k j=1\nare linearly independent, which implies that {f̂ (j)}kj=1 are linearly independent as well. To prove this, we will show that A⊺A has no zero eigenvalue, and hence A is invertible.\nFirst of all, notice that it holds by the orthonormality of {fi}ki=1 that ∣∣∣ 〈 α(i), α(j) 〉∣∣∣ = ∣∣∣ 〈 f̂i, f̂j 〉∣∣∣ = ∣∣∣ 〈 ḡi − (ḡi − f̂i), ḡj − (ḡj − f̂j) 〉∣∣∣\n= ∣∣∣〈ḡi, ḡj〉 − 〈 ḡi − f̂i, ḡj 〉 − 〈 ḡi, ḡj − f̂j 〉 + 〈 ḡi − f̂i, ḡj − f̂j 〉∣∣∣ 6 ∥∥∥ḡi − f̂i ∥∥∥+ ∥∥∥ḡj − f̂j ∥∥∥+ ∥∥∥ḡi − f̂i ∥∥∥ ∥∥∥ḡj − f̂j ∥∥∥ 6 2 √ 1/Υ+ 1/Υ,\nwhere the first inequality follows from the orthonormality of ḡi and ḡj , and the second inequality follows by Part 1 of Theorem 3.1. So it holds for any i 6= j that\n|(A⊺A)i,j| = ∣∣∣∣∣ k∑\nℓ=1\nAℓ,iAℓ,j ∣∣∣∣∣ = ∣∣∣∣∣ k∑\nℓ=1\nα (i) ℓ α (j) ℓ ∣∣∣∣∣ = ∣∣∣ 〈 α(i), α(j) 〉∣∣∣ 6 3 √ 1/Υ\nwhile\n(A⊺A)i,i = k∑\nℓ=1\n( α (i) ℓ )2 > 1− 1/Υ.\nThen, by the Geršgorin Circle Theorem (cf. Theorem 3.2), it holds that all the eigenvalues of A⊺A are at least\n1− 1/Υ − (k − 1) · 3 √ 1/Υ.\nTherefore, A has no eigenvalue with value 0 as long as Υ > 10k2, proving that the vectors{ α(j)\n}k j=1\nare linearly independent. Combining this with the fact that span {f̂1, . . . , f̂k} ⊆ span {f1, . . . , fk} and dim(span ({f1, . . . , fk})) = k, it holds that\nspan {f̂1, . . . , f̂k} = span {f1, . . . , fk}.\nHence, we can write every fi (1 6 i 6 k) as a linear combination of {f̂i}ki=1, i.e.,\nfi = β (i) 1 f̂1 + β (i) 2 f̂2 + · · ·+ β (i) k f̂k. (3.3)\nNow define the value of ĝi as\nĝi = β (i) 1 g1 + β (i) 2 g2 + · · ·+ β (i) k gk, (3.4)\nand define ‖β‖2 = ∑kj=1 ( β (i) j )2 . Then, it holds that\n1 = ‖fi‖2 = k∑\nℓ=1\n( β (i) ℓ )2 ∥∥∥f̂ℓ ∥∥∥ 2 + ∑\nℓ 6=ℓ′ β (i) ℓ β (i) ℓ′\n〈 f̂ℓ, f̂ℓ′ 〉\n> ‖β‖2(1− 1/Υ)− ∑\nℓ\n∣∣∣β(i)ℓ ∣∣∣ ∑\nℓ′ 6=ℓ\n∣∣∣β(i)ℓ′ ∣∣∣ 〈 f̂ℓ, f̂ℓ′ 〉\n> ‖β‖2(1− 1/Υ)− (√ k · ‖β‖ ) · (√ k · ‖β‖ ) · ( 3 · √ 1/Υ ) > ( 1− 1/Υ − 3k/ √ Υ ) ‖β‖2,\nwhere the second inequality holds by the Cauchy-Schwarz inequality. Since Υ = Ω(k2), we have that\n‖β‖2 6 ( 1− 1\nΥ − 3k√\nΥ\n)−1 6 1.1.\nCombining this with Part 1 of Theorem 3.1 and the Cauchy-Schwarz inequality, we have that\n‖fi − ĝi‖ 6 k∑\nj=1\n∣∣∣β(i)j ∣∣∣ ∥∥∥f̂j − gj ∥∥∥ 6 ( 1/ √ Υ ) k∑\nj=1\n∣∣∣β(i)j ∣∣∣ 6 √ 1.1k/Υ,\nwhich proves Part 2 of the theorem.\nTheorem 3.1 shows a close connection between the first k eigenvectors and the indicator vectors of the clusters. We leverage this and the fact that the {ĝi}’s are almost orthogonal between each other to show that, for any two different clusters Si and Sj, there exists an eigenvector having reasonably different values on the coordinates which correspond to Si and Sj.\nLemma 3.4. Let Υ = Ω(k3). For any 1 6 i 6 k, let ĝi = β (i) 1 g1 + · · · + β (i) k gk be such that ‖fi − ĝi‖ 6 1.1k/Υ. Then, for any ℓ 6= j, there exists i ∈ {1, . . . , k} such that ∣∣∣β(i)ℓ − β (i) j ∣∣∣ > ζ , 1\n10 √ k . (3.5)\nProof. Let β(i) = ( β (i) 1 , . . . , β (i) k )⊺ , for 1 6 i 6 k. Since ḡi ⊥ ḡj for any i 6= j, we have by the orthonormality of g1, · · · , gk that\n〈ĝi, ĝj〉 = 〈 β (i) 1 g1 + · · ·+ β (i) k gk, β (j) 1 g1 + · · ·+ β (j) k gk 〉\n=\nk∑\nℓ=1\nβ (i) ℓ β (j) ℓ ‖gℓ‖2 =\n〈 β(i), β(j) 〉 ,\nand ∣∣∣ 〈 β(i), β(j)\n〉∣∣∣ = |〈ĝi, ĝj〉| = |〈fi − (fi − ĝi), fj − (fj − ĝj)〉| = |〈fi, fj〉 − 〈fi − ĝi, fj〉 − 〈fj − ĝj , fi〉+ 〈fi − ĝi, fj − ĝj〉| 6 ‖fi − ĝi‖+ ‖fj − ĝj‖+ ‖fi − ĝi‖‖fj − ĝj‖ 6 2.2 √ k/Υ + 1.1k/Υ.\nMoreover, it holds that\n∥∥∥β(i) ∥∥∥ = ‖ĝi‖ = ‖fi + ĝi − fi‖ 6 1 + ‖ĝi − fi‖ 6 1 + √ 1.1k/Υ,\nand ∥∥∥β(i) ∥∥∥ = ‖ĝi‖ = ‖fi + ĝi − fi‖ > 1− ‖ĝi − fi‖ > 1− √ 1.1k/Υ,\nwhich implies that\n∥∥∥β(i) ∥∥∥ 2 ∈ ( 1− (2.2 √ k/Υ + 1.1k/Υ), 1 + 2.2 √ k/Υ+ 1.1k/Υ ) . (3.6)\nIn other words, we showed that β(i)’s are almost orthonormal. Now we construct a k by k matrix B, where the jth column of B is β(j). By the Geršgorin Circle Theorem (Theorem 3.2), all eigenvalues λ of B⊺B satisfies\n|λ− (B⊺B)i,i| 6 (k − 1) · (2.2 √ k/Υ+ 1.1k/Υ) (3.7)\nfor any i. Combing this with (3.6), we have that the eigenvalues of B⊺B are close to 1.\nNow we show that β (i) ℓ and β (i) j are far from each other by contradiction. Suppose there\nexist ℓ 6= j such that ζ ′ , max\n16i6k\n∣∣∣β(i)ℓ − β (i) j ∣∣∣ < 1\n10 √ k .\nThis implies that the jth row and ℓth row of matrix B are somewhat close to each other. Let us now define matrix E ∈ Rk×k, where\nEℓ,i , β (i) j − β (i) ℓ ,\nand Et,i = 0 for any t 6= ℓ and 1 6 i 6 k. Moreover, let\nQ = B+E.\nNotice that Q has two identical rows, and rank at most k − 1. Therefore, Q has an eigenvalue with value 0, and the spectral norm ‖E‖ of E, the largest singular value of E, is at most √ kζ ′. By definition of matrix Q we have that\nQ⊺Q = B⊺B+B⊺E+E⊺B+E⊺E.\nSince B⊺B is symmetric and 0 is an eigenvalue of Q⊺Q, by Theorem 3.3 we know that, if λ̂ is an eigenvalue of Q⊺Q, then there is an eigenvalue λ of B⊺B such that\n|λ̂− λ| 6 ‖B⊺E+E⊺B+E⊺E‖ 6 ‖B⊺E‖+ ‖E⊺B‖+ ‖E⊺E‖ 6 4 √ kζ ′ + kζ ′2,\nwhich implies that\nλ̂ > λ− 4 √ kζ ′ − kζ ′2 > 1− k(2.2 √ k/Υ+ 1.1k/Υ) − 4 √ kζ ′ − kζ ′2,\ndue to (3.6) and (3.7). By setting λ̂ = 0, we have that\n1− k(2.2 √ k/Υ + 1.1k/Υ) − 4 √ kζ ′ − kζ ′2 6 0.\nBy the condition of Υ = Ω(k3), the inequality above implies that ζ ′ > 1 10 √ k , which leads to a contradiction.\nWe point out that it was shown in [21] that the first k eigenvectors can be approximated by a (2k + 1)-step function. The quality of the approximation is the same as the one given by our structure theorem. However, a (2k + 1)-step approximation is not enough to show that most vertices belonging to the same cluster are mapped close to each other in the spectral embedding.\nWe further point out that standard matrix perturbation theorems cannot be applied in our setting. For instance, we look at a well-clustered graph G that contains a subset C of a cluster Si such that most neighbors of vertices in C are outside Si. In this case, the adjacency matrix representing crossing edges of G has high spectral norm, and hence standard matrix perturbation arguments could not give us a meaningful result. However, our structure theorem takes the fact that vol(C) has to be small into account, and that is why the structure theorem is needed to analyze the cut structure of a graph."
    }, {
      "heading" : "4 Analysis of Spectral Clustering",
      "text" : "In this section we analyze an algorithm based on the classical spectral clustering paradigm, and give an approximation guarantee of this method on well-clustered graphs. We will show that any k-means algorithm AlgoMean(X , k) with certain approximation guarantee can be used for the k-way partitioning problem. Furthermore, it suffices to call AlgoMean in a black-box manner with a point set X ⊆ Rk.\nThis section is structured as follows. We first give a quick overview of spectral and k-means clustering in Section 4.1. In Section 4.2, we use the structure theorem to analyze the spectral embedding. Section 4.3 gives a general result about k-means when applied to this embedding, and the proof of Theorem 1.2.\n4.1 k-Means Clustering\nGiven a set of points X ⊆ Rd, a k-means algorithm AlgoMean(X , k) seeks to find a set K of k centers c1, · · · , ck to minimize the sum of the ℓ22-distance between x ∈ X and the center to which it is assigned. Formally, for any partition X1, · · · ,Xk of the set X ⊆ Rd, we define the cost function by\nCOST(X1, . . . ,Xk) , min c1,...,ck∈Rd\nk∑\ni=1\n∑ x∈Xi ‖x− ci‖2,\ni.e., the COST function minimizes the total ℓ22-distance between the points x’s and their individually closest center ci, where c1, . . . , ck are chosen arbitrarily in R\nd. We further define the optimal clustering cost by\n∆2k(X ) , min partition X1,...,Xk COST(X1, . . . ,Xk). (4.1)\nSpectral clustering can be described as follows: (i) Compute the bottom k eigenvectors f1, · · · , fk of the normalized Laplacian matrix1 of graph G. (ii) Map every vertex u ∈ V [G] to a point F (u) ∈ Rk according to\nF (u) = 1\nNormalizationFactor(u) · (f1(u), . . . , fk(u))⊺ , (4.2)\nwith a proper normalization factor NormalizationFactor(u) ∈ R for each u ∈ V . (iii) Let X , {F (u) : u ∈ V } be the set of the embedded points from vertices in G. Run AlgoMean(X , k), and group the vertices of G into k clusters according to the output of AlgoMean(X , k). This\n1Other graph matrices (e.g. the adjacency matrix, and the Laplacian matrix) are also widely used in practice. Notice that, with proper normalization, the choice of these matrices does not substantially influence the performance of k-means algorithms.\napproach that combines a k-means algorithm with a spectral embedding has been widely used in practice for a long time, although there was a lack of rigorous analysis of its performance prior to our result."
    }, {
      "heading" : "4.2 Analysis of the Spectral Embedding",
      "text" : "The first step of spectral clustering is to map vertices of a graph into points in Euclidean space, through the spectral embedding (4.2). This subsection analyzes the properties of this embedding. Let us define the normalization factor to be\nNormalizationFactor(u) , √\ndu.\nWe will show that the embedding (4.2) with the normalization factor above has very nice properties: embedded points from the same cluster Si are concentrated around their center ci ∈ Rk, and embedded points from different clusters of G are far from each other. These properties imply that a simple k-means algorithm is able to produce a good clustering2.\nWe first define k points p(i) ∈ Rk (1 6 i 6 k), where\np(i) , 1√\nvol (Si)\n( β (1) i , . . . , β (k) i )⊺ (4.3)\nand the parameters {β(j)i }kj=1 are defined in Theorem 3.1. We will show in Theorem 4.1 that all embedded points Xi , {F (u) : u ∈ Si} (1 6 i 6 k) are concentrated around p(i). Moreover, we bound the total ℓ22-distance between points in Xi and p(i), which is proportional to 1/Υ: the bigger the value of Υ, the higher concentration the points within the same cluster have. Notice that we do not claim that p(i) is the actual center of Xi. However, these approximated points p(i)’s suffice for our analysis.\nLemma 4.1. It holds that\nk∑\ni=1\n∑ u∈Si du ∥∥∥F (u)− p(i) ∥∥∥ 2 6 1.1k2/Υ.\nProof. Since ĝj(u) = √\ndu vol(Si) β (j) i and p (i) j = 1√ vol(Si) β (j) i hold for any 1 6 j 6 k and u ∈ Si by\ndefinition, we have that\nk∑\ni=1\n∑ u∈Si du ( F (u)j − p(i)j )2 = k∑ i=1 ∑ u∈Si du ( 1√ du fj(u)− 1√ vol(Si) β (j) i )2\n=\nk∑\ni=1\n∑\nu∈Si\n( fj(u)− √ du\nvol(Si) β (j) i\n)2\n=\nk∑\ni=1\n∑ u∈Si (fj(u)− ĝj(u))2\n= ‖fj − ĝj‖2\n6 1.1k/Υ,\nwhere the last inequality follows from Theorem 3.1. Summing over all j for 1 6 j 6 k implies that\nk∑\ni=1\n∑ u∈Si du ∥∥∥F (u)− p(i) ∥∥∥ 2 = k∑ i=1 k∑ j=1 ∑ u∈Si du ( F (u)j − p(i)j )2 6 1.1k2/Υ.\n2Notice that this embedding is similar with the one used in [22], with the only difference that F (u) is not normalized and so it is not necessarily a unit vector. This difference, though, is crucial for our analysis.\nThe next lemma shows that the ℓ22-norm of p (i) is inversely proportional to the volume of Si. This implies that embedded points from a big cluster are close to the origin, while embedded points from a small cluster are far from the origin.\nLemma 4.2. It holds for every 1 6 i 6 k that\n99\n100 vol(Si) 6\n∥∥∥p(i) ∥∥∥ 2 6\n101\n100 vol(Si) .\nProof. By (4.3), we have that\n∥∥∥p(i) ∥∥∥ 2 =\n1\nvol(Si)\n∥∥∥ ( β (1) i , . . . , β (k) i )⊺∥∥∥ 2 .\nNotice that p(i) is just the ith row of the matrix B defined in the proof of Theorem 3.4, normalized by √ vol(Si). Since B and B\n⊺ share the same singular values (this follows from the SVD decomposition), by (3.7) the eigenvalues of BB⊺ are close to 1. But since (BB⊺)i,i is equal to the ℓ22-norm of the ith row of B, we have that\n∥∥∥ ( β (1) i , . . . , β (k) i )⊺∥∥∥ 2 ∈ ( 1− (2.2 √ k/Υ + 1.1k/Υ), 1 + 2.2 √ k/Υ+ 1.1k/Υ ) , (4.4)\nwhich implies the statement.\nWe will further show in Theorem 4.3 that these points p(i)(1 6 i 6 k) exhibit another excellent property: the distance between p(i) and p(j) is inversely proportional to the volume of the smaller cluster between Si and Sj. Therefore, points in Si of smaller vol(Si) are far from points in Sj of bigger vol(Sj). Notice that, if this were not the case, a misclassification of a small fraction of points in Sj could introduce a large error to Si.\nLemma 4.3. For every i 6= j, it holds that ∥∥∥p(i) − p(j) ∥∥∥ 2 >\nζ2\n10min {vol(Si), vol(Sj)} ,\nwhere ζ is defined in (3.5).\nProof. Let Si and Sj be two arbitrary clusters. By Theorem 3.4, there exists 1 6 ℓ 6 k such that ∣∣∣β(ℓ)i − β (ℓ) j ∣∣∣ > ζ.\nBy the definition of p(i) and p(j) it follows that\n∣∣∣∣∣ ∣∣∣∣∣ p(i) ‖p(i)‖ − p(j) ‖p(j)‖ ∣∣∣∣∣ ∣∣∣∣∣ 2 >   β (ℓ) i√\n∑k t=1 ( β (t) i\n)2 − β (ℓ) j√\n∑k t=1 ( β (t) j )2\n  2 .\nBy (4.4), we know that\n√√√√ k∑\nℓ=1\n( β (ℓ) j )2 = ∥∥∥ ( β (1) j , . . . , β (k) j )⊺∥∥∥ ∈ ( 1− ζ\n10 , 1 +\nζ\n10\n) .\nTherefore, we have that\n∣∣∣∣∣ ∣∣∣∣∣ p(i) ‖p(i)‖ − p(j) ‖p(j)‖ ∣∣∣∣∣ ∣∣∣∣∣ 2 > 1 2 · ( β (ℓ) i − β (ℓ) j )2 > 1 2 · ζ2,\nand 〈 p(i)\n‖p(i)‖ , p(j) ‖p(j)‖\n〉 6 1− ζ2/4.\nWithout loss of generality, we assume that ∥∥p(i) ∥∥2 > ∥∥p(j) ∥∥2. By Theorem 4.2, it holds that ∥∥∥p(i) ∥∥∥ 2 > 9\n10 · vol(Si) ,\nand ∥∥∥p(i) ∥∥∥ 2 > ∥∥∥p(j) ∥∥∥ 2 >\n9\n10 · vol(Sj) .\nHence, it holds that ∥∥∥p(i) ∥∥∥ 2 >\n9\n10min {vol(Si), vol(Sj)} .\nWe can now finish the proof by considering two cases based on ∥∥p(i) ∥∥. Case 1: Suppose that ∥∥p(i) ∥∥ > 4 ∥∥p(j) ∥∥. We have that\n∥∥∥p(i) − p(j) ∥∥∥ > ∥∥∥p(i) ∥∥∥− ∥∥∥p(j) ∥∥∥ > 3\n4\n∥∥∥p(i) ∥∥∥ ,\nwhich implies that\n∥∥∥p(i) − p(j) ∥∥∥ 2 > 9\n16\n∥∥∥p(i) ∥∥∥ 2 >\n1\n2min {vol(Si), vol(Sj)} .\nCase 2: Suppose ∥∥p(j) ∥∥ = α ∥∥p(i) ∥∥ for α ∈ (14 , 1]. In this case, we have that\n∥∥∥p(i) − p(j) ∥∥∥ 2 = ∥∥∥p(i) ∥∥∥ 2 + ∥∥∥p(j) ∥∥∥ 2 − 2\n〈 p(i)\n‖p(i)‖ , p(j) ‖p(j)‖\n〉∥∥∥p(i) ∥∥∥ ∥∥∥p(j) ∥∥∥\n> ∥∥∥p(i) ∥∥∥ 2 + ∥∥∥p(j) ∥∥∥ 2 − 2(1− ζ2/4) · ∥∥∥p(i) ∥∥∥ ∥∥∥p(j) ∥∥∥ = (1 + α2) ∥∥∥p(i) ∥∥∥ 2 − 2(1− ζ2/4)α · ∥∥∥p(i) ∥∥∥ 2 = (1 + α2 − 2α + αζ2/2) · ∥∥∥p(i) ∥∥∥ 2\n> αζ2 2 · ∥∥∥p(i) ∥∥∥ 2 > ζ2 · 1 10min {vol(Si), vol(Sj)} ,\nand the lemma follows."
    }, {
      "heading" : "4.3 Approximation Guarantees of Spectral Clustering",
      "text" : "Now we analyze why spectral clustering performs well for solving the k-way partitioning problem. We assume that A1, . . . , Ak is any k-way partition returned by a k-means algorithm with an approximation ratio of APT.\nWe map every vertex u to du identical points in R k. This “trick” allows us to bound the volume of the overlap between the clusters retrieved by a k-means algorithm and the optimal ones. For this reason we define the cost function of partition A1, . . . , Ak of V [G] by\nCOST(A1, . . . , Ak) , min c1,...,ck∈Rk\nk∑\ni=1\n∑\nu∈Ai du‖F (u)− ci‖2,\nand the optimal clustering cost is defined by\n∆2k , min partition A1,...,Ak COST(A1, . . . , Ak).\ni.e., we define the optimal clustering cost in the same way as in (4.1), except that we look at the embedded points from vertices of G in the definition. From now on, we always refer COST and ∆2k as the COST and optimal COST values of points {F (u)}u∈V , and for technical reasons every point is counted du times. The next lemma gives an upper bound to the cost of the optimal k-means clustering which depends on the gap Υ\nLemma 4.4. It holds that ∆2k 6 1.1k 2/Υ.\nProof. Since ∆2k is obtained by minimizing over all partitions A1, . . . , Ak and c1, . . . , ck, we have\n∆2k 6 k∑\ni=1\n∑ u∈Si du ∥∥∥F (u) − p(i) ∥∥∥ 2 . (4.5)\nHence the statement follows by applying Theorem 4.1.\nSince A1, · · · , Ak is the output of a k-means algorithm with approximation ratio APT, by Theorem 4.4 we have that COST(A1, . . . , Ak) 6 APT · 1.1k2/Υ. We will show that this upper bound of APT · 1.1k2/Υ suffices to show that this approximate clustering A1, . . . , Ak is close to the “actual” clustering S1, . . . , Sk, in the sense that (i) every Ai has low conductance, and (ii) under a proper permutation σ : {1, . . . , k} → {1, . . . , k}, the symmetric difference between Ai and Sσ(i) is small. The fact is proven by contradiction: If we could always find a set Ai with high symmetric difference with its correspondence Sσ(i), regardless of how we map {Ai} to their corresponding {Sσ(i)}, then the COST value will be high, which contradicts to the fact that COST(A1, . . . , Ak) 6 APT · 1.1k2/Υ. The core of of the whole contradiction arguments is the following technical lemma, whose proof will be presented in the next subsection.\nLemma 4.5. Let A1, . . . , Ak be a partition of V . Suppose that, for every permutation of the indices σ : {1, . . . , k} → {1, . . . , k}, there exists i such that vol ( Ai△Sσ(i) ) > 2ε vol ( Sσ(i) ) for ε > 105 · k3/Υ, then COST(A1, . . . , Ak) > 10−4 · ε/k.\nProof of Theorem 1.2. Let A1, . . . , Ak be a k-way partition that achieves an approximation ratio of APT, and let\nε = 2 · 105 · k3 · APT\nΥ .\nWe first show that there exists a permutation σ of the indices such that\nvol ( Ai△Sσ(i) ) 6 ε vol(Sσ(i)), for any 1 6 i 6 k. (4.6)\nAssume for contradiction that for all permutation σ there is 1 6 i 6 k such that\nvol(Ai△Sσ(i)) > ε vol ( Sσ(i) ) .\nThis implies by Theorem 4.5 that\nCOST(A1, . . . , Ak) > 10 · APT · k2/Υ,\nwhich contradicts to the fact that A1, . . . , Ak is an APT-approximation to a k-way partition, whose corresponding k-means cost is at most 1.1 · APT · k2/Υ.\nNow we assume that σ : {1, · · · , k} → {1, · · · , k} is the permutation satisfying (4.6), and bound the conductance of every cluster Ai. For any 1 6 i 6 k, the number of leaving edges of Ai is upper bounded by\n|∂ (Ai)| 6 ∣∣∂ ( Ai \\ Sσ(i) )∣∣+ ∣∣∂ ( Ai ∩ Sσ(i) )∣∣\n6 ∣∣∂ ( Ai△Sσ(i) )∣∣+ ∣∣∂ ( Ai ∩ Sσ(i) )∣∣ .\nNotice that ∣∣∂ ( Ai△Sσ(i) )∣∣ 6 ε vol ( Sσ(i) ) by our assumption on σ, and every node in ∣∣∂ ( Ai ∩ Sσ(i) )∣∣ either belongs to ∂Sσ(i) \\ Sσ(i) or ∂ ( Ai△Sσ(i) ) , hence\n|∂ (Ai)| 6 ε vol ( Sσ(i) ) + φG ( Sσ(i) ) vol ( Sσ(i) ) + ε vol ( Sσ(i) )\n= ( 2ε + φG ( Sσ(i) )) vol(Sσ(i)).\nOn the other hand, we have that\nvol (Ai) > vol ( Ai ∩ Sσ(i) ) > (1− 2ε) vol(Sσ(i)).\nHence,\nφG(Ai) 6 (2ε+ φG(Sσ(i))) vol(Sσ(i))\n(1− 2ε) vol(Sσ(i))\n= 2ε+ φG(Sσ(i)) 1− 2ε 6 1.1 · φG(Sσ(i)) +O(APT · k3/Υ)."
    }, {
      "heading" : "4.4 Proof of Theorem 4.5",
      "text" : "It remains to show Theorem 4.5. Our proof is based on the following high-level idea: suppose by contradiction that there is a cluster Sj which is very different from every cluster Aℓ. Then there is a cluster Ai with significant overlap with two different clusters Sj and Sj′ (Theorem 4.6). However, we already proved in Theorem 4.3 that any two clusters are far from each other. This implies that the COST value of A1, . . . , Ak is high, which leads to a contradiction.\nLemma 4.6. Suppose for every permutation π : {1, . . . , k} → {1, . . . , k} there exists an index i such that vol ( Ai△Sπ(i) ) > 2ε vol ( Sπ(i) ) . Then, at least one of the following two cases holds:\n1. for any index i there are indices i1 6= i2 and εi > 0 such that\nvol(Ai ∩ Si1) > vol(Ai ∩ Si2) > εi min {vol(Si1), vol(Si2)},\nand ∑k\ni=1 εi > ε;\n2. there exist indices i, ℓ and εj > 0 such that, for j 6= ℓ,\nvol(Ai ∩ Sℓ) > εi vol(Sℓ), vol(Ai ∩ Sj) > εi vol(Sℓ)\nand ∑k\ni=1 εi > ε.\nProof. Let σ : {1, . . . , k} → {1, . . . , k} be the function defined by\nσ(i) = argmax 16j6k vol(Ai ∩ Sj) vol(Sj) .\nWe first assume that σ is one-to-one, i.e. σ is a permutation. By the hypothesis of the lemma, there exists an index i such that vol(Ai△Sσ(i)) > 2ε vol(Sσ(i)). Without loss of generality, we assume that i = 1 and σ(j) = j for j = 1, . . . , k. Notice that\nvol (A1△S1) = ∑\nj 6=1 vol (Aj ∩ S1) +\n∑ j 6=1 vol (A1 ∩ Sj) . (4.7)\nHence, one of the summations on the right hand side of (4.7) is at least ε vol (S1). Now the proof is based on the case distinction.\nCase 1: Assume that ∑\nj 6=1 vol (Aj ∩ S1) > ε vol(S1). We define τj for 2 6 j 6 k to be\nτj = vol (Aj ∩ S1)\nvol (S1) .\nWe have that ∑\nj 6=1 τj > ε,\nand by the definition of σ it holds that\nvol (Aj ∩ Sj) vol (Sj) > vol (Aj ∩ S1) vol (S1) = τj\nfor 2 6 j 6 k. Setting εj = τj for 2 6 j 6 k and ε1 = 0 finishes the proof of Case 1. Case 2: Assume that ∑\nj 6=1 vol (A1 ∩ Sj) > ε vol(S1). (4.8)\nLet us define τ ′j for 1 6 j 6 k, j 6= 1, to be\nτ ′j = vol(A1 ∩ Sj)\nvol (S1) .\nThen, (4.8) implies that ∑\nj 6=1 τ ′j > ε.\nThe statement in this case holds by assuming vol (A1 ∩ S1) > ε vol (S1), since otherwise we have\nvol (S1)− vol (A1 ∩ S1) = ∑\nj 6=1 vol (Aj ∩ S1) > (1− ε) vol (S1) > ε vol (S1) ,\nand this case was proven in Case 1. So it suffices to study the case in which σ defined earlier is not one-to-one. Then, there is j (1 6 j 6 k) such that j 6∈ {σ(1), · · · , σ(k)}. For any 1 6 ℓ 6 k, let\nτ ′′ℓ = vol(Aℓ ∩ Sj)\nvol(Sj) .\nThen, ∑k\nℓ=1 τ ′′ ℓ = 1 > ε and it holds for any 1 6 ℓ 6 k that\nvol ( Aℓ ∩ Sσ(ℓ) )\nvol ( Sσ(ℓ) ) > vol(Aℓ ∩ Sj) vol(Sj) = τ ′′ℓ .\nProof of Theorem 4.5. We first consider the case when part 1 of Theorem 4.6 holds, i.e., for every i there exist i1 6= i2 such that\nvol(Ai ∩ Si1) > εimin {vol(Si1), vol(Si2)}, vol(Ai ∩ Si2) > εimin {vol(Si1), vol(Si2)},\n(4.9)\nfor some ε > 0, and ∑k\ni=1 εi > ε. Let ci be the center of Ai. Let us assume without loss of generality that ‖ci − p(i1)‖ > ‖ci−p(i2)‖, which implies ‖p(i1)−ci‖ > ‖p(i1)−p(i2)‖/2. However, points in Bi = Ai∩Si1 are far away from ci, see fig. 2. We lower bound the value of COST(A1, . . . , Ak) by only looking at the\ncontribution of points in the Bis . Notice that by Theorem 4.1 the sum of the squared-distances between points in Bi and p\n(i1) is at most k2/Υ, while the distance between p(i1) and p(i2) is large (Theorem 4.3). Therefore, we have that\nCOST(A1, . . . , Ak) = k∑\ni=1\n∑\nu∈Ai du‖F (u)− ci‖2 >\nk∑\ni=1\n∑\nu∈Bi du‖F (u) − ci‖2\nBy applying the inequality a2 + b2 > (a− b)2/2, we have that\nCOST(A1, . . . , Ak) > k∑\ni=1\n∑\nu∈Bi du\n(∥∥p(i1) − ci ∥∥2\n2 −\n∥∥∥F (u)− p(i1) ∥∥∥ 2 )\n> k∑\ni=1\n∑\nu∈Bi du\n∥∥p(i1) − ci ∥∥2\n2 −\nk∑\ni=1\n∑\nu∈Bi du\n∥∥∥F (u)− p(i1) ∥∥∥ 2\n> k∑\ni=1\n∑\nu∈Bi du\n∥∥p(i1) − ci ∥∥2\n2 − 1.1k\n2\nΥ (4.10)\n> k∑\ni=1\n∑\nu∈Bi du\n∥∥p(i1) − p(i2) ∥∥2\n8 − 1.1k\n2\nΥ\n> k∑\ni=1\nζ2 vol(Bi)\n80min {vol(Si1), vol(Si2)} − 1.1k\n2\nΥ (4.11)\n> k∑\ni=1\nζ2εi min {vol(Si1), vol(Si2)} 80min {vol(Si1), vol(Si2)} − 1.1k 2 Υ\n> k∑\ni=1\nζ2εi 80 − 1.1k 2 Υ\n> ζ2ε 80 − 1.1k 2 Υ > ζ2ε 100\nwhere (4.10) follows from Theorem 4.1, (4.11) follows from Theorem 4.3 and the last inequality follows from the assumption that ε > 105 · k3/Υ.\nNow, suppose that part 2 of Theorem 4.6 holds, i.e. there are indices i, ℓ such that, for any j 6= ℓ, it holds that\nvol(Ai ∩ Sℓ) > εi vol(Sℓ), vol(Ai ∩ Sj) > εi vol(Sℓ)\nfor some ε > 0, and ∑k\ni=1 εi > ε. In this case, we only need to repeat the proof by setting, for any j 6= i, Bj = Ai ∩ Sj , Sj1 = Sℓ, and Sj2 = Sj."
    }, {
      "heading" : "5 Partitioning Well-Clustered Graphs in Nearly-Linear Time",
      "text" : "In this section we present a nearly-linear time algorithm for partitioning well-clustered graphs, and prove Theorem 1.3. At a high level, our algorithm follows the general framework of k-means algorithms, and consists of two steps: the seeding step, and the grouping step. The seeding step chooses k candidate centers such that each one is close to the actual center of a different\ncluster. The grouping step assigns the remaining vertices to their individual closest candidate centers.\nAll the proofs for the seeding and grouping steps assume that we have an embedding {x(u)}u∈V [G] satisfying the following two conditions:\n( 1− 1\n10 log n\n) · ‖F (u)‖2 6‖x(u)‖2 6 ‖F (u)‖2 + 1\nn5 , (5.1)\n( 1− 1\n10 log n\n) · ‖F (u) − F (v)‖2 6‖x(u)− x(v)‖2 6 ‖F (u) − F (v)‖2 + 1\nn5 (5.2)\nNotice that these two conditions hold trivially if {x(u)}u∈V [G] is the spectral embedding {F (u)}u∈V [G], or any embedding produced by good approximations of the first k eigenvectors. However, obtaining such embedding becomes non-trivial for a large value of k, as directly computing the first k eigenvectors takes super-linear time. We will present a nearly-linear time algorithm that computes an embedding satisfying (5.1) and (5.2). By using standard dimensionality reduction techniques that approximately preserve pairwise distances, such as the Johnson-Lindenstrauss transform (see e.g. [11]), we can also always assume that the dimension of the embedding {x(u)}u∈V [G] is d = O(log3 n). Throughout the whole section, we assume k = ω(poly log n) and Υ = Ω̃(k5).\nThis section is organized as follows: Section 5.1 and Section 5.2 discuss the seeding and grouping steps, assuming that we have an embedding {x(u)}u∈V [G] that satisfies (5.1) and (5.2), and Section 5.3 analyzes the approximation guarantee of the partition returned by the grouping step. In Section 5.4, we present an algorithm that computes all required quantities in nearly-linear time, assuming that we know the value of λk. This assumption on λk will be finally removed in Section 5.5, and this leads to our final algorithm which corresponds to Theorem 1.3."
    }, {
      "heading" : "5.1 The Seeding Step",
      "text" : "We proved in Section 4.2 that the approximate center p(i) for every 1 6 i 6 k satisfies\n99\n100 vol(Si) 6\n∥∥∥p(i) ∥∥∥ 2 6\n101\n100 vol(Si) ,\nand most embedded points F (u) are close to their approximate centers. Together with (5.1) and (5.2), these two properties imply that, when sampling points x(u) with probability proportional to du · ‖x(u)‖2, vertices from different clusters will be approximately sampled with the same\nprobability. We will prove that, when sampling Θ(k log k) points in this way, with constant probability there is at least one point sampled from each cluster.\nIn the next step remove the sampled points which are close to each other, and call this resulting set C⋆. We prove that with constant probability there is exactly one point in C⋆ from a cluster. Algorithm 1 below gives a formal description of the seeding step.\nAlgorithm 1 SeedAndTrim(k, {x(u)}u∈V [G]) 1: input: the number of clusters k, and the embedding {x(u)}u∈V [G]. 2: Let K = Θ(k log k). 3: for i = 1, . . . ,K do 4: Set ci = u with probability proportional to du‖x(u)‖2. 5: end for\n6: for i = 2, . . . ,K do 7: Delete all cj with j < i such that ‖x(ci)− x(cj)‖2 < ‖x(ci)‖ 2\n2·104k . 8: end for 9: return the remaining sampled vertices.\nNow we analyze Algorithm 1. For any 1 6 i 6 k, we define Ei to be the sum of the ℓ22-distance between the embedded points from Si and p (i), i.e.,\nEi , ∑\nu∈Si du\n∥∥∥F (u)− p(i) ∥∥∥ 2 .\nFor any parameter ρ > 0, we define the radius of Si with respect to ρ to be\nRρi , ρ · Ei vol(Si) ,\nand define COREρi ⊆ Si to be the set of vertices whose ℓ22-distance to p(i) is at most R ρ i , i.e.,\nCORE ρ i , { u ∈ Si : ∥∥∥F (u)− p(i) ∥∥∥ 2 6 Rρi } . (5.3)\nBy the averaging argument it holds that\nvol(Si \\ COREρi ) 6 ∑ u∈Si du ∥∥F (u)− p(i) ∥∥2\nRρi =\nvol(Si)\nρ ,\nand\nvol(COREρi ) > max\n{( 1− 1\nρ\n) vol(Si), 0 } . (5.4)\nFrom now on, we set the parameter\nα , Θ(K logK).\nWe first show that most embedded points of the vertices in Si are contained in the cores CORE α i , for 1 6 i 6 k.\nLemma 5.1. The following statements hold:\n1. ∑\nu∈COREαi du · ‖F (u)‖ 2 > 1− 1100K .\n2. ∑k\ni=1 ∑ u/∈COREαi du · ‖F (u)‖ 2 6 k100K .\nProof. By the definition of COREαi , it holds that\n∑\nu∈COREαi\ndu · ‖F (u)‖2\n> 1\nα\n∫ α\n0\n∑\nu∈COREρi\ndu · ‖F (u)‖2dρ\n> 1\nα\n∫ α\n0\n(∥∥∥p(i) ∥∥∥− √ Rρi )2 vol(COREρi )dρ (5.5)\n> 1\nα\n∫ α\n0\n(∥∥∥p(i) ∥∥∥ 2 − 2 √ Rρi · ∥∥∥p(i) ∥∥∥ ) max {( 1− 1\nρ\n) vol(Si), 0 } dρ (5.6)\n> 1\nα\n∫ α\n0 max\n{( 1− (2.2 √ k/Υ+ 1.1k/Υ) − 3 √ Eiρ )( 1− 1\nρ\n) , 0 } dρ (5.7)\nwhere (5.5) follows from the fact that for all u ∈ COREρi , ‖F (u)‖ > ‖p(i)‖ − √\nRρi , (5.6) from (5.4), and (5.7) from the definition of Rρi and the fact that\n∥∥∥p(i) ∥∥∥ 2 · vol(Si) ∈ ( 1− (2.2 √ k/Υ + 1.1k/Υ), 1 + 2.2 √ k/Υ+ 1.1k/Υ ) .\nSince Ei 6 1.1k2/Υ by Theorem 4.1, it holds that ∑\nu∈COREαi\ndu · ‖F (u)‖2 > 1\nα\n∫ α\n0 max\n{( 1− (2.2 √ k/Υ + 1.1k/Υ) − 4 √ k2ρ/Υ )( 1− 1\nρ\n) , 0 } dρ\n> 1\nα\n∫ α\n0 max\n{ 1− (2.2 √ k/Υ+ 1.1k/Υ) − 4 √ k2ρ/Υ − 1\nρ , 0\n} dρ\n> 1− (2.2 √ k/Υ + 1.1k/Υ) − 4k √\nα/Υ − lnα α\n> 1− 1 100K ,\nwhere the last inequality holds by the assumption on α and Υ. The second statement follows by the fact that\nk∑\ni=1\n∑\nu∈COREαi\ndu · ‖F (u)‖2 > k ( 1− 1\n100K\n)\nand ∑\nu∈V [G] du · ‖F (u)‖2 = k.\nThe next lemma shows that the embedded points from the same core are close to each other, while the embedded points from different cores are far from each other.\nLemma 5.2. The following statements hold:\n1. For any 1 6 i 6 k and any two vertices u, v ∈ COREαi , it holds that\n‖x(u)− x(v)‖2 6 min { 11αk2\nΥvol(Si) , ‖x(u)‖2 2 · 104 · k\n} .\n2. For any i 6= j, and u ∈ COREαi , v ∈ COREαj , it holds that\n‖x(u)− x(v)‖2 > 1 7000k vol(Si) > ‖x(u)‖2 104k .\nProof. By the definition of COREαi , it holds for any u ∈ COREαi that ∥∥∥F (u)− p(i) ∥∥∥ 6 √\nRαi .\nBy the triangle inequality, it holds for any u ∈ COREαi and v ∈ COREαi that ‖F (u) − F (v)‖ 6 2 √\nRαi , and\n‖F (u)− F (v)‖2 6 4Rαi = 4αEi vol(Si) 6 5αk2 Υvol(Si) ,\nwhere the last inequality follows from Theorem 4.1. Hence, by (5.2) it holds that\n‖x(u)− x(v)‖2 6 ‖F (u) − F (v)‖2 + 1 n5 6 5αk2 Υvol(Si) + 1 n5 6 11αk2 Υvol(Si) ,\nwhere we use the fact that 1 n5 ≪ 1vol(Si) . On the other hand, we have that\n‖F (u)‖2 > (∥∥∥p(i) ∥∥∥− √\nRαi\n)2 >\n9\n10 vol(Si) ,\nwhere the last inequality follow from Theorem 4.2 and the definition of Rαi . By (5.1) and the conditions on α, Υ, it also holds\n‖x(u)− x(v)‖2 6 5αk 2\nΥvol(Si) +\n1\nn5 6\n10αk2 Υ ‖F (u)‖2 6 ‖x(u)‖ 2 2 · 104 · k .\nWith these we proved the first statement. Now for the second statement. By the triangle inequality, it holds for any pair of u ∈ COREαi and v ∈ COREαj that\n‖F (u) − F (v)‖ > ∥∥∥p(i) − p(j) ∥∥∥− ∥∥∥F (u)− p(i) ∥∥∥− ∥∥∥F (v)− p(j) ∥∥∥ .\nBy Theorem 4.3, we have for any i 6= j that ∥∥∥p(i) − p(j) ∥∥∥ 2 >\n1\n103kmin {vol(Si), vol(Sj)} .\nCombining this with the fact that\n∥∥∥F (u)− p(i) ∥∥∥ 6 √ Rαi 6\n√ 1.1αk2\nΥvol(Si) ,\nwe obtain that\n‖F (u) − F (v)‖ > ∥∥∥p(i) − p(j) ∥∥∥− ∥∥∥F (u)− p(i) ∥∥∥− ∥∥∥F (v)− p(j) ∥∥∥\n>\n√ 1\n103kmin {vol(Si), vol(Sj)} −\n√ 1.1αk2\nΥvol(Si) −\n√ 1.1αk2\nΥvol(Sj)\n>\n√ 1\n1.1 · 103kmin {vol(Si), vol(Sj)} .\nNotice that\n‖x(u)‖2 6 ‖F (u)‖2 + 1 n5\n6 (∥∥∥p(i) ∥∥∥+ √\nRαi\n)2 + 1\nn5 6\n11\n10 vol(Si) +\n1\nn5 6\n11\n9 vol(Si) ,\ntherefore we have\n‖x(u)− x(v)‖2 > ( 1− 1\n10 log n\n) ‖F (u)− F (v)‖2 > 1\n7000k vol(Si) > ‖x(u)‖2 104k .\nWe next show that, after sampling Θ(k log k) vertices, with constant probability the sampled vertices are in the cores ⋃k i=1 CORE α i , and every core contains at least one sampled vertex.\nLemma 5.3. Assume that K = Ω(k log k) vertices are sampled, in which each vertex is sampled with probability proportional to du · ‖x(u)‖2. Then, with constant probability the set C = {c1 . . . cK} of sampled vertices satisfies the following properties:\n1. Set C only contains vertices from the cores, i.e. C ⊆ ⋃ki=1 COREαi ;\n2. Set C contains at least one vertex from each cluster, i.e. C ∩ Si 6= ∅ for any 1 6 i 6 k.\nProof. By (5.1), it holds for every vertex u that\n( 1− 1\n10 log n\n) · ‖F (u)‖2 6 ‖x(u)‖2 6 ‖F (u)‖2 + 1\nn5 .\nSince ∑\nu∈V [G] du‖F (u)‖2 = k, it holds that\n∑\nu∈V [G] du‖x(u)‖2 6\n∑\nu∈V [G] du ·\n( ‖F (u)‖2 + 1\nn5\n) 6 k + 1,\nand ∑\nu∈V [G] du‖x(u)‖2 >\n∑\nu∈V [G] du ·\n( 1− 1\n10 log n\n) · ‖F (u)‖2 > ( 1− 1\n10 log n\n) · k,\ni.e., the total probability mass that we use to sample vertices, i.e. ∑\nu∈V [G] du‖x(u)‖2, is between( 1− 110 logn ) · k and k + 1.\nWe first bound the probability that we sample at least one vertex from every core. For any fixed 1 6 i 6 k, the probability that a vertex from COREαi gets sampled is at least\n∑ u∈COREαi du · ‖x(u)‖ 2\nk + 1 >\n∑ u∈COREαi du · ‖F (u)‖ 2\n3(k + 1) > 1− 1100K 3 · (k + 1) > 1 10k .\nTherefore, the probability that we never encounter a vertex from COREαi after sampling K\nvertices is at most ( 1− 110k )K 6 110k . Also, the probability that a sampled vertex is outside the cores of the clusters is at most\n∑k i=1 ∑ u∈Si\\COREαi du · ‖x(u)‖ 2\n( 1− 110 logn ) · k\n6\n∑k i=1 ∑ u∈Si\\COREαi du · ( ‖F (u)‖2 + n−5 )\nk/2\n6 k 100K + n −3\nk/2 6\n2\n100K +\n1\nn2 .\nTaking a union bound over all these events gives that the total probability of undesired events is at most\nk · 1 10k\n+K · ( 1\nn2 +\n2\n100K\n) 6 1\n3 .\nBased on Theorem 5.2 and Theorem 5.3, we can simply delete one of the two vertices ci and cj whose distance is less than 10\n−4 ·‖x(ci)‖2/(2k). The following lemma presents the correctness and runtime of the procedure SeedAndTrim, i.e., Algorithm 1.\nLemma 5.4. Given the embedding {x(u)}u∈V [G] of dimension d = O(log3 n) that satisfies (5.1) and (5.2), with constant probability the procedure SeedAndTrim returns a set C⋆ of centers c1 . . . ck in Õ(n+ k 2) time, such that each COREαi contains exactly one vertex in C ⋆.\nProof. Since the sampled set C contains at least one vertex from each core COREαi with constant probability, and only vertices from different cores will remain in C⋆ by Theorem 5.2 and the algorithm description, the SeedAndTrim procedure returns k centers with constant probability.\nNow we analyze the runtime. The procedure takes Õ(n) time to compute the norms of {x(u)}u∈V [G], since the embedding has dimension O(log3 n) by assumption. It takes Õ(k) time to sample Õ(k) vertices, and trimming the sampling vertices takes Õ(k2) time. Hence, the total runtime is Õ(n + k2) .\nAs the end of this subsection, we would like to mention that choosing good candidate centers is crucial for most k-means algorithms, and has been studied extensively in the literature (e.g. [6, 31]). Comparing with recent algorithms that obtain good initial centers by iteratively picking points from a non-uniform distribution and take Ω(nk) time, our seeding step (Algorithm 1) runs in Õ(n+ k2) time."
    }, {
      "heading" : "5.2 The Grouping Step",
      "text" : "After the seeding step, with constant probability we obtain a set of k vertices C⋆ = {c1, · · · , ck}, and these k vertices belong to k different clusters. Now we assign each remaining vertex u to a cluster Si if, comparing with all other points x(cj) with cj ∈ C⋆, x(u) is closer to x(ci). A naive implementation of this step requires Ω̃(nk) time. To speed it up, we apply ε-approximate nearest neighbor data structures (ε-NNS) [16], whose formal description is as follows:\nProblem 1 (ε-approximate nearest neighbor problem). Given a set of point P ⊂ Rd and a point q ∈ Rd, find a point p ∈ P such that, for all p′ ∈ P , ‖p − q‖ 6 (1 + ε)‖p′ − q‖.\nTheorem 5.5 ([16]). Given a set P of points in Rd, there is an algorithm that solves the ε-approximate nearest neighbor problem with\nÕ ( |P |1+ 11+ε + d · |P | )\npreprocessing time and Õ ( d · |P | 11+ε ) query time.\nNow we set P = {x(c1), . . . , x(ck)}, and apply the above ε-approximate nearest neighbor data structures to assign the remaining vertices to k clusters A1, · · · , Ak. By Theorem 5.5 and setting ε = log k − 1, this step can be finished with Õ(k) preprocessing time and Õ(1) query time for each query. Hence, the runtime of the grouping step is Õ(n). Notice that, with our choice of ε = log k− 1 and application of ε-NNS, all the remaining vertices in V \\C⋆ might not assign to the cluster Ai with the closest center ci. We will prove in the next subsection that our choice of ε suffices to obtain a good approximation of the optimal partition. The runtime of the grouping step, and the properties of the returned clusters are summarized in the following lemma:\nLemma 5.6. Given a set of centers C⋆ = {c1, . . . , ck}, the grouping step runs in Õ(n) time and returns a partition A1, . . . , Ak of vertices, such that for any i ∈ {1, . . . , k}, and every u ∈ Ai, it holds for any j 6= i that\n‖x(u)− x(ci)‖ 6 log k · ‖x(u) − x(cj)‖.\nProof. The statement follows from the definition of ε-NNS with the choice of ε = log k− 1, and Theorem 5.5."
    }, {
      "heading" : "5.3 Approximation Analysis of the Algorithm",
      "text" : "Now we study the approximation ratio of the k-way partition computed by the seeding and grouping steps. The next lemma analyzes the symmetric difference between the optimal partition and the output of the algorithm.\nLemma 5.7. Let A1, . . . , Ak be the output of the grouping procedure. Then, under a proper permutation of the indices, with constant probability for any 1 6 i 6 k it holds that (i) vol(Ai△Si) = Õ ( k3/Υ ) vol(Si), and (ii) φG(Ai) = 1.1 · φG(Si) + Õ ( k3/Υ ) .\nProof. We assume that c1, . . . , ck ∈ V are the centers returned by SeedAndTrim, and {x(u)}u∈V [G] is the embedding we used in the algorithm. Moreover, {x(u)}u∈V [G] satisfies (5.1) and (5.2). We further assume that these sampled c1, . . . , ck ⊆ ⋃k i=1 CORE α i . By Theorem 5.3, this holds with constant probability, and we assume that this event happens in the following analysis. Then, by the second statement of Theorem 5.2 it holds for any i 6= j that\n‖x(ci)− x(cj)‖2 = Ω (\n1\nk ·min{vol(Si), vol(Sj)}\n) . (5.8)\nBy Theorem 5.6, it holds for any 1 6 i 6 k that\nvol(Si \\ Ai) 6 ∑\ni 6=j vol\n({ v ∈ Si : ‖x(ci)− x(v)‖ >\n‖x(cj)− x(v)‖ log k\n})\n6 ∑\ni 6=j vol\n({ v ∈ Si : ‖x(ci)− x(v)‖ >\n‖x(ci)− x(cj)‖ − ‖x(ci)− x(v)‖ log k\n})\n6 ∑\ni 6=j vol\n({ v ∈ Si : 2‖x(ci)− x(v)‖ >\n‖x(ci)− x(cj)‖ log k\n})\n= ∑\ni 6=j vol\n({ v ∈ Si : ‖x(ci)− x(v)‖ >\n‖x(ci)− x(cj)‖ 2 log k\n}) .\nBy (5.2) and the triangle inequality, we have that\n‖x(ci)− x(v)‖ 6 ‖F (ci)− F (v)‖ + 1\nn2.5 6\n∥∥∥F (ci)− p(i) ∥∥∥+ ∥∥∥p(i) − F (v) ∥∥∥ + 1\nn2.5 ,\nand hence\nvol(Si \\ Ai) 6 ∑\ni 6=j vol\n({ v ∈ Si : ∥∥∥F (ci)− p(i) ∥∥∥+ ∥∥∥p(i) − F (v) ∥∥∥+ 1\nn2.5 > ‖x(ci)− x(cj)‖ 2 log k\n})\n6 ∑\ni 6=j vol\n({ v ∈ Si : ∥∥∥p(i) − F (v) ∥∥∥ >\n‖x(ci)− x(cj)‖ 2 log k\n− ∥∥∥F (ci)− p(i) ∥∥∥− 1 n2.5\n})\n6 ∑\ni 6=j vol\n({ v ∈ Si : ∥∥∥p(i) − F (v) ∥∥∥ >\n‖x(ci)− x(cj)‖ 2 log k − √ Rαi − 1 n2.5\n})\n6 ∑\ni 6=j vol\n({ v ∈ Si : ∥∥∥p(i) − F (v) ∥∥∥ 2 = Ω ( 1\nk log2 k ·min{vol(Si), vol(Sj)}\n)})\n= Õ ( k3/Υ ) vol(Si),\nwhere the last equality follows from Theorem 4.1. For the same reason, we have\nvol(Ai \\ Si) 6 ∑\ni 6=j vol\n({ v ∈ Sj : ‖x(cj)− x(v)‖ >\n‖x(ci)− x(v)‖ log k\n})\n= Õ ( k3/Υ ) vol(Si),\nand therefore\nvol(Si△Ai) = vol(Si \\Ai) + vol(Ai \\ Si) = Õ ( k3/Υ ) vol(Si).\nThis yields the first statement of the lemma. The second statement follows by the same argument used in proving Theorem 1.2."
    }, {
      "heading" : "5.4 Fast computation of the required embedding",
      "text" : "So far we assumed the existence of the embedding {x(u)}u∈V [G] satisfying (5.1) and (5.2), and analyzed the performance of the seeding and grouping steps. In this subsection, we will present a nearly-linear time algorithm to compute all the required distances used in the seeding and grouping steps. Our algorithm is based on the so-called heat kernal of a graph.\nFormally, the heat kernel of G with parameter t > 0 is defined by\nHt , e −tL =\nn∑\ni=1\ne−tλifif ⊺ i . (5.9)\nWe view the heat kernel as a geometric embedding from V [G] to Rn defined by\nxt(u) , 1√ du\n· ( e−t·λ1f1(u), · · · , e−t·λnfn(u) ) , (5.10)\nand define the ℓ22-distance between the points xt(u) and xt(v) by\nηt(u, v) , ‖xt(u)− xt(v)‖2. (5.11)\nThe following lemma shows that, when k = Ω(log n) and Υ = Ω(k3), the values of ηt(u, v) for all edges {u, v} ∈ E[G] can be approximately computed in Õ(m) time.\nLemma 5.8. Let k = Ω(log n) and Υ = Ω(k3). Then, there is t = O(poly(n)) such that the embedding {xt(u)}u∈V [G] defined in (5.10) satisfies (5.1) and (5.2). Moreover, the values of ηt(u, v) for all {u, v} ∈ E[G] can be approximately computed in Õ(m) time, such that with high probability the conditions (5.1) and (5.2) hold for all edges {u, v} ∈ E[G].\nOur proof of Theorem 5.8 uses the algorithm for approximating the matrix exponential in [29] as a subroutine, whose performance is summarised in Theorem 5.9. Recall that any n× n real and symmetric matrix A is diagonally dominant (SDD), if Aii > ∑ j 6=i |Aij| for each i = 1, . . . , n. It is easy to see that the Laplacian matrix of any undirected graph is diagonally dominant.\nTheorem 5.9 ([29]). Given an n×n SDD matrix A with mA nonzero entries, a vector v and a parameter δ > 0, there is an algorithm that can compute a vector x such that ‖e−Av−x‖ 6 δ‖v‖ in time Õ((mA + n) log(2 + ‖A‖)), where the Õ(·) notation hides poly log n and poly log(1/δ) factors.\nProof of Theorem 5.8. By the higher-order Cheeger inequality (1.2), we have that\nΥ = λk+1 ρ(k) 6 2λk+1 λk .\nSince k = Ω(log n) and Υ = Ω(k3), it holds that 400 · log2 n 6 λk+1/λk, and there is t such that\nt ∈ ( 10 · log n λk+1 , 1 20 · λk · log n ) .\nWe first show that the embedding {xt(u)}u∈V [G] with this t satisfies (5.1) and (5.2). By the definition of ηt(u, v), we have that\nηt(u, v) = n∑\ni=1\ne−2tλi ( fi(u)√ du − fi(v)√ dv )2\n=\nk∑\ni=1\ne−2tλi ( fi(u)√ du − fi(v)√ dv )2 + n∑\ni=k+1\ne−2tλi ( fi(u)√ du − fi(v)√ dv )2 . (5.12)\nNotice that it holds for 1 6 i 6 k that\n1− 1 10 log n 6 e−1/(10 logn) 6 e−λi/(10λk/ logn) 6 e−2tλi 6 1, (5.13)\nand it holds for k + 1 6 i 6 n that\ne−2t·λi 6 e−2λi·10 logn/λk+1 6 e−10 lognλk+1/λk+1 = 1\nn20 . (5.14)\nCombining (5.12), (5.13), and (5.14), it holds for any {u, v} ∈ E[G] that ( 1− 1\n10 · log n\n) · ‖F (u)− F (v)‖2 6 ηt(u, v) 6 ‖F (u)− F (v)‖2 + 1\nn5 ,\nwhich proves the first statement. Now we show that the distances of ‖xt(u) − xt(v)‖ for all edges {u, v} ∈ E[G] can be approximately computed in nearly-linear time. For any vertex u ∈ V [G], we define ξu ∈ Rn, where (ξu)v = 1/ √ du if v = u, and (ξu)v = 0 otherwise. Combining (5.9) with (5.10) and (5.11), we have that ηt(u, v) = ‖Ht (ξu − ξv)‖2. We define Z to be the operator of error δ which corresponds to the algorithm described in Theorem 5.9, and replacing Ht with Z we get\n∣∣∣‖Z (ξu − ξv)‖ − η1/2t (u, v) ∣∣∣ 6 δ ‖ξu − ξv‖ 6 δ,\nwhere the last inequality follows from du, dv > 1. Hence, it holds that\nη 1/2 t (u, v) − δ 6 ‖Z (ξu − ξv)‖ 6 η 1/2 t (u, v) + δ. (5.15)\nBy applying the Johnson-Lindenstrauss transform in a way analogous to the computation of effective resistances (e.g. [20] and [37]), we obtain an O(ε−2 · log n) × n Gaussian matrix Q, such that with high probability it holds for all u, v that\n(1− ε) ‖Z (ξu − ξv)‖ 6 ‖QZ (ξu − ξv)‖ 6 (1 + ε) ‖Z (ξu − ξv)‖ . (5.16) Combining (5.15) and (5.16) gives us that\n(1− ε) ( η 1/2 t (u, v) − δ ) 6 ‖QZ (ξu − ξv)‖ 6 (1 + ε) ( η 1/2 t (u, v) + δ ) .\nSquaring both sides and invoking the inequality (1− ε)α2 − (1+ ε−1)b2 6 (a+ b)2 6 (1+ ε)α2 + (1 + ε−1)b2 gives\n(1− 5ε) ηt(u, v) − 2δ2ε−1 6 ‖QZ (ξu − ξv)‖2 6 (1 + 5ε) ηt(u, v) + 2δ2ε−1\nScaling QZ by a factor of (1 + 5ε)−1, and appending an extra entry in each vector to create an additive distortion of 2δε−1 then gives the desired bounds when δ is set to εn−6. To satisfy the conditions (5.1) and (5.2) we just need to set ε = O(1/ log n).\nTo analyze the runtime of computing ‖QZ (ξu − ξv)‖2 for all edges {u, v} ∈ E[G], notice that Q has only O(log3 n) rows. We can then run the approximate exponential algorithm from [29] O(log3 n) times, where each time we use a different row of Q as input. Since ‖L‖ 6 2, by Theorem 5.9 we can compute QZ in Õ(m) time. Notice that QZξu is some column of QZ after rescaling, therefore we can compute all the required distances in time Õ(m).\nWe remark that the proof above shows an interesting property about the embedding (5.10), i.e., for a large value of k and a certain condition on Υ, there is always a t such that the values of ηt(u, v) gives a good approximation of ‖F (u) − F (v)‖2 for all edges {u, v} ∈ E[G]. A similar intuition which views the heat kernel embedding as a weighted combination of multiple eigenvectors was discussed in [29]."
    }, {
      "heading" : "5.5 Proof of Theorem 1.3",
      "text" : "We proved in Section 5.4 that if k = Ω(log n) and Υ = Ω(k3), there is a\nt ∈ ( 10 log n\nλk+1 ,\n1\n20 · λk · log n\n) (5.17)\nsuch that {xt(u)}u∈V [G] satisfies the conditions (5.1) and (5.2). Moreover, the values of ‖xt(u)− xt(v)‖ for {u, v} ∈ E[G] can be approximately computed in nearly-linear time3. However, it is unclear how to approximate λk, and without this approximation. Furthermore, without this approximation of λk, obtaining the desired embedding {x(u)}u∈V [G] becomes highly non-trivial.\nTo overcome this obstacle, we run the seeding and grouping steps for all possible t of the form 2i, where t ∈ N>0, as it allows us to run the seeding and grouping steps with the right values of t at some point. However, by (5.11) the distance between any pair of embedded vertices decreases when we increase the value of t. Moreover, all these embedded points {xt(u)}u∈V [G] tend to “concentrate” around a single point for an arbitrary large value of t. To avoid this situation, for every possible t we compute the value of ∑ v∈V [G] dv‖xt(v)‖2, and the algorithm only moves to the next iteration if\n∑\nv∈V [G] dv‖xt(v)‖2 > k\n( 1− 2\nlog n\n) . (5.18)\nBy Theorem 5.1, (5.18) is satisfied for all values of t in the right range (5.17), and the algorithm will not terminate before t = ⌊log n/λk+1⌋. See Algorithm 2 for the formal description of our final algorithm.\nAlgorithm 2 A nearly-linear time graph clustering algorithm, k = Ω(log n)\n1: input: the input graph G, and the number of clusters k 2: Let t = 2. 3: repeat 4: Let (c1, . . . , ck) = SeedAndTrim ( k, {xt(u)}u∈V [G] ) . 5: if SeedAndTrim returns exactly k points then 6: Compute a partition A1, . . . , Ak of V [G]: for every v ∈ V [G] assign v to its nearest center ci using the ε-NNS algorithm with ε = log k − 1. 7: end if 8: Let t = 2t 9: until t > n10 or ∑ v∈V [G] dv‖xt‖2 < k ( 1− 2logn ) .\n10: return (A1, · · · , Ak).\nLemma 5.10. Let t = Ω(1/(λk · log n)), and t satisfies (5.18). Suppose that SeedAndTrim uses the embedding {xt(u)}u∈V [G] and returns k centers c1, . . . , ck. Then, with constant probability, the following statements hold:\n3Theorem 5.8 shows that both of the embedding {xt(u)}u∈V [G] and the embedding that the algorithm computes in nearly-linear time satisfy the conditions (5.1) and (5.2) with high probability. For the ease of discussion, we use {xt(u)}u∈V [G] to express the embedding that the algorithm actually uses.\n1. It holds that\n{c1, . . . , ck} ⊆ k⋃\ni=1\nCOREαi .\n2. These k centers belong to different cores, and it holds for any different i, j that\n‖xt(ci)− xt(cj)‖2 = Ω̃ (\n1\nk · vol(Si)\n) .\n3. For any i = 1, . . . , k, it holds that\nk∑\ni=1\n∑ u∈Si du · ‖x(u) − x(ci)‖2 = Õ\n( k3\nΥ\n) .\nProof. Since ‖xt(u)‖ is decreasing with respect to the value of t for any vertex u, by Lemma 5.1 for any t = Ω(1/(λk · log n)) we have:\nk∑\ni=1\n∑\nu/∈COREαi\ndu · ‖xt(u)‖2 6 k∑\ni=1\n∑\nu/∈COREαi\ndu · ( ‖F (u)‖2 + 1\nn5\n) 6 k\n100K +\nkn2\nn5 6\n1\nlog k .\nOn the other hand, we only consider values of t satisfying (5.18). Since every vertex u is sampled with probability proportional to du · ‖xt(u)‖2, with constant probability it holds that\n{c1, . . . , ck} ⊆ k⋃\ni=1\nCOREαi ,\nwhich proves the first statement. Now we prove that these k centers belong to different cores. We fix an index i, and assume that ci ∈ Si. We will prove that\n‖xt(ci)‖2 = Ω̃ ( 1\nvol(Si)\n) . (5.19)\nAssume by contradiction that (5.19) does not hold, i.e.,\n‖xt(ci)‖2 = o (\n1\nlogc k vol(Si)\n)\nfor any constant c. Then, we have that\n∑\nu∈COREαi\ndu · ‖xt(u)‖2 6 ∑\nu∈COREαi\ndu · ( ‖xt(ci)‖+ √ Rαi )2\n6 2 · ∑\nu∈COREαi\n( du · ‖xt(ci)‖2 + du ·Rαi )\n= o\n( 1\nlogc k\n) + o ( 1\nk2\n)\n= o\n( 1\nlogc k\n) .\nCombining this with (5.18), the probability that vertices get sampled from COREαi is\n∑ u∈COREαi du · ‖xt(u)‖ 2\n∑ v∈V [G] dv‖xt(v)‖2 = o\n( 1\nk · logc k\n) .\nThis means if we sample K = Θ(k log k) vertices, vertices in COREαi will not get sampled with probability at least 1 − 1/ log5 k. This contradicts the fact that ci ∈ COREαi . Therefore (5.19) holds.\nNow, by description of Algorithm 1, we have for any j 6= i:\n‖xt(ci)− xt(cj)‖2 > ‖x(ci)‖2 2 · 104 · k = Ω̃\n( 1\nk · vol(Si)\n) ,\nwhere the last equality follows from (5.19). Since any vertex in COREαi has distance at most Rαi from ci, cj and ci belong to different cores. Therefore, the second statement holds.\nFinally we turn our attention to the third statement. We showed in Theorem 5.8 that, when t = Θ(1/(λk · log n)), the embedding {xt(u)}u∈V [G] satisfies the conditions (5.1) and (5.2). Hence, it holds that\nk∑\ni=1\n∑ u∈Si du · ‖x(u)− x(ci)‖2 6 k∑ i=1 ∑ u∈Si\n( du · ‖F (u)− F (ci)‖2 + 1\nn5\n)\n6 k∑\ni=1\n∑\nu∈Si\n( du · (‖F (u)− pi‖+ ‖F (ci)− pi‖)2 + 1\nn5\n)\n6 k∑\ni=1\n∑\nu∈Si\n( 2 · du · ( ‖F (u)− pi‖2 + ‖F (ci)− pi‖2 ) + 1\nn5\n) (5.20)\nNotice that by Theorem 4.1 we have\nk∑\ni=1\n∑ u∈Si du · ‖F (u)− pi‖2 6 1.1k2/Υ. (5.21)\nOn the other hand, we have ‖F (ci)− pi‖2 6 Rαi as ci ∈ COREαi , and\nk∑\ni=1\n∑ u∈Si 2 · du · ‖F (ci)− pi‖2 6 k∑ i=1 2 vol(Si) · α · Ei vol(Si) = k∑ i=1\n2α · Ei = Õ ( k3\nΥ\n) . (5.22)\nCombining (5.20) with (5.21) and (5.22), we have that\nk∑\ni=1\n∑ u∈Si du · ‖x(u) − x(ci)‖2 6 Õ\n( k3\nΥ\n) + ∑\nu∈V [G]\ndu n5 = Õ\n( k3\nΥ\n) .\nMoreover, by (5.10) and (5.11) it is straightforward to see that the distance between any embedded vertices decreases as we increase the value of t. Hence, the statement holds for any t = Ω(1/(λk · log n)).\nLemma 5.11. Let A1, . . . , Ak be a k-way partition returned by Algorithm 2. Then, under a proper permutation of the indices, with constant probability for any 1 6 i 6 k it holds that (i) vol(Ai△Si) = Õ ( k4/Υ ) vol(Si), and (ii) φG(Ai) = 1.1 · φG(Si) + Õ ( k4/Υ ) .\nProof. We assume that c1, . . . , ck are the centers returned by SeedAndTrim when obtaining A1, . . . , Ak. By Theorem 5.10, with constant probability it holds that {c1, . . . , ck} ⊆⋃k\ni=1 CORE α i , and ci and cj belong to different cores for i 6= j. Without loss of generality,\nwe assume that ci ∈ COREαi . Then, it holds that\nvol(Si \\ Ai) 6 ∑\ni 6=j vol\n({ v ∈ Si : ‖x(ci)− x(v)‖ >\n‖x(cj)− x(v)‖ log k\n})\n6 ∑\ni 6=j vol\n({ v ∈ Si : ‖x(ci)− x(v)‖ >\n‖x(ci)− x(cj)‖ − ‖x(ci)− x(v)‖ log k\n})\n6 ∑\ni 6=j vol\n({ v ∈ Si : 2‖x(ci)− x(v)‖ >\n‖x(ci)− x(cj)‖ log k\n})\n6 ∑\ni 6=j vol\n({ v ∈ Si : ‖x(ci)− x(v)‖2 = Ω̃ ( 1\nkmin{vol(Sj), vol(Si)}\n)}) (5.23)\n= Õ ( k4/Υ ) vol(Si), (5.24)\nwhere (5.23) follows from the second statement of Theorem 5.10. Similarly, we also have that\nvol(Ai \\ Si) 6 ∑\ni 6=j vol\n({ v ∈ Sj : ‖x(cj)− x(v)‖ >\n‖x(ci)− x(v)‖ log k\n})\n= Õ ( k4/Υ ) vol(Si).\nThis yields the first statement of the lemma. The second statement follows by the same argument used in proving Theorem 1.2.\nProof of Theorem 1.3. The approximation guarantee of the returned partition is shown in Theorem 5.11. For the runtime, notices that we enumerate at most O(poly log n) possible values of t. Furthermore, and for every such possible value of t the algorithm runs in Õ(m) time. This includes computing the distances of embedded points and the seeding / grouping steps. Hence, the total runtime is Õ(m)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Part of this work was done while He Sun and Luca Zanetti were at the Max Planck Institute for Informatics, and while He Sun was visiting the Simons Institute for the Theory of Computing at UC Berkeley. We are grateful to Luca Trevisan for insightful comments on an earlier version of our paper, and to Gary Miller for very helpful discussions about heat kernels on graphs. We also would like to thank Pavel Kolev, and Kurt Mehlhorn [19] for pointing out an omission in an early version of Lemma 4.6. This omission was fixed locally without effecting the statement of the main results."
    } ],
    "references" : [ {
      "title" : "A local algorithm for finding well-connected clusters",
      "author" : [ "Zeyuan Allen-Zhu", "Silvio Lattanzi", "Vahab S. Mirrokni" ],
      "venue" : "In 30th International Conference on Machine Learning",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Subexponential algorithms for unique games and related problems",
      "author" : [ "Sanjeev Arora", "Boaz Barak", "David Steurer" ],
      "venue" : "In 51st Annual IEEE Symposium on Foundations of Computer Science",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "The multiplicative weights update method: a meta-algorithm and applications",
      "author" : [ "Sanjeev Arora", "Elad Hazan", "Satyen Kale" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "A combinatorial, primal-dual approach to semidefinite programs",
      "author" : [ "Sanjeev Arora", "Satyen Kale" ],
      "venue" : "Annual ACM Symposium on Theory of Computing",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Expander flows, geometric embeddings and graph partitioning",
      "author" : [ "Sanjeev Arora", "Satish Rao", "Umesh V. Vazirani" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "k-means++: The advantages of careful seeding",
      "author" : [ "David Arthur", "Sergei Vassilvitskii" ],
      "venue" : "In 18th Annual ACM-SIAM Symposium on Discrete Algorithms",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Center-based clustering under perturbation stability",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "A theoretical approach to the clustering selection problem. In Proceedings of the 4th MultiClust Workshop on Multiple Clusterings, Multi-view Data, and Multi-source Knowledge-driven Clustering, page",
      "author" : [ "Shai Ben-David" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "A local graph partitioning algorithm using heat kernel pagerank",
      "author" : [ "Fan R.K. Chung" ],
      "venue" : "Internet Mathematics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Image segmentation by clustering",
      "author" : [ "Guy B. Coleman", "Harry C. Andrews" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1979
    }, {
      "title" : "An elementary proof of a theorem of Johnson and Lindenstrauss",
      "author" : [ "Sanjoy Dasgupta", "Anupam Gupta" ],
      "venue" : "Random Structures & Algorithms,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "The rotation of eigenvectors by a perturbation",
      "author" : [ "Chandler Davis", "William M. Kahan" ],
      "venue" : "iii. SIAM Journal on Numerical Analysis,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1970
    }, {
      "title" : "Spectral concentration, robust k-center, and simple clustering",
      "author" : [ "Tamal K. Dey", "Alfred Rossi", "Anastasios Sidiropoulos" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Community detection in graphs",
      "author" : [ "Santo Fortunato" ],
      "venue" : "Physics Reports,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Matrix Analysis",
      "author" : [ "Roger A. Horn", "Charles R. Johnson" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Approximate nearest neighbors: towards removing the curse of dimensionality",
      "author" : [ "Piotr Indyk", "Rajeev Motwani" ],
      "venue" : "In 30th Annual ACM Symposium on Theory of Computing",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1998
    }, {
      "title" : "On clusterings: Good, bad and spectral",
      "author" : [ "Ravi Kannan", "Santosh Vempala", "Adrian Vetta" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "An almostlinear-time algorithm for approximate max flow in undirected graphs, and its multicommodity generalizations",
      "author" : [ "Jonathan A. Kelner", "Yin Tat Lee", "Lorenzo Orecchia", "Aaron Sidford" ],
      "venue" : "In 25th Annual ACM-SIAM Symposium on Discrete Algorithms",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "A note on spectral clustering",
      "author" : [ "Pavel Kolev", "Kurt Mehlhorn" ],
      "venue" : "CoRR, abs/1509.09188,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Improved Spectral Sparsification and Numerical Algorithms for SDD Matrices",
      "author" : [ "Ioannis Koutis", "Alex Levin", "Richard Peng" ],
      "venue" : "In 29th International Symposium on Theoretical Aspects of Computer Science",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Improved Cheeger’s inequality: analysis of spectral partitioning algorithms through higher order spectral gap",
      "author" : [ "Tsz Chiu Kwok", "Lap Chi Lau", "Yin Tat Lee", "Shayan Oveis Gharan", "Luca Trevisan" ],
      "venue" : "In 45th Annual ACM Symposium on Theory of Computing",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Multi-way spectral partitioning and higher-order Cheeger inequalities",
      "author" : [ "James R. Lee", "Shayan Oveis Gharan", "Luca Trevisan" ],
      "venue" : "In 44th Annual ACM Symposium on Theory of Computing",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms",
      "author" : [ "Frank T. Leighton", "Satish Rao" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1999
    }, {
      "title" : "Many sparse cuts via higher eigenvalues",
      "author" : [ "Anand Louis", "Prasad Raghavendra", "Prasad Tetali", "Santosh Vempala" ],
      "venue" : "In 44th Annual ACM Symposium on Theory of Computing",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Correlation clustering with noisy partial information",
      "author" : [ "Konstantin Makarychev", "Yury Makarychev", "Aravindan Vijayaraghavan" ],
      "venue" : "In 28th Conference on Learning Theory (COLT’15),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Sparsest cuts and bottlenecks in graphs",
      "author" : [ "David W. Matula", "Farhad Shahrokhi" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1990
    }, {
      "title" : "Spectral partitioning of random graphs",
      "author" : [ "Frank McSherry" ],
      "venue" : "In 42nd Annual IEEE Symposium on Foundations of Computer Science",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2001
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "Andrew Y. Ng", "Michael I. Jordan", "Yair Weiss" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2002
    }, {
      "title" : "Approximating the exponential, the Lanczos method and an Õ(m)-time spectral algorithm for balanced separator",
      "author" : [ "Lorenzo Orecchia", "Sushant Sachdeva", "Nisheeth K Vishnoi" ],
      "venue" : "In 44th Annual ACM Symposium on Theory of Computing",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "On partitioning graphs via single commodity flows",
      "author" : [ "Lorenzo Orecchia", "Leonard J. Schulman", "Umesh V. Vazirani", "Nisheeth K. Vishnoi" ],
      "venue" : "In 40th Annual ACM Symposium on Theory of Computing",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "The effectiveness of Lloyd-type methods for the k-means problem",
      "author" : [ "Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2012
    }, {
      "title" : "Partitioning into expanders",
      "author" : [ "Shayan Oveis Gharan", "Luca Trevisan" ],
      "venue" : "In 25th Annual ACM-SIAM Symposium on Discrete Algorithms",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2014
    }, {
      "title" : "Spectral clustering and the high-dimensional stochastic blockmodel",
      "author" : [ "Karl Rohe", "Sourav Chatterjee", "Bin Yu" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1878
    }, {
      "title" : "Lectures on finite markov chains",
      "author" : [ "L. Saloff-Coste" ],
      "venue" : "Lectures on Probability Theory and Statistics,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1997
    }, {
      "title" : "Breaking the multicommodity flow barrier for O( √ log n)-approximations to sparsest cut",
      "author" : [ "Jonah Sherman" ],
      "venue" : "In 50th Annual IEEE Symposium on Foundations of Computer Science",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2009
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "Jianbo Shi", "Jitendra Malik" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2000
    }, {
      "title" : "Graph sparsification by effective resistances",
      "author" : [ "Daniel A. Spielman", "Nikhil Srivastava" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1913
    }, {
      "title" : "Spectral sparsification of graphs",
      "author" : [ "Daniel A. Spielman", "Shang-Hua Teng" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2011
    }, {
      "title" : "Approximation algorithms for unique games",
      "author" : [ "Luca Trevisan" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2008
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "Ulrike von Luxburg" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2007
    }, {
      "title" : "A simple SVD algorithm for finding hidden partitions",
      "author" : [ "Van Vu" ],
      "venue" : null,
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "This problem is known to be NP-hard [26], and the current best approximation algorithm achieves an approximation ratio of O (√ log n ) [5].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "This problem is known to be NP-hard [26], and the current best approximation algorithm achieves an approximation ratio of O (√ log n ) [5].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : "In computer vision, most image segmentation procedures are based on region-based merge and split [10], which in turn rely on partitioning graphs into multiple subsets [36].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 35,
      "context" : "In computer vision, most image segmentation procedures are based on region-based merge and split [10], which in turn rely on partitioning graphs into multiple subsets [36].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 38,
      "context" : "On a theoretical side, decomposing vertex/edge sets into multiple disjoint subsets is used in designing approximation algorithms for Unique Games [39], and efficient algorithms for graph problems [18, 23, 38].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 17,
      "context" : "On a theoretical side, decomposing vertex/edge sets into multiple disjoint subsets is used in designing approximation algorithms for Unique Games [39], and efficient algorithms for graph problems [18, 23, 38].",
      "startOffset" : 196,
      "endOffset" : 208
    }, {
      "referenceID" : 22,
      "context" : "On a theoretical side, decomposing vertex/edge sets into multiple disjoint subsets is used in designing approximation algorithms for Unique Games [39], and efficient algorithms for graph problems [18, 23, 38].",
      "startOffset" : 196,
      "endOffset" : 208
    }, {
      "referenceID" : 37,
      "context" : "On a theoretical side, decomposing vertex/edge sets into multiple disjoint subsets is used in designing approximation algorithms for Unique Games [39], and efficient algorithms for graph problems [18, 23, 38].",
      "startOffset" : 196,
      "endOffset" : 208
    }, {
      "referenceID" : 21,
      "context" : "Despite widespread use of various graph partitioning schemes over the past decades, the quantitative relationship between the k-way expansion constant and the eigenvalues of the graph Laplacians were unknown until a sequence of very recent results [22, 24].",
      "startOffset" : 248,
      "endOffset" : 256
    }, {
      "referenceID" : 23,
      "context" : "Despite widespread use of various graph partitioning schemes over the past decades, the quantitative relationship between the k-way expansion constant and the eigenvalues of the graph Laplacians were unknown until a sequence of very recent results [22, 24].",
      "startOffset" : 248,
      "endOffset" : 256
    }, {
      "referenceID" : 21,
      "context" : "[22] proved the following higher-order Cheeger inequality: λk 2 6 ρ(k) 6 O(k) √ λk, (1.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 39,
      "context" : "[40] and Section D in [14].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[40] and Section D in [14].",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "([2], Theorem 2.",
      "startOffset" : 1,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "1), and can be considered as a stronger version of the well-known Davis-Kahan theorem [12].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : "Specifically, it omits much of the machinery used in the proofs of higher-order and improved Cheeger inequalities [21, 22].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 21,
      "context" : "Specifically, it omits much of the machinery used in the proofs of higher-order and improved Cheeger inequalities [21, 22].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 27,
      "context" : "[28, 40].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 39,
      "context" : "[28, 40].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 33,
      "context" : "The heat kernel of a graph is a well-studied mathematical concept and is related to, for example, the study of random walks [34].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 28,
      "context" : "Since the heat kernel distances between vertices can be approximated in nearly-linear time [29], this approach avoids the computation of eigenvectors for a large value of k.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 35,
      "context" : "For instance, it is easy to see that ρ(k) and the normalized cut [36] studied in machine learning, which is defined as the sum of the conductance of all returned clusters, differ by at most a factor of k, and the normalized cut value of a k-way partition from spectral clustering can be derived from our results.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "[7, 8, 17, 25].",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 7,
      "context" : "[7, 8, 17, 25].",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 16,
      "context" : "[7, 8, 17, 25].",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "[7, 8, 17, 25].",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 35,
      "context" : "Among these, our work is most closely related to spectral clustering, which is closely related to normalized or low conductance cuts [36].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 31,
      "context" : "Oveis Gharan and Trevisan [32] formulate the notion of clusters with respect to the inner and outer conductance: a cluster S should have low outer conductance, and the conductance of the induced subgraph by S should be high.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "[13] studies the properties of the spectral embedding for graphs having a gap between λk and λk+1 and presents a k-way partition algorithm, which is based on k-center clustering and is similar in spirit to our work.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "The heat kernel has been used in previous algorithms on local partitioning [9], balanced separators [29].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 28,
      "context" : "The heat kernel has been used in previous algorithms on local partitioning [9], balanced separators [29].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "It also plays a key role in current efficient approximation algorithms for finding low conductance cuts [30, 35].",
      "startOffset" : 104,
      "endOffset" : 112
    }, {
      "referenceID" : 34,
      "context" : "It also plays a key role in current efficient approximation algorithms for finding low conductance cuts [30, 35].",
      "startOffset" : 104,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "However, most of these theoretical guarantees are through the matrix multiplicative weights update framework [3, 4].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "However, most of these theoretical guarantees are through the matrix multiplicative weights update framework [3, 4].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 26,
      "context" : "For instance, in the Stochastic Block Model (SBM) [27], the input graph with k clusters is generated according to probabilities p and q with p > q: an edge between any two vertices within the same cluster is placed with probability p, and an edge between any two vertices from different clusters is placed with probability q.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 26,
      "context" : "It is proven that spectral algorithms give the correct clustering for certain ranges of p and q [27, 33, 41].",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 32,
      "context" : "It is proven that spectral algorithms give the correct clustering for certain ranges of p and q [27, 33, 41].",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 40,
      "context" : "It is proven that spectral algorithms give the correct clustering for certain ranges of p and q [27, 33, 41].",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "For this reason, standard perturbation theorems used in the analysis of algorithms for SBMs, such as the Davis-Kahan theorem [12], cannot be always applied, and ad-hoc arguments specific for graphs, like our structure theorem (Theorem 1.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "2 of [2].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 14,
      "context" : "4, [15]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "We point out that it was shown in [21] that the first k eigenvectors can be approximated by a (2k + 1)-step function.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "Notice that this embedding is similar with the one used in [22], with the only difference that F (u) is not normalized and so it is not necessarily a unit vector.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "[11]), we can also always assume that the dimension of the embedding {x(u)}u∈V [G] is d = O(log n).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[6, 31]).",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 30,
      "context" : "[6, 31]).",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "To speed it up, we apply ε-approximate nearest neighbor data structures (ε-NNS) [16], whose formal description is as follows: Problem 1 (ε-approximate nearest neighbor problem).",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "5 ([16]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 28,
      "context" : "8 uses the algorithm for approximating the matrix exponential in [29] as a subroutine, whose performance is summarised in Theorem 5.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 28,
      "context" : "9 ([29]).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "[20] and [37]), we obtain an O(ε−2 · log n) × n Gaussian matrix Q, such that with high probability it holds for all u, v that (1− ε) ‖Z (ξu − ξv)‖ 6 ‖QZ (ξu − ξv)‖ 6 (1 + ε) ‖Z (ξu − ξv)‖ .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[20] and [37]), we obtain an O(ε−2 · log n) × n Gaussian matrix Q, such that with high probability it holds for all u, v that (1− ε) ‖Z (ξu − ξv)‖ 6 ‖QZ (ξu − ξv)‖ 6 (1 + ε) ‖Z (ξu − ξv)‖ .",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 28,
      "context" : "We can then run the approximate exponential algorithm from [29] O(log n) times, where each time we use a different row of Q as input.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 28,
      "context" : "A similar intuition which views the heat kernel embedding as a weighted combination of multiple eigenvectors was discussed in [29].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 18,
      "context" : "We also would like to thank Pavel Kolev, and Kurt Mehlhorn [19] for pointing out an omission in an early version of Lemma 4.",
      "startOffset" : 59,
      "endOffset" : 63
    } ],
    "year" : 2017,
    "abstractText" : "In this paper we study variants of the widely used spectral clustering that partitions a graph into k clusters by (1) embedding the vertices of a graph into a low-dimensional space using the bottom eigenvectors of the Laplacian matrix, and (2) grouping the embedded points into k clusters via k-means algorithms. We show that, for a wide class of graphs, spectral clustering gives a good approximation of the optimal clustering. While this approach was proposed in the early 1990s and has comprehensive applications, prior to our work similar results were known only for graphs generated from stochastic models. We also give a nearly-linear time algorithm for partitioning well-clustered graphs based on computing a matrix exponential and approximate nearest neighbor data structures.",
    "creator" : "LaTeX with hyperref package"
  }
}