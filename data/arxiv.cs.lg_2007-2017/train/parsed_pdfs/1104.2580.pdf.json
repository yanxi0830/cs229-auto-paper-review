{
  "name" : "1104.2580.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Computational Focus of Attention Mechanism to Process Shapes Efficiently: Theory",
    "authors" : [ "Diego Rother", "Simon Schütz", "René Vidal" ],
    "emails" : [ "diroth@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "visual sensory information available to autonomous agents and other automatic systems, it is becoming essential to endow them with a sense of what is worthwhile their attention and what can be safely disregarded. This article presents a general mathematical framework to efficiently allocate the available computational resources to process the parts of the input that are relevant to solve a perceptual problem of interest. By solving a perceptual problem we mean to find the hypothesis H (i.e., the state of the world) that maximizes a function L(H), referred to as the evidence, representing how well each hypothesis “explains” the input. However, given the large bandwidth of the sensory input, fully evaluating the evidence for each hypothesis is computationally infeasible (e.g., because it would imply checking a large number of pixels). To address this problem we propose a mathematical framework with two key ingredients. The first one is a Bounding Mechanism (BM) to compute lower and upper bounds of the evidence of a hypothesis, for a given computational budget. These bounds are much cheaper to compute than the evidence itself, can be refined at any time by increasing the budget allocated to a hypothesis, and are frequently sufficient to discard a hypothesis. The second ingredient is a Focus of Attention Mechanism (FoAM) to select which hypothesis’ bounds should be refined next, with the goal of discarding non-optimal hypotheses with the least amount of computation.\nD. Rother · R. Vidal Johns Hopkins University Tel.: +1-410-516-6736 E-mail: diroth@gmail.com\nS. Schütz University of Göttingen\nThe proposed framework has the following desirable characteristics: 1) it is very efficient since most hypotheses are discarded with minimal computation; 2) it is parallelizable; 3) it is guaranteed to find the globally optimal hypothesis or hypotheses; and 4) its running time depends on the problem at hand, not on the bandwidth of the input. In order to illustrate the general framework, in this article we instantiate it for the problem of simultaneously estimating the class, pose and a noiseless version of a 2D shape in a 2D image. To do this, we develop a novel theory of semidiscrete shapes that allows us to compute the bounds required by the BM. We believe that the theory presented in this article (i.e., the algorithmic paradigm and the theory of shapes) has multiple potential applications well beyond the application demonstrated in this article.\nKeywords Focus of Attention · shapes · shape priors · hypothesize-and-verify · coarse-to-fine · probabilistic inference · graphical models · image understanding"
    }, {
      "heading" : "1 Introduction",
      "text" : "Humans are extremely good at extracting information from images. They can recognize objects from many different classes, even objects they have never seen before; they can estimate the relative size and position of objects in 3D, even from 2D images; and they can (in general) do this in poor lighting conditions, in cluttered scenes, or when the objects are partially occluded. However, natural scenes are in general very complex (Fig. 1a), full of objects with intricate shapes, and so rich in textures, shades and details, that extracting all this information would be computationally very expensive, even for humans. Experimental psychology studies, on the other hand, suggest that (mental) computation is\nar X\niv :1\n10 4.\n25 80\nv1 [\ncs .C\nV ]\n1 3\nA pr\n2 (a) (b) (c)\nFig. 1 (a) Natural scenes are so complex that it would be computationally wasteful for humans to extract all the information that they can extract from them. (b) Attention is task oriented: when focused on the task of counting the passes of a ball between the people in this video, many humans fail to see the gorilla in the midle of the frame (image from [37], see details in that article). (c) Precision is task dependent: in order to pick up the glass, one does not need to estimate the locations of the mugs and the glass with the same precision. A rough estimate of the locations of the mugs is enough to avoid them; a better estimate is needed to pick up the glass.\na limited resource that, when demanded by one task, is unavailable for another [14]. This is arguably why humans have evolved a focus of attention mechanism (FoAM) to discriminate between the information that is needed to achieve a specific goal, and the information that can be safely disregarded.\nHumans, for example, do not perceive every object that enters their field of view; rather they perceive only the objects that receive their focused attention (Fig. 1b) [37]. Moreover, in order to save computation, it is reasonable that even for objects that are indeed perceived, only the details that are relevant towards a specific goal are extracted (as beautifully illustrated in [5]). In particular, it is reasonable to think that objects are not classified at a more concrete level than necessary (e.g., ‘terrier’ vs. ‘dog’), if this were more expensive than classifying the object at a more abstract level, and if this provided the same amount of relevant information towards the goal [29]. Also we do not expect computation to be spent estimating other properties of an object (such as size or position) with higher precision than necessary if this were more expensive and provided the same amount of relevant information towards the goal (Fig. 1c).\n1.1 Self-conscious algorithms\nIn this article we propose a mathematical framework that uses a FoAM to allocate the available computational resources where they contribute the most to solve a given task. This mechanism is one of the parts of a novel family of inference algorithms that we refer to as self-conscious (SC) algorithms. These algorithms are based on the hypothesize-and-verify paradigm. In this paradigm a set of hypotheses and a function referred to as the evidence are defined. Each hypothesis H represents a different state of the world (e.g., which objects\nare located where) and its evidence L(H) quantifies how well this hypothesis “explains” the input image. In a typical hypothesize-and-verify algorithm the evidence of each hypothesis is evaluated and the hypothesis (or group of hypotheses) with the highest evidence is selected as the optimal.\nHowever, since the number of hypotheses could be very large, it is essential to be able to evaluate the evidence of each hypothesis with the least amount of computation. For this purpose the second part of a SC algorithm is a bounding mechanism (BM), which computes lower and upper bounds for the evidence of a hypothesis, instead of evaluating it exactly. These bounds are in general significantly less expensive to compute than the evidence itself, and they are obtained for a given computational budget (allocated by the FoAM), which in turn defines the tightness of the bounds (i.e., higher budgets result in tighter bounds). In some cases, these inexpensive bounds are already sufficient to discard a hypothesis (e.g., if the upper bound of L(H1) is lower than the lower bound of L(H2), H1 can be safely discarded). Otherwise, these bounds can be efficiently and progressively refined by spending extra computational cycles on them (see Fig. 9). As mentioned above, the FoAM allocates the computational budget among the different hypotheses. This mechanism keeps track of the progress made (i.e., how much the bounds got closer to each other) for each computation cycle spent on each hypothesis, and decides on-the-fly where to spend new computation cycles in order to economically discard as many hypotheses as possible. Because computation is allocated where it is most needed, SC algorithms are in general very efficient ; and because hypotheses are discarded only when they are proved suboptimal, SC algorithms are guaranteed to find the optimal solution.\n1.2 Application of SC algorithms to vision\nThe general inference framework mentioned in the previous paragraphs is applicable to any problem in which bounds for the evidence can be inexpensively obtained for each hypothesis. Thus, to instantiate the framework to solve a particular problem, a specific BM has to be developed for that problem. (The FoAM, on the other hand, is common to many problems since it only communicates with the BM by allocating the computational budget to the different hypotheses and by “reading” the resulting bounds.)\nIn this article we illustrate the framework by instantiating it for the specific problem of jointly estimating the class and the pose of a 2D shape in a noisy 2D image, as well as recovering a noiseless version of this\n3 shape. This problem is solved by “merging” information from the input image and from probabilistic models known a priori for the shapes of different classes.\nAs mentioned above, to instantiate the framework for this problem, we must define a mechanism to compute and refine bounds for the evidence of the hypotheses. To do this, we will introduce a novel theory of shapes and shape priors that will allow us to efficiently compute and refine these bounds. While still of practical interest, we believe that the problem chosen is simple enough to best illustrate the main characteristics of SC algorithms and the proposed theory of shapes, without occluding its main ideas. Another instantiation of SC algorithms, which we describe in [32], tackles the more complex problem of simultaneous object classification, pose estimation, and 3D reconstruction, from a single 2D image. In that work we also use the theory of shapes and shape priors to be described in this article to construct the BM for that problem.\n1.3 Paper contributions\nThe framework we propose has several novel contributions that we group in two main areas, namely: 1) the inference framework using SC algorithms; and 2) the shape representations, priors, and the theory developed around them. The following paragraphs summarize these contributions, while in the next section we put them in the context of prior work.\nThe first contribution of this paper is the use of SC algorithms for inference in probabilistic graphical models. In particular, inference in graphical models containing loops. This inference method is not general, i.e., it is not applicable to any directed graph with loops. Rather it is specifically designed for the kinds of graphs containing pixels and voxels that are often used for vision tasks (which typically have a large number of variables and a huge number of loops among these variables). The proposed inference framework has several desirable characteristics. First, it is general, in the sense that the only requirement for its application is a BM to compute and refine bounds for the evidence of a hypothesis. Second, the framework is computationally very efficient, because it allocates computation dynamically where it is needed (i.e., refining the most promising hypotheses and examining the most informative image regions). Third, the total computation does not depend on the arbitrary resolution of the input image, but rather on the task at hand, or more precisely, on the similarity between the hypotheses that are to be distinguished. (In other words, “easy” tasks are solved very fast, while only “difficult” tasks require processing the image completely.) This allows us to avoid the common prepro-\ncessing step of downsampling the input image to the maximal resolution that the algorithm can handle, and permits us to use the original (possibly very high) resolution only in the parts of the image where it is needed. Fourth, the framework is fully parallelizable, which allows it to take advantage of GPUs or other parallel architectures. And fifth, it is guaranteed to find the globally optimal solution (i.e., the hypothesis with the maximum evidence), if it exists, or a set of hypotheses that can be formally proved to be undistinguishable at the maximum available image resolution. This guarantee is particularly attractive when the subjacent graphical model contains many loops, since existing probabilistic inference methods are either very inefficient or not guaranteed to find the optimal solution.\nThe second contribution relates to the novel shape representations proposed, and the priors presented to encode the shape knowledge of the different object classes. These shape representations and priors have three distinctive characteristics. First, they are able to represent a shape with multiple levels of detail. The level of detail, in turn, defines the amount of computation required to process a shape, which is critical in our framework. Second, it is straightforward and efficient to project a 3D shape expressed in these representations to the 2D image plane. This will be essential in the second part of this work [32] to efficiently compute how well a given 3D reconstruction “explains” the input image. And third, based on the theory developed for these shape representations and priors, it is possible to efficiently compute tight log-probability bounds. Moreover, the tightness of these bounds also depends on the level of detail selected, allowing us to dynamically trade computation for bound accuracy. In addition, the theory introduced is general and could be applied to solve many other problems as well.\n1.4 Paper organization\nThe remainder of this paper is organized as follows. Section 2 places the current work in the context of prior relevant work, discussing important connections. Section 3 describes the proposed FoAM. To illustrate the application of the framework to the concrete problem of 2D shape classification, denoising, and pose estimation, in Section 4 we formally define this problem, and in Section 6 we develop the BM for it. In order to develop the BM, we first introduce in Section 5 a theory of shapes and shape priors necessary to compute the desired bounds for the evidence. Because the FoAM and the theory of shapes described in section 3 and 5, respectively, are general (i.e., not only limited to solve the problem described in Section 4), these sections were\n4 written to be self contained. Section 7 presents experimental results obtained with the proposed framework, and Section 8 concludes with a discussion of the key contributions and directions for future research. In the continuation of this work [32], we extend the theory presented in this article to deal with a more complex problem involving not just 2D shapes, but also 3D shapes and their 2D projections."
    }, {
      "heading" : "2 Prior work",
      "text" : "As mentioned in the previous section, this article presents contributions in two main areas: 1) the inference framework based on the FoAM; and 2) the shape representations proposed and the theory developed around them. For this reason, in this section we briefly review prior related work in these areas.\n2.1 FoAM\nMany computational approaches that rely on a focus of attention mechanism have been proposed over the years, in particular to interpret visual stimuli. These computational approaches can be roughly classified into two groups, depending on whether they are biologically inspired or not.\nBiologically inspired approaches [9,39,36], by definition, exploit characteristics of a model proposed to describe a biological system. The goals of these approaches often include: 1) to validate a model proposed for a biological system; 2) to attain the outstanding performance of biological systems by exploiting some characteristics of these systems; and 3) to facilitate the interaction between humans and a robot by emulating mechanisms that humans use (e.g., joint attention [15]). Biological strategies, though optimized for eyes and brains during millions of years of evolution, are not necessarily optimal for current cameras and computer architectures. Moreover, since the biological attentional strategies are adopted at the foundations of these approaches by fiat (instead of emerging as the solution to a formally defined problem), it is often difficult to rigorously analyze the optimality of these strategies. In addition, these attentional strategies were in general empirically discovered for a particular sensory modality (predominantly vision) and are not directly applicable to other sensory modalities. Moreover, these strategies are not general enough to handle simultaneous stimuli coming from several sensory modalities (with some exceptions, e.g., [1]).\nSince in this article we are mainly interested in improving the performance of a perceptual system (possi-\nbly spanning several sensory modalities), and since we want to be able to obtain optimality guarantees, we do not focus further on biologically inspired approaches.\nThe second class of focus of attention mechanisms contains those approaches that are not biologically inspired. Within this class we focus on those approaches that are not ad hoc (i.e., they are rigorously derived from first principles) and are general (i.e., they are able to handle different sensory modalities and tasks). This subclass contains at least two other approaches (apart from ours): Branch and Bound (B&B) and Entropy Pursuit (EP).\nIn a B&B algorithm [4], as in a SC algorithm, an objective function is defined over the hypothesis space and the goal of the algorithm is to select the hypothesis that maximizes this function. A B&B algorithm proceeds by dividing the hypothesis space into subspaces, computing bounds of the objective function for each subspace (rather than for each hypothesis in the subspace), and discarding subspaces that can be proved to be non-optimal (because their upper bound is lower than that of some other subspace). In these algorithms computation is saved by evaluating whole groups of hypotheses, instead of evaluating each individual hypothesis. In contrast, in our approach the hypothesis space is discrete and bounds are computed for every hypothesis in the space. In this case computation is saved by discarding most of these hypotheses with very little computation. In other words, for most hypotheses the inexpensive bounds computed for the evidence of these hypotheses are enough to discard these hypotheses. As will be discussed in Section 8, this approach is complementary to B&B, and it would be beneficial to integrate both approaches into a single framework. Due to space limitations, however, this is not addressed in this article.\nIn an EP algorithm [11,38] a probability distribution is defined over the hypothesis space. Then, during each iteration of the algorithm, a test is performed on the input, and the probability distribution is updated by taking into account the result of the test. This test is selected as the one that is expected to reduce the entropy of the distribution the most. The algorithm terminates when the entropy of the distribution falls below a certain threshold. A major difference between EP and B&B/SC algorithms is that in each iteration of EP a test is selected and the probability of each (of potentially too many) hypothesis is updated. In contrast, in each iteration of B&B/SC, one hypothesis (or one group of hypotheses) is selected and only the bounds corresponding to it are updated. Unlike B&B and SC algorithms, EP algorithms are not guaranteed to find the optimal solution.\n5 A second useful criterion to classify computational\napproaches that rely on a FoAM considers whether attention is controlled only by “bottom-up” signals derived from salient stimuli, or whether it is also controlled by “top-down” signals derived from task demands, or from what a model predicts to be most relevant. Bottom-up approaches (e.g., [17]) are also known as data-driven approaches, while top-down approaches (e.g., [24]) are also known as task-driven approaches. Even though the significance of top-down signals in biological systems is well known, most current computer systems only consider bottom-up signals [9]. In contrast, all the three algorithmic paradigms described (SC, B&B and EP), depending on the specific instantiation of these paradigms, are able to handle bottom-up as well as top-down signals. In particular, in the instantiation of SC algorithms presented in Section 4, both kinds of signals are considered (in fact, it will be seen in Equation (39) that they play a symmetric role). In addition, in all the three algorithmic paradigms described above there is an explicit FoAM to control where the computation is allocated.\n2.2 Inference framework\nMany methods have been proposed to perform inference in graphical models [18]. Message passing algorithms are one class of these methods. Belief propagation (BP) is an algorithm in this class that is guaranteed to find the optimal solution in a loopless graph (i.e., a polytree) [2]. The loopy belief propagation (LBP) and the junction tree (JT) algorithms are two algorithms that extend the capabilities of the basic BP algorithm to handle graphs with loops. In LBP messages are exchanged exactly as in BP, but multiple iterations of the basic BP algorithm are required to converge to a solution. Moreover, the method is not guaranteed to converge to the optimal solution in every graph but only in some types of graphs [42]. In the JT algorithm [19], BP is run on a modified graph whose cycles have been eliminated. To construct this modified graph, the first step is “moralization,” which consists of marrying the parents of all the nodes. For the kinds of graphs we are interested in, however, this dramatically increases the clique size. While the JT algorithm is guaranteed to find the optimal solution, in our case this algorithm is not efficient because its complexity grows exponentially with the size of the largest clique in the modified graph.\nTwo standard “tricks” to perform exact inference (using BP) by eliminating the loops of a general graph are: 1) to merge nodes in the graph into a “supernode,” and 2) to make assumptions about the values of (i.e., to instantiate) certain variables, creating a different graph\nfor each possible value of the instantiated variables (i.e., for each hypothesis) [25]. These approaches, however, bring their own difficulties. On the one hand, merging nodes results in a supernode whose number of states is the product of the number of states of the merged nodes. On the other hand, instantiating variables forces us to solve an inference problem for a potentially very large number of hypotheses.\nIn this work we propose a different approach to merge nodes that does not run into the problems mentioned above. Specifically, instead of assuming that the image domain is composed of a finite number of discrete pixels and merging them into supernodes, we assume that the image domain is continuous and consists of an infinite number of “pixels.” We then compute “summaries” of the values of the pixels in each region of the domain (this is formally described in Section 5). In order to solve the inference efficiently for each hypothesis, the total computation per hypothesis is trimmed down by using lower and upper bounds and a FoAM, as mentioned in Section 1.\n2.3 Shape representations and priors\nSince shape representations and priors are such essential parts of many vision systems, over the years many shape representations and priors have been proposed (see reviews in [8,6]). Among these, only a small fraction have the three properties required by our system and mentioned in Section 1.3, i.e., support multiple levels of detail, efficient projection, and efficient computation of bounds.\nScale space and orthonormal basis representations have the property that they can encode multiple levels of detail. In the scale-space representation [20], a shape (or image in general) is represented as a oneparameter family of smoothed shapes, parameterized by the size of the smoothing kernel used for suppressing fine-scale structures. Therefore, the representation contains a smoothed copy of the original shape at each level of detail. In the orthonormal basis representation, on the other hand, a shape is represented by its coefficients in an orthonormal basis. To compute these coefficients the shape is first expressed in the same representation as the orthonormal basis. For example, in [23] and [26] the contour of a shape is expressed in spherical wavelets and Fourier bases, respectively, and in [34] the signed distance function of a shape is written in terms of the principal components of the signed distance functions of shapes in the training database. The level of detail in this case is defined by the number of coefficients used to represent the shape in the basis. While these shape representations have the first\n6 property mentioned above (i.e., multiple levels of detail), they do not have the other two, that is that it is not trivial to efficiently project 3D shapes expressed in these representations to the 2D image plane, or to compute the bounds that we want to compute.\nThe shape representations we propose, referred to as discrete and semidiscrete shape representations (defined in Section 5 and shown in Fig. 5) are respectively closer to region quadtrees/octrees [33] and to occupancy grids [7]. In fact, the discrete shape representation we propose is a special case of a region quadtree/octree in which the rule to split an element is a complex function of the input data, the prior knowledge, and the interaction with other hypotheses. Quadtrees and octrees have been previously used for 3D recognition [3] and 3D reconstruction [27] from multiple silhouettes (not from a single one, to the best of our knowledge, as we do in [32]). Occupancy grids, on the other hand, are significantly different from semidiscrete shapes since they store at each cell a qualitatively different quantity: occupancy grids store the posterior probability that an object is in the cell, while semidiscrete shapes store the measure of the object in the cell."
    }, {
      "heading" : "3 Focus of attention mechanism",
      "text" : "In Section 1 we mentioned that a self-conscious (SC) algorithm has two parts: 1) a focus of attention mechanism (FoAM) to allocate the available computation cycles among the different hypotheses; and 2) a bounding mechanism (BM) to compute and refine the bounds of each hypothesis. In this section we describe in detail the first of these two parts, the FoAM.\nLet I be some input and let H = {H1, . . . ,HNH} be a set of NH hypotheses proposed to “explain” this input. In our problem of interest (formally described in Section 4) the input I is an image, and each of the hypotheses corresponds to the 2D pose and class of a 2D shape in this input image. However, from the point of view of the FoAM, it is not important what the input actually is, or what the hypotheses actually represent. The input can be simply thought of as “some information about the world acquired through some sensors,” and the hypotheses can be simply thought of as representing a “possible state of the world.”\nSuppose that there exist a function L(H) that quantifies the evidence in the input I supporting the hypothesis H. In Section 4 the evidence for our problem is shown to be related to the log-joint probability of the image I and the hypothesis H. But again, from the point of view of the FoAM, it is not important how this function is defined; it only matters that hypotheses that “explain” the input better produce higher values. Thus,\npart of the goal of the FoAM is to select the hypothesis (or group of hypotheses) Hi∗ that best explain the input image, i.e.,\nHi∗ = arg max H∈H L(H). (1)\nNow, suppose that the evidence L(Hi) of a hypothesis Hi is very costly to evaluate (e.g., because a large number of pixels must be processed to compute it), but lower and upper bounds for it, L(Hi) and L(Hi), respectively, can be cheaply computed by the BM. Moreover, suppose that the BM can efficiently refine the bounds of a hypothesis Hi if additional computational cycles (defined below) are allocated to the hypothesis. Let us denote by Lni(Hi) and Lni(Hi), respectively, the lower and upper bounds obtained for L(Hi) after ni computational cycles have been spent on Hi. If the BM is well defined, the bounds it produces must satisfy\nLni+1(Hi) ≥ Lni(Hi), and Lni+1(Hi) ≤ Lni(Hi), (2)\nfor every hypothesis Hi, and every ni ≥ 0 (assume that ni = 0 is the initialization cycle in which the bounds are first computed). In other words, the bounds must not become looser as more computational cycles are invested in their computation. Note that we expect different numbers of cycles to be spent on different hypotheses, ideally with “bad” hypotheses being discarded earlier than “better” ones (i.e., n1 < n2 if L(H1) L(H2)). The “computational cycles” mentioned above are our unit to measure the computational resources spent. Each computational cycle, or just cycle, is the computation that the BM spends to refine the bounds. While the exact conversion rate between cycles and operations depends on the particular BM used, what is important from the point of view of the FoAM is that all refinement cycles take approximately the same number of operations (defined to be equal to one computational cycle).\nThe full goal of the FoAM can now be stated as to\nselect the hypothesis Hi that satisfies\nLni(Hi) > Lnj (Hj) ∀j 6= i, (3) while minimizing the total number of cycles spent, ∑NH j=1 nj . If these inequalities are satisfied, it can be proved that Hi is the optimal hypothesis, without having to compute exactly the evidence for every hypothesis (which is assumed to be much more expensive than just computing the bounds). However, it is possible that after all the hypotheses in a set Hi ⊂ H have been refined to the fullest extent possible, their upper bounds are\n7 still bigger than or equal to the maximum lower bound γ , maxHi∈Hi Lni(Hi), i.e.,\nLni(Hi) ≥ γ ∀Hi ∈ Hi. (4)\nIn this situation all the hypotheses in Hi could possibly be optimal, but we cannot say which one actually is. We just do not have the right input to distinguish between them (e.g., because the resolution of the input image is insufficient). We say that these hypotheses are indistinguishable given the current input. In short, the FoAM will terminate either because it has found the optimal hypothesis (satisfying (3)), or because it has found a set of hypotheses that are indistinguishable from the optimal hypothesis given the current input (and satisfies (4)).\nThese termination conditions can be achieved by very different budgets that allocate different number of cycles to each hypothesis. We are interested in finding the budget that achieves them in the minimum number of cycles. Finding this minimum is in general not possible since the FoAM does not “know,” a priori, how the bounds will change for each cycle it allocates to a hypothesis. For this reason, we propose a heuristic to select the next hypothesis to refine at each point in time. Once a hypothesis is selected, one cycle is allocated to this hypothesis, which is thus refined once by the BM. This selection-refinement cycle is continued until termination conditions are reached.\nAccording to the heuristic proposed, the next hypothesis to refine, Hi∗, is chosen as the one that is expected to produce the greatest reduction ∆P (Hi∗) in the following potential P ,\nP , ∑ Hi∈A ( Lni(Hi)− γ ) , (5)\nwhere A is the set of all the hypotheses not yet discarded (i.e., those that are active), γ is the maximum lower bound defined before, and ni is the number of refinement cycles spent on hypothesis Hi. This particular expression for the potential was chosen for two reasons: 1) because it reflects the workload left to be done by the FoAM; and 2) because it is minimal when termination conditions have been achieved.\nIn order to estimate the potential reduction ∆̂P (H) expected when hypothesis H is refined (as required by the heuristic), we need to first define a few quantities (Fig. 2). We define the margin Mn(H) of a hypothesis H after n cycles have been spent on it, as the difference between its bounds, i.e., Mn(H) , Ln(H) − Ln(H). Then we define the reduction of the margin of this hypothesis during its n-th refinement as ∆Mn(H) , Mn−1(H)−Mn(H). It can be seen that this quantity is\n(a)\n(c) (b)\nFig. 2 (a) Bounds for the evidence L(Hi) of three active hypotheses (H1, H2, and H3) after refinement cycle t. (b-c) Bounds for the same three hypotheses after refinement cycle t+1, assumming that either H1 (b) or H2 (c) was refined during cycle t + 1. Each rectangle represents the interval where the evidence of a hypothesis is known to be. Green and red rectangles denote active and discarded hypotheses, respectively. The maximum lower bound in each case (γa, γb, and γc) is represented by the black dashed line. It can be seen that ∆γ(H1) , γb − γa < γc − γa , ∆γ(H2). The potential in each case (Pa, Pb, and Pc) is represented by the sum of the gray parts of the intervals. It can be seen that ∆P (H1) , Pa − Pb < Pa − Pc , ∆P (H2).\npositive, and because in general early refinements produce larger margin reductions than later refinements, it has a general tendency to decrease. Using this quantity we predict the reduction of the margin in the next refinement using an exponentially weighted moving average, ∆̂Mn+1(H) , α ∆̂Mn(H) + (1 − α)∆Mn(H), where 0 < α < 1. This moving average is initialized as ∆̂M0(H) = βM0(H). (In this work we used α = 0.9 and β = 0.25.)\nThe predicted potential reduction ∆P (H) depends on whether the refinement of H is expected to increase γ or not: when γ increases, every term in (5) is reduced; when it does not, only the term corresponding to H is reduced (compare figures 2b and 2c). Let us assume that the reduction in the upper bound, Ln(H)− Ln+1(H), is equal to the increase in the lower bound, Ln+1(H) − Ln(H), which is therefore predicted to be equal to ∆̂Mn+1(H)/2. Let us define ∆γ(H) to be the increase in γ when H is refined, thus ∆γ(H) , max{Ln(H) + ∆̂Mn+1(H)/2 − γ, 0}. Then the following expression is an estimate for the potential reduction when H is refined,\n∆̂P (H) = { ∆̂Mn+1(H)/2 + |A|∆γ(H), if ∆γ(H) > 0 ∆̂Mn+1(H)/2, otherwise.\n(6)\nAs mentioned before, the hypothesis that maximizes this quantity is the one selected to be refined next.\nThe algorithm used by the FoAM is thus the following (a detailed explanation is provided immediately afterwards):\n1: γ ← −∞ 2: for i = 1 to NH do\n3: [ L(Hi), L(Hi) ] ← ComputeBounds(Hi)\n8 4: if L(Hi) > γ then\n5: γ ← L(Hi) 6: end if 7: if L(Hi) > γ then 8: ∆̂M0(Hi)← βM0(Hi) 9: Compute ∆̂P (Hi)\n10: A.Insert(Hi, ∆̂P (Hi)) 11: end if 12: end for 13: while not reached Termination Conditions do 14: Hi ← A.GetMax() 15: if L(Hi) > γ then\n16: [ L(Hi), L(Hi) ] ← RefineBounds(Hi)\n17: Compute ∆̂P (Hi) 18: A.Insert(Hi, ∆̂P (Hi)) 19: if L(Hi) > γ then 20: γ ← L(Hi) 21: end if 22: end if 23: end while\nThe first stage of the algorithm is to initialize the bounds for all the hypotheses (line 3 above), use these bounds to estimate the expected potential reduction for each hypothesis (lines 8-9), and to insert the hypotheses in the priority queue A using the potential reduction as the key (line 10). This priority queue A, supporting the usual operations Insert and GetMax, contains the hypotheses that are active at any given time. The GetMax operation, in particular, is used to efficiently find the hypothesis that, if refined, is expected to produce the greatest potential reduction. During this first stage the maximum lower bound γ is also initialized (lines 1 and 4-6).\nIn the second stage of the algorithm, hypotheses are selected and refined alternately until termination conditions are reached. The next hypothesis to refine is simply obtained by extracting from the priority queue A the hypothesis that is expected to produce, if refined, the greatest potential reduction (line 14). If this hypothesis is still viable (line 15), its bounds are refined (line 16), its expected potential reduction is recomputed (line 17), and it is reinserted into the queue (line 18). If necessary, the maximum lower bound is also updated (lines 19-21). One issue to note in this procedure is that the potential reductions used as key when hypotheses are inserted in the queue A are outdated once γ is modified. Nevertheless this approximation works well in practice and allows a complete hypothesis selection/refinement cycle (lines 13-23) to run in O(log |A|) where |A| is the number of active hypotheses. This complexity is determined by the operations on the priority queue.\nRother and Sapiro [31] have suggested a different heuristic to select the next hypothesis to refine. Their heuristic consists on selecting the hypothesis whose current upper bound is greatest. This heuristic, however, in general required more cycles than the heuristic we are proposing here. To see why, consider the case in which, after some refinement cycles, two active hypotheses H1 and H2 still remain. Suppose that H1 is better than H2 (Ln1(H1) Ln2(H2)), but at the same time it has been more refined (n1 n2). As mentioned before, because of the decreasing nature of ∆M , in these conditions we expect ∆Mn2(H2) ∆Mn1(H1). Therefore, if we chose to refine H1 (as in [31]) many more cycles will be necessary to distinguish between the hypotheses than if we had chosen to refine H2 (as in the heuristic explained above). However, the strategy of choosing the less promising hypothesis is only worthwhile when there are few hypotheses remaining, since computation in that case is invested in a hypothesis that is ultimately discarded. The desired behavior is simply and automatically obtained by minimizing the potential defined in (5), and this ensures that computation is spent sensibly."
    }, {
      "heading" : "4 Definition of the Problem",
      "text" : "The FoAM described in the previous section is a general algorithm that can be used to solve many different problems, as long as: 1) the problems can be formulated as selecting the hypothesis that maximizes some evidence function within a set of hypotheses, and 2) a suitable BM can be defined to bound this evidence function. To illustrate the use of the FoAM to solve a concrete problem, in this section we define the problem, and in Section 6 we derive a BM for this particular problem.\nGiven an input image I : Ω → Rc (c ∈ N, c > 0) in which there is a single “shape” corrupted by noise, the problem is to estimate the class K of the shape, its pose T , and recover a noiseless version of the shape. This problem arises, for example, in the context of optical character recognition [10] and shape matching [40]. For clarity it is assumed in this section that Ω ⊂ Z2 (i.e., the image is composed of discrete pixels arranged in a 2D grid).\nTo solve this problem using a SC algorithm, we define one hypothesis H for every possible pair (K,T ). By selecting a hypothesis, the algorithm is thus estimating the class K and pose T of the shape in the image. As we will later show, in the process a noiseless version of the shape will also be obtained.\nIn order to define the problem more formally, suppose that the image domain Ω contains n pixels, x1, . . . , xn, and that there areNK distinct possible shape classes,\n9 each one characterized by a known shape prior BK (1 ≤ K ≤ NK) defined on the whole discrete plane Z2, also containing discrete pixels. Each shape prior BK specifies, for each pixel x′ ∈ Z2, the probability that the pixel belongs to the shape q, pBK (x\n′) , P (q(x′) = 1|K), or to the complement of the shape, P (q(x′) = 0|K) = 1 − pBK (x′). We assume that pBK is zero everywhere, except (possibly) in a region ΩK ⊂ Z2 called the support of pBK . We will say that a pixel x ′ belongs to the Foreground if q(x′) = 1, and to the Background if q(x′) = 0 (Foreground and Background are the labels of the two possible states of a shape for each pixel).\nLet T ∈ {T1, . . . , TNT } be an affine transformation in R2, and call BH (recall that H = (K,T )) the shape prior that results from transforming BK by T , i.e., pBH (x) , P (q(x) = 1|H) , pBK (T−1x) (disregard for the moment the complications produced by the misalignment of pixels). The state q(x) in a pixel x is thus assumed to depend only on the class K and the transformation T (in other words, it is assumed to be conditionally independent of the states in the other pixels, given the hypothesis H).\nNow, suppose that the shape q is not observed directly, but rather that it defines the distribution of a feature (e.g., colors, edges, or in general any feature) to be observed at a pixel. In other words, if a pixel x belongs to the background (i.e., if q(x) = 0), its feature f(x) is distributed according to the probability density function px(f(x)|q(x) = 0), while if it belongs to the foreground (i.e., if q(x) = 1), f(x) is distributed according to px(f(x)|q(x) = 1) (the subscript x in px was added to emphasize the fact that the probability of observing a feature f(x) at a pixel x depends on the state of the pixel q(x) and on the particular pixel x, or in other terms, px(f0|q0) 6= py(f0|q0) if x 6= y and f0 and q0 are two arbitrary values of f and q, respectively). This feature f(x) is assumed to be independent of the feature f(y) and the state q(y) in every other pixel y, given q(x).\nThe conditional independence assumptions described above can be summarized in the factor graph of Fig. 3 (see [2] for more details on factor graphs). It then follows that the joint probability of all pixel features f , all states q, and the hypothesis H = (K,T ), is p(f, q,H) = P (H) ∏ x∈Ω Px(f(x)|q(x))P (q(x)|H). (7)\nThen, our goal can be simply stated as solving\nmax q,H p(f, q,H) = max H∈H\nL′′(H), with (8)\nL′′(H) , max q p(f, q,H). (9)\n2- Shape prior\n3- 2D Segmentation\n4- Feature model\n5- Pixel features\n1- Object Class and Pose\nFig. 3 Factor graph proposed to solve our problem of interest. A factor graph, [2], has a variable node (circle) for each variable, and a factor node (square) for each factor in the system’s joint probability. Factor nodes are connected to the variable nodes of the variables in the factor. Observed variables are shaded. A plate indicates that there is an instance of the nodes in the plate for each element in a set (indicated on the lower right). The plate in this graph hides the existing loops. See text for details.\nWe could solve this problem näıvely by computing (9) for every H ∈ H. However, to compute (9) all the pixels in the image (or at least all the pixels in the support of ΩK) need to be processed in order to evaluate the product in (7) (since the solution q∗ that maximizes (9) can be written explicitly). Because this might be very expensive, we need a BM to evaluate the evidence without having to process every pixel in the image.\nTherefore, instead of using this näıve approach, we will use an SC algorithm to find the hypothesis H that maximizes an expression simpler than L′′(H), that is equivalent to it (in the sense that it has the same maxima). This simpler expression is what we have called the evidence, L(H), and will be derived from (9) in Section 6.1. Before deriving the evidence, however, in Section 5 we present the mathematical framework that will allow us to do that, and later to develop the BM for this evidence."
    }, {
      "heading" : "5 A new theory of shapes",
      "text" : "As mentioned before, SC algorithms have two parts: a FoAM and a BM. The FoAM was already introduced in Section 3. While the same FoAM can be used to solve many different problems, each BM is specific to a particular problem. Towards defining the BM for the problem described in the previous section (that will be done in Section 6), in this section we introduce a mathematical framework that will allow us to compute the bounds.\nTo derive formulas to bound the evidence L(H) of a hypothesis H (the goal of the BM) for our specific prob-\n10\nFig. 4 The set Ω and three possible partitions of it. Each square represents a partition element. Π2 is finer than Π1 (Π2 ≤ Π1). Π3 is not comparable with neither Π1 nor with Π2.\nlem of interest, we introduce in this section a framework to represent shapes and to compute bounds for their log-probability (in Section 6.1 we will show that this log-probability is closely related to the evidence). Three different shape representations will be introduced (Fig. 5). Continuous shapes are the shapes that we would observe if our cameras (or 3D scanners) had “infinite resolution.” In that case it would be possible to compute the evidence L(H) of a hypothesis with “infinite precision” and therefore always select the (single) hypothesis whose evidence is maximum (except in concocted examples which have very low probability of occurring in practice). However, since real cameras and scanners have finite resolution, we introduce two other shape representations that are especially suited for this case: discrete and semidiscrete shape representations. Discrete shapes will allow us to compute a lower bound L(H) for the evidence of a hypothesis H. Semidiscrete shapes, on the other hand, will allow us to compute an upper bound L(H) for the evidence of a hypothesis H. Discrete and semidiscrete shapes are defined on partitions of the input image (i.e., non-overlapping regions that completely cover the image, see Fig. 4). Finer partitions result in tighter bounds, more computation, and the possibility of distinguishing more similar hypotheses. Coarser partitions on the other hand, result in looser bounds, less computation and more hypotheses that are indistinguishable (in the partition).\nPreviously we have assumed that the image domain, Ω ⊂ Z2, consisted on discrete pixels arranged in a 2D grid. For reasons that will soon become clear, however, we assume from now on that the image domain Ω ⊂ R2 is continuous. Thus, to discretize this continuouos domain we rely on “partitions,” defined next.\nDefinition 1 (Partitions) Given a setΩ ⊂ Rd, a partition Π(Ω) = {Ω1, . . . , Ωn} with Ωi 6= ∅, is a disjoint cover of the set Ω (Fig. 4). Formally, Π(Ω) satisfies n⋃ i=1 Ωi = Ω, and Ωi ∩Ωj = ∅ ∀i 6= j. (10)\nA partition Π(Ω) = {Ω1, . . . , Ωn} is said to be uniform if all the elements in the partition have the same measure |Ωi| = |Ω|n for i = 1, . . . , n. (Throughout this\nGiven two partitions Π1(Ω) and Π2(Ω) of a set Ω ⊂ Rd, Π2 is said to be finer than Π1, and Π1 is said to be coarser than Π2, if every element of Π2 is a subset of some element of Π1 (Fig. 4). We denote this relationship as Π2 ≤ Π1. Note that two partitions are not always comparable, thus the binary relationship “≤” defines a partial order in the space of all partitions.\n5.1 Discrete shapes\nDefinition 2 (Discrete shapes) Given a partition Π(Ω) of the set Ω ⊂ Rd, the discrete shape Ŝ (Fig. 5b) is defined as the function Ŝ : Π(Ω)→ {0, 1}.\nDefinition 3 (Log-probability of a discrete shape) Let Ŝ be a discrete shape in some partition Π(Ω) = {Ω1, . . . , Ωn}, and let B̂ = {B̂1, . . . , B̂n} be a family of independent Bernoulli random variables referred to as a discrete Bernoulli field (BF). Let B̂ be characterized by the success rates pB̂(i) , P (B̂i = 1) ∈ (ε, 1− ε) for i = 1, . . . , n, and 0 < ε 1. To avoid the problems derived from assuming complete certainty, i.e. success rates of 0 or 1, following Cromwell’s rule [21], we will only consider success rates in the open interval (ε, 1−ε). The log-probability of a discrete shape is defined as logP (B̂ = Ŝ) , n∑ i=1 logP ( B̂i = Ŝ(Ωi)\n) =\nn∑ i=1 [( 1− Ŝ(Ωi) ) logP ( B̂i = 0 ) +\nŜ(Ωi) logP ( B̂i = 1 )]\n= ZB̂ + n∑ i=1 Ŝ(Ωi)δB̂(i), (11)\n11\nwhere ZB̂ , ∑n i=1 log ( 1− pB̂(i) ) is a constant and\nδB̂(i) , log ( pB̂(i)/ ( 1− pB̂(i) )) is the logit function of pB̂(i).\nThe discrete BFs used in this work arise from two sources: background subtraction and shape priors. To compute a discrete BF B̂f using the Background Subtraction technique [22], recall the probability densities pΩi (f(Ωi)|q(Ωi) = 0) and pΩi (f(Ωi)|q(Ωi) = 1) defined in Section 4 to model the probability of observing a feature f(Ωi) at a given pixel Ωi, depending on the pixel’s state q(Ωi). The success rates of the discrete BF B̂f are thus defined as\npB̂f (i) , pΩi(f(Ωi)|q(Ωi) = 1)\npΩi(f(Ωi)|q(Ωi) = 0) + pΩi(f(Ωi)|q(Ωi) = 1) .\n(12)\nTo compute a discrete BF B̂s associated with a discrete shape prior, we can estimate the success rates pB̂s(i) of B̂s from a collection of N discrete shapes,\nΣ̂ = { Ŝ1, . . . , ŜN } , assumed to be aligned in the set Ω. These discrete shapes can be acquired by different means, e.g., using a 2D or 3D scanner, for d = 2 or d = 3, respectively. The success rate pB̂s(i) of a particular Bernoulli variable B̂i (i = 1, . . . , n) is thus estimated\nfrom { Ŝ1(Ωi), . . . , ŜN (Ωi) } using the standard for-\nmula for the estimation of Bernoulli distributions [16],\npB̂s(i) = 1\nN N∑ j=1 Ŝj(Ωi). (13)\nDiscrete shapes, as in Definition 2, have two limitations that must be addressed to enable subsequent developments. First, the log-probability in (11) depends (implicitly) on the unit size of the partition (which is related to the image resolution), preventing the comparison of log-probabilities of images acquired at different resolutions (this will be further explained after Definition 6). Second, it was assumed in (11) that the Bernoulli variables B̂i and the pixels Ωi were perfectly aligned. However, this assumption might be violated if a transformation (e.g., a rotation) is applied to the shape. To overcome these limitations, and also to facilitate the proofs that will follow, we introduce next the second shape representation, that of a continuous shape.\n5.2 Continuous shapes\nDefinition 4 (Continuous shapes) Given a set Ω ⊂ Rd, we define a continuous shape S to be a function\nS : Ω → {0, 1} (Fig. 5a). We will often abuse notation and refer to the set S = {x ∈ Ω : S(x) = 1} also as the shape. To avoid pathological cases, we will require the set S to satisfy two regularity conditions: 1) to be open (in the usual topology in Rd [13]) and 2) to have a boundary (as defined in [13]) of measure zero.\nGiven a discrete shape Ŝ defined on a partition Π(Ω) = {Ω1, . . . , Ωn}, the continuous shape S(x) , Ŝ(Ωi) ∀x ∈ Ωi, is referred to as the continuous shape produced by the discrete shape Ŝ, and is denoted as S ∼ Ŝ or Ŝ ∼ S. Intuitively, S extends Ŝ from every element of Π(Ω) to every point x ∈ Ω.\nWe would like now to extend the definition of the log-probability of a discrete shape (in Definition 3) to include continuous shapes. Toward this end we first introduce continuous BFs, which play in the continuous case the role that discrete BFs play in the discrete case.\nDefinition 5 (Continuous Bernoulli Fields) Given a set Ω ⊂ Rd, a continuous Bernoulli field (or simply a BF) is the construction that associates a Bernoulli random variable Bx to every point x ∈ Ω. The success rate for each variable in the field is given by the function pB(x) , P (Bx = 1). The corresponding logit func-\ntion δB(x) , log (\npB(x) 1−pB(x) ) and constant term ZB ,∫\nΩ log (1− pB(x))dx are as in Definition 3.\nWe will only consider in this work functions pB(x) such that |ZB | < ∞ and δB(x) is a measurable function [43]. Furthermore, since ε < pB(x) < 1−ε ∀x ∈ Ω, δB(x) ∈ (−δmax, δmax) ∀x ∈ Ω, with δmax , log ( 1−ε ε ) . Note that a BF is not associated to a continuous probability density on Ω (e.g., it almost never holds that∫ Ω pB(x) dx = 1), but rather to a collection of discrete probability distributions, one for each point in Ω (thus, it always holds that P (Bx = 0) +P (Bx = 1) = 1 ∀x ∈ Ω).\nDue to the finite resolution of cameras and scanners, continuous BFs cannot be directly obtained as discrete BFs were obtained in Definition 3. In contrast, continuous BFs are obtained indirectly from discrete BFs (which are possibly obtained by one of the methods described in Definition 3). Let B̂ be a discrete BF defined on a partition Π(Ω) = {Ω1, . . . , Ωn}. Then, for each partition element Ωi, and for each point x ∈ Ωi, the success rate of the Bernoulli variable Bx is defined as pB(x) , pB̂(i). The BF B produced in this fashion will be referred to as the BF produced by the discrete BF B̂. Intuitively, pB extends pB̂ from every element of Π(Ω) to every point x ∈ Ω. Note that this definition is analogous to the definition of a continuous shape produced from a discrete shape in Definition 4.\n12\nLet Ω′ ⊂ Rd be a set referred to as the canonical set, let Ω ⊂ Rd be a second set referred to as the world set, and let T : Ω′ → Ω be a bijective transformation between these sets. Given a BF B in Ω′ with success rates pB(x), the transformed BF BT in Ω is defined as\nBT , B ◦T−1 with success rates pBT (x) , pB ( T−1x ) .\nDefinition 6 (Log-probability of a continuous shape) Let B be a BF in Ω with success rates given by the function pB(x), let S be a continuous shape also in Ω, and let uo > 0 be a scalar called the equivalent unit size. We define the log-probability that a shape S is produced by a BF B, by extension of the log-probability of discrete shapes in (11), as\nlogP (B = S) , 1\nuo\n[ ZB + ∫ Ω S(x)δB(x) dx ] , (14)\nwhere ZB and δB are respectively the constant term and the logit function of B.\nSeveral things are worth noting in this definition. First, note that if there is a uniform partition Π(Ω) = {Ω1, . . . , Ωn} with |Ωi| = uo ∀i, and if the continuous shape S and the BF B are respectively produced by the discrete shape Ŝ and the discrete Bernoulli field B̂\ndefined on Π(Ω), then logP (B = S) = logP ( B̂ = Ŝ ) .\nFor this reason we said that the definition in (14) extends the definition in (11). However, keep in mind that in the case of a continuous shape (14) is not a log-probability in the traditional sense, but rather it extends the definition to cases in which S(x) is not produced from a discrete shape and δB(x) is not piecewise constant in a partition of Ω.\nSecond, note that while (14) provides the “log-probability” density that a given continuous shape is produced by a BF, sampling from a BF is not guaranteed to produce a continuous shape (because the resulting set might not satisfy the regularity conditions in Definition 4). Nevertheless, this is not an obstacle since in this work we are only interested in computing logprobabilities of continuous shapes that are given, not on sampling from BFs.\nThird, note in (14) that the log-probability of a continuous shape is the product of two factors: 1) the inverse of the unit size, which only depends on the partition (but not on the shape); and 2) a term (in brackets) that does not depend on the partition. In the case of continuous shapes, uo in the first factor is not the unit size of the partition (there is no partition defined in this case) but rather a scalar defining the unit size of an equivalent partition in which the range of logprobability values obtained would be comparable. The second factor is the sum of a constant term that only\ndepends on the BF B, and a second term that also depends on the continuous shape S.\nFourth, the continuous shape representation in Definition 4 overcomes the limitations of the discrete representation pointed out above. More specifically, by considering continuous shapes, (14) can be computed even if a discrete shape and a discrete BF are defined on partitions that are not aligned, allowing us greater freedom in the choice on the transformations (T ) that can be applied to the BF. Furthermore, the role of the partition is “decoupled” from the role of the BF and the shape, allowing us to compute (14) independently of the resolution of the partitions.\n5.3 Semi-discrete shapes\nAs mentioned at the beginning of this section, discrete shapes will be used to obtain a lower bound for the logprobability of a continuous shape. Unfortunately, upper bounds for the log-probability derived using discrete shapes are not very tight. For this reason, to obtain upper bounds for this log-probability, we need to introduce the third shape representation, that of semidiscrete shapes.\nDefinition 7 (Semidiscrete shapes) Given a partition Π(Ω) = {Ω1, . . . , Ωn} of the set Ω ⊂ Rd, the semidiscrete shape S̃ is defined as the function S̃ : Π(Ω) → [0, |Ω|], that associates to each element Ωi in the partition a real number in the interval [0, |Ωi|], i.e., S̃(Ωi) ∈ [0, |Ωi|] (Fig. 5c).\nGiven a continuous shape S in Ω, we say that this shape produces the semidiscrete shape S̃, denoted as S ∼ S̃ or S̃ ∼ S, if S̃(Ωi) = |S ∩ Ωi| for i = 1, . . . , n. An intuition that will be useful later to understand the derivation of the upper bounds is that a semidiscrete shape produced from a continuous shape “remembers” the measure of the continuous shape inside each element of the partition, but “forgets” where exactly the shape is located inside the element.\nGiven two continuous shapes in Ω, S1 and S2, that produce the the same semidiscrete shape S̃ in the partition Π(Ω), we say the these continuous shapes are related (denoted as S1 ∼ S2). The nature of this relationship is explored in the next proposition.\nProposition 1 (Equivalent classes of continuous shapes) The relationship “∼” defined above is an equivalence relation.\nProof The proof of this proposition is trivial from Definition 7. ut\n13\nWe will say that these continuous shapes are equivalent in the partition Π(Ω), and by extension, we will also say that they are equivalent to S̃ (i.e., Si ∼ S̃).\nProposition 2 (Relationships between shape representations) Let Π(Ω) = {Ω1, . . . , Ωn} be an arbitrary partition of a set Ω, and let Ŝ(Π) and S̃(Π) be the sets of all discrete and semidiscrete shapes, respectively, defined on Π(Ω). Let S be the set of all continuous shapes in Ω. Then,{ S : S ∼ Ŝ, Ŝ ∈ Ŝ(Π)\n} ⊂ S, and, (15){\nS : S ∼ S̃, S̃ ∈ S̃(Π) } = S. (16)\nProof The proof of this proposition is trivial from definitions 2, 4, and 7. ut\n5.4 LCDFs and summaries\nSo far we have introduced three different shape representations and established relationships among them. In this section we introduce the concepts of logit cumulative distribution functions (LCDFs) and summaries. These concepts will be necessary to use discrete and semidiscrete shapes to bound the log-probability of continuous shapes, and hence, to bound the evidence.\nIntuitively, a LCDF “condenses” the “information” of a BF in a partition element into a monotonous function. A summary then further “condenses” this “information” into a single vector of fixed length. Importantly, the summary of a BF in a partition element can be used to bound the evidence L(H) and can be computed in constant time, regardless of the number of pixels in the element.\nAfter formally defining LCDFs and summaries below, we will prove in this Section some of the properties that will be needed in Section 6 to compute lower and upper bounds for L(H). In the remainder of this section, unless stated otherwise, all partitions, shapes and BFs are defined on a set Ω ⊂ Rd.\nDefinition 8 (Logit cumulative distribution function) Given the logit function δB of a BF B and a partition Π, the logit cumulative distribution function (or LCDF) of the BF B in Π is the collection of functions DB = {DB,ω}ω∈Π , where each function DB,ω : [−δmax, δmax]→ [0, |ω|] is defined as\nDB,ω(δ) , |{x ∈ ω : δB(x) < δ}| (ω ∈ Π). (17)\nIt must be noted that this definition is consistent, since from Definition 5, the logit function is measurable. The LCDF is named by analogy to the probability cumulative distribution function, but must not\nbe confused with it. Informally, a LCDF “condenses” the information of the BF by “remembering” the values taken by the logit function inside each partition element, but “forgetting” where those values are inside the element. This relationship between BFs and their LCDFs is analogous to the relationship between continuous and semidiscrete shapes.\nEquation (17) defines a non-decreasing and possibly discontinuous function. To see that this function is non-decreasing, note that the set χ1 , {x ∈ ω : δB(x) < δ1} is included in the set χ2 , {x ∈ ω : δB(x) < δ2} if δ1 ≤ δ2. Moreover, this function is not necessarily strictly increasing because it is possible to have DB,ω(δ1) = DB,ω(δ2) with δ1 < δ2 if the measure of the set {x ∈ ω : δ1 ≤ δB(x) < δ2} is zero. To see that the function defined in (17) can be discontinuous, note that this function will have a discontinuity whenever the function δB(x) is constant on a set of measure greater than zero.\nLater in this section we will use the inverse of a LCDF, D−1B,ω(a). If DB,ω(δ) were strictly increasing and continuous, we could simply defineD−1B,ω(a) as the unique real number δ ∈ [−δmax, δmax] such that DB,ω(δ) = a. For general LCDFs, however, this definition does not produce a value for every a ∈ [0, |ω|]. Instead we use the following definition.\nDefinition 9 (Inverse LCDF) The inverse LCDF D−1B,ω(a) is defined as (see Fig. 6)\nD−1B,ω(a) , inf {δ : DB,ω(δ) ≥ a} . (18)\nTo avoid pathological cases, we will only consider in this work LCDFs whose inverse is continuous almost everywhere (note that this imposes an additional restriction on the logit function).\n14\nDefinition 10 (X-axis crossing point) We define the x-axis crossing point, AB,ω, as\nAB,ω , DB,ω(0\n−) +DB,ω(0 +)\n2 . (19)\nThis quantity has the property that D−1B,ω(a) ≥ 0 if a ≥ AB,ω, and D−1B,ω(a) ≤ 0 if a ≤ AB,ω.\nDefinition 11 (Summaries) Given a partition Π, a summary of a LCDF DB = {DB,ω}ω∈Π in the partition is a functional that assigns to each function DB,ω (ω ∈ Π) in the LCDF a vector YB,ω ∈ RdY . The name “summary” is motivated by the fact that the “infinite dimensional” distribution is “summarized” by just dY real numbers.\nTwo types of summaries are used in this article: m-summaries and mean-summaries. Given m > 0, an m-summary assigns to each function DB,ω in the LCDF\nthe (2m+1)-dimensional vector ỸB,ω = [ Ỹ −mB,ω . . . Ỹ m B,ω ]T of equispaced samples of the LCDF, i.e.,\nỸ jB,ω , DB,ω ( jδmax m ) (j = −m, . . . ,m). (20)\nNote that since the LCDF is known to be a non-decreasing function, the information in the m-summary can be used to bound the inverse LCDF (Fig. 7). Specifically, we know that D−1B,ω(a) ≤ δmax m (j + 1) for a ∈ [ Ỹ jB,ω, Ỹ j+1B,ω ] , which can be written as\nD−1B,ω(a) ≤ D −1 YB,ω (a)\n, δmax m J(a), ∀a ∈ [0, |ω|] , (21)\nwith J(a) , min { j : Ỹ jB,ω ≥ a } = ∣∣∣{j : Ỹ jB,ω < a}∣∣∣−m. (22)\nThese bounds will be used in turn to compute an upper bound for L(H).\nThe second type of summaries, referred to as meansummaries, will be used to compute a lower bound of L(H). The mean-summary assigns to each function DB,ω the scalar\nŶB,ω , ∫ |ω| 0 D−1B,ω(a) da = ∫ ω δB(x) dx. (23)\nThe last equality follows by setting S̃(ω) = |ω| in (29) and is proved later in Lemma 1. Note that this equality provides the means to compute the mean-summary directly from the logit function, without having to compute the LCDF.\nOne of the main properties of summaries is that, for certain kinds of sets, they can be computed in constant time regardless of the number of elements (e.g., pixels) in these sets. Next we show how to compute the meansummary ŶB,Φ and the m-summaries ỸB,Φ of a BF B for the set Φ ⊂ Ω (defined below). For simplicity we assume that Ω ⊂ R2, but the results presented here immediately generalize to higher dimensions. We also assume that Π(Ω) = {Ω1,1, Ω1,2, . . . , , Ωn,n} is a uniform partition of Ω organized in rows and columns, where each partition element Ωi,j (in the i-th row and the jth column) is a square of area |Ωi,j | = u0. We assume that B, defined by its logit function δB(x) (x ∈ Ω), was obtained from a discrete shape prior B̂ in Π(Ω) (as described in Definition 5), and therefore δB(x) , δB̂(i, j) ∀x ∈ Ωi,j . And finally, we assume that Φ is an axis-aligned rectangular region containing only whole pixels (i.e., not parts of pixels). That is,\nΦ , ⋃\niL≤i≤iU jL≤j≤jU\nΩi,j . (24)\nIn order to compute the mean-summary ŶB,Φ, note\nthat from (23), ŶB,Φ = ∑\niL≤i≤iU jL≤j≤jU\n∫ Ωi,j δB(x) dx =\n= uo ∑\niL≤i≤iU jL≤j≤jU\nδB̂(i, j). (25)\nThe sum on the rhs of (25) can be computed in constant time by relying on integral images [41], an image representation precisely proposed to compute sums in rectangular domains in constant time. To accomplish\n15\nthis, integral images precompute a matrix where each pixel stores the cumulative sum of the values in pixels with lower indices. The sum in (25) is then computed as the sum of four of these precomputed cumulative sums.\nThe formula to compute the m-summary ỸB,Φ is similarly derived. From (20), and since δB is constant inside each partition element, it holds for k = −m, . . . ,m that\nỸ kB,Φ = ∣∣∣∣{x ∈ Φ : δB(x) < kδmaxm }∣∣∣∣ = uo ∣∣∣∣{(i, j) :\niL ≤ i ≤ iU , jL ≤ j ≤ jU , δB̂(i, j) < kδmax m }∣∣∣∣ (26)\nLet us now define the matrices Ik (k = −m, . . . ,m) as\nIk(i, j) ,\n{ 1, if δB̂(i, j) < kδmax/m\n0, otherwise. (27)\nUsing this definition, (26) can be rewritten as Ỹ kB,Φ = uo ∑\niL≤i≤iU jL≤j≤jU\nIk(i, j), (28)\nwhich as before can be computed in O(1) using integral images.\nBefore deriving formulas to bound L(H), we need\nthe following two results.\nProposition 3 (Equivalent classes of BFs) Let Π be a partition, let B1 and B2 be two BFs, and consider the following binary relations: 1) B1 and B2 are related (denoted as B1 ∼D B2) if they produce the same LCDF D = {Dω}ω∈Π ; and 2) B1 and B2 are related (denoted as B1 ∼Y B2) if they produce the same summary Y = {Yω}ω∈Π . Then, “∼D” and “∼Y ” are equivalence relations.\nProof Immediate from definitions 8 and 11. ut\nIn the first case (∼D) we will say that these BFs are equivalent in the partition Π with respect to distributions. Abusing the notation, we will also say that they are equivalent to (or compatible with) the LCDF (i.e., Bi ∼ D). Similarly, in the second case (∼Y ) we will say that these BFs are equivalent in the partition Π with respect to summaries. Abusing the notation, we will also say that they are equivalent to (or compatible with) the summary (i.e., Bi ∼ Y ). Note that if two BFs are equivalent with respect to a LCDF, they are also equivalent with respect to any summary of the LCDF. The reverse, however, is not necessarily true.\nLemma 1 (Properties of LCDFs and m-Summaries) Let Π be an arbitrary partition, and let S̃ and D = {Dω}ω∈Π be respectively a semidiscrete shape and a LCDF in this partition. Then:\n1. Given a BF B such that B ∼ D, it holds that (Fig. 6)\nsup S∼S̃ ∫ Ω δB(x)S(x) dx = ∑ ω∈Π ∫ |ω| |ω|−S̃(ω) D−1ω (a) da. (29)\n2. Similarly, given a continuous shape S, such that\nS ∼ S̃, it holds that\nsup B∼D ∫ Ω δB(x)S(x) dx = ∑ ω∈Π ∫ |ω| |ω|−S̃(ω) D−1ω (a) da.\n(30)\n3. Moreover, for any α ∈ [0, |ω|], the integrals on the rhs of (29) and (30) can be bounded as∫ |ω| α D−1ω (a) da ≤ δmax m\nJ(α)(Ỹ J(α)ω − α)+ m−1∑ j=J(α) ( Ỹ j+1ω − Ỹ jω ) (j + 1)\n , (31) where J(α) is as defined in (22).\n4. The rhs of (29) and (30) are maximum when\nS̃(ω) = |ω| −Aω, thus\nsup S̃ ∑ ω∈Π ∫ |ω| |ω|−S̃(ω) D−1ω (a) da = ∑ ω∈Π ∫ |ω| Aω D−1ω (a) da.\n(32)\nProof Due to space limitations this proof was included in the supplementary material. ut\nThis concludes the presentation of the general mathematical framework to compute and bound probabilities of shapes. This framework is used in the next section to bound the evidence for the problem defined in Section 4, and also in [32] to bound the evidence for a more complex problem."
    }, {
      "heading" : "6 Bounding mechanism",
      "text" : "We are now ready to develop the BM for the problem defined in Section 4. This BM is the second part of the SC algorithm proposed to solve this problem. To develop the BM we proceed in three steps: first we derive an expression for the evidence L(H) that the FoAM will maximize (Section 6.1); second we show how to compute lower and upper bounds for the evidence for\n16\na given partition (Section 6.2); and third, we show how to construct, incrementally, a sequence of increasingly finer partitions that will result in increasingly tighter bounds (Section 6.3).\n6.1 Definition of the Evidence L(H)\nIn order to define the evidence L(H) for the problem, we re-label the pixels of the image in (7) (denoted previously by x) as the elements of a uniform partition Π, and separate the factors according to its state q(ω). Hence (7) is equal to\nP (H) ∏ ω∈Π Pω(f(ω)|q(ω))P (q(ω)|H) =\nP (H) ∏ ω∈Π [ Pω(f(ω)|q(ω) = 0)P (q(ω) = 0|H) ]1−q(ω)× × [ Pω(f(ω)|q(ω) = 1)P (q(ω) = 1|H) ]q(ω) .\n(33)\nDefining a BF Bf based on the features observed in the input image, with success rates given by\npBf (ω) , Pω (f(ω)|q(ω) = 1)\nPω (f(ω)|q(ω) = 0) + Pω (f(ω|q(ω) = 1) ,\n(34)\nassuming that all hypotheses are equally likely, and dividing (33) by the constant (and known) term\nP (H) ∏ ω∈Π (Pω(f(ω)|q(ω) = 0) + Pω(f(ω)|q(ω) = 1)),\n(35)\nwe obtain an expression equivalent to (7),∏ ω∈Π [( 1− pBf (ω) )( 1− pBH (ω) )]1−q(ω) ×\n× [ pBf (ω)pBH (ω) ]q(ω) (36)\n(recall from Section 4 that BH is a BF with success rates pBH , P (q(ω) = 1|H)). This expression can be further simplified by taking logarithms and using the variables introduced in Definition 3 to yield\nZBf + ZBH + ∑ ω∈Π q(ω) ( δBf (ω) + δBH (ω) ) . (37)\nUsing the extension to the continuous domain explained in Definition 6, and substituting (37) into (9), (9) can be rewritten as\nL′(H) , sup q\n1\nuo [ ZBf + ZBH+∫ Ω q(x) ( δBf (x) + δBH (x) ) dx ] . (38)\nNow, since ZBf and uo are constant for every hypothesis and every continuous shape q, maximizing this expression is equivalent to maximizing\nL(H) , ZBH + sup q ∫ Ω q(x) ( δBf (x) + δBH (x) ) dx.\n(39)\nThis is the final expression for the evidence. Due to the very large number of pixels in a typical image produced by a modern camera, computing L(H) directly as the integral in (39) would be prohibitively expensive. For this reason, the next step is to derive bounds for (39) that are cheaper to compute than (39) itself, and are sufficient to discard most hypotheses. These bounds are derived in the next section.\n6.2 Derivation of the bounds\nIn the following two theorems we derive bounds for the evidence of a hypothesis that can be computed from summaries of the BFs Bf and BH (in (39)), instead of computing them from the BFs directly. Because summaries can be computed in O(1) for each element in a partition, bounds for a given partition can be computed in O(n) (where n is the number of elements in the partition), regardless of the actual number of pixels in the image.\nTheorem 1 (Lower bound for L(H)) Let Π be a partition, and let Ŷf = { Ŷf,ω } ω∈Π and ŶH = { ŶH,ω } ω∈Π be the mean-summaries of two unknown BFs in Π. Then, for any Bf ∼ Ŷf and any BH ∼ ŶH , it holds that L(H) ≥ LΠ(H), where\nLΠ(H) , ZBH + ∑ ω∈Π Lω(H), (40)\nLω(H) , ( Ŷf,ω + ŶH,ω ) q̂∗(ω), (41)\nand q̂∗ is a discrete shape in Π defined as\nq̂∗(ω) ,\n{ 1, if ( Ŷf,ω + ŶH,ω ) > 0\n0, otherwise. (42)\nProof Due to space limitations this proof was included in the supplementary material. ut\nTheorem 2 (Upper bound for L(H)) Let Π be a partition, and let Ỹf = { Ỹf,ω } ω∈Π and ỸH = { ỸH,ω } ω∈Π be the m-summaries of two unknown BFs in Π. Let Ỹf ⊕ H,ω (ω ∈ Π) be a vector of length 4m+ 2 obtained\n17\nby sorting the values in Ỹf,ω and ỸH,ω (in ascending order), keeping repeated values, i.e.,\nỸf ⊕ H,ω , [ Ỹ 1f ⊕ H,ω, . . . , Ỹ 4m+2 f ⊕ H,ω ] , SortAscending ( Ỹf,ω ∪ ỸH,ω ) . (43)\nThen, for any Bf ∼ Ỹf and any BH ∼ ỸH , it holds that L(H) ≤ LΠ(H), where\nLΠ(H) , ZBH + ∑ ω∈Π Lω(H), and (44)\nLω(H) , δmax m 4m+1∑ j=2m+1 (j − 2m) ( Ỹ j+1f ⊕ H,ω − Ỹ j f ⊕ H,ω ) .\n(45)"
    }, {
      "heading" : "It also follows that the continuous shape that maximizes",
      "text" : "(39) is equivalent to a semidiscrete shape q̃∗ in Π that satisfies\nq̃∗(ω) ∈ [ |ω| − Ỹ 2m+1f⊕H,ω, |ω| − Ỹ 2mf⊕H,ω ] ∀ω ∈ Π. (46)\nProof Due to space limitations this proof was included in the supplementary material. ut\nTheorems 1 and 2 presented formulas to compute lower and upper bounds for L(H), respectively, for a given partition Π. Importantly, these theorems also include formulas to compute a discrete shape q̂∗ and a semidiscrete shape q̃∗ that approximate (in the partition Π) the continuous shape q that solves (39). In the next section we show how to reuse the computation spent to compute the bounds for a partition Πk, to compute the bounds for a finer partition Πk+1.\n6.3 Incremental refinement of bounds\nGiven a partition Πk containing hk elements, it can be seen in (40) and (44) that the bounds for the evidence corresponding to this partition can be computed in O(hk). In Section 3, however, we requested that the BM be able to compute these bounds in O(1). In order to compute a sequence of progressively tighter bounds for a hypothesis H, where each bound is computed in O(1), we inductively construct a sequence of progressively finer partitions of Ω for the hypothesis.\nLet us denote by Πk(H) , {ΩH,1, . . . , ΩH,hk} the k-th partition in the sequence corresponding to H. Each sequence is defined inductively by\nΠ1(H) , {Ω}, and (47) Πk+1(H) , [Πk(H) \\ ωk] ∪ π(ωk), (k > 0), (48)\nwhere ωk ∈ Πk(H) and π(ωk) is a partition of ωk. For each partition Πk(H) in the sequence, lower (Lk(H)) and upper (Lk(H)) bounds for the evidence L(H) could be computed in O(k) using (40) and (44), respectively. However, these bounds can be computed more efficiently by exploiting the form of (48), as\nLk+1(H) = Lk(H)− Lωk(H) + ∑\nω∈π(ωk)\nLω(H). (49)\n(A similar expression for the upper bound Lk+1(H) can be derived.) If the partition of ωk, π(ωk), is chosen to always contain a fixed number of sets (e.g., |π(ωk)| = 4 ∀k > 1), then it can be seen in (49) that O(1) evaluations of (41) are required to compute Lk+1(H).\nWhile any choice of ωk from Πk(H) in (48) would result in a new partition Πk+1(H) that is finer than Πk(H), it is natural to choose ωk to be the set in Πk(H) with the greatest local margin (Lωk(H)−Lωk(H)) since this is the set responsible for the largest contribution to the total margin of the hypothesis (Lk(H)−Lk(H)). In order to efficiently find the set ωk with the greatest local margin, we store the elements of a partition in a priority queue, using their local margin as the priority. Hence to compute Πk+1(H) from Πk(H) (in (48)) we need to extract the element ωk of the queue with the largest local margin, and then insert each element in π(ωk) into the queue. Taken together these steps have, depending on the implementation of the queue, complexity of at least O(log hk) (where hk is the number of elements in the partition) [28]. In our case, however, it is not essential to process the elements in strictly descending margin order. Any element with a margin close enough to the maximum margin would produce similar results. Moreover, in our case we know that the margins belong to the interval (0,LΩ(H)−LΩ(H)] and that they tend to decrease with time.\nBased on these considerations we propose a queue implementation based on the untidy priority queue of Yatziv et al. [44], in which the operations GetMax and Insert both have complexity O(1). This implementation consists of an array of buckets (i.e., singly-linked lists), where each bucket contains the elements whose margin is in an interval Ij . Specifically, suppose that the minimum and maximum margin of any element are known to be M and M, respectively, and that ρ > 1 is a constant (we chose ρ = 1.2). The intervals are then defined to be Ij , [Mρj−1,Mρj) (j = 1, . . . , ⌈ logρM/M ⌉ , where d·e is the ceiling function). To speed up the GetMax operation, a variable jmax keeps the index of the non-empty bucket containing the element with the largest margin. In the Insert operation, we simply compute the index j of the corresponding bucket, insert the element in this bucket, and up-\n18\ndate jmax if j > jmax. In the GetMax operation, we return any element from the jmax-th bucket, and update jmax. Note that the margin of the returned element is not necessarily the maximum margin in the queue, but it is at least 1/ρ times this value. Since both operations (Insert and GetMax) can be carried out in O(1), we have proved that (49) can also be computed in O(1).\nMoreover, since the bounds in (41) and (45) are tighter if the regions involved are close to uniform (because in this case, given the summary, there is no uncertainty regarding the value of any point in the region), this choice of ωk automatically drives the algorithm to focus on the edges of the image and the prior, avoiding the need to subdivide and work on large uniform regions of the image or the prior.\nThis concludes the derivation of the bounds to be used to solve our problem of interest. In the next section we show results obtained using these bounds integrated with the FoAM described in Section 3."
    }, {
      "heading" : "7 Experimental results",
      "text" : "In this section we apply the framework described in previous sections to the problem of simultaneously estimating the class, pose, and a denoised version (a segmentation) of a shape in an image. We start by analyzing the characteristics of the proposed algorithm on synthetic experiments (Section 7.1), and then present experiments on real data (Section 7.2). These experiments were designed to test and illustrate the proposed theory only. Achieving state-of-the-art results for each of the specific sub-problems would require further extensions of this theory.\n7.1 Synthetic experiments\nIn this section we present a series of synthetic experiments to expose the characteristics of the proposed approach.\nExperiment 1. We start with a simple experiment where both the input image (Fig. 8a) and the shape prior are constructed from a single shape (the letter ‘A’). Since we consider a single shape prior, we do not need to estimate the class of the shape in this case, only its pose. In this situation the success rates pBf of the BF corresponding to this image (Fig. 8b), and the success rates pBK of the BF corresponding to the shape prior, are related by a translation t (i.e., pBf (x) = pBK (x − t)). This translation is the pose that we want to estimate. In order to estimate it, we define four hypotheses and use the proposed approach to select the hypothesis H\nthat maximizes the evidence L(H). Each hypothesis is obtained for a different translation (Fig. 8b), but for the same shape prior.\nAs described in Section 3, at the beginning of the algorithm the bounds of all hypotheses are initialized, and then during each iteration of the algorithm one hypothesis is selected, and its bounds are refined (Fig. 9). It can be seen in Fig. 9 that the FoAM allocates more computational cycles to refine the bounds of the best hypothesis (in red), and less cycles to the other hypotheses. In particular, two hypotheses (in cyan and blue) are discarded after spending just one cycle (the initialization cycle) on each of them. Consequently, it\n19\ncan be seen in Fig. 10 that the final partition for the red hypothesis is finer than those for the othe hypotheses. It can also be seen in the figure that the partitions are preferentially refined around the edges of the image or the prior (remember from (39) that image and prior play a symmetric role), because the partition elements around these areas have greater margins. In other words, the FoAM algorithm is “paying attention” to the edges, a sensible thing to do. Furthermore, this behavior was not intentionally “coded” into the algorithm, but rather it emerged as the BM greedily minimizes the margin.\nTo select the best hypothesis in this experiment, the functions to compute the lower and upper bounds (i.e., those that implement (41)-(42) and (45)-(46)) were called a total of 88 times each. In other words, 22 pairs of bounds were computed, on average, for each hypothesis. In contrast, if L(H) were to be computed exactly, 16,384 pixels would have to be inspected for each hypothesis (because all the priors used in this section were of size 128× 128). While inspecting one pixel is significantly cheaper than computing one pair of bounds, for images of “sufficient” resolution the proposed approach is more efficient than näıvely inspecting every pixel. Since the relative cost of evaluating one pair of bounds (relative to the cost of inspecting one pixel) depends on the implementation, and since at this point an efficient implementation of the algorithm is not available, we use the average number of bound pairs evaluated per hypothesis (referred as τ) as a measure of performance (i.e., τ , Pairs of bounds computed/Number of Hypotheses).\nMoreover, for images of sufficient resolution, not only is the amount of computation required by the proposed algorithm less than that required by the näıve approach, it is also independent of the resolution of these images. For example, if in the previous experiment the resolution of the input image and the prior were doubled, the number of pixels to be processed by the näıve approach would increase four times, while the number of bound evaluations would remain the same.\nIn other words, the amount of computation needed to solve a particular problem using the proposed approach only depends on the problem, not on the resolution of the input image.\nExperiment 2. The next experiment is identical to the previous one, except that one hypothesis is defined for every possible integer translation that yields a hypothesis whose support is contained within the input image. This results in a total of 148,225 hypotheses. In this case, the set A of active hypotheses when termination conditions were reached contained 3 hypotheses. We refer to this set as the set of solutions, and to each hypothesis in this set as a solution. Note that having reached termination conditions with a set of solutions having more than one hypothesis (i.e., solution) implies that all the hypotheses in this set have been completely refined (i.e., either pBf or pBH are uniform in all their partition elements).\nTo characterize the set of solutions A, we define the translation bias, µt, and the translation standard deviation, σt, as\nµt , ∥∥∥∥∥∥ 1|A| |A|∑ i=1 ti − tT ∥∥∥∥∥∥ , and (50) σt , √√√√ 1 |A| |A|∑ i=1 ‖ti − tT ‖2, (51)\nrespectively, where ti is the translation corresponding to the i-th hypothesis in the set A and tT is the true translation. In this particular experiment we obtained µt = 0 and σt = 0.82, and the set A consisted on the true hypothesis and the two hypotheses that are one pixel translated to the left and right. These 3 hypotheses are indistinguishable under the conditions of the experiment. There are two facts contributing to the uncertainty that makes these hypotheses indistinguishable: 1) the fact that the edges of the shape in the image and the prior are not sharp (i.e., not having probabilities of 0/1, see inset in Fig. 8b); and 2) the fact that m <∞ in the m-summaries and hence some “information” of the LCDF is “lost” in the summary, making the bounds looser.\nFigure 11 shows the evolution of the set A of active hypotheses as the bounds get refined. Observe in this figure that during the refinement stage of the algorithm (cycles > 0 in the figure), the number of active hypotheses (|A|), the bias µt, and the standard deviation σt sharply decrease. It is interesting to note that during the first half of the initialization stage, because hypotheses are not discarded symmetrically around the true hypothesis, the bias increases.\n20\nFig. 11 (Left) Evolution of the number of live hypotheses (|A|) as a function of time. (Right) Evolution of the translation bias µt (blue) and standard deviation σt (green) of the set of active hypotheses A.\nFigure 12 shows the percentage of hypotheses that were refined 0 or 1 times, between 2 and 9 times, between 10 and 99 times, and between 100 and 999 times. This figure indicates that, as desired, very little computation is spent on most hypotheses, and most computation is spent on very few hypotheses. Concretely, the figure shows that for 95.5% of the hypotheses, either the initialization cycle is enough to discard the hypothesis (i.e., only 1 pair of bounds needs to be computed), or an additional refinement cycle is necessary (and hence 1 + 4 = 5 pairs of bounds are computed). On the other hand, only 0.008% of the hypotheses require between 100 and 999 refinement cycles. On average, only 1.78 pairs of bounds are computed for each hypothesis (τ = 1.78), instead of inspecting 16,384 pixels for each hypothesis as in the näıve approach. For convenience, these results are summarized in Table 1.\nExperiment 3. In the next experiment the hypothesis space is enlarged by considering not only integer translations, but also scalings. These scalings changed the horizontal and vertical size of the prior (independently in each direction) by multiples of 2%. In other terms, the transformations used were of the form\nT (x) = Diag(s)x + t, (52)\nwhere t , [tx, ty] is an integer translation (i.e., tx, ty ∈ Z), Diag(s) is a diagonal matrix containing the vector of scaling factors s , [sxsy] in its diagonal, and sx, sy ∈ {0.96, 0.98, 1, 1.02, 1.04}.\nTo characterize the set of solutions A of this experiment, in addition to the quantities µt and σt that were defined before, we also define the scaling bias, µs, and the scaling standard deviation, σs, as\nµs , ∥∥∥∥∥∥ 1|A| |A|∑ i=1 si − 1 ∥∥∥∥∥∥ , and (53) σs , √√√√ 1 |A| |A|∑ i=1 ‖si − 1‖2, (54)\nrespectively, where si are the scaling factors corresponding to the i-th hypothesis in the set A (the true scaling factors, for simplicity, are assumed to be sx = sy = 1).\nIn this experiment the set of solutions consisted of the same three hypotheses found in Experiment 2, plus two more hypotheses that had a ±2% scaling error. The performance of the framework in this case, on the other hand, improved with respect to the previous experiment (note the τ in Table 2). These results (summarized in Table 2) suggest that in addition to the translation part of the pose, the scale can also be estimated very accurately and efficiently.\nExperiment 4. The performance of the framework is obviously affected when the input image is corrupted by noise. To understand how it is affected, we run the proposed approach with the same hypothesis space defined in Experiment 2, but on images degraded by different kinds of noise (for simplicity we add the noise directly to the BF corresponding to the input image, rather than to the input image itself). Three kinds of noise have been considered (Fig. 13a): 1) additive, zero mean, white Gaussian noise with standard deviation σ, denoted by N (0, σ2); salt and pepper noise, SP(P ), produced by transforming, with probability P , the success rate p(x) of a pixel x into 1− p(x); and structured noise, S(`), produced by transforming the success rate p(x) into 1− p(x) for each pixel x in rows and colums that are multiples of `. When adding Gaussian noise to a BF some values end up outside the interval [0, 1]. In such cases we trim these values to the corresponding extreme of the interval. The results of these experiments are summarized in Table 3.\n21\nBy comparing tables 1 and 3 we observe that the approach is relatively immune to the levels and kinds of noise applied, since µt and σt did not increase much. However, the effects of the different kinds of noise are different. Adding moderate amounts of Gaussian noise, by making it uncertain whether each pixel in the image belongs to the foreground or background, increased by more than 3 times the size of the set of solutions, while doubling the amount of computation required (compare τ in tables 1 and 3). In contrast, moderate amounts of salt and pepper noise almost did not affect the set of solutions found, but they dramatically increased the necessary computation (because this kind of noise created many more “edges” in the image). To understand the effect of the position of the errors introduced by the noise, we adjusted the level of the structured noise ` so that the same (approximate) number of pixels were affected by it and by the salt and pepper noise (i.e., approximately the same number of pixels were affected by SP(0.083) and S(24), by SP(0.125) and S(16), and by SP(0.25) and S(8)). Because the structured noise concentrates the “errors” in some parts of the image, this kind of noise increased slightly the size of the set of solutions, but it did not have a consistent effect on the errors or the amount of computation required.\nFig. 13b-e show the shapes that were estimated from the noisy BFs in Fig. 13a. To understand how these shapes were estimated, recall that a discrete shape (given by (42)) is implicitly obtained while computing the lower bound of a hypothesis, and a semidiscrete shape (given by (46)) is implicitly obtained while computing the upper bound of a hypothesis. Thus, Fig. 13d-e show the discrete and semidiscrete shapes obtained for the solution with the greatest upper bound. Notice how we were able to remove almost all the noise. It is also interesting to inspect the shapes estimated at an intermediate stage of the process, after only 50 refinement cycles of the bounds had been performed (Fig. 13b-c).\nExperiment 5. Another factor that affects the framework’s performance is the “shape” of the prior. As men-\n(a)\n(b)\n(c)\n(d)\n(e)\nFig. 13 (a) BFs corresponding to the input image after being degraded by three different kinds of noise (from left to right): Gaussian noise N (0, 0.452); Salt and pepper noise SP(0.25); and structured noise S(8). (b-c) Discrete shape Ŝ (b), and semidiscrete shape S̃ (c), estimated for the best solution after 50 refinement cycles. Each shape was obtained for the corresponding input image depicted in the same column in (a). (d-e) Discrete (d) and semidiscrete (e) shapes obtained when termination conditions were achieved, for each of the input images in (a). The colors of the semidiscrete shapes at each partition element ω represent the fraction of the element (i.e., S̃(ω)/|ω|) that is covered by the shape (use the colorbar in Fig. 8 to interpret these colors).\ntioned before (in Section 6.3), the bounds computed for a partition element are tightest when either one of the BFs corresponding to the prior or the image are uniform in the element. This is because when one of the BFs is uniform, given the m-summary, there is no uncertainty about the location of the values of δ(x) in the element. Due to this fact, shapes that are uniform on elements of a coarser partition (referred below as “coarser” shapes) are processed faster than shapes that are uniform only on the elements of a finer partition (referred below as “finer” shapes). This statement is quantified in Table 4 which summarizes the results of five experiments.\nEach of these experiments is identical to Experiment 2, except that a different shape was used instead of the shape ‘A’. These shapes are depicted on the first column of Table 4. Note that changing the shape in these experiments did not affect the set of solutions found (in\n22\nall cases this set contains only the true solution and thus µt = σt = 0), but it did affect the amount of computation required in each case (see the column labeled ‘τ ’). In particular, note that since the shapes are roughly sorted from “coarser” to “finer” (as defined above), the total computation required correspondingly increases. Interestingly, to estimate the pose of the first shape (a square of size 128× 128), only one pair of bounds were computed (in constant time) per hypothesis (instead of processing the 16,384 pixels of the shape prior).\n7.2 Experiments on real data\nIn this section we apply the proposed framework to the problem of estimating the class, pose, and segmentation of a shape (a letter in this case), for each image in a set of testing images. In order to do this, as before, we define multiple hypotheses and use the proposed framework to find the group of hypotheses that best explains each test image.\nMore specifically, one hypothesis is defined for each transformation (described by (52)) and for each class. To save memory, and since it was shown before that hypotheses that are far from the ground truth are easily discarded, we only considered integer translations (around the ground truth) of up to 5 pixels in each direction (i.e., tx, ty ∈ {−5,−4, . . . , 5}), and scalings (around the ground truth) by multiples of 4% (i.e., sx, sy ∈ {0.92, 0.96, . . . , 1.08}). In contrast with the experiments presented in the previous section where only one class was considered (and hence there was no need to estimate it), in this section multiple classes are considered and estimating the class of the shape is part of the problem. We consider 6 different priors for each letter of the English alphabet, giving a total of 26×6 = 156 different priors (and classes). This results in an initial set of hypotheses containing 471,900 hypotheses (121 translations × 25 scalings × 156 priors). To construct the priors for each letter, we compiled a set of training shapes from the standard set of fonts in the Windows operating system. This set was pruned by discarding italicized fonts and other fonts that were considered outliers (e.g., wingdings). The remaining shapes were then divided into six subsets using k -medoids clustering [12], where the distance d(S1, S2) between two shapes S1 and S2 was defined as the number of different pixels between the two shapes (i.e.,\nd(S1, S2) , |S1∪S2 \\S1∩S2|) after “proper alignment” (this “alignment” is described below). It can be shown that in the case of 0/1 priors (“shapes”), this distance is equivalent to the evidence defined in (39). Then all the letters in each cluster were aligned with respect to the medoid of the cluster by finding the optimal translation and scaling that minimizes the distance d mentioned above, and (13) was used to compute the prior corresponding to the cluster. The six priors corresponding to the letters ‘A’ and ‘T’ are shown in Fig. 14. The number of priors per class (six) was chosen to maximize the classification performance P1 (defined below). We also tested having a variable number of clusters per class, but doing so did not improve the classification performance. The resulting priors used in the following experiments, as well as the set of training shapes used to construct them, can be downloaded from [30].\nThe testing images, on the other hand, were ob-\ntained by extracting rectangular regions (containing each a single letter) from old texts exhibiting decoloration and ink bleedthough, among other types of noise (see examples in Fig. 15 and in the images of these old texts in [30]). The corresponding BFs were obtained from these images by background subtraction using (12). The probability density functions (pdf’s) p(f |q = 0) and p(f |q = 1) in that equation were learned from sets of pixels in the background and foreground that were manually selected for each image. In this case we learned a single pdf for the background, and a single pdf for the foreground (not a different pdf for each pixel). While more sophisticated methods could have been used to obtain these BFs, this is not the goal of this article, and in fact, noisy BFs are desirable to test the robustness of the approach. This testing set, containing 218 letters, is used to measure the classification performance of the proposed framework. It is important to note that the testing and training sets come from different sources (i.e., there is no overlap between the testing and training sets).\nSince the proposed approach does not necessarily associate a single class to each testing image (because the set of solutions obtained for an image might contain solutions of different classes), we report the performance of the framework with a slight modification of\n23\nFig. 15 Sample letters in the testing set (top) and their corresponding BFs (bottom), obtained by background subtraction using (12). Note the different types of noise present and the different typographies.\nFig. 16 Confusion matrices for β = 0 (left), β = 0.5 (middle), and β = 1 (right). Use the colorbar in Fig. 8 to interpret the colors.\nthe traditional indicators. An element (i, j) in the confusion matrix C0 (in Fig. 16) indicates the fraction of solutions of class j (among all the solutions) obtained for all testing images of class i. It is also of interest to know what the classification performance is when only the best solutions are considered. For this purpose we define the confusion matrix Cβ (0 ≤ β ≤ 1) as before, but considering only the solutions whose upper bound is greater or equal than γβ , where γβ , L+β(L−L) and L and L are the maximum lower and upper bounds, respectively, of any solution. Note that γ0 = γ = L when γ is defined as in Section 3, and hence all the solutions are considered, and that γ1 = L and hence only the solution with the largest upper bound is considered. The confusion matrices C0.5 and C1 are also shown in Fig. 16.\nSimilarly, the total classification performance, Pβ , is defined as the number of solutions of the correct class (accumulated over all the images in the testing set), divided by the total number of solutions. As above, the solutions considered are only those whose upper bound is greater or equal than γβ . The total performance obtained in this case was P0 = 82.5%, P0.5 = 86.5% and P1 = 90.4%. As expected, when only the best solutions are considered, the performance improves (i.e., P1 ≥ P0.5 ≥ P0). For completeness, Table 5 summarizes the average pose estimation “errors” obtained in this case. The quantities |A|, µt, σt, µS , and σS in this table were respectively obtained as the average of the quantities |A|, µt, σt, µS , and σS (defined before) over all images in the testing set. In contrast with the synthetic experiments reported in Section 7.1, for the experiments reported\n(a)\n(b)\n(c)\nFig. 17 Examples of segmentations obtained in this experiment. (a) The original BFs computed from the input images. (b) Discrete shapes estimated for each of the BFs in (a). (c) Semidiscrete shapes estimated for each of the BFs in (a). As in Fig. 13, the colors in these semidiscrete shapes represent the fraction of each partition element that is “covered” by the shape.\nin this section the ground truth pose is not available. For this reason, and for lack of a universally trusted definition of the true pose (which is ill defined when the shapes to be aligned are different and unknown), in these experiments the ground truth pose was defined by the center (for the location) and the size (for the scale) of a manually defined bounding box around each letter. Therefore, the errors in Table 5 must be analyzed with caution, since it is not clear that this arbitrary defintion of the ground truth should be preferred over the solutions returned by the framework. In fact, in all the inspected cases (e.g., see Fig. 18), we did not observe a clear “misalignment.”\nNote in Table 5 that the average number of solutions per image (271) is only a small fraction (0.06%) of the total number of hypotheses (471,900). Moreover, these solutions are (in general) concentrated near the “ground truth” defined above (judging from the mean “errors” in the table).\nTo illustrate the types of segmentations obtained by the framework, Fig. 17 shows the shapes estimated for some of the images in the testing set. Note how most of the “noise” has been eliminated.\nIn order to understand why misclassifications occurr, let us look at some solutions found by the proposed approach. Each column in Fig. 18 shows two solutions found for a single testing image. In the top row a solution of the correct class is depicted, while in the\n24\nbottom row a solution from a different class is depicted. It can be seen in the figure that when the fit of the correct solution is not very good, solutions of an incorrect class cannot be discarded. This happens in the case of rotated or distorted letters (e.g., ‘D’ and ‘N’ in the figure) and in cases where there is not a good fit in the database of priors (e.g., ‘F’ in the figure). These erros can possibly be overcome by considering richer transformations and/or more numerous priors, respectively (more on this in the next section)."
    }, {
      "heading" : "8 Conclusions",
      "text" : "This article presented a new type of algorithms, namely self-conscious algorithms, to significantly trim down the amount of computation required to select the hypothesis that best explains an input (e.g., an image), from a set of predefined hypotheses. These algorithms are specifically targeted to perceptual problems in which the large size of the input imposes that these problems be solved by “paying attention” to a small fraction of the input only.\nThese algorithms have two important properties: 1) they are guaranteed to find the set of optimal hypotheses; and 2) the total amount of computation required by them is decoupled from the “resolution” of the input, and only depends on the current problem.\nAfter describing the general paradigm of SC algorithms, we instantiated this paradigm to the problem of simultaneously estimating the class, pose, and a denoised version of a 2D shape in a 2D image, by incorporating prior knowledge about the possible shapes that can be found in the image. In order to instantiate the paradigm, we developed a novel theory of shapes and shape priors that allowed us to develop the BM for this particular problem.\nWe consider that the theory and algorithms proposed here are not just limited to solve the problem described in this article, but rather that they are general enough as to find application in many other domains.\nIn fact, the application of the exact same FoAM (described in Section 3) and theory of shapes (described in Section 5) have already been instantiated to solve the problem of simultaneously estimating the class, pose, and a 3D reconstruction of an object from a single 2D image [32].\nEven though the contributions of this work are mainly theoretical, we showed preliminary results of the proposed framework on a real problem. Nevertheless, we acknowledge that to provide state-of-the-art results the framework needs to be extended in several directions (which is beyond the scope of the current article). For example, a limitation of the current framework is in its ability to deal with classes of shapes that are too diverse. These classes result in priors with large parts whose probabilities are not close to 0 or 1, and thus the bounds obtained are not tight, reducing the efficacy of the proposed method. These unspecific priors are obtained, for example, when the parts of the shapes in the class are not in consistent positions. To address this problem, in Section 7.2, we divided highly variable classes into subclasses with less variability. This addressed the problem created by the highly variable classes, at the expense of creating many more hypotheses.\nAnother direction to extend this work that addresses the growing number of hypotheses mentioned above, is to exploit the redundancy among these hypotheses. It was explained in Section 7.1 that to prove that two (or more) hypotheses are indistinguishable, it is necessary to refine these hypotheses completely. By doing so, however, the performance drops significantly. This problem gets exacerbated as the number of indistinguishable hypotheses increases, which in turn happens when classes are divided in subclasses or when the number of degrees of freedom are increased. In order to exploit the redundancy among hypotheses, bounds can be computed not just for individual hypotheses but for groups of them, a la Branch & Bound. This new algorithm would then work by splitting not just the image domain (as SC algorithms do), but also the space of hypotheses (as Branch & Bound algorithms do). This new algorithm is expected to be most effective when the hypotheses are very similar, which is exactly the case that the current approach is less efficient dealing with. [35].\nAcknowledgements We would like to thank Siddharth Mahendran for his help running part of the experiments.\n25"
    }, {
      "heading" : "9 Supplementary material",
      "text" : "Lemma 1 (Properties of LCDFs and m-Summaries) Let Π be an arbitrary partition, and let S̃ and D = {Dω}ω∈Π be respectively a semidiscrete shape and a LCDF in this partition. Then:\n1. Given a BF B such that B ∼ D, it holds that (Fig. 6)\nsup S∼S̃ ∫ Ω δB(x)S(x) dx = ∑ ω∈Π ∫ |ω| |ω|−S̃(ω) D−1ω (a) da. (29)\n2. Similarly, given a continuous shape S, such that\nS ∼ S̃, it holds that\nsup B∼D ∫ Ω δB(x)S(x) dx = ∑ ω∈Π ∫ |ω| |ω|−S̃(ω) D−1ω (a) da.\n(30)\n3. Moreover, for any α ∈ [0, |ω|], the integrals on the rhs of (29) and (30) can be bounded as∫ |ω| α D−1ω (a) da ≤ δmax m J(α)(Ỹ J(α)ω − α)+ m−1∑ j=J(α) ( Ỹ j+1ω − Ỹ jω ) (j + 1)\n , (31) where J(α) is as defined in (22).\n4. The rhs of (29) and (30) are maximum when\nS̃(ω) = |ω| −Aω, thus\nsup S̃ ∑ ω∈Π ∫ |ω| |ω|−S̃(ω) D−1ω (a) da = ∑ ω∈Π ∫ |ω| Aω D−1ω (a) da.\n(32)\nProof 1. For simplicity, let us only consider the case in which the partition Π consists of a single element ω; the generalization of the proof to partitions containing multiple elements is straightforward. Let us assume for the moment that Dω(δ) is strictly increasing and con-\ntinuous, so that Dω ( D−1ω (a) ) = a ∀a ∈ [0, |ω|].\nConsider the continuous shape S∗, defined as\nS∗(x) , 1 ( δB(x)− δ̃ ) = { 1, if δB(x) ≥ δ̃ 0, otherwise,\n(55)\nwhere 1 (·) is the Heaviside step function and δ̃ , D−1ω( |ω| − S̃(ω) ) . This shape is the solution to the lhs of\n(29). To see this, notice that S∗ ∼ S̃ because∫ ω S∗(x) dx = ∣∣∣{x ∈ ω : δB(x) ≥ δ̃}∣∣∣\n= |ω| −Dω ( δ̃ ) = S̃(ω). (56)\nNotice also that S∗ maximizes the integral on the lhs of (29) because it contains the parts of ω with the highest values of δB.\nThus, substituting (55) on the lhs of (29), and defin-\ning the function g(x) , δB(x)− δ̃, (29) reduces to∫ ω δB(x)1 ( δB(x)− δ̃ ) dx =∫\nω\ng(x)1 (g(x)) dx + δ̃ ∫ ω 1 (g(x)) dx. (57)\nUsing Lebesgue integration [43], this last expression\nis equivalent to ∫ ∞ 0 |{x ∈ ω : g(x) ≥ δ}| dδ+\nδ̃ |{x ∈ ω : g(x) ≥ 0}| =∫ ∞ 0 ∣∣∣{x ∈ ω : δB(x) ≥ δ + δ̃}∣∣∣ dδ+ δ̃ ∣∣∣{x ∈ ω : δB(x) ≥ δ̃}∣∣∣ =∫ ∞\n0\n[ |ω| −Dω(δ + δ̃) ] dδ + δ̃ ( |ω| −Dω(δ̃) ) =∫ δmax\nδ̃\n[|ω| −Dω(δ)] dδ + δ̃S̃(ω). (58)\nNow recall that the integral of a function f(x) can be written in terms of its inverse f−1(y), if it exists, as [35]∫ b a f(x) dx = xf(x)|ba − ∫ f(b) f(a) f−1(y) dy. (59)\nHence, it follows that (58) is equal to\n|ω|(δmax − δ̃)− δmaxDω(δmax) + δ̃Dω(δ̃)+∫ Dω(δmax) Dω(δ̃) D−1ω (a) da+ δ̃S̃(ω) = |ω|(δmax − δ̃)− δmax|ω|+ δ̃(|ω| − S̃(ω))+∫ |ω| |ω|−S̃(ω)\nD−1ω (a) da+ δ̃S̃(ω) =∫ |ω| |ω|−S̃(ω) D−1ω (a) da, (60)\nas we wanted to prove.\nIf Dω(δ) is not one-to-one (as we assumed above), the proof is essentially similar but complicated by the fact that S∗ can be chosen in more than one way (because there are sets θ ⊂ ω, with |θ| > 0, where δB is constant) and this has to be handled explicitly. ut\n2. To prove 2 we proceed exactly as in 1, except that in this case we choose δB to have its largest values in S, instead of choosing S to be supported where the largest values of δB are. ut\n27\n3. From (21), it follows that∫ |ω| α D−1ω (a) da ≤ δmax m ∫ |ω| α J(a) da =\nδmax m ∫ Ỹ J(α)ω α J(α) da+ m−1∑ j=J(α) ∫ Ỹ j+1ω Ỹ jω (j + 1) da  . (61)\nAnd because the integrands on the rhs of (61) are all constant, (31) follows. ut 4. It is clear that each integral on the lhs of (32) is maximum when the integration domain contains only the parts where D−1B,ω(a) is positive or zero. Therefore, from (19), each integral is maximum when computed in the interval [AB,ω, |ω|], which yields the rhs of (32), proving the lemma. ut\nTheorem 1 (Lower bound for L(H)) Let Π be a partition, and let Ŷf = { Ŷf,ω } ω∈Π and ŶH = { ŶH,ω } ω∈Π be the mean-summaries of two unknown BFs in Π. Then, for any Bf ∼ Ŷf and any BH ∼ ŶH , it holds that L(H) ≥ LΠ(H), where\nLΠ(H) , ZBH + ∑ ω∈Π Lω(H), (40)\nLω(H) , ( Ŷf,ω + ŶH,ω ) q̂∗(ω), (41)\nand q̂∗ is a discrete shape in Π defined as\nq̂∗(ω) ,\n{ 1, if ( Ŷf,ω + ŶH,ω ) > 0\n0, otherwise. (42)\nProof Since the set of continuous shapes q that are compatible with the discrete shape q̂ is a subset of the set of all continuous shapes (from (15)), it holds that\nL(H) ≥ ZBH + max q̂ [ sup q∼q̂ ∫ Ω ( δBf (x) + δBH (x) ) q(x) dx ] (62)\n= ZBH + max q̂ ∑ ω∈Π q̂(ω) ∫ ω ( δBf (x) + δBH (x) ) dx.\n(63)\nThe terms in the integral in (63) are, by definition,\nthe mean-summaries of the BFs (see (23)), thus\nL(H) ≥ ZBH + max q̂ ∑ ω∈Π q̂(ω) ( Ŷf,ω + ŶH,ω ) . (64)\nIt then follows that the discrete shape defined in (42) maximizes the rhs of (64), proving the theorem. ut\nTheorem 2 (Upper bound for L(H)) Let Π be a partition, and let Ỹf = { Ỹf,ω } ω∈Π and ỸH = { ỸH,ω } ω∈Π be the m-summaries of two unknown BFs in Π. Let Ỹf ⊕ H,ω (ω ∈ Π) be a vector of length 4m+ 2 obtained by sorting the values in Ỹf,ω and ỸH,ω (in ascending order), keeping repeated values, i.e.,\nỸf ⊕ H,ω , [ Ỹ 1f ⊕ H,ω, . . . , Ỹ 4m+2 f ⊕ H,ω ] , SortAscending ( Ỹf,ω ∪ ỸH,ω ) . (43)\nThen, for any Bf ∼ Ỹf and any BH ∼ ỸH , it holds that L(H) ≤ LΠ(H), where LΠ(H) , ZBH + ∑ ω∈Π Lω(H), and (44)\nLω(H) , δmax m 4m+1∑ j=2m+1 (j − 2m) ( Ỹ j+1f ⊕ H,ω − Ỹ j f ⊕ H,ω ) .\n(45)"
    }, {
      "heading" : "It also follows that the continuous shape that maximizes",
      "text" : "(39) is equivalent to a semidiscrete shape q̃∗ in Π that satisfies q̃∗(ω) ∈ [ |ω| − Ỹ 2m+1f⊕H,ω, |ω| − Ỹ 2mf⊕H,ω ] ∀ω ∈ Π. (46)\nProof Let Df and DH be two arbitrary cumulative distributions such that Df ∼ Ỹf and DH ∼ ỸH . Then,\nL(H) ≤ ZBH+\nsup B′f∼Df B′H∼DH sup q\n∫ Ω q(x) ( δB′f (x) + δB′H (x) ) dx, (65)\nand from (16), this expression is equal to\n= ZBH + sup B′f∼Df B′H∼DH sup q̃∈S̃(Π)\nsup q∼q̃ ∫ Ω ( δB′f (x) + δB′H (x) ) q(x) dx. (66)\nExchanging the order of the sup operations and us-\ning (30), the rhs of (66) is less or equal than\nZBH + sup q̃∈S̃(Π) [ sup q∼q̃ [ sup B′f∼Df ∫ Ω δB′f (x)q(x) dx+\nsup B′H∼DH ∫ Ω δB′H (x)q(x) dx ]] =\n28\nZBH + sup q̃∈S̃(Π) [∑ ω∈Π ∫ |ω| |ω|−q̃(ω)\nD−1f,ω(a) da+∫ |ω| |ω|−q̃(ω) D−1H,ω(a) da ] =\nZBH + ∑ ω∈Π sup q̃(ω) ∫ |ω| |ω|−q̃(ω) ( D−1f,ω(a) +D −1 H,ω(a) ) da.\n(67)\nSince Df ∼ Ỹf and DH ∼ ỸH , it follows from (21) and (22) that\nD−1f,ω(a) +D −1 H,ω(a) ≤\nδmax m (∣∣∣{j : Ỹ jf,ω < a}∣∣∣+ ∣∣∣{j : Ỹ jH,ω < a}∣∣∣− 2m) , (68)\nwhich using (43) can be rewritten as\nD−1f ⊕ H,ω(a) , δmax m (∣∣∣{j : Ỹ jf⊕H,ω < a}∣∣∣− 2m) . (69)\nTherefore, it follows that∫ |ω| Ỹ k f ⊕ H,ω D−1f ⊕ H,ω(a) da =\n= δmax m 4m+1∑ j=k ∫ Ỹ j+1 f ⊕ H,ω Ỹ j f ⊕ H,ω (j − 2m) da =\n= δmax m 4m+1∑ j=k (j − 2m) ( Ỹ j+1f ⊕ H,ω − Ỹ j f ⊕ H,ω ) . (70)\nSubstituting this last expression into (67) yields the\nbound\nL(H) ≤ ZBH + δmax m ∑ ω∈Π sup k∈{1,...,4m+1}\n4m+1∑ j=k (j − 2m) ( Ỹ j+1f ⊕ H,ω − Ỹ j f ⊕ H,ω ) .\n(71)\nSince D−1f ⊕ H,ω(a) is a non-decreasing, piecewise con-\nstant function that takes the zero value when a ∈ ( Ỹ 2mf ⊕ H,ω, Ỹ 2m+1 f ⊕ H,ω], it follows that the semidiscrete shape in (46) maximizes (67), and that the supremum in (71) is obtained when k = 2m+ 1, proving the theorem. ut"
    } ],
    "references" : [ {
      "title" : "A cognitive approach to multimodal attention",
      "author" : [ "R. Arrabales", "A. Ledezma", "A. Sanchis" ],
      "venue" : "Journal of Physical Agents 3",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Springer, New York",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Identification of 3d objects from multiple silhouettes using quadtrees/octrees",
      "author" : [ "C.H. Chien", "J.K. Aggarwal" ],
      "venue" : "Computer Vision, Graphics, and Image Processing 36",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Branch and bound algorithms-principles and examples",
      "author" : [ "J. Clausen" ],
      "venue" : "Parallel Computing in Optimization pp. 239– 267",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Statistical Shape Analysis",
      "author" : [ "I.L. Dryden", "K.V. Mardia" ],
      "venue" : "John Wiley and Sons Ltd.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Occupancy grids: a probabilistic framework for robot perception and navigation",
      "author" : [ "A. Elfes" ],
      "venue" : "PhD Dissertation, Carnegie Mellon University",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Shape Classification and Analysis: Theory and Practice, 2nd edition edn",
      "author" : [ "L. da F. Costa", "R.M. Cesar" ],
      "venue" : "CRC Press",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Computational visual attention systems and their cognitive foundations: A survey",
      "author" : [ "S. Frintrop", "E. Rome", "H.I. Christensen" ],
      "venue" : "ACM Trans. Appl. Percept. 7",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Forty years of research in character and document recognition: An industrial perspective",
      "author" : [ "H. Fujisawa" ],
      "venue" : "Pattern Recognition 41, 2435–2446",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An active testing model for tracking roads in satellite images",
      "author" : [ "D. Geman", "B. Jedynak" ],
      "venue" : "IEEE Pattern analysis and Machine Intelligence 18, 1–14",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "The elements of statistical learning: data mining, inference and prediction, 2nd edn",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : "Springer",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Topology: An Introduction to the Point-Set and Algebraic Areas",
      "author" : [ "D.W. Kahn" ],
      "venue" : "Dover Publications, Inc.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Attention and Effort",
      "author" : [ "D. Kahneman" ],
      "venue" : "Prentice-Hall",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "The challenges of joint attention",
      "author" : [ "F. Kaplan", "V. Hafner" ],
      "venue" : "Interaction Studies 7, 135–169",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fundamentals of Statistical Signal Processing: Estimation Theory",
      "author" : [ "S.M. Kay" ],
      "venue" : "Prentice Hall PTR",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Shifts in selective visual attention: towards the underlying neural circuitry",
      "author" : [ "C. Koch", "S. Ullman" ],
      "venue" : "Human Neurobiology 4, 219–227",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : "The MIT Press",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Local computations with probabilities on graphical structures and their application to expert systems",
      "author" : [ "S.L. Lauritzen", "D.J. Spiegelhalter" ],
      "venue" : "J. of the Royal Statistical Society 50, 157–224",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Scale-Space Theory in Computer Vision",
      "author" : [ "T. Lindeberg" ],
      "venue" : "Kluwer Academic Publishers",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Making decisions",
      "author" : [ "D. Lindley" ],
      "venue" : "John Wiley",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Motion-based background subtraction using adaptive kernel density estimation",
      "author" : [ "A. Mittal", "N. Paragios" ],
      "venue" : "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multiscale 3d shape analysis using spherical wavelets",
      "author" : [ "D. Nain", "S. Haker", "A. Bobick", "A. Tannenbaum" ],
      "venue" : "Proc. 8th Int. Conf. Medical Image Computing Computer-Assisted Intervention pp. 459–467",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Top-down control of visual attention in object detection",
      "author" : [ "A. Oliva", "A. Torralba", "M.S. Castelhano", "J.M. Henderson" ],
      "venue" : "International Conference on Image Processing pp. 253–256",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Probabilistic reasoning in intelligent systems: networks of plausible inference",
      "author" : [ "J. Pearl" ],
      "venue" : "Morgan Kaufmann Publishers, Inc.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Shape discrimination using fourier descriptors",
      "author" : [ "E. Persoon", "K.S. Fu" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence 8, 388 – 397",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Generating octree models of 3d objects from their silhouettes in a sequence of images",
      "author" : [ "M. Potmesil" ],
      "venue" : "Computer Vision, Graphics, and Image Processing 40, 1–29",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "A comparative study of parallel and sequential priority queue algorithms",
      "author" : [ "R. Rönngren", "R. Ayani" ],
      "venue" : "ACM Trans. Model. Comput. Simul. 7",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Principles of categorization",
      "author" : [ "E. Rosch" ],
      "venue" : "E.R..B.B. Lloyd (ed.) Cognition and categorization. Erlbaum",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Supplementary data",
      "author" : [ "D. Rother" ],
      "venue" : "http://www.cis.jhu. edu/~diroth/Research/DataShapes.zip",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Seeing 3d objects in a single 2d image",
      "author" : [ "D. Rother", "G. Sapiro" ],
      "venue" : "Proceedings of the 12th IEEE International Conference on Computer Vision",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A computational focus of attention mechanism to process shapes efficiently: Application",
      "author" : [ "D. Rother", "R. Vidal" ],
      "venue" : "(In preparation)",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An overview of quadtrees, octrees, and related hierarchical data structures",
      "author" : [ "H. Samet" ],
      "venue" : "NATO ASI Series 40, 51–68",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Non-rigid 2d-3d pose estimation and 2d image segmentation",
      "author" : [ "R. Sandhu", "S. Dambreville", "A. Yezzi", "A. Tannenbaum" ],
      "venue" : "Proc. of the IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A formula for integrating inverse functions",
      "author" : [ "S. Schnell", "C. Mendoza" ],
      "venue" : "The Mathematical Gazette",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A behavioral analysis of computational models of visual attention",
      "author" : [ "F. Shic", "B. Scassellati" ],
      "venue" : "International Journal of Computer Vision 73, 159–177",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Gorillas in our midst: sustained inattentional blindness for dynamic events",
      "author" : [ "D.J. Simons", "C.F. Chabris" ],
      "venue" : "Perception 28, 1059–1074",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Active testing for face detection and localization",
      "author" : [ "R. Sznitman", "B. Jedynak" ],
      "venue" : "IEEE Pattern analysis and Machine Intelligence 32, 1914–1920",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A brief and selective history of attention",
      "author" : [ "J. Tsotsos", "L. Itti", "G. Rees" ],
      "venue" : "L. Itti, G. Rees, J. Tsotsos (eds.) Neurobiology of Attention. Elsevier Press",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Shape matching: Similarity measures and algorithms",
      "author" : [ "R.C. Veltkamp" ],
      "venue" : "Shape Modelling International pp. 188– 197",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Rapid object detection using a boosted cascade of simple features",
      "author" : [ "P. Viola", "M. Jones" ],
      "venue" : "Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, Hawaii, December",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "On the optimality of solutions of the max-product belief-propagation algorithms in arbitrary graphs",
      "author" : [ "Y. Weiss", "W. Freeman" ],
      "venue" : "IEEE Trans. on Information Theory 47, 736–744",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "An Introduction to Lebesgue Integration and Fourier Series",
      "author" : [ "H.J. Wilcox", "D.L. Myers" ],
      "venue" : "Dover Publications, Inc.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "A fast o(n) implementation of the fast marching algorithm",
      "author" : [ "L. Yatziv", "A. Bartesaghi", "G. Sapiro" ],
      "venue" : "Journal of Computational Physics 212",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 35,
      "context" : "(b) Attention is task oriented: when focused on the task of counting the passes of a ball between the people in this video, many humans fail to see the gorilla in the midle of the frame (image from [37], see details in that article).",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 12,
      "context" : "a limited resource that, when demanded by one task, is unavailable for another [14].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 35,
      "context" : "1b) [37].",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 27,
      "context" : "‘dog’), if this were more expensive than classifying the object at a more abstract level, and if this provided the same amount of relevant information towards the goal [29].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 30,
      "context" : "Another instantiation of SC algorithms, which we describe in [32], tackles the more complex problem of simultaneous object classification, pose estimation, and 3D reconstruction, from a single 2D image.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 30,
      "context" : "This will be essential in the second part of this work [32] to efficiently compute how well a given 3D reconstruction “explains” the input image.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 30,
      "context" : "In the continuation of this work [32], we extend the theory presented in this article to deal with a more complex problem involving not just 2D shapes, but also 3D shapes and their 2D projections.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 7,
      "context" : "Biologically inspired approaches [9,39,36], by definition, exploit characteristics of a model proposed to describe a biological system.",
      "startOffset" : 33,
      "endOffset" : 42
    }, {
      "referenceID" : 37,
      "context" : "Biologically inspired approaches [9,39,36], by definition, exploit characteristics of a model proposed to describe a biological system.",
      "startOffset" : 33,
      "endOffset" : 42
    }, {
      "referenceID" : 34,
      "context" : "Biologically inspired approaches [9,39,36], by definition, exploit characteristics of a model proposed to describe a biological system.",
      "startOffset" : 33,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : ", joint attention [15]).",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : ", [1]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 3,
      "context" : "In a B&B algorithm [4], as in a SC algorithm, an objective function is defined over the hypothesis space and the goal of the algorithm is to select the hypothesis that maximizes this function.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 9,
      "context" : "In an EP algorithm [11,38] a probability distribution is defined over the hypothesis space.",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 36,
      "context" : "In an EP algorithm [11,38] a probability distribution is defined over the hypothesis space.",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : ", [17]) are also known as data-driven approaches, while top-down approaches (e.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 22,
      "context" : ", [24]) are also known as task-driven approaches.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "Even though the significance of top-down signals in biological systems is well known, most current computer systems only consider bottom-up signals [9].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 16,
      "context" : "Many methods have been proposed to perform inference in graphical models [18].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : ", a polytree) [2].",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 40,
      "context" : "Moreover, the method is not guaranteed to converge to the optimal solution in every graph but only in some types of graphs [42].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "In the JT algorithm [19], BP is run on a modified graph whose cycles have been eliminated.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : ", for each hypothesis) [25].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "Since shape representations and priors are such essential parts of many vision systems, over the years many shape representations and priors have been proposed (see reviews in [8,6]).",
      "startOffset" : 176,
      "endOffset" : 181
    }, {
      "referenceID" : 4,
      "context" : "Since shape representations and priors are such essential parts of many vision systems, over the years many shape representations and priors have been proposed (see reviews in [8,6]).",
      "startOffset" : 176,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "In the scale-space representation [20], a shape (or image in general) is represented as a oneparameter family of smoothed shapes, parameterized by the size of the smoothing kernel used for suppressing fine-scale structures.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "For example, in [23] and [26] the contour of a shape is expressed in spherical wavelets and Fourier bases, respectively, and in [34] the signed distance function of a shape is written in terms of the principal components of the signed distance functions of shapes in the training database.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 24,
      "context" : "For example, in [23] and [26] the contour of a shape is expressed in spherical wavelets and Fourier bases, respectively, and in [34] the signed distance function of a shape is written in terms of the principal components of the signed distance functions of shapes in the training database.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 32,
      "context" : "For example, in [23] and [26] the contour of a shape is expressed in spherical wavelets and Fourier bases, respectively, and in [34] the signed distance function of a shape is written in terms of the principal components of the signed distance functions of shapes in the training database.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 31,
      "context" : "5) are respectively closer to region quadtrees/octrees [33] and to occupancy grids [7].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "5) are respectively closer to region quadtrees/octrees [33] and to occupancy grids [7].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "Quadtrees and octrees have been previously used for 3D recognition [3] and 3D reconstruction [27] from multiple silhouettes (not from a single one, to the best of our knowledge, as we do in [32]).",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 25,
      "context" : "Quadtrees and octrees have been previously used for 3D recognition [3] and 3D reconstruction [27] from multiple silhouettes (not from a single one, to the best of our knowledge, as we do in [32]).",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 30,
      "context" : "Quadtrees and octrees have been previously used for 3D recognition [3] and 3D reconstruction [27] from multiple silhouettes (not from a single one, to the best of our knowledge, as we do in [32]).",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 29,
      "context" : "Rother and Sapiro [31] have suggested a different heuristic to select the next hypothesis to refine.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 29,
      "context" : "Therefore, if we chose to refine H1 (as in [31]) many more cycles will be necessary to distinguish between the hypotheses than if we had chosen to refine H2 (as in the heuristic explained above).",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "This problem arises, for example, in the context of optical character recognition [10] and shape matching [40].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 38,
      "context" : "This problem arises, for example, in the context of optical character recognition [10] and shape matching [40].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 1,
      "context" : "3 (see [2] for more details on factor graphs).",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 1,
      "context" : "A factor graph, [2], has a variable node (circle) for each variable, and a factor node (square) for each factor in the system’s joint probability.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 19,
      "context" : "success rates of 0 or 1, following Cromwell’s rule [21], we will only consider success rates in the open interval (ε, 1−ε).",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "To compute a discrete BF B̂f using the Background Subtraction technique [22], recall the probability densities pΩi (f(Ωi)|q(Ωi) = 0) and pΩi (f(Ωi)|q(Ωi) = 1) defined in Section 4 to model the probability of observing a feature f(Ωi) at a given pixel Ωi, depending on the pixel’s state q(Ωi).",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "mula for the estimation of Bernoulli distributions [16],",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "To avoid pathological cases, we will require the set S to satisfy two regularity conditions: 1) to be open (in the usual topology in R [13]) and 2) to have a boundary (as defined in [13]) of measure zero.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "To avoid pathological cases, we will require the set S to satisfy two regularity conditions: 1) to be open (in the usual topology in R [13]) and 2) to have a boundary (as defined in [13]) of measure zero.",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 41,
      "context" : "We will only consider in this work functions pB(x) such that |ZB | < ∞ and δB(x) is a measurable function [43].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 39,
      "context" : "The sum on the rhs of (25) can be computed in constant time by relying on integral images [41], an image representation precisely proposed to compute sums in rectangular domains in constant time.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 30,
      "context" : "This framework is used in the next section to bound the evidence for the problem defined in Section 4, and also in [32] to bound the evidence for a more complex problem.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 26,
      "context" : "Taken together these steps have, depending on the implementation of the queue, complexity of at least O(log hk) (where hk is the number of elements in the partition) [28].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 42,
      "context" : "[44], in which the operations GetMax and Insert both have complexity O(1).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "When adding Gaussian noise to a BF some values end up outside the interval [0, 1].",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "The remaining shapes were then divided into six subsets using k -medoids clustering [12], where the distance d(S1, S2) between two shapes S1 and S2 was defined as the number of different pixels between the two shapes (i.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 28,
      "context" : "The resulting priors used in the following experiments, as well as the set of training shapes used to construct them, can be downloaded from [30].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 28,
      "context" : "15 and in the images of these old texts in [30]).",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 30,
      "context" : "In fact, the application of the exact same FoAM (described in Section 3) and theory of shapes (described in Section 5) have already been instantiated to solve the problem of simultaneously estimating the class, pose, and a 3D reconstruction of an object from a single 2D image [32].",
      "startOffset" : 277,
      "endOffset" : 281
    }, {
      "referenceID" : 33,
      "context" : "[35].",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "Given the ever increasing bandwidth of the visual sensory information available to autonomous agents and other automatic systems, it is becoming essential to endow them with a sense of what is worthwhile their attention and what can be safely disregarded. This article presents a general mathematical framework to efficiently allocate the available computational resources to process the parts of the input that are relevant to solve a perceptual problem of interest. By solving a perceptual problem we mean to find the hypothesis H (i.e., the state of the world) that maximizes a function L(H), referred to as the evidence, representing how well each hypothesis “explains” the input. However, given the large bandwidth of the sensory input, fully evaluating the evidence for each hypothesis is computationally infeasible (e.g., because it would imply checking a large number of pixels). To address this problem we propose a mathematical framework with two key ingredients. The first one is a Bounding Mechanism (BM) to compute lower and upper bounds of the evidence of a hypothesis, for a given computational budget. These bounds are much cheaper to compute than the evidence itself, can be refined at any time by increasing the budget allocated to a hypothesis, and are frequently sufficient to discard a hypothesis. The second ingredient is a Focus of Attention Mechanism (FoAM) to select which hypothesis’ bounds should be refined next, with the goal of discarding non-optimal hypotheses with the least amount of computation. D. Rother · R. Vidal Johns Hopkins University Tel.: +1-410-516-6736 E-mail: diroth@gmail.com S. Schütz University of Göttingen The proposed framework has the following desirable characteristics: 1) it is very efficient since most hypotheses are discarded with minimal computation; 2) it is parallelizable; 3) it is guaranteed to find the globally optimal hypothesis or hypotheses; and 4) its running time depends on the problem at hand, not on the bandwidth of the input. In order to illustrate the general framework, in this article we instantiate it for the problem of simultaneously estimating the class, pose and a noiseless version of a 2D shape in a 2D image. To do this, we develop a novel theory of semidiscrete shapes that allows us to compute the bounds required by the BM. We believe that the theory presented in this article (i.e., the algorithmic paradigm and the theory of shapes) has multiple potential applications well beyond the application demonstrated in this article.",
    "creator" : "LaTeX with hyperref package"
  }
}