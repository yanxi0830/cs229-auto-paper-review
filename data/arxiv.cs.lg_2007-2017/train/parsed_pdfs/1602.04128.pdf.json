{
  "name" : "1602.04128.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Coin Betting and Parameter-Free Online Learning",
    "authors" : [ "Francesco Orabona" ],
    "emails" : [ "francesco@orabona.com", "dpal@yahoo-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the KrichevskyTrofimov estimator. The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the Online Linear Optimization (OLO) Cesa-Bianchi and Lugosi [2006], Shalev-Shwartz [2011] setting. In each round t, an algorithm chooses a point wt from a convex decision set K and then receives a reward vector gt. The algorithm’s goal is to keep its regret small, defined as the difference between its cumulative reward and the cumulative reward of a fixed strategy u ∈ K, that is\nRegretT (u) = T∑ t=1 〈gt, u〉 − T∑ t=1 〈gt, wt〉 .\nWe focus on two particular decision sets, the N -dimensional probability simplex ∆N = {x ∈ RN : x ≥ 0, ‖x‖1 = 1} and a Hilbert space H. OLO over ∆N is referred to as the problem of Learning with Expert Advice (LEA). We assume bounds on the norms of the reward vectors: For OLO over H, we assume that ‖gt‖ ≤ 1, and for LEA we assume that gt ∈ [0, 1]N .\nOLO is a basic building block of many machine learning problems. For example, Online Convex Optimization (OCO), the problem analogous to OLO where 〈gt, u〉 is generalized to an arbitrary convex function `t(u), is solved through a reduction to OLO Shalev-Shwartz [2011]. LEA Littlestone and Warmuth [1994], Vovk [1998], Cesa-Bianchi et al. [1997] provides a way of combining classifiers and it is at the heart of boosting Freund and Schapire [1997]. Batch and stochastic convex optimization can also be solved through a reduction to OLO Shalev-Shwartz [2011].\nTo achieve optimal regret, most of the existing online algorithms require the user to set the learning rate (step size) η to an unknown/oracle value. For example, to obtain the optimal bound for Online Gradient Descent (OGD), the learning rate has to be set with the knowledge of the norm of the competitor u, ‖u‖; second entry in Table 1. Likewise, the optimal learning rate for Hedge depends on the KL divergence between the prior weighting π and the unknown competitor u, D (u‖π); seventh entry in Table 1. Recently, new parameter-free algorithms have been proposed, both for LEA Chaudhuri et al. [2009], Chernov and Vovk [2010], Luo and Schapire [2014, 2015], Koolen and van Erven [2015], Foster et al. [2015] and for OLO/OCO over Hilbert spaces Streeter and McMahan [2012], Orabona [2013], McMahan and Abernethy\nar X\niv :1\n60 2.\n04 12\n8v 4\n[ cs\n.L G\n] 4\nN ov\n2 01\n[2013], McMahan and Orabona [2014], Orabona [2014]. These algorithms adapt to the number of experts and to the norm of the optimal predictor, respectively, without the need to tune parameters. However, their design and underlying intuition is still a challenge. Foster et al. [2015] proposed a unified framework, but it is not constructive. Furthermore, all existing algorithms for LEA either have sub-optimal regret bound (e.g. extra O(log log T ) factor) or sub-optimal running time (e.g. requiring solving a numerical problem in every round, or with extra factors); see Table 1.\nContributions. We show that a more fundamental notion subsumes both OLO and LEA parameterfree algorithms. We prove that the ability to maximize the wealth in bets on the outcomes of coin flips implies OLO and LEA parameter-free algorithms. We develop a novel potential-based framework for betting algorithms. It gives intuition to previous constructions and, instantiated with the Krichevsky-Trofimov estimator, provides new and elegant algorithms for OLO and LEA. The new algorithms also have optimal worst-case guarantees on regret and time complexity; see Table 1."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We begin by providing some definitions. The Kullback-Leibler (KL) divergence between two discrete distributions p and q is D (p‖q) = ∑ i pi ln (pi/qi). If p, q are real numbers in [0, 1], we denote by D (p‖q) = p ln (p/q)+(1−p) ln ((1− p)/(1− q)) the KL divergence between two Bernoulli distributions with parameters p and q. We denote by H a Hilbert space, by 〈·, ·〉 its inner product, and by ‖·‖ the induced norm. We denote by ‖·‖1 the 1-norm in RN . A function F : I → R+ is called logarithmically convex iff f(x) = ln(F (x)) is convex. Let f : V → R ∪ {±∞}, the Fenchel conjugate of f is f∗ : V ∗ → R ∪ {±∞} defined on the dual vector space V ∗ by f∗(θ) = supx∈V 〈θ, x〉− f(x). A function f : V → R∪{+∞} is said to be proper if there exists x ∈ V such that f(x) is finite. If f is a proper lower semi-continuous convex function then f∗ is also proper lower semi-continuous convex and f∗∗ = f .\nCoin Betting. We consider a gambler making repeated bets on the outcomes of adversarial coin flips. The gambler starts with an initial endowment > 0. In each round t, he bets on the outcome of a coin flip gt ∈ {−1, 1}, where +1 denotes heads and −1 denotes tails. We do not make any assumption on how gt is generated, that is, it can be chosen by an adversary.\nThe gambler can bet any amount on either heads or tails. However, he is not allowed to borrow any additional money. If he loses, he loses the betted amount; if he wins, he gets the betted amount back and, in addition to that, he gets the same amount as a reward. We encode the gambler’s bet in round t by a single number wt. The sign of wt encodes whether he is betting on heads or tails. The absolute value encodes the betted amount. We define Wealtht as the gambler’s wealth at the end of round t and Rewardt as the\n1These algorithms require to solve a numerical problem at each step. The number K is the number of steps needed to reach the required precision. Neither the precision nor K are calculated in these papers.\n2The proof in Koolen and van Erven [2015] can be modified to prove a KL bound, see http://blog.wouterkoolen.info. 3A variant of the algorithm in Foster et al. [2015] can be implemented with the stated time complexity Foster [2016].\ngambler’s net reward (the difference of wealth and initial endowment), that is\nWealtht = + t∑ i=1 wigi and Rewardt = Wealtht− . (1)\nIn the following, we will also refer to a bet with βt, where βt is such that\nwt = βt Wealtht−1 . (2)\nThe absolute value of βt is the fraction of the current wealth to bet, and sign of βt encodes whether he is betting on heads or tails. The constraint that the gambler cannot borrow money implies that βt ∈ [−1, 1]. We also generalize the problem slightly by allowing the outcome of the coin flip gt to be any real number in the interval [−1, 1]; wealth and reward in (1) remain exactly the same."
    }, {
      "heading" : "3 Warm-Up: From Betting to One-Dimensional Online Linear",
      "text" : "Optimization\nIn this section, we sketch how to reduce one-dimensional OLO to betting on a coin. The reasoning for generic Hilbert spaces (Section 5) and for LEA (Section 6) will be similar. We will show that the betting view provides a natural way for the analysis and design of online learning algorithms, where the only design choice is the potential function of the betting algorithm (Section 4). A specific example of coin betting potential and the resulting algorithms are in Section 7.\nAs a warm-up, let us consider an algorithm for OLO over one-dimensional Hilbert space R. Let {wt}∞t=1 be its sequence of predictions on a sequence of rewards {gt}∞t=1, gt ∈ [−1, 1]. The total reward of the algorithm after t rounds is Rewardt = ∑t i=1 giwi. Also, even if in OLO there is no concept of “wealth”, define the wealth of the OLO algorithm as Wealtht = + Rewardt, as in (1). We now restrict our attention to algorithms whose predictions wt are of the form of a bet, that is wt = βt Wealtht−1, where βt ∈ [−1, 1]. We will see that the restriction on βt does not prevent us from obtaining parameter-free algorithms with optimal bounds.\nGiven the above, it is immediate to see that any coin betting algorithm that, on a sequence of coin flips {gt}∞t=1, gt ∈ [−1, 1], bets the amounts wt can be used as an OLO algorithm in a one-dimensional Hilbert space R. But, what would be the regret of such OLO algorithms?\nAssume that the betting algorithm at hand guarantees that its wealth is at least F ( ∑T t=1 gt) starting\nfrom an endowment , for a given potential function F , then\nRewardT = T∑ t=1 gtwt = WealthT − ≥ F ( T∑ t=1 gt ) − . (3)\nIntuitively, if the reward is big we can expect the regret to be small. Indeed, the following lemma converts the lower bound on the reward to an upper bound on the regret.\nLemma 1 (Reward-Regret relationship McMahan and Orabona [2014]). Let V, V ∗ be a pair of dual vector spaces. Let F : V → R ∪ {+∞} be a proper convex lower semi-continuous function and let F ∗ : V ∗ → R ∪ {+∞} be its Fenchel conjugate. Let w1, w2, . . . , wT ∈ V and g1, g2, . . . , gT ∈ V ∗. Let ∈ R. Then,\nT∑ t=1\n〈gt, wt〉︸ ︷︷ ︸ RewardT ≥ F\n( T∑ t=1 gt ) − if and only if ∀u ∈ V ∗, T∑ t=1\n〈gt, u− wt〉︸ ︷︷ ︸ RegretT (u)\n≤ F ∗(u) + .\nApplying the lemma, we get a regret upper bound: RegretT (u) ≤ F ∗(u) + for all u ∈ H. To summarize, if we have a betting algorithm that guarantees a minimum wealth of F ( ∑T t=1 gt), it can be used to design and analyze a one-dimensional OLO algorithm. The faster the growth of the wealth,\nthe smaller the regret will be. Moreover, the lemma also shows that trying to design an algorithm that is adaptive to u is equivalent to designing an algorithm that is adaptive to ∑T t=1 gt. Also, most importantly, methods that guarantee optimal wealth for the betting scenario are already known, see, e.g., [Cesa-Bianchi and Lugosi, 2006, Chapter 9]. We can just re-use them to get optimal online algorithms!"
    }, {
      "heading" : "4 Designing a Betting Algorithm: Coin Betting Potentials",
      "text" : "For sequential betting on i.i.d. coin flips, an optimal strategy has been proposed by Kelly [1956]. The strategy assumes that the coin flips {gt}∞t=1, gt ∈ {+1,−1}, are generated i.i.d. with known probability of heads. If p ∈ [0, 1] is the probability of heads, the Kelly bet is to bet βt = 2p− 1 at each round. He showed that, in the long run, this strategy will provide more wealth than betting any other fixed fraction of the current wealth Kelly [1956].\nFor adversarial coins, Kelly betting does not make sense. With perfect knowledge of the future, the gambler could always bet everything on the right outcome. Hence, after T rounds from an initial endowment , the maximum wealth he would get is 2T . Instead, assume he bets the same fraction β of its wealth at each round. Let Wealtht(β) the wealth of such strategy after t rounds. As observed in McMahan and Abernethy\n[2013], the optimal fixed fraction to bet is β∗ = ( ∑T t=1 gt)/T and it gives the wealth\nWealthT (β ∗) = exp ( T ·D ( 1 2 + ∑T t=1 gt 2T ∥∥∥ 12)) ≥ exp( (∑Tt=1 gt)22T ) , (4) where the inequality follows from Pinsker’s inequality [Cover and Thomas, 2006, Lemma 11.6.1].\nHowever, even without knowledge of the future, it is possible to go very close to the wealth in (4). This problem was studied by Krichevsky and Trofimov [1981], who proposed that after seeing the coin flips g1, g2, . . . , gt−1 the empirical estimate kt = 1/2+ ∑t−1 i=1 1[gi=+1]\nt should be used instead of p. Their estimate is commonly called KT estimator.1 The KT estimator results in the betting\nβt = 2kt − 1 = ∑t−1 i=1 gi t (5)\nwhich we call adaptive Kelly betting based on the KT estimator. It looks like an online and slightly biased version of the oracle choice of β∗. This strategy guarantees2\nWealthT ≥ WealthT (β ∗)\n2 √ T\n= 2 √ T\nexp ( T ·D ( 1 2 + ∑T t=1 gt 2T ∥∥∥ 12)) . This guarantee is optimal up to constant factors [Cesa-Bianchi and Lugosi, 2006] and mirrors the guarantee of the Kelly bet.\nHere, we propose a new set of definitions that allows to generalize the strategy of adaptive Kelly betting based on the KT estimator. For these strategies it will be possible to prove that, for any g1, g2, . . . , gt ∈ [−1, 1],\nWealtht ≥ Ft ( t∑ i=1 gi ) , (6)\nwhere Ft(x) is a certain function. We call such functions potentials. The betting strategy will be determined uniquely by the potential (see (c) in the Definition 2), and we restrict our attention to potentials for which (6) holds. These constraints are specified in the definition below.\nDefinition 2 (Coin Betting Potential). Let > 0. Let {Ft}∞t=0 be a sequence of functions Ft : (−at, at) → R+ where at > t. The sequence {Ft}∞t=0 is called a sequence of coin betting potentials for initial endowment , if it satisfies the following three conditions:\n(a) F0(0) = .\n1Compared to the maximum likelihood estimate ∑t−1 i=1 1[gi=+1]\nt−1 , KT estimator shrinks slightly towards 1/2.\n2See Appendix A for a proof. For lack of space, all the appendices are in the supplementary material.\n(b) For every t ≥ 0, Ft(x) is even, logarithmically convex, strictly increasing on [0, at), and limx→at Ft(x) = +∞.\n(c) For every t ≥ 1, every x ∈ [−(t− 1), (t− 1)] and every g ∈ [−1, 1], (1 + gβt)Ft−1(x) ≥ Ft(x+ g), where\nβt = Ft(x+1)−Ft(x−1) Ft(x+1)+Ft(x−1) . (7)\nThe sequence {Ft}∞t=0 is called a sequence of excellent coin betting potentials for initial endowment if it satisfies conditions (a)–(c) and the condition (d) below.\n(d) For every t ≥ 0, Ft is twice-differentiable and satisfies x · F ′′t (x) ≥ F ′t (x) for every x ∈ [0, at).\nLet’s give some intuition on this definition. First, let’s show by induction on t that (b) and (c) of the definition together with (2) give a betting strategy that satisfies (6). The base case t = 0 is trivial. At time t ≥ 1, bet wt = βt Wealtht−1 where βt is defined in (7), then\nWealtht = Wealtht−1 +wtgt = (1 + gtβt) Wealtht−1\n≥ (1 + gtβt)Ft−1 ( t−1∑ i=1 gi ) ≥ Ft ( t−1∑ i=1 gi + gt ) = Ft ( t∑ i=1 gi ) .\nThe formula for the potential-based strategy (7) might seem strange. However, it is derived—see Theorem 8 in Appendix B—by minimizing the worst-case value of the right-hand side of the inequality used w.r.t. to gt in the induction proof above: Ft−1(x) ≥ Ft(x+gt)1+gtβt . The last point, (d), is a technical condition that allows us to seamlessly reduce OLO over a Hilbert space to the one-dimensional problem, characterizing the worst case direction for the reward vectors. Regarding the design of coin betting potentials, we expect any potential that approximates the best\npossible wealth in (4) to be a good candidate. In fact, Ft(x) = exp ( x2/(2t) ) / √ t, essentially the potential used in the parameter-free algorithms in McMahan and Orabona [2014], Orabona [2014] for OLO and in Chaudhuri et al. [2009], Luo and Schapire [2014, 2015] for LEA, approximates (4) and it is an excellent coin betting potential—see Theorem 9 in Appendix B. Hence, our framework provides intuition to previous constructions and in Section 7 we show new examples of coin betting potentials.\nIn the next two sections, we presents the reductions to effortlessly solve both the generic OLO case and LEA with a betting potential."
    }, {
      "heading" : "5 From Coin Betting to OLO over Hilbert Space",
      "text" : "In this section, generalizing the one-dimensional construction in Section 3, we show how to use a sequence of excellent coin betting potentials {Ft}∞t=0 to construct an algorithm for OLO over a Hilbert space and how to prove a regret bound for it.\nWe define reward and wealth analogously to the one-dimensional case: Rewardt = ∑t i=1〈gi, wi〉 and Wealtht = +Rewardt. Given a sequence of coin betting potentials {Ft}∞t=0, using (7) we define the fraction\nβt = Ft(‖∑t−1i=1 gi‖+1)−Ft(‖∑t−1i=1 gi‖−1) Ft(‖∑t−1i=1 gi‖+1)+Ft(‖∑t−1i=1 gi‖−1) . (8)\nThe prediction of the OLO algorithm is defined similarly to the one-dimensional case, but now we also need a direction in the Hilbert space:\nwt = βt Wealtht−1 ∑t−1 i=1 gi∥∥∥∑t−1i=1 gi∥∥∥ = βt ∑t−1 i=1 gi∥∥∥∑t−1i=1 gi∥∥∥ ( + t−1∑ i=1 〈gi, wi〉 ) . (9)\nIf ∑t−1 i=1 gi is the zero vector, we define wt to be the zero vector as well. For this prediction strategy we can prove the following regret guarantee, proved in Appendix C. The proof reduces the general Hilbert case to the 1-d case, thanks to (d) in Definition 2, then it follows the reasoning of Section 3.\nTheorem 3 (Regret Bound for OLO in Hilbert Spaces). Let {Ft}∞t=0 be a sequence of excellent coin betting potentials. Let {gt}∞t=1 be any sequence of reward vectors in a Hilbert space H such that ‖gt‖ ≤ 1 for all t. Then, the algorithm that makes prediction wt defined by (9) and (8) satisfies\n∀T ≥ 0 ∀u ∈ H RegretT (u) ≤ F ∗T (‖u‖) + ."
    }, {
      "heading" : "6 From Coin Betting to Learning with Expert Advice",
      "text" : "In this section, we show how to use the algorithm for OLO over one-dimensional Hilbert space R from Section 3—which is itself based on a coin betting strategy—to construct an algorithm for LEA.\nLet N ≥ 2 be the number of experts and ∆N be the N -dimensional probability simplex. Let π = (π1, π2, . . . , πN ) ∈ ∆N be any prior distribution. Let A be an algorithm for OLO over the one-dimensional Hilbert space R, based on a sequence of the coin betting potentials {Ft}∞t=0 with initial endowment3 1. We instantiate N copies of A.\nConsider any round t. Let wt,i ∈ R be the prediction of the i-th copy of A. The LEA algorithm computes p̂t = (p̂t,1, p̂t,2, . . . , p̂t,N ) ∈ RN0,+ as\np̂t,i = πi · [wt,i]+, (10)\nwhere [x]+ = max{0, x} is the positive part of x. Then, the LEA algorithm predicts pt = (pt,1, pt,2, . . . , pt,N ) ∈ ∆N as\npt = p̂t ‖p̂t‖1 . (11)\nIf ‖p̂t‖1 = 0, the algorithm predicts the prior π. Then, the algorithm receives the reward vector gt = (gt,1, gt,2, . . . , gt,N ) ∈ [0, 1]N . Finally, it feeds the reward to each copy of A. The reward for the i-th copy of A is g̃t,i ∈ [−1, 1] defined as\ng̃t,i = { gt,i − 〈gt, pt〉 if wt,i > 0 , [gt,i − 〈gt, pt〉]+ if wt,i ≤ 0 .\n(12)\nThe construction above defines a LEA algorithm defined by the predictions pt, based on the algorithm A. We can prove the following regret bound for it.\nTheorem 4 (Regret Bound for Experts). Let A be an algorithm for OLO over the one-dimensional Hilbert space R, based on the coin betting potentials {Ft}∞t=0 for an initial endowment of 1. Let f−1t be the inverse of ft(x) = ln(Ft(x)) restricted to [0,∞). Then, the regret of the LEA algorithm with prior π ∈ ∆N that predicts at each round with pt in (11) satisfies\n∀T ≥ 0 ∀u ∈ ∆N RegretT (u) ≤ f−1T (D (u‖π)) .\nThe proof, in Appendix D, is based on the fact that (10)–(12) guarantee that ∑N i=1 πig̃t,iwt,i ≤ 0 and on\na variation of the change of measure lemma used in the PAC-Bayes literature, e.g. McAllester [2013]."
    }, {
      "heading" : "7 Applications of the Krichevsky-Trofimov Estimator to OLO and",
      "text" : "LEA\nIn the previous sections, we have shown that a coin betting potential with a guaranteed rapid growth of the wealth will give good regret guarantees for OLO and LEA. Here, we show that the KT estimator has associated an excellent coin betting potential, which we call KT potential. Then, the optimal wealth guarantee of the KT potentials will translate to optimal parameter-free regret bounds.\n3Any initial endowment > 0 can be rescaled to 1. Instead of Ft(x) we would use Ft(x)/ . The wt would become wt/ , but pt is invariant to scaling of wt. Hence, the LEA algorithm is the same regardless of .\nAlgorithm 1 Algorithm for OLO over Hilbert space H based on KT potential Require: Initial endowment > 0\n1: for t = 1, 2, . . . do 2: Predict with wt ← 1t ( + ∑t−1 i=1〈gi, wi〉 )∑t−1 i=1 gi 3: Receive reward vector gt ∈ H such that ‖gt‖ ≤ 1 4: end for\nThe sequence of excellent coin betting potentials for an initial endowment corresponding to the adaptive Kelly betting strategy βt defined by (5) based on the KT estimator are\nFt(x) = 2t·Γ\n( t+1\n2 + x 2\n) ·Γ ( t+1\n2 − x 2 ) π·t! t ≥ 0, x ∈ (−t− 1, t+ 1), (13)\nwhere Γ(x) = ∫∞\n0 tx−1e−tdt is Euler’s gamma function—see Theorem 13 in Appendix E. This potential\nwas used to prove regret bounds for online prediction with the logarithmic loss Krichevsky and Trofimov [1981][Cesa-Bianchi and Lugosi, 2006, Chapter 9.7]. Theorem 13 also shows that the KT betting strategy βt as defined by (5) satisfies (7).\nThis potential has the nice property that is satisfies the inequality in (c) of Definition 2 with equality when gt ∈ {−1, 1}, i.e. Ft(x+ gt) = (1 + gtβt)Ft−1(x).\nWe also generalize the KT potentials to δ-shifted KT potentials, where δ ≥ 0, defined as\nFt(x) = 2t·Γ(δ+1)·Γ\n( t+δ+1\n2 + x 2\n) ·Γ ( t+δ+1\n2 − x 2 ) Γ ( δ+1\n2\n)2 ·Γ(t+δ+1)\n.\nThe reason for its name is that, up to a multiplicative constant, Ft is equal to the KT potential shifted in time by δ. Theorem 13 also proves that the δ-shifted KT potentials are excellent coin betting potentials\nwith initial endowment 1, and the corresponding betting fraction is βt = ∑t−1 j=1 gj\nδ+t ."
    }, {
      "heading" : "7.1 OLO in Hilbert Space",
      "text" : "We apply the KT potential for the construction of an OLO algorithm over a Hilbert space H. We will use (9), and we just need to calculate βt. According to Theorem 13 in Appendix E, the formula for βt simplifies\nto βt = ‖∑t−1i=1 gi‖ t so that wt = 1 t ( + ∑t−1 i=1〈gi, wi〉 )∑t−1 i=1 gi.\nThe resulting algorithm is stated as Algorithm 1. We derive a regret bound for it as a very simple corollary of Theorem 3 to the KT potential (13). The only technical part of the proof, in Appendix F, is an upper bound on F ∗t since it cannot be expressed as an elementary function. Corollary 5 (Regret Bound for Algorithm 1). Let > 0. Let {gt}∞t=1 be any sequence of reward vectors in a Hilbert space H such that ‖gt‖ ≤ 1. Then Algorithm 1 satisfies\n∀T ≥ 0 ∀u ∈ H RegretT (u) ≤ ‖u‖ √ T ln ( 1 + 24T 2‖u‖2 2 ) + ( 1− 1 e √ πT ) .\nIt is worth noting the elegance and extreme simplicity of Algorithm 1 and contrast it with the algorithms in Streeter and McMahan [2012], McMahan and Orabona [2014], Orabona [2013, 2014]. Also, the regret bound is optimal Streeter and McMahan [2012], Orabona [2013]. The parameter can be safely set to any constant, e.g. 1. Its role is equivalent to the initial guess used in doubling tricks Shalev-Shwartz [2011]."
    }, {
      "heading" : "7.2 Learning with Expert Advice",
      "text" : "We will now construct an algorithm for LEA based on the δ-shifted KT potential. We set δ to T/2, requiring the algorithm to know the number of rounds T in advance; we will fix this later with the standard doubling trick.\nAlgorithm 2 Algorithm for Learning with Expert Advice based on δ-shifted KT potential Require: Number of experts N , prior distribution π ∈ ∆N , number of rounds T 1: for t = 1, 2, . . . , T do\n2: For each i ∈ [N ], set wt,i ← ∑t−1 j=1 g̃j,i\nt+T/2\n( 1 + ∑t−1 j=1 g̃j,iwj,i ) 3: For each i ∈ [N ], set p̂t,i ← πi[wt,i]+\n4: Predict with pt ← { p̂t/ ‖p̂t‖1 if ‖p̂t‖1 > 0 π if ‖p̂t‖1 = 0 5: Receive reward vector gt ∈ [0, 1]N\n6: For each i ∈ [N ], set g̃t,i ← { gt,i − 〈gt, pt〉 if wt,i > 0 [gt,i − 〈gt, pt〉]+ if wt,i ≤ 0 7: end for\nTo use the construction in Section 6, we need an OLO algorithm for the 1-d Hilbert space R. Using the δ-shifted KT potentials, the algorithm predicts for any sequence {g̃t}∞t=1 of reward\nwt = βt Wealtht−1 = βt 1 + t−1∑ j=1 g̃jwj  = ∑t−1i=1 g̃i T/2 + t 1 + t−1∑ j=1 g̃jwj  . Then, following the construction in Section 6, we arrive at the final algorithm, Algorithm 2. We can derive a regret bound for Algorithm 2 by applying Theorem 4 to the δ-shifted KT potential.\nCorollary 6 (Regret Bound for Algorithm 2). Let N ≥ 2 and T ≥ 0 be integers. Let π ∈ ∆N be a prior. Then Algorithm 2 with input N, π, T for any rewards vectors g1, g2, . . . , gT ∈ [0, 1]N satisfies\n∀u ∈ ∆N RegretT (u) ≤ √ 3T (3 + D (u‖π)) .\nHence, the Algorithm 2 has both the best known guarantee on worst-case regret and per-round time complexity, see Table 1. Also, it has the advantage of being very simple.\nThe proof of the corollary is in the Appendix F. The only technical part of the proof is an upper bound on f−1t (x), which we conveniently do by lower bounding Ft(x).\nThe reason for using the shifted potential comes from the analysis of f−1t (x). The unshifted algorithm would have a O( √ T (log T + D (u‖π)) regret bound; the shifting improves the bound to O( √ T (1 + D (u‖π)). By changing T/2 in Algorithm 2 to another constant fraction of T , it is possible to trade-off between the two constants 3 present in the square root in the regret upper bound.\nThe requirement of knowing the number of rounds T in advance can be lifted by the standard doubling trick [Shalev-Shwartz, 2011, Section 2.3.1], obtaining an anytime guarantee with a bigger leading constant,\n∀T ≥ 0 ∀u ∈ ∆N RegretT (u) ≤ √ 2√ 2−1\n√ 3T (3 + D (u‖π)) ."
    }, {
      "heading" : "8 Discussion of the Results",
      "text" : "We have presented a new interpretation of parameter-free algorithms as coin betting algorithms. This interpretation, far from being just a mathematical gimmick, reveals the common hidden structure of previous parameter-free algorithms for both OLO and LEA and also allows the design of new algorithms. For example, we show that the characteristic of parameter-freeness is just a consequence of having an algorithm that guarantees the maximum reward possible. The reductions in Sections 5 and 6 are also novel and they are in a certain sense optimal. In fact, the obtained Algorithms 1 and 2 achieve the optimal worst case upper bounds on the regret, see Streeter and McMahan [2012], Orabona [2013] and Cesa-Bianchi and Lugosi [2006] respectively.\nWe have also run an empirical evaluation to show that the theoretical difference between classic online learning algorithms and parameter-free ones is real and not just theoretical. In Figure 1, we have used three regression datasets4, and solved the OCO problem through OLO. In all the three cases, we have used the absolute loss and normalized the input vectors to have L2 norm equal to 1. From the empirical results, it is clear that the optimal learning rate is completely data-dependent, yet parameter-free algorithms have performance very close to the unknown optimal tuning of the learning rate. Moreover, the KT-based Algorithm 1 seems to dominate all the other similar algorithms.\nFor LEA, we have used the synthetic setting in Chaudhuri et al. [2009]. The dataset is composed of Hadamard matrices of size 64, where the row with constant values is removed, the rows are duplicated to 126 inverting their signs, 0.025 is subtracted to k rows, and the matrix is replicated in order to generate T = 32768 samples. For more details, see Chaudhuri et al. [2009]. Here, the KT-based algorithm is the one in Algorithm 2, where the term T/2 is removed, so that the final regret bound has an additional lnT term. Again, we see that the parameter-free algorithms have a performance close or even better than Hedge with an oracle tuning of the learning rate, with no clear winners among the parameter-free algorithms.\nNotice that since the adaptive Kelly strategy based on KT estimator is very close to optimal, the only possible improvement is to have a data-dependent bound, for example like the ones in Orabona [2014], Koolen and van Erven [2015], Luo and Schapire [2015]. In future work, we will extend our definitions and reductions to the data-dependent case.\n4Datasets available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.\nAcknowledgments. The authors thank Jacob Abernethy, Nicolò Cesa-Bianchi, Satyen Kale, Chansoo Lee, Giuseppe Molteni, and Manfred Warmuth for useful discussions on this work."
    }, {
      "heading" : "A From Log Loss to Wealth",
      "text" : "Guarantees for betting or sequential investement algorithm are often expressed as upper bounds on the regret with respect to the log loss. Here, for the sake of completeness, we show how to convert such a guarantee to a lower bound on the wealth of the corresponding betting algorithm.\nWe consider the problem of predicting a binary outcome. The algorithm predicts at each round probability pt ∈ [0, 1]. The adversary generates a sequences of outcomes xt ∈ {0, 1} and the algorithm’s loss is\n`(pt, xt) = −xt ln pt − (1− xt) ln(1− pt) .\nWe define the regret with respect to a fixed probability vector β as\nRegretloglossT = T∑ t=1 `(pt, xt)− min β∈[0,1] T∑ t=1 `(β, xt) .\nLemma 7. Assume that an algorithm that predicts pt guarantees Regret logloss T ≤ RT . Then, the coin betting strategy with endowement and βt = 2pt − 1 guarantees\nWealthT ≥ exp ( T ·D ( 1\n2 + ∑T t=1 gt 2T ∥∥∥∥∥12 ) −RT )\nagainst any sequence of outcomes gt ∈ [−1,+1].\nProof. Define xt = 1+gt\n2 . We have\nln WealthT = ln(Wealtht−1 +wtgt)\n= ln(Wealtht−1(1 + gtβt))\n= ln T∏ t=1 (1 + gtβt)\n= ln + T∑ t=1 ln(1 + gtβt)\n≥ ln + T∑ t=1 ( 1 + gt 2 ) ln (1 + βt) + ( 1− gt 2 ) ln (1− βt)\n= ln + T∑ t=1 ( 1 + gt 2 ) ln (2pt) + ( 1− gt 2 ) ln (2(1− pt))\n= ln + T ln(2) + T∑ t=1 ( 1 + gt 2 ) ln(pt) + ( 1− gt 2 ) ln(1− pt)\n= ln + T ln(2)− T∑ t=1 `(pt, xt)\n= ln + T ln(2)− RegretloglossT − min β∈[0,1] T∑ t=1 `(β, xt)\n≥ ln + T ln(2)−RT − min β∈[0,1] T∑ t=1 `(β, xt) ,\nwhere the first inequality is due to the concavity of ln and the second one is due to the assumption of the regret.\nIt is easy to see that the β∗ = arg minβ∈[0,1] ∑T t=1 `(β, xt) = ∑T t=1 xt T . Hence, we have\nmin β∈[0,1] T∑ t=1 `(β, xt) = T (−β∗ lnβ∗ − (1− β∗) ln(1− β∗)) .\nAlso, we have that for any β ∈ [0, 1] −β lnβ − (1− β) ln(1− β) = −D ( β ∥∥∥∥12 ) + ln 2 .\nPutting all together, we have the stated lemma.\nThe lower bound on the wealth of the adaptive Kelly betting based on the KT estimator is obtained simply by the stated Lemma and reminding that the log loss regret of the KT estimator is upper bounded by 12 lnT + ln 2."
    }, {
      "heading" : "B Optimal Betting Fraction",
      "text" : "Theorem 8 (Optimal Betting Fraction). Let x ∈ R. Let F : [x− 1, x+ 1]→ R be a logarithmically convex function. Then,\narg min β∈(−1,1) max g∈[−1,1]\nF (x+ g) 1 + βg = F (x+ 1)− F (x− 1) F (x+ 1) + F (x− 1) .\nMoreover, β∗ = F (x+1)−F (x−1)F (x+1)+F (x−1) satisfies\nln(F (x+ 1))− ln(1 + β∗) = ln(F (x− 1))− ln(1− β∗) .\nProof. We define the functions h, f : [−1, 1]× (−1, 1)→ R as\nh(g, β) = F (x+ g)\n1 + βg and f(g, β) = ln(h(g, β)) = ln(F (x+ g))− ln(1 + βg) .\nClearly, arg minβ∈(−1,1) maxg∈[−1,1] h(g, β) = arg minβ∈(−1,1) maxg∈[−1,1] f(g, β) and we can work with f instead of h. The function h is logarithmically convex in g and thus f is convex in g. Therefore,\n∀β ∈ (−1, 1) max g∈[−1,1] f(g, β) = max {f(+1, β), f(−1, β)} .\nLet φ(β) = max {f(+1, β), f(−1, β)}. We seek to find the arg minβ∈(−1,1) φ(β). Since f(+1, β) is decreasing in β and f(−1, β) is increasing in β, the minimum of φ(β) is at a point β∗ such that f(+1, β∗) = f(−1, β∗). In other words, β∗ satisfies\nln(F (x+ 1))− ln(1 + β∗) = ln(F (x− 1))− ln(1− β∗) .\nThe only solution of this equation is\nβ∗ = F (x+ 1)− F (x− 1) F (x+ 1) + F (x− 1) .\nTheorem 9. The functions Ft(x) = exp( x2 2t − 1 2 ∑t i=1 1 i ) are excellent coin betting potentials.\nProof. The first and second properties of Definition 2 are trivially true. For the third property, we first use Theorem 8 to have\nln(1 + βtg)− lnFt(x+ g) ≥ ln(1 + βt)− lnFt(x+ 1) = ln 2\nFt(x+ 1) + Ft(x− 1) ,\nwhere the definition of βt is from (7). Hence, we have\nln(1 + βtg)− lnFt(x+ g) + lnFt−1(x) ≥ ln 2\nFt(x+ 1) + Ft(x− 1) + lnFt−1(x)\n= −x 2 + 1\n2t +\n1\n2 t∑ i=1 1 i − ln cosh x t +\nx2\n2(t− 1) − 1 2 t−1∑ i=1 1 i\n= −x 2 2t − ln cosh x t +\nx2\n2(t− 1)\n≥ −x 2 2t − x 2 2t2 +\nx2\n2(t− 1)\n≥ −x 2 2t − x 2 2t(t− 1) +\nx2\n2(t− 1) = 0,\nwhere in the second inequality we have used the elementary inequality ln coshx ≤ x 2\n2 . The fourth property of Definition 2 is also true because Ft(x) is of the form h(x\n2) with h(·) convex McMahan and Orabona [2014]."
    }, {
      "heading" : "C Proof of Lemma 11",
      "text" : "First we state the following Lemma from McMahan and Orabona [2014] and reported here with our notation for completeness.\nLemma 10 (Extremes). Let h : (−a, a)→ R be an even twice-differentiable function that satisfies x·h′′(x) ≥ h′(x) for all x ∈ [0, a). Let c : [0,∞)× [0,∞)→ R be an arbitrary function. Then, if vectors u, v ∈ H satisfy ‖u‖+ ‖v‖ < a, then\nc(‖u‖ , ‖v‖) · 〈u, v〉 − h(‖u+ v‖) ≥ min {c(‖u‖ , ‖v‖) · ‖u‖ · ‖v‖ − h(‖v‖+ ‖v‖), −c(‖u‖ , ‖v‖) · ‖u‖ · ‖v‖ − h(‖u‖ − ‖v‖)} . (14)\nProof. If u or v is zero, the inequality (14) clearly holds. From now on we assume that u, v are non-zero. Let α be the cosine of the angle of between u and v. More formally,\nα = 〈u, v〉 ‖u‖ · ‖v‖ .\nWith this notation, the left-hand side of (14) is f(α) = c(‖u‖ , ‖v‖) · α ‖u‖ · ‖v‖ − h( √ ‖u‖2 + ‖v‖2 + 2α ‖u‖ · ‖v‖) .\nSince h is even, the inequality (14) is equivalent to\n∀α ∈ [−1, 1] f(α) ≥ min {f(+1), f(−1)} .\nThe last inequality is clearly true if f : [−1, 1] → R is concave. We now check that f is indeed concave, which we prove by showing that the second derivative is non-positive. The first derivative of f is\nf ′(α) = c(‖u‖ , ‖v‖) · ‖u‖ · ‖v‖ − h′( √ ‖u‖2 + ‖v‖2 + 2α‖u‖ · ‖v‖) · ‖u‖ · ‖v‖√ ‖u‖2 + ‖v‖2 + 2α‖u‖ · ‖v‖ .\nThe second derivative of f is\nf ′′(α) = − ‖u‖ 2 · ‖v‖2\n‖u‖2 + ‖v‖2 + 2α‖u‖ · ‖v‖\n· ( h′′( √ ‖u‖2 + ‖v‖2 + 2α‖u‖ · ‖v‖)− h′( √ ‖u‖2 + ‖v‖2 + 2α‖u‖ · ‖v‖)√ ‖u‖2 + ‖v‖2 + 2α‖u‖ · ‖v‖ ) .\nIf we consider x = √ ‖u‖2 + ‖v‖2 + 2α‖u‖ · ‖v‖, the assumption x · h′′(x) ≥ h′(x) implies that f ′′(α) is non-positive. This finishes the proof of the inequality (14).\nWe also need the following technical Lemma whose proof relies mainly on property (d) of Definition 2.\nLemma 11. Let {Ft}∞t=0 be a sequence of excellent coin betting potentials. Let g1, g2, . . . , gt be vectors in a Hilbert space H such that ‖g1‖ , ‖g2‖ , . . . , ‖gt‖ ≤ 1. Let βt be defined by (8) and let x = ∑t−1 i=1 gi. Then,(\n1 + βt 〈gt, x〉 ‖x‖\n) Ft−1(‖x‖) ≥ Ft(‖x+ gt‖) .\nProof. Since Ft(x) is an excellent coin betting potential, it satisfies xF ′′ t (x) ≥ F ′t (x). Hence,(\n1 + βt 〈gt, x〉 ‖x‖\n) Ft−1(‖x‖)− Ft(‖x+ gt‖)\n= Ft−1(‖x‖) + βt 〈gt, x〉 ‖x‖ Ft−1(‖x‖)− Ft(‖x+ gt‖)\n≥ Ft−1(‖x‖) + min r∈{−1,1} βtr ‖gt‖Ft−1(‖x‖)− Ft(‖x‖+ r ‖gt‖)\n= min r∈{−1,1}\n(1 + βtr ‖gt‖)Ft−1(‖x‖)− Ft(‖x‖+ r ‖gt‖)\n≥ 0 .\nIf x 6= 0, the first inequality comes from Lemma 10 with c(z, ·) = Ft−1(z+1)−Ft−1(z−1)Ft−1(z+1)+Ft−1(z−1)Ft−1(z)/z and h(z) = Ft(z), u = gt, v = x. If x = 0 then, according to (8), βt = 0 and the first inequality trivially holds. The second inequality follows from the property (c) of a coin betting potential.\nProof of Theorem 3. First, by induction on t we show that\nWealtht ≥ Ft (∥∥∥∥∥ T∑ t=1 gt ∥∥∥∥∥ ) . (15)\nThe base case t = 0 is trivial, since both sides of the inequality are equal to . For t ≥ 1, if we let x = ∑t−1 i=1 gi, we have\nWealtht = 〈gt, wt〉+ Wealtht−1 = (\n1 + βt 〈gt, x〉 ‖x‖\n) Wealtht−1\n≥ (\n1 + βt 〈gt, x〉 ‖x‖\n) Ft−1(‖x‖) (*) ≥ Ft(‖x+ gt‖) = Ft (∥∥∥∥∥ t∑ i=1 gi ∥∥∥∥∥ ) .\nThe inequality marked with (∗) follows from Lemma 11. This establishes (15), from which we immediately have a reward lower bound\nRewardT = T∑ t=1 〈gt, wt〉 = WealthT − ≥ FT (∥∥∥∥∥ T∑ t=1 gt ∥∥∥∥∥ ) − . (16)\nWe apply Lemma 1 to the function F (x) = FT (‖x‖)− and we are almost done. The only remaining property we need is that if F is an even function then the Fenchel conjugate of F (‖·‖) is F ∗(‖·‖); see Bauschke and Combettes [2011, Example 13.7]."
    }, {
      "heading" : "D Proof of Theorem 4",
      "text" : "Proof. We first prove that ∑N i=1 πig̃t,iwt,i ≤ 0. Indeed,\nN∑ i=1 πig̃t,iwt,i = ∑\ni :πiwt,i>0\nπi[wt,i]+(gt,i − 〈gt, pt〉) + ∑\ni :πiwt,i≤0\nπiwt,i[gt,i − 〈gt, pt〉]+\n= ‖p̂t‖1 N∑ i=1 pt,i(gt,i − 〈gt, pt〉) + ∑\ni :πiwt,i≤0\nπiwt,i[gt,i − 〈gt, pt〉]+\n= 0 + ∑\ni :πiwt,i≤0\nπiwt,i[gt,i − 〈gt, pt〉]+ ≤ 0 .\nThe first equality follows from definition of gt,i. To see the second equality, consider two cases: If πiwt,i ≤ 0 for all i then ‖p̂t‖1 = 0 and therefore both ‖p̂t‖1 ∑N i=1 pt,i(gt,i−〈gt, pt〉) and ∑ i :πiwt,i>0\nπi[wt,i]+(gt,i−〈gt, pt〉) are trivially zero. If ‖p̂t‖1 > 0 then πi[wt,i]+ = p̂t,i = ‖p̂t‖1 pt,i for all i.\nFrom the assumption on A, we have, for any sequence {g̃t}∞t=1 such that g̃t ∈ [−1, 1], satisfies\nWealtht = 1 + t∑ i=1 g̃iwi ≥ Ft ( t∑ i=1 g̃i ) . (17)\nInequality ∑N i=1 πig̃t,iwt,i ≤ 0 and (17) imply\nN∑ i=1 πiFT ( T∑ t=1 g̃t,i ) ≤ 1 + N∑ i=1 πi T∑ t=1 g̃t,iwt,i ≤ 1 . (18)\nNow, let G̃T,i = ∑T t=1 g̃t,i. For any competitor u ∈ ∆N ,\nRegretT (u) = T∑ t=1 〈gt, u− pt〉 = T∑ t=1 N∑ i=1 ui (gt,i − 〈gt, pt〉)\n≤ T∑ t=1 N∑ i=1 uig̃t,i (by definition of g̃t,i)\n≤ N∑ i=1 ui ∣∣∣G̃T,i∣∣∣ (since ui ≥ 0, i = 1, . . . , N) =\nN∑ i=1 uif −1 T ( ln[FT (G̃T,i)] ) (since FT (x) = exp(fT (x)) is even)\n≤ f−1T ( N∑ i=1 ui ln [ FT (G̃T,i) ]) (by concavity of f−1T )\n= f−1T ( N∑ i=1 ui { ln [ ui πi ] + ln [ πi ui FT (G̃T,i) ]}) = f−1T ( D (u‖π) + N∑ i=1 ui ln [ πi ui FT (G̃T,i) ])\n≤ f−1T\n( D (u‖π) + ln ( N∑ i=1 πiFT (G̃T,i) )) (by concavity of ln(·))\n≤ f−1T (D (u‖π)) (by (18))."
    }, {
      "heading" : "E Properties of Krichevsky-Trofimov Potential",
      "text" : "Lemma 12 (Analytic Properties of KT potential). Let a > 0. The function F : (−a, a)→ R+,\nF (x) = Γ(a+ x)Γ(a− x)\nis even, logarithmically convex, strictly increasing on [0, a), satisfies\nlim x↗a F (x) = lim x↘−a\nF (x) = +∞\nand ∀x ∈ [0, a) x · F ′′(x) ≥ F ′(x) . (19)\nProof. F (x) is obviously even. Γ(z) = ∫∞\n0 tz−1e−tdt is defined for any real number z > 0. Hence, F is\ndefined on the interval (−a, a). According to Bohr-Mollerup theorem [Artin, 1964, Theorem 2.1], Γ(x) is logarithmically convex on (0,∞). Hence, F (x) is also logarithmically convex, since ln(F (x)) = ln(Γ(a + x)) + ln(Γ(a− x)) is a sum of convex functions.\nIt is well known that limz↘0 Γ(z) = +∞. Thus,\nlim x↗a F (x) = lim x↗a Γ(a+ x)Γ(a− x) = Γ(2a) lim x↗a Γ(a− x) = Γ(2a) lim z↘0 Γ(z) = +∞ ,\nsince Γ is continuous and not zero at 2a. Because F (x) is even, we also have limx↘−a F (x) = +∞. To show that F (x) is increasing and that it satisfies (19), we write f(x) = ln(F (x)) as a Mclaurin series. The derivatives of ln(Γ(z)) are the so called polygamma functions\nψ(n)(z) = dn+1\ndzn+1 ln(Γ(z)) for z > 0 and n = 0, 1, 2, . . . .\nPolygamma functions have the well-known integral representation ψ(n)(z) = (−1)n+1 ∫ ∞\n0\ntne−zt\n1− e−t dt for z > 0 and n = 1, 2, . . . .\nUsing polygamma functions, we can write the Mclaurin series for f(x) = ln(F (x)) as\nf(x) = ln(F (x)) = ln(Γ(a+ x)) + ln(Γ(a− x)) = 2 ln(Γ(a)) + 2 ∑ n≥2 n even ψ(n−1)(a)xn n! .\nThe series converges for x ∈ (−a, a), since for even n ≥ 2, ψ(n−1)(a) is positive and can be upper bounded as\nψ(n−1)(a) = ∫ ∞ 0 tn−1e−at 1− e−t dt\n= ∫ 1 0 tn−1e−at 1− e−t dt+ ∫ ∞ 1 tn−1e−zt 1− e−t dt\n≤ ∫ 1\n0\ntn−1e−at\nt(1− 1/e) dt+ ∫ ∞ 1 tn−1e−atdt\n≤ 1 1− 1/e ∫ ∞ 0 tn−2e−atdt+ ∫ ∞ 0 tn−1e−atdt = 1\n1− 1/e a1−nΓ(n− 1) + a−nΓ(n)\n≤ 1 1− 1/e a−n(a+ 1)(n− 1)! .\nFrom the Mclaurin expansion we see that f(x) is increasing on [0, a) since all the coefficients are positive (except for zero order term).\nFinally, to prove (19), note that for any x ∈ (−a, a),\nf(x) = c0 + ∞∑ n=2 cnx n\nwhere c2, c3, . . . are non-negative coefficients. Thus\nf ′(x) = ∞∑ n=2 ncnx n−1 and f ′′(x) = ∞∑ n=2 n(n− 1)cnxn−2 .\nand hence x · f ′′(x) ≥ f ′(x) for x ∈ [0, a). Since F (x) = exp(f(x)),\nF ′(x) = f ′(x) · F (x) and F ′′(x) = [ f ′′(x) + (f ′(x))2 ] · F (x) .\nTherefore, for x ∈ [0, a), x · F ′′(x) = x [ f ′′(x) + (f ′(x))2 ] F (x) ≥ [ f ′(x) + x(f ′(x))2 ] F (x) ≥ f ′(x)F (x) = F ′(x) .\nThis proves (19).\nTheorem 13 (KT potential). Let δ ≥ 0 and > 0. The sequence of functions {Ft}∞t=0, Ft : (−t− δ− 1, t+ δ + 1)→ R+ defined by\nFt(x) = 2t · Γ(δ + 1)Γ( t+δ+12 + x 2 )Γ( t+δ+1 2 − x 2 )\nΓ( δ+12 ) 2Γ(t+ δ + 1)\n.\nis a sequence of excellent coin betting potentials for initial endowment . Furthermore, for any x ∈ (−t − δ − 1, t+ δ + 1),\nFt(x+ 1)− Ft(x− 1) Ft(x+ 1) + Ft(x− 1) = x t+ δ . (20)\nProof. Property (b) and (d) of the definition follow from Lemma 12. Property (a) follows by simple substitution for t = 0 and x = 0.\nBefore verifying property (c), we prove (20). We use an algebraic property of the gamma function that states that Γ(1 + z) = zΓ(z) for any positive z. Equation (20) follows from\nFt(x+ 1)− Ft(x− 1) Ft(x+ 1) + Ft(x− 1) = Γ( t+δ+22 + x 2 )Γ( t+δ 2 − x 2 )− Γ( t+δ 2 + x 2 )Γ( t+δ+2 2 − x 2 )\nΓ( t+δ+22 + x 2 )Γ( t+δ 2 − x 2 ) + Γ( t+δ 2 + x 2 )Γ( t+δ+2 2 − x 2 )\n= ( t+δ2 + x 2 )Γ( t+δ 2 + x 2 )Γ( t+δ 2 − x 2 )− ( t+δ 2 − x 2 )Γ( t+δ 2 + x 2 )Γ( t+δ 2 − x 2 )\n( t+δ2 + x 2 )Γ( t+δ 2 + x 2 )Γ( t+δ 2 − x 2 ) + ( t+δ 2 − x 2 )Γ( t+δ 2 + x 2 )Γ( t+δ 2 − x 2 )\n= ( t+δ2 + x 2 )− ( t+δ 2 − x 2 )\n( t+δ2 + x 2 ) + ( t+δ 2 − x 2 )\n= x\nt+ δ .\nLet φ(g) = Ft(x+g)Ft−1(x) . To verify property (c) of the definition, we need to show that φ(g) ≤ 1 + g x t+δ for\nany x ∈ [−t+ 1, t− 1] and any g ∈ [−1, 1]. We can write φ(g) as\nφ(g) = Ft(x+ g)\nFt−1(x)\n= 2Γ( t+δ+12 + x+g 2 )Γ( t+δ+1 2 − x+g 2 )Γ(t+ δ)\nΓ( t+δ2 + x 2 )Γ( t+δ 2 − x 2 )Γ(t+ δ + 1)\n= 2\nt+ δ ·\nΓ( t+δ+12 + x+g 2 )Γ( t+δ+1 2 − x+g 2 )\nΓ( t+δ2 + x 2 )Γ( t+δ 2 − x 2 )\n.\nFor g = +1, using the formula Γ(1 + z) = zΓ(z), we have\nφ(+1) = 2\nt+ δ ·\nΓ( t+δ2 + x 2 + 1)Γ( t+δ 2 − x 2 )\nΓ( t+δ2 + x 2 )Γ( t+δ 2 − x 2 )\n= 2\nt+ δ\n( t+ δ\n2 + x 2\n) = 1 + x\nt+ δ .\nSimilarly, for g = −1, using the formula Γ(1 + z) = zΓ(z), we have\nφ(−1) = 2 t+ δ\n· Γ( t+δ2 + x 2 )Γ( t+δ 2 − x 2 + 1)\nΓ( t+δ2 + x 2 )Γ( t+δ 2 − x 2 )\n= 2\nt+ δ\n( t+ δ\n2 − x 2\n) = 1− x\nt+ δ .\nWe can write any g ∈ [−1, 1] as a convex combination of −1 and +1, i.e., g = λ · (−1) + (1 − λ) · (+1) for some λ ∈ [0, 1]. Since φ(g) is (logarithmically) convex,\nφ(g) = φ(λ · (−1) + (1− λ) · (+1)) ≤ λφ(−1) + (1− λ)φ(+1)\n= λ ( 1 + x\nt+ δ\n) + (1− λ) ( 1− x\nt+ δ ) = 1 + g x\nt+ δ ."
    }, {
      "heading" : "F Proofs of Corollaries 5 and 6",
      "text" : "We state some technical lemmas that will be used in the following proofs. We start with a lower bound on the Krichevsky-Trofimov (KT) potential. It is a generalization of the lower bound proved for integers in Willems et al. [1995] to real numbers.\nLemma 14 (Lower Bound on KT Potential). If c ≥ 1 and a, b are non-negative reals such that a + b = c then\nln\n( Γ(a+ 1/2) · Γ(b+ 1/2)\nπ · Γ(c+ 1)\n) ≥ − ln(e √ π)− 1\n2 ln(c) + ln ((a c )a(b c )b) .\nProof. From Whittaker and Watson [1962][p. 263 Ex. 45], we have\nΓ(a+ 1/2)Γ(b+ 1/2) Γ(a+ b+ 1) ≥ √ 2π (a+ 1/2)a(b+ 1/2)b (a+ b+ 1)a+b+1/2 .\nIt remains to show that\n√ 2π (a+ 1/2)a(b+ 1/2)b\n(a+ b+ 1)a+b+1/2 >\n√ π\ne 1√ a+ b\n( a\na+ b\n)a( b\na+ b\n)b ,\nwhich is equivalent to (1 + 12a ) a(1 + 12b ) b\n(1 + 1a+b ) a+b+1/2\n> 1\ne √ 2 .\nFrom the inequality 1 ≤ (1 + 1/x)x < e valid for any x ≥ 0, it follows that 1 ≤ (1 + 12a ) a <\n√ e and\n1 ≤ (1 + 12b ) b < √ e and 1 ≤ (1 + 1/(a+ b))a+b < e. Hence,\n(1 + 12a ) a(1 + 12b ) b\n(1 + 1a+b ) a+b+1/2\n> 1 e √\n1 + 1a+b\n≥ 1 e √ 2 .\nLemma 15. Let δ ≥ 0. Then Γ(δ + 1) 2δΓ( δ+12 ) 2 ≥ √ δ + 1 π .\nProof. We will prove the equivalent statement that\nln Γ(δ + 1)π\n2δΓ( δ+12 ) 2 √ δ + 1 ≥ 0 .\nThe inequality holds with equality in δ = 0, so it is enough to prove that the derivative of the left-hand side is positive for δ > 0. The derivative of the left-hand side is equal to\nΨ(δ + 1)− 1 2(δ + 1)\n− ln(2)−Ψ ( δ + 1\n2\n) ,\nwhere Ψ(x) is the digamma function. We will use the upper [Chen, 2005] and lower bound [Batir, 2008] to the digamma function, which state that for any x > 0,\nΨ(x) < ln(x)− 1 2x − 1 12x2 + 1 120x4\nΨ(x+ 1) > ln ( x+ 1\n2\n) .\nUsing these bounds we have\nΨ(δ + 1)− 1 2(δ + 1)\n− ln(2)−Ψ ( δ + 1\n2 ) ≥ ln ( δ + 1\n2\n) − 1\n2(δ + 1) − ln(2)− ln\n( δ + 1\n2\n) + 1\nδ + 1 +\n1 3(δ + 1)2 − 2 15(δ + 1)4\n= ln ( 1− 1\n2(δ + 1)\n) +\n1\n2(δ + 1) +\n1 3(δ + 1)2 − 2 15(δ + 1)4\n≥ − (4 ln(2)− 2) 4(δ + 1)2 + 1 3(δ + 1)2 − 2 15(δ + 1)4\n= [15(1/2− ln(2))) + 5](δ + 1)2 − 2\n15(δ + 1)4\n≥ [15(1/2− ln(2))) + 5]− 2 15(δ + 1)4 ≥ 0\nwhere in the second inequality we used the elementary inequality ln(1− x) ≥ −x− (4 ln(2)− 2)x2 valid for x ∈ [0, .5].\nLemma 16 (Lower Bound on Shifted KT Potential). Let T ≥ 1, δ ≥ 0, and x ∈ [−T, T ]. Then\n2T · Γ(δ + 1)Γ ( T+δ+1\n2 + x 2\n) · Γ ( T+δ+1\n2 − x 2 ) Γ( δ+12 ) 2Γ(T + δ + 1) ≥ exp ( x2 2(T + δ) + 1 2 ln ( 1 + δ T + δ ) − ln(e √ π) ) .\nProof. Using Lemma 14, we have\nln 2T · Γ(δ + 1)Γ\n( T+δ+1\n2 + x 2\n) · Γ ( T+δ+1\n2 − x 2 ) Γ( δ+12 ) 2Γ(T + δ + 1)\n≥ ln 2T+δ\n√ δ + 1 · Γ\n( T+δ+1\n2 + x 2\n) · Γ ( T+δ+1\n2 − x 2 ) πΓ(T + δ + 1)\n≥ − ln(e √ π) + 1\n2 ln\n( 1 + δ\nT + δ\n) + ln (( 1 + x\nT + δ\n)T+δ+x 2 ( 1 + x\nT + δ\n)T+δ−x 2 )\n= − ln(e √ π) + 1\n2 ln\n( 1 + δ\nT + δ\n) + (T + δ) D ( 1\n2 +\nx\n2(T + δ) ∥∥∥∥12 )\n≥ − ln(e √ π) + 1\n2 ln\n( 1 + δ\nT + δ\n) +\nx2\n2(T + δ) ,\nwhere in the first inequality we used Lemma 15, in the second one Lemma 14, and in third one the known lower bound to the divergence D (\n1 2 + x 2 ∥∥ 1 2 ) ≥ x 2\n2 . Exponentiating and overapproximating, we get the stated bound.\nF.1 Proof of Corollary 5\nThe Lambert function W (x) : [0,∞)→ [0,∞) is defined by the equality\nx = W (x) exp (W (x)) for x ≥ 0. (21)\nThe following lemma provides bounds on W (x).\nLemma 17. The Lambert function satisfies 0.6321 log(x+ 1) ≤W (x) ≤ log(x+ 1) for x ≥ 0.\nProof. The inequalities are satisfied for x = 0, hence we in the following we assume x > 0. We first prove the lower bound. From (21) we have\nW (x) = log\n( x\nW (x)\n) . (22)\nFrom the first equality, using the elementary inequality ln(x) ≤ aex 1 a for any a > 0, we get\nW (x) ≤ 1 a e\n( x\nW (x)\n)a ∀a > 0,\nthat is\nW (x) ≤ ( 1\na e\n) 1 1+a\nx a 1+a ∀a > 0. (23)\nUsing (23) in (22), we have\nW (x) ≥ log  x( 1 a e ) 1 1+a x a 1+a  = 1 1 + a log (a e x) ∀a > 0 .\nConsider now the function g(x) = xx+1 − b log(1+b)(b+1) log(x + 1), x ≥ b. This function has a maximum in x∗ = (1 + 1b ) log(1 + b)− 1, the derivative is positive in [0, x\n∗] and negative in [x∗, b]. Hence the minimum is in x = 0 and in x = b, where it is equal to 0. Using the property just proved on g, setting a = 1x , we have\nW (x) ≥ x x+ 1 ≥ b log(1 + b)(b+ 1) log(x+ 1) ∀x ≤ b .\nFor x > b, setting a = x+1ex , we have\nW (x) ≥ e x (e+ 1)x+ 1 log(x+ 1) ≥ e b (e+ 1)b+ 1 log(x+ 1) (24)\nHence, we set b such that e b\n(e+ 1)b+ 1 =\nb\nlog(1 + b)(b+ 1)\nNumerically, b = 1.71825..., so W (x) ≥ 0.6321 log(x+ 1) .\nFor the upper bound, we use Theorem 2.3 in Hoorfar and Hassani [2008], that says that\nW (x) ≤ log x+ C 1 + log(C) , ∀x > −1 e , C > 1 e .\nSetting C = 1, we obtain the stated bound.\nLemma 18. Define f(x) = β exp x 2\n2α , for α, β > 0, x ≥ 0. Then\nf∗(y) = y √ αW ( αy2\nβ2\n) − β exp W ( αy2 β2 ) 2  . Moreover\nf∗(y) ≤ y √ α log ( αy2\nβ2 + 1\n) − β.\nProof. From the definition of Fenchel dual, we have\nf∗(y) = max x x y − f(x) = max x\nx y − β exp x 2\n2α ≤ x∗ y − β\nwhere x∗ = arg maxx x y − f(x). We now use the fact that x∗ satisfies y = f ′(x∗), to have\nx∗ = √ αW ( αy2\nβ2\n) ,\nwhere W (·) is the Lambert function. Using Lemma 17, we obtain the stated bound.\nProof of Corollary 5. Notice that the KT potential can be written as\nFt(x) = · 2t · Γ(1)Γ\n( t+1\n2 + x 2\n) · Γ ( t+1\n2 − x 2 ) Γ( 12 ) 2Γ(t+ 1) .\nUsing Lemma 16 with δ = 0 we can lower bound Ft(x) with Ht(x) = · exp ( x2\n2t +\n1 2 ln\n( 1\nt\n) − ln(e √ π) ) .\nSince Ht(x) ≤ Ft(x), we have F ∗t (x) ≤ H∗t (x). Using Lemma 18, we have\n∀u ∈ H F ∗T (‖u‖) ≤ H∗T (‖u‖) ≤ √√√√T log(24T 2 ‖u‖2 2 + 1 ) + ( 1− 1\ne √ πT\n) .\nAn application of Theorem 3 completes the proof.\nF.2 Proof of Corollary 6\nProof. Let\nFt(x) = 2t · Γ(δ + 1)Γ( t+δ+12 + x 2 )Γ( t+δ+1 2 − x 2 )\nΓ( δ+12 ) 2Γ(t+ δ + 1)\n,\nHt(x) = exp\n( x2\n2(t+ δ) +\n1 2 ln\n( 1 + δ\nt+ δ\n) − ln(e √ π) ) .\nLet ft(x) = ln(Ft(x)) and ht(x) = ln(Ht(x)). By Lemma 16, Ht(x) ≤ Ft(x) and therefore f−1t (x) ≤ h−1t (x) for all x ≥ 0. Theorem 4 implies that\n∀u ∈ ∆t Regrett(u) ≤ f−1t (D (u‖π)) ≤ h−1t (D (u‖π)) .\nSetting t = T and δ = T/2, and overapproximating h−1t (D (u‖π)) we get the stated bound."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In the recent years, a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice. These algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices. We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the KrichevskyTrofimov estimator. The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity.",
    "creator" : "LaTeX with hyperref package"
  }
}