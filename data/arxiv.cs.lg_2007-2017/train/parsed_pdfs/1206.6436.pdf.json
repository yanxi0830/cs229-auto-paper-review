{
  "name" : "1206.6436.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Efficient Structured Prediction with Latent Variables for General Graphical Models",
    "authors" : [ "Alexander G. Schwing", "Tamir Hazan" ],
    "emails" : [ "aschwing@inf.ethz.ch", "tamir@ttic.edu", "pomarc@inf.ethz.ch", "rurtasun@ttic.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In the past few years, structured models have become an important tool in many domains such as natural language processing, computer vision and computational biology. While these models typically assume a supervised setting (i.e., one has access to fully labeled input-output pairs), existing applications can benefit largely from the use of weakly labeled data. In computer vision, for example, we might want to segment an image by classifying each pixel into a semantic category, however, gathering annotated data is a very expensive process (i.e., it takes several minutes to annotate a single image). The use of weakly annotated data is even more important in domains such as medical diagnosis, as observing all labels might not be possible (e.g ., if a hospital does not have access to a particular\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ntest/procedure).\nSeveral structured prediction frameworks have been developed to deal with weakly labeled information. The most notable examples are hidden conditional random fields (HCRFs) (Quattoni et al., 2007) and latent structured support vector machines (LSSVMs) (Yu & Joachims, 2009). Both approaches are framed as regularized surrogate loss minimization tasks, and treat the missing annotations as hidden variables. They achieve good performance when it is possible to compute the maximum a-posteriori (MAP) estimate or the partition function exactly. Unfortunately, this is only possible for a few special cases, e.g., models with sub-modular energies or models with low tree-width graphs, which are not very common in practice. To deal with cyclic graphs, HCRFs and LSSVMs usually resort to approximate inference algorithms and heuristics regarding the stopping criteria. This can, however, significantly harm their performance as guarantees regarding valid cutting planes and decreasing the cost function at each iteration are no longer possible.\nIn this paper we first show that HCRFs and LSSVMs are instances of a more general framework that we refer to as structured loss minimization with latent variables. We then construct an approximation for this general structured prediction formulation, using duality, based on a local entropy approximation and derive an efficient message-passing algorithm that is guaranteed to converge for any type of potential and graphical model structure. We demonstrate the effectiveness of our approach on a synthetic segmentation task, as well as in the challenging vision task of inferring the 3D scene layout from single images, and show that our approach significantly outperforms LSSVM in terms of performance and HCRF in terms of speed, being 35 times faster. Additionally, for the 3D scene understanding task we show that state-of-the-art results\ncan be obtained while utilizing only a subset of the annotations used by existing approaches.\nIn the following, we first derive our unified framework which contains HCRFs and LSSVMs as special cases (Sec. 2). We then describe our approximation (Sec. 3) and message passing algorithm (Sec. 4), followed by our experimental evaluation and a discussion."
    }, {
      "heading" : "2. Loss minimization with latent variables",
      "text" : "In this section we propose a general framework for loss minimization with latent variables. Consider the setting where X is the input space (e.g ., an image or a sentence) and S is a structured label space (e.g ., an image segmentation or a parse tree). Note that S can depend on the example x ∈ X . For clarity of notation we dropped this dependency while noting that neither the derivation nor our implementation have this restriction. Let φ : X × S → RF denote a mapping from input and label space to an F -dimensional feature space. We are interested in finding the parameters w ∈ RF of a log-linear model, which best describe the possible labelings s ∈ S of x ∈ X , i.e.,\npw(s|x) ∝ exp ( w>φ(x, s) ) . (1)\nIn this paper, we tackle the weakly supervised setting, where we are given a training set D = {(xi, yi)Ni=1} containing N pairs, each composed by an input x ∈ X and some partially labeled data y ∈ Y ⊆ S. For every training pair, we divide the label space S = Y×H into two non-intersecting subspaces Y and H and refer to the missing information h ∈ H as hidden or latent.\nFor many applications, we can construct a loss function `(x,y)(s) which compares a configuration s with the labeled data (x, y) ∈ D, providing a measure for the fitness of the estimate. We incorporate this loss function in learning by considering the distribution\np(x,y)(s|w) ∝ exp(w>φ(x, s) + `(x,y)(s)). (2)\nIntuitively it places more probability mass on those parts of the space S that have a high loss, forcing the model to learn in a more difficult setting than the one encountered at inference, where the loss is not present.\nA maximum likelihood approach aims at finding model parameters w which assign highest probability to the data D. As we have no information available for the unobserved space H we marginalize it out, i.e., we average over all possible hidden states. Therefore, we define the loss-augmented likelihood of a prediction\nŷ ∈ Y when observing the pair (x, y) as p(x,y)(ŷ|w) ∝ ∑ ĥ∈H p(x,y)(ŷ, ĥ|w) = ∑ ĥ∈H p(x,y)(ŝ|w). (3) Assuming the data to be independent and identically distributed (i.i.d.), our goal is to minimize the negative log-likelihood − ln[p(w) ∏ (x,y)∈D p(x,y)(y|w)] with p(w) ∝ e−‖w‖ p p being a prior on the model parameters. As a result, the negative log-likelihood is a difference of convex terms\nC p ‖w‖pp + ∑ (x,y)∈D ( ln ∑ ŝ∈S exp ( w>φ(x, ŝ) + `(x,y)(ŝ) ) − − ln ∑ ĥ∈H exp ( w>φ(x, (y, ĥ)) + `c(x,y)((y, ĥ))\n) , (4) with the first two terms being the sum of the log-prior and the logarithm of the partition function. We take the loss of a ground truth configuration `c(x,y)((y, ĥ)) = `(x,y)((y, ĥ)) ≡ 0, independent of any estimate ĥ. To control the variance of the log-linear probability model we follow (Hazan & Urtasun, 2010; Pletscher et al., 2010) and introduce a temperature parameter , i.e.,\nC p ‖w‖pp + ∑ (x,y)∈D ( ln ∑ ŝ∈S exp ( w>φ(x, ŝ) + `(x,y)(ŝ) ) − − ln ∑ ĥ∈H exp ( w>φ(x, (y, ĥ)) + `c(x,y)((y, ĥ))\n) . (5) Importantly, defines an entire family of structured prediction tasks with latent variables. For = 1 we obtain the maximum likelihood (HCRF) framework, while = 0 results in the max-margin formulation for latent variables (LSSVM) minw C p ‖w‖ p p + maxŝ(w >φ(x, ŝ) + `(x,y)(ŝ)) − maxĥ(w >φ(x, (y, ĥ)) + `c(x,y)((y, ĥ))). Note that → 0 smoothly approximates the max-function via the soft-max."
    }, {
      "heading" : "3. Approximate latent structured loss minimization",
      "text" : "The unconstrained minimization problem in Eq. (5) w.r.t. w is challenging as it involves a sum of convex and concave terms containing exponentially sized sums. To make the minimization more tractable, we follow Yuille & Rangarajan (2003) and upper-bound the concave part via a minimization over a set of dual variables subsequently referred to as q(x,y)(h). This results in a convex dual and a non-convex bi-linear term as described in the following claim.\nProgram 1 Approximated structured prediction with latent variables\nmin d,λ,w\nC 2 ‖w‖22 + ∑ (x,y)∈D (∑ i∈S ci ln ∑ si exp ( φ(x,y),i(si)− ∑ α∈N(i) λ(x,y),i→α(si) ci ) +\n+ ∑ α∈E cα ln ∑ sα exp\n( φ(x,y),α(sα) + ∑ i∈N(α) λ(x,y),i→α(si)\ncα\n)) −\n− ∑ r wr ∑ (x,y) ∑ i∈Y φr,i(x, yi) + ∑ i∈H,hi φr,i(x, hi)d(x,y),i(hi) + ∑ α∈E,hα φr,α(x, (y, h)α)d(x,y),α(hα)  − ∑ (x,y)  ∑ i∈H,hi `c(x,y),i(x, hi)d(x,y),i(hi) + ∑ α∈EH,hα `c(x,y),α(x, (y, h)α)d(x,y),α(hα)\n − ∑ (x,y) (∑ i∈H ĉiH(d(x,y),i) + ∑ α∈EH ĉαH(d(x,y),α) )\ns.t. ∑ hα\\hi d(x,y),α(hα) = d(x,y),i(hi) ∀(x, y), i ∈ H, α ∈ N(i), hi ∈ Si\nd(x,y),i, d(x,y),α ∈ ∆\nf3 \nf2\n{ f1\n\n := d(x,y) ∈ C(x,y) ∀(x, y) ∈ D Claim 1 The function\nC p ‖w‖pp + ∑ (x,y) ( ln ∑ ŝ∈S exp ( w>φ(x, ŝ) + `(x,y)(ŝ) ) −\n− H(q(x,y))− Eq(x,y) [w >φ(x, (y, ĥ)) + `c(x, (y, ĥ))]\n) (6)\nconvex in w and q(x,y) separately, is an upper bound on Eq. (5),∀q(x,y)(h) ∈ ∆, with ∆ the probability simplex, H the entropy and E the expectation w.r.t. the stated distribution. The bound holds with equality for that q∗(x,y)(h) minimizing this cost function (Eq. (6)).\nProof: In supplementary material\nFor many real-world applications, the program in Claim 1 involves sums over exponentially sized sets S and H. They are exponentially sized as the observed and unobserved labels y = (si)i∈Y ∈ Y and h = (si)i∈H ∈ H are often tuples with elements si ∈ Si taking |Si| discrete states. Note that S = Y ∪H, with product spaces Y = ∏ i∈Y Si and H = ∏ i∈H Si. But the features usually describe interactions only between smaller subsets of random variables\nφr(x, s) = ∑ α∈Er φr,α(x, sα) + ∑ i∈Sr φr,i(x, si), (7)\nwhere Er and Sr denote the sets of factors and variables, and S = ⋃ r Sr. Note that each feature is described by a bipartite factor graph Gr with nodes originating from the variable set Sr and factors from Er. An edge connects a single node i ∈ Sr to a factor α ∈ Er iff i ∈ α. Consider the factor graph G = ⋃ r Gr where we define the set of neighbors N(i) := {α : i ∈\nα, ∀ α ∈ E} and N(α) := {i : i ∈ α, ∀ i ∈ S}. In many applications the loss functions ` and `c factorize in a similar fashion and are easily introduced in the graphical model G, i.e., `(x,y)(s) decomposes into local terms `(x,y),i(si), ∀i ∈ S and interaction terms `(x,y),α(sα) ∀α ∈ E, whereas `c(x,y)(ĥ) is structured according to the locally defined variables `c(x,y),i(ŝi), ∀i ∈ H and `c(x,y),α((y, ĥ)α).\nWe make use of the local structure of features and loss, and approximate the intractable function in Claim 1. In particular, let the probability distribution q(x,y)(h) be described by local beliefs d(x,y),i(hi) ∈ ∆ and factor beliefs d(x,y),α(hα) ∈ ∆. We approximate the marginal polytope by a local one using the marginalization constraints ∑ hα\\hi d(x,y),α(hα) = d(x,y),i(hi) ∀(x, y) ∈ D, i ∈ H, α ∈ N(i), hi ∈ Si. We introduce counting numbers ĉi and ĉα to allow for more flexibility in the approximation. To further obtain a tractable approximation for the partition function over S we approximate its Legendre transform, an entropy ranging over s ∈ S, via local terms. As those local terms are required to fulfill marginalization constraints for global consistency in the dual domain, we obtain Lagrange multipliers λ(x,y),i→α(si) ∀(x, y) ∈ D, i ∈ S, α ∈ N(i) and si ∈ Si on the graph G for the primal formulation. Note that those Lagrange multipliers are often interpreted as messages. For generality of the entropy approximations we again allow for counting numbers ci and cα. We now formally state our approximation.\nTheorem 1 The approximation of the program in Eq. (6) takes the form given in Program 1 where\nφ(x,y),i(si) = `(x,y),i(x, si) + ∑ r:i∈Sr wrφr,i(x, si) and\nφ(x,y),α(sα) = `(x,y),α(x, sα) + ∑ r:α∈Er wrφr,α(x, sα).\nProof: In supplementary material"
    }, {
      "heading" : "4. Message Passing Algorithm",
      "text" : "Before deriving an algorithm for solving Program 1, we begin by discussing the properties of the approximation. For counting numbers and annealing factor larger than zero, it is jointly convex in the messages λ(x,y),i→α(si) ∀(x, y) ∈ D, i ∈ S, α ∈ N(i), si ∈ Si and the model parameters w. It is also jointly convex in the messages and the beliefs d(x,y),i ∀i ∈ H and d(x,y),α ∀α ∈ EH, but not jointly convex when optimizing for both the weights and the beliefs. Cycling through blocks of variables and updating them in a block-coordinate descent manner is not guaranteed to converge as we cannot fulfill pseudoconvexity in every pair of coordinate blocks. Similar to other latent variable frameworks we can obtain convergence guarantees when employing instances of variational methods discussed, e.g ., in (Jordan et al., 1999) like the concave-convex procedure (CCCP) (Yuille & Rangarajan, 2003; Sriperumbudur & Lanckriet, 2009) by separating the cost function into two functions f1(w, λ) and f3(d), convex in their parameters and a bilinear term f2(w, d) connecting the two. We refer the reader to Program 1 for the definition of these functions. Here λ is the vector of all messages, d the vector of all beliefs, and C(x,y) ∀(x, y) ∈ D the set of all marginalization constraints.\nWithout loss of generality we can assume Program 1 to be bounded from below. Considering the biconvex cost function, it is intuitive to alternate between solving for the beliefs and then performing a gradient step in the direction of the weights and the messages. Due to the fact that the program is unconstrained in the messages and model parameters, one gradient step of the latter is sufficient. We refer the reader to the supplementary material for a detailed derivation of the algorithm. In short, updating the beliefs, i.e., the ‘latent variable prediction problem’ requires solving\nmin d(x,y)\nf2(w, d) + f3(d) s.t. d(x,y) ∈ C(x,y) (8)\nfor every (x, y) ∈ D independently, hence possibly in parallel. This problem reduces to a standard (convex) belief propagation task (Hazan & Shashua, 2010) which is guaranteed to find the global optimum for strictly positive counting numbers ĉi, ĉα and annealing factor . To update the weights and messages we are required to decrease the cost function of the fol-\nAlgorithm 1 latent structured prediction\nrepeat repeat\n//to solve latent variable prediction problem mind f2 + f3 s.t. ∀(x, y) d(x,y) ∈ D(x,y) until convergence //message passing update ∀(x, y), i ∈ S λ(x,y),i ← ∇λ(x,y),i(f1 + f2) = 0 //gradient step with step size η w ← w − η∇w(f1 + f2)\nuntil convergence\nlowing unconstrained program, convex in w and λ:\nmin w,λ f1(w, λ) + f2(w, d). (9)\nSimilar to the program given in Eq. (8), convergence is guaranteed for counting numbers and annealing factor being strictly positive. More importantly, for weights w, one gradient step of length η obtained via line search is sufficient for convergence guarantees. A solution for a block-coordinate descent step ∇λ(x,y),i(f1 + f2) = 0 w.r.t. λ(x,y),i→α(si) for (x, y) ∈ D, i ∈ S can be analytically computed jointly ∀α ∈ N(i), si. We briefly state the proposed algorithm for latent structured prediction in Alg. 1 while pointing the interested reader to the supplementary material for details. Some convergence properties of the proposed algorithm are summarized in the following claim.\nClaim 2 Alg. 1 is guaranteed to decrease the cost function of Program 1 at every iteration and guaranteed to converge to a minimum or a saddle point for , ci, cα, ĉi, ĉα > 0.\nProof: In supplementary material"
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section we demonstrate the effectiveness of our approach in the tasks of image segmentation as well as 3D scene understanding, and show that our method significantly outperforms LSSVM in terms of performance and HCRF in terms of speed.\nSegmentation: Our first task addresses segmentation of weakly labeled images. This is an interesting example, as the graphical model contains many loops. As ground truth we use the 14× 40 sized “ICML” tag given in Fig. 1. We created a dataset composed of 10 training and 10 test instances, where each observation x is obtained by adding zero mean, uniform noise on the ground truth labels yi ∈ Si = {1, . . . , 5}. We employ F = 2 features, a local potential based on the observations and a pairwise linear smothness potential.\nG is a grid-like graph, typical for many vision applications. In our experiments, we gradually increase the amount of missing labels from 0% to 100%, and determine at random which variables are hidden/latent.\nWe compare our approach to a standard HCRF and the latent structured SVM (LSSVM) of Yu & Joachims (2009) which uses belief propagation to solve the respective sub-problems. We use at most 200 outer iterations, 1000 inner iterations for our approach, and 200 outer iterations, 1000 message passing iterations, 20 cutting plane iterations for LSSVM. For computational reasons the HCRF method is restricted to only 10 outer iterations, 1000 message passing iterations and 5 CRF iterations resulting in a maximum of 50 updates of the model parameters. For a fair comparison, we also use 50 outer iterations and 1000 message passing iterations resulting in a maximum of 50 updates for our approach with = 1. All algorithms employed the same initialization. For our framework we additionally vary from 0 for the max-margin formulation to 1 for the maximum-likelihood formulation. Mean results averaged over 5 runs are depicted in Fig. 2. Our method results in good prediction for all values of , while the LSSVM fails in the presence of large amounts of latent variables. This is due to the fact that the cutting planes are not exactly computable for loopy models ((14 · 40)5 possibilities), and thus no decrease in the cost function is guaranteed. An example of the prediction of our approach and LSSVM in the presence of 90% latent variables is illustrated in Fig. 1, where LSSVM learns a wrong model that favors neighboring pixels to be different. In contrast, the HCRF performs similarly to our approach, but it takes on average 213.2min to compute a single HCRF experiment while only 6.2min are required for our approach with = 1. Since HCRF is not practical, we focus the rest of the experimental evaluation on LSSVM.\n3D Scene Understanding: Recovering the spatial layout of indoor scenes from a single image is an important problem in applications such as personal robotics and computer vision. Existing approaches formulate the problem as a structured prediction task focusing on estimating the 3D box which best describes the scene layout. Taking advantage of the Manhat-\ntan world assumption (i.e., there exist three dominant vanishing points which are orthonormal), the problem can be formulated as inference in a fully connected pairwise graphical model G composed of four random variables. As shown in Fig. 3(a), these variables represent the angles encoding the rays that originate from the respective vanishing points. Following existing approaches (Hedau et al., 2009; Lee et al., 2010), we employ F = 55 features based on geometric context (GC) and orientation maps (OM) and refer the interested reader to (Hoiem et al., 2007) and (Lee et al., 2009) for respective details. Our features count for each face in the cuboid (given a particular configuration of the layout) the number of pixels with a certain label for OM and the probability that such label exists for GC. Performance is measured as the percentage of pixels that have been correctly labeled, with labels, i.e., leftwall, right-wall, front-wall, ceiling, floor.\nWe first investigate how the layout estimation can benefit from the use of weakly labeled data. To this end we use a set of fully annotated images, denoted ‘fixed,’ and add a varying number {25, 50, 100} of images with only 1 or 2 angles labeled, i.e., 75% or 50% missing information. The randomly chosen unlabeled angles are treated as latent variables. All results are averaged over 12 runs, each being trained on a varying portion of the training set. Learning is performed with parameters C = 1, = 0.01 and all counting numbers equal to one. The results for 50% and 75% of missing information are detailed in Fig. 3(b) and Fig. 3(c) respectively. As expected, the prediction performance improves as a function of the number of fully labeled images, but more importantly, the performance also significantly improves as a function of the amount of weakly labeled data. Our performance also increases as a function of how much supervision the weakly annotated images have, i.e., 2 hidden variables outperforms having 3 latent variables per image.\nIn the next experiment we compare our approach to LSSVM. Again, all results are averaged over 12 runs. Note that we have to modify the stopping criteria of LSSVM as we are not guaranteed to find decreasing\nsteps at each iteration. In the absence of any clear criterion, we force LSSVM to perform at least 10 outer loops. Fig. 4 shows results when adding 25 (column 1), 50 (column 2) or 100 (column 3) weakly labeled data with 50% (row 1) or 75% (row 2) missing information. Our approach significantly outperforms LSSVM in all settings.\nA comparison of our approach to the state-of-the-art is shown in Tab. 1. Great performance is achieved with a small amount of supervision. Our fully supervised approach with 200 completely labeled examples results in a prediction error of 13.6% (Schwing et al., 2012).\nFig. 5 depicts improvements achieved by our approach compared to only using the fixed set of 10 fully labeled training images, as well as LSSVM. For LSSVM and our approach, we used an additional 100 images with 50% missing annotations. Prediction errors are indicated below the figures. We also provide illustrations for the image features we employed in the last two columns, i.e., orientation maps and geometric context. Interestingly, when trained with only 10 images, the model tends to miss walls and the ceiling."
    }, {
      "heading" : "6. Related Work and Discussion",
      "text" : "HCRFs (Quattoni et al., 2007) and LSSVMs (Yu & Joachims, 2009) are the most common frameworks employed to deal with latent variable models in structured prediction problems. The first contribution\nof our work described in Sec. 2 and formalized in Eq. (5) is to unify the aforementioned frameworks. More specifically, our max-margin formulation ( = 0) is identical to the formulation presented by Yu & Joachims (2009) when having p = 2, `c ≡ 0 ∀x, h, ĥ. The weak-label structured SVM presented in (Girshick et al., 2011) is obtained when = 0 and p = 2. For\n= 1, p = 2, `c ≡ 0 ∀x, y, ĥ and ` ≡ 0 ∀x, y, ŷ, ĥ we recover the likelihood formulation presented by Quattoni et al. (2007). For general , but without latent variables, i.e., H = ∅ ∀(x, y), our formulation reduces to the one presented in (Hazan & Urtasun, 2010) which generalizes structured SVMs and CRFs. Importantly, through the parameter our work introduces a family of new latent variable models in structured prediction that range between HCRF and LSSVM.\nThe main drawback of previous works (Quattoni et al., 2007; Yu & Joachims, 2009) is that they rely on computing the MAP estimate or the partition function at each iteration. In the case of general graphical models, approximate inference techniques like belief propagation are employed. The influence of approximate inference algorithms on structured SVMs (Taskar et al., 2004; 2005; Tsochantaridis et al., 2004) without latent variables has been investigated by (Finley & Joachims, 2008; Kulesza & Pereira, 2008), where they reported a “generally poor performance” when combining belief propagation and structured SVMs. As an LSSVM approach employs a structured SVM in every iteration, we expect a similar behavior when combining LSSVM with belief propagation. This was indeed the conclusion of our experiments in Sec 5. Similar to (Finley & Joachims, 2008), we found that ties within the solution mislead LSSVM.\nTo address efficiency Komodakis (2011) suggested to use a small number of CRF iterations. This, however, would not have convergence guarantees. Our second contribution, detailed in Sec. 3, is to directly include the approximation into the cost function. As a result we are able to derive a message passing algorithm that\nis significantly more efficient and guaranteed to converge. Our method needs to solve the ‘latent variable prediction problem’ just like LSSVM or HCRF. However, LSSVM and HCRF also require to solve the lossaugmented inference problem in every iteration before performing a parameter update. In contrast, we only need a single update on the messages λ before updating w. This results in large speedups as demonstrated in the previous section.\nIn our experiments we also observe that the loss function is very important when learning from weakly labeled data. In HCRFs, no loss function was proposed. For LSSVMs, the standard structured SVM loss was applied and adapted by Komodakis (2011). Girshick et al. (2011) proposed to introduce a second loss function into the ‘latent variable prediction problem’ while Tarlow & Zemel (2012) investigate the impact of higher order loss functions. Kumar et al. (2010) proposed the self paced learning algorithm, which starts with “easy” examples before gradually adding more difficult ones. Their formulation is based on LSSVMs but can also be applied to our framework.\nOur algorithm is easily parallelized w.r.t. to the data samples. Our C++ implementation uses OpenMP and MPI for parallelization in both shared and distributed memory environments. To parallelize message passing one could employ (Schwing et al., 2011). The sources are available on http://alexander-schwing.de."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We have proposed a framework that unifies HCRF and latent structured SVMs. We have then constructed an\napproximation of the resulting intractable optimization problem using local entropies, and derived an algorithm for general graphs that leverages the graphical model structure imposed by the features. We have demonstrated the effectiveness of our approach on a segmentation task as well as predicting the 3D layout from single images. We plan to extend this work in two directions along the lines of v.d.Maaten et al. (2011), by addressing non-linear structured prediction with latent variables and by investigating relations to deep belief networks."
    } ],
    "references" : [ {
      "title" : "Training Structural SVMs when Exact Inference is Intractable",
      "author" : [ "T. Finley", "T. Joachims" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Finley and Joachims,? \\Q2008\\E",
      "shortCiteRegEx" : "Finley and Joachims",
      "year" : 2008
    }, {
      "title" : "Object Detection with Grammer Models",
      "author" : [ "R. Girshick", "P. Felzenszwalb", "D. McAllester" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2011
    }, {
      "title" : "Norm-Product Belief Propagtion: Primal-Dual Message-Passing for LPRelaxation and Approximate-Inference",
      "author" : [ "T. Hazan", "A. Shashua" ],
      "venue" : "Trans. on Information Theory,",
      "citeRegEx" : "Hazan and Shashua,? \\Q2010\\E",
      "shortCiteRegEx" : "Hazan and Shashua",
      "year" : 2010
    }, {
      "title" : "A Primal-Dual MessagePassing Algorithm for Approximated Large Scale Structured Prediction",
      "author" : [ "T. Hazan", "R. Urtasun" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Hazan and Urtasun,? \\Q2010\\E",
      "shortCiteRegEx" : "Hazan and Urtasun",
      "year" : 2010
    }, {
      "title" : "Recovering the Spatial Layout of Cluttered Rooms",
      "author" : [ "V. Hedau", "D. Hoiem", "D. Forsyth" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Hedau et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hedau et al\\.",
      "year" : 2009
    }, {
      "title" : "Recovering surface layout from an image",
      "author" : [ "D. Hoiem", "A.A. Efros", "M. Hebert" ],
      "venue" : null,
      "citeRegEx" : "Hoiem et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hoiem et al\\.",
      "year" : 2007
    }, {
      "title" : "An Introduction to Variational Methods for Graphical Models",
      "author" : [ "M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Jordan et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jordan et al\\.",
      "year" : 1999
    }, {
      "title" : "Learning to Cluster Using High Order Graphical Models with Latent Variables",
      "author" : [ "N. Komodakis" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Komodakis,? \\Q2011\\E",
      "shortCiteRegEx" : "Komodakis",
      "year" : 2011
    }, {
      "title" : "Structured Learning with Approximate Inference",
      "author" : [ "A. Kulesza", "F. Pereira" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kulesza and Pereira,? \\Q2008\\E",
      "shortCiteRegEx" : "Kulesza and Pereira",
      "year" : 2008
    }, {
      "title" : "Self-Paced Learning for Latent Variable Models",
      "author" : [ "P. Kumar", "B. Packer", "D. Koller" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kumar et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2010
    }, {
      "title" : "Geometric Reasoning for Single Image Structure Recovery",
      "author" : [ "D.C. Lee", "M. Hebert", "T. Kanade" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Lee et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2009
    }, {
      "title" : "Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces",
      "author" : [ "D.C. Lee", "A. Gupta", "M. Hebert", "T. Kanade" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lee et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2010
    }, {
      "title" : "Entropy and Margin Maximization for Structured Output Learning",
      "author" : [ "P. Pletscher", "C.S. Ong", "J.M. Buhmann" ],
      "venue" : "In ECML PKDD,",
      "citeRegEx" : "Pletscher et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pletscher et al\\.",
      "year" : 2010
    }, {
      "title" : "Distributed Message Passing for Large Scale Graphical Models",
      "author" : [ "A.G. Schwing", "T. Hazan", "M. Pollefeys", "R. Urtasun" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Schwing et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Schwing et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient Structured Prediction for 3D Indoor Scene Understanding",
      "author" : [ "A.G. Schwing", "T. Hazan", "M. Pollefeys", "R. Urtasun" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Schwing et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Schwing et al\\.",
      "year" : 2012
    }, {
      "title" : "On the Convergence of the Concave-Convex Procedure",
      "author" : [ "B. Sriperumbudur", "G. Lanckriet" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sriperumbudur and Lanckriet,? \\Q2009\\E",
      "shortCiteRegEx" : "Sriperumbudur and Lanckriet",
      "year" : 2009
    }, {
      "title" : "Structured Output Learning with Higher Order Loss Functions",
      "author" : [ "D. Tarlow", "R.S. Zemel" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Tarlow and Zemel,? \\Q2012\\E",
      "shortCiteRegEx" : "Tarlow and Zemel",
      "year" : 2012
    }, {
      "title" : "Learning Structured Prediction Models: A Large Margin Approach",
      "author" : [ "B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Taskar et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Taskar et al\\.",
      "year" : 2005
    }, {
      "title" : "Support Vector Learning for Interdependent and Structured Output Spaces",
      "author" : [ "I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Tsochantaridis et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Tsochantaridis et al\\.",
      "year" : 2004
    }, {
      "title" : "Discriminative Learning with Latent Variables for Cluttered Indoor Scene Understanding",
      "author" : [ "H. Wang", "S. Gould", "D. Koller" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Wang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning Structural SVMs with Latent Variables",
      "author" : [ "Yu", "C.-N. J", "T. Joachims" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Yu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2009
    }, {
      "title" : "The Concave-Convex Procedure (CCCP)",
      "author" : [ "A.L. Yuille", "A. Rangarajan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Yuille and Rangarajan,? \\Q2003\\E",
      "shortCiteRegEx" : "Yuille and Rangarajan",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "To control the variance of the log-linear probability model we follow (Hazan & Urtasun, 2010; Pletscher et al., 2010) and introduce a temperature parameter , i.",
      "startOffset" : 70,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : ", in (Jordan et al., 1999) like the concave-convex procedure (CCCP) (Yuille & Rangarajan, 2003; Sriperumbudur & Lanckriet, 2009) by separating the cost function into two functions f1(w, λ) and f3(d), convex in their parameters and a bilinear term f2(w, d) connecting the two.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "Following existing approaches (Hedau et al., 2009; Lee et al., 2010), we employ F = 55 features based on geometric context (GC) and orientation maps (OM) and refer the interested reader to (Hoiem et al.",
      "startOffset" : 30,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "Following existing approaches (Hedau et al., 2009; Lee et al., 2010), we employ F = 55 features based on geometric context (GC) and orientation maps (OM) and refer the interested reader to (Hoiem et al.",
      "startOffset" : 30,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : ", 2010), we employ F = 55 features based on geometric context (GC) and orientation maps (OM) and refer the interested reader to (Hoiem et al., 2007) and (Lee et al.",
      "startOffset" : 128,
      "endOffset" : 148
    }, {
      "referenceID" : 10,
      "context" : ", 2007) and (Lee et al., 2009) for respective details.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "fully weakly Error (Hoiem et al., 2007) 209 0 28.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "9% (Hedau et al., 2009) 209 0 21.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "2% (Wang et al., 2010) 209 0 22.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 11,
      "context" : "2% (Lee et al., 2010) 209 0 18.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "Comparison to state-of-the-art on the layout data set of (Hedau et al., 2009).",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : "6% (Schwing et al., 2012).",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "The weak-label structured SVM presented in (Girshick et al., 2011) is obtained when = 0 and p = 2.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : "The weak-label structured SVM presented in (Girshick et al., 2011) is obtained when = 0 and p = 2. For = 1, p = 2, ` ≡ 0 ∀x, y, ĥ and ` ≡ 0 ∀x, y, ŷ, ĥ we recover the likelihood formulation presented by Quattoni et al. (2007). For general , but without latent variables, i.",
      "startOffset" : 44,
      "endOffset" : 226
    }, {
      "referenceID" : 18,
      "context" : "The influence of approximate inference algorithms on structured SVMs (Taskar et al., 2004; 2005; Tsochantaridis et al., 2004) without latent variables has been investigated by (Finley & Joachims, 2008; Kulesza & Pereira, 2008), where they reported a “generally poor performance” when combining belief propagation and structured SVMs.",
      "startOffset" : 69,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "To address efficiency Komodakis (2011) suggested to use a small number of CRF iterations.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "For LSSVMs, the standard structured SVM loss was applied and adapted by Komodakis (2011). Girshick et al.",
      "startOffset" : 72,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "Girshick et al. (2011) proposed to introduce a second loss function into the ‘latent variable prediction problem’ while Tarlow & Zemel (2012) investigate the impact of higher order loss functions.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Girshick et al. (2011) proposed to introduce a second loss function into the ‘latent variable prediction problem’ while Tarlow & Zemel (2012) investigate the impact of higher order loss functions.",
      "startOffset" : 0,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "Girshick et al. (2011) proposed to introduce a second loss function into the ‘latent variable prediction problem’ while Tarlow & Zemel (2012) investigate the impact of higher order loss functions. Kumar et al. (2010) proposed the self paced learning algorithm, which starts with “easy” examples before gradually adding more difficult ones.",
      "startOffset" : 0,
      "endOffset" : 217
    }, {
      "referenceID" : 13,
      "context" : "To parallelize message passing one could employ (Schwing et al., 2011).",
      "startOffset" : 48,
      "endOffset" : 70
    } ],
    "year" : 2012,
    "abstractText" : "In this paper we propose a unified framework for structured prediction with latent variables which includes hidden conditional random fields and latent structured support vector machines as special cases. We describe a local entropy approximation for this general formulation using duality, and derive an efficient message passing algorithm that is guaranteed to converge. We demonstrate its effectiveness in the tasks of image segmentation as well as 3D indoor scene understanding from single images, showing that our approach is superior to latent structured support vector machines and hidden conditional random fields.",
    "creator" : "LaTeX with hyperref package"
  }
}