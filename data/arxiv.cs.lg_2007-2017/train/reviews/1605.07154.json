{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations", "abstract": "We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.", "histories": [["v1", "Mon, 23 May 2016 19:40:50 GMT  (119kb,D)", "http://arxiv.org/abs/1605.07154v1", "15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["behnam neyshabur", "yuhuai wu", "ruslan salakhutdinov", "nati srebro"], "accepted": true, "id": "1605.07154"}
