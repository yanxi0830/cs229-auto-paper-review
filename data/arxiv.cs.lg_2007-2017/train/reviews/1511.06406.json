{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Denoising Criterion for Variational Auto-Encoding Framework", "abstract": "Denoising autoencoders (DAE) are trained to reconstruct their clean input with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer that encourages this noise injection. In this paper, we show that injecting noise both in input and in the stochastic hidden layer can be advantageous and we propose a modified variational lower bound as an improved objective function in this setup. If noise is injected in input, then the standard VAE lower bound involves marginalizing the encoder conditional distribution over the input noise, which makes the training criterion intractable. Instead, we propose a modified training criterion which corresponds to a tighter bound, when noise is injected in input. Experimentally, we find that the proposed denoising variational autoencoder (DVAE) yields better average log-likelihood than the VAE and the importance weighted auto-encoder on the MNIST and Frey Face datasets.", "histories": [["v1", "Thu, 19 Nov 2015 21:56:21 GMT  (3409kb,D)", "http://arxiv.org/abs/1511.06406v1", "ICLR conference submission"], ["v2", "Mon, 4 Jan 2016 15:12:46 GMT  (3592kb,D)", "http://arxiv.org/abs/1511.06406v2", "ICLR conference submission"]], "COMMENTS": "ICLR conference submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel jiwoong im", "sungjin ahn", "roland memisevic", "yoshua bengio"], "accepted": true, "id": "1511.06406"}
