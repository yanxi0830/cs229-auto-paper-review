{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "Variance Reduced Stochastic Gradient Descent with Neighbors", "abstract": "Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet it is also known to be slow relative to steepest descent. The variance in the stochastic update directions only allows for sublinear or (with iterate averaging) linear convergence rates. Recently, variance reduction techniques such as SVRG and SAGA have been proposed to overcome this weakness. With asymptotically vanishing variance, a constant step size can be maintained, resulting in geometric convergence rates. However, these methods are either based on occasional computations of full gradients at pivot points (SVRG), or on keeping per data point corrections in memory (SAGA). This has the disadvantage that one cannot employ these methods in a streaming setting and that speed-ups relative to SGD may need a certain number of epochs in order to materialize. This paper investigates a new class of algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points. While not meant to be offering advantages in an asymptotic setting, there are significant benefits in the transient optimization phase, in particular in a streaming or single-epoch setting. We investigate this family of algorithms in a thorough analysis and show supporting experimental results. As a side-product we provide a simple and unified proof technique for a broad class of variance reduction algorithms.", "histories": [["v1", "Thu, 11 Jun 2015 13:14:33 GMT  (793kb,D)", "http://arxiv.org/abs/1506.03662v1", null], ["v2", "Thu, 5 Nov 2015 12:30:49 GMT  (332kb,D)", "http://arxiv.org/abs/1506.03662v2", null], ["v3", "Tue, 17 Nov 2015 22:00:11 GMT  (330kb,D)", "http://arxiv.org/abs/1506.03662v3", null], ["v4", "Fri, 26 Feb 2016 19:55:56 GMT  (334kb,D)", "http://arxiv.org/abs/1506.03662v4", "Appears in: Advances in Neural Information Processing Systems 28 (NIPS 2015). 13 pages"]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["thomas hofmann", "aur\u00e9lien lucchi", "simon lacoste-julien", "brian mcwilliams"], "accepted": true, "id": "1506.03662"}
