{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2016", "title": "Sequence-Level Knowledge Distillation", "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al, 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with only a decrease of 0.2 BLEU. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search.", "histories": [["v1", "Sat, 25 Jun 2016 18:16:39 GMT  (229kb,D)", "http://arxiv.org/abs/1606.07947v1", null], ["v2", "Thu, 4 Aug 2016 17:24:18 GMT  (231kb,D)", "http://arxiv.org/abs/1606.07947v2", "EMNLP 2016"], ["v3", "Mon, 8 Aug 2016 15:02:54 GMT  (231kb,D)", "http://arxiv.org/abs/1606.07947v3", "EMNLP 2016"], ["v4", "Thu, 22 Sep 2016 01:17:12 GMT  (232kb,D)", "http://arxiv.org/abs/1606.07947v4", "EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["yoon kim", "alexander m rush"], "accepted": true, "id": "1606.07947"}
