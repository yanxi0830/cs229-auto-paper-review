{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Can recursive neural tensor networks learn logical reasoning?", "abstract": "Recursive neural network models and their accompanying vector representations for words have seen success in an array of increasingly semantically sophisticated tasks, but almost nothing is known about their ability to accurately capture the aspects of linguistic meaning that are necessary for interpretation or reasoning. To evaluate this, I train a recursive model on a new corpus of constructed examples of logical reasoning in short sentences, like the inference of \"some animal walks\" from \"some dog walks\" or \"some cat walks,\" given that dogs and cats are animals. The results are promising for the ability of these models to capture logical reasoning, but the model tested here appears to learn representations that are quite specific to the templatic structures of the problems seen in training, and that generalize beyond them only to a limited degree.", "histories": [["v1", "Sat, 21 Dec 2013 02:29:42 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v1", "Submitted for presentation at ICLR 2014"], ["v2", "Tue, 24 Dec 2013 01:42:09 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v2", "Submitted for presentation at ICLR 2014. Source code and data:this http URL"], ["v3", "Tue, 4 Feb 2014 18:02:09 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v3", "Submitted for presentation at ICLR 2014. Source code and data:this http URL"], ["v4", "Sat, 15 Feb 2014 20:59:04 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v4", "Submitted for presentation at ICLR 2014. Source code and data:this http URL"]], "COMMENTS": "Submitted for presentation at ICLR 2014", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["samuel r bowman"], "accepted": false, "id": "1312.6192"}
