{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models", "abstract": "Conventional deep neural networks (DNN) for speech acoustic modeling rely on Gaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary class labels as the targets for DNN training. Subword classes in speech recognition systems correspond to context-dependent tied states or senones. The present work addresses some limitations of GMM-HMM senone alignments for DNN training. We hypothesize that the senone probabilities obtained from a DNN trained with binary labels can provide more accurate targets to learn better acoustic models. However, DNN outputs bear inaccuracies which are exhibited as high dimensional unstructured noise, whereas the informative components are structured and low-dimensional. We exploit principle component analysis (PCA) and sparse coding to characterize the senone subspaces. Enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for DNN acoustic modeling, that also enables training with untranscribed data. Experiments conducted on AMI corpus shows 4.6% relative reduction in word error rate.", "histories": [["v1", "Tue, 18 Oct 2016 16:02:10 GMT  (2065kb,D)", "http://arxiv.org/abs/1610.05688v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.HC cs.LG", "authors": ["pranay dighe", "afsaneh asaei", "herve bourlard"], "accepted": false, "id": "1610.05688"}
