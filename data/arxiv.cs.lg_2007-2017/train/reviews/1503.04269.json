{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2015", "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning", "abstract": "In this paper we introduce the idea of improving the performance of parametric temporal-difference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD($\\lambda$)'s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per-step computation linear in the number of function approximation parameters are the gradient-TD family of methods including TDC, GTD($\\lambda$), and GQ($\\lambda$). Compared to these methods, our _emphatic TD($\\lambda$)_ is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. On the other hand, the range of problems for which it is stable but does not converge with probability one is larger than for gradient-TD methods. Our treatment includes general state-dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states.", "histories": [["v1", "Sat, 14 Mar 2015 04:44:20 GMT  (3980kb,D)", "http://arxiv.org/abs/1503.04269v1", "29 pages"], ["v2", "Tue, 21 Apr 2015 02:21:57 GMT  (10208kb,D)", "http://arxiv.org/abs/1503.04269v2", "29 pages This is a significant revision based on the first set of reviews. The most important change was to signal early that the main result is about stability, not convergence"]], "COMMENTS": "29 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["richard s sutton", "a rupam mahmood", "martha white"], "accepted": false, "id": "1503.04269"}
