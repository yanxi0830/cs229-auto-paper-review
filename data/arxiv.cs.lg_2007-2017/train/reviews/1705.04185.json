{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "A First Empirical Study of Emphatic Temporal Difference Learning", "abstract": "In this paper we present the first empirical study of the emphatic temporal-difference learning algorithm (ETD), comparing it with conventional temporal-difference learning, in particular, with linear TD(0), on on-policy and off-policy variations of the Mountain Car problem. The initial motivation for developing ETD was that it has good convergence properties under \\emph{off}-policy training (Sutton, Mahmood \\&amp; White 2016), but it is also a new algorithm for the \\emph{on}-policy case. In both our on-policy and off-policy experiments, we found that each method converged to a characteristic asymptotic level of error, with ETD better than TD(0). TD(0) achieved a still lower error level temporarily before falling back to its higher asymptote, whereas ETD never showed this kind of \"bounce\". In the off-policy case (in which TD(0) is not guaranteed to converge), ETD was significantly slower.", "histories": [["v1", "Thu, 11 May 2017 13:52:52 GMT  (2609kb,D)", "https://arxiv.org/abs/1705.04185v1", "5 pages, Accepted to NIPS Continual Learning and Deep Networks workshop, 2016"], ["v2", "Fri, 12 May 2017 16:49:38 GMT  (2609kb,D)", "http://arxiv.org/abs/1705.04185v2", "5 pages, Accepted to NIPS Continual Learning and Deep Networks workshop, 2016"]], "COMMENTS": "5 pages, Accepted to NIPS Continual Learning and Deep Networks workshop, 2016", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["sina ghiassian", "banafsheh rafiee", "richard s sutton"], "accepted": false, "id": "1705.04185"}
