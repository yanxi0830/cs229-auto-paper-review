{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Automatic differentiation in machine learning: a survey", "abstract": "Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD) is a technique for calculating derivatives efficiently and accurately, established in fields such as computational fluid dynamics, nuclear engineering, and atmospheric sciences. Despite its advantages and use in other fields, machine learning practitioners have been little influenced by AD and make scant use of available tools. We survey the intersection of AD and machine learning, cover applications where AD has the potential to make a big impact, and report on the recent developments in the adoption this technique. We also aim to dispel some misconceptions that we think have impeded the widespread awareness of AD within the machine learning community.", "histories": [["v1", "Fri, 20 Feb 2015 04:20:47 GMT  (70kb,D)", "http://arxiv.org/abs/1502.05767v1", "28 pages, 5 figures"], ["v2", "Sun, 19 Apr 2015 16:49:13 GMT  (79kb,D)", "http://arxiv.org/abs/1502.05767v2", "29 pages, 5 figures"], ["v3", "Thu, 17 Aug 2017 16:45:07 GMT  (69kb)", "http://arxiv.org/abs/1502.05767v3", "34 pages, 5 figures"]], "COMMENTS": "28 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.SC cs.LG", "authors": ["atilim gunes baydin", "barak a pearlmutter", "alexey", "reyevich radul", "jeffrey mark siskind"], "accepted": false, "id": "1502.05767"}
