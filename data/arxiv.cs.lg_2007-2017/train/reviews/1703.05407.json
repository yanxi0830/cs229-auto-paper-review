{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play", "abstract": "We describe a simple scheme that allows an agent to explore its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on (nearly) reversible environments, or environments that can be reset, and Alice will \"propose\" the task by running a set of actions and then Bob must partially undo, or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When deployed on an RL task within the environment, this unsupervised training reduces the number of episodes needed to learn.", "histories": [["v1", "Wed, 15 Mar 2017 22:27:43 GMT  (1257kb,D)", "http://arxiv.org/abs/1703.05407v1", null], ["v2", "Wed, 19 Apr 2017 23:32:25 GMT  (1227kb,D)", "http://arxiv.org/abs/1703.05407v2", null], ["v3", "Sun, 4 Jun 2017 12:44:45 GMT  (1430kb,D)", "http://arxiv.org/abs/1703.05407v3", null], ["v4", "Sun, 29 Oct 2017 16:02:21 GMT  (1415kb,D)", "http://arxiv.org/abs/1703.05407v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sainbayar sukhbaatar", "zeming lin", "ilya kostrikov", "gabriel synnaeve", "arthur szlam"], "accepted": false, "id": "1703.05407"}
