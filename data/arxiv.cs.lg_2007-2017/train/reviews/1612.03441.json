{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2016", "title": "Lock-Free Optimization for Non-Convex Problems", "abstract": "Stochastic gradient descent~(SGD) and its variants have attracted much attention in machine learning due to their efficiency and effectiveness for optimization. To handle large-scale problems, researchers have recently proposed several lock-free strategy based parallel SGD~(LF-PSGD) methods for multi-core systems. However, existing works have only proved the convergence of these LF-PSGD methods for convex problems. To the best of our knowledge, no work has proved the convergence of the LF-PSGD methods for non-convex problems. In this paper, we provide the theoretical proof about the convergence of two representative LF-PSGD methods, Hogwild! and AsySVRG, for non-convex problems. Empirical results also show that both Hogwild! and AsySVRG are convergent on non-convex problems, which successfully verifies our theoretical results.", "histories": [["v1", "Sun, 11 Dec 2016 17:26:43 GMT  (326kb,D)", "http://arxiv.org/abs/1612.03441v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shen-yi zhao", "gong-duo zhang", "wu-jun li"], "accepted": true, "id": "1612.03441"}
