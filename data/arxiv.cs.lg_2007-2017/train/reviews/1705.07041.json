{"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Posterior sampling for reinforcement learning: worst-case regret bounds", "abstract": "We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of $\\tilde{O}(D\\sqrt{SAT})$ for any communicating MDP with $S$ states, $A$ actions and diameter $D$, when $T\\ge S^5A$. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon $T$. This result improves over the best previously known upper bound of $\\tilde{O}(DS\\sqrt{AT})$ achieved by any algorithm in this setting, and matches the dependence on $S$ in the established lower bound of $\\Omega(\\sqrt{DSAT})$ for this problem. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.", "histories": [["v1", "Fri, 19 May 2017 15:10:21 GMT  (48kb)", "http://arxiv.org/abs/1705.07041v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shipra agrawal", "randy jia"], "accepted": true, "id": "1705.07041"}
