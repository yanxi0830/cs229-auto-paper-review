{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2014", "title": "Reweighted Wake-Sleep", "abstract": "Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is confirmed experimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure, which also provides a natural way to estimate the likelihood itself. Based on this interpretation, we propose that a sigmoid belief network is not sufficiently powerful for the layers of the inference network, in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models.", "histories": [["v1", "Wed, 11 Jun 2014 00:44:31 GMT  (593kb,D)", "http://arxiv.org/abs/1406.2751v1", null], ["v2", "Fri, 5 Dec 2014 23:30:10 GMT  (593kb,D)", "http://arxiv.org/abs/1406.2751v2", null], ["v3", "Sat, 20 Dec 2014 04:25:43 GMT  (643kb,D)", "http://arxiv.org/abs/1406.2751v3", null], ["v4", "Thu, 16 Apr 2015 17:22:58 GMT  (643kb,D)", "http://arxiv.org/abs/1406.2751v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["j\\\"org bornschein", "yoshua bengio"], "accepted": true, "id": "1406.2751"}
