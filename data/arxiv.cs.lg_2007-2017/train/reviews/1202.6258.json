{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2012", "title": "A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets", "abstract": "We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. Numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms.", "histories": [["v1", "Tue, 28 Feb 2012 15:42:51 GMT  (124kb,D)", "https://arxiv.org/abs/1202.6258v1", null], ["v2", "Thu, 5 Jul 2012 19:41:32 GMT  (216kb,D)", "http://arxiv.org/abs/1202.6258v2", null], ["v3", "Fri, 6 Jul 2012 10:59:04 GMT  (216kb,D)", "http://arxiv.org/abs/1202.6258v3", null], ["v4", "Mon, 11 Mar 2013 19:54:48 GMT  (217kb,D)", "http://arxiv.org/abs/1202.6258v4", "The notable changes over the current version: - worked example of convergence rates showing SAG can be faster than first-order methods - pointing out that the storage cost is O(n) for linear models - the more-stable line-search - comparison to additional optimal SG methods - comparison to rates of coordinate descent methods in quadratic case"]], "reviews": [], "SUBJECTS": "math.OC cs.LG", "authors": ["nicolas le roux", "mark w schmidt", "francis r bach"], "accepted": true, "id": "1202.6258"}
