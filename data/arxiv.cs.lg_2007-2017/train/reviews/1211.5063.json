{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2012", "title": "On the difficulty of training recurrent neural networks", "abstract": "Training Recurrent Neural Networks is more troublesome than feedforward ones because of the vanishing and exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to understand the fundamental issues underlying the exploding gradient problem by exploring it from an analytical, a geometric and a dynamical system perspective. Our analysis is used to justify the simple yet effective solution of norm clipping the exploded gradient. In the experimental section, the comparison between this heuristic solution and standard SGD provides empirical evidence towards our hypothesis as well as it shows that such a heuristic is required to reach state of the art results on a character prediction task and a polyphonic music prediction one.", "histories": [["v1", "Wed, 21 Nov 2012 15:40:11 GMT  (353kb,D)", "http://arxiv.org/abs/1211.5063v1", null], ["v2", "Sat, 16 Feb 2013 00:35:48 GMT  (447kb,D)", "http://arxiv.org/abs/1211.5063v2", "Improved description of the exploding gradient problem and description and analysis of the vanishing gradient problem"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["razvan pascanu", "tomas mikolov", "yoshua bengio"], "accepted": true, "id": "1211.5063"}
