{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation", "abstract": "We propose a novel deep learning model, which supports permutation invariant training (PIT), for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Different from most of the prior arts that treat speech separation as a multi-class regression problem and the deep clustering technique that considers it a segmentation (or clustering) problem, our model optimizes for the separation regression error, ignoring the order of mixing sources. This strategy cleverly solves the long-lasting label permutation problem that has prevented progress on deep learning based techniques for speech separation. Experiments on the equal-energy mixing setup of a Danish corpus confirms the effectiveness of PIT. We believe improvements built upon PIT can eventually solve the cocktail-party problem and enable real-world adoption of, e.g., automatic meeting transcription and multi-party human-computer interaction, where overlapping speech is common.", "histories": [["v1", "Fri, 1 Jul 2016 17:34:16 GMT  (226kb,D)", "http://arxiv.org/abs/1607.00325v1", "9 pages"], ["v2", "Tue, 3 Jan 2017 19:57:37 GMT  (131kb,D)", "http://arxiv.org/abs/1607.00325v2", "5 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["dong yu", "morten kolb{\\ae}k", "zheng-hua tan", "jesper jensen"], "accepted": false, "id": "1607.00325"}
