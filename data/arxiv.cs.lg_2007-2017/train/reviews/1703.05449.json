{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Minimax Regret Bounds for Reinforcement Learning", "abstract": "We consider the problem of efficient exploration in finite horizon MDPs.We show that an optimistic modification to model-based value iteration, can achieve a regret bound $\\tilde{O}( \\sqrt{HSAT} + H^2S^2A+H\\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the time elapsed. This result improves over the best previous known bound $\\tilde{O}(HS \\sqrt{AT})$ achieved by the UCRL2 algorithm.The key significance of our new results is that when $T\\geq H^3S^3A$ and $SA\\geq H$, it leads to a regret of $\\tilde{O}(\\sqrt{HSAT})$ that matches the established lower bounds of $\\Omega(\\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we use \"exploration bonuses\" based on Bernstein's inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in $H$).", "histories": [["v1", "Thu, 16 Mar 2017 01:31:33 GMT  (33kb)", "http://arxiv.org/abs/1703.05449v1", null], ["v2", "Sat, 1 Jul 2017 13:00:06 GMT  (30kb)", "http://arxiv.org/abs/1703.05449v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["mohammad gheshlaghi azar", "ian osband", "r\u00e9mi munos"], "accepted": true, "id": "1703.05449"}
