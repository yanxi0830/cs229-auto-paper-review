{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "histories": [["v1", "Sat, 21 Dec 2013 00:56:56 GMT  (5891kb,D)", "http://arxiv.org/abs/1312.6186v1", "6 pages, 4 figures"]], "COMMENTS": "6 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CV cs.DC cs.LG cs.NE", "authors": ["thomas paine", "hailin jin", "jianchao yang", "zhe lin", "thomas huang"], "accepted": false, "id": "1312.6186"}
