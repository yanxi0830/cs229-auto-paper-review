{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "The return of AdaBoost.MH: multi-class Hamming trees", "abstract": "Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH.", "histories": [["v1", "Fri, 20 Dec 2013 19:33:26 GMT  (97kb,D)", "http://arxiv.org/abs/1312.6086v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bal\\'azs k\\'egl"], "accepted": true, "id": "1312.6086"}
