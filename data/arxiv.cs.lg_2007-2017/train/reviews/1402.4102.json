{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2014", "title": "Stochastic Gradient Hamiltonian Monte Carlo", "abstract": "Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals. The popularity of such methods has grown significantly in recent years. However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system---such a computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data. In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad. To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution. Results on simulated data validate our theory. We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.", "histories": [["v1", "Mon, 17 Feb 2014 19:57:59 GMT  (147kb,D)", "https://arxiv.org/abs/1402.4102v1", null], ["v2", "Mon, 12 May 2014 06:38:21 GMT  (110kb,D)", "http://arxiv.org/abs/1402.4102v2", "ICML 2014 version"]], "reviews": [], "SUBJECTS": "stat.ME cs.LG stat.ML", "authors": ["tianqi chen", "emily b fox", "carlos guestrin"], "accepted": true, "id": "1402.4102"}
