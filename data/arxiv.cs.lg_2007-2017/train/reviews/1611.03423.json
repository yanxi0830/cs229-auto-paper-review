{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "DiffSharp: An AD Library for .NET Languages", "abstract": "DiffSharp is an algorithmic differentiation or automatic differentiation (AD) library for the .NET ecosystem, which is targeted by the C# and F# languages, among others. The library has been designed with machine learning applications in mind, allowing very succinct implementations of models and optimization routines. DiffSharp is implemented in F# and exposes forward and reverse AD operators as general nestable higher-order functions, usable by any .NET language. It provides high-performance linear algebra primitives---scalars, vectors, and matrices, with a generalization to tensors underway---that are fully supported by all the AD operators, and which use a BLAS/LAPACK backend via the highly optimized OpenBLAS library. DiffSharp currently uses operator overloading, but we are developing a transformation-based version of the library using F#'s \"code quotation\" metaprogramming facility. Work on a CUDA-based GPU backend is also underway.", "histories": [["v1", "Thu, 10 Nov 2016 17:50:06 GMT  (10kb)", "http://arxiv.org/abs/1611.03423v1", "Extended abstract presented at the AD 2016 Conference, Sep 2016, Oxford UK"]], "COMMENTS": "Extended abstract presented at the AD 2016 Conference, Sep 2016, Oxford UK", "reviews": [], "SUBJECTS": "cs.MS cs.LG", "authors": ["at{\\i}l{\\i}m g\\\"une\\c{s} baydin", "barak a pearlmutter", "jeffrey mark siskind"], "accepted": false, "id": "1611.03423"}
