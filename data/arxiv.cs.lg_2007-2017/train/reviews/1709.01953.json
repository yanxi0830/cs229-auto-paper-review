{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2017", "title": "Implicit Regularization in Deep Learning", "abstract": "In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.", "histories": [["v1", "Wed, 6 Sep 2017 18:12:04 GMT  (4563kb,D)", "http://arxiv.org/abs/1709.01953v1", "PhD Thesis"], ["v2", "Fri, 8 Sep 2017 03:27:06 GMT  (4563kb,D)", "http://arxiv.org/abs/1709.01953v2", "PhD Thesis"]], "COMMENTS": "PhD Thesis", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["behnam neyshabur"], "accepted": false, "id": "1709.01953"}
