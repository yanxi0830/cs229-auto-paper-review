{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "CoCoA: A General Framework for Communication-Efficient Distributed Optimization", "abstract": "The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for the distributed environment, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets.", "histories": [["v1", "Mon, 7 Nov 2016 17:49:49 GMT  (406kb,D)", "http://arxiv.org/abs/1611.02189v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["virginia smith", "simone forte", "chenxin ma", "martin takac", "michael i jordan", "martin jaggi"], "accepted": false, "id": "1611.02189"}
