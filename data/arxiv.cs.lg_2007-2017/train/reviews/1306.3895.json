{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2013", "title": "On-line PCA with Optimal Regrets", "abstract": "We carefully investigate the on-line version of PCA, where in each trial a learning algorithm plays a k-dimensional subspace, and suffers the compression loss on the next instance when projected into the chosen subspace. In this setting, we analyze two popular on-line algorithms, Gradient Descent (GD) and Exponentiated Gradient (EG). We show that both algorithms are essentially optimal in the worst-case. This comes as a surprise, since EG is known to perform sub-optimally when the instances are sparse. This different behavior of EG for PCA is mainly related to the non-negativity of the loss in this case, which makes the PCA setting qualitatively different from other settings studied in the literature. Furthermore, we show that when considering regret bounds as function of a loss budget, EG remains optimal and strictly outperforms GD. Next, we study the extension of the PCA setting, in which the Nature is allowed to play with dense instances, which are positive matrices with bounded largest eigenvalue. Again we can show that EG is optimal and strictly better than GD in this setting.", "histories": [["v1", "Mon, 17 Jun 2013 15:29:00 GMT  (47kb)", "https://arxiv.org/abs/1306.3895v1", null], ["v2", "Fri, 9 May 2014 05:28:39 GMT  (47kb)", "http://arxiv.org/abs/1306.3895v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jiazhong nie", "wojciech kotlowski", "manfred k warmuth"], "accepted": false, "id": "1306.3895"}
