{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "SafetyNet: Detecting and Rejecting Adversarial Examples Robustly", "abstract": "We describe a method to produce a network where current methods such as DeepFool have great difficulty producing adversarial samples. Our construction suggests some insights into how deep networks work. We provide a reasonable analyses that our construction is difficult to defeat, and show experimentally that our method is hard to defeat using several standard networks and datasets. We use our method to produce a system that can reliably detect whether an image is a picture of a real scene or not. Our system applies to images captured with depth maps (RGBD images) and checks if a pair of image and depth map is consistent. It relies on the relative difficulty of producing naturalistic depth maps for images in post processing. We demonstrate that our system is robust to adversarial examples built from currently known attacking approaches.", "histories": [["v1", "Sat, 1 Apr 2017 02:12:40 GMT  (3783kb,D)", "http://arxiv.org/abs/1704.00103v1", null], ["v2", "Tue, 15 Aug 2017 05:59:38 GMT  (3740kb,D)", "http://arxiv.org/abs/1704.00103v2", "Accepted to ICCV 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jiajun lu", "theerasit issaranon", "david forsyth"], "accepted": false, "id": "1704.00103"}
