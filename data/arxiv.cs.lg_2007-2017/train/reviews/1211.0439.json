{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2012", "title": "Learning curves for multi-task Gaussian process regression", "abstract": "We study the average case performance of multi-task Gaussian process (GP) regression as captured in the learning curve, i.e. the average Bayes error for a chosen task versus the total number of examples $n$ for all tasks. For GP covariances that are the product of an input-dependent covariance function and a free-form inter-task covariance matrix, we show that accurate approximations for the learning curve can be obtained for an arbitrary number of tasks $T$. We use these to study the asymptotic learning behaviour for large $n$. Surprisingly, multi-task learning can be asymptotically essentially useless, in the sense that examples from other tasks help only when the degree of inter-task correlation, $\\rho$, is near its maximal value $\\rho=1$. This effect is most extreme for learning of smooth target functions as described by e.g. squared exponential kernels. We also demonstrate that when learning many tasks, the learning curves separate into an initial phase, where the Bayes error on each task is reduced down to a plateau value by \"collective learning\" even though most tasks have not seen examples, and a final decay that occurs once the number of examples is proportional to the number of tasks.", "histories": [["v1", "Fri, 2 Nov 2012 12:46:24 GMT  (137kb,D)", "http://arxiv.org/abs/1211.0439v1", "9 pages, to appear in Advances in Neural Information Processing Systems 25"]], "COMMENTS": "9 pages, to appear in Advances in Neural Information Processing Systems 25", "reviews": [], "SUBJECTS": "cs.LG cond-mat.dis-nn stat.ML", "authors": ["peter sollich", "simon r f ashton"], "accepted": true, "id": "1211.0439"}
