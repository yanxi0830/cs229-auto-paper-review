{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2015", "title": "No penalty no tears: Least squares in high-dimensional linear models", "abstract": "Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms.", "histories": [["v1", "Sun, 7 Jun 2015 05:45:24 GMT  (2975kb,D)", "http://arxiv.org/abs/1506.02222v1", "Correct several notation errors"], ["v2", "Wed, 10 Jun 2015 03:31:06 GMT  (2975kb,D)", "http://arxiv.org/abs/1506.02222v2", "Correct typos;"], ["v3", "Fri, 9 Oct 2015 21:30:39 GMT  (2979kb,D)", "http://arxiv.org/abs/1506.02222v3", "29 pages, 5 figures, 4 tables"], ["v4", "Mon, 23 Nov 2015 09:21:37 GMT  (2979kb,D)", "http://arxiv.org/abs/1506.02222v4", "corrected citation format"], ["v5", "Thu, 16 Jun 2016 07:13:40 GMT  (2984kb,D)", "http://arxiv.org/abs/1506.02222v5", "Added results for non-sparse models; Added results for elliptical distribution; Added simulations for adaptive lasso"]], "COMMENTS": "Correct several notation errors", "reviews": [], "SUBJECTS": "stat.ME cs.LG math.ST stat.ML stat.TH", "authors": ["xiangyu wang", "david b dunson", "chenlei leng"], "accepted": true, "id": "1506.02222"}
