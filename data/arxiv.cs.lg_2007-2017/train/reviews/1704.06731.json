{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2017", "title": "Batch-Expansion Training: An Efficient Optimization Paradigm for Machine Learning", "abstract": "We propose Batch-Expansion Training (BET), a framework for running a batch optimizer on a gradually expanding dataset. As opposed to stochastic approaches, batches do not need to be resampled i.i.d. at every iteration, thus making BET more resource efficient in a distributed setting, and when disk-access is constrained. Moreover, BET can be easily paired with most batch optimizers, does not require any parameter-tuning, and compares favorably to existing stochastic and batch methods. We show that when the batch size grows exponentially with the number of outer iterations, BET achieves optimal $\\tilde{O}(1/\\epsilon)$ data-access convergence rate for strongly convex objectives.", "histories": [["v1", "Sat, 22 Apr 2017 01:26:11 GMT  (169kb)", "http://arxiv.org/abs/1704.06731v1", null], ["v2", "Sun, 15 Oct 2017 22:19:28 GMT  (119kb)", "http://arxiv.org/abs/1704.06731v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michal derezinski", "dhruv mahajan", "s sathiya keerthi", "s v n vishwanathan", "markus weimer"], "accepted": false, "id": "1704.06731"}
