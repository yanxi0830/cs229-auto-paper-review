{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Multi-Scale Dense Convolutional Networks for Efficient Prediction", "abstract": "This paper studies convolutional networks that require limited computational resources at test time. We develop a new network architecture that performs on par with state-of-the-art convolutional networks, whilst facilitating prediction in two settings: (1) an anytime-prediction setting in which the network's prediction for one example is progressively updated, facilitating the output of a prediction at any time; and (2) a batch computational budget setting in which a fixed amount of computation is available to classify a set of examples that can be spent unevenly across 'easier' and 'harder' examples. Our network architecture uses multi-scale convolutions and progressively growing feature representations, which allows for the training of multiple classifiers at intermediate layers of the network. Experiments on three image-classification datasets demonstrate the efficacy of our architecture, in particular, when measured in terms of classification accuracy as a function of the amount of compute available.", "histories": [["v1", "Wed, 29 Mar 2017 00:19:20 GMT  (4488kb,D)", "http://arxiv.org/abs/1703.09844v1", null], ["v2", "Tue, 6 Jun 2017 14:17:22 GMT  (3555kb,D)", "http://arxiv.org/abs/1703.09844v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gao huang", "danlu chen", "tianhong li", "felix wu", "laurens van der maaten", "kilian q weinberger"], "accepted": false, "id": "1703.09844"}
