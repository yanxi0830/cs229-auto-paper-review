{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2015", "title": "Train faster, generalize better: Stability of stochastic gradient descent", "abstract": "We show that any model trained by a stochastic gradient method with few iterations has vanishing generalization error. We prove this by showing the method is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. Our results apply to both convex and non-convex optimization under standard Lipschitz and smoothness assumptions.", "histories": [["v1", "Thu, 3 Sep 2015 19:53:40 GMT  (31kb)", "http://arxiv.org/abs/1509.01240v1", null], ["v2", "Sun, 7 Feb 2016 17:06:58 GMT  (444kb,D)", "http://arxiv.org/abs/1509.01240v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["moritz hardt", "ben recht", "yoram singer"], "accepted": true, "id": "1509.01240"}
