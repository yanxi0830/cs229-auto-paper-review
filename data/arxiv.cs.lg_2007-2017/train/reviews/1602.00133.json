{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2016", "title": "SCOPE: Scalable Composite Optimization for Learning on Spark", "abstract": "Many machine learning models, such as logistic regression~(LR) and support vector machine~(SVM), can be formulated as composite optimization problems. Recently, many distributed stochastic optimization~(DSO) methods have been proposed to solve the large-scale composite optimization problems, which have shown better performance than traditional batch methods. However, most of these DSO methods are not scalable enough. In this paper, we propose a novel DSO method, called \\underline{s}calable \\underline{c}omposite \\underline{op}timization for l\\underline{e}arning~({SCOPE}), and implement it on the fault-tolerant distributed platform \\mbox{Spark}. SCOPE is both computation-efficient and communication-efficient. Theoretical analysis shows that SCOPE is convergent with linear convergence rate when the objective function is convex. Furthermore, empirical results on real datasets show that SCOPE can outperform other state-of-the-art distributed learning methods on Spark, including both batch learning methods and DSO methods.", "histories": [["v1", "Sat, 30 Jan 2016 16:11:53 GMT  (574kb,D)", "https://arxiv.org/abs/1602.00133v1", null], ["v2", "Sun, 7 Feb 2016 07:07:56 GMT  (597kb,D)", "http://arxiv.org/abs/1602.00133v2", null], ["v3", "Wed, 1 Jun 2016 07:50:39 GMT  (366kb,D)", "http://arxiv.org/abs/1602.00133v3", null], ["v4", "Thu, 2 Jun 2016 07:01:25 GMT  (366kb,D)", "http://arxiv.org/abs/1602.00133v4", null], ["v5", "Sun, 11 Dec 2016 16:10:37 GMT  (503kb,D)", "http://arxiv.org/abs/1602.00133v5", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shen-yi zhao", "ru xiang", "ying-hao shi", "peng gao", "wu-jun li"], "accepted": true, "id": "1602.00133"}
