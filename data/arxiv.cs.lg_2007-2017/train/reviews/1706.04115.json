{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Zero-Shot Relation Extraction via Reading Comprehension", "abstract": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.", "histories": [["v1", "Tue, 13 Jun 2017 15:17:42 GMT  (180kb,D)", "http://arxiv.org/abs/1706.04115v1", "CoNLL 2017"]], "COMMENTS": "CoNLL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["omer levy", "minjoon seo", "eunsol choi", "luke zettlemoyer"], "accepted": false, "id": "1706.04115"}
