{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2014", "title": "Communication-Efficient Distributed Dual Coordinate Ascent", "abstract": "Communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning. In this paper, we propose a communication-efficient framework, CoCoA, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication. We provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in Spark. In our experiments, we find that as compared to state-of-the-art mini-batch versions of SGD and SDCA algorithms, CoCoA converges to the same .001-accurate solution quality on average 25x as quickly.", "histories": [["v1", "Thu, 4 Sep 2014 14:59:35 GMT  (552kb)", "http://arxiv.org/abs/1409.1458v1", null], ["v2", "Mon, 29 Sep 2014 16:07:32 GMT  (187kb)", "http://arxiv.org/abs/1409.1458v2", "NIPS 2014 version, including proofs. Published in Advances in Neural Information Processing Systems 27 (NIPS 2014)"]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["martin jaggi", "virginia smith", "martin tak\u00e1c", "jonathan terhorst", "sanjay krishnan", "thomas hofmann", "michael i jordan"], "accepted": true, "id": "1409.1458"}
