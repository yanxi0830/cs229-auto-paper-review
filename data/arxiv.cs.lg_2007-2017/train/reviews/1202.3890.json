{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2012", "title": "PAC Bounds for Discounted MDPs", "abstract": "We study upper and lower bounds on the sample-complexity of learning near-optimal behaviour in finite-state discounted Markov Decision Processes (MDPs). For the upper bound we make the assumption that each action leads to at most two possible next-states and prove a new bound for a UCRL-style algorithm on the number of time-steps when it is not Probably Approximately Correct (PAC). The new lower bound strengthens previous work by being both more general (it applies to all policies) and tighter. The upper and lower bounds match up to logarithmic factors.", "histories": [["v1", "Fri, 17 Feb 2012 11:59:55 GMT  (23kb)", "http://arxiv.org/abs/1202.3890v1", "25 LaTeX pages"]], "COMMENTS": "25 LaTeX pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tor lattimore", "marcus hutter"], "accepted": false, "id": "1202.3890"}
