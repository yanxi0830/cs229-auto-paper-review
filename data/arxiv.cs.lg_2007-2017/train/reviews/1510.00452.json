{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2015", "title": "Optimal Binary Classifier Aggregation for General Losses", "abstract": "We develop a worst-case analysis of aggregation of binary classifier ensembles in a transductive setting, for a broad class of losses including but not limited to all convex surrogates. The result is a family of parameter-free ensemble aggregation algorithms, which are as efficient as linear learning and prediction for convex risk minimization but work without any relaxations whatsoever on many nonconvex losses like the 0-1 loss. The prediction algorithms take a familiar form, applying \"link functions\" to a generalized notion of ensemble margin, but without the assumptions typically made in margin-based learning - all this structure follows from a minimax interpretation of loss minimization.", "histories": [["v1", "Thu, 1 Oct 2015 23:58:46 GMT  (64kb,D)", "http://arxiv.org/abs/1510.00452v1", null], ["v2", "Mon, 5 Oct 2015 06:05:15 GMT  (62kb,D)", "http://arxiv.org/abs/1510.00452v2", null], ["v3", "Sat, 5 Dec 2015 20:28:54 GMT  (63kb,D)", "http://arxiv.org/abs/1510.00452v3", "NIPS 2015, \"Learning from Easy Data\" Workshop"], ["v4", "Fri, 26 Feb 2016 02:04:48 GMT  (65kb,D)", "http://arxiv.org/abs/1510.00452v4", "NIPS 2015, \"Learning Faster from Easy Data II\" Workshop"], ["v5", "Mon, 7 Nov 2016 10:28:36 GMT  (71kb,D)", "http://arxiv.org/abs/1510.00452v5", "NIPS 2016"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akshay balsubramani", "yoav freund"], "accepted": true, "id": "1510.00452"}
