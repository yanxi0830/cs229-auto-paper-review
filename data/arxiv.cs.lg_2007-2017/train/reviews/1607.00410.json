{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Domain Adaptation for Neural Networks by Parameter Augmentation", "abstract": "We propose a simple domain adaptation method for neural networks in a supervised setting. Supervised domain adaptation is a way of improving the generalization performance on the target domain by using the source domain dataset, assuming that both of the datasets are labeled. Recently, recurrent neural networks have been shown to be successful on a variety of NLP tasks such as caption generation; however, the existing domain adaptation techniques are limited to (1) tune the model parameters by the target dataset after the training by the source dataset, or (2) design the network to have dual output, one for the source domain and the other for the target domain. Reformulating the idea of the domain adaptation technique proposed by Daume (2007), we propose a simple domain adaptation method, which can be applied to neural networks trained with a cross-entropy loss. On captioning datasets, we show performance improvements over other domain adaptation methods.", "histories": [["v1", "Fri, 1 Jul 2016 21:24:21 GMT  (282kb,D)", "http://arxiv.org/abs/1607.00410v1", "9 page. To appear in the first ACL Workshop on Representation Learning for NLP"]], "COMMENTS": "9 page. To appear in the first ACL Workshop on Representation Learning for NLP", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["yusuke watanabe", "kazuma hashimoto", "yoshimasa tsuruoka"], "accepted": false, "id": "1607.00410"}
