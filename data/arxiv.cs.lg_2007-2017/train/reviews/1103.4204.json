{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2011", "title": "Parallel Online Learning", "abstract": "In this work we study parallelization of online learning, a core primitive in machine learning. In a parallel environment all known approaches for parallel online learning lead to delayed updates, where the model is updated using out-of-date information. In the worst case, or when examples are temporally correlated, delay can have a very adverse effect on the learning algorithm. Here, we analyze and present preliminary empirical results on a set of learning architectures based on a feature sharding approach that present various tradeoffs between delay, degree of parallelism, representation power and empirical performance.", "histories": [["v1", "Tue, 22 Mar 2011 04:54:35 GMT  (97kb,D)", "http://arxiv.org/abs/1103.4204v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel hsu", "nikos karampatziakis", "john langford", "alex smola"], "accepted": false, "id": "1103.4204"}
