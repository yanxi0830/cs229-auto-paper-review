{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "Gradient Estimation Using Stochastic Computation Graphs", "abstract": "In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.", "histories": [["v1", "Wed, 17 Jun 2015 09:32:31 GMT  (277kb,D)", "http://arxiv.org/abs/1506.05254v1", null], ["v2", "Fri, 13 Nov 2015 03:19:18 GMT  (277kb,D)", "http://arxiv.org/abs/1506.05254v2", null], ["v3", "Tue, 5 Jan 2016 19:56:22 GMT  (277kb,D)", "http://arxiv.org/abs/1506.05254v3", "Advances in Neural Information Processing Systems 28 (NIPS 2015)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["john schulman", "nicolas heess", "theophane weber", "pieter abbeel"], "accepted": true, "id": "1506.05254"}
