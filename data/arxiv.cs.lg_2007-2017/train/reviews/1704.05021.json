{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Sparse Communication for Distributed Gradient Descent", "abstract": "We make distributed stochastic gradient descent faster by exchanging 99% sparse updates instead of dense updates. In data-parallel training, nodes pull updated values of the parameters from a sharded server, compute gradients, push their gradients to the server, and repeat. These push and pull updates strain the network. However, most updates are near zero, so we map the 99% smallest updates (by absolute value) to zero then exchange sparse matrices. Even simple coordinate and value encoding achieves 50x reduction in bandwidth. Our experiment with a neural machine translation on 4 GPUs achieved a 22% speed boost without impacting BLEU score.", "histories": [["v1", "Mon, 17 Apr 2017 16:32:02 GMT  (208kb,D)", "https://arxiv.org/abs/1704.05021v1", "Submitted to EMNLP 2017"], ["v2", "Mon, 24 Jul 2017 21:47:51 GMT  (275kb,D)", "http://arxiv.org/abs/1704.05021v2", "EMNLP 2017"]], "COMMENTS": "Submitted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.DC cs.LG", "authors": ["alham fikri aji", "kenneth heafield"], "accepted": true, "id": "1704.05021"}
