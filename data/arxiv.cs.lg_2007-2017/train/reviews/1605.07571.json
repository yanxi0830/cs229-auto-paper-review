{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Sequential Neural Models with Stochastic Layers", "abstract": "How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.", "histories": [["v1", "Tue, 24 May 2016 18:23:58 GMT  (341kb,D)", "http://arxiv.org/abs/1605.07571v1", null], ["v2", "Sun, 13 Nov 2016 18:04:41 GMT  (181kb,D)", "http://arxiv.org/abs/1605.07571v2", "NIPS 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["marco fraccaro", "s\u00f8ren kaae s\u00f8nderby", "ulrich paquet", "ole winther"], "accepted": true, "id": "1605.07571"}
