{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2014", "title": "Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets", "abstract": "Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations (CP vs DNCP) of the latent variables. We derive rules for deciding when such parameterizations are beneficial for gradient-based inference in terms of decreased posterior correlations, and show that in the DNCP, a Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are validated in experiments.", "histories": [["v1", "Mon, 3 Feb 2014 19:39:20 GMT  (2904kb,D)", "https://arxiv.org/abs/1402.0480v1", null], ["v2", "Mon, 3 Mar 2014 13:56:26 GMT  (3932kb,D)", "http://arxiv.org/abs/1402.0480v2", null], ["v3", "Tue, 13 May 2014 11:17:41 GMT  (5926kb,D)", "http://arxiv.org/abs/1402.0480v3", null], ["v4", "Mon, 16 Jun 2014 09:04:26 GMT  (6071kb,D)", "http://arxiv.org/abs/1402.0480v4", null], ["v5", "Thu, 22 Jan 2015 11:05:53 GMT  (6530kb,D)", "http://arxiv.org/abs/1402.0480v5", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["diederik p kingma", "max welling"], "accepted": true, "id": "1402.0480"}
