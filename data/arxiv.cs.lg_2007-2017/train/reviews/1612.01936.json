{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2016", "title": "A Probabilistic Framework for Deep Learning", "abstract": "We develop a probabilistic framework for deep learning based on the Deep Rendering Mixture Model (DRMM), a new generative probabilistic model that explicitly capture variations in data due to latent task nuisance variables. We demonstrate that max-sum inference in the DRMM yields an algorithm that exactly reproduces the operations in deep convolutional neural networks (DCNs), providing a first principles derivation. Our framework provides new insights into the successes and shortcomings of DCNs as well as a principled route to their improvement. DRMM training via the Expectation-Maximization (EM) algorithm is a powerful alternative to DCN back-propagation, and initial training results are promising. Classification based on the DRMM and other variants outperforms DCNs in supervised digit classification, training 2-3x faster while achieving similar accuracy. Moreover, the DRMM is applicable to semi-supervised and unsupervised learning tasks, achieving results that are state-of-the-art in several categories on the MNIST benchmark and comparable to state of the art on the CIFAR10 benchmark.", "histories": [["v1", "Tue, 6 Dec 2016 18:15:40 GMT  (5731kb,D)", "http://arxiv.org/abs/1612.01936v1", "arXiv admin note: substantial text overlap witharXiv:1504.00641"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1504.00641", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["ankit b patel", "minh tan nguyen", "richard g baraniuk"], "accepted": true, "id": "1612.01936"}
