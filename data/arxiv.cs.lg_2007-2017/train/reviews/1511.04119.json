{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Action Recognition using Visual Attention", "abstract": "We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.", "histories": [["v1", "Thu, 12 Nov 2015 23:06:42 GMT  (5992kb,D)", "http://arxiv.org/abs/1511.04119v1", null], ["v2", "Wed, 6 Jan 2016 20:46:47 GMT  (5994kb,D)", "http://arxiv.org/abs/1511.04119v2", null], ["v3", "Sun, 14 Feb 2016 17:20:19 GMT  (5993kb,D)", "http://arxiv.org/abs/1511.04119v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["shikhar sharma", "ryan kiros", "ruslan salakhutdinov"], "accepted": false, "id": "1511.04119"}
