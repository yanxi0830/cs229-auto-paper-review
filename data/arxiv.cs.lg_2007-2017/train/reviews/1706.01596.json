{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Sample-Efficient Learning of Mixtures", "abstract": "We consider PAC learning of probability distributions (a.k.a. density estimation), where we are given an i.i.d. sample generated from an unknown target distribution, and want to output a distribution that is close to the target in total variation distance. Let $\\mathcal F$ be an arbitrary class of probability distributions, and let $\\mathcal{F}^k$ denote the class of $k$-mixtures of elements of $\\mathcal F$. Assuming the existence of a method for learning $\\mathcal F$ with sample complexity $m_{\\mathcal{F}}(\\varepsilon)$ in the realizable setting, we provide a method for learning $\\mathcal F^k$ with sample complexity $O({k\\log k \\cdot m_{\\mathcal F}(\\varepsilon) }/{\\varepsilon^{2}})$ in the agnostic setting. Our mixture learning algorithm has the property that, if the $\\mathcal F$-learner is proper, then the $\\mathcal F^k$-learner is proper as well.", "histories": [["v1", "Tue, 6 Jun 2017 03:47:28 GMT  (20kb)", "http://arxiv.org/abs/1706.01596v1", "13 pages"], ["v2", "Sun, 17 Sep 2017 17:12:36 GMT  (17kb)", "http://arxiv.org/abs/1706.01596v2", "a bug from previous version is fixed, applications to mixtures of log-concave distributions are added. 18 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hassan ashtiani", "shai ben-david", "abbas mehrabian"], "accepted": false, "id": "1706.01596"}
