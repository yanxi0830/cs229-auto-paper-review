{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2016", "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data-center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.", "histories": [["v1", "Wed, 17 Feb 2016 23:40:56 GMT  (903kb,D)", "http://arxiv.org/abs/1602.05629v1", null], ["v2", "Fri, 21 Oct 2016 22:39:11 GMT  (955kb,D)", "http://arxiv.org/abs/1602.05629v2", null], ["v3", "Tue, 28 Feb 2017 21:03:49 GMT  (1075kb,D)", "http://arxiv.org/abs/1602.05629v3", "This version updates the large-scale LSTM experiments, along with other minor changes. In earlier versions, an inconsistency in our implementation of FedSGD caused us to report much lower learning rates for the large-scale LSTM. We reran these experiments, and also found that fewer local epochs offers better performance, leading to slightly better results for FedAvg than previously reported"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["h brendan mcmahan", "eider moore", "daniel ramage", "seth hampson", "blaise ag\\\"uera y arcas"], "accepted": false, "id": "1602.05629"}
