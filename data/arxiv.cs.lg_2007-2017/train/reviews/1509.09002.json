{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2015", "title": "Convergence of Stochastic Gradient Descent for PCA", "abstract": "We consider the problem of principal component analysis (PCA) in a streaming stochastic setting, where our goal is to find a direction of approximate maximal variance, based on a stream of i.i.d. data points in $\\mathbb{R}^d$. A simple and computationally cheap algorithm for this is stochastic gradient descent (SGD), which incrementally updates its estimate based on each new data point. However, due to the non-convex nature of the problem, analyzing its performance has been a challenge. In particular, existing guarantees rely on a non-trivial eigengap assumption on the covariance matrix, which is intuitively unnecessary. In this note, we provide (to the best of our knowledge) the first eigengap-free convergence guarantees for SGD in the context of PCA. This also partially resolves an open problem posed in [Hardt and Price, 2014].", "histories": [["v1", "Wed, 30 Sep 2015 03:02:59 GMT  (17kb)", "https://arxiv.org/abs/1509.09002v1", "18 pages"], ["v2", "Mon, 4 Jan 2016 08:25:56 GMT  (21kb)", "http://arxiv.org/abs/1509.09002v2", "Added analysis of the positive eigengap scenario, with new results; Some minor corrections"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["ohad shamir"], "accepted": true, "id": "1509.09002"}
