{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2015", "title": "Rank Subspace Learning for Compact Hash Codes", "abstract": "The era of Big Data has spawned unprecedented interests in developing hashing algorithms for efficient storage and fast nearest neighbor search. Most existing work learn hash functions that are numeric quantizations of feature values in projected feature space. In this work, we propose a novel hash learning framework that encodes feature's rank orders instead of numeric values in a number of optimal low-dimensional ranking subspaces. We formulate the ranking subspace learning problem as the optimization of a piece-wise linear convex-concave function and present two versions of our algorithm: one with independent optimization of each hash bit and the other exploiting a sequential learning framework. Our work is a generalization of the Winner-Take-All (WTA) hash family and naturally enjoys all the numeric stability benefits of rank correlation measures while being optimized to achieve high precision at very short code length. We compare with several state-of-the-art hashing algorithms in both supervised and unsupervised domain, showing superior performance in a number of data sets.", "histories": [["v1", "Thu, 19 Mar 2015 21:34:33 GMT  (685kb,D)", "http://arxiv.org/abs/1503.05951v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["kai li", "guojun qi", "jun ye", "kien a hua"], "accepted": false, "id": "1503.05951"}
