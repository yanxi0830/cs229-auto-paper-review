{"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Hybrid Reward Architecture for Reinforcement Learning", "abstract": "One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the overall value function is much smoother and can be easier approximated by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.", "histories": [["v1", "Tue, 13 Jun 2017 18:05:48 GMT  (1018kb,D)", "http://arxiv.org/abs/1706.04208v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["harm van seijen", "mehdi fatemi", "joshua romoff", "romain laroche", "tavian barnes", "jeffrey tsang"], "accepted": true, "id": "1706.04208"}
