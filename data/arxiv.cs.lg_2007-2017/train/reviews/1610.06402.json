{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "A Growing Long-term Episodic & Semantic Memory", "abstract": "The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains.", "histories": [["v1", "Thu, 20 Oct 2016 13:29:56 GMT  (216kb,D)", "http://arxiv.org/abs/1610.06402v1", "Submission to NIPS workshop on Continual Learning. 4 page extended abstract plus 5 more pages of references, figures, and supplementary material"]], "COMMENTS": "Submission to NIPS workshop on Continual Learning. 4 page extended abstract plus 5 more pages of references, figures, and supplementary material", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["marc pickett", "rami al-rfou", "louis shao", "chris tar"], "accepted": false, "id": "1610.06402"}
