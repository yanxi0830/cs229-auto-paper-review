{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2017", "title": "Discovering objects and their relations from entangled scene representations", "abstract": "Our world can be succinctly and compactly described as structured scenes of objects and relations. A typical room, for example, contains salient objects such as tables, chairs and books, and these objects typically relate to each other by their underlying causes and semantics. This gives rise to correlated features, such as position, function and shape. Humans exploit knowledge of objects and their relations for learning a wide spectrum of tasks, and more generally when learning the structure underlying observed data. In this work, we introduce relation networks (RNs) - a general purpose neural network architecture for object-relation reasoning. We show that RNs are capable of learning object relations from scene description data. Furthermore, we show that RNs can act as a bottleneck that induces the factorization of objects from entangled scene description inputs, and from distributed deep representations of scene images provided by a variational autoencoder. The model can also be used in conjunction with differentiable memory mechanisms for implicit relation discovery in one-shot learning tasks. Our results suggest that relation networks are a potentially powerful architecture for solving a variety of problems that require object relation reasoning.", "histories": [["v1", "Thu, 16 Feb 2017 18:08:27 GMT  (8935kb,D)", "http://arxiv.org/abs/1702.05068v1", "ICLR Workshop 2017"]], "COMMENTS": "ICLR Workshop 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["david raposo", "adam santoro", "david barrett", "razvan pascanu", "timothy lillicrap", "peter battaglia"], "accepted": false, "id": "1702.05068"}
