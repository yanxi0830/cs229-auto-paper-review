{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Convolutional Neural Networks using Logarithmic Data Representation", "abstract": "Recent advances in convolutional neural networks have considered model complexity and hardware efficiency to enable deployment onto embedded systems and mobile devices. For example, it is now well-known that the arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in performance. However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance. In this paper we propose a new data representation that enables state-of-the-art networks to be encoded to 3 bits with negligible loss in classification performance. To perform this, we take advantage of the fact that the weights and activations in a trained network naturally have non-uniform distributions. Using non-uniform, base-2 logarithmic representation to encode weights, communicate activations, and perform dot-products enables networks to 1) achieve higher classification accuracies than fixed-point at the same resolution and 2) eliminate bulky digital multipliers. Finally, we propose an end-to-end training procedure that uses log representation at 5-bits, which achieves higher final test accuracy than linear at 5-bits.", "histories": [["v1", "Thu, 3 Mar 2016 08:51:52 GMT  (2526kb,D)", "http://arxiv.org/abs/1603.01025v1", "10 pages, 7 figures"], ["v2", "Thu, 17 Mar 2016 03:32:30 GMT  (2526kb,D)", "http://arxiv.org/abs/1603.01025v2", "10 pages, 7 figures"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["daisuke miyashita", "edward h lee", "boris murmann"], "accepted": false, "id": "1603.01025"}
