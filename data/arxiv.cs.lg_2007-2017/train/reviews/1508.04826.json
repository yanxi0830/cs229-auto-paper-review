{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2015", "title": "Dither is Better than Dropout for Regularising Deep Neural Networks", "abstract": "Regularisation of deep neural networks (DNN) during training is critical to performance. By far the most popular method is known as dropout. Here, cast through the prism of signal processing theory, we compare and contrast the regularisation effects of dropout with those of dither. We illustrate some serious inherent limitations of dropout and demonstrate that dither provides a far more effective regulariser which does not suffer from the same limitations.", "histories": [["v1", "Wed, 19 Aug 2015 23:02:37 GMT  (202kb)", "http://arxiv.org/abs/1508.04826v1", null], ["v2", "Wed, 26 Aug 2015 12:59:38 GMT  (206kb)", "http://arxiv.org/abs/1508.04826v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew j r simpson"], "accepted": false, "id": "1508.04826"}
