{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Gated-Attention Readers for Text Comprehension", "abstract": "In this paper we study the problem of answering cloze-style questions over short documents. We introduce a new attention mechanism which uses multiplicative interactions between the query embedding and intermediate states of a recurrent neural network reader. This enables the reader to build query-specific representations of tokens in the document which are further used for answer selection. Our model, the Gated-Attention Reader, outperforms all state-of-the-art models on several large-scale benchmark datasets for this task---the CNN \\&amp; Dailymail news stories and Children's Book Test. We also provide a detailed analysis of the performance of our model and several baselines over a subset of questions manually annotated with certain linguistic features. The analysis sheds light on the strengths and weaknesses of several existing models.", "histories": [["v1", "Sun, 5 Jun 2016 19:30:39 GMT  (1796kb,D)", "http://arxiv.org/abs/1606.01549v1", "Under review at EMNLP 2016"], ["v2", "Thu, 1 Dec 2016 19:27:42 GMT  (7419kb,D)", "http://arxiv.org/abs/1606.01549v2", "Under review as a conference paper at ICLR 2017"], ["v3", "Fri, 21 Apr 2017 18:50:05 GMT  (7428kb,D)", "http://arxiv.org/abs/1606.01549v3", "Accepted at ACL 2017"]], "COMMENTS": "Under review at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["bhuwan dhingra", "hanxiao liu", "zhilin yang", "william w cohen", "ruslan salakhutdinov"], "accepted": true, "id": "1606.01549"}
