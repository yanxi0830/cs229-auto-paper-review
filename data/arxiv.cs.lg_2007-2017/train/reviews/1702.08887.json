{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning", "abstract": "Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A key stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep RL relies. This paper proposes two methods that address this problem: 1) conditioning each agent's value function on a footprint that disambiguates the age of the data sampled from the replay memory and 2) using a multi-agent variant of importance sampling to naturally decay obsolete data. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.", "histories": [["v1", "Tue, 28 Feb 2017 17:56:41 GMT  (346kb,D)", "http://arxiv.org/abs/1702.08887v1", null], ["v2", "Mon, 12 Jun 2017 22:00:56 GMT  (1940kb,D)", "http://arxiv.org/abs/1702.08887v2", "Camera-ready version, International Conference of Machine Learning 2017"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.MA", "authors": ["jakob n foerster", "nantas nardelli", "gregory farquhar", "triantafyllos afouras", "philip h s torr", "pushmeet kohli", "shimon whiteson"], "accepted": true, "id": "1702.08887"}
