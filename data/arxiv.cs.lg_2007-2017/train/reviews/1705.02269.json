{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Sequential Attention: A Context-Aware Alignment Function for Machine Reading", "abstract": "In this paper we propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in a way that takes into account not just how well that word matches a query, but how well surrounding words match. We evaluate this approach on the task of reading comprehension (Who did What and CNN datasets) and show that it dramatically improves a strong baseline like the Stanford Reader. The resulting model is competitive with the state of the art.", "histories": [["v1", "Fri, 5 May 2017 15:37:11 GMT  (816kb,D)", "http://arxiv.org/abs/1705.02269v1", "4 pages"], ["v2", "Mon, 26 Jun 2017 22:25:55 GMT  (428kb,D)", "http://arxiv.org/abs/1705.02269v2", "To appear in ACL 2017 2nd Workshop on Representation Learning for NLP. Contains additional experiments in section 4 and a revised Figure 1"]], "COMMENTS": "4 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian brarda", "philip yeres", "samuel r bowman"], "accepted": false, "id": "1705.02269"}
