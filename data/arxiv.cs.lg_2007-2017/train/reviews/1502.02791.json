{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2015", "title": "Learning Transferable Features with Deep Adaptation Networks", "abstract": "Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is critical to formally reduce the domain bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded to a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn invariant features with enhanced transferability, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence demonstrates the proposed architecture significantly outperforms state-of-the-art results on standard domain adaptation benchmarks.", "histories": [["v1", "Tue, 10 Feb 2015 06:01:30 GMT  (143kb)", "https://arxiv.org/abs/1502.02791v1", null], ["v2", "Wed, 27 May 2015 05:28:35 GMT  (147kb)", "http://arxiv.org/abs/1502.02791v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mingsheng long", "yue cao", "jianmin wang 0001", "michael i jordan"], "accepted": true, "id": "1502.02791"}
