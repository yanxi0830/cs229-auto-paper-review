{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks", "abstract": "Modern convolutional networks, incorporating rectifiers and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. Nevertheless, methods from convex optimization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the first convergence guarantee applicable to modern convnets. The guarantee matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation -- a straightforward application of Taylor expansions to neural networks -- and the associated Taylor loss. Experiments on a range of optimizers, layers, and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization.", "histories": [["v1", "Mon, 7 Nov 2016 23:47:05 GMT  (1608kb,D)", "https://arxiv.org/abs/1611.02345v1", "13 pages, 6 figures"], ["v2", "Fri, 24 Feb 2017 02:26:15 GMT  (1609kb,D)", "http://arxiv.org/abs/1611.02345v2", "11 pages, 6 figures"]], "COMMENTS": "13 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["david balduzzi", "brian mcwilliams", "tony butler-yeoman"], "accepted": true, "id": "1611.02345"}
