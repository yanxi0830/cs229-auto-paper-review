{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2016", "title": "Accelerating Deep Learning with Shrinkage and Recall", "abstract": "Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance.", "histories": [["v1", "Wed, 4 May 2016 18:17:37 GMT  (276kb,D)", "https://arxiv.org/abs/1605.01369v1", null], ["v2", "Mon, 19 Sep 2016 19:27:39 GMT  (4119kb)", "http://arxiv.org/abs/1605.01369v2", "The 22nd IEEE International Conference on Parallel and Distributed Systems (ICPADS 2016)"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["shuai zheng", "abhinav vishnu", "chris ding"], "accepted": false, "id": "1605.01369"}
