{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Poisoning Attacks against Support Vector Machines", "abstract": "We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (504kb)", "http://arxiv.org/abs/1206.6389v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"], ["v2", "Fri, 20 Jul 2012 12:33:21 GMT  (184kb,D)", "http://arxiv.org/abs/1206.6389v2", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"], ["v3", "Mon, 25 Mar 2013 10:16:36 GMT  (184kb,D)", "http://arxiv.org/abs/1206.6389v3", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.CR stat.ML", "authors": ["battista biggio", "blaine nelson", "pavel laskov"], "accepted": true, "id": "1206.6389"}
