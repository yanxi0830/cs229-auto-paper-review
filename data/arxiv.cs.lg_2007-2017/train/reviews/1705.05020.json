{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2017", "title": "Discrete-Continuous Splitting for Weakly Supervised Learning", "abstract": "This paper proposes an approach for tackling an abstract formulation of weakly supervised learning, which is posed as a joint optimization problem in the continuous model parameters and discrete label variables. We devise a novel decomposition of the latter into purely discrete and continuous subproblems within the framework of the Alternating Direction Method of Multipliers (ADMM), which allows to efficiently compute a local minimum of the nonconvex objective function. Our approach preserves integrality of the discrete label variables and admits a globally convergent kernel formulation. The resulting method implicitly alternates between a discrete and a continuous variable update, however, it is inherently different from simple alternating optimization (hard EM). In numerous experiments we illustrate that our method can learn a classifier from weak and abstract combinatorial supervision thereby being superior towards hard EM.", "histories": [["v1", "Sun, 14 May 2017 19:32:50 GMT  (345kb,D)", "https://arxiv.org/abs/1705.05020v1", null], ["v2", "Mon, 19 Jun 2017 15:51:08 GMT  (190kb,D)", "http://arxiv.org/abs/1705.05020v2", null], ["v3", "Wed, 16 Aug 2017 15:18:11 GMT  (190kb,D)", "http://arxiv.org/abs/1705.05020v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["emanuel laude", "jan-hendrik lange", "frank r schmidt", "bjoern", "res", "daniel cremers"], "accepted": false, "id": "1705.05020"}
