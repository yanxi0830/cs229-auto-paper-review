{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Behavior Pattern Recognition using A New Representation Model", "abstract": "We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of agents' behavior on the basis of observation of their sequential decision behavior interacting with the environment. We model the problem faced by the agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of degrees of rationality with respect to optimal forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for clustering or classification models. Experimental studies with GridWorld, a navigation problem, and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for behavior pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for recognition problems.", "histories": [["v1", "Wed, 16 Jan 2013 09:01:47 GMT  (78kb)", "https://arxiv.org/abs/1301.3630v1", null], ["v2", "Mon, 21 Jan 2013 09:26:22 GMT  (77kb)", "http://arxiv.org/abs/1301.3630v2", null], ["v3", "Mon, 18 Feb 2013 06:06:08 GMT  (64kb)", "http://arxiv.org/abs/1301.3630v3", null], ["v4", "Wed, 20 Mar 2013 21:18:07 GMT  (69kb)", "http://arxiv.org/abs/1301.3630v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qifeng qiao", "peter a beling"], "accepted": false, "id": "1301.3630"}
