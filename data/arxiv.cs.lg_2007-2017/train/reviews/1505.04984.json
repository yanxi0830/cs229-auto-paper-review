{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2015", "title": "Risk and Regret of Hierarchical Bayesian Learners", "abstract": "Common statistical practice has shown that the full power of Bayesian methods is not realized until hierarchical priors are used, as these allow for greater \"robustness\" and the ability to \"share statistical strength.\" Yet it is an ongoing challenge to provide a learning-theoretically sound formalism of such notions that: offers practical guidance concerning when and how best to utilize hierarchical models; provides insights into what makes for a good hierarchical prior; and, when the form of the prior has been chosen, can guide the choice of hyperparameter settings. We present a set of analytical tools for understanding hierarchical priors in both the online and batch learning settings. We provide regret bounds under log-loss, which show how certain hierarchical models compare, in retrospect, to the best single model in the model class. We also show how to convert a Bayesian log-loss regret bound into a Bayesian risk bound for any bounded loss, a result which may be of independent interest. Risk and regret bounds for Student's $t$ and hierarchical Gaussian priors allow us to formalize the concepts of \"robustness\" and \"sharing statistical strength.\" Priors for feature selection are investigated as well. Our results suggest that the learning-theoretic benefits of using hierarchical priors can often come at little cost on practical problems.", "histories": [["v1", "Tue, 19 May 2015 13:12:41 GMT  (190kb,D)", "http://arxiv.org/abs/1505.04984v1", "In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)"]], "COMMENTS": "In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jonathan h huggins", "joshua b tenenbaum"], "accepted": true, "id": "1505.04984"}
