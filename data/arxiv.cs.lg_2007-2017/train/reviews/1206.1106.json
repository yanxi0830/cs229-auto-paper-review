{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2012", "title": "No more pesky learning rates", "abstract": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained through systematic search, and effectively removes the need for learning rate tuning.", "histories": [["v1", "Wed, 6 Jun 2012 02:06:57 GMT  (629kb,D)", "http://arxiv.org/abs/1206.1106v1", "Submitted to NIPS 2012"], ["v2", "Mon, 18 Feb 2013 16:09:50 GMT  (1097kb,D)", "http://arxiv.org/abs/1206.1106v2", null]], "COMMENTS": "Submitted to NIPS 2012", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["tom schaul", "sixin zhang", "yann lecun"], "accepted": true, "id": "1206.1106"}
