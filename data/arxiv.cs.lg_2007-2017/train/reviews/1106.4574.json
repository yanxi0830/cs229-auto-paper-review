{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2011", "title": "Better Mini-Batch Algorithms via Accelerated Gradient Methods", "abstract": "Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization problems. We study how such algorithms can be improved using accelerated gradient methods. We provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up and propose a novel accelerated gradient algorithm, which deals with this deficiency, enjoys a uniformly superior guarantee and works well in practice.", "histories": [["v1", "Wed, 22 Jun 2011 20:59:20 GMT  (37kb,D)", "http://arxiv.org/abs/1106.4574v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew cotter", "ohad shamir", "nati srebro", "karthik sridharan"], "accepted": true, "id": "1106.4574"}
