{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "MagNet: a Two-Pronged Defense against Adversarial Examples", "abstract": "Deep learning has shown promising results on hard perceptual problems in recent years. However, deep learning systems are found to be vulnerable to small adversarial perturbations that are nearly imperceptible to human. Such specially crafted perturbations cause deep learning systems to output incorrect decisions, with potentially disastrous consequences. These vulnerabilities hinder the deployment of deep learning systems where safety or security is important. Attempts to secure deep learning systems either target specific attacks or have been shown to be ineffective.", "histories": [["v1", "Thu, 25 May 2017 06:49:57 GMT  (310kb,D)", "http://arxiv.org/abs/1705.09064v1", "In submission as a conference paper"], ["v2", "Mon, 11 Sep 2017 02:41:15 GMT  (2417kb,D)", "http://arxiv.org/abs/1705.09064v2", "Accepted at the ACM Conference on Computer and Communications Security (CCS), 2017"]], "COMMENTS": "In submission as a conference paper", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["dongyu meng", "hao chen"], "accepted": false, "id": "1705.09064"}
