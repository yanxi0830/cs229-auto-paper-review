{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "A Fast Incremental Gaussian Mixture Model", "abstract": "This work builds upon previous efforts in online incremental learning, namely the Incremental Gaussian Mixture Network (IGMN). The IGMN is capable of learning from data streams in a single-pass by improving its model after analyzing each data point and discarding it thereafter. Nevertheless, it suffers from the scalability point-of-view, due to its asymptotic time complexity of $\\operatorname{O}\\bigl(NKD^3\\bigr)$ for $N$ data points, $K$ Gaussian components and $D$ dimensions, rendering it inadequate for high-dimensional data. In this paper, we manage to reduce this complexity to $\\operatorname{O}\\bigl(NKD^2\\bigr)$ by deriving formulas for working directly with precision matrices instead of covariance matrices. The final result is a much faster and scalable algorithm which can be applied to high dimensional tasks. This is confirmed by applying the modified algorithm to high-dimensional classification datasets.", "histories": [["v1", "Sun, 14 Jun 2015 17:02:49 GMT  (113kb)", "http://arxiv.org/abs/1506.04422v1", "10 pages, no figures, draft submission to Plos One"], ["v2", "Thu, 18 Jun 2015 17:04:01 GMT  (114kb)", "http://arxiv.org/abs/1506.04422v2", "10 pages, no figures, draft submission to Plos One"]], "COMMENTS": "10 pages, no figures, draft submission to Plos One", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rafael pinto", "paulo engel"], "accepted": false, "id": "1506.04422"}
