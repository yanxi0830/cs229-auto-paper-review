{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2013", "title": "Distributed Policy Evaluation Under Multiple Behavior Strategies", "abstract": "We apply diffusion strategies to develop fully-distributed cooperative reinforcement learning solutions, in which agents in a network communicate only with their immediate neighbors to improve predictions about their environment. The algorithm can also be applied to off-policy sampling, meaning that the agents can learn to predict the response to a behavior different from the actual policies they are following. The proposed distributed strategy is efficient, with linear complexity in both computation time and memory footprint. We provide a mean-square-error performance analysis and establish convergence under constant step-size updates, which endow the network with continuous learning capabilities. The results show a clear gain from cooperation: when the individual agents can estimate the solution, cooperation increases stability and reduces bias and variance of the prediction error; but, more importantly, the network is able to approach the optimal solution even when none of the individual agents could (e.g., when the individual behavior policies restrict each agent to sample a small portion of the state space).", "histories": [["v1", "Mon, 30 Dec 2013 00:16:34 GMT  (1232kb)", "https://arxiv.org/abs/1312.7606v1", null], ["v2", "Wed, 5 Nov 2014 19:50:03 GMT  (2971kb)", "http://arxiv.org/abs/1312.7606v2", "36 pages, 4 figures, accepted for publication on IEEE Transactions on Automatic Control"]], "reviews": [], "SUBJECTS": "cs.MA cs.AI cs.DC cs.LG", "authors": ["sergio valcarcel macua", "jianshu chen", "santiago zazo", "ali h sayed"], "accepted": false, "id": "1312.7606"}
