{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2015", "title": "Convergent Learning: Do different neural networks learn the same representations?", "abstract": "Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of learned parameters, but valuable because it increases our ability to understand current models and training algorithms and thus create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes are a mix between a local code and slightly, but not fully, distributed codes across multiple units.", "histories": [["v1", "Tue, 24 Nov 2015 02:31:46 GMT  (9071kb,D)", "http://arxiv.org/abs/1511.07543v1", "A preliminary version of this work will be presented at the NIPS 2015 Feature Extraction workshop"], ["v2", "Fri, 8 Jan 2016 02:33:05 GMT  (9072kb,D)", "http://arxiv.org/abs/1511.07543v2", "A preliminary version of this work was presented at the NIPS 2015 Feature Extraction workshop"], ["v3", "Sun, 28 Feb 2016 22:04:54 GMT  (9072kb,D)", "http://arxiv.org/abs/1511.07543v3", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "A preliminary version of this work will be presented at the NIPS 2015 Feature Extraction workshop", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["yixuan li", "jason yosinski", "jeff clune", "hod lipson", "john hopcroft"], "accepted": true, "id": "1511.07543"}
