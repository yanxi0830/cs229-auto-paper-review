{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2016", "title": "A Kronecker-factored approximate Fisher matrix for convolution layers", "abstract": "Second-order optimization methods such as natural gradient descent have the potential to speed up training of neural networks by correcting for the curvature of the loss function. Unfortunately, the exact natural gradient is impractical to compute for large models, and most approximations either require an expensive iterative procedure or make crude approximations to the curvature. We present Kronecker Factors for Convolution (KFC), a tractable approximation to the Fisher matrix for convolutional networks based on a structured probabilistic model for the distribution over backpropagated derivatives. Similarly to the recently proposed Kronecker-Factored Approximate Curvature (K-FAC), each block of the approximate Fisher matrix decomposes as the Kronecker product of small matrices, allowing for efficient inversion. KFC captures important curvature information while still yielding comparably efficient updates to stochastic gradient descent (SGD). We show that the updates are invariant to commonly used reparameterizations, such as centering of the activations. In our experiments, approximate natural gradient descent with KFC was able to train convolutional networks several times faster than carefully tuned SGD. Furthermore, it was able to train the networks in 10-20 times fewer iterations than SGD, suggesting its potential applicability in a distributed setting.", "histories": [["v1", "Wed, 3 Feb 2016 18:45:07 GMT  (1802kb,D)", "http://arxiv.org/abs/1602.01407v1", null], ["v2", "Mon, 23 May 2016 22:44:56 GMT  (1802kb,D)", "http://arxiv.org/abs/1602.01407v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["roger b grosse", "james martens"], "accepted": true, "id": "1602.01407"}
