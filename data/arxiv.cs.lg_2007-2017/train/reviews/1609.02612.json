{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2016", "title": "Generating Videos with Scene Dynamics", "abstract": "We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.", "histories": [["v1", "Thu, 8 Sep 2016 22:29:52 GMT  (1926kb,D)", "http://arxiv.org/abs/1609.02612v1", "NIPS 2016. See more atthis http URL"], ["v2", "Mon, 17 Oct 2016 03:13:10 GMT  (1927kb,D)", "http://arxiv.org/abs/1609.02612v2", "NIPS 2016. See more atthis http URL"], ["v3", "Wed, 26 Oct 2016 13:58:10 GMT  (1927kb,D)", "http://arxiv.org/abs/1609.02612v3", "NIPS 2016. See more atthis http URL"]], "COMMENTS": "NIPS 2016. See more atthis http URL", "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG", "authors": ["carl vondrick", "hamed pirsiavash", "antonio torralba 0001"], "accepted": true, "id": "1609.02612"}
