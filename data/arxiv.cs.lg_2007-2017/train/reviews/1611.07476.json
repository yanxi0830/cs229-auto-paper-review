{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Singularity of the Hessian in Deep Learning", "abstract": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges indicating the complexity of the input data.", "histories": [["v1", "Tue, 22 Nov 2016 19:24:49 GMT  (4520kb,D)", "http://arxiv.org/abs/1611.07476v1", "ICLR 2017 Submission on Nov 4, 2016"], ["v2", "Thu, 5 Oct 2017 13:28:50 GMT  (2514kb,D)", "http://arxiv.org/abs/1611.07476v2", "ICLR submission, 2016 - updated to match the openreview.net version"]], "COMMENTS": "ICLR 2017 Submission on Nov 4, 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["levent sagun", "leon bottou", "yann lecun"], "accepted": false, "id": "1611.07476"}
