{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2015", "title": "Regularization and Kernelization of the Maximin Correlation Approach", "abstract": "Robust classification becomes challenging when classes contain multiple subclasses. Examples include multi-font optical character recognition and automated protein function prediction. In correlation-based nearest-neighbor classification, the maximin correlation approach (MCA) provides the worst-case optimal solution by minimizing the maximum misclassification risk through an iterative procedure. Despite the optimality, the original MCA has drawbacks that have limited its wide applicability in practice. That is, the MCA tends to be sensitive to outliers, cannot effectively handle nonlinearities in datasets, and suffers from having high computational complexity. To address these limitations, we propose an improved solution, named regularized maximin correlation approach (R-MCA). We first reformulate MCA as a quadratically constrained linear programming (QCLP) problem, incorporate regularization by introducing slack variables into the primal problem of the QCLP, and derive the corresponding Lagrangian dual. The dual formulation enables us to apply the kernel trick to R-MCA so that it can better handle nonlinearities. Our experimental results demonstrate that the regularization and kernelization make the proposed R-MCA more robust and accurate for various classification tasks than the original MCA. Furthermore, when the data size or dimensionality grows, R-MCA runs substantially faster by solving either the primal or dual (whichever has a smaller variable dimension) of the QCLP.", "histories": [["v1", "Sat, 21 Feb 2015 14:37:44 GMT  (2286kb)", "http://arxiv.org/abs/1502.06105v1", null], ["v2", "Tue, 29 Mar 2016 04:42:12 GMT  (1909kb)", "http://arxiv.org/abs/1502.06105v2", "Submitted to IEEE Access"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["taehoon lee", "taesup moon", "seung jean kim", "sungroh yoon"], "accepted": false, "id": "1502.06105"}
