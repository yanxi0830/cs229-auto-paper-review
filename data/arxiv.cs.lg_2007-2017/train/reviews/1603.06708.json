{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "A Self-Paced Regularization Framework for Multi-Label Learning", "abstract": "In this paper, we propose a novel multi-label learning framework, called Multi-Label Self-Paced Learning (MLSPL), in an attempt to incorporate the self-paced learning strategy into multi-label learning regime. In light of the benefits of adopting the easy-to-hard strategy proposed by self-paced learning, the devised MLSPL aims to learn multiple labels jointly by gradually including label learning tasks and instances into model training from the easy to the hard. We first introduce a self-paced function as a regularizer in the multi-label learning formulation, so as to simultaneously rank priorities of the label learning tasks and the instances in each learning iteration. Considering that different multi-label learning scenarios often need different self-paced schemes during optimization, we thus propose a general way to find the desired self-paced functions. Experimental results on three benchmark datasets suggest the state-of-the-art performance of our approach.", "histories": [["v1", "Tue, 22 Mar 2016 09:03:40 GMT  (434kb,D)", "http://arxiv.org/abs/1603.06708v1", null], ["v2", "Wed, 6 Apr 2016 14:54:28 GMT  (434kb,D)", "http://arxiv.org/abs/1603.06708v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changsheng li", "fan wei", "junchi yan", "weishan dong", "qingshan liu", "xiaoyu zhang", "hongyuan zha"], "accepted": false, "id": "1603.06708"}
