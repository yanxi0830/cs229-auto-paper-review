{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2015", "title": "On the Quality of the Initial Basin in Overspecified Neural Networks", "abstract": "Over the past few years, artificial neural networks have seen a dramatic resurgence in popularity as a tool for solving hard learning problems in AI applications. While it is widely known that neural networks are computationally hard to train in the worst case, in practice, neural networks are trained efficiently using SGD methods and a variety of techniques which accelerate the learning process. One mechanism which has been suggested to explain this is overspecification, which is the training of a network larger than what would be needed with unbounded computational power. Empirically, despite worst-case NP-hardness results, large networks tend to achieve a smaller error over the training set.", "histories": [["v1", "Fri, 13 Nov 2015 09:35:34 GMT  (313kb,D)", "http://arxiv.org/abs/1511.04210v1", null], ["v2", "Tue, 9 Feb 2016 16:22:46 GMT  (164kb,D)", "http://arxiv.org/abs/1511.04210v2", "Significantly different version, with more general results"], ["v3", "Tue, 14 Jun 2016 05:39:27 GMT  (164kb,D)", "http://arxiv.org/abs/1511.04210v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["itay safran", "ohad shamir"], "accepted": true, "id": "1511.04210"}
