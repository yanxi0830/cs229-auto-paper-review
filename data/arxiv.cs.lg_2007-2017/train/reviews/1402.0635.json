{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Generalization and Exploration via Randomized Value Functions", "abstract": "We consider the problem of reinforcement learning with an orientation toward contexts in which an agent must generalize from past experience and explore to reduce uncertainty. We propose an approach to exploration based on randomized value functions and an algorithm -- randomized least-squares value iteration (RLSVI) -- that embodies this approach. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient and present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Our experiments focus on learning over episodes of a finite-horizon Markov decision process and use a version of RLSVI designed for that task, but we also propose a version of RLSVI that addresses continual learning in an infinite-horizon discounted Markov decision process.", "histories": [["v1", "Tue, 4 Feb 2014 06:41:59 GMT  (47kb,D)", "https://arxiv.org/abs/1402.0635v1", "arXiv admin note: text overlap witharXiv:1307.4847"], ["v2", "Tue, 7 Jul 2015 23:11:02 GMT  (176kb,D)", "http://arxiv.org/abs/1402.0635v2", "arXiv admin note: text overlap witharXiv:1307.4847"], ["v3", "Mon, 15 Feb 2016 10:20:11 GMT  (3057kb,D)", "http://arxiv.org/abs/1402.0635v3", "arXiv admin note: text overlap witharXiv:1307.4847"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1307.4847", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG cs.SY", "authors": ["ian osband", "benjamin van roy", "zheng wen"], "accepted": true, "id": "1402.0635"}
