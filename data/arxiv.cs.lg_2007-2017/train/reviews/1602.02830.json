{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1", "abstract": "We introduce BinaryNet, a method which trains DNNs with binary weights and activations when computing parameters' gradient. We show that it is possible to train a Multi Layer Perceptron (MLP) on MNIST and ConvNets on CIFAR-10 and SVHN with BinaryNet and achieve nearly state-of-the-art results. At run-time, BinaryNet drastically reduces memory usage and replaces most multiplications by 1-bit exclusive-not-or (XNOR) operations, which might have a big impact on both general-purpose and dedicated Deep Learning hardware. We wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST MLP 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for BinaryNet is available.", "histories": [["v1", "Tue, 9 Feb 2016 01:01:59 GMT  (71kb,D)", "http://arxiv.org/abs/1602.02830v1", "9 pages and 2 figures"], ["v2", "Mon, 29 Feb 2016 21:26:53 GMT  (94kb,D)", "http://arxiv.org/abs/1602.02830v2", "11 pages and 3 figures"], ["v3", "Thu, 17 Mar 2016 14:54:25 GMT  (94kb,D)", "http://arxiv.org/abs/1602.02830v3", "11 pages and 3 figures"]], "COMMENTS": "9 pages and 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["matthieu courbariaux", "itay hubara", "daniel soudry", "ran el-yaniv", "yoshua bengio"], "accepted": false, "id": "1602.02830"}
