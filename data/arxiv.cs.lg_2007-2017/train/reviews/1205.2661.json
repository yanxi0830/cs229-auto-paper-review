{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs", "abstract": "We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of ~O(HSpAT). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds.", "histories": [["v1", "Wed, 9 May 2012 14:47:06 GMT  (177kb)", "http://arxiv.org/abs/1205.2661v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["peter l bartlett", "ambuj tewari"], "accepted": false, "id": "1205.2661"}
