{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Latent Intention Dialogue Models", "abstract": "Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. Traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability. In this paper, we propose a Latent Intention Dialogue Model (LIDM) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. In a goal-oriented dialogue scenario, these latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning. The experimental evaluation of LIDM shows that the model out-performs published benchmarks for both corpus-based and human evaluation, demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues.", "histories": [["v1", "Mon, 29 May 2017 15:01:44 GMT  (397kb,D)", "http://arxiv.org/abs/1705.10229v1", "Accepted at ICML 2017"]], "COMMENTS": "Accepted at ICML 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE stat.ML", "authors": ["tsung-hsien wen", "yishu miao", "phil blunsom", "steve j young"], "accepted": true, "id": "1705.10229"}
