{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2014", "title": "Understanding Locally Competitive Networks", "abstract": "Recently proposed neural network activation functions such as rectified linear, maxout, and local winner-take-all have allowed for faster and more effective training of deep neural architectures on large and complex datasets. The common trait among these functions is that they implement local competition between small groups of units within a layer, so that only part of the network is activated for any given input pattern. In this paper, we attempt to visualize and understand this self-modularization, and suggest a unified explanation for the beneficial properties of such networks. We also show how our insights can be directly useful for efficiently performing retrieval over large datasets using neural networks.", "histories": [["v1", "Sun, 5 Oct 2014 14:46:47 GMT  (5697kb,D)", "http://arxiv.org/abs/1410.1165v1", "11 pages, 8 figures"], ["v2", "Mon, 22 Dec 2014 20:07:17 GMT  (5796kb,D)", "http://arxiv.org/abs/1410.1165v2", "9 pages + 2 supplementary, Under review as a conference paper at ICLR 2015"], ["v3", "Thu, 9 Apr 2015 01:22:49 GMT  (5725kb,D)", "http://arxiv.org/abs/1410.1165v3", "9 pages + 2 supplementary, Accepted to ICLR 2015 Conference track"]], "COMMENTS": "11 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["rupesh kumar srivastava", "jonathan masci", "faustino gomez", "j\\\"urgen schmidhuber"], "accepted": true, "id": "1410.1165"}
