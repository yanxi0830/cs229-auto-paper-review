{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Similarity Learning for Provably Accurate Sparse Linear Classification", "abstract": "In recent years, the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions. Most of the state of the art focus on learning Mahalanobis distances (requiring to fulfill a constraint of positive semi-definiteness) for use in a local k-NN algorithm. However, no theoretical link is established between the learned metrics and their performance in classification. In this paper, we make use of the formal framework of good similarities introduced by Balcan et al. to design an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space, which is then used to build a global linear classifier. We show that our approach has uniform stability and derive a generalization bound on the classification error. Experiments performed on various datasets confirm the effectiveness of our approach compared to state-of-the-art methods and provide evidence that (i) it is fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (230kb)", "http://arxiv.org/abs/1206.6476v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["aur\u00e9lien bellet", "amaury habrard", "marc sebban"], "accepted": true, "id": "1206.6476"}
