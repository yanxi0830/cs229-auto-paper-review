{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2014", "title": "Expectation-Maximization for Learning Determinantal Point Processes", "abstract": "A determinantal point process (DPP) is a probabilistic model of set diversity compactly parameterized by a positive semi-definite kernel matrix. To fit a DPP to a given task, we would like to learn the entries of its kernel matrix by maximizing the log-likelihood of the available data. However, log-likelihood is non-convex in the entries of the kernel matrix, and this learning problem is conjectured to be NP-hard. Thus, previous work has instead focused on more restricted convex learning settings: learning only a single weight for each row of the kernel matrix, or learning weights for a linear combination of DPPs with fixed kernel matrices. In this work we propose a novel algorithm for learning the full kernel matrix. By changing the kernel parameterization from matrix entries to eigenvalues and eigenvectors, and then lower-bounding the likelihood in the manner of expectation-maximization algorithms, we obtain an effective optimization procedure. We test our method on a real-world product recommendation task, and achieve relative gains of up to 16.5% in test log-likelihood compared to the naive approach of maximizing likelihood by projected gradient ascent on the entries of the kernel matrix.", "histories": [["v1", "Tue, 4 Nov 2014 21:23:35 GMT  (1413kb,D)", "http://arxiv.org/abs/1411.1088v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jennifer gillenwater", "alex kulesza", "emily b fox", "benjamin taskar"], "accepted": true, "id": "1411.1088"}
