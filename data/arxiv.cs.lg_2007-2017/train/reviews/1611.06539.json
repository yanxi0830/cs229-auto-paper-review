{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Efficient Stochastic Inference of Bitwise Deep Neural Networks", "abstract": "Recently published methods enable training of bitwise neural networks which allow reduced representation of down to a single bit per weight. We present a method that exploits ensemble decisions based on multiple stochastically sampled network models to increase performance figures of bitwise neural networks in terms of classification accuracy at inference. Our experiments with the CIFAR-10 and GTSRB datasets show that the performance of such network ensembles surpasses the performance of the high-precision base model. With this technique we achieve 5.81% best classification error on CIFAR-10 test set using bitwise networks. Concerning inference on embedded systems we evaluate these bitwise networks using a hardware efficient stochastic rounding procedure. Our work contributes to efficient embedded bitwise neural networks.", "histories": [["v1", "Sun, 20 Nov 2016 16:05:07 GMT  (307kb,D)", "http://arxiv.org/abs/1611.06539v1", "6 pages, 3 figures, Workshop on Efficient Methods for Deep Neural Networks at Neural Information Processing Systems Conference 2016, NIPS 2016, EMDNN 2016"]], "COMMENTS": "6 pages, 3 figures, Workshop on Efficient Methods for Deep Neural Networks at Neural Information Processing Systems Conference 2016, NIPS 2016, EMDNN 2016", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sebastian vogel", "christoph schorn", "re guntoro", "gerd ascheid"], "accepted": false, "id": "1611.06539"}
