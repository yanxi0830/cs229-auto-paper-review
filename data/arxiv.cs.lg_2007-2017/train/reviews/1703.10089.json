{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Position-based Content Attention for Time Series Forecasting with Sequence-to-sequence RNNs", "abstract": "In this paper, we study the use of recurrent neural networks (RNNs) for modeling and forecasting time series. We first illustrate the fact that standard sequence-to-sequence RNNs neither capture well periods in time series nor handle well missing values, even though many real life times series are periodic and contain missing values. We then propose an extended attention mechanism that can be deployed on top of any RNN and that is designed to capture periods and make the RNN more robust to missing values. We show the effectiveness of this novel model through extensive experiments with multiple univariate and multivariate datasets.", "histories": [["v1", "Wed, 29 Mar 2017 15:11:16 GMT  (248kb,D)", "http://arxiv.org/abs/1703.10089v1", null], ["v2", "Mon, 21 Aug 2017 12:36:58 GMT  (192kb,D)", "http://arxiv.org/abs/1703.10089v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["yagmur g cinar", "hamid mirisaee", "parantapa goswami", "eric gaussier", "ali ait-bachir", "vadim strijov"], "accepted": false, "id": "1703.10089"}
