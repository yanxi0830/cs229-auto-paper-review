{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2016", "title": "Low-rank passthrough neural networks", "abstract": "Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. Effective learning in this setting is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem of naive deep networks. Many of these architectures, such as LSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a single structural principle: the state passthrough.", "histories": [["v1", "Thu, 10 Mar 2016 01:04:07 GMT  (244kb,D)", "https://arxiv.org/abs/1603.03116v1", "16 pages, 7 figures"], ["v2", "Thu, 19 May 2016 19:38:30 GMT  (333kb,D)", "http://arxiv.org/abs/1603.03116v2", "17 pages, 8 figures"]], "COMMENTS": "16 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["antonio valerio miceli barone"], "accepted": false, "id": "1603.03116"}
