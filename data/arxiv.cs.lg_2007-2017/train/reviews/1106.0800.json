{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2011", "title": "Optimal Reinforcement Learning for Gaussian Systems", "abstract": "The exploration-exploitation tradeoff is among the central challenges of reinforcement learning. A hypothetical exact Bayesian learner would provide the optimal solution, but is intractable in general. I show that, however, in the specific case of Gaussian process inference, it is possible to make analytic statements about optimal learning of both rewards and transition dynamics, for nonlinear, time-varying systems in continuous time and space, subject to a relatively weak restriction on the dynamics. The solution is described by an infinite-dimensional differential equation. For a first impression of how this result may be useful, I also provide an approximate reduction to a finite-dimensional problem, with a numeric solution.", "histories": [["v1", "Sat, 4 Jun 2011 08:14:59 GMT  (2456kb,AD)", "http://arxiv.org/abs/1106.0800v1", null], ["v2", "Wed, 7 Sep 2011 16:11:15 GMT  (37kb,D)", "http://arxiv.org/abs/1106.0800v2", "updated to camera-ready version for publication in NIPS 2011. Note some nontrivial changes to the Equations on page 4"], ["v3", "Fri, 14 Oct 2011 15:01:11 GMT  (39kb,D)", "http://arxiv.org/abs/1106.0800v3", "final pre-conference version of this NIPS 2011 paper. Once again, please note some nontrivial changes to exposition and interpretation of the results, in particular in Equation (9) and Eqs. 11-14. The algorithm and results have remained the same, but their theoretical interpretation has changed"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["philipp hennig"], "accepted": true, "id": "1106.0800"}
