{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Attending to Characters in Neural Sequence Labeling Models", "abstract": "Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.", "histories": [["v1", "Mon, 14 Nov 2016 12:36:07 GMT  (149kb,D)", "http://arxiv.org/abs/1611.04361v1", "Proceedings of COLING 2016"]], "COMMENTS": "Proceedings of COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["marek rei", "gamal k o crichton", "sampo pyysalo"], "accepted": false, "id": "1611.04361"}
