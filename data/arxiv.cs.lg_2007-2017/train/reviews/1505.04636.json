{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2015", "title": "Graph Partitioning via Parallel Submodular Approximation to Accelerate Distributed Machine Learning", "abstract": "Distributed computing excels at processing large scale data, but the communication cost for synchronizing the shared parameters may slow down the overall performance. Fortunately, the interactions between parameter and data in many problems are sparse, which admits efficient partition in order to reduce the communication overhead.", "histories": [["v1", "Mon, 18 May 2015 13:43:46 GMT  (194kb,D)", "http://arxiv.org/abs/1505.04636v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.AI cs.LG", "authors": ["mu li", "dave g", "ersen", "alexander j smola"], "accepted": false, "id": "1505.04636"}
