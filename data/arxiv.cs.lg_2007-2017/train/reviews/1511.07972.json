{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Learning with Memory Embeddings", "abstract": "Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. In recent publications the embedding models were extended to also consider temporal evolutions, temporal patterns and subsymbolic representations. These extended models were used successfully to predict clinical events like procedures, lab measurements, and diagnoses. In this paper, we attempt to map these embedding models, which were developed purely as solutions to technical problems, to various cognitive memory functions, in particular to semantic and concept memory, episodic memory and sensory memory. We also make an analogy between a predictive model, which uses entity representations derived in memory models, to working memory. Cognitive memory functions are typically classified as long-term or short-term memory, where long-term memory has the subcategories declarative memory and non-declarative memory and the short term memory has the subcategories sensory memory and working memory. There is evidence that these main cognitive categories are partially dissociated from one another in the brain, as expressed in their differential sensitivity to brain damage. However, there is also evidence indicating that the different memory functions are not mutually independent. A hypothesis that arises out off this work is that mutual information exchange can be achieved by sharing or coupling of distributed latent representations of entities across different memory functions.", "histories": [["v1", "Wed, 25 Nov 2015 07:06:09 GMT  (19kb)", "https://arxiv.org/abs/1511.07972v1", "7 pages"], ["v2", "Tue, 1 Dec 2015 05:53:38 GMT  (22kb)", "http://arxiv.org/abs/1511.07972v2", "7 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v3", "Wed, 16 Dec 2015 23:38:03 GMT  (376kb,D)", "http://arxiv.org/abs/1511.07972v3", "14 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v4", "Mon, 21 Dec 2015 22:35:39 GMT  (218kb,D)", "http://arxiv.org/abs/1511.07972v4", "14 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v5", "Mon, 25 Jan 2016 20:02:39 GMT  (285kb,D)", "http://arxiv.org/abs/1511.07972v5", "14 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v6", "Wed, 13 Apr 2016 17:23:42 GMT  (439kb,D)", "http://arxiv.org/abs/1511.07972v6", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v7", "Thu, 21 Apr 2016 04:40:58 GMT  (293kb,D)", "http://arxiv.org/abs/1511.07972v7", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v8", "Thu, 5 May 2016 14:57:41 GMT  (297kb,D)", "http://arxiv.org/abs/1511.07972v8", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v9", "Sat, 7 May 2016 09:06:15 GMT  (291kb,D)", "http://arxiv.org/abs/1511.07972v9", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["volker tresp", "crist\\'obal esteban", "yinchong yang", "stephan baier", "denis krompa{\\ss}"], "accepted": false, "id": "1511.07972"}
