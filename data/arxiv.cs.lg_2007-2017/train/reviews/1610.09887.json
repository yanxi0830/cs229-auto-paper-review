{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks", "abstract": "We provide a depth-based separation result for feed-forward ReLU neural networks, showing that a wide family of non-linear, twice-differentiable functions on $[0,1]^d$, which can be approximated to accuracy $\\epsilon$ by ReLU networks of depth and width $\\mathcal{O}(\\text{poly}(\\log(1/\\epsilon)))$, cannot be approximated to similar accuracy by constant-depth ReLU networks, unless their width is at least $\\Omega(1/\\epsilon)$.", "histories": [["v1", "Mon, 31 Oct 2016 12:08:46 GMT  (16kb)", "http://arxiv.org/abs/1610.09887v1", null], ["v2", "Thu, 9 Mar 2017 18:07:37 GMT  (75kb,D)", "http://arxiv.org/abs/1610.09887v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["itay safran", "ohad shamir"], "accepted": true, "id": "1610.09887"}
