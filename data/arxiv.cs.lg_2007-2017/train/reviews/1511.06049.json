{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "What Objective Does Self-paced Learning Indeed Optimize?", "abstract": "Self-paced learning (SPL) has been attracting increasing attention in machine learning and computer vision. Albeit empirically substantiated to be effective, the investigation on its theoretical insight is still a blank. It is even unknown that what objective a general SPL regime converges to. To this issue, this study attempts to initially provide some new insights under this \"heuristic\" learning scheme. Specifically, we prove that the solving strategy on SPL exactly accords with a majorization minimization algorithm, a well known technique in optimization and machine learning, implemented on a latent objective. A more interesting finding is that, the loss function contained in this latent objective has a similar configuration with non-convex regularized penalty, an attractive topic in statistics and machine learning. In particular, we show that the previous hard and linear self-paced regularizers are equivalent to the capped norm and minimax concave plus penalties, respectively, both being widely investigated in statistics. Such connections between SPL and previous known researches enhance new insightful comprehension on SPL, like convergence and parameter setting rationality. The correctness of the proposed theory is substantiated by experimental results on synthetic and UCI data sets.", "histories": [["v1", "Thu, 19 Nov 2015 02:55:18 GMT  (249kb,D)", "http://arxiv.org/abs/1511.06049v1", "11 pages, 2 figures"], ["v2", "Tue, 1 Nov 2016 13:59:27 GMT  (205kb,D)", "http://arxiv.org/abs/1511.06049v2", "25 pages, 1 figures"]], "COMMENTS": "11 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["deyu meng", "qian zhao", "lu jiang"], "accepted": false, "id": "1511.06049"}
