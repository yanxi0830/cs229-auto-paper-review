{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems", "abstract": "The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user's intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning.", "histories": [["v1", "Tue, 24 May 2016 21:56:08 GMT  (703kb,D)", "https://arxiv.org/abs/1605.07669v1", "Accepted as a long paper in ACL 2016"], ["v2", "Thu, 2 Jun 2016 14:01:07 GMT  (857kb,D)", "http://arxiv.org/abs/1605.07669v2", "Accepted as a long paper in ACL 2016"]], "COMMENTS": "Accepted as a long paper in ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["pei-hao su", "milica gasic", "nikola mrksic", "lina maria rojas-barahona", "stefan ultes", "david vandyke", "tsung-hsien wen", "steve j young"], "accepted": true, "id": "1605.07669"}
