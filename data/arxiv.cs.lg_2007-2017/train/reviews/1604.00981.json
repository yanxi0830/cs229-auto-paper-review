{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Revisiting Distributed Synchronous SGD", "abstract": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe &amp; Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary.", "histories": [["v1", "Mon, 4 Apr 2016 18:40:05 GMT  (418kb,D)", "http://arxiv.org/abs/1604.00981v1", "5 pages with 3 figures"], ["v2", "Fri, 15 Apr 2016 18:49:12 GMT  (52kb,D)", "http://arxiv.org/abs/1604.00981v2", "5 pages with 3 figures"], ["v3", "Tue, 21 Mar 2017 07:44:39 GMT  (570kb,D)", "http://arxiv.org/abs/1604.00981v3", "10 pages"]], "COMMENTS": "5 pages with 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DC cs.NE", "authors": ["jianmin chen", "xinghao pan", "rajat monga", "samy bengio", "rafal jozefowicz"], "accepted": false, "id": "1604.00981"}
