{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2014", "title": "On the Number of Linear Regions of Deep Neural Networks", "abstract": "We study the complexity of functions computable by deep feedforward neural networks with piece-wise linear activations in terms of the number of regions of linearity that they have. Deep networks are able to sequentially map portions of each layer's input space to the same output. In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth. This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.", "histories": [["v1", "Sat, 8 Feb 2014 17:16:27 GMT  (406kb,D)", "http://arxiv.org/abs/1402.1869v1", "12 pages"], ["v2", "Sat, 7 Jun 2014 19:56:14 GMT  (3672kb,D)", "http://arxiv.org/abs/1402.1869v2", null]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["guido f mont\u00fafar", "razvan pascanu", "kyunghyun cho", "yoshua bengio"], "accepted": true, "id": "1402.1869"}
