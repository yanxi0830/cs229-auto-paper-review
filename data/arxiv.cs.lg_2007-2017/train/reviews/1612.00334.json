{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Examples", "abstract": "Recent literature has pointed out that machine learning classifiers, including deep neural networks (DNN), are vulnerable to adversarial samples that are maliciously created inputs that force a machine learning classifier to produce wrong output labels. Multiple studies have tried to analyze and thus harden machine classifiers under such adversarial noise (AN). However, they are mostly empirical and provide little understanding of the underlying principles that enable evaluation of the robustness of a classier against AN. This paper proposes a unified framework using two metric spaces to evaluate classifiers' robustness against AN and provides general guidance for hardening such classifiers. The central idea of our work is that for a certain classification task, the robustness of a classifier $f_1$ against AN is decided by both $f_1$ and its oracle $f_2$ (like human annotator of that specific task). In particular: (1) By adding oracle $f_2$ into the framework, we provide a general definition of the adversarial sample problem. (2) We theoretically formulate a definition that decides whether a classifier is always robust against AN (strong-robustness); (3) Using two metric spaces ($X_1,d_1$) and ($X_2,d_2$) defined by $f_1$ and $f_2$ respectively, we prove that the topological equivalence between ($X_1,d_1$) and ($X_2,d_2$) is sufficient in deciding whether $f_1$ is strong-robust at test time, or not; (5) By training a DNN classifier using the Siamese architecture, we propose a new defense strategy \"Siamese training\" to intuitively approach topological equivalence between ($X_1,d_1$) and ($X_2,d_2$). Experimental results show that Siamese training helps multiple DNN models achieve better accuracy compared to previous defense strategies in an adversarial setting. DNN models after Siamese training exhibit better robustness than the state-of-the-art baselines.", "histories": [["v1", "Thu, 1 Dec 2016 16:20:39 GMT  (978kb,D)", "http://arxiv.org/abs/1612.00334v1", "20 pages , submitting to ICLR 2017"], ["v2", "Mon, 5 Dec 2016 17:07:35 GMT  (1311kb,D)", "http://arxiv.org/abs/1612.00334v2", "20 pages , submitting to ICLR 2017"], ["v3", "Tue, 17 Jan 2017 22:23:55 GMT  (2899kb,D)", "http://arxiv.org/abs/1612.00334v3", "30 pages , submitting to ICLR 2017"], ["v4", "Sat, 21 Jan 2017 16:37:24 GMT  (2908kb,D)", "http://arxiv.org/abs/1612.00334v4", "30 pages , submitting to ICLR 2017"], ["v5", "Thu, 26 Jan 2017 15:32:06 GMT  (2918kb,D)", "http://arxiv.org/abs/1612.00334v5", "30 pages , submitting to ICLR 2017"], ["v6", "Wed, 1 Feb 2017 17:30:50 GMT  (2922kb,D)", "http://arxiv.org/abs/1612.00334v6", "30 pages , submitting to ICLR 2017"], ["v7", "Thu, 2 Feb 2017 14:39:50 GMT  (2922kb,D)", "http://arxiv.org/abs/1612.00334v7", "30 pages , submitting to ICLR 2017"], ["v8", "Fri, 3 Feb 2017 16:06:39 GMT  (2924kb,D)", "http://arxiv.org/abs/1612.00334v8", "30 pages , submitting to ICLR 2017"], ["v9", "Mon, 27 Feb 2017 20:18:26 GMT  (3233kb,D)", "http://arxiv.org/abs/1612.00334v9", "35 pages , submitting to ICLR 2017"], ["v10", "Thu, 9 Mar 2017 22:00:56 GMT  (3218kb,D)", "http://arxiv.org/abs/1612.00334v10", "35 pages , submitting to ICLR 2017"], ["v11", "Thu, 27 Apr 2017 14:36:40 GMT  (3029kb,D)", "http://arxiv.org/abs/1612.00334v11", "38 pages , ICLR 2017 Workshop Track"], ["v12", "Wed, 27 Sep 2017 16:02:48 GMT  (3236kb,D)", "http://arxiv.org/abs/1612.00334v12", "38 pages , ICLR 2017 Workshop Track"]], "COMMENTS": "20 pages , submitting to ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.CV", "authors": ["beilun wang", "ji gao", "yanjun qi"], "accepted": false, "id": "1612.00334"}
