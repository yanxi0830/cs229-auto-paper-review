{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2017", "title": "Collect at Once, Use Effectively: Making Non-interactive Locally Private Learning Possible", "abstract": "Non-interactive Local Differential Privacy (LDP) requires data analysts to collect data from users through noisy channel at once. In this paper, we extend the frontiers of Non-interactive LDP learning and estimation from several aspects. For learning with smooth generalized linear losses, we propose an approximate stochastic gradient oracle estimated from non-interactive LDP channel, using Chebyshev expansion. Combined with inexact gradient methods, we obtain an efficient algorithm with quasi-polynomial sample complexity bound. For the high-dimensional world, we discover that under $\\ell_2$-norm assumption on data points, high-dimensional sparse linear regression and mean estimation can be achieved with logarithmic dependence on dimension, using random projection and approximate recovery. We also extend our methods to Kernel Ridge Regression. Our work is the first one that makes learning and estimation possible for a broad range of learning tasks under non-interactive LDP model.", "histories": [["v1", "Sun, 11 Jun 2017 07:04:50 GMT  (29kb)", "http://arxiv.org/abs/1706.03316v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["kai zheng 0007", "wenlong mou", "liwei wang 0001"], "accepted": true, "id": "1706.03316"}
