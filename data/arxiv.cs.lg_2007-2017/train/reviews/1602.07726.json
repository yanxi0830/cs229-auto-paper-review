{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Adaptive Learning with Robust Generalization Guarantees", "abstract": "The traditional notion of generalization --- i.e., learning a hypothesis whose empirical error is close to its true error --- is surprisingly brittle. As has recently been noted [DFH+15b], even if several algorithms have this guarantee in isolation, the guarantee need not hold if the algorithms are composed adaptively. In this paper, we study three notions of generalization ---increasing in strength--- that are robust to post-processing and amenable to adaptive composition, and examine the relationships between them.", "histories": [["v1", "Wed, 24 Feb 2016 21:59:30 GMT  (136kb)", "http://arxiv.org/abs/1602.07726v1", null], ["v2", "Thu, 2 Jun 2016 00:07:01 GMT  (158kb)", "http://arxiv.org/abs/1602.07726v2", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["rachel cummings", "katrina ligett", "kobbi nissim", "aaron roth", "zhiwei steven wu"], "accepted": false, "id": "1602.07726"}
