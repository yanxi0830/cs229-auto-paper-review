{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Reinforcement Learning for Learning Rate Control", "abstract": "Stochastic gradient descent (SGD), which updates the model parameters by adding a local gradient times a learning rate at each step, is widely used in model training of machine learning algorithms such as neural networks. It is observed that the models trained by SGD are sensitive to learning rates and good learning rates are problem specific. We propose an algorithm to automatically learn learning rates using neural network based actor-critic methods from deep reinforcement learning (RL).In particular, we train a policy network called actor to decide the learning rate at each step during training, and a value network called critic to give feedback about quality of the decision (e.g., the goodness of the learning rate outputted by the actor) that the actor made. The introduction of auxiliary actor and critic networks helps the main network achieve better performance. Experiments on different datasets and network architectures show that our approach leads to better convergence of SGD than human-designed competitors.", "histories": [["v1", "Wed, 31 May 2017 15:58:35 GMT  (4040kb,D)", "http://arxiv.org/abs/1705.11159v1", "7 pages, 9 figures"]], "COMMENTS": "7 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chang xu", "tao qin", "gang wang", "tie-yan liu"], "accepted": false, "id": "1705.11159"}
