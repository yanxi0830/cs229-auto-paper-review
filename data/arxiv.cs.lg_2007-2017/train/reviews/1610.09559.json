{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "Fair Algorithms for Infinite and Contextual Bandits", "abstract": "Motivated by concerns that automated decision-making procedures can unintentionally lead to discriminatory behavior, we study a technical definition of fairness modeled after John Rawls' notion of \"fair equality of opportunity\". In the context of a simple model of online decision making, we give an algorithm that satisfies this fairness constraint, while still being able to learn at a rate that is comparable to (but necessarily worse than) that of the best algorithms absent a fairness constraint. We prove a regret bound for fair algorithms in the linear contextual bandit framework that is a significant improvement over our technical companion paper [16], which gives black-box reductions in a more general setting. We analyze our algorithms both theoretically and experimentally. Finally, we introduce the notion of a \"discrimination index\", and show that standard algorithms for our problem exhibit structured discriminatory behavior, whereas the \"fair\" algorithms we develop do not.", "histories": [["v1", "Sat, 29 Oct 2016 18:46:11 GMT  (819kb,D)", "http://arxiv.org/abs/1610.09559v1", null], ["v2", "Tue, 1 Nov 2016 12:14:54 GMT  (819kb,D)", "http://arxiv.org/abs/1610.09559v2", null], ["v3", "Fri, 14 Apr 2017 19:10:14 GMT  (4204kb,D)", "http://arxiv.org/abs/1610.09559v3", null], ["v4", "Thu, 29 Jun 2017 15:46:55 GMT  (1035kb,D)", "http://arxiv.org/abs/1610.09559v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["matthew joseph", "michael kearns", "jamie morgenstern", "seth neel", "aaron roth"], "accepted": false, "id": "1610.09559"}
