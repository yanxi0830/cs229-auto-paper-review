{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2016", "title": "Deep Transfer Learning with Joint Adaptation Networks", "abstract": "Deep networks rely on massive amounts of labeled data to learn powerful models. For a target task short of labeled data, transfer learning enables model adaptation from a different source domain. This paper addresses deep transfer learning under a more general scenario that the joint distributions of features and labels may change substantially across domains. Based on the theory of Hilbert space embedding of distributions, a novel joint distribution discrepancy is proposed to directly compare joint distributions across domains, eliminating the need of marginal-conditional factorization. Transfer learning is enabled in deep convolutional networks, where the dataset shifts may linger in multiple task-specific feature layers and the classifier layer. A set of joint adaptation networks are crafted to match the joint distributions of these layers across domains by minimizing the joint distribution discrepancy, which can be trained efficiently using back-propagation. Experiments show that the new approach yields state of the art results on standard domain adaptation datasets.", "histories": [["v1", "Sat, 21 May 2016 12:56:14 GMT  (112kb,D)", "http://arxiv.org/abs/1605.06636v1", null], ["v2", "Thu, 17 Aug 2017 07:35:59 GMT  (428kb,D)", "http://arxiv.org/abs/1605.06636v2", "34th International Conference on Machine Learning"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["mingsheng long", "han zhu", "jianmin wang 0001", "michael i jordan"], "accepted": true, "id": "1605.06636"}
