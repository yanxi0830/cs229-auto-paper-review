{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "abstract": "We investigate evaluation metrics for end-to-end dialogue systems where supervised labels, such as task completion, are not available. Recent works in end-to-end dialogue systems have adopted metrics from machine translation and text summarization to compare a model's generated response to a single target response. We show that these metrics correlate very weakly or not at all with human judgements of the response quality in both technical and non-technical domains. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.", "histories": [["v1", "Fri, 25 Mar 2016 20:32:21 GMT  (787kb,D)", "http://arxiv.org/abs/1603.08023v1", "First 4 authors had equal contribution. 13 pages, 5 tables, 6 figures. Submitted to ACL 2016"], ["v2", "Tue, 3 Jan 2017 18:28:32 GMT  (723kb,D)", "http://arxiv.org/abs/1603.08023v2", "First 4 authors had equal contribution. 13 pages, 5 tables, 6 figures. EMNLP 2016"]], "COMMENTS": "First 4 authors had equal contribution. 13 pages, 5 tables, 6 figures. Submitted to ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["chia-wei liu", "ryan lowe", "iulian serban", "michael noseworthy", "laurent charlin", "joelle pineau"], "accepted": true, "id": "1603.08023"}
