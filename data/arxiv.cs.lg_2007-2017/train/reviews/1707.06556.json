{"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jul-2017", "title": "High-risk learning: acquiring new word vectors from tiny data", "abstract": "Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn 'a good vector' for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences' worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.", "histories": [["v1", "Thu, 20 Jul 2017 15:02:14 GMT  (21kb,D)", "http://arxiv.org/abs/1707.06556v1", "Accepted as short paper at EMNLP 2017"]], "COMMENTS": "Accepted as short paper at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["aur\u00e9lie herbelot", "marco baroni"], "accepted": true, "id": "1707.06556"}
