{"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Scalable Stochastic Alternating Direction Method of Multipliers", "abstract": "Stochastic alternating direction method of multipliers (ADMM), which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM. However, most stochastic methods can only achieve a convergence rate $O(1/\\sqrt T)$ on general convex problems,where T is the number of iterations. Hence, these methods are not scalable with respect to convergence rate (computation cost). There exists only one stochastic method, called SA-ADMM, which can achieve convergence rate $O(1/T )$ on general convex problems. However, an extra memory is needed for SA-ADMM to store the historic gradients on all samples, and thus it is not scalable with respect to storage cost. In this paper, we propose a novel method, called scalable stochastic ADMM(SCAS-ADMM), for large-scale optimization and learning problems. Without the need to store the historic gradients, SCAS-ADMM can achieve the same convergence rate $O(1/T )$ as the best stochastic method SA-ADMM and batch ADMM on general convex problems. Experiments on graph-guided fused lasso show that SCAS-ADMM can achieve state-of-the-art performance in real applications", "histories": [["v1", "Thu, 12 Feb 2015 04:01:46 GMT  (823kb,D)", "https://arxiv.org/abs/1502.03529v1", null], ["v2", "Sun, 1 Mar 2015 13:15:14 GMT  (825kb,D)", "http://arxiv.org/abs/1502.03529v2", null], ["v3", "Mon, 20 Jul 2015 10:01:27 GMT  (829kb,D)", "http://arxiv.org/abs/1502.03529v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shen-yi zhao", "wu-jun li", "zhi-hua zhou"], "accepted": true, "id": "1502.03529"}
