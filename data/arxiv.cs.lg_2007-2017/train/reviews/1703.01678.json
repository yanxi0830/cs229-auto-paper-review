{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2017", "title": "Data-Dependent Stability of Stochastic Gradient Descent", "abstract": "We establish a data-dependent notion of algorithmic stability for Stochastic Gradient Descent (SGD) and employ it to develop novel generalization bounds. This is in contrast to previous distribution-free algorithmic stability results for SGD which depend on the worst-case constants. By virtue of the data-dependent argument, our bounds provide new insights into learning with SGD on convex and non-convex problems. In the convex case, we show that the bound on the generalization error is multiplicative in the risk at the initialization point. In the non-convex case, we prove that the expected curvature of the objective function around the initialization point has crucial influence on the generalization error. In both cases, our results suggest a simple data-driven strategy to stabilize SGD by pre-screening its initialization.", "histories": [["v1", "Sun, 5 Mar 2017 22:22:34 GMT  (29kb,D)", "https://arxiv.org/abs/1703.01678v1", null], ["v2", "Wed, 22 Mar 2017 17:16:17 GMT  (33kb,D)", "http://arxiv.org/abs/1703.01678v2", null], ["v3", "Fri, 26 May 2017 17:33:50 GMT  (119kb,D)", "http://arxiv.org/abs/1703.01678v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ilja kuzborskij", "christoph h lampert"], "accepted": false, "id": "1703.01678"}
