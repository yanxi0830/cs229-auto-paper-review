{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks", "abstract": "Convolutional Neural Networks have achieved state-of-the-art performance on a wide range of tasks. Most benchmarks are led by ensembles of these powerful learners, but ensembling is typically treated as a post-hoc procedure implemented by averaging independently trained models with model variation induced by bagging or random initialization. In this paper, we rigorously treat ensembling as a first-class problem to explicitly address the question: what are the best strategies to create an ensemble? We first compare a large number of ensembling strategies, and then propose and evaluate novel strategies, such as parameter sharing (through a new family of models we call TreeNets) as well as training under ensemble-aware and diversity-encouraging losses. We demonstrate that TreeNets can improve ensemble performance and that diverse ensembles can be trained end-to-end under a unified loss, achieving significantly higher \"oracle\" accuracies than classical ensembles.", "histories": [["v1", "Thu, 19 Nov 2015 19:19:58 GMT  (7788kb,D)", "http://arxiv.org/abs/1511.06314v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["stefan lee", "senthil purushwalkam", "michael cogswell", "david crandall", "dhruv batra"], "accepted": false, "id": "1511.06314"}
