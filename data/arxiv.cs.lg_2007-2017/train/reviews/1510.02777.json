{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Early Inference in Energy-Based Models Approximates Back-Propagation", "abstract": "We show that Langevin MCMC inference in an energy-based model with latent variables has the property that the early steps of inference, starting from a stationary point, correspond to propagating error gradients into internal layers, similarly to back-propagation. The error that is back-propagated is with respect to visible units that have received an outside driving force pushing them away from the stationary point. Back-propagated error gradients correspond to temporal derivatives of the activation of hidden units. This observation could be an element of a theory for explaining how brains perform credit assignment in deep hierarchies as efficiently as back-propagation does. In this theory, the continuous-valued latent variables correspond to averaged voltage potential (across time, spikes, and possibly neurons in the same minicolumn), and neural computation corresponds to approximate inference and error back-propagation at the same time.", "histories": [["v1", "Fri, 9 Oct 2015 19:21:32 GMT  (18kb)", "http://arxiv.org/abs/1510.02777v1", "arXiv admin note: text overlap witharXiv:1509.05936"], ["v2", "Sun, 7 Feb 2016 20:24:38 GMT  (19kb)", "http://arxiv.org/abs/1510.02777v2", "arXiv admin note: text overlap witharXiv:1509.05936"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1509.05936", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "asja fischer"], "accepted": false, "id": "1510.02777"}
