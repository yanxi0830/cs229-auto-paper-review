{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Multigrid Neural Architectures", "abstract": "We propose a multigrid extension of convolutional neural networks (CNNs). Rather than manipulating representations living on a single spatial grid, our network layers operate across scale space, on a pyramid of tensors. They consume multigrid inputs and produce multigrid outputs; convolutional filters themselves have both within-scale and cross-scale extent. This aspect is distinct from simple multiscale designs, which only process the input at different scales. Viewed in terms of information flow, a multigrid network passes messages across a spatial pyramid. As a consequence, receptive field size grows exponentially with depth, facilitating rapid integration of context. Most critically, multigrid structure enables networks to learn internal attention and dynamic routing mechanisms, and use them to accomplish tasks on which modern CNNs fail.", "histories": [["v1", "Wed, 23 Nov 2016 06:55:53 GMT  (815kb,D)", "http://arxiv.org/abs/1611.07661v1", "Code available:this http URL"], ["v2", "Thu, 11 May 2017 19:24:33 GMT  (818kb,D)", "http://arxiv.org/abs/1611.07661v2", "updated with ImageNet results; to appear at CVPR 2017"]], "COMMENTS": "Code available:this http URL", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["tsung-wei ke", "michael maire", "stella x yu"], "accepted": false, "id": "1611.07661"}
