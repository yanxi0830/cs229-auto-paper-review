{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2017", "title": "Geometry of Optimization and Implicit Regularization in Deep Learning", "abstract": "We argue that the optimization plays a crucial role in generalization of deep learning models through implicit regularization. We do this by demonstrating that generalization ability is not controlled by network size but rather by some other implicit control. We then demonstrate how changing the empirical optimization procedure can improve generalization, even if actual optimization quality is not affected. We do so by studying the geometry of the parameter space of deep networks, and devising an optimization algorithm attuned to this geometry.", "histories": [["v1", "Mon, 8 May 2017 20:12:08 GMT  (359kb,D)", "http://arxiv.org/abs/1705.03071v1", "This survey chapter was done as a part of Intel Collaborative Research institute for Computational Intelligence (ICRI-CI) \"Why &amp; When Deep Learning works -- looking inside Deep Learning\" compendium with the generous support of ICRI-CI. arXiv admin note: substantial text overlap witharXiv:1506.02617"]], "COMMENTS": "This survey chapter was done as a part of Intel Collaborative Research institute for Computational Intelligence (ICRI-CI) \"Why &amp; When Deep Learning works -- looking inside Deep Learning\" compendium with the generous support of ICRI-CI. arXiv admin note: substantial text overlap witharXiv:1506.02617", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["behnam neyshabur", "ryota tomioka", "ruslan salakhutdinov", "nathan srebro"], "accepted": false, "id": "1705.03071"}
