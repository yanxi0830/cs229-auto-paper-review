{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Learning Continuous Control Policies by Stochastic Value Gradients", "abstract": "We present a unified framework for learning continuous control policies using backpropagation. It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise. The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions. We use learned models but only require observations from the environment in- stead of observations from model-predicted trajectories, minimizing the impact of compounded model errors. We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation. One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.", "histories": [["v1", "Fri, 30 Oct 2015 16:07:51 GMT  (764kb,D)", "http://arxiv.org/abs/1510.09142v1", "13 pages, NIPS 2015"]], "COMMENTS": "13 pages, NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["nicolas heess", "gregory wayne", "david silver", "timothy p lillicrap", "tom erez", "yuval tassa"], "accepted": true, "id": "1510.09142"}
