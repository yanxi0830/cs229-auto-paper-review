{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Using Reinforcement Learning for Demand Response of Domestic Hot Water Buffers: a Real-Life Demonstration", "abstract": "This paper demonstrates a data-driven control approach for demand response in real-life residential buildings. The objective is to optimally schedule the heating cycles of the Domestic Hot Water (DHW) buffer to maximize the self-consumption of the local photovoltaic (PV) production. A model-based reinforcement learning technique is used to tackle the underlying sequential decision-making problem. The proposed algorithm learns the stochastic occupant behavior, predicts the PV production and takes into account the dynamics of the system. A real-life experiment with six residential buildings is performed using this algorithm. The results show that the self-consumption of the PV production is significantly increased, compared to the default thermostat control.", "histories": [["v1", "Thu, 16 Mar 2017 06:42:07 GMT  (61kb,D)", "http://arxiv.org/abs/1703.05486v1", "Submitted to IEEE ISGT Europe 2017"]], "COMMENTS": "Submitted to IEEE ISGT Europe 2017", "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["oscar de somer", "ana soares", "tristan kuijpers", "koen vossen", "koen vanthournout", "fred spiessens"], "accepted": false, "id": "1703.05486"}
