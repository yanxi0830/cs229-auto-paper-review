{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2015", "title": "Label Efficient Learning by Exploiting Multi-Class Output Codes", "abstract": "We analyze the popular multi-class algorithmic techniques of one-vs-all and error correcting output-codes, and show the surprising result that under the assumption that they are successful (at learning from labeled data), and under an additional mild distributional assumption, we can learn from unlabeled data (up to a permutation of the labels). The key point is that in cases where they work, these techniques implicitly assume structure on how the classes are related. We show how to exploit this relationship both in the case where the codewords are well separated (which includes the one-vs-all case) and in the case where the code matrix has the property that each bit of the codewords is important for distinguishing at least one class from impossible inputs.", "histories": [["v1", "Tue, 10 Nov 2015 18:50:03 GMT  (179kb,D)", "https://arxiv.org/abs/1511.03225v1", null], ["v2", "Mon, 22 Feb 2016 02:24:20 GMT  (296kb,D)", "http://arxiv.org/abs/1511.03225v2", null], ["v3", "Fri, 29 Jul 2016 03:58:08 GMT  (272kb,D)", "http://arxiv.org/abs/1511.03225v3", null], ["v4", "Fri, 25 Nov 2016 15:52:08 GMT  (251kb,D)", "http://arxiv.org/abs/1511.03225v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maria-florina balcan", "travis dick", "yishay mansour"], "accepted": true, "id": "1511.03225"}
