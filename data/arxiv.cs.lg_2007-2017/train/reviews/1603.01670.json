{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2016", "title": "Network Morphism", "abstract": "We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as \\emph{network morphism} in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.", "histories": [["v1", "Sat, 5 Mar 2016 02:06:43 GMT  (679kb,D)", "http://arxiv.org/abs/1603.01670v1", "Under review for ICML 2016"], ["v2", "Tue, 8 Mar 2016 16:36:00 GMT  (680kb,D)", "http://arxiv.org/abs/1603.01670v2", "Under review for ICML 2016"]], "COMMENTS": "Under review for ICML 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["tao wei", "changhu wang", "yong rui", "chang wen chen"], "accepted": true, "id": "1603.01670"}
