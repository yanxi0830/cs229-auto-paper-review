{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2012", "title": "Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes", "abstract": "Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines. In this paper, we investigate the performance of SGD \\emph{without} such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the \\emph{last} SGD iterate scales as O(\\log(T)/\\sqrt{T}) for non-smooth convex objective functions, and O(\\log(T)/T) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in \\citet{RakhShaSri12arxiv} is not as simple to implement). Finally, we provide some experimental illustrations.", "histories": [["v1", "Sat, 8 Dec 2012 18:22:42 GMT  (24kb,D)", "http://arxiv.org/abs/1212.1824v1", "To appear in ICML 2013"], ["v2", "Fri, 28 Dec 2012 10:58:48 GMT  (24kb,D)", "http://arxiv.org/abs/1212.1824v2", "To appear in ICML 2013"]], "COMMENTS": "To appear in ICML 2013", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["ohad shamir", "tong zhang 0001"], "accepted": true, "id": "1212.1824"}
