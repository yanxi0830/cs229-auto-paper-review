{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Max-Pooling Dropout for Regularization of Convolutional Neural Networks", "abstract": "Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.", "histories": [["v1", "Fri, 4 Dec 2015 13:18:37 GMT  (212kb)", "http://arxiv.org/abs/1512.01400v1", "The journal version of this paper [arXiv:1512.00242] has been published in Neural Networks,this http URL"]], "COMMENTS": "The journal version of this paper [arXiv:1512.00242] has been published in Neural Networks,this http URL", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["haibing wu", "xiaodong gu"], "accepted": false, "id": "1512.01400"}
