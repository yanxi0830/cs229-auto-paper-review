{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2015", "title": "Learning Deep Generative Models with Doubly Stochastic MCMC", "abstract": "Performing inference and learning of deep generative networks in a Bayesian setting is desirable, where a sparsity-inducing prior can be adopted on model parameters or a nonparametric Bayesian process can be used to infer the network structure. However, posterior inference for such deep models is an extremely challenging task, which has largely not been well-addressed. In this paper, we present doubly stochastic gradient-based MCMC, a simple and effective method that can be widely applied for Bayesian inference of deep generative models in continuous parameter spaces. The algorithm is doubly stochastic in the sense that at each MCMC sampling step a mini-batch of data samples are randomly drawn to estimate the gradient of log-posterior and the intractable expectation over latent variables is further estimated via a Monte Carlo sampler. We demonstrate the effectiveness on learning deep sigmoid belief networks (DSBNs). Compared to the state-of-the-art methods using Gibbs sampling with data augmentation, our algorithm is much more efficient and manages to learn DSBNs on large datasets.", "histories": [["v1", "Mon, 15 Jun 2015 11:37:09 GMT  (166kb,D)", "https://arxiv.org/abs/1506.04557v1", null], ["v2", "Sun, 11 Oct 2015 08:29:24 GMT  (370kb,D)", "http://arxiv.org/abs/1506.04557v2", null], ["v3", "Wed, 14 Oct 2015 12:28:24 GMT  (370kb,D)", "http://arxiv.org/abs/1506.04557v3", null], ["v4", "Mon, 7 Mar 2016 14:14:00 GMT  (352kb,D)", "http://arxiv.org/abs/1506.04557v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chao du", "jun zhu", "bo zhang"], "accepted": false, "id": "1506.04557"}
