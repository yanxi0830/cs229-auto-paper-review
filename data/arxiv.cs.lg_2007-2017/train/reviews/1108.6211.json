{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2011", "title": "Transfer from Multiple MDPs", "abstract": "Transfer reinforcement learning (RL) methods leverage on the experience collected on a set of source tasks to speed-up RL algorithms. A simple and effective approach is to transfer samples from source tasks and include them into the training set used to solve a given target task. In this paper, we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks. Finally, we report illustrative experimental results in a continuous chain problem.", "histories": [["v1", "Wed, 31 Aug 2011 12:46:11 GMT  (70kb)", "https://arxiv.org/abs/1108.6211v1", "2011"], ["v2", "Thu, 1 Sep 2011 09:19:00 GMT  (70kb)", "http://arxiv.org/abs/1108.6211v2", "2011"]], "COMMENTS": "2011", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["alessandro lazaric", "marcello restelli"], "accepted": true, "id": "1108.6211"}
