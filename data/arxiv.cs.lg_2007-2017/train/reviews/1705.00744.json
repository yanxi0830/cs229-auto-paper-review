{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "A Strategy for an Uncompromising Incremental Learner", "abstract": "Multi-class supervised learning systems require the knowledge of the entire range of labels they predict. Often when learnt incrementally, they suffer from catastrophic forgetting. To avoid this, generous leeways have to be made to the philosophy of incremental learning that either forces a part of the machine to not learn, or to retrain the machine again with a selection of the historic data. While these tricks work to various degrees, they do not adhere to the spirit of incremental learning. In this article, we redefine incremental learning with stringent conditions that do not allow for any undesirable relaxations and assumptions. We design a strategy involving generative models and the distillation of dark knowledge as a means of hallucinating data along with appropriate targets from past distributions. We call this technique phantom sampling. We show that phantom sampling helps avoid catastrophic forgetting during incremental learning. Using an implementation based on deep neural networks, we demonstrate that phantom sampling dramatically avoids catastrophic forgetting. We apply these strategies to competitive multi-class incremental learning of deep neural networks. Using various benchmark datasets through our strategy, we demonstrate that strict incremental learning could be achieved.", "histories": [["v1", "Tue, 2 May 2017 00:17:54 GMT  (2638kb,D)", "http://arxiv.org/abs/1705.00744v1", null], ["v2", "Mon, 17 Jul 2017 07:30:18 GMT  (4920kb,D)", "http://arxiv.org/abs/1705.00744v2", "Under review at IEEE Transactions of Neural Networks and Learning Systems"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ragav venkatesan", "hemanth venkateswara", "sethuraman panchanathan", "baoxin li"], "accepted": false, "id": "1705.00744"}
