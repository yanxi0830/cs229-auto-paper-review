{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2014", "title": "Fast Rates by Transferring from Auxiliary Hypotheses", "abstract": "In this work we consider the learning setting where in addition to the training set, the learner receives a collection of auxiliary hypotheses originating from other tasks. This paradigm, known as Hypothesis Transfer Learning (HTL), has been successfully exploited in empirical works, but only recently has received a theoretical attention. Here, we try to understand when HTL facilitates accelerated generalization -- the goal of the transfer learning paradigm. Thus, we study a broad class of algorithms, a Hypothesis Transfer Learning through Regularized ERM, that can be instantiated with any non-negative smooth loss function and any strongly convex regularizer. We establish generalization and excess risk bounds, showing that if the algorithm is fed with a good source hypotheses combination, generalization happens at the fast rate $\\mathcal{O}(1/m)$ instead of usual $\\mathcal{O}(1/\\sqrt{m})$. We also observe that if the combination is perfect, our theory formally backs up the intuition that learning is not necessary. On the other hand, if the source hypotheses combination is a misfit for the target task, we recover the usual learning rate. As a byproduct of our study, we also prove a new bound on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous works.", "histories": [["v1", "Thu, 4 Dec 2014 11:01:11 GMT  (44kb,D)", "http://arxiv.org/abs/1412.1619v1", null], ["v2", "Fri, 6 Feb 2015 10:32:37 GMT  (37kb,D)", "http://arxiv.org/abs/1412.1619v2", null], ["v3", "Fri, 17 Jul 2015 17:34:49 GMT  (39kb,D)", "http://arxiv.org/abs/1412.1619v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ilja kuzborskij", "francesco orabona"], "accepted": false, "id": "1412.1619"}
