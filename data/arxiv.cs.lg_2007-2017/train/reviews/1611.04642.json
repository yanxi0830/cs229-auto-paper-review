{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Modeling Large-Scale Structured Relationships with Shared Memory for Knowledge Base Completion", "abstract": "Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed instances could be costly. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform large-scale inference implicitly through a search controller and shared memory. Unlike previous work, IRNs use training data to learn to perform multi-step inference through the shared memory, which is also jointly updated during training. While the inference procedure is not operating on top of observed instances for IRNs, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7%.", "histories": [["v1", "Mon, 14 Nov 2016 22:54:45 GMT  (375kb,D)", "http://arxiv.org/abs/1611.04642v1", null], ["v2", "Sat, 22 Apr 2017 19:46:44 GMT  (1802kb,D)", "http://arxiv.org/abs/1611.04642v2", null], ["v3", "Sat, 28 Oct 2017 03:02:10 GMT  (1630kb,D)", "http://arxiv.org/abs/1611.04642v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["yelong shen", "po-sen huang", "ming-wei chang", "jianfeng gao"], "accepted": false, "id": "1611.04642"}
