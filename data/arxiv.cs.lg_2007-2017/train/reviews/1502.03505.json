{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Supervised LogEuclidean Metric Learning for Symmetric Positive Definite Matrices", "abstract": "Metric learning has been shown to be highly effective to improve the performance of nearest neighbor classification. In this paper, we address the problem of metric learning for Symmetric Positive Definite (SPD) matrices such as covariance matrices, which arise in many real-world applications. Naively using standard Mahalanobis metric learning methods under the Euclidean geometry for SPD matrices is not appropriate, because the difference of SPD matrices can be a non-SPD matrix and thus the obtained solution can be uninterpretable. To cope with this problem, we propose to use a properly parameterized LogEuclidean distance and optimize the metric with respect to kernel-target alignment, which is a supervised criterion for kernel learning. Then the resulting non-trivial optimization problem is solved by utilizing the Riemannian geometry. Finally, we experimentally demonstrate the usefulness of our LogEuclidean metric learning algorithm on real-world classification tasks for EEG signals and texture patches.", "histories": [["v1", "Thu, 12 Feb 2015 01:38:36 GMT  (1831kb,D)", "http://arxiv.org/abs/1502.03505v1", "19 pages, 6 figures, 3 tables"]], "COMMENTS": "19 pages, 6 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["florian yger", "masashi sugiyama"], "accepted": false, "id": "1502.03505"}
