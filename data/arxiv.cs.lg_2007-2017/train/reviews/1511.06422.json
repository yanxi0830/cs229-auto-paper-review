{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "All you need is a good init", "abstract": "Layer-sequential unit-variance (LSUV) initialization - a simple strategy for weight initialization for deep net learning - is proposed. The strategy proceeds from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. We show that with the strategy, learning of very deep nets via standard stochastic gradient descent is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance that is state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR, ImageNet datasets.", "histories": [["v1", "Thu, 19 Nov 2015 22:19:15 GMT  (242kb,D)", "http://arxiv.org/abs/1511.06422v1", "ICLR 2016 Submission"], ["v2", "Wed, 9 Dec 2015 14:38:33 GMT  (242kb,D)", "http://arxiv.org/abs/1511.06422v2", "ICLR 2016 Submission"], ["v3", "Mon, 11 Jan 2016 18:46:03 GMT  (765kb,D)", "http://arxiv.org/abs/1511.06422v3", "ICLR 2016 Submission"], ["v4", "Wed, 13 Jan 2016 17:47:07 GMT  (766kb,D)", "http://arxiv.org/abs/1511.06422v4", "ICLR 2016 Submission"], ["v5", "Mon, 18 Jan 2016 20:07:09 GMT  (815kb,D)", "http://arxiv.org/abs/1511.06422v5", "ICLR 2016 Submission"], ["v6", "Wed, 27 Jan 2016 15:10:19 GMT  (816kb,D)", "http://arxiv.org/abs/1511.06422v6", "ICLR 2016 Submission"], ["v7", "Fri, 19 Feb 2016 14:37:10 GMT  (816kb,D)", "http://arxiv.org/abs/1511.06422v7", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "ICLR 2016 Submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dmytro mishkin", "jiri matas"], "accepted": true, "id": "1511.06422"}
