{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "histories": [["v1", "Fri, 20 May 2016 15:05:58 GMT  (273kb,D)", "http://arxiv.org/abs/1605.06391v1", "Technical report, 14 pages, 6 figures"], ["v2", "Thu, 16 Feb 2017 20:40:41 GMT  (252kb,D)", "http://arxiv.org/abs/1605.06391v2", "9 pages, Accepted to ICLR 2017 Conference Track. This is a conference version of the paper. For the multi-domain learning part (not in this version), please refer tothis https URL"]], "COMMENTS": "Technical report, 14 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yongxin yang", "timothy hospedales"], "accepted": true, "id": "1605.06391"}
