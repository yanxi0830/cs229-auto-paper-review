{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2013", "title": "Parallel Coordinate Descent Newton Method for Efficient $\\ell_1$-Regularized Minimization", "abstract": "Parallel coordinate descent algorithms emerge with the growing demand for large-scale optimization. These algorithms are usually limited by their divergence under high parallelism or need data preprocessing to avoid divergence. In this paper, we propose a parallelized algorithm, termed as Parallel Coordinate Descent Newton (PCDN), to pursue more parallelism. It randomly partitions the feature set into $b$ subsets/bundles with size of $P$, then it sequentially processes each bundle by first computing the descent directions for each feature in the bundle in parallel and then conducting $P$-dimensional line search to obtain the stepsize of the bundle. We will show that: (1) PCDN is guaranteed to converge globally; (2) PCDN can converge to the specified accuracy $\\epsilon$ within the limited iteration number of $T_\\epsilon$, and the iteration number $T_\\epsilon$ decreases along with the increasing of parallelism (bundle size $P$). PCDN is applied to large-scale $L_1$-regularized logistic regression and $L_2$-loss SVM. Experimental evaluations over five public datasets indicate that PCDN can better exploit parallelism and outperforms state-of-the-art algorithms in speed, without losing test accuracy.", "histories": [["v1", "Tue, 18 Jun 2013 07:03:16 GMT  (2965kb)", "http://arxiv.org/abs/1306.4080v1", "25 pages, 25 figures"], ["v2", "Fri, 27 Dec 2013 08:41:37 GMT  (308kb,D)", "http://arxiv.org/abs/1306.4080v2", "28 pages, 27 figures"], ["v3", "Tue, 18 Mar 2014 14:55:49 GMT  (486kb,D)", "http://arxiv.org/abs/1306.4080v3", "30 pages, 36 figures"]], "COMMENTS": "25 pages, 25 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["yatao bian", "xiong li", "yuncai liu", "ming-hsuan yang"], "accepted": false, "id": "1306.4080"}
