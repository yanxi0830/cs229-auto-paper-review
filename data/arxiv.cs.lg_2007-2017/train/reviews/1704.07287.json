{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Joint Modeling of Text and Acoustic-Prosodic Cues for Neural Parsing", "abstract": "In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing a spoken utterance, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and word-based prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together improve parse F1 scores significantly over a strong text-only baseline. For this study with known sentence boundaries, error analysis shows that the main benefit of acoustic-prosodic features is in sentences with disfluencies and that attachment errors are most improved.", "histories": [["v1", "Mon, 24 Apr 2017 15:33:26 GMT  (1214kb,D)", "http://arxiv.org/abs/1704.07287v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["trang tran", "shubham toshniwal", "mohit bansal", "kevin gimpel", "karen livescu", "mari ostendorf"], "accepted": false, "id": "1704.07287"}
