{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Do Deep Nets Really Need to be Deep?", "abstract": "Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this extended abstract, we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model. We evaluate our method on TIMIT phoneme recognition task and are able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures. Our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feed-forward nets than those currently available.", "histories": [["v1", "Sat, 21 Dec 2013 00:47:43 GMT  (21kb)", "http://arxiv.org/abs/1312.6184v1", null], ["v2", "Fri, 3 Jan 2014 03:32:10 GMT  (12kb)", "http://arxiv.org/abs/1312.6184v2", "updated the discussion and the conclusion sections"], ["v3", "Mon, 6 Jan 2014 20:49:04 GMT  (12kb)", "http://arxiv.org/abs/1312.6184v3", "updated author info"], ["v4", "Wed, 8 Jan 2014 17:34:30 GMT  (12kb)", "http://arxiv.org/abs/1312.6184v4", "minor revision"], ["v5", "Fri, 21 Feb 2014 20:04:00 GMT  (13kb)", "http://arxiv.org/abs/1312.6184v5", "revision"], ["v6", "Tue, 7 Oct 2014 21:12:27 GMT  (13kb)", "http://arxiv.org/abs/1312.6184v6", "final revision coming soon"], ["v7", "Sat, 11 Oct 2014 00:19:10 GMT  (67kb,D)", "http://arxiv.org/abs/1312.6184v7", "final revision coming soon"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["jimmy ba", "rich caruana"], "accepted": true, "id": "1312.6184"}
