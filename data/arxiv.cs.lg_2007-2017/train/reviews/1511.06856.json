{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Data-dependent Initializations of Convolutional Neural Networks", "abstract": "Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.", "histories": [["v1", "Sat, 21 Nov 2015 09:07:08 GMT  (1809kb,D)", "http://arxiv.org/abs/1511.06856v1", "12 pages, Under review at ICLR 2016"], ["v2", "Fri, 29 Apr 2016 03:36:16 GMT  (1960kb,D)", "http://arxiv.org/abs/1511.06856v2", "ICLR 2016"], ["v3", "Thu, 22 Sep 2016 22:14:17 GMT  (1951kb,D)", "http://arxiv.org/abs/1511.06856v3", "ICLR 2016"]], "COMMENTS": "12 pages, Under review at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["philipp kr\\\"ahenb\\\"uhl", "carl doersch", "jeff donahue", "trevor darrell"], "accepted": true, "id": "1511.06856"}
