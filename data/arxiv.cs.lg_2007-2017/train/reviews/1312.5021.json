{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2013", "title": "Efficient Online Bootstrapping for Large Scale Learning", "abstract": "Bootstrapping is a useful technique for estimating the uncertainty of a predictor, for example, confidence intervals for prediction. It is typically used on small to moderate sized datasets, due to its high computation cost. This work describes a highly scalable online bootstrapping strategy, implemented inside Vowpal Wabbit, that is several times faster than traditional strategies. Our experiments indicate that, in addition to providing a black box-like method for estimating uncertainty, our implementation of online bootstrapping may also help to train models with better prediction performance due to model averaging.", "histories": [["v1", "Wed, 18 Dec 2013 02:10:21 GMT  (87kb,D)", "http://arxiv.org/abs/1312.5021v1", "5 pages, appeared at Big Learning Workshop at Neural Information Processing Systems 2013"]], "COMMENTS": "5 pages, appeared at Big Learning Workshop at Neural Information Processing Systems 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhen qin", "vaclav petricek", "nikos karampatziakis", "lihong li", "john langford"], "accepted": false, "id": "1312.5021"}
