{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Variational Inference for Monte Carlo Objectives", "abstract": "Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2015) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multi-sample objectives and explained the success of several related approaches.", "histories": [["v1", "Mon, 22 Feb 2016 11:06:06 GMT  (189kb,D)", "http://arxiv.org/abs/1602.06725v1", null], ["v2", "Wed, 1 Jun 2016 16:36:06 GMT  (198kb,D)", "http://arxiv.org/abs/1602.06725v2", "Appears in Proceedings of the 33rd International Conference on Machine Learning (ICML), New York, NY, USA, 2016. JMLR: W&amp;CP volume 48"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["andriy mnih", "danilo jimenez rezende"], "accepted": true, "id": "1602.06725"}
