{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Stabilizing GAN Training with Multiple Random Projections", "abstract": "Training generative adversarial networks is unstable in high-dimensions when the true data distribution lies on a lower-dimensional manifold. The discriminator is then easily able to separate nearly all generated samples leaving the generator without meaningful gradients. We propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. We show that individual discriminators then provide stable gradients to the generator, and that the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.", "histories": [["v1", "Mon, 22 May 2017 16:23:26 GMT  (5559kb,D)", "http://arxiv.org/abs/1705.07831v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["behnam neyshabur", "srinadh bhojanapalli", "ayan chakrabarti"], "accepted": false, "id": "1705.07831"}
