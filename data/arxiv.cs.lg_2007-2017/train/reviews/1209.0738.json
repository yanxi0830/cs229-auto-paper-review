{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2012", "title": "Sparse coding for multitask and transfer learning", "abstract": "We present an extension of sparse coding to the problems of multitask and transfer learning. The central assumption of the method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Preliminary experiments indicate the advantage of the sparse multitask coding method over single task learning and a previous method based on orthogonal and dense representation of the tasks.", "histories": [["v1", "Tue, 4 Sep 2012 19:06:51 GMT  (54kb)", "https://arxiv.org/abs/1209.0738v1", "25 pages, 5 figures"], ["v2", "Sat, 23 Mar 2013 19:35:27 GMT  (66kb)", "http://arxiv.org/abs/1209.0738v2", null], ["v3", "Mon, 16 Jun 2014 15:06:48 GMT  (55kb)", "http://arxiv.org/abs/1209.0738v3", "International Conference on Machine Learning 2013"]], "COMMENTS": "25 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["andreas maurer", "massimiliano pontil", "bernardino romera-paredes"], "accepted": true, "id": "1209.0738"}
