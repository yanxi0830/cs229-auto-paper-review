{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Reinforced backpropagation for deep neural network learning", "abstract": "Standard error backpropagation is used in almost all modern deep network training. However, it typically suffers from proliferation of saddle points in high-dimensional parameter space. Therefore, it is highly desirable to design an efficient algorithm to escape from these saddle points and reach a good parameter region of better generalization capabilities, especially based on rough insights about the landscape of the error surface. Here, we propose a simple extension of the backpropagation, namely reinforced backpropagation, which simply adds previous first-order gradients in a stochastic manner with a probability that increases with learning time. Extensive numerical simulations on a toy deep learning model verify its excellent performance. The reinforced backpropagation can significantly improve test performance of the deep network training, especially when the data are scarce. The performance is even better than that of state-of-the-art stochastic optimization algorithm called Adam, with an extra advantage of less computer memory required.", "histories": [["v1", "Fri, 27 Jan 2017 08:49:19 GMT  (89kb,D)", "https://arxiv.org/abs/1701.07974v1", "7 pages and 5 figures"], ["v2", "Tue, 7 Mar 2017 05:34:08 GMT  (626kb,D)", "http://arxiv.org/abs/1701.07974v2", "9 pages and 7 figures, results added, learning rate typo corrected"], ["v3", "Wed, 24 May 2017 09:30:54 GMT  (663kb,D)", "http://arxiv.org/abs/1701.07974v3", "10 pages and 8 figures, results added"], ["v4", "Tue, 19 Sep 2017 02:57:24 GMT  (674kb,D)", "http://arxiv.org/abs/1701.07974v4", "12 pages and 9 figures, extensively revised"]], "COMMENTS": "7 pages and 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["haiping huang", "taro toyoizumi"], "accepted": false, "id": "1701.07974"}
