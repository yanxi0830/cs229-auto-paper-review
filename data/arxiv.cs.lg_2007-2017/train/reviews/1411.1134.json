{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2014", "title": "Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems", "abstract": "The Burer-Monteiro decomposition ($X = Y Y^T$) with stochastic gradient descent is commonly employed to speed up and scale up matrix problems including matrix completion, subspace tracking, and SDP relaxation. Although it is widely used in practice, there exist no known global convergence results for this method. In this paper, we prove that, under broad sampling conditions, a first-order rank-1 stochastic gradient descent (SGD) matrix recovery scheme converges globally from a random starting point at a $O(\\epsilon^{-1} n \\log n)$ rate with constant probability. We demonstrate our method experimentally.", "histories": [["v1", "Wed, 5 Nov 2014 03:05:43 GMT  (48kb)", "https://arxiv.org/abs/1411.1134v1", null], ["v2", "Thu, 6 Nov 2014 03:10:29 GMT  (48kb)", "http://arxiv.org/abs/1411.1134v2", null], ["v3", "Tue, 10 Feb 2015 20:19:28 GMT  (67kb)", "http://arxiv.org/abs/1411.1134v3", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["christopher de sa", "christopher r\u00e9", "kunle olukotun"], "accepted": true, "id": "1411.1134"}
