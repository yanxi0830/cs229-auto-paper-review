{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2013", "title": "Revisiting the Nystrom method for improved large-scale machine learning", "abstract": "We reconsider randomized algorithms for the low-rank approximation of symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods based on leverage scores. We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projections methods. These bounds are qualitatively superior to existing bounds---e.g., improved additive-error bounds for the spectral and Frobenius norm errors and relative-error bounds for the trace norm error.", "histories": [["v1", "Thu, 7 Mar 2013 23:16:16 GMT  (1195kb,D)", "http://arxiv.org/abs/1303.1849v1", "45 pages, 14 color figures"], ["v2", "Mon, 3 Jun 2013 20:07:19 GMT  (2649kb,D)", "http://arxiv.org/abs/1303.1849v2", "60 pages, 15 color figures; updated proof of Frobenius norm bounds, added comparison to projection-based low-rank approximations, and an analysis of the power method applied to SPSD sketches"]], "COMMENTS": "45 pages, 14 color figures", "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.NA", "authors": ["alex gittens", "michael w mahoney"], "accepted": true, "id": "1303.1849"}
