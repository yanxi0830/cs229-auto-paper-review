{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2012", "title": "Scaling Datalog for Machine Learning on Big Data", "abstract": "In this paper, we present the case for a declarative foundation for data-intensive machine learning systems. Instead of creating a new system for each specific flavor of machine learning task, or hardcoding new optimizations, we argue for the use of recursive queries to program a variety of machine learning systems. By taking this approach, database query optimization techniques can be utilized to identify effective execution plans, and the resulting runtime plans can be executed on a single unified data-parallel query processing engine. As a proof of concept, we consider two programming models--Pregel and Iterative Map-Reduce-Update---from the machine learning domain, and show how they can be captured in Datalog, tuned for a specific task, and then compiled into an optimized physical plan. Experiments performed on a large computing cluster with real data demonstrate that this declarative approach can provide very good performance while offering both increased generality and programming ease.", "histories": [["v1", "Thu, 1 Mar 2012 11:43:43 GMT  (1632kb,D)", "http://arxiv.org/abs/1203.0160v1", null], ["v2", "Fri, 2 Mar 2012 10:14:58 GMT  (1296kb,D)", "http://arxiv.org/abs/1203.0160v2", null]], "reviews": [], "SUBJECTS": "cs.DB cs.LG cs.PF", "authors": ["yingyi bu", "vinayak borkar", "michael j carey", "joshua rosen", "neoklis polyzotis", "tyson condie", "markus weimer", "raghu ramakrishnan"], "accepted": false, "id": "1203.0160"}
