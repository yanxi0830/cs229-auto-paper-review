{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2015", "title": "Skip-Thought Vectors", "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.", "histories": [["v1", "Mon, 22 Jun 2015 19:33:40 GMT  (1199kb,D)", "http://arxiv.org/abs/1506.06726v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ryan kiros", "yukun zhu", "ruslan salakhutdinov", "richard s zemel", "raquel urtasun", "antonio torralba 0001", "sanja fidler"], "accepted": true, "id": "1506.06726"}
