{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Tight Complexity Bounds for Optimizing Composite Objectives", "abstract": "We provide tight upper and lower bounds on the complexity of minimizing the average of m convex functions using gradient and prox oracles of the component functions. We show a significant gap between the complexity of deterministic vs randomized optimization. For smooth functions, we show that accelerated gradient descent (AGD) and Katyusha are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. For non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing AGD that improve over methods using just gradient accesses.", "histories": [["v1", "Wed, 25 May 2016 18:44:54 GMT  (33kb)", "https://arxiv.org/abs/1605.08003v1", null], ["v2", "Thu, 27 Oct 2016 18:32:55 GMT  (35kb)", "http://arxiv.org/abs/1605.08003v2", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["blake e woodworth", "nati srebro"], "accepted": true, "id": "1605.08003"}
