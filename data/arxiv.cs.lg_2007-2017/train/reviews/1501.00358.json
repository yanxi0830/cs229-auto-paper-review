{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jan-2015", "title": "Comprehend DeepWalk as Matrix Factorization", "abstract": "Word2vec, as an efficient tool for learning vector representation of words has shown its effectiveness in many natural language processing tasks. Mikolov et al. issued Skip-Gram and Negative Sampling model for developing this toolbox. Perozzi et al. introduced the Skip-Gram model into the study of social network for the first time, and designed an algorithm named DeepWalk for learning node embedding on a graph. We prove that the DeepWalk algorithm is actually factoring a matrix M where each entry M_{ij} is logarithm of the average probability that node i randomly walks to node j in fix steps.", "histories": [["v1", "Fri, 2 Jan 2015 07:57:14 GMT  (4kb)", "http://arxiv.org/abs/1501.00358v1", "4 pages"]], "COMMENTS": "4 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cheng yang", "zhiyuan liu"], "accepted": false, "id": "1501.00358"}
