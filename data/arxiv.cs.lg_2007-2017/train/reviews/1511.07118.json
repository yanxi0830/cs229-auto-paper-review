{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Cascading Denoising Auto-Encoder as a Deep Directed Generative Model", "abstract": "Recent work (Bengio et al., 2013) has shown howDenoising Auto-Encoders(DAE) become gener-ative models as a density estimator. However,in practice, the framework suffers from a mixingproblem in the MCMC sampling process and nodirect method to estimate the test log-likelihood.We consider a directed model with an stochas-tic identity mapping (simple corruption pro-cess) as an inference model and a DAE as agenerative model. By cascading these mod-els, we propose Cascading Denoising Auto-Encoders(CDAE) which can generate samples ofdata distribution from tractable prior distributionunder the assumption that probabilistic distribu-tion of corrupted data approaches tractable priordistribution as the level of corruption increases.This work tries to answer two questions. On theone hand, can deep directed models be success-fully trained without intractable posterior infer-ence and difficult optimization of very deep neu-ral networks in inference and generative mod-els? These are unavoidable when recent suc-cessful directed model like VAE (Kingma &amp;Welling, 2014) is trained on complex dataset likereal images. On the other hand, can DAEs getclean samples of data distribution from heavilycorrupted samples which can be considered oftractable prior distribution far from data mani-fold? so-called global denoising scheme.Our results show positive responses of thesequestions and this work can provide fairly simpleframework for generative models of very com-plex dataset.", "histories": [["v1", "Mon, 23 Nov 2015 06:32:57 GMT  (39kb)", "http://arxiv.org/abs/1511.07118v1", null], ["v2", "Fri, 27 Jan 2017 19:09:52 GMT  (0kb,I)", "http://arxiv.org/abs/1511.07118v2", "not completed"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dong-hyun lee"], "accepted": false, "id": "1511.07118"}
