{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "Training Language Models Using Target-Propagation", "abstract": "While Truncated Back-Propagation through Time (BPTT) is the most popular approach to training Recurrent Neural Networks (RNNs), it suffers from being inherently sequential (making parallelization difficult) and from truncating gradient flow between distant time-steps. We investigate whether Target Propagation (TPROP) style approaches can address these shortcomings. Unfortunately, extensive experiments suggest that TPROP generally underperforms BPTT, and we end with an analysis of this phenomenon, and suggestions for future work.", "histories": [["v1", "Wed, 15 Feb 2017 20:56:30 GMT  (1677kb,D)", "http://arxiv.org/abs/1702.04770v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["sam wiseman", "sumit chopra", "marc'aurelio ranzato", "arthur szlam", "ruoyu sun", "soumith chintala", "nicolas vasilache"], "accepted": false, "id": "1702.04770"}
