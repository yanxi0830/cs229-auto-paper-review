{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2016", "title": "Bounded Optimal Exploration in MDP", "abstract": "Within the framework of probably approximately correct Markov decision processes (PAC-MDP), much theoretical work has focused on methods to attain near optimality after a relatively long period of learning and exploration. However, practical concerns require the attainment of satisfactory behavior within a short period of time. In this paper, we relax the PAC-MDP conditions to reconcile theoretically driven exploration methods and practical needs. We propose simple algorithms for discrete and continuous state spaces, and illustrate the benefits of our proposed relaxation via theoretical analyses and numerical examples. Our algorithms also maintain anytime error bounds and average loss bounds. Our approach accommodates both Bayesian and non-Bayesian methods.", "histories": [["v1", "Tue, 5 Apr 2016 18:00:02 GMT  (1762kb,D)", "http://arxiv.org/abs/1604.01350v1", "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI), 2016"]], "COMMENTS": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI), 2016", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["kenji kawaguchi"], "accepted": true, "id": "1604.01350"}
