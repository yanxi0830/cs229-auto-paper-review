{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jul-2013", "title": "Fast Gradient Descent for Drifting Least Squares Regression, with Application to Bandits", "abstract": "We improve the computational complexity of online learning algorithms that require to often recompute least squares regression estimates of parameters. We propose two stochastic gradient descent schemes with randomisation in order to efficiently track the true solutions of the regression problems achieving an O(d) improvement in complexity, where d is the dimension of the data. The first algorithm assumes strong convexity in the regression problem, and we provide bounds on the error both in expectation and high probability (the latter is often needed to provide theoretical guarantees for higher level algorithms). The second algorithm deals with cases where strong convexity of the regression problem cannot be guaranteed and uses adaptive regularisation. We again give error bounds in both expectation and high probability. We apply our approaches to the linear bandit algorithms PEGE and ConfidenceBall and demonstrate significant gains in complexity in both cases. Since strong convexity is guaranteed by the PEGE algorithm, we lose only logarithmic factors in the regret performance of the algorithm. On the other hand, in the ConfidenceBall algorithm we adaptively regularise to ensure strong convexity, and this results in an O(n^{1/5}) deterioration of the regret.", "histories": [["v1", "Thu, 11 Jul 2013 16:36:29 GMT  (27kb)", "http://arxiv.org/abs/1307.3176v1", null], ["v2", "Wed, 19 Feb 2014 00:27:18 GMT  (1545kb,D)", "http://arxiv.org/abs/1307.3176v2", null], ["v3", "Thu, 24 Jul 2014 14:29:52 GMT  (3757kb,D)", "http://arxiv.org/abs/1307.3176v3", null], ["v4", "Thu, 20 Nov 2014 12:40:48 GMT  (104kb,D)", "http://arxiv.org/abs/1307.3176v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nathaniel korda", "prashanth l a", "r\u00e9mi munos"], "accepted": true, "id": "1307.3176"}
