{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Bayesian Neural Word Embedding", "abstract": "Recently, several works in the domain of natural language processing presented successful methods for word embedding. Among them, the Skip-gram (SG) with negative sampling, known also as Word2Vec, advanced the state-of-the-art of various linguistics tasks. In this paper, we propose a scalable Bayesian neural word embedding algorithm that can be beneficial to general item similarity tasks as well. The algorithm relies on a Variational Bayes solution for the SG objective and a detailed step by step description of the algorithm is provided. We present experimental results that demonstrate the performance of the proposed algorithm and show it is competitive with the original SG method.", "histories": [["v1", "Mon, 21 Mar 2016 16:32:06 GMT  (145kb)", "http://arxiv.org/abs/1603.06571v1", null], ["v2", "Sun, 5 Jun 2016 16:49:11 GMT  (156kb)", "http://arxiv.org/abs/1603.06571v2", null], ["v3", "Mon, 20 Feb 2017 20:45:33 GMT  (181kb)", "http://arxiv.org/abs/1603.06571v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["oren barkan"], "accepted": true, "id": "1603.06571"}
