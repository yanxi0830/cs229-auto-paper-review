{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Gradients of Counterfactuals", "abstract": "Gradients have been used to quantify feature importance in machine learning models. Unfortunately, in nonlinear deep networks, not only individual neurons but also the whole network can saturate, and as a result an important input feature can have a tiny gradient. We study various networks, and observe that this phenomena is indeed widespread, across many inputs.", "histories": [["v1", "Tue, 8 Nov 2016 18:10:44 GMT  (2193kb,D)", "http://arxiv.org/abs/1611.02639v1", null], ["v2", "Tue, 15 Nov 2016 19:55:26 GMT  (2193kb,D)", "http://arxiv.org/abs/1611.02639v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["mukund sundararajan", "ankur taly", "qiqi yan"], "accepted": false, "id": "1611.02639"}
