{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "Complex Decomposition of the Negative Distance kernel", "abstract": "A Support Vector Machine (SVM) has become a very popular machine learning method for text classification. One reason for this relates to the range of existing kernels which allow for classifying data that is not linearly separable. The linear, polynomial and RBF (Gaussian Radial Basis Function) kernel are commonly used and serve as a basis of comparison in our study. We show how to derive the primal form of the quadratic Power Kernel (PK) -- also called the Negative Euclidean Distance Kernel (NDK) -- by means of complex numbers. We exemplify the NDK in the framework of text categorization using the Dewey Document Classification (DDC) as the target scheme. Our evaluation shows that the power kernel produces F-scores that are comparable to the reference kernels, but is -- except for the linear kernel -- faster to compute. Finally, we show how to extend the NDK-approach by including the Mahalanobis distance.", "histories": [["v1", "Tue, 5 Jan 2016 18:16:07 GMT  (71kb)", "http://arxiv.org/abs/1601.00925v1", "Proceedings of the IEEE International Conference on Machine Learning an Applications, Miami, Florida, 2015"]], "COMMENTS": "Proceedings of the IEEE International Conference on Machine Learning an Applications, Miami, Florida, 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tim vor der br\\\"uck", "steffen eger", "alexander mehler"], "accepted": false, "id": "1601.00925"}
