{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "Incremental Learning Through Deep Adaptation", "abstract": "Given an existing trained neural network, it is often desirable to be able to add new capabilities without hindering performance of already learned tasks. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method which fully preserves performance on the original task, with only a small increase (around 20%) in the number of required parameters while performing on par with more costly fine-tuning procedures, which typically double the number of parameters. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method and explore different aspects of its behavior.", "histories": [["v1", "Thu, 11 May 2017 15:04:10 GMT  (554kb,D)", "http://arxiv.org/abs/1705.04228v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["amir rosenfeld", "john k tsotsos"], "accepted": false, "id": "1705.04228"}
