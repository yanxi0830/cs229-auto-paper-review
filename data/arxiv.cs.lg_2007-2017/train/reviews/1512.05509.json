{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2015", "title": "An Empirical Comparison of Neural Architectures for Reinforcement Learning in Partially Observable Environments", "abstract": "This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long Short-Term Memory, Gated Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures. A variant of fitted Q iteration, based on Advantage values instead of Q values, is also explored. The results show that GRU performs significantly better than LSTM and MUT1 for most of the problems considered, requiring less training episodes and less CPU time before learning a very good policy. Advantage learning also tends to produce better results.", "histories": [["v1", "Thu, 17 Dec 2015 09:45:51 GMT  (120kb,D)", "http://arxiv.org/abs/1512.05509v1", "Presented at the 27th Benelux Conference on Artificial Intelligence"]], "COMMENTS": "Presented at the 27th Benelux Conference on Artificial Intelligence", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["denis steckelmacher", "peter vrancx"], "accepted": false, "id": "1512.05509"}
