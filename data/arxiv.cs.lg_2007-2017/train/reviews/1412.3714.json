{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2014", "title": "Feature Weight Tuning for Recursive Neural Networks", "abstract": "This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform \"weight tuning\" for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.", "histories": [["v1", "Thu, 11 Dec 2014 16:35:27 GMT  (207kb,D)", "http://arxiv.org/abs/1412.3714v1", null], ["v2", "Sat, 13 Dec 2014 00:57:57 GMT  (207kb,D)", "http://arxiv.org/abs/1412.3714v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL cs.LG", "authors": ["jiwei li"], "accepted": false, "id": "1412.3714"}
