{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2016", "title": "Variance Reduction for Faster Non-Convex Optimization", "abstract": "We consider the fundamental problem in non-convex optimization of efficiently reaching a stationary point. In contrast to the convex case, in the long history of this basic problem, the only known theoretical results on first-order non-convex optimization remain to be full gradient descent that converges in $O(1/\\varepsilon)$ iterations for smooth objectives, and stochastic gradient descent that converges in $O(1/\\varepsilon^2)$ iterations for objectives that are sum of smooth functions.", "histories": [["v1", "Thu, 17 Mar 2016 19:55:12 GMT  (4239kb,D)", "https://arxiv.org/abs/1603.05643v1", null], ["v2", "Thu, 25 Aug 2016 02:34:00 GMT  (4254kb,D)", "http://arxiv.org/abs/1603.05643v2", "polished writing"]], "reviews": [], "SUBJECTS": "math.OC cs.DS cs.LG cs.NE stat.ML", "authors": ["zeyuan allen zhu", "elad hazan"], "accepted": true, "id": "1603.05643"}
