{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Neural Machine Translation in Linear Time", "abstract": "We present a neural architecture for sequence processing. The ByteNet is a stack of two dilated convolutional neural networks, one to encode the source sequence and one to decode the target sequence, where the target network unfolds dynamically to generate variable length outputs. The ByteNet has two core properties: it runs in time that is linear in the length of the sequences and it preserves the sequences' temporal resolution. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent neural networks. The ByteNet also achieves a performance on raw character-level machine translation that approaches that of the best neural translation models that run in quadratic time. The implicit structure learnt by the ByteNet mirrors the expected alignments between the sequences.", "histories": [["v1", "Mon, 31 Oct 2016 19:56:39 GMT  (5893kb,D)", "http://arxiv.org/abs/1610.10099v1", "11 pages"], ["v2", "Wed, 15 Mar 2017 18:09:51 GMT  (12174kb,D)", "http://arxiv.org/abs/1610.10099v2", "9 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["nal kalchbrenner", "lasse espeholt", "karen simonyan", "aaron van den oord", "alex graves", "koray kavukcuoglu"], "accepted": false, "id": "1610.10099"}
