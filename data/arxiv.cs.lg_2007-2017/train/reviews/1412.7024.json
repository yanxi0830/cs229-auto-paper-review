{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Training deep neural networks with low precision multiplications", "abstract": "We simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those arithmetics, we assess the impact of the precision of the computations on the final error of the training. We find that very low precision computation is sufficient not just for running trained networks but also for training them. For example, almost state-of-the-art results were obtained on most datasets with around 10 bits for computing activations and gradients, and 12 bits for storing updated parameters.", "histories": [["v1", "Mon, 22 Dec 2014 15:22:45 GMT  (183kb,D)", "http://arxiv.org/abs/1412.7024v1", "9 pages, 4 figures, ICLR 2015"], ["v2", "Thu, 25 Dec 2014 18:05:12 GMT  (183kb,D)", "http://arxiv.org/abs/1412.7024v2", "Few more details on our dynamic fixed point implementation compared with the previous version"], ["v3", "Thu, 26 Feb 2015 00:26:12 GMT  (216kb,D)", "http://arxiv.org/abs/1412.7024v3", "9 pages, 4 figures, under review as conference paper at ICRL 2015"], ["v4", "Fri, 3 Apr 2015 22:52:43 GMT  (216kb,D)", "http://arxiv.org/abs/1412.7024v4", "9 pages, 4 figures, Accepted as a workshop contribution at ICLR 2015"], ["v5", "Wed, 23 Sep 2015 01:00:44 GMT  (347kb,D)", "http://arxiv.org/abs/1412.7024v5", "10 pages, 5 figures, Accepted as a workshop contribution at ICLR 2015"]], "COMMENTS": "9 pages, 4 figures, ICLR 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["matthieu courbariaux", "yoshua bengio", "jean-pierre david"], "accepted": false, "id": "1412.7024"}
