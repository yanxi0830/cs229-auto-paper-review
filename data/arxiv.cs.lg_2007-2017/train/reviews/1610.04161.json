{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Why Deep Neural Networks for Function Approximation?", "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. In this paper, we show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on this simple observation that the binary approximation of a real number in the interval $[0,1]$ can be represented by a deep neural network which uses a \"small\" number of neurons.", "histories": [["v1", "Thu, 13 Oct 2016 16:34:30 GMT  (340kb,D)", "http://arxiv.org/abs/1610.04161v1", null], ["v2", "Fri, 3 Mar 2017 20:43:04 GMT  (628kb,D)", "http://arxiv.org/abs/1610.04161v2", "The paper is published at the 5th International Conference on Learning Representations (ICLR)"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["shiyu liang", "r srikant"], "accepted": true, "id": "1610.04161"}
