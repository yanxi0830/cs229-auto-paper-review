{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2013", "title": "Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation", "abstract": "Many machine learning approaches are characterized by information constraints on how they interact with the training data. These include memory and sequential access constraints (e.g. fast first-order methods to solve stochastic optimization problems); communication constraints (e.g. distributed learning); partial access to the underlying data (e.g. missing features and multi-armed bandits) and more. However, currently we have little understanding how such information constraints fundamentally affect our performance, independent of the learning problem semantics. For example, are there learning problems where \\emph{any} algorithm which has small memory footprint (or can use any bounded number of bits from each example, or has certain communication constraints) will perform worse than what is possible without such constraints? In this paper, we describe how a single set of results implies positive answers to the above, for a variety of settings.", "histories": [["v1", "Thu, 14 Nov 2013 13:21:15 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v1", null], ["v2", "Wed, 5 Feb 2014 14:55:06 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v2", null], ["v3", "Thu, 6 Feb 2014 06:23:28 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v3", null], ["v4", "Tue, 6 May 2014 10:56:31 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v4", "New and improved results compared to previous version"], ["v5", "Wed, 21 May 2014 18:35:13 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v5", null], ["v6", "Tue, 28 Oct 2014 13:25:09 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v6", "Full version of NIPS 2014 paper"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ohad shamir"], "accepted": true, "id": "1311.3494"}
