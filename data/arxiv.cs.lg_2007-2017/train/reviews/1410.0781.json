{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2014", "title": "SimNets: A Generalization of Convolutional Networks", "abstract": "We present a deep layered architecture that generalizes classical convolutional neural networks (ConvNets). The architecture, called SimNets, is driven by two operators, one being a similarity function whose family contains the convolution operator used in ConvNets, and the other is a new 'soft max-min-mean' operator called MMECS that realizes classical operators like ReLU and max-pooling, but has additional capabilities that make SimNets a powerful generalization of ConvNets. Two interesting properties that emerge from the architecture are: (i) the basic input to hidden-units to output-nodes machinery contains as special case a kernel machine, and (ii) initializing networks using unsupervised learning is natural. Experiments demonstrate the capability of achieving state of the art accuracy with networks that are 1/8 the size of comparable ConvNets.", "histories": [["v1", "Fri, 3 Oct 2014 08:47:03 GMT  (628kb,D)", "https://arxiv.org/abs/1410.0781v1", null], ["v2", "Sat, 25 Oct 2014 09:47:07 GMT  (644kb,D)", "http://arxiv.org/abs/1410.0781v2", null], ["v3", "Sun, 7 Dec 2014 15:51:28 GMT  (648kb,D)", "http://arxiv.org/abs/1410.0781v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["nadav cohen", "amnon shashua"], "accepted": false, "id": "1410.0781"}
