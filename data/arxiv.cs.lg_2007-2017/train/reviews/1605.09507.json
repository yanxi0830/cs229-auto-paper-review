{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Deep convolutional neural networks for predominant instrument recognition in polyphonic music", "abstract": "Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In doing so, we investigated two different aggregation methods: one takes the average for each instrument and the other takes the instrument-wise sum followed by normalization. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for neural networks to find the optimal set of parameters. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.602 for micro and 0.503 for macro, respectively, achieving 19.6% and 16.4% in performance improvement compared with other state-of-the-art algorithms.", "histories": [["v1", "Tue, 31 May 2016 07:11:18 GMT  (1695kb,D)", "http://arxiv.org/abs/1605.09507v1", "13 pages, 7 figures, submitted to IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING on 17-May-2016"], ["v2", "Fri, 18 Nov 2016 08:54:57 GMT  (1693kb,D)", "http://arxiv.org/abs/1605.09507v2", "13 pages, 7 figures, accepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing on 16-Nov-2016"], ["v3", "Mon, 26 Dec 2016 12:29:26 GMT  (1695kb,D)", "http://arxiv.org/abs/1605.09507v3", "13 pages, 7 figures, accepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing on 16-Nov-2016. This is initial submission version. Fully edited version is available atthis http URL"]], "COMMENTS": "13 pages, 7 figures, submitted to IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING on 17-May-2016", "reviews": [], "SUBJECTS": "cs.SD cs.CV cs.LG cs.NE", "authors": ["yoonchang han", "jaehun kim", "kyogu lee"], "accepted": false, "id": "1605.09507"}
