{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2016", "title": "Optimal Learning for Multi-pass Stochastic Gradient Methods", "abstract": "We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases.", "histories": [["v1", "Sat, 28 May 2016 12:11:22 GMT  (76kb,D)", "http://arxiv.org/abs/1605.08882v1", "31 pages, 6 figures"], ["v2", "Sat, 21 Oct 2017 22:55:48 GMT  (91kb,D)", "http://arxiv.org/abs/1605.08882v2", "Extended versions of the previous one. Fixed some typos, JMLR, 2017"]], "COMMENTS": "31 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["junhong lin", "lorenzo rosasco"], "accepted": true, "id": "1605.08882"}
