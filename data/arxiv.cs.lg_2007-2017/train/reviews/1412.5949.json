{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "Large Scale Distributed Distance Metric Learning", "abstract": "In large scale machine learning and data mining problems with high feature dimensionality, the Euclidean distance between data points can be uninformative, and Distance Metric Learning (DML) is often desired to learn a proper similarity measure (using side information such as example data pairs being similar or dissimilar). However, high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al. (2002) and later extensions. In this paper, we present a distributed algorithm for DML, and a large-scale implementation on a parameter server architecture. Our approach builds on a parallelizable reformulation of Xing et al. (2002), and an asynchronous stochastic gradient descent optimization procedure. To our knowledge, this is the first distributed solution to DML, and we show that, on a system with 256 CPU cores, our program is able to complete a DML task on a dataset with 1 million data points, 22-thousand features, and 200 million labeled data pairs, in 15 hours; and the learned metric shows great effectiveness in properly measuring distances.", "histories": [["v1", "Thu, 18 Dec 2014 17:14:34 GMT  (638kb,D)", "http://arxiv.org/abs/1412.5949v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pengtao xie", "eric xing"], "accepted": false, "id": "1412.5949"}
