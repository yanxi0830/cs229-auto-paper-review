{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions", "abstract": "Many deep Convolutional Neural Networks (CNN) make incorrect predictions on adversarial samples obtained by imperceptible perturbations of clean samples. We hypothesize that this is caused by a failure to suppress unusual signals within network layers. As remedy we propose the use of Symmetric Activation Functions (SAF) in non-linear signal transducer units. These units suppress signals of exceptional magnitude. We prove that SAF networks can perform classification tasks to arbitrary precision in a simplified situation. In practice, rather than use SAFs alone, we add them into CNNs to improve their robustness. The modified CNNs can be easily trained using popular strategies with the moderate training load. Our experiments on MNIST and CIFAR-10 show that the modified CNNs perform similarly to plain ones on clean samples, and are remarkably more robust against adversarial and nonsense samples.", "histories": [["v1", "Wed, 16 Mar 2016 15:35:07 GMT  (1440kb,D)", "http://arxiv.org/abs/1603.05145v1", "11 pages, 12 figures"]], "COMMENTS": "11 pages, 12 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["qiyang zhao", "lewis d griffin"], "accepted": false, "id": "1603.05145"}
