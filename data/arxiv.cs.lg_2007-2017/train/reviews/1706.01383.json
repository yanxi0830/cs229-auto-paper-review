{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Sparse Stochastic Bandits", "abstract": "In the classical multi-armed bandit problem, d arms are available to the decision maker who pulls them sequentially in order to maximize his cumulative reward. Guarantees can be obtained on a relative quantity called regret, which scales linearly with d (or with sqrt(d) in the minimax sense). We here consider the sparse case of this classical problem in the sense that only a small number of arms, namely s &lt; d, have a positive expected reward. We are able to leverage this additional assumption to provide an algorithm whose regret scales with s instead of d. Moreover, we prove that this algorithm is optimal by providing a matching lower bound - at least for a wide and pertinent range of parameters that we determine - and by evaluating its performance on simulated data.", "histories": [["v1", "Mon, 5 Jun 2017 15:46:52 GMT  (2815kb,D)", "http://arxiv.org/abs/1706.01383v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joon kwon", "vianney perchet", "claire vernade"], "accepted": false, "id": "1706.01383"}
