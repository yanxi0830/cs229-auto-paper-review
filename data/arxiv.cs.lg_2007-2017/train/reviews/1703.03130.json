{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2017", "title": "A Structured Self-attentive Sentence Embedding", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "histories": [["v1", "Thu, 9 Mar 2017 04:42:30 GMT  (4058kb,D)", "http://arxiv.org/abs/1703.03130v1", "15 pages with appendix, 7 figures, 4 tables. Conference paper in 5th International Conference on Learning Representations (ICLR 2017)"]], "COMMENTS": "15 pages with appendix, 7 figures, 4 tables. Conference paper in 5th International Conference on Learning Representations (ICLR 2017)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["zhouhan lin", "minwei feng", "cicero nogueira dos santos", "mo yu", "bing xiang", "bowen zhou", "yoshua bengio"], "accepted": true, "id": "1703.03130"}
