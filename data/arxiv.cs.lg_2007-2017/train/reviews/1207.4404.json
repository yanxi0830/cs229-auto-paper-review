{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2012", "title": "Better Mixing via Deep Representations", "abstract": "It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation. To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples.", "histories": [["v1", "Wed, 18 Jul 2012 16:07:36 GMT  (561kb,D)", "http://arxiv.org/abs/1207.4404v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "gr\u00e9goire mesnil", "yann dauphin", "salah rifai"], "accepted": true, "id": "1207.4404"}
