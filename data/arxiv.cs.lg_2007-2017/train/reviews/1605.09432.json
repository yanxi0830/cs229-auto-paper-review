{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Evaluating Crowdsourcing Participants in the Absence of Ground-Truth", "abstract": "Given a supervised/semi-supervised learning scenario where multiple annotators are available, we consider the problem of identification of adversarial or unreliable annotators.", "histories": [["v1", "Mon, 30 May 2016 22:05:36 GMT  (1126kb,D)", "http://arxiv.org/abs/1605.09432v1", "4 pages, 5 figures, Workshop on Human Computation for Science and Computational Sustainability, NIPS 2012, Lake Tahoe, NV. 7 Dec 2012"]], "COMMENTS": "4 pages, 5 figures, Workshop on Human Computation for Science and Computational Sustainability, NIPS 2012, Lake Tahoe, NV. 7 Dec 2012", "reviews": [], "SUBJECTS": "cs.HC cs.LG", "authors": ["ramanathan subramanian", "romer rosales", "glenn fung", "jennifer dy"], "accepted": false, "id": "1605.09432"}
