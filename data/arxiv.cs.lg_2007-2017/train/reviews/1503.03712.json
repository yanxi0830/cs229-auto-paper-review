{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2015", "title": "On Graduated Optimization for Stochastic Non-Convex Problems", "abstract": "The graduated optimization approach, also known as the continuation method, is a popular heuristic to solving non-convex problems that has received renewed interest over the last decade. Despite its popularity, very little is known in terms of theoretical convergence analysis. In this paper we describe a new first-order algorithm based on graduated optimiza- tion and analyze its performance. We characterize a parameterized family of non- convex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an {\\epsilon}-approximate solution within O(1/\\epsilon^2) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of zero-order optimization, and devise a a variant of our algorithm which converges at rate of O(d^2/\\epsilon^4).", "histories": [["v1", "Thu, 12 Mar 2015 13:39:28 GMT  (231kb,D)", "http://arxiv.org/abs/1503.03712v1", "17 pages"], ["v2", "Wed, 8 Jul 2015 05:14:22 GMT  (231kb,D)", "http://arxiv.org/abs/1503.03712v2", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["elad hazan", "kfir yehuda levy", "shai shalev-shwartz"], "accepted": true, "id": "1503.03712"}
