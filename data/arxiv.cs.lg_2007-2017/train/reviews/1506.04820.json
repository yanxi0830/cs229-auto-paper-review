{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Online Gradient Boosting", "abstract": "We extend the theory of boosting for regression problems to the online learning setting. Generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with convex loss functions that competes with a larger class of regression functions. Our main result is an online gradient boosting algorithm which converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class. We also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class, and prove its optimality.", "histories": [["v1", "Tue, 16 Jun 2015 02:20:32 GMT  (19kb)", "https://arxiv.org/abs/1506.04820v1", null], ["v2", "Fri, 30 Oct 2015 20:04:31 GMT  (20kb)", "http://arxiv.org/abs/1506.04820v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alina beygelzimer", "elad hazan", "satyen kale", "haipeng luo"], "accepted": true, "id": "1506.04820"}
