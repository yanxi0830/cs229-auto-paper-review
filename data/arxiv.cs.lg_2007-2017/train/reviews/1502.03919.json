{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2015", "title": "Policy Gradient for Coherent Risk Measures", "abstract": "We provide sampling-based algorithms for optimization under a coherent-risk objective. The class of coherent-risk measures is widely accepted in finance and operations research, among other fields, and encompasses popular risk-measures such as the conditional value at risk (CVaR) and the mean-semi-deviation. Our approach is suitable for problems in which the tunable parameters control the distribution of the cost, such as in reinforcement learning with a parameterized policy; such problems cannot be solved using previous approaches. We consider both static risk measures, and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient algorithms, while for the dynamic risk measures our approach is actor-critic style.", "histories": [["v1", "Fri, 13 Feb 2015 09:16:24 GMT  (100kb,D)", "https://arxiv.org/abs/1502.03919v1", null], ["v2", "Mon, 8 Jun 2015 06:31:42 GMT  (250kb,D)", "http://arxiv.org/abs/1502.03919v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["aviv tamar", "yinlam chow", "mohammad ghavamzadeh", "shie mannor"], "accepted": true, "id": "1502.03919"}
