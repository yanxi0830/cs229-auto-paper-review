{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Hard-Clustering with Gaussian Mixture Models", "abstract": "Training the parameters of statistical models to describe a given data set is a central task in the field of data mining and machine learning. A very popular and powerful way of parameter estimation is the method of maximum likelihood estimation (MLE). Among the most widely used families of statistical models are mixture models, especially, mixtures of Gaussian distributions. A popular hard-clustering variant of the MLE problem is the so-called complete-data maximum likelihood estimation (CMLE) method. The standard approach to solve the CMLE problem is the Classification-Expectation-Maximization (CEM) algorithm. Unfortunately, it is only guaranteed that the algorithm converges to some (possibly arbitrarily poor) stationary point of the objective function.", "histories": [["v1", "Mon, 21 Mar 2016 16:02:27 GMT  (14kb)", "http://arxiv.org/abs/1603.06478v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["johannes bl\\\"omer", "sascha brauer", "kathrin bujna"], "accepted": false, "id": "1603.06478"}
