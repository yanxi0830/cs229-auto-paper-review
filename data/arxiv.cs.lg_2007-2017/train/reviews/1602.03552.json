{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Learning privately from multiparty data", "abstract": "Learning a classifier from private data collected by multiple parties is an important problem that has many potential applications. How can we build an accurate and differentially private global classifier by combining locally-trained classifiers from different parties, without access to any party's private data? We propose to transfer the `knowledge' of the local classifier ensemble by first creating labeled data from auxiliary unlabeled data, and then train a global $\\epsilon$-differentially private classifier. We show that majority voting is too sensitive and therefore propose a new risk weighted by class probabilities estimated from the ensemble. Relative to a non-private solution, our private solution has a generalization error bounded by $O(\\epsilon^{-2}M^{-2})$ where $M$ is the number of parties. This allows strong privacy without performance loss when $M$ is large, such as in crowdsensing applications. We demonstrate the performance of our method with realistic tasks of activity recognition, network intrusion detection, and malicious URL detection.", "histories": [["v1", "Wed, 10 Feb 2016 22:02:43 GMT  (352kb,D)", "http://arxiv.org/abs/1602.03552v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["jihun hamm", "yingjun cao", "mikhail belkin"], "accepted": true, "id": "1602.03552"}
