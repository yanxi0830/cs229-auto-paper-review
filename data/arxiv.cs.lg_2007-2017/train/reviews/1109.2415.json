{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2011", "title": "Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization", "abstract": "We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term. We show that both the basic proximal-gradient method and the accelerated proximal-gradient method achieve the same convergence rate as in the error-free case, provided that the errors decrease at appropriate rates.Using these rates, we perform as well as or better than a carefully chosen fixed error level on a set of structured sparsity problems.", "histories": [["v1", "Mon, 12 Sep 2011 09:45:02 GMT  (134kb,D)", "http://arxiv.org/abs/1109.2415v1", null], ["v2", "Thu, 1 Dec 2011 16:06:06 GMT  (120kb,D)", "http://arxiv.org/abs/1109.2415v2", "Neural Information Processing Systems (2011)"]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["mark w schmidt", "nicolas le roux", "francis r bach"], "accepted": true, "id": "1109.2415"}
