{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Practical Black-Box Attacks against Machine Learning", "abstract": "Deep Learning is increasingly used in several machine learning tasks as Deep Neural Networks (DNNs) frequently outperform other techniques. Yet, previous work showed that, once deployed, DNNs are vulnerable to integrity attacks. Indeed, adversaries can control DNN outputs and for instance force them to misclassify inputs by adding a carefully crafted and undistinguishable perturbation. However, these attacks assumed knowledge of the targeted DNN's architecture and parameters. In this paper however, we release these assumptions and introduce an attack conducted under the more realistic, yet more complex, threat model of an oracle: adversaries are only capable of accessing DNN label predictions for chosen inputs. We evaluate our attack in real-world settings by successfully forcing an oracle served by MetaMind, an online API for DNN classifiers, to misclassify inputs at a 84.24% rate. We also perform an evaluation to fine-tune our attack strategy and maximize the oracle's misclassification rate for adversarial samples.", "histories": [["v1", "Mon, 8 Feb 2016 19:12:25 GMT  (5529kb,D)", "http://arxiv.org/abs/1602.02697v1", null], ["v2", "Fri, 19 Feb 2016 01:49:44 GMT  (5484kb,D)", "http://arxiv.org/abs/1602.02697v2", null], ["v3", "Mon, 7 Nov 2016 00:01:18 GMT  (5724kb,D)", "http://arxiv.org/abs/1602.02697v3", null], ["v4", "Sun, 19 Mar 2017 14:50:18 GMT  (5512kb,D)", "http://arxiv.org/abs/1602.02697v4", "Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, Abu Dhabi, UAE"]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["nicolas papernot", "patrick mcdaniel", "ian goodfellow", "somesh jha", "z berkay celik", "ananthram swami"], "accepted": false, "id": "1602.02697"}
