{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2014", "title": "Selecting Near-Optimal Approximate State Representations in Reinforcement Learning", "abstract": "We consider a reinforcement learning setting introduced in (Maillard et al., NIPS 2011) where the learner does not have explicit access to the states of the underlying Markov decision process (MDP). Instead, the agent only has several models that map histories of past interactions to states. Here we improve over known regret bounds in this setting and more importantly generalize to the case where the models given to the learner do not contain a true model giving an MDP representation but only approximations of it. We also give improved error bounds for state aggregation.", "histories": [["v1", "Mon, 12 May 2014 07:45:54 GMT  (77kb)", "https://arxiv.org/abs/1405.2652v1", null], ["v2", "Wed, 14 May 2014 12:43:36 GMT  (43kb)", "http://arxiv.org/abs/1405.2652v2", null], ["v3", "Wed, 9 Jul 2014 14:40:20 GMT  (64kb)", "http://arxiv.org/abs/1405.2652v3", null], ["v4", "Mon, 21 Jul 2014 11:52:37 GMT  (65kb)", "http://arxiv.org/abs/1405.2652v4", null], ["v5", "Tue, 12 Aug 2014 12:19:55 GMT  (65kb)", "http://arxiv.org/abs/1405.2652v5", null], ["v6", "Mon, 15 Sep 2014 08:32:45 GMT  (65kb)", "http://arxiv.org/abs/1405.2652v6", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ronald ortner", "odalric-ambrym maillard", "daniil ryabko"], "accepted": false, "id": "1405.2652"}
