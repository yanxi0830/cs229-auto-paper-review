{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2015", "title": "Gradient-based Hyperparameter Optimization through Reversible Learning", "abstract": "Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.", "histories": [["v1", "Wed, 11 Feb 2015 23:52:36 GMT  (235kb,D)", "https://arxiv.org/abs/1502.03492v1", "10 figures. Submitted to ICML"], ["v2", "Fri, 13 Feb 2015 19:26:39 GMT  (235kb,D)", "http://arxiv.org/abs/1502.03492v2", "10 figures. Submitted to ICML"], ["v3", "Thu, 2 Apr 2015 17:40:44 GMT  (235kb,D)", "http://arxiv.org/abs/1502.03492v3", "10 figures. Submitted to ICML"]], "COMMENTS": "10 figures. Submitted to ICML", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["dougal maclaurin", "david k duvenaud", "ryan p adams"], "accepted": true, "id": "1502.03492"}
