{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Pyramid Vector Quantization for Deep Learning", "abstract": "This paper explores the use of Pyramid Vector Quantization (PVQ) to reduce the computational cost for a variety of neural networks (NNs) while, at the same time, compressing the weights that describe them. This is based on the fact that the dot product between an N dimensional vector of real numbers and an N dimensional PVQ vector can be calculated with only additions and subtractions and one multiplication. This is advantageous since tensor products, commonly used in NNs, can be re-conduced to a dot product or a set of dot products. Finally, it is stressed that any NN architecture that is based on an operation that can be re-conduced to a dot product can benefit from the techniques described here.", "histories": [["v1", "Mon, 10 Apr 2017 01:17:43 GMT  (238kb)", "http://arxiv.org/abs/1704.02681v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["vincenzo liguori"], "accepted": false, "id": "1704.02681"}
