{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "On the Depth of Deep Neural Networks: A Theoretical View", "abstract": "Deep neural networks (DNN) have achieved huge practical success in recent years. However, its theoretical properties (in particular generalization ability) are not yet very clear, since existing error bounds for neural networks cannot be directly used to explain the statistical behaviors of practically adopted DNN models (which are multi-class in their nature and may contain convolutional layers). To tackle the challenge, we derive a new margin bound for DNN in this paper, in which the expected 0-1 error of a DNN model is upper bounded by its empirical margin error plus a Rademacher Average based capacity term. This new bound is very general and is consistent with the empirical behaviors of DNN models observed in our experiments. According to the new bound, minimizing the empirical margin error can effectively improve the test performance of DNN. We therefore propose large margin DNN algorithms, which impose margin penalty terms to the cross entropy loss of DNN, so as to reduce the margin error during the training process. Experimental results show that the proposed algorithms can achieve significantly smaller empirical margin errors, as well as better test performances than the standard DNN algorithm.", "histories": [["v1", "Wed, 17 Jun 2015 07:51:42 GMT  (78kb)", "http://arxiv.org/abs/1506.05232v1", null], ["v2", "Sat, 28 Nov 2015 14:21:41 GMT  (182kb,D)", "http://arxiv.org/abs/1506.05232v2", "AAAI 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shizhao sun", "wei chen", "liwei wang 0001", "xiaoguang liu", "tie-yan liu"], "accepted": true, "id": "1506.05232"}
