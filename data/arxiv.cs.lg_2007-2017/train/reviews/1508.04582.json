{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2015", "title": "Learning to Predict Independent of Span", "abstract": "We consider how to learn multi-step predictions efficiently. Conventional algorithms wait until observing actual outcomes before performing the computations to update their predictions. If predictions are made at a high rate or span over a large amount of time, substantial computation can be required to store all relevant observations and to update all predictions when the outcome is finally observed. We show that the exact same predictions can be learned in a much more computationally congenial way, with uniform per-step computation that does not depend on the span of the predictions. We apply this idea to various settings of increasing generality, repeatedly adding desired properties and each time deriving an equivalent span-independent algorithm for the conventional algorithm that satisfies these desiderata. Interestingly, along the way several known algorithmic constructs emerge spontaneously from our derivations, including dutch eligibility traces, temporal difference errors, and averaging. This allows us to link these constructs one-to-one to the corresponding desiderata, unambiguously connecting the `how' to the `why'. Each step, we make sure that the derived algorithm subsumes the previous algorithms, thereby retaining their properties. Ultimately we arrive at a single general temporal-difference algorithm that is applicable to the full setting of reinforcement learning.", "histories": [["v1", "Wed, 19 Aug 2015 09:37:25 GMT  (153kb,D)", "http://arxiv.org/abs/1508.04582v1", "32 pages"]], "COMMENTS": "32 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hado van hasselt", "richard s sutton"], "accepted": false, "id": "1508.04582"}
