{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "Learning weakly supervised multimodal phoneme embeddings", "abstract": "Recent works have explored deep architectures for learning multimodal speech representation (e.g. audio and images, articulation and audio) in a supervised way. Here we investigate the role of combining different speech modalities, i.e. audio and visual information representing the lips movements, in a weakly supervised way using Siamese networks and lexical same-different side information. In particular, we ask whether one modality can benefit from the other to provide a richer representation for phone recognition in a weakly supervised setting. We introduce mono-task and multi-task methods for merging speech and visual modalities for phone recognition. The mono-task learning consists in applying a Siamese network on the concatenation of the two modalities, while the multi-task learning receives several different combinations of modalities at train time. We show that multi-task learning enhances discriminability for visual and multimodal inputs while minimally impacting auditory inputs. Furthermore, we present a qualitative analysis of the obtained phone embeddings, and show that cross-modal visual input can improve the discriminability of phonological features which are visually discernable (rounding, open/close, labial place of articulation), resulting in representations that are closer to abstract linguistic features than those based on audio only.", "histories": [["v1", "Sun, 23 Apr 2017 11:27:53 GMT  (1072kb,D)", "http://arxiv.org/abs/1704.06913v1", null], ["v2", "Wed, 18 Oct 2017 12:21:22 GMT  (1082kb,D)", "http://arxiv.org/abs/1704.06913v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["rahma chaabouni", "ewan dunbar", "neil zeghidour", "emmanuel dupoux"], "accepted": false, "id": "1704.06913"}
