{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Asynchronous Stochastic Gradient Descent with Delay Compensation", "abstract": "With the fast development of deep learning, people have started to train very big neural networks using massive data. Asynchronous Stochastic Gradient Descent (ASGD) is widely used to fulfill this task, which, however, is known to suffer from the problem of delayed gradient. That is, when a local worker adds the gradient it calculates to the global model, the global model may have been updated by other workers and this gradient becomes \"delayed\". We propose a novel technology to compensate this delay, so as to make the optimization behavior of ASGD closer to that of sequential SGD. This is done by leveraging Taylor expansion of the gradient function and efficient approximators to the Hessian matrix of the loss function. We call the corresponding new algorithm Delay Compensated ASGD (DC-ASGD). We evaluated the proposed algorithm on CIFAR-10 and ImageNet datasets, and experimental results demonstrate that DC-ASGD can outperform both synchronous SGD and ASGD, and nearly approaches the performance of sequential SGD.", "histories": [["v1", "Tue, 27 Sep 2016 09:22:03 GMT  (327kb,D)", "http://arxiv.org/abs/1609.08326v1", "7 pages, 5 figures"], ["v2", "Mon, 12 Jun 2017 17:53:10 GMT  (383kb,D)", "http://arxiv.org/abs/1609.08326v2", "20 pages, 5 figures"], ["v3", "Tue, 13 Jun 2017 09:02:32 GMT  (384kb,D)", "http://arxiv.org/abs/1609.08326v3", "20 pages, 5 figures"], ["v4", "Wed, 14 Jun 2017 12:45:50 GMT  (384kb,D)", "http://arxiv.org/abs/1609.08326v4", "20 pages, 5 figures"]], "COMMENTS": "7 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["shuxin zheng", "qi meng", "taifeng wang", "wei chen", "nenghai yu", "zhiming ma", "tie-yan liu"], "accepted": true, "id": "1609.08326"}
