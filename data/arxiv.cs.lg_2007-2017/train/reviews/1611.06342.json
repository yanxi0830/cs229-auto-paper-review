{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2016", "title": "Quantized neural network design under weight capacity constraint", "abstract": "The complexity of deep neural network algorithms for hardware implementation can be lowered either by scaling the number of units or reducing the word-length of weights. Both approaches, however, can accompany the performance degradation although many types of research are conducted to relieve this problem. Thus, it is an important question which one, between the network size scaling and the weight quantization, is more effective for hardware optimization. For this study, the performances of fully-connected deep neural networks (FCDNNs) and convolutional neural networks (CNNs) are evaluated while changing the network complexity and the word-length of weights. Based on these experiments, we present the effective compression ratio (ECR) to guide the trade-off between the network size and the precision of weights when the hardware resource is limited.", "histories": [["v1", "Sat, 19 Nov 2016 11:21:25 GMT  (5481kb,D)", "http://arxiv.org/abs/1611.06342v1", "This paper is accepted at NIPS 2016 workshop on Efficient Methods for Deep Neural Networks (EMDNN). arXiv admin note: text overlap witharXiv:1511.06488"]], "COMMENTS": "This paper is accepted at NIPS 2016 workshop on Efficient Methods for Deep Neural Networks (EMDNN). arXiv admin note: text overlap witharXiv:1511.06488", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sungho shin", "kyuyeon hwang", "wonyong sung"], "accepted": false, "id": "1611.06342"}
