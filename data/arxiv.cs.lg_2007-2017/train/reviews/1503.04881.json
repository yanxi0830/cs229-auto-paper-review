{"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2015", "title": "Long Short-Term Memory Over Tree Structures", "abstract": "The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.", "histories": [["v1", "Mon, 16 Mar 2015 23:59:02 GMT  (242kb)", "http://arxiv.org/abs/1503.04881v1", "On February 6th, 2015, this work was submitted to the International Conference on Machine Learning (ICML)"]], "COMMENTS": "On February 6th, 2015, this work was submitted to the International Conference on Machine Learning (ICML)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["xiaodan zhu", "parinaz sobhani", "hongyu guo"], "accepted": true, "id": "1503.04881"}
