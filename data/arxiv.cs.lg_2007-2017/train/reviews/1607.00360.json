{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "A scaled Bregman theorem with applications", "abstract": "Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms. This paper explores the use of Bregman divergences to establish reductions between such algorithms and their analyses. We present a new scaled isodistortion theorem involving Bregman divergences (scaled Bregman theorem for short) which shows that certain \"Bregman distortions'\" (employing a potentially non-convex generator) may be exactly re-written as a scaled Bregman divergence computed over transformed data. Admissible distortions include geodesic distances on curved manifolds and projections or gauge-normalisation, while admissible data include scalars, vectors and matrices.", "histories": [["v1", "Fri, 1 Jul 2016 19:27:28 GMT  (1273kb,D)", "http://arxiv.org/abs/1607.00360v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["richard nock", "aditya krishna menon", "cheng soon ong"], "accepted": true, "id": "1607.00360"}
