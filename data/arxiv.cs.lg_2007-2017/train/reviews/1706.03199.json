{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "Toward Optimal Run Racing: Application to Deep Learning Calibration", "abstract": "This paper aims at one-shot learning of deep neural nets, where a highly parallel setting is considered to address the algorithm calibration problem - selecting the best neural architecture and learning hyper-parameter values depending on the dataset at hand. The notoriously expensive calibration problem is optimally reduced by detecting and early stopping non-optimal runs. The theoretical contribution regards the optimality guarantees within the multiple hypothesis testing framework. Experimentations on the Cifar10, PTB and Wiki benchmarks demonstrate the relevance of the approach with a principled and consistent improvement on the state of the art with no extra hyper-parameter.", "histories": [["v1", "Sat, 10 Jun 2017 07:55:38 GMT  (974kb,D)", "https://arxiv.org/abs/1706.03199v1", null], ["v2", "Tue, 20 Jun 2017 11:38:25 GMT  (974kb,D)", "http://arxiv.org/abs/1706.03199v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["olivier bousquet", "sylvain gelly", "karol kurach", "marc schoenauer", "michele sebag", "olivier teytaud", "damien vincent"], "accepted": false, "id": "1706.03199"}
