{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Distributed Deep Learning Using Synchronous Stochastic Gradient Descent", "abstract": "We design and implement a distributed multinode synchronous SGD algorithm, without altering hyper parameters, or compressing data, or altering algorithmic behavior. We perform a detailed analysis of scaling, and identify optimal design points for different networks. We demonstrate scaling of CNNs on 100s of nodes, and present what we believe to be record training throughputs. A 512 minibatch VGG-A CNN training run is scaled 90X on 128 nodes. Also 256 minibatch VGG-A and OverFeat-FAST networks are scaled 53X and 42X respectively on a 64 node cluster. We also demonstrate the generality of our approach via best-in-class 6.5X scaling for a 7-layer DNN on 16 nodes. Thereafter we attempt to democratize deep-learning by training on an Ethernet based AWS cluster and show ~14X scaling on 16 nodes.", "histories": [["v1", "Mon, 22 Feb 2016 10:31:24 GMT  (313kb,D)", "http://arxiv.org/abs/1602.06709v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["dipankar das", "sasikanth avancha", "dheevatsa mudigere", "karthikeyan vaidynathan", "srinivas sridharan", "dhiraj kalamkar", "bharat kaul", "pradeep dubey"], "accepted": false, "id": "1602.06709"}
