{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2015", "title": "Stochastic Expectation Propagation", "abstract": "Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local-approximations that are iteratively refned for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large-scale datasets. However, EP has a crucial limitation in this context: the number approximating factors need to increase with the number of data-points, N, which entails a large computational burden. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP). Experiments on a number of synthetic and real-world data indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of N.", "histories": [["v1", "Fri, 12 Jun 2015 19:51:06 GMT  (780kb)", "http://arxiv.org/abs/1506.04132v1", null], ["v2", "Wed, 18 Nov 2015 10:52:17 GMT  (1879kb,D)", "http://arxiv.org/abs/1506.04132v2", "Published at NIPS 2015. 18 pages including supplementary"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yingzhen li", "jos\u00e9 miguel hern\u00e1ndez-lobato", "richard e turner"], "accepted": true, "id": "1506.04132"}
