{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2015", "title": "Selecting Near-Optimal Learners via Incremental Data Allocation", "abstract": "We study a novel machine learning (ML) problem setting of sequentially allocating small subsets of training data amongst a large set of classifiers. The goal is to select a classifier that will give near-optimal accuracy when trained on all data, while also minimizing the cost of misallocated samples. This is motivated by large modern datasets and ML toolkits with many combinations of learning algorithms and hyper-parameters. Inspired by the principle of \"optimism under uncertainty,\" we propose an innovative strategy, Data Allocation using Upper Bounds (DAUB), which robustly achieves these objectives across a variety of real-world datasets.", "histories": [["v1", "Thu, 31 Dec 2015 22:19:09 GMT  (399kb,D)", "http://arxiv.org/abs/1601.00024v1", "AAAI-2016: The Thirtieth AAAI Conference on Artificial Intelligence"]], "COMMENTS": "AAAI-2016: The Thirtieth AAAI Conference on Artificial Intelligence", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ashish sabharwal", "horst samulowitz", "gerald tesauro"], "accepted": true, "id": "1601.00024"}
