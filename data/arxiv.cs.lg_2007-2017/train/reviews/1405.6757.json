{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2014", "title": "Proximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces", "abstract": "In this paper, we set forth a new vision of reinforcement learning developed by us over the past few years, one that yields mathematically rigorous solutions to longstanding important questions that have remained unresolved: (i) how to design reliable, convergent, and robust reinforcement learning algorithms (ii) how to guarantee that reinforcement learning satisfies pre-specified \"safety\" guarantees, and remains in a stable region of the parameter space (iii) how to design \"off-policy\" temporal difference learning algorithms in a reliable and stable manner, and finally (iv) how to integrate the study of reinforcement learning into the rich theory of stochastic optimization. In this paper, we provide detailed answers to all these questions using the powerful framework of proximal operators.", "histories": [["v1", "Mon, 26 May 2014 23:11:40 GMT  (3211kb,D)", "http://arxiv.org/abs/1405.6757v1", "121 pages"]], "COMMENTS": "121 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sridhar mahadevan", "bo liu", "philip thomas", "will dabney", "steve giguere", "nicholas jacek", "ian gemp", "ji liu"], "accepted": false, "id": "1405.6757"}
