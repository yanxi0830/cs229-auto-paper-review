{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function of Deep Neural Networks (DNNs) has undergone many changes during the last decades. Since the advent of the well-known non-saturated Rectified Linear Unit (ReLU), many have tried to further improve the performance of the networks with more elaborate functions. Examples are the Leaky ReLU (LReLU) to remove zero gradients and Exponential Linear Unit (ELU) to reduce bias shift. In this paper, we introduce the Parametric ELU (PELU), an adaptive activation function that allows the DNNs to adopt different non-linear behaviors throughout the training phase. We contribute in three ways: (1) we show that PELU increases the network flexibility to counter vanishing gradient, (2) we provide a gradient-based optimization framework to learn the parameters of the function, and (3) we conduct several experiments on MNIST, CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN, ResNet and Vgg, to demonstrate the general applicability of the approach. Our proposed PELU has shown relative error improvements of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet, along with faster convergence rate in almost all test scenarios. We also observed that Vgg using PELU tended to prefer activations saturating close to zero, as in ReLU, except at last layer, which saturated near -2. These results suggest that varying the shape of the activations during training along with the other parameters helps to control vanishing gradients and bias shift, thus facilitating learning.", "histories": [["v1", "Mon, 30 May 2016 17:16:40 GMT  (78kb)", "http://arxiv.org/abs/1605.09332v1", "Submitted to Neural Information Processing Systems 2016 (NIPS)"], ["v2", "Tue, 31 May 2016 19:24:04 GMT  (75kb)", "http://arxiv.org/abs/1605.09332v2", "Submitted to Neural Information Processing Systems 2016 (NIPS)"], ["v3", "Fri, 18 Nov 2016 20:26:25 GMT  (208kb,D)", "http://arxiv.org/abs/1605.09332v3", "Submitted to International Conference on Learning Representations (ICLR) 2017"]], "COMMENTS": "Submitted to Neural Information Processing Systems 2016 (NIPS)", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["ludovic trottier", "philippe gigu\\`ere", "brahim chaib-draa"], "accepted": false, "id": "1605.09332"}
