{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Globally Normalized Transition-Based Neural Networks", "abstract": "We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.", "histories": [["v1", "Sat, 19 Mar 2016 03:56:03 GMT  (38kb)", "http://arxiv.org/abs/1603.06042v1", null], ["v2", "Wed, 8 Jun 2016 13:43:30 GMT  (39kb)", "http://arxiv.org/abs/1603.06042v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["daniel andor", "chris alberti", "david weiss", "aliaksei severyn", "alessandro presta", "kuzman ganchev", "slav petrov", "michael collins"], "accepted": true, "id": "1603.06042"}
