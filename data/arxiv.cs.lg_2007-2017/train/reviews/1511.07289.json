{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)", "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parameterized ReLUs (PReLUs), ELUs also avoid a vanishing gradient via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero. Zero means speed up learning because they bring the gradient closer to the unit natural gradient. We show that the unit natural gradient differs from the normal gradient by a bias shift term, which is proportional to the mean activation of incoming units. Like batch normalization, ELUs push the mean towards zero, but with a significantly smaller computational footprint. While other activation functions like LReLUs and PReLUs also have negative values, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. Consequently, dependencies between ELUs are much easier to model and distinct concepts are less likely to interfere. We found that ELUs lead not only to faster learning, but also to better generalization performance once networks have many layers (&gt;= 5). Using ELUs, we obtained the best published single-crop result on CIFAR-100 and CIFAR-10. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with similar classification performance, obtaining less than 10% classification error for a single crop, single model network.", "histories": [["v1", "Mon, 23 Nov 2015 15:58:05 GMT  (458kb,D)", "http://arxiv.org/abs/1511.07289v1", "14 pages, incl. supplement"], ["v2", "Thu, 3 Dec 2015 16:19:05 GMT  (612kb,D)", "http://arxiv.org/abs/1511.07289v2", "14 pages, incl. supplement"], ["v3", "Mon, 11 Jan 2016 17:55:53 GMT  (697kb,D)", "http://arxiv.org/abs/1511.07289v3", "14 pages, incl. supplement"], ["v4", "Mon, 15 Feb 2016 17:29:21 GMT  (697kb,D)", "http://arxiv.org/abs/1511.07289v4", "Published as a conference paper at ICLR 2016"], ["v5", "Mon, 22 Feb 2016 07:02:58 GMT  (697kb,D)", "http://arxiv.org/abs/1511.07289v5", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "14 pages, incl. supplement", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["djork-arn\\'e clevert", "thomas unterthiner", "sepp hochreiter"], "accepted": true, "id": "1511.07289"}
