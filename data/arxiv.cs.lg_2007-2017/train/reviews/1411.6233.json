{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "A Convex Sparse PCA for Feature Analysis", "abstract": "Principal component analysis (PCA) has been widely applied to dimensionality reduction and data pre-processing for different applications in engineering, biology and social science. Classical PCA and its variants seek for linear projections of the original variables to obtain a low dimensional feature representation with maximal variance. One limitation is that it is very difficult to interpret the results of PCA. In addition, the classical PCA is vulnerable to certain noisy data. In this paper, we propose a convex sparse principal component analysis (CSPCA) algorithm and apply it to feature analysis. First we show that PCA can be formulated as a low-rank regression optimization problem. Based on the discussion, the l 2 , 1 -norm minimization is incorporated into the objective function to make the regression coefficients sparse, thereby robust to the outliers. In addition, based on the sparse model used in CSPCA, an optimal weight is assigned to each of the original feature, which in turn provides the output with good interpretability. With the output of our CSPCA, we can effectively analyze the importance of each feature under the PCA criteria. The objective function is convex, and we propose an iterative algorithm to optimize it. We apply the CSPCA algorithm to feature selection and conduct extensive experiments on six different benchmark datasets. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art unsupervised feature selection algorithms.", "histories": [["v1", "Sun, 23 Nov 2014 13:06:43 GMT  (1210kb)", "http://arxiv.org/abs/1411.6233v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaojun chang", "feiping nie", "yi yang", "heng huang"], "accepted": false, "id": "1411.6233"}
