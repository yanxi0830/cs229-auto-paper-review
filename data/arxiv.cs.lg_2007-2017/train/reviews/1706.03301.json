{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2017", "title": "Neural Networks and Rational Functions", "abstract": "Neural networks and rational functions efficiently approximate each other. In more detail, it is shown here that for any ReLU network, there exists a rational function of degree $O(\\text{polylog}(1/\\epsilon))$ which is $\\epsilon$-close, and similarly for any rational function there exists a ReLU network of size $O(\\text{polylog}(1/\\epsilon))$ which is $\\epsilon$-close. By contrast, polynomials need degree $\\Omega(\\text{poly}(1/\\epsilon))$ to approximate even a single ReLU. When converting a ReLU network to a rational function as above, the hidden constants depend exponentially on the number of layers, which is shown to be tight; in other words, a compositional representation can be beneficial even for rational functions.", "histories": [["v1", "Sun, 11 Jun 2017 03:07:42 GMT  (731kb,D)", "http://arxiv.org/abs/1706.03301v1", "To appear, ICML 2017"]], "COMMENTS": "To appear, ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["matus telgarsky"], "accepted": true, "id": "1706.03301"}
