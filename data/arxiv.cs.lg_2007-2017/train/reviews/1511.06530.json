{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications", "abstract": "Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1?1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.", "histories": [["v1", "Fri, 20 Nov 2015 09:20:08 GMT  (1272kb,D)", "http://arxiv.org/abs/1511.06530v1", null], ["v2", "Wed, 24 Feb 2016 11:52:12 GMT  (1272kb,D)", "http://arxiv.org/abs/1511.06530v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yong-deok kim", "eunhyeok park", "sungjoo yoo", "taelim choi", "lu yang", "dongjun shin"], "accepted": true, "id": "1511.06530"}
