{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Visual Semantic Planning using Deep Successor Representations", "abstract": "A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross-task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment. The supplementary video can be accessed at the following link:", "histories": [["v1", "Tue, 23 May 2017 05:22:47 GMT  (7454kb,D)", "https://arxiv.org/abs/1705.08080v1", null], ["v2", "Tue, 15 Aug 2017 21:13:49 GMT  (7305kb,D)", "http://arxiv.org/abs/1705.08080v2", "ICCV 2017 camera ready"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["yuke zhu", "daniel gordon", "eric kolve", "dieter fox", "li fei-fei", "abhinav gupta", "roozbeh mottaghi", "ali farhadi"], "accepted": false, "id": "1705.08080"}
