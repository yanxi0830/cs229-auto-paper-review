{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2012", "title": "Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure", "abstract": "In this work we consider the stochastic minimization of nonsmooth convex loss functions, a central problem in machine learning. We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs. It is the first stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions (with strong convexity). The fast rates are confirmed by empirical comparisons, in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD.", "histories": [["v1", "Mon, 21 May 2012 03:29:17 GMT  (1334kb)", "https://arxiv.org/abs/1205.4481v1", "A short version of this paper appears in ICML 2012"], ["v2", "Mon, 2 Jul 2012 14:53:38 GMT  (1199kb)", "http://arxiv.org/abs/1205.4481v2", "This camera-ready version is uploaded for ICML 2012 proceedings"], ["v3", "Wed, 25 Jul 2012 15:15:42 GMT  (1334kb)", "http://arxiv.org/abs/1205.4481v3", "Full length version of ICML'12 with all proofs"], ["v4", "Mon, 1 Oct 2012 16:55:06 GMT  (1335kb)", "http://arxiv.org/abs/1205.4481v4", "Full length version of ICML'12 with all proofs. In this version, a bug in proving Theorem 6 is fixed. We'd like to thank Dr. Francesco Orabona for pointing it out"]], "COMMENTS": "A short version of this paper appears in ICML 2012", "reviews": [], "SUBJECTS": "cs.LG stat.CO stat.ML", "authors": ["hua ouyang", "alexander g gray"], "accepted": true, "id": "1205.4481"}
