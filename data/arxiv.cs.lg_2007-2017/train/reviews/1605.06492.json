{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Linear-Memory and Decomposition-Invariant Linearly Convergent Conditional Gradient Algorithm for Structured Polytopes", "abstract": "Recently, several works have shown that natural modifications of the classical conditional gradient method (aka Frank-Wolfe algorithm) for constrained convex optimization, provably converge with a linear rate when: i) the feasible set is a polytope, and ii) the objective is smooth and strongly-convex. However, all of these results suffer from two significant shortcomings: large memory requirement due to the need to store an explicit convex decomposition of the current iterate, and as a consequence, large running-time overhead per iteration, and worst case convergence rate that depends unfavorably on the dimension.", "histories": [["v1", "Fri, 20 May 2016 19:55:48 GMT  (275kb,D)", "http://arxiv.org/abs/1605.06492v1", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG", "authors": ["dan garber", "ofer meshi"], "accepted": true, "id": "1605.06492"}
