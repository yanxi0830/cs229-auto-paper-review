{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Prioritized Experience Replay", "abstract": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in the Deep Q-Network (DQN) algorithm, which achieved human-level performance in Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 42 out of 57 games.", "histories": [["v1", "Wed, 18 Nov 2015 20:54:44 GMT  (1194kb,D)", "http://arxiv.org/abs/1511.05952v1", "ICLR 2016 submission"], ["v2", "Thu, 19 Nov 2015 18:38:04 GMT  (1197kb,D)", "http://arxiv.org/abs/1511.05952v2", "ICLR 2016 submission"], ["v3", "Thu, 7 Jan 2016 01:53:42 GMT  (1217kb,D)", "http://arxiv.org/abs/1511.05952v3", "ICLR 2016 submission (revised after first round of reviews)"], ["v4", "Thu, 25 Feb 2016 17:55:31 GMT  (1217kb,D)", "http://arxiv.org/abs/1511.05952v4", "Published at ICLR 2016"]], "COMMENTS": "ICLR 2016 submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tom schaul", "john quan", "ioannis antonoglou", "david silver"], "accepted": true, "id": "1511.05952"}
