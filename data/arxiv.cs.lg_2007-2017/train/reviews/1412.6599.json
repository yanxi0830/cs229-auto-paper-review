{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Hot Swapping for Online Adaptation of Optimization Hyperparameters", "abstract": "We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.", "histories": [["v1", "Sat, 20 Dec 2014 04:36:28 GMT  (2681kb,D)", "https://arxiv.org/abs/1412.6599v1", "Submission to ICLR 2015"], ["v2", "Sat, 28 Feb 2015 05:18:18 GMT  (2682kb,D)", "http://arxiv.org/abs/1412.6599v2", "Submission to ICLR 2015"], ["v3", "Mon, 13 Apr 2015 23:28:01 GMT  (2682kb,D)", "http://arxiv.org/abs/1412.6599v3", "Submission to ICLR 2015"]], "COMMENTS": "Submission to ICLR 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kevin bache", "dennis decoste", "padhraic smyth"], "accepted": true, "id": "1412.6599"}
