{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2015", "title": "Scalable Discrete Sampling as a Multi-Armed Bandit Problem", "abstract": "Drawing a sample from a discrete distribution is one of the building components for Monte Carlo methods. Like other sampling algorithms, discrete sampling also suffers from high computational burden in large-scale inference problems. We study the problem of sampling a discrete random variable with a high degree of dependency that is typical in large-scale Bayesian inference and graphical models, and propose an efficient approximate solution with a subsampling approach. We make a novel connection between the discrete sampling and Multi-Armed Bandits problems with a finite reward population and provide three algorithms with theoretical guarantees. Empirical evaluations show the robustness and efficiency of the approximate algorithms in both synthetic and real-world large-scale problems.", "histories": [["v1", "Tue, 30 Jun 2015 11:20:45 GMT  (300kb,D)", "https://arxiv.org/abs/1506.09039v1", null], ["v2", "Tue, 9 Feb 2016 14:21:05 GMT  (303kb,D)", "http://arxiv.org/abs/1506.09039v2", null], ["v3", "Wed, 27 Apr 2016 21:09:43 GMT  (320kb,D)", "http://arxiv.org/abs/1506.09039v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yutian chen", "zoubin ghahramani"], "accepted": true, "id": "1506.09039"}
