{"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations", "abstract": "In this work we present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives state-of-the-art results for both.", "histories": [["v1", "Mon, 3 Apr 2017 15:39:56 GMT  (5688kb,D)", "http://arxiv.org/abs/1704.00648v1", "Supplementary visual examples available at:this http URL"], ["v2", "Thu, 8 Jun 2017 09:18:22 GMT  (1366kb,D)", "http://arxiv.org/abs/1704.00648v2", null]], "COMMENTS": "Supplementary visual examples available at:this http URL", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["eirikur agustsson", "fabian mentzer", "michael tschannen", "lukas cavigelli", "radu timofte", "luca benini", "luc van gool"], "accepted": true, "id": "1704.00648"}
