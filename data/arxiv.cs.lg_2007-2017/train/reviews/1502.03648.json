{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Over-Sampling in a Deep Neural Network", "abstract": "Deep neural networks (DNN) are the state of the art on many engineering problems such as computer vision and audition. A key factor in the success of the DNN is scalability - bigger networks work better. However, the reason for this scalability is not yet well understood. Here, we interpret the DNN as a discrete system, of linear filters followed by nonlinear activations, that is subject to the laws of sampling theory. In this context, we demonstrate that over-sampled networks are more selective, learn faster and learn more robustly. Our findings may ultimately generalize to the human brain.", "histories": [["v1", "Thu, 12 Feb 2015 13:29:03 GMT  (122kb)", "http://arxiv.org/abs/1502.03648v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["andrew j r simpson"], "accepted": false, "id": "1502.03648"}
