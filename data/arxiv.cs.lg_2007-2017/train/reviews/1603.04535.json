{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2016", "title": "Learning Domain-Invariant Subspace using Domain Features and Independence Maximization", "abstract": "When the distributions of the source and the target domains are different, domain adaptation techniques are needed. For example, in the field of sensors and measurement, discrete and continuous distributional change often exist in data because of instrumental variation and time-varying sensor drift. In this paper, we propose maximum independence domain adaptation (MIDA) to address this problem. Domain features are first defined to describe the background information of a sample, such as the device label and acquisition time. Then, MIDA learns features which have maximal independence with the domain features, so as to reduce the inter-domain discrepancy in distributions. A feature augmentation strategy is designed so that the learned projection is background-specific. Semi-supervised MIDA (SMIDA) extends MIDA by exploiting the label information. The proposed methods can handle not only discrete domains in traditional domain adaptation problems but also continuous distributional change such as the time-varying drift. In addition, they are naturally applicable in supervised/semi-supervised/unsupervised classification or regression problems with multiple domains. This flexibility brings potential for a wide range of applications. The effectiveness of our approaches is verified by experiments on synthetic datasets and four real-world ones on sensors, measurement, and computer vision.", "histories": [["v1", "Tue, 15 Mar 2016 02:56:22 GMT  (717kb,D)", "http://arxiv.org/abs/1603.04535v1", "13 pages, 9 figures, 6 tables"], ["v2", "Thu, 22 Jun 2017 01:39:22 GMT  (2544kb,D)", "http://arxiv.org/abs/1603.04535v2", "Accepted"]], "COMMENTS": "13 pages, 9 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["ke yan", "lu kou", "david zhang"], "accepted": false, "id": "1603.04535"}
