{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2014", "title": "Model-based Reinforcement Learning and the Eluder Dimension", "abstract": "We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as $\\tilde{O}(\\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is the Kolmogorov dimension and $d_E$ is the \\emph{eluder dimension}. This represents the first unified framework for model-based reinforcement learning and provides state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm \\emph{posterior sampling for reinforcement learning} (PSRL) that satisfies these bounds.", "histories": [["v1", "Sat, 7 Jun 2014 03:02:09 GMT  (18kb)", "https://arxiv.org/abs/1406.1853v1", null], ["v2", "Fri, 31 Oct 2014 23:36:00 GMT  (22kb)", "http://arxiv.org/abs/1406.1853v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ian osband", "benjamin van roy"], "accepted": true, "id": "1406.1853"}
