{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2015", "title": "A Light Touch for Heavily Constrained SGD", "abstract": "Projected stochastic gradient descent (SGD) is often the default choice for large-scale optimization in machine learning, but requires a projection after each update. For heavily-constrained objectives, we propose an efficient extension of SGD that stays close to the feasible region while only applying constraints probabilistically at each iteration. Theoretical analysis shows a good trade-off between per-iteration work and the number of iterations needed, indicating compelling advantages on problems with a large number of constraints onto which projecting is expensive. In MATLAB experiments, our algorithm successfully handles a large-scale real-world video ranking problem with tens of thousands of linear inequality constraints that was too large for projected SGD and stochastic Frank-Wolfe.", "histories": [["v1", "Tue, 15 Dec 2015 21:07:02 GMT  (55kb)", "https://arxiv.org/abs/1512.04960v1", null], ["v2", "Mon, 24 Oct 2016 20:30:25 GMT  (74kb)", "http://arxiv.org/abs/1512.04960v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew cotter", "maya gupta", "jan pfeifer"], "accepted": false, "id": "1512.04960"}
