{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2013", "title": "Accelerated Mini-Batch Stochastic Dual Coordinate Ascent", "abstract": "Stochastic dual coordinate ascent (SDCA) is an effective technique for solving regularized loss minimization problems in machine learning. This paper considers an extension of SDCA under the mini-batch setting that is often used in practice. Our main contribution is to introduce an accelerated mini-batch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of \\cite{nesterov2007gradient}.", "histories": [["v1", "Sun, 12 May 2013 12:46:25 GMT  (105kb,D)", "http://arxiv.org/abs/1305.2581v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shai shalev-shwartz", "tong zhang 0001"], "accepted": true, "id": "1305.2581"}
