{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Gradual training of deep denoising auto encoders", "abstract": "Stacked denoising auto encoders (DAEs) are well known to learn useful deep representations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE layers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on MNIST and CIFAR datasets.", "histories": [["v1", "Fri, 19 Dec 2014 09:30:33 GMT  (1698kb,D)", "http://arxiv.org/abs/1412.6257v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["alexander kalmanovich", "gal chechik"], "accepted": false, "id": "1412.6257"}
