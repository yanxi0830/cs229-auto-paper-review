{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2016", "title": "Revisiting Batch Normalization For Practical Domain Adaptation", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common (yet inconvenient) practice to prepare at least tens of thousands of labeled image to fine-tune a network on every task before the model is ready to use. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning.", "histories": [["v1", "Tue, 15 Mar 2016 17:44:32 GMT  (1263kb,D)", "http://arxiv.org/abs/1603.04779v1", null], ["v2", "Wed, 16 Mar 2016 03:57:19 GMT  (1263kb,D)", "http://arxiv.org/abs/1603.04779v2", null], ["v3", "Wed, 21 Sep 2016 08:41:43 GMT  (1223kb,D)", "http://arxiv.org/abs/1603.04779v3", null], ["v4", "Tue, 8 Nov 2016 06:11:30 GMT  (6116kb,D)", "http://arxiv.org/abs/1603.04779v4", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yanghao li", "naiyan wang", "jianping shi", "jiaying liu", "xiaodi hou"], "accepted": false, "id": "1603.04779"}
