{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2014", "title": "Winner-Take-All Autoencoders", "abstract": "We explore combining the benefits of convolutional architectures and autoencoders for learning deep representations in an unsupervised manner. A major challenge is to achieve appropriate sparsity among hidden variables, since neighbouring variables in each feature map tend to be highly correlated and a suppression mechanism is therefore needed. Previously, deconvolutional networks and convolutional predictive sparse decomposition have been used to construct systems that have a recognition pathway and a data generation pathway that are trained so that they agree and so that the hidden representation is sparse. We take a more direct approach and describe a way to train convolutional autoencoders layer by layer, where in each layer sparsity is achieved using a winner-take-all activation function within each feature map. Learning is computationally efficient and we show that our method can be used to train shallow and deep convolutional autoencoders whose representations can be used to achieve classification rates on the MNIST, CIFAR-10 and NORB datasets that are competitive with the state of the art.", "histories": [["v1", "Tue, 9 Sep 2014 14:38:43 GMT  (604kb)", "http://arxiv.org/abs/1409.2752v1", null], ["v2", "Sun, 7 Jun 2015 18:28:22 GMT  (947kb)", "http://arxiv.org/abs/1409.2752v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["alireza makhzani", "brendan j frey"], "accepted": true, "id": "1409.2752"}
