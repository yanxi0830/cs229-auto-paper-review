{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups", "abstract": "We propose a new method for training computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. Our sparse connection structure facilitates a significant reduction in computational cost and number of parameters of state-of-the-art deep CNNs without compromising accuracy. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less compute, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).", "histories": [["v1", "Fri, 20 May 2016 19:51:37 GMT  (1968kb,D)", "http://arxiv.org/abs/1605.06489v1", null], ["v2", "Tue, 29 Nov 2016 17:29:01 GMT  (1661kb,D)", "http://arxiv.org/abs/1605.06489v2", "Short workshop paper for EMDNN 2016"], ["v3", "Wed, 30 Nov 2016 15:32:03 GMT  (4435kb,D)", "http://arxiv.org/abs/1605.06489v3", "Updated full version of paper, in full letter paper two-column paper. Includes many textual changes, updated CIFAR10 results, and new analysis of inter/intra-layer correlation"]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["yani ioannou", "duncan robertson", "roberto cipolla", "antonio criminisi"], "accepted": false, "id": "1605.06489"}
