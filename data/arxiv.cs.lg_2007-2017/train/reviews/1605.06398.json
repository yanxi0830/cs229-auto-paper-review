{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Stochastic Variance Reduction Methods for Saddle-Point Problems", "abstract": "We consider convex-concave saddle-point problems where the objective functions may be split in many components, and extend recent stochastic variance reduction methods (such as SVRG or SAGA) to provide the first large-scale linearly convergent algorithms for this class of problems which is common in machine learning. While the algorithmic extension is straightforward, it comes with challenges and opportunities: (a) the convex minimization analysis does not apply and we use the notion of monotone operators to prove convergence, showing in particular that the same algorithm applies to a larger class of problems, such as variational inequalities, (b) there are two notions of splits, in terms of functions, or in terms of partial derivatives, (c) the split does need to be done with convex-concave terms, (d) non-uniform sampling is key to an efficient algorithm, both in theory and practice, and (e) these incremental algorithms can be easily accelerated using a simple extension of the \"catalyst\" framework, leading to an algorithm which is always superior to accelerated batch algorithms.", "histories": [["v1", "Fri, 20 May 2016 15:16:29 GMT  (207kb)", "http://arxiv.org/abs/1605.06398v1", null], ["v2", "Thu, 3 Nov 2016 10:24:55 GMT  (211kb)", "http://arxiv.org/abs/1605.06398v2", "Neural Information Processing Systems (NIPS), 2016, Barcelona, Spain"]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["balamurugan palaniappan", "francis r bach"], "accepted": true, "id": "1605.06398"}
