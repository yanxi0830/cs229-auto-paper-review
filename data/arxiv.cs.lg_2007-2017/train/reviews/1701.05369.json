{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jan-2017", "title": "Variational Dropout Sparsifies Deep Neural Networks", "abstract": "We explore recently proposed variational dropout technique which provided an elegant Bayesian interpretation to dropout. We extend variational dropout to the case when dropout rate is unknown and show that it can be found by optimizing evidence variational lower bound. We show that it is possible to assign and find individual dropout rates to each connection in DNN. Interestingly such assignment leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination (ARD) effect in empirical Bayes but has a number of advantages. We report up to 128 fold compression of popular architectures without a large loss of accuracy providing additional evidence to the fact that modern deep architectures are very redundant.", "histories": [["v1", "Thu, 19 Jan 2017 10:44:55 GMT  (171kb,D)", "https://arxiv.org/abs/1701.05369v1", null], ["v2", "Mon, 27 Feb 2017 20:43:27 GMT  (90kb,D)", "http://arxiv.org/abs/1701.05369v2", null], ["v3", "Tue, 13 Jun 2017 11:01:55 GMT  (93kb,D)", "http://arxiv.org/abs/1701.05369v3", "Published in ICML 2017"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["dmitry molchanov", "arsenii ashukha", "dmitry vetrov"], "accepted": true, "id": "1701.05369"}
