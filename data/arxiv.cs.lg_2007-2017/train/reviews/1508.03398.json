{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2015", "title": "End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture", "abstract": "We develop a fully discriminative learning approach for supervised Latent Dirichlet Allocation (LDA) model, which maximizes the posterior probability of the prediction variable given the input document. Different from traditional variational learning or Gibbs sampling approaches, the proposed learning method applies (i) the mirror descent algorithm for exact maximum a posterior inference and (ii) back propagation with stochastic gradient descent for model parameter estimation, leading to scalable learning of the model in an end-to-end discriminative manner. As a byproduct, we also apply this technique to develop a new learning method for the traditional unsupervised LDA model. Experimental results on two real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised/unsupervised LDA learning methods.", "histories": [["v1", "Fri, 14 Aug 2015 01:32:27 GMT  (765kb,D)", "http://arxiv.org/abs/1508.03398v1", "20 pages, 5 figures"], ["v2", "Sun, 1 Nov 2015 08:11:14 GMT  (540kb,D)", "http://arxiv.org/abs/1508.03398v2", "Proc. NIPS 2015"]], "COMMENTS": "20 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jianshu chen", "ji he", "yelong shen", "lin xiao", "xiaodong he", "jianfeng gao", "xinying song", "li deng"], "accepted": true, "id": "1508.03398"}
