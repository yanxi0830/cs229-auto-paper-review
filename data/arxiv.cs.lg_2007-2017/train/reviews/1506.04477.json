{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2015", "title": "Dual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy", "abstract": "The online learning of deep neural networks is an interesting problem of machine learning because, for example, major IT companies want to manage the information of the massive data uploaded on the web daily, and this technology can contribute to the next generation of lifelong learning. We aim to train deep models from new data that consists of new classes, distributions, and tasks at minimal computational cost, which we call online deep learning. Unfortunately, deep neural network learning through classical online and incremental methods does not work well in both theory and practice. In this paper, we introduce dual memory architectures for online incremental deep learning. The proposed architecture consists of deep representation learners and fast learnable shallow kernel networks, both of which synergize to track the information of new data. During the training phase, we use various online, incremental ensemble, and transfer learning techniques in order to achieve lower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image recognition tasks, the proposed dual memory architectures performs much better than the classical online and incremental ensemble algorithm, and their accuracies are similar to that of the batch learner.", "histories": [["v1", "Mon, 15 Jun 2015 04:44:38 GMT  (669kb)", "http://arxiv.org/abs/1506.04477v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sang-woo lee", "min-oh heo", "jiwon kim", "jeonghee kim", "byoung-tak zhang"], "accepted": false, "id": "1506.04477"}
