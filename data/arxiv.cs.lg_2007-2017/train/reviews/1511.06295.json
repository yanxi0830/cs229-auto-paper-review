{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Policy Distillation", "abstract": "Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.", "histories": [["v1", "Thu, 19 Nov 2015 18:38:47 GMT  (371kb,D)", "http://arxiv.org/abs/1511.06295v1", "Submitted to ICLR 2016"], ["v2", "Thu, 7 Jan 2016 18:43:03 GMT  (578kb,D)", "http://arxiv.org/abs/1511.06295v2", "Submitted to ICLR 2016"]], "COMMENTS": "Submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrei a rusu", "sergio gomez colmenarejo", "caglar gulcehre", "guillaume desjardins", "james kirkpatrick", "razvan pascanu", "volodymyr mnih", "koray kavukcuoglu", "raia hadsell"], "accepted": true, "id": "1511.06295"}
