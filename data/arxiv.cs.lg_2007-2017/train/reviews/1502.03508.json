{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Adding vs. Averaging in Distributed Primal-Dual Optimization", "abstract": "Distributed optimization algorithms for large-scale machine learning suffer from a communication bottleneck. Reducing communication makes the efficient aggregation of partial work from different machines more challenging. In this paper we present a novel generalization of the recent communication efficient primal-dual coordinate ascent framework (CoCoA). Our framework, CoCoA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allowed conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both CoCoA as well as our new variants, and generalize the theory for both methods to also cover non-smooth convex loss functions. We provide an extensive experimental comparison on several real-world distributed datasets, showing markedly improved performance, especially when scaling up the number of machines.", "histories": [["v1", "Thu, 12 Feb 2015 01:51:08 GMT  (767kb,D)", "http://arxiv.org/abs/1502.03508v1", null], ["v2", "Fri, 3 Jul 2015 19:35:13 GMT  (769kb,D)", "http://arxiv.org/abs/1502.03508v2", "ICML 2015: JMLR W&amp;CP volume37, Proceedings of The 32nd International Conference on Machine Learning, pp. 1973-1982"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chenxin ma", "virginia smith", "martin jaggi", "michael i jordan", "peter richt\u00e1rik", "martin tak\u00e1c"], "accepted": true, "id": "1502.03508"}
