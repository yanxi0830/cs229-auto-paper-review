{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods", "abstract": "Neural networks are known to be vulnerable to adversarial examples: inputs that are close to valid inputs but classified incorrectly. We investigate the security of ten recent proposals that are designed to detect adversarial examples. We show that all can be defeated, even when the adversary does not know the exact parameters of the detector. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and we propose several guidelines for evaluating future proposed defenses.", "histories": [["v1", "Sat, 20 May 2017 05:59:23 GMT  (483kb,D)", "http://arxiv.org/abs/1705.07263v1", null], ["v2", "Wed, 1 Nov 2017 04:07:05 GMT  (1478kb,D)", "http://arxiv.org/abs/1705.07263v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.CV", "authors": ["nicholas carlini", "david wagner"], "accepted": false, "id": "1705.07263"}
