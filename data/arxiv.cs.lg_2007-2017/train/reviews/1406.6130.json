{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2014", "title": "Generalized Mixability via Entropic Duality", "abstract": "Mixability is a property of a loss which characterizes when fast convergence is possible in the game of prediction with expert advice. We show that a key property of mixability generalizes, and the exp and log operations present in the usual theory are not as special as one might have thought. In doing this we introduce a more general notion of $\\Phi$-mixability where $\\Phi$ is a general entropy (\\ie, any convex function on probabilities). We show how a property shared by the convex dual of any such entropy yields a natural algorithm (the minimizer of a regret bound) which, analogous to the classical aggregating algorithm, is guaranteed a constant regret when used with $\\Phi$-mixable losses. We characterize precisely which $\\Phi$ have $\\Phi$-mixable losses and put forward a number of conjectures about the optimality and relationships between different choices of entropy.", "histories": [["v1", "Tue, 24 Jun 2014 03:31:16 GMT  (231kb,D)", "http://arxiv.org/abs/1406.6130v1", "20 pages, 1 figure. Supersedes the work inarXiv:1403.2433[cs.LG]"]], "COMMENTS": "20 pages, 1 figure. Supersedes the work inarXiv:1403.2433[cs.LG]", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mark d reid", "rafael m frongillo", "robert c williamson", "nishant mehta"], "accepted": false, "id": "1406.6130"}
