{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2015", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "Recurrent neural networks (RNNs) have proven to be powerful models in problems involving sequential data. Recently, RNNs have been augmented with \"attention\" mechanisms which allow the network to focus on different parts of an input sequence when computing their output. We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that it can solve some long-term memory problems (specifically, those where temporal order doesn't matter). In fact, we show empirically that our model can solve these problems for sequence lengths which are both longer and more widely varying than the best results attained with RNNs.", "histories": [["v1", "Tue, 29 Dec 2015 19:03:43 GMT  (35kb,D)", "https://arxiv.org/abs/1512.08756v1", null], ["v2", "Thu, 31 Dec 2015 17:52:46 GMT  (35kb,D)", "http://arxiv.org/abs/1512.08756v2", null], ["v3", "Fri, 8 Jan 2016 01:52:06 GMT  (36kb,D)", "http://arxiv.org/abs/1512.08756v3", null], ["v4", "Mon, 28 Mar 2016 02:41:21 GMT  (102kb,D)", "http://arxiv.org/abs/1512.08756v4", null], ["v5", "Tue, 20 Sep 2016 05:02:22 GMT  (37kb,D)", "http://arxiv.org/abs/1512.08756v5", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["colin raffel", "daniel p w ellis"], "accepted": false, "id": "1512.08756"}
