{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks", "abstract": "Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks. A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates between nodes can be very large. Consequently, lossy compresion heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge, and it is not clear whether they are optimal.", "histories": [["v1", "Fri, 7 Oct 2016 03:44:34 GMT  (701kb,D)", "http://arxiv.org/abs/1610.02132v1", null], ["v2", "Mon, 10 Apr 2017 22:25:06 GMT  (1597kb,D)", "http://arxiv.org/abs/1610.02132v2", null], ["v3", "Thu, 25 May 2017 08:05:19 GMT  (1819kb,D)", "http://arxiv.org/abs/1610.02132v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["dan alistarh", "demjan grubic", "jerry li", "ryota tomioka", "milan vojnovic"], "accepted": false, "id": "1610.02132"}
