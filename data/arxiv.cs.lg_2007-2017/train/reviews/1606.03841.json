{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity", "abstract": "The use of convex regularizers allow for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, for a large class of nonconvex regularizers, we propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex regularizer, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the standard proximal algorithm and Frank-Wolfe algorithm). Moreover, it can be shown that critical points of the transformed problem are also critical points of the original problem. Extensive experiments on a number of nonconvex regularization problems show that the proposed procedure is much faster than the state-of-the-art nonconvex solvers.", "histories": [["v1", "Mon, 13 Jun 2016 07:21:31 GMT  (482kb,D)", "https://arxiv.org/abs/1606.03841v1", "Full version of ICML 2016 paper with same title"], ["v2", "Thu, 20 Oct 2016 05:57:58 GMT  (446kb,D)", "http://arxiv.org/abs/1606.03841v2", null], ["v3", "Mon, 13 Feb 2017 01:27:29 GMT  (2689kb,D)", "http://arxiv.org/abs/1606.03841v3", "Journal version of previous conference paper appeared at ICML-2016 with same title"]], "COMMENTS": "Full version of ICML 2016 paper with same title", "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["quanming yao", "james t kwok"], "accepted": true, "id": "1606.03841"}
