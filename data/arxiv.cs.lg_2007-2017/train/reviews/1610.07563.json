{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "On Multiplicative Multitask Feature Learning", "abstract": "We investigate a general framework of multiplicative multitask feature learning which decomposes each task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods have been proposed as special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers, leading to a better understanding of the shrinkage effect. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. Empirical studies have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks.", "histories": [["v1", "Mon, 24 Oct 2016 19:27:52 GMT  (7145kb,D)", "http://arxiv.org/abs/1610.07563v1", "Advances in Neural Information Processing Systems 2014"]], "COMMENTS": "Advances in Neural Information Processing Systems 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xin wang 0023", "jinbo bi", "shipeng yu", "jiangwen sun"], "accepted": true, "id": "1610.07563"}
