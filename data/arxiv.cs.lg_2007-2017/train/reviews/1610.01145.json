{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Error bounds for approximations with deep ReLU networks", "abstract": "We study how approximation errors of neural networks with ReLU activation functions depend on the depth of the network. We establish rigorous error bounds showing that deep ReLU networks are significantly more expressive than shallow ones as long as approximations of smooth functions are concerned. At the same time, we show that on a set of functions constrained only by their degree of smoothness, a ReLU network architecture cannot in general achieve approximation accuracy with better than a power law dependence on the network size, regardless of its depth.", "histories": [["v1", "Mon, 3 Oct 2016 23:08:22 GMT  (134kb,D)", "https://arxiv.org/abs/1610.01145v1", null], ["v2", "Mon, 7 Nov 2016 16:57:35 GMT  (162kb,D)", "http://arxiv.org/abs/1610.01145v2", "16 pages; Theorem 3 added in v2"], ["v3", "Mon, 1 May 2017 14:01:32 GMT  (314kb,D)", "http://arxiv.org/abs/1610.01145v3", "31 pages; major revision in v3; submitted to Neural Networks"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["dmitry yarotsky"], "accepted": false, "id": "1610.01145"}
