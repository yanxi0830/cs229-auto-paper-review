{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Online and Differentially-Private Tensor Decomposition", "abstract": "Tensor decomposition is positioned to be a pervasive tool in the era of big data. In this paper, we resolve many of the key algorithmic questions regarding robustness, memory efficiency, and differential privacy of tensor decomposition. We propose simple variants of the tensor power method which enjoy these strong properties. We present the first guarantees for online tensor power method which has a linear memory requirement. Moreover, we present a noise calibrated tensor power method with efficient privacy guarantees. At the heart of all these guarantees lies a careful perturbation analysis derived in this paper which improves up on the existing results significantly.", "histories": [["v1", "Mon, 20 Jun 2016 18:30:10 GMT  (205kb,D)", "https://arxiv.org/abs/1606.06237v1", "20 pages, 9 figures"], ["v2", "Sat, 2 Jul 2016 22:30:01 GMT  (205kb,D)", "http://arxiv.org/abs/1606.06237v2", "20 pages, 9 figures"], ["v3", "Sun, 30 Oct 2016 21:56:58 GMT  (209kb,D)", "http://arxiv.org/abs/1606.06237v3", "19 pages, 9 figures. To appear at the 30th Annual Conference on Advances in Neural Information Processing Systems (NIPS 2016), to be held at Barcelona, Spain"], ["v4", "Thu, 15 Dec 2016 13:35:22 GMT  (208kb,D)", "http://arxiv.org/abs/1606.06237v4", "19 pages, 9 figures. To appear at the 30th Annual Conference on Advances in Neural Information Processing Systems (NIPS 2016), to be held at Barcelona, Spain. Fix small typos in proofs of Lemmas C.5 and C.6"]], "COMMENTS": "20 pages, 9 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yining wang", "anima anandkumar"], "accepted": true, "id": "1606.06237"}
