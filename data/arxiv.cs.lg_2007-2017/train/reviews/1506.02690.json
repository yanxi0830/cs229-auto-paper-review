{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Adaptive Normalized Risk-Averting Training for Deep Neural Networks", "abstract": "This paper proposes a set of new error criteria and learning approaches, Adaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex optimization problem in training deep neural networks (DNNs). Theoretically, we demonstrate its effectiveness on global and local convexity lower-bounded by the standard $L_p$-norm error. By analyzing the gradient on the convexity index $\\lambda$, we explain the reason why to learn $\\lambda$ adaptively using gradient descent works. In practice, we show how this method improves training of deep neural networks to solve visual recognition tasks on the MNIST and CIFAR-10 datasets. Without using pretraining or other tricks, we obtain results comparable or superior to those reported in recent literature on the same tasks using standard ConvNets + MSE/cross entropy. Performance on deep/shallow multilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT can be combined with other quasi-Newton training methods, innovative network variants, regularization techniques and other specific tricks in DNNs. Other than unsupervised pretraining, it provides a new perspective to address the non-convex optimization problem in DNNs.", "histories": [["v1", "Mon, 8 Jun 2015 20:42:12 GMT  (769kb,D)", "https://arxiv.org/abs/1506.02690v1", "17 pages, 4 figures. Submitted to NIPS-2015"], ["v2", "Fri, 7 Aug 2015 14:53:46 GMT  (768kb,D)", "http://arxiv.org/abs/1506.02690v2", "17 pages, 4 figures"], ["v3", "Thu, 9 Jun 2016 04:10:22 GMT  (585kb,D)", "http://arxiv.org/abs/1506.02690v3", "AAAI 2016, 0.39%~0.4% ER on MNIST with single 32-32-256-10 ConvNets, code available atthis https URL"]], "COMMENTS": "17 pages, 4 figures. Submitted to NIPS-2015", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["zhiguang wang", "tim oates", "james lo"], "accepted": true, "id": "1506.02690"}
