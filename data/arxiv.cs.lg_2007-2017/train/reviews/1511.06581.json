{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Dueling Network Architectures for Deep Reinforcement Learning", "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning inspired by advantage learning. Our dueling architecture represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art Double DQN method of van Hasselt et al. (2015) in 46 out of 57 Atari games.", "histories": [["v1", "Fri, 20 Nov 2015 13:07:54 GMT  (507kb,D)", "http://arxiv.org/abs/1511.06581v1", "14 pages, 6 figures, and 5 tables"], ["v2", "Fri, 8 Jan 2016 11:37:42 GMT  (880kb,D)", "http://arxiv.org/abs/1511.06581v2", "14 pages, 6 figures, and 5 tables"], ["v3", "Tue, 5 Apr 2016 09:03:06 GMT  (695kb,D)", "http://arxiv.org/abs/1511.06581v3", "15 pages, 5 figures, and 5 tables"]], "COMMENTS": "14 pages, 6 figures, and 5 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ziyu wang 0001", "tom schaul", "matteo hessel", "hado van hasselt", "marc lanctot", "nando de freitas"], "accepted": true, "id": "1511.06581"}
