{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient", "abstract": "In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm.", "histories": [["v1", "Wed, 1 Mar 2017 02:08:32 GMT  (1589kb)", "https://arxiv.org/abs/1703.00102v1", null], ["v2", "Sat, 3 Jun 2017 07:30:20 GMT  (1862kb)", "http://arxiv.org/abs/1703.00102v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["lam m nguyen", "jie liu", "katya scheinberg", "martin tak\u00e1c"], "accepted": true, "id": "1703.00102"}
