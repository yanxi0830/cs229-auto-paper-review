{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Learning feed-forward one-shot learners", "abstract": "One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.", "histories": [["v1", "Thu, 16 Jun 2016 15:49:26 GMT  (2553kb,D)", "http://arxiv.org/abs/1606.05233v1", "The first three authors contributed equally, and are listed in alphabetical order"]], "COMMENTS": "The first three authors contributed equally, and are listed in alphabetical order", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["luca bertinetto", "jo\u00e3o f henriques", "jack valmadre", "philip h s torr", "andrea vedaldi"], "accepted": true, "id": "1606.05233"}
