{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Semisupervised Autoencoder for Sentiment Analysis", "abstract": "In this paper, we investigate the usage of autoencoders in modeling textual data. Traditional autoencoders suffer from at least two aspects: scalability with the high dimensionality of vocabulary size and dealing with task-irrelevant words. We address this problem by introducing supervision via the loss function of autoencoders. In particular, we first train a linear classifier on the labeled data, then define a loss for the autoencoder with the weights learned from the linear classifier. To reduce the bias brought by one single classifier, we define a posterior probability distribution on the weights of the classifier, and derive the marginalized loss of the autoencoder with Laplace approximation. We show that our choice of loss function can be rationalized from the perspective of Bregman Divergence, which justifies the soundness of our model. We evaluate the effectiveness of our model on six sentiment analysis datasets, and show that our model significantly outperforms all the competing methods with respect to classification accuracy. We also show that our model is able to take advantage of unlabeled dataset and get improved performance. We further show that our model successfully learns highly discriminative feature maps, which explains its superior performance.", "histories": [["v1", "Mon, 14 Dec 2015 19:09:53 GMT  (25kb)", "http://arxiv.org/abs/1512.04466v1", "To appear in AAAI 2016"]], "COMMENTS": "To appear in AAAI 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuangfei zhai", "zhongfei zhang"], "accepted": true, "id": "1512.04466"}
