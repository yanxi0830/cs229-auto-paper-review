{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Sample Efficient Actor-Critic with Experience Replay", "abstract": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.", "histories": [["v1", "Thu, 3 Nov 2016 23:21:32 GMT  (1409kb,D)", "http://arxiv.org/abs/1611.01224v1", "20 pages. Prepared for ICLR 2017"], ["v2", "Mon, 10 Jul 2017 14:38:10 GMT  (2708kb,D)", "http://arxiv.org/abs/1611.01224v2", "20 pages. Prepared for ICLR 2017"]], "COMMENTS": "20 pages. Prepared for ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ziyu wang", "victor bapst", "nicolas heess", "volodymyr mnih", "remi munos", "koray kavukcuoglu", "nando de freitas"], "accepted": true, "id": "1611.01224"}
