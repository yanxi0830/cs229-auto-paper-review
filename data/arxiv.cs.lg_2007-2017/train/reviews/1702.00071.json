{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "On orthogonality and learning recurrent networks with long term dependencies", "abstract": "It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and can therefore be a desirable property; however, we find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance. This paper explores the issues of optimization convergence, speed and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. In particular we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation.", "histories": [["v1", "Tue, 31 Jan 2017 22:14:59 GMT  (5131kb,D)", "http://arxiv.org/abs/1702.00071v1", null], ["v2", "Fri, 3 Mar 2017 11:09:28 GMT  (4031kb,D)", "http://arxiv.org/abs/1702.00071v2", null], ["v3", "Mon, 12 Jun 2017 23:12:14 GMT  (7139kb,D)", "http://arxiv.org/abs/1702.00071v3", null], ["v4", "Thu, 12 Oct 2017 17:18:51 GMT  (8317kb,D)", "http://arxiv.org/abs/1702.00071v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["eugene vorontsov", "chiheb trabelsi", "samuel kadoury", "chris pal"], "accepted": true, "id": "1702.00071"}
