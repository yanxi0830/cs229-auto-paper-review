{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Revisiting Natural Gradient for Deep Networks", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "histories": [["v1", "Wed, 16 Jan 2013 04:47:02 GMT  (16kb)", "http://arxiv.org/abs/1301.3584v1", null], ["v2", "Tue, 22 Jan 2013 03:57:04 GMT  (374kb,D)", "http://arxiv.org/abs/1301.3584v2", null], ["v3", "Wed, 23 Jan 2013 14:59:04 GMT  (375kb,D)", "http://arxiv.org/abs/1301.3584v3", null], ["v4", "Wed, 13 Mar 2013 19:13:08 GMT  (1006kb,D)", "http://arxiv.org/abs/1301.3584v4", "The title, wording and structure of the paper was slightly changed to better reflect the final conclusions. We improved notation, providing more details where they were missing"], ["v5", "Fri, 20 Dec 2013 19:29:42 GMT  (234kb,D)", "http://arxiv.org/abs/1301.3584v5", null], ["v6", "Tue, 7 Jan 2014 18:11:36 GMT  (224kb,D)", "http://arxiv.org/abs/1301.3584v6", null], ["v7", "Mon, 17 Feb 2014 16:29:27 GMT  (228kb,D)", "http://arxiv.org/abs/1301.3584v7", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["razvan pascanu", "yoshua bengio"], "accepted": true, "id": "1301.3584"}
