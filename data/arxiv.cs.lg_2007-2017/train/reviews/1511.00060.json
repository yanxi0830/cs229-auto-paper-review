{"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2015", "title": "Top-down Tree Long Short-Term Memory Networks", "abstract": "In this paper we develop a recurrent neural network (TreeRNN), which is designed to predict a tree rather than a linear sequence as is the case in conventional recurrent neural networks. Our model defines the probability of a sentence by estimating the generation probability of its dependency tree. We construct the tree incrementally by generating the left and right dependents of a node whose probability is computed using recurrent neural networks with shared hidden layers. Application of our model to two language modeling tasks shows that it outperforms or performs on par with related models.", "histories": [["v1", "Sat, 31 Oct 2015 02:05:28 GMT  (140kb,D)", "http://arxiv.org/abs/1511.00060v1", null], ["v2", "Thu, 7 Jan 2016 00:48:42 GMT  (108kb,D)", "http://arxiv.org/abs/1511.00060v2", null], ["v3", "Sun, 3 Apr 2016 23:30:17 GMT  (109kb,D)", "http://arxiv.org/abs/1511.00060v3", "to appear in NAACL 2016; code available atthis https URL"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xingxing zhang", "liang lu", "mirella lapata"], "accepted": true, "id": "1511.00060"}
