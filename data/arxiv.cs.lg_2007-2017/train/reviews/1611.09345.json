{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Unifying Multi-Domain Multi-Task Learning: Tensor and Neural Network Perspectives", "abstract": "Multi-domain learning aims to benefit from simultaneously learning across several different but related domains. In this chapter, we propose a single framework that unifies multi-domain learning (MDL) and the related but better studied area of multi-task learning (MTL). By exploiting the concept of a \\emph{semantic descriptor} we show how our framework encompasses various classic and recent MDL/MTL algorithms as special cases with different semantic descriptor encodings. As a second contribution, we present a higher order generalisation of this framework, capable of simultaneous multi-task-multi-domain learning. This generalisation has two mathematically equivalent views in multi-linear algebra and gated neural networks respectively. Moreover, by exploiting the semantic descriptor, it provides neural networks the capability of zero-shot learning (ZSL), where a classifier is generated for an unseen class without any training data; as well as zero-shot domain adaptation (ZSDA), where a model is generated for an unseen domain without any training data. In practice, this framework provides a powerful yet easy to implement method that can be flexibly applied to MTL, MDL, ZSL and ZSDA.", "histories": [["v1", "Mon, 28 Nov 2016 20:59:18 GMT  (1309kb,D)", "http://arxiv.org/abs/1611.09345v1", "Invited book chapter"]], "COMMENTS": "Invited book chapter", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yongxin yang", "timothy m hospedales"], "accepted": false, "id": "1611.09345"}
