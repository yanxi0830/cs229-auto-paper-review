{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Segmental Recurrent Neural Networks", "abstract": "We introduce segmental recurrent neural networks (SRNNs) which define, given an input sequence, a joint probability distribution over segmentations of the input and labelings of the segments. Representations of the input segments (i.e., contiguous subsequences of the input) are computed by encoding their constituent tokens using bidirectional recurrent neural nets, and these \"segment embeddings\" are used to define compatibility scores with output labels. These local compatibility scores are integrated using a global semi-Markov conditional random field. Both fully supervised training -- in which segment boundaries and labels are observed -- as well as partially supervised training -- in which segment boundaries are latent -- are straightforward. Experiments on handwriting recognition and joint Chinese word segmentation/POS tagging show that, compared to models that do not explicitly represent segments such as BIO tagging schemes and connectionist temporal classification (CTC), SRNNs obtain substantially higher accuracies.", "histories": [["v1", "Wed, 18 Nov 2015 23:02:45 GMT  (398kb,D)", "https://arxiv.org/abs/1511.06018v1", "9 pages"], ["v2", "Tue, 1 Mar 2016 22:46:37 GMT  (405kb,D)", "http://arxiv.org/abs/1511.06018v2", "10 pages, published as a conference paper at ICLR 2016"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["lingpeng kong", "chris dyer", "noah a smith"], "accepted": true, "id": "1511.06018"}
