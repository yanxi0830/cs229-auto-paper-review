{"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Certified Defenses for Data Poisoning Attacks", "abstract": "Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our bound comes paired with a candidate attack that nearly realizes the bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.", "histories": [["v1", "Fri, 9 Jun 2017 16:26:49 GMT  (864kb,D)", "http://arxiv.org/abs/1706.03691v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["jacob steinhardt", "pang wei koh", "percy liang"], "accepted": true, "id": "1706.03691"}
