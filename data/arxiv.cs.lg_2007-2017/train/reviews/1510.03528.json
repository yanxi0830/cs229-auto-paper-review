{"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2015", "title": "$\\ell_1$-regularized Neural Networks are Improperly Learnable in Polynomial Time", "abstract": "We study the improper learning of multi-layer neural networks. Suppose that the neural network to be learned has $k$ hidden layers and that the $\\ell_1$-norm of the incoming weights of any neuron is bounded by $L$. We present a kernel-based method, such that with probability at least $1 - \\delta$, it learns a predictor whose generalization error is at most $\\epsilon$ worse than that of the neural network. The sample complexity and the time complexity of the presented method are polynomial in the input dimension and in $(1/\\epsilon,\\log(1/\\delta),F(k,L))$, where $F(k,L)$ is a function depending on $(k,L)$ and on the activation function, independent of the number of neurons. The algorithm applies to both sigmoid-like activation functions and ReLU-like activation functions. It implies that any sufficiently sparse neural network is learnable in polynomial time.", "histories": [["v1", "Tue, 13 Oct 2015 04:36:09 GMT  (290kb)", "http://arxiv.org/abs/1510.03528v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuchen zhang", "jason d lee", "michael i jordan"], "accepted": true, "id": "1510.03528"}
