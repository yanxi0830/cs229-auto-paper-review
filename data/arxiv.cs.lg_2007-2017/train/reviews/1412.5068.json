{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2014", "title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples", "abstract": "Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100% mis-classification for a state of the art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We find that DAEs can remove substantial amounts of the adversarial noise. How- ever, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial exam- ples, without a significant performance penalty.", "histories": [["v1", "Thu, 11 Dec 2014 23:03:49 GMT  (446kb)", "http://arxiv.org/abs/1412.5068v1", null], ["v2", "Wed, 17 Dec 2014 16:35:05 GMT  (390kb)", "http://arxiv.org/abs/1412.5068v2", null], ["v3", "Tue, 30 Dec 2014 14:14:24 GMT  (414kb)", "http://arxiv.org/abs/1412.5068v3", null], ["v4", "Thu, 9 Apr 2015 21:43:29 GMT  (414kb)", "http://arxiv.org/abs/1412.5068v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["shixiang gu", "luca rigazio"], "accepted": true, "id": "1412.5068"}
