{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Contextual semibandits via supervised learning oracles", "abstract": "We study a variant of the contextual bandit problem, where on each round, the learner plays a sequence of actions, receives a feature for each individual action, and reward that is linearly related to these features. This setting has applications to network routing, crowd-sourcing, personalized search, and many other domains. If the linear transformation is known, we analyze an algorithm that is structurally similar to the algorithm of Agarwal et a. [2014] and show that it enjoys a regret bound between $\\tilde{O}(\\sqrt{KLT \\ln N})$ and $\\tilde{O}(L\\sqrt{KT \\ln N})$, where $K$ is the number of actions, $L$ is the length of each action sequence, $T$ is the number of rounds, and $N$ is the number of policies. If the linear transformation is unknown, we show that an algorithm that first explores to learn the unknown weights via linear regression and thereafter uses the estimated weights can achieve $\\tilde{O}(\\|w\\|_1(KT)^{3/4} \\sqrt{\\ln N})$ regret, where $w$ is the true (unknown) weight vector. Both algorithms use an optimization oracle to avoid explicit enumeration of the policies and consequently are computationally efficient whenever an efficient algorithm for the fully supervised setting is available.", "histories": [["v1", "Fri, 20 Feb 2015 14:55:41 GMT  (39kb,D)", "http://arxiv.org/abs/1502.05890v1", null], ["v2", "Thu, 5 Mar 2015 01:38:23 GMT  (39kb,D)", "http://arxiv.org/abs/1502.05890v2", null], ["v3", "Tue, 14 Jun 2016 00:43:13 GMT  (234kb,D)", "http://arxiv.org/abs/1502.05890v3", null], ["v4", "Fri, 4 Nov 2016 19:28:07 GMT  (236kb,D)", "http://arxiv.org/abs/1502.05890v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akshay krishnamurthy", "alekh agarwal", "miroslav dud\u00edk"], "accepted": true, "id": "1502.05890"}
