{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Fixed-point optimization of deep neural networks with adaptive step size retraining", "abstract": "Fixed-point optimization of deep neural networks plays an important role in hardware based design and low-power implementations. Many deep neural networks show fairly good performance even with 2- or 3-bit precision when quantized weights are fine-tuned by retraining. We propose an improved fixedpoint optimization algorithm that estimates the quantization step size dynamically during the retraining. In addition, a gradual quantization scheme is also tested, which sequentially applies fixed-point optimizations from high- to low-precision. The experiments are conducted for feed-forward deep neural networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs).", "histories": [["v1", "Mon, 27 Feb 2017 08:00:58 GMT  (2562kb)", "http://arxiv.org/abs/1702.08171v1", "This paper is accepted in ICASSP 2017"]], "COMMENTS": "This paper is accepted in ICASSP 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sungho shin", "yoonho boo", "wonyong sung"], "accepted": false, "id": "1702.08171"}
