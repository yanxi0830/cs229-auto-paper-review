{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Input Convex Neural Networks", "abstract": "This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with only minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.", "histories": [["v1", "Thu, 22 Sep 2016 20:10:57 GMT  (2074kb,D)", "http://arxiv.org/abs/1609.07152v1", null], ["v2", "Thu, 13 Oct 2016 19:46:58 GMT  (2167kb,D)", "http://arxiv.org/abs/1609.07152v2", null], ["v3", "Wed, 14 Jun 2017 17:59:12 GMT  (2532kb,D)", "http://arxiv.org/abs/1609.07152v3", "ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["brandon amos", "lei xu", "j zico kolter"], "accepted": true, "id": "1609.07152"}
