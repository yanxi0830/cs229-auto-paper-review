{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Stochastic Modified Equations and Adaptive Stochastic Gradient Algorithms", "abstract": "Stochastic gradient algorithms (SGA) are increasingly popular in machine learning applications and have become \"the algorithm\" for extremely large scale problems. Although there are some convergence results, little is known about their dynamics. In this paper, We propose the method of stochastic modified equations (SME) to analyze the dynamics of the SGA. Using this technique, we can give precise characterizations for both the initial convergence speed and the eventual oscillations, at least in some special cases. Furthermore, the SME formalism allows us to characterize various speed-up techniques, such as introducing momentum, adjusting the learning rate and the mini-batch sizes. Previously, these techniques relied mostly on heuristics. Besides introducing simple examples to illustrate the SME formalism, we also apply the framework to improve the relaxed randomized Kaczmarz method for solving linear equations. The SME framework is a precise and unifying approach to understanding and improving the SGA, and has the potential to be applied to many more stochastic algorithms.", "histories": [["v1", "Thu, 19 Nov 2015 16:49:33 GMT  (3851kb)", "http://arxiv.org/abs/1511.06251v1", null], ["v2", "Fri, 20 Nov 2015 19:58:15 GMT  (4801kb)", "http://arxiv.org/abs/1511.06251v2", "Changes: 1. Fixed a sign mistake in eq. (74). 2. Factor of d in eq. (98), and thus figure 10's estimate of k^*. 3. Fixed some typos and figure scales"], ["v3", "Tue, 20 Jun 2017 13:56:33 GMT  (1326kb,D)", "http://arxiv.org/abs/1511.06251v3", "Major changes including a proof of the weak approximation, asymptotic expansions and application-oriented adaptive algorithms"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["qianxiao li", "cheng tai", "weinan e"], "accepted": true, "id": "1511.06251"}
