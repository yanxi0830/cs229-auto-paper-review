{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "On weight initialization in deep neural networks", "abstract": "A proper initialization of the weights in a neural network is critical to its convergence. Current insights into weight initialization come primarily from linear activation functions. In this paper, I develop a theory for weight initializations with non-linear activations. First, I derive a general weight initialization strategy for any neural network using activation functions differentiable at 0. Next, I derive the weight initialization strategy for the Rectified Linear Unit (RELU), and provide theoretical insights into why the Xavier initialization is a poor choice with RELU activations. My analysis provides a clear demonstration of the role of non-linearities in determining the proper weight initializations.", "histories": [["v1", "Fri, 28 Apr 2017 09:57:52 GMT  (248kb,D)", "http://arxiv.org/abs/1704.08863v1", "9 pages, 4 figures"], ["v2", "Tue, 2 May 2017 22:43:10 GMT  (248kb,D)", "http://arxiv.org/abs/1704.08863v2", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["siddharth krishna kumar"], "accepted": false, "id": "1704.08863"}
