{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Recursive Autoconvolution for Unsupervised Learning of Convolutional Neural Networks", "abstract": "In visual recognition tasks, supervised learning shows excellent performance. On the other hand, unsupervised learning exploits cheap unlabeled data and can help to solve the same tasks more efficiently. We show that the recursive autoconvolutional operator, adopted from physics, boosts existing unsupervised methods to learn more powerful filters. We use a well established multilayer convolutional network and train filters layer-wise. To build a stronger classifier, we design a very light committee of SVM models. The total number of trainable parameters is also greatly reduced by using shared filters in higher layers. We evaluate our networks on the MNIST, CIFAR-10 and STL-10 benchmarks and report several state of the art results among other unsupervised methods.", "histories": [["v1", "Thu, 2 Jun 2016 10:37:46 GMT  (449kb,D)", "http://arxiv.org/abs/1606.00611v1", null], ["v2", "Sun, 26 Mar 2017 18:31:05 GMT  (321kb,D)", "http://arxiv.org/abs/1606.00611v2", "8 pages, accepted to International Joint Conference on Neural Networks (IJCNN 2017)"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["boris knyazev", "erhardt barth", "thomas martinetz"], "accepted": false, "id": "1606.00611"}
