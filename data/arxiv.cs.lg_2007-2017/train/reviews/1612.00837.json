{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering", "abstract": "Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.", "histories": [["v1", "Fri, 2 Dec 2016 20:57:07 GMT  (3642kb,D)", "https://arxiv.org/abs/1612.00837v1", null], ["v2", "Fri, 14 Apr 2017 18:20:13 GMT  (3637kb,D)", "http://arxiv.org/abs/1612.00837v2", null], ["v3", "Mon, 15 May 2017 17:58:49 GMT  (3637kb,D)", "http://arxiv.org/abs/1612.00837v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG", "authors": ["yash goyal", "tejas khot", "douglas summers-stay", "dhruv batra", "devi parikh"], "accepted": false, "id": "1612.00837"}
