{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Domain Generalization via Invariant Feature Representation", "abstract": "This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.", "histories": [["v1", "Thu, 10 Jan 2013 13:29:17 GMT  (271kb)", "http://arxiv.org/abs/1301.2115v1", "The 30th International Conference on Machine Learning (ICML 2013)"]], "COMMENTS": "The 30th International Conference on Machine Learning (ICML 2013)", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["krikamol muandet", "david balduzzi", "bernhard sch\u00f6lkopf"], "accepted": true, "id": "1301.2115"}
