{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Learning Deep Representations By Distributed Random Samplings", "abstract": "In this paper, we propose an extremely simple deep model for the unsupervised nonlinear dimensionality reduction -- deep distributed random samplings, which performs like a stack of unsupervised bootstrap aggregating. First, its network structure is novel: each layer of the network is a group of mutually independent $k$-centers clusterings. Second, its learning method is extremely simple: the $k$ centers of each clustering are only $k$ randomly selected examples from the training data; for small-scale data sets, the $k$ centers are further randomly reconstructed by a simple cyclic-shift operation. Experimental results on nonlinear dimensionality reduction show that the proposed method can learn abstract representations on both large-scale and small-scale problems, and meanwhile is much faster than deep neural networks on large-scale problems.", "histories": [["v1", "Mon, 16 Dec 2013 15:40:05 GMT  (2558kb,D)", "http://arxiv.org/abs/1312.4405v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiao-lei zhang"], "accepted": false, "id": "1312.4405"}
