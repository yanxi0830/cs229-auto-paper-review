{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory.", "histories": [["v1", "Tue, 9 Feb 2016 23:24:33 GMT  (33kb)", "https://arxiv.org/abs/1602.03218v1", "9 pages, 6 figures, 2 tables"], ["v2", "Tue, 23 Feb 2016 10:22:25 GMT  (33kb)", "http://arxiv.org/abs/1602.03218v2", "Added soft attention appendix"]], "COMMENTS": "9 pages, 6 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marcin", "rychowicz", "karol kurach"], "accepted": false, "id": "1602.03218"}
