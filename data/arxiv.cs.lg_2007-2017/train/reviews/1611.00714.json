{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Scalable Semi-Supervised Learning over Networks using Nonsmooth Convex Optimization", "abstract": "We propose a scalable method for semi-supervised (transductive) learning from massive network-structured datasets. Our approach to semi-supervised learning is based on representing the underlying hypothesis as a graph signal with small total variation. Requiring a small total variation of the graph signal representing the underlying hypothesis corresponds to the central smoothness assumption that forms the basis for semi-supervised learning, i.e., input points forming clusters have similar output values or labels. We formulate the learning problem as a nonsmooth convex optimization problem which we solve by appealing to Nesterovs optimal first-order method for nonsmooth optimization. We also provide a message passing formulation of the learning method which allows for a highly scalable implementation in big data frameworks.", "histories": [["v1", "Wed, 2 Nov 2016 18:27:53 GMT  (1572kb)", "http://arxiv.org/abs/1611.00714v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["alexander jung", "alfred o hero iii", "alexandru mara", "sabeur aridhi"], "accepted": false, "id": "1611.00714"}
