{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2015", "title": "Gradual Training Method for Denoising Auto Encoders", "abstract": "Stacked denoising auto encoders (DAEs) are well known to learn useful deep representations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE layers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on MNIST and CIFAR datasets.", "histories": [["v1", "Sat, 11 Apr 2015 17:51:41 GMT  (795kb,D)", "http://arxiv.org/abs/1504.02902v1", "arXiv admin note: substantial text overlap witharXiv:1412.6257"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1412.6257", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["alexander kalmanovich", "gal chechik"], "accepted": true, "id": "1504.02902"}
