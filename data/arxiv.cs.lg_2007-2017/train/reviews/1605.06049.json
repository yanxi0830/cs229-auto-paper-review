{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "A Multi-Batch L-BFGS Method for Machine Learning", "abstract": "The question of how to parallelize the stochastic gradient descent (SGD) method has received much attention in the literature. In this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information. In order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration. This inherently gives the algorithm a stochastic flavor that can cause instability in L-BFGS, a popular batch method in machine learning. These difficulties arise because L-BFGS employs gradient differences to update the Hessian approximations; when these gradients are computed using different data points the process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases.", "histories": [["v1", "Thu, 19 May 2016 16:53:50 GMT  (1382kb,D)", "http://arxiv.org/abs/1605.06049v1", "32 pages, 22 figures"], ["v2", "Sun, 23 Oct 2016 22:48:01 GMT  (1390kb,D)", "http://arxiv.org/abs/1605.06049v2", "NIPS 2016. 31 pages, 22 figures"]], "COMMENTS": "32 pages, 22 figures", "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["albert s berahas", "jorge nocedal", "martin tak\u00e1c"], "accepted": true, "id": "1605.06049"}
