{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2014", "title": "Approximate Policy Iteration Schemes: A Comparison", "abstract": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration, Conservative Policy Iteration (CPI), a natural adaptation of the Policy Search by Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\\infty$), and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all algorithms, we describe performance bounds, and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API/API($\\alpha$), but this comes at the cost of a relative---exponential in $\\frac{1}{\\epsilon}$---increase of the number of iterations. 2) PSDP$_\\infty$ enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP$_\\infty$ is proportional to their number of iterations, which may be problematic when the discount factor $\\gamma$ is close to 1 or the approximation error $\\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis.", "histories": [["v1", "Mon, 12 May 2014 19:11:03 GMT  (4923kb,D)", "http://arxiv.org/abs/1405.2878v1", "ICML (2014)"]], "COMMENTS": "ICML (2014)", "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["bruno scherrer"], "accepted": true, "id": "1405.2878"}
