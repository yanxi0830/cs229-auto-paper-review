{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2015", "title": "Attention with Intention for a Neural Network Conversation Model", "abstract": "In a conversation or a dialogue process, attention and intention play intrinsic roles. This paper proposes a neural network based approach that models the attention and intention processes. It essentially consists of three recurrent networks. The encoder network is a word-level model representing source side sentences. The intention network is a recurrent network that models the dynamics of the intention process. The decoder network is a recurrent network produces responses to the input from the source side. It is a language model that is dependent on the intention and has an attention mechanism to attend to particular source side words, when predicting a symbol in the response. The model is trained end-to-end without labeling data. Experiments show that this model generates natural responses to user inputs.", "histories": [["v1", "Thu, 29 Oct 2015 05:31:28 GMT  (58kb,D)", "http://arxiv.org/abs/1510.08565v1", null], ["v2", "Mon, 2 Nov 2015 02:17:15 GMT  (58kb,D)", "http://arxiv.org/abs/1510.08565v2", null], ["v3", "Thu, 5 Nov 2015 07:26:01 GMT  (58kb,D)", "http://arxiv.org/abs/1510.08565v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.HC cs.LG", "authors": ["kaisheng yao", "geoffrey zweig", "baolin peng"], "accepted": false, "id": "1510.08565"}
