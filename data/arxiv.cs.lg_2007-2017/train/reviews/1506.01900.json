{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2015", "title": "Communication Complexity of Distributed Convex Learning and Optimization", "abstract": "We study the fundamental limits to communication-efficient distributed methods for convex learning and optimization, under different assumptions on the information available to individual machines, and the types of functions considered. We identify cases where existing algorithms are already worst-case optimal, as well as cases where room for further improvement is still possible. Among other things, our results indicate that without similarity between the local objective functions (due to statistical data similarity or otherwise) many communication rounds may be required, even if the machines have unbounded computational power.", "histories": [["v1", "Fri, 5 Jun 2015 13:24:17 GMT  (29kb)", "http://arxiv.org/abs/1506.01900v1", null], ["v2", "Wed, 28 Oct 2015 19:02:22 GMT  (33kb)", "http://arxiv.org/abs/1506.01900v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["yossi arjevani", "ohad shamir"], "accepted": true, "id": "1506.01900"}
