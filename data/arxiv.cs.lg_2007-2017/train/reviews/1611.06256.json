{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2016", "title": "Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU", "abstract": "We introduce and analyze the computational aspects of a hybrid CPU/GPU implementation of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. Our analysis concentrates on the critical aspects to leverage the GPU's computational power, including the introduction of a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. We also show the potential for the use of larger DNN models on a GPU. Our TensorFlow implementation achieves a significant speed up compared to our CPU-only implementation, and it will be made publicly available to other researchers.", "histories": [["v1", "Fri, 18 Nov 2016 21:34:47 GMT  (653kb,D)", "http://arxiv.org/abs/1611.06256v1", null], ["v2", "Sat, 3 Dec 2016 04:42:18 GMT  (673kb,D)", "http://arxiv.org/abs/1611.06256v2", null], ["v3", "Thu, 2 Mar 2017 19:12:19 GMT  (2705kb,D)", "http://arxiv.org/abs/1611.06256v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohammad babaeizadeh", "iuri frosio", "stephen tyree", "jason clemons", "jan kautz"], "accepted": true, "id": "1611.06256"}
