{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "Benchmarking State-of-the-Art Deep Learning Software Tools", "abstract": "Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training time. However, different tools exhibit different features and running performance when training different types of deep networks on different hardware platforms, which makes it difficult for end users to select an appropriate pair of software and hardware. In this paper, we aim to make a comparative study of the state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We benchmark the running performance of these tools with three popular types of neural networks on two CPU platforms and three GPU platforms. Our contribution is two-fold. First, for deep learning end users, our benchmarking results can serve as a guide to selecting appropriate software tool and hardware platform. Second, for deep learning software developers, our in-depth analysis points out possible future directions to further optimize the training performance.", "histories": [["v1", "Thu, 25 Aug 2016 18:48:16 GMT  (314kb,D)", "http://arxiv.org/abs/1608.07249v1", "6 pages"], ["v2", "Fri, 26 Aug 2016 06:25:05 GMT  (314kb,D)", "http://arxiv.org/abs/1608.07249v2", "6 pages"], ["v3", "Sat, 3 Sep 2016 16:40:32 GMT  (410kb,D)", "http://arxiv.org/abs/1608.07249v3", "Revision history: 1. Correct the CUDA version and re-test the experiments. 2. Revise minor difference of network configuration and delete some extra operations like dropout on AlexNet. 3. On RNN of CNTK, we remove an extra LSTM classification task which is not included in other tools and change the configuration file with \"SimpleNetworkBuilder\" to customized brain scripts"], ["v4", "Sun, 11 Sep 2016 06:13:13 GMT  (416kb,D)", "http://arxiv.org/abs/1608.07249v4", "Revision history: 1. Remedy the bug of ResNet-50 configuration in TensorFlow. 2. Change time measurement method of Caffe from \"caffe time\" to \"caffe train\". 3. Add an option of \"prefetch=true\" to configuration file of CNNs in CNTK"], ["v5", "Mon, 19 Sep 2016 07:09:07 GMT  (415kb,D)", "http://arxiv.org/abs/1608.07249v5", "Revision history: 1. Revise a bug of AlexNet configuration in TensorFlow. 2. Add an update operation in AlexNet with Torch"], ["v6", "Wed, 25 Jan 2017 09:27:52 GMT  (541kb,D)", "http://arxiv.org/abs/1608.07249v6", "Revision history: 1, Includes results of multiple GPUs. 2, Include MXNet into our evaluation. 3, Update all tools to the latest major versions. 4, Include results of real data sets: MNIST and Cifar10"], ["v7", "Fri, 17 Feb 2017 11:02:08 GMT  (336kb,D)", "http://arxiv.org/abs/1608.07249v7", "Revision history: 1. Revise ResNet-50 configuration in MXNet. 2. Add faster implementation of ResNet-56 in TensorFlow with multiple GPUs"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["shaohuai shi", "qiang wang", "pengfei xu", "xiaowen chu"], "accepted": false, "id": "1608.07249"}
