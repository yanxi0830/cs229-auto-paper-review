{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2013", "title": "Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences", "abstract": "We introduce persistent contextual neural networks (PCNNs) as a probabilistic model for learning symbolic data sequences, aimed at discovering complex algorithmic dependencies in the sequence. PCNNs are similar to recurrent neural networks but feature an architecture inspired by finite automata and a modified time evolution to better model memory effects. An effective training procedure using a gradient ascent in a metric inspired by Riemannian geometry is developed: this produces an algorithm independent from design choices such as the encoding of parameters and unit activities. This metric gradient ascent is designed to have an algorithmic cost close to backpropagation through time for sparsely connected networks.", "histories": [["v1", "Mon, 3 Jun 2013 17:36:14 GMT  (169kb)", "https://arxiv.org/abs/1306.0514v1", "Preliminary version"], ["v2", "Tue, 27 Aug 2013 16:19:13 GMT  (244kb)", "http://arxiv.org/abs/1306.0514v2", "Preliminary version. 2nd version: recurrent tensor-square differential metric added, more thorough experiments, title changed"], ["v3", "Sat, 12 Jul 2014 14:35:22 GMT  (248kb)", "http://arxiv.org/abs/1306.0514v3", "3rd version: minor changes"], ["v4", "Tue, 3 Feb 2015 18:35:36 GMT  (254kb)", "http://arxiv.org/abs/1306.0514v4", "4th version: some changes in notation, more experiments"]], "COMMENTS": "Preliminary version", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["yann ollivier"], "accepted": false, "id": "1306.0514"}
