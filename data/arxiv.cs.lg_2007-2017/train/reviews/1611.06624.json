{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Temporal Generative Adversarial Nets with Singular Value Clipping", "abstract": "In this paper we propose a generative model, the Temporal Generative Adversarial Network (TGAN), which can learn a semantic representation of unlabelled videos, and is capable of generating consistent videos. Unlike an existing GAN that generates videos with a generator consisting of 3D deconvolutional layers, our model exploits two types of generators: a temporal generator and an image generator. The temporal generator consists of 1D deconvolutional layers and outputs a set of latent variables, each of which corresponds to a frame in the generated video, and the image generator transforms them into a video with 2D deconvolutional layers. This representation allows efficient training of the network parameters. Moreover, it can handle a wider range of applications including the generation of a long sequence, frame interpolation, and the use of pre-trained models. Experimental results demonstrate the effectiveness of our method.", "histories": [["v1", "Mon, 21 Nov 2016 01:10:50 GMT  (5743kb,D)", "http://arxiv.org/abs/1611.06624v1", null], ["v2", "Mon, 24 Jul 2017 01:33:45 GMT  (6972kb,D)", "http://arxiv.org/abs/1611.06624v2", null], ["v3", "Fri, 18 Aug 2017 02:32:16 GMT  (6573kb,D)", "http://arxiv.org/abs/1611.06624v3", "to appear in ICCV 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["masaki saito", "eiichi matsumoto", "shunta saito"], "accepted": false, "id": "1611.06624"}
