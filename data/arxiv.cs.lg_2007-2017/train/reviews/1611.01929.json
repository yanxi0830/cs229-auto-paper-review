{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning", "abstract": "The commonly used Q-learning algorithm combined with function approximation induces systematic overestimations of state-action values. These systematic errors might cause instability, poor performance and sometimes divergence of learning. In this work, we present the \\textsc{Averaged Target DQN} (ADQN) algorithm, an adaptation to the DQN class of algorithms which uses a weighted average over past learned networks to reduce generalization noise variance. As a consequence, this leads to reduced overestimations, more stable learning process and improved performance. Additionally, we analyze ADQN variance reduction along trajectories and demonstrate the performance of ADQN on a toy Gridworld problem, as well as on several of the Atari 2600 games from the Arcade Learning Environment.", "histories": [["v1", "Mon, 7 Nov 2016 08:12:53 GMT  (607kb,D)", "http://arxiv.org/abs/1611.01929v1", null], ["v2", "Tue, 8 Nov 2016 08:40:02 GMT  (606kb,D)", "http://arxiv.org/abs/1611.01929v2", null], ["v3", "Wed, 8 Mar 2017 13:50:38 GMT  (4177kb,D)", "http://arxiv.org/abs/1611.01929v3", null], ["v4", "Fri, 10 Mar 2017 09:52:52 GMT  (2087kb,D)", "http://arxiv.org/abs/1611.01929v4", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["oron anschel", "nir baram", "nahum shimkin"], "accepted": true, "id": "1611.01929"}
