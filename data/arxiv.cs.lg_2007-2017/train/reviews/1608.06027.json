{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2016", "title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.39 BPC.", "histories": [["v1", "Mon, 22 Aug 2016 01:42:45 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v1", "v1, ICLR format"], ["v2", "Fri, 2 Sep 2016 17:26:02 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v2", "v2, ICLR format, added new results (feedback + zoneout)"], ["v3", "Mon, 5 Sep 2016 04:42:02 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v3", "ICLR format, added new results (feedback + zoneout)"], ["v4", "Wed, 19 Oct 2016 04:32:46 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v4", "ICLR 2017 submission, fixed some equations"]], "COMMENTS": "v1, ICLR format", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["kamil m rocki"], "accepted": false, "id": "1608.06027"}
