{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2014", "title": "Learning with Incremental Iterative Regularization", "abstract": "We study the learning algorithm corresponding to the incremental gradient descent defined by the empirical risk over an infinite dimensional hypotheses space. We consider a statistical learning setting and show that, provided with a universal step-size and a suitable early stopping rule, the learning algorithm thus obtained is universally consistent and derive finite sample bounds. Our results provide a theoretical foundation for considering early stopping in online learning algorithms and shed light on the effect of allowing for multiple passes over the data.", "histories": [["v1", "Wed, 30 Apr 2014 21:48:34 GMT  (58kb)", "http://arxiv.org/abs/1405.0042v1", "23 pages,1 figure"], ["v2", "Mon, 15 Jun 2015 13:12:12 GMT  (140kb,D)", "http://arxiv.org/abs/1405.0042v2", "30 pages"]], "COMMENTS": "23 pages,1 figure", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC math.PR", "authors": ["lorenzo rosasco", "silvia villa"], "accepted": true, "id": "1405.0042"}
