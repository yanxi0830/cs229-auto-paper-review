{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "FitNets: Hints for Thin Deep Nets", "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.", "histories": [["v1", "Fri, 19 Dec 2014 22:40:51 GMT  (124kb)", "http://arxiv.org/abs/1412.6550v1", null], ["v2", "Fri, 9 Jan 2015 20:56:15 GMT  (124kb)", "http://arxiv.org/abs/1412.6550v2", null], ["v3", "Fri, 27 Feb 2015 18:44:36 GMT  (126kb)", "http://arxiv.org/abs/1412.6550v3", null], ["v4", "Fri, 27 Mar 2015 11:52:28 GMT  (132kb)", "http://arxiv.org/abs/1412.6550v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["adriana romero", "nicolas ballas", "samira ebrahimi kahou", "antoine chassang", "carlo gatta", "yoshua bengio"], "accepted": true, "id": "1412.6550"}
