{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Efficient Distributed Learning with Sparsity", "abstract": "We propose a novel, efficient approach for distributed sparse learning in high-dimensions, where observations are randomly partitioned across machines. Computationally, at each round our method only requires the master machine to solve a shifted ell_1 regularized M-estimation problem, and other workers to compute the gradient. In respect of communication, the proposed approach provably matches the estimation error bound of centralized methods within constant rounds of communications (ignoring logarithmic factors). We conduct extensive experiments on both simulated and real world datasets, and demonstrate encouraging performances on high-dimensional regression and classification tasks.", "histories": [["v1", "Wed, 25 May 2016 18:15:43 GMT  (493kb,D)", "http://arxiv.org/abs/1605.07991v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jialei wang", "mladen kolar", "nathan srebro", "tong zhang 0001"], "accepted": true, "id": "1605.07991"}
