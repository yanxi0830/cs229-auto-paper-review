{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2016", "title": "Learning Shared Representations in Multi-task Reinforcement Learning", "abstract": "We investigate a paradigm in multi-task reinforcement learning (MT-RL) in which an agent is placed in an environment and needs to learn to perform a series of tasks, within this space. Since the environment does not change, there is potentially a lot of common ground amongst tasks and learning to solve them individually seems extremely wasteful. In this paper, we explicitly model and learn this shared structure as it arises in the state-action value space. We will show how one can jointly learn optimal value-functions by modifying the popular Value-Iteration and Policy-Iteration procedures to accommodate this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization. this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization.", "histories": [["v1", "Mon, 7 Mar 2016 13:03:30 GMT  (5333kb,D)", "http://arxiv.org/abs/1603.02041v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["diana borsa", "thore graepel", "john shawe-taylor"], "accepted": false, "id": "1603.02041"}
