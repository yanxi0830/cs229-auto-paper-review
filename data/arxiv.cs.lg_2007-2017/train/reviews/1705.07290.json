{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Deep Sparse Coding Using Optimized Linear Expansion of Thresholds", "abstract": "We address the problem of reconstructing sparse signals from noisy and compressive measurements using a feed-forward deep neural network (DNN) with an architecture motivated by the iterative shrinkage-thresholding algorithm (ISTA). We maintain the weights and biases of the network links as prescribed by ISTA and model the nonlinear activation function using a linear expansion of thresholds (LET), which has been very successful in image denoising and deconvolution. The optimal set of coefficients of the parametrized activation is learned over a training dataset containing measurement-sparse signal pairs, corresponding to a fixed sensing matrix. For training, we develop an efficient second-order algorithm, which requires only matrix-vector product computations in every training epoch (Hessian-free optimization) and offers superior convergence performance than gradient-descent optimization. Subsequently, we derive an improved network architecture inspired by FISTA, a faster version of ISTA, to achieve similar signal estimation performance with about 50% of the number of layers. The resulting architecture turns out to be a deep residual network, which has recently been shown to exhibit superior performance in several visual recognition tasks. Numerical experiments demonstrate that the proposed DNN architectures lead to 3 to 4 dB improvement in the reconstruction signal-to-noise ratio (SNR), compared with the state-of-the-art sparse coding algorithms.", "histories": [["v1", "Sat, 20 May 2017 11:14:39 GMT  (3497kb,D)", "http://arxiv.org/abs/1705.07290v1", "Submission date: November 11, 2016. 19 pages; 9 figures"]], "COMMENTS": "Submission date: November 11, 2016. 19 pages; 9 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["debabrata mahapatra", "subhadip mukherjee", "chandra sekhar seelamantula"], "accepted": false, "id": "1705.07290"}
