{"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation", "abstract": "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We also observe that the quality of the multilingual character-level translation even surpasses the models trained and tuned on one language pair, namely on CS-EN, FI-EN and RU-EN.", "histories": [["v1", "Mon, 10 Oct 2016 18:19:34 GMT  (380kb,D)", "http://arxiv.org/abs/1610.03017v1", "14 pages, 2 figures"], ["v2", "Tue, 1 Nov 2016 17:51:32 GMT  (415kb,D)", "http://arxiv.org/abs/1610.03017v2", "15 pages, 2 figures"], ["v3", "Tue, 13 Jun 2017 03:32:34 GMT  (326kb,D)", "http://arxiv.org/abs/1610.03017v3", "Transactions of the Association for Computational Linguistics (TACL), 2017"]], "COMMENTS": "14 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jason lee", "kyunghyun cho", "thomas hofmann"], "accepted": true, "id": "1610.03017"}
