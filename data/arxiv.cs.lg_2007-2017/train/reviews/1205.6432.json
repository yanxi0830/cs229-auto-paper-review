{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2012", "title": "Multiclass Learning Approaches: A Theoretical Comparison with Implications", "abstract": "We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \\emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error.", "histories": [["v1", "Tue, 29 May 2012 17:40:04 GMT  (29kb,D)", "https://arxiv.org/abs/1205.6432v1", null], ["v2", "Fri, 1 Jun 2012 14:12:58 GMT  (29kb,D)", "http://arxiv.org/abs/1205.6432v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit daniely", "sivan sabato", "shai shalev-shwartz"], "accepted": true, "id": "1205.6432"}
