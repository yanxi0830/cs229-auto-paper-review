{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Progressive Neural Networks", "abstract": "Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.", "histories": [["v1", "Wed, 15 Jun 2016 08:20:51 GMT  (2325kb,D)", "http://arxiv.org/abs/1606.04671v1", null], ["v2", "Tue, 21 Jun 2016 22:03:05 GMT  (7786kb,D)", "http://arxiv.org/abs/1606.04671v2", null], ["v3", "Wed, 7 Sep 2016 10:59:12 GMT  (7784kb,D)", "http://arxiv.org/abs/1606.04671v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrei a rusu", "neil c rabinowitz", "guillaume desjardins", "hubert soyer", "james kirkpatrick", "koray kavukcuoglu", "razvan pascanu", "raia hadsell"], "accepted": false, "id": "1606.04671"}
