{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Teaching Compositionality to CNNs", "abstract": "Convolutional neural networks (CNNs) have shown great success in computer vision, approaching human-level performance when trained for specific tasks via application-specific loss functions. In this paper, we propose a method for augmenting and training CNNs so that their learned features are compositional. It encourages networks to form representations that disentangle objects from their surroundings and from each other, thereby promoting better generalization. Our method is agnostic to the specific details of the underlying CNN to which it is applied and can in principle be used with any CNN. As we show in our experiments, the learned representations lead to feature activations that are more localized and improve performance over non-compositional baselines in object recognition tasks.", "histories": [["v1", "Wed, 14 Jun 2017 04:34:59 GMT  (9418kb,D)", "http://arxiv.org/abs/1706.04313v1", "Preprint appearing in CVPR 2017"]], "COMMENTS": "Preprint appearing in CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["austin stone", "huayan wang", "michael stark", "yi liu", "d scott phoenix", "dileep george"], "accepted": false, "id": "1706.04313"}
