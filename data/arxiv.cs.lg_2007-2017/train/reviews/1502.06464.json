{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2015", "title": "Rectified Factor Networks", "abstract": "We propose rectified factor networks (RFNs) as generative unsupervised models, which learn robust, very sparse, and non-linear codes with many code units. RFN learning is a variational expectation maximization (EM) algorithm with unknown prior which includes (i) rectified posterior means, (ii) normalized signals of hidden units, and (iii) dropout. Like factor analysis, RFNs explain the data variance by their parameters. For pretraining of deep networks on MNIST, rectangle data, convex shapes, NORB, and CIFAR, RFNs were superior to restricted Boltzmann machines (RBMs) and denoising autoencoders. On CIFAR-10 and CIFAR-100, RFN pretraining always improved the results of deep networks for different architectures like AlexNet, deep supervised net (DSN), and a simple \"Network In Network\" architecture. With RFNs success is guaranteed.", "histories": [["v1", "Mon, 23 Feb 2015 15:44:37 GMT  (1237kb,D)", "https://arxiv.org/abs/1502.06464v1", "10 pages + 30 pages supplement"], ["v2", "Thu, 11 Jun 2015 21:27:53 GMT  (3345kb,D)", "http://arxiv.org/abs/1502.06464v2", "9 pages + 49 pages supplement"]], "COMMENTS": "10 pages + 30 pages supplement", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE stat.ML", "authors": ["djork-arn\u00e9 clevert", "andreas mayr", "thomas unterthiner", "sepp hochreiter"], "accepted": true, "id": "1502.06464"}
