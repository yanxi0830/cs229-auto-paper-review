{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2016", "title": "Reward Augmented Maximum Likelihood for Neural Structured Prediction", "abstract": "A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. We establish a connection between the log-likelihood and regularized expected reward objectives, showing that at a zero temperature, they are approximately equivalent in the vicinity of the optimal solution. We show that optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated (temperature adjusted) rewards. Based on this observation, we optimize conditional log-probability of edited outputs that are sampled proportionally to their scaled exponentiated reward. We apply this framework to optimize edit distance in the output label space. Experiments on speech recognition and machine translation for neural sequence to sequence models show notable improvements over a maximum likelihood baseline by using edit distance augmented maximum likelihood.", "histories": [["v1", "Thu, 1 Sep 2016 09:00:19 GMT  (69kb)", "http://arxiv.org/abs/1609.00150v1", "NIPS 2016"], ["v2", "Sat, 10 Sep 2016 01:07:04 GMT  (55kb)", "http://arxiv.org/abs/1609.00150v2", "NIPS 2016"], ["v3", "Wed, 4 Jan 2017 18:10:36 GMT  (48kb)", "http://arxiv.org/abs/1609.00150v3", "NIPS 2016"]], "COMMENTS": "NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohammad norouzi", "samy bengio", "zhifeng chen", "navdeep jaitly", "mike schuster", "yonghui wu", "dale schuurmans"], "accepted": true, "id": "1609.00150"}
