{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Heavy-Tailed Analogues of the Covariance Matrix for ICA", "abstract": "Independent Component Analysis (ICA) is the problem of learning a square matrix $A$, given samples of $X=AS$, where $S$ is a random vector with independent coordinates. Most existing algorithms are provably efficient only when each $S_i$ has finite and moderately valued fourth moment. However, there are practical applications where this assumption need not be true, such as speech and finance. Algorithms have been proposed for heavy-tailed ICA, but they are not practical, using random walks and the full power of the ellipsoid algorithm multiple times. The main contributions of this paper are:", "histories": [["v1", "Wed, 22 Feb 2017 19:27:38 GMT  (909kb,D)", "http://arxiv.org/abs/1702.06976v1", "16 Pages, 9 Figures, AAAI 2017"]], "COMMENTS": "16 Pages, 9 Figures, AAAI 2017", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["joseph anderson", "navin goyal", "anupama nandi", "luis rademacher"], "accepted": true, "id": "1702.06976"}
