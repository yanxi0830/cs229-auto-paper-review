{"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Learning both Weights and Connections for Efficient Neural Networks", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy, by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG16 found that the network as a whole can be reduced 6.8x just by pruning the fully-connected layers, again with no loss of accuracy.", "histories": [["v1", "Mon, 8 Jun 2015 19:28:43 GMT  (2968kb,D)", "http://arxiv.org/abs/1506.02626v1", null], ["v2", "Wed, 29 Jul 2015 22:27:31 GMT  (1484kb,D)", "http://arxiv.org/abs/1506.02626v2", null], ["v3", "Fri, 30 Oct 2015 23:29:27 GMT  (1075kb,D)", "http://arxiv.org/abs/1506.02626v3", "Published as a conference paper at NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["song han", "jeff pool", "john tran", "william j dally"], "accepted": true, "id": "1506.02626"}
