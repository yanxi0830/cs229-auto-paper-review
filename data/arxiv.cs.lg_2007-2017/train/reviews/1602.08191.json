{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2016", "title": "DeepSpark: A Spark-Based Distributed Deep Learning Framework for Commodity Clusters", "abstract": "The increasing complexity of deep neural networks (DNNs) has made it challenging to exploit existing large-scale data process pipelines for handling massive data and parameters involved in DNN training. Distributed computing platforms and GPGPU-based acceleration provide a mainstream solution to this computational challenge. In this paper, we propose DeepSpark, a distributed and parallel deep learning framework that simultaneously exploits Apache Spark for large-scale distributed data management and Caffe for GPU-based acceleration. DeepSpark directly accepts Caffe input specifications, providing seamless compatibility with existing designs and network structures. To support parallel operations, DeepSpark automatically distributes workloads and parameters to Caffe-running nodes using Spark and iteratively aggregates training results by a novel lock-free asynchronous variant of the popular elastic averaging stochastic gradient descent (SGD) update scheme, effectively complementing the synchronized processing capabilities of Spark. DeepSpark is an on-going project, and the current release is available at", "histories": [["v1", "Fri, 26 Feb 2016 04:18:21 GMT  (3754kb,D)", "http://arxiv.org/abs/1602.08191v1", null], ["v2", "Tue, 8 Mar 2016 08:32:16 GMT  (3755kb,D)", "http://arxiv.org/abs/1602.08191v2", null], ["v3", "Sat, 1 Oct 2016 02:44:07 GMT  (439kb,D)", "http://arxiv.org/abs/1602.08191v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hanjoo kim", "jaehong park", "jaehee jang", "sungroh yoon"], "accepted": false, "id": "1602.08191"}
