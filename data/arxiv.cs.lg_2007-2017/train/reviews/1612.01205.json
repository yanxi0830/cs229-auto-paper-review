{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2016", "title": "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits", "abstract": "We consider the problem of off-policy evaluation---estimating the value of a target policy using data collected by another policy---under the contextual bandit model. We establish a minimax lower bound on the mean squared error (MSE), and show that it is matched up to constant factors by the inverse propensity scoring (IPS) estimator. Since in the multi-armed bandit problem the IPS is suboptimal (Li et. al, 2015), our result highlights the difficulty of the contextual setting with non-degenerate context distributions. We further consider improvements on this minimax MSE bound, given access to a reward model. We show that the existing doubly robust approach, which utilizes such a reward model, may continue to suffer from high variance even when the reward model is perfect. We propose a new estimator called SWITCH which more effectively uses the reward model and achieves a superior bias-variance tradeoff compared with prior work. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of datasets, often seeing orders of magnitude improvements over a number of baselines.", "histories": [["v1", "Sun, 4 Dec 2016 23:24:17 GMT  (171kb,D)", "http://arxiv.org/abs/1612.01205v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yu-xiang wang", "alekh agarwal", "miroslav dud\u00edk"], "accepted": true, "id": "1612.01205"}
