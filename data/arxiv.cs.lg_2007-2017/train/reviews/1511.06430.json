{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Deconstructing the Ladder Network Architecture", "abstract": "Manual labeling of data is and will remain a costly endeavor. For this reason, semi-supervised learning remains a topic of practical importance. The recently proposed Ladder Network is one such approach that has proven to be very successful. In addition to the supervised objective, the Ladder Network also adds an unsupervised objective corresponding to the reconstruction costs of a stack of denoising autoencoders. Although the results are impressive, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. In order to help elucidate and disentangle the different ingredients in the Ladder Network recipe, this paper presents an extensive experimental investigation of variants of the Ladder Network in which we replace or remove individual components to gain more insight into their relative importance. We find that all of the components are necessary for achieving optimal performance, but they do not contribute equally. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connection, followed by the application of noise, and finally the choice of combinator function in the decoder path. We also find that as the number of labeled training examples increases, the lateral connections and reconstruction criterion become less important, with most of the improvement in generalization due to the injection of noise in each layer. Furthermore, we present a new type of combinator functions that outperforms the original design in both fully- and semi-supervised tasks, reducing record test error rates on Permutation-Invariant MNIST to 0.57% for supervised setting, and to 0.97% and 1.0% for semi-supervised settings with 1000 and 100 labeled examples respectively.", "histories": [["v1", "Thu, 19 Nov 2015 22:45:20 GMT  (972kb,D)", "http://arxiv.org/abs/1511.06430v1", "15 pages"], ["v2", "Fri, 27 Nov 2015 18:17:44 GMT  (1076kb,D)", "http://arxiv.org/abs/1511.06430v2", null], ["v3", "Tue, 5 Jan 2016 09:23:24 GMT  (936kb,D)", "http://arxiv.org/abs/1511.06430v3", null], ["v4", "Tue, 24 May 2016 15:53:23 GMT  (825kb,D)", "http://arxiv.org/abs/1511.06430v4", "Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohammad pezeshki", "linxi fan", "philemon brakel", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1511.06430"}
