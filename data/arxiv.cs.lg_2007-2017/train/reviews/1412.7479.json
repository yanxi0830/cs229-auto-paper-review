{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2014", "title": "Deep Networks With Large Output Spaces", "abstract": "Deep neural networks have been extremely successful at various image, speech, video recognition tasks because of their ability to model deep structures within the data. However, they are still prohibitively expensive to train and apply for problems containing millions of classes in the output layer. Based on the observation that the key computation common to most neural network layers is a vector/matrix product, we propose a fast locality-sensitive hashing technique to approximate the actual dot product enabling us to scale up the training and inference to millions of output classes. We evaluate our technique on three diverse large-scale recognition tasks and show that our approach can train large-scale models at a faster rate (in terms of steps/total time) compared to baseline methods.", "histories": [["v1", "Tue, 23 Dec 2014 19:22:59 GMT  (1202kb)", "https://arxiv.org/abs/1412.7479v1", null], ["v2", "Mon, 29 Dec 2014 18:45:36 GMT  (594kb)", "http://arxiv.org/abs/1412.7479v2", null], ["v3", "Sat, 28 Feb 2015 01:12:58 GMT  (709kb)", "http://arxiv.org/abs/1412.7479v3", null], ["v4", "Fri, 10 Apr 2015 19:53:21 GMT  (711kb)", "http://arxiv.org/abs/1412.7479v4", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sudheendra vijayanarasimhan", "jonathon shlens", "rajat monga", "jay yagnik"], "accepted": true, "id": "1412.7479"}
