{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Deep Speech: Scaling up end-to-end speech recognition", "abstract": "We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called DeepSpeech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.5% error on the full test set. DeepSpeech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.", "histories": [["v1", "Wed, 17 Dec 2014 20:39:45 GMT  (333kb,D)", "http://arxiv.org/abs/1412.5567v1", null], ["v2", "Fri, 19 Dec 2014 21:36:13 GMT  (333kb,D)", "http://arxiv.org/abs/1412.5567v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["awni hannun", "carl case", "jared casper", "bryan catanzaro", "greg diamos", "erich elsen", "ryan prenger", "sanjeev satheesh", "shubho sengupta", "adam coates", "andrew y ng"], "accepted": false, "id": "1412.5567"}
