{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2015", "title": "Monotonic Calibrated Interpolated Look-Up Tables", "abstract": "Real-world machine learning applications may require functions that are fast-to-evaluate and interpretable. In particular, guaranteed monotonicity of the learned function can be critical to user trust. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to train monotonic look-up tables by solving a convex problem with appropriate linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large-scale learning through parallelization, mini-batching, and propose random sampling of additive regularizer terms. Experiments on seven real-world problems with five to sixteen features and thousands to millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy on practical problems while providing greater transparency to users.", "histories": [["v1", "Sat, 23 May 2015 20:57:58 GMT  (1000kb,D)", "https://arxiv.org/abs/1505.06378v1", null], ["v2", "Tue, 3 Nov 2015 21:49:53 GMT  (1007kb,D)", "http://arxiv.org/abs/1505.06378v2", "To appear (with minor revisions), Journal Machine Learning Research 2016"], ["v3", "Wed, 20 Jan 2016 22:54:21 GMT  (1300kb,D)", "http://arxiv.org/abs/1505.06378v3", "To appear (with minor revisions), Journal Machine Learning Research 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maya gupta", "rew cotter", "jan pfeifer", "konstantin voevodski", "kevin canini", "alexander mangylov", "wojtek moczydlowski", "alex van esbroeck"], "accepted": false, "id": "1505.06378"}
