{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks", "abstract": "We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.", "histories": [["v1", "Mon, 8 Jun 2015 19:01:33 GMT  (452kb,D)", "http://arxiv.org/abs/1506.02617v1", "12 pages, 5 figures"]], "COMMENTS": "12 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE stat.ML", "authors": ["behnam neyshabur", "ruslan salakhutdinov", "nathan srebro"], "accepted": true, "id": "1506.02617"}
