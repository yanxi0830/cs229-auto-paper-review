{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2015", "title": "Distilling Word Embeddings: An Encoding Approach", "abstract": "Distilling knowledge from a well-trained cumbersome network to a small one has become a new research topic recently, as lightweight neural networks with high performance are particularly in need in various resource-restricted systems. This paper addresses the problem of distilling embeddings for NLP tasks. We propose an encoding approach to distill task-specific knowledge from high-dimensional embeddings, which can retain high performance and reduce model complexity to a large extent. Experimental results show our method is better than directly training neural networks with small embeddings.", "histories": [["v1", "Mon, 15 Jun 2015 06:30:36 GMT  (32kb,D)", "https://arxiv.org/abs/1506.04488v1", null], ["v2", "Sun, 24 Jul 2016 16:22:09 GMT  (152kb,D)", "http://arxiv.org/abs/1506.04488v2", "Accepted by CIKM-16 as a short paper, and by the Representation Learning for Natural Language Processing (RL4NLP) Workshop @ACL-16 for presentation"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["lili mou", "ran jia", "yan xu", "ge li", "lu zhang", "zhi jin"], "accepted": false, "id": "1506.04488"}
