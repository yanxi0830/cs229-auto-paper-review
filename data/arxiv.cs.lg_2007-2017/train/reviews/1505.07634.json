{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2015", "title": "Learning with Symmetric Label Noise: The Importance of Being Unhinged", "abstract": "Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the SLN-robustness of the unhinged loss.", "histories": [["v1", "Thu, 28 May 2015 10:38:56 GMT  (419kb,D)", "http://arxiv.org/abs/1505.07634v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["brendan van rooyen", "aditya krishna menon", "robert c williamson"], "accepted": true, "id": "1505.07634"}
