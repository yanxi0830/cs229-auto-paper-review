{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Outperforming Word2Vec on Analogy Tasks with Random Projections", "abstract": "We present a distributed vector representation based on a simplification of the BEAGLE system, designed in the context of the Sigma cognitive architecture. Our method does not require gradient-based training of neural networks, matrix decompositions as with LSA, or convolutions as with BEAGLE. All that is involved is a sum of random vectors and their pointwise products. Despite the simplicity of this technique, it gives state-of-the-art results on analogy problems, in most cases better than Word2Vec. To explain this success, we interpret it as a dimension reduction via random projection.", "histories": [["v1", "Sat, 20 Dec 2014 07:07:29 GMT  (24kb)", "http://arxiv.org/abs/1412.6616v1", "Submission for ICLR 2015"], ["v2", "Tue, 17 Feb 2015 17:31:58 GMT  (0kb,I)", "http://arxiv.org/abs/1412.6616v2", "This paper has been withdrawn due to problems pointed out in review"]], "COMMENTS": "Submission for ICLR 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["abram demski", "volkan ustun", "paul rosenbloom", "cody kommers"], "accepted": false, "id": "1412.6616"}
