{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2014", "title": "Learning with Pseudo-Ensembles", "abstract": "We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et. al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.", "histories": [["v1", "Tue, 16 Dec 2014 02:55:05 GMT  (321kb,D)", "http://arxiv.org/abs/1412.4864v1", "To appear in Advances in Neural Information Processing Systems 27 (NIPS 2014), Advances in Neural Information Processing Systems 27, Dec. 2014"]], "COMMENTS": "To appear in Advances in Neural Information Processing Systems 27 (NIPS 2014), Advances in Neural Information Processing Systems 27, Dec. 2014", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["philip bachman", "ouais alsharif", "doina precup"], "accepted": true, "id": "1412.4864"}
