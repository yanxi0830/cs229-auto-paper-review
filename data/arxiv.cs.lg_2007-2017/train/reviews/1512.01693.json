{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2015", "title": "Deep Attention Recurrent Q-Network", "abstract": "A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels. Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN). We present an extension of DQN by \"soft\" and \"hard\" attention mechanisms. Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN. Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.", "histories": [["v1", "Sat, 5 Dec 2015 18:35:40 GMT  (126kb,D)", "http://arxiv.org/abs/1512.01693v1", "7 pages, 5 figures, Deep Reinforcement Learning Workshop, NIPS 2015"]], "COMMENTS": "7 pages, 5 figures, Deep Reinforcement Learning Workshop, NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ivan sorokin", "alexey seleznev", "mikhail pavlov", "aleksandr fedorov", "anastasiia ignateva"], "accepted": false, "id": "1512.01693"}
