{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2011", "title": "From Bandits to Experts: On the Value of Side-Observations", "abstract": "We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node $i$ is linked to node $j$ if sampling $i$ provides information on the reward of $j$. This setting naturally interpolates between the well-known \"experts\" setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, as well as partially-matching lower bounds. The regret depends on non-trivial graph theoretic properties of the information feedback structure, and reveals an interesting trade-off between regret optimality and computational efficiency.", "histories": [["v1", "Mon, 13 Jun 2011 13:11:33 GMT  (84kb,D)", "http://arxiv.org/abs/1106.2436v1", null], ["v2", "Tue, 14 Jun 2011 22:33:57 GMT  (85kb,D)", "http://arxiv.org/abs/1106.2436v2", null], ["v3", "Tue, 25 Oct 2011 15:55:47 GMT  (87kb,D)", "http://arxiv.org/abs/1106.2436v3", "Presented at the NIPS 2011 conference"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["shie mannor", "ohad shamir"], "accepted": true, "id": "1106.2436"}
