{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2012", "title": "The Role of Weight Shrinking in Large Margin Perceptron Learning", "abstract": "We introduce into the classical perceptron algorithm with margin a mechanism that shrinks the current weight vector as a first step of the update. If the shrinking factor is constant the resulting algorithm may be regarded as a margin-error-driven version of NORMA with constant learning rate. In this case we show that the allowed strength of shrinking depends on the value of the maximum margin. We also consider variable shrinking factors for which there is no such dependence. In both cases we obtain new generalizations of the perceptron with margin able to provably attain in a finite number of steps any desirable approximation of the maximal margin hyperplane. The new approximate maximum margin classifiers appear experimentally to be very competitive in 2-norm soft margin tasks involving linear kernels.", "histories": [["v1", "Mon, 21 May 2012 19:19:49 GMT  (106kb)", "https://arxiv.org/abs/1205.4698v1", "15 pages"], ["v2", "Thu, 7 Feb 2013 19:10:14 GMT  (107kb)", "http://arxiv.org/abs/1205.4698v2", "15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["constantinos panagiotakopoulos", "petroula tsampouka"], "accepted": false, "id": "1205.4698"}
