{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Convergent Tree-Backup and Retrace with Function Approximation", "abstract": "Off-policy learning is key to scaling up reinforcement learning as it allows to learn about a target policy from the experience generated by a different behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping in a way that leads to both stable and efficient algorithms. In this paper, we show that the Tree Backup and Retrace algorithms are unstable with linear function approximation, both in theory and with specific examples. Based on our analysis, we then derive stable and efficient gradient-based algorithms, compatible with accumulating or Dutch traces, using a novel methodology based on proximal methods. In addition to convergence proofs, we provide sample-complexity bounds.", "histories": [["v1", "Thu, 25 May 2017 18:37:55 GMT  (1095kb,D)", "http://arxiv.org/abs/1705.09322v1", "NIPS 2017 submission"]], "COMMENTS": "NIPS 2017 submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ahmed touati", "pierre-luc bacon", "doina precup", "pascal vincent"], "accepted": false, "id": "1705.09322"}
