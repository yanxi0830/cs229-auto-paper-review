{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2014", "title": "Indian Buffet Process Deep Generative Models", "abstract": "Denoising autoencoders (DAs) are typically applied to relatively large datasets for unsupervised learning of representative data encodings; they rely on the idea of making the learned representations robust to partial corruption of the input pattern, and perform learning using stochastic gradient descent with relatively large datasets. In this paper, we present a fully Bayesian DA architecture that allows for the application of DAs even when data is scarce. Our novel approach formulates the signal encoding problem under a nonparametric Bayesian regard, considering a Gaussian process prior over the latent input encodings generated given the (corrupt) input observations. Subsequently, the decoder modules of our model are formulated as large-margin regression models, treated under the Bayesian inference paradigm, by exploiting the maximum entropy discrimination (MED) framework. We exhibit the effectiveness of our approach using several datasets, dealing with both classification and transfer learning applications.", "histories": [["v1", "Fri, 14 Feb 2014 10:44:48 GMT  (189kb)", "http://arxiv.org/abs/1402.3427v1", null], ["v2", "Sun, 30 Aug 2015 18:41:41 GMT  (0kb,I)", "http://arxiv.org/abs/1402.3427v2", "This paper has been withdrawn by the author due to errors in the experiments (software bugs)"], ["v3", "Fri, 8 Jul 2016 19:37:54 GMT  (124kb,D)", "http://arxiv.org/abs/1402.3427v3", null], ["v4", "Sat, 16 Jul 2016 14:11:47 GMT  (124kb,D)", "http://arxiv.org/abs/1402.3427v4", null], ["v5", "Sun, 6 Aug 2017 21:27:48 GMT  (245kb,D)", "http://arxiv.org/abs/1402.3427v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sotirios p chatzis"], "accepted": false, "id": "1402.3427"}
