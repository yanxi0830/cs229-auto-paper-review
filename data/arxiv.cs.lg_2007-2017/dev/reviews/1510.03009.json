{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2015", "title": "Neural Networks with Few Multiplications", "abstract": "For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardware-friendly training of neural networks.", "histories": [["v1", "Sun, 11 Oct 2015 04:32:39 GMT  (85kb,D)", "http://arxiv.org/abs/1510.03009v1", "7 pages, 3 figures"], ["v2", "Mon, 9 Nov 2015 20:16:10 GMT  (111kb,D)", "http://arxiv.org/abs/1510.03009v2", "Submitted to ICLR 2016, 8 pages, 3 figures; More descriptions, add pseudo-code, and more experiments"], ["v3", "Fri, 26 Feb 2016 05:24:30 GMT  (111kb,D)", "http://arxiv.org/abs/1510.03009v3", "Published as a conference paper at ICLR 2016. 9 pages, 3 figures"]], "COMMENTS": "7 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["zhouhan lin", "matthieu courbariaux", "roland memisevic", "yoshua bengio"], "accepted": true, "id": "1510.03009"}
