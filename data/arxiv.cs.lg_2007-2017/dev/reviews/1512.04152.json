{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Fighting Bandits with a New Kind of Smoothness", "abstract": "We define a novel family of algorithms for the adversarial multi-armed bandit problem, and provide a simple analysis technique based on convex smoothing. We prove two main results. First, we show that regularization via the \\emph{Tsallis entropy}, which includes EXP3 as a special case, achieves the $\\Theta(\\sqrt{TN})$ minimax regret. Second, we show that a wide class of perturbation methods achieve a near-optimal regret as low as $O(\\sqrt{TN \\log N})$ if the perturbation distribution has a bounded hazard rate. For example, the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this key property.", "histories": [["v1", "Mon, 14 Dec 2015 01:57:02 GMT  (28kb,D)", "http://arxiv.org/abs/1512.04152v1", "In Proceedings of NIPS, 2015"]], "COMMENTS": "In Proceedings of NIPS, 2015", "reviews": [], "SUBJECTS": "cs.LG cs.GT stat.ML", "authors": ["jacob d abernethy", "chansoo lee", "ambuj tewari"], "accepted": true, "id": "1512.04152"}
