{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2016", "title": "Distributed stochastic optimization for deep learning (thesis)", "abstract": "We study the problem of how to distribute the training of large-scale deep learning models in the parallel computing environment. We propose a new distributed stochastic optimization method called Elastic Averaging SGD (EASGD). We analyze the convergence rate of the EASGD method in the synchronous scenario and compare its stability condition with the existing ADMM method in the round-robin scheme. An asynchronous and momentum variant of the EASGD method is applied to train deep convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Our approach accelerates the training and furthermore achieves better test accuracy. It also requires a much smaller amount of communication than other common baseline approaches such as the DOWNPOUR method.", "histories": [["v1", "Sat, 7 May 2016 16:55:22 GMT  (4615kb,D)", "http://arxiv.org/abs/1605.02216v1", "This is the author's thesis at under supervision of Yann LeCun. Part of the results are based on the paperarXiv:1412.6651in collaboration with Anna Choromanska and Yann LeCun"]], "COMMENTS": "This is the author's thesis at under supervision of Yann LeCun. Part of the results are based on the paperarXiv:1412.6651in collaboration with Anna Choromanska and Yann LeCun", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sixin zhang"], "accepted": false, "id": "1605.02216"}
