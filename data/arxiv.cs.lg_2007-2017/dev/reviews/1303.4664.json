{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2013", "title": "Large-Scale Learning with Less RAM via Randomization", "abstract": "We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this reduces RAM usage by more than 50% during training and by up to 95% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement per-coordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs.", "histories": [["v1", "Tue, 19 Mar 2013 17:00:22 GMT  (59kb,D)", "http://arxiv.org/abs/1303.4664v1", "Extended version of ICML 2013 paper"]], "COMMENTS": "Extended version of ICML 2013 paper", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel golovin", "d sculley", "h brendan mcmahan", "michael young"], "accepted": true, "id": "1303.4664"}
