{"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2014", "title": "High-Performance Distributed ML at Scale through Parameter Server Consistency Models", "abstract": "As Machine Learning (ML) applications increase in data size and model complexity, practitioners turn to distributed clusters to satisfy the increased computational and memory demands. Unfortunately, effective use of clusters for ML requires considerable expertise in writing distributed code, while highly-abstracted frameworks like Hadoop have not, in practice, approached the performance seen in specialized ML implementations. The recent Parameter Server (PS) paradigm is a middle ground between these extremes, allowing easy conversion of single-machine parallel ML applications into distributed ones, while maintaining high throughput through relaxed \"consistency models\" that allow inconsistent parameter reads. However, due to insufficient theoretical study, it is not clear which of these consistency models can really ensure correct ML algorithm output; at the same time, there remain many theoretically-motivated but undiscovered opportunities to maximize computational throughput. Motivated by this challenge, we study both the theoretical guarantees and empirical behavior of iterative-convergent ML algorithms in existing PS consistency models. We then use the gleaned insights to improve a consistency model using an \"eager\" PS communication mechanism, and implement it as a new PS system that enables ML algorithms to reach their solution more quickly.", "histories": [["v1", "Wed, 29 Oct 2014 16:19:21 GMT  (2446kb,D)", "http://arxiv.org/abs/1410.8043v1", "19 pages, 2 figures"]], "COMMENTS": "19 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["wei dai", "abhimanu kumar", "jinliang wei", "qirong ho", "garth a gibson", "eric p xing"], "accepted": true, "id": "1410.8043"}
