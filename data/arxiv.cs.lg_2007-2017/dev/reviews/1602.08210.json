{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2016", "title": "Architectural Complexity Measures of Recurrent Neural Networks", "abstract": "In this paper, we systematically analyse the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the \"depth\" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems, as we improve the state-of-the-art for sequential MNIST dataset.", "histories": [["v1", "Fri, 26 Feb 2016 06:16:27 GMT  (423kb,D)", "http://arxiv.org/abs/1602.08210v1", "19 pages, 15 figures. Under review as a conference paper at ICML 2016"], ["v2", "Mon, 29 Feb 2016 17:18:21 GMT  (423kb,D)", "http://arxiv.org/abs/1602.08210v2", "19 pages, 15 figures; comments fixed"], ["v3", "Sat, 12 Nov 2016 19:38:43 GMT  (577kb,D)", "http://arxiv.org/abs/1602.08210v3", "17 pages, 8 figures; To appear in NIPS2016"]], "COMMENTS": "19 pages, 15 figures. Under review as a conference paper at ICML 2016", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["saizheng zhang", "yuhuai wu", "tong che", "zhouhan lin", "roland memisevic", "ruslan salakhutdinov", "yoshua bengio"], "accepted": true, "id": "1602.08210"}
