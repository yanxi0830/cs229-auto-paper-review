{"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Net2Net: Accelerating Learning via Knowledge Transfer", "abstract": "We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.", "histories": [["v1", "Wed, 18 Nov 2015 02:09:20 GMT  (209kb,D)", "http://arxiv.org/abs/1511.05641v1", null], ["v2", "Thu, 19 Nov 2015 19:07:40 GMT  (172kb,D)", "http://arxiv.org/abs/1511.05641v2", "ICLR 2016 submission"], ["v3", "Thu, 7 Jan 2016 22:54:48 GMT  (255kb,D)", "http://arxiv.org/abs/1511.05641v3", "ICLR 2016 submission"], ["v4", "Sat, 23 Apr 2016 23:14:39 GMT  (391kb,D)", "http://arxiv.org/abs/1511.05641v4", "ICLR 2016 submission"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tianqi chen", "ian goodfellow", "jonathon shlens"], "accepted": true, "id": "1511.05641"}
