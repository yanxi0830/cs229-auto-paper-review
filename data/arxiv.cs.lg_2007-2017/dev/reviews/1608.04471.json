{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm", "abstract": "We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.", "histories": [["v1", "Tue, 16 Aug 2016 03:24:20 GMT  (3890kb,D)", "http://arxiv.org/abs/1608.04471v1", "To appear in NIPS 2016"], ["v2", "Fri, 19 Aug 2016 05:13:47 GMT  (3890kb,D)", "http://arxiv.org/abs/1608.04471v2", "To appear in NIPS 2016"]], "COMMENTS": "To appear in NIPS 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["qiang liu", "dilin wang"], "accepted": true, "id": "1608.04471"}
