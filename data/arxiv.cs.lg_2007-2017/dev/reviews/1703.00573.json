{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)", "abstract": "This paper makes progress on several open theoretical issues related to Generative Adversarial Networks. A definition is provided for what it means for the training to generalize, and it is shown that generalization is not guaranteed for the popular distances between distributions such as Jensen-Shannon or Wasserstein. We introduce a new metric called neural net distance for which generalization does occur. We also show that an approximate pure equilibrium in the 2-player game exists for a natural training objective (Wasserstein). Showing such a result has been an open problem (for any training objective).", "histories": [["v1", "Thu, 2 Mar 2017 01:14:03 GMT  (677kb,D)", "http://arxiv.org/abs/1703.00573v1", null], ["v2", "Fri, 3 Mar 2017 16:19:00 GMT  (677kb,D)", "http://arxiv.org/abs/1703.00573v2", null], ["v3", "Tue, 4 Apr 2017 00:41:13 GMT  (694kb,D)", "http://arxiv.org/abs/1703.00573v3", null], ["v4", "Sat, 17 Jun 2017 22:04:07 GMT  (2726kb,D)", "http://arxiv.org/abs/1703.00573v4", "To appear in ICML 2017; Major update of exposition in Section 1-3"], ["v5", "Tue, 1 Aug 2017 19:51:56 GMT  (2727kb,D)", "http://arxiv.org/abs/1703.00573v5", "This is an updated version of an ICML'17 paper with the same title. The main difference is that in the ICML'17 version the pure equilibrium result was only proved for Wasserstein GAN. In the current version the result applies to most reasonable training objectives. In particular, Theorem 4.3 now applies to both original GAN and Wasserstein GAN"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["sanjeev arora", "rong ge 0001", "yingyu liang", "tengyu ma", "yi zhang"], "accepted": true, "id": "1703.00573"}
