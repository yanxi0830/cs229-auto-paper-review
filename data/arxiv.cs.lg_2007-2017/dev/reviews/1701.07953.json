{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "The Price of Differential Privacy for Online Learning", "abstract": "We design differentially private algorithms for the problem of online linear optimization in the full information and bandit settings with optimal $\\tilde{O}(\\sqrt{T})$ regret bounds. In the full-information setting, our results demonstrate that $(\\epsilon, \\delta)$-differential privacy may be ensured for free - in particular, the regret bounds scale as $O(\\sqrt{T})+\\tilde{O}\\big(\\frac{1}{\\epsilon}\\log \\frac{1}{\\delta}\\big)$. For bandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of $O\\Big(\\frac{\\sqrt{T\\log T}}{\\epsilon}\\log \\frac{1}{\\delta}\\Big)$, while the previously best known bound was $\\tilde{O}\\Big(\\frac{T^{\\frac{3}{4}}}{\\epsilon}\\Big)$.", "histories": [["v1", "Fri, 27 Jan 2017 06:17:14 GMT  (26kb)", "http://arxiv.org/abs/1701.07953v1", null], ["v2", "Tue, 13 Jun 2017 21:25:12 GMT  (36kb)", "http://arxiv.org/abs/1701.07953v2", "To appear in the Proceedings of the 34th International Conference on Machine Learning (ICML), Sydney, Australia, 2017"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["naman agarwal", "karan singh"], "accepted": true, "id": "1701.07953"}
