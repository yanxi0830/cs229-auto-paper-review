{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Manifold Regularized Discriminative Neural Networks", "abstract": "Unregularized deep neural networks (DNNs) can be easily overfit with a limited sample size. We argue that this is mostly due to the disriminative nature of DNNs which directly model the conditional probability (or score) of labels given the input. The ignorance of input distribution makes DNNs difficult to generalize to unseen data. Recent advances in regularization techniques, such as pretraining and dropout, indicate that modeling input data distribution (either explicitly or implicitly) greatly improves the generalization ability of a DNN. In this work, we explore the manifold hypothesis which assumes that instances within the same class lie in a smooth manifold. We accordingly propose two simple regularizers to a standard discriminative DNN. The first one, named Label-Aware Manifold Regularization, assumes the availability of labels and penalizes large norms of the loss function w.r.t. data points. The second one, named Label-Independent Manifold Regularization, does not use label information and instead penalizes the Frobenius norm of the Jacobian matrix of prediction scores w.r.t. data points, which makes semi-supervised learning possible. We perform extensive control experiments on fully supervised and semi-supervised tasks using the MNIST dataset and set the state-of-the-art results on it.", "histories": [["v1", "Thu, 19 Nov 2015 19:46:39 GMT  (498kb,D)", "https://arxiv.org/abs/1511.06328v1", "In submission to ICLR 2016"], ["v2", "Thu, 3 Dec 2015 17:11:25 GMT  (498kb,D)", "http://arxiv.org/abs/1511.06328v2", "In submission to ICLR 2016"], ["v3", "Thu, 7 Jan 2016 22:05:56 GMT  (676kb,D)", "http://arxiv.org/abs/1511.06328v3", "In submission to ICLR 2016"]], "COMMENTS": "In submission to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuangfei zhai", "zhongfei zhang"], "accepted": false, "id": "1511.06328"}
