{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Meta Networks", "abstract": "Deep neural networks have been successfully applied in applications with a large amount of labeled data. However, there are major drawbacks of the neural networks that are related to rapid generalization with small data and continual learning of new concepts without forgetting. We present a novel meta learning method, Meta Networks (MetaNet), that acquires a meta-level knowledge across tasks and shifts its inductive bias via fast parameterization for the rapid generalization. When tested on the standard one-shot learning benchmarks, our MetaNet models achieved near human-level accuracy. We demonstrated several appealing properties of MetaNet relating to generalization and continual learning.", "histories": [["v1", "Thu, 2 Mar 2017 15:52:55 GMT  (250kb,D)", "http://arxiv.org/abs/1703.00837v1", "initial submission"], ["v2", "Thu, 8 Jun 2017 16:12:40 GMT  (254kb,D)", "http://arxiv.org/abs/1703.00837v2", "Accepted at ICML 2017 - rewrote: the main section; added: MetaNet algorithmic procedure; performed: Mini-ImageNet evaluation"]], "COMMENTS": "initial submission", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["tsendsuren munkhdalai", "hong yu"], "accepted": true, "id": "1703.00837"}
