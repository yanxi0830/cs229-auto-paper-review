{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2012", "title": "Memory-Efficient Topic Modeling", "abstract": "As one of the simplest probabilistic topic modeling techniques, latent Dirichlet allocation (LDA) has found many important applications in text mining, computer vision and computational biology. Recent training algorithms for LDA can be interpreted within a unified message passing framework. However, message passing requires storing previous messages with a large amount of memory space, increasing linearly with the number of documents or the number of topics. Therefore, the high memory usage is often a major problem for topic modeling of massive corpora containing a large number of topics. To reduce the space complexity, we propose a novel algorithm without storing previous messages for training LDA: tiny belief propagation (TBP). The basic idea of TBP relates the message passing algorithms with the non-negative matrix factorization (NMF) algorithms, which absorb the message updating into the message passing process, and thus avoid storing previous messages. Experimental results on four large data sets confirm that TBP performs comparably well or even better than current state-of-the-art training algorithms for LDA but with a much less memory consumption. TBP can do topic modeling when massive corpora cannot fit in the computer memory, for example, extracting thematic topics from 7 GB PUBMED corpora on a common desktop computer with 2GB memory.", "histories": [["v1", "Wed, 6 Jun 2012 08:34:43 GMT  (2235kb)", "https://arxiv.org/abs/1206.1147v1", "20 pages, 7 figures"], ["v2", "Fri, 8 Jun 2012 14:07:26 GMT  (2239kb)", "http://arxiv.org/abs/1206.1147v2", "20 pages, 7 figures"]], "COMMENTS": "20 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["jia zeng", "zhi-qiang liu", "xiao-qin cao"], "accepted": false, "id": "1206.1147"}
