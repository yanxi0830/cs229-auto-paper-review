{"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Gaussian Quadrature for Kernel Features", "abstract": "Kernel methods have recently attracted resurgent interest, matching the performance of deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that $O(\\epsilon^{-2})$ samples are required to achieve an approximation error of at most $\\epsilon$. In this paper, we investigate some alternative schemes for constructing feature maps that are deterministic, rather than random, by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed, for any $\\gamma &gt; 0$, to achieve error $\\epsilon$ with $O(e^{\\gamma} + \\epsilon^{-1/\\gamma})$ samples as $\\epsilon$ goes to 0. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve comparable accuracy to the state-of-the-art kernel methods based on random Fourier features.", "histories": [["v1", "Fri, 8 Sep 2017 09:17:59 GMT  (36kb)", "http://arxiv.org/abs/1709.02605v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tri dao", "christopher de sa", "christopher r\\'e"], "accepted": true, "id": "1709.02605"}
