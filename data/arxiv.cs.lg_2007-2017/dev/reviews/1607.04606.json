{"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jul-2016", "title": "Enriching Word Vectors with Subword Information", "abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Many popular models to learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for morphologically rich languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skip-gram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram, words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpus quickly. We evaluate the obtained word representations on five different languages, on word similarity and analogy tasks.", "histories": [["v1", "Fri, 15 Jul 2016 18:27:55 GMT  (18kb)", "http://arxiv.org/abs/1607.04606v1", "Submitted to EMNLP 2016"], ["v2", "Mon, 19 Jun 2017 17:41:07 GMT  (150kb,D)", "http://arxiv.org/abs/1607.04606v2", "Accepted to TACL. The two first authors contributed equally"]], "COMMENTS": "Submitted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["piotr bojanowski", "edouard grave", "armand joulin", "tomas mikolov"], "accepted": true, "id": "1607.04606"}
