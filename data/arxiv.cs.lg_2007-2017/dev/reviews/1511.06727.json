{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters", "abstract": "Hyperparameter selection generally relies on running multiple full training trials, with hyperparameter selection based on validation set performance. We propose a gradient-based approach for locally adjusting hyperparameters on the fly in which we adjust the hyperparameters so as to make the model parameter gradients, and hence updates, more advantageous for the validation cost. We explore the approach for tuning regularization hyperparameters and find that in experiments on MNIST the resulting regularization levels are within the optimal regions. The method is less computationally demanding compared to similar gradient-based approaches to hyperparameter selection, only requires a few trials, and consistently finds solid hyperparameter values which makes it a useful tool for training neural network models.", "histories": [["v1", "Fri, 20 Nov 2015 19:10:16 GMT  (157kb,D)", "https://arxiv.org/abs/1511.06727v1", "8 pages, 5 figures"], ["v2", "Wed, 16 Dec 2015 06:28:07 GMT  (153kb,D)", "http://arxiv.org/abs/1511.06727v2", "8 pages, 5 figures. added references, fixed typos"], ["v3", "Fri, 17 Jun 2016 19:25:32 GMT  (872kb,D)", "http://arxiv.org/abs/1511.06727v3", "9 pages, 7 figures. Accepted at ICML 2016"]], "COMMENTS": "8 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jelena luketina", "tapani raiko", "mathias berglund", "klaus greff"], "accepted": true, "id": "1511.06727"}
