{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Tree-structured composition in neural networks without tree-structured architectures", "abstract": "Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models. We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data. We demonstrate this possibility using an artificial data task for which recursive compositional structure is crucial, and find that the sequence model can learn the underlying patterning. The sequence model is better in that it learns the value of tree structure from the data in an emergent way, while the tree-structured model is better in being able to learn with greater statistical efficiency due to its informative prior model structure.", "histories": [["v1", "Tue, 16 Jun 2015 05:12:52 GMT  (553kb,D)", "http://arxiv.org/abs/1506.04834v1", null], ["v2", "Tue, 18 Aug 2015 16:08:26 GMT  (361kb,D)", "http://arxiv.org/abs/1506.04834v2", null], ["v3", "Mon, 9 Nov 2015 19:45:09 GMT  (371kb,D)", "http://arxiv.org/abs/1506.04834v3", "To appear in the proceedings of the 2015 NIPS Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["samuel r bowman", "christopher d manning", "christopher potts"], "accepted": false, "id": "1506.04834"}
