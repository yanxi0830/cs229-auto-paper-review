{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Training Deep Neural Networks via Direct Loss Minimization", "abstract": "Supervised training of deep neural nets typically relies on minimizing cross-entropy. However, in many domains, we are interested in performing well on specific application-specific metrics. In this paper we proposed a direct loss minimization approach to train deep neural networks, taking into account the application-specific loss functions. This can be non-trivial, when these functions are non-smooth and non-decomposable. We demonstrate the effectiveness of our approach in the context of maximizing average precision for ranking problems. Towards this goal, we propose a dynamic programming algorithm that can efficiently compute the weight updates. Our approach proves superior to a variety of baselines in the context of action classification and object detection.", "histories": [["v1", "Thu, 19 Nov 2015 22:02:26 GMT  (504kb,D)", "http://arxiv.org/abs/1511.06411v1", null], ["v2", "Thu, 2 Jun 2016 00:56:59 GMT  (764kb,D)", "http://arxiv.org/abs/1511.06411v2", "ICML2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yang song", "alexander g schwing", "richard s zemel", "raquel urtasun"], "accepted": true, "id": "1511.06411"}
