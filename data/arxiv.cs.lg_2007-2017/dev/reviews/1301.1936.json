{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2013", "title": "Risk-Aversion in Multi-armed Bandits", "abstract": "Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk-aversion where the objective is to compete against the arm with the best risk-return trade-off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, investigate their theoretical guarantees, and report preliminary empirical results.", "histories": [["v1", "Wed, 9 Jan 2013 18:02:54 GMT  (429kb,D)", "http://arxiv.org/abs/1301.1936v1", "(2012)"]], "COMMENTS": "(2012)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amir sani", "alessandro lazaric", "r\u00e9mi munos"], "accepted": true, "id": "1301.1936"}
