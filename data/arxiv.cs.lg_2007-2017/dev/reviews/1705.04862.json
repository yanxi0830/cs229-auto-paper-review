{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2017", "title": "Efficient Parallel Methods for Deep Reinforcement Learning", "abstract": "We propose a novel framework for efficient parallelization of deep reinforcement learning algorithms, enabling these algorithms to learn from multiple actors on a single machine. The framework is algorithm agnostic and can be applied to on-policy, off-policy, value based and policy gradient based algorithms. Given its inherent parallelism, the framework can be efficiently implemented on a GPU, allowing the usage of powerful models while significantly reducing training time. We demonstrate the effectiveness of our framework by implementing an advantage actor-critic algorithm on a GPU, using on-policy experiences and employing synchronous updates. Our algorithm achieves state-of-the-art performance on the Atari domain after only a few hours of training. Our framework thus opens the door for much faster experimentation on demanding problem domains. Our implementation is open-source and is made public at", "histories": [["v1", "Sat, 13 May 2017 17:39:54 GMT  (2296kb,D)", "https://arxiv.org/abs/1705.04862v1", null], ["v2", "Tue, 16 May 2017 14:30:14 GMT  (2204kb,D)", "http://arxiv.org/abs/1705.04862v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alfredo v clemente", "humberto n castej\\'on", "arjun chandra"], "accepted": false, "id": "1705.04862"}
