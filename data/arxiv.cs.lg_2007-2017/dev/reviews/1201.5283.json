{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2012", "title": "An Efficient Primal-Dual Prox Method for Non-Smooth Optimization", "abstract": "We consider the non-smooth optimization problems in machine learning, where both the loss function and the regularizer are non-smooth functions. Previous studies on efficient empirical loss minimization assume either a smooth loss function or a strongly convex regularizer, making them unsuitable for non-smooth optimization. We develop an efficient method for a family of non-smooth optimization where the dual form of the loss function is bilinear in primal and dual variables. We cast a non-smooth optimization problem into a minimax optimization problem, and develop a primal dual prox method that solves the minimax optimization problem at a rate of $O(1/T)$, significantly faster than a standard gradient descent method ($O(1/\\sqrt{T})$). Our empirical study verifies the efficiency of the proposed method for non-smooth optimization by comparing it to the state-of-the-art first order methods.", "histories": [["v1", "Tue, 24 Jan 2012 04:09:54 GMT  (75kb)", "https://arxiv.org/abs/1201.5283v1", null], ["v2", "Fri, 27 Jan 2012 17:50:21 GMT  (75kb)", "http://arxiv.org/abs/1201.5283v2", null], ["v3", "Fri, 10 Feb 2012 16:11:15 GMT  (79kb)", "http://arxiv.org/abs/1201.5283v3", null], ["v4", "Mon, 2 Apr 2012 15:50:38 GMT  (124kb)", "http://arxiv.org/abs/1201.5283v4", null], ["v5", "Fri, 26 Jul 2013 05:03:51 GMT  (130kb)", "http://arxiv.org/abs/1201.5283v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tianbao yang", "mehrdad mahdavi", "rong jin", "shenghuo zhu"], "accepted": false, "id": "1201.5283"}
