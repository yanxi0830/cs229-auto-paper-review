{
  "name" : "1609.07472.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Gated Neural Networks for Option Pricing: Rationality by Design",
    "authors" : [ "Yongxin Yang", "Yu Zheng", "Timothy M. Hospedales" ],
    "emails" : [ "t.hospedales}@qmul.ac.uk,", "y.zheng12@imperial.ac.uk" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Option pricing models have long been a popular research area. From a theoretical perspective, new option pricing models provide an opportunity for academics to examine financial markets’ mechanics. From a practical viewpoint, market makers desire efficient pricing models to set bid and ask prices in derivative markets. The earliest and simplest pricing model, Black–Scholes (Black and Scholes 1973) gives a rough theoretical estimate of European option price. Since then many studies attempted to find better option pricing models by relaxing the strict assumptions in Black– Scholes. The models proposed by economists usually start from a set of economic assumptions and end up with a deterministic formula that takes as input some market signals (e.g., moneyness, time to maturity, and risk-free rate). In contrast, machine learning studies solve option pricing in a data-driven way: as a regression problem, with similar inputs to econometric models, and real market option prices as outputs. The complicated relationship between input and output (e.g., a Black–Scholes like formula) is learned from a large amount of data rather than derived from econometric axioms. Progress in data-driven option pricing can be driven by improvements in model expressivity, as well as integrating selected econometric axioms into a data-driven model as inductive bias. In this paper we achieve excellent option pricing results by contributing on both of these lines.\nRegression models trained by machine learning techniques, such as kernel machines and neural networks, gen-\neralise well to out-of-sample cases as long as the training data is sufficient. Such data-driven methods give good option price estimates (Malliaris and Salchenberger 1993), and can even surpass formula derived from economic principles. One drawback of existing data-driven approaches is that they seek a unique solution for all options. However, learned pricing models fail on certain options, for example, some overestimate deep out-of-the money options (Bennell and Sutcliffe 2004), or underestimate options very close to maturity (Dugas et al. 2000). To alleviate these issues, (Gradojevic, Gencay, and Kukolj 2009) proposed a ‘divide-and-conquer’ strategy, by first grouping options into sub-categories, and building distinct pricing models for each sub-category. However, this categorisation is done by manually defined heuristics, and may not be consistent with market conditions, and their changes in time. In this paper, we propose a novel class of neural networks for option pricing. These implement a divide-and-conquer method where option grouping is automatic and learned from data rather than manual heuristics. Therefore, it can dynamically adjust both option classification and refine the per-class pricing model as the market changes with time. Experiments on S&P 500 index options show that our approach is significantly better than others.\nA limitation of all the above machine learning-based methods is that while they may fit the data well (e.g., mean square error), they do not enforce some economic principles, thus ruling out their suitability for pricing in practice, e.g., option prices have theoretical bounds, of which the violation makes investors gain risk-free profit (so-called arbitrage). This motivates another less-studied approach to improving data-driven option pricing: opening up black box models to integrate economic axioms as constraints into learning algorithms (Dugas et al. 2000). From an economic perspective, this is designing a NN to make economically meaningful predictions, and from a learning perspective it is providing domain-specific inductive bias to improve generalisation and avoid overfitting. In this paper, we derive a class of gated neural networks with stronger economic rationality guarantees than existing work. In particular our neural price predictor is the first learning based approach to carry a valid risk neutral density function, i.e., a valid probability distribution over the future asset price in risk neutral probability space. The terminology risk neutral roughly implies no arbitrage but its rigorous definition is out of scope of this work, see ar X iv :1 60 9.\n07 47\n2v 1\n[ q-\nfi n.\nC P]\n1 4\nSe p\n20 16\n(Jeanblanc, Yor, and Chesney 2009) for details. Our contribution is three-fold: (1) We propose a neural network with superior option pricing performance. (2) We evaluate our method against several baselines on a largescale dataset: it includes 5139 trading days and 3029327 option contracts – this is 70 times larger than previous studies (Dugas et al. 2000; Gradojevic, Gencay, and Kukolj 2009). (3) Our neural network model is meaningful in that it enforces all the necessary requirements for an economically valid (no arbitrage) call option pricing model. This results in a valid risk neutral density function, from which users can extract many metrics, e.g., variance, kurtosis and skewness, that are crucial for risk management purposes."
    }, {
      "heading" : "Related Work",
      "text" : "Econometric Methods Asset pricing is a very active research area in finance and mathematical finance. The oldest and most famous model for option pricing is Black– Scholes (Black and Scholes 1973). The biggest criticism of this model is its incompatibility with the volatility smile behaviour in real markets due to its constant volatility assumption. The volatility smile exists due to the fact that real-world distributions are often fat-tailed and asymmetric. Stochastic volatility models, (e.g. (Heston 1993)), aim to model the above smile behaviour through allowing randomness of volatility, compensated for by introducing random volatility process (Heston 1993). Another stream of research suggests including jumps which represent rare events in the underlying process to alleviate the smile problem. These models are called Levy models (Merton 1976; Kou 2002; Madan, Carr, and Chang 1998; Barndorff-Nielsen 1997; Carr and Geman 2002) and are able to generate volatility skew or smile. A comprehensive theoretical explanation of asset pricing models can be found in (Jeanblanc, Yor, and Chesney 2009). This paper tackles the skew/smile problem in a more data-driven way: it learns from market prices so that a model that fits the market prices well is expected to carry the same smile structure.\nThere are many methods for implementing option pricing models including: Fourier-based (Carr and Madan 1999), Tree-based (Cox, Ross, and Rubinstein 1979), Finite difference (Schwartz 1977) and Monte Carlo methods (Boyle 1977). In this paper, we employ the fractional FFT method (Cooley and Tukey 1965) for our benchmark option pricing models as their characteristic functions are known.\nNeural Network Methods There is a long history of computer scientists trying to solve option pricing using neural networks (Malliaris and Salchenberger 1993). Option pricing can be seen a standard regression task for which there are many established methods and neural networks (rebranded deep learning) are one of the most popular choices.\nSome researchers claim that it is an advantage of neural network (NN) methods that they do not make as many assumptions as the econometric methods. However, NNs are not orthogonal to econometric methods. In fact, some NN methods leverage classic econometric insights. For example, (Garcia and Gençay 2000) proposed a neural option pricing model with a Black–Scholes like formula. (Dugas\net al. 2000) chose specific activation functions and positive weight parameter constraints such that their model has the second-order derivative properties required by economic axioms. These studies suggested that introducing econometric constraints produces better option pricing models compared to vanilla feed-forward NNs. A good survey of this line of work can be found in (Garcia, Ghysels, and Renault 2010).\nWhile some NN methods have benefited from econometric insights, these methods have always tried to find a universal pricing model for all options in the market. However, it has been shown that for deep out-of-money options or those with long maturity, NN methods perform very badly (Bennell and Sutcliffe 2004). This is unsurprising because NN methods usually produce a smooth pricing surface that fails to capture these awkward and low-volume parts of the market. (Gradojevic, Gencay, and Kukolj 2009) tried to address this issue by categorising options based on their moneyness and time to maturity, and training independent NNs for each class of options. Their grouping of options is based on fixed manual heuristic that is suboptimal, and does not adapt to the changing market data over time. Our method is a neural network that exploits a similar divide-and-conquer idea, however it jointly learns the inter-related problems of separating options into groups and pricing each group. Providing this increased model expressivity challenges our previous goal of building in econometric axioms to ensure meaningful predictions, because rationality constraints are harder to enforce in this more complex model. Thus we apply significant effort to contribute both a more expressive neural learner, and stronger rationality constraints guarantees to existing work."
    }, {
      "heading" : "Methodology",
      "text" : ""
    }, {
      "heading" : "Background",
      "text" : "We focus on EU call options. A call option is a contract that gives the buyer the right, but not the obligation, to acquire the underlying asset (e.g., stock) at a specified price (called strike price) on a certain future date (called maturity date). For example, at time t = 0 (today), a company’s stock is worth $100, and a trader pays a certain amount (option cost) to buy a call option with strike price $110 and maturity date T = 5 (five days later). After five days, if the company’s stock price is $120, he can exercise the option to get the stock by paying the strike price $110. If he sells the stock immediately at $120, he profits $10 (ignoring the option cost). If the company’s stock price is below $110, the trader will not exercise the option and he has only lost the money paid to buy the option contract. The option pricing problem is: what should the price for this option at time t = 0 be?\nWe denote the true option price from the market as c, and the estimate of the option pricing model as ĉ. The strike price ($110 in above example) is K, and the time-to-maturity is τ (τ = T − t). The underlying asset price at time t is St and the underlying asset price at time T is ST ($120 in above example – but this is unknown at time t = 0).\nFor a call option pricing model ĉ(·) with three inputs K, St, and τ , with the assumption of no arbitrage we have,\nĉ(K,St, τ) = e −rτ ∫ ∞ 0 max(0, ST −K)f(ST |St, τ)dST .\nHere r is the risk-free rate constant and e−rτ serves as a discount term. f(x|St, τ) is the conditional risk neutral probability density function for the asset price at time T (i.e., ST ), given its price at time t (i.e., St) and time difference (i.e., τ = T − t). This equation can be explained intuitively: max(0, ST −K) is the potential revenue of having this option at time T , and f(ST |St, τ) is the probability density of that revenue, thus the integral term is in fact the expected revenue at time T given the current status (St and τ ) in risk neutral probability space. Because of the no arbitrage assumption, this expected revenue in the future should be discounted at risk-free rate to get the price at time t. (Note that we do not consider the dividend for simplicity).\nBecause the risk-free rate and discount term are obtained independently, what the option pricing method actually models is the integral term, denoted c̃,\nc̃(K,St, τ) = ∫ ∞ 0 max(0, ST −K)f(ST |St, τ)dST .\nc̃ can be learned from data as a regression problem, but this does not necessarily lead to a meaningful predictive model unless f(·) is a valid probability density function."
    }, {
      "heading" : "Requirements for Rational Predictions",
      "text" : "We next list six conditions (Föllmer and Schied 2004) C1C6 that a meaningful option pricing model should meet.\n∂c̃\n∂K ≤ 0 (C1)\n∂c̃ ∂K = ∫K 0 f(ST |St, τ)dST − 1 and ∫K 0 f(ST |St, τ)dST is a cumulative distribution function P(ST ≤ K) thus its value can not be larger than one.\n∂2c̃\n∂K2 ≥ 0 (C2)\n∂2c̃ ∂K2 = f(ST |St, τ) is a probability density function so its value can not be smaller than zero.\n∂c̃ ∂τ ≥ 0 (C3)\nThis is intuitive: the longer you wait (larger τ ), the higher chance that the underlying asset price will eventually be greater than the strike price. Thus the price should be nondecreasing with time to maturity.\nlim K→∞\nc̃(K,St, τ) = c̃(∞, St, τ) = 0 (C4)\nIf the strike price is infinity, the option price should be zero because the underlying asset price is always smaller than the strike price. There is no point in trading the option.\nc̃(K,St, 0) = max(0, St −K) when τ = 0 (C5) When τ = 0, the option is ready to execute immediately, so its price should be exactly max(0, St −K) since St = ST .\nmax(0, St −K) ≤ c̃(K,St, τ) ≤ St (C6) This boundary can be easily derived from put-call parity and payoff of the call option. Call option price can not exceed the underlying price, otherwise an investor can arbitrage by buying the stock and selling the option at same\ntime and closing all positions when the option is expired. Note that the upper bound implies that when K = 0 we should have c̃(0, St, τ) = ∫∞ 0 ST f(ST |St, τ)dST = erτSt (also, ĉ(0, St, τ) = St). Some studies (Roper 2010) prefer the integral formula instead of the upper bound, while they are actually the same. For the lower bound, call option price must exceed max(0, St −K) as option has time value. Assumptions In the above we have made the assumptions: (i) the first and second-order derivative of c̃ with respect to K exist. (ii) the first-order derivative of c̃ with respect to τ exists. Before we introduce our proposed option pricing model, we make the last assumption: the pricing model is rescalable w.r.t. St:\nc̃( K\nSt , 1, τ) :=\nc̃(K,St, τ)\nSt (1)\nwhere the fraction term KSt is usually called (inverse) moneyness and denoted as m = KSt ."
    }, {
      "heading" : "Single Model",
      "text" : "The core part of our option pricing model y(m, τ) is\ny(m, τ) ≡ c̃(m, 1, τ) := c̃(K,St, τ) St . (2)\nIt takes two inputs: moneyness m and time-to-maturity τ . The objective is then to minimise the difference between the true market price of the option c and the estimate ĉ produced by the pricing model, where ĉ = e−rτ c̃ = e−rτSty.\nOur pricing function y(m, τ) is modelled by a neural network illustrated in Fig. 1 and specified by the formula\ny(m, τ) = J∑ j=1 σ1(b̃j −mew̃j )σ2(b̄j + τew̄j )eŵj . (3)\nHere σ1(x) = log(1 + ex) (softplus function) and σ2(x) = 1 1+e−x (sigmoid function). J is the number of neurons in the hidden layer. The parameters to learn are weight (w̃, w̄, and ŵ) and bias (b̃ and b̄) terms. We can see that this is a gated neural network (Sigaud et al. 2015) with two sides: the left-hand side takes m and produces j = 1 . . . J neurons σ1(b̃j−mew̃j ) and the right-hand side takes τ produces j = 1 . . . J neurons σ2(b̄j+τew̄j ). Then paired neurons (with the same index) from two sides are merged by a multiplication gate. Finally the J penultimate layer neurons produce the final prediction y using weights ŵ.\nVerifying Rationality We now show how the network of Eq. 3 meets the rationality conditions laid out earlier. The derivative of softplus is sigmoid function: σ′1(x) = σ2(x), and the derivative of sigmoid is σ′2(x) = σ2(x)(1− σ2(x)). Thus, we can tell that σ1(x), σ2(x), σ′1(x), and σ ′ 2(x) = σ′′1 (x) all produce positive values. Note that the weights have constrained sign: Left-branch weights are negative by imposing −ew̃, and right and top layer weights are positive by imposing ew̄ and eŵ.\nWe can verify that Eq. 3 meets conditions C1-C3 since\n∂y ∂m = J∑ j=1 −ew̃jσ2(b̃j −mew̃j )σ2(b̄j + τew̄j )eŵj ≤ 0\n∂2y ∂m2 = J∑ j=1 e2w̃jσ′2(b̃j −mew̃j )σ2(b̄j + τew̄j )eŵj ≥ 0\n∂y ∂τ = J∑ j=1 ew̄jσ1(b̃j −mew̃j )σ′2(b̄j + τew̄j )eŵj ≥ 0\nand the conditions C1, C2 and C3 can be rewritten as,\n∂c̃ ∂K = ∂Sty ∂K = St ∂y ∂m ∂m ∂K = St ∂y ∂m 1 St = ∂y ∂m ≤ 0\n∂2c̃\n∂K2 =\n∂2y\n∂m∂K =\n∂2y\n∂m2 ∂m ∂K = 1\nSt\n∂2y ∂m2 ≥ 0\n∂c̃ ∂τ = ∂Sty ∂τ = St ∂y ∂τ ≥ 0\nCondition C4 can be easily verified as m → ∞ when K →∞, and σ1(b̃j−mew̃j ) = 0 when m→∞. Therefore y = 0 and then c̃ = Sty = 0. This also explains why there is no bias term for the top layer.\nConditions C5 and C6 are hard to achieve by network architecture design (e.g., weight constraints, or activation function selection). We therefore meet them by synthesising virtual option contracts in training – they do not exist in the real market and their true prices c are equal to their theoretically estimated prices ĉ. In detail, to meet condition C5, we generate a number of virtual data points: For every unique St, we fix τ = 0 and uniformly sample K in [0, St], and the option price should be exactly St−K. An illustration of examples of virtual options can be found in Table 1.\nCondition C6 is trickier. For the upper bound, we again synthesise virtual training options: For every unique τ , we create an option with K = 0 corresponding to the most expensive option. Empirically the lower bound is very unlikely to be violated because (i) when K ≥ St the lower bound is 0 – this is met due to the neural network design (ii) when K < St, the virtual data for condition C5 and the market data are highly unlikely to be mis-priced as we convert (outof-the-money) put options into (in-the-money) call options\n(details see the first part of Experiment section), so the NN model learns this lower bound from data. An illustration of examples of virtual options can be found in Table 2."
    }, {
      "heading" : "Multi Model",
      "text" : "The previous network provides a single rational prediction model for all options. Our full model jointly trains multiple pricing models, as well as a weighting model to softly them. As illustrated in Fig. 2, the full model’s left-hand side has i = 1 . . . I single pricing models:\nyi(m, τ) = J∑ j=1 σ1(b̃ (i) j −me w̃ (i) j )σ2(b̄ (i) j +τe w̄ (i) j )eŵ (i) j (4)\nIts right-hand branch is a network with one K unit hidden layer, and the top layer has an I-way softmax activation function that provides a model selector for the left branch.\nwi(m, τ) = e ∑K\nk=1 σ2(mẆ1,k+τẆ2,k+ḃk)Ẅk,i+b̈i∑I i=1 e ∑K k=1 σ2(mẆ1,k+τẆ2,k+ḃk)Ẅk,i+b̈i (5)\nFinally, the overall output y is the softmax weighted average of the I local option pricing models’ outputs. Due to the softmax activation, the sum of weights (wi’s) is one.\ny(m, τ) = I∑ i=1 yi(m, τ)wi(m, τ). (6)\nOne can see the multi model as a mixture of expert ensemble (Jacobs et al. 1991), or a multi-task learning model (Yang and Hospedales 2015). The parameters of the single and multi model approaches are summarised in Table 3.\nVerifying Rationality It can be verified that the multinetwork above still meets Conditions C1, C3 and C4. Conditions C5, C6 are again softly enforced by feed virtual data training data. The outstanding issue is that the multi-model breaks the Condition C2. To alleviate this, we use the learning from hints trick (Abu-Mostafa 1993).\nDenoting the first-order derivative of y(m, τ) w.r.t. m as g(m, τ) = ∂y∂m , we introduce a new loss,\nP∑ p=1 Q∑ q=1 max(0, g(mp,q, τq)− g(mp,q + ∆, τq)) (7)\nWhere ∆ is an small number, e.g., ∆ = 0.001.Q is the number of unique time-to-maturity in the training set, and P is the number of pseudo data generated for every unique timeto-maturity. Eq. 7 will push g(m, τ) to a monotonically increasing function w.r.t.m, thus ∂g∂m (equivalently ∂2y ∂m2 ) tends to be larger than zero. Recall that ∂ 2c̃\n∂K2 = 1 St ∂2y ∂m2 , so Eq. 7\nfixes the negative second derivative issue. Unlike the virtual options for condition C5 and C6, we do not consider the loss caused by price difference for these data points, i.e., the data are generated for ensuring second derivative property only and we do not actually price them. In summary, the multimodel now also passes all the rationality checks. Loss Functions and Optimisation Option pricing can be sensitive to choice of loss function (Christoffersen and Jacobs 2004). We combine two objectives: Mean Square Error (MSE) and Mean Absolute Percentage Error (MAPE). For the multi-model, we have the extra loss in Eq. 7. To train the NNs, we use the Adam Optimiser (Kingma and Ba 2015)."
    }, {
      "heading" : "Experiments",
      "text" : "Data and Preprocessing The option data for S&P500 index comes from OptionMetrics and Bloomberg, which provide historical End-of-Day bid and ask quotes. The data sample covers the period 04/01/1996-31/05/2016. The corresponding risk free rates and index dividend yields are also provided by OptionMetrics and Bloomberg. The risk-free rates are interpolated by cubic spline to match the option maturity. Several data filters should be carried out before model calibration. Bid-ask mid-point price is calculated as a proxy for closing price. We discard in-the-money option quotes because trading is very inactive for those options thus their prices are not reliable. Furthermore, we aim to keep as many contracts as possible. We only omit contracts with maturity less than 2 days. After these procedures, we have 3029327 option quotes left. As our model focuses on pricing call options, we transfer put prices into call prices through put-call parity rather than discarding all put prices – this will introduce many in-the-money call options as the complement since we discard the original in-the-money call option quotes. Time-to-maturity is annually normalised, e.g., for τ = 7 (seven days), the actual input is 7365 = 0.019178.\nExperiments I: Quantitative Comparison We design the experiment as follows: we train a model with five continuous trading days data, and use the following one day for testing. We compare our models denoted as Single and Multi with five baseline methods: PSSF (Dugas et al. 2000), Modular Neural Networks (MNN) (Gradojevic, Gencay, and Kukolj 2009), Black–Scholes (BS) (Black and Scholes 1973), Variance Gamma (VG) (Madan, Carr, and Chang 1998), and Kou Jump (Kou 2002). For the three econometric methods, namely BS, VG, and Kou Jump, we only use the last training day’s data to calibrate their parameters (see Discussion for why). For Single and PSSF, the number of hidden layer neurons is J = 5. The number of pricing models in Multi is I = 9 as MNN has this setting. The number of neurons in hidden layer for the right-branch weighting network of Multi is K = 5. We report the MSE and MAPE on (c, e−rτSty) for a meaningful comparison, though for numerical stability we train the model to (equivalently) minimise the difference on (erτ cSt , y).\nBoth Table 4 and Fig. 3 show the superiority of our multi model in the terms of both performance and stability. We note that all methods simultaneously have drops in performance in Fig. 3 at a few time points which correspond to Dot-com bubble (1998), global financial crisis (2008), and European debt crisis (2011).\nExperiments II: Analysis of Contributions In this section, we illustrate and validate our virtual-option strategy for meeting conditions C5 and C6, and the second derivative fix used by our multi-model for C2. We show an example when the testing day is 15th May 2008, on which the S&P Index is 1423. We plot the risk neutral density of the S&P Index after 7 days (i.e., τ = 7). Fig. 4 shows the necessity of both virtual option contracts and positive second derivative enforcement. Both are required to generate a valid\nprobability density, i.e., (i) non-negative and (ii) integrate to one. Furthermore, the probability density function should be economically reasonable, e.g. asset price close to zero after τ = 7 days should be a rare event (small probability).\nOur model produces a valid density as a natural consequence of constraints C1-C6. In contrast, PSSF (Dugas et al. 2000) in Fig. 5 only meets conditions C1-C3 and it produces both an invalid density and. an unreasonable large zeroprice probability. Theoretically speaking, MNN (Gradojevic, Gencay, and Kukolj 2009) can not produce a density function because its derivative w.r.t. K is not well-defined. A numerical result in Fig. 5 (Right) illustrates this, where we can see a discontinuous point.\nDiscussion We explain why we feed only one day data to the econometric methods and five days data to train the NNs. Unlike the machine learning based methods, every parameter in econometric methods has a specific meaning. There is no analogy to increasing model capacity through increasing the number of parameters. In contrast, NN methods offer flexible model capacity e.g. changing the number of hidden neurons. The econometric methods are designed, by principle, to fit at most one day data where St is unique (some of them can only fit one day’s data with a unique τ thus a separate step of interpolation is further required). Feeding multiple days’ data to the econometric models leads to severe under-fitting and catastrophically bad performance.\nIn fact, requiring our model to fit multiple days’ data (corresponding to multiple St values) increases the training difficulty. NN performance is actually negatively related to the number of training days. Our MSE/MAPE in Table 4 would improve if trained on only one day of data. Why do we take this approach? Because a model that adapts to different underlying asset prices is extremely valuable when one wants to apply the option pricing model on high-frequency data: St is no longer a constant (as underlying asset’s closed price) but a changing value (as underlying asset’s current price)."
    }, {
      "heading" : "Conclusion",
      "text" : "We introduced a neural network for option pricing that outperforms existing learning-based and some econometric alternatives, and comes with guarantees about the economic rationality of its outputs. In future work we will apply this option pricing model on high-frequency data, and exploit similar constraints for other finance problems such as implied volatility surface."
    } ],
    "references" : [ {
      "title" : "Y",
      "author" : [ "Abu-Mostafa" ],
      "venue" : "S.",
      "citeRegEx" : "Abu.Mostafa 1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "O",
      "author" : [ "Barndorff-Nielsen" ],
      "venue" : "E.",
      "citeRegEx" : "Barndorff.Nielsen 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "and Sutcliffe",
      "author" : [ "J. Bennell" ],
      "venue" : "C.",
      "citeRegEx" : "Bennell and Sutcliffe 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "and Scholes",
      "author" : [ "F. Black" ],
      "venue" : "M.",
      "citeRegEx" : "Black and Scholes 1973",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "P",
      "author" : [ "Boyle" ],
      "venue" : "P.",
      "citeRegEx" : "Boyle 1977",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "and Geman",
      "author" : [ "P. Carr" ],
      "venue" : "H.",
      "citeRegEx" : "Carr and Geman 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "D",
      "author" : [ "P. Carr", "Madan" ],
      "venue" : "B.",
      "citeRegEx" : "Carr and Madan 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "and Jacobs",
      "author" : [ "P. Christoffersen" ],
      "venue" : "K.",
      "citeRegEx" : "Christoffersen and Jacobs 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "J",
      "author" : [ "J.W. Cooley", "Tukey" ],
      "venue" : "W.",
      "citeRegEx" : "Cooley and Tukey 1965",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "S",
      "author" : [ "Cox, J.C.", "Ross" ],
      "venue" : "A.; and Rubinstein, M.",
      "citeRegEx" : "Cox. Ross. and Rubinstein 1979",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Incorporating secondorder functional knowledge for better option pricing",
      "author" : [ "Dugas" ],
      "venue" : "In Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Dugas,? \\Q2000\\E",
      "shortCiteRegEx" : "Dugas",
      "year" : 2000
    }, {
      "title" : "and Schied",
      "author" : [ "H. Föllmer" ],
      "venue" : "A.",
      "citeRegEx" : "Föllmer and Schied 2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "and Gençay",
      "author" : [ "R. Garcia" ],
      "venue" : "R.",
      "citeRegEx" : "Garcia and Gençay 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "The Econometrics of Option Pricing",
      "author" : [ "Ghysels Garcia", "R. Renault 2010] Garcia", "E. Ghysels", "E. Renault" ],
      "venue" : null,
      "citeRegEx" : "Garcia et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Garcia et al\\.",
      "year" : 2010
    }, {
      "title" : "Option pricing with modular neural networks",
      "author" : [ "Gencay Gradojevic", "N. Kukolj 2009] Gradojevic", "R. Gencay", "D. Kukolj" ],
      "venue" : "IEEE Transactions on Neural Networks",
      "citeRegEx" : "Gradojevic et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Gradojevic et al\\.",
      "year" : 2009
    }, {
      "title" : "S",
      "author" : [ "Heston" ],
      "venue" : "L.",
      "citeRegEx" : "Heston 1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "G",
      "author" : [ "R. Jacobs", "M.I. Jordan", "N.S.J.", "Hinton" ],
      "venue" : "E.",
      "citeRegEx" : "Jacobs et al. 1991",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Mathematical Methods for Financial Markets",
      "author" : [ "Yor Jeanblanc", "M. Chesney 2009] Jeanblanc", "M. Yor", "M. Chesney" ],
      "venue" : null,
      "citeRegEx" : "Jeanblanc et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jeanblanc et al\\.",
      "year" : 2009
    }, {
      "title" : "and Ba",
      "author" : [ "D. Kingma" ],
      "venue" : "J.",
      "citeRegEx" : "Kingma and Ba 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "S",
      "author" : [ "Kou" ],
      "venue" : "G.",
      "citeRegEx" : "Kou 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "E",
      "author" : [ "D.B. Madan", "P. Carr", "Chang" ],
      "venue" : "C.",
      "citeRegEx" : "Madan. Carr. and Chang 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "and Salchenberger",
      "author" : [ "M. Malliaris" ],
      "venue" : "L.",
      "citeRegEx" : "Malliaris and Salchenberger 1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "R",
      "author" : [ "Merton" ],
      "venue" : "C.",
      "citeRegEx" : "Merton 1976",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "E",
      "author" : [ "Schwartz" ],
      "venue" : "S.",
      "citeRegEx" : "Schwartz 1977",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Gated networks: an inventory",
      "author" : [ "Sigaud" ],
      "venue" : "CoRR abs/1512.03201",
      "citeRegEx" : "Sigaud,? \\Q2015\\E",
      "shortCiteRegEx" : "Sigaud",
      "year" : 2015
    }, {
      "title" : "T",
      "author" : [ "Y. Yang", "Hospedales" ],
      "venue" : "M.",
      "citeRegEx" : "Yang and Hospedales 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We propose a neural network approach to price EU call options that significantly outperforms some existing pricing models and comes with guarantees that its predictions are economically reasonable. To achieve this, we introduce a class of gated neural networks that automatically learn to divide-and-conquer the problem space for robust and accurate pricing. We then derive instantiations of these networks that are ‘rational by design’ in terms of naturally encoding a valid call option surface that enforces no arbitrage principles. This integration of human insight within data-driven learning provides significantly better generalisation in pricing performance due to the encoded inductive bias in the learning, guarantees sanity in the model’s predictions, and provides econometrically useful byproduct such as risk neutral density.",
    "creator" : "LaTeX with hyperref package"
  }
}