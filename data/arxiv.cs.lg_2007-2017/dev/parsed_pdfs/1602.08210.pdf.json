{
  "name" : "1602.08210.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Architectural Complexity Measures of Recurrent Neural Networks",
    "authors" : [ "Saizheng Zhang", "Yuhuai Wu", "Tong Che", "Zhouhan Lin", "Roland Memisevic", "Ruslan Salakhutdinov", "Yoshua Bengio" ],
    "emails" : [ "SAIZHENG.ZHANG@UMONTREAL.CA", "YWU@CS.TORONTO.EDU", "TONGCHE@IHES.FR", "LIN.ZHOUHAN@GMAIL.COM", "ROLAND.UMONTREAL@GMAIL.COM", "RSALAKHU@CS.TORONTO.EDU", "YOSHUA.BENGIO@GMAIL.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units\nlike LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015).\nThis paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of “stacked RNNs”, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures. Some examples include Raiko et al. (2012); Graves (2013) who explored the use of skip connections; Hermans & Schrauwen (2013) who proposed the deep RNNs which are stacked RNNs with skip connections; Pascanu et al. (2013a) who pointed out the distinction of constructing a “deep” RNN from the view of the recurrent paths and the view of the input-to-hidden and hidden-to-output maps. However, they did not rigorously formalize the notion of “depth” and its implications in “deep” RNNs. Besides “deep” RNNs, there still remains a vastly unexplored field of connecting architectures. We argue that one barrier for better understanding the architectural complexity is the lack of a general definition of the connecting architecture. This forced previous researchers to mostly consider the simple cases while neglecting other possible connecting variations. Another barrier is the lack of quantitative measurements of the complexity of different RNN connecting architectures: even the concept of “depth” is not clear with current RNNs.\nIn this paper, we try to address these two barriers. We first introduce a general definition of a recurrent neural network, where we divide an RNN into two basic ingredients: a welldefined graph representation of the connecting architecture, and a set of transition functions describing the computational process associated with each unit in the network. Observing that the RNN undergoes multiple transformations not only feedforwardly (from input to output within a\nar X\niv :1\n60 2.\n08 21\n0v 1\n[ cs\n.L G\n] 2\n6 Fe\ntime step) but also recurrently (across multiple time steps), we carry out a quantitative analysis of the number of transformations in these two orthogonal directions, which results in the definitions of recurrent depth and feedforward depth. These two depths can be viewed as general extensions of the work of Pascanu et al. (2013a). We also explore a quantity called the recurrent skip coefficient which measures how quickly information propagates over time. This quantity is strongly related to vanishing/exploding gradient issues, and helps deal with long term dependency problems. Skip connections crossing different timescales have also been studied by Lin et al. (1996); El Hihi & Bengio (1996); Sutskever & Hinton (2010); Koutnik et al. (2014). Instead of specific architecture design, we focus on analyzing the graph-theoretic properties of recurrent skip coefficients, revealing the fundamental difference between the regular skip connections and the ones which truly increase the recurrent skip coefficients.\nWe empirically evaluate models with different recurrent/feedforward depths and recurrent skip coefficients on language modelling and sequential MNIST tasks. We also show that our experimental results further validate the usefulness of the proposed definitions."
    }, {
      "heading" : "2. General RNN",
      "text" : "RNNs are learning machines that recursively compute new states by applying transition functions to previous states and inputs. It has two ingredients: the connecting architecture describing how information flows between different nodes and the transition function describing the nonlinear transformation at each node. The connecting architecture is usually illustrated informally by an infinite directed acyclic graph, which in turn can be viewed as a finite directed cyclic graph that is unfolded through time. In this section, we first introduce a general definition of the connecting architecture and its underlying computation, followed by a general definition of an RNN."
    }, {
      "heading" : "2.1. The Connecting Architecture",
      "text" : "We formalize the concept of the connecting architecture by extending the traditional graph-based illustration to a more general definition with a finite directed multigraph and its unfolded version. Let us first define the notion of the RNN cyclic graph Gc that can be viewed as a cyclic graphical representation of RNNs. We attach “weights” to the edges in the cyclic graph Gc that represent time delay differences between the source and destination node in the unfolded graph representation.\nDefinition 2.1.1. Let Gc = (Vc, Ec) be a weighted directed multigraph 1, in which Vc = Vin ∪Vout ∪Vhid is a finite set\n1A directed multigraph is a directed graph that allows multiple\nof nodes, and Vin, Vout, Vhid are not empty. Ec ⊂ Vc×Vc× Z is a finite set of directed edges. Each e = (u, v, σ) ∈ Ec denotes a directed weighted edge pointing from node u to node v with an integer weight σ. Each node v ∈ Vc is labelled by an integer tuple (i, p). i ∈ {0, 2, · · ·m − 1} denotes the time index of the given node, where m is the period number of the RNN, and p ∈ S, where S is a finite set of node labels. We call the weighted directed multigraph Gc = (Vc, Ec) an RNN cyclic graph, if\n(1) For every edge e = (u, v, σ) ∈ Ec, let iu and iv denote the time index of node u and v, then σ = iv − iu + k ·m for some k ∈ Z.\n(2) There exists at least one directed cycle 2 in Gc.\n(3) For any closed walk ω, the sum of all the σ along ω is not zero.\n(4) There are no incoming edges to nodes in Vin, and no outgoing edges from nodes in Vout. There are both incoming edges and outgoing edges for nodes in Vhid.\nCondition (1) assures that we can get a periodic graph (repeating pattern) when unfolding the RNN through time. Condition (2) excludes feedforward neural networks in the definition by forcing to have at least one cycle in the cyclic graph. Condition (3) simply avoids cycles after unfolding.\nThe cyclic representation can be seen as a time folded representation of RNNs, as shown in Figure 1 (a). Given an RNN cyclic graph Gc, we unfold Gc over time t ∈ Z by the following procedure: Definition 2.1.2 (Unfolding). Given an RNN cyclic graph Gc = (Vc, Ec, σ), we define a new infinite set of nodes Vun = {(i + km, p)|(i, p) ∈ V, k ∈ Z}. The new set of edges Eun ∈ Vun × Vun is constructed as follows: ((t, p), (t′, p′)) ∈ Eun if and only if there is an edge e = ((i, p), (i′, p′), σ) ∈ E such that t′ − t = σ, and t ≡ i(modm). The new directed graph Gun = (Vun, Eun) is called the unfolding of Gc. Any infinite directed graph that can be constructed from an RNN cyclic graph through unfolding is called an RNN unfolded graph. Lemma 2.1.1. The unfolding Gun of any RNN cyclic graph Gc is a directed acyclic graph (DAG).\nFigure 1(a) shows an example of two graph representations Gun and Gc of a given RNN. Consider the edge from node (1, 7) going to node (0, 3) in Gc. The fact that it has weight 1 indicates that the corresponding edge in Gun travels one time step, ((t+ 1, 7), (t+ 2, 3)). Note that node (0, 3) also has a loop with weight 2. This loop corresponds to the edge ((t, 3), (t+ 2, 3)).\nThe two kinds of graph representations we presented above\ndirected edges connecting two nodes. 2A directed cycle is a closed walk with no repetitions of edges.\n(a) (b)\nFigure 1. (a) An example of an RNN’s Gc and Gun. Vin is denoted by square, Vhid is denoted by circle and Vout is denoted by diamond. In Gc, the number on each edge is its corresponding σ. The longest path is colored in red. The longest input-output path is colored in yellow and the shortest path is colored blue. The value of three measures are dr = 32 , df = 3 and s = 2. (b) 5 more examples. (1) and (2) have dr = 2, 32 , (3) has df = 5, (4) and (5) has s = 2, 3 2 .\nhave a one-to-one correspondence. Also, any graph structure θ on Gun is naturally mapped into a graph structure θ̄ on Gc. Given an edge tuple ē = (u, v, σ) in Gc, σ stands for the number of time steps crossed by ē’s covering edges in Eun, i.e., for every corresponding edge e ∈ Gun, e must start from some time index t to t+σ. Hence σ corresponds to the “time delay” associated with e.\nIn addition, the period number m in Definition 2.1.1 can be interpreted as the time length of the entire non-repeated recurrent structure in its unfolded RNN graph Gun. Strictly speaking, m has the following properties in Gun: ∀k ∈ Z, if ∃v = (t, p) ∈ Vun, then ∃v′ = (t + km, p) ∈ Vun; if ∃e = ((t, p), (t′, p′)) ∈ Eun, then ∃e′ = ((t+km, p), (t′+ km, p′)) ∈ Eun. In other words, shifting the Gun through time by km time steps will result in a DAG which is identical to Gun, and m is the smallest number that has such property for Gun. Most traditional RNNs have m = 1, while some special structures like hierarchical or clockwork RNN (El Hihi & Bengio, 1996; Koutnik et al., 2014) have m > 1. For example, Figure 1(a) (unfolded graph representation Gun) shows that the period number of this specific RNN is 2.\nIt is clear that if there exists a directed cycle ϑ in Gc, and the sum of σ along ϑ is positive (or negative), then there is a path which is the pre-image of ϑ in Gun whose length (summing the edge σ’s) approaches +∞ (or −∞). This fact naturally induces the general definition of unidirectionality and bidirectionality of RNNs as follows: Definition 2.1.3. An RNN is called unidirectional if its cyclic graph representation Gc has the property that the sums of σ along all the directed cycles ϑ in Gc share the same sign, i.e., either all positive or all negative. An RNN is called bidirectional if it is not unidirectional."
    }, {
      "heading" : "2.2. A General Definition of RNN",
      "text" : "The connecting architecture in Sec. 2.1 describes how information flows among RNN units. Assume v̄ ∈ Vc is a node in Gc, let In(v̄) denotes the set of incoming nodes of v̄, In(v̄) = {ū|(ū, v̄) ∈ Ec}. In the forward pass of the RNN, the transition function Fv̄ takes outputs of nodes In(v̄) as\ninputs and computes a new output. For example, vanilla RNNs units with different activation functions, LSTMs and GRUs can all be viewed as units with specific transition functions.\nWe now give the general definition of an RNN:\nDefinition 2.2.1. An RNN is a tuple (Gc,Gun, {Fv̄}v̄∈Vc), in which Gun = (Vun, Eun) is the unfolding of RNN cyclic graph Gc, and {Fv̄}v̄∈Vc is the set of transition functions. In the forward pass, for each hidden and output node v ∈ Vun, the transition function Fv̄ takes all incoming nodes of v as the input to compute the output.\nAn RNN is homogeneous if all the hidden nodes share the same form of the transition function."
    }, {
      "heading" : "3. Measures of Architectural Complexity",
      "text" : "In this section, we develop different measures of RNNs’ architectural complexity, focusing mostly on the graphtheoretic properties of RNNs. To analyze an RNN solely from its architectural aspect, we make the mild assumption that the RNN is homogeneous. We further assume the RNN to be unidirectional. For a bidirectional RNN, it is more natural to measure the complexities of its unidirectional components."
    }, {
      "heading" : "3.1. Recurrent Depth",
      "text" : "Unlike feedforward models where computations are done within one time frame, RNNs map inputs to outputs over multiple time steps. In some sense, an RNN undergoes transformations along both feedforward and recurrent dimensions. This fact suggests that we should investigate its architectural complexity from these two different perspectives. We first consider the recurrent perspective.\nThe conventional definition of depth is the maximum number of nonlinear transformations from inputs to outputs. Observe that a directed path in an unfolded graph representationGun corresponds to a sequence of nonlinear transformations. Given an unfolded RNN graph Gun, ∀i, n ∈ Z, let Di(n) be the length of the longest path from any node\nat starting time i to any node at time i+ n.\nFrom the recurrent perspective, it is natural to investigate how Di(n) changes over time. Generally speaking, Di(n) increases as n increases for all i. Such increase is caused by the recurrent structure of the RNN which keeps adding new nonlinearities over time. Since Di(n) approaches ∞ as n approaches ∞,3 to measure the complexity of Di(n), we consider its asymptotic behaviour, i.e., the limit of Di(n)n as n → ∞. Under a mild assumption, this limit exists. To perform a practical calculation of this limit, the next theorem relies on Gun’s cyclic counterpart Gc, where the computation is much easier:\nTheorem 3.2 (Recurrent Depth). Given an RNN and its two graph representation Gun and Gc, we denote C(Gc) to be the set of directed cycles in Gc. For ϑ ∈ C(Gc), let l(ϑ) denote the length of ϑ and σs(ϑ) denote the sum of edge weights σ along ϑ. Under a mild assumption4,\ndr = lim n→+∞\nDi(n)\nn = max ϑ∈C(Gc)\nl(ϑ)\nσs(ϑ) . (1)\nThus, dr is a positive rational number.\nMore intuitively, dr is a measure of the average maximum number of nonlinear transformations per time step as n gets large. Thus, we call it recurrent depth:\nDefinition 3.2.1 (Recurrent Depth). Given an RNN and its two graph representations Gun and Gc, we call dr, defined in Eq.(1), the recurrent depth of the RNN.\nIn Figure 1(a), one can easily verify that Dt(1) = 5, Dt(2) = 6, Dt(3) = 8, Dt(4) = 9 . . . Thus Dt(1) 1 = 5, Dt(2) 2 = 3, Dt(3) 3 = 8 3 , Dt(4) 4 = 9 4 . . . ., which eventually converges to 32 as n → ∞. As n increases, most parts of the longest path coincides with the path colored in red. As a result, dr coincides with the number of nodes the red path goes through per time step. Similarly in Gc, observe that the red cycle achieves the maximum ( 32 ) in Eq.(1). Usually, one can directly calculate dr from Gun.\nIt is easy to verify that simple RNNs and stacked RNNs share the same recurrent depth which is equal to 1. This reveals the fact that their nonlinearities increase at the same rate, which suggests that they will behave similarly in the long run. This fact is often neglected, since one would typically consider the number of layers as a measure of depth, and think of stacked RNNs as “deep” and simple RNNs as “shallow”, even though their discrepancies are not due to recurrent depth (which regards time) but due to feedforward depth, defined next.\n3Without loss of generality, we assume the unidirectional RNN approaches positive infinity.\n4See a full treatment of the limit in general cases in Theorem A.1 and Proposition A.1.1 in Appendix."
    }, {
      "heading" : "3.3. Feedforward Depth",
      "text" : "Recurrent depth does not fully characterize the nature of nonlinearity of an RNN. As previous work suggests (Sutskever et al., 2014), stacked RNNs do outperform shallow ones with the same hidden size on problems where a more immediate input and output process is modeled. This is not surprising, since the growth rate of Di(n) only captures the number of nonlinear transformations in the time direction, not in the feedforward direction.\nThe perspective of feedforward computation puts more emphasis on the specific paths connecting inputs to outputs. Given an RNN unfolded graph Gun, let D∗i (n) be the length of the longest path from any input node at time step i to any output node at time step i + n. Clearly, when n is small, the recurrent depth cannot serve as a good description for D∗i (n). In fact. it heavily depends on another quantity which we call feedforward depth. The following proposition guarantees the existence of such a quantity and demonstrates the role of both measures in quantifying the nonlinearity of an RNN.\nProposition 3.3.1 (Input-Output Length Least Upper Bound). Given an RNN with recurrent depth dr, we denote\ndf = sup i,n∈Z\nD∗i (n)− n · dr. (2)\nThe supremum df exists and thus we have the following upper bound for D∗i (n)\nD∗i (n) ≤ n · dr + df .\nThe above upper bound explicitly shows the interplay between recurrent depth and feedforward depth: when n is small, D∗i (n) is largely bounded by df ; when n is large, dr captures the nature of the bound (≈ n · dr). These two measures are equally important, as they separately capture the maximum number of nonlinear transformations of an RNN in the long run and in the short run.\nDefinition 3.3.1. (Feedforward Depth) Given an RNN with recurrent depth dr and its two graph representations Gun and Gc, we call df , defined in Eq.(2), the feedforward depth5 of the RNN.\nTo calculate df in practice, we introduce the following theorem:\nTheorem 3.4 (Feedforward Depth). Given an RNN and its two graph representations Gun and Gc, we denote ξ(Gc) the set of directed paths that start at an input node and end\n5Conventionally, an architecture with depth 1 is a three-layer architecture containing one hidden layer. But in our definition, since it goes through two transformations, we count the depth as 2 instead of 1. This should be particularly noted with the concept of feedforward depth, which can be thought as the conventional depth plus 1.\nat an output node in Gc. For γ ∈ ξ(Gc), denote l(γ) the length and σs(γ) the sum of σ along γ. Then we have:\ndf = sup i,n∈Z D∗i (n)− n · dr = max γ∈ξ(Gc) l(γ)− σs(γ) · dr,\nwherem is the period number and dr is the recurrent depth of the RNN. Thus, df is a postive rational number.\nFor example, in Figure 1(a), one can easily verify that df = D∗t (0) = 3. Most commonly, df is the same as D ∗ t (0), i.e., the maximum length from an input to its current output."
    }, {
      "heading" : "3.5. Recurrent Skip Coefficient",
      "text" : "Depth provides a measure of the complexity of the model. But such a measure is not sufficient to characterize behavior on long-term dependency tasks. In particular, since models with large recurrent depths have more nonlinearities through time, gradients can explode or vanish more easily. On the other hand, it is known that adding skip connections across multiple time steps may help improve the performance on long-term dependency problems (Lin et al. (1996); Sutskever & Hinton (2010)). To measure such a “skipping” effect, we should instead pay attention to the length of the shortest path from time i to time i + n. In Gun, ∀i, n ∈ Z, let di(n) be the length of the shortest path. Similar to the recurrent depth, we consider the growth rate of di(n).\nTheorem 3.6 (Recurrent Skip Coefficient). Given an RNN and its two graph representations Gun and Gc, under mild assumptions6\nj = lim n→+∞\ndi(n)\nn = min ϑ∈C(Gc)\nl(ϑ)\nσs(ϑ) . (3)\nThus, j is a positive rational number.\nSince it is often the case that j is smaller or equal to 1, it is more intuitive to consider its reciprocal.\nDefinition 3.6.1. (Recurrent Skip Coefficient)7. Given an RNN and its two graph representations Gun and Gc, we define s = 1j , whose reciprocal is defined in Eq.(3), as the recurrent skip coefficient of the RNN.\nWith a larger recurrent skip coefficient, the number of transformations per time step is smaller. As a result, the nodes in the RNN are more capable of “skipping” across the network, allowing unimpeded information flow across multiple time steps, thus alleviating the problem of learning long term dependencies. In particular, such effect is more prominent in the long run, due to the network’s recurrent\n6See Proposition A.3.1 in Appendix. 7One would find this definition very similar to the definition of the recurrent depth. Therefore, we refer readers to examples in Figure 1 for some illustrations.\nstructure. Also note that not all types of skip connections can increase the recurrent skip coefficient. We will consider specific examples in our experimental results section."
    }, {
      "heading" : "4. Experiments and Results",
      "text" : "In this section we conduct a series of experiments to investigate the following questions: (1) Is recurrent depth a trivial measure? (2) Can increasing depth yield performance improvements? (3) Can increasing the recurrent skip coefficient improve the performance on long term dependency tasks? (4) Does the recurrent skip coefficient suggest something more compared to simply adding skip connections? We first show evaluations on RNNs with tanh nonlinearities, and then present similar results for LSTMs."
    }, {
      "heading" : "4.1. Tasks and Training Settings",
      "text" : "text8 dataset: We evaluate our models on character level language modeling using the text8 dataset8, which contains 100M characters from Wikipedia with an alphabet of letters a-z and a space symbol. We follow the setting from Mikolov et al. (2012): 90M for training, 5M for validation and the remaining 5M for test. We also divide the training set into non-overlapping sequences, each of length 180. Quality of fit is evaluated by the bits-per-character (BPC) metric, which is log2 of perplexity.\nsequential MNIST dataset: We also evaluate our models with a modified version of the MNIST dataset. Each MNIST image data is reshaped into a 784 × 1 sequence, turning the digit classification task into a sequence classification one with long-term dependencies (Le et al., 2015; Arjovsky et al., 2015). The model has access to 1 pixel per time step and predicts the class label at the end. A slight modification of the dataset is to permute the image sequences by a fixed random order beforehand (permuted MNIST). Since spatially local dependencies are no longer local (in the sequence), this task may be harder in terms of modelling long-term dependencies. Results in Le et al. (2015) have shown that both tanh RNNs and LSTMs did not achieve satisfying performance, which also highlights the difficulty of this task.\nFor all of our experiments we use Adam (Kingma & Ba, 2014) for optimization, and conduct a grid search on the learning rate in {10−2, 10−3, 10−4, 10−5}. For tanh RNNs, the parameters are initialized with samples from a uniform distribution. For LSTM networks we adopt a similar initialization scheme, while the forget gate biases are chosen by the grid search on {−5,−3,−1, 0, 1, 3, 5}. We employ early stopping and the batch size was set to 50.\n8http://mattmahoney.net/dc/textdata."
    }, {
      "heading" : "4.2. Recurrent Depth is Non-trivial",
      "text" : "To investigate the first question, we compare 4 similar connecting architectures: 1-layer (shallow) “sh”, 2-layers stacked “st”, 2-layers stacked with an extra bottom-up connection “bu”, and 2-layers stacked with an extra top-down connection “td”, as shown in Figure 2(a). sh has a layer size of 512, while the rest of the networks have a layer size of 256, making the number of hidden parameters in each architecture remain roughly the same. We also compare our results to the ones reported in Pascanu et al. (2012) (denoted as “P-sh”), since they used the same model architecture as the one specified by our sh.\nAlthough the four architectures look quite similar, they have different recurrent depths: sh, st and bu have dr = 1, while td has dr = 2. Note that the specific construction of the extra nonlinear transformations in td is not conventional. Instead of simply adding intermediate layers in hidden-to-hidden connection, as reported in Pascanu et al. (2013a), more nonlinearities are gained by a recurrent flow from the first layer to the second layer and then back to the first layer at each time step (see the red path in Figure 2(a)).\nTable 1 clearly shows that the td architecture outperforms all the other architectures, including the one reported in Pascanu et al. (2012). The validation curve, displayed in Figure 3(a), further shows the gap between td and the other three architectures. Even though td’s validation BPC is higher at the beginning of the training (possibly due to harder initial optimization), it outperforms all other architectures in the long run.\nIt is also interesting to note the improvement we obtain when switching from bu to td. The only difference between these two architectures lies in changing the direction of one connection (see Figure 2(a)), which also increases\nthe recurrent depth. Such a fundamental difference is by no means self-evident, but this result highlights the necessity of the concept of recurrent depth."
    }, {
      "heading" : "4.3. Comparing Depths",
      "text" : "From the previous experiment, we found some evidence that with larger recurrent depth, the performance might improve. To further investigate various implications of depths, we carry out a systematic analysis for both recurrent depth dr and feedforward depth df on text8 and sequential MNIST datasets. We build 9 models in total with dr = 1, 2, 3 and df = 2, 3, 4, respectively (as shown in Figure 2(b)). We ensure that all the models have roughly the same number of parameters (e.g., the model with dr = 1 and df = 2 has a hidden-layer size of 360).\nTable 2 and Figure 4 display results on the text8 dataset. We observed that for a fixed feedforward deapth df , increasing the recurrent depth dr does improve the model performance, and the best test BPC is achieved by the architecture with df = 2 and dr = 3. This suggests that the increase of dr can aid in better capturing the over-time nonlinearity of the input sequence. However, for a fixed dr, increasing df only helps when dr = 1. For a recurrent depth of dr = 3, increasing df only hurts models performance. This can potentially be attributed to the optimization issues when modelling large input-to-output dependencies (see Appendix B.4 for more details).\nWith sequential MNIST dataset, we next examined the effects of df and dr when modelling long term dependencies (more in Appendix B.4). In particular, we observed that increasing df does not bring any improvement to the model performance, and increasing dr might even be detrimental for training. Indeed, it appears that df only captures the local nonlinearity and has less effect on the long term prediction. This result seems to contradict previous claims (Hermans & Schrauwen, 2013) that stacked RNNs (df > 1,\n(a) (b)\n(1) (2)\n(3) (4)\nFigure 5. (a) Various architectures that we consider in Section 4.4. From top to bottom are baseline s = 1, and s = 2, s = 3. (b) Proposed architectures that we consider in Section 4.5 where we take k = 3 as an example. The shortest paths in (a) and (b) that correspond to the recurrent skip coefficients are colored in blue.\ndr = 1) could capture information in different time scales and would thus be more capable of dealing with learning long-term dependencies. On ther other hand, a large dr indicates multiple transformations per time step, resulting in greater gradient vanishing/exploding issues (Pascanu et al., 2013a), which suggests that dr should neither be too small nor too large."
    }, {
      "heading" : "4.4. Recurrent Skip Coefficients",
      "text" : "To investigate whether increasing a recurrent skip coefficient s improves model performance on long term dependency tasks, we compare models with increasing s on the sequential MNIST problem (without/with permutation, denoted as MNIST and pMNIST).\nOur baseline model is the shallow architecture proposed in Le et al. (2015). To increase the recurrent skip coefficient s, we add connections from time step t to time step t + k for some fixed integer k, shown in Figure 5 (a). By using this specific construction, the recurrent skip coefficient increases from 1 (i.e., baseline) to k. We also ensure that each model has roughly the same number of parameters: The hidden layer size of the baseline model is set to 90. Models with extra connections have 2 hidden matrices (one from t to t+1 and the other from t to t+k), each with a size of 64.\nThe results in Table 3 and Figure 6 show that models with recurrent skip coefficient s larger than 1 could improve the model performance dramatically. Within a reasonable range of s, test accuracy increases quickly as s becomes larger. We note that our model is the first tanh RNN model that achieves good performance on this task, even improving upon the method proposed in Le et al. (2015).\nIn addition, we also formally compare with the previous results reported in Le et al. (2015); Arjovsky et al. (2015), where our model (called as sTANH) has a hidden-layer size of 95, which is about the same number of parameters as in the tanh model of Arjovsky et al. (2015). Table 4 shows that our simple architecture improves upon the state-of-theart by 2.6% on pMNIST, and achieves almost the same performance as LSTM on the MNIST dataset with only 25% number of parameters (Arjovsky et al., 2015).\nNote that obtaining good performance on sequential MNIST requires a larger s than that for pMNIST (see Appendix B.4 for more details)."
    }, {
      "heading" : "4.5. Recurrent Skip Coefficients vs. Skip Connections",
      "text" : "In the next set of experiments, we investigated whether the recurrent skip coefficient can suggest something more than simply adding skip connections. We design 4 specific architectures shown in Figure 5(b): (1) is the baseline model with a 2-layer stacked architecture, the other three models add extra skip connections in different ways. Note that these extra skip connections all cross the same time length k, and in particular, (2) and (3) share quite similar architectures. However, the way in which the skip connections are allocated make a big difference on their recurrent skip coefficients: (2) has s = 1, (3) has s = k2 and (4) has s = k. Therefore, even though (2), (3) and (4) all add extra skip connections, the fact that their recurrent skip coefficients are different might result in different performance.\nWe evaluated these architectures on the sequential MNIST and pMNIST datasets. The results show that differences in s indeed cause big performance gaps regardless of the fact that they all have skip connections (see Table 5 and Figure 7). Given the same k, the model with a larger s performs better. In particular, model (3) is better than model (2) even though they only differ in the direction of the skip connections.\nIt is also very interesting to see that for MNIST (unpermuted), the extra skip connection in model (2) (which does not really increase the recurrent skip coefficient) brings almost no benefits, as model (2) and model (1) have almost the same results. This observation highlights the following point: when addressing the long term dependency problems using skip connections, instead of only considering the time intervals crossed by the skip connection, one should also consider the model’s recurrent skip coefficient, which can serve as a guide for introducing more powerful skip connections."
    }, {
      "heading" : "4.6. Results on LSTMs",
      "text" : "Finally, we also performed a similar set of experiments for LSTMs. For the first experiment comparing 4 architectures\nsh, st, bu and td9, as defined in Figure 2(a). LSTMs have similar performance benefits as tanh units, see Table 6.\nLSTMs also showed performance boosts and much faster convergence speed when using larger s, as displayed in Table 7 and Figure 6. In sequential MNIST, the LSTM with s = 3 already performs quite well and increasing s did not result in any significant improvement, while in pMNIST, the performance gradually improves as s increases from 4 to 6. We also observed that the LSTM network performed worse on permuted MNIST compared to a tanh RNN. Similar result was also reported in Le et al. (2015).\nTo conclude, the empirical evidence indicates that LSTMs might also benefit from increasing recurrent depth on some tasks and from increasing recurrent skip coefficients on long term dependency problems."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we first introduced a general definition of RNNs, which allows one to construct more general architectures, and provides a solid framework for the architectural complexity analysis. We then proposed three architectural complexity measures: recurrent depth, feedforward depth, and recurrent skip coefficients, each capturing the complexity in the long term, complexity in the short term and the speed of information flow. We also find empirical evidence that increasing recurrent depth might yield performance improvements, increasing feedforward depth might\n9bu and td are not well-defined within the conventional definition of an LSTM network, where a node can only receive two inputs. We overcome such limitation by considering a Multidimensional LSTM (Graves et al., 2007), which is explained in detail in Appendix B.2.\nnot help on long term dependency tasks, while increasing the recurrent skip coefficient can largely improve performance on long term dependency tasks. These measures and results can provide guidance for the design of new recurrent architectures for a particular learning task. Future work could involve more comprehensive studies (e.g., providing analysis on more datasets, using different architectures with various transition functions) to investigate the effectiveness of the proposed measures."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors acknowledge the following agencies for funding and support: NSERC, Canada Research Chairs, CIFAR, Calcul Quebec, Compute Canada, Samsung, and IARPA Raytheon BBN Contract No. D11PC20071. The authors thank the developers of Theano (Bastien et al., 2012) and Keras (Chollet, 2015), and also thank Nicolas Ballas, Tim Cooijmans, Ryan Lowe, Mohammad Pezeshki, Roger Grosse and Alex Schwing for their insightful comments."
    }, {
      "heading" : "A. Proofs",
      "text" : "To show theorem 3.2, we first consider the most general case in which dr is defined (Theorem A.1). Then we discuss the mild assumptions under which we can reduce to the original limit (Proposition A.1.1). Additionally, we introduce some notations that will be used throughout the proof. If v = (t, p) ∈ Gun is a node in the unfolded graph, it has a corresponding node in the folded graph, which is denoted by v̄ = (t̄, p).\nTheorem A.1. Given an RNN cyclic graph and its unfolded representation (Gc,Gun), we denote C(Gc) the set of directed cycles in Gc. For ϑ ∈ C(Gc), denote l(ϑ) the length of ϑ and σs(ϑ) the sum of σ along ϑ. Write di = lim supk→∞ Di(n) n . 10 we have :\n• The quantity di is periodic, in the sense that di+m = di,∀i ∈ N.\n• Let dr = maxi di, then\ndr = max ϑ∈C(Gc)\nl(ϑ)\nσs(ϑ) (4)\nProof. The first statement is easy to prove. Because of the periodicity of the graph, any path from time step i to i + n corresponds to an isomorphic path from time step i + m to i + m + n. Passing to limit, and we can deduce the first statement.\nNow we prove the second statement. Write ϑ0 = argmaxϑ l(ϑ) σs(ϑ) . First we prove that d ≥ l(ϑ0)σs(ϑ0) . Let c1 = (t1, p1) ∈ Gun be a node such that if we denote c1 = (t1, p1) the image of c1 on the cyclic graph, we have c1 ∈ ϑ0. Consider the subsequence S0 = { Dt1 (kσs(ϑ0))\nkσs(ϑ0) }∞ k=1 of { Dt1 (n) n }∞ n=1 . From the definition of D and the fact that ϑ0 is a directed circle,\nwe have Dt1(kσs(ϑ0)) ≥ kl(ϑ0), by considering the path on Gun corresponding to following ϑ0 k -times. So we have\ndr ≥ lim sup k→+∞\nDi(n)\nn ≥ lim sup\nk→+∞\nDt1(kσs(ϑ0)) kσs(ϑ0) ≥ kl(ϑ0) kσs(ϑ0) = l(ϑ0) σs(ϑ0)\nNext we prove dr ≤ l(ϑ0)σs(ϑ0) . It suffices to prove that, for any ≥ 0, there exists N > 0, such that for any path γ : {(t0, p0), (t1, p1), · · · , (tnγ , pnγ )} with tnγ − t1 > N , we have\nnγ tnγ−t1 ≤ l(ϑ0)σs(ϑ0) + . We denote γ̄ as the image of γ on the cyclic graph. γ̄ is a walk with repeated nodes and edges. Also, we assume there are in total Γ nodes in cyclic graph Gc.\nWe first decompose γ̄ into a path and a set of directed cycles. More precisely, there is a path γ0 and a sequence of directed cycles C = C1(γ), C2(γ), · · · , Cw(γ) on Gc such that:\n• The starting and end nodes of γ0 is the same as γ. (If γ starts and ends at the same node, take γ0 as empty.)\n• The catenation of the sequences of directed edges E(γ0), E(C1(γ)), E(C2(γ)), · · · , E(Cw(γ)) is a permutation of the sequence of edges of E(γ).\nThe existence of such a decomposition can be proved iteratively by removing directed cycles from γ. Namely, if γ is not a paths, there must be some directed cycles C ′ on γ. Removing C ′ from γ, we can get a new walk γ′. Inductively apply this\n10Di(n) is not defined when there does not exist a path from time i to time i+ n. We simply omit undefined cases when we consider the limsup. In a more rigorous sense, it is the limsup of a subsequence of {Di(n)}∞n=1, where Di(n) is defined.\nremoval, we will finally get a (possibly empty) path and a sequence of directed cycles. For a directed path or loop γ, we write D(γ) the distance between the ending node and starting node when travel through γ once. We have\nD(γ0) := tnγ − t0 + |γ0|∑ i=1 σ(ei)\nwhere ei, i ∈ {1, 2, · · · , |γ0|} is all the edges of γ0. t̄ denotes the module of t: t ≡ t̄(modm).\nSo we have: |D(γ0)| ≤ m+ Γ ·max\ne∈Gc σ(e) = M\nFor convenience, we denote l0, l1, · · · , lw to be the length of path γ0 and directed cycles C1(γ), C2(γ), · · · , Cw(γ). Obviously we have:\nnγ = w∑ i=0 li\nAnd also, we have\ntnγ − t1 = w∑ i=1 σs(Ci) +D(γ0)\nSo we have: nγ\ntnγ − t1 = l0 tnγ − t1 + w∑ i=1 li tnγ − t1 ≤ Γ N + w∑ i=1 li tnγ − t1\nIn which we have for all i ∈ {1, 2, · · · , w} :\nli tnγ − t1 = li σs(Ci) · σs(Ci) tnγ − t1 ≤ l(ϑ0) σs(ϑ0) σs(Ci) tnγ − t1\nSo we have: w∑ i=1 li tnγ − t1 ≤ l(ϑ0) σs(ϑ0) [ 1− D(γ0) tnγ − t1 ] ≤ l(ϑ0) σs(ϑ0) + M ′ N\nin which M ′ and Γ are constants depending only on the RNN Gc.\nFinally we have: nγ\ntnγ − t1 ≤ l(ϑ0) σs(ϑ0)\n+ M ′ + Γ\nN\ntake N > M ′+Γ , we can prove the fact that dr ≤ l(ϑ0) σs(ϑ0) .\nProposition A.1.1. Given an RNN and its two graph representations Gun and Gc, if ∃ϑ ∈ C(Gc) such that (1) ϑ achieves the maximum in Eq.(4) and (2) U(ϑ) in Gun visits nodes at every time step, then we have\ndr = max i∈Z ( lim sup n→+∞ Di(n) n ) = lim n→+∞ Di(n) n\nProof. We only need to prove, in such a graph, for all i ∈ Z we have\nlim inf n→+∞\nDi(n)\nn ≥ max i∈Z ( lim sup n→+∞ Di(n) n ) = dr\nBecause it is obvious that\nliminfn→+∞ Di(n)\nn ≤ dr\nNamely, it suffice to prove, for all i ∈ Z, for all > 0, there is anN > 0, such that when n > N , we have Di(n)n ≥ dr− . On the other hand, for k ∈ N, if we assume (k+ 1)σs(ϑ) + i > n ≥ i+ k ·σs(ϑ), then according to condition (2) we have\nDi(n) n ≥ k · l(ϑ) (k + 1)σs(ϑ) = l(ϑ) σs(ϑ) − l(ϑ) σs(ϑ) 1 k + 1\nWe can see that if we set k > σs(ϑ)l(ϑ) , the inequality we wanted to prove.\nWe now prove Proposition 3.3.1 and Theorem 3.4 as follows.\nProposition A.1.2. Given an RNN with recurrent depth dr, we denote\ndf = sup i,n∈Z\nD∗i (n)− n · dr.\nThe supremum df exists and we have the following least upper bound:\nD∗i (n) ≤ n · dr + df .\nProof. We first prove that df < +∞. Write df (i) = supn∈Z D∗i (n) − n · dr. It is easy to verify df (·) is m−periodic, so it suffices to prove for each i ∈ N, df (i) < +∞. Hence it suffices to prove\nlim sup n→∞\n(D∗i (n)− n · dr) < +∞.\nFrom the definition, we have Di(n) ≥ D∗i (n). So we have\nD∗i (n)− n · dr ≤ Di(n)− n · dr.\nFrom the proof of Theorem A.1, there exists two constants M ′ and Γ depending only on the RNN Gc, such that\nDi(n)\nn ≤ dr +\nM ′ + Γ\nn .\nSo we have lim sup n→∞ (D∗i (n)− n · dr) ≤ lim sup n→∞ (Di(n)− n · dr) ≤M ′ + Γ. Also, we have df = supi,n∈Z D ∗ i (n)− n · dr, so for any i, n ∈ Z,\ndf ≥ D∗i (n)− n · dr.\nTheorem A.2. Given an RNN and its two graph representations Gun and Gc, we denote ξ(Gc) the set of directed path that starts at an input node and ends at an output node in Gc. For γ ∈ ξ(Gc), denote l(γ) the length and σs(γ) the sum of σ along γ. Then we have:\ndf = sup i,n∈Z D∗i (n)− n · dr = max γ∈ξ(Gc) l(γ)− σs(γ) · dr.\nProof. Let γ : {(t0, 0), (t1, p1), · · · , (tnγ , p)} be a path in Gun from an input node (t0, 0) to an output node (tnγ , p), where t0 = i and tnγ = i + n. We denote γ̄ as the image of γ on the cyclic graph. From the proof of Theorem A.1, for each γ̄ in Gc, we can decompose it into a path γ0 and a sequence of directed cycles C = C1(γ), C2(γ), · · · , Cw(γ) on Gc satisfying those properties listed in Theorem A.1. We denote l0, l1, · · · , lw to be the length of path γ0 and directed cycles C1(γ), C2(γ), · · · , Cw(γ). We know lkσs(Ck) ≤ dr for all k = 1, 2, . . . , w by definition. Thus,\nlk ≤ dr · σs(Ck) w∑ k=1 lk ≤ dr · w∑ k=1 σs(Ck)\nNote that n = σs(γ0) + ∑w k=1 σs(Ck). Therefore,\nl(γ)− n · dr = l0 + w∑ k=1 lk − n · dr\n≤ l0 + dr · ( w∑ k=1 σs(Ck)− n)\n= l0 − dr · σs(γ0)\nfor all time step i and all integer n. The above inequality suggests that in order to take the supremum over all paths in Gun, it suffices to take the maximum over a directed path in Gc. On the other hand, the equality can be achieved simply by choosing the corresponding path of γ0 in Gun. The desired conclusion then follows immediately.\nLastly, we show Theorem 3.6.\nTheorem A.3. Given an RNN cyclic graph and its unfolded representation (Gc,Gun), we denote C(Gc) the set of directed cycles in Gc. For ϑ ∈ C(Gc), denote l(ϑ) the length of ϑ and σs(ϑ) the sum of σ along ϑ. Write si = lim infk→∞ di(n)n . We have :\n• The quantity si is periodic, in the sense that si+m = si,∀i ∈ N.\n• Let s = mini si, then dr = min\nϑ∈C(Gc)\nl(ϑ)\nσs(ϑ) . (5)\nProof. The proof is essentially the same as the proof of the first theorem. So we omit it here.\nProposition A.3.1. Given an RNN and its two graph representations Gun and Gc, if ∃ϑ ∈ C(Gc) such that (1) ϑ achieves the minimum in Eq.(5) and (2) U(ϑ) in Gun visits nodes at every time step, then we have\ns = min i∈Z ( lim inf n→+∞ di(n) n ) = lim n→+∞ di(n) n .\nProof. The proof is essentially the same as the proof of the Proposition A.1.1. So we omit it here."
    }, {
      "heading" : "B. Experiment Details",
      "text" : "B.1. RNNs with tanh\nIn this section we explain the functional dependency among nodes in RNNs with tanh in detail.\nThe transition function for each node is the tanh function. The output of a node v is a vector hv . To compute the output for an node, we simply take all incoming nodes as input, and sum over their affine transformations and then apply the tanh function (we omit the bias term for simplicity).\nhv = tanh  ∑ u∈In(v) W(u)hu  , where W(·) represents a real matrix.\nAs a more concrete example, consider the “bottom-up” architecture in Figure 8, with which we did the experiment described in Section 4.2. To compute the output of node v,\nhv = tanh(W(u)hu + W(p)hp + W(q)hq). (6)\nB.2. LSTMs\nIn this section we explain the Multidimensional LSTM (introduced by (?)) which we use for experiments with LSTMs (see Section 4.6).\nThe output of a node v of the LSTM is a 2-tuple (cv ,hv), consisting of a cell memory state cv and a hidden state hv . The transition function F is applied to each node indistinguishably. We describe the computation of F below in a sequential manner (we omit the bias term for simplicity).\nz = g  ∑ u∈In(v) Wz(u)hu  block input i = σ\n ∑ u∈In(v) Wi(u)hu  input gate o = σ\n ∑ u∈In(v) Wo(u)hu  output gate {fu} =\nσ  ∑ u′∈In(v) Wfu(u ′)hu  |u ∈ In(v)  A set of forget gates\ncv = i z + ∑\nu∈In(v)\nfu cu cell state\nhv = o cv hidden state\nNote that the Multidimensional LSTM includes the usual definition of LSTM as a special case, where the extra forget gates are 0 (i.e., bias term set to -∞) and extra weight matrices are 0. We again consider the architecture bu in Fig. 8. We first\ncompute the block input, the input gate and the output gate by summing over all affine transformed outputs of u, p, q, and then apply the activation function. For example, to compute the input gate, we have\ni = σ (Wi(u)hu + Wi(p)hp + Wi(q)hq) .\nNext, we compute one forget gate for each pair of (v, u), (v, p), (v, q). The way of computing a forget gate is the same as computing the other gates. For example, the forget gate in charge of the connection of u→ v is computed as,\nfu = σ (Wfu(u)hu + Wfu(p)hu + Wfu(q)hu) .\nThen, the cell state is simply the sum of all element-wise products of the input gate with the block output and forget gates with the incoming nodes’ cell memory states,\ncv = i z + fu cu + fp cp + fq cq.\nLastly, the hidden state is computed as usual, hv = o cv.\nB.3. Recurrent Depth is Non-trivial\nThe validation curves of the 4 different connecting architectures sh, st, bu and td on text8 dataset for both tanh and LSTM are shown below:\nB.4. Full Comparisons on Depths\nFigure 10 and Figure 11 show all the validation curves for the 9 architectures on text8 dataset, with their dr = 1, 2, 3 and df = 2, 3, 4 respectively. We use two intialization schemes: In Figure 10, hidden-to-hidden matrices are intialized from uniform distribution, while in Figure 11, hidden-to-hidden matrices are intialized using orthogonal matrices. In Figure 11, we observe that orthogonal intialization helps for the optimization issues when df is large, in which increasing df does not harm the learning. While the results are still consistent with our analysis in the main paper. Notice that one can achieve better results with much larger models on this dataset, as in (?).\nAlso, to see if increasing feedforward depth/ recurrent depth helps for long term dependency problems, we evaluate these 9 architectures on sequential MNIST task, with roughly the same number of parameters( 8K, where the first architecture with dr = 1 and df = 2 has hidden size of 90.). We try two initialization schemes where hidden-to-hidden matrices are initialized from uniform distribution or using orthogonal matrices.\nFigure 12 and 13 clearly show that, as the feedforward depth increases, the model performance stays roughly the same. In addition, note that increasing recurrent depth might even result in performance decrease. This is possibly because that larger recurrent depth amplifies the gradient vanishing/exploding problems, which is detrimental on long term dependency tasks.\nB.5. Recurrent Skip Coefficients\nThe test curves for all the experiments are shown in Figure 14. In Figure 14, we observed that obtaining good performance on MNIST requires larger s than for pMNIST. We hypothesize that this is because, for the sequential MNIST dataset, each training example contains many consecutive zero-valued subsequences, each of length 10 to 20. Thus within those subsequences, the input-output gradient flow could tend to vanish. However, when the recurrent skip coefficient is large enough to cover those zero-valued subsequences, the model starts to perform better. With pMNIST, even though the random permuted order seems harder to learn, the permutation on the other hand blends zeros and ones to form more uniform sequences, and this may explain why training is easier, less hampered by by the long sequences of zeros.\nB.6. Recurrent Skip Coefficients vs. Skip Connections\nTest curves for all the experiments are shown in Figure 15. Observe that in most cases, the test accuracy of (3) is worse than (2) in the beginning while beating (2) in the middle of the training. This is possibly because in the first several time steps, it is easier for (2) to pass information to the output thanks to the skip connections, while only after multiples of k time steps, (3) starts to show its advantage with recurrent skip connections11. The shorter paths in (2) make its gradient flow more easily in the beginning, but in the long run, (3) seems to be more superior, maybe because of its more prominent skipping effect over time.\n11It will be more clear if one checks the length of the shortest path from an node at time t to to a node at time t+k in both architectures."
    } ],
    "references" : [ {
      "title" : "Unitary evolution recurrent neural networks",
      "author" : [ "Arjovsky", "Martin", "Shah", "Amar", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1511.06464,",
      "citeRegEx" : "Arjovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Arjovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Bastien", "Frédéric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,",
      "citeRegEx" : "Bastien et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Cho", "Kyunghyun", "Van Merriënboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Keras. GitHub repository: https:// github.com/fchollet/keras",
      "author" : [ "Chollet", "François" ],
      "venue" : null,
      "citeRegEx" : "Chollet and François.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chollet and François.",
      "year" : 2015
    }, {
      "title" : "Hierarchical recurrent neural networks for long-term dependencies",
      "author" : [ "El Hihi", "Salah", "Bengio", "Yoshua" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hihi et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Hihi et al\\.",
      "year" : 1996
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Graves", "Alex" ],
      "venue" : "arXiv preprint arXiv:1308.0850,",
      "citeRegEx" : "Graves and Alex.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves and Alex.",
      "year" : 2013
    }, {
      "title" : "Multi-dimensional recurrent neural networks",
      "author" : [ "Graves", "Alex", "Fernández", "Santiago", "Schmidhuber", "Jürgen" ],
      "venue" : "In Proceedings of the 17th International Conference on Artificial Neural Networks,",
      "citeRegEx" : "Graves et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2007
    }, {
      "title" : "Lstm: A search space odyssey",
      "author" : [ "Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutnı́k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "Jürgen" ],
      "venue" : "arXiv preprint arXiv:1503.04069,",
      "citeRegEx" : "Greff et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Greff et al\\.",
      "year" : 2015
    }, {
      "title" : "Training and analysing deep recurrent neural networks",
      "author" : [ "Hermans", "Michiel", "Schrauwen", "Benjamin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hermans et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hermans et al\\.",
      "year" : 2013
    }, {
      "title" : "Untersuchungen zu dynamischen neuronalen netzen",
      "author" : [ "Hochreiter", "Sepp" ],
      "venue" : "Diploma, Technische Universität München,",
      "citeRegEx" : "Hochreiter and Sepp.,? \\Q1991\\E",
      "shortCiteRegEx" : "Hochreiter and Sepp.",
      "year" : 1991
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "An empirical exploration of recurrent network architectures",
      "author" : [ "Jozefowicz", "Rafal", "Zaremba", "Wojciech", "Sutskever", "Ilya" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "Jozefowicz et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jozefowicz et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik", "Ba", "Jimmy" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Torralba", "Antonio", "Urtasun", "Raquel", "Fidler", "Sanja" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kiros et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "A clockwork rnn",
      "author" : [ "Koutnik", "Jan", "Greff", "Klaus", "Gomez", "Faustino", "Schmidhuber", "Juergen" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "Koutnik et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Koutnik et al\\.",
      "year" : 2014
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E" ],
      "venue" : "arXiv preprint arXiv:1504.00941,",
      "citeRegEx" : "Le et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning longterm dependencies is not as difficult with NARX recurrent neural networks",
      "author" : [ "T. Lin", "B.G. Horne", "P. Tino", "C.L. Giles" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Lin et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 1996
    }, {
      "title" : "Learning recurrent neural networks with hessian-free optimization",
      "author" : [ "Martens", "James", "Sutskever", "Ilya" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "Martens et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Martens et al\\.",
      "year" : 2011
    }, {
      "title" : "Subword language modeling with neural networks",
      "author" : [ "Mikolov", "Tomáš", "Sutskever", "Ilya", "Deoras", "Anoop", "Le", "Hai-Son", "Kombrink", "Stefan" ],
      "venue" : "preprint, (http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf),",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2012
    }, {
      "title" : "Understanding the exploding gradient problem",
      "author" : [ "Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua" ],
      "venue" : "Computing Research Repository (CoRR) abs/1211.5063,",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2012
    }, {
      "title" : "How to construct deep recurrent neural networks",
      "author" : [ "Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1312.6026,",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua" ],
      "venue" : "In Proceedings of The 30th International Conference on Machine Learning,",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep learning made easier by linear transformations in perceptrons",
      "author" : [ "Raiko", "Tapani", "Valpola", "Harri", "LeCun", "Yann" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Raiko et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Raiko et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning complex, extended sequences using the principle of history compression",
      "author" : [ "Schmidhuber", "Jürgen" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Schmidhuber and Jürgen.,? \\Q1992\\E",
      "shortCiteRegEx" : "Schmidhuber and Jürgen.",
      "year" : 1992
    }, {
      "title" : "Unsupervised learning of video representations using LSTMs",
      "author" : [ "Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Temporal-kernel recurrent neural networks",
      "author" : [ "Sutskever", "Ilya", "Hinton", "Geoffrey" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2010
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015).",
      "startOffset" : 125,
      "endOffset" : 231
    }, {
      "referenceID" : 28,
      "context" : "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015).",
      "startOffset" : 125,
      "endOffset" : 231
    }, {
      "referenceID" : 26,
      "context" : "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015).",
      "startOffset" : 125,
      "endOffset" : 231
    }, {
      "referenceID" : 15,
      "context" : "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015).",
      "startOffset" : 125,
      "endOffset" : 231
    }, {
      "referenceID" : 3,
      "context" : ", 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al.",
      "startOffset" : 56,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : ", 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015).",
      "startOffset" : 101,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : ", 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015).",
      "startOffset" : 101,
      "endOffset" : 196
    }, {
      "referenceID" : 13,
      "context" : ", 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015).",
      "startOffset" : 101,
      "endOffset" : 196
    }, {
      "referenceID" : 1,
      "context" : "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of “stacked RNNs”, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures.",
      "startOffset" : 140,
      "endOffset" : 823
    }, {
      "referenceID" : 1,
      "context" : "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of “stacked RNNs”, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures.",
      "startOffset" : 140,
      "endOffset" : 848
    }, {
      "referenceID" : 1,
      "context" : "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of “stacked RNNs”, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures. Some examples include Raiko et al. (2012); Graves (2013) who explored the use of skip connections; Hermans & Schrauwen (2013) who proposed the deep RNNs which are stacked RNNs with skip connections; Pascanu et al.",
      "startOffset" : 140,
      "endOffset" : 1074
    }, {
      "referenceID" : 1,
      "context" : "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of “stacked RNNs”, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures. Some examples include Raiko et al. (2012); Graves (2013) who explored the use of skip connections; Hermans & Schrauwen (2013) who proposed the deep RNNs which are stacked RNNs with skip connections; Pascanu et al.",
      "startOffset" : 140,
      "endOffset" : 1089
    }, {
      "referenceID" : 1,
      "context" : "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of “stacked RNNs”, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures. Some examples include Raiko et al. (2012); Graves (2013) who explored the use of skip connections; Hermans & Schrauwen (2013) who proposed the deep RNNs which are stacked RNNs with skip connections; Pascanu et al.",
      "startOffset" : 140,
      "endOffset" : 1158
    }, {
      "referenceID" : 1,
      "context" : "Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems (Graves, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Srivastava et al., 2015; Kiros et al., 2015). There is also much work attempting to reveal the principles behind the challenges and successes of RNNs, including optimization issues (Martens & Sutskever, 2011; Pascanu et al., 2013b), gradient vanishing/exploding related problems (Hochreiter, 1991; Bengio et al., 1994), analysing/designing new RNN transition functional units like LSTMs, GRUs and their variants (Hochreiter & Schmidhuber, 1997; Greff et al., 2015; Cho et al., 2014; Jozefowicz et al., 2015). This paper focuses on another important theoretical aspect of RNNs: the connecting architecture. Ever since Schmidhuber (1992); El Hihi & Bengio (1996) introduced different forms of “stacked RNNs”, researchers have taken architecture design for granted and have paid less attention to the exploration of other connecting architectures. Some examples include Raiko et al. (2012); Graves (2013) who explored the use of skip connections; Hermans & Schrauwen (2013) who proposed the deep RNNs which are stacked RNNs with skip connections; Pascanu et al. (2013a) who pointed out the distinction of constructing a “deep” RNN from the view of the recurrent paths and the view of the input-to-hidden and hidden-to-output maps.",
      "startOffset" : 140,
      "endOffset" : 1254
    }, {
      "referenceID" : 19,
      "context" : "These two depths can be viewed as general extensions of the work of Pascanu et al. (2013a). We also explore a quantity called the recurrent skip coefficient which measures how quickly information propagates over time.",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "Skip connections crossing different timescales have also been studied by Lin et al. (1996); El Hihi & Bengio (1996); Sutskever & Hinton (2010); Koutnik et al.",
      "startOffset" : 73,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "Skip connections crossing different timescales have also been studied by Lin et al. (1996); El Hihi & Bengio (1996); Sutskever & Hinton (2010); Koutnik et al.",
      "startOffset" : 73,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "Skip connections crossing different timescales have also been studied by Lin et al. (1996); El Hihi & Bengio (1996); Sutskever & Hinton (2010); Koutnik et al.",
      "startOffset" : 73,
      "endOffset" : 143
    }, {
      "referenceID" : 16,
      "context" : "(1996); El Hihi & Bengio (1996); Sutskever & Hinton (2010); Koutnik et al. (2014). Instead of specific architecture design, we focus on analyzing the graph-theoretic properties of recurrent skip coefficients, revealing the fundamental difference between the regular skip connections and the ones which truly increase the recurrent skip coefficients.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 16,
      "context" : "Most traditional RNNs have m = 1, while some special structures like hierarchical or clockwork RNN (El Hihi & Bengio, 1996; Koutnik et al., 2014) have m > 1.",
      "startOffset" : 99,
      "endOffset" : 145
    }, {
      "referenceID" : 28,
      "context" : "As previous work suggests (Sutskever et al., 2014), stacked RNNs do outperform shallow ones with the same hidden size on problems where a more immediate input and output process is modeled.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 18,
      "context" : "On the other hand, it is known that adding skip connections across multiple time steps may help improve the performance on long-term dependency problems (Lin et al. (1996); Sutskever & Hinton (2010)).",
      "startOffset" : 154,
      "endOffset" : 172
    }, {
      "referenceID" : 18,
      "context" : "On the other hand, it is known that adding skip connections across multiple time steps may help improve the performance on long-term dependency problems (Lin et al. (1996); Sutskever & Hinton (2010)).",
      "startOffset" : 154,
      "endOffset" : 199
    }, {
      "referenceID" : 17,
      "context" : "Each MNIST image data is reshaped into a 784 × 1 sequence, turning the digit classification task into a sequence classification one with long-term dependencies (Le et al., 2015; Arjovsky et al., 2015).",
      "startOffset" : 160,
      "endOffset" : 200
    }, {
      "referenceID" : 0,
      "context" : "Each MNIST image data is reshaped into a 784 × 1 sequence, turning the digit classification task into a sequence classification one with long-term dependencies (Le et al., 2015; Arjovsky et al., 2015).",
      "startOffset" : 160,
      "endOffset" : 200
    }, {
      "referenceID" : 18,
      "context" : "We follow the setting from Mikolov et al. (2012): 90M for training, 5M for validation and the remaining 5M for test.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : ", 2015; Arjovsky et al., 2015). The model has access to 1 pixel per time step and predicts the class label at the end. A slight modification of the dataset is to permute the image sequences by a fixed random order beforehand (permuted MNIST). Since spatially local dependencies are no longer local (in the sequence), this task may be harder in terms of modelling long-term dependencies. Results in Le et al. (2015) have shown that both tanh RNNs and LSTMs did not achieve satisfying performance, which also highlights the difficulty of this task.",
      "startOffset" : 8,
      "endOffset" : 415
    }, {
      "referenceID" : 21,
      "context" : "We also compare our results to the ones reported in Pascanu et al. (2012) (denoted as “P-sh”), since they used the same model architecture as the one specified by our sh.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "We also compare our results to the ones reported in Pascanu et al. (2012) (denoted as “P-sh”), since they used the same model architecture as the one specified by our sh. Although the four architectures look quite similar, they have different recurrent depths: sh, st and bu have dr = 1, while td has dr = 2. Note that the specific construction of the extra nonlinear transformations in td is not conventional. Instead of simply adding intermediate layers in hidden-to-hidden connection, as reported in Pascanu et al. (2013a), more nonlinearities are gained by a recurrent flow from the first layer to the second layer and then back to the first layer at each time step (see the red path in Figure 2(a)).",
      "startOffset" : 52,
      "endOffset" : 526
    }, {
      "referenceID" : 21,
      "context" : "Table 1 clearly shows that the td architecture outperforms all the other architectures, including the one reported in Pascanu et al. (2012). The validation curve, displayed in Figure 3(a), further shows the gap between td and the other three architectures.",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : "Our baseline model is the shallow architecture proposed in Le et al. (2015). To increase the recurrent skip coefficient s, we add connections from time step t to time step t + k for some fixed integer k, shown in Figure 5 (a).",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "6% on pMNIST, and achieves almost the same performance as LSTM on the MNIST dataset with only 25% number of parameters (Arjovsky et al., 2015).",
      "startOffset" : 119,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : "We note that our model is the first tanh RNN model that achieves good performance on this task, even improving upon the method proposed in Le et al. (2015). In addition, we also formally compare with the previous results reported in Le et al.",
      "startOffset" : 139,
      "endOffset" : 156
    }, {
      "referenceID" : 16,
      "context" : "We note that our model is the first tanh RNN model that achieves good performance on this task, even improving upon the method proposed in Le et al. (2015). In addition, we also formally compare with the previous results reported in Le et al. (2015); Arjovsky et al.",
      "startOffset" : 139,
      "endOffset" : 250
    }, {
      "referenceID" : 0,
      "context" : "(2015); Arjovsky et al. (2015), where our model (called as sTANH) has a hidden-layer size of 95, which is about the same number of parameters as in the tanh model of Arjovsky et al.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "(2015); Arjovsky et al. (2015), where our model (called as sTANH) has a hidden-layer size of 95, which is about the same number of parameters as in the tanh model of Arjovsky et al. (2015). Table 4 shows that our simple architecture improves upon the state-of-theart by 2.",
      "startOffset" : 8,
      "endOffset" : 189
    }, {
      "referenceID" : 17,
      "context" : "Similar result was also reported in Le et al. (2015).",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 8,
      "context" : "We overcome such limitation by considering a Multidimensional LSTM (Graves et al., 2007), which is explained in detail in Appendix B.",
      "startOffset" : 67,
      "endOffset" : 88
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we systematically analyse the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graphtheoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN’s over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the “depth” in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems, as we improve the state-of-the-art for sequential MNIST dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}