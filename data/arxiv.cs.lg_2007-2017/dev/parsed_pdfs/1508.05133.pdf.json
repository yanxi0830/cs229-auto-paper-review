{
  "name" : "1508.05133.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Steps Toward Deep Kernel Methods from Infinite Neural Networks",
    "authors" : [ "Tamir Hazan", "Tommi Jaakkola" ],
    "emails" : [ "tamir.hazan@gmail.com", "tommi@mit.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep neural networks have become widely adopted for tasks ranging from image labeling in computer vision to parsing and machine translation in natural language processing. The networks in these tasks usually consist of an input layer, several semistructured intermediate layers and an output layer. Surprisingly, as large, complex models, they appear easier to learn at scale, rendering state of the art performance (e.g., [?]) with increasing amounts of data and computation. The setting poses new questions for learning since the number of parameters in these models, mostly residing in the deep layers, may be substantially larger than what could be supported by the training examples. Many expect such networks to overfit while in practice they (often) do not, and their decision boundaries appear smooth. Our work suggests an explanation for this behavior based on deep and infinitely wide networks where the number of parameters is uncountably infinite.\nNeural networks with a single infinite intermediate layer have been considered by various works. [?] show these networks are universal approximators and [?, ?, ?] explore their properties in the context of Gaussian processes and kernel methods. Unfortunately, since these networks interact linearly with the input layer, they are limited in\nar X\niv :1\n50 8.\n05 13\n3v 1\n[ cs\n.L G\ntheir representation power. Moreover, the recent success of neural networks seems to rely on deep architecture while current infinite networks only encode the information of a single layer. Lastly, these works do not explain why learning the likelihood of infinite networks does not overfit, and the decision boundary of the learned network is simple.\nIn our work we extend the framework of kernel methods for infinite networks to multiple layers. We introduce stochastic kernels that are derived from Gaussian processes and encode the information of two infinite layers. We also provide a generalization bound for these networks, based on stability of regularized loss minimization, and attribute the simplicity of the learned deep infinite network to the fast convergence of algorithms on our learning framework.\nWe begin by introducing infinite neural networks with a single intermediate layer. We relate their learning units to integrals over functions in the Euclidean space with respect to the Gaussian distribution, as well as describe their connections to kernel functions. We subsequently construct the second layer and relate its learning units to expectations with respect to a Gaussian process. These expectations form stochastic kernel functions that encode the multilayer and infinitely wide neural network. Finally, we analyze the generalization properties of these networks and introduce a method to incorporate localities and non-linearities such as those arising from convolutional neural networks."
    }, {
      "heading" : "2 Background",
      "text" : "Neural networks form a successful framework for classification that imitate the activation function of neurons. Finite neural networks are usually described by a layered graph, see Figure 1. Its input layer consists of nodes that receive the input signal x ∈ Rd. Its subsequent layers consist of parameters that encode the classification pro-\ncess: its intermediate layers consist of activation nodes. Each activation node rely on its parameter to produce a linear response f(〈x,w〉) according to its parameters w. The function f(t) is called an activation function or a transfer function and it introduces non-linearities to the network. Transfer functions imitate the neuron behavior, activating its value whenever its linear input 〈x,w〉 is high enough. There are various forms of non-linear transfer functions, e.g., step and rectified linear functions. Recently, the rectified linear function ReLU(t) = max(0, t) was successfully used in neural networks as it carries the neuron signal better [?]. Another popular transfer function is the step function step(t) = 1[t ≥ 0] that attains the value one it t ≥ 0 and zero otherwise. The output of a network with a single intermediate layer linearly weights the activations f(〈x,w1〉), ..., f(〈x,wk〉) with the output parameters u1, ..., uk. Its classification is determined by the sign of ∑k i=1 uif(〈x,wi〉).\nA classical result by Hornik asserts that networks with one intermediate layer are universal approximators when the number of activation units k tends to infinity [?]. Consequently, neural networks have been studied in the infinite setting [?, ?, ?, ?, ?, ?]. In this setting there are infinitely many transfer functions f(〈w, x〉) each of them is indexed by w. Summing over infinitely many transfer functions is formalized by integrating over possible w. Formally, we replace ∑ i uif(〈x,wi〉)\nwith ∫ u(w)f(〈x,w〉)dµ(w). The measure µ(w) may be any probability distribution over Rd as long as this integral is finite, e.g, the Gaussian distribution dµ(w) = (2π)−d/2 exp(−‖w‖2/2).\nWhen taking a discriminative approach, one learns a neural network that best describes the training data S = {(x1, y1), ..., (xm, ym)}, where xi is a data instance (e.g., an image or a sentence) and yi is its semantic label. While learning an infinite network with a single intermediate layer, one needs to consider compact ways to represent the function u(w). Kernel methods can be used for this task while representing the classifier by its dual [?]. Particularly, the network’s output is an inner product between u(w) and an input-dependent function φx(w) = f(〈x,w〉)\n〈u, φx〉µ = ∫ u(w)f(〈x,w〉)dµ(w). (1)\nSince u(w) is trained over a finite space of feature functions it can be restricted without loss of generality to the linear span of the training feature functions φx1(w), ..., φxm(w), namely u(w) = ∑m i=1 αiφxi(w) for some real valued numbers α1, ..., αm. Therefore, when evaluating the output value of the network 〈u, φxj 〉µ it suffices to compute the kernel entries\nkf (xi, xj) = 〈φxi , φxj 〉µ = ∫ f(〈xi, w〉)f(〈xj , w〉)dµ(w).\nVarious works have already computed the kernel function for different transfer functions with respect to the Gaussian measure, including the rectified linear and the sign function [?, ?]. In all these cases, the kernel has an analytic form although the features φx(w) are not finite vectors but functions overRd. Explicitly, let ρi,j = 〈xi, xj〉/‖xi‖‖xj‖\nthen\nkReLU(xi, xj) = ‖xi‖‖xj‖ π sin ( arccos(ρi,j) ) + (π − arccos(ρi,j))ρi,j .\nkstep(xi, xj) = π − arccos(ρi,j).\nWhenever the measure is not Gaussian there is no analytic solution for the different kernels. Nevertheless, whenever the probability density function dµ(w) is log-concave (i.e., log(dµ(w)) is a concave function) then 〈x,w〉 is a log-concave function thus the sample complexity of the kernel function decays exponentially with the number of samples."
    }, {
      "heading" : "3 Stochastic kernels for deep and infinitely wide neural networks",
      "text" : "A deep learning architecture considers multiple intermediate layers. Deep architectures have proven successful as they allow to express non-linearities easily. Unfortunately, when considering multiple infinite layers there are difficulties to represent the network parameters. Such difficulties do not appear when considering finite layers since all parameters in all layers are vectors. However, when considering infinite layers, the parameters are functions (in the second intermediate layer) functions of functions (in the subsequent layer) and so on, see Figure 1. In the following we present the framework of learning with multiple intermediate layers. For the clarity of presentation we describe two intermediate layers. We refer to these networks as deep networks to differentiate them from the known networks with a single infinite layer.\nThe main challenge in working with deep infinite networks is to establish the space in which the deep neurons exist. The neurons of the second intermediate layer take as input the functions φx(w), which are the output of the first intermediate layer, i.e., φx(w) for any w ∈ Rd. Therefore, each neuron in the second layer is a function u : Rd → R that weights its input values (which are φx(w) for any w ∈ Rd) in a linear manner 〈u, φx〉µ. The output of each such neuron is the activation of the transfer function ψx(u) = f(〈φx, u〉µ). Therefore, the output layer of deep infinite network needs to take all its inputs, i.e., ψx(u) for any function u(·), and weight their activation by v(u). Thus the output layer computes the linear function 〈v, ψx〉ν . with respect the a measure ν. Next, we determine the measure space of v(u) in terms of stochastic processes.\nIt is natural to consider the activation of neurons in the second intermediate layer 〈φx, u〉µ with respect to the measure µ(w) using probabilistic terms. This linear function is the covariance of two random variables 〈φx, u〉µ = Ew∼µ [ φx(w)u(w)]. Sim-\nilarly, the activation of the output neuron is 〈v, ψx〉ν = Eu∼ν [ ψx(u)v(u)]. With this perspective, the functions u : Rd → R are chosen randomly according to the measure ν. Equivalently, ν is a stochastic process. In our work we restrict ourselves to a Gaussian process, a stochastic process for which any finite collection of random variables u(w1), ..., u(wk) has a multivariate Gaussian distribution. A Gaussian process is completely determined by its first and second order statistics. The\nmean function µ(w) of a Gaussian process is Eu∼ν [u(w)]. Its covariance function C(w1, w2) = Eu∼ν [u(w1), u(w2)]. We consider Gaussian process with zero mean function and a general covariance function, thus we denote ν = GP (C).\nTo learn a deep infinite network that linearly separates the training examples it suffices use the stochastic kernel function:\nk (2) f (xi, xj) = 〈ψxi , ψxj 〉ν = Eu∼GP (C) [ f(〈φxi , u〉)f(〈φxj , u〉) ] (2)\nRecall that the first layer responses are the transfer functions φxi(w) = f(〈w, xi〉), φxj (w) = f(〈w, xj〉). Thus a stochastic kernel for deep infinite network averages non-linearities while considering their covariances.\nAlthough the Gaussian process has infinitely many random variables, its unique properties allows to compute the stochastic kernel function analytically.\nTheorem 1. k\n(2) f (xi, xj) = E(z1,z2)∼N(0,Σ) [ f(z1)f(z2) ] z = (z1, z2) is a bivariate Gaussian random variable with zero mean and covariance matrix Σ:\nΣ = Ew1,w2\n( f(〈w1, xi〉)C(w1, w2)f(〈w2, xi〉) f(〈w1, xi〉)C(w1, w2)f(〈w2, xj〉)\nf(〈w1, xi〉)C(w1, w2)f(〈w2, xj〉) f(〈w1, xj〉)C(w1, w2)f(〈w2, xj〉)\n) (3)\nw1, w2 are chosen independently from a d−dimensional multivariate Gaussian with zero mean and unit covariance, i.e., N(0, I).\nProof. z1 = 〈φxi , u〉 is a Gaussian random variable with zero mean1 Similarly, z2 = 〈φxj , u〉 is a Gaussian random variable and both z1, z2 are jointly Gaussian. Thus z = (z1, z2) is a bivariate Gaussian random variable with zero mean and some covariance matrix Σ. The expected value of a Gaussian process reduces to\nEu∼GP (C) [ f(〈φxi , u〉)f(〈φxj , u〉) ] = E(z1,z2)∼N(0,Σ) [ f(z1)f(z2) ] .\nThe covariance matrix of Σ is a 2 × 2 matrix whose (r, s) entry is Ez∈N(0,Σ)E[zrzs]. Recall that z1 = Ew[φxi(w)u(w)] and that Σ11 = E[z 2 1 ], then\nΣ11 = Eu [ Ew1 [φxi(w1)u(w1)] · Ew2 [φxi(w2)u(w2)] ] = Ew1,w2 [ φxi(w1)φxj (w2)Eu[u(w1)u(w2)] ] = Ew1,w2 [ φxi(w1)φxj (w2)C(w1, w2) ] .\nWe used Fubini’s theorem to change the order of integration. The values of Σr,s then follow in the same manner, while recalling that φxi(w1) = f(〈xi, w1〉) and φxj (w2) = f(〈xj , w2〉) .\n1This is a classical result and can be shown by working with the Riemann-Stieltjes integral, decomposing it to finite sums. Since any finite instantiation of a Gaussian process is a multivariate Gaussian random variable with zero mean, the Riemann-Stieltjes sum is also a Gaussian random variable, thus the limit (using the characteristic function) is also Gaussian.\nAn important family of Gaussian processes is described by shift-invariant covariance functions, namely C(w1, w2) = c(w1 − w2). Bochner’s theorem represents such covariance functions as Ew,b[g(〈w1, w〉 + b)g(〈w2, w〉 + b)], where w is drawn from a distribution ρ over Rd, b is drawn from the uniform distribution over [0, 2π] and g(t) = √ 2 cos(t) [?]. Whenever ρ is known we are able to efficiently compute a stochastic kernel for deep infinite networks and shift-invariant covariance functions:\nCorollary 1. Let C(w1, w2) = c(w1 − w2) be a shift-invariant covariance function and let ρ be its corresponding measure derived by Bochner’s theorem. Consider the 6× 6 covariance matrix\nΣ̂ =  ‖xi‖2 〈xi, w〉 〈xi, xj〉〈xi, w〉 ‖w‖2 〈w, xj〉 〈xi, xj〉 〈xj , w〉 ‖xj‖2 ⊗ ( 1 0 0 1 ) .\nA ⊗ B the tensor product of two matrices. Let g(t) = √\n2 cos(t) and assume that b is drawn form the uniform distribution over [0, 2π] and w is drawn according to ρ and ẑ ∼ N(0, Σ̂) is a multivariate Gaussian. Then the covariance matrix Σ of the stochastic kernel for deep infinite neural networks k(2)f (xi, xj) = E(z1,z2)∼N(0,Σ)[f(z1)f(z2)] is\nΣ = Ew,b,ẑ ( f(ẑ1)f(ẑ2)g(ẑ3 + b)g(ẑ4 + b) f(ẑ1)f(ẑ6)g(ẑ3 + b)g(ẑ4 + b) f(ẑ1)f(ẑ6)g(ẑ3 + b)g(ẑ4 + b) f(ẑ5)f(ẑ6)g(ẑ3 + b)g(ẑ4 + b) )\nProof. The entries of the covariance matrix of the stochastic kernel are derived in Equation (3) using Σr,s = hr,s(ẑ1, ..., ẑ6) where z1 = 〈w1, xi〉, z2 = 〈w2, xi〉, z3 = 〈w1, w〉, z4 = 〈w2, w〉, z5 = 〈w1, xj〉, z6 = 〈w2, xj〉. Since w1, w2 ∼ N(0, I) are independent then ẑ is a multivariate Gaussian with zero mean and its distribution is fully determined by its covariance matrix Σ̂. The corollary then follows by direct computation of the covariance matrix, e.g.,, Σ̂1,3 = Eẑ[ẑ1ẑ3] = ∑ r,sEw1 [w1,rw1,sxi,rws] =∑\nr,s xi,rws · Ew1 [w1,rw1,s] and Ew1 [w1,rw1,s] = 1[r = s] is the indicator function that equals one if r = s and zero otherwise.\nThe ability to realize the measure ρ that is suggested by Bochner’s theorem determines the validity of this approach. Bochner’s theorem relates a shift-invariance function c(w1−w2) to Fourier transform, thus ρ can be recovered by its inverse-transform. There are some special covariance functions for which this measure is known. For example, the covariance function C(w1, w2) = exp(−‖w1 − w2‖1) that relates to the Ornstein-Uhlenbeck Gaussian process can be computed using the Cauchy distribution dρ(w) = ∏d i=1(π(1 + w 2 i )) −1. Whenever the covariance function defines a a squared exponential Gaussian process, C(w1, w2) = β exp(−‖w1 −w2‖22), the stochastic kernel for deep neural networks can be computed analytically. This follows from the observation that the Gaussian process couples the independent d−dimensional Gaussians random variables w1, w2 to a 2d−dimensional Gaussian variable w = (w1, w2) with correlation α:\nCorollary 2. Consider the covariance functionC(w1, w2) = (1+2α)1+d/2 exp(−α‖w1− w2‖2/2). Consider the 4× 4 covariance matrix\nΣ̂ = ( ‖xi‖2 〈xi, xj〉 〈xi, xj〉 ‖xj‖2 ) ⊗ ( 1 + α α α 1 + α ) Then the covariance matrix Σ of the stochastic kernel for deep neural network k(2)f (xi, xj) = E(z1,z2)∼N(0,Σ)[f(z1)f(z2)] is\nΣ = Eẑ∼N(0,Σ̂) ( f(ẑ1)f(ẑ2) f(ẑ1)f(ẑ4) f(ẑ1)f(ẑ4) f(ẑ3)f(ẑ4) )\nMoreover, k(2)ReLU(xi, xj) and k (2) step(xi, xj) have analytic forms.\nProof. Considering Equation (3) we note that gI(w1)gI(w2)C(w1, w2) = (1+2α)gΣ̂(w), where gI(wi) is the d−dimensional Gaussian density function N(0, I) and gΣ̂(w) is the 2d−dimensional Gaussian density function N(0, Σ̃). We denote by A ⊗ B the tensor product of two matrices, thus\n(1 + 2α)Σ̃ = ( 1 + α α α 1 + α ) ⊗ Id×d\nThe form of Σ̂ is attained when setting ẑ1 = 〈w1, xi〉, ẑ2 = 〈w2, xi〉, ẑ3 = 〈w1, xj〉, ẑ4 = 〈w2, xj〉. With this notation, the form of Σ is a direct consequence of Equation (3).\nTo compute the entries of Σ when f(t) = ReLU(t) we recall that whenever z′1, z ′ 2 ∈ N(0,Σ′) with Σ′11 = σ21 , Σ′12 = Σ′21 = ρσ1σ2, Σ′22 = σ22 thenEz′1,ẑ′2 [f(z ′ 1)f(z ′ 2)] = h(σ1, σ2, ρ) and h(σ1, σ2, ρ) = σ1σ2π sin(arccos(ρ))+ρ(π−arccos(ρ)). Then ΣReLU =( h( √ 1 + α‖xi‖, √ 1 + α‖xi‖, α1+α ) h( √ 1 + α‖xi‖, √ 1 + α‖xj‖, α1+α · 〈xi,xj〉 ‖xi|‖xj‖ )\nh( √ 1 + α‖xi‖, √\n1 + α‖xj‖, α1+α · 〈xi,xj〉 ‖xi|‖xj‖ ) h(\n√ 1 + α‖xj‖, √ 1 + α‖xj‖, α1+α )\n)\nThus, k(2)ReLU(xi, xj) = E(z1,z2)∼N(0,Σ)[f(z1)f(z2)] is a recursive application of h(·) with the appropriate parameters.\nTo compute the entries of Σ when f(t) = 1[t ≥ 0] we recall that whenever z′1, z′2 ∈ N(0,Σ′) with Σ′11 = σ 2 1 , Σ ′ 12 = Σ ′ 21 = ρσ1σ2, Σ ′ 22 = σ 2 2 then Ez′1,ẑ′2 [f(z ′ 1)f(z ′ 2)] = h(ρ) = π − arccos(ρ). Then\nΣstep =\n( h( α1+α ) h( α 1+α · 〈xi,xj〉 ‖xi|‖xj‖ )\nh( α1+α · 〈xi,xj〉 ‖xi|‖xj‖ ) h( α 1+α )\n)\nAs before, k(2)step(xi, xj) = E(z1,z2)∼N(0,Σ)[f(z1)f(z2)] is a recursive application of h(·) with the appropriate parameters.\nDeep neural networks are usually applied to multiclass problems, where there are more than two labels to classify. Thus the label space resides in a discrete set y ∈\n{1, ...,K}. For notational convenience we focus above on binary classification, where y ∈ {−1, 1} is determined by the sign of the output layer 〈v, φx〉. In multiclass setting, a data-instance x can belong to any of theK classes. A standard extension of the above setting to multiclass learning is to introduce K decision boundaries v1(u), ..., vk(u). Multiclass prediction is performed by choosing the decision which is most certain, i.e., arg maxi〈vi, ψx〉. In the next section we describe the generalization properties of deep infinite neural networks in the multiclass setting."
    }, {
      "heading" : "4 Deep infinite networks, generalization and experimental validation",
      "text" : "The practice of neural networks proves that they do not overfit, even when the number of learned parameters is orders of magnitude larger than the number of training examples. In the following we address this scenario while suggesting some insight for why infinite networks generalize well. We show that generalization is mostly dependent on the expressive power of the output layer, which is regularized by its capacity. Consider a multiclass deep infinite network v1(u), ..., vk(u) that classifies the functions ψxi(u) according to the most certain linear response function yv(x) = arg maxi〈vi, ψx〉. Since each decision function vi(u) interacts linearly with the training data, it must be a finite sum of these functions, i.e., vi(u) = ∑m j=1 αi,jψxj (u). Therefore, as long as the functions ψxi(u) are simple (e.g., truncated linear functions in the case of ReLU units), the capacity of the deep infinite network is limited by the size of the training data. Whenever there are stronger guarantees on the data, i.e., that the training data is separable with a margin, they translate to a stronger regularization on vk(u) that is derived from the passive-aggressive learner [?]. To be more precise, we say that the data is separable when there are functions v∗1(u), ..., v ∗ k(u) that classifies correctly any data instance. Formally, for any data-label pair (x, y) there holds y = yv∗(x). These data-label pairs are separated with a margin if y = yv∗(x) and 〈v∗y , ψx〉 ≥ 1 + maxi6=y〈v∗i , ψx〉. In this setting, the kernel version of the passiveaggressive algorithm ensures that vi(u) = ∑t j=1 αi,jψxj (u), where t ≤ R2 ∑ i ‖v∗i ‖2 and ‖ψx‖2 ≤ R. Thus, whenever there is a separation with margin and the training size m t the passive-aggressive analysis ensures that the deep learner has restricted capacity thus a simple form.\nUnfortunately, the separable setting rarely exists in practice. Nevertheless, deep learners perform well in the non-separable setting. Usually deep learning schemes use the logistic regression framework, that maximizes the conditional probability of the training data S = {(x1, y1), ..., (xm, ym)}. The conditional probability follows the Gibbs distribution pv(y|x) = exp(〈vy, ψx〉)/Z(v) where Z(v) = ∑ i exp(〈vi, ψx〉) is the partition function. Thus the parameters of the network are learned by the optimization program:\nvS = arg max v\n1\nm m∑ i=1 log pv(yi|xi) + λm 2 ∑ j ‖vj‖2 (4)\nAs this is an infinite convex program it is appealing to consider its dual. The dual\nprogram is minα ∑ i,k αi,k logαi,k + ∑m i=1 ‖vi(α)‖2/2λm where v(α) = ∑ i(ψxi −\n( ∑ k αi,kψxk)) and ∑ k αi,k = 1. The dual program is smooth and strongly convex, therefore enjoys rapid convergence, i.e., with O(log(1/ )) updates to the elements α a dual exponentiated coordinate descent achieves an −optimal dual solution [?]. Although this algorithm achieves a good primal solution in practice, it does not guarantee that v(α) is an −optimal primal solution as well. Recently, many efficient algorithms were devised to achieve both dual and primal guarantee with O(log(1/ )) steps (cf. [?]). These algorithms aggregate data points ψxi to their separators v(u) therefore after a small number of steps a good, yet simple separator is reached. Said differently, although different separators may exist around vS(u) the algorithm outputs a fairly simple separator as it is regularized by an early stopping criterion.\nConsidering the learning problem in Equation (4) as a loss minimization task, it measures the average log-loss given training data. By the above, the empirical risk minimizer vS is simple, i.e., it consists of O(log(1/ )) functions ψx(u). We turn to show that this simple empirical risk minimizer also generalizes well, it achieves a similar log-loss even when the data-label pairs are sampled from their true distribution in the world.\nTheorem 2. Assume that ‖φx‖ ≤ 1 and that the training examples are sampled independently from the data-label generating distribution (x, y) ∼ D. Denote log-risk by LD(v) = E(x,y)∼Dpv(y|x) and the empirical risk by LS(v) = 1m ∑m i=1 log pv(yi|xi). Consider vS as defined in Equation (4), then |LD(vS)− LS(vS)| ≤ 1/mλm.\nProof. Generalization by stability for convex and Lipschitz loss functions with strongly convex regularizer was established in [?, ?, ?]. Although the technical details are obscured in some of these results, we rely on their derivations (specifically [?] Theorem 22 and [?] Theorem 2). The benefit of working with stability is that its basic concepts, convexity and Lipschitz continuity, readily generalize to infinite spaces. To apply generalization via stability to multiclass logistic regression we note that − log pv(y|x) is convex. Also, it is 1−Lipschitz since its gradient is uniformly bounded by 1 whenever ‖ψx‖ ≤ 1.\nThe regularization ratio λm is chosen such that mλm goes to zero as m tends to infinity. The important conclusion of the above theorem is that infinite models does not necessarily overfit, as long as the infinite model interacts in a constrained manner with the data. In our case the infinite model is constrained by convexity and Lipschitz continuity. These two properties stabilize the learning procedure while ensuring that small changes in v do not change the prediction by much.\nNext, we turn to experimentally validate our framework. The effectiveness of infinite network with a single infinite layer using the kernels kReLU(xi, xj) was already demonstrated by [?, ?, ?]. Thus in the following we show that our stochastic kernels k(2)ReLU(xi, xj) with a squared exponential Gaussian process improves upon kReLU(xi, xj). We run our kernels over MNIST digit database. This dataset is the standard entry point of neural networks and kernel methods.\nOur stochastic kernel k(2)ReLU(xi, xj) was able to separate the training data completely with only 50 iterations, while kReLU(xi, xj) that encodes a single infinite layer\ndid not (it nearly separated all examples). This validates the assertion that the stochastic kernel is more expressive than the single layer kernel. As for test results, the average error over all digits is 1.7% for the stochastic kernel and 1.9% for the single layer kernel. Although the improvement is modest in terms of the overall success rate (only 0.2%) it might be insightful to compare it to the possible gain over the errors of the single layer kernel, namely 0.2/1.9 which is about 10% gain. Lastly, since our kernel function is computed analytically, it is trained as fast as any kernel method."
    }, {
      "heading" : "5 Non-linearities",
      "text" : "The infinite layers, presented in Section 2 and Section 3, are limited in their expressive power. In the first intermediate layer, the inner product 〈x,w〉 is performed for any w ∈ Rd, while each parameter w acts globally on all input entries x ∈ Rd linearly. Similarly, in the second layer, the function u(w) acts linearly and globally on every φx(w) = f(〈w, x〉). These interactions ignore spatial information in the vectors x or the feature function φx(w), spatial information that is important in computer vision and language processing applications. Current deep learning architectures exploit spatial information using convolutions. These convolutions are applied to patches in an image, or equivalently to overlapping subsets of the data instance x, and recursively to their functions. These operations introduce important aspects of non-linearity and locality. Our approach can be extended to deal with such operations, thus able to increase the expressiveness of our approach to various non-linearities.\nTo describe a convolution-based operation in the first intermediate layer, we transform the data instance x ∈ Rd to subsets of its elements x(1), ...., x(P ) ∈ Rd1 , where x(p) ⊂ x. For each such subset we learn infinitely many responses w ∈ Rd1 , while each response outputs φx,p(w) = f(〈w, x(p)〉). Thus, φx = (φx,1, ..., φx,P ) is a P−dimensional function, φx : Rd1 → RP . Note that in Section 2 the feature function φx mapped Rd to R.\nConvolution based operations in the second intermediate layer may also be applied. The feature function φx is transformed to subsets of its elements φ (1) x , ..., φ (Q) x where φ (q) x ⊂ φx, i.e., φ(q)x : Rd1 → Rd2 that is attained by restricting to φx(w) to some of its coordinates. Each of these subsets is weighted by u : Rd1 → Rd2 and its resulting response is ψx,q(u) = f(〈u, φx(q)〉), while 〈u, φx(q)〉 = Ew[〈u(w), φqx(w)〉] and the latter inner product 〈u(w), φqx(w)〉 is between two vectors in Rd2 .\nThe above two constructions show how to integrate convolution-type non-linearities in deep infinite networks. The appropriate kernels follow a straight forward derivation of these higher dimension constructions."
    }, {
      "heading" : "6 Related work",
      "text" : "Neural networks, kernel methods and Gaussian processes have had a significant impact on the machine learning community and a full exposition of these methods can be found in machine learning textbooks on neural networks [?], kernel methods [?] and Gaussian processes [?].\nNeural networks are attracting a considerable attention in the last few years. Their practical success is unmatched in several machine learning applications (e.g., [?]). In recent years it was possible to construct deep learning architectures with considerable number of parameters that is significantly larger than the number of training examples. Surprisingly, these networks avoid overfitting. Several machine learning theories were devised to explain how deep networks avoid overfitting based on dropouts (e.g., [?, ?]). Our approach is different since we represent neural networks with significant amount of parameters as infinite networks with multiple layers. We encode the neurons responses in functions, while each layer increases the complexity of its functions, namely the first layer consists of functions over the Euclidean space and the second layer consists of Gaussian processes. We avoid overfitting since our algorithm achieves an almost optimal solution with a few steps, thus our resulting classifier is simple to represent and regularized by early stopping. We provide a generalization bound for our classifier based on stability [?, ?, ?].\nInfinite neural networks were introduced by [?, ?] in the context of Bayesian learning. They analyze the predictive probability of a neural network with an infinitely wide intermediate layer. In particular, when the transfer function is bounded, this predictive probability converges to a Gaussian process. When resolving the covariance function of this process, [?] realized the kernel kerf(xi, xj) along with other kernel functions. This work differs from ours in a few respects. First, our work does not consider the predictive probability of labels given data but rather we aim at maximizing the likelihood of infinitely wide layers, a task that initially was supposed to overfit and generalize poorly [?]. We establish the prediction power of our approach using stability. Second, we build on multiple intermediate layers while trying to analyze the success of deep learning architectures, as opposed to [?, ?]. Lastly, our work considers Gaussian processes differently than [?]. We use Gaussian process to define a measure over our second (e.g., deep) intermediate layer.\nMore recently, researchers explored different algorithms to learn infinite neural networks [?]. This work formulates learning an infinite network as an infinite convex program and devise an incremental algorithm that is based on its dual representation. [?] suggest to optimize an infinite networks with a single layer using randomization to decrease the computational complexity of the learning algorithm. Our work addresses other properties of learning infinite networks, mainly Gaussian processes for constructing multiple layers and analyze how infinite networks avoid overfitting.\nKernel methods for infinite neural networks are further explored in [?, ?]. These works introduce the kernels kstep(xi, xj), kReLU(xi, xj) along with other kernels thus augment the works of [?, ?]. Moreover, they introduce kernel composition approach to simulate deep architecture. Our work differs in the way we address and analyze deep architectures of infinite networks. We construct deep layers that use as input their previous layer using Gaussian processes. Connections between kernel methods and Gaussian processes were left as an open problem in [?]. We also introduce a way to incorporate non-linearities and invariances such as convolutional neural networks in our framework, another open problem raised by [?]. In addition, we analyze why our networks avoid overfitting. [?] demonstrate the effectiveness of these kernels in language processing.\nIn our work we provide unbiased estimate to our kernels in the second layer us-\ning Bochner’s theorem. These kernels consider a shift invariant covariance function. [?] suggest the same estimator for kernel functions in the context of random features in kernel methods. [?] suggested improved methods to reduce the variance of these estimates. Such estimators were recently used within kernel methods to match deep learning results in language processing [?, ?]."
    }, {
      "heading" : "7 Discussion",
      "text" : "Deep neural networks are successful in machine learning applications although the number of their parameters is orders of magnitude larger than the number of training examples. In this work we explain this behavior using deep infinite neural networks. We construct stochastic kernels that rely on Gaussian processes to encode such networks. We also explain how to introduce locality and non-linearity to such networks, similarly to the ones introduced by convolution neural networks. Lastly, we provide generalization bounds and regularity conditions that explain why these networks do not overfit. We present our framework with only two intermediate layers mainly for simplicity. It can be extended to any depth but the higher layers may not use nonlinearities. The problem of finding analytic forms of stochastic kernels that encode arbitrarily deep layers with non-linearities is largely open.\nThe work combines mostly separate areas in machine learning, including kernel methods, neural networks and Gaussian processes. As such, there are many direction that still need to be explored. Importantly, which non-linearities are significant in deep infinite networks and whether they can be learned from data. What probabilities best fit this framework and are there other properties of stochastic processes, besides of covariance, that control learning?"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Contemporary deep neural networks exhibit impressive results on practical problems. These networks generalize well although their inherent capacity may extend significantly beyond the number of training examples. We analyze this behavior in the context of deep, infinite neural networks. We show that deep infinite layers are naturally aligned with Gaussian processes and kernel methods, and devise stochastic kernels that encode the information of these networks. We show that stability results apply despite the size, offering an explanation for their empir-",
    "creator" : "LaTeX with hyperref package"
  }
}