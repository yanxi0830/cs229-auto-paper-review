{
  "name" : "1507.08788.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and Convexity",
    "authors" : [ "Ohad Shamir" ],
    "emails" : [ "ohad.shamir@weizmann.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of recovering the top k left singular vectors of a d× n matrix X = (x1, . . . ,xn), where k d. This is equivalent to recovering the top k eigenvectors of XX>, or equivalently, solving the optimization problem\nmin W∈Rd×k:W>W=I\n−W> ( 1\nn n∑ i=1 xix > i\n) W. (1)\nThis is one of the most fundamental matrix computation problems, and has numerous uses (such as low-rank matrix approximation and principal component analysis).\nFor large-scale matrices X , where exact eigendecomposition is infeasible, standard deterministic approaches are based on power iterations or variants thereof (e.g. the Lanczos method) [8]. Alternatively, one can exploit the structure of Eq. (1) and apply stochastic iterative algorithms, where in each iteration we update a current d × k matrix W based on one or more randomly-drawn columns xi of X . Such algorithms have been known for several decades ([14, 17]), and enjoyed renewed interest in recent years, e.g. [2, 4, 3, 10, 6]. Another stochastic approach is based on random projections, e.g. [9, 20].\nUnfortunately, each of these algorithms suffer from a different disadvantage: The deterministic algorithms are accurate (runtime logarithmic in the required accuracy , under an eigengap condition), but require a full pass over the matrix for each iteration, and in the worst-case many such passes would be required (polynomial in the eigengap). On the other hand, each iteration of the stochastic algorithms is cheap, and their number is independent of the size of the matrix, but on the flip side, their noisy stochastic nature means they are not suitable for obtaining a high-accuracy solution (the runtime scales polynomially with ).\nRecently, [19] proposed a new practical algorithm, VR-PCA, for solving Eq. (1), which has a “best-ofboth-worlds” property: The algorithm is based on cheap stochastic iterations, yet the algorithm’s runtime is logarithmic in the required accuracy . More precisely, for the case k = 1, xi of bounded norm, and when\nar X\niv :1\n50 7.\n08 78\n8v 1\n[ cs\n.L G\n] 3\n1 Ju\nl 2 01\nthere is an eigengap of λ between the first and second leading eigenvalues of the covariance matrix 1nXX >, the required runtime was shown to be on the order of\nd ( n+ 1\nλ2\n) log ( 1 ) . (2)\nThe algorithm is therefore suitable for obtaining high accuracy solutions (the dependence on is logarithmic), but essentially at the cost of onlyO(log(1/ )) passes over the data. The algorithm is based on a recent variance-reduction technique designed to speed up stochastic algorithms for convex optimization problems ([13]), although the optimization problem in Eq. (1) is inherently non-convex. See Section 3 for a more detailed description of this algorithm, and [19] for more discussions as well as empirical results.\nThe results and analysis in [19] left several issues open. For example, it is not clear if the quadratic dependence on 1/λ in Eq. (2) is necessary, since it is worse than the linear (or better) dependence that can be obtained with the deterministic algorithms mentioned earlier, as well as analogous results that can be obtained with similar techniques for convex optimization problems (where λ is the strong convexity parameter). Also, the analysis was only shown for the case k = 1, whereas often in practice, we may want to recover k > 1 singular vectors simultaneously. Although [19] proposed a variant of the algorithm for that case, and studied it empirically, no analysis was provided. Finally, the convergence guarantee assumed that the algorithm is initialized from a point closer to the optimum than what is attained with standard random initialization. Although one can use some other, existing stochastic algorithm to do this “warm-start”, no end-to-end analysis of the algorithm, starting from random initialization, was provided.\nIn this paper, we study these and related questions, and make the following contributions:\n• We propose a variant of VR-PCA to handle the k > 1 case, and formally analyze its convergence (Section 3). The extension to k > 1 is non-trivial, and requires tracking the evolution of the subspace spanned by the current solution at each iteration.\n• In Section 4, we study the convergence of VR-PCA starting from a random initialization. And show that with a slightly smarter initialization – essentially, random initialization followed by a single power iteration – the convergence results can be substantially improved. In fact, a similar initialization scheme should assist in the convergence of other stochastic algorithms for this problem, as long as a single power iteration can be performed.\n• In Section 5, we study whether functions similar to Eq. (1) have hidden convexity properties, which would allow applying existing convex optimization tools as-is, and improve the required runtime. For the k = 1 case, we show that this is in fact true: Close enough to the optimum, and on a suitablydesigned convex set, such a function is indeed λ-strongly convex. Unfortunately, the distance from the optimum has to be O(λ), and this precludes a better runtime in most practical regimes. However, it still indicates that a better runtime and dependence on λ should be possible."
    }, {
      "heading" : "2 Some Preliminaries and Notation",
      "text" : "We consider a d× n matrix X composed of n columns (x1, . . . ,xn), and let\nA = 1\nn XX> =\n1\nn n∑ i=1 xix > i .\nThus, Eq. (1) is equivalent to finding the k leading eigenvectors of A.\nWe generally use bold-face letters to denote vectors, and capital letters to denote matrices. We let Tr(·) denote the trace of a matrix, ‖ · ‖F to denote the Frobenius norm, and ‖ · ‖sp to denote the spectral norm. A symmetric d × d matrix B is positive semidefinite, if infz∈Rd z>Bz ≥ 0. A is positive definite if the inequality is strict. Following standard notation, we write B 0 to denote that A is positive semidefinite, and B C if B − C 0. B 0 means that B is positive definite.\nA twice-differentiable function F on a subset of Rd is convex, if its Hessian is alway positive semidefinite. If it is always positive definite, and λI for some λ > 0, we say that the function is λ-strongly convex. If the Hessian is always ≺ sI for some s ≥ 0, then the function is s-smooth."
    }, {
      "heading" : "3 The VR-PCA Algorithm and a Block Version",
      "text" : "We begin by recalling the algorithm of [19] for the k = 1 case (Algorithm 1), and then discuss its generalization for k > 1.\nAlgorithm 1 VR-PCA: Vector version (k = 1) 1: Parameters: Step size η, epoch length m 2: Input: Data matrix X = (x1, . . . ,xn); Initial unit vector w̃0 3: for s = 1, 2, . . . do 4: ũ = 1n ∑n i=1 xi ( x>i w̃s−1\n) 5: w0 = w̃s−1 6: for t = 1, 2, . . . ,m do 7: Pick it ∈ {1, . . . , n} uniformly at random 8: w′t = wt−1 + η ( xit ( x>itwt−1 − x > it w̃s−1 ) + ũ\n) 9: wt =\n1 ‖w′t‖ w′t 10: end for 11: w̃s = wm 12: end for\nThe basic idea of the algorithm is to perform stochastic updates using randomly-sampled columns xi of the matrix, but interlace them with occasional exact power iterations, and use that to gradually reduce the variance of the stochastic updates. Specifically, the algorithm is split into epochs s = 1, 2, . . ., where in each epoch we do a single exact power iteration with respect to the matrix A (by computing ũ), and then perform m stochastic updates, which can be re-written as\nw′t = (I + ηA)wt−1 + η ( xitx > it −A ) (wt−1 − w̃s−1) , wt = 1\n‖w′t‖ wt,\nThe first term is essentially a power iteration (with a finite step size η), whereas the second term is zeromean, and with variance dominated by ‖wt−1 − w̃s−1‖2. As the algorithm progresses, wt−1 and w̃s−1 both converge toward the same optimal point, hence ‖wt−1 − w̃s−1‖2 shrinks, eventually leading to an exponential convergence rate.\nTo handle the k > 1 case (where more than one eigenvector should be recovered), one simple technique is deflation, where we recover the leading eigenvectors v1,v2, . . . ,vk one-by-one, each time using the k = 1 algorithm. However, a disadvantage of this approach is that it requires a positive eigengap between all top k eigenvalues, otherwise the algorithm is not guaranteed to converge. Thus, an algorithm which simultaneously recovers all k leading eigenvectors is preferable.\nWe will study a block version of Algorithm 1, presented as Algorithm 2. It is mostly a straightforward generalization (similar to how power iterations are generalized to orthogonal iterations), where the d-dimensional vectors wt−1, w̃s−1,u are replaced by d× k matrices Wt−1, W̃s−1, Ũ , and normalization is replaced by orthogonalization1. Indeed, Algorithm 1 is equivalent to Algorithm 2 when k = 1. The main twist in Algorithm 2 is that instead of using W̃s−1, Ũ as-is, we perform a unitary transformation (via the k × k orthogonal matrix Bt−1) which maximally aligns them with Wt−1. Note that Bt−1 is a k × k matrix, and since k is assumed to be small, this does not introduce significant computational overhead.\nAlgorithm 2 VR-PCA: Block version Parameters: Rank k, Step size η, epoch length m Input: Data matrix X = (x1, . . . ,xn); Initial d × k matrix W̃0 with orthonormal columns for s = 1, 2, . . . do Ũ = 1n ∑n i=1 xi ( x>i W̃s−1\n) W0 = W̃s−1 for t = 1, 2, . . . ,m do Bt−1 = V U\n>, where USV > is an SVD decomposition of W>t−1W̃s−1 B Equivalent to Bt−1 = arg minB>B=I ‖Wt−1 − W̃s−1B‖2F\nPick it ∈ {1, . . . , n} uniformly at random W ′t = Wt−1 + η ( xit ( x>itWt−1 − x > it W̃s−1Bt−1 ) + ŨBt−1 ) Wt = W ′ t ( W ′> t W ′ t\n)−1/2 end for W̃s = Wm\nend for\nWe now turn to provide a formal analysis of Algorithm 2, which directly generalizes the analysis of Algorithm 1 given in [19]:\nTheorem 1. Define the d × d matrix A as 1nXX > = 1n ∑n i=1 xix > i , and let Vk denote the d × k matrix composed of the eigenvectors corresponding to the largest k eigenvalues. Suppose that\n• maxi ‖xi‖2 ≤ r for some r > 0.\n• A has eigenvalues s1 > s2 ≥ . . . ≥ sd, where sk − sk+1 = λ for some λ > 0.\n• k − ‖V >k W̃0‖2F ≤ 1 2 .\nLet δ, ∈ (0, 1) be fixed. If we run the algorithm with any epoch length parameter m and step size η, such that\nη ≤ cδ 2 r2 λ , m ≥ c ′ log(2/δ) ηλ , kmη2r2 + rk\n√ mη2 log(2/δ) ≤ c′′ (3)\n1The normalization Wt = W ′t ( W ′> t W ′ t )−1/2 ensures that Wt has orthonormal columns. We note that in our analysis, η is\nchosen sufficiently small so that W ′> t W ′ t is always invertible, hence the operation is well-defined.\n(where c, c′, c′′ designate certain positive numerical constants), and for T = ⌈ log(1/ ) log(2/δ) ⌉ epochs, then with probability at least 1− dlog2(1/ )eδ, it holds that\nk − ‖V >k W̃T ‖2F ≤ .\nFor any orthogonal W , k − ‖V >k W‖2F lies between 0 and k, and equals 0 when the column spaces of Vk and W are the same (i.e., when W spans the k leading singular vectors). According to the theorem, taking appropriate2 η = Θ(λ/(kr)2), and m = Θ((rk/λ)2), the algorithm converges with high probability to a high-accuracy approximation of Vk. Moreover, the runtime of each epoch of the algorithm equals O(mdk2 + dnk). Overall, we get the following corollary:\nCorollary 1. Under the conditions of Theorem 1, there exists an algorithm returning W̃T such that k − ‖V >k W̃T ‖2F ≤ with arbitrary constant accuracy, in runtime O ( dk(n+ r 2k3 λ2 ) log(1/ ) ) .\nThis runtime bound is the same3 as that of [19] for k = 1. The proof of Theorem 1 appears in Subsection 6.1, and relies on a careful tracking of the evolution of the potential function k − ‖V >k W̃t‖2F . An important challenge compared to the k = 1 case is that the matrices Wt−1 and W̃s−1 do not necessarily become closer over time, so the variance-reduction intuition discussed earlier no longer applies. However, the column space of Wt−1 and W̃s−1 do become closer, and this is utilized by introducing the transformation matrix Bt−1. We note that although Bt−1 appears essential for our analysis, it isn’t clear that using it is necessary in practice: In [19], the suggested block algorithm was Algorithm 2 with Bt−1 = I , which seemed to work well in experiments. In any case, using this matrix doesn’t affect the overall runtime beyond constants, since the additional runtime of computing and using this matrix (O(dk2)) is the same as the other computations performed at each iteration.\nA limitation of the theorem above is the assumption that the initial point W̃0 is such that k−‖V >k W̃0‖2F ≤ 1 2 . This is a non-trivial assumption, since if we initialize the algorithm from a random d × O(1) orthogonal matrix W̃0, then with overwhelming probability, ‖V >k W̃0‖2F = O(1/d). However, experimentally the algorithm seems to work well even with random initialization [19]. Moreover, if we are interested in a theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a W̃0. The idea is that W̃0 is only required to approximate Vk up to constant accuracy, so purely stochastic algorithms (which are good in obtaining a low-accuracy solution) are quite suitable. In the next section, we further delve into these issues, and show that in our setting such algorithms in fact can be substantially improved."
    }, {
      "heading" : "4 Warm-Start and the Power of a Power Iteration",
      "text" : "In this section, we study the runtime required to compute a starting point satisfying the conditions of Theorem 1, starting from a random initialization. Combined with Theorem 1, this gives us an end-to-end analysis of the runtime required to find an -accurate solution, starting from a random point. For simplicity, we will only discuss the case k = 1, i.e. where our goal is to compute the single leading eigenvector v1, although\n2Specifically, we can take m = c′ log(2/δ)/ηλ and η = aδ2/r2λ, where a is sufficiently small to ensure that the first and third condition in Eq. (3) holds. It can be verified that it’s enough to take a = min { c, c ′′\n4δ2ck log(2/δ) , 1 4δ2c\n( c′′\nk log(2/δ)\n)2} .\n3[19] showed that it’s possible to further improve the runtime for sparse X , replacing d by the average column sparsity ds. This is done by maintaining parameters in an implicit form, but it’s not clear how to implement a similar trick in the block version, where k > 1.\nour observations can be generalized to k > 1. In the k = 1 case, Theorem 1 kicks in once we find a vector w satisfying 〈v1,w〉2 ≥ 12 .\nAs mentioned previously, one way to get such a w is to run a purely stochastic algorithm, which computes the leading eigenvector of a covariance matrix E[xx>] given a stream of i.i.d. samples x. We can easily use such an algorithm in our setting, by sampling columns from our matrix X = (x1, . . . ,xn) uniformly at random, and feed to such a stochastic optimization algorithm, guaranteed to approximate the leading eigenvector of 1n ∑n i=1 xix > i .\nTo the best of our knowledge, the existing iteration complexity guarantees for such algorithms (assuming the norm constraint r ≤ 1 for simplicity) scale at least4 as d/λ2. Since the runtime of each iteration isO(d), we get an overall runtime of O((d/λ)2).\nThe dependence on d in the iteration bound stems from the fact that with a random initial unit vector w0, we have 〈v1,w0〉2 ≈ 1d . Thus, we begin with a vector almost orthogonal to the leading eigenvector v1 (depending on d). In a purely stochastic setting, where only noisy information is available, this necessitates conservative updates at first, and in all the analyses we are aware of, the number of iterations appear to necessarily scale at least linearly with d.\nHowever, it turns out that in our setting, with a finite matrix X , we can perform a smarter initialization: Sample w from the standard Gaussian distribution on Rd, perform a single power iteration w.r.t. the covariance matrix A = 1nXX\n>, i.e. w0 = Aw/‖Aw‖, and initialize from w0. For such a procedure, we have the following simple observation:\nLemma 1. For w0 as above, it holds for any δ that with probability at least 1− 1d − δ,\n〈v1,w0〉2 ≥ δ2\n12 log(d) nrank(A) ,\nwhere nrank(A) = ‖A‖ 2 F\n‖A‖2sp is the numerical rank of A.\nThe numerical rank (see e.g. [18]) is a relaxation of the standard notion of rank: For any d × d matrix A, nrank(A) is at most the rank of A (which in turn is at most d). However, it will be small even if A is just close to being low-rank. In many if not most machine learning applications, we are interested in matrices which tend to be approximately low-rank, in which case nrank(A) is much smaller than d or even a constant. Therefore, by a single power iteration, we get an initial point w0 for which 〈v1,w0〉2 is on the order of 1/nrank(A), which can be much larger than the 1/d given by a random initialization, and is never substantially worse.\nProof of Lemma 1. Let s1 ≥ s2 ≥ . . . ≥ sd ≥ 0 be the d eigenvalues of A, with eigenvectors v1, . . . ,vd. We have\n〈v1,w0〉2 = 〈v1, Aw〉2\n‖Aw‖2 = (s1〈v1,w〉)2(∑d i=1 sivi〈vi,w〉 )2 = s21〈v1,w〉2∑d i=1 s 2 i 〈vi,w〉2 .\nSince w is distributed according to a standard Gaussian distribution, which is rotationally symmetric, we can assume without loss of generality that v1, . . . ,vd correspond to the standard basis vectors e1, . . . , ed, in which case the above reduces to\ns21w 2 1∑d\ni=1 s 2 iw 2 i\n≥ s 2 1∑d\ni=1 s 2 i w21 maxiw2i ,\n4For example, this holds for [6], although the bound only guarantees the existence of some iteration which produces the desired output. The guarantee of [4] scale as d2/λ2, and the guarantee of [10] scales as d/λ3 in our setting.\nwhere w1, . . . , wd are independent and scalar random variables with a standard Gaussian distribution. First, we note that s21 equals ‖A‖2sp, the spectral norm of A, whereas ∑d i=1 s 2 i equals ‖A‖2F , the Frobenius norm of A. Therefore, s 2 1∑ i s 2 i = ‖A‖2sp ‖A‖2F = 1nrank(A) , and we get overall that\n〈v1,w0〉2 ≥ 1 nrank(A) w21 maxiw2i . (4)\nWe consider the random quantity w21/maxiw 2 i , and independently bound the deviation probability of\nthe numerator and denominator. First, for any t ≥ 0 we have\nPr(w21 ≤ t) = Pr(w1 ∈ [− √ t, √ t]) = ∫ √t z=− √ t √ 1 2π exp ( −z 2 2 ) ≤ √ 1 2π ∗ 2 √ t = √ 2 π t . (5)\nSecond, by combining two standard Gaussian concentration results (namely, that ifW = max{|w1|, . . . , |wd|}, then 0 ≤ E[W ] ≤ 2 √ 2 log(d), and by the Cirelson-Ibragimov-Sudakov inequality, Pr(W − E[W ] > t) ≤ exp(−t2/2)), we get that\nPr(max i |wi| > 2\n√ 2 log(d) + t) ≤ exp(−t2/2),\nand therefore Pr(max\ni w2i > (2\n√ 2 log(d) + t)2) ≤ exp(−t/2). (6)\nCombining Eq. (5) and Eq. (6), with a union bound, we get that for any t1, t2 ≥ 0, it holds with probability at least 1− √ 2 π t1 − exp(−t 2 2/2) that\nw21 maxiw2i ≥ t1 (2 √ 2 log(d) + t2)2 .\nTo slightly simplify this for readability, we take t2 = √ 2 log(d), and substitute δ = √ 2 π t1. This implies that with probability at least 1− δ − 1/d,\nw21 maxiw2i ≥ π 2 δ 2 18 log(d) >\nδ2\n12 log(d) .\nPlugging back into Eq. (4), the result follows.\nThis result can be plugged into the existing analyses of purely stochastic PCA/SVD algorithms, and can often improve the dependence on the d factor in the iteration complexity bounds to a dependence on the numerical rank of A. We again emphasize that this is applicable in a situation where we can actually perform a power iteration, and not in a purely stochastic setting where we only have access to an i.i.d. data stream (nevertheless, it would be interesting to explore whether this idea can be utilized in such a streaming setting as well).\nTo give a concrete example of this, we provide a convergence analysis of the VR-PCA algorithm (Algorithm 1), starting from an arbitrary initial point, bounding the total number of stochastic iterations required by the algorithm in order to produce a point satisfying the conditions of Theorem 1 (from which point the analysis of Theorem 1 takes over). Combined with Theorem 1, this analysis also justifies that VR-PCA indeed converges starting from a random initialization.\nTheorem 2. Using the notation of Theorem 1 (where λ is the eigengap, v1 is the leading eigenvector, and r = maxi ‖xi‖2), and for any δ ∈ (0, 12), suppose we run Algorithm 1 with some initial unit-norm vector w̃0 such that\n〈v1, w̃0〉2 ≥ ζ > 0,\nand a step size η satisfying\nη ≤ cδ 2λζ3\nr2 log2(2/δ) (7)\n(for some universal constant c). Then with probability at least 1− δ, after\nT =\n⌊ c′ log(2/δ)\nηλζ ⌋ stochastic iterations (lines 6− 10 in the pseudocode, where c′ is again a universal constant), we get a point wT satisfying 1−〈v1,wT 〉2 ≤ 12 . Moreover, if η is chosen on the same order as the upper bound in Eq. (7), then\nT = Θ\n( r2 log3(2/δ)\nδ2λ2ζ4\n) .\nNote that the analysis does not depend on the choice of the epoch size m, and does not use the special structure of VR-PCA (in fact, the technique we use is applicable to any algorithm which takes stochastic gradient steps to solve this type of problem5). The proof of the theorem appears in Section 6.2.\nConsidering δ, r as a constants, we get that the runtime required by VR-PCA to find a point w such that 1 − 〈v1,wT 〉2 ≤ 12 is O(d/λ\n2ζ4) where ζ is a lower bound on 〈v1, w̃0〉2. As discussed earlier, if w̃0 is a result of random initialization followed by a power iteration (requiring O(nd) time), and the covariance matrix A has small numerical rank, then ζ = 〈v1, w̃0〉2 = Ω̃(1/ log(d)), and the runtime is\nO ( nd+ d\nλ2 log4(d)\n) = O ( d ( n+ ( log2(d)\nλ\n)2)) .\nBy Corollary 1, the runtime required by VR-PCA from that point to get an -accurate solution is O ( d ( n+ 1\nλ2\n) log ( 1 )) ,\nso the sum of the two expressions (which is d ( n+ 1\nλ2\n) up to log-factors), represents the total runtime\nrequired by the algorithm. Finally, we note that this bound holds under the reasonable assumption that the numeric rank of A is constant. If this assumption doesn’t hold, ζ can be as large as d, and the resulting bound will have a worse polynomial dependence on d. We suspect that this is due to a looseness in the dependence on ζ = 〈v1, w̃0〉2 in Theorem 2, since better dependencies can be obtained, at least for slightly different algorithmic approaches (e.g. [4, 10, 6]). We leave a sharpening of the bound w.r.t. ζ as an open problem.\n5Although there exist previous analyses of such algorithms in the literature, they unfortunately do not quite apply to our algorithm, for various technical reasons."
    }, {
      "heading" : "5 Convexity and Non-Convexity of the Rayleigh Quotient",
      "text" : "As mentioned in the introduction, an intriguing open question is whether the d ( n+ 1\nλ2\n) log ( 1 ) runtime\nguarantees from the previous sections can be further improved. Although a linear dependence on d, n seems unavoidable, this is not the case for the quadratic dependence on 1/λ. Indeed, when using deterministic methods such as power iterations or the Lanczos method, the dependence on λ in the runtime is only 1/λ or even √ 1/λ [15]. In the world of convex optimization from which our algorithmic techniques are derived, the analog of λ is the strong convexity parameter of the function, and again, it is possible to get a dependence of 1/λ, or even √ 1/λ with accelerated schemes (see e.g. [13, 16, 7] in the context of the variance-reduction technique we use). Is it possible to get such a dependence for our problem as well? Another question is whether the non-convex problem that we are tackling (Eq. (1)) is really that nonconvex. Clearly, it has a nice structure (since we can solve the problem in polynomial time), but perhaps it actually has hidden convexity properties, at least close enough to the optimal points? We note that Eq. (1) can be “trivially” convexified, by re-casting it as an equivalent semidefinite program [5]. However, that would require optimization over d × d matrices, leading to poor runtime and memory requirements. The question here is whether we have any convexity with respect to the original optimization problem over “thin” d× k matrices.\nIn fact, the two questions of improved runtime and convexity are closely related: If we can show that the optimization problem is convex in some domain containing an optimal point, then we may be able to use fast stochastic algorithms designed for convex optimization problems, inheriting their good guarantees.\nTo discuss these questions, we will focus on the k = 1 case for simplicity (i.e., our goal is to find a leading eigenvector of the matrix A = 1nXX > = 1n ∑n i=1 xix > i ), and study potential convexity properties of the negative Rayleigh quotient,\nFA(w) = − w>Aw\n‖w‖2 =\n1\nn n∑ i=1 ( −〈w,xi〉 2 ‖w‖2 ) .\nNote that for k = 1, this function coincides with Eq. (1) on the unit Euclidean sphere, and with the same optimal points, but has the nice property of being defined on the entire Euclidean space (thus, at least its domain is convex).\nAt a first glance, such functions FA appear to potentially be convex at some bounded distance from an optimum, as illustrated for instance in the case where A = (\n1 0 0 0\n) (see Figure 1). Unfortunately, it turns\nout that the figure is misleading, and in fact the function is not convex almost everywhere:\nTheorem 3. For the matrixA above, the Hessian of FA is not positive semidefinite for all but a measure-zero set.\nProof. The leading eigenvector of A is v1 = (1, 0), and FA(w) = − w21\nw21+w 2 2 . The Hessian of this function at some w equals\n2\n(w21 + w 2 2) 3\n( w22(3w 2 1 − w22) −2w1w2(w21 − w22)\n−2w1w2(w21 − w22) w21(w21 − 3w22)\n) .\nw21+w\n2 2 ,\ncorresponding to FA(w) where A = (1 0 ; 0 0). It is invariant to re-scaling of w, and attains a minimum at (a, 0) for any a 6= 0.\nThe determinant of this 2× 2 matrix equals\n4\n(w21 + w 2 2) 6\n( w21w 2 2(3w 2 1 − w22)(w21 − 3w22)− 4w21w22(w21 − w22)2 ) = 4w21w 2 2\n(w21 + w 2 2) 6\n( (3w21 − w22)(w21 − 3w22)− 4(w21 − w22)2 ) = 4w21w 2 2\n(w21 + w 2 2) 6\n( −(w21 + w22)2 ) = − 4w 2 1w 2 2\n(w21 + w 2 2)\n4 ,\nwhich is always non-positive, and strictly negative for w for which w1w2 6= 0 (which holds for all but a measure-zero set of Rd). Since the determinant of a positive semidefinite matrix is always non-negative, this implies that the Hessian isn’t positive semidefinite for any such w.\nThe theorem implies that we indeed cannot use convex optimization tools as-is on the function FA, even if we’re close to an optimum. However, the non-convexity was shown for FA as a function over the entire Euclidean space, so the result does not preclude the possibility of having convexity on a more constrained, lower-dimensional set. In fact, this is what we are going to do next: We will show that if we are given some point w0 close enough to an optimum, then we can explicitly construct a simple convex set, such that\n• The set includes an optimal point of FA.\n• The function FA is O(1)-smooth and λ-strongly convex in that set.\nThis means that we can potentially use a two-stage approach: First, we use some existing algorithm (such as VR-PCA) to find w0, and then switch to a convex optimization algorithm designed to handle functions with a finite sum structure (such as FA). Since the runtime of such algorithms scale better than VR-PCA, in terms of the dependence on λ, we can hope for an overall runtime improvement.\nUnfortunately, this has a catch: To make it work, we need to have w0 very close to the optimum – in fact, we require ‖v1−w0‖ ≤ O(λ), and we show (in Theorem 5) that such a dependence on the eigengap λ cannot be avoided (perhaps up to a small polynomial factor). The issue is that the runtime to get such a w0, using stochastic-based approaches we are aware of, would scale at least quadratically with 1/λ, but getting dependence better than quadratic was our problem to begin with. For example, the runtime guarantee using VR-PCA to get such a point w0 (even if we start from a good point as specified in Theorem 1) is on the order of\nd ( n+ 1\nλ2\n) log ( 1\nλ\n) ,\nwhereas the best known guarantees on getting an -optimal solution for λ-strongly convex and smooth functions (see [1]) is on the order of\nd ( n+ √ n\nλ\n) log ( 1 ) .\nTherefore, the total runtime we can hope for would be on the order of\nd (( n+ 1\nλ2\n) log ( 1\nλ\n) + ( n+ √ n\nλ\n) log ( 1 )) . (8)\nIn comparison, the runtime guarantee of using just VR-PCA to get an -accurate solution is on the order of\nd ( n+ 1\nλ2\n) log ( 1 ) . (9)\nUnfortunately, Eq. (9) is the same as Eq. (8) up to log-factors, and the difference is not significant unless the required accuracy is extremely small (exponentially small in n, 1/λ). Therefore, our construction is mostly of theoretical interest. However, it still shows that asymptotically, as → 0, it is indeed possible to have runtime scaling better than Eq. (9). This might hint that designing practical algorithms, with better runtime guarantees for our problem, may indeed be possible.\nTo explain our construction, we need to consider two convex sets: Given a unit vector w0, define the hyperplane tangent to w0,\nHw0 = {w : 〈w,w0〉 = 1}\nas well as a Euclidean ball of radius r centered at w0:\nBw0(r) = {w : ‖w −w0‖ ≤ r}\nThe convex set we use, given such a w0, is simply the intersection of the two, Hw0 ∩ Bw0(r), where r is a sufficiently small number (see Figure 2).\nThe following theorem shows that if w0 is O(λ)-close to an optimal point (a leading eigenvector v1 of A), and we choose the radius of Bw0(r) appropriately, then Hw0 ∩ Bw0(r) contains an optimal point, and the function FA is indeed λ-strongly convex and smooth on that set. For simplicity, we will assume that A is scaled to have spectral norm of 1, but the result can be easily generalized.\nTheorem 4. For any positive semidefinite A with spectral norm 1, eigengap λ and a leading eigenvector v1, and any unit vector w0 such that ‖w0 − v1‖ ≤ λ44 , the function FA(w) is 20-smooth and λ-strongly convex on the convex set Hw0 ∩Bw0 ( λ 22 ) , which contains a global optimum of FA.\nThe proof of the theorem appears in Subsection 6.3. Finally, we show below that a polynomial dependence on the eigengap λ is unavoidable, in the sense that the convexity property is lost if w0 is significantly further away from v1.\nTheorem 5. For any λ, ∈ ( 0, 12 ) , there exists a positive semidefinite matrix A with spectral norm 1,\neigengap λ, and leading eigenvector v1, as well as a unit vector w0 for which ‖v1 −w0‖ ≤ √\n2(1 + )λ), such that FA is not convex in any neighborhood of w0 on Hw0 .\nProof. Let\nA =  1 0 00 1− λ 0 0 0 0  , for which v1 = (1, 0, 0), and take\nw0 = ( √ 1− p2, 0, p),\nwhere p = √ (1 + )λ (which ensures ‖v1−w0‖2 = √ 2p2 = √ 2(1 + )λ). Consider the ray {( √\n1− p2, t, p) : t ≥ 0}, and note that it starts from w0 and lies in Hw0 . The function FA along that ray (considering it as a function of t) is of the form\n−(1− p 2) + (1− λ)t2 (1− p2) + t2 + p2 = − 1− p 2 + (1− λ)t2 1 + t2 .\nThe second derivative with respect to t equals\n−2(3t 2 − 1)(λ− p2) (t2 + 1)3 = 2 (3t2 − 1) λ (t2 + 1)3 ,\nwhere we plugged in the definition of p. This is a negative quantity for any t < 1√ 3 . Therefore, the function FA is strictly concave (and not convex) along the ray we have defined and close enough to w0, and therefore isn’t convex in any neighborhood of w0 on Hw0 ."
    }, {
      "heading" : "6 Proofs",
      "text" : ""
    }, {
      "heading" : "6.1 Proof of Theorem 1",
      "text" : "Although the proof structure generally mimics the proof of Theorem 1 in [19] for the k = 1 special case, it is more intricate and requires several new technical tools. To streamline the presentation of the proof, we begin with proving a series of auxiliary lemmas in Subsection 6.1.1, and then move to the main proof in Subsection 6.1. The main proof itself is divided into several steps, each constituting one or more lemmas.\nThroughout the proof, we use the well-known facts that for all matrices B,C,D of suitable dimensions, Tr(B + C) = Tr(B) + Tr(C), Tr(BC) = Tr(CB), Tr(BCD) = Tr(DBC), and Tr(B>B) = ‖B‖2F . Moreover, since Tr is a linear operation, E[Tr(B)] = E[Tr(B)] for a random matrix B."
    }, {
      "heading" : "6.1.1 Auxiliary Lemmas",
      "text" : "Lemma 2. For any B,C,D 0, it holds that Tr(BC) ≥ Tr(B(C −D)) and Tr(BC) ≥ Tr((B −D)C).\nProof. It is enough to prove that for any positive semidefinite matrices E,G, it holds that Tr(EG) ≥ 0. The lemma follows by taking either E = B,G = D (in which case, Tr(BC) = Tr(B(C −D)) + Tr(BD) ≥ Tr(B(C−D))), orE = D,G = C (in which case, Tr(BC) = Tr((B−D)C)+Tr(DC) ≥ Tr((B−D)C)).\nAny positive semidefinite matrixM can be written as the productM1/2M1/2 for some symmetric matrix M1/2 (known as the matrix square root of M ). Therefore,\nTr(EG) = Tr(E1/2E1/2G1/2G1/2) = Tr(G1/2E1/2E1/2G1/2)\n= Tr((E1/2G1/2)>(E1/2G1/2)) = ‖E1/2G1/2‖2F ≥ 0.\nLemma 3. If B 0 and C 0, then\nTr(BC−1) ≥ Tr(B(2I − C)),\nwhere I is the identity matrix.\nProof. We begin by proving the one-dimensional case, where B,C are scalars b ≥ 0, c > 0. The inequality then becomes bc−1 ≥ b(2 − c), which is equivalent to 1 ≥ c(2 − c), or upon rearranging, (c − 1)2 ≥ 0, which trivially holds.\nTurning to the general case, we note that by Lemma 2, it is enough to prove that C−1 − (2I − C) 0. To prove this, we make a couple of observations. The positive definite matrix C (like any positive definite matrix) has a singular value decomposition which can be written asUSU>, whereU is an orthogonal matrix, and S is a diagonal matrix with positive entries. Its inverse is US−1U>, and 2I − C = 2I − USU> = U(2I − S)U>. Therefore,\nC−1 − (2I − C) = US−1U> − U(2I − S)U> = U(S−1 − (2I − S))U>.\nTo show this matrix is positive semidefinite, it is enough to show that each diagonal entry of S−1− (2I−S) is non-negative. But this reduces to the one-dimensional result we already proved, when b = 1 and c > 0 is any diagonal entry in S. Therefore, C−1 − (2I − C) 0, from which the result follows.\nLemma 4. For any matrices B,C, Tr(BC) ≤ ‖B‖F ‖C‖F\nand ‖BC‖F ≤ ‖B‖sp‖C‖F .\nProof. The first inequality is immediate from Cauchy-Shwartz. As to the second inequality, letting ci denote the i-th column of C, and ‖ · ‖2 the Euclidean norm for vectors,\n‖BC‖F = √∑\ni\n‖Bci‖22 ≤ √∑\ni\n(‖B‖sp‖ci‖2)2 = ‖B‖sp √∑\ni\n‖ci‖22 = ‖B‖sp‖C‖F .\nLemma 5. Let B1, B2, Z1, Z2 be k × k square matrices, where B1, B2 are fixed and Z1, Z2 are stochastic and zero-mean (i.e. their expectation is the all-zeros matrix). Furthermore, suppose that for some fixed α, γ, δ > 0, it holds with probability 1 that\n• For all ν ∈ [0, 1], B2 + νZ2 δI .\n• max{‖Z1‖F , ‖Z2‖F } ≤ α.\n• ‖B1 + ηZ1‖sp ≤ γ.\nThen\nE [ Tr ( (B1 + Z1)(B2 + Z2) −1)] ≥ Tr(B1B−12 )− α2(1 + γ/δ)δ2 . Proof. Define the function\nf(ν) = Tr ( (B1 + νZ1)(B2 + νZ2) −1) , ν ∈ [0, 1]. Since B2 + νZ2 is positive definite, it is always invertible, hence f(ν) is indeed well-defined. Moreover, it can be differentiated with respect to ν, and we have\nf ′(ν) = Tr ( Z1(B2 + νZ2) −1 − (B1 + νZ1)(B2 + νZ2)−1Z2(B2 + νZ2)−1 ) .\nAgain differentiating with respect to ν, we have f ′′(ν) = Tr ( − 2Z1(B2 + νZ2)−1Z2(B2 + νZ2)−1\n+ 2(B1 + νZ1)(B2 + νZ2) −1Z2(B2 + νZ2) −1Z2(B2 + νZ2) −1 )\n= 2 Tr (( − Z1 + (B1 + νZ1)(B2 + νZ2)−1Z2 ) (B2 + νZ2) −1Z2(B2 + νZ2) −1 ) .\nUsing Lemma 4 and the triangle inequality, this is at most\n2‖ − Z1 + (B1 + νZ1)(B2 + νZ2)−1Z2‖F ‖(B2 + νZ2)−1Z2(B2 + νZ2)−1‖F ≤ 2 ( ‖Z1‖F + ‖(B1 + νZ1)(B2 + νZ2)−1Z2‖F ) ‖(B2 + νZ2)−1‖2sp‖Z2‖F\n≤ 2 ( ‖Z1‖F + ‖B1 + νZ1‖sp‖ (B2 + νZ2)−1 ‖sp‖Z2‖F ) ‖(B2 + νZ2)−1‖2sp‖Z2‖F\n≤ 2 ( α+ γ 1\nδ α\n) 1\nδ2 α =\n2α2(1 + γ/δ)\nδ2 .\nApplying a Taylor expansion to f(·) around ν = 0, with a Lagrangian remainder term, and substituting the values for f ′(ν), f ′′(ν), we can lower bound f(1) as follows:\nf(1) ≥ f(0) + f ′(0) ∗ (1− 0)− 1 2 max ν |f ′′(ν)| ∗ (1− 0)2\n= Tr ( B1B −1 2 ) + Tr ( Z1B −1 2 −B1B −1 2 Z2B −1 2 ) − α 2(1 + γ/δ)\nδ2 .\nTaking expectation over Z1, Z2, and recalling they are zero-mean, we get that\nE[f(1)] ≥ Tr ( B1B −1 2 ) − α 2(1 + γ/δ)\nδ2 . Since E[f(1)] = E [ Tr ( (B1 + Z1)(B2 + Z2) −1)], the result in the lemma follows.\nLemma 6. Let U1, . . . , Uk and R1, R2 be positive semidefinite matrices, such that R2−R1 0, and define the function\nf(x1 . . . xk) = Tr ( k∑ i=1 xiUi +R1 )( k∑ i=1 xiUi +R2 )−1 . over all (x1 . . . xk) ∈ [α, β]d for some β ≥ α ≥ 0. Then min(x1...xk)∈[α,β]d f(x) = f(α, . . . , α). Proof. Taking a partial derivative of f with respect to some xj , we have\n∂\n∂xj f(x)\n= Tr Uj ( k∑ i=1 xiUi +R2 )−1 − ( k∑ i=1 xiUi +R1 )( k∑ i=1 xiUi +R2 )−1 Uj ( k∑ i=1 xiUi +R2 )−1 = Tr\nI −( k∑ i=1 Ui +R1 )( k∑ i=1 xiUi +R2 )−1Uj ( k∑ i=1 xiUi +R2 )−1 = Tr\n(( k∑ i=1 xiUi +R2 ) − ( k∑ i=1 xiUi +R1 ))( k∑ i=1 xiUi +R2 )−1 Uj ( k∑ i=1 xiUi +R2 )−1 = Tr\n(R2 −R1)( k∑ i=1 xiUi +R2 )−1 Uj ( k∑ i=1 xiUi +R2 )−1 . By the lemma’s assumptions, each matrix in the product above is positive semidefinite, hence the product is positive semidefinite, and the trace is non-negative. Therefore, ∂∂xj f(x) ≥ 0, which implies that the function is minimized when each xj takes its smallest possible value, i.e. α.\nLemma 7. Let B be a k × k matrix with minimal singular value δ. Then\n1− ‖B>B‖2F ‖B‖2F\n≥ max { 1− ‖B‖2F , δ2\nk\n( k − ‖B‖2F )} .\nProof. We have\n1− ‖B>B‖2F ‖B‖2F ≥ 1− ‖B‖2F ‖B‖2F ‖B‖2F = 1− ‖B‖2F ,\nso it remains to prove 1 − ‖B >B‖2F ‖B‖2F ≥ δ2k ( k − ‖B‖2F ) . Let σ1, . . . , σk denote the vector of singular values of B. The singular values of B>B are σ21, . . . , σ 2 k, and the Frobenius norm of a matrix equals the Euclidean norm of its vector of singular values. Therefore, the lemma is equivalent to requiring\n1− ∑k i=1 σ 4 i∑k\ni=1 σ 2 i\n≥ δ 2\nk\n( k −\nk∑ i=1 σ2i\n) ,\nassuming σi ∈ [δ, 1] for all i. This holds since 1− ∑ i σ 4 i∑\ni σ 2 i\n=\n∑ i σ 2 i − ∑ i σ 4 i∑\ni σ 2 i\n=\n∑ i σ 2 i ( 1− σ2i )∑ i σ 2 i ≥ δ2 ∑ i ( 1− σ2i ) k = δ2 k ( k − ∑ i σ2i ) .\nLemma 8. For any d× k matrices C,D with orthonormal columns, let\nDC = arg min DB : (DB)>(DB)=I\n‖C −DB‖2F\nbe the nearest orthonormal-columns matrix toC in the column space ofD (whereB is a k×k matrix). Then the matrix B minimizing the above equals B = V U>, where C>D = USV > is the SVD decomposition of C>D, and it holds that\n‖C −DC‖2F ≤ 2(k − ‖C>D‖2F ).\nProof. Since D has orthonormal columns, we have D>D = I , so the definition of B is equivalent to\nB = arg min B : B>B=I\n‖C −DB‖2F .\nThis is the orthogonal Procrustes problem (see e.g. [8]), and the solution is easily shown to be B = V U> where USV > is the SVD decomposition of C>D. In this case, and using the fact that ‖C‖2F = ‖D‖2F = k (as C,D have orthonormal columns), we have that ‖C −DC‖2F equals\n‖C −DB‖2F = ‖C‖2F + ‖D‖2F − 2 Tr(C>DB) = 2 ( k − Tr(USV >(V U>)) ) = 2 ( k − Tr(USU>) ) .\nSince the trace function is similarity-invariant, this equals 2k − Tr(S). Let s1 . . . , sk be the diagonal elements of S, and note that they can be at most 1 (since they are the singular values of C>D, and both C and D have orthonormal columns). Recalling that the Frobenius norm equals the Euclidean norm of the singular values, we can therefore upper bound the above as follows:\n2 ( k − Tr(USU>) ) = 2 (k − Tr(S)) = 2 ( k −\nk∑ i=1 si\n) ≤ 2 ( k −\nk∑ i=1 s2i\n) = 2 ( k − ‖C>D‖2F ) .\nLemma 9. Let Wt,W ′t be as defined in Algorithm 2, where we assume η < 13 . Then for any d × k matrix Vk with orthonormal columns, it holds that∣∣∣‖V >k Wt‖2F − ‖V >k Wt−1‖2F ∣∣∣ ≤ 12kη1− 3η . Proof. Letting st, st−1 denote the vectors of singular values of V >k Wt and V > k Wt−1, and noting that they are both in [0, 1]k (as Vk,Wt−1,Wt all have orthonormal columns), the left hand side of the inequality in the lemma statement equals\n|‖st‖2 − ‖st−1‖2| = (‖st‖2 + ‖st−1‖2) | ‖st‖2 − ‖st−1‖2 | ≤ 2 √ k‖st − st−1‖2 ≤ 2k‖st − st−1‖∞,\nwhere ‖ · ‖∞ is the infinity norm. By Weyl’s matrix perturbation theorem6 [12], this is upper bounded by\n2k‖V >k Wt − V >k Wt−1‖sp ≤ 2k‖Vk‖sp‖Wt −Wt−1‖sp ≤ 2k‖Wt −Wt−1‖sp. (10) 6Using its version for singular values, which implies that the singular values of matrices B and B + E are different by at most\n‖E‖sp.\nRecalling the relationship between Wt and Wt−1 from Algorithm 2, we have that\nW ′t = Wt−1 + ηN,\nwhere\n‖N‖sp ≤ ‖xitx>itWt−1‖sp + ‖xitx > itW̃s−1Bt−1‖sp + ‖\n1\nn n∑ i=1 xix > i W̃s−1Bt−1‖sp ≤ 3,\nas Wt−1, W̃s−1, Bt−1 all have orthonormal columns, and xitx > it and 1n ∑n i=1 xix > i have spectral norm at most 1. Therefore, W ′t equals Wt−1, up to a matrix perturbation of spectral norm at most 3η. Again by Weyl’s theorem, this implies that the k non-zero singular values of the d × k matrix W ′t are different from those of Wt−1 (which has orthonormal columns) by at most 3η, and hence all lie in [1 − 3η, 1 + 3η]. As a\nresult, the singular values of ( W ′> t W ′ t )−1/2 all lie in [ 1 1+3η , 1 1−3η ] . Collecting these observations, we have\n‖Wt −Wt−1‖sp = ‖(Wt−1 + ηN) ( W ′> t−1W ′ t−1 )−1/2 −Wt−1‖sp\n≤ ‖Wt−1 (( W ′> t−1W ′ t−1 )−1/2 − I ) + ηN ( W ′> t−1W ′ t−1 )−1/2 ‖sp\n≤ ‖ ( W ′> t−1W ′ t−1 )−1/2 − I‖sp + η‖N‖sp‖ ( W ′> t−1W ′ t−1 )−1/2 ‖sp\n≤ 3η 1− 3η + 3η 1− 3η =\n6η\n1− 3η .\nPlugging back to Eq. (10), the result follows."
    }, {
      "heading" : "6.1.2 Main Proof",
      "text" : "To simplify the technical derivations, note that the algorithm remains the same if we divide each xi by √ r, and multiply η by r. Since maxi ‖xi‖2 ≤ r, this corresponds to running the algorithm with step-size ηr rather than η, on a re-scaled dataset of points with squared norm at most 1, and with an eigengap of λ/r instead of λ. Therefore, we can simply analyze the algorithm assuming that maxi ‖xi‖2 ≤ 1, and in the end plug in λ/r instead of λ, and ηr instead of η, to get a result which holds for data with squared norm at most r.\nPart I: Establishing a Stochastic Recurrence Relation\nWe begin by focusing on a single iteration t of the algorithm, and analyze how ‖V >k Wt‖2F (which measures the similarity between the column spaces of Vk and Wt) evolves during that iteration. The key result we need is Lemma 10 below, which is specialized for our algorithm in Lemma 11.\nLemma 10. Let A be a d × d symmetric matrix with all eigenvalues s1 ≥ s2 ≥ . . . ≥ sd in [0, 1], and suppose that sk − sk+1 ≥ λ for some λ > 0.\nLet N be a d × k zero-mean random matrix such that ‖N‖F ≤ σFN and ‖N‖sp ≤ σ sp N with probability\n1, and define\nrN = 46 (σ F N ) 2\n( 1 + 8\n3\n( 1\n4 σspN + 2\n)2)\nLet W be a d× k matrix with orthonormal columns, and define\nW ′ = (I + ηA)W + ηN , W ′′ = W ′(W ′>W ′)−1/2,\nfor some η ∈ [ 0, 1\n4max{1,σFN}\n] .\nIf Vk = [v1,v2 . . . ,vk] is the d× k matrix of A’s first k eigenvectors, then the following holds: • E [ 1− ‖V >k W ′′‖2F ] ≤ ( 1− 45ηλ‖V > k W‖2F ) ( 1− ‖V >k W‖2F ) + η2rN\n• If ‖V >k W‖2F ≥ k − 1 2 , then\nEN [ k − ‖V >k W ′′‖2F ] ≤ ( k − ‖V >k W‖2F )( 1− 1\n10 ηλ\n) + η2rN .\nProof. Using the fact that Tr(BCD) = Tr(CDB) for any matrices B,C,D, we have\nE [ ‖V >k W ′′‖2F ] = E [ Tr ( W ′′>VkV > k W ′′ )]\n= E [ Tr (( W ′>W ′ )−1/2 W ′>VkV > k W ′ ( W ′>W ′ )−1/2)] = E [ Tr (( W ′>VkV > k W ′ )( W ′>W ′ )−1)] . (11)\nBy definition of W ′, we have\nW ′>VkV > k W ′ = ((I + ηA)W + ηN)> VkV > k ((I + ηA)W + ηN)\n= B1 + Z1,\nwhere we define\nB1 = W >(I + ηA)VkV > k (I + ηA)W + η 2N>VkV > k N Z1 = ηN >VkV > k (I + ηA)W + ηW >(I + ηA)VkV > k N.\nAlso, we have\nW ′>W ′ = ((I + ηA)W + ηN)> ((I + ηA)W + ηN)\n= B2 + Z2,\nwhere\nB2 = W >(I + ηA)(I + ηA)W + η2N>N Z2 = ηN >(I + ηA)W + ηW>(I + ηA)N.\nWith these definitions, we can rewrite Eq. (11) as E [ Tr((B1 + Z1)(B2 + Z2) −1) ] . We now wish to remove Z1, Z2, by applying Lemma 5. To do so, we check the lemma’s conditions:\n• Z1, Z2 are zero mean: This holds since they are linear in N , and N is assumed to be zero-mean.\n• B2 + νZ2 38I for all ν ∈ [0, 1]: Recalling the definition of B2, Z2, and the facts that A 0, N>N 0 (by construction), and W>W = I , we have that B2 I . Moreover, the spectral norm of Z2 is at most\n2η‖N>(I + ηA)W‖sp ≤ 2η‖N‖sp‖I + ηA‖sp‖W‖sp ≤ 2ησspN (1 + η) ≤ 2ησ F N (1 + η),\nwhich by the assumption on η is at most 214 ( 1 + 14 ) = 58 . This implies that the smallest singular value of B2 + νZ2 is at least 1− ν(5/8) ≥ 3/8.\n• max{‖Z1‖F , ‖Z2‖F } ≤ 52ησ F N : By definition of Z1, Z2, and using Lemma 4, the Frobenius norm of\nthese two matrices is at most\n2η‖N‖F ‖(I + ηA)‖sp‖W‖sp ≤ 2ησFN (1 + η),\nwhich by the assumption on η is at most 2ησFN ( 1 + 14 ) = 52ησ F N .\n• ‖B1 + ηZ1‖sp ≤ ( 1 4σ sp N + 2 )2: Using the definition of B1, Z1 and the assumption η ≤ 14 , ‖B1 + ηZ1‖sp ≤ ‖B1‖sp + η‖Z1‖sp\n≤ (1 + η)2 + η2(σspN ) 2 + 2ησspN (1 + η) ≤ ( 5\n4\n)2 + 1\n16 (σspN )\n2 + 5\n8 σspN\n<\n( 1\n4 σspN + 2\n)2 .\nApplying Lemma 5 and plugging back to Eq. (11), we get E [ ‖V >k W ′′‖2F ] ≥ E [ Tr((B1 + Z1)(B2 + Z2) −1) ]\n≥ Tr ( B1B −1 2 ) − 400\n9 (ησFN ) 2\n( 1 + 8\n3\n( 1\n4 σspN + 2\n)2) . (12)\nWe now turn to lower bound Tr ( B1B −1 2 ) , by first re-writing B1, B2 in a different form. For i = 1, . . . , d, let Ui = W >viv > i W,\nwhere vi is the eigenvector of A corresponding to the eigenvalue si. Note that each Ui is positive semidefinite, and ∑d i=1 Ui = W >W = I . We have\nB1 = W >(I + ηA)VkV > k (I + ηA)W + η 2N>VkV > k N\n= W> ((I + ηA)Vk) ((I + ηA)Vk) >W + η2N>VkV > k N\n= k∑ i=1 (1 + ηsi) 2W>viv > i W + η 2N>VkV > k N\n= k∑ i=1 (1 + ηsi) 2Ui + η 2N>VkV > k N. (13)\nSimilarly,\nB2 = W >(I + ηA)(I + ηA)W + η2N>N\n= d∑ i=1 (1 + ηsi) 2W>viv > i W + η 2N>N\n= d∑ i=1 (1 + ηsi) 2Ui + η 2N>N. (14)\nPlugging Eq. (13) and Eq. (14) back into Eq. (12), we get\nE [ ‖V >k W ′′‖2F ] ≥ Tr ( k∑ i=1 (1 + ηs1) 2Ui + η 2N>VkV > k N )( d∑ i=1 (1 + ηsi) 2Ui + η 2N>N )−1 − 400\n9 (ησFN ) 2\n( 1 + 8\n3\n( 1\n4 σspN + 2\n)2) . (15)\nRecalling that s1 ≥ s2 ≥ . . . ≥ sk and letting α = (1 + ηsk)2, β = (1 + ηs1)2, the trace term can be lower bounded by\nmin x1,...,xk∈[α,β] Tr ( k∑ i=1 xiUi + η 2N>VkV > k N )( k∑ i=1 xiUi + d∑ i=k+1 (1 + ηsi) 2Ui + η 2N>N )−1 . Applying Lemma 6 (noting that as required by the lemma, ∑d i=k+1(1+ηsi)\n2Ui+η 2N>N−η2N>VkV >k N =∑d\ni=k+1(1 + ηsi) 2Ui + η 2N> ( I − VkV >k ) N 0), we can lower bound the above by\nTr ((1 + ηsk)2 k∑ i=1 Ui + η 2N>VkV > k N )( (1 + ηsk) 2 k∑ i=1 Ui + d∑ i=k+1 (1 + ηsi) 2Ui + η 2N>N )−1 . Using Lemma 2, this can be lower bounded by\nTr ((1 + ηsk)2 k∑ i=1 Ui )( (1 + ηsk) 2 k∑ i=1 Ui + d∑ i=k+1 (1 + ηsi) 2Ui + η 2N>N )−1 = Tr\n( k∑ i=1 Ui )( k∑ i=1 Ui + d∑ i=k+1 ( 1 + ηsi 1 + ηsk )2 Ui + ( η 1 + ηsk )2 N>N )−1\nApplying Lemma 3, this is at least\nTr (( k∑ i=1 Ui )( 2I − k∑ i=1 Ui − d∑ i=k+1 ( 1 + ηsi 1 + ηsk )2 Ui − ( η 1 + ηsk )2 N>N )) .\nRecalling that I = ∑d i=1 Ui = ∑k i=1 Ui + ∑d i=k+1 Ui, this can be simplified to\nTr (( k∑ i=1 Ui )( k∑ i=1 Ui + d∑ i=k+1 ( 2− ( 1 + ηsi 1 + ηsk )2) Ui − ( η 1 + ηsk )2 N>N )) . (16)\nSince Ui 0, then using Lemma 3, we can lower bound the expression above by shrinking each of the( 2− ( 1+ηsi 1+ηsk )2) terms. In particular, since si ≤ sk − λ for each i ≥ k + 1,\n2− (\n1 + ηsi 1 + ηsk\n)2 ≥ 2− 1 + ηsi\n1 + ηsk ≥ 2− 1 + η(sk − λ) 1 + ηsk = 1 + ηλ 1 + ηsk ,\nwhich by the assumption that η ≤ 1/4 and sk ≤ s1 ≤ 1, is at least 1+ 45ηλ. Plugging this back into Eq. (16), and recalling that ∑d i=1 Ui = I , we get the lower bound\nTr (( k∑ i=1 Ui )( k∑ i=1 Ui + d∑ i=k+1 ( 1 + 4 5 ηλ ) Ui − ( η 1 + ηsk )2 N>N ))\n= Tr (( k∑ i=1 Ui )( I + 4 5 ηλ ( I − k∑ i=1 Ui ) − (\nη\n1 + ηsk\n)2 N>N )) .\nAgain using Lemma 2, this is at least\nTr (( k∑ i=1 Ui )( I + 4 5 ηλ ( I − k∑ i=1 Ui ))) − (\nη\n1 + ηsk\n)2 Tr (( k∑ i=1 Ui ) N>N )\n≥ Tr (( k∑ i=1 Ui )( I + 4 5 ηλ ( I − k∑ i=1 Ui ))) − (\nη\n1 + ηsk\n)2 Tr ( N>N ) ≥ Tr\n(( k∑ i=1 Ui )( I + 4 5 ηλ ( I − k∑ i=1 Ui ))) − η2 ( σFN )2 .\nRecall that this is a lower bound on the trace term in Eq. (15). Plugging it back and slightly simplifying, we get\nE [ ‖V >k W ′′‖2F ] ≥ Tr (( k∑ i=1 Ui )( I + 4 5 ηλ ( I − k∑ i=1 Ui ))) − η2rN ,\nwhere\nrN = 46 (σ F N ) 2\n( 1 + 8\n3\n( 1\n4 σspN + 2\n)2) .\nThe trace term above can be re-written (using the definition of Ui and the fact that Tr(B>B) = ‖B‖2F ) as\nTr (( W>\nk∑ i=1 viv > i W\n)( I + 4\n5 ηλ\n( I −W>\nk∑ i=1 viv > i W\n)))\n= ( 1 + 4\n5 ηλ\n) Tr ( W>VkV > k W ) − 4\n5 ηλTr\n(( W>VkV > k W )( W>VkV > k W )) = ( 1 + 4\n5 ηλ\n) ‖V >k W‖2F − 4\n5 ηλ‖W>VkV >k W‖2F\n= ‖V >k W‖2F ( 1 + 4\n5 ηλ ( 1− ‖W>VkV >k W‖2F ‖V >k W‖2F )) .\nApplying Lemma 7, and letting δ denote the minimal singular value of V >k W , this is lower bounded by\n‖V >k W‖2F ( 1 + 4\n5 ηλmax\n{ 1− ‖V >k W‖2F , δ2\nk\n( k − ‖V >k W‖2F )}) .\nOverall, we get that\nE [ ‖V >k W ′′‖2F ] ≥ ‖V >k W‖2F ( 1 + 4\n5 ηλmax\n{ 1− ‖V >k W‖2F , δ2\nk\n( k − ‖V >k W‖2F )}) − η2rN . (17)\nWe now consider two options:\n• Taking the first argument of the max term in Eq. (17), we get\nE [ ‖V >k W ′′‖2F ] ≥ ‖V >k W‖2F ( 1 + 4 5 ηλ ( 1− ‖V >k W‖2F )) − η2rN .\nSubtracting 1 from both sides and simplifying, we get\nE [ 1− ‖V >k W ′′‖2F ] ≤ (\n1− 4 5 ηλ‖V >k W‖2F\n)( 1− ‖V >k W‖2F ) + η2rN .\n• Suppose that ‖V >k W‖2F ≥ k − 1 2 . Taking the second argument of the max term in Eq. (17), we get\nE [ ‖V >k W ′′‖2F ] ≥ ‖V >k W‖2F ( 1 + 4ηλδ2\n5k\n( k − ‖V >k W‖2F )) − η2rN .\nSubtracting both sides from k, , we get\nE [ k − ‖V >k W ′′‖2F ] ≤ ( k − ‖V >k W‖2F ) − 4ηλδ 2\n5k ‖V >k W‖2F\n( k − ‖V >k W‖2F ) + η2rN\n= ( k − ‖V >k W‖2F )( 1− 4ηλδ 2\n5k ‖V >k W‖2F\n) + η2rN\n≤ ( k − ‖V >k W‖2F )( 1− 4ηλδ 2\n5k\n( k − 1\n2\n)) + η2rN\nSince k ≥ 1, we can lower bound the ( k − 12 ) term by k2 . Moreover, the condition k−‖V > k W‖2F ≤ 1 2\nimplies that the singular values σ1, . . . , σk of V >k W satisfy k− ∑k i=1 σ 2 i ≤ 12 . But each σi is in [0, 1] (as Vk,W have orthonormal columns), so no σi can be less than 12 . This implies that δ ≥ 1 2 . Plugging the lower bounds k − 12 ≥ k 2 and δ ≥ 1 2 into the above, we get\nE [ k − ‖V >k W ′′‖2F ] ≤ ( k − ‖V >k W‖2F )( 1− 1\n10 ηλ\n) + η2rN .\nLemma 11. Let A,Wt be as defined in Algorithm 2, and suppose that η ∈ [ 0, 1\n23 √ k\n] . Then the following\nholds for some positive numerical constants c1, c2, c3: • E [ 1− ‖V >k W ′′‖2F ] ≤ ( 1− c1ηλ‖V >k W‖2F ) ( 1− ‖V >k W‖2F ) + c2kη 2\n• If ‖V >k Wt‖2F ≥ k − 1 2 , then E [ k − ‖V >k Wt+1‖2F ] ≤ ( k − ‖V >k Wt‖2F ) (1− c1η (λ− c2η)) + c3η2(k − ‖V >k W̃s−1‖2F ).\nIn the above, the expectation is over the random draw of the index it, conditioned on Wt and W̃s−1.\nProof. To apply Lemma 10, we need to compute upper bounds σFN and σ sp N on the Frobenius and spectral norms of N , which in our case equals (xitx > it − A)(Wt − W̃s−1Bt). Since ‖A‖sp, ‖xitx>it‖sp ≤ 1, and Wt, W̃s−1, Bt have orthonormal columns, the spectral norm of N is at most\n‖(xitx>it −A)(Wt − W̃s−1Bt)‖sp ≤ ( ‖xitx>it‖sp + ‖A‖sp )( ‖Wt‖sp + ‖W̃s−1‖sp‖Bt‖sp ) ≤ 4,\nso we may take σspN = 4. As to the Frobenius norm, using Lemma 4 and a similar calculation, we have\n‖N‖2F ≤ 4‖Wt − W̃s−1Bt‖2F .\nTo upper bound this, define\nVWt = arg min VkB:(VkB)>(VkB)=I\n‖Wt − VkB‖2F\nto be the nearest orthonormal-columns matrix to Wt in the column space of Vk, and\nW̃V = arg min W̃s−1B:(W̃s−1B)>(W̃s−1B)=I\n‖VWt − W̃s−1B‖2F\nto be the nearest orthonormal-columns matrix to VWt in the column space of W̃s−1. Also, recall that by definition,\nW̃s−1Bt = arg min W̃s−1B:(W̃s−1B)>(W̃s−1B)=I\n‖Wt − W̃s−1B‖2F\nis the nearest orthonormal-columns matrix to Wt in the column space of W̃s−1. Therefore, we must have ‖Wt − W̃s−1Bt‖2F ≤ ‖Wt − W̃V ‖2F . Using this and Lemma 8, we have\n‖Wt − W̃s−1Bt‖2F ≤ ‖Wt − W̃V ‖2F = ‖(Wt − VWt)− (W̃V − VWt)‖2F ≤ 2‖Wt − VWt‖2F + 2‖W̃V − VWt‖2F = 4 ( k − ‖V >k Wt‖2F ) + 4 ( k − ‖V >WtW̃s−1‖ 2 F ) .\nBy definition of VWt , we have VWt = VkB where B >B = B>V >k VkB = (VkB) >(VkB) = I . ThereforeB is an orthogonal k× k matrix, and ‖V >WtW̃s−1‖ 2 F = ‖B>V >k W̃s−1‖2F = ‖V >k W̃s−1‖2F , so the above equals 4(k − ‖V >k Wt‖2F ) + 4(k − ‖V >k W̃s−1‖2F ). Overall, we get that the squared Frobenius norm of N can be upper bounded by\n(σFN ) 2 = 16 ( (k − ‖V >k Wt‖2F ) + (k − ‖V >k W̃s−1‖2F ) ) .\nPlugging σspN and (σ F N ) 2 into the rN as defined in Lemma 10, and picking any η ∈ [0, 123√k ] (which satisfies the condition in Lemma 10 that η ∈ [ 0, 1\n4max{1,σFN}\n] , since 4 max{1, σFn } ≤ 4 max{1, √ 16 ∗ 2k} <\n23 √ k), we get\nrN = 736 ( (k − ‖V >k Wt‖2F ) + (k − ‖V >k W̃s−1‖2F ) )( 1 + 8\n3\n( 1\n4 4 + 2 )2) ≤ 18400 ( (k − ‖V >k Wt‖2F ) + (k − ‖V >k W̃s−1‖2F ) ) .\nThis implies that rN ≤ 36800k always, which by application of Lemma 10, gives the first part of our lemma. As to the second part, assuming ‖V >k Wt‖2F ≥ k − 1 2 and applying Lemma 10, we get that\nE [ k − ‖V >k Wt+1‖2F ] ≤ ( k − ‖V >k Wt‖2F )( 1− 1\n10 ηλ ) + 18400 η2 ( (k − ‖V >k Wt‖2F ) + (k − ‖V >k W̃s−1‖2F )\n) = ( k − ‖V >k Wt‖2F )( 1− η ( 1\n10 λ− 18400η )) + 18400 η2(k − ‖V >k W̃s−1‖2F ).\nThis corresponds to the lemma statement.\nPart II: Solving the Recurrence Relation for a Single Epoch\nSince we focus on a single epoch, we drop the subscript from W̃s−1 and denote it simply as W̃ . Suppose that η = αλ, where α is a sufficiently small constant to be chosen later. Also, let\nbt = k − ‖V >k Wt‖2F and b̃ = k − ‖V >k W̃‖2F .\nThen Lemma 11 tells us that if α is a sufficiently small constant, bt ≤ 12 , then\nE [bt+1|Wt] ≤ ( 1− cαλ2 ) bt + c ′α2λ2b̃ (18)\nfor some numerical constants c, c′.\nLemma 12. Let B be the event that bt ≤ 12 for all t = 0, 1, 2, . . . ,m. Then for certain positive numerical constants c1, c2, c3, if α ≤ c1, then\nE[bm|B] ≤ (( 1− c2αλ2 )m + c3α ) b̃,\nwhere the expectation is over the randomness in the current epoch.\nProof. Recall that bt is a deterministic function of the random variable Wt, which depends in turn on Wt−1 and the random instance chosen at round t. We assume that W0 (and hence b̃) are fixed, and consider how bt evolves as a function of t. Using Eq. (18), we have\nE[bt+1|Wt, B] = E [ bt+1|Wt, bt+1 ≤ 1\n2\n] ≤ E[bt+1|Wt] ≤ ( 1− cαλ2 ) bt + c ′α2λ2b̃.\nNote that the first equality holds, since conditioned on Wt, bt+1 is independent of b1, . . . , bt, so the event B is equivalent to just requiring bt+1 ≤ 1/2.\nTaking expectation over Wt (conditioned on B), we get that E[bt+1|B] ≤ E [( 1− cαλ2 ) bt + c ′α2λ2b̃ ∣∣∣B]\n= ( 1− cαλ2 ) E [bt|B] + c′α2λ2b̃.\nUnwinding the recursion, and using that b0 = b̃, we therefore get that\nE[bm|B] ≤ ( 1− cαλ2 )m b̃+ c′α2λ2b̃ m−1∑ i=0 ( 1− cαλ2 )i ≤ ( 1− cαλ2 )m b̃+ c′α2λ2b̃\n∞∑ i=0 ( 1− cαλ2 )i = ( 1− cαλ2 )m b̃+ c′α2λ2b̃ 1\ncαλ2\n= (( 1− cαλ2 )m + c′\nc α\n) b̃.\nas required.\nWe now turn to prove that the event B assumed in Lemma 12 indeed holds with high probability:\nLemma 13. The following holds for certain positive numerical constants c1, c2, c3: If α ≤ c1, then for any β ∈ (0, 1) and m, if\nb̃+ c2kmα 2λ2 + c3k √ mα2λ2 log(1/β) ≤ 1\n2 , (19)\nthen it holds with probability at least 1− β that\nbt ≤ b̃+ c2kmα2λ2 + c3k √ mα2λ2 log(1/β) ≤ 1\n2\nfor all t = 0, 1, 2, . . . ,m.\nProof. To prove the lemma, we analyze the stochastic process b0(= b̃), b1, b2, . . . , bm, and use a concentration of measure argument. First, we collect the following facts:\n• b̃ = b0 ≤ 12 : This directly follows from the assumption stated in the lemma.\n• As long as bt ≤ 12 , E [bt+1|Wt] ≤ bt + c2α 2λ2b̃ for some constant c2: Supposing α is sufficiently\nsmall, then by Eq. (18), E [bt+1|Wt] ≤ ( 1− cαλ2 ) bt + c ′α2λ2b̃ ≤ bt + c′α2λ2b̃.\n• |bt+1 − bt| is bounded by c′3kαλ for some constant c′3: Applying Lemma 9, and assuming that α is at most some sufficiently small constant c1 (e.g. α ≤ 112 , so η = αλ ≤ 1 12 ),\n|bt+1 − bt| = ∣∣∣‖V >k Wt+1‖2F − ‖V >k Wt‖2F ∣∣∣ ≤ 12kη1− 3η ≤ 12kαλ3/4 = 16kαλ.\nArmed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows that with probability at least 1−β, it holds simultaneously for all t = 1, . . . ,m (and for t = 0 by assumption) that\nbt ≤ b̃+ c2mα2λ2b̃+ c3k √ mα2λ2 log(1/β)\nfor some constants c2, c3, as long as the expression above is less than 12 . If the expression is indeed less than 1 2 , then we get that bt ≤ 1 2 for all t. Upper bounding b̃ by k and slightly simplifying, we get the statement in the lemma.\nCombining Lemma 12 and Lemma 13, and using Markov’s inequality, we get the following corollary:\nLemma 14. Let confidence parameters β, γ ∈ (0, 1) be fixed. Suppose that m,α are chosen such that α ≤ c1 and\nb̃+ c2kmα 2λ2 + c3k √ mα2λ2 log(1/β) ≤ 1\n2 ,\nwhere c1, c2, c3 are certain positive numerical constants. Then with probability at least 1− (β+γ), it holds that\nbm ≤ 1\nγ\n(( 1− cαλ2 )m + c′α ) b̃.\nfor some positive numerical constants c, c′.\nPart III: Analyzing the Entire Algorithm’s Run\nGiven the analysis in Lemma 14 for a single epoch, we are now ready to prove our theorem. Let\nb̃s = k − ‖V >k W̃s‖2F .\nBy assumption, at the beginning of the first epoch, we have b̃0 = k − ‖V >k W̃0‖2F ≤ 1 2 . Therefore, by Lemma 14, for any β, γ ∈ ( 0, 12 ) , if we pick any\nα ≤ min { c1, 1 2c′ γ2 } and m ≥ 3 log(1/γ) cαλ2 such that 1 2 +c2kmα 2λ2+c3k √ mα2λ2 log(1/β) ≤ 1 2 , (20) then we get with probability at least 1− (β + γ) that\nbm ≤ 1\nγ\n(( 1− cαλ2 ) 3 log(1/γ) cαλ2 + 1 2 γ2 ) b̃0\nUsing the inequality (1 − (1/x))ax ≤ exp(−a), which holds for any x > 1 and any a, and taking x = 1/(cαλ2) and a = 3 log(1/γ), we can upper bound the above by\n1\nγ\n( exp ( −3 log ( 1\nγ\n)) + 1 2 γ2 ) b̃0\n= 1\nγ\n( γ3 + 1 2 γ2 ) b̃0 ≤ γb̃0.\nSince bm equals the starting point b̃1 for the next epoch, we get that b̃1 ≤ γb̃0 ≤ γ 12 . Again applying Lemma 14, and performing the same calculation we have that with probability at least 1− (β + γ) over the next epoch, b̃2 ≤ γb̃1 ≤ γ2b̃0. Repeatedly applying Lemma 14 and using a union bound, we get that after T epochs, with probability at least 1− T (β + γ),\nk − ‖V >k W̃T ‖2F = b̃T ≤ γT b̃0 < γT .\nTherefore, for any desired accuracy parameter , we simply need to use T = ⌈ log(1/ ) log(1/γ) ⌉ epochs, and get\nk − ‖V >k W̃s‖2F ≤ with probability at least 1− T (β + γ) = 1− ⌈ log(1/ ) log(1/γ) ⌉ (β + γ).\nUsing a confidence parameter δ, we pick β = γ = δ2 , which ensures that the accuracy bound above holds with probability at least\n1− ⌈ log(1/ )\nlog(2/δ)\n⌉ δ ≥ 1− ⌈ log(1/ )\nlog(2)\n⌉ δ = 1− ⌈ log2 ( 1 )⌉ δ.\nSubstituting this choice of β, γ into Eq. (20), and recalling that the step size η equals αλ, we get that k − ‖V >k W̃T ‖2F ≤ with probability at least 1− dlog2(1/ )eδ, provided that\nη ≤ cδ2λ , m ≥ c ′ log(2/δ)\nηλ , kmη2 + k\n√ mη2 log(2/δ) ≤ c′′\nfor suitable positive constants c, c′, c′′. To get the theorem statement, recall that the analysis we performed pertains to data whose squared norm is bounded by 1. By the reduction discussed at the beginning of the proof, we can apply it to data with squared norm at most r, by replacing λ with λ/r, and η with ηr, leading to the condition\nη ≤ cδ 2 r2 λ , m ≥ c ′ log(2/δ) ηλ , kmη2r2 + rk\n√ mη2 log(2/δ) ≤ c′′\nand establishing the theorem."
    }, {
      "heading" : "6.2 Proof of Theorem 2",
      "text" : "The proof relies mainly on the techniques and lemmas of Section 6.1, used to prove Theorem 1. As done in Section 6.1, we will assume without loss of generality that r = maxi ‖xi‖2 is at most 1, and then transform the bound to a bound for general r (see the discussion at the beginning of Subsection 6.1.2)\nFirst, we extract the following result, which is essentially the first part of Lemma 11 (for k = 1): Lemma 15. Let A,wt be as defined in Algorithm 1, and suppose that η ∈ [ 0, 123 ] . Then\nEit [ 1− 〈v1,wt+1〉2 ∣∣wt, w̃s−1] ≤ (1− cηλ〈v1,wt〉2) (1− 〈v1,wt〉2)+ c′η2, for some positive numerical constants c, c′.\nNote that this bound holds regardless of what is w̃s−1, and in particular holds across different epochs of Algorithm 1. Therefore, it is enough to show that starting from some initial point w0, after sufficiently many stochastic updates as specified in line 6-10 of the algorithm (or in terms of the analysis, sufficiently many applications of Lemma 15), we end up with a point wT for which 1 − 〈v1,wT 〉 ≤ 12 , as required.\nNote that to simplify the notation, we will use here a single running index w0,w1,w2, . . . ,wT (whereas in the algorithm we restarted the indexing after every epoch).\nThe proof is based on martingale arguments, quite similar to the ones in Subsection 6.1.2 but with slight changes. First, we let\nbt = 1− 〈v1,wt〉2\nto simplify notation. We note that b0 = 1 − 〈v1,w0〉2 is assumed fixed, whereas b1, b2, . . . are random variables based on the sampling process. Lemma 11 tells us that if η is sufficiently small, and bt ≤ 1− ξ for some ξ ∈ (0, 1), then\nE [bt+1|bt] ≤ (1− cηλξ) bt + c′η2. (21)\nfor some numerical constants c, c′.\nLemma 16. Let B be the event that bt ≤ 1− ξ for all t = 0, 1, . . . , T . Then for certain positive numerical constants c1, c2, c3, if η ≤ c1λ, then\nE[bT |B] ≤ ( (1− c2ηλξ)T + c3 η\nλξ\n) .\nProof. Using Eq. (21), we have for any bt satisfying event B that\nE[bt+1|bt, B] = E [bt+1|bt, bt+1 ≤ 1− ξ] ≤ E[bt+1|bt] ≤ (1− cηλξ) bt + c′η2.\nTaking expectation over bt (conditioned on B), we get that E[bt+1|B] ≤ E [ (1− cηλξ) bt + c′η2 ∣∣B] = (1− cηλξ)E [bt|B] + c′η2.\nUnwinding the recursion, we get\nE[bT |B] ≤ (1− cηλξ)T b0 + c′η2 T−1∑ i=0 (1− cηλξ)i\n≤ (1− cηλξ)T + c′η2 ∞∑ i=0 (1− cηλξ)i\n= (1− cηλξ)T + c′η2 1 cηλξ\n≤ (1− cηλξ)T + c ′\nc\nη\nλξ .\nWe now turn to prove that the event B assumed in Lemma 12 indeed holds with high probability:\nLemma 17. The following holds for certain positive numerical constants c1, c2, c3: If η ≤ c1λ, then for any β ∈ (0, 1), if\nb0 + c2Tη 2 + c3 √ Tη2 log(1/β) ≤ 1− ξ, (22)\nthen it holds with probability at least 1− β that bt ≤ b0 + c2Tη2 + c3 √ Tη2 log(1/β) ≤ 1− ξ\nfor all t = 0, 1, . . . , T .\nProof. To prove the lemma, we analyze the stochastic process b1, b2, . . . , bT , and use a concentration of measure argument. First, we collect the following facts:\n• b0 ≤ 1− ξ: This directly follows from the assumption stated in the lemma.\n• E [bt+1|bt] ≤ bt + c′η2 for some constant c′: By Eq. (21),\nE [bt+1|Wt] ≤ (1− cηλξ) bt + c′η2 ≤ bt + c′η2.\n• |bt+1−bt| is bounded by cη for some constant c: Applying Lemma 9 for the case k = 1, and assuming η ≤ 1/12,\n|bt+1 − bt| = ∣∣〈v1,wt+1〉2 − 〈v,wt〉2∣∣ ≤ 12η\n1− 3η ≤ 12η 3/4 = 16η.\nArmed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows that with probability at least 1− β, it holds simultaneously for all t = 0, 1, . . . , T that\nbt ≤ b0 + c2Tη2 + c3 √ Tη2 log(1/β)\nfor some constants c2, c3. If the expression is indeed less than 1 − ξ, then we get that bt ≤ 1 − ξ for all t, from which the lemma follows.\nCombining Lemma 16 and Lemma 17, and using Markov’s inequality, we get the following corollary:\nLemma 18. Let confidence parameters β, γ ∈ (0, 1) be fixed. Then for some positive numerical constants c1, c2, c3, c, c ′, if η ≤ c1λ and\nb0 + c2Tη 2 + c3 √ Tη2 log(1/β) ≤ 1− ξ,\nthen with probability at least 1− (β + γ), it holds that\nbT ≤ 1\nγ\n( (1− cηλξ)T + c′ η\nλξ\n) .\nWe are now ready to prove our theorem. By Lemma 18, for any β, γ ∈ ( 0, 12 ) and any\nη ≤ min { c1, 1 2c′ γ2 } λξ and T ≥ 3 log(1/γ) cηλξ\nsuch that b0 + c2Tη2 + c3 √ Tη2 log(1/β) ≤ 1− ξ, (23)\nwe get with probability at least 1− (β + γ) that\nbT ≤ 1\nγ\n( (1− cηλξ) 3 log(1/γ) cηλξ + 1 2 γ2 ) .\nUsing the inequality (1 − (1/x))ax ≤ exp(−a), which holds for any x > 1 and any a, and taking x = 1/(cηλξ) and a = 3 log(1/γ), we can upper bound the above by\n1\nγ\n( exp ( −3 log ( 1\nγ\n)) + 1 2 γ2 ) = 1 γ ( γ3 + 1 2 γ2 ) ,\nand since we assume γ < 12 , this is at most 1 2 . Overall, we got that with probability at least 1 − β − γ, bT ≤ 12 , and therefore 1− 〈v1,wT 〉 2 ≤ 12 as required.\nIt remains to show that the parameter choices in Eq. (23) can indeed be satisfied. First, we fix ξ = 12ζ (where we recall that 0 < ζ ≤ 〈v1,w0〉2), which trivially ensures that b0 = 1−〈v1,w0〉2 is at most 1− 2ξ. Moreover, suppose we pick β = γ in (0, exp(−1)), and η, T so that\nη ≤ c∗γ 2λξ3\nlog2(1/γ) , T =\n⌊ 3 log(1/γ)\nc′∗ηλξ\n⌋ , (24)\nwhere c∗, c′∗ are sufficiently small constants so that the bounds on η, T in Eq. (23) are satisfied. This implies that the third bound in Eq. (23) is also satisfied, since by plugging in the values / bounds of T and η, and using the assumptions γ = β ≤ exp(−1) and ξ ≤ 1, we have\nb0 + c2Tη 2 + c3 √ Tη2 log(1/γ)\n≤ 1− 2ξ + c2 3 log(1/γ)\nc′∗λξ η + c3\n√ 3 log(1/γ)\nc′∗λξ η log(1/γ)\n≤ 1− 2ξ + c2 3c∗γ\n2ξ2\nc′∗ log(1/γ) + c3\n√ 3c∗γ2ξ2\nc′∗\n≤ 1− 2ξ + ( 3c2c∗ c′∗ + c3 √ 3c∗ c′∗ ) ξ,\nwhich is less than 1− ξ if we pick c∗ sufficiently small compared to c′∗. To summarize, we get that for any γ ∈ (0, exp(−1)), by picking η as in Eq. (24), we have that after T iterations (where T is specified in Eq. (24)), with probability at least 1 − 2γ, we get wT such that 1− 〈v1,wT 〉 ≤ 12 . Substituting δ = 2γ and ζ = 2ξ, we get that if\n〈v1, w̃0〉2 ≥ ζ > 0,\nand η satisfies\nη ≤ c1δ 2λζ3\nlog2(2/δ)\n(for some universal constant c1), then with probability at least 1− δ, after\nT =\n⌊ c2 log(2/δ)\nηλζ\n⌋ .\nstochastic iterations, we get a satisfactory point wT . As discussed at the beginning of the proof, this analysis is valid assuming r = maxi ‖xi‖2 ≤ 1. By the reduction discussed at the beginning of Subsection 6.1.2, we can get an analysis for any r by substituting λ→ λ/r and η → ηr. This means that we should pick η satisfying\nηr ≤ c1δ 2(λ/r)ζ3 log2(2/δ) ⇒ η ≤ c1δ 2λζ3 r2 log2(2/δ) ,\nand getting the required point after\nT =\n⌊ c2 log(2/δ)\n(ηr)(λ/r)ζ\n⌋ = ⌊ c2 log(2/δ)\nηλζ ⌋ iterations."
    }, {
      "heading" : "6.3 Proof of Theorem 4",
      "text" : "For simplicity of notation, we drop the A subscript from FA, and refer simply to F . We first prove the following two auxiliary lemmas:\nLemma 19. If A is a symmetric matrix, then the gradient of the function F (w) = −w>Aw‖w‖2 at some w equals\n− 2 ‖w‖2 (F (w)I +A)w,\nand its Hessian equals\n− 1 ‖w‖2 (( I − 4 ‖w‖2 ww> )( F (w)I +A ))⊥ ,\nwhere B⊥ = B +B> (i.e., a matrix B plus its transpose).\nProof. By the product and chain rules (using the fact that 1‖w‖2 is a composition of w 7→ ‖w‖ 2 and z 7→ 1z ), the gradient of F (w) = − 1‖w‖2 ( w>Aw ) equals\nw 2 ‖w‖4 ( w>Aw ) − (Aw) 2 ‖w‖2 , (25)\ngiving the gradient bound in the lemma statement after a few simplifications. Differentiating the vector-valued Eq. (25) with respect to w (using the product and chain rules, and the fact that 1‖w‖4 is a composition of w 7→ ‖w‖ 2, z 7→ z2, and z 7→ 1z ), we get that the Hessian of F equals\nI 2\n‖w‖4 (w>Aw) + w ( − 2 ‖w‖8 ∗ 2‖w‖2 ∗ 2w )> ( w>Aw ) + w 2 ‖w4‖ (2Aw)>\n−A 2 ‖w‖2 − (Aw) ( − 2 ‖w‖4 ∗ 2w )>\n= − 2F (w) ‖w‖2 I + 8F (w) ‖w‖4 ww> +\n4 ‖w‖4 ww>A − 2 ‖w‖2 A +\n4\n‖w‖4 Aww>\n= − 1 ‖w‖2\n( 2F (w)I − 8F (w)\n‖w‖2 ww> − 4 ‖w‖2 ww>A+ 2A− 4 ‖w‖2 Aww>\n) ,\nwhich can be verified to equal the expression in the lemma statement (using the fact that A,ww> and I are all symmetric matrices, hence equal their transpose).\nLemma 20. Let w0,v1 be two unit vectors such that ‖w0 − v1‖ ≤ < 12 (which implies 〈w0,v1〉 > 0). Let v′1 be the intersection of the ray {av1 : a ≥ 0} with the hyperplane Hw0 = {w : 〈w,w0〉 = 1}. Then ‖v′1 −w0‖ ≤ 54 .\nProof. See Figure 2 in the main text for a graphical illustration. Letting v′1 = av, a must satisfy 〈av1,w0〉 = 1. Since v1,w0 are unit vectors, this implies\na = 1\n〈v1,w0〉 =\n2\n2− ‖v1 −w0‖2 ,\nand since ‖v1 −w0‖ ≤ , this means that a ∈ [ 1, 2\n2− 2\n] .\nTherefore,\n‖v′1−w0‖ ≤ ‖v1−w0‖+‖v′1−v1‖ ≤ +‖av1−v1‖ ≤ + |a−1| ≤ + 2\n2− 2 −1 = +\n2\n2 + 2 ,\nand since < 12 , this is at most 5 4 .\nWe now turn to prove the theorem. Let∇2(w) denote the Hessian at some point w. To show smoothness and strong convexity as stated in the theorem, it is enough to fix some unit w0 which is -close to the leading eigenvector v1 (where is assumed to be sufficiently small), and show that for any point w on Hw0 which is O( ) close to w0, and any direction g along Hw0 (i.e. any unit g such that 〈g,w0〉 = 0), it holds that g>∇2(w)g ∈ [λ, 20]. This implies that the second derivative in an O( ) neighborhood of w0 on Hw0 is always in [λ, 20], hence the function is both λ-strongly convex in that neighborhood.\nMore formally, letting ∈ (0, 1) be a small parameter to be chosen later, consider any w0 such that\n‖w0‖ = 1 , ‖w0 − v1‖ ≤ ,\nany w such that 〈w −w0,w0〉 = 0 , ‖w −w0‖ ≤ 2 ,\nand any g such that ‖g‖ = 1 , 〈g,w0〉 = 0.\nOur goal is to show that for an appropriate , we have g>∇2(w)g ∈ [λ, 20]. Moreover, by Lemma 20, the neighborhood set Hw0 ∩Bw0(2 ) would also contain a point av1 for some a, which is a global optimum of F due to its scale-invariance. This would establish the theorem.\nThe easier part is to show the upper bound on g>∇2(w)g. Since g is a unit vector, it is enough to bound the spectral norm of∇2(w), which equals∥∥∥∥∥ 1‖w‖2 (( I − 4 ‖w‖2 ww> )( F (w)I +A ))⊥∥∥∥∥∥ sp\n≤ 2 ‖w‖2 ∥∥∥∥(I − 4‖w‖2ww> )( F (w)I +A )∥∥∥∥ sp\n≤ 2 ‖w‖2 ∥∥∥∥I − 4‖w‖2ww> ∥∥∥∥ sp ‖F (w)I +A‖sp\n≤ 2 ‖w‖2\n( ‖I‖sp + ∥∥∥∥ 4‖w‖2ww> ∥∥∥∥ sp ) (‖F (w)I‖sp + ‖A‖sp) .\nSince the spectral norm of A is 1, and ‖w‖2 ≥ 1 (as w lies on a hyperplane Hw0 tangent to a unit vector w0), it is easy to verify that this is at most 2(1 + 4)(1 + 1) = 20 as required.\nWe now turn to lower bound g>∇2(w)g, which by Lemma 19 equals\n− 1 ‖w‖2\ng> ((\nI − 4 ‖w‖2\nww> )( F (w)I +A ))⊥ g.\nSince g>B⊥g = g>Bg + g>B>g = 2g>Bg, the above equals\n− 2 ‖w‖2 g> ( I − 4 ‖w‖2 ww> )( F (w)I +A ) g. (26)\nUsing the fact that w = w0 + (w −w0), and 〈g,w0〉 = 0, we get that 〈g,w〉 = 〈g,w −w0〉. Moreover, sinceA is positive semidefinite and has spectral norm of 1, F (w) = −w>Aw‖w‖2 ∈ [−1, 0]. Expanding Eq. (26) and plugging these in, we get\n− 2 ‖w‖2\n( F (w)g> ( I − 4 ‖w‖2 ww> ) g + g> ( I − 4 ‖w‖2 ww> ) Ag ) = 2\n‖w‖2\n( −F (w)‖g‖2 + 4F (w)\n‖w‖2 〈g,w −w0〉2 − g>Ag +\n4\n‖w‖2 〈g,w −w0〉w>Ag ) ≥ 2 ‖w‖2 ( −F (w)‖g‖2 − 4 ‖w‖2 ‖g‖2‖w −w0‖2 − g>Ag − 4 ‖w‖2 ‖g‖‖w −w0‖‖w‖‖A‖sp‖g‖ ) .\nSince ‖g‖ = 1, ‖A‖sp = 1, ‖w −w0‖ ≤ 2 , and ‖w‖2 = ‖w0‖2 + ‖w −w0‖2 is between 1 and 1 + 4 2, this is at least\n2 ‖w‖2 ( (−F (w))− 16 2 − g>Ag − 8 √ 1 + 4 2 ) = 2 ‖w‖2 ( −F (w)− g>Ag − 8 ( 2 + √ 1 + 4 2 )) .\n(27) Let us now analyze −F (w) and g>Ag more carefully. The idea will be to show that since we are close to the optimum, −F (w) is very close to 1, and g (which is orthogonal to the near-optimal w0) is such that g>Ag is strictly smaller than 1. This would give us a positive lower bound on Eq. (27).\n• By the triangle inequality and the assumptions ‖w0−v1‖ ≤ , ‖w−w0‖ ≤ 2 , we have ‖w−v1‖ ≤ 3 . Also, we claim that F (·) is 4-Lipschitz outside the unit Euclidean ball (since the gradient of F at any point with norm ≥ 1, according to Lemma 19, has norm at most 4). Therefore, |F (w) + 1| = |F (w)− F (v1)| ≤ 4‖w − v1‖ ≤ 12 , so overall,\nF (w) ≤ −1 + 12 . (28)\n• Since 〈w0,g〉 = 0, and ‖w0 − v1‖ ≤ , it follows that\n|〈v1,g〉| ≤ |〈v1 −w0,g〉|+ |〈w0,g〉| ≤ ‖v1 −w0‖‖g‖+ 0 ≤ .\nLetting v1, . . . ,vd and 1 = s1 > s2 ≥ .. ≥ sd ≥ 0 be the eigenvectors and eigenvalues of A in decreasing order (and recalling that s2 ≤ s1 − λ = 1− λ for some eigengap λ > 0), we get\ng>Ag = d∑ i=1 si〈vi,g〉2 ≤ 〈v1,g〉2 + (1− λ) d∑ i=1 〈vi,g〉2\n= 〈v1,g〉2 + (1− λ)(1− 〈v1,g〉2) = λ〈v1,g〉2 + (1− λ) ≤ λ 2 + (1− λ) = 1− (1− 2)λ. (29)\nPlugging Eq. (28) and Eq. (29) back into Eq. (27), we get a lower bound of\n2 ‖w‖2 ( 1− 12 − ( 1− (1− 2)λ ) − 8 ( 2 + √ 1 + 4 2 )) = 2 ‖w‖2 ( (1− 2)λ− 8 ( 1.5 + 2 + √ 1 + 4 2 ))\n= 2\n‖w‖2\n1− 2 − 8 ( 1.5 + 2 + √ 1 + 4 2 )\nλ λ. Using the fact that √ 1 + z2 ≤ 1 + z, this can be loosely lower bounded by\n2\n‖w‖2\n( 1− − 8 (2.5 + 4 )\nλ\n) λ.\nRecalling that ‖w‖2 = ‖w0‖2 +‖w−w0‖2 is at most 1+4 2, and picking sufficiently small compared to λ, (say = λ/44), we get that the above is at least λ, which implies the required strong convexity condition.\nTo summarize, by picking = λ/44, we have shown that the function F (w) is λ-strongly convex and 20-smooth in a neighborhood of size 2 = λ22 around w0 on the hyperplaneHw0 , provided that ‖w0−v1‖ ≤ = λ44 . By Lemma 20, we are guaranteed that this neighborhood contains v1 up to some rescaling (which is immaterial for our scale-invariant function F ), hence by optimizing F in that neighborhood, we will get a globally optimal solution."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is supported in part by an FP7 Marie Curie CIG grant, the Intel ICRI-CI Institute, and Israel Science Foundation grant 425/13."
    } ],
    "references" : [ {
      "title" : "A lower bound for the optimization of finite sums",
      "author" : [ "A. Agarwal", "L. Bottou" ],
      "venue" : "ICML",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Stochastic optimization for PCA and PLS",
      "author" : [ "R. Arora", "A. Cotter", "K. Livescu", "N. Srebro" ],
      "venue" : "2012 50th Annual Allerton Conference on Communication, Control, and Computing",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Stochastic optimization of PCA with capped MSG",
      "author" : [ "R. Arora", "A. Cotter", "N. Srebro" ],
      "venue" : "NIPS",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The fast convergence of incremental PCA",
      "author" : [ "A. Balsubramani", "S. Dasgupta", "Y. Freund" ],
      "venue" : "NIPS",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convex optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge university press",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Global convergence of stochastic gradient descent for some nonconvex matrix problems",
      "author" : [ "C. De Sa", "K. Olukotun", "C. Ré" ],
      "venue" : "ICML",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Matrix computations",
      "author" : [ "G. H Golub", "C. Van Loan" ],
      "venue" : "volume 3. John Hopkins University Press",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "N. Halko", "P. Martinsson", "J. Tropp" ],
      "venue" : "SIAM review, 53(2):217–288",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The noisy power method: A meta algorithm with applications",
      "author" : [ "M. Hardt", "E. Price" ],
      "venue" : "NIPS",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "W. Hoeffding" ],
      "venue" : "Journal of the American statistical association, 58(301):13–30",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1963
    }, {
      "title" : "Matrix analysis",
      "author" : [ "R. Horn", "C. Johnson" ],
      "venue" : "Cambridge university press",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "NIPS",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The method of stochastic approximation for the determination of the least eigenvalue of a symmetrical matrix",
      "author" : [ "T.P. Krasulina" ],
      "venue" : "USSR Computational Mathematics and Mathematical Physics, 9(6):189–195",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "Estimating the largest eigenvalue by the power and lanczos algorithms with a random start",
      "author" : [ "J. Kuczynski", "H. Wozniakowski" ],
      "venue" : "SIAM journal on matrix analysis and applications, 13(4):1094–1122",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Stochastic proximal gradient descent with acceleration techniques",
      "author" : [ "A. Nitanda" ],
      "venue" : "NIPS",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Simplified neuron model as a principal component analyzer",
      "author" : [ "E. Oja" ],
      "venue" : "Journal of mathematical biology, 15(3):267–273",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Sampling from large matrices: An approach through geometric functional analysis",
      "author" : [ "M. Rudelson", "R. Vershynin" ],
      "venue" : "Journal of the ACM (JACM), 54(4):21",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A stochastic PCA and SVD algorithm with an exponential convergence rate",
      "author" : [ "O. Shamir" ],
      "venue" : "ICML",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sketching as a tool for numerical linear algebra",
      "author" : [ "D. Woodruff" ],
      "venue" : "Theoretical Computer Science, 10(1- 2):1–157",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Abstract We study the convergence properties of the VR-PCA algorithm introduced by [19] for fast computation of leading singular vectors.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "the Lanczos method) [8].",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 12,
      "context" : "Such algorithms have been known for several decades ([14, 17]), and enjoyed renewed interest in recent years, e.",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : "Such algorithms have been known for several decades ([14, 17]), and enjoyed renewed interest in recent years, e.",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "[2, 4, 3, 10, 6].",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "[2, 4, 3, 10, 6].",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "[2, 4, 3, 10, 6].",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "[2, 4, 3, 10, 6].",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 5,
      "context" : "[2, 4, 3, 10, 6].",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "[9, 20].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "[9, 20].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "Recently, [19] proposed a new practical algorithm, VR-PCA, for solving Eq.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 11,
      "context" : "The algorithm is based on a recent variance-reduction technique designed to speed up stochastic algorithms for convex optimization problems ([13]), although the optimization problem in Eq.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "See Section 3 for a more detailed description of this algorithm, and [19] for more discussions as well as empirical results.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "The results and analysis in [19] left several issues open.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "Although [19] proposed a variant of the algorithm for that case, and studied it empirically, no analysis was provided.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 17,
      "context" : "We begin by recalling the algorithm of [19] for the k = 1 case (Algorithm 1), and then discuss its generalization for k > 1.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : "We now turn to provide a formal analysis of Algorithm 2, which directly generalizes the analysis of Algorithm 1 given in [19]: Theorem 1.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "This runtime bound is the same3 as that of [19] for k = 1.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "We note that although Bt−1 appears essential for our analysis, it isn’t clear that using it is necessary in practice: In [19], the suggested block algorithm was Algorithm 2 with Bt−1 = I , which seemed to work well in experiments.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "However, experimentally the algorithm seems to work well even with random initialization [19].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "Moreover, if we are interested in a theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a W̃0.",
      "startOffset" : 171,
      "endOffset" : 181
    }, {
      "referenceID" : 8,
      "context" : "Moreover, if we are interested in a theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a W̃0.",
      "startOffset" : 171,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "Moreover, if we are interested in a theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a W̃0.",
      "startOffset" : 171,
      "endOffset" : 181
    }, {
      "referenceID" : 17,
      "context" : "[19] showed that it’s possible to further improve the runtime for sparse X , replacing d by the average column sparsity ds.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[18]) is a relaxation of the standard notion of rank: For any d × d matrix A, nrank(A) is at most the rank of A (which in turn is at most d).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "For example, this holds for [6], although the bound only guarantees the existence of some iteration which produces the desired output.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "The guarantee of [4] scale as d/λ, and the guarantee of [10] scales as d/λ in our setting.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "The guarantee of [4] scale as d/λ, and the guarantee of [10] scales as d/λ in our setting.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "[4, 10, 6]).",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 8,
      "context" : "[4, 10, 6]).",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 5,
      "context" : "[4, 10, 6]).",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 13,
      "context" : "Indeed, when using deterministic methods such as power iterations or the Lanczos method, the dependence on λ in the runtime is only 1/λ or even √ 1/λ [15].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 11,
      "context" : "[13, 16, 7] in the context of the variance-reduction technique we use).",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 14,
      "context" : "[13, 16, 7] in the context of the variance-reduction technique we use).",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 4,
      "context" : "(1) can be “trivially” convexified, by re-casting it as an equivalent semidefinite program [5].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "whereas the best known guarantees on getting an -optimal solution for λ-strongly convex and smooth functions (see [1]) is on the order of",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "1 Proof of Theorem 1 Although the proof structure generally mimics the proof of Theorem 1 in [19] for the k = 1 special case, it is more intricate and requires several new technical tools.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, suppose that for some fixed α, γ, δ > 0, it holds with probability 1 that • For all ν ∈ [0, 1], B2 + νZ2 δI .",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "f(ν) = Tr ( (B1 + νZ1)(B2 + νZ2) −1) , ν ∈ [0, 1].",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "[8]), and the solution is easily shown to be B = V U> where USV > is the SVD decomposition of C>D.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Letting st, st−1 denote the vectors of singular values of V > k Wt and V > k Wt−1, and noting that they are both in [0, 1]k (as Vk,Wt−1,Wt all have orthonormal columns), the left hand side of the inequality in the lemma statement equals",
      "startOffset" : 116,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "By Weyl’s matrix perturbation theorem6 [12], this is upper bounded by 2k‖V > k Wt − V > k Wt−1‖sp ≤ 2k‖Vk‖sp‖Wt −Wt−1‖sp ≤ 2k‖Wt −Wt−1‖sp.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "≥ sd in [0, 1], and suppose that sk − sk+1 ≥ λ for some λ > 0.",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "• B2 + νZ2 3 8I for all ν ∈ [0, 1]: Recalling the definition of B2, Z2, and the facts that A 0, N>N 0 (by construction), and W>W = I , we have that B2 I .",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "But each σi is in [0, 1] (as Vk,W have orthonormal columns), so no σi can be less than 12 .",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "Armed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows that with probability at least 1−β, it holds simultaneously for all t = 1, .",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "Let A,wt be as defined in Algorithm 1, and suppose that η ∈ [ 0, 1 23 ] .",
      "startOffset" : 60,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "Armed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows that with probability at least 1− β, it holds simultaneously for all t = 0, 1, .",
      "startOffset" : 88,
      "endOffset" : 92
    } ],
    "year" : 2015,
    "abstractText" : "We study the convergence properties of the VR-PCA algorithm introduced by [19] for fast computation of leading singular vectors. We prove several new results, including a formal analysis of a block version of the algorithm, and convergence from random initialization. We also make a few observations of independent interest, such as how pre-initializing with just a single exact power iteration can significantly improve the runtime of stochastic methods, and what are the convexity and non-convexity properties of the underlying optimization problem.",
    "creator" : "LaTeX with hyperref package"
  }
}