{
  "name" : "1511.03766.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sparse Learning for Large-scale and High-dimensional Data: A Randomized Convex-concave Optimization Approach",
    "authors" : [ "Lijun Zhang", "Tianbao Yang", "Rong Jin", "Zhi-Hua Zhou" ],
    "emails" : [ "zhanglj@lamda.nju.edu.cn", "tianbao-yang@uiowa.edu", "rongjin@cse.msu.edu", "zhouzh@lamda.nju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n03 76\n6v 1\n[ cs\nKeywords: random projection, sparse learning, convex-concave optimization, primal solution, dual solution"
    }, {
      "heading" : "1. Introduction",
      "text" : "Learning the sparse representation of a predictive model has received considerable attention in recent years (Bach et al., 2012). Given a set of training examples {(xi,yi)}ni=1 with xi ∈ Rd and yi ∈ R, the optimization problem is generally formulated as\nmin w∈Ω\n1\nn\nn∑\ni=1\nℓ(yix ⊤ i w) + γψ(w) (1)\nwhere ℓ(·) is a convex function such as the logistic loss to measure the empirical error, and ψ(·) is a sparsity-inducing regularizer such as the elastic net (Zou and Hastie, 2005) to\navoid overfitting (Hastie et al., 2009). When both d and n are very large, directly solving (1) could be computational expensive. To address this challenge, a straightforward way is to first reduce the dimensionality of the data, then solve a low-dimensional problem, and finally map the solution back to the original space. The limitation of this approach is that the final solution, after mapping from the low-dimensional space to the original high-dimensional space, may not be sparse.\nThe goal of this paper is to develop an efficient algorithm for solving the problem in (1), and at the same time preserve the (approximate) sparsity of the solution. Our approach is motivated by the following simple observation:\nIf there exists a sparse model with high prediction accuracy, the dual solution to (1) is also sparse or at least approximately sparse.\nTo see this, let us formulate (1) as a convex-concave optimization problem. By writing ℓ(z) in its convex conjugate form, i.e.,\nℓ(z) = max λ∈Γ\nλz − ℓ∗(λ),\nwhere ℓ∗(·) is the Fenchel conjugate of ℓ(·) (Rockafellar, 1997) and Γ is the domain of the dual variable, we get the following convex-concave formulation:\nmax λ∈Γn min w∈Ω\nγnψ(w)− n∑\ni=1\nℓ∗(λi) + n∑\ni=1\nλiyix ⊤ i w. (2)\nDenote the optimal solutions to (2) by (w∗,λ∗). By the Fenchel conjugate theory (CesaBianchi and Lugosi, 2006, Lemma 11.4), we have\n[λ∗]i = ℓ ′(yix ⊤ i w∗).\nLet us consider the squared hinge loss for classification, where ℓ(z) = max(0, 1−z)2. Therefore, yix ⊤ i w∗ ≥ 1 indicates that [λ∗]i = 0. As a result, when most of the examples can be classified by a large margin (which is likely to occur in large-scale and high-dimensional setting), it is reasonable to assume that the dual solution is sparse. Similarly, for logistic regression, we can argue the dual solution is approximately sparse.\nAbstracting (2) slightly, in the following, we will study a general convex-concave optimization problem:\nmax λ∈∆ min w∈Ω\ng(w) − h(λ)−w⊤Aλ (3)\nwhere ∆ ⊆ Rn and Ω ⊆ Rd are the domains for λ and w, respectively, g(·) and h(·) are two convex functions, and A ∈ Rd×n is a matrix. The benefit of analyzing (3) instead of (1) is that the convex-concave formulation allows us to exploit the prior knowledge that both w∗ and λ∗ are sparse or approximately sparse. The problem in (3) has been widely studied in the optimization community, and when n and d are medium size, it can be solved iteratively by gradient based methods (Nesterov, 2005; Nemirovski, 2005).\nWe assume the two convex functions g(·) and h(·) are relatively simple such that evaluating their values or gradients take O(d) and O(n) complexities, respectively. The bottleneck is the computations involving the bilinear term w⊤Aλ, which have O(nd) complexity in\nboth time and space. To overcome this difficulty, we develop a randomized algorithm which solves (3) approximately but at a significantly low cost. The proposed algorithm combines two well-known techniques—random projection and ℓ1-norm regularization in a principled way. Specifically, random projection is used to find a low-rank approximation of A, which not only reduces the storage requirement but also accelerates the computations. The role of ℓ1-norm regularization is twofold. One one hand, it is introduced to compensate for the distortion caused by randomization, and on the other hand it enforces the sparsity of the final solutions. Under mild assumptions about the optimization problem in (3), the proposed algorithm has a small recovery error provided the optimal solutions to (3) are sparse or approximately sparse.\nNotations For a vector x ∈ Rd and a set D ⊆ [d], we denote by xD the vector which coincides with x on D and has zero coordinates outside D."
    }, {
      "heading" : "2. Related Work",
      "text" : "Random projection has been widely used as an efficient algorithm for dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001). In the case of unsupervised learning, it has been proved that random projection is able to preserve the distance (Dasgupta and Gupta, 2003), inner product (Arriaga and Vempala, 2006), volumes and distance to affine spaces (Magen, 2002). In the case of supervised learning, random projection is generally used as a pre-processing step to find a low-dimensional representation of the data, and thus reduce the computational cost of training. For classification, theoretical studies mainly focus on examining the generalization error or the preservation of classification margin in the low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013). For regression, there do exist theoretical guarantees for the recovery error, but they only hold for the least squares problem (Mahoney, 2011).\nOur work is closely related to Dual Random Projection (DRP) (Zhang et al., 2013) and Dual-sparse Regularized Randomized Reduction (DSRR) (Yang et al., 2015), which also investigate random projection from the perspective of optimization. However, both DRP and DSRR are limited to the special case that ψ(w) = ‖w‖22, which leads to a simple dual problem and can be analyzed easily. In contrast, our algorithm is designed for the case that ψ(·) is a sparsity-inducing regularizer, and built upon the convex-concave formulation. Similar to DSRR, our algorithm makes use of the sparsity of the dual solution, but we further exploit the sparsity of the primal solution. A noticeable advantage of our analysis is the mild assumption about the data matrix A. In order to recover the primal solution, DRP assumes the data matrix is low-rank and DSRR assumes it satisfies the restricted eigenvalue condition, on the other hand, our algorithm only requires columns or rows of A are well-bounded."
    }, {
      "heading" : "3. Algorithm",
      "text" : "To reduce the computational cost of (3), we first generate a random matrix R ∈ Rn×m, where m ≪ min(d, n). Define Â = AR ∈ Rd×m, we propose to solve the following problem\nmax λ∈∆ min w∈Ω\ng(w)− h(λ)−w⊤ÂR⊤λ+ γw‖w‖1 − γλ‖λ‖1 (4)\nwhere γw and γλ are two regularization parameters. The construction of the random matrix R, as well as the values of the two regularization parameters γw and γλ will be discussed later. The optimization problem in (4) can be solved by the algorithm designed for composite convex-concave problems (He and Monteiro, 2014).\nCompared to (3), the main advantage of (4) is that it only needs to load Â and R into the memory, making it convenient to deal with large-scale problems. With the help of random projection, the computational complexity for evaluating the value and gradient is reduced from O(dn) to O(dm+ nm). Compared to previous randomized algorithms (Balcan et al., 2006; Zhang et al., 2013; Yang et al., 2015), (4) has two new features: i) the optimization is still performed in the original space; and ii) the ℓ1-norm is introduced to regularize both primal and dual solutions. As we will prove later, the combination of these two features will ensure the solutions to (4) are approximately sparse. Finally, note that in (4) RR⊤ is inserted at the right side of A, it can also be put at the left side of A.\nLet (w∗,λ∗) and (ŵ, λ̂) be the optimal solution to the convex-concave optimization problem in (3) and (4), respectively. Under suitable conditions, we will show that\n‖ŵ −w∗‖2 ≤ O (√\n‖w∗‖0‖λ∗‖0 log n m\n) and ‖λ̂− λ∗‖2 ≤ O (√ ‖w∗‖0‖λ∗‖0 log d\nm\n)\nimplying a small recovery error when w∗ and λ∗ are sparse. A similar recovery guarantee also holds when the optimal solutions to (3) are approximately sparse, i.e., when they can be well-approximated by sparse vectors."
    }, {
      "heading" : "4. Main Results",
      "text" : "We first introduce assumptions that we make, and then present the theoretical guarantees of (4)."
    }, {
      "heading" : "4.1 Assumptions",
      "text" : "Assumptions about (3) We make the following assumptions about (3).\n• g(w) is α-strongly convex with respect to the Euclidean norm. Let’s take the optimization problem in (2) as an example. (2) will satisfy this assumption if some strongly convex function (e.g., ‖w‖22) is a part of the regularizer ψ(w).\n• h(λ) is β-strongly convex with respect to the Euclidean norm. For the problem in (2), if ℓ(·) is a smooth function (e.g., the logistic loss), then its convex conjugate ℓ∗(·) will be strongly convex (Rockafellar, 1997; Kakade et al., 2009).\n• Either columns or rows of A have bounded ℓ2-norm. Without loss of generality, we assume\n‖Ai∗‖2 ≤ 1, ∀i ∈ [d], (5) ‖A∗j‖2 ≤ 1, ∀j ∈ [n]. (6)\nThe above assumption can be satisfied by normalizing rows or columns of A. In a non-essential theorem (Theorem 6), which is included to enrich this study, we need a stronger assumption that both columns and rows of A are bounded.\nAssumptions about R We assume the random matrix R ∈ Rn×m has the following property.\n• With a high probability, the linear operator R⊤ : Rn 7→ Rm is able to preserve the ℓ2-norm of its input. In mathematical terms, we need the following theorem.\nTheorem 1 There exists a constant c > 0, such that\nPr { (1− ε)‖x‖22 ≤ ‖R⊤x‖22 ≤ (1 + ε)‖x‖22 } ≥ 1− 2 exp(−mε2/c)\nfor any fixed x ∈ Rd and 0 < ǫ ≤ 1/2.\nThe above theorem is widely used to prove the famous Johnson–Lindenstrauss lemma (Dasgupta and Gupta, 2003). Let R = 1√\nm S. Previous studies (Achlioptas, 2003; Arriaga and\nVempala, 2006) have proved that Theorem 1 is true if {Sij} are independent random variables sampled from the Gaussian distribution N (0, 1), uniform distribution over {±1}, or the following database-friendly distribution\nX =    √ 3, with probability 1/6; 0, with probability 2/3;\n− √ 3, with probability 1/6.\nMore generally, a sufficient condition for Theorem 1 is that columns of R are independent, isotropic, and subgaussian vectors (Mendelson et al., 2008)."
    }, {
      "heading" : "4.2 Theoretical Guarantees",
      "text" : ""
    }, {
      "heading" : "4.2.1 Sparse Solutions",
      "text" : "We first consider the case that both w∗ and λ∗ are sparse. Define\nrw = ‖w∗‖0, and rλ = ‖λ∗‖0.\nWe have the following theorem.\nTheorem 2 Set\nγλ ≥ 2‖A⊤w∗‖2 √ c\nm log\n4n\nδ , (7)\nγw ≥ 2‖λ∗‖2 √ c\nm log\n4d\nδ +\n6γλ √ rλ\nβ\n( 1 + 7 √ c\nm\n( log 4d\nδ + 16rλ log\n9n\n8rλ\n)) . (8)\nWith a probability at least 1− 3δ, we have\n‖ŵ −w∗‖2 ≤ 3γw\n√ rw\nα , ‖ŵ −w∗‖1 ≤ 12γwrw α , and ‖ŵ −w∗‖1 ‖ŵ −w∗‖2 ≤ 4√rw\nprovided\nm ≥ 4c log 4 δ . (9)\nNotice that ‖ŵ − w∗‖1/‖ŵ − w∗‖2 ≤ 4 √ rw indicates that ŵ − w∗ is approximately sparse (Plan and Vershynin, 2013a,b). Combining with the fact w∗ is sparse, we conclude that ŵ is also approximately sparse.\nThen, we discuss the recovery guarantee for the sparse learning problem in (1) or (2). Generally speaking, the domain Ω ⊆ Rd of the primal variable belongs to certain ℓ2-norm ball, thus we can take ‖w∗‖2 = O(1). So, all the elements of A⊤w∗ ∈ Rn are bounded, which implies ‖A⊤w∗‖2 = O( √ n). Since the domain Γ ⊆ R of the dual variable is always bounded and ‖λ∗‖0 = rλ, we can take ‖λ∗‖2 = O( √ rλ). According to the theoretical analysis of regularized empirical risk minimization (Wu and Zhou, 2005; Sridharan et al., 2009; Koltchinskii, 2011), the optimal γ, that minimizes the generalization error, can be chosen as γ = O(1/ √ n), and thus α = O(γn) = O( √ n). When the loss ℓ(·) is smooth, we have β = O(1). The following corollary provides a simplified result based on the above discussions.\nCorollary 1 Assume ‖A⊤w∗‖2 = O( √ n), ‖λ∗‖2 = O( √ rλ), α = O( √ n), and β = O(1). We can choose\nγλ = O\n(√ n log n\nm\n) and γw = O (√ rλ log d\nm + γλ\n√ rλ ) = O (√ nrλ log n\nm\n)\nsuch that with a high probability\n‖ŵ −w∗‖2 = O ( γw √ rw√ n ) = O (√ rwrλ log n m ) and ‖ŵ −w∗‖1 ‖ŵ −w∗‖2 ≤ 4√rw.\nA natural question to ask is whether similar recovery guarantees for λ̂ can be proved under the conditions in Theorem 2. Unfortunately, we are not able to give a positive answer, and only have the following theorem.\nTheorem 3 Assume γλ satisfies the condition in (7). With a probability at least 1− δ, we have\n‖λ̂− λ∗‖2 ≤ 3γλ\n√ rλ β + 2 β ( 1 + ‖RR⊤ − I‖2 ) ‖A⊤(ŵ −w∗)‖2\nprovided (9) holds.\nThe upper bound in the above theorem is quite loose, because ‖RR⊤ − I‖2 is roughly on the order of n log n/m (Tropp, 2012).\nTo obtain a tight bound for λ̂, we can solve a different problem given by\nmax λ∈∆ min w∈Ω\ng(w) − h(λ)−w⊤RÂλ+ γw‖w‖1 − γλ‖λ‖1 (10)\nwhere R ∈ Rd×m is a random matrix, and Â = R⊤A ∈ Rm×n. Due to the symmetry between λ and w, we directly have a theoretical guarantee for λ̂ by replacing w∗ in Theorem 2 with λ∗, ŵ with λ̂, n with d, and so on. To be specific, we have the following theorem.\nTheorem 4 Set\nγw ≥ 2‖Aλ∗‖2 √ c\nm log\n4d\nδ ,\nγλ ≥ 2‖w∗‖2 √ c\nm log\n4n\nδ +\n6γw √ rw\nα\n( 1 + 7 √ c\nm\n( log 4n\nδ + 16rw log\n9d\n8rw\n)) .\nWith a probability at least 1− 3δ, we have\n‖λ̂− λ∗‖2 ≤ 3γλ\n√ rλ β , ‖λ̂− λ∗‖1 ≤ 12γλrλ β , and ‖λ̂− λ∗‖1 ‖λ̂− λ∗‖2 ≤ 4√rλ\nprovided (9) holds.\nTo simplify the above theorem, we need to estimate the order of ‖Aλ∗‖2. Since ‖λ∗‖0 = rλ, Aλ∗ is a linear combination of rλ columns of A. Under the condition that columns of A are well-bounded, ‖Aλ∗‖2 = O( √ rλ) if the selected columns of A are orthogonal to each\nother. Since rλ ≪ n, we take a relaxed condition ‖Aλ∗‖2 = O( √ n). 1 Finally, we have the following corollary.\nCorollary 2 Assume ‖Aλ∗‖2 = O( √ n), ‖w∗‖2 = O(1), α = O( √ n), and β = O(1). We can choose\nγw = O\n(√ n log d\nm\n) and γλ = O (√ log n\nm + γw √ rw n ) = O (√ rw log d m )\nsuch that with a high probability\n‖λ̂− λ∗‖2 = O (γλ √ rλ) = O\n(√ rwrλ log d\nm\n) and\n‖λ̂− λ∗‖1 ‖λ̂− λ∗‖2 ≤ 4√rλ."
    }, {
      "heading" : "4.2.2 Approximately Sparse Solutions",
      "text" : "We now proceed to study the case that the optimal solutions to (3) are only approximately sparse. With a slight abuse of notation, we assume w∗ and λ∗ are two sparse vectors, with ‖w∗‖0 = rw and ‖λ∗‖0 = rλ, that solve (3) approximately in the sense that\n‖∇g(w∗)−Aλ∗‖∞ ≤ ς, (11) ‖∇h(λ∗) +A⊤w∗‖∞ ≤ ς, (12)\nfor some small constant ς > 0. The above conditions can be considered as sub-optimality conditions (Boyd and Vandenberghe, 2004) of w∗ and λ∗ measured in the ℓ∞-norm. After a similar analysis, we get the following theorem.\n1. Even the selected columns of A are linearly dependent, ‖Aλ∗‖2 cannot be larger than O(rλ). Then, ‖Aλ∗‖2 = O( √ n) always true if rλ ≤ O( √ n).\nTheorem 5 Assume (11) and (12) hold. Set\nγλ ≥ 2‖A⊤w∗‖2 √ c\nm log\n4n\nδ + 2ς,\nγw ≥ 2‖λ∗‖2 √ c\nm log\n4d\nδ +\n6γλ √ rλ\nβ\n( 1 + 7 √ c\nm\n( log 4d\nδ + 16rλ log\n9n\n8rλ\n)) + 2ς.\nWith a probability at least 1− 3δ, we have\n‖ŵ −w∗‖2 ≤ 3γw\n√ rw\nα , ‖ŵ −w∗‖1 ≤ 12γwrw α , and ‖ŵ −w∗‖1 ‖ŵ −w∗‖2 ≤ 4√rw\nprovided (9) holds.\nWhen ς is small enough, the upper bound in Theorem 6 is on the same order as that in Theorem 2. To be specific, we have the following corollary. Corollary 3 Assume ‖A⊤w∗‖2 = O( √ n), ‖λ∗‖2 = O( √ rλ), α = O( √ n), β = O(1), and\nς = O\n(√ n log n\nm\n) .\nWe can choose γλ and γw as in Corollary 1 such that with a high probability\n‖ŵ −w∗‖2 = O ( γw √ rw√ n ) = O (√ rwrλ log n m ) and ‖ŵ −w∗‖1 ‖ŵ −w∗‖2 ≤ 4√rw.\nWhen the optimal solutions to (3) are allowed to be approximately sparse, g(·) could be certain smooth regularizer, such as a mixture of ‖ · ‖22 and ‖ · ‖pp for 1 < p < 2. In the following, we provide a supporting theorem for this special case. we denote by w′∗ and λ ′ ∗ the optimal solutions to (3). To quantify the approximate sparsity of w′∗ and λ ′ ∗, we assume there exist two sparse vectors w∗ and λ∗, with ‖w∗‖0 = rw and ‖λ∗‖0 = rλ such that\n‖w∗ −w′∗‖2 ≤ τ and ‖λ∗ − λ′∗‖2 ≤ τ (13)\nfor some small constant τ > 0. Furthermore, we assume both g and h are µ-smooth, i.e.,\n‖∇g(w1)−∇g(w2)‖2 ≤ µ‖w1 −w2‖, ∀w1,w2, (14) ‖∇h(λ1)−∇h(λ2)‖2 ≤ µ‖λ1 − λ2‖, ∀λ1,λ2. (15)\nTheorem 6 Assume (13), (14), and (15) hold. Suppose w′∗ and λ ′ ∗ lie in the interior of Ω and ∆, respectively. Set\nγλ ≥ 2‖A⊤w∗‖2 √ c\nm log\n4n\nδ + 2(1 + µ)τ,\nγw ≥ 2‖λ∗‖2 √ c\nm log\n4d\nδ +\n6γλ √ rλ\nβ\n( 1 + 7 √ c\nm\n( log 4d\nδ + 16rλ log\n9n\n8rλ\n)) + 2(1 + µ)τ.\nWith a probability at least 1− 3δ, we have\n‖ŵ −w∗‖2 ≤ 3γw\n√ rw\nα , ‖ŵ −w∗‖1 ≤ 12γwrw α , and ‖ŵ −w∗‖1 ‖ŵ −w∗‖2 ≤ 4√rw\nprovided (9) holds. Similarly, if (1 + µ)τ = O (√ n log n/m ) , the conclusion in Corollary 1 also holds here."
    }, {
      "heading" : "5. Analysis",
      "text" : "In this section, we provide proofs of main theorems, and others can be found in the appendix."
    }, {
      "heading" : "5.1 Proof of Theorem 2",
      "text" : "To facilitate the analysis, we introduce a pseudo optimization problem\nmax λ∈∆\n−h(λ)−w⊤∗ ÂR⊤λ− γλ‖λ‖1\nwhose optimal solution is denoted by λ̃. In the following, we will first discuss how to bound the difference between λ̃ and λ∗, and then bound the difference between ŵ and w∗ in a similar way.\nFrom the optimality of λ̃ and λ∗, we derive the following lemma to bound their difference.\nLemma 1 Denote ρλ = ∥∥∥(RR⊤ − I)A⊤w∗ ∥∥∥ ∞ . (16) By choosing γλ ≥ 2ρλ, we have\n‖λ̃− λ∗‖2 ≤ 3γλ\n√ rλ β , ‖λ̃− λ∗‖1 ≤ 12γλrλ β , and ‖λ̃− λ∗‖1 ‖λ̃− λ∗‖2 ≤ 4√rλ.\nBased on the property of the random matrix R described in Theorem 1, we have the following lemma to bound ρλ in (16).\nLemma 2 With a probability at least 1− δ, we have\nρλ = ∥∥∥(RR⊤ − I)A⊤w∗ ∥∥∥ ∞ ≤ ‖A⊤w∗‖2 √ c m log 4n δ\nprovided (9) holds.\nCombining Lemma 1 with Lemma 2, we immediately obtain the following lemma.\nLemma 3 Set\nγλ ≥ 2‖A⊤w∗‖2 √ c\nm log\n4n\nδ .\nWith a probability at least 1− δ, we have\n‖λ̃− λ∗‖2 ≤ 3γλ\n√ rλ β , ‖λ̃− λ∗‖1 ≤ 12γλrλ β , and ‖λ̃− λ∗‖1 ‖λ̃− λ∗‖2 ≤ 4√rλ\nprovided (9) holds.\nWe are now in a position to formulate the key lemmas that lead to Theorem 2. Similar to Lemma 1, we introduce the following lemma to characterize the relation between ŵ and w∗.\nLemma 4 Denote\nρw = ∥∥∥A(I −RR⊤)λ∗ ∥∥∥ ∞ + ∥∥∥ARR⊤(λ∗ − λ̃) ∥∥∥ ∞ . (17)\nBy choosing γw ≥ 2ρw, we have\n‖ŵ −w∗‖2 ≤ 3γw\n√ rw\nα , ‖ŵ −w∗‖1 ≤ 12γwrw α , and ‖ŵ −w∗‖1 ‖ŵ −w∗‖2 ≤ 4√rw.\nThe last step of the proof is to derive an upper bound for ρw based on Theorem 1 and Lemma 3.\nLemma 5 Assume the conclusion in Lemma 3 happens. With a probability at least 1− 2δ, we have\nρw ≤ ‖λ∗‖2 √ c\nm log\n4d\nδ +\n3γλ √ rλ\nβ\n( 1 + 7 √ c\nm\n( log 4d\nδ + 16rλ log\n9n\n8rλ\n))\nprovided (9) holds."
    }, {
      "heading" : "5.2 Proof of Lemma 1",
      "text" : "Let Ωλ include the subset of non-zeros entries in λ∗ and Ω̄λ = [n] \\ Ωλ. Define\nL(λ) = −h(λ) + min w∈Ω g(w)−w⊤Aλ, L̃(λ) = −h(λ)−w⊤∗ ÂR⊤λ− γλ‖λ‖1.\nLet v ∈ ∂‖λ∗‖1 be any subgradient of ‖ · ‖1 at λ∗. Then, we have\nu = −∇h(λ∗)−RR⊤A⊤w∗ − γλv ∈ ∂L̃(λ∗). 2\nUsing the fact that λ̃ maximizes L̃(·) over the domain ∆ and h(·) is β-strongly convex, we have\n0 ≥ L̃(λ∗)− L̃(λ̃) ≥ 〈−(λ̃− λ∗),u〉 + β\n2 ‖λ∗ − λ̃‖22\n= 〈 λ̃− λ∗,∇h(λ∗) +RR⊤A⊤w∗ + γλv 〉 + β\n2 ‖λ∗ − λ̃‖22.\n(18)\nBy setting vi = sign(λ̃i), ∀i ∈ Ω̄λ, we have 〈λ̃Ω̄λ ,vΩ̄λ〉 = ‖λ̃Ω̄λ‖1. As a result,\n〈λ̃− λ∗,v〉 = 〈λ̃Ω̄λ ,vΩ̄λ〉+ 〈λ̃Ωλ − λ∗,vΩλ〉 ≥ ‖λ̃Ω̄λ‖1 − ‖λ̃Ωλ − λ∗‖1. (19)\n2. In the case that h(·) is non-smooth, ∇h(λ∗) refers to a subgradient of h(·) at λ∗. In particular, we choose the subgradient that satisfies (21).\nCombining (18) with (19), we have\n〈 λ̃− λ∗,∇h(λ∗) +RR⊤A⊤w∗ 〉 + β\n2 ‖λ∗ − λ̃‖22 + γλ‖λ̃Ω̄λ‖1 ≤ γλ‖λ̃Ωλ − λ∗‖1. (20)\nFrom the fact that λ∗ maximizes L(·) over the domain ∆, we have 〈∇L(λ∗),λ− λ∗〉 = 〈−∇h(λ∗)−A⊤w∗,λ− λ∗〉 ≤ 0, ∀λ ∈ ∆. (21)\nThen, 〈 λ̃− λ∗,∇h(λ∗) +RR⊤A⊤w∗ 〉\n= 〈 λ̃− λ∗,∇h(λ∗) +A⊤w∗ 〉 + 〈 λ̃− λ∗, (RR⊤ − I)A⊤w∗ 〉\n(21) ≥ − ‖λ̃− λ∗‖1 ∥∥∥(RR⊤ − I)A⊤w∗ ∥∥∥ ∞ (16) = − ρλ‖λ̃− λ∗‖1 = −ρλ ( ‖λ̃Ω̄λ‖1 + ‖λ̃Ωλ − λ∗‖1 ) .\n(22)\nFrom (20) and (22), we have\nβ 2 ‖λ̃− λ∗‖22 + (γλ − ρλ)‖λ̃Ω̄λ‖1 ≤ (γλ + ρλ)‖λ̃Ωλ − λ∗‖1.\nSince γλ ≥ 2ρλ, we have β\n2 ‖λ̃− λ∗‖22 + γλ 2 ‖λ̃Ω̄λ‖1 ≤ 3γλ 2 ‖λ̃Ωλ − λ∗‖1.\nAnd thus,\nβ 2 ‖λ̃− λ∗‖22 ≤ 3γλ 2 ‖λ̃Ωλ − λ∗‖1 ≤ 3γλ\n√ rλ 2 ‖λ̃Ωλ − λ∗‖2 ⇒ ‖λ̃− λ∗‖2 ≤ 3γλ √ rλ β ,\nβ\n2rλ ‖λ̃Ωλ − λ∗‖21 ≤\nβ 2 ‖λ̃− λ∗‖22 ≤ 3γλ 2 ‖λ̃Ωλ − λ∗‖1 ⇒ ‖λ̃Ωλ − λ∗‖1 ≤ 3γλrλ β ,\nγλ 2 ‖λ̃Ω̄λ‖1 ≤ 3γλ 2 ‖λ̃Ωλ − λ∗‖1 ⇒ ‖λ̃Ω̄λ‖1 ≤ 3‖λ̃Ωλ − λ∗‖1 ⇒ ‖λ̃− λ∗‖1 ≤ 12γλrλ β ,\n‖λ̃− λ∗‖1 ‖λ̃− λ∗‖2 ≤ ‖λ̃Ωλ − λ∗‖1 + ‖λ̃Ω̄λ‖1 ‖λ̃− λ∗‖2 ≤ 4‖λ̃Ωλ − λ∗‖1 ‖λ̃− λ∗‖2 ≤ 4 √ rλ‖λ̃Ωλ − λ∗‖2 ‖λ̃− λ∗‖2 ≤ 4√rλ."
    }, {
      "heading" : "5.3 Proof of Lemma 2",
      "text" : "We first introduce one lemma that is central to our analysis. From the property that R preserves the ℓ2-norm, it is easy to verify that it also preserves the inner product (Arriaga and Vempala, 2006). Specifically, we have the following lemma.\nLemma 6 Assume R satisfies Theorem 1. For any two fixed vectors u ∈ Rn and v ∈ Rn, with a probability at least 1− δ, we have\n∣∣∣u⊤RR⊤v − u⊤v ∣∣∣ ≤ ‖u‖2‖v‖2\n√ c\nm log\n4 δ .\nprovided (9) holds.\nFrom Lemma 6, we have with a probability at least 1− δ, ∣∣∣∣ [ (RR⊤ − I)A⊤w∗ ] j ∣∣∣∣ = ∣∣∣ej(RR⊤ − I)A⊤w∗ ∣∣∣ ≤ ‖A⊤w∗‖2 √ c m log 4 δ\nfor each j ∈ [n]. We complete the proof by taking the union bound over all j ∈ [n]."
    }, {
      "heading" : "5.4 Proof of Lemma 4",
      "text" : "Let Ωw include the subset of non-zeros entries in w∗ and Ω̄w = [d] \\ Ωw. Define G(w) = g(w) + max\nλ∈∆ −h(λ)−w⊤Aλ,\nĜ(w) = g(w) + γw‖w‖1 +max λ∈∆ −h(λ)−w⊤ÂR⊤λ− γλ‖λ‖1.\nLet v ∈ ∂‖w∗‖1 be any subgradient of ‖ · ‖1 at w∗. Then, we have u = ∇g(w∗)−ARR⊤λ̃+ γwv ∈ ∂Ĝ(w∗). 3\nUsing the fact that ŵ minimizes Ĝ(·) over the domain Ω and g(·) is α-strongly convex, we have\n0 ≥ Ĝ(ŵ)− Ĝ(w∗) ≥ 〈ŵ −w∗,u〉+ α\n2 ‖ŵ −w∗‖22\n= 〈 ŵ −w∗,∇g(w∗)−ARR⊤λ̃+ γwv 〉 + α\n2 ‖ŵ −w∗‖22.\n(23)\nBy setting vi = sign(ŵi), ∀i ∈ Ω̄w, we have 〈ŵΩ̄w ,vΩ̄w〉 = ‖ŵΩ̄w‖1. As a result, 〈ŵ −w∗,v〉 = 〈ŵΩ̄w ,vΩ̄w〉+ 〈ŵΩw −w∗,vΩw〉 ≥ ‖ŵΩ̄w‖1 − ‖ŵΩw −w∗‖1. (24)\nCombining (23) with (24), we have 〈 ŵ −w∗,∇g(w∗)−ARR⊤λ̃ 〉 + α\n2 ‖ŵ −w∗‖22 + γw‖ŵΩ̄w‖1 ≤ γw‖ŵΩw −w∗‖1. (25)\nFrom the fact that w∗ minimizes G(·) over the domain Ω, we have 〈∇G(w∗),w −w∗〉 = 〈∇g(w∗)−Aλ∗,w −w∗〉 ≥ 0, ∀w ∈ Ω. (26)\nThen, 〈 ŵ −w∗,∇g(w∗)−ARR⊤λ̃ 〉\n= 〈ŵ −w∗,∇g(w∗)−Aλ∗〉+ 〈 ŵ −w∗, A(I −RR⊤)λ∗ 〉 + 〈 ŵ −w∗, ARR⊤(λ∗ − λ̃) 〉\n(26) ≥ − ‖ŵ −w∗‖1 (∥∥∥A(I −RR⊤)λ∗ ∥∥∥ ∞ + ∥∥∥ARR⊤(λ∗ − λ̃) ∥∥∥ ∞ ) (17) = − ρw‖ŵ −w∗‖1 = −ρw ( ‖ŵΩ̄w‖1 + ‖ŵΩw −w∗‖1 ) .\n(27)\nFrom (25) and (27), we have\nα 2 ‖ŵ −w∗‖22 + (γw − ρw)‖ŵΩ̄w‖1 ≤ (γw + ρw)‖ŵΩw −w∗‖1.\nThe rest proof is identical to that of Lemma 1.\n3. In the case that g(·) is non-smooth, ∇g(w∗) refers to a subgradient of g(·) at w∗. In particular, we choose the subgradient that satisfies (26)."
    }, {
      "heading" : "5.5 Proof of Lemma 5",
      "text" : "We first upper bound ρw as\nρw ≤ ∥∥∥A(I −RR⊤)λ∗ ∥∥∥ ∞︸ ︷︷ ︸\n:=U1\n+ ∥∥∥A(λ∗ − λ̃) ∥∥∥ ∞︸ ︷︷ ︸\n:=U2\n+ ∥∥∥A(RR⊤ − I)(λ∗ − λ̃) ∥∥∥ ∞︸ ︷︷ ︸\n:=U3\n.\nBounding U1 From Lemma 6, we have with a probability at least 1− δ, ∣∣∣ [ A(I −RR⊤)λ∗\n] i ∣∣∣ = ∣∣∣Ai∗(I −RR⊤)λ∗ ∣∣∣ ≤ max i∈[d] ‖Ai∗‖2‖λ∗‖2 √ c m log 4 δ (5) ≤ ‖λ∗‖2 √ c m log 4 δ\nfor each i ∈ [d]. Taking the union bound over all i ∈ [d], we have with a probability at least 1− δ,\n∥∥∥A(I −RR⊤)λ∗ ∥∥∥ ∞ ≤ ‖λ∗‖2 √ c m log 4d δ .\nBounding U2 From our assumption, we have ∥∥∥A(λ∗ − λ̃)\n∥∥∥ ∞ ≤ max i∈[d] ‖Ai∗‖2‖λ∗ − λ̃‖2 (5) ≤ ‖λ∗ − λ̃‖2.\nBounding U3 Notice that the arguments for bounding U1 cannot be used to upper bound U3, that is because λ∗ − λ̃ is a random variable that depends on R and thus we cannot apply Lemma 6 directly. To overcome this challenge, we will exploit the fact that λ∗ − λ̃ is approximately sparse to decouple the dependence. Define\nKn,16rλ = {x ∈ Rn : ‖x‖2 ≤ 1, ‖x‖1 ≤ 4 √ rλ} .\nWhen the conclusion in Lemma 3 happens, we have\nλ̃− λ∗ ‖λ̃− λ∗‖2 ∈ Kn,16rλ (28)\nand thus\nU3 = ‖λ∗ − λ̃‖2 ∥∥∥∥∥A(RR ⊤ − I) λ∗ − λ̃ ‖λ∗ − λ̃‖2 ∥∥∥∥∥ ∞ (28) ≤ ‖λ∗ − λ̃‖2 sup z∈Kn,16rλ ∥∥∥A(RR⊤ − I)z ∥∥∥ ∞\n︸ ︷︷ ︸ :=U4\n.\nThen, we will utilize the techniques of covering number to provide an upper bound for U4. Lemma 7 With a probability at least 1− δ, we have\nsup z∈Kn,16rλ\n∥∥∥A(RR⊤ − I)z ∥∥∥ ∞ ≤ 2(2 + √ 2)\n√ c\nm\n( log 4d\nδ + 16rλ log\n9n\n8rλ\n) .\nPutting everything together, we have\nρw ≤‖λ∗‖2 √ c\nm log\n4d\nδ + ‖λ∗ − λ̃‖2\n( 1 + 2(2 + √ 2) √ c\nm\n( log 4d\nδ + 16rλ log\n9n\n8rλ\n))\n≤‖λ∗‖2 √ c\nm log\n4d\nδ +\n3γλ √ rλ\nβ\n( 1 + 7 √ c\nm\n( log 4d\nδ + 16rλ log\n9n\n8rλ\n)) ."
    }, {
      "heading" : "5.6 Proof of Lemma 6",
      "text" : "First, we assume ‖u‖2 = ‖v‖2 = 1. Following the proof of Corollary 2 in Arriaga and Vempala (2006), we apply Theorem 1 to vectors u+v and u−v. Then, with a probability at least 1− 4 exp(−mε2/c), we have\n(1− ε)‖u + v‖22 ≤ ‖R⊤(u+ v)‖22 ≤ (1 + ε)‖u + v‖22, (29) (1− ε)‖u − v‖22 ≤ ‖R⊤(u− v)‖22 ≤ (1 + ε)‖u − v‖22, (30)\nprovided ǫ ≤ 1/2. From (29) and (30), it is straightforward to show that ∣∣∣u⊤RR⊤v− u⊤v ∣∣∣ ≤ ε.\nThus, with a probability at least 1− δ, we have ∣∣∣u⊤RR⊤v− u⊤v ∣∣∣ ≤ √ c\nm log\n4\nδ\nprovided (9) holds.\nWe complete the proof by noticing\n∣∣∣u⊤RR⊤v− u⊤v ∣∣∣ = ‖u‖2‖v‖2 ∣∣∣∣ u⊤\n‖u‖2 RR⊤\nv ‖v‖2 − u\n⊤v\n‖u‖2‖v‖2\n∣∣∣∣ ."
    }, {
      "heading" : "5.7 Proof of Lemma 7",
      "text" : "First, we define Sn,16rλ = {x ∈ Rn : ‖x‖2 ≤ 1, ‖x‖0 ≤ 16rλ} . Using Lemma 3.1 from Plan and Vershynin (2013a), we have\nKn,16rλ ⊂ 2 conv(Sn,16rλ).\nand therefore\nU4 ≤ 2 sup z∈conv(Sn,16rλ )\n∥∥∥A(RR⊤ − I)z ∥∥∥ ∞\n= 2 sup z∈Sn,16rλ\n∥∥∥A(RR⊤ − I)z ∥∥∥ ∞\n︸ ︷︷ ︸ :=θ\n(31)\nwhere the last equality follows from the fact that the maximum of a convex function over a convex set generally occurs at some extreme point of the set (Rockafellar, 1997).\nLet Sn,s(ǫ) be a proper ǫ-net for Sn,s with the smallest cardinality, and |Sn,s(ǫ)| be the covering number for Sn,s. We have the following lemma for bounding |Sn,s(ǫ)|.\nLemma 8 (Plan and Vershynin, 2013a, Lemma 3.3) For ǫ ∈ (0, 1) and s ≤ n, we have\nlog |Sn,s(ǫ)| ≤ s log ( 9n\nǫs\n) .\nLet Sd,16rλ(ǫ) be a ǫ-net of Sd,16rλ with smallest cardinality. With the help of Sd,16rλ(ǫ), we define a discretized version of θ in (31) as\nθ(ǫ) = sup {∥∥∥A(RR⊤ − I)z ∥∥∥ ∞ : z ∈ Sn,16rλ(ǫ) } .\nThe following lemma relates θ with θ(ǫ). Lemma 9 (Koltchinskii, 2011, Lemma 9.2) For ǫ ∈ (0, 1/ √ 2), we have\nθ ≤ θ(ǫ) 1− √ 2ǫ .\nBy choosing ǫ = 1/2, we have θ ≤ (2 + √ 2)θ(1/2). Combining with (31), we obtain\nU4 ≤ 2(2 + √ 2) sup {∥∥∥A(RR⊤ − I)z ∥∥∥ ∞ : z ∈ Sn,16rλ(1/2) }\n︸ ︷︷ ︸ θ(1/2)\nFurthermore, Lemma 8 implies\nlog |Sn,16rλ(1/2)| ≤ 16rλ log ( 9n\n8rλ\n) .\nWe proceed by providing an upper bound for θ(1/2). Following the arguments for bounding U1 in the proof of Lemma 5, we have with a probability at least 1− δ,\n∥∥∥A ( RR⊤ − I ) z ∥∥∥ ∞ ≤ √ c m log 4d δ\nfor each z ∈ Sn,16rλ(1/2). We complete the proof by taking the union bound over all z ∈ Sn,16rλ(1/2)."
    }, {
      "heading" : "6. Conclusion and Future Work",
      "text" : "In this paper, a randomized algorithm is proposed to solve the convex-concave optimization problem in (3). Compared to previous studies, a distinctive feature of the proposed algorithm is that sparse regularization is introduced to control the damage cased by random projection. Under mild assumptions about the optimization problem, we demonstrate that it is able to accurately recover the optimal solutions to (3) provided they are sparse or approximately sparse.\nFrom the current analysis, we need to solve two different problems if our goal is to recover both w∗ and λ∗ accurately. It is unclear whether this is an artifact of the proof technique or actually unavoidable. We will investigate this issue in the future. Since the proposed algorithm is designed for the case that the optimal solutions are (approximately) sparse, it is practically important to develop a pre-precessing procedure that can estimate the sparsity of solutions before applying our algorithm. We plan to utilize random sampling to address this problem. Last but not least, we will investigate the empirical performance of the proposed algorithm."
    }, {
      "heading" : "Appendix A. Proof of Theorem 3",
      "text" : "The analysis here is similar to that for Lemma 1. Recall that in the proof of Theorem 2, we have proved that\nγλ ≥ 2‖A⊤w∗‖2 √ c\nm log\n4n\nδ ≥ 2 ∥∥∥(RR⊤ − I)A⊤w∗ ∥∥∥ ∞\n(32)\nholds with a probability at least 1− δ. Define L̂(λ) = −h(λ)− ŵ⊤ÂR⊤λ− γλ‖λ‖1. Using the fact that λ̂ maximizes L̂(·) over the domain ∆ and h(·) is β-strongly convex, we have\n〈 λ̂− λ∗,∇h(λ∗) +RR⊤A⊤ŵ 〉 + β\n2 ‖λ∗ − λ̂‖22 + γλ‖λ̂Ω̄λ‖1 ≤ γλ‖λ̂Ωλ − λ∗‖1. (33)\nOn the other hand, we have 〈 λ̂− λ∗,∇h(λ∗) +RR⊤A⊤ŵ 〉\n=〈λ̂− λ∗,∇h(λ∗) +A⊤w∗〉+ 〈 λ̂− λ∗, (RR⊤ − I)A⊤w∗ 〉 + 〈 λ̂− λ∗, RR⊤A⊤(ŵ −w∗) 〉\n(21) ≥ − ‖λ̂− λ∗‖1 ∥∥∥(RR⊤ − I)A⊤w∗ ∥∥∥ ∞ − ‖λ̂− λ∗‖2 ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2 (32)\n≥ − γλ 2 ‖λ̂− λ∗‖1 − ‖λ̂− λ∗‖2 ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2 .\n(34)\nFrom (33) and (34), we have\nβ 2 ‖λ∗ − λ̂‖22 + γλ 2 ‖λ̂Ω̄λ‖1\n≤3γλ 2\n‖λ̃Ωλ − λ∗‖1 + ‖λ̂− λ∗‖2 ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2\n≤3γλ √ rλ\n2 ‖λ̃Ωλ − λ∗‖2 + ‖λ̂− λ∗‖2 ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2\n≤‖λ̂− λ∗‖2 ( 3γλ √ rλ 2 + ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2 )\nwhich implies\n‖λ∗ − λ̂‖2\n≤ 2 β\n( 3γλ √ rλ\n2 + ∥∥∥RR⊤A⊤(ŵ −w∗) ∥∥∥ 2\n)\n≤ 2 β\n( 3γλ √ rλ\n2 + ‖A⊤(ŵ −w∗)‖2 + ∥∥∥(RR⊤ − I)A⊤(ŵ −w∗) ∥∥∥ 2\n)\n≤ 2 β\n( 3γλ √ rλ\n2 +\n( 1 + ‖RR⊤ − I‖2 ) ‖A⊤(ŵ −w∗)‖2 ) ."
    }, {
      "heading" : "Appendix B. Proof of Theorem 5",
      "text" : "The proof is almost identical to that of Theorem 2. We just need to replace Lemmas (1) and (4) with the following ones.\nLemma 10 Denote\nρλ = ∥∥∥(RR⊤ − I)A⊤w∗ ∥∥∥ ∞ + ς. (35)"
    }, {
      "heading" : "By choosing γλ ≥ 2ρλ, then the conclusion of Lemma (1) still holds.",
      "text" : "Lemma 11 Denote\nρw = ∥∥∥A ( I −RR⊤ ) λ∗ ∥∥∥ ∞ + ∥∥∥ARR⊤(λ∗ − λ̃) ∥∥∥ ∞ + ς. (36)"
    }, {
      "heading" : "By choosing γw ≥ 2ρw, then the conclusion of Lemma (4) still holds.",
      "text" : ""
    }, {
      "heading" : "Appendix C. Proof of Lemma 10",
      "text" : "From the assumption, we have\n〈 λ̃− λ∗,∇h(λ∗) +RR⊤A⊤w∗ 〉\n= 〈 λ̃− λ∗,∇h(λ∗) +A⊤w∗ 〉 + 〈 λ̃− λ∗, (RR⊤ − I)A⊤w∗ 〉\n(12) ≥ − ‖λ̃− λ∗‖1 (∥∥∥(RR⊤ − I)A⊤w∗ ∥∥∥ ∞ + ς ) (35) = − ρλ‖λ̃− λ∗‖1 = −ρλ ( ‖λ̃Ω̄λ‖1 + ‖λ̃Ωλ − λ∗‖1 ) .\nSubstituting the above inequality into (20), and the rest proof is identical to that of Lemma 1."
    }, {
      "heading" : "Appendix D. Proof of Lemma 11",
      "text" : "Similarly, we have\n〈 ŵ −w∗,∇g(w∗)−ARR⊤λ̃ 〉\n= 〈ŵ −w∗,∇g(w∗)−Aλ∗〉+ 〈 ŵ −w∗, A(I −RR⊤)λ∗ 〉 + 〈 ŵ −w∗, ARR⊤(λ∗ − λ̃) 〉\n(11) ≥ − ‖ŵ −w∗‖1 (∥∥∥A(I −RR⊤)λ∗ ∥∥∥ ∞ + ∥∥∥ARR⊤(λ∗ − λ̃) ∥∥∥ ∞ + ς ) (36) = − ρw‖ŵ −w∗‖1 = −ρw ( ‖ŵΩ̄w‖1 + ‖ŵΩw −w∗‖1 ) .\nSubstituting the above inequality into (25), and the rest proof is identical to that of Lemma 4."
    }, {
      "heading" : "Appendix E. Proof of Theorem 6",
      "text" : "The proof is almost identical to that of Theorem 2. We just need to replace Lemmas (1) and (4) with the following ones.\nLemma 12 Denote\nρλ = ∥∥∥(RR⊤ − I)A⊤w∗ ∥∥∥ ∞ + (1 + µ)τ. (37)\nBy choosing γλ ≥ 2ρλ, we have\n‖λ̃− λ∗‖2 ≤ 3γλ\n√ rλ β , ‖λ̃− λ∗‖1 ≤ 12γλrλ β , and ‖λ̃− λ∗‖1 ‖λ̃− λ∗‖2 ≤ 4√rλ.\nLemma 13 Denote\nρw = ∥∥∥A ( I −RR⊤ ) λ∗ ∥∥∥ ∞ + ∥∥∥ARR⊤(λ∗ − λ̃) ∥∥∥ ∞ + (1 + µ)τ. (38)\nBy choosing γw ≥ 2ρw, we have\n‖ŵ −w∗‖2 ≤ 3γw\n√ rw\nα , ‖ŵ −w∗‖1 ≤ 12γwrw α , and ‖ŵ −w∗‖1 ‖ŵ −w∗‖2 ≤ 4√rw."
    }, {
      "heading" : "Appendix F. Proof of Lemma 12",
      "text" : "Recall the definition of L(·) in Section 5.2. From the fact that the optimal solution λ′∗ lies in the interior of ∆, we have\n∇L(λ′∗) = −∇h(λ′∗)−A⊤w′∗ = 0. (39)\nThen,\n〈 λ̃− λ∗,∇h(λ∗) +RR⊤A⊤w∗ 〉\n= 〈 λ̃− λ∗,∇h(λ′∗) +A⊤w′∗ 〉 + 〈 λ̃− λ∗,∇h(λ∗)−∇h(λ′∗) 〉\n+ 〈 λ̃− λ∗, A⊤w∗ −A⊤w′∗ 〉 + 〈 λ̃− λ∗, (RR⊤ − I)A⊤w∗ 〉\n(39) ≥ − ‖λ̃− λ∗‖1 ( ‖∇h(λ∗)−∇h(λ′∗)‖∞ + ∥∥∥A⊤w∗ −A⊤w′∗ ∥∥∥ ∞ + ∥∥∥(RR⊤ − I)A⊤w∗ ∥∥∥ ∞ ) .\n(40)\nFrom our assumptions, we have\n‖∇h(λ∗)−∇h(λ′∗)‖∞ ≤ ‖∇h(λ∗)−∇h(λ′∗)‖2 (15) ≤ µ‖λ∗ − λ′∗‖2 (13)\n≤ µτ, (41) ∥∥∥A⊤(w∗ −w′∗) ∥∥∥ ∞ ≤ max j∈[n] ‖A∗j‖2‖w∗ −w′∗‖2 (6) ≤ ‖w∗ −w′∗‖2 (13) ≤ τ. (42)\nFrom (40), (41) and (42), we have\n〈 λ̃− λ∗,∇h(λ∗) +RR⊤A⊤w∗ 〉 ≥− ‖λ̃− λ∗‖1 (∥∥∥(RR⊤ − I)A⊤w∗\n∥∥∥ ∞ + (1 + µ)τ )\n(37) = − ρλ‖λ̃− λ∗‖1 = −ρλ ( ‖λ̃Ω̄λ‖1 + ‖λ̃Ωλ − λ∗‖1 ) .\nSubstituting the above inequality into (20), and the rest proof is identical to that of Lemma 1."
    }, {
      "heading" : "Appendix G. Proof of Lemma 13",
      "text" : "Recall the definition of G(·) in Section 5.4. From the fact that the optimal solution w′∗ lies in the interior of Ω, we have\n∇G(w′∗) = ∇g(w′∗)−Aλ′∗ = 0. (43)\nThen,\n〈 ŵ −w∗,∇g(w∗)−ARR⊤λ̃ 〉\n= 〈 ŵ −w∗,∇g(w′∗)−Aλ′∗ 〉 + 〈ŵ −w∗,∇g(w∗)−∇g(w′∗)〉+ 〈 ŵ −w∗, Aλ′∗ −Aλ∗ 〉\n+ 〈 ŵ −w∗, A(I −RR⊤)λ∗ 〉 + 〈 ŵ −w∗, ARR⊤(λ∗ − λ̃) 〉\n(43) ≥ − ‖ŵ −w∗‖1 (∥∥∇g(w∗)−∇g(w′∗) ∥∥ ∞ + ∥∥Aλ′∗ −Aλ∗ ∥∥ ∞ )\n− ‖ŵ −w∗‖1 (∥∥∥A(I −RR⊤)λ∗ ∥∥∥ ∞ + ∥∥∥ARR⊤(λ∗ − λ̃) ∥∥∥ ∞ ) .\n(44)\nFrom our assumptions, we have\n‖∇g(w∗)−∇g(w′∗)‖∞ ≤ ‖∇g(w∗)−∇g(w′∗)‖2 (14) ≤ µ‖w∗ −w′∗‖2 (13) ≤ µτ, (45)\n‖A(λ′∗ − λ∗)‖∞ ≤ max i∈[d]\n‖Ai∗‖2‖λ′∗ − λ∗‖2 (5) ≤ ‖λ′∗ − λ∗‖2 (13) ≤ τ. (46)\nFrom (44), (45) and (46), we have\n〈 ŵ −w∗,∇g(w∗)−ARR⊤λ̃ 〉\n≥− ‖ŵ −w∗‖1 (∥∥∥A(I −RR⊤)λ∗ ∥∥∥ ∞ + ∥∥∥ARR⊤(λ∗ − λ̃) ∥∥∥ ∞ + (1 + µ)τ )\n(38) = − ρw‖ŵ −w∗‖1 = −ρw ( ‖ŵΩ̄w‖1 + ‖ŵΩw −w∗‖1 ) .\nSubstituting the above inequality into (25), and the rest proof is identical to that of Lemma 4."
    } ],
    "references" : [ {
      "title" : "Database-friendly random projections: Johnson-lindenstrauss with binary coins",
      "author" : [ "Dimitris Achlioptas" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Achlioptas.,? \\Q2003\\E",
      "shortCiteRegEx" : "Achlioptas.",
      "year" : 2003
    }, {
      "title" : "An algorithmic theory of learning: Robust concepts and random projection",
      "author" : [ "Rosa I. Arriaga", "Santosh Vempala" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Arriaga and Vempala.,? \\Q2006\\E",
      "shortCiteRegEx" : "Arriaga and Vempala.",
      "year" : 2006
    }, {
      "title" : "Optimization with sparsity-inducing penalties",
      "author" : [ "Francis Bach", "Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bach et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2012
    }, {
      "title" : "Kernels as features: On kernels, margins, and low-dimensional mappings",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2006
    }, {
      "title" : "Random projection in dimensionality reduction: applications to image and text data",
      "author" : [ "Ella Bingham", "Heikki Mannila" ],
      "venue" : "In Proceedings of the 7th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Bingham and Mannila.,? \\Q2001\\E",
      "shortCiteRegEx" : "Bingham and Mannila.",
      "year" : 2001
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen Boyd", "Lieven Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2004
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "An elementary proof of a theorem of johnson and lindenstrauss",
      "author" : [ "Sanjoy Dasgupta", "Anupam Gupta" ],
      "venue" : "Random Structures & Algorithms,",
      "citeRegEx" : "Dasgupta and Gupta.,? \\Q2003\\E",
      "shortCiteRegEx" : "Dasgupta and Gupta.",
      "year" : 2003
    }, {
      "title" : "The Elements of Statistical Learning",
      "author" : [ "Trevor Hastie", "Robert Tibshirani", "Jerome Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2009
    }, {
      "title" : "An accelerated hpe-type algorithm for a class of composite convex-concave saddle-point problems",
      "author" : [ "Yunlong He", "Renato D.C. Monteiro" ],
      "venue" : "Technical report, Georgia Institute of Technology,,",
      "citeRegEx" : "He and Monteiro.,? \\Q2014\\E",
      "shortCiteRegEx" : "He and Monteiro.",
      "year" : 2014
    }, {
      "title" : "On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization",
      "author" : [ "Sham M. Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari" ],
      "venue" : "Technical report, Toyota Technological Institute at Chicago,",
      "citeRegEx" : "Kakade et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2009
    }, {
      "title" : "Dimensionality reduction by random mapping: fast similarity computation for clustering",
      "author" : [ "Samuel Kaski" ],
      "venue" : "In Proceedings of the 1998 IEEE International Joint Conference on Neural Networks,",
      "citeRegEx" : "Kaski.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kaski.",
      "year" : 1998
    }, {
      "title" : "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems",
      "author" : [ "Vladimir Koltchinskii" ],
      "venue" : null,
      "citeRegEx" : "Koltchinskii.,? \\Q2011\\E",
      "shortCiteRegEx" : "Koltchinskii.",
      "year" : 2011
    }, {
      "title" : "Dimensionality reductions that preserve volumes and distance to affine spaces, and their algorithmic applications",
      "author" : [ "Avner Magen" ],
      "venue" : "In Randomization and Approximation Techniques in Computer Science,",
      "citeRegEx" : "Magen.,? \\Q2002\\E",
      "shortCiteRegEx" : "Magen.",
      "year" : 2002
    }, {
      "title" : "Randomized algorithms for matrices and data",
      "author" : [ "Michael W. Mahoney" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Mahoney.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mahoney.",
      "year" : 2011
    }, {
      "title" : "Uniform uncertainty principle for bernoulli and subgaussian ensembles",
      "author" : [ "Shahar Mendelson", "Alain Pajor", "Nicole Tomczak-Jaegermann" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "Mendelson et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mendelson et al\\.",
      "year" : 2008
    }, {
      "title" : "Prox-method with rate of convergence O(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems",
      "author" : [ "Arkadi Nemirovski" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Nemirovski.,? \\Q2005\\E",
      "shortCiteRegEx" : "Nemirovski.",
      "year" : 2005
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Yu. Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Nesterov.,? \\Q2005\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2005
    }, {
      "title" : "Random projections for support vector machines",
      "author" : [ "Saurabh Paul", "Christos Boutsidis", "Malik Magdon-Ismail", "Petros Drineas" ],
      "venue" : "In Proceedings of the 16th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Paul et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Paul et al\\.",
      "year" : 2013
    }, {
      "title" : "One-bit compressed sensing by linear programming",
      "author" : [ "Yaniv Plan", "Roman Vershynin" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "Plan and Vershynin.,? \\Q2013\\E",
      "shortCiteRegEx" : "Plan and Vershynin.",
      "year" : 2013
    }, {
      "title" : "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach",
      "author" : [ "Yaniv Plan", "Roman Vershynin" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Plan and Vershynin.,? \\Q2013\\E",
      "shortCiteRegEx" : "Plan and Vershynin.",
      "year" : 2013
    }, {
      "title" : "Convex Analysis",
      "author" : [ "Ralph Tyrell Rockafellar" ],
      "venue" : null,
      "citeRegEx" : "Rockafellar.,? \\Q1997\\E",
      "shortCiteRegEx" : "Rockafellar.",
      "year" : 1997
    }, {
      "title" : "Is margin preserved after random projection",
      "author" : [ "Qinfeng Shi", "Chunhua Shen", "Rhys Hill", "Anton van den Hengel" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Shi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2012
    }, {
      "title" : "Fast rates for regularized objectives",
      "author" : [ "Karthik Sridharan", "Shai Shalev-shwartz", "Nathan Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sridharan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sridharan et al\\.",
      "year" : 2009
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "Joel A. Tropp" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Tropp.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tropp.",
      "year" : 2012
    }, {
      "title" : "Svm soft margin classifiers: Linear programming versus quadratic programming",
      "author" : [ "Qiang Wu", "Ding-Xuan Zhou" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Wu and Zhou.,? \\Q2005\\E",
      "shortCiteRegEx" : "Wu and Zhou.",
      "year" : 2005
    }, {
      "title" : "Theory of dual-sparse regularized randomized reduction",
      "author" : [ "Tianbao Yang", "Lijun Zhang", "Rong Jin", "Shenghuo Zhu" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Recovering the optimal solution by dual random projection",
      "author" : [ "Lijun Zhang", "Mehrdad Mahdavi", "Rong Jin", "Tianbao Yang", "Shenghuo Zhu" ],
      "venue" : "In Proceedings of the 26th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "Hui Zou", "Trevor Hastie" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Statistical Methodology),",
      "citeRegEx" : "Zou and Hastie.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zou and Hastie.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Introduction Learning the sparse representation of a predictive model has received considerable attention in recent years (Bach et al., 2012).",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "where l(·) is a convex function such as the logistic loss to measure the empirical error, and ψ(·) is a sparsity-inducing regularizer such as the elastic net (Zou and Hastie, 2005) to",
      "startOffset" : 158,
      "endOffset" : 180
    }, {
      "referenceID" : 8,
      "context" : "avoid overfitting (Hastie et al., 2009).",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 21,
      "context" : "where l∗(·) is the Fenchel conjugate of l(·) (Rockafellar, 1997) and Γ is the domain of the dual variable, we get the following convex-concave formulation:",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "The problem in (3) has been widely studied in the optimization community, and when n and d are medium size, it can be solved iteratively by gradient based methods (Nesterov, 2005; Nemirovski, 2005).",
      "startOffset" : 163,
      "endOffset" : 197
    }, {
      "referenceID" : 16,
      "context" : "The problem in (3) has been widely studied in the optimization community, and when n and d are medium size, it can be solved iteratively by gradient based methods (Nesterov, 2005; Nemirovski, 2005).",
      "startOffset" : 163,
      "endOffset" : 197
    }, {
      "referenceID" : 11,
      "context" : "Related Work Random projection has been widely used as an efficient algorithm for dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001).",
      "startOffset" : 107,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "Related Work Random projection has been widely used as an efficient algorithm for dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001).",
      "startOffset" : 107,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "In the case of unsupervised learning, it has been proved that random projection is able to preserve the distance (Dasgupta and Gupta, 2003), inner product (Arriaga and Vempala, 2006), volumes and distance to affine spaces (Magen, 2002).",
      "startOffset" : 113,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "In the case of unsupervised learning, it has been proved that random projection is able to preserve the distance (Dasgupta and Gupta, 2003), inner product (Arriaga and Vempala, 2006), volumes and distance to affine spaces (Magen, 2002).",
      "startOffset" : 155,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "In the case of unsupervised learning, it has been proved that random projection is able to preserve the distance (Dasgupta and Gupta, 2003), inner product (Arriaga and Vempala, 2006), volumes and distance to affine spaces (Magen, 2002).",
      "startOffset" : 222,
      "endOffset" : 235
    }, {
      "referenceID" : 3,
      "context" : "For classification, theoretical studies mainly focus on examining the generalization error or the preservation of classification margin in the low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013).",
      "startOffset" : 165,
      "endOffset" : 223
    }, {
      "referenceID" : 22,
      "context" : "For classification, theoretical studies mainly focus on examining the generalization error or the preservation of classification margin in the low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013).",
      "startOffset" : 165,
      "endOffset" : 223
    }, {
      "referenceID" : 18,
      "context" : "For classification, theoretical studies mainly focus on examining the generalization error or the preservation of classification margin in the low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013).",
      "startOffset" : 165,
      "endOffset" : 223
    }, {
      "referenceID" : 14,
      "context" : "For regression, there do exist theoretical guarantees for the recovery error, but they only hold for the least squares problem (Mahoney, 2011).",
      "startOffset" : 127,
      "endOffset" : 142
    }, {
      "referenceID" : 27,
      "context" : "Our work is closely related to Dual Random Projection (DRP) (Zhang et al., 2013) and Dual-sparse Regularized Randomized Reduction (DSRR) (Yang et al.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 26,
      "context" : ", 2013) and Dual-sparse Regularized Randomized Reduction (DSRR) (Yang et al., 2015), which also investigate random projection from the perspective of optimization.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "The optimization problem in (4) can be solved by the algorithm designed for composite convex-concave problems (He and Monteiro, 2014).",
      "startOffset" : 110,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "Compared to previous randomized algorithms (Balcan et al., 2006; Zhang et al., 2013; Yang et al., 2015), (4) has two new features: i) the optimization is still performed in the original space; and ii) the l1-norm is introduced to regularize both primal and dual solutions.",
      "startOffset" : 43,
      "endOffset" : 103
    }, {
      "referenceID" : 27,
      "context" : "Compared to previous randomized algorithms (Balcan et al., 2006; Zhang et al., 2013; Yang et al., 2015), (4) has two new features: i) the optimization is still performed in the original space; and ii) the l1-norm is introduced to regularize both primal and dual solutions.",
      "startOffset" : 43,
      "endOffset" : 103
    }, {
      "referenceID" : 26,
      "context" : "Compared to previous randomized algorithms (Balcan et al., 2006; Zhang et al., 2013; Yang et al., 2015), (4) has two new features: i) the optimization is still performed in the original space; and ii) the l1-norm is introduced to regularize both primal and dual solutions.",
      "startOffset" : 43,
      "endOffset" : 103
    }, {
      "referenceID" : 21,
      "context" : ", the logistic loss), then its convex conjugate l∗(·) will be strongly convex (Rockafellar, 1997; Kakade et al., 2009).",
      "startOffset" : 78,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : ", the logistic loss), then its convex conjugate l∗(·) will be strongly convex (Rockafellar, 1997; Kakade et al., 2009).",
      "startOffset" : 78,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "The above theorem is widely used to prove the famous Johnson–Lindenstrauss lemma (Dasgupta and Gupta, 2003).",
      "startOffset" : 81,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "Previous studies (Achlioptas, 2003; Arriaga and Vempala, 2006) have proved that Theorem 1 is true if {Sij} are independent random variables sampled from the Gaussian distribution N (0, 1), uniform distribution over {±1}, or the following database-friendly distribution",
      "startOffset" : 17,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "Previous studies (Achlioptas, 2003; Arriaga and Vempala, 2006) have proved that Theorem 1 is true if {Sij} are independent random variables sampled from the Gaussian distribution N (0, 1), uniform distribution over {±1}, or the following database-friendly distribution",
      "startOffset" : 17,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : "More generally, a sufficient condition for Theorem 1 is that columns of R are independent, isotropic, and subgaussian vectors (Mendelson et al., 2008).",
      "startOffset" : 126,
      "endOffset" : 150
    }, {
      "referenceID" : 25,
      "context" : "According to the theoretical analysis of regularized empirical risk minimization (Wu and Zhou, 2005; Sridharan et al., 2009; Koltchinskii, 2011), the optimal γ, that minimizes the generalization error, can be chosen as γ = O(1/ √ n), and thus α = O(γn) = O( √ n).",
      "startOffset" : 81,
      "endOffset" : 144
    }, {
      "referenceID" : 23,
      "context" : "According to the theoretical analysis of regularized empirical risk minimization (Wu and Zhou, 2005; Sridharan et al., 2009; Koltchinskii, 2011), the optimal γ, that minimizes the generalization error, can be chosen as γ = O(1/ √ n), and thus α = O(γn) = O( √ n).",
      "startOffset" : 81,
      "endOffset" : 144
    }, {
      "referenceID" : 12,
      "context" : "According to the theoretical analysis of regularized empirical risk minimization (Wu and Zhou, 2005; Sridharan et al., 2009; Koltchinskii, 2011), the optimal γ, that minimizes the generalization error, can be chosen as γ = O(1/ √ n), and thus α = O(γn) = O( √ n).",
      "startOffset" : 81,
      "endOffset" : 144
    }, {
      "referenceID" : 24,
      "context" : "The upper bound in the above theorem is quite loose, because ‖RR⊤ − I‖2 is roughly on the order of n log n/m (Tropp, 2012).",
      "startOffset" : 109,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "The above conditions can be considered as sub-optimality conditions (Boyd and Vandenberghe, 2004) of w∗ and λ∗ measured in the l∞-norm.",
      "startOffset" : 68,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "From the property that R preserves the l2-norm, it is easy to verify that it also preserves the inner product (Arriaga and Vempala, 2006).",
      "startOffset" : 110,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "Following the proof of Corollary 2 in Arriaga and Vempala (2006), we apply Theorem 1 to vectors u+v and u−v.",
      "startOffset" : 38,
      "endOffset" : 65
    }, {
      "referenceID" : 19,
      "context" : "1 from Plan and Vershynin (2013a), we have",
      "startOffset" : 7,
      "endOffset" : 34
    }, {
      "referenceID" : 21,
      "context" : "where the last equality follows from the fact that the maximum of a convex function over a convex set generally occurs at some extreme point of the set (Rockafellar, 1997).",
      "startOffset" : 152,
      "endOffset" : 171
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we develop a randomized algorithm and theory for learning a sparse model from large-scale and high-dimensional data, which is usually formulated as an empirical risk minimization problem with a sparsity-inducing regularizer. Under the assumption that there exists a (approximately) sparse solution with high classification accuracy, we argue that the dual solution is also sparse or approximately sparse. The fact that both primal and dual solutions are sparse motivates us to develop a randomized approach for a general convex-concave optimization problem. Specifically, the proposed approach combines the strength of random projection with that of sparse learning: it utilizes random projection to reduce the dimensionality, and introduces l1-norm regularization to alleviate the approximation error caused by random projection. Theoretical analysis shows that under favored conditions, the randomized algorithm can accurately recover the optimal solutions to the convex-concave optimization problem (i.e., recover both the primal and dual solutions). Furthermore, the solutions returned by our algorithm are guaranteed to be approximately sparse.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}