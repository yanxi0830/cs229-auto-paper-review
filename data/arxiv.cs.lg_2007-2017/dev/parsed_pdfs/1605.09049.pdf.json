{
  "name" : "1605.09049.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Recycling Randomness with Structure for Sublinear time Kernel Expansions",
    "authors" : [ "Krzysztof Choromanski", "Vikas Sindhwani" ],
    "emails" : [ "KCHORO@GOOGLE.COM", "SINDHWANI@GOOGLE.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Consider a k-dimensional feature map of the form,\nΨ(x) = 1√ k s(Mx) (1)\nwhere the input data vector x is drawn from Rn, s(·) denotes a real-valued or complex-valued pointwise nonlinearity (activation function), and M is a k × n Gaussian random matrix. It is well known that as a function of a pair of data vectors, the Euclidean inner product Ψ(x)TΨ(z), converges to a positive definite kernel function K(x, z) depending on the choice of the scalar nonlinearity, as k → ∞. For example, the complex exponential nonlinearity s(x) = e−i x σ corresponds to the Gaussian ker-\nnel (Rahimi & Recht, 2007), while the rectified linear function (ReLU), s(x) = max(x, 0), leads to the Arc-cosine kernel (Cho & Saul, 2009).\nIn recent years, such random feature maps have been used to dramatically accelerate the training time and inference speed of kernel methods (Schölkopf & Smola, 2002) across a variety of statistical modeling problems (Rahimi & Recht, 2007; Xie et al., 2015) and applications (Huang et al., 2014; Vedaldi & Zisserman, 2012). Standard linear techniques applied to random nonlinear embeddings of data are equivalent to learning with approximate kernels. To quantify the benefits, consider solving a kernel ridge regression task given l training examples. With traditional kernel methods, dense linear algebra operations on the Gram matrix associated with the exact kernel function imply that the training complexity grows as O(l3 + l2n) and the time to make a prediction on a test sample grows as O(ln). By contrast, random feature approximations reduce training complexity to O(lk2 + lkn) and test speed to O(kn). This is a major win on big datasets where l is very large, provided that a small value of k can provide a good approximation to the kernel function.\nIn practice, though, the optimal value of k is often large, albeit still much smaller than l. For example, in a speech recognition application (Huang et al., 2014) involving around two million training examples, about hundred thousand random features are required to achieve state of the art results. In such settings, the time to construct the random feature map is dominated by matrix multiplication against the dense Gaussian random matrix, which becomes the new computational bottleneck. To alleviate this bottleneck, (Le et al., 2013) introduce the “Fastfood” approach where Gaussian random matrices are replaced by Hadamard matrices combined with diagonal matrices with Gaussian distributed diagonal entries. It was shown in (Le et al., 2013) that for the specific case of the complex exponential nonlinearity, the Fastfood feature maps provide unbiased estimates for the Gaussian kernel function, at the expense of additional statistical variance, but with the computational benefit of reducing the feature map construction time from O(kn) to O(k log n) by using the Fast Walsh-Hadamard transform for matrix multiplication. The Fastfood construction for kernel approximations is akin to ar X iv :1 60 5.\n09 04\n9v 1\n[ cs\n.L G\n] 2\n9 M\nay 2\n01 6\nthe use of structured matrices - in lieu of Gaussian random matrices - in Fast Johnson-Lindenstrauss transform (FJLT) (Alon & Chazelle, 2009) for dimensionality reduction, fast compressed sensing (Bajwa et al., 2007; Rauhut et al., 2012), and randomized numerical linear algebra techniques (Alon & Chazelle, 2011; Mahoney, 2011) Specific structured matrices were recently applied for approximating angular kernels (Choromanska et al., 2016). Some heuristic results for approximating kernels with circulant matrices were given in (Yu et al., 2015).\nOur contributions in this paper are as follows:\n• We study a general family of structured random matrices that can be constructed by recycling a Gaussian random vector using a sequence of elementary generator matrices (introduced in Section 3). This family includes Circulant, Toeplitz and Hankel matrices. It also includes the Fastfood construction of (Le et al., 2013) as a special case. We show that fast sublinear time random feature maps obtained from these matrices provide unbiased estimates of the exact kernel, with variance comparable to the fully unstructured Gaussian case (Section 4). We introduce various structural coherence and graph-theoretic constants that control the quality of randomness we get from our model. Our approach generalizes across various choices of nonlinearities and kernel functions. • Of particular interest for us is the class of generalized structured matrices that have low-displacement rank (Pan, 2001; Sindhwani et al., 2015). Such matrices span an increasingly rich class of structures as the displacement rank is increased: from Circulant and Toeplitz matrices, to inverses and products of Toeplitz matrices, and more. The displacement rank provides a knob with which the degree of structure and randomness can be controlled to tradeoff computational and storage requirements against statistical variance. • We provide empirical support for our theoretical results (Section 5). In particular, we show that Circulant, Fastfood and low-displacement Toeplitz-like matrices provide high quality sublinear-time feature maps for approximating various kernels. With increasing displacement rank, the quality of the approximation approaches that of the fully Gaussian random matrix."
    }, {
      "heading" : "2. Background and Preliminaries",
      "text" : "We start by giving a brisk background on random feature maps and structured matrices."
    }, {
      "heading" : "2.1. Random Embeddings, Nonlinearities and Kernels",
      "text" : "Random feature maps may be viewed as arising from Monte-Carlo approximations to integral representations of\nkernel functions. The original construction by Rahimi & Recht (2007) was motivated by a classical result that characterizes the class of shift-invariant positive definite functions.\nTheorem 2.1 (Bochner’s Theorem (Bochner, 1933)). A continuous shift-invariant scaled kernel function K(x, z) ≡ φ(x − z) on Rn is positive definite if and only if it is the Fourier transform of a unique finite probability measure p on Rn. That is, for any x, z ∈ Rd,\nK(x, z) = ∫ Rn e−i(x−z) Twp(w)dw = Ew∼p[e−i(x−z) Tw] .\nBochner’s theorem stablishes one-to-one correspondence between shift-invariant kernel functions and probability densities on Rn, via the Fourier transform. In the case of the Gaussian kernel with bandwidth σ, the associated density is also Gaussian with covariance matrix σ−2 times the identity.\nWhile studying synergies between kernel methods and deep learning, (Cho & Saul, 2009) introduce bth-order arccosine kernels via the following integral representation:\nKb(x, z) = ∫ Rd i(wTx)i(wT z)(wTx)b(wT z)b p(w)dw\nwhere i(·) is the step function, i.e. i(x) = 1 if x > 0 and 0 otherwise; and the density p is chosen to be standard Gaussian. These kernels evaluate inner products in the representation induced by an infinitely wide single hidden layer neural network with random Gaussian weights, and admit closed form expressions in terms of the angle θ = cos−1( x\nT z ‖x‖2‖z‖2 ) between x and z:\nK0(x, z) = 1− θ\nπ (2)\nK1(x, z) = ‖x‖2‖z‖2\nπ [sin(θ) + (π − θ)cos(θ)] (3)\nwhere ‖ · ‖2 denotes l2 norm.\nMonte Carlo approximations to the integral representations above lead to the following,\nK(x, z) ≈ 1 k k∑ j=1 s(xTwj)s(z Twj) = Ψ(x) TΨ(z) (4)\nwhere the feature map Ψ(x) has the form given in Eqn. 1, with rows of M, i.e. wj vectors, drawn from the Gaussian density, and the nonlinearity s set to the following: complex exponential, s(x) = ei x σ , for the Gaussian kernel with bandwidth σ; hard-thresholding, s(x) = i(x), for the angular similarity kernel in Eqn. 2; and ReLU activation, s(x) = max(x, 0), for the first order arc-cosine kernel in Eqn. 3."
    }, {
      "heading" : "2.2. Structured Matrices",
      "text" : "A m × n matrix is called a structured matrix if it satisfies the following two properties: (1) it has much fewer degrees of freedom than mn independent entries, and hence can be implicitly stored more efficiently than general matrices, and (2) the structure in the matrix can be exploited for fast linear algebra operations such as fast matrix-vector multiplication. Examples include the Discrete Fourier Transform (DFT), the Discrete Cosine Transform (DCT) and the Walsh-Hadamard Transform (WHT) matrices. Here, we give other examples particularly relevant to this paper. The matrices described below are square. Rectangular matrices can be obtained by appropriately selecting rows or columns.\nCirculant Matrices: These matrices are intimately associated with circular convolutions and have been used for fast compressed sensing in (Rauhut et al., 2012). A n × n Circulant matrix is completely determined by its first column/row, i.e., n parameters. Each column/row of a Circulant matrix is generated by cyclically down/right-shifting the previous column/row. A skew-Circulant matrix has identical structure to Circulant, except that the upper triangular part of the matrix is negated. This general structure looks like, g0 fgn−1 . . . fg1 g1 g0 . . . ...\n... ... ... fgn−1 gn−1 . . . g1 g0  with f = 1 for Circulant and f = −1 for skew-Circulant\nmatrix. Both these matrices admit O(n log n) matrixvector multiplication as they are diagonalized by the DFT matrix (Pan, 2001). We will use the notation circ[g] and scirc[g] for Circulant and skew-Circulant matrices respectively.\nToeplitz and Hankel Matrices: These matrices implement discrete linear convolution and arise naturally in dynamical systems and time series analysis. Toeplitz matrices are characterized by constant diagonals as follows, t0 t−1 . . . t−(n−1) t1 t0 . . . ...\n... ... ... t−1 tn−1 . . . t1 t0  Closely related Hankel matrices have constant antidiagonals. Toeplitz-vector multiplication can be reduced to O(n log n) Circulant-vector multiplication. For detailed properties of Circulant and Toeplitz matrices, we point the reader to (Gray, 2006)\nStructured Matrices with Low-displacement Rank: The notion of displacement operators and displacement rank (Golub & Loan, 2012; Pan, 2001; Kailath et al.,\n1979) can be used to broadly generalize various classes of structured matrices. For example, under the action of the Sylvester displacement operator defined as L[T] = Z1T − TZ−1, every Toeplitz matrix can be transformed into a matrix of rank at most 2 using elementary shift and scale operations implemented by matrices of the form Zf = [e2e3 . . . en fe1] for f = ±1 where e1 . . . en are column vectors representing the standard basis of Rn.\nFor a given displacement rank parameter r, the class of matrices for which the rank of L[T] is at most r is called Toeplitz-like. Remarkably, this class of matrices admits a closed-form parameterization in terms of the low-rank factorization of L[T]:\nTheorem 2.2 (Parameterization of Toeplitz-like matrices with displacement rank r (Pan, 2001)). : If an n×nmatrix T satisfies rank(Z1T−TZ−1) ≤ r, then it can be written as,\nT = r∑ i=1 circ[gi] scirc[hi] (5)\nfor some choice of vectors {gi,hi}ri=1 ∈ Rn.\nThe family of matrices expressible by Eqn. 5 is very rich (Pan, 2001), i.e., it covers (i) all Circulant and Skewcirculant matrices for r = 1, (ii) all Toeplitz matrices and their inverses for r = 2, (iii) Products, inverses, linear combinations of distinct Toeplitz matrices with increasing r, and (iv) all n × n matrices for r = n. Since Toeplitz-like matrices under the parameterization of Eqn. 5 are a sum of products between Circulant and Skew-circulant matrices, they inherit fast FFT based matrix-vector multiplication with cost O(nrlog n), where r is the displacement rank. Hence, r provides a knob on the degree of structure imposed on the matrix with which storage requirements, computational constraints and statistical capacity can be explicitly controlled. Recently such matrices were used in the context of learning mobile-friendly neural networks in (Sindhwani et al., 2015). We note in passing that the displacement rank framework generalizes to other types of base structures (e.g. Vandermonde); see (Pan, 2001)."
    }, {
      "heading" : "2.3. FastFood",
      "text" : "In the context of fast kernel approximations, (Le et al., 2013) introduce the Fastfood technique where the matrix M in Eqn. 1 is parameterized by a product of diagonal and simple matrices as follows:\nF = 1√ n SHGPHB. (6)\nHere, S,G,B are diagonal random matrices, P is a permutation matrix and H is the Walsh-Hadamard matrix. The k × n matrix M is obtained by vertically stacking k/n independent copies of the n × n matrix F. Multiplication\nagainst such a matrix can be performed in timeO(k log n). The authors prove that (1) the Fastfood approximation is unbiased, (2) its variance is at most the variance of standard Gaussian random features with an additional O( 1k ) term, and (3) for a given error probability δ, the pointwise approximation error of a n×n block of Fastfood is at most O( √ log(n/δ)) larger than that of standard Gaussian random features. However, note that the Fastfood analysis is limited to the Gaussian kernel and their variance bound uses properties of the complex exponential. The authors also conjecture that the Hadamard matrix H above, can be replaced by any matrix T such that T/ √ n is orthonormal, the maximum entry in T is small, and matrix-vector product against T can be computed in O(n log n) time."
    }, {
      "heading" : "3. Structured Matrices from Gaussian Vectors",
      "text" : "In this section, we present a general structured matrix model that allows a small Gaussian vector to be recycled in order to mimic the properties of a Gaussian random matrix suitable for generating random features. We first introduce some basic concepts in our construction. Note that we emphasize intuitions in our exposition - formal proofs are provided in our supplementary material.\n3.1. The P-model\nBudget of Randomness: Let t be some given parameter. Consider the column vector g = (g1, ..., gt)T , where each entry is an independent Gaussian taken fromN (0, 1). This vector stands for the “budget of randomness” used in our structured matrix construction scheme.\nOur goal is to recycle the Gaussian vector g to construct random matrices with desirable properties. This is accomplished using a sequence of matrices which we call the Pmodel. Definition 3.1 (P-model). Given the budget of uncertainty parameter t, a sequence of m matrices with unit l2 norm columns, denoted as P = {Pi}mi=1, where Pi ∈ Rt×n, specifies a P-model. Such a sequence defines an m × n random matrix of the form:\nS[P] =  gTP1 gTP2\n... gTPm  (7) where g is a Gaussian random vector of length t.\nIn the constructions of interest to us, the sequence P is designed to separate structure from Gaussian randomness; though elements ofP can be deterministic or itself random, Gaussianity is restricted to the vector g. The ability of P to recycle a Gaussian vector effectively depends on certain structural constants that we now define.\nDefinition 3.2 (Coherence of a P-model). For P = {Pi}mi=1, let Pij denote the jth column of the ith matrix. The coherence of a P-model is defined as,\nµ[P] = max 1≤i≤j≤m\n√∑ 1≤n1<n2≤n(P T i,n1 Pj,n2) 2\nn (8)\nNote that µ[P] is a maximum over all pairs of rows 1 ≤ i ≤ j ≤ m of the rescaled sums of cross-correlations PTi,n1Pj,n2 for all pairs of different column indices n1, n2. Lower values of µ[P] will lead to better quality models. In practice, as we will see in subsequent analysis, it suffices if µ[P] = O(poly(log(n))) which is the case for instance for Toeplitz and Circulant matrices.\nThe coherence of the P-model is an extremal statistic of pairwise correlations. We couple it with another set of objects describing global structural properties of the model, namely the coherence graphs.\nDefinition 3.3 (Coherence Graphs for P-model and their Chromatic Numbers). Let 1 ≤ i, j ≤ m. We define by Gi,j an undirected graph with the set of vertices V (Gi,j) = {{n1, n2} : 1 ≤ n1 6= n2 ≤ n and PTi,n1Pj,n2 6= 0} and the set of edges E(Gi,j) = {{{n1, n2}, {n2, n3}} : {n1, n2}, {n2, n3} ∈ V (Gi,j)}. In other words, edges are between these vertices such that their corresponding 2-element subsets intersect. The chromatic number χ(i, j) of a graph Gi,j is the smallest number of colors that can be used to color all vertices of Gi,j in such a way that no two adjacent vertices share the same color.\nThe chromatic number of a P-model is defined as follows: Definition 3.4 (Chromatic number of a P-model). The chromatic number χ[P] of a P-model is given as:\nχ[P] = max 1≤i≤j≤m χ(i, j),\nwhere Gi,j are associated coherence graphs.\nAs it was the case for the coherence µ[P], smaller values of the chromatic number χ[P] lead to better theoretical results regarding the quality of the model. Intuitively speaking, coherence graphs encode in a compact combinatorial way correlations between different rows of the structured matrix produced by the P-model. The chromatic number χ[P] is a single combinatorial parameter measuring quantitatively these dependencies. It can be easily computed or at least upper-bounded (which is enough for us) for P-models related to all structured matrices considered in this paper. The following is a well-known fact from graph theory:\nLemma 3.1. The chromatic number χ(G) of an undirected graph G with maximum degree dmax satisfies: χ(G) ≤ dmax + 1.\nFor all instantiations of P-models considered in this paper leading to various structured matrices, the vertices of associated coherence graphs will turn out to have small degrees and hence, by Lemma 3.1, small chromatic numbers.\nWe will introduce one more structural parameter of the Pmodel, depending on whether it is specified deterministically or randomly.\nDefinition 3.5. The uni-coherence µ̃[P] of the P-model is defined as follows. If matrices Pi are constructed deterministically then µ̃[P] = max1≤i<j≤m ∑n n1=1\n|PTi,n1Pj,n1 |. If the matrices that specify P are constructed randomly, then we take µ̃[P] = max1≤i<j≤m E[| ∑n n1=1 PTi,n1Pj,n1 |].\nIt turns out that the sublinearity in n of uni-coherence µ̃[P] helps to establish strong theoretical results regarding the quality of the P-model.\n3.2. Examples of P-model structured matrices\nBelow we observe that various structured random matrices can be constructed according to the P-model, i.e. by specifying a sequence of matrices Pi in Eqn. 7. We note that chromatic numbers and coherence values of these Pmodels are low. In the next section, we show that this implies that we can get unbiased, low-variance kernel approximations from these matrices, for various choices of nonlinearities. Here we consider square structured matrices for which m = n, or rectangular matrices with m < n obtained by selecting first m rows of a structured matrix."
    }, {
      "heading" : "3.2.1. CIRCULANT MATRICES",
      "text" : "Circulant matrices can be constructed via theP-model with budget of randomness t = n and matrices {Pi}mi=1 of entries in {0, 1}. See Fig. 1 for an illustrative construction. The coherence of the related P-model trivially satisfies: µ[P] = O(1) and µ̃[P] = 0. The coherence graphs are vertex disjoint cycles. Since each cycle can be colored with at most 3 colors, the chromatic number of the P-model satisfies: χ[P] ≤ 3."
    }, {
      "heading" : "3.2.2. TOEPLITZ AND HANKEL MATRICES",
      "text" : "The associated P-models are obtained in a similar way as for circulant matrices, in particular each column of each Pi is a binary vector. The corresponding coherence graphs have vertices of degrees at most 2 and thus the chromatic number χ[P] is at most 3. As for the previous case, coherence µ[P] is of the order O(1) and µ̃[P] = 0."
    }, {
      "heading" : "3.2.3. FASTFOOD MATRICES",
      "text" : "The Fastfood (Le et al., 2013) approach is a very special case of the P-model. Note that the core term in the Fast-\nfood transform, Eqn. 6, is the structured matrix HG, where H = {hi,j} is Hadamard and G is a random diagonal gaussian matrix (the rightmost terms HB in Eqn. 6 implement data preprocessing to make all datapoints dense, and normalization is implemented by the leftmost scaling matrix S). The matrix HG can be constructed via the P-model with the fixed budget of randomness g = (g1, ..., gn) and using the sequence of matrices P = (P1, ...,Pn), where each Pi is a random diagonal matrix with entries on the diagonal of the form: hi,1, ..., hi,n. The quality of the FastFood approach can be now explained in the general P-model method framework. One can easily see that the graphs related to the model are empty (since PTi,n1Pj,n2 = 0 for n1 6= n2). The sublinearity of µ̃[P] comes from the fact that with high probability any two rows of HG are close to be orthogonal."
    }, {
      "heading" : "3.2.4. TOEPLITZ-LIKE SEMI-GAUSSIAN MATRICES",
      "text" : "Consider Toeplitz-like matrices expressible by Eqn. 5 with displacement rank r. We will assume that g1, ...,gr ∈ Rn defining the Circulant-components in Eqn. 5 are independent Gaussian vectors. They will serve as a “budget of randomness” in the related P-model that we are about to describe, with r allowing a tunable tradeoff between structure and randomness. The vectors h1, ...,hrdefining the skewCirculant components in Eqn. 5 can be defined in different ways. Below we present two general schemes:\nRandom discretized vectors hi: Each dimension of each hi is chosen independently at random from the binary set {− 1√\nnr , 1√ nr }.\nSparse setting: Each hi is sparse (but nonzero), i.e. has\nonly few nonzero entries. Furthermore, the sign of each hij is chosen independently at random and the following holds: ‖h1‖2 + ...+ ‖hr‖2 = 1. This setting is characterized by a parameter κ defining the size of the set of dimensions that are nonzero for at least one hi.\nWe refer to such matrices as Toeplitz-like semi-Gaussian matrices. We now sketch how they can be obtained from the P-model. We take t = nr and g = (g11 , ..., g 1 n, ..., g r 1, ..., g r n) T . The matrix P1 is constructed by vertically stacking r matrices Sj for j = 1, ..., r, where each Sj is constructed as follows. The first column of Sj is hj and the subsequent columns are obtained from previous by skew-Circulant downward shifts. Matrix Pi for i > 1 is obtained from Pi−1 by upward Circulant shifts, independently for each column at each block Sj .\nMatrices constructed according to this procedure satisfy conditions regarding certain structural parameters of thePmodel (see: Theorem 4.4). In particular, in the sparse semiGaussian setting the corresponding coherence graphs have vertices of degrees bounded by a constant; thus, by Lemma 3.1 the P-models associated with them have low chromatic numbers."
    }, {
      "heading" : "3.3. Construction of Random Feature Maps",
      "text" : "Given S[P], the m × n structured random matrix defined by a P-model, in lieu of using the k × n Gaussian random matrix M in Eqn. 1, the feature map for a data vector x is constructed as follows.\n• Preprocessing phase: Compute x′ = D1HD0x, where H ∈ Rn×n is a l2-normalized Hadamard matrix andD0, D1 ∈ {−1,+1}n×n are independent random diagonal matrices. Note that this transformation does not change the values of Gaussian or Arc-cosine kernels, since they are spherically-invariant. This preprocessing densifies the input data vector. • Compute x′′ = S[P]x ∈ Rm. • Compute x̄ ∈ Rk by concatenating random instantia-\ntions of the vector x′′ above obtained from k/m independent constructions of S[P].\n• Return Ψ(x) = 1√ k s(x̄)\nNote that the displacement rank r for low displacement rank matrices and the number of rows m of a single structured block can be used to control the “budget of randomness”; m = 1 reduces to a completely unstructured matrix."
    }, {
      "heading" : "4. Theoretical results",
      "text" : "In this section we provide concentration results regarding P-model for Gaussian and arc-cosine kernels, showing in particular that the variance of the computed structured approximation of the kernel is close to the unstructured one.\nWe also present results targeting specifically low displacement rank structured matrices, and show how the displacement rank knob can be used to increase the budget of randomness and reduce the variance.\nLet us denote by K̃P(x, z) the approximation of the kernel for two vectors x, z ∈ Rn if the P-model is used. By K̃G(x, z) we denote the approximation of the kernel for two vectors x, z ∈ Rn if the fully unstructured setting with truly random Gaussian matrix G is applied. All the proofs are in the Appendix. We start with the following result.\nLemma 4.1 (Unbiasedness of the P-model). Presented P-model mechanism gives an unbiased estimation of the Gaussian and bth-order arc-cosine kernels for b ∈ {0, 1} if for every Pi any two different columns Pi,j ,Pi,k of Pi satisfy PTi,jPi,k = 0. Thus, E[K̃P(x, z)] = K(x, z).\nThe orthogonality condition PTi,jPi,k = 0 is trivially satisfied by Hankel, circulant or Toeplitz structured matrices produced by the P-model as well as Toeplitz-like semiGaussian matrices, where each hi has one nonzero entry. It is also satisfied in expectation (which in practice suffices) for all presented Toeplitz-like semi-Gaussian matrices.\nFor a P-model, where matrices Pi were chosen randomly we denote as η[P] the maximum possible value that a random variable (PTi,n1Pj,n1)\n2 can take for 1 ≤ i < j ≤ m, 1 ≤ n1 ≤ n. Without loss of generality we will assume that data vectors are drawn from the ball B(0, 1) centered at 0 of unit l2 norm. Below we state results regarding dth moments of the obtained kernel’s approximation via the Pmodel that lead to the concentration results.\nTheorem 4.1. Let x, z ∈ B(0, 1) and let d ∈ N. Assume that each structured block of a matrix A (see: Section 3.3) produced according to the P-model has m rows and µ̃[P] = o( n\nlog2(n) ). If matrices Pi of the P-model\nare chosen randomly then assume furthermore that for any 1 ≤ i < j ≤ m and 1 ≤ n1 < n2 ≤ n the nth1 column of Pi is chosen independently from the nth2 column of Pj . If matrices Pi are chosen deterministically then for any T, > 0 the following is true for n large enough:\n|E[K̃dP(x, z)]−E[K̃dG(x, z)]| ≤ O(pgen(T )+pstruct(T )+d ),\nwhere:\npgen(T ) = 4d√ 2πT e− T 2 + 4ne− log2(n) 8 , (9)\npstruct(T ) = 4 m∑ i=1 χ(i, i)e − 1 8µ2[P]χ2[P] n log6(n)\n+2 ∑\n1≤i≤j≤m\nχ(i, j)e −\n2√n 8µ2[P]χ2[P]T log4(n)\n(10)\nand expectations are taken in respect to random choice for a Gaussian vector g. If Pis are chosen from the probabilistic model then the above holds with probability at least 1− pwrong in respect to random choices of Pis, where\npwrong = 2 ∑\ni≤i<j≤m\ne − n 8 log6(n)η[P] .\nLet us comment on the result above. The upper bound is built from two main components: pgen and pstruct. The first one depends on the general parameters of the setting: dimensionality of the data n and order of the computed moment d. The second one is crucial to understand how the structure of the matrix influences the quality of the model. We can immediately see that low chromatic numbers χ(i, j) (see: Section 3.1) improve quality since they decrease computed upper bound. Furthermore, low values of the coherence µ[P] and chromatic number χ[P] also lead to stronger concentration results. Both observations were noticed by us before, but now we see how they are implied by general theoretical results. Finally, for all considered settings, where matrices Pi are constructed randomly parameter η[P] is of order O(1) thus pwrong in negligibly small.\nIn particular, if both the chromatic number χ[P] and the coherence µ[P] are of the orderO(poly(log(n))) then pstruct if inversely proportional to the superpolynomial function of n thus is negligible in practice. That, as we will see soon, will be the case for proposed Toeplitz-like semi-Gaussian matrices with sparse vectors hi.\nLet us also note that Theorem 4.1 can be straightforwardly applied to the structured matrix from the Fastfood model since the condition regarding µ̃[P] is satisfied and so is the independence condition. Since all the chromatic numbers are equal to zero (because corresponding graphs are empty), pstruct = 0 and thus the theorem holds.\nTheorem 4.1 implies also that variances of the kernel approximation for the structured P-model case and unstructured setting are very similar (we borrow denotation from Theorem 4.1).\nTheorem 4.2. Consider the setting as in Theorem 4.1. If matrices Pi are chosen deterministically then for any T, > 0 the following is true for n large enough:\n|V ar(K̃P(x, z))−V ar(K̃G(x, z))| = O( m− 1\n2k ∆), (11)\nwhere V ar stands for the variance and ∆ = pgen(T ) + pstruct + . If Pis are chosen from the probabilistic model then the above holds with probability at least 1 − pwrong, where pwrong is as in Theorem 4.1.\nNote that in practice it means that the variance in the structured and unstructured setting is similar. In particular,\nchoosing = O( 1m2 ), T > 7 log(m), one can deduce that the variance in the structured setting is of the order O( 1m ) for n large enough (the well known fact is that the unstructured variance is of the order O( 1m )). Note also that as expected, for m = 1 the structured setting becomes an unstructured one, since each structured block consists of just one row and different blocks are constructed independently.\nToeplitz-like semi-Gaussian Low-displacement rank matrices: Note that the structure of a matrix affects only the pstruct factor in the statements above. Thus, we will focus on the structured parameters of the P-model. We will show that Toeplitz-like semi-Gaussian matrices can be set up so that the above parameters are of required order. Theorem 4.3. Consider Toeplitz-like semi-Gaussian matrices with sparse skew-Circulant factors (as in Subsection 3.2.4). Let κ denote the number of dimensions that are nonzero for at least one hi. Then for 1 ≤ i ≤ j ≤ m we have: χ(i, j) ≤ κ2 + 1. Furthermore, µ[P] ≤ κ and the bound on |E[K̃dP(x, z)] − E[K̃dG(x, z)]| derived in Theorem 4.1 is valid also here if r ≥ 3log5(n) and for pwrong of the order o( 1n ).\nThe richness of the low displacement rank mechanism comes from the fact that the budget of randomness can be controlled by the rank parameter r and increasing r leads to better quality approximations. In particular, we have: Theorem 4.4. Consider Toeplitz-like semi-Gaussian matrices with sparse skew-Circulant factors and parameter κ. Assume that each hi has exactly α nonzero dimensions, each nonzero dimensions taken independently at random\nfrom {− 1αr , 1 αr}. Then, P[|µ[P]| > τ ] ≤ 4n\n2e − τ2αr O(κ2) .\nNote that increasing rank r leads to sharper upper bounds on the coherence µ[P] (in practice r polynomial in log(n) suffices) and thus, from what we have said so far, to better concentration results for the entire structured scheme. Analogous variance bounds can also be derived for Toeplitz-like semi-Gaussian matrices where the hi vectors are chosen to be dense. But due to lack of space, these results are included in our supplementary material."
    }, {
      "heading" : "5. Empirical Support",
      "text" : "In this section, we compare feature maps obtained with fully Gaussian, Fastfood, Circulant, and Toeplitz-like matrices with increasing displacement rank. Our goal is to lend support to the theoretical contributions of this paper by showing that high-quality feature maps can be constructed from a broad class of structured matrices as instantiations of the proposed P-model.\nKernel Approximation Quality: In Figure 5, we report relative Frobenius error in reconstructing the Gram matrix, i.e. ‖K−K̃‖fro‖K‖fro where K, K̃ denote the exact and ap-\nproximate Gram matrices, as a function of the number of random features. We use the g50c dataset which comprises of 550 examples drawn from multivariate Gaussians in 50-dimensional space with means separated such that the Bayes error is 5%. We see that Circulant matrices and Toeplitz-like matrices with very low displacement rank (1 or 2) perform as well as Fastfood feature maps. In all experiments, for Toeplitz-like matrices, we used skew-Circulant parameters (the h vectors in Eqn. 5) with average sparsity of 5. As the displacement rank is increased, the budget of randomness increases and the reconstruction error approaches that of Gaussian Random features, as expected based on our theoretical results. Results on publicly\navailable real-world classification datasets, averaged over 100 runs, are reported in Table 1 for complex exponential nonlinearity (Gaussian kernel). Results with ReLU (arccosine) are similar but not shown for lack of space. As observed in previous papers, better Gram matrix approximation is not often correlated with higher classification accuracy. Nonetheless, it is clear that the design of space of valid feature map constructions based on structured matrices is much larger than what has so far been explored in the literature: Circulant and Toeplitz-like matrices are very competitive with Fastfood, and sometimes give better results particularly with increasing displacement rank. The effectiveness of such feature maps for nonlinearities other than the complex exponential also validates our theoretical\ncontributions. Among the unstructured baselines, we also include Quasi-Monte Carlo (QMC) feature maps of (Yang et al., 2014) using Halton low-discrepancy sequences. The use of structured matrices to accelerate QMC techniques building on (Dick et al., 2015) is of interest for future work.\nSpeedups: Figure 3 shows the speedup obtained in featuremap construction time using structured matrices relative to using unstructured Gaussian random matrices (on a 6-core 32-GB Intel(R) Xeon(R) machine running Matlab R2014a). The benefits of sub-quadratic matrix-vector multiplication with FFT-variations tend to show up beyond 1024 dimensions. Circulant-based feature maps are the fastest to compute. Fastfood (with DCT instead of Hadamard matrices) is about as fast as Toeplitz-like matrices with displacement rank 1 or 2. Higher displacement rank matrices show speedups at higher dimensions as expected. Fastfood with inbuilt fwht routine in Matlab performed poorly in our experiments."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We have theoretically justified and empirically validated the use of a broad family of structured matrices for accelerating the construction of random embeddings for approximating various kernel functions. In particular, the class of Toeplitz-like semi-Gaussian matrices allows our construction to span highly compact to fully random matrices."
    }, {
      "heading" : "7. Appendix",
      "text" : "We now prove all theoretical results of the paper. We need to introduce some technical denotation.\nFrom now on f denotes one from the following functions: sin, cos, sign or a linear rectifier. We call the set of these functions F . For two vectors v, w we denote by v · w their dot product. We denote by Gistruct for i = 1, ..., k m the building blocks of the structured matrix constructed according to the P-model that are vertically stacked to produce the final structured matrix. Let v1, v2 ∈ Rn be two datapoints from the preprocessed input-dataset D1HD0X . Let d be a fixed integer constant. Let R = {i1, ..., ir} be some r-element subset of the set {1, ...,m}, where m stands for the number of rows used in the construction of matrices Gistruct (key building blocks of our structured mechanism). Finally, let α1, ..., αr be positive integers such that α1 + ...+ αr = d.\nDefinition 7.1. For three vectors: v, w, z ∈ Rn and a given nonlinear function f ∈ F we denote:\nφ(v, w, z) = f(z · v)f(z · w).\nWe will show that for a variety of functions Ψ : Rr → R the expected value of the expression TG,dv1,v2(R, α1, ..., αr) given by the formula:\nΨ(φ1(v 1, v2, gi1)α1 , ..., φr(v 1, v2, gir )αr ), (12)\nwhere g1, ..., gm is the set of m gaussian vectors forming gaussian matrix G, each obtained by sampling independently n values from the distribution N (0, 1) and φis differ by the choice of nonlinear mapping fi ∈ F , can be accurately approximated by its structured version TA,dv1,v2((R, α1, ..., αr) which is of the form:\nΨ(φ1(v 1, v2, ai1)α1 , ..., φr(v 1, v2, air )αr ), (13)\nwhere a1, ..., am are rows of the structured matrix A = Gistruct. The importance of T G,d v1,v2(R, α1, ..., αr) and TA,dv1,v2(R, α1, ..., αr) lies in the fact that d th moments of the random variables approximating considered kernels in the unstructured and structured mechanism can be expressed as weighted sums of the expressions of the form TG,dv1,v2(α1, ..., αr) and T A,d v1,v2(α1, ..., αr) respectively if Ψ(x1, ..., xr) = x1 · ... · xr. Thus if TA,dv1,v2(α1, ..., αr) closely approximates TG,dv1,v2(α1, ..., αr) then the corresponding moments are similar. That, as we will see soon, implies several theoretical guarantees for the structured method. In particular, this means that the variances are similar. Since in the unstructured setting the variance is of the order O( 1m ), that will be also the case for the structured setting. This in turn will imply concentration results providing theoretical explanation for the observations from the\nexperimental section that show the quality of the proposed structured setting.\nWe need to introduce a few definitions.\nDefinition 7.2. We denote by ∆ξs the supremum of the expression ‖ξ(y1, ..., ym) − ξ(y′1, ..., y′m)‖ over all pairs of vectors (y1, ..., ym), (y′1, ..., y ′ m) from the domain D that differ on at most one dimension and by at most s. We say that a function ξ : Rm → R is M -bounded in the domain D if ∆ξ∞ = M .\nNote that the value of the function φi(v1, v2, gi)αi depends only on the projection giproj of g\ni on the 2-dimensional space spanned by v1 and v2. Thus for a given pair v1, v2 function φ is in fact a function Bv 1,v2\ni of this projection.\nDefinition 7.3. Define:\npλ, = sup i,v1,v2,‖ζ|∞≤\nP[|Bv 1,v2\ni (g i proj + ζ)−\nBv 1,v2\ni (g i proj)| > λ],\n(14)\nwhere the supremum is taken over all indices i = 1, ...,m, all pairs of linearly independent vectors from the domain, all coordinate systems in span(v1, v2) and vectors ζ of L1norm at most in some of these coordinate systems.\nWe will use the following notation: σi,j(n1, n2) = PTi,n1Pj,n2 . To compress the statements of our theoretical results, we will use also the following notation:\nξ(ii, i2) = 2χ(i1, i2) √ ∑ 1≤n1<n2≤n (σi1,i2(n1, n2)) 2,\nWe will also denote: λ(i1, i2) = ∑n j=1 |σi1,i2(j, j)| and\nλ̃(i1, i2) = | ∑n j=1 σi1,i2(j, j)| for 1 ≤ i1 ≤ i2 ≤ m (see: 3.1).\nNote first that the preprocessing step preserves kernels’ values since transformation HD0 is an isometry and considered kernels are spherically-invariant. We start with Lemma 4.1.\nProof. Note that it suffices to show that for any two given vectors x, y ∈ Rn the following holds:\nE[f(Gistructx) · f(Gistructy)] = E[f(Gx) · f(Gy)], (15)\nwhereG is the unstructured gaussian matrix. Let gi,jstruct be the jth row of Gistruct and let g\nj be the jth row of G. Note that we have:\nE[f(gi,jstruct ·x)f(g i,j struct ·y)] = E[f(gj ·x)f(gj ·y)]. (16)\nThe latter follows from the fact that gi,jstruct has the same distribution as g. To see this note that gi,jstruct = g ·Pi. Thus\ndimensions of gi,jstruct are projections of g onto columns of Pi. Each projection is trivially gaussian from N (0, 1) (that is implied by the fact that each column is normalized). The independence of different dimensions of gi,jstruct comes from the observation that different columns are orthogonal. Thus we can use a simple property of gaussian vectors stating that the projections of a gaussian vector on mutually orthogonal directions are independent. The equation 15 implies equation 16 by the linearity of expectation and that completes the proof.\nNow we prove Theorem 4.1. This one is easily implied by a more general result that we state below. We will assume that function Ψ from equations: 12, 13 is M -bounded for some given M > 0. We will assume that expected values defining TA,d are not with respect to the random choices determining Pis.\nTheorem 7.1. Let v1, v2 ∈ Rn be two vectors from a dataset X . Let R = {i1, ..., ir} ∈ {1, ...,m} and let α1, ..., αr be the set of positive integers such that α1 + ... + αr = d. Assume that each structured matrix Gistruct consists of m rows and either sup1≤i1<i2≤m λ(i1, i2) = o( n\nlog2(n) ) if Pis were constructed deterministically or sup1≤i1<i2≤mE[λ̃(i1, i2)] = o( nlog2(n) ) if Pis were constructed randomly. In the latter case assume also that for any 1 ≤ i1 < i2 ≤ m and 1 ≤ n1 < n2 ≤ n the nth1 column of Pi1 is chosen independently from the n th 2 column of Pi2 . Denote by Ψmax the maximal value of the function Ψ for the datapoints from X . Let qdv1,v2 = |TA,dv1,v2(R, α1, ..., αr) − T G,d v1,v2(R, α1, ..., αr)| denote the absolute value of the difference of the two fixed terms on the weighted sum for the d-moments of the kernel’s approximation in the structured P-model setting and the fully unstructured setting. Then for any λ, > 0, T > 0, n large enough and Pis chosen deterministically we have: qdv1,v2 ≤ (pgen+pstruct)Ψmax+ d∑ i=0 pif (iM+(d−i)∆Ψλ ),\nwhere:\npgen = 4r√ 2πT e− T 2 + 4ne− log2(n) 8 , (17)\npif =\n( d\ni\n) (pλ, ) i (18)\nand\npstruct = 4 m∑ i=1 χ(i, i)e − 1 2ξ2(i,i) n2 log6(n)\n+2 ∑\n1≤i1≤i2≤m\nχ(i1, i2)e − 2n\n3 2\n2ξ2(i1,i2)T log 4(n)\n(19)\nIf Pis are chosen from the probabilistic model then the above holds with probability at least 1 − pwrong, where\npwrong = 2 ∑ i≤i1<i2≤m e − n2 8 log6(n) ∑n j=1 (σi1,i2 (j,j))2 .\nProof. Consider the expression\nqdv1,v2 = |T A,d v1,v2(R, α1, ..., αr)− T G,d v1,v2(R, α1, ..., αr)|.\nWe will use formulas for TG,d and TA,d given by equations: 12 and 13. Without loss of generality we will assume that A = GistructD1 i.e. in our theoretical analysis we will make D1 a part of the structured mechanism and move it away from the preprocessing phase (obviously both ways are equivalent because of the associative property of matrix mutliplication). We have already noted that each argument of the function Ψ from equations: 12 and 13 depends only on the projections of ai1 , ..., air on the 2-dimensional space spanned by v1 and v2. Denote these projections as: ai1proj ,...,a ir proj respectively and fix some orthonormal basis B of this 2-dimensional space. As we will see soon, in the P-model setting the coordinates of aiprojs in B can be expressed as g · si,j for j = 1, 2, where g is a vector representing a budget of randomness of the corresponding P-model and si,js are some vectors from Rt (parameter t stands for the length of g).\nWe will show that si,js, even though not necessarily pairwise orthogonal, are close to be pairwise orthogonal with high probability. Let us assume now that vectors si,j can be chosen in such a way that each si,j satisfies: si,j = wi,j + ρ(i, j), where vectors wi,j are mutually orthogonal, we have ‖si,j‖2 = ‖wi,j‖2 and furthermore ‖ρ(i, j)‖2 ≤ ρ for some given ρ > 0. We call this property the ρorthogonality property. We will later show that the ρorthogonality property depends on the random diagonal matrix D1.\nAssume now that the ρ-orthogonality property is satisfied. Denote by gH the projection of the “budget-ofrandomness” vector g onto 2r-dimensional linear space H spanned by vectors from {si,j}. Note that then the coordinates of aiprojs in B can be rewritten as g · wi,j + (i, j), where | (i, j)| ≤ and = ‖gH‖2ρ. Thus each ψi in the formula from equation 13 can be then expressed as Bv 1,v2\ni (g i proj + (i)), where g i projs stand for the projections onto 2-dimensional linear space spanned by v1 and v2 of independent copies of gaussian vectors gi. Each gi is of the same distribution as the corresponding structured vector ai and (i)s are vectors with the L1-norm satisfying ‖ (i)‖ ≤ . The independence comes from the fact that variables of the form g · wi,j are independent. That, as in the proof of Lemma 4.1 is implied by the well known fact that dot products of a given gaussian vector with orthogonal vectors are independent. Note that if not the term\n(i) then the formula for TA,d would collapse to its unstructured counterpart TG,d. We will argue that both expressions are still close to each other if (i) have small L1norm.\nLet us fix λ > 0. Our goal is to count these indices i that satisfy the following: |ψi(v1, v2, gi)α\ni − ψi(v 1, v2, gi)α i | > λ, where gis corresponds to the aforementioned independent counterparts of ais. We call them bad indices. Based on what we have said so far, we can conclude that the latter inequality can be expressed as |Bv 1,v2\ni (g i proj + (i)) − B\nv1,v2 i (g i proj)| > λ. Let us first\nfind the upper bound on the probability of the event that the number of bad indices is j for some fixed 1 ≤ j ≤ d. Note that since gis are independent, we can use Bernoulli scheme to find that upped bound. Using the definition of pλ, we obtain an upper bound of the form pupper ≤( d j ) (pλ, )\nj . If the number of bad indices is j then by the definition ofM and ∆Ψλ we see that T A,d differs from TG,d by at most iM + (d− i)∆Ψλ . Summing up over all indices j we get the second term of the upper bound on qdv1,v2 from the statement of the theorem.\nHowever the ρ-orthogonality does not have to hold. Note that (by the definition of Ψmax) to finish the proof of the theorem it suffices to show that the probability of ρorthogonality not to hold is at most pgen + pstruct.\nLemma 7.1. The ρ-orthogonality property holds with probability at least 1− (pgen + pstruct).\nProof. We need the following definition.\nDefinition 7.4. Let x = (x1, ..., xn) be a vector with ‖x‖2 = 1. We say that x is θ-balanced if |xi| ≤ θ√n for i = 1, ..., n.\nFor a fixed pair of vectors v1, v2 ∈ X choose some orthonormal basis B = {x1, x2} of the 2-dimensional space spanned by v1 and v2. Let x̃1 and x̃2 be the images of x1 and x2 under transformationHD0, whereH is a Hadamard matrix and D0 is a random diagonal matrix. We will show now that with high probability x̃1 and x̃2 are log(n)balanced. Indeed, the ith dimension of x̃1 is of the form: x̃1i = hi,1x 1 1 + ...+ hi,nx 1 n, where hi,j stands for the entry in the ith row and jth column of a matrixHD0. We need to find a sharp upper bound on P[|hi,1x11 + ...+ hi,nx1n| ≥ a] for a = log(n)√\nn .\nWe will use the following concentration inequality, calles Azuma’s inequality\nLemma 7.2. Let X1, ..., Xn be a martingale and assume that −αi ≤ Xi ≤ βi for some positive constants α1, ..., αn, β1, ..., βn. Denote X = ∑n i=1Xi. Then the\nfollowing is true:\nP[|X − E[X]| > a] ≤ 2e − a2 2 ∑n i=1 (αi+βi) 2\nIn our case Xj = hi,jx1j and αi = βi = 1√ n . Applying Azuma’s inequality, we obtain the following bound:\nP[|hi,1x11 + ... + hi,nx1n| ≥ log(n)√ n ] ≤ 2e−\nlog2(n) 8 . The\nprobability that all n dimensions of x̃1 and x̃2 have absolute value at most log(n)√\nn is, by the union bound, at least\npbalanced = 1 − 2n · 2e− log2(n) 8 = 1 − 4ne− log2(n)\n8 . Thus this a lower bound on the probability that x̃1 and x̃2 are log(n)-balanced. We will use this lower bound later. Now note that it does not depend on the particular form of the structured matrix since it is only related to the preprocessing phase, where linear mappings D0 and H are applied.\nFor simplicity we will now denote x̂1 and x̂2 simply as x1 and x2, knowing these are the original vectors after applying linear transformation HD0. Let us get back to the projections of ais onto 2-dimensional linear space spanned by v1 and v2. Note that we have already noticed that ai ·xj (j = 1, 2) is of the form g · si,j for some vector si,j ∈ Rt, where t is the size of the “budget of randomness” used in the given P-model. From the definition of the P-model we obtain:\nsi,jl = d1p i l,1x j 1 + ...+ dnp i l,nx j n (20)\nfor l = 1, ..., t, where si,jl stands for the l th dimension of si,j , pil,k is the entry in the l th row and kth column of Pi and drs are the values on the diagonal of the matrix D0. As we noted earlier, we want to show that si,js are close to be mutually orthogonal. To do it, we will compute dot products si1,j1 · si2,j2 . We will first do it for i1 = i2. We have:\nsi1,j1 · si1,j2 = xj11 x j2 1 t∑ l=1 (pi1l,1) 2 + ...+ xj1n x j2 n t∑ l=1 (pi1l,n) 2\n+2 ∑\n1≤n1<n2≤n\ndn1dn2x j1 n1x j2 n2( t∑ i=1 pi1l,n1p i2 l,n2 )\n(21)\nNow we take advantage of the normalization property of the matrices Pi and the fact that x1 is orthogonal to x2 and conclude that the first term on the RHS of the equation above is equal to 0. Thus we have: si1,j1 · si1,j2 = 2 ∑\n1≤n1<n2≤n\ndn1dn2x j1 n1x j2 n2σi1,i1(n1, n2).\n(22)\nNote that if for any fixed Pi any two different columns of Pi are orthogonal then σi1,i1(n1, n2) = 0 and thus\nsi1,j1 · si1,j2 = 0. This is the case for many structured matrices constructed according to the P-model, for instance circulant, Toeplitz or Hankel matrices.\nLet us consider now si1,j1 · si2,j2 for i1 6= i2. By the previous analysis, we obtain:\nsi1,j1 · si2,j2 = σi1,i2(1, 1)x j1 1 x j2 1 + ...+ σi1,i2(n, n)x j1 n x j2 n +2 ∑\n1≤n1<n2≤n\ndn1dn2x j1 n1x j2 n2σi1,i2(n1, n2).\n(23)\nThis time in general we cannot get rid of the first term in the RHS expression. This can be done if columns of the same indices in different Pis are orthogonal. This is in fact again the case for circulant, Toeplitz or Hankel matrices.\nLet us now fix some 1 ≤ i1 ≤ m and κ > 0. Our goal is to find an upper bound on the following probability: P[|si1,j1 · si2,j2 | > κ].\nWe have:\nP[|si1,j1 · si2,j2 | > κ] = P[| ∑\n1≤n1<n2≤n\ndn1dn2x j1 n1x j2 n22σi1,i2(n1, n2)| > κ].\n(24)\nFor {n1, n2} such that n1 6= n2 and σi1,i1(n1, n2) 6= 0 let us now consider random variables Yn1,n2 that are defined as follows\nYn1,n2 = 2dn1dn2x j1 n1x j2 n2σi1,i1(n1, n2). (25)\nFrom the definition of the chromatic number χ(i1, i1) we can deduce that the set of all this random variables can be partitioned into at most χ(i1, i1) subsets such that random variables in each subset are independent. Let us denote these subsets as: L1, ...,Lr, where r ≤ χ(i1, i1). Note that an event {| ∑ 1≤n1<n2≤n dn1dn2x j1 n1x j2 n22σi1,i1(n1, n2)| > κ} is contained in the sum of the events: E = E1 ∪ ...∪ Er, where each Ej is defined as follows:\nEj = {| ∑ Y ∈Lj Y | ≥ κ χ(i1, i1) }. (26)\nThus, from the union bound we get:\nP[E ] ≤ χ(i1,i1)∑ i=1 P[Ei]. (27)\nNow we can use Azuma’s inequality to find an upper bound on P[Ei] and we obtain:\nP[Ei] ≤ 2e −\nκ2\nχ2(i1,i1) 2 ∑\n1≤n1<n2≤n(2σi1,i1 (n1,n2)) 2(x j1 n1 )2(x j2 n2 )2 . (28)\nNow, if we assume that the vectors of the orthonormal basis B are log(n)-balanced, then by the union bound we obtain the following upper bound on the probability P[E ]:\nP[E ] ≤ 2χ(i1, i1)e − κ2n2 2 log4(n)χ2(i1,i1) ∑ 1≤n1<n2≤n(2σi1,i1 (n1,n2)) 2 .\n(29)\nWe can conclude, using the union bound again, that for a log(n)-balanced basis B the probability that there exist i1, j1, j2 such that: |si1,j1 · si1,j2 | > κ is at most\np1,bad(κ) ≤ 2 m∑ i=1 χ(i, i)e − κ2 2ξ2(i,i) n2 log4(n) . (30)\nNow let us find an upper bound on the expression p2,bad(κ) = P[∃i1,i2,j1,j2,i1 6=i2 : |si1,j1 ·si2,j2 | > κ], where i1 6= i2. We will assume that vectors of the basis B are log(n)-balanced. Using the formula on si1,j1 · si2,j2 for i1 6= i2, we get:\nP[|si1,j1 · si2,j2 | > κ] = P[|σi1,i2(1, 1)x j1 1 x j2 1 + ...+ σi1,i2(n, n)x j1 n x j2 n\n+2 ∑\n1≤n1<n2≤n\ndn1dn2x j1 n1x j2 n2σi1,i2(n1, n2)| > κ].\n(31)\nAssume first that Pis are chosen deterministically. Note that by log(n)-balanceness, we have:\n| n∑\nn1=1\nσi1,i2(n1, n1)x j1 1 x j2 1 | ≤\nlog2(n)\nn λ(i1, i2). (32)\nThus, by the triangle inequality, we have:\nP[|si1,j1 · si2,j2 | > κ] ≤ P[|2 ∑\n1≤n1<n2≤n\ndn1dn2x j1 n1x j2 n2σi1,i2(n1, n2)| ≥\nκ− log 2(n)\nn λ(i1, i2)].\n(33)\nUsing the same analysis as before, we then obtain the following bound on pbad(κ, θ): p2,bad(κ) ≤ 2 ∑\n1≤i1<i2≤m\nχ(i1, i2)e −\n(κ− log 2(n) n λ(i1,i2)) 2\n2ξ2(i,i) n2\nlog4(n) .\n(34)\nWe can conclude that in the setting where Pis are chosen deterministically, under our assumptions on λ(i1, i2), for κ > 0 that does not depend on n and n large enough the following is true. The probability that there exist two different vector si1,j1 , si2,j2 such that |si1,j1 · si2,j2 | > κ satisfies:\npbad(κ) ≤ 2 ∑\n1≤i1≤i2≤m\nχ(i1, i2)e −\n(κ− log 2(n) n λ(i1,i2)) 2\n2ξ2(i,i) n2\nlog4(n) .\n(35)\nNow let us assume that Pis are chosen probabilistically. In that setting we also assume that columns of different indices are chosen independently (this is the case for instance for the FastFood Transform). Let us now denote:\nYj = σi1,i2(j, j)x j1 j x\nj2 j (36)\nfor j = 1, ..., n. Denote Y = ∑n i=1 Y1 + ... + Yn. Note that the condition on λ̃(i1, i2) from the statement of the theorem implies that E[Y ] = on(1). From the condition regarding independence of columns of different indices we deduce that Yis are independent. Therefore we can apply Azuma’s inequality and obtain the following bound on the expression: P[|Y − E[Y ]| > a]:\nP[|Y − E[Y ]| > a] ≤ 2e − a2 8 log4(n) n2 ∑n j=1 (σmax i1,i2 (j,j))2 . (37)\nIf we now take a = 1log(n) and under log(n)-balanceness assumption, we obtain:\nP[|Y − E[Y ]| > a] ≤ 2e − n2 8 log6(n) ∑n j=1 (σmax i1,i2 (j,j))2 . (38)\nAssume now that |Y − E[Y ]| ≤ 1log(n) . This happens with probability at least 1 − pwrong with respect to the random choices of Pis, where pwrong = 2e − n2 8 log6(n) ∑n j=1 (σmax i1,i2\n(j,j))2 . But then random variable |Y | is of the order on(1).\nNote that we have:\nP[|si1,j1 · si2,j2 | > κ] = P[|Y + 2 ∑\n1≤n1<n2≤n\ndn1dn2x j1 n1x j2 n2σi1,i2(n1, n2)| > κ].\n(39)\nThus, using our bound on Y for a fixed κ and n large enough we can repeat previous analysis and conclude that in the probabilistic setting of Pis the following is true:\npbad(κ) ≤ 2 ∑\n1≤i1≤i2≤m\nχ(i1, i2)e −\n(κ 2 )2\n2ξ2(i,i) n2\nlog4(n) . (40)\nThus we can conclude that in both the deterministic and probabilistic setting for Pis we get:\npbad(κ) ≤ 2 ∑\n1≤i1≤i2≤m\nχ(i1, i2)e − κ2 8ξ2(i,i) n2 log4(n) . (41)\nNow we will show that the squared lengths of vectors si,j are well concentrated around their means and that these means are equal to 1. Let us remind that we have:\nsi,jl = d1p i l,1x j 1 + ...+ dnp i l,nx j n. (42)\nThus we get: ‖si,j‖22 = ∑\n1≤n1<n2≤n\ndn1dn2x j1 n1x j2 n22σi,i(n1, n2)+\nn∑ n1=1 (σi,i(n1, n1)) 2(xjn1) 2 =\n∑ 1≤n1<n2≤n dn1dn2x j1 n1x j2 n22σi,i(n1, n2) + 1,\n(43)\nwhere the last inequality comes from the fact that each column of each Pi has l2-norm equal to 1.\nSince obviously E[dn1dn2xj1n1x j2 n22σi,i(n1, n2)] = 0, then indeed E[‖si,j‖22] = 1. Let us find the upper bound on the following probability: P[|‖si,j‖22 − 1| > 1log(n) ]. We have:\nP[|‖si,j‖22 − 1| > 1\nlog(n) ] =\nP[|dn1dn2xj1n1x j2 n22σi,i(n1, n2)| >\n1\nlog(n) ].\n(44)\nWe can again apply Azuma’s inequality and the union bound as we did before and obtain:\nP[∃i,j : |‖si,j‖22 − 1| > 1\nlog(n) ] ≤ ps, (45)\nwhere ps = 4 ∑m i=1 χ(i, i)e − 1 2ξ2(i,i) log2(n) n2 log4(n) .\nWe will assume now that all si,j satisfy: |‖si,j‖22 − 1| ≤ 1 log(n) , in particular:√ 1− 1\nlog(n) ≤ ‖si,j‖2 ≤\n√ 1 + 1\nlog(n) . (46)\nLet us assume right now that the above inequality holds. Let {wi,j} be a set of vectors obtained from {si,j} by the Gram-Schmidt process. Without loss of generality we can assume that ‖wi,j‖2 = ‖si,j‖2. Note that the size of the set {si,j} is in fact not 2m, but 2r and in all practical application r m. Assume now that |si1,j1 · si2,j2 | ≤ κ for any two different vectors si1,j1 , si2,j2 and some fixed κ > 0. Now, one can easily note that directly from the description of the Gram-Schmidt process that it leads to the set of vectors {wi,j} such that ‖si,j − wi,j‖2 ≤ κΓ(2r), where Γ is some constant that depends just on the size of the set {si,j}. Thus if we want ρ-orthogonality with ρ = ‖gH‖2 ,\nwhere gH stands for the random projection of a vector g onto 2r-dimensional linear space spanned by vectors from {si,j}, then we want to have:\n‖gH‖2 = κΓ(2r). (47)\nThus we need to take:\nκ = Γ(2r)‖gH‖2 . (48)\nNote that gH is a 2r-dimensional gaussian vector. Now let us take some T > 0. By the union bound the probability that gH has l2 norm greater than √ 2r · √ T is at most: 2rP[|ĝ|2 > T ], where ĝ stands for a gaussian random variable taken from N (0, 1). Now we use the following inequality for a tail of the gaussian random variable:\nP[|ĝ| > x] ≤ 2 e − x22\nx √ 2π . (49)\nThus we can conclude that the probability that gH has l2 norm larger than\n√ 2r · √ T is at most pgauss(T ) ≤ 4r√2πT .\nIn such a case we need to take κ of the form:\nκ = Γ(2r) √ 2r √ T . (50)\nWe are ready to finish the proof of Lemma 7.1. Take κ =\nΓ(2r) √ 2r √ T . Let us first take the setting where Pis are chosen deterministically. Take an event Ebad which is the sum of the events which probabilisites are upperbounded by pgauss(T ), 1 − pbalanced, pbad(κ) and ps. By the union bound, the probability of that event is at most pgauss + (1 − pbalanced) + pbad(κ) + ps which is upperbounded by pgen + pstruct for n large enough. Note that if Ebad does not hold then ρ-orthogonality is satisfied. Now let us take the probabilistic setting for choosing Pis. We proceed similarly. The only difference is that right now we need to assume that the event upper-bounded by pwrong does not hold (this one depends only on the random choices for setting up Pis). Thus again we get the statement of the lemma. That completes the proof of Lemma 7.1.\nAs mentioned above, the proof of Lemma 7.1 completes the proof of the theorem.\nNow we prove Theorem 4.2.\nProof. Fix some x, z ∈ Rn. Assume that a matrix A is used to compute the approximation of the kernel k(x, z). Matrix A is either a truly random Gaussian matrix as it is the case in the unstructured computation or a structured matrix produced according to the P-model. We assume that A has k rows and consists of km blocks stacked\nvertically. If A is produced via the P-model then each block is a structured matrix Gistruct. The approximation of the kernel k̃P(x, z) is of the form: k̃A(x, z) = 1 k ∑ k m i=1 ∑m j=1[φ(a\ni,j · x, ai,j · y)], where ai,j stands for the jth row of the ith block and φ : R2 → R is either of the form φ(a, b) = f(a)f(b), where f is a ReLU/sign function or φ(a, b) = cos(a) cos(b) + sin(a) sin(b). The latter formula for φ is valid if a kernel under consideration is Gaussian. Let use denote the random variable: φ(ai,j ·x, ai,j ·y) as Xi,j . Then we have:\nk̃A(x, z) = 1\nk k m∑ i=1 m∑ j=1 Xi,j . (51)\nThus we have:\nV ar(k̃A(x, z)) = V ar( 1\nk k m∑ i=1 m∑ j=1 Xi,j) =\n1\nk2 V ar( k m∑ i=1 m∑ j=1 Xi,j) = 1 k2 [ k m∑ i=1 m∑ j=1\nV ar(Xi,j)+∑ i,j1 6=j2 Cov(Xi,j1 , Xi,j2)].\n(52)\nThe last inequality in Eqn.52 is implied by the fact that different blocks of the structured matrix are computed independently and thus covariance related to rows from different blocks is 0.\nTherefore we obtain:\nV ar(k̃A(x, z)) = 1\nk2 k m∑ i=1 m∑ j=1 V ar(Xi,j)+\n1\nk2 ∑ i,j1 6=j2 (E[Xi,j1 , Xi,j2 ]− E[Xi,j1 ]E[Xi,j2 ]). (53)\nNow note that the first expression on the RHS above is the same for both the structured and unstructured setting. This is the case since one can note that Xi,j has the same distribution in the unstructured and structured setting. For the same reason the expression E[Xi,j1 ]E[Xi,j2 ] is the same for the structured and unstructured setting. Thus if G stands for the fully unstructured model and we denote k̃A(x, z) = k̃P(x, z) ifA is constructed according to theP-model, then we get:\n|V ar(k̃G(x, z))− V ar(k̃P(x, z))| ≤ 1\nk2 ∑ i,j1 6=j2 |E[XPi,j1X P i,j2 ]− E[X G i,j1X G i,j2 ]|,\n(54)\nwhere XPi,j stands for the version of Xi,j if A was costructed via the P-model and XGi,j stands for the fully unstructured one.\nTherfore we have:\n|V ar(k̃G(x, z))− V ar(k̃P(x, z))| ≤ 1\nk2 · k m ∑ j1 6=j2 |E[XP1,j1X P 1,j2 ]− E[X G 1,j1X G 1,j2 ]|, (55)\nwhere the latter inequality is implied by the fact that different blocks are constructed independently.\nTherefore we get:\n|V ar(k̃G(x, z))− V ar(k̃P(x, z))| ≤ 1 k2 · k m\n( m\n2\n) β,\n(56) where β is an upper bound as in Theorem 7.1 for d = 2. Now we can proceed in the same way as in the proof of Theorem 4.1 and the proof is completed.\nNow we prove Theorem 4.3.\nProof. The fact that µ[P] ≤ κ comes directly from the definition of the coherence number and the sparse setting of semi-gaussian matrices. To see that, note that any given column col of any matrix Pi in the related P-model has a nonzero dot-product with at most κ2 other columns of any matrix Pj . This in turn is implied by the fact that different columns are obtained by applying skew-circulant shifts blockwise, thus the number of columns from Pj that have nonzero dot product with col is at most the product of the number of nonzero dimensions of col and Pj . This is clearly upper bounded by κ2. This leads to the upper bound on the coherence µ[P].\nThe new formula for pwrong is derived by a similar analysis to the one used to obtain the formula on pwrong in the proof of Theorem 4.1. This time random variables under analysis are not independent though, but using the same trick as the one we used in the proof of Theorem 4.1 to decouple dependent random variables in the sum to be estimated and applying Azuma’s inequality (we omit details since the analysis is exactly the same as in the aforementioned proof), we obtain the following: P[|PTi,n1Pj,n1 | > c] ≤ e−Ω(rc2) for i 6= j and any constant c > 0. Taking the union bound over all the pairs of columns and fixing c = 1\nlog2(n) and r = 3 log5(n), we can conclude that with probability at least 1 − o( 1n ) the absolute value of the expression λ(i, j) from the proof of Theorem 4.1 is of the order o( n\nlog2(n) ). That enables us to finish tha analysis in\nthe same way as in the proof of Theorem 4.1 and derive similar conclusions.\nThe bound regarding the chromatic number is implied by the observation that each coherence graph in the corresponding P-model has degree at most κ2. That follows directly from the observation we used to prove the upper bound on µ[P]. But now we can use Lemma 3.1 and that completes the proof of Theorem 4.3.\nBelow we present the proof of Theorem 4.4.\nProof. Fix two columns Pi,n1 and Pj,n2 and consider the expression PTi,n1Pj,n2 . We have already mentioned in the previous proof the right approach to finding strong upper bound on |PTi,n1Pj,n2 |. We first note that P T i,n1\nPj,n2 can be written as a sum w1 + ... + wnr, where wis are not necessarily independent but can be partitioned into at most three sets such that wariables in each of these sets are independent. This is true since Gistruct is produced by skewcirculant shifts and the corresponding coherence graphs has verrtices of degree at most 2. Note also that each wk satisfies: |wk| ≤ 1αr . In each of the sum we get rid of these wis that are equal to 0. Then, by applying Azuma’s inequality independently on each of these subsets and taking union bound over these subsets, we conclude that for any a > 0:\n|PTi,n1Pj,n2 > a| ≤ 3e − a2αr O(1) (57)\nNow we can take the union bound over all pairs of columns and notice that for every columcn col in Pi and any Pj there exists at most κ columns in Pj that have nonzero dot product with col. We can then take a = τκ and the proof is completed.\nLet us now switch to dense semi-gaussian matrices. The following is true.\nTheorem 7.2. Consider the setting as in Theorem 4.1. Assume that entries of any fixed column of Pi are chosen independently at random. Assume also that for any 1 ≤ i ≤ j ≤ m and any fixed column col of Pi each column of Pj is a downward shift of col by b entries (possibly with signs of dimensions swapped) and that b = 0 for O(1) columns in Pj . Then for and T > 0 and n large enough the following holds:\n|E[k̃dP(x, z)]− E[k̃dG(x, z)]| ≤ O(∆), (58)\nwhere ∆ = pgen(T ) + pstruct(T ) + d + e−n 1 3 and\n= log3(n)\nn\nn 23 + max 1≤i≤j≤m | ∑\n1≤n1<n2≤n\nPTi,n1Pj,n2 |  ."
    }, {
      "heading" : "As a corollary:",
      "text" : "|V ar(k̃P(x, z))− V ar(k̃G(x, z))| = O( m− 1\n2k ∆). (59)\nProof. The proof of this result follows along the lines of the proof of Theorem 4.1 and Theorem 4.2. Take the formulas for si1,j1 ·si2,j2 derived in the proof of Theorem 7.1. Note that we want to have: |si1,j1 · si2,j2 | ≤\nΓ(2d)‖gH‖2 , where Γ is a constant that depends only on the degree d. Each si1,j1 · si2,j2 is a sum of random variables that can be decoupled into O(1) subsums such that variables in each subsum are independent (here we use exactly the same trick as in the proof of Theorem 4.1). In each subsum we apply Azuma’s inequality. Straightforward computations lead to the conclusion that if one sets up as in the statement of Theorem 7.2 then the probability that there exist different si1,j1 , si2,j2 such that |si1,j1 · si2,j2 | >\nΓ(2d)‖gH‖2 is of\nthe order e−n 1 3 for n large enough. That is the extra term in the formula for ∆ that was not present in the staement of Theorem 4.1. The variance results follows immediately by exactly the same analysis as in the proof of Theorem 4.2.\nNote that introduced dense semi-gaussian matrices trivially satisfy conditions of Theorem 7.2 (look for the description of matrices Pi from Subsection: 3.2.4). The role of rank is similar as in the sparse setting, i.e. larger values of r lead to sharper concentration results. Theorem 7.2 can be applied to classes of matrices for which | ∑\n1≤n1<n2≤n P T i,n1 Pj,n2 | is small and random dense semi-gaussian matrices satisfy this condition with high probability."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We propose a scheme for recycling Gaussian random vectors into structured matrices to approximate various kernel functions in sublinear time via random embeddings. Our framework includes the Fastfood construction of Le et al. (2013) as a special case, but also extends to Circulant, Toeplitz and Hankel matrices, and the broader family of structured matrices that are characterized by the concept of lowdisplacement rank. We introduce notions of coherence and graph-theoretic structural constants that control the approximation quality, and prove unbiasedness and low-variance properties of random feature maps that arise within our framework. For the case of low-displacement matrices, we show how the degree of structure and randomness can be controlled to reduce statistical variance at the cost of increased computation and storage requirements. Empirical results strongly support our theory and justify the use of a broader family of structured matrices for scaling up kernel methods using random features.",
    "creator" : "LaTeX with hyperref package"
  }
}