{
  "name" : "1505.05663.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Inferring Graphs from Cascades: A Sparse Recovery Framework",
    "authors" : [ "Jean Pouget-Abadie", "Thibaut Horel" ],
    "emails" : [ "JEANPOUGETABADIE@G.HARVARD.EDU", "THOREL@SEAS.HARVARD.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Graphs have been extensively studied for their propagative abilities: connectivity, routing, gossip algorithms, etc. A diffusion process taking place over a graph provides valuable information about the presence and weights of its edges. Influence cascades are a specific type of diffusion processes in which a particular infectious behavior spreads over the nodes of the graph. By only observing the “infection times” of the nodes in the graph, one might hope to recover the underlying graph and the parameters of the cascade model. This problem is known in the literature as the Network Inference problem.\nMore precisely, solving the Network Inference problem involves designing an algorithm taking as input a set of\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\nobserved cascades (realisations of the diffusion process) and recovers with high probability a large fraction of the graph’s edges. The goal is then to understand the relationship between the number of observations, the probability of success, and the accuracy of the reconstruction.\nThe Network Inference problem can be decomposed and analyzed “node-by-node”. Thus, we will focus on a single node of degree s and discuss how to identify its parents among the m nodes of the graph. Prior work has shown that the required number of observed cascades is O(poly(s) logm) (Netrapalli & Sanghavi, 2012; Abrahao et al., 2013).\nA more recent line of research (Daneshmand et al., 2014) has focused on applying advances in sparse recovery to the network inference problem. Indeed, the graph can be interpreted as a “sparse signal” measured through influence cascades and then recovered. The challenge is that influence cascade models typically lead to non-linear inverse problems and the measurements (the state of the nodes at different time steps) are usually correlated. The sparse recovery literature suggests that Ω(s log ms ) cascade observations should be sufficient to recover the graph (Donoho, 2006; Candes & Tao, 2006). However, the best known upper bound to this day is O(s2 logm) (Netrapalli & Sanghavi, 2012; Daneshmand et al., 2014)\nThe contributions of this paper are the following: • we formulate the Graph Inference problem in the con-\ntext of discrete-time influence cascades as a sparse recovery problem for a specific type of Generalized Linear Model. This formulation notably encompasses the well-studied Independent Cascade Model and Voter Model.\n• we give an algorithm which recovers the graph’s edges using O(s logm) cascades. Furthermore, we show that our algorithm is also able to efficiently recover the edge weights (the parameters of the influence model) up to an additive error term,\n• we show that our algorithm is robust in cases where the signal to recover is approximately s-sparse by\nar X\niv :1\n50 5.\n05 66\n3v 1\n[ cs\n.S I]\n2 1\nM ay\n2 01\nproving guarantees in the stable recovery setting.\n• we provide an almost tight lower bound of Ω(s log ms ) observations required for sparse recovery.\nThe organization of the paper is as follows: we conclude the introduction by a survey of the related work. In Section 2 we present our model of Generalized Linear Cascades and the associated sparse recovery formulation. Its theoretical guarantees are presented for various recovery settings in Section 3. The lower bound is presented in Section 4. Finally, we conclude with experiments in Section 5.\nRelated Work The study of edge prediction in graphs has been an active field of research for over a decade (Liben-Nowell & Kleinberg, 2008; Leskovec et al., 2007; Adar & Adamic, 2005). (Gomez Rodriguez et al., 2010) introduced the NETINF algorithm, which approximates the likelihood of cascades represented as a continuous process. The algorithm was improved in later work (Gomez-Rodriguez et al., 2011), but is not known to have any theoretical guarantees beside empirical validation on synthetic networks. Netrapalli & Sanghavi (2012) studied the discrete-time version of the independent cascade model and obtained the first O(s2 logm) recovery guarantee on general networks. The algorithm is based on a likelihood function similar to the one we propose, without the `1-norm penalty. Their analysis depends on a correlation decay assumption, which limits the number of new infections at every step. In this setting, they show a lower bound of the number of cascades needed for support recovery with constant probability of the order Ω(s log(m/s)). They also suggest a GREEDY algorithm, which achieves aO(s logm) guarantee in the case of tree graphs. The work of (Abrahao et al., 2013) studies the same continuous-model framework as (Gomez Rodriguez et al., 2010) and obtains an O(s9 log2 s logm) support recovery algorithm, without the correlation decay assumption. (Du et al., 2013) propose a similar algorithm to ours for recovering the weights of the graph under a continuous-time independent cascade model, without proving theoretical guarantees.\nClosest to this work is a recent paper by Daneshmand et al. (2014), wherein the authors consider a `1-regularized objective function. They adapt standard results from sparse recovery to obtain a recovery bound of O(s3 logm) under an irrepresentability condition (Zhao & Yu, 2006). Under stronger assumptions, they match the (Netrapalli & Sanghavi, 2012) bound of O(s2 logm), by exploiting similar properties of the convex program’s KKT conditions. In contrast, our work studies discrete-time diffusion processes including the Independent Cascade model under weaker assumptions. Furthermore, we analyze both the recovery of the graph’s edges and the estimation of the model’s parameters, and achieve close to optimal bounds.\nThe work of (Du et al., 2014) is slightly orthogonal to ours since they suggest learning the influence function, rather than the parameters of the network directly."
    }, {
      "heading" : "2. Model",
      "text" : "We consider a graph G = (V,E,Θ), where Θ is a |V | × |V | matrix of parameters describing the edge weights of G. Intuitively, Θi,j captures the “influence” of node i on node j. Let m ≡ |V |. For each node j, let θj be the jth column vector of Θ. A discrete-time Cascade model is a Markov process over a finite state space {0, 1, . . . ,K − 1}V with the following properties:\n1. Conditioned on the previous time step, the transition events between two states in {0, 1, . . . ,K − 1} for each i ∈ V are mutually independent across i ∈ V .\n2. Of the K possible states, there exists a contagious state such that all transition probabilities of the Markov process can be expressed as a function of the graph parameters Θ and the set of “contagious nodes” at the previous time step.\n3. The initial probability over {0, 1, . . . ,K − 1}V is such that all nodes can eventually reach a contagious state with non-zero probability. The “contagious” nodes at t = 0 are called source nodes.\nIn other words, a cascade model describes a diffusion process where a set of contagious nodes “influence” other nodes in the graph to become contagious. An influence cascade is a realisation of this random process, i.e. the successive states of the nodes in graph G. Note that both the “single source” assumption made in (Daneshmand et al., 2014) and (Abrahao et al., 2013) as well as the “uniformly chosen source set” assumption made in (Netrapalli & Sanghavi, 2012) verify condition 3. Also note that the multiple-source node assumption does not reduce to the single-source assumption, even under the assumption that cascades do not overlap. Imagining for example two cascades starting from two different nodes; since we do not observe which node propagated the contagion to which node, we cannot attribute an infected node to either cascade and treat the problem as two independent cascades.\nIn the context of Network Inference, (Netrapalli & Sanghavi, 2012) focus on the well-known discrete-time independent cascade model recalled below, which (Abrahao et al., 2013) and (Daneshmand et al., 2014) generalize to continuous time. We extend the independent cascade model in a different direction by considering a more general class of transition probabilities while staying in the discrete-time setting. We observe that despite their obvious differences, both the independent cascade and the voter models make\nthe network inference problem similar to the standard generalized linear model inference problem. In fact, we define a class of diffusion processes for which this is true: the Generalized Linear Cascade Models. The linear threshold model is a special case and is discussed in Section 6."
    }, {
      "heading" : "2.1. Generalized Linear Cascade Models",
      "text" : "Let susceptible denote any state which can become contagious at the next time step with a non-zero probability. We draw inspiration from generalized linear models to introduce Generalized Linear Cascades:\nDefinition 1. Let Xt be the indicator variable of “contagious nodes” at time step t. A generalized linear cascade model is a cascade model such that for each susceptible node j in state s at time step t, the probability of j becoming “contagious” at time step t+ 1 conditioned on Xt is a Bernoulli variable of parameter f(θj ·Xt):\nP(Xt+1j = 1|X t) = f(θj ·Xt) (1)\nwhere f : R→ [0, 1]\nIn other words, each generalized linear cascade provides, for each node j ∈ V a series of measurements (Xt, Xt+1j )t∈Tj sampled from a generalized linear model. Note also that E[Xt+1i |Xt] = f(θi ·Xt). As such, f can be interpreted as the inverse link function of our generalized linear cascade model."
    }, {
      "heading" : "2.2. Examples",
      "text" : ""
    }, {
      "heading" : "2.2.1. INDEPENDENT CASCADE MODEL",
      "text" : "In the independent cascade model, nodes can be either susceptible, contagious or immune. At t = 0, all source nodes are “contagious” and all remaining nodes are “susceptible”. At each time step t, for each edge (i, j) where j is susceptible and i is contagious, i attempts to infect j with probability pi,j ∈ [0, 1]; the infection attempts are mutually independent. If i succeeds, j will become contagious at time step t+1. Regardless of i’s success, node i will be immune at time t+ 1, such that nodes stay contagious for only one time step. The cascade process terminates when no contagious nodes remain.\nIf we denote by Xt the indicator variable of the set of contagious nodes at time step t, then if j is susceptible at time step t+ 1, we have:\nP [ Xt+1j = 1 |X t ] = 1− m∏ i=1 (1− pi,j)X t i .\nDefining Θi,j ≡ log( 11−pi,j ), this can be rewritten as:\nP [ Xt+1j = 1 |X t ] = 1− m∏ i=1 e−Θi,jX t i (IC)\n= 1− e−Θj ·X t\nTherefore, the independent cascade model is a Generalized Linear Cascade model with inverse link function f : z 7→ 1−e−z . Note that to write the Independent Cascade Model as a Generalized Linear Cascade Model, we had to introduce the change of variable Θi,j = log( 11−pi,j ). The recovery results in Section 3 pertain to the Θj parameters. Fortunately, the following lemma shows that the recovery error on Θj is an upper bound on the error on the original pj parameters.\nLemma 1. ‖θ̂ − θ∗‖2 ≥ ‖p̂− p∗‖2."
    }, {
      "heading" : "2.2.2. THE LINEAR VOTER MODEL",
      "text" : "In the Linear Voter Model, nodes can be either red or blue. Without loss of generality, we can suppose that the blue nodes are contagious. The parameters of the graph are normalized such that ∀i, ∑ j Θi,j = 1. Each round, every node j independently chooses one of its neighbors with probability Θi,j and adopts their color. The cascades stops at a fixed horizon time T or if all nodes are of the same color. If we denote by Xt the indicator variable of the set of blue nodes at time step t, then we have:\nP [ Xt+1j = 1|X t ] = m∑ i=1 Θi,jX t i = Θj ·Xt (V)\nThus, the linear voter model is a Generalized Linear Cascade model with inverse link function f : z 7→ z."
    }, {
      "heading" : "2.2.3. DISCRETIZATION OF CONTINUOUS MODEL",
      "text" : "Another motivation for the Generalized Linear Cascade model is that it captures the time-discretized formulation of the well-studied continuous-time independent cascade model with exponential transmission function (CICE) of (Gomez Rodriguez et al., 2010; Abrahao et al., 2013; Daneshmand et al., 2014). Assume that the temporal resolution of the discretization is ε, i.e. all nodes whose (continuous) infection time is within the interval [kε, (k + 1)ε) are considered infected at (discrete) time step k. Let Xk be the indicator vector of the set of nodes ‘infected’ before or during the kth time interval. Note that contrary to the discrete-time independent cascade model, Xkj = 1 =⇒ Xk+1j = 1, that is, there is no immune state and nodes remain contagious forever.\nLet Exp(p) be an exponentially-distributed random variable of parameter p and let Θi,j be the rate of transmis-\nsion along directed edge (i, j) in the CICE model. By the memoryless property of the exponential, if Xkj 6= 1:\nP(Xk+1j = 1|X k) = P( min i∈N (j) Exp(Θi,j) ≤ )\n= P(Exp( m∑ i=1 Θi,jX t i ) ≤ ) = 1− e− Θj ·X t\nTherefore, the -discretized CICE-induced process is a Generalized Linear Cascade model with inverse link function f : z 7→ 1− e− ·z ."
    }, {
      "heading" : "2.2.4. LOGISTIC CASCADES",
      "text" : "“Logistic cascades” is the specific case where the inverse link function is given by the logistic function f(z) = 1/(1 + e−z+t). Intuitively, this captures the idea that there is a threshold t such that when the sum of the parameters of the infected parents of a node is larger than the threshold, the probability of getting infected is close to one. This is a smooth approximation of the hard threshold rule of the Linear Threshold Model (Kempe et al., 2003). As we will see later in the analysis, for logistic cascades, the graph inference problem becomes a linear inverse problem."
    }, {
      "heading" : "2.3. Maximum Likelihood Estimation",
      "text" : "Inferring the model parameter Θ from observed influence cascades is the central question of the present work. Recovering the edges in E from observed influence cascades is a well-identified problem known as the Network Inference problem. However, recovering the influence parameters is no less important. In this work we focus on recovering Θ, noting that the set of edgesE can then be recovered through the following equivalence: (i, j) ∈ E ⇔ Θi,j 6= 0\nGiven observations (x1, . . . , xn) of a cascade model, we can recover Θ via Maximum Likelihood Estimation (MLE). Denoting byL the log-likelihood function, we consider the following `1-regularized MLE problem:\nΘ̂ ∈ argmax Θ\n1 n L(Θ |x1, . . . , xn)− λ‖Θ‖1\nwhere λ is the regularization factor which helps prevent\noverfitting and controls the sparsity of the solution.\nThe generalized linear cascade model is decomposable in the following sense: given Definition 1, the log-likelihood can be written as the sum of m terms, each term i ∈ {1, . . . ,m} only depending on θi. Since this is equally true for ‖Θ‖1, each column θi of Θ can be estimated by a separate optimization program:\nθ̂i ∈ argmax θ Li(θi |x1, . . . , xn)− λ‖θi‖1 (2)\nwhere we denote by Ti the time steps at which node i is susceptible and:\nLi(θi |x1, . . . , xn) = 1 |Ti| ∑ t∈Ti xt+1i log f(θi · x t)\n+ (1− xt+1i ) log ( 1− f(θi · xt) ) In the case of the voter model, the measurements include all time steps until we reach the time horizon T or the graph coalesces to a single state. For the independent cascade model, the measurements include all time steps until node i becomes contagious, after which its behavior is deterministic. Contrary to prior work, our results depend on the number of measurements and not the number of cascades.\nRegularity assumptions To solve program (2) efficiently, we would like it to be convex. A sufficient condition is to assume that Li is concave, which is the case if f and (1− f) are both log-concave. Remember that a twicedifferentiable function f is log-concave iff. f ′′f ≤ f ′2. It is easy to verify this property for f and (1 − f) in the Independent Cascade Model and Voter Model.\nFurthermore, the data-dependent bounds in Section 3.1 will require the following regularity assumption on the inverse link function f : there exists α ∈ (0, 1) such that\nmax { |(log f)′(zx)|, |(log(1− f))′(zx)| } ≤ 1 α (LF)\nfor all zx ≡ θ∗ · x such that f(zx) /∈ {0, 1}.\nIn the voter model, f ′(z) f(z) = 1 z and f ′(z) (1−f)(z) = 1 1−z . Hence (LF) will hold as soon as α ≤ Θi,j ≤ 1− α for all (i, j) ∈ E which is always satisfied for some α for non-isolated nodes. In the Independent Cascade Model, f\n′(z) f(z) = 1 ez−1\nand f ′(z)\n(1−f)(z) = 1. Hence (LF) holds as soon as pi,j ≥ α for all (i, j) ∈ E which is always satisfied for some α ∈ (0, 1).\nFor the data-independent bound of Proposition 1, we will require the following additional regularity assumption:\nmax { |(log f)′′(zx)|, |(log(1− f))′′(zx)| } ≤ 1 α (LF2)\nfor some α ∈ (0, 1) and for all zx ≡ θ∗·x such that f(zx) /∈ {0, 1}. It is again easy to see that this condition is verified for the Independent Cascade Model and the Voter model for the same α ∈ (0, 1).\nConvex constraints The voter model is only defined when Θi,j ∈ (0, 1) for all (i, j) ∈ E. Similarly the independent cascade model is only defined when Θi,j > 0. Because the likelihood function Li is equal to −∞ when the parameters are outside of the domain of definition of the models, these contraints do not need to appear explicitly in the optimization program. In the specific case of the voter model, the constraint∑ j Θi,j = 1 will not necessarily be verified by the estimator obtained in (2). In some applications, the experimenter might not need this constraint to be verified, in which case the results in Section 3 still give a bound on the recovery error. If this constraint needs to be satisfied, then by Lagrangian duality, there exists a λ ∈ R such that adding λ (∑ j θj − 1 ) to the objective function of (2) enforces the constraint. Then, it suffices to apply the results of Section 3 to the augmented objective to obtain the same recovery guarantees. Note that the added term is linear and will easily satisfy all the required regularity assumptions."
    }, {
      "heading" : "3. Results",
      "text" : "In this section, we apply the sparse recovery framework to analyze under which assumptions our program (2) recovers the true parameter θi of the cascade model. Furthermore, if we can estimate θi to a sufficiently good accuracy, it is then possible to recover the support of θi by simple thresholding, which provides a solution to the standard Network Inference problem.\nWe will first give results in the exactly sparse setting in which θi has a support of size exactly s. We will then relax this sparsity constraint and give results in the stable recovery setting where θi is approximately s-sparse.\nAs mentioned in Section 2.3, the maximum likelihood estimation program is decomposable. We will henceforth focus on a single node i ∈ V and omit the subscript i in the notations when there is no ambiguity. The recovery problem is now the one of estimating a single vector θ∗ from a set T of observations. We will write n ≡ |T |."
    }, {
      "heading" : "3.1. Main Theorem",
      "text" : "In this section, we analyze the case where θ∗ is exactly sparse. We write S ≡ supp(θ∗) and s = |S|. Recall, that θi is the vector of weights for all edges directed at the node we are solving for. In other words, S is the set of all nodes susceptible to influence node i, also referred to as its parents. Our main theorem will rely on the now standard\nrestricted eigenvalue condition introduced by (Bickel et al., 2009a).\nDefinition 2. Let Σ ∈ Sm(R) be a real symmetric matrix and S be a subset of {1, . . . ,m}. Defining C(S) ≡ {X ∈ Rm : ‖XSc‖1 ≤ 3‖XS‖1}. We say that Σ satisfies the (S, γ)-restricted eigenvalue condition iff:\n∀X ∈ C(S), XTΣX ≥ γ‖X‖22 (RE)\nA discussion of the (S, γ)-(RE) assumption in the context of generalized linear cascade models can be found in Section 3.3. In our setting we require that the (RE)-condition holds for the Hessian of the log-likelihood functionL: it essentially captures the fact that the binary vectors of the set of active nodes (i.e the measurements) are not too collinear.\nTheorem 1. Assume the Hessian ∇2L(θ∗) satisfies the (S, γ)-(RE) for some γ > 0 and that (LF) holds for some α > 0. For any δ ∈ (0, 1), let θ̂ be the solution of (2) with λ ≡ 2 √ logm αn1−δ , then:\n‖θ̂ − θ∗‖2 ≤ 6\nγ\n√ s logm\nαn1−δ w.p. 1− 1 enδ logm (3)\nNote that we have expressed the convergence rate in the number of measurements n, which is different from the number of cascades. For example, in the case of the voter model with horizon time T and for N cascades, we can expect a number of measurements proportional to N × T .\nTheorem 1 is a consequence of Theorem 1 in (Negahban et al., 2012) which gives a bound on the convergence rate of regularized estimators. We state their theorem in the context of `1 regularization in Lemma 2.\nLemma 2. Let C(S) ≡ {∆ ∈ Rm | ‖∆S‖1 ≤ 3‖∆Sc‖1}. Suppose that:\n∀∆ ∈ C(S), L(θ∗ + ∆)− L(θ∗) −∇L(θ∗) ·∆ ≥ κL‖∆‖22 − τ2L(θ∗) (4)\nfor some κL > 0 and function τL. Finally suppose that λ ≥ 2‖∇L(θ∗)‖∞, then if θ̂λ is the solution of (2):\n‖θ̂λ − θ∗‖22 ≤ 9 λ2s\nκL +\nλ\nκ2L 2τ2L(θ ∗)\nTo prove Theorem 1, we apply Lemma 2 with τL = 0. Since L is twice differentiable and convex, assumption (4) with κL = γ2 is implied by the (RE)-condition. For a good convergence rate, we must find the smallest possible value of λ such that λ ≥ 2‖∇Lθ∗‖∞. The upper bound on the `∞ norm of∇L(θ∗) is given by Lemma 3.\nLemma 3. Assume (LF) holds for some α > 0. For any δ ∈ (0, 1):\n‖∇L(θ∗)‖∞ ≤ 2 √ logm\nαn1−δ w.p. 1− 1 enδ logm\nThe proof of Lemma 3 relies crucially on AzumaHoeffding’s inequality, which allows us to handle correlated observations. This departs from the usual assumptions made in sparse recovery settings, that the measurements are independent from one another. We now show how to use Theorem 1 to recover the support of θ∗, that is, to solve the Network Inference problem.\nCorollary 1. Under the same assumptions as Theorem 1, let Ŝη ≡ {j ∈ {1, . . . ,m} : θ̂j > η} for η > 0. For 0 < < η, let S∗η+ ≡ {i ∈ {1, . . . ,m} : θ∗i > η + } be the set of all true ‘strong’ parents. Suppose the number of measurements verifies: n > 9s logmαγ2 2 . Then with probability 1 − 1m , S ∗ η+ ⊆ Ŝη ⊆ S∗. In other words we recover all ‘strong’ parents and no ‘false’ parents.\nAssuming we know a lower bound α on Θi,j , Corollary 1 can be applied to the Network Inference problem in the following manner: pick = η2 and η = α 3 , then S ∗ η+ = S\nprovided that n = Ω ( s logm α3γ2 ) . That is, the support of θ∗\ncan be found by thresholding θ̂ to the level η."
    }, {
      "heading" : "3.2. Approximate Sparsity",
      "text" : "In practice, exact sparsity is rarely verified. For social networks in particular, it is more realistic to assume that each node has few “strong” parents’ and many “weak” parents. In other words, even if θ∗ is not exactly s-sparse, it can be well approximated by s-sparse vectors.\nRather than obtaining an impossibility result, we show that the bounds obtained in Section 3.1 degrade gracefully in this setting. Formally, let θ∗bsc ∈ argmin‖θ‖0≤s ‖θ − θ\n∗‖1 be the best s-approximation to θ∗. Then we pay a cost proportional to ‖θ∗−θ∗bsc‖1 for recovering the weights of nonexactly sparse vectors. This cost is simply the “tail” of θ∗: the sum of the m − s smallest coordinates of θ∗. We recover the results of Section 3.1 in the limit of exact sparsity. These results are formalized in the following theorem, which is also a consequence of Theorem 1 in (Negahban et al., 2012).\nTheorem 2. Suppose the (RE) assumption holds for the Hessian ∇2f(θ∗) and τL(θ∗) = κ2 logmn ‖θ\n∗‖1 on the following set:\nC′ ≡{X ∈ Rp : ‖XSc‖1 ≤ 3‖XS‖1 + 4‖θ∗ − θ∗bsc‖1} ∩ {‖X‖1 ≤ 1}\nIf the number of measurements n ≥ 64κ2γ s logm, then by\nsolving (2) for λ ≡ 2 √\nlogm αn1−δ we have:\n‖θ̂ − θ∗‖2 ≤ 3\nγ\n√ s logm\nαn1−δ + 4 4\n√ s logm\nγ4αn1−δ ‖θ∗ − θ∗bsc‖1\nAs in Corollary 1, an edge recovery guarantee can be derived from Theorem 2 in the case of approximate sparsity."
    }, {
      "heading" : "3.3. Restricted Eigenvalue Condition",
      "text" : "There exists a large class of sufficient conditions under which sparse recovery is achievable in the context of regularized estimation (van de Geer & Bühlmann, 2009). The restricted eigenvalue condition, introduced in (Bickel et al., 2009b), is one of the weakest such assumption. It can be interpreted as a restricted form of non-degeneracy. Since we apply it to the Hessian of the log-likelihood function ∇2L(θ), it essentially reduces to a form of restricted strong convexity, that Lemma 2 ultimately relies on.\nObserve that the Hessian of L can be seen as a re-weighted Gram matrix of the observations:\n∇2L(θ∗) = 1 |T | ∑ t∈T xt(xt)T [ xt+1i f ′′f − f ′2 f2 (θ∗ · xt)\n− (1− xt+1i ) f ′′(1− f) + f ′2\n(1− f)2 (θ∗ · xt) ] If f and (1 − f) are c-strictly log-convex for c > 0, then min ((log f)′′, (log(1− f))′′) ≥ c. This implies that the (S, γ)-(RE) condition in Theorem 1 and Theorem 2 reduces to a condition on the Gram matrix of the observations XTX = 1|T | ∑ t∈T x t(xt)T for γ′ ≡ γ · c.\n(RE) with high probability The Generalized Linear Cascade model yields a probability distribution over the observed sets of infected nodes (xt)t∈T . It is then natural to ask whether the restricted eigenvalue condition is likely to occur under this probabilistic model. Several recent papers show that large classes of correlated designs obey the restricted eigenvalue property with high probability (Raskutti et al., 2010; Rudelson & Zhou, 2013).\nThe (RE)-condition has the following concentration property: if it holds for the expected Hessian matrix E[∇2L(θ∗)], then it holds for the finite sample Hessian matrix ∇2L(θ∗) with high probability.\nTherefore, under an assumption which only involves the probabilistic model and not the actual observations, we can obtain the same conclusion as in Theorem 1: Proposition 1. Suppose E[∇2L(θ∗)] verifies the (S, γ)(RE) condition and assume (LF) and (LF2). For δ > 0, if n1−δ ≥ 128γαs\n2 logm, then∇2L(θ∗) verifies the (S, γ2 )- (RE) condition, w.p ≥ 1− e−nδ logm.\nObserve that the number of measurements required in Proposition 1 is now quadratic in s. If we only keep the first measurement from each cascade, which are independent, we can apply Theorem 1.8 from (Rudelson & Zhou, 2013), lowering the number of required cascades to s logm log3(s logm).\nIf f and (1 − f) are strictly log-convex, then the previous observations show that the quantity E[∇2L(θ∗)] in Proposition 1 can be replaced by the expected Gram matrix: A ≡ E[XTX]. This matrix A has a natural interpretation: the entry ai,j is the probability that node i and node j are infected at the same time during a cascade. In particular, the diagonal term ai,i is simply the probability that node i is infected during a cascade."
    }, {
      "heading" : "4. A Lower Bound",
      "text" : "In (Netrapalli & Sanghavi, 2012), the authors explicitate a lower bound of Ω(s log ms ) on the number of cascades necessary to achieve good support recovery with constant probability under a correlation decay assumption. In this section, we will consider the stable sparse recovery setting of Section 3.2. Our goal is to obtain an informationtheoretic lower bound on the number of measurements necessary to approximately recover the parameter θ∗ of a cascade model from observed cascades. Similar lower bounds were obtained for sparse linear inverse problems in (Price & Woodruff, 2011; 2012; Ba et al., 2011). Theorem 3. Let us consider a cascade model of the form (1) and a recovery algorithmA which takes as input n random cascade measurements and outputs θ̂ such that with probability δ > 12 (over the measurements):\n‖θ̂ − θ∗‖2 ≤ C min ‖θ‖0≤s ‖θ − θ∗‖2 (5)\nwhere θ∗ is the true parameter of the cascade model. Then n = Ω(s log ms / logC).\nThis theorem should be contrasted with Theorem 2: up to an additive s log s factor, the number of measurements required by our algorithm is tight. The proof of Theorem 3 follows an approach similar to (Price & Woodruff, 2012). We present a sketch of the proof in the Appendix and refer the reader to their paper for more details."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section, we validate empirically the results and assumptions of Section 3 for varying levels of sparsity and different initializations of parameters (n,m, λ, pinit), where pinit is the initial probability of a node being a source node. We compare our algorithm to two different state-of-the-art algorithms: GREEDY and MLE from (Netrapalli & Sanghavi, 2012). As an extra benchmark, we also introduce\na new algorithm LASSO, which approximates our SPARSE MLE algorithm.\nExperimental setup We evaluate the performance of the algorithms on synthetic graphs, chosen for their similarity to real social networks. We therefore consider a WattsStrogatz graph (300 nodes, 4500 edges) (Watts & Strogatz, 1998), a Barabasi-Albert graph (300 nodes, 16200 edges) (Albert & Barabási, 2001), a Holme-Kim power law graph (200 nodes, 9772 edges) (Holme & Kim, 2002), and the recently introduced Kronecker graph (256 nodes, 10000 edges) (Leskovec et al., 2010). Undirected graphs are converted to directed graphs by doubling the edges.\nFor every reported data point, we sample edge weights and generate n cascades from the (IC) model for n ∈ {100, 500, 1000, 2000, 5000}. We compare for each algorithm the estimated graph Ĝ with G. The initial probability of a node being a source is fixed to 0.05, i.e. an average of 15 nodes source nodes per cascades for all experiments, except for Figure (f). All edge weights are chosen uniformly in the interval [0.2, 0.7], except when testing for approximately sparse graphs (see paragraph on robustness). Adjusting for the variance of our experiments, all data points are reported with at most a ±1 error margin. The parameter λ is chosen to be of the order O( √ logm/(αn)). We report our results as a function of the number of cascades and not the number of measurements: in practice, very few cascades have depth greater than 3.\nBenchmarks We compare our SPARSE MLE algorithm to 3 benchmarks: GREEDY and MLE from (Netrapalli & Sanghavi, 2012) and LASSO. The MLE algorithm is a maximum-likelihood estimator without `1-norm penalization. GREEDY is an iterative algorithm. We introduced the LASSO algorithm in our experiments to achieve faster computation time:\nθ̂i ∈ arg min θ ∑ t∈T |f(θi · xt)− xt+1i | 2 + λ‖θi‖1\nLASSO has the merit of being both easier and faster to optimize numerically than the other convex-optimization based algorithms. It approximates the SPARSE MLE algorithm by making the assumption that the observations xt+1i are of the form: xt+1i = f(θi ·xt)+ , where is random white noise. This is not valid in theory since depends on f(θi · xt), however the approximation is validated in practice.\nWe did not benchmark against other known algorithms (NETRATE (Gomez-Rodriguez et al., 2011) and FIRST EDGE (Abrahao et al., 2013)) due to the discrete-time assumption. These algorithms also suppose a single-source model, whereas SPARSE MLE, MLE, and GREEDY do not. Learning the graph in the case of a multi-source cascade\nmodel is harder (see Figure 2 (f)) but more realistic, since we rarely have access to “patient 0” in practice.\nGraph Estimation In the case of the LASSO, MLE and SPARSE MLE algorithms, we construct the edges of Ĝ : ∪j∈V {(i, j) : Θij > 0.1}, i.e by thresholding. Finally, we report the F1-score= 2precision·recall/(precision+recall), which considers (1) the number of true edges recovered by the algorithm over the total number of edges returned by the algorithm (precision) and (2) the number of true edges recovered by the algorithm over the total number of edges it should have recovered (recall). Over all experiments, SPARSE MLE achieves higher rates of precision, recall, and F1-score. Interestingly, both MLE and SPARSE MLE perform exceptionally well on the Watts-Strogatz graph.\nQuantifying robustness The previous experiments only considered graphs with strong edges. To test the algorithms in the approximately sparse case, we add sparse edges to the previous graphs according to a bernoulli variable of parameter 1/3 for every non-edge, and drawing a weight uniformly from [0, 0.1]. The non-sparse case is compared to the sparse case in Figure 2 (d)–(e) for the `2 norm showing that both the LASSO, followed by SPARSE MLE are the most robust to noise."
    }, {
      "heading" : "6. Future Work",
      "text" : "Solving the Graph Inference problem with sparse recovery techniques opens new venues for future work. Firstly, the sparse recovery literature has already studied regularization patterns beyond the `1-norm, notably the thresholded and adaptive lasso (van de Geer et al., 2011; Zou, 2006). Another goal would be to obtain confidence intervals for our estimator, similarly to what has been obtained for the Lasso in the recent series of papers (Javanmard & Montanari, 2014; Zhang & Zhang, 2014).\nFinally, the linear threshold model is a commonly studied diffusion process and can also be cast as a generalized linear cascade with inverse link function z 7→ 1z>0: Xt+1j = sign (θj ·Xt − tj). This model therefore falls into the 1-bit compressed sensing framework (Boufounos & Baraniuk, 2008). Several recent papers study the theoretical guarantees obtained for 1-bit compressed sensing with specific measurements (Gupta et al., 2010; Plan & Vershynin, 2014). Whilst they obtained bounds of the order O(s log ms ), no current theory exists for recovering positive bounded signals from binary measurememts. This research direction may provide the first clues to solve the “adaptive learning” problem: if we are allowed to adaptively choose the source nodes at the beginning of each cascade, how much can we improve the current results?"
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Yaron Singer, David Parkes, Jelani Nelson, Edoardo Airoldi and Or Sheffet for helpful discussions. We are also grateful to the anonymous reviewers for their insightful feedback and suggestions."
    }, {
      "heading" : "7. Appendix",
      "text" : "In this appendix, we provide the missing proofs of Section 3 and Section 4. We also show additional experiments on the running time of our recovery algorithm which could not fit in the main part of the paper."
    }, {
      "heading" : "7.1. Proofs of Section 3",
      "text" : "Proof of Lemma 1. Using the inequality ∀x > 0, log x ≥ 1 − 1x , we have | log( 1 1−p ) − log( 1 1−p′ )| ≥ max(1 − 1−p 1−p′ , 1− 1−p′ 1−p ) ≥ max(p− p ′, p′ − p).\nProof of Lemma 3. The gradient of L is given by:\n∇L(θ∗) = 1 |T | ∑ t∈T xt [ xt+1i f ′ f (θ∗ · xt)\n− (1− xt+1i ) f ′\n1− f (θ∗ · xt)\n]\nLet ∂jL(θ) be the j-th coordinate of ∇L(θ∗). Writing ∂jL(θ∗) = 1|T | ∑ t∈T Yt and since E[x t+1 i |xt] = f(θ∗·xt),\nwe have that E[Yt+1|Yt] = 0. Hence Zt = ∑t k=1 Yk is a martingale.\nUsing assumption (LF), we have almost surely |Zt+1 − Zt| ≤ 1α and we can apply Azuma’s inequality to Zt:\nP [ |ZT | ≥ λ ] ≤ 2 exp\n( −λ2α\n2n\n)\nApplying a union bound to have the previous inequality hold for all coordinates of∇L(θ) implies:\nP [ ‖∇L(θ∗)‖∞ ≥ λ ] ≤ 2m exp\n( −λ2nα\n2\n)\nChoosing λ ≡ 2 √\nlogm αn1−δ concludes the proof.\nProof of Corollary 1. By choosing δ = 0, if n > 9s logmαγ2 2 , then ‖θ̂ − θ∗‖2 < < η with probability 1− 1m . If θ ∗ i = 0 and θ̂ > η, then ‖θ̂ − θ∗‖2 ≥ |θ̂i − θ∗i | > η, which is a contradiction. Therefore we get no false positives. If θ∗i > η + , then |θ̂i − θ∗i | < =⇒ θj > η and we get all strong parents.\n(RE) with high probability We now prove Proposition 1. The proof mostly relies on showing that the Hessian of likelihood function L is sufficiently well concentrated around its expectation.\nProof. Writing H ≡ ∇2L(θ∗), if ∀∆ ∈ C(S), ‖E[H] − H]‖∞ ≤ λ and E[H] verifies the (S, γ)-(RE) condition then:\n∀∆ ∈ C(S), ∆H∆ ≥ ∆E[H]∆(1− 32sλ/γ) (6)\nIndeed, |∆(H−E[H])∆| ≤ 2λ‖∆‖21 ≤ 2λ(4 √ s‖∆s‖2)2. Writing ∂2i,jL(θ∗) = 1|T | ∑ t∈T Yt and using (LF ) and\n(LF2) we have ∣∣Yt − E[Yt]∣∣ ≤ 3α . Applying Azuma’s inequality as in the proof of Lemma 3, this implies:\nP [ ‖E[H]−H‖∞ ≥ λ ] ≤ 2 exp ( −nαλ 2\n3 + 2 logm ) Thus, if we take λ = √ 9logm αn1−δ\n, ‖E[H] − H‖∞ ≤ λ w.p at least 1 − e−nδ logm. When n1−δ ≥ 128γαs\n2 logm, (6) implies ∀∆ ∈ C(S), ∆H∆ ≥ 12∆E[H]∆, w.p. at least 1− e−nδ logm and the conclusion of Proposition 1 follows."
    }, {
      "heading" : "7.2. Proof of Theorem 3",
      "text" : "Let us consider an algorithm A which verifies the recovery guarantee of Theorem 3: there exists a probability distribution over measurements such that for all vectors θ∗, (5) holds w.p. δ. This implies by the probabilistic method that for all distribution D over vectors θ, there exists an n×m measurement matrix XD with such that (5) holds w.p. δ (θ is now the random variable).\nConsider the following distribution D: choose S uniformly at random from a “well-chosen” set of s-sparse supports F and t uniformly at random from X ≡ { t ∈\n{−1, 0, 1}m | supp(t) ∈ F }\n. Define θ = t + w where w ∼ N (0, α smIm) and α = Ω( 1 C ).\nConsider the following communication game between Alice and Bob: (1) Alice sends y ∈ Rm drawn from a Bernouilli distribution of parameter f(XDθ) to Bob. (2) Bob uses A to recover θ̂ from y. It can be shown that at the end of the game Bob now has a quantity of information Ω(s log ms ) about S. By the Shannon-Hartley theorem, this information is also upper-bounded by O(n logC). These two bounds together imply the theorem."
    }, {
      "heading" : "7.3. Running Time Analysis",
      "text" : "We include here a running time analysis of our algorithm. In Figure 3, we compared our algorithm to the benchmark algorithms for increasing values of the number of nodes. In Figure 4, we compared our algorithm to the benchmarks for a fixed graph but for increasing number of observed cascades.\nIn both Figures, unsurprisingly, the simple greedy algorithm is the fastest. Even though both the MLE algorithm\nand the algorithm we introduced are based on convex optimization, the MLE algorithm is faster. This is due to the overhead caused by the `1-regularisation in (2).\nThe dependency of the running time on the number of cascades increases is linear, as expected. The slope is largest for our algorithm, which is again caused by the overhead induced by the `1-regularization."
    } ],
    "references" : [ {
      "title" : "Trace complexity of network inference",
      "author" : [ "Abrahao", "Bruno D", "Chierichetti", "Flavio", "Kleinberg", "Robert", "Panconesi", "Alessandro" ],
      "venue" : "In The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Abrahao et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Abrahao et al\\.",
      "year" : 2013
    }, {
      "title" : "Tracking information epidemics in blogspace",
      "author" : [ "Adar", "Eytan", "Adamic", "Lada A" ],
      "venue" : "IEEE / WIC / ACM International Conference on Web Intelligence (WI 2005),",
      "citeRegEx" : "Adar et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Adar et al\\.",
      "year" : 2005
    }, {
      "title" : "Statistical mechanics of complex networks",
      "author" : [ "Albert", "Réka", "Barabási", "Albert-László" ],
      "venue" : "CoRR, condmat/0106096,",
      "citeRegEx" : "Albert et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Albert et al\\.",
      "year" : 2001
    }, {
      "title" : "Lower bounds for sparse recovery",
      "author" : [ "Ba", "Khanh Do", "Indyk", "Piotr", "Price", "Eric", "Woodruff", "David P" ],
      "venue" : "CoRR, abs/1106.0365,",
      "citeRegEx" : "Ba et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2011
    }, {
      "title" : "Simultaneous analysis of lasso and dantzig selector",
      "author" : [ "Bickel", "Peter J", "Ritov", "Ya’acov", "Tsybakov", "Alexandre B" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Bickel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bickel et al\\.",
      "year" : 2009
    }, {
      "title" : "Simultaneous analysis of lasso and dantzig selector",
      "author" : [ "Bickel", "Peter J", "Ritov", "Ya’acov", "Tsybakov", "Alexandre B" ],
      "venue" : "Ann. Statist., 37(4):1705–1732,",
      "citeRegEx" : "Bickel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bickel et al\\.",
      "year" : 2009
    }, {
      "title" : "1-bit compressive sensing",
      "author" : [ "Boufounos", "Petros", "Baraniuk", "Richard G" ],
      "venue" : "In 42nd Annual Conference on Information Sciences and Systems, CISS 2008, Princeton, NJ, USA,",
      "citeRegEx" : "Boufounos et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Boufounos et al\\.",
      "year" : 2008
    }, {
      "title" : "Near-optimal signal recovery from random projections: Universal encoding strategies",
      "author" : [ "Candes", "Emmanuel J", "Tao", "Terence" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Candes et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Candes et al\\.",
      "year" : 2006
    }, {
      "title" : "Uncover topic-sensitive information diffusion networks",
      "author" : [ "Du", "Nan", "Song", "Le", "Woo", "Hyenkyun", "Zha", "Hongyuan" ],
      "venue" : "In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Du et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2013
    }, {
      "title" : "Influence function learning in information diffusion networks",
      "author" : [ "Du", "Nan", "Liang", "Yingyu", "Balcan", "Maria", "Song", "Le" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Du et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2014
    }, {
      "title" : "Inferring networks of diffusion and influence",
      "author" : [ "Gomez Rodriguez", "Manuel", "Leskovec", "Jure", "Krause", "Andreas" ],
      "venue" : "In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
      "citeRegEx" : "Rodriguez et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rodriguez et al\\.",
      "year" : 2010
    }, {
      "title" : "Uncovering the temporal dynamics of diffusion networks",
      "author" : [ "Gomez-Rodriguez", "Manuel", "Balduzzi", "David", "Schölkopf", "Bernhard" ],
      "venue" : "CoRR, abs/1105.0697,",
      "citeRegEx" : "Gomez.Rodriguez et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gomez.Rodriguez et al\\.",
      "year" : 2011
    }, {
      "title" : "Sample complexity for 1-bit compressed sensing and sparse classification",
      "author" : [ "Gupta", "Ankit", "Nowak", "Robert", "Recht", "Benjamin" ],
      "venue" : "In IEEE International Symposium on Information Theory, ISIT 2010, June 13-18,",
      "citeRegEx" : "Gupta et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2010
    }, {
      "title" : "Growing scale-free networks with tunable clustering",
      "author" : [ "Holme", "Petter", "Kim", "Beom Jun" ],
      "venue" : "Physical review E,",
      "citeRegEx" : "Holme et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Holme et al\\.",
      "year" : 2002
    }, {
      "title" : "Confidence intervals and hypothesis testing for high-dimensional regression",
      "author" : [ "Javanmard", "Adel", "Montanari", "Andrea" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Javanmard et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Javanmard et al\\.",
      "year" : 2014
    }, {
      "title" : "Maximizing the spread of influence through a social network",
      "author" : [ "Kempe", "David", "Kleinberg", "Jon M", "Tardos", "Éva" ],
      "venue" : "In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Kempe et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kempe et al\\.",
      "year" : 2003
    }, {
      "title" : "Patterns of cascading behavior in large blog graphs",
      "author" : [ "Leskovec", "Jure", "McGlohon", "Mary", "Faloutsos", "Christos", "Glance", "Natalie S", "Hurst", "Matthew" ],
      "venue" : "In Proceedings of the Seventh SIAM International Conference on Data Mining,",
      "citeRegEx" : "Leskovec et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Leskovec et al\\.",
      "year" : 2007
    }, {
      "title" : "Kronecker graphs: An approach to modeling networks",
      "author" : [ "Leskovec", "Jure", "Chakrabarti", "Deepayan", "Kleinberg", "Jon M", "Faloutsos", "Christos", "Ghahramani", "Zoubin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Leskovec et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Leskovec et al\\.",
      "year" : 2010
    }, {
      "title" : "Tracing information flow on a global scale using Internet chain-letter data",
      "author" : [ "Liben-Nowell", "David", "Kleinberg", "Jon" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Liben.Nowell et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Liben.Nowell et al\\.",
      "year" : 2008
    }, {
      "title" : "A unified framework for highdimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "Negahban", "Sahand N", "Ravikumar", "Pradeep", "Wrainwright", "Martin J", "Yu", "Bin" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "Negahban et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Negahban et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning the graph of epidemic cascades",
      "author" : [ "Netrapalli", "Praneeth", "Sanghavi", "Sujay" ],
      "venue" : "SIGMETRICS Perform. Eval. Rev.,",
      "citeRegEx" : "Netrapalli et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Netrapalli et al\\.",
      "year" : 2012
    }, {
      "title" : "Dimension reduction by random hyperplane tessellations",
      "author" : [ "Plan", "Yaniv", "Vershynin", "Roman" ],
      "venue" : "Discrete & Computational Geometry,",
      "citeRegEx" : "Plan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Plan et al\\.",
      "year" : 2014
    }, {
      "title" : "eps)-approximate sparse recovery",
      "author" : [ "Price", "Eric", "Woodruff", "David P" ],
      "venue" : "IEEE 52nd Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Price et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Price et al\\.",
      "year" : 2011
    }, {
      "title" : "Applications of the shannon-hartley theorem to data streams and sparse recovery",
      "author" : [ "Price", "Eric", "Woodruff", "David P" ],
      "venue" : "In Proceedings of the 2012 IEEE International Symposium on Information Theory, ISIT",
      "citeRegEx" : "Price et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Price et al\\.",
      "year" : 2012
    }, {
      "title" : "Restricted eigenvalue properties for correlated gaussian designs",
      "author" : [ "Raskutti", "Garvesh", "Wainwright", "Martin J", "Yu", "Bin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raskutti et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Raskutti et al\\.",
      "year" : 2010
    }, {
      "title" : "Reconstruction from anisotropic random measurements",
      "author" : [ "Rudelson", "Mark", "Zhou", "Shuheng" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Rudelson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rudelson et al\\.",
      "year" : 2013
    }, {
      "title" : "The adaptive and the thresholded lasso for potentially misspecified models (and a lower bound for the lasso)",
      "author" : [ "van de Geer", "Sara", "Bühlmann", "Peter", "Zhou", "Shuheng" ],
      "venue" : "Electron. J. Statist.,",
      "citeRegEx" : "Geer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Geer et al\\.",
      "year" : 2011
    }, {
      "title" : "On the conditions used to prove oracle results for the lasso",
      "author" : [ "van de Geer", "Sara A", "Bühlmann", "Peter" ],
      "venue" : "Electron. J. Statist.,",
      "citeRegEx" : "Geer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Geer et al\\.",
      "year" : 2009
    }, {
      "title" : "Collective dynamics of ‘small-world",
      "author" : [ "Watts", "Duncan J", "Strogatz", "Steven H" ],
      "venue" : null,
      "citeRegEx" : "Watts et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Watts et al\\.",
      "year" : 1998
    }, {
      "title" : "On model selection consistency of lasso",
      "author" : [ "Zhao", "Peng", "Yu", "Bin" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2006
    }, {
      "title" : "The adaptive lasso and its oracle properties",
      "author" : [ "Zou", "Hui" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Zou and Hui.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zou and Hui.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Prior work has shown that the required number of observed cascades is O(poly(s) logm) (Netrapalli & Sanghavi, 2012; Abrahao et al., 2013).",
      "startOffset" : 86,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "Related Work The study of edge prediction in graphs has been an active field of research for over a decade (Liben-Nowell & Kleinberg, 2008; Leskovec et al., 2007; Adar & Adamic, 2005).",
      "startOffset" : 107,
      "endOffset" : 183
    }, {
      "referenceID" : 11,
      "context" : "The algorithm was improved in later work (Gomez-Rodriguez et al., 2011), but is not known to have any theoretical guarantees beside empirical validation on synthetic networks.",
      "startOffset" : 41,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "The work of (Abrahao et al., 2013) studies the same continuous-model framework as (Gomez Rodriguez et al.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "(Du et al., 2013) propose a similar algorithm to ours for recovering the weights of the graph under a continuous-time independent cascade model, without proving theoretical guarantees.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "(Gomez Rodriguez et al., 2010) introduced the NETINF algorithm, which approximates the likelihood of cascades represented as a continuous process. The algorithm was improved in later work (Gomez-Rodriguez et al., 2011), but is not known to have any theoretical guarantees beside empirical validation on synthetic networks. Netrapalli & Sanghavi (2012) studied the discrete-time version of the independent cascade model and obtained the first O(s logm) recovery guarantee on general networks.",
      "startOffset" : 7,
      "endOffset" : 352
    }, {
      "referenceID" : 9,
      "context" : "The work of (Du et al., 2014) is slightly orthogonal to ours since they suggest learning the influence function, rather than the parameters of the network directly.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and (Abrahao et al., 2013) as well as the “uniformly chosen source set” assumption made in (Netrapalli & Sanghavi, 2012) verify condition 3.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "In the context of Network Inference, (Netrapalli & Sanghavi, 2012) focus on the well-known discrete-time independent cascade model recalled below, which (Abrahao et al., 2013) and (Daneshmand et al.",
      "startOffset" : 153,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "Another motivation for the Generalized Linear Cascade model is that it captures the time-discretized formulation of the well-studied continuous-time independent cascade model with exponential transmission function (CICE) of (Gomez Rodriguez et al., 2010; Abrahao et al., 2013; Daneshmand et al., 2014).",
      "startOffset" : 224,
      "endOffset" : 301
    }, {
      "referenceID" : 15,
      "context" : "This is a smooth approximation of the hard threshold rule of the Linear Threshold Model (Kempe et al., 2003).",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : "Theorem 1 is a consequence of Theorem 1 in (Negahban et al., 2012) which gives a bound on the convergence rate of regularized estimators.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "These results are formalized in the following theorem, which is also a consequence of Theorem 1 in (Negahban et al., 2012).",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 24,
      "context" : "Several recent papers show that large classes of correlated designs obey the restricted eigenvalue property with high probability (Raskutti et al., 2010; Rudelson & Zhou, 2013).",
      "startOffset" : 130,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "Similar lower bounds were obtained for sparse linear inverse problems in (Price & Woodruff, 2011; 2012; Ba et al., 2011).",
      "startOffset" : 73,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "We therefore consider a WattsStrogatz graph (300 nodes, 4500 edges) (Watts & Strogatz, 1998), a Barabasi-Albert graph (300 nodes, 16200 edges) (Albert & Barabási, 2001), a Holme-Kim power law graph (200 nodes, 9772 edges) (Holme & Kim, 2002), and the recently introduced Kronecker graph (256 nodes, 10000 edges) (Leskovec et al., 2010).",
      "startOffset" : 312,
      "endOffset" : 335
    }, {
      "referenceID" : 11,
      "context" : "We did not benchmark against other known algorithms (NETRATE (Gomez-Rodriguez et al., 2011) and FIRST EDGE (Abrahao et al.",
      "startOffset" : 61,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : ", 2011) and FIRST EDGE (Abrahao et al., 2013)) due to the discrete-time assumption.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "Several recent papers study the theoretical guarantees obtained for 1-bit compressed sensing with specific measurements (Gupta et al., 2010; Plan & Vershynin, 2014).",
      "startOffset" : 120,
      "endOffset" : 164
    } ],
    "year" : 2015,
    "abstractText" : "In the Network Inference problem, one seeks to recover the edges of an unknown graph from the observations of cascades propagating over this graph. In this paper, we approach this problem from the sparse recovery perspective. We introduce a general model of cascades, including the voter model and the independent cascade model, for which we provide the first algorithm which recovers the graph’s edges with high probability and O(s logm) measurements where s is the maximum degree of the graph and m is the number of nodes. Furthermore, we show that our algorithm also recovers the edge weights (the parameters of the diffusion process) and is robust in the context of approximate sparsity. Finally we prove an almost matching lower bound of Ω(s log ms ) and validate our approach empirically on synthetic graphs.",
    "creator" : "LaTeX with hyperref package"
  }
}