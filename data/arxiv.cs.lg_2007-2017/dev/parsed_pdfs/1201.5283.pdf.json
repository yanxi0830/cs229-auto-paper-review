{
  "name" : "1201.5283.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Efficient Primal Dual Prox Method for Non-Smooth Optimization",
    "authors" : [ "Tianbao Yang", "Mehrdad Mahdavi", "Rong Jin", "Shenghuo Zhu" ],
    "emails" : [ "yangtia1@msu.edu", "mahdavim@msu.edu", "rongjin@cse.msu.edu", "zsh@sv.nec-labs.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 1.\n52 83\nv5 [\ncs .L\nG ]\nWe study the non-smooth optimization problems in machine learning, where both the loss function and the regularizer are non-smooth functions. Previous studies on efficient empirical loss minimization assume either a smooth loss function or a strongly convex regularizer, making them unsuitable for non-smooth optimization. We develop a simple yet efficient method for a family of non-smooth optimization problems where the dual form of the loss function is bilinear in primal and dual variables. We cast a non-smooth optimization problem into a minimax optimization problem, and develop a primal dual prox method that solves the minimax optimization problem at a rate of O(1/T ) assuming that the proximal step can be efficiently solved, significantly faster than a standard subgradient descent method that has an O(1/ √ T ) convergence rate. Our empirical study verifies the efficiency of the proposed method for various non-smooth optimization problems that arise ubiquitously in machine learning by comparing it to the state-of-the-art first order methods.\nKeywords: non-smooth optimization, primal dual method, convergence rate, sparsity, efficiency"
    }, {
      "heading" : "1. Introduction",
      "text" : "Formulating machine learning tasks as a regularized empirical loss minimization problem makes an intimate connection between machine learning and mathematical optimization. In regularized empirical loss minimization, one tries to jointly minimize an empirical loss over training samples plus a regularization term of the model. This formulation includes support vector machine (SVM) (Hastie et al., 2008), support vector regression (Smola and Schölkopf, 2004), Lasso (Zhu et al., 2003), logistic regression, and ridge regression (Hastie et al., 2008) among many others. Therefore, optimization methods play a central role in solving machine learning problems and challenges exist in machine learning applications demand the development of new optimization algorithms.\nDepending on the application at hand, various types of loss and regularization functions have been introduced in the literature. The efficiency of different optimization algorithms crucially depends on the specific structures of the loss and the regularization functions. Recently, there have been significant interests on gradient descent based meth-\nc© Yang et al.\nods due to their simplicity and scalability to large datasets. A well-known example is the Pegasos algorithm (Shalev-Shwartz et al., 2011) which minimizes the ℓ22 regularized hinge loss (i.e., SVM) and achieves a convergence rate of O(1/T ), where T is the number of iterations, by exploiting the strong convexity of the regularizer. Several other first order algorithms (Ji and Ye, 2009; Chen et al., 2009) are also proposed for smooth loss functions (e.g., squared loss and logistic loss) and non-smooth regularizers (i.e., ℓ1,∞ and group lasso). They achieve a convergence rate of O(1/T 2) by exploiting the smoothness of the loss functions.\nIn this paper, we focus on a more challenging case where both the loss function and the regularizer are non-smooth, to which we refer as non-smooth optimization. Non-smooth optimization of regularized empirical loss has found applications in many machine learning problems. Examples of non-smooth loss functions include hinge loss (Vapnik, 1998), generalized hinge loss (Bartlett and Wegkamp, 2008), absolute loss (Hastie et al., 2008), and ǫ-insensitive loss (Rosasco et al., 2004); examples of non-smooth regularizers include lasso (Zhu et al., 2003), group lasso (Yuan and Lin, 2006), sparse group lasso (Yang et al., 2010), exclusive lasso (Zhou et al., 2010b), ℓ1,∞ regularizer (Quattoni et al., 2009), and trace norm regularizer (Rennie and Srebro, 2005).\nAlthough there are already many existing studies on tackling smooth loss functions (e.g., square loss for regression, logistic loss for classification), or smooth regularizers (e.g., ℓ22 norm), there are serious challenges in developing efficient algorithms for non-smooth optimization. In particular, common tricks, such as smoothing non-smooth objective functions (Nesterov, 2005a,b), can not be applied to non-smooth optimization to improve convergence rate. This is because they require both the loss functions and regularizers be written in the maximization form of bilinear functions, which unfortunately are often violated, as we will discuss later. In this work, we focus on optimization problems in machine learning where both the loss function and the regularizer are non-smooth. Our goal is to develop an efficient gradient based algorithm that has a convergence rate of O(1/T ) for a wide family of non-smooth loss functions and general non-smooth regularizers.\nIt is noticeable that according to the information based complexity theory (Traub et al., 1988), it is impossible to derive an efficient first order algorithm that generally works for all non-smooth objective functions. As a result, we focus on a family of non-smooth optimization problems, where the dual form of the non-smooth loss function is bilinear in both primal and dual variables. Additionally, we show that many non-smooth loss functions have this bilinear dual form. We derive an efficient gradient based method, with a convergence rate of O(1/T ), that explicitly updates both the primal and dual variables. The proposed method is referred to as Primal Dual Prox (Pdprox) method. Besides its capability of dealing with non-smooth optimization, the proposed method is effective in handling the learning problems where additional constraints are introduced for dual variables.\nThe rest of this paper is organized as follows. Section 2 reviews the related work on minimizing regularized empirical loss especially the first order methods for large-scale optimization. Section 3 presents some notations and definitions. Section 4 presents the proposed primal dual prox method, its convergence analysis, and several extensions of the proposed method. Section 5 presents the empirical studies, and Section 6 concludes this work."
    }, {
      "heading" : "2. Related Work",
      "text" : "Our work is closely related to the previous studies on regularized empirical loss minimization. In the following discussion, we mostly focus on non-smooth loss functions and non-smooth regularizers.\nNon-smooth loss functions Hinge loss is probably the most commonly used nonsmooth loss function for classification. It is closely related to the max-margin criterion. A number of algorithms have been proposed to minimize the ℓ22 regularized hinge loss (Platt, 1998; Joachims, 1999, 2006; Hsieh et al., 2008; Shalev-Shwartz et al., 2011), and the ℓ1 regularized hinge loss (Cai et al., 2010; Zhu et al., 2003; Fung and Mangasarian, 2002). Besides the hinge loss, recently a generalized hinge loss function (Bartlett and Wegkamp, 2008) has been proposed for cost sensitive learning. For regression, square loss is commonly used due to its smoothness. However, non-smooth loss functions such as absolute loss (Hastie et al., 2008) and ǫ-insensitive loss (Rosasco et al., 2004) are useful for robust regression. The Bayes optimal predictor of square loss is the mean of the predictive distribution, while the Bayes optimal predictor of absolute loss is the median of the predictive distribution. Therefore absolute loss is more robust for long-tailed error distributions and outliers (Hastie et al., 2008). (Rosasco et al., 2004) also proved that the estimation error bound for absolute loss and ǫ-insensitive loss converges faster than that of square loss. Non-smooth piecewise linear loss function has been used in quantile regression (Koenker, 2005; Gneiting, 2008). Unlike the absolute loss, the piecewise linear loss function can model non-symmetric error in reality.\nNon-smooth regularizers Besides the simple non-smooth regularizers such as ℓ1, ℓ2, and ℓ∞ norms (Duchi and Singer, 2009), many other non-smooth regularizers have been employed in machine learning tasks. (Yuan and Lin, 2006) introduced group lasso for selecting important explanatory factors in group manner. The ℓ1,∞ norm regularizer has been used for multi-task learning (Argyriou et al., 2008). In addition, several recent works (Hou et al., 2011; Nie et al., 2010; Liu et al., 2009) considered mixed ℓ2,1 regularizer for feature selection. (Zhou et al., 2010b) introduced exclusive lasso for multitask feature selection to model the scenario where variables within a single group compete with each other. Trace norm regularizer is another non-smooth regularizer, which has found applications in matrix completion (Recht et al., 2010; Candès and Recht, 2008), matrix factorization (Rennie and Srebro, 2005; Srebro et al., 2005), and multi-task learning (Argyriou et al., 2008; Ji and Ye, 2009). The optimization algorithms presented in these works are usually limited: either the convergence rate is not guaranteed (Argyriou et al., 2008; Recht et al., 2010; Hou et al., 2011; Nie et al., 2010; Rennie and Srebro, 2005; Srebro et al., 2005) or the loss functions are assumed to be smooth (e.g., the square loss or the logistic loss) (Liu et al., 2009; Ji and Ye, 2009). Despite the significant efforts in developing algorithms for minimizing regularized empirical losses, it remains a challenge to design a first order algorithm that is able to efficiently solve non-smooth optimization problems at a rate of O(1/T ) when both the loss function and the regularizer are non-smooth.\nGradient based optimization Our work is closely related to (sub)gradient based optimization methods. The convergence rate of gradient based methods usually depends on the properties of the objective function to be optimized. When the objective function is strongly convex and smooth, it is well known that gradient descent methods can achieve a geometric convergence rate (Boyd and Vandenberghe, 2004). When the objective function is smooth but not strongly convex, the optimal convergence rate of a gradient descent method is O(1/T 2), and is achieved by the Nesterov’s methods (Nesterov, 2007). For the objective function which is strongly convex but not smooth, the convergence rate becomes O(1/T ) (Shalev-Shwartz et al., 2011). For general non-smooth objective functions, the optimal rate of any first order method is O(1/ √ T ). Although it is not improvable in general, recent studies are able to improve this rate to O(1/T ) by exploring the special structure of the objective function (Nesterov, 2005a,b). In addition, several methods are developed for composite optimization, where the objective function is written as a sum of a smooth and a non-smooth function (Lan, 2010; Nesterov, 2007; Lin, 2010). Recently,\nthese optimization techniques have been successfully applied to various machine learning problems, such as SVM (Zhou et al., 2010a), general regularized empirical loss minimization (Duchi and Singer, 2009; Hu et al., 2009), trace norm minimization (Ji and Ye, 2009), and multi-task sparse learning (Chen et al., 2009). Despite these efforts, one major limitation of the existing (sub)gradient based algorithms is that in order to achieve a convergence rate better than O(1/ √ T ), they have to assume that the loss function is smooth or the regularizer is strongly convex, making them unsuitable for non-smooth optimization.\nConvex-concave optimization The present work is also related to convex-concave minimization. Tseng (2008) and Nemirovski (2005) developed prox methods that have a convergence rate of O(1/T ), provided the gradients are Lipschitz continuous and have been applied to machine learning problems (Sun et al., 2009). In contrast, our method achieves a rate of O(1/T ) without requiring the whole gradient but part of the gradient to be Lipschitz continuous. Several other primal-dual algorithms have been developed for regularized empirical loss minimization that update both primal and dual variables. (Zhu and Chan, 2008) proposed a primal-dual method based on gradient descent, which only achieves a rate of O(1/ √ T ). It was generalized in (Esser et al., 2010), which shares the similar spirit of the proposed algorithm. However, the explicit convergence rate was not established even though the convergence is proved. (Mosci et al., 2010) presented a primal-dual algorithm for group sparse regularization, which updates the primal variable by a prox method and the dual variable by a Newton’s method. In contrast, the proposed algorithm is a first order method that does not require computing the Hessian matrix as the Newton’s method does, and is therefore more scalable to large datasets. (Combettes and Pesquet; Radu loan Bot, 2012) proposed primal-dual splitting algorithms for finding zeros of maximal monotone operators of special types. (Lan et al., 2011) considered the primal-dual convex formulations for general cone programming and apply Nesterov’s optimal first order method (Nesterov, 2007), Nesterov’s smoothing technique (Nesterov, 2005a), and Nemirovski’s prox method (Nemirovski, 2005). Nesterov (2005b) proposed a primal dual gradient method for a special class of structured non-smooth optimization problems by exploring an excessive gap technique.\nOptimizing non-smooth functions We note that Nesterov’s smoothing technique (Nesterov, 2005a) and excessive gap technique (Nesterov, 2005b) can be applied to nonsmooth optimization and both achieve O(1/T ) convergence rate for a special class of nonsmooth optimization problems. However, the limitation of these approaches is that they require all the non-smooth terms (i.e., the loss and the regularizer) to be written as an explicit max structure that consists of a bilinear function in primal and dual variables, thus limits their applications to many machine learning problems. In addition, Nesterov’s algorithms need to solve additional maximizations problem at each iteration. In contrast, the proposed algorithm only requires mild condition on the non-smooth loss functions (section 4), and allows for any commonly used non-smooth regularizers, without having to solve an additional optimization problem at each iteration. Compared to Nesterov’s algorithms, the proposed algorithm is applicable to a large class of non-smooth optimization problems, is easier to implement, its convergence analysis is much simpler, and its empirical performance is usually comparably favorable. Finally we noticed that, as we are preparing our manuscript, a related work (Chambolle and Pock, 2011) has recently been published in the Journal of Mathematical Imaging and Vision that shares a similar idea as this work. Both works maintain and update the primal and dual variables for solving a non-smooth optimization problem, and achieve the same convergence rate (i.e., O(1/T )). However, our work distinguishes from (Chambolle and Pock, 2011) in following aspects: (i) We propose and analyze two primal dual prox methods: one gives an extra gradient updating to dual variables and the other gives an extra gradient updating to primal vari-\nables. Depending on the nature of applications, one method may be more efficient than the others; (ii) In Section 4.4, we discuss how to efficiently solve the interim projection problems for updating both primal variable and dual variable, a critical issue for making the proposed algorithm practically efficient. In contrast, (Chambolle and Pock, 2011) simply assumes that the interim projection problems can be solved efficiently; (iii) We focus our analysis and empirical studies on the optimization problems that are closely related to machine learning. We demonstrate the effectiveness of the proposed algorithm on various classification, regression, and matrix completion tasks with non-smooth loss functions and non-smooth regularizers; (iv) We also conduct analysis and experiments on the convergence of the proposed methods when dealing with the ℓ1 constraint on the dual variable, an approach that is commonly used in robust optimization, and observe that the proposed methods converge much faster when the bound of the ℓ1 constraint is small and the obtained solution is more robust in terms of prediction in the presence of noise in labels. In contrast, the study (Chambolle and Pock, 2011) only considers the application in image problems.\nWe also note that the proposed algorithm is closely related to proximal point algorithm (Rockafellar, 1976) as shown in (He and Yuan, 2012), and many variants including the modified Arrow-Hurwicz method (Popov, 1980), the Doughlas-Rachford (DR) splitting algorithm (Lions and Mercier, 1979), the alternating method of multipliers (ADMM) (Boyd et al., 2011), the forward-backward splitting algorithm (Bredies, 2009), the FISTA algorithm (Beck and Teboulle, 2009). For a detailed comparison with some of these algorithms, one can refer to (Chambolle and Pock, 2011)."
    }, {
      "heading" : "3. Notations and Definitions",
      "text" : "In this section we provide the basic setup, some preliminary definitions and notations used throughout this paper.\nWe denote by [n] the set of integers {1, · · · , n}. We denote by (xi, yi), i ∈ [n] the training examples, where xi ∈ X ⊆ Rd and yi is the assigned class label, which is discrete for classification and continuous for regression. We assume ‖xi‖2 ≤ R, ∀i ∈ [n]. We denote by X = (x1, · · · ,xn)⊤ and y = (y1, · · · , yn)⊤. Let w ∈ Rd denote the linear hypothesis, ℓ(w;x, y) denote a loss of prediction made by the hypothesis w on example (x, y), which is a convex function in terms of w. Examples of convex loss function are hinge loss ℓ(w;x, y) = max(1 − yw⊤x, 0), and absolute loss ℓ(w;x, y) = |w⊤x − y|. To characterize a function, we introduce the following definitions\nDefinition 1 A function ℓ(z) : Z → R is a G-Lipschitz continuous if\n|ℓ(z1)− ℓ(z2)| ≤ G‖z1 − z2‖2, ∀z1, z2 ∈ Z.\nDefinition 2 A function ℓ(z) : Z → R is a ρ-smooth function if its gradient is ρ-Lipschitz continuous\n‖∇ℓ(z1)−∇ℓ(z2)‖2 ≤ ρ‖z1 − z2‖2, ∀z1, z2 ∈ Z.\nA function is non-smooth if either its gradient is not well defined or its gradient is not Lipschtiz continuous. Examples of smooth loss functions are logistic loss ℓ(w;x, y) = log(1 + exp(−yw⊤x)), square loss ℓ(w;x, y) = 12 (w⊤x− y)2, and examples of non-smooth loss functions are hinge loss, and absolute loss. The difference between logistic loss and hinge loss, square loss and absolute loss can be seen in Figure 1. Examples of non-smooth regularizer include R(w) = ‖w‖1, i.e. ℓ1 norm, R(w) = ‖w‖∞, i.e. ℓ∞ norm. More examples can be found in section 4.1.\nIn this paper, we aim to solve the following optimization problem, which occurs in many machine learning problems,\nmin w∈Rd L(w) = 1 n\nn∑\ni=1\nℓ(w;xi, yi) + λR(w), (1)\nwhere ℓ(w;x, y) is a non-smooth loss function, R(w) is a non-smooth regularizer on w, and λ is a regularization parameter.\nWe denote by ΠQ[ẑ] = argmin z∈Q\n1 2‖z − ẑ‖22 the projection of ẑ into domain Q, and\nby ΠQ1,Q2 ( ẑ1 ẑ2 ) the joint projection of ẑ1 and ẑ2 into domains Q1 and Q2, respectively. Finally, we use [s][0,a] to denote the projection of s into [0, a], where a > 0."
    }, {
      "heading" : "4. Pdprox: A Primal Dual Prox Method for Non-Smooth Optimization",
      "text" : "We first describe the non-smooth optimization problems that the proposed algorithm can be applied to, and then present the primal dual prox method for non-smooth optimization. We then prove the convergence rate of the proposed algorithms and discuss several extensions. Proofs for technical lemmas are deferred to the appendix."
    }, {
      "heading" : "4.1 Non-Smooth Optimization",
      "text" : "We first focus our analysis on linear classifiers and denote by w ∈ Rd a linear model. The extension to nonlinear models is discussed in section 4.5. Also, extension to a collection of linear models W ∈ Rd×K can be done in a straightforward way. We consider the following general non-smooth optimization problem:\nmin w∈Qw\n[ L(w) = max\nα∈Qα L(w,α;X,y) + λR(w)\n] . (2)\nThe parameters w in domain Qw and α in domain Qα are referred to as primal and dual variables, respectively. Since it is impossible to develop an efficient first order method for general non-smooth optimization, we focus on the family of non-smooth loss functions that can be characterized by bilinear function L(w,α;X,y), i.e.\nL(w,α;X,y) = c0(X,y) +α ⊤a(X,y) +w⊤b(X,y) +w⊤H(X,y)α, (3)\nwhere c0(X,y), a(X,y), b(X,y), and H(X,y) are the parameters depending on the training examples (X,y) with consistent sizes. In the sequel, we denote by L(w,α) = L(w,α;X,y) for simplicity, and by Gw(w,α) = ∇wL(w,α) and Gα(w,α) = ∇αL(w,α) the partial gradients of L(w,α) in terms of w and α, respectively.\nRemark 1 One direct consequence of assumption in (3) is that the partial gradient Gw(w,α) is independent of w, and Gα(w,α) is independent of α, since L(w,α) is bilinear in w and α. We will explicitly exploit this property in developing the efficient optimization algorithms. We also note that no explicit assumption is made for the regularizer R(w). This is in contrast to the smoothing techniques used in (Nesterov, 2005a,b).\nTo efficiently solve the optimization problem in (1), we need first turn it into the form (2). To this end, we assume that the loss function can be written into a dual form, which is bilinear in the primal and the dual variables, i.e.\nℓ(w;xi, yi) = max αi∈∆α f(w, αi;xi, yi), (4)\nwhere f(w, α;x, y) is a bilinear function in w and α, and ∆α is the domain of variable α. Using (4), we cast problem (1) into (2) with L(w,α;X,y) given by\nL(w,α;X,y) = 1\nn\nn∑\ni=1\nf(w, αi;xi, yi), (5)\nwith α = (α1, · · · , αn)⊤ defined in the domain Qα = {α = (α1, · · · , αn)⊤, αi ∈ ∆α}. Before delving into the description of the proposed algorithms and their analysis, we give a few examples that show many non-smooth loss functions can be written in the form of (4):\n• Hinge loss (Vapnik, 1998): ℓ(w;x, y) = max(0, 1− yw⊤x) = max\nα∈[0,1] α(1− yw⊤x).\n• Generalized hinge loss (Bartlett and Wegkamp, 2008):\nℓ(w;x, y) =    1− ayw⊤x if yw⊤x ≤ 0 1− yw⊤x if 0 < yw⊤x < 1 0 if yw⊤x ≥ 1\n= max α1≥0,α2≥0\nα1+α2≤1\nα1(1 − ayw⊤x) + α2(1− yw⊤x),\nwhere a > 1.\n• Absolute loss (Hastie et al., 2008): ℓ(w;x, y) = |w⊤x− y| = max\nα∈[−1,1] α(w⊤x− y).\n• ǫ-insensitive loss (Rosasco et al., 2004) : ℓ(w;x, y) = max(|w⊤x− y| − ǫ, 0) = max\nα1≥0,α2≥0\nα1+α2≤1\n[ (w⊤x− y)(α1 − α2)− ǫ(α1 + α2) ] .\n• Piecewise linear loss (Koenker, 2005):\nℓ(w;x, y) = { a|w⊤x− y| if w⊤x ≤ y (1− a)|w⊤x− y| if w⊤x ≥ y\n= max α1≥0,α2≥0\nα1+α2≤1\nα1a(y −w⊤x) + α2(1 − a)(w⊤x− y).\n• ℓ2 loss (Nie et al., 2010):\nℓ(W;x,y) = ‖W⊤x− y‖2 = max ‖α‖2≤1 α⊤(W⊤x− y),\nwhere y ∈ RK is multiple class label vector and W = (w1, · · · ,wK). Besides the non-smooth loss function ℓ(w;x, y), we also assume that the regularizer R(w) is a non-smooth function. Many non-smooth regularizers are used in machine learning problems. We list a few of them in the following, where W = (w1, · · · ,wK), wk ∈ Rd and wj is the jth row of W.\n• lasso: R(w) = ‖w‖1, ℓ2 norm: R(w) = ‖w‖2, and ℓ∞ norm: R(w) = ‖w‖∞. • group lasso: R(w) = ∑Kg=1 √ dg‖wg‖2, where wg ∈ Rdg . • exclusive lasso: R(W) = ∑dj=1 ‖wj‖21. • ℓ2,1 norm: R(W) = ∑d j=1 ‖wj‖2. • ℓ1,∞ norm: R(W) = ∑d\nj=1 ‖wj‖∞. • trace norm: R(W) = ‖W‖1, the summation of singular values of W. • other regularizers: R(W) = (∑K k=1 ‖wk‖2 )2 .\nNote that unlike (Nesterov, 2005a,b), we do not further require the non-smooth regularizer to be written into a bilinear dual form, which could be violated by many non-smooth regularizers, e.g. R(W) = (∑K k=1 ‖wk‖2 )2\nor more generally R(w) = V (‖w‖), where V (z) is a monotonically increasing function.\nWe close this section by presenting a lemma showing an important property of the bilinear function L(w,α).\nLemma 3 Let L(w,α) be bilinear in w and α as in (3). Given fixed X,y there exists c > 0 such that ‖H(X,y)‖22 ≤ c, then for any α1,α2 ∈ Qα, and w1,w2 ∈ Qw we have\n‖Gα(w1,α1)−Gα(w2,α2)‖22 ≤ c‖w1 −w2‖22, (6) ‖Gw(w1,α1)−Gw(w2,α2)‖22 ≤ c‖α1 −α2‖22. (7)\nRemark 2 The value of constant c in Lemma 3 is an input to our algorithms used to set the step size. In the Appendix A, we show how to estimate constant c for certain loss functions. In addition the constant c in bounds (6) and (7) do not have to be the same as shown by the the example of generalized hinge loss in Appendix A. It should be noticed that the inequalities in Lemma 3 indicate L(w,α) has Liptschitz continuos gradients, however, the gradient of the whole objective with respect to w, i.e., Gw(w,α) + λ∂R(w) is not Lipschitz continuous due to the general non-smooth term R(w), which prevents previous convex-concave minimization scheme (Tseng, 2008; Nemirovski, 2005) not applicable."
    }, {
      "heading" : "4.2 The Proposed Primal-Dual Prox Methods",
      "text" : "In this subsection, we present two variants of Primal Dual Prox (Pdprox) method for solving the non-smooth optimization problem in (2). The common feature shared by the two algorithms is that they update both the primal and the dual variables at each iteration. In contrast, most first order methods only update the primal variables. The key advantages of the proposed algorithms is that they are able to capture the sparsity structures of\nAlgorithm 1 The Pdprox-dual Algorithm for Non-Smooth Optimization\n1: Input: step size γ = √\n1/(2c), where c is specified in (6). 2: Initialization: w0 = 0,β0 = 0 3: for t = 1, 2, . . . do 4: αt = ΠQα [ βt−1 + γGα(wt−1,βt−1) ] 5: wt = argminw∈Qw 1 2 ‖w − (wt−1 − γGw(wt−1,αt))‖ 2 2 + γλR(w)\n6: βt = ΠQα [ βt−1 + γGα(wt,αt) ] 7: end for 8: Output ŵT = ∑T t=1 wt/T and α̂T = ∑T t=1 αt/T .\nboth primal and dual variables, which is usually the case when both the regularizer and the loss functions are both non-smooth. The two algorithms differ from each other in the number of copies for the dual or the primal variables, and the specific order for updating those. Although our analysis shows that the two algorithms share the same convergence rate; however, our empirical studies show that the one algorithm is more preferable than the other depending on the nature of the applications.\nPdprox-dual algorithm Algorithm 1 shows the first primal dual prox algorithm for optimizing the problem in (2). Compared to the other gradient based algorithms, Algorithm 1 has several interesting features:\n(i) it updates both the dual variable α and the primal variable w. This is useful when additional constraints are introduced for the dual variables, as we will discuss later.\n(ii) it introduces an extra dual variable β in addition to α, and updates both α and β at each iteration by a gradient mapping. The gradient mapping on the dual variables into a sparse domain allows the proposed algorithm to capture the sparsity of the dual variables (more discussion on how the sparse constraint on the dual variable affects the convergence is presented in Section 4.5). Compared to the second algorithm presented below, we refer to Algorithm 1 as Pdprox-dual algorithm since it introduces an extra dual variable in updating.\n(iii) the primal variable w is updated by a composite gradient mapping (Nesterov, 2007) in step 5. Solving a composite gradient mapping in this step allows the proposed algorithm to capture the sparsity of the primal variable. Similar to many other approaches for composite optimization (Duchi and Singer, 2009; Hu et al., 2009), we assume that the mapping in step 5 can be solved efficiently. (This is the only assumption we made on the non-smooth regularizer. The discussion in Section 4.4 shows that the proposed algorithm can be applied to a large family of non-smooth regularizers). (iv) the step size γ is fixed to √ 1/(2c), where c is the constant specified in Lemma 3. This is in\ncontrast to most gradient based methods where the step size depends on T and/or λ. This feature is particularly useful in implementation as we often observe that the performance of a gradient method is sensitive to the choice of the step size.\nPdprox-primal algorithm In Algorithm 1, we maintain two copies of the dual variables α and β, and update them by two gradient mappings 1. We can actually save one gradient mapping on the dual variable by first updating the primal variable wt, and then updating αt using partial gradient computed with wt. As a tradeoff, we add an extra primal variable u, and update it by a simple calculation. The detailed steps are shown\n1. The extra gradient mapping on β can also be replaced with a simple calculation, as discussed in subsection 4.4.\nAlgorithm 2 The Pdprox-primal Algorithm for Non-Smooth Optimization\n1: Input: step size γ = √\n1/(2c), where c is specified in (7). 2: Initialization: u0 = 0,α0 = 0 3: for t = 1, 2, . . . do 4: wt = argminw∈Qw 1 2 ‖w − (ut−1 − γGw(ut−1,αt−1))‖ 2 2 + γλR(w) 5: αt = ΠQα [αt−1 + γGα(wt,αt−1)] 6: ut = wt + γ(Gw(ut−1,αt−1)−Gw(wt,αt)) 7: end for 8: Output ŵT = ∑T t=1 wt/T and α̂T = ∑T t=1 αt/T .\nin Algorithm 2. Similar to Algorithm 1, Algorithm 2 also needs to compute two partial gradients (except for the initial partial gradient on the primal variable), i.e., Gw(·,αt) and Gα(wt, ·). Different from Algorithm 1, Algorithm 2 (i) maintains (wt,αt,ut) at each iteration with O(2d + n) memory, while Algorithm 1 maintains (αt,wt,βt) at each iteration with O(2n+ d) memory; (ii) and replaces one gradient mapping on an extra dual variable βt with a simple update on an extra primal variable ut. Depending on the nature of applications, one method may be more efficient than the other. For example, if the dimension d is much larger than the number of examples n, then Algorithm 1 would be more preferable than Algorithm 2. When the number of examples n is much larger than the dimension d, Algorithm 2 could save the memory and the computational cost. However, as shown by our analysis in Section 4.3, the convergence rate of two algorithms are the same. Because it introduces an extra primal variable, we refer to Algorithm 2 as the Pdprox-primal algorithm.\nRemark 4 It should be noted that although Algorithm 1 uses a similar strategy for updating the dual variables α and β, but it is significantly different from the mirror prox method (Nemirovski, 2005). First, unlike the mirror prox method that introduces an auxiliary variable for w, Algorithm 1 introduces a composite gradient mapping for updating w. Second, Algorithm 1 updates wt using the partial gradient computed from the updated dual variable αt rather than βt−1. Third, Algorithm 1 does not assume that the overall objective function has Lipschitz continuous gradients, a key assumption that limits the application of the mirror prox method.\nRemark 5 A similar algorithm with an extra primal variable is also proposed in a recent work (Chambolle and Pock, 2011). It is slightly different from Algorithm 2 in the order of updating on the primal variable and the dual variable, and the gradients used in the updating. We discuss the differences between the Pdprox method and the algorithm in (Chambolle and Pock, 2011) with our notations in Appendix C."
    }, {
      "heading" : "4.3 Convergence Analysis",
      "text" : "This section establishes bounds on the convergence rate of the proposed algorithms. We begin by presenting a theorem about the convergence rate of Algorithms 1 and 2. For ease of analysis, we first write (2) into the following equivalent minimax formulation\nmin w∈Qw max α∈Qα F (w,α) = L(w,α) + λR(w). (8)\nOur main result is stated in the following theorem.\nTheorem 6 By running Algorithm 1 or Algorithm 2 with T steps, we have\nF (ŵT ,α)− F (w, α̂T ) ≤ ‖w‖22 + ‖α‖22√\n(2/c)T ,\nfor any w ∈ Qw and α ∈ Qα. In particular,\nL(ŵT )−D(α̂T ) ≤ ‖w̃T ‖22 + ‖α̃T ‖22√\n(2/c)T\nwhere D(α) = minw∈Qw F (w,α) is the dual objective, and w̃T , α̃T are given by w̃T = argminw∈Qw F (w, α̂T ), α̃T = argmaxα∈Qα F (ŵT ,α).\nRemark 7 It is worth mentioning that in contrast to most previous studies whose convergence rates are derived for the optimality of either the primal objective or the dual objective, the convergence result in Theorem 6 is on the duality gap, which can serve a certificate of the convergence for the proposed algorithm. It is not difficult to show that when Qw = Rd the dual objective can be computed by\nD(α) = c0(X,y) +α⊤a(X,y) − λR∗ (−b(X,y) −H(X,y)α\nλ\n)\nwhere R∗(u) is the convex conjugate of R(w). For example, if R(w) = 1/2‖w‖22, R∗(u) = 12‖u‖22; if R(w) = ‖w‖p, R∗(u) = I(‖u‖q ≤ 1), where I(·) is an indicator function, p = 1, 2,∞ and 1/p+ 1/q = 1.\nBefore proceeding to the proof of Theorem 6, we present the following Corollary that follows immediately from Theorem 6 and states the convergence bound for the objective L(w) in (2).\nCorollary 8 Let w∗ be the optimal solution to (2), bounded by ‖w∗‖22 ≤ D1, and ‖α‖22 ≤ D2, ∀α ∈ Qα. By running Algorithm 1 or 2 with T iterations, we have\nL(ŵT )− L(w∗) ≤ D1 +D2√ (2/c)T .\nProof Let w = w∗ = argminw∈Qw L(w) and α̃T = argmaxα∈Qα F (ŵT ,α) in Theorem 6, then we have\nmax α∈Qα\nF (ŵT ,α)− F (w∗, α̂T ) ≤ ‖w∗‖22 + ‖α̃T ‖22√\n(2/c)T ,\nSince L(w) = max α∈Qα F (w,α) ≥ F (w, α̂T ), then we have\nL(ŵT )− L(w∗) ≤ D1 +D2√ (2/c)T .\nIn order to aid understanding, we present the proof of Theorem 6 for each algorithm separately in the following subsections."
    }, {
      "heading" : "4.3.1 Convergence Analysis of Algorithm 1",
      "text" : "For the simplicity of analysis, we assume Qw = Rd is the whole Euclidean space. We discuss how to generalize the analysis to a convex domain Qw in Section 4.5. In order to prove Theorem 6 for Algorithm 1, we present a series of lemmas to pave the path for the proof. We first restate the key updates in Algorithm 1 as follows:\nαt = ΠQα [ βt−1 + γGα(wt−1,βt−1) ] , (9)\nwt = arg min w∈Rd\n1 2 ‖w− (wt−1 − γGw(wt−1,αt))‖22 + γλR(w), (10)\nβt = ΠQα [ βt−1 + γGα(wt,αt) ] . (11)\nLemma 9 The updates in Algorithm 1 are equivalent to the following gradient mappings,\n( αt wt ) = ΠQα,Rd   βt−1 + γGα(ut−1,βt−1) ut−1 − γ(Gw(ut−1,αt) + λvt)   ,\nand\n( βt ut ) = ΠQα,Rd   βt−1 + γGα(wt,αt) ut−1 − γ(Gw(wt,αt) + λvt)   ,\nwith initialization u0 = w0, where vt ∈ ∂R(wt) is a partial gradient of the regularizer on wt.\nProof First, we argue that there exists a fixed (sub)gradient vt ∈ ∂R(wt) such that the composite gradient mapping (10) is equivalent to the following gradient mapping,\nwt = ΠRd [wt−1 − γ (Gw(wt−1,αt) + λvt)] . (12)\nTo see this, since wt is the optimal solution to (10), by first order optimality condition, there exists a subgradient vt = ∂R(wt) such that wt −wt−1+ γGw(wt−1,αt)+ γλvt = 0, i.e. wt = wt−1 − γGw(wt−1,αt)− γλvt, which is equivalent to (12) since the projection ΠRd is an identical mapping.\nSecond, the updates in Algorithm 1 for (α,β,w) are equivalent to the following updates for (α,β,w,u)\nαt = ΠQα [ βt−1 + γGα(ut−1,βt−1) ] , wt = ΠRd [ut−1 − γ (Gw(ut−1,αt) + λvt)] , (13) βt = ΠQα [ βt−1 + γGα(wt,αt) ] , ut = wt − γ(Gw(wt,αt)−Gw(ut−1,αt)), (14)\nwith initialization u0 = w0. The reason is because ut = wt, t = 1, · · · due to Gw(wt,αt) = Gw(ut−1,αt), where we use the fact that L(w,α) is linear in w.\nFinally, by plugging (13) for wt into the update for ut in (14), we complete the proof of Lemma 9.\nThe reason that we translate the updates for (αt,wt,βt) in Algorithm 1 into the updates for (αt,wt,βt,ut) in Lemma 9 is because it allows us to fit the updates for (αt,wt,βt,ut) into Lemma 17 as presented in Appendix D, which leads us to a key inequality as stated in Lemma 10 to prove Theorem 6.\nLemma 10 For all t = 1, 2, · · · , and any w ∈ Rd,α ∈ Qα, we have\nγ\n( Gw(wt,αt) + λvt\n−Gα(wt,αt) )⊤ ( wt −w αt −α ) ≤ 1 2 ∥∥∥∥ ( w − ut−1 α− βt−1 )∥∥∥∥ 2\n2\n− 1 2 ∥∥∥∥ ( w − ut α− βt )∥∥∥∥ 2\n2\n+ γ2 ∥∥Gα(wt,αt)−Gα(ut−1,βt−1) ∥∥2 2 − 1 2 ‖wt − ut−1‖22.\nThe proof of Lemma 10 is deferred to Appendix D. We are now ready to prove the main theorem for Algorithm 1. Proof [of Theorem 6 for Algorithm 1] Since F (w,α) is convex in w and concave in α, we have\nF (wt,αt)− F (w,αt) ≤ (Gw(wt,αt) + λvt)⊤(wt −w), F (wt,α)− F (wt,αt) ≤ −Gα(wt,αt)⊤(αt −α),\nwhere vt ∈ ∂R(wt) is the partial gradient of R(w) on wt stated in Lemma 9. Combining the above inequalities with Lemma 10, we have\nγ (F (wt,αt)− F (w,αt) + F (wt,α)− F (wt,αt))\n≤ 1 2 ∥∥∥∥ ( w− ut−1 α− βt−1 )∥∥∥∥ 2\n2\n− 1 2 ∥∥∥∥ ( w − ut α− βt )∥∥∥∥ 2\n2\n+ γ2‖Gα(wt,αt)−Gα(ut−1,βt−1)‖22\n− 1 2 ‖wt − ut−1‖22\n≤ 1 2 ∥∥∥∥ ( w− ut−1 α− βt−1 )∥∥∥∥ 2\n2\n− 1 2 ∥∥∥∥ ( w − ut α− βt )∥∥∥∥ 2\n2\n+ γ2c‖wt − ut−1‖22 − 1\n2 ‖wt − ut−1‖22\n≤ 1 2 ∥∥∥∥ ( w− ut−1 α− βt−1 )∥∥∥∥ 2\n2\n− 1 2 ∥∥∥∥ ( w − ut α− βt )∥∥∥∥ 2\n2\n,\nwhere the second inequality follows the inequality (6) in Lemma 3 and the fact γ = √ 1/(2c). By adding the inequalities of all iterations and dividing both sides by T , we have\n1\nT\nT∑\nt=1\n(F (wt,α)− F (w,αt)) ≤ ‖w‖22 + ‖α‖22√\n(2/c) T . (15)\nWe complete the proof by using the definitions of ŵT , α̂T , and the convexity-concavity of F (w,α) with respect to w and α, respectively."
    }, {
      "heading" : "4.3.2 Convergence Analysis of Algorithm 2",
      "text" : "We can prove the convergence bound for Algorithm 2 by following the same path. In the following we present the key lemmas similar to Lemmas 9 and 10, with proofs omitted.\nLemma 11 There exists a fixed partial gradient vt ∈ ∂R(wt) such that the updates in Algorithm 2 are equivalent to the following gradient mappings,\n( wt αt ) = ΠRd,Qα ( ut−1 − γ(Gw(ut−1,βt−1) + λvt) βt−1 + γGα(wt,βt−1) )\nand ( ut βt ) = ΠRd,Qα ( ut−1 − γ(Gw(wt,αt) + λvt) βt−1 + γGα(wt,αt) ) ,\nwith initialization β0 = α0.\nLemma 12 For all t = 1, 2, · · · , and any w ∈ Rd,α ∈ Qα, we have\nγ\n( Gw(wt,αt) + λvt\n−Gα(wt,αt) )⊤ ( wt −w αt −α ) ≤ 1 2 ∥∥∥∥ ( w− ut−1 α− βt−1 )∥∥∥∥ 2\n2\n− 1 2 ∥∥∥∥ ( w − ut α− βt )∥∥∥∥ 2\n2\n+ γ2 ∥∥Gw(wt,αt)−Gw(ut−1,βt−1) ∥∥2 2 − 1 2 ‖αt − βt−1‖22.\nProof [of Theorem 6 for Algorithm 2] Similar to proof of Theorem 6 for Algorithm 1, we have\nγ (F (wt,αt)− F (w,αt) + F (wt,α)− F (wt,αt))\n≤ 1 2 ∥∥∥∥ ( w − ut−1 α− βt−1 )∥∥∥∥ 2\n2\n− 1 2 ∥∥∥∥ ( w− ut α− βt )∥∥∥∥ 2\n2\n+ γ2 ∥∥Gw(wt,αt)−Gw(ut−1,βt−1) ∥∥2 2\n− 1 2 ‖αt − βt−1‖22\n≤ 1 2 ∥∥∥∥ ( w − ut−1 α− βt−1 )∥∥∥∥ 2\n2\n− 1 2 ∥∥∥∥ ( w− ut α− βt )∥∥∥∥ 2\n2\n+ γ2c‖αt − βt−1‖22 − 1\n2 ‖αt − βt−1‖22\n≤ 1 2 ∥∥∥∥ ( w − ut−1 α− βt−1 )∥∥∥∥ 2\n2\n− 1 2 ∥∥∥∥ ( w− ut α− βt )∥∥∥∥ 2\n2\n,\nwhere the last step follows the inequality (7) in Lemma 3 and the fact γ = √ 1/(2c). By adding the inequalities of all iterations and dividing both sides by T , we have\n1\nT\nT∑\nt=1\n(F (wt,α)− F (w,αt)) ≤ ‖w‖22 + ‖α‖22√\n(2/c) T . (16)\nWe complete the proof by using the definitions of ŵT , α̂T , and the convexity-concavity of F (w,α) with respect to w and α, respectively.\nComparison with Pegasos on ℓ22 regularizer We compare the proposed algorithm to the Pegasos algorithm (Shalev-Shwartz et al., 2011) 2 for minimizing the ℓ22 regularized hinge loss. Although in this case both algorithms achieve a convergence rate of O(1/T ), their dependence on the regularization parameter λ is very different. In particular, the convergence rate of the proposed algorithm is O (\n(1+nλ)R√ 2nλT\n) by noting that\n‖w∗‖22 = O(1/λ), ‖α∗‖22 ≤ ‖α∗‖1 ≤ n, and c = R2/n, while the Pegasos algorithm has a convergence rate of Õ ( ( √ λ+R)2\nλT\n) , where Õ(·) suppresses a logarithmic term ln(T ). Accord-\ning to the common assumption of learning theory (Wu and Zhou, 2005; Smale and Zhou, 2003), the optimal λ is O(n−1/(τ+1)) if the probability measure can be approximated by the closure of RKHS Hκ with exponent 0 < τ ≤ 1. As a result, the convergence rate of the proposed algorithm is O( √ nR/T ) while the convergence rate of Pegasos is O(n1/(1+τ)R2/T ). Since τ ∈ (0, 1], the proposed algorithm could be more efficient than the Pegasos algorithm, particularly when λ is sufficiently small. This is verified by our empirical studies in section 5.7 (see Figure 8). It is also interesting to note that the convergence rate of Pdprox\n2. We compare to the deterministic Pegasos that computes the gradient using all examples at each iteration. It would be criticized that it is not fair to compare with Pegasos since it is a stochastic algorithm, however, such a comparison (both theoretically and empirically) would provide a formal evidence that solving the min-max problem by a primal dual method with an extra-gradient may yield better convergence than solving the primal problem.\nhas a better dependence on R, the ℓ2 norm bound of examples ‖x‖2 ≤ R, compared to R2 in the convergence rate of Pegasos. Finally, we mention that the proposed algorithm is a deterministic algorithm that requires a full pass of all training examples at each iteration, while Pegasos can be purely stochastic by sampling one example for computing the sub-gradient, which maintains the same convergence rate. It remains an interesting and open problem to extend the Pdprox algorithm to its stochastic or randomized version with a similar convergence rate."
    }, {
      "heading" : "4.4 Implementation Issues",
      "text" : "In this subsection, we discuss some implementation issues: (1) how to efficiently solve the optimization problems for updating the primal and dual variables in Algorithms 1 and 2; (2) how to set a good step size; and (3) how to implement the algorithms efficiently.\nBoth α and β are updated by a gradient mapping that requires computing the projection into the domain Qα. When Qα is only consisted of box constraints (e.g., hinge loss, absolute loss, and ǫ-insensitive loss), the projection\n∏ Qα [α̂] can be computed by threshold-\ning. WhenQα is comprised of both box constraints and a linear constraint (e.g., generalized hinge loss), the following lemma gives an efficient algorithm for computing\n∏ Qα [α̂].\nLemma 13 For Qα = {α : α ∈ [0, s]n,α⊤v ≤ ρ}, the optimal solution α∗ to projection ∏\nQα [α̂] is computed by α∗i = [α̂i − ηvi][0,s], ∀i ∈ [n], where η = 0 if ∑ i[α̂i][0,s]vi ≤ ρ and otherwise is the solution to the following equation\n∑\ni\n[α̂i − ηvi][0,s]vi − ρ = 0. (17)\nSince ∑\ni[α̂i − ηvi][0,s]vi − 1 is monotonically decreasing in η, we can solve η in (17) by a bi-section search. Remark 14 It is notable that when the domain is a simplex type domain, i.e. ∑\ni αi ≤ ρ, Duchi et al. (Duchi et al., 2008) has proposed more efficient algorithms for solving the projection problem.\nMoreover, we can further improve the efficiency of Algorithm 1 by removing the gradient mapping on β. The key idea is similar to the analysis provided in subsection 4.5 for arguing that the convergence rate presented in Theorem 6 for Algorithm 2 holds for any convex domain Qw. Actually, the update on α is equivalent to\nαt = argmin α\n1 2 ‖α− (βt−1 + γGα(wt−1,βt−1))‖22 + γQ(α),\nwhich together with the first order optimality condition implies\nαt = βt−1 + γGα(wt−1,βt−1)− γ∂Q(αt),\nwhere\nQ(α) =\n{ 0 α ∈ Qα\n+∞ otherwise ,\nis the indicator function of the domain Qα. Then we can update the βt by\nβt = argmin α\n1 2 ‖α− (βt−1 + γGα(wt,αt)− ∂Q(αt))‖22,\n= βt−1 + γGα(wt,αt)− ∂Q(αt)\nAlgorithm 3 The Pdprox-dual Algorithm for Non-Smooth Optimization\n1: Input: step size γ = √\n1/(2c), where c is specified in (6). 2: Initialization: w0 = 0,β0 = 0 3: for t = 1, 2, . . . do 4: αt = ΠQα [ βt−1 + γGα(wt−1,βt−1) ] 5: wt = argminw∈Qw 1 2 ‖w − (wt−1 − γGw(wt−1,αt))‖ 2 2 + γλR(w) 6: βt = αt + γ(Gα(wt,αt)−Gα(wt−1,βt−1)) 7: end for 8: Output ŵT = ∑T t=1 wt/T and α̂T = ∑T t=1 αt/T .\nwhich can be computed simply by\nβt = αt + γ(Gα(wt,αt)−Gα(wt−1,βt−1)).\nThe new Pdprox-dual algorithm is presented in Algorithm 3. To prove the convergence rate of Algorithm 3, we can follow the same analysis to first prove the duality gap for L(w,α) + λR(w) − Q(α) and then absorb Q(α) into the domain constraint of α. The convergence result presented in Theorem 6 holds the same for Algorithm 3.\nRemark 15 In Appendix C, we show that the updates on (wt,αt) of Algorithm 3 are essentially the same to the Algorithm 1 in (Chambolle and Pock, 2011), if we remove the extra dual variable in Algorithm 3 and the extra primal variable in Algorithm 1 in (Chambolle and Pock, 2011). However, the difference is that in Algorithm 3, we maintain two dual variables and one primal variable at each iteration, while the Algorithm 1 in (Chambolle and Pock, 2011) maintains two primal variables and one dual variable at each iteration.\nFor the composite gradient mapping for w ∈ Qw = Rd, there is a closed form solution for simple regularizers (e.g., ℓ1, ℓ2) and decomposable regularizers (e.g., ℓ1,2). Efficient algorithms are available for composite gradient mapping when the regularizer is the ℓ∞ and ℓ1,∞, or trace norm. More details can be found in (Duchi and Singer, 2009; Ji and Ye, 2009). Here we present an efficient solution to a general regularizer V (‖w‖), where ‖w‖ is either a simple regularizer (e.g., ℓ1, ℓ2, and ℓ∞) or a decomposable regularizer (e.g., ℓ1,2 and ℓ1,∞), and V (z) is convex and monotonically increasing for z ≥ 0. An example is V (‖w‖) = ( ∑ k ‖wk‖2)2, where w1, . . . ,wK forms a partition of w.\nLemma 16 Let V∗(η) be the convex conjugate of V (z), i.e. V (z) = maxη ηz − V∗(η). Then the solution to the composite mapping\nw∗ = arg min w∈Qw\n1 2 ‖w− ŵ‖22 + λV (‖w‖),\ncan be computed by\nw∗ = arg min w∈Qw\n1 2 ‖w− ŵ‖22 + λη‖w‖,\nwhere η satisfies ‖w∗‖− V ′∗(η) = 0. Since both ‖w∗‖ and −V ′∗(η) are non-increasing functions in η, we can efficiently compute η by a bi-section search.\nThe value of the step size γ in Algorithms 2 and 3 depends on the value of c, a constant that upper bounds the spectral norm square of the matrix H(X,y). In many machine\nlearning applications, by assuming a bound on the data (e.g., ‖x‖2 ≤ R), one can easily compute an estimate of c. We present derivations of the constant c for hinge loss and generalized hinge loss in Appendix A. However, the computed value of c might be overestimated, thus the step size γ is underestimated. Therefore, to improve the empirical performances, one can scale up the estimated value of γ by a factor larger than one and choose the best factor by tuning among a set of values. In addition, the authors in (Chambolle and Pock, 2011) suggested a two step sizes scheme with τ for updating the primal variable and σ for updating the dual variable. Depending on the nature of applications, one may observe better performances by carefully choosing the ratio between the two step sizes provided that σ and τ satisfy στ ≤ 1/c. In the last subsection, we observe the improved performance for solving SVM by using the two step sizes scheme and by carefully tuning the ratio between the two step sizes. Furthermore, (Pock and Chambolle, 2011) presents a technique for computing diagonal preconditioners in the cases when estimating the value of c is difficult for complex problems, and applies it to general linear programing problems and some computer vision problems.\nFinally, we discuss the two implementation schemes for Algorithms 2 and 3. Note that in Algorithm 2, we maintain and update two primal variables wt,ut ∈ Rd, while in Algorithm 3 we maintain and update two dual variables αt,βt ∈ Rn. We refer to the implementation with two primal variables as double-primal implementation and the one with two dual variables as double-dual implementation. In fact, we can also implement Algorithm 2 by double-dual implementation and implement Algorithm 3 by double-primal implementation. For Algorithm 2, in which the updates are\nwt = min w∈Qw ‖w− (ut−1 − γGw(αt−1))‖22 2 + γλR(w)\nαt = min α∈Qα ‖α− (αt−1 + γGα(wt))‖22 2 ut = wt + γ(Gw(αt−1)−Gw(αt)),\nwe can plug the expression of ut into wt and obtain\nwt = min w∈Qw ‖w − (wt−1 + 2γGw(αt−2)− 2γGw(αt−1))‖22 2 + γλR(w)\nαt = min α∈Qα ‖α− (αt−1 + γGα(wt))‖22 2\nTo implement above updates, we can only maintain one primal variable and two dual variables. Depending on the nature of implementation, one may be better than the other. For example, if the number of examples n is much larger than the number of dimensions d, the double-primal implementation may be more efficient than the double-dual implementation, and vice versa. In subsection 5.7, we provide more examples and an experiment to demonstrate this."
    }, {
      "heading" : "4.5 Extensions and Discussion",
      "text" : "Nonlinear model For a nonlinear model, the min-max formulation becomes\nmin g∈Hκ max α∈Qα L(g,α;X,y) + λR(g),\nwhere Hκ is a Reproducing Kernel Hilbert Space (RKHS) endowed with a kernel function κ(·, ·). Algorithm 1 can be applied to obtain the nonlinear model by changing the primal\nvariable to g. For example, step 5 in Algorithm 1 is modified to the following composite gradient mapping\ngt = arg min g∈Hκ\n1 2 ‖g − ĝt−1‖2Hκ + γλR(g), (18)\nwhere\nĝt−1 = (gt−1 − γ∇gL(gt−1,αt;X,y)) .\nSimilar changes can be made to Algorithm 2 for the extension to the nonlinear model. To end this discussion, we make several remarks. (1) The gradient with respect to the primal variable (i.e., the kernel predictor g ∈ Hκ) is computed on each g(xi) = 〈g, κ(xi, ·)〉 by κ(xi, ·). (2) We can perform the computation by manipulating on a finite number of parameters due to the representer theorem provided that the regularizerR(g) is a monotonic norm (Bach et al., 2011). Therefore, we only need to maintain and update the coefficients ζ = (ζ1, . . . , ζn) in g = ∑n i=1 ζiκ(xi, ·). (3) The primal dual prox method for optimization with nonlinear model has been adopted in our prior work (Yang et al., 2012) for multiple kernel learning where the regularizer is R(g1, . . . , gm) = ( ∑m k=1 ‖gk‖Hk)2. It can also be generalized to solve MKL with more general sparsity-induced norms. ((Bach et al., 2011) considers how to compute the proximal mapping in (18) for more general sparsity induced norms.)\nt\nIncorporating the bias term It is easy to learn a bias term w0 in the classifier w⊤x+w0 by Pdprox without too many changes. We can use the augmented feature vector\nx̂i = (\n1 xi\n) and the augmented weight vector ŵ = ( w0 w ) , and run Algorithms 1 or 2 with\nno changes except that the regularizer R(ŵ) = R(w) does not involve w0 and the step size γ = √ 1/(2c) will be a different value due to the change in the bound of the new feature vectors by ‖x̂‖2 ≤ √ 1 +R2, which would yield a different value of c in Lemma 3 (c.f. Appendix A).\nDomain constraint on primal variable Now we discuss how to generalize the convergence analysis to the case when a convex domain Qw is imposed on w. We introduce R̂(w) = λR(w) +Q(w), where Q(w) is an indicator function for w ∈ Qw, i.e.\nQ(w) =\n{ 0 w ∈ Qw\n+∞ otherwise .\nThen we can write the domain constrained composite gradient mapping in step 5 of Algorithm 1 or step 4 of Algorithm 2 into a domain free composite gradient mapping as the following:\nwt = arg min w∈Rd\n1 2 ‖w− (wt−1 − γGw(wt−1,αt))‖22 + γR̂(w),\nwt = arg min w∈Rd\n1 2 ‖w− (ut−1 − γGw(ut−1,αt−1))‖22 + γR̂(w).\nThen we have an equivalent gradient mapping,\nwt = wt−1 − γGw(wt−1,αt)− γ∂R̂(wt), wt = ut−1 − γGw(ut−1,αt−1)− γ∂R̂(wt).\nThen Lemmas 9 and 10, and Lemmas 11 and 12 all hold as long as we replace λvt with v̂t ∈ ∂R̂(wt). Finally in proving Theorems 6 we can absorb Q(w) in L(w,α) + R̂(w) into the domain constraint.\nAdditional constraints on dual variables One advantage of the proposed primal dual prox method is that it provides a convenient way to handle additional constraints on the dual variables α. Several studies introduce additional constraints on the dual variables. In (Dekel and Singer, 2006), the authors address a budget SVM problem by introducing a 1 − ∞ interpolation norm on the empirical hinge loss, leading to a sparsity constraint ‖α‖1 ≤ m on the dual variables, where m is the target number of support vectors. The corresponding optimization problem is given by\nmin w∈Rd max α∈[0,1]n,‖α‖1≤m\n1\nn\nn∑\ni=1\nαi(1 − yiw⊤xi) + λR(w). (19)\nIn (Huang et al., 2010), a similar idea is applied to learn a distance metric from noisy training examples. We can directly apply Algorithms 1 or 2 to (19) with Qα given by Qα = {α : α ∈ [0, 1]n, ‖α‖1 ≤ m}. The prox mapping to this domain can be efficiently computed by Lemma 13. It is straightforward to show that the convergence rate is [D1 +m]/[ √ 2nT ] in this case."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section we present empirical studies to verify the efficiency of the proposed algorithm. We organize our experiments as follows.\n• In subsections 5.1, 5.2, and 5.3 we compare the proposed algorithm to the state-of-the-art first order methods that directly update the primal variable at each iteration. We apply all the algorithms to three different tasks with different non-smooth loss functions and regularizers. The baseline first order methods used in this study include the gradient descent algorithm (gd), the forward and backward splitting algorithm (fobos) (Duchi and Singer, 2009), the regularized dual averaging algorithm (rda) (Xiao, 2009), the accelerated gradient descent algorithm (agd) (Chen et al., 2009). Since the proposed method is a non-stochastic method, we compare it to the non-stochastic variant of gd, fobos, and rda. Note that gd, fobos, rda, and agd share the same convergence rate of O(1/ √ T ) for non-smooth problems.\n• In subsection 5.4, our algorithm is compared to the state-of-the-art primal dual gradient method (Nesterov, 2005b), which employs an excessive gap technique for non-smooth optimization, updates both the primal and dual variables at each iteration, and has a convergence rate of O(1/T ).\n• In subsection 5.5, we test the proposed algorithm for optimizing problem in (19) with a sparsity constraint on the dual variables.\n• In subsection 5.7, we compare the two variants of the proposed method on a data set when n ≫ d, and compare Pdprox to the Pegasos algorithm.\nAll the algorithms are implemented in Matlab (except otherwise mentioned) and run on a 2.4GHZ machine. Since the performance of the baseline algorithms gd, fobos and rda depends heavily on the initial value of the stepsize, we generate 21 values for the initial stepsize by scaling their theoretically optimal values with factors 2[−10:1:10], and report the best convergence among the 21 possible values. The stepsize of agd is changed adaptively in the optimization process, and we just give it an appropriate initial step size. Since in the first four subsections we focus on comparison with baselines, we use the Pdprox-dual algorithm (Algorithm 1) of the proposed Pdprox method. We also use the tuning technique to select the best scale-up factor for the step size γ of Pdprox. Finally, all algorithms are initialized with a solution of all zeros."
    }, {
      "heading" : "5.1 Group lasso regularizer for Grouped Feature Selection",
      "text" : "In this experiment we use the group lasso for regularization, i.e., R(w) = ∑\ng\n√ dg‖wg‖2,\nwhere wg corresponds to the gth group variables and dg is the number of variables in group g. To apply Nesterov’s method, we can write R(w) = max‖ug‖2≤1 ∑ g √ dgw ⊤ g ug. We use the MEMset Donar dataset (Yeo and Burge, 2003) as the testbed. This dataset was originally used for splice site detection. It is divided into a training set and a test set: the training set consists of 8, 415 true and 179, 438 false donor sites, and the testing set has 4, 208 true and 89, 717 false donor sites. Each example in this dataset was originally described by a sequence of {A, C, G, T} of length 7. We follow (Yang et al., 2010) and generate group features with up to three-way interactions between the 7 positions, leading to 2, 604 attributes in 63 groups. We normalize the length of each example to 1. Following the experimental setup in (Yang et al., 2010), we construct a balanced training dataset consisting of all 8, 415 true and 8, 415 false donor sites that are randomly sampled from all 179, 438 false sites.\nTwo non-smooth loss functions are examined in this experiment: hinge loss and absolute loss. Figure 2 plots the values of the objective function vs. running time (second), using two different values of regularization parameter, i.e., λ = 10−3, 10−5 to produce different levels\nof sparsity. We observe that (i) the proposed algorithm Pdprox clearly outperforms all the baseline algorithms in all the cases; (ii) for the absolute loss, which has a sharp curvature change at zero compared to hinge loss, the baseline algorithms of gd, fobos, rda, agd, especially of agd that is originally designed for smooth loss functions, deteriorate significantly compared to the proposed algorithm Pdprox. Finally, we observe that for the hinge loss and λ = 10−3, the classification performance of the proposed algorithm on the testing dataset is 0.6565, measured by maximum correlation coefficient (Yeo and Burge, 2003). This is almost identical to the best result reported in (Yang et al., 2010) (i.e., 0.6520)."
    }, {
      "heading" : "5.2 ℓ1,∞ regularization for Multi-Task Learning",
      "text" : "In this experiment, we perform multi-task regression with ℓ1,∞ regularizer (Chen et al., 2009). Let W = (w1, · · · ,wk) ∈ Rd×k denote the k linear hypotheses for regression. The ℓ1,∞ regularizer is given by R(W) = ∑d j=1 ‖wj‖∞, where wj is the jth row ofW. To apply Nesterov’s method, we rewrite the ℓ1,∞ regularizer as R(W) = max‖uj‖1≤1 ∑d j=1 uj ⊤wj . We use the School data set (Argyriou et al., 2008), a common dataset for multi-task learn-\ning. This data set contains the examination scores of 15, 362 students from 139 secondary schools corresponding to 139 tasks, one for each school. Each student in this dataset is described by 27 attributes. We follow the setup in (Argyriou et al., 2008), and generate a training data set with 75% of the examples from each school and a testing data set with the remaining examples. We test the algorithms using both the absolute loss and the ǫ-insensitive loss with ǫ = 0.01. The initial stepsize for gd, fobos, and rda are tuned similarly as that for the experiment of group lasso. We plot the objective versus the running time in Figure 3, from which we observe the similar results in the group feature selection task, i.e. (i) the proposed Pdprox algorithm outperforms the baseline algorithms, (ii) the baseline algorithm of agd becomes even worse for ǫ-insensitive loss than for absolute loss. Finally, we observe that the regression performance measured by root mean square error (RMSE) on the testing data set for absolute loss and ǫ-insensitive loss is 10.34 (optimized by Pdprox), comparable to the performance reported in (Chen et al., 2009)."
    }, {
      "heading" : "5.3 Trace norm regularization for Max-Margin Matrix Factorization/ Matrix Completion",
      "text" : "In this experiment, we evaluate the proposed method using trace norm regularization, a regularizer often used in max-margin matrix factorization and matrix completion, where the goal is to recover a full matrix X from partially observed matrix Y. The objective is composed of a loss function measuring the difference between X and Y on the observed entries and a trace norm regularizer on X, assuming that X is low rank. Hinge loss function is used in max-margin matrix factorization (Rennie and Srebro, 2005; Srebro et al., 2005), and absolute loss is used instead of square loss in matrix completion. We test on 100K MovieLens data set 3 that contains 1 million ratings from 943 users on 1682 movies. Since there are five distinct ratings that can be assigned to each movie, we follow (Rennie and Srebro, 2005; Srebro et al., 2005) by introducing four thresholds θ1,2,3,4 to measure the hinge loss between the predicted value Xij and the ground truth Yij . Because our goal is to demonstrate the efficiency of the proposed algorithm for non-smooth optimization, therefore we simply set θ1,2,3,4 = (0, 3, 6, 9). Note that we did not compare to the optimization algorithm in (Rennie and Srebro, 2005) since it cast the problem into a non-convex problem by using explicit factorization of X = UV⊤, which suffers a local minimum, and the optimization algorithm in (Srebro et al., 2005) since it formulated the problem into a SDP problem, which suffers from a high computational cost. To apply Nesterov’s method, we write ‖X‖1 = max‖A‖≤1 tr(A⊤X), and at each iteration we need to solve a maximization problem max‖A‖≤1 λtr(A\n⊤X) − µ2 ‖A‖2F , where ‖A‖ is the spectral norm on A. The solution of this optimization is obtained by performing SVD decomposition of X and thresholding the singular values appropriately. Since MovieLens data set is much larger than the data sets used in last two subsections, in this experiment, we (i) run all the algorithms for 1000 iterations and plot the objective versus time; (ii) enlarge the range of tuning parameters to 2[−15:1:15]. The results are shown in Figure 4, from which we observe that (i) Pdprox can quickly reduce the objective in a small amount of time, e.g., for absolute loss when setting λ = 10−3 in order to obtain a solution with an accuracy of 10−3, Pdprox needs 103 second, while agd needs 3.2 × 104 seconds; (ii) for absolute loss no matter how we tune the stepsizes for each baseline algorithm, Pdprox performs the best; and (iii) for hinge loss when λ = 10−5, by tuning the stepsizes of baseline algorithms, gd, fobos, and rda can achieve comparable performance to Pdprox. We note that although agd can achieve smaller objective value than Pdprox at the end of 1000 iterations, however, the objective value is reduced slowly."
    }, {
      "heading" : "5.4 Comparison: Pdprox vs Primal-Dual method with excessive gap technique",
      "text" : "In this section, we compare the proposed primal dual prox method to Nesterov’s primal dual method (Nesterov, 2005b), which is an improvement of his algorithm in (Nesterov, 2005a). The algorithm in (Nesterov, 2005a) for non-smooth optimization suffers a problem of setting the value of smoothing parameter that requires the number of iterations to be fixed in advance. (Nesterov, 2005b) addresses the problem by exploring an excessive gap technique and updating both the primal and dual variables, which is similar to the proposed Pdprox method. We refer to this baseline as Pdexg. We run both algorithms on the three tasks as in subsections 5.1, 5.2, and 5.3, i.e., group feature selection with hinge loss and group lasso regularizer on MEMset Donar data set, multi-task learning with ǫ-insensitive loss and ℓ1,∞ regularizer on School data set, and matrix completion with absolute loss and trace norm regularizer on 100K MovieLens data set. To implement the primal dual method with excessive gap technique, we need to intentionally add a domain on the optimal primal\n3. http://www.cs.umn.edu/Research/GroupLens/\nvariable, which can be derived from the formulation. For example, in group feature selection problem whose objective is 1/n ∑n i=1 ℓ(w ⊤xi, yi) + λ ∑ g √ dg‖wg‖2, we can derive that\nthe optimal primal variable w∗ lies in ‖w‖2 ≤ ∑\ng ‖wg‖2 ≤ 1λ√dmin , where dmin = ming dg. Similar techniques are applied to multi-task learning and matrix completion.\nThe performance of the two algorithms on the three tasks is shown in Figure 5. Since both algorithms are in the same category, i.e. updating both primal and dual variables at each iteration and having a convergence rate in the order of O(1/T ), we also plot the objective versus the number of iterations in the bottom panels of each subfigure in Figure 5. The results show that the proposed Pdprox method converges faster than Pdexg onMEMset Donar data set for group feature selection with hinge loss and group lasso regularizer, and on 100K MovieLens data set for matrix completion with absolute loss and trace norm regularizer. However, Pdexg performs better on School data set for multi-task learning with ǫ-insensitive loss and ℓ1,∞ regularizer. One interesting phenomenon we can observe from Figure 5 is that for larger values of λ (e.g., 10−3), the improvement of Pdprox over Pdexg is also larger. The reason is that the proposed Pdprox captures the sparsity of primal variable at each iteration. This does not hold for Pdexg because it casts the non-smooth regularizer into a dual form and consequently does not explore the sparsity of the primal variable at each iteration. Therefore the larger of λ, the sparser of the primal variable at each iteration in Pdprox that yields to larger improvement over Pdexg. For the example of group feature selection task with hinge loss and group lasso regularizer, when setting λ = 10−3, the sparsity of the primal variable (i.e., the proportion of the number of group features with zero norm) in Pdprox averaged over all iterations is 0.7886. However, by reducing λ to 10−5 the average sparsity of the primal variable in Pdprox is reduced to 0. In both settings the average sparsity of the primal variable in Pdexg is 0. The same argument also explains why Pdprox does not perform as well as Pdexg on School data set when setting λ = 10−5, since in this case the primal variables in both algorithms are not sparse. When setting λ = 10−3, the average sparsity (i.e., the proportion of the number of features with zero norm across all tasks) of the primal variable in Pdprox and Pdexg is 0.3766 and 0, respectively. Finally, we also observe similar performance of the two algorithms on the three tasks with other loss functions including absolute loss for group feature selection, absolute loss for multi-task learning, and hinge loss for max-margin matrix factorization."
    }, {
      "heading" : "5.5 Sparsity constraint on the dual variables",
      "text" : "In this subsection, we examine empirically the proposed algorithm for optimizing the problem in equation (19), in which a sparsity constraint is introduced for the dual variables.\nWe test the algorithm on three large data sets from the UCI repository, namely, a9a, rcv1(binary) and covtye4. In the experiments we use ℓ22 regularizer and fix λ = 1/n. First, we run the proposed algorithm 100 seconds on the three data sets with different values of m = 100, 200, 400 and plot the objective versus the number of iterations. The results are shown in Figure 6, which verify that the convergence is faster with smaller m, which is consistent with the convergence bound O([D + m]/[ √ 2nλ]) of the proposed algorithm for (19).\nSecond, we demonstrate that the formulation in equation (19) with a sparsity constraint on the dual variables is useful in the case when labels are contaminated with noise. To generate the noise in labels, we randomly flip the labels with a probability 0.2. We run both the proposed algorithm for (19) and Liblinear5 on the training data with noise added to the labels. The stopping criterion for the proposed algorithm is when duality gap is less than 10−3, and for Liblinear is when the maximal dual violation is less than 10−3. The running time and accuracy on testing data averaged over 5 random trials are reported in Table 1, which demonstrate that in the presence of noise in labels, by adding a sparsity constraint on the dual variables, we are able to obtain better performance than Liblinear trained on the noisily labeled data. Furthermore the running time of Pdprox is comparable to, if not less than, that of Liblinear.\nFinally, we note that choosing a small m in equation (19) is different from simply training a classifier with a small number of examples. For instance, for rcv1, we have run the experiment with 200 training examples, randomly selected from the entire data set. With the same stopping criterion, the testing performance is 0.8131(±0.05), significantly lower than that of optimizing (19) with m = 200."
    }, {
      "heading" : "5.6 Comparison: double-primal vs double-dual implementation",
      "text" : "From the discussion in subsection 4.4, we have seen that both Pdprox-primal and Pdproxdual algorithm can be implemented either by maintaining two dual variables, to which we refer as double-dual implementation, or by maintaining two primal variables, to which we refer as double-primal implementation. One implementation could be more efficient than the other implementation, depending on the nature of applications. For example, in multitask regression with ℓ2 loss (Nie et al., 2010), if the number of examples is much larger than the number of attributes, i.e., n ≫ d, and the number of tasks K is large, then the size of dual variable α ∈ Rn×K is much larger than the size of primal variable W ∈ Rd×K . It would be expected that the double-primal implementation is more efficient than the doubledual implementation. In contrast, in matrix completion with absolute loss, if the number of\n4. http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets 5. http://www.csie.ntu.edu.tw/~cjlin/liblinear\nobserved entries |Ω| which corresponds to the size of dual variable is much less than the total number of entries n2 which corresponds to the size of primal variable, then the double-dual implementation would be more efficient than the double-primal implementation.\nIn the following experiment, we restrict our demonstration to a binary classification problem that given a set of training examples (xi, yi), i = 1, . . . , n, where xi ∈ Rd, one aims to learn a prediction model w ∈ Rd. We choose web spam data set 6 as the testing bed, which contains 350000 examples, and 16609143 trigrams extracted for each example. We use hinge loss and ℓ22 regularizer with λ = 1/n, where n is the number of experimented data.\nWe demonstrate that when d ≫ n, the double-dual implementation is more efficient than double-primal implementation. For the purpose of demonstration, we randomly sample from the whole data a subset of n = 1000 examples, which have a total of 8287348 features, and we solve the sub-optimization problem over the subset. It is worth noting that such kind of problem appears commonly in distributed computing on individual nodes when the number of attributes is huge. The objective value versus running time of the two implementations of Pdprox-dual are plotted in Figure 7, which shows that doubledual implementation is more efficient than double-primal implementation is this case. As a complement, we also plot the objective of Pdprox-dual and Pdprox-primal both with double-dual implementation, which shows that Pdprox-primal and Pdprox-dual performs similarly."
    }, {
      "heading" : "5.7 Comparison for solving ℓ22 regularized SVM",
      "text" : "In this subsection, we compare the proposed Pdprox method with Pegasos for solving ℓ22 regularized SVM when λ = O(n\n−1/(1+ǫ), ǫ ∈ (0, 1]. We also compare Pdprox using one step size and two step sizes, and compare them to the accelerated version proposed in (Chambolle and Pock, 2011) for strongly convex functions. We implement Pdprox-dual algorithm (by double-dual implementation) in C++ using the same data structures as coded by Shai Shalev-Shwartz 7.\nFigure 8 shows the comparison of Pdprox vs. Pegasos on covtype data set with three different levels of λ = n−0.5, n−0.8, n−1. We compute the objective value of Pdprox after each iteration and compute the objective value of Pegasos after one effective pass of all data (i.e., n number of iterations where n is the total number of training examples). We also compare the one step size scheme (Pdprox (γ)) with the two step sizes scheme (Pdprox\n6. http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html 7. http://www.cs.huji.ac.il/~shais/code/index.html\n(τ, σ)) and the accelerated version (Pdprox-ac(τ, σ)) proposed in (Chambolle and Pock, 2011) for strongly convex functions. The relative ratio between the step size τ for updating the primal variable and the step size σ for updating the dual variable is selected among a set of values {1000, 100, 10, 1, 0.1, 0.01, 0.001}.\nThe results demonstrate that (1) the two step sizes scheme with careful tuning of the relative ratio yields better convergences than the one step size scheme; (2) Pegasos still remains a state-of-the-art algorithm for solving the ℓ22 regularized SVM; but when the problem is relatively difficult, i.e., λ is relatively small (e.g., less than 1/n), the Pdprox algorithm with the two step sizes may converge faster in terms of running time; (3) the accelerated version for solving SVM is almost identical the basic version."
    }, {
      "heading" : "6. Conclusions",
      "text" : "In this paper, we study non-smooth optimization in machine learning where both the loss function and the regularizer are non-smooth. We develop an efficient gradient based method for a family of non-smooth optimization problems in which the dual form of the loss function can be expressed as a bilinear function in primal and dual variables. We show that, assuming the proximal step can be efficiently solved, the proposed algorithm achieves a convergence rate of O(1/T ), faster than O(1/ √ T ) suffered by many other first order methods for non-smooth optimization. In contrast to existing studies on non-smooth optimization, our work enjoys more simplicity in implementation and analysis, and provides a unified methodology for a diverse set of non-smooth optimization problems. Our empirical studies demonstrate the efficiency of the proposed algorithm in comparison with the state-of-theart first order methods for solving many non-smooth machine learning problems, and the effectiveness of the proposed algorithm for optimizing the problem with a sparse constraint on the dual variables for tackling the noise in labels. In future, we plan to adapt the proposed algorithm for stochastic updating and for distributed computing environments."
    }, {
      "heading" : "Appendix A. Derivation of constant c for (generalized) hinge loss",
      "text" : "As mentioned before, it is easy to derive the constant c in equations (6) and (7) for the nonsmooth loss functions listed before under the assumption that ‖x‖2 ≤ R. As an example, here we derive the constant for hinge loss and generalized hinge loss. For other non-smooth loss functions, we can derive the value of c in a similar way. For hinge loss, L(w,α) in (5) is given by\nL(w,α;X,y) = 1\nn\nn∑\ni=1\nαi(1 − yiw⊤xi),\nand its partial gradients are\nGα(w,α) = 1 n 1− 1 n (x1y1, · · · ,xnyn)⊤w, Gw(w,α) = − 1\nn X(α ◦ y),\nwhere 1 denotes a vector of all ones, and ◦ denotes the element-wise product. Then,\n‖Gα(w1,α1)−Gα(w2,α2)‖22 = 1\nn2\nn∑\ni=1\n(w⊤1 xiyi −w⊤2 xiyi)2 ≤ R2\nn ‖w1 −w2‖22,\n‖Gw(w1,α1)−Gw(w2,α2)‖22 = 1\nn2 ∥∥∥∥∥ n∑\ni=1\n(α1i − α2i )yixi ∥∥∥∥∥ 2\n2\n≤ R 2\nn\nn∑\ni=1\n(α1i − α2i )2 = R2\nn ‖α1 −α2‖22,\nwhich implies c = R2/n. For the example of generalized hinge loss, L(w,α) in (5) is\nL(w,α;X,y) = 1\nn\nn∑\ni=1\nα1i (1− ayiw⊤xi) + α2i (1 − yiw⊤xi),\nwhere α = [α1,α2] ∈ Qα = {α : α ∈ [0, 1]n×2,α1 +α2 ≤ 1}, and its partial gradients are\nGα(w,α) = 1 n [1,1]− 1 n [a(x1y1, · · · ,xnyn)⊤w, (x1y1, · · · ,xnyn)⊤w], Gw(w,α) = − 1\nn X(a(α1 ◦ y) +α2 ◦ y),\nwhere 1 denotes a vector of all ones, and ◦ denotes the element-wise product. Then for any w1,w2 and α1 = (α 1,1,α2,1),α2 = (α 1,2,α2,2) ∈ Qα, given ‖x‖2 ≤ R, we have\n‖Gα(w1,α1)−Gα(w2,α2)‖2F = a2 + 1\nn2\nn∑\ni=1\n(w⊤1 xiyi −w⊤2 xiyi)2 ≤ (a2 + 1)R2\nn ‖w1 −w2‖22,\n‖Gw(w1,α1)−Gw(w2,α2)‖22 = 1\nn2 ∥∥∥∥∥ n∑\ni=1\na(α1,1i − α 1,2 i )yixi +\nn∑\ni=1\n(α2,1i − α 2,2 i )yixi ∥∥∥∥∥ 2\n2\n≤ 2a 2R2\nn\nn∑\ni=1\n(α1,1i − α 1,2 i )\n2 + 2R2\nn\nn∑\ni=1\n(α2,1i − α 2,2 i ) 2\n≤ 2a 2R2\nn ‖α1 −α2‖2F ,\nwhich implies c = (a2 + 1)R2/n in equation (6) and c = (2a2R2)/n in equation (7). We can derive the value of c in (6) and (7) similarly for other non-smooth loss functions."
    }, {
      "heading" : "Appendix B. Proof of Lemma 3",
      "text" : "Since\nGα(w,α;X,y) = a(X,y) +H(X,y) ⊤w,\nGw(w,α;X,y) = b(X,y) +H(X,y)α.\nThen\n‖Gα(w1,α1;X,y) −Gα(w2,α2;X,y)‖22 ≤ ‖H(X,y)⊤(w1 −w2)‖22 ≤ c‖w1 −w2‖22, ‖Gw(w1,α1;X,y) −Gw(w2,α2;X,y)‖22 ≤ ‖H(X,y)(α1 −α2)‖22 ≤ c‖α1 −α2‖22,\nwhere we use the assumption ‖H(X,y)‖22 = ‖H(X,y)⊤‖22 ≤ c."
    }, {
      "heading" : "Appendix C. The differences between Algorithm 1",
      "text" : "in (Chambolle and Pock, 2011) and Pdprox-primal algorithm (Algorithm 2) and Pdprox-dual algorithm (Algorithm 3)\nWe make the following correspondences between our notations (appearing the R.H.S of the following equalities) and the notations in (Chambolle and Pock, 2011) (appearing the L.H.S of the following equalities),\nx = w, y = α, x̄ = u G(w) = λR(w) +w⊤b+ IQw (w) F ∗(α) = −α⊤a+ IQα(α) K = H⊤ α⊤H⊤w +w⊤b+α⊤a+ c0 = L(w,α)\nδ = τ = γ\nθ = 1\nwhere we suppress the dependence of a,b, H, c0 on (X,y), and IQ(x) is an indicator function\nIQ(x) =\n{ 0, if x ∈ Q\n+∞, otherwise\nThe problem in (Chambolle and Pock, 2011) is to solve\nmin w max α\nO(w,α) = α⊤H⊤w+G(w) − F ∗(α)\nand the updates in (Chambolle and Pock, 2011) are calculated by\nαt = min α ‖α− (αt−1 + γH⊤ut−1)‖22 2γ + F ∗(α)\nwt = min w ‖w− (wt−1 − γHαt))‖22 2γ +G(w) ut = wt + θ(wt −wt−1)\nor equivalently\nαt = min α∈Qα ‖α− (αt−1 + γ(H⊤ut−1 + a))‖22 2γ\nwt = min w∈Qw ‖w− (wt−1 − γ(Hαt + b))‖22 2γ + λR(w) ut = wt + θ(wt −wt−1)\nNote that the partial gradients of L(w,α) are Gw(w,α) = Gw(α) = Hα + b and Gα(w,α) = Gα(w) = H ⊤w + a 8, then we can write the above updates as\nαt = min α∈Qα ‖α− (αt−1 + γGα(ut−1))‖22 2\nwt = min w∈Qw ‖w− (wt−1 − γGw(αt))‖22 2 + γλR(w) ut = wt + θ(wt −wt−1)\n8. We use Gw and Gα to denote partial gradients.\nHowever the updates of Pdprox-primal algorithm (Algorithm 2) in our paper are\nwt = min w∈Qw ‖w− (ut−1 − γGw(αt−1))‖22 2 + γλR(w)\nαt = min α∈Qα ‖α− (αt−1 + γGα(wt))‖22 2\nut = wt + γ(Gw(αt−1)−Gw(αt)) If we remove the extra primal variable ut, we have the following updates of Algorithm 1 in (Chambolle and Pock, 2011):\nαt = min α∈Qα ‖α− (αt−1 + γGα(2wt−1 −wt−2))‖22 2\nwt = min w∈Qw ‖w− (wt−1 − γGw(αt))‖22 2 + γλR(w)\n(20)\nand the following updates of the Pdprox-primal algorithm:\nwt = min w∈Qw ‖w − (wt−1 − γGw(2αt−1 −αt−2))‖22 2 + γλR(w)\nαt = min α∈Qα ‖α− (αt−1 + γGα(wt))‖22 2\n(21)\nWe can clearly see the difference between our updates and the updates of Algorithm 1 in (Chambolle and Pock, 2011), which lies in the order of updating on the primal variable and the dual variable, and the gradients used in the updating as well. On the other hand, if we remove the extra dual variable in Algorithm 3, the updates are the same to that of Algorithm in (Chambolle and Pock, 2011), i.e.,\nαt = min α∈Qα ‖α− (αt−1 + γ(2Gα(wt−1)−Gα(wt−2))‖22 2\nwt = min w∈Qw ‖w− (wt−1 − γGw(αt))‖22 2 + γλR(w)\n(22)\nby noting that Gα(w) is linear in w. It is also worth noting that Pdprox-primal can be implemented by maintaing one primal variable and two dual variables as in (20), and similarly Pdprox-dual can be implemented by maintaing two primal variables and one dual variable as in (21). Depending on the nature of applications, we can choose different implementations for Pdprox-primal or Pdprox-dual to achieve better efficiency."
    }, {
      "heading" : "Appendix D. Proof of Lemma 10",
      "text" : "In order to prove Lemma 10, we first present the following lemma with its proof.\nLemma 17 Let Z be a convex compact set, and U ⊆ Z be convex and closed, z0 ∈ Z, and γ > 0. Considering the following points with fixed η, ξ,\nzh = argmin z∈U\n1 2 ‖z− (z0 − γξ)‖22,\nz1 = argmin z∈U\n1 2 ‖z− (z0 − γη)‖22,\nthen for any z ∈ U , we have\nγη⊤(zh − z) ≤ 1\n2 ‖z− z0‖2 −\n1 2 ‖z− z1‖2 + γ2‖ξ − η‖22 − 1 2\n[ ‖zh − z0‖22 + ‖zh − z1‖22 ] .\nEquipped with above lemma, it is straightforward to prove Lemma 10. We note that the two updates in Lemma 9 are the same as the two updates in Lemma 17 if we make the following correspondences:\nU = Z = Rd ×Qα, z = (w α ) ∈ U,\nz0 = ( ut−1 βt−1 ) , zh = ( wt αt ) , z1 = ( ut βt ) ,\nξ = ( Gw(ut−1,αt) + λvt −Gα(ut−1,βt−1) ) , η = ( Gw(wt,αt) + λvt −Gα(wt,αt) ) .\nThen the inequality in Lemma 10 follows immediately the inequality in Lemma 17, which is stated explicitly again:\nγ\n( Gw(wt,αt) + λvt\n−Gα(wt,αt) )⊤ ( wt −w αt −α ) ≤ 1 2 ∥∥∥∥ ( w − ut−1 α− βt−1 )∥∥∥∥ 2\n2\n− 1 2 ∥∥∥∥ ( w− ut α− βt )∥∥∥∥ 2\n2\n+ γ2 ∥∥Gα(wt,αt)−Gα(ut−1,βt−1) ∥∥2 2 − 1\n2  ‖wt − ut−1‖22 + ‖αt − βt−1‖22 + ‖wt − ut‖22 + ‖αt − βt‖22︸ ︷︷ ︸\n≥0\n  .\nLemma 17 is a special case of Lemma 3.1 (Nemirovski, 2005) for Euclidean norm. A proof is provided here for completeness.\nProof [of Lemma 17] Since\nzh = argmin z∈U\n1 2 ‖z− (z0 − γξ)‖22,\nz1 = argmin z∈U\n1 2 ‖z− (z0 − γη)‖22,\nby the first order optimality condition, we have\n(z − zh)⊤(γξ − z0 + zh) ≥ 0, ∀z ∈ U, (23) (z − z1)⊤(γη − z0 + z1) ≥ 0, ∀z ∈ U. (24)\nApplying (23) with z = z1 and (24) with z = zh, we get\nγ(zh − z1)⊤ξ ≤ (z0 − zh)⊤(zh − z1), γ(z1 − zh)⊤η ≤ (z0 − z1)⊤(z1 − zh).\nSumming up the two inequalities, we have\nγ(zh − z1)⊤(ξ − η) ≤ (z1 − zh)⊤(zh − z1) = −‖z1 − zh‖22.\nThen\nγ‖ξ − η‖2‖zh − z1‖2 ≥ −γ(zh − z1)⊤(ξ − η) ≥ ‖z1 − zh‖22. (25)\nWe continue the proof as follows:\n1 2 ‖z− z0‖22 − 1 2 ‖z− z1‖22\n= 1\n2 ‖z1‖22 −\n1 2 ‖z0‖22 − (z1 − z0)⊤z0 + (z− z1)⊤(z1 − z0)\n= 1\n2 ‖z1‖22 −\n1 2 ‖z0‖22 − (z1 − z0)⊤z0 + (z− z1)⊤(γη + z1 − z0)− (z− z1)⊤γη\n≥1 2 ‖z1‖22 − 1 2 ‖z0‖22 − (z1 − z0)⊤z0 − (z− z1)⊤γη = 1\n2 ‖z1‖22 −\n1 2 ‖z0‖22 − (z1 − z0)⊤z0 − (zh − z1)⊤γη\n︸ ︷︷ ︸ ǫ\n+(zh − z)⊤γη,\nwhere the inequality follows (24).\nǫ = 1\n2 ‖z1‖22 −\n1 2 ‖z0‖22 − (z1 − z0)⊤z0 − (zh − z1)⊤γη\n= 1\n2 ‖z1‖22 −\n1 2 ‖z0‖22 − (z1 − z0)⊤z0 − (zh − z1)⊤γ(η − ξ)− (zh − z1)⊤γξ\n= 1\n2 ‖z1‖22 −\n1 2 ‖z0‖22 − (z1 − z0)⊤z0 − (zh − z1)⊤γ(η − ξ)\n+ (z1 − zh)⊤(γξ − z0 + zh)− (z1 − zh)⊤(zh − z0)\n≥1 2 ‖z1‖22 − 1 2 ‖z0‖22 − (z1 − z0)⊤z0 − (zh − z1)⊤γ(η − ξ)− (z1 − zh)⊤(zh − z0) = 1\n2 ‖z1‖22 −\n1 2 ‖z0‖22 − (zh − z0)⊤z0 − (zh − z1)⊤γ(η − ξ)− (z1 − zh)⊤zh\n=\n[ 1\n2 ‖z1‖22 −\n1 2 ‖zh‖22 − (z1 − zh)⊤zh\n] + [ 1\n2 ‖zh‖22 −\n1 2 ‖z0‖22 − (zh − z0)⊤z0\n]\n− (zh − z1)⊤γ(η − ξ)\n≥1 2 ‖zh − z1‖22 + 1 2 ‖zh − z0‖22 − γ‖zh − z1‖2‖η − ξ‖2 ≥1 2 {‖zh − z1‖2 + ‖zh − z0‖2} − γ2‖η − ξ‖22,\nwhere the first inequality follows (23), and the last inequality follows (25). Combining the above results, we have\nγ(zh − z)⊤η ≤ 1\n2 ‖z− z0‖22 −\n1 2 ‖z− z1‖22 + γ2‖η − ξ‖22 − 1 2 {‖zh − z1‖22 + ‖zh − z0‖22}."
    }, {
      "heading" : "Appendix E. Proof of Lemma 13",
      "text" : "By introducing Lagrangian multiplier for constraint ∑\ni αivi ≤ ρ, we have the following min-max problem\nmax η min α∈[0,1]n\n1 2 ‖α− α̂‖2 + η\n( ∑\ni\nαivi − ρ ) .\nThe solution to α is αi = [α̂i − η∗vi][0,1]. By KKT condition, the optimal η∗ is equal to 0 if ∑\ni[α̂i][0,1]vi < ρ, otherwise we have\n∑\ni\n[α̂i − η∗vi][0,1]vi − ρ = 0.\nSince the left side of above equation is a monotonically decreasing function in η∗, we can compute η∗ by efficient bi-section search."
    }, {
      "heading" : "Appendix F. Proof of Lemma 16",
      "text" : "Using the convex conjugate V∗(η) of V (z), the composite mapping can be written as\nmin w\n1 2 ‖w− ŵ‖22 + λmaxη (η‖w‖ − V∗(η)) .\nThe problem is equivalent to maximize the following function on η,\n( min w 1 2 ‖w− ŵ‖22 + λη‖w‖ ) − λV∗(η).\nLet w(η) denote the solution to the minimization problem. Then the optimal solution of η satisfies\nλ‖w(η)‖ − λV ′∗(η) = 0,\ni.e.\n‖w(η)‖ − V ′∗(η) = 0.\nIt is easy to show that ‖w(η)‖ is a non-increasing function in η. Similarly, since V∗(η) is a convex function, its negative gradient −V ′∗(η) is a non-increasing function. Therefore, we can compute the optimal η by bi-section search."
    } ],
    "references" : [ {
      "title" : "Convex multi-task feature learning",
      "author" : [ "Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Argyriou et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Argyriou et al\\.",
      "year" : 2008
    }, {
      "title" : "Optimization with Sparsity-Inducing Penalties (Foundations and Trends(R) in Machine Learning)",
      "author" : [ "Francis Bach", "Rodolphe Jenatton", "Julien Mairal" ],
      "venue" : "Now Publishers Inc.,",
      "citeRegEx" : "Bach et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2011
    }, {
      "title" : "Classification with a reject option using a hinge loss",
      "author" : [ "Peter L. Bartlett", "Marten H. Wegkamp" ],
      "venue" : "JMLR, 9:1823–1840,",
      "citeRegEx" : "Bartlett and Wegkamp.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bartlett and Wegkamp.",
      "year" : 2008
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "Amir Beck", "Marc Teboulle" ],
      "venue" : "SIAM J. Img. Sci.,",
      "citeRegEx" : "Beck and Teboulle.,? \\Q2009\\E",
      "shortCiteRegEx" : "Beck and Teboulle.",
      "year" : 2009
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen Boyd", "Lieven Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2004
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein" ],
      "venue" : "Found. Trends Mach. Learn.,",
      "citeRegEx" : "Boyd et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boyd et al\\.",
      "year" : 2011
    }, {
      "title" : "A forward-backward splitting algorithm for the minimization of non-smooth convex functionals in banach space",
      "author" : [ "Kristian Bredies" ],
      "venue" : "Inverse Problems, 25:Article ID 015005,",
      "citeRegEx" : "Bredies.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bredies.",
      "year" : 2009
    }, {
      "title" : "Fast implementation of l1 regularized learning algorithms using gradient descent methods",
      "author" : [ "Yunpeng Cai", "Yijun Sun", "Yubo Cheng", "Jian Li", "Steve Goodison" ],
      "venue" : "In SDM,",
      "citeRegEx" : "Cai et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2010
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J. Candès", "Benjamin Recht" ],
      "venue" : "CoRR, abs/0805.4471,",
      "citeRegEx" : "Candès and Recht.,? \\Q2008\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2008
    }, {
      "title" : "A first-order primal-dual algorithm for convex problems with applications to imaging",
      "author" : [ "Antonin Chambolle", "Thomas Pock" ],
      "venue" : "J. Math. Imaging Vis.,",
      "citeRegEx" : "Chambolle and Pock.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chambolle and Pock.",
      "year" : 2011
    }, {
      "title" : "Accelerated gradient method for multi-task sparse learning problem",
      "author" : [ "Xi Chen", "Weike Pan", "James T. Kwok", "Jaime G. Carbonell" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "Chen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2009
    }, {
      "title" : "Support vector machines on a budget",
      "author" : [ "Ofer Dekel", "Yoram Singer" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Dekel and Singer.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dekel and Singer.",
      "year" : 2006
    }, {
      "title" : "Efficient online and batch learning using forward backward splitting",
      "author" : [ "John Duchi", "Yoram Singer" ],
      "venue" : "JMLR., 10:2899–2934,",
      "citeRegEx" : "Duchi and Singer.,? \\Q2009\\E",
      "shortCiteRegEx" : "Duchi and Singer.",
      "year" : 2009
    }, {
      "title" : "Efficient projections onto the l1-ball for learning in high dimensions",
      "author" : [ "John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Tushar Chandra" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2008
    }, {
      "title" : "A general framework for a class of first order Primal-Dual algorithms for convex optimization in imaging science",
      "author" : [ "Ernie Esser", "Xiaoqun Zhang", "Tony F. Chan" ],
      "venue" : "SIAM J. Imaging Sciences,",
      "citeRegEx" : "Esser et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Esser et al\\.",
      "year" : 2010
    }, {
      "title" : "A feature selection newton method for support vector machine classification",
      "author" : [ "Glenn Fung", "O.L. Mangasarian" ],
      "venue" : "Technical report, Computational Optimization and Applications,",
      "citeRegEx" : "Fung and Mangasarian.,? \\Q2002\\E",
      "shortCiteRegEx" : "Fung and Mangasarian.",
      "year" : 2002
    }, {
      "title" : "The elements of statistical learning: data",
      "author" : [ "Trevor Hastie", "Robert Tibshirani", "Jerome Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2008
    }, {
      "title" : "1/epsilon iteration-complexity for cone programming",
      "author" : [ "Qihang Lin" ],
      "venue" : "Math. Program.,",
      "citeRegEx" : "Lin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 2011
    }, {
      "title" : "A primal-dual algorithm for group",
      "author" : [ "Sofia Mosci", "Silvia Villa", "Alessandro Verri", "Lorenzo Rosasco" ],
      "venue" : null,
      "citeRegEx" : "Mosci et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mosci et al\\.",
      "year" : 2009
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Yu. Nesterov" ],
      "venue" : "Core discussion papers,",
      "citeRegEx" : "Nesterov.,? \\Q2007\\E",
      "shortCiteRegEx" : "Nesterov.",
      "year" : 2007
    }, {
      "title" : "Efficient and robust feature selection via joint 2,1-Norms minimization",
      "author" : [ "Feiping Nie", "Heng Huang", "Xiao Cai", "Chris Ding" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Nie et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2010
    }, {
      "title" : "Fast training of support vector machines using sequential minimal optimization",
      "author" : [ "John C. Platt" ],
      "venue" : "In Advances in Kernel Methods: Support Vector Learning,",
      "citeRegEx" : "Platt.,? \\Q1998\\E",
      "shortCiteRegEx" : "Platt.",
      "year" : 1998
    }, {
      "title" : "Diagonal preconditioning for first order primal-dual algorithms in convex optimization",
      "author" : [ "Thomas Pock", "Antonin Chambolle" ],
      "venue" : "In Proceedings of the 2011 International Conference on Computer Vision,",
      "citeRegEx" : "Pock and Chambolle.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pock and Chambolle.",
      "year" : 2011
    }, {
      "title" : "A modification of the arrow-hurwitz method of search for saddle points",
      "author" : [ "L.D. Popov" ],
      "venue" : "Mat. Zametki,",
      "citeRegEx" : "Popov.,? \\Q1980\\E",
      "shortCiteRegEx" : "Popov.",
      "year" : 1980
    }, {
      "title" : "An efficient projection for l1, infinity regularization",
      "author" : [ "Ariadna Quattoni", "Xavier Carreras", "Michael Collins", "Trevor Darrell" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Quattoni et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Quattoni et al\\.",
      "year" : 2009
    }, {
      "title" : "A primal-dual splitting algorithm for finding zeros of sums of maximally monotone operators",
      "author" : [ "Andre Heinrich Radu loan Bot", "Ernö Robert Csetnek" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Bot and Csetnek.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bot and Csetnek.",
      "year" : 2012
    }, {
      "title" : "Guaranteed Minimum-Rank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "Benjamin Recht", "Maryam Fazel", "Pablo A. Parrilo" ],
      "venue" : "SIAM Rev.,",
      "citeRegEx" : "Recht et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2010
    }, {
      "title" : "Fast maximum margin matrix factorization for collaborative prediction",
      "author" : [ "Jasson D.M. Rennie", "Nathan Srebro" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "Rennie and Srebro.,? \\Q2005\\E",
      "shortCiteRegEx" : "Rennie and Srebro.",
      "year" : 2005
    }, {
      "title" : "Monotone operators and the proximal point algorithm",
      "author" : [ "R. Tyrrell Rockafellar" ],
      "venue" : "SIAM J. on Control and Optimization,",
      "citeRegEx" : "Rockafellar.,? \\Q1976\\E",
      "shortCiteRegEx" : "Rockafellar.",
      "year" : 1976
    }, {
      "title" : "Are loss functions all the same",
      "author" : [ "Lorenzo Rosasco", "Ernesto De Vito", "Andrea Caponnetto", "Michele Piana", "Alessandro Verri" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Rosasco et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Rosasco et al\\.",
      "year" : 2004
    }, {
      "title" : "Pegasos: primal estimated sub-gradient solver for svm",
      "author" : [ "Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter" ],
      "venue" : "Math. Program.,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Estimating the approximation error in learning theory",
      "author" : [ "Steve Smale", "Ding-Xuan Zhou" ],
      "venue" : "Anal. Appl. (Singap.),",
      "citeRegEx" : "Smale and Zhou.,? \\Q2003\\E",
      "shortCiteRegEx" : "Smale and Zhou.",
      "year" : 2003
    }, {
      "title" : "A tutorial on support vector regression",
      "author" : [ "Alex J. Smola", "Bernhard Schölkopf" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "Smola and Schölkopf.,? \\Q2004\\E",
      "shortCiteRegEx" : "Smola and Schölkopf.",
      "year" : 2004
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Srebro et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2005
    }, {
      "title" : "Efficient recovery of jointly sparse vectors",
      "author" : [ "Liang Sun", "Jun Liu", "Jianhui Chen", "Jieping Ye" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sun et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2009
    }, {
      "title" : "On accelerated proximal gradient methods for convex-concave optimization",
      "author" : [ "Paul Tseng" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Tseng.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tseng.",
      "year" : 2008
    }, {
      "title" : "Svm soft margin classifiers: Linear programming versus quadratic",
      "author" : [ "Yang" ],
      "venue" : "V.N. Vapnik. Statistical Learning Theory. Wiley-Interscience,",
      "citeRegEx" : "Yang,? \\Q1998\\E",
      "shortCiteRegEx" : "Yang",
      "year" : 1998
    }, {
      "title" : "Nesvm: a fast gradient method for support vector",
      "author" : [ "Tianyi Zhou", "Dacheng Tao", "Xindong Wu" ],
      "venue" : null,
      "citeRegEx" : "Zhou et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2006
    }, {
      "title" : "1-norm support vector machines",
      "author" : [ "Ji Zhu", "Saharon Rosset", "Trevor Hastie", "Rob Tibshirani" ],
      "venue" : "AISTAT,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2010
    }, {
      "title" : "An efficient primal-dual hybrid gradient algorithm for total variation image",
      "author" : [ "M. Zhu", "T. Chan" ],
      "venue" : null,
      "citeRegEx" : "Zhu and Chan.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhu and Chan.",
      "year" : 2003
    }, {
      "title" : "The differences between Algorithm 1 in (Chambolle and Pock, 2011) and Pdprox-primal algorithm (Algorithm 2) and Pdprox-dual algorithm",
      "author" : [ "C. Appendix" ],
      "venue" : null,
      "citeRegEx" : "Appendix,? \\Q2011\\E",
      "shortCiteRegEx" : "Appendix",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "This formulation includes support vector machine (SVM) (Hastie et al., 2008), support vector regression (Smola and Schölkopf, 2004), Lasso (Zhu et al.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 32,
      "context" : ", 2008), support vector regression (Smola and Schölkopf, 2004), Lasso (Zhu et al.",
      "startOffset" : 35,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : ", 2003), logistic regression, and ridge regression (Hastie et al., 2008) among many others.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 30,
      "context" : "A well-known example is the Pegasos algorithm (Shalev-Shwartz et al., 2011) which minimizes the l2 regularized hinge loss (i.",
      "startOffset" : 46,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "Several other first order algorithms (Ji and Ye, 2009; Chen et al., 2009) are also proposed for smooth loss functions (e.",
      "startOffset" : 37,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "Examples of non-smooth loss functions include hinge loss (Vapnik, 1998), generalized hinge loss (Bartlett and Wegkamp, 2008), absolute loss (Hastie et al.",
      "startOffset" : 96,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "Examples of non-smooth loss functions include hinge loss (Vapnik, 1998), generalized hinge loss (Bartlett and Wegkamp, 2008), absolute loss (Hastie et al., 2008), and ǫ-insensitive loss (Rosasco et al.",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 29,
      "context" : ", 2008), and ǫ-insensitive loss (Rosasco et al., 2004); examples of non-smooth regularizers include lasso (Zhu et al.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 24,
      "context" : ", 2010b), l1,∞ regularizer (Quattoni et al., 2009), and trace norm regularizer (Rennie and Srebro, 2005).",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 27,
      "context" : ", 2009), and trace norm regularizer (Rennie and Srebro, 2005).",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "A number of algorithms have been proposed to minimize the l2 regularized hinge loss (Platt, 1998; Joachims, 1999, 2006; Hsieh et al., 2008; Shalev-Shwartz et al., 2011), and the l1 regularized hinge loss (Cai et al.",
      "startOffset" : 84,
      "endOffset" : 168
    }, {
      "referenceID" : 30,
      "context" : "A number of algorithms have been proposed to minimize the l2 regularized hinge loss (Platt, 1998; Joachims, 1999, 2006; Hsieh et al., 2008; Shalev-Shwartz et al., 2011), and the l1 regularized hinge loss (Cai et al.",
      "startOffset" : 84,
      "endOffset" : 168
    }, {
      "referenceID" : 7,
      "context" : ", 2011), and the l1 regularized hinge loss (Cai et al., 2010; Zhu et al., 2003; Fung and Mangasarian, 2002).",
      "startOffset" : 43,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : ", 2011), and the l1 regularized hinge loss (Cai et al., 2010; Zhu et al., 2003; Fung and Mangasarian, 2002).",
      "startOffset" : 43,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "Besides the hinge loss, recently a generalized hinge loss function (Bartlett and Wegkamp, 2008) has been proposed for cost sensitive learning.",
      "startOffset" : 67,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "However, non-smooth loss functions such as absolute loss (Hastie et al., 2008) and ǫ-insensitive loss (Rosasco et al.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 29,
      "context" : ", 2008) and ǫ-insensitive loss (Rosasco et al., 2004) are useful for robust regression.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : "Therefore absolute loss is more robust for long-tailed error distributions and outliers (Hastie et al., 2008).",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 29,
      "context" : "(Rosasco et al., 2004) also proved that the estimation error bound for absolute loss and ǫ-insensitive loss converges faster than that of square loss.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "Non-smooth regularizers Besides the simple non-smooth regularizers such as l1, l2, and l∞ norms (Duchi and Singer, 2009), many other non-smooth regularizers have been employed in machine learning tasks.",
      "startOffset" : 96,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "The l1,∞ norm regularizer has been used for multi-task learning (Argyriou et al., 2008).",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : "In addition, several recent works (Hou et al., 2011; Nie et al., 2010; Liu et al., 2009) considered mixed l2,1 regularizer for feature selection.",
      "startOffset" : 34,
      "endOffset" : 88
    }, {
      "referenceID" : 26,
      "context" : "Trace norm regularizer is another non-smooth regularizer, which has found applications in matrix completion (Recht et al., 2010; Candès and Recht, 2008), matrix factorization (Rennie and Srebro, 2005; Srebro et al.",
      "startOffset" : 108,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "Trace norm regularizer is another non-smooth regularizer, which has found applications in matrix completion (Recht et al., 2010; Candès and Recht, 2008), matrix factorization (Rennie and Srebro, 2005; Srebro et al.",
      "startOffset" : 108,
      "endOffset" : 152
    }, {
      "referenceID" : 27,
      "context" : ", 2010; Candès and Recht, 2008), matrix factorization (Rennie and Srebro, 2005; Srebro et al., 2005), and multi-task learning (Argyriou et al.",
      "startOffset" : 54,
      "endOffset" : 100
    }, {
      "referenceID" : 33,
      "context" : ", 2010; Candès and Recht, 2008), matrix factorization (Rennie and Srebro, 2005; Srebro et al., 2005), and multi-task learning (Argyriou et al.",
      "startOffset" : 54,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : ", 2005), and multi-task learning (Argyriou et al., 2008; Ji and Ye, 2009).",
      "startOffset" : 33,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "The optimization algorithms presented in these works are usually limited: either the convergence rate is not guaranteed (Argyriou et al., 2008; Recht et al., 2010; Hou et al., 2011; Nie et al., 2010; Rennie and Srebro, 2005; Srebro et al., 2005) or the loss functions are assumed to be smooth (e.",
      "startOffset" : 120,
      "endOffset" : 245
    }, {
      "referenceID" : 26,
      "context" : "The optimization algorithms presented in these works are usually limited: either the convergence rate is not guaranteed (Argyriou et al., 2008; Recht et al., 2010; Hou et al., 2011; Nie et al., 2010; Rennie and Srebro, 2005; Srebro et al., 2005) or the loss functions are assumed to be smooth (e.",
      "startOffset" : 120,
      "endOffset" : 245
    }, {
      "referenceID" : 20,
      "context" : "The optimization algorithms presented in these works are usually limited: either the convergence rate is not guaranteed (Argyriou et al., 2008; Recht et al., 2010; Hou et al., 2011; Nie et al., 2010; Rennie and Srebro, 2005; Srebro et al., 2005) or the loss functions are assumed to be smooth (e.",
      "startOffset" : 120,
      "endOffset" : 245
    }, {
      "referenceID" : 27,
      "context" : "The optimization algorithms presented in these works are usually limited: either the convergence rate is not guaranteed (Argyriou et al., 2008; Recht et al., 2010; Hou et al., 2011; Nie et al., 2010; Rennie and Srebro, 2005; Srebro et al., 2005) or the loss functions are assumed to be smooth (e.",
      "startOffset" : 120,
      "endOffset" : 245
    }, {
      "referenceID" : 33,
      "context" : "The optimization algorithms presented in these works are usually limited: either the convergence rate is not guaranteed (Argyriou et al., 2008; Recht et al., 2010; Hou et al., 2011; Nie et al., 2010; Rennie and Srebro, 2005; Srebro et al., 2005) or the loss functions are assumed to be smooth (e.",
      "startOffset" : 120,
      "endOffset" : 245
    }, {
      "referenceID" : 4,
      "context" : "When the objective function is strongly convex and smooth, it is well known that gradient descent methods can achieve a geometric convergence rate (Boyd and Vandenberghe, 2004).",
      "startOffset" : 147,
      "endOffset" : 176
    }, {
      "referenceID" : 19,
      "context" : "When the objective function is smooth but not strongly convex, the optimal convergence rate of a gradient descent method is O(1/T ), and is achieved by the Nesterov’s methods (Nesterov, 2007).",
      "startOffset" : 175,
      "endOffset" : 191
    }, {
      "referenceID" : 30,
      "context" : "For the objective function which is strongly convex but not smooth, the convergence rate becomes O(1/T ) (Shalev-Shwartz et al., 2011).",
      "startOffset" : 105,
      "endOffset" : 134
    }, {
      "referenceID" : 19,
      "context" : "In addition, several methods are developed for composite optimization, where the objective function is written as a sum of a smooth and a non-smooth function (Lan, 2010; Nesterov, 2007; Lin, 2010).",
      "startOffset" : 158,
      "endOffset" : 196
    }, {
      "referenceID" : 12,
      "context" : ", 2010a), general regularized empirical loss minimization (Duchi and Singer, 2009; Hu et al., 2009), trace norm minimization (Ji and Ye, 2009), and multi-task sparse learning (Chen et al.",
      "startOffset" : 58,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : ", 2009), trace norm minimization (Ji and Ye, 2009), and multi-task sparse learning (Chen et al., 2009).",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 34,
      "context" : "Tseng (2008) and Nemirovski (2005) developed prox methods that have a convergence rate of O(1/T ), provided the gradients are Lipschitz continuous and have been applied to machine learning problems (Sun et al., 2009).",
      "startOffset" : 198,
      "endOffset" : 216
    }, {
      "referenceID" : 14,
      "context" : "It was generalized in (Esser et al., 2010), which shares the similar spirit of the proposed algorithm.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 19,
      "context" : ", 2011) considered the primal-dual convex formulations for general cone programming and apply Nesterov’s optimal first order method (Nesterov, 2007), Nesterov’s smoothing technique (Nesterov, 2005a), and Nemirovski’s prox method (Nemirovski, 2005).",
      "startOffset" : 132,
      "endOffset" : 148
    }, {
      "referenceID" : 9,
      "context" : "Finally we noticed that, as we are preparing our manuscript, a related work (Chambolle and Pock, 2011) has recently been published in the Journal of Mathematical Imaging and Vision that shares a similar idea as this work.",
      "startOffset" : 76,
      "endOffset" : 102
    }, {
      "referenceID" : 9,
      "context" : "However, our work distinguishes from (Chambolle and Pock, 2011) in following aspects: (i) We propose and analyze two primal dual prox methods: one gives an extra gradient updating to dual variables and the other gives an extra gradient updating to primal vari-",
      "startOffset" : 37,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : ", 2009), trace norm minimization (Ji and Ye, 2009), and multi-task sparse learning (Chen et al., 2009). Despite these efforts, one major limitation of the existing (sub)gradient based algorithms is that in order to achieve a convergence rate better than O(1/ √ T ), they have to assume that the loss function is smooth or the regularizer is strongly convex, making them unsuitable for non-smooth optimization. Convex-concave optimization The present work is also related to convex-concave minimization. Tseng (2008) and Nemirovski (2005) developed prox methods that have a convergence rate of O(1/T ), provided the gradients are Lipschitz continuous and have been applied to machine learning problems (Sun et al.",
      "startOffset" : 84,
      "endOffset" : 516
    }, {
      "referenceID" : 9,
      "context" : ", 2009), trace norm minimization (Ji and Ye, 2009), and multi-task sparse learning (Chen et al., 2009). Despite these efforts, one major limitation of the existing (sub)gradient based algorithms is that in order to achieve a convergence rate better than O(1/ √ T ), they have to assume that the loss function is smooth or the regularizer is strongly convex, making them unsuitable for non-smooth optimization. Convex-concave optimization The present work is also related to convex-concave minimization. Tseng (2008) and Nemirovski (2005) developed prox methods that have a convergence rate of O(1/T ), provided the gradients are Lipschitz continuous and have been applied to machine learning problems (Sun et al.",
      "startOffset" : 84,
      "endOffset" : 538
    }, {
      "referenceID" : 9,
      "context" : ", 2009), trace norm minimization (Ji and Ye, 2009), and multi-task sparse learning (Chen et al., 2009). Despite these efforts, one major limitation of the existing (sub)gradient based algorithms is that in order to achieve a convergence rate better than O(1/ √ T ), they have to assume that the loss function is smooth or the regularizer is strongly convex, making them unsuitable for non-smooth optimization. Convex-concave optimization The present work is also related to convex-concave minimization. Tseng (2008) and Nemirovski (2005) developed prox methods that have a convergence rate of O(1/T ), provided the gradients are Lipschitz continuous and have been applied to machine learning problems (Sun et al., 2009). In contrast, our method achieves a rate of O(1/T ) without requiring the whole gradient but part of the gradient to be Lipschitz continuous. Several other primal-dual algorithms have been developed for regularized empirical loss minimization that update both primal and dual variables. (Zhu and Chan, 2008) proposed a primal-dual method based on gradient descent, which only achieves a rate of O(1/ √ T ). It was generalized in (Esser et al., 2010), which shares the similar spirit of the proposed algorithm. However, the explicit convergence rate was not established even though the convergence is proved. (Mosci et al., 2010) presented a primal-dual algorithm for group sparse regularization, which updates the primal variable by a prox method and the dual variable by a Newton’s method. In contrast, the proposed algorithm is a first order method that does not require computing the Hessian matrix as the Newton’s method does, and is therefore more scalable to large datasets. (Combettes and Pesquet; Radu loan Bot, 2012) proposed primal-dual splitting algorithms for finding zeros of maximal monotone operators of special types. (Lan et al., 2011) considered the primal-dual convex formulations for general cone programming and apply Nesterov’s optimal first order method (Nesterov, 2007), Nesterov’s smoothing technique (Nesterov, 2005a), and Nemirovski’s prox method (Nemirovski, 2005). Nesterov (2005b) proposed a primal dual gradient method for a special class of structured non-smooth optimization problems by exploring an excessive gap technique.",
      "startOffset" : 84,
      "endOffset" : 2131
    }, {
      "referenceID" : 9,
      "context" : "In contrast, (Chambolle and Pock, 2011) simply assumes that the interim projection problems can be solved efficiently; (iii) We focus our analysis and empirical studies on the optimization problems that are closely related to machine learning.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "In contrast, the study (Chambolle and Pock, 2011) only considers the application in image problems.",
      "startOffset" : 23,
      "endOffset" : 49
    }, {
      "referenceID" : 28,
      "context" : "We also note that the proposed algorithm is closely related to proximal point algorithm (Rockafellar, 1976) as shown in (He and Yuan, 2012), and many variants including the modified Arrow-Hurwicz method (Popov, 1980), the Doughlas-Rachford (DR) splitting algorithm (Lions and Mercier, 1979), the alternating method of multipliers (ADMM) (Boyd et al.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 23,
      "context" : "We also note that the proposed algorithm is closely related to proximal point algorithm (Rockafellar, 1976) as shown in (He and Yuan, 2012), and many variants including the modified Arrow-Hurwicz method (Popov, 1980), the Doughlas-Rachford (DR) splitting algorithm (Lions and Mercier, 1979), the alternating method of multipliers (ADMM) (Boyd et al.",
      "startOffset" : 203,
      "endOffset" : 216
    }, {
      "referenceID" : 5,
      "context" : "We also note that the proposed algorithm is closely related to proximal point algorithm (Rockafellar, 1976) as shown in (He and Yuan, 2012), and many variants including the modified Arrow-Hurwicz method (Popov, 1980), the Doughlas-Rachford (DR) splitting algorithm (Lions and Mercier, 1979), the alternating method of multipliers (ADMM) (Boyd et al., 2011), the forward-backward splitting algorithm (Bredies, 2009), the FISTA algorithm (Beck and Teboulle, 2009).",
      "startOffset" : 337,
      "endOffset" : 356
    }, {
      "referenceID" : 6,
      "context" : ", 2011), the forward-backward splitting algorithm (Bredies, 2009), the FISTA algorithm (Beck and Teboulle, 2009).",
      "startOffset" : 50,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : ", 2011), the forward-backward splitting algorithm (Bredies, 2009), the FISTA algorithm (Beck and Teboulle, 2009).",
      "startOffset" : 87,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "For a detailed comparison with some of these algorithms, one can refer to (Chambolle and Pock, 2011).",
      "startOffset" : 74,
      "endOffset" : 100
    }, {
      "referenceID" : 2,
      "context" : "• Generalized hinge loss (Bartlett and Wegkamp, 2008):",
      "startOffset" : 25,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : "• Absolute loss (Hastie et al., 2008): l(w;x, y) = |w⊤x− y| = max α∈[−1,1] α(w⊤x− y).",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 29,
      "context" : "• ǫ-insensitive loss (Rosasco et al., 2004) : l(w;x, y) = max(|w⊤x− y| − ǫ, 0) = max α1≥0,α2≥0 α1+α2≤1 [ (w⊤x− y)(α1 − α2)− ǫ(α1 + α2) ] .",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 20,
      "context" : "• l2 loss (Nie et al., 2010): l(W;x,y) = ‖W⊤x− y‖2 = max ‖α‖2≤1 α⊤(W⊤x− y), where y ∈ R is multiple class label vector and W = (w1, · · · ,wK).",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 35,
      "context" : ", Gw(w,α) + λ∂R(w) is not Lipschitz continuous due to the general non-smooth term R(w), which prevents previous convex-concave minimization scheme (Tseng, 2008; Nemirovski, 2005) not applicable.",
      "startOffset" : 147,
      "endOffset" : 178
    }, {
      "referenceID" : 19,
      "context" : "(iii) the primal variable w is updated by a composite gradient mapping (Nesterov, 2007) in step 5.",
      "startOffset" : 71,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "Similar to many other approaches for composite optimization (Duchi and Singer, 2009; Hu et al., 2009), we assume that the mapping in step 5 can be solved efficiently.",
      "startOffset" : 60,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "Remark 5 A similar algorithm with an extra primal variable is also proposed in a recent work (Chambolle and Pock, 2011).",
      "startOffset" : 93,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "We discuss the differences between the Pdprox method and the algorithm in (Chambolle and Pock, 2011) with our notations in Appendix C.",
      "startOffset" : 74,
      "endOffset" : 100
    }, {
      "referenceID" : 30,
      "context" : "Comparison with Pegasos on l2 regularizer We compare the proposed algorithm to the Pegasos algorithm (Shalev-Shwartz et al., 2011) 2 for minimizing the l2 regularized hinge loss.",
      "startOffset" : 101,
      "endOffset" : 130
    }, {
      "referenceID" : 31,
      "context" : "According to the common assumption of learning theory (Wu and Zhou, 2005; Smale and Zhou, 2003), the optimal λ is O(n−1/(τ+1)) if the probability measure can be approximated by the closure of RKHS Hκ with exponent 0 < τ ≤ 1.",
      "startOffset" : 54,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "(Duchi et al., 2008) has proposed more efficient algorithms for solving the projection problem.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "Remark 15 In Appendix C, we show that the updates on (wt,αt) of Algorithm 3 are essentially the same to the Algorithm 1 in (Chambolle and Pock, 2011), if we remove the extra dual variable in Algorithm 3 and the extra primal variable in Algorithm 1 in (Chambolle and Pock, 2011).",
      "startOffset" : 123,
      "endOffset" : 149
    }, {
      "referenceID" : 9,
      "context" : "Remark 15 In Appendix C, we show that the updates on (wt,αt) of Algorithm 3 are essentially the same to the Algorithm 1 in (Chambolle and Pock, 2011), if we remove the extra dual variable in Algorithm 3 and the extra primal variable in Algorithm 1 in (Chambolle and Pock, 2011).",
      "startOffset" : 251,
      "endOffset" : 277
    }, {
      "referenceID" : 9,
      "context" : "However, the difference is that in Algorithm 3, we maintain two dual variables and one primal variable at each iteration, while the Algorithm 1 in (Chambolle and Pock, 2011) maintains two primal variables and one dual variable at each iteration.",
      "startOffset" : 147,
      "endOffset" : 173
    }, {
      "referenceID" : 12,
      "context" : "More details can be found in (Duchi and Singer, 2009; Ji and Ye, 2009).",
      "startOffset" : 29,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "In addition, the authors in (Chambolle and Pock, 2011) suggested a two step sizes scheme with τ for updating the primal variable and σ for updating the dual variable.",
      "startOffset" : 28,
      "endOffset" : 54
    }, {
      "referenceID" : 22,
      "context" : "Furthermore, (Pock and Chambolle, 2011) presents a technique for computing diagonal preconditioners in the cases when estimating the value of c is difficult for complex problems, and applies it to general linear programing problems and some computer vision problems.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "(2) We can perform the computation by manipulating on a finite number of parameters due to the representer theorem provided that the regularizerR(g) is a monotonic norm (Bach et al., 2011).",
      "startOffset" : 169,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "((Bach et al., 2011) considers how to compute the proximal mapping in (18) for more general sparsity induced norms.",
      "startOffset" : 1,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "In (Dekel and Singer, 2006), the authors address a budget SVM problem by introducing a 1 − ∞ interpolation norm on the empirical hinge loss, leading to a sparsity constraint ‖α‖1 ≤ m on the dual variables, where m is the target number of support vectors.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 12,
      "context" : "The baseline first order methods used in this study include the gradient descent algorithm (gd), the forward and backward splitting algorithm (fobos) (Duchi and Singer, 2009), the regularized dual averaging algorithm (rda) (Xiao, 2009), the accelerated gradient descent algorithm (agd) (Chen et al.",
      "startOffset" : 150,
      "endOffset" : 174
    }, {
      "referenceID" : 10,
      "context" : "The baseline first order methods used in this study include the gradient descent algorithm (gd), the forward and backward splitting algorithm (fobos) (Duchi and Singer, 2009), the regularized dual averaging algorithm (rda) (Xiao, 2009), the accelerated gradient descent algorithm (agd) (Chen et al., 2009).",
      "startOffset" : 286,
      "endOffset" : 305
    }, {
      "referenceID" : 10,
      "context" : "2 l1,∞ regularization for Multi-Task Learning In this experiment, we perform multi-task regression with l1,∞ regularizer (Chen et al., 2009).",
      "startOffset" : 121,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "We use the School data set (Argyriou et al., 2008), a common dataset for multi-task learn-",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "We follow the setup in (Argyriou et al., 2008), and generate a training data set with 75% of the examples from each school and a testing data set with the remaining examples.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "34 (optimized by Pdprox), comparable to the performance reported in (Chen et al., 2009).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 27,
      "context" : "Hinge loss function is used in max-margin matrix factorization (Rennie and Srebro, 2005; Srebro et al., 2005), and absolute loss is used instead of square loss in matrix completion.",
      "startOffset" : 63,
      "endOffset" : 109
    }, {
      "referenceID" : 33,
      "context" : "Hinge loss function is used in max-margin matrix factorization (Rennie and Srebro, 2005; Srebro et al., 2005), and absolute loss is used instead of square loss in matrix completion.",
      "startOffset" : 63,
      "endOffset" : 109
    }, {
      "referenceID" : 27,
      "context" : "Since there are five distinct ratings that can be assigned to each movie, we follow (Rennie and Srebro, 2005; Srebro et al., 2005) by introducing four thresholds θ1,2,3,4 to measure the hinge loss between the predicted value Xij and the ground truth Yij .",
      "startOffset" : 84,
      "endOffset" : 130
    }, {
      "referenceID" : 33,
      "context" : "Since there are five distinct ratings that can be assigned to each movie, we follow (Rennie and Srebro, 2005; Srebro et al., 2005) by introducing four thresholds θ1,2,3,4 to measure the hinge loss between the predicted value Xij and the ground truth Yij .",
      "startOffset" : 84,
      "endOffset" : 130
    }, {
      "referenceID" : 27,
      "context" : "Note that we did not compare to the optimization algorithm in (Rennie and Srebro, 2005) since it cast the problem into a non-convex problem by using explicit factorization of X = UV⊤, which suffers a local minimum, and the optimization algorithm in (Srebro et al.",
      "startOffset" : 62,
      "endOffset" : 87
    }, {
      "referenceID" : 33,
      "context" : "Note that we did not compare to the optimization algorithm in (Rennie and Srebro, 2005) since it cast the problem into a non-convex problem by using explicit factorization of X = UV⊤, which suffers a local minimum, and the optimization algorithm in (Srebro et al., 2005) since it formulated the problem into a SDP problem, which suffers from a high computational cost.",
      "startOffset" : 249,
      "endOffset" : 270
    }, {
      "referenceID" : 20,
      "context" : "For example, in multitask regression with l2 loss (Nie et al., 2010), if the number of examples is much larger than the number of attributes, i.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "We also compare Pdprox using one step size and two step sizes, and compare them to the accelerated version proposed in (Chambolle and Pock, 2011) for strongly convex functions.",
      "startOffset" : 119,
      "endOffset" : 145
    }, {
      "referenceID" : 9,
      "context" : "(τ, σ)) and the accelerated version (Pdprox-ac(τ, σ)) proposed in (Chambolle and Pock, 2011) for strongly convex functions.",
      "startOffset" : 66,
      "endOffset" : 92
    } ],
    "year" : 2013,
    "abstractText" : "We study the non-smooth optimization problems in machine learning, where both the loss function and the regularizer are non-smooth functions. Previous studies on efficient empirical loss minimization assume either a smooth loss function or a strongly convex regularizer, making them unsuitable for non-smooth optimization. We develop a simple yet efficient method for a family of non-smooth optimization problems where the dual form of the loss function is bilinear in primal and dual variables. We cast a non-smooth optimization problem into a minimax optimization problem, and develop a primal dual prox method that solves the minimax optimization problem at a rate of O(1/T ) assuming that the proximal step can be efficiently solved, significantly faster than a standard subgradient descent method that has an O(1/ √ T ) convergence rate. Our empirical study verifies the efficiency of the proposed method for various non-smooth optimization problems that arise ubiquitously in machine learning by comparing it to the state-of-the-art first order methods.",
    "creator" : "LaTeX with hyperref package"
  }
}