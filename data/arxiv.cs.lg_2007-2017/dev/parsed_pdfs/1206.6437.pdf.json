{
  "name" : "1206.6437.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Large Scale Variational Bayesian Inference for  Structured Scale Mixture Models",
    "authors" : [ "Young Jun Ko", "Matthias Seeger" ],
    "emails" : [ "youngjun.ko@epfl.ch", "matthias.seeger@epfl.ch" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Leaps in performance have been realized for low-level computer vision problems, such as denoising, inpainting, deconvolution (debluring), image coding, undersampled reconstruction or acquisition optimization, by adopting super-Gaussian (“sparse”) image priors. While most such methods employ simple factorial priors on single coefficients or groups, further substantial gains can be obtained by modelling higher-order dependencies via structured non-factorial prior distributions (Portilla et al., 2003; Wipf & Nagarajan, 2008; Cevher et al., 2010). For example, representing the dependencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010). However, previous approaches employing such non-factorial priors either run much slower than standard factorial methodology, or sacrifice performance by adopting\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nsuboptimal MAP estimation or naive mean field factorization assumptions, ignoring posterior covariance or uncertainty altogether in problems which are highly underdetermined.\nIn this paper, we derive a large scale approximate Bayesian inference algorithm for generalized linear models with non-factorial (latent tree-structured) scale mixture priors. Our contributions are as follows:\n• An image model with scale mixture prior on multi-scale wavelet coefficients, based on a latent discrete tree distribution. Mixture potentials can have arbitrary super-Gaussian components.\n• A large scale double loop algorithm for Bayesian inference in these hybrid models. We do not require factorization assumptions between image pixels or wavelet coefficients. Our method is based on standard scalable technology (preconditioned conjugate gradients, penalized least squares) and can operate at the same scales as MAP estimation.\n• An extension to incorporate non-log-concave potentials, such as Student’s t, without sacrificing robustness of the optimization.\n• Automatic Bayesian learning of a substantial number of hyperparameters from raw data. Folded into the variational optimization, this process does not require much overhead. It improves performance very significantly.\n• An extensive evaluation on many inpainting and denoising datasets, comparing variational inference and MAP estimation for factorial and nonfactorial priors featuring different sparsity potentials.\nOur findings suggest (a) that predicting the variational posterior mean strongly and consistently outperforms the popular posterior mode (MAP estimation), (b) that non-factorial priors tend to\nboost performance compared to factorial ones, and (c) that Bayesian hyperparameter learning improves posterior mean prediction substantially compared to a default initialization.\nThe structure of the paper is as follows. We describe and motivate our image model in Section 2, and develop our large scale Bayesian inference and learning algorithm in Section 3. We present experimental results on image denoising and inpainting in Section 4, and close with conclusions."
    }, {
      "heading" : "1.1. Related Work",
      "text" : "A range of prior work has employed latent treestructured priors to represent dependencies between wavelet coefficients. Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities. They estimate parameters by nonlinear optimization. Papandreou et al. (2008) employ a hidden Markov tree and Gaussian mixture potentials on an overcomplete wavelet representation, estimating signal and parameters by Viterbi training. None of these employ Bayesian inference over the image or non-Gaussian potentials. He et al. (2010) use spike-and-slab prior potentials with a hidden Markov tree over the indicators. They perform standard naive mean field variational inference, employing a posterior distribution which does not represent any dependencies between coefficients. These assumptions lead to simple update equations, which they iterate in parallel. Their algorithm does not seem to reduce to standard scalable optimization primitives. Moreover, their method does not extend beyond spikeand-slab to other sparsity potentials.\nOur framework can be seen as extension of the scalable double loop algorithm for variational inference in super-Gaussian models proposed in (Seeger & Nickisch, 2011). However, they did not consider hybrid models (featuring discrete and continuous variables), latent tree non-factorial prior distributions, or Bayesian learning of hyperparameters, as we do here."
    }, {
      "heading" : "2. Structured Image Model",
      "text" : "In problems such as image denoising, inpainting or deconvolution (debluring), we seek to reconstruct a latent image u ∈ Rn (an ny × nx bitmap, where\nn = nynx) from noisy linear measurements y ∈ Rm:\ny = Xu + ε, ε ∼ N(0, σ2I),\nor P (y |u) = N(y |Xu, σ2I), where σ2 > 0 is the noise variance. For example, X = I for denoising, X = IJ,· for inpainting (J the index of observed pixel positions), or Xu = k ∗ u for deconvolution (k the blur kernel). With m ≤ n (more unknowns than measurements), noise and/or blur, only additional statistical information in form of a image prior distribution P (u) renders image reconstruction a well-posed problem. Given P (u), we can infer the image from the posterior distribution\nP (u|y) = P (y |u)P (u) P (y) , P (y) =\n∫ P (y |u)P (u) du.\n(1) For example, image statistics tend to be leptokurtic (super-Gaussian, or “sparse”), and even simple factorial image priors P (u) respecting these properties can lead to dramatic improvements over classical methodology (least squares, Wiener filtering). Beyond marginals, image statistics exhibit complex dependencies, and capturing these in non-factorial priors can lead to further leaps in performance (Portilla et al., 2003). However, with probabilistic inference becoming much more difficult and expensive, such extended models enjoy little popularity so far compared to simpler factorial alternatives. In contrast, the methodology1 developed in this paper scales up in the same way as MAP estimation and variational inference for factorial priors.\nConsider an orthonormal discrete wavelet transform B , and denote corresponding wavelet coefficients by s = Bu. A factorial image prior has the form P (u) ∝∏n j=1 tj(sj), where common super-Gaussian potentials include Laplacian\ntj(sj) ∝ τje−τj |sj |, τj > 0, (2)\nor Student’s t potentials\ntj(sj) ∝ τ1/2j ( 1 + (τj/ν)s 2 j )−(ν+1)/2 , τj , ν > 0. (3)\nIn the sequel, we assume that ∫ tj(sj) dsj = 1. Each coefficient sj belongs to a scale level of analysis l(j) ∈ {1, . . . , L}, l = 1 the coarsest, l = L the finest scale (assume that ny, nx are multiples of 2\nL). Even though the sj are approximately uncorrelated for natural images, it is well known that there are substantial causal dependencies between coefficients in neighbouring levels (Portilla et al., 2003). These are typically mod-\n1 Our model is somewhat simpler than that of (Portilla et al., 2003).\nelled by a quad-tree2 T , linking a coefficient at level l < L to four children at level l + 1. For example, energy localized at a fine scale coefficient percolates its way up the tree through coarser scales. In order to capture this signature in a non-factorial prior, we use binary variables δj ∈ {0, 1}, one for each sj , as well as mixture potentials tj(sj ; δj) = t0j(sj) 1−δj t1j(sj) δj , where t0j(sj) enforces sj ≈ 0 more strongly than t1j(sj). Each coefficient sj can be in low (δj = 0) or high (δj = 1) state, with |sj | penalized accordingly. Moreover, we use a prior P (δ) of directed graphical quad-tree structure T , which encourages the inheritance of high/low states from a parent node to its children: P (δ) = ∏ j P (δj |δπ(j)), where π(j) is the parent node of j in the quad-tree. All nodes of a level share a common conditional probability table, so that P (δ) is parameterized by 2L hyperparameters θr,l ≡ P (δj = 1|δπ(j) = r) for l(j) = l. The implied prior on the image\nP (u) = ∑ δ n∏ j=1 t0j(sj) 1−δj t1j(sj)\nδj︸ ︷︷ ︸ =:tj(sj ;δj) P (δ), s = Bu,\n(4) is a non-factorial scale mixture model, which faithfully represents the causal inheritance between wavelet coefficient sizes from coarse to fine levels. The normalization constant of P (u) is one, sinceB is an orthonormal transform.\nIn Figure 1, we illustrate the effect a non-factorial prior (4) on results for an inpainting problem (75% pixels removed). The latent tree prior employs Student’s t potentials t1j(sj) for the high, Gaussian potentials t0j(sj) = N(0, ξ −1 0l(j)) for the low state, where hyperparameters τ1l, ξ0l (two at each level) are learned automatically (results in third, δj marginals in fourth row). We also show results for a factorial prior with Student’s t potentials for comparison (second row), whose hyperparameters τl are learned in the same way. The nonfactorial prior leads to a more faithful reconstruction of wavelet coefficient at coarser scales, which motivates superior inpainting results in Section 4.\nThe outcome of our procedure is an approximate posterior distribution Q(u|y)Q(δ |y). The covariance of Q(u|y) can be used for decision making, e.g. Bayesian experimental design (Seeger & Nickisch, 2011), and our results indicate that it helps the hyperparameter learning. Moreover, we predict the (approximate) posterior mean E[u|y ], not the posterior mode (MAP estimation). Our experiments demonstrate that the\n2 T is the computation tree for the fast Haar wavelet transform and describes dominating dependencies also for wavelets with larger support.\nvariational posterior mean leads to superior results in strongly ill-posed problems."
    }, {
      "heading" : "3. Large Scale Variational Inference",
      "text" : "In this section, we derive a scalable algorithm for computing variational approximations to the posterior (1) for a non-factorial scale mixture prior (4). There are\nobviously strong dependencies between components of u or s, and in contrast to previous work (He et al., 2010), we do not require any factorization assumptions between them. The high level idea behind our approach is iterative decoupling. We combine a standard variational bound to decouple u and δ (allowing us to tackle inference over the latter by belief propagation) with the double loop framework of (Seeger & Nickisch, 2011), which decouples mean and covariance computations over u. The latter provides a computational reduction to convex penalized least squares optimization and Gaussian sampling, which is crucial for scalability. As is shown below, minor simplifications result in inference or maximum a posteriori (MAP) estimation algorithms for non-factorial or factorial priors, all based on the same underlying code.\nFor the purpose of image priors, we restrict ourselves to even, super-Gaussian potentials trj(sj) (Palmer et al., 2006), which can be represented as\n− 2 log t(s) = min γ≥0 s2/γ + h(γ). (5)\nIntuitively, t(s) can be tightly lower bounded by (unnormalized) Gaussians of any variance γ. Here, h(γ) is convex if and only if −2 log t(s) is convex (Seeger & Nickisch, 2011). Both Laplacian (2) and Student’s t potentials (3) are super-Gaussian, while −2 log t(s) is convex for Laplacian, but not for Student’s t potentials. The criterion we will minimize is an upper bound on the negative log marginal likelihood −2 logP (y) from (1). First, we introduce γ from (5), consisting of γ0j for t0j , h0j and γ1j for t1j , h1j , and pull minγ out of the integral:\n− 2 logP (y) = −2 log ∫ P (y |u)P (u) du\n≤ min γ −2 log ∑ δ ∫ P (y |u)e− 12s T (diagπ)sP (δ) du\n+ ∑\nj (1− δj)h0j(γ0j) + δjh1j(γ1j)︸ ︷︷ ︸\n=:h(γ ;δ)\n, πj := 1−δj γ0j + δj γ1j ,\nwhere s = Bu. Second, we apply the variational mean field bound to the remaining log partition function:\nmax Q(u|y),Q(δ |y) EQ\n[ log\nP (y |u)e− 12sT (diagπ)sP (δ) Q(u|y)Q(δ |y)\n] ,\nusing the factorization assumption Q(u, δ |y) = Q(u|y)Q(δ |y). In the sequel, we denote EQ(δ |y)[·] by 〈·〉. The bound maximizer is Q(u|y) = ZQ(〈π〉)−1P (y |u)e− 1 2s T (diag〈π〉)s , where ZQ(〈π〉) is a Gaussian partition function. Plugging this in, we ob-\ntain the bound\nmin Q(δ |y),γ\n−2 logZQ(〈π〉)+h(γ ; 〈δ〉)+2D[Q(δ |y) ‖P (δ)],\nusing that h(γ ; δ) is linear in δ . The relative entropy term is defined as D[Q(δ |y) ‖P (δ)] = 〈logQ(δ |y) − logP (δ)〉. Finally, and crucially for scalability, we use the variational transformation from (Seeger & Nickisch, 2011): −2 logZQ is equal to\nmin u∗,z (z + s2∗) T 〈π〉+ σ−2‖y −Xu∗‖2︸ ︷︷ ︸\n=:Rz (u∗,〈π〉)\n−g∗(z),\nwhere s∗ = Bu∗, and z 0. If A(π) := σ−2XTX + BT (diagπ)B denotes the inverse covariance matrix of Q(u|y), then π 7→ log |A(π)| is concave, and the −2 logZQ representation is based on the corresponding Fenchel duality. Since the dependence of A on δ is through π , neither z nor g∗(z) depends on δ . Plugging this in and pulling minu∗,z outside, we obtain our final upper bound on −2 logP (y):\nφ = Rz (u∗, 〈π〉)+h(γ ; 〈δ〉)+2D[Q(δ |y) ‖P (δ)]−g∗(z),\nto be minimized over Q(δ |y), z , u∗, γ . Notice that 〈π(δ)〉 = π(〈δ〉) by linearity.\nWe adapt the scalable convergent double loop algorithm from (Seeger & Nickisch, 2011). During the inner loop, we fix z and minimize φ over Q(δ |y) and (u∗,γ). In between, once per outer loop iteration, we update z to obtain a tangential fit to log |A(〈π〉)|: z ← VarQ[s|y ], where Q(u|y) is based on 〈π〉 for the current γ and 〈δ〉. Computing these Gaussian variances is the most computationally intensive part of a variational inference method. We approximate z using the Perturb&MAP technique from (Papandreou & Yuille, 2010), at the cost of solving a small number of linear systems A(〈π〉)xk = rk by preconditioned conjugate gradients.\nIn the inner loop, we update Q(δ |y) and (u∗,γ) alternatingly. For the latter, we can eliminate γ by reversing the super-Gaussian representation of− log trj(s∗j), plugging in pj := (zj +(s∗j)\n2)1/2 instead of s∗j . Dropping terms independent of u∗, we need to solve\nmin u∗ σ−2‖y −Xu∗‖2 − 2 ∑ j log tj (pj ; 〈δj〉) ,\npj = √ zj + (s∗j)2, s∗ = Bu∗.\n(6)\nHere, we used that log tj(sj ; δj) is linear in δj . This is a standard-form penalized least squares problem, which can be solved by any of a large number of recent algorithms developed for MAP estimation. In our experiments, we employ a nonlinear conjugate gradients\nalgorithm from the glm-ie toolbox (see Section 4). Importantly, this inner loop problem is convex iff the − log trj(sj) are convex, thus iff MAP estimation is convex for the same model (the precise relationship to MAP is detailed shortly). It is not convex if Student’s t potentials (3) are used, and we will use additional bounding in order to retain inner loop convexity (Section 3.3). For the update of Q(δ |y), note that\nφ . = 〈 −2 ∑\nj log tj(pj ; δj)\n〉 + 2D[Q(δ |y) ‖P (δ)]\n. = 2 〈 log\nQ(δ |y) P (δ) ∏ j e log tj(pj ;δj)\n〉 ,\nwhere “ . =” denotes equality up to an additive constant. We can read off the minimizer Q(δ |y) ∝ P (δ) ∏ j tj(pj ; δj). This is a distribution of the same quad-tree structure T as P (δ), differing from the latter in the single node potentials only. We can compute both 〈δ〉 and D[Q(δ |y) ‖P (δ)] in O(n), using Pearl’s belief propagation algorithm.\nThis completes the description of our large scale inference algorithm. At convergence, u∗ constitutes our posterior mean prediction EQ[u|y ]. Apart from simple direct primitives, it reduces entirely to solving a moderate number of linear systems A(π)x = r for different (π , r), which can be done very efficiently by stateof-the-art preconditioned conjugate gradients solvers. Our double loop structure implies that the most expensive updates of z have to be done least frequently."
    }, {
      "heading" : "3.1. MAP Estimation. Factorial Priors",
      "text" : "We derived an algorithm for variational Bayesian inference with a structured non-factorial prior (4). Importantly, we can obtain scalable algorithms for MAP estimation or inference with factorial or non-factorial priors by making minor simplifying modification, otherwise using exactly the same underlying code. First, when using a factorial prior of the form P (u) =∏ j tj(sj), we simply eliminate t1j , set 〈δj〉 = 0 and eliminate Q(δ |y) altogether. The inner loop now consists of a single penalized least squares problem (6). Second, MAP estimation is obtained by simply setting z = 0, skipping variances computations and running a single outer loop iteration. MAP estimation for a non-factorial prior proceeds in an expectationmaximization fashion, alternating between penalized least squares (PLS) and belief propagation (Crouse et al., 1998)."
    }, {
      "heading" : "3.2. Learning Prior Hyperparameters",
      "text" : "In order to obtain best performance, it is necessary to endow an image prior P (u) (whether factorial or not)\nwith a substantial number of hyperparameters, which have to be adjusted to the problem at hand. For example, wavelet coefficients sj exhibit higher variance at coarser than at finer scales l(j), and corresponding prior potentials tj(sj) should take this into account, via τj in (2) or (3), or ξj in Gaussians N(0, ξ −1 j ). In our experiments with non-factorial priors, we use L = 8 scale levels, giving rise to 16 hyperparameters (one for each level l and low/high state r): too many to be reasonably set by non-Bayesian methods like cross-validation. In this section, we show how to learn hyperparameters in an automatic Bayesian way. Our method is folded into the variational inference process, which lets us optimize hyperparameters for each dataset at hand. We stress that Bayesian learning operates on the raw data (noisy, incomplete, blurred): clean underlying images are not required.\nBayesian learning works by maximizing the log marginal likelihood logP (y) w.r.t. hyperparameters. The obvious variational approximation is to maximize the lower bound (or, equivalently, minimize φ) instead. Indeed, we can treat the hyperparameters (say, θ) as just another set of parameters to minimize φ over, thereby folding the learning into the inference approximation. Importantly, we can update θ as part of our inner loop optimization, for fixed z , without compromising overall convergence (to a stationary point). Recall that z comes from the Fenchel duality log |A(〈π〉)| = minz zT 〈π〉 − g∗(z). Since A depends on all other parameters3 only through 〈π〉, there is no direct dependence between z and θ. Suppose that all potentials trj(sj) for fixed r ∈ {0, 1} and l = l(j) share a hyperparameter τrl. Denote qrj := Q(δj = r|y). We write “j : l” short for “j : l(j) = l”. The relevant part of φ is\nφ . = −2 ∑ j:l qrj log trj(pj), pj = √ zj + (s∗j)2.\nWe need to minimize φ w.r.t. τrl, which can often be done analytically. For Laplacian potentials (2):\nφ . = 2 ∑ j:l qrj (τrlpj − log τrl) ⇒ τrl ← ∑ j:l qrj∑ j:l qrjpj .\nFor Gaussian potentials trj(sj) ∝ ξ1/2rl e − 12 ξrls 2 j :\nφ . = ∑ j:l qrj ( ξrlp 2 j − log ξrl ) ⇒ ξrl ← ∑ j:l qrj∑ j:l qrjp 2 j .\nFor Student’s t potentials, we update hyperparameters\n3 An exception would be the noise variance σ2. We can decouple log |A(〈π〉)| w.r.t. σ2 as well, but this is not done here. σ2 is fixed in our experiments.\nτrl only once per outer loop iteration, as detailed in Section 3.3.\nRecall the parameterization of the tree prior P (δ) in terms of θr,l from Section 2. The relevant criterion part is φ . = 〈logP (δ)〉, which implies the updates\nθr,l ←  ∑ j:1 q1j∑ j:1 1 l = 1∑ j:l q1rj∑\nj:l(q1rj+q0rj) l > 1\nwhere qkrj := Q(δj = k, δπ(j) = r|y), k, r = 0, 1, are double node marginals."
    }, {
      "heading" : "3.3. Student’s T Potentials",
      "text" : "When applied to models featuring Student’s t potentials (3), the algorithm just detailed requires nonconvex PLS problems (6) to be solved in the inner loop. Commonly used first order solvers can fail dramatically on non-convex problems. Since we adopt a double loop strategy anyway, it is simpler and far more robust to use additional bounding in order to obtain a convex inner loop problem. This idea has previously been described in (Seeger & Nickisch, 2011) and applied to Student’s t potentials, but our applications here, as well as our hyperparameter learning method, are novel.\nRecall the representation (5) of t(s). For a Student’s t potential, h(γ) is not convex, but can be written as sum of a convex term h∪(γ) and a concave term h∩(γ) (Seeger & Nickisch, 2011, Appendix A.6). Moreover, the concave part can be represented by Fenchel duality: h∩(γ) = mine>0 eγ − g∗∩(e). Overall, we end up with a further parameter vector e = [erj ], which is updated alongside z . Define t∪(s; e) as\n−2 log t∪(s; e) = min γ≥0 s2/γ + h∪(γ) + eγ.\nA minimum over jointly convex functions in (s, γ), this is a convex function in s. We end up with a modified convex inner loop PLS problem of the form (6), where −2 log trj(pj) is replaced by −2 log t∪;rj(pj ; erj) for every Student’s t potential trj . Here, −2 log t∪(s; e) is easily computed by a single case distinction. We have to replace log trj(pj) by log t∪;rj(pj ; erj) also in the update of Q(δ |y).\nThe τrl hyperparameters for Student’s t potentials are updated once per outer loop, alongside the erj . We use the same procedure as in Section 3.2, but applied to −2 log trj(pj), not its convexification. The relevant criterion part is\nφ . = ∑ j:l qrj ( (ν + 1) log ( 1 + (p2j/ν)e log τrl ) − log τrl ) .\nThis is a smooth convex function in log τrl, which is easily minimized by a one-dimensional Newton solver. Once all Student’s t hyperparameters have been updated, we refit the corresponding erj and continue with another inner loop."
    }, {
      "heading" : "4. Experiments",
      "text" : "We present experiments on a range of denoising and inpainting problems, comparing variational inference and MAP estimation for different models. Our results are averaged over 77 frequently used images (greyscale, 256×256), a dataset4 from (Seeger & Nickisch, 2008). Our implementation is based on the glm-ie toolbox (www.mloss.org/software/view/269/). We compare 8 methods: MAP estimation (MAP) vs. variational inference (VB), factorial prior (fact) vs. latent tree scale mixture prior (tree), and Laplacian (Lap) vs. Student’s t potentials (T). The Lap-tree model uses two Laplace potentials (2) t0j(sj), t1j(sj) with different hyperparameters τ0l, τ1l, a pair for each level. The T-tree model employs Gaussian N(sj |0, ξ−10l ) for the low, Student’s t potentials (3) for the high state, with a pair of hyperparameters (ξ0l, τ1l) at each level. We use 2L hyperparameters in the tree, L (namely, {τl}) in the fact setups. The Student’s t shape parameter ν is fixed to 2.1. For each run, we initialize hyperparameters θ as in (Crouse et al., 1998), by maximizing the prior probability of the raw5 data y (for the tree cases, this involves a few steps of expectation maximization), then optimize them by minimizing φ. Hyperparameters were updated once per outer loop iteration.\nVB runs use up to 15 outer loop (OL) iterations. Perturb&MAP estimation of z , required for inpainting only, is run with 30 samples à 70 conjugate gradients (CG) iterations. We did 3 belief propagation and PLS calls per OL iteration for tree setups, PLS ran up to 150 iterations of nonlinear CG. Each iteration of CG requires two matrix-vector multiplications with B and X . These choices have not been optimized for maximum efficiency."
    }, {
      "heading" : "4.1. Denoising",
      "text" : "We add Gaussian random noise of variance σ2 = 0.01 to each image (with pixel values ui ∈ [0, 1]). All methods use the correct value of σ2 in their likelihood. Notice that in this case, Gaussian variances z can be computed exactly at no cost. Namely,\n4 Thanks to H. Nickisch for providing the data. We added 2 further images.\n5 For inpainting, the missing pixels are set to mean(yi).\nA(π) = σ−2I +BTΠB , where BTB = I, so that\nz = diag−1 ( BA(π)−1BT ) = (σ−21 + π)−1.\nTherefore, Perturb&MAP, the dominating cost for VB in general, is not required. Results are shown in Table 1. For this application, differences between MAP and VB reconstruction are not significant. On the other hand, the non-factorial prior improves PSNR somewhat. Hyperparameter learning improves VB performance substantially, especially when Student’s t potentials are used. In contrast, it does not help6 (and can even hurt) MAP performance."
    }, {
      "heading" : "4.2. Inpainting",
      "text" : "We remove 75% of pixels at random, using the same mask J ⊂ {1, . . . , n} for all images. The design matrix is X = IJ,·, the noise variance was fixed to σ\n2 = 10−5. Results are shown in Table 2. As PSNR does not always correlate well with visual quality, we show a range of images in Figure 2, Figure 3, Figure 4.\nVB posterior mean predictions are clearly superior to MAP reconstruction, and VB with non-factorial latent tree prior performs best. While VB with a factorial Laplace prior (Lap-fact) shows similar PSNR values to VB-Lap-tree, the visual appearance of results with the latter is clearly superior (further results, provided in the supplemental material, support these findings). The additional runtime compared to MAP estimation, mainly due to the estimation of variances z , pays off for these problems.\n6 There is no justification for maximizing the posterior w.r.t. θ . We include these results only for the fact that “alternating MAP” learning is frequently done in practice."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We presented a double loop algorithm for variational Bayesian inference in linear models with non-factorial scale mixture priors, based on a latent discrete tree distribution. Our method can operate at the same scales as MAP estimation, yet its (approximate) posterior mean prediction strongly and consistently outperforms the posterior mode across a range of inpainting problems. Both the selective smoothing by predictive variances and the coupling of wavelet coefficient across scales by way of the latent tree contribute to the removal of artefacts which plague results of MAP estimation, and of inference with factorial priors. Free hyperparameters are learned automatically by marginal likelihood maximization folded into the variational optimization.\nIn future work, we will try to adapt our large scale inference methodology to more complex hierarchical models, featuring non-Gaussian continuous latent variables (Portilla et al., 2003)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Support through a DFG Sachbeihilfe SE 2008/1-1 (AOBJ 578593) and an ERC Starting Grant (277815 – SCALABIM) are gratefully acknowledged."
    } ],
    "references" : [ {
      "title" : "Sparse signal acquisition and recovery with graphical models",
      "author" : [ "V. Cevher", "P. Indyk", "L. Carin", "R. Baraniuk" ],
      "venue" : "IEEE Sig. Proc. Mag.,",
      "citeRegEx" : "Cevher et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cevher et al\\.",
      "year" : 2010
    }, {
      "title" : "Waveletbased statistical signal processing using hidden Markov models",
      "author" : [ "M. Crouse", "R. Nowak", "R. Baraniuk" ],
      "venue" : "IEEE Trans. Sig. Proc.,",
      "citeRegEx" : "Crouse et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Crouse et al\\.",
      "year" : 1998
    }, {
      "title" : "Tree-structured compressive sensing with variational Bayesian analysis",
      "author" : [ "L. He", "H. Chen", "L. Carin" ],
      "venue" : "IEEE Sig. Proc. Letters,",
      "citeRegEx" : "He et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2010
    }, {
      "title" : "Variational EM algorithms for non-Gaussian latent variable models",
      "author" : [ "J. Palmer", "D. Wipf", "K. Kreutz-Delgado", "B. Rao" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Palmer et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2006
    }, {
      "title" : "Gaussian sampling by local perturbations",
      "author" : [ "G. Papandreou", "A. Yuille" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Papandreou and Yuille,? \\Q2010\\E",
      "shortCiteRegEx" : "Papandreou and Yuille",
      "year" : 2010
    }, {
      "title" : "Image inpainting with a wavelet domain hidden markov tree model",
      "author" : [ "G. Papandreou", "P. Maragos", "A. Kokaram" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Papandreou et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Papandreou et al\\.",
      "year" : 2008
    }, {
      "title" : "Image denoising using Gaussian scale mixtures in the wavelet domain",
      "author" : [ "J. Portilla", "V. Strela", "M. Wainwright", "E. Simoncelli" ],
      "venue" : "IEEE Trans. Image Proc.,",
      "citeRegEx" : "Portilla et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Portilla et al\\.",
      "year" : 2003
    }, {
      "title" : "Compressed sensing and Bayesian experimental design",
      "author" : [ "M. Seeger", "H. Nickisch" ],
      "venue" : "In ICML",
      "citeRegEx" : "Seeger and Nickisch,? \\Q2008\\E",
      "shortCiteRegEx" : "Seeger and Nickisch",
      "year" : 2008
    }, {
      "title" : "Large scale Bayesian inference and experimental design for sparse linear models",
      "author" : [ "M. Seeger", "H. Nickisch" ],
      "venue" : "SIAM J. Imag. Sciences,",
      "citeRegEx" : "Seeger and Nickisch,? \\Q2011\\E",
      "shortCiteRegEx" : "Seeger and Nickisch",
      "year" : 2011
    }, {
      "title" : "A new view of automatic relevance determination",
      "author" : [ "D. Wipf", "S. Nagarajan" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Wipf and Nagarajan,? \\Q2008\\E",
      "shortCiteRegEx" : "Wipf and Nagarajan",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "While most such methods employ simple factorial priors on single coefficients or groups, further substantial gains can be obtained by modelling higher-order dependencies via structured non-factorial prior distributions (Portilla et al., 2003; Wipf & Nagarajan, 2008; Cevher et al., 2010).",
      "startOffset" : 219,
      "endOffset" : 287
    }, {
      "referenceID" : 0,
      "context" : "While most such methods employ simple factorial priors on single coefficients or groups, further substantial gains can be obtained by modelling higher-order dependencies via structured non-factorial prior distributions (Portilla et al., 2003; Wipf & Nagarajan, 2008; Cevher et al., 2010).",
      "startOffset" : 219,
      "endOffset" : 287
    }, {
      "referenceID" : 1,
      "context" : "For example, representing the dependencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010).",
      "startOffset" : 171,
      "endOffset" : 234
    }, {
      "referenceID" : 5,
      "context" : "For example, representing the dependencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010).",
      "startOffset" : 171,
      "endOffset" : 234
    }, {
      "referenceID" : 2,
      "context" : "For example, representing the dependencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010).",
      "startOffset" : 171,
      "endOffset" : 234
    }, {
      "referenceID" : 1,
      "context" : "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities.",
      "startOffset" : 0,
      "endOffset" : 221
    }, {
      "referenceID" : 1,
      "context" : "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities. They estimate parameters by nonlinear optimization. Papandreou et al. (2008) employ a hidden Markov tree and Gaussian mixture potentials on an overcomplete wavelet representation, estimating signal and parameters by Viterbi training.",
      "startOffset" : 0,
      "endOffset" : 411
    }, {
      "referenceID" : 1,
      "context" : "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities. They estimate parameters by nonlinear optimization. Papandreou et al. (2008) employ a hidden Markov tree and Gaussian mixture potentials on an overcomplete wavelet representation, estimating signal and parameters by Viterbi training. None of these employ Bayesian inference over the image or non-Gaussian potentials. He et al. (2010) use spike-and-slab prior potentials with a hidden Markov tree over the indicators.",
      "startOffset" : 0,
      "endOffset" : 668
    }, {
      "referenceID" : 6,
      "context" : "Beyond marginals, image statistics exhibit complex dependencies, and capturing these in non-factorial priors can lead to further leaps in performance (Portilla et al., 2003).",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 6,
      "context" : "Even though the sj are approximately uncorrelated for natural images, it is well known that there are substantial causal dependencies between coefficients in neighbouring levels (Portilla et al., 2003).",
      "startOffset" : 178,
      "endOffset" : 201
    }, {
      "referenceID" : 6,
      "context" : "1 Our model is somewhat simpler than that of (Portilla et al., 2003).",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "obviously strong dependencies between components of u or s, and in contrast to previous work (He et al., 2010), we do not require any factorization assumptions between them.",
      "startOffset" : 93,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "For the purpose of image priors, we restrict ourselves to even, super-Gaussian potentials trj(sj) (Palmer et al., 2006), which can be represented as",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "MAP estimation for a non-factorial prior proceeds in an expectationmaximization fashion, alternating between penalized least squares (PLS) and belief propagation (Crouse et al., 1998).",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 1,
      "context" : "For each run, we initialize hyperparameters θ as in (Crouse et al., 1998), by maximizing the prior probability of the raw data y (for the tree cases, this involves a few steps of expectation maximization), then optimize them by minimizing φ.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "In future work, we will try to adapt our large scale inference methodology to more complex hierarchical models, featuring non-Gaussian continuous latent variables (Portilla et al., 2003).",
      "startOffset" : 163,
      "endOffset" : 186
    } ],
    "year" : 2012,
    "abstractText" : "Natural image statistics exhibit hierarchical dependencies across multiple scales. Representing such prior knowledge in non-factorial latent tree models can boost performance of image denoising, inpainting, deconvolution or reconstruction substantially, beyond standard factorial “sparse” methodology. We derive a large scale approximate Bayesian inference algorithm for linear models with nonfactorial (latent tree-structured) scale mixture priors. Experimental results on a range of denoising and inpainting problems demonstrate substantially improved performance compared to MAP estimation or to inference with factorial priors.",
    "creator" : "LaTeX with hyperref package"
  }
}